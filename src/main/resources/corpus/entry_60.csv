2019,A Direct tilde{O}(1/epsilon) Iteration Parallel Algorithm for Optimal Transport,Optimal transportation  or computing the Wasserstein or ``earth mover's'' distance between two $n$-dimensional distributions  is a fundamental primitive which arises in many learning and statistical settings. We give an algorithm which solves the problem to additive $\epsilon$ accuracy with $\tilde{O}(1/\epsilon)$ parallel depth and $\tilde{O}\left(n^2/\epsilon\right)$ work. 
		\cite{BlanchetJKS18  Quanrud19} obtained this runtime through reductions to positive linear programming and matrix scaling. However  these reduction-based algorithms use subroutines which may be impractical due to requiring solvers for second-order iterations (matrix scaling) or non-parallelizability (positive LP). Our methods match the previous-best work bounds by \cite{BlanchetJKS18  Quanrud19} while either improving parallelization or removing the need for linear system solves  and improve upon the previous best first-order methods running in time $\tilde{O}(\min(n^2 / \epsilon^2  n^{2.5}  / \epsilon))$ \cite{DvurechenskyGK18  LinHJ19}. We obtain our results by a primal-dual extragradient method  motivated by recent theoretical improvements to maximum flow \cite{Sherman17}.,A Direct ˜O(1/) Iteration Parallel Algorithm for

Optimal Transport

Arun Jambulapati  Aaron Sidford  and Kevin Tian
{jmblpati sidford kjtian}@stanford.edu

Stanford University

Abstract

Optimal transportation  or computing the Wasserstein or “earth mover’s” distance
between two n-dimensional distributions  is a fundamental primitive which arises
in many learning and statistical settings. We give an algorithm which solves the

problem to additive  accuracy with ˜O(1/) parallel depth and ˜O(cid:0)n2/(cid:1) work.

[BJKS18  Qua19] obtained this runtime through reductions to positive linear pro-
gramming and matrix scaling. However  these reduction-based algorithms use
subroutines which may be impractical due to requiring solvers for second-order
iterations (matrix scaling) or non-parallelizability (positive LP). Our methods
match the previous-best work bounds by [BJKS18  Qua19] while either improv-
ing parallelization or removing the need for linear system solves  and improve
upon the previous best ﬁrst-order methods running in time ˜O(min(n2/2  n2.5/))
[DGK18  LHJ19]. We obtain our results by a primal-dual extragradient method 
motivated by recent theoretical improvements to maximum ﬂow [She17].

1

Introduction

Optimal transport is playing an increasingly important role as a subroutine in tasks arising
in machine learning [ACB17]  computer vision [BvdPPH11  SdGP+15]  robust optimization
[EK18  BK17]  and statistics [PZ16]. Given these applications for large scale learning  design-
ing algorithms for efﬁciently approximately solving the problem has been the subject of extensive
recent research [Cut13  AWR17  GCPB16  CK18  DGK18  LHJ19  BJKS18  Qua19].
Given two vectors r and c in the n-dimensional probability simplex ∆n and a cost matrix C ∈
Rn×n≥0

(cid:110)
(cid:111)
X ∈ Rn×n≥0   X1 = r  X(cid:62)1 = c

.

(1)

1  the optimal transportation problem is
(cid:104)C  X(cid:105)  where Ur c

def=

min
X∈Ur c

This problem arises from deﬁning the Wasserstein or Earth mover’s distance between discrete prob-
ability measures r and c  as the cheapest coupling between the distributions  where the cost of the
coupling X ∈ Ur c is (cid:104)C  X(cid:105). If r and c are viewed as distributions of masses placed on n points in
some space (typically metric)  the Wasserstein distance is the cheapest way to move mass to trans-
form r into c. In (1)  X represents the transport plan (Xij is the amount moved from ri to cj) and
C represents the cost of movement (Cij is the cost of moving mass from ri to cj).
Throughout  the value of (1) is denoted OPT. We call ˆX ∈ Ur c an -approximate transportation
plan if (cid:104)C  ˆX(cid:105) ≤ OPT + . Our goal is to design an efﬁcient algorithm to produce such a ˆX.

1Similarly to earlier works  we focus on square matrices; generalizations to rectangular matrices are straight-

forward.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Our Contributions
Our main contribution is an algorithm running in ˜O((cid:107)C(cid:107)max/) parallelelizable iterations2 and
˜O(n2(cid:107)C(cid:107)max/) total work producing an -approximate transport plan.
Matching runtimes were given in the recent work of [BJKS18  Qua19]. Their runtimes were ob-
tained via reductions to matrix scaling and positive linear programming  each well-studied problems
in theoretical computer science. However  the matrix scaling algorithm is a second-order Newton-
type method which makes calls to structured linear system solvers  and the positive LP algorithm
is not parallelizable (i.e. has depth polynomial in dimension). These features potentially limit the
practicality of these algorithms. The key remaining open question this paper addresses is  is there an
efﬁcient ﬁrst-order  parallelizable algorithm for approximating optimal transport? We answer this
afﬁrmatively and give an efﬁcient  parallelizable primal-dual ﬁrst-order method; the only additional
overhead is a scheme for implementing steps  incurring roughly an additional log −1 factor.
Our approach heavily leverages the recent improvement to the maximum ﬂow problem  and more
broadly two-player games on a simplex ((cid:96)1 ball) and a box ((cid:96)∞ ball)  due to the breakthrough
work of [She17]. First  we recast (1) as a minimax game between a box and a simplex  proving
correctness via a rounding procedure known in the optimal transport literature. Second  we show
how to adapt the dual extrapolation scheme under the weaker convergence requirements of area-
convexity  following [She17]  to obtain an approximate minimizer to our primal-dual objective in
the stated runtime. En route  we slightly simplify analysis in [She17] and relate it more closely to
the existing extragradient literature.
Finally  we give preliminary experimental evidence showing our algorithm can be practical  and
highlight some open directions in bridging the gap between theory and practice of our method  as
well as accelerated gradient schemes [DGK18  LHJ19] and Sinkhorn iteration.

1.2 Previous Work

Optimal Transport. The problem of giving efﬁcient algorithms to ﬁnd -approximate transport
plans ˆX which run in nearly linear time3 has been addressed by a line of recent work  starting with
[Cut13] and improved upon in [GCPB16  AWR17  DGK18  LHJ19  BJKS18  Qua19]. We brieﬂy
discuss their approaches here.
Works by [Cut13  AWR17] studied the Sinkhorn algorithm  an alternating minimization scheme.
Regularizing (1) with an η−1 multiple of entropy and computing the dual  we arrive at the problem

1(cid:62)BηC(x  y)1 − r(cid:62)x − c(cid:62)y where BηC(x  y)ij = exi+yj−ηCij .

min
x y∈Rn

This problem is equivalent to computing diagonal scalings X and Y for M = exp(−ηC) such that
XM Y has row sums r and column sums c. The Sinkhorn iteration alternates ﬁxing the row sums
and the column sums by left and right scaling by diagonal matrices until an approximation of such
scalings is found  or equivalently until XM Y is close to being in Ur c.
As shown in [AWR17]  we can round the resulting almost-transportation plan to a transportation
the objective. Further  [AWR17] showed that ˜O((cid:107)C(cid:107)3
max/3) iterations of this scheme sufﬁced
to obtain a matrix which /(cid:107)C(cid:107)max-approximately meets the demands in (cid:96)1 with good objective
value  by analyzing it as an instance of mirror descent with an entropic regularizer. The same
work proposed an alternative algorithm  Greenkhorn  based on greedy coordinate descent. [DGK18 

plan which lies in Ur c in linear time  losing at most 2(cid:107)C(cid:107)max((cid:107)X1 − r(cid:107)1 +(cid:13)(cid:13)X(cid:62)1 − c(cid:13)(cid:13)1) in
max/2(cid:1) work  sufﬁce
LHJ19] showed that ˜O(cid:0)(cid:107)C(cid:107)2

max/2(cid:1) iterations  corresponding to ˜O(cid:0)n2(cid:107)C(cid:107)2

for both Sinkhorn and Greenkhorn  the current state-of-the-art for this line of analysis.
An alternative approach based on ﬁrst-order methods was studied by [DGK18  LHJ19]. These works
considered minimizing an entropy-regularized Equation 1; the resulting weighted softmax function
is prevalent in the literature on approximate linear programming [Nes05]  and has found similar

2Our iterations consist of vector operations and matrix-vector products  which are easily parallelizable.

Throughout (cid:107)C(cid:107)max is the largest entry of C.
(where the size of input C is n2)  and polynomial dependence on (cid:107)C(cid:107)max   −1.

3We use “nearly linear” to describe complexities which have an n2polylog(n) dependence on the dimension

2

applications in near-linear algorithms for maximum ﬂow [She13  KLOS14  ST18] and positive linear
programming [You01  AO15]. An unaccelerated algorithm  viewable as (cid:96)∞ gradient descent  was
analyzed in [DGK18] and ran in ˜O((cid:107)C(cid:107)max/2) iterations. Further  an accelerated algorithm was
discussed  for which the authors claimed an ˜O(n1/4(cid:107)C(cid:107)0.5
max/) iteration count. [LHJ19] showed
that the algorithm had an additional dependence on a parameter as bad as n1/4  roughly due to a
gap between the (cid:96)2 and (cid:96)∞ norms. Thus  the state of the art runtime in this line is the better of

max/(cid:1)  ˜O(cid:0)n2(cid:107)C(cid:107)max/2(cid:1) operations. The dependence on dimension of the former

˜O(cid:0)n2.5(cid:107)C(cid:107)0.5

of these runtimes matches that of the linear programming solver of [LS14  LS15]  which obtain
a polylogarithmic dependence on −1  rather than a polynomial dependence; thus  the question of
obtaining an accelerated −1 dependence without worse dimension dependence remained open.
This was partially settled in [BJKS18  Qua19]  which studied the relationship of optimal trans-
port to fundamental algorithmic problems in theoretical computer science  namely positive linear
programming and matrix scaling  for which signiﬁcantly-improved runtimes have been recently ob-
tained [AO15  ZLdOW17  CMTV17]. In particular  they showed that optimal transport could be
reduced to instances of either of these objectives  for which ˜O ((cid:107)C(cid:107)max/) iterations  each of which
required linear O(n2) work  sufﬁced. However  both of these reductions are based on black-box
methods for which practical implementations are not known; furthermore  in the case of positive
linear programming a parallel ˜O(1/)-iteration algorithm is not known. [BJKS18] also showed any
polynomial improvement to the runtime of our paper in the dependence on either  or n would result
in maximum-cardinality bipartite matching in dense graphs faster than ˜O(n2.5) without fast matrix
multiplication [San09]  a fundamental open problem unresolved for almost 50 years [HK73].

1st-order Parallel

Approach
Interior point

Sink/Greenkhorn
Gradient descent

Acceleration
Matrix scaling
Positive LP

Dual extrapolation

No
Yes
Yes
Yes
No
Yes
Yes

No
Yes
Yes
Yes
Yes
No
Yes

Year
2015

2017-19

2018

2018-19

2018

Author
[LS15]
[AWR17]
[DGK18]
[LHJ19]
[BJKS18]

2018-19

[BJKS18  Qua19]

2019

This work

Complexity

˜O(n2.5)
˜O(n2(cid:107)C(cid:107)2
max/2)
˜O(n2(cid:107)C(cid:107)2
max/2)
˜O(n2.5(cid:107)C(cid:107)max/)
˜O(n2(cid:107)C(cid:107)max/)
˜O(n2(cid:107)C(cid:107)max/)
˜O(n2(cid:107)C(cid:107)max/)

Table 1: Optimal transport algorithms. Algorithms using second-order information use potentially-
expensive SDD system solvers; the runtime analysis of Sink/Greenkhorn is due to [DGK18  LHJ19].

Specializations of the transportation problem to (cid:96)p metric spaces or arising from geometric settings
have been studied [SA12  AS14  ANOY14]. These specialized approaches seem fundamentally
different than those concerning the more general transportation problem.
Finally  we note recent work [ABRW18] showed the promise of using the Nystr¨om method for low-
rank approximations to achieve speedup in theory and practice for transport problems arising from
speciﬁc metrics. As our method is based on matrix-vector operations  where low-rank approxima-
tions may be applicable  we ﬁnd it interesting to see if our method can be combined with these
improvements.
Remark. During the revision process for this work  an independent result [LMR19] was published
to arXiv  obtaining improved runtimes for optimal transport via a combinatorial algorithm. The
work obtains a runtime of ˜O(n2(cid:107)C(cid:107)max/ + n(cid:107)C(cid:107)2
max/2)  which is worse than our runtime by a
low-order term. Furthermore  it does not appear to be parallelizable.
Box-simplex objectives. Our main result follows from improved algorithms for bilinear minimax
problems over one simplex domain and one box domain developed in [She17]. This fundamental
minimax problem captures (cid:96)1 and (cid:96)∞ regression over a simplex and box respectively  and inspired
the development of conjugate smoothing [Nes05] as well as mirror prox / dual extrapolation [Nem04 
Nes07]. These latter two approaches are extragradient methods (using two gradient operations per
iteration rather than one) for approximately solving a family of problems  which includes convex
minimization and ﬁnding a saddle point to a convex-concave function. These methods simulate
backwards Euler discretization of the gradient ﬂow  similar to how mirror descent simulates forwards

3

Euler discretization [DO19]. The role of the extragradient step is a ﬁxed point iteration (of two steps)
which is a good approximation of the backwards Euler step when the operator is Lipschitz.
Nonetheless  the analysis of [Nem04  Nes07] fell short in obtaining a 1/T rate of convergence
without worse dependence on dimension for these domains  where T is the iteration count (which
would correspond to a ˜O (1/) runtime for approximate minimization). The fundamental barrier was
that over a box  any strongly-convex regularizer in the (cid:96)∞ norm has a dimension-dependent domain
size (shown in [ST18]). This barrier can also be viewed as the reason for the worse dimension
dependence in the accelerated scheme of [DGK18  LHJ19].
The primary insight of [She17] was that previous approaches attempted to regularize the schemes of
[Nem04  Nes07] with separable regularizers  i.e. the sum of a regularizer which depends only on the
primal block and one which depends only on the dual. If  say  the domain of the primal block was a
box  then such a regularization scheme would run into the (cid:96)∞ barrier and incur a worse dependence
on dimension. However  by more carefully analyzing the requirements of these algorithms  [She17]
constructed a non-separable regularizer with small domain size  satisfying a property termed area-
convexity which sufﬁced for provable convergence of dual extrapolation [Nes07]. Interestingly  the
property seems specialized to dual extrapolation and not mirror prox [Nem04].

2 Overview

First  in Section 2.1 we ﬁrst describe a reformulation of (1) as a primal-dual objective  which we
solve approximately in Section 3. Then in Section 2.2 we give additional notation critical for our
analysis4. In Section 3 we leverage this to give an overview of our main algorithm.

2.1

(cid:96)1-regression formulation

We adapt the view of [BJKS18  Qua19] of the objective (1) as a positive linear program. Let d be the
(vectorized) cost matrix C associated with the instance and let ∆n2 be the n2 dimensional simplex5.
We recall r  c are speciﬁed row and column sums with 1(cid:62)r = 1(cid:62)c = 1. The optimal transport
problem can be written as  for m = n2  and A ∈ {0  1}2n×m  b ∈ R2n≥0  for A the (unsigned)
edge-incidence matrix of the underlying bipartite graph and b the concatenation of r and c.

min

x∈∆m Ax=b

d(cid:62)x.

1 1
0 0
0 0
0 0
1 0
0 1

0
1
0
1
0
0

0
1
0
0
1
0

0
1
0
0
0
1

0
0
1
1
0
0

0
0
1
0
1
0

   b =

0
0
1
0
0
1



1
0
0
1
0
0

A =



 .

1/3
1/3
1/3
1/3
1/3
1/3

(2)

Figure 1: Edge-incidence matrix A of a 3 × 3 bipartite graph and uniform demands.

In particular  A is the 0-1 matrix on V × E such that Ave = 1 iff v is an endpoint of edge e. We
summarize some additional properties of the constraint matrix A and vector b.
Fact 2.1. A  b have the following properties.

1. A ∈ {0  1}2n×m has 2-sparse columns and n-sparse rows. Thus (cid:107)A(cid:107)1→1 = 2.

2. b(cid:62) =(cid:0)r(cid:62) c(cid:62)(cid:1)  so that (cid:107)b(cid:107)1 = 2.

3. A has 2n2 nonzero entries.

4Because many of the objects deﬁned in Section 2.2 are developed in Section 2.1  we postpone their state-

ment  but refer the reader to Section 2.2 for any ambiguous deﬁnitions.

5We use d because C often arises from distances in a metric space  and to avoid overloading c.

4

Section 4 recalls the proof of the following theorem  which ﬁrst appeared in [AWR17].
Theorem 2.2 (Rounding guarantee  Lemma 7 in [AWR17]). There is an algorithm which takes ˜x
with (cid:107)A˜x − b(cid:107)1 ≤ δ and produces ˆx in O(n2) time  with

Aˆx = b (cid:107)˜x − ˆx(cid:107)1 ≤ 2δ.

We now show how the rounding procedure gives a roadmap for our approach. Consider the following
(cid:96)1 regression objective over the simplex (a similar penalized objective appeared in [She13]):

d(cid:62)x + 2(cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 .

min
x∈∆m

(3)

We show that the penalized objective value is still OPT  and furthermore any approximate minimizer
yields an approximate transport plan.
Lemma 2.3 (Penalized (cid:96)1 regression). The value of (3) is OPT. Also  given ˜x  an -approximate
minimizer to (3)  we can ﬁnd -approximate transportation plan ˆx in O(n2) time.

Proof. Recall OPT = minx∈∆m Ax=b d(cid:62)x. Let ˜x be the minimizing argument in (3). We claim
there is some optimal ˜x with A˜x = b; clearly  the ﬁrst claim is then true. Suppose otherwise  and let
(cid:107)A˜x − b(cid:107)1 = δ > 0. Then  let ˆx be the result of the algorithm in Theorem 2.2  applied to ˜x  so that
Aˆx = b (cid:107)˜x − ˆx(cid:107)1 ≤ 2δ. We then have
d(cid:62) ˆx + 2(cid:107)d(cid:107)∞ (cid:107)Aˆx − b(cid:107)1 = d(cid:62)(ˆx − ˜x) + d(cid:62) ˜x ≤ d(cid:62) ˜x + (cid:107)d(cid:107)∞ (cid:107)ˆx − ˜x(cid:107)1 ≤ d(cid:62) ˜x + 2(cid:107)d(cid:107)∞ δ.
The objective value of ˆx is no more than of ˜x  a contradiction. By this discussion  we can take any
approximate minimizer to (3) and round it to a transport plan without increasing the objective.

Section 3 proves Theorem 2.4  which says we can efﬁciently ﬁnd an approximate minimizer to (3).
Theorem 2.4 (Approximate (cid:96)1 regression over the simplex). There is an algorithm (Algorithm 1)
taking input   which has O(((cid:107)d(cid:107)∞ log n log γ)/) parallel depth for γ = log n·(cid:107)d(cid:107)∞ /  and total
work O(n2((cid:107)d(cid:107)∞ log n log γ)/)  and obtains ˜x an -additive approximation to the objective in (3).
We will approach proving Theorem 2.4 through a primal-dual viewpoint  in light of the following
(based on the deﬁnition of the (cid:96)1 norm):

(cid:0)y(cid:62)Ax − b(cid:62)y(cid:1) .

(4)

min
x∈∆m

d(cid:62)x + 2(cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 = min
x∈∆m

max

y∈[−1 1]2n

d(cid:62)x + 2(cid:107)d(cid:107)∞

Further  a low-duality gap pair to (4) yields an approximate minimizer to (3).
Lemma 2.5 (Duality gap to error). Suppose x  y is feasible (x ∈ ∆m  y ∈ [−1  1]2n)  and for any

feasible u  v (cid:0)d(cid:62)x + 2(cid:107)d(cid:107)∞

(cid:0)v(cid:62)Ax − b(cid:62)v(cid:1)(cid:1) −(cid:0)d(cid:62)u + 2(cid:107)d(cid:107)∞

(cid:0)y(cid:62)Au − b(cid:62)y(cid:1)(cid:1) ≤ δ.

Then  we have d(cid:62)x + 2(cid:107)d(cid:107)∞ (cid:107)Ax − b(cid:107)1 ≤ δ + OPT.

Proof. The result follows from maximizing over v  and noting that for the minimizing u 

(cid:0)y(cid:62)Au − b(cid:62)y(cid:1) ≤ d(cid:62)u + 2(cid:107)d(cid:107)∞ (cid:107)Au − b(cid:107)1 = OPT.

d(cid:62)u + 2(cid:107)d(cid:107)∞

Correspondingly  Section 3 gives an algorithm which obtains (x  y) with bounded duality gap within
the runtime of Theorem 2.4.

5

2.2 Notation
R≥0 is the nonnegative reals. 1 is the all-ones vector of appropriate dimension when clear. The
probability simplex is ∆d def= {v | v ∈ Rd≥0  1(cid:62)v = 1}. We say matrix X is in the simplex of
(cid:107)·(cid:107)1 and (cid:107)·(cid:107)∞ are the (cid:96)1 and (cid:96)∞ norms  i.e. (cid:107)v(cid:107)1 = (cid:80)
appropriate dimensions when its (nonnegative) entries sum to one.
i |vi| and (cid:107)v(cid:107)∞ = maxi |vi|. When A is
a matrix  we let (cid:107)A(cid:107)p→q be the matrix operator norm  i.e. sup(cid:107)v(cid:107)p=1 (cid:107)Av(cid:107)q  where (cid:107)·(cid:107)p is the (cid:96)p
Throughout log is the natural logarithm. For x ∈ ∆d  h(x) =(cid:80)
norm. In particular  (cid:107)A(cid:107)1→1 is the largest (cid:96)1 norm of a column of A.

i∈[d] xi log xi is (negative) entropy
where 0 log 0 = 0 by convention. It is well-known that maxx∈∆d h(x) − minx∈∆d h(x) = log d.
We also use the Bregman divergence of a regularizer and the proximal operator of a divergence.
Deﬁnition 2.6 (Bregman divergence). For (differentiable) regularizer r and z  w in its domain  the
Bregman divergence from z to w is

z (w) def= r(w) − r(z) − (cid:104)∇r(z)  w − z(cid:105).
V r

When r is convex  the divergence is nonnegative and convex in the argument (w in the deﬁnition).
Deﬁnition 2.7 (Proximal operator). For (differentiable) regularizer r  z in its domain  and g in the
dual space (when the domain is in Rd  so is the dual space)  we deﬁne the proximal operator as

Proxr

z(g) def= argminw {(cid:104)g  w(cid:105) + V r

z (w)} .

Several variables have specialized meaning throughout. All graphs considered will be on 2n vertices
with m edges  i.e. m = n2. A ∈ R2n×m is the edge-incidence matrix. d is the vectorized cost
matrix C. b is the constraint vector  concatenating row and column constraints r  c. In algorithms
for solving (4)  x and y are primal (in a simplex) and dual (in a box) variables respectively. In
Section 3  we adopt the linear programming perspective where the decision variable x ∈ ∆m is
a vector. In Section 4  for convenience we take the perspective where X is an unﬂattened n × n
matrix. Ur c is the feasible polytope: when the domain is vectors  Ur c is x | Ax = b  and when it is
matrices  Ur c is X | X1 = r  X(cid:62)1 = c (by ﬂattening X this is consistent).

3 Main Algorithm

This section describes our algorithm for ﬁnding a primal-dual pair (x  y) with a small duality gap 
with respect to the objective in (4)  which we restate here for convenience:

(cid:0)y(cid:62)Ax − b(cid:62)y(cid:1)   X def= ∆m  Y def= [−1  1]2n.

y∈Y d(cid:62)x + 2(cid:107)d(cid:107)∞

min
x∈X max

(Restatement of (4))

Our algorithm is a specialization of the algorithm in [She17]. One of our technical contributions in
this regard is an analysis of the algorithm which more closely relates it to the analysis of dual extrap-
olation [Nes07]  an algorithm for ﬁnding approximate saddle points with a more standard analysis.
In Section 3.1  we give the algorithmic framework and convergence analysis. In Section B.1  we
provide analysis of an alternating minimization scheme for implementing steps of the procedure.
The same procedure was used in [She17] which claimed without proof the linear convergence rate
of the alternating minimization; we hope the analysis will make the method more broadly accessible
to the optimization community. We defer many proofs to Appendix B.

3.1 Dual Extrapolation Framework

For an objective F (x  y) convex in x and concave in y  the standard way to measure the duality gap is
to deﬁne the gradient operator g(x  y) = (∇xF (x  y) −∇yF (x  y))  and show that for z = (x  y)
and any u on the product space  the regret  (cid:104)g(z)  z − u(cid:105)  is small. Correspondingly  we deﬁne

g(x  y) def=(cid:0)d + 2(cid:107)d(cid:107)∞ A(cid:62)y  2(cid:107)d(cid:107)∞ (b − Ax)(cid:1) .

The dual extrapolation framework [Nes07] requires a regularizer on the product space. The algo-
rithm is simple to state; it takes two “mirror descent-like” steps each iteration  maintaining a state

6

st in the dual space6. A typical setup is a Lipschitz gradient operator and a regularizer which is
the sum of canonical strongly-convex regularizers in the norms corresponding to the product space
X  Y. However  recent works have shown that this setup can be greatly relaxed and still obtain
similar rates of convergence. In particular  [She17] introduced the following deﬁnition.
Deﬁnition 3.1 (Area-convexity). Regularizer r is κ-area-convex with respect to operator g if for
any points a  b  c in its domain 

r(a) + r(b) + r(c) − 3r

κ

≥ (cid:104)g(b) − g(a)  b − c(cid:105).

(5)

(cid:18) a + b + c

(cid:19)(cid:19)

3

(cid:18)

Area-convexity is so named because (cid:104)g(b)−g(a)  b−c(cid:105) can be viewed as measuring the “area” of the
triangle with vertices a  b  c with respect to some Jacobian matrix. In the case of bilinear objectives 
the left hand side in the deﬁnition of area-convexity is invariant to permuting a  b  c  whereas the
sign of the right hand side can be ﬂipped by interchanging a  c  so area-convexity implies convexity.
However  it does not even imply the regularizer r is strongly-convex  a typical assumption for the
convergence of mirror descent methods.
We state the algorithm for time horizon T ; the only difference from [Nes07] is a factor of 2 in
deﬁning st+1  i.e. adding a 1/2κ multiple rather than 1/κ. We ﬁnd it of interest to explore whether
this change is necessary or speciﬁc to the analysis of [She17].

Algorithm 1 ¯w = Dual-Extrapolation(κ  r  g  T ): Dual extrapolation with area-convex r.

Initialize s0 = 0  let ¯z be the minimizer of r.
for t < T do
zt ← Proxr
¯z(st).
wt ← Proxr
¯z
st+1 ← st + 1
t ← t + 1.

κ g(zt)(cid:1).

2κ g(wt).

(cid:0)st + 1
(cid:80)

end for
return ¯w def= 1
T

t∈[T ] wt.

Lemma 3.2 (Dual extrapolation convergence). Suppose r is κ-area-convex with respect to g. Fur-
ther  suppose for some u  Θ ≥ r(u) − r(¯z). Then  the output ¯w to Algorithm 1 satisﬁes

(cid:104)g( ¯w)  ¯w − u(cid:105) ≤ 2κΘ
T

.

In fact  by more carefully analyzing the requirements of dual extrapolation we have the following.
Corollary 3.3. Suppose in Algorithm 1  the proximal steps are implemented with (cid:48)/4κ additive
error. Then  the upper bound of the regret in Lemma 3.2 is 2κΘ/T + (cid:48).

We now state a useful second-order characterization of area-convexity involving a relationship be-
tween the Jacobian of g and the Hessian of r  which was proved in [She17].
Theorem 3.4 (Second-order area-convexity  Theorem 1.6 in [She17]). For bilinear minimax objec-
tives  i.e. whose associated operator g has Jacobian

−M 0
and for twice-differentiable r  if for all z in the domain 
−J

(cid:18)κ∇2r(z)

J =

then r is 3κ-area-convex with respect to g.

J

(cid:18) 0 M(cid:62)

κ∇2r(z)

(cid:19)
(cid:19)

 

(cid:23) 0 

6In this regard  it is more similar to the “dual averaging” or “lazy” mirror descent setup [Bub15].

7

Finally  we complete the outline of the algorithm by stating the speciﬁc regularizer we use  which
ﬁrst appeared in [She17]. We then prove its 3-area-convexity with respect to g by using Theorem 3.4.

r(x  y) = 2(cid:107)d(cid:107)∞

xj log xj + x(cid:62)A(cid:62)(y2)

(6)

10

(cid:88)

j∈[n]

  

2 (cid:107)y(cid:107)2

where (y2) is entry-wise. To give some motivation for this regularizer  one (cid:96)∞-strongly convex
2  but over the (cid:96)∞ ball  this regularizer has large range. The term x(cid:62)A(cid:62)(y2) in
regularizer is 1
(6) captures the curvature required for strong-convexity locally  but has a smaller range due to the
restrictions on x  A. The constants chosen were the smallest which satisfy the assumptions of the
following Lemma 3.5.
Lemma 3.5 (Area-convexity of the Sherman regularizer). For the Jacobian J associated with the
objective in (4) and the regularizer r deﬁned in (6)  we have

(cid:18)∇2r(z)

J

(cid:19)

−J
∇2r(z)

(cid:23) 0.

We now give the proof of Theorem 2.4  requiring some claims in Appendix B.1 for the complexity
of Algorithm 1. In particular  Appendix B.1 implies that although the minimizer to the proximal
steps cannot be computed in closed form because of non-separability  a simple alternating scheme
converges to an approximate-minimizer in near-constant time.

mer summand comes from the range of entropy and the latter (cid:13)(cid:13)A(cid:62)(cid:13)(cid:13)∞ = 2. We may choose

Proof of Theorem 2.4. The algorithm is Algorithm 1  using the regularizer r in (6). Clearly  in
the feasible region the range of the regularizer is at most 20(cid:107)d(cid:107)∞ log n + 4(cid:107)d(cid:107)∞  where the for-
Θ = O((cid:107)d(cid:107)∞ log n) to be the range of r to satisfy the assumptions of Lemma 3.2  since for all u 
(cid:104)∇r(¯z)  ¯z − u(cid:105) ≤ 0 ⇒ V r
By Theorem 3.4 and Lemma 3.5  r is 3-area-convex with respect to g. By Corollary 3.3  T =
12Θ/ iterations sufﬁce  implementing each proximal step to /12-additive accuracy. Finally  using
Theorem B.5 to bound this implementation runtime concludes the proof.
4 Rounding to Ur c

¯z (u) ≤ r(u) − r(¯z).

We state the rounding procedure in [AWR17] for completeness here  which takes a transport plan ˜X
close to Ur c and transforms it into a plan which exactly meets the constraints and is close to ˜X in
(cid:96)1  and then prove its correctness in Appendix C. Throughout r(X) def= X1  c(X) def= X(cid:62)1.

Algorithm 2 ˆX = Rounding( ˜X  r  c): Rounding to feasible polytope

(cid:16)

(cid:16) r
(cid:16) c

r( ˜X)

  1

(cid:17)(cid:17) ˜X.
(cid:17)(cid:17)

(cid:16)

min

X(cid:48) ← diag
X(cid:48)(cid:48) ← X(cid:48)diag
er ← r − 1(cid:62)r(X(cid:48)(cid:48))  ec ← c − 1(cid:62)c(X(cid:48)(cid:48))  E ← 1(cid:62)er.
ˆX ← X(cid:48)(cid:48) + 1
return ˆX.

E ere(cid:62)
c .

c(X(cid:48))   1

min

.

5 Experiments

We show experiments illustrating the potential of our algorithm to be useful in practice  by consider-
ing its performance on computing optimal transport distances on the MNIST dataset and comparing
against algorithms in the literature including APDAMD [LHJ19] and Sinkhorn iteration. All com-
parisons are based on the number of matrix-vector multiplications (rather than iterations  due to our
algorithm’s alternating subroutine)  the main computational component of all algorithms considered.

8

(a) Comparison with Sinkhorn iteration with different
parameters.

(b) Comparison with APDAMD [LHJ19] with differ-
ent parameters.

(a) Comparison with Sinkhorn iteration on 20 ran-
domly chosen MNIST digit pairs.

(b) Comparison with APDAMD [LHJ19] on 20 ran-
domly chosen MNIST digit pairs.

While our unoptimized algorithm performs poorly  slightly optimizing the size of the regularizer and
step sizes used results in an algorithm with competitive performance to APDAMD  the ﬁrst-order
method with the best provable guarantees and observed practical performance. Sinkhorn iteration
outperformed all ﬁrst-order methods experimentally; however  an optimized version of our algorithm
performed better than conservatively-regularized Sinkhorn iteration  and was more competitive with
variants of Sinkhorn found in practice than other ﬁrst-order methods.
As we discuss in our implementation details (Appendix D)  we acknowledge that implementations
of our algorithm illustrated are not the same as those with provable guarantees in our paper. How-
ever  we believe that our modiﬁcations are justiﬁable in theory  and consistent with those made in
practice to existing algorithms. Further  we hope that studying the modiﬁcations we made (step
size  using mirror prox [Nem04] for stability considerations)  as well as the consideration of other
numerical speedups such as greedy updates [AWR17] or kernel approximations [ABRW18]  will
become fruitful for understanding the potential of accelerated ﬁrst-order methods in both the theory
and practice of computational optimal transport.

9

Acknowledgements

We thank Jose Blanchet and Carson Kent
ported by NSF Graduate Fellowship DGE-114747.
REER Award CCF-1844855.
1656518.

for helpful conversations.

sup-
AS was supported by NSF CA-
KT was supported by NSF Graduate Fellowship DGE-

AJ was

References
[ABRW18] Jason Altschuler  Francis Bach  Alessandro Rudi  and Jonathan Weed. Approximating
the quadratic transportation metric in near-linear time. CoRR  abs/1810.10046  2018.
1.2  5  D

[ACB17] Mart´ın Arjovsky  Soumith Chintala  and L´eon Bottou. Wasserstein generative adver-
In Proceedings of the 34th International Conference on Machine
sarial networks.
Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages 214–223 
2017. 1

[ANOY14] Alexandr Andoni  Aleksandar Nikolov  Krzysztof Onak  and Grigory Yaroslavtsev.
Parallel algorithms for geometric graph problems. In Symposium on Theory of Com-
puting  STOC 2014  New York  NY  USA  May 31 - June 03  2014  pages 574–583 
2014. 1.2

[AO15] Zeyuan Allen Zhu and Lorenzo Orecchia. Nearly-linear time positive LP solver with
faster convergence rate. In Proceedings of the Forty-Seventh Annual ACM on Sym-
posium on Theory of Computing  STOC 2015  Portland  OR  USA  June 14-17  2015 
pages 229–236  2015. 1.2

[AS14] Pankaj K. Agarwal and R. Sharathkumar. Approximation algorithms for bipartite
matching with metric and geometric costs. In Symposium on Theory of Computing 
STOC 2014  New York  NY  USA  May 31 - June 03  2014  pages 555–564  2014. 1.2

[AWR17] Jason Altschuler  Jonathan Weed  and Philippe Rigollet. Near-linear time approxima-
tion algorithms for optimal transport via sinkhorn iteration. In Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Pro-
cessing Systems 2017  4-9 December 2017  Long Beach  CA  USA  pages 1961–1971 
2017. 1  1.2  ??  2.1  2.2  4  5  2.2  D

[BJKS18] Jose Blanchet  Arun Jambulapati  Carson Kent  and Aaron Sidford. Towards optimal
running times for optimal transport. CoRR  abs/1810.07717  2018. (document)  1 
1.1  1.2  ??  ??  2.1

[BK17] Jose H. Blanchet and Yang Kang. Distributionally robust groupwise regularization
estimator. In Proceedings of The 9th Asian Conference on Machine Learning  ACML
2017  Seoul  Korea  November 15-17  2017.  pages 97–112  2017. 1

[Bub15] S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations

and Trends in Machine Learning  8(3-4):231–357  2015. 6

[BvdPPH11] Nicolas Bonneel  Michiel van de Panne  Sylvain Paris  and Wolfgang Heidrich.
Displacement interpolation using lagrangian mass transport. ACM Trans. Graph. 
30(6):158:1–158:12  2011. 1

[CK18] Deeparnab Chakrabarty and Sanjeev Khanna. Better and simpler error analysis of
the sinkhorn-knopp algorithm for matrix scaling. In 1st Symposium on Simplicity in
Algorithms  SOSA 2018  January 7-10  2018  New Orleans  LA  USA  pages 4:1–4:11 
2018. 1

[CMTV17] Michael B. Cohen  Aleksander Madry  Dimitris Tsipras  and Adrian Vladu. Matrix
scaling and balancing via box constrained newton’s method and interior point meth-
ods. In 58th IEEE Annual Symposium on Foundations of Computer Science  FOCS
2017  Berkeley  CA  USA  October 15-17  2017  pages 902–913  2017. 1.2

10

[Cut13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In
Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings of a meeting held Decem-
ber 5-8  2013  Lake Tahoe  Nevada  United States.  pages 2292–2300  2013. 1  1.2

[DGK18] Pavel Dvurechensky  Alexander Gasnikov  and Alexey Kroshnin. Computational opti-
mal transport: Complexity by accelerated gradient descent is better than by sinkhorn’s
algorithm. In Proceedings of the 35th International Conference on Machine Learn-
ing  ICML 2018  Stockholmsm¨assan  Stockholm  Sweden  July 10-15  2018  pages
1366–1375  2018. (document)  1  1.1  1.2  ??  1  1.2

[DO19] Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique:
A uniﬁed theory of ﬁrst-order methods. SIAM Journal on Optimization  29(1):660–
689  2019. 1.2

[EK18] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust
optimization using the wasserstein metric: performance guarantees and tractable re-
formulations. Math. Program.  171(1-2):115–166  2018. 1

[GCPB16] Aude Genevay  Marco Cuturi  Gabriel Peyr´e  and Francis R. Bach. Stochastic opti-
mization for large-scale optimal transport. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural Information Processing Systems
2016  December 5-10  2016  Barcelona  Spain  pages 3432–3440  2016. 1  1.2

[HK73] John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings

in bipartite graphs. SIAM J. Comput.  2(4):225–231  1973. 1.2

[KLOS14] Jonathan A. Kelner  Yin Tat Lee  Lorenzo Orecchia  and Aaron Sidford. An almost-
linear-time algorithm for approximate max ﬂow in undirected graphs  and its multi-
commodity generalizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM
Symposium on Discrete Algorithms  SODA 2014  Portland  Oregon  USA  January
5-7  2014  pages 217–226  2014. 1.2

[LHJ19] Tianyi Lin  Nhat Ho  and Michael I. Jordan. On efﬁcient optimal transport: An anal-
ysis of greedy and accelerated mirror descent algorithms. CoRR  abs/1901.06482 
2019. (document)  1  1.1  1.2  ??  1  1.2  5  2b  3b  D

[LMR19] Nathaniel Lahn  Deepika Mulchandani  and Sharath Raghvendra. A graph theoretic

additive approximation of optimal transport. CoRR  abs/1905.11830  2019. 1.2

[LS14] Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving
linear programs in ˜o(vrank) iterations and faster algorithms for maximum ﬂow.
In
55th IEEE Annual Symposium on Foundations of Computer Science  FOCS 2014 
Philadelphia  PA  USA  October 18-21  2014  pages 424–433  2014. 1.2

[LS15] Yin Tat Lee and Aaron Sidford. Efﬁcient inverse maintenance and faster algorithms
for linear programming. In IEEE 56th Annual Symposium on Foundations of Com-
puter Science  FOCS 2015  Berkeley  CA  USA  17-20 October  2015  pages 230–249 
2015. 1.2  ??

[Nem04] Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational in-
equalities with lipschitz continuous monotone operators and smooth convex-concave
saddle point problems. SIAM Journal on Optimization  15(1):229–251  2004. 1.2  5 
D

[Nes05] Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program. 

103(1):127–152  2005. 1.2  1.2

[Nes07] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequal-
ities and related problems. Math. Program.  109(2-3):319–344  2007. 1.2  3  3.1 
3.1

11

[PZ16] Victor M. Panaretos and Yoav Zemel. Amplitude and phase variation of point pro-

cesses. Annals of Statistics  44(2):771–812  2016. 1

[Qua19] Kent Quanrud. Approximating optimal transport with linear programs. In 2nd Sym-
posium on Simplicity in Algorithms  SOSA@SODA 2019  January 8-9  2019 - San
Diego  CA  USA  pages 6:1–6:9  2019. (document)  1  1.1  1.2  ??  2.1

[SA12] R. Sharathkumar and Pankaj K. Agarwal. A near-linear time -approximation algo-
In Proceedings of the 44th Symposium on
rithm for geometric bipartite matching.
Theory of Computing Conference  STOC 2012  New York  NY  USA  May 19 - 22 
2012  pages 385–394  2012. 1.2

[San09] Piotr Sankowski. Maximum weight bipartite matching in matrix multiplication time.

Theor. Comput. Sci.  410(44):4480–4488  2009. 1.2

[SdGP+15] Justin Solomon  Fernando de Goes  Gabriel Peyr´e  Marco Cuturi  Adrian Butscher 
Andy Nguyen  Tao Du  and Leonidas J. Guibas. Convolutional wasserstein dis-
tances: efﬁcient optimal transportation on geometric domains. ACM Trans. Graph. 
34(4):66:1–66:11  2015. 1

[She13] Jonah Sherman. Nearly maximum ﬂows in nearly linear time. In 54th Annual IEEE
Symposium on Foundations of Computer Science  FOCS 2013  26-29 October  2013 
Berkeley  CA  USA  pages 263–269  2013. 1.2  2.1

[She17] Jonah Sherman. Area-convexity  l∞ regularization  and undirected multicommodity
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
ﬂow.
Computing  STOC 2017  Montreal  QC  Canada  June 19-23  2017  pages 452–460 
2017. (document)  1.1  1.2  3  3.1  3.1  3.1  3.4  3.1

[ST18] Aaron Sidford and Kevin Tian. Coordinate methods for accelerating (cid:96)∞ regression
and faster approximate maximum ﬂow. In 59th Annual IEEE Symposium on Founda-
tions of Computer Science  FOCS 2018  7-9 October  2018  Paris  France  2018. 1.2 
1.2

[You01] Neal E. Young. Sequential and parallel algorithms for mixed packing and covering.
In 42nd Annual Symposium on Foundations of Computer Science  FOCS 2001  14-17
October 2001  Las Vegas  Nevada  USA  pages 538–546  2001. 1.2

[ZLdOW17] Zeyuan Allen Zhu  Yuanzhi Li  Rafael Mendes de Oliveira  and Avi Wigderson. Much
faster algorithms for matrix scaling. In 58th IEEE Annual Symposium on Foundations
of Computer Science  FOCS 2017  Berkeley  CA  USA  October 15-17  2017  pages
890–901  2017. 1.2

12

,Arun Jambulapati
Aaron Sidford
Kevin Tian