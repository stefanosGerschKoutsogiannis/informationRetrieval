2018,Representation Balancing MDPs for Off-policy Policy Evaluation,We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work  we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning  and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective  we develop a learning algorithm of an MDP model with a balanced representation  and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.,Representation Balancing MDPs
for Off-Policy Policy Evaluation

Yao Liu

Stanford University

yaoliu@stanford.edu

Aniruddh Raghu

Cambridge University

aniruddhraghu@gmail.com

Omer Gottesman
Harvard University

gottesman@fas.harvard.edu

Matthieu Komorowski
Imperial College London

matthieu.komorowski@gmail.com

Aldo Faisal

Imperial College London

a.faisal@imperial.ac.uk

Finale Doshi-Velez
Harvard University

finale@seas.harvard.edu

Emma Brunskill
Stanford University

ebrun@cs.stanford.edu

Abstract

We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast
to prior work  we consider how to estimate both the individual policy value and
average policy value accurately. We draw inspiration from recent work in causal
reasoning  and propose a new ﬁnite sample generalization error bound for value
estimates from MDP models. Using this upper bound as an objective  we develop a
learning algorithm of an MDP model with a balanced representation  and show that
our approach can yield substantially lower MSE in common synthetic benchmarks
and a HIV treatment simulation domain.

1

Introduction

In reinforcement learning  off-policy (batch) policy evaluation is the task of estimating the perfor-
mance of some evaluation policy given data gathered under a different behavior policy. Off-policy
policy evaluation (OPPE) is essential when deploying a new policy might be costly or risky  such as
in consumer marketing  healthcare  and education. Technically off-policy evaluation relates to other
ﬁelds that study counterfactual reasoning  including causal reasoning  statistics and economics.
Off-policy batch policy evaluation is challenging because the distribution of the data under the
behavior policy will in general be different than the distribution under the desired evaluation policy.
This difference in distributions comes from two sources. First  at a given state  the behavior policy
may select a different action than the one preferred by the evaluation policy—for example  a clinician
may chose to amputate a limb  whereas we may be interested in what might have happened if the
clinician had not. We never see the counterfactual outcome. Second  the distribution of future
states—not just the immediate outcomes—is also determined by the behavior policy. This challenge
is unique to sequential decision processes and is not covered by most causal reasoning work: for
example  the resulting series of a patient’s health states observed after amputating a patient’s limb is
likely to be signiﬁcantly different than if the limb was not amputated.
Approaches for OPPE must make a choice about whether and how to address this data distribution
mismatch. Importance sampling (IS) based approaches [16  23  8  5  10  22] are typically unbiased
and strongly consistent  but despite recent progress tend to have high variance—especially if the
evaluation policy is deterministic  as evaluating deterministic policies requires ﬁnding in the data

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

sequences where the actions exactly match the evaluation policy. However  in most real-world
applications deterministic evaluation policies are more common—policies are typically to either
amputate or not  rather than a policy that that ﬂips a biased coin (to sample randomness) to decide
whether to amputate. IS approaches also often rely on explicit knowledge of the behavior policy 
which may not be feasible in situations such as medicine where the behaviors results from human
actions. In contrast  some model based approaches ignore the data distribution mismatch  such as by
ﬁtting a maximum-likelihood model of the rewards and dynamics from the behavioral data  and then
using that model to evaluate the desired evaluation policy. These methods may not converge to the
true estimate of the evaluation policy’s value  even in the limit of inﬁnite data [15]. However  such
model based approaches often achieve better empirical performance than the IS-based estimators
[10].
In this work  we address the question of building model-based estimators for OPPE that both do have
theoretical guarantees and yield better empirical performance that model-based approaches that ignore

s0 is an initial state  by evaluating its mean squared error (MSE). Most previous research (e.g. [10  22])

the data distribution mismatch. Typically we evaluate the quality of an OPPE estimatebV ⇡e(s0)  where
evaluates their methods using MSE for the average policy value (APV): [Es0bV ⇡e(s0)Es0V ⇡e(s0)]2 
rather than the MSE for individual policy values (IPV): Es0[bV ⇡e(s0)  V ⇡e(s0)]2. This difference is

crucial for applications such as personalized healthcare since ultimately we may want to assess the
performance of a policy for an speciﬁc individual (patient) state.
Instead  in this paper we develop an upper bound of the MSE for individual policy value estimates.
Note that this bound is automatically an upper bound on the average treatment effect. Our work is
inspired by recent advances[19  11  12] in estimating conditional averaged treatment effects (CATE) 
also known as heterogeneous treatment effects (HTE)  in the contextual bandit setting with a single
(typically binary) action choice. CATE research aims to obtain precise estimates in the difference in
outcomes for giving the treatment vs control intervention for an individual (state).
Recent work [11  19] on CATE1 has obtained very promising results by learning a model to predict
individual outcomes using a (model ﬁtting) loss function that explicitly accounts for the data distribu-
tion shift between the treatment and control policies. We build on this work to introduce a new bound
on the MSE for individual policy values  and a new loss function for ﬁtting a model-based OPPE
estimator. In contrast to most other OPPE theoretical analyses (e.g. [10  5  22])  we provide a ﬁnite
sample generalization error instead of asymptotic consistency. In contrast to previous model value
generalization bounds such as the Simulation Lemma [13]  our bound accounts for the underlying
data distribution shift if the data used to estimate the value of an evaluation policy were collected by
following an alternate policy.
We use this to derive a loss function that we can use to ﬁt a model for OPPE for deterministic
evaluation policies. Conceptually  this process gives us a model that prioritizes ﬁtting the trajectories
in the batch data that match the evaluation policy. Our current estimation procedure works for
deterministic evaluation policies which covers a wide range of scenarios in real-world applications
that are particularly hard for previous methods. Like recently proposed IS-based estimators [22  10  7] 
and unlike the MLE model-based estimator that ignores the distribution shift [15]  we prove that
our model-based estimator is asymptotically consistent  as long as the true MDP model is realizable
within our chosen model class; we use neural models to give our model class high expressivity.
We demonstrate that our resulting models can yield substantially lower mean squared error estimators
than prior model-based and IS-based estimators on a classic benchmark RL task (even when the
IS-based estimators are given access to the true behavior policy). We also demonstrate our approach
can yield improved results on a HIV treatment simulator [6].

2 Related Work

Most prior work on OPPE in reinforcement learning falls into one of three approaches. The ﬁrst 
importance sampling (IS)  reweights the trajectories to account for the data distribution shift. Under
mild assumptions importance sampling estimators are guaranteed to be both unbiased and strongly
consistent  and were ﬁrst introduced to reinforcement learning OPPE by Precup et al. [16]. Despite

1Shalit et al. [19] use the term individual treatment effect (ITE) to refer to a criterion which is actually deﬁned
as CATE in most causal inference literature. We discuss the confusion about the two terms in the appendix B.

2

recent progress (e.g.[23  8]) IS-only estimators still often yield very high variance estimates  par-
ticularly when the decision horizon is large  and/or when the evaluation policy is deterministic. IS
estimators also typically result in extremely noisy estimates for policy values of individual states.
A second common approach is to estimate a dynamics and reward model  which can substantially
reduce variance  but can be biased and inconsistent (as noted by [15]). The third approach  doubly
robust estimators  originates from the statistics community [17]. Recently proposed doubly robust
estimators for OPPE from the machine and reinforcement learning communities [5  10  22] have
sometimes yielded orders of magnitude tighter estimates. However  most prior work that leverages
an approximate model has largely ignored the choice of how to select and ﬁt the model parameters.
Recently  Farajtabar et al. [7] introduced more robust doubly robust (MRDR)  which involves ﬁtting
a Q function for the model-value function part of the doubly robust estimator based on ﬁtting a
weighted return to minimize the variance of doubly robust. In contrast  our work learns a dynamics
and reward model using a novel loss function  to estimate a model that yields accurate individual
policy value estimates. While our method can be combined in doubly robust estimators  we will also
see in our experimental results that directly estimating the performance of the model estimator can
yield substantially beneﬁts over estimating a Q function for use in doubly robust.
OPPE in contextual bandits and RL also has strong similarities with the treatment effect estimation
problem common in causal inference and statistics. Recently  different kinds of machine learning
models such as Gaussian Processes [1]  random forests [24]  and GANs [25] have been used to
estimate heterogeneous treatment effects (HTE)  in non-sequential settings. Schulam and Saria [18]
study using Gaussian process models for treatment effect estimation in continuous time settings. Their
setting differs from MDPs by not having sequential states. Most theoretical analysis of treatment
effects focuses on asymptotic consistency rather than generalization error.
Our work is inspired by recent research that learns complicated outcome models (reward models in
RL) to estimate HTE using new loss functions to account for covariate shift [11  19  2  12]. In contrast
to this prior work we consider the sequential state-action setting. In particular  Shalit et al. [19]
provided an algorithm with a more general model class  and a corresponding generalization bound.
We extend this idea from the binary treatment setting to sequential and multiple action settings.

3 Preliminaries: Notation and Setting

We consider undiscounted ﬁnite horizon MDPs  with ﬁnite horizon H < 1  bounded state space
S⇢ Rd  and ﬁnite action space A. Let p0(s) be the initial state distribution  and T (s0|s  a) be the
transition probability. Given a state action pair  the expectation of reward r is E[r|x  a] = ¯r(x  a).
Given n trajectories collected from a stochastic behavior policy µ  our goal is to evaluate the policy
value of ⇡(s). We assume the policy ⇡(s) is deterministic. We will learn a model of both reward and

transition dynamics cM = hbr(s  a) bT (s0  s  a)i  based on a learned representation. The representation
function  : S 7! Z is a reversible and twice-differentiable function  where Z is the representation
space. is the reverse representation such that ((s)) = s. The speciﬁc form of our MDP model is:
cM = hbr(s  a) bT (s0  s  a)i = hhr((s)  a)  hT ((s0)  (s)  a)i  where hr and hT is some function
over space Z. We will use the notationcM instead ofcM later for simplicity.
Let ⌧ = (s0  a0  . . .   sH) be a trajectory of H + 1 states and actions  sampled from the
joint distribution of MDP M and a policy µ. The joint distributions of ⌧ are: pM µ(⌧ ) =
p0(s0)QH1
t=0 [T (st+1|st  at)µ(at|st)]. Given the joint distribution  we denote the associated
marginal and conditional distributions as pM µ(s0)  pM µ(s0  a0)  pM µ(s0|a0) etc. We also have the
joint  marginal and conditional  distributions p
M µ(·) based on the representation space Z. We focus
on the undiscounted ﬁnite horizon case  using V ⇡
M t(s) to denote the t-step value function of policy ⇡.

4 Generalization Error Bound for MDP based OPPE estimator

Our goal is to learn a MDP model cM that directly minimizes a good upper bound of the MSE for

the individual evaluation policy ⇡ values: Es0[V ⇡
cM
function estimates of the policy ⇡ and be used as part of doubly robust methods.

(s0)  V ⇡

M (s0)]2. This model can provide value

3

In the on-policy case  the Simulation Lemma ( [13] and repeated for completeness in Lemma 1)
shows that MSE of a policy value estimate can be upper bounded by a function of the reward and
transition prediction losses. Before we state this result  we ﬁrst deﬁne some useful notation.
Deﬁnition 1. The square error loss function of value function  reward  transition are:

¯`V (s cM   H  t) =⇣V ⇡
cM  Ht

(s)  V ⇡

M Ht(s)⌘2

¯`T (st  at cM ) =✓ZS⇣bT (s0|st  at)  T (s0|st  at)⌘ V ⇡
Es0hV ⇡
cM

M (s0)i2

(s0)  V ⇡

H1Xt=0

 2H

Then the Simulation lemma ensures that

¯`r(st  at cM ) = (br(st  at)  ¯r(st  at))2

(s0)ds0◆2

cM  Ht1

Est at⇠pM ⇡h¯lr(st  at  ˆM ) + ¯lT (st  at  ˆM )i  

(1)

(2)

The right hand side can be used to formulate an objective to ﬁt a model for policy evaluation. In
off-policy case our data is from a different policy µ  and one can get unbiased estimation of the RHS
of Equation 2 by importance sampling. However  this will provide an objective function with high
variance  especially for a long horizon MDP or a deterministic evaluation policy due to the product of
IS weights. An alternative is to learn an MDP model by directly optimizing the prediction loss over
our observational data  ignoring the covariate shift. From the Simulation Lemma this minimizes an
upper bound of MSE of behavior policy value  but the resulting model may not be a good one for
estimating the evaluation policy value. In this paper we propose a new upper bound on the MSE of
the individual evaluation policy values inspired by recent work in treatment effect estimation  and use
this as a loss function for ﬁtting models.
Before proceeding we ﬁrst state our assumptions  which are common in most OPPE algorithms:

1. Support of behavior policy covers the evaluation policy: for any state s and action a 

2. Strong ignorability: there are no hidden confounders that inﬂuence the choice of actions

µ(a|s) = 0 only if ⇡(a|s) = 0.
other than the current observed state.

Denote a factual sequence to be a trajectory that matches the evaluation policy  a0 =
⇡(s0)  . . .   at1 = ⇡(st1) as a0:t1 = ⇡. Let a counterfactual action sequence a0:t1 6= ⇡
be an action sequence with at least one action that does not match ⇡(s). pM µ(·) is the distribution
over trajectories under M and policy µ. We deﬁne the H  t step value error with respect to the state
distribution given the factual action sequence.
¯`V (st  H  t)pM µ(st|a0:t1 = ⇡)dst
We use the idea of bounding the distance between representations given factual and counterfactual
action sequences to adjust the distribution mismatch. Here the distance between representation
distributions is formalized by Integral Probability Metric (IPM).
Deﬁnition 3. Let p  q be two distributions and let G be a family of real-valued functions deﬁned over

Deﬁnition 2. H  t step value error is: ✏V (cM   H  t) =RS

the same space. The integral probability metric is: IPMG(p  q) = supg2GR g(x)(p(x)  q(x))dx

Some important instances of IPM include the Wasserstein metric where G is 1-Lipschitz continuous
function class  and Maximum Mean Discrepancy where G is norm-1 function class in RKHS.
Let p F
M µ(zt|at 6= ⇡  a0:t1 = ⇡)  where F and
CF denote factual and counterfactual. We ﬁrst give an upper bound of MSE in terms of an expected
loss term and then develop a ﬁnite sample bound which can be used as a learning objective.
Theorem 1. For any MDP M  approximate MDP model cM  behavior policy µ and deterministic
Es0⇥V ⇡

evaluation policy ⇡  let B t and Gt be a real number and function family that satisfy the condition
in Lemma 4. Then:

M (s0)⇤2  2H
pM µ(a0:t = ⇡)⇣¯`r(st ⇡ (st) cM ) + ¯`T (st ⇡ (st) cM )⌘ pM µ(st  a0:t = ⇡)dst

H1Xt=0 hB tIPMGt⇣p F

cM (s0)  V ⇡
+ZS

M µ(zt|a0:t = ⇡) and p CF

M µ (zt)⌘

M µ (zt) = p

M µ(zt) = p

M µ(zt)  p CF

(3)

1

4

(Proof Sketch) The key idea is to use Equation 20 in Lemma 1 to view each step as a contextual

bandit problem  and bound ✏V (cM   H) recursively. We decompose the value function error into a one

step reward loss  a transition loss and a next step value loss  with respect to the on-policy distribution.
We can treat this as a contextual bandit problem  and we build on the method in Shalit et al.’s work
[19] about binary action bandits to bound the distribution mismatch by a representation distance
penalty term; however  additional care is required due to the sequential setting since the next states
are also inﬂuenced by the policy. By adjusting the distribution for the next step value loss  we reduce

it into ✏V (cM   H  t  1)  allowing us recursively repeat this process for H steps.

This theorem bounds the MSE for the individual evaluation policy value by a loss on the distribution
of the behavior policy  with the cost of an additional representation distribution metric. The ﬁrst IPM
term measures how different the state representations are conditional on factual and counterfactual
action history. Intuitively  a balanced representation can generalize better from the observational
data distribution to the data distribution under the evaluation policy  but we also need to consider the
prediction ability of the representation on the observational data distribution. This bound quantitatively
describes those two effects about MSE by the IPM term and the loss terms. The re-weighted expected
loss terms over the observational data distribution is weighted by the marginal action probabilities
ratio instead of the conditional action probability ratio  which is used in importance sampling. The
marginal probabilities ratio has lower variance than the importance sampling weights (See Appendix
C.3).
One natural approach might be to use the right hand side of Equation 3 as a loss  and try to directly
optimize a representation and model that minimizes this upper bound on the mean squared error in
the individual value estimates. Unfortunately  doing so can suffer from two important issues. (1) The
subset of the data that matches the evaluation policy can be very sparse for large t  and though the
above bound re-weights data  ﬁtting a model to it can be challenging due to the limited data size. (2)
Unfortunately this approach ignores all the other data present that do not match the evaluation policy.
If we are also learning a representation of the domain in order to scale up to very large problems  we
suspect that we may beneﬁt from framing the problem as related to transfer or multitask learning.
Motivated by viewing off-policy policy evaluation as a transfer learning task  we can view the source
task as the evaluating the behavior policy  for which we have on-policy data  and view the target
task as evaluating the evaluation policy  for which we have the high-variance re-weighted data from
importance sampling. This is similar to transfer learning where we only have a few  potentially noisy 
data points for the target task. Thus we can take the idea of co-learning a source task and a target task
at the same time as a sort of regularization given limited data. More precisely  we now bound the
OPPE error by an upper bound of the sum of two terms:

Es0⇥V ⇡
|

cM (s0)  V ⇡

MSE⇡

{z

M (s0)⇤2
}

+ Es0hV µ
cM
|

(s0)  V µ
{z

MSEµ

M (s0)i2
}

 

(4)

where we bound the former part using Theorem 1. Thus our upper bound of this objective can
address the issues with separately using MSE⇡ and MSEµ as objective: compared with IS estimation
of MSE⇡  the "marginal" action probability ratio has lower variance. The representation distribution
distance term regularizes the representation layer such that the learned representation would not vary
signiﬁcantly between the state distribution under the evaluation policy and the state distribution under
the behavior policy. That reduces the concern that using MSEµ as an objective will force our model
to evaluate the behavior policy  rather than the evaluation policy  more effectively.
Our work is also inspired by treatment effect estimation in the casual inference literature  where
we estimate the difference between the treated and control groups. An analogue in RL would be
estimating the difference between the target policy value and the behavior policy value  by minimizing
the MSE of policy difference estimation. The objective above is an upper bound of the MSE of policy
difference estimator: 1

 MSE⇡ + MSEµ

We now bound Equation 4 further by ﬁnite sample terms. For the ﬁnite sample generalization bound 
we ﬁrst introduce a minor variant of the loss functions  with respect to the sample set.

2Es0h⇣V ⇡
cM

(s0)  V µ
cM

(s0)⌘  (V ⇡

M (s0)  V µ

M (s0))i2

5

Deﬁnition 4. Let rt and s0t be an observation of reward and next step given state action pair st  at.
Deﬁne the loss functions as:

Deﬁnition 5. Deﬁne the empirical risk over the behavior distribution and weighted distribution as:

cM  Ht1

(s0)ds0  V ⇡

cM  Ht1

(s0t)◆2

(5)

(6)

(7)

(8)

`r(st  at  rt cM ) = (br(st  at)  rt)2
`T (st  at  s0t cM ) =✓ZS bT (s0|st  at)V ⇡
bRµ(cM ) =
bR⇡ u(cM ) =

H1Xt=0
H1Xt=0

nXi=1
nXi=1

0:t = ⇡)

`r(s(i)
t

1(a(i)

  a(i)
t

1
n

1
n

t

t

  s0(i)

  a(i)
t

  r(i) cM ) + `T (s(i)
h`r(s(i)

 cM )
  r(i) cM ) + `T (s(i)

  a(i)
t

t

t

  a(i)
t

  s0(i)

t

 cM )i  

1

.

i=1

1(a(i)
0:t=⇡)
n

where n is the dataset size  s(i)
t

bu0:t
is the state of the tth step in the ith trajectory  and bu0:t =
Pn
Theorem 2. Suppose M is a model class of MDP models based on representation . For n
trajectories sampled by µ  let `t(st  at cM) = `r(st  at  rt cM ) + `T (st  at  s0t cM )  and dt be the
pseudo-dimension of function class {`t(st  at cM) cM 2M }. Suppose H is the reproducing
kernel Hilbert space induced by k  and F is the unit ball in it. Assume there exists a constant B t
`t( (z) ⇡ ( (z)) cM) 2F . With probability 1  3  for anycM 2M :
such that
M (s0)|cMi2
Es0hV ⇡
 MSE⇡ + MSEµ  2HbRµ(cM ) + 2HbR⇡ u(cM )
(s0)  V ⇡
cM
M µ (zt)⌘ + min⇢DF ✓ 1
B t✓IPMF⇣bp F
H1Xt=0
M µ(zt) bp CF
n3/8 ✓V[
H1Xt=0
CMn  t
bu0:t

pmt 2◆   2⌫◆
  1]◆ (9)

 ` t] + V[1 ` t] + `t maxV[

1(a0:t = ⇡)

1(a0:t = ⇡)

pmt 1

+ 2H

+ 2H

u0:t

B t

+

1

mt 1 and mt 2 are the number of samples used to estimate bp F
spectively. DF
max{qEpM µ[w2`2

t ]}. `t max = maxst at |`t(st  at)|.

t ] qEbpM µ[w2`2

is a function of the kernel k.

CMn  t

M µ (zt) re-
is a function of dt. V[w  `t] =

M µ(zt) and bp CF

The ﬁrst term is the empirical loss over the observational data distribution. The second term is a
re-weighted empirical loss  which is an empirical version of the ﬁrst term in Theorem 1. As said
previously  this re-weighting has less variance than importance sampling in practice  especially when
the sample size is limited. Theorem 3 in Appendix C.3 shows that the variance of this ratio is also no
greater than the variance of IS weights. Our bound is based on the empirical estimate of the marginal
probability u0:t and we are not required to know the behavior policy. Our method’s independence of
the behavior policy is a signiﬁcant advantage over IS methods which are very susceptible to errors
its estimation  as we discuss in appendix A. In practice  this marginal probability u0:t is easier to
estimate than µ when µ is unknown. The third term is an empirical estimate of IPM  which we
described in Theorem 1. We use norm-1 RKHS functions and MMD distance in this theorem and our
algorithm. There are similar but worse results for Wasserstein distance and total variation distance
[20]. DF measures how complex F is. It is obtained from concentration measures about empirical
IPM estimators [20]. The constant CMn  t measures how complex the model class is and it is derived
from traditional learning theory results [4].
We compare our bound with the upper bound of model error for OPPE in [9]. In the corrected version
of corollary 2 in [9]  the upper bound of absolute error has a linear dependency on p¯⇢1:H where
¯⇢1:H is an upper bound of the importance ratio  which is usually a dominant term in long horizon
cases. As we clariﬁed in last paragraph  the re-weighting weights in our bound  which are marginal
action probability ratios  enjoy a lower variance than IS weights (See Appendix C.3).

6

5 Algorithm for Representation Balancing MDPs

Based on our generalization bound above  we propose an algorithm to learn an MDP model for OPPE 
minimizing the following objective function:

t=0

(10)
This objective is based on Equation 9 in Theorem 2. We minimize the terms in this upper bound

L(cM; ↵t) = bRµ(cM) + bR⇡ u(cM) +XH1
that are related to the model cM. Note that since B t depends on the loss function  we cannot
know B t in practice. We therefore use a tunable factor ↵ in our algorithm. R(cM) here is some

kind of bounded regularization term of model that one could choose  corresponding to the model
class complexity term in Equation 9. This objective function matches our intuition about using
lower-variance weights for the re-weighting component and using IPM of the representation to avoid
ﬁtting the behavior data distribution.

M µ(zt) bp CF

↵tIPMF⇣bp F

M µ (zt)⌘ +

R(cM)

n3/8

In this work  (s) andcM are parameterized by neural networks  due to their strong ability to learn

representations. We use an estimator of IPM term from Sriperumbudur et al. [21]. All terms in the
objective function are differentiable  allowing us to train them jointly by minimizing the objective by
a gradient based optimization algorithm.
After we learn an MDP by minimizing the objective above  we use Monte-Carlo estimates or value
iteration to get the value for any initial state s0 as an estimator of policy value for that state. We show
that if there exists an MDP and representation model in our model class that could achieve:

cM ✓Rµ(cM) + R⇡ u(cM) +XH1

↵tIPMF⇣p F
M (s0)]2 ! 0 and estimator V ⇡
cM⇤⇤

then limn!1 Es0[V ⇡
cM⇤⇤
any s0. See Corollary 2 in Appendix for detail.
We can use our model in any OPPE estimators that leverage model-based estimators  such as doubly
robust [10] and MAGIC [22]  though our generalization MSE bound is just for the model value.

(s0) is a consistent estimator for

M µ (zt)⌘◆ = 0 

(s0)  V ⇡

min

t=0

M µ(zt)  p CF

6 Experiments

6.1 Synthetic control domain: Cart Pole and Montain Car
We test our algorithm on two continuous-state benchmark domains. We use a greedy policy from a
learned Q function as the evaluation policy  and an ✏-greedy policy with ✏ = 0.2 as the behavior policy.
We collect 1024 trajectories for OPPE. In Cart Pole domain the average length of trajectories is
around 190 (long horizon variant)  or around 23 (short horizon variant). In Mountain Car the average
length of trajectories is around 150. The long horizon setting (H>100) is challenging for IS-based
OPPE estimators due to the deterministic evaluation policy and long horizon  which will give the IS
weights high variance. Deterministic dynamics and long horizons are common in real-world domains 
and most off policy policy evaluation algorithms struggle in such scenarios.
We compare our method RepBM  with two baseline approximate models (AM and AM(⇡))  doubly
robust (DR)  more robust doubly robust (MRDR)  and importance sampling (IS). The baseline
approximate model (AM) is an MDP model-based estimator trained by minimizing the empirical risk 
using the same model class as RepBM. AM(⇡) is an MDP model trained with the same objective
as our method but without the MSEµ term. DR is a doubly robust estimator using our model and
DR(AM) is a doubly robust estimator using the baseline model. MRDR [7] is a recent method that
trains a Q function as the model-based part in DR to minimize the resulting variance. We include
their Q function estimator (MRDR Q)  the doubly robust estimator that combines this Q function
with IS (MRDR).
The reported results are square root of the average MSE over 100 runs. ↵ is set to 0.01 for RepBM.
We report mean and individual MSEs  corresponding to MSEs of average policy value and individual

policy value  [Es0bV (s0)Es0V (s0)]2 and Es0[bV (s0) V (s0)]2 respectively. IS and DR methods re-

weight samples  so their estimates for single initial states are not applicable  especially in continuous
state space. A comparison across more methods is included in the appendix.

7

Table 1: Root MSE for Cart Pole

DR(AM) AM(⇡) MRDR Q MRDR

IS

Long Horizon

Mean

Individual

Short Horizon

Mean

Individual

RepBM
0.4121
1.033
RepBM
0.07836
0.4811

DR
1.359

-
DR

0.02081

AM
0.7535
1.313
AM
0.1254
0.5506

1.786

-

41.80
47.63

151.1
151.9

202
-

DR(AM) AM(⇡) MRDR Q MRDR
0.258
0.0235

0.1233
0.5974

3.013
3.823

-

194.5

-
IS
2.86

-

-
Table 2: Root MSE for Mountain Car

-

Mean

Individual

RepBM DR
12.31
135.8
31.38

-

AM DR(AM) AM(⇡) MRDR Q MRDR
17.15
172.7
36.36

135.4
138.1

72.61
79.46

141.6

-

-

IS

149.7

-

Representation Balancing MDPs outperform baselines for long time horizons. We observe that
MRDR variants and IS methods have high MSE in the long horizon setting. The reason is that the IS
weights for 200-step trajectories are extremely high-variance  and MRDR whose objective depends
on the square of IS weights  also fails. Compared with the baseline model  we can see that our method
is better than AM for both the pure model case and when used in doubly robust. We also observe that
the IS part in doubly robust actually hurts the estimates  for both RepBM and AM.
Representation Balancing MDPs outperform baselines in deterministic settings. To observe the
beneﬁt of our method beyond long horizon cases  we also include results on Cart Pole with a shorter
horizon  by using weaker evaluation and behavior policies. The average length of trajectories is about
23 in this setting. Here  we observe that RepBM is still better than other model-based estimators 
and doubly robust that uses RepBM is still better than other doubly robust methods. Though MRDR
produces substantially lower MSE than IS  which matches the report in Farajtabar et al. [7]  it still
has higher MSE than RepBM and AM  due to the high variance of its learning objective when the
evaluation policy is deterministic.
Representation Balancing MDPs produce accurate estimates even when the behavior policy
is unknown. For both horizon cases  we observe that RepBM learned with no knowledge of the
behavior policy is better than methods such as MRDR and IS that use the true behavior policy.

6.2 HIV simulator

We demonstrate our method on an HIV treatment simulation domain. The simulator is described in
Ernst et al. [6]  and consists of 6 parameters describing the state of the patient and 4 possible actions.
The HIV simulator has richer dynamics than the two simple control domains above. We learn an
evaluation policy by ﬁtted Q iteration and use the ✏-greedy policy of the optimal Q function as the
behavior policy.
We collect 50 trajectories from the behavior policy and learn our model with the baseline approximate
model (AM). We compare the root average MSE of our model with the baseline approximate
MDP model  importance sampling (IS)  per-step importance sampling (PSIS) and weighted per-step
importance sampling (WPSIS). The root average MSEs reported are averaged over 80 runs. We
observe that RepBM has the lowest root MSE on estimating the value of the evaluation policy.

Table 3: Relative Root MSE for HIV

RepBM AM
0.062
0.067

IS
0.95

PSIS WPSIS
0.273
0.146

Mean

7 Discussion and Conclusion

One interesting issue for our method is the effect of the hyper-parameter ↵ on the quality of estimator.
In the appendix  we include the results of RepBM across different values of ↵. We ﬁnd that our
method outperforms prior work for a large range of alphas  for both domains. In both domains

8

we observe that the effect of IPM adjustment (non-zero ↵) is less than the effect of "marginal" IS
re-weighting  which matches the results in Shalit et al.’s work in the binary action bandit case [19].
To conclude  in this work we give an MDP model learning method for the individual OPPE problem
in RL  based on a new ﬁnite sample generalization bound of MSE for the model value estimator.
We show our method results in substantially smaller MSE estimates compared to state-of-the-art
baselines in common benchmark control tasks and on a more challenging HIV simulator.

Acknowledgments
This work was supported in part by the Harvard Data Science Initiative  Siemens  and a NSF CAREER
grant.

References
[1] A. M. Alaa and M. van der Schaar. Bayesian inference of individualized treatment effects using multi-task

gaussian processes. In Advances in Neural Information Processing Systems  pages 3424–3432  2017.

[2] O. Atan  W. R. Zame  and M. van der Schaar. Learning optimal policies from observational data. arXiv

preprint arXiv:1802.08679  2018.

[3] G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang  and W. Zaremba. Openai gym.

arXiv preprint arXiv:1606.01540  2016.

[4] C. Cortes  Y. Mansour  and M. Mohri. Learning bounds for importance weighting. In Advances in neural

information processing systems  pages 442–450  2010.

[5] M. Dudík  J. Langford  and L. Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th
International Conference on International Conference on Machine Learning  pages 1097–1104. Omnipress 
2011.

[6] D. Ernst  G.-B. Stan  J. Goncalves  and L. Wehenkel. Clinical data based optimal sti strategies for hiv: a
reinforcement learning approach. In Decision and Control  2006 45th IEEE Conference on  pages 667–672.
IEEE  2006.

[7] M. Farajtabar  Y. Chow  and M. Ghavamzadeh. More robust doubly robust off-policy evaluation. In

Proceedings of the 35th International Conference on Machine Learning  pages 1447–1456  2018.

[8] Z. Guo  P. S. Thomas  and E. Brunskill. Using options and covariance testing for long horizon off-policy

policy evaluation. In Advances in Neural Information Processing Systems  pages 2492–2501  2017.

[9] J. P. Hanna  P. Stone  and S. Niekum. Bootstrapping with models: Conﬁdence intervals for off-policy
evaluation. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems  pages
538–546. International Foundation for Autonomous Agents and Multiagent Systems  2017.

[10] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In Proceedings
of the 33rd International Conference on International Conference on Machine Learning-Volume 48  pages
652–661. JMLR. org  2016.

[11] F. Johansson  U. Shalit  and D. Sontag. Learning representations for counterfactual inference. In Interna-

tional Conference on Machine Learning  pages 3020–3029  2016.

[12] F. D. Johansson  N. Kallus  U. Shalit  and D. Sontag. Learning weighted representations for generalization

across designs. arXiv preprint arXiv:1802.08598  2018.

[13] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine learning 

49(2-3):209–232  2002.

[14] S. Künzel  J. Sekhon  P. Bickel  and B. Yu. Meta-learners for estimating heterogeneous treatment effects

using machine learning. arXiv preprint arXiv:1706.03461  2017.

[15] T. Mandel  Y.-E. Liu  S. Levine  E. Brunskill  and Z. Popovic. Ofﬂine policy evaluation across represen-
tations with applications to educational games. In Proceedings of the 2014 international conference on
Autonomous agents and multi-agent systems  pages 1077–1084. International Foundation for Autonomous
Agents and Multiagent Systems  2014.

[16] D. Precup  R. S. Sutton  and S. P. Singh. Eligibility traces for off-policy policy evaluation. In ICML  pages

759–766. Citeseer  2000.

9

[17] J. M. Robins  A. Rotnitzky  and L. P. Zhao. Estimation of regression coefﬁcients when some regressors are

not always observed. Journal of the American statistical Association  89(427):846–866  1994.

[18] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In Advances in Neural

Information Processing Systems  pages 1697–1708  2017.

[19] U. Shalit  F. D. Johansson  and D. Sontag. Estimating individual treatment effect: generalization bounds

and algorithms. In International Conference on Machine Learning  pages 3076–3085  2017.

[20] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  B. Schölkopf  and G. R. Lanckriet. On integral probability

metrics  -divergences and binary classiﬁcation. arXiv preprint arXiv:0901.2698  2009.

[21] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  B. Schölkopf  G. R. Lanckriet  et al. On the empirical

estimation of integral probability metrics. Electronic Journal of Statistics  6:1550–1599  2012.

[22] P. Thomas and E. Brunskill. Data-efﬁcient off-policy policy evaluation for reinforcement learning. In

International Conference on Machine Learning  pages 2139–2148  2016.

[23] P. S. Thomas  G. Theocharous  and M. Ghavamzadeh. High-conﬁdence off-policy evaluation. In AAAI 

2015.

[24] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using random forests.

Journal of the American Statistical Association  just-accepted  2017.

[25] J. Yoon  J. Jordon  and M. van der Schaar. Ganite: Estimation of individualized treatment effects using

generative adversarial nets. ICLR  2018.

10

,Osbert Bastani
Yani Ioannou
Leonidas Lampropoulos
Dimitrios Vytiniotis
Aditya Nori
Antonio Criminisi
Yao Liu
Omer Gottesman
Aniruddh Raghu
Matthieu Komorowski
Aldo Faisal
Finale Doshi-Velez
Emma Brunskill