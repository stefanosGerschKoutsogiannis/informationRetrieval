2013,Summary Statistics for Partitionings and Feature Allocations,Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However  in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper  we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.,Summary Statistics for

Partitionings and Feature Allocations

Is¸ık Barıs¸ Fidaner

Computer Engineering Department

Bo˘gazic¸i University  Istanbul

Ali Taylan Cemgil

Computer Engineering Department

Bo˘gazic¸i University  Istanbul

fidaner@alternatifbilisim.org

taylan.cemgil@boun.edu.tr

Abstract

Inﬁnite mixture models are commonly used for clustering. One can sample from
the posterior of mixture assignments by Monte Carlo methods or ﬁnd its maximum
a posteriori solution by optimization. However  in some problems the posterior
is diffuse and it is hard to interpret the sampled partitionings. In this paper  we
introduce novel statistics based on block sizes for representing sample sets of par-
titionings and feature allocations. We develop an element-based deﬁnition of en-
tropy to quantify segmentation among their elements. Then we propose a simple
algorithm called entropy agglomeration (EA) to summarize and visualize this in-
formation. Experiments on various inﬁnite mixture posteriors as well as a feature
allocation dataset demonstrate that the proposed statistics are useful in practice.

1 Introduction

Clustering aims to summarize observed data by grouping its elements according to their similarities.
Depending on the application  clusters may represent words belonging to topics  genes belonging to
metabolic processes or any other relation assumed by the deployed approach. Inﬁnite mixture mod-
els provide a general solution by allowing a potentially unlimited number of mixture components.
These models are based on nonparametric priors such as Dirichlet process (DP) [1  2]  its super-
class Poisson-Dirichlet process (PDP) [3  4] and constructions such as Chinese restaurant process
(CRP) [5] and stick-breaking process [6] that enable formulations of efﬁcient inference methods [7].
Studies on inﬁnite mixture models inspired the development of several other models [8  9] includ-
ing Indian buffet process (IBP) for inﬁnite feature models [10  11] and fragmentation-coagulation
process for sequence data [12] all of which belong to Bayesian nonparametrics [13].

In making inference on inﬁnite mixture models  a sample set of partitionings can be obtained from
the posterior.1 If the posterior is peaked around a single partitioning  then the maximum a posteriori
solution will be quite informative. However  in some cases the posterior is more diffuse and one
needs to extract statistical information about the random partitioning induced by the model. This
problem to ‘summarize’ the samples from the inﬁnite mixture posterior was raised in bioinformatics
literature in 2002 by Medvedovic and Sivaganesan for clustering gene expression proﬁles [14]. But
the question proved difﬁcult and they ‘circumvented’ it by using a heuristic linkage algorithm based
on pairwise occurence probabilities [15  16]. In this paper  we approach this problem and propose
basic methodology for summarizing sample sets of partitionings as well as feature allocations.

Nemenman et al. showed in 2002 that the entropy [17] of a DP posterior was strongly determined
by its prior hyperparameters [18]. Archer et al. recently elaborated these results with respect to
PDP [19].
In other work  entropy was generalized to partitionings by interpreting partitionings
as probability distributions [20  21]. Therefore  entropy emerges as an important statistic for our
problem  but new deﬁnitions will be needed for quantifying information in feature allocations.

1In methods such as collapsed Gibbs sampling  slice sampling  retrospective sampling  truncation methods

1

In the following sections  we deﬁne the problem and introduce cumulative statistics for representing
partitionings and feature allocations. Then  we develop an interpretation for entropy function in
terms of per-element information in order to quantify segmentation among their elements. Finally 
we describe entropy agglomeration (EA) algorithm that generates dendrograms to summarize sam-
ple sets of partitionings and feature allocations. We demonstrate EA on inﬁnite mixture posteriors
for synthetic and real datasets as well as on a real dataset directly interpreted as a feature allocation.

2 Basic deﬁnitions and the motivating problem

We begin with basic deﬁnitions. A partitioning of a set of elements [n] = {1  2  . . .   n} is a set of
blocks Z = {B1  . . .   B|Z|} such that Bi ⊂ [n] and Bi 6= ∅ for all i ∈ {1  . . .   n}  Bi ∩ Bj = ∅
for all i 6= j  and ∪iBi = [n].2 We write Z ⊢ [n] to designate that Z is a partitioning of [n].3 A
sample set E = {Z (1)  . . .   Z (T )} from a distribution π(Z) over partitionings is a multiset such that
Z (t) ∼ π(Z) for all t ∈ {1  . . .   T }. We are required to extract information from this sample set.
Our motivation is the following problem: a set of observed elements (x1  . . .   xn) are clustered
by an inﬁnite mixture model with parameters θ(k) for each component k and mixture assignments
(z1  . . .   zn) drawn from a two-parameter CRP prior with concentration α and discount d [5].

z ∼ CRP (z; α  d)

θ(k) ∼ p(θ)

xi | zi  θ ∼ F (xi | θ(zi))

(1)

In the conjugate case  all θ(k) can be integrated out to get p(zi | z−i  x) for sampling zi [22]:

p(zi | z−i  x) ∝ Z p(z  x  θ) dθ ∝ 


nk−d

α+dK +

n−1+α R F (xi|θ) p(θ|x−i  z−i) dθ
n−1+α R F (xi|θ) p(θ) dθ

if k ≤ K +

otherwise

(2)

There are K + non-empty components and nk elements in each component k. In each iteration  xi
will either be put into an existing component k ≤ K + or it will be assigned to a new component. By
sampling all zi repeatedly  a sample set of assignments z(t) are obtained from the posterior p(z | x) =
π(Z). These z(t) are then represented by partitionings Z (t) ⊢ [n]. The induced sample set contains
information regarding (1) CRP prior over partitioning structure given by the hyperparameters (α  d)
and (2) integrals over θ that capture the relation among the observed elements (x1  . . .   xn).
In addition  we aim to extract information from feature allocations  which constitute a superclass of
partitionings [11]. A feature allocation of [n] is a multiset of blocks F = {B1  . . .   B|F |} such that
Bi ⊂ [n] and Bi 6= ∅ for all i ∈ {1  . . .   n}. A sample set E = {F (1)  . . .   F (T )} from a distribution
π(F ) over feature allocations is a multiset such that F (t) ∼ π(F ) for all t. Current exposition will
focus on partitionings  but we are also going to show how our statistics apply to feature allocations.

Assume that we have obtained a sample set E of partitionings. If it was obtained by sampling from
an inﬁnite mixture posterior  then its blocks B ∈ Z (t) correspond to the mixture components. Given
a sample set E  we can approximate any statistic f (Z) over π(Z) by averaging it over the set E:

Z (1)  . . .   Z (T ) ∼ π(Z)

⇒

1
T

T

Xt=1

f (Z (t)) ≈ h f (Z) iπ(Z)

(3)

Which f (Z) would be a useful statistic for Z? Three statistics commonly appear in the literature:
First one is the number of blocks |Z|  which has been analyzed theoretically for various nonpara-
metric priors [2  5]. It is simple  general and exchangable with respect to the elements [n]  but it is
not very informative about the distribution π(Z) and therefore is not very useful in practice.
A common statistic is pairwise occurence  which is used to extract information from inﬁnite mixture
posteriors in applications like bioinformatics [14]. For given pairs of elements {a  b}  it counts the

number of blocks that contain these pairs  written Pi[{a  b} ⊂ Bi]. It is a very useful similarity

measure  but it cannot express information regarding relations among three or more elements.

Another statistic is exact block size distribution (referred to as ‘multiplicities’ in [11  19]). It counts

the partitioning’s blocks that contain exactly k elements  written Pi[|Bi| = k]. It is exchangable

with respect to the elements [n]  but its weighted average over a sample set is difﬁcult to interpret.

2We use the term ‘partitioning’ to indicate a ‘set partition’ as distinguished from an integer ‘partition’.
3The symbol ‘⊢’ is usually used for integer partitions  but here we use it for partitionings (=set partitions).

2

Let us illustrate the problem by a practical example  to which we will return in the formulations:

E3 = {Z (1)  Z (2)  Z (3)}

Z (1) = {{1  3  6  7}  {2}  {4  5}}
Z (2) = {{1  3  6}  {2  7}  {4  5}}
Z (3) = {{1  2  3  6  7}  {4  5}}

S1 = {1  2  3  4}

S2 = {1  3  6  7}

S3 = {1  2  3}

Suppose that E3 represents interactions among seven genes. We want to compare the subsets of
these genes S1  S2  S3. The projection of a partitioning Z ⊢ [n] onto S ⊂ [n] is deﬁned as the set
of non-empty intersections between S and B ∈ Z. Projection onto S induces a partitioning of S.

P ROJ(Z  S) = {B ∩ S}B∈Z\{∅}

⇒

P ROJ(Z  S) ⊢ S

(4)

Let us represent gene interactions in Z (1) and Z (2) by projecting them onto each of the given subsets:

P ROJ(Z (1)  S1) = {{1  3}  {2}  {4}}
P ROJ(Z (1)  S2) = {{1  3  6  7}}
P ROJ(Z (1)  S3) = {{1  3}  {2}}

P ROJ(Z (2)  S1) = {{1  3}  {2}  {4}}
P ROJ(Z (2)  S2) = {{1  3  6}  {7}}
P ROJ(Z (2)  S3) = {{1  3}  {2}}

Comparing S1 to S2  we can say that S1 is ‘more segmented’ than S2  and therefore genes in S2
should be more closely related than those in S1. However  it is more subtle and difﬁcult to compare
S2 to S3. A clear understanding would allow us to explore the subsets S ⊂ [n] in an informed
manner. In the following section  we develop a novel and general approach based on block sizes that
opens up a systematic method for analyzing sample sets over partitionings and feature allocations.

3 Cumulative statistics to represent structure

We deﬁne cumulative block size distribution  or ‘cumulative statistic’ in short  as the function

φk(Z) = Pi[|Bi| ≥ k]  which counts the partitioning’s blocks of size at least k. We can rewrite the
previous statistics: number of blocks as φ1(Z)  exact block size distribution as φk(Z) − φk+1(Z) 
and pairwise occurence as φ2(P ROJ(Z  {a  b})). Moreover  cumulative statistics satisfy the fol-
lowing property: for partitionings of [n]  φ(Z) always sums up to n  just like a probability mass
function that sums up to 1. When blocks of Z are sorted according to their sizes and the indicators
[|Bi| ≥ k] are arranged on a matrix as in Figure 1a  they form a Young diagram  showing that φ(Z)
is always the conjugate partition of the integer partition of Z. As a result  φ(Z) as well as weighted
averages over several φ(Z) always sum up to n  just like taking averages over probability mass func-
tions (Figure 2). Therefore  cumulative statistics of a random partitioning ‘conserve mass’. In the

Z(1) = {{1  3  6  7}  {2}  {4  5}}

P ROJ(Z(1)  S1) = {{1  3}  {2}  {4}}

B1 = {2}

B2 = {4  5}

B3 = {1  3  6  7}

1

2

4

|B1| ≥ 1

|B2| ≥ 1

|B2| ≥ 2

B1 = {2}

B2 = {4}

|B3| ≥ 1

|B3| ≥ 2 |B3| ≥ 3

|B3| ≥ 4

B3 = {1  3}

1

1

2

|B1| ≥ 1

|B2| ≥ 1

|B3| ≥ 1

|B3| ≥ 2

φ(Z(1)) =

3

2

1

1

φ(P ROJ(Z(1)  S1)) =

3

1

(a) Cumulative block size distribution for a partitioning

(b) For its projection onto a subset

Figure 1: Young diagrams show the conjugacy between a partitioning and its cumulative statistic

4

3

2

1

0

)
)
1
(

Z
(
φ

1

2

4

5

3
k

4

3

2

1

0

)
)
2
(

Z
(
φ

1

2

4

5

3
k

4

3

2

1

0

)
)
3
(

Z
(
φ

1

2

4

5

3
k

4

3

2

1

0

e
e
r
h
t

r
e
v
o
e
g
a
r
e
v
A

1

2

4

5

3
k

Figure 2: Cumulative statistics of the three examples and their average: all sum up to 7

3

case of feature allocations  since elements can be omitted or repeated  this property does not hold.

Z ⊢ [n]

⇒

n

Xk=1

φk(Z) = n

⇒

n

h φk(Z) iπ(Z) = n

Xk=1

(5)

When we project the partitioning Z onto a subset S ⊂ [n]  the resulting vector φ(P ROJ(Z  S))
will then sum up to |S| (Figure 1b). A ‘taller’ Young diagram implies a ‘more segmented’ subset.

We can form a partitioning Z by inserting elements 1  2  3  4  . . . into its blocks (Figure 3a). In
such a scheme  each step brings a new element and requires a new decision that will depend on all
previous decisions. It would be better if we could determine the whole path by few initial decisions.

Now suppose that we know Z from the start and we generate an incremental sequence of subsets
S1 = {1}  S2 = {1  2}  S3 = {1  2  3}  S4 = {1  2  3  4}  . . . according to a permutation of [n]:
σ = (1  2  3  4  . . . ). We can then represent any path in Figure 3a by a sequence of P ROJ(Z  Si)
and determine the whole path by two initial parameters: Z and σ. The resulting tree can be simpliﬁed
by representing the partitionings by their cumulative statistics instead of their blocks (Figure 3b).

Based on this concept  we deﬁne cumulative occurence distribution (COD) as the triangular matrix
of incremental cumulative statistic vectors  written ∆i k(Z  σ) = φk(P ROJ(Z  Si)) where Z ⊢ [n] 
σ is a permutation of [n] and Si = {σ1  . . .   σi} for i ∈ {1  . . .   n}. COD matrices for two extreme
paths (Figure 3c  3e) and for the example partitioning Z (1) (Figure 3d) are shown. For partitionings 
ith row of a COD matrix always sums up to i  even when averaged over a sample set as in Figure 4.

Z ⊢ [n]

⇒

i

Xk=1

∆i k(Z  σ) = i

⇒

i

h ∆i k(Z  σ) iπ(Z) = i

Xk=1

(6)

Expected COD matrix of a random partitioning expresses (1) cumulation of elements by the differ-
ences between its rows  and (2) cumulation of block sizes by the differences between its columns.

As an illustrative example  consider π(Z) = CRP (Z|α  d). Since CRP is exchangable and projec-
tive  its expected cumulative statistic hφ(Z)iπ(Z) for n elements depends only on its hyperparam-
eters (α  d). As a result  its expected COD matrix ∆ = h∆(Z  σ)iπ(Z) is independent of σ  and it

{{1}  {2}  {3}}

{{1}  {2}}

{{1  3}  {2}}
{{1}  {2  3}}
{{1  2}  {3}}

{{1}}

{{1  2}}

{{1  2  3}}

{{1}  {2}  {3}  {4}}

{{1  4}  {2}  {3}}
{{1}  {2  4}  {3}}
{{1}  {2}  {3  4}}
{{1  3}  {2}  {4}}
{{1}  {2  3}  {4}}
{{1  2}  {3}  {4}}

{{1  3}  {2  4}}
{{1  4}  {2  3}}
{{1  2}  {3  4}}

{{1  3  4}  {2}}
{{1}  {2  3  4}}
{{1  2  4}  {3}}
{{1  2  3}  {4}}

{{1  2  3  4}}

(4  0  0  0)

–1.39

(3  0  0)

(3  1  0  0)

–1.04

(2  0)

(2  1  0)

(2  2  0  0)

–0.69

(2  1  1  0)

–0.56

y
p
o
r
t
n
e
n
o
i
t
i
t
r
a
p

(1)

(1  1)

(1  1  1)

(1  1  1  1)

–0

(a) Form a partitioning by inserting elements

(b) Form the statistic vector by inserting elements

1

1 1

1 1 1

1 1 1 1

1 1 1 1 1

1 1 1 1 1 1

1 1 1 1 1 1 1

1

2

3

4

5

6

7

1

2 0

2 1 0

3 1 0 0

3 2 0 0 0

3 2 1 0 0 0

3 2 1 1 0 0 0

1

2 0

3 0 0

4 0 0 0

5 0 0 0 0

6 0 0 0 0 0

7 0 0 0 0 0 0

(c) All elements into one block

(d) COD matrix ∆(Z(1)  (1  . . .   7))

(e) Each element into a new block

Figure 3: Three COD matrices correspond to the three red dotted paths on the trees above

4

1

2

3

4

5

6

7

1.0

1.7 0.3

1.7 1.0 0.3

2.7 1.0 0.3 0.0

2.7 2.0 0.3 0.0 0.0

2.7 2.0 1.0 0.3 0.0 0.0

2.7 2.3 1.0 0.7 0.3 0.0 0.0

y
p
o
r
t

n
e

1

0.8

0.6

0.4

0.2

0

1 2 3 4 5 6 7

1

3

6

7

2

4

5

1.0

1.0 1.0

1.0 1.0 1.0

1.3 1.0 1.0 0.7

1.7 1.3 1.0 0.7 0.3

2.7 2.3 1.0 0.7 0.3 0.0

2.7 2.3 1.0 0.7 0.3 0.0 0.0

y
p
o
r
t

n
e

1

0.8

0.6

0.4

0.2

0

1 3 6 7 2 4 5

Figure 4: CODs and entropies over E3 for permutations (1  2  3  4  5  6  7) and (1  3  6  7  2  4  5)

satisﬁes an incremental formulation with the parameters (α  d) over the indices i ∈ N  k ∈ Z+:

∆0 k = 0

∆i+1 k = ∆i k +


α+d∆i k

i+α

(k−1−d)(∆i k−1−∆i k)

i+α

if k = 1

otherwise

(7)

By allowing k = 0 and setting ∆i 0 = − α
the same matrix can be formulated by a difference equation over the indices i ∈ N  k ∈ N:

d   and ∆0 k = 0 for k > 0 as the two boundary conditions 

(∆i+1 k − ∆i k)(i + α) = (∆i k−1 − ∆i k)(k − 1 − d)

(8)

By setting ∆ = ∆(0) we get an inﬁnite sequence of matrices ∆(m) that satisfy the same equation:

(∆(m)

i+1 k − ∆(m)

i k )(i + α) = (∆(m)

i k−1 − ∆(m)

i k )(k − 1 − d) = ∆(m+1)

i k

(9)

Therefore  expected COD matrix of a CRP-distributed random partitioning is at a constant ‘equilib-
rium’ determined by α and d. This example shows that the COD matrix can reveal speciﬁc infor-
mation about a distribution over partitionings; of course in practice we encounter non-exchangeable
and almost arbitrary distributions over partitionings (e.g.  the posterior distribution of an inﬁnite
mixture)  therefore in the following section we will develop a measure to quantify this information.

4 Entropy to quantify segmentation

Shannon’s entropy [17] can be an appropriate quantity to measure ‘segmentation’ with respect to
partitionings  which can be interpreted as probability distributions [20  21]. Since this interpretation
does not cover feature allocations  we will make an alternative  element-based deﬁnition of entropy.

How does a block B inform us about its elements? Each element has a proportion 1/|B|  let us call
this quantity per-element segment size. Information is zero for |B| = n  since 1/n is the minimum
possible segment size. If |B| < n  the block supplies positive information since the segment size is
larger than minimum  and we know that its segment size could be smaller if the block were larger.
To quantify this information  we deﬁne per-element information for a block B as the integral of
segment size 1/s over the range [|B|  n] of block sizes that make this segment smaller (Figure 5).

pein(B) = Z n

|B|

1
s

ds = log

n
|B|

(10)

In pein(B)  n is a ‘base’ that determines the minimum possible per-element segment size. Since
segment size expresses the signiﬁcance of elements  the function integrates segment sizes over the
block sizes that make the elements less signiﬁcant. This deﬁnition is comparable to the well-known
p-value  which integrates probabilities over the values that make the observations more signiﬁcant.

1
s

1

|B|

|B|

log n
|B|

1
n

n

n

|

B

|

0.4

g
o
l

0.2

|

B

|

n

0

2

4

6

8
block size |B|

10

12

Figure 5: Per-element information for B

Figure 6: Weighted information plotted for each n

5

)
Z
(
H
 
y
p
o
r
t
n
e
 
n
o
i
t
i
t
r
a
p

2.5

2

1.5

1

0.5

0

2

Subset occurence:

Pi[S ⊂ Bi]

a ∈ B b ∈ B

Projection entropy:
H(P ROJ(Z  S))

a ∈ B b ∈ B

0

1

0

2 log 2
1
1

0

1
2 log 2
1

0

0

a ∈ B b ∈ B

a ∈ B b ∈ B

0

0

0
1

0

0

0

0

3 log 3
1
1

2
3 log 3
2

1
3 log 3
1

0

3 log 3
2
2
1
3 log 3
1

2
3 log 3
2

0

S
=
{
a
 
b
}

S
=
{
a
 
b
 
c
}

4
number of elements n

6

8

10

12

c ∈ B

c ∈ B

Figure 7: H(Z) in incremental construction of Z

Figure 8: Comparing two subset statistics

We can then compute the per-element information supplied by a partitioning Z  by taking a weighted
average over its blocks  since each block B ∈ Z supplies information for a different proportion
|B|/n of the elements being partitioned. For large n  weighted per-element information reaches its
maximum near |B| ≈ n/2 (Figure 6). Total weighted information for Z gives Shannon’s entropy
function [17] which can be written in terms of the cumulative statistics (assuming φn+1 = 0):

H(Z) =

|Z|

Xi=1

|Bi|
n

pein(Bi) =

|Z|

Xi=1

|Bi|
n

log

n
|Bi|

=

n

Xk=1

(φk(Z) − φk+1(Z))

k
n

log

n
k

(11)

Entropy of a partitioning increases as its elements become more segmented among themselves. A
partitioning with a single block has zero entropy  and a partitioning with n blocks has the maximum
entropy log n. Nodes of the tree we examined in the previous section (Figure 3b) were vertically
arranged according to their entropies. On the extended tree (Figure 7)  nth column of nodes represent
the possible partitionings of n. This tree serves as a ‘grid’ for both H(Z) and φ(Z)  as they are
linearly related with the general coefﬁcient ( k
k−1 ). A similar grid for feature
allocations can be generated by inserting nodes for cumulative statistics that do not conserve mass.

n log n

n log n

k − k−1

To quantify the segmentation of a subset S  we compute projection entropy H(P ROJ(Z  S)). To
understand this function  we compare it to subset occurence in Figure 8. Subset occurence acts as a
‘score’ that counts the ‘successful’ blocks that contain all of S  whereas projection entropy acts as a
‘penalty’ that quantiﬁes how much S is being divided and segmented by the given blocks B ∈ Z.

A partitioning Z and a permutation σ of its elements induce an entropy sequence (h1  . . .   hn) such
that hi(Z  σ) = H(P ROJ(Z  Si)) where Si = {σ1  . . .   σi} for i ∈ {1  . . .   n}. To ﬁnd subsets of
elements that are more closely related  one can seek permutations σ that keep the entropies low. The
generated subsets Si will be those that are less segmented by B ∈ Z. For the example problem  the
permutation 1  3  6  7  . . . keeps the expected entropies lower  compared to 1  2  3  4  . . . (Figure 4).

5 Entropy agglomeration and experimental results

We want to summarize a sample set using the proposed statistics. Permutations that yield lower
entropy sequences can be meaningful  but a feasible algorithm can only involve a small subset of the
n! permutations. We deﬁne entropy agglomeration (EA) algorithm  which begins from 1-element
subsets  and merges in each iteration the pair of subsets that yield the minimum expected entropy:

Entropy Agglomeration Algorithm:

1. Initialize Ψ ← {{1}  {2}  . . .   {n}}.
2. Find the subset pair {Sa  Sb} ⊂ Ψ that minimizes the entropy h H(P ROJ(Z  Sa ∪ Sb)) iπ(Z).
3. Update Ψ ← (Ψ\{Sa  Sb}) ∪ {Sa ∪ Sb}.
4. If |Ψ| > 1 then go to 2.
5. Generate the dendrogram of chosen pairs by plotting minimum entropies for every split.

6

The resulting dendrogram for the example partitionings are shown in Figure 9a. The subsets {4  5}
and {1  3  6} are shown in individual nodes  because their entropies are zero. Besides using this
dendrogram as a general summary  one can also generate more speciﬁc dendrograms by choos-
ing speciﬁc elements or speciﬁc parts of the data. For a detailed element-wise analysis  entropy
sequences of particular permutations σ can be assessed. Entropy Agglomeration is inspired by ‘ag-
glomerative clustering’  a standard approach in bioinformatics [23]. To summarize partitionings
of gene expressions  [14] applied agglomerative clustering by pairwise occurences. Although very
useful and informative  such methods remain ‘heuristic’ because they require a ‘linkage criterion’ in
merging subsets. EA avoids this drawback  since projection entropy is already deﬁned over subsets.
To test the proposed algorithm  we apply it to partitionings sampled from inﬁnite mixture posteriors.
In the ﬁrst three experiments  data is modeled by an inﬁnite mixture of Gaussians  where α =
0.05  d = 0  p(θ) = N (θ|0  5) and F (x|θ) = N (x|θ  0.15) (see Equation 1). Samples from the
posterior are used to plot the histogram over the number of blocks  pairwise occurences  and the
EA dendrogram. Pairwise occurences are ordered according to the EA dendrogram. In the fourth
experiment  EA is directly applied on the data. We describe each experiment and make observations:
1) Synthetic data (Figure 9b): 30 points on R2 are arranged in three clusters. Plots are based on
450 partitionings from the posterior. Clearly separating the three clusters  EA also reﬂects their
qualitative differences. The dispersedness of the ﬁrst cluster is represented by distinguishing ‘inner’
elements 1  10  from ‘outer’ elements 6  7. This is also seen as shades of gray in pairwise occurences.
2) Iris ﬂower data (Figure 9c): This well-known dataset contains 150 points on R4 from three
ﬂower species [24]. Plots are based on 150 partitionings obtained from the posterior. For conve-
nience  small subtrees are shown as single leaves and elements are labeled by their species. All of
50 A points appear in a single leaf  as they are clearly separated from B and C. The dendrogram
automatically scales to cover the points that are more uncertain with respect to the distribution.
3) Galactose data (Figure 9d): This is a dataset of gene expressions by 820 genes in 20 experimental
conditions [25]. First 204 genes are chosen  and ﬁrst two letters of gene names are used for labels.
Plots are based on 250 partitionings from the posterior. 70 RP (ribosomal protein) genes and 12
HX (hexose transport) genes appear in individual leaves. In the large subtree on the top  an ‘outer’
grouping of 19 genes (circles in data plot) is distinguished from the ‘inner’ long tail of 68 genes.
4) IGO (Figure 9e): This is a dataset of intergovernmental organizations (IGO) [26 v2.1] that con-
tains IGO memberships of 214 countries through the years 1815-2000. In this experiment  we take
a different approach and apply EA directly on the dataset interpreted as a sample set of single-block
feature allocations  where the blocks are IGO-year tuples and elements are the countries. We take
the subset of 138 countries that appear in at least 1000 of the 12856 blocks. With some excep-
tions  the countries display a general ordering of continents. From the ‘outermost’ continent to the
‘innermost’ continent they are: Europe  America-Australia-NZ  Asia  Africa and Middle East.

6 Conclusion
In this paper  we developed a novel approach for summarizing sample sets of partitionings and fea-
ture allocations. After presenting the problem  we introduced cumulative statistics and cumulative
occurence distribution matrices for each of its permutations  to represent a sample set in a systematic
manner. We deﬁned per-element information to compute entropy sequences for these permutations.
We developed entropy agglomeration (EA) algorithm that chooses and visualises a small subset of
these entropy sequences. Finally  we experimented with various datasets to demonstrate the method.
Entropy agglomeration is a simple algorithm that does not require much knowledge to implement 
but it is conceptually based on the cumulative statistics we have presented. Since we primarily aimed
to formulate a useful algorithm  we only made the essential deﬁnitions  and several points remain
to be elucidated. For instance  cumulative statistics can be investigated with respect to various
nonparametric priors. Our deﬁnition of per-element information can be developed with respect to
information theory and hypothesis testing. Last but not least  algorithms like entropy agglomeration
can be designed for summarization tasks concerning various types of combinatorial sample sets.

Acknowledgements
We thank Ayc¸a Cankorur  Erkan Karabekmez  Duygu Dikicio˘glu and Bet¨ul Kırdar from Bo˘gazic¸i
University Chemical Engineering for introducing us to this problem by very helpful discussions.
This work was funded by T ¨UB˙ITAK (110E292) and BAP (6882-12A01D5).

7

Number of blocks

Galactose data:

(e) IGO data:

1 SN
1 PR
1 SL
1 PR
1 PR
1 PA
1 LS
1 SM
1 SR
1 HA
1 HA
1 SN
1 SM
1 SR
1 MT
1 SL
1 MT
1 FI
1 AB
1 SR
1 TF
1 FI
1 NT
1 ME
1 NT
1 PR
1 AB
1 PR
1 RR
1 PR
1 HA
1 LS
1 PA
1 TF
1 PR
1 PR
1 HA
1 SR
1 ME
1 SR
1 SR
1 SM
1 SN
1 SM
1 SR
1 PR
1 SL
1 MT
1 MT
1 LS
1 PR
1 SL
1 RR
1 NC
1 SN
1 HA
1 SN
1 AB
1 HP
1 PR
1 PA
1 LS
1 HP
1 NC
1 SR
1 AB
1 PR
1 TF
1 SR
1 PR
1 SN
1 HP
1 NC
1 FI
1 FI
1 ME
1 PR
1 RR
1 ME
1 NT
1 NC
1 HA
1 PR
1 HA
1 PR
1 TF
1 HA
1 ST
12 HX
1 SN
1 ST
1 SN
1 SN
1 SN
1 HP
1 PA
1 ST
1 ST
1 NT
1 SR
1 SN
1 SR
1 RR
1 SN
1 SR
1 RP
1 RP
1 RP
1 RP
1 RP
1 RP
70 RP  4 YD
1 CD
1 CD
1 PG
1 PG
1 CD
1 CD
1 PG
1 PG

germany
russia
poland
hungary
romania
bulgaria
luxembourg
ireland
spain
portugal
italy
greece
uk
france
netherlands
belgium
wgermany
iceland
norway
finland
sweden
denmark
yugoslaviaserb
switzerland
austria
usa
japan
canada
soafrica
newzealand
australia
cuba
haiti
domrepublic
nicaragua
guatemala
honduras
elsalvador
panama
costarica
venezuela
ecuador
peru
colombia
uruguay
chile
paraguay
bolivia
mexico
brazil
argentina
trinidad
jamaica
guyana
barbados
suriname
grenada
bahamas
czechoslovakia
albania
thailand
philippines
malaysia
indonesia
srilanka
pakistan
india
sokorea
china
vietnam
singapore
papuanewguinea
fiji
nepal
myanmar
bangladesh
laos
cambodia
afghanistan
nigeria
ghana
liberia
sierraleone
gambia
madagascar
ethiopia
zaire
rwanda
burundi
mauritius
zambia
malawi
uganda
tanzania
kenya
guineabissau
eqguinea
zimbabwe
mozambique
swaziland
lesotho
botswana
sudan
somalia
mauritania
gabon
cameroon
chad
congobrazz
car
senegal
ivorycoast
mali
niger
burkinafaso
guinea
togo
benin
turkey
iran
israel
malta
cyprus
egypt
tunisia
morocco
algeria
syria
lebanon
libya
jordan
saudiarabia
kuwait
iraq
oman
bahrain
uae
qatar

0

0.5

1

1.5

entropy

0

0.2

0.4

0.6

0.8

entropy

(a) Example partitionings:

Z(1) = {{1  3  6  7}  {2}  {4  5}}
Z(2) = {{1  3  6}  {2  7}  {4  5}}
Z(3) = {{1  2  3  6  7}  {4  5}}

4  5

2

7

1  3  6

0

0.2

0.6

0.4
entropy

0.8

2

1.5

1

0.5

0

4
5
2
7
1
3
6

2

3

Pairwise occurences

4 5 2 7 1 3 6

Number of blocks

100

50

0

3 5 7 9 11 13

7
6
8
3
9
5
4
2
10
1
19
15
17
16
20
14
18
12
13
11
30
25
28
26
24
22
29
27
23
21

(b) Synthetic data:

1.5

1

15

19

0.5

0

16
14
17

13

20

11

18
12

−0.5

30

25

26

28

22
24

29

23
21
27

−1

−1.5

7

6

5
1

10

3

9

4

2

8

−1.5

−1

−0.5

0

0.5

1

1.5

Number of blocks

5

6

7

8

Pairwise occurences

2 C
1 C
1 C
2 C
2 C
4 C
12 C
1 C
1 C
8 C
1 C
1 B
50 A  1 B  1 C
1 B
1 B
2 B
2 B
2 B
9 B
1 B
5 B  1 C
1 B
1 B
1 B
1 B
2 B
1 C
11 B  8 C
3 C
8 B  1 C

50

100

150

1

0
entropy

60

40

20

0

50

100

150

Pairwise occurences

10

20

30

0

1

entropy

10

20

30

(c) Iris ﬂower data:

A

B

C

2

1

0

−1

−2
−4

−2

2
(PCA projection R4 → R2)

0

4

(d) Galactose data:

2

0

−2

−4
−4

HX

others

RP

−3

−2

−1

0

1

2

(PCA projection R20 → R2)

Number of blocks

Pairwise occurences

50

40

30

20

10

0

50

100

150

200

9

11

13

15

17

19

50

100

150

200

Figure 9: Entropy agglomeration and other results from the experiments (See the text)

8

References
[1] Ferguson  T. S. (1973) A Bayesian analysis of some nonparametric problems. Annals of Statistics 

1(2):209–230.

[2] Teh  Y. W. (2010) Dirichlet Processes. In Encyclopedia of Machine Learning. Springer.
[3] Kingman  J. F. C. (1992). Poisson processes. Oxford University Press.
[4] Pitman  J.  & Yor  M. (1997) The two-parameter Poisson–Dirichlet distribution derived from a stable subor-

dinator. Annals of Probability  25:855-900.

[5] Pitman  J. (2006) Combinatorial Stochastic Processes. Lecture Notes in Mathematics. Springer-Verlag.
[6] Sethuraman  J. (1994) A constructive deﬁnition of Dirichlet priors. Statistica Sinica  4  639-650.
[7] Neal  R. M. (2000) Markov chain sampling methods for Dirichlet process mixture models  Journal of Com-

putational and Graphical Statistics  9:249–265.

[8] Meeds  E.  Ghahramani  Z.  Neal  R.  & Roweis  S. (2007) Modelling dyadic data with binary latent factors.

In Advances in Neural Information Processing 19.

[9] Teh  Y. W.  Jordan  M. I.  Beal  M. J.  & Blei  D. M. (2006) Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581.

[10] Grifﬁths  T. L. and Ghahramani  Z. (2011) The Indian buffet process: An introduction and review. Journal

of Machine Learning Research  12:1185–1224.

[11] Broderick  T.  Pitman  J.  & Jordan  M. I. (2013). Feature allocations  probability functions  and paintboxes.

arXiv preprint arXiv:1301.6647.

[12] Teh  Y. W.  Blundell  C.  & Elliott  L. T. (2011). Modelling genetic variations with fragmentation-

coagulation processes. In Advances in Neural Information Processing Systems 23.

[13] Orbanz  P. & Teh  Y. W. (2010). Bayesian Nonparametric Models. In Encyclopedia of Machine Learning.

Springer.

[14] Medvedovic  M. & Sivaganesan  S. (2002) Bayesian inﬁnite mixture model based clustering of gene expres-

sion proﬁles. Bioinformatics  18:1194–1206.

[15] Medvedovic  M.  Yeung  K. and Bumgarner  R. (2004) Bayesian mixture model based clustering of repli-

cated microarray data. Bioinformatics 20:1222–1232.

[16] Liu X.  Sivanagesan  S.  Yeung  K.Y.  Guo  J.  Bumgarner  R. E. and Medvedovic  M. (2006) Context-
speciﬁc inﬁnite mixtures for clustering gene expression proﬁles across diverse microarray dataset. Bioinfor-
matics  22:1737-1744.

[17] Shannon  C. E. (1948) A Mathematical Theory of Communication. Bell System Technical Journal

27(3):379–423.

[18] I. Nemenman  F. Shafee  & W. Bialek. (2002) Entropy and inference  revisited. In Advances in Neural

Information Processing Systems  14.

[19] Archer  E.  Park  I. M.  & Pillow  J. (2013) Bayesian Entropy Estimation for Countable Discrete Distribu-

tions. arXiv preprint arXiv:1302.0328.

[20] Simovici  D. (2007) On Generalized Entropy and Entropic Metrics. Journal of Multiple Valued Logic and

Soft Computing  13(4/6):295.

[21] Ellerman  D. (2009) Counting distinctions: on the conceptual foundations of Shannon’s information theory.

Synthese  168(1):119-149.

[22] Neal  R. M. (1992) Bayesian mixture modeling  in Maximum Entropy and Bayesian Methods: Proceedings
of the 11th International Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis 
Seattle  1991  eds  Smith  Erickson  & Neudorfer  Dordrecht: Kluwer Academic Publishers  197-211.

[23] Eisen  M. B.  Spellman  P. T.  Brown  P. O.  & Botstein  D. (1998) Cluster analysis and display of genome-

wide expression patterns. Proceedings of the National Academy of Sciences  95(25):14863-14868.

[24] Fisher  R. A. (1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics 

7(2):179-188.

[25] Ideker  T.  Thorsson  V.  Ranish  J. A.  Christmas  R.  Buhler  J.  Eng  J. K.  Bumgarner  R.  Goodlett  D. R. 
Aebersold  R. & Hood  L. (2001) Integrated genomic and proteomic analyses of a systematically perturbed
metabolic network. Science  292(5518):929-934.

[26] Pevehouse 

J. C.  Nordstrom  T. & Warnke  K.

nizations Dataset Version 2.0.
http://www.correlatesofwar.org/COW2%20Data/IGOs/IGOv2-1.htm

Conﬂict Management

(2004) The COW-2 International Orga-
and Peace Science 21(2):101-119.

9

,Isik Fidaner
Taylan Cemgil