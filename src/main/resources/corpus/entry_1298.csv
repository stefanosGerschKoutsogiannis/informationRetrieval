2013,Data-driven Distributionally Robust Polynomial Optimization,We consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions. This set is a ball around a density function estimated from data samples  i.e.  it is data-driven and random.  Polynomial optimization problems are inherently hard due to nonconvex objectives and constraints.  However  we show that by employing polynomial and histogram density estimates  we can introduce robustness with respect to distributional uncertainty sets without making the problem harder.  We show that the solution to the distributionally robust problem is the limit of a sequence of tractable semidefinite programming relaxations.  We also give finite-sample consistency guarantees for the data-driven uncertainty  sets.  Finally  we apply our model and solution method in a water network problem.,Data-driven Distributionally Robust Polynomial

Optimization

Martin Mevissen

IBM Research—Ireland

martmevi@ie.ibm.com

Emanuele Ragnoli

IBM Research—Ireland

eragnoli@ie.ibm.com

Jia Yuan Yu

IBM Research—Ireland

jy@osore.ca

Abstract

We consider robust optimization for polynomial optimization problems where the
uncertainty set is a set of candidate probability density functions. This set is a ball
around a density function estimated from data samples  i.e.  it is data-driven and
random. Polynomial optimization problems are inherently hard due to noncon-
vex objectives and constraints. However  we show that by employing polynomial
and histogram density estimates  we can introduce robustness with respect to dis-
tributional uncertainty sets without making the problem harder. We show that
the optimum to the distributionally robust problem is the limit of a sequence of
tractable semideﬁnite programming relaxations. We also give ﬁnite-sample con-
sistency guarantees for the data-driven uncertainty sets. Finally  we apply our
model and solution method in a water network optimization problem.

1

Introduction

For many optimization problems  the objective and constraint functions are not adequately modeled
by linear or convex functions (e.g.  physical phenomena such as ﬂuid or gas ﬂow  energy conser-
vation  etc.). Non-convex polynomial functions are needed to describe the model accurately. The
resulting polynomial optimization problems are hard in general. Another salient feature of real-
world problems is uncertainty in the parameters of the problem (e.g.  due to measurement errors 
fundamental principles  or incomplete information)  and the need for optimal solutions to be robust
against worst case realizations of the uncertainty. Robust optimization and polynomial optimiza-
tion are already an important topic in machine learning and operations research. In this paper  we
combine the polynomial and uncertain features and consider robust polynomial optimization.
We introduce a new notion of data-driven distributional robustness: the uncertain problem parame-
ter is a probability distribution from which samples can be observed. Consequently  it is natural to
take as the uncertainty set a set of functions  such as a norm ball around an estimated probability dis-
tribution. This approach gives solutions that are less conservative than classical robust optimization
with a set for the uncertain parameters. It is easy to see that the set uncertainty setting is an extreme
case of a distributional uncertainty set comprised of a set of Dirac densities. This stands in sharp
contrast with real-world problems where more information is at hand than the support of the distri-
bution of the parameters affected by uncertainty. Uncertain parameters may follow normal  Poisson 
or unknown nonparametric distributions. Such parameters arise in queueing theory  economics  etc.
We employ methods from both machine learning and optimization. First  we take care to estimate
the distribution of the uncertain parameter using polynomial basis functions. This ensures that the
resulting robust optimization problem can be reduced to a polynomial optimization problem. In turn 
we can then employ an iterative method of SDP relaxations to solve it. Using tools from machine
learning  we give a ﬁnite-sample consistency guarantee on the estimated uncertainty set. Using tools
from optimization  we give an asymptotic guarantee on the solutions of the SDP relaxations.

1

Section 2 presents the model of data-driven distributionally robust polynomial optimization—DRO
for short. Section 3 situates our work in the context of the literature. Our contributions are the
following. In Section 4  we consider the general case of uncertain multivariate distribution  which
yields a generalized problem of moments for the distributionally robust counterpart. In Section 5 
we introduce an efﬁcient histogram approximation for the case of uncertain univariate distributions 
which yields instead a polynomial optimization problem for the distributionally robust counterpart.
In Section 6  we present an application of our model and solution method in the domain of water
network optimization with real data.

2 Problem statement

Consider the following polynomial optimization problem

min
x∈X

h(x  ξ) 

(1)
where ξ ∈ Rn is an uncertain parameter of the problem. We allow h to be a polynomial in x ∈ Rm
and X to be a basic closed semialgebraic set. That is  even if ξ is ﬁxed  (1) is a hard problem in
general.
In this work  we are interested in distributionally robust optimization (DRO) problems that take the
form

(DRO) min
x∈X

max
f∈Dε N

Ef h(x  ξ) 

for all t 

(2)

where x is the decision variable  ξ is a random variable distributed according to an unknown prob-
ability density function f∗  which is the uncertain parameter in this setting. The expectation Ef is
with respect to a density function f  which belongs to an uncertainty set Dε N . This uncertainty set
itself is a set of possible probability density functions constructed from a given sequence of samples
ξ1  . . .   ξN distributed i.i.d. according to the unknown density function f∗ of the uncertain parameter
ξ. We call Dε N a distributional uncertainty set  it is a random set constructed as follows:

Dε N = {f : a prob. density s.t. (cid:107)f − (cid:98)fN(cid:107) (cid:54) ε} 

(3)

where ε > 0 is a given constant  (cid:107)·(cid:107) is a norm  and (cid:98)fN is an density function estimated from the

samples ξ1  . . .   ξN . We describe the construction of the distributional uncertainty set in the cases
of multivariate and univariate samples in Sections 4 and 5.
We say that a robust optimization problem is data-driven when the uncertainty set is an element of
a sequence of uncertainty sets Dε 1 ⊇ Dε 2 ⊇ . . .  where the index N represents the number of
samples of ξ observed by the decision-maker. This deﬁnition allows us to completely separate the
problem of robust optimization from that of constructing the appropriate uncertainty set Dε N . The
underlying assumption is that the uncertainty set (due to ﬁnite-sample estimation of the parameter
ξ) adapts continuously to the data as the sample size N increases. By considering data-driven
problems  we are essentially employing tools from statistical learning theory to derive consistency
guarantees.
Let R[x] denote the vector space of real-valued  multivariate polynomials  i.e.  every g ∈ R[x] is a
function g : Rm → R such that

(cid:88)

|α|(cid:54)d

(cid:88)

|α|(cid:54)d

g(x) =

gαxα =

gαxα1

1 . . . xαm

m   α ∈ Nm 

where {gα} is a set of real numbers. A polynomial optimization problem (POP) is given by

min
x∈K

q(x) 

(4)

where K = {x ∈ Rd | g1(x) (cid:62) 0  . . .   gm(x) (cid:62) 0}  q ∈ R[x]  and gj ∈ R[x] for j = 1  . . .   m.
One of our key results arises from the observation that the distributional robust counterpart of a
POP is a POP as well. A set K deﬁned by a ﬁnite number of multivariate polynomial inequality
constraints is called a basic closed semialgebraic set. As shown in [1]  if the basic closed semi-
algebraic set K compact and archimedian  there is a hierarchy of SDP relaxations whose minima

2

converge to the minimum of (4) for increasing order of the relaxation. Moreover  if (4) has an unique
minimal solution x(cid:63)  then the optimal solution y(cid:63)
τ of the τ-th order SDP relaxation converges to x(cid:63)
as τ → ∞.
Our work combines robust optimization with notions from statistical machine learning  such as den-
sity estimation and consistency. Our data-driven robust polynomial optimization method applies to
a number of machine learning problems. One example arises in Markov decision problems where a
high-dimensional value-function is approximated by a low-dimensional polynomial V . A distribu-
tionally robust variant of value iteration can be cast as:

(cid:88)

x(cid:48)∈X

max
a∈A

min
f∈Dε N

Ef{r(x  a  ξ) + γ

P (x(cid:48) | x  a  ξ)V (x(cid:48))} 

where ξ is a random parameter with unknown distribution and the uncertainty set Dε N of possible
distribution is constructed by estimation. We present next two further examples.
Example 2.1 (Distributionally robust ridge regression). We are given an i.i.d. sequence of
observation-label samples {(ξi  yi) ∈ Rn−1 × R : i = 1  . . .   N} from an unknown distribution
f∗  where each observation ξi has an associated label yi ∈ R. Ridge regression minimizes the
empirical residual with (cid:96)2-regularization and uses the samples to construct residual function. The
distributionally robust version of ridge regression is a conceptually different approach: it uses the
samples to construct a random uncertainty set Dε N to estimate the distribution f∗ and can be for-
mulated as

min
u∈Rn

max
f∈Dε N

Ef (yN +1 − ξN +1 · u)2 + λ(u · u) 

where Dε N is the uncertainty set of possible densities constructed from the N samples. Our solution
methods can even be applied to regression problems with nonconvex loss and penalty functions.
Example 2.2 (Robust investment). Optimization problems of the form of (2) arise in problems that
involve monetary measures of risk in ﬁnance [2]. For instance  the problem of robust investment in
a vector of (random) ﬁnancial positions ξ ∈ Rn is
−EQ

(cid:2)U (v · ξ)(cid:3) 

min
v∈∆n

sup
Q∈Q

where Q denotes a set of probability distributions  U is a utility function  and v · ξ is an allocation
among ﬁnancial positions. If U is polynomial  then the robust utility functional is a special case of
DRO.

3 Our contribution in context

To situate our work within the literature  it is important to note that we consider distributional
uncertainty sets and polynomial constraints and objectives. In this section  we outline related works
with different and similar uncertainty sets  constraints and objectives.
Robust optimization problems of the form of (2) have been studied in the literature with different
uncertain sets. In several works  the uncertainty sets are deﬁned in terms of moment constraints
[3  4  5]. Moment based uncertainty sets are motivated by the fact that probabilistic constraints can
be replaced by constraints on the ﬁrst and second moments in some cases [6].
In contrast  we do not consider moment constraints  but distributional uncertainty sets based on
probability density functions with the Lp-norm as the metric. One reason for our approach is that
higher moments are difﬁcult to estimate [7]. In contrast  probability density functions can be readily
estimated using a variety of data-driven methods  e.g.  empirical histograms  kernel-based [8  9]  and
orthogonal basis [10] estimates. Uncertainty sets deﬁned by distribution-based constraints appear
also in problems of risk measures [11]. For example uncertainty sets deﬁned using Kantorovich
distance are considered in [5  Section 4] and [11] while [5  Section 3] and [12] consider distributional
uncertainty with both measure bounds (of the form µ1 (cid:54) µ (cid:54) µ2) and moment constraints.
[13] considers distributional uncertainty sets with a φ−divergence metric. A notion of distributional
uncertainty set has also been studied in the setting of Markov decision problems [14]. However  in
those works  the uncertainty set is not data-driven.

3

Robust optimization formulations for polynomial optimization problems have been studied in [1  15]
with deterministic uncertainty sets (i.e.  neither distributional  nor data-driven). A contribution is to
show how to transform distributionally robust counterparts of polynomial optimization problems
into polynomial optimization problems. In order to solve these POP  we take advantage of the hier-
archy of SDP relaxations from [1]. Another contribution of this work is to use sampled information
to construct distributional uncertainty sets more suitable for problems where more and more data is
collected over time.

4 Multivariate uncertainty around polynomial density estimate

In this section  we construct a data-driven uncertainty set in the L2-space—with the uniform norm
(cid:107)·(cid:107)2. Furthermore we assume  the support of ξ is contained in some basic closed semialgebraic set
S := {z ∈ Rn | sj(z) (cid:62) 0  j = 1  . . .   r}  where sj ∈ R[z].
In order to construct a data-driven distributional uncertainty set  we need to estimate the density f∗
of the parameter ξ. Various density estimation approaches exist—e.g.  kernel-density and histogram
estimation. Some of these give rise to a computational problem due to the curse of dimension-
ality. However  to ensure that the resulting robust optimization problem remains an polynomial

optimization problem  we deﬁne the empirical density estimate (cid:98)fN as a multivariate polynomial (cf.

Section 2).
Let {πk} denote univariate Legendre polynomials:

πk(a) =

2k + 1

1

2

2kk!

dk

dak (a2 − 1)k 

a ∈ R  k = 0  1  . . .

Let α ∈ Nn  z ∈ Rn  and πα(z) = πα1(z1) . . . παn (zn) denote the multivariate Legendre
In this section  we employ the following Legendre series density estimator [10]:
polynomial.

(cid:98)fN (z) =(cid:80)|α|(cid:54)d

(cid:80)N

1
N

j=1 πα(ξj)πα(z).

In turn  we deﬁne the following uncertainty set:

Dd  N =

f ∈ R[z]d |

f (z) d z = 1 

(cid:90)

S

(cid:13)(cid:13)(cid:13)f − (cid:98)fN

(cid:13)(cid:13)(cid:13)2

(cid:27)

.

(cid:54) 

(cid:114)

(cid:26)

where R[z]d denotes the vector space of polynomials in R[z] of degree at most d. Observe that
the polynomials in Dd  N are not required to be non-negative on S. However  the non-negativity
constraint on S can be added at the expense of making the resulting DRO problem for a POP a
generalized problem of moments.

4.1 Solving the DRO

Next  we present asymptotic guarantees for solving distributionally robust polynomial optimization
through SDP relaxations. This result rests on the following assumptions  which are detailed in [1].
Assumption 4.1. The sets X = {x ∈ Rm | kj(z) (cid:62) 0  j = 1  . . .   t} and S = {z ∈ Rn | sj(z) (cid:62)
j=1 uj kj
j=0  and the level

0  j = 1  . . .   r} are compact. There exist u ∈ R[x] and v ∈ R[z] such that u = u0 +(cid:80)t
and v = v0 +(cid:80)r

j=1 vj sj for some sum-of-squares polynomials {uj}t

j=0  {vj}r

sets {x | u(x) (cid:62) 0} and {z | v(z) (cid:62) 0} compact.

Note that sets X and S satisfying Assumption 4.1 are called archimedian. This assumption is not
much more restrictive than compactness  e.g.  if S := {z ∈ Rn | sj(z) (cid:62) 0  j = 1  . . .   r} is
compact  then there exists a L2-ball of radius R that contains S. Thus  S = ˜S = {z ∈ Rn | sj(z) (cid:62)
(cid:54) R}. With Theorem 1 in [22] it follows that ˜S satisﬁes Assumption 4.1.

0  j = 1  . . .   r (cid:80)n
Theorem 4.1. Suppose that Assumption 4.1 holds. Let h ∈ R[x  z]  (cid:98)fN ∈ R[z]  and let X and S be

basic closed semialgebraic sets. Let V (cid:63) ∈ R denote the optimum of problem

i=1 z2
i

min
x∈X

max

f∈Dd ε N

h(x  z)f (z)dz.

(5)

(cid:90)

S

4

(i) Then  there exists a sequence of SDP relaxations SDPr such that min SDPr (cid:37) V (cid:63) for

r → ∞.
If (5) has a unique minimizer x(cid:63)  and mr the sequence of subvectors of optimal solutions
of SDPr associated with the ﬁrst order moments of monomials in x only. Then  mr → x(cid:63)
componentwise for r → ∞.

(ii)

All proofs appear in the appendix of the supplementary material.

4.2 Consistency of the uncertainty set

In this section  we show that the uncertainty set that we constructed is consistent. In other words 
given constants  and δ  we give number of samples N needed to ensure that the closest polynomial
to the unknown density f∗ belongs to the uncertainty set Dd ε N with probability 1 − δ.

Theorem 4.2 ([10  Section 3]). Let cα denote the coefﬁcients cα = (cid:82) παf∗ for all values of the
multi-index α. Suppose that the density function f∗ is square-integrable. We have E(cid:107)f∗ − (cid:98)fN(cid:107)2
(cid:80)

α)  where CH is a constant that depends only on f∗.

α:|α|(cid:54)d min(1/N  c2

CH

(cid:54)

2

As a corollary of Theorem 4.2  we obtain the following.
Corollary 4.3. Suppose that the assumptions of Theorem 4.2 hold. Let g∗
function g∗
such that

d denote the polynomial
α:|α|(cid:54)d cαxα. There exists a function1 Φ such that Φ(d) (cid:38) 0 as d → ∞ and

d(x) =(cid:80)

(cid:80)

P(g∗

d ∈ Dd ε N ) (cid:62) 1 − CH

for ε > Φ(d).

Remark 1. Observe that since(cid:80)

α:|α|(cid:54)d min(1/N  c2

α) + Φ2(d)

 

(ε − Φ(d))2

α) (cid:54)(cid:0)n+d

d

(cid:1)/N = (n + d)!/(N d! n!)  by an

α:|α|(cid:54)d min(1/N  c2

appropriate choice of N  it is possible to guarantee that the right-hand side tends to zero  even as
d → ∞.

5 Univariate uncertainty around histogram density estimate

In this section  we describe an additional layer of approximation for the univariate uncertainty set-
ting. In contrast to Section 4  by approximating the uncertainty set Dε N by a set of histogram
density functions  we reduce the DRO problem to a polynomial optimization problem of degree
identical with the original problem. Moreover  we derive ﬁnite-sample consistency guarantees. We
assume that samples ξ1  . . .   ξN are given for the uncertain parameter ξ  which takes values in a
given interval [A  B] ⊂ R. I.e.  in contrast to the previous section  we assume that the uncertain
parameter takes values in a bounded interval. We partition R into K-intervals u0  . . .   uK−1  such
that |uk| = |B − A| /K for all k = 0  . . .   K − 1. Let m0  . . .   mK−1 denote the midpoints of the

respective intervals. We deﬁne the empirical density vector(cid:98)pN K:

(cid:98)pN K(k) =

1
N

N(cid:88)

i=1

1[ξi∈uk]

for all k = 0  . . .   K − 1.

Recall that the L∞-norm of a function G : X → Rn is: (cid:107)G(cid:107)∞ = supx∈X |G(x)| . In this section 
we approximate the uncertainty set Dε N by a subset of the simplex in RK:

Wε N =(cid:8)p ∈ ∆K : (cid:107)p −(cid:98)pN K(cid:107)∞ (cid:54) ε(cid:9)  

where p = (p1  . . .   pK) denote a vector in RK. In turn  this will allow us to approximate the DRO
problem (2) by the following:

K−1(cid:88)

k=0

(ADRO) : min
x∈X

max
p∈Wε N

h (x  mk) pk.

(6)

1The function Φ(d) quantiﬁes the error due to estimation with in a basis of polynomials with ﬁnite degree

d.

5

5.1 Solving the DRO

The following result is an analogue of Theorem 4.1.
Theorem 5.1. Suppose that Assumption 4.1 holds. Let h ∈ R[x  z]  and let X be basic closed
semialgebraic2. Let W (cid:63) ∈ R denote the optimum of problem

K−1(cid:88)

k=0

min
x∈X

max
p∈Wε N

h (x  mk) pk.

(7)

(i) Then  there exists a sequence of SDP relaxations SDPr such that min SDPr (cid:37) W (cid:63) for

r → ∞.
If (7) has a unique minimizer x(cid:63)  let mr the sequence of subvectors of optimal solutions of
SDPr associated with the ﬁrst order moments of the monomials in x only. Then  mr → x(cid:63)
componentwise for r → ∞.

(ii)

5.2 Approximation error

Next  we bound the error of approximating Dε N with Wε N . This error depends only on the “de-
gree” K of the histogram approximation.
Theorem 5.2. Suppose that the support of ξ is the interval [A  B]. Suppose that |h(x  z)| (cid:54) H
for all x ∈ X and z ∈ [A  B]. Let ˜M (cid:44) sup{f(cid:48)(cid:48)(z) : f ∈ Dγ N   z ∈ [A  B]} be ﬁnite. Let
x(z) : f ∈ Dγ N   z ∈ [A  B]} be ﬁnite. For every
gx(z) (cid:44) h(x  z)f (z) and let M (cid:44) sup{g(cid:48)
γ (cid:54) Kε/(B − A) and density function f ∈ Dγ N   we have a density vector p ∈ Wε N such that

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

z∈[A B]

h(x  z) f (z)dz − K−1(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:54) (M + H ˜M )(B − A)3/(24K 2).

h (x  mk) pk

k=0

5.3 Consistency of the uncertainty set

Given ε and δ  we consider in this section the number of samples N that we need to ensure that
the unknown probability density is in the uncertainty set Dε N with probability 1 − δ. The consis-
tency guarantee for the univariate histogram uncertainty set follows as a corollary of the following
univariate Dvoretzky-Kiefer-Wolfowitz Inequality.

Theorem 5.3 ([16]). Let (cid:98)FN k denote the distribution function associated with the probabilities
(cid:98)pN K  and F ∗ the distribution function associated with the density function f∗. If F ∗ is continuous 
then P((cid:107)F ∗ − (cid:98)FN K(cid:107)∞ > ε) (cid:54) 2 exp(−2ε2/N ).

Corollary 5.4. Let p∗ denotes the histogram density vector of ξ induced by the true density f∗. As
N → ∞  we have P(p∗ ∈ Wε N ) (cid:62) 1 − 2 exp(−2ε2/N ).
Remark 2. Provided that the density f∗ is Lipchitz continuous  it follows that the optimal value of
(A1) converges to the optimal value without uncertainty as the size ε of the uncertainty set tend to
zero and the number of sample N tends to inﬁnity.

6 Application to water network optimization

In this section  we consider a problem of optimal operation of a water distribution network (WDN).
Let G = (V  E) denote a graph  i.e.  V is the set of nodes and E the set of pipes connecting the
nodes in a WDN. Let wi denote the pressure  ei the elevation  and ξi the demand at node i ∈ V   qi j
the ﬂow from i to j  and (cid:96)i j the loss caused by friction in case of ﬂow from i to j for (i  j) ∈ E.
Our objective is to minimize the overall pressure at selected critical points V1 ⊂ V in the WDN
by optimally setting a number of pressure reducing valves (PRVs) located on certain pipes in the
network while adhering to the conservations laws for ﬂow and pressure:

min

(w q)∈X

h(w  q  ξ)  where

(8)

2Since S is an interval  the assumption is trivially satisﬁed for S.

6

(cid:88)

i∈V1

(cid:88)

(cid:16)

ξj −(cid:88)

j∈V

k(cid:54)=j

(cid:17)2

 

qj l

(cid:88)

l(cid:54)=j

h(w  q  ξ) :=

wi + σ

qk j +

X := {(w  q) ∈ R|N|+2|E| | wmin (cid:54) wi (cid:54) wmax 
qmin (cid:54) qi j (cid:54) qmax 
qi j (wj + ej − wi − ei + (cid:96)i j(qi j)) (cid:54) 0 
wj + ej − wi − ei + (cid:96)i j(qi j) (cid:62) 0 

∀(i  j)}.

We assume that (cid:96)i j is a quadratic function in qi j. The PRV sets the pressure wi at the node i. The
derivation of (8) and a detailed description of the problem appear in [17]. Thus  h ∈ R[w  q  ξ] and
X is a basic closed semialgebraic set. For a ﬁxed vector of demands ξ = (ξ1  . . .   ξ|V |)  (8) falls
into the class (1). In real-world water networks  the demand ξ is uncertain. Given are ranges for the
possible realization of nodal demands  i.e.  the support of ξ is given by S := {˜z ∈ R|N| | zmin
(cid:54)
}. Moreover  we assume that samples ξ1  . . .   ξN of ξ are given and that they corresponds
˜zi (cid:54) zmax
to sensors measurements. Therefore  the distributionally robust counterpart of (8) is of the form of
ADRO (6).

i

i

Figure 1: (a) 25 node network with PRVs on pipes 1  5 and 11. (b) Scatter plot of demand at node
15 over four months overlaid over the 24 hours of a day.
We consider the benchmark WDN with |V | = 25 and |E| = 37 of [18]  which is illustrated in
Figure 1 (a). We assign demand values at the nodes of this WDN according to real data collected in
an anonymous major city. In our experiment we assume the demands at all nodes  except at node
15  are ﬁxed; for node 15 N = 120 samples of daily demands were collected over four months—the
dataset is shown in Figure 1 (b). Node 15 has been selected because it is one of the largest consumers
and has a demand proﬁle with the largest variation.
First  we consider the uncertainty set Wε N constructed from a histogram estimation with K = 5
  ¯ξ :=
bins. We consider  (a) the deterministic problem (8) with three values ξmin := mini ξ15
i
as the demand at node 15  (b) the distributionally robust
1
N
counterpart (A1) with  = 0.2 and σ = 1  and (c) the classical robust formulation of (8)
with an uncertainty range [ξmin  ξmax] without any distributional assumption  i.e.  the problem
min(w q)∈X maxξ15∈[ξmin  ξmax] h(w  q  ξ15) which is equivalent to

and ξmax := maxi ξ15
i

(cid:80)

i ξ15
i

max(cid:0)h(w  q  ξmin)   h(w  q  ξmax)(cid:1)

since(cid:0)ξ15 −(cid:80)

min

(w q)∈X

k(cid:54)=15 qk 15 +(cid:80)

l(cid:54)=15 q15 l

(cid:1)2 in (8) is convex quadratic in ξ15 attains its maximum at

(9)

the boundary of [ξmin  ξmax]. We solve (9) by solving the two polynomial optimization problems.
All three cases (a)–(c)  are polynomial optimization problems which we solve by ﬁrst applying the
sparse SDP relaxation of ﬁrst order [19] with SDPA [20] as the SDP solver  and then applying
IPOPT [21] with the SparsePOP solution as starting point. Computations on single blade server
with 100GB (total  80 GB free) of RAM and a processor speed of 3.5GHz. Total computation time
is denoted as tC.

7

12345679101112131415161718192021222324256885123479103130323334363528372726252420191213141516171829112321220510152025101520253035404550ξ15
ξmin
¯ξ
ξmax

tC
738
868
624

optimal setting
(15.0  15.7  15.9)
(15.0  15.5  15.6)
(15.0  15.4  15.5)

(cid:80)

46.7
46.1
45.9

i∈V1

wi

Table 1: Results for non-robust case (a).

Problem tC
DRO (b)
RO (c)

1315
1460

optimal setting
(15.0  15.5  15.7)
(15.0  16.9  17.3)

objective (cid:80) wi

6.62 × 105
1.54 × 106

46.2
49.2

Table 2: Results for DRO case (b) and classical robust case (c).

(cid:80)

i∈V1

The results for the deterministic case (a) show that the optimal setting and the overall pressure sum

wi differ even when the demand at only one node changes  as reported in Table 1.

Comparing the distributionally robust (b) and robust (c) optimal solution for the optimal PRV setting
problem  we observe  that the objective value of the distributionally robust counterpart is substan-
tially smaller than the robust one. Thus  the distributionally robust solution is less conservative than
the robust solution. Moreover  the distributionally robust setting is very close to the average case
deterministic solution ¯ξ - but it does not coincide. It seems to hedge the solution against the worst
case realization for the demand  given by the scenario ξ = ξmin  which results in the highest pressure
proﬁle. Moreover  note that solving the distributionally robust (and robust ) counterpart requires the
same order of magnitude in computational time as the deterministic problem. That may be due to the
fact that both the deterministic and the robust problems are hard polynomial optimization problems.

7 Discussion

We introduced a notion of distributional robustness for polynomial optimization problems. The
distributional uncertainty sets based on statistical estimates for the probability density functions
have the advantage that they are data-driven and consistent with the data for increasing sample-
size. Moreover  they give solutions that are less conservative than classical robust optimization
with valued-based uncertainty sets. We have shown that these distributional robust counterparts of
polynomial optimization problems remain in the same class problems from the perspective of com-
putational complexity. This methodology is promising for a numerous real-world decision problems 
where one faces the combined challenge of hard  non-convex models and uncertainty in the input
parameters.
We can extend the histogram method of Section 5 to the case of multivariate uncertainty  but it is
well-known that the sample-complexity of histogram density-estimation is greater than polynomial
density-estimation. An alternative deﬁnition of the distributional uncertainty set Dε N is to allow
functions that are not proper density functions by removing some constraints; this gives a trade-off
between reduced computational complexity and more conservative solutions.
The solution method of SDP relaxations comes without any ﬁnite-time guarantees. Although such
guarantees are hard to come by in general  an open problem is to identify special cases that give
insight into the rate of convergence of this method.

Acknowledgments

J. Y. Yu was supported in part by the EU FP7 project INSIGHT under grant 318225.

References
[1] J. B. Lasserre. A semideﬁnite programming approach to the generalized problem of moments.

Math. Programming  112:65–92  2008.

8

[2] A. Schied. Optimal investments for robust utility functionals in complete market models. Math.

Oper. Research  30(3):750–764  2005.

[3] E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with

applications to data-driven problems. Operations Research  2009.

[4] D. Bertsimas  X. V. Doan  K. Natarajan  and C.-P. Teo. Models for minimax stochastic linear

optimization problems with risk aversion. Math. Oper. Res.  35(3):580–602  2010.

[5] S. Mehrotra and H. Zhang. Models and algorithms for distributionally robust least squares

problems. Preprint  2011.

[6] D. Bertsimas and I. Popescu. Optimal inequalities in probability theory: a convex optimization

approach. SIAM J. Optimization  15:780–804  2000.

[7] P. R. Halmos. The theory of unbiased estimation. The Annals of Mathematical Statistics 

17(1):34–43  1946.

[8] B.W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall/CRC 

1998.

[9] L. Devroye and L. Györﬁ. Nonparametric Density Estimation. Wiley  1985.
[10] P. Hall. On the rate of convergence of orthogonal series density estimators. Journal of the

Royal Statistical Society. Series B  48(1):115–122  1986.

[11] G. Pﬂug and D. Wozabal. Ambiguity in portfolio selection. Quantitative Finance  7(4):435–

442  2007.

[12] A. Shapiro and S. Ahmed. On a class of minimax stochastic programs. SIAM J. Optim. 

14(4):1237–1249  2004.

[13] A. Ben-Tal  D. den Hertog  A. de Waegenaere  B. Melenerg  and G. Rennen. Robust solutions

of optimization problems affected by uncertain probabilities. Management Science  2012.

[14] H. Xu and S. Mannor. Distributionally robust markov decision processes. Mathematics of

Operations Research  37(2):288–300  2012.

[15] R. Laraki and J. B. Lasserre. Semideﬁnite programming for min-max problems and games.

Math. Programming A  131:305–332  2010.

[16] P. Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. Annals of Prob-

ability  18(3):1269–1283  1990.

[17] B. J. Eck and M. Mevissen. Valve placement in water networks. Technical report  IBM Re-

search  2012. Report No. RC25307 (IRE1209-014).

[18] A. Sterling and A. Bargiela. Leakage reduction by optimised control of valves in water net-

works. Transactions of the Institute of Measurement and Control  6(6):293–298  1984.

[19] H. Waki  S. Kim  M. Kojima  M. Muramatsu  and H. Sugimoto. SparsePOP: a sparse semidef-
inite programming relaxation of polynomial optimization problems. ACM Transactions on
Mathematical Software  35(2)  2008.

[20] M. Yamashita  K. Fujisawa  K. Nakata  M. Nakata  M. Fukuda  K. Kobayashi  and K. Goto.
A high-performance software package for semideﬁnite programs: SDPA 7. Technical report 
Tokyo Institute of Technology  2010.

[21] A. Waechter and L. T. Biegler. On the implementation of a primal-dual interior point ﬁlter
line search algorithm for large-scale nonlinear programming. Mathematical Programming 
106(1):25–57  2006.

[22] M. Schweighofer. Optimization of polynomials on compact semialgebraic sets. SIAM J. Opti-

mization  15:805–825  2005.

9

,Martin Mevissen
Emanuele Ragnoli
Jia Yuan Yu
Ferran Diego Andilla
Fred Hamprecht