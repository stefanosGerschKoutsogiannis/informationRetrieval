2012,Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data  by adopting a nonlinear and nonparametric model. In our model  the GP prior on the dynamics function enables representing the complex dynamic structure of speech  while the GP prior on the emission function successfully models the global dependency over the observations. Additionally  we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation  classification performance on the synthetic and benchmark datasets.,Phoneme Classiﬁcation using Constrained Variational

Gaussian Process Dynamical System

Hyunsin Park

Department of EE  KAIST

Daejeon  South Korea

hs.park@kaist.ac.kr

Sungrack Yun
Qualcomm Korea
Seoul  South Korea

sungrack@qualcomm.com

Sanghyuk Park

Department of EE  KAIST

Daejeon  South Korea

Jongmin Kim

Chang D. Yoo

shine0624@kaist.ac.kr

kimjm0309@gmail.com

cdyoo@ee.kaist.ac.kr

Department of EE  KAIST

Daejeon  South Korea

Department of EE  KAIST

Daejeon  South Korea

Abstract

For phoneme classiﬁcation  this paper describes an acoustic model based on the
variational Gaussian process dynamical system (VGPDS). The nonlinear and non-
parametric acoustic model is adopted to overcome the limitations of classical hid-
den Markov models (HMMs) in modeling speech. The Gaussian process prior
on the dynamics and emission functions respectively enable the complex dynamic
structure and long-range dependency of speech to be better represented than that
by an HMM. In addition  a variance constraint in the VGPDS is introduced to
eliminate the sparse approximation error in the kernel matrix. The effectiveness
of the proposed model is demonstrated with three experimental results  including
parameter estimation and classiﬁcation performance  on the synthetic and bench-
mark datasets.

1

Introduction

Automatic speech recognition (ASR)  the process of automatically translating spoken words into
text  has been an important research topic for several decades owing to its wide array of potential
applications in the area of human-computer interaction (HCI). The state-of-the-art ASR systems
typically use hidden Markov models (HMMs) [1] to model the sequential articulator structure of
speech signals. There are various issues to consider in designing a successful ASR and certainly
the following two limitations of an HMM need to be overcome. 1) An HMM with a ﬁrst-order
Markovian structure is suitable for capturing short-range dependency in observations and speech
requires a more ﬂexible model that can capture long-range dependency in speech. 2) Discrete latent
state variables and sudden state transitions in an HMM have limited capacity when used to represent
the continuous and complex dynamic structure of speech. These limitations must be considered
when seeking to improve the performance of an ASR.
To overcome these limitations  various models have been considered to model the complex structure
of speech. For example  the stochastic segment model [2] is a well-known generalization of the
HMM that represents long-range dependency over observations using a time-dependent emission
function. And the hidden dynamical model [3] is used for modeling the complex nonlinear dynamics
of a physiological articulator.
Another promising research direction is to consider a nonparametric Bayesian model for nonlinear
probabilistic modeling of speech. Owing to the fact that nonparametric models do not assume any

1

ﬁxed model structure  they are generally more ﬂexible than parametric models and can allow de-
pendency among observations naturally. The Gaussian process (GP) [4]  a stochastic process over
a real-valued function  has been a key ingredient in solving such problems as nonlinear regression
and classiﬁcation. As a standard supervised learning task using the GP  Gaussian process regression
(GPR) offers a nonparametric Bayesian framework to infer the nonlinear latent function relating the
input and the output data. Recently  researchers have begun focusing on applying the GP to un-
supervised learning tasks with high-dimensional data  such as the Gaussian process latent variable
model (GP-LVM) for reduction of dimensionality [5-6]. In [7]  a variational inference framework
was proposed for training the GP-LVM. The variational approach is one of the sparse approxima-
tion approaches [8]. The framework was extended to the variational Gaussian process dynamical
system (VGPDS) in [9] by augmenting latent dynamics for modeling high-dimensional time series
data. High-dimensional time series have been incorporated in many applications of machine learn-
ing such as robotics (sensor data)  computational biology (gene expression data)  computer vision
(video sequences)  and graphics (motion capture data). However  no previous work has considered
the GP-based approach for speech recognition tasks that involve high-dimensional time series data.
In this paper  we propose a GP-based acoustic model for phoneme classiﬁcation. The proposed
model is based on the assumption that the continuous dynamics and nonlinearity of the VGPDS can
be better represent the statistical characteristic of real speech than an HMM. The GP prior over the
emission function allows the model to represent long-range dependency over the observations of
speech  while the HMM does not. Furthermore  the GP prior over the dynamics function enables
the model to capture the nonlinear dynamics of a physiological articulator.
Our contributions are as follows: 1) we introduce a GP-based model for phoneme classiﬁcation tasks
for the ﬁrst time  showing that the model has the potential of describing the underlying character-
istics of speech in a nonparametric way; 2) we propose a prior for hyperparameters and a variance
constraint that are specially designed for ASR; and 3) we provide extensive experimental results and
analyses to reveal clearly the strength of our proposed model.
The remainder of the paper is structured as follows: Section 2 introduces the proposed model after a
brief description of the VGPDS. Section 3 provides extensive experimental evaluations to prove the
effectiveness of our model  and Section 4 concludes the paper with a discussion and plans for future
work.

2 Acoustic modeling using Gaussian Processes

2.1 Variational Gaussian Process Dynamical System

The VGPDS [9] models time series data by assuming that there exist latent states that govern the
data. Let Y = [[y11 ··· yN 1]T  ···   [y1D ··· yN D]T ] ∈ RN×D  t = [t1 ···   tN ]T ∈ RN
+   and
X = [[x11 ··· xN 1]T  ···   [x1Q ··· xN Q]T ] ∈ RN×Q be observed data  time  and corresponding
latent state  where N  D  and Q(< D) are the number of samples  the dimension of the observation
space  and the dimension of the latent space  respectively. In the VGPDS  these variables are related
as follows:

xnj = gj(tn) + ηnj 
yni = fi(xn) + ni 

ηnj ∼ N (0  1/βx
j ) 
ni ∼ N (0  1/βy
i ) 

p(Y|X)p(X|t)dX.

(2)

2

i (x)  kf

i (x  x(cid:48))) and gj(t) ∼ GP(µg

(1)
where fi(x) ∼ GP(µf
j (t  t(cid:48))) are the emission function
from the latent space to the i-th dimension of the observation space and the dynamics function
from the time space to the j-th dimension of the latent space  respectively. Here  n ∈ {1 ···   N} 
i ∈ {1 ···   D}  and j ∈ {1 ···   Q}. In this paper  a zero-mean function is used for all GPs. Fig.
1 shows graphical representations of HMM and VGPDS. Although the Gaussian process dynamical
model (GPDM) [10]  which involves an auto-regressive dynamics function  is also a GP-based model
for time-series  it is not considered in this paper.
The marginal likelihood of the VGPDS is given as

j (t)  kg

(cid:90)

p(Y|t) =

(cid:90)
(cid:90)

(cid:90)

D(cid:88)

D(cid:88)

Figure 1: Graphical representations of (left) the left-to-right HMM and (right) the VGPDS: In the
left ﬁgure  yn ∈ RD and xn ∈ {1 ···   C} are observations and discrete latent states. In the right
ﬁgure  yni  fni  xnj  gnj  and tn are observations  emission function points  latent states  dynamics
function points  and times  respectively. All function points in the same plate are fully connected.

Since the integral in Eq. (2) is not tractable  a variational method is used by introducing a variational
distribution q(X). A variational lower bound on the logarithm of the marginal likelihood is

log p(Y|t) ≥

q(X) log

p(Y|X)p(X|t)

q(X)

dX

(cid:90)

q(X) log p(Y|X)dX −
=
= L − KL(q(X)||p(X|t)).

q(X) log

q(X)
p(X|t)

dX

(3)
By the assumption of independence over the observation dimension  the ﬁrst term in Eq. (3) is given
as

L =

q(X) log p(yi|X)dX =

Li.

(4)

i=1

i=1

i Ψ2i + ˜Ki|1/2
i Ψ2i + ˜Ki)−1ΨT

In [9]  a variational approach which involves sparse approximation of the covariance matrix obtained
from GP is proposed. The variational lower bound on Li is given as
− βy
i
2

i )N/2| ˜Ki|1/2
(βy

(ψ0i − Tr( ˜K−1

Li ≥ log

i Ψ2i)) 

e(− 1

(2π)N/2|βy

2 yT

i Wiyi)

(cid:35)

(cid:34)

(5)

i )2Ψ1i(βy

i IN − (βy

j=1 p(xj) and q(X) =(cid:81)N

ond term of Eq. (3)  p(X|t) =(cid:81)Q

1i. Here  ˜Ki ∈ RM×M is a kernel matrix calcu-
where Wi = βy
lated using the i-th kernel function and inducing input variables ˜X ∈ RM×Q that are used for sparse
approximation of the full kernel matrix Ki. The closed-form of the statistics {ψ0i  Ψ1i  Ψ2i}D
(cid:81)Q
i=1 
which are functions of variational parameters and inducing points  can be found in [9]. In the sec-
j=1 N (µnj  snj) are the prior for
the latent state and the variational distribution that is used for approximating the posterior of the
latent state  respectively.
The parameter set Θ  which consists of the hyperparameters {θf   θg} of the kernel functions 
the noise variances {βy  βx}  the variational parameters {[µn1 ···   µnQ]  [sn1 ···   snQ]}N
n=1 of
q(X)  and the inducing input points ˜X  is estimated by maximizing the lower bound on log p(Y|t)
in Eq. (3) using a scaled conjugate gradient (SCG) algorithm.

n

2.2 Acoustic modeling using VGPDS

For several decades  HMM has been the predominant model for acoustic speech modeling. However 
as we mentioned in Section 1  the model suffers from two major limitations: discrete state variables
and ﬁrst-order Markovian structure which can model short-range dependency over the observations.

3

To overcome such limitations of the HMM  we propose an acoustic speech model based on the
VGPDS  which is a nonlinear and nonparametric model that can be used to represent the complex
dynamic structure of speech and long-range dependency over observations of speech. In addition 
to ﬁt the model to large-scale speech data  we describe various implementation issues.

2.2.1 Time scale modiﬁcation

The time length of each phoneme segment in an utterance varies with various conditions such as
position of the phoneme segment in the utterance  emotion  gender  and other speaker and environ-
ment conditions. To incorporate this fact into the proposed acoustic model  the time points tn are
modiﬁed as follows:

n − 1
N − 1

tn =

 

(6)

where n and N are the observation index and the number of observations in a phoneme segment 
respectively. This time scale modiﬁcation makes all phoneme signals have unit time length.

2.2.2 Hyperparameters

To compute the kernel matrices in Eq. (5)  the kernel function must be deﬁned. We use the radial
basis function (RBF) kernel for the emission function f as follows:

kf (x  x(cid:48)) = αf exp

j (xj − x(cid:48)
ωf

j)2

(7)

− Q(cid:88)

j=1

  

where αf and ωf
j are the RBF kernel variance and the j-th inverse length scale  respectively. The
RBF kernel function is adopted for representing smoothness of speech. For the dynamics function
g  the following kernel function is used:

kg(t  t(cid:48)) = αg exp(cid:0)−ωg(t − t(cid:48))2(cid:1) + λtt(cid:48) + b 

(8)
where λ and b are linear kernel variance and bias  respectively. The above dynamics kernel  which
consists of both linear and nonlinear components  is used for representing the complex dynamics of
the articulator. All hyperparameters are assumed to be independent in this paper.
In [11]  same kernel function parameters are shared over all dimensions of human-motion capture
data and high-dimensional raw video data. However  this extensive sharing of the hyperparame-
ters is unsuitable for speech modeling. Even though each dimension of observations is normal-
ized in advance to have unit variance  the signal-to-noise ratio (SNR) is not consistent over all
dimensions. To handle this problem  this paper considers each dimension to be modeled indepen-
dently using different kernel function parameters. Therefore  the hyperparameter sets are deﬁned as
θf = {αf

i=1 and θg = {αg

j   λj  bj}Q

i  {ωf

1i ···   ωf

Qi}}D

j   ωg

j=1.

2.2.3 Priors on the hyperparameters

In the parameter estimation of the VGPDS  the SCG algorithm does not guarantee the optimal solu-
tion. To overcome this problem  we place the following prior on the hyperparameters of the kernel
functions as given below

p(γ) ∝ exp(−γ2/¯γ) 

(9)
where γ ∈ {θf   θg} and ¯γ are the hyper-parameter and the model parameter of the prior  respec-
tively. In this paper  ¯γ is set to the sample variance for the hyperparameters of the emission kernel
functions  and ¯γ is set to 1 for the hyperparameters of the dynamics kernel functions. Uniform
priors are adopted for other hyperparameters  then the parameters of the VGPDS are estimated by
maximizing the joint distribution p(Y  Θ|t) = p(Y|t  Θ)p(Θ).

2.2.4 Variance constraint

In the lower bound of Eq. (5)  the second term on the right-hand side is the regularization term that
represents the sparse approximation error of the full kernel matrix Ki. Note that with more inducing

4

input points  approximation error becomes smaller. However  only a small number of inducing
input points can be used owing to the limited availability of computational power  which increases
the effect of the regularization term.
To mitigate this problem  we introduce the following constraint on the diagonal terms of the covari-
ance matrix as given below:

Tr((cid:104)Ki(cid:105)q(X))

N

+ 1/βy

i = σ2
i  

(10)

where (cid:104)Ki(cid:105)q(X) and σ2
i are the expectation of the full kernel matrix Ki and the sample variance of
the i-th dimension of the observation  respectively. This constraint is designed so that the variance
of each observation calculated from the estimated model is equal to the sample variance. By using
ψ0i = Tr((cid:104)Ki(cid:105)q(X))  the inverse noise variance parameter is obtained directly by βy
i −
i = (σ2
ψ0i/N )−1 without separate gradient-based optimization. Then  the partial derivative ∂ log βy
=
In Section 3.1  the effectiveness of the variance
N σ2−ψ0i
constraint is demonstrated empirically.

is used for SCG-based optimization.

1

i

∂ψ0i

2.3 Classiﬁcation
For classiﬁcation with trained VGPDSs  maximum-likelihood (ML) decoding is used. Let D(l) =
{Y(l)  t(l)} and Θ(l) be the observation and parameter sets of the l-th VGPDS  respectively. Given
the test data D∗ = {Y∗  t∗}  the classiﬁcation result ˆl ∈ {1 ···   L} can be obtained by

ˆl = arg max

l

= arg max

l

3 Experiments

log p(Y∗|t∗  Y(l)  t(l)  Θ(l))
p(Y(l)  Y∗|t(l)  t∗  Θ(l))

p(Y(l)|t(l)  Θ(l))

log

.

(11)

To evaluate the effectiveness of the proposed model  three different kinds of experiments have been
designed:

1. Parameter estimation: validating the effectiveness of the proposed variance constraint (Sec-

tion 2.2.4) on model parameter estimation

2. Two-class classiﬁcation using synthetic data: demonstrating explicitly the advantages of
the proposed model over the HMM with respect to the degree of dependency over the
observations

3. Phoneme classiﬁcation: evaluating the performance of the proposed model on real speech

data

Each experiment is described in detail in the following subsections. In this paper  the proposed
model is referred to as the constrained-VGPDS (CVGPDS).

3.1 Parameter estimation

In this subsection  the experiments of parameter estimation on synthetic data are described. Syn-
thetic data are generated by using a phoneme model that is selected from the trained models in
Section 3.3 and then modiﬁed. The RBF kernel variances of the emission functions and the emis-
sion noise variances are modiﬁed from the selected model. In this experiment  the emission noise
variances and inducing input points are estimated  while all other parameters are ﬁxed to the true
values used in generating the data.
Fig. 2 shows the parameter estimation results. The estimates of the 39-dimensional noise variance
of the emission functions are shown with the true noise variances  the true RBF kernel variances  and
the sample variances of the synthetic data. The top row denotes the estimation results without the
variance constraint  and the bottom row with the variance constraint. By comparing the two ﬁgures

5

Figure 2: Results of parameter estimation: (top-left) VGPDS with M = 5  (top-right) VGPDS with

M = 30  and (bottom) CVGPDS with M = 5

on the top row  we can conﬁrm that the estimation result of the noise variance with M = 30 inducing
input points is better than that with M = 5 inducing input points. This result is obvious in the sense
that smaller values of M produce more errors in the sparse approximation of the covariance matrix.
However  both noise variance estimates are still different from the true values. By comparing the
top and bottom rows  we can see that the proposed CVGPDS outperforms the VGPDS in terms of
parameter estimation. Remarkably  the estimation result of the CVGPDS with M = 5 inducing input
points is much better than the result of the VGPDS with M = 30. Based on these observations  we
can conclude that the proposed CVGPDS is considerably more robust to the sparse approximation
error compared to the VGPDS  as we claimed in Section 2.2.4.

3.2 Two-class classiﬁcation using synthetic data

This section aims to show that when there is strong dependency over the observations  the proposed
CVGPDS is a more appropriate model than the HMM for the classiﬁcation task. To this end  we
ﬁrst generated several sets of two-class classiﬁcation datasets with different degrees of dependency
over the observations. The considered classiﬁcation task is to map each input segment to one of two
class labels. Using s ∈ {1  ...  S} as the segment index  the synthetic dataset D = {Ys  ts  ls}S
consists of S segments  where the s-th segment has Ns samples. Here  Ys ∈ RNs×D  ts ∈ RNs 
s=1
and ls are the observation data  time  and class label of the s-th segment  respectively. The synthetic
dataset is generated as follows:

• Mean and kernel functions of two GPs gj(t) and fi(x) are deﬁned as

gj(t) : µg
fi(x) : µf

i (x) =(cid:80)Zi

j (t) = ajt + bj 

(12)
i }  and {αi  ωi} are respectively the parameters of the linear 
where {aj  bj}  {wz  mz
Gaussian mixture  and RBF kernel functions. The superscript z denotes the component
index of the Gaussian mixture  and Zi is the number of components in fi(x).

z=1 wzN (x; mz
i   Λz

j (t  t(cid:48)) = 1t=t(cid:48)
kg
i (x  x(cid:48)) = αi exp(−ωi||x − x(cid:48)||)
i )  kf

i Λz

6

• For the s-th segment  {Ys  ts  ls} 

1. ls is selected as either class 1 or 2.
2. Ns is randomly selected from interval [20  30]  and ts is obtained by using Eq. (6).
3. From ts  the mean vector µg

j are obtained for j =
1  ...  Q. Let Xs ∈ RNs×Q be the latent state of the s-th segment. Then  the j-th col-
umn of Xs is generated by the Ns-dimensional Gaussian distribution N (µg
j (ts)  Kg
j ).
i are obtained for i =
1  ...  D. Then  the i-th column of Ys is generated by the Ns-dimensional Gaussian
distribution  N (µf

i (Xs) and covariance matrix Kf

j (ts) and covariance matrix Kg

4. From Xs  the mean vector µf

i (Xs)  Kf

i ).

Note that parameter ωi controls the degree of dependency over the observations. For instance  if ωi
decreases  the off-diagonal terms of the emission kernel matrix Kf
i increase  which means stronger
correlations over the observations.
The experimental setups are as follows. The synthesized dataset consists of 200 segments in total
(100 segments per class). The dimensions of the latent space and observation space are set to Q = 2
and D = 5  respectively. We use 6(= Zi) components for the mean function of the emission kernel
function. In this experiment  three datasets are synthesized and used to compare the CVGPDS and
the HMM. When generating each dataset  we use two different ωi values  one for each class  while
all other parameters in Eq. (12) are shared between the two classes. As a result  the degree of
correlation between the observations is the only factor that distinguishes the two classes. The three
generated datasets have different degrees of correlation over the observations  as a result of setting
different ωi values for generating each dataset. In particular  the third dataset is constructed with two
limitations of HMM such that it is well represented by an HMM. This could be achieved simply by
j (t) from a linear to a step function  and setting ωi = ∞ so
changing the form of the mean function µg
that each data sample is generated independently of the others. In the third dataset  the two classes
are set to have different αi values. The classiﬁcation experiments are conducted using an HMM and
CVGPDS.

Table 1: Classiﬁcation accuracy for the two-class synthetic datasets (10-fold CV average [%]):

All parameters except ωi are set to be equal for classes 1 and 2.

In the case of ωi = ∞  αi are set to be different.

ωi (class 1 : class 2)

0.1 : 0.5

HMM

CVGPDS

61.0

78.0

1.0 : 2.0 ∞ : ∞
88.5
68.5

79.0

92.0

Table 1 summarizes the classiﬁcation performance of the HMM and CVGPDS for the three synthetic
datasets. Remarkably  in all cases  the proposed CVGPDS outperforms the HMM  even in the case
of ωi = ∞ (the fourth column)  where we assumed the dataset follows HMM-like characteristics.
Comparing the second and the third columns of Table 1  we can see that the performance of the
HMM degrades by 6.5% as ωi becomes smaller  while the proposed CVGPDS almost maintains
its performance with only 1.0% reduction. This result demonstrates the superiority of the proposed
CVGPDS in modeling data with strong correlations over the observations. Apparently  the HMM
failed to distinguish the two classes with different degree of dependency over the observations. In
contrast  the proposed CVGPDS distinguishes the two classes more effectively by capturing the
different degrees of inter-dependencies over the observations incorporated in each class.

3.3 Phoneme classiﬁcation

In this section  phoneme classiﬁcation experiments is described on real speech data from the TIMIT
database. The TIMIT database contains a total of 6300 phonetically rich utterances  each of which
is manually segmented based on 61 phoneme transcriptions. Following the standard regrouping of
phoneme labels [11]  61 phonemes are reduced to 48 phonemes selected for modeling. As observa-
tions  39-dimensional Mel-frequency cepstral coefﬁcients (MFCCs) (13 static coefﬁcients  ∆  and

7

∆∆) extracted from the speech signals with standard 25 ms frame size  and 10 ms frame shifts are
used. The dimension of the latent space is set to Q = 2.
For the ﬁrst phoneme classiﬁcation experiment  100 segments per phoneme are randomly selected
using the phoneme boundary provided information in the TIMIT database. The number of inducing
input points is set to M = 10. A 10-fold cross-validation test was conducted to evaluate the proposed
model in comparison with an HMM that has three states and a single Gaussian distribution with a
full covariance matrix per state. Parameters of the HMMs are estimated by using the conventional
expectation-maximization (EM) algorithm with a maximum likelihood criterion.

Table 2: Classiﬁcation accuracy on the 48-phoneme dataset (10-fold CV average [%]):

100 segments are used for training and testing each phoneme model

HMM VGPDS CVGPDS
49.19

49.36

48.17

Table 2 shows the experimental results of a 48-phoneme classiﬁcation. Compared to the HMM and
VGPDS  the proposed CVGPDS performs more effectively.
For the second phoneme classiﬁcation experiment  the TIMIT core test set consisting of 192 sen-
tences is used for evaluation. We use the same 100 segments for training the phoneme models as in
the ﬁrst phoneme classiﬁcation experiment. The size of the training dataset is smaller than that of
conventional approaches due to our limited computational ability. When evaluating the models  we
merge the labels of 48 phonemes into the commonly used 39 phonemes [11]. Given speech obser-
vations with boundary information  a sequence of log-likelihoods is obtained  and then a bigram is
constructed to incorporate linguistic information into the classiﬁcation score. In this experiment  the
number of inducing input points is set to M = 5.

Table 3: Classiﬁcation accuracy on the TIMIT core test set [%]:

100 segments are used for training each phoneme model

HMM VGPDS CVGPDS
57.83

61.54

61.44

Table 3 shows the experimental results of phoneme classiﬁcation for the TIMIT core test set. As
shown by the results in Table 2  the proposed CVGPDS performed better than the HMM and VG-
PDS. However  the classiﬁcation accuracies in Table 3 are lower than the state-of-the-art phoneme
classiﬁcation results [12-13]. The reasons for low accuracy are as follows: 1) insufﬁcient amount
of data is used for training the model owing to limited availability of computational power; 2) a
mixture model for the emission is not considered. These remaining issues need to be addressed for
improved performance.

4 Conclusion

In this paper  a VGPDS-based acoustic model for phoneme classiﬁcation was considered. The pro-
posed acoustic model can represent the nonlinear latent dynamics and dependency among observa-
tions by GP priors. In addition  we introduced a variance constraint on the VGPDS. Although the
proposed model could not achieve the state-of-the-art performance of phoneme classiﬁcation  the
experimental results showed that the proposed acoustic model has potential for speech modeling.
For future works  extension to phonetic recognition and mixture of the VGPDS will be considered.

Acknowledgments

This work was supported by the National Research Foundation of Korea(NRF) grant funded by the
Korea government(MEST) (No.2012-0005378 and No.2012-0000985)

8

References

[1] F. Jelinek  “Continuous speech recognition by statistical methods ” Proceedings of the IEEE  Vol.64  pp.532-
556  1976.
[2] M. Ostendorf  V. Digalakis  and J. Rohlicek  “From HMMs to segment models: A uniﬁed view of stochastic
modeling for speech recognition ” IEEE Trans. on Speech and Audio Processing  Vol.4  pp.360-378  1996.
[3] L. Deng  D. Yu  and A. Acero  “Structured Speech Modeling ” IEEE Trans. on Audio  Speech  and Lan-
guage Processing  Vol.14  pp.1492-1504  2006.
[4] C. E. Rasmussen and C. K. I. Williams  “Gaussian Process for Machine Learning ” MIT Press  Cambridge 
MA  2006.
[5] N. D. Lawrence  “Probabilistic non-linear principal component analysis with Gaussian process latent vari-
able models ” Journal of Machine Learning Research (JMLR)  Vol.6  pp.1783-1816  2005.
[6] N. D. Lawrence  “Learning for larger datasets with the Gaussian process latent variable model ” Interna-
tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pp.243-250  2007.
[7] M. K. Titsias and N. D. Lawrence  “Bayesian Gaussian Process Latent Variable Model ” International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pp.844-851  2010.
[8] J. Qui˜nonero-Candela and C. E. Rasmussen  “A Unifying View of Sparse Approximate Gaussian Process
Regression ” Journal of Machine Learning Research (JMLR)  Vol.6  pp.1939-1959  2005.
[9] A. C. Damianou  M. K. Titsias  and N. D. Lawrence  “Variational Gaussian Process Dynamical Systems ”
Advances in Neural Information Processing Systems (NIPS)  2011.
[10] J. M. Wang  D. J. Fleet  and A. Hertzmann  “Gaussian Process Dynamical Models for Human Motion ”
IEEE Trans. Pattern Analysis and Machine Intelligence  Vol.30  pp.283-298  2008.
[11] K. F. Lee and H. W. Hon  “Speaker-independent phone recognition using hidden Markov models ” IEEE
Trans. on Acoustics  Speech and Signal Processing  vol.37  pp.1641-1648  1989.
[12] A. Mohamed  G. Dahl  and G. Hinton  “Acoustic modeling using deep belief networks ” IEEE Trans. on
Audio  Speech  and Language Processing  Vol.20  no.1  pp. 14-22  2012.
[13] F. Sha and L. K. Saul  “Large margin hidden markov models for automatic speech recognition ” Advances
in Neural Information Processing Systems (NIPS)  2007.

9

,Sven Peter
Ferran Diego
Fred Hamprecht
Boaz Nadler