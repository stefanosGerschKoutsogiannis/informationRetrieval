2015,Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach,The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data  our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints  through a simple alternating projection algorithm. Compared with conventional approaches that estimate the similarity matrix based on the imputed data  our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the un-calibrated matrix (except in special cases they are identical). We carried out a series of empirical experiments and the results confirmed our theoretical justification. The evaluation also reported significantly improved results in real learning tasks on benchmarked datasets.,Estimating Jaccard Index with Missing Observations:

A Matrix Calibration Approach

Wenye Li

Macao Polytechnic Institute

Macao SAR  China

wyli@ipm.edu.mo

Abstract

The Jaccard index is a standard statistics for comparing the pairwise similarity be-
tween data samples. This paper investigates the problem of estimating a Jaccard
index matrix when there are missing observations in data samples. Starting from
a Jaccard index matrix approximated from the incomplete data  our method cali-
brates the matrix to meet the requirement of positive semi-deﬁniteness and other
constraints  through a simple alternating projection algorithm. Compared with
conventional approaches that estimate the similarity matrix based on the imputed
data  our method has a strong advantage in that the calibrated matrix is guaran-
teed to be closer to the unknown ground truth in the Frobenius norm than the
un-calibrated matrix (except in special cases they are identical). We carried out a
series of empirical experiments and the results conﬁrmed our theoretical justiﬁca-
tion. The evaluation also reported signiﬁcantly improved results in real learning
tasks on benchmark datasets.

1

Introduction

A critical task in data analysis is to determine how similar two data samples are. The applications
arise in many science and engineering disciplines. For example  in statistical and computing sci-
ences  similarity analysis lays a foundation for cluster analysis  pattern classiﬁcation  image analysis
and recommender systems [15  8  17].

A variety of similarity models have been established for different types of data. When data samples
can be represented as algebraic vectors  popular choices include cosine similarity model  linear
kernel model  and so on [24  25]. When each vector element takes a value of zero or one  the
Jaccard index model is routinely applied  which measures the similarity by the ratio of the number
of unique elements common to two samples against the total number of unique elements in either of
them [14  23].

Despite the wide applications  the Jaccard index model faces a non-trivial challenge when data
samples are not fully observed. As a treatment  imputation approaches may be applied  which
replace the missing observations with substituted values and then calculate the Jaccard index based
on the imputed data. Unfortunately  with a large portion of missing observations  imputing data
samples often becomes un-reliable or even infeasible  as evidenced in our evaluation.

Instead of trying to ﬁll in the missing values  this paper investigates a completely different approach
based on matrix calibration. Starting from an approximate Jaccard index matrix that is estimated
from incomplete samples  the proposed method calibrates the matrix to meet the requirement of
positive semi-deﬁniteness and other constraints. The calibration procedure is carried out with a
simple yet ﬂexible alternating projection algorithm.

1

The proposed method has a strong theoretical advantage. The calibrated matrix is guaranteed to be
better than  or at least identical to (in special cases)  the un-calibrated matrix in terms of a shorter
Frobenius distance to the true Jaccard index matrix  which was veriﬁed empirically as well. Be-
sides  our evaluation of the method also reported improved results in learning applications  and the
improvement was especially signiﬁcant with a high portion of missing values.
A note on notation. Throughout the discussion  a data sample  Ai (1 ≤ i ≤ n)  is treated as a set of
features. Let F = {f1  · · ·   fd} be the set of all possible features. Without causing ambiguity  Ai
also represents a binary-valued vector. If the j-th (1 ≤ j ≤ d) element of vector Ai is one  it means
fj ∈ Ai (feature fj belongs to sample Ai); if the element is zero  fj 6∈ Ai; if the element is marked
as missing  it remains unknown whether feature fj belongs to sample Ai or not.

2 Background

2.1 The Jaccard index

The Jaccard index is a commonly used statistical indicator for measuring the pairwise similarity
[14  23]. For two nonempty and ﬁnite sets Ai and Aj  it is deﬁned to be the ratio of the number of
elements in their intersection against the number of elements in their union:

J ∗
ij =

|Ai ∩ Aj|
|Ai ∪ Aj|

where |·| denotes the cardinality of a set.
The Jaccard index has a value of 0 when the two sets have no elements in common  1 when they have
exactly the same elements  and strictly between 0 and 1 otherwise. The two sets are more similar
(have more common elements) when the value gets closer to 1.
For n sets A1  · · ·   An (n ≥ 2)  the Jaccard index matrix is deﬁned as an n × n matrix J ∗ =

. The matrix is symmetric and all diagonal elements of the matrix are 1.

ij(cid:9)n
(cid:8)J ∗

i j=1

2.2 Handling missing observations

When data samples are fully observed  the accurate Jaccard index can be obtained trivially by enu-
merating the intersection and the union between each pair of samples if both the number of samples
and the number of features are small. For samples with a large number of features  the index can
often be approximated by MinHash and related methods [5  18]  which avoid the explicit counting
of the intersection and the union of the two sets.

When data samples are not fully observed  however  obtaining the accurate Jaccard index generally
becomes infeasible. One na¨ıve approximation is to ignore the features with missing values. Only
those features that have no missing values in all samples are used to calculate the Jaccard index.
Obviously  for a large dataset with missing-at-random features  it is very likely that this method will
throw away all features and therefore does not work at all.

The mainstream work tries to replace the missing observations with substituted values  and then
calculates the Jaccard index based on the imputed data. Several simple approaches  including zero 
median and k-nearest neighbors (kNN) methods  are popularly used. A missing element is set to
zero  often implying the corresponding feature does not exist in a sample. It can also be set to the
median value (or the mean value) of the feature over all samples  or sometimes over a number of
nearest neighboring instances.
A more systematical imputation framework is based on the classical expectation maximization (EM)
algorithm [6]  which generalizes maximum likelihood estimation to the case of incomplete data.
Assuming the existence of un-observed latent variables  the algorithm alternates between the ex-
pectation step and the maximization step  and ﬁnds maximum likelihood or maximum a posterior
estimates of the un-observed variables. In practice  the imputation is often carried out through it-
erating between learning a mixture of clusters of the ﬁlled data and re-ﬁlling missing values using
cluster means  weighted by the posterior probability that a cluster generates the samples [11].

2

3 Solution

Our work investigates the Jaccard index matrix estimation problem for incomplete data. Instead
of throwing away the un-observed features or imputing the missing values  a completely different
solution based on matrix calibration is designed.

3.1

Initial approximation

the set of features that are known to be in Ai  and denote by O−
i

the
For a sample Ai  denote by O+
i
i . If Oi = F   Ai is fully observed
set of features that are known to be not in Ai. Let Oi = O+
without missing values; otherwise  Ai is not fully observed with missing values. The complement
of Oi with respect to F   denoted by Oi  gives Ai’s unknown features and missing values.
For two samples Ai and Aj with missing values  we approximate their Jaccard index by:

i ∪ O−

J 0

ij = (cid:12)(cid:12)(cid:0)O+
(cid:12)(cid:12)(cid:0)O+

i ∩ Oj(cid:1) ∩(cid:0)O+
i ∩ Oj(cid:1) ∪(cid:0)O+

j ∩ Oi(cid:1)(cid:12)(cid:12)
j ∩ Oi(cid:1)(cid:12)(cid:12)

=

(cid:12)(cid:12)O+

i ∩ O+
j (cid:12)(cid:12)
i ∩ Oj(cid:1) ∪(cid:0)O+
j ∩ Oi(cid:1)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:0)O+

Here we assume that each sample has at least one observed feature. It is obvious that J 0
the ground truth J ∗

ij if the samples are fully observed.
There exists an interval [ℓij  µij] that the true value J ∗

ij lies in  where

ij is equal to

and

ℓij = 


1 
|O+
i
(cid:12)
O−
(cid:12)
i
(cid:12)

j |
∩O+
(cid:12)
∩O−
(cid:12)
j
(cid:12)

 

if i = j
otherwise

1 

(cid:12)
(cid:12)
(cid:12)

O−
i

∪O−
j
|Oi∪Oj ∪O+

i

(cid:12)
(cid:12)
(cid:12)
j |
∪O+

if i = j

 

otherwise

.

µij = 


The lower bound ℓij is obtained from the extreme case of setting the missing values in a way that the
two sets have the fewest features in their intersection while having the most features in their union.
On the contrary  the upper bound µij is obtained from the other extreme. When the samples are fully
observed  the interval shrinks to a single point ℓij = µij = J ∗
ij.

3.2 Matrix calibration

ij(cid:9)n
Denote by J ∗ = (cid:8)J ∗

ij=1

the true Jaccard index matrix for a set of data samples {A1  · · ·   An} 

we have [2]:
Theorem 1. For a given set of data samples  its Jaccard index matrix J ∗ is positive semi-deﬁnite.

ij(cid:9)n
For data samples with missing values  the matrix J 0 = (cid:8)J 0
often loses positive semi-
deﬁniteness. Nevertheless  it can be calibrated to ensure the property by seeking an n × n matrix
J = {Jij}n

ij=1 to minimize:

ij=1

subject to the constraints:

L0 (J) = (cid:13)(cid:13)J − J 0(cid:13)(cid:13)

2

F

J (cid:23) 0  and  ℓij ≤ Jij ≤ µij (1 ≤ i  j ≤ n)

where J (cid:23) 0 requires J to be positive semi-deﬁnite and k·kF denotes the Frobenius norm of a
matrix and kJk2

Let Mn be the set of n × n symmetric matrices. The feasible region deﬁned by the constraints 
denoted by R  is a nonempty closed and convex subset of Mn. Following standard results in op-
timization theory [20  3  10]  the problem of minimizing L0 (J) is convex. Denote by PR the
projection onto R. Its unique solution is given by the projection of J0 onto R: J 0

ij.
F = Pij J 2

R = PR(cid:0)J 0(cid:1).

For J 0

R  we have:

3

Theorem 2. (cid:13)(cid:13)J ∗ − J 0
R(cid:13)(cid:13)

F ≤ (cid:13)(cid:13)J ∗ − J 0(cid:13)(cid:13)

2

Proof. Deﬁne an inner product on Mn that induces the Frobenius norm:

2

F

. The equality holds iff J 0 ∈ R  i.e.  J 0 = J 0
R.

Then

2

F

hX  Y i = trace(cid:0)X T Y(cid:1)   for X  Y ∈ Mn.
(cid:13)(cid:13)J ∗ − J 0(cid:13)(cid:13)
R(cid:1)(cid:13)(cid:13)
= (cid:13)(cid:13)(cid:0)J ∗ − J 0
R(cid:1) −(cid:0)J 0 − J 0
R(cid:13)(cid:13)
R(cid:13)(cid:13)
F +(cid:13)(cid:13)J 0 − J 0
= (cid:13)(cid:13)J ∗ − J 0
R(cid:13)(cid:13)
≥ (cid:13)(cid:13)J ∗ − J 0
F − 2(cid:10)J ∗ − J 0
R(cid:13)(cid:13)
≥ (cid:13)(cid:13)J ∗ − J 0

F − 2(cid:10)J ∗ − J 0
R  J 0 − J 0
R(cid:11)

F
2

F

2

2

2

2

R  J 0 − J 0
R(cid:11)

The second “≥” holds due to the Kolmogrov’s criterion  which states that the projection of J 0 onto
R  J 0

R  is unique and characterized by:

The equality holds iff (cid:13)(cid:13)J 0 − J 0
R(cid:13)(cid:13)

J 0

R ∈ R  and(cid:10)J − J 0

R  J 0 − J 0
F = 0 and (cid:10)J ∗ − J 0

2

R(cid:11) ≤ 0 for all J ∈ R.
R  J 0 − J 0

R .
R(cid:11) = 0  i.e.  J 0 = J 0

This key observation shows that projecting J 0 onto the feasible region R will produce an improved
estimate towards J ∗  although this ground truth matrix remains unknown to us.

3.3 Projection onto subsets

Based on the results in Section 3.2  we are to seek a minimizer to L0 (J) to improve the estimate
J 0. Deﬁne two nonempty closed and convex subsets of Mn:

and

T = {X|X ∈ Mn  ℓij ≤ Xij ≤ µij (1 ≤ i  j ≤ n)} .

S = {X|X ∈ Mn  X (cid:23) 0}

Obviously R = S ∩ T . Now our minimization problem becomes ﬁnding the projection of J 0 onto
the intersection of two sets S and T with respect to the Frobenius norm. This can be done by
studying the projection onto the two sets individually. Denote by PS the projection onto S  and PT
the projection onto T . For projection onto T   a straightforward result based on the Kolmogrov’s
criterion is:
Theorem 3. For a given matrix X ∈ Mn  its projection onto T   XT = PT (X)  is given by

(XT )ij = 


Xij 
ℓij 
µij 

if ℓij ≤ Xij ≤ µij
if Xij < ℓij
if Xij > µij

.

For projection onto S  a well known result is the following [12  16  13]:
Theorem 4. For X ∈ Mn and its singular value decomposition X = U ΣV T where Σ =
diag (λ1  · · ·   λn)  the projection of X onto S is given by: XS = PS (X) = U Σ′V T where
Σ′ = diag (λ′

1  · · ·   λ′

n) and

λ′

i = (cid:26)λi 

0 

if λi ≥ 0
otherwise

.

The matrix XS = PS (X) gives the positive semi-deﬁnite matrix that most closely approximates X
with respect to the Frobenius norm.

4

3.4 Dykstra’s algorithm

To study the orthogonal projection onto the intersection of subspaces  a classical result is von Neu-
mann’s alternating projection algorithm. Let H be a Hilbert space with two closed subspaces C1
and C2. The orthogonal projection onto the intersection C1 ∩ C2 can be obtained by the product of
the two projections PC1PC2 when the two projections commute (PC1PC2 = PC2PC1). When they
do not commute  the work shows that for each x0 ∈ H  the projection of x0 onto the intersection
can be obtained by the limit point of a sequence of projections onto each subspace respectively:

limk→∞ (PC2PC1)k(cid:0)x0(cid:1) = PC1∩C2 (cid:0)x0(cid:1). The algorithm generalizes to any ﬁnite number of sub-

spaces and projections onto them.

Unfortunately  different from the application in [19]  in our problem both S and T are not subspaces
but subsets  and von Neumann’s convergence result does not apply. The limit point of the generated
sequence may converge to non-optimal points.

To handle the difﬁculty  Dykstra extended von Neumann’s work and proposed an algorithm that
works with subsets [9]. Consider the case of C = Tr
i=1 Ci where C is nonempty and each Ci is
a closed and convex subset in H. Assume that for any x ∈ H  obtaining PC (x) is hard  while
obtaining each PCi (x) is easy. Starting from x0 ∈ H  Dykstra’s algorithm produces two sequences 
the iterates (cid:8)xk

i (cid:9). The two sequences are generated by:

i(cid:9) and the increments (cid:8)I k

r

xk
0 = xk−1
xk
i = PCi (cid:0)xk
i −(cid:0)xk
I k
i = xk

i−1 − I k−1
(cid:1)
i−1 − I k−1

i

i

(cid:1)

where i = 1  · · ·   r and k = 1  2  · · · . The initial values are given by x0

r = x0  I 0

i = 0.

i(cid:9) converges to the optimal solution with a theoretical guarantee [9  10].

The sequence of (cid:8)xk
Theorem 5. Let C1  · · ·   Cr be closed and convex subsets of a Hilbert space H such that C =
r
Ck 6= Φ. For any i = 1  · · ·   r and any x0 ∈ H  the sequence (cid:8)xk
i(cid:9) converges strongly to
Tk=1
C = PC (cid:0)x0(cid:1) (i.e. (cid:13)(cid:13)xk

C(cid:13)(cid:13) → 0 as k → ∞).

The convergent rate of Dykstra’s algorithm for polyhedral sets is linear [7]  which coincides with
the convergence rate of von Neumann’s alternating projection method.

i − x0

x0

3.5 An iterative method

Based on the discussion in Section 3.4  we have a simple approach  shown in Algorithm 1  that ﬁnds
the projection of an initial matrix J 0 onto the nonempty set R = S ∩ T . Here the projections onto
S and T are given by the two theorems in Section 3.3. The algorithm stops when J k falls into the
feasible region or when a maximal number of iterations is achieved. For practical implementation 
a more robust stopping criterion can be adopted [1].

3.6 Related work

It is a known study in mathematical optimization ﬁeld to ﬁnd a positive semi-deﬁnite matrix that
is closest to a given matrix. A number of methods have been proposed recently. The idea of alter-
nating projection method was ﬁrstly applied in a ﬁnancial application [13]. The problem can also
be phrased as a semi-deﬁnite programming (SDP) model [13] and be solved via the interior-point
method. In the work of [21] and [4]  the quasi-Newton method and the projected gradient method
to the Lagrangian dual of the original problem were applied  which reported faster results than the
SDP formulation. An even faster Newton’s method was developed in [22] by investigating the dual
problem  which is unconstrained with a twice continuously differentiable objective function and has
a quadratically convergent solution.

5

Algorithm 1 Projection onto R = S ∩ T
Require: Initial matrix J 0

k = 0
J 0
T = J 0
I 0
S = 0
I 0
T = 0
while NOT CONVERGENT do

J k+1
T − I k
S = PS (cid:0)J k
S(cid:1)
S = J k+1
I k+1
S −(cid:0)J k
T − I k
S(cid:1)
J k+1
T = PT (cid:0)J k+1
S − I k
T(cid:1)
I k+1
T = J k+1
T −(cid:0)J k+1
S − I k
T(cid:1)
k = k + 1

end while
return J k = J k
T

4 Evaluation

To evaluate the performance of the proposed method  four benchmark datasets were used in our
experiments.

• MNIST: a grayscale image database of handwritten digits (“0” to “9”). After binarization 

each image is represented as a 784-dimensional 0-1 vector.

• USPS: another grayscale image database of handwritten digits. After binarization  each

image is represented as a 256-dimensional 0-1 vector.

• PROTEIN: a bioinformatics database with three classes of instances. Each instance is rep-

resented as a sparse 357-dimensional 0-1 vector.

• WEBSPAM: a dataset with both spam and non-spam web pages. Each page is represented
as a 0-1 vector. The data are highly sparse. On average one vector has about 4  000 non-zero
values out of more than 16 million features.

Our experiments have two objectives. One is to verify the effectiveness of the proposed method in
estimating the Jaccard index matrix by measuring the derivation of the calibrated matrix from the
ground truth in Frobenius norm. The other is to evaluate the performance of the calibrated matrix in
general learning applications. The comparison is made against the popular imputation approaches
listed in Section 2.2  including the zero  kNN and EM 1 approaches. (As the median approach gave
very similar performance as the zero approach  its results were not reported separately.)

4.1

Jaccard index matrix estimation

The experiment was carried out under various settings. For each dataset  we experimented with
1  000 and 10  000 samples respectively. For each sample  different portions (from 10% to 90%)
of feature values were marked as missing  which was assumed to be “missing at random” and all
features had the same probability of being marked.

As mentioned in Section 3  for the proposed calibration approach  an initial Jaccard index matrix
was ﬁrstly built based on the incomplete data. Then the matrix was calibrated to meet the positive
semi-deﬁnite requirement and the lower and upper bounds requirement. While for the imputation
approaches  the Jaccard index matrix was calculated directly from the imputed data.
Note that for the kNN approach  we iterated different k from 1 to 5 and the best result was collected 
which actually overestimated its performance. Under some settings  the results of the EM approach
were not available due to its prohibitive computational requirement to our platform.

The results are presented through the comparison of mean square deviations from the ground truth
of the Jaccard index matrix J ∗. For an n × n estimated matrix J ′  its mean square deviation from

1ftp://ftp.cs.toronto.edu/pub/zoubin/old/EMcode.tar.Z

6

Mean Square Deviation (1 000 Samples)

(cid:46)(cid:70)(cid:66)(cid:79)(cid:1)(cid:52)(cid:82)(cid:86)(cid:66)(cid:83)(cid:70)(cid:1)(cid:37)(cid:70)(cid:87)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79) (1 000 Samples)

Mean Square Deviation (1 000 Samples)

(cid:46)(cid:70)(cid:66)(cid:79)(cid:1)(cid:52)(cid:82)(cid:86)(cid:66)(cid:83)(cid:70)(cid:1)(cid:37)(cid:70)(cid:87)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79) (1 000 Samples)

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

n
o

i
t

−2

10

ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

(cid:81)
(cid:82)

(cid:76)
(cid:87)

−2

10

ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

n
o

i
t

−2

10

ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

−2

10

l

)
e
a
c
s
−
g
o
l
(
 

(cid:81)
(cid:82)

(cid:76)
(cid:87)

−3

10

ZERO/MEDIAN
kNN

NO_CALIBRATION
CALIBRATION

i

 

a
v
e
D
e
r
a
u
q
S
n
a
e
M

 

−3

10

−4

10

(cid:76)

(cid:3)

(cid:68)
(cid:89)
(cid:72)
(cid:39)
(cid:72)
(cid:85)
(cid:68)
(cid:88)
(cid:84)
(cid:54)
(cid:81)
(cid:68)
(cid:72)
(cid:48)

(cid:3)

−3

10

−4

10

i

 

a
v
e
D
e
r
a
u
q
S
n
a
e
M

 

−3

10

−4

10

(cid:76)

(cid:3)

(cid:68)
(cid:89)
(cid:72)
(cid:39)
(cid:72)
(cid:85)
(cid:68)
(cid:88)
(cid:84)
(cid:54)
(cid:81)
(cid:68)
(cid:72)
(cid:48)

(cid:3)

−4

10

−5

10

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

Ratio of Observed Features

Ratio of Observed Features

Ratio of Observed Features

(a) MNIST

(b) USPS

(c) PROTEIN

(d) WEBSPAM

Mean Square Deviation (10 000 Samples)

(cid:46)(cid:70)(cid:66)(cid:79)(cid:1)(cid:52)(cid:82)(cid:86)(cid:66)(cid:83)(cid:70)(cid:1)(cid:37)(cid:70)(cid:87)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79) (10 000 Samples)

Mean Square Deviation (10 000 Samples)

(cid:46)(cid:70)(cid:66)(cid:79)(cid:1)(cid:52)(cid:82)(cid:86)(cid:66)(cid:83)(cid:70)(cid:1)(cid:37)(cid:70)(cid:87)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79) (10 000 Samples)

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

n
o

i
t

−2

10

ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

(cid:81)
(cid:82)

(cid:76)
(cid:87)

−2

10

ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

−1

10

l

)
e
a
c
s
−
g
o
l
(
 

n
o

i
t

−2

10

ZERO/MEDIAN
kNN

NO_CALIBRATION
CALIBRATION

−2

10

l

)
e
a
c
s
−
g
o
l
(
 

(cid:81)
(cid:82)

(cid:76)
(cid:87)

−3

10

ZERO/MEDIAN
kNN

NO_CALIBRATION
CALIBRATION

i

 

a
v
e
D
e
r
a
u
q
S
n
a
e
M

 

−3

10

−4

10

(cid:76)

(cid:3)

(cid:68)
(cid:89)
(cid:72)
(cid:39)
(cid:72)
(cid:85)
(cid:68)
(cid:88)
(cid:84)
(cid:54)
(cid:81)
(cid:68)
(cid:72)
(cid:48)

(cid:3)

−3

10

−4

10

i

 

a
v
e
D
e
r
a
u
q
S
n
a
e
M

 

−3

10

−4

10

(cid:76)

(cid:3)

(cid:68)
(cid:89)
(cid:72)
(cid:39)
(cid:72)
(cid:85)
(cid:68)
(cid:88)
(cid:84)
(cid:54)
(cid:81)
(cid:68)
(cid:72)
(cid:48)

(cid:3)

−4

10

−5

10

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(e) MNIST

Ratio of Observed Features

(f) USPS

Ratio of Observed Features

Ratio of Observed Features

(g) PROTEIN

(h) WEBSPAM

Figure 1: Mean square deviations from the ground truth on benchmark datasets by different methods.
Horizontal: percentages of observed values (from 10% to 90%); Vertical: mean square deviations
in log-scale. (a)-(d): 1  000 samples; (e)-(f): 10  000 samples. (For better visualization effect of the
results shown in color  the reader is referred to the soft copy of this paper.)

J ∗ is deﬁned as the square Frobenius distance between the two matrices  divided by the number of
elements  i.e.  Pn
. In addition to the comparison with the popular approaches  the mean
square deviation between the un-calibrated matrix J 0 and J ∗  shown as NO CALIBRATION  is
also reported as a baseline.

ij=1(J ′
ij
n2

ij)2

−J ∗

Figure 1 shows the results. It can be seen that the calibrated matrices reported the smallest derivation
from the ground truth in nearly all experiments. The improvement is especially signiﬁcant when the
ratio of observed features is low (the missing ratio is high). It is guaranteed to be no worse than the
un-calibrated matrix. As evidenced in the results  for all the imputation approaches  there is no such
a guarantee.

4.2 Supervised learning

Knowing the improved results in reducing the deviation from the ground truth matrix  we would like
to further investigate whether this improvement indeed beneﬁts practical applications  speciﬁcally
in supervised learning.

We applied the calibrated results in nearest neighbor classiﬁcation tasks. Given a training set of
labeled samples  we tried to predict the labels of the samples in the testing set. For each testing
sample  its label was determined by the label of the sample in the training set that had the largest
Jaccard index value with it.

Similarly the experiment was carried out with 1  000/10  000 samples and different portions of miss-
ing values from 10% to 90% respectively. In each run  90% of the samples were randomly chosen as
the training set and the remaining 10% were used as the testing set. The mean and standard deviation
of the classiﬁcation errors in 1  000 runs were reported. As a reference  the results from the ground
truth matrix J ∗  shown as FULLY OBSERVED  were also included.
Figure 2 shows the results. Again the matrix calibration method reported evidently improved results
over the imputation approaches in most experiments. The improvement veriﬁed the beneﬁts brought
by the reduced deviation from the true Jaccard index matrix  and therefore justiﬁed the usefulness
of the proposed method in learning applications.

7

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Classification Error (1 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(a) MNIST

Classification Error (10 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(e) MNIST

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

Classification Error (1 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(b) USPS

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

Classification Error (1 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(c) PROTEIN

Classification Error (10 000 Samples)

Classification Error (10 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(f) USPS

0.65

0.6

0.55

0.5

0.45

0.4

0

FULLY_OBSERVED
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION

r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(g) PROTEIN

0.11

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

Classification Error (1 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(d) WEBSPAM

Classification Error (10 000 Samples)

FULLY_OBSERVED
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Ratio of Observed Features

(h) WEBSPAM

Figure 2: Classiﬁcation errors on benchmark datasets by different methods. Horizontal: percentage
of observed values (from 10% to 90%); Vertical: classiﬁcation errors. (a)-(d): 1  000 samples; (e)-
(f): 10  000 samples. (For better visualization effect of the results shown in color  the reader is
referred to the soft copy of this paper.)

5 Discussion and conclusion

The Jaccard index measures the pairwise similarity between data samples  which is routinely used
in real applications. Unfortunately in practice  it is non-trivial to estimate the Jaccard index matrix
for incomplete data samples. This paper investigates the problem  and proposes a matrix calibration
approach in a way that is completely different from the existing methods.
Instead of throwing
away the unknown features or imputing the missing values  the proposed approach calibrates any
approximate Jaccard index matrix by ensuring the positive semi-deﬁnite requirement on the matrix.
It is theoretically shown and empirically veriﬁed that the approach indeed brings about improvement
in practical problems.

One point that is not particularly addressed in this paper is the computational complexity issue. We
adopted a simple alternating projection procedure based on Dykstra’s algorithm. The computational
complexity of the algorithm heavily depends on the successive matrix decompositions. It is ex-
pensive when the size of the matrix becomes large. Calibrating a Jaccard index matrix for 1  000
samples can be ﬁnished in seconds of time on our platform  while calibrating a matrix for 10  000
samples quickly increases to more than an hour. Further investigations for faster solutions are thus
necessary for scalability.

Actually  there is a simple divide-and-conquer heuristic to calibrate a large matrix. Firstly divide
the matrix into small sub-matrices. Then calibrate each sub-matrix to meet the constraints. Finally
merge the results. Although the heuristic may not give the optimal result  it also guarantees to
produce a matrix better than or identical to the un-calibrated matrix. The heuristic runs with high
parallel efﬁciency and easily scales to very large matrices. The detailed discussion is omitted here
due to the space limit.

Acknowledgments

The work is supported by The Science and Technology Development Fund (Project No.
006/2014/A)  Macao SAR  China.

8

References

[1] E.G. Birgin and M. Raydan. Robust stopping criteria for Dykstra’s algorithm. SIAM Journal on Scientiﬁc

Computing  26(4):1405–1414  2005.

[2] M. Bouchard  A.L. Jousselme  and P.E. Dor´e. A proof for the positive deﬁniteness of the Jaccard index

matrix. International Journal of Approximate Reasoning  54(5):615–626  2013.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  New York  NY  USA 

2004.

[4] S. Boyd and L. Xiao. Least-squares covariance matrix adjustment. SIAM Journal on Matrix Analysis and

Applications  27(2):532–546  2005.

[5] A.Z. Broder  M. Charikar  A.M. Frieze  and M. Mitzenmacher. Min-wise independent permutations. In
Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing  pages 327–336. ACM 
1998.

[6] A.P. Dempster  N.M. Laird  and D.B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society  Series B  39:1–38  1977.

[7] F. Deutsch. Best Approximation in Inner Product Spaces. Springer  New York  NY  USA  2001.
[8] R.O. Duda and P.E. Hart. Pattern Classiﬁcation. John Wiley and Sons  Hoboken  NJ  USA  2000.
[9] R.L. Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical

Association  78(384):837–842  1983.

[10] R. Escalante and M. Raydan. Alternating Projection Methods. SIAM  Philadelphia  PA  USA  2011.
[11] Z. Ghahramani and M.I. Jordan. Supervised learning from incomplete data via an EM approach.

In
Advances in Neural Information Processing Systems  volume 6  pages 120–127. Morgan Kaufmann  1994.
[12] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press  Baltimore  MD 

USA  1996.

[13] N.J. Higham. Computing the nearest correlation matrix - a problem from ﬁnance. IMA Journal of Nu-

merical Analysis  22:329–343  2002.

[14] P. Jaccard. The distribution of the ﬂora in the alpine zone. New Phytologist  11(2):37–50  1912.
[15] A.K. Jain  M.N. Murty  and P.J. Flynn. Data clustering: A review. ACM Computing Surveys  31(3):264–

323  1999.

[16] D.L. Knol and J.M.F. ten Berge. Least-squares approximation of an improper correlation matrix by a

proper one. Psychometrika  54(1):53–61  1989.

[17] J. Leskovec  A. Rajaraman  and J. Ullman. Mining of Massive Datasets. Cambridge University Press 

New York  NY  USA  2014.

[18] P. Li and A.C. K¨onig. Theory and applications of b-bit minwise hashing. Communications of the ACM 

54(8):101–109  2011.

[19] W. Li  K.H. Lee  and K.S. Leung. Large-scale RLSC learning without agony. In Proceedings of the 24th

International Conference on Machine Learning  pages 529–536. ACM  2007.

[20] D.G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons  New York  NY  USA 

1969.

[21] J. Malick. A dual approach to semideﬁnite least-squares problems. SIAM Journal on Matrix Analysis and

Applications  26(1):272–284  2004.

[22] H. Qi and D. Sun. A quadratically convergent newton method for computing the nearest correlation

matrix. SIAM Journal on Matrix Analysis and Applications  28(2):360–385  2006.

[23] D.J. Rogers and T.T. Tanimoto. A computer program for classifying plants. Science  132(3434):1115–

1118  1960.

[24] G. Salton  A. Wong  and C.S. Yang. A vector space model for automatic indexing. Communications of

the ACM  18(11):613–620  1975.

[25] B. Scholk¨opf and A.J. Smola. Learning With Kernels  Support Vector Machines  Regularization  Opti-

mization  and Beyond. The MIT Press  Cambridge  MA  USA  2001.

9

,Wenye Li
Ayan Chakrabarti