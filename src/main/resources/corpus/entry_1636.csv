2016,Dual Decomposed Learning with Factorwise Oracle for Structural SVM of Large Output Domain,Many applications of machine learning involve structured output with large domain  where learning of structured predictor is prohibitive due to repetitive calls to expensive inference oracle. In this work  we show that  by decomposing training of Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages  one can replace expensive structured oracle with Factorwise Maximization Oracle (FMO) that allows efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is proposed to exploit sparsity of messages which guarantees $\epsilon$ sub-optimality after $O(log(1/\epsilon))$ passes of FMO calls. We conduct experiments on chain-structured problems and fully-connected problems of large output domains. The proposed approach is orders-of-magnitude faster than the state-of-the-art training algorithms for Structural SVM.,Dual Decomposed Learning with Factorwise Oracles

for Structural SVMs of Large Output Domain

Ian E.H. Yen † Xiangru Huang ‡ Kai Zhong ‡ Ruohan Zhang ‡

Inderjit S. Dhillon ‡
‡ University of Texas at Austin

Pradeep Ravikumar †

† Carnegie Mellon University

Abstract

Many applications of machine learning involve structured outputs with large do-
mains  where learning of a structured predictor is prohibitive due to repetitive
calls to an expensive inference oracle. In this work  we show that by decomposing
training of a Structural Support Vector Machine (SVM) into a series of multiclass
SVM problems connected through messages  one can replace an expensive struc-
tured oracle with Factorwise Maximization Oracles (FMOs) that allow efﬁcient
implementation of complexity sublinear to the factor domain. A Greedy Direction
Method of Multiplier (GDMM) algorithm is then proposed to exploit the sparsity
of messages while guarantees convergence to  sub-optimality after O(log(1/))
passes of FMOs over every factor. We conduct experiments on chain-structured
and fully-connected problems of large output domains  where the proposed ap-
proach is orders-of-magnitude faster than current state-of-the-art algorithms for
training Structural SVMs.

1

Introduction

Structured prediction has become prevalent with wide applications in Natural Language Process-
ing (NLP)  Computer Vision  and Bioinformatics to name a few  where one is interested in outputs
of strong interdependence. Although many dependency structures yield intractable inference prob-
lems  approximation techniques such as convex relaxations with theoretical guarantees [10  14  7]
have been developed. However  solving the relaxed problems (LP  QP  SDP  etc.) is computationally
expensive for factor graphs of large output domain and results in prohibitive training time when em-
bedded into a learning algorithm relying on inference oracles [9  6]. For instance  many applications
in NLP such as Machine Translation [3]  Speech Recognition [21]  and Semantic Parsing [1] have
output domains as large as the size of vocabulary  for which the prediction of even a single sentence
takes considerable time.
One approach to avoid inference during training is by introducing a loss function conditioned on
the given labels of neighboring output variables [15]. However  it also introduces more variance
to the estimation of model and could degrade testing performance signiﬁcantly. Another thread of
research aims to formulate parameter learning and output inference as a joint optimization problem
that avoids treating inference as a subroutine [12  11]. In this appraoch  the structured hinge loss
is reformulated via dual decomposition  so both messages between factors and model parameters
are treated as ﬁrst-class variables. The new formulation  however  does not yield computational ad-
vantage due to the constraints entangling the two types of variables. In particular  [11] employs a
hybrid method (DLPW) that alternatingly optimizes model parameters and messages  but the algo-
rithm is not signiﬁcantly faster than directly performing stochastic gradient on the structured hinge
loss. More recently  [12] proposes an approximate objective for structural SVMs that leads to an
algorithm considerably faster than DLPW on problems requiring expensive inference. However  the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: (left) Factors with large output domains in Sequence Labeling. (right) Large number of
factors in a Correlated Multilabel Prediction problem. Circles denote variables and black boxes
denote factors. (Yu: domain of unigram factor. Yb: domain of bigram factor.)

approximate objective requires a trade-off between efﬁciency and approximation quality  yielding
an O(1/2) overall iteration complexity for achieving  sub-optimality.
The contribution of this work is twofold. First  we propose a Greedy Direction Method of Multiplier
(GDMM) algorithm that decomposes the training of a structural SVM into factorwise multiclass
SVMs connected through sparse messages conﬁned to the active labels. The algorithm guarantees
an O(log(1/)) iteration complexity for achieving an  sub-optimality and each iteration requires
only one pass of Factorwise Maximization Oracles (FMOs) over every factor. Second  we show that
the FMO can be realized in time sublinear to the cardinality of factor domains  hence is consider-
ably more efﬁcient than a structured maximization oracle when it comes to large output domain.
For problems consisting of numerous binary variables  we further give realization of a joint FMO
that has complexity sublinear to the number of factors. We conduct experiments on both chain-
structured problems that allow exact inference and fully-connected problems that rely on Linear
Program relaxations  where we show the proposed approach is orders-of-magnitude faster than cur-
rent state-of-the-art training algorithms for Structured SVMs.

2 Problem Formulation
Structured prediction aims to predict a set of outputs y ∈ Y(x) from their interdependency
and inputs x ∈ X . Given a feature map φ(x  y) : X × Y(x) → Rd that extracts rele-
vant information from (x  y)  a linear classiﬁer with parameters w can be deﬁned as h(x; w) =
arg maxy∈Y(x) (cid:104)w  φ(x  y)(cid:105)  where we estimate the parameters w from a training set D =
{(xi  ¯yi)}n

i=1 by solving a regularized Empirical Risk Minimization (ERM) problem

min
w

(cid:107)w(cid:107)2 + C

1
2

L(w; xi  ¯yi) .

n(cid:88)

i=1

In case of a Structural SVM [19  20]  we consider the structured hinge loss

L(w; x  ¯y) = max
y∈Y(x)

(cid:104)w  φ(x  y) − φ(x  ¯y)(cid:105) + δ(y  ¯y) 

(1)

(2)

(3)

where δ(y  ¯yi) is a task-dependent error function  for which the Hamming distance δH (y  ¯yi) is
commonly used. Since the size of domain |Y(x)| typically grows exponentially with the num-
ber of output variables  the tractability of problem (1) lies in the decomposition of the responses
(cid:104)w  φ(x  y)(cid:105) into several factors  each involving only a few outputs. The factor decomposition can
be represented as a bipartite graph G(F V E) between factors F and variables V  where an edge
(f  j) ∈ E exists if the factor f involves the variable j. Typically  a set of factor templates T exists
so that factors of the same template F ∈ T share the same feature map φF (.) and parameter vector
wF . Then the response on input-output pair (x  y) is given by

(cid:104)w  φ(x  y)(cid:105) =

(cid:104)wF   φF (xf   yf )(cid:105) 

(cid:88)

(cid:88)

F∈T

f∈F (x)

where F (x) denotes the set of factors on x that share a template F   and yf denotes output variables
relevant to factor f of domain Yf = YF . We will use F(x) to denote the union of factors of
different templates {F (x)}F∈T . Figure 1 shows two examples that both have two factor templates

2

(i.e. unigram and bigram) for which the responses have decomposition(cid:80)
(cid:80)
f∈u(x)(cid:104)wu  φu(xf   yf )(cid:105)+
f∈b(x)(cid:104)wb  φb(yf )(cid:105). Unfortunately  even with such decomposition  the maximization in (2) is
still computationally expensive. First  most of graph structures do not allow exact maximization 
so in practice one would minimize an upper bound of the original loss (2) obtained from relaxation
[10  18]. Second  even for the relaxed loss or a tree-structured graph that allows polynomial-time
maximization  its complexity is at least linear to the cardinality of factor domain |Yf| times the
number of factors |F|. This results in a prohibitive computational cost for problems with large output
domain. As in Figure 1  one example has a factor domain |Yb| which grows quadratically with the
size of output domain; the other has the number of factors |F| which grows quadratically with the
number of outputs. A key observation of this paper is  in contrast to the structural maximization
(2) that requires larger extent of exploration on locally suboptimal assignments in order to achieve
global optimality  the Factorwise Maximization Oracle (FMO)

y∗
f := argmax

(cid:104)wF   φ(xf   yf )(cid:105)

yf

(4)

can be realized in a more efﬁcient way by maintaining data structures on the factor parameters wF .
In the next section  we develop globally-convergent algorithms that rely only on FMO  and provide
realizations of message-augmented FMO with cost sublinear to the size of factor domain or to the
number of factors.

3 Dual-Decomposed Learning

(5)

(cid:11)

yf∈Yf

f∈F (x)

(q p)∈ML

ML :=

We consider an upper bound of the loss (2) based on a Linear Program (LP) relaxation that is tight
in case of a tree-structured graph and leads to a tractable approximation for general factor graphs
[11  18]:

(cid:10)θf (w)  qf
where θf (w) :=(cid:0)(cid:10)wF   φF (xf   yf ) − φF (xf   ¯yf )(cid:11) + δf (yf   ¯yf )(cid:1)

. ML is a polytope that

LLP (w; x  ¯y) = max

(cid:27)
constrains qf in a |Yf|-dimensional simplex ∆|Yf| and also enforces local consistency:

(cid:26) q = (qf )f∈F (x)

∀f ∈ F (x) ∀F ∈ T

(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12) qf ∈ ∆|Yf| 

Mjf qf = pj  ∀(j  f ) ∈ E(x)

where Mjf is a |Yj| by |Yf| matrix that has Mjf (yj  yf ) = 1 if yj is consistent with yf (i.e.
yj = [yf ]j) and Mjf (yj  yf ) = 0 otherwise. For a tree-structured graph G(F V E)  the LP
relaxation is tight and thus loss (5) is equivalent to (2). For a general factor graph  (5) is an upper
bound on the original loss (2). It is observed that parameters w learned from the upper bound (5)
tend to tightening the LP relaxation and thus in practice lead to tight LP in the testing phase [10].
Instead of solving LP (5) as a subroutine  a recent attempt formulates (1) as a problem that optimizes
(p  q) and w jointly via dual decomposition [11  12]. We denote λjf as dual variables associated
with constraint Mjf qf = pj  and λf := (λjf )j∈N (f ) where N (f ) = {j | (j  f ) ∈ E}. We have
LLP (w; x  ¯y) = max
q p

(cid:104)λjf   Mjf qf − pj(cid:105)

(cid:88)

(cid:88)

p = (pj)j∈V(x)

min

(6)

 

λ

(cid:88)
(cid:88)

= min
λ∈Λ

f∈F (x)

= min
λ∈Λ

|Yf |

max
qf∈∆

 max

yf∈Yf

f∈F (x)

(θf (w) +

j∈N (f )
jf λjf )T qf

(cid:104)θf (w)  qf(cid:105) +
(cid:88)
(cid:88)

j∈N (f )

M T

θf (yf ; w) +

λjf ([yf ]j)

Lf (w; xf   ¯yf   λf ) (8)

f∈F (x)

f∈F (x)
(j f )∈E(x) λjf = 0 ∀j ∈ V(x)
where (7) follows the strong duality  and the domain Λ =
follows the maximization w.r.t. p in (6). The result (8) is a loss function Lf (.) that penalizes the
response of each factor separately given λf . The ERM problem (1) can then be expressed as

j∈N (f )

λ

min
w λ∈Λ

(cid:107)wF(cid:107)2 + C

Lf (wF ; xf   ¯yf   λf )

 1

2

(cid:88)

F∈T

(cid:88)

f∈F

3

(cid:88)

 = min
(cid:12)(cid:12)(cid:12)(cid:80)
(cid:110)

λ∈Λ

  

(7)

(cid:111)

(9)

(cid:107)wF (α)(cid:107)2 −(cid:88)

δT
j αj

min
αf∈∆

|Yf |

s.t.

G(α) :=
j∈V
Mjf αf = αj  j ∈ N (f )  f ∈ F.
wF (α) =

ΦT

f αf

1
2

(cid:88)
(cid:88)

F∈T

f∈F

where αf lie in the shifted simplex

(cid:26)

∆|Yf| :=

αf

(cid:12)(cid:12) αf (¯yf ) ≤ C   αf (yf ) ≤ 0  ∀yf (cid:54)= ¯yf  

(10)

(11)

(cid:27)

αf (yf ) = 0.

.

(cid:88)

yf∈Yf

Algorithm 1 Greedy Direction Method of Multiplier

0. Initialize t = 0  α0 = 0  λ0 = 0 and A0 = Ainit.
for t = 0  1  ... do

1. Compute (αt+1 At+1) via one pass of Algorithm 2  3  or 4.
  j ∈ N (f )  ∀f ∈ F.
2. λt+1

f − αt+1

jf = λt

Mjf αt+1

jf + η

j

(cid:16)

(cid:17)

end for

where F = (cid:83)N

i=1 F (xi) and F = (cid:83)

F∈T F . The formulation (9) has an insightful interpretation:
each factor template F learns a multiclass SVM given by parameters wF from factors f ∈ F   while
each factor is augmented with messages λf passed from all variables related to f.
Despite the insightful interpretation  formulation (9) does not yield computational advantage di-
rectly. In particular  the non-smooth loss Lf (.) entangles parameters w and messages λ  which
leads to a difﬁcult optimization problem. Previous attempts to solve (9) either have slow conver-
gence [11] or rely on an approximation objective [12]. In the next section  we propose a Greedy
Direction Method of Multiplier (GDMM) algorithm for solving (9)  which achieves  sub-optimality
in O(log(1/)) iterations while requiring only one pass of FMOs for each iteration.

3.1 Greedy Direction Method of Multiplier
Let αf (yf ) be dual variables for the factor responses zf (yf ) = (cid:104)w  φ(xf   yf )(cid:105) and {αj}j∈V be
that for constraints in Λ. The dual problem of (9) can be expressed as 1

Problem (10) can be interpreted as a summation of the dual objectives of |T | multiclass SVMs
(each per factor template)  connected with consistency constraints. To minimize (10) one factor at a
time  we adopt a Greedy Direction Method of Multiplier (GDMM) algorithm that alternates between
minimizing the Augmented Lagrangian function

(cid:88)

(cid:13)(cid:13)mjf (α  λt)(cid:13)(cid:13)2 − (cid:107)λt

jf(cid:107)2

(12)

min
αf∈∆

|Yf |

L(α  λt) := G(α) +

ρ
2

j∈N (f )  f∈F

and updating the Lagrangian Multipliers (of consistency constraints)

jf = λt
λt+1

jf + η (Mjf αf − αj) . ∀j ∈ N (f )  f ∈ F 

(13)
jf plays the role of messages between |T | multiclass
where mjf (α  λt) = Mjf αf − αj + λt
problems  and η is a constant step size. The procedure is outlined in Algorithm 1. The minimization
(12) is conducted in an approximate and greedy fashion  in the aim of involving as few dual variables
as possible. We discuss two greedy algorithms that suit two different cases in the following.
Factor of Large Domain For problems with large factor domains  we minimize (12) via a variant
of Frank-Wolfe algorithm with away steps (AFW) [8]  outlined in Algorithm 2. The AFW algorithm
maintains the iterate αt as a linear combination of bases constructed during iterates

αt =

vv  At := {v | ct
ct

v (cid:54)= 0}

(14)

(cid:88)

v∈At

1αj is also dual variables for responses on unigram factors. We deﬁne U := V and αf := αj  ∀f ∈ U.

4

Algorithm 2 Away-step Frank-Wolfe (AFW)

Algorithm 3 Block-Greedy Coordinate Descent

repeat

1. Find a greedy direction v+ satisfying (15).
2. Find an away direction v− satisfying (16).
3. Compute αt+1 according to (17).
4. Maintain active set At by (14).
5. Maintain wF (α) according to (10).

until a non-drop step is performed.

for i ∈ [n] do

i ∪ {f∗}.

1. Find f∗ satisfying (18) for i-th sample.
i = As
2. As+1
for f ∈ Ai do
3.1 Update αf according to (19).
3.2 Maintain wF (α) according to (10).

end for

end for

v− := argmax
v∈At

(cid:26) αt + γF dF  

where At maintains an active set of bases of non-zero coefﬁcients. Each iteration of AFW ﬁnds
a direction v+ := (v+
f )f∈F leading to the most descent amount according to the current gradient 
subject to the simplex constraints:
v+
f := argmin
|Yf |

(cid:104)∇αfL(αt  λt)  vf(cid:105) = C(e¯yf − ey∗

)  ∀f ∈ F

(15)

f

vf∈∆

where y∗
f of highest response. In addition  AFW ﬁnds the away direction

f := arg maxyf∈Yf\{¯yf} (cid:104)∇αfL(αt  λt)  eyf(cid:105) is the non-ground-truth labeling of factor

(cid:104)∇αL(αt  λt)  v(cid:105) 

(16)

which corresponds to the basis that leads to the most descent amount when being removed. Then
the update is determined by

(cid:104)∇αL  dF(cid:105) < (cid:104)∇αL  dA(cid:105)
otherwise.

αt+1 :=

αt + γAdA 

(17)
where we choose between two descent directions dF := v+− αt and dA := αt− v−. The step size
of each direction γF := arg minγ∈[0 1] L(αt + γdF ) and γA := arg minγ∈[0 cv− ] L(αt + γdA)
can be computed exactly due to the quadratic nature of (12). A step is called drop step if a step size
γ∗ = cv− is chosen  which leads to the removal of a basis v− from the active set  and therefore
the total number of drop steps can be bounded by half of the number of iterations t. Since a drop
step could lead to insufﬁcient descent  Algorithm 2 stops only if a non-drop step is performed. Note
Algorithm 2 requires only a factorwise greedy search (15) instead of a structural maximization (2).
In section 3.2 we show how the factorwise search can be implemented much more efﬁciently than
structural ones. All the other steps (2-5) in Algorithm 2 can be computed in O(|Af|nnz(φf )) 
where |Af| is the number of active states in factor f  which can be much smaller than |Yf| when
output domain is large.
In practice  a Block-Coordinate Frank-Wolfe (BCFW) method has much faster convergence than
Frank-Wolfe method (Algorithm 2) [13  9]  but proving linear convergence for BCFW is also much
more difﬁcult [13]  which prohibits its use in our analysis. In our implementation  however  we adopt
the BCFW version since it turns out to be much more efﬁcient. We include a detailed description on
the BCFW version in Appendix-A (Algorithm 4).

Large Number of Factors Many structured prediction problems  such as alignment  segmenta-
tion  and multilabel prediction (Fig. 1  right)  comprise binary variables and large number of factors
with small domains  for which Algorithm 2 does not yield any computational advantage. For this
type of problem  we minimize (12) via one pass of Block-Greedy Coordinate Descent (BGCD) (Al-
gorithm 3) instead. Let Qmax be an upper bound on the eigenvalue of Hessian matrix of each block
L(α). For binary variables of pairwise factor  we have Qmax=4(maxf∈F (cid:107)φf(cid:107)2 + 1). Each
∇2
iteration of BGCD ﬁnds a factor that leads to the most progress

αf

f∗ := argmin
f∈F (xi)

(18)
for each instance xi  adds them into the set of active factors Ai  and performs updates by solving
block subproblems

min
αf +d∈∆

|Yf |

2

.

(cid:104)∇αfL(αt  λt)  d(cid:105) +

Qmax

(cid:107)d(cid:107)2

(cid:18)

(cid:19)

∗
d
f = argmin

αf +d∈∆

|Yf |

(cid:104)∇αfL(αt  λt)  d(cid:105) +

Qmax

2

(cid:107)d(cid:107)2

(19)

5

for each factor f ∈ Ai. Note |Ai| is bounded by the number of GDMM iterations and it converges
to a constant much smaller than |F(xi)| in practice. We address in the next section how a joint FMO
can be performed to compute (18) in time sublinear to |F(xi)| in the binary-variable case.

3.2 Greedy Search via Factorwise Maximization Oracle (FMO)

The main difference between the FMO and structural maximization oracle (2) is that the former
involves only simple operations such as inner products or table look-ups for which one can easily
come up with data structures or approximation schemes to lower the complexity. In this section 
we present two approaches to realize sublinear-time FMOs for two types of factors widely used in
practice. We will describe in terms of pairwise factors  but the approach can be naturally generalized
to factors involving more variables.

Indicator Factor Factors θf (xf   yf ) of the form

(cid:104)wF   φF (xf   yf )(cid:105) = v(xf   yf )

(20)

are widely used in practice. It subsumes the bigram factor v(yi  yj) that is prevalent in sequence 
grid  and network labeling problems  and also factors that map an input-output pair (x  y) directly
to a score v(x  y). For this type of factor  one can maintain ordered multimaps for each factor
template F   which support ordered visits of {v(x  (yi  yj))}(yi yj )∈Yf   {v(x  (yi  yj))}yj∈Yj and
{v(x  (yi  yj))}yi∈Yi. Then to ﬁnd yf that maximizes (26)  we compare the maximizers in 4 cases:
(i) (yi  yj) : mif (yi) = mjf (yj) = 0  (ii) (yi  yj) : mif (yi) = 0  (iii) (yi  yj) : mjf (yj) = 0 
(iv) (yi  yj) : mjf (yj) (cid:54)= 0  mif (yi) (cid:54)= 0. The maximization requires O(|Ai||Aj|) in cases (ii)-(iv)
and O(max(|Ai||Yj| |Yi||Aj|)) in case (i) (see details in Appendix C-1). However  in practice we
observe an O(1) cost for case (i) and the bottleneck is actually case (iv)  which requires O(|Ai||Aj|).
Note the ordered multimaps need maintenance whenever the vector wF (α) is changed. Fortunately 
f∈F xf =x αf (yf )  each update (25) leads to at most
|Af| changed elements  which gives a maintenance cost bounded by O(|Af| log(|YF|)). On the
other hand  the space complexity is bounded by O(|YF||XF|) since the map is shared among factors.

since the indicator factor has v(yf   x) = (cid:80)

Binary-Variable Interaction Factor Many problems consider pairwise-interaction factor be-
tween binary variables  where the factor domain is small but the number of factors is large. For
f ∈ YF . We call factors exhibiting such
this type of problem  there is typically an rare outcome yA
outcome as active factors and the score of a labeling is determined by the score of the active factors
(inactive factors give score 0). For example  in the problem of multilabel prediction with pairwise
interactions (Fig. 1  right)  an active unigram factor has outcome yA
j = 1 and an active bigram
factor has yA
For this type of problem  we show that the gradient magnitude w.r.t. αf for a bigram factor f can be
determined by the gradient w.r.t. αf (yA
f ) when one of its incoming message mjf or mif is 0 (see
details in Appendix C-2). Therefore  we can ﬁnd the greedy factor (18) by maintaining an ordered
f   xf )}f∈F . The resulting complexity
f in each factor {v(yA
multimap for the scores of outcome yA
for ﬁnding a factor that maximizes (18) is then reduced from O(|Yi||Yj|) to O(|Ai||Aj|)  where the
latter is for comparison among factors that have both messages mif and mjf being non-zero.

f = (1  1)  and each sample typically has only few outputs with value 1.

Inner-Product Factor We consider another widely-used type of factor of the form

θf (xf   yf ) = (cid:104)wF   φF (xf   yf )(cid:105) = (cid:104)wF (yf )  φF (xf )(cid:105)

where all labels yf ∈ Yf share the same feature mapping φF (xf ) but with different parameters
wF (yf ). We propose a simple sampling approximation method with a performance guarantee for
the convergence of GDMM. Note although one can apply similar sampling schemes to the structural
maximization oracle (2)  it is hard to guarantee the quality of approximation. The sampling method
  and realizes an approximate FMO

divides Yf into ν mutually exclusive subsets Yf =(cid:83)ν

by ﬁrst sampling k uniformly from [ν] and returning

k=1 Y (k)

f

ˆyf ∈ arg max
yf∈Y (k)

f

(cid:104)wF (yf )  φF (xf )(cid:105).

(21)

6

f

Note there is at least 1/ν probability that ˆyf ∈ arg maxyf∈Yf (cid:104)wF (yf )  φF (xf )(cid:105) since at least
one partition Y (k)
contains a label of the highest score. In section 3.3  we show that this approximate
FMO still ensures convergence with a rate scaled by 1/ν. In practice  since the set of active labels
is not changing frequently during training  once an active label yf is sampled  it will be kept in the
active set Af till the end of the algorithm and thus results in a convergence rate similar to that of
an exact FMO. Note for problems of binary variables with large number of inner-product factors 
and

the sampling technique applies similarly by simply partitioning factors as Fi = (cid:83)ν

k=1 F (k)

searching active factors only within one randomly chosen partition at a time.

i

3.3 Convergence Analysis

We show the iteration complexity of the GDMM algorithm with an 1/ν-approximated FMO given
in section 3.2. The convergence guarantee for exact FMOs can be obtained by setting ν = 1. The
analysis leverages recent analysis on the global linear convergence of Frank-Wolfe variants [8] for
function of the form (12) with a polyhedral domain  and also the analysis in [5] for Augmented
Lagrangian based method. This type of greedy Augmented Lagrangian Method was also analyzed
previously under different context [23  24  22].
Let d(λ) = minα L(α  λ) be the dual objective of (12)  and let

d := d∗ − d(λt)  ∆t
∆t

p := L(αt  λt) − d(λt)

(22)
be the dual and primal suboptimality of problem (10) respectively. We have the following theorems.
Theorem 1 (Convergence of GDMM with AFW). The iterates {(αt  λt)}∞
t=1 produced by Algo-
rithm 1 with step 1 performed by Algorithm 2 has

E[∆t

p + ∆t

d] ≤  for t ≥ ω log(

1


)

(cid:110)

(cid:111)

ρ

4+16(1+ν)mQ/µM with ω = max

for any 0 < η ≤
  where µM is the
generalized geometric strong convexity constant of (12)  Q is the Lipschitz-continuous constant for
the gradient of objective (12)  and τ > 0 is a constant depending on optimal solution set.
Theorem 2 (Convergence of GDMM with BGCD). The iterates {(αt  λt)}∞
rithm 1 with step 1 performed by Algorithm 3 has

t=1 produced by Algo-

2(1 + 4 mQ(1+ν)

µM )  τ

η

(23)

(24)

E[∆t

p + ∆t

(cid:110)

d] ≤  for t ≥ ω1 log(
2(1 + Qmaxν

)

1

)  τ
η

(cid:111)

for any 0 < η ≤
  where µ1 is the generalized
strong convexity constant of objective (12) and Qmax = maxf∈F Qf is the factorwise Lipschitz-
continuous constant on the gradient.

4(1+Qmaxν/µ1) with ω1 = max

µ1

ρ

4 Experiments

In this section  we compare with existing approaches on Sequence Labeling and Multi-label predic-
tion with pairwise interaction. The algorithms in comparison are: (i) BCFW: a Block-Coordinate
Frank-Wolfe method based on structural oracle [9]  which outperforms other competitors such as
Cutting-Plane  FW  and online-EG methods in [9]. (ii) SSG: an implementation of the Stochastic
Subgradient method [16]. (iii) Soft-BCFW: Algorithm proposed in ([12])  which avoids structural
oracle by minimizing an approximate objective  where a parameter ρ controls the precision of the
approximation. We tuned the parameter and chose two of the best on the ﬁgure. For BCFW and
SSG  we adapted the MATLAB implementation provided by authors of [9] into C++  which is an
order of magnitude faster. All other implementations are also in C++. The results are compared in
terms of primal objective (achieved by w) and test accuracy.
Our experiments are conducted on 4 public datasets: POS  ChineseOCR  RCV1-regions  and EUR-
Lex (directory codes). For sequence labeling we experiment on POS and ChineseOCR. The POS
dataset is a subset of Penn treebank2 that contains 3 808 sentences  196 223 words  and 45 POS la-
bels. The HIT-MW3 ChineseOCR dataset is a hand-written Chinese character dataset from [17]. The

2https://catalog.ldc.upenn.edu/LDC99T42
3https://sites.google.com/site/hitmwdb/

7

Figure 2: (left) Compare two FMO-based algorithms (GDMM  Soft-BCFW) in number of iterations.
(right) Improvement in training time given by sublinear-time FMO.

Figure 3: Primal Objective v.s. Time and Test error v.s. Time plots. Note that ﬁgures of objective
have showed that SSG converges to a objective value much higher than all other methods  this is also
observed in [9]. Note the training objective for the EUR-Lex data set is too expensive to compute
and we are unable to plot the ﬁgure.

dataset has 12 064 hand-written sentences  and a total of 174 074 characters. The vocabulary (label)
size is 3 039. For the Correlated Multilabel Prediction problems  we experiment on two benchmark
datasets RCV1-regions4 and EUR-Lex (directory codes)5. The RCV1-regions dataset has 228 labels 
23 149 training instances and 47 236 features. Note that a smaller version of RCV1 with only 30
labels and 6000 instances is used in [11  12]. EUR-Lex (directory codes) has 410 directory codes as
labels with a sample size of 19 348. We ﬁrst compare GDMM (without subFMO) with Soft-BCFW
in Figure 2. Due to the approximation (controlled by ρ)  Soft-BCFW can converge to a subopti-
mal primal objective value. While the gap decreases as ρ increases  its convergence becomes also
slower. GDMM  on the other hand  enjoys a faster convergence. The sublinear-time implementation
of FMO also reduces the training time by an order of magnitude on the ChineseOCR data set  as
showed in Figure 2 (right). More general experiments are showed in Figure 3. When the size of out-
put domain is small (POS dataset)  GDMM-subFMO is competitive to other solvers. As the size of
output domain grows (ChineseOCR  RCV1  EUR-Lex)  the complexity of structural maximization
oracle grows linearly or even quadratically  while the complexity of GDMM-subFMO only grows
sublinearly in the experiments. Therefore  GDMM-subFMO achieves orders-of-magnitude speedup
over other methods. In particular  when running on ChineseOCR and EUR-Lex  each iteration of
SSG  GDMM  BCFW and Soft-BCFW take over 103 seconds  while it only takes a few seconds in
GDMM-subFMO.

Acknowledgements. We acknowledge the support of ARO via W911NF-12-1-0390  NSF via
grants CCF-1320746  CCF-1117055  IIS-1149803  IIS-1546452  IIS-1320894  IIS-1447574  IIS-
1546459  CCF-1564000  DMS-1264033  and NIH via R01 GM117594-01 as part of the Joint
DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical
Sciences.

4www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html
5mulan.sourceforge.net/datasets-mlc.html

8

100101102103iteration10-1100objectivePOSGDMMSoft-BCFW-ρ=1Soft-BCFW-ρ=10100101102iteration1.41.51.61.71.81.922.12.22.32.4Objective×105ChineseOCRGDMMSoft-BCFW-ρ=1Soft-BCFW-ρ=10103104time1.41.51.61.71.81.922.12.22.32.4Objective×105ChineseOCRGDMMGDMM-subFMO102104time100101Relative-ObjectivePOSBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10103104time1.522.53Objective×105ChineseOCRBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10102103104105time104105106107108109ObjectiveRCV1-regionsBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10102104time0.070.080.090.10.110.120.130.14test errorPOSBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10103104time0.450.50.550.60.650.70.750.80.850.90.95test errorChineseOCRBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10102103104105time10-2test errorRCV1-regionsBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10102103104time0.010.020.030.040.050.060.070.080.090.1test errorEUR-LexBCFWGDMM-subFMOSSGSoft-BCFW-ρ=1Soft-BCFW-ρ=10References
[1] D. Das  D. Chen  A. F. Martins  N. Schneider  and N. A. Smith. Frame-semantic parsing. Computational

linguistics  40(1):9–56  2014.

[2] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research  9:1871–1874  2008.

[3] K. Gimpel and N. A. Smith. Structured ramp loss minimization for machine translation. In NAACL  pages

221–231. Association for Computational Linguistics  2012.

[4] A. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the

National Bureau of Standards  1952.

[5] M. Hong and Z.-Q. Luo. On the linear convergence of the alternating direction method of multipliers.

arXiv preprint arXiv:1208.3922  2012.

[6] T. Joachims  T. Finley  and C.-N. J. Yu. Cutting-plane training of structural svms. Machine Learning 

77(1):27–59  2009.

[7] M. P. Kumar  V. Kolmogorov  and P. H. Torr. An analysis of convex relaxations for map estimation.

Advances in Neural Information Processing Systems  20:1041–1048  2007.

[8] S. Lacoste-Julien and M. Jaggi. On the global linear convergence of frank-wolfe optimization variants.

In Advances in Neural Information Processing Systems  pages 496–504  2015.

[9] S. Lacoste-Julien  M. Jaggi  M. Schmidt  and P. Pletscher. Block-coordinate frank-wolfe optimization for

structural svms. In ICML 2013 International Conference on Machine Learning  pages 53–61  2013.

[10] O. Meshi  M. Mahdavi  and D. Sontag. On the tightness of lp relaxations for structured prediction. arXiv

preprint arXiv:1511.01419  2015.

[11] O. Meshi  D. Sontag  T. Jaakkola  and A. Globerson. Learning efﬁciently with approximate inference via

dual losses. 2010.

[12] O. Meshi  N. Srebro  and T. Hazan. Efﬁcient training of structured svms via soft constraints. In AISTAT 

2015.

[13] A. Osokin  J.-B. Alayrac  I. Lukasewitz  P. K. Dokania  and S. Lacoste-Julien. Minding the gaps for block

frank-wolfe optimization of structured svms. arXiv preprint arXiv:1605.09346  2016.

[14] P. Ravikumar and J. Lafferty. Quadratic programming relaxations for metric labeling and markov random

ﬁeld map estimation. In ICML  2006.

[15] R. Samdani and D. Roth. Efﬁcient decomposed learning for structured prediction. ICML  2012.

[16] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

svm. Mathematical programming  2011.

[17] T. Su  T. Zhang  and D. Guan. Corpus-based hit-mw database for ofﬂine recognition of general-purpose

chinese handwritten text. IJDAR  10(1):27–38  2007.

[18] B. Taskar  V. Chatalbashev  D. Koller  and C. Guestrin. Learning structured prediction models: A large

margin approach. In ICML  2005.

[19] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In Advances in neural information

processing systems  volume 16  2003.

[20] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for interde-

pendent and structured output spaces. In ICML  2004.

[21] P. Woodland and D. Povey. Large scale discriminative training for speech recognition.

Automatic Speech Recognition Workshop (ITRW)  2000.

In ASR2000-

[22] I. E. Yen  X. Lin  E. J. Zhang  E. P. Ravikumar  and I. S. Dhillon. A convex atomic-norm approach to

multiple sequence alignment and motif discovery. 2016.

[23] I. E. Yen  D. Malioutov  and A. Kumar. Scalable exemplar clustering and facility location via augmented

block coordinate descent with column generation. In AISTAT  2016.

[24] I. E.-H. Yen  K. Zhong  C.-J. Hsieh  P. K. Ravikumar  and I. S. Dhillon. Sparse linear programming via

primal and dual augmented coordinate descent. In NIPS  2015.

9

,Ian En-Hsu Yen
Xiangru Huang
Kai Zhong
Ruohan Zhang
Pradeep Ravikumar
Inderjit Dhillon
Casper Freksen
Lior Kamma
Kasper Green Larsen