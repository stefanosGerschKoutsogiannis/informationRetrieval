2012,Multi-task Vector Field Learning,Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks  both are crucial for semi-supervised multi-task learning. In this paper  we develop multi-task vector field learning (MTVFL) which learns the prediction functions and the vector fields simultaneously. MTVFL has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task  the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach.,Multi-task Vector Field Learning

2Sen Yang

1Chiyuan Zhang

2Jieping Ye

1Xiaofei He

1Binbin Lin

1State Key Lab of CAD&CG  Zhejiang University  Hangzhou 310058  China
{binbinlinzju  chiyuan.zhang.zju  xiaofeihe}@gmail.com

2The Biodesign Institute  Arizona State University  Tempe  AZ  85287

{senyang  jieping.ye}@asu.edu

Abstract

Multi-task learning (MTL) aims to improve generalization performance by learn-
ing multiple related tasks simultaneously and identifying the shared information
among tasks. Most of existing MTL methods focus on learning linear models
under the supervised setting. We propose a novel semi-supervised and nonlinear
approach for MTL using vector ﬁelds. A vector ﬁeld is a smooth mapping from
the manifold to the tangent spaces which can be viewed as a directional derivative
of functions on the manifold. We argue that vector ﬁelds provide a natural way to
exploit the geometric structure of data as well as the shared differential structure
of tasks  both of which are crucial for semi-supervised multi-task learning. In this
paper  we develop multi-task vector ﬁeld learning (MTVFL) which learns the pre-
dictor functions and the vector ﬁelds simultaneously. MTVFL has the following
key properties. (1) The vector ﬁelds MTVFL learns are close to the gradient ﬁelds
of the predictor functions. (2) Within each task  the vector ﬁeld is required to be as
parallel as possible which is expected to span a low dimensional subspace. (3) The
vector ﬁelds from all tasks share a low dimensional subspace. We formalize our
idea in a regularization framework and also provide a convex relaxation method
to solve the original non-convex problem. The experimental results on synthetic
and real data demonstrate the effectiveness of our proposed approach.

1

Introduction

In many applications  labeled data are expensive and time consuming to obtain while unlabeled data
are abundant. The problem of using unlabeled data to improve the generalization performance is
often referred to as semi-supervised learning (SSL). It is well known that in order to make semi-
supervised learning work  some assumptions on the dependency between the predictor function and
the marginal distribution of data are needed. The manifold assumption [15  5]  which has been
widely adopted in the last decade  states that the predictor function lives in a low dimensional man-
ifold of the marginal distribution.
Multi-task learning was proposed to enhance the generalization performance by learning multiple
related tasks simultaneously. The abundant literature on multi-task learning demonstrates that the
learning performance indeed improves when the tasks are related [4  6  7]. The key step in MTL
is to ﬁnd the shared information among tasks. Evgeniou et al. [12] proposed a regularization MTL
framework which assumes all tasks are related and close to each other. Ando and Zhang [2] pro-
posed a structural learning framework  which assumed multiple predictors for different tasks shared
a common structure on the underlying predictor space. An alternating structure optimization (ASO)
method was proposed for linear predictors which assumed the task parameters shared a low dimen-
sional subspace. Arvind et al. [1] generalized the idea of sharing a subspace by assuming that all
task parameters lie on a manifold.

1

(a) A parallel ﬁeld on R2

(b) A parallel ﬁeld on Swiss roll

Figure 1: Examples of parallel ﬁelds. The parallel ﬁeld on R2 spans a one dimensional subspace
and the parallel ﬁeld on the Swiss roll spans a two dimensional subspace.

In this paper  we consider semi-supervised multi-task learning (SSMTL). Although many SSL meth-
ods have been proposed in the literature [10]  these methods are often not directly amenable to MTL
extensions [18]. Liu et al. [18] proposed an SSMTL framework which encouraged related models
to have similar parameters. However they require that related tasks share similar representations [9].
Wang et al. [19] proposed another SSMTL method under the assumption that the tasks are clus-
tered [4  14]. The cluster structure is characterized by task parameters of linear predictor functions.
For linear predictors  the task parameters they used are actually the constant gradient of the predictor
functions which form a ﬁrst order differential structure. For general nonlinear predictor functions 
we show it is more natural to capture the shared differential structure using vector ﬁelds.
In this paper  we propose a novel SSMTL formulation using vector ﬁelds. A vector ﬁeld is a smooth
mapping from the manifold to the tangent spaces which can be viewed as a directional derivative
of functions on the manifold.
In this way  a vector ﬁeld naturally characterizes the differential
structure of functions while also providing a natural way to exploit the geometric structure of data;
these are the two most important aspects for SSMTL. Based on this idea  we develop the multi-task
vector ﬁeld learning (MTVFL) method which learns the prediction functions and the vector ﬁelds
simultaneously. The vector ﬁelds we learned are forced to be close to the gradient ﬁelds of predictor
functions.
In each task  the vector ﬁeld is required to be as parallel as possible. We say that a
vector ﬁeld is parallel if the vectors are parallel along the geodesics on the manifold. In extreme
cases  when the manifold is a linear (or an afﬁne) space  then the geodesics of such manifold are
straight lines. In such cases  the space spanned by these parallel vectors is a simply one-dimensional
subspace. Thus when the manifold is ﬂat (i.e.  with zero curvature) or the curvature is small  it is
expected that these parallel vectors concentrate on a low dimensional subspace. As an example  we
can see from Fig. 1 that the parallel ﬁeld on the plane spans a one-dimensional subspace and the
parallel ﬁeld on the Swiss roll spans a two-dimensional subspace. For the multi-task case  these
vector ﬁelds share a low dimensional subspace. In addition  we assume these vector ﬁelds share a
low dimensional subspace among all tasks. In essence  we use a ﬁrst-order differential structure to
characterize the shared structure of tasks and use a second-order differential structure to characterize
the speciﬁc parts of tasks. We formalize our idea in a regularization framework and provide a convex
relaxation method to solve the original non-convex problem. We have performed experiments using
both synthetic and real data; results demonstrate the effectiveness of our proposed approach.

2 Multi-task Learning: A Vector Field Approach

In this section  we ﬁrst introduce vector ﬁelds and then present multi-task learning via exploring
shared structure using vector ﬁelds.

2.1 Multi-task Learning Setting and Vector Fields

We ﬁrst introduce notation and symbols. We are given m tasks  with nl samples xl

for the l-th task. The total number of samples is n =(cid:80)
(cid:8)xl
(cid:9) are on a dl-dimensional manifold Ml. All of these data manifolds are embedded in the same

i  i = 1  . . .   nl
l nl. For the l-th task  we assume the data

i

2

i

i

i

l (n(cid:48)

M.

i

i ∼ xl

i = T l

i T l
i

i)  n(cid:48)

l (xl

i a) ⊥ P l
i a.

M and (a − P l

It is easy to show that P l

j are neighbors. Let wl

l n(cid:48)
l + 1 ≤ i ≤ nl.

j ∈ {−1  1} for classiﬁcation  j = 1  . . .   n(cid:48)

number of labeled samples is n(cid:48) = (cid:80)

j ∈ R for regression and yl

j denote that xl
i and xl
i  we estimate its tangent space Txl

D-dimensional ambient space RD. It is worth noting that the dimensions of different data manifolds
are not required to be the same. Without loss of generality  we assume the ﬁrst n(cid:48)
l < nl) samples
l. The total
are labeled  with yl
l. For the l-th task  we denote the regression function or
classiﬁcation function by f∗
l . The goal of semi-supervised multi-task learning is to learn the function
value on unlabeled data  i.e.  f∗
Given the l-th task  we ﬁrst construct a nearest neighbor graph by either -neighborhood or k nearest
neighbors. Let xl
ij denote the weight which
i and xl
j. It can be approximated by the heat kernel weight or the
measures the similarity between xl
M by performing PCA on its
simple 0-1 weight. For each point xl
M has
neighborhood. We choose the largest dl eigenvectors as the bases since the tangent space Txl
i ∈ RD×dl be the matrix whose columns constitute
the same dimension as the manifold Ml. Let T l
T is the unique orthogonal
an orthonormal basis for Txl
M [13]. That is  for any vector a ∈ Rm  we have
projection from RD onto the tangent space Txl
i a ∈ Txl
P l
We now formally deﬁne the vector ﬁeld and show how to represent it in the discrete case.
Deﬁnition 2.1 ([16]). A vector ﬁeld X on the manifold M is a continuous map X : M → TM
where TM is the set of tangent spaces  written as p (cid:55)→ Xp  with the property that for each p ∈ M 
Xp is an element of TpM.
We can think of a vector ﬁeld on the manifold as an arrow in the same way as we think of the vector
ﬁeld in the Euclidean space  with a given magnitude and direction attached to each point on the
manifold  and chosen to be tangent to the manifold. A vector ﬁeld V on the manifold is called a
gradient ﬁeld if there exists a function f on the manifold such that ∇f = V where ∇ is the covariant
derivative on the manifold. Therefore  gradient ﬁelds are one kind of vector ﬁelds. It plays a critical
role in connecting vector ﬁelds and functions.
Let Vl be a vector ﬁeld on the manifold Ml. For each point xl
i  let Vxl
denote the value of the
i. Recall the deﬁnition of vector ﬁeld  Vxl
vector ﬁeld Vl at xl
should be a vector in the tangent
Ml as
space Txl
i ∈ Rdl is the local representation of Vxl
i . Let fl be a function
= T l
Vxl
on the manifold Ml. By abusing the notation without confusion  we also use fl to denote the vector
fl = (fl(x1
is  Vl is a dlnl-dimensional big column vector which concatenates all the vl
each task  we aim to compute the vector fl and the vector Vl.

Ml. Therefore  we can represent it by the coordinates of the tangent space Txl
i  where vl
i vl

T(cid:17)T ∈ Rdlnl. That

))T and use Vl to denote the vector Vl =

i’s for a ﬁxed l. Then for

with respect of T l

l )  . . .   fl(xl
nl

T

vl
1

  . . .   vl
nl

i

i

i

i

(cid:16)

i

i

i

2.2 Multi-task Vector Field Learning

In this section  we introduce multi-task vector ﬁeld learning (MTVFL).
Many existing MTL methods capture the task relatedness by sharing task parameters. For linear
predictors  the task parameters they used are actually the constant gradient vectors of the predictor
functions. For general nonlinear predictor functions  we show it is natural to capture the shared
m)T and V denote the
differential structure using vector ﬁelds. Let f denote the vector (f T
vector (V T

T )T . We propose to learn f and V simultaneously:

1   . . .   f T

1   . . .   V T

m )T = (v1
1

T   . . .   vm
nl

• The vector ﬁeld Vl should be close to the gradient ﬁeld ∇fl of fl  which can be formularized

as follows:

(cid:90)
m(cid:88)
(cid:90)
m(cid:88)

l=1

Ml

l=1

m(cid:88)
m(cid:88)

l=1

l=1

3

R1(fl  Vl) :=
• The vector ﬁeld Vl should be as parallel as possible:

R1(f  V ) =

min
f V

(cid:107)∇fl − Vl(cid:107)2.

Ml

min

V

R2(V ) =

R2(Vl) :=

(cid:107)∇Vl(cid:107)2

HS 

(1)

(2)

ing(cid:82)

where ∇ is the covariant derivative on the manifold  where (cid:107) · (cid:107)HS denotes the Hilbert-
Schmidt tensor norm [11]. ∇Vl measures the change of the vector ﬁeld  therefore minimiz-

(cid:107)∇Vl(cid:107)2

Ml

HS enforces the vector ﬁeld Vl to be parallel.

• All vector ﬁelds share an h-dimensional subspace where h is a predeﬁned parameter:

T l
i vl

i = ul

i + ΘT wl
i 

s.t. ΘΘT = Ih×h.

(3)

Since these vector ﬁelds are assumed to share a low dimensional space  it is expected that the residual
vector ul

i is small. We deﬁne another term R3 to control the complexity as follows:

R3(vl

i  wl

i  Θ) =

=

α(cid:107)ul

i(cid:107)2 + β(cid:107)T l
i(cid:107)2
i vl

α(cid:107)T l
i − ΘT wl
i vl

i(cid:107)2 + β(cid:107)T l
i(cid:107)2.
i vl

(4)

(5)

m(cid:88)
m(cid:88)

l=1

nl(cid:88)
nl(cid:88)

i=1

l=1

i=1

Note that α and β are pre-speciﬁed coefﬁcients  indicating the importance of the corresponding
regularization component. Since we would like the vector ﬁeld to be parallel  the vector norm is not
expected to be too small. Besides  we assume the vector ﬁelds share a low dimensional subspace 
the residual vector ul
i is expected to be small. In practice we suggest to use a small β and a large α.
By setting β = 0  R3 will reduce to the regularization term proposed in ASO if we also replace the
tangent vectors by the task parameters. Therefore  this formulation is a generalization of ASO.

It can be veriﬁed that wl
i
i = (I − ΘT Θ)T l
i. Therefore  we can rewrite R3 as follows:
ΘT wl
i vl

i = arg minwl

= ΘT l

R3(vl

i  wl

i vl

i

∗

i  Θ). Thus we have ul

i = T l

i −
i vl

R3(V  Θ) =

=

α(cid:107)ul

i(cid:107)2 + β(cid:107)T l
i(cid:107)2
i vl

(cid:0)α(cid:107)(I − ΘT Θ)T l

i(cid:107)2(cid:1)

i(cid:107)2 + β(cid:107)T l
i vl
i vl

(6)

l=1

i=1

= αV T AΘV + βV T HV 

where H is a block diagonal matrix with the diagonal blocks being T l
i
diagonal matrix with the diagonal blocks being T l
i
Therefore  the proposed formulation solves the following optimization problem:

(I−ΘT Θ)T (I−ΘT Θ)T l

T

i   and AΘ is another block
T l
(I−ΘT Θ)T l
i .

i = T l
i

T

T

arg min

E(f  V  Θ) = R0(f ) + λ1R1(f  V ) + λ2R2(V ) + λ3R3(V  Θ)

s.t. ΘΘT = Ih×h  (7)

m(cid:88)
m(cid:88)

l=1

nl(cid:88)
nl(cid:88)

i=1

where R0(f ) is the loss function. For simplicity  we use the quadratic loss function R0(f ) =

f V Θ

(cid:80)m

l=1

(cid:80)n(cid:48)

l

i=1(fl(xl

i) − yl

i)2.

2.3 Objective Function in the Matrix Form

To simplify Eq. (7)  in this section we rewrite our objective function in the matrix form.
Using the discrete methods in [17]  we have the following discrete form equations:

(cid:1)2

R1(fl  Vl) =

R2(fl  Vl) =

j − xl

i)T T l

i − f l
i vl

j + f l
i

(cid:13)(cid:13)2

.

j − T l
i vl
j vl
i

i T l

 

(8)

(9)

(cid:0)(xl
(cid:13)(cid:13)P l

wl
ij

wl
ij

(cid:88)
(cid:88)

i∼j

i∼j

Interestingly  with some algebraic transformations  we have the following matrix forms for our
objective functions:

R1(fl  Vl) = 2f T

l Llfl + V T

l GlVl − 2V T

l Clfl 

(10)

4

T

T

  . . .   C l
n

where Ll is the graph Laplacian matrix  Gl is a dlnl × dlnl block diagonal matrix  and Cl =
]T is a dlnl × nl block matrix. Denote the i-th dl × dl diagonal block of Gl by Gl
[C l
and the i-th dl × nl block of Cl by C l
1
j − xl

i  we have
j − xl

(cid:88)

(cid:88)

j − xl

i)T   C l

i)(xl

ij(xl

ij(xl

i)sl
ij

(11)

Gl

ii =

i =

T

 

ii

wl

j∼i

wl

j∼i

ij ∈ Rnl is a selection vector of all zero elements except for the i-th element being −1 and

where sl
the j-th element being 1. And R2 becomes

where Bl is a dlnl × dlnl sparse block matrix. If we index each dl × dl block by Bl

R2(Vl) = V T

l BlVl 

(12)
ij  then we have

(cid:88)
(cid:40)−2wl

j∼i

wl

Bl

ii =

Bl

ij =

ij(Ql

ijQl
ij

ijQl

ij 
0 

T

+ I) 

if xi ∼ xj
otherwise

 

(13)

(14)

(15)

(16)

where Ql

ij = T l
i

T

j. It is worth nothing that both R1 and R2 depend on tangent spaces T l
i .
T l

Thus we can further write R1(f  V ) and R2(V ) as follows

m(cid:88)
m(cid:88)

l=1

R1(f  V ) =

R2(V ) =

R1(fl  Vl) = 2f T Lf + V T GV − 2V T Cf 

R2(Vl) = V T BV 

l=1

where L  G and B are block diagonal matrices with the corresponding l-th block matrix being Ll 
Gl and Bl  respectively. C is a column block matrix with the l-th block matrix being Cl.
Let I denote an n × n diagonal matrix where Iii = 1 if the corresponding i-th data is labeled and
Iii = 0 otherwise. And let y ∈ Rn be a column vector whose i-th element is the corresponding label
n(cid:48) (f − y)T I(f − y). Finally  we get the
of the i-th labeled data and 0 otherwise. Then R0(f ) = 1
following matrix form for our objective function in Eq. (7) with the constraint ΘΘT = Ih×h as:

=

E(f  V  Θ) = R0(f ) + λ1R1(f  V ) + λ2R2(V ) + λ3R3(V  Θ)
1

n(cid:48) (f − y)T I(f − y) + λ1(2f T Lf + V T GV − 2V T Cf ) + λ2V T BV + λ3V T (αAΘ + βH)V
n(cid:48) (f − y)T I(f − y) + 2λ1f T Lf + V T (λ1G + λ2B + λ3(αAΘ + βH))V − 2λ1V T Cf.
It is worth noting that matrices L  G  B  C depend on data  and only the matrix AΘ is related to Θ.

=

1

3 Optimization

In this section  we discuss how to solve the following optimization problem:

arg min

E(f  V  Θ) 

f V Θ

s.t. ΘΘT = Ih×h.

(17)

We use the alternating optimization to solve this problem.

• Optimization of f and V . For a ﬁxed Θ  the optimal f and V can be obtained via solving
(18)

E(f  V  Θ).

arg min

• Optimization of Θ. For a ﬁxed V   the optimal Θ can be obtained via solving.

f V

arg min

Θ

R3(V  Θ) 

s.t. ΘΘT = Ih×h.

(19)

5

3.1 Optimization of f and V for a Given Θ

When Θ is ﬁxed  the objective function is similar to that of the single task case. However  there are
some differences we would like to mention. Firstly  when constructing the nearest neighbor graph 
data points from different tasks are disconnected. Therefore when estimating tangent spaces  data
points from different tasks are independent. Secondly  we do not require the dimension of tangent
spaces from each task to be the same.
We note that

(cid:19)
(cid:18) 1
n(cid:48) I + 2λ1L

= 2

f − 2λ1C T V − 2

1
n(cid:48) y 

= −2λ1Cf + 2(λ1G + λ2H + λ3(αAΘ + βH))V.

∂E
∂f
∂E
∂V

(cid:18) 1

n(cid:48) I + 2λ1L

−λ1C

(20)

(21)

(22)

(cid:19)

.

(cid:19)(cid:18) f

(cid:19)

(cid:18) 1

=

n(cid:48) y
0

Requiring the derivatives to be vanish  we get the following linear system

−λ1C T

λ1G + λ2B + λ3(αAΘ + βH)

V

Except for the matrix AΘ  all other matrices can be computed in advance and will not change during
the iterative process.

3.2 Optimization of Θ for a Given V

Since functions R0(f )  R1(f  V ) and R2(V ) are not related to the variable Θ  we only need to
optimize R3(V  Θ) subject to ΘΘT = Ih×h.
Recall Eq. (6)  we rewrite R3(V  Θ) as follows:

(cid:18)

α

m(cid:88)

nl(cid:88)
(cid:18)

i=1

ˆΘ = arg min

Θ

l=1

= arg min

α tr

Θ

i(cid:107)2 +
(cid:107)(I − ΘT Θ)T l
(cid:19)
i vl

)I − ΘT Θ)V

β
α

VT ((1 +

(cid:19)

(cid:107)T l
i(cid:107)2
i vl

β
α

(23)

= arg max

tr(ΘVVT ΘT ) 
) is a D × n matrix with each column being a tangent vector. The
where V = (T 1
optimal ˆΘ can be obtained by using singular value decomposition (SVD). Let V = Z1ΣZ T
2 be the
SVD of V and we assume that the singular values are in a decreasing order in Σ. Then the rows of
ˆΘ are given by the ﬁrst h columns of Z1.

1  . . .   T m
nm

Θ
vm
nm

1 v1

3.3 Convex Relaxation

ΘT Θ)−1. Then we can rewrite R3(V  Θ) as R3(V  Θ) = αη(1 + η) tr(cid:0)VT (ηI + ΘT Θ)−1V(cid:1) .

The orthogonality constraint in Eq. (23) is non-convex. Next  we propose to convert Eq. (23) into a
convex formulation by relaxing its feasible domain into a convex set.
Let η = β/α. It can be veriﬁed that the following equality holds: (1 + η)I − ΘT Θ = η(1 + η)(ηI +
Let Me be deﬁned as Me = {M : M = ΘT Θ  ΘΘT = I  Θ ∈ Rh×d}. The convex hull [8] of Me
can be expressed as the convex set Mc given by Mc = {M : tr(M ) = h  M (cid:22) I  M ∈ Sd
+} and
each element in Me is referred to as an extreme point of Mc.
To convert the non-convex problem Eq. (23) into a convex formulation  we replace ΘT Θ with M 
and naturally relax its feasible domain into a convex set based on the relationship between Me and
Mc presented above; this results in an optimization problem as

where R3(V  M ) is deﬁned as R3(V  M ) = αη(1 + η) tr(cid:0)VT (ηI + M )−1V(cid:1) . It follows from

s.t.   tr(M ) = h  M (cid:22) I  M ∈ Sd
+ 

[3  Theorem 3.1] that the relaxed R3 is jointly convex in V and M. After we obtain the optimal
M  the optimal Θ can be approximated using the ﬁrst h eigenvectors (corresponding to the largest
h eigenvalues) of the optimal M.

R3(V  M ) 

arg min

(24)

Θ

6

4 Experiments

In this section  we evaluate our method on one synthetic data and one real data set. We compare
the proposed Multi-Task Vector Field Learning (MTVFL) algorithm against the following methods:
(a) Single Task Vector Field Learning (STVFL  or PFR)  (b) Alternating Structure Optimization
(ASO) and (c) its nonlinear version - Kernelized Alternating Structure Optimization (KASO). The
kernel constructed in KASO uses both labeled data and unlabeled data. Thus it can be viewed as a
semi-supervised MTL method.

4.1 Synthetic Data

(a) MSE

(b) Singular value distribution

Figure 2: (a) Performance of MTVFL and STVFL; (b) The singular value distribution.

We ﬁrst construct a synthetic data to evaluate our method in comparison with the semi-supervised
single task learning method (STVFL). We generate two data sets including Swiss roll and Swiss roll
with hole embedded in 3-dimensional Euclidean space. The Swiss roll is generated by the following
equations x = t1 cos t1; y = t2; z = t1 sin t1 where t1 ∈ [3π/2  9π/2]; t2 ∈ [0  21]. The Swill
roll with hole excludes points within t1 ∈ [9  12] and t2 ∈ [9  14]. The ground truth function is
f (x  y  z) = t1. This test is a semi-supervised multi-task regression problem. We randomly select a
number of labeled data in each task and try to predict the value on other unlabeled data.
Each data set has 400 points. We construct a nearest neighbor graph for each task. The number of
nearest neighbors is set to 5 and the manifold dimension is set to 2 as they are both 2 dimensional
manifolds. The shared subspace dimension is set to 2. The regularization parameters are chosen
via cross-validation. We perform 100 independent trials with randomly selected labeled sets. The
performance is measured by the mean squared error (MSE). We also try ASO and KASO  however
they perform poorly since the data is highly nonlinear. The averaged MSE over two tasks is presented
in Fig. 2. We can observe that MTVFL consistently outperforms STVFL which demonstrates the
effectiveness of SSMTL.
We also show the singular value distribution of the ground truth gradient ﬁelds. Given the ground
truth f  we can compute the gradient ﬁeld V by taking derivatives of R1(f  V ) with respect to V .
Requiring the derivative to vanish  we get the following equation GV = Cf. After obtaining V   the
gradient vector Vxl
i. Then we perform PCA on these
= T l
vectors and the singular values of the covariance matrix of Vxl
are shown in Fig. 2 (b). As can be
seen from Fig. 2 (b)  the number of dominant singular values is 2 which indicates that the ground
truth gradient ﬁelds concentrate on a 2-dimensional subspace.

at each point can be obtained as Vxl

i

i

i vl

i

4.2 Landmine Detection

We use the landmine data set studied in [20]. There are totally 29 sets of data which are collected
from various real landmine ﬁelds. Each data example is represented by a 9-dimensional vector with
a binary label  which is either 1 for landmine or 0 for clutter. The problem of landmine detection

1The data set is available at http://www.ee.duke.edu/˜lcarin/LandmineData.zip.

7

102030405000.050.10.150.20.25Number of Labeled DataMSE  MTVFLSTVFL123012345678Sigular ValuePrincipal Component(a) Averaged AUC

(b) Singular value distribution

Figure 3: (a) Performance of various MTL algorithms; (b) The singular value distribution.

is to predict the labels of unlabeled objects. Among the 29 data sets  1-15 correspond to relatively
highly foliated regions and 16-29 correspond to bare earth or desert regions. Following [20]  we
choose the data sets 1-10 and 16-24 to form 19 tasks.
The basic setup of all the algorithms is as follows. First  we construct a nearest neighbor graph
for each task. The number of nearest neighbors is set to 10 and the manifold dimension is set to 4
empirically. These two parameters are the same for all 19 tasks. The shared subspace dimension
is set to be 5 for both of MTVFL and ASO and the shared subspace dimension of KASO is set to
10. All the regularization parameters for the four algorithms are chosen via cross-validation. Note
that KASO needs to construct a kernel matrix. We use Gaussian kernel in KASO and the Gaussian
width is set to be optimal by searching within [0.01  10].
We perform 100 independent trials with randomly selected labeled sets. We measure the perfor-
mance by AUC which denotes area under the Receiver Operation Characteristic (ROC) curve. A
large AUC value indicates good classiﬁcation performance. Since the data have severely unbal-
anced labels  following [20]  we do a special setting that assures there is at least one “1” and one “0”
labeled sample in the training set of each task. The AUC averaged over the 19 tasks is presented in
Fig. 3 (a). As can be seen  MTVFL consistently outperforms the other three algorithms. When the
number of labeled data increases  KASO outperforms STVFL. ASO does not improve much when
the amount of labeled data increases  which is probably because the data have severely unbalanced
labels and the ground truth predictor function is nonlinear. We also show the singular value distri-
bution of the ground truth gradient ﬁelds in Fig. 3 (b). The computation of the singular values is the
same as in Section. 4.1. As can be seen from Fig. 3 (b)  the number of dominant singular values
is 5. The percentage of the sum of the ﬁrst 5 singular values over the total sum is 91.34%  which
indicates that the ground truth gradient ﬁelds concentrate on a 5-dimensional subspace.

5 Conclusion

In this paper  we propose a new semi-supervised multi-task learning formulation using vector ﬁelds.
We show that vector ﬁelds can naturally capture the shared differential structure among tasks as well
as the structure of the data manifolds. Our experimental results on synthetic and real data demon-
strate the effectiveness of the proposed method. There are several interesting directions suggested
in this work. One is the relation between learning on task parameters and learning on vector ﬁelds.
Ultimately  both of them are learning functions. Another one is to apply other assumptions made in
the multi-task learning community into vector ﬁeld learning  e.g.  the cluster assumption.

Acknowledgments

This work was supported by the National Natural Science Foundation of China under Grants
61125203  61233011 and 90920303  the National Basic Research Program of China (973 Program)
under Grant 2012CB316404  the Fundamental Research Funds for the Central Universities under
grant 2011FZA5022  NIH (R01 LM010730) and NSF (IIS-0953662  CCF-1025177).

8

203040506070800.650.70.750.80.85Number of Labeled DataAverage AUC on 19 Tasks  MTVFLSTVFLKASOASO2468020040060080010001200Principal ComponentSigular ValueReferences
[1] A. Agarwal  H. D. III  and S. Gerber. Learning multiple tasks using manifold regularization.

In Advances in Neural Information Processing Systems 23  pages 46–54. 2010.

[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks

and unlabeled data. Journal of Machine Learning Research  6:1817–1853  2005.

[3] A. Argyriou  C. A. Micchelli  M. Pontil  and Y. Ying. A spectral regularization framework
for multi-task structure learning. In Advances in Neural Information Processing Systems 20 
pages 25–32. 2008.

[4] B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal

of Machine Learning Research  4:83–99  2003.

[5] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: A geometric framework
for learning from labeled and unlabeled examples. Journal of Machine Learning Research 
7:2399–2434  December 2006.

[6] S. Ben-David  J. Gehrke  and R. Schuller. A theoretical framework for learning from a pool of
disparate data sources. In Proceedings of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining  pages 443–449  2002.

[7] S. Ben-David and R. Schuller. Exploiting task relatedness for mulitple task learning. In Con-

ference on Learning Theory  pages 567–580  2003.

[8] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[9] A. Carlson  J. Betteridge  R. C. Wang  E. R. Hruschka  Jr.  and T. M. Mitchell. Coupled semi-
supervised learning for information extraction. In Proceedings of the third ACM international
conference on Web search and data mining  pages 101–110  2010.

[10] O. Chapelle  B. Sch¨olkopf  and A. Zien  editors. Semi-Supervised Learning. MIT Press  2006.
[11] A. Defant and K. Floret. Tensor Norms and Operator Ideals. North-Holland Mathematics

Studies  North-Holland  Amsterdam  1993.

[12] T. Evgeniou  C. A. Micchelli  and M. Pontil. Learning multiple tasks with kernel methods.

Journal of Machine Learning Research  6:615–637  2005.

[13] G. H. Golub and C. F. V. Loan. Matrix computations. Johns Hopkins University Press  3rd

edition  1996.

[14] L. Jacob  F. Bach  and J.-P. Vert. Clustered multi-task learning: A convex formulation.

Advances in Neural Information Processing Systems 21  pages 745–752. 2009.

In

[15] J. Lafferty and L. Wasserman. Statistical analysis of semi-supervised regression. In Advances

in Neural Information Processing Systems 20  pages 801–808  2007.

[16] J. M. Lee. Introduction to Smooth Manifolds. Springer Verlag  New York  2nd edition  2003.
[17] B. Lin  C. Zhang  and X. He. Semi-supervised regression via parallel ﬁeld regularization. In

Advances in Neural Information Processing Systems 24  pages 433–441. 2011.

[18] Q. Liu  X. Liao  and L. Carin. Semi-supervised multitask learning. In Advances in Neural

Information Processing Systems 20  pages 937–944. 2008.

[19] F. Wang  X. Wang  and T. Li. Semi-supervised multi-task learning with task regularizations.
In Proceedings of the 2009 Ninth IEEE International Conference on Data Mining  pages 562–
568. IEEE Computer Society  2009.

[20] Y. Xue  X. Liao  L. Carin  and B. Krishnapuram. Multi-task learning for classiﬁcation with

dirichlet process priors. Journal of Machine Learning Research  8:35–63  2007.

9

,Leon Gatys
Alexander Ecker
Matthias Bethge