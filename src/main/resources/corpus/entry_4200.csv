2018,PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits,We address the problem of regret minimization in logistic contextual bandits  where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables  we propose an improved version of Thompson Sampling  a Bayesian formulation of contextual bandits with near-optimal performance. Our approach  Polya-Gamma augmented Thompson Sampling (PG-TS)  achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms  quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of Polya-Gamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.,PG-TS: Improved Thompson Sampling for Logistic

Contextual Bandits

Lewis Sigler Institute for Integrative Genomics

Department of Computer Science

Bianca Dumitrascu∗

Princeton University
Princeton  NJ 08540

biancad@princeton.edu

Karen Feng∗

Princeton University
Princeton  NJ 08540

karenfeng@princeton.edu

Barbara E Engelhardt

Department of Computer Science

Princeton University
Princeton  NJ 08540
bee@princeton.edu

Abstract

We address the problem of regret minimization in logistic contextual bandits  where
a learner decides among sequential actions or arms given their respective contexts
to maximize binary rewards. Using a fast inference procedure with Pólya-Gamma
distributed augmentation variables  we propose an improved version of Thompson
Sampling  a Bayesian formulation of contextual bandits with near-optimal perfor-
mance. Our approach  Pólya-Gamma augmented Thompson Sampling (PG-TS) 
achieves state-of-the-art performance on simulated and real data. PG-TS explores
the action space efﬁciently and exploits high-reward arms  quickly converging to
solutions of low regret. Its explicit estimation of the posterior distribution of the
context feature covariance leads to substantial empirical gains over approximate
approaches. PG-TS is the ﬁrst approach to demonstrate the beneﬁts of Pólya-
Gamma augmentation in bandits and to propose an efﬁcient Gibbs sampler for
approximating the analytically unsolvable integral of logistic contextual bandits.

1

Introduction

A contextual bandit is an online learning framework for modeling sequential decision-making
problems. Contextual bandits have been applied to problems ranging from advertising [1] and
recommendations [22  21] to clinical trials [37] and mobile health [33]. In a contextual bandit
algorithm  a learner is given a choice among K actions or arms  for which contexts are available
as d-dimensional feature vectors  across T sequential rounds. During each round  the learner uses
information from previous rounds to estimate associations between contexts and rewards. The
learner’s goal in each round is to select the arm that minimizes the cumulative regret  which is the
difference between the optimal oracle rewards and the observed rewards from the chosen arms. To
do this  the learner must balance exploring arms that improve the expected reward estimates and
exploiting the current expected reward estimates to select arms with the largest expected reward. In
this work  we focus on scenarios with binary rewards.
To address the exploration-exploitation trade-off in sequential decision making  two directions are
generally considered: Upper Conﬁdence Bound algorithms (UCB) and Thompson Sampling (TS).

∗indicates equal authorship

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

UCB algorithms are based on the principle of optimism in the face of adversity [3  6  15] and rely
on choosing actions according to expected rewards perturbed by their respective upper conﬁdence
bounds. Based on Bayesian ideas  TS [34] assumes a prior distribution over the parameters governing
the relationship between contexts and rewards. At each step  an action corresponding to a random
parameter sampled from the posterior distribution is chosen. Upon observing the reward for each
round  the posterior distribution is updated via Bayes’ rule. TS has been successfully applied in a
wide range of settings [2  32  9  28].
While UCB algorithms have simple implementations and good theoretical regret bounds [22]  TS
achieves better empirical performance in many simulated and real-world settings without sacriﬁcing
simplicity [9  15]. Furthermore  TS is amenable to scaling through hashing  thus making it attractive
for large scale applications [20]. In addition  recent studies have bridged the theoretical gap between
TS and UCB based methods by analyzing regret and Bayesian regret in TS approaches for both
generalized linear bandits and reinforcement learning settings [2  28  26  29  4  5].
In this work  we focus on improving the TS approach for contextual bandits with logistic rewards
[9  15]. The logistic rewards setting is of pragmatic interest because of its natural application to
problems such as modeling click-through rates in advertisement applications [22]. Computationally 
the functional form of its logistic regression likelihood leads to an intractable posterior – the necessary
integrals are not available in closed form and difﬁcult to approximate. This intractability makes
the sampling step of TS with binary or categorical rewards challenging. From an optimization
perspective  the logistic loss is exp-concave  thus allowing second-order methods in a purely online
setting [19  25]. However  the convergence rate is exponential in the number of features d  making
these solutions impractical in most real-world settings [19].
Existing Bayesian solutions to logistic contextual bandits rely on regularized logistic regression
with batch updates in which the posterior distribution is estimated via Laplace approximations. The
Laplace approximation is a second-order moment matching method that estimates the posterior with
a multivariate Gaussian distribution. Despite offering asymptotic convergence guarantees under
restricted assumptions [7]  the Laplace approximation struggles when the dimension of the context
(arm features) is larger than the number of arms  and when the features themselves are non-Gaussian.
Both of these situations arise in the online learning setting  creating a need for novel TS approaches
to inference. Recent work suggests that a double sampling approach via MCMC can improve TS [35].
This approach provides MCMC schemes for bandits with binary and Gaussian rewards  but these
algorithms do not generalize to the logistic contextual bandit.
We propose Pólya-Gamma augmented Thompson sampling (PG-TS)  a fully Bayesian alternative
to Laplace-TS. PG-TS uses a Gibbs sampler built on parameter augmentation with a Pólya-Gamma
distribution [27  36  31]. We compare results from PG-TS to state-of-the-art approaches on simula-
tions that include toy models with speciﬁed and unspeciﬁed priors  and on two data sets previously
considered in the contextual bandit literature.
The remainder of this paper is organized as follows. Section 2 reviews relevant background and
introduces the problem. The details of Pólya-Gamma augmentation are provided in Section 3. Section
4 includes an empirical evaluation and shows substantial performance improvements in favor of
PG-TS over existing approaches. We conclude in Section 5.

2 Background
In the following  x ∈ Rd denotes a d-dimensional column vector with scalar entries xj  indexed by
integers j = {1  2 . . . d}; x(cid:62) is transposed vector x. X denotes a square matrix  while X refers to a
random variable. We use (cid:107) · (cid:107) for the 2-norm  while (cid:107)x(cid:107)A denotes x(cid:62)Ax  for a matrix A. Let 1B(x)
be the indicator function of a set B deﬁned as 1 if x ∈ B  and 0 otherwise. M V N (b  B) denotes a
multivariate normal distribution with mean b and covariance B  and Id is the d × d identity matrix.

2.1 Contextual Bandits with Binary Rewards

We consider contextual bandits with binary rewards with a ﬁnite  but possibly large  number of arms
K. These models belong to the class of generalized linear bandits with binary rewards [15]. Let
A be the set of arms. At each time step t  the learner observes contexts xt a ∈ Rd  where d is the
number of features per arm. The learner then chooses an arm at and receives a reward rt ∈ {0  1}.

2

The expectation of this reward is related to the context through a parameter θ∗ ∈ Rd and a logistic
link function µ: E[r|x] = µ(x(cid:62)θ∗)  where µ(z) = exp(z)/(1 + exp(z)).
For example  in a news article recommendation setting  the recommendation algorithm (learner) has
access to a discrete number of news articles (arms) A and interacts with users across discrete trials
t = 1  2  . . . where the logistic reward is whether or not the user clicks on the recommended article.
The articles and the users are characterized by attributes (context)  such as genre and popularity
(articles)  or age and gender (users). At trial t  the learner observes the current user ut  the available
articles a ∈ A  and the corresponding contexts xt a. The context is a d-dimensional summary of both
the user’s and the available articles’ context. At each time point  the goal of the learner is to provide
the user with an article recommendation (arm choice) that they then may choose to click (reward
of 1) or not (reward of 0). The relationship between rewards and contexts is mediated through an
underlying coefﬁcient vector θ∗  which can be interpreted as an encoding of the users’ preferences
with respect to the various context features of the articles.
Formally  let Dt be the set of triplets (xi ai  ai  ri) for i = 1  . . .   t representing the past t observations
of the contexts  the actions chosen  and their corresponding rewards. The objective of the learner is to
minimize the cumulative regret given Dt−1 after a ﬁxed budget of t steps. The regret is the expected
difference between the optimal reward received by always playing the optimal arm a∗ and the reward
received following the actual arm choices made by the learner.
i a∗ θ∗) − µ(x(cid:62)

t(cid:88)

µ(x(cid:62)

θ∗)

rt =

(cid:104)

(cid:105)

(1)

i ai

i=1

The parameter θ is reestimated after each round t using a generalized linear model estimator [15]  The
point estimate of the coefﬁcient at round t  θt  can be computed using approaches for online convex
optimization [18  19]. However  these approaches scale exponentially with the context dimension
d  leading to computationally intractable solutions for many real world contextual logistic bandit
problems [19  25].

2.2 Thompson Sampling for Contextual Logistic Bandits

rule and is proportional to the distribution(cid:81)t−1

TS provides a ﬂexible and computationally tractable framework for inference in contextual logistic
bandits. TS for the contextual bandit is broadly deﬁned in Bayesian terms  where a prior distribution
p(θ) over the parameter θ is updated iteratively using a set of historical observations Dt−1 =
{(xi ai  ai  ri)|i = 1  . . .   t − 1}. The posterior distribution p(θ|Dt−1) is calculated using Bayes’
i=1 p(ri|ai  xi ai  θ)p(θ). A random sample θt is drawn
from this posterior  corresponding to a stochastic estimate of θ∗ after t steps. The optimal arm is then
the arm offering the highest reward with respect to the current estimate θt. In other words  the arm
with the highest expected reward is chosen according to a probability p(at = a|θt Dt−1)  which is
expressed formally as

(cid:16)

(cid:17)

1Amax

t

(θt)

E[rt|a  xt a  θt]

p(θt|Dt−1)dθt 

(2)

where Amax
After t steps  the joint probability mass function over the rewards r1  r2  . . .   rt observed upon taking

(θt) is the set of arms with maximum rewards at step t if the true parameter were θt.

(cid:90)
actions a1  a2  . . .   at is(cid:81)t

t

i=1 p(ri = 1|ai  xi a  θi) or
t(cid:89)

µ(x(cid:62)

i ai

θi)ri[1 − µ(x(cid:62)

i ai

θi)]1−ri 

(3)

i=1

where θ1  θ2  . . .   θt are the estimates of θ∗ at each trial up to t.
In the case of logistic regression for binary rewards  the posterior derived from this joint probability
is intractable. Laplace-TS addresses this issue by approximating the posterior with a multivariate
Gaussian distribution with a diagonal covariance matrix following a Laplace approximation. The
mean of this distribution is the maximum a posteriori estimate and the inverse variance of each feature
is the curvature [15].
Laplace approximations are effective in ﬁnding smooth densities peaked around their posterior modes 
and are thus applicable to the logistic posterior  which is strictly exp-concave [18]. This approach has

3

shown superior empirical performance versus UCB algorithms [9] and other TS-based approximation
methods [30]. Laplace-TS is therefore an appropriate benchmark in the evaluation of contextual
bandit algorithms using TS approaches.

3 Pólya-Gamma Augmentation for Logistic Contextual Bandits

The Laplace approximation leads to simple  iterative algorithms  which in the ofﬂine setting lead to
accurate estimates across a potentially large number of sparse models [7]. In this section  we propose
PG-TS  an alternative approach stemming from recent developments in augmentation for Bayesian
inference in logit models [27  31].

3.1 The Pólya-Gamma Augmentation Scheme
Consider a logit model with t binary observations ri ∼ Bin(1  µ(x(cid:62)
i θ))  parameter θ ∈ Rd and
corresponding regressors xi ∈ Rd  i = 1  . . .   t. To estimate the posterior p(θ|Dt)  classic MCMC
methods use independent and identically distributed (i.i.d) samples. Such samples can be challenging
to obtain  especially if the dimension d is large [10]. To address this challenge  we reframe the discrete
rewards as functions of latent variables with Pólya-Gamma (PG) distributions over a continuous
space [27]. The PG latent variable construction relies on the theoretical properties of PG random
variables to exploit the fact that the logistic likelihood is a mixture of Gaussians with PG mixing
distributions [27  12  13].

Deﬁnition 1 Let X be a real-valued random variable. X follows a Pólya-Gamma distribution with
parameters b > 0 and c ∈ R  X ∼ P G(b  c) if the following holds:

∞(cid:88)

k=1

X =

1
2π2

Gk

(k − 1/2)2 + c2/(4π2)

 

where Gk ∼ Ga(b  1) are independent gamma variables.
The identity central to the PG augmentation scheme [27] is

(cid:90) ∞

0

(cid:90) ∞

0

t(cid:89)

i=1

t(cid:89)

i=1

(eψ)a

(1 + eψ)b = 2−beκψ

e−ωψ2/2p(ω)dω 

(4)

where ψ ∈ R  a ∈ R  b > 0  κ = a − b/2 and ω ∼ P G(b  0). When ψ = x(cid:62)
allows us to write the logistic likelihood contribution of step t as

t θ  the previous identity

Lt(θ) =

[exp(x(cid:62)
t θ)]rt
1 + exp(x(cid:62)
t θ)

∝ exp(κtx(cid:62)

t θ)

exp[−ωt(x(cid:62)

t θ)2/2]p(ωt; 1  0)dωt 

where κt = rt−1/2 and p(ωt; 1  0) is the density of a PG-distributed random variable with parameters
(1  0). In turn  the conditional posterior of θ given latent variables ω = [ω1  . . .   ωt] and past rewards
r = [r1  . . .   rt] is a conditional Gaussian:

p(θ|ω  r) = p(θ)

Li(θ|ωi) ∝ p(θ)

exp{ ωi
2

(x(cid:62)

i θ − κi/ωi)2}.

With a multivariate Gaussian prior for θ ∼ M V N (b  B)  this identity leads to an efﬁcient Gibbs
sampler. The main parameters are drawn from a Gaussian distribution  which is parameterized with
latent variables drawn from the PG distribution [27]. The two steps are:

(ωi|θ) ∼ P G(1  x(cid:62)
i θ)
(θ|r  ω) ∼ N (mω  Vω) 

(5)
(6)

with Vω = (X(cid:62)ΩX + B−1)−1  and mω = Vω(X(cid:62)κ + B−1b) where κ = [κ1  . . .   κt].
Conveniently  efﬁcient algorithms for sampling from the PG distribution exist [27]. Based on ideas
from Devroye [12  13]  which avoid the need to truncate the inﬁnite sum in Eq 4  the algorithm relies
on an accept-reject strategy for which the proposal distribution only requires exponential  uniform 

4

and Gaussian random variables. With an acceptance probability uniformly lower bounded by 0.9992
(at most 9 rejected draws out of every 10  000 proposed)  the resulting algorithm is more efﬁcient than
all previously proposed augmentation schemes in terms of both effective sample size and effective
sampling rate [27]. Furthermore  the PG sampling procedure leads to a uniformly ergodic mixture
transition distribution of the iterative estimates {θi}∞
i=0 [10]. This result guarantees the existence
of central limit theorems regarding sample averages involving {θi}∞
i=0 and allows for consistent
estimators of the asymptotic variance. The advantage of PG augmentation has been proven in multiple
Gibbs sampling and variational inference approaches  including binomial models [27]  multinomial
models [24]  and negative binomial regression models with logit link functions [38  31]. In the next
section  we leverage its strengths to perform online  fully Bayesian inference for logistic contextual
bandits with state-of-the-art performance.

3.2 PG-TS Algorithm Deﬁnition

2   . . .   rt − 1

Algorithm 1 PG-TS

Our algorithm  PG-TS  uses the PG augmentation scheme to represent the binomial distributions of
the sequential rewards in terms of latent variables with Gaussian distributions to perform tractable
Bayesian logistic regression in a Thompson sampling setting.
We consider a multivariate Gaussian distribution over parameter θ ∼ M V N (b  B) with prior mean
b and covariance B. For simplicity  let Xt be the d × t design matrix [x1  . . .   xt] that includes the
context of all arms chosen up to round t. Ωt is the diagonal matrix of the PG auxiliary variables
[ω1  . . .   ωt] and let κt = [r1 − 1
2 ]. Further  let rt = [r1  . . .   rt] be the history of rewards.
The PG-TS algorithm uses a Gibbs sampler
based on the PG augmentation scheme to ap-
proximate the logistic likelihood corresponding
to observations up to the current step. At each
step  sampling from the posterior is exact. The
ergodicity of the sampler guarantees that  as the
number of trials increases  the algorithm is able
to consistently estimate both the mean and the
variance of parameter θ [36].
We sample from the PG distribution [24  27] in-
cluding M = 100 burn-in steps. This number
is empirically tuned  as evaluating how close a
sampled θt is to the true GLM estimator θGLM
as a function of the burn-in step M is a challeng-
ing problem. Thus  frequentist-derived TS algo-
rithms and regret bounds cannot be derived for
the PG distributions  unlike other formulations
of this problem [2]. In our empirical studies  we
ﬁnd PG-TS with M = 100 to be sufﬁcient for
reliable mixing  as measured by the competitive
regret achieved. When M = 1  the resulting
algorithm  PG-TS-stream  is reminiscent of a
streaming Gibbs inference scheme. In practice 
this leads to a faster algorithm. As shown in
the Results  PG-TS-stream shows competitive performance in terms of cumulative rewards in both
simulated and real-world data scenarios.

Input: b  B  M  D = ∅  θ0 ∼ M V N (b  B)
for t = 1  2  . . . do
Receive contexts xt a ∈ Rd
t ← θt−1
θ(0)
for m = 1 to M do
for i = 1 to t − 1 do
ωi|θ(m−1)
(cid:104)

θt ← θ(M )
Select arm at ← argmaxaµ(x(cid:62)
Observe reward rt ∈ {0  1}
D = D ∪ {xt at  at  rt}

∼ P G(1  x(cid:62)

θ(m−1)

t aθt)

t

t

)

(cid:105)(cid:62)

t

i ai

t
Ωt−1 = diag(ω1  ω2  . . . ωt−1)
2   ...  rt−1 − 1
κt−1 =
Vω ← (X(cid:62)
mω ← Vω(X(cid:62)κt−1 + B−1b)
θ(m)
t

r1 − 1
t−1Ωt−1Xt−1 + B−1)−1

|rt−1  ω ∼ M V N (mω  Vω)

2

4 Results of PG-TS for contextual bandit applications

We evaluate and compare our PG-TS method with Laplace-TS. Laplace-TS has been shown to
outperform its UCB competitors in all settings considered here [9].
We evaluate our algorithm in three scenarios: simulated data sets with parameters sampled from
Gaussian and mixed Gaussian distributions  a toy data set based on the Forest Cover Type data set
from the UCI repository [15]  and an ofﬂine evaluation method for bandit algorithms that relies on
real-world log data [23].

5

4.1 Generating Simulated Data

Figure 1: Comparison of the average cumulative re-
gret of the PG-TS  PG-TS-stream  and Laplace-TS
algorithms on the simulated data set with Gaussian
θ∗ over 100 runs with 1  000 trials (standard devi-
ation shown as shaded region). Both PG-TS and
PG-TS-stream outperform Laplace-TS in consis-
tently achieving lower cumulative regret.

Gaussian simulation. We generated a simu-
lated data set with 100 arms and 10 features per
context across 1  000 trials. We generated con-
texts xt a ∈ R10 from multivariate Gaussian dis-
tributions xt a ∼ M V N (−3  I10) for all arms
a. The true parameters were simulated from a
multivariate Gaussian with mean 0 and identity
covariance matrix  θ∗ ∼ M V N (0  I10). The
resulting reward associated with the optimal arm
was 0.994 and the mean reward was 0.195. We
set the hyperparameters b = 0  and B = I10.
We averaged the experiments over 100 runs.
We ﬁrst considered the effect of the burn-in pa-
rameter M on the resulting average cumulative
regret (Eq. 1; Fig. S1 Supplementary Material).
As expected  larger M led to lower regret  as
the Markov chain had more time to mix. We
note that M > 100 burn-in iterations was not
noticeably better than M = 100  while the com-
putational time grew. Interestingly  the average cumulative regret of PG-TS-stream with M = 1 was
comparable to that of PG-TS. This suggests that  after a number of steps greater than the number of
iterations necessary for mixing  the sampler in PG-TS-stream has had sufﬁcient time to mix.
In this simulation  both PG-TS strategies outperformed their Laplace counterpart  which failed to
converge on average (Fig. 1). This behavior is expected: due to its simple Gaussian approximation 
Laplace-TS does not always converge to the global optimum of the logistic likelihood in non-
asymptotic settings.
Furthermore  the PG-TS algorithms outperform Laplace-TS in terms of balancing exploration and
exploitation: Laplace-TS gets stuck on sub-optimal arm choices  while PG-TS continues to explore
relative to the estimated variance of the posterior distribution of θ to ﬁnd the optimal arm (Fig. 2).
Mixture of Gaussians: Prior misspeciﬁcation. Laplace approximations are sensitive to multimodal-
ity. We therefore explored a prior misspeciﬁcation scenario  where true parameter θ∗ is sampled from
a four-component Gaussian mixture model  as opposed to the Gaussian distribution assumed by both
algorithms. As before  we simulated a data set with 100 arms  each with 10 features  and marginally
independent contexts xt a ∼ M V N (0  I10)  across 5  000 trials.
The true parameters were generated from a mixed model with variances σ2
Inverse-Gamma(3  1)  means µj=1:4 ∼ N (−3  σ2

such that θ∗(i) ∼ (cid:80)4

j=1:4 ∼
j )  and mixture weights φ ∼ Dirichlet(1  3  5  7)
10]. The reward associated with
the optimal arm was 0.999 and the mean reward was 0.306. We found that the misspeciﬁed model
does not prevent the PG-TS algorithms from consistently ﬁnding the correct arm  while Laplace-TS
exhibits poor average behavior (Fig. S3 Supplementary Materials).

j=1 φjN (µj  σ2

j )  with θ∗ = [θ∗

1  θ∗

2  . . .   θ∗

4.2 PG-TS applied to Forest Cover Type Data

We further compared these methods using the Forest Cover Type data from the UCI Machine Learning
repository [8]. These data contain 581  021 labeled observations from regions of a forest area. The
labels indicate the dominant species of trees (cover type) in each region. Following the preprocessing
pipeline proposed by [15]  we centered and standarized the 10 non-categorical variables and added a
constant covariate; we then partitioned the 581  012 samples into k = 32 clusters using unsupervised
mini-batch k-means clustering. We took the cluster centroids to be the contexts corresponding to
each of our arms. To ﬁt the logistic reward model  rewards were binarized for each data point by
associating the ﬁrst class "Spruce/Fir" to a reward of 1  and to a reward of 0 otherwise. We then set
the reward for each arm to be the average reward of the data points in the corresponding cluster; these
ranged from 0.020 to 0.579. The task then becomes the problem of ﬁnding the cluster with the highest

6

Figure 2: Comparison of arm choices for the PG-TS (Left) and Laplace-TS algorithms (Right)
on simulated data with Gaussian θ∗ across 1  000 trials. Arms were sorted by expected reward in
decreasing order  with arm 0 giving the highest reward  and arm 99 the lowest. The selected arms are
colored by the distance of their expected reward from the optimal reward (regret). Laplace-TS gets
stuck on a sub-optimal arm  while PG-TS explores successfully and settles on the optimal one.

Figure 3: Left: comparison of the average cumulative regret of the PG-TS  PG-TS-stream  Laplace-
TS  and GLM-UCB algorithms on the Forest Cover Type data over 100 runs with 1  000 trials (one
standard deviation shaded). PG-TS signiﬁcantly outperforms Laplace-TS and GLM-UCB  with slight
improvement over PG-TS-stream. Right: median frequencies of the six best arms’ draws. The arms
were sorted by expected reward in decreasing order  with arm 0 giving the highest reward  and arm 5
the lowest. PG-TS explores better than Laplace-TS  which gets stuck in a sub-optimal arm.

proportion of Spruce/Fir forest cover in a setting with 32 arms and 11 context features. As a baseline 
we implemented the generalized linear model upper conﬁdence bound algorithm (GLM-UCB) [15].
On this forest cover task  the PG-TS algorithms show improved cumulative regret with respect to both
the Laplace-TS and the GLM-UCB procedures  with PG-TS performing slightly better of the two
(Fig. 3). Both PG-TS and PG-TS-stream explored the arm space more successfully  and exploited
high-reward arms with a higher frequency than their competitors (Fig. 3).

4.3 PG-TS Applied to News Article Recommendation

We evaluated the performance of PG-TS in the context of news article recommendations on the public
benchmark Yahoo! Today Module data through an unbiased ofﬂine evaluation protocol [22]. As
before  users are assumed to click on articles in an i.i.d manner. Available articles represent the pool
of arms  the binary payoff is whether a user clicks on a recommended article  and the expected payoff
of an article is the click-through rate (CTR). Our goal is to choose the article with the maximum
expected CTR at each visit  which is equivalent to maximizing the total expected reward. The full data
set contains 45  811  883 user visits from the ﬁrst 10 days of May 2009; for each user visit  the module
features one article from a changing pool of K ≈ 20 articles  which the user either clicks (r = 1) or
does not click (r = 0). We use 200  000 of these events in our evaluation for efﬁciency; ≤ 24  000

7

02004006008001000Trial020406080Arm0.00.20.40.60.8Regret02004006008001000Trial020406080100Arm0.00.20.40.60.8RegretFigure 4: Comparison of the average click-through rate (CTR) achieved by the PG-TS  PG-TS-stream 
and Laplace-TS algorithms with 10-minute delay (Left) and with varying delay (Right) on 24  000
events in the Yahoo! Today Module data set over 20 runs. Left: the moving average CTR is observed
every 1  000 observations. Right: the standard deviation of the average CTR is shown. PG-TS
achieves higher CTR across all delays  especially for short delays.

of these are valid events for each of our evaluated algorithms. Each article is associated with a
feature vector (context) x ∈ R6 including a constant feature capturing an intercept  preprocessed
using a conjoint analysis with a bilinear model [11]; note that we do not use user features as context.
In this evaluation  we maintain separate estimates θa for each arm. We also update the model in
batches (groups of observations across time delays) to better match the real-world scenario where
computation is expensive and delay is necessary. In all settings  PG-TS consistently and signiﬁcantly
out-performs the Laplace-TS approach (Fig. 4). In particular  PG-TS shows a signiﬁcant improvement
in CTR across all delays. Note that PG-TS beneﬁts in performance in particular with short delays.
Despite showing only marginal improvement when compared to Laplace-TS  PG-TS-stream offers
the advantage of a ﬂexible  fast data streaming approach without compromising performance on this
task.

5 Discussion

We introduced PG-TS  a fully Bayesian algorithm based on the Pólya-Gamma augmentation scheme
for contextual bandits with logistic rewards. This is the ﬁrst method where Pólya-Gamma augmen-
tation is leveraged to improve bandit performance. Our approach addresses two deﬁciencies in
current methods. First  PG-TS provides an efﬁcient online approximation scheme for the analytically
intractable logistic posterior. Second  because PG-TS explicitly estimates context feature covariances 
it is more effective in balancing exploration and exploitation relative to Laplace-TS  which assumes
independence of each context feature. We showed through extensive evaluation in both simulated
and real-world data that our approach offers improved empirical performance while maintaining
comparable computational costs by leveraging the simplicity of the Thompson sampling framework.
We plan to extend our framework to address computational challenges in high-dimensional data via
hash-amenable extensions [20].
Motivated by our results and by recent work on PG inference in dependent multinomial models [24] 
we aim to extend our work to the problem of multi-armed bandits with categorical rewards. We
further envision a generalization of this approach to sampling in bandit problems where additional
structure is imposed on the contexts; for example  settings where arm contexts are sampled from
dynamic linear topic models [17]  or settings in which social network information is available for
users and contexts [16].
Future work will address the more general reinforcement learning setting of Bayes-Adaptive MDP
with discrete state and action sets [14]. In this case  the state transition probabilities are multinomial
distributions; therefore  our online Pólya-Gamma Gibbs sampling procedure can be extended to
approximate the emerging intractable posteriors.

8

Acknowledgments

We would like to thank Scott Linderman  Diana Cai  and Jean Feng for insightful discussions and
their helpful feedback. Lastly  we thank all the anonymous reviewers for their valuable comments.

References
[1] N. Abe and A. Nakamura. Learning to optimally schedule internet banner advertisements. In

ICML  volume 99  pages 12–21  1999.

[2] M. Abeille and A. Lazaric. Linear Thompson Sampling Revisited. In AISTATS 2017-20th

International Conference on Artiﬁcial Intelligence and Statistics  2017.

[3] R. Agrawal. Sample mean based index policies by o (log n) regret for the multi-armed bandit

problem. Advances in Applied Probability  27(4):1054–1078  1995.

[4] S. Agrawal and N. Goyal. Thompson Sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning  pages 127–135  2013.

[5] S. Agrawal and N. Goyal. Near-Optimal Regret Bounds for Thompson Sampling. Journal of

the ACM (JACM)  64(5):30  2017.

[6] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

[7] R. F. Barber  M. Drton  and K. M. Tan. Laplace approximation in high-dimensional Bayesian

regression. In Statistical Analysis for High-Dimensional Data  pages 15–36. Springer  2016.

[8] S. D. Bay  D. Kibler  M. J. Pazzani  and P. Smyth. The uci kdd archive of large data sets for
data mining research and experimentation. ACM SIGKDD Explorations Newsletter  2(2):81–85 
2000.

[9] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in neural

information processing systems  pages 2249–2257  2011.

[10] H. M. Choi  J. P. Hobert  et al. The Polya-Gamma Gibbs sampler for Bayesian logistic regression

is uniformly ergodic. Electronic Journal of Statistics  7:2054–2064  2013.

[11] W. Chu  S. taek Park  T. Beaupre  N. Motgi  A. Phadke  S. Chakraborty  and J. Zachariah. A
case study of behavior-driven conjoint analysis on Yahoo! Front Page Today module. In Proc.
of KDD  2009.

[12] L. Devroye. Introduction. In Non-Uniform Random Variate Generation  pages 1–26. Springer 

1986.

[13] L. Devroye. On exact simulation algorithms for some distributions related to Jacobi theta

functions. Statistics & Probability Letters  79(21):2251–2259  2009.

[14] M. O. Duff. Design for an optimal probe. In Proceedings of the 20th International Conference

on Machine Learning (ICML-03)  pages 131–138  2003.

[15] S. Filippi  O. Cappe  A. Garivier  and C. Szepesvári. Parametric bandits: The generalized linear

case. In Advances in Neural Information Processing Systems  pages 586–594  2010.

[16] C. Gentile  S. Li  and G. Zappella. Online clustering of bandits. In International Conference on

Machine Learning  pages 757–765  2014.

[17] C. Glynn  S. T. Tokdar  D. L. Banks  and B. Howard. Bayesian Analysis of Dynamic Linear

Topic Models. arXiv preprint arXiv:1511.03947  2015.

[18] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[19] E. Hazan  T. Koren  and K. Y. Levy. Logistic regression: Tight bounds for stochastic and online

optimization. In Conference on Learning Theory  pages 197–209  2014.

9

[20] K.-S. Jun  A. Bhargava  R. Nowak  and R. Willett. Scalable Generalized Linear Bandits: Online

Computation and Hashing. arXiv preprint arXiv:1706.00136  2017.

[21] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in neural information processing systems  pages 817–824  2008.

[22] L. Li  W. Chu  J. Langford  and R. E. Schapire. A Contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World
wide web  pages 661–670. ACM  2010.

[23] L. Li  W. Chu  J. Langford  and X. Wang. Unbiased ofﬂine evaluation of contextual-bandit-
based news article recommendation algorithms. In Proceedings of the fourth ACM International
Conference on Web search and Data Mining  pages 297–306. ACM  2011.

[24] S. Linderman  M. Johnson  and R. P. Adams. Dependent multinomial models made easy: Stick-
breaking with the pólya-gamma augmentation. In Advances in Neural Information Processing
Systems  pages 3456–3464  2015.

[25] H. B. McMahan and M. Streeter. Open problem: Better bounds for online logistic regression.

In Conference on Learning Theory  pages 44–1  2012.

[26] I. Osband and B. Van Roy. Bootstrapped Thompson sampling and deep exploration. arXiv

preprint arXiv:1507.00300  2015.

[27] N. G. Polson  J. G. Scott  and J. Windle. Bayesian inference for logistic models using Pólya-
Gamma latent variables. Journal of the American statistical Association  108(504):1339–1349 
2013.

[28] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research  39(4):1221–1243  2014.

[29] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. The

Journal of Machine Learning Research  17(1):2442–2471  2016.

[30] D. Russo  B. Van Roy  A. Kazerouni  and I. Osband. A Tutorial on Thompson Sampling. arXiv

preprint arXiv:1707.02038  2017.

[31] J. Scott and J. W. Pillow. Fully Bayesian inference for neural models with negative-binomial

spiking. In Advances in Neural Information Processing Systems  pages 1898–1906  2012.

[32] M. Strens. A Bayesian framework for reinforcement learning. In ICML  pages 943–950  2000.

[33] A. Tewari and S. A. Murphy. From ads to interventions: Contextual bandits in mobile health.

In Mobile Health  pages 495–517. Springer  2017.

[34] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[35] I. Urteaga and C. H. Wiggins. Bayesian bandits: balancing the exploration-exploitation tradeoff

via double sampling. arXiv preprint arXiv:1709.03162  2017.

[36] J. Windle  N. G. Polson  and J. G. Scott. Sampling Polya-Gamma random variates: alternate

and approximate techniques. arXiv preprint arXiv:1405.0506  2014.

[37] M. Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the

American Statistical Association  74(368):799–806  1979.

[38] M. Zhou  L. Li  D. Dunson  and L. Carin. Lognormal and gamma mixed negative binomial
regression. In Proceedings of the 29th International Conference on Machine Learning  volume
2012  page 1343. NIH Public Access  2012.

10

,Bianca Dumitrascu
Karen Feng
Barbara Engelhardt