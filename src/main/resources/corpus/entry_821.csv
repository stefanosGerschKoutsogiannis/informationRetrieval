2013,Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent,Discovering hierarchical regularities in data is a key problem in interacting   with large datasets  modeling cognition  and encoding knowledge. A previous   Bayesian solution---Kingman's coalescent---provides a convenient probabilistic   model for data represented as a binary tree. Unfortunately  this is   inappropriate for data better described by bushier trees. We generalize an   existing belief propagation framework of Kingman's coalescent to the   beta coalescent  which models a wider range of tree structures.   Because of the complex combinatorial search over possible structures  we   develop new sampling schemes using sequential Monte Carlo and Dirichlet   process mixture models  which render inference efficient and tractable.     We present results on both synthetic and real data that show the beta coalescent      outperforms Kingman's coalescent on real datasets and is qualitatively better at    capturing data in bushy hierarchies.,Binary to Bushy: Bayesian Hierarchical Clustering

with the Beta Coalescent

Yuening Hu1  Jordan Boyd-Graber2  Hal Daum`e III3  Z. Irene Ying4

1  3: Computer Science  2: iSchool and UMIACS  4: Agricultural Research Service

ynhu@cs.umd.edu  {jbg hal}@umiacs.umd.edu  zhu.ying@ars.usda.gov

1  2  3: University of Maryland  4: Department of Agriculture

Abstract

Discovering hierarchical regularities in data is a key problem in interacting with
large datasets  modeling cognition  and encoding knowledge. A previous Bayesian
solution—Kingman’s coalescent—provides a probabilistic model for data repre-
sented as a binary tree. Unfortunately  this is inappropriate for data better described
by bushier trees. We generalize an existing belief propagation framework of
Kingman’s coalescent to the beta coalescent  which models a wider range of tree
structures. Because of the complex combinatorial search over possible structures 
we develop new sampling schemes using sequential Monte Carlo and Dirichlet
process mixture models  which render inference efﬁcient and tractable. We present
results on synthetic and real data that show the beta coalescent outperforms King-
man’s coalescent and is qualitatively better at capturing data in bushy hierarchies.

1 The Need For Bushy Hierarchical Clustering

Hierarchical clustering is a fundamental data analysis problem: given observations  what hierarchical
grouping of those observations effectively encodes the similarities between observations? This is a
critical task for understanding and describing observations in many domains [1  2]  including natural
language processing [3]  computer vision [4]  and network analysis [5]. In all of these cases  natural
and intuitive hierarchies are not binary but are instead bushy  with more than two children per parent
node. Our goal is to provide efﬁcient algorithms to discover bushy hierarchies.
We review existing nonparametric probabilistic clustering algorithms in Section 2  with particular
focus on Kingman’s coalescent [6] and its generalization  the beta coalescent [7  8]. While Kingman’s
coalescent has attractive properties—it is probabilistic and has edge “lengths” that encode how
similar clusters are—it only produces binary trees. The beta coalescent (Section 3) does not have this
restriction. However  na¨ıve inference is impractical  because bushy trees are more complex: we need
to consider all possible subsets of nodes to construct each internal nodes in the hierarchy.
Our ﬁrst contribution is a generalization of the belief propagation framework [9] for beta coalescent to
compute the joint probability of observations and trees (Section 3). After describing sequential Monte
Carlo posterior inference for the beta coalescent  we develop efﬁcient inference strategies in Section 4 
where we use proposal distributions that draw on the connection between Dirichlet processes—a
ubiquitous Bayesian nonparametric tool for non-hierarchical clustering—and hierarchical coalescents
to make inference tractable. We present results on both synthetic and real data that show the beta
coalescent captures bushy hierarchies and outperforms Kingman’s coalescent (Section 5).

2 Bayesian Clustering Approaches

Recent hierarchical clustering techniques have been incorporated inside statistical models; this
requires formulating clustering as a statistical—often Bayesian—problem. Heller et al. [10] build

1

binary trees based on the marginal likelihoods  extended by Blundell et al. [11] to trees with arbitrary
branching structure. Ryan et al. [12] propose a tree-structured stick-breaking process to generate trees
with unbounded width and depth  which supports data observations at leaves and internal nodes.1
However  these models do not distinguish edge lengths  an important property in distinguishing how
“tight” the clustering is at particular nodes.
Hierarchical models can be divided into complementary “fragmentation” and “coagulation” frame-
works [7]. Both produce hierarchical partitions of a dataset. Fragmentation models start with a single
partition and divide it into ever more speciﬁc partitions until only singleton partitions remain. Coagu-
lation frameworks repeatedly merge singleton partitions until only one partition remains. Pitman-Yor
diffusion trees [13]  a generalization of Dirichlet diffusion trees [14]  are an example of a bushy
fragmentation model  and they model edge lengths and build non-binary trees.
Instead  our focus is on bottom-up coalescent models [8]  one of the coagulation models and
complementary to diffusion trees  which can also discover hierarchies and edge lengths. In this
model  n nodes are observed (we use both observed to emphasize that nodes are known and leaves to
emphasize topology). These observed nodes are generated through some unknown tree with latent
edges and unobserved internal nodes. Each node (both observed and latent) has a single parent. The
convention in such models is to assume our observed nodes come at time t = 0  and at time −∞ all
nodes share a common ur-parent through some sequence of intermediate parents.
Consider a set of n individuals observed at the present (time t = 0). All individuals start in one of n
singleton sets. After time ti  a set of these nodes coalesce into a new node. Once a set merges  their
parent replaces the original nodes. This is called a coalescent event. This process repeats until there
is only one node left  and a complete tree structure π (Figure 1) is obtained.
Different coalescents are deﬁned by different probabilities of merging a set of nodes. This is called
the coalescent rate  deﬁned by a general family of coalescents: the lambda coalescent [7  15]. We
represent the rate via the symbol λk
n  the rate at which k out of n nodes merge into a parent node.
From a collection of n nodes  k ≤ n can coalesce at some coalescent event (k can be different for
different coalescent events). The rate of a fraction γ of the nodes coalescing is given by γ−2Λ(dγ) 
where Λ(dγ) is a ﬁnite measure on [0  1]. So k nodes merge at rate

λk
n =

γk−2(1 − γ)n−kΛ(dγ)

(2 ≤ k ≤ n).

(1)

(cid:90) 1

0

Choosing different measures yields different coalescents. A degenerate Dirac delta measure at 0
results in Kingman’s coalescent [6]  where λk
n is 1 when k = 2 and zero otherwise. Because this
gives zero probability to non-binary coalescent events  this only creates binary trees.
Alternatively  using a beta distribution BETA(2 − α  α) as the measure Λ yields the beta coalescent.
When α is closer to 1  the tree is bushier; as α approaches 2  it becomes Kingman’s coalescent. If we
have ni−1 nodes at time ti−1 in a beta coalescent  the rate λki
ni−1 for a children set of ki nodes at time
ti and the total rate λni−1 of any children set merging—summing over all possible mergers—is

λki
ni−1

=

Γ(ki − α)Γ(ni−1 − ki + α)

Γ(2 − α)Γ(α)Γ(ni−1)

and λni−1 =

(cid:0)ni−1

ki

(cid:1)λki

ni−1(cid:88)

ki=2

.

(2)

ni−1

Each coalescent event also has an edge length—duration—δi. The duration of an event comes from
an exponential distribution  δi ∼ exp(λni−1)  and the parent node forms at time ti = ti−1 − δi.
Shorter durations mean that the children more closely resemble their parent (the mathematical basis
for similarity is speciﬁed by a transition kernel  Section 3).
Analogous to Kingman’s coalescent  the prior probability of a complete tree π is the product of all of
its constituent coalescent events i = 1  . . . m  merging ki children after duration δi 

m(cid:89)

i=1

(cid:124)

p(π) =

(cid:123)(cid:122)

(cid:125)

p(ki|ni−1)
Merge ki nodes

· p(δi|ki  ni−1)
After duration δi

=

(cid:123)(cid:122)

(cid:125)

(cid:124)

m(cid:89)

i=1

λki
ni−1

· exp(−λni−1δi).

(3)

1This is appropriate where the entirety of a population is known—both ancestors and descendants. We focus

on the case where only the descendants are known. For a concrete example  see Section 5.2.

2

if ns == 1 then

Continue.

0 = 1.

Update i = i + 1.
for Particle s = 1  2 ···   S do

Algorithm 1 MCMC inference for generating a tree
1: for Particle s = 1  2 ···   S do
Initialize ns = n  i = 0  ts
0 = 0  ws
2:
Initialize the node set V s = {ρ0  ρ1 ···   ρn}.
3:
4: while ∃s ∈ {1··· S} where ns > 1 do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Propose a duration δs
Set coalescent time ts
Sample partitions ps
Propose a set ρs
Update weight ws
Update ns = ns − |ρs
Remove ρs

i to V s.
Compute effective sample size ESS [16].
if ESS < S/2 then

i by Equation 10.
i−1 − δs
i .
i = ts
i from DPMM.

| + 1.
(cid:126)ci from V s  add ρs

(cid:126)ci according to Equation 11.

Resample particles [17].

i by Equation 13.

(cid:126)ci

3 Beta Coalescent Belief Propagation

(a) Kingman’s coalescent

(b) the beta coalescent

Figure 1: The beta coalescent can merge four simi-
lar nodes at once  while Kingman’s coalescent only
merges two each time.

The beta coalescent prior only depends on the topology of the tree. In real clustering applications  we
also care about a node’s children and features. In this section  we deﬁne the nodes and their features 
and then review how we use message passing to compute the probabilities of trees.
An internal node ρi is deﬁned as the merger of other nodes. The children set of node ρi  ρ(cid:126)ci  coalesces
into a new node ρi ≡ ∪b∈(cid:126)ciρb. This encodes the identity of the nodes that participate in speciﬁc
coalescent events; Equation 3  in contrast  only considers the number of nodes involved in an event.
In addition  each node is associated with a multidimensional feature vector yi.
Two terms specify the relationship between nodes’ features: an initial distribution p0(yi) and a
transition kernel κtitb (yi  yb). The initial distribution can be viewed as a prior or regularizer for
feature representations. The transition kernel encourages a child’s feature yb (at time tb) to resemble
feature yi (formed at ti); shorter durations tb − ti increase the resemblance.
Intuitively  the transition kernel can be thought as a similarity score; the more similar the features
are  the more likely nodes are. For Brownian diffusion (discussed in Section 4.3)  the transition
kernel follows a Gaussian distribution centered at a feature. The covariance matrix Σ is decided
by the mutation rate µ [18  9]  the probability of a mutation in an individual. Different kernels
(e.g.  multinomial  tree kernels) can be applied depending on modeling assumptions of the feature
representations.
To compute the probability of the beta coalescent tree π and observed data x  we generalize the
belief propagation framework used by Teh et al. [9] for Kingman’s coalescent; this is a more scalable
alternative to other approaches for computing the probability of a Beta coalescent tree [19]. We
deﬁne a subtree structure θi = {θi−1  δi  ρ(cid:126)ci}  thus the tree θm after the ﬁnal coalescent event m is a
complete tree π. The message for node ρi marginalizes over the features of the nodes in its children
set.2 The total message for a parent node ρi is
(x|θi)

Mρi(yi) = Z−1

κtitb (yi  yb)Mρb (yb)dyb.

(cid:89)

(cid:90)

(4)

ρi

where Zρi(x|θi) is the local normalizer  which can be computed as the combination of initial
distribution and messages from a set of children 

(cid:19)

(cid:90)

Zρi(x|θi) =

b∈(cid:126)ci

(cid:18)(cid:90)

(cid:89)

b∈(cid:126)ci

p0(yi)

κtitb (yi  yb)Mρb (yb)dyb

dyi.

(5)

2When ρb is a leaf  the message Mρb (yb) is a delta function centered on the observation.

3

Recursively performing this marginalization through message passing provides the joint probability
of a complete tree π and the observations x. At the root 

Z−∞(x|θm) =

p0(y−∞)κ−∞ tm(y−∞  ym)Mρm (ym)dymdy−∞

(6)

(cid:90)

where p0(y−∞) is the initial feature distribution and m is the number of coalescent events. This gives
the marginal probability of the whole tree 

p(x|π) = Z−∞(x|θm)

Zρi(x|θi) 

m(cid:89)

i=1

m(cid:89)

(7)

(8)

The joint probability of a tree π combines the prior (Equation 3) and likelihood (Equation 7) 

p(x  π) = Z−∞(x|θm)

λki
ni−1

exp(−λni−1 δi) · Zρi (x|θi).

3.1 Sequential Monte Carlo Inference

i=1

Sequential Monte Carlo (SMC)—often called particle ﬁlters—estimates a structured sequence of
hidden variables based on observations [20]. For coalescent models  this estimates the posterior
distribution over tree structures given observations x. Initially (i = 0) each observation is in a
singleton cluster;3 in subsequent particles (i > 0)  points coalesce into more complicated tree
structures θs
i   where s is the particle index and we add superscript s to all the related notations to
distinguish between particles. We use sequential importance resampling [21  SIR] to weight each
particle s at time ti  denoted as ws
i .
The weights from SIR approximate the posterior. Computing the weights requires a conditional distri-
bution of data given a latent state p(x|θs
i−1) 
and a proposal distribution f (θs

i )  a transition distribution between latent states p(θs

i−1  x). Together  these distributions deﬁne weights

i|θs

i|θs

ws

i = ws

i−1

p(x| θs
f (θs

i | θs
i−1  x)

i )p(θs
i | θs

i−1)

.

(9)

i takes a set of nodes ρs
(cid:126)ci
i−1  δs

Then we can approximate the posterior distribution of the hidden structure using the normalized
weights  which become more accurate with more particles.
To apply SIR inference to belief propagation with the beta coalescent prior  we ﬁrst deﬁne the particle
space structure. The sth particle represents a subtree θs
i−1  and a transition to a new
subtree θs
i   where ts
i and
i = {θs
i and the children set ρs
θs
(cid:126)ci
to merge based on the previous subtree θs
We propose the duration δs
i from the prior exponential distribution and propose a children set from the
posterior distribution based on the local normalizers. 4 This is the “priorpost” method in Teh et al. [9].
However  this approach is intractable. Given ni−1 nodes at time ti  we must consider all possible
i−1)

(cid:1). The computational complexity grows from O(n2

}. Our proposal distribution must provide the duration δs

children sets(cid:0)ni−1

i−1 at time ts
i−1  and merges them at ts

(cid:1) + ··· +(cid:0)ni−1

(cid:1) +(cid:0)ni−1

i−1 − δs

from θs

i = ts

i   ρs
(cid:126)ci

i−1.

2

3

ni−1

(Kingman’s coalescent) to O(2ni−1 ) (beta coalescent).

4 Efﬁciently Finding Children Sets with DPMM

We need a more efﬁcient way to consider possible children sets. Even for Kingman’s coalescent  which
only considers pairs of nodes  Gorur et al. [22] do not exhaustively consider all pairs. Instead  they
use data structures from computational geometry to select the R closest pairs as their restriction set 
reducing inference to O(n log n). While ﬁnding closest pairs is a traditional problem in computational
geometry  discovering arbitrary-sized sets is less studied.

3The relationship between time and particles is non-intuitive. Time t goes backward with subsequent particles.

When we use time-speciﬁc adjectives for particles  this is with respect to inference.

4This is a special case of Section 4.2’s algorithm  where the restriction set Ωi is all possible subsets.

4

In this section  we describe how we use a Dirichlet process mixture model [23  DPMM] to discover a
restriction set Ω  integrating DPMMs into the SMC proposal. We ﬁrst brieﬂy review what DPMMs are 
describe why they are attractive  and then describe how we incorporate DPMMs in SMC inference.
The DPMM is deﬁned by a concentration β and a base distribution G0. A distribution over mixtures is
drawn from a Dirichlet process (DP): G ∼ DP(β  G0). Each observation xi is assigned to a mixture
component µi drawn from G. Because the Dirichlet process is a discrete distribution  observations i
and j can have the same mixture component (µi = µj). When this happens  points are said to be in
the same partition. Posterior inference can discover a distribution over partitions. A full derivation of
these sampling equations appears in the supplemental material.

4.1 Attractive Properties of DPMMs

DPMMs and Coalescents Berestycki et al. [8] showed that the distribution over partitions in a
Dirichlet process is equivalent to the distribution over coalescents’ allelic partitions—the set of
members that have the same feature representation—when the mutation rate µ of the associated
kernel is half of the Dirichlet concentration β (Section 3). For Brownian diffusion  we can connect
DPMM with coalescents by setting the kernel covariance Σ = µI to Σ = β/2I.
The base distribution G0 is also related with nodes’ feature. The base distribution G0 of a Dirichlet
process generates the probability measure G for each block  which generates the nodes in a block.
As a result  we can select a base distribution which ﬁts the distribution of the samples in coalescent
process. For example  if we use Gaussian distribution for the transition kernel and prior  a Gaussian
is also appropriate as the DPMM base distribution.

Effectiveness as a Proposal The necessary condition for a valid proposal [24] is that it should have
support on a superset of the true posterior. In our case  the distribution over partitions provided by
the DPMM considers all possible children sets that could be merged in the coalescent. Thus the new
proposal with DPMM satisﬁes this requirement  and it is a valid proposal.
In addition  Chen [25] gives a set of desirable criteria for a good proposal distribution: accounts for
outliers  considers the likelihood  and lies close to the true posterior. The DPMM fulﬁlls these criteria.
First  the DPMM provides a distribution over all partitions. Varying the concentration parameter β can
control the length of the tail of the distribution over partitions. Second  choosing the base distribution
of the DPMM appropriately models the feature likelihood; i.e.  ensuring the DPMM places similar
nodes together in a partition with high probability. Third  the DPMM qualitatively provides reasonable
children sets when compared with exhaustively considering all children sets (Figure 2(c)).

4.2

Incorporating DPMM in SMC Proposals

To address the inference intractability in Section 3.1  we use the DPMM to obtain a distribution over
partitions of nodes. Each partition contains clusters of nodes  and we take a union over all partitions
to create a restriction set Ωi = {ωi1  ωi2 ···}  where each ωij is a subset of the ni−1 nodes. A
standard Gibbs sampler provides these partitions (see supplemental).
With this restriction set Ωi  we propose the duration time δs
propose a children set ρs
(cid:126)ci
exp(−λs

i from the exponential distribution and
Zρi (x|θs
i−1  δs
Z0

based on the local normalizers

(cid:3)   (11)

· I(cid:2)ρs

|δs
i   θs

i ) (10)
δs

i ) = λs

∈ Ωs

i−1) =

fi(ρs
(cid:126)ci

i   ρs
(cid:126)ci

ni−1

ni−1

fi(δs

(cid:126)ci

)

i

where Ωs
Zρi (x|θs

i restricts the candidate children sets  I is the indicator  and we replace Zρi(x|θs
i−1  δs

) since they are equivalent here. The normalizer is

i   ρs
(cid:126)ci

i ) with

Z0 =

Zρi (x|θs

i−1  δs

i   ρ(cid:48)

(cid:126)c) · I [ρ(cid:48)

(cid:126)c ∈ Ωs

i ] =

Zρi(x|θs

i−1  δs

i   ρ(cid:48)
(cid:126)c).

(12)

(cid:88)

(cid:126)c∈Ωs
ρ(cid:48)

i

(cid:88)

ρ(cid:48)

(cid:126)c

Applying the true distribution (the ith multiplicand from Equation 8) and the proposal distribution
(Equation 10 and Equation 11) to the SIR weight update (Equation 9) 
i   ρ(cid:48)
(cid:126)c)

ni−1 ·(cid:80)

i−1  δs

|ρs
(cid:126)ci

λ

|

 

(13)

ws

i = ws

i−1

Zρi(x|θs
(cid:126)c∈Ωs
ρ(cid:48)
λs

i

ni−1

5

where |ρs
tion 2); and λs

(cid:126)ci

| is the size of children set ρs

(cid:126)ci; parameter λ

|

|ρs
ni−1 is the rate of the children set ρs
(cid:126)ci
(cid:126)ci

(Equa-

ni−1 is the rate of all possible sets given a total number of nodes ni−1 (Equation 2).

We can view this new proposal as a coarse-to-ﬁne process: DPMM proposes candidate children
sets; SMC selects a children set from DPMM to coalesce. Since the coarse step is faster and ﬁlters
“bad” children sets  the slower ﬁner step considers fewer children sets  saving computation time
(Algorithm 1). If Ωi has all children sets  it recovers exhaustive SMC. We estimate the effective sample
size [16] and resample [17] when needed. For smaller sets  the DPMM is sometimes impractical (and
only provides singleton clusters). In such cases it is simpler to enumerate all children sets.

4.3 Example Transition Kernel: Brownian Diffusion

This section uses Brownian diffusion as an example for message passing framework. The initial
distribution p0(y) of each node is N (0 ∞); the transition kernel κtitb (y ·) is a Gaussian centered
at y with variance (ti − tb)Σ  where Σ = µI  µ = β/2  β is the concentration parameter of DPMM.
Then the local normalizer Zρi(x|θi) is

N (yi; 0 ∞)

(cid:90)
(cid:89)
(vρb + tb − ti)−1(cid:17)−1

 

b∈(cid:126)ci

Zρi(x|θi) =

(cid:16)(cid:88)

vρi =

b∈(cid:126)ci

and the node message Mρi(yi) is normally distributed Mρi(yi) ∼ N (yi; ˆyρi  Σvρi )  where

N (yi; ˆyb  Σ(vρb + tb − ti))dyi 
(cid:19)

(cid:18)(cid:88)

ˆyρb

ˆyρi =

b∈(cid:126)ci

vρb + tb − ti

vρi.

(14)

5 Experiments: Finding Bushy Trees

In this section  we compare trees built by the beta coalescent (beta) against those built by Kingman’s
coalescent (kingman) and hierarchical agglomerative clustering [26  hac] on both synthetic and real
data. We show beta performs best and can capture data in more interpretable  bushier trees.

Setup The parameter α for the beta coalescent is between 1 and 2. The closer α is to 1  bushier the
tree is  and we set α = 1.2.5 We set the mutation rate as 1  thus the DPMM parameter is initialized
as β = 2  and updated using slice sampling [27]. All experiments use 100 initial iterations of DPMM
inference with 30 more iterations after each coalescent event (forming a new particle).

Metrics We use three metrics to evaluate the quality of the trees discovered by our algorithm:
purity  subtree and path length. The dendrogram purity score [28  10] measures how well the leaves
in a subtree belong to the same class. For any two leaf nodes  we ﬁnd the least common subsumer
node s and—for the subtree rooted at s—measure the fraction of leaves with same class labels. The
subtree score [9] is the ratio between the number of internal nodes with all children in the same
class and the total number of internal nodes. The path length score is the average difference—over
all pairs—of the lowest common subsumer distance between the true tree and the generated tree 
where the lowest common subsumer distance is the distance between the root and the lowest common
subsumer of two nodes. For purity and subtree  higher is better  while for length  lower is better.
Scores are in expectation over particles and averaged across chains.

5.1 Synthetic Hierarchies

To test our inference method  we generated synthetic data with edge length (full details available
in the supplemental material); we also assume each child of the root has a unique label and the
descendants also have the same label as their parent node (except the root node).
We compared beta against kingman and hac by varying the number of observations (Figure 2(a))
and feature dimensions (Figure 2(b)). In both cases  beta is comparable to kingman and hac (no
edge length). While increasing the feature dimension improves both scores  more observations do
not: for synthetic data  a small number of observations sufﬁce to construct a good tree.

5With DPMM proposals  α has a negligible effect  so we elide further analysis for different α values.

6

(a) Increasing observations

(b) Increasing dimension

(c) beta v.s. enum

Figure 2: Figure 2(a) and 2(b) show the effect of changing the underlying data size or number
of dimension. Figure 2(c) shows that our DPMM proposal for children sets is comparable to an
exhaustive enumeration of all possible children sets (enum).

To evaluate the effectiveness of using our DPMM as a proposal distribution  we compare exhaustively
enumerating all children set candidates (enum) while keeping the SMC otherwise unchanged; this
experiment uses ten data points (enum is completely intractable on larger data). Beta uses the DPMM
and achieved similar accuracy (Figure 2(c)) while greatly improving efﬁciency.

5.2 Human Tissue Development

Our ﬁrst real dataset is based on the developmental biology of human tissues. As a human develops 
tissues specialize  starting from three embryonic germ layers: the endoderm  ectoderm  and mesoderm.
These eventually form all human tissues. For example  one developmental pathway is ectoderm →
neural crest → cranial neural crest → optic vesicle → cornea. Because each germ layer specializes
into many different types of cells at speciﬁc times  it is inappropriate to model this development as a
binary tree  or with clustering models lacking path lengths.
Historically  uncovering these specialization pathways is a painstaking process  requiring inspection
of embryos at many stages of development; however  massively parallel sequencing data make it
possible to efﬁciently form developmental hypotheses based on similar patterns of gene expression.
To investigate this question we use the transcriptome of 27 tissues with known  unambiguous 
time-speciﬁc lineages [29]. We reduce the original 182727 dimensions via principle component
analysis [30  PCA]. We use ﬁve chains with ﬁve particles per chain.
Using reference developmental trees  beta performs better on all three scores (Table 1) because beta
builds up a bushy hierarchy more similar to the true tree. The tree recovered by beta (Figure 3)
reﬂects human development. The ﬁrst major differentiation is the division of embryonic cells into
three layers of tissue: endoderm  mesoderm  and ectoderm. These go on to form almost all adult
organs and cells. The placenta (magenta)  however  forms from a fourth cell type  the trophoblast;
this is placed in its own cluster at the root of the tree. It also successfully captures ectodermal tissue
lineage. However  mesodermic and endodermic tissues  which are highly diverse  do not cluster as
well. Tissues known to secrete endocrine hormones (dashed borders) cluster together.

5.3 Clustering 20-newsgroups Data

Following Heller et al. [10]  we also compare the three models on 20-newsgroups 6 a multilevel
hierarchy ﬁrst dividing into general areas (rec  space  and religion) before specializing into areas such
as baseball or hockey.7 This true hierarchy is inset in the bottom right of Figure 4  and we assume
each edge has the same length. We apply latent Dirichlet allocation [31] with 50 topics to this corpus 
and use the topic distribution for each document as the document feature. We use ﬁve chains with
eighty particles per chain.

6 http://qwone.com/˜jason/20Newsgroups/
7We use “rec.autos”  “rec.sport.baseball”  “rec.sport.hockey”  “sci.space” newsgroups but also—in contrast

to Heller et al. [10]—added “soc.religion.christian”.

7

purity0.40.60.81.020406080100Number of ObservationsScoresbetahackingmanpurity0.40.60.81.0246810DimensionScoresbetahackingmanpurity0.40.60.81.0246810DimensionScoresbetaenumlength0.00.20.40.620406080100Number of ObservationsScoresbetakingmanlength0.00.20.40.6246810DimensionScoresbetakingmanlength0.00.20.40.6246810DimensionScoresbetaenumFigure 3: One sample hierarchy of human tissue
from beta. Color indicates germ layer origin of
tissue. Dashed border indicates secretory func-
tion. While neural tissues from the ectoderm
were clustered correctly  some mesoderm and
endoderm tissues were commingled. The cluster
also preferred placing secretory tissues together
and higher in the tree.

Figure 4: One sample hierarchy of the 20news-
groups from beta. Each small square is a docu-
ment colored by its class label. Large rectangles
represent a subtree with all the enclosed docu-
ments as leaf nodes. Most of the documents from
the same group are clustered together; the three
“rec” groups are merged together ﬁrst  and then
merged with the religion and space groups.

Biological Data

purity ↑
subtree ↑
length ↓

hac
0.453
0.240
−

kingman

0.474 ± 0.029
0.302 ± 0.033
0.654 ± 0.041

beta

0.492 ± 0.028
0.331 ± 0.050
0.586 ± 0.051

hac
0.465
0.571
−

20-newsgroups Data
kingman
0.510 ± 0.047
0.651 ± 0.013
0.477 ± 0.027

0.565 ± 0.081
0.720 ± 0.013
0.333 ± 0.047

beta

Table 1: Comparing the three models: beta performs best on all three scores.

As with the biological data  beta performs best on all scores for 20-newsgroups. Figure 4 shows a
bushy tree built by beta  which mostly recovered the true hierarchy. Documents within a newsgroup
merge ﬁrst  then the three “rec” groups  followed by “space” and “religion” groups. We only use
topic distribution as features  so better results could be possible with more comprehensive features.

6 Conclusion

This paper generalizes Bayesian hierarchical clustering  moving from Kingman’s coalescent to the
beta coalescent. Our novel inference scheme based on SMC and DPMM make this generalization
practical and efﬁcient. This new model provides a bushier tree  often a more realistic view of data.
While we only consider real-valued vectors  which we model through the ubiquitous Gaussian 
other likelihoods might be better suited to other applications. For example  for discrete data such
as in natural language processing  a multinomial likelihood may be more appropriate. This is a
straightforward extension of our model via other transition kernels and DPMM base distributions.
Recent work uses the coalescent as a means of producing a clustering in tandem with a downstream
task such as classiﬁcation [32]. Hierarchies are often taken a priori in natural language processing.
Particularly for linguistic tasks  a fully statistical model like the beta coalescent that jointly learns the
hierarchy and a downstream task could improve performance in dependency parsing [33] (clustering
parts of speech)  multilingual sentiment [34] (ﬁnding sentiment-correlated words across languages) 
or topic modeling [35] (ﬁnding coherent words that should co-occur in a topic).

Acknowledgments

We would like to thank the anonymous reviewers for their helpful comments  and thank H´ector
Corrada Bravo for pointing us to human tissue data. This research was supported by NSF grant
#1018625. Any opinions  ﬁndings  conclusions  or recommendations expressed here are those of the
authors and do not necessarily reﬂect the view of the sponsor.

8

PlacentaStomachPancreasTracheaThyroidBoneMarrowColonHeartKidneyPeripheralBloodLymphocytesBrainCorpusCallosumSpinalCordLungProstateSpleenUterusBrainHypothalamusBrainCaudateNucleusBrainThalamusRetinaThymusMonocytesBladderMammaryGlandSmallIntestineBrainAmygdalaBrainCerebellumectodermendodermmesodermplacentarec.sport.hockyrec.sport.baseballrec.autossci.spacesoc.religion.christian...............True TreeDoc LabelReferences
[1] Kaufman  L.  P. Rousseeuw. Finding Groups in Data: An Introduction to Cluster Analysis. John Wiley 

1990.

[2] Jain  A. K. Data clustering: 50 years beyond k-means. Pattern Recognition Letters  31(8):651–666  2010.
[3] Brown  P. F.  V. J. D. Pietra  P. V. deSouza  et al. Class-based n-gram models of natural language.

Computational Linguistics  18:18–4  1990.

[4] Bergen  J.  P. Anandan  K. Hanna  et al. Hierarchical model-based motion estimation. In ECCV. 1992.
[5] Girvan  M.  M. E. J. Newman. Community structure in social and biological networks. PNAS  99:7821–

7826  2002.

[6] Kingman  J. F. C. On the genealogy of large populations. Journal of Applied Probability  19:27–43  1982.
[7] Pitman  J. Coalescents with multiple collisions. The Annals of Probability  27:1870–1902  1999.
[8] Berestycki  N. Recent progress in coalescent theory. In Ensaios Matematicos  vol. 16. 2009.
[9] Teh  Y. W.  H. Daum´e III  D. M. Roy. Bayesian agglomerative clustering with coalescents. In NIPS. 2008.
[10] Heller  K. A.  Z. Ghahramani. Bayesian hierarchical clustering. In ICML. 2005.
[11] Blundell  C.  Y. W. Teh  K. A. Heller. Bayesian rose trees. In UAI. 2010.
[12] Adams  R.  Z. Ghahramani  M. Jordan. Tree-structured stick breaking for hierarchical data. In NIPS. 2010.
[13] Knowles  D.  Z. Ghahramani. Pitman-Yor diffusion trees. In UAI. 2011.
[14] Neal  R. M. Density modeling and clustering using Dirichlet diffusion trees. Bayesian Statistics  7:619–629 

2003.

[15] Sagitov  S. The general coalescent with asynchronous mergers of ancestral lines. Journal of Applied

Probability  36:1116–1125  1999.

[16] Neal  R. M. Annealed importance sampling. Technical report 9805  University of Toronto  1998.
[17] Fearhhead  P. Sequential Monte Carlo method in ﬁlter theory. PhD thesis  University of Oxford  1998.
[18] Felsenstein  J. Maximum-likelihood estimation of evolutionary trees from continuous characters. Am J

Hum Genet  25(5):471–492  1973.

[19] Birkner  M.  J. Blath  M. Steinrucken. Importance sampling for lambda-coalescents in the inﬁnitely many

sites model. Theoretical population biology  79(4):155–73  2011.

[20] Doucet  A.  N. De Freitas  N. Gordon  eds. Sequential Monte Carlo methods in practice. 2001.
[21] Gordon  N.  D. Salmond  A. Smith. Novel approach to nonlinear/non-Gaussian Bayesian state estimation.

IEEE Proceedings F  Radar and Signal Processing  140(2):107–113  1993.

[22] G¨or¨ur  D.  L. Boyles  M. Welling. Scalable inference on Kingman’s coalescent using pair similarity. JMLR 

22:440–448  2012.

[23] Antoniak  C. E. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.

The Annals of Statistics  2(6):1152–1174  1974.

[24] Cappe  O.  S. Godsill  E. Moulines. An overview of existing methods and recent advances in sequential

Monte Carlo. PROCEEDINGS-IEEE  95(5):899  2007.

[25] Chen  Z. Bayesian ﬁltering: From kalman ﬁlters to particle ﬁlters  and beyond. McMaster  [Online]  2003.
[26] Eads  D. Hierarchical clustering (scipy.cluster.hierarchy). SciPy  2007.
[27] Neal  R. M. Slice sampling. Annals of Statistics  31:705–767  2003.
[28] Powers  D. M. W. Unsupervised learning of linguistic structure an empirical evaluation. International

Journal of Corpus Linguistics  2:91–131  1997.

[29] Jongeneel  C.  M. Delorenzi  C. Iseli  et al. An atlas of human gene expression from massively parallel

signature sequencing (mpss). Genome Res  15:1007–1014  2005.

[30] Shlens  J. A tutorial on principal component analysis. In Systems Neurobiology Laboratory  Salk Institute

for Biological Studies. 2005.

[31] Blei  D. M.  A. Ng  M. Jordan. Latent Dirichlet allocation. JMLR  2003.
[32] Rai  P.  H. Daum´e III. The inﬁnite hierarchical factor regression model. In NIPS. 2008.
[33] Koo  T.  X. Carreras  M. Collins. Simple semi-supervised dependency parsing. In ACL. 2008.
[34] Boyd-Graber  J.  P. Resnik. Holistic sentiment analysis across languages: Multilingual supervised latent

Dirichlet allocation. In EMNLP. 2010.

[35] Andrzejewski  D.  X. Zhu  M. Craven. Incorporating domain knowledge into topic modeling via Dirichlet

forest priors. In ICML. 2009.

9

,Yuening Hu
Jordan Ying
Hal Daume III
Z. Irene Ying
Yunbo Wang
Mingsheng Long
Jianmin Wang
Zhifeng Gao
Philip Yu