2019,Online EXP3 Learning in Adversarial Bandits with Delayed Feedback,Consider a player that in each of T rounds chooses one of K arms. An adversary chooses the cost of each arm in a bounded interval  and a sequence of feedback delays \left\{ d_{t}\right\}  that are unknown to the player. After picking arm a_{t} at round t  the player receives the cost of playing this arm d_{t} rounds later. In cases where t+d_{t}>T  this feedback is simply missing. We prove that the EXP3 algorithm (that uses the delayed feedback upon its arrival) achieves a regret of O\left(\sqrt{\ln K\left(KT+\sum_{t=1}^{T}d_{t}\right)}\right). For the case where \sum_{t=1}^{T}d_{t} and T are unknown  we propose a novel doubling trick for online learning with delays and prove that this adaptive EXP3 achieves a regret of O\left(\sqrt{\ln K\left(K^{2}T+\sum_{t=1}^{T}d_{t}\right)}\right). We then consider a two player zero-sum game where players experience asynchronous delays. We show that even when the delays are large enough such that players no longer enjoy the “no-regret property”  (e.g.  where d_{t}=O\left(t\log t\right)) the ergodic average of the strategy profile still converges to the set of Nash equilibria of the game. The result is made possible by choosing an adaptive step size \eta_{t} that is not summable but is square summable  and proving a “weighted regret bound” for this general case.,Online EXP3 Learning in Adversarial Bandits with

Delayed Feedback

Ilai Bistritz1  Zhengyuan Zhou23  Xi Chen2  Nicholas Bambos1  Jose Blanchet1

2New York University  Stern School of Business

1Stanford University

3IBM Research

{bistritz bambos jose.blanchet}@stanford.edu  {zzhou xchen3}@stern.nyu.edu

Abstract

(cid:80)T
(cid:18)(cid:114)

(cid:16)

K 2T +(cid:80)T

(cid:18)(cid:114)
(cid:17)(cid:19)

Consider a player that in each of T rounds chooses one of K arms. An ad-
versary chooses the cost of each arm in a bounded interval  and a sequence
of feedback delays {dt} that are unknown to the player. After picking arm
at at round t 
the player receives the cost of playing this arm dt rounds
later.
In cases where t + dt > T   this feedback is simply missing. We
prove that the EXP3 algorithm (that uses the delayed feedback upon its ar-

(cid:16)

KT +(cid:80)T

(cid:17)(cid:19)

rival) achieves a regret of O

ln K

t=1 dt

. For the case where

t=1 dt and T are unknown  we propose a novel doubling trick for online
learning with delays and prove that this adaptive EXP3 achieves a regret of

ln K

t=1 dt

. We then consider a two player zero-sum game
O
where players experience asynchronous delays. We show that even when the de-
lays are large enough such that players no longer enjoy the “no-regret property” 
(e.g.  where dt = O (t log t)) the ergodic average of the strategy proﬁle still con-
verges to the set of Nash equilibria of the game. The result is made possible by
choosing an adaptive step size ηt that is not summable but is square summable 
and proving a “weighted regret bound” for this general case.

1

Introduction

Consider an agent that makes T sequential decisions from a set of K options (i.e.  arms)  where each
decision incurs some cost. The cost sequences are chosen by an adversary that knows the agent’s
strategy. The agent’s goal is to minimize this cost over time. In the full information case the agent
gets to know the cost of all arms after choosing a single arm. A more challenging case is the bandit
feedback one  where the agent only observes the cost of the chosen arm. In this paper  we consider
the bandit feedback case. The question of what the agent learns about the costs (i.e.  full information
or bandit) naturally inﬂuences the best performance the agent can guarantee. Another fundamental
question is when the agent gets to know the cost.
An online learning scenario with no delays means that the agent always knows how beneﬁcial all
the past actions were when making the current decision. This is rarely the case in practice  where
many decisions have to be made before all the feedback from past choices is received. Determining
the feedback in practice is not always straightforward and might involve some computations and es-
timations. Furthermore  the time it takes to receive the feedback varies between different decisions
and times. All of these effects are accentuated when an adversary has control over the feedback
mechanism. Following this reasoning  online learning with delayed feedback has attracted consid-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(cid:17)

T K ln K

t /∈M dt

(cid:16)(cid:113)

(cid:1) + |M|(cid:17)

ln K(cid:0)KT +(cid:80)

erable attention [1–12]. The concept of adversarial delays (i.e.  arbitrary delay sequences) was ﬁrst
introduced in [13]  for the full information case and under the assumption that all feedback is re-
ceived before round T (which we do not make here). The ﬁrst goal of this paper is to address the
more challenging bandit cost scenario.
When there is no delayed feedback  EXP3 [14–16] is the state-of-the-art algorithm for adversarial
online learning with bandit feedback. In EXP3  the agent keeps a weight for each arm  and picks
an arm at random with a probability that is proportional to the exponents of the weights. When a
cost l(i) is incurred for choosing arm i  which was picked with probability p(i)  l(i)
p(i) is added to the
weight of this arm. The idea is that on average over the randomness of the decisions  the weights

are adjusted with the vector of costs(cid:0)l(1)  ...  l(K)(cid:1). With no delays  the expected regret of EXP3 is
(cid:16)√

. Having a sublinear regret  the average regret per round goes to zero as T → ∞ 

O
which is known as the “no-regret property” [17].
Our ﬁrst main contribution in this paper is to show that with an arbitrary sequence of delays dt 
  where M is the set
EXP3 achieves an expected regret of O
of rounds whose feedback is not received before round T . This expression makes clear which delay
sequences will maintain the no-regret property and which will lead to linear regret in T .
An omnipotent adversary represents the embodiment of the agent’s worst fears when learning to
optimize its decisions in an unknown environment. An algorithm with performance guarantees in
this worst case scenario is an appealing choice from a designer’s point of view. As such  it is more
likely that the opponents that the agent will face are online learning agents like itself  which have
limited knowledge and power. These agents have interests of their own  but in the worst case these
interests are in a direct conﬂict with those of our agent. Therefore  zero-sum games are the natural
framework to analyze the outcome of an interaction against another agent instead of against an all
powerful adversary. Interestingly enough  it turns out that with delayed feedback  the outcome of
playing against another agent can be essentially different from playing against an adversary.
It is well known that when two agents use a no-regret learning algorithm against each other in a
zero-sum game  the dynamics will result in a Nash equilibrium (NE) [18]. To be precise  the ergodic
average strategy converges to the set of NE strategies and the ergodic average cost to the value of
the zero-sum game. The last iterate does not converge in general to a NE  and even moves away
from it [19]. However  the emergence of a NE in a game where such an agent ﬁnds itself against
another agent using a no-regret algorithm provides yet another strong evidence for the importance of
the concept of NE. From a more practical point of view  convergence of the ergodic average to a NE
makes no-regret algorithms an appealing way to compute a NE when the game matrix is unknown
and only simulating the game is possible. In such a simulation of an unknown game  bandit feedback
is a more realistic assumption than full information.
With no delays  the only purpose of the step size of the EXP3 algorithm is to minimize the regret.
If the horizon of the game T is unknown  one can use the doubling trick and choose the step sizes
accordingly. With delayed feedback  a varying step-size plays a much more central role. With
delayed feedback  it is not surprising that convergence of the ergodic average to the set of NE is
maintained if the algorithm still has a sublinear regret (asymptotically zero average regret). When
the delays become larger  for example super-linear delays that grow like O (t log t)  this is no longer
true and the regret of EXP3 (or any other algorithm) becomes linear in the horizon T .
Our second main contribution in this paper is to show that even with delays that cause a linear regret 
the ergodic average may still converge to the set of NE by using a time-varying step size ηt. This
means that computing a NE using EXP3 is still possible even in scenarios where EXP3 does not
enjoy a sublinear regret (i.e.  the no-regret). Since delays are a prominent feature of almost every
computational environment  this is an encouraging ﬁnding.

2 EXP3 in Adversarial Bandits under Feedback Delays

Consider a player that at each round t has to pick one out of K arms. Denote the arm the player
chooses at round t by at. The cost at round t from arm i is l(i)
be the cost vector. These costs are arbitrarily chosen by an adversary that knows the player’s strategy

t ∈ [0  1]  and let lt =

  ...  l(K)

l(1)
t

t

(cid:16)

(cid:17)

2

Algorithm 1 EXP3 with delays
Initialization: Let {ηt} be a positive non-increasing sequence  and set ˜L(i)
i = 1  ...  K.
For t = 1  ...  T do

1 = 0 and p(i)

1 = 1

K for

1. Choose an arm at at random according to the distribution pt.
2. Obtain a set of delayed costs l(as)
3. Update the weights of arm as for all s ∈ St  using

s

for all s ∈ St  where as is the arm played at round s.

˜L(as)
t = ˜L(as)

t−1 + ηs

l(as)
s
p(as)
s

.

4. Update the mixed strategy

p(i)
t+1 =

End

t(cid:80)n

e− ˜L(i)
j=1 e− ˜L(j)

t

.

(cid:110)

(cid:111)

(3)

(4)

l(i)
t

t

in advance. Hence  we can assume that the adversary chooses
for each i in advance  knowing
exactly how the player is going to react. The player gets to know the cost of playing at at round t at
the end of the t + dt − 1 round (i.e.  after a delay of dt ≥ 1 rounds)  so the feedback is available at
the beginning of round t + dt. The set of costs (feedback samples) received at round t is denoted St 
so s ∈ St means that the cost of as from round s is received at round t. Since the game lasts for T
rounds  all costs for which t + dt > T are never received. Of course  the value of dt does not matter
as long as t + dt > T   and these are just samples that the adversary chose to prevent the player from
receiving. We name these costs the missing samples  and denote their set by M.
The player wants to have a learning algorithm that uses the past observations to make good decisions
over time. Denote the vector of probabilities of the player for choosing arms at round t by pt ∈ ∆K 
where ∆K denotes the K-simplex. This is also known as the mixed strategy of the player. The
performance of the player’s algorithm  or strategy  is measured using the regret. The expected regret
is the total expected cost over an horizon of T rounds  compared to the total cost that would result
from playing the best ﬁxed mixed strategy in all rounds:
Deﬁnition 1. The expected regret is deﬁned as:

(cid:40) T(cid:88)

t=1

(cid:41)

T(cid:88)

t=1

Ea {R (T )} = Ea

t − min
l(at)

i

l(i)
t

(1)

where Ea is the expectation over the random actions a1  ...  aT the agent chooses at each round.

At round t  EXP3 (detailed in Algorithm 1) chooses an arm at random according to the distribution
pt that depends on the history of the game. Deﬁne the following ﬁltration

Ft = σ ({as | s + ds ≤ t})

(2)
which is generated from all the actions for which the feedback was received up to round t. Note that
the mixed strategy of the player pt is a Ft-measurable random variable  since pt is a function of all
feedback received up to round t.
Our main result of this section establishes the expected regret bound for EXP3 with delays. Note
that Algorithm 1 is nothing but the obvious variant of EXP3 for the case of delayed feedback.
Therefore  the importance of the following result is in the novel analysis of how delays  which are a
part of every practical system  affect a well-known and widely used algorithm such as EXP3. While
waiting for the delayed feedback  the agent is making decisions that incur a larger regret than in
the usual no-delay case where all the past feedback has been received. The proof of Theorem 1
bounds this addition to the regret. The proof analyzes the novel notion of weighted-regret  given in
the following Lemma. The goal of this more general result is to be both used here and for the proof
of Theorem 3 in the next section.

3

(cid:110)

(cid:111)

Lemma 1. Let {ηt} be a non-increasing step size sequence. Let
be a cost sequence such that
t ∈ [0  1] for every t  i. Let {dt} be a delay sequence such that the cost from round t is received
l(i)
at round t + dt. Deﬁne M to be the set of all samples that are not received before round T . Then
using EXP3 (Algorithm 1) guarantees

l(i)
t

(cid:41)

T(cid:88)

T(cid:88)

t=1

(cid:88)

t /∈M

≤ ln K +

K
2

(cid:88)

t∈M

η2
t + 4

η2
t dt +

ηt.

(5)

Proof. Let ei be the pure strategy that picks arm i with probability 1. Then for each i

(cid:40) T(cid:88)

Ea

(cid:40) T(cid:88)

t=1

Ea

ηtl(at)

t=1

i

t=1

ηtl(i)
t

ηtl(at)

t − min
(cid:40) T(cid:88)
(cid:41)
ηt (cid:104)lt  pt(cid:105) − T(cid:88)
(cid:88)

Ea(cid:110)
(cid:41)
(cid:41)
ηs (cid:104)ls  ps − ei(cid:105)

t − T(cid:88)
(cid:40) T(cid:88)
(cid:40) T(cid:88)

ηtl(i)
t

ηtl(i)
t

= Ea

Ea

Ea

t=1

t=1

t=1

t=1

t=1

s∈St

ηtl(at)

t

= Ea

=

t=1

|Ft

ηtl(i)
t

(cid:41)
(cid:41)
ηt (cid:104)lt  pt − ei(cid:105)
(cid:41)

(cid:111) − T(cid:88)
(cid:40) T(cid:88)
(cid:40)(cid:88)
(cid:40) T(cid:88)

ηt (cid:104)lt  pt − ei(cid:105)
(cid:88)

(cid:41)
ηs (cid:104)ls  ps − ei(cid:105)

t∈M

≤

t=1

=

(a)

t=1

s∈St

+ Ea

Ea

(cid:88)

t∈M

+

ηt

(6)

where (a) follows from (cid:104)lt  pt − ei(cid:105) ≤ 1  since 0 ≤ l(i)
Deﬁne St s = {r ∈ St; r < s}. This is the set of feedback samples arriving at round t that the
algorithm uses before s. Deﬁne s− as the step a moment before using the feedback from round s  so
ps− is the mixed strategy at this moment. Deﬁne s+ as the step a moment after using the feedback
from round s. This step is taking place in round t if s ∈ St. We analyze the ﬁrst term in (6) by
splitting it as follows

t ≤ 1 for every i.

(cid:40) T(cid:88)

(cid:88)

t=1

s∈St

Ea

(cid:41)
ηs (cid:104)ls  ps − ei(cid:105)

(cid:40) T(cid:88)

(cid:88)

t=1

s∈St

(cid:68)

= Ea

ls  ps− − ei

ηs

+

(cid:69)

T(cid:88)

(cid:88)

t=1

s∈St

(cid:68)
ls  ps − ps−

ηs

(cid:69)(cid:41)

(7)
where the ﬁrst part is interpreted as the regret with no delays  and the second as the regret penalty
the delays incur. From Lemma 3 we have

Next we analyze the delay term. Let ˜lt =

  ...  0

. First note that for all i we have

(cid:40) T(cid:88)

(cid:88)

t=1

s∈St

(cid:68)

Ea

ηs

ls  ps−

T(cid:88)

t=1

η2
t .

ηtl(i)
t

≤ ln K +

K
2

(cid:41)

(cid:19)
(cid:16) ˜Lq

(cid:17)

(cid:44) hi

(cid:18)

(cid:69) − T(cid:88)

t=1

0  ...  l(at)
t
p(at)
t

− ˜L(i)
q−
− ˜L(j)
q−

j=1 e

e

˜lq

(cid:17)

p(i)
q− =

(cid:80)K
(cid:40) K(cid:88)
(cid:111) ≤ 2ηqEa
|Fq−
q− Ea(cid:110)˜l(i)
K(cid:88)

p(i)

2ηq

i=1

q

i=1

4

(cid:41)
|Fq−
K(cid:88)

2ηq

i=1

=
(a)

p(i)
q−

˜l(i)
q

(cid:111)

|Fq−

=
(b)

and p(i)
obtain

q+ = hi

(cid:16) ˜Lq− + ηq
Ea(cid:110)(cid:13)(cid:13)pq+ − pq−(cid:13)(cid:13)1

q ≤ 2ηq

q− l(i)
p(i)

K(cid:88)

i=1

p(i)
q− = 2ηq

(10)

(8)

(9)

  so from Lemma 2 using x = ˜Lq− and ∆ = ηq

˜lq  so h (x) = pq− we

where (a) uses p(i)
l(i)
q
p(i)
q
the feedback from aq was not received until round q−. Therefore

is
q and zero otherwise. Note that aq is independent of Fq− since by deﬁnition

q ∈ Fq− (since q < q−) together with the fact that ˜l(i)

with probability p(i)

q

(b)

(a)

+

+

=

+

+

r=s

r=s

r=s

r=s

ls 

q∈Sr

q∈Sr

q∈Sr

(cid:11)(cid:33)(cid:41)
t−1(cid:88)
(cid:10)ls  pr − pr+1
(cid:1)(cid:43) ≤
(cid:42)
(cid:88)
t−1(cid:88)
(cid:0)pq− − pq+
 ≤
(cid:0)pq+ − pq−(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
t−1(cid:88)
(cid:107)ls(cid:107)∞
 =
(cid:88)
t−1(cid:88)
(cid:13)(cid:13)pq+ − pq−(cid:13)(cid:13)1
(cid:111) +
(cid:110)(cid:13)(cid:13)pq+ − pq−(cid:13)(cid:13)1
(cid:111) ≤
(cid:110)(cid:13)(cid:13)pq+ − pq−(cid:13)(cid:13)1
 ≤
 (cid:88)
t−1(cid:88)
(cid:12)(cid:12)(cid:12) ≤ 1 for every i and using the triangle
(cid:12)(cid:12)(cid:12)l(i)
T(cid:88)

|Fq−
(cid:88)

(cid:88)

(cid:88)

(cid:88)

|Fq−

q∈St s

η2
t dt

q∈Sr

t /∈M

ηq +

(11)

r=s

ηq

(d)

(c)

4

t

η2
t + 4

η2
t dt +

t=1

t /∈M

ηt.

t∈M

(12)

Ea

Ea

=

t=1

t=1

t=1

ηs

ηs

ηs

Ea

ls 

Ea

s∈St

s∈St

s∈St

q∈St s

ls  pt − ps−

q− ∈ Fq− and (b) uses p(i)
(cid:69)(cid:41)
(cid:40) T(cid:88)
(cid:68)
(cid:88)
ls  ps − ps−
(cid:40) T(cid:88)
(cid:32)(cid:68)
(cid:69)
(cid:88)
 T(cid:88)
(cid:42)
(cid:1)(cid:43)
(cid:88)
(cid:88)
(cid:0)pq− − pq+
(cid:0)pq+ − pq−(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
 T(cid:88)
(cid:107)ls(cid:107)∞
(cid:88)
(cid:88)
 T(cid:88)
 (cid:88)
(cid:88)
(cid:13)(cid:13)pq+ − pq−(cid:13)(cid:13)1
 T(cid:88)
(cid:88)
(cid:88)
 T(cid:88)
t−1(cid:88)
(cid:88)
(cid:88)
 T(cid:88)
(cid:88)

q∈St s

q∈St s

q∈St s

q∈Sr

s∈St

s∈St

s∈St

s∈St

2Ea

Ea

Ea

Ea

r=s

ηs

ηs

ηs

ηs

ηs

t=1

t=1

t=1

t=1

E

E

t=1

s∈St

where (a) follows from H¨older’s inequality  (b) since
inequality  (c) from (10) and (d) follows from Lemma 4.
Combining (6)  (8) and (11) yields  for all i = 1  ...  K

Ea

ηtl(i)
t

≤ ln K +

K
2

t=1

t=1

ηtl(at)

(cid:40) T(cid:88)
t − T(cid:88)
(cid:113) ln K
KT +(cid:80)
(cid:40) T(cid:88)

(cid:41)

(cid:110)
T(cid:88)

t=1

(cid:41)

(cid:111)
(cid:41)

Theorem 1. Deﬁne M to be the set of all samples that are not received before round T . Choose the
t ∈ [0  1] for every
ﬁxed step size η =
(cid:33)
t  i. Let {dt} be a delay sequence such that the cost from round t is received at round t + dt. Then

be a cost sequence such that l(i)

(cid:32)

. Let

t /∈M dt

l(i)
t

Ea (R (T )) = E

t − min
l(at)

i

l(i)
t

≤ O

t=1

KT +

dt

+ |M|

(13)

Proof of Theorem 1. To obtain Theorem 1  substitute ηt = η in (5) of Lemma 1  and divide both
sides by η:

 .

(cid:33)

l(i)
t

≤ O

ln K

+ η

KT +

+ |M|

dt

(14)

Ea

(cid:40) T(cid:88)
t − min
l(at)
(cid:113) ln K
KT +(cid:80)

t=1

i

T(cid:88)

t=1

Then  choosing η =

yields (13).

t /∈M dt

(cid:88)

t /∈M

(cid:33)

(cid:88)

t /∈M

(cid:118)(cid:117)(cid:117)(cid:116)ln K
(cid:32)

(cid:32)

η

5

(cid:18)(cid:114)

(cid:16)

KT +(cid:80)T

(cid:17)(cid:19)

It is worthwhile noting that our bound is tighter than O
that does not
take M into account  since counting delays that go beyond round T is redundant. For example  if
dt = t2 then

. Our subsequent Corollary formalizes this intuition.

(cid:113)(cid:80)T

t=1 dt

ln K

2

t=1 dt = O

t ∈ [0  1] for
Corollary 1. Let η =
every t  i. Let {dt} be a delay sequence such that the cost from round t is received at round t + dt.
Then

be a cost sequence such that l(i)

. Let

t /∈M dt

l(i)
t

T 3

(cid:16)
(cid:17)
(cid:113) ln K
KT +(cid:80)
(cid:40) T(cid:88)



(cid:118)(cid:117)(cid:117)(cid:116)ln K

(cid:32)

T(cid:88)

(cid:33)

KT +

dt

(15)

Ea (R (T )) = Ea

t − min
l(at)

i

l(i)
t

≤ O

t=1

t=1

t=1

(cid:110)

(cid:111)

(cid:41)

T(cid:88)

Proof. The m = |M| missing samples (received after T ) contribute at least m(m+1)
to the sum of
t=1 dt (since the best case is when the feedback of T is delayed by one and arrives after T  
the feedback of T − 1 now has to be delayed by at least 2 to be received after T and so on m times).

2

delays(cid:80)T
Hence(cid:118)(cid:117)(cid:117)(cid:116)ln K
(cid:32)
(cid:118)(cid:117)(cid:117)(cid:116)ln K
(cid:32)

1
2

KT +

dt

KT +

(cid:32)

(cid:33)
(cid:33)

≥

(cid:118)(cid:117)(cid:117)(cid:116)ln K
(cid:114)

T(cid:88)
(cid:88)

t=1

t /∈M

KT +

dt

+

ln K

1
2

m (m + 1)

2

≥ O

(cid:88)

t /∈M

dt +

(cid:33)
(cid:32)

≥

(a)

KT +

m (m + 1)

2

(cid:118)(cid:117)(cid:117)(cid:116)ln K



+ |M|

(cid:33)

(cid:88)

t /∈M

dt

(16)

where (a) follows from the concavity of f (x) =

x.

(cid:17)

(cid:16)√

While the ﬁrst term in the regret  KT ln K  has a factor of K  the delay term(cid:80)T

The expression in (15) reveals a robustness property of the regret bound of EXP3 under delays.
t=1 dt does not have
a factor of K. Consider bounded delays of the form dt = K. Then  the order of magnitude of the
  exactly as that of EXP3 without delays [14]. For
regret as a function of T and K is O
comparison  consider the full information case where at each round the cost of all arms is received.
Assume that the player uses the exponential weights algorithm  which is the equivalent of EXP3
for the full information case. For the same delay sequence dt = K  exponential weights achieves
a regret bound of O
that exponential
weights with no delays achieves. The intuition for this result is that EXP3 already “paid the price”
for using K times less feedback than in the full information case. Depending on less feedback 
EXP3 is inherently more robust to feedback delays.

K times worse than the O

(cid:16)√

(cid:16)√

T K ln K

T K ln K

T ln K

[13] 

(cid:17)

(cid:17)

√

2.1 Adaptive Algorithm: Doubling Trick with Delays

used in Algorithm 1 requires knowledge of T and(cid:80)T

t /∈M dt

The step size η =
t=1 dt. With
no delays  the standard doubling trick (see [20]) can be used if T is unknown. However  the same
doubling trick does not work with delayed feedback.We now present a novel doubling trick for the
t=1 dt are unknown. Deﬁne mt as the number of missing
feedback samples at round t  starting from the t-th feedback. The idea is to start a new epoch every

(cid:113) ln K
KT +(cid:80)
τ =1 mτ   that tracks(cid:80)t

delayed feedback case  where T and(cid:80)T
time(cid:80)t
(cid:40)

τ =1 dτ   doubles. Deﬁne the e-th epoch as

t|2e−1 ≤ t(cid:88)

τ =1

Te =

(cid:41)

mτ < 2e

.

(17)

which is a set of consecutive rounds when the sum of delays is within a given interval. During
the e-th epoch  the EXP3 algorithm using our doubling trick uses step size ηe =
2e . Feedback

(cid:113) ln K

√

6

Algorithm 2 Adaptive EXP3 with delays for unknown T and(cid:80)T

t=1 dt

Initialization: Set ˜L(i)
For t = 1  ...  T do

1 = 0 and p(i)

1 = 1

K for i = 1  ...  K. Set the epoch index e = 0 and η0 = 1.

1. Choose an arm at at random according to the distribution pt.
2. Obtain a set of delayed costs l(as)
3. Update the number of missing samples so far

s

for all s ∈ St  where as is the arm played at round s.

mt = t − t(cid:88)

τ =1

|Sτ| .

4. If(cid:80)t

τ =1 mτ ≥ 2e  then update e = e + 1 and initialize ˜L(i)

t = 0 for i = 1  ...  K.
5. Update the weights of arm as for all s ∈ St such that s ∈ Te using step size ηe =

(19)

(cid:113) ln K

2e :

(20)

(21)

˜L(as)
t = ˜L(as)

t−1 + ηe

l(as)
s
p(as)
s

.

6. Update the mixed strategy

p(i)
t+1 =

End

t(cid:80)n

e− ˜L(i)
j=1 e− ˜L(j)

t

.

regret guarantee (up to a constant) as in Theorem 1  despite the fact that T and(cid:80)T

samples originated in previous epoch are discarded once received. The resulting algorithm is detailed
in Algorithm 2.
The next Theorem shows that thanks to our novel doubling trick  Algorithm 2 achieves the same
t=1 dt are un-
known. We conjecture that the K 2 factor replacing K can be improved with a more careful analysis.
However  this factor has no effect on the order of the regret when the average delay is larger than
K 2.
t ∈ [0  1] for every t  i. Let {dt} be a delay
Theorem 2. Let
sequence such that the cost from round t is received at round t + dt. If player uses Algorithm 2 then

be a cost sequence such that l(i)

(cid:110)

l(i)
t

Ea (R (T )) = E

(cid:118)(cid:117)(cid:117)(cid:116)ln K


(cid:32)

O

t − min
l(at)
T(cid:88)

i

KT +

min{dt  T − t + 1}

(cid:33) ≤ O



(cid:118)(cid:117)(cid:117)(cid:116)ln K

(cid:32)

K 2T +

(cid:33) .

dt

T(cid:88)

t=1

(18)

(cid:111)
(cid:40) T(cid:88)

t=1

(cid:41)

l(i)
t

≤

T(cid:88)

t=1

t=1

Proof. See Appendix.

3 Two Player Zero-Sum Game with Delayed Bandit Feedback

In this section we consider a two player zero-sum game where both players play according to the
EXP3 algorithm with feedback delays.
It is well known that without delays  an algorithm with
sublinear regret such as EXP3  played against itself  will converge to a NE (in the ergodic average
sense) [18]. Our main result in this section  given in Theorem 3  generalizes this statement for the
case of arbitrarily (i.e.  adversarially) delayed feedback  and reveals that with delays  convergence
to a NE can occur even without sublinear regret.

7

Let U be the cost matrix  such that when the row player plays i and the column player plays j  the
ﬁrst pays a cost of U (i  j) and the second gains a reward of U (i  j) (i.e.  a cost of −U (i  j)). We
assume without loss of generality that 0 ≤ U (i  j) ≤ 1 for any i  j. Note that if pt  qt ∈ ∆K are
mixed strategies  then we use the convention that

U (pt  j) (cid:44) K(cid:88)
U (pt  qt) (cid:44) K(cid:88)
K(cid:88)

i=1

i=1

j=1

p(i)
t U (i  j)

t q(j)
p(i)

t U (i  j) .

(22)

(23)

and

t   q∗

Nash Equilibrium (NE) is a key concept in game theory for predicting the outcome of a game. A
NE is a strategy proﬁle (p∗
t ) such that no player wants to switch a strategy given that the other
player keeps his strategy. For our result  we need to deﬁne the set of all approximate (and pure) NE:
Deﬁnition 2. The set of all ε-NE points is
(p∗  q∗) | U (p∗  q∗) ≤ min

U (p  q) + ε   U (p∗  q∗) ≥ max

U (p  q) − ε

(cid:27)

(cid:26)

(24)

Nε =

p

q

and the set of NE points is N0.
The entity that converges to the set of NE in our case is the ergodic average of (pt  qt). For the
t   the ergodic average of pt is simply the running average of the sequence pt.
special case of ητ = 1
Deﬁnition 3. The ergodic average of a sequence of distributions pt is deﬁned as:

(cid:80)t
(cid:80)t

(cid:44)

¯pt

τ =1 ητ pτ
τ =1 ητ

.

(25)

(26)

(27)

We say that ( ¯pT   ¯qT ) converges in L1 to the set of NE if

T→∞ arg min
lim
T )∈N0
which also implies that for every ε > 0

T  q∗

(p∗

E {(cid:107)( ¯pT   ¯qT ) − (p∗

T   q∗

T )(cid:107)1} = 0

T→∞ arg min
lim
T )∈N0

T  q∗

(p∗

P ((cid:107)( ¯pT   ¯qT ) − (p∗

T   q∗

T )(cid:107)1 ≥ ε) = 0.

Our theorem below establishes the convergence of EXP3 versus itself to a NE  even under signiﬁcant
delays. Note that the convergence of the ergodic mean to the set of NE is in the L1 sense (so also in
probability)  which is much stronger than convergence of the expected ergodic mean.
Theorem 3. Let two players play a zero-sum game with a cost matrix U such that 0 ≤ U (i  j) ≤ 1
for each i  j  using EXP3. The step size sequence of both players is {ηt}∞
t=1. Let the delay sequences
of the row player and the column player be {dr
t}  respectively. Let the mixed strategies of the
row and column players at round t be pt and qt  respectively. If

t}  {dc

t < ∞.

t→∞ηtdc
t=1 dc

t < ∞.
t η2

2.

1. (cid:80)∞
t=1 ηt = ∞.
3. (cid:80)∞
t→∞ηtdr
lim
t=1 dr
t η2
(cid:16)(cid:80)T
Then  as T → ∞:
(cid:80)T
(cid:16)(cid:80)T
(cid:80)T

t=1 ηtpt
t=1 ηt

t < ∞ and lim

t < ∞ and(cid:80)∞
(cid:17)
(cid:80)T
(cid:80)T
(cid:80)T
(cid:80)T

t=1 ηtqt
t=1 ηt

t=1 ηtpt
t=1 ηt

t=1 ηtqt
t=1 ηt
is the value of the game.

2. U

(cid:17)

1.

 

 

converges in L1 to the set of NE of the zero-sum game.

converges in L1 to min
p

max

j

U (p  j) = max

q

min

i

U (i  q)  which

8

Somewhat surprisingly  the delays do not have to be bounded (in t) for the convergence to NE to
hold. Key examples of application of Theorem 3 are:

• For bounded delays dr

t ≤ D and dc

t ≤ D for all t:
– For a ﬁnite horizon T one can choose ηt = 1√

– For the inﬁnite horizon case one can choose any ηt such that (cid:80)∞
(cid:80)∞
t=1 ηt = ∞.

for all t.

T

• For unbounded sublinear delays such as dr
• For unbounded superlinear delays such as dr

ηt = 1

t2/3 .

t and dc

t ≤ √
t ≤ √
t ≤ t log t and dc

t for all t  one can choose
t ≤ t log t  one can choose

t=1 η2

t < ∞ and

ηt =

t(log t)(log log t).

1

In general  the feedback of the players does not need to be synchronized  and they may have a
completely different sequence of delays.
Next we show that the ergodic average of the EXP3 strategies converges to the set of NE even in a
delayed feedback scenario where EXP3 has linear regret  so the “no-regret” property does not hold.
Proposition 1. Let the mixed strategies of the row and column players at round t be pt and qt 
respectively. There exist {dr

t}t and a cost sequence

such that

  ...  l(K)

(cid:111)

t   dc

l(1)
t

t

(cid:40) T(cid:88)

t=1

Ea

T(cid:88)

t=1

(cid:110)
(cid:41)

(cid:18)

t

(cid:19) T

2

t − min
l(at)

p

p(i)l(i)
t

≥

1 − 1
K

(28)

but still the step sizes {ηt} for Algorithm 1 can be chosen such that the conclusion of Theorem 3 still
holds (“convergence to NE”).

t=1 η2

t = dc

Proof. Let dr

t < ∞ (cid:80)T

t = dt = t and ηt = 1
t < ∞ and lim

 (cid:80)T
This sequence yields an expected regret of exactly(cid:0)1 − 1

t=1 ηt = ∞
t log t for all t  for which dtη2
t→∞ηtdt = 0. Hence  Theorem 3 applies and ( ¯pT   ¯qT )
2 rounds is
2 . Consider the
t = 1 for all j > 1 and all t > T
2 .

converges in L1 to the set of NE of the game. However  the feedback for the last T
never received. Therefore  the mixed strategies pt and qt stay constant for all t ≥ T
sequence of costs l(i)

t = 0 for all i and all t ≤ T

t = 0  l(j)

2 and l(1)

t = 1

t=1 dtη2

(cid:1) T

t log2 t so(cid:80)T

2 .

K

4 Conclusions

(cid:18)(cid:114)

(cid:16)

KT +(cid:80)T

(cid:17)(cid:19)
(cid:16)√

In this paper  we analyzed the regret of the EXP3 algorithm subjected to an arbitrary (i.e. 
adversarial) sequence dt of feedback delays. We have shown that
is

the expected regret

(cid:17)

ln K

t=1 dt

. This shows that the EXP3 algorithm is inherently robust to de-
O
lays  since for dt ≤ K the order of magnitude of the regret does not change (as a function of T
and K) from the famous O
. We have also proved that the convergence of the ergodic
average to a Nash equilibrium under delays is a more robust property than the no-regret property of
EXP3. The ergodic average converges to the set of Nash equilibria even under super-linear delays
where EXP3 has a linear regret in T . This serves as a concrete example where competing versus
another agent is essentially easier than competing versus an omnipotent adversary  even if the other
agent is not subject to any delays.

K ln KT

Acknowledgments

This research was supported by the Koret Foundation grant for Smart Cities and Digital Living.
Zhengyuan Zhou gratefully acknowledges IBM Goldstine Fellowship. Xi Chen is supported by
NSF via IIS-1845444.

9

References
[1] N. Cesa-Bianchi  C. Gentile  and Y. Mansour  “Delay and cooperation in nonstochastic ban-

dits ” The Journal of Machine Learning Research  vol. 20  no. 1  pp. 613–650  2019.

[2] Z. Zhou  P. Mertikopoulos  N. Bambos  P. W. Glynn  and C. Tomlin  “Countering feedback
delays in multi-agent learning ” in Advances in Neural Information Processing Systems  2017 
pp. 6171–6181.

[3] P. Joulani  A. Gyorgy  and C. Szepesv´ari  “Online learning under delayed feedback ” in Inter-

national Conference on Machine Learning  2013  pp. 1453–1461.

[4] A. Agarwal and J. C. Duchi  “Distributed delayed stochastic optimization ” in Advances in

Neural Information Processing Systems  2011  pp. 873–881.

[5] G. Neu  A. Antos  A. Gy¨orgy  and C. Szepesv´ari  “Online markov decision processes under
bandit feedback ” in Advances in Neural Information Processing Systems  2010  pp. 1804–
1812.

[6] Z. Zhou  R. Xu  and J. Blanchet  “Learning in generalized linear contextual bandits with

stochastic delays ” in Advances in Neural Information Processing Systems  2019.

[7] M. J. Weinberger and E. Ordentlich  “On delayed prediction of individual sequences ” IEEE

Transactions on Information Theory  vol. 48  no. 7  pp. 1959–1976  2002.

[8] M. Zinkevich  J. Langford  and A. J. Smola  “Slow learners are fast ” in Advances in neural

information processing systems  2009  pp. 2331–2339.

[9] T. Mandel  Y.-E. Liu  E. Brunskill  and Z. Popovi´c  “The queue method: Handling delay 
heuristics  prior data  and evaluation in bandits ” in Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence  2015.

[10] C. Vernade  O. Capp´e  and V. Perchet  “Stochastic bandit models for delayed conversions ”

arXiv preprint arXiv:1706.09186  2017.

[11] C. Pike-Burke  S. Agrawal  C. Szepesvari  and S. Grunewalder  “Bandits with delayed  aggre-
gated anonymous feedback ” in Proceedings of the 35th International Conference on Machine
Learning  2018  pp. 4105–4113.

[12] Z. Zhou  P. Mertikopoulos  N. Bambos  P. Glynn  Y. Ye  L.-J. Li  and L. Fei-Fei  “Distributed
asynchronous optimization with unbounded delays: How slow can you go?” in International
Conference on Machine Learning  2018  pp. 5965–5974.

[13] K. Quanrud and D. Khashabi  “Online learning with adversarial delays ” in Advances in neural

information processing systems  2015  pp. 1270–1278.

[14] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire  “Gambling in a rigged casino: The
adversarial multi-armed bandit problem ” in Proceedings of IEEE 36th Annual Foundations of
Computer Science.

IEEE  1995  pp. 322–331.

[15] G. Stoltz  “Information incompl´ete et regret interne en pr´ediction de suites individuelles ”

Ph.D. dissertation  Universit´e Paris-XI Orsay  Orsay  France  2005.
[16] S. Bubeck  N. Cesa-Bianchi et al.  “Regret analysis of stochastic and nonstochastic multi-
armed bandit problems ” Foundations and Trends R(cid:13) in Machine Learning  vol. 5  no. 1  pp.
1–122  2012.

[17] M. Bowling  “Convergence and no-regret in multiagent learning ” in Advances in neural infor-

mation processing systems  2005  pp. 209–216.

[18] Y. Cai and C. Daskalakis  “On minmax theorems for multiplayer games ” in Proceedings of the
twenty-second annual ACM-SIAM symposium on Discrete Algorithms. Society for Industrial
and Applied Mathematics  2011  pp. 217–234.

[19] J. P. Bailey and G. Piliouras  “Multiplicative weights update in zero-sum games ” in Proceed-
ings of the 2018 ACM Conference on Economics and Computation. ACM  2018  pp. 321–338.
[20] N. Cesa-Bianchi  Y. Freund  D. Haussler  D. P. Helmbold  R. E. Schapire  and M. K. Warmuth 

“How to use expert advice ” Journal of the ACM (JACM)  vol. 44  no. 3  pp. 427–485  1997.

10

,Ilai Bistritz
Zhengyuan Zhou
Xi Chen
Jose Blanchet