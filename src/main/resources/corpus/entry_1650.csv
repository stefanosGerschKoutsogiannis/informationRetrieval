2017,Dykstra's Algorithm  ADMM  and Coordinate Descent: Connections  Insights  and Extensions,We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets  the augmented Lagrangian method of multipliers or ADMM  and block coordinate descent. We prove that coordinate descent for a regularized regression problem  in which the penalty is a separable sum of support functions  is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent  in the special case of two sets  with one being a linear subspace. These connections  aside from being interesting in their own right  suggest new ways of analyzing and extending coordinate descent. For example  from existing convergence theory on Dykstra's algorithm over polyhedra  we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent  based on the Dykstra and ADMM connections.,Dykstra’s Algorithm  ADMM  and Coordinate
Descent: Connections  Insights  and Extensions

Department of Statistics and Machine Learning Department

Ryan J. Tibshirani

Carnegie Mellon University

Pittsburgh  PA 15213

ryantibs@stat.cmu.edu

Abstract

We study connections between Dykstra’s algorithm for projecting onto an intersec-
tion of convex sets  the augmented Lagrangian method of multipliers or ADMM 
and block coordinate descent. We prove that coordinate descent for a regularized
regression problem  in which the penalty is a separable sum of support functions 
is exactly equivalent to Dykstra’s algorithm applied to the dual problem. ADMM
on the dual problem is also seen to be equivalent  in the special case of two sets 
with one being a linear subspace. These connections  aside from being interesting
in their own right  suggest new ways of analyzing and extending coordinate de-
scent. For example  from existing convergence theory on Dykstra’s algorithm over
polyhedra  we discern that coordinate descent for the lasso problem converges at
an (asymptotically) linear rate. We also develop two parallel versions of coordinate
descent  based on the Dykstra and ADMM connections.

1

Introduction

In this paper  we study two seemingly unrelated but closely connected convex optimization problems 
and associated algorithms. The ﬁrst is the best approximation problem: given closed  convex sets
C1  . . .   Cd ⊆ Rn and y ∈ Rn  we seek the point in C1 ∩ ··· ∩ Cd (assumed nonempty) closest to y 
and solve

(1)
The second problem is the regularized regression problem: given a response y ∈ Rn and predictors
X ∈ Rn×p  and a block decomposition Xi ∈ Rn×pi  i = 1  . . .   d of the columns of X (i.e.  these
could be columns  or groups of columns)  we build a working linear model by applying blockwise
regularization over the coefﬁcients  and solve

subject to

u ∈ C1 ∩ ··· ∩ Cd.

(cid:107)y − u(cid:107)2

min
u∈Rn

2

d(cid:88)

(cid:107)y − Xw(cid:107)2

1
2

(2)
where hi : Rpi → R  i = 1  . . .   d are convex functions  and we write wi ∈ Rpi  i = 1  . . .   d for the

appropriate block decomposition of a coefﬁcient vector w ∈ Rp (so that Xw =(cid:80)d

hi(wi) 

min
w∈Rp

2 +

i=1

i=1 Xiwi).

Two well-studied algorithms for problems (1)  (2) are Dykstra’s algorithm (Dykstra  1983; Boyle and
Dykstra  1986) and (block) coordinate descent (Warga  1963; Bertsekas and Tsitsiklis  1989; Tseng 
1990)  respectively. The jumping-off point for our work in this paper is the following fact: these two
algorithms are equivalent for solving (1) and (2). That is  for a particular relationship between the
sets C1  . . .   Cd and penalty functions h1  . . .   hd  the problems (1) and (2) are duals of each other 
and Dykstra’s algorithm on the primal problem (1) is exactly the same as coordinate descent on the
dual problem (2). We provide details in Section 2.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

This equivalence between Dykstra’s algorithm and coordinate descent can be essentially found in
the optimization literature  dating back to the late 1980s  and possibly earlier. (We say “essentially”
here because  to our knowledge  this equivalence has not been stated for a general regression matrix
X  and only in the special case X = I; but  in truth  the extension to a general matrix X is fairly
straightforward.) Though this equivalence has been cited and discussed in various ways over the
years  we feel that it is not as well-known as it should be  especially in light of the recent resurgence
of interest in coordinate descent methods. We revisit the connection between Dykstra’s algorithm
and coordinate descent  and draw further connections to a third method—the augmented Lagrangian
method of multipliers or ADMM (Glowinski and Marroco  1975; Gabay and Mercier  1976)—that
has also received a great deal of attention recently. While these basic connections are interesting in
their own right  they also have important implications for analyzing and extending coordinate descent.
Below we give a summary of our contributions.

1. We prove in Section 2 (under a particular relationship between C1  . . .   Cd and h1  . . .   hd)
that Dykstra’s algorithm for (1) is equivalent to block coordinate descent for (2). (This is a
mild generalization of the previously known connection when X = I.)

2. We also show in Section 2 that ADMM is closely connected to Dykstra’s algorithm  in that

ADMM for (1)  when d = 2 and C1 is a linear subspace  matches Dykstra’s algorithm.

3. Leveraging existing results on the convergence of Dykstra’s algorithm for an intersection of
halfspaces  we establish in Section 3 that coordinate descent for the lasso problem has an
(asymptotically) linear rate of convergence  regardless of the dimensions of X (i.e.  without
assumptions about strong convexity of the problem). We derive two different explicit forms
for the error constant  which shed light onto how correlations among the predictor variables
affect the speed of convergence.

4. Appealing to parallel versions of Dykstra’s algorithm and ADMM  we present in Section 4
two parallel versions of coordinate descent (each guaranteed to converge in full generality).
5. We extend in Section 5 the equivalence between coordinate descent and Dykstra’s algorithm
to the case of nonquadratic loss in (2)  i.e.  non-Euclidean projection in (1). This leads to a
Dykstra-based parallel version of coordinate descent for (separably regularized) problems
with nonquadratic loss  and we also derive an alternative ADMM-based parallel version of
coordinate descent for the same class of problems.

2 Preliminaries and connections

Dykstra’s algorithm. Dykstra’s algorithm was ﬁrst proposed by Dykstra (1983)  and was extended
to Hilbert spaces by Boyle and Dykstra (1986). Since these seminal papers  a number of works have
analyzed and extended Dykstra’s algorithm in various interesting ways. We will reference many of
these works in the coming sections  when we discuss connections between Dykstra’s algorithm and
other methods; for other developments  see the comprehensive books Deutsch (2001); Bauschke and
Combettes (2011) and review article Bauschke and Koch (2013).
Dykstra’s algorithm for the best approximation problem (1) can be described as follows. We initialize
u(0) = y  z(−d+1) = ··· = z(0) = 0  and then repeat  for k = 1  2  3  . . .:

u(k) = PC[k] (u(k−1) + z(k−d)) 
z(k) = u(k−1) + z(k−d) − u(k) 

(3)

where PC(x) = argminc∈C (cid:107)x − c(cid:107)2
2 denotes the (Euclidean) projection of x onto a closed  convex
set C  and [·] denotes the modulo operator taking values in {1  . . .   d}. What differentiates Dykstra’s
algorithm from the classical alternating projections method of von Neumann (1950); Halperin (1962)
is the sequence of (what we may call) dual variables z(k)  k = 1  2  3  . . .. These track  in a cyclic
fashion  the residuals from projecting onto C1  . . .   Cd. The simpler alternating projections method
will always converge to a feasible point in C1 ∩ ··· ∩ Cd  but will not necessarily converge to the
solution in (1) unless C1  . . .   Cd are subspaces (in which case alternating projections and Dykstra’s
algorithm coincide). Meanwhile  Dykstra’s algorithm converges in general (for any closed  convex
sets C1  . . .   Cd with nonempty intersection  see  e.g.  Boyle and Dykstra (1986); Han (1988); Gaffke
and Mathar (1989)). We note that Dykstra’s algorithm (3) can be rewritten in a different form  which

2

will be helpful for future comparisons. First  we initialize u(0)
repeat  for k = 1  2  3  . . .:

d = y  z(0)

1 = ··· = z(0)

d = 0  and then

 

d

0 = u(k−1)
u(k)
u(k)
i = PCi(u(k)
z(k)
i = u(k)

i−1 + z(k−1)
) 
− u(k)

i−1 + z(k−1)

i

i

i

(cid:41)

for i = 1  . . .   d.

 

(4)

Coordinate descent. Coordinate descent methods have a long history in optimization  and have
been studied and discussed in early papers and books such as Warga (1963); Ortega and Rheinboldt
(1970); Luenberger (1973); Auslender (1976); Bertsekas and Tsitsiklis (1989)  though coordinate
descent was still likely in use much earlier. (Of course  for solving linear systems  coordinate descent
reduces to Gauss-Seidel iterations  which dates back to the 1800s.) Some key papers analyzing the
convergence of coordinate descent methods are Tseng and Bertsekas (1987); Tseng (1990); Luo and
Tseng (1992  1993); Tseng (2001). In the last 10 or 15 years  a considerable interest in coordinate
descent has developed across the optimization community. With the ﬂurry of recent work  it would be
difﬁcult to give a thorough account of the recent progress on the topic. To give just a few examples 
recent developments include ﬁnite-time (nonasymptotic) convergence rates for coordinate descent 
and exciting extensions such as accelerated  parallel  and distributed versions of coordinate descent.
We refer to Wright (2015)  an excellent survey that describes this recent progress.
In (block) coordinate descent1 for (2)  we initialize say w(0) = 0  and repeat  for k = 1  2  3  . . .:

(cid:13)(cid:13)(cid:13)(cid:13)y −(cid:88)

j<i

j −(cid:88)

j>i

Xjw(k)

w(k)

i = argmin
wi∈Rpi

1
2

(cid:13)(cid:13)(cid:13)(cid:13)2

2

Xjw(k−1)

j

− Xiwi

+ hi(wi) 

i = 1  . . .   d.

(5)

We assume here and throughout that Xi ∈ Rn×pi  i = 1  . . .   d each have full column rank so that
the updates in (5) are uniquely deﬁned (this is used for convenience  and is not a strong assumption;
note that there is no restriction on the dimensionality of the full problem in (2)  i.e.  we could still
have X ∈ Rn×p with p (cid:29) n). The precise form of these updates  of course  depends on the penalty
functions. Suppose that each hi is the support function of a closed  convex set Di ⊆ Rpi  i.e. 

(cid:104)d  v(cid:105) 

hi(v) = max
d∈Di

i )−1(Di) = {v ∈ Rn : X T

for i = 1  . . .   d.
i v ∈ Di}  the inverse image of Di under the
Suppose also that Ci = (X T
linear map X T
i   for i = 1  . . .   d. Then  perhaps surprisingly  it turns out that the coordinate descent
iterations (5) are exactly the same as the Dykstra iterations (4)  via a duality argument. We extract
the key relationship as a lemma below  for future reference  and then state the formal equivalence.
Proofs of these results  as with all results in this paper  are given in the supplement.
Lemma 1. Assume that Xi ∈ Rn×pi has full column rank and hi(v) = maxd∈Di(cid:104)d  v(cid:105) for a closed 
convex set Di ⊆ Rpi. Then for Ci = (X T
(cid:107)b − Xiwi(cid:107)2

2 + hi(wi) ⇐⇒ Xi ˆwi = (Id − PCi)(b).

i )−1(Di) ⊆ Rn and any b ∈ Rn 

ˆwi = argmin
wi∈Rpi

1
2

where Id(·) denotes the identity mapping.
Theorem 1. Assume the setup in Lemma 1  for each i = 1  . . .   d. Then problems (1)  (2) are dual to
each other  and their solutions  denoted ˆu  ˆw  respectively  satisfy ˆu = y − X ˆw. Further  Dykstra’s
algorithm (4) and coordinate descent (5) are equivalent  and satisfy at all iterations k = 1  2  3  . . .:

z(k)
i = Xiw(k)

i

and u(k)

Xjw(k−1)

j

 

for i = 1  . . .   d.

i = y −(cid:88)

j≤i

j −(cid:88)

j>i

Xjw(k)

The equivalence between coordinate descent and Dykstra’s algorithm dates back to (at least) Han
(1988); Gaffke and Mathar (1989)  under the special case X = I. In fact  Han (1988)  presumably
unaware of Dykstra’s algorithm  seems to have reinvented the method and established convergence

1To be precise  this is cyclic coordinate descent  where exact minimization is performed along each block of
coordinates. Randomized versions of this algorithm have recently become popular  as have inexact or proximal
versions. While these variants are interesting  they are not the focus of our paper.

3

through its relationship to coordinate descent. This work then inspired Tseng (1993) (who must have
also been unaware of Dykstra’s algorithm) to improve the existing analyses of coordinate descent 
which at the time all assumed smoothness of the objective function. (Tseng continued on to become
arguably the single most important contributor to the theory of coordinate descent of the 1990s and
2000s  and his seminal work Tseng (2001) is still one of the most comprehensive analyses to date.)
References to this equivalence can be found speckled throughout the literature on Dykstra’s method 
but given the importance of the regularized problem form (2) for modern statistical and machine
learning estimation tasks  we feel that the connection between Dykstra’s algorithm and coordinate
descent and is not well-known enough and should be better explored. In what follows  we show that
some old work on Dykstra’s algorithm  fed through this equivalence  yields new convergence results
for coordinate descent for the lasso and a new parallel version of coordinate descent.

ADMM. The augmented Lagrangian method of multipliers or ADMM was invented by Glowinski
and Marroco (1975); Gabay and Mercier (1976). ADMM is a member of a class of methods generally
called operator splitting techniques  and is equivalent (via a duality argument) to Douglas-Rachford
splitting (Douglas and Rachford  1956; Lions and Mercier  1979). Recently  there has been a strong
revival of interest in ADMM (and operator splitting techniques in general)  arguably due (at least in
part) to the popular monograph of Boyd et al. (2011)  where it is argued that the ADMM framework
offers an appealing ﬂexibility in algorithm design  which permits parallelization in many nontrivial
situations. As with coordinate descent  it would be difﬁcult thoroughly describe recent developments
on ADMM  given the magnitude and pace of the literature on this topic. To give just a few examples 
recent progress includes ﬁnite-time linear convergence rates for ADMM (see Nishihara et al. 2015;
Hong and Luo 2017 and references therein)  and accelerated extensions of ADMM (see Goldstein
et al. 2014; Kadkhodaie et al. 2015 and references therein).
To derive an ADMM algorithm for (1)  we introduce auxiliary variables and equality constraints to
put the problem in a suitable ADMM form. While different formulations for the auxiliary variables
and constraints give rise to different algorithms  loosely speaking  these algorithms generally take on
similar forms to Dykstra’s algorithm for (1). The same is also true of ADMM for the set intersection
problem  a simpler task than the best approximation problem (1)  in which we only seek a point in
the intersection C1 ∩ ··· ∩ Cd  and solve

min
u∈Rn

ICi(ui) 

(6)
where IC(·) denotes the indicator function of a set C (equal to 0 on C  and ∞ otherwise). Consider
the case of d = 2 sets  in which case the translation of (6) into ADMM form is unambiguous. ADMM
for (6)  properly initialized  appears highly similar to Dykstra’s algorithm for (1); so similar  in fact 
that Boyd et al. (2011) mistook the two algorithms for being equivalent  which is not generally true 
and was shortly thereafter corrected by Bauschke and Koch (2013).
Below we show that when d = 2  C1 is a linear subspace  and y ∈ C1  an ADMM algorithm for (1)
(and not the simpler set intersection problem (6)) is indeed equivalent to Dykstra’s algorithm for (1).
Introducing auxiliary variables  the problem (1) becomes
2 + IC1 (u1) + IC2(u2)
The augmented Lagrangian is L(u1  u2  z) = (cid:107)y − u1(cid:107)2
−ρ(cid:107)z(cid:107)2

2 + IC1(u1) + IC2(u2) + ρ(cid:107)u1 − u2 + z(cid:107)2
2
2  where ρ > 0 is an augmented Lagrangian parameter. ADMM repeats  for k = 1  2  3  . . .:

(cid:107)y − u1(cid:107)2

u1 u2∈Rn

subject to

u1 = u2.

min

d(cid:88)

i=1

(cid:18) y

(cid:19)

ρ(u(k−1)

2

− z(k−1))

+

u(k)
1 = PC1

2 = PC2 (u(k)
u(k)
z(k) = z(k−1) + u(k)

1 + ρ
1 + z(k−1)) 
1 − u(k)
2 .

1 + ρ

 

(7)

Suppose we initialize u(0)
and a simple inductive argument  the above iterations can be rewritten as

2 = y  z(0) = 0  and set ρ = 1. Using linearity of PC1  the fact that y ∈ C1 

2

) 

1 = PC1 (u(k−1)
u(k)
u(k)
2 = PC2 (u(k)
z(k) = z(k−1) + u(k)

1 + z(k−1)) 
1 − u(k)
2  

4

(8)

1   k = 1  2  3  . . . in Dykstra’s iterations plays no role and can be ignored.

which is precisely the same as Dykstra’s iterations (4)  once we realize that  due again to linearity of
PC1  the sequence z(k)
Though d = 2 sets in (1) may seem like a rather special case  the strategy for parallelization in both
Dykstra’s algorithm and ADMM stems from rewriting a general d-set problem as a 2-set problem  so
the above connection between Dykstra’s algorithm and ADMM can be relevant even for problems
with d > 2  and will reappear in our later discussion of parallel coordinate descent. As a matter of
conceptual interest only  we note that for general d (and no constraints on the sets being subspaces) 
Dykstra’s iterations (4) can be viewed as a limiting version of the ADMM iterations either for (1) or
for (6)  as we send the augmented Lagrangian parameters to ∞ or to 0 at particular scalings. See the
supplement for details.

3 Coordinate descent for the lasso
The lasso problem (Tibshirani  1996; Chen et al.  1998)  deﬁned for a tuning parameter λ ≥ 0 as

1
2

(cid:107)y − Xw(cid:107)2

2 + λ(cid:107)w(cid:107)1 

min
w∈Rp

i )−1(Di) = {v ∈ Rn : |X T

(9)
is a special case of (2) where the coordinate blocks are of each size 1  so that Xi ∈ Rn  i = 1  . . .   p
are just the columns of X  and wi ∈ R  i = 1  . . .   p are the components of w. This problem ﬁts into
the framework of (2) with hi(wi) = λ|wi| = maxd∈Di dwi for Di = [−λ  λ]  for each i = 1  . . .   d.
Coordinate descent is widely-used for the lasso (9)  both because of the simplicity of the coordinate-
wise updates  which reduce to soft-thresholding  and because careful implementations can achieve
state-of-the-art performance  at the right problem sizes. The use of coordinate descent for the lasso
was popularized by Friedman et al. (2007  2010)  but was studied earlier or concurrently by several
others  e.g.  Fu (1998); Sardy et al. (2000); Wu and Lange (2008).
As we know from Theorem 1  the dual of problem (9) is the best approximation problem (1)  where
i v| ≤ λ} is an intersection of two halfspaces  for i = 1  . . .   p.
Ci = (X T
This makes C1 ∩ ··· ∩ Cd an intersection of 2p halfspaces  i.e.  a (centrally symmetric) polyhedron.
For projecting onto a polyhedron  it is well-known that Dykstra’s algorithm reduces to Hildreth’s
algorithm (Hildreth  1957)  an older method for quadratic programming that itself has an interesting
history in optimization. Theorem 1 hence shows coordinate descent for the lasso (9) is equivalent not
only to Dykstra’s algorithm  but also to Hildreth’s algorithm  for (1).
This equivalence suggests a number of interesting directions to consider. For example  key practical
speedups have been developed for coordinate descent for the lasso that enable this method to attain
state-of-the-art performance at the right problem sizes  such as clever updating rules and screening
rules (e.g.  Friedman et al. 2010; El Ghaoui et al. 2012; Tibshirani et al. 2012; Wang et al. 2015).
These implementation tricks can now be used with Dykstra’s (Hildreth’s) algorithm. On the ﬂip side 
as we show next  older results from Iusem and De Pierro (1990); Deutsch and Hundal (1994) on
Dykstra’s algorithm for polyhedra  lead to interesting new results on coordinate descent for the lasso.
Theorem 2 (Adaptation of Iusem and De Pierro 1990). Assume the columns of X ∈ Rn×p are in
general position  and λ > 0. Then coordinate descent for the lasso (9) has an asymptotically linear
convergence rate  in that for large enough k 

a2

a2 + λmin(X T

(10)
where ˆw is the lasso solution in (9)  Σ = X T X  and (cid:107)z(cid:107)2
Σ = zT Σz for z ∈ Rp  A = supp( ˆw) is
the active set of ˆw  a = |A| is its size  XA ∈ Rn×a denotes the columns of X indexed by A  and
λmin(X T
Theorem 3 (Adaptation of Deutsch and Hundal 1994). Assume the same conditions and notation
as in Theorem 2. Then for large enough k 

A XA) denotes the smallest eigenvalue of X T

A XA)/ maxi∈A (cid:107)Xi(cid:107)2

A XA.

2

 

(cid:18)

(cid:107)w(k+1) − ˆw(cid:107)Σ
(cid:107)w(k) − ˆw(cid:107)Σ

≤

(cid:19)1/2

(cid:33)1/2

(cid:107)w(k+1) − ˆw(cid:107)Σ
(cid:107)w(k) − ˆw(cid:107)Σ

≤

(cid:107)P ⊥

{ij+1 ... ia}Xij(cid:107)2

2

(cid:107)Xij(cid:107)2

2

 

(11)

where we enumerate A = {i1  . . .   ia}  i1 < . . . < ia  and we denote by P ⊥
onto the orthocomplement of the column span of X{ij+1 ... ia}.

{ij+1 ... ia} the projection

(cid:32)
1 − a−1(cid:89)

j=1

5

The results in Theorems 2  3 both rely on the assumption of general position for the columns of X.
This is only used for convenience and can be removed at the expense of more complicated notation.
Loosely put  the general position condition simply rules out trivial linear dependencies between small
numbers of columns of X  but places no restriction on the dimensions of X (i.e.  it still allows for
p (cid:29) n). It implies that the lasso solution ˆw is unique  and that XA (where A = supp( ˆw)) has full
column rank. See Tibshirani (2013) for a precise deﬁnition of general position and proofs of these
facts. We note that when XA has full column rank  the bounds in (10)  (11) are strictly less than 1.
Remark 1 (Comparing (10) and (11)). Clearly  both the bounds in (10)  (11) are adversely affected
by correlations among Xi  i ∈ A (i.e.  stronger correlations will bring each closer to 1). It seems to
us that (11) is usually the smaller of the two bounds  based on simple mathematical and numerical
comparisons. More detailed comparisons would be interesting  but is beyond the scope of this paper.
Remark 2 (Linear convergence without strong convexity). One striking feature of the results in
Theorems 2  3 is that they guarantee (asymptotically) linear convergence of the coordinate descent
iterates for the lasso  with no assumption about strong convexity of the objective. More precisely 
there are no restrictions on the dimensionality of X  so we enjoy linear convergence even without an
assumption on the smooth part of the objective. This is in line with classical results on coordinate
descent for smooth functions  see  e.g.  Luo and Tseng (1992). The modern ﬁnite-time convergence
analyses of coordinate descent do not  as far as we understand  replicate this remarkable property.
For example  Beck and Tetruashvili (2013); Li et al. (2016) establish ﬁnite-time linear convergence
rates for coordinate descent  but require strong convexity of the entire objective.
Remark 3 (Active set identiﬁcation). The asymptotics developed in Iusem and De Pierro (1990);
Deutsch and Hundal (1994) are based on a notion of (in)active set identiﬁcation: the critical value of
k after which (10)  (11) hold is based on the (provably ﬁnite) iteration number at which Dykstra’s
algorithm identiﬁes the inactive halfspaces  i.e.  at which coordinate descent identiﬁes the inactive
set of variables  Ac = supp( ˆw)c. This might help explain why in practice coordinate descent for the
lasso performs exceptionally well with warm starts  over a decreasing sequence of tuning parameter
values λ (e.g.  Friedman et al. 2007  2010): here  each coordinate descent run is likely to identify the
(in)active set—and hence enter the linear convergence phase—at an early iteration number.

4 Parallel coordinate descent

d(cid:88)

Parallel-Dykstra-CD. An important consequence of the connection between Dykstra’s algorithm
and coordinate descent is a new parallel version of the latter  stemming from an old parallel version
of the former. A parallel version of Dykstra’s algorithm is usually credited to Iusem and Pierro (1987)
for polyhedra and Gaffke and Mathar (1989) for general sets  but really the idea dates back to the
product space formalization of Pierra (1984). We rewrite problem (1) as

γi(cid:107)y − ui(cid:107)2

2

i=1

min

subject to

u=(u1 ... ud)∈Rnd

(12)
where C0 = {(u1  . . .   ud) ∈ Rnd : u1 = ··· = ud}  and γ1  . . .   γd > 0 are weights that sum to 1.
After rescaling appropriately to turn (12) into an unweighted best approximation problem  we can
apply Dykstra’s algorithm  which sets u(0)
γiu(k−1)

1 = ··· = z(0)

d = 0  and repeats:

d = y  z(0)

d(cid:88)

u(k)
0 =

u ∈ C0 ∩ (C1 × ··· × Cd) 

(13)

1 = ··· = u(0)
(cid:41)

 

i

i=1

u(k)
i = PCi(u(k)
z(k)
i = u(k)

0 + z(k−1)

0 + z(k−1)
) 
− u(k)

i

i

i

for i = 1  . . .   d 

 

for k = 1  2  3  . . .. The steps enclosed in curly brace above can all be performed in parallel  so that
(13) is a parallel version of Dykstra’s algorithm (4) for (1). Applying Lemma 1  and a straightforward
inductive argument  the above algorithm can be rewritten as follows. We set w(0) = 0  and repeat:

(cid:13)(cid:13)(cid:13)y− Xw(k−1) + Xiw(k−1)

i

w(k)

i = argmin
wi∈Rpi

1
2

/γi− Xiwi/γi

+ hi(wi/γi) 

i = 1  . . .   d  (14)

(cid:13)(cid:13)(cid:13)2

2

for k = 1  2  3  . . .  which we call parallel-Dykstra-CD (with CD being short for coordinate descent).
Again  note that the each of the d coordinate updates in (14) can be performed in parallel  so that

6

(14) is a parallel version of coordinate descent (5) for (2). Also  as (14) is just a reparametrization of
Dykstra’s algorithm (13) for the 2-set problem (12)  it is guaranteed to converge in full generality  as
per the standard results on Dykstra’s algorithm (Han  1988; Gaffke and Mathar  1989).
Theorem 4. Assume that Xi ∈ Rn×pi has full column rank and hi(v) = maxd∈Di(cid:104)d  v(cid:105) for a closed 
convex set Di ⊆ Rpi  for i = 1  . . .   d. If (2) has a unique solution  then the iterates in (14) converge
to this solution. More generally  if the interior of ∩d
i )−1(Di) is nonempty  then the sequence
w(k)  k = 1  2  3  . . . from (14) has at least one accumulation point  and any such point solves (2).
Further  Xw(k)  k = 1  2  3  . . . converges to X ˆw  the optimal ﬁtted value in (2).

i=1(X T

There have been many recent exciting contributions to the parallel coordinate descent literature; two
standouts are Jaggi et al. (2014); Richtarik and Takac (2016)  and numerous others are described in
Wright (2015). What sets parallel-Dykstra-CD apart  perhaps  is its simplicity: convergence of the
iterations (14)  given in Theorem 4  just stems from the connection between coordinate descent and
Dykstra’s algorithm  and the fact that the parallel Dykstra iterations (13) are nothing more than the
usual Dykstra iterations after a product space reformulation. Moreover  parallel-Dykstra-CD for the
lasso enjoys an (asymptotic) linear convergence rate under essentially no assumptions  thanks once
again to an old result on the parallel Dykstra (Hildreth) algorithm from Iusem and De Pierro (1990).
The details can be found in the supplement.

Parallel-ADMM-CD. As an alternative to the parallel method derived using Dykstra’s algorithm 
ADMM can also offer a version of parallel coordinate descent. Since (12) is a best approximation
problem with d = 2 sets  we can refer back to our earlier ADMM algorithm in (7) for this problem.
By passing these ADMM iterations through the connection developed in Lemma 1  we arrive at what
we call parallel-ADMM-CD  which initializes u(0)

0 = y  w(−1) = w(0) = 0  and repeats:

((cid:80)d
1 +(cid:80)d

0

i=1 ρi)u(k−1)
i=1 ρi
1
2

(cid:13)(cid:13)(cid:13)u(k)

+

1 +(cid:80)d

y − Xw(k−1)
i=1 ρi
/ρi − Xiwi/ρi

+

1 +(cid:80)d
(cid:13)(cid:13)(cid:13)2

2

0 + Xiw(k−1)

i

X(w(k−2) − w(k−1))

 

i=1 ρi

+ hi(wi/ρi) 

u(k)
0 =

w(k)

i = argmin
wi∈Rpi

(15)

i = 1  . . .   d 

i

for k = 1  2  3  . . .  where ρ1  . . .   ρd > 0 are augmented Lagrangian parameters. In each iteration 
the updates to w(k)
  i = 1  . . .   d above can be done in parallel. Just based on their form  it seems
that (15) can be seen as a parallel version of coordinate descent (5) for problem (2). The next result
conﬁrms this  leveraging standard theory for ADMM (Gabay  1983; Eckstein and Bertsekas  1992).
Theorem 5. Assume that Xi ∈ Rn×pi has full column rank and hi(v) = maxd∈Di(cid:104)d  v(cid:105) for a closed 
convex set Di ⊆ Rpi  for i = 1  . . .   d. Then the sequence w(k)  k = 1  2  3  . . . in (15) converges to
a solution in (2).

0

0

The parallel-ADMM-CD iterations in (15) and parallel-Dykstra-CD iterations in (14) differ in that 
where the latter uses a residual y − Xw(k−1)  the former uses an iterate u(k)
that seems to have a
more complicated form  being a convex combination of u(k−1)
and y − Xw(k−1)  plus a quantity
that acts like a momentum term. It turns out that when ρ1  . . .   ρd sum to 1  the two methods (14) 
(15) are exactly the same. While this may seem like a surprising coincidence  it is in fact nothing
more than a reincarnation of the previously established equivalence between Dykstra’s algorithm (4)
and ADMM (8) for a 2-set best approximation problem  as here C0 is a linear subspace.
Of course  with ADMM we need not choose probability weights for ρ1  . . .   ρd  and the convergence
in Theorem 5 is guaranteed for any ﬁxed values of these parameters. Thus  even though they were
derived from different perspectives  parallel-ADMM-CD subsumes parallel-Dykstra-CD  and it is a
strictly more general approach. It is important to note that larger values of ρ1  . . .   ρd can often lead
to faster convergence in practice  as we show in Figure 1. More detailed study and comparisons to
related parallel methods are worthwhile  but are beyond the scope of this work.

5 Discussion and extensions

We studied connections between Dykstra’s algorithm  ADMM  and coordinate descent. Leveraging
these connections  we established an (asymptotically) linear convergence rate for coordinate descent
for the lasso  as well as two parallel versions of coordinate descent (one based on Dykstra’s algorithm
and the other on ADMM). Some extensions and possibilities for future work are described below.

7

of parallel-ADMM-CD (i.e.  three different values of ρ =(cid:80)p

Figure 1: Suboptimality curves for serial coordinate descent  parallel-Dykstra-CD  and three tunings
i=1 ρi)  each run over the same 30 lasso

problems with n = 100 and p = 500. For details of the experimental setup  see the supplement.

Nonquadratic loss: Dykstra’s algorithm and coordinate descent. Given a convex function f  a
generalization of (2) is the regularized estimation problem

d(cid:88)
2(cid:107)y − z(cid:107)2

i=1

min
w∈Rp

f (Xw) +

hi(wi).

(16)

2  and e.g.  regularized classiﬁcation (under
i=1 log(1 + ezi). In (block) coordinate descent for (16)  we

Regularized regression (2) is given by f (z) = 1

the logistic loss) by f (z) = −yT z +(cid:80)n
(cid:88)

initialize say w(0) = 0  and repeat  for k = 1  2  3  . . .:
Xjw(k−1)

(cid:18)(cid:88)

Xjw(k)

w(k)

f

j +

j

i = argmin
wi∈Rpi

j<i

j>i

(cid:19)

+ Xiwi

+ hi(wi) 

i = 1  . . .   d.

(17)

On the other hand  given a differentiable and strictly convex function g  we can generalize (1) to the
following best Bregman-approximation problem 

min
u∈Rn

Dg(u  b)

(18)
where Dg(u  b) = g(u) − g(b) − (cid:104)∇g(b)  u − b(cid:105) is the Bregman divergence between u and b with
2(cid:107)v(cid:107)2
2 (and b = y)  this recovers the best approximation problem (1). As
respect to g. When g(v) = 1
shown in Censor and Reich (1998); Bauschke and Lewis (2000)  Dykstra’s algorithm can be extended
to apply to (18). We initialize u(0)
d = b  z(0)

d = 0  and repeat for k = 1  2  3  . . .:

1 = ··· = z(0)

subject to

u ∈ C1 ∩ ··· ∩ Cd.

d

(cid:16)∇g(u(k)

0 = u(k−1)
u(k)
u(k)
i = (P g
Ci
i = ∇g(u(k)
z(k)

 
◦ ∇g∗)
i−1) + z(k−1)

i

i−1) + z(k−1)
− ∇g(u(k)

) 

i

i

(cid:17)

 

 for i = 1  . . .   d 

(19)

where P g
C(x) = argminc∈C Dg(c  x) denotes the Bregman (rather than Euclidean) projection of x
onto a set C  and g∗ is the conjugate function of g. Though it may not be immediately obvious  when
2(cid:107)v(cid:107)2
2 the above iterations (19) reduce to the standard (Euclidean) Dykstra iterations in (4).
g(v) = 1
Furthermore  Dykstra’s algorithm and coordinate descent are equivalent in the more general setting.
Theorem 6. Let f be a strictly convex  differentiable function that has full domain. Assume that
Xi ∈ Rn×pi has full column rank and hi(v) = maxd∈Di(cid:104)d  v(cid:105) for a closed  convex set Di ⊆ Rpi  for
i = 1  . . .   d. Also  let g(v) = f∗(−v)  b = −∇f (0)  and Ci = (X T
i )−1(Di) ⊆ Rn  i = 1  . . .   d.

8

05001000150020001e−081e−051e−021e+011e+04No parallelizationActual iteration numberSuboptimalityCoordinate descentPar−Dykstra−CDPar−ADMM−CD  rho=10Par−ADMM−CD  rho=50Par−ADMM−CD  rho=2000501001501e−081e−051e−021e+011e+0410% parallelizationEffective iteration numberSuboptimalityCoordinate descentPar−Dykstra−CDPar−ADMM−CD  rho=10Par−ADMM−CD  rho=50Par−ADMM−CD  rho=200Then (16)  (18) are dual to each other  and their solutions ˆw  ˆu satisfy ˆu = −∇f (X ˆw). Moreover 
Dykstra’s algorithm (19) and coordinate descent (17) are equivalent  i.e.  for k = 1  2  3  . . .:

z(k)
i = Xiw(k)

i

and u(k)

i = −∇f

Xjw(k)

j +

Xjw(k−1)

j

 

for i = 1  . . .   d.

(cid:19)

(cid:18)(cid:88)

j≤i

(cid:88)

j>i

Nonquadratic loss: parallel coordinate descent methods. For a general regularized estimation
problem (16)  parallel coordinate descent methods can be derived by applying Dykstra’s algorithm
and ADMM to a product space reformulation of the dual. Interestingly  the subsequent coordinate
descent algorithms are no longer equivalent (for a unity augmented Lagrangian parameter)  and they
feature quite different computational structures. Parallel-Dykstra-CD for (16) initializes w(0) = 0 
and repeats:

Xw(k) − Xiw(k)

i /γi + Xiwi/γi

+ hi(wi/γi) 

i = 1  . . .   d 

(20)

w(k)

i = argmin
wi∈Rpi

f

for k = 1  2  3  . . .  and weights γ1  . . .   γd > 0 that sum to 1. In comparison  parallel-ADMM-CD
for (16) begins with u(0)

0 = 0  w(−1) = w(0) = 0  and repeats:
0 − u(k−1)
(u(k)

0 = −∇f

ρi

0

(cid:32)(cid:18) d(cid:88)

(cid:19)

i=1

0 + Xiw(k−1)

i

/ρi − Xiwi/ρi

Find u(k)

0

such that: u(k)

w(k)

i = argmin
wi∈Rpi

1
2

(cid:16)

(cid:13)(cid:13)(cid:13)u(k)

(cid:17)

(cid:13)(cid:13)(cid:13)2

2

(cid:33)

 

(21)

) − X(w(k−2) − 2w(k−1))

+ hi(wi/ρi) 

i = 1  . . .   d 

for k = 1  2  3  . . .  and parameters ρ1  . . .   ρd > 0. Derivation details are given in the supplement.
Notice the stark contrast between the parallel-Dykstra-CD iterations (20) and the parallel-ADMM-
CD iterations (21). In (20)  we perform (in parallel) coordinatewise hi-regularized minimizations
involving f  for i = 1  . . .   d. In (21)  we perform a single quadratically-regularized minimization
involving f for the u0-update  and then for the w-update  we perform (in parallel) coordinatewise
hi-regularized minimizations involving a quadratic loss  for i = 1  . . .   d (these are typically much
cheaper than the analogous minimizations for typical nonquadratic losses f of interest).
We note that the u0-update in the parallel-ADMM-CD iterations (21) simpliﬁes for many losses f
i=1 fi(vi)  for convex 
univariate functions fi  i = 1  . . .   n  the u0-update separates into n univariate minimizations. As an
example  consider the logistic lasso problem 
−yT Xw +

of interest; in particular  for separable loss functions of the form f (v) =(cid:80)n

i=1 ρi  and denoting by
σ(x) = 1/(1 + e−x) the sigmoid function  and by St(x) = sign(x)(|x| − t)+ the soft-thresholding
function at a level t > 0  the parallel-ADMM-CD iterations (21) for (22) reduce to:

where xi ∈ Rp  i = 1  . . .   n denote the rows of X. Abbreviating ρ =(cid:80)p
(cid:17)
i (w(k−2) − 2w(k−1)) +
i (w(k−2) − 2w(k−1))
 

i w) + λ(cid:107)w(cid:107)1 

(ρ − 1)u(k)
ρu(k)

0i = ρu(k−1)
0i − ρu(k−1)

0i such that:

+ xT
− xT

log(1 + exT

n(cid:88)

Find u(k)

min
w∈Rp

(cid:16)

(22)

i=1

σ

0i

0i

i = 1  . . .   n 

(23)

w(k)

i = Sλρi/(cid:107)Xi(cid:107)2

2

i (u(k)

0 + Xiw(k−1)

i

/ρi)

(cid:107)Xi(cid:107)2

2

 

i = 1  . . .   p 

(cid:18) ρiX T

(cid:19)

for k = 1  2  3  . . .. Now we see that both the u0-update and w-update in (23) can be parallelized 
and each coordinate update in the former can be done with  say  a simple bisection search.

Asynchronous parallel algorithms  and coordinate descent in Hilbert spaces. We ﬁnish with
some directions for possible future work. Asynchronous variants of parallel coordinate descent are
currently of great interest  e.g.  see the review in Wright (2015). Given the link between ADMM and
coordinate descent developed in this paper  it would be interesting to investigate the implications of
the recent exciting progress on asynchronous ADMM  e.g.  see Chang et al. (2016a b) and references
therein  for coordinate descent. In a separate direction  much of the literature on Dykstra’s algorithm
emphasizes that this method works seamlessly in Hilbert spaces. It would be interesting to consider
the connections to (parallel) coordinate descent in inﬁnite-dimensional function spaces  which we
would encounter  e.g.  in alternating conditional expectation algorithms or backﬁtting algorithms in
additive models.

9

References
Alfred Auslender. Optimisation: Methodes Numeriques. Masson  1976.

Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in

Hilbert Spaces. Springer  2011.

Heinz H. Bauschke and Valentin R. Koch. Projection methods: Swiss army knives for solving

feasibility and best approximation problems with halfspaces. arXiv: 1301.4506  2013.

Heinz H. Bauschke and Adrian S. Lewis. Dykstra’s algorithm with Bregman projections: a conver-

gence proof. Optimization  48:409–427  2000.

Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type methods.

SIAM Journal on Optimization  23(4):2037–2060  2013.

Dimitri P. Bertsekas and John N. Tsitsiklis. Parallel and Distributed Computation: Numerical

Methods. Prentice Hall  1989.

Steve Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternative direction method of multipliers. Foundations and Trends
in Machine Learning  3(1):1–122  2011.

James P. Boyle and Richard L. Dykstra. A method for ﬁnding projections onto the intersection of
convex sets in hilbert spaces. Advances in Order Restricted Statistical Inference: Proceedings of
the Symposium on Order Restricted Statistical Inference  pages 28–47  1986.

Yair Censor and Simeon Reich. The Dykstra algorithm with Bregman projections. Communications

in Applied Analysis  48:407–419  1998.

Tsung-Hui Chang  Mingyi Hong  Wei-Cheng Liao  and Xiangfeng Wang. Asynchronous distributed
ADMM for large-scale optimization—part i: Algorithm and convergence analysis. IEEE Transac-
tions on Signal Processing  64(12):3118–3130  2016a.

Tsung-Hui Chang  Wei-Cheng Liao  Mingyi Hong  and Xiangfeng Wang. Asynchronous distributed
ADMM for large-scale optimization—part ii: Linear convergence analysis and numerical perfor-
mance. IEEE Transactions on Signal Processing  64(12):3131–3144  2016b.

Scott Chen  David L. Donoho  and Michael Saunders. Atomic decomposition for basis pursuit. SIAM

Journal on Scientiﬁc Computing  20(1):33–61  1998.

Frank Deutsch. Best Approximation in Inner Product Spaces. Springer  2001.

Frank Deutsch and Hein Hundal. The rate of convergence of Dykstra’s cyclic projections algorithm:
The polyhedral case. Numerical Functional Analysis and Optimization  15(5–6):537–565  1994.

Jim Douglas and H. H. Rachford. On the numerical solution of heat conduction problems in two and

three space variables. Transactions of the American Mathematical Society  82:421–439  1956.

Richard L. Dykstra. An algorithm for restricted least squares regression. Journal of the American

Statistical Association  78(384):837–842  1983.

Jonathan Eckstein and Dimitri P. Bertsekas. On the Douglas-Rachford splitting method and the
proximal point algorithm for maximal monotone operators. Mathematical Programming  55(1):
293–318  1992.

Laurent El Ghaoui  Vivian Viallon  and Tarek Rabbani. Safe feature elimination in sparse supervised

learning. Paciﬁc Journal of Optimization  8(4):667–698  2012.

Jerome Friedman  Trevor Hastie  Holger Hoeﬂing  and Robert Tibshirani. Pathwise coordinate

optimization. Annals of Applied Statistics  1(2):302–332  2007.

Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Regularization paths for generalized linear

models via coordinate descent. Journal of Statistical Software  33(1):1–22  2010.

10

Wenjiang J. Fu. Penalized regressions: The bridge versus the lasso. Journal of Computational and

Graphical Statistics  7(3):397–416  1998.

Daniel Gabay. Applications of the method of multipliers to variational inequalities. Studies in

Mathematics and Its Applications  15:299–331  1983.

Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via ﬁnite element approximation. Computers & Mathematics with Applications  2(1):
17–40  1976.

Norbert Gaffke and Rudolf Mathar. A cyclic projection algorithm via duality. Metrika  36(1):29–54 

1989.

Roland Glowinski and A. Marroco. Sur l’approximation  par elements ﬁnis d’ordre un  et la resolution 
par penalisation-dualite d’une classe de problemes de Dirichlet non lineaires. Modelisation
Mathematique et Analyse Numerique  9(R2):41–76  1975.

Tom Goldstein  Brendan O’Donoghue  Simon Setzer  and Richard Baraniuk. Fast alternating direction

optimization methods. SIAM Journal on Imaging Sciences  7(3):1588–1623  2014.

Israel Halperin. The product of projection operators. Acta Scientiarum Mathematicarum  23:96–99 

1962.

Shih-Ping Han. A successive projection algorithm. Mathematical Programming  40(1):1–14  1988.

Clifford Hildreth. A quadratic programming procedure. Naval Research Logistics Quarterly  4(1):

79–85  1957.

Mingyi Hong and Zhi-Quan Luo. On the linear convergence of the alternating direction method of

multipliers. Mathematical Programming  162(1):165–199  2017.

Alfredo N. Iusem and Alvaro R. De Pierro. On the convergence properties of Hildreth’s quadratic

programming algorithm. Mathematical Programming  47(1):37–51  1990.

Alfredo N. Iusem and Alvaro R. De Pierro. A simultaneous iterative method for computing projections

on polyhedra. SIAM Journal on Control and Optimization  25(1):231–243  1987.

Martin Jaggi  Virginia Smith  Martin Takac  Jonathan Terhorst  Sanjay Krishnan  Thomas Hofmann 
and Michael I. Jordan. Communication-efﬁcient distributed dual coordinate ascent. Advances in
Neural Information Processing  27:3068–3076  2014.

Mojtaba Kadkhodaie  Konstantina Christakopoulou  Maziar Sanjabi  and Arindam Banerjee. Ac-
celerated alternating direction method of multipliers. International Conference on Knowledge
Discovery and Data Mining  21:497–506  2015.

Xingguo Li  Tuo Zhao  Raman Arora  Han Liu  and Mingyi Hong. An improved convergence analysis
of cyclic block coordinate descent-type methods for strongly convex minimization. International
Conference on Artiﬁcial Intelligence and Statistics  19:491–499  2016.

P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal

on Numerical Analysis  16(6):964–979  1979.

David Luenberger. Introduction to Linear and Nonlinear Programming. Addison-Wesley  1973.

Zhi-Quan Luo and Paul Tseng. On the convergence of the coordinate descent method for convex
differentiable minimization. Journal of Optimization Theory and Applications  72(1):7–35  1992.

Zhi-Quan Luo and Paul Tseng. On the convergence rate of dual ascent methods for linearly constrained

convex minimization. Mathematics of Operations Research  18(4):846–867  1993.

Robert Nishihara  Laurent Lessard  Benjamin Recht  Andrew Packard  and Michael I. Jordan. A
general analysis of the convergence of ADMM. International Conference on Machine Learning 
32:343–352  2015.

11

James M. Ortega and Werner C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several

Variables. Academic Press  1970.

G. Pierra. Decomposition through formalization in a product space. Mathematical Programming  28

(1):96–115  1984.

Peter Richtarik and Martin Takac. Parallel coordinate descent methods for big data optimization.

Mathematical Programming  156(1):433–484  2016.

Sylvain Sardy  Andrew G. Bruce  and Paul Tseng. Block coordinate relaxation methods for non-
parametric wavelet denoising. Journal of Computational and Graphical Statistics  9(2):361–379 
2000.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B  58(1):267–288  1996.

Robert Tibshirani  Jacob Bien  Jerome Friedman  Trevor Hastie  Noah Simon  Jonathan Taylor  and
Ryan J. Tibshirani. Strong rules for discarding predictors in lasso-type problems. Journal of the
Royal Statistical Society: Series B  74(2):245–266  2012.

Ryan J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics  7:1456–1490 

2013.

Paul Tseng. Dual ascent methods for problems with strictly convex costs and linear constraints: A

uniﬁed approach. SIAM Journal on Control and Optimization  28(1):214–29  1990.

Paul Tseng. Dual coordinate ascent methods for non-strictly convex minimization. Mathematical

Programming  59(1):231–247  1993.

Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.

Journal of Optimization Theory and Applications  109(3):475–494  2001.

Paul Tseng and Dimitri P. Bertsekas. Relaxation methods for problems with strictly convex separable

costs and linear constraints. Mathematical Programming  38(3):303–321  1987.

John von Neumann. Functional Operators  Volume II: The Geometry of Orthogonal Spaces. Princeton

University Press  1950.

Jie Wang  Peter Wonka  and Jieping Ye. Lasso screening rules via dual polytope projection. Journal

of Machine Learning Research  16:1063–1101  2015.

Jack Warga. Minimizing certain convex functions. Journal of the Society for Industrial and Applied

Mathematics  11(3):588–593  1963.

Stephen J. Wright. Coordinate descent algorithms. Mathematical Programming  151(1):3–34  2015.

Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression.

The Annals of Applied Statistics  2(1):224–244  2008.

12

,Ryan Tibshirani