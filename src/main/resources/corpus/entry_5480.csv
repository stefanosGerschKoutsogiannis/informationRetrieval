2010,The Maximal Causes of Natural Scenes are Edge Filters,We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis  the model assumes a sparse prior with independent hidden variables. However  in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore  we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively  the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics  the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.,The Maximal Causes of Natural Scenes are Edge Filters

Gervasio Puertas∗

J¨org Bornschein∗

Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt  Germany

Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt  Germany

puertas@fias.uni-frankfurt.de

bornschein@fias.uni-frankfurt.de

J¨org L¨ucke

Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt  Germany
luecke@fias.uni-frankfurt.de

Abstract

We study the application of a strongly non-linear generative model to image
patches. As in standard approaches such as Sparse Coding or Independent Com-
ponent Analysis  the model assumes a sparse prior with independent hidden vari-
ables. However  in the place where standard approaches use the sum to combine
basis functions we use the maximum. To derive tractable approximations for pa-
rameter estimation we apply a novel approach based on variational Expectation
Maximization. The derived learning algorithm can be applied to large-scale prob-
lems with hundreds of observed and hidden variables. Furthermore  we can infer
all model parameters including observation noise and the degree of sparseness.
In applications to image patches we ﬁnd that Gabor-like basis functions are ob-
tained. Gabor-like functions are thus not a feature exclusive to approaches assum-
ing linear superposition. Quantitatively  the inferred basis functions show a large
diversity of shapes with many strongly elongated and many circular symmetric
functions. The distribution of basis function shapes reﬂects properties of simple
cell receptive ﬁelds that are not reproduced by standard linear approaches. In the
study of natural image statistics  the implications of using different superposition
assumptions have so far not been investigated systematically because models with
strong non-linearities have been found analytically and computationally challeng-
ing. The presented algorithm represents the ﬁrst large-scale application of such an
approach.

1

Introduction

If Sparse Coding (SC  [1]) or Independent Component Analysis (ICA; [2  3]) are applied to image
patches  basis functions are inferred that closely resemble Gabor wavelet functions. Because of the
similarity of these functions to simple-cell receptive ﬁelds in primary visual cortex  SC and ICA
became the standard models to explain simple-cell responses  and they are the primary choice in
modelling the local statistics of natural images. Since they were ﬁrst introduced  many different
versions of SC and ICA have been investigated. While many studies focused on different ways to
efﬁciently infer the model parameters (e.g. [4  5  6])  many others investigated the assumptions used
in the underlying generative model itself. The modelling of observation noise can thus be regarded
as the major difference between SC and ICA (see  e.g.  [7]). Furthermore  different forms of inde-
pendent sparse priors have been investigated by many modelers [8  9  10]  while other approaches
have gone a step further and studied a relaxation of the assumption of independence between hidden
variables [11  12  13].

∗authors contributed equally

1

An assumption that has  in the context of image statistics  been investigated relatively little is the
assumption of linear superpositions of basis functions. This assumption is not only a hallmark of
SC and ICA but  indeed  is an essential part of many standard algorithms including Principal Com-
ponent Analysis (PCA)  Factor Analysis (FA; [14])  or Non-negative Matrix Factorization (NMF;
[15]). For many types of data  linear superposition can be motivated by the actual combination rule
of the data components (e.g.  sound waveforms combine linearly). For other types of data  including
visual data  linear superposition can represent a severe approximation  however. Models assuming
linearity are  nevertheless  often used because they are easier to study analytically and many de-
rived algorithms can be applied to large-scale problems. Furthermore  they perform well in many
applications and may  to certain extents  succeed well in modelling the distribution  e.g.  of local
image structure. From the perspective of probabilistic generative models  a major aim is  however 
to recover the actual data generating process  i.e.  to recover the actual generating causes (see  e.g. 
[7]). To accomplish this  the crucial properties of the data generation should be modelled as re-
alistically as possible. If the data components combine non-linearly  this should thus be reﬂected
by the generative model. Unfortunately  inferring the parameters in probabilistic models assuming
non-linear superpositions has been found to be much more challenging than in the linear case (e.g.
[16  17  18  19]  also compare [20  21]). To model image patches  for instance  large-scale applica-
tions of non-linear models  with the required large numbers of observed and hidden variables  have
so far not been reported.
In this paper we study the application of a probabilistic generative model with strongly non-linear
superposition to natural image patches. The basic model has ﬁrst been suggested in [19] where
tractable learning algorithms for parameter optimization where inferred for the case of a superpo-
sition based on a point-wise maximum. The model (which was termed Maximal Causes Analysis;
MCA) used a sparse prior for independent and binary hidden variables. The derived algorithms
compared favorably with state-of-the-art approaches on standard non-linear benchmarks and they
were applied to realistic data. However  the still demanding computational costs limited the appli-
cation domain to relatively small-scale problems. The unconstrained model for instance was used
with at most H = 20 hidden units. Here we use a novel learning algorithm to infer the parameters
of a variant of the MCA generative model. The approach allows for scaling the model up to several
hundreds of observed and hidden variables. It enables large-scale applications to image patches and 
thus  allows for studying the inferred basis functions as it is commonly done for linear approaches.

2 The Maximal Causes Generative Model

Consider a set of N data points {�y (n)}n=1 ... N sampled independently from an underlying distri-
bution (�y (n) ∈ D×1  D is the number of observed variables). For these data we seek parameters
n=1 p(�y (n) | Θ) under a variant of the MCA

Θ = (W  σ  π) that maximize the data likelihood L =�N

generative model [19] which is given by:

πsh (1 − π)1−sh  

(Bernoulli distribution)

N (yd; W d(�s  W )  σ2)   where W d(�s  W ) = max

h

{sh Wdh}

and where N (yd; w  σ2) denotes a scalar Gaussian distribution. H denotes the number of hidden
variables sh  and W ∈ RD×H. The model differs from the one previously introduced by the use
of Gaussian noise instead of Poisson noise in [19]. Eqn. 2 results in the basis functions �Wh =
(W1h  . . .   WDh)T of the MCA model to be combined non-linearly by a point-wise maximum. This
becomes salient if we compare (2) with the linear case using the vectorial notation maxh{ �W �
h} =

p(�s | Θ) = �h
p(�y | �s  Θ) = �d

(1)

(2)

� maxh{W �

h ∈ RD×1:

1h}  . . .   maxh{W �

p(�y | �s  Θ) = N (�y; max

Dh}�T for vectors �W �
p(�y | �s  Θ) = N (�y; �h sh �Wh  σ2 1)

{sh �Wh}  σ2 1)

h

where N (�y; �µ  Σ) denotes the multi-variate Gaussian distribution (note that �h sh �Wh = W �s ).

As in linear approaches such as SC  the combined basis functions set the mean values of the ob-
served variables yd  which are independently and identically drawn from Gaussian distributions

(non-linear superposition)

(linear superposition)

(3)

(4)

2

A

B

sum

C

max

sum

max

D

sum

max

Figure 1: A Example patches extracted from an image and preprocessed using a Difference of
Gaussians ﬁlter. B Two generated patches constructed from two Gabor basis functions with approx-
imately orthogonal wave vectors. In the upper-right the basis functions were combined using linear
superposition. In the lower-right they were combined using a point-wise maximum (note that the
max was taken after channel-splitting (see Eqn. 15 and Fig. 2). C Superposition of two collinear Ga-
bor functions using the sum (upper-right) or point-wise maximum (lower-right). D Cross-sections
through basis functions (along maximum amplitude direction). Left: Cross-sections through two
different collinear Gabor functions (compare C). Right: Cross-sections through their superpositions
using sum (top) and max (bottom).

with variance σ2 (Eqn. 2). The difference between linear and non-linear superposition is illustrated
in Fig. 1. In general  the maximum superposition results in much weaker interferences. This is the
case for diagonally overlapping basis functions (Fig. 1B) and  at closer inspection  also for overlap-
ping collinear basis functions (Fig. 1C D). Strong interferences as with linear combinations can not
be expected from combinations of image components. For preprocessed image patches (compare
Fig. 4D)  it could thus be argued that the maximum combination is closer to the actual combination
rule of image causes. In any case  the maximum represents an alternative to study the implications
of combination rules in the image domain.
To optimize the parameters Θ of the MCA model (1) and (2)  we use a variational EM approach (see 
e.g.  [22]). That is  instead of maximizing the likelihood directly  we maximize the free-energy:

F (q  Θ)=

N

�n=1���s

q(n)(�s ; Θ�) �log�p(�y (n) | �s  W  σ)� + log�p(�s | π)��� + H(q)  

(5)

where q(n)(�s ; Θ�) is an approximation to the exact posterior. In the variational EM scheme F(q  Θ)
is maximized alternately with respect to q in the E-step (while Θ is kept ﬁxed) and with respect to Θ
in the M-step (while q is kept ﬁxed). As a multiple-cause model  an exact E-step is computationally
intractable for MCA. Additionally  the M-step is analytically intractable because of the non-linearity
in MCA. The computational intractability in the E-step takes the form of expectation values of
functions g  �g(�s)�q(n). These expectations are intractable if the optimal choice of q(n) in (5) is used
(i.e.  if q(n) is equal to the posterior: q(n)(�s ; Θ�) = p(�s | �y (n)  Θ�)). To derive an efﬁcient learning
algorithm  our approach approximates the intractable expectations �g(�s)�q(n) by truncating the sums
over the hidden space of �s:

�g(�s)�q(n) = ��s
�∼

�s

≈ ��s∈K n
�∼

�s ∈K n

p(�s  �y (n) | Θ�) g(�s)

p(�s  �y (n) | Θ�) g(�s)

p(

∼

�s   �y (n) | Θ�)

p(

∼

�s   �y (n) | Θ�)

 

(6)

where K n is a small subset of the hidden space. Eqn. 6 represents a good approximation if the
set K n contains most of the posterior probability mass. The approximation will be referred to
as Expectation Truncation and can be derived as a variational EM approach (see Suppl. A). For
other generative models similar truncation approaches have successfully been used [19  23]. For
the learning algorithm  K n in (6) is chosen to contain hidden states �s with at most γ active causes

�h sh ≤ γ. Furthermore  we only consider the combinatorics of H � ≥ γ hidden variables. More

formally we deﬁne:

(7)
where the index set I contains those H � hidden variables that are the most likely to have generated
data point �y (n) (the last term in Eqn. 7 assures that all states �s with just one non-zero entry are also

K n = {�s |��j sj ≤ γ and ∀i �∈ I : si = 0� or �j sj ≤ 1} 

3

evaluated). To determine the H � hidden variables for I we use those variables h with the H � largest
values of a selection function Sh(�y (n)) which is given by:

Sh(�y (n)) = π N (�y (n); �W eﬀ

dh = max{yd  Wdh} . (8)
Selecting hidden variables based on Sh(�y (n)) is equivalent to selecting them based on an upper
bound of p(sh=1 | �y (n)  Θ). To see this note that p(�y (n) | Θ) is independent of h and that:

h   σ2 1)   with an effective weight W eﬀ

p(sh=1 | �y (n)  Θ)

p(�y (n) | Θ)

p(y(n)

d

=��s
sh = 1��d

| W d(�s  W )  σ)�p(�s | π) ≤��d

p(y(n)

d

| W eﬀ

dh   σ)���s

sh = 1

p(�s | π) 

with the right-hand-side being equal to Sh(�y (n)) in Eqn. 8 (see Suppl. B for details). A low value
of Sh(�y (n)) thus implies a low value of p(sh = 1 | �y (n)  Θ) and hence a low likelihood that cause h
has generated data point �y (n). In numerical experiments on ground-truth data we have veriﬁed that
for most data points Eqn. 6 with Eqn. 7 indeed ﬁnally approximates the true expectation values with
high accuracy.
Having derived tractable approximations for the expectation values (6) in the E-step  let us now
derive parameter update equations in the M-step. An update rule for the weight matrix W of this
model was derived in [19] and is given by:

W new

dh = �n∈M
�n∈M

�Aρ

dh(�s  W )�q(n) y(n)

d

�Aρ

dh(�s  W )�q(n)

 

Aρ

W

dh(�s  W ) = � ∂
d(�s  W ) = � H
�h=1

ρ

∂Wdh

W

ρ

d(�s  W )�  

1
ρ

(shWdh)ρ�

(9)

 

(10)

where the parameter ρ is set to a large value (we used ρ = 20). The derivation of the update rule
for σ (Gaussian noise has previously not been used) is straight-forward  and the update equation is
given by:

σnew = � 1

|M| D �n∈M������y (n) − max

h

{sh �Wh}����

2�qn

.

(11)

Note that in (9) to (11) we do not sum over all data points �y (n) but only over those in a subset M
(|M| is the number of elements in M). The subset contains the data points for which (6) ﬁnally
represents a good approximation. It is deﬁned to contain the N cut data points with largest values
p(�s  �y (n) | Θ�)  i.e.  with the largest values for the denominator in (6). N cut is hereby the
expected number of data points that have been generated by states with less or equal to γ non-zero
entries:

��s∈K n

The selection of data points is an important difference to earlier truncation approaches (compare
[19  23])  and its necessity can be shown analytically (Suppl. A).
Update equations (9)  (10)  and (11) have been derived by setting the derivatives of the free-energy
(w.r.t. W and σ) to zero. Similarly  we can derive the update equation for the sparseness parameter
π. However  as the approximation only considers states �s with a maximum of γ non-zero entries  the
update has to correct for an underestimation of π (compare Suppl. A). If such a correction is taken
into account  we obtain the update rule:

πnew =

A(π) π
B(π)

1

|M| �n∈M

�|�s |�qn with |�s | =

A(π) =

γ

�γ�=0� H

γ� �πγ� (1 − π)H−γ� and B(π) =

γ

�γ�=0

H

sh  

�h=1
γ�� H
γ� �πγ� (1 − π)H−γ� .

(13)

(14)

Note that the correction factor A(π) π
in (13) is equal to one over H if we allow for all possible states
B(π)
(i.e.  γ = H � = H). Also the set M becomes equal to the set of all data points in this case (because

4

N cut = N ��s  |�s|≤γ

p(�s | π) = N

γ

�γ �=0� H

γ � � πγ �

(1 − π)H−γ �

.

(12)

N cut = N). For γ = H � = H  Eqn. 13 thus falls back to the exact EM update rule that can canonically
be derived by setting the derivative of (5) w.r.t. π to zero (while using the exact posterior). Also the
update equations (9)  (10)  and (11) fall back to their canonical form for γ = H � = H. By choosing a
γ between one and H we can thus choose the accuracy of the used approximation. The higher the
value of γ the more accurate is the approximation but the larger are also the computational costs. For
intermediate values of γ we can obtain very good approximations with small computational costs.
Crucial for the scalability to large-scale problems is hereby the preselection of H � < H hidden
variables using the selection function in Eqn. 8.

Channel Splitting

Recombination

Learning Algorithm

+Channel

-Channel

non-neg. patches

basis functions

Figure 2: Illustration of patch preprocessing and basis function visualization. The left-hand-side
shows data points obtained from gray-value patches after DoG ﬁltering. These patches are trans-
formed to non-negative data by Eqn. 15. The algorithm maximizes the data likelihood under the
MCA model (1) and (2)  and infers basis functions (second from the right). For visualization  the
basis functions are displayed after their parts have been recombined again.

3 Numerical Experiments
The update equations (9)  (10)  (11)  and (13) together with approximation (6) with (7) and (8) deﬁne
a learning algorithm that optimizes the full set of parameters of the MCA generative model (1) and
(2). We will apply the algorithm to visual data as received by the primary visual cortex of mammals.
In mammals  visual information is transferred to the cortex via two types of neurons in the lateral
geniculus nucleus (LGN): center-on and center-off cells. The sensitivity of center-on neurons can be
modeled by a Difference of Gaussians (DoG) ﬁlter with positive central part  while the sensitivity of
center-off cells can be modelled by an inverted such ﬁlter. A model for preprocessing of an image
patch is thus given by a DoG ﬁlter and a successive splitting of the positive and the negative parts
of the ﬁltered image. More formally  we use a DoG ﬁlter to generate patches ˜�y with ˜D = 26 × 26
pixels. Such a patch is then converted to a patch of size D = 2 ˜D by assigning:

yd = [˜yd]+ and yD+d = [−˜yd]+

(15)

(for d = 1  . . .   D) where [x]+ = x for x ≥ 0 and [x]+ = 0 otherwise. This procedure has
repeatedly been used in the context of visual data processing (see  e.g.  [24]) and is  as discussed 
closely aligned with mammalian visual preprocessing (see Fig. 2 for an illustration).
Before we applied the algorithm to natural image patches  it was ﬁrst evaluated on artiﬁcial
data with ground-truth. As inferred basis functions of images most commonly resemble Gabor
wavelets  we used Gabor functions for the generation of artiﬁcial data. The Gabor basis functions
were combined according to the MCA generative model (1) and (2). We used H gen = 400 Gabor
functions for generation. The variances of the Gaussian envelop of each Gabor were sampled
from a distribution in nx/ny-space (Fig. 3C) with σx and σy denoting the standard deviations
of the Gaussian envelope  and with f denoting the Gabor frequency. Angular phases and cen-
ters of the Gabors were sampled from uniform distributions. The wave vector’s module was
set to 1 (f = 1
2π ) and the envelope amplitude was 10. The parameters were chosen to lie in the
same range as the parameters inferred in preliminary runs of the algorithm on natural image patches.
For the generation of each artiﬁcial patch we drew a binary vector �s according to (1) with πH gen = 2.
We then selected the |�s| corresponding Gabor functions and used channel-splitting (15) to convert
them into basis functions with only non-negative parts. To form an artiﬁcial patch  these basis func-
tions were combined using the point-wise maximum according to (2). We generated N = 150 000
patches as data points in this way (Fig. 3A shows some examples).
The algorithm was applied with H = 300 hidden variables and approximation parameters γ = 3 and
H � = 8. We generated the data with a larger number of basis functions to better match the continuous
distribution of the real generating components of images. The basis functions �Wh were initialized

5

A

B

C

n

y

Figure 3: A Artiﬁcial patches gen-
erated by combining artiﬁcial Gabors
using a point-wise maximum. B In-
ferred basis functions if the MCA
learning algorithm is applied. C Com-
parison between the shapes of gen-
erating (green) and inferred (blue)
Gabors. The brighter the blue data
points the larger the error between the
basis function and the matched Gabor
(also for Fig. 5).

n

x

by setting them to the average over all the preprocessed input patches plus a small Gaussian white
noise (≈ 0.5% of the corresponding mean). The initial noise parameter σ was set following Eqn. 11
by using all data points (setting |M| = N initially). Finally  the initial sparseness level was set to
π H = 2. The model parameters were updated according to Eqns. 9 to 13 using 60 EM iterations. To
help avoiding local optima  a small amount of Gaussian white noise (≈ 0.5% of the average basis
function value) was added during the ﬁrst 20 iterations  was linearly decreased to zero between
iterations 20 and 40  and kept at zero for the last 20 iterations. During the ﬁrst 20 iterations the
updates considered all N data points (|M| = N). Between iteration number 20 and 40 the amount
of used data points was linearly decreased to (|M| = N cut) where it was kept constant for the last
20 iterations. Considering all data points for the updates initially  has proven beneﬁcial because the
selection of data points is based on very incomplete knowledge during the ﬁrst iterations.
Fig. 3B displays some of the typical basis functions that were recovered in a run of the algorithm
on artiﬁcial patches. As can be observed (and as could have been expected)  they resemble Gabor
functions. When we matched the obtained basis functions with Gabor functions (compare  e.g. 
[25  26  27] for details)  the Gabor parameters obtained can be analyzed further. We thus plotted
the values parameterizing the Gabor shapes in an nx/ny-plot. This also allowed us to investigate
how well the generating distribution of artiﬁcial Gabors was recovered. Fig. 3C shows the gener-
ating (green) and the recovered distribution of Gabors (blue). Although some few recovered basis
functions lie a relatively distant from the generating distribution  it is in general recovered well. The
recovered sparseness level was with π H = 2.62 a bit larger than the initial level of π H gen = 2.
This is presumably due to the smaller number of basis function in the model H < H gen. Also
the ﬁnite inferred noise level of σ = 0.37 (despite a generation without noise) can be explained by
this mismatch. Depending on the parameters of the controls  we can observe different amounts of
outliers (usually not more than 5% − 10%). These outliers are usually basis functions that represent
more than one Gabor or small Gabor parts. Importantly  however  we found that the large majority
of inferred Gabors consistently recovered the generating Gabor functions in nx/ny-plots. In partic-
ular  when we changed the angle of the generating distribution in the nx/ny-plots (e.g.  to 25o or
65o)  the angle of the recovered distributions changed accordingly. Note that these controls are a
quantitative version of the artiﬁcial Gabor and grating data used for controls in [1].
Application to Image Patches. The dataset used in the experiment on natural images was prepared
by sampling N = 200 000 patches of ˜D = 26 × 26 pixels from the van Hateren image database
[28] (while constraining random selection to patches of images without man-made structures). We
preprocessed the patches as described above using a DoG ﬁlter1 with a ratio of 3 : 1 between positive
and negative parts (see  e.g.  [29]) before converting the patches using Eqn. 15.
The algorithm was applied with H = 400 hidden variables and approximation parameters γ = 4
and H � = 12. We used parameter initialization as described above and ran 120 EM iterations
(also as described above). After learning the inferred sparseness level was π H = 1.63 and the
inferred noise level was σ = 1.59. The inferred basis functions we found to resembled Gabor-like
functions at different locations  and with different orientations and frequencies. Additionally 
we obtained many globular basis functions with no or very little orientation preferences. Fig. 4
shows a selection of the H = 400 functions after a run of the algorithm (see suppl. Fig. C.1 for

1Filter parameters were chosen as in [27]; before the brightest 2% of the pixels were clamped to the maximal

value of the remaining 98% (inﬂuence of light-reﬂections were reduced in this way).

6

A

B

D

C

E

Figure 4: Numerical experiment on image patches. A Random selection of 125 basis functions of
the H=400 inferred. B Selection of most globular functions and C most elongated functions. D Se-
lection of preprocessed patches extracted from natural images. E Selection of data points generated
according to the model using the inferred basis functions and sparseness level (but no noise).

all functions). The patches in Fig. 4D E were chosen to demonstrate the high similarity between
preprocessed natural patches (in D) and generated ones (in E). To highlight the diversity of obtained
basis functions  Figs. 4B C display some of the most globular and elongated examples  respectively.
The variety of Gabor shapes is currently actively discussed [30  31  10  32  27] since it became
obvious that standard linear models (e.g.  SC and ICA)  could not explain this diversity [33]. To
facilitate comparison with earlier approaches  we have applied Gabor matching (compare [25])
and analyzed the obtained parameters. Instead of matching the basis functions directly  we ﬁrst
computed estimates of their corresponding receptive ﬁelds (RFs). These estimates were obtained by
convoluting the basis functions with the same DoG ﬁlter as used for preprocessing (see  e.g.  [27]
and Suppl. C.1 for details). In controls we found that these convoluted ﬁelds were closely matched
by RFs estimated using reverse correlation as described  e.g.  in [7].

A

135◦

90◦

B

45◦

0◦

n

y

180◦

Figure 5: Analysis of Gabor pa-
A Angle-
rameters (H=400).
frequency plot of basis functions.
B nx/ny distribution of basis
functions. C Distribution mea-
sured in vivo [33] (red triangles)
and corresponding distribution of
MCA basis functions (blue).

C

n

y

n

x

n

x

After matching the (convoluted) ﬁelds with Gabor functions  we found a relatively homogeneous
distribution of the ﬁelds’ orientations as it is commonly observed (Fig. 5A). The frequencies are
distributed around 0.1 cycles per pixel  which reﬂects the band-pass property of the DoG ﬁlter. To
analyze the Gabor shapes  we plotted the parameters using an nx/ny-plot (as suggested in [33]). The
broad distribution in nx/ny-space hereby reﬂects the high diversity of basis functions obtained by
our algorithm (see Fig. 5B). The speciﬁc form of the obtained shape distribution is  hereby  similar
to the distribution of macaque V1 simple cells as measure in in vivo recordings [33]. However  the
MCA basis functions do quantitatively not match the measurements exactly (see Fig. 5C): the MCA
distribution contains a higher percentage of strongly elongated basis functions  and many MCA
functions are shifted slightly to the right relative to the measurements. If the basis functions are
matched with Gabors directly  we actually do not observe the latter effect (see suppl. Fig. C.2). If
simple-cell responses are associated with the posterior probabilities of multiple-cause models  the
basis functions should  however  not be compared to measured RFs directly (although it is frequently
done in the literature).

7

To investigate the implications of different numbers of hidden variables  we also ran the algorithm
with H = 200 and H = 800. In both cases we observed qualitatively and quantitatively similar
distributions of basis functions. Runs with H = 200 thus also contained many circular symmetric
basis functions (see suppl. Fig. C.3 for the distribution of shapes). This observation is remarkable
because it shows that such ‘globular’ ﬁelds are a very stable feature for the MCA approach  also for
small numbers of hidden variables. Based on standard generative models with linear superposition
it has recently been argued [32] that such functions are only obtained in a regime with large numbers
of hidden variables relative to the input dimensionality (see [34] for an early contribution).

4 Discussion
We have studied the application of a strongly non-linear generative model to image patches. The
model combines basis functions using a point-wise maximum as an alternative to the linear com-
bination as assumed by Sparse Coding  ICA  and most other approaches. Our results suggest that
changing the component combination rule has a strong impact on the distribution of inferred ba-
sis functions. While we still obtain Gabor-like functions  we robustly observe a large variety of
basis functions. Most notably  we obtain circular symmetric functions as well as many elongated
functions that are closely associated with edges traversing the entire patch (compare Figs. 1 and 4).
Approaches using linear component combination  e.g. ICA or SC  do usually not show these fea-
tures. The differences in basis function shapes between non-linear and linear approaches are  in this
respect  consistent with the different types of interferences between basis functions. The maximum
results in basis function combinations with much less pronounced interferences  while the stronger
interferences of linear combinations might result in a repulsive effect fostering less elongated ﬁelds
(compare Fig. 1).
For linear approaches  a large diversity of Gabor shapes (including circular symmetric ﬁelds) could
only be obtained in very over-complete settings [34]  or speciﬁcally modelled priors with hand-set
sparseness levels [10]. Such studies were motivated by a recently observed discrepancy of recep-
tive ﬁelds as predicted by SC or ICA  and receptive ﬁelds as measured in vivo [33]. Compared to
these measurements  the MCA basis functions and their approximate receptive ﬁelds show a similar
diversity of shapes. MCA functions and measured RFs both show circular symmetric ﬁelds and
in both cases there is a tendency towards ﬁelds elongated orthogonal to the wave-vector direction
(compare Fig. 4). Possible factors that can inﬂuence the distributions of basis functions  for MCA as
well as for other methods  are hereby different types of preprocessing  different prior distributions 
and different noise models. Even if the prior type is ﬁxed  differences for the basis functions have
been reported for different settings of prior parameters (e.g.  [10]). If possible  these parameters
should thus be learned along with the basis functions. All the different factors named above may
result in quantitative differences  and the shift of the MCA functions relative to the measurements
might have been caused by one of these factors. For the MCA model  possible effects of assuming
binary hidden variables remain to be investigated. Presumably  also dependencies between hidden
variables as investigated in recent contributions [e.g. 13  12  11] play an important role  e.g.  if larger
structures of speciﬁc arrangements of edges and textures are considered. As the components in such
models are combined less randomly  the implications of their combination rule may even be more
pronounced in these cases.
In conclusion  probably neither the linear nor the maximum combination rule does represent
the exact model for local visual component combinations. However  while linear component
combinations have extensively been studied in the context of image statistics  the investigation of
other combination rules has been limited to relatively small scale applications [17  16  35  19].
Applying a novel training scheme  we could overcome this limitation in the case of the MCA
generative model. As with linear approaches  we found that Gabor-like basis functions are obtained.
The statistics of their shapes  a subject that is currently and actively discussed [31  10  32  26  27]  is
markedly different  however. Future work should  thus  at least be aware that a linear combination
of components is not the only possible choice. To recover the generating causes of image patches 
a linear combination might  furthermore  not be the best choice. With the results presented in this
work  it can neither be considered as the only practical one anymore.
Acknowledgements. We gratefully acknowledge funding by the German Federal Ministry of Education and
Research (BMBF) in the project 01GQ0840 (BFNT Frankfurt) and by the German Research Foundation
(DFG) in the project LU 1196/4-1. Furthermore  we gratefully acknowledge support by the Frankfurt Center
for Scientiﬁc Computing (CSC Frankfurt) and thank Marc Henniges for his help with Fig. 2.

8

References
[1] B. A. Olshausen  D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607 – 609  1996.

[2] P. Comon. Independent component analysis  a new concept? Signal Proc  36(3):287–314  1994.
[3] A. J. Bell  T. J. Sejnowski. The “independent components” of natural scenes are edge ﬁlters. Vision

Research  37(23):3327 – 38  1997.

[4] A. Hyv¨arinen  E. Oja. A fast ﬁxed-point algorithm for independent component analysis. Neural Compu-

tation  9(7):1483–1492  1997.

[5] H. Lee  A. Battle  R. Raina  A. Ng. Efﬁcient sparse coding algorithms. NIPS 22  801–808  2007.
[6] M. W. Seeger. Bayesian Inference and Optimal Design for the Sparse Linear Model. Journal of Machine

Learning Research  759–813  2008.

[7] P. Dayan  L. F. Abbott. Theoretical Neuroscience. MIT Press  Cambridge  2001.
[8] P. Berkes  R. Turner  M. Sahani. On sparsity and overcompleteness in image models. NIPS 20  2008.
[9] B. A. Olshausen  K. J. Millman. Learning sparse codes with a mixture-of-Gaussians prior. NIPS 12 

841–847  2000.

[10] M. Rehn  F. T. Sommer. A network that uses few active neurones to code visual input predicts the diverse

shapes of cortical receptive ﬁelds. J Comp Neurosci  22(2):135–146  2007.

[11] A. Hyv¨arinen  P. Hoyer. Emergence of phase-and shift-invariant features by decomposition of natural

images into independent feature subspaces. Neural Computation  12(7):1705–1720  2000.

[12] F. Sinz  E. P. Simoncelli  M. Bethge. Hierarchical modeling of local image features through Lp-nested

symmetric distributions. NIPS 22  1696–1704  2009.

[13] D. Zoran  Y. Weiss. The ”Tree-Dependent Components” of Natural Images are Edge Filters. NIPS 22 

2340–2348  2009.

[14] B. S. Everitt. An Introduction to Latent Variable Models. Chapman and Hall  1984.
[15] D. D. Lee  H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature 

401(6755):788–91  1999.

[16] P. Dayan  R. S. Zemel. Competition and multiple cause models. Neural Computation  7:565-579  1995.
[17] E. Saund. A multiple cause mixture model for unsupervised learning. Neural Computation  7:51-71  1995.
[18] H. Lappalainen  X. Giannakopoulos  A. Honkela  J. Karhunen. Nonlinear independent component analy-

sis using ensemble learning: Experiments and discussion. Proc. ICA  2000.

[19] J. L¨ucke  M. Sahani. Maximal causes for non-linear component extraction. Journal of Machine Learning

Research  9:1227 – 1267  2008.

[20] N. Jojic  B. Frey. Learning ﬂexible sprites in video layers. CVPR  199–206  2001.
[21] N. Le Roux  N. Heess  J. Shotton  J. Winn. Learning a generative model of images by factoring appearance

and shape. Technical Report  Microsoft Research  2010.

[22] R. Neal  G. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and other variants.

M. I. Jordan  editor  Learning in Graphical Models. Kluwer  1998.

[23] J. L¨ucke  R. Turner  M. Sahani  M. Henniges. Occlusive Components Analysis. NIPS  1069-1077  2009.
[24] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning

Research  5:1457–1469  2004.

[25] J. P. Jones  L. A. Palmer. An evaluation of the two-dimensional gabor ﬁlter model of simple receptive

ﬁelds in cat striate cortex. Journal of Neurophysiology  58(6):1233 – 1258  1987.

[26] P. Berkes  B.L. White  J. Fiser. No evidence for active sparsiﬁcation in the visual cortex. NIPS 22  2009.
[27] J. L¨ucke. Receptive ﬁeld self-organization in a model of the ﬁne-structure in V1 cortical columns. Neural

Computation  21(10):2805–2845  2009.

[28] J. H. van Hateren  A. van der Schaaf. Independent component ﬁlters of natural images compared with

simple cells in primary visual cortex. Proc Roy Soc London B  265:359 – 366  1998.

[29] D. C. Somers  S. B. Nelson  M. Sur. An emergent model of orientation selectivity in cat visual cortical

simple cells. The Journal of Neuroscience  15:5448 – 5465  1995.

[30] J. L¨ucke. Learning of representations in a canonical model of cortical columns. Cosyne 2006  100  2006.
[31] S. Osindero  M. Welling  G. E. Hinton. Topographic product models applied to natural scene statistics.

Neural Computation  18:381 – 414  2006.

[32] D. Arathorn  B. Olshausen  J. DiCarlo. Functional requirements of a visual theory. Workshop Cosyne.

www.cosyne.org/c/index.php?title=Functional requirements of a visual theory  2007.

[33] D. L. Ringach.

mary visual cortex.
manuelita.psych.ucla.edu/∼dario.

Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque pri-
Journal of Neurophysiology  88:455 – 463  2002. Data retrieved 2006 from

[34] B. A. Olshausen  D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1?

Vision Research  37(23):3311–3325  1997.

[35] S. Den´eve  T. Lochmann  U. Ernst. Spike based inference in a network with divisive inhibition. Neural-

Comp  Marseille  2008.

9

,Lionel Ott
Linsey Pang
Fabio Ramos
Sanjay Chawla
Mainak Jas
Tom Dupré la Tour
Umut Simsekli
Alexandre Gramfort
Sid Reddy
Anca Dragan
Sergey Levine
Shibani Santurkar
Andrew Ilyas
Dimitris Tsipras
Logan Engstrom
Brandon Tran
Aleksander Madry