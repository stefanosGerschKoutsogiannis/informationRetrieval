2014,Large-Margin Convex Polytope Machine,We present the Convex Polytope Machine (CPM)  a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets  and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition  text topic  and web security) demonstrate that the CPM trains models faster  sometimes several orders of magnitude  than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that  unlike prior similar approaches  we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.,Large-Margin Convex Polytope Machine

Alex Kantchelian Michael Carl Tschantz Ling Huang†

Peter L. Bartlett Anthony D. Joseph

J. D. Tygar

UC Berkeley – {akant|mct|bartlett|adj|tygar}@cs.berkeley.edu

†Datavisor – ling.huang@datavisor.com

Abstract

We present the Convex Polytope Machine (CPM)  a novel non-linear learning al-
gorithm for large-scale binary classiﬁcation tasks. The CPM ﬁnds a large margin
convex polytope separator which encloses one class. We develop a stochastic gra-
dient descent based algorithm that is amenable to massive data sets  and augment
it with a heuristic procedure to avoid sub-optimal local minima. Our experimental
evaluations of the CPM on large-scale data sets from distinct domains (MNIST
handwritten digit recognition  text topic  and web security) demonstrate that the
CPM trains models faster  sometimes by several orders of magnitude  than state-
of-the-art similar approaches and kernel-SVM methods while achieving compara-
ble or better classiﬁcation performance. Our empirical results suggest that  unlike
prior similar approaches  we do not need to control the number of sub-classiﬁers
(sides of the polytope) to avoid overﬁtting.

1

Introduction

Many application domains of machine learning use massive data sets in dense medium-dimensional
or sparse high-dimensional spaces. Some domains also require near real-time responses in both
the prediction and the model training phases. These applications often deal with inherent non-
stationarity; thus  the models need to be constantly updated in order to catch up with drift. Today 
the de facto algorithm for binary classiﬁcation tasks at these scales is linear SVM. Indeed  since
Shalev-Shwartz et al. demonstrated both theoretically and experimentally that large margin linear
classiﬁers can be efﬁciently trained at scale using stochastic gradient descent (SGD)  the Pegasos [1]
algorithm has become a standard building tool for the machine learning practitioner.
We propose a novel algorithm for Convex Polytope Machine (CPM) separation exhibiting superior
empirical performance to existing algorithms  with running times on a large data set that are up to
ﬁve orders of magnitude faster. We conjecture that worst case bounds are independent of the number
K of faces of the convex polytope and state a theorem of loose upper bounds in terms of
In theory  as the VC dimension of d-dimensional linear separators is d + 1  a linear classiﬁer in
very high dimension d is expected to have a considerable expressiveness power. This argument is
often understood as “everything is separable in high dimensional spaces; hence  linear separation is
good enough”. However  in practice  deployed systems rarely use a single naked linear separator.
One explanation for this gap between theory and practice is that while the probability of a single
hyperplane perfectly separating both classes in very high dimensions is high  the resulting classiﬁer
margin might be very small. Since the classiﬁer margin also accounts for the generalization power 
we might experience poor future classiﬁcation performance in this scenario.
Figure 1a provides a two-dimensional example of a data set that has a small margin when using a
single separator (solid line) despite being linearly separable and intuitively easily classiﬁed. The
intuition that the data is easily classiﬁed comes from the data naturally separating into three clusters

√

K.

1

with two of them in the positive class. Such clusters can form due to the positive instances being
generated by a collection of different processes.

(a) Instances are perfectly linearly separable (dashed
line)  although with small margin due to positive in-
stances (A & B) having conﬂicting patterns. We can
obtain higher margin by separately training two linear
sub-classiﬁers (solid lines) on left and right clusters
of positive instances  each against all the negative in-
stances  yielding a prediction value of the maximum
of the sub-classiﬁers.
Figure 1: Positive (•) and negative (◦) instances in continuous two dimensional feature space.

(b) The worst-case margin is insensitive to wiggling
of sub-classiﬁers having non-minimal margin. Sub-
classiﬁer 2 has the smallest margin  and sub-classiﬁer
1 is allowed to freely move without affecting δWC.
For comparison 
is
shown (dashed lines).

the largest-margin solution 1(cid:48)

As Figure 1a shows  a way of increasing the margins is to introduce two linear separators (dashed
lines)  one for each positive cluster. We take advantage of this intuition to design a novel machine
learning algorithm that will provide larger margins than a single linear classiﬁer while still enjoying
much of the computational effectiveness of a simple linear separator. Our algorithm learns a bounded
number of linear classiﬁers simultaneously. The global classiﬁer will aggregate all the sub-classiﬁers
decisions by taking the maximum sub-classiﬁer score. The maximum aggregation has the effect of
assigning a positive point to a unique sub-classiﬁer. The model class we have intuitively described
above corresponds to convex polytope separators.
In Section 2  we present related work in convex polytope classiﬁers and in Section 3  we deﬁne the
CPM optimization problem and derive loose upper bounds. In Section 4  we discuss a Stochastic
Gradient Descent-based algorithm for the CPM and perform a comparative evaluation in Section 5.

2 Related Work

Fischer focuses on ﬁnding the polygon with the fewest misclassiﬁed points drawn independently
from an unknown distribution using an algorithm with a running time of more than O(n12) where
n is the number of sample points [2]. We instead focus on ﬁnding good  not optimal  polygons that
generalize well in practice despite having fast running times. Our focus on generalization leads us
to maximize the margin  unlike this work  which actually minimizes it to make their proofs easier.
Takacs proposes algorithms for training convex polytope classiﬁers based on the smooth approxi-
mation of the maximum function [3]. While his algorithms use smooth approximation during train-
ing  it uses the original formula during prediction  which introduces a gap that could deteriorate
the accuracy. The proposed algorithms achieve similar classiﬁcation accuracy to several nonlinear
classiﬁers  including KNN  decision tree and kernel SVM. However  the training time of the al-
gorithms is often much longer than those nonlinear classiﬁers (e.g.  an order of magnitude longer
than ID3 algorithm and eight times longer than kernel SVM on CHESS DATASET)  diminishing
the motivation to use the proposed algorithms in realistic setting. Zhang et al. propose an Adaptive
Multi-hyperplane Machine (AMM) algorithm that is fast during both training and prediction  and
capable of handling nonlinear classiﬁcation problems [4]. They develop an iterative algorithm based
on the SGD method to search for the number of hyperplanes and train the model. Their experiments
on several large data sets show that AMM is nearly as fast as the state-of-the-art linear SVM solver
and achieves classiﬁcation accuracy between linear and kernel SVMs.
Manwani and Sastry propose two methods for learning polytope classiﬁers  one based on the logistic
function [5]  and another based on the perceptron method [6]  and propose alternating optimization

2

+-AB+-+-211’algorithms to train the classiﬁers. However  they only evaluate the proposed methods on a few small
data sets (with no more than 1000 samples in each)  and do not compare them to other widely used
(nonlinear) classiﬁers (e.g.  KNN  decision tree  SVM). It is unclear how applicable these algorithms
are to large-scale data. Our work makes three signiﬁcant contributions over their work: (1) deriving
the formulation from a large-margin argument and obtaining a regularization term that is missing
in [6]; (2) safely restricting the choice of assignments to only positive instances  leading to a training
time optimization heuristic; and (3) demonstrating higher performance on non-synthetic  large scale
data sets when using two CPMs together.
The CPM is a special case of the more general Latent SVM [7] formulation. In particular  the latent
variable represents the sub-classiﬁer  or face of the polytope  used for classifying the given instance.

3 Large-Margin Convex Polytopes

In this section  we derive and discuss several alternative optimization problems for ﬁnding a large-
margin convex polytope which separates binary labeled points of Rd.

3.1 Problem Setup and Model Space
Let D = {(xi  yi)}1≤i≤n be a binary labeled data set of n instances  where x ∈ Rd and y ∈ {−1  1}.
For the sake of notational brevity  we assume that the xi include a constant unitary component
corresponding to a bias term. Our prediction problem is to ﬁnd a classiﬁer c : Rd → {−1  1}
such that c(xi) is a good estimator of yi. To do so  we consider classiﬁers constructed from convex
K-faced polytope separators for a ﬁxed positive integer K. Let PK be the model space of convex
K-faced polytope separators:

(cid:27)

(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12) f (x) = max

1≤k≤K

PK =

f : Rd → R

(Wx)k  W ∈ RK×d

For each such function f in PK  we can get a classiﬁer cf such that cf (x) is 1 if f (x) > 0 and
−1 otherwise. This model space corresponds to a shallow single hidden layer neural network with a
max aggregator. Note that when K = 1  P1 is simply the space of all linear classiﬁers. Importantly 
when K ≥ 2  elements of PK are not guaranteed to have additive inverses in PK. Thus  the labels
y = −1 and y = +1 are not interchangeable. Geometrically  the negative class remains enclosed
within the convex polytope while the positive class lives outside of it  leading to the label asymmetry.
To construct a classiﬁer without label asymmetry  we can use two polytopes  one with the negative
instances on the inside the polytope to get a classiﬁcation function f− and one with the positive
instances on the inside to get f+. From these two polytopes  we construct the classiﬁer cf− f+
where cf− f+(x) is 1 if f−(x) − f+(x) > 0 and −1 otherwise.
To better understand the nature of the faces of a single polytope  for a given polytope W and a data
point x  we denote by zW(x) the index of the maximum sub-classiﬁer for x:

zW (x) = argmax
1≤k≤K

(Wx)k

We call zW(x) the assigned sub-classiﬁer for instance x. When clear from context  we drop W
from zW. We use the notation Wk to designate the kth row of W  which corresponds to the kth
face of the polytope  or the kth sub-classiﬁer. Hence  Wz(x) identiﬁes the separator assigned to x.
We now pursue a geometric large-margin-based approach for formulating the concrete optimization
problem. To simplify notation and without loss of generality  we suppose that W is row-normalized
such that ||Wk|| = 1 for all k. We also initially suppose our data set is perfectly separable by a
K-faced convex polytope.

3.2 Margins for Convex Polytopes

When K = 1  the problem reduces to ﬁnding a good linear classiﬁer and only a single natural
margin δ of the separator exists [8]:

δW = min
1≤i≤n

yiW1xi

3

Maximizing δW yields the well known (linear) Support Vector Machine. However  multiple notions
of margin exist for a K-faced convex polytope with K ≥ 2. We consider two.
Let the worst case margin δWC
sub-classiﬁers  we ﬁnd the one with the minimal margin to the closest point assigned to it:

W be the smallest margin of any point to the polytope. Over all the K

δWC
W = min
1≤i≤n

yiWz(xi)xi = min
1≤k≤K

min

i:z(xi)=k

yiWkxi

The worst case margin is very similar to the linear classiﬁer margin but suffers from an important
drawback. Maximizing δWC leaves K − 1 sub-classiﬁers wiggling while over-focusing on the sub-
classiﬁer with the smallest margin. See Figure 1b for a geometrical intuition.
Thus  we instead focus on the total margin  which measures each sub-classiﬁer’s margin with respect
to just its assigned points. The total margin δT

W is the sum of the K sub-classiﬁers margins:

K(cid:88)

k=1

δT
W =

min

i:z(xi)=k

yiWkxi

The total margin gives the same importance to each of the K sub-classiﬁer margins.

3.3 Maximizing the Margin

We now turn to the question of maximizing the margin. Here  we provide an overview of a smoothed
but non-convex optimization problem for maximizing the total margin.
We would like to optimize the margin by solving the optimization problem
W subject to (cid:107)W1(cid:107) = ··· = (cid:107)WK(cid:107) = 1
δT
K(cid:88)

Introducing one additional variable ζk per classiﬁer  problem (1) is equivalent to

max
W

(1)

subject to ∀i  ζz(xi) ≤ yiWz(xi)xi

(2)

ζk

max
W ζ

k=1

ζ1 > 0  . . .   ζK > 0
(cid:107)W1(cid:107) = ··· = (cid:107)WK(cid:107) = 1

Considering the unnormalized rows Wk/ζk  we obtain the following equivalent formulation:

max
W

1

(cid:107)Wk(cid:107)

subject to ∀i  1 ≤ yiWz(xi)xi

(3)

K(cid:88)

k=1

K(cid:88)

When y = −1 and z(xi) satisfy the margin constraint in (3)  we have that the constraint holds for
every sub-classiﬁer k since yiWkxi is minimal at k = z(xi). Thus  when y = −1  we can enforce
the constraint for all k. We can also smooth the objective into a convex  deﬁned-everywhere one by
minimizing the sum of the inverse squares of the terms instead of maximizing the sum of the terms.
We obtain the following smoothed problem:

k=1

min
W

(cid:107)Wk(cid:107)2

subject to ∀i : yi = −1  ∀k ∈ {1  . . .   K}  1 + Wkxi ≤ 0

∀i : yi = +1  1 − Wz(xi)xi ≤ 0

(4)
(5)
The objective of the above program is now the familiar L2 regularization term (cid:107)W(cid:107)2. The con-
straints (4)  on the negative instances  are convex (linear functions)  but the positive terms (5) result
in non-convex constraints because of the instance-dependent assignment z. As for the Support Vec-
tor Machine  we can introduce n slack variables ξi and a regularization factor C > 0 for the common
case of noisy  non-separable data. Hence  the practical problem becomes

(cid:107)W(cid:107)2 + C

min
W ξ

ξi

subject to ∀i : yi = −1  ∀k ∈ {1  . . .   K}  1 + Wkxi ≤ ξi ≥ 0 (6)

∀i : yi = +1  1 − Wz(xi)xi ≤ ξi ≥ 0

n(cid:88)

i=1

Following the same steps  we obtain the following problem for maximizing the worst-case margin.
The only difference is the regularization term in the objective function which becomes maxk (cid:107)Wk(cid:107)2
instead of (cid:107)W(cid:107)2.

4

(cid:26)

1≤k≤K

(cid:12)(cid:12)(cid:12)(cid:12) f (x) = max
n(cid:88)

(cid:27)

Discussion. The goal of our relaxation is to demonstrate that our solution involves two intuitive
steps: (i) assigning positive instances to sub-classiﬁers  and (ii) solving a collection of SVM-like
sub-problems. While our solution taken as a whole remains non-convex  this decomposition isolates
the non-convexity to a single intuitive assignment problem that is similar to clustering. This isola-
tion enables us to use intuitive heuristics or clustering-like algorithms to handle the non-convexity.
Indeed  in our ﬁnal form (6)  if the optimal assignment function z(xi) of positive instances to sub-
classiﬁers were known and ﬁxed  the problem would be reduced to a collection of perfectly inde-
pendent convex minimization problems. Each such sub-problem corresponds to a classical SVM
deﬁned on all negative instances and the subset of positive instances assigned by z(xi).

3.4 Choice of K  Generalization Bound for CPM

Assuming we can efﬁciently solve this optimization problem  we would need to adjust the number
K of faces and the degree C of regulation. The following result gives a preliminary generalization
bound for the CPM. For B1  . . .   Bk ≥ 0  let FK B be the following subset of the set PK of convex
polytope separators:

FK B =

f : Rd → R

(Wx)k  W ∈ RK×d ∀k (cid:107)Wk(cid:107) ≤ Bk

Theorem 1. There exists some constant A > 0 such that for all distributions P over X × {−1  1} 
K in {1  2  3  . . .}  B1  . . .   Bk ≥ 0  and δ > 0  with probability at least 1 − δ over the training set
(x1  y1)  . . .   (xn  yn) ∼ P   any f in FK B is such that:

(cid:80)

(cid:114)

P (yf (x) ≤ 0) ≤ 1
n

i=1

max(0  1 − yif (xi)) + A

k Bk√
n

+

ln (2/δ)

2n

proportional to the sum of the sub-classiﬁer norms. Note that as we have(cid:80)

This is a uniform bound on the 0-1 risk of classiﬁers in FK B. It shows that with high probability 
the risk is bounded by the empirical hinge loss plus a capacity term that decreases in n−1/2 and is
K(cid:107)W(cid:107) 
K(cid:107)W(cid:107). As a comparison  the generalization error
the capacity term is essentially equivalent to
for AMM [4]  a related piecewise-linear classiﬁcation method has previously been shown to be
proportional to K(cid:107)W(cid:107) in [4  Thm. 2]. In practice  this bound is very loose as it does not explain the
observed absence of over ﬁtting as K gets large. We experimentally demonstrate this phenomenon
in Section 5. We conjecture that there exists a bound that is independent of K altogether. The proof
of Theorem 1 relies on a result due to Bartlett et al. on Rademacher complexities [9]. We ﬁrst prove
n). We then invoke Theorem 7 of [9]

that the Rademacher complexity of FK B is in O((cid:80)

k (cid:107)Wk(cid:107) ≤ √

√

√

k Bk/

to show our result. The appendix contains the full proof.

4 SGD-based Learning

In this section  we present a learning algorithm based on Stochastic Gradient Descent (SGD) for
approximately solving the total margin maximization problem (6). The choice of SGD is motivated
by two factors. First  we would like our learning technique to efﬁciently scale to several million
instances of sparse high dimensional space. The sample-iterative nature of SGD makes it a very
suitable candidate for this end [10]. Second  the optimization problem we are solving is non-convex.
SGD has recently been shown to work well for such learning problems when near-optimum solutions
are acceptable [11].
Problem (6) can be expressed as an unconstrained minimization problem as follows:
[1 − Wz(xi)xi]+ + λ(cid:107)W(cid:107)2

[1 + Wkxi]+ +

(cid:88)

(cid:88)

K(cid:88)

min
W

i:yi=−1

k=1

i:yi=+1

where [x]+ = max(0  x) and λ > 0. This form reveals the strong similarity with optimizing K
unconstrained linear SVMs [1]. The difference is that although each sub-classiﬁer is trained on
all the negative instances  positive instances are associated to a unique sub-classiﬁer. From the
unconstrained form  we can derive the stochastic gradient descent Algorithm 1. For the positive
instances  we isolate the task of ﬁnding the assigned sub-classiﬁer z to a separate procedure ASSIGN.
We use the Pegasos inverse schedule ηt = 1/(λt).

5

Wk ← Wk − ηtx

else if y = +1 then

z ← argmaxk Wkx
if Wzx < 1 then

z ← ASSIGN(W  x  h)
Wz ← Wz + ηtx

return W

for k ← 1  . . .   K do
if Wkx > −1 then

Algorithm 1 Stochastic gradient descent al-
gorithm for solving problem (6).

function SGDTRAIN(D  λ  T  (ηt)  h)

Initialize W ∈ RK×d  W ← 0
for t ← 1  . . .   T do
Pick (x  y) ∈ D
if y = −1 then

Since the optimization problem (6) is non-convex  a
pure SGD approach could get stuck in a low-quality
local optimum and we  indeed  found that this prob-
lem occurs in practice. These optima assign most
of the positive instances to a small number of sub-
classiﬁers. In this conﬁguration  the remaining sub-
classiﬁers serve no purpose.
Intuitively  the algo-
rithm clustered the data into large “super-clusters”
ignoring the more subtle sub-clusters comprising the
larger super-clusters. The large clusters represent
an appealing local optima since breaking one down
into sub-clusters often requires transitioning through
a patch of lower accuracy as the sub-classiﬁers align
themselves to the new cluster boundaries. We view
the local optima as the algorithm underﬁtting the
data by using too simple of a model. In this case 
the algorithm needs encouragement to explore more
complex clusterings.
With this intuition in mind  we add a term encour-
aging the algorithm to explore higher entropy con-
ﬁgurations of the sub-classiﬁers. To do so  we use
the entropy of the random variable Z = argmaxk Wkx where x ∼ D+  a distribution deﬁned on
the set of all positive instances as follows. Let nk be the number of positive instances assigned to
sub-classiﬁer k  and n be the total number of positive instances. We deﬁne D+ as the empirical
instances  and maximal at log2 K when every classiﬁer ﬁres on a K−1 fraction of the positive in-
stances. Thus  maximizing the entropy encourages the algorithm to break down large clusters into
smaller clusters of near equal size.
We use this notion of entropy in our heuristic procedure for assignment  described in Algorithm 2.
ASSIGN takes a predeﬁned minimum entropy level h ≥ 0 and compensates for disparities in how
positive instances are assigned to sub-classiﬁers  where the disparity is measured by entropy. When
the entropy is above h  ASSIGN uses the natural argmaxk Wkx assignment. Conversely  if the
current entropy is below h  then it picks an assignment that is guaranteed to increase the entropy.
Thus  when h = 0  there is no adjustment made. It keeps a dictionary UNADJ mapping the previous
points it has encountered to the unadjusted assignment that the natural argmax assignment would
had made at the time of encountering the point. We write UNADJ + (x  k) to denote the new dictio-
nary U such that U [v] is equal to k if v = x and to UNADJ[v] otherwise. Dictionary UNADJ keeps
track of the assigned positives per sub-classiﬁers  and serves to estimate the current entropy in the
conﬁguration without needing to recompute every prior point’s assignment.

(cid:1). The entropy is zero when the same classiﬁer ﬁres for all positive

distribution on(cid:0) n1

W ← (1 − ηtλ)W

n   n2

n   . . .   nk

n

5 Evaluation

We use seven data sets1 to evaluate the CPM: (1) an MNIST data set consisting of labeled handwrit-
ten digits encoded in 28× 28 gray scale pictures (60 000 training and 10 000 testing instances) [12];
(2) an MNIST8m data set consisting of 8 100 000 pictures obtained by applying various random
deformations to MNIST training instances MNIST [13]; (3) a URL data set used for malicious URL
detection (1.1 million training and 1.1 million testing instances with more than 2.3 million fea-
tures) [14]; (4) the RCV1-bin data set corresponding to a binary classiﬁcation task (separating cor-
porate and economics categories from government and markets categories) deﬁned over the RCV1
data set of news articles (20 242 training and 677 399 testing instances) [15]; (5) the IJCNN1 data set
for detecting misﬁrings of combustion engine (35 000 training and 91 701 testing instances with 22
features); (6) the A9A data set representing census data for predicting annual income (32 561 train-
ing and 16 281 testing instances with 123 features); (7) the KDDA data set for predicting student
performance (8 407 752 training and 510 302 testing instances with more than 20 million features).

1All data sets available at http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets

6

Since our main focus is on binary classiﬁcation  for the two MNIST data sets we evaluate distin-
guishing 2’s from any other digit  which we call MNIST-2 and MNIST8m-2.

5.1 Parameter Tuning

kadj ← kunadj
hcur ← ENTROPY(UNADJ)
Kinc ← {k: ENTROPY(UNADJ+(x  k)) > hcur}
kadj ← argmax
k∈Kinc

Wkx

UNADJ ← UNADJ + (x  kunadj)
return kadj

Algorithm 2 Heuristic maximum assignment algorithm.
The input is the current weight matrix W  positive in-
stance x  and the desired assignment entropy h ≥ 0.

Initialize UNADJ← {}
function ASSIGN(W  x  h)
kunadj ← argmaxk Wkx
if ENTROPY(UNADJ + (x  kunadj)) ≥ h then
else

All seven data sets have well deﬁned
training and testing subsets and to tune
each algorithms meta-parameters (λ
and h for the CPM  C and γ for RBF-
SVM  and λ for AMM)  we randomly
select a ﬁxed validation subset from the
training set (1 000 to 10 000 depending
on the size of the training set).
For the CPM  we use a double-sided
CPM as described in section 3.1  where
both CPMs share the same meta-
parameters. We start by ﬁxing a num-
ber of iterations T and a number of
hyperplanes K which will result in a
reasonable execution time  effectively
treating these parameters as a com-
putational budget  and we experimen-
tally demonstrate that increasing either
K or T always results in a decrease of the testing error. Once these are selected  we let
h = 0 and select the best λ in {T −1  10 × T −1  . . .   104 × T −1}. We then choose h from
{0  log K/10  log 2K/10  . . .   log 9K/10}  effectively performing a one-round coordinate descent
on λ  h. To test the effectiveness of our empirical entropy-driven assignment procedure  we mute
the mechanism by also testing with h = 0. We make our C++11 implementation available.2
The AMM [16] has three parameters to adjust (excluding T and the equivalent of K)  two of which
control the weight pruning mechanism and are left set at default values. We only adjust λ. Contrary
to the CPM  we do not observe AMM testing error to strictly decrease with the number of iterations
T . We observe erratic behavior and thus we manually select the smallest T for which the mean vali-
dation error appears to reach a minimum. For RBF-SVM  we use the LibSVM [17] implementation
and perform the usual grid search on the parameter space.

5.2 Performance

Unless stated otherwise  we used one core of an Intel Xeon E5 (3.2Ghz  64GB RAM) for experi-
ments. Table 1 presents the results of experiments and shows that the CPM achieves comparable  and
at times better  classiﬁcation accuracy than the RBF-SVM  while working at a relatively small and
constant computational budget. For the CPM  T was up to 32 million and K ranged from 10 to 100.
For AMM  T ranged from 500 000 to 36 million. Across methods  the worst execution time is for
the MNIST8m-2 task  where a 512 core parallel implementation of RBF-SVM runs in 2 days [18] 
and our sequential single-core algorithm runs in less than 5 minutes. The AMM has signiﬁcantly
larger errors and/or execution times. For small training sets such as MNIST-2 and RCV1-bin  we
were not able to achieve consistent results  regardless of how we set T and λ  and we conjecture that
this is a consequence of the weight pruning mechanism. The results show that our empirical entropy-
driven assignment procedure for the CPM leads to better solutions for all tasks. In the RCV1-bin
and MNIST-2 tasks  the improvement in accuracy from using a tuned entropy parameter is 31% and
21%  respectively.
We use the MNIST8m-2 task to the study the effects of tuning T and K on the CPM. We ﬁrst choose
a grid of values for T  K and for a ﬁxed regularization factor C and h = 0  we train a model for
each point of the parameter grid  and evaluate its performance on the testing set. Note that for C
to remain constant  we adjust λ = 1
CT . We run each experiment 5 times and only report the mean
accuracy. Figure 2 shows how this mean error rate evolves as a function of both T and K. We

2CPM implementation available at https://github.com/alkant/cpm

7

MNIST8m-2

URL

CPM
CPM h=0
RBF-SVM
AMM

Error
0.30 ± 0.023
0.35 ± 0.034
0.43∗
0.38 ± 0.024

Time
4m
4m
2d∗∗
1hr

Error
1.32 ± 0.012
1.35 ± 0.029
2.20 ± 0.067

Timed out

Time
3m
3m

5m

KDDA

Error
10.38 ± 0.027
10.40 ± 0.021
Timed out
18.25 ± 6.51

Time
6m
6m

53m

(a) Large Data Sets

* for unadjusted parameters [18]
** running on 512 processors [18]

MNIST-2

IJCNN1

A9A

CPM
CPM h=0
RBF-SVM
AMM

Error
0.38 ± 0.028
0.46 ± 0.026
0.35
2.83 ± 1.090

Time
2m
2m
7m
1m

Error
3.00 ± 0.114

Same as CPM

1.44
2.84 ± 0.312

Time

2m

1s
14s

Error
15.15 ± 0.062

Same as CPM

14.96
15.29 ± 0.181

Time

15s

1m
12s

RCV1-bin

Error
2.82 ± 0.059
3.69 ± 0.156
3.7
15.40 ± 6.420

Time
2m
2m
46m
1m

(b) Small Data Sets

Table 1: Error rates and running times (include both training and testing periods) for binary tasks.
Means and standard deviations for 5 runs with random shufﬂing of the training set.

observe two phenomena. First  for any value K > 1  the error rate decreases with T . Second 
for large enough values of T   the error rate decreases when K increases. These two observations
validate our treatment of both K and T as budgeting parameters. The observation about K also
provides empirical evidence of our conjecture that large values of K do not lead to overﬁtting.

5.3 Multi-class Classiﬁcation

a

(cid:1) = 45

2

We
performed
preliminary multi-
class
classiﬁcation experiment using the
MNIST/MNIST8m data sets. There are several
approaches for building a multi-class classiﬁer
from a binary classiﬁer [19  20  21]. We used a

one-vs-one approach where we train(cid:0)10

one-vs-one classiﬁers and classify by a major-
ity vote rule with random tie breaking. While
this approach is not optimal  it approximates
achievable performance. For MNIST  CPM’s
testing error is 1.61± 0.019 and RBF-SVM’s is
1.47  with running times of 7m20s and 6m43s 
respectively. On MNIST8m  CPM’s error is
1.03 ± 0.074 (2h3m) and RBF-SVM’s is 0.67
(8 days) as reported by [13].

Figure 2: Error rate on MNIST8m-2 as a function
of K  T . C = 0.01 and h = 0 are ﬁxed.

6 Conclusion

We propose a novel algorithm for Convex Polytope Machine (CPM) separation that provides larger
margins than a single linear classiﬁer  while still enjoying the computational effectiveness of a simple
linear separator. Our algorithm learns a bounded number of linear classiﬁers simultaneously. On
large data sets  the CPM outperforms RBF-SVM and AMM  both in terms of running times and
error rates. Furthermore  by not pruning the number of sub-classiﬁers used  CPM is algorithmically
simpler than AMM. CPM avoids such complications by having little tendency to overﬁt the data as
the number K of sub-classiﬁers increases  shown empirically in Section 5.2.
Acknowledgements. This research was supported in part by Intel’s ISTC for Secure Computing 
NSF grants 0424422 (TRUST) and 1139158  the Freedom 2 Connect Foundation  US State Dept.
DRL  LBNL Award 7076018  DARPA XData Award FA8750-12-2-0331  and gifts from Amazon 
Google  SAP  Apple  Cisco  Clearstory Data  Cloudera  Ericsson  Facebook  GameOnTalis  Gen-
eral Electric  Hortonworks  Huawei  Intel  Microsoft  NetApp  Oracle  Samsung  Splunk  VMware 
WANdisco and Yahoo!. The opinions in this paper are those of the authors and do not necessarily
reﬂect those of any funding sponsor or the United States Government.

8

References
[1] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal Estimated sub-
In International Conference on Machine Learning  ICML ’07 

GrAdient SOlver for SVM.
pages 807–814  2007.

[2] Paul Fischer. More or less efﬁcient agnostic learning of convex polygons.

In Conference
on Computational Learning Theory  COLT ’95  pages 337–344  New York  NY  USA  1995.
ACM.

[3] Gabor Takacs. Smooth maximum based algorithms for classiﬁcation  regression  and collabo-

rative ﬁltering. Acta Technica Jaurinensis  3(1)  2010.

[4] Zhuang Wang  Nemanja Djuric  Koby Crammer  and Slobodan Vucetic. Trading representabil-
ity for scalability: adaptive multi-hyperplane machine for nonlinear classiﬁcation. In interna-
tional conference on Knowledge discovery and data mining (KDD 2011)  2011.

[5] Naresh Manwani and P. S. Sastry. Learning polyhedral classiﬁers using logistic function. In
Proceedings of the 2nd Asian Conference on Machine Learning (ACML 2010)  Tokyo  Japan 
2010.

[6] Naresh Manwani and P. S. Sastry.

arXiv:1107.1564  2013.

Polyceptron: A polyhedral

learning algorithm.

[7] P. F. Felzenszwalb  R. B. Girshick  D. McAllester  and D. Ramanan. Object detection with dis-
criminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine
Intelligence  32(9):1627–1645  2010.

[8] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning  20(3):273–

297  1995.

[9] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds

and structural results. J. Mach. Learn. Res.  3:463–482  March 2003.

[10] L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTAT’2010  pages 177–186. 2010.

[11] Geoffrey E Hinton. A practical guide to training restricted Boltzmann machines. In Neural

Networks: Tricks of the Trade  pages 599–619. 2012.

[12] Yann LeCun  Corinna Cortes  and Christopher J.C. Burges. MNIST dataset  1998.
[13] St´ephane Canu and Leon Bottou. Training invariant support vector machines using selective

sampling. In Large Scale Kernel Machines  pages 301–320. MIT  2007.

[14] Justin Ma  Lawrence K. Saul  Stefan Savage  and Geoffrey M. Voelker. Beyond blacklists:
Learning to detect malicious web sites from suspicious URLs. In International Conference on
Knowledge Discovery and Data Mining (KDD ’09)  pages 1245–1254  2009.

[15] David D. Lewis  Yiming Yang  Tony G. Rose  and Fan Li. RCV1: A new benchmark collection

for text categorization research. J. Mach. Learn. Res.  5:361–397  December 2004.

[16] Nemanja Djuric  Liang Lan  Slobodan Vucetic  and Zhuang Wang. BudgetedSVM: A toolbox
for scalable SVM approximations. Journal of Machine Learning Research  14:3813–3817 
2013.

[17] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM

Trans. Intell. Syst. Technol.  2(3):27:1–27:27  May 2011.

[18] Zeyuan Allen Zhu  Weizhu Chen  Gang Wang  Chenguang Zhu  and Zheng Chen. P-packSVM:
In International Conference on Data Mining

Parallel primal gradient descent kernel SVM.
(ICDM’09).  pages 677–686. IEEE  2009.

[19] Alina Beygelzimer  John Langford  Yuri Lifshits  Gregory Sorkin  and Alex Strehl. Conditional
probability tree estimation analysis and algorithms. In Uncertainty in Artiﬁcial Intelligence 
UAI ’09  pages 51–58  2009.

[20] Alina Beygelzimer  John Langford  and Bianca Zadrozny. Weighted one-against-all. In Na-

tional Conference on Artiﬁcial Intelligence - Volume 2  AAAI’05  pages 720–725  2005.

[21] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-

correcting output codes. J. Artif. Int. Res.  2(1):263–286  January 1995.

9

,Alex Kantchelian
Michael Tschantz
Ling Huang
Peter Bartlett
Anthony Joseph