2018,Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net,The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve   regression shrinkage and variable selection   allowing the inference of robust models from large data sets.  However  there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models  due to a need to evaluate analytically intractable partition function integrals. Here  the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over "regression frequencies". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net  where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients  and allows for Bayesian inference of much higher dimensional models than previously possible.,Analytic solution and stationary phase approximation

for the Bayesian lasso and elastic net

Computational Biology Unit  Department of Informatics  University of Bergen  Norway

The Roslin Institute  The University of Edinburgh  UK

Tom Michoel

tom.michoel@uib.no

Abstract

The lasso and elastic net linear regression models impose a double-exponential
prior distribution on the model parameters to achieve regression shrinkage and
variable selection  allowing the inference of robust models from large data sets.
However  there has been limited success in deriving estimates for the full posterior
distribution of regression coefﬁcients in these models  due to a need to evaluate
analytically intractable partition function integrals. Here  the Fourier transform
is used to express these integrals as complex-valued oscillatory integrals over
“regression frequencies”. This results in an analytic expansion and stationary
phase approximation for the partition functions of the Bayesian lasso and elastic
net  where the non-differentiability of the double-exponential prior has so far
eluded such an approach. Use of this approximation leads to highly accurate
numerical estimates for the expectation values and marginal posterior distributions
of the regression coefﬁcients  and allows for Bayesian inference of much higher
dimensional models than previously possible.

1

Introduction

Statistical modelling of high-dimensional data sets where the number of variables exceeds the
number of experimental samples may result in over-ﬁtted models that do not generalize well to
unseen data. Prediction accuracy in these situations can often be improved by shrinking regression
coefﬁcients towards zero [1]. Bayesian methods achieve this by imposing a prior distribution on
the regression coefﬁcients whose mass is concentrated around zero. For linear regression  the most
popular methods are ridge regression [2]  which has a normally distributed prior; lasso regression [3] 
which has a double-exponential or Laplace distribution prior; and elastic net regression [4]  whose
prior interpolates between the lasso and ridge priors. The lasso and elastic net are of particular interest 
because in their maximum-likelihood solutions  a subset of regression coefﬁcients are exactly zero.
However  maximum-likelihood solutions only provide a point estimate for the regression coefﬁcients.
A fully Bayesian treatment that takes into account uncertainty due to data noise and limited sample
size  and provides posterior distributions and conﬁdence intervals  is therefore of great interest.
Unsurprisingly  Bayesian inference for the lasso and elastic net involves analytically intractable
partition function integrals and requires the use of numerical Gibbs sampling techniques [5–8].
However  Gibbs sampling is computationally expensive and  particularly in high-dimensional settings 
convergence may be slow and difﬁcult to assess or remedy [9–12]. An alternative to Gibbs sampling
for Bayesian inference is to use asymptotic approximations to the intractable integrals based on
Laplace’s method [13  14]. However  the Laplace approximation requires twice differentiable log-
likelihood functions  and cannot be applied to the lasso and elastic net models as they contain a
non-differentiable term proportional to the sum of absolute values (i.e. (cid:96)1-norm) of the regression
coefﬁcients.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Alternatives to the Laplace approximation have been considered for statistical models where the Fisher
information matrix is singular  and no asymptotic approximation using normal distributions is feasible
[15  16]. However  in (cid:96)1-penalized models  the singularity originates from the prior distributions
on the model parameters  and the Fisher information matrix remains positive deﬁnite. Here we
show that in such models  approximate Bayesian inference is in fact possible using a Laplace-like
approximation  more precisely the stationary phase or saddle point approximation for complex-valued
oscillatory integrals [17]. This is achieved by rewriting the partition function integrals in terms
of “frequencies” instead of regression coefﬁcients  through the use of the Fourier transform. The
appearance of the Fourier transform in this context should not come as a big surprise. The stationary
phase approximation can be used to obtain or invert characteristic functions  which are of course
Fourier transforms [18]. More to the point of this paper  there is an intimate connection between the
Fourier transform of the exponential of a convex function and the Legendre-Fenchel transform of
that convex function  which plays a fundamental role in physics by linking microscopic statistical
mechanics to macroscopic thermodynamics and quantum to classical mechanics [19]. In particular 
convex duality [20  21]  which maps the solution of a convex optimization problem to that of its
dual  is essentially equivalent to writing the partition function of a Gibbs probability distribution in
coordinate or frequency space (Appendix A).
Convex duality principles have been essential to characterize analytical properties of the maximum-
likelihood solutions of the lasso and elastic net regression models [22–27]. This paper shows that
equally powerful duality principles exist to study Bayesian inference problems.

2 Analytic results

We consider the usual setup for linear regression where there are n observations of p predictor
variables and one response variable  and the effects of the predictors on the response are to be
coefﬁcients which need to be estimated and (cid:107)v(cid:107) = ((cid:80)n
determined by minimizing the least squares cost function (cid:107)y − Ax(cid:107)2 subject to additional constraints 
where y ∈ Rn are the response data  A ∈ Rn×p are the predictor data  x ∈ Rp are the regression
i=1 |vi|2)1/2 is the (cid:96)2-norm. Without loss of
n(cid:88)

generality  it is assumed that the response and predictors are centred and standardized 
ij = n for j ∈ {1  2  . . .   p}.
A2

Aij = 0 and

n(cid:88)

n(cid:88)

n(cid:88)

y2
i =

yi =

(1)

i=1

i=1

i=1

i=1

In a Bayesian setting  a hierarchical model is assumed where each sample yi is drawn independently
from a normal distribution with mean Ai•x and variance σ2  where Ai• denotes the ith row of A  or
more succintly 

(2)
where N denotes a multivariate normal distribution  and the regression coefﬁcients x are assumed to
have a prior distribution

p(y | A  x) = N (Ax  σ21) 

(cid:104)− n

(cid:0)λ(cid:107)x(cid:107)2 + 2µ(cid:107)x(cid:107)1

(cid:1)(cid:105)

p(x) ∝ exp

where (cid:107)x(cid:107)1 =(cid:80)p

(3)
j=1 |xj| is the (cid:96)1-norm  and the prior distribution is deﬁned upto a normalization
constant. The apparent dependence of the prior distribution on the data via the dimension paramater
n only serves to simplify notation  allowing the posterior distribution of the regression coefﬁcients to
be written  using Bayes’ theorem  as

σ2

 

p(x | y  A) ∝ p(y | x  A)p(x) ∝ e

− n
σ2 L(x|y A) 

where

L(x | y  A) =

1
2n

= xT(cid:0) AT A

2n

(cid:107)y − Ax(cid:107)2 + λ(cid:107)x(cid:107)2 + 2µ(cid:107)x(cid:107)1

+ λ1(cid:1)x − 2(cid:0) AT y

(cid:1)T

2n

x + 2µ(cid:107)x(cid:107)1 +

(cid:107)y(cid:107)2

1
2n

(4)

(5)

(6)

is minus the posterior log-likelihood function. The maximum-likelihood solutions of the lasso (λ = 0)
and elastic net (λ > 0) models are obtained by minimizing L  where the relative scaling of the penalty

2

parameters to the sample size n corresponds to the notational conventions of [28]1. In the current
setup  it is assumed that the parameters λ ≥ 0  µ > 0 and σ2 > 0 are given a priori.
To facilitate notation  a slightly more general class of cost functions is deﬁned as

H(x | C  w  µ) = xT Cx − 2wT x + 2µ(cid:107)x(cid:107)1 

(7)
where C ∈ Rp×p is a positive-deﬁnite matrix  w ∈ Rp is an arbitrary vector and µ > 0. After
discarding a constant term  L(x | y  A) is of this form  as is the so-called “non-naive” elastic net 
where C = ( 1
2n AT A + λ1)/(λ + 1) [4]. More importantly perhaps  eq. (7) also covers linear
mixed models  where samples need not be independent [29]. In this case  eq. (2) is replaced by
p(y | A  x) = N (Ax  σ2K)  for some covariance matrix K ∈ Rn×n  resulting in a posterior minus
2n AT K−1y. The requirement that C
log-likelihood function with C = 1
is positive deﬁnite  and hence invertible  implies that H is strictly convex and hence has a unique
minimizer. For the lasso (λ = 0) this only holds without further assumptions if n ≥ p [26]; for the
elastic net (λ > 0) there is no such constraint.
The Gibbs distribution on Rp for the cost function H(x | C  w  µ) with inverse temperature τ is
deﬁned as

2n AT K−1A + λ1 and w = 1

p(x | C  w  µ) =

e−τ H(x|C w µ)
Z(C  w  µ)

.

constant Z =(cid:82)

For ease of notation we will henceforth drop explicit reference to C  w and µ. The normalization
Rp e−τ H(x)dx is called the partition function. There is no known analytic solution
for the partition function integral. However  in the posterior distribution (4)  the inverse temperature
σ2 is large  ﬁrstly because we are interested in high-dimensional problems where n is large
τ = n
(even if it may be small compared to p)  and secondly because we assume a priori that (some
of) the predictors are informative for the response variable and that therefore σ2  the amount of
variance of y unexplained by the predictors  must be small. It therefore makes sense to seek an
analytic approximation to the partition function for large values of τ. However  the usual approach
to approximate e−τ H(x) by a Gaussian in the vicinity of the minimizer of H and apply a Laplace
approximation [17] is not feasible  because H is not twice differentiable. Instead we observe that
e−τ H(x) = e−2τ f (x)e−2τ g(x) where

(8)

(9)

xT Cx − wT x

f (x) =

1
2

g(x) = µ

p(cid:88)

j=1

|xj|.

(cid:90)

Rp

p

1

2(cid:112)det(C)
(cid:90) i∞

(πτ )

(cid:90) i∞

p(cid:89)

j=1

(cid:90)

Z =

Rp

Using Parseval’s identity for Fourier transforms (Appendix A.1)  it follows that (Appendix A.3)

e−2τ f (x)e−2τ g(x)dx =

e−τ (k−iw)T C−1(k−iw)

µ
j + µ2 dk.
k2

(10)
After a change of variables z = −ik  Z can be written as a p-dimensional complex contour integral

p(cid:89)

j=1

(−iµ)p

2(cid:112)det(C)

p

Z =

(πτ )

···

eτ (z−w)T C−1(z−w)

−i∞

−i∞

1

µ2 − z2

j

dz1 . . . dzp.

(11)

Cauchy’s theorem [30  31] states that this integral remains invariant if the integration contours are
deformed  as long as we remain in a domain where the integrand does not diverge (Appendix A.4).
The analogue of Laplace’s approximation for complex contour integrals  known as the stationary
phase  steepest descent or saddle point approximation  then states that an integral of the form (11) can
be approximated by a Gaussian integral along a steepest descent contour passing through the saddle
point of the argument of the exponential function [17]. Here  the function (z − w)T C−1(z − w)
has a saddle point at z = w. If |wj| < µ for all j  the standard stationary phase approximation can
be applied directly  but this only covers the uninteresting situation where the maximum-likelihood
solution ˆx = argminx H(x) = 0 (Appendix A.5). As soon as |wj| > µ for at least one j  the standard
2 + α(cid:107)x(cid:107)1)  wich is obtained from (5) by

1To be precise  in [28] the penalty term is written as ˜λ( 1−α

2 (cid:107)x(cid:107)2

setting ˜λ = 2(λ + µ) and α = µ

λ+µ .

3

p(cid:88)

j=1

of the function(cid:81)

argument breaks down  since to deform the integration contours from the imaginary axes to parallel
contours passing through the saddle point z0 = w  we would have to pass through a pole (divergence)
j )−1 (Figure S1). Motivated by similar  albeit one-dimensional  analyses

j(µ2 − z2

in non-equilibrium physics [32  33]  we instead consider a temperature-dependent function

τ (z) = (z − w)T C−1(z − w) − 1
H∗
τ

ln(µ2 − z2
j ) 

(12)

which is well-deﬁned on the domain D = {z ∈ Cp : |(cid:60)zj| < µ  j = 1  . . .   p}  where (cid:60) denotes
the real part of a complex number. This function has a unique saddle point in D  regardless whether
|wj| < µ or not (Figure S1). Our main result is a steepest descent approximation of the partition
function around this saddle point.
Theorem 1 Let C ∈ Rp×p be a positive deﬁnite matrix  w ∈ Rp and µ > 0. Then the complex
τ deﬁned in eq. (12) has a unique saddle point ˆuτ that is real  ˆuτ ∈ D ∩ Rp  and is a
function H∗
solution of the set of third order equations

u ∈ Rp  j ∈ {1  . . .   p}.
For Q(z) a complex analytic function of z ∈ Cp  the generalized partition function

j )[C−1(w − u)]j − uj
τ

(µ2 − u2

= 0  

(cid:90)

2(cid:112)det(C)

1

p

Z[Q] =

(πτ )

(cid:17)p

(cid:16) µ√

τ

can be analytically expressed as

Z[Q] =

eτ (w−ˆuτ )T C−1(w−ˆuτ )

p(cid:89)

e−τ (k−iw)T C−1(k−iw)Q(−ik)

Rp

j=1

µ
j + µ2 dk.
k2

p(cid:89)

j=1

1(cid:113)

µ2 + ˆu2

τ j

1

(cid:112)det(C + Dτ )
(cid:110) 1

(cid:111)

exp

4τ 2 ∆τ

eRτ (ik)Q(ˆuτ + ik)

(cid:12)(cid:12)(cid:12)(cid:12)k=0

(13)

 

(14)

(15)

(16)

(17)

(18)

where Dτ is a diagonal matrix with diagonal elements

∆τ is the differential operator

∆τ =

and

Rτ (z) =

(Dτ )jj =

τ (µ2 − ˆu2
µ2 + ˆu2

τ j)2

 

τ j

p(cid:88)
(cid:88)

i j=1

(cid:2)τ Dτ (C + Dτ )−1C(cid:3)
(cid:104)

1

(µ − ˆuτ j)m +

1
m

p(cid:88)

j=1

m≥3

ij

∂2

∂ki∂kj

(−1)m

(µ + ˆuτ j)m

(cid:105)

zm
j .

This results in an analytic approximation

Z[Q] ∼(cid:16) µ√

(cid:17)p

τ

eτ (w−ˆuτ )T C−1(w−ˆuτ )

p(cid:89)

j=1

1(cid:113)

µ2 + ˆu2

τ j

(cid:112)det(C + Dτ )

Q(ˆuτ )

.

The analytic expression in eq. (14) follows by changing the integration contours to pass through
the saddle point ˆuτ   and using a Taylor expansion of H∗
τ (z) around the saddle point along the
steepest descent contour. However  because ∆τ and Rτ depend on τ  it is not a priori evident
that (18) holds. A detailed proof is given in Appendix B. The analytic approximation in eq. (18)
can be simpliﬁed further by expanding ˆuτ around its leading term  resulting in an expression that
recognizably converges to the sparse maximum-likelihood solution (Appendix C). While eq. (18)
is computationally more expensive to calculate than the corresponding expression in terms of the
maximum-likelihood solution  it was found to be numerically more accurate (Section 3).

4

Various quantities derived from the posterior distribution can be expressed in terms of generalized
partition functions. The most important of these are the expectation values of the regression coefﬁ-
cients  which  using elementary properties of the Fourier transform (Appendix A.6)  can be expressed
as

(cid:90)

Z(cid:2)C−1(w − z)(cid:3)

Z

E(x) =

1
Z

Rp

x e−τ H(x) =

∼ C−1(w − ˆuτ ).

The leading term 

ˆxτ ≡ C−1(w − ˆuτ ) 

(19)
can be interpreted as an estimator for the regression coefﬁcients in its own right  which interpolates
smoothly (as a function of τ) between the ridge regression estimator ˆxridge = C−1w at τ = 0 and
the maximum-likelihood elastic net estimator ˆx = C−1(w − ˆu) at τ = ∞  where ˆu = limτ→∞ ˆuτ
satisﬁes a box-constrained optimization problem (Appendix C).
The marginal posterior distribution for a subset I ⊂ {1  . . .   p} of regression coefﬁcients is deﬁned as

p(xI ) =

1

Z(C  w  µ)

R|Ic|

e−τ H(x|C w µ)dxI c

where I c = {1  . . .   p} \ I is the complement of I  |I| denotes the size of a set I  and we have
reintroduced temporarily the dependency on C  w and µ as in eq. (7). A simple calculation shows
that the remaining integral is again a partition function of the same form  more precisely:

p(xI ) = e−τ (xT

I CI xI−2wT

I xI +2µ(cid:107)xI(cid:107)1) Z(CI c  wI c − xT

I CI I c  µ)

Z(C  w  µ)

 

(20)

where the subscripts I and I c indicate sub-vectors and sub-matrices on their respective coordinate
sets. Hence the analytic approximation in eq. (14) can be used to approximate numerically each term
in the partition function ratio and obtain an approximation to the marginal posterior distributions.
The posterior predictive distribution [1] for a new sample a ∈ Rp of predictor data can also be written
as a ratio of partition functions:

(cid:16) τ

(cid:17) 1

2

2πn

2n y2 Z(cid:0)C + 1

e− τ

2n aaT   w + y
Z(C  w  µ)

2n a  µ(cid:1)

 

p(y) =

Rp

p(y | a  x)p(x | C  w  µ) dx =

where C ∈ Rp×p and w ∈ Rp are obtained from the training data as before  n is the number of
training samples  and y ∈ R is the unknown response to a with distribution p(y). Note that

(cid:90)

(cid:90)

(cid:90)

(cid:104)(cid:90)

(cid:90)

(cid:105)
(cid:90)

E(y) =

yp(y)dy =

p(y | a  x)dy

p(x | C  w  µ) dx

R

Rp

R

=

3 Numerical experiments

Rp

aT xp(x | C  w  µ) dx = aT E(x) ∼ aT ˆxτ .

(21)

To test the accuracy of the stationary phase approximation  we implemented algorithms to solve the
saddle point equations and compute the partition function and marginal posterior distribution  as well
as an existing Gibbs sampler algorithm [8] in Matlab (see Appendix E for algorithm details  source
code available from https://github.com/tmichoel/bayonet/). Results were ﬁrst evaluated
for independent predictors (or equivalently  one predictor) and two commonly used data sets: the
“diabetes data” (n = 442  p = 10) [34] and the “leukemia data” (n = 72  p = 3571) [4] (see
Appendix F for further experimental details and data sources).
First we tested the rate of convergence in the asymptotic relation (see Appendix C)

τ→∞− 1

lim

τ

log Z = Hmin = min
x∈Rp

H(x).

For independent predictors (p = 1)  the partition function can be calculated analytically using
the error function (Appendix D)  and rapid convergence to Hmin is observed (Figure 1a). After
scaling by the number of predictors p  a similar rate of convergence is observed for the stationary

5

Figure 1: Convergence to the minimum-energy solution. Top row: (− 1
τ log Z − Hmin)/p vs. τ and
µ for the exact partition function for independent predictors (p = 1) (a)  and for the stationary phase
approximation for the diabetes (b) and leukemia (c) data. Bottom row: (cid:107)ˆxτ − ˆx(cid:107)∞ for the exact
expectation value for independent predictors (d)  and using the stationary phase approximation for
the diabetes (e) and leukemia (f) data. Parameter values were C = 1.0  w = 0.5  and µ ranging from
0.05 to 5 in geometric steps (a)  and λ = 0.1 and µ ranging from 0.01µmax upto  but not including 
µmax = maxj |wj| in geometric steps (b c).

Figure 2: Marginal posterior distributions for the diabetes data (λ = 0.1  µ = 0.0397  τ = 682.3) (a–
c) and leukemia data (λ = 0.1  µ = 0.1835  τ = 9943.9) (d–f;). In blue  Gibbs sampling histogram
(104 samples). In red  stationary phase approximation for the marginal posterior distribution of
selected predictors. In yellow  maximum-likelihood-based approximation for the same distributions.
The distributions for a zero  transition and non-zero maximum-likelihood predictor are shown (from
left to right). The ∗ on the x-axes indicate the location of the maximum-likelihood and posterior
expectation value.

6

a0.11020.050.21040.150.31060.5 1.5 1085 b00.021020.040.061040.0810-20.110610-1108c01020.0510410-20.110610-1108d01020.050.051040.10.151060.5 1.5 1085 e0.021020.040.060.081040.110-20.1210610-1108f0.021020.040.060.0810410-20.10.1210610-1108a-0.1-0.0500.050.1x051015202530p(x)b-0.2-0.100.10.2x051015p(x)c0.10.20.30.4x0246810p(x)d-3-2-101x10-3050010001500p(x)e-0.0500.05x0204060p(x)f-0.0500.050.10.15x0510152025p(x)phase approximation to the partition function for both the diabetes and leukemia data (Figure 1b c).
However  convergence of the posterior expectation values ˆxτ to the maximum-likelihood coefﬁcients
ˆx  as measured by the (cid:96)∞-norm difference (cid:107)ˆxτ − ˆx(cid:107)∞ = maxj |ˆxτ j − ˆxj| is noticeably slower 
particularly in the p (cid:29) n setting of the leukemia data (Figure 1d–f).
Next  the accuracy of the stationary phase approximation at ﬁnite τ was determined by comparing
the marginal distributions for single predictors [i.e. where I is a singleton in eq. (20)] to results
obtained from Gibbs sampling. For simplicity  representative results are shown for speciﬁc hyper-
parameter values (Appendix F.2). Application of the stationary phase approximation resulted in
marginal posterior distributions which were indistinguishable from those obtained by Gibbs sampling
(Figure 2). In view of the convergence of the log-partition function to the minimum-energy value
(Figure 1)  an approximation to eq. (20) of the form

p(xI ) ≈ e−τ (xT

I CI xI−2wT

I CI Ic  µ)−Hmin(C w µ)]

I xI +2µ(cid:107)xI(cid:107)1)e−τ [Hmin(CIc  wIc−xT

(22)
was also tested. However  while eq. (22) is indistinguishable from eq. (20) for predictors with zero
effect size in the maximum-likelihood solution  it resulted in distributions that were squeezed towards
zero for transition predictors  and often wildly inaccurate for non-zero predictors (Figure 2). This
is because eq. (22) is maximized at xI = ˆxI  the maximum-likelihood value  whereas for non-zero
coordinates  eq. (20) is (approximately) symmetric around its expectation value E(xI ) = ˆxτ I. Hence 
accurate estimations of the marginal posterior distributions requires using the full stationary phase
approximations [eq. (18)] to the partition functions in eq. (20).
The stationary phase approximation can be particularly advantageous in prediction problems  where
the response value ˆy ∈ R for a newly measured predictor sample a ∈ Rp is obtained using regression
coefﬁcients learned from training data (yt  At). In Bayesian inference  ˆy is set to the expectation
value of the posterior predictive distribution  ˆy = E(y) = aT ˆxτ [eq. (21)]. Computation of the
posterior expectation values ˆxτ [eq. (19)] using the stationary phase approximation requires solving
only one set of saddle point equations  and hence can be performed efﬁciently across a range of
hyper-parameter values  in contrast to Gibbs sampling  where the full posterior needs to be sampled
even if only expectation values are needed.
To illustrate how this beneﬁts large-scale applications of the Bayesian elastic net  its prediction
performance was compared to state-of-the-art Gibbs sampling implementations of Bayesian horseshoe
and Bayesian lasso regression [35]  as well as to maximum-likelihood elastic net and ridge regression 
using gene expression and drug sensitivity data for 17 anticancer drugs in 474 human cancer cell lines
from the Cancer Cell Line Encyclopedia [36]. Ten-fold cross-validation was used  using p = 1000 pre-
selected genes and 427 samples for training regression coefﬁcients and 47 for validating predictions
in each fold. To obtain unbiased predictions at a single choice for the hyper-parameters  µ and
τ were optimized over a 10 × 13 grid using an additional internal 10-fold cross-validation on the
training data only (385 samples for training  42 for testing); BayReg’s lasso and horseshoe methods
sample hyper-parameter values from their posteriors and do not require an additional cross-validation
loop (see Appendix F.3 for complete experimental details and data sources). Despite evaluating
a much greater number of models (in each cross-validation fold  10× cross-validation over 130
hyper-parameter combinations vs. 1 model per fold)  the overall computation time was still much
lower than BayReg’s Gibbs sampling approach (on average 30 sec. per fold  i.e. 0.023 sec. per model 
vs. 44 sec. per fold for BayReg). In terms of predictive performance  Bayesian methods tended to
perform better than maximum-likelihood methods  in particular for the most ‘predictable’ responses 
with little variation between the three Bayesian methods (Figure 3a).
While the difference in optimal performance between Bayesian and maximum-likelihood elastic net
was not always large  Bayesian elastic net tended to be optimized at larger values of µ (i.e. at sparser
maximum-likelihood solutions)  and at these values the performance improvement over maximum-
likelihood elastic net was more pronounced (Figure 3b). As expected  τ acts as a tuning parameter
that allows to smoothly vary from the maximum-likelihood solution at large τ (here  τ ∼ 106) to the
solution with best cross-validation performance (here  τ ∼ 103 − 104) (Figure 3c). The improved
performance at sparsity-inducing values of µ suggests that the Bayesian elastic net is uniquely able to
identify the dominant predictors for a given response (the non-zero maximum-likelihood coefﬁcients) 
while still accounting for the cumulative contribution of predictors with small effects. Comparison
with the unpenalized (µ = 0) ridge regression coefﬁcients shows that the Bayesian expectation
values are strongly shrunk towards zero  except for the non-zero maximum-likelihood coefﬁcients 
which remain relatively unchanged (Figure 3d)  resulting in a double-exponential distribution for

7

Figure 3: Predictive accuracy on the Cancer Cell Line Encyclopedia. a. Median correlation coefﬁcient
between predicted and true drug sensitivities over 10-fold cross-validation  using Bayesian posterior
expectation values from the analytic approximation for elastic net (red) and from BayReg’s lasso
(blue) and horseshoe (yellow) implementations  and maximum-likelihood elastic net (purple) and
ridge regression (green) values for the regression coefﬁcients. See main text for details on hyper-
parameter optimization. b. Median 10-fold cross-validation value for the correlation coefﬁcient
between predicted and true sensitivities for the compound PD-0325901 vs. µ  for the Bayesian elastic
net at optimal τ (red)  maximum-likelihood elastic net (blue) and ridge regression (dashed). c. Median
10-fold cross-validation value for the correlation coefﬁcient between predicted and true sensitivities
for PD-0325901 for the Bayesian elastic net vs. τ and µ; the black dots show the overall maximum
and the ML maximum. d. Scatter plot of expected regression coefﬁcients in the Bayesian elastic net
for PD-0325901 at µ = 0.055 and optimal τ = 3.16 · 103 vs. ridge regression coefﬁcient estimates;
coefﬁcients with non-zero maximum-likelihood elastic net value at the same µ are indicated in red.
See Supp. Figures S2 and S3 for the other 16 compounds.

the regression coefﬁcients. This contrasts with ridge regression  where regression coefﬁcients are
normally distributed leading to over-estimation of small effects  and maximum-likelihood elastic net 
where small effects become identically zero and don’t contribute to the predicted value at all.

4 Conclusions

The application of Bayesian methods to infer expected effect sizes and marginal posterior distributions
in (cid:96)1-penalized models has so far required the use of computationally expensive Gibbs sampling
methods. Here it was shown that highly accurate inference in these models is also possible using
an analytic stationary phase approximation to the partition function integrals. This approximation
exploits the fact that the Fourier transform of the non-differentiable double-exponential prior dis-
tribution is a well-behaved exponential of a log-barrier function  which is intimately related to the
Legendre-Fenchel transform of the (cid:96)1-penalty term. Thus  the Fourier transform plays the same role
for Bayesian inference problems as convex duality plays for maximum-likelihood approaches.
For simplicity  we have focused on the linear regression model  where the invariance of multivariate
normal distributions under the Fourier transform greatly facilitates the analytic derivations. Prelim-
inary work shows that the results can probably be extended to generalized linear models (or any

8

aPD-0325901TopotecanAZD6244PaclitaxelPanobinostatLapatinibSorafenibTKI25817-AAGAEW541TAE684ErlotinibPF2341066ZD-6474AZD0530Nutlin-3LBW24200.10.20.30.40.50.60.7corr(ytrue ypred)BAY Elastic Net (analytic approx)BAYREG LassoBAYREG HorseshoeML Elastic NetML Ridgeb 0.17 0.11 0.069 0.044 0.027 0.017 0.0110.00690.00440.002700.10.20.30.40.50.60.7corr(ytrue ypred)BAY Elastic NetML Elastic NetML Ridgec01030.20.4corr(ytrue ypred)1040.00440.6 0.011105 0.027 0.069106 0.17d-0.08-0.06-0.04-0.0200.020.040.06xRidge-0.02-0.0100.010.020.030.040.05xBayELNmodel with convex energy function) with L1 penalty  using the argument sketched in Appendix A.2.
In such models  the predictor correlation matrix C will need to be replaced by the Hessian matrix
of the energy function evaluated at the saddle point. Numerically  this will require updates of the
Hessian during the coordinate descent algorithm for solving the saddle point equations. How to
balance the accuracy of the approximation and the frequency of the Hessian updates will require
further in-depth investigation. In principle  the same analysis can also be performed using other non-
twice-differentiable sparse penalty functions  but if their Fourier transform is not known analytically 
or not twice differentiable either  the analysis and implementation will become more complicated
still.
A limitation of the current approach may be that values of the hyper-parameters need to be speciﬁed
in advance  whereas in complete hierarchical models  these are subject to their own prior distributions.
Incorporation of such priors will require careful attention to the interchange between taking the limit
of and integrating over the inverse temperature parameter. However  in many practical situations
(cid:96)1 and (cid:96)2-penalty parameters are pre-determined by cross-validation. Setting the residual variance
parameter to its maximum a-posteriori value then allows to evaluate the maximum-likelihood solution
in the context of the posterior distribution of which it is the mode [8]. Alternatively  if the posterior
expectation values of the regression coefﬁcients are used instead of their maximum-likelihood values
to predict unmeasured responses  the optimal inverse-temperature parameter can be determined by
standard cross-validation on the training data  as in the drug response prediction experiments.
No attempt was made to optimize the efﬁciency of the coordinate descent algorithm to solve the
saddle point equations. However  comparison to the Gibbs sampling algorithm shows that one cycle
through all coordinates in the coordinate descent algorithm is approximately equivalent to one cycle
in the Gibbs sampler  i.e. to adding one more sample. The coordinate descent algorithm typically
converges in 5-10 cycles starting from the maximum-likelihood solution  and 1-2 cycles when starting
from a neighbouring solution in the estimation of marginal distributions. In contrast  Gibbs sampling
typically requires 103-105 coordinate cycles to obtain stable distributions. Hence  if only the posterior
expectation values or the posterior distributions for a limited number of coordinates are sought  the
computational advantage of the stationary phase approximation is vast. On the other hand  each
evaluation of the marginal distribution functions requires the solution of a separate set of saddle point
equations. Hence  computing these distributions for all predictors at a very large number of points
with the current algorithm could become equally expensive as Gibbs sampling.
In summary  expressing intractable partition function integrals as complex-valued oscillatory integrals
through the Fourier transform is a powerful approach for performing Bayesian inference in the lasso
and elastic net regression models  and (cid:96)1-penalized models more generally. Use of the stationary
phase approximation to these integrals results in highly accurate estimates for the posterior expectation
values and marginal distributions at a much reduced computational cost compared to Gibbs sampling.

Acknowledgments

This research was supported by the BBSRC (grant numbers BB/J004235/1 and BB/M020053/1).

References
[1] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The elements of statistical learning.

Springer series in statistics Springer  Berlin  2001.

[2] Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthogonal

problems. Technometrics  12(1):55–67  1970.

[3] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 267–288  1996.

[4] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

[5] Trevor Park and George Casella. The Bayesian lasso. Journal of the American Statistical

Association  103(482):681–686  2008.

[6] Chris Hans. Bayesian lasso regression. Biometrika  pages 835–845  2009.

9

[7] Qing Li  Nan Lin  et al. The Bayesian elastic net. Bayesian Analysis  5(1):151–170  2010.

[8] Chris Hans. Elastic net regression modeling with the orthant normal prior. Journal of the

American Statistical Association  106(496):1383–1393  2011.

[9] Jun S Liu. Monte Carlo strategies in scientiﬁc computing. Springer  2004.

[10] Himel Mallick and Nengjun Yi. Bayesian methods for high dimensional linear models. Journal

of Biometrics & Biostatistics  1:005  2013.

[11] Bala Rajaratnam and Doug Sparks. Fast Bayesian lasso for high-dimensional regression. arXiv

preprint arXiv:1509.03697  2015.

[12] Bala Rajaratnam and Doug Sparks. MCMC-based inference in the era of big data: A funda-
mental analysis of the convergence complexity of high-dimensional chains. arXiv preprint
arXiv:1508.00947  2015.

[13] Robert E Kass and Duane Steffey. Approximate Bayesian inference in conditionally independent
hierarchical models (parametric empirical Bayes models). Journal of the American Statistical
Association  84(407):717–726  1989.

[14] Håvard Rue  Sara Martino  and Nicolas Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of the Royal
Statistical Society: Series B (Statistical Methodology)  71(2):319–392  2009.

[15] Sumio Watanabe. A widely applicable Bayesian information criterion. Journal of Machine

Learning Research  14(Mar):867–897  2013.

[16] Mathias Drton and Martyn Plummer. A Bayesian information criterion for singular models.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  79(2):323–380 
2017.

[17] Roderick Wong. Asymptotic approximation of integrals  volume 34. SIAM  2001.

[18] Henry E Daniels. Saddlepoint approximations in statistics. The Annals of Mathematical

Statistics  pages 631–650  1954.

[19] Grigory Lazarevich Litvinov. The Maslov dequantization  idempotent and tropical mathematics:

a brief introduction. arXiv preprint math/0507014  2005.

[20] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[21] R Tyrrell Rockafellar. Convex Analysis. Princeton University Press  1970.

[22] M.R. Osborne  B. Presnell  and B.A. Turlach. On the lasso and its dual. Journal of Computa-

tional and Graphical Statistics  9(2):319–337  2000.

[23] Michael R Osborne  Brett Presnell  and Berwin A Turlach. A new approach to variable selection

in least squares problems. IMA Journal of Numerical Analysis  20(3):389–403  2000.

[24] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination for the lasso and sparse

supervised learning problems. Paciﬁc Journal of Optimization  8:667–698  2012.

[25] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R.J. Tibshirani. Strong
rules for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society:
Series B (Statistical Methodology)  74(2):245–266  2012.

[26] R.J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics  7:1456–

1490  2013.

[27] Tom Michoel. Natural coordinate descent algorithm for L1-penalised regression in generalised

linear models. Computational Statistics & Data Analysis  97:60–70  2016.

[28] J. Friedman  T. Hastie  and R. Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of Statistical Software  33(1):1  2010.

10

[29] Barbara Rakitsch  Christoph Lippert  Oliver Stegle  and Karsten Borgwardt. A lasso multi-
marker mixed model for association mapping with population structure correction. Bioinformat-
ics  29(2):206–214  2012.

[30] Serge Lang. Complex Analysis  volume 103 of Graduate Texts in Mathematics. Springrer  2002.

[31] Volker Schneidemann. Introduction to Complex Analysis in Several Variables. Birkhäuser

Verlag  2005.

[32] R Van Zon and EGD Cohen. Extended heat-ﬂuctuation theorems for a system with deterministic

and stochastic forces. Physical Review E  69(5):056121  2004.

[33] Jae Sung Lee  Chulan Kwon  and Hyunggyu Park. Modiﬁed saddle-point integral near a
singularity for the large deviation function. Journal of Statistical Mechanics: Theory and
Experiment  2013(11):P11002  2013.

[34] B. Efron  T. Hastie  I.M. Johnstone  and R. Tibshirani. Least angle regression. The Annals of

Statistics  32(2):407–499  2004.

[35] Enes Makalic and Daniel F Schmidt. High-dimensional Bayesian regularised regression with

the BayesReg package. arXiv:1611.06649  2016.

[36] J. Barretina  G. Caponigro  N. Stransky  K. Venkatesan  A.A. Margolin  S. Kim  C.J. Wilson 
J. Lehár  G.V. Kryukov  D. Sonkin  et al. The Cancer Cell Line Encyclopedia enables predictive
modelling of anticancer drug sensitivity. Nature  483(7391):603–607  2012.

11

,Yong Ren
Jun Zhu
Jialian Li
Yucen Luo
Tom Michoel