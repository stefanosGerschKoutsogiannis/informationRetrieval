2008,Integrating Locally Learned Causal Structures with Overlapping Variables,In many domains  data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. There are several asymptotically correct  informative algorithms that search for causal information given a single dataset  even with missing values and hidden variables. There are  however  no such reliable procedures for distributed data with overlapping variables  and only a single heuristic procedure (Structural EM). This paper describes an asymptotically correct procedure  ION  that provides all the information about structure obtainable from the marginal independence relations. Using simulated and real data  the accuracy of ION is compared with that of Structural EM  and with inference on complete  unified data.,Integrating locally learned causal structures

with overlapping variables

Robert E. Tillman

Carnegie Mellon University

Pittsburgh  PA 15213

rtillman@andrew.cmu.edu

David Danks  Clark Glymour
Carnegie Mellon University &

Institute for Human & Machine Cognition

Pittsburgh  PA 15213

{ddanks cg09}@andrew.cmu.edu

Abstract

In many domains  data are distributed among datasets that share only some vari-
ables; other recorded variables may occur in only one dataset. While there are
asymptotically correct  informative algorithms for discovering causal relation-
ships from a single dataset  even with missing values and hidden variables  there
have been no such reliable procedures for distributed data with overlapping vari-
ables. We present a novel  asymptotically correct procedure that discovers a min-
imal equivalence class of causal DAG structures using local independence infor-
mation from distributed data of this form and evaluate its performance using syn-
thetic and real-world data against causal discovery algorithms for single datasets
and applying Structural EM  a heuristic DAG structure learning procedure for data
with missing values  to the concatenated data.

1 Introduction

In many domains  researchers are interested in predicting the effects of interventions  or manipulat-
ing variables  on other observed variables. Such predictions require knowledge of causal relation-
ships between observed variables. There are existing asymptotically correct algorithms for learning
such relationships from data  possibly with missing values and hidden variables [1][2][3]  but these
algorithms all assume that every variable is measured in a single study. Datasets for such studies are
not always readily available  often due to privacy  ethical  ﬁnancial  and practical concerns. How-
ever  given the increasing availability of large amounts of data  it is often possible to obtain several
similar studies that individually measure subsets of the variables a researcher is interested in and
together include all such variables. For instance  models of the United States and United Kingdom
economies share some but not all variables  due to different ﬁnancial recording conventions; fMRI
studies with similar stimuli may record different variables  since the images vary according to mag-
net strength  data reduction procedures  etc.; and U.S. states report some of the same educational
testing variables  but also report state-speciﬁc variables. In these cases  if each dataset has over-
lapping variable(s) with at least one other dataset  e.g. if two datasets D1 and D2  which measure
variables V1 and V2  respectively  have at least one variable in common (V1 ∩ V2 6= ∅)  then we
should be able to learn many of the causal relationships between the observed variables using this
set of datasets. The existing algorithms  however  cannot in general be directly applied to such cases 
since they may require joint observations for variables that are not all measured in a single dataset.

While this problem has been discussed in [4] and [5]  there are no general  useful algorithms for
learning causal relationships from data of this form. A typical response is to concatenate the datasets
to form a single common dataset with missing values for the variables that are not measured in each
of the original datasets. Statistical matching [6] or multiple imputation [7] procedures may then
be used to ﬁll in the missing values by assuming an underlying model (or small class of models) 
estimating model parameters using the available data  and then using this model to interpolate the

1

missing values. While the assumption of some underlying model may be unproblematic in many
standard prediction scenarios  i.e. classiﬁcation  it is unreliable for causal inference; the causal re-
lationships learned using the interpolated dataset that are between variables which are never jointly
measured in single dataset will only be correct if the corresponding relationships between variables
in the assumed model happen to be causal relationships in the correct model. The Structural EM
algorithm [8] avoids this problem by iteratively updating the assumed model using the current inter-
polated dataset and then reestimating values for the missing data to form a new interpolated dataset
until the model converges. The Structural EM algorithm is only justiﬁed  however  when missing
data are missing at random (or indicator variables can be used to make them so) [8]. The pattern
of missing values in the concatenated datasets described above is highly structured. Furthermore 
Structural EM is a heuristic procedure and may converge to local maxima. While this may not be
problematic in practice when doing prediction  it is problematic when learning causal relationships.
Our experiments in section 4 show that Structural EM performs poorly in this scenario.

We present a novel  asymptotically correct algorithm—the Integration of Overlapping Networks
(ION) algorithm—for learning causal relationships (or more properly  the complete set of possible
causal DAG structures) from data of this form. Section 2 provides the relevant background and
terminology. Section 3 discusses the algorithm. Section 4 presents experimental evaluations of the
algorithm using synthetic and real-world data. Finally  section 5 provides conclusions.

2 Formal preliminaries

We now introduce some terminology. A directed graph G = hV  Ei is a set of nodes V  which rep-
resent variables  and a set of directed edges E connecting distinct nodes. If two nodes are connected
by an edge then the nodes are adjacent. For pairs of nodes {X  Y } ⊆ V  X is a parent (child) of Y  
if there is a directed edge from X to Y (Y to X) in E. A trail in G is a sequence of nodes such that
each consecutive pair of nodes in the sequence is adjacent in G and no node appears more than once
in the sequence. A trail is a directed path if every edge between consecutive pairs of nodes points in
the same direction. X is an ancestor (descendant) of Y if there is a directed path from X to Y (Y
to X). G is a directed acyclic graph (DAG) if for every pair {X  Y } ⊆ V  X is not both an ancestor
and a descendent of Y (no directed cycles). A collider (v-structure) is a triple of nodes hX  Y  Zi
such that X and Z are parents of Y . A trail is active given C ⊆ V if (i) for every collider hX  Y  Zi
in the trail either Y ∈ C or some descendant of Y is in C and (ii) no other node in the trail is in C.
For disjoint sets of nodes X  Y  and Z  X is d-separated (d-connected) from Y given Z if and only if
there are no (at least one) active trails between any X ∈ X and any Y ∈ Y given Z.
A Bayesian network B is a pair hG  Pi  where G = hV  Ei is a DAG and P is a joint probability
distribution over the variables represented by the nodes in V such that P can be decomposed as
follows:

P(V) = Y

P (V |Parents(V ))

V ∈V

For B = hG  Pi  if X is d-separated from Y given Z in G  then X is conditionally independent of Y
given Z in P [9]. For disjoint sets of nodes  X  Y  and Z in V  P is faithful to G if X is d-separated
from Y given Z in G whenever X is conditionally independent of Y given Z in P [1]. B is a causal
Bayesian network if an edge from X to Y indicates that X is a direct cause of Y relative to V.
Most algorithms for causal discovery  or learning causal relationships from nonexperimental data 
assume that the distribution over the observed variables P is decomposable according to a DAG
G and P is faithful to G. The goal is to learn G using the data from P. Most causal discovery
algorithms return a set of possible DAGs which entail the same d-separations and d-connections 
e.g. the Markov equivalence class  rather than a single DAG. The DAGs in this set have the same
adjacencies but only some of the same directed edges. The directed edges common to each DAG
represent causal relationships that are learned from the data. If we admit the possibility that there
may be unobserved (latent) common causes between observed variables  then this set of possible
DAGs is usually larger.

A partial ancestral graph (PAG) represents the set of DAGs in a particular Markov equivalence class
when latent common causes may be present. Nodes in a PAG correspond to observed variables.
Edges are of four types: −◮  ◦−◮  ◦−◦ and ◭−◮  where a ◦ indicates either an ◮ or − orientation 
bidirected edges indicate the presence of a latent common cause  and fully directed edges (−◮)

2

indicate that the directed edge is present in every DAG  e.g. a causal relationship. For {X  Y } ⊆ V 
a possibly active trail between X and Y given Z ⊆ V/{X  Y } is a trail in a PAG between X and Y
such that some orientation of ◦’s on edges between consecutive nodes in the trail  to either − or ◮ 
makes the trail active given Z.

3 Integration of Overlapping Networks (ION) algorithm

The ION algorithm uses conditional independence information to discover the complete set of PAGs
over a set of variables V that are consistent with a set of datasets over subsets of V which have
overlapping variables. ION accepts as input a set of PAGs which correspond to each of such datasets.
A standard causal discovery algorithm that checks for latent common causes  such as FCI [1] or GES
[3] with latent variable postprocessing steps1  must ﬁrst be applied to each of the original datasets
to learn these PAGs that will be input to ION. Expert domain knowledge can also be encoded in the
input PAGs  if available. The ION algorithm is shown as algorithm 1 and described below.

: PAGs Gi ∈ G with nodes Vi ⊆ V for i = 1  . . .   k
Input
Output: PAGs Hi ∈ H with nodes Vi = V for i = 1  . . .   m
K ← the complete graph over V with ◦’s at every endpoint
A ← ∅
Transfer nonadjacencies and endpoint orientations from each Gi ∈ G to K and propagate the
changes in K using the rules described in [10]
PAT({X  Y}  Z) ← all possibly active trails between X and Y given Z for all {X  Y} ⊆ V and
Z ⊆ V/{X  Y} such that X and Y are d-separated given Z in some Gi ∈ G
PC ← all minimal hitting sets of changes to K  such that all PATi ∈ PAT are not active
for PCi ∈ PC do

Ai ← K after making and propagating the changes PCi
if Ai is consistent with every Gi ∈ G then add Ai to A

end
for Ai ∈ A do

Remove Ai from A
Mark all edges in Ai as ‘?’
For each {X  Y} ⊆ V such that X and Y are adjacent in Ai  if X and Y are d-connected
given ∅ in some Gi ∈ G  then remove ‘?’ from the edge between X and Y in Ai
PR ← every combination of removing or not removing ‘?’ marked edges from Ai
for PRi ∈ PR do

Hi ← Ai after making and propagating the changes PRi
if Hi is consistent with every Gi ∈ G then add Hi to H

end

end

1
2
3

4

5
6
7
8
9
10
11
12
13

14
15
16
17
18
19

Algorithm 1: The Integration of Overlapping Networks (ION) algorithm

The algorithm begins with the complete graph over V with all ◦ endpoints and transfers nonadja-
cencies and endpoint orientations from each Gi ∈ G at line 3  e.g. if X and Y are not adjacent in Gi
then remove the edge between X and Y   if X is directed into Y in Gi then set the endpoint at Y on
the edge between X and Y to ◮. Once these orientations and edge removals are made  the changes
to the complete graph are propagated using the rules in [10]  which provably make every change
that is entailed by the current changes made to the graph. Lines 4-9 ﬁnd every possibly active trail
for every {X  Y } ⊆ V given Z ⊆ V/{X  Y } such that X and Y are d-separated given Z in some
Gi ∈ G. The constructed set PC includes all minimal hitting sets of graphical changes  e.g. unique
sets of minimal changes that are not subsets of other sets of changes  which make these paths no
longer active. For each minimal hitting set  a new graph is constructed by making the changes in
the set and propagating these changes. If the graph is consistent with each Gi ∈ G  e.g. the graph
does not imply a d-separation for some {X  Y } ⊆ V given Z ⊆ V/{X  Y } such that X and Y are
d-connected in some Gi ∈ G  then this graph is added to the current set of possible graphs. Lines 10-

1We use the standard GES algorithm to learn a DAG structure from the data and then use the FCI rules to

check for possible latent common causes.

3

19 attempt to discover any additional PAGs that may be consistent with each Gi ∈ G after deleting
edges from PAGs in the current set and propagating the changes. If some pair of nodes {X  Y } ⊆ V
that are adjacent in a current PAG are d-connected given ∅ in some Gi ∈ G  then we do not consider
sets of edge removals which remove this edge.

The ION algorithm is provably sound in the sense that the output PAGs are consistent with every
Gi ∈ G  e.g. no Hi ∈ H entails a d-separation or d-connection that contradicts a d-separation or
d-connection entailed by some Gi ∈ G. This property follows from the fact that d-separation and
d-connection are mutually exclusive  exhaustive relations.
Theorem 3.1 (soundness). If X and Y are d-separated (d-connected) given Z in some Gi ∈ G  then
X and Y are d-separated (d-connected) given Z in every Hi ∈ H.

Proof Sketch. Every structure Ai constructed at line 7 provably entails every d-separation entailed
by some Gi ∈ G. Such structures are only added to A if they do not entail a d-separation correspond-
ing to a d-connection in some Gi ∈ G. The only changes made (other than changes resulting from
propagating other changes which are provably correct by [10]) in lines 10-19 are edge removals 
which can only create new d-separations. If a new d-separation is created which corresponds to a
d-connection in some Gi ∈ G  then the PAG entailing this new d-separation is not added to H.

The ION algorithm is provably complete in the sense that if there is some structure Hi over the
variables V that is consistent with every Gi ∈ G  then Hi ∈ H.
Theorem 3.2 (completeness). Let Hi be a PAG over the variables V such that for every pair
{X  Y } ⊆ V  if X and Y are d-separated (d-connected) given Z ⊆ V/{X  Y } in some Gi ∈ G  then
X and Y are d-separated (d-connected) given Z in Hi. Then  Hi ∈ H.

Proof Sketch. Every change made at line 3 is provably necessary to ensure soundness. At least
one graph added to A at line 8 provably has every adjacency (possibly more) in Hi and no non-◦
endpoints on an edge found in Hi that is not also present in Hi. Some sequence of edge removals
will provably produce Hi at line 16 and it will be added to the output set since it is consistent with
every Gi ∈ G.

Thus  by theorems 3.1 and 3.2  ION is an asymptotically correct algorithm for learning the complete
set of PAGs over V that are consistent with a set of datasets over subsets of V with overlapping
variables  if the input PAGs are discovered using an asymtotically correct algorithm that detects the
presence of latent common causes  i.e. FCI  with each of these datasets.

Finding all minimal hitting sets is an NP-complete problem [11]. Since learning a DAG structure
from data is also an NP-complete problem [12]  the ION algorithm  as given above  requires a
superexponential (in V) number of operations and is often computationally intractable even for small
sizes of |V|. In practice  however  we can break the minimal hitting set problem into a sequence
of smaller subproblems and use a branch and bound approach that is tractable in many cases and
still results in an asymptotically correct algorithm. We tested several such strategies. The method
which most effectively balanced time and space complexity tradeoffs was to ﬁrst ﬁnd all minimal
hitting sets which make all possibly active trails of length 2 that correspond to d-separations in some
Gi ∈ G not active  then ﬁnd the structures resulting from making and propagating these changes that
are consistent with every Gi ∈ G  and iteratively do the same for each of these structures  increasing
the length of possibly active trails considered until trails of all sizes are considered.

4 Experimental results

We ﬁrst used synthetic data to evaluate the performance of ION with known ground truth. In the ﬁrst
experiment  we generated 100 random 4-node DAGs using the MCMC algorithm described in [13]
with random discrete parameters (conditional probability tables for the factors in the decomposition
shown in section 2). For each DAG  we then randomly chose two subsets of size 2 or 3 of the nodes
in the DAG such that the union of the subsets included all 4 nodes and at least one overlapping
variable between the two subsets was present. We used forward sampling to generate two i.i.d.
samples of sizes N = 50  N = 100  N = 500  N = 1000 and N = 2500 from the DAGs for
only the variables in each subset. We used both FCI and GES with latent variable postprocessing to

4

i

i

s
n
o
s
s
m
o
e
g
d
E

 

s
r
o
r
r
e
 
n
o
i
t
a
t
n
e
i
r

O

5

4

3

2

1

0

 

5

4

3

2

1

0

 

FCI−baseline
ION−FCI
GES−baseline
ION−GES
Structural EM

i

i

s
n
o
s
s
m
m
o
c
 

e
g
d
E

N=50 N=100 N=500 N=1000 N=2500

Sample size

(a)

3

2.5

2

1.5

1

0.5

0

N=50 N=100 N=500 N=1000 N=2500

Sample size
(b)

)
s
d
n
o
c
e
s
(
 

e
m
T

i

14000

12000

10000

8000

6000

4000

2000

0

N=50 N=100 N=500 N=1000 N=2500

Sample size
(d)

N=50 N=100 N=500 N=1000 N=2500

Sample size

(c)

Figure 1: (a) edge omissions  (b) edge commissions  (c) orientation errors  and (d) runtimes

generate PAGs for each of these samples which were input to ION. To evaluate the accuracy of ION 
we counted the number of edge omission  edge commision  and orientation errors (◮ instead of −)
for each PAG in the ION output set and averaged the results. These results were then averaged across
all of the 100 4-node structures. Figure 1 shows the averaged results for these methods along with
3 other methods we included for comparison. ION-FCI and ION-GES refer the the performance of
ION when the input PAGs are obtained using the FCI algorithm and the GES algorithm with latent
variable postprocessing  respectively. For Structural EM  we took each of the datasets over subsets
of the nodes in each DAG and formed a concatenated dataset  as described in section 1  which
was input to the Structural EM algorithm.2 For FCI-baseline and GES-baseline  we used forward
sampling to generate another i.i.d. sample of sizes N = 50  N = 100  N = 500  N = 1000 and
N = 2500 for all of the variables in each DAG and used these datasets as input for the FCI and GES
with latent variable postprocessing algorithms  respectively  to obtain a measure for how well these
algorithms perform when no data is missing. The average runtimes for each method are also reported
in ﬁgure 1. Error bars show 95% conﬁdence intervals. We ﬁrst note the performance of Structural
EM. Almost no edge omission errors are made  but more edge commissions errors are made than any
of the other methods and the edge commission errors do not decrease as the sample size increases.
When we looked at the results  we found that Structural EM always returned either the complete
graph or a graph that was almost complete  indicating that Structural EM is not a reliable method
for causal discovery in this scenario where there is a highly structured pattern to the missing data.
Furthermore  the runtime for Structural EM was considerably higher than any of the other methods.
For the larger sample sizes (where more missing values need to be estimated at each iteration)  a
single run required several hours in some instances. Due to its signiﬁcant computation time  we

2We ran Structural EM with 5 random restarts and chose the model with the highest BDeu score to avoid
converging to local maxima. Random “chains” of nodes were used as the initial models. Structural EM was
never stopped before convergence.

5

i

i

s
n
o
s
s
m
o
 
e
g
d
E

i

s
n
o
s
s
m
o

i

 

e
g
d
E

8
7
6
5
4
3
2
1
0

 

8
7
6
5
4
3
2
1
0

 

 

FCI−baseline
ION−FCI
GES−baseline
ION−GES

i

i

s
n
o
s
s
m
m
o
c
 

e
g
d
E

N=50 N=100 N=500 N=1000N=2500

Sample size
(a)

0.5

0.4

0.3

0.2

0.1

0

N=50 N=100 N=500 N=1000N=2500

Sample size
(b)

s
r
o
r
r
e

 

n
o

i
t

a

t

n
e
i
r

O

6

5

4

3

2

1

0

N=50 N=100 N=500 N=1000N=2500

Sample size
(c)

Figure 2: (a) edge omissions  (b) edge comissions  and (c) orientation errors

 

FCI−baseline
ION−FCI
GES−baseline
ION−GES

i

i

s
n
o
s
s
m
m
o
c
 

e
g
d
E

N=50 N=100 N=500 N=1000N=2500

Sample size
(a)

0.5

0.4

0.3

0.2

0.1

0

s
r
o
r
r
e
n
o
i
t

 

a

t

n
e
i
r

O

N=50 N=100 N=500 N=1000N=2500

Sample size
(b)

6

5

4

3

2

1

0

N=50 N=100 N=500 N=1000N=2500

Sample size
(c)

Figure 3: (a) edge omissions  (b) edge comissions  and (c) orientation errors

were unable to use Structural EM with larger DAG structures so it is excluded in the experiments
below. The FCI-baseline and GES-baseline methods performed similarly to previous simulations
of them. The ION-FCI and ION-GES methods performed similarly to the FCI-baseline and GES-
baseline methods but made slightly more errors and showed slower convergence (due to the missing
data). Very few edge commission errors were made. Slightly more edge omission errors were made 
but these errors decrease as the sample size increases. Some edge orientation errors were made even
for the larger sample sizes. This is due to the fact that each of the algorithms returns an equivalence
class of DAGs rather than a single DAG. Even if the correct equivalence class is discovered  errors
result after comparing the ground truth DAG to every DAG in the equivalence class and averaging.
We also note that there are fewer orientation errors for the GES-baseline and ION-GES methods on
the two smallest sample sizes than all of the other sample sizes. While this may seem surprising  it
is simply a result of the fact that more edge omission errors are made for these cases.

We repeated the above experiment for 3 similar cases where we used 6-node DAG structures rather
than 4-node DAG structures: (i) two i.i.d. samples were generated for random subsets of sizes 2-5
with only 1 variable that is not overlapping between the two subsets; (ii) two i.i.d. samples were
generated for random subsets of sizes 2-5 with only 2 variables that are not overlapping between
the two subsets; (iii) three i.i.d. samples were generated for random subsets of sizes 2-5 with only
1 variable that is not overlapping between any pair of subsets. Figures 2  3  and 4 show edge
omission  edge commission  and orientation errors for each of these cases  respectively. In general 
the performance in each case is similar to the performance for the 4-node case.

We also tested the performance of ION-FCI using a real world dataset measuring IQ and various
neuroanatomical and other traits [14]. We divided the variables into two subsets with overlapping
variables based on domain grounds: (a) variables that might be included in a study on the relationship
between neuroanatomical traits and IQ; and (b) variables for a study on the relationship between IQ 
sex  and genotype  with brain volume and head circumference included as possible confounders.
Figures 5a and 5b show the FCI output PAGs when only the data for each of these subsets of the
variables is provided as input  respectively. Figure 5c shows the output PAG of ION-FCI when these
two resulting PAGs are used as input. We also ran FCI on the complete dataset to have a comparison.
Figure 5d shows this PAG.

6

i

i

s
n
o
s
s
m
o
 
e
g
d
E

8
7
6
5
4
3
2
1
0

 

 

FCI−baseline
ION−FCI
GES−baseline
ION−GES

i

i

s
n
o
s
s
m
m
o
c
 

e
g
d
E

N=50 N=100 N=500 N=1000N=2500

Sample size
(a)

0.5

0.4

0.3

0.2

0.1

0

N=50 N=100 N=500 N=1000N=2500

Sample size
(b)

s
r
o
r
r
e

 

n
o

i
t

a

t

n
e
i
r

O

6

5

4

3

2

1

0

N=50 N=100 N=500 N=1000N=2500

Sample size
(c)

Figure 4: (a) edge omissions  (b) edge comissions  and (c) orientation errors

Corpus Collosum Surface Area

Brain Surface Area

IQ

Body Weight

Head Circumference

(a)

(b)

(c)

(d)

Brain Volume

Genotype

IQ

Body Weight

Head Circumference

Brain Volume

Sex

Corpus Collosum Surface Area

Brain Surface Area

Genotype

IQ

Body Weight

Head Circumference

Brain Volume

Sex

Corpus Collosum Surface Area

Brain Surface Area

Genotype

IQ

Body Weight

Head Circumference

Brain Volume

Sex

Figure 5: (a) FCI output PAG for variables in subset a  (b) FCI output PAG for variables in subset b 
(c) ION output PAG when using the FCI ouput PAGs for variables in subset a and variables in subset
b as input  and (d) FCI output PAG for all variables

In this particular case  the output of ION-FCI consists of only a single PAG  which is identical to
the result when FCI is given the complete dataset as input. This case shows that in some instances 
ION-FCI can recover as much information about the true DAG structure as FCI even when less
information can be extracted from the ION-FCI input. We note that the graphical structure of the
complete PAG (ﬁgures 5c and 5d) is the union of the structures shown in ﬁgures 5a and 5b. While
visually this may appear to be a trivial example for ION where all of the relevant information can be
extracted in the ﬁrst steps  there is in fact much processing required in later stages in the algorithm
to determine the structure around the nonoverlapping variables.

5 Conclusions

In practice  researchers are often unable to ﬁnd or construct a single  complete dataset containing
every variable they may be interested in (or doing so is very costly). We thus need some way
of integrating information about causal relationships that can be discovered from a collection of
datasets with related variables [5]. Standard causal discovery algorithms cannot be used  since they
take only a single dataset as input. To address this open problem  we proposed the ION algorithm 
an asymptotically correct algorithm for discovering the complete set of causal DAG structures that
are consistent with such data.

While the results presented in section 4 indicate that ION is useful in smaller domains when the
branch and bound approach described in section 3 is used  a number of issues must be addressed
before ION or a simlar algorithm is useful for higher dimensional datasets. Probably the most sig-
niﬁcant problem is resolving contradictory information among overlapping variables in different

7

input PAGs  i.e. X is a parent of Y in one PAG and a child of Y in another PAG  resulting from
statistical errors or when the input samples are not identiﬁcally distributed. ION currently ignores
such information rather than attempting to resolve it. This increases uncertainty and thus the size of
the resulting output set of PAGs. Furthermore  simply ignoring such information does not always
avoid conﬂicts. In some of such cases  ION will not discover any PAGs which entail the correct
d-separations and d-connections. Thus  no output PAGs are returned. When performing condi-
tional independence tests or evaluating score functions  statistical errors occur more frequently as
the dimensionality of a dataset increases  unless the sample size also increases at an exponential
rate (resulting from the so-called curse of dimensionality). Thus  until reliable methods for resolv-
ing conﬂicting information from input PAGs are developed  ION and similar algorithms will not
in general be useful for higher dimensional datasets. Furthermore  while the branch and bound
approach described in section 3 is a signiﬁcant improvement over other methods we tested for com-
puting minimal hitting sets  its memory requirements are still considerable in some instances. Other
algorithmic strategies should be explored in future research.

Acknowledgements

We thank Joseph Ramsey  Peter Spirtes  and Jiji Zhang for helpful discussions and pointers. We
thank Frank Wimberley for implementing the version of Structural EM we used. R.E.T. was sup-
ported by the James S. McDonnell Foundation Causal Learning Collaborative Initiative. C.G. was
supported by a grant from the James S. McDonnell Foundation.

References
[1] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. MIT Press  2nd

edition  2000.

[2] J. Pearl. Causality: Models  Reasoning  and Inference. Cambridge University Press  2000.
[3] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine

Learning Research  3:507–554  2002.

[4] D. Danks. Learning the causal structure of overlapping variable sets. In Discovery Science:

Proceedings of the 5th International Conference  2002.

[5] D. Danks. Scientiﬁc coherence and the fusion of experimental results. The British Journal for

the Philosophy of Science  56:791–807  2005.

[6] S. R¨assler. Statistical Matching. Springer  2002.
[7] D. B. Rubin. Multiple Imputation for Nonresponse in Surveys. Wiley & Sons  1987.
[8] N. Friedman. The Bayesian structural EM algorithm. In Proceedings of the 14th Conference

on Uncertainty in Artiﬁcial Intelligence  1998.

[9] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.

Morgan Kauffmann Publishers  1988.

[10] J. Zhang. A characterization of markov equivalence classes for causal models with latent
In Proceedings of the 23rd Conference on Uncertainty in Artiﬁcial Intelligence 

variables.
2007.

[11] R. Greiner  B. A. Smith  and R. W. Wilkerson. A correction to the algorithm in Reiter’s theory

of diagnosis. Artiﬁcial Intelligence  41:79–88  1989.

[12] D. M. Chickering. Learning Bayesian networks is NP-complete. In Proceedings of the 5th

International Workshop on Artiﬁcial Intelligence and Statistics  1995.

[13] G. Melanc¸on  I. Dutour  and M. Bousquet-M´elou. Random generation of dags for graph draw-
ing. Technical Report INS-R0005  Centre for Mathematics and Computer Sciences  Amster-
dam  2000.

[14] M. J. Tramo  W. C. Loftus  R. L Green  T. A. Stukel  J. B. Weaver  and M. S. Gazzaniga. Brain

size  head size  and IQ in monozygotic twins. Neurology  50:1246–1252  1998.

8

,Andreas Veit
Michael Wilber
Serge Belongie