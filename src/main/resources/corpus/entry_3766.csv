2015,Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma,We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs)when the number of observations is much larger than the number of coefficients (n > > p > > 1). In this regime  optimization algorithms can immensely benefit fromapproximate second order information.We propose an alternative way of constructing the curvature information by formulatingit as an estimation problem and applying a Stein-type lemma  which allows further improvements through sub-sampling andeigenvalue thresholding.Our algorithm enjoys fast convergence rates  resembling that of second order methods  with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support.We show that the convergence has two phases  aquadratic phase followed by a linear phase. Finally we empirically demonstrate that our algorithm achieves the highest performancecompared to various algorithms on several datasets.,Newton-Stein Method:

A Second Order Method for GLMs via Stein’s Lemma

Murat A. Erdogdu

Department of Statistics

Stanford University

erdogdu@stanford.edu

Abstract

We consider the problem of efﬁciently computing the maximum likelihood esti-
mator in Generalized Linear Models (GLMs) when the number of observations
is much larger than the number of coefﬁcients (n  p  1). In this regime  op-
timization algorithms can immensely beneﬁt from approximate second order in-
formation. We propose an alternative way of constructing the curvature informa-
tion by formulating it as an estimation problem and applying a Stein-type lemma 
which allows further improvements through sub-sampling and eigenvalue thresh-
olding. Our algorithm enjoys fast convergence rates  resembling that of second
order methods  with modest per-iteration cost. We provide its convergence analy-
sis for the case where the rows of the design matrix are i.i.d. samples with bounded
support. We show that the convergence has two phases  a quadratic phase followed
by a linear phase. Finally  we empirically demonstrate that our algorithm achieves
the highest performance compared to various algorithms on several datasets.

Introduction

1
Generalized Linear Models (GLMs) play a crucial role in numerous statistical and machine learn-
ing problems. GLMs formulate the natural parameter in exponential families as a linear model
and provide a miscellaneous framework for statistical methodology and supervised learning tasks.
Celebrated examples include linear  logistic  multinomial regressions and applications to graphical
models [MN89  KF09].
In this paper  we focus on how to solve the maximum likelihood problem efﬁciently in the GLM
setting when the number of observations n is much larger than the dimension of the coefﬁcient
vector p  i.e.  n  p. GLM optimization task is typically expressed as a minimization problem
where the objective function is the negative log-likelihood that is denoted by `() where  2 Rp is
the coefﬁcient vector. Many optimization algorithms are available for such minimization problems
[Bis95  BV04  Nes04]. However  only a few uses the special structure of GLMs. In this paper  we
consider updates that are speciﬁcally designed for GLMs  which are of the from

   Qr`()  

(1.1)

where  is the step size and Q is a scaling matrix which provides curvature information.
For the updates of the form Eq. (1.1)  the performance of the algorithm is mainly determined by the
scaling matrix Q. Classical Newton’s Method (NM) and Natural Gradient Descent (NG) are recov-
ered by simply taking Q to be the inverse Hessian and the inverse Fisher’s information at the current
iterate  respectively [Ama98  Nes04]. Second order methods may achieve quadratic convergence
rate  yet they suffer from excessive cost of computing the scaling matrix at every iteration. On the
other hand  if we take Q to be the identity matrix  we recover the simple Gradient Descent (GD)
method which has a linear convergence rate. Although GD’s convergence rate is slow compared to
that of second order methods  modest per-iteration cost makes it practical for large-scale problems.
The trade-off between the convergence rate and per-iteration cost has been extensively studied
[BV04  Nes04].
In n  p regime  the main objective is to construct a scaling matrix Q that

1

is computational feasible and provides sufﬁcient curvature information. For this purpose  several
Quasi-Newton methods have been proposed [Bis95  Nes04]. Updates given by Quasi-Newton meth-
ods satisfy an equation which is often referred as the Quasi-Newton relation. A well-known member
of this class of algorithms is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [Nes04].
In this paper  we propose an algorithm that utilizes the structure of GLMs by relying on a Stein-type
lemma [Ste81]. It attains fast convergence rate with low per-iteration cost. We call our algorithm
Newton-Stein Method which we abbreviate as NewSt. Our contributions are summarized as follows:
• We recast the problem of constructing a scaling matrix as an estimation problem and apply
a Stein-type lemma along with sub-sampling to form a computationally feasible Q.
• Newton method’s O(np2 + p3) per-iteration cost is replaced by O(np + p2) per-iteration
cost and a one-time O(n|S|2) cost  where |S| is the sub-sample size.
• Assuming that the rows of the design matrix are i.i.d. and have bounded support  and
denoting the iterates of Newton-Stein method by { ˆt}t0  we prove a bound of the form
(1.2)
where ⇤ is the minimizer and ⌧1 ⌧ 2 are the convergence coefﬁcients. The above bound
implies that the convergence starts with a quadratic phase and transitions into linear later.
• We demonstrate its performance on four datasets by comparing it to several algorithms.

 ˆt+1  ⇤2  ⌧1 ˆt  ⇤2 + ⌧2 ˆt  ⇤2

2 

The rest of the paper is organized as follows: Section 1.1 surveys the related work and Section 1.2
introduces the notations used throughout the paper. Section 2 brieﬂy discusses the GLM framework
and its relevant properties. In Section 3  we introduce Newton-Stein method  develop its intuition 
and discuss the computational aspects. Section 4 covers the theoretical results and in Section 4.3
we discuss how to choose the algorithm parameters. Finally  in Section 5  we provide the empirical
results where we compare the proposed algorithm with several other methods on four datasets.
1.1 Related work
There are numerous optimization techniques that can be used to ﬁnd the maximum likelihood esti-
mator in GLMs. For moderate values of n and p  classical second order methods such as NM  NG
are commonly used. In large-scale problems  data dimensionality is the main factor while choos-
ing the right optimization method. Large-scale optimization tasks have been extensively studied
through online and batch methods. Online methods use a gradient (or sub-gradient) of a single 
randomly selected observation to update the current iterate [Bot10]. Their per-iteration cost is inde-
pendent of n  but the convergence rate might be extremely slow. There are several extensions of the
classical stochastic descent algorithms (SGD)  providing signiﬁcant improvement and/or stability
[Bot10  DHS11  SRB13].
On the other hand  batch algorithms enjoy faster convergence rates  though their per-iteration cost
may be prohibitive. In particular  second order methods attain quadratic rate  but constructing the
Hessian matrix requires excessive computation. Many algorithms aim at forming an approximate 
cost-efﬁcient scaling matrix . This idea lies at the core of Quasi-Newton methods [Bis95].
Another approach to construct an approximate Hessian makes use of sub-sampling techniques
[Mar10  BCNN11  VP12  EM15]. Many contemporary learning methods rely on sub-sampling as
it is simple and it provides signiﬁcant boost over the ﬁrst order methods. Further improvements
through conjugate gradient methods and Krylov sub-spaces are available.
Many hybrid variants of the aforementioned methods are proposed. Examples include the combina-
tions of sub-sampling and Quasi-Newton methods [BHNS14]  SGD and GD [FS12]  NG and NM
[LRF10]  NG and low-rank approximation [LRMB08]. Lastly  algorithms that specialize on cer-
tain types of GLMs include coordinate descent methods for the penalized GLMs [FHT10] and trust
region Newton methods [LWK08].
1.2 Notation
Let [n] = {1  2  ...  n}  and denote the size of a set S by |S|. The gradient and the Hessian of f
with respect to  are denoted by rf and r2
f  respectively. The j-th derivative of a function g
is denoted by g(j). For vector x 2 Rp and matrix X 2 Rp⇥p  kxk2 and kXk2 denote the `2 and
spectral norms  respectively. PC is the Euclidean projection onto set C  and Bp(R) ⇢ Rp is the
ball of radius R. For random variables x  y  d(x  y) and D(x  y) denote probability metrics (to be
explicitly deﬁned later)  measuring the distance between the distributions of x and y.

2

2 Generalized Linear Models
Distribution of a random variable y 2 R belongs to an exponential family with natural parameter ⌘ 2
R if its density can be written of the form f (y|⌘) = exp⌘y  (⌘)h(y)  where  is the cumulant
generating function and h is the carrier density. Let y1  y2  ...  yn be independent observations such
that 8i 2 [n]  yi ⇠ f (yi|⌘i). For ⌘ = (⌘1  ... ⌘ n)  the joint likelihood is

f (y1  y2  ...  yn|⌘) = exp( nXi=1

[yi⌘i  (⌘i)]) nYi=1

h(yi).

We consider the problem of learning the maximum likelihood estimator in the above exponential
family framework  where the vector ⌘ 2 Rn is modeled through the linear relation 

⌘ = X 

for some design matrix X 2 Rn⇥p with rows xi 2 Rp  and a coefﬁcient vector  2 Rp. This formu-
lation is known as Generalized Linear Models (GLMs) in canonical form. The cumulant generating
function  determines the class of GLMs  i.e.  for the ordinary least squares (OLS) (z) = z2 and
for the logistic regression (LR) (z) = log(1 + ez).
Maximum likelihood estimation in the above formulation is equivalent to minimizing the negative
log-likelihood function `() 

`() =

1
n

nXi=1

[(hxi  i)  yihxi  i]  

(2.1)

where hx  i is the inner product between the vectors x and . The relation to OLS and LR can be
seen much easier by plugging in the corresponding (z) in Eq. (2.1). The gradient and the Hessian
of `() can be written as:

1
n

r`() =

nXi=1h(1)(hxi  i)xi  yixii   r2

(2)(hxi  i)xixT
i .
For a sequence of scaling matrices {Qt}t>0 2 Rp⇥p  we consider iterations of the form

nXi=1

`() =

1
n

(2.2)

ˆt+1 ˆt  tQtr`( ˆt) 

where t is the step size. The above iteration is our main focus  but with a new approach on how to
compute the sequence of matrices {Qt}t>0. We formulate the problem of ﬁnding a scalable Qt as
an estimation problem and use a Stein-type lemma that provides a computationally efﬁcient update.
3 Newton-Stein Method
Classical Newton-Raphson update is generally used for training GLMs. However  its per-iteration
cost makes it impractical for large-scale optimization. The main bottleneck is the computation of
the Hessian matrix that requires O(np2) ﬂops which is prohibitive when n  p  1. Numerous
methods have been proposed to achieve NM’s fast convergence rate while keeping the per-iteration
cost manageable.
The task of constructing an approximate Hessian can be viewed as an estimation problem. Assuming
that the rows of X are i.i.d. random vectors  the Hessian of GLMs with cumulant generating function
 has the following form

⇥Qt⇤1 =

1
n

nXi=1

xixT

i (2)(hxi  i) ⇡ E[xxT (2)(hx  i)] .

We observe that [Qt]1 is just a sum of i.i.d. matrices. Hence  the true Hessian is nothing but a sam-
ple mean estimator to its expectation. Another natural estimator would be the sub-sampled Hessian
method suggested by [Mar10  BCNN11  EM15]. Similarly  our goal is to propose an appropriate
estimator that is also computationally efﬁcient.
We use the following Stein-type lemma to derive an efﬁcient estimator to the expectation of Hessian.
Lemma 3.1 (Stein-type lemma). Assume that x ⇠ Np(0  ⌃) and  2 Rp is a constant vector. Then
for any function f : R ! R that is twice “weakly" differentiable  we have
(3.1)

E[xxT f (hx  i)] = E[f (hx  i)]⌃ + E[f (2)(hx  i)]⌃T ⌃ .

3

Algorithm 1 Newton-Stein method

Input: ˆ0  r ✏  .
1. Set t = 0 and sub-sample a set of indices S ⇢ [n] uniformly at random.

ˆµ2( ˆt) = 1

2. Compute: ˆ2 = r+1(b⌃S)  and ⇣r(b⌃S) = ˆ2I + argminrank(M) = rb⌃S  ˆ2I  MF .
3. while ˆt+1  ˆt2  ✏ do
nPn
i=1 (2)(hxi  ˆti) 
ˆµ2( ˆt)h⇣r(b⌃S)1 
ˆt+1 = PBp(R)⇣ ˆt  Qtr`( ˆt)⌘ 
t t + 1.
4. end while
Output: ˆt.

nPn
i=1 (4)(hxi  ˆti) 
ˆµ2( ˆt)/ˆµ4( ˆt)+h⇣r(b⌃S ) ˆt  ˆtii 

ˆµ4( ˆt) = 1

Qt = 1

ˆt[ ˆt]T

The proof of Lemma 3.1 is given in Appendix. The right hand side of Eq.(3.1) is a rank-1 update to
the ﬁrst term. Hence  its inverse can be computed with O(p2) cost. Quantities that change at each
iteration are the ones that depend on   i.e. 

µ2() = E[(2)(hx  i)] and µ4() = E[(4)(hx  i)].

is denoted by b⌃S = Pi2S xixT

µ2() and µ4() are scalar quantities and can be estimated by their corresponding sample means
ˆµ2() and ˆµ4() (explicitly deﬁned at Step 3 of Algorithm 1)  with only O(np) computation.
To complete the estimation task suggested by Eq. (3.1)  we need an estimator for the covariance
matrix ⌃. A natural estimator is the sample mean where  we only use a sub-sample S ⇢ [n] so
that the cost is reduced to O(|S|p2) from O(np2). Sub-sampling based sample mean estimator
i /|S|  which is widely used in large-scale problems [Ver10]. We
highlight the fact that Lemma 3.1 replaces NM’s O(np2) per-iteration cost with a one-time cost of
O(np2). We further use sub-sampling to reduce this one-time cost to O(|S|p2).
In general  important curvature information is contained in the largest few spectral features. Follow-
ing [EM15]  we take the largest r eigenvalues of the sub-sampled covariance estimator  setting rest
of them to (r + 1)-th eigenvalue. This operation helps denoising and would require only O(rp2)
computation. Step 2 of Algorithm 1 performs this procedure.
Inverting the constructed Hessian estimator can make use of the low-rank structure several times.
First  notice that the updates in Eq. (3.1) are based on rank-1 matrix additions. Hence  we can sim-
ply use a matrix inversion formula to derive an explicit equation (See Qt in Step 3 of Algorithm
1). This formulation would impose another inverse operation on the covariance estimator. Since
the covariance estimator is also based on rank-r approximation  one can utilize the low-rank in-
version formula again. We emphasize that this operation is performed once. Therefore  instead of
NM’s per-iteration cost of O(p3) due to inversion  Newton-Stein method (NewSt ) requires O(p2)
per-iteration and a one-time cost of O(rp2). Assuming that NewSt and NM converge in T1 and
T2 iterations respectively  the overall complexity of NewSt is OnpT1 + p2T1 + (|S| + r)p2 ⇡
OnpT1 + p2T1 + |S|p2 whereas that of NM is Onp2T2 + p3T2.

Even though Proposition 3.1 assumes that the covariates are multivariate Gaussian random vectors 
in Section 4  the only assumption we make on the covariates is that they have bounded support 
which covers a wide class of random variables. The left plot of Figure 1 shows that the estimation
is accurate for various distributions. This is a consequence of the fact that the proposed estimator in
Eq. (3.1) relies on the distribution of x only through inner products of the form hx  vi  which in turn
results in approximate normal distribution due to the central limit theorem when p is sufﬁciently
large. We will discuss this phenomenon in detail in Section 4.
The convergence rate of Newton-Stein method has two phases. Convergence starts quadratically and
transitions into a linear rate when it gets close to the true minimizer. The phase transition behavior
can be observed through the right plot in Figure 1. This is a consequence of the bound provided in
Eq. (1.2)  which is the main result of our theorems stated in Section 4.

4

Difference between estimated and true Hessian
Randomness

Bernoulli
Gaussian
Poisson
Uniform

0

)
r
o
r
r
e
 
n
o
i
t
a
m

i
t
s
E
(
0
1
g
o

l

−1

−2

−3

−4

0

100

200

Dimension (p)

300

400

)
r
o
r
r

E
(
0
1
g
o

l

0

−1

−2

−3

Convergence Rate

Sub−sample size
NewSt : S = 1000
NewSt : S = 10000

0

10

20
30
Iterations

40

50

Figure 1: The left plot demonstrates the accuracy of proposed Hessian estimation over different distributions.
Number of observations is set to be n = O(p log(p)). The right plot shows the phase transition in the con-
vergence rate of Newton-Stein method (NewSt). Convergence starts with a quadratic rate and transitions into
linear. Plots are obtained using Covertype dataset.
4 Theoretical results
We start this section by introducing the terms that will appear in the theorems. Then  we provide our
technical results on uniformly bounded covariates. The proofs are provided in Appendix.
4.1 Preliminaries
Hessian estimation described in the previous section relies on a Gaussian approximation. For theo-
retical purposes  we use the following probability metric to quantify the gap between the distribution
of xi’s and that of a normal vector.
Deﬁnition 1. Given a family of functions H  and random vectors x  y 2 Rp  and any h 2H   deﬁne

dH(x  y) = sup
h2H

dh(x  y) where

dh(x  y) =E [h(x)]  E [h(y)].

H3 =nh(x) = hv  xi2(2)(hx  i) :  2 Bp(R) kvk2 = 1o  

Many probability metrics can be expressed as above by choosing a suitable function class H. Exam-
ples include Total Variation (TV)  Kolmogorov and Wasserstein metrics [GS02  CGS10]. Based on
the second and fourth derivatives of cumulant generating function  we deﬁne the following classes:
H1 =nh(x) = (2)(hx  i) :  2 Bp(R)o   H2 =nh(x) = (4)(hx  i) :  2 Bp(R)o  
where Bp(R) 2 Rp is the ball of radius R. Exact calculation of such probability metrics are often
difﬁcult. The general approach is to upper bound the distance by a more intuitive metric. In our
case  we observe that dHj (x  y) for j = 1  2  3  can be easily upper bounded by dTV(x  y) up to a
scaling constant  when the covariates have bounded support.
We will further assume that the covariance matrix follows r-spiked model  i.e.  ⌃ = 2I +
i   which is commonly encountered in practice [BS06]. This simply means that the ﬁrst
r eigenvalues of the covariance matrix are large and the rest are small and equal to each other. Large
eigenvalues of ⌃ correspond to the signal part and small ones (denoted by 2) can be considered as
the noise component.

i=1 ✓iuiuT

Pr

4.2 Composite convergence rate
We have the following per-step bound for the iterates generated by the Newton-Stein method  when
the covariates are supported on a p-dimensional ball.
Theorem 4.1. Assume that the covariates x1  x2  ...  xn are i.i.d. random vectors supported on a
ball of radius pK with

E[xi] = 0

and

E⇥xixT

i⇤ = ⌃ 

where ⌃ follows the r-spiked model. Further assume that the cumulant generating function  has

bounded 2nd-5th derivatives and that R is the radius of the projection PBp(R). For ˆt t>0 given

5

by the Newton-Stein method for  = 1  deﬁne the event

(4.1)
for some positive constant ⇠  and the optimal value ⇤. If n |S| and p are sufﬁciently large  then
there exist constants c  c1  c2 and  depending on the radii K  R  P(E) and the bounds on |(2)| and
|(4)| such that conditioned on the event E  with probability at least 1  c/p2  we have

E =nµ2( ˆt) + µ4( ˆt)h⌃ ˆt  ˆti >⇠   ⇤ 2 Bp(R)o
 ˆt+1  ⇤2  ⌧1 ˆt  ⇤2 + ⌧2 ˆt  ⇤2

where the coefﬁcients ⌧1 and ⌧2 are deterministic constants deﬁned as

(4.2)

2 

⌧1 = D(x  z) + c1r

p

min{p/ log(p)|S|  n/ log(n)}

 ⌧

2 = c2 

and D(x  z) is deﬁned as

D(x  z) = k⌃k2 dH1(x  z) + k⌃k2

2R2 dH2(x  z) + dH3(x  z) 

(4.3)

tolerance ✏ satisfying

for a multivariate Gaussian random variable z with the same mean and covariance as xi’s.
The bound in Eq. (4.2) holds with high probability  and the coefﬁcients ⌧1 and ⌧2 are deterministic
constants which will describe the convergence behavior of the Newton-Stein method. Observe that
the coefﬁcient ⌧1 is sum of two terms: D(x  z) measures how accurate the Hessian estimation is 
and the second term depends on the sub-sample size and the data dimensions.
Theorem 4.1 shows that the convergence of Newton-Stein method can be upper bounded by a com-
positely converging sequence  that is  the squared term will dominate at ﬁrst giving a quadratic
rate  then the convergence will transition into a linear phase as the iterate gets close to the optimal
value. The coefﬁcients ⌧1 and ⌧2 govern the linear and quadratic terms  respectively. The effect of
sub-sampling appears in the coefﬁcient of linear term. In theory  there is a threshold for the sub-
sampling size |S|  namely O(n/ log(n))  beyond which further sub-sampling has no effect. The
transition point between the quadratic and the linear phases is determined by the sub-sampling size
and the properties of the data. The phase transition can be observed through the right plot in Figure
1. Using the above theorem  we state the following corollary.

Corollary 4.2. Assume that the assumptions of Theorem 4.1 hold. For a constant   PE C  a
and for an iterate satisfying E⇥k ˆt  ⇤k2⇤ >✏   the iterates of Newton-Stein method will satisfy 

✏  20Rc/p2 +   

Ehk ˆt+1  ⇤k2i  ˜⌧1Ehk ˆt  ⇤k2i + ⌧2Ehk ˆt  ⇤k2
2i  

where ˜⌧1 = ⌧1 + 0.1 and  ⌧ 1 ⌧ 2 are as in Theorem 4.1.
The bound stated in the above corollary is an analogue of composite convergence (given in Eq. (4.2))
in expectation. Note that our results make strong assumptions on the derivatives of the cumulant gen-
erating function . We emphasize that these assumptions are valid for linear and logistic regressions.
An example that does not ﬁt in our scheme is Poisson regression with (z) = ez. However  we ob-
served empirically that the algorithm still provides signiﬁcant improvement. The following theorem
states a sufﬁcient condition for the convergence of composite sequence.
Theorem 4.3. Let { ˆt}t0 be a compositely converging sequence with convergence coefﬁcients
⌧1 and ⌧2 as in Eq. (4.2) to the minimizer ⇤. Let the starting point satisfy ˆ0  ⇤2 = #<
(1 ⌧1)/⌧2 and deﬁne ⌅= ⇣ ⌧1#
1⌧2#  #⌘. Then the sequence of `2-distances converges to 0. Further 
the number of iterations to reach a tolerance of ✏ can be upper bounded by inf ⇠2⌅ J (⇠)  where

J (⇠) = log2✓ log ( (⌧1/⇠ + ⌧2))
log (⌧1/⇠ + ⌧2) # ◆ +

log(✏/⇠)

log(⌧1 + ⌧2⇠)

.

(4.4)

Above theorem gives an upper bound on the number of iterations until reaching a tolerance of ✏. The
ﬁrst and second terms on the right hand side of Eq. (4.4) stem from the quadratic and linear phases 
respectively.

6

4.3 Algorithm parameters
NewSt takes three input parameters and for those  we suggest near-optimal choices based on our
theoretical results.

• Sub-sample size: NewSt uses a subset of indices to approximate the covariance matrix ⌃.
Corollary 5.50 of [Ver10] proves that a sample size of O(p) is sufﬁcient for sub-gaussian
covariates and that of O(p log(p)) is sufﬁcient for arbitrary distributions supported in some
ball to estimate a covariance matrix by its sample mean estimator. In the regime we con-
sider  n  p  we suggest to use a sample size of |S| = O(p log(p)).
• Rank: Many methods have been suggested to improve the estimation of covariance ma-
trix and almost all of them rely on the concept of shrinkage [CCS10  DGJ13]. Eigenvalue
thresholding can be considered as a shrinkage operation which will retain only the impor-
tant second order information [EM15]. Choosing the rank threshold r can be simply done
on the sample mean estimator of ⌃. After obtaining the sub-sampled estimate of the mean 
one can either plot the spectrum and choose manually or use a technique from [DG13].
• Step size: Step size choices of NewSt are quite similar to Newton’s method (i.e.  See
[BV04]). The main difference comes from the eigenvalue thresholding. If the data follows
the r-spiked model  the optimal step size will be close to 1 if there is no sub-sampling.
However  due to ﬂuctuations resulting from sub-sampling  we suggest the following step
size choice for NewSt:

 =

2

1 + ˆ2O(pp/|S|)

ˆ2

.

(4.5)

In general  this formula yields a step size greater than 1  which is due to rank thresholding 
providing faster convergence. See [EM15] for a detailed discussion.

5 Experiments
In this section  we validate the performance of NewSt through extensive numerical studies. We
experimented on two commonly used GLM optimization problems  namely  Logistic Regression
(LR) and Linear Regression (OLS). LR minimizes Eq. (2.1) for the logistic function (z) = log(1 +
ez)  whereas OLS minimizes the same equation for (z) = z2. In the following  we brieﬂy describe
the algorithms that are used in the experiments:

• Newton’s Method (NM) uses the inverse Hessian evaluated at the current iterate  and may
achieve quadratic convergence. NM steps require O(np2 + p3) computation which makes
it impractical for large-scale datasets.
• Broyden-Fletcher-Goldfarb-Shanno (BFGS) forms a curvature matrix by cultivating the
information from the iterates and the gradients at each iteration. Under certain assumptions 
the convergence rate is locally super-linear and the per-iteration cost is comparable to that
of ﬁrst order methods.
• Limited Memory BFGS (L-BFGS) is similar to BFGS  and uses only the recent few iter-
ates to construct the curvature matrix  gaining signiﬁcant performance in terms of memory.
• Gradient Descent (GD) update is proportional to the negative of the full gradient evaluated
at the current iterate. Under smoothness assumptions  GD achieves a linear convergence
rate  with O(np) per-iteration cost.
• Accelerated Gradient Descent (AGD) is proposed by Nesterov [Nes83]  which improves
over the gradient descent by using a momentum term. Performance of AGD strongly de-
pends of the smoothness of the function.

For all the algorithms  we use a constant step size that provides the fastest convergence. Sub-sample
size  rank and the constant step size for NewSt is selected by following the guidelines in Section 4.3.
We experimented over two real  two synthetic datasets which are summarized in Table 1. Synthetic
data are generated through a multivariate Gaussian distribution and data dimensions are chosen so
that Newton’s method still does well. The experimental results are summarized in Figure 2. We
observe that NewSt provides a signiﬁcant improvement over the classical techniques. The methods
that come closer to NewSt is Newton’s method for moderate n and p and BFGS when n is large.
Observe that the convergence rate of NewSt has a clear phase transition point. As argued earlier 
this point depends on various factors including sub-sampling size |S| and data dimensions n  p  the

7

Dataset:#

−1

)
r
o
r
r

E
(
g
o

l

−2

−3

−4

0

−1

)
r
o
r
r

E
(
g
o

l

−2

−3

S3#

Logistic Regression  rank=3
Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

−1

−2

−3

)
r
o
r
r

E
(
g
o

l

S20#

Logistic Regression  rank=20
Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

CT#Slices#

Logistic Regression  rank=40
Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

0

−1

−2

−3

)
r
o
r
r

E
(
g
o

l

0

−1

−2

−3

)
r
o
r
r

E
(
g
o

l

10
20
Time(sec)

30

−4

0

10
20
Time(sec)

30

−4

0.0

2.5

5.0

Time(sec)

7.5

10.0

Linear Regression  rank=3

Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

−1

−2

−3

)
r
o
r
r

E
(
g
o

l

Linear Regression  rank=20

Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

Linear Regression  rank=40

Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

Covertype#

Logistic Regression  rank=2
Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

10
20
Time(sec)

30

Linear Regression  rank=2

Method
NewSt
BFGS
LBFGS
Newton
GD
AGD

−4

0

−1

)
r
o
r
r

E
(
g
o

l

−2

−3

2
1
0
−1
−2
−3
−4

)
r
o
r
r

E
(
g
o

l

30

−4

0

10
20
Time(sec)

30

−4

0

10
20
Time(sec)

0

1

2

3
Time(sec)

4

5

−4

0

1

2

3
Time(sec)

4

5

Figure 2: Performance of various optimization methods on different datasets. Red straight line represents
the proposed method NewSt. Algorithm parameters including the rank threshold is selected by the guidelines
described in Section 4.3.

rank threshold r and structure of the covariance matrix. The prediction of the phase transition point
is an interesting line of research  which would allow further tuning of algorithm parameters.
The optimal step-size for NewSt will typically be larger than 1 which is mainly due to the eigenvalue
thresholding operation. This feature is desirable if one is able to obtain a large step-size that provides
convergence. In such cases  the convergence is likely to be faster  yet more unstable compared to
the smaller step size choices. We observed that similar to other second order algorithms  NewSt is
susceptible to the step size selection. If the data is not well-conditioned  and the sub-sample size
is not sufﬁciently large  algorithm might have poor performance. This is mainly because the sub-
sampling operation is performed only once at the beginning. Therefore  it might be good in practice
to sub-sample once in every few iterations.

Dataset
CT slices
Covertype
S3
S20

n
53500
581012
500000
500000

p
386
54
300
300

Reference  UCI repo [Lic13]
[GKS+11]
[BD99]
3-spiked model  [DGJ13]
20-spiked model  [DGJ13]

Table 1: Datasets used in the experiments.

6 Discussion
In this paper  we proposed an efﬁcient algorithm for training GLMs. We call our algorithm
Newton-Stein method (NewSt) as it takes a Newton update at each iteration relying on a Stein-type
lemma. The algorithm requires a one time O(|S|p2) cost to estimate the covariance structure and
O(np) per-iteration cost to form the update equations. We observe that the convergence of NewSt
has a phase transition from quadratic rate to linear. This observation is justiﬁed theoretically along
with several other guarantees for covariates with bounded support  such as per-step bounds  condi-
tions for convergence  etc. Parameter selection guidelines of NewSt are based on our theoretical
results. Our experiments show that NewSt provides high performance in GLM optimization.
Relaxing some of the theoretical constraints is an interesting line of research. In particular  bounded
support assumption as well as strong constraints on the cumulant generating functions might be
loosened. Another interesting direction is to determine when the phase transition point occurs 
which would provide a better understanding of the effects of sub-sampling and rank thresholding.

Acknowledgements
The author is grateful to Mohsen Bayati and Andrea Montanari for stimulating conversations on the
topic of this work. The author would like to thank Bhaswar B. Bhattacharya and Qingyuan Zhao for
carefully reading this article and providing valuable feedback.

8

Shun-Ichi Amari  Natural gradient works efﬁciently in learning  Neural computation 10 (1998).

References
[Ama98]
[BCNN11] Richard H Byrd  Gillian M Chin  Will Neveitt  and Jorge Nocedal  On the use of stochastic hessian
information in optimization methods for machine learning  SIAM Journal on Optimization (2011).
Jock A Blackard and Denis J Dean  Comparative accuracies of artiﬁcial neural networks and
discriminant analysis in predicting forest cover types from cartographic variables  Computers and
electronics in agriculture (1999)  131–151.

[BD99]

[BHNS14] Richard H Byrd  SL Hansen  Jorge Nocedal  and Yoram Singer  A stochastic quasi-newton method

[Bis95]
[Bot10]
[BS06]

[BV04]
[CCS10]

[CGS10]

[DE15]

[DG13]

[DGJ13]

[DHS11]

[EM15]

[FHT10]

[FS12]

for large-scale optimization  arXiv preprint arXiv:1401.7020 (2014).
Christopher M. Bishop  Neural networks for pattern recognition  Oxford University Press  1995.
Lèon Bottou  Large-scale machine learning with stochastic gradient descent  COMPSTAT  2010.
Jinho Baik and Jack W Silverstein  Eigenvalues of large sample covariance matrices of spiked
population models  Journal of Multivariate Analysis 97 (2006)  no. 6  1382–1408.
Stephen Boyd and Lieven Vandenberghe  Convex optimization  Cambridge University Press  2004.
Jian-Feng Cai  Emmanuel J Candès  and Zuowei Shen  A singular value thresholding algorithm
for matrix completion  SIAM Journal on Optimization 20 (2010)  no. 4  1956–1982.
Louis HY Chen  Larry Goldstein  and Qi-Man Shao  Normal approximation by Steinâ ˘A ´Zs method 
Springer Science  2010.
Lee H Dicker and Murat A Erdogdu  Flexible results for quadratic forms with applications to
variance components estimation  arXiv preprint arXiv:1509.04388 (2015).
David L Donoho and Matan Gavish  The optimal hard threshold for singular values is 4/sqrt3 
arXiv:1305.5870 (2013).
David L Donoho  Matan Gavish  and Iain M Johnstone  Optimal shrinkage of eigenvalues in the
spiked covariance model  arXiv preprint arXiv:1311.0851 (2013).
John Duchi  Elad Hazan  and Yoram Singer  Adaptive subgradient methods for online learning
and stochastic optimization  J. Mach. Learn. Res. 12 (2011)  2121–2159.
Murat A Erdogdu and Andrea Montanari  Convergence rates of sub-sampled Newton methods 
arXiv preprint arXiv:1508.02810 (2015).
Jerome Friedman  Trevor Hastie  and Rob Tibshirani  Regularization paths for generalized linear
models via coordinate descent  Journal of statistical software 33 (2010)  no. 1  1.
Michael P Friedlander and Mark Schmidt  Hybrid deterministic-stochastic methods for data ﬁtting 
SIAM Journal on Scientiﬁc Computing 34 (2012)  no. 3  A1380–A1405.

[GKS+11] Franz Graf  Hans-Peter Kriegel  Matthias Schubert  Sebastian Pölsterl  and Alexander Cavallaro 
2d image registration in ct images using radial image descriptors  MICCAI 2011  Springer  2011.
Alison L Gibbs and Francis E Su  On choosing and bounding probability metrics  ISR 70 (2002).
Daphne Koller and Nir Friedman  Probabilistic graphical models  MIT press  2009.
M. Lichman  UCI machine learning repository  2013.
Nicolas Le Roux and Andrew W Fitzgibbon  A fast natural newton method  ICML  2010.

[GS02]
[KF09]
[Lic13]
[LRF10]
[LRMB08] Nicolas Le Roux  Pierre-A Manzagol  and Yoshua Bengio  Topmoumoute online natural gradient

algorithm  NIPS  2008.

[LWK08] Chih-J Lin  Ruby C Weng  and Sathiya Keerthi  Trust region newton method for logistic regression 

JMLR (2008).
James Martens  Deep learning via hessian-free optimization  ICML  2010  pp. 735–742.
Peter McCullagh and John A Nelder  Generalized linear models  vol. 2  Chapman and Hall  1989.
Yurii Nesterov  A method for unconstrained convex minimization problem with the rate of conver-
gence o (1/k2)  Doklady AN SSSR  vol. 269  1983  pp. 543–547.

[Nes04]
[SRB13] Mark Schmidt  Nicolas Le Roux  and Francis Bach  Minimizing ﬁnite sums with the stochastic

  Introductory lectures on convex optimization: A basic course  vol. 87  Springer  2004.

average gradient  arXiv preprint arXiv:1309.2388 (2013).
Charles M Stein  Estimation of the mean of a multivariate normal distribution  Annals of Statistics
(1981)  1135–1151.
Roman Vershynin 
arXiv:1011.3027 (2010).
Oriol Vinyals and Daniel Povey  Krylov Subspace Descent for Deep Learning  AISTATS  2012.

Introduction to the non-asymptotic analysis of

random matrices 

[Mar10]
[MN89]
[Nes83]

[Ste81]

[Ver10]

[VP12]

9

,Murat Erdogdu
Lane McIntosh
Niru Maheswaranathan
Aran Nayebi
Surya Ganguli
Stephen Baccus