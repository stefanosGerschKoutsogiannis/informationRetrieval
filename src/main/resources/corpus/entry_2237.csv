2016,General Tensor Spectral Co-clustering for Higher-Order Data,Spectral clustering and co-clustering are well-known techniques in data analysis  and recent work has extended spectral clustering to square  symmetric tensors and hypermatrices derived from a network.  We develop a new tensor spectral co-clustering method that simultaneously clusters the rows  columns  and slices of a nonnegative three-mode tensor and generalizes to tensors with any number of modes.  The algorithm is based on a new random walk model which we call the super-spacey random surfer.  We show that our method out-performs state-of-the-art co-clustering methods on several synthetic datasets with ground truth clusters and then use the algorithm to analyze several real-world datasets.,General Tensor Spectral Co-clustering

for Higher-Order Data

Tao Wu

Purdue University
wu577@purdue.edu

Austin R. Benson
Stanford University

arbenson@stanford.edu

David F. Gleich
Purdue University

dgleich@purdue.edu

Abstract

Spectral clustering and co-clustering are well-known techniques in data analysis 
and recent work has extended spectral clustering to square  symmetric tensors
and hypermatrices derived from a network. We develop a new tensor spectral
co-clustering method that simultaneously clusters the rows  columns  and slices
of a nonnegative three-mode tensor and generalizes to tensors with any number of
modes. The algorithm is based on a new random walk model which we call the
super-spacey random surfer. We show that our method out-performs state-of-the-art
co-clustering methods on several synthetic datasets with ground truth clusters and
then use the algorithm to analyze several real-world datasets.

1

Introduction

Clustering is a fundamental task in machine learning that aims to assign closely related entities to
the same group. Traditional methods optimize some aggregate measure of the strength of pairwise
relationships (e.g.  similarities) between items. Spectral clustering is a particularly powerful technique
for computing the clusters when the pairwise similarities are encoded into the adjacency matrix of a
graph. However  many graph-like datasets are more naturally described by higher-order connections
among several entities. For instance  multilayer or multiplex networks describe the interactions
between several graphs simultaneously with node-node-layer relationships [17]. Nonnegative tensors
are a common representation for many of these higher-order datasets. For instance the i  j  k entry in
a third-order tensor might represent the similarity between items i and j in layer k.
Here we develop the General Tensor Spectral Co-clustering (GTSC) framework for clustering tensor
data. The algorithm takes as input a nonnegative tensor  which may be sparse  non-square  and
asymmetric  and outputs subsets of indices from each dimension (co-clusters). Underlying our
method is a new stochastic process that models higher-order Markov chains  which we call a super-
spacey random walk. This is used to generalize ideas from spectral clustering based on random walks.
We introduce a variant on the well-known conductance measure from spectral graph partitioning [24]
that we call biased conductance and describe how this provides a tensor partition quality metric; this
is akin to Chung’s use of circulations to spectrally-partition directed graphs [7]. Essentially  biased
conductance is the exit probability from a set following our new super-spacey random walk model.
We use experiments on both synthetic and real-world problems to validate the effectiveness of our
method1. For the synthetic experiments  we devise a “planted cluster” model for tensors and show that
GTSC has superior performance compared to other state-of-the-art clustering methods in recovering
the planted clusters. In real-world tensor data experiments  we ﬁnd that our GTSC framework
identiﬁes stop-words and semantically independent sets in n-gram tensors as well as worldwide and
regional airlines and airports in a ﬂight multiplex network.

1Code and data for this paper are available at: https://github.com/wutao27/GtensorSC

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

1.1 Related work

The Tensor Spectral Clustering (TSC) algorithm [4]  another generalization of spectral methods to
higher-order graph data [4]  is closely related. Both the perspective and high-level view are similar 
but the details differ in important ways. For instance  TSC was designed for the case when the
higher-order tensor recorded the occurrences of small subgraph patterns within the network. This
imposes limitations  including how  because the tensor arose based on some underlying graph that
the partitioning metric was designed explicitly for a graph. Thus  the applications are limited in
scope and cannot model  for example  the airplane-airplane-airport multiplex network we analyze
in Section 3.2. Second  for sparse data  the model used by TSC required a correction term with
magnitude proportional to the sparsity in the tensor. In sparse tensors  this makes it difﬁcult to
accurately identify clusters  which we show in Section 3.1.
Most other approaches to tensor clustering proceed by using low-rank factorizations [15  21]. or
a k-means objective [16]. In contrast  our work is based on a stochastic interpretation (escape
probabilities from a set)  in the spirit of random walks in spectral clustering for graphs. There are
also several methods speciﬁc to clustering multiplex networks [27  20] and clustering graphs with
multiple entities [11  2]. Our method handles general tensor data  which includes these types of
datasets as a special case. Hypergraphs clustering [14] can also model the higher-order structures of
the data  and in the case of tensor data  it is approximated by a standard weighted graph.

1.2 Background on spectral clustering of graphs from the perspective of random walks

+

We ﬁrst review graph clustering methods from the view of graph cuts and random walks  and then
review the standard spectral clustering method using sweep cuts. In Section 2  we generalize these
notions to higher-order data in order to develop our GTSC framework.
Let A ∈ Rn×n
be the adjacency matrix of an undirected graph G = (V  E) and let n = |V |
be the number of nodes in the graph. Deﬁne the diagonal matrix of degrees of vertices in V as
D = diag(Ae)  where e is the vector with all ones. The graph Laplacian is L = D − A and
the transition matrix is P = AD−1 = AT D−1. The transition matrix represents the transition
probabilities of a random walk on the graph. If a walker is at node j  it transitions to node i with
probability Pij = Aji/Djj.
Conductance. One of the most widely-used quality metrics for partitioning a graph’s vertices into
two sets S and ¯S = V \S is conductance [24]. Intuitively  conductance measures the ratio of the
number of edges in the graph that go between S and ¯S to the number of edges in S or ¯S. Formally 
we deﬁne conductance as:

where

φ(S) = cut(S)/min(cid:0)vol(S)  vol( ¯S)(cid:1) 
(cid:88)
(cid:88)

and

vol(S) =

Aij.

i∈S j∈V

cut(S) =

Aij

i∈S j∈ ¯S

(1)

(2)

A set S with small conductance is a good partition (S  ¯S). The following well-known observation
relates conductance to random walks on the graph.

Observation 1 ([18]) Let G be undirected  connected  and not bipartite. Start a random walk
(Zt)t∈N where the initial state X0 is randomly chosen following the stationary distribution of the
random walk. Then for any set S ∈ V  

φ(S) = max(cid:8)Pr(Z1 ∈ ¯S | Z0 ∈ S)  Pr(Z1 ∈ S | Z0 ∈ ¯S)(cid:9).

This provides an alternative view of conductance—it measures the probability that one step of a
random walk will traverse between S and ¯S. This random walk view  in concert with the super-spacey
random walk  will serve as the basis for our biased conductance idea to partition tensors in Section 2.4.
Partitioning with a sweep cut. Finding the set of minimum conductance is an NP-hard combinatorial
optimization problem [26]. However  there are real-valued relaxations of the problem that are
tractable to solve and provide a guaranteed approximation [19  9]. The most well known computes
an eigenvector called the Fiedler vector and then uses a sweep cut to identify a partition based on this
eigenvector.

2

The Fiedler eigenvector z solves Lz = λDz where λ is the second smallest generalized eigenvalue.
This can be equivalently formulated in terms of the random walk transition matrix P . Speciﬁcally 

Lz = λDz ⇔ (I − D−1A)z = λz ⇔ zT P = (1 − λ)zT .

The sweep cut procedure to identify a low-conductance set S from z is as follows:

1. Sort the vertices by z as zσ1 ≤ zσ2 ≤ ··· ≤ zσn.
2. Consider the n − 1 candidate sets Sk = {σ1  σ2 ···   σk} for 1 ≤ k ≤ n − 1
φ(Sk) as the solution set.
3. Choose S = argminSk

The solution set S from this algorithm satisﬁes the celebrated Cheeger inequality [19  8]: φ(S) ≤

2(cid:112)φopt  where φopt = minS⊂V φ(S) is the minimum conductance over any set of nodes. Computing

φ(Sk) for all k only takes time linear in the number of edges in the graph because Sk+1 and Sk differ
only in the vertex σk+1.
To summarize  the spectral method requires two components: the second left eigenvector of P and
the conductance criterion. We generalize these ideas to tensors in the following section.

2 A higher-order spectral method for tensor co-clustering

We now generalize the ideas from spectral graph partitioning to nonnegative tensor data. We ﬁrst
review our notation for tensors and then show how tensor data can be interpreted as a higher-order
Markov chain. We brieﬂy review Tensor Spectral Clustering [4] before introducing the new super-
spacey random walk that we use here. This super-spacey random walk will allow us to compute a
vector akin to the Fiedler vector for a tensor and to generalize conductance to tensors. Furthermore 
we generalize the ideas from co-clustering in bipartite graph data [10] to rectangular tensors.

2.1 Preliminaries and tensor notation

We use T to denote a tensor. As a generalization of a matrix  T has m indices (making T an
mth-order or m-mode tensor)  with the (i1  i2 ·  im) entry denoted Ti1 i2 ···  im. We will work with
non-negative tensors where Ti1 i2 ···  im ≥ 0. We call a subset of the tensor entries with all but the
ﬁrst element ﬁxed a column of the tensor. For instance  the j  k column of a three-mode tensor T
is T: j k. A tensor is square if the dimension of all the modes is equal and rectangular if not  and
a square tensor is symmetric if it is equal for any permutation of the indices. For simplicity in the
remainder of our exposition  we will focus on three-mode tensors. However  all of or ideas generalize
to an arbitrary number of modes. (See  e.g.  the work of Gleich et al. [13] and Benson et al. [5] for
representative examples of how these generalizations work.) Finally  we use two operations between
a tensor and a vector. First  a tensor-vector product with a three-mode tensor can output a vector 
which we denote by:

Second  a tensor-vector product can also produce a matrix  which we denote by:

y = T x2 ⇔ yi =(cid:80)
A = T [x] ⇔ Ai j =(cid:80)

j k Ti j kxjxk.

k Ti j kxk.

2.2 Forming higher-order Markov chains from nonnegative tensor data

Recall from Section 1.2 that we can form the transition matrix for a Markov chain from a square
non-negative matrix A by normalizing the columns of the matrix AT . We can generalize this idea
to deﬁne a higher-order Markov chain by normalizing a square tensor. This leads to a probability
transition tensor P :

(3)
i Ti j k > 0. In Section 2.3  we will discuss the sparse case where the column
T: j k may be entirely zero. When that case does not arise  entries of P can be interpreted as the
transition probabilities of a second-order Markov chain (Zt)t∈N:

where we assume(cid:80)

Pi j k = Ti j k/(cid:80)

i Ti j k

Pi j k = Pr(Zt+1 = i | Zt = j  Zt−1 = k).

In other words  If the last two states were j and k  then the next state is i with probability Pi j k.

3

It is possible to turn any higher-order Markov chain into a ﬁrst-order Markov chain on the product
state space of all ordered pairs (i  j). The new Markov chain moves to the state-pair (i  j) from
(j  k) with probability Pi j k. Computing the Fiedler vector associated with this chain would be one
approach to tensor clustering. However  there are two immediate problems. First  the eigenvector
is of size n2  which quickly becomes infeasible to store. Second  the eigenvector gives information
about the product space—not the original state space. (In future work we plan to explore insights
from marginals of this distribution.)
Recent work uses the spacey random walk and spacey random surfer stochastic processes to circum-
vent these issues [5]. The process is non-Markovian and generates a sequence of states Xt as follows.
After arriving at state Xt  the walker promptly “spaces out” and forgets the state Xt−1  yet it still
wants to transition according to the higher-order transitions P . Thus  it invents a state Yt by drawing
a random state from its history and then transitions to state Xt+1 with probability PXt+1 Xt Yt. We
denote Ind{·} as the indicator event and Ht as the history of the process up to time t 2 then

Pr(Yt = j | Ht) = 1

(4)
In this case  we assume that the process has a non-zero probability of picking any state by inﬂating
its history count by 1 visit. The spacey random surfer is a generalization where the walk follows
the above process with probability α and teleports at random following a stochastic vector v with
probability 1 − α. This is akin to how the PageRank random walk includes teleportation.
Limiting stationary distributions are solutions to the multilinear PageRank problem [13]:

t+n

.

αP x2 + (1 − α)v = x 

(5)
and the limiting distribution x represents the stationary distribution of the transition matrix P [x] [5].
The transition matrix P [x] asymptotically approximates the spacey walk or spacey random surfer.
Thus  it is feasible to compute an eigenvector of P [x] matrix and use it with the sweep cut procedure
on a generalized notion of conductance. However  this derivation assumes that all n2 columns of T
were non-zero  which does not occur in real-world datasets. The TSC method adjusted the tensor
T and replaced any columns of all zeros with the uniform distribution vector [4]. Because the
number of zero-columns may be large  this strategy dilutes the information in the eigenvector (see
Appendix D.1). We deal with this issue more generally in the following section  and note that our
new solution outperforms TSC in our experiments (Section 3).

(cid:16)

1 +(cid:80)t

r=1 Ind{Xr = j}(cid:17)

2.3 A stochastic process for sparse tensors

Here we consider another model of the random surfer that avoids the issue of undeﬁned transitions—
which correspond to columns of T that are all zero—entirely. If the surfer attempts to use an
undeﬁned transition  then the surfer moves to a random state drawn from history. Formally  deﬁne
the set of feasible states by

(6)
Here  the set F denotes all the columns in T that are non-zero. The transition probabilities of our
proposed stochastic process are given by

i

F = {(j  k) |(cid:88)

Ti j k > 0}.

Pr(Xt+1 = i | Xt = j  Ht)

= (1 − α)vi + α(cid:80)

Pr(Xt+1 = i | Xt = j  Yt = k  Ht) =

k Pr(Xt+1 = i | Xt = j  Yt = k  Ht)Pr(Yt = k | Ht)

(cid:40)
Ti j k/(cid:80)
n+t (1 +(cid:80)t

1

i Ti j k
r=1 Ind{Xr = i})

(j  k) ∈ F
(j  k) (cid:54)∈ F 

where vi is the teleportation probability. Again Yt is chosen according to Equation (4). We call
this process the super-spacey random surfer because when the transitions are not deﬁned it picks a
random state from history.
This process is a (generalized) vertex-reinforced random walk [3]. Let P be the normalized tensor
i Ti j k only for the columns in F and where all other entries are zero. Stationary

Pi j k = Ti j k/(cid:80)

distributions of the stochastic process must satisfy the following equation:
αP x2 + α(1 − (cid:107)P x2(cid:107)1)x + (1 − α)v = x 

2Formally  this is the σ-algebra generated by the states X1  . . .   Xt.

4

(7)

(8)

(9)

where x is a probability distribution vector (see Appendix A.1 for a proof). At least one solution
vector x must exist  which follows directly from Brouwer’s ﬁxed-point theorem. Here we give a
sufﬁcient condition for it to be unique and easily computable.
Theorem 2.1 If α < 1/(2m − 1) then there is a unique solution x to (9) for the general m-mode
tensor. Furthermore  the iterative ﬁxed point algorithm
k + α(1 − (cid:107)P x2

k(cid:107)1)xk + (1 − α)vk

will converge at least linearly to this solution.
This is a nonlinear setting and tighter convergence results are currently unknown  but these are
unlikely to be tight on real-world data. For our experiments  we found that high values (e.g.  0.95) of
α do not impede convergence. We use α = 0.8 for all our experiments.
In the following section  we show how to form a Markov chain from x and then develop our spectral
clustering technique by operating on the corresponding transition matrix.

xk+1 = αP x2

(10)

2.4 First-order Markov approximations and biased conductance for tensor partitions

From Observation 1 in Section 1.2  we know that conductance may be interpreted as the exit
probability between two sets that form a partition of the nodes in the graph. In this section  we derive
an equivalent ﬁrst-order Markov chain from the stationary distribution of the super-spacey random
surfer. If this Markov chain was guaranteed to be reversible  then we could apply the standard
deﬁnitions of conductance and the Fiedler vector. This will not generally be the case  and so we
introduce a biased conductance measure to partition this non-reversible Markov chain with respect to
starting in the stationary distribution of the super-spacey random walk. We use the second largest 
real-valued eigenvector of the Markov chain as an approximate Fiedler vector. Thus  we can use the
sweep cut procedure described in Section 1.2 to identify the partition.
Forming a ﬁrst-order Markov chain approximation. In the following derivation  we use the
property of the two tensor-vector products that P [x]x = P x2. The stationary distribution x for
the super-spacey random surfer is equivalently the stationary distribution of the Markov chain with
transition matrix
(Here we have used the fact that x ≥ 0 and eT x = 1.) The above transition matrix denotes
transitioning based on a ﬁrst-order Markov chain with probability α  and based on a ﬁxed vector v
with probability 1 − α. We introduce this following ﬁrst-order Markov chain

α(cid:0)P [x] + x(eT − eT P [x])(cid:1) + (1 − α)veT .

˜P = P [x] + x(eT − eT P [x]) 

which represents a useful (but crude) approximation of the higher-order structure in the data. First 
we determine how often we visit states using the super-spacey random surfer to get a vector x.
Then the Markov chain ˜P will tend to have a large probability of spending time in states where the
higher-order information concentrates. This matrix represents a ﬁrst-order Markov chain on which
we can compute an eigenvector and run a sweep cut.
Biased conductance. Consider a random walk (Zt)t∈N. We deﬁne the biased conductance φp(S)
of a set S ⊂ {1  . . .   n} to be

φp(S) = max(cid:8)Pr(Z1 ∈ ¯S | Z0 ∈ S)  Pr(Z1 ∈ S | Z0 ∈ ¯S)(cid:9) 

where Z0 is chosen according to a ﬁxed distribution p. Just as with the standard deﬁnition of
conductance  we can interpret biased conductance as an escape probability. However  the initial state
Z0 is not chosen following the stationary distribution (as in the standard deﬁnition with a reversible
chain) but following p instead. This is why we call it biased conductance. We apply this measure
to ˜P using p = x (the stationary distribution of the super-spacey walk). This choice emphasizes
the higher-order information. Our idea of biased conductance is equivalent to how Chung deﬁnes a
conductance score for a directed graph [7].
We use the eigenvector of ˜P with the second-largest real eigenvalue as an analogue of the Fiedler
vector. If the chain were reversible  this would be exactly the Fiedler vector. When it is not  then
the vector coordinates still encode indications of state clustering [25]; hence  this vector serves as a
principled heuristic. It is important to note that although ˜P is a dense matrix  we can implement the
two operations we need with ˜P in time and space that depends only on the number of non-zeros of
the sparse tensor P using standard iterative methods for eigenvalues of matrices (see Appendix B.1).

5

(cid:20) 0

T =

(cid:21)

 

2.5 Handling rectangular tensor data

So far  we have only considered square  symmetric tensor data. However  tensor data are of-
ten rectangular. This is usually the case when the different modes represent different types of
data. For example  in Section 3.2  we examine a tensor T ∈ Rp×n×n of airline ﬂight data 
where Ti j k represents that there is a ﬂight from airport j to airport k on airline i. Our approach
is to embed the rectangular tensor into a larger square tensor and then symmetrize this tensor 
using approaches developed by Ragnarsson and Van Loan [23]. After the embedding  we can
run our algorithm to simultaneously cluster rows 
columns  and slices of the tensor. This approach is
similar in style to the symmetrization of bipartite
graphs for co-clustering proposed by Dhillon [10].
Let U be an n-by-m-by-(cid:96) rectangular tensor. Then
we embed U into a square three-mode tensor T
with n + m + (cid:96) dimensions and where Ui j k =
Ti j+n k+n+m. This is illustrated in Figure 1 (left).
Then we symmetrize the tensor by using all permu-
tations of the indices Figure 1 (right). When viewed
as a 3-by-3-by-3 block tensor  the tensor is

Figure 1: The tensor is ﬁrst embedded into a
larger square tensor (left) and then this square
tensor is symmetrized (right).

0
0

0
0 U (3 2 1)

0

U (2 3 1)

0

0
0

0 U (1 3 2)
0
U (3 1 2) 0

0
0

0

U (2 1 3)

0

U (1 2 3) 0
0
0

0
0

where U (1 3 2) is a generalized transpose of U with the dimensions permuted.

2.6 Summary of the algorithm

Our GTSC algorithm works by recursively applying the sweep cut procedure  similar to the recursive
bisection procedures for clustering matrix-based data [6]. Formally for each cut  we:
1. Compute the super-spacey stationary vector x (Equation (9)) and form P [x].
2. Compute second largest left  real-valued eigenvector z of ˜P = P [x] + x(eT − eT P [x]).
3. Sort the vertices by the eigenvector z as zσ1 ≤ zσ2 ≤ ··· ≤ zσn.
4. Find the set Sk = {σ1  . . .   σk} for which the biased conductance φx(Sk) on transition

matrix ˜P is minimized.

We continue partitioning as long as the clusters are large enough or we can get good enough splits.
Speciﬁcally  if a cluster has dimension less than a speciﬁed size minimum size  we do not consider
it for splitting. Otherwise  the algorithm recursively splits the cluster if either (1) its dimension is
above some threshold or (2) the biased conductance of a new split is less than a target value φ∗3. The
overall algorithm is summarized in Appendix B as well as the algorithm complexity. Essentially  the
algorithm scales linearly in the number of non-zeros of the tensor for each cluster that is produced.

3 Experiments

We now demonstrate the efﬁcacy of our method by clustering synthetic and real-world data. We ﬁnd
that our method is better at recovering planted cluster structure in synthetically generated tensor data
compared to other state-of-the-art methods. Please refer to Appendix C for the parameter details.

3.1 Synthetic data
We generate tensors with planted clusters and try to recover the clusters. For each dataset  we
generate 20 groups of nodes that will serve as our planted clusters  where the number of nodes in
each group from a truncated normal distribution with mean 20 and variance 5 so that each group
has at least 4 nodes. For each group g we also assign a weight wg where the weight depends on the
2π)−1 exp(−(i − 10.5)2/(2σ2))  where σ varies by
group number. For group i  the weight is (σ
experiment. Non-zeros correspond to interactions between three indices (triples). We generate tw
triples whose indices are within a group and ta triples whose indices span across more than one group.
3We tested φ∗ from 0.3 to 0.4  and we found the value of φ∗ is not very sensitive to the experimental results.

√

6

Table 1: Adjusted Rand Index (ARI)  Normalized Mutual Information (NMI)  and F1 scores on
various clustering methods for recovering synthetically generated tensor data with planted cluster
structure. The ± entries are the standard deviation over 5 trials.
ARI

NMI

F1

F1

ARI
Square tensor with σ = 4

NMI

0.99±0.01
GTSC
0.42± 0.05
TSC
PARAFAC 0.82± 0.05
0.99±0.01
0.48± 0.05

MulDec

SC

0.78±0.13
GTSC
0.41± 0.11
TSC
PARAFAC 0.48± 0.08
0.43± 0.07
0.19± 0.01

MulDec

SC

0.99±0.00
0.60± 0.04
0.94± 0.02
0.99±0.01
0.66± 0.03

0.89±0.06
0.60± 0.09
0.67± 0.04
0.66± 0.04
0.37± 0.01

0.99±0.01
0.45± 0.04
0.83± 0.04
0.99±0.01
0.51± 0.05

0.79±0.12
0.44± 0.10
0.50± 0.07
0.47± 0.06
0.24± 0.01

Square tensor with σ = 2

0.98±0.03
0.53± 0.15
0.90± 0.02
0.94± 0.04
0.39± 0.05

Rectangular tensor with σ = 4
0.97±0.06
0.38± 0.17
0.81± 0.04
0.91± 0.06
0.27± 0.06
Rectangular tensor with σ = 2
0.96±0.06
0.28± 0.08
0.10± 0.04
0.38± 0.07
0.08± 0.01

0.97±0.04
0.44± 0.10
0.24± 0.05
0.52± 0.05
0.19± 0.02

0.97±0.05
0.41± 0.16
0.82± 0.04
0.91± 0.06
0.32± 0.05

0.96±0.06
0.32± 0.08
0.15± 0.04
0.41± 0.07
0.14± 0.01

The tw triples are chosen by ﬁrst uniformly selecting a group g and then uniformly selecting three
indices i  j  and k from group g and ﬁnally assigning a weight of wg. For the ta triples  the sampling
procedure ﬁrst selects an index i from group gi with a probability proportional to the weights of the
group. In other words  indices in group g are chosen proportional to wg. Two indices j and k are
then selected uniformly at random from groups gj and gk other than gi. Finally  the weight in the
tensor is assigned to be the average of the three group weights. For rectangular data  we follow a
similar procedure where we distinguish between the indices for each mode of the tensor.
For our experiments  tw = 10  000 and ta = 1  000  and the variance σ that controls the group
weights is 2 or 4. For each value of σ  we create 5 sample datasets. The value of σ affects the
concentration of the weights and how certain groups of nodes interact with others. This skew reﬂects
properties of the real-world networks we examine in the next section.
Our GTSC method is compared with Tensor Spectral Clustering (TSC) [4]  the Tensor Decomposition
PARAFAC [1]  Spectral Clustering (SC) via Multilinear SVD [12] and Multilinear Decomposition
(MulDec) [21]. Table 1 depicts the performances of the four algorithms in recovering the planted
clusters. In all cases  GTSC has the best performance. We note that the running time is a few seconds
for GTSC  TSC and SC and nearly 30 minutes for PARAFAC and MulDec per trial. Note that the
tensors have roughly 50  000 non-zeros. The poor scalability prohibits the later two methods from
being applied to the real-world tensors in the following section.

3.2 Case study in airline ﬂight networks

We now turn to studying real-world tensor
datasets. We ﬁrst cluster an airline-airport mul-
timodal network which consists of global air
ﬂight routes from 539 airlines and 2  939 air-
ports4. In this application  the entry Ti j k of
the three-mode tensor T is 1 if airline i ﬂies be-
Figure 2: Visualization of the airline-airport data
tween airports j and k and 0 otherwise. Figure 2
tensor. The x and y axes index airports and the
illustrates the connectivity of the tensor with a
z axis indexes airlines. A dot represents that an
random ordering of the indices (left) and the
airline ﬂies between those two airports. On the left 
ordering given by the popularity of co-clusters
indices are sorted randomly. On the right  indices
(right). We can see that after the co-clustering 
are sorted by the co-clusters found by our GTSC
there is clear structure in the data tensor.
framework  which reveals structure in the tensor.
One prominent cluster found by the method corresponds to large international airports in cities such as
Beijing and New York City. This group only accounts for 8.5% of the total number of airports  but it
is responsible for 59% of the total routes. Figure 2 illustrates this result—the airports with the highest

4Data were collected from http://openflights.org/data.html#route.

7

indices are connected to almost every other airport. This cluster is analogous to the “stop word” group
we will see in the n-gram experiments. Most other clusters are organized geographically. Our GTSC
framework ﬁnds large clusters for Europe  the United States  China/Taiwan  Oceania/SouthEast Asia 
and Mexico/Americas. Interestingly  Cancún International Airport is included with the United States
cluster  likely due to large amounts of tourism.

3.3 Case study on n-grams
Next  we study data from n-grams (consective sequences of words in texts). We construct a square
mode-n tensor where indices correspond to words. An entry in the tensor is the number of occurrences
this n-gram. We form tensors from both English and Chinese corpora for n = 3  4.5 The non-zeros
in the tensor consist of the frequencies of the one million most frequent n-grams.
English n-grams. We ﬁnd several conclusions that hold for both tensor datasets. Two large groups in
both datasets consist of stop words  i.e.  frequently occuring connector words. In fact  48% (3-gram)
and 64% (4-gram) of words in one cluster are prepositions (e.g.  in  of  as  to) and link verbs (e.g. 
is  get  does). In the another cluster  64% (3-gram) and 57% (4-gram) of the words are pronouns
(e.g.  we  you  them) and link verbs. This result matches the structure of English language where link
verbs can connect both prepositions and pronouns whereas prepositions and pronouns are unlikely to
appear in close vicinity. Other groups consist of mostly semantically related English words  e.g. 

{cheese  cream  sour  low-fat  frosting  nonfat  fat-free} and
{bag  plastic  garbage  grocery  trash  freezer}.

The clustering of the 4-gram tensor contains some groups that the 3-gram tensor fails to ﬁnd  e.g. 

{german  chancellor  angela  merkel  gerhard  schroeder  helmut  kohl}.

In this case  Angela Merkel  Gerhard Schroeder  and Helmut Kohl have all been German chancellors 
but it requires a 4-gram to make this connection strong. Likewise  some clusters only appear from
clustering the 3-gram tensor. One such cluster is

{church  bishop  catholic  priest  greek  orthodox  methodist  roman  episcopal}.

In 3-grams  we may see phrases such as “catholic church bishop"  but 4-grams containing these words
likely also contain stop words  e.g.  “bishop of the church". However  since stop words already form
their own cluster  this connection is destoryed.
Chinese n-grams. We ﬁnd that many of the conclusions from the English n-gram datasets also hold
for the Chinese n-gram datasets. This includes groups of stop words and semantically related words.
For example  there are two clusters consisting of mostly stop words (200 most frequently occurring
words) from the 3-gram and 4-gram tensors. In the 4-gram data  one cluster of 31 words consists
entirely of stop words and another cluster contains 36 total words  of which 23 are stop words.
There are some words from the two groups that are not typically considered as stop words  e.g. 
社会 society  经济 economy  发展 develop  主义 -ism  国家 nation  政府 government

These words are also among the top 200 most common words according to the corpus. This is a
consequence of the dataset coming from scanned Chinese-language books and is a known issue with
the Google Books corpus [22]. In this case  it is a feature as we are illustrating the efﬁcacy of our
tensor clustering framework rather than making any linguistic claims.

4 CONCLUSION

In this paper we developed the General Tensor Spectral Co-clustering (GTSC) method for co-
clustering the modes of nonnegative tensor data. Our method models higher-order data with a new
stochastic process  the super-spacey random walk  which is a variant of a higher-order Markov
chain. With the stationary distribution of this process  we can form a ﬁrst-order Markov chain which
captures properties of the higher-order data and then use tools from spectral graph partitioning to
ﬁnd co-clusters. In future work  we plan to create tensors that bridge information from multiple
modes. For instance  clusters in the n-gram data depended on n  e.g.  the names of various German
chancellors only appeared as a 4-gram cluster. It would be useful to have a holistic tensor to jointly
partition both 3- and 4-gram information.
Acknowledgements. TW and DFG are supported by NSF IIS-1422918 and DARPA SIMPLEX.
ARB is supported by a Stanford Graduate Fellowship.

5English n-gram data were collected from http://www.ngrams.info/intro.asp and Chinese n-gram

data were collected from https://books.google.com/ngrams.

8

References
[1] B. W. Bader  T. G. Kolda  et al. Matlab tensor toolbox version 2.6. Available online  February 2015.

[2] B.-K. Bao  W. Min  K. Lu  and C. Xu. Social event detection with robust high-order co-clustering. In

ICMR  pages 135–142  2013.

[3] M. Benaïm. Vertex-reinforced random walks and a conjecture of pemantle. Ann. Prob.  25(1):361–392 

1997.

[4] A. R. Benson  D. F. Gleich  and J. Leskovec. Tensor spectral clustering for partitioning higher-order

network structures. In SDM  pages 118–126  2015.

[5] A. R. Benson  D. F. Gleich  and L.-H. Lim. The spacey random walk: a stochastic process for higher-order

data. arXiv  cs.NA:1602.02102  2016.

[6] D. Boley. Principal direction divisive partitioning. Data Mining and Knowledge Discovery  2(4):325–344 

1998.

[7] F. Chung. Laplacians and the Cheeger inequality for directed graphs. Annals of Combinatorics  9(1):1–19 

2005. 10.1007/s00026-005-0237-z.

[8] F. Chung. Four proofs for the Cheeger inequality and graph partition algorithms. In ICCM  2007.
[9] F. R. L. Chung. Spectral Graph Theory. AMS  1992.
[10] I. S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In KDD 

pages 269–274  2001.

[11] B. Gao  T.-Y. Liu  X. Zheng  Q.-S. Cheng  and W.-Y. Ma. Consistent bipartite graph co-partitioning for

star-structured high-order heterogeneous data co-clustering. In KDD  pages 41–50  2005.

[12] D. Ghoshdastidar and A. Dukkipati. Spectral clustering using multilinear svd: Analysis  approximations

and applications. In AAAI  pages 2610–2616  2015.

[13] D. F. Gleich  L.-H. Lim  and Y. Yu. Multilinear PageRank. SIAM J. Matrix Ann. Appl.  36(4):1507–1541 

2015.

[14] M. Hein  S. Setzer  L. Jost  and S. S. Rangapuram. The total variation on hypergraphs-learning on
hypergraphs revisited. In Advances in Neural Information Processing Systems  pages 2427–2435  2013.
[15] H. Huang  C. Ding  D. Luo  and T. Li. Simultaneous tensor subspace selection and clustering: the

equivalence of high order SVD and k-means clustering. In KDD  pages 327–335  2008.

[16] S. Jegelka  S. Sra  and A. Banerjee. Approximation algorithms for tensor clustering. In Algorithmic

learning theory  pages 368–383. Springer  2009.

[17] M. Kivelä  A. Arenas  M. Barthelemy  J. P. Gleeson  Y. Moreno  and M. A. Porter. Multilayer networks.

Journal of Complex Networks  2(3):203–271  2014.

[18] M. Meil˘a and J. Shi. A random walks view of spectral segmentation. In AISTATS  2001.
[19] M. Mihail. Conductance and convergence of markov chains—a combinatorial treatment of expanders. In

FOCS  pages 526–531  1989.

[20] J. Ni  H. Tong  W. Fan  and X. Zhang. Flexible and robust multi-network clustering. In KDD  pages

835–844  2015.

[21] E. E. Papalexakis and N. D. Sidiropoulos. Co-clustering as multilinear decomposition with sparse latent

factors. In ICASSP  pages 2064–2067. IEEE  2011.

[22] E. A. Pechenick  C. M. Danforth  and P. S. Dodds. Characterizing the Google Books corpus: strong limits

to inferences of socio-cultural and linguistic evolution. PloS one  10(10):e0137041  2015.

[23] S. Ragnarsson and C. F. Van Loan. Block tensors and symmetric embeddings. Linear Algebra Appl. 

438(2):853–874  2013.

[24] S. E. Schaeffer. Graph clustering. Computer Science Review  1(1):27–64  2007.
[25] W. J. Stewart. Introduction to the numerical solutions of Markov chains. Princeton Univ. Press  1994.
[26] D. Wagner and F. Wagner. Between min cut and graph bisection. In MFCS  pages 744–750  1993.
[27] D. Zhou and C. J. Burges. Spectral clustering and transductive learning with multiple views. In ICML 

pages 1159–1166  2007.

9

,Tao Wu
Austin Benson
David Gleich