2015,Structured Transforms for Small-Footprint Deep Learning,We consider the task of building compact deep learning pipelines suitable for deploymenton storage and power constrained mobile devices. We propose a uni-fied framework to learn a broad family of structured parameter matrices that arecharacterized by the notion of low displacement rank. Our structured transformsadmit fast function and gradient evaluation  and span a rich range of parametersharing configurations whose statistical modeling capacity can be explicitly tunedalong a continuum from structured to unstructured. Experimental results showthat these transforms can significantly accelerate inference and forward/backwardpasses during training  and offer superior accuracy-compactness-speed tradeoffsin comparison to a number of existing techniques. In keyword spotting applicationsin mobile speech recognition  our methods are much more effective thanstandard linear low-rank bottleneck layers and nearly retain the performance ofstate of the art models  while providing more than 3.5-fold compression.,Structured Transforms for

Small-Footprint Deep Learning

Vikas Sindhwani

Tara N. Sainath
Google  New York

{sindhwani  tsainath  sanjivk}@google.com

Sanjiv Kumar

Abstract

We consider the task of building compact deep learning pipelines suitable for de-
ployment on storage and power constrained mobile devices. We propose a uni-
ﬁed framework to learn a broad family of structured parameter matrices that are
characterized by the notion of low displacement rank. Our structured transforms
admit fast function and gradient evaluation  and span a rich range of parameter
sharing conﬁgurations whose statistical modeling capacity can be explicitly tuned
along a continuum from structured to unstructured. Experimental results show
that these transforms can signiﬁcantly accelerate inference and forward/backward
passes during training  and offer superior accuracy-compactness-speed tradeoffs
in comparison to a number of existing techniques. In keyword spotting applica-
tions in mobile speech recognition  our methods are much more effective than
standard linear low-rank bottleneck layers and nearly retain the performance of
state of the art models  while providing more than 3.5-fold compression.

1

Introduction

Non-linear vector-valued transforms of the form  f (x  M) = s(Mx)  where s is an elementwise
nonlinearity  x is an input vector  and M is an m × n matrix of parameters are building blocks
of complex deep learning pipelines and non-parametric function estimators arising in randomized
kernel methods [20]. When M is a large general dense matrix  the cost of storing mn parameters
and computing matrix-vector products in O(mn) time can make it prohibitive to deploy such models
on lightweight mobile devices and wearables where battery life is precious and storage is limited.
This is particularly relevant for “always-on” mobile applications  such as continuously looking for
speciﬁc keywords spoken by the user or processing a live video stream onboard a mobile robot. In
such settings  the models may need to be hosted on specialized low-power digital signal processing
components which are even more resource constrained than the device CPU.
A parsimonious structure typically imposed on parameter matrices is that of low-rankness [22]. If
M is a rank r matrix  with r (cid:28) min(m  n)  then it has a (non-unique) product representation of the
form M = GHT where G  H have only r columns. Clearly  this representation reduces the storage
requirements to (mr + nr) parameters  and accelerates the matrix-vector multiplication time to
O(mr+nr) via Mx = G(HT x). Another popular structure is that of sparsity [6] typically imposed
during optimization via zero-inducing l0 or l1 regularizers. Other techniques include freezing M
to be a random matrix as motivated via approximations to kernel functions [20]  storing M in low
ﬁxed-precision formats [7  24]  using speciﬁc parameter sharing mechanisms [3]  or training smaller
models on outputs of larger models (“distillation”) [11].
Structured Matrices: An m × n matrix which can be described in much fewer than mn param-
eters is referred to as a structured matrix. Typically  the structure should not only reduce memory

1

requirements  but also dramatically accelerate inference and training via fast matrix-vector products
and gradient computations. Below are classes of structured matrices arising pervasively in many
contexts [18] with different types of parameter sharing (indicated by the color).



(i) Toeplitz

t0

t1

...
tn−1

t−1

t0

...

. . .

. . .

. . .

...

t1

t−(n−1)

...

t−1
t0





1
1

...

1

v0
v1

(ii) Vandermonde
vn−1
vn−1
...
vn−1
n−1

...
vn−1

. . .
. . .

. . .

0

1

...





(iii) Cauchy

1

u0−v0

. . .

. . .

1

u1−v0

...

. . .

. . .

...

...

1

u0−vn−1

...
...

1

un−1−v0

. . .

. . .

1

un−1−vn−1



n

Toeplitz matrices have constant values along each of their diagonals. When the same property holds
for anti-diagonals  the resulting class of matrices are called Hankel matrices. Toeplitz and Hankel
matrices are intimately related to one-dimensional discrete convolutions [10]  and arise naturally
in time series analysis and dynamical systems. A Vandermonde matrix is determined by taking
elementwise powers of its second column. A very important special case is the complex matrix
associated with the Discrete Fourier transform (DFT) which has Vandermonde structure with vj =
n  j = 1 . . . n where ωn = exp −2πi
is the primitive nth root of unity. Similarly  the entries of
ωj
n × n Cauchy matrices are completely deﬁned by two length n vectors. Vandermonde and Cauchy
matrices arise naturally in polynomial and rational interpolation problems.
“Superfast” Numerical Linear Algebra: The structure in these matrices can be exploited for faster
linear algebraic operations such as matrix-vector multiplication  inversion and factorization. In par-
ticular  the matrix-vector product can be computed in time O(n log n) for Toeplitz and Hankel ma-
trices  and in time O(n log2 n) for Vandermonde and Cauchy matrices.
Displacement Operators: At ﬁrst glance  these matrices appear to have very different kinds of
parameter sharing and consequently very different algorithms to support fast linear algebra. It turns
out  however  that each structured matrix class described above  can be associated with a speciﬁc
displacement operator  L : Rm×n (cid:55)→ Rm×n which transforms each matrix  say M  in that class into
an m × n matrix L[M] that has very low-rank  i.e. rank(L[M]) (cid:28) min(m  n). This displacement
rank approach  which can be traced back to a seminal 1979 paper [13]  greatly uniﬁes algorithm
design and complexity analysis for structured matrices [13]  [18]  [14].
Generalizations of Structured Matrices: Consider deriving a matrix by taking arbitrary linear
combinations of products of structured matrices and their inverses  e.g. α1T1T−1
4 T5
where each Ti is a Toeplitz matrix. The parameter sharing structure in such a derived matrix is by
no means apparent anymore. Yet  it turns out that the associated displacement operator remarkably
continues to expose the underlying parsimony structure  i.e. such derived matrices are still mapped to
relatively low-rank matrices! The displacement rank approach allows fast linear algebra algorithms
to be seamlessly extended to these broader classes of matrices. The displacement rank parameter
controls the degree of structure in these generalized matrices.
Technical Preview  Contributions and Outline: We propose building deep learning pipelines
where parameter matrices belong to the class of generalized structured matrices characterized by
low displacement rank. In Section 2  we attempt to give a self-contained overview of the displace-
ment rank approach [13]  [18] drawing key results from the relevant literature on structured matrix
computations (proved in our supplementary material [1] for completeness). In Section 3  we show
that the proposed structured transforms for deep learning admit fast matrix multiplication and gra-
dient computations  and have rich statistical modeling capacity that can be explicitly controlled by
the displacement rank hyperparameter  covering  along a continuum  an entire spectrum of con-
ﬁgurations from highly structured to unstructured matrices. While our focus in this paper is on
Toeplitz-related transforms  our proposal extends to other structured matrix generalizations. In Sec-
tion 4  we study inference and training-time acceleration with structured transforms as a function of
displacement rank and dimensionality. We ﬁnd that our approach compares highly favorably with
numerous other techniques for learning size-constrained models on several benchmark datasets. Fi-
nally  we demonstrate our approach on mobile speech recognition applications where we are able to
match the performance of much bigger state of the art models with a fraction of parameters.
Notation: Let e1 . . . en denote the canonical basis elements of Rn (viewed as column vectors).
In  0n denote n × n identity and zero matrices respectively. Jn = [en . . . e1] is the anti-identity
reﬂection matrix whose action on a vector is to reverse its entries. When the dimension is obvious

2 + α2T3T−1

2

we may drop the subscript; for rectangular matrices  we may specify both the dimensions explicitly 
e.g. we use 01×n for a zero-valued row-vector  and 1n for all ones column vector of length n. u ◦ v
denotes Hadamard (elementwise) product between two vectors v  u. For a complex vector u  ¯u will
denote the vector of complex conjugate of its entries. The Discrete Fourier Transform (DFT) matrix
will be denoted by Ω (or Ωn); we will also use ﬀt(x) to denote Ωx  and iﬀt(x) to denote Ω−1x.
For a vector v  diag(v) denotes a diagonal matrix given by diag(v)ii = vi.

2 Displacement Operators associated with Structured Matrices

We begin by providing a brisk background on the displacement rank approach. Unless otherwise
speciﬁed  for notational convenience we will henceforth assume squared transforms  i.e.  m = n 
and discuss rectangular transforms later. Proofs of various assertions can be found in our self-
contained supplementary material [1] or in [18  19].
The Sylvester displacement operator  denoted as L = ∇A B : Rn×n (cid:55)→ Rn×n is deﬁned by 

(1)
where A ∈ Rn×n  B ∈ Rn×n are ﬁxed matrices referred to as operator matrices. Closely related is
the Stein displacement operator  denoted as L = (cid:52)A B : Rn×n (cid:55)→ Rn×n  and deﬁned by 

∇A B[M] = AM − MB

(cid:52)A B[M] = M − AMB

(2)

By carefully choosing A and B one can instantiate Sylvester and Stein displacement operators with
desirable properties. In particular  for several important classes of displacement operators  A and/or
B are chosen to be an f-unit-circulant matrix deﬁned as follows.
Deﬁnition 2.1 (f-unit-Circulant Matrix). For a real-valued scalar f  the (n× n) f-circulant matrix 
denoted by Zf   is deﬁned as follows 

 0

1
...
0

0
0
...
. . .

. . .
. . .
...
1

f
0
...
0

 =

(cid:20) 01×(n−1)

In−1

(cid:21)

f

0(n−1)×1

Zf = [e2  e3 . . . en  f e1] =

The f-unit-circulant matrix is associated with a basic downward shift-and-scale transformation  i.e. 
the matrix-vector product Zf v shifts the elements of the column vector v “downwards”  and scales
and brings the last element vn to the “top”  resulting in [f vn  v1  . . . vn−1]T . It has several basic
algebraic properties (see Proposition 1.1 [1]) that are crucial for the results stated in this section
Figure 1 lists the rank of the Sylvester displacement operator in Eqn 1 when applied to matrices
belonging to various structured matrix classes  where the operator matrices A  B in Eqn. 1 are
chosen to be diagonal and/or f-unit-circulant. It can be seen that despite the difference in their
structures  all these classes are characterized by very low displacement rank. Figure 2 shows how
this low-rank transformation happens in the case of a 4 × 4 Toeplitz matrix (also see section 1 
Lemma 1.2 [1]). Embedded in the 4 × 4 Toeplitz matrix T are two copies of a 3 × 3 Toeplitz matrix
shown in black and red boxes. The shift and scale action of Z1 and Z−1 aligns these sub-matrices.
By taking the difference  the Sylvester displacement operator nulliﬁes the aligned submatrix leaving
a rank 2 matrix with non-zero elements only along its ﬁrst row and last column. Note that the
negative sign introduced by TZ−1 term prevents the complete zeroing out of the value of t (marked
by red star) and is hence critical for invertibility of the displacement action.

Figure 2: Displacement Action on Toeplitz Matrix

Figure 1: Below r is rank(∇A B[M])

Structured Matrix M
Toeplitz T  T−1
Hankel H  H−1

Vandermonde V (v)

T + H
V (v)−1
V (v)T
Cauchy C(s  t)
C(s  t)−1

A
Z1
Z1

Z0 + ZT
0
diag(v)

Z0
ZT
0

diag(s)
diag(t)

B
Z−1
ZT
0

Z0 + ZT
0

Z0

diag(v)
diag(v)
diag(t)
diag(s)

r
≤ 2
≤ 2
≤ 4
≤ 1
≤ 1
≤ 1
≤ 1
≤ 1





T

t u v w
x t u v
y x t u
z y x t

—




do w nshift



Z1T
z y x
t
t u v w
x t u v
y x t u

3

l
e
ft
s

h

ift

TZ−1
u v w -t
t u v -x
x t u -y
y x t
-z



=





Z1T − TZ−1
* * * *
0 0 0 *
0 0 0 *
0 0 0 *

Each class of structured matrices listed in Figure 1 can be naturally generalized by allowing
the rank of the displacement operator to be higher. Speciﬁcally  given a displacement opera-
tor L  and displacement rank parameter r  one may consider the class of matrices M that satis-
ﬁes rank(L(M)) ≤ r. Clearly then  L[M] = GHT for rank r matrices G  H. We refer to
rank(L(M)) as the displacement rank of M under L  and to the low-rank factors G  H ∈ Rn×r as
the associated low-displacement generators. For the operators listed in Table 1  these broader classes
of structured matrices are correspondingly called Toeplitz-like  Vandermonde-like and Cauchy-like.
Fast numerical linear algebra algorithms extend to such matrices [18].
In order to express structured matrices with low-displacement rank directly as a function of its low-
displacement generators  we need to invert L and obtain a learnable parameterization. For Stein type
displacement operator  the following elegant result is known (see proof in [1]):
Theorem 2.2 ( [19]  Krylov Decomposition). If an n× n matrix M is such that (cid:52)A B[M] = GHT
where G = [g1 . . . gr]  H = [h1 . . . hr] ∈ Rn×r and the operator matrices satisfy: An = aI 
Bn = bI for some scalars a  b  then M can be expressed as:

r(cid:88)

j=1

M =

1

1 − ab

krylov(A  gj)krylov(BT   hj)T

where krylov(A  v) is deﬁned by:

krylov(A  v) = [v Av A2v . . . An−1v]

(3)

(4)

Henceforth  our focus in this paper will be on Toeplitz-like matrices for which the displacement op-
erator of interest (see Table 1) is of Sylvester type: ∇Z1 Z−1. In order to apply Theorem 2.2  one can
switch between Sylvester and Stein operators  setting A = Z1 and B = Z−1 which both satisfy the
conditions of Theorem 2.2 (see property 3  Proposition 1.1 [1]). The resulting expressions involve
Krylov matrices generated by f-unit-circulant matrices which are called f-circulant matrices in the
literature.
Deﬁnition 2.3 (f-circulant matrix). Given a vector v  the f-Circulant matrix  Zf (v)  is deﬁned as
follows:

Zf (v) = krylov(Zf   v) =

 v0

v1
...
vn−1



f vn−1

v0
...
. . .

. . .
. . .
...
v1

f v1
f v2

f vn−1

v0

Two special cases are of interest: f = 1 corresponds to Circulant matrices  and f = −1 corre-
sponds to skew-Circulant matrices.

Finally  one can obtain an explicit parameterization for Toeplitz-like matrices which turns out to
involve taking sums of products of Circulant and skew-Circulant matrices.
If an n × n matrix M satisﬁes ∇Z1 Z−1 [M] = GHT where G =
Theorem 2.4 ([18]).
[g1 . . . gr]  H = [h1 . . . hr] ∈ Rn×r  then M can be written as:

Z1(gj)Z−1(Jhj)

(5)

r(cid:88)

j=1

M =

1
2

3 Learning Toeplitz-like Structured Transforms

Motivated by Theorem 2.4  we propose learning parameter matrices of the form in Eqn. 5 by opti-
mizing the displacement factors G  H. First  from the properties of displacement operators [18]  it
follows that this class of matrices is very rich from a statistical modeling perspective.
Theorem 3.1 (Richness). The set of all n × n matrices that can be written as 

M(G  H) =

Z1(gi)Z−1(hi)

(6)

r(cid:88)

for some G = [g1 . . . gr]  H = [h1 . . . hr] ∈ Rn×r contains:

i=1

4

• All n × n Circulant and Skew-Circulant matrices for r ≥ 1.
• All n × n Toeplitz matrices for r ≥ 2.
• Inverses of Toeplitz matrices for r ≥ 2.
• All products of the form A1 . . . At for r ≥ 2t.
• All n × n matrices for r = n.

• All linear combinations of the form(cid:80)p

i=1 βiA(i)

1 . . . A(i)

t where r ≥ 2tp.

where each Ai above is a Toeplitz matrix or the inverse of a Toeplitz matrix.

When we learn a parameter matrix structured as Eqn. 6 with displacement rank equal to 1 or 2 
we also search over convolutional transforms. In this sense  structured transforms with higher dis-
placement rank generalize (one-dimensional) convolutional layers. The displacement rank provides
a knob on modeling capacity: low displacement matrices are highly structured and compact  while
high displacement matrices start to contain increasingly unstructured dense matrices.
Next  we show that associated structured transforms of the form f (x) = M(G  H)x admit fast
evaluation  and gradient computations with respect to G  H. First we recall the following well-
known result concerning the diagonalization of f-Circulant matrices.
Theorem 3.2 (Diagonalization of f-circulant matrices  Theorem 2.6.4 [18]). For any f (cid:54)= 0  let
f = [1  f 1

n ]T ∈ Cn  and Df = diag(f ). Then 

n   . . . f

n   f 2

n−1

Zf (v) = D−1

f Ω−1 diag(Ω(f ◦ v))ΩDf

(7)
This result implies that for the special cases of f = 1 and f = −1 corresponding to Circu-
lant and Skew-circulant matrices respectively  the matrix-vector multiplication can be computed
in O(n log n) time via the Fast Fourier transform:

(cid:16)

y = Z1(v)x = iﬀt (ﬀt(v) ◦ ﬀt(x))
ﬀt(v) ◦ ﬀt(x)
y = Z1(v)T x = iﬀt
y = Z−1(v)x = ¯η ◦ iﬀt (ﬀt(η ◦ v) ◦ ﬀt(η ◦ x))
y = Z−1(v)T x = ¯η ◦ iﬀt (ﬀt(η ◦ v) ◦ ﬀt(η ◦ x))

(cid:17)

(8)
(9)
(10)
(11)

(cid:33)

r(cid:88)

(cid:32) r(cid:88)

where η = [1  η  η2 . . . ηn−1]T where η = (−1) 1
In particular  a single matrix-vector product for Circulant and Skew-circulant matrices has the com-
putational cost of 3 FFTs. Therefore  for matrices of the form in Eqn. 6 comprising of r products
of Circulant and Skew-Circulant matrices  naively computing a matrix-vector product for a batch
of b input vectors would take 6rb FFTs. However  this cost can be signiﬁcantly lowered to that of
2(rb + r + b) FFTs by making the following observation:

n )  the root of negative unity.

n = exp(i π

Y =

Z1(gi)Z−1(hi)X = Ω−1

diag(Ωgi) Ω diag( ¯η) Ω−1 diag(Ω(η ◦ hi)) ˜X

i=1

i=1

algorithm.

where ˜X = Ω diag(η) X. Here  (1) The FFT of the parameters  Ωgi and Ω(η ◦ hi) is computed
once and shared across multiple input vectors in the minibatch  (2) The (scaled) FFT of the input 
(Ω diag(η) X) is computed once and shared across the sum in Eqn. 6  and (3) The ﬁnal inverse FFT
is also shared. Thus  the following result is immediate.
Theorem 3.3 (Fast Multiplication). Given an n × b matrix X  the matrix-matrix product  Y =
i=1 Z1(gi)Z−1(hi)) X  can be computed at the cost of 2(rb + b + r) FFTs  using the following

((cid:80)r
(cid:17) Set η = [1  η  η2 . . . ηn−1]T where η = (−1) 1
(cid:17) Initialize Y = 0n×b
(cid:17) Set ˜X = ﬀt(diag(η)X)
(cid:17) Set ˜G = ﬀt(G) = [˜g1 . . . ˜gr] and ˜H = ﬀt(diag(η)H) = [˜h1 . . . ˜hr]
(cid:17) for i = 1 to r
◦ U = Z−1(hi)X = diag( ¯η)iﬀt
◦ V = diag(˜gi) ﬀt(U)

n = exp(i π
n )

diag(˜hi) ˜X

(cid:17)

(cid:16)

5

◦ Y = Y + V
(cid:17) Set Y = iﬀt (Y)
(cid:17) Return Y

We now show that when our structured transforms are embedded in a deep learning pipeline  the gra-
dient computation can also be accelerated. First  we note that the Jacobian structure of f-Circulant
matrices has the following pleasing form.
Proposition 3.4 (Jacobian of f-circulant transforms). The Jacobian of the map f (x  v) = Zf (v)x
with respect to the parameters v is Zf (x).

This leads to the following expressions for the Jacobians of the structured transforms of interest.
Proposition 3.5 (Jacobians with respect to displacement generators G  H). Consider parameterized
vector-valued transforms of the form 

r(cid:88)

i=1

f (x  G  H) =

Z1(gi)Z−1(hi)x

(12)

(13)
(14)

The Jacobians of f with respect to the jth column of G  H  i.e. gj  hj  at x  are as follows:

Jgj f|x = Z1 (Z−1(hj)x)
Jhj f|x = Z1(gj)Z−1(x)

i=1[Jhj f|xi ]T δi where  {xi}b

Based on Eqns. 13  14 the gradient over a minibatch of size b requires computing (cid:80)b
and(cid:80)b

i [Jgj f|xi ]T δi
i=1 are batches of forward and backward inputs
during backpropagation. These can be naively computed with 6rb FFTs. However  as before  by
sharing FFT of the forward and backward inputs  and the fft of the parameters  this can be lowered
to (4br + 4r + 2b) FFTs. Below we give matricized implementation.
Proposition 3.6 (Fast Gradients). Let X  Z be n × b matrices whose columns are forward and
backward inputs respectively of minibatch size b during backpropagation. The gradient with respect
to gj  hj can be computed at the cost of (4br + 4r + 2b) FFTs as follows:

i=1 and {δi}b

(cid:16)

(cid:20)(cid:18)

(cid:17) Compute ˜Z = ﬀt(Z)  ˜X = ﬀt(diag(η)X)  ˜G = ﬀt(G)  ˜H = ﬀt(diag(η)H)
(cid:17) Gradient wrt gj (2b + 1 FFTs)
◦ return iﬀt
(cid:104)(cid:16) ˜X ◦ ﬀt
(cid:17) Gradient wrt hj (2b + 1 FFTs)
◦ return diag ( ¯η) iﬀt

(cid:17)(cid:17) ◦ ˜Z
(cid:16)

diag(˜hj) ˜X

(cid:16)
(cid:16)

diag(η)iﬀt

ﬀt

diag(¯η)iﬀt

(cid:21)
(cid:17)(cid:17)(cid:17)

diag(˜gi)˜Z

1b

(cid:19)

1b

(cid:105)

Rectangular Transforms: Variants of Theorems 2.2  2.4 exist for rectangular transforms  see [19].
Alternatively  for m < n we can subsample the outputs of square n × n transforms at the cost of
n output vectors
extra computations  while for m > n  assuming m is a multiple of n  we can stack m
of square n × n transforms.

4 Empirical Studies

Acceleration with Structured Transforms: In Figure 3  we analyze the speedup obtained in prac-
tice using n × n Circulant and Toeplitz-like matrices relative to a dense unstructured n × n matrix
(fully connected layer) as a function of displacement rank and dimension n. Three scenarios are
considered: inference speed per test instance  training speed as implicitly dictated by forward passes
on a minibatch  and gradient computations on a minibatch. Factors such as differences in cache
optimization  SIMD vectorization and multithreading between Level-2 BLAS (matrix-vector multi-
plication)  Level-3 BLAS (matrix-matrix multiplication) and FFT implementations (we use FFTW:
http://www.fftw.org) inﬂuence the speedup observed in practice. Speedup gains start to
show for dimensions as small as 512 for Circulant matrices. The gains become dramatic with accel-
eration of the order of 10 to 100 times for several thousand dimensions  even for higher displacement
rank Toeplitz-like transforms.

6

Figure 3: Acceleration with n × n Structured Transforms (6-core 32-GB Intel(R) Xeon(R) machine; random
datasets). In the plot  displacement rank = 0 corresponds to a Circulant Transform.

Effectiveness for learning compact Neural Networks: Next  we compare the proposed structured
transforms with several existing techniques for learning compact feedforward neural networks. We
exactly replicate the experimental setting from the recent paper on HASHEDNETS [3] which uses
several image classiﬁcation datasets ﬁrst prepared by [15]. MNIST is the original 10-class MNIST
digit classiﬁcation dataset with 60000 training examples and 10000 test examples. BG-IMG-ROT
refers to a challenging version of MNIST where digits are randomly rotated and placed against a
random black and white background. RECT (1200 training images  50000 test images) and CONVEX
(8000 training images  50000 test images) are 2-class binary image datasets where the task is to
distinguish between tall and wide rectangles  and whether the “on” pixels form a convex region or
not  respectively. In all datasets  input images are of size 28 × 28. Several existing techniques are
benchmarked in [3] for compressing a reference single hidden layer model with 1000 hidden nodes.
• Random Edge Removal (RER) [5] where a fraction of weights are randomly frozen to be zero-valued.
• Low-rank Decomposition (LRD) [9]
• Neural Network (NN) where the hidden layer size is reduced to satisfy a parameter budget.
• Dark Knowledge (DK) [11]: A small neural network is trained with respect to both the original
• HashedNets (HN) [3]: This approach uses a low-cost hash function to randomly group connection
• HashedNets with Dark Knowledge (HNDK): Trains a HashedNet with respect to both the original

labeled data  as well as soft targets generated by a full uncompressed neural network.

weights which share the same value.

labeled data  as well as soft targets generated by a full uncompressed neural network.

We consider learning models of comparable size with the weights in the hidden layer structured as
a Toeplitz-like matrix. We also compare with the FASTFOOD approach of [25  16] where the weight
matrix is a product of diagonal parameter matrices and ﬁxed permutation and Walsh-Hadamard
matrices  also admitting O(n log n) multiplication and gradient computation time. The CIRCULANT
Neural Network approach proposed in [4] is a special case of our framework (Theorem 3.1).
Results in Table 1 show that Toeplitz-like structured transforms outperform all competing ap-
proaches on all datasets  sometimes by a very signiﬁcant margin  with similar or drastically lesser
number of parameters. It should also be noted that while random weight tying in HASHEDNETS
reduces the number of parameters  the lack of structure in the resulting weight matrix cannot be
exploited for FFT-like O(n log n) multiplication time. We note in passing that for HASHEDNETS
weight matrices whose entries assume only one of B distinct values  the Mailman algorithm [17]
can be used for faster matrix-vector multiplication  with complexity O(n2 log(B)/(log n))  which
still is much slower than matrix-vector multiplication time for Toeplitz-like matrices. Also note that
the distillation ideas of [11] are complementary to our approach and can further improve our results.

MNIST

BG-IMG-ROT

CONVEX

RECT

RER
15.03
12406
73.17
12406
37.22
12281
18.23
12281

LRD
28.99
12406
80.63
12406
39.93
12281
23.67
12281

NN
6.28
12406
79.03
12406
34.37
12281
5.68
12281

DK
6.32
12406
77.40
12406
31.85
12281
5.78
12281

HN
2.79
12406
59.20
12406
31.77
12281
3.67
12281

HNDK
2.65
12406
58.25
12406
30.43
12281
3.37
12281

Fastfood CIRCULANT

TOEPLITZ (1)

TOEPLITZ (2)

6.61
10202
68.4
10202
33.92
3922
21.45
3922

3.12
8634
62.11
8634
24.76
2352
2.91
2352

2.79
9418
57.66
9418
17.43
3138
0.70
3138

2.54
10986
55.21
10986
16.18
4706
0.89
4706

TOEPLITZ (3)

2.09
12554
53.94
12554
20.23
6774
0.66
6774

Table 1: Error rate and number of parameters (italicized). Best results in blue.

7

010203010−1100101102Displacement RankSpeedup (unstructured / structured)Inference 010203010−1100101102Displacement RankForward Pass (minibatch 100) n=512n=1024n=2048n=4096n=8192n=16384n=32768010203010−1100101102Displacement RankGradient (minibatch 100) Mobile Speech Recognition: We now demonstrate the techniques developed in this paper on a
speech recognition application meant for mobile deployment. Speciﬁcally  we consider a keyword
spotting (KWS) task  where a deep neural network is trained to detect a speciﬁc phrase  such as “Ok
Google” [2]. The data used for these experiments consists of 10−15K utterances of selected phrases
(such as “play-music”  “decline-call”)  and a larger set of 396K utterances to serve as negative
training examples. The utterances were randomly split into training  development and evaluation
sets in the ratio of 80 : 5 : 15. We created a noisy evaluation set by artiﬁcially adding babble-type
cafeteria noise at 0dB SNR to the “play-music” clean data set. We will refer to this noisy data
set as CAFE0. We refer the reader to [23] for more details about the datasets. We consider the
task of shrinking a large model for this task whose architecture is as follows [23]: the input layer
consists of 40 dimensional log-mel ﬁlterbanks  stacked with a temporal context of 32  to produce
an input of 32 × 40 whose dimensions are in time and frequency respectively. This input is fed to
a convolutional layer with ﬁlter size 32 × 8  frequency stride 4 and 186 ﬁlters. The output of the
convolutional layer is of size 9 × 186 = 1674. The output of this layer is fed to a 1674 × 1674 fully
connected layer  followed by a softmax layer for predicting 4 classes constituting the phrase “play-
music”. The full training set contains about 90 million samples. We use asynchronous distributed
stochastic gradient descent (SGD) in a parameter server framework [8]  with 25 worker nodes for
optimizing various models. The global learning rate is set to 0.002  while our structured transform
layers use a layer-speciﬁc learning rate of 0.0005; both are decayed by an exponential factor of 0.1.

Figure 4: “play-music” detection performance: (left) End-to-end keyword spotting performance in terms of
false reject (FR) rate per false alarm (FA) rate (lower is better) (right): Classiﬁcation accuracy as a function of
training time. Displacement rank is in parenthesis for Toeplitz-like models.

Results with 11 different models are reported in Figure 4 (left) including the state of the art keyword
spotting model developed in [23]. At an operating point of 1 False Alarm per hour  the follow-
ing observations can be made: With just 3348 parameters  a displacement rank=1 TOEPLITZ-LIKE
structured transform outperforms a standard low-rank bottleneck model with rank=16 containing 16
times more parameters; it also lowers false reject rates from 10.2% with CIRCULANT and 14.2%
with FASTFOOD transforms to about 8.2%. With displacement rank 10  the false reject rate is 6.2% 
in comparison to 6.8% with the 3 times larger rank=32 standard low-rank bottleneck model. Our best
Toeplitz-like model comes within 0.4% of the performance of the 80-times larger fully-connected
and 3.6 times larger reference [23] models. In terms of raw classiﬁcation accuracy as a function of
training time  Figure 4 (right) shows that our models (with displacement ranks 1  2 and 10) come
within 0.2% accuracy of the fully-connected and reference models  and easily provide much bet-
ter accuracy-time tradeoffs in comparison to standard low-rank bottleneck models  Circulant and
Fastfood baselines. The conclusions are similar for other noise conditions (see supplementary ma-
terial [1]).

5 Perspective

We have introduced and shown the effectiveness of new notions of parsimony rooted in the theory of
structured matrices. Our proposal can be extended to various other structured matrix classes  includ-
ing Block and multi-level Toeplitz-like [12] matrices related to multidimensional convolution [21].
We hope that such ideas might lead to new generalizations of Convolutional Neural Networks.
Acknowledgements: We thank Yu-hsin Chen  Carolina Parada  Rohit Prabhavalkar  Alex Gruen-
stein  Rajat Monga  Baris Sumengen  Kilian Weinberger and Wenlin Chen for their contributions.

8

0.511.522.530.050.060.070.080.090.10.110.120.130.14False Alarms per hourFalse Rejectsplay−music:cafe0 fullyconnected (2.8M)reference (122K)lowrank4 (13.4K)lowrank8 (26.8K)lowrank16 (53.6K)lowrank32 (107K)circulant (1674)fastfood (5022)toeplitz−disprank1 (3348)toeplitz−disprank2 (6696)toeplitz−disprank10 (33.5K)51015202530354096.496.696.89797.297.497.697.89898.298.4Time (hours)Accuracy (%)play−music:accuracy fullyconnected (2.8M)reference (122K)lowrank4 (13.4K)lowrank8 (26.8K)lowrank16 (53.6K)lowrank32 (107K)circulant (1674)fastfood (5022)toeplitz−disprank1 (3348)toeplitz−disprank2 (6696)toeplitz−disprank10 (33.5K)References
[1] Supplementary material: Structured transforms for small footprint deep learning.

http://vikas.sindhwani.org/st_supplementary.pdf.

2015.

[2] G. Chen  C. Parada  and G. Heigold. Small-footprint keyword spotting using deep neural

networks. In ICASSP  2014.

[3] W. Chen  J. T. Wilson  S. Tyree  K. Q. Weinberger  and Y. Chen. Compressing neural networks

with the hashing trick. In ICML  2015.

[4] Y. Cheng  F. X. Xu  R. S. Feris  S. Kumar  A. Choudhary  and S.-F. Chang. Fast neural networks

with circulant projections. In arXiv:1502.03436  2015.

[5] D. C. Ciresan  U. Meier  J. Masci  L. M. Gambardella  and Schmidhuber. High-performance

neural networks for visual object classiﬁcation. In arXiv:1102.0183  2011.

[6] M. D. Collins and P. Kohli. Memory-bounded deep convolutional neural networks. In ICASSP 

2013.

[7] M. Courbariaux  J.-P. David  and Y. Bengio. Low-precision storage for deep learning. In ICLR 

2015.

[8] J. Dean  G. S. Corrado  R. Monga  K. Chen  M. Devin  Q. V. Le  M. Z. Mao  M. Ranzato 
A. Senior  P. Tucker  K. Yang    and A. Y. Ng. Large-scale distributed deep networks. In NIPS 
2012.

[9] M. Denil  B. Shakibi  L. Dinh  and N. de Freitas. Predicting parameters in deep learning. In

NIPS  2013.

[10] R. M. Gray. Toeplitz and circulant matrices: A review. Foundations and Trends in Communi-

cations and Information Theory  2005.

[11] G. Hinton  O. Vinyals  and J. Dean. Distilling the knowledge in a neural network. In NIPS

workshop  2014.

[12] T. Kailath and J. Chun. Generalized displacement structure for block toeplitz  toeplitz block

and toeplitz-derived matrices. SIAM J. Matrix Anal. Appl.  15  1994.

[13] T. Kailath  S. Y. Kung  and M. Morf. Displacement ranks of matrices and linear equations.

Journal of Mathematical Analysis and Applications  pages 395–407  1979.

[14] T. Kailath and A. H. Sayed. Displacement structure: Theory and applications. SIAM Review 

37  1995.

[15] H. Larochelle  D. Erhan  A. C. Courville  J. Bergstra  and Y. Bengio. An empirical evaluation

of deep architectures on problems with many factors of variation. In ICML  2007.

[16] Q. Le  T. Sarlos  and A. Smola. Fastfood – approximating kernel expansions in loglinear time.

In ICML  2013.

[17] E. Liberty and S. W. Zucker. The mailman algorithm: a note on matrix vector multiplication.

In Information Processing Letters  2009.

[18] V. Pan. Structured Matrices and Polynomials: Uniﬁed Superfast Algorithms. Springer  2001.
[19] V. Pan. Inversion of displacement operators. SIAM Journal of Matrix Analysis and Applica-

tions  pages 660–677  2003.

[20] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2007.
[21] M. V. Rakhuba and I. V. Oseledets. Fast multidimensional convolution in low-rank tensor

formats via cross approximation. SIAM J. Sci. Comput.  37  2015.

[22] T. Sainath  B. Kingsbury  V. Sindhwani  E. Arisoy  and B. Ramabhadran. Low-rank matrix fac-
torization for deep neural network training with high-dimensional output targets. In ICASSP 
2013.

[23] T. Sainath and C. Parada. Convolutional neural networks for small-footprint keyword spotting.

In Proc. Interspeech  2015.

[24] V. Vanhoucke  A. Senior  and M. Z. Mao. Improving the speed of neural networks on cpus. In

NIPS Workshop on Deep Learning and Unsupervised Feature Learning  2011.

[25] Z. Yang  M. Moczulski  M. Denil  N. de Freitas  A. Smola  L. Song  and Z. Wang. Deep fried

convnets. In arXiv:1412.7149  2015.

9

,Vikas Sindhwani
Tara Sainath
Sanjiv Kumar
Yasin Abbasi Yadkori
Peter Bartlett
Victor Gabillon
Sindy Löwe
Peter O'Connor
Bastiaan Veeling