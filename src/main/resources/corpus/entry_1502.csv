2012,Dimensionality Dependent PAC-Bayes Margin Bound,Margin is one of the most important concepts in machine learning. Previous margin bounds  both for SVM and for boosting  are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or infinite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors fixed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity. In addition  we show that the VC bound for linear classifiers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classifiers.,Dimensionality Dependent PAC-Bayes Margin Bound

Chi Jin

School of Physics
Peking University

Liwei Wang

School of EECS
Peking University

Key Laboratory of Machine Perception  MOE

Key Laboratory of Machine Perception  MOE

chijin06@gmail.com

wanglw@cis.pku.edu.cn

Abstract

Margin is one of the most important concepts in machine learning. Previous mar-
gin bounds  both for SVM and for boosting  are dimensionality independent. A
major advantage of this dimensionality independency is that it can explain the ex-
cellent performance of SVM whose feature spaces are often of high or inﬁnite
dimension. In this paper we address the problem whether such dimensionality in-
dependency is intrinsic for the margin bounds. We prove a dimensionality depen-
dent PAC-Bayes margin bound. The bound is monotone increasing with respect
to the dimension when keeping all other factors ﬁxed. We show that our bound
is strictly sharper than a previously well-known PAC-Bayes margin bound if the
feature space is of ﬁnite dimension; and the two bounds tend to be equivalent as
the dimension goes to inﬁnity. In addition  we show that the VC bound for linear
classiﬁers can be recovered from our bound under mild conditions. We conduct
extensive experiments on benchmark datasets and ﬁnd that the new bound is use-
ful for model selection and is usually signiﬁcantly sharper than the dimensionality
independent PAC-Bayes margin bound as well as the VC bound for linear classi-
ﬁers.

1 Introduction

Linear classiﬁers  including SVM and boosting  play an important role in machine learning. A cen-
tral concept in the generalization analysis of linear classiﬁers is margin. There have been extensive
works on bounding the generalization errors of SVM and boosting in terms of margins (with various
deﬁnitions such l2  l1  soft  hard  average  minimum  etc.)
In 1970’s Vapnik pointed out that large margin can imply good generalization. Using the fat-
shattering dimension  Shawe-Taylor et al. [1] proved a margin bound for linear classiﬁers. This
bound was improved and simpliﬁed in a series of works [2  3  4  5] mainly based on the PAC-Bayes
theory [6] which was developed originally for stochastic classiﬁers. (See Section 2 for a brief review
of the PAC-Bayes theory and the PAC-Bayes margin bounds.) All these bounds state that if a linear
classiﬁer in the feature space induces large margins for most of the training examples  then it has a
small generalization error bound independent of the dimensionality of the feature space.
The (l1) margin has also been extensively studied for boosting to explain its generalization ability.
Schapire et al. [7] proved a margin bound for the generalization error of voting classiﬁers. The bound
is independent of the number of base classiﬁers combined in the voting classiﬁer1. This margin
bound was greatly improved in [8  9] using (local) Rademacher complexities. There also exist
improved margin bounds for boosting from the viewpoint of PAC-Bayes theory [10]  the diversity
of base classiﬁers [11]  and different deﬁnition of margins [12  13].

1The bound depends on the VC dimension of the base hypothesis class. Nevertheless  given the VC dimen-
sion of the base hypothesis space  the bound does not depend on the number of the base classiﬁers  which can
be seen as the dimension of the feature space.

1

The aforementioned margin bounds are all dimensionality independent. That is  the bounds are
solely characterized by the margins on the training data and do not depend on the dimension of
feature space. A major advantage of such dimensionality independent margin bounds is that they
can explain the generalization ability of SVM and boosting whose feature spaces have high or inﬁnite
dimension  in which case the standard VC bound becomes trivial.
Although very successful in bounding the generalization error  a natural question is whether this
dimensionality independency is intrinsic for margin bounds. In this paper we explore this problem.
Building upon the PAC-Bayes theory  we prove a dimensionality dependent margin bound. This
bound is monotone increasing with respect to the dimension when keeping all other factors ﬁxed.
Comparing with the PAC-Bayes margin bound of Langford [4]  the new bound is strictly sharper
when the feature space is of ﬁnite dimension; and the two bounds tend to be equal as the dimension
goes to inﬁnity.
We conduct extensive experiments on benchmark datasets. The experimental results show that the
new bound is signiﬁcantly sharper than the dimensionality independent PAC-Bayes margin bound
as well as the VC bound for linear classiﬁers on relatively large datasets. The bound is also found
useful for model selection.
The rest of this paper is organized as follows. Section 2 contains a brief review of the PAC-Bayes
theory and the dimensionality independent PAC-Bayes margin bound.
In Section 3 we give the
dimensionality dependent PAC-Bayes margin bound and further improvements. We provide the
experimental results in Section 4  and conclude in Section 5. Due to the space limit  all the proofs
are given in the supplementary material.

2 Background
Let X be the instance space or generally the feature space. In this paper we always assume X = Rd.
We consider binary classiﬁcation problems and let Y = {−1; 1}. Examples are drawn independently
according to an underlying distribution D over X × Y. Let PD(A(x; y)) denote the probability of
event A when an example (x; y) is chosen according to D. Let S denote a training set of n i.i.d.
examples. We denote by PS (A(x; y)) the probability of event A when an example (x; y) is chosen
at random from S. Similarly we denote by ED and ES the corresponding expectations. If c is
a classiﬁer  then we denote by erD(c) = PD(y ̸= c(x)) the generalization error of c  and let
erS (c) = PS (y ̸= c(x)) be the empirical error.
An important type of classiﬁers studied in this paper is stochastic classiﬁers. Let C be a set of
classiﬁers  and let Q be a probability distribution of classiﬁers on C. A stochastic classiﬁer deﬁned
by Q randomly selects c ∈ C according to Q. When clear from the context  we often denote by
erD(Q) and erS(Q) the generalization and empirical error of the stochastic classiﬁer Q respectively.
That is 

erD(Q) = Ec∼Q[erD(c)];

erS (Q) = Ec∼Q[erS (c)]

A probability distribution Q of classiﬁers also deﬁnes a deterministic classiﬁer—the voting classiﬁer 
which we denote by vQ. For x ∈ X

vQ(x) = sgn[Ec∼Qc(x)]:

In this paper we always consider homogeneous linear classiﬁers2  or stochastic classiﬁers whose
distribution is over homogeneous linear classiﬁers. Let X = Rd. For any w ∈ Rd  the linear
classiﬁer cw is deﬁned as cw(·) = sgn[< w;· >]. When we consider a probability distribution over
all homogeneous linear classiﬁers cw in Rd  we can equivalently consider a distribution of w ∈ Rd.
The work in this paper is based on the PAC-Bayes theory. PAC-Bayes theory is a beautiful gener-
alization of the classical PAC theory to the setting of Bayes learning. It gives generalization error
bounds for stochastic classiﬁers. The PAC-Bayes theorem was ﬁrst proposed by McAllester [6].
The following elegant version is due to Langford [4].

2This does not sacriﬁce any generality since linear classiﬁers can be easily transformed to homogeneous

linear classiﬁers by adding a new dimension.

2

Theorem 2.1. Let P   Q denote probability distributions of classiﬁers. For any P and any (cid:14) ∈ (0; 1) 
with probability 1 − (cid:14) over the random draw of n training examples

kl (erS(Q) || erD(Q)) ≤ KL(Q||P ) + ln n+1

(cid:14)

(1)
holds simultaneously for all distributions Q. Here KL(Q||P ) is the Kullback-Leibler divergence of
distributions Q and P ; kl(a||b) for a; b ∈ [0; 1] is the Bernoulli KL divergence deﬁned as kl(a||b) =
a log a

b + (1 − a) log 1−a
1−b .

n

The above PAC-Bayes theorem states that if a stochastic classiﬁer  whose distribution Q is close (in
the sense of KL divergence) to the ﬁxed prior P   has a small training error  then its generalization
error is small.
PAC-Bayes theory has been improved and generalized in a series of works [5  14]. For important
recent results please referred to [14]. [15] generalizes the KL divergence in the PAC-Bayes theorem
to arbitrary convex functions. [15  16  17  18  19] utilize improved PAC-Bayes bounds to develop
learning algorithms and perform model selections.
Very interestingly  it is shown in [2] that one can derive a margin bound for linear classiﬁers (in-
cluding SVM) from the PAC-Bayes theorem quite easily. It is much simpler and slightly tighter than
previous margin bounds for SVM [1  20]. The following simpliﬁed and reﬁned version can be found
in [4].
Theorem 2.2 ([4]). Let X = Rd. Let Q((cid:22); ^w) ((cid:22) > 0  ^w ∈ Rd  ∥^w∥ = 1) denote the distribution of
homogeneous linear classiﬁers cw  where w ∼ N ((cid:22)^w; I). For any (cid:14) ∈ (0; 1)  with probability 1 − (cid:14)
over the random draw of n training examples

kl (erS (Q((cid:22); ^w)) || erD(Q((cid:22); ^w))) ≤ (cid:22)2

(2)
holds simultaneously for all (cid:22) > 0 and all ^w ∈ Rd with ∥^w∥ = 1. In addition  the empirical error
of the stochastic classiﬁer can be written as

n

2 + ln n+1

(cid:14)

erS(Q((cid:22); ^w)) = ES (cid:8)((cid:22)(cid:13)(^w; x; y));

(3)

∫ ∞

t

e

(4)

−(cid:28) 2=2d(cid:28)

where (cid:13)(^w; x; y) = y <^w;x>∥x∥

is the margin of (x; y) with respect to the unit vector ^w; and
(cid:8)(t) = 1 − (cid:8)(t) =

1√
2(cid:25)
is the probability of the upper tail of Gaussian distribution.
According to Theorem 2.2  if there is a linear classiﬁer ^w ∈ Rd inducing large margins for most
training examples  i.e.  (cid:13)( ^w; x; y) is large for most (x; y)   then choosing a relatively small (cid:22) would
yield a small erS (Q((cid:22); ^w)) and in turn a small upper bound for the generalization error of the
stochastic classiﬁer Q((cid:22); ^w). Note that this bound does not depend on the dimensionality d. In fact
almost all previously known margin bounds are dimensionality independent3.
PAC-Bayes theory only provides bounds for stochastic classiﬁers. In practice however  users often
prefer deterministic classiﬁers. There is a close relation between the error of a stochastic classiﬁer
deﬁned by distribution Q and the error of the deterministic voting classiﬁer vQ. The following
simple result is well-known.
Proposition 2.3. Let vQ be the voting classiﬁer deﬁned by distribution Q. That is  vQ(·) =
sgn[Ec∼Qc(·)]. Then for any Q
(5)

erD(vQ) ≤ 2 erD(Q):

Combining Theorem 2.2 and Proposition 2.3  one can upper bound the generalization error of the
voting classiﬁer vQ associated with Q((cid:22); ^w) given in Theorem 2.2. In fact  it is easy to see that
vQ = c^w  the voting classiﬁer is exactly the linear classiﬁer ^w. Thus

erD(c^w) ≤ 2erD(Q((cid:22); ^w)):

(6)

3There exist dimensionality dependent margin bounds [21]. However these bounds grow unboundedly as

the dimensionality tends to inﬁnity.

3

From Theorem 2.2  Proposition 2.3 and (6)  we have that with probability 1−(cid:14) the following margin
bound holds for all classiﬁers c^w with ^w ∈ Rd  ∥ ^w∥ = 1 and all (cid:22) > 0:
2 + ln n+1

(
erS(Q((cid:22); ^w)) || erD(c^w)

≤ (cid:22)2

)

(7)

kl

:

(cid:14)

2

n

(

One disadvantage of the bounds in (5)  (6) and (7) is that they involve a multiplicative factor of 2.
In general  the factor 2 cannot be improved. However for linear classiﬁers with large margins there
can exist tighter bounds. The following is a slightly reﬁned version of the bounds given in [2  3].
Proposition 2.4 ([2  3]). Let Q((cid:22); ^w) and vQ = c^w be deﬁned as above. Let erD;(cid:18)(Q((cid:22); ^w)) =
Ew∼N ((cid:22)^w;I)PD
be the error of the stochastic classiﬁer with margin (cid:18). Then for all
(cid:18) ≥ 0
erD(c^w) ≤ erD;(cid:18)(Q((cid:22); ^w)) + (cid:8)((cid:18)):

y <w;x>∥x∥ ≤ (cid:18)

)

(8)

The bound states that if the stochastic classiﬁer induces small errors with large margin (cid:18)  then the
linear (voting) classiﬁer has only a slightly larger generalization error than the stochastic classiﬁer.
However sometimes (8) can be larger than (5). The two bounds have a different regime in which
they dominate [2]. It is also worth pointing out that the margin y <w;x>∥x∥
considered in Proposition
2.4 is unnormalized with respect to w. See Section 3 for more discussions.
To apply Proposition 2.4  one needs to further bound erD;(cid:18)(Q((cid:22); ^w)) by its empirical version
= ES (cid:8)((cid:22)y <^w;x>∥x∥ − (cid:18)). With slight modiﬁ-
erS;(cid:18)(Q((cid:22); ^w)) := Ew∼N ((cid:22)^w;I)PS
cations of Theorem 2.2  one can show that for any (cid:18) ≥ 0 with probability 1− (cid:14) the following bound
is valid for all (cid:22) and ^w uniformly:

y <w;x>∥x∥ ≤ (cid:18)

(

)

kl (erS;(cid:18)(Q((cid:22); ^w)) || erD;(cid:18)(Q((cid:22); ^w))) ≤ (cid:22)2

2 + ln n+1

(cid:14)

n

:

(9)

The following Proposition combines the above results.
Proposition 2.5. For any (cid:18) ≥ 0 and any (cid:14) > 0 with probability 1 − (cid:14) the following bound is valid
for all (cid:22) and ^w uniformly:

(
erS;(cid:18)(Q((cid:22); ^w)) || erD(c^w) − (cid:8)((cid:18)))

) ≤ (cid:22)2

kl

2 + ln n+1

(cid:14)

n

:

(10)

Note that this last bound is not uniform for (cid:18)  see also [3].
Improving the multiplicative factor was also studied in [22  17]  in which the variance of the stochas-
tic classiﬁer is also bounded by PAC-Bayes theorem  and Chebyshev inequality can be used.

3 Theoretical Results

In this section we give the theoretical results. The main result of this paper is Theorem 3.1  which
provides a dimensionality dependent PAC-Bayes margin bound.
Theorem 3.1. Let Q((cid:22); ^w) ((cid:22) > 0  ^w ∈ Rd  ∥^w∥ = 1) denote the distribution of linear classiﬁers
cw(·) = sgn[< w;· >]  where w ∼ N ((cid:22)^w; I). For any (cid:14) ∈ (0; 1)  with probability 1 − (cid:14) over the
random draw of n training examples

2 ln(1 + (cid:22)2
d ) + ln n+1
n

kl (erS(Q((cid:22); ^w)) || erD(Q((cid:22); ^w))) ≤ d

(11)
holds simultaneously for all (cid:22) > 0 and all ^w ∈ Rd with ∥^w∥ = 1. Here erS(Q((cid:22); ^w)) =
ES (cid:8)((cid:22)(cid:13)(^w; x; y)) and (cid:13)(^w; x; y) = y <^w;x>∥x∥ are the same as in Theorem 2.2.
Comparing Theorem 3.1 with Theorem 2.2  it is easy to see the following Proposition holds.
Proposition 3.2. The bound (11) is sharper than (2) for any d < ∞  and the two bounds tend to be
equivalent as d → ∞.

(cid:14)

4

Theorem 3.1 is the ﬁrst dimensionality dependent margin bound that remains nontrivial in inﬁnite
dimension.
Theorem 3.1 and Theorem 2.2 are uniform bounds for (cid:22). Thus one can choose appropriate (cid:22) to op-
timize each bound respectively. Note that erS (Q((cid:22); ^w)) in the LHS of the two bounds is monotone
decreasing with respect to (cid:22). Comparing to Theorem 2.2  Theorem 3.1 has the advantage that its
RHS scales only in O(ln (cid:22)) rather than O((cid:22)2)  and therefore allows choosing a very large (cid:22).
As described in (7) in Section 2  we can also obtain a margin bound for the deterministic linear
classiﬁer c^w by combining (11) with erD(c^w) ≤ 2 erD(Q((cid:22); ^w)).
In addition  note that the VC dimension of homogeneous linear classiﬁers in Rd is d. From Theorem
))
3.1 we can almost recover the VC bound [23]

√

(

(

erD(c) ≤ erS (c) +

d

1 + ln

+ ln 4
(cid:14)

2n
d
n

for homogenous linear classiﬁers in Rd under mild conditions. Formally we have the following
Corollary.
Corollary 3.3. Theorem 3.1 implies the following result. Suppose n > 5. For any (cid:14) > 2e
with probability 1 − (cid:14) over the random draw of n training examples

− 1
8  

− d

8 n

(12)

(13)

(14)

√

2n
d

1 +

d ln

(

(
))
(cid:12)(cid:12)(cid:12)(cid:12) ≤ (ln n)1=2d3=2

n

4n2

)

erD(cw) ≤ erS (cw) +

((cid:12)(cid:12)(cid:12)(cid:12)y

PD

< w; x >
∥w∥∥x∥

√

+ 1

2 ln 2(n+1)

(cid:14)

+

d + ln n

n

√

≤ 1
4

d + ln n

n

:

holds simultaneously for all homogeneous linear classiﬁers cw with w ∈ Rd satisfying

Condition (14) is easy to satisfy if d ≪ n.
In a sense  the dimensionality dependent margin bound in Theorem 3.1 uniﬁes the dimensionality
independent margin bound and the VC bound for linear classiﬁers.
Although it is not easy to theoretically quantify how much sharper (11) is than (2) and the VC bound
(12) (because the ﬁrst two bounds hold uniformly for all (cid:22))  in Section 4 we will demonstrate by
experiments that the new bound is usually signiﬁcantly better than (2) and (12) on relatively large
datasets.

3.1

Improving the Multiplicative Factor

As we mentioned in Section 2  Proposition 2.3 involves a multiplicative factor of 2 when bounding
the error of the deterministic voting classiﬁer by the error of the stochastic classiﬁer. Note that in
general erD(c^w) ≤ 2erD(Q((cid:22); ^w)) cannot be improved (consider the case that with probability one
the data has zero margin with respect to ^w). Here we study how to improve it for large margin
classiﬁers.
Recall that Proposition 2.4 gives erD(c^w) ≤ erD;(cid:18)(Q((cid:22); ^w)) + (cid:8)((cid:18))  which bounds the gener-
alization error of the linear classiﬁer in terms of the error of the stochastic classiﬁer with mar-
gin (cid:18) ≥ 0. As pointed out in [2]  this bound is not always better than Proposition 2.3 (i.e. 
erD(c^w) ≤ 2erD(Q((cid:22); ^w))). The two bounds each has a different dominant regime. Our ﬁrst result
in this subsection is the following simple improvement over both Proposition 2.3 and Proposition
2.4.
Proposition 3.4. Using the notions in Proposition 2.4  we have that for all (cid:18) ≥ 0 

erD(c^w) ≤ 1

(cid:8)((cid:18))

erD;(cid:18)(Q((cid:22); ^w));

(15)

where (cid:8)((cid:18)) is deﬁned in Theorem 2.2.

5

It is easy to see that Proposition 2.3 is a special case of Proposition 3.4: just let (cid:18) = 0 in (15) we
recover (6). Thus Proposition 3.4 is always sharper than Proposition 2.3. It is also easy to show that
(15) is sharper than (8) in Proposition 2.4 whenever the bounds are nontrivial. Formally we have the
following proposition.
Proposition 3.5. Suppose the RHS of (8) or the RHS of (15) is smaller than 1  i.e.  at least one of
the two bounds is nontrivial. Then (15) is sharper than (8).

As mentioned in Section 2  the margins discussed so far in this subsection are unnormalized with
respect to w ∈ Rd. That is  we consider y <w;x>∥x∥ . In the following we will focus on normalized
margins y <w;x>
∥w∥∥x∥. It will soon be clear that this brings additional beneﬁts when combining with the
dimensionality dependent margin bound.
∥w∥∥x∥ ≤ (cid:18)) be the true error of the stochastic classiﬁer
Let erND;(cid:18)(Q((cid:22); ^w)) = Ew∼N ((cid:22)^w;I)PD(y <w;x>
Q((cid:22); ^w) with normalized margin (cid:18) ∈ [−1; 1]. Also let erNS;(cid:18)(Q((cid:22); ^w)) be its empirical version. We
have the following lemma.
Lemma 3.6. For any (cid:22) > 0  any ^w ∈ Rd with ∥^w∥ = 1 and any (cid:18) ≥ 0 

erD(c^w) ≤ erND;(cid:18) (Q((cid:22); ^w))

(cid:8)((cid:22)(cid:18))

:

(16)

(

) ≤ d

If erND;(cid:18)(Q) is only slightly larger than erD(Q) for a not-too-small (cid:18) > 0  then erND;(cid:18)(Q)
can be
much smaller than 2erD(Q) even with a not too large (cid:22). Also note that setting (cid:18) = 0 in (16)  we
can recover (6).
The true margin error erND;(cid:18)(Q) can be bounded by its empirical version similar to Theorem 3.1: For
any (cid:18) ≥ 0 and any (cid:14) > 0  with probability 1 − (cid:14)

(cid:8)((cid:22)(cid:18))

erNS;(cid:18)(Q((cid:22); ^w))||erND;(cid:18)(Q((cid:22); ^w))

kl

2 ln(1 + (cid:22)2
d ) + ln n+1
n

(cid:14)

(17)

(

holds simultaneously for all (cid:22) > 0 and ^w ∈ Rd with ∥ ^w∥ = 1.
Combining the previous two results we have a dimensionality dependent margin bound for the linear
classiﬁer c^w.
Proposition 3.7. Let Q((cid:22); ^w) deﬁned as before. For any (cid:18) ≥ 0 and any (cid:14) > 0  with probability
1 − (cid:14) over the random draw of n training examples
erNS;(cid:18)(Q((cid:22); ^w))||erD(c^w)(cid:8)((cid:22)(cid:18))

) ≤ d

2 ln(1 + (cid:22)2
d ) + ln n+1
n

(18)

holds simultaneously for all (cid:22) > 0 and ^w ∈ Rd with ∥^w∥ = 1.
)
To see how Proposition 3.7 improves the multiplicative factor  let’s take a closer look at the bound
∥w∥∥x∥ ≤ (cid:18)) tends to the
(18). Observe that as (cid:22) getting large  erNS;(cid:18)(Q((cid:22); ^w)) = Ew∼N ((cid:22)^w;I)PD(y <w;x>
(recall that ∥ ^w∥=1).
empirical error of the linear classiﬁer ^w with margin (cid:18)  i.e.  PS
Also if (cid:22)(cid:18) > 3  (cid:8)((cid:22)(cid:18)) ≈ 1. Taking into the consideration that the RHS of (18) scales only in
O(ln (cid:22))  we can choose a relatively large (cid:22) and (18) gives a dimensionality dependent margin bound
whose multiplicative factor can be very close to 1.

y <^w;x>∥x∥ ≤ (cid:18)

(

kl

(cid:14)

4 Experiments

In this section we conduct a series of experiments on benchmark datasets. The goal is to see to
what extent the Dimensionality Dependent margin bound (will be referred to as DD-margin bound)
is sharper than the Dimensionality Independent margin bound (will be referred to as DI-margin
bound) as well as the VC bound. More importantly  we want to see from the experiments how
useful the DD-margin bound is for model selection.

6

Dataset

# Examples

# Features

Dataset

# Examples

# Features

Table 1: Description of dataset

Image
Magic04
Optdigits
Pendigits

BreastCancer

Pima

2310
19020
5620
10992
683
768

20
10
64
16
9
8

Letter

Mushroom
PageBlock
Waveform

Glass
wdbc

20000
8124
5473
3304
214
569

16
22
10
21
9
30

We use 12 datasets all from the UCI repository [24]. A description of the datasets is given in Table
1. For each dataset  we use 5-fold cross validation and average the results over 10 runs (for a total
50 runs). If the dataset is a multiclass problem  we group the data into two classes since we study
binary classiﬁcation problems. In the data preprocessing stage each feature is normalized to [0; 1].
To compare the bounds and to do model selection  we use SVM with polynomial kernels K(x; x′
) =
(a < x; x′
> +b)t and let t varies4. For each t  we train a classiﬁer by libsvm [25]. We plot the
values of the three bounds—the DD-margin bound  the DI-margin bound  the VC bound (12) as
well as the test and training error (see Figure 1 - Figure 12). For the two margin bounds  since they
hold uniformly for (cid:22) > 0  we select the optimal (cid:22) to make the bounds as small as possible. For
simplicity  we combine Proposition 2.3 with Theorem 3.1 and Theorem 2.2 respectively to obtain
the ﬁnal bound for the generalization error of the deterministic linear classiﬁers. In each ﬁgure  the
horizonal axis represents the degree t of the polynomial kernel. All bounds in the ﬁgures (including
training and test error) are for deterministic (voting) classiﬁer.
To analyze the experimental results  we group the 12 results into two categories as follows.

1. Figure 1 - Figure 8. This category consists of eight datasets  and each of them contains
at least 2000 examples (relatively large datasets). On all these datasets  the DD-margin
bounds are signiﬁcantly sharper than the DI-margin bounds as well as the VC bounds. More
importantly  the DD-margin bounds work well for model selection. We can use this bound
to choose the degree of the polynomial kernel. On all the datasets except “Image”  the curve
of the DD-margin bound is highly correlated with the curve of the test error: When the test
error decreases (or increases)  the DD-margin bound also decreases (or increases); And as
the test error remains unchanged as the degree t grows  the DD-margin bound selects the
model with the lowest complexity.

2. Figure 9 - Figure 12. This category consists of four small datasets  each contains less than
1000 examples. On these small datasets  the VC bounds often become trivial (larger than
1). The DD-margin bounds are still always  but less signiﬁcantly  sharper than the DI-
margin bounds. However  on these small datasets  it is difﬁcult to tell if the bounds select
good models.

In sum  the experimental results demonstrate that the DD-margin bound is usually signiﬁcantly
sharper than the DI-margin bound as well as the VC bound if the dataset is relatively large. Also the
DD-margin bound is useful for model selection. However  for small datasets  all three bounds seem
not useful for practical purpose.

5 Conclusion

In this paper we study the problem whether dimensionality independency is intrinsic for margin
bounds. We prove a dimensionality dependent PAC-Bayes margin bound. This bound is sharper
than a previously well-known dimensionality independent margin bound when the feature space is of
ﬁnite dimension; and they tend to be equivalent as the dimensionality grows to inﬁnity. Experimental
results demonstrate that for relatively large datasets the new bound is often useful for model selection
and signiﬁcantly sharper than previous margin bound as well as the VC bound.

4For simplicity we ﬁx a and b as constants in all the experiments.

7

Figure 1: Image

Figure 2: Letter

Figure 3: Magic04

Figure 4: Mushroom

Figure 5: Optdigits

Figure 6: PageBlocks

Figure 7: Pendigits

Figure 8: Waveform

Figure 9: BreastCancer

Figure 10: Glass

Figure 11: Pima

Figure 12: wdbc

Our work is based on the PAC-Bayes theory. One limitation is that it involves a multiplicative factor
of 2 when transforming stochastic classiﬁers to deterministic classiﬁers. Although we provide two
improved bounds (Proposition 3.4  3.7) over previous results (Proposition 2.3  2.4)  the multiplica-
tive factor is still strictly larger than 1. A future work is to study whether there exist dimensionality
dependent margin bounds (not necessarily PAC-Bayes) without this multiplicative factor.

Acknowledgments

This work was supported by NSFC(61222307  61075003) and a grant from Microsoft Research
Asia. We also thank Chicheng Zhang for very helpful discussions.

8

02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest error02468101200.20.40.60.811.21.41.6terror DD−marginDI−marginVCtrain errortest errorReferences

[1] John Shawe-Taylor  Peter L. Bartlett  Robert C. Williamson  and Martin Anthony. Structural risk mini-
mization over data-dependent hierarchies. IEEE Transactions on Information Theory  44(5):1926–1940 
1998.

[2] John Langford and John Shawe-Taylor. PAC-Bayes & Margins.

Processing Systems  pages 423–430  2002.

In Advances in Neural Information

[3] David A. McAllester. Simpliﬁed PAC-Bayesian margin bounds. Learning Theory and Kernel Machines 

2777:203–215  2003.

[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning

Research  6:273–306  2005.

[5] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation. Journal

of Machine Learning Research  3:233–269  2002.

[6] David A. McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999.
[7] Robert E. Schapire  Yoav Freund  Peter Barlett  and Wee Sun Lee. Boosting the margin: A new explana-

tion for the effectiveness of voting methods. Annals of Statistics  26(5):1651–1686  1998.

[8] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the general-

ization error of combined classiﬁers. Annals of Statistics  30:1–50  2002.

[9] Vladimir Koltchinskii and Dmitry Panchenko. Complexities of convex combinations and bounding the

generalization error in classiﬁcation. Annals of Statistics  33:1455–1496  2005.

[10] John Langford  Matthias Seeger  and Nimrod Megiddo. An improved predictive accuracy bound for

averaging classiﬁers. In International Conference on Machine Learning  pages 290–297  2001.

[11] Sanjoy Dasgupta and Philip M. Long. Boosting with diverse base classiﬁers. In Annual Conference on

Learning Theory  pages 273–287  2003.

[12] Leo Breiman. Prediction games and arcing algorithms. Neural Computation  11:1493–1518  1999.
[13] Liwei Wang  Masashi Sugiyama  Zhaoxiang Jing  Cheng Yang  Zhi-Hua Zhou  and Jufu Feng. A reﬁned
margin analysis for boosting algorithms via equilibrium margin. Journal of Machine Learning Research 
12:1835–1863  2011.

[14] Olivier Catoni. PAC-Bayesian supervised classiﬁcation: The thermodynamics of statistical learning. IMS

Lecture Notes–Monograph Series  56  2007.

[15] Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  and Mario Marchand. PAC-Bayesian learning

of linear classiﬁers. In International Conference on Machine Learning  page 45  2009.

[16] Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  Mario Marchand  and Sara Shanian. From
PAC-Bayes bounds to KL regularization. In Advances in Neural Information Processing Systems  pages
603–610  2009.

[17] Jean-Francis Roy  Franc¸ois Laviolette  and Mario Marchand. From PAC-Bayes bounds to quadratic pro-

grams for majority votes. In International Conference on Machine Learning  pages 649–656  2011.

[18] Amiran Ambroladze  Emilio Parrado-Hern´andez  and John Shawe-Taylor. Tighter pac-bayes bounds. In

Advances in Neural Information Processing Systems  pages 9–16  2006.

[19] John Shawe-Taylor  Emilio Parrado-Hern´andez  and Amiran Ambroladze. Data dependent priors in PAC-

Bayes bounds. In International Conference on Computational Statistics  pages 231–240  2010.

[20] Peter L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of the
IEEE Transactions on Information Theory 

weights is more important than the size of the network.
44(2):525–536  1998.

[21] Ralf Herbrich and Thore Graepel. A PAC-Bayesian margin bound for linear classiﬁers. IEEE Transactions

on Information Theory  48(12):3140–3150  2002.

[22] Alexandre Lacasse  Franc¸ois Laviolette  Mario Marchand  Pascal Germain  and Nicolas Usunier. PAC-
Bayes bounds for the risk of the majority vote and the variance of the gibbs classiﬁer. In Advances in
Neural Information Processing Systems  pages 769–776  2006.

[23] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.
[24] Andrew Frank and Arthur Asuncion. UCI machine learning repository  2010.
[25] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transac-

tions on Intelligent Systems and Technology  2:27:1–27:27  2011.

9

,Pablo Sprechmann
Roee Litman
Tal Ben Yakar
Alexander Bronstein
Guillermo Sapiro
Yuxin Chen
Emmanuel Candes