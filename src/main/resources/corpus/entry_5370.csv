2007,Support Vector Machine Classification with Indefinite Kernels,In this paper  we propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function  our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss. This can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method. We compare the performance of our technique with other methods on several data sets.,Support Vector Machine Classiﬁcation

with Indeﬁnite Kernels

Ronny Luss

ORFE  Princeton University

Princeton  NJ 08544

Alexandre d’Aspremont
ORFE  Princeton University

Princeton  NJ 08544

rluss@princeton.edu

aspremon@princeton.edu

Abstract

In this paper  we propose a method for support vector machine classiﬁcation using
indeﬁnite kernels. Instead of directly minimizing or stabilizing a nonconvex loss
function  our method simultaneously ﬁnds the support vectors and a proxy kernel
matrix used in computing the loss. This can be interpreted as a robust classiﬁcation
problem where the indeﬁnite kernel matrix is treated as a noisy observation of the
true positive semideﬁnite kernel. Our formulation keeps the problem convex and
relatively large problems can be solved efﬁciently using the analytic center cutting
plane method. We compare the performance of our technique with other methods
on several data sets.

1 Introduction

Here  we present an algorithm for support vector machine (SVM) classiﬁcation using indeﬁnite ker-
nels. Our interest in indeﬁnite kernels is motivated by several observations. First  certain similarity
measures take advantage of application-speciﬁc structure in the data and often display excellent
empirical classiﬁcation performance. Unlike popular kernels used in support vector machine clas-
siﬁcation  these similarity matrices are often indeﬁnite and so do not necessarily correspond to a
reproducing kernel Hilbert space (see [1] for a discussion).

An application of classiﬁcation with indeﬁnite kernels to image classiﬁcation using Earth Mover’s
Distance was discussed in [2]. Similarity measures for protein sequences such as the Smith-
Waterman and BLAST scores are indeﬁnite yet have provided hints for constructing useful positive
semideﬁnite kernels such as those decribed in [3] or have been transformed into positive semideﬁnite
kernels (see [4] for example). Here instead  our objective is to directly use these indeﬁnite similarity
measures for classiﬁcation.

Our work also closely follows recent results on kernel learning (see [5] or [6])  where the kernel
matrix is learned as a linear combination of given kernels  and the resulting kernel is explicitly
constrained to be positive semideﬁnite (the authors of [7] have adapted the SMO algorithm to solve
the case where the kernel is written as a positively weighted combination of other kernels). In our
case however  we never explicitly optimize the kernel matrix because this part of the problem can be
solved explicitly  which means that the complexity of our method is substantially lower than that of
classical kernel learning methods and closer in spirit to the algorithm used in [8]  who formulate the
multiple kernel learning problem of [7] as a semi-inﬁnite linear program and solve it with a column
generation technique similar to the analytic center cutting plane method we use here.

Finally  it is sometimes impossible to prove that some kernels satisfy Mercer’s condition or the
numerical complexity of evaluating the exact positive semideﬁnite kernel is too high and a proxy
(and not necessarily positive semideﬁnite) kernel has to be used instead (see [9] for example). In
both cases  our method allows us to bypass these limitations.

1

1.1 Current results

Several methods have been proposed for dealing with indeﬁnite kernels in SVMs. A ﬁrst direction
embeds data in a pseudo-Euclidean (pE) space: [10] for example  formulates the classiﬁcation prob-
lem with an indeﬁnite kernel as that of minimizing the distance between convex hulls formed from
the two categories of data embedded in the pE space. The nonseparable case is handled in the same
manner using reduced convex hulls (see [11] for a discussion of SVM geometric interpretations).

Another direction applies direct spectral transformations to indeﬁnite kernels: ﬂipping the nega-
tive eigenvalues or shifting the kernel’s eigenvalues and reconstructing the kernel with the original
eigenvectors in order to produce a positive semideﬁnite kernel (see [12] and [2]).

Yet another option is to reformulate either the maximum margin problem or its dual in order to
use the indeﬁnite kernel in a convex optimization problem (see [13]). An equivalent formulation
of SVM with the same objective but where the kernel appears in the constraints can be modiﬁed
to a convex problem by eliminating the kernel from the objective. Directly solving the nonconvex
problem sometimes gives good results as well (see [14] and [10]).

1.2 Contribution

Here  instead of directly transforming the indeﬁnite kernel  we simultaneously learn the support vec-
tor weights and a proxy positive semideﬁnite kernel matrix  while penalizing the distance between
this proxy kernel and the original  indeﬁnite one. Our main result is that the kernel learning part of
that problem can be solved explicitly  meaning that the classiﬁcation problem with indeﬁnite kernels
can simply be formulated as a perturbation of the positive semideﬁnite case.

Our formulation can also be interpreted as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. In that sense  indeﬁnite similarity matrices are seen as noisy observa-
tions of an unknown positive semideﬁnite kernel. From a complexity standpoint  while the original
SVM classiﬁcation problem with indeﬁnite kernel is nonconvex  the robustiﬁcation we detail here is
a convex problem  and hence can be solved efﬁciently with guaranteed complexity bounds.

The paper is organized as follows. In Section 2 we formulate our main classiﬁcation problem and
detail its interpretation as a robust SVM. In Section 3 we describe an algorithm for solving this
problem. Finally  in Section 4  we test the numerical performance of these methods on various
applications.

2 SVM with indeﬁnite kernels

Here  we introduce our robustiﬁcation of the SVM classiﬁcation problem with indeﬁnite kernels.

2.1 Robust classiﬁcation

Let K ∈ Sn be a given kernel matrix and y ∈ Rn be the vector of labels  with Y = diag(y) the
matrix with diagonal y  where Sn is the set of symmetric matrices of size n and Rn is the set of
n-vectors of real numbers. We can write the dual of the SVM classiﬁcation problem with hinge loss
and quadratic penalty as:

maximize αT e − Tr(K(Y α)(Y α)T )/2
subject to αT y = 0

0 ≤ α ≤ C

(1)

in the variable α ∈ Rn and where e is an n-vector of ones. When K is positive semideﬁnite  this
problem is a convex quadratic program. Suppose now that we are given an indeﬁnite kernel matrix
K0 ∈ Sn. We formulate a robust version of problem (1) by restricting K to be a positive semideﬁnite
kernel matrix in some given neighborhood of the original (indeﬁnite) kernel matrix K0:

max

{αT y=0  0≤α≤C}

min

{K(cid:23)0  kK−K0k2

F ≤β}

αT e −

1
2

Tr(K(Y α)(Y α)T )

(2)

in the variables K ∈ Sn and α ∈ Rn  where the parameter β > 0 controls the distance between
the original matrix K0 and the proxy kernel K. This can be interpreted as a worst-case robust

2

classiﬁcation problem with bounded uncertainty on the kernel matrix K. The above problem is
infeasible for some values of β so we replace here the hard constraint on K by a penalty on the
distance between the proxy positive semideﬁnite kernel and the given indeﬁnite matrix. The problem
we solve is now:

max

{αT y=0 0≤α≤C}

min
{K(cid:23)0}

αT e −

1
2

Tr(K(Y α)(Y α)T ) + ρkK − K0k2
F

(3)

in the variables K ∈ Sn and α ∈ Rn  where the parameter ρ > 0 controls the magnitude of the
penalty on the distance between K and K0. The inner minimization problem is a convex conic
program on K. Also  as the pointwise minimum of a family of concave quadratic functions of α  the
solution to the inner problem is a concave function of α  and hence the outer optimization problem
is also convex (see [15] for further details). Thus  (3) is a concave maximization problem subject to
linear constraints and is therefore a convex problem in α.
Our key result here is that the inner kernel learning optimization problem can be solved in closed
form. For a ﬁxed α  the inner minimization problem is equivalent to the following problem:

minimize
subject to K (cid:23) 0

kK − (K0 + 1

4ρ (Y α)(Y α)T )k2

F

in the variable K ∈ Sn. This is the projection of the K0 + (1/4ρ)(Y α)(Y α)T on the cone of
positive semideﬁnite matrices. The optimal solution to this problem is then given by:

K ∗ = (K0 + (1/4ρ)(Y α)(Y α)T )+

(4)
i where λi and xi are

where X+ is the positive part of the matrix X  i.e. X+ = Pi max(0  λi)xixT

the ith eigenvalue and eigenvector of the matrix X. Plugging this solution into (3)  we get:

max

{αT y=0  0≤α≤C}

αT e −

1
2

Tr(K ∗(Y α)(Y α)T ) + ρkK ∗ − K0k2
F

in the variable α ∈ Rn  where (Y α)(Y α)T is the rank one matrix with coefﬁcients yiαiαj yj 
i  j = 1  . . .   n. We can rewrite this as an eigenvalue optimization problem by using the eigenvalue
representation of X+. Letting the eigenvalue decomposition of K0+(1/4ρ)(Y α)(Y α)T be V DV T  
we get K ∗ = V D+V T and  with vi the ith column of V   we can write:

Tr(K ∗(Y α)(Y α)T ) = (Y α)T V D+V T (Y α)

=

n

Xi=1

max(cid:18)0  λi(cid:18)K0 +

1
4ρ

(Y α)(Y α)T(cid:19)(cid:19) (αT Y vi)2

where λi (X) is the ith eigenvalue of the quantity X. Using the same technique  we can also rewrite
the term kK ∗ − K0|2
F using this eigenvalue decomposition. Our original optimization problem (3)
ﬁnally becomes:

maximize αT e − 1

2 Pi max(0  λi(K0 + (Y α)(Y α)T /4ρ))(αT Y vi)2

+ρPi (max(0  λi(K0 + (Y α)(Y α)T /4ρ)))2
−2ρPi Tr((vivT

i )K0)max(0  λi(K0 + (Y α)(Y α)T /4ρ)) + ρ Tr(K0K0)

(5)

subject to αT y = 0  0 ≤ α ≤ C

in the variable α ∈ Rn.

2.2 Dual problem

Because problem (3) is convex with at least one compact feasible set  we can formulate the dual
problem to (5) by simply switching the max and the min. The inner maximization is a quadratic
program in α  and hence has a quadratic program as its dual. We then get the dual by plugging this
inner dual quadratic program into the outer minimization  to get the following problem:

minimize Tr(K −1(Y (e − λ + µ + yν))(Y (e − λ + µ + yν))T )/2 + CµT e + ρkK − K0k2
F
subject to K (cid:23) 0  λ  µ ≥ 0

(6)

3

in the variables K ∈ Sn  λ  µ ∈ Rn and ν ∈ R. This dual problem is a quadratic program in the
variables λ and µ which correspond to the primal constraints 0 ≤ α ≤ C and ν which is the dual
variable for the constraint αT y = 0. As we have seen earlier  any feasible solution to the primal
problem produces a corresponding kernel in (4)  and plugging this kernel into the dual problem in
(6) allows us to calculate a dual feasible point by solving a quadratic program which gives a dual
objective value  i.e. an upper bound on the optimum of (5). This bound can then be used to compute
a duality gap and track convergence.

2.3 Interpretation

We noted that our problem can be viewed as a worst-case robust classiﬁcation problem with uncer-
tainty on the kernel matrix. Our explicit solution of the optimal worst-case kernel given in (4) is the
projection of a penalized rank-one update to the indeﬁnite kernel on the cone of positive semideﬁnite
matrices. As ρ tends to inﬁnity  the rank-one update has less effect and in the limit  the optimal ker-
nel is the kernel given by zeroing out the negative eigenvalues of the indeﬁnite kernel. This means
that if the indeﬁnite kernel contains a very small amount of noise  the best positive semideﬁnite
kernel to use with SVM in our framework is the positive part of the indeﬁnite kernel.

This limit as ρ tends to inﬁnity also motivates a heuristic for the transformation of the kernel on
the testing set. Since the negative eigenvalues of the training kernel are thresholded to zero in the
limit  the same transformation should occur for the test kernel. Hence  we update the entries of the
full kernel corresponding to training instances by the rank-one update resulting from the optimal
solution to (3) and threshold the negative eigenvalues of the full kernel matrix to zero. We then use
the test kernel values from the resulting positive semideﬁnite matrix.

3 Algorithms

We now detail two algorithms that can be used to solve Problem (5). The optimization problem is
the maximization of a nondifferentiable concave function subject to convex constraints. An optimal
point always exists since the feasibility set is bounded and nonempty. For numerical stability  in both
algorithms  we quadratically smooth our objective to calculate a gradient instead. We ﬁrst describe
a simple projected gradient method which has numerically cheap iterations but has no convergence
bound. We then show how to apply the much more efﬁcient analytic center cutting plane method
whose iterations are slightly more complex but which converges linearly.

Smoothing Our objective contains terms of the form max{0  f (x)} for some function f (x)  which
are not differentiable (described in the section below). These functions are easily smoothed out by
a regularization technique (see [16] for example). We replace them by a continuously differentiable
2 -approximation as follows:
ǫ

ϕǫ(f (x)) = max
0≤u≤1

(uf (x) −

ǫ
2

u2).

and the gradient is given by ∇ϕǫ(f (x)) = u∗(x)∇f (x) where u∗(x) = argmax ϕǫ(f (x)).

Gradient Calculating the gradient of our objective requires a full eigenvalue decomposition to
compute the gradient of each eigenvalue. Given a matrix X(α)  the derivative of the ith eigenvalue
with respect to α is given by:

∂λi(X(α))

∂α

= vT
i

∂X(α)

∂α

vi

(7)

where vi is the ith eigenvector of X(α). We can then combine this expression with the smooth
approximation above to get the gradient.

We note that eigenvalues of symmetric matrices are not differentiable when some of them have mul-
tiplicities greater than one (see [17] for a discussion). In practice however  most tested kernels were
of full rank with distinct eigenvalues so we ignore this issue here. One may also consider projected
subgradient methods  which are much slower  or use subgradients for analytic center cutting plane
methods (which does not affect complexity).

4

3.1 Projected gradient method

The projected gradient method takes a steepest descent  then projects the new point back onto the
feasible region (see [18] for example). In order to use these methods the objective function must be
differentiable and the method is only efﬁcient if the projection step is numerically cheap. We choose
an initial point α0 ∈ Rn and the algorithm proceeds as follows:

Projected gradient method

1. Compute αi+1 = αi + t∇f (αi).
2. Set αi+1 = pA(αi+1).
3. If gap ≤ ǫ stop  otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This requires an eigenvalue decomposition and costs O(n3). We note that a line search would
be costly because it would require multiple eigenvalue decompositions to recalculate the objective
multiple times.

Step 2. This is a projection onto the region A = {αT y = 0  0 ≤ α ≤ C} and can be solved
explicitly by sorting the vector of entries  with cost O(n log n).
Stopping Criterion. We can compute a duality gap using the results of §2.2: let Ki = (K0 +
(Y αi)(Y αi)T /4ρ)+ (the kernel at iteration i)  then solving problem (1) which is just an SVM with
a convex kernel Ki produces an upper bound on (5)  and hence a bound on the suboptimality of the
current solution.

Complexity. The number of iterations required by this method to reach a target precision of ǫ is
typically in O(1/ǫ2).

3.2 Analytic center cutting plane method

The analytic center cutting plane method (ACCPM) reduces the feasible region on each iteration
using a new cut of the feasible region computed by evaluating a subgradient of the objective function
at the analytic center of the current set  until the volume of the reduced region converges to the target
precision. This method does not require differentiability. We set A0 = {αT y = 0  0 ≤ α ≤ C}
which we can write as {A0 ≤ b0} to be our ﬁrst localization set for the optimal solution. The
method then works as follows (see [18] for a more complete reference on cutting plane methods):

Analytic center cutting plane method

1. Compute αi as the analytic center of Ai by solving:

αi+1 = argmin
y∈Rn

−

m

Xi=1

log(bi − aT

i y)

where aT

i represents the ith row of coefﬁcients from the left-hand side of {A0 ≤ b0}.

2. Compute ∇f (x) at the center αi+1 and update the (polyhedral) localization set:

Ai+1 = Ai ∪ {∇f (αi+1)(α − αi+1) ≥ 0}

3. If gap ≤ ǫ stop  otherwise go back to step 1.

The complexity of each iteration breaks down as follows.

Step 1. This step computes the analytic center of a polyhedron and can be solved in O(n3) operations
using interior point methods for example.

5

Step 2. This simply updates the polyhedral description.

Stopping Criterion. An upper bound is computed by maximizing a ﬁrst order Taylor approximation
of f (α) at αi over all points in an ellipsoid that covers Ai  which can be done explicitly.
Complexity. ACCPM is provably convergent in O(n log(1/ǫ)2) iterations when using cut elimina-
tion which keeps the complexity of the localization set bounded. Other schemes are available with
slightly different complexities: an O(n2/ǫ2) is achieved in [19] using (cheaper) approximate centers
for example.

4 Experiments

In this section we compare the generalization performance of our technique to other methods of
applying SVM classiﬁcation given an indeﬁnite similarity measure. We also test SVM classiﬁcation
performance on positive semideﬁnite kernels using the LIBSVM library. We ﬁnish with experiments
showing convergence of our algorithms. Our algorithms were implemented in Matlab.

4.1 Generalization

We compare our method for SVM classiﬁcation with indeﬁnite kernels to several of the kernel pre-
processing techniques discussed earlier. The ﬁrst three techniques perform spectral transformations
on the indeﬁnite kernel. The ﬁrst  denoted denoise  thresholds the negative eigenvalues to zero. The
second transformation  called ﬂip  takes the absolute value of all eigenvalues. The last transforma-
tion  shift  adds a constant to each eigenvalue making them all positive. See [12] for further details.
We ﬁnally also compare with using SVM on the original indeﬁnite kernel (SVM converges but the
solution is only a stationary point and is not guaranteed to be optimal).

We experiment on data from the USPS handwritten digits database (described in [20]) using the
indeﬁnite Simpson score (SS) to compare two digits and on two data sets from the UCI repository
(see [21]) using the indeﬁnite Epanechnikov (EP) kernel. The data is randomly divided into training
and testing data. We apply 5-fold cross validation and use an accuracy measure (described below)
to determine the optimal parameters C and ρ. We then train a model with the full training set and
optimal parameters and test on the independent test set.

Table 1: Statistics for various data sets.

Data Set

# Train

USPS-3-5-SS
USPS-4-6-SS
Diabetes-EP

Liver-EP

767
829
614
276

# Test
773
857
154
69

λmin

-34.76
-37.30
-0.27

-1.38e-15

λmax

453.58
413.17
18.17
3.74

Table 1 provides statistics including the minimum and maximum eigenvalues of the training kernels.
The main observation is that the USPS data uses highly indeﬁnite kernels while the UCI data use
kernels that are nearly positive semideﬁnite. Table 2 displays performance by comparing accuracy
and recall. Accuracy is deﬁned as the percentage of total instances predicted correctly. Recall is the
percentage of true positives that were correctly predicted positive.

Our method is referred to as Indeﬁnite SVM. We see that our method performs favorably among
the USPS data. Both measures of performance are quite high for all methods. Our method does
not perform as well on the UCI data sets but is still favorable on one of the measures in each
experiment. Notice though that recall is not good in the liver data set overall which could be the
result of overﬁtting one of the classiﬁcation categories. The liver data set uses a kernel that is almost
positive semideﬁnite - this is an example where the input is almost a true kernel and Indeﬁnite
SVM ﬁnds one slightly better. We postulate that our method will perform better in situations where
the similarity measure is highly indeﬁnite as in the USPS dataset  while measures that are almost
positive semideﬁnite maybe be seen as having a small amount of noise.

6

Table 2: Performance Measures for various data sets.

Data Set

Measure
Accuracy

USPS-3-5-SS

Recall
USPS-4-6-SS Accuracy
Recall

Diabetes-EP

Liver-EP

Accuracy

Recall

Accuracy

Recall

4.2 Algorithm Convergence

Denoise
95.47
94.50
97.78
98.42
75.32
90.00
63.77
22.58

Flip
95.73
95.45
97.90
98.65
74.68
90.00
63.77
22.58

Shift
90.43
92.11
94.28
93.68
68.83
92.00
57.97
25.81

SVM Indeﬁnite SVM
74.90
72.73
90.08
88.49
75.32
90.00
63.77
22.58

96.25
96.65
97.90
98.87
68.83
95.00
65.22
22.58

We ran our two algorithms on data sets created by randomly perturbing the four USPS data sets used
above. The average results with one standard deviation above and below the mean are displayed in
Figure 1 with the duality gap in log scale (note that the codes were not stopped here and that the
target gap improvement is usually much smaller than 10−8). As expected  ACCPM converges much
faster (in fact linearly) to a higher precision while each iteration requires solving a linear program
of size n. The gradient projection method converges faster in the beginning but stalls at a higher
precision  however each iteration only requires sorting the current point.

p
a
G
y
t
i
l
a
u
D

104

103

102

101

100

10−1

10−2

10−3

10−4

0

101

100

10−1

10−2

p
a
G
y
t
i
l
a
u
D

50

100

150

200

10−3

0

200

Iteration

400
600
Iteration

800

1000

Figure 1: Convergence plots for ACCPM (left) & projected gradient method (right) on randomly perturbed
USPS data sets (average gap versus iteration number  dashed lines at plus and minus one standard deviation).

5 Conclusion

We have proposed a technique for incorporating indeﬁnite kernels into the SVM framework with-
out any explicit transformations. We have shown that if we view the indeﬁnite kernel as a noisy
instance of a true kernel  we can learn an explicit solution for the optimal kernel with a tractable
convex optimization problem. We give two convergent algorithms for solving this problem on rel-
atively large data sets. Our initial experiments show that our method can at least fare comparably
with other methods handling indeﬁnite kernels in the SVM framework but provides a much clearer
interpretation for these heuristics.

7

References

[1] C. S. Ong  X. Mary  S. Canu  and A. J. Smola. Learning with non-positive kernels. Proceedings of the

21st International Conference on Machine Learning  2004.

[2] A. Zamolotskikh and P. Cunningham. An assessment of alternative strategies for constructing emd-based

kernel functions for use in an svm for image classiﬁcation. Technical Report UCD-CSI-2007-3  2004.

[3] H. Saigo  J. P. Vert  N. Ueda  and T. Akutsu. Protein homology detection using string alignment kernels.

Bioinformatics  20(11):1682–1689  2004.

[4] G. R. G. Lanckriet  N. Cristianini  M. I. Jordan  and W. S. Noble. Kernel-based integration of genomic

data using semideﬁnite programming. 2003. citeseer.ist.psu.edu/648978.html.

[5] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine Learning Research  5:27–72  2004.

[6] C. S. Ong  A. J. Smola  and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine

Learning Research  6:1043–1071  2005.

[7] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the smo

algorithm. Proceedings of the 21st International Conference on Machine Learning  2004.

[8] S. Sonnenberg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. Journal of

Machine Learning Research  7:1531–1565  2006.

[9] Marco Cuturi. Permanents  transport polytopes and positive deﬁnite kernels on histograms. Proceedings

of the Twentieth International Joint Conference on Artiﬁcial Intelligence  2007.

[10] B. Haasdonk. Feature space interpretation of svms with indeﬁnite kernels. IEEE Transactions on Pattern

Analysis and Machine Intelligence  27(4)  2005.

[11] K. P. Bennet and E. J. Bredensteiner. Duality and geometry in svm classiﬁers. Proceedings of the 17th

International conference on Machine Learning  pages 57–64  2000.

[12] G. Wu  E. Y. Chang  and Z. Zhang. An analysis of transformation on non-positive semideﬁnite similarity
matrix for kernel machines. Proceedings of the 22nd International Conference on Machine Learning 
2005.

[13] H.-T. Lin and C.-J. Lin. A study on sigmoid kernel for svm and the training of non-psd kernels by

smo-type methods. 2003.

[14] A. Wo´znica  A. Kalousis  and M. Hilario. Distances and (indeﬁnite) kernels for set of objects. Proceedings

of the 6th International Conference on Data Mining  pages 1151–1156  2006.

[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[16] C. Gigola and S. Gomez. A regularization method for solving the ﬁnite convex min-max problem. SIAM

Journal on Numerical Analysis  27(6):1621–1634  1990.

[17] M. Overton. Large-scale optimization of eigenvalues. SIAM Journal on Optimization  2(1):88–120  1992.
[18] D. Bertsekas. Nonlinear Programming  2nd Edition. Athena Scientiﬁc  1999.
[19] J.-L. Gofﬁn and J.-P. Vial. Convex nondifferentiable optimization: A survey focused on the analytic center

cutting plane method. Optimization Methods and Software  17(5):805–867  2002.

[20] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis

and Machine Intelligence  16(5)  1994.
and D.J. Newman.

[21] A. Asuncion

of California 

sity
http://www.ics.uci.edu/∼mlearn/MLRepository.html.

School

Irvine 

UCI Machine Learning Repository.
of

and Computer

Information

Sciences 

Univer-
2007.

8

,Yuanjun Xiong
Wei Liu
Deli Zhao
Xiaoou Tang