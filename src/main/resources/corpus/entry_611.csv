2014,Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS),We present the first provably sublinear time hashing algorithm for approximate \emph{Maximum Inner Product Search} (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS  in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products  after independent asymmetric transformations  can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework  this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on $p$-stable distributions for $L_2$ norm (L2LSH)  in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.,Asymmetric LSH (ALSH) for Sublinear Time

Maximum Inner Product Search (MIPS)

Anshumali Shrivastava

Department of Computer Science
Computing and Information Science

Cornell University

Ithaca  NY 14853  USA

anshu@cs.cornell.edu

Ping Li

Department of Statistics and Biostatistics

Department of Computer Science

Rutgers University

Piscataway  NJ 08854  USA

pingli@stat.rutgers.edu

Abstract

We present the ﬁrst provably sublinear time hashing algorithm for approximate
Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner
product as the underlying similarity measure is a known difﬁcult problem and
ﬁnding hashing schemes for MIPS was considered hard. While the existing Lo-
cality Sensitive Hashing (LSH) framework is insufﬁcient for solving MIPS  in this
paper we extend the LSH framework to allow asymmetric hashing schemes. Our
proposal is based on a key observation that the problem of ﬁnding maximum in-
ner products  after independent asymmetric transformations  can be converted into
the problem of approximate near neighbor search in classical settings. This key
observation makes efﬁcient sublinear hashing scheme for MIPS possible. Under
the extended asymmetric LSH (ALSH) framework  this paper provides an exam-
ple of explicit construction of provably fast hashing scheme for MIPS. Our pro-
posed algorithm is simple and easy to implement. The proposed hashing scheme
leads to signiﬁcant computational savings over the two popular conventional LSH
schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable
distributions for L2 norm (L2LSH)  in the collaborative ﬁltering task of item rec-
ommendations on Netﬂix and Movielens (10M) datasets.

Introduction and Motivation

1
The focus of this paper is on the problem of Maximum Inner Product Search (MIPS). In this problem 

we are given a giant data vector collectionS of size N  whereS ⊂ RD  and a given query point
q∈ RD. We are interested in searching for p∈S which maximizes (or approximately maximizes)

the inner product qT p. Formally  we are interested in efﬁciently computing

(1)

p= arg max
x∈S
x∈Sq− x2
2= arg min
x∈S(x2

qT x

p= arg min

2− 2qT x)

The MIPS problem is related to near neighbor search (NNS)  which instead requires computing

These two problems are equivalent if the norm of every element x∈S is constant. Note that the
value of the normq2 has no effect as it is a constant and does not change the identity of arg max
the elements inS have signiﬁcant variations [13] and cannot be controlled  e.g.  (i) recommender

or arg min. There are many scenarios in which MIPS arises naturally at places where the norms of

system  (ii) large-scale object detection with DPM  and (iii) multi-class label prediction.
Recommender systems: Recommender systems are often based on collaborative ﬁltering which
relies on past behavior of users  e.g.  past purchases and ratings. Latent factor modeling based on
matrix factorization [14] is a popular approach for solving collaborative ﬁltering. In a typical matrix
factorization model  a user i is associated with a latent user characteristic vector ui  and similarly 
an item j is associated with a latent item characteristic vector vj. The rating ri j of item j by user i
is modeled as the inner product between the corresponding characteristic vectors.

(2)

1

In this setting  given a user i and the corresponding learned latent vector ui ﬁnding the right item j 
to recommend to this user  involves computing

over the norm of the learned vector  i.e. vj2  which often has a wide range in practice [13].

which is an instance of the standard MIPS problem. It should be noted that we do not have control

(3)

j= arg max
j′

ri j′= arg max
j′

i vj′
uT

(4)

test wi

If there are N items to recommend  solving (3) requires computing N inner products. Recommen-
dation systems are typically deployed in on-line application over web where the number N is huge.
A brute force linear scan over all items  for computing arg max  would be prohibitively expensive.
Large-scale object detection with DPM: Deformable Part Model (DPM) based representation of
images is the state-of-the-art in object detection tasks [8]. In DPM model  ﬁrstly a set of part ﬁlters
are learned from the training dataset. During detection  these learned ﬁlter activations over various
patches of the test image are used to score the test image. The activation of a ﬁlter on an image patch
is an inner product between them. Typically  the number of possible ﬁlters are large (e.g.  millions)
and so scoring the test image is costly. Recently  it was shown that scoring based only on ﬁlters with
high activations performs well in practice [7]. Identifying those ﬁlters having high activations on a
given image patch requires computing top inner products. Consequently  an efﬁcient solution to the
MIPS problem will beneﬁt large scale object detections based on DPM.
Multi-class (and/or multi-label) prediction: The models for multi-class SVM (or logistic regres-
sion) learn a weight vector wi for each of the class label i. After the weights are learned  given a
new test data vector xtest  predicting its class label is basically an MIPS problem:

whereL is the set of possible class labels. Note that the norms of the vectorswi2 are not constant.
The size L  of the set of class labels differs in applications. Classifying with large number of possi-
prediction task withL= 100  000 [7]. Computing such high-dimensional vector multiplications for

ble class labels is common in multi-label learning and ﬁne grained object classiﬁcation  for instance 

ytest= arg max
i∈L xT

predicting the class label of a single instance can be expensive in  e.g.  user-facing applications.
1.1 The Need for Hashing Inner Products
Solving the MIPS problem can have signiﬁcant practical impact. [19  13] proposed solutions based
on tree data structure combined with branch and bound space partitioning technique similar to k-d
trees [9]. Later  the same method was generalized for general max kernel search [5]  where the run-
time guarantees  like other space partitioning methods  are heavily dependent on the dimensionality
and the expansion constants. In fact  it is well-known that techniques based on space partitioning
(such as k-d trees) suffer from the curse of dimensionality. For example  [24] showed that techniques
based on space partitioning degrade to linear search  even for dimensions as small as 10 or 20.
Locality Sensitive Hashing (LSH) [12] based randomized techniques are common and successful
in industrial practice for efﬁciently solving NNS (near neighbor search). Unlike space partitioning
techniques  both the running time as well as the accuracy guarantee of LSH based NNS are in a way
independent of the dimensionality of the data. This makes LSH suitable for large scale processing
system dealing with ultra-high dimensional datasets which are common in modern applications.
Furthermore  LSH based schemes are massively parallelizable  which makes them ideal for modern
“Big” datasets. The prime focus of this paper will be on efﬁcient hashing based algorithms for
MIPS  which do not suffer from the curse of dimensionality.
1.2 Our Contributions
We develop Asymmetric LSH (ALSH)  an extended LSH scheme for efﬁciently solving the approxi-
mate MIPS problem. Finding hashing based algorithms for MIPS was considered hard [19  13]. We
formally show that  under the current framework of LSH  there cannot exist any LSH for solving
MIPS. Despite this negative result  we show that it is possible to relax the current LSH framework to
allow asymmetric hash functions which can efﬁciently solve MIPS. This generalization comes with
no extra cost and the ALSH framework inherits all the theoretical guarantees of LSH.
Our construction of asymmetric LSH is based on an interesting fact that the original MIPS problem 
after asymmetric transformations  reduces to the problem of approximate near neighbor search in

2

cS0-near neighbor of q in P .

classical settings. Based on this key observation  we provide an example of explicit construction of
asymmetric hash function  leading to the ﬁrst provably sublinear query time hashing algorithm for
approximate similarity search with (un-normalized) inner product as the similarity. The new ALSH
framework is of independent theoretical interest. We report other explicit constructions in [22  21].
We also provide experimental evaluations on the task of recommending top-ranked items with col-
laborative ﬁltering  on Netﬂix and Movielens (10M) datasets. The evaluations not only support our
theoretical ﬁndings but also quantify the obtained beneﬁt of the proposed scheme  in a useful task.
2 Background
2.1 Locality Sensitive Hashing (LSH)
A commonly adopted formalism for approximate near-neighbor search is the following:
Deﬁnition: (c-Approximate Near Neighbor or c-NN) Given a set of points in a D-dimensional space

similarity of interest. Popular techniques for c-NN are often based on Locality Sensitive Hashing
(LSH) [12]  which is a family of functions with the nice property that more similar objects in the
domain of these functions have a higher probability of colliding in the range space than less similar

RD  and parameters S0> 0  δ> 0  construct a data structure which  given any query point q  does
the following with probability 1− δ: if there exists an S0-near neighbor of q in P   it reports some
In the deﬁnition  the S0-near neighbor of point q is a point p with Sim(q  p)≥ S0  where Sim is the
ones. In formal terms  considerH a family of hash functions mapping RD to a setI.
Deﬁnition: (Locality Sensitive Hashing (LSH)) A familyH is called(S0  cS0  p1  p2)-sensitive if 
for any two point x  y∈ RD  h chosen uniformly fromH satisﬁes the following:
For efﬁcient approximate nearest neighbor search  p1> p2 and c< 1 is needed.
Fact 1 [12]: Given a family of(S0  cS0  p1  p2) -sensitive hash functions  one can construct a data
structure for c-NN with O(nρ log n) query time and space O(n1+ρ)  where ρ= log p1
[6] presented a novel LSH family for all Lp (p∈(0  2]) distances. In particular  when p= 2  this
a random vector a with each component generated from i.i.d. normal  i.e.  ai∼ N(0  1)  and a scalar
b generated uniformly at random from[0  r]. The hash function is deﬁned as:
whereææ is the ﬂoor operation. The collision probability under this scheme can be shown to be
2√
2π(r~d)1− e−(r~d)2~2
a b(x)= hL2
P r(hL2
where Φ(x) = ∫ x−∞ 1√
e− x2
tribution and d=x− y2 is the Euclidean distance between the vectors x and y. This collision
probability Fr(d) is a monotonically decreasing function of the distance d and hence hL2
previously x− y2=(x2
2− 2xT y) is not monotonic in the inner product xT y unless the
2+y2

• if Sim(x  y)≥ S0 then P rH(h(x)= h(y))≥ p1
• if Sim(x  y)≤ cS0 then P rH(h(x)= h(y))≤ p2

a b is an LSH
for L2 distances. This scheme is also the part of LSH package [1]. Here r is a parameter. As argued

scheme provides an LSH family for L2 distances. Formally  given a ﬁxed (real) number r  we choose

2 dx is the cumulative density function (cdf) of standard normal dis-

hL2

a b(x)=æ aT x+ b
æ
Fr(d)= 1− 2Φ(−r~d)−

r

a b(y))= Fr(d);

2.2 LSH for L2 Distance (L2LSH)

< 1.

log p2

(5)

(6)

2π

a b is not suitable for MIPS.

given data has a constant norm. Hence  hL2
The recent work on coding for random projections [16] showed that L2LSH can be improved when
the data are normalized for building large-scale linear classiﬁers as well as near neighbor search [17].
In particular  [17] showed that 1-bit coding (i.e.  sign random projections (SRP) [10  3]) or 2-bit
coding are often better compared to using more bits. It is known that SRP is designed for retrieving
. Again  ordering under this similarity can be very

with cosine similarity: Sim(x  y) =

x2y2

xT y

different from the ordering of inner product and hence SRP is also unsuitable for solving MIPS.

3

3 Hashing for MIPS
3.1 A Negative Result
We ﬁrst show that  under the current LSH framework  it is impossible to obtain a locality sensitive
hashing scheme for MIPS. In [19  13]  the authors also argued that ﬁnding locality sensitive hashing
for inner products could be hard  but to the best of our knowledge we have not seen a formal proof.

Theorem 1 There cannot exist any LSH family for MIPS.
Proof: Suppose there exists such hash function h. For un-normalized inner products the self similar-
2 and there may exist another points y  such that

ity of a point x with itself is Sim(x  x)= xT x=x2
Sim(x  y)= yT x>x2
2+ C  for any constant C. Under any single randomized hash function h 
the collision probability of the event{h(x)= h(x)} is always 1. So if h is an LSH for inner product
then the event{h(x)= h(y)} should have higher probability compared to the event{h(x)= h(x)} 
since we can always choose y with Sim(x  y)= S0+ δ> S0 and cS0> Sim(x  x)∀S0 and c< 1.
This is not possible because the probability cannot be greater than 1. This completes the proof. 

3.2 Our Proposal: Asymmetric LSH (ALSH)
The basic idea of LSH is probabilistic bucketing and it is more general than the requirement of
having a single hash function h. The classical LSH algorithms use the same hash function h for both
the preprocessing step and the query step. One assigns buckets in the hash table to all the candidates

x∈S using h  then uses the same h on the query q to identify relevant buckets. The only requirement
for the proof of Fact 1  to work is that the collision probability of the event{h(q)= h(x)} increases
with the similarity Sim(q  x). The theory [11] behind LSH still works if we use hash function h1
for preprocessing x∈S and a different hash function h2 for querying  as long as the probability of
the event{h2(q)= h1(x)} increases with Sim(q  x)  and there exist p1 and p2 with the required

(Preprocessing

(Query Transformation) and P ∶ RD  RD

′

property. The traditional LSH deﬁnition does not allow this asymmetry but it is not a required
condition in the proof. For this reason  we can relax the deﬁnition of c-NN without losing runtime
guarantees. [20] used a related (asymmetric) idea for solving 3-way similarity search.
We ﬁrst deﬁne a modiﬁed locality sensitive hashing in a form which will be useful later.

• if Sim(q  x)≥ S0 then P rH(h(Q(q)))= h(P(x)))≥ p1
• if Sim(q  x)≤ cS0 then P rH(h(Q(q))= h(P(x)))≤ p2

Deﬁnition: (Asymmetric Locality Sensitive Hashing (ALSH)) A familyH  along with the two
vector functions Q ∶ RD  RD
Transformation)  is called(S0  cS0  p1  p2)-sensitive if  for a given c-NN instance with query q and
any x in the collectionS  the hash function h chosen uniformly fromH satisﬁes the following:
When Q(x) = P(x) = x  we recover the vanilla LSH deﬁnition with h(.) as the required hash
function. Coming back to the problem of MIPS  if Q and P are different  the event{h(Q(x))=
h(P(x))} will not have probability equal to 1 in general. Thus  Q≠ P can counter the fact that self
{h(Q(q))= h(P(y))} to satisfy the conditions in the deﬁnition of c-NN for Sim(q  y)= qT y. Note
P is applied to x∈S while creating hash tables. It is this asymmetry which will allow us to solve
both q and x∈S is the same. Formally  it is not difﬁcult to show a result analogous to Fact 1.
Theorem 2 Given a family of hash functionH and the associated query and preprocessing trans-
formations P and Q  which is(S0  cS0  p1  p2) -sensitive  one can construct a data structure for
c-NN with O(nρ log n) query time and space O(n1+ρ)  where ρ= log p1
Without loss of any generality  let U < 1 be a number such thatxi2≤ U < 1  ∀xi∈S. If this is

MIPS efﬁciently. In Section 3.3  we explicitly show a construction (and hence the existence) of
asymmetric locality sensitive hash function for solving MIPS. The source of randomization h for

similarity is not highest with inner products. We just need the probability of the new collision event

that the query transformation Q is only applied on the query and the pre-processing transformation

3.3 From MIPS to Near Neighbor Search (NNS)

not the case then deﬁne a scaling transformation 

S(x)= U

M

× x; M= maxxi∈Sxi2;

(7)

′

.

log p2

4

while Q(x) simply appends m “1/2” to the end of the vector x. By observing that

Note that we are allowed one time preprocessing and asymmetry  S is the part of asymmetric trans-

pendent of the norm of the query. Later we show in Section 3.6 that it can be easily removed.
We are now ready to describe the key step in our algorithm. First  we deﬁne two vector transforma-

formation. For simplicity of arguments  let us assume thatq2= 1  the arg max is anyway inde-
tions P∶ RD RD+m and Q∶ RD RD+m as follows:
2; ....;x2m
2;x4
P(x)=[x;x2
2 ];
Q(x)=[x; 1~2; 1~2; ....; 1~2] 
where [;] is the concatenation. P(x) appends m scalers of the formx2i
2+ ...+xi2m
(xi2
2+xi4
Q(q)T P(xi)= qT xi+ 1
2+xi4
P(xi)2
2 );
2=xi2
2=(1+ m~4)− 2qT xi+xi2m+1
Q(q)− P(xi)2
x∈SQ(q)− P(x)2

Sincexi2 ≤ U < 1  xi2m+1 → 0  at the tower rate (exponential to exponential). The term
(1+ m~4) is a ﬁxed constant. As long as m is not too small (e.g.  m≥ 3 would sufﬁce)  we have

x∈S qT xࣃ arg min

we obtain the following key equality:

2+ ...+xi2m+1

2 at the end of the vector x 

arg max

(10)

(8)

(9)

2

2

2

.

2

This gives us the connection between solving un-normalized MIPS and approximate near neighbor
search. Transformations P and Q  when norms are less than 1  provide correction to the L2 distance

3.4 Fast Algorithms for MIPS
Eq. (10) shows that MIPS reduces to the standard approximate near neighbor search problem which

becomes negligible for any practical purposes. In fact  from theoretical perspective  since we are
interested in guarantees for c-approximate solutions  this additional error can be absorbed in the
approximation parameter c. Formally  we can state the following theorem.

Q(q)− P(xi)2 making it rank correlate with the (un-normalized) inner product. This works only
after shrinking the norms  as norms greater than 1 will instead blow the termxi2m+1
can be efﬁciently solved. As the error termxi2m+1
< U 2m+1 goes to zero at a tower rate  it quickly
Theorem 3 Given a c-approximate instance of MIPS  i.e.  Sim(q  x)= qT x  and a query q such
thatq2= 1 along with a collectionS havingx2≤ U < 1∀x∈S. Let P and Q be the vector
1) if qT x≥ S0 then P r[hL2
2) if qT x≤ cS0 then P r[hL2
Thus  we have obtained p1= Fr(1+ m~4)− 2S0+ U 2m+1 and p2= Fr(1+ m~4)− 2cS0.
Applying Theorem 2  we can construct data structures with worst case O(nρ log n) query time

a b(P(x))]≥ Fr
a b(P(x))]≤ Fr

1+ m~4− 2S0+ U 2m+1
1+ m~4− 2cS0

transformations deﬁned in (8). We have the following two conditions for hash function hL2

where the function Fr is deﬁned in (6).

a b (5)

2

a b(Q(q))= hL2
a b(Q(q))= hL2
ρ= log Fr
1+ m~4− 2S0+ U 2m+1
log Fr
1+ m~4− 2cS0
. Note that U 2m+1

We need p1> p2 in order for ρ< 1. This requires us to have−2S0+ U 2m+1 <−2cS0  which boils
down to the condition c< 1− U 2m+1
appropriate value of m. For any given c< 1  there always exist U< 1 and m such that ρ< 1. This
2π(r~d)1− e−(r~d)2~2. Thus  given a c-approximate MIPS instance  ρ
Fr(d)= 1− 2Φ(−r~d)−
2√

way  we obtain a sublinear query time algorithm for MIPS.
We also have one more parameter r for the hash function ha b. Recall the deﬁnition of Fr in Eq. (6):

can be made arbitrarily close to zero with the

2S0

2S0

guarantees for c-approximate MIPS  where

(11)

5

Figure 1: Left panel: Optimal values of ρ∗ with respect to approximation ratio c for different S0.
and c. Right Panel: ρ values (dashed curves) for m= 3  U= 0.83 and r= 2.5. The solid curves are
ρ∗ values. See more details about parameter recommendations in arXiv:1405.5869.

The optimization of Eq. (14) was conducted by a grid search over parameters r  U and m  given S0

U m r

(12)

U 2m+1

2S0

s.t.

∗

3.5 Practical Recommendation of Parameters

< 1− c  m∈ N+  0< U< 1.

instance we aim to solve  which requires knowing the similarity threshold S0 and the approximation

log Fr
1+ m~4− 2S0+ U 2m+1
log Fr
1+ m~4− 2cS0

is a function of 3 parameters: U  m  r. The algorithm with the best query time chooses U  m and r 
which minimizes the value of ρ. For convenience  we deﬁne

log n) query time and space O(n1+ρ

ρ∗= min
See Figure 1 for the plots of ρ∗. With this best value of ρ  we can state our main result in Theorem 4.
Theorem 4 (Approximate MIPS is Efﬁcient) For the problem of c-approximate MIPS withq2=
1  one can construct a data structure having O(nρ
∗)  where
ρ∗< 1 is the solution to constraint optimization (14).
Just like in the typical LSH framework  the value of ρ∗ in Theorem 4 depends on the c-approximate
ratio c. Since q2= 1 andx2≤ U < 1  ∀x∈S  we have qtx≤ U. A reasonable choice of the
threshold S0 is to choose a high fraction of U  for example  S0= 0.9U or S0= 0.8U.
The computation of ρ∗ and the optimal values of corresponding parameters can be conducted via a
grid search over the possible values of U  m and r. We compute ρ∗ in Figure 1 (Left Panel). For
convenience  we recommend m= 3  U = 0.83  and r= 2.5. With this choice of the parameters 
Figure 1 (Right Panel) shows that the ρ values using these parameters are very close to ρ∗ values.
3.6 Removing the Conditionq2= 1
Changing norms of the query does not affect the arg maxx∈C qT x. Thus in practice for retrieving top-
we want the runtime guarantee to be independent ofq2. We are interested in the c-approximate
Eq (7). Let the transformation S ∶ RD → RD be the ones deﬁned in Eq (7). Deﬁne asymmetric
transformations P′∶ RD→ RD+2m and Q′∶ RD→ RD+2m as
2; ....;x2m
2 ; 1~2; ...1~2]; Q′(x)=[x; 1~2; ....; 1~2;x2
P′(x)=[x;x2
2; ....;x2m
2 ] 
Given the query q and data point x  our new asymmetric transformations are Q′(S(q)) and
P′(S(x)) respectively. We observe that

Q′(S(q))− P′(S(x))2
2= m
≤ U 2m+1→ 0. Using exactly same arguments as before  we obtain
BothS(x)2m+1

instance which being a threshold based approximation changes if the query is normalized.
Previously  transformations P and Q were precisely meant to remove the dependency on the norms
of x. Realizing the fact that we are allowed asymmetry  we can use the same idea to get rid of the
norm of q. Let M be the upper bound on all the norms or the radius of the space as deﬁned in

2;x4
− 2qtx× U 2

+S(x)2m+1

2

+S(q)2m+1

2

ranked items  normalizing the query should not affect the performance. But for theoretical purposes 

 S(q)2m+1

2

2

2

2;x4

M 2

(13)

6

00.20.40.60.810.30.40.50.60.70.80.91cρ*S0 = 0.9US0 = 0.5U0.60.70.800.20.40.60.810.30.40.50.60.70.80.91cρS0 = 0.5U0.6S0 = 0.9U0.80.7m=3 U=0.83  r=2.5∗

min

u log n) query time and

∗

Theorem 5 (Unconditional Approximate MIPS is Efﬁcient) For the problem of c-approximate

MIPS in a bounded space  one can construct a data structure having O(nρ
u)  where ρ∗
space O(n1+ρ
u< 1 is the solution to constraint optimization (14).
log Fr
M 2+ 2U 2m+1
m~2− 2S0 U 2
< 1− c 
u=
log Fr
U(2m+1−2)M 2
ρ∗
m~2− 2cS0 U 2
M 2
0<U<1 m∈N r
u< 1. The theoretical guarantee only depends on the radius of the space M.
such that ρ∗
struct ALSH for new similaritiesS that we are interested in. The generic idea is to take a particular
similarity Sim(x  q) for which we know an existing LSH or ALSH. Then we construct transforma-
tions P and Q such Sim(P(x)  Q(q)) is monotonic in the similarityS that we are interested in.

3.7 A Generic Recipe for Constructing Asymmetric LSHs
We are allowed any asymmetric transformation on x and q. This gives us a lot of ﬂexibility to con-

Again  for any c-approximate MIPS instance  with S0 and c  we can always choose m big enough

s.t.

S0

(14)

The other observation that makes it easier to construct P and Q is that LSH based guarantees are
independent of dimensions  thus we can expand the dimensions like we did for P and Q.
This paper focuses on using L2LSH to convert near neighbor search of L2 distance into an ALSH
(i.e.  L2-ALSH) for MIPS. We can devise new ALSHs for MIPS using other similarities and hash
functions. For instance  utilizing sign random projections (SRP)  the known LSH for correlations 
we can construct different P and Q leading to a better ALSH (i.e.  Sign-ALSH) for MIPS [22]. We
are aware another work [18] which performs very similarly to Sign-ALSH. Utilizing minwise hash-
ing [2  15]  which is the LSH for resemblance and is known to outperform SRP in sparse data [23] 
we can construct an even better ALSH (i.e.  MinHash-ALSH) for MIPS over binary data [21].
4 Evaluations
Datasets. We evaluate the proposed ALSH scheme for the MIPS problem on two popular collabo-
rative ﬁltering datasets on the task of item recommendations: (i) Movielens(10M)  and (ii) Netﬂix.

Each dataset forms a sparse user-item matrix R  where the value of R(i  j) indicates the rating
i vj  ∀j. Despite its simplicity  PureSVD

of user i for movie j. Given the user-item ratings matrix R  we follow the standard PureSVD pro-
cedure [4] to generate user and item latent vectors. This procedure generates latent vectors ui for
each user i and vector vj for each item j  in some chosen ﬁxed dimension f. The PureSVD method
returns top-ranked items based on the inner products uT
outperforms other popular recommendation algorithms [4]. Following [4]  we use the same choices

for the latent dimension f  i.e.  f= 150 for Movielens and f= 300 for Netﬂix.

4.1 Ranking Experiment for Hash Code Quality Evaluations
We are interested in knowing  how the two hash functions correlate with the top-10 inner products.
For this task  given a user i and its corresponding user vector ui  we compute the top-10 gold
standard items based on the actual inner products uT
codes of the vector ui and all the item vectors vjs. For every item vj  we compute the number of
times its hash values matches (or collides) with the hash values of query which is user ui  i.e.  we

i vj ∀j. We then compute K different hash

t=1 1(ht(ui)= ht(vj))  based on which we rank all the items.

compute M atchesj=∑K

Figure 2 reports the precision-recall curves in our ranking experiments for top-10 items  for com-
paring our proposed method with two baseline methods: the original L2LSH and the original sign
random projections (SRP). These results conﬁrm the substantial advantage of our proposed method.
4.2 LSH Bucketing Experiment

We implemented the standard(K  L)-parameterized (where L is number of hash tables) bucketing
(K  L) on the evaluations  we report the result from the best performing K and L chosen from
K∈{5  6  ...  30} and L∈{1  2  ...  200} for each query. We use m= 3  U = 0.83  and r= 2.5 for

algorithm [1] for retrieving top-50 items based on PureSVD procedure using the proposed ALSH
hash function and the two baselines: SRP and L2LSH. We plot the recall vs the mean ratio of inner
product required to achieve that recall. The ratio being computed relative to the number of inner
products required in a brute force linear scan. In order to remove the effect of algorithm parameters

7

Figure 2: Ranking. Precision-Recall curves (higher is better)  of retrieving top-10 items  with the

number of hashes K∈{16  64  256}. The proposed algorithm (solid  red if color is available) sig-
niﬁcantly outperforms L2LSH. We ﬁx the parameters m= 3  U= 0.83  and r= 2.5 for our proposed
method and we present the results of L2LSH for all r values in{1  1.5  2  2.5  3  3.5  4  4.5  5}.

Figure 3: Bucketing. Mean number of inner products per query  relative to a linear scan  evalu-
ated by different hashing schemes at different recall levels  for generating top-50 recommendations
(Lower is better). The results corresponding to the best performing K and L (for a wide range of K
and L) at a given recall value  separately for all the three hashing schemes  are shown.

our hashing scheme. For L2LSH  we observe that using r= 4 usually performs well and so we show
results for r= 4. The results are summarized in Figure 3  conﬁrming that the proposed ALSH leads

to signiﬁcant savings compared to baseline hash functions.
5 Conclusion
MIPS (maximum inner product search) naturally arises in numerous practical scenarios  e.g.  col-
laborative ﬁltering. This problem is challenging and  prior to our work  there existed no provably
sublinear time hashing algorithms for MIPS. Also  the existing framework of classical LSH (locality
sensitive hashing) is not sufﬁcient for solving MIPS. In this study  we develop ALSH (asymmetric
LSH)  which generalizes the existing LSH framework by applying (appropriately chosen) asymmet-
ric transformations to the input query vector and the data vectors in the repository. We present an
implementation of ALSH by proposing a novel transformation which converts the original inner
products into L2 distances in the transformed space. We demonstrate  both theoretically and em-
pirically  that this implementation of ALSH provides provably efﬁcient as well as practical solution
to MIPS. Other explicit constructions of ALSH  for example  ALSH through cosine similarity  or
ALSH through resemblance (for binary data)  will be presented in followup technical reports.
Acknowledgments
The research is partially supported by NSF-DMS-1444124  NSF-III-1360971  NSF-Bigdata-
1419210  ONR-N00014-13-1-0764  and AFOSR-FA9550-13-1-0137. We appreciate the construc-
tive comments from the program committees of KDD 2014 and NIPS 2014. Shrivastava would also
like to thank Thorsten Joachims and the Class of CS6784 (Spring 2014) for valuable feedbacks.

8

020406080100051015Recall (%)Precision (%) MovielensTop 10  K = 16ProposedL2LSHSRP0204060801000102030Recall (%)Precision (%) MovielensTop 10  K = 64ProposedL2LSHSRP0204060801000204060Recall (%)Precision (%) MovielensTop 10  K = 256ProposedL2LSHSRP0204060801000246810Recall (%)Precision (%) NetFlixTop 10  K = 16ProposedL2LSHSRP02040608010005101520Recall (%)Precision (%) NetFlixTop 10  K = 64ProposedL2LSHSRP02040608010001020304050Recall (%)Precision (%) NetFlixTop 10  K = 256ProposedL2LSHSRP00.20.40.60.8100.20.40.60.81RecallFraction MultiplicationsTop 50Movielens ProposedSRPL2LSH00.20.40.60.8100.20.40.60.81RecallFraction MultiplicationsTop 50Netflix ProposedSRPL2LSHReferences
[1] A. Andoni and P. Indyk. E2lsh: Exact euclidean locality sensitive hashing. Technical report 

2004.

[2] A. Z. Broder. On the resemblance and containment of documents. In the Compression and

Complexity of Sequences  pages 21–29  Positano  Italy  1997.

[3] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  pages

380–388  Montreal  Quebec  Canada  2002.

[4] P. Cremonesi  Y. Koren  and R. Turrin. Performance of recommender algorithms on top-
In Proceedings of the fourth ACM conference on Recommender

n recommendation tasks.
systems  pages 39–46. ACM  2010.

[5] R. R. Curtin  A. G. Gray  and P. Ram. Fast exact max-kernel search. In SDM  pages 1–9  2013.
[6] M. Datar  N. Immorlica  P. Indyk  and V. S. Mirrokn. Locality-sensitive hashing scheme based

on p-stable distributions. In SCG  pages 253 – 262  Brooklyn  NY  2004.

[7] T. Dean  M. A. Ruzon  M. Segal  J. Shlens  S. Vijayanarasimhan  and J. Yagnik. Fast  accurate
In Computer Vision and Pattern

detection of 100 000 object classes on a single machine.
Recognition (CVPR)  2013 IEEE Conference on  pages 1814–1821. IEEE  2013.

[8] P. F. Felzenszwalb  R. B. Girshick  D. McAllester  and D. Ramanan. Object detection with
discriminatively trained part-based models. Pattern Analysis and Machine Intelligence  IEEE
Transactions on  32(9):1627–1645  2010.

[9] J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis.

IEEE Transactions on Computers  23(9):881–890  1974.

[10] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of ACM  42(6):1115–
1145  1995.

[11] S. Har-Peled  P. Indyk  and R. Motwani. Approximate nearest neighbor: Towards removing

the curse of dimensionality. Theory of Computing  8(14):321–350  2012.

[12] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of

dimensionality. In STOC  pages 604–613  Dallas  TX  1998.

[13] N. Koenigstein  P. Ram  and Y. Shavitt. Efﬁcient retrieval of recommendations in a matrix

factorization framework. In CIKM  pages 535–544  2012.

[14] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems.
[15] P. Li and A. C. K¨onig. Theory and applications b-bit minwise hashing. Commun. ACM  2011.
[16] P. Li  M. Mitzenmacher  and A. Shrivastava. Coding for random projections. In ICML  2014.
[17] P. Li  M. Mitzenmacher  and A. Shrivastava. Coding for random projections and approximate

near neighbor search. Technical report  arXiv:1403.8144  2014.

[18] B. Neyshabur and N. Srebro. A simpler and better lsh for maximum inner product search

(mips). Technical report  arXiv:1410.5518  2014.

[19] P. Ram and A. G. Gray. Maximum inner-product search using cone trees.

931–939  2012.

In KDD  pages

[20] A. Shrivastava and P. Li. Beyond pairwise: Provably fast algorithms for approximate k-way

similarity search. In NIPS  Lake Tahoe  NV  2013.

[21] A. Shrivastava and P. Li. Asymmetric minwise hashing. Technical report  2014.
[22] A. Shrivastava and P. Li. An improved scheme for asymmetric lsh. Technical report 

arXiv:1410.5410  2014.

[23] A. Shrivastava and P. Li. In defense of minhash over simhash. In AISTATS  2014.
[24] R. Weber  H.-J. Schek  and S. Blott. A quantitative analysis and performance study for

similarity-search methods in high-dimensional spaces. In VLDB  pages 194–205  1998.

9

,Anshumali Shrivastava
Ping Li
Louis Kirsch
Julius Kunze
David Barber