2018,Complex Gated Recurrent Neural Networks,Complex numbers have long been favoured for digital signal processing  yet
complex representations rarely appear in deep learning architectures. RNNs  widely
used to process time series and sequence information  could greatly benefit from
complex representations. We present a novel complex gated recurrent cell  which
is a hybrid cell combining complex-valued and norm-preserving state transitions
with a gating mechanism. The resulting RNN exhibits excellent stability and
convergence properties and performs competitively on the synthetic memory and
adding task  as well as on the real-world tasks of human motion prediction.,Complex Gated Recurrent Neural Networks

Moritz Wolter

Institute for Computer Science

University of Bonn

wolter@cs.uni-bonn.de

Angela Yao

School of Computing

National University of Singapore

yaoa@comp.nus.edu.sg

Abstract

Complex numbers have long been favoured for digital signal processing  yet
complex representations rarely appear in deep learning architectures. RNNs  widely
used to process time series and sequence information  could greatly beneﬁt from
complex representations. We present a novel complex gated recurrent cell  which
is a hybrid cell combining complex-valued and norm-preserving state transitions
with a gating mechanism. The resulting RNN exhibits excellent stability and
convergence properties and performs competitively on the synthetic memory and
adding task  as well as on the real-world tasks of human motion prediction.

1 Introduction

Recurrent neural networks (RNNs) are widely used for processing time series and sequential infor-
mation. The difﬁculties of training RNNs  especially when trying to learn long-term dependencies 
are well-established  as RNNs are prone to vanishing and exploding gradients [2  12  31]. Heuristics
developed to alleviate some of the optimization instabilities and learning difﬁculties include gradient
clipping [9  29]  gating [4  12]  and using norm-preserving state transition matrices [1  13  16  40].
Gating  as used in gated recurrent units (GRUs) [4] and long short-term memory (LSTM) net-
works [12]  has become common-place in recurrent architectures. Gates facilitate the learning of
longer term temporal relationships [12]. Furthermore  in the presence of noise in the input signal 
gates can protect the cell state from undesired updates  thereby improving overall stability and
convergence.
A matrix W is norm-preserving if its repeated multiplication with a vector leaves the vector norm
unchanged  i.e.(cid:107)Wh(cid:107)2 = (cid:107)h(cid:107)2. Norm-preserving state transition matrices are particularly interesting
for RNNs because they preserve gradients over time [1]  thereby preventing both the vanishing
and exploding gradient problem. To be norm-preserving  state transition matrices need to be either
orthogonal or unitary1. Complex numbers have long been favored for signal processing [11  24  27]. A
complex signal does not simply double the dimensionality of the signal. Instead  the representational
richness of complex signals is rooted in its physical relevance and the mathematical theory of
complex analysis. Complex arithmetic  and in particular multiplication  is different from its real
counterpart and allows us to construct novel network architectures with several desirable properties.
Despite networks being complex-valued  however  it is often necessary to work with real-valued cost
functions and/or existing real-valued network components. Mappings from C → R are therefore
indispensable. Unfortunately such functions violate the Cauchy-Riemann equations and are not
complex-differentiable in the traditional sense. We advocate the use of Wirtinger calculus [39] (also
known as CR-calculus [21])  which makes it possible to deﬁne complex (partial) derivatives  even
when working with non-holomorph or non-analytic functions.

1Unitary matrices are the complex analogue of orthogonal matrices  i.e. a complex matrix W is unitary if

WW

T

T

= W

W = I  where W

T is its conjugate transpose and I is the identity matrix.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Complex-valued representations have begun receiving some attention in the the deep learning
community but they have been applied only to the most basic of architectures [1  10  36]. For
recurrent networks  complex representations could gain more acceptance if they were shown to be
compatible with more commonly used gated architectures and also competitive for real-world data.
This is exactly the aim of this work  where we propose a complex-valued gated recurrent network
and show how it can easily be implemented with a standard deep learning library such as TensorFlow.
Our contributions can be summarized as follows2:

• We introduce a novel complex-gated recurrent unit; to the best of our knowledge  we are the

ﬁrst to explore such a structure using complex number representations.

• We compare experimentally the effects of a bounded versus unbounded non-linearity in
recurrent networks  ﬁnding additional evidence countering the commonly held heuristic
that only bounded non-linearities should be applied in RNNs. In our case unbounded
non-linearities perform better  but must be coupled with the stabilizing measure of using
norm-preserving state transition matrices.

• Our complex gated network is stable and fast to train; it outperforms the state of the art
with equal parameters on synthetic tasks and delivers state-of-the-art performance one the
real-world application of predicting poses in human motion capture using fewer weights.

2 Related work

The current body of literature in deep learning focuses predominantly on real-valued neural net-
works. Theory for learning with complex-valued data  however  was established long before the
breakthroughs of deep learning. This includes the development of complex non-linearities and
activation functions [7  18]  the computation of complex gradients and Hessians [37]  and complex
backpropagation [3  23].
Complex-valued representations were ﬁrst used in deep networks to model phase dependencies for
more biologically plausible neurons [33] and to augment the memory of LSTMs [5]  i.e. whereby half
of the cell state is interpreted as the imaginary component. In contrast  true complex-valued networks
(including this work) have not only complex valued states but also kernels. Recently  complex CNNs
have been proposed as an alternative for classifying natural images [10  36] and inverse mapping of
MRI signals [38]. Complex CNNs were found to be competitive or better than state-of-the-art [36]
and signiﬁcantly less prone to over-ﬁtting [10].
For temporal sequences  complex-valued RNNs have also been explored [1  13  17  40]  though
interest in complex representations stems from improved learning stability. In [1]  norm-preserving
state transition matrices are used to prevent vanishing and exploding gradients. Since it is difﬁcult
to parameterize real-valued orthogonal weights  [1] recommends shifting to the complex domain 
resulting in a unitary RNN (uRNN). The weights of the uRNN in [1]  for computational efﬁciency 
are constructed as a product of component unitary matrices. As such  they span only a reduced
subset of unitary matrices and do not have the expressiveness of the full set. Alternative methods of
parameterizing the unitary matrices have been explored [13  17  40]. Our proposed complex gated
RNN (cgRNN) builds on these works in that we also use unitary state transition matrices. In particular 
we adopt the parameterization of [40] in which weights are parameterized by full-dimensional unitary
matrices  though any of the other parameterizations [1  13  17] can also be substituted.

3 Preliminaries
We represent a complex number z ∈ C as z = x + ib  where x = (cid:60)(z) and y = (cid:61)(z) are the real
and imaginary parts respectively. The complex conjugate of z is ¯z = x − iy. In polar coordinates 
z can be expressed as z = |z|eiθz  where |z| and θ are the magnitude and phase respectively and
θz = atan2(x  y). Note that z1 · z2 = |z1||z2|ei(θ1+θ2)  z1 + z2 = x1 + x2 + i(y1 + y2) and
s · z = s · reiθ  s ∈ R. The expression s · z scales z’s magnitude  while leaving the phase intact.

2Source code available at https://github.com/v0lta/Complex-gated-recurrent-neural-networks

2

3.1 Complex Gradients
A complex-valued function f : C → C can be expressed as f (z) = u(x  y) + iv(x  y) where u(· ·)
and v(· ·) are two real-valued functions. The complex derivative of f (z)  or the C-derivative  is
deﬁned if and only if f is holomorph. In such a case  the partial derivatives of u and v must not only
exist but also satisfy the Cauchy-Riemann equations  where ∂u/∂x = ∂v/∂y and ∂v/∂x = −∂u/∂y.
Strict holomorphy can be overly stringent for deep learning purposes. In fact  Liouville’s theorem [25]
states that the only complex function which is both holomorph and bounded is a constant function.
This implies that for complex (activation) functions  one must trade off either boundedness or
differentiability. One can forgo holomorphy and still leverage the theoretical framework of Wirtinger
or CR-calculus [21  27] to work separately with the R- and R- derivatives3:
|z=const=

∂f
∂y
Based on these derivatives  one can deﬁne the chain rule for a function g(f (z)) as follows:

)  R-derivative (cid:44) ∂f
∂ ¯z

R-derivative (cid:44) ∂f
∂z

|¯z=const=

∂f
∂x

∂f
∂x

∂f
∂y

− i

1
2

(

1
2

(

+ i

).

(1)

∂g(f (z))

∂g
∂f

∂f
∂z

∂g
∂ ¯f

∂ ¯f
∂z

=

∂z

(2)
Since mappings from C → R can generally be expressed in terms of the complex variable z and its
conjugate ¯z  the Wirtinger-Calculus allows us to formulate and theoretically understand the gradient
of real-valued loss functions in an easy yet principled way.

where

+

¯f = u(x  y) − iv(x  y).

3.2 A Split Complex Approach

We work with a split-complex approach  where real-valued non-linear activations are applied sepa-
rately to the real and imaginary parts of the complex number. This makes it convenient for imple-
mentation  since standard deep learning libraries are not designed to work natively with complex
representations. Instead  we store complex numbers as two real-valued components. Split-complex
activation functions process either the magnitude and phase  or the real and imaginary components
with two real-valued nonlinear functions and then recombine the two into a new complex quantity.
While some may argue this reduces the utility of having complex representations  we prefer this to
fully complex activations. Fully complex non-linearities do exist and may seem favorable [36]  since
one needs to keep track of only the R derivatives  but due to Liouville’s theorem  we must forgo
boundedness and then deal with forward pass instabilities.

4 Complex Gated RNNs

4.1 Basic Complex RNN Formulation

Without any assumptions on real versus complex representations  we deﬁne a basic RNN as follows:
(3)
(4)
where xt and ht represent the input and hidden unit vectors at time t. fa is a point-wise non-linear
activation function  and W and V are the hidden and input state transition matrices respectively.
In working with complex networks  xt ∈ Cnx×1  ht ∈ Cnh×1  W ∈ Cnh×nh  V ∈ Cnh×nx and
b ∈ Cnh×1  where nx and nh are the dimensionalities of the input and hidden states respectively.

zt = Wht−1 + Vxt + b
ht = fa(zt)

4.2 Complex Non-linear Activation Functions

Choosing a non-linear activation function fa for complex networks can be non-trivial. Though
holomorph non-linearities using transcendental functions have also been explored in the literature [27] 
the presence of singularities makes them difﬁcult to learn in a stable manner. Instead  bounded non-
holomorph non-linearities tend to be favoured [11  27]  where bounded real-valued non-linearities
are applied on the real and imaginary part separately. This also parallels the convention of using
(bounded) tanh non-linearities in real RNNs.

3For holomorph functions the R-derivative is zero and the C- derivative is equal to the R-derivative.

3

Hirose

ModRelu

|
)
z
(
f
|

1

0.8

0.6

0.4

0.2

0
2

|
)
z
(
f
|

2.5

2

1.5

1

0.5

0
2

1

0

(cid:61)(z)

−1

−2

−2

−1

0
(cid:60)(z)

2

1

1

0

(cid:61)(z)

−1

−1

0
(cid:60)(z)

−2

−2

2

1

Figure 1: Surface plots of the magnitude of the Hirose (m2 = 1) and modReLU (b =−0.5) activations.

A common split is with respect to the magnitude and phase. This non-linearity was popularized by
Hirose [11] and scales the magnitude by a factor m2 before passing it through a tanh:

(cid:18) |z|

(cid:19)

m2

(cid:18) |z|

(cid:19) z

m2

|z| .

fHirose(z) = tanh

e−i·θz = tanh

(5)

(7)

In other areas of deep learning  the rectiﬁed linear unit (ReLU) is now the go-to non-linearity. In
comparison to sigmoid or tanh activations  they are computationally cheap  expedite convergence [22]
and also perform better [30  26  42]. However  there is no direct extension into the complex domain 
and as such  modiﬁed versions have been proposed [10  38]. The most popular is the modReLU [1] –
a variation of the Hirose non-linearity  where the tanh is replaced with a ReLU and b is an offset:

fmodReLU(z) = ReLU(|z| + b)e−i·θz = ReLU(|z| + b)

z
|z| .

(6)

4.3 R → C input and C → R output mappings
While several time series problems are inherently complex  especially when considering their Fourier
representations  the majority of benchmark problems in machine learning are still only deﬁned in
the real number domain. However  one can still solve these problems with complex representations 
since a real z has simply a zero imaginary component  i.e.(cid:61)(z) = 0 and z = x + i · 0.
To map the complex state h into a real output or  we use a linear combination of the real and
imaginary components  similar to [1]  with Wo and bo as weights and offset:

or = Wo[(cid:60)(h) (cid:61)(h)] + bo.

4.4 Optimization on the Stiefel Manifold for Norm Preservation

In [1]  it was proven that a unitary 4 W would prevent vanishing and exploding gradients of the cost
function C with respect to ht  since the gradient magnitude is bounded. However  this proof hinges
on the assumption that the derivative of fa is also unity. This assumption is valid if the pre-activations
are real and one chooses the ReLU as the non-linearity. For complex pre-activations  however  this
is no longer a valid assumption. Neither the Hirose non-linearity (Equation 5) nor the modReLU
(Equation 6) can guarantee stability (despite the suggestion otherwise in the original proof [1]).
Even though it is not possible to guarantee stability  we strongly advocate using norm-preserving
state transition matrices  since they do still have excellent stabilizing effects. This was proven
experimentally in [1  13  40] and we ﬁnd similar evidence in our own experiments (see Figure 2).
Ensuring that W remains unitary during the optimization can be challenging  especially since the
group of unitary matrices is not closed under addition. As such  it is not possible to learn W with
4Since R ⊆ C  we use the term unitary to refer to both real orthogonal and complex unitary matrices and

make a distinction for clarity purposes only where necessary.

4

standard update-based gradient descent. Alternatively  one can learn W on the Stiefel manifold [40] 
with the k + 1 update Wk+1 given as follows by [34]  where λ is the learning rate  I the identity
matrix  and F the cost function:

Wk+1 = (I +

λ
2

Ak)−1(I − λ
2

Ak)Wk

where

A = W∇wF

T − W

T∇wF.

(8)

4.5 Complex-Valued Gating Units

In keeping with the spirit that gates determine the amount of a signal to pass  we construct a complex
gate as a Cnh×nh → Rnh×1 mapping. Like in real gated RNNs  the gate is applied as an element-
wise product  i.e.g (cid:12) h = g (cid:12) |h|eiθh. In our complex case  this type of operation results in an
element-wise scaling of the hidden state’s magnitude. When the gate is 0  it completely resets a
signal  whereas if it is 1  then it ensures that the signal is passed entirely. We introduce our gates into
the RNN in a similar fashion as the classic GRU [4]:

(cid:101)zt = W(gr (cid:12) ht−1) + Vxt + b 
ht = gz (cid:12) fa((cid:101)zt) + (1 − gz) (cid:12) ht−1 

(9)
(10)
where gr and gz represent reset and update gates respectively and are deﬁned with corresponding
subscripts r and z as

gr = fg(zr) 
gz = fg(zz) 

(11)
(12)
Above  fg denotes the gate activation  Wr ∈ Cnh×nh and Wz ∈ Cnh×nh denote state to state
transition matrices  Vr ∈ Cnh×ni and Vz ∈ Cnh×ni the input to state transition matrices  and
br ∈ Cnh and bz ∈ Cnh the biases. fg is a non-linear gate activation function deﬁned as:

zr = Wrh + Vrxt + br 
zz = Wzh + Vzxt + bz.

where
where

f mod sigmoid(z) = σ(α(cid:60)(z) + β(cid:61)(z)) 

α  β ∈ [0  1].

(13)

We call this the modSigmoid and justify the choice experimentally in section 5.3.
As mentioned previously  even with unitary state transition matrices  this type of gating is not
mathematically guaranteed to be stable. However  the effects of vanishing gradients are mitigated by
the fact that the derivatives are distributed over a sum [12  4]. Exploding gradients are clipped.

5 Experimentation

5.1 Tasks & Evaluation Metrics

We test our cgRNN on two benchmark synthetic tasks: the memory problem and the adding prob-
lem [12]. These problems are designed especially to challenge RNNs  and require the networks to
store information over time scales on the order of hundreds of time steps. The ﬁrst is the memory
problem  where the RNN should remember n input symbols over a time period of length T + 2n
based on a dictionary set {s1  s2  ...  sn  sb  sd}  where s1 to sn are symbols to memorize and sb and
si are blank and delimiter symbols respectively. The input sequence  of length T + 2n  is composed
of n symbols drawn randomly with replacement from {s1  ...  sn}  followed by T − 1 repetitions
of sb  sd  and another n repetitions of sb. The objective of the RNN  after being presented with the
initial n symbols  is to generate an output sequence of length T + S  with T repetitions of sb  and
upon seeing sd  recall the original n input symbols. A network without memory would output sb and
once presented with sd  randomly predict any of the original n symbols; this results in a categorical
cross entropy of (n + 1 log(8))/(T + 2(n + 2)). For our experiments  we choose n = 8 and T = 250.
In the adding problem  two sequences of length T are given as input  where the ﬁrst sequence
consists of numbers randomly sampled from U[0  1]5  while the second is an indicator sequence of
all 0(cid:48)s and exactly two 1(cid:48)s  with the ﬁrst 1 placed randomly in the ﬁrst half of the sequence and the
second 1 randomly in the second half. The objective of the RNN is to predict the sum of the two
entries of the ﬁrst input sequence once the second 1 is presented in the indicator input sequence. A
5Note that this is a variant of [12]’s original adding problem  which draws numbers from U[−1  1] and used

three indicators {−1  0  1}. Our variant is consistent with state-of-the-art [1  13  40]

5

naive baseline would predict 1 at every time step  regardless of the input indicator sequence’s value;
this produces an mean squared error (MSE) of 0.167  i.e. the variance of the sum of two independent
uniform distributions. For our experiments  we choose T = 250.
We apply the cgRNN to the real-world task of human motion prediction  i.e. predicting future 3D
poses of a person given the past motion sequence. This task is of interest to diverse areas of research 
including 3D tracking in computer vision [41]  motion synthesis for graphics [20] as well as pose
and action predictions for assistive robotics [19]. We follow the same experimental setting as [28] 
working with the full Human 3.6M dataset [14]. For training  we use six of the seven actors and
test on actor ﬁve. We use the pre-processed data of [15]  which converts the motion capture into
exponential map representations of each joint. Based on an input sequence of body poses from 50
frames  the future 10 frames are predicted. This is equivalent of predicting 400ms. The error is
measured by the euclidean distance in Euler angles with respect to the ground truth poses.
We also test the cgRNN on native complex data drawn from the frequency domain by testing it on the
real world task of music transcription. Given a music wave form ﬁle  the network should determine
the notes of each instrument. We use the Music-Net dataset [35]  which consists of 330 classical
music recordings  of which 327 are used for training and 3 are held out for testing. Each recording 
sampled at 11kHz  is divided into segments of 2048 samples with a step size of 512 samples. The
transcription problem is deﬁned as a multi-label classiﬁcation problem  where for each segment  a
label vector y ∈ 0  1128 describing the active keys in the corresponding midi ﬁle has to be found. We
use the windowed Fourier-transform of each segment as network input  the real and imaginary parts
of the Fourier transform  i.e.the odd and even components respectively  are used directly as inputs
into the cgRNN.

5.2 RNN Implementation Details

We work in Tensorﬂow  using RMS-prop to update standard weights and the multiplicative Stiefel-
manifold update as described Equation 8 for all unitary state transition matrices. The unitary state
transition matrices are initialized the same as [1] as the product of component unitary matrices. All
other weights are initialized using the uniform initialisation method recommended in [8]  i.e. U [−l  l]

with l =(cid:112)6/(nin + nout)  where nin and nout are the input and output dimensions of the tensor

to be initialised. All biases are intialized as zero  with the exception of the gate biases br and bz 
which are initialized at 4 to ensure fully open gates and linear behaviour at the start of training. All
synthetic tasks are run for 2 · 104 iterations with a batch-size of 50 and a constant learning rate of
0.001 for both the RMS-Prop and the Stiefel-Manifold updates.
For the human motion prediction task  we adopt the state-of-the-art implementation of [28]  which
introduces residual velocity connections into the standard GRU. Our setup shares these modiﬁcations;
we simply replace their core GRU cell with our cgRNN cell. The learning rate and batch size are
kept the same (0.005  16) though we reduce our state size to 512 to be compatible with [28]’s 10246.
For music transcription  we work with a bidirectional cgRNN encoder followed by a simple cgRNN
decoder. All cells are set with nh = 1024; the learning rate is set to 0.0001 and batch size to 5.

5.3

Impact of Gating and Choice of Gating Functions

We ﬁrst analyse the impact that gating has on the synthetic tasks by comparing our cgRNN with the
gateless uRNN from [1]. Both networks use complex representations and also unitary state transition
matrices. As additional baselines  we also compare with TensorFlow’s out-of-the-box GRU. We
choose the hidden state size nh of each network to ensure that the resulting number of parameters
is approximately equivalent (around 44k). We ﬁnd that our cgRNN successfully solves both the
memory problem as well as the adding problem. On the memory problem (see Figure 2(a)  Table 1) 
gating does not play a role. Instead  having norm-preserving weight matrices is key to ensure stability
during the learning. The GRU  which does not have norm-preserving state matrices  is highly unstable
and fails to solve the problem. Our cgRNN achieves very similar performance as the uRNN. This has
to do with the fact that we initialize our gate bias term to be fully open  i.e. gr = 1  gz = 1. Under this
setting  the formulation is the same as the uRNN  and the unitary W dominates the cell’s dynamics.

6This reduction is larger than necessary – parameter-wise  the equivalent state size is

6

(cid:113)

10242

2 ≈ 724

memory problem

adding problem

0.2

0.15

0.1

y
p
o
r
t
n
e

s
s
o
r
c

5 · 10−2

0.2

0.15

e
s

m

0.1

5 · 10−2

cgRNN
uRNN
GRU

cgRNN
uRNN
GRU

0

0

0.5

1

updates

1.5

2·104

(a)

0

0

0.5

1

updates

1.5

2·104

(b)

Figure 2: Comparison of our cgRNN (blue  nh = 80) with the uRNN [1] (orange  nh = 140) and standard
GRU [4] (green  nh = 112) on the memory (a) and adding (b) problem for T = 250. The hidden state size nh for
each network are chosen so as to approximately match the number of parameters (approximately 44k parameters
total). On the memory problem  having norm-preserving state transition matrices is critical for stable learning 
while on the adding problem  having gates is important. Figure best viewed in colour.

For the adding problem  previous works [1  13  40] have suggested that gates are beneﬁcial and we
conﬁrm this result in Figure 2(b) and Table 1. We speculate that the advantage comes from the gates
shielding the network from the irrelevant inputs of the adding problem  hence the success of our
cgRNN as well as the GRU  but not the uRNN. Surprisingly  the standard GRU baseline  without
any norm-preserving state transition matrices works very well on the adding problem; in fact  it
marginally outperforms our cgRNN. However  we believe this result does not speak to the inferiority
of complex representations; instead  it is likely that the adding problem  as a synthetic task  is not
able to leverage the advantages offered by the representation.
The gating function (Equation 13) was selected experimentally based on a systematic comparison of
various functions. The performance of different gate functions are compared statistically in Table 1 
where we look at the fraction of converged experiments over 20 runs as well as the mean number of
iterations required until convergence. The product as well as the tied and free weighted sum variations
of the gating function are designed to resemble the bilinear gating mechanism used in [6]. From our
experiments  we ﬁnd that it is important to scale the real and imaginary components before passing
through the sigmoid to leverage the saturation constraint  and that the real and imaginary components
should be combined linearly. The exact weighting seems not to be important and the best performing

Table 1: Comparison of gating functions on the adding and memory problems.

memory problem

adding problem

uRNN [40]
product
tied 1
tied 2
free

N
N
R
g
c

free real

frac.conv.
gating function
1.0
no gate
σ((cid:60)(z))σ((cid:61)(z))
0.10
ασ((cid:60)(z)) + (1−α)σ((cid:61)(z))
0.55
σ(α(cid:60)(z) + (1−α)(cid:61)(z))
0.80
σ(α(cid:60)(z) + β(cid:61)(z))
0.75
σ(αz1 + βz2)  (z1  z2) ∈ R 0.0

avg.iters.
2235
4625
4186
3800
2850
-

frac.conv.
0.0
1.0
1.0
1.0
1.0
1.0

avg.iters.
-
4245
5458
5070
5235
5313

The different gates are evaluated over 20 runs by looking at the fraction of convergence (frac.conv.) and average
number of iterations required for convergence (avg.iters.) if convergent. A run is considered convergent if the
loss falls below 5·10−7 for the memory problem and 0.01 for the adding problem. We ﬁnd that gating has no
impact for the memory problem  i.e. the gateless uRNN [40] always converges  but is necessary for the adding
problem. All experiments use weight normalized recurrent weights  a cell size of nh = 80  and have networks
with approximately 44k parameters; to keep approximately the same number of parameters  we set nh = 140
for the uRNN and two independent gates each with nh = 90 for the real free real case.

7

memory problem

adding problem

0.2

0.15

0.1

y
p
o
r
t
n
e

s
s
o
r
c

5 · 10−2

norm. modRelu
norm. Hirose
non-norm. modRelu
non-norm. Hirose

0.2

0.15

e
s

m

0.1

5 · 10−2

0

0

0.5

1

updates

1.5

2·104

(a)

0

0

0.5

norm. modRelu
norm. Hirose
non-norm. modRelu
non-norm. Hirose

1

updates

1.5

2·104

(b)

Figure 3: Comparison of non-linearities and norm preserving state transition matrices on the cgRNNs for the
memory (a) and adding (b) problems for T=250. The unbounded modReLU (see Equation 6) performs best for
both problems  but only if the state transition matrices are kept unitary. Without unitary state-transition matrices 
the bounded Hirose non-linearity (see Equation 5) performs better. We use nh = 80 for all experiments.

variants are the tied 2 and the free; to preserve generality  we advocate the use of the free variant.
We note that over 20 runs  our cgRNN converged only on 15-16 runs; adding the gates introduces
instabilities  however  we ﬁnd the ability to solve the adding problem a reasonable trade-off.
Finally  we compare the cgRNN to a free real variant (see last row of Table 1)  which is the most
similar architecture in R  i.e.normalized hidden transition matrices  same gate formulation  and two
independently real-valued versions of Equations 11 and 12. This real variant has similar performance
on the adding problem (for which having gates is critical)  but cannot solve the memory problem.
This is likely due to the set of real orthogonal matrices being too restrictive  making the problem
more difﬁcult in the real domain than the complex.

5.4 Non-Linearity Choice and Norm Preservation

We compare the bounded Hirose tanh non-linearity versus the unbounded modReLU (see Section 4.2)
in our cgRNN in Figure 3 and discover a strong interaction effect from the norm-preservation. First 
we ﬁnd that optimizing on the Stiefel manifold to preserve norms for the state transition matrices
signiﬁcantly improves learning  regardless of the non-linearity. In both the memory and the adding
problem  keeping the state transition matrices unitary ensures faster and smoother convergence of the
learning curve.
Without unitary state transition matrices  the bounded tanh non-linearity  i.e.the conventional choice
is better than the unbounded modReLU. However  with unitary state transition matrices  the modReLU
pulls ahead. We speculate that the modReLU  like the ReLU in the real setting  is a better choice of
non-linearity. The advantages afforded upon it by being unbounded  however  also makes it more
sensitive to stability  which is why these advantages are present only when the state-transition matrices
are kept unitary. Similar effects were observed in real RNNs in [32]  in which batch normalization
was required in order to learn a standard RNN with the ReLU non-linearity.

5.5 Real World Tasks: Human Motion Prediction & Music Transcription

We compare our cgRNN to the state of the art GRU proposed by [28] on the task of human motion
prediction  showing the results in Table 2. Our cgRNN delivers state-of-the-art performance  while
reducing the number of network parameters by almost 50%. However this reduction comes at the
cost of having to compute the matrix inverse in Equation 8.
On the music transcription task  we are able to accurately transcribe the input signals with an accuracy
of 53%. While this falls short of the complex convolutional state-of-the-art 72.9% of [36]  their

8

complex convolution-based network is fundamentally different from our approach. We conclude that
our cgRNN is able to extract meaningful information from complex valued input data and will look
into integrating complex convolutions into our RNN as future work.

Table 2: Comparison of our cgRNN with the GRU [28] on human motion prediction.

cgRNN

GRU[28]

Action
walking
eating
smoking
discussion
directions
greeting
phoning
posing
purchases
sitting
sitting down
taking photo
waiting
walking dog
walking together
average

80ms
0.29
0.23
0.31
0.33
0.41
0.53
0.58
0.37
0.61
0.46
0.55
0.29
0.35
0.57
0.27
0.41

160 ms
0.48
0.38
0.58
0.72
0.65
0.87
1.09
0.72
0.86
0.75
1.02
0.59
0.68
1.09
0.53
0.73

320ms
0.74
0.66
1.01
1.02
0.83
1.26
1.57
1.38
1.21
1.22
1.54
0.92
1.16
1.45
0.77
1.12

400ms
0.84
0.82
1.1
1.08
0.93
1.43
1.72
1.65
1.31
1.44
1.73
1.07
1.36
1.55
0.86
1.26

80ms
0.27
0.23
0.32
0.31
0.41
0.52
0.59
0.64
0.6
0.44
0.48
0.29
0.33
0.54
0.28
0.42

160ms
0.47
0.39
0.6
0.7
0.65
0.86
1.07
1.16
0.82
0.73
0.89
0.59
0.65
0.94
0.56
0.74

320ms
0.67
0.62
1.02
1.05
0.83
1.30
1.50
1.82
1.13
1.21
1.36
0.95
1.14
1.32
0.8
1.12

400ms
0.73
0.77
1.13
1.12
0.96
1.47
1.67
2.1
1.21
1.45
1.57
1.1
1.37
1.49
0.88
1.27

Our cgRNN (nh = 512  1.8M params) predicts human motions which are either comparable or slightly better
than the real-valued GRU [28] (nh = 1024  3.4M params) despite having only approximately half the parameters.

6 Conclusion

In this paper  we have proposed a novel complex gated recurrent unit which we use together with
unitary state transition matrices to form a stable and fast to train recurrent neural network. To enforce
unitarity  we optimize the state transition matrices on the Stiefel manifold  which we show to work
well with the modReLU. Our complex gated RNN achieves state-of-the-art performance on the
adding problem while remaining competitive on the memory problem. We further demonstrate
the applicability of our network on real-world tasks. In particular  for human motion prediction
we achieve state-of-the-art performance while signiﬁcantly reducing the number of weights. The
experimental success of the cgRNN leads us to believe that complex representations have signiﬁcant
potential and advocate for their use not only in recurrent networks but in deep learning as a whole.
Acknowledgements: Research was supported by the DFG project YA 447/2-1 (DFG Research Unit FOR 2535
Anticipating Human Behavior). We also gratefully acknowledge NVIDIA’s donation of a Titan X Pascal GPU.

References
[1] M. Arjovsky  A. Shah  and Y. Bengio. Unitary evolution recurrent neural networks. In ICML  2016.

[2] Y. Bengio  P. Simard  and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.

IEEE Trans. on Neural Networks  5(2):157–166  1994.

[3] N. Benvenuto and F. Piazza. On the complex backpropagation algorithm. IEEE Trans. Signal Processing 

40(4):967–969  1992.

[4] K. Cho  B. van Merriënboer  Ç. Gülçehre  D. Bahdanau  F. Bougares  H. Schwenk  and Y. Bengio. Learning
phrase representations using RNN encoder–decoder for statistical machine translation. In EMNLP  2014.

[5] I. Danihelka  G. Wayne  B. Uria  N. Kalchbrenner  and A. Graves. Associative long short-term memory. In

ICML  2016.

[6] J. Gehring  M. Auli  D. Grangier  D. Yarats  and Y.N. Dauphin. Convolutional sequence to sequence

learning. In ICML  2017.

[7] G. Georgiou and C. Koutsougeras. Complex domain backpropagation. IEEE Trans. Circuits and systems

II: Analog and Digital Signal Processing  39(5):330–334  1992.

9

[8] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In

AISTATS  2010.

[9] I. Goodfellow  Y. Bengio  A. Courville  and Y. Bengio. Deep learning. MIT press Cambridge  2016.

[10] N. Guberman. On complex valued convolutional neural networks. Technical report  The Hebrew University

of Jerusalem Israel  2016.

[11] A. Hirose. Complex-valued neural networks: Advances and applications. John Wiley & Sons  2013.

[12] S. Hochreiter and J. Schmidhuber. Long short term memory. Neural Computation  1997.

[13] S. Hyland and G. Rätsch. Learning unitary operators with help from u(n). In AAAI  2017.

[14] C. Ionescu  D. Papava  V. Olaru  and C. Sminchisescu. Human3.6M: Large Scale Datasets and Predictive
Methods for 3D Human Sensing in Natural Environments. IEEE Trans. Pattern Analysis and Machine
Intelligence  36(7):1325–1339  jul 2014.

[15] A. Jain  A.R. Zamir  S. Savarese  and A. Saxena. Structural-RNN: Deep learning on spatio-temporal

graphs. In CVPR  2016.

[16] L. Jing  C. Gulcehre  J. Peurifoy  Y. Shen  M. Tegmark  M. Solja  and Y. Bengio. Gated orthogonal

recurrent units: On learning to forget. In AAAI Workshops  2018.

[17] L. Jing  Y. Shen  T. Dubˇcek  J. Peurifoy  S. Skirlo  Y. LeCun  M. Tegmark  and M. Soljaˇci´c. Tunable

efﬁcient unitary neural networks (EUNN) and their application to RNNs. In ICML  2017.

[18] T. Kim and T. Adali. Complex backpropagation neural network using elementary transcendental activation

functions. In ICASSP  2001.

[19] H. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic

response. IEEE Trans. Pattern Analysis and Machine Intelligence  38(1):14–29  2016.

[20] L. Kovar  M. Gleicher  and F. Pighin. Motion graphs. ACM Tran. Graphics (TOG)  21(3):473–482  2002.

[21] K. Kreutz-Delgado. The complex gradient operator and the CR-calculus. arXiv preprint:0906.4835  2009.

[22] A. Krizhevsky  I. Sutskever  and G. Hinton.

networks. In NIPS  2012.

Imagenet classiﬁcation with deep convolutional neural

[23] H. Leung and S. Haykin. The complex backpropagation algorithm. IEEE Trans. Signal Processing 

39(9):2101–2104  1991.

[24] H. Li and T. Adali. Complex-valued adaptive signal processing using nonlinear functions. EURASIP

Journal on Advances in Signal Processing  2008.

[25] J. Liouville. Leçons sur les fonctions doublement périodiques. Journal für die reine und angewandte

Mathematik / Zeitschriftenband (1879)  1879.

[26] A. Maas  A. Hannun  and A. Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In

ICML  2013.

[27] D.P. Mandic and V.S.L. Goh. Complex valued nonlinear adaptive ﬁlters: noncircularity  widely linear and

neural models. John Wiley & Sons  2009.

[28] J. Martinez  M.J. Black  and J. Romero. On human motion prediction using recurrent neural networks. In

CVPR  2017.

[29] T. Mikolov. Statistical language models based on neural networks. Technical report  Brno University of

Technology  2012.

[30] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In ICML  2010.

[31] R. Pascanu  T. Mikolov  and Y. Bengio. On the difﬁculty of training recurrent neural networks. In ICML 

2013.

[32] M. Ravanelli  P. Brakel  M. Omologo  and Y. Bengio. Improving speech recognition by revising gated

recurrent units. In INTERSPEECH  2017.

[33] D.P. Reichert and T. Serre. Neuronal synchrony in complex-valued deep networks. In ICLR  2014.

10

[34] H. Tagare. Notes on optimization on stiefel manifolds. Technical report  Yale University  2011.

[35] J. Thickstun  Z. Harchaoui  and S.M. Kakade. Learning features of music from scratch. In ICLR  2017.

[36] C. Trabelsi  O. Bilaniuk  Y. Zhang  D. Serdyuk  S. Subramanian  J. F. Santos  S. Mehri  N. Rostamzadeh 

Y. Bengio  and C.J. Pal. Deep complex networks. In ICLR  2018.

[37] A Van Den Bos. Complex gradient and hessian. IEE Proceedings-Vision  Image and Signal Processing 

141(6):380–382  1994.

[38] P. Virtue  S.X. Yu  and M. Lustig. Better than real: Complex-valued neural nets for MRI ﬁngerprinting. In

ICIP  2017.

[39] W. Wirtinger. Zur formalen theorie der funktionen von mehr komplexen veränderlichen. Mathematische

Annalen  1927.

[40] S. Wisdom  T. Powers  J.R. Hershey  J. Le Roux    and L. Atlas. Full-capacity unitary recurrent neural

networks. In NIPS  2016.

[41] A. Yao  J. Gall  and L. Van Gool. Coupled action recognition and pose estimation from multiple views.

International Journal of Computer Vision  100(1):16–37  2012.

[42] M. Zeiler  M. Ranzato  R. Monga  M. Mao  K. Yang  Q. V. Le  P. Nguyen  A. Senior  V. Vanhoucke 

J. Dean  and G. E. Hinton. On rectiﬁed linear units for speech processing. In ICASSP  2013.

11

,Moritz Wolter
Angela Yao