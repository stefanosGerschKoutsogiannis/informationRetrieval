2018,Enhancing the Accuracy and Fairness of Human Decision Making,Societies often rely on human experts to take a wide variety of decisions affecting their members  from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken  by academics. In this context  each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However  these decisions may be imperfect due to limited experience  implicit biases  or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?

In this paper  we address the above problem from the perspective of sequential decision making and show that  for different fairness notions from the literature  it reduces to a sequence of (constrained) weighted bipartite matchings  which can be solved efficiently using algorithms with approximation guarantees. Moreover  these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the experts' preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.,Enhancing the Accuracy and Fairness

of Human Decision Making

Isabel Valera∗

MPI for Intelligent Systems

ivalera@tue.mpg.de

Adish Singla†
MPI-SWS

adishs@mpi-sws.org

Abstract

Manuel Gomez-Rodriguez‡

MPI-SWS

manuelgr@mpi-sws.org

Societies often rely on human experts to take a wide variety of decisions affecting
their members  from jail-or-release decisions taken by judges and stop-and-frisk
decisions taken by police ofﬁcers to accept-or-reject decisions taken by academics.
In this context  each decision is taken by an expert who is typically chosen uniformly
at random from a pool of experts. However  these decisions may be imperfect
due to limited experience  implicit biases  or faulty probabilistic reasoning. Can
we improve the accuracy and fairness of the overall decision making process by
optimizing the assignment between experts and decisions?
In this paper  we address the above problem from the perspective of sequential
decision making and show that  for different fairness notions in the literature  it
reduces to a sequence of (constrained) weighted bipartite matchings  which can
be solved efﬁciently using algorithms with approximation guarantees. More-
over  these algorithms also beneﬁt from posterior sampling to actively trade
off exploitation—selecting expert assignments which lead to accurate and fair
decisions—and exploration—selecting expert assignments to learn about the ex-
perts’ preferences. We demonstrate the effectiveness of our algorithms on both
synthetic and real-world data and show that they can signiﬁcantly improve both the
accuracy and fairness of the decisions taken by pools of experts.

Introduction

1
In recent years  there have been increasing concerns about the potential for unfairness of algorithmic
decision making. Moreover  these concerns have been often supported by empirical studies  which
have provided  e.g.  evidence of racial discrimination [8  10]. As a consequence  there have been
a ﬂurry of work on developing computational mechanisms to make sure that the machine learning
methods that fuel algorithmic decision making are fair [3  4  5  6  13  14  15]. In contrast  to the
best of our knowledge  there is a lack of machine learning methods to ensure accuracy and fairness
in human decision making  which is still prevalent in a wide range of critical applications such as 
e.g.  jail-or-release decisions by judges  stop-and-frisk decisions by police ofﬁcers or accept-or-reject
decisions by academics. In this work  we take a ﬁrst step towards ﬁlling this gap.
More speciﬁcally  we focus on a problem setting that ﬁts a variety of real-world applications  including
the ones mentioned above: binary decisions come sequentially over time and each decision need to
be taken by a human decision maker  typically an expert  who is chosen from a pool of experts. For
example  in jail-or-release decisions  the expert is a judge who needs to decide whether she grants
bail to a defendant; in stop-and-frisk decisions  the expert is a police ofﬁcer who needs to decide
whether she stop (and potentially frisk) a pedestrian; or  in accept-or-reject decisions  the expert is
an academic who needs to decide whether a paper is accepted in a conference (or a journal). In
this context  our goal is then to ﬁnd the optimal assignments between human decision makers and

∗Max Planck Institute for Intelligent Systems. Max Planck Ring 4  472076 Tuebingen (Germany).
†Max Planck Institute for Software Systems (MPI-SWS). Campus E1 5  66123 Saarbruecken (Germany).
‡Max Planck Institute for Software Systems. Paul-Ehrlich-Strasse  G26  67663 Kaiserslautern (Germany).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

decisions which maximizes the accuracy of the overall decision making process while satisfying
several popular notions of fairness studied in the literature.
In this paper  we represent human decision making using threshold decisions rules [3] and then
show that  if the thresholds used by each expert are known  the above problem can be reduced to
a sequence of matching problems  which can be solved efﬁciently with approximation guarantees.
More speciﬁcally:

I. Under no fairness constraints  the problem can be cast as a sequence of maximum weighted
bipartite matching problems  which can be solved exactly in polynomial (quadratic)
time [12].

II. Under (some of the most popular) fairness constraints  the problem can be cast as a sequence
of bounded color matching problems  which can be solved using a bi-criteria algorithm
based on linear programming techniques with a 1/2 approximation guarantee [9].

Moreover  if the thresholds used by each expert are unknown  we also show that  if we estimate the
value of each threshold using posterior sampling  we can effectively trade off exploitation—taking
accurate and fair decisions—and exploration—learning about the experts’ preferences and biases.
More formally  we can show that posterior samples achieve a sublinear regret in contrast to point
estimates  which suffer from linear regret.
Finally  we experiment on synthetic data and real jail-or-release decisions by judges [8]. The results
show that: (i) our algorithms improve the accuracy and fairness of the overall human decision making
process with respect to random assignment; (ii) our algorithms are able to ensure fairness more
effectively if the pool of experts is diverse  e.g.  there exist harsh judges  lenient judges  and judges in
between; and  (iii) our algorithms are able to ensure fairness even if a signiﬁcant percentage of judges
(e.g.  50%) has preferences (biases) against a group of individuals sharing a certain sensitive attribute
value (e.g.  race). The implementations of our algorithms and the data used in our experiments are
available at https://github.com/Networks-Learning/FairHumanDecisions.
2 Preliminaries
In this section  we ﬁrst deﬁne decision rules and formally deﬁne their utility and group beneﬁt. Then 
we revisit threshold decision rules  a type of decision rules which are optimal in terms of accuracy
under several notions of fairness from the literature.
Decision rules  their utilities  and their group beneﬁts. Given an individual with a feature vector
x ∈ Rd  a (ground-truth) label y ∈ {0  1}  and a sensitive attribute z ∈ {0  1}  a decision rule
d(x  z) ∈ {0  1} controls whether the ground-truth label y is realized by means of a binary decision
about the individual. As an example  in a pretrial release scenario  the decision rule speciﬁes whether
the individual remains in jail  i.e.  d(x  z) = 1 if she remains in jail and d(x  z) = 0 otherwise; the
label indicates whether a released individual would reoffend  i.e.  y = 1 if she would reoffend and
y = 0 otherwise; the feature vector x may include the current offense  previous offenses  or times
she failed to appear in court; and the sensitive attribute z may be race  i.e.  black vs white.
Further  we deﬁne random variables X  Y   and Z that take on values X = x  Y = y  and Z = z for
an individual drawn randomly from the population of interest. Then  we measure the (immediate)
utility as the overall proﬁt obtained by the decision maker using the decision rule [3]  i.e. 

u(d  c) = E [Y d(X  Z) − c d(X  Z)] = E(cid:2)d(X  Z)(cid:0)PY |X Z − c(cid:1)(cid:3)

(1)
where c ∈ (0  1) is a given constant. For example  in a pretrial release scenario  the ﬁrst term
is proportional to the expected number of violent crimes prevented under d  the second term is
proportional to the expected number of people detained  and c measures the cost of detention in units
of crime prevented. Here  note that the above utility reﬂects only the proximate costs and beneﬁts of
decisions rather than long-term  systematic effects. Finally  we deﬁne the (immediate) group beneﬁt
as the fraction of beneﬁcial decisions received by a group of individuals sharing the sensitive attribute
value z [15]  i.e. 
(2)
For example  in a pretrial release scenario  one may deﬁne f (x) = 1 − x and thus the beneﬁt to the
group of white individuals be proportional to the expected number of them who are released under d.
Remarkably  most of the notions of (un)fairness used in the literature  such as disparate impact [1] 
equality of opportunity [6] or disparate mistreatment [13] can be expressed in terms of group beneﬁts.
Finally  note that  in some applications  the beneﬁcial outcome may correspond to d(X  Z) = 1.

bz(d  c) = E [f (d(X  Z = z))] .

2

(cid:26)1

0

(cid:26)1

0

(cid:26)1

0

Optimal threshold decision rules. Assume the conditional distribution P (Y |X  Z) is given4. Then 
the optimal decision rules d∗ that maximize u(d  c) under the most popular fairness constraints from
the literature are threshold decision rules [3  6]:

— No fairness constraints: the optimal decision rule under no fairness constraints is given by

the following deterministic threshold rule:

d∗(X  Z) =

if pY =1|X Z ≥ c
otherwise.

(3)

— Disparate impact  equality of opportunity  and disparate mistreatment: the optimal decision
rule which satisﬁes (avoids) the three most common notions of (un)fairness is given by the
following deterministic threshold decision rule:

d∗(X  Z) =

if pY =1|X Z ≥ θZ
otherwise 

(4)

where θZ ∈ [0  1] are constants that depend only on the sensitive attribute and the fairness
notion of interest. Note that the unconstraint optimum can be also expressed using the above
form if we take θZ = c.
3 Problem Formulation
In this section  we ﬁrst use threshold decision rules to represent biased human decisions and then
formally deﬁne our sequential human decision making process.
Humans as threshold decision rules. Inspired by recent work by Kleinberg et al. [7]  we model a
human decision maker v who has access to pY |X Z using the following threshold decision rule:

dv(X  Z) =

if pY =1|X Z ≥ θV Z
otherwise 

(5)

where θV Z ∈ [0  1] are constants that depend on the decision maker and the sensitive attribute  and
they represent human decision makers’ biases (or preferences) towards groups of people sharing
a certain value of the sensitive attribute z. For example  in a pretrial release scenario  if a judge v
is generally more lenient towards white people (z = 0) than towards black people (z = 1)  then
θv z=0 > θv z=1.
In the above formulation  note that we assume all experts make predictions using the same (true)
conditional distribution pY |X Z  i.e.  all experts have the same prediction ability.
It would be
very interesting to relax this assumption and account for experts with different prediction abilities.
However  this entails a number of non trivial challenges and is left for future work.
Sequential human decision making problem. A set of human decision makers V = {vk}k∈[n] need
to take decisions about individuals over time. More speciﬁcally  at each time t ∈ {1  . . .   T}  there
are m decisions to be taken and each decision i ∈ [m] is taken by a human decision maker vi(t) ∈ V 
who applies her threshold decision rule dvi(t)(X  Z)  deﬁned by Eq. 5  to the corresponding feature
vector xi(t) and sensitive attribute zi(t). Note that we assume that yi(t)|xi(t)  zi(t) ∼ pY |X Z for
all t ∈ {1  . . .   T} and i ∈ [m].
At each time t  our goal is then to ﬁnd the assignment of human decision makers vi(t) to individuals
(xi(t)  zi(t))  with vi(t) (cid:54)= vj(t) for all i (cid:54)= j  that maximizes the expected utility of a sequence of
decisions  i.e. 

u≤T ({dvi(t)}i t  c) =

1
mT

dvi(t)(xi(t)  zi(t))(pY =1|xi(t) zi(t) − c) 

(6)

where u≤T ({dvi(t)}i t  c) is a empirical estimate of a straight forward generalization of the utility
deﬁned by Eq. 1 to multiple decision rules.

4In practice  the conditional distribution may be approximated using a machine learning model trained on

historical data.

3

T(cid:88)

m(cid:88)

t=1

i=1

4 Proposed Algorithms
In this section  we formally address the problem deﬁned in the previous section without and with
fairness constraints. Note that  without fairness constraints  we aim to approximate the solution
provided by Eq. 3  and with fairness constraints  Eq. 4. In both cases  we ﬁrst consider the setting in
which the human decision makers’ thresholds θV Z are known and then generalize our algorithms to
the setting in which they are unknown and need to be learned over time.
Decisions under no fairness constraints. We can ﬁnd the assignment of human decision makers
{v(t)}T

t=1 with the highest expected utility by solving the following optimization problem:

T(cid:88)

m(cid:88)

dvi(t)(xi(t)  zi(t))(pY =1|xi(t) zi(t) − c)

maximize
subject to vi(t) ∈ V for all t ∈ {1  . . .   T} 

t=1

i=1

vi(t) (cid:54)= vj(t) for all i (cid:54)= j.

(7)

— Known thresholds. If the thresholds θV Z are known for all human decision makers  the above
problem decouples into T independent subproblems  one per time t ∈ {1  . . .   T}  and each of these
subproblems can be cast as a maximum weighted bipartite matching  which can be solved exactly
in polynomial (quadratic) time [12]. To do so  for each time t  we build a weighted bipartite graph
where each human decision maker vj is connected to each individual (xi(t)  zi(t)) with weigh:

(cid:26)pY |xi(t) zi(t) − c

wji =

0

if pY |xi(t) zi(t) ≥ θvj  zi(t)
otherwise 

Note that the maximum weighted bipartite matching is the optimal assignment as deﬁned by Eq. 7.
— Unknown thresholds. If the thresholds are unknown  we need to trade off exploration  i.e.  learning
about the thresholds θV Z  and exploitation  i.e.  maximizing the average utility. To this aim  for
every decision maker v  we assume a Beta prior over each threshold θv z ∼ Beta(α  β). Under this
assumption  after round t  we can update the (domain of the) distribution of θv z(t) as:

max(0  θL

v z(t)) ≤ θv z(t) ≤ min(1  θH

v z(t)) 

where

θL
v z(t) =

θH
v z(t) =

t(cid:48)≤t | zi(t(cid:48))=z  vi(t(cid:48))=v  dv(xi(t(cid:48)) z)=0

max

pY =1|xi(t(cid:48)) z

t(cid:48)≤t | zi(t(cid:48))=z  vi(t(cid:48))=v  dv(xi(t(cid:48)) z)=1

min

pY =1|xi(t(cid:48)) z 

and write the posterior distribution of θv z(t) as

p(θv z(t)|D(t)) =

Γ(α + β)(θH

v z(t) − θv z(t))α−1(θv z(t) − θL
v z(t))α+β−1
Γ(α)Γ(β)(θH

v z(t) − θL

v z(t))β−1

(8)

 

(9)

Then  at the beginning of round t + 1  one can think of estimating the value of each threshold θv z(t)
using point estimates  i.e.  ˆθv z = argmax p(θv z(t)|D(t))  and use the same algorithm as for known
thresholds. Unfortunately  if we deﬁne regret as follows:

R(T ) = u≤T ({dvi(t)}i t  c) − u≤T ({dv∗

i (t)}i t  c) 

where vi(t) is the optimal assignment under the point estimates of the thresholds and v∗
optimal assignment under the true thresholds  we can show that (proven in Appendix A):

(10)
i (t) is the

Proposition 1 The optimal assignments with deterministic point estimates for the thresholds suffers
linear regret Θ(T ).

The above result is a consequence of insufﬁcient exploration  which we can overcome if we esti-
mate the value of each threshold θv z(t) using posterior sampling  i.e.  ˆθv z ∼ p(θv z(t)|D(t))  as
formalized by the following theorem:

√
Theorem 2 The expected regret of the optimal assignments with posterior samples for the thresholds
is O(

T ).

4

Proof Sketch. The proof of this theorem follows via interpreting the problem setting as a reinforcement
learning problem. Then  we can apply the generic results for reinforcement learning via posterior
sampling of [11]. In particular  we map our problem to an MDP with horizon 1 as follows. The
actions in the MDP correspond to assigning m individuals to n experts (given by K) and the reward
is given by the utility at time t.
Then  it is easy to conclude that the expected regret of the optimal assignments with posterior samples

for the thresholds is O(S(cid:112)KT log(SKT ))  where K = n.(n − 1).(n − 2) . . . (n − m + 1) denotes

the possible assignments of m individuals to n experts and S is a problem dependent parameter. S
quantiﬁes the the total number of states/realizations of feature vectors xi and sensitive features zi to
the i ∈ [m] individuals—note that S is bounded only for the setting where feature vectors xi and
sensitive features zi are discrete.

Given that the regret only grows as O(
T ) (i.e.  sublinear in T )  this theorem implies that the
algorithm based on optimal assignments with posterior samples converges to the optimal assignments
given the true thresholds as T → ∞.
Decisions under fairness constraints. For ease of exposition  we focus on disparate impact  however 
a similar reasoning follows for equality of opportunity and disparate mistreatment [6  13].
To avoid disparate impact  the optimal decision rule d∗(X  Z)  given by Eq. 4  maximizes the utility 
as deﬁned by Eq. 1  under the fairness constraint [3  13] DI(d∗  c) = |bz=1(d∗  c)−bz=0(d∗  c)| ≤ α 
where α ∈ [0  1] is a given parameter which controls the amount of disparate impact—the smaller
the value of α  the lower the disparate impact of the corresponding decision rule. Similarly  we can
calculate a empirical estimate of the disparate impact of a decision rule d at each time t as:

√

1
m

|bt z=1(d  c) − bt z=0(d  c)|  

where bt z(d  c) =(cid:80)m
converges to DI(d∗  c) as m → ∞  and 1/T(cid:80)T

(11)
DIt(d  c) =
I(zi = z)f (d(xi(t)  zi(t)))  where f (·) deﬁnes what is a beneﬁcial out-
come. Here  it is easy to see that  for the optimal decision rule d∗ under impact parity  DIt(d∗  c)
t=1 DIt(d∗  c) converges to DI(d∗  c) as T → ∞.
For a ﬁxed α  assume there are at least m(1 − α) experts with θv z < c  at least m(1 − α) experts
with θv z ≥ c for each z = 0  1  and n ≥ 2m. Then  we can ﬁnd the assignment of human decision
makers {v(t)} with the highest expected utility and disparate impact less than α as:

i=1

T(cid:88)

m(cid:88)

dvi(t)(xi(t)  zi(t))(pY =1|xi(t) zi(t) − c) 

maximize
subject to vi(t) ∈ V for all t ∈ {1  . . .   T} 

t=1

i=1

vi(t) (cid:54)= vj(t) for all i (cid:54)= j 
bt z(d∗  c) − αmz(t) ≤ bt z({dvi(t)}i)∀ t  z
bt z({dvi(t)}i) ≤ bt z(d∗  c) + αmz(t)∀ t  z.

i=1

(12)
(cid:80)m
where and mz(t) is the number of decisions with sensitive attribute z at round t and bt z({dvi(t)}i) =
I(zi = z)f (dvi(t)(xi(t)  zi(t)))). Here  the assignment v∗(t) given by the solution to the
above optimization problem satisﬁes that DIt({dv∗
i (t)}i  c) ∈ [DIt(d∗  c) − α  DIt(d∗  c) + α] and
thus limT→∞ DI≤T ({dv∗
— Known thresholds. If the thresholds are known  the problem decouples into T independent
subproblems  one per time t ∈ {1  . . .   T}  and each of these subproblems can be cast as a constrained
maximum weighted bipartite matching. To do so  for each time t  we build a weighted bipartite graph
where each human decision maker vj is connected to each individual (xi(t)  zi(t)) with weight wji 
where

i (t)}i t  c) ≤ α.

(cid:26)pY |xi(t) zi(t) − c

if pY |xi(t) zi(t) ≥ θvj  zi(t)
otherwise 

and we additionally need to ensure that  for z ∈ {0  1}  the matching S satisﬁes that

wji =

0

bt z(d∗  c) − αmz(t) ≤ (cid:88)

g(wji)

(j i)∈S:zi=z

and (cid:88)

(j i)∈S:zi=z

5

g(wji) ≤ bt z(d∗  c) + αmz(t) 

where mz(t) denotes the number of individuals with sensitive attribute z at round t and the function
g(wji) depends on what is the beneﬁcial outcome  e.g.  in a pretrial release scenario  g(wji) =
I(wji (cid:54)= 0). Remarkably  we can reduce the above constrained maximum weighted bipartite matching
problem to an instance of the bounded color matching problem [9]  which allows for a bi-criteria
algorithm based on linear programming techniques with a 1/2 approximation guarantee. To do so 
we just need to rewrite the above constraints as

g(wji) ≤ bt z(d∗  c) + αmz(t) 

and

(13)

(cid:88)
(cid:88)

(j i)∈S:zi=z

gC(wji) ≤ (1 + α)mz(t) − bt z(d∗  c).

(14)

(j i)∈S:zi=z

we are looking for a perfect matching and thus(cid:80)

To see the equivalence between the above constraints and the original ones  one needs to realize that
example  in a pretrial release scenario  g(wji) = I(wji (cid:54)= 0) and gC(wji) = I(wji = 0).
— Unknown thresholds. If the threshold are unknown  we proceed similarly as in the case under
no fairness constraints  i.e.  we again assume Beta priors over each threshold  update their posterior
distributions after each time t  and use posterior sampling to set their values at each time.
Finally  for the regret analysis  we focus on an alternative unconstrained problem  which is equivalent
to the one deﬁned by Eq. 12 by Lagrangian duality [2]:

(cid:2)g(wji) + gC(wji)(cid:3) = mz(t). For

(j i)∈S:zi=z

t=1

T(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

dvi(t)(xi(t)  zi(t))(pY =1|xi(t) zi(t) − c)

m(cid:88)
(cid:0)bt z(d∗  c) − bt z({dvi(t)}i) − αmz(t)(cid:1)
(cid:0)bt z({dvi(t)}i) − bt z(d∗  c) − αmz(t)(cid:1)

λl t z

λu t z
vi(t) ∈ V for all t ∈ {1  . . .   T} 
vi(t) (cid:54)= vj(t) for all i (cid:54)= j.

maximize

T(cid:88)
T(cid:88)

t=1

+

+

t=1
subject to

i=1

(15)
where λl t z ≥ 0 and λu t z ≥ 0 are the Lagrange multipliers for the band constraints. Then  we can
then state the following theoretical result (the proof easily follows from the proof of Theorem 2):

√
Theorem 3 The expected regret of the optimal assignments for the problem deﬁned by Eq. 15 with
posterior samples for the thresholds is O(

T ).

Remark. In the above formulation  we do not enforce a speciﬁc mechanism to reduce disparate
impact–our framework ﬁnds the solution with maximum utility that satisﬁes the disparate impact
constraint. Depending on the distribution pY |X Z and the deﬁnition of utility and beneﬁts  such a
solution will result in an increase (decrease) of release rates for group z = 0 (z = 1) or viceversa.
5 Experiments
In this section we empirically evaluate our framework on both synthetic and real data. To this end 
we compare the performance  in terms of both utility and fairness  of the following algorithms:
— Optimal: Every decision is taken using the optimal decision rule d∗  which is deﬁned by Eq. 3
under no fairness constraints and by Eq. 4 under fairness constraints.
— Known: Every decision is taken by a judge following a (potentially biased) decision rule dv 
as given by Eq. 5. The threshold for each judge is known and the assignment between judges and
decisions is found by solving the corresponding matching problem  i.e.  Eq  7 under no fairness
constraints and Eq. 12 under fairness constraints.
— Unknown: Every decision is taken by a judge following a (potentially biased) decision rule dv 
proceeding similarly as in “Known". However  the threshold for each judge is unknown it is necessary
to use posterior sampling to estimate the thresholds.

6

(a) Expected utility

(b) Disparate impact

(c) Regret

Figure 1: Performance in synthetic data. Panels (a) and (b) show the trade-off between expected utility
and disparate impact. For the utility  the higher the better and  for the disparate impact  the lower the
better. Panel (c) shows the regret achieved by our algorithm under unknown experts’ thresholds as
deﬁned in Eq. 10. Here  the solid lines show the results for m = 20 and dashed lines for m = 10.
— Random: Every decision is taken by a judge following a (potentially biased) decision rule dv. The
assignment between judges and decision is random.

5.1 Experiments on Synthetic Data
Experimental setup. For every decision  we ﬁrst sample the sensitive attribute zi ∈ {0  1} from
Bernouilli(0.5) and then sample pY =1|xi zi ∼ Beta(3  5) if zi = 0 and from pY =1|xi zi ∼
Beta(4  3)  otherwise. For every expert  we generate her decision thresholds θv 0 ∼ Beta(0.5  0.5)
and θv 1 ∼ Beta(5  5). Here we assume there are n = 3m experts  to ensure that in each round there
are at least m(1 − α) experts with θv z < c  at least m(1 − α) experts with θv z ≥ c for z ∈ {0  1}.
In practice  if there is no feasible assignment for a round and desired level of fairness α  one may
decide to: i) add experts to the pool to increase diversity; ii) decrease the number of cases per round;
or (iii) use a random assignment in that round. Finally  we set m = 20  T = 1000 and c = 0.5  and
the beneﬁcial outcome for an individual is d = 1  i.e.  f (d) = d.
Results. Figures 1(a)-(b) show the expected utility and the disparate impact after T units of time for
the optimal decision rule and for the group of experts under the assignments provided our algorithms
and under random assignments. We ﬁnd that the experts chosen by our algorithm provide decisions
with higher utility and lower disparate impact than the experts chosen at random  even if the thresholds
are unknown. Moreover  if the threshold are known  the experts chosen by our algorithm closely
match the performance of the optimal decision rule both in terms of utility and disparate impact.
Finally  we compute the regret as deﬁned by Eq. 10  i.e.  the difference between the utilities provided
√
by algorithm with Known and Unknown thresholds over time. Figure 1(c) summarizes the results 
which show that  as time progresses  the regret degreases at a rate O(

T ).

5.2 Experiments on Real Data
Experimental setup. We use the COMPAS recidivism prediction dataset compiled by ProPublica [8] 
which comprises of information about all criminal offenders screened through the COMPAS (Correc-
tional Offender Management Proﬁling for Alternative Sanctions) tool in Broward County  Florida
during 2013-2014. In particular  for each offender  it contains a set of demographic features (gender 
race  age)  the offender’s criminal history (e.g.  the reason why the person was arrested  number of
prior offenses)  and the risk score assigned to the offender by COMPAS. Moreover  ProPublica also
collected whether or not these individuals actually recidivated within two years after the screening.
In our experiments  the sensitive attribute z ∈ {0  1} is the race (white  black)  the label y indicates
whether the individual recidivated (y = 1) or not (y = 0)  the decision rule d speciﬁes whether
an individual is released from jail (d = 0) or not (d = 1) and  for each sensitive attribute z  we
approximate pY |X Z=z using a logistic regression classiﬁer  which we train on 25% of the data.
Then  we use the remaining 75% of the data to evaluate our algorithm as follows. Since we do not
have information about the identify of the judges who took each decision in the dataset  we create
N = 3m (ﬁctitious) judges and sample their thresholds from a θ ∼ Beta(τ  τ )  where τ controls the
diversity (lenient vs harsh) across judges by means the standard deviation of the distribution since
4τ (2τ +1). Here  we consider two scenarios: (i) all experts are unbiased towards race and
std(θ) =
thus θv0 = θv1 and (ii) 50% of the experts are unbiased towards race and the other 50% are biased 
i.e.  θv1 = 1.2θv0. Finally  we consider m = 20 decisions per round  which results into 197 rounds 
where we assign decisions to rounds at random.
Results. Figure 2 shows the expected utility  the true utility and the disparate impact after T
units of time for the optimal decision rule and for the group of unbiased experts (scenario (i))

1

7

Unfair= 0.2= 0.1= 0.05= 0.0100.10.20.3Disparate ImpactRandomOptimalKnown Unknown Unfair= 0.4= 0.2= 0.1= 0.0100.020.040.060.08UtilityUnfair= 0.4= 0.2= 0.1= 0.0100.050.10.150.20.25Disparate Impact0100200300400500T0.010.020.030.040.05RegretUnfair= 0.2= 0.1(a) Expected Utility

(b) True Utility

(c) Disparate Impact

Figure 2: Performance in COMPAS data. Panels (a) and (b) show the expected utility and true utility
and panel (c) shows the disparate impact. For the expected and true utility  the higher the better and 
for the disparate impact  the lower the better.

(a) Known θ

(b) Unknown θ

t=1

(cid:80)T

Figure 3: Feasibility in COMPAS data. Probability that a round does not allow for an assignment
between judges and decisions with less than α disparate impact for different pools of experts of
varying diversity and percentage of biased judges.
(cid:80)m
under the assignments provided our algorithms and under random assignments. The true utility
ˆu≤T (d  c) is just the utility after T units of time given the actual true y values rather than pY |X Z 
i=1 d(xi(t)  zi(t))(yi − c). Similarly as in the case of synthetic data 
i.e.  ˆu≤T (d  c) = 1
T
we ﬁnd that the judges chosen by our algorithm provide higher expected utility and true utility as
well as lower disparate impact than the judges chosen at random  even if the thresholds are unknown.
We notice that our algorithm achieves a level of fairness α by decreasing the release rate of white
defendants  since our deﬁnition of utility penalizes more to release individuals who will be more likely
to recidivate than to keep in jail those who will not. Note also that under fairness constraints  our
algorithm relies on a bi-criteria algorithm with a 1/2 approximation guarantee to solve the maximum
weighted bipartite matching problem. As a consequence  it sometimes ﬁnds matchings violate the
fairness constraints but have higher utility.
Figure 3 shows the probability that a round does not allow for an assignment between judges and
decisions with less than α disparate impact for different pools of experts of varying diversity and
percentage of biased judges. The results show that  on the one hand  our algorithms are able to ensure
fairness more effectively if the pool of experts is diverse and  on the other hand  our algorithms are
able to ensure fairness even if a signiﬁcant percentage of judges (e.g.  50%) are biased against a group
of individuals sharing a certain sensitive attribute value.
6 Conclusions
In this paper  we have proposed a set of practical algorithms to improve the utility and fairness of a
sequential decision making process  where each decision is taken by a human expert  who is selected
from a pool experts. Experiments on synthetic data and real jail-or-release decisions by judges show
that our algorithms are able to mitigate imperfect human decisions due to limited experience  implicit
biases or faulty probabilistic reasoning. Moreover  they also reveal that our algorithms beneﬁt from
higher diversity across the pool experts  being able to ensure fairness even if a signiﬁcant percentage
of judges are biased against a group of individuals sharing a sensitive attribute value (e.g.  race).
There are many interesting venues for future work. For example  in our work  we assumed all experts
make predictions using the same (true) conditional distribution and then apply (potentially) different
thresholds. We have also assumed that experts do not learn from the decisions they take over time  i.e. 
their prediction model and thresholds are ﬁxed. It would be very interesting to relax these assumptions
and account for experts with different prediction abilities. In some scenarios  a decision is taking
jointly by a group of experts  e.g.  faculty recruiting decisions. It would be a natural follow-up to
the current work to design our algorithms for such scenario. Finally  in our experiments  we have to
generate ﬁctitious judges since we do not have information about the identify of the judges who took
each decision. It would be very valuable to gain access to datasets with such information [7].

8

Unfair= 0.2= 0.1= 0.05= 0.0100.10.20.3Disparate ImpactRandomOptimalKnown Unknown Unfair= 0.2= 0.1= 0.05= 0.0100.020.040.060.080.1Expected UtilityUnfair= 0.2= 0.1= 0.05= 0.0100.020.040.060.080.1True UtilityUnfair= 0.2= 0.1= 0.05= 0.0100.10.20.3Disparate Impact0.290.160.110.080.070.060.050.040.03Diversity  std()0.50.60.70.80.91Prob[round is feasible] = 0.1  bias = 0% = 0.1  bias = 50% = 0.01  bias = 0% = 0.01  bias = 50%0.290.160.110.080.070.060.050.040.03Diversity  std()0.50.60.70.80.91Prob[round is feasible] = 0.1  bias = 0% = 0.1  bias = 50% = 0.01  bias = 0% = 0.01  bias = 50%Acknowledgments. Isabel Valera acknowledges funding from a MPG Minerva Fast Track Grant.
References
[1] S. Barocas and A. D. Selbst. Big data´s disparate impact. California Law Review  2016.

[2] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[3] S. Corbett-Davies  E. Pierson  A. Feller  S. Goel  and A. Huq. Algorithmic decision making

and the cost of fairness. KDD  2017.

[4] C. Dwork  M. Hardt  T. Pitassi  O. Reingold  and R. Zemel. Fairness through awareness. In

ITCS  2012.

[5] M. Feldman  S. A. Friedler  J. Moeller  C. Scheidegger  and S. Venkatasubramanian. Certifying

and removing disparate impact. In KDD  2015.

[6] M. Hardt  E. Price  N. Srebro  et al. Equality of opportunity in supervised learning. In NIPS 

2016.

[7] J. Kleinberg  H. Lakkaraju  J. Leskovec  J. Ludwig  and S. Mullainathan. Human decisions and

machine predictions. The Quarterly Journal of Economics  133(1):237–293  2017.

[8] J. Larson  S. Mattu  L. Kirchner  and J. Angwin. https://github.com/propublica/compas-analysis 

2016.

[9] M. Mastrolilli and G. Stamoulis. Constrained matching problems in bipartite graphs. In ISCO 

pages 344–355. Springer  2012.

[10] C. Muñoz  M. Smith  and D. Patil. Big Data: A Report on Algorithmic Systems  Opportunity 

and Civil Rights. Executive Ofﬁce of the President. The White House.  2016.

[11] I. Osband  D. Russo  and B. Van Roy. (more) efﬁcient reinforcement learning via posterior

sampling. In NIPS  pages 3003–3011  2013.

[12] D. B. West et al. Introduction to graph theory  volume 2. Prentice hall Upper Saddle River 

2001.

[13] B. Zafar  I. Valera  M. Gomez-Rodriguez  and K. Gummadi. Fairness beyond disparate treatment

& disparate impact: Learning classiﬁcation without disparate mistreatment. In WWW  2017.

[14] B. Zafar  I. Valera  M. Gomez-Rodriguez  and K. Gummadi. Training fair classiﬁers. AISTATS 

2017.

[15] B. Zafar  I. Valera  M. Gomez-Rodriguez  K. Gummadi  and A. Weller. From parity to

preference: Learning with cost-effective notions of fairness. In NIPS  2017.

9

A Proof sketch of Proposition 1
Consider a simple setup with n = 2 experts and m = 1 decision at each round t ∈ [T ]. Furthermore 
we ﬁx the following two things before setting up the problem instance: (i) let g(·) be a deterministic
function which computes a point estimate of a distribution (e.g.  mean  or MAP); (ii) we assume a
deterministic tie-breaking by the assignment algorithm  and w.l.o.g. expert j = 1 is preferred over
expert j = 2 for assignment when both of them have same edge weights.
For the ﬁrst expert j = 1  we know the exact value of the threshold θ1 z. For the second expert j = 2 
the threshold θ2 z could take any value in the range [0  1] and we are given a prior distribution p(θ2 z).

Let us denote(cid:101)θ2 z = g(cid:0)p(θ2 z)(cid:1). Now  we construct a problem instance for which the algorithm
would suffer linear regret separately for(cid:101)θ2 z > 0 and(cid:101)θ2 z = 0.
Problem instance if(cid:101)θ2 z > 0
We consider a problem instance as follows: c = 0  θ2 z = 0  θ1 z = c+(cid:101)θ2 z
have pY =1|xi(t) zi(t) uniformly sampled from the range (c (cid:101)θ2 z) (note that m = 1 and there is only
j = 1 and has a cumulative expected utility of 3T(cid:101)θ2 z
expected utility of T(cid:101)θ2 z
Problem instance if(cid:101)θ2 z = 0
We consider a problem instance as follows: c = 1  θ2 z = 1  θ1 z = c+(cid:101)θ2 z
have pY =1|xi(t) zi(t) uniformly sampled from the range ((cid:101)θ2 z  c) (note that m = 1 and there is only
j = 1 and has a cumulative expected utility of −T(cid:101)θ2 z
expected utility of 0. Hence  the algorithm suffers a linear regret of R(T ) = T(cid:101)θ2 z

one individual i = 1 at each round t). The algorithm would always assign the individual to expert
. However  given the true thresholds  the
algorithm would have always assigned the individual to expert j = 2 and would have a cumulative

  and for all t ∈ [T ] we

  and for all t ∈ [T ] we

one individual i = 1 at each round t). The algorithm would always assign the individual to expert
. However  given the true thresholds  the
algorithm would have always assigned the individual to expert j = 2 and would have a cumulative

. Hence  the algorithm suffers a linear regret of R(T ) = T(cid:101)θ2 z

.

8

2

2

2

8

.

8

8

10

,Hastagiri Vanchinathan
Gábor Bartók
Andreas Krause
Ming Lin
Jieping Ye
Isabel Valera
Adish Singla
Manuel Gomez Rodriguez