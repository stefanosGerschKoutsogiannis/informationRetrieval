2019,On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks,Mixup~\cite{zhang2017mixup} is  a recently proposed  method for training deep neural networks  where additional samples are generated during training  by convexly combining random pairs of images and their associated labels. While simple to implement  it has shown to be a surprisingly effective method of data augmentation for image classification;  DNNs trained with mixup show noticeable gains in classification performance on a number of  image classification benchmarks. In this work  we discuss a hitherto untouched aspect of mixup training -- the calibration and predictive uncertainty   of models trained with mixup. We find that DNNs trained with mixup  are significantly better calibrated --  i.e the predicted softmax scores  are  much better indicators of the actual likelihood of a correct prediction --  than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets --  including large-scale datasets like ImageNet -- and find this to be the case.  
	Additionally  we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration.  Finally   we also observe that mixup-trained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data.  We conclude that the  typical overconfidence seen in neural networks  even on in-distribution data is likely a consequence of training with hard labels  suggesting that mixup training be employed for classification tasks where predictive uncertainty is a significant concern.,On Mixup Training: Improved Calibration and
Predictive Uncertainty for Deep Neural Networks

Sunil Thulasidasan∗∗ 1 2  Gopinath Chennupati1  Jeff Bilmes2 

Tanmoy Bhattacharya1  Sarah Michalak1

2Department of Electrical and Computer Engineering  University of Washington

1Los Alamos National Laboratory

Abstract

Mixup [37] is a recently proposed method for training deep neural networks where
additional samples are generated during training by convexly combining random
pairs of images and their associated labels. While simple to implement  it has shown
to be a surprisingly effective method of data augmentation for image classiﬁcation:
DNNs trained with mixup show noticeable gains in classiﬁcation performance on
a number of image classiﬁcation benchmarks. In this work  we discuss a hitherto
untouched aspect of mixup training – the calibration and predictive uncertainty of
models trained with mixup. We ﬁnd that DNNs trained with mixup are signiﬁcantly
better calibrated – i.e the predicted softmax scores are much better indicators of
the actual likelihood of a correct prediction – than DNNs trained in the regular
fashion. We conduct experiments on a number of image classiﬁcation architectures
and datasets – including large-scale datasets like ImageNet – and ﬁnd this to be
the case. Additionally  we ﬁnd that merely mixing features does not result in the
same calibration beneﬁt and that the label smoothing in mixup training plays a
signiﬁcant role in improving calibration. Finally  we also observe that mixup-
trained DNNs are less prone to over-conﬁdent predictions on out-of-distribution
and random-noise data. We conclude that the typical overconﬁdence seen in neural
networks  even on in-distribution data is likely a consequence of training with hard
labels  suggesting that mixup training be employed for classiﬁcation tasks where
predictive uncertainty is a signiﬁcant concern.

1

Introduction: Overconﬁdence and Uncertainty in Deep Learning

Machine learning algorithms are replacing or expected to increasingly replace humans in decision-
making pipelines. With the deployment of AI-based systems in high risk ﬁelds such as medical
diagnosis [24]  autonomous vehicle control [20] and the legal sector [1]  the major challenges of
the upcoming era are thus going to be in issues of uncertainty and trust-worthiness of a classiﬁer.
With deep neural networks having established supremacy in many pattern recognition tasks  it is the
predictive uncertainty of these types of classiﬁers that will be of increasing importance. The DNN
must not only be accurate  but also indicate when it is likely to get the wrong answer. This allows
the decision-making to be routed to a human or another more accurate  but possibly more expensive 
classiﬁer  with the assumption being that the additional cost incurred is greatly surpassed by the
consequences of a wrong prediction.
For this reason  quantifying the predictive uncertainty and conﬁdence calibration for deep neural
networks has seen increased attention in recent years ([6  18  7  9  19  15  28]). One of the ﬁrst works
to examine the issue of calibration for modern neural networks was [9]; in a well-calibrated classiﬁer 

∗Correspondence to sunil@lanl.gov

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

Figure 1: Joint density plot of accuracy vs conﬁdence (captured by the winning softmax score) on
the CIFAR-100 validation set at different training epochs for the VGG-16 deep neural network. Top
Row: In regular training  the DNN moves from under-conﬁdence  at the beginning of training  to
overconﬁdence at the end. A well-calibrated classiﬁer would have most of the density lying on
the x = y gray line. Bottom Row: Training with mixup on the same architecture and dataset. At
corresponding epochs  the network is much better calibrated.

predictive scores should be indicative of the actual likelihood of correctness. The authors in [9]
show signiﬁcant empirical evidence that modern deep neural networks are poorly calibrated  with
depth  weight decay and batch normalization all inﬂuencing calibration. Modern architectures  it
turns out  are prone to overconﬁdence  meaning accuracy is likely to be lower than what is indicated
by the predictive score. The top row in Figure 1 illustrates this phenomena: shown are a series
of joint density plots of the average winning score and accuracy of a VGG-16 [29] network over
the CIFAR-100 [17] validation set  plotted at different epochs. Both the conﬁdence (captured by
the winning score) as well as accuracy start out low and gradually increase as the network learns.
However  what is interesting – and concerning – is that the conﬁdence always leads accuracy in the
later stages of training. Towards the end of training  accuracy saturates while conﬁdence continues
to increase resulting in a very sharply peaked distribution of winning scores and an overconﬁdent
model.
Most modern DNNs  when trained for classiﬁcation in a supervised learning setting  are trained using
one-hot encoded labels that have all the probability mass in one class; the training labels are thus
zero-entropy signals that admit no uncertainty about the input. The DNN is thus  in some sense 
trained to become overconﬁdent. Hence a worthwhile line of exploration is whether principled
approaches to label smoothing can somehow temper overconﬁdence. Label smoothing and related
work has been explored before [30  27]. In this work  we carry out an exploration along these lines by
investigating the effect of the recently proposed mixup [37] method of training deep neural networks.
In mixup  additional synthetic samples are generated during training by convexly combining random
pairs of images and  importantly  their labels as well. While simple to implement  it has shown to be a
surprisingly effective method of data augmentation: DNNs trained with mixup show noticeable gains
in classiﬁcation performance on a number of image classiﬁcation benchmarks. However neither the
original work nor any subsequent extensions to mixup [33  10  22] have explored the effect of mixup
on predictive uncertainty and DNN calibration; this is precisely what we aim to do in this paper.
Our ﬁndings are as follows: mixup trained DNNs are signiﬁcantly better calibrated – i.e the predicted
softmax scores are much better indicators of the actual likelihood of a correct prediction – than DNNs
trained without mixup (see Figure 1 bottom row for an example). We also observe that merely mixing
features does not result in the same calibration beneﬁt and that the label smoothing in mixup training
plays a signiﬁcant role in improving calibration. Further  we also observe that mixup-trained DNNs
are less prone to over-conﬁdent predictions on out-of-distribution and random-noise data. We note
here that in this work we do not consider the calibration and uncertainty over adversarially perturbed
inputs; we leave that for future exploration.

2

0.00.20.40.60.81.0Mean of Winning Score0.00.20.40.60.81.0accuracyEpoch00.00.20.40.60.81.0Mean of Winning Score0.00.20.40.60.81.0accuracyEpoch200.00.20.40.60.81.0Mean of Winning Score0.00.20.40.60.81.0accuracyEpoch610.00.20.40.60.81.0Mean of Winning Score0.00.20.40.60.81.0accuracyEpoch1200.00.20.40.60.81.0Mean of Winning Score0.00.20.40.60.81.0accuracyEpoch1610.00.51.00.000.250.500.751.00AccuracyAccuracyEpoch00.00.51.00.000.250.500.751.00Epoch200.00.51.0Confidence0.000.250.500.751.00Epoch610.00.51.00.000.250.500.751.00Epoch1200.00.51.00.000.250.500.751.00Epoch161The rest of the paper is organized as follows: Section 2 provides a brief overview of the mixup
training process. Section 3 discusses calibration metrics  experimental setup and mixup’s calibration
beneﬁts results on image data. In Section 5 we explore in more detail the effect of mixup-based label
smoothing on calibration. In Section 7 we show additional evidence of the beneﬁt of mixup training
on predictive uncertainty when dealing with unseen data. Further discussions and conclusions are in
Section 8

2 An Overview of Mixup Training

Mixup training [37] is based on the principle of Vicinal Risk Minimization [3](VRM): the classiﬁer
is trained not only on the training data  but also in the vicinity of each training sample. The vicinal
points are generated according to the following simple rule introduced in [37]:

˜x = λxi + (1 − λ)xj
˜y = λyi + (1 − λ)yj

where xi and xj are two randomly sampled input points  and yi and yj are their associated one-hot
encoded labels. This has the effect of the empirical Dirac delta distribution

centered at (xi  yi) being replaced with the empirical vicinal distribution

Pδ(x  y) =

1
n

δ(x = xi  y = yi)

Pν(˜x  ˜y) =

ν(˜x  ˜y|xi  yi)

n(cid:88)
n(cid:88)

i

1
n

i

m(cid:88)

i=1

The vicinal samples (˜x  ˜y) are generated as above  and during training minimization is performed on
the empirical vicinal risk:

Rν(f ) =

1
m

L(f ( ˜xi)  ˜yi)

where L is the standard cross-entropy loss  but calculated on the soft-labels ˜yi instead of hard
labels. Training this way not only augments the feature set ˜X  but the induced set of soft-labels
also encourages the strength of the classiﬁcation regions to vary linearly betweens samples. The
experiments in [37] and related work in [14  33  10] show noticeable performance gains in various
image classiﬁcation tasks.
The linear interpolator λ ∈ [0  1] that determines the mixing ratio is drawn from a symmetric Beta
distribution  Beta(α  α)  where α is the hyper-parameter that controls the strength of the interpolation
between pairs of images and the associated smoothing of the training labels. α = 0 recovers the base
case corresponding to zero-entropy training labels (one-hot encodings  in which case the resulting
image is either just xi or xj)  while a high value of α ends up in always averaging the inputs and
labels. The authors in [37] remark that relatively smaller values of α ∈ [0.1  0.4] gave the best
performing results for classiﬁcation  while high values of α resulted in signiﬁcant under-ﬁtting. In
this work  we also look at the effect of α on calibration performance.

3 Experiments

We perform numerous experiments to analyze the effect of mixup training on the calibration of the
resulting trained classiﬁers on both image and NLP data. We experiment with various deep architec-
tures and standard datasets  including large-scale training with ImageNet. In all the experiments in
this paper  we only apply mixup to pairs of images as done in [37]. The mixup functionality was
implemented using the mixup authors’ code available at [36].

3.1 Setup

For the small-scale image experiments  we use the following datasets in our experiments: STL-10 [4] 
CIFAR-10 and CIFAR-100 [17] and Fashion-MNIST [34]. For STL-10  we use the VGG-16 [29]

3

network. CIFAR-10 and CIFAR-100 experiments were carried out on VGG-16 as well as ResNet-34
models. For Fashion-MNIST  we used a ResNet-18 model. For all experiments  we use batch
normalization  weight decay of 5 × 10−4  trained the network using SGD with Nesterov momentum 
training for 200 epochs with an initial learning rate of 0.1 halved at 2 at 60 120 and 160 epochs.
Unless otherwise noted  calibration results are reported for the best performing epoch on the validation
set.

3.2 Calibration Metrics

We measure the calibration of the network as follows (and as described in [9]): predictions are
grouped into M interval bins of equal size. Let Bm be the set of samples whose prediction scores
(the winning softmax score) fall into bin Bm. The accuracy and conﬁdence of Bm are deﬁned as

where ˆpi is the conﬁdence (winning score) of sample i. The Expected Calibration Error (ECE) is
then deﬁned as:

acc(Bm) =

conf(Bm) =

M(cid:88)

m=1

|Bm|
n

1
|Bm|
1
|Bm|

ˆpi

iinBm

iinBm

1(ˆyi = yi)

(cid:88)
(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)acc(Bm) − conf(Bm)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:16)

ECE =

(cid:104)

M(cid:88)

m=1

|Bm|
n

In high-risk applications  conﬁdent but wrong predictions can be especially harmful; thus we also
deﬁne an additional calibration metric – the Overconﬁdence Error (OE)– as follows

OE =

conf(Bm) × max

conf(Bm) − acc(Bm)  0

(cid:17)(cid:105)

This penalizes predictions by the weight of the conﬁdence but only when conﬁdence exceeds accuracy;
thus overconﬁdent bins incur a high penalty.

3.3 Comparison Methods

Since mixup produces smoothed labels over mixtures of inputs  we compare the calibration perfor-
mance of mixup to two other label smoothing techniques:

by distributing an  mass over the other (i.e.  non ground-truth) classes.

• −label smoothing described in [30] where the one-hot encoded training signal is smoothed
• We also compare the performance of mixup against the entropy-regularized loss (ERL) de-
scribed in [27] that discourages the neural network from being over-conﬁdent by penalizing
low-entropy distributions.

Our baseline comparison is regular training where no label smoothing or mixing of features is applied
(no-mixup). We also note that in this section we do not compare against the temperature scaling
method described in [9]  which is a post-training calibration method and will generally produce
well-calibrated scores. Here we would like to see the effect of label smoothing while training;
experiments with temperature scaling are reported in Section 7.

3.4 Results

Results on the various datasets and architectures are shown in Figure 2. While the performance gains
in validation accuracy are generally consistent with the results reported in [37]  it is the effects on
network calibration that we focus here. The top row shows a calibration scatter plot for STL-10
and CIFAR-100  highlighting the effect of mixup training. In a well calibrated model  where the
conﬁdence matches the accuracy most of the points will be on x = y line. We see that in the base
case  both for STL-10 and CIFAR-100  most of the points tend to lie in the overconﬁdent region. The
mixup case is much better calibrated  noticeably in the high-conﬁdence regions.

4

(a)

(b)

(c)

(d)

(e)

(h)

(f)

(i)

(g)

(j)

Figure 2: Calibration results for mixup and base-case on various image datasets and architectures.
Top Row: Scatterplots for accuracy and conﬁdence for STL-10(a b) and CIFAR-100(c d). The mixup
case is much better calibrated with the points lying closer to the x = y line  while in the base case 
points tend to lie in the overconﬁdent region. Middle Row: Mixup versus comparison methods.
label smoothing is the -label smoothing method  while ERL is the entropy regularized loss. Bottom
Row: Calibration Error (e) and Overconﬁdence error (f) on various architectures. Experiments
suggest best ECE is achieved in the [0.2 0.4] range for α (h)  while overconﬁdence error decreases
monotonically with α due to underﬁtting (i). Accuracy behavior for differently calibrated models is
showin in (j).

The bar plots in the middle row show the results on various combinations of datasets and architectures
on accuracy and calibration against comparison methods. We report the calibration error for the
best performing model (in terms of validation accuracy). For label smoothing  an  ∈ [0.05  0.1]
performed best while for ERL  the best-performing conﬁdence penalty hyper-parameter was 0.1. The
trends in the comparison are clear: label smoothing either via -smoothing  ERL or mixup generally
provides a calibration advantage and tempers overconﬁdence  with the latter generally performing the
best in comparison to other methods. We also show the effect on ECE as we vary the hyperparameter
α of the mixing parameter distribution. For very low values of α  the behavior is similar to the base
case (as expected)  but ECE also noticeably worsens for higher values of α due to the model being
under-conﬁdent. Indeed  mixup models can be underconﬁdent if α is large whic is related to manifold
intrusion [10]: a mixed-up sample is more likely to lie away from the original manifold and thus be
affected by manifold intrusion if α is large. Overconﬁdence alone decreases monotonically as we
increase α as shown in Figure 2i. We also show the accuracy of mixup models at various levels of

5

0.000.250.500.751.00Score of Winning Class0.00.20.40.60.81.0accuracySTL-10/VGG-16: Calibrationno mixup0.000.250.500.751.00Score of Winning Class0.00.20.40.60.81.0accuracySTL-10/VGG-16: Calibrationmixup0.000.250.500.751.00Score of Winning Class0.00.20.40.60.81.0accuracyCIFAR-100/ResNet34: Calibrationno mixup0.000.250.500.751.00Score of Winning Class0.00.20.40.60.81.0accuracyCIFAR-100/ResNet34: CalibrationmixupSTL-10VGG-16CIFAR-100ResNet-34CIFAR-10ResNet-34FashionResNet-180.000.010.020.030.040.050.060.070.08Calibration ErrorCalibration Errorno_mixupmixuplabel_smoothingERLSTL-10VGG-16CIFAR-100ResNet-34CIFAR-10ResNet-34FashionResNet-180.000.010.020.030.040.050.060.07Overconfidence ErrorOverconfidence Errorno_mixupmixuplabel_smoothingERLSTL-10VGG-16CIFAR-100ResNet-34CIFAR-10ResNet-34FashionResNet-18020406080100Validation AccuracyAccuracyno_mixupmixuplabel_smoothingERL0.10.20.30.40.5alpha0.020.040.060.08Calibration ErrorCalibration Error0.10.20.30.40.5alpha0.0000.0050.0100.0150.020OverconfidenceOverconfidence0.000.020.040.06Calibration Error7580859095100accuracyAccuracy vs CalibrationSTL-10/VGG-16CIFAR-100/ResNet-34CIFAR-10/ResNet-34Fashion-MNIST/ResNet-18calibration determined by α. As can be seen  a well-tuned α can result in a better-calibrated model
with very little loss in performance. Our classiﬁcation results here are consistent with those reported
in [37] where the best performing α was in the [0.1  .0.4] range.

3.4.1 Large-scale Experiments on ImageNet

(a)

(b)

(c)

Figure 3: Calibration on ImageNet for ResNet architectures

Here we report the results of calibration metrics resulting form mixup training on the 1000-class
version of the ImageNet [5] data comprising of over 1.2 million images. One of the advantages of
mixup and its implementation is that it adds very little overhead to the training time  and thus can be
easily applied to large scale datasets like ImageNet. We perform distributed parallel training using the
synchronous version of stochastic gradient descent. We use the learning-rate schedule described in
[8] on a 32-GPU cluster and train till 93% accuracy is reached over the top-5 predictions. We test on
two modern state-of-the-art archictures: ResNet-50 [12] and ResNext-101 (32x4d) [35]. The results
are shown in Figure 3. The scatter-plot showing calibration for ResNext-101 architecture suggests
that mixup training provides noticeable beneﬁts even in the large-data scenario  where the models
should be less prone to over-ﬁtting the one-hot labels. On the deeper ResNext  mixup provides better
calibration than the label smoothing models  though this same effect was not visible for the ResNet-50
model. However  both calibration error and overconﬁdence show noticeable improvements using
label smoothing over the baseline. The mixup model did however achieve a consistently higher
classiﬁcation performance of ≈ 0.4 percent over the other methods.

4 Experiments on Natural Language Data

(a)

(b)

(c)

Figure 4: Accuracy  calibration and overconﬁdence on various NLP datasets

While mixup was originally suggested as a method to mostly improve performance on image
classiﬁcation tasks  here we explore the effect of mixup training in the natural language processing
(NLP) domain. A straight-forward mixing of inputs (as in pixel-mixing in images) will generally
produce nonsense input since the semantics are unclear. To avoid this  we modify the mixup strategy
to perform mixup on the embeddings layer rather than directly on the input documents. We note
that this approach is similar to the recent work described in [11] that utilizes mixup for improving
sentence classiﬁcation which is among the few works  besides ours  studying the effects of mixup in

6

0.20.40.60.8Confidence0.20.40.60.81.0AccuracyImageNet/ResNext-101no mixupmixupResNext-1010.000.020.040.06Calibration ErrorImageNetResNet-500.000.010.020.030.040.05Calibration ErrorImageNetno_mixupmixuplabel_smoothingERLTRECIMDBMR020406080Validation Accuracyno_mixupmixup-feats-onlymixupERLlabel_smoothingTRECIMDBMR0.0000.0250.0500.0750.1000.1250.1500.1750.200Expected Calibration Errorno_mixupmixup-feats-onlymixupERLlabel_smoothingTRECIMDBMR0.0000.0250.0500.0750.1000.1250.1500.1750.200Overconfidence Penaltyno_mixupmixup-feats-onlymixupERLlabel_smoothingthe NLP domain. For our experiments  we employ mixup on NLP data for text classiﬁcation using the
MR [25]  TREC [21] and [23] datasets.We train a CNN for sentence classiﬁcation (Sentence-level
CNN) [16]  where we initialize all the words with pre-trained GloVe [26] embeddings  which are
modiﬁed while training on each dataset. For the remaining parameters  we use the values suggested
in [16]. We refrain from training the most recent NLP models [13  2  38]  since our aim here is not to
show state-of-art classiﬁcation performance on these datasets  but to study the effect on calibration.
Also  the design of the more recent NLP models makes embedding mixup less straightforward.
Nevertheless  the performance beneﬁts on calibration  shown in Figure 4 are evident where mixup
provides noticeable gains for all datasets  both in terms of calibration and overconﬁdence. We leave
further exploration of principled strategies for mixup for NLP as future work.

5 Effect of Soft Labels on Calibration

So far we have seen that mixup consistently leads to better calibrated networks compared to the
base case  in addition to improving classiﬁcation performance as has been observed in a number of
works [33  10  22]. This behavior is not surprising given that mixup is a form of data augmentation:
in mixup training  due to random sampling of both images as well as the mixing parameter λ  the
probability that the learner sees the same image twice is small. This has a strong regularizing effect
in terms of preventing memorization and over-ﬁtting  even for high-capacity neural networks. Indeed 
unlike regular training  the train loss in the mixup case is always signiﬁcantly higher than the base
case as observed by the mixup authors [37]. From the perspective of statistical learning theory  the
improved calibration of a mixup classiﬁer can be viewed as the classiﬁer learning the true posteriors
in the inﬁnite data limit [32] due to the signiﬁcant amount of data augmentation resulting from
the random combination in mixup. However this leads to the following question: if the improved
calibration is essentially an effect of data augmentation  does simply combining the images but not
combining the labels provide the same calibration beneﬁt?
We perform a series of experiments on various image datasets and architectures to explore this
question. Results from the earlier sections show that existing label smoothing techniques that
increase the entropy of the training signal do provide better calibration without exploiting any data
augmentation effects and thus we expect to see this in play in the mixup case as well. In the latter case 
the entropies of the train labels are determined by the α parameter of the Beta(α  α) distribution
from which the mixing parameter is sampled. The distribution of training entropies for a few cases
of α are shown in Figure 5. The base-case is equivalent to α = 0 (not shown) where the entropy
distribution is a point-mass at 0.

(a)

(b)

(c)

(d)

Figure 5: Entropy distribution of training labels as a function of the α parameter of the Beta(α  α)
distribution from which the mixing parameter is sampled.

To tease out the effect of full mixup versus only mixing features  we convexly combine images as
before  but the resulting image assumes the hard label of the nearer class; this provides data augmen-
tation without the label smoothing effect. Results on a number of benchmarks and architectures are
shown in Figure 6. The results are clear: merely mixing features does not provide the calibration ben-
eﬁt seen in the full-mixup case suggesting that the point-mass distributions in hard-coded labels are
contributing factors to overconﬁdence. As in label smoothing and entropy regularization  having (or
enforcing via a loss penalty) a non-zero mass in more than one class prevents the largest pre-softmax
logit from becoming much larger than the others tempering overconﬁdence and leading to improved
calibration.

7

0.00.20.40.6alpha=0.10.00.20.40.6alpha=0.30.00.20.40.6alpha=0.50.00.20.40.6alpha=1.0In addition to feature and label mixing  a recent extension to mixup [33] also proposes convexly
combining the representations in the hidden layer of the network; we report the calibration effects of
this approach in the supplementary material.

(a)

(b)

(c)

(d)

(e)

Figure 6: Calibration performance when only features are mixed vs. full mixup  on various datasets
and architectures

6 Effect of Extended Training on Mixup Calibration

As remarked in the previous section  one of the contributing factors to improved calibration in mixup
is the signiﬁcant data augmentation aspect of mixup training  where the model is unlikely to see
the same mixed-up sample more than once. The natural question here is whether these models will
eventually become overconﬁdent if trained for much longer periods. Below  we show the training
curves for a few extended training experiments where the models were trained for 1000 epochs:
for the baseline (i.e when α = 0.)  the train loss and accuracy approach 0 and 100% respectively
(i.e.  over-ﬁtting)  while in the mixup case (non-zero α’s)  the strong data augmentation prevents
over-ﬁtting. This behavior is sustained over the entire duration of the training as can be seen in
the corresponding values of ECE. Mixup models  even when trained for much longer  continue to
have a low calibration error  suggesting that the mixing of data has a sustained inhibitive effect on
over-ﬁtting the training data (the training loss for mixup continues to be signiﬁcantly higher than
baseline even after extended training) and preventing the model from becoming overconﬁdent.

Figure 7: Training loss and calibration error under extended training for CIFAR-10 and CIFAR-100
with mixup. Baseline train error (orange) goes to zero early on while mixup continues to have
non-zero training loss even after 1000 epochs. Meanwhile  calibration error for mixup does not
exhibit an upward trend even after extended training.

7 Testing on Out-of-Distribution and Random Data

In this section  we explore the effect
of mixup training when predicting on
samples from unseen classes (out-of-
distribution) and random noise im-
ages. We ﬁrst train a VGG-16 network
on in-distribution data (STL-10) and
then predict on classes sampled from
the ImageNet database that have not
been encountered during training. For
the random noise images  we test on
gaussian random noise with the same
mean and variance as the training set.

Figure 8: Distribution of winning scores of various mod-
els when tested on out-of-distribution and gaussian noise
samples  after being trained on the STL-10 dataset.

8

STL-10VGG-160.000.010.020.030.04Calibration ErrorCIFAR-100ResNet-340.000.020.040.06Calibration ErrorCIFAR-100VGG-160.0000.0250.0500.0750.100Calibration Errormixup-feats-onlymixupFashionResNet-180.0000.0050.0100.0150.020Calibration ErrorResNext-101ImageNet0.000.020.040.06Calibration Error02505007501000epoch01Train LossCIFAR-10/ResNet-18MixupBaseline02505007501000epoch0.050.10ECECIFAR-10/ResNet-18Mixup ECE02505007501000epoch024Train LossCIFAR-100/ResNet-18MixupBaseline02505007501000epoch0.0250.0500.075ECECIFAR-100/ResNet-18Mixup ECE0.00.20.40.60.81.0ConfidencePredictions on Out-of-Distribution Imagesno mixupmixuptemp_scaledMC dropout avg (p=0.3)0.00.20.40.60.81.0ConfidencePredictions on Random Noise Imagesno mixupmixuptemp_scaledMC dropout avg (p=0.3)We compare the performance of a mixup-trained model with that of the baseline  as well as a
temperature calibrated pre-trained baseline as described in [9]. Since the latter is a post-training
calibration method  we expect it to be well calibrated on in-distribution data. We also compare the
prediction uncertainty using the Monte Carlo dropout method described in [6] where multiple forward
passes using dropout are made during test-time. We average predictions over 10 runs. The distribution
over prediction scores for out-of-distribution and random data for mixup and comparison methods
are shown in Figure 8. The differences versus the baseline are striking; in both cases  the mixup DNN
is noticeably less conﬁdent than its non-mixup counterpart  with the score distribution being nearly
perfectly separable in the random noise case. While temperature scaling is more conservative than
mixup on real but out-of-sample data  it is noticeably more overconﬁdent in the random-noise case.
Further  mixup performs signiﬁcantly better than MC-dropout in both cases.
In Table 1  we also show a comparison of the performance
of the aforementioned models for reliably detecting of out-of-
distributon and random-noise data  using Area under the ROC
curve as the metric. Mixup is the best performing model in both
cases  signiﬁcantly outperforming the others as a random-noise
detector. Temperature scaling  while producing well-calibrated
models for in-distribution data is not a reliable detector. The
scaling process reduces the conﬁdence on both in and out-of-
distribution data  signiﬁcantly reducing the ability to discrimi-
nate between these two types of data. Mixup  on the other hand 
does well in both cases. The results here suggest that the effect of training with interpolated samples
and the resulting label smoothing tempers over-conﬁdence in regions away from the training data.
While these experiments were limited to two datasets and one architecture  the results indicate that
training by minimizing vicinal risk can be an effective way to enhance reliability of predictions in
DNNs. Note that since mixup trains the model by convexly combining pairs of images  the synthe-
sized images all lie within the convex hull of the training data. In the suppllementary material  we
provide results on the prediction conﬁdence when images lie outside the convex hull of the training
set.

Table 1: Out-of-category detection
results for the DAC on STL-10 and
Tiny ImageNet.

Baseline
Mixup (α=0.4)
Temp. Scaling
Dropout(p=0.3)

Method

AUROC(In/Out)
STL-10/
Gaussian
73.28
95.93
54.2
70.57

STL-10/
ImageNet
80.57
83.28
56.2
78.93

8 Conclusion and Future Work

We presented results on an unexplored area of mixup based training – its effect on DNN calibration
and predictive uncertainty. Existing empirical work has conclusively shown the beneﬁts of mixup
for boosting classiﬁcation performance; in this work  we show an additional important beneﬁt –
mixup trained networks turn out to be better calibrated and provide more reliable estimates both for
in-sample and out-of-sample data (being under-conﬁdent in the latter case).
There are possibly multiple reasons for this: the data augmentation provided by mixup is a form of
regularization that prevents over-ﬁtting and memorization  tempering overconﬁdence in the process.
The label smoothing resulting from mixup might be viewed as a form of entropic regularization on
the training signals  again preventing the DNN from driving the training error to zero. The results
in the paper provide further evidence that training with hard labels is likely one of the contributing
factors leading to overconﬁdence seen in modern neural networks. Recent work [33] has shown how
the classiﬁcation regions in mixup are smoother  without sudden jumps from one high conﬁdence
region to the other suggesting that the lack of sharp boundary transitions in classiﬁcation regions play
an important role in producing well-calibrated classiﬁers.
Since mixup is implemented while training  it can also be employed with post-training calibration
like temperature scaling  model perturbations like the dropout method or even the ensemble models
described in [18]. Further mixup based models can also be combined with rejection classiﬁers  both
during training [31] for dealing with label noise  as well as inference [7] to improve the training and
classiﬁcation pipeline in modern deep learning. Indeed  the classiﬁcation performance-boost coupled
with the well-calibrated nature of mixup trained DNNs as studied in this paper suggest that mixup
based training be employed in situations where predictive uncertainty is a signiﬁcant concern.

9

Acknowledgments

We would like to thank the anonymous referees for their valuable suggestions on improving the
paper. The authors were supported in part by the Joint Design of Advanced Computing Solutions for
Cancer (JDACS4C) program established by the U.S. Department of Energy (DOE) and the National
Cancer Institute (NCI) of the National Institutes of Health. This work was performed under the
auspices of the U.S. Department of Energy by Los Alamos National Laboratory under Contract
DE-AC5206NA25396. This work was also supported in part by the CONIX Research Center  one of
six centers in JUMP  a Semiconductor Research Corporation (SRC) program sponsored by DARPA.

References
[1] Richard Berk. An impact assessment of machine learning risk forecasts on parole board

decisions and recidivism. Journal of Experimental Criminology  13(2):193–216  2017.

[2] Daniel Cer  Yinfei Yang  Sheng-yi Kong  Nan Hua  Nicole Limtiaco  Rhomni St John  Noah
Constant  Mario Guajardo-Cespedes  Steve Yuan  Chris Tar  et al. Universal sentence encoder.
arXiv preprint arXiv:1803.11175  2018.

[3] Olivier Chapelle  Jason Weston  L´eon Bottou  and Vladimir Vapnik. Vicinal risk minimization.

In Advances in neural information processing systems  pages 416–422  2001.

[4] Adam Coates  Andrew Ng  and Honglak Lee. An analysis of single-layer networks in unsuper-
vised feature learning. In Proceedings of the fourteenth international conference on artiﬁcial
intelligence and statistics  pages 215–223  2011.

[5] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition  2009. CVPR 2009.
IEEE Conference on  pages 248–255. Ieee  2009.

[6] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning  pages 1050–1059 
2016.

[7] Yonatan Geifman and Ran El-Yaniv. Selective classiﬁcation for deep neural networks. In

Advances in neural information processing systems  pages 4885–4894  2017.

[8] Priya Goyal  Piotr Doll´ar  Ross Girshick  Pieter Noordhuis  Lukasz Wesolowski  Aapo Kyrola 
Andrew Tulloch  Yangqing Jia  and Kaiming He. Accurate  large minibatch sgd: Training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

[9] Chuan Guo  Geoff Pleiss  Yu Sun  and Kilian Q Weinberger. On calibration of modern neural

networks. arXiv preprint arXiv:1706.04599  2017.

[10] Hongyu Guo  Yongyi Mao  and Richong Zhang. Mixup as locally linear out-of-manifold

regularization. arXiv preprint arXiv:1809.02499  2018.

[11] Hongyu Guo  Yongyi Mao  and Richong Zhang. Augmenting data with mixup for sentence

classiﬁcation: An empirical study. arXiv preprint arXiv:1905.08941  2019.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[13] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-

tion. arXiv preprint arXiv:1801.06146  2018.

[14] Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint

arXiv:1801.02929  2018.

[15] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for
computer vision? In Advances in neural information processing systems  pages 5574–5584 
2017.

[16] Yoon Kim. Convolutional neural networks for sentence classiﬁcation.

arXiv:1408.5882  2014.

arXiv preprint

[17] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

10

[18] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems  pages 6402–6413  2017.

[19] Kimin Lee  Honglak Lee  Kibok Lee  and Jinwoo Shin. Training conﬁdence-calibrated classiﬁers

for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325  2017.

[20] Jesse Levinson  Jake Askeland  Jan Becker  Jennifer Dolson  David Held  Soeren Kammel 
J Zico Kolter  Dirk Langer  Oliver Pink  Vaughan Pratt  et al. Towards fully autonomous driving:
Systems and algorithms. In Intelligent Vehicles Symposium (IV)  2011 IEEE  pages 163–168.
IEEE  2011.

[21] Xin Li and Dan Roth. Learning question classiﬁers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1  pages 1–7. Association for Computational
Linguistics  2002.

[22] Daojun Liang  Feng Yang  Tian Zhang  and Peter Yang. Understanding mixup training methods.

IEEE Access  6:58774–58783  2018.

[23] Andrew L Maas  Raymond E Daly  Peter T Pham  Dan Huang  Andrew Y Ng  and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting
of the association for computational linguistics: Human language technologies-volume 1  pages
142–150. Association for Computational Linguistics  2011.

[24] Riccardo Miotto  Li Li  Brian A Kidd  and Joel T Dudley. Deep patient: an unsupervised
representation to predict the future of patients from the electronic health records. Scientiﬁc
reports  6:26094  2016.

[25] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the 43rd annual meeting on association for
computational linguistics  pages 115–124. Association for Computational Linguistics  2005.

[26] Jeffrey Pennington  Richard Socher  and Christopher Manning. Glove: Global vectors for
word representation. In Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP)  pages 1532–1543  2014.

[27] Gabriel Pereyra  George Tucker  Jan Chorowski  Łukasz Kaiser  and Geoffrey Hinton. Reg-
arXiv preprint

ularizing neural networks by penalizing conﬁdent output distributions.
arXiv:1701.06548  2017.

[28] Murat Sensoy  Lance Kaplan  and Melih Kandemir. Evidential deep learning to quantify
classiﬁcation uncertainty. In Advances in Neural Information Processing Systems  pages 3183–
3193  2018.

[29] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[30] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
on computer vision and pattern recognition  pages 2818–2826  2016.

[31] Sunil Thulasidasan  Tanmoy Bhattacharya  Jeff Bilmes  Gopinath Chennupati  and Jamal
Mohd-Yusof. Combating label noise in deep learning using abstention. arXiv preprint
arXiv:1905.10964  2019.

[32] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies

of events to their probabilities. In Measures of complexity  pages 11–30. Springer  2015.

[33] Vikas Verma  Alex Lamb  Christopher Beckham  Aaron Courville  Ioannis Mitliagkis  and
Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a
regularizer. arXiv preprint arXiv:1806.05236  2018.

[34] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747  2017.

[35] Saining Xie  Ross Girshick  Piotr Doll´ar  Zhuowen Tu  and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition  pages 1492–1500  2017.
[36] Hongyi Zhang. https://github.com/hongyi-zhang/mixup.

11

[37] Hongyi Zhang  Moustapha Cisse  Yann N Dauphin  and David Lopez-Paz. mixup: Beyond

empirical risk minimization. arXiv preprint arXiv:1710.09412  2017.

[38] Peng Zhou  Zhenyu Qi  Suncong Zheng  Jiaming Xu  Hongyun Bao  and Bo Xu. Text classiﬁ-
cation improved by integrating bidirectional lstm with two-dimensional max pooling. arXiv
preprint arXiv:1611.06639  2016.

12

,Sunil Thulasidasan
Gopinath Chennupati
Jeff Bilmes
Tanmoy Bhattacharya
Sarah Michalak