2008,Fast Computation of Posterior Mode in Multi-Level Hierarchical Models,Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable organized in a hierarchy. Model fitting is challenging  especially for hierarchies with large number of nodes. We provide a novel algorithm based on a multi-scale Kalman filter that is both scalable and easy to implement. For non-Gaussian responses  quadratic approximation to the log-likelihood results in biased estimates. We suggest a bootstrap strategy to correct such biases. Our method is illustrated through simulation studies and analyses of real world data sets in health care and online advertising.,Fast Computation of Posterior Mode in Multi-Level

Hierarchical Models

Liang Zhang

Department of Statistical Science

Duke University

Durham  NC 27708

Deepak Agarwal
Yahoo! Research

2821 Mission College Blvd.

Santa Clara  CA 95054

lz9@stat.duke.edu

dagarwal@yahoo-inc.com

Abstract

Multi-level hierarchical models provide an attractive framework for incorporating
correlations induced in a response variable that is organized hierarchically. Model
ﬁtting is challenging  especially for a hierarchy with a large number of nodes. We
provide a novel algorithm based on a multi-scale Kalman ﬁlter that is both scalable
and easy to implement. For Gaussian response  we show our method provides the
maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response 
parameter estimation is performed through a Laplace approximation. However 
the Laplace approximation provides biased parameter estimates that is corrected
through a parametric bootstrap procedure. We illustrate through simulation studies
and analyses of real world data sets in health care and online advertising.

1 Introduction

In many real-world prediction problems  the response variable of interest is clustered hierarchically.
For instance  in studying the immunization status of a set of children in a particular geographic loca-
tion  the children are naturally clustered by families  which in turn are clustered into communities.
The clustering often induce correlations in the response variable; models that exploit this provide
signiﬁcant improvement in predictive performance. Multi-level hierarchical models provide an at-
tractive framework for modeling such correlations. Although routinely applied to moderate sized
data (few thousand nodes) in several ﬁelds like epidemiology  social sciences  biology [3]  model
ﬁtting is computationally expensive and is usually performed through a Cholesky decomposition
of a q (number of nodes in the hierarchy) dimensional matrix. Recently  such models have shown
promise in a novel application of internet advertising [1] where the goal is to select top-k adver-
tisements to be shown on a webpage to maximize the click-through rates. To capture the semantic
meaning of content in a parsimonious way  it is commonplace to classify webpages and ads into
large pre-deﬁned hierarchies. The hierarchy in such applications consist of several levels and the
total number of nodes may run into millions. Moreover  the main goal is to exploit the hierarchy
for obtaining better predictions; computing the full posterior predictive distribution is of secondary
importance. Existing ﬁtting algorithms are difﬁcult to implement and do not scale well for such
problems. In this paper  we provide a novel  fast and easy to implement algorithm to compute the
posterior mode of parameters for such models on datasets organized hierarchically into millions of
nodes with several levels. The key component of our algorithm is a multi-scale Kalman ﬁlter that
expedites the computation of an expensive to compute conditional posterior.

The central idea in multi-level hierarchical (MLH hereafter) models is “shrinkage” across the nodes
in the hierarchy. More speciﬁcally  these models assume a multi-level prior wherein parameters of
children nodes are assumed to be drawn from a distribution centered around the parameter of the
parent. This bottom-up  recursive assumption provides a posterior whose estimates at the ﬁnest res-
olution are smoothed using data on the lineage path of the node in the hierarchy. The fundamental

1

Notation Meaning
Tj
mj
q
pa(r)
ci(r)
nr
yir
Y
xir
X
β
φj
r
φ
V
γj
γ
φj
r|r
σj
r|r
ˆ
φj
r
σj
r

Level j of the hierarchy T
The number of nodes at level j in T
The total number of nodes in T
The parent node of node r in T
The ith child node of node r in T
The number of observations at leaf node r
The ith observation (response) at leaf node r
{yir  i = 1 ···   nr  r ∈ T}
The ith observation (p-dimensional covariates) at leaf node r
{xir  i = 1 ···   nr  r ∈ T}
The regression parameter vector associated with X
The random effect parameter at node r at level j
{φj
The residual variance of yir  if yir has a Gaussian model
The variance of φj
{γ1 ···   γL}
The mean of φj
The variance of φj
The mean of φj
The variance of φj

r|{yir′  i = 1 ···   nr′ ∀r′ ≺ r}

r|{yir′  i = 1 ···   nr′ ∀r′ ≺ r}
r|{yir′  i = 1 ···   nr′ ∀r′ ∈ TL}

r|{yir′  i = 1 ···   nr′ ∀r′ ∈ TL}

r  r ∈ T  j = 1 ···   L}

r for all the nodes at level j

Table 1: A list of the key notations.

assumption is that the hierarchy  determined from domain knowledge  provides a natural clustering
to account for latent processes generating the data which  when incorporated into the model  im-
prove predictions. Although MLH models are intuitive  parameter estimation presents a formidable
challenge  especially for large hierarchies. For Gaussian response  the main computational bottle-
neck is the Cholesky factorization of a dense covariance matrix whose order depends on the number
of nodes  this is expensive for large problems. For non-Gaussian response (e.g binary data)  the non-
quadratic nature of the log-likelihood adds on an additional challenge of approximating an integral
whose dimension depends on the number of nodes in the hierarchy. This is an active area of research
in statistics with several solutions being proposed  such as [5] (see references therein as well). For
Cholesky factorization  techniques based on sparse factorization of the covariance matrix have been
recently proposed in [5]. For non-Gaussian models  solutions require marginalization over a high di-
mensional integral and is often accomplished through higher order Taylor series approximations[6].
However  these techniques involve linear algebra that is often non-intuitive and difﬁcult to imple-
ment. A more natural computational scheme that exploits the structure of the model is based on
Gibbs sampling; however  it is not scalable due to slow convergence.
Our contributions are as follows: We provide a novel ﬁtting procedure based on multi-scale Kalman
ﬁlter algorithm that directly exploits the hierarchical structure of the problem and computes the pos-
terior mode of MLH parameters. The complexity of our method is almost linear in the number of
nodes in the hierarchy. Other than scalability  our ﬁtting procedure is more intuitive and easy to
implement. We note that although multi-scale Kalman ﬁlters have been studied in the electrical
engineering literature [2] and spatial statistics  their application to ﬁtting MLH is novel. Moreover 
ﬁtting such models to non-Gaussian data present formidable challenges as we illustrate in the paper.
We provide strategies to overcome those through a bootstrap correction and compare with the com-
monly used cross-validation approach. Our methods are illustrated on simulated data  benchmark
data and data obtained from an internet advertising application.

2 MLH for Gaussian Responses

Assume we have a hierarchy T consisting of L levels (root is level 0)  for which mj  j = 0 ···   L 
denotes the number of nodes at level j. Denote the set of nodes at level j in the hierarchy T as Tj.
For node r in T   denote the parent of r as pa(r)  and the ith child of node r as ci(r). If a node r′ is

2

a descendent of r  we say r′ ≺ r. Since the hierarchy has L levels  TL denotes the set of leaf nodes
in the hierarchy. Let yir  i = 1 ···   nr denote the ith observation at leaf node r  and xir denote the
p-dimensional covariate vector associated with yir. For simplicity  we assume all observations are
available at leaf nodes (a more general case where each node in the hierarchy can have observations
is easily obtained from our algorithm). Consider the Gaussian MLH deﬁned by

′

irβ + φL

r   V ) 

yir|φL

r ∼ N (x

r|φj−1

pa(r) ∼ N (φj−1

0 = 0. The form of p(φj

r|φj−1
pa(r)  γj); j = 1 ···   L 

pa(r))  j = 0 ···   L  where φ0
r|φj−1
φj

(1)
where β is a ﬁxed effect parameter vector and φj
r is a random effect associated with node r
at level j with joint distribution deﬁned through a set of hierarchical conditional distributions
pa(r))  j = 1 ···   L is assumed
p(φj
to be
(2)
where γ = (γ1 ···   γL) is a vector of level-speciﬁc variance components that control the amount
of smoothing. To complete the model speciﬁcation in a Bayesian framework  we put a vague prior
on V (π(V ) ∝ 1/V ) and a mild quadratic prior on γi (π(γi|V ) ∝ V /(V + γi)2). For β  we assume
a non-informative prior  i.e.  π(β) ∝ 1.
The speciﬁcation of MLH given by Equation 2 is referred to as the centered parametrization and
was shown to provide good performance in a fully Bayesian framework by [9]. An equivalent way
r ∼ N (0  γj) to the
of specifying MLH is obtained by associating independent random variables bj
nodes and replacing φL
r parameters along the lineage path from root to
leaf node in the hierarchy. We denote this compactly as z′
r for all the
nodes in the hierarchy  and zr is a vector of 0/1’s turned on for nodes in the path of node r. More
compactly  let y = {yir  i = 1 ···   nr  r ∈ T}  and X as well as Z be the corresponding matrix of
β + Zb  V I) with b ∼ N (0  Ω(γ)).
vectors xir and zr for i = 1 ··· nr and r ∈ T   then y ∼ N (X
The problem is to compute the posterior mode of (βp×1  bq×1  γL×1  V ) where q = PL
j=1 mj. The
main computational bottleneck is computing the Cholesky factor of a q×q matrix (Z
Z +Ω−1)  this
is expensive for large values of q. Existing state-of-the-art methods are based on sparse Cholesky
factorization; we provide a more direct way. In fact  our method provides a MAP estimate of the
parameters for the Gaussian case. For non-Gaussian case  we provide an approximation to the MAP
through the Laplace method coupled with a bootstrap correction. We also note that our method
apply if the random effects are vectors and enter into equation (2) as linear combination with some
covariate vector. In this paper  we illustrate through a scalar.

rb  where b is a vector of bj

r in (1) by the sum of the bj

′

′

2.1 Model Fitting

Throughout  we work with the parametrization speciﬁed by φ. The main component of our ﬁtting
algorithm is computing the conditional posterior distribution of φ = {φj
r  r ∈ T  j = 1 ···   L}
given (β  V  γ). Since the parameters V and γ are unknown  we estimate them through an EM
algorithm. The multi-scale Kalman ﬁlter (described next) computes the conditional posterior of φ
mentioned above and is used in the inner loop of the EM.

As in temporal state space models  the Kalman ﬁlter consists of two steps - a)Filtering: where one
propagates information from leaves to the root and b) Smoothing: where information is propagated
from root all the way down to the leaves.

Filtering:
Denote the current estimates of β  γ and V as ˆβ  ˆγ  and ˆV respectively. Then  eir = yir − x
are the residuals and V ar(φj
effects. If the conditional posterior distribution φL
step is to update φL
for Gaussian models

ˆβ
i=1 ˆγi  r ∈ Tj are the marginal variances of the random
r|r)  the ﬁrst
r using standard Bayesian update formula

r |{yir  i = 1 ···   nr} ∼ N (φL

r|r for all leaf random effects φL

r) = Σj = Pj

r|r and σL

r|r  σL

ir

′

nr

φL
r|r =

σL
r|r =

ΣL

eir

P

i=1

ˆV + nrΣL

ΣL ˆV

ˆV + nrΣL

 

.

3

(3)

(4)

Next  the posteriors φj
r|r)  are recursively updated
from j = L − 1 to j = 1  by regressing the parent node effect towards each child and combining
information from all the children.

r|{yir′  i = 1 ···   nr′ ∀r′ ≺ r} ∼ N (φj

r|r  σj

To provide intuition about regression step  it is useful to invert the state equation (2) and express the
distribution of φj−1

pa(r) conditional on φj

(5)

(6)

pa(r)|φj
r))
Simple algebra provides the conditional expectation and variance of φj−1

pa(r) − E(φj−1

φj−1
pa(r) = E(φj−1

r) + (φj−1

pa(r)|φj

r as

r. Note that
pa(r)|φj

φj−1
pa(r) = Bjφj

r + ψj
r 

i=1 ˆγi/ Pj

i=1 ˆγi  correlation between any two siblings at level j and ψj

where Bj = Pj−1
N (0  Bj ˆγj).
First  a new prior is obtained for the parent node based on the current estimate of each child by
plugging-in the current estimates of a child into equation (6). For the ith child of node r (here we
assume that r is at level j − 1  and ci(r) is at level j) 
φj−1
r|ci(r) = Bjφj

ci(r)|ci(r) 

r ∼

(7)

σj−1
r|ci(r) = B2

j σj

ci(r)|ci(r) + Bj ˆγj 

Next  we combine information obtained by the parent from all its children.

φj−1
r|r = σj−1

r|r

kr

X

i=1

(φj−1

r|ci(r)/σj−1

r|ci(r)) 

1/σj−1

r|r = Σ−1

j−1 +

kr

X

i=1

((1/σj−1

r|ci(r)) − Σ−1

j−1).

(8)

(9)

(10)

where kr is the number of children of node r at level j − 1.
Smoothing:
In the smoothing step  parents propagate information recursively from root to the leaves to provide
us with the posterior of each φj
r based on the entire data. Denoting the posterior mean and variance
of φj
For level 1 nodes  set ˆφ1

r respectively  the update equations are given below.
r|r.

r given all the observations by ˆ
φj
r and σj

r|r  and σ1

r = σ1

r = φ1

For node r at other levels 
ˆ
φj
r = φj

and let

r|rBj(

r|r + σj
r|r + σj2

r|rB2

j (σj−1

ˆ
pa(r) − φj−1
φj−1
pa(r) − σj−1

pa(r)|r)/σj
pa(r)|r)/σj2

pa(r)|r 

pa(r)|r 

r = σj
σj

σj j−1
r pa(r) = σj

r|rBjσj−1

pa(r)/σj−1

pa(r)|r.

(11)

(12)

(13)

The computational complexity of the algorithm is linear in the number of nodes in the hierarchy and
for each parent node  we perform an operation which is cubic in the number of children. Hence 
for most hierarchies that arise in practical applications  the complexity is “essentially” linear in the
number of nodes.

Expectation Maximization:

To estimate all parameters simultaneously  we use an EM algorithm which assumes the φ parameters
to be the missing latent variables. The expectation step consists of computing the expected value of
complete log-posterior with respect to the conditional distribution of missing data φ  obtained using
the multi-scale Kalman ﬁlter algorithm. The maximization step obtains revised estimates of other
parameters by maximizing the expected complete log-posterior.

4

ˆV = X

r∈TL

nr

P

i=1

(eir − ˆφL
P
r∈TL

r )2 + nrσL

r

nr

 

For j = 1 ···   L 

P
r∈Tj

ˆγj =

(σj

r + σj−1

r pa(r) + ( ˆφr

pa(r) − 2σj j−1
|mj|

j

− ˆφpa(r)

j−1

)2)

.

(14)

(15)

Updating ˆβ:
We use the posterior mean of φ obtained from the Kalman ﬁltering step  to compute the posterior
mean of β as given in equation (16).

where ˆφL is the vector of ˆφL

r corresponding to each observation yir at different leaf node r.

ˆβ = (X ′X)−1X ′(Y − ˆφL) 

(16)

2.2 Simulation Performance

We ﬁrst perform a simulation study with a hierarchy described in [7  8]. The data focus on 2449
Guatemalan children who belong to 1558 families who in turn live in 161 communities. The re-
sponse variable of interest is binary with a positive label assigned to a child if he/she received a
full set of immunizations. The actual data contains 15 covariates capturing individual  family and
community level characteristics as shown in Table 2. For our simulation study  we consider only
three covariates  with the coefﬁcient vector β set with entries all equal to 1. We simulated Gaussian
response as follows: yir|b ∼ N (x
r ∼ N (0  1). We
simulated 100 data sets and compared the estimates from Kalman ﬁlter to the one obtained from
standard routine lme4 in the statistical software R. Results from our procedure agreed almost ex-
actly with those obtained from lme4  our computations was many times faster than lme4. The EM
method converged rapidly and required at most 30 iterations.

r ∼ N (0  4)  and b2

r  10) where b1

irβ + b1

r + b2

′

3 MLH for Non-Gaussian Responses

We discuss model ﬁtting for Bernoulli response but note that other distributions in the general-
ized linear model family can be easily ﬁtted using the procedure. Let yir ∼ Bernoulli(pir)  i.e.
P (yir) = pyir
be the log-odds. The MLH logistic regression is
deﬁned as:

ir (1 − pir)1−yir . Let θir = log pir
θir = x

(17)

1−pir

irβ + φL
r  

′

with the same multi-level prior as described in equation (2). The non-conjugacy of the normal
multi-level prior makes the computation more difﬁcult. We take recourse to Taylor series approxi-
mation coupled with the Kalman ﬁlter algorithm. The estimates obtained are biased; we recommend
cross-validation and parametric bootstrap (adapted from [4]) to correct for the bias. The bootstrap
procedure though expensive is easily parallelizable and accurate.

3.1 Approximation Methods

r   where ˆβ  ˆφL

Let ηir = xir ˆβ + ˆφL
r are current estimates of the parameters in our algorithm. We do
a quadratic approximation of the log-likelihood through a second order Taylor expansion (Laplace
approximation) around ηir. This enables us to do the calculations as in the Gaussian case with the
response yir being replaced by Zir where

Zir = ηir +

2yir − 1

g((2yir − 1)ηir)

 

5

(18)

Algorithm 1 The bootstrap procedure

Let θ = (β  γ).
Obtain ˜θ as an initial estimate of θ. Bias b(0) = 0.
for i = 1 to N do
ˆθ = ˜θ − b(i).
for j = 1 to M do

Use ˆθ to simulate new data j  by simulating φ and the corresponding Y .
For data j  obtain an new estimate of θ as ˜θ(j).

end for
b(i+1) = 1
M

end for

M

P

j=1

˜θ(j) − ˆθ.

and g(x) = 1/(1 + exp(−x)). Approximately 

irβ + φL
r  

Zir ∼ N (x′
ˆβ  and the approximated variance of Zir as Vir. Analogous to equa-

g(ηir)g(−ηir)

(19)

1

).

Now denote eir = Zir − x′
tion (3) and (4)  the resulting ﬁltering step for the leaf nodes becomes:

ir

φL
r|r = σL
r|r

σL
r|r = (

1
ΣL

+

nr

X

i=1

nr

X

i=1

eir
Vir

 

1
Vir

)−1.

The step for estimating β becomes:

(20)

(21)

(22)

ˆβ = (X ′W X)−1X ′W (Z − ˆφL) 

). All the other computational steps remain the same as in the Gaussian case.

where W = diag( 1
Vir

3.2 Bias correction

Table 2 shows estimates of parameters obtained from our approximation method in the column titled
KF . Compared to the unbiased estimates obtained from the slow Gibbs sampler  it is clear our
estimates are biased. Our bias correction procedure is described in Algorithm 1. In general  a value
of M = 50 with about 100 − 200 iterations worked well for us. The bias corrected estimates are
reported under KF-B in Table 2. The estimates after bootstrap correction are closer to the estimates
obtained from Gibbs sampling. It is also customary to estimate hyper parameters like the γ using
a tuning dataset. To test the performance of such a strategy  we created a two-dimensional grid for

(√γ1 √γ2) for the epidemiological Guatemalan data set ranging in [.1  3]×[.1  3] and computed the

log-likelihood on a 10% randomly sampled hold-out data. For each point on the two-dimensional
grid  we estimated the other parameters φ and β  using our EM algorithm that does not update the
value of γ. The estimates at the optimal value of γ are shown in Table 2 under KF-C. The estimates
are better than KF but worse than KF-B.

Based on our ﬁndings  we recommend KF-B when computing resources are available (especially
multiple processors) and running time is not a big constraint; if runtime is an issue we recommend
grid search using a small number of points around the initial estimate.

4 Content Match Data Analysis

We analyze data from an internet advertising application where every showing of an ad on a web
page (called an impression) constitutes an event. The goal is to rank ads on a given page based on
click-through rates. Building a predictive model for click-rates via features derived from pages and

6

Effects
Fixed effects
Individual
Child age ≥ 2 years
Mother age ≥ 25 years
Birth order 2-3
Birth order 4-6
Birth order ≥ 7
Family
Indigenous  no Spanish
Indigenous Spanish
Mother’s education primary
Mother’s education secondary
or better
Husband’s education primary
Husband’s education secondary
or better
Husband’s education missing
Mother ever worked
Community
Rural
Proportion indigenous  1981

Random effects
Standard deviations γ
Family
Community

KF

KF-B

KF-C

Gibbs

0.99
-0.09
-0.10
0.13
0.20

-0.05
0.00
0.22
0.23

0.30
0.27

0.02
0.21

-0.50
-0.67

1.77
-0.16
-0.18
0.25
0.36

-0.11
0.01
0.44
0.44

0.53
0.48

0.04
0.35

-0.91
-1.23

1.18
-0.10
-0.25
0.10
0.21

0.02
0.02
0.32
0.27

0.39
0.35

-0.08
0.24

-0.62
-0.89

1.84
-0.26
-0.29
0.21
0.50

-0.22
-0.11
0.48
0.46

0.59
0.55

0.00
0.42

-0.96
-1.22

0.74
0.56

2.40
1.05

1.92
0.81

2.60
1.13

Table 2: Estimates for the binary MLH model of complete immunization (Kalman Filtering results)

ads is an attractive approach. In our case  semantic features are obtained by classifying pages and
ads into a large seven-level content hierarchy that is manually constructed by humans. We form
a new hierarchy (a pyramid) by taking the cross product of the two hierarchies. This is used to
estimate smooth click-rates of (page ad) pairs.

4.1 Training and Test Data

Although the page and ad hierarchies consist of 7 levels  classiﬁcation is often done at coarser levels
by the classiﬁer. In fact  the average level at which classiﬁcation took place is 3.8. To train our
model  we only consider the top 3 levels of the original hierarchy. Pages and ads that are classiﬁed at
coarser levels are randomly assigned to the children nodes. Overall  the pyramid has 441  25751 and
241292 nodes for the top 3 levels. The training data were collected by conﬁning to a speciﬁc subset
of data which is sufﬁcient to illustrate our methodology but in no way representative of the actual
publisher trafﬁc received by the ad-network under consideration. The training data we collected
spans 23 days and consisted of approximately 11M binary observations with approximately 1.9M
clicks. The test set consisted of 1 day’s worth of data with approximately .5M observations. We
randomly split the test data into 20 equal sized partitions to report our results. The covariates include
the position at which an ad is shown; ranking ads on pages after adjusting for positional effects is
important since the positional effects introduce strong bias in the estimates In the training data a large
fraction of leaf nodes in the pyramid (approx 95%) have zero clicks  this provides a good motivation
to ﬁt the binary MLH on this data to get smoother estimates at leaf nodes by using information at
coarser resolutions.

4.2 Results

We compare the following models using log-likelihood on the test data: a) The model which predicts
a constant probability for all examples  b) 3 level MLH but without positional effects  c) top 2 level
MLH to illustrate the gains of using information at a ﬁner resolution  and d) 3 level MLH with
positional effects to illustrate the generality of the approach; one can incorporate both additional
features and the hierarchy into a single model. Figure 1 shows the distribution of average test
likelihood on the partitions. As expected  all variations of MLH are better than the constant model.
The MLH model which uses only 2 levels is inferior to the 3 level MLH while the general model
that uses both covariates and hierarchy is the best.

7

d
o
o
h

i
l

e
k

i
l

−
g
o
L

5
4

.

2
−

.

5
5
2
−

.

5
6
2
−

2lev

3lev

3lev−pos

con

Model

Figure 1: Distribution of test log-likelihood on 20 equal sized splits of test data.

5 Discussion

In applications where data is aggregated at multiple resolutions with sparsity at ﬁner resolutions 
multi-level hierarchical models provide an attractive class to reduce variance by smoothing estimates
at ﬁner resolutions using data at coarser resolutions. However  the smoothing provides a better bias-
variance tradeoff only when the hierarchy provides a natural clustering for the response variable and
captures some latent characteristics of the process; often true in practice. We proposed a fast novel
algorithm to ﬁt these models based on a multi-scale Kalman ﬁlter that is both scalable and easy to
implement. For the non-Gaussian case  the estimates are biased but performance can be improved
by using a bootstrap correction or estimation through a tuning set. In future work  we will report on
models that generalize our approach to arbitrary number of hierarchies that may all have different
structure. This is a challenging problem since in general cross-product of trees is not a hierarchy but
a graph.

References

[1] D. Agarwal  A. Broder  D. Chakrabarti  D. Diklic  V. Josifovski  and M. Sayyadian. Estimating

rates of rare events at multiple resolutions. In KDD  pages 16–25  2007.

[2] K. C. Chou  A. S. Willsky  and R. Nikoukhah. Multiscale systems  Kalman ﬁlters  and Ricatti

equations. IEEE Transactions on Automatic Control  39:479–492  1994.

[3] A. Gelman and J. Hill. Data Analysis sing Regression and Multi-Level/Hierarchical Models.

Cambridge University Press  2007.

[4] A. Y. C. Kuk. Asymptotically unbiased estimation in generalized linear models with random
effects. Journal of the Royal Statistical Society  Series B (Methodological)   57:395–407  1995.
[5] J. C. Pinheiro and D. M. Bates. Mixed-Effects Models in S and S-PLUS. Springer-Verlag  New

York  2000.

[6] S. W. Raudenbush  M. L. Yang  and M. Yosef. Maximum likelihood for generalized linear
models with nested random effects via high-order  multivariate Laplace approximation. Journal
of Computational and Graphical Statistics  9(1):141–157  2000.

[7] G. Rodriguez and N. Goldman. An assessment of estimation procedures for multilevel models

with binary responses. Journal of Royal Statistical Society  Series A   158:73–89  1995.

[8] G. Rodriguez and N. Goldman.

Improved estimation procedures for multilevel models with
binary response: A case-study. Journal of the Royal Statistical Society  Series A   164(2):339–
355  2001.

[9] S. K. Sahu and A. E. Gelfand. Identiﬁability  improper Priors  and Gibbs sampling for general-

ized linear models. Journal of the American Statistical Association  94(445):247–254  1999.

8

,Samory Kpotufe
Francesco Orabona