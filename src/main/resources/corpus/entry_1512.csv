2013,Direct 0-1 Loss Minimization and Margin Maximization with Boosting,We propose a boosting method  DirectBoost  a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum  DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives consistently better results than AdaBoost  LogitBoost  LPBoost with column generation and BrownBoost  and is noise tolerant when it maximizes an n'th order bottom sample margin.,Direct 0-1 Loss Minimization and Margin

Maximization with Boosting

Shaodan Zhai  Tian Xia  Ming Tan and Shaojun Wang

Kno.e.sis Center

Department of Computer Science and Engineering

Wright State University

{zhai.6 xia.7 tan.6 shaojun.wang}@wright.edu

Abstract

We propose a boosting method  DirectBoost  a greedy coordinate descent algo-
rithm that builds an ensemble classiﬁer of weak classiﬁers through directly min-
imizing empirical classiﬁcation error over labeled training examples; once the
training classiﬁcation error is reduced to a local coordinatewise minimum  Direct-
Boost runs a greedy coordinate ascent algorithm that continuously adds weak clas-
siﬁers to maximize any targeted arbitrarily deﬁned margins until reaching a local
coordinatewise maximum of the margins in a certain sense. Experimental results
on a collection of machine-learning benchmark datasets show that DirectBoost
gives better results than AdaBoost  LogitBoost  LPBoost with column generation
and BrownBoost  and is noise tolerant when it maximizes an n′th order bottom
sample margin.

Introduction

1
The classiﬁcation problem in machine learning and data mining is to predict an unobserved discrete
output value y based on an observed input vector x. In the spirit of the model-free framework  it
is always assumed that the relationship between the input vector and the output value is stochastic
and described by a ﬁxed but unknown probability distribution p(X  Y ) [7]. The goal is to learn a
classiﬁer  i.e.  a mapping function f (x) from x to y ∈ Y such that the probability of the classiﬁcation
error is small. As it is well known  the optimal choice is the Bayes classiﬁer [7]. However  since
p(X  Y ) is unknown  we cannot learn the Bayes classiﬁer directly.
Instead  following Vapnik’s
general setting of the empirical risk minimization [7  24]  we focus on a more realistic goal: Given a
set of training data D = {(x1  y1)  · · ·   (xn  yn)} independently drawn from p(X  Y )  we consider
ﬁnding f (x) in a function class H that minimizes the empirical classiﬁcation error 

1(ˆyi 6= yi)

(1)

1
n

n

Xi=1

where ˆyi = arg maxy∈Y yf (xi)  Y = {−1  1} and 1(·) is an indicator function. Under certain
conditions  direct empirical classiﬁcation error minimization is consistent [24] and under low noise
situations it has a fast convergence rate [15  23]. However  due to the nonconvexity  nondifferen-
tiability and discontinuity of the classiﬁcation error function  the minimization of (1) is typically
NP-hard for general linear models [13]. The common approach is to minimize a surrogate function
which is usually a convex upper bound of the classiﬁcation error function. The problem of minimiz-
ing the empirical surrogate loss turns out to be a convex programming problem with considerable
computational advantages and learned classiﬁers remain consistent to Bayes classiﬁer [1  20  28  29] 
however clearly there is a mismatch between “desired” loss function used in inference and “train-
ing” loss function during the training process [16]. Moreover  it has been shown that all boosting
algorithms based on convex functions are susceptible to random classiﬁcation noise [14].

Boosting is a machine-learning method based on the idea of creating a single  highly accurate clas-
siﬁer by combining many weak and inaccurate “rules of thumb.” A remarkably rich theory and
a record of empirical success [18] have evolved around boosting  nevertheless it is still not clear
how to best exploit what is known about how boosting operates  even for binary classiﬁcation. In

1

this paper  we propose a boosting method for binary classiﬁcation – DirectBoost – a greedy coor-
dinate descent algorithm that directly minimizes classiﬁcation error over labeled training examples
to build an ensemble linear classiﬁer of weak classiﬁers. Once the training error is reduced to a
(local coordinatewise) minimum  DirectBoost runs a coordinate ascent algorithm that greedily adds
weak classiﬁers by directly maximizing any targeted arbitrarily deﬁned margins  it might escape the
region of minimum training error in order to achieve a larger margin. The algorithm stops once a
(local coordinatewise) maximum of the margins is reached. In the next section  we ﬁrst present a
coordinate descent algorithm that directly minimizes 0-1 loss over labeled training examples. We
then describe coordinate ascent algorithms that aims to directly maximize any targeted arbitrarily
deﬁned margins right after we reach a (local coordinatewise) minimum of 0-1 loss. In Section 3  we
show experimental results on a collection of machine-learning benchmark data sets for DirectBoost 
AdaBoost [9]  LogitBoost [11]  LPBoost with column generation [6] and BrownBoost [10]  and dis-
cuss our ﬁndings. Due to space limitation  the proofs of theorems  related works  technical details
as well as conclustions and future works are given in the full version of this paper [27].
2 DirectBoost: Minimizing 0-1 Loss and Maximizing Margins
Let H = {h1  ...  hl} denote the set of all possible weak classiﬁers that can be produced by the
weak learning algorithm  where a weak classiﬁer hj ∈ H is a mapping from an instance space X to
Y = {−1  1}. The hjs are not assumed to be linearly independent  and H is closed under negation 
i.e.  both h and −h belong to H. We assume that the training set consists of examples with labels
{(xi  yi)}  i = 1  · · ·   n  where (xi  yi) ∈ X × Y that are generated independently from p(X  Y ).
We deﬁne C of H as the set of mappings that can be generated by taking a weighted average of
classiﬁers from H:

C =(f : x → Xh∈H

αhh(x) | αh ≥ 0)  

(2)

The goal here is to ﬁnd f ∈ C that minimizes the empirical classiﬁcation error (1)  and has good
generalization performance.

2.1 Minimizing 0-1 Loss
Similar to AdaBoost  DirectBoost works by sequentially running an iterative greedy coordinate
descent algorithm  each time directly minimizing true empirical classiﬁcation error (1) instead of
a weighted empirical classiﬁcation error in AdaBoost. That is  for each iteration  only the parameter
of a weak classiﬁer that leads to the most signiﬁcant true classiﬁcation error reduction is updated 
while the weights of all other weak classiﬁers are kept unchanged. The rationale is that the inference
used to predict the label of a sample can be written as a linear function with a single parameter.
Consider the tth iteration  the ensemble classiﬁer is

t

ft(x) =

αkhk(x)

(3)

Xk=1

where previous t − 1 weak classiﬁers hk(x) and corresponding weights αk  k = 1  · · ·   t − 1 have
been selected and determined. The inference function for sample xi is deﬁned as

Ft(xi  y) = yft(xi) = y (

αkhk(xi)) + αtyht(xi)

(4)

t−1

Xk=1

Since a(xi) = Pt−1
we re-write the equation above as 

k=1 αkhk(xi) is constant and hk(xi) is either +1 or -1 depending on sample xi 

Ft(xi  y) = y ht(xi)αt + ya(xi)

(5)

Note that for each label y of sample xi  there is a linear function of αt with the slope to be either +1 or
-1 and intercept to be ya(xi). Given an input of αt  each example xi has two linear scoring functions 
Ft(xi  +1) and Ft(xi  −1)  i = 1  · · ·   n  one for the positive label y = +1 and one for the negative
label y = −1. From these two linear scoring functions  the one with the higher score determines
the predicted label ˆyi of the ensemble classiﬁer ft(xi). The intersection point ei of these two linear
scoring functions is the critical point that the predicted label ˆyi switches its sign  the intersection
point satisﬁes the condition that Ft(xi  +1) = Ft(xi  −1) = 0  i.e. a(xi) + αtht(xi) = 0  and can
be computed as ei = − a(xi)
ht(xi)   i = 1  · · ·   n. These points divide αt into (at most) n + 1 intervals 
each interval has the value of a true classiﬁcation error  thus the classiﬁcation error is a stepwise

2

Visit each sample in the order that |a(xi)| is increasing.

Algorithm 1 Greedy coordinate descent algorithm that minimizes a 0-1 loss.
1: D = {(xi  yi)  i = 1  · · ·   n}
2: Sort |a(xi)|  i = 1  · · ·   n in an increasing order.
3: for a weak classiﬁer hk ∈ H do
4:
5:
6:
7:
8:
9:
10:
11: end for
12: Pick the weak classiﬁers that lead to largest classiﬁcation error reduction.
13: Among selected these weak classiﬁers  only update the weight of one weak classiﬁer that gives

Compute the slope and intercept of F (xi  yi) = yihk(xi)α + yia(xi).
Let ˆei = |a(xi)|.
If (slope > 0 and intercept < 0)  error update on the righthand side of ˆei is -1.
If (slope < 0 and intercept > 0)  error update on the righthand side of ˆei is +1.

Incrementally calculate classiﬁcation error on intervals of ˆeis.
Get the interval that has minimum classiﬁcation error.

the smallest exponential loss.

14: Repeat 2-13 until training error reaches minimum.

function of αt. The value of ei  i = 1  · · ·   n can be negative or positive  however since H is closed
in negation  we only care about these that are positive.

The greedy coordinate descent algorithm that sequentially minimizes a 0-1 loss is described in
Algorithm 1  lines 3-11 are the weak learning steps and the rest are boosting steps. Consider
an example with 4 samples to illustrate this procedure. Suppose for a weak classiﬁer  we have
Ft(xi  yi)  i = 1  2  3  4 as shown in Figure 1. At αt = 0  samples x1 and x2 have negative margins 
thus they are misclassiﬁed  the error rate is 50%. We incrementally update the classiﬁcation error on
intervals of ˆei  i = 1  2  3  4: For Ft(x1  y1)  its slope is negative and its intercept is negative  sample
x1 always has a negative margin for αt > 0  thus there is no error update on the right-hand side of
ˆe1. For Ft(x2  y2)  its slope is positive and its intercept is negative  then when αt is at the right side
of ˆe2  sample x2 has positive margin and becomes correctly classiﬁed  so we update the error by -1 
the error rate is reduced to 25%. For Ft(x3  y3)  its slope is negative and its intercept is positive  then
when αt is at the right side of ˆe3  sample x3 has a negative margin and becomes misclassiﬁed  so
we update the error rate changes to 50% again. For Ft(x4  y4)  its slope is positive and its intercept
is positive  sample x4 always has positive margin for αt > 0  thus there is no error update on the
right-hand side of ˆe4. We ﬁnally have the minimum error rate of 25% on the interval of [ˆe2  ˆe3].
We repeat this procedure until the training error reaches
its minimum  which may be 0 in a data separable case.
We then go to the next stage  explained below  that aims to
maximize margins. A nice property of the above greedy
coordinate descent algorithm is that the classiﬁcation er-
ror is monotonically decreasing. Assume there are M
weak classiﬁers be considered  the computational com-
plexity of Algorithm 1 in the training stage is O(M n) for
each iteration.

Ft(x3  y3)

Ft(x1  y1)

Ft(x4  y4)

|a2|
|a1|

a4 |a4|

a3 |a3|

0
a1
a2

ˆe1

ˆe2

ˆe3

ˆe4

αt

Ft(x2  y2)

50%

25%

0

Classiﬁcation error

ˆe2

ˆe3

αt

For boosting  as long as the weaker learner is strong
enough to achieve reasonably high accuracy  the data will
be linearly separable and the minimum 0-1 loss is usually
0. As shown in Theorem 1  the region of zero 0-1 loss is
a (convex) cone.

Figure 1: An example of computing mini-
mum 0-1 loss of a weak learner over 4 sam-
ples.
Theorem 1 The region of zero training error  if exists  is a cone  and it is not a set of isolated cones.
Algorithm 1 is a heuristic procedure that minimizes 0-1 loss  it is not guaranteed to ﬁnd the global
minimum  it may trap to a coordinatewise local minimum [22] of 0-1 loss. Nevertheless  we switch
to algorithms that directly maximize the margins we present below.
2.2 Maximizing Margins
The margins theory [17] provides an insightful analysis for the success of AdaBoost where the
authors proved that the generalization error of any ensemble classiﬁers is bounded in terms of the

3

entire distribution of margins of training examples  as well as the number of training examples and
the complexity of the base classiﬁers  and AdaBoost’s dynamics has a strong tendency to increase the
margins of training examples. Instead  we can prove that the generalization error of any ensemble
classiﬁer is bounded in terms of the average margin of bottom n′ samples or n′th order margin
of training examples  as well as the number of training examples and the complexity of the base
classiﬁers. This view motivates us to propose a coordinate ascent algorithm to directly maximize
several types of margins just right after the training error reaches a (local coordinatewise) minimum.
The margin of a labeled example (xi  yi) with respect
to an ensemble classiﬁer ft(x) =
Pt

k=1 αkhk(xi) is deﬁned to be

mi =

k=1 αkhk(xi)

k=1 αk

yiPt
Pt

(6)

This is a real number between -1 and +1 that intuitively measures the conﬁdence of the classiﬁer in
its prediction on the ith example. It is equal to the weighted fraction of base classiﬁers voting for
the correct label minus the weighted fraction voting for the incorrect label [17].

We denote the minimum margin  the average margin  and median margin over the training examples
as gmin = mini∈{1 ···  n} mi  gaverage = 1
i=1 mi  and gmedian = median{mi  i = 1  · · ·   n}.
Furthermore  we can sort the margins over all training examples in an increasing order  and consider
n′ worst training examples n′ ≤ n that have smaller margins  and compute the average margin over
those n′ training examples. We call this the average margin of the bottom n′ samples  and denote
it as gaverage n′ = 1
n′ Pi∈Bn′ mi  where Bn′ denotes the set of n′ samples having the smallest
margins.

n Pn

The margin maximization method described below is a greedy coordinate ascent algorithm that adds
a weak classiﬁer achieving maximum margin. It allows us to continuously maximize the margin
while keeping the training error at a minimum by running the greedy coordinate descent algorithm
presented in the previous section. The margin mi is a linear fractional function of α  and it is
quasiconvex  and quasiconcave  i.e.  quasilinear [2  5]. Theorem 2 shows that the average margin of
bottom n′ examples is quasiconcave in the region of the zero training error.
Theorem 2 Denote the average margin of bottom n′ samples as

gaverage n′ (α) = Xi∈{Bn′ |α}

k=1 αkhk(xi)

k=1 αk

yiPt
Pt

where {Bn′ |α} denotes the set of n′ samples whose margins are at the bottom for ﬁxed α. Then
gaverage n′ (α) in the region of zero training error is quasiconcave.
We denote ai = Pt−1
margin on the ith example (xi  yi) can be rewritten as 
ai + bi tαt

k=1 yiαkhk(xi)  bi t = yiht(xi) ∈ {−1  +1} and c = Pt−1

k=1 αk  then the

(7)

mi =

c + αt

The derivative of the margin on ith example with respect to αt is calculated as 

Margin

m6

m5
m4

m3
m2

m1

0

q1

q2

q3

q4

d

αt

Figure 2: Margin curves of six exam-
ples. At points q1  q2  q3 and q4  the me-
dian example is changed. At points q2
and q4  the set of bottom n′ = 3 exam-
ples are changed.

∂gaverage

∂αt

∂mi
∂αt

=

bi tc − ai
(c + αt)2

(8)

Since c ≥ ai  depending on the sign of bi t  the derivative
of the margin on the ith sample (xi  yi) is either positive or
negative  which is irrelevant to the value of αt. This is also
true for the second derivative of the margin. Therefore  the
margin on the ith example (xi  yi) with respect to αt is either
concave when it is monotonically increasing or convex when
it is monotonically decreasing. See Figure 2 for a simple
illustration.
Consider a greedy coordinate ascent algorithm that maxi-
mizes the average margin gaverage over all training examples.
The derivative of gaverage can be written as 

= Pn

i=1 bi tc −Pn

(c + αt)2

i=1 ai

4

(9)

Algorithm 2 Greedy coordinate ascent algorithm that maximizes the average margin of bottom n′
examples.
1: Input: ai=1 ···  n and c from previous round.
2: Sort ai=1 ···  n in an increasing order. Bn′ ← {n′ samples having the smallest ai at αt = 0}.
3: for a weak classiﬁer do
4:
5:
6:
7:

Determine the lowest sample whose margin is decreasing and determine d.
Compute Dn′ ← Pi∈Bn′ (bi tc − ai).
j ← 0  qj ← 0.
Compute the intersection qj+1 of the j + 1th highest increasing margin in Bn′ and the j + 1th
smallest decreasing margin in Bc
if qj+1 < d and Dn′ > 0 then

n′ (the complement of the set Bn′).

Incrementally update Bn′  Bc
Go back to Line 7.

8:
9:
10:
11:
12:
13:
end if
14:
15: end for
16: Pick the weak classiﬁer with the largest increment of the average margin of bottom n′ examples

if Dn′ > 0 then q∗ ← d; otherwise q∗ ← qj.
Compute the average margin of the bottom n′ examples at q∗.

n′ and Dn′ at αt = qj+1; j ← j + 1.

else

with weight being q∗.

17: Repeat 2-16 until no increment in average margin of bottom n′ examples.

Therefore  the maximum average margin can only happen at two ends of the interval. As shown in
Figure 2  the maximum average margin is either at the origin or at point d  which depends on the
sign of the derivative in (9). If it is positive  the average margin is monotonically increasing  we set
αt = d − ǫ  otherwise we set αt = 0. The greedy coordinate ascent algorithm found by: looking
at all weak classiﬁers in H  if the nominator in (9) is positive  we let its weight ǫ close to the right
value on the interval where the training error is minimum  and compute the value of the average
margin. We add the weak classiﬁer which has the largest average margin increment. We iterate this
procedure until convergence. Its convergence is given by Theorem 3 shown below.

Theorem 3 When constrained to the region of zero training error  the greedy coordinate ascent
algorithm that maximizes the average margin over all examples converges to an optimal solution.

Now consider a greedy coordinate ascent algorithm maximizing the average margin of bottom n′
training examples  gaverage n′. Apparently maximizing the minimum margin is a special case by
choosing n′ = 1. Figure 2 is a simple illustration with six training examples. Our aim is to maximize
the average margin of the bottom 3 examples. The interval [0  d] of αt indicates an interval where
the training error is zero. On the point of d  the sample margin m3 alters from positive to negative 
which causes the training error jump from 0 to 1/6. As shown in Figure 2  the margin of each of six
training examples is either monotonically increasing or decreasing.
If we know a ﬁxed set of bottom n′ training examples having smaller margins for an interval of αt
with a minimum training error  it is straightforward to compute the derivative of the average margin
of bottom n′ training examples as

∂gaverage n′

∂αt

= Pi∈Bn′

bi tc −Pi∈Bn′

(c + αt)2

ai

(10)

Again gaverage n′ is a monotonic function of αt  depending on the sign of the derivative in (10)  it is
maximized either on the left side or on the right side of the interval.
In general  the set of bottom n′ training examples for an interval of αt with a minimum training
error varies over αt  it is required to precisely search for any snapshot of bottom n′ examples with a
different value of α.
To address this  we ﬁrst examine when the margins of two examples intersect. Consider the ith
example (xi  yi) with margin mi = ai+bi tαt
and the jth example (xj  yj) with margin mj =
aj +bj tαt
. Notice bi  bj is either -1 or +1. Assume bi = bj  then because mi 6= mj (since ai 6= aj) 
the margins of example i and example j never intersect; assume bi 6= bj  then because mi = mj

c+αt

c+αt

5

2

at αt = |ai−aj |
  the margins of example i and example j might intersect with each other if |ai−aj |
belongs to the interval of αt with the minimum training error. In summary  given any two samples 
we can decide whether they intersect by checking whether b terms have the same sign  if not  they
do intersect  and we can determine the intersection point.

2

The greedy coordinate ascent algorithm that sequentially maximizes the average margin of bottom
n′ examples is described in Algorithm 2  lines 3-15 are the weak learning steps and the rest are
boosting steps. At line 5 we compute Dn′ which can be used to check the sign of the derivative
in (10). Since the function of the average margin of bottom n′ examples is quasiconcave  we can
determine the optimal point q∗ by Dn′  and only need to compute the margin value at q∗. We add the
weak learner  which has the largest increment of the average margin over bottom n′ examples  into
the ensembled classiﬁer. This procedure terminates if there is no increment in the average margin
of bottom n′ examples over the considered weak classiﬁers. If M weak learners are considered  the
computational complexity of Algorithm 2 in the training stage is O (max(n log n  M n′)) for each
iteration. The convergence analysis of Algorithm 2 is given by Theorem 4.
Theorem 4 When constrained to the region of zero training error  the greedy coordinate ascent
algorithm that maximizes average margin of bottom n′ samples converges to a coordinatewise max-
imum solution  but it is not guaranteed to converge to an optimal solution due to the non-smoothness
of the average margin of bottom n′ samples.
ǫ-relaxation: Unfortunately  there is a fundamental difﬁculty in the greedy coordinate ascent al-
gorithm that maximizes the average margin of bottom n′ samples: It gets stuck at a corner  from
which it is impossible to make progress along any coordinate direction. We propose an ǫ-relaxation
method to overcome this difﬁculty. This method was ﬁrst proposed by [3] for the assignment prob-
lem  and was extended to the linear cost network ﬂow problem and strictly convex costs and linear
constraints [4  21]. The main idea is to allow a single coordinate to change even if this worsens the
margin function. When a coordinate is changed  it is set to ǫ plus or ǫ minus the value that maximizes
the margin function along that coordinate  where ǫ is a positive number.
We can design a similar greedy coordinate ascent algorithm to directly maximize the bottom n′th
sample margin by only making a slight modiﬁcation to Algorithm 2: for a weak classiﬁer  we choose
the intersection point that led to the largest increasing of the bottom n′th margin. When combined
with ǫ-relaxation  this algorithm will eventually approach a small neighbourhood of a local optimal
solution that maximizes the bottom n′th sample margin. As shown in Figure 2  bottom n′th margin
is a multimodal function  this algorithm with ǫ-relaxation is very sensitive to the choice of n′  and it
usually gets stuck in a bad coordinatewise point without using ǫ-relaxation. However  an impressive
advantage is that this method is tolerant to noise  which will be shown in Section 3.
3 Experimental Results
In the experiments below  we ﬁrst evaluate the performance of DirectBoost on 10 UCI data sets.
We then evaluate noise robustness of DirectBoost. For all the algorithms in our comparison  we
use decision trees with depth of either 1 or 3 as weak learners since for the small datasets  decision
stumps (tree depth of 1) is already strong enough. DirectBoost with decision trees is implemented
by a greedy top-down recursive partition algorithm to ﬁnd the tree but differently from AdaBoost
and LPBoost  since DirectBoost does not maintain a distribution over training samples. Instead  for
each splitting node  DirectBoost simply chooses the attribute to split on by minimizing 0-1 loss or
maximizing the predeﬁned margin value. In all the experiments that ǫ-relaxation is used  the value
of ǫ is 0.01. Note that our empirical study is focused on whether the proposed boosting algorithm
is able to effectively improve the accuracy of state-of-the-art boosting algorithms with the same
weak learner space H  thus we restrict our comparison to boosting algorithms with the same weak
learners  rather than a wide range of classiﬁcation algorithms  such as SVMs and KNN.

3.1 Experiments on UCI data
We ﬁrst compare DirectBoost with AdaBoost  LogitBoost  soft margin LPBoost and BrownBoost
on 10 UCI data sets1 from the UCI Machine Learning Repository [8]. We partition each UCI dataset
into ﬁve parts with the same number of samples for ﬁve-fold cross validation. In each fold  we use
three parts for training  one part for validation  and the remaining part for testing. The validation

1For Adult data  where we use a subset a5a in LIBSVM set http://www.csie.ntu.edu.tw/˜cjlin/libsvm. We

do not use the original Adult data which has 48842 examples since LPBoost runs very slow on it.

6

LPBoost BrownBoost DirectBoostavg DirectBoostǫ
N D depth AdaBoost LogitBoost
1.15(0.8)
2.62(0.8)
1.47(0.7)
9
958
1.47(1.0)
25.49(3.0)
26.01(3.3)
27.71(1.7) 27.32(1.3)
768
8
13.33(3.0)
16.23(2.6) 14.49(4.4)
14.2(1.8)
690 14
2.44(1.6)
3.02(2.3)
1.86(1.3)
862
2
1.86(1.3)
8.29(2.7)
8.57(2.7)
9.71(3.1)
9.71(3.7)
351 34
4.0(0.5)
4.8(1.4)
5.3(2.6)
5.3(1.4)
1000 61
4.07(2.0)
4.25(2.5)
Cancer-wdbc 569 29
4.42(1.4)
3.89(1.5)
24.62(7.6)
27.69(7.6) 30.26(7.3) 26.15(10.5)
Cancer-wpbc 198 32
16.67(7.5)
270 13
17.41(7.7) 18.52(5.1)
19.26(8.1)
15.28(0.8)
16.2(1.1)
15.39(0.8)
15.6(0.7)
6414 14

3.66(1.3)
26.67(2.6)
13.77(4.6)
2.33(1.7)
10.86(2.8)
6.1(1.1)
4.25(2.2)
28.72(8.4)
18.15(7.2)
15.56(0.9)

0.63(0.4)
25.62(2.5)
14.06(3.6)
2.33(1.0)
7.71(3.0)
4.8(0.7)
4.96(3.0)
27.69(8.1)
18.15(5.1)
16.25(1.7)

3
3
3
3
3
3
1
1
1
3

Datasets
Tic-tac-toe
Diabetes
Australian
Fourclass
Ionosphere

Splice

Heart
Adult

avg DirectBoostorder

1.05(0.4)
23.4(3.7)
13.48(2.9)
1.74(1.5)
7.71(4.4)
6.7(1.6)
3.72(2.9)
27.18(10.0)
18.15(7.6)
15.8(1.1)

Table 1: Percent test errors of AdaBoost  LogitBoost  soft margin LPBoost with column generation  Brown-
Boost  and three DirectBoost methods on 10 UCI datasets each with N samples and D attributes.

set is used to choose the optimal model for each algorithm: For AdaBoost and LogitBoost  the
validation data is used to perform early stopping since there is no nature stopping criteria for these
algorithms. We run the algorithms until convergence where the stopping criterion is that the change
of loss is less than 1e-6  and then choose the ensemble classiﬁer from the round with minimum error
on the validation data. For BrownBoost  we select the optimal cutoff parameters by the validation
set  which are chosen from {0.0001  0.001  0.01  0.03  0.05  0.08  0.1  0.14  0.17  0.2}. LPBoost
maximizes the soft margin subject to linear constraints  its objective is equivalent to DirectBoost
with maximizing the average margin of bottom n′ samples [19]  thus we set the same candidate
parameters n′/n = {0.01  0.05  0.1  0.2  0.5  0.8} for them. For LPBoost  the termination rule we
use is same to the one in [6]  and we select the optimal regularization parameter by the validation
set. For DirectBoost  the algorithm terminates when there is no increment in the targeted margin
value  and we select the model with the optimal n′ by the validation set.
We use DirectBoostavg to denote our method that runs Algorithm 1 ﬁrst and then maximizes the
average of bottom n′ margins without ǫ-relaxation  DirectBoostǫ
avg to denote our method that runs
Algorithm 1 ﬁrst and then maximizes the average margin of bottom n′ samples with ǫ-relaxation  and
DirectBoostorder to denote our method that runs Algorithm 1 ﬁrst and then maximizes the bottom
n′th margin with ǫ-relaxation. The means and standard deviations of test errors are given in Table 1.
Clearly DirectBoostavg  DirectBoostǫ
avg and DirectBoostorder outperform other boosting algorithms
avg is better than AdaBoost  LogitBoost  LPBoost and BrownBoost
in general  specially DirectBoostǫ
over all data sets except Cancer-wdbc. Among the family of DirectBoost algorithms  DirectBoostavg
wins on two datasets where it searches the optimal margin solution in the region of zero training
error  this means that keeping the training error at zero may lead to good performance in some
cases. DirectBoostorder wins on three other datasets  but its results are unstable and sensitive to
n′. With ǫ-relaxation  DirectBoostǫ
avg searches the optimal margin solution in the whole parameter
space and gives the best performance on the remaining 5 data sets. It is well known that AdaBoost
performs well on the datasets with a small test error such as Tic-tac-toe and Fourclass  it is extremely
hard for other boosting algorithms to beat AdaBoost. Nevertheless  DirectBoost is still able to give
even better results in this case. For example  on Tic-tac-toe data set  the test error becomes 0.63% 
more than half the error rate reduction. Our method would be more valuable for those who value
prediction accuracy  which might be the case in areas of medical and genetic research.

DirectBoostǫ
avg and LPBoost are both designed
to maximize the average margin over bot-
tom n′ samples [19]  but as shown by the
avg gener-
left ﬁgure in Figure 3  DirectBoostǫ
ates a larger margin value than LPBoost when
decision trees with depth greater than 1 are
used as weak learners  this may explain why
DirectBoostǫ
avg outperforms LPBoost. When
decision stumps are used as weak learners 
LPBoost converges to a global optimal solu-
tion  and DirectBoostǫ
avg nearly converges to
the maximum margin as shown by the right ﬁg-

ure in Figure 3  even though no theoretical justiﬁcation is known for this observed phenomenon.

Figure 3: The value of average margins of bottom n′
samples vs. the number of iterations for LPBoost with
column generation and DirectBoostǫ
avg on Australian
dataset  left: Decision tree  right: Decision stump.

7

# of iterations

Total running times

AdaBoost
LPBoost
DirectBoostǫ

Table 2 shows the number of iterations and total
run times (in seconds) for AdaBoost  LPBoost
avg at the training stage  where
and DirectBoostǫ
we use the Adult dataset with 10000 training
samples. All these three algorithms employ de-
cision trees with a depth of 3 as weak learners.
The experiments are conducted on a PC with
Core2 Duo 2.6GHz CPU and 2G RAM. Clearly
avg takes less time for the entire training stage since it converges much faster. LPBoost
DirectBoostǫ
converges in less than three hundred rounds  but as a total corrective algorithm  it has a greater com-
putational cost on each round. To handle large scale data sets in practice  similar to AdaBoost  we
can use many tricks. For example  we can partition the data into many parts and use distributed
algorithms to select the weak classiﬁer.

Table 2: Number of iterations and total run times (in
seconds) in training stage on Adult dataset with 10000
training samples and the depth of DecisionTrees is 3.

31168
167520

606

117852

286
1737

avg

3.2 Evaluate noise robustness
In the experiments conducted below  we evaluate the noise robustness of each boosting method.
First  we run the above algorithms on a synthetic example created by [14]. This is a simple coun-
terexample to show that for a broad class of convex loss functions  no boosting algorithm is provably
robust to random label noise  this class includes AdaBoost  LogitBoost  etc. For LPBoost and its
variations [25  26]  they do not satisfy the preconditions of the theorem presented by [14]  but Glo-
cer [12] showed experimentally that these soft margin boosting methods have the same problem as
the AdaBoost and LogitBoost to handle random noise.

l
5

20

η
0

0.05
0.2
0

0.05
0.2

AB
0

17.6
24.2

0

30.0
29.9

LB
0
0

LPB

0
0

23.4

14.5

0

29.6
30.0

0

27.0
29.8

BB DBǫ
avg DBorder
0
0
1.2
0
2.2
0.6
15.0
19.6

0
0
0
0
0
3.2

25.4
29.6

24.7

0

data
wdbc

Iono.

η
0

0.05
0.2
0

0.05
0.2

AB
4.3
6.6
8.8
9.7
10.3
16.6

LB
4.4
6.8
8.8
9.7
12.3
15.0

LPB
4.0
4.9
7.6
8.6
9.3
14.6

BB DBǫ
4.1
4.5
5.0
6.5
8.4
8.3
8.8
8.3
9.3
11.5
17.9
14.4

3.7
5.0
6.6
7.7
8.6
9.5

avg DBorder

Percent

Table 3:
test errors of AdaBoost (AB) 
LogitBoost (LB)  LPBoost (LPB)  BrownBoost (BB) 
DirectBoostǫ
avg  and DirectBoostorder on Long and
Servedio’s example with random noise.

Percent

Table 4:
test errors of AdaBoost (AB) 
LogitBoost (LB)  LPBoost (LPB)  BrownBoost (BB) 
DirectBoostǫ
and DirectBoostorder on two UCI
datasets with random noise.

avg 

We repeat the synthetic learning problem with binary-valued weak classiﬁers that is described
in [14]. We set the number of training examples to 1000 and the labels are corrupted with a noise
rate η at 0%  5%  and 20% respectively. Examples in this setting are binary vectors of length 2l +11.
Table 3 reports the error rates on a clean test data set with size 5000  that is  the labels of test data
are uncorrupted  and a same size clean data is generated as validation data. AdaBoost performs very
poor on this problem. This result is not surprising at all since [14] designed this example on pur-
pose to explain the inadequacy of convex optimization methods. LogitBoost  LPBoost with column
generation  and DirectBoostǫ
avg perform better in the case that l = 5 and η = 5%  but for the other
cases they do as bad as AdaBoost. BrownBoost is designed for noise tolerance  and it does well in
the case of l = 5  but it also cannot handle the case of l = 20 and η > 0%. On the other hand 
DirectBoostorder performs very well for all cases  showing DirectBoostorder’s impressive noise tol-
erance property since the most difﬁcult examples are given up without any penalty.

These algorithms are also tested on two UCI datasets  randomly corrupted with additional label
noise on training data at rates of 5% and 20% respectively. Again  we keep the validation and the
test data are clean. The results are reported in Table 4 by ﬁve-fold cross validation  the same as
avg and DirectBoostorder do well in
Experiment 1. LPBoost with column generation  DirectBoostǫ
the case of η = 5%  and their performance is better than AdaBoost  LogitBoost  and BrownBoost.
For the case of η = 20%  all the algorithms perform much worse than the corresponding noise-free
case  except DirectBoostorder which still generates a good performance close to the noise-free case.

4 Acknowledgements
This research is supported in part by AFOSR under grant FA9550-10-1-0335  NSF under grant
IIS:RI-small 1218863  DoD under grant FA2386-13-1-3023  and a Google research award.

8

References

[1] P. Bartlett and M. Traskin. AdaBoost is consistent. Journal of Machine Learning Research  8:2347–2368 

2007.

[2] M. Bazaraa  H. Sherali and C. Shetty. Nonlinear Programming: Theory and Algorithms  3rd Edition.

Wiley-Interscience  2006.

[3] D. P. Bertsekas. A distributed algorithm for the assignment problem. Technical Report  MIT  1979.
[4] D. Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc  1998.
[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[6] A. Demiriz  K. Bennett and J. Shawe-Taylor. Linear programming boosting via column generation  Ma-

chine Learning  46:225–254  2002.

[7] L. Devroye  L. Gy¨orﬁ and G. Lugosi. A Probabilistic Theory of Pattern Recognition Springer  New York 

1996.

[8] A. Frank and A. Asuncion. UCI Machine Learning Repository. School of Information and Computer

Science  University of California at Irvine  2006.

[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to

boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[10] Y. Freund. An adaptive version of the boost by majority algorithm. Machine Learning  43(3):293–318 

2001.

[11] J. Friedman  T. Hastie and R. Tibshirani. Additive logistic regression: A statistical view of boosting. The

Annals of Statistics  28(2):337–374  2000.

[12] K. Glocer. Entropy regularization and soft margin maximization. Ph.D. Dissertation  UCSC  2009.
[13] K. Hoffgen  H. Simon and K. van Horn. Robust trainability of single neurons. Journal of Computer and

System Sciences  50(1):114–125  1995.

[14] P. Long and R. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine

Learning  78:287-304  2010.

[15] E. Mammen and A. Tsybakov. Smooth discrimination analysis. The Annals of Statistics  27  1808-1829 

1999.

[16] D. McAllester  T. Hazan and J. Keshet. Direct loss minimization for structured prediction. Neural Infor-

mation Processing Systems (NIPS)  1594-1602  2010.

[17] R. Schapire  Y. Freund  P. Bartlett and W. Lee. Boosting the margin: A new explanation for the effective-

ness of voting methods. The Annals of Statistics  26(5):1651–1686  1998.

[18] R. Schapire and Y. Freund. Boosting: Foundations and Algorithms. MIT Press  2012.
[19] S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: new

relaxations and efﬁcient boosting algorithms. Machine Learning  80(2-3): 141-163  2010.

[20] I. Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers.

Transactions on Information Theory  51(1):128-142  2005.

IEEE

[21] P. Tseng and D. Bertsekas. Relaxation methods for strictly convex costs and linear constraints. Mathe-

matics of Operations Research  16:462-481  1991.

[22] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. Journal

of Optimization Theory and Applications  109(3):475–494  2001.

[23] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics  32(1):135-

166  2004.

[24] V. Vapnik. Statistical Learning Theory. John Wiley  1998.
[25] M. Warmuth  K. Glocer and G. Ratsch. Boosting algorithms for maximizing the soft margin. Advances

in Neural Information Processing Systems (NIPS)  21  1585-1592  2007.

[26] M. Warmuth  K. Glocer and S. Vishwanathan. Entropy regularized LPBoost. The 19th International

conference on Algorithmic Learning Theory (ALT)  256-271  2008.

[27] S. Zhai  T. Xia  M. Tan and S. Wang. Direct 0-1 loss minimization and margin maximization with

boosting. Technical Report  2013.

[28] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimiza-

tion. The Annals of Statistics  32(1):56–85  2004.

[29] T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. The Annals of Statistics 

33:1538–1579  2005.

9

,Shaodan Zhai
Tian Xia
Ming Tan
Shaojun Wang