2017,Revenue Optimization with Approximate Bid Predictions,In the context of advertising auctions  finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types  and the non-convexity of the objective function. In this work  we show how to reduce reserve price optimization to the standard setting of prediction under squared loss  a well understood problem in the learning community.  We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.,Revenue Optimization with Approximate Bid

Predictions

Andr´es Mu˜noz Medina

Google Research

76 9th Ave

New York  NY 10011

Sergei Vassilvitskii
Google Research

76 9th Ave

New York  NY 10011

Abstract

In the context of advertising auctions  ﬁnding good reserve prices is a notoriously
challenging learning problem. This is due to the heterogeneity of ad opportunity
types  and the non-convexity of the objective function. In this work  we show how
to reduce reserve price optimization to the standard setting of prediction under
squared loss  a well understood problem in the learning community. We further
bound the gap between the expected bid and revenue in terms of the average loss
of the predictor. This is the ﬁrst result that formally relates the revenue gained to
the quality of a standard machine learned model.

1

Introduction

A crucial task for revenue optimization in auctions is setting a good reserve (or minimum) price. Set
it too low  and the sale may yield little revenue  set it too high and there may not be anyone willing
to buy the item. The celebrated work by Myerson [1981] shows how to optimally set reserves in
second price auctions  provided the value distribution of each bidder is known.
In practice there are two challenges that make this problem signiﬁcantly more complicated. First 
the value distribution is never known directly; rather  the auctioneer can only observe samples drawn
from it. Second  in the context of ad auctions  the items for sale (impressions) are heterogeneous  and
there are literally trillions of different types of items being sold. It is therefore likely that a speciﬁc
type of item has never been observed previously  and no information about its value is known.
A standard machine learning approach addressing the heterogeneity problem is to parametrize each
impression by a feature vector  with the underlying assumption that bids observed from auctions
with similar features will be similar. In online advertising. these features encode  for instance  the
ad size  whether it’s mobile or desktop  etc.
The question is  then  how to use the features to set a good reserve price for a particular ad opportu-
nity. On the face of it  this sounds like a standard machine learning question—given a set of features 
predict the value of the maximum bid. The difﬁculty comes from the shape of the loss function.
Much of the machine learning literature is concerned with optimizing well behaved loss functions 
such as squared loss  or hinge loss. The revenue function  on the other hand is non-continuous and
strongly non-concave  making a direct attack a challenging proposition.
In this work we take a different approach and reduce the problem of ﬁnding good reserve prices to
a prediction problem under the squared loss. In this way we can rely upon many widely available
and scalable algorithms developed to minimize this objective. We proceed by using the predictor to
deﬁne a judicious clustering of the data  and then compute the empirically maximizing reserve price
for each group. Our reduction is simple and practical  and directly ties the revenue gained by the
algorithm to the prediction error.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 Related Work

Optimizing revenue in auctions has been a rich area of study  beginning with the seminal work
of Myerson [1981] who introduced optimal auction design. Follow up work by Chawla et al.
[2007] and Hartline and Roughgarden [2009]  among others  reﬁned his results to increasingly
more complex settings  taking into account multiple items  diverse demand functions  and weaker
assumptions on the shape of the value distributions.
Most of the classical literature on revenue optimization focuses on the design of optimal auctions
when the bidding distribution of buyers is known. More recent work has considered the compu-
tational and information theoretic challenges in learning optimal auctions from data. A long line
of work [Cole and Roughgarden  2015  Devanur et al.  2016  Dhangwatnotai et al.  2015  Morgen-
stern and Roughgarden  2015  2016] analyzes the sample complexity of designing optimal auctions.
The main contribution of this direction is to show that under fairly general bidding scenarios  a
near-optimal auction can be designed knowing only a polynomial number of samples from bidders’
valuations. Other authors  [Leme et al.  2016  Roughgarden and Wang  2016] have focused on the
computational complexity of ﬁnding optimal reserve prices from samples  showing that even for
simple mechanisms the problem is often NP-hard to solve directly.
Another well studied approach to data-driven revenue optimization is that of online learning. Here 
auctions occur one at a time  and the learning algorithm must compute prices as a function of the his-
tory of the algorithm. These algorithms generally make no distributional assumptions and measure
their performance in terms of regret: the difference between the algorithm’s performance and the
performance of the best ﬁxed reserve price in hindsight. Kleinberg and Leighton [2003] developed
an online revenue optimization algorithm for posted-price auctions that achieves low regret. Their
work was later extended to second-price auctions by Cesa-Bianchi et al. [2015].
A natural approach in both of these settings is to attempt to predict an optimal reserve price  equiv-
alently the highest bid submitted by any of the buyers. While the problem of learning this reserve
price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al. 
2015  Devanur et al.  2016  Kleinberg and Leighton  2003]  the problem becomes much more chal-
lenging in practice  when the valuations of a buyer also depend on features associated with the ad
opportunity (for instance user demographics  and publisher information).
This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014]
provide learning guarantees and an algorithm based on DC programming to optimize revenue in
second-price auctions with reserve. The proposed algorithm  however  does not easily scale to large
auction data sets as each iteration involves solving a convex optimization problem. A smoother
version of this algorithm is given by [Rudolph et al.  2016]. However  being a highly non-convex
problem  neither algorithm provides a guarantee on the revenue attainable by the algorithm’s output.
Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side
information. However  the authors consider only cases where this side information is given by
σ ∈ [0  1]. More importantly  their proposed algorithm only works under the unveriﬁable assumption
that the conditional distributions of bids given σ satisfy stochastic dominance.
Our results. We show that given a predictor of the bid with squared loss of η2  we can construct
a reserve function r that extracts all but g(η) revenue  for a simple increasing function g.
(See
Theorem 2 for the exact statement.) To the best of our knowledge  this is the ﬁrst result that ties
the revenue one can achieve directly to the quality of a standard prediction task. Our algorithm for
computing r is scalable  practical  and efﬁcient.
Along the way we show what kinds of distributions are amenable to revenue optimization via reserve
prices. We prove that when bids are drawn i.i.d. from a distribution F   the ratio between the mean bid
and the revenue extracted with the optimum monopoly reserve scales as O(log Var(F )) – Theorem
5. This result reﬁnes the log h bound derived by Goldberg et al. [2001]  and formalizes the intuition
that reserve prices are more successful for low variance distributions.
2 Setup
We consider a repeated posted price auction setup where every auction is parametrized by a feature
vector x ∈ X and a bid b ∈ [0  1]. Let D be a distribution over X × [0  1]. Let h : X → [0  1]  be a
bid prediction function and denote by η2 the squared loss incurred by h:

E[(h(x) − b)2] = η2.

2

We assume h is given  and make no assumption on the structure of h or how it is obtained. Notice
that while the existence of such h is not guaranteed for all values of η  using historical data one
could use one of multiple readily available regression algorithms to ﬁnd the best hypothesis h.

Let S =(cid:0)(x1  b1)  . . .   (xm  bm)(cid:1) ∼ D be a set of m i.i.d. samples drawn from D and denote by

SX = (x1  . . .   xm) its projection on X . Given a price p let Rev(p  b) = p1b≥p denote the revenue
obtained when the bidder bids b. For a reserve price function r : X → [0  1] we let:

(cid:98)R(r) =

1
m

(cid:88)

(x b)∈S

and

Rev(r(x)  b)

R(r) = E

(x b)∼D

(cid:2)Rev(r(x)  b)(cid:3)
(cid:80)m

denote the expected and empirical revenue of reserve price function r.

We also let B = E[b]  (cid:98)B = 1
B − R(r)  (cid:98)S(r) = (cid:98)B − (cid:98)R(r) denote the expected and empirical separation between bid values

i=1 bi denote the population and empirical mean bid  and S(r) =

and the revenue. Notice that for a given reserve price function r  S(r) corresponds to revenue left
on the table. Our goal is  given S and h  to ﬁnd a function r that maximizes R(r) or equivalently
minimizes S(r).

m

2.1 Generalization Error

Note that in our set up we are only given samples from the distribution  D  but aim to maximize the
expected revenue. Understanding the difference between the empirical performance of an algorithm
and its expected performance  also known as the generalization error  is a key tenet of learning
theory.
At a high level  the generalization error is a function of the training set size: larger training sets lead
to smaller generalization error; and the inherent complexity of the learning algorithm: simple rules
such as linear classiﬁers generalize better than more complex ones.
In this paper we characterize the complexity of a class G of functions by its growth function Π. The
growth function corresponds to the maximum number of binary labelings that can be obtained by
G over all possible samples SX . It is closely related to the VC-dimension when G takes values in
{0  1} and to the pseudo-dimension [Morgenstern and Roughgarden  2015  Mohri et al.  2012] when
G takes values in R.
We can give a bound on the generalization error associated with minimizing the empirical separation
over a class of functions G. The following theorem is an adaptation of Theorem 1 of [Mohri and
Medina  2014] to our particular setup.
Theorem 1. Let δ > 0  with probability at least 1 − δ over the choice of the sample S the following
bound holds uniformly for r ∈ G

(cid:114)

S(r) ≤ (cid:98)S(r) + 2

(cid:114)

Therefore  in order to minimize the expected separation S(r) it sufﬁces to minimize the empirical

separation (cid:98)S(r) over a class of functions G whose growth function scales polynomially in m.

log 1/δ

2m

+ 4

2 log(Π(G  m))

m

.

(1)

3 Warmup

In order to better understand the problem at hand  we begin by introducing a straightforward mech-
anism for transforming the hypothesis function h to a reserve price function r with guarantees on its
achievable revenue.
Lemma 1. Let r : X → [0  1] be deﬁned by r(x) := max(h(x) − η2/3  0). The function r then
satisﬁes S(r) ≤ η1/2 + 2η2/3.
The proof is a simple application of Jensen’s and Markov’s inequalities and it is deferred to Ap-
pendix B.
This surprisingly simple algorithm shows there are ways to obtain revenue guarantees from a simple
regressor. To the best of our knowledge these is the ﬁrst guarantee of its kind. The reader may be

3

curious about the choice of η2/3 as the offset in our reserve price function. We will show that the
dependence on η2/3 is not a simple artifact of our analysis  but a cost inherent to the problem of
revenue optimization.
Moreover  observe that this simple algorithm ﬁxes a static offset  and does not make a distinction
between those parts of the feature space  where the algorithm makes a low error  and those where the
error is relatively high. By contrast our proposed algorithm partitions the space appropriately and
calculates a different reserve for each partition. More importantly we will provide a data dependent
bound on the performance of our algorithm that only in the worst case scenario behaves like η2/3.

4 Results Overview

In principle to maximize revenue we need to ﬁnd a class of functions G with small complexity  but
that contains a function which approximately minimizes the empirical separation. The challenge
comes from the fact that the revenue function  Rev  is not continuous and highly non-concave—a
small change in the price  p  may lead to very large changes in revenue. This is the main reason why
simply using the predictor h(x) as a proxy for a reserve function is a poor choice  even if its average
error  η2 is small. For example a function h  that is just as likely to over-predict by η as to under
predict by η will have very small error  but lead to 0 revenue in half the cases.
A solution on the other end of the spectrum would simply memorize the optimum prices from the
sample S  setting r(xi) = bi. While this leads to optimal empirical revenue  a function class G
containing r would satisfy Π(G  m) = 2m  making the bound of Theorem 1 vacuous.
In this work we introduce a family G(h  k) of classes parameterized by k ∈ N. This family admits
an approximate minimizer that can be computed in polynomial time  has low generalization error 
and achieves provable guarantees to the overall revenue.
More precisely  we show that given S  and a hypothesis h with expected squared loss of η2:

• For every k ≥ 1 there exists a set of functions G(h  k) such that Π(G(h  k)  m) = O(m2k).
• For every k ≥ 1  there is a polynomial time algorithm that outputs rk ∈ G(h  k) such that

in the worst case scenario (cid:98)S(rk) is bounded by O( 1

k2/3 + η2/3 + 1

m1/6 ).

Effectively  we show how to transform any classiﬁer h with low squared loss  η2  to a reserve price
predictor that recovers all but O(η2/3) revenue in expectation.

4.1 Algorithm Description

In this section we give an overview of the algorithm that uses both the predictor h and the set of
samples in S to develop a pricing function r. Our approach has two steps. First we partition the
set of feasible prices  0 ≤ p ≤ 1  into k partitions  C1  C2  . . .   Ck. The exact boundaries between
partitions depend on the samples S and their predicted values  as given by h. For each partition
we ﬁnd the price that maximizes the empirical revenue in the partition. We let r(x) return the
empirically optimum price in the partition that contains h(x).
For a more formal description  let Tk be the set of k-partitions of the interval [0  1] that is:

Tk = {t = (t0  t1  . . .   tk−1  tk) | 0 = t0 < . . . < tk = 1}.

We deﬁne G(h  k) = {x (cid:55)→ (cid:80)k−1

j=0 ri1tj≤h(x)<tj+1 | rj ∈ [ti  tj+1] and t ∈ Tk}. A function in
G(h  k) chooses k level sets of h and k reserve prices. Given x  price rj is chosen if x falls on the
j-th level set.
It remains to deﬁne the function rk ∈ G(h  k). Given a partition vector t ∈ Tk  let the partition
j | be
Ch = {C h
the number of elements that fall into the j-th partition.
We deﬁne the predicted mean and variance of each group C h

j = {x ∈ X|tj−1 < h(x) ≤ tj}. Let mj = |SX ∩ C h

k} of X be given by C h

1   . . .   C h

(cid:88)

xi∈Ch

j

µh

j =

1
mj

h(xi)

and

(σh

j )2 =

4

j as
1
mj

(cid:88)

xi∈Ch

j

(h(xi) − µj)2.

We are now ready to present algorithm RIC-h for computing rk ∈ Hk.
Algorithm 1. Reserve Inference from Clusters

Compute th ∈ Tk that minimizes 1
Let Ch = C h
For each j ∈ 1  . . .   k  set rj = maxr r · |{i|bi ≥ r ∧ xi ∈ C h

k be the induced partitions.

j .
j=0 mjσh

2   . . .   C h

m

1   C h

Return x (cid:55)→(cid:80)k−1

j=0 rj1

h(x)∈Ch

.

j }|.

(cid:80)k−1

j

end
Our main theorem states that the separation of rk is bounded by the cluster variance of Ch. For a
partition C = {C1  . . .   Ck} of X let σj denote the empirical variance of bids for auctions in Cj. We
deﬁne the weighted empirical variance by:

Φ(C) : =

(bi − bi(cid:48))2 = 2

(2)

k(cid:88)

(cid:115) (cid:88)

j=1

i i(cid:48):xi xi(cid:48)∈Ck

k(cid:88)

j=1

mj(cid:98)σj

Theorem 2. Let δ > 0 and let rk denote the output of Algorithm 1 then rk ∈ G(h  k) and with
probability at least 1 − δ over the samples S:

(cid:98)S(rk) ≤ (3(cid:98)B)1/3(cid:16) 1

Φ(Ch)

2m

(cid:17) ≤ (3(cid:98)B)1/3(cid:16) 1

2k

(cid:114)

(cid:16)

+ 2

η2 +

log 1/δ

2m

(cid:17)1/2(cid:33)2/3

.

Notice that our bound is data dependent and only in he worst case scenario it behaves like η2/3. In
general it could be much smaller.
We also show that the complexity of G(h  k) admits a favorable bound. The proof is similar to that
in [Morgenstern and Roughgarden  2015]; we include it in Appendix E for completness.
Theorem 3. The growth function of the class G(h  k) can be bounded as: Π(G(h  k)  m) ≤ m2k−1

We can combine these results with Equation 1 and an easy bound on (cid:98)B in terms of B to conclude:
S(rk) ≤ (3(cid:98)B)1/3(cid:16) Φ(Ch)

Corollary 1. Let δ > 0 and let rk denote the output of Algorithm 1 then rk ∈ G(h  k) and with
probability at least 1 − δ over the samples S:

(cid:17) ≤ (12Bη2)1/3+O

(cid:16) log 1/δ

(cid:16) 1

(cid:16)(cid:114)

(cid:17)1/6

(cid:114)

k log m

k log m

(cid:17)

+O

+

+

.

kk

(cid:17)

.

k2/3

2m

m

2m

m

Since B ∈ [0  1]  this implies that when k = Θ(m3/7)  the separation is bounded by 2.28η2/3 plus
additional error factors that go to 0 with the number of samples  m  as ˜O(m−2/7).

5 Bounding Separation

In this section we prove the main bound motivating our algorithm. This bound relates the variance
of the bid distribution and the maximum revenue that can be extracted when a buyer’s bids follow
such distribution. It formally shows what makes a distribution amenable to revenue optimization.
To gain intuition for the kind of bound we are striving for  consider a bid distribution F . If the
variance of F is 0  that is F is a point mass at some value v  then setting a reserve price to v leads to
no separation. On the other hand  consider the equal revenue distribution  with F (x) = 1− 1/x. Here
any reserve price leads to revenue of 1. However  the distribution has unbounded expected bid and
variance  so it is not too surprising that more revenue cannot be extracted. We make this connection
precise  showing that after setting the optimal reserve price  the separation can be bounded by a
function of the variance of the distribution.
Given any bid distribution F over [0  1] we denote by G(r) = 1 − limr(cid:48)→r− F (r(cid:48)) the probability
that a bid is greater than or equal to r. Finally  we will let R = maxr rG(r) denote the maximum
revenue achievable when facing a bidder whose bids are drawn from distribution F . As before we
denote by B = Eb∼F [b] the mean bid and by S = B − R the expected separation of distribution F .

5

Theorem 4. Let σ2 denote the variance of F . Then σ2 ≥ 2R2e S
R − B2 − R2.
The proof of this theorem is highly technical and we present it in Appendix A.
Corollary 2. The following bound holds for any distribution F: S ≤ (3R)1/3σ2/3 ≤ (3B)1/3σ2/3
The proof of this corollary follows immediately by an application of Taylor’s theorem to the bound
of Theorem 4. It is also easy to show that this bound is tight (see Appendix D).

5.1 Approximating Maximum Revenue

(cid:1)

B2

R ≤ 4.78 + 2 log(cid:0)1 + σ2

In their seminal work Goldberg et al. [2001] showed that when faced with a bidder drawing values
distribution F on [1  M ] with mean B  an auctioneer setting the optimum monopoly reserve would
recover at least Ω(B/ log M ) revenue. We show how to adapt the result of Theorem 4 to reﬁne this
approximation ratio as a function of the variance of F . We defer the proof to Appendix B.
Theorem 5. For any distribution F with mean B and variance σ2  the maximum revenue with
monopoly reserves  R  satisﬁes: B
Note that since σ2 ≤ M 2 this always leads to a tighter bound on the revenue.
5.2 Partition of X
Corollary 2 suggests clustering points in such a way that the variance of the bids in each cluster

is minimized. Given a partition C = {C1  . . .   Ck} of X we denote by mj = |SX ∪ Cj|  (cid:98)Bj =
(cid:80)
(cid:80)
(bi −(cid:98)Bj)2. Let also rj = argmaxp>0 p|{bi > p|xi ∈ Cj}| and
(cid:98)Rj = rj|{bi > rj|xi ∈ Cj}|.
(cid:17)2/3
Lemma 2. Let r(x) = (cid:80)k
(cid:17)1/3(cid:16) 1
(cid:16)
3(cid:98)B
Proof. Let (cid:98)Sj = (cid:98)Bj−(cid:98)Rj  Corollary 2 applied to the empirical bid distribution in Cj yields (cid:98)Sj ≤
(3(cid:98)Bj)1/3(cid:98)σ2/3
(cid:98)S(r) =

then (cid:98)S(r) ≤ (cid:16)
3(cid:98)B

m   summing over all clusters and using H¨older’s inequality gives:

(cid:80)k
j=1 mj(cid:98)σj

(3(cid:98)Bj)1/3(cid:98)σ2/3

j mj ≤(cid:16) k(cid:88)

(cid:17)1/3(cid:16) k(cid:88)

(cid:17)1/3(cid:16) 1

. Multiplying by mj

(cid:17)2/3

.

j = 1
mj

i:xi∈Cj

bi (cid:98)σ2

1
mj

i:xi∈Cj

j=1 rj1x∈Cj

2m Φ(C)

k(cid:88)

(cid:17)

.

m

=

(cid:98)Bj

3mj
m

j

k(cid:88)

j=1

1
m

mjSj ≤ 1
m

j=1

j=1

j=1

m(cid:98)σj

mj

6 Clustering Algorithm

In view of Lemma 2 and since the quantity (cid:98)B is ﬁxed  we can ﬁnd a function minimizing the

expected separation by ﬁnding a partition of X that minimizes the weighted variance Φ(C) deﬁned
Section 4.1. From the deﬁnition of Φ  this problem resembles a traditional k-means clustering
problem with distance function d(xi  xi(cid:48)) = (bi−bi(cid:48))2. Thus  one could use one of several clustering
algorithms to solve it. Nevertheless  in order to allocate a new point x ∈ X to a cluster  we would
require access to the bid b which at evaluation time is unknown. Instead  we show how to utilize the
predictions of h to deﬁne an almost optimal clustering of X .
For any partition C = {C1  . . .   Ck} of X deﬁne

k(cid:88)

(cid:115) (cid:88)

j=1

i i(cid:48):xi xi(cid:48)∈Ck

Φh(C) =

(h(xi) − h(xi(cid:48)))2.

2m Φh(C) is the function minimized by Algorithm 1. The following lemma  proved in
Notice that 1
Appendix B  bounds the cluster variance achieved by clustering bids according to their predictions.

6

Lemma 3. Let h be a function such that 1
m

that minimizes Φ(C). If Ch minimizes Φh(C) then Φ(Ch) ≤ Φ(C∗) + 4m(cid:98)η.

Corollary 3. Let rk be the output of Algorithm 1. If 1
m

(cid:80)m
i=1(h(xi)− bi)2 ≤(cid:98)η2  and let C∗ denote the partition
(cid:80)m
j=1(h(xi) − bi)2 ≤(cid:98)η2 then:
(cid:17)2/3
(cid:17)2/3 ≤ (3(cid:98)B)1/3Big(
Φ(C∗) + 2(cid:98)η

(3)

.

1
2m

(cid:98)S(rk) ≤ (3(cid:98)B)1/3(cid:16) 1

Φ(Ch)

2m

j of Ch are of the form Cj = {x|tj ≤ h(x) ≤ tj+1} for
Proof. It is easy to see that the elements C h
t ∈ Tk. Thus if rk is the hypothesis induced by the partition Ch  then rk ∈ G(h  k). The result now
follows by deﬁnition of Φ and lemmas 2 and 3.

The proof of Theorem 2 is now straightforward. Deﬁne a partition C by xi ∈ Cj if bi ∈(cid:2) j−1

(cid:3).

k   j

k

Since (bi − bi(cid:48))2 ≤ 1

k2 for bi  bi(cid:48) ∈ Cj we have

Furthermore since E[(h(x) − b)2] ≤ η2  Hoeffding’s inequality implies that with probability 1 − δ:

m
k

.

(4)

(cid:115)

j=1

m2
j
k2 =

Φ(C) ≤ k(cid:88)
(h(xi) − bi)2 ≤(cid:16)
(cid:17)1/2(cid:33)2/3
(cid:114)

log 1/δ

η2 +

1
m

m(cid:88)

i=1

(cid:16)

In view of inequalities (4) and (5) as well as Corollary 3 we have:

(cid:32)

(cid:98)S(rk) ≤ (3(cid:98)B)1/3

Φ(C)+2

1
2m

η2+

2m

(cid:114)

.

2m

log 1/δ

(cid:17)
≤ (3(cid:98)B)1/3(cid:16) 1

2k

(5)

(cid:17)1/2(cid:33)2/3

(cid:114)

(cid:16)

+2

η2+

log 1/δ

2m

This completes the proof of the main result. To implement the algorithm  note that the problem
of minimizing Φh(C) reduces to ﬁnding a partition t ∈ Tk such that the sum of the variances
within the partitions is minimized. It is clear that it sufﬁces to consider points tj in the set B =
{h(x1)  . . .   h(xm)}. With this observation  a simple dynamic program leads to a polynomial time
algorithm with an O(km2) running time (see Appendix C).

7 Experiments

We now compare the performance of our algorithm against the following baselines:

η2/3 we ﬁnd the optimal t maximizing the empirical revenue(cid:80)m

1. The offset algorithm presented in Section 3  where instead of using the theoretical offset

(cid:0)h(xi)−t)1h(xi)−t≤bi.

2. The DC algorithm introduced by Mohri and Medina [2014]  which represents the state of

i=1

the art in learning a revenue optimal reserve price.

Synthetic data. We begin by running experiments on synthetic data to demonstrate the regimes
where each algorithm excels. We generate feature vectors xi ∈ R10 with coordinates sampled from
a mixture of lognormal distributions with means µ1 = 0  µ2 = 1  variance σ1 = σ2 = 0.5 and
mixture parameter p = 0.5. Let 1 ∈ Rd denote the vector with entries set to 1. Bids are generated
according to two different scenarios:
Linear Bids bi generated according to bi = max(x(cid:62)
Bimodal Bids bi generated according to the following rule: let si = max(x(cid:62)

variable with mean 0  and standard deviation σ ∈ {0.01  0.1  1.0  2.0  4.0}.

i 1 + βi  0) where βi is a Gaussian random

i 1 + βi  0) if si > 30

then bi = 40 + αi otherwise bi = si. Here αi has the same distribution as βi.

The linear scenario demonstrates what happens when we have a good estimate of the bids. The
bimodal scenario models a buyer  which for the most part will bid as a continuous function of
features but that is interested in a particular set of objects (for instance retargeting buyers in online
advertisement) for which she is willing to pay a much higher price.

7

(a)

(b)

(c)

Figure 1: (a) Mean revenue of the three algorithms on the linear scenario. (b) Mean revenue of the
three algorithms on the bimodal scenario. (c) Mean revenue on auction data.

For each experiment we generated a training dataset Strain  a holdout set Sholdout and a test set
Stest each with 16 000 examples. The function h used by RIC-h and the offset algorithm is found
by training a linear regressor over Strain. For efﬁciency  we ran RIC-h algorithm on quantizations
of predictions h(xi). Quantized predictions belong to one of 1000 buckets over the interval [0  50].
Finally  the choice of hyperparameters γ for the Lipchitz loss and k for the clustering algorithm was
done by selecting the best performing parameter over the holdout set. Following the suggestions in
[Mohri and Medina  2014] we chose γ ∈ {0.001  0.01  0.1  1.0} and k ∈ {2  4  . . .   24}.
Figure 1(a) (b) shows the average revenue of the three approaches across 20 replicas of the experi-
ment as a function of the log of σ. Revenue is normalized so that the DC algorithm revenue is 1.0
when σ = 0.01. The error bars at one standard deviation are indistinguishable in the plot. It is
not surprising to see that in the linear scenario  the DC algorithm of [Mohri and Medina  2014] and
the offset algorithm outperform RIC-h under low noise conditions. Both algorithms will recover a
solution close to the true weight vector 1. In this case the offset is minimal  thus recovering virtually
all revenue. On the other hand  even if we set the optimal reserve price for every cluster  the inherent
variance of each cluster makes us leave some revenue on the table. Nevertheless  notice that as the
noise increases all three algorithms seem to achieve the same revenue. This is due to the fact that
the variance in each cluster is comparable with the error in the prediction function h.
The results are reversed for the bimodal scenario where RIC-h outperforms both algorithms under
low noise. This is due to the fact that RIC-h recovers virtually all revenue obtained from high bids
while the offset and DC algorithms must set conservative prices to avoid losing revenue from lower
bids.
Auction data. In practice  however  neither of the synthetic regimes is fully representative of the
bidding patterns. In order to fully evaluate RIC-h  we collected auction bid data from AdExchange
for 4 different publisher-advertiser pairs. For each pair we sampled 100 000 examples with a set of
discrete and continuous features. The ﬁnal feature vectors are in Rd for d ∈ [100  200] depending
on the publisher-buyer pair. For each experiment  we extract a random training sample of 20 0000
points as well as a holdout and test sample. We repeated this experiment 20 times and present
the results on Figure 1 (c) where we have normalized the data so that the performance of the DC
algorithm is always 1. The error bars represent one standard deviation from the mean revenue
lift. Notice that our proposed algorithm achieves on average up to 30% improvement over the DC
algorithm. Moreover  the simple offset strategy never outperforms the clustering algorithm  and in
some cases achieves signiﬁcantly less revenue.

8 Conclusion

We provided a simple  scalable reduction of the problem of revenue optimization with side infor-
mation to the well studied problem of minimizing the squared loss. Our reduction provides the ﬁrst
polynomial time algoritm with a quantiﬁable bound on the achieved revenue. In the analysis of our
algorithm we also provided the ﬁrst variance dependent lower bound on the revenue attained by set-
ting optimal monopoly prices. Finally  we provided extensive empirical evidence of the advantages
of RIC-h over the current state of theart.

8

References
Nicol`o Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Regret minimization for reserve prices

in second-price auctions. IEEE Trans. Information Theory  61(1):549–564  2015.

Shuchi Chawla  Jason D. Hartline  and Robert D. Kleinberg. Algorithmic pricing via virtual val-
uations. In Proceedings 8th ACM Conference on Electronic Commerce (EC-2007)  San Diego 
California  USA  June 11-15  2007  pages 243–251  2007. doi: 10.1145/1250910.1250946.

Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. CoRR 

abs/1502.00963  2015.

Nikhil R. Devanur  Zhiyi Huang  and Christos-Alexandros Psomas. The sample complexity of

auctions with side information. In Proceedings of STOC  pages 426–439  2016.

Peerapong Dhangwatnotai  Tim Roughgarden  and Qiqi Yan. Revenue maximization with a single

sample. Games and Economic Behavior  91:318–333  2015.

Andrew V. Goldberg  Jason D. Hartline  and Andrew Wright. Competitive auctions and digital
goods. In Proceedings of the Twelfth Annual Symposium on Discrete Algorithms  January 7-9 
2001  Washington  DC  USA.  pages 735–744  2001.

Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. In Proceedings 10th
ACM Conference on Electronic Commerce (EC-2009)  Stanford  California  USA  July 6–10 
2009  pages 225–234  2009.

Robert D. Kleinberg and Frank Thomson Leighton. The value of knowing a demand curve: Bounds

on regret for online posted-price auctions. In Proceedings of FOCS  pages 594–605  2003.

Renato Paes Leme  Martin P´al  and Sergei Vassilvitskii. A ﬁeld guide to personalized reserve prices.
In Proceedings of the 25th International Conference on World Wide Web  WWW 2016  Montreal 
Canada  April 11 - 15  2016  pages 1093–1102  2016. doi: 10.1145/2872427.2883071.

Mehryar Mohri and Andres Mu˜noz Medina. Learning theory and algorithms for revenue optimiza-

tion in second-price auctions with reserve. In Proceedings of ICML  pages 262–270  2014.

Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of Machine Learning.

The MIT Press  2012. ISBN 026201825X  9780262018258.

Jamie Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly optimal auctions. In

Proceedings of NIPS  pages 136–144  2015.

Jamie Morgenstern and Tim Roughgarden. Learning simple auctions.

pages 1298–1318  2016.

In Proceedings ofCOLT 

R. Myerson. Optimal auction design. Mathematics of Operations Research  6(1):58–73  1981.

Tim Roughgarden and Joshua R. Wang. Minimizing regret with multiple reserves. In Proceedings of
the 2016 ACM Conference on Economics and Computation  EC ’16  Maastricht  The Netherlands 
July 24-28  2016  pages 601–616  2016. doi: 10.1145/2940716.2940792.

Maja R. Rudolph  Joseph G. Ellis  and David M. Blei. Objective variables for probabilistic revenue
maximization in second-price auctions with reserve. In Proceedings of WWW 2016  pages 1113–
1122  2016.

9

,Sergei Vassilvitskii