2011,Universal low-rank matrix recovery from Pauli measurements,We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd polylog d) Pauli measurements.  This has applications in quantum state tomography  and is a non-commutative analogue of a well-known problem in compressed sensing:  recovering a sparse vector from a few of its Fourier coefficients.      We show that almost all sets of O(rd log^6 d) Pauli measurements satisfy the rank-r restricted isometry property (RIP).  This implies that M can be recovered from a fixed ("universal") set of Pauli measurements  using nuclear-norm minimization (e.g.  the matrix Lasso)  with nearly-optimal bounds on the error.  A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm.  Our proof uses Dudley's inequality for Gaussian processes  together with bounds on covering numbers obtained via entropy duality.,Universal low-rank matrix recovery

from Pauli measurements

Yi-Kai Liu

Applied and Computational Mathematics Division

National Institute of Standards and Technology

Gaithersburg  MD  USA

yi-kai.liu@nist.gov

Abstract

We study the problem of reconstructing an unknown matrix M of rank r and di-
mension d using O(rd poly log d) Pauli measurements. This has applications in
quantum state tomography  and is a non-commutative analogue of a well-known
problem in compressed sensing: recovering a sparse vector from a few of its
Fourier coefﬁcients.
We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rank-
r restricted isometry property (RIP). This implies that M can be recovered from
a ﬁxed (“universal”) set of Pauli measurements  using nuclear-norm minimization
(e.g.  the matrix Lasso)  with nearly-optimal bounds on the error. A similar result
holds for any class of measurements that use an orthonormal operator basis whose
elements have small operator norm. Our proof uses Dudley’s inequality for Gaus-
sian processes  together with bounds on covering numbers obtained via entropy
duality.

1

Introduction

Low-rank matrix recovery is the following problem: let M be some unknown matrix of dimension
d and rank r (cid:28) d  and let A1  A2  . . .   Am be a set of measurement matrices; then can one recon-
struct M from its inner products tr(M∗A1)  tr(M∗A2)  . . .   tr(M∗Am)? This problem has many
applications in machine learning [1  2]  e.g.  collaborative ﬁltering (the Netﬂix problem). Remark-
ably  it turns out that for many useful choices of measurement matrices  low-rank matrix recovery
is possible  and can even be done efﬁciently. For example  when the Ai are Gaussian random ma-
trices  then it is known that m = O(rd) measurements are sufﬁcient to uniquely determine M  and
furthermore  M can be reconstructed by solving a convex program (minimizing the nuclear norm)
[3  4  5]. Another example is the “matrix completion” problem  where the measurements return a
random subset of matrix elements of M; in this case  m = O(rd poly log d) measurements sufﬁce 
provided that M satisﬁes some “incoherence” conditions [6  7  8  9  10].
The focus of this paper is on a different class of measurements  known as Pauli measurements. Here 
the Ai are randomly chosen elements of the Pauli basis  a particular orthonormal basis of Cd×d. The
Pauli basis is a non-commutative analogue of the Fourier basis in Cd; thus  low-rank matrix recovery
using Pauli measurements can be viewed as a generalization of the idea of compressed sensing of
sparse vectors using their Fourier coefﬁcients [11  12]. In addition  this problem has applications
in quantum state tomography  the task of learning an unknown quantum state by performing mea-
surements [13]. This is because most quantum states of physical interest are accurately described by
density matrices that have low rank; and Pauli measurements are especially easy to carry out in an
experiment (due to the tensor product structure of the Pauli basis).

1

In this paper we show stronger results on low-rank matrix recovery from Pauli measurements. Pre-
viously [13  8]  it was known that  for every rank-r matrix M ∈ Cd×d  almost all choices of
m = O(rd poly log d) random Pauli measurements will lead to successful recovery of M. Here
we show a stronger statement: there is a ﬁxed (“universal”) set of m = O(rd poly log d) Pauli mea-
surements  such that for all rank-r matrices M ∈ Cd×d  we have successful recovery.1 We do this
by showing that the random Pauli sampling operator obeys the “restricted isometry property” (RIP).
Intuitively  RIP says that the sampling operator is an approximate isometry  acting on the set of all
low-rank matrices. In geometric terms  it says that the sampling operator embeds the manifold of
low-rank matrices into O(rd poly log d) dimensions  with low distortion in the 2-norm.
RIP for low-rank matrices is a very strong property  and prior to this work  it was only known to hold
for very unstructured types of random measurements  such as Gaussian measurements [3]  which
are unsuitable for most applications. RIP was known to fail in the matrix completion case  and
whether it held for Pauli measurements was an open question. Once we have established RIP for
Pauli measurements  we can use known results [3  4  5] to show low-rank matrix recovery from a
universal set of Pauli measurements. In particular  using [5]  we can get nearly-optimal universal
bounds on the error of the reconstructed density matrix  when the data are noisy; and we can even get
bounds on the recovery of arbitrary (not necessarily low-rank) matrices. These RIP-based bounds are
qualitatively stronger than those obtained using “dual certiﬁcates” [14] (though the latter technique
is applicable in some situations where RIP fails).
In the context of quantum state tomography  this implies that  given a quantum state that consists
of a low-rank component Mr plus a residual full-rank component Mc  we can reconstruct Mr up
to an error that is not much larger than Mc. In particular  let (cid:107)·(cid:107)∗ denote the nuclear norm  and let
(cid:107)·(cid:107)F denote the Frobenius norm. Then the error can be bounded in the nuclear norm by O((cid:107)Mc(cid:107)∗)
(assuming noiseless data)  and it can be bounded in the Frobenius norm by O((cid:107)Mc(cid:107)F poly log d)
(which holds even with noisy data2). This shows that our reconstruction is nearly as good as the
√
best rank-r approximation to M (which is given by the truncated SVD). In addition  a completely
arbitrary quantum state can be reconstructed up to an error of O(1/
r) in Frobenius norm. Lastly 
the RIP gives some insight into the optimal design of tomography experiments  in particular  the
tradeoff between the number of measurement settings (which is essentially m)  and the number of
repetitions of the experiment at each setting (which determines the statistical noise that enters the
data) [15].
These results can be generalized beyond the class of Pauli measurements. Essentially  one can
replace the Pauli basis with any orthonormal basis of Cd×d that is incoherent  i.e.  whose elements
√
have small operator norm (of order O(1/
d)  say); a similar generalization was noted in the earlier
not just for all rank-r matrices  but for all matrices X that satisfy (cid:107)X(cid:107)∗ ≤ √
results of [8]. Also  our proof shows that the RIP actually holds in a slightly stronger sense: it holds

r(cid:107)X(cid:107)F .

To prove this result  we combine a number of techniques that have appeared elsewhere. RIP results
were previously known for Gaussian measurements and some of their close relatives [3]. Also 
restricted strong convexity (RSC)  a similar but somewhat weaker property  was recently shown
in the context of the matrix completion problem (with additional “non-spikiness” conditions) [10].
These results follow from covering arguments (i.e.  using a concentration inequality to upper-bound
the failure probability on each individual low-rank matrix X  and then taking the union bound over
all such X). Showing RIP for Pauli measurements seems to be more delicate  however. Pauli
measurements have more structure and less randomness  so the concentration of measure phenomena
are weaker  and the union bound no longer gives the desired result.
Instead  one must take into account the favorable correlations between the behavior of the sampling
operator on different matrices — intuitively  if two low-rank matrices M and M(cid:48) have overlapping
supports  then good behavior on M is positively correlated with good behavior on M(cid:48). This can be
done by transforming the problem into a Gaussian process  and using Dudley’s entropy bound. This
is the same approach used in classical compressed sensing  to show RIP for Fourier measurements
[12  11]. The key difference is that in our case  the Gaussian process is indexed by low-rank matrices 
rather than sparse vectors. To bound the correlations in this process  one then needs to bound the
covering numbers of the nuclear norm ball (of matrices)  rather than the (cid:96)1 ball (of vectors). This

1Note that in the universal result  m is slightly larger  by a factor of poly log d.
2However  this bound is not universal.

2

requires a different technique  using entropy duality  which is due to Gu´edon et al [16]. (See also
the related work in [17].)
As a side note  we remark that matrix recovery can sometimes fail because there exist large sets of
up to d Pauli matrices that all commute  i.e.  they have a simultaneous eigenbasis φ1  . . .   φd. (These
φi are of interest in quantum information — they are called stabilizer states [18].) If one were to
measure such a set of Pauli’s  one would gain complete knowledge about the diagonal elements of
the unknown matrix M in the φi basis  but one would learn nothing about the off-diagonal elements.
This is reminiscent of the difﬁculties that arise in matrix completion. However  in our case  these
pathological cases turn out to be rare  since it is unlikely that a random subset of Pauli matrices will
all commute.
Finally  we note that there is a large body of related work on estimating a low-rank matrix by solving
a regularized convex program; see  e.g.  [19  20].
This paper is organized as follows. In section 2  we state our results precisely  and discuss some
speciﬁc applications to quantum state tomography. In section 3 we prove the RIP for Pauli matrices 
and in section 4 we discuss some directions for future work. Some technical details appear in
sections A and B in the supplementary material [21].
Notation: For vectors  (cid:107)·(cid:107)2 denotes the (cid:96)2 norm. For matrices  (cid:107)·(cid:107)p denotes the Schatten p-norm 
i σi(X)p)1/p  where σi(X) are the singular values of X. In particular  (cid:107)·(cid:107)∗ = (cid:107)·(cid:107)1
is the trace or nuclear norm  (cid:107)·(cid:107)F = (cid:107)·(cid:107)2 is the Frobenius norm  and (cid:107)·(cid:107) = (cid:107)·(cid:107)∞ is the operator
norm. Finally  for matrices  A∗ is the adjoint of A  and (· ·) is the Hilbert-Schmidt inner product 
the superoperator that maps every matrix X ∈ Cd×d to the matrix A tr(A∗X).

(cid:107)X(cid:107)p = ((cid:80)
(A  B) = tr(A∗B). Calligraphic letters denote superoperators acting on matrices. Also (cid:12)(cid:12)A(cid:1)(cid:0)A(cid:12)(cid:12) is

2 Our Results
We will consider the following approach to low-rank matrix recovery. Let M ∈ Cd×d be an un-
known matrix of rank at most r. Let W1  . . .   Wd2 be an orthonormal basis for Cd×d  with respect
to the inner product (A  B) = tr(A∗B). We choose m basis elements  S1  . . .   Sm  iid uniformly
at random from {W1  . . .   Wd2} (“sampling with replacement”). We then observe the coefﬁcients
(Si  M ). From this data  we want to reconstruct M.
For this to be possible  the measurement matrices Wi must be “incoherent” with respect to M.
Roughly speaking  this means that the inner products (Wi  M ) must be small. Formally  we say that
the basis W1  . . .   Wd2 is incoherent if the Wi all have small operator norm 

√
(cid:107)Wi(cid:107) ≤ K/

d 

(1)

where K is a constant.3 (This assumption was also used in [8].)
Before proceeding further  let us sketch the connection between this problem and quantum state
tomography. Consider a system of n qubits  with Hilbert space dimension d = 2n. We want to learn
the state of the system  which is described by a density matrix ρ ∈ Cd×d; ρ is positive semideﬁnite 
has trace 1  and has rank r (cid:28) d when the state is nearly pure. There is a class of convenient (and
experimentally feasible) measurements  which are described by Pauli matrices (also called Pauli
observables). These are matrices of the form P1 ⊗ ··· ⊗ Pn  where ⊗ denotes the tensor product
(Kronecker product)  and each Pi is a 2 × 2 matrix chosen from the following four possibilities:

(cid:18)0 −i
(cid:19)

(cid:18)1

(cid:19)

(cid:18)1 0

(cid:19)

(cid:18)0

1

(cid:19)

1
0

 

 

I =

0 1

σx =

(2)
One can estimate expectation values of Pauli observables  which are given by (ρ  (P1 ⊗ ··· ⊗ Pn)).
√
√
This is a special case of the above measurement model  where the measurement matrices Wi are
the (scaled) Pauli observables (P1 ⊗ ··· ⊗ Pn)/
d 
K = 1.

d  and they are incoherent with (cid:107)Wi(cid:107) ≤ K/

σy =

 

σz =

i

0

0
0 −1

.

3Note that (cid:107)Wi(cid:107) is the maximum inner product between Wi and any rank-1 matrix M (normalized so that

(cid:107)M(cid:107)F = 1).

3

Now we return to our discussion of the general problem. We choose S1  . . .   Sm iid uniformly at
random from {W1  . . .   Wd2}  and we deﬁne the sampling operator A : Cd×d → Cm as

The normalization is chosen so that EA∗A = I. (Note that A∗A =(cid:80)m

(A(X))i = d√

m tr(S∗

i = 1  . . .   m.

i X) 

(3)

(cid:12)(cid:12)Sj

(cid:1)(cid:0)Sj

(cid:12)(cid:12) · d2

m .)

j=1

We assume we are given the data y = A(M ) + z  where z ∈ Cm is some (unknown) noise contribu-
tion. We will construct an estimator ˆM by minimizing the nuclear norm  subject to the constraints
speciﬁed by y. (Note that one can view the nuclear norm as a convex relaxation of the rank function
— thus these estimators can be computed efﬁciently.) One approach is the matrix Dantzig selector:

ˆM = arg min
X

(cid:107)X(cid:107)∗ such that (cid:107)A∗(y − A(X))(cid:107) ≤ λ.

Alternatively  one can solve a regularized least-squares problem  also called the matrix Lasso:

ˆM = arg min
X

1

2(cid:107)A(X) − y(cid:107)2

2 + µ(cid:107)X(cid:107)∗.

(4)

(5)

Here  the parameters λ and µ are set according to the strength of the noise component z (we will
discuss this later). We will be interested in bounding the error of these estimators. To do this  we
will show that the sampling operator A satisﬁes the restricted isometry property (RIP).

2.1 RIP for Pauli Measurements
Fix some constant 0 ≤ δ < 1. Fix d  and some set U ⊂ Cd×d. We say that A satisﬁes the restricted
isometry property (RIP) over U if  for all X ∈ U  we have

(1 − δ)(cid:107)X(cid:107)F ≤ (cid:107)A(X)(cid:107)2 ≤ (1 + δ)(cid:107)X(cid:107)F .

(6)
(Here  (cid:107)A(X)(cid:107)2 denotes the (cid:96)2 norm of a vector  while (cid:107)X(cid:107)F denotes the Frobenius norm of a
matrix.) When U is the set of all X ∈ Cd×d with rank r  this is precisely the notion of RIP studied
all X ∈ Cd×d such that (cid:107)X(cid:107)∗ ≤ √
in [3  5]. We will show that Pauli measurements satisfy the RIP over a slightly larger set (the set of
r(cid:107)X(cid:107)F )  provided the number of measurements m is at least
Ω(rd poly log d). This result generalizes to measurements in any basis with small operator norm.
Theorem 2.1 Fix some constant 0 ≤ δ < 1. Let {W1  . . .   Wd2} be an orthonormal basis for Cd×d
that is incoherent in the sense of (1). Let m = CK 2 · rd log6 d  for some constant C that depends
only on δ  C = O(1/δ2). Let A be deﬁned as in (3). Then  with high probability (over the choice
of S1  . . .   Sm)  A satisﬁes the RIP over the set of all X ∈ Cd×d such that (cid:107)X(cid:107)∗ ≤ √
r(cid:107)X(cid:107)F .

Furthermore  the failure probability is exponentially small in δ2C.

We will prove this theorem in section 3. In the remainder of this section  we discuss its applications
to low-rank matrix recovery  and quantum state tomography in particular.

2.2 Applications

By combining Theorem 2.1 with previous results [3  4  5]  we immediately obtain bounds on the
accuracy of the matrix Dantzig selector (4) and the matrix Lasso (5). In particular  for the ﬁrst time
we can show universal recovery of low-rank matrices via Pauli measurements  and near-optimal
bounds on the accuracy of the reconstruction when the data is noisy [5]. (Similar results hold for
measurements in any incoherent operator basis.) These RIP-based results improve on the earlier
results based on dual certiﬁcates [13  8  14]. See [3  4  5] for details.
Here  we will sketch a couple of these results that are of particular interest for quantum state to-
mography. Here  M is the density matrix describing the state of a quantum mechanical object  and
A(M ) is a vector of Pauli expectation values for the state M. (M has some additional properties:
it is positive semideﬁnite  and has trace 1; thus A(M ) is a real vector.) There are two main issues
that arise. First  M is not precisely low-rank. In many situations  the ideal state has low rank (for
instance  a pure state has rank 1); however  for the actual state observed in an experiment  the den-
sity matrix M is full-rank with decaying eigenvalues. Typically  we will be interested in obtaining a
good low-rank approximation to M  ignoring the tail of the spectrum.

4

Secondly  the measurements of A(M ) are inherently noisy. We do not observe A(M ) directly;
rather  we estimate each entry (A(M ))i by preparing many copies of the state M  measuring the
Pauli observable Si on each copy  and averaging the results. Thus  we observe yi = (A(M ))i + zi 
where zi is binomially distributed. When the number of experiments being averaged is large  zi can
be approximated by Gaussian noise. We will be interested in getting an estimate of M that is stable
with respect to this noise. (We remark that one can also reduce the statistical noise by performing
more repetitions of each experiment. This suggests the possibility of a tradeoff between the accuracy
of estimating each parameter  and the number of parameters one chooses to measure overall. This
will be discussed elsewhere [15].)
We would like to reconstruct M up to a small error in the nuclear or Frobenius norm. Let ˆM be
our estimate. Bounding the error in nuclear norm implies that  for any measurement allowed by
quantum mechanics  the probability of distinguishing the state ˆM from M is small. Bounding the
error in Frobenius norm implies that the difference ˆM − M is highly “mixed” (and thus does not
contribute to the coherent or “quantum” behavior of the system).
We now sketch a few results from [4  5] that apply to this situation. Write M = Mr + Mc  where
Mr is a rank-r approximation to M  corresponding to the r largest singular values of M  and Mc
is the residual part of M (the “tail” of M). Ideally  our goal is to estimate M up to an error that is
not much larger than Mc. First  we can bound the error in nuclear norm (assuming the data has no
noise):
Proposition 2.2 (Theorem 5 from [4]) Let A : Cd×d → Cm be the random Pauli sampling operator 
with m = Crd log6 d  for some absolute constant C. Then  with high probability over the choice of
A  the following holds:
Let M be any matrix in Cd×d  and write M = Mr + Mc  as described above. Say we observe
y = A(M )  with no noise. Let ˆM be the Dantzig selector (4) with λ = 0. Then

where C(cid:48)

0 is an absolute constant.

(cid:107) ˆM − M(cid:107)∗ ≤ C(cid:48)

0(cid:107)Mc(cid:107)∗ 

(7)

We can also bound the error in Frobenius norm  allowing for noisy data:

√
Proposition 2.3 (Lemma 3.2 from [5]) Assume the same set-up as above  but say we observe y =
A(M ) + z  where z ∼ N (0  σ2I). Let ˆM be the Dantzig selector (4) with λ = 8
√
dσ  or the Lasso
(5) with µ = 16

dσ. Then  with high probability over the noise z 
√
rdσ + C1(cid:107)Mc(cid:107)∗/

(cid:107) ˆM − M(cid:107)F ≤ C0

√

r 

(8)

where C0 and C1 are absolute constants.

√

This bounds the error of ˆM in terms of the noise strength σ and the size of the tail Mc. It is universal:
one sampling operator A works for all matrices M. While this bound may seem unnatural because
it mixes different norms  it can be quite useful. When M actually is low-rank (with rank r)  then
Mc = 0  and the bound (8) becomes particularly simple. The dependence on the noise strength σ
is known to be nearly minimax-optimal [5]. Furthermore  when some of the singular values of M
dσ  one can show a tighter bound  with a nearly-optimal bias-variance
fall below the “noise level”
tradeoff; see Theorem 2.7 in [5] for details.
On the other hand  when M is full-rank  then the error of ˆM depends on the behavior of the tail Mc.
We will consider a couple of cases. First  suppose we do not assume anything about M  besides the
fact that it is a density matrix for a quantum state. Then (cid:107)M(cid:107)∗ = 1  hence (cid:107)Mc(cid:107)∗ ≤ 1 − r
d  and we
can use (8) to get (cid:107) ˆM − M(cid:107)F ≤ C0
r . Thus  even for arbitrary (not necessarily low-rank)
quantum states  the estimator ˆM gives nontrivial results. The O(1/
r) term can be interpreted as
the penalty for only measuring an incomplete subset of the Pauli observables.
Finally  consider the case where M is full-rank  but we do know that the tail Mc is small. If we
know that Mc is small in nuclear norm  then we can use equation (8). However  if we know that Mc
is small in Frobenius norm  one can give a different bound  using ideas from [5]  as follows.

rdσ + C1√

√

√

5

Proposition 2.4 Let M be any matrix in Cd×d  with singular values σ1(M ) ≥ ··· ≥ σd(M ).
Choose a random Pauli sampling operator A : Cd×d → Cm  with m = Crd log6 d  for some
absolute constant C. Say we observe y = A(M ) + z  where z ∼ N (0  σ2I). Let ˆM be the Dantzig
√
selector (4) with λ = 16
dσ. Then  with high probability over
the choice of A and the noise z 
F ≤ C0

√
dσ  or the Lasso (5) with µ = 32

i (M )  dσ2) + C2(log6 d)

(cid:107) ˆM − M(cid:107)2

r(cid:88)

d(cid:88)

σ2
i (M ) 

min(σ2

(9)

where C0 and C2 are absolute constants.

i=1

i=r+1

This bound can be interpreted as follows. The ﬁrst term expresses the bias-variance tradeoff for esti-
may not be tight.) In particular  this implies: (cid:107) ˆM − M(cid:107)F ≤ √
mating Mr  while the second term depends on the Frobenius norm of Mc. (Note that the log6 d factor
C2(log3 d)(cid:107)Mc(cid:107)F .
This can be compared with equation (8) (involving (cid:107)Mc(cid:107)∗). This bound will be better when
(cid:107)Mc(cid:107)F (cid:28) (cid:107)Mc(cid:107)∗  i.e.  when the tail Mc has slowly-decaying eigenvalues (in physical terms  it
is highly mixed).
Proposition 2.4 is an adaptation of Theorem 2.8 in [5]. We sketch the proof in section B in [21]. Note
that this bound is not universal: it shows that for all matrices M  a random choice of the sampling
operator A is likely to work.

rdσ +

√

√

C0

3 Proof of the RIP for Pauli Measurements

We now prove Theorem 2.1. The general approach involving Dudley’s entropy bound is similar to
[12]  while the technical part of the proof (bounding certain covering numbers) uses ideas from [16].
We summarize the argument here; the details are given in section A in [21].

r(cid:107)X(cid:107)F}. Let B be the set of all self-adjoint linear

3.1 Overview
Let U2 = {X ∈ Cd×d | (cid:107)X(cid:107)F ≤ 1  (cid:107)X(cid:107)∗ ≤ √
operators from Cd×d to Cd×d  and deﬁne the following norm on B:
(cid:107)M(cid:107)(r) = sup
X∈U2

|(X MX)|.

(10)
(Suppose r ≥ 2  which is sufﬁcient for our purposes. It is straightforward to show that (cid:107)·(cid:107)(r) is a
norm  and that B is a Banach space with respect to this norm.) Then let us deﬁne

εr(A) = (cid:107)A∗A − I(cid:107)(r).

(11)
By an elementary argument  in order to prove RIP  it sufﬁces to show that εr(A) < 2δ − δ2. We
will proceed as follows: we will ﬁrst bound Eεr(A)  then show that εr(A) is concentrated around
its mean.

Using a standard symmetrization argument  we have that Eεr(A) ≤ 2E(cid:13)(cid:13)(cid:13)(cid:80)m
(cid:12)(cid:12)Sj
(cid:1)(cid:0)Sj
where the εj are Rademacher (iid ±1) random variables. Here the round ket notation(cid:12)(cid:12)Sj
(cid:12)(cid:12) denotes the adjoint element in the (dual) vector space.
the round bra(cid:0)Sj

we view the matrix Sj as an element of the vector space Cd2 with Hilbert-Schmidt inner product;

(cid:13)(cid:13)(cid:13)(r)
(cid:12)(cid:12) d2
(cid:1) means

j=1 εj

m

 

Now we use the following lemma  which we will prove later. This bounds the expected magnitude
in (r)-norm of a Rademacher sum of a ﬁxed collection of operators V1  . . .   Vm that have small
operator norm.
Lemma 3.1 Let m ≤ d2. Fix some V1  . . .   Vm ∈ Cd×d that have uniformly bounded operator
norm  (cid:107)Vi(cid:107) ≤ K (for all i). Let ε1  . . .   εm be iid uniform ±1 random variables. Then

(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

Eε

(cid:12)(cid:12)Vi

(cid:1)(cid:0)Vi

(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)(r)

≤ C5 ·(cid:13)(cid:13)(cid:13) m(cid:88)
(cid:12)(cid:12)Vi

(cid:1)(cid:0)Vi

(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)1/2

(r)

εi

i=1

√

where C5 =

r · C4K log5/2 d log1/2 m and C4 is some universal constant.

 

(12)

6

After some algebra  one gets that Eεr(A) ≤ 2(Eεr(A) + 1)1/2 · C5 ·(cid:113) d

r ·
C4K log3 d. By ﬁnding the roots of this quadratic equation  we get the following bound on Eεr(A).
4 · dr · K 2 log6 d. Then we have the desired result:
Let λ ≥ 1. Assume that m ≥ λd(2C5)2 = λ· 4C 2
Eεr(A) ≤ 1
λ + 1√
(13)
It remains to show that εr(A) is concentrated around its expectation. For this we use a concentration
inequality from [22] for sums of independent symmetric random variables that take values in some
Banach space. See section A in [21] for details.

m  where C5 =

√

λ

.

3.2 Proof of Lemma 3.1 (bounding a Rademacher sum in (r)-norm)

(cid:12)(cid:12)Vi

(cid:1)(cid:0)Vi

(cid:12)(cid:12)(cid:107)(r); this is the quantity we want to bound. Using a standard com-

Let L0 = Eε(cid:107)(cid:80)m

parison principle  we can replace the ±1 random variables εi with iid N (0  1) Gaussian random
variables gi; then we get

i=1 εi

L0 ≤ Eg sup
X∈U2

(14)
The random variables G(X) (indexed by X ∈ U2) form a Gaussian process  and L0 is upper-
bounded by the expected supremum of this process. Using the fact that G(0) = 0 and G(·) is
symmetric  and Dudley’s inequality (Theorem 11.17 in [22])  we have

i=1

gi|(Vi  X)|2.

m(cid:88)

(cid:113) π
2|G(X)|  G(X) =
(cid:90) ∞

√
G(X) ≤ 24

2π

(15)
where N (U2  dG  ε) is a covering number (the number of balls in Cd×d of radius ε in the metric dG
that are needed to cover the set U2)  and the metric dG is given by

log1/2 N (U2  dG  ε)dε 

0

2πEg sup
X∈U2

√

L0 ≤

(cid:16)E[(G(X) − G(Y ))2]
(cid:17)1/2

dG(X  Y ) =

.

(16)

Deﬁne a new norm (actually a semi-norm) (cid:107)·(cid:107)X on Cd×d  as follows:

(17)
We use this to upper-bound the metric dG. An elementary calculation shows that dG(X  Y ) ≤
(r) . This lets us upper-bound the covering numbers in

2R(cid:107)X − Y (cid:107)X  where R = (cid:107)(cid:80)m

(cid:12)(cid:12)(cid:107)1/2

(cid:1)(cid:0)Vi

(cid:12)(cid:12)Vi

i=1 ... m

i=1

(cid:107)M(cid:107)X = max

|(Vi  M )|.

dG with covering numbers in (cid:107)·(cid:107)X:

r ).

√
ε
2R

2R ) = N ( 1√

r U2 (cid:107)·(cid:107)X  

N (U2  dG  ε) ≤ N (U2 (cid:107)·(cid:107)X   ε

(18)
We will now bound these covering numbers. First  we introduce some notation: let (cid:107)·(cid:107)p denote the
Schatten p-norm on Cd×d  and let Bp be the unit ball in this norm. Also  let BX be the unit ball in
the (cid:107)·(cid:107)X norm.
r U2 ⊆ B1 ⊆ K · BX.
1√
Observe that
maxi=1 ... m(cid:107)Vi(cid:107)(cid:107)M(cid:107)∗ ≤ K(cid:107)M(cid:107)∗.) This gives a simple bound on the covering numbers:

(The second inclusion follows because (cid:107)M(cid:107)X ≤

r U2 (cid:107)·(cid:107)X   ε) ≤ N (B1 (cid:107)·(cid:107)X   ε) ≤ N (K · BX  (cid:107)·(cid:107)X   ε).

(19)
This is 1 when ε ≥ K. So  in Dudley’s inequality  we can restrict the integral to the interval [0  K].
When ε is small  we will use the following simple bound (equation (5.7) in [23]):

N ( 1√

(20)
When ε is large  we will use a more sophisticated bound based on Maurey’s empirical method and
entropy duality  which is due to [16] (see also [17]):

N (K · BX  (cid:107)·(cid:107)X   ε) ≤ (1 + 2K

ε )2d2

.

N (B1 (cid:107)·(cid:107)X   ε) ≤ exp( C2

1 K2
ε2
We defer the proof of (21) to the next section.
Using (20) and (21)  we can bound the integral in Dudley’s inequality. We get
√

log3 d log m) 

for some constant C1.

L0 ≤ C4R

rK log5/2 d log1/2 m 

(21)

(22)

where C4 is some universal constant. This proves the lemma.

7

3.3 Proof of Equation (21) (covering numbers of the nuclear-norm ball)

Our result will follow easily from a bound on covering numbers introduced in [16] (where it appears
as Lemma 1):

Lemma 3.2 Let E be a Banach space  having modulus of convexity of power type 2 with constant
λ(E). Let E∗ be the dual space  and let T2(E∗) denote its type 2 constant. Let BE denote the unit
ball in E.
Let V1  . . .   Vm ∈ E∗  such that (cid:107)Vj(cid:107)E∗ ≤ K (for all j). Deﬁne the norm on E 

(23)

(24)

(cid:107)M(cid:107)X = max

|(Vj  M )|.

j=1 ... m

Then  for any ε > 0 

ε log1/2 N (BE (cid:107)·(cid:107)X   ε) ≤ C2λ(E)2T2(E∗)K log1/2 m 

where C2 is some universal constant.

The proof uses entropy duality to reduce the problem to bounding the “dual” covering number. The
p denote the complex vector space Cm with the (cid:96)p norm. Consider
basic idea is as follows. Let (cid:96)m
1 → E∗ that takes the j’th coordinate vector to Vj. Let N (S) denote the number of
the map S : (cid:96)m
balls in E∗ needed to cover the image (under the map S) of the unit ball in (cid:96)m
1 . We can bound N (S)
using Maurey’s empirical method. Also deﬁne the dual map S∗ : E → (cid:96)m∞  and the associated dual
covering number N (S∗). Then N (BE (cid:107)·(cid:107)X   ε) is related to N (S∗). Finally  N (S) and N (S∗) are
related via entropy duality inequalities. See [16] for details.
We will apply this lemma as follows  using the same approach as [17]. Let Sp denote the Banach
space consisting of all matrices in Cd×d with the Schatten p-norm.
Intuitively  we want to set
E = S1 and E∗ = S∞  but this won’t work because λ(S1) is inﬁnite. Instead  we let E = Sp 
p = (log d)/(log d − 1)  and E∗ = Sq  q = log d. Note that (cid:107)M(cid:107)p ≤ (cid:107)M(cid:107)∗  hence B1 ⊆ Bp and
(25)
Also  we have λ(E) ≤ 1/
log d − 1 (see the
Appendix in [17]). Note that (cid:107)M(cid:107)q ≤ e(cid:107)M(cid:107)  thus we have (cid:107)Vj(cid:107)q ≤ eK (for all j). Then  using
the lemma  we have

log d − 1 and T2(E∗) ≤ λ(E) ≤ √

ε log1/2 N (B1 (cid:107)·(cid:107)X   ε) ≤ ε log1/2 N (Bp (cid:107)·(cid:107)X   ε).

√

√

p − 1 =

ε log1/2 N (Bp (cid:107)·(cid:107)X   ε) ≤ C2 log3/2 d (eK) log1/2 m 

(26)

which proves the claim.

4 Outlook

We have showed that random Pauli measurements obey the restricted isometry property (RIP)  which
implies strong error bounds for low-rank matrix recovery. The key technical tool was a bound on
covering numbers of the nuclear norm ball  due to Gu´edon et al [16].
An interesting question is whether this method can be applied to other problems  such as matrix com-
pletion  or constructing embeddings of low-dimensional manifolds into linear spaces with slightly
higher dimension. For matrix completion  one can compare with the work of Negahban and Wain-
wright [10]  where the sampling operator satisﬁes restricted strong convexity (RSC) over a certain set
of “non-spiky” low-rank matrices. For manifold embeddings  one could try to generalize the results
of [24]  which use the sparse-vector RIP to construct Johnson-Lindenstrauss metric embeddings.
There are also many questions pertaining to low-rank quantum state tomography. For example 
how does the matrix Lasso compare to the traditional approach using maximum likelihood estima-
tion? Also  there are several variations on the basic tomography problem  and alternative notions of
sparsity (e.g.  elementwise sparsity in a known basis) [25]  which have not been fully explored.
Acknowledgements: Thanks to David Gross  Yaniv Plan  Emmanuel Cand`es  Stephen Jordan  and
the anonymous reviewers  for helpful suggestions. Parts of this work were done at the University
of California  Berkeley  and supported by NIST grant number 60NANB10D262. This paper is
a contribution of the National Institute of Standards and Technology  and is not subject to U.S.
copyright.

8

References
[1] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis  Stanford  2002.
[2] N. Srebro. Learning with Matrix Factorizations. PhD thesis  MIT  2004.
[3] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum rank solutions to linear matrix

equations via nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[4] M. Fazel  E. Candes  B. Recht  and P. Parrilo. Compressed sensing and robust recovery of
low rank matrices. In 42nd Asilomar Conference on Signals  Systems and Computers  pages
1043–1047  2008.

[5] E. J. Candes and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal

number of random measurements. 2009.

[6] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. of

Comput. Math.  9:717–772.

[7] E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Trans. Inform. Theory  56(5):2053–2080  2009.

[8] D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis.

Inform. Theory  to appear. arXiv:0910.1879  2010.

IEEE Trans.

[9] B. Recht. A simpler approach to matrix completion. J. Machine Learning Research (to appear) 

2010.

[10] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix comple-

tion: Optimal bounds with noise. arXiv:1009.2118  2010.

[11] E. J. Candes and T. Tao. Near-optimal signal recovery from random projections: universal

encoding strategies. IEEE Trans. Inform. Theory  52:5406–5425  2004.

[12] M. Rudelson and R. Vershynin. On sparse reconstruction from Fourier and Gaussian measure-

ments. Commun. Pure and Applied Math.  61:1025–1045  2008.

[13] D. Gross  Y.-K. Liu  S. T. Flammia  S. Becker  and J. Eisert. Quantum state tomography via

compressed sensing. Phys. Rev. Lett.  105(15):150401  Oct 2010. arXiv:0909.3304.

[14] E. J. Candes and Y. Plan. Matrix completion with noise. Proc. IEEE  98(6):925 – 936  2010.
[15] B. Brown  S. Flammia  D. Gross  and Y.-K. Liu. in preparation  2011.
[16] O. Gu´edon  S. Mendelson  A. Pajor  and N. Tomczak-Jaegermann. Majorizing measures and
proportional subsets of bounded orthonormal systems. Rev. Mat. Iberoamericana  24(3):1075–
1095  2008.

[17] G. Aubrun. On almost randomizing channels with a short Kraus decomposition. Commun.

Math. Phys.  288:1103–1116  2009.

[18] M. A. Nielsen and I. Chuang. Quantum Computation and Quantum Information. Cambridge

University Press  2001.

[19] A. Rohde and A. Tsybakov.

arXiv:0912.5338  2009.

Estimation of high-dimensional

low-rank matrices.

[20] V. Koltchinskii  K. Lounici  and A. B. Tsybakov. Nuclear norm penalization and optimal rates

for noisy low rank matrix completion. arXiv:1011.6256  2010.

[21] Y.-K. Liu. Universal low-rank matrix recovery from Pauli measurements. arXiv:1103.2816 

2011.

[22] M. Ledoux and M. Talagrand. Probability in Banach spaces. Springer  1991.
[23] G. Pisier. The volume of convex bodies and Banach space geometry. Cambridge  1999.
[24] F. Krahmer and R. Ward. New and improved Johnson-Lindenstrauss embeddings via the re-

stricted isometry property. SIAM J. Math. Anal.  43(3):1269–1281  2011.

[25] A. Shabani  R. L. Kosut  M. Mohseni  H. Rabitz  M. A. Broome  M. P. Almeida  A. Fedrizzi 
and A. G. White. Efﬁcient measurement of quantum dynamics via compressive sensing. Phys.
Rev. Lett.  106(10):100401  2011.

[26] P. Wojtaszczyk. Stability and instance optimality for gaussian measurements in compressed

sensing. Found. Comput. Math.  10(1):1–13  2009.

9

,Xinchen Yan
Jimei Yang
Ersin Yumer
Yijie Guo
Honglak Lee