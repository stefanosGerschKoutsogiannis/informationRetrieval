2019,A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off,Reducing the precision of weights and activation functions in neural network training  with minimal impact on performance  is essential for the deployment of these models in resource-constrained environments. We apply mean field techniques to networks with quantized activations in order to evaluate the degree to which quantization degrades signal propagation at initialization. We derive initialization schemes which maximize signal propagation in such networks  and suggest why this is helpful for generalization. Building on these results  we obtain a closed form implicit equation for $L_{\max}$  the maximal trainable depth (and hence model capacity)  given $N$  the number of quantization levels in the activation function. Solving this equation numerically  we obtain asymptotically: $L_{\max}\propto N^{1.82}$.,A Mean Field Theory of Quantized Deep Networks:

The Quantization-Depth Trade-Off

Yaniv Blumenfeld
Technion  Israel

yanivblm6@gmail.com

Dar Gilboa

Columbia University

dargilboa@gmail.com

Daniel Soudry
Technion  Israel

daniel.soudry@gmail.com

Abstract

Reducing the precision of weights and activation functions in neural network train-
ing  with minimal impact on performance  is essential for the deployment of these
models in resource-constrained environments. We apply mean ﬁeld techniques
to networks with quantized activations in order to evaluate the degree to which
quantization degrades signal propagation at initialization. We derive initialization
schemes which maximize signal propagation in such networks  and suggest why
this is helpful for generalization. Building on these results  we obtain a closed
form implicit equation for Lmax  the maximal trainable depth (and hence model
capacity)  given N  the number of quantization levels in the activation function.
Solving this equation numerically  we obtain asymptotically: Lmax ∝ N 1.82.

1

Introduction

As neural networks are increasingly trained and deployed on-device in settings with memory and space
constraints [12  5]  a better understanding of the trade-offs involved in the choice of architecture and
training procedure are gaining in importance. One widely used method to conserve resources is the
quantization (discretization) of the weights and/or activation functions during training [6  25  13  3].
When choosing a quantized architecture  it is natural to expect depth to increase the ﬂexibility of
the model class  yet choosing a deeper architecture can make the training process more difﬁcult.
Additionally  due to resource constraints  when using a quantized activation function whose image
is a ﬁnite set of size N  one would like to choose the smallest possible N such that the model is
trainable and performance is minimally affected. There is a trade-off here between the capacity of the
network which depends on its depth and the ability to train it efﬁciently on the one hand — and the
parsimony of the activation function used on the other.
We quantify this trade-off between capacity/trainability and the degree of quantization by an analysis
of wide neural networks at initialization. This is achieved by studying signal propagation in deep
quantized networks  using techniques introduced in [24  26] that have been applied to numerous
architectures. Signal propagation will refer to the propagation of correlations between inputs into the
hidden states of a deep network. Additionally  we consider the dynamics of training in this regime
and the effect of signal propagation on the change in generalization error during training.
In this paper 

• We suggest (section 3.2) that if the signal propagation conditions do not hold  generalization
error in early stages of training should not decrease at a typical test point  potentially
explaining the empirically observed beneﬁt of signal propagation to generalization. This
is done using an analysis of learning dynamics in wide neural networks  and corroborated
numerically.
• We obtain (section 4.2) initialization schemes that maximize signal propagation in certain

classes of feed-forward networks with quantized activations.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

• Combining these results  we obtain an expression for the trade-off between the quantization
level and the maximal trainable depth of the network (eq. 18)  in terms of the depth scale of
signal propagation. We experimentally corroborate these predictions (Figure 3).

2 Related work

Several works have shown that training a 16 bit numerical precision is sufﬁcient for most machine
learning applications [10  7]  with little to no cost to model accuracy. Since  many more aggressive
quantization schemes were suggested [13  18  22  21]  ranging from the extreme usage of 1-bit at
representations and math operations [25  6]  to a more conservative usage of 8-bits [3  28]  all in
effort to minimize the computational cost with minimal loss to model accuracy. Theoretically  it is
well known that a small amount of imprecision can signiﬁcantly degrade the representational capacity
of a model. For example  an inﬁnite precision recurrent neural network can simulate a universal
Turing machine [27]. However  any numerical imprecision reduces the representational power of
these models to that of ﬁnite automata [19]. In this paper  we focus on the effects of quantization on
training. So far  these effects are typically quantiﬁed empirically  though some theoretical work has
been done in this direction (e.g. [17  1  36  33]).
Signal propagation in wide neural networks has been the subject of recent work for fully-connected
[24  26  23  31]  convolutional [30] and recurrent architectures [4  8]. These works study the
evolution of covariances between the hidden states of the network and the stability of the gradients.
These depend only on the leading moments of the weight distributions and the nonlinearities at the
inﬁnite width limit  greatly simplifying analysis. They identify critical initialization schemes that
allow training of very deep networks (or recurrent networks on long time sequence tasks) without
performing costly hyperparameter searches. While the analytical results in these works assume that
the layer widths are taken to inﬁnity sequentially (which we will refer to this as the sequential limit) 
the predictions prove predictive when applied to networks with layers of equal width once the width
is typically of the order of hundreds of neurons. For fully connected networks it was also shown using
an application of the Central Limit Theorem for exchangeable random variables that the asymptotic
behavior at inﬁnite width is independent of the order of limits [20].

3 Preliminaries: the mean ﬁeld approach

3.1 Signal propagation in feed-forward networks

We now review the analysis of signal propagation in feed-forward networks performed in [24  26].
The network function f : Rn0 → RnL+1 is given by

φ(α(0)(x)) = x

α(l)(x) = W (l)φ(α(l−1)(x)) + b(l)
f (x) = α(L+1)(x)

l = 1  ...  L

(1)

w

i ∼ N (0  σ2

ij ∼ N (0  σ2

for input x ∈ Rn0  weight matrices W (l) ∈ Rnl×nl−1 and nonlinearity φ : R → R. The weights are
n(l−1) )  b(l)
initialized using W (l)
b ) so that the variance of the neurons at every
layer is independent of the layer widths 1.
According to Theorem 4 in [20]  under a mild condition on the activation function that is satisﬁed
by any saturating nonlinearity  the pre-activations α(l)(x) converge in distribution to a multivariate
Gaussian as the layer widths n1  ...  nL are taken to inﬁnity in any order (with n0  nL+1 ﬁnite) 2.
In the physics literature the approximation obtained by taking this limit is known as the mean ﬁeld
approximation.

1In principle the following results should hold under more generally mild moment conditions alone.
2When taking the sequential limit  asymptotic normality is a consequence of repeated application of the

Central Limit Theorem [24]

2

The covariance of this Gaussian at a given layer is then obtained by the recursive formula

Eα(l)

i (x)α(l)

j (x(cid:48)) = E nl−1(cid:80)

(cid:104)

W (l)
k k(cid:48)=1
Eφ(α(l−1)

σ2
w

=

δij.
Omitting the dependence on the inputs x  x(cid:48) in the RHS below  we deﬁne
C (l)
1

(cid:18) 1

i (x) Eα(l)
i (x(cid:48)) Eα(l)

i (x)α(l)
i (x(cid:48))α(l)

i (x)α(l)
i (x)α(l)

i (x(cid:48))
i (x(cid:48))

= Q(l)

C (l)

1

b

(x))φ(α(l−1)
k(cid:48)
(x(cid:48))) + σ2

(cid:105)

k

jk(cid:48)φ(α(l−1)
ik W (l)
(x))φ(α(l−1)
(cid:33)

1

Combining eqs. 2 and 3 we obtain the following two-dimensional dynamical system:

(cid:20)

σ2
w

u∼N (0 Q(l−1))

φ2(u) + σ2
b

E

E

1

Q(l−1)

σ2
w

(u1 u2)∼N (0 Σ(Q(l−1) C(l−1)))

φ(u1)φ(u2) + σ2
b

(2)

(3)

(x(cid:48))) + b(l)

i b(l)

j

(cid:19)
(cid:21)  ≡ M

= Σ(Q(l)  C (l)).

(cid:20)(cid:18) Q(l−1)

C (l−1)

(cid:19)(cid:21)

 

Eα(l)

(cid:32) Eα(l)

(cid:19)

=

(cid:18) Q(l)

C (l)

w  σ2

(4)
where M depends on the nonlinearity and the initialization hyperparameters σ2
b and the initial
conditions (Q(0)  C (0))T depend also on x  x(cid:48). See Figure 1 for a visualization of the covariance
propagation.
Once the above dynamical system converges to a ﬁxed point (Q∗  C∗) or at least approaches it to
within numerical precision  information about the initial conditions is lost. As argued in [26]  this is
detrimental to learning as inputs in different classes can no longer be distinguished in terms of the
network output (assuming the ﬁxed point C∗ is independent of C (0)  see Lemma 1). The convergence
rate to the ﬁxed point can be obtained by linearizing the dynamics around it. This can be done for
the two dimensional system as a whole  yet in [26] it was also shown that  for any monotonically
increasing nonlinearity  convergence of this linearized dynamical system in the direction C (l) cannot
be faster than convergence in the Q(l) direction  and thus studying convergence can be reduced to the
simpler one dimensional system C (l) = MQ∗ (C (l−1)) that is obtained by assuming Q(l) has already
converged  as assumption we review in appendix K. The convergence rate is given by the following
known results of [26  8] which we recapitulate for completeness:

(cid:19)

(cid:18) 1 C

C 1

Lemma 1. [26  8] Deﬁning Σ(Q  C) = Q
system

MQ∗ (C) =

1
Q∗

σ2
w

(u1 u2)∼N (0 Σ(Q∗ C))
when linearized around a ﬁxed point C∗  converges at a rate

E

for Q ≥ 0  C ∈ [−1  1] the dynamical

(cid:21)

φ(u1)φ(u2) + σ2
b

(5)

∂MQ∗ (C)

χ =

(6)
Additionally  MQ∗ (C) has at most one stable ﬁxed point in the range [0  1] for any choice of φ such
that φ is odd or φ(cid:48)(cid:48) is non-negative.

(ua ub)∼N (0 Σ(Q∗ C∗))

= σ2
w

∂C

E

φ(cid:48)(u1)φ(cid:48)(u2).

(cid:20)
(cid:12)(cid:12)(cid:12)(cid:12)C∗

Proof: See Appendix A.
We subsequently drop the subscript in MQ∗ (C) to lighten notation. The corresponding time scale of
convergence in the linearized regime is

ξ = − 1
log χ

.

(7)

χ depends on the initialization hyperparameters and choice of nonlinearity  and it follows from the
considerations above that signal propagation from the inputs to the outputs of a deep network would
be facilitated by a choice of χ such that ξ diverges  which occurs as χ approaches 1 from below.
Indeed  as observed empirically across multiple architectures and tasks [30  4  8  31]  up to a constant
factor ξ typically gives the maximal depth up to which a network is trainable. These calculations
motivate initialization schemes that satisfy:

χ = 1

3

in order to train very deep networks. We will show shortly that this condition is unattainable for a
large class of quantized activation functions. 3
The analysis of forward signal propagation in the sense described above in networks with continuous
activations is related to the stability of the gradients as well [26]. The connection is obtained by
relating the rate of convergence χ to the ﬁrst moment of the state-to-state Jacobian

.

∂ ˆα(l)
∂ ˆα(l−1)

Etr(cid:0)JJ T(cid:1) .

(8)

(9)

Taking all the layer widths to be equal to n  the ﬁrst moment is given by

J = lim
l→∞

mJJ T =

1
n

Since high powers of this matrix will appear in the gradient  controlling its spectrum can prevent the
gradient from exploding or vanishing. In the case of quantized activations  however  the relationship
between the Jacobian and the convergence rate χ no longer holds since the gradients vanish almost
surely and modiﬁed weight update schemes such as the Straight-Through Estimator (STE) [11  13]
are used instead. However  one can deﬁne a modiﬁed Jacobian JSTE that takes the modiﬁed update
scheme into account and control its moments instead.

Figure 1: Propagation of empirical covariance between hidden states at different layers  in quantized
feed-forward networks with N = 16  varying the standard deviation of the weights σw. ∆θ is
the angle between two normalized inputs. Signal propagation is maximized when σw = σopt
w   and
degrades as σw deviates from it.
3.2 Signal propagation may improve generalization

The argument that a network will be untrainable if signals cannot propagate from the inputs to the loss 
corresponding to the rapid convergence of the dynamical system eq. 4  has empirical support across
numerous architectures. A choice of initialization hyperparameters that facilitates signal propagation
has also been shown to lead to slight improvements in generalization error  yet understanding of
this was beyond the scope of the existing analysis. Indeed  there is also empirical evidence that
when training very deep networks it is only the generalization error that is impacted adversely but
the training error is not [30]. Additionally  one may wonder whether a deep network could still be
trainable despite a lack of signal propagation. On the one hand  rapid convergence of the correlation
map between the pre-activations is equivalent to the distance between f (x)  f (x(cid:48)) converging to a
value that is independent of the distance between x  x(cid:48). On the other  since deep networks can ﬁt
random inputs and labels [34] this convergence may not impede training. .
To understand the effect of signal propagation on generalization  we consider the dynamics of learning
for wide  deep neural networks in the setting studied in [14  16]. We note that this setting introduces
an unconventional scaling of the weights. Despite this  it should be a good approximation for the
early stages of learning in networks with standard initialization  as long as the weights do not change
too much from their initial values. In this regime  the function implemented by the network evolves
linearly in time  with the dynamics determined by the Neural Tangent Kernel (NTK). We argue that

3It will at times be convenient to consider the dynamics of the correlations of the post-activations(cid:98)α(l) =
φ(α(l)) which we denote by (cid:99)M((cid:98)C). The rates of convergence are identical in both cases  as shown in Appendix

B.

4

202020406080Layersoptw-0.2202020406080optw-0.1202020406080optw202020406080optw+0.1202020406080optw+0.21.000.750.500.250.000.250.500.751.00rapid convergence of eq. 4 in deep networks implies that the error at a typical test point should not
decrease during training since the resulting form of the NTK will be independent of the label of the
test point. Conversely  this effect will be mitigated with a choice of hyperparameters that maximizes
signal propagation  which could explain the beneﬁcial effect on generalization error that is observed
empirically. We provide details and empirical evidence in support of this claim for networks with
both quantized and continuous activation functions in Appendix M.

4 Mean ﬁeld theory of signal propagation with quantized activations

In this section  we will explore the effects of using a quantized activation function on signal prop-
agation in feed-forward networks. We will start by developing the mean ﬁeld equations for a sign
activations and then consider more general activation function  and establish a theory that predicts
the relationship between the number of quantization states  the initialization parameters  and the
feed-forward network depth.

4.1 Warm-up: sign activations

We begin by considering signal propagation in the network in eq. 1 with φ(x) = sign(x). Substituting
φ(x) = sign(x)  φ(cid:48)(u) = 2δ(u) in eqs. 4 and 6 gives

Q∗ = σ2

w + σ2

b   χ = 4σ2
w

E

(u1 u2)∼N (0 Σ(Q∗ C∗))

δ(u1)δ(u2).

(10)

(11)

(12)

As shown in Appendix C  we obtain

χ =

b )(cid:112)1 − (C∗)2

2σ2
w

π (σ2

w + σ2

M(C) =

2σ2
π arcsin (C) + σ2
w

b

w + σ2
σ2
b

.

The closed form expressions 11 and 12  which are not available for more complex architectures 
expose the main challenge to signal propagation. It is clear from these expressions that the derivative
of M(C) diverges at 1  and since M(C) is differentiable and convex  it can have no stable ﬁxed
point in [0  1] that satisﬁes the signal propagation condition χ = 1. In fact  as we show in Appendix
L.1 that the maximal value of χ for this architecture is achievable when σb = 0  and is bounded from
above by χmax = 2
π for all choices of the initialization hyperparameters. The corresponding depth
scale is bounded by ξmax < 3.
Additionally  one may wonder if using stochastic binary activations [13] might improve signal propa-
gation. In Appendix D we show this is not the case: we consider a stochastic rounding quantization
scheme and show that stochastic rounding can only further decrease the signal propagation depth
scale.

4.2 General quantized activations
We consider a general activation function φN : R → S  where S is a ﬁnite set of real numbers of size
|S| = N. To obtain a ﬂexible class of non-decreasing functions of this form  we deﬁne

H (x − gi) hi  

φN (x) = A +

(13)
where A ∈ R ∀i ∈ {1  2  ...  N − 1}  gi ∈ R  hi ∈ R>0  and H : R → R is the Heaviside function.
A to the maximum state A +(cid:80)N−1
This activation function can be thought of as a "stairs" function  going from the minimum state of
i=1 hi  over N − 1 stairs  with stair i located at an offset gi with
a height hi. We will assume that the offsets gi are ordered  for simplicity. The development of the
mean ﬁeld equations for this activation function is located in appendix E  where we ﬁnd that:
(cid:98)Q(l) =
(cid:112)

w(cid:98)Q(l) + σ2

− max(gi  gj)

  Q(l+1) = σ2

N−1(cid:88)

N−1(cid:88)

(14)

b

min(gi  gj)

(cid:112)

(cid:33)

(cid:32)

Φ

(cid:32)

(cid:33)

hihjΦ

i=1

j=1

Q(l)

Q(l)

N−1(cid:88)

i=1

5

N−1(cid:88)

N−1(cid:88)

hihj exp

− g2
2Q∗(cid:16)

i − 2C∗gigj + g2

1 − (C∗)2(cid:17)

j

  

2πQ∗(cid:113)

σ2
w
1 − (C∗)2

χ =

(15)

i

g2
i

i=1

j=1

(cid:105)

2Q∗(1+C∗)

(cid:104)−

2πQ∗√

wh2
σ2
1−(C∗)2 exp

where Φ is the gaussian CDF and (cid:98)Q(l) is the hidden state covariance  as explained in appendix B.

This expression diverges as C∗ → 1 since all the summands are non-negative and the diagonal ones
. Since M(C) is convex (see Lemma 1)  we ﬁnd that
simplify to
as in the case of sign activation  χ = 1 is not achievable for any choice of a quantized activation
function.
To optimize the signal propagation for any given number of states  we would like to ﬁnd the parameters
that will bring the ﬁxed point slope χ as close as possible to 1. For simplicity  we will henceforth use
the initialization σb = 0  which is quite common [9]. Empirical evidence in appendix F suggest that
using σb > 0 is sub-optimal  which is not very surprising  given our similar (exact) results for sign
activation. For σb = 0  C = 0 becomes a ﬁxed point. We eliminate eq. 15 direct dependency on Q∗ 
by deﬁning normalized offsets ˜g ≡ g√
Q∗ . By moving to normalized offsets  substituting C∗ = 0 and
(cid:80)N−1
(cid:80)N−1
the remaining Q∗ by eq. 14  our expression for the ﬁxed point slope becomes:
(cid:80)N−1
j=1 Φ (− max(˜gi  ˜gj)) Φ (min(˜gi  ˜gj)) hihj

2π exp(cid:2)− 1

(cid:1)(cid:3) hihj

(cid:80)N−1

(cid:0)˜g2

i + ˜g2
j

(16)

χ =

j=1

i=1

i=1

1

2

2

Eq. 16 provides us with way to determine the quality of any quantized activation function in regard
to signal propagation  without concerning ourselves with the initialization parameters  that will only

eq. 15  moving from normalized offsets to actual offsets becomes trivial.
To measure the relation between the number of states and depth scale  we will use eq. 16 over a limited
set of constant-spaced activations  where we choose A < 0 ∀i ∈ {1  ..  N − 1}  hi = const. and
the offsets are evenly spaced and centered around zero  with D deﬁned as the distance between two
Q∗ . We view this conﬁguration
as the most obvious selection of activation function  where the ’stairs’ are evenly spaced between the
minimal and the maximal state. Using eq. 16 on an activation in this set  we get:

have a linear effect on the offsets. Since the normalized offsets are sufﬁcient to determine (cid:98)Q  Q  using
(cid:1)  and ˜D deﬁned as ˜D = D√
sequential offsets so that gi = D(cid:0)i − N
(cid:104)− 1
(cid:0)i2 + j2(cid:1) ˜D2(cid:105)
(cid:80)
(cid:16)
(cid:16)− max (i  j) ˜D
(cid:17)

2 |∀k ∈ N  k < N(cid:9). A numeric analysis using of eq. 17 is presented in ﬁgure 2 

when K =(cid:8)k − N

and reveals a clear logarithmic relation between the level of quantization to the optimal ﬁxed point
slope  and the normalized spacing required to reach this optimal conﬁguration. By extrapolating the
numerical results  as seen in the right panels of Fig. 2  we ﬁnd a good approximations for the the
maximal achievable slope for any quantization level χmax(N ) and the corresponding normalized
spacing Dopt(N ). Using those extrapolated values  we predict the depth-scale of a quantized  feed-
forward network to be:

(cid:80)
(cid:80)

min (i  j) ˜D

(cid:80)

1
2π exp

j∈K Φ

(cid:17)

(17)

j∈K

i∈K

i∈K

χ =

Φ

2

ξN = −

1

log(χmax(N ))

(cid:39) −

1

log(1 − e0.71 (N + 1)

−1.82)

(cid:39) 1
2

(N + 1)1.82 .

(18)

where the latter approximation is valid for large N. While the depth scale in eq. 18 is applicable
to uniformly spaced quantized activations  numerical results presented in Appendix G suggest that
using more complex activations with the same quantization level will not produce better results.
In their work regarding mean ﬁeld theory of convolutional neural networks  [30] shows that the
dynamics of hidden-layer’s correlations in CNNs decouple into independently evolving Fourier
modes that evolves near the ﬁxed point  each with a corresponding ﬁxed-point-slope of χcλi  with χc
depending the initialization hyperparameters and equivalent to the ﬁxed point slope as calculated for
fully connected networks  and λi ≤ 1 being a frequency dependant modiﬁer corresponding to mod i.
While the exact dynamics in this case may depend on the decomposition of the input to Fourier mods 
it is apparent that the maximal depth-scale of each mod can not exceed the depth-scale calculated for
the fully-connected case  and thus our upper limit on the number of layers holds for the case of CNNs.

6

Similarly  following [4] and [8]  our results can be easily extended to single layer RNNs  LSTMs and
GRUS  in which case the limitation applies to the timescale of the network memory.

Figure 2: Numerical analysis of the covariance propagation ﬁxed point slope for quantized activation
functions. Left: The convergence rate in eq. 17 of the covariances of the hidden states as a function
of the normalized spacing between offsets ˜D for activations with different levels of quantization N.
Top Right: The difference between 1 and maximal achievable convergence rate χmax as a function of
N. Bottom Right: The normalized spacing between states ˜D corresponding to χmax as a function of
N. We ﬁnd that the dependence of 1 − χmax on N is approximated well by a power law.

5 Experimental results

s

r   2

r   ..  r−1

(cid:8)ui =(cid:112)Q∗
(cid:112)Q∗

(cid:0)u0 cos(θ) + u1 sin(θ)(cid:1)|i ∈ {0  1

r }  θ = 2πi(cid:9)  where r = 500 is the num-

To visualize the covariance propagation in eq. 2 we reconstruct an experiment presented
in [24]  and apply it to untrained quantized neural networks. We consider a neural net-
work with L = 100 fully-connected layers  all of width n = 1000. We draw two
orthonormal vectors u0  u1 ∈ R1000 and generate the 1 dimensional manifold U =
ber of samples  and Q∗
s is the ﬁxed point  calculated numerically. After initializing the neural network 
we use the manifold values as inputs to the neural network and measure the covariance in all hidden
layers. We then plot in Figure 1 the empirical covariance of the hidden states as a function of the
difference in the angle θ of their corresponding inputs. The reason for multiplying the initial values by
s is so we can isolate the convergence of the off-diagonal correlations from that of the diagonal.
To test the predictions of the theory  we have constructed a similar experiment to the one described in
[26]  training neural networks of varying depths over the MNIST dataset. We study how the maximal
trainable depth of a quantized activation fully-connected network depends on the weight variance
w and the number of states in the activation function N. For our quantized activations  we used the
σ2
constant-spaced activations we have analyzed in section 4.2:
x − 2

φN (x) = −1 +

N−1(cid:88)

(cid:19)(cid:19)

(cid:18)

H

i=1

N − 1

N − 1
which describes an activation function with a distance of D = 2
ranging between -1 and 1.
To ﬁnd the best initialization parameters for each activation function  we ﬁrst used eq. 14 to compute
Q∗ is optimized ( ˜Dopt  computed using the linear regression
  and
thus ensured that the normalized offsets are indeed optimal. Gradients are computed using the

(cid:98)Q∗ assuming our normalized spacing D√
parameters of Figure 2 bottom right panel). Then  we picked σb = 0  σw = 1√(cid:98)Q∗

N−1 between offsets  and with states

˜Dopt

D

 

(cid:18)

2

i − N
2

7

012̃D0.650.700.750.800.850.900.951.00χ#̃States:234581632102410−510−410−310−210−11001−χmaxlog(1−χmax)=−1.82log(N+1)+0.71R2=0.9998ExtrapolationNumerical Re ult 101102103# State 10−210−1100̃Doptlog̃̃Dopt)=−0.88log̃N+1)+1.40R2=0.99996E#trapolationNumerical Re ult Figure 3: Test accuracy of feed-forward networks of different depth with quantized activation
functions trained on MNIST classiﬁcation after 1600 training steps  compared with the theoretical
depth scale predictions (eq. 7). Up to a constant factor  the theoretical depth scale predicts the phase
transition between regimes where a network is trainable and one where training fails. Left: Networks
with a 10 states activation function and different values of the weight variance. Right: Networks
with different quantization levels (number of states)  with variances adjusted to allow optimal signal
propagation.

(cid:26)∆output

0

Straight-Through Estimator (STE) [13]:

∆input =

|input| < 1
else

 

(19)

where ∆output is gradient we get from the next layer and ∆input is the gradient we pass to the preceding
layer. The conditions required for allowing the gradients information to propagate backward are
discussed in appendix J. Those conditions are not enforced in this experiment  as they have no
signiﬁcant effect on the results  as shown in appendix H  where we add more results that isolate
the forward-pass from the backward pass. Also included in appendix H are results that show the
evolution of the training and test accuracy in training time. A simpliﬁed initialization scheme for the
use of practitioners is included in appendix I.
We set the hidden layer width to 2048. We use SGD for training  a learning rate of 10−3 for networks
with 10-90 layers  and a learning rate of 5 × 10−4 when training 100-220 layers. Those parameters
were selected to match those reported in [26]  with the second learning rate adjusted to ﬁt our
area-of-search. We also use a batch size of 32  and use a standard preprocessing of the MNIST input4.
Figure 3 shows that the initialization of the network using the parameters suggested by our theory
achieves the optimal trainability when the number of layers is high. When measuring test accuracy
at the early stage of the network  we can see that the accuracy is high when the network has ∼ 4ξ
layers or less. As demonstrated by the advanced training stage results shown in appendix H  and by
the results of [26]  networks of depth exceeding ∼ 6ξ appear to be untrainable.

6 Discussion
In this paper  we study the effect of using quantized activations on the propagation of signals in
deep neural networks  from the inputs to the outputs. We focus on quantized activations  which
maps its input to a ﬁnite set of N possible outputs. Our analysis suggests an initialization scheme
that improves network trainability  and that fully-connected and convolutional networks to become
untrainable when the number of layers exceeds Lmax ∼ 3 (N + 1)1.82.
Additionally  we propose a possible explanation for the improved generalization observed when
training networks that are initialized to enable stable signal propagation. While the motivation for
the critical initialization has been improved trainability [26]  empirically these initialization schemes
were shown to improve generalization as well  an observation that was beyond the scope of the
analysis which motivated them. By considering the dynamics of learning in wide networks that

4The code for running this experiment and more is provided in https://github.com/yanivbl6/

quantized_meanfield.

8

0.60.81.01.21.4σw101102Layers4ξ6ξ9ξ46810121416# States101102Layers4ξ6ξ9ξ020406080100exhibit poor signal propagation  we ﬁnd that generalization error in the early stages of training will
typically not improve. This effect will be minimized when using a critical initialization.
The limitations of poor signal propagation can perhaps be overcome with certain modiﬁcations to
the architecture or training procedure. Residual connections  for example  can be initialized [35] to
maintain the signal propagation conditions even when the full-network depth exceeds our theoretical
limit [31]. Another possible modiﬁcation is batch normalization  which we did not consider in
the analysis. While batch normalization by itself was shown to have negative side effects on the
signal propagation [32]  other studies [3  6  13] have already suggested that applying proper batch
normalization is key when training quantized feed-forward networks. There are  however  cases
where batch normalization does not work well  like in the case of recurrent neural networks. We
expect our ﬁndings to have as increased signiﬁcance if generalized to such architectures  as was done
previously for continuous activations [4  8].

Acknowledgements

The work of DS was supported by the Israel Science foundation (grant No. 31/1031)  the Taub
Foundation and used a Titan Xp donated by the NVIDIA Corporation. The work of DG was supported
by the NSF NeuroNex Award DBI-1707398 and the Gatsby Charitable Foundation. The work of
DG and DS was done in part while the authors were visiting the Simons Institute for the Theory of
Computing.

References
[1] Alexander G. Anderson and Cory P. Berg. The High-Dimensional Geometry of Binary Neural

Networks. ICLR  (2014):1–13  2018.

[2] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 
2019.

[3] Ron Banner  Itay Hubara  Elad Hoffer  and Daniel Soudry. Scalable methods for 8-bit training
of neural networks. In Advances in Neural Information Processing Systems  pages 5145–5153 
2018.

[4] Minmin Chen  Jeffrey Pennington  and Samuel S Schoenholz. Dynamical isometry and a mean
ﬁeld theory of rnns: Gating enables signal propagation in recurrent neural networks. arXiv
preprint arXiv:1806.05394  2018.

[5] Wenlin Chen  James Wilson  Stephen Tyree  Kilian Weinberger  and Yixin Chen. Compressing
neural networks with the hashing trick. In International Conference on Machine Learning 
pages 2285–2294  2015.

[6] Matthieu Courbariaux  Itay Hubara  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized

neural networks. Advances in Neural Information Processing Systems  2016.

[7] Dipankar Das  Naveen Mellempudi  Dheevatsa Mudigere  Dhiraj Kalamkar  Sasikanth Avancha 
Kunal Banerjee  Srinivas Sridharan  Karthik Vaidyanathan  Bharat Kaul  Evangelos Georganas 
et al. Mixed precision training of convolutional neural networks using integer operations. arXiv
preprint arXiv:1802.00930  2018.

[8] Dar Gilboa  Bo Chang  Minmin Chen  Greg Yang  Samuel S Schoenholz  Ed H Chi  and Jeffrey
Pennington. Dynamical isometry and a mean ﬁeld theory of lstms and grus. arXiv preprint
arXiv:1901.08987  2019.

[9] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor-
ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial
intelligence and statistics  pages 249–256  2010.

[10] Suyog Gupta  Ankur Agrawal  Kailash Gopalakrishnan  and Pritish Narayanan. Deep learning
with limited numerical precision. In International Conference on Machine Learning  pages
1737–1746  2015.

[11] G Hinton. Neural networks for machine learning. coursera [video lectures]  2012.

9

[12] Andrew G Howard  Menglong Zhu  Bo Chen  Dmitry Kalenichenko  Weijun Wang  Tobias
Weyand  Marco Andreetto  and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications. arXiv preprint arXiv:1704.04861  2017.

[13] Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Quan-
tized neural networks: Training neural networks with low precision weights and activations.
The Journal of Machine Learning Research  18(1):6869–6898  2017.

[14] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and

generalization in neural networks. June 2018.

[15] Jaehoon Lee  Yasaman Bahri  Roman Novak  Samuel S Schoenholz  Jeffrey Pennington 
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165  2017.

[16] Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720  2019.

[17] Hao Li  Soham De  Zheng Xu  Christoph Studer  Hanan Samet  and Tom Goldstein. Training

Quantized Nets: A Deeper Understanding. NIPS  jun 2017.

[18] Xiaofan Lin  Cong Zhao  and Wei Pan. Towards accurate binary convolutional neural network.

In Advances in Neural Information Processing Systems  pages 345–353  2017.

[19] Wolfgang Maass and Pekka Orponen. On the Effect of Analog Noise in Discrete-Time Analog

Computations. Neural Computation  10(5):1071–1095  jul 1998.

[20] Alexander G de G Matthews  Mark Rowland  Jiri Hron  Richard E Turner  and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint
arXiv:1804.11271  2018.

[21] Asit Mishra  Eriko Nurvitadhi  Jeffrey J Cook  and Debbie Marr. Wrpn: wide reduced-precision

networks. arXiv preprint arXiv:1709.01134  2017.

[22] Daisuke Miyashita  Edward H Lee  and Boris Murmann. Convolutional neural networks using

logarithmic data representation. arXiv preprint arXiv:1603.01025  2016.

[23] Jeffrey Pennington  Samuel Schoenholz  and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems  pages 4785–4795  2017.

[24] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In Advances in neural
information processing systems  pages 3360–3368  2016.

[25] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European Conference on Computer
Vision  pages 525–542. Springer  2016.

[26] Samuel S Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep informa-

tion propagation. arXiv preprint arXiv:1611.01232  2016.

[27] Hava T. Siegelmann and Eduardo D. Sontag. Turing computability with neural nets. Applied

Mathematics Letters  4(6):77–80  jan 1991.

[28] Naigang Wang  Jungwook Choi  Daniel Brand  Chia-Yu Chen  and Kailash Gopalakrishnan.
In Advances in neural

Training deep neural networks with 8-bit ﬂoating point numbers.
information processing systems  pages 7675–7684  2018.

[29] Anqi Wu  Sebastian Nowozin  Edward Meeds  Richard E. Turner  Jose Miguel Hernandez-
Lobato  and Alexander L. Gaunt. Deterministic variational inference for robust bayesian neural
networks. In International Conference on Learning Representations  2019.

[30] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel S Schoenholz  and Jeffrey Pen-
nington. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10 000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393  2018.

[31] Ge Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In

Advances in neural information processing systems  pages 7103–7114  2017.

[32] Greg Yang  Jeffrey Pennington  Vinay Rao  Jascha Sohl-Dickstein  and Samuel S Schoenholz.

A mean ﬁeld theory of batch normalization. arXiv preprint arXiv:1902.08129  2019.

10

[33] Penghang Yin  Jiancheng Lyu  Shuai Zhang  Stanley Osher  Yingyong Qi  and Jack Xin.
Understanding straight-through estimator in training activation quantized neural nets. ICLR 
pages 1–30  2019.

[34] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

[35] Hongyi Zhang  Yann N Dauphin  and Tengyu Ma. Fixup initialization: Residual learning

without normalization. arXiv preprint arXiv:1901.09321  2019.

[36] Yiren Zhou  Seyed-Mohsen Moosavi-Dezfooli  Ngai-Man Cheung  and Pascal Frossard. Adap-
tive quantization for deep neural network. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence  2018.

11

,Yaniv Blumenfeld
Dar Gilboa
Daniel Soudry