2009,Modeling the spacing effect in sequential category learning,We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters  the mean and the variance  over time. We define conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect  we introduce a generic prior in the temporal updating stage to capture a learning preference  namely  less change for repetition and more change for variation. Finally  we show how this approach can be generalized to efficiently performmodel selection to decide whether observations are from one or multiple categories.,Modeling the spacing effect in sequential category

learning

Hongjing Lu

Department of Psychology & Statistics

Hongjing@ucla.edu

Matthew Weiden

Department of Psychology
mweiden@ucla.edu

Department of Statistics  Computer Science & Psychology

Alan Yuille

University of California  Los Angeles

Los Angeles  CA 90095

yuille@stat.ucla.edu

Abstract

We develop a Bayesian sequential model for category learning. The sequential
model updates two category parameters  the mean and the variance  over time. We
deﬁne conjugate temporal priors to enable closed form solutions to be obtained.
This model can be easily extended to supervised and unsupervised learning in-
volving multiple categories. To model the spacing effect  we introduce a generic
prior in the temporal updating stage to capture a learning preference  namely  less
change for repetition and more change for variation. Finally  we show how this ap-
proach can be generalized to efﬁciently perform model selection to decide whether
observations are from one or multiple categories.

1 Introduction

Inductive learning the process by which a new concept or category is acquired through observation
of exemplars - poses a fundamental theoretical problem for cognitive science. When exemplars are
encountered sequentially  as is typical in everyday learning  then learning is inﬂuenced in systematic
ways by presentation order. One pervasive phenomenon is the spacing effect  manifested in the
ﬁnding that given a ﬁxed amount of total study time with a given item  learning is facilitated when
presentations of the item are spread across a longer time interval rather than massed into a continuous
study period.
In category learning  for example  exemplars of two categories can be spaced by
presenting them in an interleaved manner (e.g.  A1B1A2B2A3B3)  or massed by presenting them
in consecutive blocks (e.g.  A1A2A3B1B2B3). Kornell & Bjork [1] show that when tested later on
classiﬁcation of novel category members  spaced presentation yields superior performance relative
to massed presentation. Similar spacing effects have been obtained in studies of item learning [2]
and motor learning [3]. Moreover  spacing effects are found not only in human learning  but also in
various types of learning in other species  including rats and Aplysia [4][5].

In the present paper we will focus on spacing effects in the context of sequential category learning.
Standard statistical methods based on summary information are unable to deal with order effects 
including the performance difference between spaced and massed conditions. From a computa-
tional perspective  a sequential learning model is needed to construct category representations from
training examples and dynamically update parameters of these representations from trial to trial.
Bayesian sequential models have been successfully applied to model causal learning and animal
conditioning [6] [7]. In the context of category learning  if we assume that the representation for
each category can be speciﬁed by a Gaussian distribution where the mean µ and the variance σ2 are
both random variables [8]  then the learning model must aim to compute the posterior distribution
of the parameters for each category given all the observations xt from trial 1 to trial t  P (µ  σ2|xt).

1

However  given that both the mean and the variance of a category are random variables  standard
Kalman ﬁltering [9] is not directly applicable in this case since it assumes a known variance  which
is not warranted in the current application.

In this paper  we extend traditional Kalman ﬁltering in order to update two category parameters  the
mean and the variance  over time in the context of category learning. We deﬁne conjugate tempo-
ral priors to enable closed form solutions to be obtained in this learning model with two unknown
parameters. We will illustrate how the learning model can be easily extended to learning situations
involving multiple categories either with supervision (i.e.  learners are informed of category mem-
bership for each training observation) or without supervision (i.e.  category membership of each
training observation is not provided to learners). Surprisingly  we can also derive closed form so-
lutions in the latter case. This reduces the need for employing particle ﬁlters as an approximation
to exact inference  commonly used in the case of unsupervised learning [10]. To model the spacing
effect  we introduce a generic prior in the temporal updating stage. Finally  we will show how this
approach can be generalized to efﬁciently perform model selection.

The organization of the present paper is as follows. In Section 2 we introduce the Bayesian se-
quential learning framework in the context of category learning  and discuss the conjugacy property
of the model. Section 3 and 4 demonstrate how to develop supervised and unsupervised learning
models  which can be compared with human performance. We draw general conclusions in section
5.

2 Bayesian sequential model

We adopt the framework of Bayesian sequential learning [11]  termed Bayes-Kalman  a probabilistic
model in which learning is assumed to be a Markov process with unobserved states. The exemplars
in training are directly observable  but the representations of categories are hidden and unobserv-
able. In this paper  we assume that categories can be represented as Gaussian distributions with two
unknown parameters  means and variances. These two unknown parameters need to be learned from
a limited number of exemplars (e.g.  less than ten exemplars).

We now state the general framework and give the update rule for the simplest situation where the
training data is generated by a single category speciﬁed by a mean m and precision r – the precision
is the inverse of the variance and is used to simplify the algebra. Our model assumes that the mean
can change over time and is denoted by mt  where t is the time step. The model is speciﬁed by the
prior distribution P (m0  r)  the likelihood function P (x|mt  r) for generating the observations  and
the temporal prior P (mt+1|mt) specifying how mt can vary over time. Note that the precision r is
estimated over time  which differs from standard Kalman ﬁltering where it is assumed to be known.
Bayes-Kalman [11] gives iterative equations to determine the posterior P (mt  r|Xt) after a sequence
of observations XT = {x1  ...  xt}. The update equations are divided into two stages  prediction and
correction:

P (mt+1  r|Xt) = Z ∞

−∞

dmtP (mt+1|mt)P (mt  r|Xt) 

P (mt+1  r|Xt+1) = P (mt+1  r|xt+1  Xt) =

P (xt+1|mt+1  r)P (mt+1  r|Xt)

P (xt+1|Xt)

.

(1)

(2)

Intuitively  the Bayes-Kalman ﬁrst predicts the distribution P (mt+1  r|Xt) and then uses this as a
prior to correct for the new observation xt+1 and determine the new posterior P (mt+1  r|Xt+1).
Note that the temporal prior P (mt+1|mt) implies that the model automatically pays most attention
to recent data and does not memorize the data  thus exhibiting sensitivity to the data ordering.

2.1 Conjugate priors

The distributions P (m0  r)  P (x|mt  r)  P (mt+1|mt) are chosen to be conjugate  so that the distri-
bution P (mt  r|Xt) takes the same functional form as P (m0  r). As shown in the following section 
this reduces the Bayes-Kalman equations to closed form update rules for the parameters of the dis-

2

tributions. The distributions are speciﬁed in terms of Gamma and Gaussian distributions:

g(r : α  β) =

G(x : µ  ρ) = {

βα
Γ(α)
ρ
2π

rα−1 exp{−βr}  r ≥ 0. Gamma.

} exp{−ρ/2(x − µ)2}. Gaussian.

We specify the prior P (m0  r) as the product of a Gaussian P (m0|r) and a Gamma P (r):

P (m0|r) = G(m0 : µ  τ r) 

P (r) = g(r : α  β) 

(3)

(4)

(5)

where µ  τ  α  β are the parameters of the distribution. For simplicity  we call this a Gamma-
Gaussian distribution with parameters µ  τ  α  β.
The likelihood function and temporal prior are both Gaussians:

P (xt|mt  r) = G(xt : mt  ζr) 

P (mt+1|mt) = G(mt+1 : mt  γr) 

(6)

where ζ  γ are constants.
The conjugacy of the distributions ensures that the posterior distribution P (mt  r|Xt) will also be
a Gamma-Gaussian distribution with parameters µt  τt  αt  βt  where the update rules for these pa-
rameters are speciﬁed in the next section.

2.2 Update rules for the model parameters

The update rules for the model parameters follow from substituting the distributions into the Bayes-
Kalman equations 1  2. We sketch how these update rules are obtained assuming that P (mt  r|Xt)
is a Gamma-Gaussian with parameters µt  τt  αt  βt  which is true for t = 0 using equations (5 6).
The form of the prediction equation and the temporal prior  see equations (1 6)  ensures that
P (mt+1  r|Xt) is also a Gamma-Gaussian distribution with parameters µt  τ p

t   αt  βt  where

τ p
t =

τtγ

τt + γ

.

(7)

The correction equation and the likelihood function 
P (mt+1  r|Xt+1) is also Gamma-Gaussian with parameters µt+1  τt+1  αt+1  βt+1 given by:

see equations

(2 6) 

ensure that

αt+1 = αt + 1/2 

βt+1 = βt +

µt+1 =

ζxt+1 + τ p
ζ + τ p
t

t µt

 

ζτ p

t (xt+1 − µt)2
2(ζ + τ p
t )

 

τt+1 = ζ + τ p
t .

(8)

Intuitively  the prediction only reduces the precision of m but makes no change to its mean or to the
distribution over r. By contrast  the new observation alters the mean of m (moving it closer to the
new observation xt+1)  and also increases its precision  which sharpens the distribution on r.

2.3 Model evidence

We also need to compute the probability of the observation sequence Xt from the model (which will
be used later for model selection). This can be expressed recursively as:

p(Xt) = p(xt|Xt−1)p(xt−1|Xt−2)...p(x1).

(9)

This computation is also simpliﬁed because we use conjugate distributions. The terms in equa-

tion (9) can be expressed as P (xt+1|Xt) = R dmt+1drP (xt+1|mt+1  r)P (mt+1  r|Xt) and these

integrals can be calculated analytically yielding:

P (xt+1|Xt) = (cid:8)βt +

ζτt

2(ζ + τt)

(x − µt)2(cid:9)−(αt+1/2)

{

1
2π

ζτt

ζ + τt

3

}1/2 βαt

t Γ(αt + 1/2)

Γ(αt)

.

(10)

3 Supervised category learning

Although the learning model is presented for one category  it can easily be extended to learning
multiple categories with known category membership for training data (i.e.  under supervision). In
this section  we will ﬁrst describe an experiment with two categories to show how the category
representations change over time; then we will simulate learning with six categories and compare
predictions with human data in psychological experiments.

3.1 Two-category learning with supervision

We ﬁrst conduct a synthetic experiment with two categories under supervision. We generate six
training observations from one of two one-dimensional Gaussian distributions (representing cate-
gories A and B  respectively) with means [−0.4  0.4] and standard deviation of 0.4. Two training
conditions are included  a massed condition with the data presentation order of AAABBB and a
spaced condition with the order of ABABAB.

To model the acquisition of category representations during training  we employ the Bayesian learn-
ing model as described in the previous section.
In the correction stage of each trial  the model
updates the parameters corresponding to the category that produced the observation based on the
supervision (i.e.  known category membership)  following equation (8).

In the prediction stage  however  different values of a ﬁxed model parameter γ are introduced to
incorporate a generic prior that controls how much the learner is willing to update category repre-
sentations from one trial to the next. The basic hypothesis is that learners will have greater con-
ﬁdence in knowledge of a category presented on trial t than of a category absent on trial t. As a
consequence  the learner will be willing to accept more change in a category representation if the
observation on the previous trial was drawn from a different category. This generic prior does share
some conceptual similarity with a model developed by Kording et. al [?]  which assumes that the
moment-moment variance of the states is higher for faster timescales (p. 779).

More speciﬁcally  if the observation on trial t is from the ﬁrst category  in the prediction phase we
will update the τt parameters for the two categories  τt

2  as:

1  τt

τt

1 7→

1γs

τt
1 + γs

τt

 

τt

2 7→

2γd
τt
2 + γd

τt

 

(11)

in which γs > γd. In the simulation  we used γs = 50 and γd = .5

Massed @ t = 2

t = 4

t = 6

Massed @ t = 2

t = 4

t = 6
 

)

m
P

(

)

m
P

(

)

m
P

(

−2

0
m

2

−2

Spaced @ t = 2

0
m
t = 4

2

−2

2

0
m
t = 6

Category 1
Category 2

)
r
(

P

0

20
r

)
r
(

P

40

0

Spaced @ t = 2

20
r

t = 4

)
r
(

P

40

0

40

20
r

t = 6

)

m
P

(

)

m
P

(

)

m
P

(

)
r
(

P

)
r
(

P

)
r
(

P

−2

0
m

2

−2

0
m

2

−2

2

0
m

0

20
r

40

0

 

20
r

40

0

40

20
r

Figure 1: Posterior distributions of means P (mt|Xt) and precisions P (rt|Xt) updated on training
trials in two-category supervised learning. Blue lines indicate category parameters in the ﬁrst cat-
egory; and red lines indicate parameters in the second category. The top panel shows the results
for the massed condition (i.e.  AAABBB)  and the bottom panel shows the results for the spaced
condition (i.e.  ABABAB). Please see in colour. We show the distributions only on even trials to
save space. See section 3.1.

Figure (1) shows the change of posterior distributions of the two unknown category parameters 
means P (mt|Xt) and precisions P (rt|Xt)  over training trials. Figure (2) shows the category rep-
resentation in the form of the posterior distribution of P (xt|Xt).
In the massed condition (i.e. 

4

AAABBB)  the variance of the ﬁrst category decreases over the ﬁrst three trials  and then increases
over the second three trials because the observations are from the second category. The increase of
category variance reﬂects the forgetting that occurs if no new observations are provided for a partic-
ular category after a long interval. This type of forgetting does not occur in the spaced condition  as
the interleaved presentation order ABABAB ensured that each category recurs after a short interval.

Based upon the learned category representations  we can compute accuracy (the ability to discrim-
inate between the two learnt distributions) using the posterior distributions of the two categories.
After 100 simulations  the average accuracy in the massed condition is 0.78  which is lower than the
0.84 accuracy in the spaced condition. Thus our model is able to predict the spacing effect found in
two-category supervised learning.

Massed @ t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

 

)
x
(
P

)
x
(
P

)
x
(
P

−2

0
x

2

−2

0
x

2

−2

Spaced @ t = 1

t = 2

)
x
(
P

2

−2

)
x
(
P

2

−2

0
x

t = 4

0
x

t = 3

0
x

t = 5

)
x
(
P

2

−2

2

0
x

t = 6

Category 1
Category 2

)
x
(
P

)
x
(
P

)
x
(
P

)
x
(
P

)
x
(
P

)
x
(
P

−2

0
x

2

−2

0
x

2

−2

0
x

2

−2

0
x

2

−2

0
x

2

−2

2

0
x

 

Figure 2: Posterior distribution of each category  P (xt|Xt)  updated on training trials in the two-
category supervised learning. Same conventions as in ﬁgure (1). See section 3.1.

3.2 Modeling the spacing effect in six-category learning

Kornell and Bjork [1] asked human subjects to study six paintings by six different artists  with a
given artists paintings presented consecutively (massed) or interleaved with other artists paintings
(spaced). In the training phase  subjects were informed which artist created each training painting.
The same 36 paintings were studied in the training phase  but with different presentation orders
in the massed and spaced conditions. In the subsequent test phase  six new paintings (one from
each artist) were presented and subjects had to identify which artist painted each of a series of new
paintings. Four test blocks were tested with random display order for artists. In each test block 
participants were given feedback after making an identiﬁcation response. Paintings presented in one
test block thus served as training examples for the subsequent test block. Human results are shown
in ﬁgure (4). Human subjects showed signiﬁcantly better test performance after spaced than massed
training. Given that feedback was provided and one painting from each artist appeared in one test
block  it is not surprising that test performance increased across test blocks and the spacing effect
decreased with more test blocks.

To simulate the data  we generated training and test data from six one-dimensional Gaussian distri-
butions with means [−2  −1.2  −0.4  0.4  1.2  2] and standard deviation of 0.4. Figure (3) shows the
learned category representations in terms of posterior distributions. Depending on the presentation
order of training data (massed or spaced)  the learned distributions differ in terms of means and vari-
ances for each category. To compare with human performance reported by Kornell and Bjork  the
model estimates accuracy in terms of discrimination between the two categories based upon learned
distributions. Figure (4) shows average accuracy from 1000 simulations. The result plot illustrates
that the model predictions match human performance well.

4 Unsupervised category learning

Both humans and animals can learn without supervision. For example  in the animal conditioning
literature  various studies have shown that exposing two stimuli in blocks (equivalent to a massed
condition) is less effective in producing generalization [12]. Balleine et. al. [4] found that with rats 
preexposure to two stimuli A and B (massed or spaced) determines the degree to which backward
blocking is subsequently obtained – backward blocking occurs if the preexposure is spaced but not

5

Massed @ t = 6
1

t = 12

t = 18

t = 24

t = 30

t = 36

 

)

X
P

(

0.5

)

X
P

(

)

X
P

(

)

X
P

(

)

X
P

(

)

X
P

(

0

−10

10

0
X

Spaced @ t = 6
1

X

X

X

X

X

t = 12

t = 18

t = 24

t = 30

t = 36

)

X
P

(

0.5

)

X
P

(

)

X
P

(

)

X
P

(

)

X
P

(

)

X
P

(

Category 1
Category 2
Category 3
Category 4
Category 5
Category 6

0

−10

10

0
X

X

X

X

X

X

 

Figure 3: Posterior distribution of each category  P (xt|Xt)  updated on training trials in the six-
category supervised learning. Same conventions as in ﬁgure (1). See section 3.2.

t
c
e
r
r
o
C
n
o

 

i
t
r
o
p
o
r
P

0.8

0.7

0.6

0.5

0.4

0.3

0.2

 

 

Massed
Spaced

1

2
3
Test Block

4

Figure 4: Human performance (left) and model prediction (right). Proportion correct as a function
of presentation training conditions (massed and spaced) and test block. See section 3.2.

if it is massed. They conclude that in the massed preexposure the rats are unable to distinguish
two separate categories for A and B  and therefore treat them as members of a single category. By
contrast  they conclude that rats can distinguish the categories A and B in the spaced preexposure.

In this section  we generalize the sequential category model to unsupervised learning  when the cate-
gory membership of each training example is not provided to observers. We ﬁrst derive the extension
of the sequential model to this case (surprisingly  showing we can obtain all results in closed form).
Then we determine whether massed and spaced stimuli (as in Balleine et. al.’s experiment [4]) are
most likely to have been generated by a single category or by two categories. We also assess the
importance of supervision in training by comparing performance after unsupervised learning with
that after supervised learning.

We consider a model with two hidden categories. Each category can be represented as a Gaussian
distribution with a mean and precision m1  r1 and m2  r2. The likelihood function assumes that the
data is generated by either category with equal probability  since the category membership is not
provided 

P (x|m1  r1  m2  r2) =

1
2

P (x|m1  r1) +

1
2

P (x|m2  r2) 

with P (x|m1  r1) = G(x : m1  ζr1)  P (x|m2  r2) = G(x : m2  ζr2).

We specify prior distributions and temporal priors as before:

P (m1

t+1|m1

P (m2
t+1|m2

0  r2) = G(m2
t ) = G(m2

0 : µ2  τ r2)
t   γr2).

t+1 : m2

P (m1
t ) = G(m1

0  r1) = G(m1
t+1 : m1

0 : µ1  τ r1) 
t   γr1) 

P (m2

(12)

(13)

(14)
(15)

t   r1  m2

t   r1  m2

The joint posterior distribution P (m1
t   r2|Xt) after observations Xt can be formally ob-
tained by applying the Bayes-Kalman update rules to the joint distribution – i.e.  replace (mt  r) by
t   r2) in equations (1 2)). But this update is more complicated because we do not know
(m1
whether the new observation xt should be assigned to category 1 or category 2. Instead we have to
sum over all the possible assignments of the observations to the categories which gives 2t possible
assignments at time t. This can be performed efﬁciently in a recursive manner. Let At denote the set
of possible assignments at time t where each assignment is a string (a1  ...  at) of binary variables

6

of length t  where (1  ...  1) is the assignment where all the observations are assigned to category 1 
(2  1  ...  1) assigns the ﬁrst observation to category 2 and the remainder to category 1  and so on.
By substituting equations (12 14 15) into Bayes-Kalman we can obtain an iterative update equation
for P (m1

t   r2|Xt). At time t we represent:

t   r1  m2

P (m1

t   r1  m2

t   r2|Xt) = X(a1 ... at)∈At

P (m1  r|~α1

a1 ... at)P (m2  r|~α2

(a1 ... at))P (a1  ...  at|Xt) 

(16)
where αi
(a1 ... at) denotes the values of the parameters ~α = (α  β  µ  τ ) for category i (i ∈ {1  2})
for observation sequence (a1  ...  at) and P (a1  ...  at) is the probability of assignment (a1  ...  at).
At t = 0 there is no observation sequence and P (m1
0  r2|Xt) = P (m1  r|~α1)P (m2  r|~α2)
which corresponds to A0 containing a single element which has probability one.
The prediction stage updates the τ component of ~αi(a1  ...  at) by:
γi(at)τ i(a1  ...  at)

0  r1  m2

τ i(a1  ...  at) 7→

γi(at) + τ i(a1  ...  at)

.

(17)

We deﬁne γi(at) as larger if i = at and smaller if i 6= at  as speciﬁed in equation (11) to incorporate
the generic prior described in section 3.1.

The correction stage at time t + 1 introduces another observation  which must be assigned to the
two categories. This gives a new set At+1 of 2t+1 assignments of form (a1  ...  at+1) and a new
posterior:

P (m1

t+1  r1  m2

t+1  r2|Xt+1) = X(a1 ... at+1)∈At+1

P (m1  r|~α1

a1 ... at+1)P (m2  r|~α2

(a1 ... at+1))P (a1  ...  at+1|Xt+1) 

where we compute ~αi

(a1 ... at+1) for i ∈ {1  2} by:

αi
(a1 ... at+1) = αi

(a1 ... at) + 1/2  β1

(a1 ... at+1) = β1

(a1 ... at) +

(18)

ζτ i

(a1 ... at)(xt+1 − µ1

(a1 ... at))2

2(ζ + τ i

(a1 ... at))

 

µi

(a1 ... at+1) =

ζxt+1 + τ i

(a1 ... at)µi

(a1 ... at)

ζ + τ 1

(a1 ... at)

 

(a1 ... at+1 = ζ + τ i
τ i

(a1 .... at)  (19)

and we compute P (a1  ...  at+1) by:

P (a1  ...  at+1|Xt+1) =

P (xt+1|~αat+1

(a1 ... at))P (a1  ...  at)

P(a1 ... at) P (xt+1|~αat+1

(a1 ... at))P (a1  ...  at)

 

(20)

where

P (xt+1|~αat+1

(a1 ... at)) = Z dmat+1 drat+1 P (xt+a|m(at+1)  rat+1 )P (m(at+1)  rat+1 |~α(a1 ... at))

(21)

The model selection can  as before  be expressed as P (xt|Xt−1)P (xt−1|Xt)....P (x1)  where

P (xt+1|Xt) = X(a1 ... at)∈At

P (xt+1|~αat+1

(a1 ... at))P (a1  ...  at).

(22)

We can now address the problem posed by Balleine et. al.’s preexposure experiments [4] – why
do rats identify a single category for the massed stimuli but two categories for the spaced stimuli?

7

We treat this as a model selection problem. We compare the evidence for the sequential model
with one category  see equations (9 10)  versus the evidence for the model with two categories  see
equations (9 22)  for the two cases AAABBB (massed) and ABABAB (spaced).

We use the same data as described in section (3.1) but without providing category membership for
any of the training data. The left plot in ﬁgure (5) shows the result obtained by comparing model
evidence for the one-category model with model evidence for the two-category model. A greater
ratio value indicates greater support for the one-category account. As shown in ﬁgure (5)  the model
decides that all training observations are from one category in the massed condition  but from two
different categories in the spaced condition (using zero as the decision threshold). These predictions
agree with with Balleine et. al.’s ﬁndings.

o
i
t
a
r
 
e
c
n
e
d
v
e
 
l
e
d
o
M

i

0

−0.5

−1

−1.5

−2

Massed

Conditions

Spaced

t
c
e
r
r
o
C
n
o

 

i
t
r
o
p
o
r
P

1

0.9

0.8

0.7

0.6

0.5

 

 

Massed
Spaced

Supervised

Unsupervised

Learning conditions

Figure 5: Model selection and accuracy results. Left  model selection results as a function of pre-
sentation training conditions (massed and spaced). A greater ratio indicates more support for the
one-category account. Error bars indicate the standard error from 100 simulations. See section 4.2.
Right  comparison of supervised and unsupervised learning in terms of accuracy. See section 4.3.
To assess the inﬂuence of supervision on learning  we compare performance between supervised
learning (described in section (3.1)) with unsupervised learning (described in this section). To make
the comparison  we assume that learners are provided with the same training data and are informed
that the data are from two different categories  either with known category membership (supervised)
or unknown category membership (unsupervised) for each training observation. Accuracy measured
by discrimination between the two categories is compared in the right plot of ﬁgure (5). The model
predicts higher accuracy given supervised than unsupervised learning. Furthermore  the model pre-
dicts a spacing effect for both types of learning  although the effect is reduced with unsupervised
learning.

5 Conclusions

In this paper  we develop a Bayesian sequential model for category learning by updating category
representations over time based on two category parameters  the mean and the variance. Analytic
updating rules are obtained by deﬁning conjugate temporal priors to enable closed form solutions.
A generic prior in the temporal updating stage is introduced to model the spacing effect. Parameter
estimation and model selection can be performed on the basis of updating rules. The current work
extends standard Kalman ﬁltering  and is able to predict learning phenomena that have been observed
for humans and other animals.

In addition to explaining the spacing effect  our model predicts that subjects will become less certain
about their knowledge of learned categories as time passes  see the increase in category variance in
Figure 2. But our model is not standard Kalman ﬁlter (since the measurement variance is unknown) 
so we do not predict exponential decay. Instead  as shown in Equation 10  our model predicts the
pattern of power-law forgetting that is fairly universal in human memory [14]

For small number of observations  our model is extremely efﬁcient because we can derive analytic
solutions. For example  the analytic solutions for unsupervised learning requires only 0.2 seconds
for six observations while numerical integration takes 18 minutes. However  our model will scale
exponentially with the number of observations in unsupervised learning. Future work is to include
a pruning strategy to keep the complexity practical.

Acknowledgement

This research was supported a grant from Air Force FA 9550-08-1-0489.

8

References

[1] Kornell  N.  & Bjork  R. A. (2008a). Learning concepts and categories:

induction”? Psychological Science  19  585-592.

Is spacing the ”enemy of

[2] Bahrick  H.P.  Bahrick  L.E.  Bahrick  A.S.  & Bahrick  P.E. (1993). Maintenance of foreign language

vocabulary and the spacing effect. Psychological Science  4  316321.

[3] Shea  J.B.  & Morgan  R.L. (1979). Contextual interference effects on the acquisition  retention  and
transfer of a motor skill. Journal of Experimental Psychology: Human Learning and Memory  5  179187.
[4] Balleine  B. W. Espinet  A. & Gonzalez  F. (2005).Perceptual learning enhances retrospective revalu-
ation of conditioned ﬂavor preferences in rats. Journal of Experimental Psychology: Animal Behavior
Processes  31(3): 341-50.

[5] Carew  T.J.  Pinsker  H.M.  & Kandel  E.R. (1972). Long-term habituation of a defensive withdrawal

reﬂex in Aplysia. Science  175  451454.

[6] Daw  N.  Courville  A. C  & Dayan  P. (2007). Semi-rational Models of Conditioning: The Case of Trial
Order. In M. Oaksford and N. Chater (Eds.). The probabilistic mind: Prospects for rational models of
cognition. Oxford: Oxford University Press.

[7] Dayan  P. & Kakade  S. (2000). Explaining away in weight space. In T. K. Leen et al.  (Eds.)  Advances

in neural information processing systems (Vol. 13  pp. 451-457). Cambridge  MA: MIT Press.

[8] Fried  L. S.  & Holyoak  K. J. (1984). Induction of category distributions: A framework for classiﬁcation

learning. Journal of Experimental Psychology: Learning  Memory and Cognition  10  234-257.

[9] Kalman  R. E. (1960). A new approach to linear ﬁltering and prediction problems. Transactions of the

ASME-Journal of Basic Engineering  82:35-45.

[10] Schubert  J.  & Sidenbladh  H. (2005). Sequential clustering with particle ﬁlters: Extimating the number

of clusters from data. 7th International Conference on Information Fusion (FUSION).

[11] Ho  Y-C & Lee  R.C.K. (1964). A Bayesian appraoch to problems in stochastic estimation and control.

IEEE Transactions on Automatic Control  9  333-339.

[12] Honey  R. C.  Bateson  P.  & Horn  G. (1994). The role of stimulus comparison in perceptual learning:
An investigation with the domestic chick. Quarterly Journal of Experimental Psychology: Comparative
and Physiological Psychology  47(B)  83103.

[13] Kording  K. P.  Tenenbaum  J. B.  and Shadmehr  R. (2007). The dynamics of memory as a consequence

of optimal adaptation to a changing body. Nature Neuroscience  10:779-786.

[14] Anderson  J. R. And Schooler  L. J. (1991). Reﬂections of the environment in memory. Psychological

Science  2  395-408.

9

,Hao Zheng
Faming Fang
Guixu Zhang