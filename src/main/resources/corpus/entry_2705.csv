2018,IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand  the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand  the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs  such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs  IntroVAE requires no extra discriminators  because the inference model itself serves as a discriminator to distinguish between the generated and real samples.  Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g.  CELEBA images at \(1024^{2}\))  which are comparable to or better than the state-of-the-art GANs.,IntroVAE: Introspective Variational Autoencoders for

Photographic Image Synthesis

Huaibo Huang  Zhihang Li  Ran He∗  Zhenan Sun  Tieniu Tan

1School of Artiﬁcial Intelligence  University of Chinese Academy of Sciences  Beijing  China

2Center for Research on Intelligent Perception and Computing  CASIA  Beijing  China

3National Laboratory of Pattern Recognition  CASIA  Beijing  China

4Center for Excellence in Brain Science and Intelligence Technology  CAS  Beijing  China

huaibo.huang@cripac.ia.ac.cn

{zhihang.li  rhe  znsun  tnt}@nlpr.ia.ac.cn

Abstract

We present a novel introspective variational autoencoder (IntroVAE) model for
synthesizing high-resolution photographic images. IntroVAE is capable of self-
evaluating the quality of its generated samples and improving itself accordingly.
Its inference and generator models are jointly trained in an introspective way. On
one hand  the generator is required to reconstruct the input images from the noisy
outputs of the inference model as normal VAEs. On the other hand  the inference
model is encouraged to classify between the generated and real samples while the
generator tries to fool it as GANs. These two famous generative frameworks are
integrated in a simple yet efﬁcient single-stream architecture that can be trained in
a single stage. IntroVAE preserves the advantages of VAEs  such as stable training
and nice latent manifold. Unlike most other hybrid models of VAEs and GANs 
IntroVAE requires no extra discriminators  because the inference model itself
serves as a discriminator to distinguish between the generated and real samples.
Experiments demonstrate that our method produces high-resolution photo-realistic
images (e.g.  CELEBA images at 10242)  which are comparable to or better than
the state-of-the-art GANs.

1

Introduction

In the recent years  many types of generative models such as autoregressive models [38  37]  vari-
ational autoencoders (VAEs) [20  32]  generative adversarial networks (GANs) [13]  real-valued
non-volume preserving (real NVP) transformations [7] and generative moment matching networks
(GMMNs) [24] have been proposed and widely studied. They have achieved remarkable success
in various tasks  such as unconditional or conditional image synthesis [22  27]  image-to-image
translation [25  46]  image restoration [5  17] and speech synthesis [12]. While each model has its
own signiﬁcant strengths and limitations  the two most prominent models are VAEs and GANs. VAEs
are theoretically elegant and easy to train. They have nice manifold representations but produce very
blurry images that lack details. GANs usually generate much sharper images but face challenges in
training stability and sampling diversity  especially when synthesizing high-resolution images.
Many techniques have been developed to address these challenges. LAPGAN [6] and StackGAN [41]
train a stack of GANs within a Laplacian pyramid to generate high-resolution images in a coarse-
to-ﬁne manner. StackGAN-v2 [42] and HDGAN [43] adopt multi-scale discriminators in a tree-
like structure. Some studies [11  39] have trained a single generator with multiple discriminators

∗Ran He is the corresponding author.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

to improve the image quality. PGGAN [18] achieves the state-of-the-art by training symmetric
generators and discriminators progressively. As illustrated in Fig. 1(a) (A  B  C  and D show the
above GANs respectively)  most existing GANs require multi-scale discriminators to decompose
high-resolution tasks to from-low-to-high resolution tasks  which increases the training complexity.
In addition  much effort has been devoted to combining the strengths of VAEs and GANs via hybrid
models. VAE/GAN [23] imposes a discriminator on the data space to improve the quality of the
results generated by VAEs. AAE [28] discriminates in the latent space to match the posterior to the
prior distribution. ALI [10] and BiGAN [8] discriminate jointly in the data and latent space  while
VEEGAN [35] uses additional constraints in the latent space. However  hybrid models usually have
more complex network architectures (as illustrated in Fig. 1(b)  A  B  C  and D show the above hybrid
models respectively) and still lag behind GANs in image quality [18].
To alleviate this problem  we introduce an introspective variational autoencoder (IntroVAE)  a simple
yet efﬁcient approach to training VAEs for photographic image synthesis. One of the reasons why
samples from VAEs tend to be blurry could be that the training principle makes VAEs assign a
high probability to training points  which cannot ensure that blurry points are assigned to a low
probability [14]. Motivated by this issue  we train VAEs in an introspective manner such that the
model can self-estimate the differences between generated and real images. In the training phase 
the inference model attempts to minimize the divergence of the approximate posterior with the prior
for real data while maximize it for the generated samples; the generator model attempts to mislead
the inference model by minimizing the divergence of the generated samples. The model acts like
a standard VAE for real data and acts like a GAN when handling generated samples. Compared to
most VAE and GAN hybrid models  our version requires no extra discriminators  which reduces
the complexity of the model. Another advantage of the proposed method is that it can generate
high-resolution realistic images through a single-stream network in a single stage. The divergence
object is adversarially optimized along with the reconstruction error  which increases the difﬁculty of
distinguishing between the generated and real images for the inference model  even for those with
high-resolution. This arrangement greatly improves the stability of the adversarial training. The
reason could be that the instability of GANs is often due to the fact that the discriminator distinguishes
the generated images from the training images too easily [18  30].
Our contribution is three-fold. i) We propose a new training technique for VAEs  that trains VAEs in
an introspective manner such that the model itself estimates the differences between the generated
and real images without extra discriminators. ii) We propose a single-stream single-stage adversarial
model for high-resolution photographic image synthesis  which is  to our knowledge  the ﬁrst feasible
method for GANs to generate high-resolution images in such a simple yet efﬁcient manner  e.g. 
CELEBA images at 10242. iii) Experiments demonstrate that our method combines the strengths of
GANs and VAEs  producing high-resolution photographic images comparable to those produced by
the state-of-the-art GANs while preserving the advantages of VAEs  such as stable training and nice
latent manifold.

2 Background

As our work is a speciﬁc hybrid model of VAEs and GANs  we start with a brief review of VAEs 
GANs and their hybrid models.
Variational Autoencoders (VAEs) consist of two networks: a generative network (Generator)
pθ(x|z) that samples the visible variables x given the latent variables z and an approximate in-
ference network (Encoder) qφ(z|x) that maps the visible variables x to the latent variables z which
approximate a prior p(z). The object of VAEs is to maximize the variational lower bound (or evidence
lower bound  ELBO) of pθ(x):

logpθ(x) ≥ Eqφ(z|x) log pθ(x|z) − DKL(qφ(z|x)||p(z)).

(1)

The main limitation of VAEs is that the generated samples tend to be blurry  which is often attributed
to the limited expressiveness of the inference models  the injected noise and imperfect element-wise
criteria such as the squared error [23  45]. Although recent studies [4  9  21  34  45] have greatly
improved the predicted log-likelihood  they still face challenges in generating high-resolution images.
Generative Adversarial Networks (GANs) employ a two-player min-max game with two models:
the generative model (Generator) G produces samples G(z) from the prior p(z) to confuse the

2

(a) Several GANs

(b) Hybrid models

Figure 1: Overviews of several typical GANs for high-resolution image generation and hybrid models
of VAEs and GANs.

discriminator D(x)  while D(x) is trained to distinguish between the generated samples and the
given training data. The training object is

min

G

max

D

Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 − D(G(z)))].

(2)

GANs are promising tools for generating sharp images  but they are difﬁcult to train. The training
process is usually unstable and is prone to mode collapse  especially when generating high-resolution
images. Many methods [44  1  2  15  33] have been attempted to improve GANs in terms of training
stability and sample variation. To synthesize high-resolution images  several studies have trained
GANs in a Laplacian pyramid [6  41] or a tree-like structure [42  43] with multi-scale discrimina-
tors [11  29  39]  mostly in a coarse-to-ﬁne manner  including the state-of-the-art PGGAN [18].
Hybrid Models of VAEs and GANs usually consist of three components: an encoder and a decoder 
as in autoencoders (AEs) or VAEs  to map between the latent space and the data space  and an extra
discriminator to add an adversarial constraint into the latent space [28]  data space [23]  or their
joint space [8  10  35]. Recently  Ulyanov et al. [36] propose adversarial generator-encoder networks
(AGE) that shares some similarity with ours in the architecture of two components  while the two
models differ in many ways  such as the design of the inference models  the training objects  and the
divergence computations. Brock et al. [3] also propose an introspective adversarial network (IAN)
that the encoder and discriminator share most of the layers except the last layer  and their adversarial
loss is a variation of the standard GAN loss. In addition  existing hybrid models  including AGE and
IAN  still lag far behind GANs in generating high-resolution images  which is one of the focuses of
our method.

3 Approach

In this section  we train VAEs in an introspective manner such that the model can self-estimate the
differences between the generated samples and the training data and then updates itself to produce
more realistic samples. To achieve this goal  one part of the model needs to discriminate the generated
samples from the training data  and another part should mislead the former part  analogous to the
generator and discriminator in GANs. Speciﬁcally  we select the approximate inference model (or
encoder) of VAEs as the discriminator of GANs and the generator model of VAEs as the generator of
GANs. In addition to performing adversarial learning like GANs  the inference and generator models
are also expected to train jointly for the given training data to preserve the advantages of VAEs.
There are two components in the ELBO objective of VAEs  a log-likelihood (autoencoding) term
LAE and a prior regularization term LREG  which are listed below in the negative version:

LAE = −Eqφ(z|x) log pθ(x|z) 

(3)

LREG = DKL(qφ(z|x)||p(z)).

(4)
The ﬁrst term LAE is the reconstruction error in a probabilistic autoencoder  and the second term
LREG regularizes the encoder by encouraging the approximate posterior qφ(z|x) to match the prior
p(z). In the following  we describe the proposed introspective VAE (IntroVAE) with the modiﬁed
combination objective of these two terms.

3

D1D2zBx1x2x1D1G1x2D2G2zAD1D2D3xGzCxzDzˆ EDGRecxrealxgenAdvKLAzˆEDGRecxrealxgenAdvzBzˆEDxrealAdvzGxgenCzˆEDxrealAdvzGxgenReczˆˆD3.1 Adversarial distribution matching

To match the distribution of the generated samples with the true distribution of the given training
data  we use the regularization term LREG as the adversarial training cost function. The inference
model is trained to minimize LREG to encourage the posterior qφ(z|x) of the real data x to match
the prior p(z)  and simultaneously to maximize LREG to encourage the posterior qφ(z|G(z(cid:48))) of the
generated samples G(z(cid:48)) to deviate from the prior p(z)  where z(cid:48) is sampled from p(z). Conversely 
the generator G is trained to produce samples G(z(cid:48)) that have a small LREG  such that the samples’
posterior distribution approximately matches the prior distribution.
Given a data sample x and a generated sample G(z)  we design two different losses  one to train the
inference model E  and another to train the generator G:

LE(x  z) = E(x) + [m − E(G(z))]+ 

(5)

LG(z) = E(G(z)) 

ty V (E  G) = (cid:82)
U (E  G) =(cid:82)

(6)
where E(x) = DKL(qφ(z|x)||p(z))  [·]+ = max(0 ·)  and m is a positive margin. The above
two equations form a min-max game between the inference model E and the generator G when
E(G(z)) ≤ m  i.e.  minimizing LG for the generator G is equal to maximizing the second term of
LE for the inference model E.∗
Following the original GANs [14]  we train the inference model E to minimize the quanti-
x z LE(x  z)pdata(x)pz(z)dxdz  and the generator G to minimize the quantity
z LG(z)pz(z)dz. In a non-parametric setting  i.e.  E and G are assumed to have inﬁnite
capacity  the following theorem shows that when the system reaches a Nash equilibrium (a saddle
point) (E∗  G∗)  the generator G∗ produces samples that are distinguishable from the given training
distribution  i.e.  pG∗ = pdata.
Theorem 1. Assuming that no region exists where pdata(x) = 0  (E∗  G∗) forms a saddle point of
the above system if and only if (a) pG∗ = pdata and (b) E∗(x) = γ  where γ ∈ [0  m] is a constant.
Proof. See Appendix A.
Relationships with other GANs To some degree  the proposed adversarial method appears to be
similar to Energy-based GANs (EBGAN) [44]  which views the discriminator as an energy function
that assigns low energies to the regions of high data density and higher energies to the other regions.
The proposed KL-divergence function can be considered as a speciﬁc type of energy function
that is computed by the inference model instead of an extra auto-encoder discriminator [44]. The
architecture of our system is simpler and the KL-divergence shows more promising properties than
the reconstruction error [44]  such as stable training for high-resolution images.

3.2

Introspective variational inference

As demonstrated in the previous subsection  playing a min-max game between the inference model E
and the generator G is a promising method for the model to align the generated and true distributions
and thus produce visual-realistic samples. However  training the model in this adversarial manner
could still cause problems such as mode collapse and training instability  like in other GANs. As
discussed above  we introduce IntroVAE to alleviate these problems by combining GANs with VAEs
in an introspective manner.
The solution is surprisingly simple  and we only need to combine the adversarial object in Eq. (5)
and Eq. (6) with the ELBO object of VAEs. The training objects for the inference model E and the
generator G can be reformulated as below:

LE(x  z) = E(x) + [m − E(G(z))]+ + LAE(x) 

LG(z) = E(G(z)) + LAE(x).

The addition of the reconstruction error LAE builds a bridge between the inference model E and the
generator G and results in a speciﬁc hybrid models of VAEs and GANs. For a data sample x from the
∗It should be noted that we use E to denote the inference model and E(x) to denote the kl-divergence

function for representation convenience.

4

(7)

(8)

training set  the object of the proposed method collapses to the standard ELBO object of VAEs  thus
preserving the properties of VAEs; for a generated sample G(z)  this object generates a min-max
game of GANs between E and G and makes G(z) more realistic.
Relationships with other hybrid models Compared to other hybrid models [28  23  8  10  35] of
VAEs and GANs  which always use a discriminator to regularize the latent code and generated data
individually or jointly  the proposed method adds prior regularization into both the latent space and
data space in an introspective manner. The ﬁrst term in Eq. (7) (i.e.  LREG in Eq. (4)) encourages
the latent code of the training data to approximately follow the prior distribution. The adversarial part
of Eq. (7) and Eq. (8) encourages the generated samples to have the same distribution as the training
data. The inference model E and the generator G are trained both jointly and adversarially without
extra discriminators.
Compared to AGE [36]  the major differences are addressed in three-fold. 1) AGE is designed in
an autoencoder-type where the encoder has one output variable and no noise term is injected when
reconstructing the input data. The proposed method follows the original VAEs that the inference model
has two output variables  i.e.  µ and σ  to utilize the reparameterization trick  i.e.  z = µ + σ(cid:12)  where
 ∼ N (0  I). 2) AGE uses different reconstruction errors to regularize the encoder and generator
respectively  while the proposed method uses the reconstruction error LAE to regularize both the
encoder and generator. 3) AGE computes the KL-divergence using batch-level statistics  i.e.  mj and
sj in Eq. (7) in [36]  while we compute it using the two batch-independent outputs of the inference
model  i.e.  µ and σ in Eq. (9). For high-resolution image synthesis  the training batch-size is usually
limited to be very small  which may harm the performance of AGE but has little inﬂuence on ours.
As AGE is trained on 64 × 64 images  we re-train AGE and ﬁnd it hard to converge on 256 × 256
images; there is no improvement even when replacing AGE’s network with ours.

3.3 Training IntroVAE networks

Following the original VAEs [20]  we select the centered isotropic multivariate Gaussian N (0  I) as
the prior p(z) over the latent variables. As illustrated in Fig. 2  the inference model E is designed to
output two individual variables  µ and σ  and thus the posterior qφ(z|x) = N (z; µ  σ2). The input
z of the generator G is sampled from N (z; µ  σ2) using a reparameterization trick: z = µ + σ (cid:12) 
where  ∼ N (0  I). In this setting  the KL-divergence LREG (i.e.  E(x) in Eq. (7) and Eq. (8)) 
given N data samples  can be computed as below:

LREG(z; µ  σ) =

1
2

(1 + log(σ2

ij) − µ2

ij − σ2
ij) 

(9)

where Mz is the dimension of the latent code z.
For the reconstruction error LAE in Eq. (7) and Eq. (8)  we choose the commonly-used pixel-wise
mean squared error (MSE) function. Let xr be the reconstruction sample  LAE is deﬁned as:

N(cid:88)

Mz(cid:88)

i=1

j=1

N(cid:88)

Mx(cid:88)

i=1

j=1

LAE(x  xr) =

1
2

(cid:107)xr ij − xij(cid:107)2
F  

(10)

where Mx is the dimension of the data x.
Similar to VAE/GAN [23]  we train IntroVAE to discriminate real samples from both the model
samples and reconstructions. As shown in Fig. 2  these two types of samples are the reconstruction
samples xr and the new samples xp. When the KL-divergence object of VAEs is adequately optimized 
the posterior qφ(z|x) matches the prior p(z) approximately and the samples are similar to each other.
The combined use of samples from p(z) and qφ(z|x) is expected to provide a more useful signal for
the model to learn more expressive latent code and synthesize more realistic samples. The total loss
functions for E and G are respectively redeﬁned as:

LE = LREG(z) + α

[m − LREG(zs)]+ + βLAE(x  xr)

s=r p

= LREG(Enc(x)) + α

[m − LREG(Enc(ng(xs)))]+ + βLAE(x  xr) 

(11)

(cid:88)

(cid:88)

s=r p

5

Figure 2: The architecture and training ﬂow of IntroVAE. The left part shows that the model consists
of two components  the inference model E and the generator G  in a circulation loop. The right part
is the unrolled training ﬂow of the proposed method.

Algorithm 1 Training IntroVAE model
1: θG  φE ← Initialize network parameters
2: while not converged do
X ← Random mini-batch from dataset
3:
Z ← Enc(X)
4:
Zp ← Samples from prior N (0  I)
5:
Xr ← Dec(Z)  Xp ← Dec(Zp)
6:
LAE ← LAE(Xr  X)
7:
Zr ← Enc(ng(Xr))  Zpp ← Enc(ng(Xp))
8:
adv ← LREG(Z) + α{[m − LREG(Zr)]+ + [m − LREG(Zpp)]+}
LE
9:
φE ← φE − η∇φE (LE
10:
adv + βLAE)
Zr ← Enc(Xr)  Zpp ← Enc(Xp)
11:
adv ← α{LREG(Zr) + LREG(Zpp)}
LG
12:
θG ← θG − η∇θG(LG
13:
14: end while

adv + βLAE)

(cid:46) Perform Adam updates for φE

(cid:46) Perform Adam updates for θG

(cid:88)

LG = α

LREG(Enc(xs)) + βLAE(x  xr) 

(12)

s=r p

where ng(·) indicates that the back propagation of the gradients is stopped at this point  Enc(·)
represents the mapping function of E  and α and β are weighting parameters used to balance the
importance of each item.
The networks of E and G are designed in a similar manner to other GANs [31  18]  except that E
has two output variables with respect to µ and σ. As shown in Algorithm 1  E and G are trained
iteratively by updating E using LE to distinguish the real data X and generated samples  Xr and Xp 
and then updating G using LG to generate samples that are increasingly similar to the real data; these
steps are repeated until convergence.

4 Experiments

In this section  we conduct a set of experiments to evaluate the performance of the proposed method.
We ﬁrst give an introduction of the experimental implementations  and then discuss in detail the
image quality  training stability and sample diversity of our method. Besides  we also investigate the
learned manifold via interpolation in the latent space.

4.1

Implementations

Dataset We condider three data sets  namely CelebA [26]   CelebA-HQ [18] and LSUN BED-
ROOM [40]. The CelebA dataset consists of 202 599 celebrity images with large variations in facial
attributes. Following the standard protocol of CelebA  we use 162 770 images for training  19 867
for validation and 19 962 for testing. The CelebA-HQ dataset is a high-quality version of CelebA
that consists of 30 000 images at 1024 × 1024 resolution. The dataset is split into two sets: the ﬁrst

6

EGXZxEμσGzxrEμrσrGZp~N(0 I)xpEμpσpε~N(0 I)LREG(z)LREG(zr)LREG(zp)LAE29 000 images as the training set and the rest 1 000 images as the testing set. We take the testing set
to evaluate the reconstruction quality. The LSUN BEDROOM is a subset of the Large-scale Scene
Understanding (LSUN) dataset [40]. We adopt its whole training set of 3 033 042 images in our
experiments.
Network architecture We design the inference and generator models of IntroVAE in a similar way
to the discriminator and generator in PGGAN except of the use of residual blocks to accelerate the
training convergence (see Appendix B for more details). Like other VAEs  the inference model has
two output vectors  respectively representing the mean µ and the covariance σ2 in Eq. (9). For the
images at 1024 × 1024  the dimension of the the latent code is set to be 512 and the hyperparameters
in Eq. (11) and Eq. (12) are set empirically to hold the training balance of the inference and generator
models: m = 90   α = 0.25 and β = 0.0025. For the images at 256 × 256  the latent dimension
is 512  m = 120   α = 0.25 and β = 0.05. For the images at 128 × 128  the latent dimension is
256  m = 110   α = 0.25 and β = 0.5. The key is to hold the regularization term LREG in Eq. (11)
and Eq. (12) below the margin value m for most of the time. It is suggested to pre-train the model
with 1 ∼ 2 epochs in the original VAEs form (i.e.  α = 0) to ﬁnd the appropriate conﬁguration of the
hyper-parameters for different image sizes. More analyses and results for different hyper-parameters
are provided in Appendix D.
As illustrated in Algorithm 1  the inference and generator models are trained iteratively using Adam
algorithm [19] (β1 = 0.9  β2 = 0.999) with a batch size of 8 and a ﬁxed learning rate of 0.0002. An
additional illustration of the training ﬂow is provided in Appendix C.

4.2 High quality image synthesis
As shown in Fig. 3  our method produces visually appealing high-resolution images of 1024 × 1024
resolution both in reconstruction and sampling. The images in Fig. 3(c) are the reconstruction results
of the original images in Fig. 3(a) from the CelebA-HQ testing set. Due to the training principle
of VAEs that injects random noise in the training phase  the reconstruction images cannot keep
accurate pixel-wise similarity with the original images. In spite of this  our results preserve the most
global topology information of the input images while achieve photographic high-quality in visual
perception.
We also compare our sampling results against PGGAN [18]  the state-of-the-art in synthesizing
high-resolution images. As illustrated in Fig. 3(d)  our method is able to synthesize high-resolution
high-quality samples comparable with PGGAN  which are both distinguishable with the real images.
While PGGAN is trained with symmetric generators and discriminators in a progressive multi-stage
manner  our model is trained in a much simpler manner that iteratively trains a single inference
model and a single generator in a single stage like the original GANs [13]. The results of our method
demonstrate that it is possible to synthesize very high-resolution images by training directly with
high-resolution images without decomposing the single task to multiple from-low-to-high resolution
tasks. Additionally  we provide the visual quality results in LSUN BEDROOM in Fig. 4  which
further demonstrate that our method is capable to synthesize high quality images that are comparable
with PGGAN’s. (More visual results on extra datasets are provided in Appendix F & G.)

4.3 Training stability and speed

Figure 5 illustrates the quality of the samples with regard to the loss functions of the reconstruction
error LAE and the KL-divergences. It can be seen that the losses converge very fast to a stable
stage in which their values ﬂuctuate slightly around a balance line. As described in Theorem 1  the
prediction E(x) of the inference model reaches a constant γ in [0  m]. This is consistent with the
curves in Fig. 4  that when approximately converged  the KL-divergence of real images is around a
constant value lower than m while those of the reconstruction and sample images ﬂuctuate around m.
Besides  the image quality of the samples improves stably along with the training process.
We evaluate the training speed on CelebA images of various resolutions  i.e.  128 × 128  256 × 256 
512 × 512 and 1024 × 1024. As illustrated in Tab. 1  The convergence time increases along with the
resolution since the hardware limits the minibatch size for high-resolutions.

7

(a) Original

(b) PGGAN [18]

(c) Ours-Reconstructions

(d) Ours-Samples

Figure 3: Qualitative results of 1024 × 1024 images. (a) and (c) are the original and reconstruction
images from the testing split  respectively. (b) and (d) are sample images of PGGAN (copied from
the cited paper [18]) and our method  respectively. Best viewed by zooming in the electronic version.

(a) WGAN-GP [15](128 × 128)

(b) PGGAN [18](256 × 256)

(c) Ours(256 × 256)

Figure 4: Qualitative comparison in LSUN BEDROOM. The images in (a) and (b) are copied from
the cited papers [15  18]

4.4 Diversity analysis

We take two metrics to evaluate the sample diversity of our method  namely multi-scale structural
similarity (MS-SSIM) [30] and Fréchet Inception Distance (FID) [16]. The MS-SSIM measures
the similarity of two images and FID measures the Fréchet distance of two distributions in feature
space. For fair comparison with PGGAN  the MS-SSIM scores are computed among an average of

Table 1: Training speed w.r.t. the image resolutions.

128 × 128

256 × 256

512 × 512

1024 × 1024

Resolution
Minibatch
Time (days)

64
0.5

12
7

8
21

32
1

8

Figure 5: Illustration of the training process.

Table 2: Quantitative comparison with two metrics: MS-SSIM and FID.

Method

WGAN-GP [15]

PGGAN [18]

Ours

MS-SSIM

FID

CELEBA LSUN BEDROOM CELEBA-HQ LSUN BEDROOM
0.2854
0.2828
0.2719

0.0587
0.0636
0.0532

7.30
5.19

-

-

8.34
8.84

10K pairs of synthesize images at 128 × 128 for CelebA and LSUN BEDROOM  respectively. FID is
computed from 50K images at 1024 × 1024 for CelebA-HQ and from 50K images at 256 × 256 for
LSUN BEDROOM. As illustrated in Tab. 2  our method achieves comparable or better quantitative
performance than PGGAN  which reﬂects the sample diversity to some degree. More visual results
are provided in Appendix H to further demonstrate the diversity.

4.5 Latent manifold analysis

We conduct interpolations of real images in the latent space to estimate the manifold continuity.
For a pair of real images  we ﬁrst map them to latent codes z using the inference model and then
make linear interpolations between the codes. As illustrated in Fig. 6  our model demonstrates
continuity in the latent space in interpolating from a male to a female or rotating a proﬁle face. This
manifold continuity veriﬁes that the proposed model generalizes the image contents instead of simply
memorizing them.

Figure 6:
Interpolations of real images in the latent space. The leftmost and rightmost are real
images in CelebA-HQ testing set and the images immediately next to them are their reconstructions
via our model. The rest are the interpolations. The images are compressed to save space.

5 Conclusion

We have introduced introspective VAEs  a novel and simple approach to training VAEs for synthesiz-
ing high-resolution photographic images. The learning objective is to play a min-max game between
the inference and generator models of VAEs. The inference model not only learns a nice latent
manifold structure  but also acts as a discriminator to maximize the divergence of the approximate
posterior with the prior for the generated data. Thus  the proposed IntroVAE has an introspection
capability to self-estimate the quality of the generated images and improve itself accordingly. Com-
pared to other state-of-the-art methods  the proposed model is simpler and more efﬁcient with a
single-stream network in a single stage  and it can synthesize high-resolution photographic images
via a stable training process. Since our model has a standard VAE architecture  it may be easily
extended to various VAEs-related tasks  such as conditional image synthesis.

9

01234567x 105050100150200250300350Global StepLoss LAEEREGrealEREGrecEREGsamGREGrecGREGsamAcknowledgments

This work is partially funded by the State Key Development Program (Grant No. 2016YFB1001001)
and National Natural Science Foundation of China (Grant No. 61622310  61427811).

References
[1] Arjovsky  Martin  Chintala  Soumith  and Bottou  Léon. Wasserstein GAN. arXiv preprint arX-

iv:1701.07875  2017.

[2] Berthelot  David  Schumm  Tom  and Metz  Luke. BEGAN: Boundary equilibrium generative adversarial

networks. arXiv preprint arXiv:1703.10717  2017.

[3] Brock  Andrew  Lim  Theodore  Ritchie  James M  and Weston  Nick. Neural photo editing with introspec-

tive adversarial networks. In ICLR  2017.

[4] Chen  Xi  Kingma  Diederik P  Salimans  Tim  Duan  Yan  Dhariwal  Prafulla  Schulman  John  Sutskever 

Ilya  and Abbeel  Pieter. Variational lossy autoencoder. In ICLR  2017.

[5] Dahl  Ryan  Norouzi  Mohammad  and Shlens  Jonathon. Pixel recursive super resolution. In ICCV  2017.

[6] Denton  Emily L  Chintala  Soumith  Fergus  Rob  et al. Deep generative image models using a laplacian

pyramid of adversarial networks. In NeurIPS  pp. 1486–1494  2015.

[7] Dinh  Laurent  Sohl-Dickstein  Jascha  and Bengio  Samy. Density estimation using real NVP. In ICLR 

2017.

[8] Donahue  Jeff  Krähenbühl  Philipp  and Darrell  Trevor. Adversarial feature learning. In ICLR  2017.

[9] Dosovitskiy  Alexey and Brox  Thomas. Generating images with perceptual similarity metrics based on

deep networks. In NeurIPS  pp. 658–666  2016.

[10] Dumoulin  Vincent  Belghazi  Ishmael  Poole  Ben  Mastropietro  Olivier  Lamb  Alex  Arjovsky  Martin 

and Courville  Aaron. Adversarially learned inference. In ICLR  2017.

[11] Durugkar  Ishan  Gemp  Ian  and Mahadevan  Sridhar. Generative multi-adversarial networks. In ICLR 

2017.

[12] Gibiansky  Andrew  Arik  Sercan  Diamos  Gregory  Miller  John  Peng  Kainan  Ping  Wei  Raiman 
Jonathan  and Zhou  Yanqi. Deep voice 2: Multi-speaker neural text-to-speech. In NeurIPS  pp. 2966–2974 
2017.

[13] Goodfellow  Ian  Pouget-Abadie  Jean  Mirza  Mehdi  Xu  Bing  Warde-Farley  David  Ozair  Sherjil 

Courville  Aaron  and Bengio  Yoshua. Generative adversarial nets. In NeurIPS  pp. 2672–2680  2014.

[14] Goodfellow  Ian  Bengio  Yoshua  Courville  Aaron  and Bengio  Yoshua. Deep learning  volume 1. MIT

press Cambridge  2016.

[15] Gulrajani  Ishaan  Ahmed  Faruk  Arjovsky  Martin  Dumoulin  Vincent  and Courville  Aaron C. Improved

training of wasserstein GANs. In NeurIPS  pp. 5769–5779  2017.

[16] Heusel  Martin  Ramsauer  Hubert  Unterthiner  Thomas  Nessler  Bernhard  and Hochreiter  Sepp. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS  pp. 6626–6637 
2017.

[17] Huang  Huaibo  He  Ran  Sun  Zhenan  and Tan  Tieniu. Wavelet-srnet: A wavelet-based cnn for multi-scale

face super resolution. In ICCV  pp. 1689–1697  2017.

[18] Karras  Tero  Aila  Timo  Laine  Samuli  and Lehtinen  Jaakko. Progressive growing of GANs for improved

quality  stability  and variation. In ICLR  2018.

[19] Kingma  Diederik and Ba  Jimmy. Adam: A method for stochastic optimization. In ICLR  2014.

[20] Kingma  Diederik P and Welling  Max. Auto-encoding variational bayes. In ICLR  2014.

[21] Kingma  Diederik P  Salimans  Tim  Jozefowicz  Rafal  Chen  Xi  Sutskever  Ilya  and Welling  Max.

Improved variational inference with inverse autoregressive ﬂow. In NeurIPS  pp. 4743–4751  2016.

10

[22] Lample  Guillaume  Zeghidour  Neil  Usunier  Nicolas  Bordes  Antoine  Denoyer  Ludovic  et al. Fader

networks: Manipulating images by sliding attributes. In NeurIPS  pp. 5969–5978  2017.

[23] Larsen  Anders Boesen Lindbo  Sønderby  Søren Kaae  Larochelle  Hugo  and Winther  Ole. Autoencoding

beyond pixels using a learned similarity metric. In ICML  pp. 1558–1566  2016.

[24] Li  Yujia  Swersky  Kevin  and Zemel  Rich. Generative moment matching networks. In ICML  pp.

1718–1727  2015.

[25] Liu  Ming-Yu  Breuel  Thomas  and Kautz  Jan. Unsupervised image-to-image translation networks. In

NeurIPS  pp. 700–708  2017.

[26] Liu  Ziwei  Luo  Ping  Wang  Xiaogang  and Tang  Xiaoou. Deep learning face attributes in the wild. In

ICCV  pp. 3730–3738  2015.

[27] Ma  Liqian  Jia  Xu  Sun  Qianru  Schiele  Bernt  Tuytelaars  Tinne  and Van Gool  Luc. Pose guided

person image generation. In NeurIPS  pp. 405–415  2017.

[28] Makhzani  Alireza  Shlens  Jonathon  Jaitly  Navdeep  Goodfellow  Ian  and Frey  Brendan. Adversarial

autoencoders. arXiv preprint arXiv:1511.05644  2015.

[29] Nguyen  Tu  Le  Trung  Vu  Hung  and Phung  Dinh. Dual discriminator generative adversarial nets. In

NeurIPS  pp. 2667–2677  2017.

[30] Odena  Augustus  Olah  Christopher  and Shlens  Jonathon. Conditional image synthesis with auxiliary

classiﬁer GANs. In ICML  pp. 2642–2651  2017.

[31] Radford  Alec  Metz  Luke  and Chintala  Soumith. Unsupervised representation learning with deep

convolutional generative adversarial networks. In ICLR  2016.

[32] Rezende  Danilo Jimenez  Mohamed  Shakir  and Wierstra  Daan. Stochastic backpropagation and

approximate inference in deep generative models. In ICML  pp. 1278–1286  2014.

[33] Salimans  Tim  Goodfellow  Ian  Zaremba  Wojciech  Cheung  Vicki  Radford  Alec  and Chen  Xi.

Improved techniques for training GANs. In NeurIPS  pp. 2234–2242  2016.

[34] Sønderby  Casper Kaae  Raiko  Tapani  Maaløe  Lars  Sønderby  Søren Kaae  and Winther  Ole. Ladder

variational autoencoders. In NeurIPS  pp. 3738–3746  2016.

[35] Srivastava  Akash  Valkoz  Lazar  Russell  Chris  Gutmann  Michael U  and Sutton  Charles. VEEGAN:

Reducing mode collapse in gans using implicit variational learning. In NeurIPS  pp. 3310–3320  2017.

[36] Ulyanov  Dmitry  Vedaldi  Andrea  and Lempitsky  Victor. It takes (only) two: Adversarial generator-

encoder networks. In AAAI  2018.

[37] van den Oord  Aaron  Kalchbrenner  Nal  Espeholt  Lasse  Vinyals  Oriol  Graves  Alex  et al. Conditional

image generation with pixelcnn decoders. In NeurIPS  pp. 4790–4798  2016.

[38] Van Oord  Aaron  Kalchbrenner  Nal  and Kavukcuoglu  Koray. Pixel recurrent neural networks. In ICML 

pp. 1747–1756  2016.

[39] Wang  Ting-Chun  Liu  Ming-Yu  Zhu  Jun-Yan  Tao  Andrew  Kautz  Jan  and Catanzaro  Bryan. High-

resolution image synthesis and semantic manipulation with conditional GANs. In CVPR  2018.

[40] Yu  Fisher  Seff  Ari  Zhang  Yinda  Song  Shuran  Funkhouser  Thomas  and Xiao  Jianxiong. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365  2015.

[41] Zhang  Han  Xu  Tao  Li  Hongsheng  Zhang  Shaoting  Huang  Xiaolei  Wang  Xiaogang  and Metaxas 
Dimitris. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In ICCV  pp. 5907–5915  2017.

[42] Zhang  Han  Xu  Tao  Li  Hongsheng  Zhang  Shaoting  Wang  Xiaogang  Huang  Xiaolei  and Metaxas 
Dimitris. StackGAN++: Realistic image synthesis with stacked generative adversarial networks. arXiv
preprint arXiv:1710.10916v2  2017.

[43] Zhang  Zizhao  Xie  Yuanpu  and Yang  Lin. Photographic text-to-image synthesis with a hierarchically-

nested adversarial network. arXiv preprint arXiv:1802.09178  2018.

11

[44] Zhao  Junbo  Mathieu  Michael  and LeCun  Yann. Energy-based generative adversarial network. In ICLR 

2017.

[45] Zhao  Shengjia  Song  Jiaming  and Ermon  Stefano.
autoencoders. arXiv preprint arXiv:1706.02262  2017.

InfoVAE: Information maximizing variational

[46] Zhu  Jun-Yan  Zhang  Richard  Pathak  Deepak  Darrell  Trevor  Efros  Alexei A  Wang  Oliver  and

Shechtman  Eli. Toward multimodal image-to-image translation. In NeurIPS  pp. 465–476  2017.

12

,Huaibo Huang
zhihang li
Ran He
Zhenan Sun
Tieniu Tan
Yanping Huang
Youlong Cheng
Ankur Bapna
Orhan Firat
Dehao Chen
Mia Chen
HyoukJoong Lee
Jiquan Ngiam
Quoc Le
Yonghui Wu
zhifeng Chen