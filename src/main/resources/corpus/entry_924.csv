2018,Causal Inference with Noisy and Missing Covariates via Matrix Factorization,Valid causal inference in observational studies often requires controlling for confounders. However  in practice measurements of confounders may be noisy  and can lead to biased estimates of causal effects. We show that we can reduce bias induced by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates. This flexible and principled framework adapts to missing values  accommodates a wide variety of data types  and can enhance a wide variety of causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting  using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.,Causal Inference with Noisy and Missing Covariates

via Matrix Factorization

Nathan Kallus∗

Xiaojie Mao∗
Cornell University

Madeleine Udell∗

{kallus  xm77  udell}@cornell.edu

Abstract

Valid causal inference in observational studies often requires controlling for con-
founders. However  in practice measurements of confounders may be noisy  and
can lead to biased estimates of causal effects. We show that we can reduce bias
induced by measurement noise using a large number of noisy measurements of
the underlying confounders. We propose the use of matrix factorization to infer
the confounders from noisy covariates. This ﬂexible and principled framework
adapts to missing values  accommodates a wide variety of data types  and can
enhance a wide variety of causal inference methods. We bound the error for the
induced average treatment effect estimator and show it is consistent in a linear
regression setting  using Exponential Family Matrix Completion preprocessing. We
demonstrate the effectiveness of the proposed procedure in numerical experiments
with both synthetic data and real clinical data.

1

Introduction

Estimating the causal effect of an intervention is a fundamental goal across many domains. Examples
include evaluating the effectiveness of recommender systems [1]  identifying the effect of therapies on
patients’ health [2] and understanding the impact of compulsory schooling on earnings [3]. However 
this task is notoriously difﬁcult in observatonal studies due to the presence of confounders: variables
that affect both the intervention and the outcomes. For example  intelligence level can inﬂuence both
students’ decisions regarding whether to go to college  and their earnings later on. Students who
choose to go to college may have higher intelligence than those who do not. As a result  the observed
increase in earnings associated with attending college is confounded with the effect of intelligence
and thus cannot faithfully represent the causal effect of college education.
One standard way to avoid such confounding effect is to control for all confounders [4]. However 
this solution poses practical difﬁculties. On the one hand  an exhaustive list of confounders is not
known a priori  so investigators usually adjust for a large number of covariates for fear of missing
important confounders. On the other hand  measurement noise may abound in the collected data:
some confounder measurements may be contaminated with noise (e.g.  data recording error)  while
other confounders may not be amenable to direct measurements and instead admit only proxy
measurements. For example  we may use an IQ test score as a proxy for intelligence. It is well
known that using proxies in place of the true confounders leads to biased causal effect estimates
[5  6  7]. However  we show in a linear regression setting that the bias due to measurement noise
can be effectively alleviated by using many proxies for the underlying confounders (Section 2.2).
For example  in addition to IQ test score  we may also use coursework grades and other academic
achievements to characterize the intelligence. Intuitively  using more proxies may allow for a more
accurate reconstruction of the confounder and thus may facilitate more accurate causal inference.

∗Alphabetical order

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Therefore  collecting a large number of covariates is beneﬁcial for causal inference not only to avoid
confounding effects but also to alleviate bias caused by measurement noise.
Although in the big-data era  collecting myriad covariates is easier than ever before  it is still
challenging to use the collected noisy covariates in causal inference. On the one hand  data is
inevitably contaminated with missing values  especially when we collect many covariates. Inaccurate
imputation of these missing values may aggravate measurement noise. Moreover  missing value
imputation can at most gauge the values of noisy covariates but inferring the latent confounders is the
most critical for accurate causal inference. On the other hand  the large number of covariates may
include heterogeneous data types (e.g.  continuous  ordinal  categorical  etc.) that must be handled
appropriately to exploit covariate information.
To address the aforementioned problems  we propose to use low rank matrix factorization as a
principled approach to preprocess covariate matrices for causal inference. This preprocessing step
infers the confounders for subsequent causal inference from partially observed noisy covariates.
Investigators can thus collect more covariates to control for potential confounders and use more proxy
variables to characterize the unmeasured traits of the subjects without being hindered by missing
values. Moreover  matrix factorization preprocessing is a very general framework. It can adapt to a
wide variety of data types and it can be seamlessly integrated with many causal inference techniques 
e.g.  regression adjustment  propensity score reweighting  matching [4]. Using matrix factorization as
a preprocessing step makes the whole procedure modular and enables investigators to take advantage
of existing packages for matrix factorization and causal inference.
We rigorously investigate the theoretical implication of the matrix factorization preprocessing with
respect to causal effect estimation. We establish a convergence rate for the induced average treatment
effect (ATE) estimator and show its consistency in a linear regression setting with Exponential
Family Matrix Completion preprocessing [8].
In contrast to traditional applications of matrix
factorization methods with matrix reconstruction as the end goal  our theoretical analysis validates
matrix factorization as a preprocessing step for causal inference.
We evaluate the effectiveness of our proposed procedure on both synthetic datasets and a clinical
dataset involving the mortality of twins born in the USA introduced by Louizos et al. [9]. We
empirically demonstrate that matrix factorization can accurately estimate causal effects by inferring
the latent confounders from a large number of noisy covariates. Moreover  matrix factorization
preprocessing enhances the performance of many causal inference methods and is robust to the
presence of missing values.
Related work. Our paper builds upon low rank matrix completion methods that have been success-
fully applied in many domains to recover data matrices from incomplete and noisy observations
[10  11  12]. These methods are not only computationally efﬁcient but also theoretically sound
with provable guarantees [8  13  14  15  16  17]. Moreover  matrix completion methods have been
developed to accommodate heterogeneous data types prevalent in empirical studies by using a rich
library of loss functions and penalties [18]. Recently  Athey et al. [19] use matrix completion methods
to impute the unobservable counterfactual outcomes and estimate the ATE for panel data. In contrast 
our paper focuses on measurement noise in the covariate matrix. Measurement noise has been
considered in literature for a long time [5  6  20]. Kuroki and Pearl [21] and Miao et al. [22] show
that causal effects are identiﬁable when the emission probabilities of proxies given confounders are
known and satisfy an invertibility condition. In contrast  our method assumes a simpler proxy model
(Figure 1) and provides a practical approach to carry out the estimation based on matrix factorization.
Louizos et al. recently [9] propose to use Variational Autoencoder as a heuristic way to recover the
latent confounders from multiple proxies. In contrast  matrix factorization methods  despite stronger
parametric assumptions  address the problem of missing values simultaneously  require considerably
less parameter tuning  and have theoretical justiﬁcations.
Notation. For two scalars a  b ∈ R  denote a ∨ b = max{a  b} and a ∧ b = min{a  b}. For an
positive integer N  we let [N ] = {1  2  . . .   N}. For a set Ω  |Ω| is the total number of elements in Ω.
For matrix X ∈ RN×p  denote its singular values as σ1 ≥ σ2 ≥ ··· ≥ σN∧p ≥ 0  and its smallest
singular value as σmin. The spectral norm  nuclear norm  Frobenius norm and max norm of X are
|Xij|
respectively. The projection matrix for X is deﬁned as PX = X(X(cid:62)X)−1X(cid:62). We use col(X) to
denote the column space of X and σ(z) to denote the sigmoid function 1/(1 + exp(−z)).

deﬁned as (cid:107)X(cid:107) = σ1  (cid:107)X(cid:107)(cid:63) =(cid:80)N∧p

i=1 σi  (cid:107)X(cid:107)F =

(cid:113)

1 + ··· + σ2
σ2

N∧p and (cid:107)X(cid:107)max = max

ij

2

Xi

Ui

Ti

Yi

Figure 1: Causal graph for the ith individual  i ∈ [N ]. The confounders Ui are unobserved (dashed);
the proxy variables Xi  treatment Ti  and outcome Yi are all observed (solid).

2 Causal inference with low rank matrix factorization

In this section  we ﬁrst introduce the problem of causal inference under measurement noise and
missing values formally and deﬁne notation. We then show that the bias caused by measurement
noise in linear regression is alleviated when more covariates are used. Finally we review low rank
matrix factorization methods and describe the proposed procedure for causal inference.

2.1 Problem formulation

We consider an observational study with N subjects. For subject i  Ti is the treatment variable and we
assume Ti ∈ {0  1} for simplicity. We use Yi(0)  Yi(1) to denote the potential outcomes for subject
i under control and treatment respectively [4]. We can only observe the potential outcome corre-
sponding to the received treatment level  i.e.  Yi = Yi(Ti). Assume that {Yi(0)  Yi(1)  Ti}N
i=1 are
independently and identically distributed (i.i.d). We denote T = [T1  ...  TN ](cid:62) and Y = [Y1  ...  YN ](cid:62).
For the ease of exposition  we focus on estimating the average treatment effect (ATE):

τ = E(Yi(1) − Yi(0)).

One standard way to estimate ATE is to adjust for the confounders. Suppose we have access to the
confounders Ui ∈ Rr for subject i  ∀i ∈ [N ]. Then we can employ many standard causal inference
techniques (e.g.  regression adjustment  propensity score reweighting  matching  etc.) to estimate
ATE under the following unconfoundedness assumption:
Assumption 1 (Unconfoundedness given unobservables). For each t = 0  1 and i = 1  ...  N  Yi(t)
is independent of Ti conditionally on Ui: P(Ti = 1 | Yi(t)  Ui) = P(Ti = 1 | Ui).
However  in practice we may not observe {Ui}N
i=1 directly. Instead suppose we can only partially
observe covariates Xi ∈ Rp  which is a collection of noisy measurements for the confounders. The
causal graph is given in Figure 1. The covariates Xi can represent various data types by canonical
encoding schemes. For example  Boolean data is encoded using 1 for true and −1 for false. Many
other encoding examples  e.g.  categorical data or ordinal data  can be found in Udell et al. [18]. We
concatenate these covariates into X ∈ RN×p. We assume that only entries of X over a subset of
indices Ω ⊂ [N ] × [p] are observed.
We further specify the generative model for individual entries Xij  (i  j) ∈ [N ]× [p]. We assume that
i Vj)  where Vj ∈ Rp represents loadings
Xij are drawn indepedently from distributions P(Xij | U(cid:62)
of the jth covariate on confounders. The distribution P(Xij | U(cid:62)
i Vj) models the measurement noise
mechanism for Xij. For example  if Xi1 is a measurement for Ui1 contaminated with standard
Gaussian noise  then P(Xi1 | U(cid:62)
i V1  1) where V1 = [1  0  ...  0](cid:62). This generative
model also accomodates proxy variables. Consider a simpliﬁed version of Spearman’s measureable
intelligence theory [23] where multiple kinds of test scores are used to characterize two kinds of
(unobservable) intelligence: quantitative and verbal. Suppose that there are p tests (e.g.  Classics 
Math  Music  etc.) which are recorded in Xi1  ...  Xip and the two intelligence are represented by
Ui1 and Ui2. We assume that these proxy variables are noisy realizations of linear combinations
of two intelligence. This can be modelled using the generative model Xij ∼ P(Xij | U(cid:62)
i Vj)
with Vj = [Vi1  Vi2  0  ...  0](cid:62) for j ∈ [p]. While this linear assumption seems restrictive  it’s
approximately true for a large class of nonlinear latent variable models when many proxies are used
for a small number of latent variables [24].
We aim to estimate ATE based on PΩ(X)  Y and T . It is however very challenging for the presence
of measurement noise and missing values. One the one hand  most causal inference techniques cannot

i V1) ∼ N (U(cid:62)

3

adapt to missing values directly and appropriate preprocessing is needed. On the other hand  it is
well known that measurement noise can dramatically undermine the unconfoundedness assumption
and lead to biased causal effect estimation [5  6]  i.e.  P(Yi(t)|Ti  Xi) (cid:54)= P(Yi(t)|Xi) for t = 0  1.

2.2 Measurement noise and bias

In this subsection  we show that using a large number of noisy covariates can effectively alleviate
the ATE estimation bias resulted from measurement noise in linear regression setting. Suppose there
are no missing values. We consider the linear regression model: ∀i ∈ [N ]  Yi = U(cid:62)
i α + τ Ti + i  
where α ∈ Rr is the coefﬁcient for confounders Ui  τ is the ATE  and i
i.i.d∼ N (0  σ2). For ∀i ∈ [N ] 
Ti are independently and probabilistically assigned according to confounders Ui. Unconfoundedness
(Assumtpion 1) implies that Ti are independent with i conditionally on Ui.
Proposition 1. Consider the additive noise model: X = U V (cid:62) + W where {Ui}N
W ∈ RN×p contains independent noisy entries with mean 0 and variance σ2
independent with {Ui}N
of least squares estimator in linear regression of Yi on Xi and Ti has the following form:

i=1 are i.i.d samples 
w  and entries in W are
i=1. Suppose that r  p are ﬁxed and p < N. As N → ∞  the asymptotic bias

E(TiUi)E(U(cid:62)
i ) − E(TiUi)[( 1

i Ui)−1[ 1

V (cid:62)V + E(U(cid:62)
V (cid:62)V )−1 + E(U(cid:62)

i Ui)−1]−1α
i Ui)]−1E(U(cid:62)

σ2
w

E(T 2

σ2
w

i Ti)
Corollary 1.1. The asymptotic bias (1) diminishes to 0 when σmin(V ) → ∞.
Corollary 1.1 suggests an important fact: collecting a large number of noisy covariates is an effective
remedy for the bias induced by measurement noise  as long as the noisy covariates are sufﬁciently
informative about the confounders. The condition σmin(V ) → ∞ requires that all confounders have
asymptotically inﬁnitely many proxies as p → ∞.2 Surprisingly  in this independent additive noise
case  the asymptotic bias (1) is even nearly optimal: it is identical to the optimal asymptotic bias we
would have if we knew the unobservable V (Proposition 2  Appendix A). In the rest of the paper  we
further exploit this fact by using matrix factorization preprocessing which adapts to missing values 
heterogenenous data types and more general noise models.

(1)

i=1 from noisy and incomplete

2.3 Low rank matrix factorization preprocessing
In this paper  we propose to recover the latent confounders {Ui}N
observations of X by using low rank matrix factorization methods  which rely on the assumption:
Assumption 2 (Low Rank Matrix). The full matrix X is a noisy realization of a low rank matrix
Φ ∈ RN×p with rank r (cid:28) min{N  p}.
In the context of causal inference  Assumption 2 corresponds to the surrogate-rich setting where many
proxies are used for a small number of latent confounders. For example  we have access to IQ test
scores  coursework grades  academic achievements and other proxies for the unobserved confounder
intelligence. Under the generative model in section 2.1  Assumption 2 implies that Φ = U V T where
U = [U1  ...  UN ](cid:62) is the confounder matrix and V = [V1  ...  Vp]T is the covariate loading matrix.
Although this assumption is unveriﬁable  low rank structure is shown to pervade in many domains
such as images [11]  customer preferences [10]  healthcare [12]  etc. The recent work by Udell and
Townsend [24] provides theoretical justiﬁcations that low rank structure arises naturally from a large
class of latent variable models.
Moreover  low rank matrix factorization methods usually assume the Missing Completely at Random
(MCAR) setting where the observed entries are sampled uniformly at random [8  25].
Assumption 3 (MCAR). ∀(i  j) ∈ Ω  i ∼ uniform([N ]) and j ∼ uniform([p]) independently and
the sampling is independent with the measurement noise.

Our paper takes the Exponential Family Matrix Completion (EFMC) as a concrete example  which
further assumes exponential family noise mechanism for the measurement noise [8].

2For example  suppose the number of confounders is r = 2 and V =

. . .
. . .
the ﬁrst covariate is a noisy proxy for the ﬁrst confounder and σmin(V ) = 1 < ∞ for any p.

0
1

0
1

0

0
1

. Then only

(cid:20)1

(cid:21)(cid:62)

4

Assumption 4 (Natural Exponential Family). Suppose that each entry Xij is drawn independently
from the corresponding natural exponential family with Φij as the natural parameter:

P(Xij|Φij) = h(Xij) exp(XijΦij − G(Φij))

where h : R → R is any function and G : R → R (called the log-partition function) is a strictly
convex analytic function with ∇2G(u) ≥ e−η|u| for some η > 0.
Exponential family encompass a wide variety of distributions like Gaussian  Poisson  Bernoulli 
which are extensively used for modelling different data types [26]. For example  if Xij takes binary
values ±1  then we can model it using Bernoulli distribution: P(Xij | Φij) = σ(XijΦij).
EFMC estimates Φ by the following regularized M-estimator:

ˆΦ = min(cid:107)Φ(cid:107)max≤ α∗√

N p

N p|Ω| [(cid:80)

(i j)∈Ω − log P(Xij|Φij)] + λ(cid:107)Φ(cid:107)(cid:63)

(2)

The estimator in (2) involves solving a convex optimization problem  whose solution can be found
efﬁciently by many off-the-shelf algorithms [27]. The nuclear norm regularization encourages a
low-rank solution: the larger the tuning parameter λ  the smaller the rank of the solution ˆΦ. In
practice  λ is usually selected by cross-validation. Moreover  the constraint (cid:107)Φ(cid:107)max ≤ α∗√
N p appears
merely as an artifact of the proof and it is recommended to drop this constraint in practice [28]. It can
be proved that under Assumptions 2 − 4 and some regularity assumptions the relative reconstruction
error of ˆΦ converges to 0 with high probability (Lemma 4  Appendix A). Furthermore  EFMC can be
extended by using a rich library of loss functions and regularization functions [18  29].
We use the left singular matrix of ˆΦ corresponding to nonzero singular values to estimate the column
space of the confounder matrix U. Although ˆU has orthonormal columns  the original confounders
are allowed to be correlated (Assumption 5). The estimated confounder space matrix ˆU is then used
in place of the covariate matrix for subsequent causal inference methods (e.g.  regression adjustment 
propensity reweighting  matching  etc.). Admittedly  only the column space of the confounder matrix
U can be identiﬁed  and any nonsingular linear transformation of ˆU is a valid estimator. However 
this sufﬁces for many causal inference techniques. For example  regression adjustment methods
based on linear regression [7]  polynomial regression  neural networks trained by backpropagation
[30]  propensity reweighting or propensity matching using propensity score estimated by logistic
regressions  and Mahalanobis matching are invariant to nonsingular linear transformations. Moreover 
the invariance to linear transformation and scale-free property is important since the latent confounders
may be abstract without commonly acknowledged scale or units (e.g.  intelligence).

3 Theoretical guarantee

In this section  we derive an error bound for the ATE estimator induced by EFMC preprocessing (2)
in linear regression setting. Proofs are deferred to Appendix A.
Consider the linear regression model in Section 2.2. Suppose that we use EFMC preprocessing and
linear regression for causal inference  which leads to the ATE estimator ˆτ. It is well known that the
accuracy of ˆτ relies on how well the estimated column space col( ˆU ) approximates the column space
of true confounder matrix col(U ). Ideally  if col( ˆU ) aligns with col(U ) perfectly  then ˆτ is identical
to the least squares estimator based on true confounders and is thus consistent. We introduce the
following distance metric between two column spaces [31]:
Deﬁnition 1. Consider two matrices ˆM ∈ RN×k and M ∈ RN×r with orthonormal columns  the
principal angle between their column spaces is deﬁned as

(cid:113)

∠(M  ˆM ) =

1 − σ2

r∧k( ˆM(cid:62)M )

This metric measures the magnitude of the “angle” between two column spaces. For example 
∠(M  ˆM ) = 0 if col(M ) = col( ˆM ) while ∠(M  ˆM ) = 1 if they are orthogonal.
Theorem 1. Assume the following assumptions hold:

(a) (cid:107)α(cid:107)max ≤ A for a positive constant A;

5

(b)

1√

N r

(cid:107)U(cid:107) is bounded above for any N;

(c) Ui is almost surely not linearly dependent with Ti;
(d) r∠( ˆU   U ) → 0 as N → 0;
(e) unconfoundedness (Assumption 1).

Then there exists a constant c > 0 such that with probability at least 1 − 2 exp(−cN 1/2) 

|ˆτ − τ∗| ≤ ( 2A√

N

(cid:107)T(cid:107))(

1√

N r

1

N T (cid:62)(I − PU )T − 2

(cid:107)U(cid:107))(r∠(U  ˆU ))
N (cid:107)T(cid:107)2∠(U  ˆU )

N→∞−→ 0

(3)

In the above theorem  assumption (c) rules out multicollinearity between the treatment and the
N T (cid:62)(I −
confounders  which is necessary for identifying ATE. This assumption guarantees that 1
PU )T in (3) is bounded away from 0 for any N (Lemma 5  Appendix A). Assumption (d) states that
col( ˆU ) should converge to col(U )with rate faster than 1/r to guarantee consistency of the resulting
ATE estimator. Theorem 1 shows that bounding the ATE estimation error requires bounding ∠(U  ˆU ) 
i.e.  the error of estimating col(U ) in matrix factorization. In the following theorem  we derive an
upper bound on the column space estimation error for EFMC (2).
Theorem 2. Assume that the following assumptions hold:

(a) Assumption 1 - 4 (Unconfoundedness  Low Rank Matrix  Missing Completely at Random 

Natural Exponential Family);

(b) Xij is sub-Exponential conditionally on Ui for any (i  j);
(c) For Φ = U V (cid:62)  σr(Φ)
σ1(Φ) is bounded away from 0;
(d) ˆU is estimated by EFMC (2) with λ = 2c0σ(cid:48)√
N p
|Ω| > c1rN log N for positive constants c0 and c1;

(cid:113) rN log N|Ω|

Then the following holds with probability at least 1 − 4e−2 log2 ¯N − e−2 log ¯N :

(cid:113) r3 ¯N log ¯N
(cid:113) r3 ¯N log ¯N

|Ω|

|Ω|

∠( ˆU   U ) ≤

c2αsp(Φ)

σr(Φ)

σ1(Φ) − c2αsp(Φ)

√

  where N = N ∨ p and

∧ 1

(4)

where c2 > 0 is a constant and αsp(Φ) =

N p(cid:107)Φ(cid:107)max
(cid:107)Φ(cid:107)F

is the spikeness ratio of Φ = U V (cid:62).

σr(Φ)

Theorem 2 shows that the column space estimation error of EFMC depends on two critical quantities:
αsp(Φ) and σr(Φ)
σ1(Φ) . The spikeness ratio αsp(Φ) is a standard measure quantifying the ill-posedness
of matrix factorization problems [8  32]. Small αsp(Φ) is necessary for accurate matrix estimation
error for matrix factorization  i.e.  small (cid:107) ˆΦ − Φ(cid:107) (Lemma 6  Appendix A). Moreover  nonvanishing
σ1(Φ) means that Φ does not lose information of any direction in col(U )  and thus guarantees that
small matrix estimation error (cid:107) ˆΦ − Φ(cid:107) translates into small column space estimation error ∠( ˆU   U )
(Lemma 7  Appendix). Next we introduce some generative assumptions on confounder matrix U and
covariate loading matrix V . Under these assumptions  EFMC accurately estimates the column space
of confounder matrix U such that r∠(U  ˆU ) → 0  and thus results in accurate ATE estimator.
Assumption 5 (Latent Confounders and Covariate Loadings). U and V satisfy the following for
some positive constants v  v  cV and cL:

(a) For i ∈ [N ]  Ui are i.i.d Gaussian samples with covariance matrix Σr×r = LL(cid:62) for some

full rank matrix L ∈ Rr×r such that 1√

r(cid:107)L(cid:107) < cL;
1(V L(cid:62)) ≤ vp and maxj (cid:107)Vj(cid:107)
(cid:107)V (cid:107)F

(b) vp ≤ σ2

r (V L(cid:62)) ≤ σ2

≤ cV√

p   j = 1  ...  p.

6

Assumption 5(a) speciﬁes a Gaussian distribution for latent confounders  which implies assumption
(b) in Theorem 1 with high probability (Lemma 10  Appendix A). It also assumes without loss of
generality that the latent confounders are not perfectly linearly correlated. Moreover  Assumption 5(b)
exludes the case where almost all covariates have vanishing loadings on the latent confounders. 3 In
this case  the collected covariates are not informative enough for recoverying the latent confounders.
Theorem 3. Suppose that r/N → 0 and ∃δ > 0 such that p1+δ/N → 0. Under the Assumption 5 
there exist positive constants c3 − c5 such that

r ∨ log N with probability at least 1 − N−1/2 − 2 exp(−c4N 1/2);

√

• αsp(Φ) ≤ c3cV
• σr(Φ)

σ1(Φ) ≥(cid:113) v

If we further assume the assumptions in Theorem 2 and that (cid:107)α(cid:107)max ≤ A  then for a constant C > 0 

v+2v with high probability 1 − 2 exp(−c5pδ);
(cid:113) r5r ¯N log ¯N
v+2v − Λ(r  N  |Ω|)(cid:3) − 2Λ(r  N  |Ω|)

N T (cid:62)(I − PU )T(cid:2)(cid:113) v
(cid:113) ¯rr3N log N

2AC

|Ω|

1

 

|ˆτ − τ∗| ≤

|Ω|

and r = r ∨ log N.

where Λ(r  N  |Ω|) = C
The assumption that p1+δ/N → 0 appears as an artifact of proof and our simulation shows that
the consistency also holds when N < p (Figure 3  Appendix B). Theorem 3 guarantees that the
ATE estimator induced by EFMC is consistent as long as r5rN log N /|Ω| → 0 when N  p →
∞. This seems much more restrictive than consistent matrix reconstruction that merely requires
rN log N /|Ω| → 0 (Lemma 6  Appendix A). However  this is due to the pessimistic nature of the
error bound. Our simulations in Section 4.1 show that matrix factorization works very well for r = 5 
N = 1500 and p = 1450 such that r6 (cid:29) N.

4 Numerical results

In this section  we show that low rank matrix factorization effectively reduces the ATE estimation
error caused by measurement noise using two experimental settings: 1) synthetic datasets with both
continuous and binary covariates and 2) the twins dataset introduced by Louizos et al. [9]. To
implement matrix factorization  we use the following nonconvex formulation:

(cid:80)
(i j)∈Ω Li j(Xij  U(cid:62)

ˆU   ˆV =

argmin

U∈RN×k V ∈Rp×k

i Vj) + λ

2 ((cid:107)U(cid:107)F + (cid:107)V (cid:107)F )

(5)

i Vj ﬁts the observation Xij for (i  j) ∈ Ω. The
where Lij is a loss function assessing how well U(cid:62)
solution ˆU is an estimator for the confounder space. This nonconvex formulation (5) is proved to
equivalently recover the solution of the convex formulation (2) when log-likelihood loss functions
and sufﬁcient large k are used [18  28]. Solving the nonconvex formulation (5) approximately is
usually much faster than solving the convex counterpart. In our experiments  we use the the R
package softImpute [33] for continuous covariates and quadratic loss  the R package logisticPCA [34]
for binary covariates and logistic loss  and the Julia package LowRankModels [18] for categorical
variables and multinomial loss. All tuning parameters are chosen via 5-fold cross-validation.

4.1 Synthetic experiment
We generate synthetic samples according to the following linear regression process: Yi | Ui  Ti ∼
N (α(cid:62)Ui + τ Ti  1) where confounder Uij ∼ N (0  1) and treatment variable Ti
| Ui ∼
Bernoulli(σ(β(cid:62)Ui)) for i ∈ [N ]  j ∈ [r]. We consider covariates generated from both in-
depedent Gaussian noise and independent Bernoulli noise: Xij ∼ N (U(cid:62)
i Vj  5) and Xij ∼
3For example  if only nV noisy covariates have nonvanishing loadings on the confounders  and their loading
.

vectors have norms of similar order  then (cid:107)V (cid:107)F =
≈ 1√
When limp→∞ nV /p → 0  i.e.  almost all covariates have vanishing loading  Assumption 5(b) is violated.

nV maxj (cid:107)Vj(cid:107)  so maxj (cid:107)Vj(cid:107)

j=1 (cid:107)Vj(cid:107)2 ≈ √

(cid:113)(cid:80)p

(cid:107)V (cid:107)F

nV

7

Figure 2: Estimation error for different ATE estimators with Gaussian and Binary proxy variables .

i Vj)) for Vj ∈ Rr. We set the dimension of the latent confounders r = 5  use
Bernoulli(σ(U(cid:62)
α = [−2  3 −2 −3 −2] and β = [1  2  2  2  2]  and choose τ = 2 in our example. (But our conclu-
sion is robust to different values of these parameter.) We consider low dimensional case where the
number of covariates p varies from 100 to 1000 and the sample size N = 2p and high dimensional
case where p varies from 150 to 1500 and N = p + 50. For each dimensional setting  we compute the
error metrics based on 50 replications of the experiments and we generate entries of V independently
from standard normal distribution with V ﬁxed across the replications.
We compare the root mean squared error (RMSE) scaled by the true ATE in Figure 2 for the following
ﬁve ATE estimators in linear regression: the Lasso  Ridge and OLS estimators from regressing
Yi on Ti and noisy covariates Xi  the OLS estimator from regressing Yi on Ti and the estimated
confounders ˆUi from matrix factorization (MF)  and the OLS estimator from regressing Yi on Ti and
the true confounders Ui (Oracle). The shaded area corresponds to the 2-standard-deviation error band
for the estimated relative RMSE across 50 replications.
Figure 2 shows that OLS leads to accurate ATE estimation for Gaussian additive noise when the
number of covariates is sufﬁciently large  which is consistent with Corollary 1.1. However  for
high dimensional data  matrix factorization preprocessing dominates all other feasible methods and
its RMSE is very close to the oracle regression for sufﬁciently large number of covariates. While
all feasible methods tend to have better performance when more covariates are available  matrix
factorization preprocessing is the most effective in exploiting the noisy covariates for accurate causal
inference. Sufﬁciently many noisy covariates are very important for accurate ATE estimation in the
presence of measurement noise. We can show that the error does not converge when only N grows but
p is ﬁxed (Figure 6  Appendix B). With only a few covariates  matrix factorization preprocessing may
have high error because the cross-validation chooses rank smaller than the ground truth. Furthermore 
the gain from using matrix factorization is more dramatic for binary covariates  which demonstrates
the advantage of matrix factorization preprocessing with loss functions adapting to the data types.
More numerical results on different dimensional settings and missing data can be found in Appendix.

4.2 Twin mortality

We further examine the effectiveness of matrix factorization preprocessing using the twins dataset
introduced by Louizos et al. [9]. This dataset includes information for N = 11984 pairs of twins
of same sex who were born in the USA between 1998-1991 and weighted less than 2kg. For the
ith twin-pair  the treatment variable Ti corresponds to being the heavier twin and the outcomes
Yi(0)  Yi(1) are the mortality in the ﬁrst year after they were born. We have outcome records for both
twins and view them as two potential outcomes for the treatment variable. Therefore  the −2.5%
difference between the average mortality rate of heavier twins and that of ligher twins can be viewed
as the "true" ATE. This dataset also includes other 46 covariates relating to the parents  the pregnancy
and birth for each pair of twins. More details about the dataset can be found in Louizos et al. [9].
To simulate confounders in observational studies  we follow the practice in Louizos et al. [9] and
selectively hide one of the two twins based on one variable highly correlated with the outcome:
GESTAT10  the number of gestation weeks prior to the birth. This is an ordinal variable with values
from 0 to 9 indicating less than 20 gestation weeks  20 − 27 gestation weeks and so on. We simulate
Ti | Ui ∼ Bernoulli(σ(5(Ui/10 − 0.1)))  where Ui is the confounder GESTAT10. Then for each

8

0.00.10.20.30.42505007501000pRelative RMSEGaussian Covariates  Low Dimension0.00.20.40.650010001500pRelative RMSEGaussian Covariates  High Dimension0.00.20.40.650010001500pRelative RMSEBinary Covariates  High DimensionMethodLassoRidgeOLSMF (Proposal)OracleFigure 3: Left: estimation errors of different ATE estimators with no missing values. Right: estimation
error of ATE estimators based on matrix factorization and imputation methods when each entry of
the proxy variable matrix is missing with probability 30%.

twin-pair  we only observe the lighter twin if Ti = 0 and the heavier twin otherwise. We create noisy
proxies for the confounder as follows: we replicate the GESTAT10 p times and independently perturb
the entries of these p copies with probability 0.5. Each perturbed entry is assigned with a new value
sampled from 0 to 9 uniformly at random. We also consider the presence of missing values: we set
each entry as missing value independently with probability 0.3. We vary p from 5 to 50 and for each
p we repeat the experiments 20 times for computing error metrics.
We compare the performance of different methods for both complete data and missing data in Figure 3.
For complete data  we consider logistic regression (LR)  doubly robust estimator (DR)  Mahalanobis
matching (Match) and propensity score matching (PS Match) using noisy covariates  and their
counterparts using the estimated confounders from matrix factorization. All propensity scores are
estimated by logistic regression using noisy covariates or estimated confounders accordingly. The
matching methods are implemented via the full match algorithm in the R package optmatch [35]. For
missing data  we consider logistic regression using data output from different preprocessing method:
imputing missing values by column-wise mode  multiple imputation using the R package MICE with
5 repeated imputations [36]  and the estimated confounders { ˆUi}N
i=1 from matrix factorization. We
also discuss comparisons to [9] in Appendix C.
We can observe that all methods that use matrix factorization clearly outperform their counterparts
that do not  even though the noise mechanism does not obey common noise assumptions in matrix
factorization literature. In particular  the Mahalanobis matching (Match) beneﬁts the most from matrix
factorization that simultaneously alleviates the measurement noise and reduces the dimension. The
effect of solely reducing measurement noise is shown in the result of the propensity score matching
where matching is based on the univariate propensity score and thus dimensionality is not the primary
issue. Our results also demonstrate that matrix factorization preprocessing can augment popular
causal inference methods beyond linear regression. Furthermore  matrix factorization preprocessing is
robust to a considerable amount of missing values and it dominates both the ad-hoc mode imputation
method and the state-of-art multiple imputation method. This suggests that inferring the latent
confounders is more important for causal inference than imputing the noisy covariates.

5 Conclusion

In this paper  we address the problem of measurement noise prevalent in causal inference. We show
that with a large number of noisy proxies  we can reduce the bias resulting from measurement noise by
using matrix factorization preprocessing to infer latent confounders. We guarantee the effectiveness
of this approach in a linear regression setting  and show its effectiveness numerically on both synthetic
and real clinical datasets. These results demonstrate that preprocessing by matrix factorization to
infer latent confounders has a number of advantages: it can accommodate a wide variety of data
types  ensures robustness to missing values  and can improve causal effect estimation when used in
conjunction with a wide variety of causal inference methods. As such  matrix factorization allows
more principled and accurate estimation of causal effects from observational data.

9

0.0250.0500.0751020304050Number of ProxiesRMSEMethodMatchMF + MatchLRMF + LRDRMF + DRPS MatchMF + PS MatchComplete Data0.040.060.081020304050Number of ProxiesRMSEMethodMF + LRMICE + LRMode + LRMissing DataAcknowledgments

This work was supported by the National Science Foundation under Grant No. 1656996. This work
was supported by the DARPA Award FA8750-17-2-0101.

References
[1] Tobias Schnabel  Adith Swaminathan  Ashudeep Singh  Navin Chandak  and Thorsten
Joachims. Recommendations as treatments: Debiasing learning and evaluation. arXiv preprint
arXiv:1602.05352  2016.

[2] Alfred F Connors  Theodore Speroff  Neal V Dawson  Charles Thomas  Frank E Harrell 
Douglas Wagner  Norman Desbiens  Lee Goldman  Albert W Wu  Robert M Califf  et al. The
effectiveness of right heart catheterization in the initial care of critically iii patients. Journal of
American Medical Association  276(11):889–897  1996.

[3] Joshua D Angrist and Alan B Keueger. Does compulsory school attendance affect schooling

and earnings? The Quarterly Journal of Economics  106(4):979–1014  1991.

[4] Guido W Imbens and Donald B Rubin. Causal inference in statistics  social  and biomedical

sciences. Cambridge University Press  2015.

[5] Peter A Frost. Proxy variables and speciﬁcation bias. The review of economics and Statistics 

pages 323–325  1979.

[6] Michael R Wickens. A note on the use of proxy variables. Econometrica: Journal of the

Econometric Society  pages 759–761  1972.

[7] Jeffrey M Wooldridge. Introductory econometrics: A modern approach. Nelson Education 

2015.

[8] Suriya Gunasekar  Pradeep Ravikumar  and Joydeep Ghosh. Exponential family matrix com-
pletion under structural constraints. In International Conference on Machine Learning  pages
1917–1925  2014.

[9] Christos Louizos  Uri Shalit  Joris M Mooij  David Sontag  Richard Zemel  and Max Welling.
Causal effect inference with deep latent-variable models. In Advances in Neural Information
Processing Systems  pages 6449–6459  2017.

[10] James Bennett  Stan Lanning  et al. The netﬂix prize. In Proceedings of KDD cup and workshop 

volume 2007  page 35. New York  NY  USA  2007.

[11] Feilong Cao  Miaomiao Cai  and Yuanpeng Tan. Image interpolation via low-rank matrix
completion and recovery. IEEE Transactions on Circuits and Systems for Video Technology 
25(8):1261–1270  2015.

[12] Alejandro Schuler  Vincent Liu  Joe Wan  Alison Callahan  Madeleine Udell  David E Stark 
and Nigam H Shah. Discovering patient phenotypes using generalized low rank models. In
Biocomputing 2016: Proceedings of the Paciﬁc Symposium  pages 144–155. World Scientiﬁc 
2016.

[13] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics  9(6):717  2009.

[14] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. IEEE Transactions on Information Theory  56(5):2053–2080  2010.

[15] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE 

98(6):925–936  2010.

[16] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning

Research  12(Dec):3413–3430  2011.

10

[17] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a

few entries. IEEE Transactions on Information Theory  56(6):2980–2998  2010.

[18] Madeleine Udell  Corinne Horn  Reza Zadeh  Stephen Boyd  et al. Generalized low rank models.

Foundations and Trends R(cid:13) in Machine Learning  9(1):1–118  2016.

[19] Susan Athey  Mohsen Bayati  Nikolay Doudchenko  Guido Imbens  and Khashayar Khosravi.
Matrix completion methods for causal panel data models. arXiv preprint arXiv:1710.10251 
2017.

[20] Raymond J Carroll  David Ruppert  Ciprian M Crainiceanu  and Leonard A Stefanski. Mea-

surement error in nonlinear models: a modern perspective. Chapman and Hall/CRC  2006.

[21] Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference.

Biometrika  101(2):423–437  2014.

[22] Wang Miao  Zhi Geng  and Eric Tchetgen Tchetgen. Identifying causal effects with proxy

variables of an unmeasured confounder. arXiv preprint arXiv:1609.08816  2016.

[23] Charles Spearman. " general intelligence " objectively determined and measured. The American

Journal of Psychology  15(2):201–292  1904.

[24] Madeleine Udell and Alex Townsend. Nice latent variable models have log-rank. arXiv preprint

arXiv:1705.07474  2017.

[25] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data  volume 333.

John Wiley & Sons  2014.

[26] Peter McCullagh. Generalized linear models. European Journal of Operational Research 

16(3):285–292  1984.

[27] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[28] Nathan Kallus and Madeleine Udell. Dynamic assortment personalization in high dimensions.

arXiv preprint arXiv:1610.05604  2016.

[29] Ajit P Singh and Geoffrey J Gordon. A uniﬁed view of matrix factorization models. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases  pages
358–373. Springer  2008.

[30] Andrew Y Ng. Feature selection  l 1 vs. l 2 regularization  and rotational invariance.

In
Proceedings of the twenty-ﬁrst international conference on Machine learning  page 78. ACM 
2004.

[31] T Tony Cai  Anru Zhang  et al. Rate-optimal perturbation bounds for singular subspaces with

applications to high-dimensional statistics. The Annals of Statistics  46(1):60–89  2018.

[32] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix
completion: Optimal bounds with noise. Journal of Machine Learning Research  13(May):1665–
1697  2012.

[33] Trevor Hastie  Rahul Mazumder  Jason D Lee  and Reza Zadeh. Matrix completion and low-rank
svd via fast alternating least squares. Journal of Machine Learning Research  16:3367–3402 
2015.

[34] Michael Collins  Sanjoy Dasgupta  and Robert E Schapire. A generalization of principal
components analysis to the exponential family. In Advances in neural information processing
systems  pages 617–624  2002.

[35] Ben B. Hansen and Stephanie Olsen Klopfer. Optimal full matching and related designs via

network ﬂows. Journal of Computational and Graphical Statistics  15(3):609–627  2006.

[36] Stef van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained

equations in r. Journal of Statistical Software  45(3):1–67  2011.

11

[37] Gautam Tripathi. A matrix extension of the cauchy-schwarz inequality. Economics Letters 

63(1):1–3  1999.

[38] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[39] Daniel Hsu  Sham Kakade  Tong Zhang  et al. A tail inequality for quadratic forms of subgaus-

sian random vectors. Electronic Communications in Probability  17  2012.

12

,Tatsunori Hashimoto
Percy Liang
John Duchi
Nathan Kallus
Xiaojie Mao
Madeleine Udell