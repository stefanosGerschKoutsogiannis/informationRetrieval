2010,New Adaptive Algorithms for Online Classification,We propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting. This framework allows to design and prove   relative mistake bounds for any generic loss function. The mistake   bounds can be specialized for the hinge loss  allowing to recover   and improve the bounds of known online classification   algorithms. By optimizing the general bound we derive a new online   classification algorithm  called NAROW  that hybridly uses adaptive- and fixed- second order   information. We analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.,New Adaptive Algorithms for Online Classiﬁcation

Francesco Orabona

DSI

Universit`a degli Studi di Milano

Milano  20135 Italy

orabona@dsi.unimi.it

koby@ee.technion.ac.il

Koby Crammer

Department of Electrical Enginering

The Technion

Haifa  32000 Israel

Abstract

We propose a general framework to online learning for classiﬁcation problems
with time-varying potential functions in the adversarial setting. This framework
allows to design and prove relative mistake bounds for any generic loss function.
The mistake bounds can be specialized for the hinge loss  allowing to recover and
improve the bounds of known online classiﬁcation algorithms. By optimizing the
general bound we derive a new online classiﬁcation algorithm  called NAROW 
that hybridly uses adaptive- and ﬁxed- second order information. We analyze the
properties of the algorithm and illustrate its performance using synthetic dataset.

1

Introduction

Linear discriminative online algorithms have been shown to perform very well on binary and mul-
ticlass labeling problems [10  6  14  3]. These algorithms work in rounds  where at each round a
new instance is given and the algorithm makes a prediction. After the true class of the instance is
revealed  the learning algorithm updates its internal hypothesis. Often  such update is taking place
only on rounds where the online algorithm makes a prediction mistake or when the conﬁdence in
the prediction is not sufﬁcient. The aim of the classiﬁer is to minimize the cumulative loss it suffers
due to its prediction  such as the total number of mistakes.
Until few years ago  most of these algorithms were using only ﬁrst-order information of the in-
put features. Recently [1  8  4  12  5  9]  researchers proposed to improve online learning algo-
rithms by incorporating second order information. Speciﬁcally  the Second-Order-Perceptron (SOP)
proposed by Cesa-Bianchi et al. [1] builds on the famous Perceptron algorithm with an additional
data-dependent time-varying “whitening” step. Conﬁdence weighted learning (CW) [8  4] and the
adaptive regularization of weights algorithm (AROW) [5] are motivated from an alternative view:
maintaining conﬁdence in the weights of the linear models maintained by the algorithm. Both CW
and AROW use the input data to modify the weights as well and the conﬁdence in them. CW and
AROW are motivated from the speciﬁc properties of natural-language-precessing (NLP) data and
indeed were shown to perform very well in practice  and on NLP problems in particular. However 
the theoretical foundations of this empirical success were not known  especially when using only
the diagonal elements of the second order information matrix. Filling this gap is one contribution of
this paper.
In this paper we extend and generalizes the framework for deriving algorithms and analyzing them
through a potential function [2]. Our framework contains as a special case the second order Percep-
tron and a (variant of) AROW. While it can also be used to derive new algorithms based on other
loss functions.
For carefully designed algorithms  it is possible to bound the cumulative loss on any sequence of
samples  even adversarially chosen [2]. In particular  many of the recent analyses are based on the
online convex optimization framework  that focuses on minimizing the sum of convex functions.

1

Two common view-points for online convex optimization are of regularization [15] or primal-dual
progress [16  17  13]. Recently new bounds have been proposed for time-varying regularizations
in [18  9]  focusing on the general case of regression problems. The proof technique derived from
our framework extends the work of Kakade et al. [13] to support time varying potential functions.
We also show how the use of widely used classiﬁcation losses  as the hinge loss  allows us to derive
new powerful mistake bounds superior to existing bounds. Moreover the framework introduced
supports the design of aggressive algorithms  i.e. algorithms that update their hypothesis not only
when they make a prediction mistake.
Finally  current second order algorithms suffer from a common problem. All these algorithms main-
tain the cumulative second-moment of the input features  and its inverse  qualitatively speaking  is
used as a learning rate. Thus  if there is a single feature with large second-moment in the preﬁx of the
input sequence  its effective learning rate would drop to a relatively low value  and the learning algo-
rithm will take more time to update its value. When the instances are ordered such that the value of
this feature seems to be correlated with the target label  such algorithms will set the value of weight
corresponding to this feature to a wrong value and will decrease its associated learning rate to a low
value. This combination makes it hard to recover from the wrong value set to the weight associated
with this feature. Our ﬁnal contribution is a new algorithm that adapts the way the second order
information is used. We call this algorithm Narrow Adaptive Regularization Of Weights (NAROW).
Intuitively  it interpolates its update rule from adaptive-second-order-information to ﬁxed-second-
order-information  to have a narrower decrease of the learning rate for common appearing features.
We derive a bound for this algorithm and illustrate its properties using synthetic data simulations.

2 Online Learning for Classiﬁcation

t xt).

(cid:80)T
the regret  R(u) +(cid:80)T
(cid:80)T

We work in the online binary classiﬁcation scenario where learning algorithms work in rounds.
At each round t  an instance xt ∈ Rd is presented to the algorithm  which then predicts a label
ˆyt ∈ {−1  +1}. Then  the correct label yt is revealed  and the algorithm may modify its hypothesis.
The aim of the online learning algorithm is to make as few mistakes as possible (on any sequence
of samples/labels {(xt  yt)}T
t=1). In this paper we focus on linear prediction functions of the form
ˆyt = sign(w(cid:62)
We strive to design online learning algorithms for which it is possible to prove a relative mistakes
bound or a loss bound. Typical such analysis bounds the cumulative loss the algorithm suffers 
t=1 (cid:96)(wt  xt  yt)  with the cumulative loss of any classiﬁer u plus an additional penalty called
t=1 (cid:96)(u  xt  yt). Given that we focus on classiﬁcation  we are more interested
in relative mistakes bound  where we bound the number of mistakes of the learner with R(u) +
t=1 (cid:96)(u  xt  yt). Since the classiﬁer u is arbitrary  we can choose  in particular  the best classiﬁer
that can be found in hindsight given all the samples. Often R(·) depends on a function measuring
the complexity of u and the number of samples T   and (cid:96) is a non-negative loss function. Usually (cid:96)
is chosen to be a convex upper bound of the 0/1 loss. We will also denote by (cid:96)t(u) = (cid:96)(u  xt  yt).
In the following we denote by M to be the set of round indexes for which the algorithm performed a
mistake. We assume that the algorithm always update if it rules in such events. Similarly  we denote
by U the set of the margin error rounds  that is  rounds in which the algorithm updates its hypothesis
and the prediction is correct  but the loss (cid:96)t(wt) is different from zero. Their cardinality will be
indicated with M and U respectively. Formally  M = {t : sign(w(cid:62)
t xt) (cid:54)= yt & wt (cid:54)= wt+1} 
and U = {t : sign(w(cid:62)
t xt) = yt & wt (cid:54)= wt+1}. An algorithm that updates its hypothesis only on
mistake rounds is called conservative (e.g. [3]). Following previous naming convention [3]  we call
aggressive an algorithm that updates is rule on rounds for which the loss (cid:96)t(wt) is different from
zero  even if its prediction was correct.
We deﬁne now few basic concepts from convex analysis that will be used in the paper. Given a
convex function f : X → R  its sub-gradient ∂f (v) at v satisﬁes: ∀u ∈ X  f (u) − f (v) ≥ (u −
v)· ∂f (v). The Fenchel conjugate of f  f∗ : S → R  is deﬁned by f∗(u) = supv∈S
A differentiable function f : X → R is β-strongly convex w.r.t. a norm (cid:107) · (cid:107) if for any u  v ∈ S and
α ∈ (0  1)  h(αu + (1 − α)v) ≤ αf (u) + (1 − α)f (v) − β
2 α(1 − α)(cid:107)u − v(cid:107)2. Strong convexity
turns out to be a key property to design online learning algorithms.

(cid:0)v · u− f (v)(cid:1).

2

3 General Algorithm and Analysis

We now introduce a general framework to design online learning algorithms and a general lemma
which serves as a general tool to prove their relative regret bounds. Our algorithm builds on previous
algorithms for online convex programming with a one signiﬁcant difference. Instead of using a ﬁxed
link function as ﬁrst order algorithms  we allow a sequence of link functions ft(·)  one for each time
t. In a nutshell  the algorithm maintains a weight vector θt. Given a new examples it uses the current
link function ft to compute a prediction weight vector wt. After the target label is received it sets
the new weight θt+1 to be the sum of θt and minus the gradient of the loss at wt. The algorithm is
summarized in Fig. 1.
The following lemma is a generalization of Corollary 7 in [13] and Corollary 3 in [9]  for online
learning. All the proofs can be found in the Appendix.
Lemma 1. Let ft  t = 1  . . .   T be βt-strongly convex functions with respect to the norms
(cid:107) · (cid:107)f1  . . .  (cid:107) · (cid:107)fT over a set S and let (cid:107) · (cid:107)f∗
be the respective dual norms. Let f0(0) = 0 
and x1  . . .   xT be an arbitrary sequence of vectors in Rd. Assume that algorithm in Fig. 1 is run
T(cid:88)
on this sequence with the functions fi. Then  for any u ∈ S  and any λ > 0 we have

(cid:32) η2

(cid:18) 1

T(cid:88)

(cid:33)

(cid:19)

i

t (θt) − f∗
(f∗

t−1(θt))

.

t (cid:107)zt(cid:107)2
f∗
2λβt

t

+

1
λ

ηtz(cid:62)

t

λ

t=1

wt − u

≤ fT (λu)

+

λ

t=1

This Lemma can appear difﬁcult to interpret  but we now show that it is straightforward to use
the lemma to recover known bounds of different online learning algorithms.
In particular we
can state the following Corollary that holds for any convex loss (cid:96) that upper bounds the 0/1 loss.

1: Input: A series of strongly convex

functions f1  . . .   fT .

t (θt)

Receive xt
Set wt = ∇f∗
Predict ˆyt = sign(w(cid:62)
Receive yt
if (cid:96)t(wt) > 0 then
zt = ∂(cid:96)t(wt)
θt+1 = θt − ηtzt

2: Initialize: θ1 = 0
3: for t = 1  2  . . .   T do
4:
5:
6:
7:
8:
9:
10:
11:
12:
end if
13:
14: end for

Corollary 1. Deﬁne B =(cid:80)T

t=1(f∗

t (θt)− f∗

t−1(θt)). Under
the hypothesis of Lemma 1  if (cid:96) is convex and it upper bounds
the 0/1 loss  and ηt = η  then for any u ∈ S the algorithm
in Fig. 1 has the following bound on the maximum number of
mistakes M 

M ≤ T(cid:88)

T(cid:88)

(cid:107)zt(cid:107)2
f∗
2βt

t

B
η

t xt)

fT (u)

.

η

+

t=1

t=1

+ η

else

(cid:96)t(u) +

θt+1 = θt

(1)
Moreover if ft(x) ≤ ft+1(x) ∀x ∈ S  t = 0  . . .   T − 1 then
B ≤ 0.
A similar bound has been recently presented in [9] as a re-
gret bound. Yet  there are two differences. First  our analysis
bounds the number of mistakes  a more natural quantity in
classiﬁcation setting  rather than of a general loss function.
Second  we retain the additional term B which may be nega-
tive  and thus possibly provide a better bound. Moreover  to
choose the optimal tuning of η we should know quantities that are unknown to the learner. We could
use adaptive regularization methods  as the one proposed in [16  18]  but in this way we would lose
the possibility to prove mistake bounds for second order algorithms  like the ones in [1  5]. In the
next Section we show how to obtain bounds with an automatic tuning  using additional assumption-
ion on the loss function.

Figure 1: Prediction algorithm

3.1 Better bounds for linear losses
The hinge loss  (cid:96)(u  xt  yt) = max(1 − ytu(cid:62)xt  0)  is a very popular evaluation metric in classi-
ﬁcation. It has been used  for example  in Support Vector Machines [7] as well as in many online
learning algorithms [3]. It has also been extended to the multiclass case [3]. Often mistake bounds
are expressed in terms of the hinge loss. One reason is that it is a tighter upper bound of the 0/1 loss
compared to other losses  as the squared hinge loss. However  this loss is particularly interesting for
us  because it allows an automatic tuning of the bound in (1). In particular it is easy to verify that it
satisﬁes the following condition

(cid:96)(u  xt  yt) ≥ 1 + u(cid:62)∂(cid:96)t(wt)  ∀u ∈ S  wt : (cid:96)t(wt) > 0 .

(2)

3

Thanks to this condition we can state the following Corollary for any loss satisfying (2).
Corollary 2. Under the hypothesis of Lemma 1  if fT (λu) ≤ λ2fT (u)  and (cid:96) satisﬁes (2)  then for
any u ∈ S  and any λ > 0 we have

ηt ≤ L + λfT (u) +

(cid:32)
(cid:88)
t∈M∪U ηt(cid:96)t(u)  and B = (cid:80)T
where L = (cid:80)
(cid:118)(cid:117)(cid:117)(cid:116)2B +
optimal λ  we obtain(cid:88)
ηt ≤ L +(cid:112)2fT (u)

(cid:88)
t
2βt
t∈M∪U
t (θt) − f∗
t=1(f∗
(cid:88)

(cid:18) η2

(cid:18) η2

t∈M∪U

B +

1
λ

t∈M∪U

(cid:107)zt(cid:107)2
f∗

t

− ηtw(cid:62)
t zt

t−1(θt)). In particular  choosing the

(cid:107)zt(cid:107)2
f∗

t

− 2ηtw(cid:62)
t zt

t
βt

.

(3)

t∈M∪U

(cid:19)(cid:33)

 

(cid:19)

The intuition and motivation behind this Corollary is that a classiﬁcation algorithm should be inde-
pendent of the particular scaling of the hyperplane. In other words  wt and αwt (with α > 0) make
exactly the same predictions  because only the sign of the prediction matters. Exactly this indepen-
dence in a scale factor allows us to improve the mistake bound (1) to the bound of (3). Hence  when
(2) holds  the update of the algorithm becomes somehow independent from the scale factor  and we
have the better bound. Finally  note that when the hinge loss is used  the vector θt is updated as in
an aggressive version of the Perceptron algorithm  with a possible variable learning rate.

4 New Bounds for Existing Algorithms

We now show the versatility of our framework  proving better bounds for some known ﬁrst order
and second order algorithms.

4.1 An Aggressive p-norm Algorithm

2(q−1)(cid:107)u(cid:107)2

We can use the algorithm in Fig. 1 to obtain an aggressive version of the p-norm algorithm [11]. Set
q  that is 1-strongly convex w.r.t. the norm (cid:107) · (cid:107)q. The dual norm of (cid:107) · (cid:107)q is
ft(u) = 1
(cid:107) · (cid:107)p  where 1/p + 1/q = 1. Moreover set ηt = 1 in mistake error rounds  so using the second
bound of Corollary 2  and deﬁning R such that (cid:107)xt(cid:107)2
(cid:0)η2
t (cid:107)xt(cid:107)2
(cid:88)

p ≤ R2  we have
p + 2ηtytw(cid:62)

(cid:115)(cid:107)u(cid:107)2
(cid:115)(cid:107)u(cid:107)2

(cid:115) (cid:88)
(cid:115)

(cid:1) −(cid:88)
(cid:1) −(cid:88)

M ≤ L +

≤ L +

q − 1

M R2 +

t∈M∪U

p + 2ηtytw(cid:62)

t xt

t xt

t∈U

ηt .

ηt

q

q

q − 1

t∈U

Solving for M we have

1

R2

t xt

t∈U

t∈U

(cid:19)

− ηt

(cid:107)u(cid:107)2

(cid:107)u(cid:107)2

qR2 + R

p+2ηtytw(cid:62)

ηt 

(4)

2(q − 1)

M ≤ L +

4(q − 1)
t (cid:107)xt(cid:107)2

(cid:107)u(cid:107)q√
q − 1
t∈M∪U ηt(cid:96)t(u)  and D = (cid:80)

where L = (cid:80)
we have that D is negative  and L ≤(cid:80)

. We have still the
freedom to set ηt in margin error rounds. If we set ηt = 0  the algorithm of Fig. 1 becomes the
p-norm algorithm and we recover its best bound [11]. However if 0 ≤ ηt ≤ min
  1
t∈M∪U (cid:96)t(u). Hence the aggressive updates gives us a better

bound  thanks to last term that is subtracted to the bound.
In the particular case of p = q = 2 we recover the Perceptron algorithm. In particular the minimum
of D  under the constraint ηt ≤ 1  can be found setting ηt = min
. If R is equal
to
2  we recover the PA-I update rule  when C = 1. However note that the mistake bound in (4) is
better than the one proved for PA-I in [3] and the ones in [16]. Hence the bound (4) provides the ﬁrst
theoretical justiﬁcation to the good performance of the PA-I  and it can be seen as a general evidence
supporting the aggressive updates versus the conservative ones.

(cid:16) R2/2−ytw(cid:62)

(cid:16) R2−2ytw(cid:62)
(cid:17)

t xt
p

(cid:107)xt(cid:107)2

(cid:107)xt(cid:107)2

(cid:17)

√

t xt

  1

(cid:0)η2
t (cid:107)xt(cid:107)2
(cid:115)
(cid:18) η2

1

t∈U

qR2 + L + D −(cid:88)

4

4.2 Second Order Algorithms

Figure 2: NLP Data: the number of
words vs. the word-rank on two sen-
timent data sets.

identity we have f∗
in Corollary 2  and setting ηt = 1 we have

t (θt) − f∗

t−1(θt) = − (x(cid:62)
t A
2(r+x(cid:62)

(cid:112)
(cid:115)
(cid:115)

u(cid:62)AT u

(cid:107)u(cid:107)2 +

M + U ≤ L +

≤ L +

≤ L +

r(cid:107)u(cid:107)2 +

(u(cid:62)xt)2

t

t

r

ft

t (x)  are equal to 1
t x. Denote by χt = x(cid:62)

We show now how to derive in a simple way the bound of the
2 x(cid:62)Atx 
SOP [1] and the one of AROW [5]. Set ft(x) = 1
where At = At−1 + xtx(cid:62)
  r > 0 and A0 = I. The functions
ft are 1-strongly convex w.r.t. the norms (cid:107)x(cid:107)2
= x(cid:62)Atx.
2 x(cid:62)A−1
The dual functions of ft(x)  f∗
t x 
while (cid:107)x(cid:107)2
is x(cid:62)A−1
t A−1
t−1xt and
f∗
t A−1
mt = ytx(cid:62)
t−1θt. With these deﬁnitions it easy to see
that the conservative version of the algorithm corresponds di-
rectly to SOP. The aggressive version corresponds to AROW 
with a minor difference. In fact  the prediction of the algo-
rithm in Fig. 1 specialized in this case is ytw(cid:62)
 
r+χt
on the other hand AROW predicts with mt. The sign of the
predictions is the same  but here the aggressive version is up-
≤ 1  while AROW updates if mt ≤ 1.
dating when mt
To derive the bound  observe that using Woodbury matrix
2(r+χt). Using the second bound

= − m2

t xt = mt

−1
t−1θt)2
t A

−1
t−1xt)

r+χt

r

r

t

(cid:18)

t A−1
x(cid:62)

(u(cid:62)xt)2

t∈M∪U

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)
(cid:88)
(cid:88)

1
r

t∈M∪U

t∈M∪U

t

t xt + 2ytw(cid:62)

t xt − m2
(cid:118)(cid:117)(cid:117)(cid:116)r log(det(AT )) +
r + χt
(cid:88)
(cid:115)
(cid:88)

log(det(AT )) +

t∈M∪U

t∈M∪U

(cid:19)
(cid:18)

(cid:19)

2ytw(cid:62)

t xt − m2
r + χt

t

mt(2r − mt)
r(r + χt)

.

This bound recovers the SOP’s one in the conservative case  and improves slightly the one of AROW
for the aggressive case. It would be possible to improve the AROW bound even more  setting ηt to a
value different from 1 in margin error rounds. We leave the details for a longer version of this paper.

4.3 Diagonal updates for AROW

Both CW and AROW has an efﬁcient version that use diagonal matrices instead of full ones. In this
case the complexity of the algorithm becomes linear in dimension. Here we prove a mistake bound
for the diagonal version of AROW  using Corollary 2. We denote Dt = diag{At}  where At is
2 x(cid:62)Dtx. Setting ηt = 1  and using the second bound
deﬁned as in SOP and AROW  and ft(x) = 1
in Corollary 2 and Lemma 12 in [9]  we have1
d(cid:88)
(cid:88)

M + U ≤ (cid:88)
(cid:88)

(cid:118)(cid:117)(cid:117)(cid:116)uT DT u
d(cid:88)

(cid:32)(cid:80)

t∈M∪U x2
t i

t∈M∪U x2
t i

d(cid:88)

(cid:96)t(u) +

t∈M∪U

(cid:32)

(cid:33)

(cid:33)

(cid:33)

+ 2U

+ 1

log

i=1

r

r

(cid:118)(cid:117)(cid:117)(cid:116)(cid:107)u(cid:107)2 +

(cid:32)(cid:80)
(cid:118)(cid:117)(cid:117)(cid:116)r

+ 1

+ 2U .

(cid:96)t(u) +

log

=

x2
t i

1
r

u2
i

i=1

t∈M∪U

i=1

r

t∈M∪U

The presence of a mistake bound allows us to theoretically analyze the cases where this algorithm
could be advantageous respect to a simple Perceptron. In particular  for NLP data the features are
binary and it is often the case that most of the features are zero most of the time. On the other hand 

1We did not optimize the constant multiplying U in the bound.

5

(cid:88)

d(cid:88)

these “rare” features are usually the most informative ones (e.g. [8]). Fig. 2 shows the number of
times each feature (word) appears in two sentiment datasets vs the word rank. Clearly there are few
very frequent words and many rate words. These exact properties were used to originally derive the
CW algorithm. Our analysis justiﬁes this derivation. Concretely  the above considerations leads us
to think that the optimal hyperplane u will be such that

t i ≤(cid:88)
features appear in the sequence. In general each time that(cid:80)d

t i ≈(cid:88)

where I is the set of the informative and rare features and s is the maximum number of times these
t i ≤ s(cid:107)u(cid:107)2 with
s small enough  it is possible to show that  with an optimal tuning of r  this bound is better of the
Perceptron’s one. In particular  using a proof similar to the one in [1]  in the conservative version of
this algorithm  it is enough to have s < M R2

2d   and to set r = sM R2

M R2−2sd.

i s ≈ s(cid:107)u(cid:107)2
u2
(cid:80)

t∈M∪U x2

(cid:88)

i=1 u2
i

t∈M∪U

t∈M∪U

i∈I

i∈I

u2
i

u2
i

x2

x2

i=1

5 A New Adaptive Second Order Algorithm

We now introduce a new algorithm with an update rule that interpolates from adaptive-second-order-
information to ﬁxed-second-order-information. We start from the ﬁrst bound in Corollary 2. We set
  and A0 = I. This is similar to the regularization used
ft(x) = 1
in AROW and SOP  but here we have rt > 0 changing over time. Again  denote χt = x(cid:62)
t−1xt 
and set ηt = 1. With this choices  we obtain the bound

2 x(cid:62)Atx  where At = At−1 + xtx(cid:62)
(cid:88)

(cid:18) λ(u(cid:62)xt)2

t A−1
(cid:19)
− mt(2rt − mt)

M + U ≤ (cid:88)

λ(cid:107)u(cid:107)2

+

rt

 

t

(cid:96)t(u) +

+

χtrt

2λ(rt + χt)

2λ(rt + χt)

t∈M∪U

2

t∈M∪U

2rt

that holds for any λ > 0 and any choice of rt > 0. We would like to choose rt at each step to
minimize the bound  in particular to have a small value of the sum λ(u(cid:62)xt)2
λ(rt+χt). Altough
we do not know the values of (u(cid:62)xt)2 and λ  still we can have a good trade-off setting rt = χt
bχt−1
when χt ≥ 1
b and rt = +∞ otherwise. Here b is a parameter. With this choice we have that

+ χtrt

rt

χtrt
rt+χt

= 1

= χt(u(cid:62)xt)2b

rt+χt

  when χt ≥ 1

b . Hence we have

where in the last inequality we used an extension of Lemma 4 in [5] to varying values of rt. Tuning
λ we have
min (1  bχt) − bmt(2rt − mt)

M + U ≤ (cid:88)

(cid:96)t(u) + (cid:107)u(cid:107)R

(cid:114) 1

(cid:19)

.

bR2 + log det(AT )

t∈M∪U

t∈M∪U

rt + χt

This algorithm interpolates between a second order algorithm with adaptive second order informa-
tion  like AROW  and one with a ﬁxed second order information. Even the bound is in between
these two worlds. In particular the matrix At is updated only if χt ≥ 1
b   preventing its eigenvalues
from growing too much  as in AROW/SOP. We thus call this algorithm NAROW  since its is a new
adaptive algorithm  which narrows the range of possible eigenvalues of the matrix At. We illustrate
empirically its properties in the next section.

6

t∈M∪U

rt

2

b   and (u(cid:62)xt)2
− (cid:88)
M + U − λ(cid:107)u(cid:107)2
(cid:18) λbχt(u(cid:62)xt)2
≤ (cid:88)
(cid:88)

2(rt + χt)
χt(cid:107)u(cid:107)2R2
2(rt + χt)

≤ λb

t:bχt>1

1
2λ
λbR2(cid:107)u(cid:107)2 log det(AT ) +

t:bχt>1

+

≤ 1
2

(cid:96)t(u)

(cid:19)
(cid:88)

+

1

2λb

+

1
2λ

min

(cid:88)

t∈M∪U
1
2λ

t∈M∪U

min

mt(2rt − mt)
2λ(rt + χt)
mt(2rt − mt)
2λ(rt + χt)

mt(2rt − mt)
2λ(rt + χt)

 

(cid:88)
(cid:18) 1

b

t:bχt≤1

  χt

t∈M∪U

χt − (cid:88)
(cid:19)
− (cid:88)
(cid:18) 1
(cid:19)
− (cid:88)
(cid:115) (cid:88)
(cid:18)

t∈M∪U

  χt

b

t∈M∪U

Figure 3: Top: Four sequences used for training  the colors represents the ordering in the sequence from blue
to yellow  to red. Middle: cumulative number of mistakes of four algorithms on data with no labels noise.
Bottom: results when training using data with 10% label-noise.

6 Experiments

We illustrate the characteristics of our algorithm NAROW using a synthetic data generated in a
similar manner of previous work [4]. We repeat its properties for completeness. We generated 5  000
points in R20 where the ﬁrst two coordinates were drawn from a 45◦ rotated Gaussian distribution
with standard deviation 1 and 10. The remaining 18 coordinates were drawn from independent
Gaussian distributions N (0  8.5). Each point’s label depended on the ﬁrst two coordinates using
a separator parallel to the long axis of the ellipsoid  yielding a linearly separable set. Finally  we
ordered the training set in four different ways: from easy examples to hard examples (measured by
the signed distance to the separating-hyperplane)  from hard examples to easy examples  ordered by
their signed value of the ﬁrst feature  and by the signed value of the third (noisy) feature - that is by
xi × y for i = 1 and i = 3 - respectively. An illustration of these ordering appears in the top row of
Fig. 3  the colors code the ordering of points from blue via yellow to red (last points). We evaluated
four algorithms: version I of the passive-aggressive (PA-I) algorithm [3]  AROW [5]  AdaGrad [9]
and NAROW. All algorithms  except AdaGrad  have one parameter to be tuned  while AdaGrad has
two. These parameters were chosen on a single random set  and the plots summarizes the results
averaged over 100 repetitions.
The second row of Fig. 3 summarizes the cumulative number of mistakes averaged over 100 repe-
titions and the third row shows the cumulative number of mistakes where 10% artiﬁcial label noise
was used. (Mistakes are counted using the unnoisy labels.)
Focusing on the left plot  we observe that all the second order algorithms outperform the single
ﬁrst order algorithm - PA-I. All algorithms make few mistakes when receiving the ﬁrst half of the
data - the easy examples. Then all algorithms start to make more mistakes - PA-I the most  then
AdaGrad and closely following NAROW  and AROW the least. In other words  AROW was able to
converge faster to the target separating hyperplane just using “easy” examples which are far from
the separating hyperplane  then NAROW and AdaGrad  with PA-I being the worst in this aspect.
The second plot from the left  showing the results for ordering the examples from hard to easy. All
algorithms follow a general trend of making mistakes in a linear rate and then stop making mistakes
when the data is easy and there are many possible classiﬁers that can predict correctly. Clearly 

7

−20−1001020−25−20−15−10−50510152025 500100015002000250030003500400045005000−20−1001020−25−20−15−10−50510152025 500100015002000250030003500400045005000−20−1001020−25−20−15−10−50510152025 500100015002000250030003500400045005000−20−1001020−25−20−15−10−50510152025 50010001500200025003000350040004500500010002000300040005000100200300400500600700800ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500020040060080010001200ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500050100150200250300350ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500050100150200250300350ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad10002000300040005000100200300400500600700800ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500020040060080010001200ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500050100150200250300350ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGrad1000200030004000500050100150200250300350ExamplesCumulative Number of Mistakes PAAROWNAROWAdaGradAROW and NAROW stop making mistakes ﬁrst  then AdaGrad and PA-I last. A similar trend can
be found in the noisy dataset  with each algorithm making relatively more mistakes.
The third and fourth columns tell a similar story  although the plots in the third column summarize
results when the instances are ordered using the ﬁrst feature (which is informative with the second)
and the plots in the fourth column summarize when the instances are ordered using the third unin-
formative feature. In both cases  all algorithms do not make many mistakes in the beginning  then at
some point  close to the middle of the input sequence  they start making many mistakes for a while 
and then they converge. In terms of total performance: PA-I makes more mistakes  then AdaGrad 
AROW and NAROW. However  NAROW starts to make many mistakes before the other algorithms
and takes more “examples” to converge until it stopped making mistakes. This phenomena is further
shown in the bottom plots where label noise is injected.
We hypothesize that this relation is due to the fact that NAROW does not let the eigenvalues of the
matrix A to grow unbounded. Since its inverse is proportional to the effective learning rate  it means
that it does not allow the learning rate to drop too low as opposed to AROW and even to some extent
AdaGrad.
7 Conclusion
We presented a framework for online convex classiﬁcation  specializing it for particular losses  as the
hinge loss. This general tool allows to design theoretical motivated online classiﬁcation algorithms
and to prove their relative mistake bound. In particular it supports the analysis of aggressive updates.
Our framework also provided a missing bound for AROW for diagonal matrices. We have shown
its utility proving better bounds for known online algorithms  and proposing a new algorithm  called
NAROW. This is a hybrid between adaptive second order algorithms  like AROW and SOP  and a
static second order one. We have validated it using synthetic datasets  showing its robustness to the
malicious orderings of the sample  comparing it with other state-of-art algorithms. Future work will
focus on exploring the new possibilities offered by our framework and on testing NAROW on real
world data.
Acknowledgments We thank Nicol`o Cesa-Bianchi for his helpful comments. Francesco Orabona was
sponsored by the PASCAL2 NoE under EC grant no. 216886. Koby Crammer is a Horev Fellow  supported by
the Taub Foundations. This work was also supported by the German-Israeli Foundation grant GIF-2209-1912.

− T(cid:88)

t ∇f∗
t−1(θt) − ηtz(cid:62)
T(cid:88)
T(cid:88)

t=1 ηtu(cid:62)zt − 1
∆t ≤ 1
λ

t the Fenchel dual of ft  and ∆t = f∗

A Appendix
Proof of Lemma 1. Deﬁne by f∗

(cid:80)T
t=1 ∆t = f∗
t (θt) − f∗
(cid:107)zt(cid:107)2
f∗
f∗
in [13]. Moreover using the Fenchel-Young inequality  we have that 1
λ
u(cid:62)θT +1 − 1

λ fT (λu) = −(cid:80)T

T (θT +1) − f∗

t−1(θt) ≤ f∗

t (θt) − f∗

0 (θ1) = f∗

t (θt) + η2

t
2βt

t

T (θT +1). Moreover we have that ∆t = f∗

λ f∗
λ fT (λu). Hence putting all togheter we have

t=1 ∆t = 1

t (θt+1) − f∗

t (θt+1) − f∗
(cid:80)T

t−1(θt). We have
t (θt) +
  where we used Theorem 6
T (θT +1) ≥

ηtu(cid:62)zt − 1
λ

fT (λu) ≤ 1
λ

t (θt) − f∗
(f∗

t−1(θt) − ηtw(cid:62)

t zt +

η2
t
2βt

(cid:107)zt(cid:107)2
f∗

t

) 

t=1

t=1

t=1
where we used the deﬁnition of wt in Algorithm 1.
Proof of Corollary 1. By convexity  (cid:96)(wt  xt  yt) − (cid:96)(u  xt  yt) ≤ z(cid:62)
t (wt − u)  so setting λ = 1
in Lemma 1 we have the stated bound. For the additional statement  using Lemma 12 in [16] and
t+1(x)  so B ≤ 0. The additional statement on B is
ft(x) ≤ ft+1(x) we have that f∗
t (x) ≥
proved using Lemma 12 in [16]. Using it  we have that ft(x) ≤ ft+1(x) implies that f∗
t+1(x)  so we have that B ≤ 0.
f∗
(cid:33)
T(cid:88)

Proof of Corollary 2. Lemma 1  the condition on the loss (2)  and the hypothesis on fT gives us

ηt(1 − (cid:96)t(u)) ≤ − T(cid:88)

ηtu(cid:62)zt ≤ λfT (u) +

+ B − ηtz(cid:62)

t (x) ≥ f∗

(cid:32) η2

T(cid:88)

.

t wt

1
λ

t=1

t (cid:107)zt(cid:107)2
f∗
2βt

t

t=1

t=1

Note that λ is free  so choosing its optimal value we get the second bound.

8

References
[1] N. Cesa-Bianchi  A. Conconi  and C. Gentile. A second-order Perceptron algorithm. SIAM

Journal on Computing  34(3):640–668  2005.

[2] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[3] K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive-aggressive

algorithms. Journal of Machine Learning Research  7:551–585  2006.

[4] K. Crammer  M. Dredze  and F. Pereira. Exact Convex Conﬁdence-Weighted learning. Ad-

vances in Neural Information Processing Systems  22  2008.

[5] K. Crammer  A. Kulesza  and M. Dredze. Adaptive regularization of weight vectors. Advances

in Neural Information Processing Systems  23  2009.

[6] K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Jour-

nal of Machine Learning Research  3:951–991  2003.

[7] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other

Kernel-Based Learning Methods. Cambridge University Press  2000.

[8] M. Dredze  K. Crammer  and F. Pereira. Online Conﬁdence-Weighted learning. Proceedings

of the 25th International Conference on Machine Learning  2008.

[9] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Technical Report 2010-24  UC Berkeley Electrical Engineering
and Computer Science  2010. Available at http://cs.berkeley.edu/˜jduchi/
projects/DuchiHaSi10.pdf.

[10] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the Perceptron algorithm.

Machine Learning  pages 277–296  1999.

[11] C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3):265–299  2003.
[12] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in

costs. In Proc. of the 21st Conference on Learning Theory  2008.

[13] S. Kakade  S. Shalev-Shwartz  and A. Tewari. On the duality of strong convexity and strong
smoothness: Learning applications and matrix regularization. Technical report  TTI  2009.
http://www.cs.huji.ac.il/ shais/papers/KakadeShalevTewari09.pdf.

[14] J. Kivinen  A. Smola  and R. Williamson. Online learning with kernels. IEEE Trans. on Signal

Processing  52(8):2165–2176  2004.

[15] A. Rakhlin and A. Tewari. Lecture notes on online learning. Technical report  2008. Avail-
able at http://www-stat.wharton.upenn.edu/˜rakhlin/papers/online_
learning.pdf.

[16] S. Shalev-Shwartz. Online learning: Theory  algorithms  and applications. Technical report 

The Hebrew University  2007. PhD thesis.

[17] S. Shalev-Shwartz and Y. Singer. A primal-dual perspective of online learning algorithms.

Machine Learning Journal  2007.

[18] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization.

In Advances in Neural Information Processing Systems 22  pages 2116–2124. 2009.

9

,Agnieszka Grabska-Barwinska
Jeff Beck
Alexandre Pouget
Peter Latham
Sarath Chandar A P
Stanislas Lauly
Hugo Larochelle
Mitesh Khapra
Balaraman Ravindran
Vikas Raykar
Amrita Saha