2018,Fairness Through Computationally-Bounded Awareness,We study the problem of fair classification within the versatile framework of Dwork et al. [ITCS '12]  which assumes the existence of a metric that measures similarity between pairs of individuals.  Unlike earlier work  we do not assume that the entire metric is known to the learning algorithm; instead  the learner can query this *arbitrary* metric a bounded number of times.  We propose a new notion of fairness called *metric multifairness* and show how to achieve this notion in our setting.
Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) "comparison sets" over pairs of individuals.  At a high level  metric multifairness guarantees that *similar subpopulations are treated similarly*  as long as these subpopulations are identified within the class C.,Fairness Through Computationally-Bounded

Awareness

Michael P. Kim⇤
Stanford University

mpk@cs.stanford.edu

Omer Reingold⇤
Stanford University

reingold@stanford.edu

Guy N. Rothblum†

Weizmann Institute of Science
rothblum@alum.mit.edu

Abstract

We study the problem of fair classiﬁcation within the versatile framework of
Dwork et al. [6]  which assumes the existence of a metric that measures similarity
between pairs of individuals. Unlike earlier work  we do not assume that the
entire metric is known to the learning algorithm; instead  the learner can query
this arbitrary metric a bounded number of times. We propose a new notion of
fairness called metric multifairness and show how to achieve this notion in our
setting. Metric multifairness is parameterized by a similarity metric d on pairs of
individuals to classify and a rich collection C of (possibly overlapping) “comparison
sets" over pairs of individuals. At a high level  metric multifairness guarantees that
similar subpopulations are treated similarly  as long as these subpopulations are
identiﬁed within the class C.

1

Introduction

More and more  machine learning systems are being used to make predictions about people. Algo-
rithmic predictions are now being used to answer questions of signiﬁcant personal consequence; for
instance  Is this person likely to repay a loan? [24] or Is this person likely to recommit a crime?
[1]. As these classiﬁcation systems have become more ubiquitous  concerns have also grown that
classiﬁers obtained via machine learning might discriminate based on sensitive attributes like race 
gender  or sexual orientation. Indeed  machine-learned classiﬁers run the risk of perpetuating or
amplifying historical biases present in the training data. Examples of discrimination in classiﬁcation
have been well-illustrated [24  1  5  19  13  3]; nevertheless  developing a systematic approach to
fairness has been challenging. Often  it feels that the objectives of fair classiﬁcation are at odds with
obtaining high-utility predictions.
In an inﬂuential work  Dwork et al. [6] proposed a framework to resolve the apparent conﬂict
between utility and fairness  which they call “fairness through awareness." This framework takes the
perspective that a fair classiﬁer should treat similar individuals similarly. The work formalizes this
abstract goal by assuming access to a task-speciﬁc similarity metric d on pairs of individuals. The
proposed notion of fairness requires that if the distance between two individuals is small  then the
predictions of a fair classiﬁer cannot be very different. More formally  for some small constant ⌧  0 
we say a hypothesis f : X! [1  1] satisﬁes (d  ⌧ )-metric fairness1 if the following (approximate)
Lipschitz condition holds for all pairs of individuals from the population X .
|f (x)  f (x0)| d(x  x0) + ⌧
(1)
Subject to these intuitive similarity constraints  the classiﬁer may be chosen to maximize utility. Note
that  in general  the metric may be designed externally (say  by a regulatory agency) to address legal

8x  x0 2X⇥X :

⇤Supported by NSF Grant CCF-1763299.
†Supported by ISF grant No. 5219/17.
1Note the deﬁnition given in [6] is slightly different; in particular  they propose a more general Lipschitz

condition  but ﬁx ⌧ = 0.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

and ethical concerns  independent from the task of learning. In particular  in certain settings  the
metric designers may have access to a different set of features than the learner. For instance  perhaps
the metric designers have access to sensitive attributes  but for legal  social  or pragmatic reasons  the
learner does not. In addition to its conceptual simplicity  the modularity of fairness through awareness
makes it a very appealing framework. Currently  there are many (sometimes contradictory) notions of
what it means for a classiﬁer to be fair [19  5  9  13  14]  and there is much debate on which deﬁnitions
should be applied in a given context. Discrimination comes in many forms and classiﬁcation is used
in a variety of settings  so naturally  it is hard to imagine any universally-applicable deﬁnition of
fairness. Basing fairness on a similarity metric offers a ﬂexible approach for formalizing a variety of
guarantees and protections from discrimination.
Still  a challenging aspect of this approach is the assumption that the similarity metric is known
for all pairs of individuals.2 Deciding on an appropriate metric is itself a delicate matter and could
require human input from sociologists  legal scholars  and specialists with domain expertise. For
instance  in the loan repayment example  a simple  seemingly-objective metric might be a comparison
of credit scores. A potential concern  however  is that these scores might themselves be biased
(i.e. encode historical discriminations). In this case  a more nuanced metric requiring human input
may be necessary. Further  if the metric depends on features that are latent to the learner (e.g. some
missing sensitive feature) then the metric could appear arbitrarily complex to the learner. As such 
in many realistic settings  the resulting metric will not be a simple function of the learner’s feature
vectors of individuals.
In most machine learning applications  where the universe of individuals is assumed to be very
large  even writing down an appropriate metric could be completely infeasible. In these cases  rather
than require the metric value to be speciﬁed for all pairs of individuals  we could instead ask a
panel of experts to provide similarity scores for a small sample of pairs of individuals. While it is
information-theoretically impossible to guarantee metric fairness from a sampling-based approach 
we still might hope to provide a strong  provable notion of fairness that maintains the theoretical
appeal and practical modularity of the fairness through awareness framework.
In this work  we propose a new theoretical framework for fair classiﬁcation based on fairness through
awareness – which we dub “fairness through computationally-bounded awareness” – that eliminates
the considerable issue of requiring the metric to be known exactly. Our approach maintains the
simplicity and ﬂexibility of fairness through awareness  but provably only requires a small number of
random samples from the underlying metric  even though we make no structural assumptions about the
metric. In particular  our approach works even if the metric provably cannot be learned. Speciﬁcally 
our notion will require that a fair classiﬁer treat similar subpopulations of individuals similarly  in a
sense we will make formal next. While our deﬁnition relaxes fairness through awareness  we argue
that it still protects against important forms of discrimination that the original work aimed to combat;
further  we show that stronger notions necessarily require a larger sample complexity from the metric.
As in [6]  we investigate how to learn a classiﬁer that achieves optimal utility under similarity-based
fairness constraints  assuming a weaker model of limited access to the metric. We give positive and
negative results that show connections between achieving our fairness notion and learning.

2 Setting and metric multifairness

Notation. Let X✓ Rn denote the universe over individuals we wish to classify  where x 2X
encodes the features of an individual. Let D denote the data distribution over individuals and labels
supported on X ⇥ {1  1}; denote by x  y ⇠D a random draw from this distribution. Additionally 
let M denote the metric sample distribution over pairs of individuals. For a subset S ✓X⇥X  
we denote by (x  x0) ⇠ S a random draw from the distribution M conditioned on (x  x0) 2 S. Let
d : X⇥X! [0  2] denote the underlying fairness metric that maps pairs to their associated distance.3
Our learning objective will be to minimize the expectation over D of some convex loss function
L : [1  1] ⇥ [1  1] ! R+ over a convex hypothesis class F  subject to the fairness constraints.
We focus on agnostically learning the hypothesis class of linear functions with bounded weight; for
some constant B > 0  let F = [B  B]n. For w 2F   deﬁne fw(x) = hw  xi  projecting fw(x)

2Indeed  [6] identiﬁes this assumption as “one of the most challenging aspects” of the framework.
3In fact  all of our results hold for a more general class of non-negative symmetric distance functions.

2

onto [1  1] to get a valid prediction. We assume kxk1  1 for all x 2X ; this is without loss of
generality  by normalizing and increasing B appropriately.
The focus on linear functions is not too restrictive; in particular  by increasing the dimension to
n0 = O(nk)  we can learn any degree-k polynomial function of the original features. By increasing
k  we can approximate increasingly complex functions.

2.1 Metric multifairness
We deﬁne our relaxation of metric fairness with respect to a rich class of statistical tests on the pairs
of individuals. Let a comparison be any subset of the pairs of X⇥X . Our deﬁnition  which we call
metric multifairness  is parameterized by a collection of comparisons C✓ 2X⇥X and requires that a
hypothesis appear Lipschitz according to all of the statistical tests deﬁned by the comparisons S 2C .
Deﬁnition (Metric multifairness). Let C✓ 2X⇥X be a collection of comparisons and let d : X⇥X !
[0  2] be a metric. For some constants ⌧  0  a hypothesis f is (C  d ⌧ )-metric multifair if for all
S 2C  
(2)

E

(x x0)⇠S⇥|f (x)  f (x0)|⇤  E

(x x0)⇠S⇥d(x  x0)⇤ + ⌧.

To begin  note that metric multifairness is indeed a relaxation of metric fairness; if we take the
collection C = {{(x  x0)} : x  x0 2X⇥X}
to be the collection of all pairwise comparisons  then
(C  d ⌧ )-metric multifairness is equivalent to (d  ⌧ )-metric fairness.
In order to achieve metric multifairness from a small sample from the metric  however  we need a
lower bound on the density of each comparison in C; in particular  we can’t hope to enforce metric
fairness from a small sample. For some > 0  we say that a collection of comparisons C is -large if
for all S 2C   Pr(x x0)⇠M[(x  x0) 2 S]  . A natural next choice for C would be a collection of
comparisons that represent the Cartesian products between traditionally-protected groups  deﬁned by
race  gender  etc. In this case  as long as the minority populations are not too small  then a random
sample from the metric will concentrate around the true expectation  and we could hope to enforce
this statistical relaxation of metric fairness. While this approach is information-theoretically feasible 
its protections are very weak.
To highlight this weakness  suppose we want to predict the probability individuals will repay a loan 
and our metric is an adjusted credit score. Even after adjusting scores  two populations P  Q ✓X
(say  deﬁned by race) may have large average distance because overall P has better credit than Q; still 
within P and Q  there may be signiﬁcant subpopulations P 0 ✓ P and Q0 ✓ Q that should be treated
similarly (possibly representing the qualiﬁed members of each group). In this case  a coarse statistical
relaxation of metric fairness will not require that a classiﬁer treat P 0 and Q0 similarly; instead 
the classiﬁer could treat everyone in P better than everyone in Q – including treating unqualiﬁed
members of P better than qualiﬁed members of Q. Indeed  the weaknesses of broad-strokes statistical
deﬁnitions served as motivation for the original work of [6]. We would like to choose a class C
that strengthens the fairness guarantees of metric multifairness  but maintains its efﬁcient sample
complexity.

Computationally-bounded awareness. While we can deﬁne metric multifairness with respect to
any collection C  typically  we will think of C as a rich class of overlapping subsets; equivalently 
we can think of the collection C as an expressive class of boolean functions  where for S 2C  
cS(x  x0) = 1 if and only if (x  x0) 2 S. In particular  C should be much more expressive than simply
deﬁning comparisons across traditionally-protected groups. The motivation for choosing such an
expressive class C is exempliﬁed in the following proposition.
Proposition 1. Suppose there is some S 2C   such that E(x x0)⇠S[d(x  x0)]  ". Then if f is
(C  d ⌧ )-metric multifair  then f satisﬁes (d  (" + ⌧ )/p)-metric fairness for at least a (1 p)-fraction
of the pairs in S.
That is  if there is some subset S 2C that identiﬁes a set of pairs whose metric distance is small  then
any metric multifair hypothesis must also satisfy the stronger individual metric fairness notion on many
pairs from S. This effect will compound if many different (possibly overlapping) comparisons are
identiﬁed that have small average distance. We emphasize that these small-distance comparisons are
not known before sampling from the metric; indeed  this would imply the metric was (approximately)
known a priori. Still  if the class C is rich enough to correlate well with various comparisons that

3

reveal signiﬁcant information about the metric  then any metric multifair hypothesis will satisfy
individual-level fairness on a signiﬁcant fraction of the population!
While increasing the expressiveness of C increases the strength of the fairness guarantee  in order to
learn from a small sample  we cannot choose C to be arbitrarily complex. Thus  in choosing C we
must balance the strength of the fairness guarantee with the information bottleneck in accessing d
through random samples. Our resolution to these competing needs is complexity-theoretic: while
information-theoretically  we can’t hope to ensure fair treatment across all subpopulations  we can
hope ensure fair treatment across efﬁciently-identiﬁable subpopulations. For instance  if we take C
to be a family deﬁned according to some class of computations of bounded dimension – think  the
set of conjunctions of a constant number of boolean features or short decision trees – then we can
hope to accurately estimate and enforce the metric multifairness conditions. Taking such a bounded C
ensures that a hypothesis will be fair on all comparisons identiﬁable within this computational bound.
This is the sense in which metric multifairness provides fairness through computationally-bounded
awareness.

2.2 Learning model

Metric access. Throughout  our goal is to learn a hypothesis from noisy samples from the metric
that satisﬁes multifairness. Speciﬁcally  we assume an algorithm can obtain a small number of
independent random metric samples (x  x0  (x  x0)) 2X⇥X⇥ [0  2] where (x  x0) ⇠M is drawn
at random over the distribution of pairs of individuals  and (x  x0) is a random variable of bounded
magnitude with E[(x  x0)] = d(x  x0).
We emphasize that this is a very limited access model. As Theorem 2 shows we achieve (C  d ⌧ )-
metric multifairness from a number of samples that depends logarithmically on the size of C indepen-
dent of the complexity of the similarity metric.4 Recall that d : X⇥X! [0  2] can be an arbitrary
symmetric function; thus  the learner does not necessarily have enough information to learn d. Still 
for exponentially-sized C  the learner can guarantee metric multifairness from a polynomial-sized
sample  and the strength of the guarantee will scale up with the complexity of C (as per Propsition 1).
In order to ensure a strong notion of fairness  we assume that the subpopulations we wish to protect
are well-represented in the pairs drawn from M. This assumption  while important  is not especially
restrictive  as we think of the metric samples as coming from a regulatory committee or ethically-
motivated party; in other words  in practical settings  it is reasonable to assume that one can choose
the metric sampling distribution based on the notion of fairness one wishes to enforce.

Label access. When we learn linear families  our goal will be to learn from a sample of labeled
examples. We assume the algorithm can ask for independent random samples x  y ⇠D .
Measuring optimality. To evaluate the utility guarantees of our learned predictions  we take a
comparative approach. Suppose H✓ 2X⇥X is a collection of comparisons. For "  0  we say a
hypothesis f is (H " )-optimal with respect to F  if
[L(f (x)  y)]  E
x y⇠D

[L(f⇤(x)  y)] + "

(3)

E

x y⇠D

where f⇤ 2F is an optimal (H  d  0)-metric multifair hypothesis.
3 Learning a metric multifair hypothesis

As in [6]  we formulate the problem of learning a fair set of predictions as a convex program. Our
objective is to minimize the expected loss Ex y⇠D[L(f (x)  y)]  subject to the multifairness constraints
deﬁned by C.5 Speciﬁcally  we show that a simple variant of stochastic gradient descent due to [20]
learns such linear families efﬁciently.

dimension  metric entropy  etc.) through a uniform convergence argument.

4Alternatively  for continuous classes of C  we can replace log(|C|) with some notion of dimension (VC-
5For the sake of presentation  throughout the theorem statements  we will assume that L is O(1)-Lipschitz

on the domain of legal predictions/labels to guarantee bounded error; our results are proved more generally.

4

⌧ 2

Theorem 2. Suppose   ⌧   > 0 and C✓ 2X⇥X is -large. With probability at least 1   
stochastic switching subgradient descent learns a hypothesis w 2F that is (C  d ⌧ )-metric multifair
and (C  O(⌧ ))-optimal with respect to F in O⇣ B2n2 log(n/)
⌘ iterations from m = ˜O⇣ log(|C|/)
⌘
metric samples. Each iteration uses at most 1 labeled example and can be implemented in
˜O (|C| · n · poly(1/  1/⌧  log(1/))) time.
Note that the metric sample complexity depends logarithmically on |C|. Thus  information-
theoretically  we can hope to enforce metric mutlifairness with a class C that grows exponentially and
still be efﬁcient. While the running time of each iteration depends on |C|  note that the number of
iterations is independent of |C|. In Section 4  we show conditions on |C| under which we can speed
up the running time of each iteration to depend logarithmically on |C|.
We give a description of the switching subgradient method in Algorithm 1. At a high level  at each
iteration  the procedure checks to see if any constraint is signiﬁcantly violated. If it ﬁnds a violation 
it takes a (stochastic) step towards feasibility. Otherwise  it steps according a stochastic subgradient
for the objective.
For convenience of analysis  we deﬁne the residual on the constraint deﬁned by S as follows.

⌧ 2

RS(w) = E

(x x0)⇠S⇥|fw(x)  fw(x0)|⇤  E

(x x0)⇠S⇥d(x  x0)⇤

Note that RS(w) is convex in the predctions fw(x) and thus  for linear families is convex in w.
We describe the algorithm assuming access to the following estimators  which we can implement
efﬁciently (in terms of time and samples). First  we assume we can estimate the residual ˆRS(w) on

each S 2C with tolerance ⌧ such that for all w 2F  RS(w)  ˆRS(w)  ⌧. Next  we assume
a vector-valued random variable where E[r(w)w] 2 @(w). We assume access to stochastic

access to a stochastic subgradient oracle for the constraints and the objective. For a function (w) 
let @(w) denote the set of subgradients of  at w. We abuse notation  and let r(w) refer to
subgradients for @RS(w) for all S 2C and @L(w). We include a full analysis of the algorithm and
proof of Theorem 2 in the Appendix.

(4)

3.1 Post-processing for metric multifairness
One speciﬁc application of this result is as a way post-process learned predictions to ensure fairness.
In particular  suppose we are given the predictions from some pre-trained model for N individuals 
but are concerned that these predictions may not be fair. We can use Algorithm 1 to post-process these
labels to select near-optimal metric multifair predictions. Note these predictions will be optimal with
respect to the unconstrained family of predictions – not just predictions that come from a speciﬁc
hypothesis class (like linear functions).
Speciﬁcally  in this setting we can represent an unconstrained set of predictions as a linear hypothesis
in N dimensions: take B = 1  and let the feature vector for xi 2X be the ith standard basis vector.
Algorithm 1: Switching Subgradient Descent
Let ⌧> 0  T 2 N  and C✓ 2X⇥X .
Initialize w0 2F = [B  B]n; W = ;
For k = 1  . . .   T :

• If 9S 2C such that ˆRS(wk) > 4⌧/ 5:

– Sk any S 2C such that ˆRS(wk) > 4⌧/ 5
– wk+1 wk  ⌧

M 2rRSk (wk)

// some constraint violated

/* step according to constraint
project onto F if necessary */
// no violations found
// update set of feasible iterates
/* step according to objective
project onto F if necessary */
// output average of feasible iterates

• Else:

– W W [{ wk}
– wk+1 wk  ⌧
|W| ·Pw2W w

GM rL(wk)

Output ¯w = 1

5

Then  we can think of the input labels to Algorithm 1 to be the output of any predictor that was trained
separately.6 For instance  if we have learned a highly-accurate model  but are unsure of its fairness 
we can instantiate our framework with  say  the squared loss between the original predictions and the
returned predictions; then  we can view the program as a procedure to project the highly-accurate
predictions onto the set of metric multifair predictions. Importantly  our procedure only needs a small
set of samples from the metric and not the original data used to train the model.
Post-processing prediction models for fairness has been studied in a few contexts [25  14  18]. This
post-processing setting should be contrasted to these settings. In our setting  the predictions are
not required to generalize out of sample (in terms of loss or fairness). On the one hand  this means
the metric multifairness guarantee does not generalize outside the N individuals; on the other hand 
because the predictions need not come from a bounded hypothesis class  their utility can only improve
compared to learning a metric multifair hypothesis directly.
In addition to preserving the utility of previously-trained classiﬁers  separating the tasks of training
for utility and enforcing fairness may be desirable when intentional malicious discrimination may
be anticipated. For instance  when addressing the forms of racial proﬁling that can occur through
targeted advertising (as described in [6])  we may not expect self-interested advertisers to adhere
to classiﬁcation under strict fairness constraints  but it stands to reason that prominent advertising
platforms might want to prevent such blatant abuses of their platform. In this setting  the platform
could impose metric multifairness after the advertisers specify their ideal policy.

4 Reducing search to agnostic learning

As presented above  the switching subgradient descent method converges to a nearly-optimal point
in a bounded number of subgradient steps  independent of |C|. The catch is that at the beginning of
each iteration  we need to search over C to determine if there is a signiﬁcantly violated multifairness
constraint. As we generally want to take C to be a rich class of comparisons  in many cases |C| will
be prohibitive. As such  we would hope to ﬁnd violated constraints in sublinear time  preferably
even poly-logarithmic in |C|. Indeed  we show that if a concept class C admits an efﬁcient agnostic
learner  then we can solve the violated constraint search problem over the corresponding collection of
comparisons efﬁciently.
Agnostic learning can be phrased as a problem of detecting correlations. Suppose g  h : U! [1  1] 
and let D be some distribution supported on U. We denote the correlation between g and h on D as
hg  hi = Ei⇠D[g(i) · h(i)]. We let C✓ [1  1]U denote the concept class and H✓ [1  1]U denote
the hypothesis class. The task of agnostic learning can be stated as follows: given sample access
over some distribution (i  g(i)) ⇠D⇥ [1  1] to some function g 2 [1  1]N  ﬁnd some hypothesis
h 2H that is comparably correlated with g as the best c 2C . That is  given access to g  an agnostic
learner with accuracy " for concept class C returns some h from the hypothesis class H such that

hg  hi + "  max

c2C hg  ci.

(5)

An agnostic learner is typically considered efﬁcient if it runs in polynomial time in log(|C|) (or an
appropriate notion of dimension of C)  1/"  and log(1/d). Additionally  distribution-speciﬁc learners
and learners with query access to the function have been studied [12  7]. In particular  membership
queries tend to make agnostic learning easier. Our reduction does not use any metric samples other
than those that the agnostic learner requests. Thus  if we are able to query a panel of experts according
to the learner  rather than randomly  then an agnostic learner that uses queries could be used to speed
up our learning procedure.
Theorem 3. Suppose there is an algorithm A for agnostic learning the concept class C with hy-
pothesis class H that achieves accuracy " with probability 1   in time TA("  ) from mA("  )
labeled samples. Suppose that C is -large. Then  there is an algorithm that  given access to
T = ˜O⇣ B2n2
⌧ 2 ⌘ labeled examples  outputs a set of predictions that are (C  d ⌧ )-metric multifair
and (H  O(⌧ ))-optimal with respect to F = [B  B]n that runs in time ˜O⇣ TA(⌧ /T )·B2n2
⌘ and
requires m = ˜O⇣ log(|C|)

⌘ metric samples.

6Nothing in our analysis required labels y 2{ 1  1}; we can instead take the labels y 2 [1  1].

⌧ 2 + nA(⌧ /T )

⌧ 2

2⌧ 2

6

When we solve the convex program with switching subgradient descent  at the beginning of each
iteration  we check if there is any S 2C such that the residual quantity RS(w) is greater than
some threshold. If we ﬁnd such an S  we step according to the subgradient of RS(w). In fact 
the proof of the convergence of switching subgradient descent reveals that as long as when there
is some S 2C where RS(w) is in violation  we can ﬁnd some RS0(w) >⇢ for some constant ⇢ 
where S0 2H✓ [1  1]X⇥X   then we can argue that the learned hypothesis will be (C  d ⌧ )-metric
multifair and achieve utility commensurate with the best (H  d  0)-metric multifair hypothesis.
We show a general reduction from the problem of searching for a violated comparison S 2C to
the problem of agnostic learning over the corresponding family of boolean functions. In particular 
recall that for a collection of comparisons C✓ 2X⇥X   we can also think of C as a family of boolean
concepts  where for each S 2C   there is an associated boolean function cS : X ⇥ X ! {1  1}
where cS(xi  xj) = 1 if and only if (xi  xj) 2 S. We frame this search problem as an agnostic
such that any
learning problem  where we design a set of “labels” for each pair (x  x0) 2X⇥X
function that is highly correlated with these labels encodes a way to update the parameters towards
feasibility.

Proof. Recall the search problem: given a current hypothesis fw  is there some S 2C such that
RS(w) = E(x x0)⇠S[|fw(xi)  fw(xj)| d(xi  xj)] >⌧ ? Consider the labeling each pair (xi  xj)
with v(xi  xj) = |fw(xi)  fw(xj)| d(xi  xj). Let ⇢ = RX⇥X (w); note that we can treat ⇢
as a constant for all S 2C . Further  suppose S 2C is such that RS(w) >⌧   or equivalently 
E(xi xj )⇠S[v(xi  xj)] >⌧ . Then  by the assumption that C is -large  the correlation between the
corresponding boolean function cS and labels v can be lower bounded as hcS  vi > 2⌧  ⇢. Suppose
accuracy. Then  we know that
we have an agnostic learner that returns a hypothesis h with "<⌧
hh  vi  ⌧  ⇢ by the lower bound on the optimal cS 2C . Then  consider the function Rh(w)
deﬁned as follows.

(x x0)⇠X⇥X✓ h(x  x0) + 1

E

2

◆ · v(x  x0)

RSh(w) =

= hh  vi + ⇢

2

(6)

(7)

Thus  given that there exists some S 2C where RS(w) >⌧   we can ﬁnd some real-valued comparison
Sh(x  x0) = h(x x0)+1

  such that RSh(w) =⌦( hh  vi + ⇢)  ⌦(⌧ ).

2

Discovering a violation of at least ⌦(⌧ ) guarantees ⌦(2⌧ 2) progress in the duality gap at each step 
so the theorem follows from the analysis of Algorithm 1.

5 Hardness of learning metric multifair hypotheses

In this section  we show that our algorithmic results cannot be improved signiﬁcantly. In particular 
we focus on the post-processing setting of Section 3.1. We show that the metric sample complexity is
tight up to a ⌦(log log(|C|)) factor unconditionally. We also show that some learnability assumption
on C is necessary in order to achieve a high-utility (C  d ⌧ )-metric multifair predictions efﬁciently. In
particular  we give a reduction from inverting a boolean concept c 2C to learning a hypothesis f
that is metric multifair on a collection H derived from C  where the metric samples from d encode
information about the concept c. Recall  that for any H and d  we can always output a trivial
(H  d  0)-metric multifair hypothesis by outputting a constant hypothesis. This leads to a subtlety in
our reductions  where we need to leverage the learner’s ability to simultaneously satisfy the metric
multifairness constraints and achieve high utility.
Both lower bounds follow the same general construction. Suppose we have some boolean concept
class C ✓ {1  1}X0 for some universe X0. We will construct a new universe X = X0 [X 1 and
deﬁne a collection of “bipartite” comparisons over subsets of X0 ⇥X 1. Then  given samples from
(x0  c(x0))  we deﬁne corresponding metric values where d(x0  x1) is some function of c(x0) for all
x1 2X 1. Finally  we need to additionally encode the objective of inverting c into labels for x0 2X 0 
such that to obtain good loss  the post-processor must invert c on X0. We give a full description of
the reduction in the Appendix.

7

Lower bounding the sample complexity. While we argued earlier that some relaxation of metric
fairness is necessary if we want to learn from a small set of metric samples  it is not clear that
multifairness with respect to C is the strongest relaxation we can obtain. In particular  we might
hope to guarantee fairness on all large comparisons  rather than just a ﬁnite class C. The following
theorem shows that such a hope is misplaced: in order for an algorithm to guarantee that the Lipschitz
condition holds in expectation over a ﬁnite collection of large comparisons C  then either the algorithm
takes ⌦(log |C|) random metric samples  or the algorithm outputs a set of nearly useless predictions.
For concreteness  we state the theorem in the post-processing setting of Section 3.1; the construction
can be made to work in the learning setting as well.
Theorem 4. Let   ⌧ > 0 be constants and suppose A is an algorithm that has random sample
access to d and outputs a (C  d ⌧ )-metric multifair set of predictions for -large C. Then  A takes
⌦(log |C|) random samples from d or outputs a set of predictions with loss that approaches the loss
achievable with no metric queries.
The construction uses a reduction from the problem of learning a linear function; we then appeal to a
lower bound from linear algebra on the number of random queries needed to span a basis.

Hardness from pseudorandom functions. Our reduction implies that a post-processing algorithm
for (C  d ⌧ )-metric multifairness with respect to an arbitrary metric d gives us a way of distinguishing
functions in C from random.
Proposition 5 (Informal). Assuming one-way functions exist  there is no efﬁcient algorithm for
computing (C ⌧ )-optimal (C  d ⌧ )-metric multifair predictions for general C  d  and constant ⌧.
Essentially  without assumptions that C is a learnable class of boolean functions  some nontrivial
running time dependence on |C| is necessary. The connection between learning and pseudorandom
functions [23  11] is well-established; under stronger cryptographic assumptions as in [2]  the
reduction implies that a running time of ⌦(|C|↵) is necessary for some constant ↵> 0.
6 Related works and discussion

Many exciting recent works have investigated fairness in machine learning. In particular  there is
much debate on the very deﬁnitions of what it means for a classiﬁer to be fair [19  4  21  13  5  14].
Beyond the work of Dwork et al. [6]  our work bears most similarity to two recent works of Hébert-
Johnson et al. and Kearns et al. [14  16]. As in this work  both of these papers investigate notions of
fairness that aim to strengthen the guarantees of statistical notions  while maintaining their practicality.
These works also both draw connections between achieving notions of fairness and efﬁcient agnostic
learning. In general  agnostic learning is considered a notoriously hard computational problem
[15  17  8]; that said  in the context of fairness in machine learning  [16] show that using heuristic
methods to agnostically learn linear hypotheses seems to work well in practice.
Metric multifairness does not directly generalize either [14] or [16]  but we argue that it provides a
more ﬂexible alternative to these approaches for subpopulation fairness. In particular  these works
aim to achieve speciﬁc notions of fairness – either calibration or equalized error rates – across a rich
class of subpopulations. As has been well-documented [19  4  21]  calibration and equalized error
rates  in general  cannot be simultaneously satisﬁed. Often  researchers frame this incompatibility
as a choice: either you satisfy calibration or you satisfy equalized error rates; nevertheless  there
are many applications where some interpolation between accuracy (à la calibration) and corrective
treatment (à la equalized error rates) seems appropriate.
Metric-based fairness offers a way to balance these conﬂicting fairness desiderata. In particular  one
could design a similarity metric that preserves accuracy in predictions and separately a metric that
performs corrective treatment  and then enforce metric multifairness on an appropriate combination
of the metrics. For instance  returning to the loan repayment example  an ideal metric might be a
combination of credit scores (which tend to be calibrated) and a metric that aims to increase the
loans given to historically underrepresented populations (by  say  requiring the top percentiles of
each subpopulation be treated similarly). Different combinations of the two metrics would place
different weights on the degree of calibration and corrective discrimination in the resulting predictor.
Of course  one could equally apply this metric in the framework of [6]  but the big advantage with
metric multifairness is that we only need a small sample from the metric to provide a relaxed  but still
strong guarantee of fairness.

8

We are optimistic that metric multifairness will provide an avenue towards implementing metric-based
fairness notions. At present  the results are theoretical  but we hope this work can open the door to
empirical studies across diverse domains  especially since one of the strengths of the framework is its
generality. We view testing the empirical performance of metric multifairness with various choices of
metric d and collection C as an exciting direction for future research.
Finally  two recent theoretical works also investigate extensions to the fairness through awareness
framework of [6]. Gillen et al. [10] study metric-based individually fair online decision-making in
the presence of an unknown fairness metric. In their setting  every day  a decision maker must choose
between candidates available on that day; the goal is to have the decision maker’s choices appear
metric fair on each day (but not across days). Their work makes a strong learnability assumption
about the underlying metric; in particular  they assume that the unknown metric is a Mahalanobis
metric  whereas our focus is on fair classiﬁcation when the metric is unknown and unrestricted.
Rothblum and Yona [22] study fair machine learning under a different relaxation of metric fairness 
which they call approximate metric fairness. They assume that the metric is fully speciﬁed and known
to the learning algorithm  whereas our focus is on addressing the challenge of an unknown metric.
Their notion of approximate metric fairness aims to protect all (large enough) groups  and thus  is
more strict than metric multifairness.

Acknowledgements. The authors thank Cynthia Dwork  Roy Frostig  Fereshte Khani  Vatsal
Sharan  Paris Siminelakis  and Gregory Valiant for helpful conversations and feedback on earlier
drafts of this work. We thank the anonymous reviewers for their careful reading and suggestions on
how to improve the clarity of the presentation.

9

References
[1] Julia Angwin  Jeff Larson  Surya Mattu  and Lauren Kirchner. Machine bias: There’s software
used across the country to predict future criminals. and it’s biased against blacks. ProPublica 
2016.

[2] Andrej Bogdanov and Alon Rosen. Pseudorandom functions: Three decades later. In Tutorials

on the Foundations of Cryptography  pages 79–158. Springer  2017.

[3] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in
commercial gender classiﬁcation. In Conference on Fairness  Accountability and Transparency 
pages 77–91  2018.

[4] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big Data  2017.

[5] Sam Corbett-Davies  Emma Pierson  Avi Feller  Sharad Goel  and Aziz Huq. Algorithmic

decision making and the cost of fairness. KDD  2017.

[6] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard S. Zemel. Fairness
through awareness. In Innovations in Theoretical Computer Science (ITCS)  pages 214–226 
2012.

[7] Vitaly Feldman. Distribution-speciﬁc agnostic boosting. In Proceedings of the First Symposium

on Innovations in Computer Science’10  2010.

[8] Vitaly Feldman  Venkatesan Guruswami  Prasad Raghavendra  and Yi Wu. Agnostic learning

of monomials by halfspaces is hard. SIAM Journal on Computing  41(6):1558–1590  2012.

[9] Avi Feller  Emma Pierson  Sam Corbett-Davies  and Sharad Goel. A computer program used
for bail and sentencing decisions was labeled biased against blacks. it’s actually not that clear.
The Washington Post  2016.

[10] Stephen Gillen  Christopher Jung  Michael J. Kearns  and Aaron Roth. Online learning with an

unknown fairness metric. arXiv preprint arXiv:1802.06936  2018.

[11] Oded Goldreich  Shaﬁ Goldwasser  and Silvio Micali. How to construct random functions. In
Foundations of Computer Science  1984. 25th Annual Symposium on  pages 464–479. IEEE 
1984.

[12] Parikshit Gopalan  Adam Tauman Kalai  and Adam R Klivans. Agnostically learning decision
trees. In Proceedings of the fortieth annual ACM symposium on Theory of computing  pages
527–536. ACM  2008.

[13] Moritz Hardt  Eric Price  and Nathan Srebro. Equality of opportunity in supervised learning. In

Advances in Neural Information Processing Systems  pages 3315–3323  2016.

[14] Úrsula Hébert-Johnson  Michael P. Kim  Omer Reingold  and Guy N. Rothblum. Calibration

for the (computationally-identiﬁable) masses. ICML  2018.

[15] Michael Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM

(JACM)  45(6):983–1006  1998.

[16] Michael Kearns  Seth Neel  Aaron Roth  and Zhiwei Steven Wu. Preventing fairness gerryman-

dering: Auditing and learning for subgroup fairness. ICML  2018.

[17] Michael J. Kearns  Robert E. Schapire  and Linda M. Sellie. Toward efﬁcient agnostic learning.

Machine Learning  17(2-3):115–141  1994.

[18] Michael P. Kim  Amirata Ghorbani  and James Zou. Multiaccuracy: Black-box post-processing

for fairness in classiﬁcation. arXiv preprint arXiv:1805.12317  2018.

[19] Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. Inherent trade-offs in the fair

determination of risk scores. ITCS  2017.

10

[20] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical program-

ming  120(1):221–259  2009.

[21] Geoff Pleiss  Manish Raghavan  Felix Wu  Jon Kleinberg  and Kilian Q. Weinberger. On

fairness and calibration. NIPS  2017.

[22] Guy N. Rothblum and Gal Yona. Probably approximately metric-fair learning. ICML  2018.
[23] Leslie G. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142 

1984.

[24] Kaveh Waddell. How algorithms can bring down minorities’ credit scores. The Atlantic  2016.
[25] Blake Woodworth  Suriya Gunasekar  Mesrob I Ohannessian  and Nathan Srebro. Learning

non-discriminatory predictors. COLT  2017.

11

,Michael Kim
Omer Reingold
Guy Rothblum