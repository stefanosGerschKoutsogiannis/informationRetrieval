2008,QUIC-SVD: Fast SVD Using Cosine Trees,The Singular Value Decomposition is a key operation in many machine learning methods. Its computational cost  however  makes it unscalable and impractical for the massive-sized datasets becoming common in applications. We present a new method  QUIC-SVD  for fast approximation of the full SVD with automatic sample size minimization and empirical relative error control. Previous Monte Carlo approaches have not addressed the full SVD nor benefited from the efficiency of automatic  empirically-driven sample sizing. Our empirical tests show speedups of several orders of magnitude over exact SVD. Such scalability should enable QUIC-SVD to meet the needs of a wide array of methods and applications.,QUIC-SVD: Fast SVD Using Cosine Trees

Michael P. Holmes  Alexander G. Gray and Charles Lee Isbell  Jr.

{mph  agray  isbell}@cc.gatech.edu

Atlanta  GA 30327

College of Computing

Georgia Tech

Abstract

The Singular Value Decomposition is a key operation in many machine learning
methods. Its computational cost  however  makes it unscalable and impractical
for applications involving large datasets or real-time responsiveness  which are
becoming increasingly common. We present a new method  QUIC-SVD  for fast
approximation of the whole-matrix SVD based on a new sampling mechanism
called the cosine tree. Our empirical tests show speedups of several orders of
magnitude over exact SVD. Such scalability should enable QUIC-SVD to accel-
erate and enable a wide array of SVD-based methods and applications.

1 Introduction

The Singular Value Decomposition (SVD) is a fundamental linear algebraic operation whose abun-
dant useful properties have placed it at the computational center of many methods in machine learn-
ing and related ﬁelds. Principal component analysis (PCA) and its kernel and nonlinear variants
are prominent examples  and countless other instances are found in manifold and metric learning 
clustering  natural language processing/search  collaborative ﬁltering  bioinformatics and more.
Notwithstanding the utility of the SVD  it is critically bottlenecked by a computational complexity
that renders it impractical on massive datasets. Yet massive datasets are increasingly common in
applications  many of which require real-time responsiveness. Such applications could use SVD-
based methods more liberally if the SVD were not so slow to compute. We present a new method 
QUIC-SVD  for fast  sample-based SVD approximation with automatic relative error control. This
algorithm is based on a new type of data partitioning tree  the cosine tree  that shows excellent ability
to home in on the subspaces needed for good SVD approximation. We demonstrate several-order-
of-magnitude speedups on medium-sized datasets  and verify that approximation error is properly
controlled. Based on these results  QUIC-SVD seems able to help address the scale of modern
problems and datasets  with the potential to beneﬁt a wide array of methods and applications.

2 Background
For A ∈ Rm×n  we write A(i) for the ith row of A and A(j) for the jth column. We use Om×n to
represent the subset of Rm×n whose columns are orthonormal. Since the columns of V ∈ Om×n
are an orthonormal basis  we sometimes use expressions such as “the subspace V ” to refer to the
subspace spanned by the columns of V . Throughout this paper we assume m ≥ n  such that
sampling rows gives bigger speedup than sampling columns. This is no loss of generality  since
whenever m < n we can perform SVD on the transpose  then swap U and V to get the SVD of
the original matrix. Alternatively  row-sampling-based methods have analogous column-sampling
versions that can be used in place of transposition; we leave this implicit and develop only the
row-sampling version of our algorithm.

1

EXTRACTSVD

Algorithm 1 Optimal approximate SVD within a row subspace (cid:98)V .
Input: target matrix A ∈ Rm×n  subspace basis (cid:98)V ∈ On×k
Output: U  Σ  V   the SVD of the best approximation to A within the subspace spanned by (cid:98)V ’s columns
1. Compute A(cid:98)V   then (A(cid:98)V )T A(cid:98)V and its SVD: U(cid:48)Σ(cid:48)V (cid:48)T = (A(cid:98)V )T A(cid:98)V
2. Let V = (cid:98)V V (cid:48)  Σ = (Σ(cid:48))1/2  and U = (A(cid:98)V )V (cid:48)Σ−1

3. Return U  Σ  V

(cid:80)ρ

A = UΣV T  

The singular value decomposition is deﬁned as follows:
Deﬁnition 1. Let A be an m × n real matrix of rank ρ. Then there exists a factorization of the form
(1)
where U and V each have orthonormal columns and are of size m × ρ and n × ρ  respectively  and
Σ is diagonal with entries σ1 ≥ σ2 ≥ . . . ≥ σρ > 0.
Equivalently  we can write the SVD as a weighted sum of rank-one outer products: A =
i   where ui and vi represent the ith columns of U and V . The columns ui and vi
are referred to as the left and right singular vectors  while the weights σi are the singular values.
Though it is sometimes overkill  the SVD can be used to solve essentially any problem in numerical
linear algebra. Instances of such problems abound in machine learning.
Given m ≥ n  the exact SVD has O(mn2) runtime (O(n3) for square matrices). This is highly
unscalable  rendering exact SVD impractical for large datasets. However  it is often the case that
good approximations can be found using subsets of the rows or columns. Of signiﬁcant interest are
low-rank approximations to a matrix. The optimal k-rank approximation  in the sense of minimizing

i=1 σiuivT

the squared error ||A − (cid:98)A||2

F   is the k-rank truncation of the SVD:

k(cid:88)

Ak =

σiuivT

i = UkΣkVk .

(2)

i=1

without enumeration of its properties. We state some of the key properties as a lemma.

Ak is the projection of A’s rows onto the subspace spanned by the top k right singular vectors  i.e. 
Ak = AVkV T
k . The optimality of Ak implies that the columns of Vk span the subspace of dimension
at most k in which the squared error of A’s row-wise projection is minimized. This leads us to a
formulation of SVD approximation in which we seek to ﬁnd a subspace in which A’s projection has
sufﬁciently low error  then perform the SVD of A in that subspace. If the subspace is substantially
lower in rank/dimension than A  the SVD of the projection can be computed signiﬁcantly faster
than the SVD of the original A (quadratically so  as we will have decreased the n in O(mn2)). An
important procedure we will require is the extraction of the best approximate SVD within a given

subspace (cid:98)V . Algorithm 1 describes this process; portions of this idea appeared in [1] and [2]  but
Lemma 1. Given a target matrix A and a row subspace basis stored in the columns of (cid:98)V  
2. UΣV T = A(cid:98)V(cid:98)V T   i.e.  the extracted SVD reconstructs exactly to the projection of A’s rows
onto the subspace spanned by (cid:98)V .
stricted to the span of (cid:98)V .

3. UΣV T minimizes squared-error reconstruction of A among all SVDs whose rows are re-

1. Returns a full SVD  meaning U and V with orthonormal columns  and Σ diagonal.

EXTRACTSVD has the following properties:

We omit the fairly straightforward proof. The runtime of the procedure is O(kmn)  where k is the

rank of(cid:98)V . As this SVD extraction will constitute the last and most expensive step of our algorithm 

we therefore require a subspace discovery method that ﬁnds a subspace of sufﬁcient quality with as
low a rank k as possible. This motivates the essential idea of our approach  which is to leverage the

2

Table 1: Distinctions between whole-matrix SVD approximation and LRMA.
Low-Rank Matrix Approximation

(cid:98)A or unaligned (cid:98)V &(cid:98)Σ only

Whole-Matrix SVD Approximation
True SVD: U  Σ  and V
Addresses full-rank matrix
Full-rank relative error bound
Table 2: Distinctions between subspace construction in QUIC-SVD and previous LRMA methods.
QUIC-SVD
Iterative buildup  fast empirical error control
Adaptive sample size minimization
Cosine tree sampling

Previous LRMA Methods
One-off computation  loose error bound
Fixed a priori sample size (loose)
Various sampling schemes

Fixed low-rank k
k-rank error bound  additive or relative

F /||A||2

geometric structure of a matrix to efﬁciently derive compact (i.e.  minimal-rank) subspaces in which
to carry out the approximate SVD.
Previous Work. A recent vein of work in the theory and algorithms community has focused on
using sampling to solve the problem of low-rank matrix approximation (LRMA). The user speciﬁes
a desired low rank k  and the algorithms try to output something close to the optimal k-rank approxi-
mation. This problem is different from the whole-matrix SVD approximation we address  but a close
relationship allow us to draw on some of the LRMA ideas. Table 1 highlights the distinctions be-
tween whole-matrix SVD approximation and LRMA. Table 2 summarizes the differences between
our algorithmic approach and the more theoretically-oriented approaches taken in the LRMA work.
Each LRMA algorithm has a way of sampling to build up a subspace in which the matrix projection
has bounded error. Our SVD also samples to build a subspace  so the LRMA sampling methods
are directly comparable to our tree-based approach. Three main LRMA sampling techniques have
emerged 1 and we will discuss each from the perspective of iteratively sampling a row  updating a
subspace so it spans the new row  and continuing until the subspace captures the input matrix to
within a desired error threshold. This is how our method works  and it is similar to the framework
used by Friedland et al. [1]. The key to efﬁciency (i.e.  rank-compactness) is for each sampled row
to represent well the rows that are not yet well represented in the subspace.
Length-squared (LS) sampling. Rows are sampled with probability proportional to their squared
lengths: pi = ||A(i)||2
F . LS sampling was used in the seminal work of Frieze  Kannan  and
Vempala [3]  and in much of the follow-on work [4  5]. It is essentially an importance sampling
scheme for the squared error objective. However  it has two important weaknesses. First  a row
can have high norm while not being representative of other rows. Second  the distribution is non-
adaptive  in that a point is equally likely to be drawn whether or not it is already well represented in
the subspace. Both of these lead to wasted samples and needless inﬂation of the subspace rank.
Residual length-squared (RLS) sampling. Introduced by Deshpande and Vempala [2]  RLS modi-
F /||A −
ﬁes the LS probabilities after each subspace update by setting pi = ||A(i) − ΠV (A(i))||2
ΠV (A)||2
F   where ΠV represents projection onto the current subspace V . By adapting the LS distri-
bution to be over residuals  this method avoids drawing samples that are already well represented in
the subspace. Unfortunately  there is still nothing to enforce that any sample will be representative
of other high-residual samples. Further  updating residuals requires an expensive s passes through
the matrix for every s samples that are added  which signiﬁcantly limits practical utility.
Random projections (RP). Introduced by Sarl´os [6]  the idea is to sample linear combinations of
rows  with random combination coefﬁcients drawn from a Gaussian. This method is strong where
LS and RLS are weak — because all rows inﬂuence every sample  each sample is likely to represent a
sizeable number of rows. Unfortunately the combination coefﬁcients are not informed by importance
(squared length)  and the sampling distribution is non-adaptive. Further  each linear combination
requires a full matrix pass  again limiting practicality.
Also deserving mention is the randomized sparsiﬁcation used by Achlioptas et al. [7]. Each of the
LRMA sampling methods has strengths we can draw on and weaknesses we can improve upon. In
particular  our cosine tree sampling method can be viewed as combining the representativeness of
RP sampling with the adaptivity of RLS  which explains its empirically dominant rank efﬁciency.

1Note that our summary of related work is necessarily incomplete due to space constraints; our intent is to

summarize the essential results from the LRMA literature inasmuch as they pertain to our approach.

3

Algorithm 2 Cosine tree construction.
CTNODE
Input: A ∈ Rm×n
Output: cosine tree node containing the rows of A

1. N ← new cosine tree node
2. N.A ← A
3. N.splitP t ← ROWSAMPLELS(A) // split point sampled from length-squared distribution
4. return N

CTNODESPLIT
Input: cosine tree node N
Output: left and right children obtained by cosine-splitting of N

1. for each N.A(i)  compute ci = |cos(N.A(i)  N.splitP t)|
2. if ∀i  ci = 1  return nil
3. cmax = max{ci|ci < 1}; cmin = min{ci}
4. Al ← [ ]; Ar ← [ ]
5. for i = 1 to N.nRows

(a) if cmax − ci ≤ ci − cmin  Al ←

(cid:21)

(cid:20) Al

N.A(i)

(cid:21)

(cid:20) Ar

N.A(i)

(b) else Ar ←

6. return CTNODE(Al)  CTNODE(Ar)

3 Our Approach

||A − (cid:98)A||2

Rather than a ﬁxed low-rank matrix approximation  our objective is to approximate the whole-matrix
SVD with as high a rank as is required to obtain the following whole-matrix relative error bound:

F ≤ ||A||2
F  

(3)

where (cid:98)A = UΣV T is the matrix reconstructed by our SVD approximation. In contrast to the error

bounds of previous methods  which are stated in terms of the unknown low-rank Ak  our error
bound is in terms of the known A. This enables us to use a fast  empirical Monte Carlo technique to
determine with high conﬁdence when we have achieved the error target  and therefore to terminate
with as few samples and as compact a subspace as possible. Minimizing subspace rank is crucial for
speed  as the ﬁnal SVD extraction is greatly slowed by excess rank when the input matrix is large.
We use an iterative subspace buildup as described in the previous section  with sampling governed
by a new spatial partitioning structure we call the cosine tree. Cosine trees are designed to leverage
the geometrical structure of a matrix and a partial subspace in order to quickly home in on good rep-
resentative samples from the regions least well represented. Key to the efﬁciency of our algorithm is
an efﬁcient error checking scheme  which we accomplish by Monte Carlo error estimation at judi-
ciously chosen stages. Such a combination of spatial partitioning trees and Monte Carlo estimation
has been used before to good effect [8]  and we ﬁnd it to be a successful pairing here as well.
Cosine Trees for Efﬁcient Subspace Discovery. The ideal subspace discovery algorithm would
oracularly choose as samples the singular vectors vi. Each vi is precisely the direction that  added to
the subspace spanned by the previous singular vectors  will maximally decrease residual error over
all rows of the matrix. This intuition is the guiding idea for cosine trees.
A cosine tree is constructed as follows. Starting with a root node  which contains all points (rows) 
we take its centroid as a representative to include in our subspace span  and randomly sample a point
to serve as the pivot for splitting. We sample the pivot from the basic LS distribution  that being the
cheapest source of information as to sample importance. The remaining points are sorted by their
absolute cosines relative to the pivot point  then split according to whether they are closer to the high
or low end of the cosines. The two groups are assigned to two child nodes  which are placed in a

4

Algorithm 3 Monte Carlo estimation of the squared error of a matrix projection onto a subspace.
MCSQERROR

Input: A ∈ Rm×n  (cid:98)V ∈ On×k  s ∈ {1 . . . m}  δ ∈ [0  1]

sqErr ∈ R s.t. with probability at least 1 − δ  ||A − A(cid:98)V(cid:98)V T||2

F ≤ sqErr

Output:

1. S = rowSamplesLS(A  s) // sample s rows from the length-squared distribution
2. for i = 1 to s : // compute weighted sq. mag. of each sampled row’s projection onto V

(a) wgtM agSq[i] = 1

pS(i)

||S(i)V ||2

F // pS(i) is prob. of drawing Si under LS sampling

3. ˆµ = avg(wgtM agSq); ˆσ2 = var(wgtM agSq); magSqLB = lowBound(ˆµ  ˆσ2  s  δ)
4. return ||A||2

F − magSqLB

Algorithm 4 QUIC-SVD: fast whole-matrix approximate SVD with relative error control.
QUIC-SVD
Input: A ∈ Rm×n   ∈ [0  1]  and δ ∈ [0  1]

Output: an SVD U  Σ  V s.t. (cid:98)A = U ΣV T satisﬁes ||A − (cid:98)A||2

F with probability at least 1 − δ

F ≤ ||A||2

1. V = [ ]; mcSqErr = ||A||2
2. Q = EMPTYPRIORITYQUEUE(); Q.insert(Nroot  0)
3. do until mcSqErr ≤ ||A||2
F :

F ; Nroot = CTNODE(A)

(a) N = Q.pop(); C = CTNODESPLIT(N ) // C = {Nl  Nr}  the children of N
(b) Remove N’s contributed basis vector from V
(c) for each Nc ∈ C :
(d) for each Nc ∈ C :

i. V = [V MGS(V  Nc.centroid)] // MGS = modiﬁed Gram-Schmidt orthonormalization

i. errC = MCSQERROR(Nc.A  V  O(log[Nc.nRows])  δ)
ii. Q.insert(Nc  errC)

(e) mcSqErr = MCSQERROR(A  V  O(log m)  δ)

4. return EXTRACTSVD(A  V )

queue prioritized by the residual error of each node. The process is then repeated according to the
priority order of the queue. Algorithm 2 deﬁnes the splitting process.
Why do cosine trees improve sampling efﬁciency? By prioritizing expansion by the residual error
of the frontier nodes  sampling is always focused on the areas with maximum potential for error
reduction. Since cosine-based splitting guides the nodes toward groupings with higher parallelism 
the residual magnitude of each node is increasingly likely to be well captured along the direction of
the node centroid. Expanding the subspace in the direction of the highest-priority node centroid is
therefore a good guess as to the direction that will maximally reduce residual error. Thus  cosine
tree sampling approximates the ideal of oracularly sampling the true singular vectors.

3.1 QUIC-SVD

Strong error control. Algorithm 4  QUIC-SVD (QUantized Iterative Cosine tree)2  speciﬁes a
way to leverage cosine trees in the construction of an approximate SVD while providing a strong
probabilistic error guarantee. The algorithm builds a subspace by expanding a cosine tree as de-
scribed above  checking residual error after each expansion. Once the residual error is sufﬁciently
low  we return the SVD of the projection into the subspace. Note that exact error checking would
require an expensive O(k2mn) total cost  where k is the ﬁnal subspace rank  so we instead use a
Monte Carlo error estimate as speciﬁed in Algorithm 3. We also employ Algorithm 3 for the error
estimates used in node prioritization. With Monte Carlo instead of exact error computations  the
total cost for error checking decreases to O(k2n log m)  a signiﬁcant practical reduction.

2Quantized alludes to each node being represented by a single point that is added to the subspace basis.

5

The other main contributions to runtime are: 1) k cosine tree node splits for a total of O(kmn)  2)
O(k) single-vector Gram-Schmidt orthonormalizations at O(km) each for a total of O(k2m)  and
3) ﬁnal SVD extraction at O(kmn). Total runtime is therefore O(kmn)  with the ﬁnal projection
onto the subspace being the costliest step since the O(kmn) from node splitting is a very loose
worst-case bound. We now state the QUIC-SVD error guarantee.
SVD U  Σ  V such that (cid:98)A = UΣV T satisﬁes ||A − (cid:98)A||2
Theorem 1. Given a matrix A ∈ Rm×n and   δ ∈ [0  1]  the algorithm QUIC-SVD returns an
F with probability at least 1 − δ.
F ≤ ||A||2
Proof sketch. The algorithm terminates after mcSqErr ≤ ||A||2
onto V (i.e.  (cid:98)A = AV V T ). Thus  we have only to show that mcSqErr in the terminal iteration
F with a call to EXTRACTSVD.
From Lemma 1 we know that EXTRACTSVD returns an SVD that reconstructs to A’s projection
is an upper bound on the error ||A − (cid:98)A||2
F with probability at least 1 − δ. Note that intermediate
error checks do not affect the success probability  since they only ever tell us to continue expand-
ing the subspace  which is never a failure. From the Pythagorean theorem  ||A − AV V T||2
F =
F − ||AV V T||2
||A||2
F   and  since rotations do not affect lengths  ||AV V T||2
F = ||AV ||2
F . The
call to MCSQERROR (step 3(e)) performs a Monte Carlo estimate of ||AV ||2
F in order to esti-
mate ||A||2
F − ||AV ||2
F . It is easily veriﬁed that the length-squared-weighted sample mean used by
MCSQERROR produces an unbiased estimate of ||AV ||2
F . By using a valid conﬁdence interval to
generate a 1 − δ lower bound on ||AV ||2
F from the sample mean and variance (e.g.  Theorem 1 of
[9] or similar)  MCSQERROR is guaranteed to return an upper bound on ||A||2
F with
probability at least 1 − δ  which establishes the theorem.
Relaxed error control. Though the QUIC-SVD procedure speciﬁed in Algorithm 4 provides a
strong error guarantee  in practice its error checking routine is overconservative and is invoked more
frequently than necessary. For practical usage  we therefore approximate the strict error checking of
Algorithm 4 by making three modiﬁcations:

F − ||AV ||2

1. Set mcSqErr to the mean  rather than the lower bound  of the MCSQERROR estimate.
2. At each error check  estimate mcSqErr with several repeated Monte Carlo evaluations
(i.e.  calls to MCSQERROR)  terminating only if they all result in mcSqErr ≤ ||A||2
F .

3. In each iteration  use a linear extrapolation from past decreases in error to estimate the
number of additional node splits required to achieve the error target. Perform this projected
number of splits before checking error again  thus eliminating needless intermediate error
checks.

Although these modiﬁcations forfeit the strict guarantee of Theorem 1  they are principled approx-
imations that more aggressively accelerate the computation while still keeping error well under
control (this will be demonstrated empirically). Changes 1 and 2 are based on the fact that  because
mcSqErr is an unbiased estimate generated by a sample mean  it obeys the Central Limit Theorem
and thus approaches a normal distribution centered on the true squared error. Under such a sym-
metric distribution  the probability that a single evaluation of mcSqErr will exceed the true error is
0.5. The probability that  in a series of x evaluations  at least one of them will exceed the true error
is approximately 1 − 0.5x (1 minus the probability that they all come in below the true error). The
probability that at least one of our mcSqErr evaluations results in an upper bound on the true error
(i.e.  the probability that our error check is correct) thus goes quickly to 1. In our experiments  we
use x = 3  corresponding to a success probability of approximately 0.9 (i.e.  δ ≈ 0.1).
Change 3 exploits that fact that the rate at which error decreases is typically monotonically non-
increasing. Thus  extrapolating the rate of error decrease from past error evaluations yields a con-
servative estimate of the number of splits required to achieve the error target. Naturally  we have to
impose limits to guard against outlier cases where the estimated number is unreasonably high. Our
experiments limit the size of the split jumps to be no more than 100.

4 Performance

We report the results of two sets of experiments  one comparing the sample efﬁciency of cosine
trees to previous LRMA sampling methods  and the other evaluating the composite speed and error
performance of QUIC-SVD. Due to space considerations we give results for only two datasets  and

6

0.025

0.02

madelon

LS
RLS
RP
CT
Opt

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
v

i
t

l

a
e
r

0.015

0.01

0.005

0

0

declaration

LS
RLS
RP
CT
Opt

0.035

0.03

0.025

0.02

0.015

0.01

0.005

 

r
o
r
r
e
d
e
r
a
u
q
s
 
e
v

i
t

l

a
e
r

10

20

30

subspace rank

40

(a) madelon kernel (2000 (cid:215)

50

60

0

0

2000)

50

100

150

200

subspace rank

250

300

350

400

(b) declaration (4656 (cid:215)

3923)

Figure 1: Relative squared error vs. subspace rank for various subspace discovery methods. LS is
length-squared  RLS is residual length-squared  RP is random projection  and CT is cosine tree.

due to the need to compute the exact SVD as a baseline we limit ourselves to medium-sized matrices.
Nonetheless  these results are illustrative of the more general performance of the algorithm.
Sample efﬁciency. Because the runtime of our algorithm is O(kmn)  where k is the ﬁnal dimension
of the projection subspace  it is critical that we use a sampling method that achieves the error target
with the minimum possible subspace rank k. We therefore compare our cosine tree sampling method
to the previous sampling methods proposed in the LRMA literature. Figure 1 shows results for the
various sampling methods on two matrices  one a 2000 (cid:215)
2000 Gaussian kernel matrix produced
by the Madelon dataset from the NIPS 2003 Workshop on Feature Extraction (madelon kernel)  and
the other a 4656 (cid:215)
3923 scan of the US Declaration of Independence (declaration). Plotted is the
relative squared error of the input matrix’s projection onto the subspaces generated by each method
at each subspace rank. Also shown is the optimal error produced by the exact SVD at each rank.
Both graphs show cosine trees dominating the other methods in terms of rank efﬁciency. This
dominance has been conﬁrmed by many other empirical results we lack space to report here. It is
particularly interesting how closely the cosine tree error can track that of the exact SVD. This would
seem to give some justiﬁcation to the principle of grouping points according to their degree of mutual
parallelism  and validates our use of cosine trees as the sampling mechanism for QUIC-SVD.
Speedup and error. In the second set of experiments we evaluate the runtime and error performance
of QUIC-SVD. Figure 2 shows results for the madelon kernel and declaration matrices. On the top
row we show how speedup over exact SVD varies with the target error (cid:31). Speedups range from 831
at (cid:31) = 0(cid:46)0025 to over 3 600 at (cid:31) = 0(cid:46)023 for madelon kernel  and from 118 at (cid:31) = 0(cid:46)01 to nearly
20 000 at (cid:31) = 0(cid:46)03 for declaration. On the bottom row we show the actual error of the algorithm
in comparison to the target error. While the actual error is most often slightly above the target  it
nevertheless hugs the target line quite closely  never exceeding the target by more than 10%. Overall 
the several-order-of-magnitude speedups and controlled error shown by QUIC-SVD would seem
to make it an attractive option for any algorithm computing costly SVDs.

5 Conclusion

We have presented a fast approximate SVD algorithm  QUIC-SVD  and demonstrated several-
order-of-magnitude speedups with controlled error on medium-sized datasets. This algorithm dif-
fers from previous related work in that it addresses the whole-matrix SVD  not low-rank matrix
approximation  it uses a new efﬁcient sampling procedure based on cosine trees  and it uses em-
pirical Monte Carlo error estimates to adaptively minimize needed sample sizes  rather than ﬁxing
a loose sample size a priori. In addition to theoretical justiﬁcations  the empirical performance of
QUIC-SVD argues for its effectiveness and utility. We note that a reﬁned version of QUIC-SVD is
forthcoming. The new version is greatly simpliﬁed  and features even greater speed with a determin-
istic error guarantee. More work is needed to explore the SVD-using methods to which QUIC-SVD
can be applied  particularly with an eye to how the introduction of controlled error in the SVD will

7

madelon

declaration

4 000

3 000

p
u
d
e
e
p
s

2 000

1 000

2.5e+04

2e+04

1.5e+04

1e+04

5 000

0

p
u
d
e
e
p
s

0

0

0.005

0.01

epsilon

0.015

0.02

0.025

0.005

0.01

0.015

0.02

epsilon

0.025

0.03

0.035

(a) speedup - madelon kernel

(b) speedup - declaration

madelon

declaration

128

477

1

0.8

0.6

0.4

0.2

 

r
o
r
r
e
d
e
r
a
u
q
s
 

e
v
i
t
a
l
e
r

0

0

actual error
target error

actual error
target error

 

r
o
r
r
e
d
e
r
a
u
q
s
 

e
v
i
t
a
l
e
r

1

0.8

0.6

0.4

0.2

0

0.005

0.01

epsilon

0.015

0.02

0.025

0.01

0.015

0.02

epsilon

0.025

0.03

0.035

(c) relative error - madelon kernel

(d) relative error - declaration

Figure 2: Speedup and actual relative error vs. (cid:31)for QUIC-SVD on madelon kernel and declaration.

affect the quality of the methods using it. We expect there will be many opportunities to enable new
applications through the scalability of this approximation.

References
[1] S. Friedland  A. Niknejad  M. Kaveh  and H. Zare. Fast Monte-Carlo Low Rank Approximations for

Matrices. In Proceedings of Int. Conf. on System of Systems Engineering  2006.

[2] A. Deshpande and S. Vempala. Adaptive Sampling and Fast Low-Rank Matrix Approximation. In 10th

International Workshop on Randomization and Computation (RANDOM06)  2006.

[3] A. M. Frieze  R. Kannan  and S. Vempala. Fast Monte-Carlo Algorithms for Finding Low-Rank Approxi-

mations. In IEEE Symposium on Foundations of Computer Science  pages 370–378  1998.

[4] P. Drineas  R. Kannan  and M. W. Mahoney. Fast Monte Carlo Algorithms for Matrices II: Computing a

Low-Rank Approximation to a Matrix. SIAM Journal on Computing  36(1):158–183  2006.

[5] P. Drineas  E. Drinea  and P. S. Huggins. An Experimental Evaluation of a Monte-Carlo Algorithm for

Singular Value Decomposition. Lectures Notes in Computer Science  2563:279–296  2003.

[6] T. Sarlos. Improved Approximation Algorithms for Large Matrices via Random Projections. In 47th IEEE

Symposium on Foundations of Computer Science (FOCS)  pages 143–152  2006.

[7] D. Achlioptas  F. McSherry  and B. Scholkopf. Sampling Techniques for Kernel Methods. In Advances in

Neural Information Processing Systems (NIPS) 17  2002.

[8] M. P. Holmes  A. G. Gray  and C. L.Isbell  Jr. Ultrafast Monte Carlo for Kernel Estimators and Generalized

Statistical Summations. In Advances in Neural Information Processing Systems (NIPS) 21  2008.

[9] J. Audibert  R. Munos  and C. Szepesvari. Variance estimates and exploration function in multi-armed

bandits. Technical report  CERTIS  2007.

8

,Anima Anandkumar
Daniel Hsu
Majid Janzamin
Sham Kakade
Mahito Sugiyama
Karsten Borgwardt