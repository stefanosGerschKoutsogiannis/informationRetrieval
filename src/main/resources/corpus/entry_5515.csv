2018,The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models,We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions  we establish an $\Omega(K\log K)$ labeled sample complexity bound without imposing parametric assumptions  where $K$ is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification  we consider general multiclass classification ($K>2$)  which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures  and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally  we describe three algorithms for computing these estimators based on a connection to bipartite graph matching  and perform experiments to illustrate the superiority of the MLE over the majority vote estimator.,The Sample Complexity of Semi-Supervised Learning

with Nonparametric Mixture Models

Chen Dan1  Liu Leqi1  Bryon Aragam1  Pradeep Ravikumar1  Eric P. Xing1 2

1Carnegie Mellon University

2Petuum Inc.

{cdan leqil naragam pradeepr epxing}@cs.cmu.edu

Abstract

We study the sample complexity of semi-supervised learning (SSL) and introduce
new assumptions based on the mismatch between a mixture model learned from
unlabeled data and the true mixture model induced by the (unknown) class condi-
tional distributions. Under these assumptions  we establish an Ω(K log K) labeled
sample complexity bound without imposing parametric assumptions  where K is
the number of classes. Our results suggest that even in nonparametric settings it is
possible to learn a near-optimal classiﬁer using only a few labeled samples. Un-
like previous theoretical work which focuses on binary classiﬁcation  we consider
general multiclass classiﬁcation (K > 2)  which requires solving a difﬁcult permu-
tation learning problem. This permutation deﬁnes a classiﬁer whose classiﬁcation
error is controlled by the Wasserstein distance between mixing measures  and we
provide ﬁnite-sample results characterizing the behaviour of the excess risk of this
classiﬁer. Finally  we describe three algorithms for computing these estimators
based on a connection to bipartite graph matching  and perform experiments to
illustrate the superiority of the MLE over the majority vote estimator.

1

Introduction

With the rapid growth of modern datasets and increasingly passive collection of data  labeled data
is becoming more and more expensive to obtain while unlabeled data remains cheap and plentiful
in many applications. Leveraging unlabeled data to improve the predictions of a machine learning
system is the problem of semi-supervised learning (SSL)  which has been the source of many
empirical successes [1–3] and theoretical inquiries [4–16]. Commonly studied assumptions include
identiﬁability of the class conditional distributions [5  6]  the cluster assumption [10  11] and the
manifold assumption [9  12  13  15]. In this work  we propose a new type of assumption that loosely
combines ideas from both the identiﬁability and cluster assumption perspectives. Importantly  we
consider the general multiclass (K > 2) scenario  which introduces signiﬁcant complications. In
this setting  we study the sample complexity and rates of convergence for SSL and propose simple
algorithms to implement the proposed estimators.
The basic question behind SSL is to connect the marginal distribution over the unlabeled data
P(X) to the regression function P(Y | X). We consider multiclass classiﬁcation  so that Y ∈ Y =
{α1  . . .   αK} for some K ≥ 2. In order to motivate our perspective  let F ∗ denote the marginal
density of the unlabeled samples and suppose that F ∗ can be written as a mixture model

K(cid:88)

F ∗(x) =

λbfb(x).

(1)

Assuming the unlabeled data can be used to learn the mixture model (1)  the question becomes when
is this mixture model useful for predicting Y ? Figure 1 illustrates an idealized example.

b=1

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a) Decision boundaries learned
from the unlabeled data.

(b) Learned decision boundaries
correspond exactly to true bound-
aries.

(c) Learned decision boundaries
are only approximately correct.

Figure 1: Illustration of the main idea for K = 4. The decision boundaries learned from the unlabeled
data (cf. (1)) are depicted by the dashed black lines and the true decision boundaries are depicted by
the solid red lines. (a) The unlabeled data is used to learn some approximate decision boundaries
via the mixture model Λ. Even with these decision boundaries  it is not known which class each
region corresponds to. The labeled data is used to learn this assignment. (b) Previous work assumes
that the true and learned decision boundaries are the same. (c) In the current work  we assume that
the true decision boundaries are unknown  but that it is be possible to learn a mixture model that
approximates the true boundaries using unlabeled data.

(cid:80)
k λ∗

Clearly  we can always write F ∗(x) = (cid:80)K

k=1 λ∗

kf∗

k (x)  where f∗

k   nor do we assume that λb corresponds to some λ∗

k is the density of the kth class
conditional P(X | Y = αk) and λ∗
k = P(Y = αk). This will be called the true mixture model in the
sequel. In this work  we do not assume that (1) is the true mixture model  i.e. we do not assume that
fb corresponds to one of the f∗
k. In other words 
we allow the true mixture model to be nonidentiﬁable and consider the case where some misspeciﬁed
mixture model is learned from the unlabeled data. We assume that the number of mixture components
K is the same as the number of classes  which is always known  although extending our analysis to
overﬁtted mixtures is straightforward.
In an early series of papers  Castelli and Cover [5  6] considered this question under the following
assumptions: (a) For each b there is some k such that fb = f∗
k  (b) F ∗ is known  and (c)
K = 2. Thus  they assumed that the true components and weights were known but it was unknown
which class each mixture component represents. In Figure 1  this corresponds to the case (b) where
the true and learned decision boundaries are identical. Given labeled data  the special case K = 2
reduces to a simple hypothesis testing problem which can be tackled using the Neyman-Pearson
lemma. In this paper  we are interested in settings where each of these three assumptions fail:

k and λb = λ∗

kf∗

(a) What if the class conditionals f∗

k are unknown? Although we can always write F ∗(x) =
k (x)  it is generally not the case that this mixture model is learnable from unlabeled
data alone. In practice  what is learned will be different from this ideal case  but the hope is
that it will still be useful. In this case  the argument in Castelli and Cover [5] breaks down.
Motivated by recent work on nonparametric mixture models [17]  we study the general case
where the true mixture model is not known or even learnable from unlabeled data.

(b) What if F ∗ is unknown? In a follow-up paper  Castelli and Cover [6] studied the case where
2} are
F ∗ is unknown by assuming that K = 2 and the class conditional densities {f∗
known up to a permutation. In this setting  the unlabeled data is used to ascertain the relative
mixing proportions  but estimation error in the densities is not considered. We are interested
in the general case in which a ﬁnite amount of unlabeled data is used to estimate both the
mixture weights and densities.

1   f∗

(c) What if K > 2? If K > 2  once again the argument in Castelli and Cover [5] no longer
applies  and we are faced with a challenging permutation learning problem. Permutation
learning problems have gained notoriety recently owing to their applicability to a wide
variety of problems  including statistical matching and seriation [18–20]  graphical models
[21  22]  and regression [23  24]  so these results may be of independent interest.

With these goals in mind  we study the MLE and majority voting (MV) rules for learning the unknown
class assignment introduced in the next section. Our assumptions for MV are closely related to recent
work based on the so-called cluster assumption [4  10  11  25]; see Section 4.2 for more details.

2

D2(Λ)=?D1(Λ)=?D4(Λ)=?D3(Λ)=?D2(Λ)D1(Λ)D4(Λ)D3(Λ)D2(Λ∗)D1(Λ∗)D3(Λ∗)D4(Λ∗)D2(Λ)D1(Λ)D4(Λ)D3(Λ)D2(Λ∗)D1(Λ∗)D3(Λ∗)D4(Λ∗)Contributions A key aspect of our analysis is to establish conditions that connect the mixture
model (1) to the true mixture model. Under these conditions we prove nonasymptotic rates of
convergence for learning the class assignment (Figure 1a) from labeled data when K > 2  establish
an Ω(K log K) sample complexity for learning this assignment  and prove that the resulting classiﬁer
converges to the Bayes classiﬁer. We then propose simple algorithms based on a connection to
bipartite graph matching  and illustrate their performance on real and simulated data.

2 SSL as permutation learning

In this section  we formalize the ideas from the introduction using the language of mixing measures.
We adopt this language for several reasons: 1) It makes it easy to refer to the parameters in the
mixture model (1) by wrapping everything into a single  coherent statistical parameter Λ  2) We can
talk about convergence of these parameters via the Wasserstein metric  and 3) It simpliﬁes discussions
of identiﬁability in mixture models. Before going into technical details  we summarize the main idea
as follows (see also Figure 1):

1. Use the unlabeled data to learn a K-component mixture model that approximates F ∗  which

is represented by the mixing measure Λ deﬁned below;
regions Db(Λ) deﬁned by Λ;

2. Use the labeled data to determine the correct assignment π of classes αk to the decision
3. Based on the pair (Λ  π)  deﬁne a classiﬁer gΛ π : X → Y by (3) below.

Mixing measures and mixture models For concreteness  we will work on X = Rd  however  our
results generalize naturally to any space X with a dominating measure and well-deﬁned density
functions. Let P = {f ∈ L1(Rd) :
f dx = 1} be the set of probability density functions on Rd 
and MK(P) denote the space of probability measures over P with precisely K atoms. An element
Λ ∈ MK(P) is called a (ﬁnite) mixing measure  and can be thought of as a convenient mathematical
device for encoding the weights {λk} and the densities {fk} into a single statistical parameter. By
integrating against this measure  we obtain a new probability density which is denoted by

ş

Decision regions  assignments  and classiﬁers Any mixing measure Λ deﬁnes K decision regions
given by Db = Db(Λ) := {x ∈ X : λbfb(x) > λjfj(x)∀j (cid:54)= b} (Figure 1). This allows us to assign
an index from 1  . . .   K to any x ∈ X   and hence deﬁnes a function ˇgΛ : X → [K] := {1  . . .   K}.
This function is not a genuine classiﬁer  however  since its output is an uninformative index b ∈ [K]
as opposed to a proper class label αk ∈ Y. The key point is that even if we know Λ  we still must
identify each label αk with a decision region Db(Λ)  i.e. we must learn a permutation π : Y → [K].
With some abuse of notation  we will sometimes write π(k) instead of π(αk) for any permutation π.
Together  any pair (Λ  π) deﬁnes a classiﬁer gΛ π : X → Y by

gΛ π(x) = π(ˇgΛ(x)) =

π−1(b)1(x ∈ Db(Λ)).

(3)

b=1

This mixing measure perspective helps to clarify the role of the unknown permutation in supervised
learning: The unlabeled data is enough to learn Λ (and hence the decision regions Db(Λ))  however 
labeled data are necessary to learn an assignment π between classes and decision regions. This
formulates SSL as a coupled mixture modeling and permutation learning problem: Given unlabeled

and labeled data  learn a pair ((cid:98)Λ (cid:98)π) which yields a classiﬁer(cid:98)g = g(cid:98)Λ (cid:98)π.

3

m(Λ) :=

λbfb(x) 

(2)

ş
(cid:40)(cid:88)

where fb is a particular enumeration of the densities in the support of Λ and λb is the probability
of the bth density. Thus  (1) can be written as F ∗ = m(Λ). By metrizing P via the total variation
|f − g| dx  the distance between two ﬁnite K-mixtures can be computed
distance dTV(f  g) = 1
2
via the Wasserstein metric [26]:
W1(Λ  Λ(cid:48)) = inf

j) : 0 ≤ σij ≤ 1 

σijdTV(fi  f(cid:48)

σij = λ(cid:48)
j 

(cid:88)

(cid:88)

(cid:88)

σij = 1 

σij = λi

.

(cid:41)

i j

i j

i

j

K(cid:88)

b=1

K(cid:88)

k to the density f∗

Bayes classiﬁers and the true permutation The true permutation π∗ : Y → [K] is deﬁned to be
the permutation that assigns each class αk to the correct decision region D∗
b = Db(Λ∗) (Figure 1).
As usual  the target classiﬁer is the Bayes classiﬁer  which can also be written in the form (3):
Let Λ∗ denote the true mixing measure that assigns probability λ∗
k and note that
F ∗ = m(Λ∗)  which is the true mixture model deﬁned previously. Then it is easy to check that
gΛ∗ π∗ is the Bayes classiﬁer.
Identiﬁability Although the true mixing measure Λ∗ may not be identiﬁable from F ∗  some other
mixture model may be. In other words  although it may not be possible to learn Λ∗ from unlabeled
data  it may be possible to learn some other mixing measure Λ (cid:54)= Λ∗ such that m(Λ) = F ∗ =
m(Λ∗) (Figure 1c). This essentially amounts to a violation of the cluster assumption: High-density
clusters are identiﬁable  but in practice the true class labels may not respect the cluster boundaries.
Assumptions that guarantee a mixture model are identiﬁable are well-studied [27–29]  including both
parametric [30] and nonparametric [17  31  32] assumptions. In particular  Aragam et al. [17] have
proved general conditions under which mixture models with arbitrary  overlapping nonparametric
components are identiﬁable and estimable  including extreme cases where each component fk has
the same mean. Since this problem is well-studied  we focus hereafter on the problem of learning the
permutation π∗. Thus  in the sequel we will assume that we are given an arbitrary mixing measure Λ
which will be used to estimate π∗. We do not assume that Λ = Λ∗ or even that these mixing measures
are close: The idea is to elicit conditions on Λ that ensure consistent estimation of π∗. This makes
our analysis applicable to a wide variety of methods  including heuristic approaches for learning Λ
from the unlabeled data  which are common in the literature on nonparametric mixtures.

3 Two estimators
Assume we are given a mixing measure Λ along with the labeled samples (X (i)  Y (i)) ∈ X × Y.
Two natural estimators of π∗ are the MLE and majority vote. Although both estimators depend on Λ 
this dependence will be suppressed for brevity.

Maximum likelihood Deﬁne (cid:96)(π; Λ  X  Y ) := log λπ(Y )fπ(Y )(X). We will work with the fol-
lowing misspeciﬁed MLE (i.e. Λ (cid:54)= Λ∗):

(cid:98)πMLE ∈ arg max

π

(cid:96)n(π; Λ) 

(cid:96)n(π; Λ) :=

1
n

n(cid:88)

i=1

(cid:96)(π; Λ  X (i)  Y (i)).

(4)

When Λ = Λ∗  this is the correctly speciﬁed MLE of the unknown permutation π∗  however  the
deﬁnition above allows for the general misspeciﬁed case Λ (cid:54)= Λ∗.

Majority vote The majority vote estimator (MV) is given by a simple majority vote over each
MV :

decision region. Formally  we deﬁne a permutation(cid:98)πMV as follows: The inverse assignment(cid:98)π−1
[K] → Y is deﬁned by(cid:98)π−1
undeﬁned. Note that when K = 2  the MV classiﬁer deﬁned by (3) with π =(cid:98)πMV is essentially the

If there is no majority class in a given decision region  we consider this a failure of MV and treat it as

1(Y (i) = α  X (i) ∈ Db(Λ)).

same as the three-step procedure described in Rigollet [10]  which focuses on bounding the excess
risk under the cluster assumption. In contrast  we are interested in the consistency of the unknown
permutation π∗ when K > 2  which is a more difﬁcult problem.

n(cid:88)

i=1

MV(b) = arg max

α∈Y

(5)

4 Statistical results

Our main results establish rates of convergence for both the MLE and MV introduced in the previous
section. We will use the notation E∗h(X  Y ) to denote the expectation with respect to the true
distribution (X  Y ) ∼ P(X  Y ). Without loss of generality  we assume that π∗(αk) = k and
fb = f∗
notation in the sequel.

b + hb for some hb. Then(cid:98)π = π∗ if and only if(cid:98)π(αk) = k  which helps to simplify the

4

4.1 Maximum likelihood
Given Λ  the notation E∗(cid:96)(π; Λ  X  Y ) = E∗ log λπ(Y )fπ(Y )(X) denotes the expectation of the
misspeciﬁed log-likelihood with respect to the true distribution. Deﬁne the “gap”

∆MLE(Λ) := E∗(cid:96)(π∗; Λ  X  Y ) − max
π(cid:54)=π∗

(6)
For any function a : R → R  deﬁne the usual Fenchel-Legendre dual a∗(t) = sups∈R(st − a(s)).
Let Ub = log λbfb(X) and βb(s) = log E∗ exp(sUb). Finally  let nk := |{i : Y (i) = αk}| denote
Theorem 4.1. Let(cid:98)πMLE be the MLE deﬁned in (4). If ∆MLE := ∆MLE(Λ) > 0 then
the number of labeled samples with the kth label.
(cid:17)

E∗(cid:96)(π; Λ  X  Y ).

P((cid:98)πMLE = π∗) ≥ 1 − 2K 2 exp

(cid:16) − inf

β∗
b (∆MLE/3)

nk · inf

.

k

b

The condition ∆MLE(Λ) > 0 is important to ensure that π∗ is learnable from Λ  and the size of
∆MLE(Λ) quantiﬁes “how easy” it is to learn π∗ is given Λ. A bigger gap implies an easier problem.
Thus  it is of interest to understand this quantity better. The following proposition shows that when
Λ = Λ∗  this gap is always nonnegative:
Proposition 4.2. For any permutation π and any Λ 

E∗(cid:96)(π; Λ  X  Y ) ≤ E∗(cid:96)(π∗; Λ∗  X  Y )

and hence ∆MLE(Λ∗) ≥ 0.
In general  assuming ∆MLE(Λ) > 0 is a weak assumption  but bounds on ∆MLE(Λ) are difﬁcult to
obtain without making additional assumptions on the densities fk and f∗
k . A brief discussion of this
can be found in Appendix B; we leave it to future work to study this quantity more carefully.

(cid:80)n
4.2 Majority vote
For any Λ  deﬁne mb := |i : X (i) ∈ Db(Λ)| and χbj(Λ) := 1
i=1 1(Y (i) = j  X (i) ∈ Db(Λ)) 
where 1(·) is the indicator function. Similar to the MLE  our results for MV depend crucially on a
(cid:111)
“gap” quantity  given by

(cid:110)E∗χbb(Λ) − max

E∗χbj(Λ)

(7)

mb

.

∆MV(Λ) := inf
b

j(cid:54)=b

This quantity essentially measures how much more likely it is to sample the bth label in the bth
decision region than any other label  averaged over the entire region. Thus  conditions on ∆MV(Λ)
are closely related to the well-known cluster assumption [4  10  11  25].

Theorem 4.3. Let(cid:98)πMV be the MV deﬁned in (5). If ∆MV := ∆MV(Λ) > 0 then
(cid:17)

(cid:16)−2∆2

P((cid:98)πMV = π∗) ≥ 1 − 2K 2 exp

MV minb mb

.

9

As with the MLE  the gap ∆MV(Λ) is an important quantity. Fortunately  when Λ = Λ∗ it is always
positive:
Proposition 4.4. For each b = 1  . . .   K 

E∗χbb(Λ∗) > max
j(cid:54)=b

E∗χbj(Λ∗)

and hence ∆MV(Λ∗) > 0.
When Λ (cid:54)= Λ∗  ∆MV(Λ) has the following interpretation: ∆MV(Λ) measures how well the decision
regions deﬁned by Λ match up with the decision regions deﬁned by Λ∗. When Λ deﬁnes decision
regions that assign high probability to one class  ∆MV(Λ) will be large. If Λ deﬁnes decision regions
where multiple classes have approximately the same probability  however  then it is possible that
∆MV(Λ) will be small. In this case  our experiments in Section 6 indicate that the MLE performs
much better by managing overlapping decision regions more gracefully.

5

4.3 Sample complexity

Theorems 4.1 and 4.3 imply upper bounds on the minimum number of samples required to learn the
permutation π∗: For any δ ∈ (0  1)  as long as

(MLE)

(MV)

inf
b

log 2K2
δ
b (∆MLE/3)

inf
k

nk := n0 ≥
inf b β∗
mb := m0 ≥ 9 log 2K2

δ

2∆2

MV

(8)

(9)

we recover π∗ with probability at least 1−δ. Surprisingly  as stated these lower bounds are dimension-
free  however  in practice the gaps ∆MLE and ∆MV may be dimension-dependent.
To derive the sample complexity in terms of the total number of labeled samples n  it sufﬁces to
determine the minimum number of samples per class given n draws from a multinomial random
variable. For the general case with unequal probabilities  Lemma D.2 provides a precise answer. For
simplicity here  we summarize the special case where each class (resp. decision region) is equally
probable for the MLE (resp. MV).
Corollary 4.5 (Sample complexity of MLE). Suppose that λ∗
4

k = 1/K for each k  ∆MLE > 0  and

(cid:104)

(cid:105)

n ≥ K log(K/δ)

1 +

inf b β∗

b (∆MLE/3)

.

Then P((cid:98)πMLE = π∗) ≥ 1 − δ.

Then P((cid:98)πMV = π∗) ≥ 1 − δ.

Corollary 4.6 (Sample complexity of MV). Suppose that P(X ∈ Db(Λ)) = 1/K for each k 
∆MV > 0  and

(cid:104)

(cid:105)

.

n ≥ K log(K/δ)

1 +

18
∆2

MV

Coupon collector’s problem and SSL To better understand these bounds  consider arguably
simplest possible case: Suppose that each density f∗
k = 1/K  and that
we know Λ∗. Under these very strong assumptions  an alternative way to learn π∗ is to simply
sample from P(X) until we have visited each decision region D∗
k at least once. This is the classical
coupon collector’s problem (CCP)  which is known to require Θ(K log K) samples [33  34]. Thus 
under these assumptions the expected number of samples required to learn π∗ is Θ(K log K). By
comparison  our results indicate that even if the f∗
k have overlapping supports and we do not know
Λ∗  as long as ∆MLE = Ω(1) (resp. ∆MV = Ω(1)) then Ω(K log K) samples sufﬁce to learn π∗. In
other words  SSL is approximately as difﬁcult as CCP in very general settings.

k has disjoint support  λ∗

4.4 Classiﬁcation error
So far our results have focused on the probability of recovery of the unknown permutation π∗. We
can further bound the classiﬁcation error of the classiﬁer (3) in terms of the Wasserstein distance
W1(Λ  Λ∗) between Λ and Λ∗ as follows:
Theorem 4.7 (Classiﬁcation error). Let g∗ = gΛ∗ π∗ denote the Bayes classiﬁer. If π∗(αb) =
arg mini dTV(fi  f∗

b ) then there is a constant C > 0 depending on K and Λ∗ such that
|λπ∗(αb) − λ∗
b|.

P(gΛ π∗ (X) (cid:54)= Y ) ≤ P(g∗(X) (cid:54)= Y ) + C · W1(Λ  Λ∗) +

(cid:88)

This theorem allows for the possibility that the mixture model Λ learned from the unlabeled data is
not the same as Λ∗ (e.g. the true mixing measure corresponding to the true class conditionals). It is
thus necessary to assume that the mismatch between Λ and Λ∗ is not so bad that the closest density
fi to f∗

The interpretation of this theorem is as follows: Given Λ  we learn a permutation(cid:98)π =(cid:98)πn(Λ) from n
labeled samples  e.g. using either the MLE (4) or MV (5). Together  the pair (Λ (cid:98)π) deﬁnes a classiﬁer

b is something other than fπ∗(αb).

b

6

terms of the Bayes error. Since(cid:98)π = π∗ with high probability  Theorem 4.7 implies that
gΛ (cid:98)π via (3). We are interested in bounding the probability of misclassiﬁcation P(gΛ (cid:98)π(X) (cid:54)= Y ) in
|λπ∗(αb) − λ∗
b|.

P(gΛ (cid:98)π(X) (cid:54)= Y ) ≤ P(g∗(X) (cid:54)= Y ) + C · W1(Λ  Λ∗) +

(cid:88)

b

risk is zero. This is clearly a very strong conclusion.
Identiﬁability and misspeciﬁcation As discussed in Section 2  although Λ∗ will in general be
nonidentiﬁable  the unlabeled data may identify some other mixing measure Λ (see e.g. [17]).

In this case  there is an irreducible error quantiﬁed by the Wasserstein distance W1(Λ  Λ∗). In fact  if
W1(Λ  Λ∗) = 0  then Theorem 4.7 implies that P(gΛ (cid:98)π(X) (cid:54)= Y ) ≤ P(g∗(X) (cid:54)= Y )  i.e. the excess
Suppose that(cid:98)Λm is a mixing measure estimated from m unlabeled samples and that W1((cid:98)Λm  Λ) → 0.
Corollary 4.8. Suppose W1((cid:98)Λm  Λ) = O(rm) for some rm → 0 where m is the number of unlabeled

The question then is how much the misspeciﬁed Λ helps in classiﬁcation.

samples and π∗(αb) = arg mini dTV(fi  f∗

b ). Then if(cid:98)π = π∗ 

P(g(cid:98)Λm (cid:98)π(X) (cid:54)= Y ) ≤ P(g∗(X) (cid:54)= Y ) + C · rm + C · W1(Λ  Λ∗).

In particular  if W1(Λ  Λ∗) = 0  then

P(g(cid:98)Λm (cid:98)π(X) (cid:54)= Y ) − P(g∗(X) (cid:54)= Y ) = O(rm).

it is assumed that we know (1) perfectly. This amounts to taking(cid:98)Λm = Λ in the previous results  or

Clairvoyant SSL Previous work [5  6  11] has studied the so-called clairvoyant SSL case in which
equivalently m = ∞. Under this assumption  we have perfect knowledge of the decision regions and
only need to learn the label permutation π∗. Then Corollary 4.8 implies that with high probability 
we can learn a Bayes classiﬁer for the problem using ﬁnitely many labeled samples.

Convergence rates The convergence rate rm used here is essentially the rate of convergence in
estimating an identiﬁable mixture model  which is well-studied for parametric mixture models [35–
37]. In particular  for so-called strongly identiﬁable parametric mixture models  the minimax rate of
convergence attains the optimal root-m rate rm = m−1/2 [35].1 Asymptotic consistency theorems
for nonparametric mixtures can be found in Aragam et al. [17].

Comparison to supervised learning (SL). Previous work [11] has compared the sample complex-
ity of SSL to SL under a cluster-type assumption. While a precise characterization of these trade-offs
is not the main focus of this paper  we note in passing here the following: If the minimax risk of
SL for a particular problem is larger than W1(Λ  Λ∗)  then Theorem 4.7 implies that SSL provably
outperforms SL on ﬁnite samples.

5 Algorithms

One of the signiﬁcant appeals of MV (5) is its simplicity. It is conceptually easy to understand and
trivial to implement. The MLE (4)  on the other hand  is more subtle and difﬁcult to compute in
practice. In this section  we discuss two algorithms for computing the MLE: 1) An exact algorithm
based on ﬁnding the maximum weight perfect matching in a bipartite graph by the Hungarian
algorithm [39]  and 2) Greedy optimization.
Deﬁne Ck = {i : Y (i) = αk}. Consider the weighted complete bipartite graph G = (VK K  w) with
edge weights

Since a permutation π deﬁnes a perfect matching on G  the log-likelihood can be rewritten as

w(k  k(cid:48)) =

(cid:88)
log(cid:0)λk(cid:48)fk(cid:48)(X (i))(cid:1) 
(cid:88)
log(cid:0)λπ(αk)fπ(αk)(X (i))(cid:1) =

i∈Ck

K(cid:88)

∀k  k(cid:48) ∈ [K]

K(cid:88)

k=1

w(k  π(αk)) 

(cid:96)n(π; Λ) =

k=1

i∈Ck

1This paper corrects an earlier result due to Chen [38] that claimed an m−1/4 minimax rate.

7

the right side of which is the total weight of the matching π. Hence  the maximizer(cid:98)πMLE can be

found by ﬁnding a perfect matching for this graph that has maximum weight. This can be done in
O(K 3) using the well-known Hungarian algorithm [39].
We can also approximately solve the matching problem by a greedy method: Assign the kth class to

(cid:98)πG(αk) = arg max

k(cid:48)∈[K]

w(k  k(cid:48)) = arg max
k(cid:48)∈[K]

log(cid:0)λk(cid:48)fk(cid:48)(X (i))(cid:1) 

(cid:88)

i∈Ck

plement and can be viewed as a “soft interpolation” of(cid:98)πMLE and(cid:98)πMV as follows: If we deﬁne
wMV(k  k(cid:48)) = (cid:80)

This greedy heuristic isn’t guaranteed to achieve optimal matching  however  it is simple to im-
1(X (i) ∈ Dk(cid:48)(Λ))  we can see that a training example (X (i)  Y (i) = αk)
contributes 1 to wMV(k  k(cid:48)) if k(cid:48) = arg maxj λjfj(X (i))  and contributes 0 to wMV(k  k(cid:48)) other-
wise. By comparison  for the greedy heuristic  a training example (X (i)  Y (i) = αk) contributes
log(λk(cid:48)fk(cid:48)(X (i))) to w(k  k(cid:48)). Therefore  the greedy estimator can be seen as a “soft” version of MV
that also greedily optimizes the MLE objective.

i∈Ck

6 Experiments

In order to evaluate the relative performance of the proposed estimators in practice  we implemented
each of the three methods described in Section 5 on simulated and real data. These experiments
also illustrate the gap ∆MLE(Λ) (resp. ∆MV(Λ)) that appears in Theorem 4.1 (resp. Theorem 4.3):
In many of the examples  although the learned mixture is badly misspeciﬁed and the true class
conditionals overlap signiﬁcantly  it is still possible to recover π∗ with fewer than 100 labeled samples
(sometimes signiﬁcantly fewer).
Our experiments consider three settings: (i) Parametric mixtures of Gaussians  (ii) A nonparametric
mixture model  and (iii) Real data from MNIST. In each experiment  a random true mixture model
Λ∗ was generated from one of these settings  and then N = 99 labeled samples were drawn from
this mixture model. We generated Λ∗ under different separation conditions  from well-separated to
overlapping. Then  Λ was generated in two ways: (a) Λ = Λ∗  corresponding to a setting where the
true decision boundaries are known  and (b) Λ (cid:54)= Λ∗ by perturbing the components and weights of
Λ∗ by a parameter η > 0 (see Appendix A for details). Λ was then used to estimate π∗ using each of
the three algorithms described in the previous section for the ﬁrst n = 3  6  9  . . .   99 labeled samples.

This procedure was repeated T = 50 times (holding Λ∗ and Λ ﬁxed) in order to estimate P((cid:98)π = π∗).

Full details of the experiments can be found in Appendix A.
Mixture of Gaussians A random Gaussian mixture model with K ∈ {2  4  9  16} and dimension
d = 2. Since the lower bounds (8) and (9) are dimension-free  we tested examples with d = 10 as
well with similar results.
Nonparametric mixture model A nonparametric mixture model with K = 4. Each f∗
to be a random Gaussian mixture. Thus  the overall density is a “mixture of Gaussian mixtures”.

k was chosen

MNIST To approximate real data  we used training data from the MNIST dataset to build K = 10
class conditionals f∗
k from real data using kernel density estimates. For labeled data  we sampled
from the test data. To simulate the case Λ (cid:54)= Λ∗  we contaminated the training labels by randomly
switching 10% of the labels.
The results are shown in Figure 2. As expected  the MLE performs by far the best  obtaining near
perfect recovery of π∗ with fewer than n = 20 labeled samples on synthetic data  and fewer than
n = 40 on MNIST. Unsurprisingly  the most difﬁcult case was K = 16  in which only the MLE was
able recover the true permutation > 50% of the time. By increasing n  the MLE is eventually able to
learn this most difﬁcult case  in accordance with our theory. Furthermore  the MLE is much more
robust to misspeciﬁcation Λ (cid:54)= Λ∗ and component overlap compared to the others. This highlights
the advantage of leveraging density information in the MLE  which is ignored by the MV estimator).
These results also illustrate how the gaps ∆MLE and ∆MV affect learning π∗: Even when W1(Λ  Λ∗)
is large  the labeled sample complexity is relatively small (fewer than n = 100 in general). For
example  see Fig 4 in Appendix A for an illustration of the difﬁculty of the case K = 16. Furthermore 
in Appendix A  we also compare the classiﬁcation accuracy of the resulting SSL classiﬁers with a

8

(a) Mixture of Gaussians

(b) Mixture of Gaussian mixtures

(c) MNIST

Figure 2: Performance of MLE (Hungarian - Green; Greedy - Blue) and MV (Red). Solid line
and dashed line correspond to the performance when Λ∗ = Λ and Λ∗ (cid:54)= Λ  respectively. Columns
correspond to the number of classes K; rows correspond to decreasing separation; e.g. the bottom
rows in each ﬁgure are the least separated.

standard supervised baseline (LeNet) on the MNIST dataset. When sample size is small  it is clear
that our proposed estimators are more accurate. In accordance with our theory  the accuracy of the
SSL classiﬁers plateaus around 96% due to misspeciﬁcation of Λ∗  as measured by W1(Λ  Λ∗).

7 Discussion

Using nonparametric mixture models as a foundation  we analyzed the labeled sample complexity
of semi-supervised learning. Our results allow for arbitrary  possibly heuristic estimators of a
mixing measure Λ that is used to approximate the unlabeled data distribution F ∗. This mixing
measure deﬁnes decision boundaries that can be used to deﬁne a semi-supervised classiﬁer whose
classiﬁcation accuracy is controlled by the Wasserstein distance between Λ and Λ∗  the true mixing
measure corresponding to the class conditional distributions. This draws an explicit connection
between the quality of what is learned from the unlabeled data (i.e. Λ) and the quality of the resulting
classiﬁer. Our experiments convey two main takeaway messages: 1) It pays off to use density
information as with the MLE  and 2) When the mixture model learned from the unlabeled data is
a poor approximation of the true mixing measure  or the true class conditionals have substantial
overlap  the MLE can still learn a reasonable semi-supervised classiﬁer.
This work poses many interesting questions for future work  including instantiating our results for
practical methods for learning Λ and quantifying the dependence of ∆MLE and ∆MV on K and
d. Furthermore  it would be interesting to provide a rigorous comparison of ∆MLE and ∆MV in
speciﬁc settings in order to better understand the trade-off between the MLE and MV estimators.
Finally  exploring additional connections with existing assumptions such as the cluster and manifold
assumptions is an interesting problem.

9

0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=20918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=40918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=90918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=160918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*) number of labeled samples0918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=40918273645546372819099n0.00.10.20.30.40.50.60.70.80.91.0P(=*)number of labeled samples0918273645546372819099number of labeled samples0.00.10.20.30.40.50.60.70.80.91.0P(=*)K=10Acknowledgments

P.R. acknowledges the support of NSF via IIS-1149803  IIS-1664720  DMS-1264033  and ONR via
N000141812861. E.X. acknowledges the support of NIH R01GM114311  P30DA035778.

References
[1] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In
Proceedings of the eleventh annual conference on Computational learning theory  pages 92–100.
ACM  1998.

[2] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-
supervised learning with deep generative models. In Advances in Neural Information Processing
Systems  pages 3581–3589  2014.

[3] Zihang Dai  Zhilin Yang  Fan Yang  William W Cohen  and Ruslan R Salakhutdinov. Good
semi-supervised learning that requires a bad gan. In Advances in Neural Information Processing
Systems  pages 6513–6523  2017.

[4] Martin Azizyan  Aarti Singh  Larry Wasserman  et al. Density-sensitive semisupervised infer-

ence. The Annals of Statistics  41(2):751–771  2013.

[5] Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern

Recognition Letters  16(1):105–111  1995.

[6] Vittorio Castelli and Thomas M Cover. The relative value of labeled and unlabeled samples
in pattern recognition with an unknown mixing parameter. IEEE Transactions on information
theory  42(6):2102–2117  1996.

[7] Fabio G Cozman  Ira Cohen  and Marcelo C Cirelo. Semi-supervised learning of mixture
models. In Proceedings of the 20th International Conference on Machine Learning (ICML-03) 
pages 99–106  2003.

[8] Matti Kääriäinen. Generalization error bounds using unlabeled data. In International Conference

on Computational Learning Theory  pages 127–142. Springer  2005.

[9] Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses.

The Journal of Machine Learning Research  14(1):1229–1250  2013.

[10] Philippe Rigollet. Generalization error bounds in semi-supervised classiﬁcation under the

cluster assumption. Journal of Machine Learning Research  8(Jul):1369–1392  2007.

[11] Aarti Singh  Robert Nowak  and Xiaojin Zhu. Unlabeled data: Now it helps  now it doesn’t. In

Advances in neural information processing systems  pages 1513–1520  2009.

[12] Larry Wasserman and John D Lafferty. Statistical analysis of semi-supervised regression. In

Advances in Neural Information Processing Systems  pages 801–808  2008.

[13] Xiaojin Zhu  Zoubin Ghahramani  and John D Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03)  pages 912–919  2003.

[14] Shai Ben-David  Tyler Lu  and Dávid Pál. Does unlabeled data provably help? worst-case
analysis of the sample complexity of semi-supervised learning. In COLT  pages 33–44  2008.

[15] Amir Globerson  Roi Livni  and Shai Shalev-Shwartz. Effective semisupervised learning on

manifolds. In Conference on Learning Theory  pages 978–1003  2017.

[16] Malte Darnstädt  Hans Ulrich Simon  and Balázs Szörényi. Unlabeled data does provably help.

2013.

[17] Bryon Aragam  Chen Dan  Pradeep Ravikumar  and Eric Xing. Identiﬁability of nonparametric

mixture models and bayes optimal clustering. arXiv preprint  arXiv:1802.04397  2018.

10

[18] Olivier Collier and Arnak S Dalalyan. Minimax rates in permutation estimation for feature

matching. The Journal of Machine Learning Research  17(1):162–192  2016.

[19] Fajwel Fogel  Rodolphe Jenatton  Francis Bach  and Alexandre d’Aspremont. Convex relax-
ations for permutation problems. In Advances in Neural Information Processing Systems  pages
1016–1024  2013.

[20] Cong Han Lim and Stephen Wright. Beyond the birkhoff polytope: Convex relaxations for
vector permutation problems. In Advances in Neural Information Processing Systems  pages
2168–2176  2014.

[21] Sara van de Geer and Peter Bühlmann. (cid:96)0-penalized maximum likelihood for sparse directed

acyclic graphs. Annals of Statistics  41(2):536–567  2013.

[22] Bryon Aragam  Arash A. Amini  and Qing Zhou. Learning directed acyclic graphs with

penalized neighbourhood regression. arXiv:1511.08963  2016.

[23] Ashwin Pananjady  Martin J Wainwright  and Thomas A Courtade. Linear regression with an
unknown permutation: Statistical and computational limits. In Communication  Control  and
Computing (Allerton)  2016 54th Annual Allerton Conference on  pages 417–424. IEEE  2016.

[24] Nicolas Flammarion  Cheng Mao  and Philippe Rigollet. Optimal rates of statistical seriation.

arXiv preprint arXiv:1607.02435  2016.

[25] Matthias Seeger. Learning with labeled and unlabeled data. Technical report  2000.

[26] XuanLong Nguyen. Convergence of latent mixing measures in ﬁnite and inﬁnite mixture models.

The Annals of Statistics  41(1):370–400  2013.

[27] Henry Teicher. Identiﬁability of mixtures. The annals of Mathematical statistics  32(1):244–248 

1961.

[28] Henry Teicher. Identiﬁability of ﬁnite mixtures. The annals of Mathematical statistics  pages

1265–1269  1963.

[29] Sidney J Yakowitz and John D Spragins. On the identiﬁability of ﬁnite mixtures. The Annals of

Mathematical Statistics  pages 209–214  1968.

[30] O Barndorff-Nielsen. Identiﬁability of mixtures of exponential families. Journal of Mathemati-

cal Analysis and Applications  12(1):115–121  1965.

[31] Henry Teicher. Identiﬁability of mixtures of product measures. The Annals of Mathematical

Statistics  38(4):1300–1302  1967.

[32] Peter Hall and Xiao-Hua Zhou. Nonparametric estimation of component distributions in a

multivariate mixture. Annals of Statistics  pages 201–224  2003.

[33] Donald J Newman. The double dixie cup problem. The American Mathematical Monthly  67

(1):58–61  1960.

[34] Philippe Flajolet  Daniele Gardy  and Loÿs Thimonier. Birthday paradox  coupon collectors 
caching algorithms and self-organizing search. Discrete Applied Mathematics  39(3):207–229 
1992.

[35] Philippe Heinrich and Jonas Kahn. Minimax rates for ﬁnite mixture estimation. arXiv preprint

arXiv:1504.03506  2015.

[36] Nhat Ho and XuanLong Nguyen. On strong identiﬁability and convergence rates of parameter

estimation in ﬁnite mixtures. Electronic Journal of Statistics  10(1):271–307  2016.

[37] Nhat Ho and XuanLong Nguyen. Singularity structures and impacts on parameter estimation in

ﬁnite mixtures of distributions. arXiv preprint arXiv:1609.02655  2016.

[38] Jiahua Chen. Optimal rate of convergence for ﬁnite mixture models. Annals of Statistics  pages

221–233  1995.

11

[39] Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics

(NRL)  2(1-2):83–97  1955.

[40] Luc Devroye  László Györﬁ  and Gábor Lugosi. A probabilistic theory of pattern recognition 

volume 31. Springer Science &amp; Business Media  2013.

12

,Chen Dan
Liu Leqi
Bryon Aragam
Pradeep Ravikumar
Eric Xing
Tanner Fiez
Lalit Jain
Kevin Jamieson
Lillian Ratliff