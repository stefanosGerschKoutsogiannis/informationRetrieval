2018,Model-Agnostic Private Learning,We design differentially private learning algorithms that are agnostic to the learning model assuming access to limited amount of unlabeled public data. First  we give a new differentially private algorithm for answering a sequence of $m$ online classification queries (given by a sequence of $m$ unlabeled public feature vectors) based on a private training set. Our private algorithm follows the paradigm of subsample-and-aggregate  in which any generic non-private learner is trained on disjoint subsets of the private training set  then for each classification query  the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of distance-to-instability framework [Smith & Thakurta 2013] and the sparse-vector technique [Dwork et al. 2009  Hardt & Talwar 2010].  We show that our algorithm makes a conservative use of the privacy budget. In particular  if the underlying non-private learner yields classification error at most $\alpha\in (0  1)$  then our construction answers more queries  by at least a factor of $1/\alpha$ in some cases  than what is implied by a straightforward application of the advanced composition theorem for differential privacy. Next  we apply the knowledge transfer technique to construct a private learner that outputs a classifier  which can be used to answer unlimited number of queries. In the PAC model  we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases. As in non-private sample complexity  our bounds are completely characterized by the VC dimension of the concept class.,Model-Agnostic Private Learning

Raef Bassily∗

Om Thakkar†

Abhradeep Thakurta‡

Abstract

We design differentially private learning algorithms that are agnostic to the learn-
ing model assuming access to a limited amount of unlabeled public data. First 
we provide a new differentially private algorithm for answering a sequence of m
online classiﬁcation queries (given by a sequence of m unlabeled public feature
vectors) based on a private training set. Our algorithm follows the paradigm of
subsample-and-aggregate  in which any generic non-private learner is trained on
disjoint subsets of the private training set  and then for each classiﬁcation query 
the votes of the resulting classiﬁers ensemble are aggregated in a differentially
private fashion. Our private aggregation is based on a novel combination of the
distance-to-instability framework [26]  and the sparse-vector technique [15  18].
We show that our algorithm makes a conservative use of the privacy budget. In
particular  if the underlying non-private learner yields a classiﬁcation error of at
most α ∈ (0  1)  then our construction answers more queries  by at least a fac-
tor of 1/α in some cases  than what is implied by a straightforward application
of the advanced composition theorem for differential privacy. Next  we apply the
knowledge transfer technique to construct a private learner that outputs a classiﬁer 
which can be used to answer an unlimited number of queries. In the PAC model 
we analyze our construction and prove upper bounds on the sample complexity
for both the realizable and the non-realizable cases. Similar to non-private sample
complexity  our bounds are completely characterized by the VC dimension of the
concept class.

1

Introduction

The main goal in the standard setting of differentially private learning is to design a differentially
private learner that  given a private training set as input  outputs a model (or  a classiﬁer) that is
safe to publish. Despite being a natural way to deﬁne the private learning problem  there are several
limitations with this standard approach. First  there are pessimistic lower bounds in various learning
problems implying that the error associated with the ﬁnal private model will generally have neces-
sary dependence on the dimensionality of the model [2]  or the size of the model class [9]. Second 
this approach often requires non-trivial  white-box modiﬁcation of the existing non-private learn-
ers [19  11  21  26  2  27  1]  which can make some of these constructions less practical since they
require making changes in the infrastructure of the existing systems. Third  designing algorithms
for this setting often requires knowledge about the underlying structure of the learning problem 
e.g.  speciﬁc properties of the model class [4  5  9]; or convexity  compactness  and other geometric
properties of the model space [2  27].
We study the problem of differentially private learning when the learner has access to a limited
amount of public unlabeled data. Our central goal is to characterize in a basic model  such as the
standard PAC model  the improvements one can achieve for private learning in such a relaxed setting

∗Department of Computer Science & Engineering  The Ohio State University. bassily.1@osu.edu
†Department of Computer Science  Boston University. omthkkr@bu.edu
‡Department of Computer Science  University of California Santa Cruz. aguhatha@ucsc.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

compared to the aforementioned standard setting. Towards this goal  we ﬁrst consider a simpler
problem  namely  privately answering classiﬁcation queries given by a sequence of public unlabeled
data Q = {x1 ···   xm}. In this problem  one is given a private labeled dataset denoted by D 
and the goal is to design an (  δ)-differentially private algorithm that labels all m public feature
vectors in Q. In designing such an algorithm  there are four main goals we aim to achieve: (i) We
wish to provide an algorithm that enables answering as many classiﬁcation queries as possible while
ensuring (  δ)-differential privacy. This is a crucial property for the utility of such an algorithm
since the utility in this problem is limited by the number of queries we can answer while satisfying
the target privacy guarantee. (ii) We want to have a modular design paradigm in which the private
algorithm can use any generic non-private algorithm (learner) in a black-box fashion  i.e.  it only
has oracle access to the non-private algorithm. This property is very attractive from a practical
standpoint as the implementation of such an algorithm does not require changing the internal design
of the existing non-private algorithms. (iii) We want to have a design paradigm that enables us to
easily and formally transfer the accuracy guarantees of the underlying non-private algorithm into
meaningful accuracy guarantees for the private algorithm. The most natural measure of accuracy in
that setting would be the misclassiﬁcation rate. (iv) We want to be able to use such an algorithm
together with the public unlabeled data to construct a differentially private learner that outputs a
classiﬁer  which can then be used to answer as many classiﬁcation queries as we wish. In particular 
given the second goal above  the ﬁnal private learner would be completely agnostic to the intricacies
of the underlying non-private learner and its model. Namely  it would be oblivious to whether the
model is simple logistic regression  or a multi-layer deep neural network.
Given the above goals  a natural framework to consider is knowledge aggregation and transfer 
which is inspired by the early work of Breiman [8]. The general idea is to train a non-private
learner on different subsamples from the private dataset to generate an ensemble of classiﬁers. The
ensemble is collectively used in a differentially private manner to generate privatized labels for the
given unlabeled public data. Finally  the public data together with the private labels are used to train
a non-private learner  which produces a ﬁnal classiﬁer that is safe to publish.
Related Work: For private learning via knowledge aggregation and transfer  Hamm et al. [17]
explored a similar technique  however their construction deviated from the above description. In
particular  it was a white-box construction with weak accuracy guarantees; their guarantees also in-
volved making strong assumptions about the learning model and the loss function used in training. In
a recent work [23  24]  of which [24] is independent from our work  Papernot et al. gave algorithms
that follow the knowledge transfer paradigm described above. Their constructions are black-box.
However  only empirical evaluations are given for their constructions; no formal utility guarantees
are provided. For the query-answering setting  a recent independent work [12] considers the prob-
lem of private prediction  but only in the single-query setting  whereas we study the multiple-query
setting. The earliest idea of using ensemble classiﬁers to provide differentially private prediction
can be traced to Dwork  Rothblum  and Thakurta from 2013.
Our Techniques: In this work  we give a new construction for privately answering classiﬁcation
queries that is based on a novel framework combining two special techniques in the literature of dif-
ferential privacy  namely  the subsampling stability framework [22  26] and the sparse vector tech-
nique [15  18  16]. Our construction also follows the knowledge aggregation and transfer paradigm 
but it exploits the stability properties of good non-private learners in a quantiﬁable and formal man-
ner. Our construction is based on the following idea: if a good learner is independently trained k
times on equally sized  independent training sets  then one would expect the corresponding output
classiﬁers h1 ···   hk to predict “similarly” on a new example from the same distribution. Using
this idea  we show that among m classiﬁcation queries  one only needs to “pay the price of privacy”
for the queries for which there is signiﬁcant disagreement among the k classiﬁers. Using our con-
struction and the unlabeled public data  we also provide a ﬁnal private learner. We show via formal
and quantiﬁable guarantees that our construction achieves our four main goals stated earlier.
We note that our framework is not restricted to classiﬁcation queries; it can be used for privately
answering any sequence of online queries that satisfy certain stability properties in the sense of [26].
Due to space limitations  and to avoid distracting the reader from the main results of this work  we
defer the description of the generic framework to the full version  and focus here on the special case
of classiﬁcation queries.

2

1.1 Our Contributions

Answering online classiﬁcation queries using the privacy budget conservatively: In Section 3 
we give our (  δ)-differentially private construction for answering a sequence of online classiﬁcation
queries. Our construction uses any generic non-private learner in a black-box fashion. The privacy
guarantee is completely independent of the non-private learner and its accuracy. Moreover  the
accuracy guarantee can be obtained directly from the accuracy of the non-private learner  i.e.  the
construction allows us to directly and formally “transform” the accuracy guarantee for the non-
private learner into an accuracy guarantee for the ﬁnal private algorithm.
We provide a new privacy analysis for the novel framework combining subsampling stability and
sparse vector techniques. We analyze the accuracy of our algorithm in terms of its misclassiﬁcation
rate  deﬁned as the ratio of misclassiﬁed queries to the total number of queries  in the standard (ag-
nostic) PAC model. Our accuracy analysis is new  and is based on a simple counting argument. We
consider both the realizable and non-realizable (agnostic) cases. In the realizable case  the underly-
ing non-private learner is assumed to be a PAC learner for a hypothesis class H of VC-dimension
V . The private training set consists of n labeled examples  where the labels are generated by some
unknown hypothesis h∗ ∈ H. The queries are given by a sequence of m i.i.d. unlabeled domain
points drawn from the same distribution as the domain points in the training set. We show that  with
high probability  our private algorithm can answer up to ≈ n/V queries with a misclassiﬁcation rate
of ≈ V /n  which is essentially the optimal misclassiﬁcation rate attainable without privacy. Thus 
answering those queries essentially comes with no cost for privacy. When answering m > n/V
queries  the misclassiﬁcation rate is ≈ mV 2/n2. A straightforward application of the advanced
composition theorem of differential privacy would have led to a misclassiﬁcation rate ≈ √
mV /n 
which can be signiﬁcantly larger than our rate. This is because our construction pays a privacy cost
only for “hard” queries for which the PAC learner tends to be incorrect. Our result for the realiz-
able case is summarized below. We also provide an analogous statement for the non-realizable case
(Theorem 3.5).
Informal Theorem 1.1 (Corresponding to Theorem 3.4). Given a PAC learner for a class H of VC-
dimension V   a private training set of size n  and assuming realizability  our private construction
(Algorithm 2) answers a sequence of up to ˜Ω(n/V ) binary classiﬁcation queries such that  with
high probability  the misclassiﬁcation rate is ˜O(V /n). When the number of queries m is beyond
˜Ω(n/V )  then with high probability  the misclassiﬁcation rate is ˜O(mV 2/n2).

A model-agnostic private learner with formal guarantees: In Section 4  we use the knowledge
transfer technique to bootstrap a private learner from our construction above. The idea is to use
our private construction to label a sufﬁcient number of public feature vectors. Then  we use these
newly labeled public data for training a non-private learner to ﬁnally output a classiﬁer. Since there
is no privacy constraint associated with the public data  the overall construction remains private
as differential privacy is closed under post-processing. Note that this construction also uses the
non-private learner as a black box  and hence it is agnostic to the structure of such learner and the
associated model. This general technique has also been adopted in [23]. Our main contribution here
is that we provide formal and explicit utility guarantees for the ﬁnal private learner in the standard
(agnostic) PAC model. Our guarantees are in terms of upper bounds on the sample complexity
(x y)∼D [h(x) (cid:54)= y] denote the true
in both realizable and non-realizable cases. Let err(h;D) (cid:44)
classiﬁcation error of a hypothesis h. Given black-box access to an agnostic PAC learner for a class
H of VC-dimension V   we obtain the following results:
given access to m = ˜O(cid:0) V
(cid:1) unlabeled public data points  w.h.p. outputs a classiﬁer ˆh ∈ H such
Informal Theorem 1.2 (Corresponding to Theorems 4.2  4.3). Let 0 < α < 1. Let n be the size
that the following guarantees hold: (i) Realizable case: err(ˆh;D) ≤ α for n = ˜O(cid:0)V 3/2/α3/2(cid:1) 
of the private training set. There exists an (  δ)-differentially private algorithm (Algorithm 3) that 
and (ii) Agnostic case: err(ˆh;D) ≤ α + O(γ) for n = ˜O(cid:0)V 3/2/α5/2(cid:1)   where γ = min
h∈H err(h;D).
Our bounds are only a factor of ˜O((cid:112)V /α) worse than the corresponding optimal non-private

bounds. In the agnostic case  however  we note that the accuracy of the output hypothesis in our
case has a suboptimal dependency (by a small constant factor) on γ (cid:44) min

P

α2

h∈H err(h;D).

3

We note that the same construction can serve as a private learner in a less restrictive setting where
only the labels of the training set are considered private information. This setting is known as label-
private learning  and it has been explored before in [10] and [6]. Both works have only considered
pure  i.e.  (  0)  differentially private learners  and their constructions are white-box  i.e.  they do not
allow for using a black-box non-private learner. The bounds in [10] involve smoothness assumptions
on the underlying distribution. In [6]  an upper bound on the sample complexity is derived for the
realizable case. Their bound is a factor of O(1/α) worse than the optimal non-private bound for the
realizable case.

2 Preliminaries

In this section  we formally deﬁne the notation  provide important deﬁnitions  and state the main
existing results used in this work.
We denote the data universe by U = X × Y  where X denotes abstract domain for unla-
beled data (feature-vector space) and Y = {0  1}. An n-element dataset is denoted by D =
{(x1  y1)  (x2  y2)  . . .   (xn  yn)} ∈ U n. For any two datasets D  D(cid:48) ∈ U∗  we denote the sym-
metric difference between them by D∆D(cid:48).
We will use the standard notion of agnostic PAC learning [20] (see the full version for a deﬁnition).
We will also use the following parameterized version of the deﬁnition of agnostic PAC learning.
Deﬁnition 2.1 ((α  β  n)-learner for a class H). Let α  β ∈ (0  1) and n ∈ N. An algorithm Θ is an
(α  β  n) (agnostic PAC) learner if  given an input dataset D of n i.i.d. examples from the underlying
unknown distribution D  with probability 1− β it outputs a hypothesis hD with err(hD;D) ≤ γ + α 
where err(h;D) (cid:44) P

(x y)∼D [hD(x) (cid:54)= y] and γ (cid:44) min

h∈H err(h;D).

Next  we deﬁne the notion of differential privacy.
Deﬁnition 2.2 ((  δ)-Differential Privacy [13  14]). A (randomized) algorithm M with input domain
U∗ and output range R is (  δ)-differentially private (DP) if for all pairs of datasets D  D(cid:48) ∈
|D∆D(cid:48)| = 1  and every measurable S ⊆ R  we have that: Pr (M (D) ∈ S) ≤ e ·
U∗ s.t.
Pr (M (D(cid:48)) ∈ S) + δ  where the probability is over the coin ﬂips of M.

2.1 Distance to Instability Framework

Next  we describe the distance to instability framework from [26] that releases the exact value of
a function on a dataset while preserving differential privacy  provided the function is sufﬁciently
stable on the dataset. We deﬁne the notion of stability ﬁrst  and provide a pseudocode for a private
estimator for any function via this framework in Algorithm 1.
Deﬁnition 2.3 (k-stability [26]). A function f : U∗ → R is k-stable on dataset D if adding or
removing any k elements from D does not change the value of f  i.e.  ∀D(cid:48)s.t. |D∆D(cid:48)| ≤ k  we have
f (D) = f (D(cid:48)). We say f is stable on D if it is (at least) 1-stable on D  and unstable otherwise.
The distance to instability of a dataset D ∈ U∗ with respect to a function f is the number of elements
that must be added to or removed from D to reach a dataset that is not stable.
Algorithm 1 Astab [26]: Private release of a classiﬁcation query via distance to instability
Input: Dataset D ∈ U∗  a function f : U∗ → R for some range R  distance to instability function

associated with f: distf : U∗ → R  threshold: Γ  privacy parameter  > 0

1: (cid:100)dist ← distf (D) + Lap (1/)
2: If(cid:100)dist > Γ  then output f (D)  else output ⊥

Theorem 2.4 (Privacy guarantee for Astab). If the threshold Γ = log(1/δ)/  and the distance to
[f (D) is k-stable]  then Astab (Algorithm 1) is (  δ)-DP.
instability function distf (D) = arg max
Theorem 2.5 (Utility guarantee for Astab). If the threshold Γ = log(1/δ)/  the distance to insta-
bility function is chosen as in Theorem 2.4  and f (D) is ((log(1/δ) + log(1/β)) /)-stable  then
Algorithm 1 outputs f (D) with probability at least 1 − β.
The proof of the above two theorems follows from [26  Proposition 3].

k

4

3 Privately Answering Classiﬁcation Queries

In this section  we instantiate the distance to instability framework (Algorithm 1) with the sub-
sample and aggregate framework [22  26]  and then combine it with the sparse vector technique
[15  16] to obtain a construction for privately answering classiﬁcation queries with a conservative
use of the privacy budget (Algorithm 2 below). We consider here the case of binary classiﬁcation
for simplicity. However  we note that one can easily extend the construction (and obtain analogous
guarantees) for multi-class classiﬁcation.
Note: The full version [3] contains a detailed discussion of a more general framework together
with a more modular and generic description of the algorithmic techniques. The description of the
algorithms in this short version involves only classiﬁcation queries.
A private training set  denoted by D 
is a set of n private binary-labeled data points
{(x1  y1)  . . .   (xn  yn)} ∈ (X × Y)n drawn i.i.d. from some (arbitrary unknown) distribution D
over X × Y. We will refer to the induced marginal distribution over X as DX . We consider a
sequence of (online) classiﬁcation queries deﬁned by a sequence of m unlabeled points from X
Q = {˜x1 ···   ˜xm} ∈ X m  drawn i.i.d. from DX   and let {˜y1 ···   ˜ym} ∈ {0  1}m be the cor-
responding true unknown labels. Algorithm 2 has oracle access to a non-private learner Θ for a
hypothesis class H. We will consider both realizable and non-realizable cases of the standard PAC
model. In particular  Θ is assumed to be an (agnostic) PAC learner for H.
Algorithm 2 AbinClas: Private Online Binary Classiﬁcation via subsample and aggregate; and sparse
vector
Input: Private dataset: D  sequence of online unlabeled public data (deﬁning the classiﬁcation
queries) Q = {˜x1 ···   ˜xm}  oracle access to a non-private learner Θ : U∗ → H for a hypothe-
sis class H  cutoff parameter: T   privacy parameters   δ > 0  failure probability: β

1: c ← 0  λ ←(cid:112)32T log(2/δ)/  and k ← 34
2: w ← 2λ · log(2m/δ)  and (cid:98)w ← w + Lap(λ)

2λ · log (4mT / min (δ  β/2))

√

3: Arbitrarily split D into k non-overlapping chunks of size n/k. Call them D1 ···   Dk
4: for j ∈ [k]  train Θ on Dj to get a classiﬁer hj ∈ H
5: for i ∈ [m] and c ≤ T do
6:
7:

Let Si = {h1(xi) ···   hk(xi)}  and for y ∈ {0  1}  let ct(y) = # times y appears in Si

← max{0  ct ((cid:98)qxi) − ct (1 −(cid:98)qxi ) − 1}

(cid:98)qxi(D) ← arg max
(cid:17)
  Γ = (cid:98)w   = 1/2λ
If outi = ⊥  then c ← c + 1 and (cid:98)w ← w + Lap(λ)

(cid:16)
y∈{0 1} [ct(y)]  dist(cid:98)qxi
D (cid:98)qxi  dist(cid:98)qxi

outi ← Astab

Output outi

8:
9:
10:
Theorem 3.1 (Privacy guarantee for AbinClas). Algorithm AbinClas (Algorithm 2) is (  δ)-DP.
The proof of this theorem follows from combining the guarantees of the distance to instability frame-
work [26]  and the sparse vector technique [16]. The idea is that in each round of query response 
if the algorithm outputs a label in {0  1}  then there is “no loss in privacy” in terms of  (as there
is sufﬁcient consensus). However  when the output is ⊥  there is a loss of privacy. This argument
is formalized via the distance to instability framework. Sparse vector helps account for the privacy
loss across all the m queries. A formal proof of this theorem is deferred to the full version.
Theorem 3.2. Let α  β ∈ (0  1)  and γ (cid:44) min
(Note that in the realizable case
In Algorithm AbinClas (Algorithm 2)  suppose we set the cutoff parameter as T =
γ = 0).
. If Θ is an (α  β/k  n/k)-agnostic PAC learner (Deﬁ-
3
nition 2.1)  where k is as deﬁned in AbinClas  then i) with probability at least 1 − 2β  AbinClas does
not halt before answering all the m queries in Q  and outputs ⊥ for at most T queries; and ii) the
misclassiﬁcation rate of AbinClas is at most T /m = O(γ + α).
Proof. First  notice that Θ is an (α  β/k  n/k)-agnostic PAC learner  hence w.p. ≥ 1 − β  the
misclassiﬁcation rate of hj for all j ∈ [k] is at most γ + α. So  by the standard Chernoff’s
bound  with probability at least 1 − β none of the hj’s misclassify more than (γ + α)m +

(γ + α)m +(cid:112)(γ + α)m log(m/β)/2

h∈H err(h;D).

(cid:16)

(cid:17)

5

(cid:12)(cid:12)(cid:12)(cid:12)(cid:26)

i ∈ [m] : |{j ∈ [k] : hj(˜xi) (cid:54)= ˜yi}| > ξk

there are at most 3B queries ˜xi ∈ Q  where the votes of

argument (Lemma 3.3) to bound the number of queries for which at least k/3 classiﬁers in the
ensemble {h1  . . .   hk} result in a misclassiﬁcation.
Lemma 3.3. Consider a set of {(˜x1  ˜y1)  . . .   (˜xm  ˜ym)} ⊂ X × Y  and k binary classiﬁers
h1  . . .   hk  where each classiﬁer is guaranteed to make at most B mistakes in predicting the m
labels {˜y1  . . .   ˜ym}. For any ξ ∈ (0  1/2] 

(cid:112)(γ + α)m log(m/β)/2 (cid:44) B queries in Q. Now  we use the following Markov-style counting
(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) < B/ξ.
32 log (4mT / min (δ  β/2))(cid:112)2T log(2/δ)/ (taking into account the noise in the threshold passed

Therefore 
the ensemble
{h1(˜xi)  . . .   hk(˜xi)} has number of ones (or  zeros) > k/3 (i.e.  they signiﬁcantly disagree).
Now  to prove part (i) of the theorem  observe that to satisfy the distance to instability con-
dition (in Theorem 2.5) for the remaining m − 3B queries  it would sufﬁce to have k/3 ≥
to Astab in Step 8 of AbinClas
4). This condition on k is satisﬁed by the setting of k in AbinClas. For
part (ii)  note that by the same lemma above  w.p. 1− β  there are at least 2k/3 classiﬁers that output
the correct label in each of the remaining m−3B queries. Hence  w.p. ≥ 1−2β  Algorithm AbinClas
will correctly classify such queries. This completes the proof.
Remark 1. A natural question for using Theorem 3.2 in the agnostic case is that how would one
know the value of γ in practice  in order to set the right value for T ? One simple approach is to
set aside half the training dataset  and compute the empirical misclassiﬁcation rate with differential
privacy to get a sufﬁciently accurate estimate for γ + α (as in standard validation techniques [25]) 
and use it to set T . Since the sensitivity of misclassiﬁcation rate is small  the amount of noise added
would not affect the accuracy of the estimation. Furthermore  with a large enough training dataset 
the asymptotics of Theorem 3.2 would not change either.

Explicit misclassiﬁcation rate: In Theorem 3.2  it might seem that there is a circular dependency
of the following terms: T → α → k → T . However  the number of independent relations is
equal to the number of parameters  and hence  we can set them meaningfully to obtain non-trivial
misclassiﬁcation rates. We now obtain an explicit misclassiﬁcation rate for AbinClas in terms of the
VC-dimension of H. Let V denote the VC-dimension of H. First  we consider the realizable case
(γ = 0). Our result for this case is formally stated in the following theorem.
Theorem 3.4 (Misclassiﬁcation rate in the realizable case). For any β ∈ (0  1)  there exists M =
1 − β  AbinClas yields the following misclassiﬁcation rate: (i) ˜O(V / n) for up to M queries  and

˜Ω( n/V )  and a setting for T = ˜O(cid:0) ¯m2 V 2/2 n2(cid:1)  where ¯m (cid:44) max(M  m)  such that w.p. ≥
(ii) ˜O(cid:0)mV 2/2 n2(cid:1) for m > M queries.

Proof. By standard uniform convergence arguments [25]  there is an (α  β  n/k)-PAC learner with
misclassiﬁcation rate α = ˜O (kV /n). Setting T as in Theorem 3.2 with the aforementioned setting
of α  and setting k as in Algorithm AbinClas gives the setting of T in the theorem statement. For up
to m = ˜Ω( n/V ) queries  the setting of T becomes T = O(1)  and hence Theorem 3.2 implies
AbinClas yields a misclassiﬁcation rate ˜O(V / n)  which is essentially the same as the optimal non-
private rate. Beyond ˜Ω( n/V ) queries  T = ˜O(m2 V 2/2 n2)  and hence  Theorem 3.2 implies that

the misclassiﬁcation rate of AbinClas is ˜O(cid:0)mV 2/2 n2(cid:1).

√

(cid:16)

We note that
the attainable misclassiﬁcation rate is signiﬁcantly smaller than the rate of
˜O (
mV / n) implied by a direct application of the advanced composition theorem of differential
privacy. Next  we provide analogous statement for the non-realizable case (γ > 0).
Theorem 3.5 (Misclassiﬁcation rate in the non-realizable case). For any β ∈ (0  1)  there exists
M = ˜Ω
¯m (cid:44) max{M  m}  such that w.p. ≥ 1 − β  AbinClas yields the following misclassiﬁcation rate:
(i) O(γ) + ˜O
m > M queries.

(cid:111)(cid:17)
  and a setting for T = O( ¯mγ) + ˜O(cid:0) ¯m4/3 V 2/3/2/3 n2/3(cid:1)  where
for up to M queries  and (ii) O(γ) + ˜O(cid:0)m1/3 V 2/3/2/3 n2/3(cid:1) for

(cid:110)
1/γ (cid:112) n/V
(cid:16)(cid:112)V / n
(cid:17)

min

4Detailed argument given in the full version.

6

(cid:16)(cid:112)kV /n
(cid:17)

(cid:16)(cid:112)kV /n
(cid:17)

(cid:16)

(cid:110)
1/γ (cid:112) n/V

  and hence  it has a misclassiﬁcation rate of ≈ γ + ˜O

Proof. Again  by a standard argument  Θ is (α  β  n/k)-agnostic PAC learner with α =
˜O
when trained
on a dataset of size n/k. Setting T as in Theorem 3.2 with this value of α  and setting k as in
AbinClas  and then solving for T in the resulting expression  we get the setting of T as in the the-
orem statement (it would help here to consider the cases where γ > α and γ ≤ α separately).
For up to m = ˜Ω
queries  the setting of T becomes T = O(1)  and
hence Theorem 3.2 implies AbinClas yields a misclassiﬁcation rate O(γ) + ˜O
is essentially the same as the optimal non-private rate. Beyond ˜Ω

T = O(mγ) + ˜O(cid:0)m4/3 V 2/3/2/3 n2/3(cid:1)  and hence  Theorem 3.2 implies that the misclassiﬁcation
rate of AbinClas is O(γ) + ˜O(cid:0)m1/3 V 2/3/2/3 n2/3(cid:1).

(cid:16)(cid:112)V / n
(cid:17)
(cid:110)
(cid:111)(cid:17)
1/γ (cid:112) n/V

(cid:111)(cid:17)

queries 

  which

(cid:16)

min

min

4 From Answering Queries to Model-agnostic Private Learning

In this section  we build on our algorithm and results in Section 3 to achieve a stronger objective.
In particular  we bootstrap from our previous algorithm an (  δ)-differentially private learner that
publishes a ﬁnal classiﬁer. The idea is based on a knowledge transfer technique: we use our private
construction above to generate labels for sufﬁcient number of unlabeled domain points. Then  we
use the resulting labeled set as a new training set for any standard (non-private) learner  which in
turn outputs a classiﬁer. We prove explicit sample complexity bounds for the ﬁnal private learner in
both PAC and agnostic PAC settings.
Our ﬁnal construction can also be viewed as a private learner in the less restrictive setting of label-
private learning where the learner is only required to protect the privacy of the labels in the training
set. Note that any construction for our original setting can be used as a label-private learner simply
by splitting the training set into two parts and throwing away the labels of one of them.
Let hpriv denote the mapping deﬁned by AbinClas (Algorithm 2) on a single query (unlabeled data
point). That is  for x ∈ X   hpriv(x) ∈ {0  1 ⊥} denotes the output of AbinClas on a single input
query x. Note that w.l.o.g.  we can view hpriv as a binary classiﬁer by replacing ⊥ with a uniformly
random label in {0  1}. Our private learner is described in Algorithm 3 below.
Algorithm 3 APriv: Private Learner
Input: Unlabeled set of m i.i.d. feature vectors: Q = {˜x1  . . .   ˜xm}  oracle access to our private
1: for t = 1  . . .   m do
2:
3: Output ˆh ← Θ( ˜D)  where ˜D = {(˜x1  ˆy1)  . . .   (˜xm  ˆym)}
Note that since differential privacy is closed under post-processing  APriv is (  δ)-DP w.r.t.
the
original dataset (input to AbinClas). Note also that the mapping hpriv is independent of Q; it only
depends on the input training set D (in particular  on h1  . . .   hk)  and the internal randomness of
AbinClas. We now make the following claim about hpriv.
Claim 4.1. Let 0 < β ≤ α < 1  and m ≥ 4 log(1/αβ)/α. Suppose that Θ in AbinClas (Algorithm 2)
is an (α  β/k  n/k)-(agnostic) PAC learner for the hypothesis class H. Then  with probability at
least 1− 2β (over the randomness of the private training set D  and the randomness in AbinClas)  we
have err(hpriv;D) ≤ 3γ + 7α = O(γ + α)  where γ = min

classiﬁer hpriv  oracle access to an agnostic PAC learner Θ for a class H.

ˆyt ← hpriv(˜xt)

h∈H err(h;D).

Proof. The proof largely relies on the proof of Theorem 3.2. First  note that w.p. ≥ 1 − β (over
the randomness of the input dataset D)  for all j ∈ [k]  we have err(hj;D) ≤ α. For the remainder
of the proof  we will condition on this event. Let ˜x1  . . .   ˜xm be a sequence of i.i.d. domain points 
and ˜y1  . . .   ˜ym be the corresponding (unknown) labels. Now  for every t ∈ [m]  deﬁne vt (cid:44)
1 (|{j ∈ [k] : hj(˜xt) (cid:54)= ˜yt}| > k/3). Note that since (˜x1  ˜y1)  . . .   (˜xm  ˜ym) are i.i.d.  it follows
that v1  . . .   vm are i.i.d. (this is true conditioned on the original dataset D). As in the proof of
Theorem 3.2  we have:
< β. Hence  for any

(cid:113) log(m/β)

(cid:80)m

(cid:104) 1

(cid:17)(cid:105)

α + γ +

(cid:16)

P

t=1 vt > 3

˜x1 ... ˜xm

m

2m(α+γ)

7

(cid:80)m

(cid:2) 1

m

(cid:3) < β + 3

(cid:16)

(cid:113) log(m/β)

2m(α+γ)

(cid:17) ≤ 7α + 3γ. Let

[vt] = E

t ∈ [m]  we have E
¯vt = 1 − vt. Using the same technique as in the proof of Theorem 3.2  we can show that w.p. at
least 1 − β over the internal randomness in Algorithm 2  we have ¯vt = 1 ⇒ hpriv(˜xt) = ˜yt. Hence 
conditioned on this event  we have P

[hpriv(˜xt) (cid:54)= ˜yt] ≤ P

[vt] ≤ 7α + 3γ.

[vt = 1] = E

α + γ +

t=1 vt

˜x1 ... ˜xm

˜xt

˜xt

˜xt

˜xt

α2

(cid:17)

We now state and prove the main results of this section. Let V denote the VC-dimension of H.
Theorem 4.2 (Sample complexity bound in the realizable case). Let 0 < β ≤ α < 1. Let m be such
that Θ is an (α  β  m)-agnostic PAC learner of H  i.e.  m = O
. Let the parameter T

of AbinClas (Algorithm 2) be set as in Theorem 3.4. There exists n = ˜O(cid:0)V 3/2/ α3/2(cid:1) for the size of

(cid:16) V +log(1/β)

statement  we get k = ˜O(V 2/2 α2n). Hence  there is a setting n = ˜O(cid:0)V 3/2/ α3/2(cid:1) such that

the private dataset such that  w.p. ≥ 1 − 3β  the output hypothesis ˆh of APriv (Algorithm 3) satisﬁes
err(ˆh;D) = O(α).
Proof. Let h∗ ∈ H denote the true labeling hypothesis. We will denote the true distribution D as
(DX   h∗). Note that since T is set as in Theorem 3.4  and given the value of m in the theorem
Θ is an (α  β/k  n/k)-PAC learner for H (in particular  sample complexity in the realizable case
= n/k = ˜O(V /α)). Hence  by Claim 4.1  w.p. ≥ 1 − 2β  err(hpriv;D) ≤ 7α. For the remainder of
the proof  we will condition on this event. Note that each (˜xt  ˆyt)  t ∈ [m]  is drawn independently
from (DX   hpriv). Now  since Θ is also an (α  β  m)-agnostic PAC learner for H  w.p. ≥ 1− β (over
the new set ˜D)  the output hypothesis ˆh satisﬁes
err(ˆh; (DX   hpriv)) − err(h∗; (DX   hpriv)) ≤ err(ˆh; (DX   hpriv)) − min

h∈H err(h; (DX   hpriv)) ≤ α.

Observe that
[1 (h∗(x) (cid:54)= hpriv(x))] = err(hpriv; (DX   h∗)) = err(hpriv;D) ≤ 7α 
err(h∗; (DX   hpriv)) = E
x∼DX
where the last inequality follows from Claim 4.1 (with γ = 0). Hence  err(ˆh; (DX   hpriv)) ≤ 8α.
Furthermore  observe that

(cid:104)

(cid:105) ≤ E

x∼DX

(cid:104)

(cid:105)

err(ˆh;D) = E
x∼DX

1(ˆh(x) (cid:54)= h∗(x))

= err(ˆh; (DX   hpriv)) + err(hpriv;D) ≤ 15α.

1(ˆh(x) (cid:54)= hpriv(x)) + 1(hpriv(x) (cid:54)= h∗(x))

Hence  w.p. ≥ 1 − 3β  we have err(ˆh;D) ≤ 15α.
Remark 2. In Theorem 4.2  if Θ is an ERM learner  then the value of m can be reduced to ˜O(V /α).
Hence  the resulting sample complexity would be n = ˜O(V 3/2/ α)  saving us a factor of 1√
α . This
is because the disagreement rate in the labels produced by AbinClas is ≈ α  and agnostic learning
with such a low disagreement rate can be done using ˜O(V /α) if the learner is an ERM [7  Corollary
5.2].
Remark 3. Our result involves using an agnostic PAC learner Θ. Agnostic PAC learners with opti-
mal sample complexity can be computationally inefﬁcient. One way to give an efﬁcient construction
in the realizable case (with a slightly worse sample complexity) is to use a PAC learner (rather than
an agnostic one) in APriv with target accuracy α (and hence  m = ˜O(V /α))  but then train the PAC
learner in AbinClas towards a target accuracy 1/m. Hence  the misclassiﬁcation rate of AbinClas can
be driven to zero. This yields a sample complexity bound n = ˜O(V 2/ α).
Theorem 4.3 (Sample complexity bound in the non-realizable case). Let 0 < β ≤ α < 1  and
m = O
that  w.p. ≥ 1 − 3β  the output hypothesis ˆh of (Algorithm 3) satisﬁes err(ˆh;D) = O(α + γ).

. Let T be set as in Theorem 3.5. There exists n = ˜O(cid:0)V 3/2/ α5/2(cid:1) such

(cid:16) V +log(1/β)

(cid:17)

α2

Proof. The proof is similar to the proof of Theorem 4.2.

8

5 Discussion

Implications  and comparison to prior work on label privacy: Our results also apply to the
setting of label-private learning  where the learner is only required to protect the privacy of the labels
in the training set. That is  in this setting  all unlabeled features in the training set can be viewed
as public information. This is a less restrictive setting than the setting we consider in this paper.
In particular  our construction can be directly used as a label-private learner simply by splitting the
training set into two parts and discarding the labels in one of them. The above theorems give sample
complexity upper bounds that are only a factor of ˜O
worse than the optimal non-private
sample complexity bounds. We note  however  that our sample complexity upper bound for the
agnostic case has a suboptimal dependency (by a small constant factor) on γ (cid:44) min

(cid:16)(cid:112)V /α

(cid:17)

h∈H err(h;D).

Label-private learning has been considered before in [10] and [6]. Both works have only considered
pure  i.e.  (  0)  differentially private learners for those settings  and the constructions in both works
are white-box  i.e.  they do not allow for modular construction based on a black-box access to a non-
private learner. The work of [10] gave upper and lower bounds on the sample complexity in terms
of the doubling dimension. Their upper bound involves a smoothness condition on the distribution
of the features DX . The work of [6] showed that the sample complexity (of pure differentially label-
private learners) can be characterized in terms of the VC dimension. They proved an upper bound
on the sample complexity for the realizable case. The bound of [6] is only a factor of O(1/α) worse
than the optimal non-private bound for the realizable case.
Beyond standard PAC learning with binary loss: In this paper  we used our algorithmic frame-
work to derive sample complexity bounds for the standard (agnostic) PAC model with the binary 0-1
loss. However  it is worth pointing out that our framework is applicable in more general settings.
In particular  if a surrogate loss (e.g.  hinge loss or logistic loss) is used instead of the binary loss 
then our framework can be instantiated with any non-private learner with respect to that loss. That
is  our construction does not necessarily require an (agnostic) PAC learner. However  in such case 
the accuracy guarantees of our construction will be different from what we have here for the stan-
dard PAC model. In particular  in the surrogate loss model  one often needs to invoke some weak
assumptions on the data distribution in order to bound the optimization error [25]. One can still pro-
vide meaningful accuracy guarantees since our framework allows for transferring the classiﬁcation
error guarantee of the underlying non-private learner to a classiﬁcation error guarantee for the ﬁnal
private learner.
Acknowledgement: The authors would like to thank Vitaly Feldman  and Adam Smith for helpful
discussions during the course of this project. In particular  the authors are grateful for Vitaly’s ideas
about the possible extensions of the results in Section 4  which we outlined in Remarks 2 and 3. This
work is supported by NSF grants TRIPODS-1740850  TRIPODS+X-1839317  and IIS-1447700  a
grant from the Sloan foundation  and start-up supports from OSU and UC Santa Cruz.

References
[1] Martin Abadi  Andy Chu  Ian Goodfellow  H Brendan McMahan  Ilya Mironov  Kunal Talwar 
In Proceedings of the 2016 ACM
and Li Zhang. Deep learning with differential privacy.
SIGSAC Conference on Computer and Communications Security  pages 308–318. ACM  2016.

[2] Raef Bassily  Adam Smith  and Abhradeep Thakurta. Private empirical risk minimization:
In Foundations of Computer Science (FOCS) 

Efﬁcient algorithms and tight error bounds.
2014 IEEE 55th Annual Symposium on  pages 464–473. IEEE  2014.

[3] Raef Bassily  Om Thakkar  and Abhradeep Thakurta. Model-agnostic private learning via

stability. arXiv preprint arXiv:1803.05101  2018.

[4] Amos Beimel  Shiva Prasad Kasiviswanathan  and Kobbi Nissim. Bounds on the Sample
Complexity for Private Learning and Private Data Release. In TCC  pages 437–454. Springer 
2010.

[5] Amos Beimel  Kobbi Nissim  and Uri Stemmer. Characterizing the sample complexity of

private learners. In ITCS. ACM  2013.

9

[6] Amos Beimel  Kobbi Nissim  and Uri Stemmer. Private learning and sanitization: Pure vs.

approximate differential privacy. Theory of Computing  12(1):1–61  2016.

[7] St´ephane Boucheron  Olivier Bousquet  and G´abor Lugosi. Theory of classiﬁcation: A survey

of some recent advances. ESAIM: probability and statistics  9:323–375  2005.

[8] Leo Breiman. Bagging predictors. Machine learning  24(2):123–140  1996.

[9] Mark Bun  Kobbi Nissim  Uri Stemmer  and Salil P. Vadhan. Differentially private release and
learning of threshold functions. In Venkatesan Guruswami  editor  IEEE 56th Annual Sympo-
sium on Foundations of Computer Science  FOCS 2015  Berkeley  CA  USA  17-20 October 
2015  pages 634–649. IEEE Computer Society  2015.

[10] Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private
learning. In Proceedings of the 24th Annual Conference on Learning Theory  pages 155–186 
2011.

[11] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empir-

ical risk minimization. JMLR  2011.

[12] Cynthia Dwork and Vitaly Feldman.

arXiv:1803.10266  2018.

Privacy-preserving prediction.

arXiv preprint

[13] Cynthia Dwork  Krishnaram Kenthapadi  Frank McSherry  Ilya Mironov  and Moni Naor. Our

data  ourselves: Privacy via distributed noise generation. In EUROCRYPT  2006.

[14] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference  pages 265–284.
Springer  2006.

[15] Cynthia Dwork  Moni Naor  Omer Reingold  Guy Rothblum  and Salil Vadhan. On the com-
plexity of differentially private data release: efﬁcient algorithms and hardness results. In STOC 
pages 381–390  2009.

[16] Cynthia Dwork  Aaron Roth  et al. The algorithmic foundations of differential privacy. Foun-

dations and Trends in Theoretical Computer Science  9(3-4):211–407  2014.

[17] Jihun Hamm  Yingjun Cao  and Mikhail Belkin. Learning privately from multiparty data. In

International Conference on Machine Learning  pages 555–563  2016.

[18] Moritz Hardt and Guy N. Rothblum. A multiplicative weights mechanism for privacy-

preserving data analysis. In FOCS  2010.

[19] Shiva Prasad Kasiviswanathan  Homin K. Lee  Kobbi Nissim  Sofya Raskhodnikova  and
Adam Smith. What can we learn privately? In FOCS  pages 531–540. IEEE Computer Society 
2008.

[20] Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory.

MIT Press  Cambridge  MA  USA  1994.

[21] Daniel Kifer  Adam Smith  and Abhradeep Thakurta. Private convex empirical risk minimiza-

tion and high-dimensional regression. Journal of Machine Learning Research  1:41  2012.

[22] Kobbi Nissim  Sofya Raskhodnikova  and Adam Smith. Smooth sensitivity and sampling in

private data analysis. In STOC  2007.

[23] Nicolas Papernot  Martın Abadi  ´Ulfar Erlingsson  Ian Goodfellow  and Kunal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. stat  1050  2017.
[24] Nicolas Papernot  Shuang Song  Ilya Mironov  Ananth Raghunathan  Kunal Talwar  and ´Ulfar

Erlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908  2018.

[25] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

10

[26] Adam Smith and Abhradeep Thakurta. Differentially private feature selection via stability

arguments  and the robustness of the lasso. In COLT  2013.

[27] Kunal Talwar  Abhradeep Thakurta  and Li Zhang. Nearly optimal private lasso.

2015.

In NIPS 

11

,Raef Bassily
Om Thakkar
Abhradeep Guha Thakurta