2017,Convergence Analysis of Two-layer Neural Networks with ReLU Activation,In recent years  stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However  formal theoretical understanding of why SGD can train neural networks in practice is largely missing.  In this paper  we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that  if input follows from Gaussian distribution  with standard $O(1/\sqrt{d})$ initialization of the weights  SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks  the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory  we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.  Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I  the gradient points to the wrong direction  however  a potential function $g$ gradually decreases. Then in phase II  SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence  as it moves the initial point to a better place for optimization. Experiment verifies our claims.,Convergence Analysis of Two-layer Neural Networks

with ReLU Activation

Yuanzhi Li

Computer Science Department

Princeton University

yuanzhil@cs.princeton.edu

Yang Yuan

Computer Science Department

Cornell University

yangyuan@cs.cornell.edu

Abstract

In recent years  stochastic gradient descent (SGD) based techniques has become
the standard tools for training neural networks. However  formal theoretical under-
standing of why SGD can train neural networks in practice is largely missing.
In this paper  we make progress on understanding this mystery by providing a
convergence analysis for SGD on a rich subset of two-layer feedforward networks
with ReLU activations. This subset is characterized by a special structure called
√
“identity mapping”. We prove that  if input follows from Gaussian distribution 
with standard O(1/
d) initialization of the weights  SGD converges to the global
minimum in polynomial number of steps. Unlike normal vanilla networks  the
“identity mapping” makes our network asymmetric and thus the global minimum is
unique. To complement our theory  we are also able to show experimentally that
multi-layer networks with this mapping have better performance compared with
normal vanilla networks.
Our convergence theorem differs from traditional non-convex optimization tech-
niques. We show that SGD converges to optimal in “two phases”: In phase I  the
gradient points to the wrong direction  however  a potential function g gradually
decreases. Then in phase II  SGD enters a nice one point convex region and con-
verges. We also show that the identity mapping is necessary for convergence  as it
moves the initial point to a better place for optimization. Experiment veriﬁes our
claims.

1

Introduction

Deep learning is the mainstream technique for many machine learning tasks  including image
recognition  machine translation  speech recognition  etc. [17]. Despite its success  the theoretical
understanding on how it works remains poor. It is well known that neural networks have great
expressive power [22  7  3  8  31]. That is  for every function there exists a set of weights on the
neural network such that it approximates the function everywhere. However  it is unclear how to
obtain the desired weights. In practice  the most commonly used method is stochastic gradient
descent based methods (e.g.  SGD  Momentum [40]  Adagrad [10]  Adam [25])  but to the best of
our knowledge  there were no theoretical guarantees that such methods will ﬁnd good weights.
In this paper  we give the ﬁrst convergence analysis of SGD for two-layer feedforward network with
ReLU activations. For this basic network  it is known that even in the simpliﬁed setting where the
weights are initialized symmetrically and the ground truth forms orthonormal basis  gradient descent
might get stuck at saddle points [41].
Inspired by the structure of residual network (ResNet) [21]  we add an extra identity mapping for
the hidden layer (see Figure 1). Surprisingly  we show that simply by adding this mapping  with the
standard initialization scheme and small step size  SGD always converges to the ground truth. In other

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

output

Take sum

ReLU(W(cid:62) x)

output

Take sum

(cid:76)

ReLU((I + W)(cid:62) x)

Identity
Link +x

W(cid:62)x

W(cid:62)x

input x

input x

I + W∗

I + W

I

Identity mapping

Easy for SGD

Seems hard

O

Unknown

Figure 1: Vanilla network (left)  with identity mapping (right)

Figure 2: Illustration for our result.

words  the optimization becomes signiﬁcantly easier  after adding the identity mapping. See Figure
2  based on our analysis  the region near the identity matrix I contains only one global minimum
without any saddle points or local minima  thus is easy for SGD to optimize. The role of the identity
mapping here  is to move the initial point to this easier region (better initialization).
Other than being feedforward and shallow  our network is different from ResNet in the sense that
our identity mapping skips one layer instead of two. However  as we will show in Section 5.1  the
skip-one-layer identity mapping already brings signiﬁcant improvement to vanilla networks.
Formally  we consider the following function.

f (x  W) = (cid:107)ReLU((I + W)(cid:62)x)(cid:107)1

(1)
where ReLU(v) = max(v  0) is the ReLU activation function. x ∈ Rd is the input vector sampled
from a Gaussian distribution  and W ∈ Rd×d is the weight matrix  where d is the number of input
units. Notice that I adds ei to column i of W  which makes f asymmetric in the sense that by
switching any two columns in W  we get different functions.
Following the standard setting [34  41]  we assume that there exists a two-layer teacher network with
weight W∗. We train the student network using (cid:96)2 loss:

L(W) = Ex[(f (x  W) − f (x  W∗))2]

(2)

We will deﬁne a potential function g  and show that if g is small  the gradient points to partially
correct direction and we get closer to W∗ after every SGD step. However  g could be large and thus
gradient might point to the reverse direction. Fortunately  we also show that if g is large  by doing
SGD  it will keep decreasing until it is small enough while maintaining the weight W in a nice region.
We call the process of decreasing g as Phase I  and the process of approaching W∗ as Phase II. See
Figure 3 and simulations in Section 5.3.
Our two phases framework is fundamentally different from any type of local convergence  as in Phase
I  the gradient is pointing to the wrong direction to W∗  so the path from W to W∗ is non-convex 
and SGD takes a long detour to arrive W∗. This framework could be potentially useful for analyzing
other non-convex problems.
To support our theory  we have done a few other experiments and got interesting observations.
For example  as predicted by our theorem  we found that for multilayer feedforward network with
identity mappings  zero initialization performs as good as random initialization. At the ﬁrst glance  it
contradicts the common belief “random initialization is necessary to break symmetry”  but actually
the identity mapping itself serves as the asymmetric component. See Section 5.4.
Another common belief is that neural network has lots of local minima and saddle points [9]  so
even if there exists a global minimum  we may not be able to arrive there. As a result  even when
the teacher network is shallow  the student network usually needs to be deeper  otherwise it will
underﬁt. However  both our theorem and our experiment show that if the shallow teacher network
is in a pretty large region near identity (Figure 2)  SGD always converges to the global minimum
by initializing the weights I + W in this region  with equally shallow student network. By contrast 
wrong initialization gets stuck at local minimum and underﬁt. See Section 5.2.

2

Related Work

Expressivity. Even two-layer network has great expressive power. For example  two-layer network
with sigmoid activations could approximate any continuous function [22  7  3]. ReLU is the state-of-
the-art activation function [30  13]  and has great expressive power as well [29  32  31  4  26].
Learning. Most previous results on learning neural network are negative [39  28  38]  or positive but
with algorithms other than SGD [23  43  37  14  15  16]  or with strong assumptions on the model
[1  2]. [35] proved that with high probability  there exists a continuous decreasing path from random
initial point to the global minimum  but SGD may not follow this path. Recently  Zhong et al. showed
that with initialization point found using tensor decomposition  gradient descent could ﬁnd the ground
truth for one hidden layer network [44].
Linear network and independent activation. Some previous works simpliﬁed the model by ignor-
ing the activation functions and considering deep linear networks [36  24] or deep linear residual
networks [19]  which can only learn linear functions. Some previous results are based on independent
activation assumption that the activations of ReLU and the input are independent [5  24].
Saddle points. It is observed that saddle point is not a big problem for neural networks [9  18]. In
general  if the objective is strict-saddle [11]  SGD could escape all saddle points.
2 Preliminaries
Denote x as the input vector in Rd. For now  we ﬁrst consider x sampled from normal distribution
n) ∈ Rd×d as the weights for the teacher network  W =
N (0  I). Denote W∗ = (w∗
i   wi ∈ Rd are column
(w1 ···   wn) ∈ Rd×d as the weights for the student network  where w∗
vectors. f (x  W∗)  f (x  W) are deﬁned in (1)  representing the teacher and student network.
We want to know whether a randomly initialized W will converge to W∗  if we run SGD with l2
loss deﬁned in (2). Alternatively  we can write the loss L(W) as

1 ···   w∗

Taking derivative with respect to wj  we get

∇L(W)j = 2Ex

Ex[(ΣiReLU((cid:104)ei + wi  x(cid:105)) − ΣiReLU((cid:104)ei + w∗

(cid:34)(cid:32)(cid:88)

ReLU((cid:104)ei + wi  x(cid:105)) −(cid:88)

i

i

i   x(cid:105)))2]
(cid:33)
i   x(cid:105))

(cid:35)

x1(cid:104)ej +wj  x(cid:105)≥0

ReLU((cid:104)ei + w∗

(cid:16) π

i=1

(cid:1) (ei + w∗

i )−(cid:0) π

2 − θi j

(cid:1) (ei + wi) +(cid:0)(cid:107)ei + w∗

where 1e is the indicator function that equals 1 if the event e is true  and 0 otherwise. Here
∇L(W) ∈ Rd×d  and ∇L(W)j is its j-th column.
Denote θi j as the angle between ei + wi and ej + wj  θi∗ j as the angle between ei + w∗
i and ej + wj.
. Denote I + W∗ and I + W∗ as the column-normalized version of I + W∗ and
Denote ¯v = v(cid:107)v(cid:107)2
I + W such that every column has unit norm. Since the input is from a normal distribution  one can
compute the expectation inside the gradient as follows.

Lemma 2.1 (Eqn (13) from [41]). If x ∼ N (0  I)  then −∇L(W)j = (cid:80)d
(cid:0) π
2 − θi∗ j
Remark. Although the gradient of ReLU is not well deﬁned at the point of zero  if we assume input x
is from the Gaussian distribution  the loss function becomes smooth  and the gradient is well deﬁned
everywhere.
Denote u ∈ Rd as the all one vector. Denote Diag(W) as the diagonal matrix of matrix W 
Diag(v) as a diagonal matrix whose main diagonal equals to the vector v. Denote Oﬀ-Diag(W) (cid:44)
W − Diag(W). Denote [d] as the set {1 ···   d}. Throughout the paper  we abuse the notation of
inner product between matrices W  W∗ ∇L(W)  such that (cid:104)∇L(W)  W(cid:105) means the summation of
the entrywise products. (cid:107)W(cid:107)2 is the spectral norm of W  and (cid:107)W(cid:107)F is the Frobenius norm of W.
We deﬁne the potential function g and variables gj  Aj  A below  which will be useful in the proof.
i (cid:107)2 − (cid:107)ei + wi(cid:107)2)  and variable

2 (w∗
i (cid:107)2 sin θi∗ j −(cid:107)ei + wi(cid:107)2 sin θi j

Deﬁnition 2.2. We deﬁne the potential function g (cid:44)(cid:80)d
gj (cid:44)(cid:80)

i (cid:107)2 − (cid:107)ei + wi(cid:107)2).

(cid:17)
i − wi) +

(cid:1)ej + wj

i=1((cid:107)ei + w∗

i(cid:54)=j((cid:107)ei + w∗

3

W1

W6

W10

W∗

Figure 3: Phase I: W1 → W6  W may go to
the wrong direction but the potential is shrinking.
Phase II: W6 → W10  W gets closer to W∗ in
every step by one point convexity.

Deﬁnition 2.3. Denote Aj (cid:44)(cid:80)

Figure 4: The function is one point strongly con-
vex as every point’s negative gradient points to
the center  but not convex as any line between
the center and the red region is below surface.

)  A (cid:44)(cid:80)d

i=1((ei +

(cid:62) − (ei + wi)ei + wi
(cid:62)
i(cid:54)=j((ei + w∗
) = (I + W∗)I + W∗(cid:62) − (I + W)I + W
(cid:62)

i )ei + w∗

i

i

(cid:62) − (ei + wi)ei + wi

w∗
i )ei + w∗
In this paper  we consider the standard SGD with mini batch method for training the neural network.
Assume W0 is the initial point  and in step t > 0  we have the following updating rule:

.

(cid:62)

Wt+1 = Wt − ηtGt

where the stochastic gradient Gt = ∇L(Wt) + Et with E[Et] = 0 and (cid:107)Et(cid:107)F ≤ ε. Let G2 (cid:44)
6dγ + ε  GF (cid:44) 6d1.5γ + ε  where γ is the upper bound of (cid:107)W∗(cid:107)2 and (cid:107)W0(cid:107)2 (deﬁned later). As
we will see in Lemma C.2  they are the upper bound of (cid:107)Gt(cid:107)2 and (cid:107)Gt(cid:107)F respectively.
It’s clear that L is not convex  In order to get convergence guarantees  we need a weaker condition
called one point convexity.
Deﬁnition 2.4 (One point strongly convexity). A function f (x) is called δ-one point strongly convex
in domain D with respect to point x∗  if ∀x ∈ D (cid:104)−∇f (x)  x∗ − x(cid:105) > δ(cid:107)x∗ − x(cid:107)2
2.

By deﬁnition  if a function f is strongly convex  it is also one point strongly convex in the entire space
with respect to the global minimum. However  the reverse is not necessarily true  e.g.  see Figure
4. If a function is one point strongly convex  then in every step a positive fraction of the negative
gradient is pointing to the optimal point. As long as the step size is small enough  we will ﬁnally
arrive the optimal point  possibly by a winding path. See Figure 3 for illustration  where starting from
W6 (Phase II)  we get closer to W∗ in every step. Formally  we have the following lemma.
Lemma 2.5. For function f (W)  consider the SGD update Wt+1 = Wt − ηGt  where E[Gt] =
∇f (Wt)  E[(cid:107)Gt(cid:107)2
F ] ≤ G2. Suppose for all t  Wt is always inside the δ-one point strongly convex
region with diameter D  i.e.  (cid:107)Wt − W∗(cid:107)F ≤ D. Then for any α > 0 and any T such that
T α log T ≥ D2δ2

  we have E(cid:107)WT − W∗(cid:107)2

F ≤ (1+α) log T G2

(1+α)G2   if η = (1+α) log T

δ2T

δT

.

The proof can be found in Appendix J. Lemma 2.5 uses ﬁxed step size  so it easily ﬁts the standard
practical scheme that shrinks η by a factor of 10 after every a few epochs. For example  we may
apply Lemma 2.5 every time η gets changed. Notice that our lemma does not imply that WT will
converge to W∗. Instead  it only says WT will be sufﬁciently close to W∗ with small step size η.
3 Main Theorem
Theorem 3.1 (Main Theorem). There exists constants γ > γ0 > 0 such that If x ∼ N (0  I) 
(cid:107)W0(cid:107)2 (cid:107)W∗(cid:107)2 ≤ γ0  d ≥ 100  ε ≤ γ2  then SGD for L(W) will ﬁnd the ground truth W∗ by
two phases. In Phase I  by setting η ≤ γ2
  the potential function will keep decreasing until it is
G2
2
16η steps. In Phase II  for any α > 0 and any T such that
smaller than 197γ2  which takes at most 1
T α log T ≥
F ≤ 1002(1+α) log T G2
.

  we have E(cid:107)WT − W∗(cid:107)2

  if we set η = (1+α) log T

√
√
Remarks. Randomly initializing the weights with O(1/
[27  12  20]. It is also well known that if the entries are initialized with O(1/

d) is standard in deep learning  see
d)  the spectral norm

1004(1+α)G2
F

36d

9T

δT

F

4

050100150200050100150200−5051015d).

of the random matrix is O(1) [33]. So our result matches with the common practice. Moreover  as we
will show in Section 5.5  networks with small average spectral norm already have good performance.
Thus  our assumption (cid:107)W∗(cid:107)2 = O(1) is reasonable. Notice that here we assume the spectral norm
√
of W∗ to be constant  which means the Frobenius norm (cid:107)W∗(cid:107)F could be as big as O(
The assumption that the input follows a Gaussian distribution is not necessarily true in practice
(Although this is a common assumption appeared in the previous papers [5  41  42]  and also
considered plausible in [6]). We could easily generalize the analysis to rotation invariant distributions 
and potentially more general distributions (see Section 6). Moreover  previous analyses either ignore
the nonlinear activations and thus consider linear model [36  24  19]  or directly [5  24] or indirectly
[41]1 assume that the activations are independent. By contrast  in our model the ReLU activations
are highly correlated2 as (cid:107)W(cid:107)2 (cid:107)W∗(cid:107)2 = Ω(1). As pointed out by [6]  eliminating the unrealistic
assumptions on activation independence is the central problem of analyzing the loss surface of neural
network  which was not fully addressed by the previous analyses.
To prove the main theorem  we split the process and present the following two theorems  which will
be proved in Appendix C and D.
Theorem 3.2 (Phase I). There exists a constant γ > γ0 > 0 such that If (cid:107)W0(cid:107)2 (cid:107)W∗(cid:107)2 ≤ γ0 
d ≥ 100  η ≤ γ2
  ε ≤ γ2  then gt will keep decreasing by a factor of 1 − 0.5ηd for every step 
until gt1 ≤ 197γ2 for step t1 ≤ 1
16η . After that  Phase II starts. That is  for every T > t1  we have
(cid:107)WT(cid:107)2 ≤ 1
Theorem 3.3 (Phase II). There exists a constant γ such that if (cid:107)W(cid:107)2 (cid:107)W∗(cid:107)2 ≤ γ  and g ≤ 0.1 

100 and gT ≤ 0.1.

then (cid:104)−∇L(W)  W∗ − W(cid:105) =(cid:80)d

j − wj(cid:105) > 0.03(cid:107)W∗ − W(cid:107)2
F .

j=1(cid:104)−∇L(W)j  w∗

G2
2

With these two theorems  we get the main theorem immediately.

Proof for Theorem 3.1. By Theorem 3.2  we know the statement for Phase I is true  and we will enter
phase II in 1
16η steps. After entering Phase II  based on Theorem 3.3  we simply use Lemma 2.5 by
setting δ = 0.03  D =

√
50   G = GF to get the convergence guarantee.
d

4 Overview of the Proofs

General Picture. In many convergence analyses for non-convex functions  one would like to show
that L is one point strongly convex  and directly apply Lemma 2.5 to get the convergence result.
However  this is not true for 2-layer neural network  as the gradient may point to the wrong direction 
see Section 5.3.
So when is our L one point convex? Consider the following thought experiment: First  suppose
i (cid:107)2 also go to 0. Thus  ei + wi and ei + w∗
(cid:107)W(cid:107)2 (cid:107)W∗(cid:107)2 → 0  we know (cid:107)wi(cid:107)2 (cid:107)w∗
(cid:80)
i are close to ei.
2   and θi∗ i ≈ 0. Based on Lemma 2.1  this gives us a naïve approximation
As a result  θi j  θi∗ j ≈ π
i − wi) + ej + wj
of the negative gradient  i.e.  −∇L(W)j ≈ π
i(cid:54)=j((cid:107)ei +
2 (w∗
(cid:80)d
i (cid:107)2 − (cid:107)ei + wi(cid:107)2) .
w∗
(cid:80)
i −wi) have positive inner product with W∗−W 
j−wj) and π
i=1(w∗
2 (w∗
While the ﬁrst two terms π
i (cid:107)2−(cid:107)ei +wi(cid:107)2) can point to arbitrary direction. If the last
i(cid:54)=j((cid:107)ei +w∗
the last term gj = ej + wj
term is small  it can be covered by the ﬁrst two terms  and L becomes one point strongly convex. So
i (cid:107)2 − (cid:107)ei + wi(cid:107)2).

we deﬁne a potential function closely related to the last term: g =(cid:80)d

(cid:80)d
i=1(w∗

We show that if g is small enough  L is also one point strongly convex (Theorem 3.3).
√
However  from random initialization  g can be as large as of Ω(
d)  which is too big to be covered.
Fortunately  we show that if g is big  it will gradually decrease simply by doing SGD on L. More
speciﬁcally  we introduce a two phases convergence analysis framework:

i=1((cid:107)ei + w∗

j − wj) + π

2

2

1They assume input is Gaussian and the W∗ is orthonormal  which means the activations are independent in

teacher network.

2 Let σi be the output of i-th ReLU unit  then in our setting (cid:80)

is far from being independent.

i j Cov[σi  σj] can be as large as Ω(d)  which

5

(cid:104)

Constant

Part

+

First
Order

+

Higher
Order

  W∗ − W(cid:105)

≥ [ π

2 − O(g)](cid:107)W∗ − W(cid:107)2
Lemma D.2 + Lemma D.3

F

−1.3(cid:107)W∗ − W(cid:107)2

−0.085(cid:107)W∗ − W(cid:107)2

F

Lemma D.1

F

Lemma B.2

Figure 5: Lower bounds of inner product using Taylor expansion

1. In Phase I  the potential function g is decreasing to a small value.
2. In Phase II  g remains small  so L is one point convex and thus W starts to converge to W∗.

We believe that this framework could be helpful for other non-convex problems.
Technical difﬁculty: Phase I. Our key technical challenge is to show that in Phase I  the potential
function actually decreases to O(1) after polynomial number of iterations. However  we cannot show
this by merely looking at g itself. Instead  we introduce an auxiliary variable s = (W∗ − W)u 
where u is the all one vector. By doing a careful calculation  we get their joint update rules (Lemma

C.3 and Lemma C.4): (cid:26) st+1 ≈ st − πηd

√
√
dgt +
2 st + ηO(
gt+1 ≈ gt − ηdgt + ηO(γ
d(cid:107)st(cid:107)2 + dγ2)

dγ)

√

π
2

θi∗ j =

Solving this dynamics  we can show that gt will approach to (and stay around) O(γ)  thus we enter
Phase II.
Technical difﬁculty: Phase II. Although the overall approximation in the thought experiment looks
simple  the argument is based on an over simpliﬁed assumption that θi∗ j  θi j ≈ π
2 for i (cid:54)= j.
However  when W∗ has constant spectral norm  even when W is very close to W∗  θi j∗ could be
constantly far away from π
2   which prevents us from applying this approximation directly. To get a
formal proof  we use the standard Taylor expansion and control the higher order terms. Speciﬁcally 
we write θi∗ j as θi∗ j = arccos(cid:104)ei + w∗
− (cid:104)ei + w∗

i   ej + wj(cid:105) and expand arccos at point 0  thus 
i   ej + wj(cid:105) + O((cid:104)ei + w∗
However  even when W ≈ W∗  the higher order term O((cid:104)ei + w∗
as a constant  which is too big for us. Our trick here is to consider the “joint Taylor expansion”:
θi∗ j − θi j = (cid:104)ei + wi − ei + w∗
i   ej + wj(cid:105) + O(|(cid:104)ei + w∗
i   ej + wj(cid:105)3 − (cid:104)ei + wi  ej + wj(cid:105)3|)
i   ej + wj(cid:105)3 − (cid:104)ei + wi  ej + wj(cid:105)3| also tends to zero  therefore
As W approaches W∗  |(cid:104)ei + w∗
our approximation has bounded error.
In the thought experiment  we already know that the constant part in the Taylor expansion of ∇L(W)
2 − O(g)-one point convex. We show that after taking inner product with W∗ − W  the ﬁrst
is π
order terms are lower bounded by (roughly) −1.3(cid:107)W∗ − W(cid:107)2
F and the higher order terms are lower
bounded by −0.085(cid:107)W∗ − W(cid:107)2
F . Adding them together  we can see that L(W) is one point convex
as long as g is small. See Figure 5.
Geometric Lemma. In order to get through the whole analysis  we need tight bounds on a few
common terms that appear everywhere. Instead of using naïve algebraic techniques  we come up with
a nice geometric proof to get nearly optimal bounds. Due to space limit  we defer it to Appendix E.
5 Experiments

i   ej + wj(cid:105)3)
i   ej + wj(cid:105)3) still can be as large

In this section  we present several simulation results to support our theory. Our code can be found in
the supplementary materials.

5.1

Importance of identity mapping

In this experiment  we compare the standard ResNet [21] and single skip model where identity
mapping skips only one layer. See Figure 6 for the single skip model. We also ran the vanilla network 
where the identity mappings are completely removed.

6

Table 1: Test error of three 56-layer networks on
Cifar-10

input

Test Err

ResNet
6.97%

Single skip Vanilla
12.04%

9.01%

C
o
n
v
o
l
u
t
i
o
n

B
a
t
c
h
N
o
r
m

⊕ R

e
L
U

output

Identity

Figure 6: Illustration of one block in single skip
model in Sec 5.1

(a) Test Error  Train Error

(b) (cid:107)W∗ − W(cid:107)F   (cid:107)W(cid:107)F

Figure 7: Verifying the global convergence

In this experiment  we choose Cifar-10 as the dataset  and all the networks have 56-layers. Other than
the identity mappings  all other settings are identical and default. We run the experiments for 5 times
and report the average test error. As we can see in Table 1  compared with vanilla network  by simply
using a single skip identity mapping  one can already improve the test error by 3.03%  and is 2.04%
close to the ResNet. So single skip identity mapping brings signiﬁcant improvement on test accuracy.

5.2 Global minimum convergence

In this experiment  we verify our main theorem that for two-layer teacher network and student network
with identity mappings  as long as (cid:107)W0(cid:107)2 (cid:107)W∗(cid:107)2 is small  SGD always converges to the global
minimum W∗  thus gives almost 0 training error and test error. We consider three student networks.
The ﬁrst one (ResLink) is deﬁned using (2)  the second one (Vanilla) is the same model without the
identity mapping. The last one (3-Block) is a three block network with each block containing a linear
layer (500 hidden nodes)  a batch normalization and a ReLU layer. The teacher network always
shares the same structure as the student network.
The input dimension is 100. We generated a ﬁxed W∗ for all the trials with (cid:107)W∗(cid:107)2 ≈ 0.6 (cid:107)W∗(cid:107)F ≈
5.7. We generated a training set of size 100  000  and test set of size 10  000  sampled from a Gaussian
distribution. We use batch size 200  step size 0.001. We run ResLink for 5 times with random
initialization ((cid:107)W(cid:107)2 ≈ 0.6 and (cid:107)W(cid:107)F ≈ 5)  and plot the curves by taking the average.
Figure 7(a) shows test error and training error of the three networks. Comparing Vanilla with 3-Block 
we ﬁnd that 3-Block is more expressive  so its training error is smaller compared with vanilla network;
but it suffers from overﬁtting and has bigger test error. This is the standard overﬁtting vs underﬁtting
tradeoff. Surprisingly  with only one hidden layer  ResLink has both zero test error and training
error. If we look at Figure 7(b)  we know the distance between W and W∗ converges to 0  meaning
ResLink indeed ﬁnds the global optimal in all 5 trials. By contrast  for vanilla network  which is
essentially the same network with different initialization  (cid:107)W − W∗(cid:107)2 does not converge to zero3.
This is exactly what our theory predicted.

5.3 Verify the dynamics

In this experiment  we verify our claims on the dynamics. Based on the analysis  we construct a
1500× 1500 matrix W s.t. (cid:107)W(cid:107)2 ≈ 0.15 (cid:107)W(cid:107)F ≈ 5   and set W∗ = 0. By plugging them into (2) 
one can see that even in this simple case that W∗ = 0  initially the gradient is pointing to the wrong
direction  i.e.  not one point convex. We then run SGD on W by using samples x from Gaussian
distribution  with batch size 300  step size 0.0001.

3To make comparison meaningful  we set W − I to be the actual weight for Vanilla as its identity mapping

is missing  which is why it has a much bigger initial norm.

7

0255075100125150175200epochs0246810lossTest (ResLink)Test (Vanilla)Test (3-Block)Train (ResLink)Train (Vanilla)Train (3-Block)0255075100125150175200epochs0.02.55.07.510.012.515.017.520.0l2 normW (ResLink)W-W* (ResLink)W (Vanilla)W-W* (Vanilla)(a) First 100 iterations

(b) The entire process

Figure 8: Verifying the dynamics

Figure 8(a) shows the ﬁrst 100 iterations. We can see that initially the inner product deﬁned in
Deﬁnition 2.4 is negative  then after about 15 iterations  it turns positive  which means W is in the
one point strongly convex region. At the same time  the potential g keeps decreasing to a small value 
while the distance to optimal (which also equals to (cid:107)W(cid:107)F in this experiment) is not affected. They
precisely match with our description of Phase I in Theorem 3.2.
After that  we enter Phase II and slowly approach to W∗  see Figure 8(b). Notice that the potential
g is always very small  the inner product is always positive  and the distance to optimal is slowly
decreasing. Again  they precisely match with our Theorem 3.3.

5.4 Zero initialization works

In this experiment  we used a simple 5-block neural network on MNIST  where every block contains
a 784 ∗ 784 feedforward layer  an identity mapping  and a ReLU layer. Cross entropy criterion is
used. We compare zero initialization with standard O(1/
d) random initialization. We found that
for zero initialization  we can get 1.28% test error  while for random initialization  we can get 1.27%
test error. Both results were obtained by taking average among 5 runs and use step size 0.1  batch
size 256. If the identity mapping is removed  zero initialization no longer works.

√

5.5 Spectral norm of W∗

We also applied the exact model f deﬁned in (1) to distinguish two classes in MNIST. For any input
image x  We say it’s in class A if f (x  W) < TA B  and in class B otherwise. Here TA B is the
optimal threshold for the function f (x  0) to distinguish A and B. If W = 0  we get 7% training
error for distinguish class 0 and class 1. However  it can be improved to 1% with (cid:107)W(cid:107)2 = 0.6.
We tried this experiment for all possible 45 pairs of classes in MNIST  and improve the average
training error from 34% (using W = 0) to 14% (using (cid:107)W(cid:107)2 = 0.6). Therefore our model with
(cid:107)W(cid:107)2 = Ω(1) has reasonable expressive power  and is substantially different from just using the
identity mapping alone.

6 Discussions

The assumption that the input is Gaussian can be relaxed in several ways. For example  when
the distribution is N (0  Σ) where (cid:107)Σ − I(cid:107)2 is bounded by a small constant  the same result holds
with slightly worse constants. Moreover  since the analysis relies Lemma 2.1  which is proved by
converting the original input space into polar space  it is easy to generalize the calculation to rotation
invariant distributions. Finally  for more general distributions  as long as we could explicitly compute
the expectation  which is in the form of O(W∗ − W) plus certain potential function  our analysis
framework may also be applied.
There are many exciting open problems. For example  Our paper is the ﬁrst one that gives solid
SGD analysis for neural network with nonlinear activations  without unrealistic assumptions like
independent activation assumption. It would be great if one could further extend it to multiple layers 
which would be a major breakthrough of understanding optimization for deep learning. Moreover 
our two phase framework could be applied to other non-convex problems as well.

8

020406080100−10−50510152025P-IP-II050000100000150000200000250000300000350000−20246810121416Distance to optimalInner productPotential gLossAcknowledgement

The authors want to thank Robert Kleinberg  Kilian Weinberger  Gao Huang  Adam Klivans and
Surbhi Goel for helpful discussions  and the anonymous reviewers for their comments.

References
[1] Alexandr Andoni  Rina Panigrahy  Gregory Valiant  and Li Zhang. Learning polynomials with

neural networks. In ICML  pages 1908–1916  2014.

[2] Sanjeev Arora  Aditya Bhaskara  Rong Ge  and Tengyu Ma. Provable bounds for learning
some deep representations. In Proceedings of the 31th International Conference on Machine
Learning  ICML 2014  Beijing  China  21-26 June 2014  pages 584–592  2014.

[3] Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.

IEEE Trans. Information Theory  39(3):930–945  1993.

[4] Leo Breiman. Hinging hyperplanes for regression  classiﬁcation  and function approximation.

IEEE Trans. Information Theory  39(3):999–1013  1993.

[5] Anna Choromanska  Mikael Henaff  Michaël Mathieu  Gérard Ben Arous  and Yann LeCun.

The loss surfaces of multilayer networks. In AISTATS  2015.

[6] Anna Choromanska  Yann LeCun  and Gérard Ben Arous. Open problem: The landscape of
the loss surfaces of multilayer networks. In Proceedings of The 28th Conference on Learning
Theory  COLT 2015  Paris  France  July 3-6  2015  pages 1756–1760  2015.

[7] George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS  5(4):455 

1992.

[8] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In NIPS  pages 2253–2261  2016.

[9] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In NIPS 2014  pages 2933–2941  2014.

[10] John C. Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research  12:2121–2159 
2011.

[11] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points - online
stochastic gradient for tensor decomposition. In COLT 2015  volume 40  pages 797–842  2015.

[12] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward

neural networks. In AISTATS  pages 249–256  2010.

[13] Xavier Glorot  Antoine Bordes  and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In

AISTATS  pages 315–323  2011.

[14] Surbhi Goel  Varun Kanade  Adam R. Klivans  and Justin Thaler. Reliably learning the relu in

polynomial time. CoRR  abs/1611.10258  2016.

[15] Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability for

neural networks. In NIPS 2017  2017.

[16] Surbhi Goel and Adam Klivans. Learning Depth-Three Neural Networks in Polynomial Time.

ArXiv e-prints  2017.

[17] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016.

http://www.deeplearningbook.org.

[18] Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization

problems. CoRR  abs/1412.6544  2014.

9

[19] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. CoRR  abs/1611.04231  2016.

[20] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In ICCV  pages 1026–1034 
2015.

[21] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In CVPR  pages 770–778  2016.

[22] Kurt Hornik  Maxwell B. Stinchcombe  and Halbert White. Multilayer feedforward networks

are universal approximators. Neural Networks  2(5):359–366  1989.

[23] Majid Janzamin  Hanie Sedghi  and Anima Anandkumar. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473 
2015.

[24] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS  pages 586–594  2016.

[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR 

abs/1412.6980  2014.

[26] J. M. Klusowski and A. R. Barron. Risk Bounds for High-dimensional Ridge Function Combi-

nations Including Neural Networks. ArXiv e-prints  July 2016.

[27] Yann LeCun  Leon Bottou  Genevieve B. Orr  and Klaus Robert Müller. Efﬁcient BackProp 

pages 9–50. Springer Berlin Heidelberg  Berlin  Heidelberg  1998.

[28] Roi Livni  Shai Shalev-Shwartz  and Ohad Shamir. On the computational efﬁciency of training

neural networks. In NIPS  pages 855–863  2014.

[29] Guido F. Montúfar  Razvan Pascanu  KyungHyun Cho  and Yoshua Bengio. On the number of

linear regions of deep neural networks. In NIPS  pages 2924–2932  2014.

[30] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann

machines. In ICML  pages 807–814  2010.

[31] Xingyuan Pan and Vivek Srikumar. Expressiveness of rectiﬁer networks. In ICML  pages

2427–2435  2016.

[32] Razvan Pascanu  Guido Montúfar  and Yoshua Bengio. On the number of inference regions of

deep feed forward networks with piece-wise linear activations. CoRR  abs/1312.6098  2013.

[33] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular

values. ArXiv e-prints  2010.

[34] David Saad and Sara A. Solla. Dynamics of on-line gradient descent learning for multilayer

neural networks. Advances in Neural Information Processing Systems  8:302–308  1996.

[35] Itay Safran and Ohad Shamir. On the quality of the initial basin in overspeciﬁed neural networks.

In ICML  pages 774–782  2016.

[36] Andrew M. Saxe  James L. McClelland  and Surya Ganguli. Exact solutions to the nonlinear

dynamics of learning in deep linear neural networks. CoRR  abs/1312.6120  2013.

[37] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with

sparse connectivity. ICLR  2015.

[38] Ohad Shamir.

Distribution-speciﬁc hardness of learning neural networks.

abs/1609.01037  2016.

CoRR 

[39] Jirí Síma. Training a single sigmoidal neuron is hard. Neural Computation  14(11):2709–2728 

2002.

[40] Ilya Sutskever  James Martens  George E. Dahl  and Geoffrey E. Hinton. On the importance of

initialization and momentum in deep learning. In ICML  pages 1139–1147  2013.

10

[41] Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural net-

works with relu nonlinearity. In Submitted to ICLR 2017  2016.

[42] Bo Xie  Yingyu Liang  and Le Song. Diversity leads to generalization in neural networks. In

AISTATS  2017.

[43] Yuchen Zhang  Jason D. Lee  Martin J. Wainwright  and Michael I. Jordan. Learning halfspaces

and neural networks with random initialization. CoRR  abs/1511.07948  2015.

[44] Kai Zhong  Zhao Song  Prateek Jain  Peter L. Bartlett  and Inderjit S. Dhillon. Recovery

guarantees for one-hidden-layer neural networks. In ICML 2017  2017.

11

,Yuanzhi Li
Yang Yuan