2012,Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data,We propose an efficient  generalized  nonparametric  statistical Kolmogorov-Smirnov test for detecting distributional change in high-dimensional data. To implement the test  we introduce a novel  hierarchical  minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams  and the test is especially efficient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods.,Learning High-Density Regions for a Generalized

Kolmogorov-Smirnov Test in High-Dimensional Data

Assaf Glazer

Michael Lindenbaoum

Department of Computer Science

Technion – Israel Institute of Technology

Department of Computer Science

Technion – Israel Institute of Technology

Haifa 32000  Israel

Haifa 32000  Israel

assafgr@cs.technion.ac.il

mic@cs.technion.ac.il

Shaul Markovitch

Department of Computer Science

Technion – Israel Institute of Technology

Haifa 32000  Israel

Address

shaulm@cs.technion.ac.il

Abstract

We propose an efﬁcient  generalized  nonparametric  statistical Kolmogorov-
Smirnov test for detecting distributional change in high-dimensional data. To
implement the test  we introduce a novel  hierarchical  minimum-volume sets es-
timator to represent the distributions to be tested. Our work is motivated by the
need to detect changes in data streams  and the test is especially efﬁcient in this
context. We provide the theoretical foundations of our test and show its superiority
over existing methods.

1

Introduction

1  . . .   x(cid:48)

The Kolmogorov-Smirnov (KS) test is efﬁcient  simple  and often considered the choice method for
m} be two sets of feature
comparing distributions. Let X = {x1  . . .   xn} and X (cid:48) = {x(cid:48)
vectors sampled i.i.d. with respect to F and F (cid:48) distributions. The goal of the KS test is to determine
whether F (cid:54)= F (cid:48). For one-dimensional distributions  the KS statistics are based on the maximal
difference between cumulative distribution functions (CDFs) of the two distributions. However 
nonparametric extensions of this test to high-dimensional data are hard to deﬁne since there are
2d−1 ways to represent a d-dimensional distribution by a CDF. Indeed  due to this limitation  several
extensions of the KS test to more than one dimension have been proposed [17  9] but their practical
applications are mostly limited to a few dimensions.
One prominent approach of generalizing the KS test to beyond one-dimensional data is that
of Polonik [18].
It is based on a generalized quantile transform to a set of high-density hierar-
chical regions. The transform is used to construct two sets of plots  expected and empirical  which
serve as the two input CDFs for the KS test. Polonik’s transform is based on a density estimation
over X . It maps the input quantile in [0  1] to a level-set of the estimated density such that the ex-
pected probability of feature vectors to lie within it is equal to its associated quantile. The expected
plots are the quantiles  and the empirical plots are fractions of examples in X (cid:48) that lie within each
mapped region.
Polonik’s approach can handle multivariate data  but is hard to apply in high-dimensional or small-
sample-sized settings where a reliable density estimation is hard. In this paper we introduce a gen-
eralized KS test  based on Polonik’s theory  to determine whether two samples are drawn from dif-

1

ferent distributions. However  instead of a density estimator  we use a novel hierarchical minimum-
volume sets estimator to estimate the set of high-density regions directly. Because the estimation of
such regions is intrinsically simpler than density estimation  our test is more accurate than density-
estimation approaches. In addition  whereas Polonik’s work was largely theoretical  we take a prac-
tical approach and empirically show the superiority of our test over existing nonparametric tests in
realistic  high-dimensional data.
To use Polonik’s generalization of the KS test  the high-density regions should be hierarchical.
Using classical minimum-volume set (MV-set) estimators  however  does not  in itself  guarantee
this property. We present here a novel method for approximate MV-sets estimation that guarantees
the hierarchy  thus allowing the KS test to be generalized to high dimensions. Our method uses
classical MV-set estimators as a basic component. We test our method with two types of estimators:
one-class SVMs (OCSVMs) and one-class neighbor machines (OCNMs).
While the statistical test introduced in this paper traces distributional changes in high dimensional
data in general  it is effective in particular for change detection in data streams. Many real-world
applications (e.g. process control) work in dynamic environments where streams of multivariate
data are collected over time  during which unanticipated distributional changes in data streams might
prevent the proper operation of these applications. Change-detection methods are thus required to
trace such changes (e.g. [6]). We extensively evaluate our test on a collection of change-detection
tasks. We also show that our proposed test can be used for the classical setting of the two-sample
problem using symmetric and asymmetric variations of our test.

2 Learning Hierarchical High-Density Regions

with respect to a probability distribution F deﬁned on a measurable space(cid:0)Rd S(cid:1). Let λ be a real-

Our approach for generalizing the KS test is based on estimating a hierarchical set of MV-sets in
input space. In this section we introduce a method for ﬁnding such a set in high-dimensional data.
Following the notion of multivariate quantiles [8]  let X = {x1  . . .   xn} be a set of examples i.i.d.
valued function deﬁned on C ⊂ S. Then  the minimum-volume set (MV-set) with respect to F   λ 
and C at level α is

C (α) = argmin

C(cid:48)∈C

{λ(C(cid:48)) : F (C(cid:48)) ≥ α} .

(1)

(cid:80)n

If more than one set attains the minimum  one will be picked. Equivalently  if F (C) is replaced with
1 1C (xi)  then Cn(α) is one of the empirical MV-sets that attains the minimum. In
Fn (C) = 1
n
the following we think of λ as a Lebesgue measure on Rd.
Polonik introduced a new approach that uses a hierarchical set of MV-sets to generalize the KS test
beyond one dimension. Assume F has a density function f with respect to λ  and let Lf (c) =
{x : f (x) ≥ c} be the level set of f at level c. Sufﬁcient regularity conditions on f are assumed.
Polonik observed that if Lf (c) ∈ C  then Lf (c) is an MV-set of F at level α = F (Lf (c)). He thus
suggested that level-sets can be used as approximations of the MV-sets of a distribution. Hence  a
density estimator was used to deﬁne a family of MV-sets {C(α)  α ∈ [0  1]} such that a hierarchy
constraint C(α) ⊂ C(β) is satisﬁed for 0 ≤ α < β ≤ 1.
We also use hierarchical MV-sets to represent distributions in our research. However  since a density
estimation is hard to apply in high-dimensional data  a more practical solution is proposed. Instead
of basing our method on the products of a density estimation method  we introduce a novel non-
parametric method  which uses MV-set estimators (OCSVM and OCNM) as a basic component  to
estimate hierarchical MV-sets without the need for a density estimation step.

2.1 Learning Minimum-Volume Sets with One-Class SVM Estimators

OCSVM is a nonparametric method for estimating a high-density region in a high-dimensional dis-
tribution [19]. Consider a function Φ : Rd → F mapping the feature vectors in X to a hyper-
sphere in an inﬁnite Hilbert space F. Let H be a hypothesis space of half-space decision functions
fC(x) = sgn ((w · Φ(x)) − ρ) such that fC(x) = +1 if x ∈ C  and −1 otherwise. To separate X

2

from the origin  the learner is asked to solve this quadratic program:

min

w∈F  ξ∈Rn ρ∈R

||w||2 − ρ +

1
2

1
νn

ξi  s.t. (w · Φ (xi)) ≥ ρ − ξi  ξi ≥ 0 

(2)

(cid:88)

i

where ξ is the vector of the slack variables  and 0 < ν < 1 is a regularization parameter related to the
proportion of outliers in the training data. All training examples xi for which (w · Φ(x))− ρ ≤ 0 are
called support vectors (SVs). Outliers are referred to as examples that strictly satisfy (w · Φ(x)) −
ρ < 0. Since the algorithm only depends on the dot product in F  Φ never needs to be explicitly
computed  and a kernel function k (· ·) is used instead such that k (xi  xj) = (Φ(xi) · Φ(xj))F .
The following theorem draws the connection between the ν regularization parameter and the region
C provided by the solution of Equation 2:
Theorem 1 (Sch¨olkopf et al. [19]). Assume the solution of Equation 2 satisﬁes ρ (cid:54)= 0. The following
statements hold: (1) ν is an upper bound on the fraction of outliers. (2) ν is a lower bound on the
fraction of SVs. (3) Suppose X were generated i.i.d. from a distribution F which does not contain
discrete components. Suppose  moreover  that the kernel k is analytic and non-constant. Then  with
probability 1  asymptotically  ν is equal to both the fraction of SVs and to the fraction of outliers.

This theorem shows that we can use OCSVMs to estimate high-density regions in the input space
while bounding the number of examples in X lying outside these regions. Thus  by setting ν = 1−α 
we can use OCSVMs to estimate regions approximating C(α). We use this estimation method with
its original quadratic optimization scheme to learn a family of MV-sets. However  a straightforward
approach of training a set of OCSVMs  each with different ν ∈ (0  1)  would not necessarily satisfy
the hierarchy requirement. In the following algorithm  we propose a modiﬁed construction of these
regions such that both the hierarchical constraint and the density assumption (Theorem 1) will hold
for each region.
Let 0 < α1 < α2  . . .   < αq < 1 be a sequence of quantiles. Given X and a kernel function k (· ·) 
our hierarchical MV-sets estimator iteratively trains a set of q OCSVMs  one for each quantile 
and returns a set of decision functions  ˆfC(α1)  . . .   ˆfC(αq) that satisfy both hierarchy and density
requirements. Training starts from the largest quantile (αq). Let Di be the training set of the OCSVM
trained for the αi quantile. Let fC(αi)  SVbi be the decision function and the calculated outliers
j=i SVbj . At each iteration 
Di contains examples in X that were not classiﬁed as outliers in previous iterations (not in Oi+1).
In addition  ν is set to the required fraction of outliers over Di that will keep the total fraction of
outliers over X equal to 1− αi. After each iteration  ˆfC(αi) corresponds to the intersection between
the region associated with the previous decision function and the half-space associated with the
current learned OCSVM. Thus ˆfC(αi) corresponds to the region speciﬁed by an intersection of half-
spaces. The outliers in Oi are points that lie strictly outside the constructed region. The pseudo-code
of our estimator is given in Algorithm 1.

(bounded SVs) of the OCSVM trained for the i-th quantile. Let Oi =(cid:83)q

Algorithm 1 Hierarchical MV-sets Estimator (HMVE)
1: Input: X   0 < α1 < α2  . . .   < αq < 1  k (·  ·)
7:
8:
2: Output: ˆfC(α1)  . . .   ˆfC(αq )
3: Initialize: Dq ← X   Oq+1 ← ∅
9:
4: for i = q to 1 do
10:
5:
11:
6:
12: return ˆfC(α1)  . . .   ˆfC(αq )

ν ← (1−αi)|X|−|Oi+1|
fC(αi)  SVbi ← OCSV M (Di  ν  k)

if i = q then

|Di|

ˆfC(αi)(x) ← fC(αi(x))
ˆfC(αi)(x) ←

(cid:26) fC(αi(x))

−1

else

Oi ← Oi+1 ∪ SVbi   Di−1 ← Di \ SVbi

:
:

ˆfC(αi+1)(x)
otherwise

The following theorem shows that the regions speciﬁed by the decision functions ˆfC(α1)  . . .   ˆfC(αq)
are: (a) approximations for the MV-sets in the same sense suggested by Sch¨olkopf et al.  and (b)
hierarchically nested. In the following  ˆC(αi) is denoted as the estimates of C(αi) with respect to
ˆfC(αi).
Theorem 2. Let ˆfC(α1)  . . .   ˆf eC(αq) be the decision functions returned by Algorithm 1 with param-
eters {α1  . . .   αq} X   k (· ·). Assume X is separable. Let ˆC(αi) be the region in the input space

3

Figure 1: Left: Estimated MV-sets ˆC(αi) in
the original input space  q = 3. Right: the
projected ˆC(αi) in F.

Figure 2: Averaged symmetric differences
against the number of training points for the
OCSVM / OCNM versions of our estimator 
and the KDE2d density estimator

associated with ˆfC(αi)  and SVubi be the set of (unbounded) SVs lying on the separating hyperplane
in the region associated with fC(αi(x)). Then  the following statements hold:(1) ˆC(αi) ⊆ ˆC(αj) for
αi < αj. (2) |Oi|
. (3) Suppose X were i.i.d. drawn from a distribution
F which does not contain discrete components  and k is analytic and non-constant. Then  1 − αi is
asymptotically equal to |Oi|
|X| .

|X| ≤ 1 − αi ≤ |SVubi|+|Oi|

|X|

|Di|

Proof. Statement (1) holds by deﬁnition of ˆfC(αi). Statements (2)-(3) are proved by induction
on the number of iterations. In the ﬁrst iteration ˆfC(αq) equals fC(αq). Thus  since Oq = SVbq
and ν = 1 − αq  statements (2)-(3) follow directly from Theorem 1 1. Then  by the induction
hypothesis  statements (2)-(3) hold for the ﬁrst n − 1 iterations over the αq  . . .   αq−n+1 quantiles.
We now prove that statements (2)-(3) hold for ˆfC(αq−n) in the next iteration. Since ˆfC(αq−n+1)(x) =
−1 implies ˆfC(αq−n)(x) = −1  Oq−n+1 are outliers with respect to ˆfC(αq−n). In addition  ν =
(1−αq−n)|X|−|Oq−n+1|
. Hence  following Theorem 1  the total proportion of outliers with respect to
X is |Oq−n| = |SVbq−n| + |Oq−n+1| ≤ ν|Di| + |Oq−n+1| = (1 − αq−n)|X|  and |SVubq−n| +
|Oq−n+1| ≥ (1 − αq−n)|X|. Hence  |Oq−n|
. In the same manner 
under the conditions of statement (3)  |Oq−n| is asymptotically equal to (1 − αq−n)|X|  and hence 
asymptotically  1 − αq−n =
Figure 1 illustrates the estimated MV-sets ˆC(αi) in both the original and the projected spaces. On
the left  all ˆC(αi) regions in the original input space are colored with decreased gray levels. Note
that ˆC(αi) is a subset of ˆC(αj) if i < j. On the right  the projected regions of all ˆC(αi)s in F are
marked with the same colors. Examples xi in the input space and their mapped vectors φ(xi) in F
are contained in the same relative regions in both spaces. It can be seen that the projections of ˆC(αi)
in F are the intersecting half-spaces learned by Algorithm 1.

|X| ≤ 1 − αq−n ≤ |SVubq−n|+|Oq−n|

|Oq−n|

|X|

|X|

.

2.2 Learning Minimum-Volume Sets with One-Class Neighbor Machine Estimators

OCNM [15] is as an alternative method to the OCSVM estimator for ﬁnding regions close to C(α).
Unlike OCSVM  the OCNM solution is proven to be asymptotically close to the MV-set speciﬁed 2.
Degenerated structures in data that may damage the generalization of SVMs could be another reason
for choosing OCNM [24]. In practice  for ﬁnite sample size  it is not clear which estimator is more
accurate.

1Note that the separability of the data implies that the solution of Equation 2 satisﬁes ρ (cid:54)= 0.
2Sch¨olkopf et al. [19] proved that the set provided by OCSVM converges asymptotically to the correct
probability and not to the correct MV-set. Although this property should be sufﬁcient for the correctness of our
test  Polonik observed that MV-sets are preferred.

4

C1C2C2C3C3C4S1x2x3xF1FdO1x3x2xjh1jhjtopp1jtoppjsvp1jsvpOjjw11jjwHypersphere with radius 11100 ... xxTime101150 ... xx49 ... iixx49 ... mnmnxx. . .. . .Training setTesting sets. . .. . .1ˆC2ˆC3ˆC2ˆC1015202530354045500.020.040.060.080.10.120.140.160.180.2# training pointssymmetric difference2D level−sets estimations: qcd ocsvm/ocnm Vs. kde  HMVE (OCSVM)HMVE (OCNM)KDE2DOCNM uses either a sparsity or a concentration neighborhood measure. M (Rd X ) → R is a
sparsity measure if f (x) > f (y) implies lim|X|→∞P (M (x X ) < M (y X )) = 1. An example
for a valid sparsity measure is the distance of x to its kth-nearest neighbor in X . When a sparsity
measure is used  the OCNM estimator solves the following linear problem

ξi  s.t. M (xi X ) ≥ ρ − ξi  ξi ≥ 0 

(3)

ξ∈Rn ρ∈R νnρ − n(cid:88)

max

i

such that the resulting decision function fC(x) = sgn (ρ − M (x X )) satisﬁes bounds and conver-
gence properties similar to those mentioned in Theorem 1 (ν-property).
OCNM can replace OCSVM in our hierarchical MV-sets estimator. In contrast to OCSVMs  when
OCNMs are iteratively trained on X using a growing sequence of ν values  outliers need not be
moved from previous iterations to ensure that the ν-property will hold for each decision function.
Hence  a simpler version of Algorithm 1 can be used  where X is used for training all OCNMs and
ν = 1 − αi for each step 3. Since Theorem 2 relies on the ν-property of the estimator  it can be
shown that similar statements to those of Theorem 2 also hold when OCNM is used.
As previously discussed  since the estimation of MV-sets is simpler than density estimation 
our test can achieve higher accuracy than approaches based on density estimation. To illus-
trate this hypothesis empirically  we conducted the following preliminary experiment. We sam-
pled 10 to 50 i.i.d. points with respect to a two-dimensional  mixture of Gaussians  distribution
2N (µ = (−0.5 −0.5)  Σ = 0.5I). We use the OCNM
2N (µ = (0.5  0.5)  Σ = 0.1I) + 1
p = 1
and OCSVM versions of our estimator to approximate hierarchical MV-sets for qα = 9 quantiles:
α = 0.1  0  2  . . .   0.9 (detailed setup parameters are discussed in Section 4). MV-sets estimated
(cid:82)
(cid:80)
with a KDE2d kernel-density estimation [2] were used for comparison. For each sample size  we
measured the error of each method according to the mean weighted symmetric difference between
x∈C(α)∆ ˆC(α) p(x)dx. Results  averaged over 50 sim-
the true MV-sets and their estimates  1
qα
ulations  are shows in Figure 2. The advantages of our approach can easily be seen: both versions
of our estimator preform notably better  especially for small sample sizes.

α

3 Generalized Kolmogorov-Smirnov Test

We now introduce a nonparametric  generalized Kolmogorov-Smirnov (GKS) statistical test for de-
termining whether F (cid:54)= F (cid:48) in high-dimensional data. Assume F  F (cid:48) are one-dimensional continuous
distributions and Fn  F (cid:48)
m are empirical distributions estimated from n and m examples i.i.d. drawn
from F  F (cid:48). Then  the two-sample Kolmogorov-Smirnov (KS) statistic is

(cid:113) nm

KSn m = sup
x∈R

|Fn(x) − F (cid:48)

m(x)|

n+m KSn m is asymptotically distributed  under the null hypothesis  as the distribution of
and
supx∈R |B(F (x))| for a standard Brownian bridge B when F = F (cid:48). Under the null hypothesis 
assume F = F (cid:48) and let F −1 be a quantile transform of F   i.e.  the inverse of F . Then we can
replace the supremum over x ∈ R with the supremum over α ∈ [0  1] as follows:

(cid:12)(cid:12)Fn(F −1(α)) − F (cid:48)

m(F −1(α))(cid:12)(cid:12).

KSn m = sup
α∈[0 1]

Note that in the one-dimensional setting  F −1(α) is the point x s.t. F (X ≤ x) ≤ α where X is a
random variable drawn from F . Equivalently  F −1(α) can be identiﬁed with the interval [−∞  x].
In a high-dimensional space these intervals can be replaced by hierarchical MV-sets C(α) [18] 
and hence  Equation 5 can be calculated regardless of the input space dimensionality. We suggest
replacing KSn m with

Tn m = sup
α∈[0 1]

|Fn(C(α)) − F (cid:48)

m(C(α))|.

For estimating C(α) we use our nonparametric method from Section 2. ˆC(α) is learned with X
and marked as ˆCX (α). In practice  when |X| is ﬁnite  the expected proportion of examples that lie
3Note that intersection is still needed (Algorithm 1  line 10) to ensure the hierarchical property on ˆC(αi).

5

(4)

(5)

(6)

within ˆCX (αi) is not guaranteed to be exactly αi. Therefore  after learning the decision functions 
we estimate Fn( ˆCX (αi)) by a k-folds cross-validation procedure. Our ﬁnal test statistic is

(7)

(cid:12)(cid:12)(cid:12) ˆFn( ˆCX (αi)) − Fm( ˆCX (αi))

(cid:12)(cid:12)(cid:12) 

ˆTn m = sup
1≤i≤q

where ˆFn( ˆCX (αi)) is the estimate of Fn( ˆCX (αi)). The two-sample KS statistical test is used over
ˆTn m to calculate the resulting p-value.
The test deﬁned above works only in one direction by predicting whether distributions of the samples
share the same “concentrations” as regions estimated according to X   and not according to X (cid:48). We
may symmetrize it by running the non-symmetric test twice  once in each direction  and return twice
their minimum p-value (Bonferroni correction). Note that by doing so in the context of a change
detection task  we pay in runtime required for learning MV-sets for each X (cid:48).

4 Empirical Evaluation

We ﬁrst evaluated our test on concept-drift detection problems in data-stream classiﬁcation tasks.
Concept drifts are associated with distributional changes in data streams that occur due to hidden
context [22] — changes of which the classiﬁer is unaware. We used the 27 UCI datasets used
in [6]  and 6 additional high-dimensionality UCI datasets: arrhythmia  madelon  semeion  internet
advertisement  hill-valley  and musk. The average number of features for all datasets is 123 4.
Following the experimental setup used by [11  6]  we generated  for each dataset  a sequence
(cid:104)x1  . . .   xn+m(cid:105)  where the ﬁrst n examples are associated with the most frequent label  and the
following m examples with the second most frequent. Within each label the examples were shufﬂed
randomly. The ﬁrst 100 examples (cid:104)x1  . . .   x100(cid:105)  associated  in all datasets  with the most common
label  were used as the baseline dataset X . A sliding window of 50 consecutive examples over the
following sequence of examples was iteratively used to deﬁne the most recent data X (cid:48) at hand. Sta-
tistical tests were evaluated with X and all possible X (cid:48) windows. In total  for each dataset  the set
i = {xi  . . .   xi+49}   101 ≤ i ≤ n + m − 49} of pairs were used for evaluation. The
{(cid:104)X  X (cid:48)
following ﬁgure illustrates this setup:

i(cid:105)|X (cid:48)

i(cid:105)   i ≤ n − 49  where all examples in X (cid:48)

The pairs (cid:104)X  X (cid:48)
i have the same labels as in X   are
considered “unchanged.” The remaining pairs are considered “changed.” Performance is evaluated
using precision-recall values with respect to the change detection task.
We compare our one-directional (GKS1d) and two-directional (GKS2d) tests to the following 5 ref-
erence tests: kdq-tree test (KDQ) [4]  Metavariable Wald-Wolfowitz test (WW) [10]  Kernel change
detection (KCD) [5]  Maximum mean discrepancy test (MMD) [12]  and PAC-Bayesian margin test
(PBM) [6]. See section 5 for details. All tests  except of MMD  were implemented and parameters
were set with accordance to their suggested setting in their associate papers. The implementation of
MMD test provided by the authors 5 was used with default parameters (RBF kernels with automatic
kernel width detection) and Rademacher bounds. Similar results were also measured for asymp-
totic bounds. Note that we cannot compare our test to Polonik’s test since density estimations and
level-sets extractions are not practically feasible on high-dimensional data.
#f eatures) was used for the OCSVMs. A
The LibSVM package [3] with a Gaussian kernel (γ =
distance from a point to its kth-nearest neighbor was used as a sparsity measure for the OCNMs. k
is set to 10% of the sample size 6. α = 0.1  0.2  . . .   0.9 were used for all experiments.

2

4Nominal features were transformed into numeric ones using binary encoding; missing values were replaced

by their features’ average values.

5The code can be downloaded at http://people.kyb.tuebingen.mpg.de/arthur/mmd.htm.
6Preliminary experiments show similar results obtained with k equal to 10  20  . . .   50% of |X|.

6

C1C2C2C3C3C4S1x2x3xF1FdC3C2C2C1O1x3x2xjh1jhjtopp1jtoppjsvp1jsvpOjjw11jjwHypersphere with radius 11100 ... xxTime101150 ... xx49 ... iixx49 ... mnmnxx. . .. . .Training setTesting sets. . .. . .Figure 3:
Precision-recall curves aver-
aged over all 33 experiments for GKS1d
(OCSVMs)  GKS2d (OCSVMs)  and the 5
reference tests.

Figure 4:
Precision-recall curves aver-
aged over all 33 experiments for GKS1d
(OCSVMs)  GKS2d (OCSVMs)  GKS1d
(OSNMs)  and GKS2d (OSNMs).

4.1 Results

For better visualization  results are shown in two separate ﬁgures: Figure 3 shows the precision-
recall plots averaged over the 33 experiments for the OCSVM version of our tests  and the 5 reference
tests. Figure 4 shows the precision-recall plots averaged over the 33 experiments for the OCSVM
and OCNM versions of our tests. In both versions  GKS1d and GKS2d provide the best precision-
recall compromise. For example  for the OCSVM version  at a recall of 0.86  GKS1d accurately
detects distributional changes with 0.90 precision and GKS2d with 0.88 precision  while the second
best competitor does so with 0.84 precision. In terms of their break even point (BEP) measures –
the points at which precision equals recall – GKS1d outperforms the other 5 reference tests with
a BEP of 0.89 while its second best competitor does so with BEP of 0.84. Mean precisions for
each dataset were compared using the Wilcoxon statistical test with α = 0.05. Here  too  GKS1d
performs signiﬁcantly better than all others for both OCSVM and OCNM versions  except for the
MMD with a p-value of 0.08 for GKS1d(OCSVM) and 0.12 for GKS1d(OCNM).
Although the plots for our GKS1d (OCSVM) test (Figure 4) look better than GKS2d  no signiﬁcant
difference was found. This result is consistent with previous studies which claim that variants of
solutions whose goal is to make the tests more symmetric have empirically shown no conclusive
superiority [4]. We also found that the GKS1d (OCSVM) version of our test has the least runtime
and scales well with dimensionality  while the GKS1d (OSNM) version suffers from increased time
complexity  especially in high dimensions  due to its expensive neighborhood measure. However 
note that this observation is true only when off-line computational processing on X is not considered.
As opposed to the KCD  and  PBM  tests  our GKS1d test need not be retrained on each X (cid:48). Hence 
in the context where X is treated as a baseline dataset  GKS1d (OCSVM) is relatively cheap  and
estimated in O (nm) time (the total number of SVs used to calculate f(cid:48)
C(αq) is O (n)).
In comparison to other tests  it is still the least computationally demanding 7.

C(α1)  . . .   f(cid:48)

4.2 Topic Change Detection among Documents

We evaluated our test on an additional setup of high-dimensionality problems pertaining to the de-
tection of topic changes in streams of documents. We used the 20-Newsgroup document corpus 8.
1000 words were randomly picked to generate 1000 bag-of-words features. 12 categories were used
for the experiments 9. Topic changes were simulated between all pairs of categories (66 pairs in to-
tal)  using the same methodology as in the previous UCI experiments. Due to the excessive runtime

7MMD and WW complexities are estimated in O(cid:0)(n + m)2(cid:1) time where n  m are the sample sizes. KDQ

uses bootstrapping for p-value estimations  and hence  is more expensive.

8The 20-Newsgroup corpus is at http://people.csail.mit.edu/jrennie/20Newsgroups/.
9The selection of these categories is based on the train/test split deﬁned in http://www.cad.zju.

edu.cn/home/dengcai/Data/TextData.html.

7

00.10.20.30.40.50.60.70.80.910.40.50.60.70.80.91recallprecision  GKS1d (OCSVM)GKS2d (OCSVM)WWMMDPBMKCDKDQBEP00.10.20.30.40.50.60.70.80.910.40.50.60.70.80.91recallprecision  GKS1d (OCSVM)GKS2d (OCSVM)GKS1d (OCNM)GKS2d (OCNM)of some of the tests  especially with high-dimensional data  we evaluated only 4 of the 7 methods:
GKS1d (OCSVM)  WW  MMD  and KDQ  whose expected runtime may be more reasonable.
Once again  our GKS1d test dominates the others with the best precision-recall compromise. With
regard to BEP values  GKS1d outperforms the other reference tests with a BEP of 0.67 (0.70 pre-
cision on average)  while its second best competitor (MMD) does so with a BEP of 0.62 (0.64
precision on average). According to the Wilcoxon statistical test with α = 0.05  GKS1d performs
signiﬁcantly better than the others in terms of their average precision measures.

5 Related Work

Our proposed test belongs to a family of nonparametric tests for detecting change in multivariate
data that compare distributions without the intermediate density estimation step. Our reference tests
were thus taken from this family of studies. The kdq-tree test (KDQ) [4] uses a spatial scheme (called
kdq-tree) to partition the data into small cells. Then  the Kullback-Leibler (KL) distance is used to
measure the difference between data counts for the two samples in each cell. A permutation (boot-
strapping) test [7] is used to calculate the signiﬁcant difference (p-value). The metavariable Wald-
Wolfowitz test (WW) [10] measures the differences between two samples according to the minimum
spanning tree in the graph of distances between all pairs in both samples. Then  the Wald-Wolfowitz
test statistics are computed over the number of components left in the graph after removing edges
between examples of different samples. The kernel change detection (KCD) [5] measures the dis-
tance between two samples according to a “Fisher-like” distance between samples. This distance is
based on hypercircle characteristics of the resulting two OCSVMs  which were trained separately on
each sample. The maximum mean discrepancy test (MMD) [12] meausres discrepancy according to
a complete matrix of kernel-based dissimilarity measures between all examples  and test statistics
are then computed. (5) The PAC-Bayesian margin test (PBM) [6] measures the distance between
two samples according to the average margins of a linear SVM classiﬁer between the samples  and
test statistics are computed.
As discussed in detail before  our test follows the general approach of Polonik but differs in three
important ways: (1) While Polonik uses a density estimator for specifying the MV-sets  we introduce
a simpler method that ﬁnds the MV-sets directly from the data. Our method is thus more practical
and accurate in high-dimensional or small-sample-sized settings. (2) Once the MV-sets are deﬁned 
Polonik uses their hypothetical quantiles as the expected plots  and hence  runs the KS test in its one-
sample version (goodness-of-ﬁt test). We take a more practically accurate approach for ﬁnite sample
size when approximations of MV-sets are not precise. Instead of using the hypothetical measures 
we estimate the expected plots of X empirically and use the two-sample KS test instead. (3) Unlike
Polonik’s work  ours was evaluated empirically and its superiority demonstrated over a wide range
of nonparametric tests. Moreover  since Polonik’s test relies on a density estimation and the ability
to extract its level-sets  it is not practically feasible in high-dimensional settings.
Other methods for estimating MV-sets exist in the literature [21  1  16  13  20  23  14]. Unfortu-
nately  for problems beyond two dimensions and non-convex sets  there is often a gap between their
theoretical and practical estimates [20]. We chose here OCSVM and OSNM because they perform
well on small  high-dimensional samples.

6 Discussion and Summary

This paper makes two contributions. First  it proposes a new method that uses OCSVMs or OCNMs
to represent high-dimensional distributions as a hierarchy of high-density regions. This method
is used for statistical tests  but can also be used as a general  black-box  method for efﬁcient and
practical representations of high-dimensional distributions. Second  it presents a nonparametric 
generalized  KS test that uses our representation method to detect distributional changes in high-
dimensional data. Our test was found superior to competing tests in the sense of average precision
and BEP measures  especially in the context of change-detection tasks.
An interesting and still open question is how we should set the input α quantiles for our method.
The problem of determining the number of quantiles – and the gaps between consecutive ones – is
related to the problem of histogram design.

8

References
[1] S. Ben-David and M. Lindenbaum. Learning distributions by their density levels: A paradigm
for learning without a teacher. Journal of Computer and System Sciences  55(1):171–182 
1997.

[2] ZI Botev  JF Grotowski  and DP Kroese. Kernel density estimation via diffusion. The Annals

of Statistics  38(5):2916–2957  2010.

[3] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines  2001.
[4] T. Dasu  S. Krishnan  S. Venkatasubramanian  and K. Yi. An information-theoretic approach

to detecting changes in multi-dimensional data streams. In INTERFACE  2006.

[5] F. Desobry  M. Davy  and C. Doncarli. An online kernel change detection algorithm. Signal

Processing  Transactions on Information Theory  53(8):2961–2974  2005.

[6] Anton Dries and Ulrich R¨uckert. Adaptive concept drift detection. Statistical Analysis and

Data Mining  2(5-6):311–327  2009.

[7] B. Efron and R.J. Tibshirani. An Introduction to the Bootstrap. Chapman and Hall/CRC  1994.
[8] J.H.J. Einmahl and D.M. Mason. Generalized quantile processes. The Annals of Statistics 

pages 1062–1078  1992.

[9] G. Fasano and A. Franceschini. A multidimensional version of the kolmogorov-smirnov test.

Monthly Notices of the Royal Astronomical Society  225:155–170  1987.

[10] J.H. Friedman and L.C. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and

Smirnov two-sample tests. The Annals of Statistics  7(4):697–717  1979.

[11] J. Gama  P. Medas  G. Castillo  and P. Rodrigues. Learning with drift detection. In SBIA  pages

66–112. Springer  2004.

[12] A. Gretton  K.M. Borgwardt  M. Rasch  B. Scholkopf  and A.J. Smola. A kernel method for

the two-sample-problem. Machine Learning  1:1–10  2008.

[13] X. Huo and J.C. Lu. A network ﬂow approach in ﬁnding maximum likelihood estimate of high

concentration regions. Computational Statistics & Data Analysis  46(1):33–56  2004.

[14] D.M. Mason and W. Polonik. Asymptotic normality of plug-in level set estimates. The Annals

of Applied Probability  19(3):1108–1142  2009.

[15] A. Munoz and J.M. Moguerza. Estimation of high-density regions using one-class neighbor

machines. In PAMI  pages 476–480  2006.

[16] J. Nunez Garcia  Z. Kutalik  K.H. Cho  and O. Wolkenhauer. Level sets and minimum volume
sets of probability density functions. International Journal of Approximate Reasoning  34(1):
25–47  2003.

[17] JA Peacock. Two-dimensional goodness-of-ﬁt testing in astronomy. Monthly Notices of the

Royal Astronomical Society  202:615–627  1983.

[18] W. Polonik.

Concentration and goodness-of-ﬁt

in higher dimensions:(asymptotically)

distribution-free methods. The Annals of Statistics  27(4):1210–1229  1999.

[19] Bernhard Sch¨olkopf  John C. Platt  John C. Shawe-Taylor  Alex J. Smola  and Robert C.
Williamson. Estimating the support of a high-dimensional distribution. Neural Computation 
13(7):1443–1471  2001.

[20] C.D. Scott and R.D. Nowak. Learning minimum volume sets. The Journal of Machine Learn-

ing Research  7:665–704  2006.

[21] G. Walther. Granulometric smoothing. The Annals of Statistics  pages 2273–2299  1997.
[22] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts.

Machine Learning  23(1):69–101  1996.

[23] R.M. Willett and R.D. Nowak. Minimax optimal level-set estimation. Image Processing  IEEE

Transactions on  16(12):2965–2979  2007.

[24] John Wright  Yi Ma  Yangyu Tao  Zhouchen Lin  and Heung-Yeung Shum. Classiﬁcation via

minimum incremental coding length. SIAM J. Imaging Sciences  2(2):367–395  2009.

9

,Alexander Novikov
Dmitrii Podoprikhin
Anton Osokin
Dmitry Vetrov