2019,Privacy Amplification by Mixing and Diffusion Mechanisms,A fundamental result in differential privacy states that the privacy guarantees of a mechanism are preserved by any post-processing of its output. In this paper we investigate under what conditions stochastic post-processing can amplify the privacy of a mechanism. By interpreting post-processing as the application of a Markov operator  we first give a series of amplification results in terms of uniform mixing properties of the Markov process defined by said operator. Next we provide amplification bounds in terms of coupling arguments which can be applied in cases where uniform mixing is not available. Finally  we introduce a new family of mechanisms based on diffusion processes which are closed under post-processing  and analyze their privacy via a novel heat flow argument. On the applied side  we generalize the analysis of "privacy amplification by iteration" in Noisy SGD and show it admits an exponential improvement in the strongly convex case  and study a mechanism based on the Ornstein–Uhlenbeck diffusion process which contains the Gaussian mechanism with optimal post-processing on bounded inputs as a special case.,PrivacyAmpliﬁcationbyMixingandDiffusionMechanismsBorjaBalleGillesBartheMPIforSecurityandPrivacyIMDEASoftwareInstituteMarcoGaboardiBostonUniversityJosephGeumlekUniversityofCalifornia SanDiegoAbstractAfundamentalresultindifferentialprivacystatesthattheprivacyguaranteesofamechanismarepreservedbyanypost-processingofitsoutput.Inthispaperweinvestigateunderwhatconditionsstochasticpost-processingcanamplifytheprivacyofamechanism.Byinterpretingpost-processingastheapplicationofaMarkovoperator weﬁrstgiveaseriesofampliﬁcationresultsintermsofuniformmixingpropertiesoftheMarkovprocessdeﬁnedbysaidoperator.Nextweprovideampliﬁcationboundsintermsofcouplingargumentswhichcanbeappliedincaseswhereuniformmixingisnotavailable.Finally weintroduceanewfamilyofmechanismsbasedondiffusionprocesseswhichareclosedunderpost-processing andanalyzetheirprivacyviaanovelheatﬂowargument.Ontheappliedside wegeneralizetheanalysisof“privacyampliﬁcationbyiteration”inNoisySGDandshowitadmitsanexponentialimprovementinthestronglyconvexcase andstudyamechanismbasedontheOrnstein–UhlenbeckdiffusionprocesswhichcontainstheGaussianmechanismwithoptimalpost-processingonboundedinputsasaspecialcase.1IntroductionDifferentialprivacy(DP)[15]hasariseninthelastdecadeintoastrongde-factostandardforprivacy-preservingcomputationinthecontextofstatisticalanalysis.ThesuccessofDPisbased atleastinpart ontheavailabilityofrobustbuildingblocks(e.g. theLaplace exponentialandGaussianmechanisms)togetherwithrelativelysimplerulesforanalyzingcomplexmechanismsbuiltoutoftheseblocks(e.g. compositionandrobustnesstopost-processing).Theinherenttensionbetweenprivacyandutilityinpracticalapplicationshassparkedarenewedinterestintothedevelopmentoffurtherrulesleadingtotighterprivacybounds.Atrendinthisdirectionistoﬁndwaystomeasuretheprivacyintroducedbysourcesofrandomnessthatarenotaccountedforbystandardcompositionrules.Generallyspeaking thesearereferredtoasprivacyampliﬁcationrules withprominentexamplesbeingampliﬁcationbysubsampling[9 18 20 6 5 8 2 27] shufﬂing[16 10 3]anditeration[17].Motivatedbytheseconsiderations inthispaperweinitiateasystematicstudyofprivacyampliﬁcationbystochasticpost-processing.Speciﬁcally givenaDPmechanismMproducing(probabilistic)outputsinXandaMarkovoperatorKdeﬁningastochastictransitionbetweenXandY weareinterestedinmeasuringtheprivacyofthepost-processedmechanismK◦MproducingoutputsinY.Thestandardpost-processingpropertyofDPstatesthatK◦MisatleastasprivateasM.Ourgoalistounderstandunderwhatconditionsthepost-processedmechanismK◦MisstrictlymoreprivatethanM.Roughlyspeaking thisampliﬁcationshouldbenon-trivialwhentheoperatorK“forgets”informationaboutthedistributionofitsinputM(D).Ourmaininsightisthat atleastwhenY=X 33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.theforgetfulnessofKfromthepointofviewofDPcanbemeasuredusingsimilartoolstotheonesdevelopedtoanalyzethespeedofconvergence i.e.mixing oftheMarkovprocessassociatedwithK.Inthissetting weprovidethreetypesofresults eachassociatedwithastandardmethodusedinthestudyofconvergenceforMarkovprocesses.Intheﬁrstplace Section3providesDPampliﬁcationresultsforthecasewheretheoperatorKsatisﬁesauniformmixingcondition.TheseincludestandardconditionsusedintheanalysisofMarkovchainsondiscretespaces includingthewell-knownDobrushincoefﬁcentandDoeblin’sminorizationcondition[19].Althoughinprincipleuniformmixingconditionscanalsobedeﬁnedinmoregeneralnon-discretespaces[12] mostMarkovoperatorsofinterestinRddonotexhibituniformmixingsincethespeedofconvergencedependsonhowfaraparttheinitialinputsare.Convergenceanalysesinthiscaserelyonmoresophisticatedtools includingLyapunovfunctions[22] couplingmethods[21]andfunctionalinequalities[1].Followingtheseideas Section4investigatestheuseofcouplingmethodstoquantifyprivacyampliﬁcationbypost-processingunderRényiDP[23].Thesemethodsapplytooperatorsgivenby e.g. GaussianandLaplacedistributions forwhichuniformmixingdoesnothold.Resultsinthissectionareintimatelyrelatedtotheprivacyampliﬁcationbyiterationphenomenonstudiedin[17]andcanbeinterpretedasextensionsoftheirmainresultstomoregeneralsettings.Inparticular ouranalysisunpackstheshiftedRényidivergenceusedintheproofsfrom[17]andallowsustoeasilytracktheeffectofiteratingarbitrarynoisyLipschitzmaps.Asaconsequence weshowanexponentialimprovementontheprivacyampliﬁcationbyiterationofNoisySGDinthestronglyconvexcasewhichfollowsfromapplyingthisgeneralizedanalysistostrictcontractions.OurlastsetofresultsconcernsthecasewhereKisreplacedbyafamilyofoperators(Pt)t≥0formingaMarkovsemigroup[1].Thisisthenaturalsettingforcontinuous-timeMarkovprocesses andincludesdiffusionprocessesdeﬁnedintermsofstochasticdifferentialequations[25].InSection5weassociate(acollectionof)diffusionmechanisms(Mt)t≥0toadiffusionsemigroup.Interestingly thesemechanismsare byconstruction closedunderpost-processinginthesensethatPs◦Mt=Ms+t.WeshowtheGaussianmechanismfallsintothisfamily–sinceGaussiannoiseisclosedunderaddition–andalsopresentanewmechanismbasedontheOrnstein-UhlenbeckprocesswhichhasbettermeansquarederrorthanthestandardGaussianmechanism(andmatchestheerroroftheoptimallypost-processedGaussianmechanismwithboundedinputs).OurmainresultondiffusionmechanismsprovidesagenericRényiDPguaranteebasedonanintrinsicnotionofsensitivityderivedfromthegeometryinducedbythesemigroup.Theproofreliesonaheatﬂowargumentreminiscentoftheanalysisofmixingindiffusionprocessesbasedonfunctionalinequalities[1].2BackgroundWestartbyintroducingnotationandconceptsthatwillbeusedthroughoutthepaper.Wewrite[n]={1 ... n} a∧b=min{a b}and[a]+=max{a 0}.Probability.LetX=(X Σ λ)beameasurablespacewithsigma-algebraΣandbasemeasureλ.WewriteP(X)todenotethesetofprobabilitydistributionsonX.Givenaprobabilitydistributionµ∈P(X)andameasurableeventE⊆Xwewriteµ(E)=P[X∈E]forarandomvariableX∼µ denoteitsexpectationunderf:X→RdbyE[f(X)] andcangetbackitsdistributionasµ=Law(X).Giventwodistributionsµ ν(or ingeneral arbitrarymeasures)wewriteµ(cid:28)νtodenotethatµisabsolutelycontinuouswithrespecttoν inwhichcasethereexistsaRadon-Nikodymderivativedµdν.Weshallreservethenotationpµ=dµdλtodenotethedensityofµwithrespecttothebasemeasure.WealsowriteC(µ ν)todenotethesetofcouplingsbetweenµandν;i.e.π∈C(µ ν)isadistributiononP(X×X)withmarginalsµandν.Thesupportofadistributionissupp(µ).MarkovOperators.WewilluseK(X Y)todenotethesetofMarkovoperatorsK:X→P(Y)deﬁningastochastictransitionmapbetweenXandYandsatisfyingthatx7→K(x)(E)ismeasurableforeverymeasurableE⊆Y.Markovoperatorsactondistributionsµ∈P(X)ontheleftthrough(µK)(E)=RK(x)(E)µ(dx) andonfunctionsf:Y→Rontherightthrough(Kf)(x)=Rf(y)K(x dy) whichcanalsobewrittenas(Kf)(x)=E[f(X)]withX∼K(x).ThekernelofaMarkovoperatorK(withrespecttoλ)isthefunctionk(x ·)=dK(x)dλassociatingwithxthedensityofK(x)withrespecttoaﬁxedmeasure.Divergences.ApopularwaytomeasuredissimilaritybetweendistributionsistouseCsiszárdivergencesDφ(µkν)=Rφ(dµdν)dν whereφ:R+→Risconvexwithφ(1)=0.Taking2φ(u)=12|u−1|yieldsthetotalvariationdistanceTV(µ ν) andthechoiceφ(u)=[u−eε]+withε≥0givesthehockey-stickdivergenceDeε whichsatisﬁesDeε(µkν)=Z(cid:20)dµdν−eε(cid:21)+dν=Z[pµ−eεpν]+dλ=supE⊆X(µ(E)−eεν(E)).Itiseasytocheckthatε7→Deε(µkν)ismonotonicallydecreasingandD1=TV.AllCsiszárdivergencessatisfyjointconvexityD((1−γ)µ1+γµ2k(1−γ)ν1+γν2)≤(1−γ)D(µ1kν1)+γD(µ2kν2)andthedataprocessinginequalityD(µKkνK)≤D(µkν)foranyMarkovoperatorK.Rényidivergences1areanotherwaytocomparedistributions.Forα>1theRényidivergenceoforderαisdeﬁnedasRα(µkν)=1α−1logR(dµdν)αdν andalsosatisﬁesthedataprocessinginequality.Finally tomeasuresimilaritybetweenµ ν∈P(Rd)wesometimesusethe∞-Wassersteindistance:W∞(µ ν)=infπ∈C(µ ν)inf{w≥0:kX−Yk≤wholdsalmostsurelyfor(X Y)∼π}.DifferentialPrivacy.AmechanismM:Dn→P(X)isarandomizedfunctionthattakesadatasetD∈DnoversomeuniverseofrecordsDandreturnsa(samplefrom)distributionM(D).WewriteD’D0todenotetwodatabasesdifferinginasinglerecord.WesaythatMsatisﬁes2(ε δ)-DPifsupD’D0Deε(M(D)kM(D0))≤δ[15].Furthermore wesaythatMsatisﬁes(α )-RDPifsupD’D0Rα(M(D)kM(D0))≤[23].3AmpliﬁcationFromUniformMixingWestartouranalysisofprivacyampliﬁcationbystochasticpost-processingbyconsideringsettingswheretheMarkovoperatorKsatisﬁesoneofthefollowinguniformmixingconditions.Deﬁnition1.LetK∈K(X Y)beaMarkovoperator γ∈[0 1]andε≥0.WesaythatKis:(1)γ-Dobrushinifsupx x0TV(K(x) K(x0))≤γ (2)(γ ε)-Dobrushinifsupx x0Deε(K(x)kK(x0))≤γ (3)γ-Doeblinifthereexistsadistributionω∈P(Y)suchthatK(x)≥(1−γ)ωforallx∈X (4)γ-ultra-mixingifforallx x0∈XwehaveK(x)(cid:28)K(x0)anddK(x)dK(x0)≥1−γ.MostoftheseconditionsariseinthecontextofmixinganalysesinMarkovchains.Inparticular theDobrushinconditioncanbetrackedbackto[13] whileDoeblin’sconditionwasintroducedearlier[14](seealso[24]).Ultra-mixingisastrengtheningofDoeblin’sconditionusedin[12].The(γ ε)-Dobrushinis ontheotherhand newandisdesignedtobeageneralizationofDobrushintailoredforampliﬁcationunderthehockey-stickdivergence.ItisnothardtoseethatDobrushin’sistheweakestamongtheseconditions andinfactwehavetheimplicationssummarizedinFigure1(seeLemma9).Thisexplainswhytheampliﬁcationboundsinthefollowingresultareincreasinglystronger andinparticularwhytheﬁrsttwoonlyprovideampliﬁcationinδ whilethelasttwoalsoamplifytheεparameter.Theorem1.LetMbean(ε δ)-DPmechanism.ForagivenMarkovoperatorK thepost-processedmechanismK◦Msatisﬁes:(1)(ε δ0)-DPwithδ0=γδifKisγ-Dobrushin (2)(ε δ0)-DPwithδ0=γδifKis(γ ˜ε)-Dobrushinwith3˜ε=log(1+eε−1δ) (3)(ε0 δ0)-DPwithε0=log(1+γ(eε−1))andδ0=γ(1−eε0−ε(1−δ))ifKisγ-Doeblin (4)(ε0 δ0)-DPwithε0=log(1+γ(eε−1))andδ0=γδeε0−εifKisγ-ultra-mixing.Afewremarksaboutthisresultareinorder.Firstwenotethat(2)isstrongerthan(1)sincethemonotonicityofhockey-stickdivergencesimpliesTV=D1≥De˜ε.Alsonotehowintheresultsabovewealwayshaveε0≤ε andinfacttheformofε0isthesameasobtainedunderampliﬁcation1RényidivergencesdonotbelongtothefamilyofCsiszárdivergences.2ThisdivergencecharacterizationofDPisdueto[4].3Wetaketheconvention˜ε=∞wheneverδ=0 inwhichcasethe(γ ∞)-DobrushinconditionisobtainedwithrespecttothedivergenceD∞(µkν)=µ(supp(µ)\supp(ν)).3γ-ultra-mixingγ-Doeblinγ-Dobrushin(γ ε)-DobrushinFigure1:ImplicationsbetweenmixingconditionsMixingConditionLocalDPConditionγ-Dobrushin(0 γ)-LDP(γ ε)-Dobrushin(ε γ)-LDPγ-DoeblinBlanketcondition4γ-ultra-mixing(log11−γ 0)-LDPTable1:Relationbetweenmixingcondi-tionsandlocalDPbysubsamplingwhen e.g. aγ-fractionoftheoriginaldatasetiskept.Thisisnotacoincidencesincetheproofsof(3)and(4)leveragetheoverlappingmixturestechniqueusedtoanalyzeampliﬁcationbysubsamplingin[2].However wenotethatfor(3)wecanhaveδ0>0evenwithδ=0.InfacttheDoeblinconditiononlyleadstoanampliﬁcationinδifγ≤δeε(1−δ)(eε−1).WeconcludethissectionbynotingthattheconditionsinDeﬁnition1 despitebeingquitenatural mightbetoostringentforprovingampliﬁcationforDPmechanismson say Rd.OnewaytoseethisistointerprettheoperatorK:X→P(Y)asamechanismandtonotethattheuniformmixingconditionsonKcanberephrasedintermsoflocalDP(LDP)[18]properties(seeTable1forproperty4translations)wherethesupremumistakenoveranypairofinputs(insteadofneighboringones).Thismotivatestheresultsonnextsection wherewelookforﬁnerconditionstoproveampliﬁcationbystochasticpost-processing.4AmpliﬁcationFromCouplingsInthissectionweturntocoupling-basedproofsofampliﬁcationbypost-processingundertheRényiDPframework.Ourﬁrstresultisameasure-theoreticgeneralizationoftheshift-reductionlemmain[17]whichdoesnotrequiretheunderlyingspacetobeanormedvectorspace.ThemaindifferencesinourproofaretouseexplicitcouplingsinsteadoftheshiftedRényidivergencewhichimplicitlyreliesontheexistenceofanorm(throughtheuseofW∞) andreplacetheidentityU+W−W=Ubetweenrandomvariableswhichdependsonthevector-spacestructurewithatransportoperatorsHπandHπ0whichsatisfyµHπ0Hπ=µinageneralmeasure-theoreticsetting.Givenacouplingπ∈C(µ ν)withµ ν∈P(X) weconstructatransportMarkovoperatorHπ:X→P(X)withkernel5hπ(x y)=pπ(x y)pµ(x) wherepπ=dπdλ⊗λandpµ=dµdλ.ItisimmediatetoverifyfromthedeﬁnitionthatHπisaMarkovoperatorsatisfyingthetransportpropertyµHπ=ν(seeLemma16).Theorem2.Letα≥1 µ ν∈P(X)andK∈K(X Y).Foranydistributionω∈P(X)andcouplingπ∈C(ω µ)wehaveRα(µKkνK)≤Rα(ωkν)+supx∈supp(ν)Rα((HπK)(x)kK(x)).(1)Notethatthisresultcapturesthedata-processinginequalityforRényidivergencessincetakingω=µandtheidentitycouplingyieldsRα(µKkνK)≤Rα(µkν).ThenextexamplesillustratetheuseofthistheoremtoobtainampliﬁcationbyoperatorscorrespondingtotheadditionofGaussianandLaplacenoise.Example1(IteratedGaussian).Wecanshowthat(1)istightandequivalenttotheshift-reductionlemma[17]onRdbyconsideringthesimplescenarioofaddingGaussiannoisetotheoutputofaGaussianmechanism.Inparticular supposeM(D)=N(f(D) σ21I)forsomefunctionfwithglobalL2-sensitivity∆andtheMarkovoperatorKisgivenbyK(x)=N(x σ22I).Thepost-processedmechanismisgivenby(K◦M)(D)=N(f(D) (σ21+σ22)I) whichsatisﬁes(α α∆22(σ21+σ22))-RDP.WenowshowhowthisresultalsofollowsfromTheorem2.GiventwodatasetsD’D0wewriteµ=M(D)=N(u σ21I)andν=M(D0)=N(v σ21I)withku−vk≤∆.We4TheblanketconditionisanecessaryconditionforLDPintroducedin[3]toanalyzeprivacyampliﬁcationbyshufﬂing.5Hereweusetheconvention00=0.4takeω=N(w σ21I)forsomewtobedeterminedlater andcoupleωandµthroughatranslationτ=u−w yieldingacouplingπwithpπ(x y)∝exp(−kx−wk22σ21)I[y=x+τ]andatransportoperatorHπwithkernelhπ(x y)=I[y=x+τ].Pluggingtheseinto(1)wegetRα(µKkνK)≤αkw−vk22σ21+supx∈RdRα(K(x+τ)kK(x))=α2(cid:18)kw−vk2σ21+ku−wk2σ22(cid:19).Finally takingw=θu+(1−θ)vwithθ=(1+σ22σ21)−1yieldsRα(µKkνK)≤α∆22(σ21+σ22).Example2(IteratedLaplace).Toillustratetheﬂexibilityofthistechnique wealsoapplyittogetanampliﬁcationresultforiteratedLaplacenoise inwhichLaplacenoiseisaddedtotheoutputofaLaplacemechanism.Webeginbynotinganegativeresultthatthereisnoampliﬁcationinthe(ε 0)-DPregime.Lemma3.LetM(D)=Lap(f(D) λ1)forsomefunctionf:D→RwithglobalL1-sensitivity∆andlettheMarkovoperatorKbegivenbyK(x)=Lap(x λ2).Thepost-processedmechanism(K◦M)doesnotachieve(ε 0)-DPforanyε<∆max{λ1 λ2}.NotethatMachieves(∆λ1 0)-DPandK(f(D))achieves(∆λ2 0)-DP.However theiteratedLaplacemechanismK◦MabovestilloffersadditionalprivacyintherelaxedRDPsetting.Anapplicationof(1)allowsustoidentifysomeofthisimprovement.Recallfrom[23 Corollary2]thatMsatisﬁes(α 1α−1loggα(∆λ1))-RDPwithgα(z)=α2α−1exp(z(α−1))+α−12α−1exp(−zα).AsinExample1 wetakeω=Lap(w λ1)forsomewtobedeterminedlater andcoupleωandµthroughatranslationτ=u−w.Through(1)weobtainRα(µKkνK)≤1α−1log(cid:18)gα(cid:18)|w−v|λ1(cid:19)(cid:19)+supx∈RRα(K(x+τ)kK(x))=1α−1log(cid:18)gα(cid:18)|w−v|λ1(cid:19)gα(cid:18)|u−w|λ2(cid:19)(cid:19).Inthesimplecasewhereλ1=λ2 anampliﬁcationresultisobservedfromthelog-convexityofgα sincegα(a)gα(b)≤gα(a+b).Whenλ16=λ2 certainvaluesofwstillresultinampliﬁcation buttheydependnontriviallyonα.However wealsoobservethatthisimprovementvanishesasα→∞ sincethenecessaryconvexityalsovanishes.Inthelimit thelowestupperboundofferedby(1)forR∞(whichreducesto(ε 0)-DP)matchesthe∆max{λ1 λ2}resultofLemma3.Example3(LipschitzKernel).Asawarm-upfortheresultsinSection4.1 wenowre-workExample1withaslightlymorecomplexMarkovoperator.SupposeψisanL-Lipschitzmap6andletK(x)=N(ψ(x) σ22I).TakingMtobetheGaussianmechanismfromExample1 wewillshowthatthepost-processedmechanismK◦Msatisﬁes(α α∆22σ2∗)-RDPwithσ2∗=σ21+σ22L2.Toprovethisbound weinstantiatethenotationfromExample1 andusethesamecouplingstrategytoobtainRα(µKkνK)≤α2(cid:18)kw−vk2σ21+supx∈Rdkψ(x+τ)−ψ(x)k2σ22(cid:19)≤α2(cid:18)kw−vk2σ21+L2ku−wk2σ22(cid:19) wherethesecondinequalityusestheLipschitzproperty.Asbefore theresultfollowsfromtakingw=θu+(1−θ)vwithθ=(1+σ22L2σ21)−1.Thisexampleshowsthatwegetampliﬁcation(i.e.σ2∗>σ21)foranyL<∞andσ2>0 althoughtheamountofampliﬁcationdecreasesasLgrows.Ontheotherhand forL<1theampliﬁcationisstrongerthanjustaddingGaussiannoise(Example1).4.1AmpliﬁcationbyIterationinNoisyProjectedSGDwithStronglyConvexLossesNowweuseTheorem2andthecomputationsabovetoshowthattheproofofprivacyampliﬁcationbyiteration[17 Theorem22]canbeextendedtoexplicitlytracktheLipschitzcoefﬁcientsina“noisyiteration”algorithm.Inparticular thisallowsustoshowanexponentialimprovementontherateofprivacyampliﬁcationbyiterationinnoisySGDwhenthelossisstronglyconvex.ToobtainthisresultweﬁrstprovideaniteratedversionofTheorem2inRdwithLipschitzGaussian6Thatis kψ(x)−ψ(y)k≤Lkx−ykforanypairx y.5kernels.ThisversionoftheanalysisintroducesanexplicitdependenceontheW∞distancesalongan“interpolating”pathbetweentheinitialdistributionsµ ν∈P(Rd)whichcouldlaterbeoptimizedfordifferentapplications.Inourview thishelpstoclarifytheintuitionbehindthepreviousanalysisofampliﬁcationbyiteration.Theorem4.Letα≥1 µ ν∈P(Rd)andletK⊆Rdbeaconvexset.SupposeK1 ... Kr∈K(Rd Rd)areMarkovoperatorswhereYi∼Ki(x)isobtainedas7Yi=ΠK(ψi(x)+Zi)withZi∼N(0 σ2I) wherethemapsψi:K→RdareL-Lipschitzforalli∈[r].Foranyµ0 µ1 ... µr∈P(Rd)withµ0=µandµr=νwehaveRα(µK1···KrkνK1···Kr)≤αL22σ2rXi=1L2(r−i)W∞(µi µi−1)2.(2)Furthermore ifL≤1andW∞(µ ν)=∆ thenRα(µK1···KrkνK1···Kr)≤α∆2Lr+12rσ2.(3)NotehowtakingL=1intheboundaboveweobtainα∆22rσ2=O(1/r) whichmatches[17 Theorem1].Ontheotherhand forLstrictlysmallerthan1 theanalysisaboveshowsthattheampliﬁcationrateisO(Lr+1/r)asaconsequenceofthemapsψibeingstrictcontractions i.e.kψi(x)−ψi(y)k<kx−yk.ForL>1thisresultisnotusefulsincethesumwilldiverge;however theproofcouldeasilybeadaptedtohandlethecasewhereeachψiisLi-LipschitzwithsomeLi>1andsomeLi<1.Wenowapplythisresulttoimprovetheper-personprivacyguaranteesofnoisyprojectedSGD(Algorithm1)inthecasewherethelossfunctionissmoothandstronglyconvex.Algorithm1:NoisyProjectedStochasticGradientDescent—NoisyProjSGD(D ‘ η σ ξ0)Input:DatasetD=(z1 ... zn) lossfunction‘:K×D→R learningrateη noiseparameterσ initialdistributionξ0∈P(K)Samplex0∼ξ0fori∈[n]doxi←ΠK(xi−1−η(∇x‘(xi−1 zi)+Z))withZ∼N(0 σ2I)returnxnAfunctionf:K⊆Rd→Rdeﬁnedonaconvexsetisβ-smoothifitiscontinuouslydifferentiableand∇fisβ-Lipschitz i.e. k∇f(x)−∇f(y)k≤βkx−yk andisρ-stronglyconvexifthefunctiong(x)=f(x)−ρ2kxk2isconvex.Whenwesaythatalossfunction‘:K×D→Rsatisﬁesaproperty(e.g.smoothness)wemeanthepropertyissatisﬁedby‘(· z)forallz∈D.Furthermore werecallfrom[17]thatamechanismM:Dn→Xsatisﬁes(α )-RDPatindexiifRα(M(D)kM(D0))≤holdsforanypairofdatabasesDandD0differingontheithcoordinate.Theorem5.Let‘:K×D→RbeaC-Lipschitz β-smooth ρ-stronglyconvexlossfunction.Ifη≤2β+ρ thenNoisyProjSGD(D ‘ η σ ξ0)satisﬁes(α αi)-RDPatindexi wheren=2C2σ2andi=2C2(n−i)σ2(1−2ηβρβ+ρ)n−i+12for1≤i≤n−1.Since[17 Theorem23]showsthatforsmoothLipschitzlossfunctionstheguaranteeatindexiofNoisyProjSGDisgivenbyi=O(C2(n−i)σ2) ourresultprovidesanexponentialimprovementinthestronglyconvexcase.Thisimplies forexample thatusingthetechniquein[17 Corollary31]onecanshowthat inthestronglyconvexsetting runningΘ(log(d))additionaliterationsofNoisyProjSGDonpublicdataisenoughtoattain(uptoconstantfactors)thesameoptimizationerrorasnon-privateSGDwhileprovidingprivacyforallindividuals.5DiffusionMechanismsNowwegobeyondtheanalysisfromprevioussectionsandsimultaneouslyconsiderafamilyofMarkovoperatorsP=(Pt)t≥0indexedbyacontinuousparametertandsatisfyingthesemigroup7HereΠK(x)=argminy∈Kkx−ykdenotestheprojectionoperatorontotheconvexsetK⊆Rd.6propertyPtPs=Pt+s.SuchPiscalledaMarkovsemigroupandcanbeusedtodeﬁneafamilyofoutputperturbationmechanismsMft(D)=Pt(f(D))whichareclosedunderpost-processingbyPinthesensethatPs◦Mft=Mft+s.Thesemigrouppropertygreatlysimpliﬁestheanalysisofprivacyampliﬁcationbypost-processing since forexample ifweshowthatMftsatisﬁes(α (t))-RDP thenthisimmediatelyprovidesRDPguaranteesforanypost-processingofMtbyanynumberofoperatorsinP.ThemainresultofthissectionprovidessuchprivacyanalysisformechanismsarisingfromsymmetricdiffusionMarkovsemigroupsinEuclideanspace.Wewillshowthisclassincludesthewell-knownGaussianmechanism andalsoidentifyanotherinterestingmechanisminthisclassarisingfromtheOrnstein-Uhlenbeckdiffusionprocess.Roughlyspeaking adiffusionMarkovsemigroupP=(Pt)t≥0onRdcorrespondstothecasewhereXt∼Pt(x)deﬁnesaMarkovprocess(Xt)t≥0arisingfroma(time-homogeneousItô)stochasticdifferentialequation(SDE)oftheformX0=xanddXt=u(Xt)dt+v(Xt)dWt whereWtisastandardd-dimensionalWienerprocess andthedriftu:Rd→Rdanddiffusionv:Rd→Rd×dcoefﬁcientssatisfyappropriateregularityassumptions.8Inthispaper however weshallfollow[1]andtakeamoreabstractapproachtoMarkovdiffusionsemigroups.WesynthesizethisapproachbymakinganumberofhypothesesonPthatwediscussafterintroducingtwocoreconceptsfromthetheoryofMarkovsemigroups.InthecontextofaMarkovsemigroupP theactionoftheMarkovoperatorsPtonfunctionscanbeusedtodeﬁnethegeneratorLofthesemigroupastheoperatorgivenbyLf=ddt(Ptf)|t=0.Inparticular foradiffusionsemigrouparisingfromtheSDEdXt=u(Xt)dt+v(Xt)dWtitiswell-knownthatonecanwritethegeneratorasLf=hu ∇fi+12hvv> H(f)i whereH(f)istheHessianoffandthesecondtermisaFrobeniusinnerproduct.Usingthegeneratoronealsodeﬁnestheso-calledcarréduchampoperatorΓ(f g)=12(L(fg)−fLg−gLf).Thisoperatorisbilinearandnon-negativeinthesensethatΓ(f) Γ(f f)≥0.ThecarréduchampoperatoroperatorcanbeinterpretedasadevicetomeasurehowfarLisfrombeingaﬁrst-orderdifferentialoperator since e.g. ifL=Piai∂∂xithenL(fg)=fLg+gLfandthereforeΓ(f g)=0.TheoperatorΓcanalsoberelatedtonotionsofcurvature/contractivityoftheunderlyingsemigroup[1].BelowweillustratetheseconceptswiththeexampleofBrownianmotion;butﬁrstweformallystateourassumptionsonthesemigroup.Assumption1.SupposetheMarkovsemigroupP=(Pt)t≥0⊂K(Rd Rd)satisﬁesthefollowing:(1)Thereexistsauniquenon-negativeinvariantmeasureλ;thatis λPt=λforallt≥0.Whentheinvariantmeasureisﬁnitewenormalizeittobeaprobabilitymeasure.(2)TheoperatorsPtadmitasymmetrickernelpt(x y)=pt(y x)withrespecttotheinvariantmeasure.Equivalently theinvariantmeasureλisreversiblefortheMarkovprocessXt.(3)ThegeneratorLsatisﬁesthediffusionpropertyLφ(f)=φ0(f)Lf+φ00(f)Γ(f)foranydifferentiableφ:R→R.ThisisachainrulepropertysayingthatLisasecond-orderdifferentialoperatorwithoutconstantterms.Example4(BrownianMotion).ThesimplestdiffusionprocessistheBrownianmotiongivenbythesimpleSDEdXt=√2dWt. whichcorrespondstothesemigroupPgivenbyPt(x)=N(x 2t).Inthiscase themechanismMft(D)=Pt(f(D))isaGaussianmechanismwithvarianceσ2=2tandthereforesatisﬁes(α α∆24t)-RDP where∆istheglobalL2-sensitivityoff.Adirectsubstitutionwithu=0andv=√2Ishowsthatthesemigroup’sgeneratoristhestandardLaplacianinRd L=∇2=Pdi=1∂2∂x2i andasimplecalculationyieldstheexpressionΓ(f g)=h∇f ∇giforthecarréduchampoperator.NowwecheckthatPsatisﬁestheconditionsinAssumption1.First werecallthatBrownianmotionhastheLebesguemeasureλonRdasitsuniqueinvariantmeasure;thishappenstobeanon-ﬁnitemeasure.Withrespecttoλ thesemigrouphaskernelpt(x y)∝exp(−kx−yk24t)whichisclearlysymmetric.Finally weusethechainruleforthegradienttoverifythatLf=∇2φ(f)=∇(φ0(f)∇f)=φ00(f)h∇f ∇fi+φ0(f)∇2f=φ00(f)Γ(f)+φ0(f)Lf.Nowweturntothemainresultofthissection whichprovidesaprivacyanalysisforthediffusionmechanismMftassociatedwithanarbitrarysymmetricdiffusionMarkovsemigroup.Thekeyinsight8ThedetailsarenotrelevantheresinceweworkdirectlywithsemigroupssatisfyingAssumption1.Wereferto[25]fordetails.7behindthisresultisthatthecarréduchampoperatorofthesemigroupprovidesameasureΛ(t)ofintrinsicsensitivityforthemechanismMftdeﬁnedas:Λ(t)=supD’D0Z∞tκf(D) f(D0)(s)ds whereκx x0(t)=supy∈RdΓ(cid:18)logpt(x y)pt(x0 y)(cid:19).Theorem6.Letf:Dn→RdandletP=(Pt)t≥0byaMarkovsemigrouponRdsatisfyingAssumption1.IfthemechanismMft(D)=Pt(f(D))hasintrinsicsensitivityΛ(t) thenitsatisﬁes(α αΛ(t))-RDPforanyα>1andt>0.Example5(BrownianMotion Continued).ToillustratetheuseofTheorem6weshowhowitcanbeusedtorecovertheprivacyguaranteesoftheGaussianmechanismthroughitsconnectionwithBrownianmotion.WeletPbethesemigroupfromExample4andstartbyusingΓ(f)=k∇fk2tocomputeκx x0(t)asfollows:Γ(cid:18)logpt(x y)pt(x0 y)(cid:19)=(cid:13)(cid:13)(cid:13)(cid:13)∇y(cid:18)kx0−yk2−kx−yk24t(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2=kx−x0k24t2.NowweuseR∞t1s2ds=1tand∆2=supD’D0kf(D)−f(D0)k2toseethatthemechanismassociatedwithPhasintrinsicsensitivityΛ(t)=∆24t yieldingtheprivacyguaranteefromExample4.5.1TheOrnstein-UhlenbeckMechanismBeyondBrownianmotion anotherwell-knowndiffusionprocessistheOrnstein-Uhlenbeckprocesswithparametersθ ρ>0givenbytheSDEdXt=−θXtdt+√2ρdWt.ThisdiffusionprocessisassociatewiththesemigroupP=(Pt)t≥0givenbyPt(x)=N(e−θtx ρ2θ(1−e−2θt)I).OneinterpretationofthisdiffusionprocessistothinkofXtasaBrownianmotionwithvarianceρ2appliedtoameanrevertingﬂowthatpullsaparticletowardstheoriginatarateθ.Inparticular themechanismMft(D)isgivenbyreleasinge−θtf(D)+N(0 ρ2θ(1−e−2θt)).Takingthelimitt→∞oneseesthatthe(unique)invariantmeasureofPistheGaussiandistributionλ=N(0 ρ2θI).FromtheSDEcharacterizationofthisprocessitiseasytocheckthatitsgeneratorisLf=ρ2∇2f−θhx ∇fiandtheassociatedcarréduchampoperatorisΓ(f g)=ρ2h∇f ∇gi.Thus Psatisﬁesconditions(1)and(3)inAssumption1.TocheckthesymmetryconditionweapplyachangeofmeasuretotheGaussiandensity˜pt(x y)ofPtwithrespecttotheLebesguemeasuretogetitsdensityw.r.t.λ:pt(x y)=˜pt(x y)˜pλ(y)∝exp(cid:16)−θky−e−θtxk22ρ2(1−e−2θt)(cid:17)exp(cid:16)−θkyk22ρ2(cid:17)=exp(cid:18)−θkxk2−2eθthx yi+kyk22ρ2(e2θt−1)(cid:19) where˜pλisthedensityofλw.r.t.theLebesguemeasure.Thus Theorem6yieldsthefollowing.Corollary7.Letf:Dn→RdhaveglobalL2-sensitivity∆andP=(Pt)t≥0betheOrnstein-Uhlenbecksemigroupwithparametersθ ρ.Foranyα>1andt>0themechanismMft(D)=Pt(f(D))satisﬁes(α αΛ(t))-RDPwithΛ(t)=θ∆22ρ2(e2θt−1).TheOrnstein-UhlenbeckmechanismisnotanunbiasedmechanismsinceE[Mft(D)]=e−θtf(D).ThisbiasisthereasonwhytheprivacyguaranteeinCorollary7exhibitsarateO(e−2θt) while forexample theBrownianmotionmechanismonlyexhibitsarateO(t−1).Inparticular theOrnstein-Uhlenbeckmechanismachievesitsprivacynotonlybyintroducingnoise butalsobyshrinkingf(D)towardsadata-independentpoint(theorigininthiscase);thiseffectivelycorrespondstoreducingthesensitivityofffrom∆toe−θt∆.Thisprovidesawaytotrade-offvarianceandbiasinthemean-squarederror(MSE)incurredbyprivatelyreleasingf(D)inasimilarwaythatcanbeachievedbypost-processingtheGaussianmechanismwhenf(D)isknowntobebounded.ToformalizethisresultwedeﬁnethemeansquarederrorEOU(θ ρ t)oftheOrnstein-Uhlenbeckmechanismwithparametersθ ρattimet whichisgivenby:EOU(θ ρ t) E[kf(D)−Mft(D)k2]=(1−e−θt)2kf(D)k2+dρ2θ(1−e−2θt).(4)8Similarly wedeﬁneEGM(θ ρ t)asthemeansquarederrorofaGaussianmechanismwiththesameprivacyguaranteesasMftwithparametersθ ρ.Inparticular wehaveEGM(θ ρ t)=d˜σ2 where˜σ2 ρ2(e2θt−1)θ(cf.Corollary7).Wealsonotethepost-processedGaussianmechanism(PGM)D7→β(f(D)+N(0 ˜σ2I))whichmultipliestheoutputbyascalarβoptimizedtominimizetheMSEundertheconditionkf(D)k≤RyieldserrorEPGM(θ ρ t)≤EGM(θ ρ t)(1+d˜σ2R2)−1.Theorem8.Supposef:Dn→RdhasglobalL2-sensitivity∆andsatisﬁessupDkf(D)k≤R.IfθR2≤4dρ2thenwehaveEOU(θ ρ t)EGM(θ ρ t)≤1forallt≥0andlimt→∞EOU(θ ρ t)EGM(θ ρ t)=0.Inparticular takingθ=log(cid:16)1+d∆22R2(cid:17)andρ2=θ∆22(e2θ−1)with>0 themechanismMftsatisﬁes(α α)-RDPattimet=1andwehaveEOU(θ ρ 1)EGM(θ ρ 1)≤(cid:16)1+d∆22R2(cid:17)−1.ThisresultnotonlyshowsthattheOrnstein-UhlenbeckmechanismisuniformlybetterthantheGaussianmechanismforanylevelofprivacy butalsoshowsthatinthismechanismtheerroralwaysstaysboundedandcanattainthesameleveloferrorastheGaussianmechanismwithoptimalpost-processing.ToseethisnotethatwiththechoicesofparametersmadeinthesecondstatementgiveEGM(θ ρ 1)=d∆22andthereforeEOU(θ ρ 1)≤d∆2R22R2+d∆2 whichbehaveslikeO(R2)with∆constantandeither→0ord→∞.6ConclusionWehaveundertakenasystematicstudyofampliﬁcationbypost-processing.Ourresultsyieldimprovementsoverrecentworkonampliﬁcationbyiteration andintroduceanewOrnstein-UhlenbeckmechanismwhichismoreaccuratethantheGaussianmechanism.Inthefutureitwouldbeinterestingtostudyapplicationsofampliﬁcationbypost-processing.OnepromisingapplicationisHierarchicalDifferentialPrivacy whereinformationisreleasedunderincreasinglystrongprivacyconstraints(e.g.toarestrictedgroupwithinacompany globallywithinacompany andﬁnallytooutsideparties).AcknowledgementsMGwaspartiallysupportedbyNSFgrantCCF-1718220.References[1]DominiqueBakry IvanGentil andMichelLedoux.AnalysisandgeometryofMarkovdiffusionoperators volume348.SpringerScience&BusinessMedia 2013.[2]BorjaBalle GillesBarthe andMarcoGaboardi.Privacyampliﬁcationbysubsampling:Tightanalysesviacouplingsanddivergences.InAdvancesinNeuralInformationProcessingSystems31:AnnualConferenceonNeuralInformationProcessingSystems2018 NeurIPS2018 3-8December2018 Montréal Canada. pages6280–6290 2018.[3]BorjaBalle JamesBell AdriàGascón andKobbiNissim.Theprivacyblanketoftheshufﬂemodel.CoRR abs/1903.02837 2019.[4]GillesBartheandFedericoOlmedo.Beyonddifferentialprivacy:Compositiontheoremsandrelationallogicforf-divergencesbetweenprobabilisticprograms.InInternationalColloquiumonAutomata Languages andProgramming pages49–60.Springer 2013.[5]AmosBeimel HaiBrenner ShivaPrasadKasiviswanathan andKobbiNissim.Boundsonthesamplecomplexityforprivatelearningandprivatedatarelease.Machinelearning 94(3):401–437 2014.[6]AmosBeimel KobbiNissim andUriStemmer.Characterizingthesamplecomplexityofprivatelearners.InProceedingsofthe4thconferenceonInnovationsinTheoreticalComputerScience pages97–110.ACM 2013.[7]SébastienBubeck.Convexoptimization:Algorithmsandcomplexity.FoundationsandTrendsR(cid:13)inMachineLearning 8(3-4):231–357 2015.9[8]MarkBun KobbiNissim UriStemmer andSalilVadhan.Differentiallyprivatereleaseandlearningofthresholdfunctions.InFoundationsofComputerScience(FOCS) 2015IEEE56thAnnualSymposiumon pages634–649.IEEE 2015.[9]KamalikaChaudhuriandNinaMishra.Whenrandomsamplingpreservesprivacy.InAnnualInternationalCryptologyConference pages198–213.Springer 2006.[10]AlbertCheu AdamD.Smith JonathanUllman DavidZeber andMaximZhilyaev.Distributeddifferentialprivacyviashufﬂing.InAdvancesinCryptology-EUROCRYPT2019-38thAnnualInternationalConferenceontheTheoryandApplicationsofCryptographicTechniques Darmstadt Germany May19-23 2019 Proceedings PartI pages375–403 2019.[11]JoelECohen YohIwasa GhRautu MaryBethRuskai EugeneSeneta andGhZbaganu.Relativeentropyundermappingsbystochasticmatrices.Linearalgebraanditsapplications 179:211–235 1993.[12]PDelMoral MLedoux andLMiclo.OncontractionpropertiesofMarkovkernels.Probabilitytheoryandrelatedﬁelds 126(3):395–420 2003.[13]RolandLDobrushin.CentrallimittheoremfornonstationaryMarkovchains.I.TheoryofProbability&ItsApplications 1(1):65–80 1956.[14]W.Doeblin.SurlesproprietesasymptotiquesdemouvementsrÉgisparcertainstypesdechaÎnessimples(suiteetﬁn).BulletinmathématiquedelaSociétéRoumainedesSciences 39(2):3–61 1937.[15]CynthiaDwork FrankMcSherry KobbiNissim andAdamSmith.Calibratingnoisetosensitivityinprivatedataanalysis.InTheoryofcryptography pages265–284.Springer 2006.[16]ÚlfarErlingsson VitalyFeldman IlyaMironov AnanthRaghunathan KunalTalwar andAbhradeepThakurta.Ampliﬁcationbyshufﬂing:Fromlocaltocentraldifferentialprivacyviaanonymity.InProceedingsoftheThirtiethAnnualACM-SIAMSymposiumonDiscreteAlgorithms pages2468–2479.SIAM 2019.[17]VitalyFeldman IlyaMironov KunalTalwar andAbhradeepThakurta.Privacyampliﬁcationbyiteration.In2018IEEE59thAnnualSymposiumonFoundationsofComputerScience(FOCS) pages521–532.IEEE 2018.[18]ShivaPrasadKasiviswanathan HominKLee KobbiNissim SofyaRaskhodnikova andAdamSmith.Whatcanwelearnprivately?SIAMJournalonComputing 40(3):793–826 2011.[19]DavidALevinandYuvalPeres.Markovchainsandmixingtimes volume107.AmericanMathematicalSoc. 2017.[20]NinghuiLi WahbehQardaji andDongSu.Onsampling anonymization anddifferentialpri-vacyor k-anonymizationmeetsdifferentialprivacy.InProceedingsofthe7thACMSymposiumonInformation ComputerandCommunicationsSecurity pages32–33.ACM 2012.[21]TorgnyLindvall.Lecturesonthecouplingmethod.CourierCorporation 2002.[22]SeanPMeynandRichardLTweedie.Markovchainsandstochasticstability.SpringerScience&BusinessMedia 2012.[23]IlyaMironov.Rényidifferentialprivacy.In30thIEEEComputerSecurityFoundationsSymposium CSF2017 SantaBarbara CA USA August21-25 2017 pages263–275 2017.[24]EsaNummelin.GeneralirreducibleMarkovchainsandnon-negativeoperators volume83.CambridgeUniversityPress 2004.[25]BerntØksendal.Stochasticdifferentialequations.InStochasticdifferentialequations pages65–84.Springer 2003.[26]MaximRaginsky.StrongdataprocessinginequalitiesandΦ-Sobolevinequalitiesfordiscretechannels.IEEETransactionsonInformationTheory 62(6):3355–3389 2016.10[27]Yu-XiangWang BorjaBalle andShivaKasiviswanathan.Subsampledrényidifferentialprivacyandanalyticalmomentsaccountant.InProceedingsofthe22ndInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS) 2019.11,Borja Balle
Gilles Barthe
Marco Gaboardi
Joseph Geumlek