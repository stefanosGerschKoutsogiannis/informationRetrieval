2018,Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?,We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta  given by the sum of the reciprocals of the hidden layer widths. When beta is large  the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view  we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.,Which Neural Net Architectures Give Rise to

Exploding and Vanishing Gradients?

Boris Hanin

Department of Mathematics

Texas A& M University
College Station  TX  USA
bhanin@math.tamu.edu

Abstract

We give a rigorous analysis of the statistical behavior of gradients in a randomly
initialized fully connected network N with ReLU activations. Our results show
that the empirical variance of the squares of the entries in the input-output Jacobian
of N is exponential in a simple architecture-dependent constant   given by the
sum of the reciprocals of the hidden layer widths. When  is large  the gradients
computed by N at initialization vary wildly. Our approach complements the mean
ﬁeld theory analysis of random networks. From this point of view  we rigorously
compute ﬁnite width corrections to the statistics of gradients at the edge of chaos.

1

Introduction

A fundamental obstacle in training deep neural nets using gradient based optimization is the exploding
and vanishing gradient problem (EVGP)  which has attracted much attention (e.g. [BSF94  HBF+01 
MM15  XXP17  PSG17  PSG18]) after ﬁrst being studied by Hochreiter [Hoc91]. The EVGP occurs
when the derivative of the loss in the SGD update

W

 

W  

@L
@W

 

(1)

is very large for some trainable parameters W and very small for others:



@L

@W ⇡ 0 or 1.

This makes the increment in (1) either too small to be meaningful or too large to be precise. In practice 
a number of ways of overcoming the EVGP have been proposed (see e.g. [Sch]). Let us mention
three general approaches: (i) using architectures such as LSTMs [HS97]  highway networks [SGS15] 
or ResNets [HZRS16] that are designed speciﬁcally to control gradients; (ii) precisely initializing
weights (e.g. i.i.d. with properly chosen variances [MM15  HZRS15] or using orthogonal weight
matrices [ASB16  HSL16]); (iii) choosing non-linearities that that tend to compute numerically stable
gradients or activations at initialization [KUMH17].
A number of articles (e.g. [PLR+16  RPK+17  PSG17  PSG18]) use mean ﬁeld theory to show
that even vanilla fully connected architectures can avoid the EVGP in the limit of inﬁnitely wide
hidden layers. In this article  we continue this line of investigation. We focus speciﬁcally on fully
connected ReLU nets  and give a rigorous answer to the question of which combinations of depths d
and hidden layer widths nj give ReLU nets that suffer from the EVGP at initialization. In particular 
we avoid approach (iii) to the EVGP by setting once and for all the activations in N to be ReLU
and that we study approach (ii) in the limited sense that we consider only initializations in which
weights and biases are independent (and properly scaled as in Deﬁnition 1) but do not investigate

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

other initialization strategies. Instead  we focus on rigorously understanding the effects of ﬁnite depth
and width on gradients in randomly initialized networks. The main contributions of this work are:

1. We derive new exact formulas for the joint even moments of the entries of the
input-output Jacobian in a fully connected ReLU net with random weights and biases.
These formulas hold at ﬁnite depth and width (see Theorem 3).

2. We prove that the empirical variance of gradients in a fully connected ReLU net is
exponential in the sum of the reciprocals of the hidden layer widths. This suggests that
when this sum of reciprocals is too large  early training dynamics are very slow and it may
take many epochs to achieve better-than-chance performance (see Figure 1).

3. We prove that  so long as weights and biases are initialized independently with the correct
variance scaling (see Deﬁnition 1)  whether the EVGP occurs (in the precise sense
explained in §3) in fully connected ReLU nets is a function only of the architecture
and not the distributions from which the weights and biases are drawn.

Figure 1: Comparison of early training dynamics on vectorized MNIST for fully connected ReLU
nets with various architectures. Plot shows the mean number of epochs (over 100 independent training
runs) that a given architecture takes to reach 20% accuracy as a function of the sum of reciprocals of
hidden layer widths. (Figure reprinted with permission from [HR18] with caption modiﬁed).

1.1 Practical Implications

The second of the listed contributions has several concrete consequences for architecture selection and
for understanding initial training dynamics in ReLU nets. Speciﬁcally  our main results  Theorems
1-3  prove that the EVGP will occur in a ReLU net N (in either the annealed or the quenched sense
described in §3) if and only if a single scalar parameter  the sum

 =

1
nj

d1Xj=1

of reciprocals of the hidden layer widths of N   is large. Here nj denotes the width of the jth hidden
layer  and we prove in Theorem 1 that the variance of entries in the input-output Jacobian of N

2

1

0@ 1

d  1

d1Xj=1

1

nj1A



1
d  1

d1Xj=1

1/2

nj  0@ 1

d  1

d1Xj=1

n2

j1A

 

(2)

is exponential in . Implications for architecture selection then follow from special cases of the
power-mean inequality:

in which equality is achieved if and only if nj are all equal. We interpret the leftmost inequality

as follows. Fix d and a total budgetPj nj of hidden layer neurons. Theorems 1 and 2 say that to
avoid the EVGP in both the quenched and annealed senses  one should minimize  and hence make
the leftmost expression in (2) as large as possible. This occurs precisely when nj are all equal. Fix
instead d and a budget of trainable parameters Pj nj(nj1 + 1)  which is close toPj n2
j if the nj’s
don’t ﬂuctuate too much. Again using (2)  we ﬁnd that from the point of view of avoiding the EVGP 
it is advantageous to take the nj’s to be equal.
In short  our theoretical results (Theorems 1 and 2) show that if  is large then  at initialization  N
will compute gradients that ﬂuctuate wildly  intuitively leading to slow initial training dynamics. This
heuristic is corroborated by an experiment from [HR18] about the start of training on MNIST for
fully connected neural nets with varying depths and hidden layer widths (the parameter  appeared in
[HR18] in a different context). Figure 1 shows that  is a good summary statistic for predicting how
quickly deep networks will start to train.
We conclude the introduction by mentioning what we see as the principal weaknesses of the present
work. First  our analysis holds only for ReLU activations and assumes that all non-zero weights are
independent and zero centered. Therefore  our conclusions do not directly carry over to convolutional 
residual  and recurrent networks. Second  our results yield information about the ﬂuctuations of
the entries Zp q of the input-output Jacobian JN at any ﬁxed input to N . It would be interesting to
have information about the joint distribution of the Zp q’s with inputs ranging over an entire dataset.
Third  our techniques do not directly extend to initializations such as orthogonal weight matrices. We
hope to address these issues in the future and  speciﬁcally  believe that the qualitative results of this
article will generalize to convolutional networks in which the number of channels grows with the
layer number.

2 Relation to Prior Work

To provide some context for our results  we contrast both our approach and contributions with
the recent work [PSG17  PSG18]. These articles consider two senses in which a fully connected
neural net N with random weights and biases can avoid the EVGP. The ﬁrst is that the average
singular value of the input-output Jacobian JN remains approximately 1  while the second  termed
dynamical isometry  requires that all the singular values of JN are approximately 1. The authors
of [PSG17  PSG18] study the full distribution of the singular values of the Jacobian JN ﬁrst in the
inﬁnite width limit n ! 1 and then in the inﬁnite depth limit d ! 1.
Let us emphasize two particularly attractive features of [PSG17  PSG18]. First  neither the initializa-
tion nor the non-linearity in the neural nets N is assumed to be ﬁxed  allowing the authors to consider
solutions of types (ii) and (iii) above to the EVGP. The techniques used in these articles are also
rather general  and point to the emergence of universality classes for singular values of the Jacobian
of deep neural nets at initialization. Second  the results in these articles access the full distribution of
singular values for the Jacobian JN   providing signiﬁcantly more reﬁned information than simply
controlling the mean singular value.
The neural nets considered in [PSG17  PSG18] are essentially assumed to be inﬁnitely wide  however.
This raises the question of whether there is any ﬁnite width at which the behavior of a randomly
initialized network will resemble the inﬁnite width regime  and moreover  if such a width exists  how
wide is wide enough? In this work we give rigorous answers to such questions by quantifying ﬁnite
width effects  leaving aside questions about both different choices of non-linearity and about good
initializations that go beyond independent weights.
Instead of taking the singular value deﬁnition of the EVGP as in [PSG17  PSG18]  we propose
two non-spectral formulations of the EVGP  which we term annealed and quenched. Their precise
deﬁnitions are given in §3.2 and §3.3  and we provide in §3.1 a discussion of the relation between the
different senses in which the EVGP can occur.

3

Theorem 1 below implies  in the inﬁnite width limit  that all ReLU nets avoid the EVGP in both the
quenched and annealed sense. Hence  our deﬁnition of the EVGP (see §3.2 and §3.3) is weaker than
the dynamical isometry condition from [PSG17  PSG18]. But  as explained in §3.1  it is stronger the
condition that the average singular value equal 1. Both the quenched and annealed versions of the
EVGP concern the ﬂuctuations of the partial derivatives

Zp q :=

@ (fN )q
@ Act(0)
p

(3)

of the qth component of the function fN computed by N with respect to the pth component of its
input (Act(0) is an input vector - see (10)). The stronger  quenched version of the EVGP concerns
the empirical variance of the squares of all the different Zp q :

dVar⇥Z2⇤ :=

1
M

MXm=1

pm qm  1

M

Z4

MXm=1

pm qm!2

Z2

 

M = n0nd.

(4)

Here  n0 is the input dimension to N   nd is the output dimension  and the index m runs over all n0nd
possible input-output neuron pairs (pm  qm). Intuitively  since we will show in Theorem 1 that

E⇥Z2

p q⇤ = ⇥(1) 

independently of the depth  having a large mean fordVar⇥Z2⇤ means that for a typical realization of

the weights and biases in N   the derivatives of fN with respect to different trainable parameters will
vary over several orders of magnitude  leading to inefﬁcient SGD updates (1) for any ﬁxed learning
rate  (see §3.1 - §3.3).
To avoid the EVGP (in the annealed or quenched senses described below) in deep feed-forward
networks with ReLU activations  our results advise letting the widths of hidden layers grow as a
function of the depth. In fact  as the width of a given hidden layer tends to inﬁnity  the input to the
next hidden layer can viewed as a Gaussian process and can be understood using mean ﬁeld theory
(in which case one ﬁrst considers the inﬁnite width limit and only then the inﬁnite depth limit). This
point of view was taken in several interesting papers (e.g. [PLR+16  RPK+17  PSG17  PSG18] and
references therein)  which analyze the dynamics of signal propagation through such deep nets. In their
notation  the fan-in normalization (condition (ii) in Deﬁnition 1) guarantees that we’ve initialized
our neural nets at the edge of chaos (see e.g. around (7) in [PLR+16] and (5) in [RPK+17]). Indeed 
writing µ(j) for the weight distribution at layer j and using our normalization Var[µ(j)] = 2/nj1 
the order parameter 1 from [PLR+16  RPK+17] becomes

1 = nj1 · Var[µ(j)]ZR

ez2/20(pq⇤z)2 dz
p2⇡

= 1 

since  = ReLU  making 0(z) the indicator function 1[0 1)(z) and the value of 0(pq⇤z) indepen-
dent of the asymptotic length q⇤ for activations. The condition 1 = 1 deﬁnes the edge of chaos
regime. This gives a heuristic explanation for why the nets considered in the present article cannot
have just one of vanishing and exploding gradients. It also allows us to interpret our results as a
rigorous computation for ReLU nets of the 1/nj corrections at the edge of chaos.
In addition to the mean ﬁeld theory papers  we mention the article [SPSD17]. It does not deal
directly with gradients  but it does treat the ﬁnite width corrections to the statistical distribution of
pre-activations in a feed-forward network with Gaussian initialized weights and biases. A nice aspect
of this work is that the results give the joint distribution not only over all the neurons but also over
any number of inputs to the network. In a similar vein  we bring to the reader’s attention [BFL+17] 
which gives interesting heuristic computations about the structure of correlations between gradients
corresponding to different inputs in both fully connected and residual ReLU nets.

3 Deﬁning the EVGP for Feed-Forward Networks

We now explain in exactly what sense we study the EVGP and contrast our deﬁnition  which depends
on the behavior of the entries of the input-output Jacobian JN   with the more usual deﬁnition  which
depends on the behavior of its singular values (see §3.1). To do this  consider a feed-forward fully

4

connected depth d network N with hidden layer widths n0  . . .   nd  and ﬁx an input Act(0) 2 Rn0.
We denote by Act(j) the corresponding vector of activations at layer j (see (10)). The exploding and
vanishing gradient problem can be roughly stated as follows:

 !

Zp q has large ﬂuctuations 

Exploding/Vanishing Gradients

(5)
where Zp q the entries of the Jacobian JN (see (3)). A common way to formalize this statement is
to interpret “Zp q has large ﬂuctuations” to mean that the Jacobian JN of the function computed by
N has both very large and very small singular values [BSF94  HBF+01  PSG17]. We give in §3.1 a
brief account of the reasoning behind this formulation of the EVGP and explain why is also natural
to deﬁne the EVGP via the moments of Zp q. Then  in §3.2 and §3.3  we deﬁne two precise senses 
which we call annealed and quenched  in which that EVGP can occur  phrased directly in terms of
the joint moments of Zp q.

↵

0(Act(j)

 ) 

@L/@W (j)

3.1 Spectral vs. Entrywise Deﬁnitions of the EVGP
Let us recall the rationale behind using the spectral theory of JN to deﬁne the EVGP. The gradient in
(1) of the loss with respect to  say  a weight W (j)
↵  connecting neuron ↵ in layer j  1 to neuron  in
layer j is
(6)
 ) is the derivative of the non-linearity  the derivative of the loss L with respect to the

↵  = hrAct(d)L  JN  (j ! d)i Act(j1)
q   q = 1  . . .   nd⌘  

and we’ve denoted the th row in the layer j to output Jacobian JN (j ! d) by
   q = 1  . . .   nd⌘ .

rAct(d)L =⇣@L@ Act(d)
JN  (j ! d) =⇣@ Act(d)
q @ Act(j)

where 0(Act(j)
output Act(d) of N is

Since JN (j ! d) is the product of d  j layer-to-layer Jacobians  its inner product with rAct(d)L is
usually the term considered responsible for the EVGP. The worst case distortion it can achieve on the
vector rAct(d)L is captured precisely by its condition number  the ratio of its largest and smallest
singular values.
However  unlike the case of recurrent networks in which JN (j ! d) is (d  j)fold product of a
ﬁxed matrix  when the hidden layer widths grow with the depth d  the dimensions of the layer j to
layer j0 Jacobians JN (j ! j0) are not ﬁxed and it is not clear to what extent the vector rAct(d)L
will actually be stretched or compressed by the worst case bounds coming from estimates on the
condition number of JN (j ! d).
Moreover  on a practical level  the EVGP is about the numerical stability of the increments of the
SGD updates (1) over all weights (and biases) in the network  which is directly captured by the joint
distribution of the random variables

{|@L/@W (j)

↵ |2  j = 1  . . .   nd  ↵ = 1  . . .   nj1   = 1  . . .   nj}.

Due to the relation (6)  two terms inﬂuence the moments of |@L/@W (j)
↵ |2: one coming from the
activations at layer j  1 and the other from the entries of JN (j ! d). We focus in this article on
the second term and hence interpret the ﬂuctuations of the entries of JN (j ! d) as a measure of the
EVGP.
To conclude  we recall a simple relationship between the moments of the entries of the input-output
Jacobian JN and the distribution of its singular values  which can be used to directly compare spectral
and entrywise deﬁnitions of the EVGP. Suppose for instance one is interested in the average singular
value of JN (as in [PSG17  PSG18]). The sum of the singular values of JN is given by

tr(J T

N JN ) =

n0Xj=1⌦J T

N JN uj  uj↵ =

n0Xj=1

kJN ujk2  

where {uj} is any orthonormal basis. Hence  the average singular value can be obtained directly from
the joint even moments of the entries of JN . Both the quenched and annealed EVGP (see (7) (9))

5

entail that the average singular value for JN equals 1  and we prove in Theorem 1 (speciﬁcally (11))
that even at ﬁnite depth and width the average singular value for JN equals 1 for all the random
ReLU nets we consider!
One can push this line of reasoning further. Namely  the singular values of any matrix M are
determined by the Stieltjes transform of the empirical distribution M of the eigenvalues of M T M :

SM (z) =ZR

dM (x)
z  x

 

z 2 C\R.

Writing (z  x)1 as a power series in z shows that SJN is determined by traces of powers of J T
and hence by the joint even moments of the entries of JN . We hope to estimate SJN
future work.

N JN
(z) directly in

3.2 Annealed Exploding and Vanishing Gradients
Fix a sequence of positive integers n0  n1  . . . . For each d  1 write Nd for the depth d ReLU net
with hidden layer widths n0  . . .   nd and random weights and biases (see Deﬁnition 1 below). As in
(3)  write Zp q(d) for the partial derivative of the qth component of the output of Nd with respect to
pth component of its input. We say that the family of architectures given by {n0  n1  . . .} avoids the
exploding and vanishing gradient problem in the annealed sense if for each ﬁxed input to Nd and
every p  q we have

E⇥Z2

p q(d)⇤ = 1  Var[Z2

p q(d)] = ⇥(1) 

sup
d1

E⇥Z2K

p q (d)⇤ < 1  8K  3.

(7)

Here the expectation is over the weights and biases in Nd. Architectures that avoid the EVGP in the
annealed sense are ones where the typical magnitude of the partial derivatives Zp q(d) have bounded
(both above and below) ﬂuctuations around a constant mean value. This allows for a reliable a priori
selection of the learning rate  from (1) even for deep architectures. Our main result about the
annealed EVGP is Theorem 1: a family of neural net architectures avoids the EVGP in the annealed
sense if and only if

1
nj

< 1.

(8)

1Xj=1

We prove in Theorem 1 that E⇥Z2K

p q (d)⇤ is exponential inPjd 1/nj for every K.

3.3 Quenched Exploding and Vanishing Gradients
There is an important objection to deﬁning the EVGP as in the previous section. Namely  if a neural
net N suffers from the annealed EVGP  then it is impossible to choose an appropriate a priori learning
rate  that works for a typical initialization. However  it may still be that for a typical realization
of the weights and biases there is some choice of  (depending on the particular initialization)  that
works well for all (or most) trainable parameters in N . To study whether this is the case  we must
consider the variation of the Zp q’s across different p  q in a ﬁxed realization of weights and biases.
This is the essence of the quenched EVGP.
To formulate the precise deﬁnition  we again ﬁx a sequence of positive integers n0  n1  . . . and write
Nd for a depth d ReLU net with hidden layer widths n0  . . .   nd. We write as in (4)

dVar⇥Z(d)2⇤ :=

1
M

MXm=1

Zpm qm(d)4  1

M

Zpm qm(d)2!2

MXm=1

 

M = n0nd

for the empirical variance of the squares all the entries Zp q(d) of the input-output Jacobian of Nd.
We will say that the family of architectures given by {n0  n1  . . .} avoids the exploding and vanishing
gradient problem in the quenched sense if

(9)
Just as in the annealed case (7)  the expectation E [·] is with respect to the weights and biases
of N . In words  a neural net architecture suffers from the EVGP in the quenched sense if for a

EhdVar[Z(d)2]i = ⇥(1).

E⇥Zp q(d)2⇤ = 1

and

6

pm qm} is large.

typical realization of the weights and biases the empirical variance of the squared partial derivatives
{Z2
Our main result about the quenched sense of the EVGP is Theorem 2. It turns out  at least for the
ReLU nets we study  that a family of neural net architectures avoids the quenched EVGP if and only
if it also avoids the annealed exploding and vanishing gradient problem (i.e. if (8) holds).

4 Acknowledgements

I thank Leonid Hanin for a number of useful conversations and for his comments on an early draft. I
am also grateful to Jeffrey Pennington for pointing out an important typo in the proof of Theorem 3
and to David Rolnick for several helpful conversations and  speciﬁcally  for pointing out the relevance
of the power-mean inequality for understanding . Finally  I would like to thank several anonymous
referees for their help in improving the exposition. One referee in particular raised concerns about the
annealed and quenched deﬁnitions of the EVGP. Addressing these concerns resulted in the discussion
in §3.1.

5 Notation and Main Results

5.1 Deﬁnition of Random Networks
To formally state our results  we ﬁrst give the precise deﬁnition of the random networks we study.
For every d  1 and each n = (ni)d

+   write

The function fN computed by N 2 N(n  d) is determined by a collection of weights and biases

i=0 2 Zd+1
N(n  d) =nfully connected feed-forward nets with ReLU activations 
depth d  and whose jth hidden layer has width nj o .
{w(j)

1  ↵  nj  1    nj+1  j = 0  . . .   d  1}.

↵   b(j)
  
Speciﬁcally  given an input

Act(0) =⇣Act(0)
i ⌘n0
i=1 2 Rn0

to N   we deﬁne for every j = 1  . . .   d
Act(j1)

act(j)

 +

↵

Act(j)

w(j)
↵  

 = b(j)

1    nj.

 = (act(j)
 ) 

nj1X↵=1
The vectors act(j)  Act(j) therefore represent the vectors of inputs and outputs of the neurons in the
jth layer of N . The function computed by N takes the form
fN⇣Act(0)⌘ = fN⇣Act(0)  w(j)

 ⌘ = Act(d) .
A random network is obtained by randomizing weights and biases.
Deﬁnition 1 (Random Nets). Fix d  1  n = (n0  . . .   nd) 2 Zd+1
ity measures µ =µ(1)  . . .   µ(d) and ⌫ =⌫(1)  . . .   ⌫(d) on R such that

+   and two collections of probabil-

↵   b(j)

(10)

(i) µ(j)  ⌫(j) are symmetric around 0 for every 1  j  d.
(ii) the variance of µ(j) is 2/(nj1).
(iii) ⌫(j) has no atoms.

A random net N 2 Nµ ⌫ (n  d) is obtained by requiring that the weights and biases for neurons at
layer j are drawn independently from µ(j)  ⌫(j) :
w(j)
↵  ⇠ µ(j)  b(j)

Remark 1. Condition (iii) is used when we apply Lemma 1 in the proof of Theorem 3. It can be

 ⇠ ⌫(j)
j=1 nj⌘ . Since this yields slightly messier but not

removed under the restriction that d ⌧ exp⇣Pd

meaningfully different results  we do not pursue this point.

i.i.d.

7

5.2 Results
Our main theoretical results are Theorems 1 and 3. They concern the statistics of the slopes of the
functions computed by a random neural net in the sense of Deﬁnition 1. To state them compactly  we
deﬁne for any probability measure µ on R

eµ2K := RR x2Kdµ
RR x2dµK  
and  given a collection of probability measures {µ(j)}d
eµ2K max := max

2K.

1jdeµ(j)

K  0 

j=1 on R  set for any K  1

We also continue to write Zp q for the entries of the input-output Jacobian of a neural net (see (3)).
Theorem 1. Fix d  1 and a multi-index n = (n0  . . .   nd) 2 Zd+1
+ . Let N 2 Nµ ⌫ (n  d) be a
random network as in Deﬁnition 1. For any ﬁxed input to N   we have

d1Xj=1

1

nj1A .

(11)

(12)

(13)

.

1

1
n0

2
n2
0

p q⇤ =

exp0@ 1

E⇥Z2
In contrast  the fourth moment of Zp q(x) is exponential inPj
nj1A  E⇥Z4
p q⇤ 
6eµ4 max
j=1{nj}  then
exp0@CK µ
CK µ
nK
0

E⇥Z2K
p q⇤ 

d1Xj=1

n2
0

2

1
nj

:

exp0@6eµ4 max
nj1A .
d1Xj=1

1

Moreover  there exists a constant CK µ > 0 depending only on K and the ﬁrst 2K moments of the
measures {µ(j)}d

j=1 such that if K < mind1

the right hand side of (12) is not optimal and can be reduced by a more careful analysis along the
same lines as the proof of Theorem 1 given below. We do not pursue this here  however  since we are

Remark 2. In (11)  (12)  and (13)  the bias distributions ⌫(j) play no role. However  in the derivation
of these relations  we use in Lemma 1 that ⌫(j) has no atoms (see Remark 1). Also  the condition
K < mind
j=1{nj1} can be relaxed by allowing K to violate this inequality a ﬁxed ﬁnite number `
of times. This causes the constant CK µ to depend on ` as well.
We prove Theorem 1 in Appendix B. The constant factor multiplyingPj 1/nj in the exponent on
primarily interested in ﬁxing K and understanding the dependence of E⇥Zp q(x)2K⇤ on the widths
techniques will give analogous estimates for any mixed even moments E⇥Z2K1
pm q⇤ when K
is set toPm Km (see Remark 3). In particular  we can estimate the mean of the empirical variance
of gradients.
Theorem 2. Fix n0  . . .   nd 2 Z+  and let N be a random fully connected depth d ReLU net with
hidden layer widths n0  . . .   nd and random weights and biases as in Deﬁnition 1. Write M = n0nd
and writedVar⇥Z2⇤ for the empirical variance of the squares {Z2
pm qm} of all M input-output neuron

nj and the depth d. Although we’ve stated Theorem 1 only for the even moments of Zp q  the same

pairs as in (4). We have

p1 q ··· Z2Km

EhdVar[Z2]i  ✓1 

and

EhdVar[Z2]i 

1
n2

0✓1 

1

M◆✓1  ⌘ +

1

1

n2
0

M◆ 6eµ4 max
n1⇣eµ(1)

exp0@6eµ4 max
4  1⌘ e 1

nj1A
d1Xj=1
n1◆ exp0@ 1
d1Xj=1

4⌘

2

(14)

(15)

1

nj1A

8

where

⌘ :=

#{m1  m2 | m1 6= m2  qm1 = qm2}

=

M (M  1)

n0  1
n0nd  1

.

Hence  the family Nd of ReLU nets avoids the exploding and vanishing gradient problem in the
quenched sense if and only if

1
nj

< 1.

1Xj=1

We prove Theorem 2 in Appendix C. The results in Theorems 1 and 2 are based on exact expressions 

given in Theorem 3  for the even moments E⇥Zp q(x)2K⇤ in terms only of the moments of the
weight distributions µ(j). To give the formal statement  we introduce the following notation. For any
n = (ni)d
i=0 and any 1  p  n0  1  q  nd  we say that a path  from the pth input neuron to the
qth output neuron in N 2 N (n  d) is a sequence

{(j)}d

j=0 

1  (j)  nj 

(0) = p 

(d) = q 

so that (j) represents a neuron in the jth layer of N . Similarly  given any collection of K  1 paths
 = (k)K
k=1 that connect (possibly different) neurons in the input of N with neurons in its output
and any 1  j  d  denote by

(j) = [2

{(j)}

the neurons in the jth layer of N that belong to at least one element of . Finally  for every
↵ 2 (j  1) and  2 (j)  denote by

|↵ (j)| = #{ 2 | (j  1) = ↵  (j) = }

where the sum is over ordered tuples  = (1  . . .   2K) of paths in N from p to q and

the number of paths in  that pass through neuron ↵ at layer j  1 and through neuron  at layer j.
Theorem 3. Fix d  1 and n = (n0  . . .   nd) 2 Zd+1
+ . Let N 2 Nµ ⌫ (n  d) be a random network
as in Deﬁnition 1. For every K  1 and all 1  p  n0  1  q  nd  we have
dYj=1
p q⇤ =X
E⇥Z2K
2◆|(j)| Y↵2(j1)
Cj() =✓ 1
where for every r  0  the quantity µ(j)
Remark 3. The expression (16) can be generalized to case of mixed even moments. Namely  given
m  1 and for each 1  m  M integers Km  0 and 1  pm  n0 1  qm  nd  we have

r denotes the rth moment of the measure µ(j).

µ(j)
|↵  (j)|

 

Cj() 

2(j)

(16)

E" MYm=1

Zpm qm(x)2Km# =X

dYj=1

Cj() 

(17)

where now the sum is over collections  = (1  . . .   2K) of 2K = Pm 2Km paths in N with

exactly 2Km paths from pm to qm. The proof is identical up to the addition of several well-placed
subscripts.

See Appendix A for the proof of Theorem 3.

References
[ASB16] Martin Arjovsky  Amar Shah  and Yoshua Bengio. Unitary evolution recurrent neural
networks. In International Conference on Machine Learning  pages 1120–1128  2016.

9

[BFL+17] David Balduzzi  Marcus Frean  Lennox Leary  JP Lewis  Kurt Wan-Duo Ma  and Brian
McWilliams. The shattered gradients problem: If resnets are the answer  then what is
the question? arXiv preprint arXiv:1702.08591  2017.
Yoshua Bengio  Patrice Simard  and Paolo Frasconi. Learning long-term dependencies
with gradient descent is difﬁcult. IEEE transactions on neural networks  5(2):157–166 
1994.

[BSF94]

[CHM+15] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann
LeCun. The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics 
pages 192–204  2015.

[HBF+01] Sepp Hochreiter  Yoshua Bengio  Paolo Frasconi  Jürgen Schmidhuber  et al. Gradient

[Hoc91]

[HR18]

[HS97]

ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies  2001.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma  Tech-
nische Universität München  91  1991.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and
architecture. In Advances in Neural Information Processing Systems 32  2018.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computa-
tion  9(8):1735–1780  1997.

[HSL16] Mikael Henaff  Arthur Szlam  and Yann LeCun. Recurrent orthogonal networks and
long-memory tasks. In Proceedings of The 33rd International Conference on Machine
Learning  volume 48  pages 2034–2042  2016.

[HZRS15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the
IEEE international conference on computer vision  pages 1026–1034  2015.

[HZRS16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 770–778  2016.

[KUMH17] Günter Klambauer  Thomas Unterthiner  Andreas Mayr  and Sepp Hochreiter. Self-
normalizing neural networks. In Advances in Neural Information Processing Systems 
pages 972–981  2017.
Dmytro Mishkin and Jiri Matas. All you need is a good init.
arXiv:1511.06422  2015.

arXiv preprint

[MM15]

[PSG17]

[PLR+16] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In Advances
in neural information processing systems  pages 3360–3368  2016.
Jeffrey Pennington  Samuel Schoenholz  and Surya Ganguli. Resurrecting the sigmoid
in deep learning through dynamical isometry: theory and practice. In Advances in
neural information processing systems  pages 4788–4798  2017.
Jeffrey Pennington  Samuel S Schoenholz  and Surya Ganguli. The emergence of
spectral universality in deep networks. arXiv preprint arXiv:1802.09979  2018.

[PSG18]

[Sch]

[SGS15]

[RPK+17] Maithra Raghu  Ben Poole  Jon M. Kleinberg  Surya Ganguli  and Jascha Sohl-Dickstein.
On the expressive power of deep neural networks. In Proceedings of the 34th Interna-
tional Conference on Machine Learning  ICML 2017  pages 2847–2854  2017.
Sepp hochreiter’s fundamental deep learning problem (1991). http://people.idsia.
ch/~juergen/fundamentaldeeplearningproblem.html. Accessed: 2017-12-26.
Rupesh Kumar Srivastava  Klaus Greff  and Jürgen Schmidhuber. Highway networks.
2015.
Samuel S Schoenholz  Jeffrey Pennington  and Jascha Sohl-Dickstein. A correspon-
dence between random neural networks and statistical ﬁeld theory. arXiv preprint
arXiv:1710.06570  2017.
Di Xie  Jiang Xiong  and Shiliang Pu. All you need is beyond a good init: Explor-
ing better solution for training extremely deep convolutional neural networks with
orthonormality and modulation. arXiv preprint arXiv:1703.01827  2017.

[SPSD17]

[XXP17]

10

,Boris Hanin