2019,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods,We investigate time-dependent data analysis from the perspective of recurrent kernel machines  from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell  a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to $n$-gram filters  the convolutional neural network (CNN)  Gated CNN  and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM  while also extending it to $n$-gram convolutional filters. Experiments are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application  the new models demonstrate significant improvements relative to the prior state of the art.,Kernel-Based Approaches for Sequence Modeling:

Connections to Neural Methods

Kevin J Liang∗ Guoyin Wang∗ Yitong Li Ricardo Henao Lawrence Carin

Department of Electrical and Computer Engineering

Duke University

{kevin.liang  guoyin.wang  yitong.li  ricardo.henao  lcarin}@duke.edu

Abstract

We investigate time-dependent data analysis from the perspective of recurrent
kernel machines  from which models with hidden units and gated memory cells
arise naturally. By considering dynamic gating of the memory cell  a model closely
related to the long short-term memory (LSTM) recurrent neural network is derived.
Extending this setup to n-gram ﬁlters  the convolutional neural network (CNN) 
Gated CNN  and recurrent additive network (RAN) are also recovered as special
cases. Our analysis provides a new perspective on the LSTM  while also extending
it to n-gram convolutional ﬁlters. Experiments1 are performed on natural language
processing tasks and on analysis of local ﬁeld potentials (neuroscience). We
demonstrate that the variants we derive from kernels perform on par or even better
than traditional neural methods. For the neuroscience application  the new models
demonstrate signiﬁcant improvements relative to the prior state of the art.

Introduction

1
There has been signiﬁcant recent effort directed at connecting deep learning to kernel machines
[1  5  23  36]. Speciﬁcally  it has been recognized that a deep neural network may be viewed as
constituting a feature mapping x → ϕθ(x)  for input data x ∈ Rm. The nonlinear function ϕθ(x) 
with model parameters θ  has an output that corresponds to a d-dimensional feature vector; ϕθ(x)
may be viewed as a mapping of x to a Hilbert space H  where H ⊂ Rd. The ﬁnal layer of deep
ϕθ(x)  with weight vector ω ∈ H; for
neural networks typically corresponds to an inner product ω
(cid:124)
a vector output  there are multiple ω  with ω
i ϕθ(x) deﬁning the i-th component of the output. For
example  in a deep convolutional neural network (CNN) [19]  ϕθ(x) is a function deﬁned by the
multiple convolutional layers  the output of which is a d-dimensional feature map; ω represents the
fully-connected layer that imposes inner products on the feature map. Learning ω and θ  i.e.  the
cumulative neural network parameters  may be interpreted as learning within a reproducing kernel
Hilbert space (RKHS) [4]  with ω the function in H; ϕθ(x) represents the mapping from the space of
the input x to H  with associated kernel kθ(x  x(cid:48)) = ϕθ(x)
Insights garnered about neural networks from the perspective of kernel machines provide valuable
theoretical underpinnings  helping to explain why such models work well in practice. As an example 
the RKHS perspective helps explain invariance and stability of deep models  as a consequence of
the smoothness properties of an appropriate RKHS to variations in the input x [5  23]. Further  such
insights provide the opportunity for the development of new models.
Most prior research on connecting neural networks to kernel machines has assumed a single input x 
e.g.  image analysis in the context of a CNN [1  5  23]. However  the recurrent neural network (RNN)
has also received renewed interest for analysis of sequential data. For example  long short-term

ϕθ(x(cid:48))  where x(cid:48) is another input.

(cid:124)

(cid:124)

∗These authors contributed equally to this work.
1Implementations can be found at https://github.com/kevinjliang/kernels2rnns.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

memory (LSTM) [15  13] and the gated recurrent unit (GRU) [9] have become fundamental elements
in many natural language processing (NLP) pipelines [16  9  12]. In this context  a sequence of
data vectors (. . .   xt−1  xt  xt+1  . . . ) is analyzed  and the aforementioned single-input models are
inappropriate.
In this paper  we extend to recurrent neural networks (RNNs) the concept of analyzing neural
networks from the perspective of kernel machines. Leveraging recent work on recurrent kernel
machines (RKMs) for sequential data [14]  we make new connections between RKMs and RNNs 
showing how RNNs may be constructed in terms of recurrent kernel machines  using simple ﬁlters.
We demonstrate that these recurrent kernel machines are composed of a memory cell that is updated
sequentially as new data come in  as well as in terms of a (distinct) hidden unit. A recurrent model
that employs a memory cell and a hidden unit evokes ideas from the LSTM. However  within the
recurrent kernel machine representation of a basic RNN  the rate at which memory fades with time
is ﬁxed. To impose adaptivity within the recurrent kernel machine  we introduce adaptive gating
elements on the updated and prior components of the memory cell  and we also impose a gating
network on the output of the model. We demonstrate that the result of this reﬁnement of the recurrent
kernel machine is a model closely related to the LSTM  providing new insights on the LSTM and its
connection to kernel machines.
Continuing with this framework  we also introduce new concepts to models of the LSTM type. The
reﬁned LSTM framework may be viewed as convolving learned ﬁlters across the input sequence and
using the convolutional output to constitute the time-dependent memory cell. Multiple ﬁlters  possibly
of different temporal lengths  can be utilized  like in the CNN. One recovers the CNN [18  37  17]
and Gated CNN [10] models of sequential data as special cases  by turning off elements of the new
LSTM setup. From another perspective  we demonstrate that the new LSTM-like model may be
viewed as introducing gated memory cells and feedback to a CNN model of sequential data.
In addition to developing the aforementioned models for sequential data  we demonstrate them in an
extensive set of experiments  focusing on applications in natural language processing (NLP) and in
analysis of multi-channel  time-dependent local ﬁeld potential (LFP) recordings from mouse brains.
Concerning the latter  we demonstrate marked improvements in performance of the proposed methods
relative to recently-developed alternative approaches [22].

2 Recurrent Kernel Network
Consider a sequence of vectors (. . .   xt−1  xt  xt+1  . . . )  with xt ∈ Rm. For a language model  xt
is the embedding vector for the t-th word wt in a sequence of words. To model this sequence  we
introduce yt = U ht  with the recurrent hidden variable satisfying

ht = f (W (x)xt + W (h)ht−1 + b)

(1)
where ht ∈ Rd  U ∈ RV ×d  W (x) ∈ Rd×m  W (h) ∈ Rd×d  and b ∈ Rd. In the context of a language
model  the vector yt ∈ RV may be fed into a nonlinear function to predict the next word wt+1 in the
sequence. Speciﬁcally  the probability that wt+1 corresponds to i ∈ {1  . . .   V } in a vocabulary of V
words is deﬁned by element i of vector Softmax(yt + β)  with bias β ∈ RV . In classiﬁcation  such
as the LFP-analysis example in Section 6  V is the number of classes under consideration.
We constitute the factorization U = AE  where A ∈ RV ×j and E ∈ Rj×d  often with j (cid:28) V .
Hence  we may write yt = Ah(cid:48)
t = Eht; the columns of A may be viewed as time-invariant
factor loadings  and h(cid:48)
t represents a vector of dynamic factor scores. Let zt = [xt  ht−1] represent a
column vector corresponding to the concatenation of xt and ht−1; then ht = f (W (z)zt + b) where
W (z) = [W (x)  W (h)] ∈ Rd×(d+m). Computation of Eht corresponds to inner products of the
rows of E with the vector ht. Let ei ∈ Rd be a column vector  with elements corresponding to row
i ∈ {1  . . .   j} of E. Then component i of h(cid:48)
(cid:124)
i ht = e

(2)
We view f (W (z)zt + b) as mapping zt into a RKHS H  and vector ei is also assumed to reside within
H. We consequently assume

t  with h(cid:48)

t is

h(cid:48)
i t = e

(cid:124)
i f (W (z)zt + b)

ei = f (W (z) ˜zi + b)

(3)

2

(a)

(b)

(c)

Figure 1: a) A traditional recurrent neural network (RNN)  with the factorization U = AE. b) A
recurrent kernel machine (RKM)  with an implicit hidden state and recurrence through recursion. c)
The recurrent kernel machine expressed in terms of a memory cell.

(cid:124)
i xt + qθ[˜x

(cid:124)
i xt−1 + qθ[˜x

(cid:124)
i xt−2 + qθ[˜x

(cid:124)
(cid:124)
i xt−3 + ˜h
−4ht−4]]]]

(5)

e

(cid:124)

where ˜zi = [˜xi  ˜h0]. Note that here ˜h0 also depends on index i  which we omit for simplicity; as
discussed below  ˜xi will play the primary role when performing computations.

(cid:124)
i ht = e

(cid:124)
i f (W (z)zt + b) = f (W (z) ˜zi + b)

f (W (z)zt + b) = kθ(˜zi  zt)

(4)
(cid:124)
where kθ(˜zi  zt) = h(˜zi)
h(zt) is a Mercer kernel [29]. Particular kernel choices correspond to
different functions f (W (z)zt + b)  and θ is meant to represent kernel parameters that may be adjusted.
(cid:124)
1 ht 2 where qθ(·) is a function of
(cid:124)
zt) = ˜h
We initially focus on kernels of the form kθ(˜z  zt) = qθ(˜z
parameters θ  ht = h(zt)  and ˜h1 is the implicit latent vector associated with the inner product  i.e. 
˜h1 = f (W (x) ˜x + W (h)˜h0 + b). As discussed below  we will not need to explicitly evaluate ht or ˜h1
to evaluate the kernel  taking advantage of the recursive relationship in (1). In fact  depending on
the choice of qθ(·)  the hidden vectors may even be inﬁnite-dimensional. However  because of the
(cid:124)
1 ht  for rigorous analysis qθ(·) should satisfy Mercer’s condition [11  29].
(cid:124)
zt) = ˜h
relationship qθ(˜z
The vectors (˜h1  ˜h0  ˜h−1  . . . ) are assumed to satisfy the same recurrence setup as (1)  with each
vector in the associated sequence (˜xt  ˜xt−1  . . . ) assumed to be the same ˜xi at each time  i.e. 
associated with ei  (˜xt  ˜xt−1  . . . ) → (˜xi  ˜xi  . . . ). Stepping backwards in time three steps  for
example  one may show

kθ(˜zi  zt) = qθ[˜x

(cid:124)
The inner product ˜h
−4ht−4 encapsulates contributions for all times further backwards  and for a
(cid:124)
sequence of length N  ˜h
−N ht−N plays a role analogous to a bias. As discussed below  for stability
the repeated application of qθ(·) yields diminishing (fading) contributions from terms earlier in time 
(cid:124)
and therefore for large N the impact of ˜h
−N ht−N on kθ(˜zi  zt) is small.
The overall model may be expressed as

h(cid:48)
t = qθ(ct)  

ct = ˜ct + qθ(ct−1)   ˜ct = ˜Xxt

(6)
(cid:124)
where ct ∈ Rj is a memory cell at time t  row i of ˜X corresponds to ˜x
i   and qθ(ct) operates pointwise
on the components of ct (see Figure 1). At the start of the sequence of length N  qθ(ct−N ) may be
(cid:124)
seen as a vector of biases  effectively corresponding to ˜h
N ht−N ; we henceforth omit discussion of
this initial bias for notational simplicity  and because for sufﬁciently large N its impact on h(cid:48)
t is small.
Note that via the recursive process by which ct is evaluated in (6)  the kernel evaluations reﬂected
by qθ(ct) are deﬁned entirely by the elements of the sequence (˜ct  ˜ct−1  ˜ct−2  . . . ). Let ˜ci t repre-
sent the i-th component in vector ˜ct  and deﬁne x≤t = (xt  xt−1  xt−2  . . . ). Then the sequence
(˜ci t  ˜ci t−1  ˜ci t−2  . . . ) is speciﬁed by convolving in time ˜xi with x≤t  denoted ˜xi ∗ x≤t. Hence  the
j components of the sequence (˜ct  ˜ct−1  ˜ct−2  . . . ) are completely speciﬁed by convolving x≤t with
each of the j ﬁlters  ˜xi  i ∈ {1  . . .   j}  i.e.  taking an inner product of ˜xi with the vector in x≤t at
each time point.
In (4) we represented h(cid:48)
i t = kθ(˜zi  zt); now  because of the recursive form of the
(cid:124)
i zt)  we have demonstrated that we
model in (1)  and because of the assumption kθ(˜zi  zt) = qθ(˜z
2) [14]  as for a Gaussian kernel 
zt).

(cid:124)
(cid:124)
i ˜xi = 1)  then qθ((cid:107)˜z − zt(cid:107)2
but if vectors xt and ﬁlters ˜xi are normalized (e.g.  x
t xt = ˜x

2One may also design recurrent kernels of the form kθ(˜z  zt) = qθ((cid:107)˜z − zt(cid:107)2

i t = qθ(ci t) as h(cid:48)

(cid:124)
2) reduces to qθ(˜z

3

may express the kernel equivalently as kθ(˜xi ∗ x≤t)  to underscore that it is deﬁned entirely by the
elements at the output of the convolution ˜xi ∗ x≤t. Hence  we may express component i of h(cid:48)
t as
i t = kθ(˜xi ∗ x≤t).
h(cid:48)
Component l ∈ {1  . . .   V } of yt = Ah(cid:48)

t may be expressed

yl t =

Al ikθ(˜xi ∗ x≤t)

(7)

j(cid:88)

i=1

where Al i represents component (l  i) of matrix A. Considering (7)  the connection of an RNN to an
RKHS is clear  as made explicit by the kernel kθ(˜xi ∗ x≤t). The RKHS is manifested for the ﬁnal
output yt  with the hidden ht now absorbed within the kernel  via the inner product (4). The feedback
imposed via latent vector ht is constituted via update of the memory cell ct = ˜ct + qθ(ct−1) used to
evaluate the kernel.
Rather than evaluating yt as in (7)  it will prove convenient to return to (6). Speciﬁcally  we may
consider modifying (6) by injecting further feedback via h(cid:48)

t  augmenting (6) as

h(cid:48)
t = qθ(ct)  

ct = ˜ct + qθ(ct−1)   ˜ct = ˜Xxt + ˜Hh(cid:48)

t−1

(8)

where ˜H ∈ Rj×j  and recalling yt = Ah(cid:48)
t (see Figure 2a for illustration). In (8) the input to the
kernel is dependent on the input elements (xt  xt−1  . . . ) and is now also a function of the kernel
outputs at the previous time  via h(cid:48)
t is still speciﬁed entirely by the elements
of ˜xi ∗ x≤t  for i ∈ {1  . . .   j}.

t−1. However  note that h(cid:48)

3 Choice of Recurrent Kernels & Introduction of Gating Networks
3.1 Fixed kernel parameters & time-invariant memory-cell gating
The function qθ(·) discussed above may take several forms  the simplest of which is a linear kernel 
with which (8) takes the form

(9)

h(cid:48)
t = ct  

ct = σ2

i ˜ct + σ2

i and σ2
i and σ2

f ct−1   ˜ct = ˜Xxt + ˜Hh(cid:48)
t−1
f < 1 for stability. The
f (using analogous notation from [14]) are scalars  with σ2
where σ2
f may be viewed as static (i.e.  time-invariant) gating elements  with σ2
i controlling
scalars σ2
f controlling how much of the prior
weighting on the new input element to the memory cell  and σ2
memory unit is retained; given σ2
f < 1  this means information from previous time steps tends to
fade away and over time is largely forgotten. However  such a kernel leads to time-invariant decay
of memory: the contribution ˜ct−N from N steps before to the current memory ct is (σiσN
f )2˜ct−N  
meaning that it decays at a constant exponential rate. Because the information contained at each time
step can vary  this can be problematic. This suggests augmenting the model  with time-varying gating
weights  with memory-component dependence on the weights  which we consider below.
3.2 Dynamic gating networks & LSTM-like model
Recent work has shown that dynamic gating can be seen as making a recurrent network quasi-invariant
to temporal warpings [30]. Motivated by the form of the model in (9) then  it is natural to impose
dynamic versions of σ2
f ; we also introduce dynamic gating at the output of the hidden vector.
This yields the model:
t = ot (cid:12) ct  
h(cid:48)
t + bo)  

(10)
(11)
t−1]  and Wc encapsulates ˜X and ˜H. In (10)-(11) the symbol (cid:12) represents a
where z(cid:48)
pointwise vector product (Hadamard); Wc  Wo  Wη and Wf are weight matrices; bo  bη and bf are
bias vectors; and σ(α) = 1/(1 + exp(−α)). In (10)  ηt and ft play dynamic counterparts to σ2
i and
f   respectively. Further  ot  ηt and ft are vectors  constituting vector-component-dependent gating.
σ2
Note that starting from a recurrent kernel machine  we have thus derived a model closely resembling
the LSTM. We call this model RKM-LSTM (see Figure 2).
Concerning the update of the hidden state  h(cid:48)
hyperbolic-tangent tanh nonlinearity: h(cid:48)

t = ot (cid:12) ct in (10)  one may also consider appending a
t = ot (cid:12) tanh(ct). However  recent research has suggested

ct = ηt (cid:12) ˜ct + ft (cid:12) ct−1  

˜ct = Wcz(cid:48)
t
t + bf )

ηt = σ(Wηz(cid:48)

t + bη)  

ft = σ(Wf z(cid:48)

i and σ2

ot = σ(Woz(cid:48)

t = [xt  h(cid:48)

4

(a)

(b)

Figure 2: a) Recurrent kernel machine  with feedback  as deﬁned in (8). b) Making a linear kernel
assumption and adding input  forget  and output gating  this model becomes the RKM-LSTM.

not using such a nonlinearity [20  10  7]  and this is a natural consequence of our recurrent kernel
t = ot (cid:12) tanh(ct)  the model in (10) and (11) is in the form of the LSTM  except
analysis. Using h(cid:48)
without the nonlinearity imposed on the memory cell ˜ct  while in the LSTM a tanh nonlinearity (and
biases) is employed when updating the memory cell [15  13]  i.e.  for the LSTM ˜ct = tanh(Wcz(cid:48)
t +
bc). If ot = 1 for all time t (no output gating network)  and if ˜ct = Wcxt (no dependence on h(cid:48)
t−1
for update of the memory cell)  this model reduces to the recurrent additive network (RAN) [20].
While separate gates ηt and ft were constituted in (10) and (11) to operate on the new and prior
composition of the memory cell  one may also also consider a simpler model with memory cell
updated ct = (1− ft)(cid:12) ˜ct + ft(cid:12) ct−1; this was referred to as having a Coupled Input and Forget Gate
(CIFG) in [13]. In such a model  the decisions of what to add to the memory cell and what to forget
are made jointly  obviating the need for a separate input gate ηt. We call this variant RKM-CIFG.
4 Extending the Filter Length
4.1 Generalized form of recurrent model
Consider a generalization of (1):

ht = f (W (x0)xt + W (x−1)xt−1 + ··· + W (x−n+1)xt−n+1 + W (h)ht−1 + b)

(12)
where W (x·) ∈ Rd×m  W (h) ∈ Rd×d  and therefore the update of the hidden state ht
3 depends on
data observed n ≥ 1 time steps prior  and also on the previous hidden state ht−1. Analogous to (3) 
we may express

i t = e

(cid:124)
i ht.

ei = f (W (x0) ˜xi 0 + W (x−1) ˜xi −1 + ··· + W (x−n+1) ˜xi −n+1 + W (h)˜hi + b)

(13)
The inner product f (W (x0)xt + W (x−1)xt−1 + ··· + W (x−n+1)xt−n+1 + W (h)ht−1 +
f (W (x0) ˜xi 0 + W (x−1) ˜xi −1 + ··· + W (x−n+1) ˜xi −n+1 + W (h)˜hi + b) is assumed represented
(cid:124)
b)
by a Mercer kernel  and h(cid:48)
Let Xt = (xt  xt−1  . . .   xt−n+1) ∈ Rm×n be an n-gram input with zero padding if t < (n − 1) 
and ˜X = ( ˜X0  ˜X−1  . . .   ˜X−n+1) be n sets of ﬁlters  with the i-th rows of ˜X0  ˜X−1  . . .   ˜X−n+1
collectively represent the i-th n-gram ﬁlter  with i ∈ {1  . . .   j}. Extending Section 2  the kernel is
deﬁned
(14)
where ˜X · Xt ≡ ˜X0xt + ˜X−1xt−1 + ··· + ˜X−n+1xt−n+1 ∈ Rj. Note that ˜X · Xt corresponds
to the t-th component output from the n-gram convolution of the ﬁlters ˜X and the input sequence;
t = kθ( ˜X ∗ x≤t)  emphasizing that
therefore  similar to Section 2  we represent h(cid:48)
the kernel evaluation is a function of outputs of the convolution ˜X ∗ x≤t  here with n-gram ﬁlters.
Like in the CNN [18  37  17]  different ﬁlter lengths (and kernels) may be considered to constitute
different components of the memory cell.
4.2 Linear kernel  CNN and Gated CNN
For the linear kernel discussed in connection to (9)  equation (14) becomes

t = qθ(ct) as h(cid:48)

h(cid:48)
t = qθ(ct)  

ct = ˜ct + qθ(ct−1)  

˜ct = ˜X · Xt

h(cid:48)
t = ct = σ2

i ( ˜X · Xt) + σ2
f h(cid:48)

t−1

(15)

3Note that while the same symbol is used as in (12)  ht clearly takes on a different meaning when n > 1.

5

ηt = σ( ˜X η · Xt + bη)

t = ηt (cid:12) ( ˜X · Xt)  
h(cid:48)

f = 0 and σ2

For the special case of σ2
i = 1)  (15) reduces to a
convolutional neural network (CNN)  with a nonlinear operation typically applied subsequently to h(cid:48)
t.
i to a constant  one may impose dynamic gating  yielding the model (with
Rather than setting σ2
f = 0)
σ2

i equal to a constant (e.g.  σ2

(16)
where ˜X η are distinct convolutional ﬁlters for calculating ηt  and bη is a vector of biases. The form
of the model in (16) corresponds to the Gated CNN [10]  which we see as a a special case of the
recurrent model with linear kernel  and dynamic kernel weights (and without feedback  i.e.  σ2
f = 0).
Note that in (16) a nonlinear function is not imposed on the output of the convolution ˜X · Xt  there
is only dynamic gating via multiplication with ηt; the advantages of which are discussed in [10].
Further  the n-gram input considered in (12) need not be consecutive. If spacings between inputs of
more than 1 are considered  then the dilated convolution (e.g.  as used in [31]) is recovered.
4.3 Feedback and the generalized LSTM
Now introducing feedback into the memory cell  the model in (8) is extended to

ct = ˜ct + qθ(ct−1)   ˜ct = ˜X · Xt + ˜Hh(cid:48)

t−1

h(cid:48)
t = qθ(ct)  

(17)

Again motivated by the linear kernel  generalization of (17) to include gating networks is

t−1

t = ot (cid:12) ct  
h(cid:48)

ct = ηt (cid:12) ˜ct + ft (cid:12) ct−1   ˜ct = ˜X · Xt + ˜Hh(cid:48)

t−1 +bo)  ηt = σ( ˜Xη·Xt + ˜Wηh(cid:48)

t−1 +bη)  ft = σ( ˜Xf ·Xt + ˜Wf h(cid:48)

(18)
t−1 +bf )
(19)
t and ˜Xo  ˜Xη  and ˜Xf are separate sets of n-gram convolutional ﬁlters akin to ˜X.

ot = σ( ˜Xo·Xt + ˜Woh(cid:48)
where yt = Ah(cid:48)
As an n-gram generalization of (10)-(11)  we refer to (18)-(19) as an n-gram RKM-LSTM.
The model in (18) and (19) is similar to the LSTM  with important differences: (i) there is not a
nonlinearity imposed on the update to the memory cell  ˜ct  and therefore there are also no biases
imposed on this cell update; (ii) there is no nonlinearity on the output; and (iii) via the convolutions
with ˜X  ˜Xo  ˜Xη  and ˜Xf   the memory cell can take into account n-grams  and the length of such
sequences ni may vary as a function of the element of the memory cell.
5 Related Work
In our development of the kernel perspective of the RNN  we have emphasized that the form of the
(cid:124)
kernel kθ(˜zi  zt) = qθ(˜z
i zt) yields a recursive means of kernel evaluation that is only a function of
the elements at the output of the convolutions ˜X ∗ x≤t or ˜X ∗ x≤t  for 1-gram and (n > 1)-gram
ﬁlters  respectively. This underscores that at the heart of such models  one performs convolutions
between the sequence of data (. . .   xt+1  xt  xt−1  . . . ) and ﬁlters ˜X or ˜X. Consideration of ﬁlters
of length greater than one (in time) yields a generalization of the traditional LSTM. The dependence
of such models entirely on convolutions of the data sequence and ﬁlters is evocative of CNN and
Gated CNN models for text [18  37  17  10]  with this made explicit in Section 4.2 as a special case.
The Gated CNN in (16) and the generalized LSTM in (18)-(19) both employ dynamic gating.
However  the generalized LSTM explicitly employs a memory cell (and feedback)  and hence offers
the potential to leverage long-term memory. While memory affords advantages  a noted limitation
of the LSTM is that computation of h(cid:48)
t is sequential  undermining parallel computation  particularly
while training [10  33]. In the Gated CNN  h(cid:48)
t comes directly from the output of the gated convolution 
allowing parallel ﬁtting of the model to time-dependent data. While the Gated CNN does not employ
recurrence  the ﬁlters of length n > 1 do leverage extended temporal dependence. Further  via deep
Gated CNNs [10]  the effective support of the ﬁlters at deeper layers can be expansive.
(cid:124)
Recurrent kernels of the form kθ(˜z  zt) = qθ(˜z
zt) were also developed in [14]  but with the goal of
extending recurrent kernel machines to sequential inputs  rather than making connections with RNNs.
The formulation in Section 2 has two important differences with that prior work. First  we employ
(cid:124)
the same vector ˜xi for all shift positions t of the inner product ˜x
i xt. By contrast  in [14] effectively
inﬁnite-dimensional ﬁlters are used  because the ﬁlter ˜xt i changes with t. This makes implementation
computationally impractical  necessitating truncation of the long temporal ﬁlter. Additionally  the
feedback of h(cid:48)
t in (8) was not considered  and as discussed in Section 3.2  our proposed setup yields
natural connections to long short-term memory (LSTM) [15  13].

6

Parameters

Cell

Output

(nm + d)(4d)
(nm + d)(4d)
(nm + d)(3d)
(nm + d)(2d)
(nm + d)(d)

Model
LSTM [15]
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot
Linear Kernel
Gated CNN [10]
CNN [18]
Table 1: Model variants under consideration  assuming 1-gram inputs. Concatenating additional
inputs xt−1  . . .   xt−n+1 to z(cid:48)
t in the Input column yields the corresponding n-gram model. Number
of model parameters are shown for input xt ∈ Rm and output h(cid:48)

t = ot (cid:12) tanh(ct)
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
h(cid:48)
t = tanh(ct)
t = ot (cid:12) ct
h(cid:48)
h(cid:48)
t = tanh(ct)

Input
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = xt
z(cid:48)
t = xt

ct = ηt (cid:12) tanh(˜ct) + ft (cid:12) ct−1
ct = (1 − ft) (cid:12) ˜ct + ft (cid:12) ct−1

ct = ηt (cid:12) ˜ct + ft (cid:12) ct−1

i ˜ct + σ2
i ˜ct + σ2
ct = σ2
i ˜ct
ct = σ2
i ˜ct

t−1]
t−1]
t−1]
t−1]
t−1]

ct = σ2
ct = σ2

f ct−1
f ct−1

(nm)(2d)
(nm)(d)

t ∈ Rd.

Prior work analyzing neural networks from an RKHS perspective has largely been based on the
feature mapping ϕθ(x) and the weight ω [1  5  23  36]. For the recurrent model of interest here 
function ht = f (W (x)xt + W (h)ht−1 + b) plays a role like ϕθ(x) as a mapping of an input xt to
what may be viewed as a feature vector ht. However  because of the recurrence  ht is a function of
(xt  xt−1  . . . ) for an arbitrarily long time period prior to time t:
ht(xt  xt−1  . . . ) = f (W (x)xt + b + W (h)f (W (x)xt−1 + b + W (h)f (W (x)xt−2 + b + . . . ))) (20)
However  rather than explicitly working with ht(xt  xt−1  . . . )  we focus on the kernel kθ(˜zi  zt) =
(cid:124)
i zt) = kθ(˜xi ∗ x≤t).
qθ(˜z
The authors of [21] derive recurrent neural networks from a string kernel by replacing the exact
matching function with an inner product and assume the decay factor to be a nonlinear function.
Convolutional neural networks are recovered by replacing a pointwise multiplication with addition.
However  the formulation cannot recover the standard LSTM formulation  nor is there a consistent
formulation for all the gates. The authors of [28] introduce a kernel-based update rule to approximate
backpropagation through time (BPTT) for RNN training  but still follow the standard RNN structure.
Previous works have considered recurrent models with n-gram inputs as in (12). For example 
strongly-typed RNNs [3] consider bigram inputs  but the previous input xt−1 is used as a replacement
for ht−1 rather than in conjunction  as in our formulation. Quasi-RNNs [6] are similar to [3]  but
generalize them with a convolutional ﬁlter for the input and use different nonlinearities. Inputs
corresponding to n-grams have also been implicitly considered by models that use convolutional
layers to extract features from n-grams that are then fed into a recurrent network (e.g.  [8  35  38]).
Relative to (18)  these models contain an extra nonlinearity f (·) from the convolution and projection
matrix W (x) from the recurrent cell  and no longer recover the CNN [18  37  17] or Gated CNN [10]
as special cases.
6 Experiments
In the following experiments  we consider several model variants  with nomenclature as follows.
The n-gram LSTM developed in Sec. 4.3 is a generalization of the standard LSTM [15] (for which
n = 1). We denote RKM-LSTM (recurrent kernel machine LSTM) as corresponding to (10)-(11) 
which resembles the n-gram LSTM  but without a tanh nonlinearity on the cell update ˜ct or emission
ct. We term RKM-CIFG as a RKM-LSTM with ηt = 1 − ft  as discussed in Section 3.2. Linear
Kernel w/ ot corresponds to (10)-(11) with ηt = σ2
f time-invariant
constants; this corresponds to a linear kernel for the update of the memory cell  and dynamic gating
on the output  via ot. We also consider the same model without dynamic gating on the output  i.e. 
ot = 1 for all t (with a tanh nonlinearity on the output)  which we call Linear Kernel. The Gated
CNN corresponds to the model in [10]  which is the same as Linear Kernel w/ ot  but with σ2
f = 0
(i.e.  no memory). Finally  we consider a CNN model [18]  that is the same as the Linear Kernel
model  but without feedback or memory  i.e.  z(cid:48)
f = 0. For all of these  we may also
consider an n-gram generalization as introduced in Section 4. For example  a 3-gram RKM-LSTM
corresponds to (18)-(19)  with length-3 convolutional ﬁlters in the time dimension. The models are
summarized in Table 1. All experiments are run on a single NVIDIA Titan X GPU.
Document Classiﬁcation We show results for several popular document classiﬁcation datasets
[37] in Table 2. The AGNews and Yahoo! datasets are topic classiﬁcation tasks  while Yelp Full
is sentiment analysis and DBpedia is ontology classiﬁcation. The same basic network architecture

i and ft = σ2

t = xt and σ2

f   with σ2

i and σ2

7

Parameters

AGNews

DBpedia

Yahoo!

Yelp Full

Model
LSTM
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot
Linear Kernel
Gated CNN [10]
CNN [18]
Table 2: Document classiﬁcation accuracy for 1-gram and 3-gram versions of various models. Total
parameters of each model are shown  excluding word embeddings and the classiﬁer.

1-gram 3-gram 1-gram 3-gram 1-gram 3-gram 1-gram 3-gram 1-gram 3-gram
66.37
720K
66.43
720K
65.92
540K
65.94
360K
180K
62.11
64.30
180K
90K
62.08

1.44M
1.44M
1.08M
720K
360K
540K
270K

92.46
92.28
92.39
91.49
91.50
91.78
91.53

77.74
77.70
77.71
77.41
76.93
72.92
72.51

66.27
65.92
65.93
65.35
61.18
60.25
59.77

77.72
77.72
77.91
77.53
76.53
76.66
75.97

98.98
98.97
98.99
98.96
98.65
98.37
98.17

91.82
91.76
92.29
92.07
91.62
91.54
91.20

98.97
99.00
99.05
98.94
98.77
98.77
98.52

Model
LSTM [15  25]
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot

PTB

Wikitext-2

PPL valid PPL test PPL valid PPL test

61.2
60.3
61.9
72.3

58.9
58.2
59.5
69.7

68.74
67.85
69.12
84.23

65.68
65.22
66.03
80.21

Table 3: Language model perplexity (PPL) on validation and test sets of the Penn Treebank and
Wikitext-2 language modeling tasks.

i = σ2

f = 0.5.

is used for all models  with the only difference being the choice of recurrent cell  which we make
single-layer and unidirectional. Hidden representations h(cid:48)
t are aggregated with mean pooling across
time  followed by two fully connected layers  with the second having output size corresponding to
the number of classes of the dataset. We use 300-dimensional GloVe [27] as our word embedding
initialization and set the dimensions of all hidden units to 300. We follow the same preprocessing
procedure as in [34]. Layer normalization [2] is performed after the computation of the cell state ct.
For the Linear Kernel w/ ot and the Linear Kernel  we set4 σ2
Notably  the derived RKM-LSTM model performs comparably to the standard LSTM model across
all considered datasets. We also ﬁnd the CIFG version of the RKM-LSTM model to have similar
accuracy. As the recurrent model becomes less sophisticated with regard to gating and memory 
we see a corresponding decrease in classiﬁcation accuracy. This decrease is especially signiﬁcant
for Yelp Full  which requires a more intricate comprehension of the entire text to make a correct
prediction. This is in contrast to AGNews and DBpedia  where the success of the 1-gram CNN
indicates that simple keyword matching is sufﬁcient to do well. We also observe that generalizing the
model to consider n-gram inputs typically improves performance; the highest accuracies for each
dataset were achieved by an n-gram model.
Language Modeling We also perform experiments on popular word-level language generation
datasets Penn Tree Bank (PTB) [24] and Wikitext-2 [26]  reporting validation and test perplexities
(PPL) in Table 3. We adopt AWD-LSTM [25] as our base model5  replacing the standard LSTM
with RKM-LSTM  RKM-CIFG  and Linear Kernel w/ ot to do our comparison. We keep all other
hyperparameters the same as the default. Here we consider 1-gram ﬁlters  as they performed best
for this task; given that the datasets considered here are smaller than those for the classiﬁcation
experiments  1-grams are less likely to overﬁt. Note that the static gating on the update of the memory
cell (Linear Kernel w/ ot) does considerably worse than the models with dynamic input and forget
gates on the memory cell. The RKM-LSTM model consistently outperforms the traditional LSTM 
again showing that the models derived from recurrent kernel machines work well in practice for the
data considered.
LFP Classiﬁcation We perform experiments on a Local Field Potential (LFP) dataset. The LFP
signal is multi-channel time series recorded inside the brain to measure neural activity. The LFP
dataset used in this work contains recordings from 29 mice (wild-type or CLOCK∆19 [32])  while
the mice were (i) in their home cages  (ii) in an open ﬁeld  and (iii) suspended by their tails. There
are a total of m = 11 channels and the sampling rate is 1000Hz. The goal of this task is to predict

i and σ2

4σ2
5We use the ofﬁcial codebase https://github.com/salesforce/awd-lstm-lm and report ex-

f can also be learned  but we found this not to have much effect on the ﬁnal performance.

periment results before two-step ﬁne-tuning.

8

Model

Accuracy

n-gram
LSTM
80.24

RKM-
LSTM
79.02

RKM-
CIFG
77.58

Linear

Kernel w/ ot

76.11

Linear
Kernel
73.13

Gated
CNN [10] CNN [22]

76.02

73.40

Table 4: Mean leave-one-out classiﬁcation accuracies for mouse LFP data. For each model  (n = 40)-
gram ﬁlters are considered  and the number of ﬁlters in each model is 30.

the state of a mouse from a 1 second segment of its LFP recording as a 3-way classiﬁcation problem.
In order to test the model generalizability  we perform leave-one-out cross-validation testing: data
from each mouse is left out as testing iteratively while the remaining mice are used as training.
SyncNet [22] is a CNN model with speciﬁcally designed wavelet ﬁlters for neural data. We incorporate
the SyncNet form of n-gram convolutional ﬁlters into our recurrent framework (we have parameteric
n-gram convolutional ﬁlters  with parameters learned). As was demonstrated in Section 4.2  the
CNN is a memory-less special case of our derived generalized LSTM. An illustration of the modiﬁed
model (Figure 3) can be found in Appendix A  along with other further details on SyncNet.
While the ﬁlters of SyncNet are interpretable and can prevent overﬁtting (because they have a small
number of parameters)  the same kind of generalization to an n-gram LSTM can be made without
increasing the number of learned parameters. We do so for all of the recurrent cell types in Table
1  with the CNN corresponding to the original SyncNet model. Compared to the original SyncNet
model  our newly proposed models can jointly consider the time dependency within the whole signal.
The mean classiﬁcation accuracies across all mice are compared in Table 4  where we observe
substantial improvements in prediction accuracy through the addition of memory cells to the model.
Thus  considering the time dependency in the neural signal appears to be beneﬁcial for identifying
hidden patterns. Classiﬁcation performances per subject (Figure 4) can be found in Appendix A.
7 Conclusions
The principal contribution of this paper is a new perspective on gated RNNs  leveraging concepts
from recurrent kernel machines. From that standpoint  we have derived a model closely connected
to the LSTM [15  13] (for convolutional ﬁlters of length one)  and have extended such models to
convolutional ﬁlters of length greater than one  yielding a generalization of the LSTM. The CNN
[18  37  17]  Gated CNN [10] and RAN [20] models are recovered as special cases of the developed
framework. We have demonstrated the efﬁcacy of the derived models on NLP and neuroscience tasks 
for which our RKM variants show comparable or better performance than the LSTM. In particular 
we observe that extending LSTM variants with convolutional ﬁlters of length greater than one can
signiﬁcantly improve the performance in LFP classiﬁcation relative to recent prior work.

Acknowledgments
The research reported here was supported in part by DARPA  DOE  NIH  NSF and ONR.

References
[1] Fabio Anselmi  Lorenzo Rosasco  Cheston Tan  and Tomaso Poggio. Deep Convolutional

Networks are Hierarchical Kernel Machines. arXiv:1508.01084  2015.

[2] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E. Hinton.

arXiv:1607.06450  2016.

Layer Normalization.

[3] David Balduzzi and Muhammad Ghifary. Strongly-Typed Recurrent Neural Networks. Interna-

tional Conference on Machine Learning  2016.

[4] Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert spaces in Probability

and Statistics. Kluwer Publishers  2004.

[5] Alberto Bietti and Julien Mairal. Invariance and Stability of Deep Convolutional Representations.

Neural Information Processing Systems  2017.

[6] James Bradbury  Stephen Merity  Caiming Xiong  and Richard Socher. Quasi-recurrent neural

networks. International Conference of Learning Representations  2017.

9

[7] Mia Xu Chen  Orhan Firat  Ankur Bapna  Melvin Johnson  Wolfgang Macherey  George
Foster  Llion Jones  Niki Parmar  Mike Schuster  Zhifeng Chen  Yonghui Wu  and Macduff
Hughes. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.
arXiv:1804.09849v2  2018.

[8] Jianpeng Cheng and Mirella Lapata. Neural Summarization by Extracting Sentences and Words.

Association for Computational Linguistics  2016.

[9] Kyunghyun Cho  Bart van Merrienboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares 
Holger Schwenk  and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. Empirical Methods in Natural Language Process-
ing  2014.

[10] Yann N. Dauphin  Angela Fan  Michael Auli  and David Grangier. Language Modeling with

Gated Convolutional Networks. International Conference on Machine Learning  2017.

[11] Marc G. Genton. Classes of Kernels for Machine Learning: A Statistics Perspective. Journal of

Machine Learning Research  2001.

[12] David Golub and Xiaodong He. Character-Level Question Answering with Attention. Empirical

Methods in Natural Language Processing  2016.

[13] Klaus Greff  Rupesh Kumar Srivastava  Jan Koutník  Bas R. Steunebrink  and Jürgen Schmidhu-
ber. LSTM: A Search Space Odyssey. Transactions on Neural Networks and Learning Systems 
2017.

[14] Michiel Hermans and Benjamin Schrauwen. Recurrent Kernel Machines: Computing with

Inﬁnite Echo State Networks. Neural Computation  2012.

[15] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation 

1997.

[16] Rafal Jozefowicz  Wojciech Zaremba  and Ilya Sutskever. An Empirical Exploration of Recur-

rent Network Architectures. International Conference on Machine Learning  2015.

[17] Yoon Kim. Convolutional Neural Networks for Sentence Classiﬁcation. Empirical Methods in

Natural Language Processing  2014.

[18] Yann LeCun and Yoshua Bengio. Convolutional Networks for Images  Speech  and Time Series.

The Handbook of Brain Theory and Neural Networks  1995.

[19] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based Learning

Applied to Document Recognition. Proceedings of IEEE  1998.

[20] Kenton Lee  Omer Levy  and Luke Zettlemoyer.

arXiv:1705.07393v2  2017.

Recurrent Additive Networks.

[21] Tao Lei  Wengong Jin  Regina Barzilay  and Tommi Jaakkola. Deriving Neural Architectures

from Sequence and Graph Kernels. International Conference on Machine Learning  2017.

[22] Yitong Li  Michael Murias  Samantha Major  Geraldine Dawson  Kafui Dzirasa  Lawrence
Carin  and David E. Carlson. Targeting EEG/LFP Synchrony with Neural Nets. Neural
Information Processing Systems  2017.

[23] Julien Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks.

Neural Information Processing Systems  2016.

[24] Mitchell P. Marcus  Beatrice Santorini  and Mary Ann Marcinkiewicz. Building a Large
Annotated Corpus of English: The Penn Treebank. Association for Computational Linguistics 
1993.

[25] Stephen Merity  Nitish Shirish Keskar  and Richard Socher. Regularizing and Optimizing LSTM

Language Models. International Conference on Learning Representations  2018.

10

[26] Stephen Merity  Caiming Xiong  James Bradbury  and Richard Socher. Pointer Sentinel Mixture

Models. International Conference of Learning Representations  2017.

[27] Jeffrey Pennington  Richard Socher  and Christopher D. Manning. GloVe: Global Vectors for

Word Representation. Empirical Methods in Natural Language Processing  2014.

[28] Christopher Roth  Ingmar Kanitscheider  and Ila Fiete. Kernel rnn learning (kernl). International

Conference Learning Representation  2019.

[29] Bernhard Scholkopf and Alexander J. Smola. Learning with kernels. MIT Press  2002.

[30] Corentin Tallec and Yann Ollivier. Can Recurrent Neural Networks Warp Time? International

Conference of Learning Representations  2018.

[31] Aaron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex
Graves  Nal Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. WaveNet: A Generative
Model for Raw Audio. arXiv:1609.03499  2016.

[32] Jordy van Enkhuizen  Arpi Minassian  and Jared W Young. Further evidence for Clock∆19 mice
as a model for bipolar disorder mania using cross-species tests of exploration and sensorimotor
gating. Behavioural Brain Research  249:44–54  2013.

[33] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N. Gomez 
Lukasz Kaiser  and Illia Polosukhin. Attention Is All You Need. Neural Information Processing
Systems  2017.

[34] Guoyin Wang  Chunyuan Li  Wenlin Wang  Yizhe Zhang  Dinghan Shen  Xinyuan Zhang  Ri-
cardo Henao  and Lawrence Carin. Joint Embedding of Words and Labels for Text Classiﬁcation.
Association for Computational Linguistics  2018.

[35] Jin Wang  Liang-Chih Yu  K. Robert Lai  and Xuejie Zhang. Dimensional Sentiment Analysis

Using a Regional CNN-LSTM Model. Association for Computational Linguistics  2016.

[36] Andrew Gordon Wilson  Zhiting Hu  Ruslan Salakhutdinov  and Eric P. Xing. Deep Kernel

Learning. International Conference on Artiﬁcial Intelligence and Statistics  2016.

[37] Xiang Zhang  Junbo Zhao  and Yann LeCun. Character-level Convolutional Networks for Text

Classiﬁcation. Neural Information Processing Systems  2015.

[38] Chunting Zhou  Chonglin Sun  Zhiyuan Liu  and Francis C.M. Lau. A C-LSTM Neural Network

for Text Classiﬁcation. arXiv:1511.08630  2015.

11

A More Details of the LFP Experiment

model [22] into our framework  the weight W (x) = (cid:2)W (x0)  W (x−1) ···   W (x−n+1)(cid:3) deﬁned in

In this section  we provide more details on the Sync-RKM model. In order to incorporate the SyncNet

Eq. (12) is parameterized as wavelet ﬁlters. If there is a total of K ﬁlters  then W (x) is of size
K × C × n.
Speciﬁcally  suppose the n-gram input data at time t is given as Xt = [xt−n+1 ···   xt] ∈ RC×n
with channel number C and window size n. The k-th ﬁlter for channel c can be written as

W (x)

kc = αkc cos (ωkt + φkc) exp(−βkt2)

(21)

W (x)
kc has the form of the Morlet wavelet base function. Parameters to be learned are αkc  ωk  φkc
and βk for c = 1 ··· C and k = 1 ···   K. t is a time grid of length n  which is a constant vector.
In the recurrent cell  each W (x)
is convolved with the c-th channel of Xt using 1-d convolution.
kc
Figure 3 gives the framework of this Sync-RKM model. For more details of how the ﬁlter works 
please refer to the original work [22].

Figure 3: Illustration of the proposed model with SyncNet ﬁlters. The input LFP signal is given by
the C × T matrix. The SyncNet ﬁlters (right) are applied on signal chunks at each time step.

When applying the Sync-RKM model on LFP data  we choose the window size as n = 40 to consider
the time dependencies in the signal. Since the experiment is performed by treating each mouse as test
iteratively  we show the subject-wise classiﬁcation accuracy in Figure 4. The proposed model does
consistently better across nearly all subjects.

Figure 4: Subject-wise classiﬁcation accuracy comparison for LFP dataset.

12

𝑪𝑻1𝑡𝑇Sync-RKMSync-RKMSync-RKM…Sync-RKMSync-RKM…23𝒉𝒕=𝒇𝑿𝒕+𝑾(𝒉)𝒉𝒕−𝟏+𝒃𝑾(𝒙),Kevin Liang
Guoyin Wang
Yitong Li
Ricardo Henao
Lawrence Carin