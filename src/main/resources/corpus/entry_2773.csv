2019,Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods,We investigate time-dependent data analysis from the perspective of recurrent kernel machines  from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell  a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to $n$-gram filters  the convolutional neural network (CNN)  Gated CNN  and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM  while also extending it to $n$-gram convolutional filters. Experiments are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application  the new models demonstrate significant improvements relative to the prior state of the art.,Kernel-Based Approaches for Sequence Modeling:

Connections to Neural Methods

Kevin J Liangâˆ— Guoyin Wangâˆ— Yitong Li Ricardo Henao Lawrence Carin

Department of Electrical and Computer Engineering

Duke University

{kevin.liang  guoyin.wang  yitong.li  ricardo.henao  lcarin}@duke.edu

Abstract

We investigate time-dependent data analysis from the perspective of recurrent
kernel machines  from which models with hidden units and gated memory cells
arise naturally. By considering dynamic gating of the memory cell  a model closely
related to the long short-term memory (LSTM) recurrent neural network is derived.
Extending this setup to n-gram ï¬lters  the convolutional neural network (CNN) 
Gated CNN  and recurrent additive network (RAN) are also recovered as special
cases. Our analysis provides a new perspective on the LSTM  while also extending
it to n-gram convolutional ï¬lters. Experiments1 are performed on natural language
processing tasks and on analysis of local ï¬eld potentials (neuroscience). We
demonstrate that the variants we derive from kernels perform on par or even better
than traditional neural methods. For the neuroscience application  the new models
demonstrate signiï¬cant improvements relative to the prior state of the art.

Introduction

1
There has been signiï¬cant recent effort directed at connecting deep learning to kernel machines
[1  5  23  36]. Speciï¬cally  it has been recognized that a deep neural network may be viewed as
constituting a feature mapping x â†’ Ï•Î¸(x)  for input data x âˆˆ Rm. The nonlinear function Ï•Î¸(x) 
with model parameters Î¸  has an output that corresponds to a d-dimensional feature vector; Ï•Î¸(x)
may be viewed as a mapping of x to a Hilbert space H  where H âŠ‚ Rd. The ï¬nal layer of deep
Ï•Î¸(x)  with weight vector Ï‰ âˆˆ H; for
neural networks typically corresponds to an inner product Ï‰
(cid:124)
a vector output  there are multiple Ï‰  with Ï‰
i Ï•Î¸(x) deï¬ning the i-th component of the output. For
example  in a deep convolutional neural network (CNN) [19]  Ï•Î¸(x) is a function deï¬ned by the
multiple convolutional layers  the output of which is a d-dimensional feature map; Ï‰ represents the
fully-connected layer that imposes inner products on the feature map. Learning Ï‰ and Î¸  i.e.  the
cumulative neural network parameters  may be interpreted as learning within a reproducing kernel
Hilbert space (RKHS) [4]  with Ï‰ the function in H; Ï•Î¸(x) represents the mapping from the space of
the input x to H  with associated kernel kÎ¸(x  x(cid:48)) = Ï•Î¸(x)
Insights garnered about neural networks from the perspective of kernel machines provide valuable
theoretical underpinnings  helping to explain why such models work well in practice. As an example 
the RKHS perspective helps explain invariance and stability of deep models  as a consequence of
the smoothness properties of an appropriate RKHS to variations in the input x [5  23]. Further  such
insights provide the opportunity for the development of new models.
Most prior research on connecting neural networks to kernel machines has assumed a single input x 
e.g.  image analysis in the context of a CNN [1  5  23]. However  the recurrent neural network (RNN)
has also received renewed interest for analysis of sequential data. For example  long short-term

Ï•Î¸(x(cid:48))  where x(cid:48) is another input.

(cid:124)

(cid:124)

âˆ—These authors contributed equally to this work.
1Implementations can be found at https://github.com/kevinjliang/kernels2rnns.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

memory (LSTM) [15  13] and the gated recurrent unit (GRU) [9] have become fundamental elements
in many natural language processing (NLP) pipelines [16  9  12]. In this context  a sequence of
data vectors (. . .   xtâˆ’1  xt  xt+1  . . . ) is analyzed  and the aforementioned single-input models are
inappropriate.
In this paper  we extend to recurrent neural networks (RNNs) the concept of analyzing neural
networks from the perspective of kernel machines. Leveraging recent work on recurrent kernel
machines (RKMs) for sequential data [14]  we make new connections between RKMs and RNNs 
showing how RNNs may be constructed in terms of recurrent kernel machines  using simple ï¬lters.
We demonstrate that these recurrent kernel machines are composed of a memory cell that is updated
sequentially as new data come in  as well as in terms of a (distinct) hidden unit. A recurrent model
that employs a memory cell and a hidden unit evokes ideas from the LSTM. However  within the
recurrent kernel machine representation of a basic RNN  the rate at which memory fades with time
is ï¬xed. To impose adaptivity within the recurrent kernel machine  we introduce adaptive gating
elements on the updated and prior components of the memory cell  and we also impose a gating
network on the output of the model. We demonstrate that the result of this reï¬nement of the recurrent
kernel machine is a model closely related to the LSTM  providing new insights on the LSTM and its
connection to kernel machines.
Continuing with this framework  we also introduce new concepts to models of the LSTM type. The
reï¬ned LSTM framework may be viewed as convolving learned ï¬lters across the input sequence and
using the convolutional output to constitute the time-dependent memory cell. Multiple ï¬lters  possibly
of different temporal lengths  can be utilized  like in the CNN. One recovers the CNN [18  37  17]
and Gated CNN [10] models of sequential data as special cases  by turning off elements of the new
LSTM setup. From another perspective  we demonstrate that the new LSTM-like model may be
viewed as introducing gated memory cells and feedback to a CNN model of sequential data.
In addition to developing the aforementioned models for sequential data  we demonstrate them in an
extensive set of experiments  focusing on applications in natural language processing (NLP) and in
analysis of multi-channel  time-dependent local ï¬eld potential (LFP) recordings from mouse brains.
Concerning the latter  we demonstrate marked improvements in performance of the proposed methods
relative to recently-developed alternative approaches [22].

2 Recurrent Kernel Network
Consider a sequence of vectors (. . .   xtâˆ’1  xt  xt+1  . . . )  with xt âˆˆ Rm. For a language model  xt
is the embedding vector for the t-th word wt in a sequence of words. To model this sequence  we
introduce yt = U ht  with the recurrent hidden variable satisfying

ht = f (W (x)xt + W (h)htâˆ’1 + b)

(1)
where ht âˆˆ Rd  U âˆˆ RV Ã—d  W (x) âˆˆ RdÃ—m  W (h) âˆˆ RdÃ—d  and b âˆˆ Rd. In the context of a language
model  the vector yt âˆˆ RV may be fed into a nonlinear function to predict the next word wt+1 in the
sequence. Speciï¬cally  the probability that wt+1 corresponds to i âˆˆ {1  . . .   V } in a vocabulary of V
words is deï¬ned by element i of vector Softmax(yt + Î²)  with bias Î² âˆˆ RV . In classiï¬cation  such
as the LFP-analysis example in Section 6  V is the number of classes under consideration.
We constitute the factorization U = AE  where A âˆˆ RV Ã—j and E âˆˆ RjÃ—d  often with j (cid:28) V .
Hence  we may write yt = Ah(cid:48)
t = Eht; the columns of A may be viewed as time-invariant
factor loadings  and h(cid:48)
t represents a vector of dynamic factor scores. Let zt = [xt  htâˆ’1] represent a
column vector corresponding to the concatenation of xt and htâˆ’1; then ht = f (W (z)zt + b) where
W (z) = [W (x)  W (h)] âˆˆ RdÃ—(d+m). Computation of Eht corresponds to inner products of the
rows of E with the vector ht. Let ei âˆˆ Rd be a column vector  with elements corresponding to row
i âˆˆ {1  . . .   j} of E. Then component i of h(cid:48)
(cid:124)
i ht = e

(2)
We view f (W (z)zt + b) as mapping zt into a RKHS H  and vector ei is also assumed to reside within
H. We consequently assume

t  with h(cid:48)

t is

h(cid:48)
i t = e

(cid:124)
i f (W (z)zt + b)

ei = f (W (z) Ëœzi + b)

(3)

2

(a)

(b)

(c)

Figure 1: a) A traditional recurrent neural network (RNN)  with the factorization U = AE. b) A
recurrent kernel machine (RKM)  with an implicit hidden state and recurrence through recursion. c)
The recurrent kernel machine expressed in terms of a memory cell.

(cid:124)
i xt + qÎ¸[Ëœx

(cid:124)
i xtâˆ’1 + qÎ¸[Ëœx

(cid:124)
i xtâˆ’2 + qÎ¸[Ëœx

(cid:124)
(cid:124)
i xtâˆ’3 + Ëœh
âˆ’4htâˆ’4]]]]

(5)

e

(cid:124)

where Ëœzi = [Ëœxi  Ëœh0]. Note that here Ëœh0 also depends on index i  which we omit for simplicity; as
discussed below  Ëœxi will play the primary role when performing computations.

(cid:124)
i ht = e

(cid:124)
i f (W (z)zt + b) = f (W (z) Ëœzi + b)

f (W (z)zt + b) = kÎ¸(Ëœzi  zt)

(4)
(cid:124)
where kÎ¸(Ëœzi  zt) = h(Ëœzi)
h(zt) is a Mercer kernel [29]. Particular kernel choices correspond to
different functions f (W (z)zt + b)  and Î¸ is meant to represent kernel parameters that may be adjusted.
(cid:124)
1 ht 2 where qÎ¸(Â·) is a function of
(cid:124)
zt) = Ëœh
We initially focus on kernels of the form kÎ¸(Ëœz  zt) = qÎ¸(Ëœz
parameters Î¸  ht = h(zt)  and Ëœh1 is the implicit latent vector associated with the inner product  i.e. 
Ëœh1 = f (W (x) Ëœx + W (h)Ëœh0 + b). As discussed below  we will not need to explicitly evaluate ht or Ëœh1
to evaluate the kernel  taking advantage of the recursive relationship in (1). In fact  depending on
the choice of qÎ¸(Â·)  the hidden vectors may even be inï¬nite-dimensional. However  because of the
(cid:124)
1 ht  for rigorous analysis qÎ¸(Â·) should satisfy Mercerâ€™s condition [11  29].
(cid:124)
zt) = Ëœh
relationship qÎ¸(Ëœz
The vectors (Ëœh1  Ëœh0  Ëœhâˆ’1  . . . ) are assumed to satisfy the same recurrence setup as (1)  with each
vector in the associated sequence (Ëœxt  Ëœxtâˆ’1  . . . ) assumed to be the same Ëœxi at each time  i.e. 
associated with ei  (Ëœxt  Ëœxtâˆ’1  . . . ) â†’ (Ëœxi  Ëœxi  . . . ). Stepping backwards in time three steps  for
example  one may show

kÎ¸(Ëœzi  zt) = qÎ¸[Ëœx

(cid:124)
The inner product Ëœh
âˆ’4htâˆ’4 encapsulates contributions for all times further backwards  and for a
(cid:124)
sequence of length N  Ëœh
âˆ’N htâˆ’N plays a role analogous to a bias. As discussed below  for stability
the repeated application of qÎ¸(Â·) yields diminishing (fading) contributions from terms earlier in time 
(cid:124)
and therefore for large N the impact of Ëœh
âˆ’N htâˆ’N on kÎ¸(Ëœzi  zt) is small.
The overall model may be expressed as

h(cid:48)
t = qÎ¸(ct)  

ct = Ëœct + qÎ¸(ctâˆ’1)   Ëœct = ËœXxt

(6)
(cid:124)
where ct âˆˆ Rj is a memory cell at time t  row i of ËœX corresponds to Ëœx
i   and qÎ¸(ct) operates pointwise
on the components of ct (see Figure 1). At the start of the sequence of length N  qÎ¸(ctâˆ’N ) may be
(cid:124)
seen as a vector of biases  effectively corresponding to Ëœh
N htâˆ’N ; we henceforth omit discussion of
this initial bias for notational simplicity  and because for sufï¬ciently large N its impact on h(cid:48)
t is small.
Note that via the recursive process by which ct is evaluated in (6)  the kernel evaluations reï¬‚ected
by qÎ¸(ct) are deï¬ned entirely by the elements of the sequence (Ëœct  Ëœctâˆ’1  Ëœctâˆ’2  . . . ). Let Ëœci t repre-
sent the i-th component in vector Ëœct  and deï¬ne xâ‰¤t = (xt  xtâˆ’1  xtâˆ’2  . . . ). Then the sequence
(Ëœci t  Ëœci tâˆ’1  Ëœci tâˆ’2  . . . ) is speciï¬ed by convolving in time Ëœxi with xâ‰¤t  denoted Ëœxi âˆ— xâ‰¤t. Hence  the
j components of the sequence (Ëœct  Ëœctâˆ’1  Ëœctâˆ’2  . . . ) are completely speciï¬ed by convolving xâ‰¤t with
each of the j ï¬lters  Ëœxi  i âˆˆ {1  . . .   j}  i.e.  taking an inner product of Ëœxi with the vector in xâ‰¤t at
each time point.
In (4) we represented h(cid:48)
i t = kÎ¸(Ëœzi  zt); now  because of the recursive form of the
(cid:124)
i zt)  we have demonstrated that we
model in (1)  and because of the assumption kÎ¸(Ëœzi  zt) = qÎ¸(Ëœz
2) [14]  as for a Gaussian kernel 
zt).

(cid:124)
(cid:124)
i Ëœxi = 1)  then qÎ¸((cid:107)Ëœz âˆ’ zt(cid:107)2
but if vectors xt and ï¬lters Ëœxi are normalized (e.g.  x
t xt = Ëœx

2One may also design recurrent kernels of the form kÎ¸(Ëœz  zt) = qÎ¸((cid:107)Ëœz âˆ’ zt(cid:107)2

i t = qÎ¸(ci t) as h(cid:48)

(cid:124)
2) reduces to qÎ¸(Ëœz

3

may express the kernel equivalently as kÎ¸(Ëœxi âˆ— xâ‰¤t)  to underscore that it is deï¬ned entirely by the
elements at the output of the convolution Ëœxi âˆ— xâ‰¤t. Hence  we may express component i of h(cid:48)
t as
i t = kÎ¸(Ëœxi âˆ— xâ‰¤t).
h(cid:48)
Component l âˆˆ {1  . . .   V } of yt = Ah(cid:48)

t may be expressed

yl t =

Al ikÎ¸(Ëœxi âˆ— xâ‰¤t)

(7)

j(cid:88)

i=1

where Al i represents component (l  i) of matrix A. Considering (7)  the connection of an RNN to an
RKHS is clear  as made explicit by the kernel kÎ¸(Ëœxi âˆ— xâ‰¤t). The RKHS is manifested for the ï¬nal
output yt  with the hidden ht now absorbed within the kernel  via the inner product (4). The feedback
imposed via latent vector ht is constituted via update of the memory cell ct = Ëœct + qÎ¸(ctâˆ’1) used to
evaluate the kernel.
Rather than evaluating yt as in (7)  it will prove convenient to return to (6). Speciï¬cally  we may
consider modifying (6) by injecting further feedback via h(cid:48)

t  augmenting (6) as

h(cid:48)
t = qÎ¸(ct)  

ct = Ëœct + qÎ¸(ctâˆ’1)   Ëœct = ËœXxt + ËœHh(cid:48)

tâˆ’1

(8)

where ËœH âˆˆ RjÃ—j  and recalling yt = Ah(cid:48)
t (see Figure 2a for illustration). In (8) the input to the
kernel is dependent on the input elements (xt  xtâˆ’1  . . . ) and is now also a function of the kernel
outputs at the previous time  via h(cid:48)
t is still speciï¬ed entirely by the elements
of Ëœxi âˆ— xâ‰¤t  for i âˆˆ {1  . . .   j}.

tâˆ’1. However  note that h(cid:48)

3 Choice of Recurrent Kernels & Introduction of Gating Networks
3.1 Fixed kernel parameters & time-invariant memory-cell gating
The function qÎ¸(Â·) discussed above may take several forms  the simplest of which is a linear kernel 
with which (8) takes the form

(9)

h(cid:48)
t = ct  

ct = Ïƒ2

i Ëœct + Ïƒ2

i and Ïƒ2
i and Ïƒ2

f ctâˆ’1   Ëœct = ËœXxt + ËœHh(cid:48)
tâˆ’1
f < 1 for stability. The
f (using analogous notation from [14]) are scalars  with Ïƒ2
where Ïƒ2
f may be viewed as static (i.e.  time-invariant) gating elements  with Ïƒ2
i controlling
scalars Ïƒ2
f controlling how much of the prior
weighting on the new input element to the memory cell  and Ïƒ2
memory unit is retained; given Ïƒ2
f < 1  this means information from previous time steps tends to
fade away and over time is largely forgotten. However  such a kernel leads to time-invariant decay
of memory: the contribution Ëœctâˆ’N from N steps before to the current memory ct is (ÏƒiÏƒN
f )2Ëœctâˆ’N  
meaning that it decays at a constant exponential rate. Because the information contained at each time
step can vary  this can be problematic. This suggests augmenting the model  with time-varying gating
weights  with memory-component dependence on the weights  which we consider below.
3.2 Dynamic gating networks & LSTM-like model
Recent work has shown that dynamic gating can be seen as making a recurrent network quasi-invariant
to temporal warpings [30]. Motivated by the form of the model in (9) then  it is natural to impose
dynamic versions of Ïƒ2
f ; we also introduce dynamic gating at the output of the hidden vector.
This yields the model:
t = ot (cid:12) ct  
h(cid:48)
t + bo)  

(10)
(11)
tâˆ’1]  and Wc encapsulates ËœX and ËœH. In (10)-(11) the symbol (cid:12) represents a
where z(cid:48)
pointwise vector product (Hadamard); Wc  Wo  WÎ· and Wf are weight matrices; bo  bÎ· and bf are
bias vectors; and Ïƒ(Î±) = 1/(1 + exp(âˆ’Î±)). In (10)  Î·t and ft play dynamic counterparts to Ïƒ2
i and
f   respectively. Further  ot  Î·t and ft are vectors  constituting vector-component-dependent gating.
Ïƒ2
Note that starting from a recurrent kernel machine  we have thus derived a model closely resembling
the LSTM. We call this model RKM-LSTM (see Figure 2).
Concerning the update of the hidden state  h(cid:48)
hyperbolic-tangent tanh nonlinearity: h(cid:48)

t = ot (cid:12) ct in (10)  one may also consider appending a
t = ot (cid:12) tanh(ct). However  recent research has suggested

ct = Î·t (cid:12) Ëœct + ft (cid:12) ctâˆ’1  

Ëœct = Wcz(cid:48)
t
t + bf )

Î·t = Ïƒ(WÎ·z(cid:48)

t + bÎ·)  

ft = Ïƒ(Wf z(cid:48)

i and Ïƒ2

ot = Ïƒ(Woz(cid:48)

t = [xt  h(cid:48)

4

(a)

(b)

Figure 2: a) Recurrent kernel machine  with feedback  as deï¬ned in (8). b) Making a linear kernel
assumption and adding input  forget  and output gating  this model becomes the RKM-LSTM.

not using such a nonlinearity [20  10  7]  and this is a natural consequence of our recurrent kernel
t = ot (cid:12) tanh(ct)  the model in (10) and (11) is in the form of the LSTM  except
analysis. Using h(cid:48)
without the nonlinearity imposed on the memory cell Ëœct  while in the LSTM a tanh nonlinearity (and
biases) is employed when updating the memory cell [15  13]  i.e.  for the LSTM Ëœct = tanh(Wcz(cid:48)
t +
bc). If ot = 1 for all time t (no output gating network)  and if Ëœct = Wcxt (no dependence on h(cid:48)
tâˆ’1
for update of the memory cell)  this model reduces to the recurrent additive network (RAN) [20].
While separate gates Î·t and ft were constituted in (10) and (11) to operate on the new and prior
composition of the memory cell  one may also also consider a simpler model with memory cell
updated ct = (1âˆ’ ft)(cid:12) Ëœct + ft(cid:12) ctâˆ’1; this was referred to as having a Coupled Input and Forget Gate
(CIFG) in [13]. In such a model  the decisions of what to add to the memory cell and what to forget
are made jointly  obviating the need for a separate input gate Î·t. We call this variant RKM-CIFG.
4 Extending the Filter Length
4.1 Generalized form of recurrent model
Consider a generalization of (1):

ht = f (W (x0)xt + W (xâˆ’1)xtâˆ’1 + Â·Â·Â· + W (xâˆ’n+1)xtâˆ’n+1 + W (h)htâˆ’1 + b)

(12)
where W (xÂ·) âˆˆ RdÃ—m  W (h) âˆˆ RdÃ—d  and therefore the update of the hidden state ht
3 depends on
data observed n â‰¥ 1 time steps prior  and also on the previous hidden state htâˆ’1. Analogous to (3) 
we may express

i t = e

(cid:124)
i ht.

ei = f (W (x0) Ëœxi 0 + W (xâˆ’1) Ëœxi âˆ’1 + Â·Â·Â· + W (xâˆ’n+1) Ëœxi âˆ’n+1 + W (h)Ëœhi + b)

(13)
The inner product f (W (x0)xt + W (xâˆ’1)xtâˆ’1 + Â·Â·Â· + W (xâˆ’n+1)xtâˆ’n+1 + W (h)htâˆ’1 +
f (W (x0) Ëœxi 0 + W (xâˆ’1) Ëœxi âˆ’1 + Â·Â·Â· + W (xâˆ’n+1) Ëœxi âˆ’n+1 + W (h)Ëœhi + b) is assumed represented
(cid:124)
b)
by a Mercer kernel  and h(cid:48)
Let Xt = (xt  xtâˆ’1  . . .   xtâˆ’n+1) âˆˆ RmÃ—n be an n-gram input with zero padding if t < (n âˆ’ 1) 
and ËœX = ( ËœX0  ËœXâˆ’1  . . .   ËœXâˆ’n+1) be n sets of ï¬lters  with the i-th rows of ËœX0  ËœXâˆ’1  . . .   ËœXâˆ’n+1
collectively represent the i-th n-gram ï¬lter  with i âˆˆ {1  . . .   j}. Extending Section 2  the kernel is
deï¬ned
(14)
where ËœX Â· Xt â‰¡ ËœX0xt + ËœXâˆ’1xtâˆ’1 + Â·Â·Â· + ËœXâˆ’n+1xtâˆ’n+1 âˆˆ Rj. Note that ËœX Â· Xt corresponds
to the t-th component output from the n-gram convolution of the ï¬lters ËœX and the input sequence;
t = kÎ¸( ËœX âˆ— xâ‰¤t)  emphasizing that
therefore  similar to Section 2  we represent h(cid:48)
the kernel evaluation is a function of outputs of the convolution ËœX âˆ— xâ‰¤t  here with n-gram ï¬lters.
Like in the CNN [18  37  17]  different ï¬lter lengths (and kernels) may be considered to constitute
different components of the memory cell.
4.2 Linear kernel  CNN and Gated CNN
For the linear kernel discussed in connection to (9)  equation (14) becomes

t = qÎ¸(ct) as h(cid:48)

h(cid:48)
t = qÎ¸(ct)  

ct = Ëœct + qÎ¸(ctâˆ’1)  

Ëœct = ËœX Â· Xt

h(cid:48)
t = ct = Ïƒ2

i ( ËœX Â· Xt) + Ïƒ2
f h(cid:48)

tâˆ’1

(15)

3Note that while the same symbol is used as in (12)  ht clearly takes on a different meaning when n > 1.

5

Î·t = Ïƒ( ËœX Î· Â· Xt + bÎ·)

t = Î·t (cid:12) ( ËœX Â· Xt)  
h(cid:48)

f = 0 and Ïƒ2

For the special case of Ïƒ2
i = 1)  (15) reduces to a
convolutional neural network (CNN)  with a nonlinear operation typically applied subsequently to h(cid:48)
t.
i to a constant  one may impose dynamic gating  yielding the model (with
Rather than setting Ïƒ2
f = 0)
Ïƒ2

i equal to a constant (e.g.  Ïƒ2

(16)
where ËœX Î· are distinct convolutional ï¬lters for calculating Î·t  and bÎ· is a vector of biases. The form
of the model in (16) corresponds to the Gated CNN [10]  which we see as a a special case of the
recurrent model with linear kernel  and dynamic kernel weights (and without feedback  i.e.  Ïƒ2
f = 0).
Note that in (16) a nonlinear function is not imposed on the output of the convolution ËœX Â· Xt  there
is only dynamic gating via multiplication with Î·t; the advantages of which are discussed in [10].
Further  the n-gram input considered in (12) need not be consecutive. If spacings between inputs of
more than 1 are considered  then the dilated convolution (e.g.  as used in [31]) is recovered.
4.3 Feedback and the generalized LSTM
Now introducing feedback into the memory cell  the model in (8) is extended to

ct = Ëœct + qÎ¸(ctâˆ’1)   Ëœct = ËœX Â· Xt + ËœHh(cid:48)

tâˆ’1

h(cid:48)
t = qÎ¸(ct)  

(17)

Again motivated by the linear kernel  generalization of (17) to include gating networks is

tâˆ’1

t = ot (cid:12) ct  
h(cid:48)

ct = Î·t (cid:12) Ëœct + ft (cid:12) ctâˆ’1   Ëœct = ËœX Â· Xt + ËœHh(cid:48)

tâˆ’1 +bo)  Î·t = Ïƒ( ËœXÎ·Â·Xt + ËœWÎ·h(cid:48)

tâˆ’1 +bÎ·)  ft = Ïƒ( ËœXf Â·Xt + ËœWf h(cid:48)

(18)
tâˆ’1 +bf )
(19)
t and ËœXo  ËœXÎ·  and ËœXf are separate sets of n-gram convolutional ï¬lters akin to ËœX.

ot = Ïƒ( ËœXoÂ·Xt + ËœWoh(cid:48)
where yt = Ah(cid:48)
As an n-gram generalization of (10)-(11)  we refer to (18)-(19) as an n-gram RKM-LSTM.
The model in (18) and (19) is similar to the LSTM  with important differences: (i) there is not a
nonlinearity imposed on the update to the memory cell  Ëœct  and therefore there are also no biases
imposed on this cell update; (ii) there is no nonlinearity on the output; and (iii) via the convolutions
with ËœX  ËœXo  ËœXÎ·  and ËœXf   the memory cell can take into account n-grams  and the length of such
sequences ni may vary as a function of the element of the memory cell.
5 Related Work
In our development of the kernel perspective of the RNN  we have emphasized that the form of the
(cid:124)
kernel kÎ¸(Ëœzi  zt) = qÎ¸(Ëœz
i zt) yields a recursive means of kernel evaluation that is only a function of
the elements at the output of the convolutions ËœX âˆ— xâ‰¤t or ËœX âˆ— xâ‰¤t  for 1-gram and (n > 1)-gram
ï¬lters  respectively. This underscores that at the heart of such models  one performs convolutions
between the sequence of data (. . .   xt+1  xt  xtâˆ’1  . . . ) and ï¬lters ËœX or ËœX. Consideration of ï¬lters
of length greater than one (in time) yields a generalization of the traditional LSTM. The dependence
of such models entirely on convolutions of the data sequence and ï¬lters is evocative of CNN and
Gated CNN models for text [18  37  17  10]  with this made explicit in Section 4.2 as a special case.
The Gated CNN in (16) and the generalized LSTM in (18)-(19) both employ dynamic gating.
However  the generalized LSTM explicitly employs a memory cell (and feedback)  and hence offers
the potential to leverage long-term memory. While memory affords advantages  a noted limitation
of the LSTM is that computation of h(cid:48)
t is sequential  undermining parallel computation  particularly
while training [10  33]. In the Gated CNN  h(cid:48)
t comes directly from the output of the gated convolution 
allowing parallel ï¬tting of the model to time-dependent data. While the Gated CNN does not employ
recurrence  the ï¬lters of length n > 1 do leverage extended temporal dependence. Further  via deep
Gated CNNs [10]  the effective support of the ï¬lters at deeper layers can be expansive.
(cid:124)
Recurrent kernels of the form kÎ¸(Ëœz  zt) = qÎ¸(Ëœz
zt) were also developed in [14]  but with the goal of
extending recurrent kernel machines to sequential inputs  rather than making connections with RNNs.
The formulation in Section 2 has two important differences with that prior work. First  we employ
(cid:124)
the same vector Ëœxi for all shift positions t of the inner product Ëœx
i xt. By contrast  in [14] effectively
inï¬nite-dimensional ï¬lters are used  because the ï¬lter Ëœxt i changes with t. This makes implementation
computationally impractical  necessitating truncation of the long temporal ï¬lter. Additionally  the
feedback of h(cid:48)
t in (8) was not considered  and as discussed in Section 3.2  our proposed setup yields
natural connections to long short-term memory (LSTM) [15  13].

6

Parameters

Cell

Output

(nm + d)(4d)
(nm + d)(4d)
(nm + d)(3d)
(nm + d)(2d)
(nm + d)(d)

Model
LSTM [15]
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot
Linear Kernel
Gated CNN [10]
CNN [18]
Table 1: Model variants under consideration  assuming 1-gram inputs. Concatenating additional
inputs xtâˆ’1  . . .   xtâˆ’n+1 to z(cid:48)
t in the Input column yields the corresponding n-gram model. Number
of model parameters are shown for input xt âˆˆ Rm and output h(cid:48)

t = ot (cid:12) tanh(ct)
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
t = ot (cid:12) ct
h(cid:48)
h(cid:48)
t = tanh(ct)
t = ot (cid:12) ct
h(cid:48)
h(cid:48)
t = tanh(ct)

Input
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = [xt  h(cid:48)
z(cid:48)
t = xt
z(cid:48)
t = xt

ct = Î·t (cid:12) tanh(Ëœct) + ft (cid:12) ctâˆ’1
ct = (1 âˆ’ ft) (cid:12) Ëœct + ft (cid:12) ctâˆ’1

ct = Î·t (cid:12) Ëœct + ft (cid:12) ctâˆ’1

i Ëœct + Ïƒ2
i Ëœct + Ïƒ2
ct = Ïƒ2
i Ëœct
ct = Ïƒ2
i Ëœct

tâˆ’1]
tâˆ’1]
tâˆ’1]
tâˆ’1]
tâˆ’1]

ct = Ïƒ2
ct = Ïƒ2

f ctâˆ’1
f ctâˆ’1

(nm)(2d)
(nm)(d)

t âˆˆ Rd.

Prior work analyzing neural networks from an RKHS perspective has largely been based on the
feature mapping Ï•Î¸(x) and the weight Ï‰ [1  5  23  36]. For the recurrent model of interest here 
function ht = f (W (x)xt + W (h)htâˆ’1 + b) plays a role like Ï•Î¸(x) as a mapping of an input xt to
what may be viewed as a feature vector ht. However  because of the recurrence  ht is a function of
(xt  xtâˆ’1  . . . ) for an arbitrarily long time period prior to time t:
ht(xt  xtâˆ’1  . . . ) = f (W (x)xt + b + W (h)f (W (x)xtâˆ’1 + b + W (h)f (W (x)xtâˆ’2 + b + . . . ))) (20)
However  rather than explicitly working with ht(xt  xtâˆ’1  . . . )  we focus on the kernel kÎ¸(Ëœzi  zt) =
(cid:124)
i zt) = kÎ¸(Ëœxi âˆ— xâ‰¤t).
qÎ¸(Ëœz
The authors of [21] derive recurrent neural networks from a string kernel by replacing the exact
matching function with an inner product and assume the decay factor to be a nonlinear function.
Convolutional neural networks are recovered by replacing a pointwise multiplication with addition.
However  the formulation cannot recover the standard LSTM formulation  nor is there a consistent
formulation for all the gates. The authors of [28] introduce a kernel-based update rule to approximate
backpropagation through time (BPTT) for RNN training  but still follow the standard RNN structure.
Previous works have considered recurrent models with n-gram inputs as in (12). For example 
strongly-typed RNNs [3] consider bigram inputs  but the previous input xtâˆ’1 is used as a replacement
for htâˆ’1 rather than in conjunction  as in our formulation. Quasi-RNNs [6] are similar to [3]  but
generalize them with a convolutional ï¬lter for the input and use different nonlinearities. Inputs
corresponding to n-grams have also been implicitly considered by models that use convolutional
layers to extract features from n-grams that are then fed into a recurrent network (e.g.  [8  35  38]).
Relative to (18)  these models contain an extra nonlinearity f (Â·) from the convolution and projection
matrix W (x) from the recurrent cell  and no longer recover the CNN [18  37  17] or Gated CNN [10]
as special cases.
6 Experiments
In the following experiments  we consider several model variants  with nomenclature as follows.
The n-gram LSTM developed in Sec. 4.3 is a generalization of the standard LSTM [15] (for which
n = 1). We denote RKM-LSTM (recurrent kernel machine LSTM) as corresponding to (10)-(11) 
which resembles the n-gram LSTM  but without a tanh nonlinearity on the cell update Ëœct or emission
ct. We term RKM-CIFG as a RKM-LSTM with Î·t = 1 âˆ’ ft  as discussed in Section 3.2. Linear
Kernel w/ ot corresponds to (10)-(11) with Î·t = Ïƒ2
f time-invariant
constants; this corresponds to a linear kernel for the update of the memory cell  and dynamic gating
on the output  via ot. We also consider the same model without dynamic gating on the output  i.e. 
ot = 1 for all t (with a tanh nonlinearity on the output)  which we call Linear Kernel. The Gated
CNN corresponds to the model in [10]  which is the same as Linear Kernel w/ ot  but with Ïƒ2
f = 0
(i.e.  no memory). Finally  we consider a CNN model [18]  that is the same as the Linear Kernel
model  but without feedback or memory  i.e.  z(cid:48)
f = 0. For all of these  we may also
consider an n-gram generalization as introduced in Section 4. For example  a 3-gram RKM-LSTM
corresponds to (18)-(19)  with length-3 convolutional ï¬lters in the time dimension. The models are
summarized in Table 1. All experiments are run on a single NVIDIA Titan X GPU.
Document Classiï¬cation We show results for several popular document classiï¬cation datasets
[37] in Table 2. The AGNews and Yahoo! datasets are topic classiï¬cation tasks  while Yelp Full
is sentiment analysis and DBpedia is ontology classiï¬cation. The same basic network architecture

i and ft = Ïƒ2

t = xt and Ïƒ2

f   with Ïƒ2

i and Ïƒ2

7

Parameters

AGNews

DBpedia

Yahoo!

Yelp Full

Model
LSTM
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot
Linear Kernel
Gated CNN [10]
CNN [18]
Table 2: Document classiï¬cation accuracy for 1-gram and 3-gram versions of various models. Total
parameters of each model are shown  excluding word embeddings and the classiï¬er.

1-gram 3-gram 1-gram 3-gram 1-gram 3-gram 1-gram 3-gram 1-gram 3-gram
66.37
720K
66.43
720K
65.92
540K
65.94
360K
180K
62.11
64.30
180K
90K
62.08

1.44M
1.44M
1.08M
720K
360K
540K
270K

92.46
92.28
92.39
91.49
91.50
91.78
91.53

77.74
77.70
77.71
77.41
76.93
72.92
72.51

66.27
65.92
65.93
65.35
61.18
60.25
59.77

77.72
77.72
77.91
77.53
76.53
76.66
75.97

98.98
98.97
98.99
98.96
98.65
98.37
98.17

91.82
91.76
92.29
92.07
91.62
91.54
91.20

98.97
99.00
99.05
98.94
98.77
98.77
98.52

Model
LSTM [15  25]
RKM-LSTM
RKM-CIFG
Linear Kernel w/ ot

PTB

Wikitext-2

PPL valid PPL test PPL valid PPL test

61.2
60.3
61.9
72.3

58.9
58.2
59.5
69.7

68.74
67.85
69.12
84.23

65.68
65.22
66.03
80.21

Table 3: Language model perplexity (PPL) on validation and test sets of the Penn Treebank and
Wikitext-2 language modeling tasks.

i = Ïƒ2

f = 0.5.

is used for all models  with the only difference being the choice of recurrent cell  which we make
single-layer and unidirectional. Hidden representations h(cid:48)
t are aggregated with mean pooling across
time  followed by two fully connected layers  with the second having output size corresponding to
the number of classes of the dataset. We use 300-dimensional GloVe [27] as our word embedding
initialization and set the dimensions of all hidden units to 300. We follow the same preprocessing
procedure as in [34]. Layer normalization [2] is performed after the computation of the cell state ct.
For the Linear Kernel w/ ot and the Linear Kernel  we set4 Ïƒ2
Notably  the derived RKM-LSTM model performs comparably to the standard LSTM model across
all considered datasets. We also ï¬nd the CIFG version of the RKM-LSTM model to have similar
accuracy. As the recurrent model becomes less sophisticated with regard to gating and memory 
we see a corresponding decrease in classiï¬cation accuracy. This decrease is especially signiï¬cant
for Yelp Full  which requires a more intricate comprehension of the entire text to make a correct
prediction. This is in contrast to AGNews and DBpedia  where the success of the 1-gram CNN
indicates that simple keyword matching is sufï¬cient to do well. We also observe that generalizing the
model to consider n-gram inputs typically improves performance; the highest accuracies for each
dataset were achieved by an n-gram model.
Language Modeling We also perform experiments on popular word-level language generation
datasets Penn Tree Bank (PTB) [24] and Wikitext-2 [26]  reporting validation and test perplexities
(PPL) in Table 3. We adopt AWD-LSTM [25] as our base model5  replacing the standard LSTM
with RKM-LSTM  RKM-CIFG  and Linear Kernel w/ ot to do our comparison. We keep all other
hyperparameters the same as the default. Here we consider 1-gram ï¬lters  as they performed best
for this task; given that the datasets considered here are smaller than those for the classiï¬cation
experiments  1-grams are less likely to overï¬t. Note that the static gating on the update of the memory
cell (Linear Kernel w/ ot) does considerably worse than the models with dynamic input and forget
gates on the memory cell. The RKM-LSTM model consistently outperforms the traditional LSTM 
again showing that the models derived from recurrent kernel machines work well in practice for the
data considered.
LFP Classiï¬cation We perform experiments on a Local Field Potential (LFP) dataset. The LFP
signal is multi-channel time series recorded inside the brain to measure neural activity. The LFP
dataset used in this work contains recordings from 29 mice (wild-type or CLOCKâˆ†19 [32])  while
the mice were (i) in their home cages  (ii) in an open ï¬eld  and (iii) suspended by their tails. There
are a total of m = 11 channels and the sampling rate is 1000Hz. The goal of this task is to predict

i and Ïƒ2

4Ïƒ2
5We use the ofï¬cial codebase https://github.com/salesforce/awd-lstm-lm and report ex-

f can also be learned  but we found this not to have much effect on the ï¬nal performance.

periment results before two-step ï¬ne-tuning.

8

Model

Accuracy

n-gram
LSTM
80.24

RKM-
LSTM
79.02

RKM-
CIFG
77.58

Linear

Kernel w/ ot

76.11

Linear
Kernel
73.13

Gated
CNN [10] CNN [22]

76.02

73.40

Table 4: Mean leave-one-out classiï¬cation accuracies for mouse LFP data. For each model  (n = 40)-
gram ï¬lters are considered  and the number of ï¬lters in each model is 30.

the state of a mouse from a 1 second segment of its LFP recording as a 3-way classiï¬cation problem.
In order to test the model generalizability  we perform leave-one-out cross-validation testing: data
from each mouse is left out as testing iteratively while the remaining mice are used as training.
SyncNet [22] is a CNN model with speciï¬cally designed wavelet ï¬lters for neural data. We incorporate
the SyncNet form of n-gram convolutional ï¬lters into our recurrent framework (we have parameteric
n-gram convolutional ï¬lters  with parameters learned). As was demonstrated in Section 4.2  the
CNN is a memory-less special case of our derived generalized LSTM. An illustration of the modiï¬ed
model (Figure 3) can be found in Appendix A  along with other further details on SyncNet.
While the ï¬lters of SyncNet are interpretable and can prevent overï¬tting (because they have a small
number of parameters)  the same kind of generalization to an n-gram LSTM can be made without
increasing the number of learned parameters. We do so for all of the recurrent cell types in Table
1  with the CNN corresponding to the original SyncNet model. Compared to the original SyncNet
model  our newly proposed models can jointly consider the time dependency within the whole signal.
The mean classiï¬cation accuracies across all mice are compared in Table 4  where we observe
substantial improvements in prediction accuracy through the addition of memory cells to the model.
Thus  considering the time dependency in the neural signal appears to be beneï¬cial for identifying
hidden patterns. Classiï¬cation performances per subject (Figure 4) can be found in Appendix A.
7 Conclusions
The principal contribution of this paper is a new perspective on gated RNNs  leveraging concepts
from recurrent kernel machines. From that standpoint  we have derived a model closely connected
to the LSTM [15  13] (for convolutional ï¬lters of length one)  and have extended such models to
convolutional ï¬lters of length greater than one  yielding a generalization of the LSTM. The CNN
[18  37  17]  Gated CNN [10] and RAN [20] models are recovered as special cases of the developed
framework. We have demonstrated the efï¬cacy of the derived models on NLP and neuroscience tasks 
for which our RKM variants show comparable or better performance than the LSTM. In particular 
we observe that extending LSTM variants with convolutional ï¬lters of length greater than one can
signiï¬cantly improve the performance in LFP classiï¬cation relative to recent prior work.

Acknowledgments
The research reported here was supported in part by DARPA  DOE  NIH  NSF and ONR.

References
[1] Fabio Anselmi  Lorenzo Rosasco  Cheston Tan  and Tomaso Poggio. Deep Convolutional

Networks are Hierarchical Kernel Machines. arXiv:1508.01084  2015.

[2] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E. Hinton.

arXiv:1607.06450  2016.

Layer Normalization.

[3] David Balduzzi and Muhammad Ghifary. Strongly-Typed Recurrent Neural Networks. Interna-

tional Conference on Machine Learning  2016.

[4] Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert spaces in Probability

and Statistics. Kluwer Publishers  2004.

[5] Alberto Bietti and Julien Mairal. Invariance and Stability of Deep Convolutional Representations.

Neural Information Processing Systems  2017.

[6] James Bradbury  Stephen Merity  Caiming Xiong  and Richard Socher. Quasi-recurrent neural

networks. International Conference of Learning Representations  2017.

9

[7] Mia Xu Chen  Orhan Firat  Ankur Bapna  Melvin Johnson  Wolfgang Macherey  George
Foster  Llion Jones  Niki Parmar  Mike Schuster  Zhifeng Chen  Yonghui Wu  and Macduff
Hughes. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.
arXiv:1804.09849v2  2018.

[8] Jianpeng Cheng and Mirella Lapata. Neural Summarization by Extracting Sentences and Words.

Association for Computational Linguistics  2016.

[9] Kyunghyun Cho  Bart van Merrienboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares 
Holger Schwenk  and Yoshua Bengio. Learning Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. Empirical Methods in Natural Language Process-
ing  2014.

[10] Yann N. Dauphin  Angela Fan  Michael Auli  and David Grangier. Language Modeling with

Gated Convolutional Networks. International Conference on Machine Learning  2017.

[11] Marc G. Genton. Classes of Kernels for Machine Learning: A Statistics Perspective. Journal of

Machine Learning Research  2001.

[12] David Golub and Xiaodong He. Character-Level Question Answering with Attention. Empirical

Methods in Natural Language Processing  2016.

[13] Klaus Greff  Rupesh Kumar Srivastava  Jan KoutnÃ­k  Bas R. Steunebrink  and JÃ¼rgen Schmidhu-
ber. LSTM: A Search Space Odyssey. Transactions on Neural Networks and Learning Systems 
2017.

[14] Michiel Hermans and Benjamin Schrauwen. Recurrent Kernel Machines: Computing with

Inï¬nite Echo State Networks. Neural Computation  2012.

[15] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long Short-Term Memory. Neural Computation 

1997.

[16] Rafal Jozefowicz  Wojciech Zaremba  and Ilya Sutskever. An Empirical Exploration of Recur-

rent Network Architectures. International Conference on Machine Learning  2015.

[17] Yoon Kim. Convolutional Neural Networks for Sentence Classiï¬cation. Empirical Methods in

Natural Language Processing  2014.

[18] Yann LeCun and Yoshua Bengio. Convolutional Networks for Images  Speech  and Time Series.

The Handbook of Brain Theory and Neural Networks  1995.

[19] Yann LeCun  LÃ©on Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based Learning

Applied to Document Recognition. Proceedings of IEEE  1998.

[20] Kenton Lee  Omer Levy  and Luke Zettlemoyer.

arXiv:1705.07393v2  2017.

Recurrent Additive Networks.

[21] Tao Lei  Wengong Jin  Regina Barzilay  and Tommi Jaakkola. Deriving Neural Architectures

from Sequence and Graph Kernels. International Conference on Machine Learning  2017.

[22] Yitong Li  Michael Murias  Samantha Major  Geraldine Dawson  Kafui Dzirasa  Lawrence
Carin  and David E. Carlson. Targeting EEG/LFP Synchrony with Neural Nets. Neural
Information Processing Systems  2017.

[23] Julien Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks.

Neural Information Processing Systems  2016.

[24] Mitchell P. Marcus  Beatrice Santorini  and Mary Ann Marcinkiewicz. Building a Large
Annotated Corpus of English: The Penn Treebank. Association for Computational Linguistics 
1993.

[25] Stephen Merity  Nitish Shirish Keskar  and Richard Socher. Regularizing and Optimizing LSTM

Language Models. International Conference on Learning Representations  2018.

10

[26] Stephen Merity  Caiming Xiong  James Bradbury  and Richard Socher. Pointer Sentinel Mixture

Models. International Conference of Learning Representations  2017.

[27] Jeffrey Pennington  Richard Socher  and Christopher D. Manning. GloVe: Global Vectors for

Word Representation. Empirical Methods in Natural Language Processing  2014.

[28] Christopher Roth  Ingmar Kanitscheider  and Ila Fiete. Kernel rnn learning (kernl). International

Conference Learning Representation  2019.

[29] Bernhard Scholkopf and Alexander J. Smola. Learning with kernels. MIT Press  2002.

[30] Corentin Tallec and Yann Ollivier. Can Recurrent Neural Networks Warp Time? International

Conference of Learning Representations  2018.

[31] Aaron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex
Graves  Nal Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. WaveNet: A Generative
Model for Raw Audio. arXiv:1609.03499  2016.

[32] Jordy van Enkhuizen  Arpi Minassian  and Jared W Young. Further evidence for Clockâˆ†19 mice
as a model for bipolar disorder mania using cross-species tests of exploration and sensorimotor
gating. Behavioural Brain Research  249:44â€“54  2013.

[33] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N. Gomez 
Lukasz Kaiser  and Illia Polosukhin. Attention Is All You Need. Neural Information Processing
Systems  2017.

[34] Guoyin Wang  Chunyuan Li  Wenlin Wang  Yizhe Zhang  Dinghan Shen  Xinyuan Zhang  Ri-
cardo Henao  and Lawrence Carin. Joint Embedding of Words and Labels for Text Classiï¬cation.
Association for Computational Linguistics  2018.

[35] Jin Wang  Liang-Chih Yu  K. Robert Lai  and Xuejie Zhang. Dimensional Sentiment Analysis

Using a Regional CNN-LSTM Model. Association for Computational Linguistics  2016.

[36] Andrew Gordon Wilson  Zhiting Hu  Ruslan Salakhutdinov  and Eric P. Xing. Deep Kernel

Learning. International Conference on Artiï¬cial Intelligence and Statistics  2016.

[37] Xiang Zhang  Junbo Zhao  and Yann LeCun. Character-level Convolutional Networks for Text

Classiï¬cation. Neural Information Processing Systems  2015.

[38] Chunting Zhou  Chonglin Sun  Zhiyuan Liu  and Francis C.M. Lau. A C-LSTM Neural Network

for Text Classiï¬cation. arXiv:1511.08630  2015.

11

A More Details of the LFP Experiment

model [22] into our framework  the weight W (x) = (cid:2)W (x0)  W (xâˆ’1) Â·Â·Â·   W (xâˆ’n+1)(cid:3) deï¬ned in

In this section  we provide more details on the Sync-RKM model. In order to incorporate the SyncNet

Eq. (12) is parameterized as wavelet ï¬lters. If there is a total of K ï¬lters  then W (x) is of size
K Ã— C Ã— n.
Speciï¬cally  suppose the n-gram input data at time t is given as Xt = [xtâˆ’n+1 Â·Â·Â·   xt] âˆˆ RCÃ—n
with channel number C and window size n. The k-th ï¬lter for channel c can be written as

W (x)

kc = Î±kc cos (Ï‰kt + Ï†kc) exp(âˆ’Î²kt2)

(21)

W (x)
kc has the form of the Morlet wavelet base function. Parameters to be learned are Î±kc  Ï‰k  Ï†kc
and Î²k for c = 1 Â·Â·Â· C and k = 1 Â·Â·Â·   K. t is a time grid of length n  which is a constant vector.
In the recurrent cell  each W (x)
is convolved with the c-th channel of Xt using 1-d convolution.
kc
Figure 3 gives the framework of this Sync-RKM model. For more details of how the ï¬lter works 
please refer to the original work [22].

Figure 3: Illustration of the proposed model with SyncNet ï¬lters. The input LFP signal is given by
the C Ã— T matrix. The SyncNet ï¬lters (right) are applied on signal chunks at each time step.

When applying the Sync-RKM model on LFP data  we choose the window size as n = 40 to consider
the time dependencies in the signal. Since the experiment is performed by treating each mouse as test
iteratively  we show the subject-wise classiï¬cation accuracy in Figure 4. The proposed model does
consistently better across nearly all subjects.

Figure 4: Subject-wise classiï¬cation accuracy comparison for LFP dataset.

12

ğ‘ªğ‘»1ğ‘¡ğ‘‡Sync-RKMSync-RKMSync-RKMâ€¦Sync-RKMSync-RKMâ€¦23ğ’‰ğ’•=ğ’‡ğ‘¿ğ’•+ğ‘¾(ğ’‰)ğ’‰ğ’•âˆ’ğŸ+ğ’ƒğ‘¾(ğ’™),Kevin Liang
Guoyin Wang
Yitong Li
Ricardo Henao
Lawrence Carin