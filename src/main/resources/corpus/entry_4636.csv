2019,Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems,Restless bandit problems are instances of non-stationary multi-armed bandits. These problems have been studied well from the optimization perspective  where the goal is to efficiently find a near-optimal policy when system parameters are known. However  very few papers adopt a learning perspective  where the parameters are unknown. In this paper  we analyze the performance of Thompson sampling in episodic restless bandits with unknown parameters. We consider a general policy map to define our competitor and prove an $\tilde{\bigO}(\sqrt{T})$ Bayesian regret bound. Our competitor is flexible enough to represent various benchmarks including the best fixed action policy  the optimal policy  the Whittle index policy  or the myopic policy. We also present empirical results that support our theoretical findings.,Regret Bounds for Thompson Sampling in

Episodic Restless Bandit Problems

Young Hun Jung

Department of Statistics
University of Michigan
yhjung@umich.edu

Ambuj Tewari

Department of Statistics
University of Michigan
tewaria@umich.edu

Abstract

Restless bandit problems are instances of non-stationary multi-armed bandits.
These problems have been studied well from the optimization perspective  where
the goal is to efﬁciently ﬁnd a near-optimal policy when system parameters are
known. However  very few papers adopt a learning perspective  where the parame-
ters are unknown. In this paper  we analyze the performance of Thompson sampling
√
in episodic restless bandits with unknown parameters. We consider a general policy
map to deﬁne our competitor and prove an ˜O(
T ) Bayesian regret bound. Our
competitor is ﬂexible enough to represent various benchmarks including the best
ﬁxed action policy  the optimal policy  the Whittle index policy  or the myopic
policy. We also present empirical results that support our theoretical ﬁndings.

1

Introduction

Restless bandits [Whittle  1988] are variants of multi-armed bandit (MAB) problems [Robbins  1952].
Unlike the classical MABs  the arms have non-stationary reward distributions. Speciﬁcally  we
will focus on the class of restless bandits whose arms change their states based on Markov chains.
Restless bandits are also distinguished from rested bandits where only the active arms evolve and
the passive arms remain frozen. We will assume that each arm changes according to two different
Markov chains depending on whether it is played or not. Because of their extra ﬂexibility in modeling
non-stationarity  restless bandits have been applied to practical problems such as dynamic channel
access problems [Liu et al.  2011  2013] and online recommendation systems [Meshram et al.  2017].
Due to the arms’ non-stationary nature  playing the same set of arms for every round usually
does not produce the optimal performance. This makes the optimal policy highly non-trivial  and
Papadimitriou and Tsitsiklis [1999] show that it is generally PSPACE hard to identify the optimal
policy for restless bandits. As a consequence  many researchers have been devoted to ﬁnd an efﬁcient
way to approximate the optimal policy [Liu and Zhao  2010  Meshram et al.  2018]. This line of work
primarily focuses on the optimization perspective in that the system parameters are already known.
Since the true system parameters are unavailable in many cases  it becomes important to examine
restless bandits from a learning perspective. Due to the learner’s additional uncertainty  however 
analyzing a learning algorithm in restless bandits is signiﬁcantly challenging. Liu et al. [2011  2013]
and Tekin and Liu [2012] prove O(log T ) bounds for conﬁdence bound based algorithms  but their
competitor always selects a ﬁxed set of actions  which is known to be weak (see Section 5 for an
empirical example of the weakness of the best ﬁxed action competitor). Dai et al. [2011  2014] show
O(log T ) bounds against the optimal policy  but their assumptions on the underlying model are very
limited. Ortner et al. [2012] prove an ˜O(
T ) bound in general restless bandits  but their algorithm is
intractable in general.

√

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

√
In a different line of work  Osband et al. [2013] study Thompson sampling in the setting of a fully
observable Markov decision process (MDP) and show the Bayesian regret bound of ˜O(
T ) (hiding
dependence on system parameters like state and action space size). Unfortunately  this result is not
applicable in our setting as ours is partially observable due to bandit feedback. Following Ortner et al.
[2012]  it is possible to transform our setting to the fully observable case  but then we end up having
exponentially many states  which restricts the practical utility of existing results.
In this work  we analyze Thompson sampling in restless bandits where the system resets at the end of
every ﬁxed-length episode and the rewards are binary. We emphasize that this episodic assumption
simpliﬁes our analysis as the problem boils down to a ﬁnite time horizon problem. This assumption
can be arguably limited  but there are applications such as dynamic channel access problems where
the channel provider might reset their system every night for a maintenance-related reason and
the episodic assumption becomes natural. We directly tackle the partial observability and achieve
a meaningful regret bound  which when restricted to the classical MABs matches the Thompson
sampling result in that setting. We are not the ﬁrst to analyze Thompson sampling in restless bandits 
and Meshram et al. [2016] study this type of algorithm as well  but their regret analysis remains in
the one-armed-case with a ﬁxed reward of not pulling the arm. They explicitly mention that a regret
analysis of Thompson sampling in the multi-armed case is an interesting open question.

  depending on whether the learner pulled the arm or not.

2 Problem setting
We begin by introducing our setting. There are K arms indexed by k = 1 ···   K  and the algorithm
selects N arms every round. We denote the learner’s action at time t by a binary vector At ∈ {0  1}K
where ||At||1 = N. We call the selected arms as active and the rest as passive. We assume each arm
k has binary states  {0  1}  which evolve as a Markov chain with transition matrix either P active
or
P passive
k
At round t  pulling an arm k incurs a binary reward Xt k  which is the arm’s current state. As we are
in the bandit setting  the learner only observes the rewards of active arms  which we denote by Xt At 
and does not observe the passive arms’ rewards nor their states. This feature makes our setting to be
a partially observable Markov decision process  or POMDP. We denote the history of the learner’s
actions and rewards up to time t by Ht = (A1  X1 A1  ···   At  Xt At).
We assume the system resets every episode of length L  which is also known to the learner. This means
that at the beginning of each episode  the states of the arms are drawn from an initial distribution. The
entire time horizon is denoted by T   and for simplicity  we assume it is a multiple of L  or T = mL.

k

2.1 Bayesian regret and competitor policy
Let θ ∈ Θ denote the entire parameters of the system. It includes transition matrices P active and
P passive  and an initial distribution of each arm’s state. The learner only knows the prior distribution
of this parameter at the beginning and does not have access to the exact value.
In order to deﬁne a regret  we need a competitor policy  or a benchmark. We ﬁrst deﬁne a class of
deterministic policies and policy mappings.
Deﬁnition 1. A deterministic policy π takes time index and history (t Ht−1) as an input and outputs
a ﬁxed action At = π(t Ht−1). A deterministic policy mapping µ takes a system parameter θ as an
input and outputs a deterministic policy π = µ(θ).

We ﬁx a deterministic policy mapping µ and let our algorithm compete against a deterministic policy
π(cid:63) = µ(θ(cid:63))  where θ(cid:63) represents the true system parameter  which is unknown to the learner.
We keep our competitor policy abstract mainly because we are in the non-stationary setting. Unlike
the classical (stationary) MABs  pulling the same set of arms with the largest expected rewards is not
necessarily optimal. Moreover  it is in general PSPACE hard to compute the optimal policy when θ(cid:63)
is given. Regarding these statements  we refer the readers to the book by Gittins et al. [1989]. As
a consequence  researchers have identiﬁed conditions that the (efﬁcient) myopic policy is optimal
[Ahmad et al.  2009] or proven that a tractable index-based policy has a reasonable performance
against the optimal policy [Liu and Zhao  2010].

2

Draw a parameter θl ∼ Ql and compute the policy πl = µ(θl)
Set H0 = ∅
for t = 1 ···   L do

Algorithm 1 Thompson sampling in restless bandits
1: Input prior Q  episode length L  policy mapping µ
2: Initialize posterior Q1 = Q  history H = ∅
3: for episodes l = 1 ···   m do
4:
5:
6:
7:
8:
9:
10:
11: end for

Select N active arms At = πl(t Ht−1)
Observe rewards Xt At and update Ht

end for
Append HL to H and update posterior distribution Ql+1 using H

We observe that most of proposed policies including the optimal policy  the myopic policy  or the
index-based policy are deterministic. Therefore  researchers can plug in whatever competitor policy
of their choice  and our regret bound will apply as long as the chosen policy mapping is deterministic.
Before deﬁning the regret  we introduce a value function

π i(H) = Eθ π[
V θ

Aj · Xj|H].

(1)

L(cid:88)

j=i

T(cid:88)

t=1

m(cid:88)

π(cid:63) 1(∅) − Eθ(cid:63)
m(cid:88)

This is the expected reward of running a policy π from round i to L where the system parameter
is θ and the starting history is H. Note that the benchmark policy π(cid:63) obtains V θ(cid:63)
π(cid:63) 1(∅) rewards per
episode in expectation. Thus  we can deﬁne the regret as

R(T ; θ(cid:63)) = mV θ(cid:63)

At · Xt.

(2)

If an algorithm chooses to ﬁx a policy πl for the entire episode l  which is the case of our algorithm 
then the regret can be written as

R(T ; θ(cid:63)) = mV θ(cid:63)

π(cid:63) 1(∅) − Eθ(cid:63)

πl 1(∅) = Eθ(cid:63)
V θ(cid:63)

π(cid:63) 1(∅) − V θ(cid:63)
V θ(cid:63)

πl 1(∅).

l=1

l=1

We particularly focus on the case where θ(cid:63) is a random and bound the following Bayesian regret 

BR(T ) = Eθ(cid:63)∼QR(T ; θ(cid:63)) 

where Q is a prior distribution over the set of system parameters Θ. We assume that the prior is
known to the learner. We caution our readers that there is at least one other regret deﬁnition in the
literature  which is called either frequentist regret or worst-case regret. For this type of regret  one
views θ(cid:63) as a ﬁxed unknown object and directly bounds R(T ; θ(cid:63)). Even though our primary interest
is to bound the Bayesian regret  we can establish a connection to the frequentist regret in the special
case where the prior Q has a ﬁnite support and the benchmark is the optimal policy (see Corollary 6).

3 Algorithm

Our algorithm is an instance of Thompson sampling or posterior sampling  ﬁrst proposed by Thomp-
son [1933]. At the beginning of episode l  the algorithm draws a system parameter θl from the
posterior and plays πl = µ(θl) throughout the episode. Once an episode is over  it updates the
posterior based on additional observations. Algorithm 1 describes the steps.
We want to point out that the history H fulﬁlls two different purposes. One is to update the posterior
Ql  and the other is as an input to a policy πl. For the latter  however  we do not need the entire
history as the arms reset every episode. That is why we set H0 = ∅ (step 5) and feed Ht−1 to πl (step
7). Furthermore  as we assume that the arms evolve based on Markov chains  the history Ht−1 can
be summarized as

(3)

(r1  n1 ···   rK  nK) 

3

which means that an arm k is played nk rounds ago and rk is the observed reward in that round. If
an arm k is never played in the episode  then nk becomes t  and rk becomes the expected reward
from the initial distribution based on θl. As we assume the episode length is ﬁxed to be L  there are
L possible values for nk. Due to the binary reward assumption  rk can take three values including
the case where the arm k is never played. From these  we can infer that there are (3L)K possible
tuples of (r1  n1 ···   rK  nK). By considering these tuples as states and following the reasoning
of Ortner et al. [2012]  one can view our POMDP as a fully observable MDP. Then one can use the
existing algorithms for fully observable MDPs (e.g.  Osband et al. [2013])  but the regret bounds
easily become vacuous since the number of states depends exponentially on the number of arms K.
Additionally  as we assumed a policy mapping  one might argue to use existing expert learning or
classical MAB algorithms considering potential policies as experts or arms. This is possible  but the
number of potential policies corresponds to the size of Θ  which can be very large or even inﬁnite.
For this reason  existing algorithms are not efﬁcient and/or their regret bounds become too loose.
Due to its generality  it is hard to analyze the time and space complexity of Algorithm 1. Two major
steps are computing the policy (step 4) and updating posterior (step 10). Computing the policy
depends on our choice of competitor mapping µ. If the competitor policy has better performance but
is harder to compute  then our regret bound gets more meaningful as the benchmark is stronger  but the
running time gets longer. Regarding the posterior update  the computational burden depends on the
choice of the prior Q and its support. If there is a closed-form update  then the step is computationally
cheap  but otherwise the burden increases with respect to the size of the support.

4 Regret bound
In this section  we bound the Bayesian regret of Algorithm 1 by ˜O(
T ). A key idea in our analysis of
Thompson sampling is that the distributions of θ(cid:63) and θl are identical given the history up to the end
of episode l − 1 (e.g.  see Lattimore and Szepesvári  Chp. 36). To state it more formally  let σ(H) be
the σ-algebra generated by the history H. Then we call a random variable X is σ(H)-measurable  or
simply H-measurable  if its value is deterministically known given the information σ(H). Similarly 
we call a random function f is H-measurable if its mapping is deterministically known given σ(H).
We record as a lemma an observation made by Russo and Van Roy [2014].
Lemma 2. (Expectation identity) Suppose θ(cid:63) and θl have the same distribution given H. For any
H-measurable function f  we have

√

E[f (θ(cid:63))|H] = E[f (θl)|H].

Recall that we assume the competitor mapping µ is deterministic. Furthermore  the value function
π i(∅) in (1) is deterministic given θ and π. This implies E[V θ(cid:63)
π(cid:63) i(∅)|H] = E[V θl
πl i(∅)|H]  where
V θ
H is the history up to the end of episode l − 1. This observation leads to the following regret
decomposition.
m(cid:88)
Lemma 3. (Regret decomposition) The Bayesian regret of Algorithm 1 can be decomposed as
πl 1(∅)].

πl 1(∅)] = Eθ(cid:63)∼Q

BR(T ) = Eθ(cid:63)∼Q

π(cid:63) 1(∅) − V θ(cid:63)

πl 1(∅) − V θ(cid:63)

Eθl∼Ql[V θ(cid:63)

Eθl∼Ql [V θl

m(cid:88)

l=1

l=1

Proof. The ﬁrst equality is a simple rewriting of (2) because Algorithm 1 ﬁxes a policy πl for the
entire episode l. Then we apply Lemma 2 along with the tower rule to get

Eθ(cid:63)∼Q

Eθl∼Ql V θ(cid:63)

π(cid:63) 1(∅) = Eθ(cid:63)∼Q

Eθl∼Ql V θl

πl 1(∅).

l=1

l=1

πl 1(∅)
Note that we can compute V θl
from the algorithm’s observations. The main point of Lemma 3 is to rewrite the Bayesian regret using
terms that are relatively easy to analyze.
Next  we deﬁne the Bellman operator

πl 1(∅) as we know θl and πl. We can also infer the value of V θ(cid:63)

T θ
π V (Ht−1) = Eθ π[At · Xt + V (Ht)|Ht−1].
π i = T θ
It is not hard to check that V θ

π V θ

π i+1. The next lemma further decomposes the regret.

4

m(cid:88)

m(cid:88)

Lemma 4. (Per-episode regret decomposition) Fix θ(cid:63) and θl  and let H0 = ∅. Then we have

πl 1(H0) − V θ(cid:63)
V θl

πl 1(H0) = Eθ(cid:63) πl

(T θl

πl

− T θ(cid:63)

πl

πl t+1(Ht−1).
)V θl

L(cid:88)

t=1

Proof. Using the relation V θ
πl 1(H0) − V θ(cid:63)
V θl

π i = T θ
π V θ
πl 1(H0) = (T θl
= (T θl

πl

The second term can be written as Eθ(cid:63) πl [(V θl
obtain the desired equation.

π i+1  we may write
πl 2)(H0)
πl 2 − T θ(cid:63)
V θ(cid:63)
V θl
πl
πl 2(H0) + T θ(cid:63)
− T θ(cid:63)
)V θl
πl 2 − V θ(cid:63)
πl 2)(H1)|H0]  and we can repeat this L times to

πl 2 − V θ(cid:63)
(V θl

πl 2)(H0).

πl

πl

πl

Now we are ready to prove our main theorem. A complete proof can be found in Appendix A.
Theorem 5. (Bayesian regret bound of the Thompson sampling) The Bayesian regret of Algo-
rithm 1 satisﬁes the following bound

KL3N 3T log T ) = O((cid:112)mKL4N 3 log(mL)).

BR(T ) = O(

(cid:112)

√

√
Remark. If the system is the classical stationary MAB  then it corresponds to the case L = 1  N = 1 
and our result reproduces the result of O(
KT log T ) [Lattimore and Szepesvári  Chp. 36]. This
suggests our bound is optimal in K and T up to a logarithmic factor. Further  when N > K
2   we
can think of the problem as choosing the passive arms  and the smaller bound with N replaced by
K − N would apply. When L = 1  the problem becomes combinatorial bandits of choosing N
active arms out of K. Cesa-Bianchi and Lugosi [2012] propose an algorithm with a regret bound
O(
KN T log K) with an assumption that the loss is always bounded by 1. Since our reward can
be as big as N  our bound has the same dependence on N with theirs  suggesting tight dependence
of our bound on N.
Proof Sketch. We ﬁx an episode l and analyze the regret in this episode. Let tl = (l − 1)L so that
t=1 1{At k = 1  rk = r  nk = n}  which
counts the number of rounds where the arm k was chosen by the learner with history rk = r and
nk = n (see (3) for deﬁnition). Note that k ∈ [K]  r ∈ {0  1  ρ(k)}  and n ∈ [L]  where ρ(k) is the
initial success rate of the arm k. This implies there are 3KL tuples of (k  r  n).
Let ωθ(k  r  n) denote the conditional probability of Xk = 1 given a history (r  n) and a system
parameter θ. Also let ˆω(k  r  n) denote the empirical mean of this quantity (using Nl(k  r  n) past
observations and set the estimate to 0 if Nl(k  r  n) = 0). Then deﬁne

the episode starts at time tl + 1. Deﬁne Nl(k  r  n) =(cid:80)tl

Θl =

θ | ∀(k  r  n) 

|(ˆω − ωθ)(k  r  n)| <

2 log(1/δ)
1 ∨ Nl(k  r  n)

Since ˆω(k  r  n) is Htl-measurable  so is the set Θl. Using the Hoeffding inequality  one can show
P(θ(cid:63) /∈ Θl) = P(θl /∈ Θl) ≤ 3δKL. In other words  we can claim that with high probability 
|ωθl (k  r  n) − ωθ(cid:63)
We now turn our attention to the following Bellman operator

(k  r  n)| is small for all (k  r  n).

T θ

πl

πl t(Ht−1) = Eθ πl [Atl+t · Xtl+t + V θl
V θl

πl t(Ht)|Ht−1].

Since πl is a deterministic policy  Atl+t is also deterministic given Ht−1 and πl. Let (k1  . . .   kN )
be the active arms at time tl + t and write ωθ(ki  rki  nki ) = ωθ i. Then we can rewrite

(cid:115)

(cid:41)

.

(cid:40)

N(cid:88)

i=1

(cid:88)

x∈{0 1}N

x =(cid:81)N

where P θ

i=1 ωxi

T θ

πl

πl t(Ht−1) =
V θl

ωθ i +

x V θl
P θ

πl t(Ht−1 ∪ (Atl+t  x)) 

θ i(1 − ωθ i)1−xi. Under the event that θ(cid:63)  θl ∈ Θl  we have
|ωθl i − ωθ(cid:63) i| < 1 ∧

=: ∆i(tl + t) 

8 log(1/δ)

1 ∨ Nl(ki  rki  nki)

(cid:115)

5

where the dependence on tl + t comes from the mapping from i to ki. When ωθl i and ωθ(cid:63) i are close
for all (k  r  n)  we can actually bound the difference between the following Bellman operators as

|(T θ(cid:63)

πl

− T θl

πl

Then by applying Lemma 4  we get |V θl
i=1 ∆i(tl + t) 
which holds whenever θ(cid:63)  θl ∈ Θl. When θ(cid:63) /∈ Θl or θl /∈ Θl  which happens with probability less
than 6δKL  we have a trivial bound |V θl

πl t(Ht−1)| ≤ 3LN
)V θl
πl 1(∅) − V θ(cid:63)
πl 1(∅) − V θ(cid:63)

(cid:80)L
∆i(tl + t).
πl 1(∅)| ≤ 3LNEθ(cid:63) πl
πl 1(∅)| ≤ LN. We can deduce

(cid:80)N

t=1

i=1

N(cid:88)

L(cid:88)

N(cid:88)

πl 1(∅) − V θ(cid:63)
|V θl

πl 1(∅)| ≤ 3LN1(θ(cid:63)  θl ∈ Θl)Eθ(cid:63) πl

∆i(tl + t) + 6δKL2N.

Combining this with Lemma 3  we can show
BR(T ) ≤ 6δmKL2N + Eθ(cid:63)∼Q3LN

t=1

i=1

1(θ(cid:63)  θl ∈ Θl)Eθ(cid:63) πl

m(cid:88)

l=1

L(cid:88)

N(cid:88)

t=1

i=1

∆i(tl + t).

(4)

After some algebra  bounding sums of ﬁnite differences by integrals  and applying the Cauchy-
Schwartz inequality  we can bound the second summation by

18KL3N + 24(cid:112)3KL3N 3T log(1/δ).

(5)

Combining (4)  (5)  and our assumption that T = mL  we obtain

BR(T ) = O(δKLN T + KL3N +(cid:112)KL3N 3T log(1/δ)).

Since N T is a trivial upper bound of BR(T )  we may ignore the KL3N term. Setting δ = 1
T
completes the proof.

As discussed in Section 2  researchers sometimes pay more attention to the case where the true
parameter θ(cid:63) is deterministically ﬁxed in advance  in which the frequentist regret becomes more
relevant. It is not easy to directly extend our analysis to the frequentist regret in general  but we
can achieve a meaningful bound with extra assumptions. Suppose our prior Q is discrete and the
competitor is the optimal policy. Then we know R(T ; θ(cid:63)) is always non-negative due to the optimality
of the benchmark and can deduce qR(T ; θ(cid:63)) ≤ BR(T )  where q is the probability mass on θ(cid:63). This
leads to the following corollary.
Corollary 6. (Frequentist regret bound of Thompson sampling) Suppose the prior Q is discrete
and puts a non-zero mass on the parameter θ(cid:63). Additionally  assume that the competitor policy is the
optimal policy. Then Algorithm 1 satisﬁes the following bound

(cid:112)
KL3N 3T log T ) = O((cid:112)mKL4N 3 log(mL)).

R(T ; θ(cid:63)) = O(

5 Experiments

We empirically investigate the Gilbert-Elliott channel model  which is studied by Liu and Zhao [2010]
in a restless bandit perspective1. This model can be broadly used in communication systems such as
cognitive radio networks  downlink scheduling in cellular systems  opportunistic transmission over
fading channels  and resource-constrained jamming and anti-jamming.
Each arm k has two parameters pk
11  which determine the transition matrix. We assume
P active = P passive and each arm’s transition matrix is independent on the learner’s action. There
are only two states  good and bad  and the reward of playing an arm is 1 if its state is good and 0
otherwise. Figure 1 summarizes this model. We assume the initial distribution of an arm k follows
the stationary distribution. In other words  its initial state is good with probability ωk =
.

01 and pk

pk
01

01+1−pk
pk

11

We ﬁx L = 50 and m = 30. We use Monte Carlo simulation with size 100 or greater to approximate
expectations. As each arm has two parameters  there are 2K parameters. For these  we set the prior
distribution to be uniform over a ﬁnite support {0.1  0.2 ···   0.9}.

1Our code is available at https://github.com/yhjung88/ThompsonSamplinginRestlessBandits

6

Figure 1: The Gilbert-Elliott channel model

5.1 Competitors

As mentioned earlier  one important strength of our result is that various policy mappings can be used
as benchmarks. Here we test three different policies: the best ﬁxed arm policy  the myopic policy 
and the Whittle index policy. We want to emphasize again that these competitor policies know the
system parameters while our algorithm does not.

pk
01

11

01+1−pk
pk

The best ﬁxed arm policy computes the stationary distribution ωk =
for all k and pulls the
arms with top N values. The myopic policy keeps updating the belief ωk(t) for the arm k being in a
good state and pulls the top N arms. Finally  the Whittle index policy computes the Whittle index
of each arm and uses it to rank the arms. The Whittle index is proposed by Whittle [1988]  and Liu
and Zhao [2010] ﬁnd a closed-form formula to compute the Whittle index in this particular setting.
The Whittle index policy is very popular in optimization literature as it decouples the optimization
process into K independent problems for each arm  which signiﬁcantly reduces the computational
complexity while maintaining a reasonable performance against the optimal policy.
One observation is that these three policies are reduced to the best ﬁxed arm policy in the stationary
case. However  the ﬁrst two policies are known to be sub-optimal in general [Gittins et al.  1989].
Liu and Zhao [2010] justify both theoretically and empirically the performance of the Whittle index
policy for the Gilbert-Elliott channel model.

5.2 Results

π 1(∅)
We ﬁrst analyze the Bayesian regret. For this  we use K = 8 and N = 3. The value functions V θ
of the best ﬁxed arm policy  the myopic policy  and the Whittle index policy are 105.4  110.3  and
111.4  respectively. If a competitor policy has a weak performance  then Thompson sampling also
uses this weak policy mapping to get a policy πl for the episode l. This implies that the regret does
not necessarily become negative when the benchmark policy is weak. Figure 2 shows the trend of the
Bayesian regret as a function of episode indices. Regardless of the choice of policy mapping  the
regret is sub-linear  and the slope of log-log plot is less than 1

2  which agrees with Theorem 5.

Figure 2: Bayesian regret of Thompson sampling versus episode (left) and its log-log plot (right)

Next we ﬁx true parameters and investigate the model’s behavior more closely. For this  we choose
K = 4  N = 2  and {(pk
11)}k=1 2 3 4 = {(0.3  0.7)  (0.4  0.6)  (0.5  0.5)  (0.6  0.4)}. This

01  pk

7

choice results in ωk = 0.5 for all k  and the best ﬁxed arm policy becomes indifferent. Therefore
achieving zero regret against the best ﬁxed arm becomes trivial. We use the same uniform prior as
the previous experiment. Figure 3 presents the trend of value functions and how Thompson sampling
puts more posterior weights on the correct parameters as it proceeds. Three horizontal lines in the
left ﬁgure represent the values of the competitor policies. The values of the best ﬁxed arm policy 
the myopic policy  and the Whittle index policy are 50.2  54.6  and 55.6  respectively. It is a good
example why one should not pull the same arms all the time in restless bandits. The value function of
Thompson sampling successfully converges to the competitor value for every benchmark while the
one with the myopic policy needs more episodes to fully converge. This supports Corollary 6 in that
our model can be used even in the non-Bayesian setting as far as the prior has a non-zero weight on
the true parameters. Also  the posterior weights on the correct parameters monotonically increase
(Figure 3  right)  which again conﬁrms our model’s performance. We measure these weights when
the competitor map is the Whittle index policy.

Figure 3: Average per-episode value versus episode and the benchmark values (left); the posterior
weights of the correct parameters versus episode in the case of the Whittle index policy (right)

6 Discussion and future directions

√
In this paper  we have analyzed Thompson sampling in restless bandits with binary rewards. The
Bayesian regret can be theoretically bounded as ˜O(
T )  which naturally extends the results in
the stationary MAB. One primary strength of our analysis is that the bound applies to arbitrary
deterministic competitor policy mappings  which include the optimal policy and many other practical
policies. Experiments with the simulated Gilbert-Elliott channel models support the theoretical
results. In the special case where the prior has a discrete support and the benchmark is the optimal
policy  our result extends to the frequentist regret  which is also supported by empirical results.
There are at least two interesting directions to be explored.

1. Our setting is episodic with known length L. The system resets periodically  which makes the
analysis of the regret simpler. However  it is sometimes unrealistic to assume this periodic reset
(e.g.  online recommendation system studied by Meshram et al. [2017]). Analyzing a learning
algorithm in the non-episodic setting will be useful.

2. Corollary 6 is not directly applicable in the case of continuous prior. In stationary MABs  it
has been shown that Thompson sampling enjoys the frequentist regret bound of ˜O(
T ) with
additional assumptions [Lattimore and Szepesvári  Chp. 36]. Extending this to the restless bandit
setting will be an interesting problem.

√

Acknowledgments

We acknowledge the support of NSF CAREER grant IIS-1452099. AT was also supported by a Sloan
Research Fellowship. AT visited Criteo AI Lab  Paris and had discussions with Criteo researchers –
Marc Abeille  Clément Calauzènes  and Jérémie Mary – regarding non-stationarity in bandit problems.
These discussions were very helpful in attracting our attention to the regret analysis of restless bandit
problems and the need for considering a variety of benchmark competitors when deﬁning regret.

8

References
Sahand Haji Ali Ahmad  Mingyan Liu  Tara Javidi  Qing Zhao  and Bhaskar Krishnamachari. Opti-
mality of myopic sensing in multichannel opportunistic access. IEEE Transactions on Information
Theory  55(9):4040–4050  2009.

Nicolo Cesa-Bianchi and Gábor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences  78(5):1404–1422  2012.

Wenhan Dai  Yi Gai  Bhaskar Krishnamachari  and Qing Zhao. The non-bayesian restless multi-
armed bandit: A case of near-logarithmic regret. In IEEE International Conference on Acoustics 
Speech and Signal Processing (ICASSP)  pages 2940–2943. IEEE  2011.

Wenhan Dai  Yi Gai  and Bhaskar Krishnamachari. Online learning for multi-channel opportunis-
tic access over unknown markovian channels. In IEEE International Conference on Sensing 
Communication  and Networking (SECON)  pages 64–71. IEEE  2014.

John C Gittins  Kevin D Glazebrook  Richard Weber  and Richard Weber. Multi-armed bandit

allocation indices  volume 25. Wiley Online Library  1989.

Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press. forthcoming.

Haoyang Liu  Keqin Liu  and Qing Zhao. Logarithmic weak regret of non-bayesian restless multi-
armed bandit. In IEEE International Conference on Acoustics  Speech and Signal Processing
(ICASSP)  pages 1968–1971. IEEE  2011.

Haoyang Liu  Keqin Liu  and Qing Zhao. Learning in a changing world: Restless multiarmed bandit

with unknown dynamics. IEEE Transactions on Information Theory  59(3):1902–1916  2013.

Keqin Liu and Qing Zhao. Indexability of restless bandit problems and optimality of whittle index
for dynamic multichannel access. IEEE Transactions on Information Theory  56(11):5547–5567 
2010.

Rahul Meshram  Aditya Gopalan  and D Manjunath. Optimal recommendation to users that react:
Online learning for a class of pomdps. In IEEE 55th Conference on Decision and Control (CDC) 
pages 7210–7215. IEEE  2016.

Rahul Meshram  Aditya Gopalan  and D Manjunath. Restless bandits that hide their hand and
recommendation systems. In IEEE International Conference on Communication Systems and
Networks (COMSNETS)  pages 206–213. IEEE  2017.

Rahul Meshram  D Manjunath  and Aditya Gopalan. On the whittle index for restless multiarmed

hidden markov bandits. IEEE Transactions on Automatic Control  63(9):3046–3053  2018.

Ronald Ortner  Daniil Ryabko  Peter Auer  and Rémi Munos. Regret bounds for restless markov
bandits. In International Conference on Algorithmic Learning Theory  pages 214–228. Springer 
2012.

Ian Osband  Daniel Russo  and Benjamin Van Roy. (more) efﬁcient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems  pages 3003–3011 
2013.

Christos H Papadimitriou and John N Tsitsiklis. The complexity of optimal queuing network control.

Mathematics of Operations Research  24(2):293–305  1999.

Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  58(5):527–535  1952.

Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research  39(4):1221–1243  2014.

Cem Tekin and Mingyan Liu. Online learning of rested and restless bandits. IEEE Transactions on

Information Theory  58(8):5588–5611  2012.

9

William R Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied probability 

25(A):287–298  1988.

10

,Young Hun Jung
Ambuj Tewari