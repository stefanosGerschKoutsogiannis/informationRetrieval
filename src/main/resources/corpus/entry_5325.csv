2009,Submanifold density estimation,Kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation. However  long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades. In practice  it has been recognized that often such data have a much lower-dimensional intrinsic structure. We propose a small modification to kernel density estimation for estimating probability density functions on Riemannian submanifolds of Euclidean space. Using ideas from Riemannian geometry  we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold. We conclude with empirical results demonstrating the behavior predicted by our theory.,Submanifold density estimation

Arkadas Ozakin

Georgia Tech Research Institute
Georgia Insitute of Technology

Alexander Gray

College of Computing

Georgia Institute of Technology

arkadas.ozakin@gtri.gatech.edu

agray@cc.gatech.edu

Abstract

Kernel density estimation is the most widely-used practical method for accurate
nonparametric density estimation. However  long-standing worst-case theoretical
results showing that its performance worsens exponentially with the dimension
of the data have quashed its application to modern high-dimensional datasets for
decades. In practice  it has been recognized that often such data have a much
lower-dimensional intrinsic structure. We propose a small modiﬁcation to ker-
nel density estimation for estimating probability density functions on Riemannian
submanifolds of Euclidean space. Using ideas from Riemannian geometry  we
prove the consistency of this modiﬁed estimator and show that the convergence
rate is determined by the intrinsic dimension of the submanifold. We conclude
with empirical results demonstrating the behavior predicted by our theory.

1 Introduction: Density estimation and the curse of dimensionality

Kernel density estimation (KDE) [8] is one of the most popular methods for estimating the under-
lying probability density function (PDF) of a dataset. Roughly speaking  KDE consists of having
the data points “contribute” to the estimate at a given point according to their distances from the
point. In the simplest multi-dimensional KDE [3]  the estimate ˆfm(y0) of the PDF f (y0) at a point
y0 ∈ RN is given in terms of a sample {y1  . . .   ym} as 

ˆfm(y0) =

1
m

m

Xi=1

1
hN
m

K (cid:18) kyi − y0k

hm

(cid:19)  

(1)

where hm > 0  the bandwidth  is chosen to approach to zero at a suitable rate as the number
m of data points increases  and K : [0.∞) → [0  ∞) is a kernel function that satisﬁes certain
properties such as boundedness. Various theorems exist on the different types of convergence of
the estimator to the correct result and the rates of convergence. The earliest result on the pointwise
convergence rate in the multivariable case seems to be given in [3]  where it is stated that under
certain conditions for f and K  assuming hm → 0 and mhm → ∞ as m → ∞  the mean squared
error in the estimate ˆf (y0) of the density at a point goes to zero with the rate  MSE[ ˆfm(y0)] =
m(cid:17) as m → ∞. If hm is chosen to be proportional to

E(cid:20)(cid:16) ˆfm(y0) − f (y0)(cid:17)2(cid:21) = O(cid:16)h4

m + 1
mhN

m−1/(N +4)  one gets 

MSE[ ˆfm(p)] = O(cid:18)

1

m4/(N +4)(cid:19)  

(2)

as m → ∞. This is an example of a curse of dimensionality; the convergence rate slows as the
dimensionality N of the data set increases. In Table 4.2 of [12]  Silverman demonstrates how the
sample size required for a given mean square error for the estimate of a multivariable normal distri-
bution increases with the dimensionality. The numbers look as discouraging as the formula 2.

1

One source of optimism towards various curses of dimensionality is the fact that although the data
for a given problem may have many features  in reality the intrinsic dimensionality of the “data
subspace” of the full feature space may be low. This may result in there being no curse at all  if
the performance of the method/algorithm under consideration can be shown to depend only on the
intrinsic dimensionality of the data. Alternatively  one may be able to avoid the curse by devising
ways to work with the low-dimensional data subspace by using dimensional reduction techniques
on the data. One example of the former case is the results on nearest neighbor search [6  2] which
indicate that the performance of certain nearest-neighbor search algortihms is determined not by the
full dimensionality of the feature space  but only on the intrinsic dimensionality of the data subspace.

Riemannian manifolds.
In this paper  we will assume that the data subspace is a Riemannian
manifold. Riemannian manifolds provide a generalization of the notion of a smooth surface in R3
to higher dimensions. As ﬁrst clariﬁed by Gauss in the two-dimensional case (and by Riemann in
the general case) it turns out that intrinsic features of the geometry of a surface such as lengths of
its curves or intrinsic distances between its points  etc.  can be given in terms of the so-called metric
tensor1 g without referring to the particular way the the surface is embedded in R3. A space whose
geometry is deﬁned in terms of a metric tensor is called a Riemannian manifold (for a rigorous
deﬁnition  see  e.g.  [5  7  1]).

Previous work.
In [9]  Pelletier deﬁnes an estimator of a PDF on a Riemannian manifold M by
using the distances measured on M via its metric tensor  and obtains the same convergence rate
as in (2)  with N being replaced by the dimensionality of the Riemannian manifold. Thus  if we
know that the data lives on a Riemannian manifold M   the convergence rate of this estimator will
be determined by the dimensionality of M   instead of the full dimensionality of the feature space
on which the data may have been originally sampled. While an interesting generalization of the
usual KDE  this approach assumes that the data manifold M is known in advance  and that we have
access to certain geometric quantities related to this manifold such as intrinsic distances between
its points and the so-called volume density function. Thus  this Riemannian KDE cannot be used
directly in a case where the data lives on an unknown Riemannian submanifold of RN . Certain tools
from existing nonlinear dimensionality reduction methods could perhaps be utilized to estimate
the quantities needed in the estimator of [9]  however  a more straightforward method that directly
estimates the density of the data as measured in the subspace is desirable.

Other related works include [13]  where the authors propose a submanifold density estimation
method that uses a kernel function with a variable covariance but do not present theorerical re-
sults  [4] where the author proposes a method for doing density estimation on a Riemannian man-
ifold by using the eigenfunctions of the Laplace-Beltrami operator  which  as in [9]  assumes that
the manifold is known in advance  together with intricate geometric information pertaining to it  and
[10  11]  which discuss various issues related to statistics on a Riemannian manifold.

This paper.
In this paper  we propose a direct way to estimate the density of Euclidean data that
lives on a Riemannian submanifold of RN with known dimension n < N . We prove the pointwise
consistency of the estimator  and prove bounds on its convergence rates given in terms of the intrinsic
dimension of the submanifold the data lives in. This is an example of the avoidance of the curse of
dimensionality in the manner mentioned above  by a method whose performance depends on the
intrinsic dimensionality of the data instead of the full dimensionality of the feature space. Our
method is practical in that it works with Euclidean distances on RN . In particular  we do not assume
any knowledge of the quantities pertaining to the intrinsic geometry of the underlying submanifold
such as its metric tensor  geodesic distances between its points  its volume form  etc.

2 The estimator and its convergence rate

Motivation.
In this paper  we are concerned with the estimation of a PDF that lives on an (un-
known) n-dimensional Riemannian submanifold M of RN   where N > n. Usual  N -dimensional
kernel density estimation would not work for this problem  since if interpreted as living on RN   the

1The metric tensor can be thought of as giving the “inﬁnitesimal distance” ds between two points whose

coordinates differ by the inﬁnitesimal amounts (dy1  . . .   dyN ) as ds2 = Pij gijdyidyj.

2

underlying PDF would involve a “delta function” that vanishes when one moves away from M   and
“becomes inﬁnite” on M in order to have proper normalization. More formally  the N -dimensional
probability measure for such an n-dimensional PDF on M will have support only on M   will not be
absolutely continuous with respect to the Lebesgue measure on RN   and will not have a probability
density function on RN . If one attempts to use the usual  N -dimensional KDE for data drawn from
such a probability measure  the estimator will “try to converge” to a singular PDF  one that is inﬁnite
on M   zero outside.
In order to estimate the probability density function on M by using data given in RN   we pro-
pose a simple modiﬁcation of usual KDE on RN   namely  to use a kernel that is normalized for
n-dimensions instead of N   while still using the Euclidean distances in RN . The intuition behind
this approach is based on three facts: 1) For small distances  an n-dimensional Riemannian mani-
fold “looks like” Rn  and densities in Rn should be estimated by an n-dimensional kernel  2) For
points of M that are close enough to each other  the intrinsic distances as measured on M are close
to Euclidean distances as measured in RN   and  3) For small bandwidths  the main contribution to
the estimate at a point comes from data points that are nearby. Thus  as the number of data points
increases and the bandwidth is taken to be smaller and smaller  estimating the density by using a
kernel normalized for n-dimensions and distances as measured in RN should give a result closer
and closer to the correct value.

We will next give the formal deﬁnition of the estimator motivated by these considerations  and state
our theorem on its asymptotics. As in the original work of Parzen [8]  the proof that the estimator
is asymptotically unbiased consists of proving that as the bandwidth converges to zero  the kernel
function becomes a “delta function”. This result is also used in showing that with an appropriate
choice of vanishing rate for the bandwidth  the variance also vanishes asymptotically  hence the
estimator is pointwise consistent.

Statement of the theorem Let M be an n-dimensional  embedded  complete Riemannian sub-
manifold of RN (n < N ) with an induced metric g and injectivity radius rinj > 0.2 Let d(p  q) be
the length of a length-minimizing geodesic in M between p  q ∈ M   and let u(p  q) be the geodesic
(linear) distance between p and q as measured in RN . Note that u(p  q) ≤ d(p  q). We will use the
notation up(q) = u(p  q) and dp(q) = d(p  q). We will denote the Riemannian volume measure on
M by V   and the volume form by dV .
Theorem 2.1. Let f : M → [0  ∞) be a probability density function deﬁned on M (so that the
related probability measure is f V )  and K : [0  ∞) → [0  ∞) be a continous function that sat-
isﬁes vanishes outside [0  1)  is differentiable with a bounded derivative in [0  1)  and satisﬁes 

Rkzk≤1 K(kzk)dnz = 1. Assume f is differentiable to second order in a neighborhood of p ∈ M  
and for a sample q1  . . .   qm of size m drawn from the density f   deﬁne an estimator ˆfm(p) of f (p)
as 

ˆfm(p) =

1
m

1
hn
m

hm (cid:19)
K(cid:18) up(qj)

(3)

m

Xj=1

where hm > 0. If hm satisﬁes limm→∞ hm = 0 and limm→∞ mhn
non-negative numbers m∗  Cb  and CV such that for all m > m∗ we have 

m = ∞  then  there exists

MSEh ˆfm(p)i = E(cid:20)(cid:16) ˆfm(p) − f (p)(cid:17)2(cid:21) < Cbh4

m +

CV
mhn
m

.

(4)

If hm is chosen to be proportional to m−1/(n+4)  this gives  Eh(fm(p) − f (p))2i = O(cid:0)

as m → ∞.

1

m4/(n+4)(cid:1)

Thus  the convergence rate of the estimator is given as in [3  9]  with the dimensionality replaced
by the intrinsic dimension n of M . The proof will follow from the two lemmas below on the
convergence rates of the bias and the variance.

2The injectivity radius rinj of a Riemannian manifold is a distance such that all geodesic pieces (i.e.  curves
with zero intrinsic acceleration) of length less than rinj minimize the length between their endpoints. On a
complete Riemannian manifold  there exists a distance-minimizing geodesic between any given pair of points 
however  an arbitrary geodesic need not be distance minimizing. For example  any two non-antipodal points
on the sphere can be connected with two geodesics with different lengths  namely  the two pieces of the great
circle passing throught the points. For a detailed discussion of these issues  see  e.g.  [1].

3

3 Preliminary results

The following theorem  which is analogous to Theorem 1A in [8]  tells that up to a constant  the
kernel becomes a “delta function” as the bandwidth gets smaller.
Theorem 3.1. Let K : [0  ∞) → [0  ∞) be a continuous function that vanishes outside [0  1) and
is differentiable with a bounded derivative in [0  1)  and let ξ : M → R be a function that is
differentiable to second order in a neighborhood of p ∈ M . Let

ξh(p) =

1

hn ZM

K(cid:18) up(q)

h (cid:19) ξ(q) dV (q)  

where h > 0 and dV (q) denotes the Riemannian volume form on M at point q. Then  as h → 0 

ξh(p) − ξ(p)ZRn

K(kzk)dnz = O(h2)  

(5)

(6)

where z = (z1  . . .   zn) denotes the Cartesian coordinates on Rn and dnz = dz1 . . . dzn denotes

the volume form on Rn. In particular  limh→0 ξh(p) = ξ(p)RRn K(kzk)dnz.

Before proving this theorem  we prove some results on the relation between up(q) and dp(q).
Lemma 3.1. There exist δup > 0 and Mup > 0 such that for all q with dp(q) ≤ δup  we have 

dp(q) ≥ up(q) ≥ dp(q) − Mup [dp(q)]3 .

(7)

In particular  limq→p

up(q)
dp(q) = 1.

(cid:12)(cid:12)(cid:12)s=0

v0 (s)k = 1  which gives3 x′

v0 (s) · x′′

= 1   and d2up(cv0 (s))

ds (cid:12)(cid:12)s=0 = v0. When s < rinj  s is equal to dp(cv0 (s)) [7  1]. Now let xv0 (s) be the

Proof. Let cv0(s) be a geodesic in M parametrized by arclength s  with c(0) = p and initial ve-
locity dcv0
representation of cv0(s) in RN in terms of Cartesian coordinates with the origin at p. We have
v0 (s) = 0. Using these
up(cv0(s)) = kxv0(s)k and kx′
we get  dup(cv0 (s))
= 0. Let M3 ≥ 0 be an upper bound on
the absolute value of the third derivative of up(cv0 (s)) for all s ≤ rinj and all unit length v0:
d3up(cv0 (s))
(cid:12)(cid:12)(cid:12)
s3
3! .
Thus  (7) holds with Mup = M3
3!   for all r < rinj. For later convenience  instead of δu = rinj 
we will pick δup as follows. The polynomial r − Mupr3 is monotonically increasing in the interval
0 ≤ r ≤ 1/p3Mup. We let δup = min{rinj   1/pMup}  so that r − Mupr3 is ensured to be
monotonic for 0 ≤ r ≤ δup.
Deﬁnition 3.2. For 0 ≤ r1 < r2  let 

≤ M3. Taylor’s theorem gives up(cv0(s)) = s + Rv0 (s) where |Rv0 (s)| ≤ M3

(cid:12)(cid:12)(cid:12)s=0

ds2

ds

(cid:12)(cid:12)(cid:12)

ds3

Hp(r1  r2) = inf{up(q) : r1 ≤ dp(q) < r2}  

Hp(r) = Hp(r  ∞) = inf{up(q) : r1 ≤ dp(q)}  

(8)
(9)

i.e.  Hp(r1  r2) is the smallest u-distance from p among all points that have a d-distance between r1
and r2.

Since M is assumed to be an embedded submanifold  we have Hp(r) > 0 for all r > 0. In the
below  we will assume that all radii are smaller than rinj  in particular  a set of the form {q : r1 ≤
dp(q) < r2} will be assumed to be non-empty and so  due to the completeness of M   to contain a
point q ∈ M such that dp(q) = r1. Note that 

Hp(r1) = min{H(r1  r2)  H(r2)} .

(10)

Lemma 3.2. Hp(r) is a non-decreasing  non-negative function  and there exist δHp > 0 and MHp ≥
0 such that  r ≥ Hp(r) ≥ r − MHpr3   for all r < δHp . In particular  limr→0

Hp(r)

r = 1.

3Primes denote differentiation with respect to s.

4

Proof. Hp(r) is clearly non-decreasing and Hp(r) ≤ r follows from up(q) ≤ dp(q) and the fact
that there exists at least one point q with dp(q) = r in the set {q : r ≤ dp(q)}
Let δHp = Hp(δup) where δup is as in the proof of Lemma 3.1 and let r < δHp. Since r < δHp =
Hp(δup ) ≤ δup  by Lemma 3.1 we have 

r ≥ up(r) ≥ r − Mupr3  

(11)

for some Mup > 0. Now  since r and r − Mupr3 are both monotonic for 0 ≤ r ≤ δup  we have (see
ﬁgure)

(12)
In particular  H(r  δup) ≤ r < δHp = Hp(δup )  i.e  H(r  δup) < Hp(δup). Using (10) this
gives  Hp(r) = Hp(r  δup). Combining this with (12)  we get r ≥ Hp(r) ≥ r − Mupr3 for all
r < δHp .

r ≥ Hp(r  δup) ≥ r − Mupr3 .

Next we show that for all small enough h  there exists some radius Rp(h) such that for all points q
with a dp(q) ≥ Rp(h)  we have up(q) ≥ h. Rp(h) will roughly be the inverse function of Hp(r).
Lemma 3.3. For any h < Hp(rinj )  let Rp(h) = sup{r : Hp(r) ≤ h}. Then  up(q) ≥ h for all
q with dp(q) ≥ Rp(h) and there exist δRp > 0 and MRp > 0 such that for all h ≤ δRp  Rp(h)
satisﬁes 

h ≤ Rp(h) ≤ h + MRph3 .

(13)

In particular  limh→0

Rp(h)

h = 1.

Proof. That up(q) ≥ h when dq(q) ≥ Rp(h) follows from the deﬁnitions. In order to show (13)  we
will use Lemma 3.2. Let α(r) = r − MHpr3  where MHp is as in Lemma 3.2. Then  α(r) is one-
to-one and continuous in the interval 0 ≤ r ≤ δHp ≤ δup. Let β = α−1 be the inverse function of
α in this interval. From the deﬁnition of Rp(h) and Lemma 3.2  it follows that h ≤ Rp(h) ≤ β(h)
for all h ≤ α(δHp ). Now  β(0) = 0  β′(0) = 1  β′′(0) = 0  so by Taylor’s theorem and the fact
that the third derivative of β is bounded in a neighborhood of 0  there exists δg and MRp such that
β(h) ≤ h + MRph3 for all h ≤ δg. Thus 

h ≤ Rp(h) ≤ h + MRph3 

(14)

for all h ≤ δR where δR = min{α(δHp)  δg}.

Proof of Theorem 3.1. We will begin by proving that for small enough h  there is no contribution to
the integral in the deﬁnition of ξh(p) (see (5)) from outside the coordinate patch covered by normal
coordinates.4
Let h0 > 0 be such that Rp(h0) < rinj (such an h0 exists since limh→0 Rp(h) = 0). For any
h ≤ h0  all points q with dp(q) > rinj will satisfy up(q) > h. This means if h is small enough 
K( up(q)
h ) = 0 for all points outside the injectivity radius and we can perform the integral in (5)
solely in the patch of normal coordinates at p.
For normal coordinates y = (y1  . . .   yn) around the point p with y(p) = 0  we have dp(q) =
ky(q)k [7  1]. With slight abuse of notation  we will write up(y(q)) = up(q)  ξ(y(q)) = ξ(q) and
g(q) = g(y(q))  where g is the metric tensor of M .

Since K( up(q)

h ) = 0 for all q with dp(q) > Rp(h)  we have 

ξh(p) =

1

hn Zkyk≤Rp(h)

K(cid:18) up(y)

h (cid:19) ξ(y)pg(y)dy1 . . . dyn  

(15)

4Normal coordinates at a point p in a Riemannian manifold are a close approximation to Cartesian coordi-
nates  in the sense that the components of the metric have vanishing ﬁrst derivatives at p  and gij (p) = δij [1].
Normal coordinates can be deﬁned in a “geodesic ball” of radius less than rinj.

5

where g denotes the determinant of g as calculated in normal coordinates. Changing the variable of
integration to z = y/h  we get 

h (cid:19) ξ(zh)pg(zh)dnz − ξ(0)Zkzk≤1

h (cid:19) ξ(zh)(cid:16)pg(zh) − 1(cid:17) dnz +

h (cid:19) − K(kzk)(cid:19) dnz +

K(cid:18) up(zh)

ξh(p) − ξ(p)Z K(kzk)dnz =
Zkzk≤Rp(h)/h
= Zkzk≤1
Zkzk≤1
Zkzk≤1
Z1<kzk≤Rp(h)/h

K(cid:18) up(zh)
ξ(zh)(cid:18)K(cid:18) up(zh)

K(cid:18) up(zh)

K(kzk) (ξ(zh) − ξ(0)) dnz +

h (cid:19) ξ(zh)pg(zh)dnz .

Thus 

K(kzk)dnz

(16)

(17)

(18)

(19)

(cid:12)(cid:12)(cid:12)(cid:12)

ξh(p) − ξ(p)Z K (kzk) dnz(cid:12)(cid:12)(cid:12)(cid:12)

|ξ(zh)| . sup

sup
t∈R

K(t) . sup
kzk≤1

≤

sup
kzk≤1

Zkzk≤1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

K(t) .

sup
t∈R

|ξ(zh)| . sup

K(

kzk≤1(cid:12)(cid:12)(cid:12)(cid:12)

K(kzk)(ξ(zh) − ξ(0))dnz(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
1<kzk≤Rp(h)/hpg(zh) .

sup

+

up(zh)

kzk≤1(cid:12)(cid:12)(cid:12)pg(zh) − 1(cid:12)(cid:12)(cid:12)
) − K(kzk)(cid:12)(cid:12)(cid:12)(cid:12)

h

. Zkzk≤1
. Zkzk≤1

dnz +

dnz +

sup

1<kzk≤Rp(h)/h

|ξ(zh)| . Z1<kzk≤Rp(h)/h

dnz . (20)

Letting h → 0  the terms (17)-(20) approach zero at the following rates:
(17): K(t) is bounded and ξ(y) is continuous at y = 0  so the ﬁrst two terms can be bounded
In normal coordinates y  gij(y) = δij + O(kyk2) as kyk → 0  so 
by constants as h → 0.

= O(h2) as h → 0.

supkzk≤1(cid:12)(cid:12)(cid:12)pg(zh) − 1(cid:12)(cid:12)(cid:12)
(18): Since K is assumed to be differentiable with a bounded derivative in [0  1)  we get K(b) −
K(a) = O(b − a) as b → a. By Lemma 3.1 we have up(zh)
h − kzk = O(h2) as h → 0. Thus 
K(cid:16) up(zh)

h (cid:17) − K(kzk) = O(h2) as h → 0.

(19): Since ξ(y) is assumed to have partial derivatives up to second order in a neighborhood of
y(p) = 0  for kzk ≤ 1  Taylor’s theorem gives 

ξ(zh) = ξ(0) + h

n

Xi=1

zi ∂ξ(y)
∂yi

(cid:12)(cid:12)(cid:12)y=0

+ O(h2)

(21)

as h → 0. Since Rkzk≤1 zK(kzk)dnz = 0  we get (cid:12)(cid:12)(cid:12)Rkzk≤1 K(kzk)(ξ(zh) − ξ(0))dnz(cid:12)(cid:12)(cid:12)

h → 0.
(20): The ﬁrst three terms can be bounded by constants. By Lemma 3.3  Rp(h) = h + O(h3) as
h → 0. A spherical shell 1 < kzk ≤ 1 + ǫ has volume O(ǫ) as ǫ → 0+. Thus  the volume of
1 < kzk ≤ Rp(h)/h is O(Rp(h)/h − 1) = O(h2) as h → 0.
Thus  the sum of the terms (17-20)  is O(h2) as h → 0  as claimed in Theorem 3.1.

= O(h2) as

6

4 Bias  variance and mean squared error

m)  as m → ∞.

Let M   f   ˆfm  K  p be as in Theorem 2.1 and assume hm → 0 as m → ∞.
Lemma 4.1. Biash ˆfm(p)i = O(h2
Proof. We have Bias[fm(p)] = Biash 1
Lemma 4.2. If in addition to hm → 0  we have mhn
O(cid:16) 1

follows from Theorem 3.1 with ξ replaced with f .

m(cid:17)  as m → ∞.

K (cid:16) up(q)

mhn

hm

h (cid:17)i  so recallingRRn K(kzk)dnz = 1  the lemma

m → ∞ as m → ∞  then  Var[fm(p)] =

Proof.

Now 

and 

Var(cid:20) 1

hn
m

Var[fm(p)] =

1
m

Var(cid:20) 1

hn
m

K(cid:18) up(q)

hm (cid:19)(cid:21)

K (cid:18) up(q)

hm (cid:19)(cid:21) = E(cid:20) 1

h2n
m

K 2(cid:18) up(q)

hm (cid:19)(cid:21) −(cid:18)E(cid:20) 1

hn
m

K(cid:18) up(q)

hm (cid:19)(cid:21)(cid:19)2

 

E(cid:20) 1

h2n
m

K 2(cid:18) up(q)

hm (cid:19)(cid:21) =

1
hn

m ZM

f (q)

1
hn
m

K 2(cid:18) up(q)

hm (cid:19) dV (q) .

(22)

(23)

(24)

(25)

hn

m(cid:17) as m → ∞. By Lemma 4.1 we have  (cid:16)Eh 1

By Theorem 3.1  the integral in (25) converges to f (p)R K 2(kzk)dnz  so  the right hand side of
(25) is O(cid:16) 1
Var[ ˆfm(p)] = O(cid:16) 1
Proof of Theorem 2.1 Finally  since MSEh ˆfm(p)i = Bias2[ ˆfm(p)] + Var[ ˆfm(p)]  the theorem

m(cid:17) as m → ∞.

hm (cid:17)i(cid:17)2

K(cid:16) up(q)

→ f 2(p). Thus 

mhn

hn
m

follows from Lemma 4.1 and 4.2.

5 Experiments and discussion

We have empirically tested the estimator (3) on two datasets: A unit normal distribution mapped
onto a piece of a spiral in the plane  so that n = 1 and N = 2  and a uniform distribution on the unit

disc x2 + y2 ≤ 1 mapped onto the unit hemisphere by (x  y) → (x  y  1 −px2 + y2)  so that n = 2

and N = 3. We picked the bandwidths to be proportional to m−1/(n+4) where m is the number of
data points. We performed live-one-out estimates of the density on the data points  and obtained the
MSE for a range of ms. See Figure 5.

6 Conclusion and future work

We have proposed a small modiﬁcation of the usual KDE in order to estimate the density of data
that lives on an n-dimensional submanifold of RN   and proved that the rate of convergence of the
estimator is determined by the intrinsic dimension n. This shows that the curse of dimensionality in
KDE can be overcome for data with low intrinsic dimension. Our method assumes that the intrinsic
dimensionality n is given  so it has to be supplemented with an estimator of the dimension. We
have assumed various smoothness properties for the submanifold M   the density f   and the kernel
K. We ﬁnd it likely that our estimator or slight modiﬁcations of it will be consistent under weaker
requirements. Such a relaxation of requirements would have practical consequences  since it is
unlikely that a generic data set lives on a smooth Riemannian manifold.

7

MSE

Mean squared error for the spiral data

MSE

Mean squared error for the hemisphere data

0.000175

0.00015

0.000125

0.0001

0.000075

0.00005

0.000025

0.0008

0.0006

0.0004

0.0002

50000

100000

150000

200000

# of data points

50000

100000

150000

200000

# of data points

Figure 1: Mean squared error as a function of the number of data points for the spiral data (left) and the
hemisphere data. In each case  we ﬁt a curve of the form M SE(m) = amb  which gave b = −0.80 for the
spiral and b = −0.69 for the hemisphere. Theorem 2.1 bounds the MSE by Cm−4/(n+4)  which gives the
exponent as −0.80 for the spiral and −0.67 for the hemisphere.

References

[1] M. Berger and N. Hitchin. A panoramic view of Riemannian geometry. The Mathematical

Intelligencer  28(2):73–74  2006.

[2] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbor. In Proceedings
of the 23rd international conference on Machine learning  pages 97–104. ACM New York 
NY  USA  2006.

[3] T. Cacoullos. Estimation of a multivariate density. Annals of the Institute of Statistical Mathe-

matics  18(1):179–189  1966.

[4] H. Hendriks. Nonparametric estimation of a probability density on a Riemannian manifold

using Fourier expansions. The Annals of Statistics  18(2):832–849  1990.

[5] J. Jost. Riemannian geometry and geometric analysis. Springer  2008.
[6] F. Korn  B. Pagel  and C. Faloutsos. On dimensionality and self-similarity . IEEE Transactions

on Knowledge and Data Engineering  13(1):96–111  2001.

[7] J. Lee. Riemannian manifolds: an introduction to curvature. Springer Verlag  1997.
[8] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathe-

matical Statistics  pages 1065–1076  1962.

[9] B. Pelletier. Kernel density estimation on Riemannian manifolds. Statistics and Probability

Letters  73(3):297–304  2005.

[10] X. Pennec. Probabilities and statistics on Riemannian manifolds: Basic tools for geometric
In IEEE Workshop on Nonlinear Signal and Image Processing  volume 4.

measurements.
Citeseer  1999.

[11] X. Pennec. Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measure-

ments. Journal of Mathematical Imaging and Vision  25(1):127–154  2006.

[12] B. Silverman. Density estimation for statistics and data analysis. Chapman & Hall/CRC 

1986.

[13] P. Vincent and Y. Bengio. Manifold Parzen Windows. Advances in Neural Information Pro-

cessing Systems  pages 849–856  2003.

8

,Cem Subakan
Johannes Traa
Paris Smaragdis