2013,Universal models for binary spike patterns using centered Dirichlet processes,Probabilistic models for binary spike patterns provide a powerful   tool for understanding the statistical dependencies in large-scale   neural recordings.  Maximum entropy (or maxent'') models  which   seek to explain dependencies in terms of low-order interactions   between neurons  have enjoyed remarkable success in modeling such   patterns  particularly for small groups of neurons. However  these   models are computationally intractable for large populations  and   low-order maxent models have been shown to be inadequate for some   datasets.  To overcome these limitations  we propose a family of   "universal'' models for binary spike patterns  where universality   refers to the ability to model arbitrary distributions over all   $2^m$ binary patterns.  We construct universal models using a   Dirichlet process centered on a well-behaved parametric base   measure  which naturally combines the flexibility of a histogram and   the parsimony of a parametric model.  We derive computationally   efficient inference methods using Bernoulli and cascade-logistic   base measures  which scale tractably to large populations. We also   establish a condition for equivalence between the cascade-logistic   and the 2nd-order maxent or "Ising'' model  making cascade-logistic   a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data.",Universal models for binary spike patterns using

centered Dirichlet processes

Il Memming Park123  Evan Archer24  Kenneth Latimer12  Jonathan W. Pillow1234
1. Institue for Neuroscience  2. Center for Perceptual Systems  3. Department of Psychology

4. Division of Statistics & Scientiﬁc Computation

The University of Texas at Austin

{memming@austin.  earcher@  latimerk@  pillow@mail.} utexas.edu

Abstract

Probabilistic models for binary spike patterns provide a powerful tool for un-
derstanding the statistical dependencies in large-scale neural recordings. Maxi-
mum entropy (or “maxent”) models  which seek to explain dependencies in terms
of low-order interactions between neurons  have enjoyed remarkable success in
modeling such patterns  particularly for small groups of neurons. However  these
models are computationally intractable for large populations  and low-order max-
ent models have been shown to be inadequate for some datasets. To overcome
these limitations  we propose a family of “universal” models for binary spike pat-
terns  where universality refers to the ability to model arbitrary distributions over
all 2m binary patterns. We construct universal models using a Dirichlet process
centered on a well-behaved parametric base measure  which naturally combines
the ﬂexibility of a histogram and the parsimony of a parametric model. We derive
computationally efﬁcient inference methods using Bernoulli and cascaded logis-
tic base measures  which scale tractably to large populations. We also establish a
condition for equivalence between the cascaded logistic and the 2nd-order maxent
or “Ising” model  making cascaded logistic a reasonable choice for base measure
in a universal model. We illustrate the performance of these models using neural
data.

1

Introduction

Probability distributions over spike words form the fundamental building blocks of the neural code.
Accurate estimates of these distributions are difﬁcult to obtain in the context of modern experimen-
tal techniques  which make it possible to record the simultaneous spiking activity of hundreds of
neurons. These difﬁculties  both computational and statistical  arise fundamentally from the expo-
nential scaling (in population size) of the number of possible words a given population is capable
of expressing. One strategy for combating this combinatorial explosion is to introduce a parametric
model which seeks to make trade-offs between ﬂexibility  computational expense [1  2]  or math-
ematical completeness [3] in order to be applicable to large-scale neural recordings. A variety of
parametric models have been proposed in the literature  including the 2nd-order maxent or Ising
model [4  5]  the reliable interaction model [3]  restricted Boltzmann machine [6]  deep learning [7] 
mixture of Bernoulli model [8]  and the dichotomized Gaussian model [9]. However  while the num-
ber of parameters in a model chosen from a given parametric family may increase with the number
of neurons  it cannot increase exponentially with the number of words. Thus  as the size of a popula-
tion increases  a parametric model rapidly loses ﬂexibility in describing the full spike distribution. In
contrast  nonparametric models allow ﬂexibility to grow with the amount of data [10  11  12  13  14].
A naive nonparametric model  such as the histogram of spike words  theoretically preserves repre-
sentational power and computational simplicity. Yet in practice  the empirical histogram may be
extremely slow to converge  especially for the high dimensional data we are primarily interested

1

A

s
n
o
r
u
e
n
m

 

B

C

independent Bernoulli model

D

cascaded logistic model

time

Figure 1: (A) Binary representation of neural population activity. A single spike word x is indicated
in red. (B) Hierarchical Dirichlet process prior for the universal binary model (UBM) over spike
words. Each word is drawn with probability ⇡j. The ⇡’s are drawn from a Dirichlet with parameters
given by ↵ and a base distribution over spike words with parameter ✓. (C  D) Graphical models
of two base measures over spike words: independent Bernoulli model and cascaded logistic model.
The base measure is also a distribution over each spike word x = (x1  . . .   xm).

in. In most cases  we expect never to have enough data for the empirical histogram to converge.
Perhaps even more concerning is that a naive histogram model fails smooth over the space of words:
unobserved words are not accounted for in the model.
We propose a framework which combines the parsimony of parametric models with the ﬂexibility
of nonparametric models. We model the spike word distribution as a Dirichlet process centered on a
parametric base measure. An appropriately chosen base measure smooths the observations  while the
Dirichlet process allows for data that depart systematically from the base measure. These models
are universal in the sense that they can converge to any distribution supported on the (2m  1)-
dimensional simplex. The inﬂuence of any base measure diminishes with increasing sample size 
and the model ultimately converges to the empirical distribution function.
The choice of base measure inﬂuences the small-sample behavior and computational tractability of
universal models  both of which are crucial for neural applications. We consider two base measures
that exploit a priori knowledge about neural data while remaining computationally tractable for large
populations: the independent Bernoulli spiking model  and the cascaded logistic model [15  16].
Both the Bernoulli and cascaded logistic models show better performance when used as a base
measure for a universal model than when used alone. We apply these models to several simulated
and neural data examples.

2 Universal binary model
Consider a (random) binary spike word of length m  x 2{ 0  1}m  where m denotes the number of
distinct neurons (and/or time bins; Fig. 1A). There are K = 2m possible words  which we index
by k 2{ 1  . . .   K}. The universal binary model is a hierarchical probabilistic model where on the
bottom level (Fig. 1B)  x is drawn from a multinomial (categorical) distribution with the probability
of observing each word given by the vector ⇡ (spike word distribution). On the top level  we model
⇡ as a Dirichlet process [11] with a discrete base measure G✓  hence 

x ⇠ Cat(⇡) 

⇡ ⇠ DP(↵G✓) ✓

⇠ p(✓|) 

(1)

where ↵ is the concentration parameter  G✓ is the base measure  a discrete probability distribution
over spike words  parameterized by ✓  and p(✓|) is the hyper-prior. We choose a discrete probability
measure for G✓ such that it has positive measure only over {1  . . .   K}  and denote gk = G✓(k).
Thus  the Dirichlet process has probability mass only on the K spike words  and is described by a
(ﬁnite dimensional) Dirichlet distribution 

In the absence of data  the parametric base measure controls the mean of this nonparametric model 

⇡ ⇠ Dir(↵g1  . . .  ↵g K).

(2)

E[⇡|↵] = G✓ 

2

(3)

regardless of ↵. Therefore  we loosely say that ⇡ is “centered” around G✓.1 We can start with good
parametric models of neural populations  and extend them into a nonparametric model by using
them as the base measure [17]. Under this scheme  the base measure quickly learns much of the
basic structure of the data while the Dirichlet extension takes into account any deviations in the data
which are not predicted by the parametric component. We call such an extension a universal binary
model (UBM) with base measure G✓.
The marginal distribution of a collection of words X = {xi}N
grating over ⇡  and has the form of a Polya (a.k.a. Dirichlet-Multinomial) distribution:

i=1 under UBM is obtained by inte-

P (X|↵  G✓) =

(↵)

( N + ↵)

(nk + ↵gk)

(↵gk)

 

(4)

KYk=1

where nk is the number of observations of the word k. This leads to a simple formula for sampling
from the predictive distribution over words:

Pr(xN +1 = k|XN  ↵  G ✓) =

nk + ↵gk
N + ↵

.

(5)

Thus  sampling proceeds exactly as in the Chinese restaurant process (CRP): we set the (N + 1)-th
word to be k with probability proportional to nk + ↵gk  and with probability proportional to ↵ we
draw a new word from G✓ (which in turn increases the probability of getting word k on the next
draw). Note that as ↵ ! 0  the predictive distribution converges to the histogram estimate nk
N   and
as ↵ ! 1  it converges to the base measure itself. We use the Jensen-Shannon divergence to the
predictive distribution to quantify the performance in our experiments.

2.1 Model ﬁtting

Given data  we ﬁt the UBM via maximum a posteriori (MAP) inference for ↵ and ✓  using coordinate
ascent. The marginal log-likelihood from (4) is given by 

log (↵gk) + log (↵)  log  (N + ↵) . (6)

(7)

(8)

log (nk + ↵gk) Xk

Derivatives with respect to ↵ and ✓ are 

L = log P (XN|↵  ✓) =Xk
= ↵Xk
=Xk

@L
@↵

@L
@✓

( (nk + ↵gk)  (↵gk))

@
@✓

gk 

gk ( (nk + ↵gk)  (↵gk)) + (↵)  (N + ↵)  

where denotes the digamma function. Note that the summation terms vanish when we have no
observations (nk = 0)  so we only need to consider the words observed in the dataset.
Note also that in the limit ↵ ! 1  dL
@✓ gk  the derivative of the logarithm
of the base measure with respect to ✓. On the other hand  in the limit ↵ ! 0  the derivative
goes toP 1
@✓ gk  reﬂecting the fact that the number of observations nk is ignored: the likelihood
effectively reﬂects only a single draw from the base distribution with probability gk.
Even when the likelihood deﬁned by the base measure is a convex or log-convex in ✓  the UBM
likelihood is not guaranteed to be convex. Hence  we optimize by a coordinate ascent procedure that
alternates between optimizing ↵ and ✓.

d✓ converges toP nk

gk

gk

@

@

2.2 Hyper-prior

When modeling large populations of neurons  the number of parameters ✓ of the base measure grows
and over-ﬁtting becomes a concern. Since the UBM relies on the base measure to provide smoothing
over words  it is critical to properly regularize our estimate of ✓.

1 Technically  the mode of ⇡ is G✓ only for ↵  1  and for ↵< 1  the distribution is symmetric around G✓ 

but the probability mass is concentrated on the corners of the simplex.

3

We place a hyper-prior p(✓|) on ✓ for regularization. We consider both l2 and l1 regularization 
which correspond to Gaussian and double exponential priors  respectively. With regularization  the
loss function for optimization is L  k✓kp
p  where p = 1  2. In a typical multi-neuron recording 
the connectivity is known to be sparse and lower order [1  3]  and so we assume the connectivity is
sparse. The l1 prior in particular promotes sparsity.

3 Base measures

The scalability of UBM hinges on the scalability of its base measure. We describe two computation-
ally efﬁcient base measures.

3.1

Independent Bernoulli model

We consider the independent Bernoulli model which assumes (statistically) independent spiking
neurons. It is often used as a baseline model for its simplicity [4  3]. The Bernoulli base measure
takes the form 

G✓(k) = p(x1  . . .   xm|✓) =

pxi
i (1  pi)1xi 

(9)

where pi  0 and ✓ = (p1  . . .   pm). The distribution has full support on K spike words as long
as all pi’s are non-zero. Although the Bernoulli model cannot capture the higher-order correlation
structure of the spike word distribution with only m parameters  inference is fast and memory-
efﬁcient.

3.2 Cascaded logistic model

To introduce a rich dependence structure among the neurons  we assume the joint ﬁring probability
of each neuron factors with a cascaded structure (see Fig. 1D):

p(x1  x2  . . .   xm) = p(x1)p(x2|x1)p(x3|x1  x2)··· p(xm|x1  x2  . . .   xm1).

(10)
Along with a parametric form of conditional distribution p(xi|x1  . . .   pi1)  it provides a proba-
bilistic model of spike words.
A natural choice of the conditional is the logistic-Bernoulli linear model—a widely used model for
binary observations [2].

p(xi = 1|x1:i1 ✓ ) = logistic(hi +Xj<i

wijxj)

(11)

where ✓ = (hi  wij)i j<i are the parameters. The combination of the factorization and the likeli-
hoods give rise to the cascaded logistic (Bernoulli) model2  which can be written as 

The cascaded logistic model and the Ising model (second order maxent model) have the same num-
ber of parameters m(m+1)

  but a different parametric form. The Ising model can be written as3 

G✓(k) = p(x1  . . .   xm|✓) =

p(xi|x1:i1)

p(xi|x1:i1 ✓ ) =h1 + exp⇣(2xi  1)⇣hi +Pi1
j=1wijxj⌘⌘i1
Jijxixj1A

exp0@Xi ji

1

p(x1  . . .   xm|✓) =

Z(J)

2

where ✓ = J is a upper triangular matrix of parameters  and Z(J) is the normalizer. However  unlike
the cascaded logistic model  it is difﬁcult to evaluate the likelihood of the Ising model  since it does
not have a computationally tractable normalizer (partition function). Hence  ﬁtting an Ising model
is typically challenging. Since each conditional can be independently ﬁt with a logistic regression (a

(12)

(13)

(14)

mYi

mYi=1

4

DBAC10(cid:239)(cid:20)(cid:27)10(cid:239)(cid:28)10(cid:239)(cid:20)10(cid:239)(cid:20)(cid:27)10(cid:239)(cid:28)10(cid:239)(cid:20)00.20.4050100BernoulliBernoullicascaded logisticExample cascaded logistic modelfor Theorem 1Equivalent Ising modelcascadedlogisticsparse IsingEF10(cid:239)(cid:20)(cid:27)10(cid:239)(cid:28)10(cid:239)(cid:20)10(cid:239)(cid:20)(cid:27)10(cid:239)(cid:28)10(cid:239)(cid:20)00.20.4050100(cid:45)(cid:54)(cid:239)(cid:71)(cid:76)(cid:89)(cid:72)(cid:85)(cid:74)(cid:72)(cid:81)(cid:70)(cid:72)(cid:41)(cid:85)(cid:72)(cid:84)(cid:88)(cid:72)(cid:81)(cid:70)(cid:92)(cid:83)(cid:11)(cid:91)(cid:12)(cid:29)(cid:3)(cid:44)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:11)(cid:68)(cid:70)(cid:87)(cid:88)(cid:68)(cid:79)(cid:12)(cid:83)(cid:11)(cid:91)(cid:12)(cid:29)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:73)(cid:76)(cid:87)dense Isingsparse Isingdense IsingFigure2:TightrelationbetweencascadedlogisticmodelandtheIsingmodel.(A)Acascadedlogisticmodeldepictedasagraphicalmodelwithatmosttwoconditioning(incomingarrow)pernode(seeTheorem2).Thehiparametersaregiveninthenodesandtheinteractionterms wijareshownonthearrowsbetweennodes.(B)ParametermatrixJofanIsingmodelequivalentto(A).(C)AscatterplotofthreesimulatedIsingmodelsﬁtwithcascadedlogistic(bluetone)andindependentBernoulli(redtone)models.Eachpointisawordinthem=15spikewordspace.Thex-axisgivesprobabilityofthewordundertheactualIsingmodelandthey-axisshowstheestimatedprobabilityfromtheﬁtmodel.TheIsingmodelparametersweresparselyconnectandgeneratedrandomly.Thediagonalterms(Jii)weredrawnfromastandardnormal.80%oftheoff-diagonal(Jij i6=j)termsweresetto0andtherestdrawnfromanormalwithmean0andstandarddeviation3.Bothmodelswereﬁtbymaximumlikelihoodusing107samples.(D)AhistogramoftheJensen-Shannon(JS)divergencebetween100randompairsofsparseIsingmodelandtheﬁtmodels.(E F)Sameas(C D)forIsingmodelsgeneratedwithdenseconnectivity.ThediagonaltermsintheIsingmodelparameterswereconstant-2.Theoff-diagonaltermsweredrawnfromastandardnormaldistribution.convexoptimization) cascadedlogisticmodel’sestimationiscomputationallytractableforalargenumberofneurons[2].Despitethesedifferences remarkably theIsingmodelandthecascadedlogisticmodelsoverlapsubstantially.Uptom=3neurons Isingmodelandcascadedlogisticmodelareequivalent.Forlargerpopulations thefollowingtheoremdescribestheintersectionofthetwomodels.Theorem1(PentadiagonalIsingmodelisacascadedlogisticmodel).AnIsingmodelwithJij=0forj<i2orj>i+2 isalsoacascadedlogisticmodel.Moreover theparametertransformationisbijective.ThemappingbetweenmodelsparametersisgivenbyJm m=hm(15)Jm1 m=wm m1(16)Jm1 m1=hm1+log✓1+exp(hm)1+exp(hm+wm m1)◆(17)Ji i=hi+log✓1+exp(hi+1)1+exp(hi+1+wi+1 i)◆+log✓1+exp(hi+2)1+exp(hi+2+wi+2 i)◆(18)Ji i+1=wi+1 i+log✓(1+exp(hi+2+wi+2 i))(1+exp(hi+2+wi+2 i+1))(1+exp(hi+2))(1+exp(hi+2+wi+2 i+1+wi+2 i))◆(19)Ji i+2=wi+2 i(20)for1in2 forasymmetricJ.Proofcanbefoundinthesupplementalmaterial.2Alsoknownasthelogisticautoregressivenetwork.See[15] chapter3.2.3Notethatforxi2{0 1} themeanhi’scanbeincorporatedasthediagonalofJ.5A
10(cid:239)(cid:20)

(cid:72)
(cid:70)
(cid:81)
(cid:72)
(cid:74)
(cid:85)
(cid:72)
(cid:89)
(cid:71)
(cid:239)
(cid:54)
(cid:45)

(cid:76)

10(cid:239)(cid:21)

B

(cid:80)
(cid:68)
(cid:85)
(cid:74)
(cid:82)

(cid:87)
(cid:86)
(cid:75)

(cid:76)

(cid:3)
(cid:87)

(cid:81)
(cid:88)
(cid:82)
(cid:70)

6

4

2

0

(cid:76)(cid:81)(cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:72)(cid:81)(cid:87)(cid:3)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)

(cid:43)(cid:76)(cid:86)(cid:87)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)

(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)(cid:12)

(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)
(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:12)
104
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

102

x 104

103

105

0 1 2 3 4 5 6 7 8
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:83)(cid:76)(cid:78)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:90)(cid:82)(cid:85)(cid:71)

3rd order maxent distribution experiment.

(A) Convergence in Jensen-Shannon (JS)
Figure 3:
divergence between the ﬁt model and the true model. Error bar represents SEM over 10 repeats.
(B) Histogram of the number of spikes per word.
(C) Scatter plots of the log-likelihood ratio
log(Pemp(k))  log(Pmodel(k)) for each model (column)  and two sample sizes of N = 1000 and
N = 100000 (rows). Note the scale difference on the y-axes. Error line represents twice the standard
deviation over 10 repeats. Shaded area represents frequentist 95% conﬁdence interval for histogram
estimator assuming the same amount of data. The number on the bottom right is the JS divergence.

Unlike the Ising model  the order of the neurons plays a role in the formulation of the cascaded
logistic model. Since a permutation of a pentadiagonal matrix is not necessarily pentadiagonal 
this poses a potential challenge to the application of this equivalency. However  the Cuthill-McKee
algorithm can be used as a heuristic to ﬁnd a permutation of J with the lowest bandwidth (i.e. 
closest to pentadiagonal) [18].
This theorem can be generalized to sparse  structured cascaded logistic models.
Theorem 2 (Intersection between cascaded logistic model and Ising model). A cascaded logistic
model with at most two interactions with other neurons is also an Ising model.
For example  cascaded logistic with a sparse cascade p(x1)p(x2|x1)p(x3|x1)p(x4|x1  x3)p(x5|x2  x4)
is an Ising model (Fig. 2A)4. We remark that although the cascaded logistic model can be written
as an exponential family form  the cascaded logistic does not correspond to a simple family of
maximum entropy models in general.
The theorems show that only a subset of Ising models are equivalent to cascaded logistic models.
However  cascaded logistic models generally provide good approximations to the Ising model. We
demonstrate this by drawing random Ising models (both with sparse and dense pairwise coupling J) 
and then ﬁtting with a cascaded logistic model (Fig. 2C-F). Since Ising models are widely accepted
as effective models of neural populations  the cascaded logistic model presents a computationally
tractable alternative.

4 Simulations

We compare two parametric models (independent Bernoulli and cascaded logistic model) with three
nonparametric models (two universal binary models centered on the parametric models  and a naive
histogram estimator) on simulated data with 15 neurons. We ﬁnd the MAP solution as the parameter
estimate for each model. We use an l1 regularization to ﬁt the cascaded logistic model and the cor-
responding UBM. The l1 regularizer  was selected by scanning on a grid until the cross-validation
likelihood started decreasing on 10% of the training data.
In Fig. 3  we simulate a maximum entropy (maxent) distribution with a third order interaction. As
the number of samples increases  Jensen-Shannon (JS) divergence between the estimated model and
true maxent model decreases exponentially for the nonparametric models. The JS-divergence of the

4We provide MATLAB code to convert back and forth between a subset of Ising models and the correspond-

ing subset of cascaded logistic models (see online supplemental material).

6

A

(cid:72)
(cid:70)
(cid:81)
(cid:72)
(cid:74)
(cid:85)
(cid:72)
(cid:89)
(cid:71)
(cid:239)
(cid:54)
(cid:45)

(cid:76)

10(cid:239)(cid:20)

10(cid:239)(cid:21)

B

(cid:80)
(cid:68)
(cid:85)
(cid:74)
(cid:82)

(cid:87)
(cid:86)
(cid:75)

(cid:76)

(cid:3)
(cid:87)

(cid:81)
(cid:88)
(cid:82)
(cid:70)

4
3
2
1
0

(cid:76)(cid:81)(cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:72)(cid:81)(cid:87)(cid:3)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)

(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)

(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)(cid:12)
(cid:43)(cid:76)(cid:86)(cid:87)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)
(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:12)
102

104
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

103

x 104

105

0 1 2 3 4 5 6 7 8 91011

(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:83)(cid:76)(cid:78)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:90)(cid:82)(cid:85)(cid:71)

Figure 4: Synchrony histogram model. Each word with the same number of total spikes regardless
of neuron identity has the same probability. Both Bernoulli and cascaded logistic models do not
provide a good approximation in this case and saturate  in terms of JS divergence. Same format as
Fig. 3.

(cid:43)(cid:76)(cid:86)(cid:87)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)

(cid:76)(cid:81)(cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:72)(cid:81)(cid:87)(cid:3)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)
(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)(cid:12)

10(cid:239)(cid:21)

(cid:72)
(cid:70)
(cid:81)
(cid:72)
(cid:74)
(cid:85)
(cid:72)
(cid:89)
(cid:71)
(cid:239)
(cid:54)
(cid:45)

(cid:76)

(cid:80)
(cid:68)
(cid:85)
(cid:74)
(cid:82)
(cid:87)
(cid:86)
(cid:75)
(cid:3)
(cid:87)
(cid:81)
(cid:88)
(cid:82)
(cid:70)

(cid:76)

4
3
2
1
0

(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)
(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:12)
104
102
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

103

x 104

105

0 1 2 3 4 5 6 7 8
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:83)(cid:76)(cid:78)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:90)(cid:82)(cid:85)(cid:71)

Figure 5:
Ising model with 1-D nearest neighbor interaction. Same format as Fig. 3. Note that
cascaded logistic and UBM with cascaded logistic base measure perform almost identically  and
their convergence does not saturate (as expected by Theorem 1).

parametric models saturates since the actual distribution does not lie within the same parametric
family. The cascaded logistic model and the UBM centered on it show the best performance for the
small sample regime  but eventually other nonparametric models catch up with the cascaded logistic
model.
The scatter plot (Fig. 3C) displays the log-likelihood ratio log(Ptrue)  log(Pmodel) to quantify the
accuracy of the predictive distribution. Where signiﬁcant deviations from the base measure model
can be observed in Fig. 3C  the corresponding UBM adapts to account for those deviations.
In Fig. 4  we draw samples from a distribution with higher-order dependences; Each word with the
same number of total spikes are assigned the same probability. For example  words with exactly
10 neurons spiking (and 5 not spiking  out of 15 neurons) occur with high probability as can be
seen from the histogram of the total spikes (Fig. 4B). Neither the Bernoulli model nor the cascaded
logistic model can capture this structure accurately  indicated by a plateau in the convergence plots
(Fig. 4A C). In this case  all three nonparameteric models behave similarly: both UBMs converge
with the histogram.
In addition  we see that if the data comes from the model class assumed by the base measure  then
UBM is just as good as the base measure alone (Fig. 5). Together  these results suggest that UBM

7

A

(cid:72)
(cid:70)
(cid:81)
(cid:72)
(cid:74)
(cid:85)
(cid:72)
(cid:89)
(cid:71)
(cid:239)
(cid:54)
(cid:45)

(cid:76)

B

(cid:80)
(cid:68)
(cid:85)
(cid:74)
(cid:82)

(cid:87)
(cid:86)
(cid:75)

(cid:76)

(cid:3)
(cid:87)

(cid:81)
(cid:88)
(cid:82)
(cid:70)

10(cid:239)(cid:21)

(cid:20)(cid:21)

10

8

6

4

(cid:21)

0

(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)

(cid:71)

(cid:72)
(cid:71)(cid:3)(cid:79)(cid:82)

(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)

(cid:76)(cid:81)(cid:71)(cid:72)(cid:83)(cid:72)(cid:81)(cid:71)(cid:72)(cid:81)(cid:87)(cid:3)(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)

(cid:43)(cid:76)(cid:86)(cid:87)(cid:82)(cid:74)(cid:85)(cid:68)

(cid:80)

(cid:44)(cid:86)(cid:76)(cid:81)(cid:74)

C

(cid:53)
(cid:47)
(cid:47)

(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:70)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:3)(cid:79)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:12)

104

(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

105

103

x 104

(cid:53)
(cid:47)
(cid:47)

9

0

1

(cid:21)

4

8
3
(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:83)(cid:76)(cid:78)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:90)(cid:82)(cid:85)(cid:71)

5

6

7

4

(cid:21)

0

(cid:239)(cid:21)

(cid:239)(cid:23)

(cid:239)(cid:25)

4

(cid:21)

0

(cid:239)(cid:21)

(cid:239)(cid:23)

(cid:239)(cid:25)

(cid:56)(cid:37)(cid:48)(cid:3)(cid:11)(cid:38)(cid:16)(cid:47)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:12)

10(cid:239)(cid:24)

JS = 0.0016
100

(cid:38)(cid:68)(cid:86)(cid:70)(cid:68)(cid:71)(cid:72)(cid:71)(cid:3)(cid:47)(cid:82)(cid:74)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)

10(cid:239)(cid:24)

(cid:45)(cid:54)(cid:3)(cid:32)(cid:3)(cid:19)(cid:17)(cid:19)(cid:19)(cid:21)(cid:21)
100

4

(cid:21)

0

(cid:239)(cid:21)

(cid:239)(cid:23)

(cid:239)(cid:25)

4

(cid:21)

0

(cid:239)(cid:21)

(cid:239)(cid:23)

(cid:239)(cid:25)

(cid:37)(cid:72)(cid:85)(cid:81)(cid:82)(cid:88)(cid:79)(cid:79)(cid:76)

10(cid:239)(cid:24)

JS = 0.0349
100

(cid:44)(cid:86)(cid:76)(cid:81)(cid:74)

D

10(cid:239)(cid:24)

(cid:45)(cid:54)(cid:3)(cid:32)(cid:3)(cid:19)(cid:17)(cid:19)(cid:19)(cid:21)(cid:21)

100

1014

10(cid:20)(cid:21)

1010

108

106

104

103

104

(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

105

Figure 6: Various models ﬁt to a population of ten retinal ganglion neurons’ response to naturalistic
movie [3]. Words consisted of 20 ms  binarized responses. 1 ⇥ 105 samples were reserved for
testing. (A) JS divergence between the estimated model  and histogram constructed from the test
data. Ising model is included  and its trace is closely followed by the cascaded logistic model. (B)
Histogram of number of spikes per word. (C) Log-likelihood ratio scatter plot for the models trained
with 105 randomized observations. (D) The concentration parameter ↵ as a function of sample size.

supplements the base measure to model ﬂexibly the observed ﬁring patterns  and performs at least
as well as the histogram in the worst case.

5 Neural data

We apply UBMs to a simultaneously recorded population of 10 retinal gangilion cells  and compare
to the Ising model. In Fig. 6A we evaluate the convergence of each model. Three models—cascaded
logistic  its corresponding UBM  and the Ising model—initially perform similarly  however  as more
data is provided  UBM predicts the probabilities better. In panel C  we conﬁrm that the cascaded
logistic UBM gives the best ﬁt. The decrease in corresponding ↵  shown in panel D  indicates
that the cascaded logistic UBM is becoming less conﬁdent that the data is from an actual cascaded
logistic model as we obtain more data.

6 Conclusion

We proposed universal binary models (UBMs)  a nonparametric framework that extends parametric
models of neural recordings. UBMs ﬂexibly trade off between smoothing from the base measure and
“histogram-like” behavior. The Dirichlet process can incorporate deviations from the base measure
when supported by the data  even as the base measure buttresses the nonparametric approach with
desirable properties of parametric models  such as fast convergence and interpretability. Unlike the
reliable interaction model [3]  which aims to provide the same features in a heuristic manner  the
UBM is a well-deﬁned probabilistic model.
Since the main source of smoothing is the base measure  UBM’s ability to extrapolate is limited
to repeatedly observed words. However  UBM is capable of adjusting the probabilities of the most
frequent words to focus on ﬁtting the regularities of small probability events.
We proposed the cascaded logistic model for use as a powerful  but still computationally tractable 
base measure. We showed  both theoretically and empirically  that the cascaded logistic model is
an effective  scalable alternative to the Ising model  which is usually limited to smaller populations.
The UBM model class has the potential to reveal complex structure in large-scale recordings without
the limitations of a priori parametric assumptions.

Acknowledgments

We thank R. Segev and E. Ganmor for the retinal data. This work was supported by a Sloan Research Fellow-
ship  McKnight Scholar’s Award  and NSF CAREER Award IIS-1150186 (JP).

8

References

[1] I. E. Ohiorhenuan  F. Mechler  K. P. Purpura  A. M. Schmid  Q. Hu  and J. D. Victor. Sparse coding and

high-order correlations in ﬁne-scale cortical networks. Nature  466(7306):617–621  July 2010.

[2] P. Ravikumar  M. Wainwright  and J. Lafferty. High-dimensional Ising model selection using L1-

regularized logistic regression. The Annals of Statistics  38(3):1287–1319  2010.

[3] E. Ganmor  R. Segev  and E. Schneidman. Sparse low-order interaction network underlies a highly
correlated and learnable neural population code. Proceedings of the National Academy of Sciences 
108(23):9679–9684  2011.

[4] E. Schneidman  M. J. Berry  R. Segev  and W. Bialek. Weak pairwise correlations imply strongly corre-

lated network states in a neural population. Nature  440(7087):1007–1012  Apr 2006.

[5] J. Shlens  G. Field  J. Gauthier  M. Grivich  D. Petrusca  A. Sher  L. A. M.  and E. J. Chichilnisky. The

structure of multi-neuron ﬁring patterns in primate retina. J Neurosci  26:8254–8266  2006.

[6] P. Smolensky. Parallel distributed processing: explorations in the microstructure of cognition  vol. 1.
chapter Information processing in dynamical systems: foundations of harmony theory  pages 194–281.
MIT Press  Cambridge  MA  USA  1986.

[7] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504–507  2006.

[8] G. J. McLachlan and D. Peel. Finite mixture models. Wiley  2000.
[9] M. Bethge and P. Berens. Near-maximum entropy models for binary neural representations of natural

images. Advances in neural information processing systems  20:97–104  2008.

[10] P. M¨uller and F. A. Quintana. Nonparametric bayesian data analysis. Statistical science  19(1):95–110 

2004.

[11] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[12] W. Truccolo and J. P. Donoghue. Nonparametric modeling of neural point processes via stochastic gradi-

ent boosting regression. Neural computation  19(3):672–705  2007.

[13] R. P. Adams  I. Murray  and D. J. C. MacKay. Tractable nonparametric bayesian inference in poisson
processes with gaussian process intensities. In Proceedings of the 26th Annual International Conference
on Machine Learning. ACM New York  NY  USA  2009.

[14] A. Kottas  S. Behseta  D. E. Moorman  V. Poynor  and C. R. Olson. Bayesian nonparametric analysis of

neuronal intensity rates. Journal of Neuroscience Methods  203(1):241–253  January 2012.

[15] B. J. Frey. Graphical models for machine learning and digital communication. MIT Press  1998.
[16] M. Pachitariu  B. Petreska  and M. Sahani. Recurrent linear models of simultaneously-recorded neural

populations. Advances in Neural Information Processing (NIPS)  2013.

[17] E. Archer  I. M. Park  and J. W. Pillow. Bayesian entropy estimation for binary spike train data using

parametric prior knowledge. In Advances in Neural Information Processing Systems (NIPS)  2013.

[18] E. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In Proceedings of the

1969 24th national conference  ACM ’69  pages 157–172  New York  NY  USA  1969. ACM.

9

,Il Memming Park
Evan Archer
Kenneth Latimer
Jonathan Pillow
Jose Alvarez
Mathieu Salzmann