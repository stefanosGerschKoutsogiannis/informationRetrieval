2018,Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima,We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems  by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically  the proposed algorithm only needs $\tilde{O}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to an approximate local minimum $\mathbf{x}$  which satisfies $\|\nabla f(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq -\sqrt{\epsilon}$ in unconstrained stochastic optimization  where $\tilde{O}(\cdot)$ hides logarithm polynomial terms and constants. This improves upon the $\tilde{O}(\epsilon^{-7/2})$ gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of $\tilde{O}(\epsilon^{-1/6})$. Experiments on two nonconvex optimization problems demonstrate the effectiveness of our algorithm and corroborate our theory.,Third-order Smoothness Helps: Faster Stochastic
Optimization Algorithms for Finding Local Minima

Yaodong Yu⇤

Department of Computer Science

University of Virginia

Charlottesville  VA 22904
yy8ms@virginia.edu

Pan Xu⇤

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095
panxu@cs.ucla.edu

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095

qgu@cs.ucla.edu

Abstract

We propose stochastic optimization algorithms that can ﬁnd local minima faster
than existing algorithms for nonconvex optimization problems  by exploiting the
third-order smoothness to escape non-degenerate saddle points more efﬁciently.

More speciﬁcally  the proposed algorithm only needs eO(✏10/3) stochastic gradi-
ent evaluations to converge to an approximate local minimum x  which satisﬁes
krf (x)k2  ✏ and min(r2f (x))  p✏ in unconstrained stochastic optimiza-
tion  where eO(·) hides logarithm polynomial terms and constants. This improves
upon the eO(✏7/2) gradient complexity achieved by the state-of-the-art stochastic
local minima ﬁnding algorithms by a factor of eO(✏1/6). Experiments on two

nonconvex optimization problems demonstrate the effectiveness of our algorithm
and corroborate our theory.

1

Introduction

We study the following unconstrained stochastic optimization problem

f (x) = E⇠⇠D[F (x; ⇠)] 

(1.1)

min
x2Rd

where F (x; ⇠) : Rd ! R is a stochastic function and ⇠ is a random variable sampled from a ﬁxed
distribution D. In particular  we are interested in nonconvex optimization where the expected function
f (x) is not convex. This kind of nonconvex optimization is ubiquitous in machine learning  especially
deep learning [24]. Finding a global minimum of nonconvex problem (1.1) is generally NP hard [18].
Nevertheless  for many nonconvex optimization problems in machine learning  a local minimum is
adequate and can be as good as a global minimum in terms of generalization performance  such as in
deep learning [10  13].
In this paper  we aim to design efﬁcient stochastic optimization algorithms that can ﬁnd an approxi-
mate local minimum of (1.1)  i.e.  an (✏  ✏H)-second-order stationary point x deﬁned as follows

⇤Equal contribution.

krf (x)k2  ✏  and minr2f (x)  ✏H 

(1.2)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

where ✏  ✏H 2 (0  1). Notably  when ✏H = pL2✏ for Hessian Lipschitz f with parameter L2  (1.2)

is equivalent to the deﬁnition of ✏-second-order stationary point [28]. Algorithms based on cubic
regularized Newton’s method [28] and its variants [1  7  12  23  33  31] have been proposed to ﬁnd
such approximate local minima. However  all of them need to solve the cubic problems exactly [28] or
approximately [1  7] in each iteration  which poses a rather heavy computational overhead. Another
line of research employs the negative curvature direction to ﬁnd the local minimum by combining
accelerated gradient descent and negative curvature descent [8  2]  which yet becomes impractical
in large scale and high dimensional machine learning problems due to the frequent computation of
negative curvature in each iteration.
To alleviate the computational burden of local minimum ﬁnding algorithms  there has emerged
a fresh line of research [34  5  21] that tries to achieve the iteration complexity as the state-of-
the-art second-order methods  while only utilizing ﬁrst-order oracles. The key observation is that
ﬁrst-order methods with noise injection [15  20] are essentially an equivalent way to extract the
negative curvature direction around saddle points [34  5]. Together with the Stochastically Controlled
Stochastic Gradient (SCSG) method [25]  the aforementioned methods [34  5] converge to an (✏ p✏)-

by [9] which employed the third-order smoothness of f in deterministic nonconvex optimization to
ﬁnd a ﬁrst-order stationary point  we explore the beneﬁts of third-order smoothness in ﬁnding an
approximate local minimum in the stochastic nonconvex optimization. In particular  we propose a
stochastic optimization algorithm  named as FLASH  which only utilizes ﬁrst-order oracles and ﬁnds

second-order stationary point (an approximate local minimum) within eO(✏7/2) stochastic gradient
evaluations  where eO(·) hides logarithm polynomial factors and constants. In this work  motivated
the (✏  ✏H)-second-order stationary point within eO(✏10/3) stochastic gradient evaluations. Note

that our gradient complexity matches that of the state-of-the-art stochastic optimization algorithm
SCSG [25] for ﬁnding ﬁrst-order stationary points. At the core of our algorithm is an exploitation of
the third-order smoothness of the objective function f which enables us to choose a larger step size
in the negative curvature descent stage  and therefore leads to a faster convergence rate. The main
contributions of our work are as follows
• We show that the third-order smoothness of the nonconvex function can lead to a faster escape from
saddle points in the stochastic optimization. We characterize  for the ﬁrst time  the improvement
brought by third-order smoothness in ﬁnding the approximate local minimum.

evaluations.

• We propose an efﬁcient stochastic algorithm for general stochastic objective functions and prove
faster convergence rates for ﬁnding local minima. More speciﬁcally  for stochastic optimization 
our algorithm converges to an approximate local minimum with only eO(✏10/3) stochastic gradient
• In each outer iteration  our proposed algorithm only performs either one step of negative curvature
descent  or an epoch of SCSG  which saves a lot of gradient and negative curvature computations
compared with existing algorithms.
Notation For a vector x = (x1  ...  xd)> 2 Rd  we denote the `q norm as kxkq = (Pd
i=1 |xi|q)1/q
for 0 < q < +1. For a matrix A = [Aij] 2 Rd⇥d  we use kAk2 and kAkF to denote the spectral
and Frobenius norm. For a three-way tensor T2 Rd⇥d⇥d and vector x 2 Rd  we denote their
inner product as hT   x⌦3i. For a symmetric matrix A  let max(A) and min(A) be the maximum 
minimum eigenvalues of matrix A. We use A ⌫ 0 to denote A is positive semideﬁnite. For two
sequences {an} and {bn}  we denote an = O(bn) if an  C bn for some constant C independent of
n. The notation eO(·) hides logarithmic factors. Additionally  we denote an . bn (an & bn) if an is
less than (larger than) bn up to a constant.

2 Related Work

In this section  we discuss related work for ﬁnding approximate second-order stationary points
in nonconvex optimization. In general  existing literature can be divided into the following three
categories.
Hessian-based: The pioneer work of [28] proposed the cubic regularized Newton’s method to

ﬁnd an (✏  ✏H)-second-order stationary point in O max{✏3/2 ✏ 3

H } iterations. Curtis et al. [12]

showed that the trust-region Newton method can achieve the same iteration complexity as the cubic

2

regularization method. Recently  Kohler and Lucchi [23]  Xu et al. [33] showed that by using
subsampled Hessian matrix instead of the entire Hessian matrix in cubic regularization method and
trust-region method  the iteration complexity can still match the original exact methods under certain
conditions. Zhou et al. [36] improved the second-order oracle complexity (including gradient and
Hessian evaluations) by proposing a variance-reduced Cubic regularization method. However  these
methods need to compute the Hessian matrix and solve a very expensive subproblem either exactly
or approximately in each iteration  which can be computationally intractable for high-dimensional
problems.
Hessian-vector product-based: Through different approaches  Carmon et al. [8] and Agarwal et al.
[1] independently proposed algorithms that are able to ﬁnd (✏ p✏)-second-order stationary points

evaluations. In the general stochastic optimization setting  Allen-Zhu [2] proposed an algorithm
named Natasha2  which is based on variance reduction and negative curvature descent  and is able to

H + n3/4✏7/2

assumption of the third-order smoothness on the objective function and combining the negative
curvature descent with the “convex until proven guilty” algorithm  Carmon et al. [9] proposed

gradient and Hessian-vector product evaluations.2 For nonconvex ﬁnite-sum optimization problems 
Agarwal et al. [1] proposed an algorithm which is able to ﬁnd approximate local minima within

n is the number of component functions. Reddi et al. [30] proposed an algorithm  which combines
ﬁrst-order and second-order methods to ﬁnd approximate (✏  ✏H)-second-order stationary points  and

within eO(✏7/4) full gradient and Hessian-vector product evaluations. By making an additional
an algorithm that is able to ﬁnd an (✏ p✏)-second-order stationary point within eO(✏5/3) full
eO(n✏3/2 + n3/4✏7/4) stochastic gradient and stochastic Hessian-vector product evaluations  where
requires eOn2/3✏2 + n✏3
H  stochastic gradient and stochastic Hessian-vector product
ﬁnd (✏ p✏)-second-order stationary points with at most eO(✏7/2) stochastic gradient and stochastic

Hessian-vector product evaluations. Tripuraneni et al. [31] proposed a stochastic cubic regularization
algorithm to ﬁnd (✏ p✏)-second-order stationary points and achieved the same runtime complexity
as [2].
Gradient-based: For general nonconvex problems  Ghadimi and Lan [16] proposed a randomized
stochastic gradient method and established the complexity of this method for ﬁnding a ﬁrst-order
stationary point. Levy [26]  Jin et al. [20  21] showed that it is possible to escape from saddle points
and ﬁnd local minima only using gradient evaluations plus random perturbation. The best-known

H + n5/12✏2✏1/2

runtime complexity of these methods is eO✏7/4 when ✏H = p✏ [21]. For nonconvex ﬁnite-sum
leading to an algorithm that ﬁnds (✏  ✏H)-second-order stationary points within eOn2/3✏2 + n✏3
H  stochastic gradient evaluations. For nonconvex stochastic optimization

problems  Allen-Zhu and Li [5] proposed a ﬁrst-order negative curvature ﬁnding method called
Neon2 and combined it with the stochastic variance reduced gradient (SVRG) method [22  29  3  25] 
H +
n3/4✏7/2
problems  a variant of stochastic gradient descent (SGD) [15] is proved to ﬁnd the (✏ p✏)-second-
order stationary point within O(✏4poly(d)) stochastic gradient evaluations. More recently  Xu and
Yang [34]  Allen-Zhu and Li [5] turned the ﬁrst-order stationary point ﬁnding method SCSG [25] into
approximate local minima ﬁnding algorithms  which only involves stochastic gradient computation.
H ). In order to further save gradient
and negative curvature computations  [35] considered the number of saddle points encountered in the
algorithm and proposed the gradient descent with one-step escaping algorithm (GOSE) that saves
negative curvature computation. However  none of the above algorithms explore the third-order
smoothness of the nonconvex objective function.

The runtime complexity of these algorithms is eO(✏10/3 + ✏2✏3

3 Preliminaries

In this section  we present deﬁnitions that will be used in our algorithm design and later theoretical
analysis.
Deﬁnition 3.1 (Smoothness). A differentiable function f is L1-smooth  if for any x  y 2 Rd:

krf (x)  rf (y)k2  L1kx  yk2.

2As shown in [9]  the second-order accuracy parameter ✏H can be set as ✏2/3 and the total runtime complexity

remains the same  i.e.  eO(✏5/3).

3

Deﬁnition 3.2 (Hessian Lipschitz). A twice-differentiable function f is L2-Hessian Lipschitz  if for
any x  y 2 Rd:

kr2f (x)  r2f (y)k2  L2kx  yk2.

Note that Hessian-Lipschitz is also referred to as the second-order smoothness. The above two
smoothness conditions are widely used in nonconvex optimization problems [28]. In this paper 
we will further explore the effectiveness of third-order derivative Lipschitz condition in nonconvex
optimization. We use a three-way tensor r3f (x) 2 Rd⇥d⇥d to denote the third-order derivative of a
function  which is formally deﬁned below.
Deﬁnition 3.3 (Third-order Derivative). The third-order derivative of function f: Rd ! R is a
three-way tensor r3f (x) 2 Rd⇥d⇥d which is deﬁned as
@

f (x) 

[r3f (x)]ijk =

@xi@xj@xk

for i  j  k = 1  . . .   d and x 2 Rd.
Next we introduce the deﬁnition of third-order smoothness for function f  which implies that the
third-order derivative will not change rapidly.
Deﬁnition 3.4 (Third-order Derivative Lipschitz). A thrice-differentiable function f has L3-Lipschitz
third-order derivative  if for any x  y 2 Rd:

kr3f (x)  r3f (y)kF  L3kx  yk2.

The above deﬁnition has been introduced in [6]  and the third-order derivative Lipschitz is also
referred to as third-order smoothness in [9]. One can also use another equivalent notion of third-order
derivative Lipschitz condition used in [9]. Note that the third-order Lipschitz condition is critical
in our algorithms and theoretical analysis in later sections. In the sequel  we will use third-order
derivative Lipschitz and third-order smoothness interchangeably.
Deﬁnition 3.5 (Optimal Gap). For a function f  we deﬁne the optimal gap f at point x0 as

f (x0)  inf
x2Rd
Without loss of generality  we assume f < +1.
Deﬁnition 3.6 (Geometric Distribution). For a random integer X  deﬁne X has a geometric distribu-
tion with parameter p  denoted as Geom(p)  if it satisﬁes that

f (x)  f .

P(X = k) = pk(1  p) 

8k = 0  1  . . . .

Deﬁnition 3.7 (Sub-Gaussian Stochastic Gradient). For any x 2 Rd and random variable ⇠ 2D   the
stochastic gradient rF (x; ⇠) is sub-Gaussian with parameter  if it satisﬁes
◆  exp(1).

E exp✓krF (x; ⇠)  rf (x)k2

2

2

In addition  we introduce Tg to denote the time complexity of stochastic function value and gradient
evaluation  i.e.  (F (x; ⇠i) rF (x; ⇠i)) for ⇠i 2D   and Th to denote the time complexity of stochastic
Hessian-vector product evaluation  i.e.  r2F (x; ⇠i)v for a given vector v and ⇠i 2D .
4 Exploiting Third-order Smoothness

In this section we will show how to employ the third-order smoothness of the objective function
to make better use of the negative curvature direction for escaping saddle points. We ﬁrst give an
enlightening explanation on why third-order smoothness helps in general nonconvex optimization
problems. Then we present our main algorithm which is able to utilize the third-order smoothness to
take a larger step size for general stochastic optimization.
In order to ﬁnd local minima in nonconvex problems  different kinds of approaches have been
explored to escape from saddle points. One of these approaches is to use negative curvature direction
[27] to escape from saddle points  which has been explored in many existing studies [8  11  2].

4

.

✏H
2

According to recent work by [34  5]  one can extract the negative curvature direction by only using
stochastic gradient evaluations  which makes the negative curvature descent approach more appealing.
We ﬁrst consider a simple case to illustrate how to utilize the third-order smoothness when taking a
negative curvature descent step. For nonconvex optimization problems  an ✏-ﬁrst-order stationary

pointbx can be found by using ﬁrst-order methods such as gradient descent. Ifbx is not an (✏  ✏H)-
second-order stationary point deﬁned in (1.2)  then there must exist a unit vectorbv such that
As studied in [8  34  5]  one can take a negative curvature descent step along the direction ofbv to
escape from the saddle pointbx  i.e. 
ey = argmin
wheree↵ is the step size. Suppose the function f is L1-smooth and L2-Hessian Lipschitz  then the
step size can be set ase↵ = O(✏H/L2) and the negative curvature descent step (4.1) is guaranteed to

bv>r2f (bx)bv  
f (y)  u =bx e↵bv  w =bx +e↵bv 
f (ey)  f (bx) = O✓ ✏3
2◆.

Inspired by the previous work [9]  we aim to achieve more function value decrease than (4.2) by
incorporating an additional assumption that the objective function has L3-Lipschitz third-order
derivatives (third-order smoothness). More speciﬁcally  we adjust the negative curvature descent step
in (4.1) as follows 

attain the following function value decrease 

y2{u w}

H
L2

where ↵ = O(p✏H/L3 ) is the adjusted step size which can be much larger than the step sizee↵ in

(4.1) when ✏H is sufﬁciently small. The adjusted negative curvature descent step (4.3) is guaranteed
to decrease the function value by a larger decrement  i.e. 

by = argmin

y2{u w}

f (y)  u =bx  ↵bv  w =bx + ↵bv 
L3◆.
f (by)  f (bx) = O✓ ✏2

H

(4.1)

(4.2)

(4.3)

(4.4)

Compared with (4.2)  the decrement in (4.4) can be substantially larger. In other words  if we make
the additional assumption of the third-order smoothness  the negative curvature descent with larger
step size will make more progress toward decreasing the function value. Note that [9] focuses on
deterministic optimization  while our work is focused on the stochastic optimization. Here we need
to carefully design our algorithm to improve the computational complexity in the stochastic setting.
In the following  we will present an algorithm for stochastic nonconvex optimization which exploits
the beneﬁts of third-order smoothness to escape from saddle points . Recall the general stochastic
optimization problem in (1.1). In this setting  one cannot have access to the full gradient or Hessian
information. Instead  only stochastic gradient and stochastic Hessian-vector product evaluations are
accessible. As a result  we have to employ stochastic optimization methods to calculate the negative

curvature direction. There exist two kinds of methods to calculate the negative curvature directionbv

for the general stochastic problem. The ﬁrst kind is an online PCA method  i.e.  Oja’s algorithm [4] 
which uses Hessian-vector product evaluations and can be seen as a stochastic variant of FastPCA
[14]. Another method is the online version of the Neon algorithm  denote as Neon2online [5]  which
only requires stochastic gradient evaluations.
By using either Oja’s algorithm or Neon2online  there exists an algorithm  denoted by ApproxNC-
Stochastic  which uses stochastic gradient evaluations or stochastic Hessian-vector product evaluations
to ﬁnd the negative curvature direction for general stochastic nonconvex optimization problem (1.1).

Speciﬁcally  ApproxNC-Stochastic returns a unit vectorbv that satisﬁesbv>r2f (x)bv  ✏H/2
provided min(r2f (x)) < ✏H  otherwise it will returnbv = ?. Based on ApproxNC-Stochastic 

we present our negative curvature descent algorithm in Algorithm 1.
Note that the Rademacher random variable ⇣ is an important feature in Algorithm 1. As we cannot
access the full objective function value in stochastic setting  we use a Rademacher variable (⇣ = 1 or
⇣ = 1 with probability 1/2) in our algorithm to decide the direction of negative curvature descent step.

5

i=1    ✏H)

Algorithm 1 NCD3-Stochastic (f  x  {Li}3
1: Set ↵ =p3✏H/L3
2: bv ApproxNC-Stochastic(f  x  L1  L2  ✏ H)
3: ifbv 6= ?
by x + ⇣ ↵bv
returnby
return ?

generate a Rademacher random variable ⇣

4:
5:
6:
7: else
8:

Therefore  with the step size ↵ = O(p✏H/L3) for the negative curvature descent step  Algorithm
1 can make greater progress in expectation when min(r2f (x)) < ✏H  and we summarize this
property as follows.
Lemma 4.1. Let f (x) = E⇠⇠D[F (x; ⇠)] and each stochastic function F (x; ⇠) is L1-smooth  L2-
Hessian Lipschitz continuous  and the third derivative of f (x) is L3-Lipschitz. Set ✏H 2 (0  1) and
step size as ↵ =p3✏H/L3. If the input x of Algorithm 1 satisﬁes min(r2f (x)) < ✏H  then
with probability 1    Algorithm 1 will returnby such that E⇣[f (x)  f (by)]  3✏2
H/8L3  where
 2 (0  1) and E⇣ denotes the expectation over the Rademacher random variable ⇣. Furthermore  if
we choose   ✏H/(3✏H + 8L2)  it holds that

E[f (by)  f (x)]  

✏2
H
8L3

 

where E is over all randomness of the algorithm  and the total runtime is eOL2
ApproxNC-Stochastic adopts online Oja’s algorithm  and eOL2

HTh if
HTg if ApproxNC-Stochastic

adopts Neon2online.

1/✏2

1/✏2

5 Fast Local Minima Finding Algorithm

In this section  we present our main algorithm to ﬁnd approximate local minima for nonconvex
stochastic optimization problems  based on the negative curvature descent algorithms proposed in
previous section.
To ﬁnd the local minimum  we use SCSG [25]  which is the state-of-the-art stochastic optimization
algorithm  to ﬁnd a ﬁrst-order stationary point and then apply Algorithm 1 to escape the saddle point
using negative curvature direction. The proposed method is presented in Algorithm 2  We use a
subsampled stochastic gradient rfS(x) in the outer loop (Line 4) of Algorithm 2  which is deﬁned
as rfS(x) = 1/|S|Pi2S rF (x; ⇠i).
As shown in Algorithm 2  we use subsampled gradient to check whether xk1 is a ﬁrst-order
stationary point. Suppose the stochastic gradient rF (x; ⇠) satisﬁes the gradient sub-Gaussian
condition (3.7) and the batch size |Sk| is large enough  then krf (xk1)k2 >✏/ 4 holds with high
probability if krfSk (xk1)k2 >✏/ 2. Similarly  krf (xk1)k2  ✏ holds with high probability if
krfSk (xk1)k2  ✏/2.
Note that each iteration of the outer loop in Algorithm 2 consists of two cases: (1) if the norm
of subsampled gradient rfSk (xk1) is small  then we run one subroutine NCD3-Stochastic  i.e. 
Algorithm 1; and (2) if the norm of rfSk (xk1) is large  then we run one epoch of SCSG algorithm.
This design can reduce the number of negative curvature calculations. There are two major differences
between Algorithm 2 and existing algorithms in [34  5]: (1) the step size of negative curvature descent
step in Algorithm 2 is larger; and (2) the minibatch size in each epoch of SCSG in Algorithm 2 can
be set to 1 instead of being related to the accuracy parameters ✏ and ✏H  while the minibatch size in
each epoch of SCSG in the existing algorithms [34  5] has to depend on ✏ and ✏H.
Now we present the following theorem which spells out the runtime complexity of Algorithm 2.
Theorem 5.1. Let f (x) = E⇠⇠D[F (x; ⇠)]. Suppose the third derivative of f (x) is L3-Lipschitz 
and each stochastic function F (x; ⇠) is L1-smooth and L2-Hessian Lipschitz continuous. Suppose
that the stochastic gradient rF (x; ⇠) satisﬁes the gradient sub-Gaussian condition with parameter

6

Algorithm 2 Fast Local minimA ﬁnding with third-order SmootHness (FLASH-Stochastic)
1: Input: f  x0  L1  L2  L3    ✏  ✏H  b  K

2: Set B eO(2/✏2)  ⌘ = b2/3/(3L1B2/3)
uniformly sample a batch Sk ⇠D with |Sk| = B
gk rfSk (xk1)
if kgkk2 >✏/ 2
generate Tk ⇠ Geom(B/(B + b))
y(k)
0 xk1
for t = 1  ...  Tk
randomly pick It ⇢D with |It| = b
t1 rfIt(y(k)
⌫(k)
t1)  rfIt(y(k)
y(k)
t y(k)
t1  ⌘⌫(k)
t1
end for
xk y(k)
else
xk NCD3-Stochastic(f  xk1 {Li}3
if xk = ?

3: for k = 1  2  ...  K
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for

return xk1

0 ) + gk

Tk

i=1  ✏ H)

. Set batch size B = eO(2/✏2) and ✏H & ✏2/3. If Algorithm 2 adopts online Oja’s algorithm to

compute the negative curvature  then Algorithm 2 ﬁnds an (✏  ✏H)-second-order stationary point with
probability at least 1/3 in runtime

If Algorithm 2 adopts Neon2online  then it ﬁnds an (✏  ✏H)-second-order stationary point with proba-
bility at least 1/3 in runtime

✏10/3 +

eO✓✓ L14/3f
eO✓✓ L14/3f

L32f

✏2✏2

H ◆Tg +✓ L2

1L3f
✏4

H ◆Th◆.

✏10/3 +

L32f

✏2✏2
H

+

L2

1L3f
✏4

H ◆Tg◆.

H +✏4

✏2✏2
H +
✏5
H ) runtime complexity achieved by the state-of-the-art [5]  the runtime complexity of Algorithm 2
is improved upon the state-of-the-art in the second and third terms. If we set ✏H = p✏  the runtime of

Remark 5.2. Although the runtime complexity in Theorem 5.1 holds with a constant probability 
one can repeatedly run Algorithm 2 for at most log(1/) times to achieve a high probability result
with probability at least 1  .
Remark 5.3. Theorem 5.1 suggests that the runtime complexity of Algorithm 2 is eO(✏10/3 +
H ) to ﬁnd an (✏  ✏H)-second-order stationary point. Compared with eO(✏10/3 +✏2✏3
Algorithm 2 is eO(✏10/3) and that of the state-of-the-art stochastic local minima ﬁnding algorithms
[2  31  34  5] becomes eO(✏7/2)  thus Algorithm 2 outperforms the state-of-the-art algorithms by a
factor of eO(✏1/6).
complexity of Algorithm 2 remains eO(✏10/3). It is also worth noting that the runtime complexity of

Algorithm 2 matches that of the state-of-the-art stochastic optimization algorithm (SCSG) [25] which
only ﬁnds ﬁrst-order stationary points but does not impose the third-order smoothness assumption.

Remark 5.4. Note that we can set ✏H to a smaller value  i.e.  ✏H = ✏2/3  and the total runtime

6 Experiments

In this section  we conduct numerical experiment on two nonconvex optimization problems  i.e. 
matrix sensing and deep autoencoder. All the experiments are carried on Amazon AWS p2.xlarge
nodes with NVIDIA GK210 GPUs  and we use Pytorch 0.3.0 to implement all the algorithms.

7

1.2

1

0.8

0.6

0.4

0.2

e
u

l
a
V
n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

1.2

1

0.8

0.6

0.4

0.2

e
u

l
a
V
n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

1.2

1

0.8

0.6

0.4

0.2

e
u

l
a
V
n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

2 =  
2 = 0.1 
2 = 0.01 
2 = 0.001 

1.2

1

0.8

0.6

0.4

0.2

e
u

l
a
V
n
o
i
t
c
n
u
F

e
v
i
t
c
e
j
b
O

2 =  
2 = 0.1 
2 = 0.01 
2 = 0.001 

0

2

6

8
4
Oracle Calls

10

12
#104

0

2

6

8
4
Oracle Calls

10

12
#104

0

2

Oracle Calls

4

#104

0

4
2
Oracle Calls

6
#104

(a) Matrix Sensing (d =
50)

(b) Matrix Sensing (d =
100)

(c) Varying NC Step Size
(d = 50)

(d) Varying NC Step Size 
(d = 100)

0.1

0.08

0.06

0.04

0.02

s
s
o
L

g
n

i

n

i
a
r
T

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

0.1

0.08

0.06

0.04

0.02

s
s
o
L

t
s
e
T

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

0.1

0.08

0.06

0.04

0.02

s
s
o
L

g
n

i

n

i
a
r
T

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

0.1

0.08

0.06

0.04

0.02

s
s
o
L

t
s
e
T

SGD
NSGD
SGD-m
SCSG
Neon
Neon2
FLASH

0

0

2

4

6

Oracle Calls

8

10
#106

0

0

2

4

6

Oracle Calls

8

10
#106

0

0

2

4

6

Oracle Calls

8

10
#106

0

0

2

4

6

Oracle Calls

8

10
#106

(e) AE-1  Training

(f) AE-1  Test

(g) AE-2  Training

(h) AE-2  Test

Figure 1: Numerical results for matrix sensing and deep autoencoder. (a)-(b) Convergence of different
algorithms for matrix sensing: objective function value versus the number of oracle calls. (c)-(d)
Different negative curvature step size comparison of FLASH for matrix sensing. (e)-(h) Convergence
of different algorithms for two deep autoencoders: Training loss versus the number of oracle calls
and test loss versus the number of oracle calls.

Matrix Sensing We consider the symmetric matrix sensing problem  which is deﬁned as:

f (U) =

min
U2Rd⇥r

1
2m

mXi=1hAi  UU>i  bi2 

(6.1)

where the matrices {Ai}i=1 ... m are known sensing matrices  bi = hAi  M⇤i is the i-th observation 
and M⇤ = U⇤(U⇤)> is an unknown low-rank matrix  which needs to be recovered. For the data
generation  we consider two matrix sensing problems: (1) d = 50  r = 3  and (2) d = 100  r = 3 
then generate m = 20d sensing matrices A1  . . .   Am  where each element of the sensing matrix Ai
follows i.i.d. standard normal distribution  and the unknown low-rank matrix M⇤ as M⇤ = U⇤(U⇤)> 
where U⇤ 2 Rd⇥r is randomly generated  and thus bi = hAi  M⇤i. Next we randomly initialize a
vector u0 2 Rd satisfying ku0k2 < max(M⇤) and set the initial input U0 as U0 = [u0  0  . . .   0].
Deep Autoencoder We also perform experiments of training a deep autoencoder on MNIST dataset
[19]. The MNIST dataset contains images of handwritten digits  including 60  000 training examples
and 10  000 test examples. Each image has 28 ⇥ 28 pixels. We consider two autoencoders: (1) a
fully connected encoder with layers of size (28 ⇥ 28)-1024-512-256-32 and a symmetric decoder
(AE-1) and (2) a fully connected encoder with layers of size (28 ⇥ 28)-1024-512-256-128-56-32 and
a symmetric decoder (AE-2);. The code layer with 32 units are linear and we use softplus function as
the activation function for other layers. We use mean squared error (MSE) as the loss function.
We evaluate our algorithm FLASH-Stochastic (FLASH for short) together with the following state-
of-the-art stochastic optimization algorithms for nonconvex problems: (1) stochastic gradient descent
(SGD); (2) SGD with momentum (SGD-m); (3) noisy stochastic gradient descent (NSGD) [15]; (4)
Stochastically Controlled Stochastic Gradient (SCSG) [25]; (5) NEgative-curvature-Originated-from-
Noise (Neon) [34]; (6) NEgative-curvature-Originated-from-Noise 2 (Neon2) [5]. A ﬁxed gradient
mini-batch size of 100 is used for all the algorithms. We apply Oja’s algorithm with a Hessian
mini-batch size of 100 to calculate the negative curvature in FLASH. We perform a grid search
over step sizes for each method. For the negative curvature step size ↵  we choose ↵ = O(✏H/L2

for Neon  Neon2 and ↵ = O(p✏H/L3) for our algorithm FLASH according to the corresponding

theories  where ✏H = 0.001  and tune the constant parameter in the negative curvature step size by
grid search. We report the objective function value versus oracle calls on matrix sensing and training
loss versus oracle calls on matrix sensing and deep autoencoder.

8

The experimental results of the above two nonconvex problems are shown in Figure 1. For the matrix
sensing problem  in Figure 1(a)-1(b)  we observe that without adding noise or using second-order
information  SGD  SGD-m and SCSG are not able to escape from saddle points. Our algorithm and
NSGD  Neon  Neon2 can escape from saddle points  and our algorithm converges to the unknown
matrix faster than NSGD  Neon  Neon2. As we can see from Figure 1(e)-1(h)  for deep autoencoder 
compared with SGD  SGD-m  NSGD  SCSG  Neon and Neon2  our algorithm escapes from saddle
points faster and converges faster. Our algorithm outperforms Neon and Neon2 on both problems
and validates our theoretical analysis that negative curvature step with a larger step size is helpful
in stochastic nonconvex optimization problems. We also compare the convergence behavior of our
algorithm with different step sizes for negative curvature descent. We ﬁrst set initial step size ↵ = 0.2
(for negative curvature descent) and then decrease the step size by a factor of 0.1 each time  while the
other parameters remain the same. We can see from Figure 1(c) and 1(d) that our algorithm FLASH
converges faster with larger step sizes for negative curvature descent  which validates our theories on
third-order smoothness can be helpful in the nonconvex stochastic optimization.

7 Conclusions

In this paper  we investigated the beneﬁt of third-order smoothness of nonconvex objective functions
in stochastic optimization. We illustrated that third-order smoothness can help faster escape saddle
points  by proposing a new negative curvature descent algorithms with improved theoretical guarantee.
Based on the proposed negative curvature descent algorithm  we further proposed a practical stochastic
optimization algorithm with improved run time complexity that ﬁnds local minima for stochastic
nonconvex optimization problems.

Acknowledgements

We would like to thank the anonymous reviewers for their helpful comments  and Yu Chen  Xuwang
Yin for their helpful discussions on the experiments. This research was sponsored in part by
the National Science Foundation IIS-1652539 and BIGDATA IIS-1855099. We also thank AWS
for providing cloud computing credits associated with the NSF BIGDATA award. The views
and conclusions contained in this paper are those of the authors and should not be interpreted as
representing any funding agencies.

References
[1] Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Find-
ing approximate local minima for nonconvex optimization in linear time. arXiv preprint
arXiv:1611.01146  2016.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. arXiv preprint

arXiv:1708.08694  2017.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[4] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the compressed leader: Faster algorithms for matrix

multiplicative weight updates. arXiv preprint arXiv:1701.01722  2017.

[5] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via ﬁrst-order oracles. arXiv

preprint arXiv:1711.06673  2017.

[6] Animashree Anandkumar and Rong Ge. Efﬁcient approaches for escaping higher order saddle
points in non-convex optimization. In Conference on Learning Theory  pages 81–102  2016.

[7] Yair Carmon and John C Duchi. Gradient descent efﬁciently ﬁnds the cubic-regularized

non-convex newton step. arXiv preprint arXiv:1612.00547  2016.

[8] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for

non-convex optimization. arXiv preprint arXiv:1611.00756  2016.

9

[9] Yair Carmon  Oliver Hinder  John C Duchi  and Aaron Sidford. " convex until proven guilty":
Dimension-free acceleration of gradient descent on non-convex functions. arXiv preprint
arXiv:1705.02766  2017.

[10] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204 
2015.

[11] Frank E Curtis and Daniel P Robinson. Exploiting negative curvature in deterministic and

stochastic optimization. arXiv preprint arXiv:1703.00412  2017.

[12] Frank E Curtis  Daniel P Robinson  and Mohammadreza Samadi. A trust region algorithm with
a worst-case iteration complexity of\mathcal {O}(\epsilonˆ{-3/2}) for nonconvex optimization.
Mathematical Programming  162(1-2):1–32  2017.

[13] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in neural information processing systems  pages 2933–2941 
2014.

[14] Dan Garber  Elad Hazan  Chi Jin  Cameron Musco  Praneeth Netrapalli  Aaron Sidford  et al.
Faster eigenvector computation via shift-and-invert preconditioning. In International Conference
on Machine Learning  pages 2626–2634  2016.

[15] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online
stochastic gradient for tensor decomposition. In Conference on Learning Theory  pages 797–
842  2015.

[16] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[17] Saeed Ghadimi  Guanghui Lan  and Hongchao Zhang. Mini-batch stochastic approximation
methods for nonconvex stochastic composite optimization. Mathematical Programming  155
(1-2):267–305  2016.

[18] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM

(JACM)  60(6):45  2013.

[19] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with

neural networks. science  313(5786):504–507  2006.

[20] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape

saddle points efﬁciently. arXiv preprint arXiv:1703.00887  2017.

[21] Chi Jin  Praneeth Netrapalli  and Michael I Jordan. Accelerated gradient descent escapes saddle

points faster than gradient descent. arXiv preprint arXiv:1711.10456  2017.

[22] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[23] Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex

optimization. arXiv preprint arXiv:1705.05933  2017.

[24] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[25] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex ﬁnite-sum optimization

via scsg methods. arXiv preprint arXiv:1706.09156  2017.

[26] Kﬁr Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint

arXiv:1611.04831  2016.

[27] Jorge J Moré and Danny C Sorensen. On the use of directions of negative curvature in a modiﬁed

newton method. Mathematical Programming  16(1):1–20  1979.

10

[28] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[29] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In International conference on machine learning 
pages 314–323  2016.

[30] Sashank J Reddi  Manzil Zaheer  Suvrit Sra  Barnabas Poczos  Francis Bach  Ruslan Salakhut-
dinov  and Alexander J Smola. A generic approach for escaping saddle points. arXiv preprint
arXiv:1709.01434  2017.

[31] Nilesh Tripuraneni  Mitchell Stern  Chi Jin  Jeffrey Regier  and Michael I Jordan. Stochastic
cubic regularization for fast nonconvex optimization. arXiv preprint arXiv:1711.02838  2017.
[32] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[33] Peng Xu  Farbod Roosta-Khorasani  and Michael W Mahoney. Newton-type methods for
non-convex optimization under inexact hessian information. arXiv preprint arXiv:1708.07164 
2017.

[34] Yi Xu and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in

almost linear time. arXiv preprint arXiv:1711.01944  2017.

[35] Yaodong Yu  Difan Zou  and Quanquan Gu. Saving gradient and negative curvature computa-

tions: Finding local minima more efﬁciently. arXiv preprint arXiv:1712.03950  2017.

[36] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic variance-reduced cubic regularized
Newton methods. In Proceedings of the 35th International Conference on Machine Learning 
volume 80  pages 5990–5999  10–15 Jul 2018.

11

,Yaodong Yu
Pan Xu
Quanquan Gu