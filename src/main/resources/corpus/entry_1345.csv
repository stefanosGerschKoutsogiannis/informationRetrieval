2018,Deep Generative Models with Learnable Knowledge Constraints,The broad set of deep generative models (DGMs) has achieved remarkable advances. However  it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models  but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified {\it a priori}  which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper  we establish mathematical correspondence between PR and reinforcement learning (RL)  and  based on the connection  expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs  and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.,Deep Generative Models with Learnable

Knowledge Constraints

Zhiting Hu  Zichao Yang  Ruslan Salakhutdinov 

Xiaodan Liang  Lianhui Qin  Haoye Dong  Eric P. Xing

{zhitingh zichaoy rsalakhu xiaodan1}@cs.cmu.edu  eric.xing@petuum.com

Carnegie Mellon University  Petuum Inc.

Abstract

The broad set of deep generative models (DGMs) has achieved remarkable ad-
vances. However  it is often difÔ¨Åcult to incorporate rich structured domain knowl-
edge with the end-to-end DGMs. Posterior regularization (PR) offers a principled
framework to impose structured constraints on probabilistic models  but has limited
applicability to the diverse DGMs that can lack a Bayesian formulation or even
explicit density evaluation. PR also requires constraints to be fully speciÔ¨Åed a
priori  which is impractical or suboptimal for complex knowledge with learnable
uncertain parts. In this paper  we establish mathematical correspondence between
PR and reinforcement learning (RL)  and  based on the connection  expand PR to
learn constraints as the extrinsic reward in RL. The resulting algorithm is model-
agnostic to apply to any DGMs  and is Ô¨Çexible to adapt arbitrary constraints with
the model jointly. Experiments on human image generation and templated sentence
generation show models with learned knowledge constraints by our algorithm
greatly improve over base generative models.

1

Introduction

Generative models provide a powerful mechanism for learning data distributions and simulating
samples. Recent years have seen remarkable advances especially on the deep approaches [16  25]
such as Generative Adversarial Networks (GANs) [15]  Variational Autoencoders (VAEs) [27] 
auto-regressive networks [29  42]  and so forth. However  it is usually difÔ¨Åcult to exploit in these
various deep generative models rich problem structures and domain knowledge (e.g.  the human
body structure in image generation  Figure 1). Many times we have to hope the deep networks can
discover the structures from massive data by themselves  leaving much valuable domain knowledge
unused. Recent efforts of designing specialized network architectures or learning disentangled
representations [5  23] are usually only applicable to speciÔ¨Åc knowledge  models  or tasks. It is
therefore highly desirable to have a general means of incorporating arbitrary structured knowledge
with any types of deep generative models in a principled way.
On the other hand  posterior regularization (PR) [13] is a principled framework to impose knowledge
constraints on posterior distributions of probabilistic models  and has shown effectiveness in regulating
the learning of models in different context. For example  [21] extends PR to incorporate structured
logic rules with neural classiÔ¨Åers. However  the previous approaches are not directly applicable to the
general case of deep generative models  as many of the models (e.g.  GANs  many auto-regressive
networks) are not straightforwardly formulated with the probabilistic Bayesian framework and do not
possess a posterior distribution or even meaningful latent variables. Moreover  PR has required a
priori Ô¨Åxed constraints. That means users have to fully specify the constraints beforehand  which can
be impractical due to heavy engineering  or suboptimal without adaptivity to the data and models. To
extend the scope of applicable knowledge and reduce engineering burden  it is necessary to allow

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr√©al  Canada.

Figure 1: Two example applications of imposing learnable knowledge constraints on generative
models. Left: Given a person image and a target pose (deÔ¨Åned by key points)  the goal is to generate
an image of the person under the new pose. The constraint is to force the human parts (e.g.  head) of
the generated image to match those of the true target image. Right: Given a text template  the goal is
to generate a complete sentence following the template. The constraint is to force the match between
the inÔ¨Ålling content of the generated sentence with the true content. (See sec 5 for more details.)

users to specify only partial or fuzzy structures  while learning remaining parts of the constraints
jointly with the regulated model.
To this end  we establish formal connections between the PR framework with a broad set of algorithms
in the control and reinforcement learning (RL) domains  and  based on the connections  transfer
well-developed RL techniques for constraint learning in PR. In particular  though the PR framework
and the RL are apparently distinct paradigms applied in different context  we show mathematical
correspondence between the model and constraints in PR with the policy and reward in entropy-
regularized policy optimization [43  45  1]  respectively. This thus naturally inspires to leverage
relevant approach from the RL domain (speciÔ¨Åcally  the maximum entropy inverse RL [56  11]) to
learn the PR constraints from data (i.e.  demonstrations in RL).
Based on the uniÔ¨Åed perspective  we drive a practical algorithm with efÔ¨Åcient estimations and
moderate approximations. The algorithm is efÔ¨Åcient to regularize large target space with arbitrary
constraints  Ô¨Çexible to couple adapting the constraints with learning the model  and model-agnostic to
apply to diverse deep generative models  including implicit models where generative density cannot
be evaluated [40  15]. We demonstrate the effectiveness of the proposed approach in both image and
text generation (Figure 1). Leveraging domain knowledge of structure-preserving constraints  the
resulting models improve over base generative models.

2 Related Work

It is of increasing interest to incorporate problem structures and domain knowledge in machine
learning approaches [49  13  21]. The added structure helps to facilitate learning  enhance general-
ization  and improve interpretability. For deep neural models  one of the common ways is to design
specialized network architectures or features for speciÔ¨Åc tasks (e.g.  [2  34  28  33]). Such a method
typically has a limited scope of applicable tasks  models  or knowledge. On the other hand  for
structured probabilistic models  posterior regularization (PR) and related frameworks [13  32  4]
provide a general means to impose knowledge constraints during model estimation. [21] develops
iterative knowledge distillation based on PR to regularize neural networks with any logic rules.
However  the application of PR to the broad class of deep generative models has been hindered  as
many of the models do not even possess meaningful latent variables or explicit density evaluation (i.e. 
implicit models). Previous attempts thus are limited to applying simple max-margin constraints [31].
The requirement of a priori Ô¨Åxed constraints has also made PR impractical for complex  uncertain
knowledge. Previous efforts to alleviate the issue either require additional manual supervision [39] or
is limited to regularizing small label space [22]. This paper develops a practical algorithm that is
generally applicable to any deep generative models and any learnable constraints on arbitrary (large)
target space.
Our work builds connections between the Bayesian PR framework and reinforcement learning. A
relevant  broad research topic of formalizing RL as a probabilistic inference problem has been
explored in the RL literature [6  7  41  30  1  48]  where rich approximate inference tools are used
to improve the modeling and reasoning for various RL algorithms. The link between RL and PR

2

Structured	consistencyConstraint ùëì"source	imagetarget	poseGenerativemodel ùëù$truetargetgenerated	imageHuman	part	parserLearnable	module	ùúô‚Äúmeant	todnot	to	.‚Äù‚ÄúIt	wasmeant	to	dazzlenot	to	make	it.‚Äù‚ÄúIt	was	meant	to	dazzle	not	to	make	sense.‚ÄùGenerativemodel ùëù$true target:generated:Infilling	contentmatchingLearnable	module	ùúôConstraint ùëì"template:Components

PR

x data/generations

p(x)
f (x)/R(x)
q(x)

generative model pŒ∏
constraint fœÜ
variational distr. q  Eq.3

Entropy-Reg RL
action-state samples
(old) policy pœÄ
reward R
(new) policy qœÄ

MaxEnt IRL
demonstrations
‚Äî
reward RœÜ
policy qœÜ

(Energy) GANs
data/generations
generator
discriminator
‚Äî

Table 1: UniÔ¨Åed perspective of the different approaches  showing mathematical correspondence
of PR with the entropy-regularized RL (sec 3.2.1) and maximum entropy IRL (sec 3.2.2)  and its
(conceptual) relations to (energy-based) GANs (sec 4).

has not been previously studied. We establish the mathematical correspondence  and  differing from
the RL literature  we in turn transfer the tools from RL to expand the probabilistic PR framework.
Inverse reinforcement learning (IRL) seeks to learn a reward function from expert demonstrations.
Recent approaches based on maximum-entropy IRL [56] are developed to learn both the reward and
policy [11  10  12]. We adopt the maximum-entropy IRL formulation to derive the constraint learning
objective in our algorithm  and leverage the unique structure of PR for efÔ¨Åcient importance sampling
estimation  which differs from these previous approaches.

3 Connecting Posterior Regularization to Reinforcement Learning

3.1 PR for Deep Generative Models

PR [13] was originally proposed to provide a principled framework for incorporating constraints on
posterior distributions of probabilistic models with latent variables. The formulation is not generally
applicable to deep generative models as many of them (e.g.  GANs and autoregressive models) are not
formulated within the Bayesian framework and do not possess a valid posterior distribution or even
semantically meaningful latent variables. Here we adopt a slightly adapted formulation that makes
minimal assumptions on the speciÔ¨Åcations of the model to regularize. It is worth noting that though
we present in the generative model context  the formulations  including the algorithm developed later
(sec 4)  can straightforwardly be extended to other settings such as discriminative models.
Consider a generative model x ‚àº pŒ∏(x) with parameters Œ∏. Note that generation of x can condition
on arbitrary other elements (e.g.  the source image for image transformation) which are omitted for
simplicity of notations. Denote the original objective of pŒ∏(x) with L(Œ∏). PR augments the objective
by adding a constraint term encoding the domain knowledge. Without loss of generality  consider
constraint function f (x) ‚àà R  such that a higher f (x) value indicates a better x in terms of the
particular knowledge. Note that f can also involve other factors such as latent variables and extra
supervisions  and can include a set of multiple constraints.
A straightforward way to impose the constraint on the model is to maximize EpŒ∏ [f (x)]. Such method
is efÔ¨Åcient only when pŒ∏ is a GAN-like implicit generative model or an explicit distribution that
can be efÔ¨Åciently reparameterized (e.g.  Gaussian [27]). For other models such as the large set of
non-reparameterizable explicit distributions  the gradient ‚àáŒ∏EpŒ∏ [f (x)] is usually computed with
the log-derivative trick and can suffer from high variance. For broad applicability and efÔ¨Åcient
optimization  PR instead imposes the constraint on an auxiliary variational distribution q  which is
encouraged to stay close to pŒ∏ through a KL divergence term:

L(Œ∏  q) = KL(q(x)(cid:107)pŒ∏(x)) ‚àí Œ±Eq [f (x)]  

(1)
where Œ± is the weight of the constraint term. The PR objective for learning the model is written as:
(2)
where Œª is the balancing hyperparameter. As optimizing the original model objective L(Œ∏) is
straightforward and depends on the speciÔ¨Åc generative model of choice  in the following we omit the
discussion of L(Œ∏) and focus on L(Œ∏  q) introduced by the framework.
The problem is solved using an EM-style algorithm [13  21]. SpeciÔ¨Åcally  the E-step optimizes Eq.(1)
w.r.t q  which is convex and has a closed-form solution at each iteration given Œ∏:

minŒ∏ q L(Œ∏) + ŒªL(Œ∏  q) 

‚àó

q

(x) = pŒ∏(x) exp{Œ±f (x)} /Z 

(3)

3

where Z is the normalization term. We can see q‚àó as an energy-based distribution with the negative
energy deÔ¨Åned by Œ±f (x) + log pŒ∏(x). With q from the E-step Ô¨Åxed  the M-step optimizes Eq.(1)
w.r.t Œ∏ with:

minŒ∏ KL(q(x)(cid:107)pŒ∏(x)) = minŒ∏ ‚àíEq [log pŒ∏(x)] + const.

(4)

Constraint f in PR has to be fully-speciÔ¨Åed a priori and is Ô¨Åxed throughout the learning. It would be
desirable or even necessary to enable learnable constraints so that practitioners are allowed to specify
only the known components of f while leaving any unknown or uncertain components automatically
learned. For example  for human image generation in Figure 1  left panel  users are able to specify
structures on the parsed human parts  while it is impractical to also manually engineer the human part
parser that involves recognizing parts from raw image pixels. It is favorable to instead cast the parser
as a learnable module in the constraint. Though it is possible to pre-train the module and simply Ô¨Åx
in PR  the lack of adaptivity to the data and model can lead to suboptimal results  as shown in the
empirical study (Table 2). This necessitates to expand the PR framework to enable joint learning of
constraints with the model.
Denote the constraint function with learnable components as fœÜ(x)  where œÜ can be of various
forms that are optimizable  such as the free parameters of a structural model  or a graph structure to
optimize.
Simple way of learning the constraint. A straightforward way to learn the constraint is to directly
optimize Eq.(1) w.r.t œÜ in the M-step  yielding

maxœÜ Ex‚àºq(x)[fœÜ(x)].

(5)
That is  the constraint is trained to Ô¨Åt to the samples from the current regularized model q. However 
such objective can be problematic as the generated samples can be of low quality  e.g.  due to poor
state of the generative parameter Œ∏ at initial stages  or insufÔ¨Åcient capability of the generative model
per se.
In this paper  we propose to treat the learning of constraint as an extrinsic reward  as motivated by the
connections between PR with the reinforcement learning domain presented below.

3.2 PR and RL

RL or optimal control has been studied primarily for determining optimal action sequences or
strategies  which is signiÔ¨Åcantly different from the context of PR that aims at regulating generative
models. However  formulations very similar to PR (e.g.  Eqs.1 and 3) have been developed and
widely used  in both the (forward) RL for policy optimization and the inverse RL for reward learning.
To make the mathematical correspondence clearer  we intentionally re-use most of the notations from
PR. Table 1 lists the correspondence. SpeciÔ¨Åcally  consider a stationary Markov decision process
(MDP). An agent in state s draws an action a following the policy pœÄ(a|s). The state subsequently
transfers to s(cid:48) (with some transition probability of the MDP)  and a reward is obtained R(s  a) ‚àà R.
Let x = (s  a) denote the state-action pair  and pœÄ(x) = ¬µœÄ(s)pœÄ(a|s) where ¬µœÄ(s) is the stationary
state distribution [47].

3.2.1 Entropy regularized policy optimization

The goal of policy optimization is to Ô¨Ånd the optimal policy that maximizes the expected reward.
The rich research line of entropy regularized policy optimization has augmented the objective with
information theoretic regularizers such as KL divergence between the new policy and the old policy
for stabilized learning. With a slight abuse of notations  let qœÄ(x) denote the new policy and pœÄ(x)
the old one. A prominent algorithm for example is the relative entropy policy search (REPS) [43]
which follows the objective:

minqœÄ L(qœÄ) = KL(qœÄ(x)(cid:107)pœÄ(x)) ‚àí Œ±EqœÄ [R(x)]  

(6)
where the KL divergence prevents the policy from changing too rapidly. Similar objectives have also
been widely used in other workhorse algorithms such as trust-region policy optimization (TRPO) [45] 
soft Q-learning [17  46]  and others.
We can see the close resemblance between Eq.(6) with the PR objective in Eq.(1)  where the generative
model pŒ∏(x) in PR corresponds to the reference policy pœÄ(x)  while the constraint f (x) corresponds

4

to the reward R(x). The new policy qœÄ can be either a parametric distribution [45] or a non-parametric
distribution [43  1]. For the latter  the optimization of Eq.(6) precisely corresponds to the E-step
of PR  yielding the optimal policy q‚àó
œÄ(x) that takes the same form of q‚àó(x) in Eq.(3)  with pŒ∏ and
f replaced with the respective counterparts pœÄ and R  respectively. The parametric policy pœÄ is
subsequently updated with samples from q‚àó
œÄ  which is exactly equivalent to the M-step in PR (Eq.4).
While the above policy optimization algorithms have assumed a reward function given by the external
environment  just as the pre-deÔ¨Åned constraint function in PR  the strong connections above inspire
us to treat the PR constraint as an extrinsic reward  and utilize the rich tools in RL (especially the
inverse RL) for learning the constraint.

3.2.2 Maximum entropy inverse reinforcement learning

Maximum entropy (MaxEnt) IRL [56] is among the most widely-used methods that induce the reward
function from expert demonstrations x ‚àº pd(x)  where pd is the empirical demonstration (data)
distribution. MaxEnt IRL adopts the same principle as the above entropy regularized RL (Eq.6)
that maximizes the expected reward regularized by the relative entropy (i.e.  the KL)  except that 
in MaxEnt IRL  pœÄ is replaced with a uniform distribution and the regularization reduces to the
entropy of qœÄ. Therefore  same as above  the optimal policy takes the form exp{Œ±R(x)}/Z. MaxEnt
IRL assumes the demonstrations are drawn from the optimal policy. Learning the reward function
RœÜ(x) with unknown parameters œÜ is then cast as maximizing the likelihood of the distribution
qœÜ(x) := exp{Œ±RœÜ(x)}/ZœÜ:

= arg maxœÜ Ex‚àºpd [log qœÜ(x)] .

(7)
Given the direct correspondence between the policy qœÜ‚àó in MaxEnt IRL and the policy optimization
solution q‚àó
œÄ of Eq.(6)  plus the connection between the regularized distribution q‚àó of PR (Eq.3) and q‚àó
as built in sec 3.2.1  we can readily link q‚àó and qœÜ‚àó. This motivates to plug q‚àó in the above maximum
œÄ
likelihood objective to learn the constraint fœÜ(x) which is parallel to the reward function RœÜ(x).
We present the resulting full algorithm in the next section. Table 1 summarizes the correspondence
between PR  entropy regularized policy gradient  and maximum entropy IRL.

‚àó
œÜ

4 Algorithm

We have formally related PR to the RL methods. With the uniÔ¨Åed view of these approaches  we
derive a practical algorithm for arbitrary learnable constraints on any deep generative models. The
algorithm alternates the optimization of the constraint fœÜ and the generative model pŒ∏.

4.1 Learning the Constraint fœÜ

‚àáœÜEx‚àºpd [log q(x)] = ‚àáœÜ

As motivated in section 3.2  instead of directly optimizing fœÜ in the original PR objectives (Eq.5)
which can be problematic  we treat fœÜ as the reward function to be induced with the MaxEnt IRL
framework. That is  we maximize the data likelihood of q(x) (Eq.3) w.r.t œÜ  yielding the gradient:

(cid:2)Ex‚àºpd [Œ±fœÜ(x)] ‚àí log ZœÜ

(cid:3)

(8)
The second term involves estimating the expectation w.r.t an energy-based distribution Eq(x)[¬∑]  which
is in general very challenging. However  we can exploit the special structure of q ‚àù pŒ∏ exp{Œ±fœÜ}
for efÔ¨Åcient approximation. SpeciÔ¨Åcally  we use pŒ∏ as the proposal distribution  and obtain the
importance sampling estimate of the second term as following:

= Ex‚àºpd [Œ±‚àáœÜfœÜ(x)] ‚àí Eq(x) [Œ±‚àáœÜfœÜ(x)] .

(cid:20) q(x)

(cid:21)

Eq(x) [Œ±‚àáœÜfœÜ(x)] = Ex‚àºpŒ∏ (x)

¬∑ Œ±‚àáœÜfœÜ(x)

pŒ∏(x)

= 1/ZœÜ ¬∑ Ex‚àºpŒ∏ (x) [exp{Œ±fœÜ(x)} ¬∑ Œ±‚àáœÜfœÜ(x)] .

Note that the normalization ZœÜ =(cid:82) pŒ∏(x) exp{Œ±fœÜ(x)} can also be estimated efÔ¨Åciently with MC
sampling: ÀÜZœÜ = 1/N(cid:80)

exp{Œ±fœÜ(xi)}  where xi ‚àº pŒ∏. The base generative distribution pŒ∏ is a
natural choice for the proposal as it is in general amenable to efÔ¨Åcient sampling  and is close to q
as forced by the KL divergence in Eq.(1). Our empirical study shows low variance of the learning
process (sec 5). Moreover  using pŒ∏ as the proposal distribution allows pŒ∏ to be an implicit generative
model (as no likelihood evaluation of pŒ∏ is needed). Note that the importance sampling estimation is
consistent yet biased.

(9)

xi

5

4.2 Learning the Generative Model pŒ∏

Given the current parameter state (Œ∏ = Œ∏t  œÜ = œÜt)  and q(x) evaluated at the parameters  we
continue to update the generative model. Recall that optimization of the generative parameter Œ∏ is
performed by minimizing the KL divergence in Eq.(4)  which we replicate here:

minŒ∏ KL(q(x)(cid:107)pŒ∏(x)) = minŒ∏ ‚àíEq(x) [log pŒ∏(x)] + const.

(10)
The expectation w.r.t q(x) can be estimated as above (Eq.9). A drawback of the objective is the
requirement of evaluating the generative density pŒ∏(x)  which is incompatible to the emerging
implicit generative models [40] that only permit simulating samples but not evaluating density.
To address the restriction  when it comes to regularizing implicit models  we propose to instead
minimize the reverse KL divergence:

(cid:20)
minŒ∏ KL (pŒ∏(x)(cid:107)q(x)) = minŒ∏ EpŒ∏
= minŒ∏ ‚àíEpŒ∏

(cid:21)

pŒ∏ ¬∑ ZœÜt

log

pŒ∏t exp{Œ±fœÜt}

(cid:2)Œ±fœÜt (x)(cid:3) + KL(pŒ∏(cid:107)pŒ∏t ) + const.

(11)

(cid:2)Œ±fœÜt (x)(cid:3)|Œ∏=Œ∏t .

By noting that ‚àáŒ∏KL (pŒ∏(cid:107)pŒ∏t)|Œ∏=Œ∏t = 0  we obtain the gradient w.r.t Œ∏:

‚àáŒ∏KL (pŒ∏(x)(cid:107)q(x))|Œ∏=Œ∏t = ‚àí‚àáŒ∏EpŒ∏

(12)
That is  the gradient of minimizing the reversed KL divergence equals the gradient of maximizing
EpŒ∏ [Œ±fœÜt(x)]. Intuitively  the objective encourages the generative model pŒ∏ to generate samples that
the constraint function assigns high scores. Though the objective for implicit model deviates the
original PR framework  reversing KL for computationality was also used previously such as in the
classic wake-sleep method [19]. The resulting algorithm also resembles the adversarial learning in
GANs  as we discuss in the next section. Empirical results on implicit models show the effectiveness
of the objective.
The resulting algorithm is summarized in Alg.1.

Algorithm 1 Joint Learning of Deep Generative Model and Constraints
Input: The base generative model pŒ∏(x)

The (set of) constraints fœÜ(x)

Optimize constraints œÜ with Eq.(8)
if pŒ∏ is an implicit model then

1: Initialize generative parameter Œ∏ and constraint parameter œÜ
2: repeat
3:
4:
5:
6:
7:
8:
9: until convergence
Output: Jointly learned generative model pŒ∏‚àó (x) and constraints fœÜ‚àó (x)

Optimize model Œ∏ with Eq.(12) along with minimizing original model objective L(Œ∏)
Optimize model Œ∏ with Eq.(10) along with minimizing L(Œ∏)

end if

else

Connections to adversarial learning For implicit generative models  the two objectives w.r.t œÜ
and Œ∏ (Eq.8 and Eq.12) are conceptually similar to the adversarial learning in GANs [15] and the
variants such as energy-based GANs [26  55  54  50]. SpeciÔ¨Åcally  the constraint fœÜ(x) can be seen
as being optimized to assign lower energy (with the energy-based distribution q(x)) to real examples
from pd(x)  and higher energy to fake samples from q(x) which is the regularized model of the
generator pŒ∏(x). In contrast  the generator pŒ∏(x) is optimized to generate samples that confuse fœÜ
and obtain lower energy. Such adversarial relation links the PR constraint fœÜ(x) to the discriminator
in GANs (Table 1). Note that here fake samples are generated from q(x) and pŒ∏(x) in the two
learning phases  respectively  which differs from previous adversarial methods for energy-based
model estimation that simulate only from a generator. Besides  distinct from the discriminator-centric
view of the previous work [26  54  50]  we primarily aim at improving the generative model by
incorporating learned constraints. Last but not the least  as discussed in sec 3.1  the proposed
framework and algorithm are more generally and efÔ¨Åciently applicable to not only implicit generative
models as in GANs  but also (non-)reparameterizable explicit generative models.

6

5 Experiments

We demonstrate the applications and effectiveness of the algorithm in two tasks related to image and
text generation [24]  respectively.

Method

Pumarola et al. [44]

1 Ma et al. [38]
2
3 Ma et al. [37]
4 Base model
5 With Ô¨Åxed constraint
6 With learned constraint

SSIM Human
0.614 ‚Äî
0.747 ‚Äî
0.762 ‚Äî
0.676
0.679
0.727

0.03
0.12
0.77

Table 2: Results of image generation on Structural
Similarity (SSIM) [52] between generated and true
images  and human survey where the full model
yields better generations than the base models (Rows
5-6) on 77% test cases. See the text for more results
and discussion.

Figure 2: Training losses of the three mod-
els. The model with learned constraint con-
verges smoothly as base models.

Figure 3: Samples generated by the models in Table 2. The model with learned human part constraint
generates correct poses and preserves human body structure much better.

5.1 Pose Conditional Person Image Generation

Given a person image and a new body pose  the goal is to generate an image of the same person under
the new pose (Figure 1  left). The task is challenging due to body self-occlusions and many cloth
and shape ambiguities. Complete end-to-end generative networks have previously failed [37] and
existing work designed specialized generative processes or network architectures [37  44  38]. We
show that with an added body part consistency constraint  a plain end-to-end generative model can
also be trained to produce highly competitive results  signiÔ¨Åcantly improving over base models that
do not incorporate the problem structure.
Setup. We follow the previous work [37] and obtain from DeepFashion [35] a set of triples (source
image  pose keypoints  target image) as supervision data. The base generative model pœÜ is an implicit
model that transforms the input source and pose directly to the pixels of generated image (and
hence deÔ¨Ånes a Dirac-delta distribution). We use the residual block architecture [51] widely-used in
image generation for the generative model. The base model is trained to minimize the L1 distance
loss between the real and generated pixel values  as well as to confuse a binary discriminator that
distinguishes between the generation and the true target image.
Knowledge constraint. Neither the pixel-wise distance nor the binary discriminator loss encode
any task structures. We introduce a structured consistency constraint fœÜ that encourages each of the
body parts (e.g.  head  legs) of the generated image to match the respective part of the true image.
SpeciÔ¨Åcally  the constraint fœÜ includes a human parsing module that classiÔ¨Åes each pixel of a person
image into possible body parts. The constraint then evaluates cross entropies of the per-pixel part

7

040080012001600Iterations6.12.18.LossBase modelWith fixed constraintWith learned constraintsource	imagetarget	posetarget	imageLearned	constraintBase	modelFixed	constraintModel

1 Base model
2 With binary D
3 With constraint updated

in M-step (Eq.5)

Perplexity Human
30.30
30.01
31.27

0.19
0.20
0.15

4 With learned constraint
Table 3: Sentence generation results on test set per-
plexity and human survey. Samples by the full model
are considered as of higher quality in 24% cases.

28.69

0.24

acting
acting is the acting .
acting is also very good .

the
the

out of 10 .
10 out of 10 .
I will give the movie 7 out of 10 .

Table 4: Two test examples  including the
template  the sample by the base model  and
the sample by the constrained model.

distributions between the generated and true images. The average negative cross entropy serves as
the constraint score. The parsing module is parameterized as a neural network with parameters œÜ 
pre-trained on an external parsing dataset [14]  and subsequently adapted within our algorithm jointly
with the generative model.
Results. Table 2 compares the full model (with the learned constraint  Row 6) with the base model
(Row 4) and the one regularized with the constraint that is Ô¨Åxed after pre-training (Row 5). Human
survey is performed by asking annotators to rank the quality of images generated by the three models
on each of 200 test cases  and the percentages of ranked as the best are reported (Tied ranking is
treated as negative result). We can see great improvement by the proposed algorithm. The model
with Ô¨Åxed constraint fails  partially because pre-training on external data does not necessarily Ô¨Åt to
the current problem domain. This highlights the necessity of the constraint learning. Figure 3 shows
examples further validating the effectiveness of the algorithm.
In sec 4  we have discussed the close connection between the proposed algorithm and (energy-based)
GANs. The conventional discriminator in GANs can be seen as a special type of constraint. With this
connection and given that the generator in the task is an implicit generative model  here we can also
apply and learn the structured consistency constraint using GANs  which is equivalent to replacing
q(x) in Eq.(8) with pŒ∏(x). Such a variant produces a SSIM score of 0.716  slightly inferior to the
result of the full algorithm (Row 6). We suspect this is because fake samples by q (instead of p) can
help with better constraint learning. It would be interesting to explore this in more applications.
To give a sense of the state of the task  Table 2 also lists the performance of previous work. It is worth
noting that these results are not directly comparable  as discussed in [44]  due to different settings
(e.g.  the test splits) between each of them. We follow [37  38] mostly  while our generative model is
much simpler than these work with specialized  multi-stage architectures. The proposed algorithm
learns constraints with moderate approximations. Figure 2 validates that the training is stable and
converges smoothly as the base models.

5.2 Template Guided Sentence Generation

The task is to generate a text sentence x that follows a given template t (Figure 1  right). Each missing
part in the template can contain arbitrary number of words. This differs from previous sentence
completion tasks [9  57] which designate each masked position to have a single word. Thus directly
applying these approaches to the task can be problematic.
Setup. We use an attentional sequence-to-sequence (seq2seq) [3] model pŒ∏(x|t) as the base
generative model for the task. Paired (template  sentence) data is obtained by randomly masking out
different parts of sentences from the IMDB corpus [8]. The base model is trained in an end-to-end
supervised manner  which allows it to memorize the words in the input template and repeat them
almost precisely in the generation. However  the main challenge is to generate meaningful and
coherent content to Ô¨Åll in the missing parts.
Knowledge constraint. To tackle the issue  we add a constraint that enforces matching between
the generated sentence and the ground-truth text in the missing parts. SpeciÔ¨Åcally  let t‚àí be the
masked-out true text. That is  plugging t‚àí into the template t recovers the true complete sentence.
The constraint is deÔ¨Åned as fœÜ(x  t‚àí) which returns a high score if the sentence x matches t‚àí well.
The actual implementation of the matching strategy can vary. Here we simply specify fœÜ as another
seq2seq network that takes as input a sentence x and evaluates the likelihood of recovering t‚àí‚ÄîThis

8

is all we have to specify  while the unknown parameters œÜ are learned jointly with the generative
model. Despite the simplicity  the empirical results show the usefulness of the constraint.
Results. Table 3 shows the results. Row 2 is the base model with an additional binary discriminator
that adversarial distinguishes between the generated sentence and the ground truth (i.e.  a GAN
model). Row 3 is the base model with the constraint learned in the direct way through Eq.(5). We see
that the improper learning method for the constraint harms the model performance  partially because
of the relatively low-quality model samples the constraint is trained to Ô¨Åt. In contrast  the proposed
algorithm effectively improves the model results. Its superiority over the binary discriminator (Row 2)
shows the usefulness of incorporating problem structures. Table 4 demonstrates samples by the base
and constrained models. Without the explicit constraint forcing in-Ô¨Ålling content matching  the base
model tends to generate less meaningful content (e.g.  duplications  short and general expressions).

6 Discussions: Combining Structured Knowledge with Black-box NNs

We revealed the connections between posterior regularization and reinforcement learning  which
motivates to learn the knowledge constraints in PR as reward learning in RL. The resulting algorithm
is generally applicable to any deep generative models  and Ô¨Çexible to learn the constraints and model
jointly. Experiments on image and text generation showed the effectiveness of the algorithm.
The proposed algorithm  along with the previous work (e.g.  [21  22  18  36  23])  represents a general
means of adding (structured) knowledge to black-box neural networks by devising knowledge-inspired
losses/constraints that drive the model to learn the desired structures. This differs from the other
popular way that embeds domain knowledge into speciÔ¨Åcally-designed neural architectures (e.g. 
the knowledge of translation-invariance in image classiÔ¨Åcation is hard-coded in the conv-pooling
architecture of ConvNet). While the specialized neural architectures can usually be very effective
to capture the designated knowledge  incorporating knowledge via specialized losses enjoys the
advantage of generality and Ô¨Çexibility:

e.g.  ConvNets  RNNs  and other specialized ones [21].

‚Ä¢ Model-agnostic. The learning framework is applicable to neural models with any architectures 
‚Ä¢ Richer supervisions. Compared to the conventional end-to-end maximum likelihood learning
that usually requires fully-annotated or paired data  the knowledge-aware losses provide ad-
ditional supervisions based on  e.g.  structured rules [21]  other models [18  22  53  20]  and
datasets for other related tasks (e.g.  the human image generation method in Figure 1  and [23]).
In particular  [23] leverages datasets of sentence sentiment and phrase tense to learn to control
the both attributes (sentiment and tense) when generating sentences.
‚Ä¢ Modularized design and learning. With the rich sources of supervisions  design and learning
of the model can still be simple and efÔ¨Åcient  because each of the supervision sources can be
formulated independently to each other and each forms a separate loss term. For example  [23]
separately learns two classiÔ¨Åers  one for sentiment and the other for tense  on two separate
datasets  respectively. The two classiÔ¨Åers carry respective semantic knowledge  and are then
jointly applied to a text generation model for attribute control. In comparison  mixing and
hard-coding multiple knowledge in a single neural architecture can be difÔ¨Åcult and quickly
becoming impossible when the number of knowledge increases.
‚Ä¢ Generation with discrimination knowledge. In generation tasks  it can sometimes be difÔ¨Åcult
to incorporate knowledge directly in the generative process (or model architecture)  i.e.  deÔ¨Åning
how to generate. In contrast  it is often easier to instead specify a evaluation metric that measures
the quality of a given sample in terms of the knowledge  i.e.  deÔ¨Åning what desired generation is.
For example  in the human image generation task (Figure 1)  evaluating the structured human
part consistency could be easier than designing a generator architecture that hard-codes the
structured generation process for the human parts.

It is worth noting that the two paradigms are not mutually exclusive. A model with knowledge-inspired
specialized architecture can still be learned by optimizing knowledge-inspired losses. Different types
of knowledge can be best Ô¨Åt for either architecture hard-coding or loss optimization. It would be
interesting to explore the combination of both in the above tasks and others.

9

Acknowledgment This material is based upon work supported by the National Science Foundation
grant IIS1563887. Any opinions  Ô¨Åndings and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily reÔ¨Çect the views of the National Science
Foundation.

References
[1] A. Abdolmaleki  J. T. Springenberg  Y. Tassa  R. Munos  N. Heess  and M. Riedmiller. Maximum a

posteriori policy optimisation. In ICLR  2018.

[2] J. Andreas  M. Rohrbach  T. Darrell  and D. Klein. Learning to compose neural networks for question

answering. arXiv preprint arXiv:1601.01705  2016.

[3] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align and translate.

arXiv preprint arXiv:1409.0473  2014.

[4] K. Bellare  G. Druck  and A. McCallum. Alternating projections for learning with expectation constraints.

In UAI  pages 43‚Äì50. AUAI Press  2009.

[5] X. Chen  Y. Duan  R. Houthooft  J. Schulman  I. Sutskever  and P. Abbeel. InfoGAN: Interpretable

representation learning by information maximizing generative adversarial nets. In NeurIPS  2016.

[6] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computa-

tion  9(2):271‚Äì278  1997.

[7] M. P. Deisenroth  G. Neumann  J. Peters  et al. A survey on policy search for robotics. Foundations and

Trends R(cid:13) in Robotics  2(1‚Äì2):1‚Äì142  2013.

[8] Q. Diao  M. Qiu  C.-Y. Wu  A. J. Smola  J. Jiang  and C. Wang. Jointly modeling aspects  ratings and

sentiments for movie recommendation (JMARS). In KDD  pages 193‚Äì202. ACM  2014.

[9] W. Fedus  I. Goodfellow  and A. M. Dai. MaskGAN: Better text generation via Ô¨Ålling in the _. arXiv

preprint arXiv:1801.07736  2018.

[10] C. Finn  P. Christiano  P. Abbeel  and S. Levine. A connection between generative adversarial networks 

inverse reinforcement learning  and energy-based models. arXiv preprint arXiv:1611.03852  2016.

[11] C. Finn  S. Levine  and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy

optimization. In ICML  pages 49‚Äì58  2016.

[12] J. Fu  K. Luo  and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning.

arXiv preprint arXiv:1710.11248  2017.

[13] K. Ganchev  J. Gillenwater  B. Taskar  et al. Posterior regularization for structured latent variable models.

JMLR  11(Jul):2001‚Äì2049  2010.

[14] K. Gong  X. Liang  X. Shen  and L. Lin. Look into person: Self-supervised structure-sensitive learning

and a new benchmark for human parsing. In CVPR  pages 6757‚Äì6765  2017.

[15] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NeurIPS  pages 2672‚Äì2680  2014.

[16] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press  2016.

deeplearningbook.org.

http://www.

[17] T. Haarnoja  H. Tang  P. Abbeel  and S. Levine. Reinforcement learning with deep energy-based policies.

arXiv preprint arXiv:1702.08165  2017.

[18] G. Hinton  O. Vinyals  and J. Dean. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531  2015.

[19] G. E. Hinton  P. Dayan  B. J. Frey  and R. M. Neal. The ‚Äúwake-sleep‚Äù algorithm for unsupervised neural

networks. Science  268(5214):1158  1995.

[20] A. Holtzman  J. Buys  M. Forbes  A. Bosselut  D. Golub  and Y. Choi. Learning to write with cooperative

discriminators. In ACL  2018.

[21] Z. Hu  X. Ma  Z. Liu  E. Hovy  and E. Xing. Harnessing deep neural networks with logic rules. In ACL 

2016.

10

[22] Z. Hu  Z. Yang  R. Salakhutdinov  and E. P. Xing. Deep neural networks with massive learned knowledge.

In EMNLP  2016.

[23] Z. Hu  Z. Yang  X. Liang  R. Salakhutdinov  and E. P. Xing. Toward controlled generation of text. In

ICML  2017.

[24] Z. Hu  H. Shi  Z. Yang  B. Tan  T. Zhao  J. He  W. Wang  L. Qin  D. Wang  et al. Texar: A modularized 

versatile  and extensible toolkit for text generation. arXiv preprint arXiv:1809.00794  2018.

[25] Z. Hu  Z. Yang  R. Salakhutdinov  and E. P. Xing. On unifying deep generative models. In ICLR  2018.

[26] T. Kim and Y. Bengio. Deep directed generative models with energy-based probability estimation. arXiv

preprint arXiv:1606.03439  2016.

[27] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114  2013.

[28] M. J. Kusner  B. Paige  and J. M. Hern√°ndez-Lobato. Grammar variational autoencoder. arXiv preprint

arXiv:1703.01925  2017.

[29] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS  2011.

[30] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv

preprint arXiv:1805.00909  2018.

[31] C. Li  J. Zhu  T. Shi  and B. Zhang. Max-margin deep generative models. In NeurIPS  pages 1837‚Äì1845 

2015.

[32] P. Liang  M. I. Jordan  and D. Klein. Learning from measurements in exponential families. In ICML  pages

641‚Äì648. ACM  2009.

[33] X. Liang  Z. Hu  H. Zhang  C. Gan  and E. P. Xing. Recurrent topic-transition GAN for visual paragraph

generation. In ICCV  2017.

[34] X. Liang  Z. Hu  and E. Xing. Symbolic graph reasoning meets convolutions. In NeurIPS  2018.

[35] Z. Liu  P. Luo  S. Qiu  X. Wang  and X. Tang. Deepfashion: Powering robust clothes recognition and

retrieval with rich annotations. In CVPR  pages 1096‚Äì1104  2016.

[36] D. Lopez-Paz  L. Bottou  B. Sch√∂lkopf  and V. Vapnik. Unifying distillation and privileged information.

arXiv preprint arXiv:1511.03643  2015.

[37] L. Ma  X. Jia  Q. Sun  B. Schiele  T. Tuytelaars  and L. Van Gool. Pose guided person image generation.

In NeurIPS  pages 405‚Äì415  2017.

[38] L. Ma  Q. Sun  S. Georgoulis  L. Van Gool  B. Schiele  and M. Fritz. Disentangled person image generation.

In CVPR  2018.

[39] S. Mei  J. Zhu  and J. Zhu. Robust regBayes: Selectively incorporating Ô¨Årst-order logic domain knowledge

into bayesian models. In ICML  pages 253‚Äì261  2014.

[40] S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint

arXiv:1610.03483  2016.

[41] G. Neumann et al. Variational inference for policy search in changing situations. In ICML  pages 817‚Äì824 

2011.

[42] A. v. d. Oord  N. Kalchbrenner  and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint

arXiv:1601.06759  2016.

[43] J. Peters  K. M√ºlling  and Y. Altun. Relative entropy policy search. In AAAI  pages 1607‚Äì1612. Atlanta 

2010.

[44] A. Pumarola  A. Agudo  A. Sanfeliu  and F. Moreno-Noguer. Unsupervised person image synthesis in

arbitrary poses. In CVPR  2018.

[45] J. Schulman  S. Levine  P. Abbeel  M. Jordan  and P. Moritz. Trust region policy optimization. In ICML 

pages 1889‚Äì1897  2015.

[46] J. Schulman  X. Chen  and P. Abbeel. Equivalence between policy gradients and soft Q-learning. arXiv

preprint arXiv:1704.06440  2017.

11

[47] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction  volume 1. MIT press Cambridge 

1998.

[48] B. Tan  Z. Hu  Z. Yang  R. Salakhutdinov  and E. Xing. Connecting the dots between MLE and RL for text

generation. 2018.

[49] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In NeurIPS  pages 25‚Äì32  2004.

[50] D. Wang and Q. Liu. Learning to draw samples: With application to amortized MLE for generative

adversarial learning. arXiv preprint arXiv:1611.01722  2016.

[51] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  A. Tao  J. Kautz  and B. Catanzaro. High-resolution image synthesis and

semantic manipulation with conditional GANs. arXiv preprint arXiv:1711.11585  2017.

[52] Z. Wang  A. C. Bovik  H. R. Sheikh  and E. P. Simoncelli. Image quality assessment: from error visibility

to structural similarity. IEEE transactions on image processing  13(4):600‚Äì612  2004.

[53] Z. Yang  Z. Hu  C. Dyer  E. Xing  and T. Berg-Kirkpatrick. Unsupervised text style transfer using language

models as discriminators. In NeurIPS  2018.

[54] S. Zhai  Y. Cheng  R. Feris  and Z. Zhang. Generative adversarial networks as variational training of energy

based models. arXiv preprint arXiv:1611.01799  2016.

[55] J. Zhao  M. Mathieu  and Y. LeCun. Energy-based generative adversarial network. arXiv preprint

arXiv:1609.03126  2016.

[56] B. D. Ziebart  A. L. Maas  J. A. Bagnell  and A. K. Dey. Maximum entropy inverse reinforcement learning.

In AAAI  volume 8  pages 1433‚Äì1438. Chicago  IL  USA  2008.

[57] G. Zweig and C. J. Burges. The Microsoft Research sentence completion challenge. Technical report 

Citeseer  2011.

12

,Zhiting Hu
Russ Salakhutdinov
Xiaodan Liang
Haoye Dong
Eric Xing