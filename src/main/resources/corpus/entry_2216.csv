2013,Gaussian Process Conditional Copulas with Applications to Financial Time Series,The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be innacurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this  a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.,Gaussian Process Conditional Copulas with

Applications to Financial Time Series

Jos´e Miguel Hern´andez-Lobato

Engineering Department
University of Cambridge
jmh233@cam.ac.uk

James Robert Lloyd
Engineering Department
University of Cambridge
jrl44@cam.ac.uk

Daniel Hern´andez-Lobato
Computer Science Department

Universidad Aut´onoma de Madrid
daniel.hernandez@uam.es

Abstract

The estimation of dependencies between multiple variables is a central problem
in the analysis of ﬁnancial time series. A common approach is to express these
dependencies in terms of a copula function. Typically the copula function is as-
sumed to be constant but this may be inaccurate when there are covariates that
could have a large inﬂuence on the dependence structure of the data. To account
for this  a Bayesian framework for the estimation of conditional copulas is pro-
posed. In this framework the parameters of a copula are non-linearly related to
some arbitrary conditioning variables. We evaluate the ability of our method to
predict time-varying dependencies on several equities and currencies and observe
consistent performance gains compared to static copula models and other time-
varying copula methods.

1

Introduction

Understanding dependencies within multivariate data is a central problem in the analysis of ﬁnancial
time series  underpinning common tasks such as portfolio construction and calculation of value-at-
risk. Classical methods estimate these dependencies in terms of a covariance matrix (possibly time
varying) which is induced from the data [4  5  7  1]. However  a more general approach is to use
copula functions to model dependencies [6]. Copulas have become popular since they separate
the estimation of marginal distributions from the estimation of the dependence structure  which is
completely determined by the copula.
The use of standard copula methods to estimate dependencies is likely to be inaccurate when the
actual dependencies are strongly inﬂuenced by other covariates. For example  dependencies can
vary with time or be affected by observations of other time series. Standard copula methods can-
not handle such conditional dependencies. To address this limitation  we propose a probabilistic
framework to estimate conditional copulas. Speciﬁcally we assume parametric copulas whose pa-
rameters are speciﬁed by unknown non-linear functions of arbitrary conditioning variables. These
latent functions are approximated using Gaussian processes (GP) [17].
GPs have previously been used to model conditional copulas in [12] but that work only applies
to copulas speciﬁed by a single parameter. We extend this work to accommodate copulas with
multiple parameters. This is an important improvement since it allows the use of a richer set of
copulas including Student’s t and asymmetric copulas. We demonstrate our method by choosing the
conditioning variables to be time and evaluating its ability to estimate time-varying dependencies

1

Figure 1: Left  Gaussian copula density for τ = 0.3. Middle  Student’s t copula density for τ = 0.3
and ν = 1. Right  symmetrized Joe Clayton copula density for τ U = 0.1 and τ L = 0.6. The latter
copula model is asymmetric along the main diagonal of the unit square.

on several currency and equity time series. Our method achieves consistently superior predictive
performance compared to static copula models and other dynamic copula methods. These include
models that allow their parameters to change with time  e.g. regime switching models [11] and
methods proposing GARCH-style updates to copula parameters [20  11].

2 Copulas and Conditional Copulas

Copulas provide a powerful framework for the construction of multivariate probabilistic models by
separating the modeling of univariate marginal distributions from the modeling of dependencies
between variables [6]. We focus on bivariate copulas since higher dimensional copulas are typically
constructed using bivariate copulas as building blocks [e.g 2  12].
Sklar’s theorem [18] states that given two one-dimensional random variables  X and Y   with con-
tinuous marginal cumulative distribution functions (cdfs) FX (X) and FY (Y )  we can express their
joint cdf FX Y as FX Y (x  y) = CX Y [FX (x)  FY (y)]  where CX Y is the unique copula for X and
Y . Since FX (X) and FY (Y ) are marginally uniformly distributed on [0  1]  CX Y is the cdf of a
probability distribution on the unit square [0  1] × [0  1] with uniform marginals. Figure 1 shows
plots of the copula densities for three parametric copula models: Gaussian  Student’s t and the sym-
metrized Joe Clayton (SJC) copulas. Copula models can be learnt in a two step process [10]. First 
the marginals FX and FY are learnt by ﬁtting univariate models. Second  the data are mapped to
the unit square by U = FX (X)  V = FY (Y ) (i.e. a probability integral transform) and then CX Y
is then ﬁt to the transformed data.

2.1 Conditional Copulas

FX Y |Z(x  y|z) = CX Y |Z

(cid:2)FX|Z(x|z)  FY |Z(y|z)|z(cid:3) .

When one has access to a covariate vector Z  one may wish to estimate a conditional version of a
copula model i.e.

(1)
Here  the same two-step estimation process can be used to estimate FX Y |Z(x  y|z). The estimation
of the marginals FX|Z and FY |Z can be implemented using standard methods for univariate con-
ditional distribution estimation. However  the estimation of CX Y |Z is constrained to have uniform
marginal distributions; this is a problem that has only been considered recently [12]. We propose a
general Bayesian non-parametric framework for the estimation of conditional copulas based on GPs
and an alternating expectation propagation (EP) algorithm for efﬁcient approximate inference.

3 Gaussian Process Conditional Copulas
Let DZ = {zi}n
i=1 where (ui  vi) is a sample drawn from CX Y |zi.
We assume that CX Y |Z is a parametric copula model Cpar[u  v|θ1(z)  . . .   θk(z)] speciﬁed by k
parameters θ1  . . .   θk that may be functions of the conditioning variable z. Let θi(z) = σi[fi(z)] 

i=1 and DU V = {(ui  vi)}n

2

Gaussian Copula0.20.40.60.80.20.40.60.8Student's t Copula0.20.40.60.80.20.40.60.8Symmetrized Joe Clayton Copula0.20.40.60.80.20.40.60.8where fi is an arbitrary real function and σi is a function that maps the real line to a set Θi of valid
conﬁgurations for θi. For example  Cpar could be a Student’s t copula. In this case  k = 2 and θ1
and θ2 are the correlation and the degrees of freedom in the Student’s t copula  Θ1 = (−1  1) and
Θ2 = (0 ∞). One could then choose σ1(·) = 2Φ(·) − 1  where Φ is the standard Gaussian cdf and
σ2(·) = exp(·) to satisfy the constraint sets Θ1 and Θ2 respectively.
Once we have speciﬁed the parametric form of Cpar and the mapping functions σ1  . . .   σk  we need
to learn the latent functions f1  . . .   fk. We perform a Bayesian non-parametric analysis by placing
GP priors on these functions and computing their posterior distribution given the observed data.
Let fi = (fi(z1)  . . .   fi(zn))T. The prior distribution for fi given DZ is p(fi|DZ) = N (fi|mi  Ki) 
where mi = (mi(z1)  . . .   mi(zn))T for some mean function mi(z) and Ki is an n × n covariance
matrix generated by the squared exponential covariance function  i.e.

[Ki]jk = Cov[fi(zj)  fi(zk)] = βi exp(cid:8)−(zj − zk)Tdiag(λi)(zj − zk)(cid:9) + γi  

(2)

where λi is a vector of inverse length-scales and βi  γi are amplitude and noise parameters. The
posterior distribution for f1  . . .   fk given DU V and DZ is

(cid:104)(cid:81)n

i=1 cpar

(cid:2)ui  vi|σ1 [f1(zi)]   . . .   σk [fk(zi)](cid:3)(cid:105)(cid:104)(cid:81)k

p(f1  . . .   fk|DU V  DZ) =
(3)
where cpar is the density of the parametric copula model and p(DU V |DZ) is a normalization con-
stant often called the model evidence. Given a particular value of Z denoted by z(cid:63)  we can make
predictions about the conditional distribution of U and V using the standard GP prediction formula

p(DU V |DZ)

 

(cid:105)

i=1 N (fi|mi  Ki)

p(u(cid:63)  v(cid:63)|z(cid:63)) =

k ])p(f (cid:63)|f1  . . .   fk  z(cid:63) Dz)

(cid:90)
k )T  p(f (cid:63)|f1  . . .   fk  z(cid:63) Dz) = (cid:81)k

cpar(u(cid:63)  v(cid:63)|σ1[f (cid:63)
p(f1  . . .   fk|DU V  DZ) df1 ··· dfk df (cid:63)  

1 ]  . . .   σk[f (cid:63)

1   . . .   f (cid:63)
i |fi  z(cid:63) Dz) = N (f (cid:63)

where f (cid:63) = (f (cid:63)
i = fi(z(cid:63)) 
i ki)  ki = Cov[fi(z(cid:63))  fi(z(cid:63))]
p(f (cid:63)
and ki = (Cov[fi(z(cid:63))  fi(z1)]  . . .   Cov[fi(z(cid:63))  fi(zn)])T. Unfortunately  (3) and (4) cannot be
computed analytically  so we approximate them using expectation propagation (EP) [13].

(fi − mi)  ki − kT

i |mi(z(cid:63)) + kT

i=1 p(f (cid:63)
i K−1

i K−1

i

i |fi  z(cid:63) Dz)  f (cid:63)

3.1 An Alternating EP Algorithm for Approximate Bayesian Inference
The joint distribution for f1  . . .   fk and DU V given DZ can be written as a product of n + k factors:

(4)

p(f1  . . .   fk DU V |DZ) =

gi(f1i  . . .   fki  )

hi(fi)

 

(5)

(cid:34) n(cid:89)

i=1

(cid:35)

(cid:35)(cid:34) k(cid:89)

i=1

j=1 exp(cid:8)−(fji − ˜mji)2/[2˜vji](cid:9)  where si > 0  ˜mji and ˜vji are param-
(cid:81)k

where fji = fj(zi)  hi(fi) = N (fi|mi  Ki) and gi(f1i  . . .   fki) = cpar[ui  vi|σ1[f1i]  . . .   σk[fki]].
EP approximates each factor gi with an approximate Gaussian factor ˜gi that may not integrate to one 
i.e. ˜gi(f1i  . . .   fki) = si
eters to be calculated by EP. The other factors hi already have a Gaussian form so they do not need
to be approximated. Since all the ˜gi and hi are Gaussian  their product is  up to a normalization con-
stant  a multivariate Gaussian distribution q(f1  . . .   fk) which approximates the exact posterior (3)
and factorizes across f1  . . .   fk. The predictive distribution (4) is approximated by ﬁrst integrating
p(f (cid:63)|f1  . . .   fk  z(cid:63) Dz) with respect to q(f1  . . .   fk). This results in a factorized Gaussian distribu-
tion q(cid:63)(f (cid:63)) which approximates p(f (cid:63)|DU V  DZ). Finally  (4) is approximated by Monte-Carlo by
sampling from q(cid:63) and then averaging cpar(u(cid:63)  v(cid:63)|σ1[f (cid:63)
EP iteratively updates each ˜gi until convergence by ﬁrst computing q\i ∝ q/˜gi and then minimizing
the Kullback-Leibler divergence [3] between giq\i and ˜giq\i. This involves updating ˜gi so that the
ﬁrst and second marginal moments of giq\i and ˜giq\i match. However  it is not possible to compute
the moments of giq\i analytically due to the complicated form of gi. A solution is to use numerical
methods to compute these k-dimensional integrals. However  this typically has an exponentially
large computational cost in k which is prohibitive for k > 1. Instead we perform an additional
approximation when computing the marginal moments of fji with respect to giq\i. Without loss of

k ]) over the samples.

1 ]  . . .   σk[f (cid:63)

3

generality  assume that we want to compute the expectation of f1i with respect to giq\i. We make
the following approximation:

(cid:90)

f1igi(f1i  . . .   fki)q

\i(f1i  . . .  fki) df1i  . . .   dfki ≈

(cid:90)

C ×

f1igi(f1i  ¯f2i  . . .   ¯fki)q

\i(f1i  ¯f2i  . . .   ¯fki) df1i  

(6)

where ¯f1i  . . .   ¯fki are the means of f1i  . . .   fki with respect to q\i  and C is a constant that approx-
imates the width of the integrand around its maximum in all dimensions except f1i. In practice all
moments are normalized by the 0-th moment so C can be ignored. The right hand side of (6) is a one-
dimensional integral that can be easily computed using numerical techniques. The approximation
above is similar to approximating an integral by the product of the maximum value of the integrand
and an estimate of its width. However  instead of maximizing gi(f1i  . . .   fki)q\i(f1i  . . .   fki) with
respect to f2i  . . .   fki  we are maximizing q\i. This is a much easier task because q\i is Gaussian
and its maximizer is its own mean vector. In practice  gi(f1i  . . .   fki) is very ﬂat when compared to
q\i and the maximizer of q\i approximates well the maximizer of gi(f1i  . . .   fki)q\i(f1i  . . .   fki).
Since q factorizes across f1  . . .   fk (as well as q\i)  our implementation of EP decouples into k EP
sub-routines among which we alternate; the j-th sub-routine approximates the posterior distribution
of fj using as input the means of q\i generated by the other EP sub-routines. Each sub-routine ﬁnds
a Gaussian approximation to a set of n one-dimensional factors; one factor per data point. In the
j-th EP sub-routine  the i-th factor is given by gi(f1i  . . .   fki)  where each {f1i  . . .   fki} \ {fji}
is kept ﬁxed to the current mean of q\i  as estimated by the other EP sub-routines. We iteratively
alternate between sub-routines  running each one until convergence before re-running the next one.
Convergence is achieved very quickly; we only run each EP sub-routine four times.
The EP sub-routines are implemented using the parallel EP update scheme described in [21]. To
speed up GP related computations  we use the generalized FITC approximation [19  14]: Each
n × n covariance matrix Ki is approximated by K(cid:48)
i = Qi + diag(Ki − Qi)  where Qi =
n0n0 is the n0 × n0 covariance matrix generated by evaluating (2) at
]T  Ki
Ki
n0 (cid:28) n pseudo-inputs  and Ki
nn0 is the n× n0 matrix with the covariances between training points
0). Each time we call the j-th EP subroutine  we opti-
and pseudo-inputs. The cost of EP is O(knn2
mize the corresponding kernel hyper-parameters λj  βj and γj and the pseudo-inputs by maximizing
the EP approximation of the model evidence [17].

]−1[Ki

[Ki

nn0

n0n0

nn0

4 Related Work

The model proposed here is an extension of the conditional copula model of [12]. In the case of
bivariate data and a copula based on one parameter the models are identical. We have extended the
approximate inference for this model to accommodate copulas with multiple parameters; previously
computationally infeasible due to requiring the numerical calculation of multidimensional integrals
within an inner loop of EP inference. We have also demonstrated that one can use this model to
produce excellent predictive results on ﬁnancial time series by conditioning the copula on time.

4.1 Dynamic Copula Models

In [11] a dynamic copula model is proposed based on a two-state hidden Markov model (HMM)
(St ∈ {0  1}) that assumes that the data generating process changes between two regimes of
low/high correlation. At any time t the copula density is Student’s t with different parameters for
the two values of the hidden state St. Maximum likelihood estimation of the copula parameters and
transition probabilities is performed using an EM algorithm [e.g. 3].
A time-varying correlation (TVC) model based on the Student’s t copula is described in [20  11].
The correlation parameter1of a Student’s t copula is assumed to satisfy ρt = (1 − α − β)ρ +
αεt−1 + βρt−1  where εt−1 is the empirical correlation of the previous 10 observations and ρ  α
and β satisfy −1 ≤ ρ ≤ 1  0 ≤ α  β ≤ 1 and α + β ≤ 1. The number of degrees of freedom ν

4

is assumed to be constant. The previous formula is the GARCH equation for correlation instead of
variance. Estimation of ρ  α  β and ν is easily performed by maximum likelihood.
In [15] a dynamic copula based on the SJC copula (DSJCC) is introduced.
parameters τ U and τ L of an SJC copula are assumed to depend on time according to

In this method  the

τ U (t) = 0.01 + 0.98Λ(cid:2)ωU + αU εt−1 + βU τ U (t − 1)(cid:3)  
τ L(t) = 0.01 + 0.98Λ(cid:2)ωL + αLεt−1 + βLτ L(t − 1)(cid:3)  

(7)
(cid:80)10
(8)
where Λ[·] is the logistic function  εt−1 = 1
j=1 |ut−j − vt−j|  (ut  vt) is a copula sample at
time t and the constants are used to avoid numerical instabilities. These formulae are the GARCH
equation for correlations  with an additional logistic function to constrain parameter values. The
estimation of ωU   αU   βU   ωL  αL and βL is performed by maximum likelihood.
We go beyond this prior work by allowing copula parameters to depend on an arbitrary conditioning
variables rather than time alone. Also  the models above either assume Markov independence or
GARCH-like updates to copula parameters. These assumptions have been empirically proven to
be effective for the estimation of univariate variances  but the consistent performance gains of our
proposed method suggest these assumptions are less applicable for the estimation of dependencies.

10

4.2 Other Dynamic Covariance Models

A direct extension of the GARCH equations to multiple time series  VEC  was proposed by [5].
Let x(t) be a multivariate time series assumed to satisfy x(t) ∼ N (0  Σ(t)). VEC(p  q) models the
dynamics of Σ(t) by an equation of the form

vech(Σ(t)) = c +

Ak vech(x(t − k)x(t − k)T) +

Bk vech(Σ(t − k))

(9)

k=1

k=1

where vech is the operation that stacks the lower triangular part on a matrix into a column vector.
The VEC model has a very large number of parameters and hence a more commonly used model is
the BEKK(p  q) model [7] which assumes the following dynamics

p(cid:88)

p(cid:88)

q(cid:88)

q(cid:88)

ν(cid:88)

Σ(t) = CTC +

kx(t − k)x(t − k)TAk +
AT

k Σ(t − k)Bk.
BT

(10)

k=1

k=1

This model also has many parameters and many restricted versions of these models have been pro-
posed to avoid over-ﬁtting (see e.g. section 2 of [1]).
An alternative solution to over-ﬁtting due to over-parameterization is the Bayesian approach of [23]
where Bayesian inference is performed in a dynamic BEKK(1  1) model. Other Bayesian approaches
include the non-parametric generalized Wishart process [22  8]. In these works Σ(t) is modeled by
a generalized Wishart process i.e.

Σ(t) =

Lui(t)ui(t)TLT

(11)

where uid(·) are distributed as independent GPs.

i=1

5 Experiments

We evaluate the proposed Gaussian process conditional copula models (GPCC) on a one-step-ahead
prediction task with synthetic data and ﬁnancial time series. We use time as the conditioning vari-
able and consider three parametric copula families; Gaussian (GPCC-G)  Student’s t (GPCC-T) and
symmetrized Joe Clayton (GPCC-SJC). The parameters of these copulas are presented in Table 1
along with the transformations used to model them. Figure 1 shows plots of the densities of these
three parametric copula models. The code and data are publicly available at http://jmhl.org.

1The parameterization used in this paper is related by ρ = sin(0.5τ π)

5

Copula
Gaussian
Student’s t

SJC

Parameters
correlation  τ
correlation  τ

degrees of freedom  ν
upper dependence  τ U
lower dependence  τ L

Transformation
0.99(2Φ[f (t)] − 1)
0.99(2Φ[f (t)] − 1)
1 + 106Φ[g(t)]

0.01 + 0.98Φ[g(t)]
0.01 + 0.98Φ[g(t)]

Synthetic parameter function
τ (t) = 0.3 + 0.2 cos(tπ/125)
τ (t) = 0.3 + 0.2 cos(tπ/125)
ν(t) = 1 + 2(1 + cos(tπ/250))

τ U (t) = 0.1 + 0.3(1 + cos(tπ/125))

τ L(t) = 0.1 + 0.3(1 + cos(tπ/125 + π/2))

Table 1: Copula parameters  modeling formulae and parameter functions used to generate synthetic
data. Φ is the standard Gaussian cumulative density function f and g are GPs.

The three variants of GPCC were compared against three dynamic copula methods and three con-
stant copula models. The three dynamic methods include the HMM based model  TVC and DSJCC
introduced in Section 4. The three constant copula models use Gaussian  Student’s t and SJC copulas
with parameter values that do not change with time (CONST-G  CONST-T and CONST-SJC). We
perform a one-step-ahead rolling-window prediction task on bivariate time series {(ut  vt)}. Each
model is trained on the ﬁrst nW data points and the predictive log-likelihood of the (nW +1)−th data
point is recorded  where nW = 1000. This is then repeated  shifting the training and test windows
forward by one data point. The methods are then compared by average predictive log-likelihood; an
appropriate performance measure for copula estimation since copulas are probability distributions.

5.1 Synthetic Data

We generated three synthetic datasets of length 5001 from copula models (Gaussian  Student’s t 
SJC) whose parameters vary as periodic functions of time  as speciﬁed in Table 1. Table 2 reports
the average predictive log-likelihood for each method on each synthetic time series. The results of
the best performing method on each synthetic time series are shown in bold. The results of any
other method are underlined when the differences with respect to the best performing method are
not statistically signiﬁcant according to a paired t test at α = 0.05.
GPCC-T and GPCC-SJC obtain the best results in the Student’s t and SJC time series respectively.
However  HMM is the best performing method for the Gaussian time series. This technique suc-
cessfully captures the two regimes of low/high correlation corresponding to the peaks and troughs
of the sinusoid that maps time t to correlation τ. The proposed methods GPCC-[G T SJC] are more
ﬂexible and hence less efﬁcient than HMM in this particular problem. However  HMM performs
signiﬁcantly worse in the Student’s t and SJC time series since the different periods for the different
copula parameter functions cannot be captured by a two state model. Figure 2 shows how GPCC-T
successfully tracks τ (t) and ν(t) in the Student’s t time series. The plots display the mean (red) and
conﬁdence bands (orange  0.1 and 0.9 quantiles) for the predictive distribution of τ (t) and ν(t) as
well as the ground truth values (blue). Finally  Table 2 also shows that the static copula methods
CONST-[G T SJC] are usually outperformed by all dynamic techniques GPCC-[G T SJC]  DSJCC 
TVC and HMM.

5.2 Foreign Exchange Time Series

We evaluated each method on the daily logarithmic returns of nine currencies shown in Table 3 (all
priced with respect to the U.S. dollar).The date range of the data is 02-01-1990 to 15-01-2013; a
total of 6011 observations. We evaluated the methods on eight bivariate time series  pairing each
currency pair with the Swiss franc (CHF). CHF is known to be a safe haven currency  meaning that
investors ﬂock to it during times of uncertainty [16]. Consequently we expect correlations between
CHF and other currencies to have large variability across time in response to changes in ﬁnancial
conditions.
We ﬁrst process our data using an asymmetric AR(1)-GARCH(1 1) process with non-parametric
innovations [9] to estimate the univariate marginal cdfs at all time points. We train this GARCH
model on nW = 2016 data points and then predict the cdf of the next data point; subsequent cdfs
are predicted by shifting the training window by one data point in a rolling-window methodology.
The cdf estimates are used to transform the raw logarithmic returns (xt  yt) into a pseudo-sample
of the underlying copula (ut  vt) as described in Section 2. We note that any method for predicting
univariate cdfs could have been used to produce pseudo-samples from the copula. We then perform

6

the rolling-window predictive likelihood experiment on the transformed data. The results are shown
in Table 4; overall the best technique is GPCC-T  followed by GPCC-G. The dynamic copula meth-
ods GPCC-[G T SJC]  HMM  and TVC outperform the static methods CONST-[G T SJC] in all the
analyzed series. The dynamic method DSJCC occasionally performed poorly; worse than the static
methods for 3 experiments.

Gaussian Student SJC
Method
0.3879 0.2513
0.3347
GPCC-G
0.4656 0.2610
0.3397
GPCC-T
0.4132 0.2771
0.3355
GPCC-SJC
0.3555
0.4422 0.2547
HMM
0.4273 0.2534
0.3277
TVC
0.4096 0.2612
0.3329
DSJCC
0.3201 0.2339
0.3129
CONST-G
0.3178
0.4218 0.2499
CONST-T
0.3812 0.2502
CONST-SJC 0.3002

test log-likelihood of

Figure 2: Predictions made by GPCC-T for ν(t) and τ (t) on
the synthetic time series sampled from a Student’s t copula.
Code Currency Name
CHF Swiss Franc
AUD Australian Dollar
CAD Canadian Dollar
JPY Japanese Yen
NOK Norwegian Krone
SEK Swedish Krona
EUR Euro
NZD New Zeland Dollar
GBP British Pound

Table 2: Avg.
each method on each time series.
AUD CAD
JPY NOK SEK EUR GBP NZD
Method
0.1260 0.0562 0.1221 0.4106 0.4132 0.8842 0.2487 0.1045
GPCC-G
0.1319 0.0589 0.1201 0.4161 0.4192 0.8995 0.2514 0.1079
GPCC-T
GPCC-SJC 0.1168 0.0469 0.1064 0.3941 0.3905 0.8287 0.2404 0.0921
0.1164 0.0478 0.1009 0.4069 0.3955 0.8700 0.2374 0.0926
HMM
0.1181 0.0524 0.1038 0.3930 0.3878 0.7855 0.2301 0.0974
TVC
DSJCC
0.0798 0.0259 0.0891 0.3994 0.3937 0.8335 0.2320 0.0560
CONST-G 0.0925 0.0398 0.0771 0.3413 0.3426 0.6803 0.2085 0.0745
CONST-T
0.1078 0.0463 0.0898 0.3765 0.3760 0.7732 0.2231 0.0875
CONST-SJC 0.1000 0.0425 0.0852 0.3536 0.3544 0.7113 0.2165 0.0796

Table 3: Currencies.

Table 4: Avg. test log-likelihood of each method on the currency data.

Figure 3: Left and middle  predictions made by GPCC-T for ν(t) and τ (t) on the time series EUR-
CHF when trained on data from 10-10-2006 to 09-08-2010. There is a signiﬁcant reduction in ν(t)
at the onset of the 2008-2012 global recession. Right  predictions made by GPCC-SJC for τ U (t) and
τ L(t) when trained on the same time-series data. The predictions for τ L(t) are much more erratic
than those for τ U (t).

The proposed method GPCC-T can capture changes across time in the parameters of the Student’s t
copula. The left and middle plots in Figure 3 show predictions for ν(t) and τ (t) generated by GPCC-
T. In the left plot  we observe a reduction in ν(t) at the onset of the 2008-2012 global recession
indicating that the return series became more prone to outliers. The plot for τ (t) (middle) also
shows large changes across time.
In particular  we observe large drops in the dependence level
between EUR-USD and CHF-USD during the fall of 2008 (at the onset of the global recession) and
the summer of 2010 (corresponding to the worsening European sovereign debt crisis).
For comparison  we include predictions for τ L(t) and τ U (t) made by GPCC-SJC in the right plot
of Figure 3. In this case  the prediction for τ U (t) is similar to the one made by GPCC-T for τ (t) 

7

02004006008001000515Student's t Time Series Mean GPCC−TGround truth020040060080010000.20.40.60.8Student's t Time SeriesMean GPCC−TGround truth020406080100120140EUR−CHF GPCC-T Oct 06Mar 07Aug 07Jan 08Jun 08Nov 08Apr 09Oct 09Mar 10Aug 10Mean GPCC−T0.30.40.50.60.70.80.91.0EUR−CHF GPCC-T Oct 06Mar 07Aug 07Jan 08Jun 08Nov 08Apr 09Oct 09Mar 10Aug 10Mean GPCC−T0.20.40.60.81.01.2EUR−CHF  GPCC-SJC Oct 06Mar 07Aug 07Jan 08Jun 08Nov 08Apr 09Oct 09Mar 10Aug 10Mean GPCC-SJCMean GPCC-SJCbut the prediction for τ L(t) is much noisier and erratic. This suggests that GPCC-SJC is less robust
than GPCC-T. All the copula densities in Figure 1 take large values in the proximity of the points
(0 0) and (1 1) i.e. positive correlation. However  the Student’s t copula is the only one of these
three copulas which can take high values in the proximity of the points (0 1) and (1 0) i.e. negative
correlation. The plot in the left of Figure 3 shows how ν(t) takes very low values at the end of the
time period  increasing the robustness of GPCC-T to negatively correlated outliers.

5.3 Equity Time Series

As a further comparison  we evaluated each method on the logarithmic returns of 8 equity pairs  from
the same date range and processed using the same AR(1)-GARCH(1 1) model discussed previously.
The equities were chosen to include pairs with both high correlation (e.g. RBS and BARC) and low
correlation (e.g. AXP and BA).
The results are shown in Table 5; again the best technique is GPCC-T  followed by GPCC-G.

CSX

AXP CNW ED
BA
EIX

HPQ BARC RBS RBS
HD
HON
IBM HSBC BARC HSBC
Method
0.1247 0.1133 0.1450 0.2072 0.1536 0.2424 0.3401 0.1860
GPCC-G
0.1289 0.1187 0.1499 0.2059 0.1591 0.2486 0.3501 0.1882
GPCC-T
GPCC-SJC 0.1210 0.1095 0.1399 0.1935 0.1462 0.2342 0.3234 0.1753
0.1260 0.1119 0.1458 0.2040 0.1511 0.2486 0.3414 0.1818
HMM
0.1251 0.1119 0.1459 0.2011 0.1511 0.2449 0.3336 0.1823
TVC
DSJCC
0.0935 0.0750 0.1196 0.1721 0.1163 0.2188 0.3051 0.1582
CONST-G 0.1162 0.1027 0.1288 0.1962 0.1325 0.2307 0.2979 0.1663
CONST-T
0.1239 0.1091 0.1408 0.2007 0.1481 0.2426 0.3301 0.1775
CONST-SJC 0.1175 0.1046 0.1307 0.1891 0.1373 0.2268 0.2992 0.1639

Figure 4: Prediction for ν(t)
on RBS-BARC.

Table 5: Average test log-likelihood for each method on each pair of
stocks.

Figure 4 shows predictions for ν(t) generated by GPCC-T. We observe low values of ν during
2010 suggesting that a Gaussian copula would be a bad ﬁt to the data. Indeed  GPCC-G performs
signiﬁcantly worse than GPCC-T on this equity pair.

6 Conclusions and Future Work

We have proposed an inference scheme to ﬁt a conditional copula model to multivariate data where
the copula is speciﬁed by multiple parameters. The copula parameters are modeled as unknown non-
linear functions of arbitrary conditioning variables. We evaluated this framework by estimating time-
varying copula parameters for bivariate ﬁnancial time series. Our method consistently outperforms
static copula models and other dynamic copula models.
In this initial investigation we have focused on bivariate copulas. Higher dimensional copulas are
typically constructed using bivariate copulas as building blocks [2  12]. Our framework could be
applied to these constructions and our empirical predictive performance gains will likely transfer to
this setting. Evaluating the effectiveness of this approach compared to other models of multivariate
covariance would be a proﬁtable area of empirical research.
One could also extend the analysis presented here by including additional conditioning variables
as well as time. For example  including a prediction of univariate volatility as a conditioning vari-
able would allow copula parameters to change in response to changing volatility. This would pose
inference challenges as the dimension of the GP increases  but could create richer models.

Acknowledgements

We thank David L´opez-Paz and Andrew Gordon Wilson for interesting discussions. Jos´e Miguel
Hern´andez-Lobato acknowledges support from Infosys Labs  Infosys Limited. Daniel Hernandez-
Lobato acknowledges support from the Spanish Direcci´on General de Investigaci´on  project ALLS
(TIN2010-21575-C02-02).

8

01020304050RBS−BARC GPCC-TApr 09Sep 09Aug 10Jan 11Jul 11Dec 11Jun 12Nov 12Apr 13Mean GPCC−TReferences
[1] L. Bauwens  S. Laurent  and J. V. K. Rombouts. Multivariate GARCH models: a survey. Journal of

Applied Econometrics  21(1):79–109  2006.

[2] T. Bedford and R. M. Cooke. Probability density decomposition for conditionally dependent random

variables modeled by vines. Annals of Mathematics and Artiﬁcial Intelligence  32(1-4):245–268  2001.

[3] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer 

2007.

[4] T. Bollerslev. Generalized autoregressive conditional heteroskedasticity.

31(3):307–327  1986.

Journal of Econometrics 

[5] T. Bollerslev  R. F. Engle  and J. M. Wooldridge. A capital asset pricing model with time-varying covari-

ances. The Journal of Political Economy  pages 116–131  1988.

[6] G. Elidan. Copulas and machine learning. In Invited survey to appear in the proceedings of the Copulae

in Mathematical and Quantitative Finance workshop  2012.

[7] R. F. Engle and K. F. Kroner. Multivariate simultaneous generalized ARCH. Econometric theory 

11(1):122–150  1995.

[8] E. B. Fox and D. B. Dunson. Bayesian nonparametric covariance regression. arXiv:1101.2017  2011.
[9] J. M. Hern´andez-Lobato  D. Hern´andez-Lobato  and A. Su´arez. GARCH processes with non-parametric
In Artiﬁcial Neural Networks ICANN 2007  volume 4669 of

innovations for market risk estimation.
Lecture Notes in Computer Science  pages 718–727. Springer Berlin Heidelberg  2007.

[10] H. Joe. Asymptotic efﬁciency of the two-stage estimation method for copula-based models. Journal of

Multivariate Analysis  94(2):401–419  2005.

[11] E. Jondeau and M. Rockinger. The Copula-GARCH model of conditional dependencies: An international

stock market application. Journal of International Money and Finance  25(5):827–853  2006.

[12] D. Lopez-Paz  J. M. Hern´andez-Lobato  and Z. Ghahramani. Gaussian process vine copulas for multi-
variate dependence. In S Dasgupta and D McAllester  editors  JMLR W&CP 28(2): Proceedings of The
30th International Conference on Machine Learning  pages 10–18. JMLR  2013.

[13] T. P. Minka. Expectation Propagation for approximate Bayesian inference. Proceedings of the 17th

Conference in Uncertainty in Artiﬁcial Intelligence  pages 362–369  2001.

[14] A. Naish-Guzman and S. Holden. The generalized ﬁtc approximation. In J.C. Platt  D. Koller  Y. Singer 
and S. Roweis  editors  Advances in Neural Information Processing Systems 20  pages 1057–1064. MIT
Press  Cambridge  MA  2008.

[15] A. J. Patton. Modelling asymmetric exchange rate dependence.

47(2):527–556  2006.

International Economic Review 

[16] A. Ranaldo and P. S¨oderlind. Safe haven currencies. Review of Finance  14(3):385–407  2010.
[17] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press  2006.
[18] A. Sklar. Fonctions de r´epartition `a n dimensions et leurs marges. Publ. Inst. Statis. Univ. Paris  8(1):229–

231  1959.

[19] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. In Y. Weiss  B. Sch¨olkopf 
and J. Platt  editors  Advances in Neural Information Processing Systems 18  pages 1257–1264. MIT
Press  Cambridge  MA  2006.

[20] Y. K. Tse and A. K. C. Tsui. A multivariate generalized autoregressive conditional heteroscedasticity
model with time-varying correlations. Journal of Business & Economic Statistics  20(3):351–362  2002.
[21] M. A. J. van Gerven  B. Cseke  F. P. de Lange  and T. Heskes. Efﬁcient bayesian multivariate fmri analysis

using a sparsifying spatio-temporal prior. NeuroImage  50(1):150–161  2010.

[22] A. G. Wilson and Z. Ghahramani. Generalised Wishart processes. In F. Cozman and A. Pfeffer  editors 
Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artiﬁcial Intelli-
gence (UAI-11)  Barcelona  Spain  2011. AUAI Press.

[23] Y. Wu  J. M. Hernandez-Lobato  and Z. Ghahramani. Dynamic covariance models for multivariate ﬁ-
In S. Dasgupta and D. McAllester  editors  Proceedings of the 30th International
nancial time series.
Conference on Machine Learning (ICML-13)  volume 28  pages 558–566. JMLR Workshop and Confer-
ence Proceedings  2013.

9

,José Miguel Hernández-Lobato
James Lloyd
Daniel Hernández-Lobato
Cong Xie
Ling Yan
Wu-Jun Li
Zhihua Zhang
Ho Chung Law
Christopher Yau
Dino Sejdinovic
Lisa Zhang
Gregory Rosenblatt
Ethan Fetaya
Renjie Liao
William Byrd
Matthew Might
Raquel Urtasun
Richard Zemel