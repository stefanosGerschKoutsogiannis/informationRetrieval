2009,Learning a Small Mixture of Trees,The problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning  e.g. variational inference and classification. Within this context  we consider the task of learning a mixture of tree distributions. Although mixtures of trees can be learned by minimizing the KL-divergence using an EM algorithm  its success depends heavily on the initialization. We propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the $\alpha$-divergence with $\alpha = \infty$. We formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration. Compared to previous methods  our approach results in a significantly smaller mixture of trees that provides similar or better accuracies. We demonstrate the usefulness of our approach by learning pictorial structures for face recognition.,Learning a Small Mixture of Trees∗

M. Pawan Kumar

Computer Science Department

Stanford University

Daphne Koller

Computer Science Department

Stanford University

pawan@cs.stanford.edu

koller@cs.stanford.edu

Abstract

The problem of approximating a given probability distribution using a simpler dis-
tribution plays an important role in several areas of machine learning  for example
variational inference and classiﬁcation. Within this context  we consider the task
of learning a mixture of tree distributions. Although mixtures of trees can be
learned by minimizing the KL-divergence using an EM algorithm  its success de-
pends heavily on the initialization. We propose an efﬁcient strategy for obtaining
a good initial set of trees that attempts to cover the entire observed distribution by
minimizing the α-divergence with α = ∞. We formulate the problem using the
fractional covering framework and present a convergent sequential algorithm that
only relies on solving a convex program at each iteration. Compared to previous
methods  our approach results in a signiﬁcantly smaller mixture of trees that pro-
vides similar or better accuracies. We demonstrate the usefulness of our approach
by learning pictorial structures for face recognition.

1 Introduction
Probabilistic models provide a powerful and intuitive framework for formulating several problems
in machine learning and its application areas  such as computer vision and computational biology. A
critical choice to be made when using a probabilistic model is its complexity. For example  consider
a system that involves n random variables. A probabilistic model that deﬁnes a clique of size n
has the ability to model any distribution over these random variables. However  the task of learning
and inference on such a model becomes computationally intractable. The other extreme case is to
deﬁne a tree structured model that allows for efﬁcient learning [3] and inference [23]. However  tree
distributions have a restrictive form. Hence  they are not suitable for all applications.

A natural way to alleviate the deﬁciencies of tree distributions is to use a mixture of trees [21].
Mixtures of trees can be employed as accurate models for several interesting problems such as pose
estimation [11] and recognition [5  12]. In order to facilitate their use  we consider the problem
of learning them by approximating an observed distribution. Note that the mixture can be learned
by minimizing the Kullback-Leibler (KL) divergence with respect to the observed distribution using
an expectation-maximization (EM) algorithm [21]. However  there are two main drawbacks of this
approach: (i) minimization of KL divergence mostly tries to explain the dominant mode of the
observed distribution [22]  that is it does not explain the entire distribution; and (ii) as the EM
algorithm is prone to local minima  its success depends heavily on the initialization. An intuitive
solution to both these problems is to obtain an initial set of trees that covers as much of the observed
distribution as possible. To this end  we pose the learning problem as that of obtaining a set of trees
that minimize a suitable α-divergence [25].
The α-divergence measures are a family of functions over two probability distributions that measure
the information gain contained in them: that is  given the ﬁrst distribution  how much information
is obtained by observing the second distribution. They form a complete family of measures  in that
no other function satisﬁes all the postulates of information gain [25]. When used as an objective

∗This work was supported by DARPA SA4996-10929-4 and the Boeing company.

1

function to approximate an observed distribution  the value of α plays a signiﬁcant role. For exam-
ple  when α = 1 we obtain the KL divergence. As the value of α keeps increasing  the divergence
measure becomes more and more inclusive [8]  that is it tries to cover as much of the observed dis-
tribution as possible [22]. Hence  a natural choice for our task of obtaining a good initial estimate
would be to set α = ∞.
We formulate the minimization of α-divergence with α = ∞ within the fractional covering frame-
work [24]. However  the standard iterative algorithm for solving fractional covering is not readily
applicable to our problem due to its small stepsize. In order to overcome this deﬁciency we adapt
this approach speciﬁcally for the task of learning mixtures of trees. Each iteration of our approach
adds one tree to the mixture and only requires solving a convex optimization problem. In practice 
our strategy converges within a small number of iterations thereby resulting in a small mixture of
trees. We demonstrate the effectiveness of our approach by providing a comparison with state of the
art methods and learning pictorial structures [6] for face recognition.
2 Related Work
The mixture of trees model was introduced by Meila and Jordan [21] who highlighted its appeal
by providing simple inference and sampling algorithms. They also described an EM algorithm that
learned a mixture of trees by minimizing the KL divergence. However  the accuracy of the EM
algorithm is highly dependent on the initial estimate of the mixture. This is evident in the fact
that their experiments required a large mixture of trees to explain the observed distribution  due to
random initialization.

Several works have attempted to obtain a good set of trees by devising algorithms for minimizing
the KL divergence [8  13  19  26]. In contrast  our method uses α = ∞  thereby providing a set of
trees that covers the entire observed distribution. It has been shown that mixture of trees admit a
decomposable prior [20]. In other words  one can concisely specify a certain prior probability for
each of the exponential number of tree structures for a given set of random variables. Kirschner and
Smyth [14] have also proposed a method to handle a countably inﬁnite mixture of trees. However 
the complexity of both learning and inference in these models restricts their practical use.

Researchers have also considered mixtures of trees in the log-probability space. Unlike a mixture in
the probability space considered in this paper (which contains a hidden variable)  mixtures of trees
in log-probability space still deﬁne pairwise Markov networks. Such mixtures of trees have been
used to obtain upper bounds on the log partition function [27]. However  in this case  the mixture is
obtained by considering subgraphs of a given graphical model instead of minimizing a divergence
measure with respect to the observed data. Finally  we note that semi-metric distance functions can
be approximated to a mixture of tree metrics using the fractional packing framework [24]. This
allows us to approximate semi-metric probabilistic models to a simpler mixture of (not necessarily
tree) models whose pairwise potentials are deﬁned by tree metrics [15  17].
3 Preliminaries
Tree Distribution. Consider a set of n random variables V = {v1  · · ·   vn}  where each variable
va can take a value xa ∈ Xa. We represent a labeling of the random variables (i.e. a particular
assignment of values) as a vector x = {xa|a = 1  · · ·   n}. A tree structured model deﬁned over the
random variables V is a graph whose nodes correspond to the random variables and whose edges E
deﬁne a tree. Such a model assigns a probability to each labeling that can be written as

Pr(x|θT ) =

1

Z(θT ) Q(va vb)∈E θT

ab(xa  xb)
a (xa)deg(a)−1 .

Qva∈V θT

Pr(x|θM ) =XT

2

a (·) refers to unary potentials whose values depend on one variable at a time  and θT

Here θT
ab(·  ·)
refers to pairwise potentials whose values depend on two neighboring variables at a time. The vector
θT is the parameter of the model (which consists of all the potentials) and Z(θT ) is the partition
function which ensures that the probability sums to one. The term deg(a) denotes the degree of the
variable va.
Mixture of Trees. As the name suggests  a mixture of trees is deﬁned by a set of trees along with
a probability distribution over them  that is θM = {(θT   ρT )} such that mixture coefﬁcients ρT > 0

for all T andPT ρT = 1. It deﬁnes the probability of a given labeling as

ρT Pr(x|θT ).

(1)

(2)

α-Divergence. The α-divergence between distributions Pr(·|θ1) (say the observed distribution)
and Pr(·|θ2) (the simpler distribution) is given by

Dα(θ1||θ2) =

1

α − 1

log Xx

Pr(x|θ1)α

Pr(x|θ2)α−1! .

(3)

The α-divergence measure is strictly non-negative and is equal to 0 if and only if θ1 is a reparame-
terization of θ2. It is a generalization of KL divergence which corresponds to α = 1  that is

D1(θ1||θ2) =Xx

Pr(x|θ1) log

Pr(x|θ1)
Pr(x|θ2)

.

As mentioned earlier  we are interested in the case where α = ∞  that is

D∞(θ1||θ2) = max

x

log

Pr(x|θ1)
Pr(x|θ2)

.

(4)

(5)

The inclusive property of α = ∞ is evident from the above formula. Since we would like to
minimize the maximum ratio of probabilities (i.e. the worst case)  we need to ensure that no value of
Pr(x|θ2) is very small  that is the entire distribution is covered. In contrast  the KL divergence can
admit very small values of Pr(x|θ2) since it is concerned with the summation shown in equation (4)
(and not the worst case). To avoid confusion  we shall refer to the case where α = 1 as KL divergence
and the α = ∞ case as α-divergence throughout this paper.
The Learning Problem. Given a set of samples {xi  i = 1  · · ·   m} along with their probabilities
ˆP (xi)  our task is to learn a mixture of trees θM ∗ such that

θM ∗

= arg min

θM  max

i

log

ˆP (xi)

Pr(xi|θM )! = arg max

θM  min

i

Pr(xi|θM )

ˆP (xi) ! .

(6)

We will concentrate on the second form in the above equation (where the logarithm has been
dropped). We deﬁne T = {θTj } to be the set of all t tree distributions that are deﬁned over n
variables. It follows that the probability of a labeling for any mixture of trees can be written as

Pr(x|θM ) =Xj

ρj Pr(x|θTj ) 

(7)

for suitable values of ρj. Note that the mixing coefﬁcients ρ should deﬁne a valid probability
distribution. In other words  ρ belongs to the polytope P deﬁned as

ρ ∈ P ⇒Xj

ρj = 1  ρj ≥ 0  ∀j = 1  · · ·   t.

(8)

Our task is to ﬁnd a sparse vector ρ that minimizes the α-divergence with respect to the observed dis-
tribution. In order to formally specify the minimization of α-divergence as an optimization problem 
we deﬁne an m × t matrix A and an m × 1 vector b such that

A(i  j) = Pr(xi|θTj ) and bi = ˆP (xi).

(9)

We denote the ith row of A as ai and the ith element of b as bi. Using the above notation  the
learning problem can be speciﬁed as

s.t.

max

ρ

λρ 

aiρ ≥ λρbi  ∀i
ρ ∈ P 

(10)

where λρ = mini aiρ/bi due to the form of the above LP. The above formulation suggests that a
natural way to attack the problem would be to use the fractional covering framework [24]. We begin
by brieﬂy describing fractional covering in the next section.

3

4 Fractional Covering
Given an m× t matrix A and an m× 1 vector b > 0  the fractional covering problem is to determine
whether there exists a vector ρ ∈ P such that Aρ ≥ b. The only restriction on the polytope P is
that Aρ ≥ 0 for all ρ ∈ P  which is clearly satisﬁed by our learning problem (since aiρ is the
probability of xi speciﬁed by the mixture of trees corresponding to ρ). Let

λ∗ = max

ρ

min

i

aiρ
bi

.

(11)

If λ∗ < 1 then clearly there does not exist a ρ such that Aρ ≥ b. However  if λ∗ ≥ 1  then the
fractional covering problem requires us to ﬁnd an ǫ-optimal solution  that is ﬁnd a ρ such that

Aρ ≥ (1 − ǫ)λ∗b 

(12)

where ǫ > 0 is a user-speciﬁed tolerance factor. Using the deﬁnitions of A  b and ρ from the
previous section  we observe that in our case λ∗ = 1. In other words  there exists a solution such
that Aρ = b. This can easily be seen by considering a tree with parameter θTj such that

Pr(xi|θTj ) =(cid:26) 1

if

0 otherwise 

i = j 

(13)

and setting ρj = ˆP (xj ). The above solution provides an α-divergence of 0 but at the cost of
introducing m trees in the mixture (where m is the number of samples provided). We would like
to ﬁnd an ǫ-optimal solution with a smaller number of trees by solving the LP (10). However  we
cannot employ standard interior point algorithms for optimizing problem (10). This is due to the
fact that each of its m constraints is deﬁned over an inﬁnite number of unknowns (speciﬁcally  the
mixture coefﬁcients for each of the inﬁnite number of tree distributions deﬁned over the n random
variables). Fortunately  Plotkin et al. [24] provide an iterative algorithm for solving problem (10)
that can handle arbitrarily large number of unknowns in every constraint.
The Fractional Covering Algorithm.
following related problem:

In order to obtain a solution to problem (10)  we solve the

min
ρ∈P

yi =

s.t.

Φ(y) ≡ y⊤b 

1
bi

exp(cid:18)−β

aiρ

bi (cid:19) .

(14)

The objective function Φ(y) is called the potential function for fractional covering. Plotkin et al.
[24] showed that minimizing Φ(y) solves the original fractional covering problem. The term β is a
parameter that is inversely proportional to the stepsize σ of the algorithm. The fractional covering
algorithm is an iterative strategy. At iteration t  the variable ρt is updated as ρt ← (1−σ)ρt−1 +σρ′
such that the update attempts to decrease the potential function. Speciﬁcally  the algorithm proposed
in [24] suggests using the ﬁrst order approximation of Φ(y)  that is

ρ′ = arg min

ρ  Xi

y′

i(bi − βσaiρ)! = arg max

ρ

where

y′⊤Aρ.

(15)

(16)

y′
i =

1
bi

exp(cid:18)−β

(1 − σ)aiρ

bi

(cid:19) .

Typically  the above problem is easy to solve (including for our case  as will be seen in the next
section). Furthermore  for a sufﬁciently large value of β (∝ log m) the above update rule decreases
Φ(y). In more detail  the algorithm of [24] is as follows:

• Deﬁne w = maxρ maxi aiρ/bi to be the width of the problem.
• Start with an initial solution ρ0.
• Deﬁne λρ0 = mini aiρ0/bi  and σ = ǫ/(4βw).
• While λρ < 2λρ0  at iteration t:

– Deﬁne y′ as shown in equation (16).
– Find ρ′ = arg maxρ∈P y′⊤Aρ.
– Update ρt ← (1 − σ)ρt−1 + σρ∗.

4

Plotkin et al. [24] suggest starting with a tolerance factor of ǫ0 = 1/6 and dividing the value of ǫ0
by 2 after every call to the above procedure terminates. This process is continued until a sufﬁciently
accurate (i.e. an ǫ-optimal) solution is recovered. Note that during each call to the above procedure
the potential function Φ(y) is both upper and lower bounded  speciﬁcally

exp(−2βλρ0 ) ≤ Φ(y) ≤ m exp(−βλρ0).

(17)

Furthermore  we are guaranteed to decrease the value of Φ(y) at each iteration. Hence  it follows
that the above algorithm will converge. We refer the reader to [24] for more details.
5 Modifying Fractional Covering
The above algorithm provides an elegant way to solve the general fractional covering problem.
However  as will be seen shortly  in our case it leads to undesirable solutions. Nevertheless  we
show that appropriate modiﬁcations can be made to obtain a small and accurate mixture of trees. We
begin by identify the deﬁciencies of the fractional covering algorithm for our learning problem.
5.1 Drawbacks of the Algorithm
There are two main drawbacks of fractional covering. First  the value of β is typically very large 
which results in a small stepsize σ. In our experiments  β was of the order of 103  which resulted
in slow convergence of the algorithm. Second  the update step provides singleton trees  that is trees
with a probability of 1 for one labeling and 0 for all others. This is due to the fact that  in our case 
the update step solves the following problem:

max

ρ∈PXj  Xi

y′

iρj Pr(xi|θTj )! .

(18)

Note that the above problem is an LP in ρ. Hence  there must exist an optimal solution on the vertex
on the polytope P. In other words  we obtain a single tree distribution θT ∗ such that

θT ∗

= arg max

θT  Xi

y′

i Pr(xi|θT )! .

(19)

The optimal tree distribution for the above problem concentrates the entire mass on the sample
i. Such singleton trees are not desirable as they also result in slow
xi′ where i′ = arg maxi y′
convergence of the algorithm. Furthermore  the learned mixture only provides a non-zero probability
for the samples used during training. Hence  the mixture cannot be used for previously unseen
samples  thereby rendering it practically useless. Note that the method of Rosset and Segal [26]
also faces a similar problem during their update steps for minimizing the KL divergence. In order to
overcome this difﬁculty  they suggest approximating problem (18) by

θT ∗

= arg max

θT Xi

y′

i log(cid:16)Pr(xi|θT )(cid:17)  

(20)

which can be solved efﬁciently using the Chow-Liu algorithm [3]. However  our preliminary exper-
iments (accuracies not reported) indicate that this approach does not work well for minimizing the
potential function Φ(y).
5.2 Fixing the Drawbacks
We adapt the original fractional covering algorithm for our problem in order to overcome the draw-
backs mentioned above. The ﬁrst drawback is handled easily. We start with a small value of β and
increase it by a factor of 2 if we are not able to reduce the potential function Φ(y) at a given itera-
tion. Since we are assured that the value of Φ(y) decreases for a ﬁnite value of β  this procedure is
guaranteed to terminate. In our experiments  we initialized β = 1/w and its value never exceeded
32/w. Note that choosing β to be inversely proportional to w ensures that the initial values of y′
i in
equation (16) are sufﬁciently large (at least exp(−(1 − σ))).
In order to address the second drawback  we note that our aim at an iteration t of the algorithm is to
reduce the potential function Φ(y). That is  given the current distribution parameterized by θMt we
would like to add a new tree θTt to the mixture that solves the following problem:

θTt =

arg min

θT "Φ(y) ≡Xi

y′

i exp −β

σ Pr(xi|θT )

ˆP (xi) !#

(21)

5

s.t. Xi

Pr(xi|θT ) ≤ 1  Pr(xi|θT ) ≥ 0  ∀i = 1  · · ·   m 

(22)

θT ∈ T .

(23)
Here  T is the set of all tree distributions deﬁned over n random variables. Note that the algorithm
of [24] optimizes the ﬁrst order approximation of the objective function (21). However  as seen pre-
viously  for our problem this results in an undesirable solution. Instead  we directly optimize Φ(y)
using an alternative two step strategy. In the ﬁrst step  we drop the last constraint from the above
problem. In other words  we obtain the values of Pr(xi|θT ) that form a valid (but not necessarily
tree-structured) distribution and minimize the function Φ(y). Note that since the Φ(y) is not linear
in Pr(xi|θT )  the optimal solution provides a dense distribution Pr(·|θT ) (as opposed to the ﬁrst
order linear approximation which provides a singleton distribution). In the second step  we project
these values to a tree distribution. It is easy to see that dropping constraint (23) results in a convex
relaxation of the original problem. We solve the convex relaxation using a log-barrier method [1].
Brieﬂy  this implies solving a series of unconstrained optimization problems until we are within a
user-speciﬁed tolerance value of τ from the optimal solution. Speciﬁcally 

• Set f = 1.
• Solve min
• If m/f ≤ τ  then stop. Otherwise  update f = µf and repeat the previous step.

)(cid:16)f Φ(y) −Pi log(Pr(xi|θT )) − log(1 −Pi Pr(xi|θT ))(cid:17).

Pr(·|θT

We used µ = 1.5 in all our experiments  which was sufﬁcient to obtain accurate solutions for
the convex relaxation. At each iteration  the unconstrained optimization problem is solved using
Newton’s method. Recall that Newton’s method minimizes a function g(z) by updating the current
solution as

g(z) ← g(z) −(cid:0)∇2g(z)(cid:1)−1

(24)
where ∇2g(·) denotes the Hessian matrix and ∇g(·) denotes the gradient vector. Note that the most
expensive step in the above approach is the inversion of the Hessian matrix. However  it is easy to
verify that in our case all the off-diagonal elements of the Hessian are equal to each other. By taking
advantage of this special form of the Hessian  we compute its inverse in O(m2) time using Gaussian
elimination (i.e. linear in the number of elements of the Hessian).

∇g(z) 

Once the values of Pr(xi|θT ) are computed in this manner  they are projected to a tree distribution
using the Chow-Liu algorithm [3]. Note that after the projection step we are no longer guaranteed to
decrease the function Φ(y). This would imply that the overall algorithm would not be guaranteed to
converge. In order to overcome this problem  if we are unable to decrease Φ(y) then we determine
the sample xi′ such that

i′ = arg max

i

Pr(xi|θMt )

ˆP (xi)

 

(25)

that is the sample best explained by the current mixture. We enforce Pr(xi′ |θT ) = 0 and solve
the above convex relaxation again. Note that the solution to the new convex relaxation (i.e. the one
with the newly introduced constraint for sample xi′ ) can easily be obtained from the solution of the
previous convex relaxation using the following update:

Pr(xi|θT ) ←(cid:26) Pr(xi|θT ) + ˆP (xi) Pr(xi′ |θT )/s

0

if

otherwise 

i 6= i′ 

(26)

where s = Pi

ˆP (xi). In other words  we do not need to use the log-barrier method to solve the
new convex relaxation. We then project the updated values of Pr(xi|θT ) to a tree distribution. This
process of eliminating one sample and projecting to a tree is repeated until we are able to reduce
the value of Φ(y). Note that in the worst case we will eliminate all but one sample (speciﬁcally  the
one that corresponds to the update scheme of [24]). In other words  we will add a singleton tree.
However  in practice our algorithm converges in a small number (≪ m) of iterations and provides an
accurate mixture of trees. In fact  in all our experiments we never obtained any singleton trees. We
conclude the description of our method by noting that once the new tree distribution θTt is obtained 
the value of σ is easily updated as σ = arg minσ Φ(y).
6 Experiments
We present a comparison of our method with the state of the art algorithms. We also use it to learn
pictorial structures for face recognition. Note that our method is efﬁcient in practice due to the

6

Dataset
Agaricus
Nursery
Splice

TANB

100.0 ± 0
93.0 ± 0
94.9 ± 0.9

MF

99.45 ± 0.004
98.0 ± 0.01

-

Tree

98.65 ± 0.32
92.17 ± 0.38
95.7 ± 0.2

MT

99.98 ± 0.04
99.2 ± 0.02
95.5 ± 0.3

[26] + MT
100.0 ± 0

98.35 ± 0.30
95.6 ± 0.42

Our + MT
100.0 ± 0

99.28 ± 0.13
96.1 ± 0.15

Table 1: Classiﬁcation accuracies for the datasets used in [21]. The ﬁrst column shows the name of the dataset.
The subsequent columns show the mean accuracies and the standard deviation over 5 trials of tree-augmented
naive Bayes [10]  mixture of factorial distributions [2]  single tree classiﬁer [3]  mixture of trees with random
initialization (i.e. the numbers reported in [21])  initialization with [26] and initialization with our approach.
Note that our method provides similar accuracies to [21] while using a smaller mixture of trees (see text).

special form of the Hessian matrix (for the log-barrier method) and the Chow-Liu algorithm [3  21]
(for the projection to tree distributions). In all our experiments  each iteration takes only 5 to 10
minutes (and the number of iterations is equal to the number of trees in the mixture).
Comparison with Previous Work. As mentioned earlier  our approach can be used to obtain a
good initialization for the EM algorithm of [21] since it minimizes α-divergence (providing comple-
mentary information to the KL-divergence used in [21]). This is in contrast to the random initial-
izations used in the experiments of [21] or the initialization obtained by [26] (that also attempts to
minimize the KL-divergence). We consider the task of using the mixture of trees as a classiﬁer  that
is given training data that consists of feature vectors xi together with the class values ci  the task
is to correctly classify previously unseen test feature vectors. Following the protocol of [21]  this
can be achieved in two ways. For the ﬁrst type of classiﬁer  we append the feature vector xi with
i. We then learn a mixture of tree that predicts the
its class value ci to obtain a new feature vector x′
i. Given a new feature vector x we assign it the class c that results in the highest
probability of x′
probability. For the second type of classiﬁer  we learn a mixture of trees for each class value such
that it predicts the probability of a feature vector belonging to that particular class. Once again 
given a new feature vector x we assign it the class c which results in the probability.
We tested our approach on the three discrete valued datasets used in [21]. In all our experiments 
we initialized the mixture with a single tree obtained from the Chow-Liu algorithm. We closely
followed the experimental setup of [21] to ensure that the comparisons are fair. Table 1 provides the
accuracy of our approach together with the results reported in [21]. For ‘Splice’ the ﬁrst classiﬁer
provides the best results  while ‘Agaricus’ and ‘Nursery’ use the second classiﬁer. Note that our
method provides similar accuracies to [21]. More importantly  it uses a smaller mixture of trees to
achieve these results. Speciﬁcally  the method of [21] uses 12  30 and 3 trees for the three datasets
respectively. In contrast our method uses 3-5 trees for ‘Agaricus’  10-15 trees for ‘Nursery’ and 2
trees for Splice (where the number of trees in the mixture was obtained using a validation dataset 
see [21] for details). Furthermore  unlike [21  26]  we obtain better accuracies by using a mixture
of trees instead of a single tree for the ‘Splice’ dataset. It is worth noting that [26] also provided a
small set of initial trees (with comparable size to our method). However  since the trees do not cover
the entire observed distribution  their method provides less accurate results.
Face Recognition. We tested our approach on the task of recognizing faces using the publicly
available dataset1 containing the faces of 11 characters in an episode of ‘Buffy the Vampire Slayer’.
The total number of faces in the dataset is 24 244. For each face we are provided with the location
of 13 facial features (see Fig. 1). Furthermore  for each facial feature  we are also provided with
a vector that represents the appearance of that facial feature [5] (using the normalized grayscale
values present in a circular region of radius 7 centered at the facial feature). As noted in previous
work [5  18] the task is challenging due to large intra-class variations in expression and lighting
conditions.

Given the appearance vector  the likelihood of each facial feature belonging to a particular character
can be found using logistic regression. However  the relative locations of the facial features also
offer important cues in distinguishing one character from the other (e.g. the width of the eyes or the
distance between an eye and the nose). Typically  in vision systems  this information is not used.
In other words  the so-called bag of visual words model is employed. This is due to the somewhat
counter-intuitive observation made by several researchers that models that employ spatial prior on
the features  e.g. pictorial structures [6]  often provide worse recognition accuracies than those that
throw away this information. However  this may be due to the fact that often the structure and
parameters of pictorial structures and other related models are set by hand.

1Available at http://www.robots.ox.ac.uk/˜vgg/research/nface/data.html

7

Figure 1: The structure of the seven trees learned for 3 of the 11 characters using our method. The red squares
show the position of the facial features while the blue lines indicate the edges. The structure and parameters of
the trees vary signiﬁcantly  thereby indicating the multimodality of the observed distribution.

0

1

2

3

[26]
Our

65.68% 66.05% 66.01% 66.01% 66.08% 66.08% 66.16% 66.20%
65.68% 66.05% 66.65% 66.86% 67.25% 67.48% 67.50% 67.68%

4

5

6

7

Table 2: Accuracy for the face recognition experiments. The columns indicate the size of the mixture  ranging
from 0 (i.e. the bag of visual words model) to 7 (where the results saturate). Note that our approach  which
minimizes the α-divergence  provides better results than the method of [26]  which minimizes KL-divergence.
In order to test whether a spatial model can help improve recognition  we learned a mixture of trees
for each of the characters. The random variables of the trees correspond to the facial features and
their values correspond to the relative location of the facial feature with respect to the center of the
nose. The unary potentials of each random variable is speciﬁed using the appearance vectors (i.e.
the likelihood obtained by logistic regression). In order to obtain the pairwise potentials (i.e. the
structure and parameters of the mixture of trees)  the faces are normalized to remove global scaling
and in-plane rotation using the location of the facial features. We use the faces found in the ﬁrst 80%
of the episode to learn the mixture of trees. The faces found in the remaining 20% of the episode
were used as test data. Splitting the dataset in this manner (i.e. a non-random split) ensures that we
do not have any trivial cases where a face found in frame t is used for training and a (very similar)
face found in frame t + 1 is used for testing.
Fig. 1 shows the structure of the trees learned for 3 characters. The structures differ signiﬁcantly
between characters  which indicates that different spatial priors are dominant for different characters.
Although the structure of the trees for a particular character are similar  they vary considerably in
the parameters. This suggests that the distribution is in fact multimodal and therefore cannot be
represented accurately using a single tree. Although vision researchers have tried to overcome this
problem by using more complex models  e.g. see [4]  their use is limited by a lack of efﬁcient
learning algorithms. Table 2 shows the accuracy of the mixture of trees learned by the method
of [26] and our approach. In this experiment  reﬁning the mixture of trees using the EM algorithm
of [21] did not improve the results. This is due to the fact that the training and testing data differ
signiﬁcantly (due to non-random splits  unlike the previous experiments which used random splits of
the UCI datasets). In fact  when we split the face dataset randomly  we found that the EM algorithm
did help. However  classiﬁcation problems simulated using random splits of video frames are rare
in real-world applications. Since [26] tries to minimize the KL divergence  it mostly tries to explain
the dominant mode of the observed distribution. This is evident in the fact that the accuracy of the
mixture of trees does not increase signiﬁcantly as the size of the mixture increases (see table 2  ﬁrst
row). In contrast  the minimization of α-divergence provides a diverse set of trees that attempt to
explain the entire distribution thereby providing signiﬁcantly better results (table 2  second row).
7 Discussion
We formulated the problem of obtaining a small mixture of trees by minimizing the α-divergence
within the fractional covering framework. Our experiments indicate that the suitably modiﬁed frac-
tional covering algorithm provides accurate models. We believe that our approach offers a natural
framework for addressing the problem of minimizing α-divergence and could prove useful for other
classes of mixture models  for example mixtures of trees in log-probability space for which there
exist several efﬁcient and accurate inference algorithms [16  27]. There also appears to be a connec-
tion between fractional covering (proposed in the theory community) and Discrete AdaBoost [7  9]
(proposed in the machine learning community) that merits further exploration.

8

References

[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[2] P. Cheeseman and J. Stutz. Bayesian classiﬁcation (AutoClass): Theory and results. In KDD 

pages 153–180  1995.

[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees.

IEEE Transactions on Information Theory  14(3):462–467  1968.

[4] D. Crandall  P. Felzenszwalb  and D. Huttenlocher. Spatial priors for parts-based recognition

using statistical models. In CVPR  2005.

[5] M. Everingham  J. Sivic  and A. Zisserman. Hello! My name is... Buffy - Automatic naming

of characters in TV video. In BMVC  2006.

[6] M. Fischler and R. Elschlager. The representation and matching of pictorial structures. TC 

22:67–92  January 1973.

[7] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[8] B. Frey  R. Patrascu  T. Jaakkola  and J. Moran. Sequentially ﬁtting inclusive trees for inference

in noisy-OR networks. In NIPS  2000.

[9] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: A statistical view of

boosting. Annals of Statistics  28(2):337–407  2000.

[10] N. Friedman  D. Geiger  and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning 

29:131–163  1997.

[11] S. Ioffe and D. Forsyth. Human tracking with mixtures of trees. In ICCV  pages 690–695 

2001.

[12] S. Ioffe and D. Forsyth. Mixtures of trees for object recognition. In CVPR  pages 180–185 

2001.

[13] Y. Jing  V. Pavlovic  and J. Rehg. Boosted bayesian network classiﬁers. Machine Learning 

73(2):155–184  2008.

[14] S. Kirschner and P. Smyth. Inﬁnite mixture of trees. In ICML  pages 417–423  2007.
[15] J. Kleinberg and E. Tardos. Approximation algorithms for classiﬁcation problems with pair-

wise relationships: Metric labeling and Markov random ﬁelds. In STOC  1999.

[16] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. PAMI 

2006.

[17] M. P. Kumar and D. Koller. MAP estimation of semi-metric MRFs via hierarchical graph cuts.

In UAI  2009.

[18] M. P. Kumar  P. Torr  and A. Zisserman. An invariant large margin nearest neighbour classiﬁer.

In ICCV  2007.

[19] Y. Lin  S. Zhu  D. Lee  and B. Taskar. Learning sparse Markov network structure via ensemble-

of-trees models. In AISTATS  2009.

[20] M. Meila and T. Jaakkola. Tractable Bayesian learning of tree belief networks. In UAI  2000.
[21] M. Meila and M. Jordan. Learning with a mixture of trees. JMLR  1:1–48  2000.
[22] T. Minka. Divergence measures and message passing. Technical report  Microsoft Research 

2005.

[23] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.

Morgan-Kauffman  1988.

[24] S. Plotkin  D. Shmoys  and E. Tardos. Fast approximation algorithms for fractional packing

and covering problems. Mathematics of Operations Research  20:257–301  1995.

[25] A. Renyi. On measures of information and entropy. In Berkeley Symposium on Mathematics 

Statistics and Probability  pages 547–561  1961.

[26] S. Rosset and E. Segal. Boosting density estimation. In NIPS  2002.
[27] M. Wainwright  T. Jaakkola  and A. Willsky. A new class of upper bounds on the log partition

function. IEEE Transactions on Information Theory  51:2313–2335  2005.

9

,Mohammad Emtiyaz Khan
Mauro Scanagatta
Giorgio Corani
Cassio de Campos
Marco Zaffalon