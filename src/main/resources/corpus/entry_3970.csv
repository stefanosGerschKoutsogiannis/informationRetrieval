2019,On the Expressive Power of Deep Polynomial Neural Networks,We study deep neural networks with polynomial activations  particularly their expressive power.  For a fixed architecture and activation degree  a polynomial neural network defines an algebraic map from weights to polynomials.  The image of this map is the functional space associated to the network  and it is an irreducible algebraic variety upon taking closure.  This paper proposes the dimension of this variety as a precise measure of the expressive power of polynomial neural networks.  We obtain several theoretical results regarding this dimension as a function of architecture  including an exact formula for high activation degrees  as well as upper and lower bounds on layer widths in order for deep polynomials networks to fill the ambient functional space. We also present computational evidence that it is profitable in terms of expressiveness for layer widths to increase monotonically and then decrease monotonically.  Finally  we link our study to favorable optimization properties when training weights  and we draw  intriguing connections with tensor and polynomial decompositions.,On the Expressive Power of

Deep Polynomial Neural Networks

Joe Kileel⇤

Princeton University

Matthew Trager⇤
New York University

Joan Bruna

New York University

Abstract

We study deep neural networks with polynomial activations  particularly their
expressive power. For a ﬁxed architecture and activation degree  a polynomial
neural network deﬁnes an algebraic map from weights to polynomials. The image
of this map is the functional space associated to the network  and it is an irreducible
algebraic variety upon taking closure. This paper proposes the dimension of this
variety as a precise measure of the expressive power of polynomial neural networks.
We obtain several theoretical results regarding this dimension as a function of
architecture  including an exact formula for high activation degrees  as well as
upper and lower bounds on layer widths in order for deep polynomials networks to
ﬁll the ambient functional space. We also present computational evidence that it is
proﬁtable in terms of expressiveness for layer widths to increase monotonically and
then decrease monotonically. Finally  we link our study to favorable optimization
properties when training weights  and we draw intriguing connections with tensor
and polynomial decompositions.

1

Introduction

A fundamental problem in the theory of deep learning is to study the functional space of deep neural
networks. A network can be modeled as a composition of elementary maps  however the family of
all functions that can be obtained in this way is extremely complex. Many recent papers paint an
accurate picture for the case of shallow networks (e.g.  using mean ﬁeld theory [7  27]) and of deep
linear networks [2  3  21]  however a similar investigation of deep nonlinear networks appears to be
signiﬁcantly more challenging  and require very different tools.
In this paper  we consider a general model for deep polynomial neural networks  where the activation
function is a polynomial (r-th power) exponentiation. The advantage of this framework is that
the functional space associated with a network architecture is algebraic  so we can use tools from
algebraic geometry [17] for a precise investigation of deep neural networks. Indeed  for a ﬁxed
activation degree r and architecture d = (d0  . . .   dh) (expressed as a sequence of widths)  the family
of all networks with varying weights can be identiﬁed with an algebraic variety Vd r  embedded
in a ﬁnite-dimensional Euclidean space. In this setting  an algebraic variety can be thought of as a
manifold that may have singularities.
In this paper  our main object of study is the dimension of Vd r as a variety (in practice  as a manifold) 
which may be regarded as a precise measure of the architecture’s expressiveness. Speciﬁcally  we
prove that this dimension stabilizes when activations are high degree  and we provide an exact
dimension formula for this case (Theorem 14). We also investigate conditions under which Vd r
ﬁlls its ambient space. This question is important from the vantage point of optimization  since an
architecture is “ﬁlling” if and only if it corresponds to a convex functional space (Proposition 6). In

⇤Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

this direction  we prove a bottleneck property  that if a width is not sufﬁciently large  the network can
never ﬁll the ambient space regardless of the size of other layers (Theorem 19).
In a broader sense  our work introduces a powerful language and suite of mathematical tools for
studying the geometry of network architectures. Although this setting requires polynomial activations 
it may be used as a testing ground for more general situations and  e.g.  to verify rules of thumb
rigorously. Finally  our results show that polynomial neural networks are intimately related to
the theory of tensor decompositions [22]. In fact  representing a polynomial as a deep network
corresponds to a type of decomposition of tensors which may be viewed as a composition of
decompositions of a recently introduced sort [24]. Using this connection  we establish general
non-trivial upper bounds on ﬁlling widths (Theorem 10). We believe that our work can serve as a
ﬁrst step towards many interesting research challenges in developing the theoretical underpinnings of
deep learning.

1.1 Related work

The study of the expressive power of neural networks dates back to seminal work on the universality
of networks as function approximators [10  19]. More recently  there has been research supporting
the hypothesis of “depth efﬁciency”  i.e.  the fact that deep networks can approximate functions more
efﬁciently than shallow networks [11  25  8  9]. In contrast to this line of work  we study the class
of functions that can be expressed exactly using a network. Our analysis may of course be used to
investigate the problem of approximation  however this is not the focus of this paper.
Most of the aforementioned studies make strong hypotheses on the network architecture. In par-
ticular  [11  25] focus on arithmetic circuits  or sum-product networks [29]. These are networks
composed of units that compute either the product or a weighted sum of their inputs. In [8]  the
authors introduce a model of convolutional arithmetic circuits. This is a particular class of arithmetic
circuits that includes networks with layers of 1D convolutions and product pooling. This model does
not allow for non-linear activations (beside the product pooling)  although the follow-up paper [9]
extends some results to ReLU activations with sum pooling. Interestingly  these networks are related
to Hierarchical Tucker (HT) decomposition of tensors.
The polynomial networks studied in this paper are not arithmetic circuits  but feedforward deep
networks with polynomial r-th power activations. This is a vast generalization of a setting consid-
ered in several recent papers [33  14  31]  that study shallow (two layer) networks with quadratic
activations (r = 2). These papers show that if the width of the intermediate layer is at least twice
the input dimension  then the quadratic loss has no “bad” local minima. This result in line with our
Proposition 5  which explains in this case the functional space is convex and ﬁlls the ambient space.
We also point out that polynomial activations are required for the functional space of the network to
span a ﬁnite dimensional vector space [23  33].
The polynomial networks considered in this paper do not correspond to HT tensor decompositions as
in [8  9]  rather they are related to a different polynomial/tensor decomposition attracting very recent
interest [16  24]. These generalize usual decompositions  however their algorithmic and theoretical
understanding are  mostly  wide open. Neural networks motivate several questions in this vein.
Finally  we mention other recent works that study neural networks from the perspective of algebraic
geometry [26  32  20].

Main contributions. Our main contributions can be summarized as follows.

• We give a precise formulation of the expressiveness of polynomial networks in terms of the

algebraic dimension of the functional space as an algebraic variety.

• We spell out the close  two-way relationship between polynomial networks and a particular

family of decompositions of tensors.

• We prove several theoretical results on the functional space of polynomial networks. Notably 
we give a formula for the dimension that holds for sufﬁciently high activation degrees
(Theorem 14) and we prove a tight lower bound on the width of the layers for the network
to be “ﬁlling” in the functional space (Theorem 19).

2

Notation. We use Symd(Rn) to denote the space of homogeneous polynomials of degree d in n

variables with coefﬁcients in R. This set is a vector space over R of dimension Nd n =n+d1

spanned by all monomials of degree d in n variables. In practice  Symd(Rn) is isomorphic to RNd n 
and our networks will correspond to points in this high dimensional space. The notation Symd(Rn)
expresses the fact that a polynomial of degree d in n variables can always be identiﬁed with a
symmetric tensor in (Rn)⌦d that collects all of its coefﬁcients.

d

 

2 Basic setup

A polynomial network is a function p✓ : Rd0 ! Rdh of the form

p✓(x) = Wh⇢rWh1⇢r . . .⇢ rW1x  Wi 2 Rdi⇥di1 

where the activation ⇢r(z) raises all elements of z to the r-th power (r 2 N). The parameters
✓ = (Wh  . . .   W1) 2 Rd✓ (with d✓ =Ph
i=1 didi1) are the network’s weights  and the network’s
architecture is encoded by the sequence d = (d0  . . .   dh) (specifying the depth h and widths
di). Clearly  p✓ is a homogeneous polynomial mapping Rd0 ! Rdh of degree rh1  i.e.  p✓ 2
Symrh1(Rd0)dh.
For ﬁxed degree r and architecture d = (d0  . . .   dh)  there exists an algebraic map

d r : ✓ 7! p✓ =264

p✓1
...

p✓dh+1

375  

(1)

where each p✓i is a polynomial in d0 variables. The image of d r is a set of vectors of polynomials 
i.e.  a subset Fd r of Symrh1(Rd0)dh  and it is the functional space represented by the network. In
this paper  we consider the “Zariski closure” Vd r = Fd r of the functional space.1 We refer to Vd r
as functional variety of the network architecture  as it is in fact an irreducible algebraic variety. In
particular  Vd r can be studied using powerful machinery from algebraic geometry.
Remark 1. The functional variety Vd r may be signiﬁcantly larger than the actual functional space
Fd r  since the Zariski closure is typically larger than the closure with respect to the standard the
Euclidean topology. On the other hand  the dimensions of the spaces Vd r and Fd r agree  and the
set Vd r is usually “nicer” (it can be described by polynomial equations  whereas an exact implicit
description of Fd r may require inequalities).
2.1 Examples
We present some examples that describe the functional variety Vd r in simple cases.
Example 2. A linear network is a polynomial network with r = 1. In this case  the network map
d r : Rd✓ ! Sym1(Rd0)dh ⇠= Rdh⇥d0 is simply matrix multiplication:

✓ = (Wh  Wh1  . . .   W1) 7! p✓ = WhWh1 . . . W1x.

The functional space Fd r ✓ Rdh⇥d0 is the set of matrices with rank at most dmin = mini{di}. This
set is already characterized by polynomial equations  as the common zero set of all (1 + dmin)⇥ (1 +
dmin) minors  so Fd r = Vd r in this case. The dimension of Vd r ⇢ Rdh⇥d0 is dmin(d0 + dh dmin).
Example 3. Consider d = (2  2  3) and r = 2. The input variables are x = [x1  x2]T   and the
parameters ✓ are the weights

W1 =w111 w112

w121 w122   W2 =264

w211 w212
w221 w222
w231 w232

375 .

1The Zariski closure of a set X is the smallest set containing X that can be described by polynomial

equations.

3

det2664

c(1)
11
c(2)
11
c(3)
11

c(1)
12
c(2)
12
c(3)
12

c(1)
22
c(2)
22
c(3)
22

3775 = 0.

The network map p✓ is a triple of quadratic polynomials in x1  x2  that can be written as

W2⇢2W1x = 264

w211(w111x1 + w112x2)2 + w212(w121x1 + w122x2)2
w221(w111x1 + w112x2)2 + w222(w121x1 + w122x2)2
w231(w111x1 + w112x2)2 + w232(w121x1 + w122x2)2

375 .

(2)

The map (2 2 3) 2 in (1) takes W1  W2 (that have d✓ = 10 parameters) to the three quadratics in
x1  x2 displayed above. The quadratics have a total of dim Sym2(R2)3 = 9 coefﬁcients  however
these coefﬁcients are not arbitrary  i.e.  not all possible triples of polynomials occur in the functional
space. Writing c(k)
for the coefﬁcient of xixj in p✓k in (2) (with k = 1  2  3 i  j = 1  2) then it is a
ij
simple exercise to show that

This cubic equation describes the functional variety V(2 3 3) 2  which is in this case an eight-
dimensional subset (hypersurface) of Sym2(R2)3 ⇠= R9.
2.2 Objectives
The main goal of this paper is to study the dimension of Vd r as the network’s architecture d and
the activation degree r vary. This dimension may be considered a precise and intrinsic measure of
the polynomial network’s expressivity  quantifying degrees of freedom of the functional space. For
example  the dimension reﬂects the number of input/output pairs the network can interpolate  as each
sample imposes one linear constraint on the variety Vd r.
In general  the variety Vd r lives in the ambient space Symrh1(Rd0)dh  which in turn only depends
on the activation degree r  network depth h  and the input/output dimensions d0 and dh. We are thus
interested in the role of the intermediate widths in the dimension of Vd r.
Deﬁnition 4. A network architecture d = (d0  . . .   dh) has a ﬁlling functional variety for the
activation degree r if Vd r = Symrh1(Rd0)dh.
It is important to note that if the functional variety Vd r is ﬁlling  then actual functional space
Fd r (before taking closure) is in general only thick  i.e.  it has positive Lebesgue measure in
Symrh1(Rd0)dh (see Remark 1). On the other hand  given an architecture with a thick functional
space  we can ﬁnd another architecture whose functional space is the whole ambient space.
Proposition 5 (Filling functional space). Fix r and suppose d = (d0  d1  . . .   dh1  dh) has a ﬁlling
functional variety Vd r. Then the architecture d0 = (d0  2d1  . . .   2dh1  dh) has a ﬁlling functional
space  i.e.  Fd0 r = Symrh1(Rd0)dh.
In summary  while an architecture with a ﬁlling functional variety may not necessarily have a ﬁlling
functional space  it is sufﬁcient to double all the intermediate widths for this stronger condition to
hold. As argued below  we expect architectures with thick/ﬁlling functional spaces to have more
favorable properties in terms of optimization and training. On the other hand  non-ﬁlling architectures
may lead to interesting functional spaces for capturing patterns in data. In fact  we show in Section 3.2
that non-ﬁlling architectures generalize families of low-rank tensors.

2.3 Connection to optimization

The following two results illustrate that thick/ﬁlling functional spaces are helpful for optimization.
Proposition 6. If the closure of a set C ⇢ Rn is not convex  then there exists a convex function f
on Rn whose restriction to C has arbitrarily “bad” local minima (that is  there exist local minima
whose value is arbitrarily larger than that of a global minimum).
Proposition 7. If a functional space Fd r is not thick  then it is not convex.

4

These two facts show that if the functional space is not thick  we can always ﬁnd a convex loss
function and a data distribution that lead to a landscape with arbitrarily bad local minima. There is
also an obvious weak converse  namely that if the functional space is ﬁlling Fd r = Symrh1(Rd0)dh 
then any convex loss function Fd r will have a unique global minimum (although there may be
“spurious” critical points that arise from the non-convex parameterization).

3 Architecture dimensions

In this section  we begin our study of the dimension of Vd r. We describe the connection between
polynomial networks and tensor decompositions for both shallow (Section 3.1) and deep (Section 3.2)
networks  and we present some computational examples (Section 3.3).

3.1 Shallow networks and tensors

Polynomial networks with h = 2 are closely related to CP tensor decomposition [22]. Indeed in the
shallow case  we can verify the network map (d0 d1 d2) r sends W1 2 Rd1⇥d0  W2 2 Rd2⇥d1 to:

W2⇢rW1x = ⇣ d1Xi=1

W2(:  i) ⌦ W1(i  :)⌦r⌘ · x⌦r =: (W2  W1) · x⌦r.

0

Here (W2  W1) 2 Rd2 ⇥ Symr(Rd0) is a partially symmetric d2 ⇥ d⇥r
tensor  expressed as a sum
of d1 partially symmetric rank 1 terms  and · denotes contraction of the last r indices. Thus the
functional space F(d0 d1 d2) r is the set of rank  d1 partially symmetric tensors. Algorithms for
low-rank CP decomposition could be applied to (W2  W1) to recover W2 and W1. In particular 
when d2 = 1  we obtain a symmetric d⇥r
0
Lemma 8. A shallow architecture d = (d0  d1  1) is ﬁlling for the activation degree r if and only if
every symmetric tensor T 2 Symr(Rd0) has rank at most d1.
Furthermore  the celebrated Alexander-Hirschowitz Theorem [1] from algebraic geometry provides
the dimension of Vd r for all shallow  single-output architectures.
Theorem 9 (Alexander-Hirschowitz). If d = (d0  d1  1)  the dimension of Vd r is given by

tensor. For this case  we have the following.

min⇣d0d1 d0+r1

r

⌘  except for the following cases:

• r = 2  2  d1  d0  1 
• r = 3  d0 = 5  d1 = 7 
• r = 4  d0 = 3  d1 = 5 
• r = 4  d0 = 4  d1 = 9 
• r = 4  d0 = 5  d1 = 15.
3.2 Deep networks and tensors

Deep polynomial networks also relate to a certain iterated tensor decomposition. We ﬁrst note the
map d r may be expressed via the so-called Khatri-Rao product from multilinear algebra. Indeed ✓
maps to:

SymRow Wh((Wh1 . . . (W2(W •r

(3)
Here the Khatri-Rao product operates on rows: for M 2 Ra⇥b  the power M•r 2 Ra⇥br replaces
each row  M (i  :)  by its vectorized r-fold outer product  vec(M (i  :)⌦r). Also in (3)  SymRow
denotes symmetrization of rows  regarded as points in (Rd0)⌦rh1  a certain linear operator.
Another viewpoint comes from using polynomials and inspecting the layers in reverse order. Writing
[p✓1  . . .   p✓dh1]T for the output polynomials at depth h  1  the top output at depth h is:

1 ))•r . . . )•r).

wh11 pr

✓1 + wh12 pr

✓2 + . . . + wh1dh1 pr

✓dh1.

(4)

5

rhi

This expresses a polynomial as a weighted sum of r-th powers of other (nonlinear) polynomials.
Recently  a study of such decompositions has been initiated in the algebra community [24]. Such
expressions extend usual tensor decompositions  since weighted sums of powers of homogeneous
linear forms correspond to CP symmetric decompositions. Accounting for earlier layers  our neural
network expresses each p✓i in (4) as r-th powers of lower-degree polynomials at depth h 2  so forth.
Iterating the main result in [16] on decompositions of type (4)  we obtain the following bound on
ﬁlling intermediate widths.
Theorem 10 (Bound on ﬁlling widths). Suppose d = (d0  d1  . . .   dh) and r  2 satisfy

dhi  min✓dh · ri(d01) ✓rhi + d0  1
◆◆
for each i = 1  . . .   h  1. Then the functional variety Vd r is ﬁlling.
3.3 Computational investigation of dimensions
We have written code2 in the mathematical software SageMath [12] that computes the dimension
of Vd r for a general architecture d and activation degree r. Our approach is based on randomly
selecting parameters ✓ = (Wh  . . .   W1) and computing the rank of the Jacobian of d r(✓) in (1).
This method is based on the following lemma  coming from the fact that the map d r is algebraic.
Lemma 11. For all ✓ 2 Rd✓  the rank of the Jacobian matrix Jac d r(✓) is at most the dimension
of the variety Vd r. Furthermore  there is equality for almost all ✓ (i.e.  for a non-empty Zariski-open
subset of Rd✓).
Thus if Jac d r(✓) is full rank at any ✓  this witnesses a mathematical proof Vd r is ﬁlling. On the
other hand if the Jacobian is rank-deﬁcient at random ✓  this indicates with “probability 1" that Vd r
is not ﬁlling. We have implemented two variations of this strategy  by leveraging backpropagation.
Both work over a ﬁnite ﬁeld F = Z/pZ to avoid ﬂoating-point computations (for almost all primes p 
this provides the correct dimension over R).

1. Backpropagation over a polynomial ring. We deﬁned a network class over a ring
F[x1  . . .   xd0]  taking as input a vector variables x = (x1  . . .   xd0). Performing automatic
differentiation (backpropagation) of the output function yields polynomials corresponding
to dp✓(x)/dw  for any entry w of a weight matrix Wi. Extracting the coefﬁcients of the
monomials in x  we recover the entries of the Jacobian of d r(✓).

2. Backpropagation over a ﬁnite ﬁeld. We deﬁned a network class over the ﬁnite ﬁeld F =
Z/pZ. After performing backpropagation at a sufﬁcient number of random sample points x 
we can recover the entries of the Jacobian of d r(✓) by solving a linear system (this system
is overdetermined  but it will have an exact solution in ﬁnite ﬁeld arithmetic).

The ﬁrst algorithm is simpler and does not require interpolation  but is generally slower. We present
examples of some of our computations in Tables 1 and 2. Table 1 shows minimal architectures
d = (d0  . . .   dh) that are ﬁlling  as the depth h varies. Here  “minimal” is with respect to the partial
ordering comparing all widths. It is interesting to note that for deeper networks  there is not a unique

2Available at https://github.com/mtrager/polynomial_networks.

Table 1: Minimal ﬁlling widths for r = 2  d0 = 2  dh = 1

Depth (h) Degree (rh1)

Minimal ﬁlling (d)

3
4
5
6
7
8
9

4
8
16
32
64
128
256

(2 2 2 1)
(2 3 3 2 1)
(2 3 3 3 2 1)
(2 3 3 4 4 2 1)
(2 3 4 5 6 4 2 1)

(2 3 4 5 7 7 6 2 1) or (2 3 5 5 7 7 5 2 1)

(2 3 4 8 8 8 8 8 4 1) or (2 3 4 5 8 9 8 8 4 1)

6

Table 2: Examples of dimensions of Vd r
r = 5

r = 3

r = 4

r = 2

d = (3  2  1)
d = (2  3  2)
d = (2  3  2  3)
d = (2  3  2  3  4)

5
6
10
16

6
8
12
21

6
9
13
22

6
9
13
22

r = 6

6
9
13
22

minimally ﬁlling network. Also conspicuous is that minimal ﬁlling widths are “unimodal"  (weakly)
increasing and then (weakly) decreasing. Arguably  this pattern conforms with common wisdom.
Conjecture 12 (Minimal ﬁlling widths are unimodal). Fix r  h  d0 and dh. If d = (d0  d1  . . .   dh)
is a minimal ﬁlling architecture  there is i such that d0  d1  . . .  di and di  di+1  . . .  dh.
Table 2 shows examples of computed dimensions  for varying architectures and degrees. Notice that
the dimension of an architecture stabilizes as the degree r increases.

4 General results
This section presents general results on the dimension of Vd r. We begin by pointing out symmetries
in the network map d r  under suitable scaling and permutation.
Lemma 13 (Multi-homogeneity). For arbitrary invertible diagonal matrices Di 2 Rdi⇥di and
permutation matrices Pi 2 Zdi⇥di (i = 1  . . .   h  1)  the map d r returns the same output under
the replacement:

W1 P1D1W1
W2 P2D2W2Dr
W3 P3D3W3Dr

1 P T
1
2 P T
2

...

Wh WhDr

h1P T

h1.

Thus the dimension of a generic ﬁber (pre-image) of d r is at leastPh1
Our next result deduces a general upper bound on the dimension of Vd r. Conditional on a standalone
conjecture in algebra  we prove that equality in the bound is achieved for all sufﬁciently high activation
degrees r. An unconditional result is achieved by varying the activation degrees per layer.
Theorem 14 (Naive bound and equality for high activation degree). If d = (d0  . . .   dh)  then

i=1 di.

dimVd r  min dh +

hXi=1

(di1  1)di  dh✓d0 + rh1  1

rh1

◆! .

(5)

Conditional on Conjecture 16  for ﬁxed d satisfying di > 1 (i = 1  . . .   h  1)  there exists ˜r = ˜r(d)
such that whenever r > ˜r  we have an equality in (5). Unconditionally  for ﬁxed d satisfying
di > 1 (i = 1  . . .   h  1)  there exist inﬁnitely many (rh1  rh2  . . .   r1) such that the image of
(Wh  . . .   W1) 7! Wh⇢rh1Wh1⇢rh2 . . .⇢ 1W1x has dimension dh +Pi(di1  1)di.
Proposition 15. Given positive integers d  k  s  there exists ˜r = ˜r(d  k  s) with the following property.
Whenever p1  . . .   pk 2 R[x1  . . .   xd] are k homogeneous polynomials of the same degree s in d
variables  no two of which are linearly dependent  then pr
k are linearly independent if r > ˜r.

1  . . .   pr

Conjecture 16. In the setting of Proposition 15  ˜r may be taken to depend only on d and k.

Proposition 15 and Conjecture 16 are used in induction on h for the equality statements in Theorem 14.
We remark that following our arXiv version of this paper  progress toward Conjecture 16 was made

7

in [30]. There  it is shown that there exists r between 1 and k! such that pr
independent; however  it remains open whether there exists ˜r as we conjecture.
The next result uses the iterative nature of neural networks to provide a recursive dimension bound.
Proposition 17 (Recursive Bound). For all (d0  . . .   dk  . . .   dh) and r  we have:
dimV(d0 ... dh) r  dimV(d0 ... dk) r + dimV(dk ... dh) r  dk.

k are linearly

1  . . .   pr

Using the recursive bound  we can prove an interesting bottleneck property for polynomial networks.
Deﬁnition 18. The width di in layer i is an asymptotic bottleneck (for r  d0 and i) if there exists ˜h
such that for all h > ˜h and all d1  . . .   di1  di+1  . . .   dh  then the widths (d0  d1  . . .   di  . . .   dh)
are non-ﬁlling.

This expresses our ﬁnding that too narrow a layer can “choke" a polynomial network  such that there
is no hope of ﬁlling the ambient space  regardless of how wide elsewhere or how deep the network is.
Theorem 19 (Bottlenecks). If r  2  d0  2  i  1  then di = 2d0  2 is an asymptotic bottleneck.
Moreover conditional on Conjecture 2 in [28]  then di = 2d0 is not an asymptotic bottleneck.
Proposition 17 affords a simple proof that di = d0 1 is an asymptotic bottleneck. However to obtain
the full statement of Theorem 19  we seem to need more powerful tools from algebraic geometry.

5 Conclusion

We have studied the functional space of neural networks from a novel perspective. Deep polynomial
networks furnish a framework for nonlinear networks  to which the powerful mathematical machinery
of algebraic geometry may be applied. In this respect  we believe polynomial networks can help us
access a better understanding of deep nonlinear architectures  for which a precise theoretical analysis
has been extremely difﬁcult to obtain. Furthermore  polynomials can be used to approximate any
continuous activation function over any compact support (Stone-Weierstrass theorem). For these
reasons  developing a theory of deep polynomial networks is likely to pay dividends in building
understanding of general neural networks.
In this paper  we have focused our attention on the dimension of the functional space of polynomial
networks. The dimension is the ﬁrst and most basic descriptor of an algebraic variety  and in this
context it provides an exact measure of the expressive power of an architecture. Our novel theoretical
results include a general formula for the dimension of the architecture attained in high degree  as well
as a tight lower bound and nontrivial upper bounds on the width of layers in order for the functional
variety to be ﬁlling. We have also demonstrated intriguing connections with tensor and polynomial
decompositions  including some which appear in very recent literature in algebraic geometry.
The tools and concepts introduced in this work for fully connected feedforward polynomial networks
can be applied in principle to more general algebraic network architectures. Variations of our algebraic
model could include multiple polynomial activations (rather than just single exponentiations) or
more complex connectivity patterns of the network (convolutions  skip connections  etc.). The
functional varieties of these architectures could be studied in detail and compared. Another possible
research direction is a geometric study of the functional varieties  beyond the simple dimension. For
example  the degree or the Euclidean distance degree [13] of these varieties could be used to bound
the number of critical points of a loss function. Additionally  motivated by Section 3.2  we would
like to develop computational methods for constructing a network architecture that represents an
assigned polynomial mapping. Such algorithms might lead to “closed form” approaches for learning
using polynomial networks (similar to SVD or tensor decomposition)  as a provable counterpoint to
gradient descent methods. Our research program might also shed light on the practical problem of
choosing an appropriate architecture for a given application.

Acknowledgements

We thank Justin Chen  Amit Moscovich  Claudiu Raicu and Steven Sam for helpful conversations. JK
was partially supported by the Simons Collaboration on Algorithms and Geometry. MT and JB were
partially supported by the Alfred P. Sloan Foundation  NSF RI-1816753 and Samsung Electronics.

8

References
[1] James Alexander and André Hirschowitz. Polynomial interpolation in several variables. Journal

of Algebraic Geometry  4(2):201–222  1995.

[2] Sanjeev Arora  Nadav Cohen  Noah Golowich  and Wei Hu. A convergence analysis of
gradient descent for deep linear neural networks. In International Conference on Learning
Representations  2019.

[3] Sanjeev Arora  Nadav Cohen  and Elad Hazan. On the optimization of deep networks: implicit
acceleration by overparameterization. In International Conference on Machine Learning  pages
244–253  2018.

[4] Pranav Bisht. On hitting sets for special depth-4 circuits. Master’s thesis  Indian Institute of

Technology Kanpur  2017.

[5] Grigoriy Blekherman and Zach Teitler. On maximum  typical and generic ranks. Mathematische

Annalen  362(3-4):1021–1031  2015.

[6] Winfried Bruns and Jürgen Herzog. Cohen-Macaulay rings  volume 39 of Cambridge Studies

in Advanced Mathematics. Cambridge University Press  Cambridge  1993.

[7] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems  pages 3036–3046  2018.

[8] Nadav Cohen  Or Sharir  and Amnon Shashua. On the expressive power of deep learning: a

tensor analysis. In Conference on Learning Theory  pages 698–728  2016.

[9] Nadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor

decompositions. In International Conference on Machine Learning  pages 955–963  2016.

[10] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of

Control  Signals and Systems  2(4):303–314  1989.

[11] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in

Neural Information Processing Systems  pages 666–674  2011.

[12] The Sage Developers. SageMath  the Sage Mathematics Software System (Version 8.0.0)  2017.

http://www.sagemath.org.

[13] Jan Draisma  Emil Horobe¸t  Giorgio Ottaviani  Bernd Sturmfels  and Rekha R. Thomas. The
Euclidean distance degree of an algebraic variety. Foundations of Computational Mathematics 
16(1):99–149  2016.

[14] Simon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International Conference on Machine Learning  pages 1329–1338 
2018.

[15] David Eisenbud. Commutative algebra: with a view toward algebraic geometry  volume 150 of

Graduate Texts in Mathematics. Springer-Verlag  New York  1995.

[16] Ralf Fröberg  Giorgio Ottaviani  and Boris Shapiro. On the Waring problem for polynomial

rings. Proceedings of the National Academy of Sciences  109(15):5600–5602  2012.

[17] Joe Harris. Algebraic geometry: a ﬁrst course  volume 133 of Graduate Texts in Mathematics.

Springer-Verlag  New York  corrected 3rd print edition  1995.

[18] Robin Hartshorne. Algebraic geometry  volume 52 of Graduate Texts in Mathematics. Springer-

Verlag  New York-Heidelberg  corrected 8th print edition  1997.

[19] Kurt Hornik  Maxwell Stinchcombe  and Halbert White. Multilayer feedforward networks are

universal approximators. Neural Networks  2(5):359–366  1989.

[20] Hamza Jaffali and Luke Oeding. Learning algebraic models of quantum entanglement. arXiv

preprint arXiv:1908.10247  2019.

9

[21] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  pages 586–594  2016.

[22] J. M. Landsberg. Tensors: geometry and applications  volume 128 of Graduate Studies in

Mathematics. American Mathematical Society  Providence  RI  2012.

[23] Moshe Leshno  Vladimir Ya. Lin  Allan Pinkus  and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural
Networks  6(6):861–867  1993.

[24] Samuel Lundqvist  Alessandro Oneto  Bruce Reznick  and Boris Shapiro. On generic and
maximal k-ranks of binary forms. Journal of Pure and Applied Algebra  223(5):2062 – 2079 
2019.

[25] James Martens and Venkatesh Medabalimi. On the expressive efﬁciency of sum product

networks. arXiv preprint arXiv:1411.7717  2014.

[26] Dhagash Mehta  Tianran Chen  Tingting Tang  and Jonathan D. Hauenstein. The loss surface of
deep linear networks viewed through the algebraic geometry lens. arXiv:1810.07716  2018-10-
17.

[27] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences  115(33):7665–
7671  2018.

[28] Lisa Nicklasson. On the Hilbert series of ideals generated by generic forms. Communications

in Algebra  45(8):3390–3395  2017.

[29] Hoifung Poon and Pedro Domingos. Sum-product networks: a new deep architecture. arXiv

preprint arXiv:1202.3732  2012.

[30] Steven V. Sam and Andrew Snowden. Linear independence of power. arXiv preprint

arXiv:1907.02659  2019.

[31] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D. Lee. Theoretical insights into the op-
timization landscape of over-parameterized shallow neural networks. IEEE Transactions on
Information Theory  65(2):742–769  2019.

[32] Matthew Trager  Kathlén Kohn  and Joan Bruna. Pure and spurious critical points: a geometric

study of linear networks. arXiv preprint arXiv:1910.01671  2019.

[33] Luca Venturi  Afonso S. Bandeira  and Joan Bruna. Spurious valleys in two-layers neural

network optimization landscapes. arXiv preprint arXiv:1802.06384  2018.

10

,Joe Kileel
Matthew Trager
Joan Bruna