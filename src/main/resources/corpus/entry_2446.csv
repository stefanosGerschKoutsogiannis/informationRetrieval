2019,Hierarchical Optimal Transport for Document Representation,The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative  we introduce hierarchical optimal transport as a meta-distance between documents  where documents are modeled as distributions over topics  which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance  and we relate it to the word mover's distance. 
We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.,Hierarchical Optimal Transport
for Document Representation

Mikhail Yurochkin1 3

mikhail.yurochkin@ibm.com

Sebastian Claici2 3
sclaici@mit.edu

Edward Chien2 3

edchien@mit.edu

Farzaneh Mirzazadeh1 3
farzaneh@ibm.com

Justin Solomon2 3

jsolomon@mit.edu

IBM Research 1 MIT CSAIL 2 MIT-IBM Watson AI Lab3

Abstract

The ability to measure similarity between documents enables intelligent summa-
rization and analysis of large corpora. Past distances between documents suffer
from either an inability to incorporate semantic similarities between words or from
scalability issues. As an alternative  we introduce hierarchical optimal transport
as a meta-distance between documents  where documents are modeled as distribu-
tions over topics  which themselves are modeled as distributions over words. We
then solve an optimal transport problem on the smaller topic space to compute a
similarity score. We give conditions on the topics under which this construction
deﬁnes a distance  and we relate it to the word mover’s distance. We evaluate our
technique for k-NN classiﬁcation and show better interpretability and scalability
with comparable performance to current methods at a fraction of the cost.1

1

Introduction

Topic models like latent Dirichlet allocation (LDA) (Blei et al.  2003) are major workhorses for
summarizing document collections. Typically  a topic model represents topics as distributions over
the vocabulary (i.e.  unique words in the corpus); documents are then modeled as distributions over
topics. In this approach  words are vertices of a simplex whose dimension equals the vocabulary size
and for which the distance between any pair of words is the same. More recently  word embeddings
map words into high-dimensional space such that co-occurring words tend to be closer to each other
than unrelated words (Mikolov et al.  2013; Pennington et al.  2014). Kusner et al. (2015) combine
the geometry of word embedding space with optimal transport to propose the word mover’s distance
(WMD)  a powerful document distance metric limited mostly by computational complexity.
As an alternative to WMD  in this paper we combine hierarchical latent structures from topic models
with geometry from word embeddings. We propose hierarchical optimal topic transport (HOTT)
document distances  which combine language information from word embeddings with corpus-
speciﬁc  semantically-meaningful topic distributions from latent Dirichlet allocation (LDA) (Blei
et al.  2003). This document distance is more efﬁcient and more interpretable than WMD.
We give conditions under which HOTT gives a metric and show how it relates to WMD. We test
against existing metrics on k-NN classiﬁcation and show that it outperforms others on average. It
performs especially well on corpora with longer documents and is robust to the number of topics and
word embedding quality. Additionally  we consider two applications requiring pairwise distances.

1Code: https://github.com/IBM/HOTT

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The ﬁrst is visualization of the metric with t-SNE (van der Maaten & Hinton  2008). The second is
link prediction from a citation network  cast as pairwise classiﬁcation using HOTT features.
Contributions. We introduce hierarchical optimal transport to measure dissimilarities between
distributions with common structure. We apply our method to document classiﬁcation  where topics
from a topic modeler represent the shared structure. Our approach
• is computationally efﬁcient  since HOTT distances involve transport with small numbers of sites;
• uses corpus-speciﬁc topic and document distributions  providing higher-level interpretability;
• has comparable performance to WMD and other baselines for k-NN classiﬁcation; and
• is practical in applications where all pairwise document distances are needed.

2 Related work

Document representation and similarity assessment are key applications in learning. Many methods
are based on the bag-of-words (BOW)  which represents documents as vectors in R|V |  where |V | is
the vocabulary size; each coordinate equals the number of times a word appears. Other weightings
include term frequency inverse document frequency (TF-IDF) (Luhn  1957; Spärck Jones  1972) and
latent semantic indexing (LSI) (Deerwester et al.  1990). Latent Dirichlet allocation (LDA) (Blei
et al.  2003) is a hierarchical Bayesian model where documents are represented as admixtures of
latent topics and admixture weights provide low-dimensional representations. These representations
equipped with the l2 metric comprise early examples of document dissimilarity scores.
Recent document distances employ more sophisticated methods. WMD incorporates word embed-
dings to account for word similarities (Kusner et al.  2015) (see §3). Huang et al. (2016) extend
WMD to the supervised setting  modifying embeddings so that documents in the same class are close
and documents from different classes are far. Due to computational complexity  these approaches are
impractical for large corpora or documents with many unique words.
Wu & Li (2017) attempt to address the complexity of WMD via a topic mover’s distance (TMD).
While their k-NN classiﬁcation results are comparable to WMD  they use signiﬁcantly more topics 
generated with a Poisson inﬁnite relational model. This reduces semantic content and interpretability 
with less signiﬁcant computational speedup. They also do not leverage language information from
word embeddings or otherwise. Xu et al. (2018) jointly learn topics and word embeddings  limiting
the complexity to under a hundred words  which is not suited for natural language processing.
Wu et al. (2018) approximate WMD using a random feature kernel. In their method  the WMD from
corpus documents to a selection of random short documents facilitates approximation of pairwise
WMD. The resulting word mover’s embedding (WME) has similar performance with signiﬁcant
speedups. Their method  however  requires parameter tuning in selecting the random document set
and lacks topic-level interpretability. Additionally  they do not show full-metric applications. Lastly 
Wan (2007)  whose work predates (Kusner et al.  2015)  applies transport to blocks of text.

3 Background

Discrete optimal transport. Optimal transport (OT) is a rich theory; we only need a small part and
refer the reader to (Villani  2009; Santambrogio  2015) for mathematical foundations and to (Peyré &
Cuturi  2018; Solomon  2018) for applications. Here  we focus on discrete-to-discrete OT.
Let x = {x1  . . .   xn} and y = {y1  . . .   ym} be two sets of points (sites) in a metric space. Let
∆n ⊂ Rn+1 denote the probability simplex on n elements  and let p ∈ ∆n and q ∈ ∆m be
distributions over x and y. Then  the 1-Wasserstein distance between p and q is

(cid:26) minΓ∈Rn×m

(cid:80)
j Γi j = pi and (cid:80)
subject to (cid:80)

i j Ci jΓi j

+

W1(p  q) =

(1)
where the cost matrix C has entries Ci j = d(xi  yj)  where d(· ·) denotes the distance. The
constraints allow Γ to be interpreted as a transport plan or matching between p and q. The linear
program (1) can be solved using the Hungarian algorithm (Kuhn  1955)  with complexity O(l3 log l)
where l = max(n  m). While entropic regularization can accelerate OT in learning environments
(Cuturi  2013)  it is most successful when the support of the distributions is large as it has complexity

i Γi j = qj 

2

j is the count of word vj in di and(cid:12)(cid:12)di(cid:12)(cid:12) is the number of words

O(l2/ε2). In our case  the number of topics in each document is small  and the linear program is
typically faster if we need an accurate solution (i.e. if ε is small).
Word mover’s distance. Given an embedding of a vocabulary as V ⊂ Rn  the Euclidean metric puts
a geometry on the words in V . A corpus D = {d1  d2  . . . d|D|} can be represented using distributions
over V via a normalized BOW. In particular  di ∈ ∆li  where li is the number of unique words in a
document di  and di
in di. The WMD between documents d1 and d2 is then WMD(d1  d2) = W1(d1  d2).
The complexity of computing WMD depends heavily on l = max(l1  l2); for longer documents 
l may be a signiﬁcant fraction of |V |. To evaluate the full metric on a corpus  the complexity
is O(|D|2l3 log l)  since WMD must be computed pairwise. Kusner et al. (2015) test WMD for
k-NN classiﬁcation. To circumvent complexity issues  they introduce a pruning procedure using a
relaxed word mover’s distance (RWMD) to lower-bound WMD. On the larger 20NEWS dataset  they
additionally remove infrequent words by using only the top 500 words to generate a representation.

j/|di|  where ci

j = ci

4 Hierarchical optimal transport
Assume a topic model produces corpus-speciﬁc topics T = {t1  t2  . . .   t|T|} ⊂ ∆|V |  which are
distributions over words  as well as document distributions ¯di ∈ ∆|T| over topics. WMD deﬁnes a
metric WMD(ti  tj) between topics; we consider discrete transport over T as a metric space.
We deﬁne the hierarchical topic transport distance (HOTT) between documents d1 and d2 as

 |T|(cid:88)

|T|(cid:88)

  

HOTT (d1  d2) = W1

¯d1
kδtk  

¯d2
kδtk

k=1

k=1

where each Dirac delta δtk is a probability distribution supported on the corresponding topic tk and
where the ground metric is WMD between topics as distributions over words. The resulting transport
problem leverages topic correspondences provided by WMD in the base metric. This explains the
hierarchical nature of our proposed distance.
Our construction uses transport twice: WMD provides topic distances  which are subsequently the
costs in the HOTT problem. This hierarchical structure greatly reduces runtime  since |T| (cid:28) l; the
costs for HOTT can be precomputed once per corpus. The expense of evaluating pairwise distances
is drastically lower  since pairwise distances between topics may be precomputed and stored. Even as
document length and corpus size increase  the transport problem for HOTT remains the same size.
Hence  full metric computations are feasible on larger datasets with longer documents.
When computing WMD(ti  tj)  we reduce computational time by truncating topics to a small amount
of words carrying the majority of the topic mass and re-normalize. This procedure is motivated by
interpretability considerations and estimation variance of the tail probabilities. On the interpretability
side  LDA topics are often displayed using a few dozen top words  providing a human-understandable
tag. Semantic coherence  a popular topic modeling evaluation metric  also is based on heavily-
weighted words and was demonstrated to align with human evaluation of topic models (Newman
et al.  2010). Moreover  any topic modeling inference procedure  e.g. Gibbs sampling (Grifﬁths &
Steyvers  2004)  has estimation variance that may dominate tail probabilities  making them unreliable.
Hence  we truncate to the top 20 words when computing WMD between topics. We empirically
verify that truncation to any small number of words performs equally well in §5.3.
In topic models  documents are assumed to be represented by a small subset of topics of size
κi (cid:28) |T| (e.g.  in Figure 1  books are majorly described by three topics)  but in practice document
topic proportions tend to be dense with little mass outside of the dominant topics. Williamson et al.
(2010) propose an LDA extension enforcing sparsity of the topic proportions  at the cost of slower
inference. When computing HOTT  we simply truncate LDA topic proportions at 1/(|T| + 1)  the
value below LDA’s uniform topic proportion prior  and re-normalize. This reduces complexity of our
approach without performance loss as we show empirically in §5.2 and §5.3.
Metric properties of HOTT. If each document can be uniquely represented as a linear combination
¯di
ktk  and each topic is unique  then HOTT is a metric on document space. We

of topics di =(cid:80)|T|

k=1

present a brief proof in the supplementary material.

3

Figure 1: Topic transport interpretability. We show two books from GUTENBERG and their heaviest-
weighted topics (bolded topic names are manually assigned). The ﬁrst involves steamship warfare 
while the second involves biology. Left and right column percentages indicate the weights of the
topics in the corresponding texts. Percentages labeling the arrows indicate the transported mass
between the corresponding topics  which match semantically-similar topics.

Topic-level interpretability. The additional level of abstraction promotes higher-level interpretability
at the level of topics as opposed to dense word-level correspondences from WMD. We provide an
example in Figure 1. This diagram illustrates two books from the GUTENBERG dataset and the
semantically meaningful transport between their three most heavily-weighted topics. Remaining
topics and less prominent transport terms account for the remainder of the transport plan not illustrated.
Relation to WMD. First we note that if |T| = |V | and topics consist of single words covering the
vocabulary  then HOTT becomes WMD. In well-behaved topic models  this is expected as |T| → |V |.
Allowing |T| to vary produces different levels of granularity for our topics as well as a trade-off
between computational speed and topic speciﬁcity. When |T| (cid:28) |V |  we argue that WMD is upper
bounded by HOTT and two terms that represent topic modeling loss. By the triangle inequality 

+W1

|T|(cid:88)

|T|(cid:88)

+W1

|T|(cid:88)

¯di
ktk 

¯dj
ktk

¯dj
ktk  dj

(2)

k=1

k=1

k=1

.

di 

WMD(di  dj) ≤ W1

|T|(cid:88)
LDA inference minimizes KL(di(cid:107)(cid:80)|T|

k=1

¯di
ktk

k=1

(cid:113) 1
ktk) over topic proportions ¯di for a given document di;
¯di
hence  we look to relate Kullback–Leibler divergence to W1. In ﬁnite-diameter metric spaces 
2KL(µ(cid:107)ν)  which follows from inequalities relating Wasserstein distances
W1(µ  ν) ≤ diam(X)
 |T|(cid:88)
to KL divergence (Otto & Villani  2000). The middle term satisﬁes the following inequality:

|T|(cid:88)

|T|(cid:88)

¯di
kδtk  

¯di
ktk 

W1

(3)

¯dj
kδtk

¯dj
ktk

k=1

k=1

k=1

where on the right we have HOTT (d1  d2). The optimal topic transport on the right implies an
equal-cost transport of the corresponding linear combinations of topic distributions on the left. The
inequality follows since W1 gives the optimal transport cost. Combining into a single inequality 

WMD(di  dj) ≤ HOTT (di  dj)+diam(X)

¯dj
ktk

KL

¯di
ktk

 ≤ W1
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116) 1


KL

2

k=1

 |T|(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) |T|(cid:88)

dj

k=1

  
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116) 1

2

 +

di

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) |T|(cid:88)

k=1

 .


WMD involves a large transport problem and Kusner et al. (2015) propose relaxed WMD (RWMD) 
a relaxation via a lower bound (see also Atasu & Mittelholzer (2019) for a GPU-accelerated variant).
We next show that RWMD is not always a good lower bound on WMD.

4

RWMD–Hausdorff bound. Consider the optimization in (1) for calculating WMD(d1  d2)  and
remove the marginal constraint on d2. The resulting optimal Γ is no longer a transport plan  but
rather moves mass on words in d1 to their nearest words in d2  only considering the support of d2
and not its density values. Removing the marginal constraint on d1 produces symmetric behavior;
RWMD(d1  d2) is deﬁned to be the larger cost of these relaxed problems.
Suppose that word vj is shared by d1 and d2. Then  the mass on vj in d1 and d2 in each re-
laxed problems will not move and contributes zero cost. In the worst case  if d1 and d2 contain

the same words  i.e.  supp(cid:0)d1(cid:1) = supp(cid:0)d2(cid:1)  then RWMD(d1  d2) = 0. More generally  the

closer the supports of two documents (over V )  the looser RWMD might be as a lower bound.

Figure 2 illustrates two examples. In the 2D example  1− and  denote
the masses in the teal and maroon documents. The 1D example uses
histograms to represent masses in the two documents. In both  RWMD
is nearly zero as masses do not have far to move  while the WMD will
be larger thanks to the constraints.
To make this precise we provide the following tight upper bound:

RWMD(d1  d2) ≤ dH(supp(cid:0)d1(cid:1)  supp(cid:0)d2(cid:1))  the Hausdorff distance
between the supports of d1 and d2. Let X = supp(cid:0)d1(cid:1) and Y =
supp(cid:0)d2(cid:1); and let RWMD1(d1  d2) and RWMD2(d1  d2) denote the
(cid:19)

relaxed optimal values when the marginal constraints on d1 and d2 are
kept  respectively:

(cid:18)

dH (X  Y ) = max

inf
y∈Y

Figure 2: RWMD as a poor
approximation to WMD

≥ max(cid:0)RWMD1(d1  d2)  RWMD2(d1  d2)(cid:1) = RWMD(d1  d2).

d(x  y)  sup
y∈Y

sup
x∈X

d(x  y)

inf
x∈X

The inequality follows since the left argument of the max is the furthest mass must travel in the
solution to RWMD1  while the right is the furthest mass must travel in the solution to RWMD2. It
is tight if the documents have singleton support and whenever d1 and d2 are supported on parallel
afﬁne subspaces and are translates in a normal direction. A 2D example is in Figure 2.
The preceding discussion suggests that RWMD is not an appropriate way to speed up WMD for long
documents with overlapping support  scenario where WMD computational complexity is especially
prohibitive. The GUTENBERG dataset showcases this failure  in which documents frequently have
common words. Our proposed HOTT does not suffer from this failure mode  while being signiﬁcantly
faster and as accurate as WMD. We verify this in the subsequent experimental studies. In the
supplementary materials we present a brief empirical analysis relating HOTT and RWMD to WMD
in terms of Mantel correlation and a Frobenius norm.

5 Experiments

We present timings for metric computation and consider applications where distance between docu-
ments plays a crucial role: k-NN classiﬁcation  low-dimensional visualization  and link prediction.

5.1 Computational timings

HOTT implementation. During training  we ﬁt LDA with 70 topics using a Gibbs sampler (Grifﬁths
& Steyvers  2004). Topics are truncated to the 20 most heavily-weighted words and renormalized.
The pairwise distances between topics WMD(ti  tj) are precomputed with words embedded in R300
using GloVe (Pennington et al.  2014). To evaluate HOTT at testing time  a few iterations of the
Gibbs sampler are run to obtain topic proportions ¯di of a new document di. When computing HOTT
between a pair of documents we truncate topic proportions at 1/(|T| + 1) and renormalize. Every
instance of the OT linear program is solved using Gurobi (Gurobi Optimization  2018).
We note that LDA inference may be carried out using any other approaches  e.g. stochastic/streaming
variational inference (Hoffman et al.  2013; Broderick et al.  2013) or geometric algorithms (Yurochkin
& Nguyen  2016; Yurochkin et al.  2019). We chose the MCMC variant (Grifﬁths & Steyvers  2004)
for its strong theoretical guarantees  simplicity and wide adoption in the topic modeling literature.

5

Figure 3: k-NN classiﬁcation performance across datasets

Table 1: Dataset statistics and document pairs per second; higher is better. HOTT has higher
throughput and excels on long documents with large portions of the vocabulary (as in GUTENBERG).

DATASET STATISTICS

PAIRS PER SECOND

DATASET

BBCSPORT
TWITTER
OHSUMED
CLASSIC
REUTERS8
AMAZON
20NEWS
GUTENBERG

|D|
737
3108
9152
7093
7674
8000
13277
3037

|V |
3657
1205
8261
5813
5495
16753
9251
15000

IOU AVG(l) AVG(κ) CLASSES

0.066
0.029
0.046
0.017
0.06
0.019
0.011
0.25

116.5
9.7
59.4
38.5
35.7
44.3
69.3
4367

11.7
6.3
11.0
8.7
8.7
9.0
10.5
13.3

5
3
10
4
8
4
20
142

RWMD WMD WMDT20 HOFTT HOTT
2548
1552
908
1053
989
966
699
1720

2016
1384
829
980
918
927
652
1503

1545
2194
473
720
672
253
384
359

1494
2664
454
816
834
289
338
2

526
2536
377
689
685
259
260
0.3

Topic computations. The preprocessing steps of our method—computing LDA topics and the topic to
topic pairwise distance matrix—are dwarfed by the cost of computing the full document-to-document
pairwise distance matrix. The complexity of base metric computation in our implementation is
O(|T|2)  since |supp(ti)| = 20 for all topics  leading to a relatively small OT instance.
HOTT computations. All distance computations were implemented in Python 3.7 and run on an
Intel i7-6700K at 4GHz with 32GB of RAM. Timings for pairwise distance computations are in
Table 1 (right). HOTT outperforms RWMD and WMD in terms of speed as it solves a signiﬁcantly
smaller linear program. On the left side of Table 1 we summarize relevant dataset statistics: |D| is
the number of documents; |V | is the vocabulary size; intersection over union (IOU) characterizes
average overlap in words between pairs of documents; AVG(l) is the average number of unique words
per document and AVG(κ) is the average number of major topics (i.e.  after truncation) per document.

5.2 k-NN classiﬁcation

We follow the setup of Kusner et al. (2015) to evaluate performance of HOTT on k-NN classiﬁcation.
Datasets. We consider 8 document classiﬁcation datasets: BBC sports news articles (BBCSPORT)
labeled by sport; tweets labeled by sentiments (TWITTER) (Sanders  2011); Amazon reviews labeled
by category (AMAZON); Reuters news articles labeled by topic (REUTERS) (we use the 8-class version
and train-test split of Cachopo et al. (2007)); medical abstracts labeled by cardiovascular disease
types (OHSUMED) (using 10 classes and train-test split as in Kusner et al. (2015)); sentences from
scientiﬁc articles labeled by publisher (CLASSIC); newsgroup posts labeled by category (20NEWS) 
with “by-date” train-test split and removing headers  footers and quotes;2 and Project Gutenberg
full-length books from 142 authors (GUTENBERG) using the author names as classes and 80/20
train-test split in the order of document appearance. For GUTENBERG  we reduced the vocabulary to
the most common 15000 words. For 20NEWS  we removed words appearing in ≤ 5 documents.

2https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html

6

ohsumed20newstwittergutenbergamazonr8bbcsportclassic0%10%20%30%40%50%60%Test error %5855484847423845454460554234424631323240363232323130293131292634301822294217193735201310138.49.78.88.78.9126.2146.18.75.6104.74.84.6111076.25.343.24.13.66.3179.19.59.46.23.85.15.75.14.5nBOW(Frakes&Baeza-Yates 1992)LSI(Deerwesteretal. 1990)SIF(Aroraetal. 2016)LDA(Bleietal. 2003)CosineRWMD(Kusneretal. 2015)TF-IDF(Jones 1972)HOFTTHOTTWMD-T20(Kusneretal. 2015)Baselines. We focus on evaluating HOTT and a variation without topic proportion truncation (HOFTT:
hierarchical optimal full topic transport) as alternatives to RWMD in a variety of metric-dependent
tasks. As demonstrated by the authors  RWMD has nearly identical performance to WMD  while
being more computationally feasible. Additionally  we analyze a naïve approach for speeding-up
WMD where we truncate documents to their top 20 unique words (WMD-T20)  making complexity
comparable to HOTT (yet 20 >AVG(κ) on all datasets). For k-NN classiﬁcation  we also consider
baselines that represent documents in vector form and use Euclidean distances: normalized bag-of-
words (nBOW) (Frakes & Baeza-Yates  1992); latent semantic indexing (LSI) (Deerwester et al. 
1990); latent Dirichlet allocation (LDA) (Blei et al.  2003) trained with a Gibbs sampler (Grifﬁths &
Steyvers  2004); and term frequency inverse document frequency (TF-IDF) (Spärck Jones  1972).
We omit comparison to embedding via BOW weighted averaging as it was shown to be inferior to
RWMD by Kusner et al. (2015) (i.e.  Word Centroid Distance) and instead consider smooth inverse
frequency (SIF)  a recent document embedding method by Arora et al. (2016). We also compare
to bag-of-words  where neighbors are identiﬁed using cosine similarity (Cosine). We use same
pre-trained GloVe embeddings for HOTT  RWMD  SIF and truncated WMD and set the same number
of topics |T| = 70 for HOTT  LDA and LSI; we provide experiments testing parameter sensitivity.
Results. We evaluate each method on k-NN classiﬁca-
tion (Fig. 3). There is no uniformly best method  but
HOTT performs best on average (Fig. 4) We highlight
the performance on the GUTENBERG dataset compared to
RWMD. We anticipate poor performance of RWMD on
GUTENBERG  since books contain more words  which can
make RWMD degenerate (see §4 and Fig. 2). Also note
strong performance of TF-IDF on OHSUMED and 20NEWS 
which differs from results of Kusner et al. (2015). We be-
lieve this is due to a different normalization scheme. We
used TﬁdfTransformer from scikit-learn (Pedregosa et al. 
2011) with default settings. We conclude that HOTT is
most powerful  both computationally (Table 1 right) and as
a distance metric for k-NN classiﬁcation (Figures 3 and 4) 
on larger corpora of longer documents  whereas on shorter
documents both RWMD and HOTT perform similarly.
Another interesting observation is the effect of truncation: HOTT performs as well as HOFTT 
meaning that truncating topic proportions of LDA does not prevent us from obtaining high-quality
document distances in less computational time  whereas truncating unique words for WMD degrades
its performance. This observation emphasizes the challenge of speeding up WMD  i.e. WMD cannot
be made computationally efﬁcient using truncation without degrading its performance. WMD-T20 is
slower than HOTT (Table 1) and performs noticeably worse (Figure 4). Truncating WMD further
will make its performance even worse  while truncating less will quickly lead to impractical run-time.
In the supplement  we complement our results considering 2-Wasserstein distance  and stemming  a
popular text pre-processing procedure for topic models to reduce vocabulary size. HOTT continues
to produce best performance on average. We restate that in all main text experiments we used
1-Wasserstein (i.e. eq. (1)) and did not stem  following experimental setup of Kusner et al. (2015).

Figure 4: Aggregated k-NN classiﬁca-
tion performance normalized by nBOW

5.3 Sensitivity analysis of HOTT

We analyze senstitivity of HOTT with respect to its components: word embeddings  number of LDA
topics  and topic truncation level.
Sensitivity to word embeddings. We train word2vec (Mikolov et al.  2013) 200-dimensional
embeddings on REUTERS and compare relevant methods with our default embedding (i.e.  GloVe)
and newly-trained word2vec embeddings. According to Mikolov et al. (2013)  word embedding
quality largely depends on data quantity rather than quality; hence we expect the performance to
degrade. In Fig. 5a  RWMD and WMD truncated performances drop as expected  but HOTT and
HOFTT remain stable; this behavior is likely due to the embedding-independent topic structure taken
into consideration.

7

AllDatasetsAverageerrorw.r.t.nBOW10.820.790.610.650.590.660.520.520.64nBOW(Frakes&Baeza-Yates 1992)LSI(Deerwesteretal. 1990)SIF(Aroraetal. 2016)LDA(Bleietal. 2003)CosineRWMD(Kusneretal. 2015)TF-IDF(Jones 1972)HOFTTHOTTWMD-T20(Kusneretal. 2015)(a) Embedding sensitivity on

(b) Topic number sensitivity on

(c) Topic truncation sensitivity on

REUTERS

CLASSIC

REUTERS

Figure 5: Sensitivity analysis: embedding  topic number and topic truncation

Number of LDA topics. In our experiments  we set |T| = 70. When the |T| increases  LDA
resembles the nBOW representation; correspondingly  HOTT approaches the WMD. The difference 
however  is that nBOW is a weaker baseline  while WMD is powerful document distance. Using
the CLASSIC dataset  in Fig. 5b we demonstrate that LDA (and LSI) may degrade with too many
topics  while HOTT and HOFTT are robust to topic overparameterization. In this example  better
performance of HOTT over HOFTT is likely due relatively short documents of the CLASSIC dataset.
While we have shown that HOTT is not sensitive to the choice of the number of topics  it is also
possible to eliminate this parameter by using LDA inference algorithms that learn number of topics
(Yurochkin et al.  2017) or adopting Bayesian nonparametric topic modes and corresponding inference
schemes (Teh et al.  2006; Wang et al.  2011; Bryant & Sudderth  2012).
Topic truncation. Fig. 5c demonstrates k-NN classiﬁcation performance on the REUTERS dataset
with varying topic truncation: top 10  20 (HOTT and HOFTT)  50  100 words and no truncation
(HOTT full and HOFTT full); LDA performance is given for reference. Varying the truncation level
does not affect the results signiﬁcantly  however no truncation results in unstable performance.

5.4

t-SNE metric visualization

Visualizing metrics as point clouds provides useful qual-
itative information for human users. Unlike k-NN clas-
siﬁcation  most methods for this task require long-range
distances and a full metric. Here  we use t-SNE (van der
Maaten & Hinton  2008) to visualize HOTT and RWMD
on the CLASSIC dataset in Fig. 6. HOTT appears to more
accurately separate the labeled points (color-coded). The
supplementary material gives additional t-SNE results.

5.5 Supervised link prediction

Figure 6: t-SNE on CLASSIC

We next evaluate HOTT in a different prediction task: supervised link prediction on graphs deﬁned
on text domains  here citation networks. The speciﬁc task we address is the Kaggle challenge of Link
Prediction TU.3 In this challenge  a citation network is given as an undirected graph  where nodes
are research papers and (undirected) edges represent citations. From this graph  edges have been
removed at random. The task is to reconstruct the full network. The dataset contains 27770 papers
(nodes). The training and testing sets consist of 615512 and 32648 node pairs (edges) respectively.
For each paper  the available data only includes publication year  title  authors  and abstract.
To study the effectiveness of a distance-based model with HOTT for link prediction  we train a
linear SVM classiﬁer over the feature set Φ  which includes the distance between the two abstracts
φdist computed via one of {HOFT  HOTT  RWMD  WMD-T20}. For completeness  we also
examine excluding the distance totally. We incrementally grow the feature sets Φ as: Φ0 = {φdist} 
Φ1 = {φdist} ∪ {φ1}  Φn = {φdist} ∪ {φ1  . . .   φn} where φ1 is the number of common words

3www.kaggle.com/c/link-prediction-tu

8

GloVeword2vec on R8Word embedding method0246810Test error %5.67.74.69.94.74.94.84.5RWMDWMD-T20HOFTTHOTT20406080100Number of topics4681012Test error %HOTTHOFTTLDALSI20406080100Number of topics510152025Test error %HOTTHOTT fullHOFTT 50HOTT 50HOFTT 10HOFTTHOTT 10LDAHOTT 100HOFTT 100HOFTT fullHOTTHOTTHOTTHOTTCACMMEDCRANCISIRWMDRWMDRWMDRWMDCACMMEDCRANCISITable 2: Link prediction: using distance (rows) for node-pair representations (cols).

Distance

F1 Score

Φ0
HOFTT 73.22
HOTT 73.19
RWMD 71.60
67.22

Φ1
76.27
76.03
74.90
63.38
None — 61.13

WMD-T20

Φ2
76.62
76.24
75.20
65.20
64.27

Φ3
78.85
78.64
77.16
70.38
67.72

Φ4
83.37
83.25
82.92
81.84
81.68

in the titles  φ2 the number of common authors  and φ3 and φ4 the signed and absolute difference
between the publication years.
Table 2 presents the results; evaluation is based on the F1-Score. Consistently  HOFTT and HOTT are
more effective than RWMD and WMD-T20 in all tests  and not using any of the distances consistently
degrades the performance.

6 Conclusion

We have proposed a hierarchical method for comparing natural language documents that leverages
optimal transport  topic modeling  and word embeddings. Speciﬁcally  word embeddings provide
global semantic language information  while LDA topic models provide corpus-speciﬁc topics and
topic distributions. Empirically these combine to give superior performance on various metric-based
tasks. We hypothesize that modeling documents by their representative topics is better for highlighting
differences despite the loss in resolution. HOTT appears to capture differences in the same way
a person asked to compare two documents would: by breaking down each document into easy to
understand concepts  and then comparing the concepts.
There are many avenues for future work. From a theoretical perspective  our use of a nested
Wasserstein metric suggests further analysis of this hierarchical transport space. Insight gained in this
direction may reveal the learning capacity of our method and inspire faster or more accurate algorithms.
From a computational perspective  our approach currently combines word embeddings  topic models
and OT  but these are all trained separately. End-to-end training that efﬁciently optimizes these three
components jointly would likely improve performance and facilitate analysis of our algorithm as a
uniﬁed approach to document comparison.
Finally  from an empirical perspective  the performance improvements we observe stem directly
from a reduction in the size of the transport problem. Investigation of larger corpora with longer
documents  and applications requiring the full set of pairwise distances are now feasible. We also can
consider applications to modeling of images or 3D data.

Acknowledgements.
J. Solomon acknowledges the generous support of Army Research Ofﬁce
grant W911NF1710068  Air Force Ofﬁce of Scientiﬁc Research award FA9550-19-1-031  of National
Science Foundation grant IIS-1838071  from an Amazon Research Award  from the MIT-IBM Watson
AI Laboratory  from the Toyota-CSAIL Joint Research Center  from the QCRI–CSAIL Computer
Science Research Program  and from a gift from Adobe Systems. Any opinions  ﬁndings  and
conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reﬂect the views of these organizations.

9

References
Arora  S.  Liang  Y.  and Ma  T. A simple but tough-to-beat baseline for sentence embeddings. 2016.

Atasu  K. and Mittelholzer  T. Linear-complexity data-parallel earth mover’s distance approximations.

In International Conference on Machine Learning  pp. 364–373  2019.

Blei  D. M.  Ng  A. Y.  and Jordan  M. I. Latent Dirichlet Allocation. Journal of Machine Learning

Research  3:993–1022  March 2003.

Broderick  T.  Boyd  N.  Wibisono  A.  Wilson  A. C.  and Jordan  M. I. Streaming variational Bayes.

In Advances in Neural Information Processing Systems  pp. 1727–1735  2013.

Bryant  M. and Sudderth  E. B. Truly nonparametric online variational inference for hierarchical
Dirichlet processes. In Advances in Neural Information Processing Systems  pp. 2699–2707  2012.

Cachopo  A. M. d. J. C. et al. Improving methods for single-label text categorization. Instituto

Superior Técnico  Portugal  2007.

Cuturi  M. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems  pp. 2292–2300  2013.

Deerwester  S.  Dumais  S. T.  Furnas  G. W.  Landauer  T. K.  and Harshman  R. Indexing by latent
semantic analysis. Journal of the American Society for Information Science  41(6):391  Sep 01
1990.

Frakes  W. B. and Baeza-Yates  R. Information retrieval: Data structures & algorithms  volume 331.

prentice Hall Englewood Cliffs  NJ  1992.

Grifﬁths  T. L. and Steyvers  M. Finding scientiﬁc topics. PNAS  101(suppl. 1):5228–5235  2004.

Gurobi Optimization  L. Gurobi optimizer reference manual  2018. URL http://www.gurobi.

com.

Hoffman  M. D.  Blei  D. M.  Wang  C.  and Paisley  J. Stochastic variational inference. Journal of

Machine Learning Research  14(1):1303–1347  May 2013.

Huang  G.  Guo  C.  Kusner  M. J.  Sun  Y.  Sha  F.  and Weinberger  K. Q. Supervised word mover’s

distance. In Advances in Neural Information Processing Systems  pp. 4862–4870  2016.

Kuhn  H. W. The Hungarian method for the assignment problem. Naval Research Logistics (NRL)  2

(1-2):83–97  1955.

Kusner  M.  Sun  Y.  Kolkin  N.  and Weinberger  K. From word embeddings to document distances.

In International Conference on Machine Learning  pp. 957–966  2015.

Luhn  H. P. A statistical approach to mechanized encoding and searching of literary information.

IBM Journal of Research and Development  1(4):309–317  1957.

Mikolov  T.  Sutskever  I.  Chen  K.  Corrado  G. S.  and Dean  J. Distributed representations of words
and phrases and their compositionality. In Advances in neural information processing systems  pp.
3111–3119  2013.

Newman  D.  Lau  J. H.  Grieser  K.  and Baldwin  T. Automatic evaluation of topic coherence. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics  pp. 100–108. Association for Computational
Linguistics  2010.

Otto  F. and Villani  C. Generalization of an inequality by talagrand and links with the logarithmic

sobolev inequality. Journal of Functional Analysis  173(2):361–400  2000.

Pedregosa  F.  Varoquaux  G.  Gramfort  A.  Michel  V.  Thirion  B.  Grisel  O.  Blondel  M. 
Prettenhofer  P.  Weiss  R.  Dubourg  V.  et al. Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research  12(Oct):2825–2830  2011.

10

Pennington  J.  Socher  R.  and Manning  C. D. Glove: Global vectors for word representation. In

Empirical Methods in Natural Language Processing (EMNLP)  pp. 1532–1543  2014.

Peyré  G. and Cuturi  M. Computational Optimal Transport. Submitted  2018.

Sanders  N. J. Sanders-twitter sentiment corpus. Sanders Analytics LLC  2011.

Santambrogio  F. Optimal Transport for Applied Mathematicians  volume 87 of Progress in Nonlinear
Differential Equations and Their Applications. Springer International Publishing  2015. ISBN
978-3-319-20827-5 978-3-319-20828-2. doi: 10.1007/978-3-319-20828-2.

Solomon  J. Optimal Transport on Discrete Domains. AMS Short Course on Discrete Differential

Geometry  2018.

Spärck Jones  K. A statistical interpretation of term speciﬁcity and its application in retrieval. Journal

of Documentation  28(1):11–21  1972.

Teh  Y. W.  Jordan  M. I.  Beal  M. J.  and Blei  D. M. Hierarchical Dirichlet processes. Journal of

the American Statistical Association  101(476)  2006.

van der Maaten  L. and Hinton  G. Visualizing data using t-SNE. Journal of Machine Learning

Research  9:2579–2605  2008.

Villani  C. Optimal Transport: Old and New. Number 338 in Grundlehren der mathematischen

Wissenschaften. Springer  Berlin  2009. ISBN 978-3-540-71049-3. OCLC: ocn244421231.

Wan  X. A novel document similarity measure based on earth mover’s distance. Information Sciences 

177(18):3718 – 3730  2007. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2007.02.045.

Wang  C.  Paisley  J.  and Blei  D. Online variational inference for the hierarchical Dirichlet process.
In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics  pp.
752–760  2011.

Williamson  S.  Wang  C.  Heller  K. A.  and Blei  D. M. The IBP compound Dirichlet process and
its application to focused topic modeling. In Proceedings of the 27th International Conference on
Machine Learning  pp. 1151–1158  2010.

Wu  L.  Yen  I. E.  Xu  K.  Xu  F.  Balakrishnan  A.  Chen  P.-Y.  Ravikumar  P.  and Witbrock  M. J.
Word mover’s embedding: From word2vec to document embedding. Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing  pp. 4524–4534  2018.

Wu  X. and Li  H. Topic mover’s distance based document classiﬁcation.

In Communication

Technology (ICCT)  2017 IEEE 17th International Conference on  pp. 1998–2002. IEEE  2017.

Xu  H.  Wang  W.  Liu  W.  and Carin  L. Distilled wasserstein learning for word embedding and

topic modeling. In Advances in Neural Information Processing Systems  pp. 1716–1725  2018.

Yurochkin  M. and Nguyen  X. Geometric Dirichlet Means Algorithm for topic inference.

Advances in Neural Information Processing Systems  pp. 2505–2513  2016.

In

Yurochkin  M.  Guha  A.  and Nguyen  X. Conic Scan-and-Cover algorithms for nonparametric topic

modeling. In Advances in Neural Information Processing Systems  pp. 3881–3890  2017.

Yurochkin  M.  Guha  A.  Sun  Y.  and Nguyen  X. Dirichlet simplex nest and geometric inference.

In International Conference on Machine Learning  pp. 7262–7271  2019.

11

,Francesca Petralia
Joshua Vogelstein
David Dunson
Emmanuel Abbe
Sanjeev Kulkarni
Eun Jee Lee
Mikhail Yurochkin
Sebastian Claici
Edward Chien
Farzaneh Mirzazadeh
Justin Solomon