2019,Understanding and Improving Layer Normalization,Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients  faster training  and better generalization accuracy. However  it is still unclear where the effectiveness stems from. In this paper  our main contribution is to take a step further in understanding LayerNorm.  Many of previous studies believe that the success of LayerNorm comes from  forward normalization. Unlike them  we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore  we find that the parameters of LayerNorm  including the bias and gain  increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version  of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. 
To address the over-fitting problem  we propose a new normalization method  Adaptive Normalization (AdaNorm)  by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm  on seven out of eight datasets.,Understanding and Improving Layer Normalization

Jingjing Xu1  Xu Sun1 2∗  Zhiyuan Zhang1  Guangxiang Zhao2  Junyang Lin1
1 MOE Key Lab of Computational Linguistics  School of EECS  Peking University

2 Center for Data Science  Peking University

{jingjingxu xusun zzy1210 zhaoguangxiang linjunyang}@pku.edu.cn

Abstract

Layer normalization (LayerNorm) is a technique to normalize the distributions
of intermediate layers. It enables smoother gradients  faster training  and better
generalization accuracy. However  it is still unclear where the effectiveness stems
from. In this paper  our main contribution is to take a step further in understanding
LayerNorm. Many of previous studies believe that the success of LayerNorm
comes from forward normalization. Unlike them  we ﬁnd that the derivatives of the
mean and variance are more important than forward normalization by re-centering
and re-scaling backward gradients. Furthermore  we ﬁnd that the parameters of
LayerNorm  including the bias and gain  increase the risk of over-ﬁtting and do
not work in most cases. Experiments show that a simple version of LayerNorm
(LayerNorm-simple) without the bias and gain outperforms LayerNorm on four
datasets. It obtains the state-of-the-art performance on En-Vi machine translation.
To address the over-ﬁtting problem  we propose a new normalization method 
Adaptive Normalization (AdaNorm)  by replacing the bias and gain with a new
transformation function. Experiments show that AdaNorm demonstrates better
results than LayerNorm on seven out of eight datasets.

1

Introduction

Neural network training has long been a focus in Deep Learning research area. One of the prominent
progress is the application of normalization methods. Initially  Ioffe and Szegedy [2015] introduce
the concept of normalizing layers with the proposed Batch Normalization (BatchNorm). It is widely
believed that by controlling the mean and variance of layer inputs across mini-batches  BatchNorm
stabilizes the distribution and improves training efﬁciency. Following this work  Lei Ba et al. [2016]
point out its limitation in Recurrent Neural Networks (RNN) and propose Layer Normalization
(LayerNorm) that is performed across the neurons in a layer. LayerNorm is adaptive to RNN and
self-attention-based models. A typical example is its application in the state-of-the-art framework 
Transformer [Vaswani et al.  2017]. LayerNorm enables faster training of Transformer and is
irreplaceable in this framework.
Despite its great success  it is still unclear why LayerNorm is so effective. The widely accepted
explanation is that forward normalization brings distribution stability [Ioffe and Szegedy  2015 
Lei Ba et al.  2016]. Recent studies show that the effects of BatchNorm are not related to the stability
of input distribution [Zhang et al.  2017  Santurkar et al.  2018]. They also propose that the reason
why BatchNorm is effective is that normalization smooths the optimization landscape. However  it is
still unclear whether these theories can explain the success of LayerNorm.
The main contribution of this paper is to explore how LayerNorm works. Through a series of analyses 
we ﬁnd that the derivatives of the mean and variance are important by re-centering and re-scaling

∗Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

backward gradients. Furthermore  it is beyond our expectation that the bias and gain do not work in
most cases. The details of our ﬁndings are illustrated below.
The derivatives of the mean and variance are more important to LayerNorm than forward
normalization. Many of the previous studies believe that the forward normalization is the only
decisive factor to LayerNorm. It makes the input distribution more stable  thus brings better conver-
gence. Unlike them  our experimental results show that forward normalization has little to do with the
effectiveness and the derivatives of the mean and variance play a signiﬁcant role in LayerNorm. To
illustrate how these derivatives work  we propose DetachNorm  which adds an additional detaching
operation to LayerNorm to change the mean and variance from variables to constants. It preserves the
re-centering and re-scaling fact but cuts off the derivative of the mean and variance with respect to
the input. DetachNorm performs worse than LayerNorm on six out of eight datasets. This proves that
the derivatives of the mean and variance are useful to LayerNorm. Furthermore  to investigate the
reason for the above observation  we analyze the gradients in LayerNorm and DetachNorm  and ﬁnd
that the derivatives of means re-center gradients and the derivatives of variances re-scale gradients.
The parameters of LayerNorm  including the bias and gain  increase the risk of over-ﬁtting and
do not work in most cases. The bias and gain are applied for afﬁne transformation on normalized
vectors. They are expected to enhance the expressive power by re-shaping the distribution. To
evaluate their effects on results  we build a simple version of LayerNorm (LayerNorm-simple) by
removing the bias and gain. Our experimental results show that LayerNorm-simple achieves better
results than LayerNorm on four datasets. It even achieves the state-of-the-art performance on En-Vi
machine translation. By comparing loss curves of LayerNorm with and without the bias and gain  we
ﬁnd that the bias and gain cause over-ﬁtting. We speculate the reason of over-ﬁtting is mainly that
the bias and gain are learned from the training set and cannot adjust themself towards different input
distributions when testing.
Motivated by this assumption  we propose a novel normalization method  Adaptive Normalization
(AdaNorm). AdaNorm replaces the bias and gain with a new transformation function. This function
adaptively adjusts scaling weights based on input values. We evaluate AdaNorm and LayerNorm on
eight datasets  covering tasks of machine translation  language modeling  text classiﬁcation  image
classiﬁcation  and dependency parsing. Results show that AdaNorm achieves better results on seven
datasets.

2 Preliminaries

In this section  we ﬁrst review the algorithm of LayerNorm and then introduce the datasets and
models used in the following analysis sections.

2.1 LayerNorm Algorithm

Let x = (x1  x2  . . .   xH ) be the vector representation of an input of size H to normalization layers.
LayerNorm re-centers and re-scales input x as
x − µ
σ

h = g (cid:12) N (x) + b  N (x) =

(xi − µ)2

(cid:118)(cid:117)(cid:117)(cid:116) 1

H

H(cid:88)

1
H

H(cid:88)

i=1

  µ =

xi 

σ =

(1)

i=1

where h is the output of a LayerNorm layer. (cid:12) is a dot production operation. µ and σ are the mean
and standard deviation of input. Bias b and gain g are parameters with the same dimension H.

2.2 Experimental Setup

To investigate how LayerNorm works  we conduct a series of experiments in this paper. Since
LayerNorm is a default setting in Transformer [Vaswani et al.  2017] and Transformer-XL [Dai et al. 
2019]  which have shown state-of-the-art results on a variety of tasks (e.g.  machine translation) 
we primarily consider normalization on Transformer and Transformer-XL networks. Also  to avoid
the impact of model architecture  we evaluate the effects of normalization on feed-forward neural
networks and convolutional neural networks. Here list the datasets and models. More details can be
found at the arxiv version.

2

Machine translation includes three widely-used datasets  WMT English-German (En-De)  IWSLT
14 German-English (De-En) [Cettolo et al.  2014] and IWSLT 15 English-Vietnamese (En-Vi) [Cettolo
et al.  2015]. For all dataset  we use the setting of PreNorm where normalization is applied before
each layer. We re-implement Transformer with the released code of Fairseq [Ott et al.  2019]2. The
evaluation metric is BLEU [Papineni et al.  2002].
For En-De dataset  we use the same dataset splits and the same compound splitting following previous
work [Vaswani et al.  2017]. BPE is used to get vocabularies. We use the shared embedding setting
and the vocabulary size is 32 765. We use “transformer_wmt_en_de_big_t2t” as our basic model.
The dropout rate is 0.3. The learning rate is 0.001. The training batch size is 4 096 tokens. We use
optimizer Adam with β1 = 0.9 and β2 = 0.98. The number of warmup steps is 4K.
The De-En dataset is provided by the IWSLT 2014 Evaluation Campaign. We use the same dataset
splits following previous work [Ott et al.  2019  Ranzato et al.  2016  Wiseman and Rush  2016]. It
contains 153K sentences for training  7K sentences for validation  and 7K sentences for testing. BPE
is used to get vocabularies. We use the shared embedding setting and the vocabulary size is 10 149.
We use “transformer_iwslt_de_en” as our basic model. The dropout rate is 0.3. The attention dropout
rate is 0.1. The activation dropout is 0.1. The initialization learning rate is 1e-07 and the learning rate
is 0.0015. The training batch size is 4 096 tokens. We update gradients for every 2 steps. The number
of warmup steps is 8K.
The En-Vi dataset contains 133K training sentence pairs provided by the IWSLT 2015 Evaluation
Campaign. We use TED tst2012 (1 553 sentences) as the validation set and TED tst2013 (1 268
sentences) as the test set. BPE is used to get input and output vocabularies. The English and
Vietnamese vocabulary sizes are 7 669 and 6 669 respectively. The dropout rate is 0.1. The learning
rate is 0.001. The training batch size is 4 096 tokens. The number of warmup steps is 8K. We use
“transformer_wmt_en_de” as our basic model. We use optimizer Adam with β1 = 0.9 and β2 = 0.98.
Language modeling includes a large dataset  Enwiki83 that contains 100M bytes of unprocessed
Wikipedia text. We implement a 12-layer Transformer-XL model. The dimension of each layer is
512. Multi-head attention contains 8 heads and the dimension of each head is 64. The dropout rate is
0.1. The batch size is 22. We use optimizer Adam with a learning rate 0.00025. We use the average
number of Bits-Per-Character (BPC) as the evaluation metric [Al-Rfou et al.  2018  Dai et al.  2019].
Text classiﬁcation includes two sentence classiﬁcation datasets: RT [Pang and Lee  2005]  and
SST5 [Socher et al.  2013]. RT is a binary sentiment classiﬁcation dataset from online movie reviews.
We randomly divide all examples into 8 608 for training  964 for validation  and 1 089 for testing.
SST5 is a single-sentence classiﬁcation dataset built on movie reviews. We run experiments on a ﬁve
label set. We build a Transformer model with a 4-layer encoder. The batch size is 4 096 tokens. The
word embedding dimension is 128 and the hidden dimension is 128. The dropout rate is 0.2. We use
optimizer Adam with β1 = 0.9  β2 = 0.998. Normalization is applied before each layer. Accuracy is
the evaluation metric.
Image classiﬁcation includes a widely-used dataset  MNIST [LeCun et al.  1998]. It consists of
55 000 training images  5 000 validation images  and additional 10 000 testing images. We implement
a 3-layer convolutional neural network for classiﬁcation. The ﬁrst 2D-convolution layer has 1 in-
channel  20 out-channels. The second 2D-convolution layer has 20 in-channels  50 out-channels. We
ﬂatten the output of the second 2D-convolution layer and send it to a linear layer. The batch size is
32. We use optimizer Adam with a learning rate of 0.001. We apply LayerNorm before the activation
in every linear layer. We train the model for 20 epochs. Normalization is applied before each layer.
Accuracy is the evaluation metric.
Dependency parsing includes a dataset  English Penn TreeBank (PTB) [Marcus et al.  1993]. We
follow the standard split of the corpus with sections 2-21 as the training set (39 832 sentences 
1 900 056 transition examples)  section 22 as the validation set (1 700 sentences  80 234 transition
examples)  and section 23 as the testing set (2 416 sentences  113 368 transition examples). We
implement a MLP-based parser following the work [Chen and Manning  2014]. The dimension of
the hidden state is 512  the batch size is 1  024  the dropout rate is 0.2. We use optimizer Adam and
initialize the learning rate to 0.001. We apply normalization before activation in every linear layer.

2https://github.com/pytorch/fairseq
3http://mattmahoney.net/dc/text.html

3

Following the work [Chen and Manning  2014]  we use Unlabeled Attachment Score (UAS) as the
evaluation metric.

3 Understanding LayerNorm

To investigate how LayerNorm facilitates training  we conduct ablation studies to observe each part’s
contribution to the performance. In this section  we analyse the effects of the bias and gain  forward
normalization  and backward normalization.

Table 1: The bias and gain do not work on six out of eight datasets. “w/o Norm” is a naive model
without LayerNorm. “LayerNorm-simple” is a variant of LayerNorm that drops the bias and gain.
“(+)” means higher is better. “(-)” means lower is better.

Models

Machine Translation

Language Modeling

En-De(+) De-En(+) En-Vi(+)

Enwiki8(-)

Model Layers

12

w/o Norm
LayerNorm

LayerNorm-simple

Diverge

28.3
28.4

12
34.0
35.5
35.5

12
28.4
31.2
31.6

12
1.04
1.07
1.07

Classiﬁcation

Parsing
RT(+) SST5(+) MNIST(+) PTB(+)

4

76.85
77.21
76.66

4

38.55
39.23
40.54

3

99.14
99.13
99.09

3

88.31
89.12
89.19

3.1 The Effect of the Bias and Gain in LayerNorm

The bias and gain do not work in most cases. From Table 1  it can be found that LayerNorm
is an effective approach. It brings large performance improvements on six out of eight datasets
compared with the naive baseline without LayerNorm (“w/o Norm”). By comparing LayerNorm
and LayerNorm-simple  we ﬁnd that dropping the bias and gain (“LayerNorm-simple”) does not
decrease the performance on six datasets. Surprisingly  LayerNorm-simple outperforms LayerNorm
on four datasets  even with a 0.4 BLEU improvement on En-Vi and a 1.31 ACC improvement on
SST-5. Also  it needs to notice that 31.6 achieved by LayerNorm-simple is the state-of-the-art result
on En-Vi machine translation.
Furthermore  we ﬁnd that the bias and gain increase the risk of over-ﬁtting. Initially  considering that
input information may be lost when normalizing input distributions  the bias and gain are designed
for afﬁne transformation on normalized vectors to enhance the expressive power. However  since
the bias and gain are learned from the training set and they ignore the input distributions of the
testing data  the risk of over-ﬁtting may increase in LayerNorm. It is veriﬁed by convergence curves
in Figure 1. LayerNorm achieves lower training loss (or BPC) but higher validation loss (or BPC)
than LayerNorm-simple on En-Vi  Enwiki8. These results indicate that current afﬁne transformation
mechanism has a potential risk of over-ﬁtting and needs to be further improved.

Figure 1: Convergence curves of LayerNorm and LayerNorm-simple on En-Vi  Enwiki8. Lower is
better. The bias and gain increase the risk of over-ﬁtting.

3.2 The Effect of Forward Normalization

For easier analysis  we only consider LayerNorm without the bias and gain here. Let y =
(y1  y2  . . .   yH ) be the normalized vector  the calculation process of LayerNorm without the bias and

4

10152025302.502.753.003.253.503.754.004.254.50Train Loss3.63.84.04.24.44.64.85.0Valid LossEn-ViLayerNorm-simple trainLayerNorm trainLayerNorm-simple validLayerNorm valid1020304050600.900.951.001.051.101.151.201.251.30Train BPC1.101.151.201.251.301.35Valid BPCEnwiki8LayerNorm-simple trainLayerNorm trainLayerNorm-simple validLayerNorm validTable 2: The derivatives of the mean and variance matter. “w/o Norm” is the naive model without
normalization. “DetachNorm” is a variant of “LayerNorm-simple”. It detaches the derivatives of the
mean and variance. “(+)” means higher is better. “(-)” means lower is better. The top table shows the
effect of forward normalization. The bottom table shows the effect of the derivatives of the mean and
variance.

Models

Machine Translation

Language Modeling

En-De De-En(+) En-Vi(+)

Enwiki8(-)

Model Layers

12

w/o Norm
DetachNorm
Improvement

Diverge
Diverge

–

12
34.0
33.9
-0.1

12
28.4
27.7
-0.7

12
1.04
1.12
-0.08

Models

Machine Translation

Language Modeling

En-De De-En(+) En-Vi(+)

Enwiki8(-)

Classiﬁcation

Parsing
RT(+) SST5(+) MNIST(+) PTB(+)

4

4

3

3

76.85
76.40
-0.45

99.14
99.10
-0.04

38.55
40.04
1.49

88.31
89.79
1.48
Parsing
RT(+) SST5(+) MNIST(+) PTB(+)

Classiﬁcation

Model Layers
DetachNorm

12

Diverge

LayerNorm-simple

Improvement

28.4
–

12
33.9
35.5
1.6

12
27.7
31.6
3.9

12
1.12
1.07
0.05

4

76.40
76.66
0.26

4

40.04
40.54
0.50

3

99.10
99.09
-0.01

3

89.79
89.19
-0.60

gain can be written as

y =

x − µ
σ

  µ =

H(cid:88)

i=1

1
H

xi 

σ =

(cid:118)(cid:117)(cid:117)(cid:116) 1

H

H(cid:88)

(xi − µ)2

i=1

(2)

where x = (x1  x2  . . .   xH ) is the input vector and H is the dimension of x. µ and σ are the mean
and standard deviation of x1  x2  . . .   xH. Then  suppose ¯y and Dy are the mean and variance of
y1  y2  . . .   yH. It is easy to verify

H(cid:88)

i=1

¯y =

1
H

yi =

1
H

H(cid:88)

(xi − µ)

σ

i=1

= 0  Dy =

1
H

(xi − µ)2

σ2

= 1.

(3)

H(cid:88)

i=1

Eq. (3) shows that normalization re-centers and re-scales input vector x. By now  a widely accepted
belief is that the effectiveness of LayerNorm comes from steady layer distributions brought by
forward normalization [Lei Ba et al.  2016]. To evaluate whether forward normalization explains
the effectiveness of LayerNorm  we need to separate the effect on forward layer inputs and that on
backward gradients. In this paper  we design a new method  called DetachNorm. The difference
between LayerNorm and DetachNorm is that DetachNorm detaches the derivatives of the mean and
variance4. Detaching derivatives means treating the mean and variance as changeable constants 
rather than variables  which do not require gradients in backward propagation. The calculation of
DetachNorm can be written as

y =

x − ˆµ
ˆσ

 

ˆµ = θ(µ) 

ˆσ = θ(σ)

(4)

where µ and σ are the mean and standard deviation of input x  as calculated in Eq. (2). The function
θ(·) can be seen as a special copy function  which copies the values of µ and σ into constants ˆµ and
ˆσ. In all  DetachNorm keeps the same forward normalization fact as LayerNorm does  but cuts offs
the derivatives of the mean and variance.
Since DetachNorm keeps the same re-centering and re-scaling way in forward propagation as
LayerNorm-simple does  the gap between DetachNorm and “w/o Norm” shows the effect of forward
normalization. As we can see  DetachNorm perform worse than “w/o Norm”  showing that forward
normalization has little to do with the success of LayerNorm.
Furthermore  the only difference between DetachNorm and LayerNorm-simple lies in that Detach-
Norm detaches the derivatives of the mean and variance. As shown in Table 2  DetachNorm performs

4In our implementation  we detach the derivative of standard deviation  the square root of variance.

5

Figure 2: Convergence curves of LayerNorm-simple and DetachNorm on two translation datasets.

worse than LayerNorm-simple on six datasets. It is mainly because that DetachNorm converges to
much worse local optima compared with LayerNorm-simple  as shown in Figure 2. The gap between
DetachNorm and LayerNorm-simple shows the effectiveness of the derivatives of the mean and
variance. By comparing the achieved improvements  we ﬁnd that the derivatives of the mean and
variance bring higher improvements than forward normalization does.
These results demonstrate that the derivatives of the mean and variance play a signiﬁcant role. In
addition  the extremely worse results of DetachNorm on En-De  De-En and En-Vi indicate that the
derivatives of the mean and variance may be more important for deeper models. In the following
section  we will give a detailed analysis of why and how the derivatives of the mean and variance
contribute to the performance.

3.3 The Effect of the Derivatives of the Mean and Variance

To understand how the derivatives of the mean and variance work  we analyze the gradients of
LayerNorm-simple and DetachNorm. According to the chain rule  the gradient of x is5

∂(cid:96)
∂x

← dy
dx

∂(cid:96)
∂y

(5)

where (cid:96) is the loss function  x is the input vector and y is the normalized vector. We here analyze the
effect of detaching the derivatives of the mean and variance on backward gradients. Our results are
summarized in the following theorem  whose proof is listed in the Appendix of the arxiv version.
Theorem 1. Given ∂(cid:96)
For the case of detaching the derivatives of µ and σ  suppose ∂(cid:96)
of x with mean ¯a and variance Da. We have ¯a = ¯g/σ and Da = Dg/(σ2).

∂y = (g1  g2  ...  gH )T  let ¯g and Dg be the mean and variance of g1  g2  ...  gH.
∂x = (a1  a2  ...  aH )T is the gradient

(1) For the case of standard LayerNorm-simple  suppose ∂(cid:96)

∂x = (b1  b2  ...  bH )T is the gradient of x

with mean ¯b and variance Db.

We have ¯b = 0 and Db ≤ Dg/(σ2).

(2) For the case of detaching the derivative of µ  suppose ∂(cid:96)

∂x = (c1  c2  ...  cH )T is the gradient of

x with mean ¯c and variance Dc.

We have ¯c = ¯g/σ and Dc ≤ Dg/(σ2).

(3) For the case of detaching the derivative of σ  suppose ∂(cid:96)

∂x = (d1  d2  ...  dH )T is the gradient of

x with mean ¯d and variance Dd.

We have ¯d = 0 and Dc = Dg/(σ2).

By comparing the case of detaching the derivative of µ with that of LayerNorm-simple in Theorem 1 
∂x to zero. By comparing the case of detaching the
we ﬁnd that the derivative of µ re-centers ∂(cid:96)

5When calculating the gradient  we adopt the denominator layout.

6

01020304050Epoch4.04.55.05.56.06.57.07.5Valid LossDe-EnDetachNormLayerNorm-simple0.02.55.07.510.012.515.017.5Epoch4.55.05.56.06.5Valid LossEn-ViDetachNormLayerNorm-simplederivative of σ with of LayerNorm-simple  we ﬁnd that the derivative of σ reduces the variance of ∂(cid:96)
∂x 
which can be seen a kind of re-scaling. We refer to gradient re-centering and re-scaling as gradient
normalization.
To further evaluate the effect of gradient normalization on model performance  we test the derivatives
of the mean and variance separately. Table 3 shows that detaching the derivative of variance decreases
the performance signiﬁcantly on deeper networks. Therefore  it is necessary to control the variance
of gradients for deeper networks.
In conclusion  LayerNorm normalizes forward layer inputs and backward gradients. The derivatives
of the mean and variance play more important roles than forward normalization in LayerNorm.
Furthermore  unlike previous work [Santurkar et al.  2018] only noticing that normalization smooths
gradients  this paper provides deeper insight about how normalization impacts backward gradients.

Table 3: The derivative of variance is more important than that of mean for deeper networks. “(+)”
means higher is better. “(-)” means lower is better.

Models

Machine Translation

Language Model

En-De(+) De-En(+) En-Vi(+)

Enwiki8(-)

Model Layers
LayerNorm-simple
Detach Mean
Detach Variance

12
28.4
28.3

Diverge

12
35.5
35.6
34.2

12
31.6
31.3
29.8

12
1.07
1.07
1.10

Classiﬁcation

Parsing
RT(+) SST5(+) MNIST(+) PTB(+)

4

76.66
75.02
77.04

4

40.54
40.99
41.74

3

99.09
99.25
99.10

3

89.19
89.45
89.80

4 AdaNorm

AdaNorm adopts a new transformation function which can adaptively control scaling weights towards
different inputs.6

4.1 AdaNorm Algorithm
Formally  let y = N (x) = (x − µ)/ σ be the normalized vector where µ and σ are the mean and
variance of the input x = (x1  x2  . . .   xH ). We use φ(y)  a function with respect to input x  to
replace the bias and gain with the following equation:

z = φ(y) (cid:12) y = φ(N (x)) (cid:12) N (x)

(6)
where z = (z1  z2  . . .   zH ) is the output of AdaNorm and (cid:12) is a dot product operation. Unlike
the bias and gain being ﬁxed in LayerNorm  φ(y) can adaptively adjust scaling weights based on
inputs. To keep the stability of training  we expect that φ(·) has some features. First  φ(·) must
be differentiable. Second  we expect that the average scaling weight is ﬁxed  namely the average
of φ(y) is a constant C where C > 0. Third  we expect that the average of z is bounded  which
can avoid the problem of exploding loss. Namely  we require that there exists a constant M such
zi| < M. Theorem 2 proves that there exists a unique solution which can satisfy these
that | 1
requirements. The proof is listed in the Appendix of the arxiv version.
Theorem 2. Suppose φ(yi) is derivable  ∀y   1
M (M > 0)  where H is the hidden size. There exists only one solution:

φ(yi) = C > 0  and ∃M  s.t. | 1

zi| <

H(cid:80)

H(cid:80)

H

i=1

H(cid:80)

H

i=1

H

i=1

φ(yi) = C(1 − kyi)

which can satisfy these requirements.
Since 1 − kyi < 0 will undesirably change the direction of vector  we expect that φ(yi) > 0 holds 
which means yi < 1/k must hold. Due to the symmetry of yi  |yi| < 1/k is required to hold too.

6Our code is released at https://github.com/lancopku/AdaNorm

7

Based on Chebyshev’s Inequality  we have

P (|yi| < 1/k) = P (|yi − E(yi)| < 1/k) ≥ 1 − Dy

(1/k)2 = 1 − k2Dy

(7)

where Dy is the variance of y = (y1  y2  . . .   yH ) and H is the dimension of y. Based on Eq. (3)  we
can verify Dy = 1. If we expect that |yi| < 1/k holds with a probability higher than 99%  k = 1/10
should be choose based on Eq. (7). Namely  we choose

Given an input vector x  the complete calculation process of AdaNorm is

z = C(1 − ky) (cid:12) y 

y =

x − µ
σ

  µ =

1
H

xi 

σ =

φ(yi) = C(1 − yi
10

).

(cid:118)(cid:117)(cid:117)(cid:116) 1

H

(8)

(9)

H(cid:88)

(xi − µ)2

i=1

H(cid:88)

i=1

where C is a hyper-parameter. (cid:12) is a dot product operation. k is recommended to set as 1/10. To
prevent the introduced term C(1 − ky) dismissing the feature of gradient re-centering and re-scaling 
we detach the gradient of C(1 − ky) and only treat it as a changeable constant in implementation.

Table 4: Results of LayerNorm and AdaNorm. “(+)” means higher is better. “(-)” means lower is
better. AdaNorm outperforms LayerNorm on seven datasets.

Machine Translation

Language Model

Models

w/o Norm
LayerNorm
LayerNorm-simple
AdaNorm

28.3
28.4
28.5

En-De(+) De-En(+) En-Vi(+)
Diverge

34.0
35.5
35.5
35.6

28.4
31.2
31.6
31.4

Enwiki8(-)

1.04
1.07
1.07
1.07

Classiﬁcation

Parsing
RT(+) SST5(+) MNIST(+) PTB(+)
88.31
76.85
89.12
77.21
89.19
76.66
77.50
89.23

99.14
99.13
99.09
99.35

38.55
39.23
40.54
40.54

4.2 Comparison between AdaNorm and LayerNorm

The comparison between LayerNorm and AdaNorm is shown in Table 4.7 AdaNorm outperforms
LayerNorm on seven datasets  with 0.2 BLEU on En-De  0.1 BLEU on De-En  0.2 BLEU on En-
Vi  0.29 ACC on RT  1.31 ACC on SST  0.22 ACC on MNIST  and 0.11 UAC on PTB. Unlike
LayerNorm-simple only performing well on bigger models  AdaNorm achieves more balanced results.
Figure 3 shows the loss curves of LayerNorm and AdaNorm on the validation set of En-Vi  PTB  and
De-En. Compared to AdaNorm  LayerNorm has lower training loss but higher validation loss. Lower
validation loss proves that AdaNorm has better convergence.

Figure 3: Loss curves of LayerNorm and AdaNorm on En-Vi  PTB  and De-En.

5 Related Work

Deep neural networks have outperformed shallow models in a variety of ﬁelds  such as natural
language processing [Sutskever et al.  2014  Bahdanau et al.  2015  Devlin et al.  2018]  computer
vision [He et al.  2016  Huang et al.  2017]  etc. The improvement mainly comes from the stronger

7For AdaNorm implementation  Kaiming initialization and the setting of prenorm are recommended.

8

51015202530352.502.753.003.253.503.754.004.254.50Train Loss3.63.84.04.24.44.64.85.0Valid LossEn-ViAdaNorm trainLayerNorm trainAdaNorm validLayerNorm valid510152025300.000.050.100.150.200.250.30Train Loss0.100.120.140.160.180.20Valid LossPTBAdaNorm trainLayerNorm trainAdaNorm validLayerNorm valid204060801001203.03.23.43.63.84.04.24.4Train Loss3.703.753.803.853.903.954.00Valid LossDe-EnAdaNorm trainLayerNorm trainAdaNorm validLayerNorm validexpressive power of deep layers. However  with the increase of depth  the network training process
becomes complicated and requires advanced architectural techniques. One of the important techniques
of such advances is normalization.
Currently  it is widely accepted that normalization layers assist training by smoothing gradients 
enabling large learning rates  accelerating convergence  and improving generalization results [Zhang
et al.  2019]. First introduced by Ioffe and Szegedy [2015]  BatchNorm ﬁxes layer distributions to
reduce ICS (Internal Covariate Shift)  a phenomenon that the upper layers need to continuously adapt
to the new distributions of lower layers. Following this work  several normalization methods have
been proposed  like instance normalization [Ulyanov et al.  2016] and group normalization [Wu and
He  2018]. In addition  there are several studies exploring better activation functions [Klambauer
et al.  2017] or initialization methods [Zhang et al.  2019] to avoid the dependency on normalization
layers.
LayerNorm is proposed to expand BatchNorm into RNN. LayerNorm normalizes the mean and
variance of all summed inputs to the neurons in one layer. Unlike BatchNorm that depends on the size
of mini-batch  LayerNorm has fewer limitations. LayerNorm is adaptive to RNN and self-attention-
based models. It has been applied to the state-of-the-art frameworks such as Transformer [Vaswani
et al.  2017]  BERT [Devlin et al.  2018]  and Transformer-XL [Dai et al.  2019]. LayerNorm brings
better performance and is irreplaceable in these frameworks.
Despite the good performance  it is still unclear how layer normalization works. Ioffe and Szegedy
[2015] claim that the effectiveness of BatchNorm comes from reducing ICS. It has been a popular
belief about BatchNorm [Santurkar et al.  2018]. However  some recent studies point out that
the success of BatchNorm relates to the smoother gradients and has little to do with reducing
ICS [Santurkar et al.  2018  Bjorck et al.  2018]. Although these studies provide a pioneering
perspective to understand BatchNorm  there still remain some unanswered questions  such as how
BatchNorm helps smooth gradients. Also  there are little work studying whether these theories can
explain the success of LayerNorm. In this paper  we take a further step to a better understanding of
LayerNorm.

6 Conclusion

In this paper  we investigate how layer normalization works. Based on a series of experiments and
theoretical analysis  we summarize some interesting conclusions. We ﬁnd that the derivatives of
the mean and variance are important to the success of LayerNorm by re-centering and re-scaling
backward gradients. Furthermore  experiments show that the bias and gain increase the risk of
over-ﬁtting and do not work in most cases. To address this problem  we propose a normalization
method AdaNorm. It replaces the bias and gain in LayerNorm with a new adaptive transformation
function that can update scaling weights based on input values. Experiments show that AdaNorm
outperforms LayerNorm on seven datasets. In the future work  we would like to explore more
alternatives to LayerNorm from the perspective of gradient normalization.

Acknowledgments

We thank all reviewers for providing the thoughtful and constructive suggestions. This work was
supported in part by National Natural Science Foundation of China (No. 61673028).

References
R. Al-Rfou  D. Choe  N. Constant  M. Guo  and L. Jones. Character-level language modeling with

deeper self-attention. CoRR  abs/1808.04444  2018.

D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align and
translate. In 3rd International Conference on Learning Representations  ICLR 2015  San Diego 
CA  USA  May 7-9  2015  Conference Track Proceedings  2015.

N. Bjorck  C. P. Gomes  B. Selman  and K. Q. Weinberger. Understanding batch normalization. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information

9

Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages 7705–
7716  2018.

M. Cettolo  J. Niehues  S. Stüker  L. Bentivogli  and M. Federico. The iwslt 2015 evaluation campaign.

In IWSLT 2014  International Workshop on Spoken Language Translation  2014.

M. Cettolo  J. Niehues  S. Stüker  L. Bentivogli  R. Cattoni  and M. Federico. The iwslt 2015
evaluation campaign. In IWSLT 2015  International Workshop on Spoken Language Translation 
2015.

D. Chen and C. D. Manning. A fast and accurate dependency parser using neural networks. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing 
EMNLP 2014  October 25-29  2014  Doha  Qatar  A meeting of SIGDAT  a Special Interest Group
of the ACL  pages 740–750  2014.

Z. Dai  Z. Yang  Y. Yang  W. W. Cohen  J. Carbonell  Q. V. Le  and R. Salakhutdinov. Transformer-xl:
Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860  2019.

J. Devlin  M.-W. Chang  K. Lee  and K. Toutanova. Bert: Pre-training of deep bidirectional

transformers for language understanding. arXiv preprint arXiv:1810.04805  2018.

K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

G. Huang  Z. Liu  L. Van Der Maaten  and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 4700–4708  2017.

S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning 
ICML 2015  Lille  France  6-11 July 2015  pages 448–456  2015.

G. Klambauer  T. Unterthiner  A. Mayr  and S. Hochreiter. Self-normalizing neural networks. In

Advances in neural information processing systems  pages 971–980  2017.

Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

J. Lei Ba  J. R. Kiros  and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 

2016.

M. P. Marcus  B. Santorini  and M. A. Marcinkiewicz. Building a large annotated corpus of english:

The penn treebank. Computational Linguistics  19(2):313–330  1993.

M. Ott  S. Edunov  A. Baevski  A. Fan  S. Gross  N. Ng  D. Grangier  and M. Auli. fairseq: A fast 

extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038  2019.

B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Association for Computational Linguistics (ACL) 
pages 115–124  2005.

K. Papineni  S. Roukos  T. Ward  and W. Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics  July 6-12  2002  Philadelphia  PA  USA.  pages 311–318  2002.

M. Ranzato  S. Chopra  M. Auli  and W. Zaremba. Sequence level training with recurrent neural
networks. In 4th International Conference on Learning Representations  ICLR 2016  San Juan 
Puerto Rico  May 2-4  2016  Conference Track Proceedings  2016.

S. Santurkar  D. Tsipras  A. Ilyas  and A. Madry. How does batch normalization help optimization?

In Advances in Neural Information Processing Systems  pages 2483–2493  2018.

R. Socher  A. Perelygin  J. Wu  J. Chuang  C. D. Manning  A. Ng  and C. Potts. Recursive deep
models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013
conference on empirical methods in natural language processing  pages 1631–1642  2013.

10

I. Sutskever  O. Vinyals  and Q. V. Le. Sequence to sequence learning with neural networks. In

Advances in neural information processing systems  pages 3104–3112  2014.

D. Ulyanov  A. Vedaldi  and V. S. Lempitsky. Instance normalization: The missing ingredient for fast

stylization. CoRR  abs/1607.08022  2016.

A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  L. Kaiser  and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017  4-9 December 2017  Long Beach 
CA  USA  pages 6000–6010  2017.

S. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. In
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing 
EMNLP 2016  Austin  Texas  USA  November 1-4  2016  pages 1296–1306  2016.

Y. Wu and K. He. Group normalization.

In Computer Vision - ECCV 2018 - 15th European
Conference  Munich  Germany  September 8-14  2018  Proceedings  Part XIII  pages 3–19  2018.

C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires
rethinking generalization. In 5th International Conference on Learning Representations  ICLR
2017  Toulon  France  April 24-26  2017  Conference Track Proceedings  2017.

H. Zhang  Y. N. Dauphin  and T. Ma. Fixup initialization: Residual learning without normalization.

CoRR  abs/1901.09321  2019.

11

,Dmitry Ostrovsky
Zaid Harchaoui
Anatoli Juditsky
Arkadi Nemirovski
Jingjing Xu
Xu Sun
Zhiyuan Zhang
Guangxiang Zhao
Junyang Lin