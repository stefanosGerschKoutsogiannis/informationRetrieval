2009,Slow  Decorrelated Features for Pretraining Complex Cell-like Networks,We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1.  A single-hidden-layer neural network of this kind of model achieves 1.5% error on MNIST.  We also introduce an existing criterion for learning slow  decorrelated features as a pretraining strategy for image models.  This pretraining strategy results in orientation-selective features  similar to the receptive fields of complex cells.  With this pretraining  the same single-hidden-layer model achieves better generalization error  even though the pretraining sample distribution is very different from the fine-tuning distribution.  To implement this pretraining strategy  we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.,Slow  Decorrelated Features for

Pretraining Complex Cell-like Networks

James Bergstra

University of Montreal

james.bergstra@umontreal.ca

Yoshua Bengio

University of Montreal

yoshua.bengio@umontreal.ca

Abstract

We introduce a new type of neural network activation function based on recent
physiological rate models for complex cells in visual area V1. A single-hidden-
layer neural network of this kind of model achieves 1.50% error on MNIST.
We also introduce an existing criterion for learning slow  decorrelated features
as a pretraining strategy for image models. This pretraining strategy results in
orientation-selective features  similar to the receptive ﬁelds of complex cells. With
this pretraining  the same single-hidden-layer model achieves 1.34% error  even
though the pretraining sample distribution is very different from the ﬁne-tuning
distribution. To implement this pretraining strategy  we derive a fast algorithm for
online learning of decorrelated features such that each iteration of the algorithm
runs in linear time with respect to the number of features.

1

Introduction

Visual area V1 is the ﬁrst area of cortex devoted to handling visual input in the human visual sys-
tem (Dayan & Abbott  2001). One convenient simpliﬁcation in the study of cell behaviour is to
ignore the timing of individual spikes  and to look instead at their frequency. Some cells in V1
are described well by a linear ﬁlter that has been rectiﬁed to be non-negative and perhaps bounded.
These so-called simple cells are similar to sigmoidal activation functions: their activity (ﬁring fre-
quency) is greater as an image stimulus looks more like some particular linear ﬁlter. However  these
simple cells are a minority in visual area V1 and the characterization of the remaining cells there
(and even beyond in visual areas V2  V4  MT  and so on) is a very active area of ongoing research.
Complex cells are the next-simplest kind of cell. They are characterized by an ability to respond to
narrow bars of light with particular orientations in some region (translation invariance) but to turn off
when all those overlapping bars are presented at once. This non-linear response has been modeled
by quadrature pairs (Adelson & Bergen  1985; Dayan & Abbott  2001): pairs of linear ﬁlters with
the property that the sum of their squared responses is constant for an input image with particular
spatial frequency and orientation (i.e. edges). It has also been modeled by max-pooling across two
or more linear ﬁlters (Riesenhuber & Poggio  1999). More recently  it has been argued that V1 cells
exhibit a range of behaviour that blurs distinctions between simple and complex cells and between
energy models and max-pooling models (Rust et al.  2005; Kouh & Poggio  2008; Finn & Ferster 
2007).
Another theme in neural modeling is that cells do not react to single images  they react to image
sequences. It is a gross approximation to suppose that each cell implements a function from image
to activity level. Furthermore  the temporal sequence of images in a video sequence contains a lot
of information about the invariances that we would like our models to learn. Throwing away that
temporal structure makes learning about objects from images much more difﬁcult. The principle
of identifying slowly moving/changing factors in temporal/spatial data has been investigated by
many (Becker & Hinton  1993; Wiskott & Sejnowski  2002; Hurri & Hyv¨arinen  2003; K¨ording
et al.  2004; Cadieu & Olshausen  2009) as a principle for ﬁnding useful representations of images 

1

and as an explanation for why V1 simple and complex cells behave the way they do. A good
overview can be found in (Berkes & Wiskott  2005).
This work follows the pattern of initializing neural networks with unsupervised learning (pretrain-
ing) before ﬁne-tuning with a supervised learning criterion. Supervised gradient descent explores the
parameter space sufﬁciently to get low training error on smaller training sets (tens of thousands of
examples  like MNIST). However  models that have been pretrained with appropriate unsupervised
learning procedures (such as RBMs and various forms of auto-encoders) generalize better (Hinton
et al.  2006; Larochelle et al.  2007; Lee et al.  2008; Ranzato et al.  2008; Vincent et al.  2008).
See Bengio (2009) for a comprehensive review and Erhan et al. (2009) for a thorough experimental
analysis of the improvements obtained. It appears that unsupervised pretraining guides the learning
dynamics in better regions of parameter space associated with basins of attraction of the supervised
gradient procedure corresponding to local minima with lower generalization error  even for very
large training sets (unlike other regularizers whose effects tend to quickly vanish on large training
sets) with millions of examples.
Recent work in the pretraining of neural networks has taken a generative modeling perspective. For
example  the Restricted Boltzmann Machine is an undirected graphical model  and training it (by
maximum likelihood) as such has been demonstrated to also be a good initialization. However  it is
an interesting open question whether a better generative model is necessarily (or even typically) a
better point of departure for ﬁne-tuning. Contrastive divergence (CD) is not maximum likelihood 
and works just ﬁne as pretraining. Reconstruction error is an even poorer approximation of the
maximum likelihood gradient  and sometimes works better than CD (with additional twists like
sparsity or the denoising of (Vincent et al.  2008)).
The temporal coherence and decorrelation criterion is an alternative to training generative models
such as RBMs or auto-encoder variants. Recently (Mobahi et al.  2009) demonstrated that a slowness
criterion regularizing the top-most internal layer of a deep convolutional network during supervised
learning helps their model to generalize better. Our model is similar in spirit to pre-training with
the semi-supervised embedding criterion at each level (Weston et al.  2008; Mobahi et al.  2009) 
but differs in the use of decorrelation as a mechanism for preventing trivial solutions to a slowness
criterion. Whereas RBMs and denoising autoencoders are deﬁned for general input distributions 
the temporal coherence and decorrelation criterion makes sense only in the context of data with
slowly-changing temporal or spatial structure  such as images  video  and sound.
In the same way that simple cell models were the inspiration for sigmoidal activation units in arti-
ﬁcial neural networks and validated simple cell models  we investigate in artiﬁcial neural network
classiﬁers the value of complex cell models. This paper builds on these results by showing that
the principle of temporal coherence is useful for ﬁnding initial conditions for the hidden layer of
a neural network that biases it towards better generalization in object recognition. We introduce
temporal coherence and decorrelation as a pretraining algorithm. Hidden units are initialized so that
they are invariant to irrelevant transformations of the image  and sensitive to relevant ones. In order
for this criterion to be useful in the context of large models  we derive a fast online algorithm for
decorrelating units and maximizing temporal coherence.

2 Algorithm

2.1 Slow  decorrelated feature learning algorithm

(K¨ording et al.  2004) introduced a principle (and training criterion) to explain the formation of
complex cell receptive ﬁelds. They based their analysis on the complex-cell model of (Adelson &
Bergen  1985)  which describes a complex cell as a pair of half-rectiﬁed linear ﬁlters whose outputs
are squared and added together and then a square root is applied to that sum.
Suppose x is an input

p(ui · x)2 + (vi · x)2. (K¨ording et al.  2004) showed that by minimizing the following cost 

image and we have F complex cells h1  ...  hF such that hi =

(hi t − hi t−1)2

Var(hi)

(1)

X

i!=j

LK2004 = α

+X

X

t

i

Covt(hi  hj)2
Var(hi)Var(hj)

2

over consecutive natural movie frames (with respect to model parameters)  the ﬁlters ui and vi of
each complex cell form local Gabor ﬁlters whose phases are offset by about 90 degrees  like the sine
and cosine curves that implement a Fourier transform.
The criterion in Equation 1 requires a batch minimization algorithm because of the variance and
covariance statistics that must be collected. This makes the criterion too slow for use with large
datasets. At the same time  the size of the covariance matrix is quadratic in the number of features  so
it is computationally expensive (perhaps prohibitively) to apply the criterion to train large numbers
of features.

2.1.1 Online Stochastic Estimation of Covariance

This section presents an algorithm for approximately minimizing LK2004 using an online algorithm
whose iterations run in linear time with respect to the number of features. One way to apply the
criterion to large or inﬁnite datasets is by estimating the covariance (and variance) from consecutive
minibatches of N movie frames. Then the cost can be minimized by stochastic gradient descent.
We used an exponentially-decaying moving average to track the mean of each feature over time.

¯hi(t) = ρ¯hi(t − 1) + (1 − ρ)hi(t)

For good results  ρ should be chosen so that the estimates change very slowly. We used a value of
1.0 − 5.0 × 10−5.
Then we estimated the variance of each feature over a minibatch like this:

Var(h) ≈ 1

N − 1

(hi(t) − ¯hi(t))2

t+N−1X

τ =t

With this mean and variance  we computed normalized features for each minibatch:

zi(t) = (hi(t) − ¯hi(t))/pVar(h) + 10−10

Letting Z denote an F × N matrix with N columns of F normalized feature values  we estimate the
N Z(t)Z(t)0.
correlation between features hi by the covariance in these normalized features: C(t) = 1
We can now write down L(t)  a minibatch-wise approximation to Eq. 1:

X

i!=j

C 2

ij(t) +

N−1X

X

τ =0

i

L(t) = α

(zi(t + τ) − zi(t + τ − 1))2

(2)

The time complexity of evaluating L(t) from Z using this expression is O(F F N +N F ). In practice
we use small minibatches and our model has lots of features  so the fact that the time complexity of
the algorithm is quadratic in F is troublesome.
There is  however  a way to compute this value exactly in time linear in F . The key observation
is that the sum of the squared elements of C can be computed from the N × N Gram matrix
G(t) = Z(t)0Z(t).

1
N 2 Tr(Z(t)0Z(t)Z(t)0Z(t))

FX

FX

i=1

C 2

ij(t) = Tr(C(t)C(t))

j=1
1
N 2 Tr(Z(t)Z(t)0Z(t)Z(t)0) =
1
NX
N 2 Tr(G(t)G(t)) =
1
kl(t) .=
G2
N 2

1
N 2 Tr(G(t)G(t)0)
1
N 2|Z(t)0Z(t)|2

NX

=

=

=

k=1

l=1

3

Subtracting the C 2
that suggests the linear-time implementation.

ii terms from the sum of all squared elements lets us rewrite Equation 2 in a way

zi(τ)2)2

+

1

N − 1

(zi(τ) − zi(τ − 1))2

(3)

 
|Z(t)Z0(t)|2 − FX

NX

(
τ =1

i=1

L(t) = α
N 2

!

N−1X

FX

τ =1

i=1

The time complexity of computing L(t) using Equation 3 from Z(t) is O(N N F ). The sum of
squared correlations is still the most expensive term  but for the case where N << F   this expression
makes L(t)’s computation linear in F . Considering that each iteration treats N training examples 
the per-training-example cost of this algorithm can be seen as O(N F ).
In implementation  an
additional factor of two in runtime can be obtained by only computing half of the Gram matrix G 
which is symmetric.

2.2 Complex-cell activation function

Recently  (Rust et al.  2005) have argued that existing models  such as that of (Adelson & Bergen 
1985) cannot account for the variety of behaviour found in visual area V1. Some complex cells
behave like simple cells to some extent and vice versa; there is a continuous range of simple to com-
plex cells. They put forward a similar but more involved expression that can capture the simple and
complex cells as special cases  but ultimately parameterizes a larger class of cell-response functions
(Eq. 4).

(cid:16)

max(0  wx)2 +PI
(cid:16)
max(0  wx)2 +PI

i=1(u(i)x)2(cid:17)ζ − δ
(cid:17)ζ

i=1(u(i)x)2

j=1(v(j)x)2(cid:17)ζ
(cid:16)PJ
(cid:17)ζ
(cid:16)PJ

j=1(v(j)x)2

+ 

β

a +

1 + γ

(4)

The numerator in Eq 4 describes the difference between an excitation term and a shunting inhibition
term. The denominator acts to normalize this difference. Parameters w  u(i)  v(j) have the same
shape as the input image x  and can be thought of as image ﬁlters like the ﬁrst layer of a neural
network or the codebook of a sparse-coding model. The parameters a  β  δ  γ    ζ are scalars that
control the range and shape of the activation function  given all the ﬁlter responses. The numbers I
and J of quadratic ﬁlters required to explain a particular cellular response were on the order of 2-16.
We introduce the approximation in Equation 5 because it is easier to learn by gradient descent. We
replaced the max operation with a softplus(x) = log(1 + ex) function so that there is always a
gradient on w and b  even when wx + b is negative. We ﬁxed the scalar parameters to prevent the
system from entering regimes of extreme non-linearity. We ﬁxed β  δ  γ   to 1  and a to 0. We chose
to ﬁx the exponent ζ to 0.5 because (Rust et al.  2005) found that values close to 0.5 offered good
ﬁts to cell ﬁring-rate data. Future work might look at choosing these constants in a principled way
or adapting them; we found that these values worked well. The range of this activation function (as
a function of x) is a connected set on the (−1  1) interval. However  the whole (−1  1) range is not
always available  depending on the parameters. If the inhibition term is always 0 for example  then
the activation function will be non-negative.

q

log(1 + ewx+b)2 +PI
q
log(1 + ewx+b)2 +PI

i=1(u(i)x)2 −qPJ
qPJ

i=1(u(i)x)2 +

1.0 +

j=1(v(j)x)2

j=1(v(j)x)2

(5)

3 Results

Classiﬁcation results were obtained by adding a logistic regression model on top of the features
learned  and treating the resulting model as a single-hidden-layer neural network. The weights of
the logistic regression were always initialized to zero.
All work was done on 28x28 images (MNIST-sized)  using a model with 300 hidden units. Each
hidden unit had one linear ﬁlter w  a bias b  two quadratic excitatory ﬁlters u1  u2 and two quadratic
inhibitory ﬁlters v1  v2. The computational cost of evaluating each unit was thus ﬁve times the cost
of evaluating a normal sigmoidal activation function of the form tanh(w0x + b).

4

3.1 Random initialization

As a baseline  our model parameters were initialized to small random weights and used as the hidden
layer of a neural network. Training this randomly-initialized model by stochastic gradient descent
yielded test-set performance of 1.56% on MNIST.
The ﬁlters learned by this procedure looked somewhat noisy for the most part  but had low-frequency
trends. For example  some of the quadratic ﬁlters had small local Gabor-like ﬁlters. We believe that
these phase-offset pairs of Gabor-like functions allow the units to implement some shift-invariant
response to edges with a speciﬁc orientation (Fig. 1).

Figure 1: Four of the three hundred activation functions learned by training our model from random
initialization to perform classiﬁcation. Top row: the red and blue channels are the two quadratic
ﬁlters of the excitation term. Bottom row: the red and blue channels are the two quadratic ﬁlters of
the shunting inhibition term. Training approximately yields locally orientation-selective edge ﬁlters 
opposite-orientation edges are inhibitory.

3.2 Pretraining with natural movies

Under the hypothesis that the matched Gabor functions (see Fig. 1) allowed our model to generalize
better across slight translations of the image  we appealed to a pretraining process to initialize our
model with values better than random noise.
We pretrained the hidden layer according to the online version of the cost in Eq. 3  using movies
(MIXED-movies) made by sliding a 28 x 28 pixel window across large photographs. Each of these
movies was short (just four frames long) and ten movies were used in each minibatch (N = 40). The
sliding speed was sampled uniformly between 0.5 and 2 pixels per frame. The sliding direction was
sampled uniformly from 0 to 2π. The sliding initial position was sampled uniformly from image
coordinates. Any sampled movie that slid off of the underlying image was rejected. We used two
photographs to generate the movies. The ﬁrst photograph was a grey-scale forest scene (resolution
1744x1308). The second photograph was a tiling of 100x100 MNIST digits (resolution 2800x2800).
As a result of this procedure  digits are not at all centered in MIXED-movies: there might part of a
’3’ in the upper-left part of a frame  and part of a ’7’ in the lower right.
The shunting inhibition ﬁlters (v1  v2) learned after ﬁve hundred thousand movies (ﬁfty thousand
iterations of stochastic gradient descent) are shown in Figure 2. The ﬁlters learn to implement
orientation-selective  shift-invariant ﬁlters at different spatial frequencies. The ﬁlters shown in ﬁg-
ure 2 have fairly global receptive ﬁelds  but smaller more local receptive ﬁelds were obtained by
applying ‘1 weight-penalization during pretraining. The α parameter that balances decorrelation
and slowness was chosen manually on the basis of the trained ﬁlters. We were looking for a diver-
sity of ﬁlters with relatively low spatial frequency. The excitatory ﬁlters learned similar Gabor pairs
but the receptive ﬁelds tended to be both smaller (more localized) and lower-frequency. Fine-tuning
this pre-trained model with a learning rate of 0.003 with L1 weight decay of 10−5 yielded a test
error rate of 1.34% on MNIST.

3.3 Pretraining with MNIST movies

We also tried pretraining with videos whose frames follow a similar distribution to the images used
for ﬁne-tuning and testing. We created MNIST movies by sampling an image from the training set 
and moving around (translating it) according to a Brownian motion. The initial velocity was sampled
from a zero-mean normal distribution with std-deviation 0.2. Changes in that velocity between each

5

Figure 2: Filters from some of the units of the model  pretrained on small sliding image patches from
two large images. The features learn to be direction-selective for moving edges by approximately
implementing windowed Fourier transforms. These features have global receptive ﬁeld  but become
more local when an ‘1 weight penalization is applied during pretraining. Excitatory ﬁlters looked
similar  but tended to be more localized and with lower spatial frequency (fewer  shorter  broader
stripes). Columns of the ﬁgure are arranged in triples: linear ﬁlter w in grey  u(1)  u(2) in red and
green  v(1)  v(2) in blue and green.

frame were sampled from zero-mean normal distribution with std-deviation 0.2. Furthermore  the
digit image in each frame was modiﬁed according to a randomly chosen elastic deformation  as
in (Loosli et al.  2007). As before  movies of four frames were created in this way and training
was conducted on minibatches of ten movies (N = 4 ∗ 10 = 40). Unlike the mnist frames in
MIXED-movies  the frames of MNIST-movies contain a single digit that is approximately centered.
The activation functions learned by minimizing Equation 3 on these MNIST movies were quali-
tatively different from the activation functions learned from the MIXED movies. The inhibitory
weights (v1  v2) learned from MNIST movies are shown in 3. Once again  the inhibitory weights
exhibit the narrow red and green stripes that indicate edge-orientation selectivity. But this time they
are not parallel straight stripes  they follow contours that are adapted to digit edges. The excita-
tion ﬁlters u1  u2 were also qualitatively different. Instead of forming localized Gabor pairs  some
formed large smooth blob-like shapes but most converged toward zero. Fine-tuning this pre-trained
model with a learning rate of 0.003 with L1 weight decay of 10−5 yielded a test error rate of 1.37
% on MNIST.

Figure 3: Filters of our model  pretrained on movies of centered MNIST training images subjected
to Brownian translation. The features learn to be direction-selective for moving edges by approx-
imately implementing windowed Fourier transforms. The ﬁlters are tuned to the higher spatial
frequency in MNIST digits  as compared with the natural scene. Columns of the ﬁgure are arranged
in triples: linear ﬁlter w in grey  u(1)  u(2) in red and green  v(1)  v(2) in blue and green.

6

Table 1: Generalization error (% error) from 100 labeled MNIST examples after pretraining on
MIXED-movies and MNIST-movies.

Pre-training Dataset Number of pretraining iterations (×104)

0

1

2

3

4

5

MIXED-movies
MNIST-movies

23.1
23.1

21.2
19.0

20.8
18.7

20.8
18.8

20.6
18.4

20.6
18.6

4 Discussion

The results on MNIST compare well with many results in the literature. A single-hidden layer neural
network of sigmoidal units can achieve 1.8% error by training from random initial conditions  and
our model achieves 1.5% from random initial conditions. A single-hidden layer sigmoidal neural
network pretrained as a denoising auto-encoder (and then ﬁne-tuned) can achieve 1.4% error on
average  and our model is able to achieve 1.34% error from many different ﬁne-tuned models (Erhan
et al.  2009). Gaussian SVMs trained just on the original MNIST data achieve 1.4%; our pretraining
strategy allows our single-layer model be better than Gaussian SVMs (Decoste & Sch¨olkopf  2002).
Deep learning algorithms based on denoising auto-encoders and RBMs are typically able to achieve
slightly lower scores in the range of 1.2 − 1.3% (Hinton et al.  2006; Erhan et al.  2009). The
best convolutional architectures and models that have access to enriched datasets for ﬁne-tuning can
achieve classiﬁcation accuriacies under 0.4% (Ranzato et al.  2007). In future work  we will explore
strategies for combining these methods and with our decorrelation criterion to train deep networks
of models with quadratic input interactions. We will also look at comparative performance on a
wider variety of tasks.

4.1 Transfer learning  the value of pretraining

To evaluate our unsupervised criterion of slow  decorrelated features as a pretraining step for clas-
siﬁcation by a neural network  we ﬁne-tuned the weights obtained after ten  twenty  thirty  forty 
and ﬁfty thousand iterations of unsupervised learning. We used only a small subset (the ﬁrst 100
training examples) from the MNIST data to magnify the importance of pre-training. The results
are listed in Table 1. Training from random weights initial led to 23.1 % error. The value of pre-
training is evident right away: after two unsupervised passes over the MNIST training data (100K
movies and 10K iterations)  the weights have been initialized better. Fine-tuning the weights learned
on the MIXED-movies led to test error rate of 21.2%  and ﬁne-tuning the weights learned on the
MNIST-movies led to a test error rate of 19.0%. Further pretraining offers a diminishing marginal
return  although after ten unsupervised passes through the training data (500K movies) there is no
evidence of over-pretraining. The best score (20.6%) on MIXED-movies occurs at both eight and
ten unsupervised passes  and the best score on MNIST-movies (18.4%) occurs after eight. A larger
test set would be required to make a strong conclusion about a downward trend in test set scores
for larger numbers of pretraining iterations. The results with MNIST-movies pretraining are slightly
better than MIXED-movies but these results suggest strong transfer learning: the videos featuring
digits in random locations and natural image patches are almost as good for pretraining as compared
with videos featuring images very similar to those in the test set.

4.2 Slowness in normalized features encourages binary activations

Somewhat counter-intuitively  the slowness criterion requires movement in the features h. Suppose
a feature hi has activation levels that are normally distributed around 0.1 and 0.2  but the activation
at each frame of a movie is independent of previous frames. Since the features has a small variance 
then the normalized feature zi will oscillate in the same way  but with unit variance. This will cause
zi(t) − zi(t − 1) to be relatively high  and for our slowness criterion not to be well satisﬁed. In this
way the lack of variance in hi can actually make for a relatively fast normalized feature zi rather
than a slow one.
However  if hi has activation levels that are normally distributed around .1 and .2 for some image
sequences and around .8 and .9 for other image sequences  the marginal variance in hi will be larger.

7

The larger marginal variance will make the oscillations between .1 and .2 lead to much smaller
changes in the normalized feature zi(t). In this sense  the slowness objective can be maximally
satisﬁed by features hi(t) that take near-minimum and near-maximum values for most movies  and
never transition from a near-minimum to a near-maximum value during a movie.
When training on multiple short videos instead of one continuous one  it is possible for large changes
in normalized-feature-activation never [or rarely] to occur during a video. Perhaps this is one of the
roles of saccades in the visual system: to suspend the normal objective of temporal coherence during
a rapid widespread change of activation levels.

4.3 Eigenvalue interpretation of decorrelation term

What does our unsupervised cost mean? One way of thinking about the decorrelation term (ﬁrst
term in Eq. 1) which helped us to design an efﬁcient algorithm for computing it  is to think of it as
ﬂattening the eigen-spectrum of the correlation matrix of our features h (over time). It is helpful to
rewrite this cost in terms of normalized features: zi = hi− ¯hi
  and to consider that we sum over all
the elements of the correlation matrix including the diagonal.

σi

X

i!=j

Covt(hi  hj)2
Var(hi)Var(hj)

= 2

F−1X

FX

 FX

FX

 − F

Covt(zi  zj)2 =

Covt(zi  zj)2

i=1

j=i+1

i=1

j=1

If we use C to denote the matrix whose i  j entry is Covt(zi  zj)  and we use U0ΛU to denote the
eigen-decomposition of C  then we can transform this sum over i! = j further.

FX

FX

(

Covt(zi  zj)2) − F = Tr(C0C) − F = Tr(CC) − F

i=1

j=1

= Tr(U0ΛU U0ΛU) − F = Tr(U U0ΛU U0Λ) − F =

FX

k − F
Λ2

We can interpret the ﬁrst term of Eq. 1 as penalizing the squared eigenvalues of the covariance
matrix between features in a normalized feature space (z as opposed to h)  or as minimizing the
squared eigenvalues of the correlation matrix between features h.

k=1

5 Conclusion

We have presented an activation function for use in neural networks that is a simpliﬁcation of a
recent rate model of visual area V1 complex cells. This model learns shift-invariant  orientation-
selective edge ﬁlters from purely supervised training on MNIST and achieves lower generalization
error than conventional neural nets.
Temporal coherence and decorrelation has been put forward as a principle for explaining the func-
tional behaviour of visual area V1 complex cells. We have described an online algorithm for min-
imizing correlation that has linear time complexity in the number of hidden units. Pretraining our
model with this unsupervised criterion yields even lower generalization error: better than Gaus-
sian SVMs  and competitive with deep denoising auto-encoders and 3-layer deep belief networks.
The good performance of our model compared with poorer approximations of V1 is encouraging
machine learning research inspired by neural information processing in the brain. It also helps to
validate the corresponding computational neuroscience theories by showing that these neuron acti-
vations and unsupervised criteria have value in terms of learning.

Acknowledgments

This research was performed thanks to funding from NSERC  MITACS  and the Canada Research
Chairs.

8

References
Adelson  E. H.  & Bergen  J. R. (1985). Spatiotemporal energy models for the perception of motion.

Journal of the Optical Society of America  2  284–99.

Becker  S.  & Hinton  G. E. (1993). Learning mixture models of spatial coherence. Neural Compu-

tation  5  267–277.

Bengio  Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learn-

ing  to appear.

Berkes  P.  & Wiskott  L. (2005). Slow feature analysis yields a rich repertoire of complex cell

properties. Journal of Vision  5  579–602.

Cadieu  C.  & Olshausen  B. (2009). Learning transformational invariants from natural movies. In

Advances in neural information processing systems 21 (nips’08)  209–216. MIT Press.

Dayan  P.  & Abbott  L. F. (2001). Theoretical neuroscience. The MIT Press.
Decoste  D.  & Sch¨olkopf  B. (2002). Training invariant support vector machines. Machine Learn-

ing  46  161–190.

Erhan  D.  Manzagol  P.-A.  Bengio  Y.  Bengio  S.  & Vincent  P. (2009). The difﬁculty of training
deep architectures and the effect of unsupervised pre-training. AISTATS’2009 (pp. 153–160).
Clearwater (Florida)  USA.

Finn  I.  & Ferster  D. (2007). Computational diversity in complex cells of cat primary visual cortex.

Journal of Neuroscience  27  9638–48.

Hinton  G. E.  Osindero  S.  & Teh  Y. (2006). A fast learning algorithm for deep belief nets. Neural

Computation  18  1527–1554.

Hurri  J.  & Hyv¨arinen  A. (2003). Temporal coherence  natural image sequences  and the visual

cortex. Advances in Neural Information Processing Systems 15 (NIPS’02) (pp. 141–148).

K¨ording  K. P.  Kayser  C.  Einh¨auser  W.  & K¨onig  P. (2004). How are complex cell properties

adapted to the statistics of natural stimuli? Journal of Neurophysiology  91  206–212.

Kouh  M. M.  & Poggio  T. T. (2008). A canonical neural circuit for cortical nonlinear operations.

Neural Computation  20  1427–1451.

Larochelle  H.  Erhan  D.  Courville  A.  Bergstra  J.  & Bengio  Y. (2007). An empirical evaluation
ICML 2007 (pp. 473–480).

of deep architectures on problems with many factors of variation.
Corvallis  OR: ACM.

Lee  H.  Ekanadham  C.  & Ng  A. (2008). Sparse deep belief net model for visual area V2. In

Advances in neural information processing systems 20 (nips’07). Cambridge  MA: MIT Press.

Loosli  G.  Canu  S.  & Bottou  L. (2007). Training invariant support vector machines using selec-
tive sampling. In L. Bottou  O. Chapelle  D. DeCoste and J. Weston (Eds.)  Large scale kernel
machines  301–320. Cambridge  MA.: MIT Press.

Mobahi  H.  Collobert  R.  & Weston  J. (2009). Deep learning from temporal coherence in video.

ICML 2009. ACM. To appear.

Ranzato  M.  Boureau  Y.  & LeCun  Y. (2008). Sparse feature learning for deep belief networks.

NIPS 20.

Ranzato  M.  Poultney  C.  Chopra  S.  & LeCun  Y. (2007). Efﬁcient learning of sparse representa-

tions with an energy-based model. NIPS 19.

Riesenhuber  M.  & Poggio  T. (1999). Hierarchical models of object recognition in cortex. Nature

Neuroscience  2  1019–1025.

Rust  N.  Schwartz  O.  Movshon  J. A.  & Simoncelli  E. (2005). Spatiotemporal elements of

macaque V1 receptive ﬁelds. Neuron  46  945–956.

Vincent  P.  Larochelle  H.  Bengio  Y.  & Manzagol  P.-A. (2008). Extracting and composing robust

features with denoising autoencoders. ICML 2008 (pp. 1096–1103). ACM.

Weston  J.  Ratle  F.  & Collobert  R. (2008). Deep learning via semi-supervised embedding. ICML

2008 (pp. 1168–1175). New York  NY  USA: ACM.

Wiskott  L.  & Sejnowski  T. (2002). Slow feature analysis: Unsupervised learning of invariances.

Neural Computation  14  715–770.

9

,Moustapha Cisse
Nicolas Usunier
Thierry Artières
Patrick Gallinari