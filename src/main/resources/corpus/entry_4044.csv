2013,Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions,We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that  under a mixing assumption  achieves $O(\sqrt{T\log|\Pi|}+\log|\Pi|)$ regret with respect to a comparison set of policies $\Pi$.  The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set $\Pi$ has polynomial size  this algorithm is efficient.  We also consider the episodic adversarial online shortest path problem.  Here  in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. For randomly chosen graphs and adversarial losses  this problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses.  When both graphs and losses are adversarially chosen  we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.  Finally  we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise  a notoriously difficult problem that has been used to design efficient cryptographic schemes.,Online Learning in Markov Decision Processes with

Adversarially Chosen Transition Probability

Distributions

Yasin Abbasi-Yadkori

Queensland University of Technology

yasin.abbasiyadkori@qut.edu.au

Peter L. Bartlett

UC Berkeley and QUT

bartlett@eecs.berkeley.edu

Varun Kanade
UC Berkeley

vkanade@eecs.berkeley.edu

Yevgeny Seldin

Queensland University of Technology
yevgeny.seldin@gmail.com

Csaba Szepesv´ari
University of Alberta

szepesva@cs.ualberta.ca

Abstract

We study the problem of online learning Markov Decision Processes (MDPs)
when both the transition distributions and loss functions are chosen by an ad-
versary. We present an algorithm that  under a mixing assumption  achieves

O(pT log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧.

The regret is independent of the size of the state and action spaces. When expec-
tations over sample paths can be computed efﬁciently and the comparison set ⇧
has polynomial size  this algorithm is efﬁcient.
We also consider the episodic adversarial online shortest path problem. Here  in
each episode an adversary may choose a weighted directed acyclic graph with an
identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a
path that minimizes the loss while traversing from the start to ﬁnish node. At the
end of each episode the loss function (given by weights on the edges) is revealed
to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed
policy for selecting paths. This problem is a special case of the online MDP
problem. It was shown that for randomly chosen graphs and adversarial losses 
the problem can be efﬁciently solved. We show that it also can be efﬁciently
solved for adversarial graphs and randomly chosen losses. When both graphs and
losses are adversarially chosen  we show that designing efﬁcient algorithms for
the adversarial online shortest path problem (and hence for the adversarial MDP
problem) is as hard as learning parity with noise  a notoriously difﬁcult problem
that has been used to design efﬁcient cryptographic schemes. Finally  we present
an efﬁcient algorithm whose regret scales linearly with the number of distinct
graphs.

1

Introduction

In many sequential decision problems  the transition dynamics can change with time. For example 
in steering a vehicle  the state of the vehicle is determined by the actions taken by the driver  but
also by external factors  such as terrain and weather conditions. As another example  the state of a

1

robot that moves in a room is determined both by its actions and by how people in the room interact
with it. The robot might not have inﬂuence over these external factors  or it might be very difﬁcult
to model them. Other examples occur in portfolio optimization  clinical trials  and two player games
such as poker.
We consider the problem of online learning Markov Decision Processes (MDPs) when the transition
probability distributions and loss functions are chosen adversarially and are allowed to change with
time. We study the following game between a learner and an adversary:

1. The (oblivious) adversary chooses a sequence of transition kernels mt and loss functions `t.
2. At time t:
space A.

(a) The learner observes the state xt in state space X and chooses an action at in the action
(b) The new state xt+1 2X is drawn at random according to distribution mt(·|xt  at).
(c) The learner observes the transition kernel mt and the loss function `t  and suffers the loss

`t(xt  at).

in this game. Then the loss of the policy ⇡ isPT
a policy ⇡ 2 ⇧ is deﬁned as the random variable RT (A  ⇡) =PT

To handle the case when the representation of mt or `t is very large  we assume that the learner has
a black-box access to mt and `t. The above game is played for a total of T rounds and the total
loss suffered by the learner isPT
t=1 `t(xt  at). In the absence of state variables  the MDP problem
reduces to a full information online learning problem (Cesa-Bianchi and Lugosi [1]). The difﬁculty
with MDP problems is that  unlike full information online learning problems  the choice of a policy
at each round changes the future states and losses.
A policy is a mapping ⇡ : X! A  where A denotes the set of distributions over A. To evaluate
the learner’s performance  we imagine a hypothetical game where at each round the action played is
chosen according to a ﬁxed policy ⇡  and the transition kernels mt and loss functions `t are the same
as those chosen by the oblivious adversary. Let (x⇡
t ) denote a sequence of state and action pairs
t ). Deﬁne a set ⇧ of policies that will be
t   a⇡
used as a benchmark to evaluate the learner’s performance. The regret of a learner A with respect to
t ).
The goal in adversarial online learning is to design learning algorithms for which the regret with
respect to any policy grows sublinearly with T   the total number of rounds played. Algorithms with
such a guarantee  somewhat unfortunately  are typically termed no-regret algorithms.
We also study a special case of this problem: the episodic online adversarial shortest path problem.
Here  in each episode the adversary chooses a layered directed acyclic graph with a unique start and
ﬁnish node. The adversary also chooses a loss function  i.e.  a weight for every edge in the graph.
The goal of the learning algorithm is to choose a path from start to ﬁnish that minimizes the total
loss. The loss along any path is simply the sum of the weights on the edges. At the end of the round
the graph and the loss function are revealed to the learner. The goal  as in the case of the online
MDP problem  is to minimize regret with respect to a class of policies for choosing the path. Note
that the online shortest path problem is a special case of the online MDP problem; the states are the
nodes in the graph and the transition dynamics is speciﬁed by the edges.

t=1 `t(xt  at) PT

t   a⇡
t=1 `t(x⇡

t=1 `t(x⇡

t   a⇡

1.1 Related Work

Burnetas and Katehakis [2]  Jaksch et al. [3]  and Bartlett and Tewari [4] propose efﬁcient algorithms
for ﬁnite MDP problems with stochastic transitions and loss functions. These results are extended
to MDPs with large state and action spaces in [5  6  7]. Abbasi-Yadkori and Szepesv´ari [5] and
Abbasi-Yadkori [6] derive algorithms with O(pT ) regret for linearly parameterized MDP problems 
while Ortner and Ryabko [7] derive O(T (2d+1)/(2d+2)) regret bounds under a Lipschitz assumption 
where d is the dimensionality of the state space. We note that these algorithms are computationally
expensive.
Even-Dar et al. [8] consider the problem of online learning MDPs with ﬁxed and known dynamics 
but adversarially changing loss functions. They show that when the transition kernel satisﬁes a
mixing condition (see Section 3)  there is an algorithm with regret bound O(pT ). Yu and Mannor [9 
10] study a harder setting  where the transition dynamics may also change adversarially over time.

2

However  their regret bound scales with the amount of variation in the transition kernels and in the
worst case may grow linearly with time.
Recently  Neu et al. [11] give a no-regret algorithm for the episodic shortest path problem with
adversarial losses but stochastic transition dynamics.

1.2 Our Contributions

First  we study a general MDP problem with large (possibly continuous) state and action spaces
and adversarially changing dynamics and loss functions. We present an algorithm that guarantees
O(pT ) regret with respect to a suitably small (totally bounded) class of policies ⇧ for this online
MDP problem. The regret grows with the metric entropy of ⇧  so that if the comparison class is
the set of all policies (that is  the algorithm must compete with the optimal ﬁxed policy)  it scales
polynomially with the size of the state and action spaces. The above algorithm is efﬁcient as long
as the comparison class has polynomial size and we can compute expectations over sample paths
for each policy. This result has several advantages over the results of [5  6  7]. First  the transition
distributions and loss functions are chosen adversarially. Second  by designing an appropriate small
class of comparison policies  the algorithm is efﬁcient  even in the face of very large state and action
spaces.
Next  we present efﬁcient no-regret algorithms for the episodic online shortest path problem for two
cases: when the graphs and loss functions (edge weights) are chosen adversarially and the set of
graphs is small; and when the graphs are chosen adversarially  but the loss is stochastic.
Finally  we show that for the general adversarial online shortest path problem  designing an efﬁcient
no-regret algorithm is at least as hard as learning parity with noise. Since the online shortest path
problem is a special case of online MDP problem  the hardness result is also applicable there.1 The
noisy parity problem is widely believed to be computationally intractable  and has been used to
design cryptographic schemes.
Organization: In Section 3 we introduce an algorithm for MDP problems with adversarially chosen
transition kernels and loss functions. Section 4 discusses how this algorithm can also be applied to
the online episodic shortest path problem with adversarially varying graphs and loss functions and
also considers the case of stochastic loss functions. Finally  in Section 4.2  we show the reduction
from the adversarial online epsiodic shortest path problem to learning parity with noise.

2 Notations
Let X⇢ Rn be a state space and A⇢ Rd be an action space. Let S be the space of probability
distributions over a set S. Deﬁne a policy ⇡ as a mapping ⇡ : X! A. We use ⇡(a|x) to
denote the probability of choosing an action a in state x under policy ⇡. A random action under
policy ⇡ is denoted by ⇡(x). A transition probability kernel (or transition kernel) m is a mapping
m : X⇥A! X . For ﬁnite X   let P (⇡  m) be the transition probability matrix of policy ⇡ under
transition kernel m. A loss function is a bounded real-valued function over state and action spaces 
` : X⇥A! R. For a vector v  deﬁne kvk1 =Pi |vi|. For a real-valued function f deﬁned over
X⇥A   deﬁne kfk1 1 = maxx2XPa2A |f (x  a)|. The inner product between two vectors v and
w is denoted by hv  wi.
3 Online MDP Problems

In this section  we study a general MDP problem with large state and action spaces. The adversary
can change the dynamics and the loss functions  but is restricted to choose dynamics that satisfy a
mixing condition.
Assumption A1 Uniform Mixing There exists a constant ⌧> 0 such that for all distributions
d and d0 over the state space  any deterministic policy ⇡  and any transition kernel m 2 M 
kdP (⇡  m)  d0P (⇡  m)k1  e1/⌧ kd  d0k1.
this claim has since been retracted [12  13].

1There was an error in the proof of a claimed hardness result for the online adversarial MDP problem [8];

3

Choose ⇡1 uniformly at random.
for t := 1  2  . . .   T do

For all policies ⇡ 2 ⇧  w⇡ 0 = 1. ⌘ = min{plog |⇧| /T   1/2}.
Learner takes the action at ⇠ ⇡t(.|xt) and adversary chooses mt and `t.
Learner suffers loss `t(xt  at) and observes mt and `t. Update state: xt+1 ⇠ mt(.|xt  at).
For all policies ⇡  w⇡ t = w⇡ t1(1  ⌘)E[`t(x⇡
Wt =P⇡2⇧ w⇡ t. For any ⇡  p⇡ t+1 = w⇡ t/Wt.
With probability t = w⇡t t/w⇡t t1 choose the previous policy  ⇡t+1 = ⇡t  while with
probability 1  t  choose ⇡t+1 based on the distribution p⇡ t+1.

t  ⇡)].

end for

Figure 1: OMDP: The Online Algorithm for Markov Decision Processes

This assumption excludes deterministic MDPs that can be more difﬁcult to deal with. As discussed
by Neu et al. [14]  if Assumption A1 holds for deterministic policies  then it holds for all policies.
We propose an exponentially-weighted average algorithm for this problem. The algorithm  called
OMDP and shown in Figure 1  maintains a distribution over the policy class  but changes its policy
with a small probability. The main results of this section are the following regret bounds for the
OMDP algorithm. The proofs can be found in Appendix A.
Theorem 1. Let the loss functions selected by the adversary be bounded in [0  1]  and the transition
kernels selected by the adversary satisfy Assumption A1. Then  for any policy ⇡ 2 ⇧ 

E [RT (OMDP ⇡ )]  (4 + 2⌧ 2)pT log |⇧| + log |⇧| .

Corollary 2. Let ⇧ be an arbitrary policy space  N (✏) be the ✏-covering number of the metric space
(⇧ k.k1 1)  and C(✏) be an ✏-cover. Assume that we run the OMDP algorithm on C(✏). Then  under
the same assumptions as in Theorem 1  for any policy ⇡ 2 ⇧ 

E [RT (OMDP ⇡ )]  (4 + 2⌧ 2)pT log N (✏) + log N (✏) + ⌧T✏ .

Remark 3. If we choose ⇧ to be the space of deterministic policies and X and A are ﬁnite spaces 
from Theorem 1 we obtain that E [RT (OMDP ⇡ )]  (4 + 2⌧ 2)pT|X| log |A| + |X| log |A|. This
result  however  is not sufﬁcient to show that the average regret with respect to the optimal stationary
policy converges to zero. This is because  unlike in the standard MDP framework  the optimal
stationary policy is not necessarily deterministic. Corollary 2 extends the result of Theorem 1 to
continuous policy spaces.
In particular  if X and A are ﬁnite spaces and ⇧ is the space of all policies  N (✏)  (|A|/✏)|A||X| 
so the expected regret satisﬁes E [RT (OMDP ⇡ )]  (4+2⌧ 2)qT|A||X| log |A|✏ +|A||X| log |A|✏ +

T   we get that E [RT (OMDP ⇡ )] = O(⌧ 2pT |A||X| log(|A|T )).

⌧T✏ . By the choice of ✏ = 1

3.1 Proof Sketch

The main idea behind the design and the analysis of our algorithm is the following regret decompo-
sition:

RT (A  ⇡) =

TXt=1

TXt=1

`t(xA

t   at) 

`t(x⇡t

t  ⇡ t) +

`t(x⇡t

t  ⇡ t) 

`t(x⇡

t  ⇡ )  

(1)

TXt=1

TXt=1

where A is an online learning algorithm that generates a policy ⇡t at round t  xA
t if we have followed the policies generated by algorithm A  and `(x  ⇡) = `(x  ⇡(x)). Let

t is the state at round

BT (A) =

TXt=1

`t(xA

t   at) 

TXt=1

`t(x⇡t

t  ⇡ t)   CT (A  ⇡) =

TXt=1

`t(x⇡t

t  ⇡ t) 

TXt=1

`t(x⇡

t  ⇡ ) .

4

Note that the choice of policies has no inﬂuence over future losses in CT (A  ⇡). Thus  CT (A  ⇡)
can be bounded by a reduction to full information online learning algorithms. Also  notice that the
competitor policy ⇡ does not appear in BT (A). In fact  BT (A) depends only on the algorithm A.
We will show that if the class of transition kernels satisﬁes Assumption A1 and algorithm A rarely
changes its policies  then BT (A) can be bounded by a sublinear term. To be more precise  let ↵t
be the probability that algorithm A changes its policy at round t. We will require that there exists
a constant D such that for any 1  t  T   any sequence of models m1  . . .   mt and loss functions
`1  . . .  ` t  ↵t  D/pt.
We would like to have a full information online learning algorithm that rarely changes its policy.
The ﬁrst candidate that we consider is the well-known Exponentially Weighted Average (EWA)
algorithm [15  16]. In our MDP problem  the EWA algorithm chooses a policy ⇡ 2 ⇧ accord-
s  ⇡ )]⌘ for some > 0. The policies that this
EWA algorithm generates most likely are different in consecutive rounds and thus  the EWA algo-
rithm might change its policy frequently. However  a variant of EWA  called Shrinking Dartboard
(SD) (Guelen et al. [17])  rarely changes its policy (see Lemma 8 in Appendix A). The OMDP al-
gorithm is based on the SD algorithm. Note that the algorithm needs to know the number of rounds 
T   in advance. Also note that we could use any rarely switching algorithm such as Follow the Lazy
Leader of Kalai and Vempala [18] as the subroutine.

ing to distribution qt(⇡) / exp⇣Pt1

s=1 E [`s(x⇡

4 Adversarial Online Shortest Path Problem

We consider the following adversarial online shortest path problem with changing graphs. The
problem is a repeated game played between a decision-maker and an (oblivious) adversary over
T rounds. At each round t the adversary presents a directed acyclic graph gt on n nodes to the
decision maker  with L layers indexed by {1  . . .   L} and a special start and ﬁnish node. Each layer
contains a ﬁxed set of nodes and has connections only with the next layer. 2 The decision-maker
must choose a path pt from the start to the ﬁnish node. Then  the adversary reveals weights across
all the edges of the graph. The loss `t(gt  pt) of the decision-maker is the weight along the path that
the decision-maker took on that round.
Denote by [k] the set {1  2  . . .   k}. A policy is a mapping ⇡ : [n] ! [n]. Each policy may be
interpreted as giving a start to ﬁnish path. Suppose that the start node is s 2 [n]  then ⇡(i) gives the
subsequent node. The path is interpreted as follows : if at a node v  the edge (v  ⇡(v)) exists then the
next node is ⇡(v). Otherwise  the next node is an arbitrary (pre-determined) choice that is adjacent
to v. We compete against the class of such policies for choosing the shortest path. Denote the class
of such policies by ⇧. The regret of a decision-maker A with respect to a policy ⇡ 2 ⇧ is deﬁned as:
RT (A  ⇡) =PT
t=1 `t(gt ⇡ (gt))  where ⇡(gt) is the path obtained by following
the policy ⇡ starting at the source node. Note that it is possible that there exists no policy that would
result in an actual path that leads to the sink for some graph. In this case we say that the loss of the
policy is inﬁnite. Thus  there may be adversarially chosen sequences of graphs for which the regret
of a decision-maker is not well-deﬁned. This can be easily corrected by the adversary ensuring that
the graph always has some ﬁxed set of edges which result in a (possibly high loss) s ! f path.
In fact  we show that the adversary can choose a sequence of graphs and loss functions that make
this problem at least as hard as learning noisy parities. Learning noisy parities is a notoriously hard
problem in computational learning theory. The best known algorithm runs in time 2O(n/ log(n)) [20]
and the presumed hardness of this and related problems has been used for designing cryptographic
protocols [21].
Interestingly  for the hardness result to hold  it is essential that the adversary have the ability to
control both the sequence of graphs and losses. The problem is well-understood when the graphs
are generated randomly and the losses are adversarial. Jaksch et al. [3] and Bartlett and Tewari [4]
propose efﬁcient algorithms for problems with stochastic losses.3 Neu et al. [22] extend these results
to problems with adversarial loss functions.

t=1 `t(gt  pt) PT

2As noted by Neu et al. [19]  any directed acyclic graph can be transformed into a graph that satisﬁes our

assumptions.

problems with small modiﬁcations.

3These algorithms are originally proposed for continuing problems  but we can use them in shortest path

5

One can also ask what happens in the case when the graphs are chosen by the adversary  but the
weight of each edge is drawn at random according to a ﬁxed stationary distribution. In this setting 
we show a reduction to bandit linear optimization. Thus  in fact  that algorithm does not need to see
the weights of all edges at the end of the round  but only needs to know the loss it suffered.
Finally  we consider the case when both graphs and losses are chosen adversarially. Although the
general problem is at least as hard as learning noisy parities  we give an efﬁcient algorithm whose
regret scales linearly with the number of different graphs. Thus  if the adversary is forced to choose
graphs from some small set G  then we have an efﬁcient algorithm for solving the problem. We note
that in fact  our algorithm does not need to see the graph gt at the beginning of the round  in which
case an algorithm achieving O(|G|pT ) may be trivially obtained.
4.1 Stochastic Loss Functions and Adversarial Graphs

Consider the case when the weight of each edge is chosen from a ﬁxed distribution. Then it is easy
to see that the expected loss of any path is a ﬁxed linear function of the expected weights vector. The
set of available paths depends on the graph and it may change from time to time. This is an instance
of stochastic linear bandit problem  for which efﬁcient algorithms exist [23  24  25].
Theorem 4. Let us represent each path by a binary vector of length n(n  1)/2  such that the ith
element is 1 only if the corresponding edge is present in the path. Assume that the learner suffers
the loss of c(p) for choosing path p  where E [c(p)] = h`  pi and the loss vector ` 2 Rn(n1)/2 is
ﬁxed. Let Pt be the set of paths in a graph gt. Consider the CONFIDENCEBALL1 algorithm of Dani
et al. [24] applied to the shortest path problem with a changing action set Pt and the loss function
`. Then the regret with respect to the best path in each round is Cn3pT for a problem-independent
constant C.

Letb`t be the least squares estimate of ` at round t  Vt =Pt1
norm-1 ball conﬁdence set  Ct =n` : V 1/2

t

s=1 psp>s be the covariance matrix  and
Pt be the decision set at round t. The CONFIDENCEBALL1 algorithm constructs a high probability

(` b`t)1  to for an appropriate t  and chooses

an action pt according to pt = argmin`2Ct p2Pth`  pi. Dani et al. [24] prove that the regret of the
CONFIDENCEBALL1 algorithm is bounded by O(m3/2pT )  where m is the dimensionality of the
action set (in our case m = n(n  1)/2). The above optimization can be solved efﬁciently  because
only 2n corners of Ct need to be evaluated.
Note that the regret in Theorem 4 is with respect to the best path in each round  which is a stronger
result than competing with a ﬁxed policy.

4.2 Hardness Result

In this section  we show that the setting when both the graphs and the losses are chosen by an
adversary  the problem is at least as hard as the noisy parity problem. We consider the online agnostic
parity learning problem. Recall that the class of parity function over {0  1}n is the following: For
S ✓ [n]  PARS(x) = i2Sxi  where  denotes modulo 2 addition. The class is PARITIES =
{PARS | S ✓ [n]}. In the online setting  the learning algorithm is given xt 2{ 0  1}n  the learning
algorithm then picks ˆyt 2{ 0  1}  and then the true label yt is revealed. The learning algorithm
suffers loss I(ˆyt 6= yt). The regret of the learning algorithm with respect to PARITIES is deﬁned
as: Regret = PT
t=1 I(PARS(xt) 6= yt). The goal is to
design a learning algorithm that runs in time polynomial in n  T and suffers regret O(poly(n)T 1)
for some constant > 0. It follows from prior work that online agnostic learning of parities is at
least as hard as the ofﬂine version (see Littlestone [26]  Kanade and Steinke [27]). As mentioned
previously  the agnostic parity learning problem is notoriously difﬁcult. Thus  it seems unlikely that
a computationally efﬁcient no-regret algorithm for this problem exists.
Theorem 5. Suppose there is a no-regret algorithm for the online adversarial shortest path prob-
lem that runs in time poly(n  T ) and achieves regret O(poly(n)T 1) for any constant > 0.
Then there is a polynomial-time algorithm for online agnostic parity learning that achieves regret
O(poly(n)T 1). By the online to batch reduction  this would imply a polynomial time algorithm
for agnostically learning parities.

t=1 I(ˆyt 6= yt)  minPARS2PARITIESPT

6

1a

2a

3a

4a

5a

6a

2b

3b

4b

5b

6b

1  y

y

?
(a)

for t := 1  2  . . . do

Adversary chooses a graph gt 2G
for l = 1  . . .   L do

Initialize an EWA expert algorithm  E
for s = 1  . . .   t  1 do
if gs 2 C(xt l) then

Feed expert E with the value function Qs =
Q⇡s gs cs

end if
end for
Let ⇡t(.|xt l) be the distribution over actions of the expert
E
Take the action at l ⇠ ⇡t(.|xt l)  suffer the loss
ct(nt l  at l)  and move to the node nt l+1 = gt(nt l  at l)

end for
Learner observes the graph gt and the loss function ct
Compute the value function Qt = Q⇡t gt ct for all nodes
n0 2 [n]

end for

(b)

Figure 2: (a) Encoding the example (1  0  1  0  1) 2{ 0  1}5 as a graph. (b) Improved Algorithm for
the Online Shortest Path Problem.

Proof. We ﬁrst show how to map a point (x  y) to a graph and a loss function. Let (x  y) 2{ 0  1}n⇥
{0  1}. We deﬁne a graph  g(x) and a loss function `x y associated with (x  y). Deﬁne a graph on
2n + 2 nodes – named 1a  2a  2b  3a  3b  . . .   na  nb  (n + 1)a  (n + 1)b ? in that order. Let E(x)
denote the set of edges of g(x). The set E(x) contains the following edges:
(i) If x1 = 1  both (1a  2a) and (1a  2b) are in E(x)  else if x1 = 0  only (1a  2a) is present.
(ii) For 1 < i  n  if xi = 1  the edges (ia  (i + 1)a)  (ia  (i + 1)b)  (ib  (i + 1)a)  (ib  (i + 1)b)
are all present; if xi = 0 only the two edges (ia  (i + 1)a) and (ib  (i + 1)b) are present.
(iii) The two edges ((n + 1)a ?) and ((n + 1)b ?) are always present.
For the loss function  deﬁne the weights as follows. The weight of the edge ((n + 1)a ?) is y;
the weight of the edge ((n + 1)b ?) is 1  y. The weights of all the remaining edges are set to 0.
Figure 2(a) shows the encoding of the example (1  0  1  0  1) 2{ 0  1}5.
Suppose an algorithm with the stated regret bound for the online shortest path problem exists  call
it U. We will use this algorithm to solve the online parity learning problem. Let xt be an example
received; then pass the graph g(xt) to the algorithm U. The start vertex is 1a and the ﬁnish vertex
is ?. Suppose the path pt chosen by U reaches ? using the edge ((n + 1)a ?) then set ˆyt to be 0.
Otherwise  choose ˆyt = 1.
Thus  in effect we are using algorithm U as a meta-algorithm for the online agnostic parity learning
problem. First  it is easy to check that the loss suffered by the meta-algorithm on the parity problem
is exactly the same as the loss of U on the online shortest path problem. This follows directly from
the deﬁnition of the losses on the edges.
Next  we claim that for any S ✓ [n]  there is a policy ⇡S that achieves the same loss (on the online
shortest path problem) as the parity PARS does (on the parity learning problem). The policy is as
follows:
(i) From node ia  if i 2 S and (ia  (i + 1)b) 2 E(gt)  go to (i + 1)b  otherwise go to (i + 1)a.
(ii) From node ib  if i 2 S and (ib  (i + 1)a) 2 E(gt)  go to (i + 1)a  otherwise go to (i + 1)b.
(iii) Finally  from either (n + 1)a or (n + 1)b  just move to ?.
We can think of the path pt as being in type a nodes or type b nodes. For each i 2 S  such that
i = 1  the path pt switches types. Thus  if PARS(xt) = 1  pt reaches ? via the edge ((n + 1)b ?)
xt
and if PARS(xt) = 0  pt reaches ? via the edge ((n + 1)a ?). Recall that the loss function is

7

deﬁned as follows: weight of the edge ((n + 1)a ?) is yt  weight of the edge ((n + 1)b ?) is
1  yt; other edges have loss 0. Thus  the loss suffered by the policy ⇡S is 1 if PARS(xt) 6= yt
and 0 otherwise. This is exactly the loss of the parity function PARS on the agnostic parity learning
problem. Thus  if the algorithm U has regret O(poly(n)  T 1)  then the meta-algorithm for the
online agnostic parity learning problem also has regret O(poly(n)  T 1).
Remark 6. We observe that the online shortest path problem is a special case of online MDP
learning. Thus  the above reduction also shows that  short of a major breakthrough  it is unlikely
that there exists a computationally efﬁcient algorithm for the fully adversarial online MDP problem.

4.3 Small Number of Graphs
In this section  we design an efﬁcient algorithm and prove a O(|G|pT ) regret bound  where G is the
set of graphs played by the adversary up to round T . The computational complexity of the algorithm
is O(L2t) at round t. The algorithm does not need to know the set G or |G|. This regret bound holds
even if the graphs are revealed at the end of the rounds. Notice that if the graphs are shown at the
beginning of the rounds  obtaining regret bounds that scale like O(|G|pT ) is trivial; the learner only
needs to run |G| copies of the MDP-E algorithm of Even-Dar et al. [12]  one for each graph.
Let n⇡
t l denote the node at layer l of round t if we run policy ⇡. Let ct(n0  a) be the loss incurred
for taking action a in node n0 at round t.4 We construct a new graph  called G  as follows: graph G
also has a layered structure with the same number of layers  L. At each layer  we have a number of
states that represent all possible observations that we might have upon arriving at that layer. Thus 
a state at layer l has the form of x = (s  a0  n1  a1  . . .   nl1  al1  nl)  where ni belongs to layer i
and ai 2A .
Let X be the set of states in G and Xl be the set of states in layer l of G. For (x  a) 2X⇥A  
let c(x  a) = c(n(x)  a)  where n(x) is the last node observed in state x. Let g(n0  a) be the next
node under graph g if we take action a in node n0. Let g(x  a) = g(n(x)  a). Let c(x  ⇡) =
Pa ⇡(a|x)c(x  a). For a graph g and a loss function `  deﬁne the value functions by

8n0 2 [n]  Q⇡ g c(n0 ⇡ 0) = Ea⇠⇡0(n0) [c(n0  a) + Q⇡ g c(g(n0  a) ⇡ )]  

8x  s.t. g 2 C(x)  Q⇡ g c(x  ⇡0) = Q⇡ g c(n(x) ⇡ 0)  

with Q⇡ g c(f  a) = 0 for any ⇡  g  c  a where f is the ﬁnish node. Let Qt = Q⇡t gt ct denote the
value function associated with policy ⇡t at time t. For x = (s  a0  n1  a1  . . .   nl1  al1  nl)  deﬁne
C(x) = {g 2G : n1 = g(s  a0)  . . .   nl = g(nl1  al1)}  the set of graphs that are consistent
with the state x.
We can use the MDP-E algorithm to generate policies. The algorithm  however  is computationally
expensive as it updates a large set of experts at each round. Notice that the number of states at stage l 
|Xl|  can be exponential in the number of graphs. We show a modiﬁcation of the MDP-E algorithm
that would generate the same sequence of policies  with the advantage that the new algorithm is
computationally efﬁcient. The algorithm is shown in Figure 2(b). As the generated policies are
always the same  the regret bound in the next theorem  that is proven for the MDP-E algorithm  also
applies to the new algorithm. The proof can be found in Appendix B.
Theorem 7. For

2Lp8T log(2T ) +
L min{|G|   maxl |Xl|}qT log |A|2 + 2L.
The theorem gives a sublinear regret as long as |G| = o(pT ). On the other hand  the hardness
result in Theorem 5 applies when |G| =⇥( T ). Characterizing regret vs. computational complexity
tradeoffs when |G| is in between remains for future work.
References
[1] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  Learning  and Games. Cambridge University Press 

E [RT (MDP-E ⇡ )]

policy



any

⇡ 

New York  NY  USA  2006.

4Thus  `t(Gt ⇡ (Gt)) =PL

l=1 ct(n⇡

t l ⇡ ).

8

[2] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision pro-

cesses. Mathematics of Operations Research  22(1):222–255  1997.

[3] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of

Machine Learning Research  11:1563—1600  2010.

[4] P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning in

weakly communicating MDPs. In UAI  2009.

[5] Yasin Abbasi-Yadkori and Csaba Szepesv´ari. Regret bounds for the adaptive control of linear quadratic

systems. In COLT  2011.

[6] Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis  Uni-

versity of Alberta  2012.

[7] Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement

learning. In NIPS  2012.

[8] Eyal Even-Dar  Sham M. Kakade  and Yishay Mansour. Experts in a Markov decision process. In NIPS 

2004.

[9] Jia Yuan Yu and Shie Mannor. Arbitrarily modulated Markov decision processes. In IEEE Conference on

Decision and Control  2009.

[10] Jia Yuan Yu and Shie Mannor. Online learning in Markov decision processes with arbitrarily changing

rewards and transitions. In GameNets  2009.

[11] Gergely Neu  Andr´as Gy¨orgy  and Csaba Szepesv´ari. The adversarial stochastic shortest path problem

with unknown transition probabilities. In AISTATS  2012.

[12] Eyal Even-Dar  Sham M. Kakade  and Yishay Mansour. Online Markov decision processes. Mathematics

of Operations Research  34(3):726–736  2009.

[13] Eyal Even-Dar. Personal communication.  2013.
[14] Gergely Neu  Andr´as Gy¨orgy  Csaba Szepesv´ari  and Andr´as Antos. Online Markov decision processes

under bandit feedback. In NIPS  2010.

[15] Vladimir Vovk. Aggregating strategies. In COLT  pages 372–383  1990.
[16] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Compu-

tation  108(2):212–261  1994.

[17] Sascha Geulen  Berthold V¨ocking  and Melanie Winkler. Regret minimization for online buffering prob-

lems using the weighted majority algorithm. In COLT  2010.

[18] Adam Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems. Journal of Com-

puter and System Sciences  71(3):291–307  2005.

[19] Gergely Neu  Andr´as Gy¨orgy  and Csaba Szepesv´ari. The online loop-free stochastic shortest path prob-

lem. In COLT  2010.

[20] Adam Tauman Kalai  Yishay Mansour  and Elad Verbin. On agnostic boosting and parity learning. In

STOC  pages 629–638  2008.

[21] Oded Regev. On lattices  learning with errors  random linear codes  and cryptography. In STOC  pages

84–93  2005.

[22] Gergely Neu  Andr´as Gy¨orgy  and Csaba Szepesv´ari. The adversarial stochastic shortest path problem

with unknown transition probabilities. In AISTATS  2012.

[23] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning

Research  2002.

[24] V. Dani  T. P. Hayes  and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Rocco

Servedio and Tong Zhang  editors  COLT  pages 355–366  2008.

[25] Yasin Abbasi-Yadkori  D´avid P´al  and Csaba Szepesv´ari. Improved algorithms for linear stochastic ban-

dits. In NIPS  2011.

[26] Nick Littlestone. From on-line to batch learning. In COLT  pages 269–284  1989.
[27] Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. In Proceedings of the 3rd

Innovations in Theoretical Computer Science Conference  ITCS ’12  pages 11–18  2012.

9

,Yasin Abbasi Yadkori
Peter Bartlett
Varun Kanade
Yevgeny Seldin
Csaba Szepesvari
Shangtong Zhang
Shimon Whiteson