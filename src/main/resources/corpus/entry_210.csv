2018,A Smoother Way to Train Structured Prediction Models,We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum margin structured prediction objective  and paves the way for the use of fast primal gradient-based optimization algorithms. We illustrate the proposed framework by developing a novel primal incremental optimization algorithm for the structural support vector machine. The proposed algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study several practical variants. We present experimental results on two real-world problems  namely named entity recognition and visual object localization. The experimental results show that the proposed framework allows us to build upon efficient inference algorithms to develop large-scale optimization algorithms for structured prediction which can achieve competitive performance on the two real-world problems.,A Smoother Way to Train

Structured Prediction Models

Krishna Pillutla  Vincent Roulet  Sham M. Kakade  Zaid Harchaoui

Paul G. Allen School of Computer Science & Engineering and Department of Statistics

University of Washington

name@uw.edu

Abstract

We present a framework to train a structured prediction model by performing
smoothing on the inference algorithm it builds upon. Smoothing overcomes the
non-smoothness inherent to the maximum margin structured prediction objective 
and paves the way for the use of fast primal gradient-based optimization algorithms.
We illustrate the proposed framework by developing a novel primal incremental
optimization algorithm for the structural support vector machine. The proposed
algorithm blends an extrapolation scheme for acceleration and an adaptive smooth-
ing scheme and builds upon the stochastic variance-reduced gradient algorithm.
We establish its worst-case global complexity bound and study several practical
variants. We present experimental results on two real-world problems  namely
named entity recognition and visual object localization. The experimental results
show that the proposed framework allows us to build upon efﬁcient inference
algorithms to develop large-scale optimization algorithms for structured prediction
which can achieve competitive performance on the two real-world problems.

1

Introduction

Consider the optimization problem arising when training structural support vector machines:

(cid:34)

n(cid:88)

i=1

1
n

(cid:35)

min
w∈Rd

F (w) :=

f (i)(w) +

(cid:107)w(cid:107)2

2

λ
2

 

(1)

where each f (i) is the structural hinge loss. Structural support vector machines were designed for
prediction problems where outputs are discrete data structures such as sequences or trees [59  65].
Batch nonsmooth optimization algorithms such as cutting plane methods are appropriate for problems
with small or moderate sample sizes [65  21]. Stochastic nonsmooth optimization algorithms such as
stochastic subgradient methods can tackle problems with large sample sizes [49  57]. However both
families of methods achieve the typical worst-case complexity bounds of nonsmooth optimization
algorithms and cannot easily leverage a possible hidden smoothness of the objective.
Furthermore  as signiﬁcant progress is being made on incremental smooth optimization algorithms
for training unstructured prediction models [36]  we would like to transfer such advances and design
faster optimization algorithms to train structured prediction models. Indeed if each term in the
ﬁnite-sum were L-smooth 1  incremental optimization algorithms such as MISO [37]  SAG [33  53] 
SAGA [10]  SDCA [55]  and SVRG [23] could leverage the ﬁnite-sum structure of the objective (1)
and achieve faster convergence than batch algorithms on large-scale problems.

1We say f is L-smooth with respect to (cid:107)·(cid:107) when ∇f exists everywhere and is L-Lipschitz with respect to

(cid:107)·(cid:107). Smoothness and strong convexity are taken to be with respect to (cid:107) · (cid:107)2 unless stated otherwise.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Incremental optimization algorithms can be further accelerated  either on a case-by-case basis [56 
14  1  9] or using the Catalyst acceleration scheme [35  36]  to achieve near-optimal convergence
rates [67]. Accelerated incremental optimization algorithms demonstrate stable and fast convergence
behavior on a wide range of problems  in particular for ill-conditioned ones.
We introduce a general framework that allows us to bring the power of accelerated incremental
optimization algorithms to the realm of structured prediction problems. To illustrate our framework 
we focus on the problem of training a structural support vector machine (SSVM). The same ideas can
be applied to other structured prediction models to obtain faster training algorithms.
We seek primal optimization algorithms  as opposed to saddle-point or primal-dual optimization
algorithms  in order to be able to tackle structured prediction models with afﬁne mappings such as
SSVM as well as deep structured prediction models with nonlinear mappings. We show how to shade
off the inherent non-smoothness of the objective while still being able to rely on efﬁcient inference
algorithms.
Smooth inference oracles. We introduce a notion of smooth inference oracles that gracefully ﬁts
the framework of black-box ﬁrst-order optimization. While the exp inference oracle reveals the
relationship between max-margin and probabilistic structured prediction models  the top-K inference
oracle can be efﬁciently computed using simple modiﬁcations of efﬁcient inference algorithms in
many cases of interest.
Incremental optimization algorithms. We present a new algorithm built on top of SVRG  blending
an extrapolation scheme for acceleration and an adaptive smoothing scheme. We establish the worst-
case complexity bounds of the proposed algorithm and demonstrate its effectiveness compared to
competing algorithms on two tasks  namely named entity recognition and visual object localization.
The code is publicly available on the authors’ websites. All the proofs are provided in [48].

2 Smoothing Inference for Structured Prediction
Given an input x ∈ X of arbitrary structure  e.g. a sentence  a structured prediction model outputs its
prediction as a structured object y ∈ Y  such as a parse tree  where the set of all outputs Y may be
ﬁnite yet often large. The score function φ  parameterized by w ∈ Rd  quantiﬁes the compatibility
of an input x and an output y as φ(x  y; w). It is assumed to decompose onto the structure at hand
such that the inference problem y∗(x; w) ∈ argmaxy∈Y φ(x  y; w) can be solved efﬁciently by a
combinatorial optimization algorithm. Training a structured prediction model then amounts to ﬁnding
the best score function such that the inference procedure provides correct predictions.
Structural hinge loss. The standard formulation uses a feature map Φ : X ×Y → Rd such that score
functions are linear in w  i.e. φ(x  y; w) = Φ(x  y)(cid:62)w. The structural hinge loss  an extension of
binary and multi-class hinge losses  considers a majorizing surrogate of a given loss function (cid:96) such
as the Hamming loss  that measures the error incurred by predicting y∗(x; w) on a sample (x  y) as
(cid:96)(y  y∗(x; w)). For an input-output pair (xi  yi)  the structural hinge loss is deﬁned as
y(cid:48)∈Y ψi(y(cid:48); w)  

y(cid:48)∈Y {φ(xi  y(cid:48); w) + (cid:96)(yi  y(cid:48))} − φ(xi  yi; w) = max

f (i)(w) = max

(2)

where ψi(y(cid:48); w) := φ(xi  y(cid:48); w) + (cid:96)(yi  y(cid:48)) − φ(xi  yi; w) = a(cid:62)
i y(cid:48)w + bi y(cid:48) is the augmented
score function  an afﬁne function of w. The loss (cid:96) is also assumed to decompose onto the structure so
that the maximization in (2)  also known as loss augmented inference  is no harder than the inference
problem consisting in computing y∗(x; w). The learning problem (1) is the minimization of the
structural hinge losses on the training data (xi  yi)n
i=1 with a regularization penalty. We shall refer to
a generic term f (w) = maxy(cid:48)∈Y ψ(y(cid:48); w) in the ﬁnite-sum from now on.
Smoothing strategy. To smooth the structural hinge loss  we decompose it as the composition of
the max function with a linear mapping. The former can then be easily smoothed through its dual
formulation to obtain a smooth surrogate of (2). Formally  deﬁne the mapping g and the max function
h respectively as

(cid:26)Rm → R

 

(3)

(cid:40)Rd → Rm

g :

w (cid:55)→ (ψ(y(cid:48); w))y(cid:48)∈Y = Aw + b

 

h :

z

(cid:55)→ maxi∈[m] zi

where m = |Y|. The structural hinge loss can now be expressed as f = h ◦ g.

2

(a) Non-smooth.

(b) (cid:96)2

2 smoothing.

(c) Entropy smoothing.

Figure 1: Viterbi trellis for a chain graph with four nodes and three labels.

The max function can be written as h(z) = maxi∈[m] zi = maxu∈∆m−1 z(cid:62)u where ∆m−1 is the
probability simplex in Rm. Its simplicity allows us to analytically compute its inﬁmal convolution
with a smooth function [2]. The smoothing hµω of h by a strongly convex function ω with smoothing
coefﬁcient µ > 0 is deﬁned as

(cid:8)z(cid:62)u − µω(u)(cid:9)  

hµω(z) := maxu∈∆m−1

whose gradient is the maximizer of the above expression. The smooth approximation of the structural
hinge loss is then given by fµω := hµω◦g. This smoothing technique was introduced by Nesterov [43]
who showed that if ω is 1-strongly convex with respect to (cid:107) · (cid:107)α  then fµω is ((cid:107)A(cid:107)2
2 α/µ)-smooth2 
and approximates f for any w as
µ minu∈∆m−1 ω(u) ≤ f (w) − fµω(w) ≤ µ maxu∈∆m−1 ω(u) .

Smoothing variants. We focus on the negative entropy and the squared Euclidean norm as choices
for ω  denoted respectively

i=1 ui log ui

and

2(u) := 1
(cid:96)2

2 ((cid:107)u(cid:107)2

2 − 1) .

−H(u) :=(cid:80)m
(cid:104)

(cid:80)m

(cid:105)

(cid:110)

[K]u − µ(cid:96)2
z(cid:62)

(cid:111)

The gradient of their corresponding smooth counterparts can be computed respectively by the softmax
and the orthogonal projection onto the simplex  i.e.

∇h−µH (z) =

exp(zi/µ)
j=1 exp(zj /µ)

i=1 ... m

and

∇hµ(cid:96)2

2

(z) = proj∆m−1 (z/µ) .

The gradient of the smooth surrogate fµω can be written using the chain rule. This involves computing
∇g along all m = |Y| of its components  which may be intractable. However  for the (cid:96)2
2 smoothing 
the gradient ∇hµ(cid:96)2
(z) is given by the projection of z/µ onto the simplex  which selects a small
number  denoted Kz/µ  of its largest coordinates. We shall approximate this projection by ﬁxing K
independently of z/µ and deﬁning

2

2

 

2(u)

u∈∆K−1

hµ K(z) = max
(z)  where z[K] ∈ RK denote the K largest components of z. If
as an approximation of hµ(cid:96)2
Kz/µ < K this approximation is exact and for ﬁxed z  this holds for small enough µ  as shown
in [48]. The resulting surrogate is denoted fµ K = hµ K ◦ g.
Smooth inference oracles. We deﬁne a smooth inference oracle as a ﬁrst-order oracle for a smooth
counterpart of the structural hinge loss. Recall that a ﬁrst-order oracle for a function f is a numerical
routine which  given a point w ∈ dom(f )  returns the function value f (w) and a (sub)gradient
v ∈ ∂f (w). We deﬁne three variants of a smooth inference oracle: i) the max oracle; ii) the exp
oracle; iii) the top-K oracle. The max oracle corresponds to the usual inference oracle in maximum
margin structured prediction  while the exp oracle and the the top-K oracle correspond resp. to the
entropy-based and (cid:96)2
Figure 1 illustrates the notion on a chain structured output. The inference problem is non-smooth
and a small change in w might lead to a radical change in the best scoring path as shown in Fig. 1a.
The (cid:96)2
2-based smooth inference amounts to picking some number of the top scoring paths. Notice the
sparsity pattern in Fig. 1b. The entropy-based smooth inference amounts to weighting all paths  with
a higher weight for top scoring paths as shown in Fig. 1c.

2-based smoothing.

2 (cid:107)A(cid:107)β α := max{u(cid:62)Aw |(cid:107)u(cid:107)α ≤ 1   (cid:107)w(cid:107)β ≤ 1}.

3

ShesellsseashellsShesellsseashellsShesellsseashellsTable 1: Smooth inference oracles  algorithms and complexity. Here  p is the size of each y ∈ Y.
The time complexity is phrased in terms of the time complexity T of the max oracle.

Max oracle

Algo

Dynamic

Programming

Graph cut

Graph matching

Branch and
Bound search

Top-K oracle

Algo

Time

Exp oracle
Algo

Top-K DP

O(KT log K)

Sum-Product

Time
O(T )

BMMF

BMMF

Top-K search

O(pKT )
O(KT )
N/A

Intractable

Intractable

Intractable

Deﬁnition 1. Consider f (w) = maxy(cid:48)∈Y ψ(y(cid:48); w) and w ∈ Rd 
• the max oracle returns f (w) and ∇ψ(y∗; w) ∈ ∂f (w)  where y∗ ∈ argmaxy(cid:48)∈Y ψ(y(cid:48); w);
• the exp oracle returns f−µH (w) and ∇f−µH (w) = Ey(cid:48)∼pµ[∇ψ(y(cid:48); w)]  where pµ(y(cid:48)) ∝
exp(ψ(y(cid:48); w)/µ);
• the top-K oracle computes the K best outputs {y(i)}K

ψ(y(1); w) ≥ ··· ≥ ψ(y(K); w) ≥ max
y(cid:48)∈Y\YK
(w) and ∇fµ(cid:96)2

to return fµ K(w) and ∇fµ K(w) as surrogates for fµ(cid:96)2

2

i=1 = YK satisfying
ψ(y(cid:48); w)

(w).

2

On the one hand  the entropy-based smoothing of a structural support vector machine somewhat
interpolates between a regular structural support vector machine and a conditional random ﬁeld [31]
through the smoothing parameter µ. On the other hand  the (cid:96)2
2-based smoothing only requires a top-K
oracle  making it a more practical option  as illustrated in Table 1.
Smooth inference algorithms. The implementation of inference oracles depends on the structure of
the output  given by a probabilistic graphical model [48]. When the latter is a tree  exact procedures
are available  otherwise some algorithms may not be practical. See Table 1 for a summary3. The
formal description  algorithms and proofs of correctness are provided in [48].
Dynamic Programming. For graphs with a tree structure or bounded tree-width  the max oracle is
implemented by dynamic programming (DP) algorithms such as the popular Viterbi algorithm. The
exp-oracle can be achieved by replacing the max in DP with log-sum-exp and using back-propagation
at O(1) times the cost of the max oracle. The top-K oracle is implemented by the top-K DP
algorithm which keeps track of the K largest intermediate scores and the back-pointers at O(K)
times the cost of the max oracle; see [48] for details.
Graph cut and matching. For speciﬁc probabilistic graphical models  exact inference is possible in
loopy graphs by the use of graph cuts [27] or perfect matchings in bipartite graphs [60]. In this case 
a top-K oracle can be implemented by the best max marginal ﬁrst (BMMF) algorithm [68] at 2K
computations of max-marginals  which can be efﬁciently computed for graph cuts [26] and matchings
[12]. The exp oracle is intractable (in fact  it is #P-complete) [20].
Branch and bound search. In special cases  branch and bound search allows exact inference in loopy
graphs by partitioning Y and exploring promising parts ﬁrst using a heuristic. Examples include the
celebrated efﬁcient subwindow search [32] in computer vision or A(cid:63) algorithm in natural language
processing [34  18]. Here  the top-K oracle can be implemented by letting the search run until K
outputs are found while the exp oracle is intractable. The running time of both the max and top-K
oracles depends on the heuristic used and might be exponential in the worst case.

3 The notation O(·) may hide constants and factors logarithmic in problem parameters. See [48] for detailed

complexities.

4

(cid:34)

(cid:35)

 

(5)

n(cid:88)

i=1

1
n

Algorithm 1 Catalyst with smoothing
1: Input: Objective F in (1)  linearly convergent method M  initial w0  α0 ∈ (0  1).
2: Initialize: z0 = w0.
3: for k = 1 to T do
4:

Using M with zk−1 as the starting point  ﬁnd

Smoothing (µk)k≥1 and regularization (κk)k≥1 parameters  relative accuracies (δk)k≥1.

wk ≈ argmin
w∈Rd

Fµkω κk (w; zk−1) :=

f (i)
µkω(w) +

(cid:107)w(cid:107)2

2 +

λ
2

κk
2

(cid:107)w − zk−1(cid:107)2

2

such that Fµkω κk (wk; zk−1) − minw Fµkω κk (w; zk−1) ≤ δkκk
Compute αk and βk such that

2 (cid:107)wk − zk−1(cid:107)2
2.

5:

k(κk+1 + λ) = (1 − αk)α2
α2

k−1(κk + λ) + αkλ  

βk = αk−1(1−αk−1)(κk+λ)

k−1(κk+λ)+αk(κk+1+λ) .
α2

Set zk = wk + βk(wk − wk−1).

6:
7: end for
8: return wT .

3 Catalyst with smoothing

2(cid:107)w(cid:107)2

For a single input-output pair (n = 1)  the problem (1) is minw∈Rd h(Aw + b) + λ
2  where
h is a simple non-smooth convex function. The Nesterov smoothing technique overcomes the non-
smoothness of the objective by considering a smooth surrogate instead [43  42]. We combine this
with the Catalyst scheme to accelerate a linearly-convergent smooth optimization algorithm [36].
Catalyst with smoothing. The Catalyst approach considers at each outer iteration a regularized
objective centered around the current iterate [36]. The algorithm proceeds by performing approximate
proximal point steps  that is from a point z and for a step-size 1/κ one computes the minimizer
of minw∈Rm F (w) + κ
2. We only need an approximate solution returned by a given
optimization method M that enjoys a linear convergence guarantee.
We extend the Catalyst approach to non-smooth optimization problems by performing adaptive
smoothing in the outer-loop and adjusting the level of accuracy accordingly in the inner-loop. We
deﬁne

2(cid:107)w − z(cid:107)2

(cid:107)w(cid:107)2

λ
2

(cid:107)w − z(cid:107)2

2

κ
2

i=1

2 +

f (i)
µkω(w) +

Fµω κ(w; z) :=

(4)
as a smooth surrogate to the objective centered around a given point z ∈ Rd. Note that the original
Catalyst considered a ﬁxed regularization term κ [36]  while we vary κ and µ. Doing so enables us to
get adaptive smoothing strategies.
The proposed inner-outer scheme is presented in Algorithm 1. In view of the strong convexity of
Fµkω κk (· ; zk−1)  the stopping criterion for the subproblem (5) can be checked by looking at the
gradient of Fµkω κk (· ; zk−1). As it is smooth and strongly convex  the maximal number of iterations
to satisfy the stopping criterion can also be derived. In practice  however  we recommend a practical
variant similar in spirit to the one proposed by [36] that lets M run for a ﬁxed budget of iterations in
each inner loop. Below  we denote w∗ ∈ argminw∈Rd F (w) and F ∗ = F (w∗).
Theorem 1. Consider problem (1) and a smoothing function ω s.t. −D ≤ ω(u) ≤ 0 for all u ∈ ∆.
Assume parameters (µk)k≥1  (κk)k≥1  (δk)k≥1 of Algorithm 1 are non-negative with (µk)k≥1
non-increasing  δk ∈ [0  1)  and αk ∈ (0  1) for all k. Then  Algorithm 1 generates (wk)k≥0 such
that

n(cid:88)

1
n

F (wk) − F ∗ ≤ Ak−1
0Bk
j =(cid:81)k
j =(cid:81)k

i=j(1− αi)  Bk

1

where Ak
and µ0 = 2µ1.

∆0 + µkD +

(µj−1 − (1 − δj)µj) D  

(6)

i=j(1− δi)  ∆0 = F (w0)− F ∗ + (κ1+λ)α2
2(1−α0)

0−λα0

(cid:107)w0 − w∗(cid:107)2
2  

k(cid:88)

j=1

Ak−1
jBk

j

5

Table 2: Summary of global complexity of SC-SVRG  i.e.  Algorithm 1 with SVRG as the inner solver for
various parameter settings. We show E[N ]  the expected total number of SVRG iterations required to obtain an
accuracy   up to constants and factors logarithmic in problem parameters. We denote ∆F0 := F (w0) − F ∗
and ∆0 = (cid:107)w0 − w∗(cid:107)2. Constants a and D are deﬁned so that −D ≤ ω(u) ≤ 0 for all u ∈ ∆ and each f (i)
is a/µ-smooth for i ∈ [n].

µω

Scheme

λ > 0

1

2

3

4

Yes

Yes

No

No

µk


D

κk
n − λ

aD

µck

λ

δk

(cid:113) λn

aD

c(cid:48)

/D

aD/n

1/k2

n

µ/k

κ0 k

1/k2

n +

n + a
λ

(cid:113) ∆F0
(cid:16)
(cid:98)∆0



E[N ]

(cid:113) aDn

λ

∆F0+µD

µ
√

(cid:17)



 +

aDn∆0

n + a
µκ0

Remark

ﬁx  in advance

c  c(cid:48) < 1 are universal constants

ﬁx  in advance

(cid:98)∆0 = ∆F0 + κ0

2 ∆2

0 + µD

Theorem 1 establishes the complexity of the Catalyst smoothing scheme for a general smoothing
function and a general linearly-convergent smooth optimization algorithm M.
Using Theorem 1  we can derive strategies for strongly or non-strongly convex objectives (λ > 0 or
not) with adaptive smoothing that vanishes over time to get progressively better surrogates of the
original objective.
The global complexity of the algorithm depends then on the choice of M. We present in Table 2 the
total complexity for different strategies when SVRG [23] is used as M  resulting in an algorithm
called SC-SVRG in the remainder of the paper. Note that the adaptive smoothing schemes (2  4) do
not match the rate obtained by a ﬁxed smoothing (1  3). A standard doubling trick can easily ﬁx this.
Yet we choose to use an adaptive smoothing scheme  easier to use and working well in practice (see
Sec. 4). All proofs are given in [48].
Extension to nonlinear mappings. When the score function is not linear in w  the overall problem
is not convex in general. However  if the score function is smooth  then one could take advantage
of the composite structure of the structural hinge loss f = h ◦ g by using the prox-linear algorithm
[6  11]. At each step  the latter linearizes the mapping g around the current iterate wk  resulting in a
convex model w (cid:55)→ h(wk + ∇g(wk)(cid:62)(w − wk)) of h ◦ g around wk. The overall convex model
of the objective F with an additional proximal term is then minimized. The next iterate is given by

n(cid:88)

i=1

wk+1 = argmin
w∈Rd

1
n

h(g(i)(w) + ∇g(i)(wk)(cid:62)(w − wk)) +

(cid:107)w(cid:107)2

2 +

λ
2

1
2γ

(cid:107)w − wk(cid:107)2

(7)

where g(i) is the mapping associated with the ith sample and γ > 0 is the parameter of the proximal
term. This subproblem reduces to training a structured prediction model with an afﬁne augmented
score function. Therefore we can solve it with the SC-SVRG algorithm introduced earlier. Note that
only approximate solutions are required to get a global convergence to a stationary point [11]. The
theoretical analysis and numerical experiments showing the potential of this approach compared to
subgradient methods can be found in [48].

4 Experiments

We compare the proposed algorithm and several competing algorithms to train a structural support
vector machine on the tasks of named entity recognition and visual object localization. Additional
details on the datasets  algorithms  parameters as well as an extensive evaluation in different settings
can be found in [48]. We use the (cid:96)2
Named entity Recognition. The task consists in predicting the tagging of a sequence into named
entities. We consider the CoNLL 2003 dataset with n = 14987 [63]. The Viterbi algorithm provides
an efﬁcient max oracle and the top-K oracle is obtained following the discussion in Sec. 2. The loss
(cid:96) is the Hamming loss here. The features Φ(x  y) are obtained from the local context around each
word [64]. We use the F1 score as the performance metric for evaluation.

2-based smoothing in all experiments as explained in Sec. 2.

6

(a) Performance on CoNLL-2003 for named entity recognition.

(b) Sample performance on PASCAL VOC 2007 for visual object localization for λ = 10/n.

Figure 2: Experimental comparison of proposed methods for the tasks of named entity recognition (Fig. 2a)
and visual object localization (Fig. 2b). All shaded areas represent one standard deviation over ten random runs.
See [48] for all plots.

parameters to be tuned  and returns the averaged iterate wt = 2/t(t + 1)(cid:80)t

Visual object localization. The task consists in predicting the spatial location of the visual object
in an image. We consider the PASCAL VOC 2007 [13] dataset and focus on the “cat” and “dog”
categories. Additional experimental results for other categories can be found in [48]. We train
an independent classiﬁer for each class. We follow the methodology outlined in [15] to construct
Φ(x  y). We crop image x to the bounding box y  resize the resulting patch and pass it through
a convolutional network pre-trained on a different dataset. We use here AlexNet [28] pre-trained
on ImageNet [50] and take as Φ(x  y) the output of the layer conv4. We use selective search to
restrict |Y| to 1000 [66]. The max and top-K oracles are implemented as exhaustive searches over
this reduced set. We use 1 − IoU as the task loss where IoU(y  y(cid:48)) = Area(y ∩ y(cid:48))/Area(y ∪ y(cid:48)).
Moreover  the ground truth label y is replaced by argmaxy(cid:48)∈Y IoU(y  y(cid:48)). We use the average
precision (AP) as the performance metric for evaluation [13] .
Methods. The plots compare two non-smooth optimization methods  (a) SGD  which is a primal
stochastic subgradient method with step-sizes chosen as γt = γ0/(1 + t/t0)  where γ0  t0 are
j=1 jwj [29]  and (b)
BCFW  the Block-Coordinate Frank-Wolfe algorithm [30]  with the tuning of the parameters proposed
by the authors  and the averaged iterate as above (bcfw-wavg). The methods that use smoothing
are SVRG [23] with constant smoothing and two variants of SC-SVRG  namely SC-SVRG-const 
which uses constant smoothing (Scheme 1 in Table 2) and SC-SVRG-adapt  which uses adaptive
smoothing (Scheme 2 in Table 2). Note that the step-size scheme of SGD does not follow from a
classical theoretical analysis  yet performs better in practice than the one used by Pegasos [57].
Parameters. BCFW requires no tuning  while SGD requires the tuning of γ0 and t0. The SVRG-
based methods require the tuning of a ﬁxed learning rate. Moreover  SVRG and SC-SVRG-const
also require tuning the amount of smoothing µ. The validation F1 score and the train loss are used as
the tuning criteria for named entity recognition and visual object localization respectively. A ﬁxed
budget Tinner = n is used as the stopping criteria in Algorithm 1. This corresponds to the one-pass
heuristic of [36]  who found the theoretical stopping criteria to be overly pessimistic. We use the
value κk = λ for SC-SVRG-adapt. All smooth optimization methods turned out to be robust to the
choice of K for the top-K oracle (Fig. 3) - we use K = 5 for named entity recognition and K = 10
for visual object localization.
Experiments. We present in Fig. 2 the convergence behavior of the different methods on the named
entity recognition and visual object localization tasks. We plot the error on the training set vs. the
number of oracle calls and the performance metric on a held-out set vs. the number of oracle calls.

7

020406080100#(oracle calls)/n1001.1×100lossTrain loss  =0.01/n020406080100#(oracle calls)/n0.750.760.770.780.79F1Val. F1  =0.01/n020406080100#(oracle calls)/n1001.2×100lossTrain loss  =0.1/n020406080100#(oracle calls)/n0.7600.7650.7700.7750.7800.7850.790F1Val. F1  =0.1/nSC-SVRG-constSC-SVRG-adaptSVRGBCFWSGD (1/t)051015202530#(oracle calls)/n1002×1013×1014×1016×101lossTrain loss (cat)051015202530#(oracle calls)/n0.380.400.420.44APVal. AP (cat)051015202530#(oracle calls)/n1003×1014×1016×101lossTrain loss (dog)051015202530#(oracle calls)/n0.200.230.250.280.30APVal. AP (dog)SC-SVRG-constSC-SVRG-adaptSVRGBCFWSGD (1/t)(a) Effect of smoothing hyperparameter on SC-SVRG-const and SC-SVRG-adapt for CoNLL-2003.

(b) Effect of hyperparameter K on SC-SVRG-const and SC-SVRG-adapt for CoNLL-2003.
Figure 3: Effect of hyperparameters µ and K on SC-SVRG-const and SC-SVRG-adapt.

As we can see in Fig. 2  the proposed methods converge faster in terms of training error while
achieving a competitive performance in terms of the performance metric on a held-out set. Further-
more  BCFW and SGD make twice as many actual passes as SVRG based algorithms. In Fig. 3  we
explore the effect of the parameters µ and K on the convergence of the different methods. We can
see that SC-SVRG-adapt is rather robust to the choice of µ  while SC-SVRG-const and SVRG are
more sensitive to the choice of µ. Therefore SC-SVRG-adapt seems to appear as the most practical
variant of our approach. We can also notice that SC-SVRG-adapt is rather robust to the choice of K.
Setting K = 5 is sufﬁcient here to obtain competitive results.

5 Related Work

The general framework for global training of structured prediction models was introduced in [4] and
applied to handwriting recognition in [3] and to document processing in [5].
Smooth inference oracles. Smooth inference oracles with (cid:96)2
2-smoothing echo older heuristics in
speech and language processing [25]. In the probabilistic graphical models literature  efﬁcient
algorithms to solve the top-K inference combinatorial optimization problems were studied under
the name “M-best MAP” in [54  45  68]. See [48] for a longer survey. Previous works considering
smooth inference oracles yet encompassed by our framework can be found in [22  24  51  41].
Instances of smooth inference oracles framed in the context of ﬁrst-order optimization were studied
in [58  69] and in [38]. We framed here a general notion of smooth inference oracles in the context of
ﬁrst-order optimization. The framework not only includes previously proposed inference oracles but
also introduces new ones.
Related ideas to ours appear in the independent works [39  44]. These works partially overlap with
ours  but the papers choose different perspectives  making them complementary to each other. In [39] 
the authors proceed differently when  e.g.  smoothing inference based on dynamic programming.
Moreover  they do not establish complexity bounds for optimization algorithms making calls to the
resulting smooth inference oracles. We deﬁne smooth inference oracles in the context of black-box
ﬁrst-order optimization and establish worst-case complexity bounds for incremental optimization
algorithms making calls to these oracles. Indeed we relate the amount of smoothing controlled by µ
to the resulting complexity of the optimization algorithms relying on smooth inference oracles.
Batch and incremental optimization algorithms. Several families of algorithms for structural
support vector machines were proposed. Table 3 gives an overview with their oracle complexities.
Early works [59  65  21  62] considered batch dual quadratic optimization (QP) algorithms.

8

020406080100#(oracle calls)/n1002×1003×1004×100train lossSC-SVRG-const  =102/n020406080100#(oracle calls)/n1002×1003×1004×100train lossSC-SVRG-const  =1/n020406080100#(oracle calls)/n1002×1003×1004×100train lossSC-SVRG-adapt  =102/n020406080100#(oracle calls)/n1002×1003×1004×100train lossSC-SVRG-adapt  =1/n = 1.0 = 8.0 = 32.0 = 2.0 = 8.0 = 32.0020406080100#(oracle calls)/n100lossTrain loss  SC-SVRG-const020406080100#(oracle calls)/n0.770.780.79F1Val. F1  SC-SVRG-const020406080100#(oracle calls)/n100lossTrain loss  SC-SVRG-adapt020406080100#(oracle calls)/n0.770.780.79F1Val. F1  SC-SVRG-adaptK=2K=5K=10Table 3: Convergence rates given in the number of calls to various oracles for different optimization algorithms
on the learning problem (1) in case of SSVMs (2). The rates are speciﬁed in terms of the target accuracy   the
number of training examples n  the regularization λ  the size of the label space |Y|  the feature norm [48]. The
rates are speciﬁed up to constants and factors logarithmic in the problem parameters. The dependence on the
initial error is ignored. * denotes algorithms that make O(1) oracle calls per iteration.

Algo. (exp oracle)

Exponentiated
gradient* [7]

Excessive gap
reduction [69]
This work* 

ﬁxed smoothing 
entropy smoother

This work* 

adaptive smoothing 
entropy smoother

# Oracle calls
(n + log |Y|)R2

(cid:114)

λ
log |Y|

λ

nR2 log|Y|

λ

nR

(cid:114)

R2 log|Y|

λ

n +

Algo. (max oracle) # Oracle calls

BMRM [62]

QP 1-slack [21]

Stochastic

subgradient* [57]

Block-Coordinate
Frank-Wolfe* [30]

nR2
λ
nR2
λ
R2
λ

n +

R2
λ

Algo.

(top-K oracle)
This work* 

ﬁxed smoothing 

2 smoother
(cid:96)2
This work* 

adaptive smoothing 

2 smoother
(cid:96)2

# Oracle calls

(cid:114)
n(cid:101)R2
n + (cid:101)R2

λ

λ

The stochastic subgradient method considered by [49  57] operated directly on the non-smooth
primal formulation [49  57]. More recently  [30] proposed a block coordinate Frank-Wolfe (BCFW)
algorithm to optimize the dual formulation of structural support vector machines; see also [46] for
variants and extensions. Saddle-point or primal-dual optimization algorithms are another family
of algorithms  including the dual extra-gradient algorithm of [61] and the mirror-prox algorithms
of [8  19]. In [47]  an incremental optimization algorithm for saddle-point problems is proposed.
However it is unclear how to extend it to the structured prediction problems we consider here.
Incremental optimization algorithms for conditional random ﬁelds were proposed in [52]. We focus
here on primal optimization algorithms in order to be able to train structured prediction models with
afﬁne or nonlinear mappings with a uniﬁed approach  and on incremental optimization algorithms in
order to be able to scale to large datasets.

6 Conclusion

We introduced a general notion of smooth inference oracles in the context of black-box ﬁrst-order
optimization. This allows us to set the scene to extend the scope of fast incremental optimization
algorithms to structured prediction problems owing to a careful blend of a smoothing strategy
and an acceleration scheme. We illustrated the potential of our framework by proposing a new
incremental optimization algorithm to train structural support vector machines both enjoying worst-
case complexity bounds and demonstrating competitive performance on two real-world problems.
This work paves the way to faster incremental primal optimization algorithms for deep structured
prediction models explored in more detail in [48]. There are several potential venues for future work.
When there is no discrete structure that admits efﬁcient inference algorithms  it could be beneﬁcial
to not treat inference as a blackbox numerical procedure [40  16  17]. Instance-level improved
algorithms along the lines of [17] could also be interesting to explore.

Acknowledgements This work was supported by NSF Award CCF-1740551  the Washington
Research Foundation for innovation in Data-intensive Discovery  and the program “Learning in
Machines and Brains” of CIFAR.

9

References
[1] Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. Journal

of Machine Learning Research  18:221:1–221:51  2017.

[2] A. Beck and M. Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM

Journal on Optimization  22(2):557–580  2012.

[3] Y. Bengio  Y. LeCun  C. Nohl  and C. Burges. LeRec: A NN/HMM Hybrid for On-Line

Handwriting Recognition. Neural Computation  7(6):1289–1303  1995.

[4] L. Bottou and P. Gallinari. A Framework for the Cooperation of Learning Algorithms. In

Advances in Neural Information Processing Systems  pages 781–788  1990.

[5] L. Bottou  Y. Bengio  and Y. LeCun. Global Training of Document Processing Systems Using
Graph Transformer Networks. In Conference on Computer Vision and Pattern Recognition 
pages 489–494  1997.

[6] J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathe-

matical Programming  33(3):260–279  Dec 1985.

[7] M. Collins  A. Globerson  T. Koo  X. Carreras  and P. L. Bartlett. Exponentiated gradient
algorithms for conditional random ﬁelds and max-margin markov networks. Journal of Machine
Learning Research  9(Aug):1775–1822  2008.

[8] B. Cox  A. Juditsky  and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth

learning problems. Mathematical Programming  148(1-2):143–180  2014.

[9] A. Defazio. A simple practical accelerated method for ﬁnite sums. In Advances in Neural

Information Processing Systems  pages 676–684  2016.

[10] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems  pages 1646–1654  2014.

[11] D. Drusvyatskiy and C. Paquette. Efﬁciency of minimizing compositions of convex functions

and smooth maps. Mathematical Programming  Jul 2018.

[12] J. C. Duchi  D. Tarlow  G. Elidan  and D. Koller. Using Combinatorial Optimization within
Max-Product Belief Propagation. In Advances in Neural Information Processing Systems  pages
369–376  2006.

[13] M. Everingham  L. Van Gool  C. K. Williams  J. Winn  and A. Zisserman. The Pascal Visual
Object Classes (VOC) challenge. International Journal of Computer Vision  88(2):303–338 
2010.

[14] R. Frostig  R. Ge  S. Kakade  and A. Sidford. Un-regularizing: approximate proximal point and
faster stochastic algorithms for empirical risk minimization. In International Conference on
Machine Learning  pages 2540–2548  2015.

[15] R. Girshick  J. Donahue  T. Darrell  and J. Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Conference on Computer Vision and Pattern
Recognition  pages 580–587  2014.

[16] T. Hazan and R. Urtasun. A Primal-Dual Message-Passing Algorithm for Approximated Large
Scale Structured Prediction. In Advances in Neural Information Processing Systems  pages
838–846  2010.

[17] T. Hazan  A. G. Schwing  and R. Urtasun. Blending Learning and Inference in Conditional

Random Fields. Journal of Machine Learning Research  17:237:1–237:25  2016.

[18] L. He  K. Lee  M. Lewis  and L. Zettlemoyer. Deep Semantic Role Labeling: What Works
and What’s Next. In Annual Meeting of the Association for Computational Linguistics  pages
473–483  2017.

10

[19] N. He and Z. Harchaoui. Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization.

In Advances in Neural Information Processing Systems  pages 3411–3419  2015.

[20] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model.

SIAM Journal on computing  22(5):1087–1116  1993.

[21] T. Joachims  T. Finley  and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine

Learning  77(1):27–59  2009.

[22] J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum
entropy approaches. PhD thesis  Massachusetts Institute of Technology  Cambridge  MA  USA 
2008.

[23] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.

[24] V. Jojic  S. Gould  and D. Koller. Accelerated dual decomposition for MAP inference. In

International Conference on Machine Learning  pages 503–510  2010.

[25] D. Jurafsky  J. H. Martin  P. Norvig  and S. Russell. Speech and Language Processing. Pearson

Education  2014. ISBN 9780133252934.

[26] P. Kohli and P. H. Torr. Measuring uncertainty in graph cut solutions. Computer Vision and

Image Understanding  112(1):30–38  2008.

[27] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE

Transactions on Pattern Analysis and Machine Intelligence  26(2):147–159  2004.

[28] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems  pages 1097–1105 
2012.

[29] S. Lacoste-Julien  M. Schmidt  and F. Bach. A simpler approach to obtaining an O(1/t) conver-
gence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002 
2012.

[30] S. Lacoste-Julien  M. Jaggi  M. Schmidt  and P. Pletscher. Block-Coordinate Frank-Wolfe
Optimization for Structural SVMs. In International Conference on Machine Learning  pages
53–61  2013.

[31] J. Lafferty  A. McCallum  and F. C. Pereira. Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. In International Conference on Machine Learning 
pages 282–289  2001.

[32] C. H. Lampert  M. B. Blaschko  and T. Hofmann. Beyond sliding windows: Object localization
by efﬁcient subwindow search. In Conference on Computer Vision and Pattern Recognition 
pages 1–8  2008.

[33] N. Le Roux  M. W. Schmidt  and F. R. Bach. A Stochastic Gradient Method with an Exponential
Convergence Rate for Strongly-Convex Optimization with Finite Training Sets. In Advances in
Neural Information Processing Systems  pages 2672–2680  2012.

[34] M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Conference

on Empirical Methods in Natural Language Processing  pages 990–1000  2014.

[35] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization.

Advances in Neural Information Processing Systems  pages 3384–3392  2015.

In

[36] H. Lin  J. Mairal  and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization:

from Theory to Practice. Journal of Machine Learning Research  18(212):1–54  2018.

[37] J. Mairal. Incremental majorization-minimization optimization with application to large-scale

machine learning. SIAM Journal on Optimization  25(2):829–855  2015.

11

[38] A. F. T. Martins and R. F. Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention
In International Conference on Machine Learning  pages

and Multi-Label Classiﬁcation.
1614–1623  2016.

[39] A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and

attention. In International Conference on Machine Learning  pages 3459–3468  2018.

[40] O. Meshi  D. Sontag  T. S. Jaakkola  and A. Globerson. Learning Efﬁciently with Approximate
Inference via Dual Losses. In International Conference on Machine Learning  pages 783–790 
2010.

[41] O. Meshi  T. S. Jaakkola  and A. Globerson. Convergence Rate Analysis of MAP Coordinate
In Advances in Neural Information Processing Systems  pages

Minimization Algorithms.
3023–3031  2012.

[42] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on

Optimization  16(1):235–249  2005.

[43] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming  103

(1):127–152  2005.

[44] V. Niculae  A. F. Martins  M. Blondel  and C. Cardie. SparseMAP: Differentiable Sparse
Structured Inference. In International Conference on Machine Learning  pages 3796–3805 
2018.

[45] D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in proba-

bilistic expert systems. Statistics and computing  8(2):159–173  1998.

[46] A. Osokin  J.-B. Alayrac  I. Lukasewitz  P. Dokania  and S. Lacoste-Julien. Minding the gaps for
block Frank-Wolfe optimization of structured SVMs. In International Conference on Machine
Learning  pages 593–602  2016.

[47] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems.

In Advances in Neural Information Processing Systems  pages 1408–1416  2016.

[48] K. Pillutla  V. Roulet  S. M. Kakade  and Z. Harchaoui. A Smoother Way to Train Structured

Prediction Models. arXiv preprint  2019.

[49] N. D. Ratliff  J. A. Bagnell  and M. Zinkevich.

(Approximate) Subgradient Methods for
Structured Prediction. In International Conference on Artiﬁcial Intelligence and Statistics 
pages 380–387  2007.

[50] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy 
A. Khosla  M. Bernstein  A. C. Berg  and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. International Journal of Computer Vision  115(3):211–252  2015.

[51] B. Savchynskyy  J. H. Kappes  S. Schmidt  and C. Schnörr. A study of Nesterov’s scheme for
Lagrangian decomposition and MAP labeling. In Conference on Computer Vision and Pattern
Recognition  pages 1817–1823  2011.

[52] M. Schmidt  R. Babanezhad  M. Ahmed  A. Defazio  A. Clifton  and A. Sarkar. Non-uniform
stochastic average gradient method for training conditional random ﬁelds. In International
Conference on Artiﬁcial Intelligence and Statistics  pages 819–828  2015.

[53] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112  2017.

[54] B. Seroussi and J. Golmard. An algorithm directly ﬁnding the K most probable conﬁgurations
in Bayesian networks. International Journal of Approximate Reasoning  11(3):205 – 233  1994.

[55] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

12

[56] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. In International Conference on Machine Learning  pages 64–72 
2014.

[57] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient

solver for SVM. Mathematical programming  127(1):3–30  2011.

[58] H. O. Song  R. B. Girshick  S. Jegelka  J. Mairal  Z. Harchaoui  and T. Darrell. On learning to

localize objects with minimal supervision. pages 1611–1619  2014.

[59] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In Advances in Neural

Information Processing Systems  pages 25–32  2004.

[60] B. Taskar  S. Lacoste-Julien  and D. Klein. A discriminative matching approach to word
alignment. In Human Language Technology Conference and Conference on Empirical Methods
in Natural Language Processing  pages 73–80  2005.

[61] B. Taskar  S. Lacoste-Julien  and M. I. Jordan. Structured prediction  dual extragradient and

Bregman projections. Journal of Machine Learning Research  7(Jul):1627–1653  2006.

[62] C. H. Teo  S. Vishwanathan  A. Smola  and Q. V. Le. Bundle methods for regularized risk

minimization. Journal of Machine Learning Research  1(55)  2009.

[63] E. F. Tjong Kim Sang and F. De Meulder.

Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In Conference on Natural Language Learning 
pages 142–147  2003.

[64] M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In Empirical

Methods in Natural Language Processing  pages 118–127  2012.

[65] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for
interdependent and structured output spaces. In International Conference on Machine Learning 
page 104  2004.

[66] K. E. Van de Sande  J. R. Uijlings  T. Gevers  and A. W. Smeulders. Segmentation as selective
search for object recognition. In International Conference on Computer Vision  pages 1879–
1886  2011.

[67] B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives.

In Advances in Neural Information Processing Systems  pages 3639–3647  2016.

[68] C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief

propagation. In Advances in Neural Information Processing Systems  pages 289–296  2004.

[69] X. Zhang  A. Saha  and S. Vishwanathan. Accelerated training of max-margin markov networks

with kernels. Theoretical Computer Science  519:88–102  2014.

13

,Virginia Smith
Chao-Kai Chiang
Maziar Sanjabi
Ameet Talwalkar
Venkata Krishna Pillutla
Vincent Roulet
Sham Kakade
Zaid Harchaoui