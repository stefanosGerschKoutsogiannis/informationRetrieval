2013,A Deep Architecture for Matching Short Texts,Many machine learning problems can be interpreted as learning for matching two types of objects (e.g.  images and captions  users and products  queries and documents). The matching level of two objects is usually measured as the inner product in a certain feature space  while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema  although proven successful on a range of matching tasks  is insufficient for capturing the rich structure in the matching process of more complicated objects.  In this paper  we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More specifically  we apply this model to matching tasks in natural language  e.g.  finding sensible responses for a tweet  or  relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems  and therefore greatly improves upon the state-of-the-art models.,A Deep Architecture for Matching Short Texts

Zhengdong Lu
Noah’s Ark Lab

Huawei Technologies Co. Ltd.

Sha Tin  Hong Kong

Lu.Zhengdong@huawei.com

Hang Li

Noah’s Ark Lab

Huawei Technologies Co. Ltd.

Sha Tin  Hong Kong

HangLi.HL@huawei.com

Abstract

Many machine learning problems can be interpreted as learning for matching two
types of objects (e.g.  images and captions  users and products  queries and doc-
uments  etc.). The matching level of two objects is usually measured as the inner
product in a certain feature space  while the modeling effort focuses on mapping of
objects from the original space to the feature space. This schema  although proven
successful on a range of matching tasks  is insufﬁcient for capturing the rich struc-
ture in the matching process of more complicated objects. In this paper  we pro-
pose a new deep architecture to more effectively model the complicated matching
relations between two objects from heterogeneous domains. More speciﬁcally  we
apply this model to matching tasks in natural language  e.g.  ﬁnding sensible re-
sponses for a tweet  or relevant answers to a given question. This new architecture
naturally combines the localness and hierarchy intrinsic to the natural language
problems  and therefore greatly improves upon the state-of-the-art models.

1

Introduction

Many machine learning problems can be interpreted as matching two objects  e.g.  images and
captions in automatic captioning [11  14]  users and products in recommender systems  queries
and retrieved documents in information retrieval. It is different from the usual notion of similarity
since it is usually deﬁned between objects from two different domains (e.g.  texts and images)  and
it is usually associated with a particular purpose. The degree of matching is typically modeled as an
inner-product of two representing feature vectors for objects x and y in a Hilbert space H 

match(x  y) =< ΦY (x)  ΦX (y) >H

(1)
while the modeling effort boils down to ﬁnding the mapping from the original inputs to the feature
vectors. Linear models of this direction include the Partial Least Square (PLS) [19  20]  Canonical
Correlation Analysis (CCA) [7]  and their large margin variants [1]. In addition  there is also limited
effort on ﬁnding the nonlinear mappings for that [3  18].
In this paper  we focus on a rather difﬁcult task of matching a given short text and candidate re-
sponses. Examples include retrieving answers for a given question and automatically commenting
on a given tweet. This inner-product based schema  although proven effective on tasks like infor-
mation retrieval  are often incapable for modeling the matching between complicated objects. First 
representing structured objects like text as compact and meaningful vectors can be difﬁcult; Second 
inner-product cannot sufﬁciently take into account the complicated interaction between components
within the objects  often in a rather nonlinear manner.
In this paper  we attack the problem of matching short texts from a brand new angle. Instead of
representing the text objects in each domain as semantically meaningful vectors  we directly model
object-object interactions with a deep architecture. This new architecture allows us to explicitly
capture the natural nonlinearity and the hierarchical structure in matching two structured objects.

1

2 Model Overview

We start with the bilinear model. Assume we can
represent objects in domain X and Y with vectors
x ∈ RDx and y ∈ RDy. The bilinear matching
model decides the score for any pair (x  y) as
match(x  y) = x(cid:62)Ay =

Anmxmyn  (2)

Dy(cid:88)

Dx(cid:88)

m=1

n=1

Figure 1: Architecture for linear matching.

made considering all the local decisions  while in the bilinear case match(x  y) =(cid:80)

it simply sums all the local decisions with a weight speciﬁed by A  as illustrated in Figure 1.

with a pre-determined A. From a different angle 
each element product xnym in the above sum can
be viewed as a micro and local decision about the
matching level of x and y. The outer-product matrix
M = xy(cid:62) speciﬁes the space of element-wise inter-
action between objects x and y. The ﬁnal decision is
nm AnmMnm 

2.1 From Linear to Deep
This simple summarization strategy can be extended to a deep architecture to explore the nonlin-
earity and hierarchy in matching short texts. Unlike tasks like text classiﬁcation  we need to work
on a pair of text objects to be matched  which we refer to as parallel texts  borrowed from machine
translation. This new architecture is mainly based on the following two intuitions:
Localness:
there is a salient local structure in the semantic space of parallel text objects to be
matched  which can be roughly captured via the co-occurrence pattern of words across the objects.
This localness however should not prevent two “distant” components from correlating with each
other on a higher level  hence calls for the hierarchical characteristic of our model;
Hierarchy:
the decision making for matching has different levels of abstraction. The local deci-
sions  capturing the interaction between semantically close words  will be combined later layer-by-
layer to form the ﬁnal and global decision on matching.

2.2 Localness

“text patch”

“image patch”

Figure 2: Image patches vs. parallel-text patches.

The localness of the text matching problem can
be best described using an analogy with the
patches in images  as illustrated in Figure 2.
Loosely speaking  a patch for parallel texts de-
ﬁnes the set of interacting pairs of words from
the two text objects. Like the coordinate of an
image patch  we can use (Ωx p  Ωy p) to specify
the range of the path  with Ωx p and Ωy p each
specifying a subset of terms in X and Y respec-
tively. Like the patches of images  the patches
deﬁned here are meant to capture the segments
of rich inherent structure. But unlike the natu-
rally formed rectangular patches of images  the
patches deﬁned here do not come with a pre-given spatial continuity. It is so since in texts  the
nearness of words are not naturally given as location of pixels in images  but instead needs to be
discovered from the co-occurrence patterns of the matched texts. As shown later in Section 3  we
actually do that with a method resembling bilingual topic modeling  which nicely captures the co-
occurrence of the words within-domain and cross-domain simultaneously. The basic intuitions here
are  1) when the words co-occur frequently across the domains (e.g.  fever—antibiotics)  they
are likely to have strong interaction in determining the matching score  and 2) when the words co-
occur frequently in the same domain (e.g.  {Hawaii vacation})  they are likely to collaborate in
making the matching decision. For example  modeling the matching between the word “Hawaii”
in question (likely to be a travel-related question) and the word “RAM” in answer (likely an answer
to a computer-related question) is probably useless  judging from their co-occurrence pattern in
Question-Answer pairs. In other words  our architecture models only “local” pairwise relations on

2

a low level with patches  while describing the interaction between semantically distant terms on
higher levels in the hierarchy.

2.3 Hierarchy
Once the local decisions on patches are made (most of them are NULL for a particular short
text pair)  they will be sent to the next layer  where the lower-level decisions are further com-
bined to form more composite decisions  which in turn will be sent to still higher levels. This
process runs until it reaches the ﬁnal decision. Figure 3 gives an illustrative example on hier-
archical decision making. As it shows  the local decision on patch “SIGHTSEEING IN PARIS”
and “SIGHTSEEING IN BERLIN” can be combined to form a higher level decision on patch for
“SIGHTSEEING”  which in turn can be combined with decisions on patches like “HOTEL” and
“TRANSPORTATION” to form a even higher level decision on “TRAVEL”. Note that one low-
level topic does not exclusively belong to a higher-level one. For example  the “WEATHER”
patch may belong to higher level patches “TRAVEL” and “AGRICULTURE” at the same time.

Quite intuitively  this decision composition mecha-
nism is also local and varies with the “locations”.
For example  when combining “SIGHTSEEING IN
PARIS” and “SIGHTSEEING IN BERLIN”  it is more
like an OR logic since it only takes one of them to
be positive. A more complicated strategy is often
needed in  for example  a decision on “TRAVELING” 
which often takes more than one element 
like
“SIGHTSEEING”  “HOTEL”  “TRANSPORTATION” 
or “WEATHER”  but not necessarily all of them. The
particular strategy taken by a local decision compo-

Figure 3: An example of decision hierarchy.
sition unit is fully encoded in the weights of the corresponding neuron through

sp(x  y) = f(cid:0)w(cid:62)

p Φp(x  y)(cid:1)  

(3)
where f is the active function. As stated in [12]  a simple nonlinear function (such as sigmoid) with
proper weights is capable of realizing basic logics such as AND and OR. Here we decide the hierar-
chical architecture of the decision making  but leave the exact mechanism for decision combination
(encoded in the weights) to the learning algorithm later.
3 The Construction of Deep Architecture

The process for constructing the deep architecture for matching consists of two steps. First  we
deﬁne parallel text patches with different resolutions using bilingual topic models. Second  we
construct a layered directed acyclic graph (DAG) describing the hierarchy of the topics  based on
which we further construct the topology of the deep neural network.
3.1 Topic Modeling for Parallel Texts
This step is to discover parallel text segments for meaningful co-occurrence patterns of words in
both domains. Although more sophisticated methods may exist for capturing this relationship  we
take an approach similar to the multi-lingual pLSI proposed in [10]  and simply put the words
from parallel texts together to a joint document  while using a different virtual vocabulary for each
domain to avoid any mixing up. For example  the word hotel appearing in domain X is treated as
a different word as hotel in domain Y. For modeling tool  we use latent Dirichlet allocation (LDA)
with Gibbs sampling [2] on all the training data. Notice that by using topic modeling  we allow the
overlapping sets of words  which is advantageous over non-overlapping clustering of words  since
we may expect some words (e.g.  hotel and price) to appear in multiple segments. Table 1 gives
two example parallel-topics learned from a traveling-related Question-Answer corpus (see Section
5 for more details). As we can see intuitively  in the same topic  a word in domain X co-occurs
frequently not only with words in the same domain  but also with those in domain Y. We ﬁt the
same corpus with L topic models with decreasing resolutions1  with the series of learned topic sets
denoted as H = {T1 ···  T(cid:96) ···  TL}  with (cid:96) indexing the topic resolution.

1Topic resolution is controlled mainly by the number of topics  i.e.  a topic model with 100 topics is consid-

ered to be of lower resolution (or more general) than the one with 500 topics.

3

Topic Label
SPECIAL
PRODUCT
TRANSPORTATION

Question

Answer

local delicacy  special product
snack food  quality  tasty  ···
route  arrangement  location
arrive  train station  fare  ···

tofu  speciality  aroma  duck  sweet  game  cuisine
sticky rice  dumpling  mushroom  traditional ···
distance  safety  spending  gateway  air ticket  pass
trafﬁc control  highway  metroplis  tunnel  ···

Table 1: Examples of parallel topics. Originally in Chinese  translated into English by the authors.

3.2 Getting Matching Architecture
With the set of topics H  the architecture of the deep matching model can then be obtained in the
following three steps. First  we trim the words (in both domains X and Y) with the low probability
for each topic in T(cid:96) ∈ H  and the remaining words in each topic specify a patch p. With a slight
abuse of symbols  we still use H to denote the patch sets with different resolutions. Second  based
on the patches speciﬁed in H  we construct a layered DAG G by assigning each patch with resolution
(cid:96) to a number of patches with resolution (cid:96) − 1 based on the word overlapping between patches  as
illustrated in Figure 4 (left panel). If a patch p in layer (cid:96) − 1 is assigned to patch p(cid:48) in layer (cid:96)  we
denote this relation as p ≺ p(cid:48) 2. Third  based on G  we can construct the architecture of the patch-
induced layers of the neural network. More speciﬁcally  each patch p in layer (cid:96) will be transformed
into K(cid:96) neurons in the ((cid:96)−1)th hidden layer in the neural network  and the K(cid:96) neurons are connected
to the neurons in the (cid:96)th layer corresponding to patch p(cid:48) iff p ≺ p(cid:48). In other words  we determine the
sparsity-pattern of the weights  but leave the values of the weights to the later learning phase. Using
the image analogy  the neurons corresponding to patch p are referred to as ﬁlters. Figure 4 illustrates
the process of transforming patches in layer (cid:96) − 1 (speciﬁc topics) and layer (cid:96) (general topics) into
two layers in neural network with K(cid:96) = 2.

patches

neural network

Figure 4: An illustration of constructing the deep architecture from hierarchical patches.

The overall structure is illustrated in Figure 5. The input layer is a two-dimensional interaction
space  which connects to the ﬁrst patch-induced layer p-layerI followed by the second patch-
induced layer p-layerII. The connections to p-layerI and p-layerII have pre-speciﬁed s-
parsity patterns. Following p-layerII is a committee layer (c-layer)  with full connections from
p-layerII. With an input (x  y)  we ﬁrst get the local matching decisions on p-layerI  associ-
ated with patches in the interaction space. Those local decisions will be sent to the corresponding
neurons in p-layerII to get the ﬁrst round of fusion. The outputs of p-layerII are then sent to
c-layer for further decision composition. Finally the logistic regression unit in the output layer
summarizes the decisions on c-layer to get the ﬁnal matching score s(x  y). This architecture is
referred to as DEEPMATCH in the remainder of the paper.

Figure 5: An illustration of the deep architecture for matching decisions.

2In the assignment  we make sure each patch in layer (cid:96) is assigned to at least m(cid:96) patches in layer (cid:96) − 1.

4

3.3 Sparsity
The ﬁnal constructed neural network has two types of sparsity. The ﬁrst type of sparsity is enforced
through architecture  since most of the connections between neurons in adjacent layers are turned
off in construction. In our experiments  only about 2% of parameters are allowed to be nonzero.
The second type of sparsity is from the characteristics of the texts. For most object pairs in our
experiment  only a small percentage of neurons in the lower layers are active (see Section 5 for more
details). This is mainly due to two factors  1) the input parallel texts are very short (usually < 100
words)  and 2) the patches are well designed to give a compact and sparse representation of each of
the texts  as describe in Section 3.1.
To understand the second type of sparsity  let us start with the following deﬁnition:
Deﬁnition 3.1. An input pair (x  y) overlaps with patch p  iff x ∩ px (cid:54)= ∅ and y ∩ py (cid:54)= ∅  where px
and py are respectively the word indices of patch p in domain X and Y.
= (cid:107)px ∩ x(cid:107)0 · (cid:107)py ∩ y(cid:107)0. The
We also deﬁne the following indicator function overlap((x  y)  p)
proposed architecture only allows neurons associated with patches overlapped with the input to have
nonzero output. More speciﬁcally  the output of neurons associated with patch p is

sp(x  y) = ap(x  y) · overlap((x  y)  p)

(4)
to ensure that sp(x  y) ≥ 0 only when there is non-empty cross-talking of x and y within patch p 
where ap(x  y) is the activation of neuron before this rule is enforced. It is not hard to understand 
for any input (x  y)  when we track any upwards path of decisions from input to a higher level  there
is nonzero matching vote until we reach a patch that contains terms from both x and y. This view is
particularly useful in parameter tuning with back-propagation: the supervision signal can only get
down to a patch p when it overlaps with input (x  y). It is easy to show from the deﬁnition  once
the supervision signal stops at one patch p  it will not get pass p and propagate to p’s children  even
if those children have other ancestors. This indicates that when using stochastic gradient descent 
the updating of weights usually only involves a very small number of neurons  and therefore can be
very efﬁcient.

def

(cid:16)(cid:80)

3.4 Local Decision Models
In the hidden layers p-layerI  p-layerII  and c-layer  we allow two types of neurons  cor-
responding to two active functions: 1) linear flin(t) = x  and 2) sigmoid fsig(t) = (1 + e−t)−1. In
the ﬁrst layer  each patch p for (x  y) takes the value of the interaction matrix Mp = xpy(cid:62)
p   and the
kth local decision on p is given by a(k)
  with weight
p ∈ {flin  fsig} . With low-rank constraint on A(k) to
given by A(k) and the activation function f (k)
reduce the complexity  we essentially have
x(cid:62)
p L(k)

(5)
y p ∈ R|py|×Dp  with the latent dimension Dp. As indicated in Figure 5 
where L(k)
the two-dimensional structure is lost after leaving the input layer  while the local structure is kept in
the second patch-induced layer p-layerII. Basically  a neuron in layer p-layerII processes the
low-level decisions assigned to it made in layer p-layerI

x p ∈ R|px|×Dp  L(k)

  k = 1 ···   K1 

y p)(cid:62)yp + b(k)

p nmMp nm + b(k)
p

p (x  y) = f (k)

a(k)
p (x  y) = f (k)

p

p

n m A(k)

(cid:17)

p

(cid:16)

x p(L(k)

(cid:17)

p kΦp(x  y)(cid:1)   k = 1 ···   K2 
(cid:0)w(cid:62)

p (x  y) = f (k)
a(k)

p

(6)

where Φp(x  y) lists all the lower-level decisions assigned to unit p:

Φp(x  y) = [···   s(1)

p(cid:48) (x  y)  s(2)

p(cid:48) (x  y) ···   s(K1)

p(cid:48)

(x  y) ··· ] 

∀p(cid:48) ≺ p  p(cid:48) ∈ T1

which contains all the decisions on patches in layer p-layerI subsumed by p. The local decision
models in the committee layer c-layer are the same as in p-layerII  except that they are fully
connected to neurons in the previous layer.

4 Learning
We divide the parameters  denoted W  into three sets: 1) the low-rank bilinear model for mapping
for all p ∈ P and ﬁlter index
from input patches to p-layerI  namely L(k)
1 ≤ k ≤ K1  2) the parameters for connections between patch-induced neurons  i.e.  the weights

y p  and offset b(k)
p

x p  L(k)

5

between p-layerI and p-layerII  denoted (w(k)
1 ≤ k ≤ K2  and 3) the weights for committee layer (c-layer) and after  denoted as wc.
We employ a discriminative training strategy with a large margin objective. Suppose that we are
given the following triples (x  y+  y−) from the oracle  with x (∈ X ) matched with y+ better than
with y− (both ∈ Y). We have the following ranking-based loss as objective:

p ) for associated patch p and ﬁlter index

p   b(k)

L(W Dtrn) =

eW (xi  y+

i   y−

i ) + R(W) 

(7)

where R(W) is the regularization term  and eW (xi  y+
given by the following large margin form:

i   y−

i ) is the error for triple (xi  y+

i   y−
i ) 

ei = eW (xi  y+

i   y−

i ) = max(0  m + s(xi  y−

i ) − s(xi  y+

i )) 

with 0 < m < 1 controlling the margin in training. In the experiments  we use m = 0.1.

(cid:88)

(xi y+

i  y

−
i )∈Dtrn

4.1 Back-Propagation
All three sets of parameters are updated through back-propagation (BP). The updating of the weights
from hidden layers are almost the same as that for conventional Multi-layer Perceptron (MLP)  with
two slight differences: 1) we have a different input model and two types of activation function  and
2) we could gain some efﬁciency by leveraging the sparsity pattern of the neural network  but the
advantage diminishes quickly after the ﬁrst two layers.
This sparsity however greatly reduces the number of parameters for the ﬁrst two layers  and hence
the time on updating them. From Equation (4-6)  the sub-gradient of L(k)
x p w.r.t. empirical error e is

(cid:88)

(cid:16)

∂e
∂L(k)
x p

=

∂ei

∂ s(k)

p (xi  y+
i )
∂ s(k)
∂ pot(k)

p (xi  y−
i )

∂ei

p (xi  y+
i )
p (xi  y+
i )

∂ s(k)
∂ pot(k)
p (xi  y−
i )
p (xi  y−
i )

∂ s(k)

i

−

(cid:0)xi p(y+
(cid:0)xi p(y−

i p)(cid:62)L(k)

y p

i p)(cid:62)L(k)

(cid:1) · overlap(cid:0)(xi  y+
i )  p(cid:1)
i )  p(cid:1)(cid:17)
(cid:1) · overlap(cid:0)(xi  y−

y p

 

(8)

where i indices the training instances  and

p

pot(k)

p L(k)

x p(L(k)

p (x  y) = x(cid:62)
p . The gradient for L(k)

y p)(cid:62)yp + b(k)
stands for the potential value for s(k)
y p is given in a slightly different way.For
the weights between p-layerI and p-layerII  the gradient can also beneﬁt from the sparsity in
activation.
We use stochastic sub-gradient descent with mini-batches [9]  each of which consists of 50 randomly
generated triples (x  y+  y−)  where the (x  y+) is the original pair  and y− is a randomly selected
response. With this type of optimization  most of the patches in p-layerI and p-layerII get zero
inputs  and therefore remain inactive by deﬁnition during the prediction as well as updating process.
On the tasks we have tried  only about 2% of parameters are allowed to be nonzero for weights
among the patch-induced layers. Moreover  during stochastic gradient descent  only about 5% of
neurons in p-layerI and p-layerII are active on average for each training instance  indicating
that the designed architecture has greatly reduced the essential capacity of the model.

5 Experiments
We compare our deep matching model to the inner-product based models  ranging from variants of
bilinear models to nonlinear mappings for ΦX (·) and ΦY (·). For bilinear models  we consider only
the low-rank models with ΦX (x) = P (cid:62)

x x and Φy(y) = P (cid:62)

x y  which gives

match(x  y) =< P (cid:62)

x x  P (cid:62)

y y >= x(cid:62)PxP (cid:62)

y y.

With different kinds of constraints on Px and Py  we get different models. More speciﬁcally  with 1)
orthnormality constraints P (cid:62)
x Py = Id×d  we get partial least square (PLS) [19]  and with 2) (cid:96)2 and
(cid:96)1 based constraints put on rows or columns  we get Regularized Mapping to Latent Space (RMLS)

6

[20]. For nonlinear models  we use a modiﬁed version of the Siamese architecture [3]  which uses
two different neural networks for mapping objects in the two domains to the same d-dimensional
latent space  where inner product can be used as a measure of matching and is trained with a similar
large margin objective. Different from the original model in [3]  we allow different parameters for
mapping to handle the domain heterogeneity. Please note here that we omit the nonlinear model for
shared representation [13  18  17] since they are essentially also inner product based models (when
used for matching) and not designed to deal with short texts with large vocabulary.

5.1 Data Sets
We use the learned matching function for retrieving response texts y for a given query text x  which
will be ranked purely based on the matching scores. We consider the following two data sets:
Question-Answer: This data set contains around 20 000 traveling-related (Question  Answer) pairs
collected from Baidu Zhidao (zhidao.baidu.com) and Soso Wenwen (wenwen.soso.com) 
two famous Chinese community QA Web sites. The vocabulary size is 52 315.
Weibo-Comments: This data set contains half million (Weibo  comment) pairs collected from Sina
Weibo (weibo.com)  a Chinese Twitter-like microblog service. The task is to ﬁnd the appropriate
responses (e.g.  comments) to given Weibo posts. This task is signiﬁcantly harder than the Question-
Answer task since the Weibo data are usually shorter  more informal  and harder to capture with
bag-of-words. The vocabulary size for tweets and comments are both 48  724.
On both data sets  we generate (x  y+  y−) triples  with y− being randomly selected. The training
data are randomly split into training data and testing data  and the parameters of all models (including
the learned patches for DEEPMATCH) are learned on training data. The hyper parameters (e.g.  the
latent dimensions of low-rank models and the regularization coefﬁcients) are tuned on a validation
set (as part of the training set). We use NDCG@1 and NDCG@6 [8] on random pool with size 6
(one positive + ﬁve negative) to measure the performance of different matching models.

5.2 Performance Comparison
The retrieval performances of all four models are reported in Table 2. Among the two data sets  the
Question-Answer data set is relatively easy  with all four matching models improve upon random
guesses. As another observation  we get signiﬁcant gain of performance by introducing nonlinearity
in the mapping function  but all the inner-product based matching models are outperformed by
the proposed DEEPMATCH with large margin on this data set. The story is slightly different on
the Weibo-Response data set  which is signiﬁcantly more challenging than the Q-A task in that it
relies more on the content of texts and is harder to be captured by bag-of-words. This difﬁculty
can be hardly handled by inner-product based methods  even with nonlinear mappings of SIAMESE
NETWORK. In contrast  DEEPMATCH still manages to perform signiﬁcantly better than all other
models.
To further understand the performances of the different matching models  we also compare the
generalization ability of two nonlinear models. We ﬁnd that the SIAMESE NETWORK can achieve
over 90% correct pairwise comparisons on training set with small regularization  but generalizes
relatively poorly on the test set with all the conﬁgurations we tried. This is not surprising since
SIAMESE NETWORK has the same level of parameters (varying with the number of hidden units)
as DEEPMATCH. We argue that our model has better generalization property than the Siamese
architecture with similar model complexity.

RANDOM GUESS
PLS
RMLS
SIAMESE NETWORK
DEEPMATCH

Question-Answer
nDCG@1
0.167
0.285
0.282
0.357
0.723

nDCG@6
0.550
0.662
0.659
0.735
0.856

Weibo-Response
nDCG@1
0.167
0.171
0.165
0.175
0.336

nDCG@6
0.550
0.587
0.553
0.574
0.665

Table 2: The retrieval performance of matching models on the Q-A and Weibo data sets.

7

5.3 Model Selection
We tested different variants of the current DEEPMATCH architecture  with results reported in Figure
6. There are two ways to increase the depth of the proposed method: adding patch-induced layers
and committee layers. As shown in Figure 6 (left and middle panels)  the performance of DEEP-
MATCH stops increasing in either way when the overall depth goes beyond 6  while the training
gets signiﬁcantly slower with each added hidden layer. The number of neurons associated with each
patch (Figure 6  right panel) follows a similar story: the performance gets ﬂat out after the number
of neurons per patch reaches 3  again with training time and memory increased signiﬁcantly. As
another observation about the architecture  DEEPMATCH with both linear and sigmoid activation
functions in hidden layers yields slightly but consistently better performance than that with only
sigmoid function. Our conjecture is that linear neurons provide shortcuts for low-level matching
decision to high level composition units  and therefore facilitate the informative low-level units in
determining the ﬁnal matching score.

size of patch-induced layers

size of committee layer(s)

number of ﬁlters/patch

Figure 6: Choices of architecture for DEEPMATCH. For the left and middle panels  the numbers in
parentheses stand for number of neurons in each layer.
6 Related Work
Our model is apparently a special case of the learning-to-match models  for which much effort is on
designing a bilinear form [1  19  7]. As we discussed earlier  this kind of models cannot sufﬁciently
model the rich and nonlinear structure of matching complicated objects. In order to introduce more
modeling ﬂexibility  there has been some works on replacing Φ(·) in Equation (1) with an nonlinear
mapping  e.g.  with neural networks [3] or implicitly through kernelization [6]. Another similar
thread of work is the recent advances of deep learning models on multi-modal input [13  17]. It
essentially ﬁnds a joint representation of inputs in two different domains  and hence can be used to
predict the other side. Those deep learning models however do not give a direct matching function 
and cannot handle short texts with a large vocabulary.
Our work is in a sense related to the sum-product network (SPN)[4  5  15]  especially the work in
[4] that learns the deep architecture from clustering in the feature space for the image completion
task. However  it is difﬁcult to determine a regular architecture like SPN for short texts  since the
structure of the matching task for short texts is not as well-deﬁned as that for images. We therefore
adopt a more traditional MLP-like architecture in this paper.
Our work is conceptually close to the dynamic pooling algorithm recently proposed by Socher et al
[16] for paraphrase identiﬁcation  which is essentially a special case of matching between two ho-
mogeneous domains. Similar to our model  their proposed model also constructs a neural network
on the interaction space of two objects (sentences in their case)  and outputs the measure of semantic
similarity between them. The major differences are three-fold  1) their model relies on a predeﬁned
compact vectorial representation of short text  and therefore the similarity metric is not much more
than summing over the local decisions  2) the nature of dynamic pooling allows no space for ex-
ploring more complicated structure in the interaction space  and 3) we do not exploit the syntactic
structure in the current model  although the proposed architecture has the ﬂexibility for that.

7 Conclusion and Future Work
We proposed a novel deep architecture for matching problems  inspired partially by the long thread
of work on deep learning. The proposed architecture can sufﬁciently explore the nonlinearity and
hierarchy in the matching process  and has been empirically shown to be superior to various inner-
product based matching models on real-world data sets.

8

References
[1] B. Bai  J. Weston  D. Grangier  R. Collobert  K. Sadamasa  Y. Qi  O. Chapelle  and K. Weinberger.

Supervised semantic indexing. In CIKM’09  pages 187–196  2009.

[2] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research 

3:993–1022  2003.

[3] S. Chopra  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively  with application to

face veriﬁcation. In Proc. of Computer Vision and Pattern Recognition Conference. IEEE Press  2005.

[4] A. Dennis and D. Ventura. Learning the architecture of sum-product networks using clustering on vari-

ables. In Advances in Neural Information Processing Systems 25.

[5] R. Gens and P. Domingos. Discriminative learning of sum-product networks. In NIPS  pages 3248–3256 

2012.

[6] D. Grangier and S. Bengio. A discriminative kernel-based model to rank images from text queries. IEEE

transactions on PAMI  30(8):1371–1384  2008.

[7] D. Hardoon and J. Shawe-Taylor. Kcca for different level precision in content-based image retrieval. In

Proceedings of Third International Workshop on Content-Based Multimedia Indexing  2003.

[8] K. J¨arvelin and J. Kek¨al¨ainen. Ir evaluation methods for retrieving highly relevant documents. In SIGIR 

pages 41–48  2000.

[9] Y. LeCun  L. Bottou  G. Orr  and K. Muller. Efﬁcient backprop. In G. Orr and M. K.  editors  Neural

Networks: Tricks of the trade. Springer  1998.

[10] M. Littman  S. Dumais  and T. Landauer. Automatic cross-language information retrieval using latent

semantic indexing. In Cross-Language Information Retrieval  chapter 5  pages 51–62  1998.

[11] A. K. Menon and C. Elkan. Link prediction via matrix factorization. In Proceedings of the 2011 Eu-
ropean conference on Machine learning and knowledge discovery in databases - Volume Part II  ECML
PKDD’11  pages 437–452  2011.

[12] M. Minsky and S. Papert. Perceptrons - an introduction to computational geometry. MIT Press  1987.
[13] J. Ngiam  A. Khosla  M. Kim  J. Nam  H. Lee  and A. Y. Ng. Multimodal deep learning. In International

Conference on Machine Learning (ICML)  Bellevue  USA  June 2011.

[14] V. Ordonez  G. Kulkarni  and T. L. Berg. Im2text: Describing images using 1 million captioned pho-

tographs. In Neural Information Processing Systems (NIPS)  2011.

[15] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In UAI  pages 337–346 

2011.

[16] R. Socher and E. Huang and J. Pennington and A. Ng and C. Manning. Dynamic Pooling and Unfolding

Recursive Autoencoders for Paraphrase Detection. In Advances in NIPS 24. 2011.

[17] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS  pages

2231–2239  2012.

[18] B. Wang  X. Wang  C. Sun  B. Liu  and L. Sun. Modeling semantic relevance for question-answer pairs

in web social communities. In ACL  pages 1230–1238  2010.

[19] W. Wu  H. Li  and J. Xu. Learning query and document similarities from click-through bipartite graph
In Proceedings of the sixth ACM international conference on WSDM  pages 687–696 

with metadata.
2013.

[20] W. Wu  Z. Lu  and H. Li. Regularized mapping to latent structures and its application to web search.

Technical report.

9

,Zhengdong Lu
Hang Li
Chao Qu
Shie Mannor
Huan Xu
Yuan Qi
Le Song
Junwu Xiong