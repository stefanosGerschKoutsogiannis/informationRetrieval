2008,Sparse Signal Recovery Using Markov Random Fields,Compressive Sensing (CS) combines sampling and compression into a single sub-Nyquist linear measurement process for sparse and compressible signals. In this paper  we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular  we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefficients are clustered. Our new model-based reconstruction algorithm  dubbed Lattice Matching Pursuit (LaMP)  stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.,Sparse Signal Recovery Using Markov Random Fields

Volkan Cevher
Rice University

volkan@rice.edu

Chinmay Hegde
Rice University

chinmay@rice.edu

Marco F. Duarte
Rice University

duarte@rice.edu

Richard G. Baraniuk

Rice University

richb@rice.edu

Abstract

Compressive Sensing (CS) combines sampling and compression into a single sub-
Nyquist linear measurement process for sparse and compressible signals. In this
paper  we extend the theory of CS to include signals that are concisely repre-
sented in terms of a graphical model. In particular  we use Markov Random Fields
(MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our
new model-based recovery algorithm  dubbed Lattice Matching Pursuit (LaMP) 
stably recovers MRF-modeled signals using many fewer measurements and com-
putations than the current state-of-the-art algorithms.

1 Introduction
The Shannon/Nyquist sampling theorem tells us that in order to preserve information when uni-
formly sampling a signal we must sample at least two times faster than its bandwidth. In many
important and emerging applications  the resulting Nyquist rate can be so high that we end up with
too many samples and must compress in order to store or transmit them. In other applications  in-
cluding imaging systems and high-speed analog-to-digital converters  increasing the sampling rate
or density beyond the current state-of-the-art is very expensive. A transform compression system
reduces the effective dimensionality of an N -dimensional signal by re-representing it in terms of a
sparse expansion in some basis (for example  the discrete cosine transform for JPEG). By sparse we
mean that only K ≪ N of the basis coefﬁcients are nonzero.
The new theory of compressive sensing (CS) combines sampling and compression into a single sub-
Nyquist linear measurement process for sparse signals [1  2]. In CS  we measure not periodic signal
samples but rather inner products with M < N known measurement vectors; random measurement
vectors play a starring role. We then recover the signal by searching for the sparsest signal that
agrees with the measurements. Research in CS to date has focused on reducing both the number
of measurements M (as a function of N and K) and on reducing the computational complexity of
the recovery algorithm. Today’s state-of-the-art CS systems can recover K-sparse and more general
compressible signals using M = O(K log(N/K)) measurements using polynomial-time linear
programming or greedy algorithms.

While such sub-Nyquist measurement rates are impressive  our contention in this paper is that for
CS to truly live up its name it must more fully leverage concepts from state-of-the-art compression
algorithms. In virtually all such algorithms  the key ingredient is a signal model that goes beyond
simple sparsity by providing a model for the basis coefﬁcient structure. For instance  JPEG does not
only use the fact that most of the DCT of a natural image are small. Rather  it also exploits the fact
that the values and locations of the large coefﬁcients have a particular structure that is characteristic
of natural images. Coding this structure using an appropriate model enables JPEG and other similar
algorithms to compress images close to the maximum amount possible  and signiﬁcantly better than
a naive coder that just assigns bits to each large coefﬁcient independently.

1

In this paper  we extend the theory of CS to include signals that are concisely represented in terms
of a graphical model [3]. We use Markov Random Fields (MRFs) to represent sparse signals whose
nonzero coefﬁcients also cluster together. Our new model-based recovery algorithm  dubbed Lattice
Matching Pursuit (LaMP)  performs rapid and numerically stable recovery of MRF-modeled signals
using far fewer measurements than standard algorithms.

The organization of the paper is as follows. In Sections 2 and 3  we brieﬂy review the CS and MRF
theories. We develop LaMP in Section 4 and present experimental results in Section 5 using both
simulated and real world data. We conclude by offering our perspective on the future directions of
model-based CS research in Section 6.
2 Compressive sensing: From sparsity to structured sparsity
Sparse signal recovery. Any signal x ∈ RN can be represented in terms of N coefﬁcients {αi}
i=1; stacking the ψi as columns into the matrix ΨN ×N   we can write succinctly
in a basis {ψi}N
that x = Ψθ. We say that x has a sparse representation if only K ≪ N entries of θ are nonzero 
K(cid:1) possible supports for such K-sparse signals. We say that x is
and we denote by ΩK the set of (cid:0)N

compressible if the sorted magnitudes of the entries of θ decay rapidly enough that it can be well
approximated as K-sparse.
In Compressive Sensing (CS)  the signal is not acquired by measuring x or α directly. Rather  we
measure the M < N linear projections y = Φx = ΦΨθ using the M × N matrix Φ. In the
sequel  without loss of generality  we focus on two-dimensional image data and assume that Ψ = I
(the N × N identity matrix) so that x = θ. The most commonly used criterion for evaluating the
quality of a CS measurement matrix is the restricted isometry property (RIP). A matrix Φ satisﬁes
the K-RIP if there exists a constant δK > 0 such that for all K-sparse vectors x 

(1 − δK)kxk2 ≤ kΦxk2 ≤ (1 + δK)kxk2.

(1)

The recovery of the set of signiﬁcant coefﬁcients θi is achieved using optimization: we search for
the sparsest θ that agrees with the measurements y. While in principle recovery is possible using a
matrix that has the 2K-RIP with δ2K < 1  such an optimization is combinatorially complex (NP-
complete) and numerically unstable. If we instead use a matrix that has the 3K-RIP with δ3K < 1/2 
then numerically stable recovery is possible in polynomial time using either a linear program [1  2]
or a greedy algorithm [4]. Intriguingly  a random Gaussian or Bernoulli matrix works with high
probability  leading to a randomized acquisition protocol instead of uniform sampling.
Structured sparsity. While many natural and manmade signals and images can be described to the
ﬁrst-order as sparse or compressible  their sparse supports (set of nonzero coefﬁcients) often have an
underlying order. This order plays a central role in the transform compression literature  but it has
barely been explored in the CS context [5  6]. The theme of this paper is that by exploiting a priori
information on coefﬁcient structure in addition to signal sparsity  we can make CS better  stronger 
and faster.

Figure 1 illustrates a real-world example of structured sparse support in a computer vision applica-
tion. Figure 1(b) is a background subtracted image computed from a video sequence of a parking
lot with two moving people (one image frame is shown in Figure 1(a)). The moving people form
the foreground (white in (b))  while the rest of the scene forms the background (black in (b)). The
background subtraction was computed from CS measurements of the video sequence. Background
subtracted images play a fundamental role in making inferences about objects and activities in a
scene and  by nature  they have structured spatial sparsity corresponding to the foreground innova-
tions. In other words  compared to the scale of the scene  the foreground innovations are usually not
only sparse but also clustered in a distinct way  e.g.  corresponding to the silhouettes of humans and
vehicles. Nevertheless  this clustering property is not exploited by current CS recovery algorithms.
Probabilistic RIP. The RIP treats all possible K-sparse supports equally. However  if we incor-
porate a probabilistic model on our signal supports and consider only the signal supports with the
highest likelihoods  then we can potentially do much better in terms of the required number of
measurements required for stable recovery.

We say that Φ satisﬁes the (K  ǫ)-probabilistic RIP (PRIP) if there exists a constant δK > 0 such
that for a K-sparse signal x generated by a speciﬁed probabilistic signal model  (1) holds with
probability at least 1 − ǫ over the signal probability space. We propose a preliminary result on the

2

PSfrag

(a)

(b)

(c)

Figure 1: A camera surveillance image (b) withthe background subtracted image (b) recovered using com-
pressive measurements of the scene. The background subtracted image has resolution N = 240 × 320 and
sparsity K = 390. (c)Arandom K = 390 sparseimagein N = 240 × 320 dimensions. Theprobabilityof
image(b)undertheIsingmodelisapproximately 10856 timesgreaterthantheprobabilityofimage(c).

number of random measurements needed under this new criterion; this is a direct consequence of
Theorem 5.2 of [8]. (See also [9] for related results.)
Lemma 1. Suppose that M   N   and δ ∈ [0  1] are given and that the signal x is generated by
a known probabilistic model. Let ΩK ǫ ⊆ ΩK denote the smallest set of supports for which the
probability that a K-sparse signal x has supp(x) /∈ ΩK ǫ is less than ǫ  and denote D = |ΩK ǫ|.
If Φ is a matrix with normalized i.i.d. Gaussian or Bernoulli/Rademacher (±1) random entries 
then Φ has the (K  ǫ)-PRIP with probability at least 1 − e−c2M if M ≥ c1(K + log(D))  where
c1  c2 > 0 depend only on the PRIP constant δK.

To illustrate the signiﬁcance of the above lemma  consider the following probabilistic model for
an N -dimensional  K-sparse signal. We assume that the locations of the non-zeros follow a ho-
mogeneous Poisson process with rate λ = − log(ǫ/K)N −α  where α ≪ 1. Thus  a particular
non-zero coefﬁcient occurs within a distance of N α of its predecessor with probability 1 − ǫ/K. We
determine the size of the likely K-sparse support set ΩK under this particular signal model using
a simple counting argument. The location of the ﬁrst non-zero coefﬁcients is among the ﬁrst N α
indices with probability 1 − ǫ/K. After ﬁxing the location of the ﬁrst coefﬁcient  the location of
the second coefﬁcient is among the next N α indices immediately following the ﬁrst location with
probability 1 − ǫ/K. Proceeding this way  after the locations of the ﬁrst j − 1 coefﬁcients  have been
ﬁxed  we have that the jth non-zero coefﬁcient is among N α candidate locations with probability
1 − ǫ/K. In this way  we obtain a set of supports ΩK ǫ of size N αK that will occur with probability
(1 − ǫ/K)K > 1 − ǫ. Thus for the (K  ǫ)-PRIP to hold for a random matrix  the matrix must have
M = cK(1 + α log N ) rows  as compared to the cK log(N/K) rows required for the standard
K-RIP to hold. When α is on the order of (log N )−1  the number of measurements required and the
complexity of the solution method grow essentially linearly in K  which is a considerable improve-
ment over the best possible M = O(K log(N/K)) measurements required without such a priori
information.
3 Graphical models for compressive sensing
Clustering of the nonzero coefﬁcients in a sparse signal representation can be realistically captured
by a probabilistic graphical model such as a Markov random ﬁeld (MRF); in this paper we will
focus for concreteness on the classical Ising model [10].
Support model. We begin with an Ising model for the signal support. Suppose we have a K-sparse
signal x ∈ RN whose support is represented by s ∈ {−1  1}N such that si = −1 when xi = 0 and
si = 1 when xi 6= 0. The probability density function (PDF) of the signal support can be modeled
using a graph Gs = (Vs  Es)  where Vs = {1  . . .   N } denotes a set of N vertices – one for each
of the support indices – and Es denotes the set of edges connecting support indices that are spatial
neighbors (see Figure 2(a)). The contribution of the interaction between two elements {si  sj} in
the support of x is controlled by the coefﬁcient λij > 0. The contribution of each element si is
controlled by a coefﬁcient λi  resulting in the following PDF for the sparse support s:

where Zs(λ) is a strictly convex partition function with respect to λ that normalizes the distribution
so that it integrates to one. The parameter vector λ quantiﬁes our prior knowledge regarding the

λisi − Zs(λ)


 

(2)

p(s; λ) = exp


X

(i j)∈Es

λij sisj + X

i∈Vs

3

sj

si

xj

xi

sj

si

y1

yM

xj

xi

sj

si

(a)

Figure 2: Examplegraphicalmodels: (a)Isingmodelforthesupport (b)Markovrandomﬁeldmodelforthe
resultingcoefﬁcients (c)MarkovrandomﬁeldwithCSmeasurements.

(b)

(c)

signal support s and consists of the edge interaction parameters λij and the vertex bias parameters
λi. These parameters can be learned from data using ℓ1-minimization techniques [11].
The Ising model enforces coefﬁcient clustering. For example  compare the clustered sparsity of the
real background subtracted image in Figure 1(b) with the dispersed “independent” sparsity of the
random image in Figure 1(c). While both images (b) and (c) are equally sparse  under a trained Ising
model (λij = 0.45 and λi = 0)  the image (b) is approximately 10856 times more likely than the
image (c).
Signal model. Without loss of generality  we focus on 2D images that are sparse in the space
domain  as in Figure 1(b). Leveraging the Ising support model from above  we apply the MRF
graphical model in Figure 2(b) for the pixel coefﬁcient values. Under this model  the support is
controlled by an Ising model  and the signal values are independent given the support. We now
develop a joint PDF for the image pixel values x  the support labels s  and the CS measurements y.

We begin with the support PDF p(s) from (2) and assume that we are equipped with a sparsity-
promoting PDF p(x|s) for x given s. The most commonly used PDF is the Laplacian density (which
is related to the ℓ1-norm of x); however  other reference priors  such as generalized Gaussians that
are related to the ℓp-norm of x  p < 1  can be more effective [12]. We assume that the measurements

y are corrupted by i.i.d. Gaussian noise  i.e.  p(y|x) = N (cid:0)y|Φx  σ2I(cid:1)  where σ2 is the unknown

noise variance.

From Figure 2(c)  it is easy to show that  given the signal x  the signal support s and the compressive
measurements y are independent using the D-separation property of graphs [13]. Hence  the joint
distribution of the vertices in the graph in Figure 2(b) can be written as

p(z) = p(s  x  y) = p(s  x)p(y|s  x) = p(s)p(x|s)p(y|x) 

where z = [sT   xT   yT ]T . Then  (3) can be explicitly written as

p(z) ∝ exp


X

(i j)∈Es

λijsisj + X

i∈Vs

[λisi + log(p(xi|si))] −

1
2σ2 ||y − Φx||2

2

(3)

(4)




.

4 Lattice matching pursuit
Using the coefﬁcient graphical model from Section 3  we are now equipped to develop a new model-
based CS signal recovery algorithm. Lattice Matching Pursuit (LaMP) is a greedy algorithm for
signals on 2D lattices (images) in which the likelihood of the signal support is iteratively evaluated
and optimized under an Ising model. By enforcing a graphical model  (i) partial knowledge of
the sparse signal support greatly decreases the ambiguity and thus size of the search space for the
remaining unknown part  accelerating the speed of the algorithm; and (ii) signal supports of the same
size but different structures result in different likelihoods (recall Figure 1(b) and (c))  decreasing the
required number of CS measurements and increasing the numerical stability.
Algorithm. The LaMP pseudocode is given in Algorithm 1. Similar to other greedy recovery al-
gorithms such as matching pursuit and CoSaMP [4]  each iteration of LaMP starts by estimating a
data residual r{k} given the current estimate of the signal x{k−1} (Step 1). After calculating the
residual  LaMP calculates a temporary signal estimate (Step 2) denoted by x{k}
. This signal esti-
mate is the sum of the previous estimate x{k−1} and Φ′r{k}  accounting for the current residual.
Using this temporary signal estimate as a starting point  LaMP then maximizes the likelihood (4)
over the support via optimization (Step 3). This can be efﬁciently solved using graph cuts with

t

4

Algorithm 1: LaMP – Lattice Matching Pursuit

Input: y  Φ  x{0} = 0  s{0} = −1  and eK (desired sparsity).
Output: A eK-sparse approximation x of the acquired signal.

Algorithm:
repeat {Matching Pursuit Iterations}
Step 1. Calculate data residual:

r{k} = y − Φx{k−1};

Step 2. Propose a temporary target signal estimate:

x{k}
t = Φ′r{k} + x{k−1};

Step 3. Determine MAP estimate of the support using graph cuts:

s{k} = maxs∈{−1 +1}N P(i j)∈Es λij sisj +Pi∈Vs hλisi + log(p([x{k}

t

Step 4. Estimate target signal:

t = 0;

Step 5. Iterate:
k = k + 1;

t[s{k} = 1] = Φ†[:  s{k} = 1]y; x{k} = Prune{t; eK};

until Maximum iterations or (cid:13)(cid:13)r{k}(cid:13)(cid:13) < threshold;

Return x = x{k}.

p(xi|si = −1)

p(xi|si = +1)

1
ǫ1

ǫ1

L

⇒ log p(xi|si = −1)

− log ǫ1

ǫ2
ǫ3

⇒ log p(xi|si = +1)

τ ′

τ

log ǫ1

− log ǫ1

⇒ log p(xi|si=−1)
log ǫ2
log ǫ3
⇒ log p(xi|si=+1)

log ǫ1

]i|si))i;

1

−1

U−1(xi; τ )
≈ 1
τ
0

U+1(xi; τ )

Figure 3: Geometrical approximations of p(xi|si = −1) and log p(xi|si = +1).

O(N ) complexity [14]. In particular  for planar Ising models  the global minimum of the problem
can be obtained. Once a likely signal support s{k} is obtained in Step 3  LaMP obtains an up-
dated signal estimate x{k} using least squares with the selected columns of the measurement matrix

Φ[:  s{k} = 1] and pruning back to the largest eK signal coefﬁcients (Step 4). Hence  the parameter
eK controls the sparsity of the approximation. In Step 4  a conjugate gradient method is used for

efﬁciently performing the product by a pseudoinverse. If the graphical model includes dependencies
between the signal values xi  we then replace the pseudoinverse product by a belief propagation
algorithm to efﬁciently solve for the signal values x{k} within Step 4.
Signal log-likelihood log p(x|s).
The correct signal PDF to use given the support p(x|s) is
problem-dependent. Here  we provide one approximation that mimics the ℓ0 minimization for CS
recovery for the signal graphical model in Figure 2(c); we also use this in our experiments in Sec-
tion 5. The state si = 1 represents a nonzero coefﬁcient; thus  all nonzero values of xi should
have equal probability  and the value xi = 0 should have zero probability. Similarly  the state
si = −1 represents a zero-valued coefﬁcient; thus  the mass of its probability function is concen-
trated at zero. Hence  we use the approximations for xi ∈ [−L  L]  a restricted dynamic range:
p(xi|si = −1) = δ(xi) and p(xi|si = 1) = (1 − δ(xi))/2L. However  the optimization over
the joint PDF in (4) requires a “smoothing” of these PDFs for two reasons: (i) to obtain robustness
against noise and numerical issues; and (ii) to extend the usage of the algorithm from sparse to
compressible signals.
We approximate log p(xi|si = ±1) using the parametric form illustrated in Figure 3. Here  the
constant τ is a slack parameter to separate large and small signal coefﬁcients  and ǫ1  ǫ2  and ǫ3 are
chosen according to τ and L to normalize each PDF. We also denote a = ǫ3L  with a ≈ 1. Using
the normalization constraints  it is possible to show that as the dynamic range increases 

lim
L→∞

−

log ǫ2
log ǫ1

→

1
τ a

and

lim
L→∞

−

log ǫ3
log ǫ1

→ 0.

5

Hence  we approximate the likelihoods using the utility functions Usi (x; τ ) that follow this form.
The optimization problem used by Step 3 of LaMP to determine the support is then approximately
equivalent to the following problem

(5)

s{k+1} =

s∈{−1 +1}N X

(i j)∈Eseλij sisj + X

max

i∈Vs

heλisi + Usi ([x

{k+1}
t

]i; τ )i  

log ǫ1

sparseness on the lattice structure.

. If the signal values are known to be positive  then the deﬁnitions of Usi can

cost incurred by enforcing the lattice structure is too large. The pruning operation in Step 4 of LaMP

is prior information on the signal support. The threshold τ is chosen at each iteration adaptively by

4 )  where m is called the average
magnetization. In our recovery problem  the average magnetization and the desired signal sparsity

where eλ = λ
be changed to enforce the positivity during estimation. The choice of eλij is related to the desired
To enforce a desired sparsity eK on the lattice structure  we apply statistical mechanics results on
the 2D Ising model and choose eλij = 0.5 arcsin((1 − m8)− 1
has a simple relationship: m = h(+1) × eK + (−1) × (N − eK)i /N . We set eλi = 0 unless there
sorting the magnitudes of the temporary target signal estimate coefﬁcients and determining the 5eK
threshold; this gives preference to the largest 5eK coefﬁcients that attain states si = 1  unless the
then enforces the desired sparsity eK.
5 Experiments
We now use several numerical simulations to demonstrate that for spatially clustered sparse signals 
which have high likelihood under our MRF model  LaMP requires far fewer measurements and
fewer computations for robust signal recovery than state-of-the-art greedy and optimization tech-
niques.1
Experiment 1: Shepp-Logan phantom. Figure 4 (top left) shows the classical N = 100 ×
100 Shepp-Logan phantom image. Its sparsity in the space domain is K = 1740. We obtained
compressive measurements of this image  which were then immersed in additive white Gaussian
noise to an SNR of 10dB. The top row of Figure 4 illustrates the iterative image estimates obtained
using LaMP from just M = 2K = 3480 random Gaussian measurements of the noisy target.
Within 3 iterations  the support of the image is accurately determined; convergence occurs at the 5th
iteration.

Figure 4 (bottom) compares LaMP to CoSaMP [4]  a state-of-the-art greedy recovery algorithm  and
ﬁxed-point continuation (FPC) [17]  a state-of-the-art ℓ1-norm minimization recovery algorithm us-
ing the same set of measurements. Despite the presence of high noise (10dB SNR)  LaMP perfectly
recovers the signal support from only a small number of measurements. It also outperforms both
CoSaMP and FPC in terms of speed.
Experiment 2: Numerical stability. We demonstrate LaMP’s stability in the face of substantial
measurement noise. We tested both LaMP and FPC with a number of measurements that gave close
to perfect recovery of the Shepp-Logan phantom in the presence of a small amount of noise; for
LaMP  setting M = 1.7K sufﬁces  while FPC requires M = 4K. We then studied the degradation
of the recovery quality as a function of the noise level for both algorithms. For reference  a value
of σ = 20 corresponds to a measurement-to-noise ratio of just 6dB. The results in Figure 5(a)
demonstrate that LaMP is stable for a wide range of measurement noise levels. Indeed  the rate of
increase of the LaMP recovery error as a function of the noise variance σ (a measure of the stability
to noise) is comparable to that of FPC  while using far fewer measurements.
Experiment 3: Performance on real background subtracted images. We test the recovery
algorithms over a set of background subtraction images. The images were obtained from a test
video sequence  one image frame of which is shown in Figure 1  by choosing at random two frames
from the video and subtracting them in a pixel-wise fashion. The large-valued pixels in the resulting
images are spatially clustered and thus are well-modeled by the MRF enforced by LaMP. We created
100 different test images; for each image  we deﬁne the sparsity K as the number of coefﬁcients

1We use the GCOptimization package [14–16] to solve the support recovery problem in Step 3 in Algorithm

1 in our implementation of LaMP.

6

Noise-free target

LaMP Iter. #1

LaMP Iter. #2

LaMP Iter. #3

LaMP Iter. #4

LaMP Iter. #5  0.9s

CoSaMP  6.2s

FPC  6.5s

Figure 4: Top: LaMPrecoveryoftheShepp-Logan phantom(N = 100 × 100  K = 1740  SNR = 10dB)
from M = 2K = 3480 noisymeasurements. Bottom: RecoveriesfromLaMP CoSaMP andFPC including
runningtimesonthesamecomputer.

3000

2500

2000

1500

1000

500

r
o
r
r
e

 

n
o

i
t
c
u
r
t
s
n
o
c
e
r
 

m
u
m
x
a
M

i

 
0
0

LaMP  M = 1.7K
FPC  M = 5K
FPC  M = 4K

 

5

10
σ

(a)

15

20

e
d
u

t
i

n
g
a
m

 
r
o
r
r
e

 

d
e
z

i
l

a
m
r
o
n

 

e
g
a
r
e
v
A

1.5

1

0.5

0
 
0

 

LaMP
CoSaMP
FPC

1

2

3
M/K

4

5

(b)

Figure 5: PerformanceofLaMP.(a)Maximumrecoveryerrorover1000 noiseiterationsasafunctionofthe
input noise variance. LaMP has the same robustness to noise as the FPC algorithm. (b) Performance over
backgroundsubtractiondatasetof100images. LaMPachievesthebestperformanceatM ≈ 2.5K whileboth
FPCandCoSaMPrequire M > 5K toachievethesameperformance.

that contain 97% of the image energy. We then performed recovery of the image using the LaMP 
CoSaMP  and FPC algorithms under varying number of measurements M   from 0.5K to 5K. An
example recovery is shown in Figure 6.

For each test and algorithm  we measured the magnitude of the estimation error normalized by the
magnitude of the original image. Figure 5(b) shows the mean and standard deviations for the nor-
malized error magnitudes of the three algorithms. LaMP’s graphical model reduces the number of
measurements necessary for acceptable recovery quality to M ≈ 2.5K  while the standard algo-
rithms require M ≥ 5K measurements to achieve the same quality.

6 Conclusions
We have presented an initial study of model-based CS signal recovery using an MRF model to cap-
ture the structure of the signal’s sparse coefﬁcients. As demonstrated in our numerical simulations 
for signals conforming to our model  the resulting LaMP algorithm requires signiﬁcantly fewer CS
measurements  has lower computational complexity  and has equivalent numerical stability to the
current state-of-the-art algorithms. We view this as an initial step toward harnessing the power of
modern compression and data modeling methods for CS reconstruction.

Much work needs to be done  however. We are working to precisely quantify the reduction in the
required number of measurements (our numerical experiments suggest that M = O(K) is sufﬁcient
for stable recovery) and computations. We also assert that probabilistic signal models hold the key
to formulating inference problems in the compressive measurement domain since in many signal
processing applications  signals are acquired merely for the purpose of making an inference such as
a detection or classiﬁcation decision.

7

Target

LaMP

CoSaMP

FPC

Figure 6: Examplerecoveriesforbackgroundsubtractionimages using M = 3K foreachimage.

Acknowledgements. We thank Wotao Yin for helpful discussions  and Aswin Sankaranarayanan
for data used in Experiment 3. This work was supported by grants NSF CCF-0431150 and CCF-
0728867  DARPA/ONR N66001-08-1-2065  ONR N00014-07-1-0936 and N00014-08-1-1112 
AFOSR FA9550-07-1-0301  ARO MURI W311NF-07-1-0185  and the TI Leadership Program.
References
[1] D. L. Donoho. Compressed sensing. IEEE Trans. Info. Theory  52(4):1289–1306  Sept. 2006.
[2] E. J. Cand`es. Compressive sampling.
pages 1433–1452  Madrid  Spain  2006.

In Proc. International Congress of Mathematicians  volume 3 

[3] S. L. Lauritzen. Graphical Models. Oxford University Press  1996.
[4] D. Needell and J. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples.

Applied and Computational Harmonic Analysis  June 2008. To appear.

[5] C. La and M. N. Do. Tree-based orthogonal matching pursuit algorithm for signal reconstruction.

IEEE Int. Conf. Image Processing (ICIP)  pages 1277–1280  Atlanta  GA  Oct. 2006.

In

[6] M. F. Duarte  M. B. Wakin  and R. G. Baraniuk. Wavelet-domain compressive signal reconstruction using

a hidden Markov tree model. In ICASSP  pages 5137–5140  Las Vegas  NV  April 2008.

[7] V. Cevher  A. Sankaranarayanan  M. F. Duarte  D. Reddy  R. G. Baraniuk  and R. Chellappa. Compressive

sensing for background subtraction. In ECCV  Marseille  France  Oct. 2008.

[8] R. G. Baraniuk  M. Davenport  R. A. DeVore  and M. B. Wakin. A simple proof of the restricted isometry

property for random matrices. 2006. To appear in Const. Approx.

[9] T. Blumensath and M. E. Davies. Sampling theorems for signals from the union of linear subspaces.

2007. Preprint.

[10] B. M. McCoy and T. T. Wu. The two-dimensional Ising model. Harvard Univ. Press  1973.
[11] M. J. Wainwright  P. Ravikumar  and J. D. Lafferty. High-dimensional graphical model selection using

ℓ1-regularized logistic regression. In Proc. of Advances in NIPS  2006.

[12] D. P. Wipf and B. D. Rao. Sparse bayesian learning for basis selection.

52(8):2153–2164  August 2004.

IEEE Trans. Sig. Proc. 

[13] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kauf-

mann Publishers  1988.

[14] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Trans. on

Pattern Anal. and Mach. Int.  26(2):147–159  2004.

[15] Y. Boykov  O. Veksler  and R. Zabih. Efﬁcient approximate energy minimization via graph cuts. IEEE

Trans. on Pattern Anal. and Mach. Int.  20(12):1222–1239  Nov. 2001.

[16] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. IEEE Trans. on Pattern Anal. and Mach. Int.  26(9):1124–1137  Sept. 2004.

[17] E. T. Hale  W Yin  and Y. Zhang. A ﬁxed-point continuation method for ℓ1-regularized minimization with

applications to compressed sensing. Technical Report TR07-07  Rice University  CAM Dept.  2007.

8

,Siu On Chan
Ilias Diakonikolas
Rocco Servedio
Xiaorui Sun
Liang Zhang
Guangming Zhu
Lin Mei
Peiyi Shen
Syed Afaq Ali Shah
Mohammed Bennamoun
Yuki Yoshida
Masato Okada