2009,Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction,Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Outgoing from these observations we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both of these problems  in particular  if the data lies on or close to a low-dimensional submanifold in the feature space  the Hessian energy prefers functions which vary ``linearly with respect to the natural parameters in the data. This property makes it also particularly suited for the task of semi-supervised dimensionality reduction where the goal is to find the natural parameters in the data based on a few labeled points. The experimental result suggest that our method is superior to semi-supervised regression using Laplacian regularization and standard supervised methods and is particularly suited for semi-supervised dimensionality reduction.,Semi-supervised Regression using Hessian Energy

with an Application to Semi-supervised

Dimensionality Reduction

Kwang In Kim1  Florian Steinke2 3  and Matthias Hein1

1Department of Computer Science  Saarland University Saarbr¨ucken  Germany

2Siemens AG Corporate Technology Munich  Germany

{kimki hein}@cs.uni-sb.de  Florian.Steinke@siemens.com

3MPI for Biological Cybernetics  Germany

Abstract

Semi-supervised regression based on the graph Laplacian suffers from the fact
that the solution is biased towards a constant and the lack of extrapolating power.
Based on these observations  we propose to use the second-order Hessian energy
for semi-supervised regression which overcomes both these problems. If the data
lies on or close to a low-dimensional submanifold in feature space  the Hessian
energy prefers functions whose values vary linearly with respect to geodesic dis-
tance. We ﬁrst derive the Hessian energy for smooth manifolds and continue to
give a stable estimation procedure for the common case where only samples of
the underlying manifold are given. The preference of ‘’linear” functions on mani-
folds renders the Hessian energy particularly suited for the task of semi-supervised
dimensionality reduction  where the goal is to ﬁnd a user-deﬁned embedding
function given some labeled points which varies smoothly (and ideally linearly)
along the manifold. The experimental results suggest superior performance of our
method compared with semi-supervised regression using Laplacian regularization
or standard supervised regression techniques applied to this task.

1

Introduction

Central to semi-supervised learning is the question how unlabeled data can help in either classiﬁca-
tion or regression. A large class of methods for semi-supervised learning is based on the manifold
assumption  that is  the data points do not ﬁll the whole feature space but they are concentrated
around a low-dimensional submanifold. Under this assumption unlabeled data points can be used
to build adaptive regularization functionals which penalize variation of the regression function only
along the underlying manifold.
One of the main goals of this paper is to propose an appropriate regularization functional on a man-
ifold  the Hessian energy  and show that it has favourable properties for semi-supervised regression
compared to the well known Laplacian regularization [2  12]. Opposite to the Laplacian regularizer 
the Hessian energy allows functions that extrapolate  i.e. functions whose values are not limited to
the range of the training outputs. Particularly if only few labeled points are available  we show that
this extrapolation capability leads to signiﬁcant improvements. The second property of the proposed
Hessian energy is that it favors functions which vary linearly along the manifold  so-called geodesic
functions deﬁned later. By linearity we mean that the output values of the functions change linearly
along geodesics in the input manifold. This property makes it particularly useful as a tool for semi-
supervised dimensionality reduction [13]  where the task is to construct user-deﬁned embeddings
based on a given subset of labels. These user-guided embeddings are supposed to vary smoothly or
even linearly along the manifold  where the later case corresponds to a setting where the user tries to

1

recover a low-distortion parameterization of the manifold. Moreover  due to user deﬁned labels the
interpretability of the resulting parameterization is signiﬁcantly improved over unsupervised meth-
ods like Laplacian [1] or Hessian [3] eigenmaps. The proposed Hessian energy is motivated by the
recently proposed Eells energy for mappings between manifolds [11]  which contains as a special
case the regularization of real-valued functions on a manifold. In ﬂavour  it is also quite similar to
the operator constructed in Hessian eigenmaps [3]. However  we will show that their operator due
to problems in the estimation of the Hessian  leads to useless results when used as regularizer for re-
gression. On the contrary  our novel estimation procedure turns out to be more stable for regression
and as a side effects leads also to a better estimation of the eigenvectors used in Hessian eigenmaps.
We present experimental results on several datasets  which show that our method for semi-supervised
regression is often superior to other semi-supervised and supervised regression techniques.

2 Regression on manifolds

Our approach for regression is based on regularized empirical risk minimization. First  we will
discuss the problem and the regularizer in the ideal case where we know the manifold exactly 
corresponding to the case where we have access to an unlimited number of unlabeled data. In the
following we denote by M the m-dimensional data-submanifold in Rd. The supervised regression
problems for a training set of l points (Xi  Yi)l

i=1 can then be formulated as 

l(cid:88)

i=1

arg min
f∈C∞(M )

1
l

L(Yi  f(Xi)) + λ S(f) 

where C∞(M) is the set of smooth functions on M  L : R × R → R is the loss function and S :
C∞(M) → R is the regularization functional. For simplicity we use the squared loss L(y  f(x)) =
(y − f(x))2  but the framework can be easily extended to other convex loss functions.
Naturally  we do not know the manifold M the data is lying on. However  we have unlabeled data
which can be used to estimate it  or more precisely we can use the unlabeled data to build an estimate
ˆS(f) of the true regularizer S(f). The proper estimation of S(f) will be the topic of the next section.
For the moment we just want to discuss regularization functionals in the ideal case  where we know
the manifold. However  we would like to stress already here that for our framework to work it does
not matter if the data lies on or close to a low-dimensional manifold. Even the dimension can change
from point to point. The only assumption we make is that the data generating process does not ﬁll
the whole space but is concentrated on a low-dimensional structure.

Regularization on manifolds. Our main goal is to construct a regularization functional on mani-
folds  which is particularly suited for semi-supervised regression and semi-supervised dimensional-
ity reduction. We follow here the framework of [11] who discuss regularization of mappings between
manifolds  where we are interested in the special case of real-valued output. They propose to use the
so called Eells-energy SEells(f)  which can be written for real-valued functions  f : M → R  as 

where ∇a∇bf is the second covariant derivative of f and dV (x) is the natural volume element  see
[7]. Note  that the energy is by deﬁnition independent of the coordinate representation and depends
only on the properties of M. For details we refer to [11]. This energy functional looks quite abstract.
However  in a special coordinate system on M  the so called normal coordinates  one can evaluate
it quite easily. In sloppy terms  normal coordinates at a given point p are coordinates on M such
that the manifold looks as Euclidean as possible (up to second order) around p. Thus in normal
coordinates xr centered at p 
∇a∇bf

b =⇒ (cid:107)∇a∇bf(cid:107)2
p M⊗T ∗
T ∗

(cid:18) ∂2f

m(cid:88)

a ⊗ dxs

dxr

m(cid:88)

(cid:19)2

p M =

  (1)

(cid:12)(cid:12)(cid:12)p

=

(cid:12)(cid:12)(cid:12)p

∂2f

∂xr∂xs

r s=1

∂xr∂xs

r s=1

so that at p the norm of the second covariant derivative is just the Frobenius norm of the Hessian of f
in normal coordinates. Therefore we call the resulting functional the Hessian regularizer SHess(f).

2

(cid:90)

M

SEells(f) =

(cid:107)∇a∇bf(cid:107)2
x M⊗T ∗
T ∗

x M dV (x) 

Figure 1: Difference between semi-supervised regression using
Laplacian and Hessian regularization for ﬁtting two points on the
one-dimensional spiral. The Laplacian regularization has always
a bias towards the constant function (for a non-zero regularization
parameter it will not ﬁt the data exactly) and the extrapolation be-
yond data points to the boundary of the domain is always constant.
The non-linearity of the ﬁtted function between the data point arises
due to the non-uniform sampling of the spiral. On the contrary the
Hessian regularization ﬁts the data perfectly and extrapolates nicely
to unseen data  since it’s null space contains functions which vary
linearly with the geodesic distance.

Before we discuss the discretization  we would like to discuss some properties of this regularizer. In
particular  its difference to the regularizer S∆(f) using the Laplacian 

(cid:90)

M

S∆(f) =

(cid:107)∇f(cid:107)2 dV (x)

proposed by Belkin and Niyogi [2] for semi-supervised classiﬁcation and in the meantime also
adopted for semi-supervised regression [12]. While this regularizer makes sense for classiﬁcation  it
is of limited use for regression. The problem is that the null space NS = {f ∈ C∞(M)| S(f) = 0}
of S∆  that is the functions which are not penalized  are only the constant functions on M. The
following adaptation of a result in [4] shows that the Hessian regularizer has a richer null-space.
Proposition 1 (Eells  Lemaire [4]) A function f : M → R with f ∈ C∞(M) has zero second
derivative  ∇a∇bf
∀x ∈ M  if and only if for any geodesic γ : (−ε  ε) → M parameter-
ized by arc length s  there exists a constant cγ depending only on γ such that

(cid:12)(cid:12)(cid:12)x

= 0 

f(cid:0)γ(s)(cid:1) = cγ 

∂
∂s

∀ − ε < s < ε.

∂s f(γ(s)) = const. geodesic functions. They correspond to linear
We call functions f which fulﬁll ∂
maps in Euclidean space and encode a constant variation with respect to the geodesic distance of
the manifold. It is however possible that apart from the trivial case f = const. no other geodesic
functions exist on M. What is the implication of these results for regression? First  the use of Lapla-
cian regularization leads always to a bias towards the constant function and does not extrapolate
beyond data points. On the contrary  Hessian regularization is not biased towards constant functions
if geodesic functions exist and extrapolates “linearly” (if possible) beyond data points. These crucial
differences are illustrated in Figure 1 where we compare Laplacian regularization using the graph
Laplacian as in [2] to Hessian regularization as introduced in the next section for a densely sampled
spiral. Since the spiral is isometric to a subset of R  it allows “geodesic” functions.

3 Semi-supervised regression using Hessian energy

As discussed in the last section unlabeled data provides us valuable information about the data man-
ifold. We use this information to construct normal coordinates around each unlabeled point  which
requires the estimation of the local structure of the manifold. Subsequently  we employ the normal
coordinates to estimate the Hessian regularizer using the simple form of the second covariant deriva-
tive provided in Equation (1). It turns out that these two parts of our construction are similar to the
one done in Hessian eigenmaps [3]. However  their estimate of the regularizer has stability prob-
lems when applied to semi-supervised regression as is discussed below. In contrast  the proposed
method does not suffer from this short-coming and leads to signiﬁcantly better performance. The
solution of the semi-supervised regression problem is obtained by solving a sparse linear system. In
the following  capital letters Xi correspond to sample points and xr denote normal coordinates.

Construction of local normal coordinates. The estimation of local normal coordinates can be
done using the set of k nearest neighbors (NN) Nk(Xi) of point Xi. The cardinality k will be
chosen later on by cross-validation. In order to estimate the local tangent space TXiM (seen as an

3

−202−202−10105101520−2−1012geodesic distance along spiralestimated output  Laplacian regularizationHessian regularizationm-dimensional afﬁne subspace of Rd)  we perform PCA on the points in Nk(Xi). The m leading
eigenvectors then correspond to an orthogonal basis of TXiM. In the ideal case  where one has a
densely sampled manifold  the number of dominating eigenvalues should be equal to the dimension
m. However  for real-world datasets like images the sampling is usually not dense enough so that the
dimension of the manifold can not be detected automatically. Therefore the number of dimensions
has to be provided by the user using prior knowledge about the problem or alternatively  and this is
the way we choose in this paper  by cross-validation.
Having the exact tangent space TXiM one can determine the normal coordinates xr of a point
Xj ∈ Nk(Xi) as follows. Let {ur}m
r=1 be the m leading PCA eigenvectors  which have been
normalized  then the normal coordinates {xr}m

xr(Xj) = (cid:104)ur  Xj − Xi(cid:105)

r=1 of Xj are given as 
(cid:80)m
dM (Xj  Xi)2
r=1 (cid:104)ur  Xj − Xi(cid:105)2  

dM (Xj  Xi) of Xj to Xi on M  (cid:107)x(Xj)(cid:107)2 =(cid:80)m

where the ﬁrst term is just the projection of the difference vector  Xj − Xi  on the basis vector ur ∈
TXi M and the second component is just a rescaling to fulﬁll the property of normal coordinates that
the distance of a point Xj ∈ M to the origin (corresponding to Xi) is equal to the geodesic distance
r=1 xr(Xi)2 = dM (Xj  Xi)2. The rescaling makes
sense only if local geodesic distances can be accurately estimated. In our experiments  this was only
the case for the 1D-toy dataset of Figure 1. For all other datasets we therefore use xr(Xj) =
(cid:104)ur  Xj − Xi(cid:105) as normal coordinates. In [11] it is shown that this replacement yields an error of
order O((cid:107)∇af(cid:107)2 κ2) in the estimation of (cid:107)∇a∇bf(cid:107)2  where κ is the maximal principal curvature
(the curvature of M with respect to the ambient space Rd).

Estimation of the Hessian energy. The Hessian regularizer  the squared norm of the second co-
variant derivative  (cid:107)∇a∇bf(cid:107)2  corresponds to the Frobenius norm of the Hessian of f in normal
coordinates  see Equation 1. Thus  given normal coordinates xr at Xi we would like to have an
operator H which given the function values f(Xj) on Nk(Xi) estimates the Hessian of f at Xi 

with Ars = Asr. In order to ﬁt the polynomial we use standard linear least squares 

where Φ ∈ Rk×P is the design matrix with P = m+ m(m+1)
. The corresponding basis functions φ 
are the monomials  φ = [x1  . . .   xm  x1x1  x1x2  . . .   xmxm]  of the normal coordinates (centered
at Xi) of Xj ∈ Nk(xi) up to second order. The solution w ∈ RP is w = Φ+f  where f ∈ Rk and
fj = f(Xj) with Xj ∈ Nk(Xi) and Φ+ denotes the pseudo-inverse of Φ.
Note  that the last m(m+1)
components of w correspond to the coefﬁcients Ars of the polynomial
(up to rescaling for the diagonal components) and thus with Equation (3) we obtain the desired form
H (i)

rsj. An estimate of the Frobenius norm of the Hessian of f at Xi is thus given as 

2

2

(cid:107)∇a∇bf(cid:107)2 ≈ m(cid:88)

(cid:16) k(cid:88)

(cid:17)2

k(cid:88)

H (i)

rsαfα

=

fαfβB(i)
αβ 

r s=1

α=1

α β=1

4

This can be done by ﬁtting a second-order polynomial p(x) in normal coordinates to {f(Xj)}k

j=1 

p(i)(x) = f(Xi) +

Brxr +

Arsxrxs 

(2)

where the zeroth-order term is ﬁxed at f(Xi). In the limit as the neighborhood size tends to zero 
p(i)(x) becomes the second-order Taylor expansion of f around Xi  that is 

(3)

∂2f

∂xr∂xs

(cid:12)(cid:12)(cid:12)Xi
≈ k(cid:88)
n(cid:88)

j=1

H (i)

rsj f(Xj).

n(cid:88)

n(cid:88)

r=1

r

s=r

 

Ars =

(cid:12)(cid:12)(cid:12)Xi
(cid:12)(cid:12)(cid:12)Xi
(cid:16)(cid:0)f(Xj) − f(Xi)(cid:1) − (Φw)j
(cid:17)2

∂xr∂xs

∂2f

1
2

 

 

Br = ∂f
∂xr

k(cid:88)

j=1

arg min
w∈RP

αβ =(cid:80)m
n(cid:88)

ˆSHess(f) =

where B(i)
over all data points  where n denotes the number of unlabeled and labeled points 

rsβ and ﬁnally the total estimated Hessian energy ˆSHess(f) is the sum

r s=1 H (i)

rsαH (i)

m(cid:88)

(cid:18) ∂2f

(cid:19)2

(cid:12)(cid:12)(cid:12)Xi

n(cid:88)

=

(cid:88)

(cid:88)

i=1

r s=1

∂xr∂xs

i=1

α∈Nk(Xi)

β∈Nk(Xi)

fαfβB(i)

αβ = (cid:104)f  Bf(cid:105)  

where B is the accumulated matrix summing up all the matrices B(i). Note  that B is sparse since
each point Xi has only contributions from its neighbors.
Moreover  since we sum up the energy over all points  the squared norm of the Hessian is actu-
ally weighted with the local density of the points leading to a stronger penalization of the Hessian
in densely sampled regions. The same holds for the estimate ˆS∆(f) of Laplacian regularization 
i j=1 wij(fi − fj)2  where one also sums up the contributions of all data points (the

ˆS∆(f) = (cid:80)n

rigorous connection between ˆS∆(f) and S∆(f) has been established in [2  5]).
The effect of non-uniform sampling can be observed in Figure 1. There the samples of the spiral are
generated by uniform sampling of the angle leading to a more densely sampled “interior” region 
which leads to the non-linear behavior of the function for the Laplacian regularization. For the
Hessian energy this phenomena cannot be seen in this example  since the Hessian of a “geodesic”
function is zero everywhere and therefore it does not matter if it is weighted with the density. On
the other hand for non-geodesic functions the weighting matters also for the Hessian energy. We did
not try to enforce a weighting with respect to the uniform density. However  it would be no problem
to compensate the effects of non-uniform sampling by using a weighted from of the Hessian energy.

Final algorithm. Using the ideas of the previous paragraphs the ﬁnal algorithmic scheme for
semi-supervised regression can now be immediately stated. We have to solve 

(Yi − f(Xi))2 + λ(cid:104)f  Bf(cid:105)  

(4)

l(cid:88)

i=1

arg min

f∈Rn

1
l

where for notational simplicity we assume that the data is ordered such that the ﬁrst l points are
labeled. The solution is obtained by solving the following sparse linear system 

(I(cid:48) + l λB)f = Y 
where I(cid:48) is the diagonal matrix with I(cid:48)
ii = 1 if i is labeled and zero else and Yi = 0 if i is not
labeled. The sparsity structure of B is mainly inﬂuencing the complexity to solve this linear system.
However  the number of non-zeros entries of B is between O(nk) and O(nk2) depending on how
well behaved the neighborhoods are (the later case corresponds basically to random neighbors) and
thus grows linearly with the number of data points.

Stability of estimation procedure of Hessian energy. Since we optimize the objective in Equa-
tion (4) for any possible assignment of function values f on the data points  we have to ensure that
the estimation of the Hessian is accurate for any possible function. However  the quality of the es-
timate of the Hessian energy depends on the quality of the local ﬁt of p(i) for each data point Xi.
Clearly  there are function assignments where the estimation goes wrong. If k < P (P is the number
of parameters of the polynomial) p can overﬁt the function and if k > P then p generally underﬁts.
In both cases  the Hessian estimation is inaccurate. Most dangerous are the cases where the norm of
the Hessian is underestimated in particular if the function is heavily oscillating. Note that during the
estimation of local Hessian  we do not use the full second-order polynomial but ﬁx its zeroth-order
term at the value of f (i.e. p(i)(Xi) = f(Xi); cf. Eq. (2)). The reason for this is that underﬁtting is
much more likely if one ﬁts a full second-order polynomial since the additional ﬂexibility in ﬁtting
the constant term always reduces the Hessian estimate. In the worst case a function which is heavily
oscillating can even have zero Hessian energy  if it allows a linear ﬁt at each point  see Figure 3. If
such a function ﬁts the data well we get useless regression results1 see Fig. 3. While ﬁxing the con-
stant term does not completely rule out such undesired behavior  we did not observe such irregular
solutions in any experiment. In the appendix we discuss a modiﬁcation of (Eq. (4)) which rules out

1For the full second-order polynomial even cross-validation does not rule out these irregular solutions.

5

Figure 2: Fitting two points on the spiral revis-
ited (see Fig. 1): Left image shows the regres-
sion result f using the Hessian energy estimated
by ﬁtting a full polynomial in normal coordi-
nates. The Hessian energy of this heavily oscil-
lating function is 0  since every local ﬁt is lin-
ear (an example shown in the right image; green
curve). However  ﬁxing the zeroth-order term
yields a high Hessian energy as desired (local ﬁt
is shown as the red curve in the right image).

Figure 3: Sinusoid on the spiral: Left two im-
ages show the result of semi-supervised regres-
sion using the Hessian estimate of [3] and the
corresponding smallest eigenvectors of the Hes-
sian “matrix”. One observes heavy oscillations 
due to the bad estimation of the Hessian. The
right two images show the result of our method.
Note  that in particular the third eigenvector cor-
responding to a non-zero eigenvalue of B is
much better behaved.

for sure irregular solutions  but since it did not lead to signiﬁcantly better experimental results and
requires an additional parameter to tune we do not recommend to use it.
Our estimation procedure of the Hessian has similar motivation as the one done in Hessian eigen-
maps [3]. However  in their approach they do not ﬁx the zeroth-order term. This seems to be suitable
for Hessian eigenmaps as they do not use the full Hessian  but only its m + 1-dimensional null space
(where m is the intrinsic dimension of the manifold). Apparently  this resolves the issues discussed
above so that the null space can still be well estimated also with their procedure. However  us-
ing their estimator for semi-supervised regression leads to useless results  see Fig. 3. Moreover 
we would like to note that using our estimator not only the eigenvectors of the null space but also
eigenvectors corresponding to higher eigenvalues can be well estimated  see Fig. 3.

4 Experiments

We test our semi-supervised regression method using Hessian regularization on one synthetic and
two real-world data sets. We compare with the results obtained using Laplacian-based regulariza-
tion and kernel ridge regression (KRR) trained only with the labeled examples. The free parameters
for our method are the number of neighbors k for k-NN  the dimensionality of the PCA subspace 
and the regularization parameter λ while the parameters for the Laplacian regularization-based re-
gression are: k for k-NN  the regularization parameter and the width of the Gaussian kernel. For
KRR we used also the Gaussian kernel with the width as free parameter. These parameters were
chosen for each method using 5-fold cross-validation on the labeled examples. For the digit and
ﬁgure datasets  the experiments were repeated with 5 different assignments of labeled examples.

Digit Dataset.
In the ﬁrst set of experiments  we generated 10000 random samples of artiﬁcially
generated images (size 28× 28) of the digit 1. There are four variations in the data: translation (two
variations)  rotation and line thickness. For this dataset we are doing semi-supervised dimensionality
reduction since the task is to estimate the natural parameters which were used to generate the digits.
This is done based on 50 and 100 labeled images. Each of the variation corresponds then to a separate
regression problem which we ﬁnally stick together to get an embedding into four dimensions. Note 
that this dataset is quite challenging since translation of the digit leads to huge Euclidean distances
between digits although they look visually very similar. Fig. 2 and Table 1 summarize the results.
As observed in the ﬁrst row of Fig. 2  KRR (K) and Hessian (H) regularization recover well the
two parameters of line width and rotation (all other embeddings can be found in the supplementary
material). As discussed previously  the Laplacian (L) regularization tends to shrink the estimated
parameters towards a constant as it penalizes the “geodesic” functions. This results in the poor
estimation of parameters  especially the line-thickness parameter.2 Although KRR estimates well
the thickness parameter  it fails for the rotation parameter (cf. the second row of Fig. 2 where we

2In this ﬁgure  each parameter is normalized to lie in the unit interval while the regression was performed

in the original scale. The point (0.5  0.5) corresponds roughly to the origin in the original parameters.

6

−505−4−2024−10010203040Regression results using full polynomial−0.2−0.100.10.2−10−505101520  fFull polynomialFixed zeroth−order−4−2024−4−2024−10−5051001020−0.1−0.0500.050.1  1st eigenvector2nd eigenvector3rd eigenvector−4−2024−4−2024−10−5051001020−0.1−0.0500.050.1  1st eigenvector2nd eigenvector3rd eigenvectorFigure 2: Results on the digit 1 dataset. First row: the 2D-embedding of the digits obtained by
regression for the rotation and thickness parameter with 100 labels. Second row: 21 digit images
sampled at regular intervals in the estimated parameter spaces: two reference points (inverted im-
ages) are sampled in the ground truth parameter space and then in the corresponding estimated
embedding. Then  19 points are sampled in the estimated parameter spaces based on linear in-
ter/extrapolation of the parameters. The shown image samples are the ones which have parameters
closest to the interpolated ones. In each parameter space the interpolated points  the corresponding
closest data points and the reference points are marked with red dots  blue and cyan circles.

Table 1: Results on digits: mean squared error (standard deviation) (both in units 10−3).

50 labeled points

v-trans.

rotation

h-trans.
thickness
0.78(0.13) 0.85(0.14) 45.49(7.20) 0.02(0.01)
2.41(0.26) 3.91(0.59) 64.56(3.90) 0.39(0.02)
0.34(0.03) 0.88(0.07) 4.03(1.15) 0.15(0.02)

100 labeled points

v-trans.

rotation

h-trans.
thickness
0.39(0.10) 0.48(0.08) 26.02(2.98) 0.01(0.00)
1.17(0.13) 2.20(0.22) 30.73(6.05) 0.34(0.01)
0.16(0.03) 0.39(0.07) 1.48(0.26) 0.06(0.01)

K
L
H

show the images corresponding to equidistant inter/extrapolation in the estimated parameter space
between two ﬁxed digits (inverted image)). The Hessian regularization provided a moderate level of
accuracy in recovering the thickness parameter and performed best on the remaining ones.

Figure Dataset. The second dataset consists of 2500 views of a toy ﬁgure (see Fig. 3) sampled
based on regular intervals in zenith and azimuth angles on the upper hemisphere around the centered
object [10]. Fig. 3 shows the results of regression for three parameters - the zenith angle  and
the azimuth angle is transformed into Euclidean x y coordinates.3 Both Laplacian and Hessian
regularizers provided signiﬁcantly better estimation of the parameters in comparison to KRR  which
demonstrates the effectiveness of semi-supervised regression. However  the Laplacian shows again
contracting behavior which is observed in the top view of hemisphere. Note that for our method this
does not occur and the spacing of the points in the parameter space is much more regular  which
again stresses the effectiveness of our proposed regularizer.

Image Colorization.
Image colorization refers to the task of estimating the color components of a
given gray level image. Often  this problem is approached based on the color information of a subset
of pixels in the image  which is speciﬁed by a user (cf. [8] for more details). This is essentially a
semi-supervised regression problem where the user-speciﬁed color components correspond to the
labels. To facilitate quantitative evaluation  we adopted 20 color images  sampled a subset of pixels
in each image as labels  and used the corresponding gray levels as inputs. The number of labeled
points were 30 and 100 for each images  which we regard as a moderate level of user intervention. As
error measure  we use the mean square distance between the original image and the corresponding

3Although the underlying manifold is two dimensional  the parametrization cannot be directly found based
on regression as the azimuth angle is periodic. This results in contradicting assignments of ground truth labels.

7

Figure 3: Results of regression on the ﬁgure dataset. First row: embedding in the three dimensional
spaces with 50 labels. Second row: Left: some example images of the dataset  Right: error plots for
each regression variable for different number of labeled points.

Original image

KRR

L

H

Levin et al. [8]

Figure 4: Example of image colorization with 30 labels. KRR failed in reconstructing (the color of)
the red pepper at the lower-right corner  while the Laplacian regularizer produced overall  a greenish
image. Levin et al’s method well-recovered the lower central part however failed in reconstructing
the upper central pepper. Despite the slight diffusion of red color at the upper-left corner  overall 
the result of Hessian regularization looks best which is also conﬁrmed by the reconstruction error.

reconstruction in the RGB space. During the colorization  we go over to the YUV color model such
that the Y components  containing the gray level values  are used as the input  based on which the
U and V components are estimated. The estimated U-V components are then combined with the Y
component and converted into RGB format. For the regression  for each pixel  we use as features
the 3 × 3-size image patch centered at the pixel of interest plus the 2-dimensional coordinate value
of that pixel. The coordinate values are weighted by 10 such that the contribution of coordinate
values and gray levels is balanced. For comparison  we performed experiments with the method
of Levin et al. [8] as one of the state-of-the-art methods.4 Figure 4 shows an example and Table 2
summarizes the results. The Hessian regularizer clearly outperformed the KRR and the Laplacian-
based regression and produced slightly better results than those of Levin et al. [8]. We expect that
the performance can be further improved by exploiting a priori knowledge on structure of natural
images (e.g.  by exploiting the segmentation information (cf. [9  6]) in the NN structure).

4Code is available at: http://www.cs.huji.ac.il/˜yweiss/Colorization/.

Table 2: Results on colorization: mean squared error (standard deviation) (both in units 10−3).

# labels

30
100

K

1.18(1.10)
0.66(0.65)

L

0.83(0.64)
0.50(0.33)

H

0.64(0.50)
0.32(0.25)

Levin et al. [8]

0.74(0.61)
0.37(0.26)

8

−101−10100.51ground truth−101−10100.51KRR−101−10100.51Laplacian regularization−101−10100.51Hessian regularization10255010000.050.10.150.2x coord.  Laplacian regularizationKRRHessian regularization10255010000.050.10.150.2y coord.  Laplacian regularizationKRRHessian regularization10255010000.050.10.150.2number of labeled pointserrorzenith coord.  Laplacian regularizationKRRHessian regularizationReferences
[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-

sentation. Neural Computation  15(6):1373–1396  2003. 2

[2] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning  56:209–

239  2004. 1  3  5

[3] D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proc. of the National Academy of Sciences  100(10):5591–5596  2003. 2  3 
6

[4] J. Eells and L. Lemaire. Selected topics in harmonic maps. AMS  Providence  RI  1983. 3
[5] M. Hein. Uniform convergence of adaptive graph-based regularization.

In G. Lugosi and
H. Simon  editors  Proc. of the 19th Conf. on Learning Theory (COLT)  pages 50–64  Berlin 
2006. Springer. 5

[6] R. Irony  D. Cohen-Or  and D. Lischinski. Colorization by example. In Proc. Eurographics

Symposium on Rendering  pages 201–210  2005. 8

[7] J. M. Lee. Riemannian Manifolds - An introduction to curvature. Springer  New York  1997. 2
[8] A. Levin  D. Lischinski  and Y. Weiss. Colorization using optimization. In Proc. SIGGRAPH 

pages 689–694  2004. 7  8

[9] Q. Luan  F. Wen  D. Cohen-Or  L. Liang  Y.-Q. Xu  and H.-Y. Shum. Natural image coloriza-

tion. In Proc. Eurographics Symposium on Rendering  pages 309–320  2007. 8

[10] G. Peters. Efﬁcient pose estimation using view-based object representations. Machine Vision

and Applications  16(1):59–63  2004. 7

[11] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Ad-

vances in Neural Information Processing Systems  pages 1561–1568  2009. 2  4

[12] J. J. Verbeek and N. Vlassis. Gaussian ﬁelds for semi-supervised regression and correspon-

dence learning. Pattern Recognition  39:1864–1875  2006. 1  3

[13] X. Yang  H. Fu  H. Zha  and J. Barlow. Semi-supervised nonlinear dimensionality reduction.
In Proc. of the 23rd international conference on Machine learning  pages 1065–1072  New
York  NY  USA  2006. ACM. 1

9

,Amin Karbasi
Amir Hesam Salavati
Amin Shokrollahi
Lav Varshney
Zhoutong Zhang
Qiujia Li
Zhengjia Huang
Jiajun Wu
Josh Tenenbaum
Bill Freeman