2012,Co-Regularized Hashing for Multimodal Data,Hashing-based methods provide a very promising approach to large-scale similarity search.  To obtain compact hash codes  a recent trend seeks to learn the hash functions from data automatically.  In this paper  we study hash function learning in the context of multimodal data.  We propose a novel multimodal hash function learning method  called Co-Regularized Hashing (CRH)  based on a boosted co-regularization framework.  The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs  while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.  We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.,Co-Regularized Hashing for Multimodal Data

Yi Zhen and Dit-Yan Yeung

Department of Computer Science and Engineering
Hong Kong University of Science and Technology

Clear Water Bay  Kowloon  Hong Kong
{yzhen dyyeung}@cse.ust.hk

Abstract

Hashing-based methods provide a very promising approach to large-scale similar-
ity search. To obtain compact hash codes  a recent trend seeks to learn the hash
functions from data automatically. In this paper  we study hash function learning
in the context of multimodal data. We propose a novel multimodal hash function
learning method  called Co-Regularized Hashing (CRH)  based on a boosted co-
regularization framework. The hash functions for each bit of the hash codes are
learned by solving DC (difference of convex functions) programs  while the learn-
ing for multiple bits proceeds via a boosting procedure so that the bias introduced
by the hash functions can be sequentially minimized. We empirically compare
CRH with two state-of-the-art multimodal hash function learning methods on two
publicly available data sets.

1

Introduction

Nearest neighbor search  a.k.a. similarity search  plays a fundamental role in many important ap-
plications  including document retrieval  object recognition  and near-duplicate detection. Among
the methods proposed thus far for nearest neighbor search [1]  hashing-based methods [2  3] have
attracted considerable interest in recent years. The major advantage of hashing-based methods is
that they index data using binary hash codes which enjoy not only low storage requirements but
also high computational efﬁciency. To preserve similarity in the data  a family of algorithms called
locality sensitive hashing (LSH) [4  5] has been developed over the past decade. The basic idea of
LSH is to hash the data into bins so that the collision probability reﬂects data similarity. LSH is very
appealing in that it has theoretical guarantee and is also simple to implement. However  in practice
LSH algorithms often generate long hash codes in order to achieve acceptable performance because
the theoretical guarantee only holds asymptotically. This shortcoming can be attributed largely to
their data-independent nature which cannot capture the data characteristics very accurately in the
hash codes. Besides  in many applications  neighbors cannot be deﬁned easily using some generic
distance or similarity measures. As such  a new research trend has emerged over the past few years
by learning the hash functions from data automatically. In the sequel  we refer to this new trend as
hash function learning (HFL).
Boosting  as one of the most popular machine learning approaches  was ﬁrst applied to learning hash
functions for pose estimation [6]. Later  impressive performance for HFL using restricted Boltz-
mann machines was reported [7]. These two early HFL methods have been successfully applied to
content-based image retrieval in which large-scale data sets are commonly encountered [8]. A num-
ber of algorithms have been proposed since then. Spectral hashing (SH) [9] treats HFL as a special
case of manifold learning and uses an efﬁcient algorithm based on eigenfunctions. One shortcom-
ing of spectral hashing is in its assumption  which requires that the data be uniformly distributed.
To overcome this limitation  several methods have been proposed  including binary reconstructive
embeddings [10]  shift-invariant kernel hashing [11]  distribution matching [12]  optmized kernel
hashing [13]  and minimal loss hashing [14]. Recently  some semi-supervised hashing models have

1

been developed to combine both feature similarity and semantic similarity for HFL [15  16  17  18].
To further improve the scalability of these methods  Liu et al. [19] presented a fast algorithm based
on anchor graphs.
Existing HFL algorithms have enjoyed wide success in challenging applications. Nevertheless  they
can only be applied to a single type of data  called unimodal data  which refer to data from a single
modality such as image  text  or audio. Nowadays  it is common to ﬁnd similarity search applications
that involve multimodal data. For example  given an image of a tourist attraction as query  one
would like to retrieve some textual documents that provide more detailed information about the place
of interest. Because data from different modalities reside in different feature spaces  performing
multimodal similarity search will be made much easier and faster if the multimodal data can be
mapped into a common Hamming space. However  it is challenging to do so because data from
different modalities generally have very different representations.
As far as we know  there exist only two multimodal HFL methods. Bronstein et al. [20] made the
ﬁrst attempt to learn linear hash functions using eigendecomposition and boosting  while Kumar
et al. [21] extended spectral hashing to the multiview setting and proposed a cross-view hashing
model. One major limitation of these two methods is that they both rely on eigendecomposition
operations which are computationally very demanding when the data dimensionality is high. More-
over  they consider applications for shape retrieval  image alignment  and people search which are
quite different from the multimodal retrieval applications of interest here.
In this paper  we propose a novel multimodal HFL method  called Co-Regularized Hashing (CRH) 
based on a boosted co-regularization framework. For each bit of the hash codes  CRH learns a group
of hash functions  one for each modality  by minimizing a novel loss function. Although the loss
function is non-convex  it is in a special form which can be expressed as a difference of convex
functions. As a consequence  the Concave-Convex Procedure (CCCP) [22] can be applied to solve
the optimization problem iteratively. We use a stochastic sub-gradient method  which converges
very fast  in each CCCP iteration to ﬁnd a local optimum. After learning the hash functions for one
bit  CRH proceeds to learn more bits via a boosting procedure such that the bias introduced by the
hash functions can be sequentially minimized.
In the next section  we present the CRH method in detail. Extensive empirical study using two data
sets is reported in Section 3. Finally  Section 4 concludes the paper.

2 Co-Regularized Hashing

We use boldface lowercase letters and calligraphic letters to denote vectors and sets  respectively.
For a vector x  xT denotes its transpose and (cid:107)x(cid:107) its (cid:96)2 norm.

2.1 Objective Function
Suppose that there are two sets of data points from two modalities 1 e.g.  {xi ∈ X}I
i=1 for a
set of I images from some feature space X and {yj ∈ Y}J
j=1 for a set of J textual docu-
ments from another feature space Y. We also have a set of N inter-modality point pairs Θ =
{(xa1  yb1 )  (xa2   yb2)  . . .   (xaN   ybN )}  where  for the nth pair  an and bn are indices of the points
in X and Y  respectively. We further assume that each pair has a label sn = 1 if xan and ybn are
similar and sn = 0 otherwise. The notion of inter-modality similarity varies from application to
application. For example  if an image includes a tiger and a textual document is a research paper on
tigers  they should be labeled as similar. On the other hand  it is highly unlikely to label the image
as similar to a textual document on basketball.
For each bit of the hash codes  we deﬁne two linear hash functions as follows:

f (x) = sgn(wT

x x) and g(y) = sgn(wT

y y) 

where sgn(·) denotes the sign function  and wx and wy are projection vectors which  ideally  should
map similar points to the same hash bin and dissimilar points to different bins. Our goal is to achieve
HFL by learning wx and wy from the multimodal data.

1For simplicity of our presentation  we focus on the bimodal case here and leave the discussion on extension

to more than two modalities to Section 2.4.

2

To achieve this goal  we propose to minimize the following objective function w.r.t. (with respect
to) wx and wy:

O =

1
I

ωn(cid:96)∗

n +

(cid:107)wx(cid:107)2 +

λx
2

(cid:107)wy(cid:107)2 

λy
2

(1)

j are intra-modality loss terms for modalities X and Y  respectively. In this work  we

i and (cid:96)y
where (cid:96)x
deﬁne them as:

i=1

j=1

1
J

(cid:96)x
i +

(cid:96)y
j + γ

J(cid:88)

I(cid:88)
N(cid:88)
i =(cid:2)1 − f (xi)(wT
x xi)(cid:3)
j =(cid:2)1 − g(yj)(wT
y yj)(cid:3)

(cid:96)x
(cid:96)y

n=1

+ =(cid:2)1 − |wT
x xi|(cid:3)
+ =(cid:2)1 − |wT
y yj|(cid:3)

+ 
+ 

associate with each point pair a weight ωn  with(cid:80)N

where [a]+ is equal to a if a ≥ 0 and 0 otherwise. We note that the intra-modality loss terms
are similar to the hinge loss in the (linear) support vector machine but have quite different mean-
ing. Conceptually  we want the projected values to be far away from 0 and hence expect the hash
functions learned to have good generalization ability [16]. For the inter-modality loss term (cid:96)∗
n  we
n=1 ωn = 1  to normalize the loss as well as
compute the bias of the hash functions. In this paper  we deﬁne (cid:96)∗
n + (1 − sn)τ (dn) 

(cid:96)∗
n = snd2

n as

x xan − wT

where dn = wT
y ybn and τ (d) is called the smoothly clipped inverted squared deviation
(SCISD) function. The loss function such deﬁned requires that the similar inter-modality points 
i.e.  sn = 1  have small distance after projection  and the dissimilar ones  i.e.  sn = 0  have large
distance. With these two kinds of loss terms  we expect that the learned hash functions can enjoy
the large-margin property while effectively preserving the inter-modality similarity.
The SCISD function was ﬁrst proposed in [23]. It can be deﬁned as follows:

 − 1

0

τ (d) =

2 d2 + aλ2
2
d2−2aλ|d|+a2λ2
2(a−1)

if |d| ≤ λ
if λ < |d| ≤ aλ
if aλ < |d| 

where a and λ are two user-speciﬁed parameters. The SCISD function penalizes projection vectors
that result in small distance between dissimilar points after projection. A more important property
is that it can be expressed as a difference of two convex functions. Speciﬁcally  we can express
τ (d) = τ1(d) − τ2(d) where

 0

ad2−2aλ|d|+aλ2
2(a−1)
2 d2 − aλ2

1

2

if |d| ≤ λ
if λ < |d| ≤ aλ
if aλ < |d|

and τ2(d) =

1
2

d2 − aλ2
2

.

τ1(d) =

2.2 Optimization

Though the objective function (1) is nonconvex w.r.t. wx and wy  we can optimize it w.r.t. wx and
wy in an alternating manner. Take wx for example  we remove the irrelevant terms and get the
following objective:

(cid:96)x
i +

(cid:107)wx(cid:107)2 + γ

λx
2

ωn(cid:96)∗
n 

(2)

N(cid:88)

n=1

where

1
I

(cid:96)x
i =

i=1

I(cid:88)
 0

1 − wT
1 + wT

x xi
x xi

if |wT
x xi| ≥ 1
if 0 ≤ wT
if −1 < wT

x xi < 1

x xi < 0.

It is easy to realize that the objective function (2) can be expressed as a difference of two convex
functions in different cases. As a consequence  we can use CCCP to solve the nonconvex opti-
mization problem iteratively with each iteration minimizing a convex upper bound of the original
objective function.

3

Brieﬂy speaking  given an objective function f0(x)− g0(x) where both f0 and g0 are convex  CCCP
works iteratively as follows. The variable x is ﬁrst randomly initialized to x(0). At the tth iteration 
CCCP minimizes the following convex upper bound of f0(x) − g0(x) at location x(t):

f0(x) −(cid:0)g0(x(t)) + ∂xg0(x(t))(x − x(t))(cid:1) 

where ∂xg0(x(t)) is the ﬁrst derivative of g0(x) at x(t). This optimization problem can be solved
using any convex optimization solver to obtain x(t+1). Given an initial value x(0)  the solution
sequence {x(t)} found by CCCP is guaranteed to reach a local minimum or a saddle point.
For our problem  the optimization problem at the tth iteration minimizes the following upper bound
of Equation (2) w.r.t. wx:

Ox =

λx(cid:107)wx(cid:107)2

2

+ γ

(cid:0)snd2

ωn

N(cid:88)

n=1

(cid:1) +

I(cid:88)

i=1

1
I

n + (1 − sn)ζ x

n

(cid:96)x
i  

(3)

n xT
an

x is the

n )− d(t)

x )  d(t)

n = (w(t)

(wx − w(t)

y ybn  and w(t)

x )T xan − wT

n = τ1(dn)− τ2(d(t)
where ζ x
value of wx at the tth iteration.
To ﬁnd a locally optimal solution to problem (3)  we can use any gradient-based method. In this
work  we develop a stochastic sub-gradient solver based on Pegasos [24]  which is known to be one
of the fastest solvers for margin-based classiﬁers. Speciﬁcally  we randomly select k points from
each modality and l point pairs to evaluate the sub-gradient at each iteration.
The key step of our method is to evaluate the sub-gradient of objective function (3) w.r.t. wx  which
can be computed as
∂Ox
∂wx
n = (1 − sn)

n + λxwx − 1
I

(cid:16) ∂τ1

ωnsndnxan + γ

N(cid:88)

N(cid:88)

I(cid:88)

where µx

ωnµx

(cid:17)

= 2γ

πx
i  

(4)

n=1

n=1

i=1

∂dn

 0

dn

∂τ1
∂dn

=

adn−2aλ sgn(dn)

(a−1)

n

− d(t)
xan 
if |dn| ≤ λ
if λ < |dn| ≤ aλ
if aλ < |dn|

and πx

i =

if |wT
if |wT

x xi| ≥ 1
x xi| < 1.

Similarly  the objective function for the optimization problem w.r.t. wy at the tth CCCP iteration is:

x xi

(cid:26) 0
sgn(cid:0)wT
(cid:1) +

1
J

(cid:1) xi
I(cid:88)

j=1

(cid:96)y
j  

λy(cid:107)wy(cid:107)2

Oy =

+ γ

ωn

n + (1 − sn)ζ y

n

N(cid:88)

n=1

(cid:0)snd2

2
n = τ1(dn) − τ2(d(t)

where ζ y
value of wy at the tth iteration and

n ) + d(t)

n yT
bn

(wy − w(t)

y )  d(t)

n = wT

x xan − (w(t)

y )T ybn  w(t)

y

(cid:96)y
j =

 0
N(cid:88)

1 − wT
1 + wT

y yj
y yj

y yj| ≥ 1

if |wT
if 0 ≤ wT
if −1 < wT

y yj < 1

y yj < 0.

N(cid:88)

n=1

ωnµy

n + λywy − 1
J

I(cid:88)

j=1

πy
j  

(5)

is the

(6)

The corresponding sub-gradient is given by

∂Oy
∂wy
n = (1 − sn)

where µy

= −2γ

(cid:16) ∂τ1

∂dn

n=1

− d(t)

n

ωnsndnybn − γ
(cid:17)
(cid:26) 0
sgn(cid:0)wT

ybn and

y yj

(cid:1) yj

πy

j =

if |wT
if |wT

y yj| ≥ 1
y yj| < 1.

4

2.3 Algorithm

So far we have only discussed how to learn the hash functions for one bit of the hash codes. To learn
the hash functions for multiple bits  one could repeat the same procedure and treat the learning for
each bit independently. However  as reported in previous studies [15  19]  it is very important to take
into consideration the relationships between different bits in HFL. In other words  to learn compact
hash codes  we should coordinate the learning of hash functions for different bits.
To this end  we take the standard AdaBoost [25] approach to learn multiple bits sequentially. In-
tuitively  this approach allows learning of the hash functions in later stages to be aware of the bias
introduced by their antecedents. The overall algorithm of CRH is summarized in Algorithm 1.

Algorithm 1 Co-Regularized Hashing

Input:
X  Y – multimodal data
Θ – inter-modality point pairs
K – code length
λx  λy  γ – regularization parameters
a  λ – parameters for SCISD function
Output:
x   k = 1  . . .   K – projection vectors for X
w(k)
y   k = 1  . . .   K – projection vectors for Y
w(k)
Procedure:
Initialize ω(1)
for k = 1 to K do

n = 1/N  ∀n ∈ {1  2  . . .   N}.

repeat

until convergence.
Compute error of current hash functions

k =

ω(k)
n I[sn(cid:54)=hn] 

n=1

where I[a] = 1 if a is true and I[a] = 0 oth-
erwise  and

(cid:88)N
(cid:26) 1

hn =

if f (xan ) = g(ybn )
if f (xan ) (cid:54)= g(ybn ).

0

Set βk = k/(1 − k).
Update the weight for each point pair:

ω(k+1)

n

= ω(k)

n β

1−I[sn(cid:54)=hn ]

k

.

Optimize Equation (3) to get w(k)
x ;
Optimize Equation (5) to get w(k)
y ;

end for

In the following  we brieﬂy analyze the time complexity of Algorithm 1 for one bit. The ﬁrst com-
putationally expensive part of the algorithm is to evaluate the sub-gradients. The time complexity is
O((k + l)d)  where d is the data dimensionality  and k and l are the numbers of random points and
random pairs  respectively  for the stochastic sub-gradient solver. In our experiments  we set k = 1
and l = 500. We notice that further increasing the two numbers brings no signiﬁcant performance
improvement. We leave the theoretical study of the impact of k and l to our future work. Another
major computational cost comes from updating the weights of the inter-modality point pairs. The
time complexity is O(dN )  where N is the number of inter-modality point pairs.
To summarize  our algorithm scales linearly with the number of inter-modality point pairs and the
data dimensionality. In practice  the number of inter-modality point pairs is usually small  making
our algorithm very efﬁcient.

2.4 Extensions

We brieﬂy discuss two possible extensions of CRH in this subsection. First  we note that it is easy
to extend CRH to learn nonlinear hash functions via the kernel trick [26]. Speciﬁcally  according to
the generalized representer theorem [27]  we can represent the projection vectors wx and wy as

wx =

αiφx(xi) and wy =

i=1

βjφy(yj) 

j=1

where φx(·) and φy(·) are kernel-induced feature maps for modalities X and Y  respectively. Then
the objective function (1) can be expressed in kernel form and kernel-based hash functions can be
learned by minimizing a new but very similar objective function.
Another possible extension is to make CRH support more than two modalities. Taking a new modal-
ity Z for example  we need to incorporate into Equation (1) the following terms: loss and regular-
ization terms for Z  and all pairwise loss terms involving Z and other modalities  e.g.  X and Y.

(cid:88)J

(cid:88)I

5

For both extensions  it is straightforward to adapt the algorithm presented above to solve the new
optimization problems.

2.5 Discussions

CRH is closely related to a recent multimodal metric learning method called Multiview Neighbor-
hood Preserving Projections (Multi-NPP) [23]  because CRH uses a loss function for inter-modality
point pairs which is similar to Multi-NPP. However  CRH is a general framework and other loss
functions for inter-modality point pairs can also be adopted. The two methods have at least three
signiﬁcant differences. First  our focus is on HFL while Multi-NPP is on metric learning through
embedding. Second  in addition to the inter-modality loss term  the objective function in CRH in-
cludes two intra-modality loss terms for large margin HFL while Multi-NPP only has a loss term for
the inter-modality point pairs. Third  CRH uses boosting to sequentially learn the hash functions but
Multi-NPP does not take this aspect into consideration.
As discussed brieﬂy in [23]  one may ﬁrst use Multi-NPP to map multimodal data into a common
real space and then apply any unimodal HFL method for multimodal hashing. However  this naive
two-stage approach has some limitations. First  both stages can introduce information loss which
impairs the quality of the hash functions learned. Second  a two-stage approach generally needs
more computational resources. These two limitations can be overcome by using a one-stage method
such as CRH.

3 Experiments

3.1 Experimental Settings

In our experiments  we compare CRH with two state-of-the-art multimodal hashing methods 
namely  Cross-Modal Similarity Sensitive Hashing (CMSSH) [20]2 and Cross-View Hashing
(CVH) [21] 3 for two cross-modal retrieval tasks: (1) image query vs. text database; (2) text query
vs. image database. The goal of each retrieval task is to ﬁnd from the text (image) database the
nearest neighbors for the image (text) query.
We use two benchmark data sets which are  to the best of our knowledge  the largest fully paired
and labeled multimodal data sets. We further divide each data set into a database set and a query
set. To train the models  we randomly select a group of documents from the database set to form the
training set. Moreover  we randomly select 0.1% of the point pairs from the training set. For fair
comparison  all models are trained on the same training set and the experiments are repeated with 5
independent training sets.
The mean average precision (mAP) is used as the performance measure. To compute the
mAP  we ﬁrst evaluate the average precision (AP) of a set of R retrieved documents by AP =
r=1 P (r) δ(r)  where L is the number of true neighbors in the retrieved set  P (r) denotes the
1
L
precision of the top r retrieved documents  and δ(r) = 1 if the rth retrieved document is a true
neighbor and δ(r) = 0 otherwise. The mAP is then computed by averaging the AP values over all
the queries in the query set. The larger the mAP  the better the performance. In the experiments  we
set R = 50. Besides  we also report the precision and recall within a ﬁxed Hamming radius.
We use cross-validation to choose the parameters for CRH and ﬁnd that the model performance is
only mildly sensitive to the parameters. As a result  in all experiments  we set λx = 0.01  λy =
0.01  γ = 1000  a = 3.7  and λ = 1/a. Besides  unless speciﬁed otherwise  we ﬁx the training set
size to 2 000 and the code length K to 24.

(cid:80)R

3.2 Results on Wiki

The Wiki data set  generated from Wikipedia featured articles  consists of 2 866 image-text pairs.4
In each pair  the text is an article describing some events or people and the image is closely related to

2We used the implementation generously provided by the authors.
3We implemented the method ourselves because the code is not publicly available.
4http://www.svcl.ucsd.edu/projects/crossmodal/

6

the content of the article. The images are represented by 128-dimensional SIFT [28] feature vectors 
while the text articles are represented by the probability distributions over 10 topics learned by a
latent Dirichlet allocation (LDA) model [29]. Each pair is labeled with one of 10 semantic classes.
We simply use these class labels to identify the neighbors. Moreover  we use 80% of the data as the
database set and the remaining 20% to form the query set.
The mAP values of the three methods are reported in Table 1. We can see that CRH outperforms
CVH and CMSSH under all settings and CVH performs slightly better than CMSSH. We note that
CMSSH ignores the intra-modality relational information and CVH simply treats each bit indepen-
dently. Hence the performance difference is expected.

Task

vs.

vs.

Image Query

Text Database
Text Query

Image Database

Table 1: mAP comparison on Wiki

K = 24

0.2537 ± 0.0206
0.2043 ± 0.0150
0.1965 ± 0.0123
0.2896 ± 0.0214
0.2714 ± 0.0164
0.2179 ± 0.0161

Code Length

K = 48

0.2399 ± 0.0185
0.1788 ± 0.0149
0.1780 ± 0.0080
0.2882 ± 0.0261
0.2304 ± 0.0104
0.2094 ± 0.0072

K = 64

0.2392 ± 0.0131
0.1732 ± 0.0072
0.1624 ± 0.0073
0.2989 ± 0.0293
0.2156 ± 0.0202
0.2040 ± 0.0135

Method
CRH
CVH

CMSSH

CRH
CVH

CMSSH

We further compare the three methods on several aspects in Figure 1. We ﬁrst vary the size of the
training set in subﬁgures 1(a) and 1(d). Although CVH performs the best when the training set
is small  its performance is gradually surpassed by CRH as the size increases. We then plot the
precision-recall curves and recall curves for all three methods in the remaining subﬁgures. It is clear
that CRH outperforms its two counterparts by a large margin.

(a) Varying training set size

(b) Precision-recall curve

(c) Recall curve

(d) Varying training set size

(e) Precision-recall curve

(f) Recall curve

Figure 1: Results on Wiki

3.3 Results on Flickr

The Flickr data set consists of 186 577 image-tag pairs pruned from the NUS data set5 [30] by
keeping the pairs that belong to one of the 10 largest classes. The images are represented by 500-
dimensional SIFT vectors. To obtain more compact representations of the tags  we perform PCA
on the original tag occurrence features and obtain 1000-dimensional feature vectors. Each pair is
annotated by at least one of 10 semantic labels  and two points are deﬁned as neighbors if they share
at least one label. We use 99% of the data as the database set and the remaining 1% to form the
query set.

5http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

7

050010001500200000.050.10.150.20.250.30.350.4Size of Training SetPrecision within Hamming Radius 2Image Query vs. Text Database CRHCVHCMSSH00.20.40.60.8100.10.20.30.40.5RecallPrecisionImage Query vs. Text Database CRHCVHCMSSH051015x 10500.20.40.60.81No. of Retrieved PointsRecallImage Query vs. Text Database CRHCVHCMSSH050010001500200000.050.10.150.20.250.30.35Size of Training SetPrecision within Hamming Radius 2Text Query vs. Image Database CRHCVHCMSSH00.20.40.60.8100.10.20.30.40.5RecallPrecisionText Query vs. Image Database CRHCVHCMSSH051015x 10500.20.40.60.81No. of Retrieved PointsRecallText Query vs. Image Database CRHCVHCMSSHThe mAP values of the three methods are reported in Table 2. In the task of image query vs. text
database  CRH performs comparably to CMSSH  which is better than CVH. However  in the other
task  CRH achieves the best performance.

Table 2: mAP comparison on Flickr

Task

vs.

vs.

Image Query

Text Database
Text Query

Image Database

Method
CRH
CVH

CMSSH

CRH
CVH

CMSSH

K = 24

0.5259 ± 0.0094
0.4717 ± 0.0035
0.5287 ± 0.0123
0.5364 ± 0.0021
0.4598 ± 0.0020
0.5029 ± 0.0321

Code Length

K = 48

0.4990 ± 0.0075
0.4515 ± 0.0041
0.5098 ± 0.0141
0.5185 ± 0.0050
0.4519 ± 0.0029
0.4815 ± 0.0101

K = 64

0.4929 ± 0.0064
0.4471 ± 0.0023
0.4911 ± 0.0220
0.5064 ± 0.0055
0.4477 ± 0.0058
0.4660 ± 0.0298

Similar to the previous subsection  we have conducted a group of experiments to compare the three
methods on several aspects and report the results in Figure 2. The results for varying the size of
the training set are plotted in subﬁgures 2(a) and 2(d). As more training data are used  CRH always
performs better but the performance of CVH and CMSSH has high variance. The precision-recall
curves and recall curves are shown in the remaining subﬁgures. Similar to the results on Wiki  CRH
performs the best. However  the performance gap is smaller here.

(a) Varying training set size

(b) Precision-recall curve

(c) Recall curve

(d) Varying training set size

(e) Precision-recall curve

(f) Recall curve

Figure 2: Results on Flickr

4 Conclusions

In this paper  we have presented a novel method for multimodal hash function learning based on a
boosted co-regularization framework. Because the objective function of the optimization problem is
in the form of a difference of convex functions  we can devise an efﬁcient learning algorithm based
on CCCP and a stochastic sub-gradient method. Comparative studies based on two benchmark data
sets show that CRH outperforms two state-of-the-art multimodal hashing methods.
To take this work further  we would like to conduct theoretical analysis of CRH and apply it to
some other tasks such as multimodal medical image alignment. Another possible research issue is
to develop more efﬁcient optimization algorithms to further improve the scalability of CRH.

Acknowledgement

This research has been supported by General Research Fund 621310 from the Research Grants
Council of Hong Kong.

8

050010001500200000.10.20.30.40.5Size of Training SetPrecision within Hamming Radius 2Image Query vs. Text Database CRHCVHCMSSH00.20.40.60.810.250.30.350.40.450.50.550.6RecallPrecisionImage Query vs. Text Database CRHCVHCMSSH01234x 10800.20.40.60.81No. of Retrieved PointsRecallImage Query vs. Text Database CRHCVHCMSSH050010001500200000.10.20.30.40.50.60.7Size of Training SetPrecision within Hamming Radius 2Text Query vs. Image Database CRHCVHCMSSH00.20.40.60.810.250.30.350.40.450.50.550.6RecallPrecisionText Query vs. Image Database CRHCVHCMSSH01234x 10800.20.40.60.81No. of Retrieved PointsRecallText Query vs. Image Database CRHCVHCMSSHReferences
[1] Gregory Shakhnarovich  Trevor Darrell  and Piotr Indyk  editors. Nearest-Neighbor Methods in Learning

and Vision: Theory and Practice. MIT Press  March 2006.

[2] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimen-

sionality. In STOC  1998.

[3] Moses Charikar. Similarity estimation techniques from rounding algorithms. In STOC  2002.
[4] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in

high dimensions. Communications of the ACM  51(1):117–122  2008.

[5] Brian Kulis and Kristen Grauman. Kernelized locality-sensitive hashing for scalable image search. In

ICCV  2009.

[6] Gregory Shakhnarovich  Paul Viola  and Trevor Darrell. Fast pose estimation with parameter-sensitive

hashing. In ICCV  2003.

[7] Ruslan Salakhutdinov and Geoffrey E. Hinton. Semantic hashing. In SIGIR Workshop on Information

Retrieval and Applications of Graphical Models  2007.

[8] Antonio Torralba  Rob Fergus  and Yair Weiss. Small codes and large image databases for recognition.

In CVPR  2008.

[9] Yair Weiss  Antonio Torralba  and Rob Fergus. Spectral hashing. In NIPS 21  2008.
[10] Brian Kulis and Trevor Darrell. Learning to hash with binary reconstructive embeddings. In NIPS 22 

2009.

[11] Maxim Raginsky and Svetlana Lazebnik. Locality-sensitive binary codes from shift-invariant kernels. In

NIPS 22  2009.

[12] Ruei-Sung Lin  David A. Ross  and Jay Yagnik. SPEC hashing: Similarity preserving algorithm for

entropy-based coding. In CVPR  2010.

[13] Junfeng He  Wei Liu  and Shih-Fu Chang. Scalable similarity search with optimized kernel hashing. In

KDD  2010.

[14] Mohammad Norouzi and David J. Fleet. Minimal loss hashing for compact binary codes. In ICML  2011.
[15] Jun Wang  Sanjiv Kumar  and Shih-Fu Chang. Semi-supervised hashing for scalable image retrieval. In

CVPR  2010.

[16] Yadong Mu  Jialie Shen  and Shuicheng Yan. Weakly-supervised hashing in kernel space. In CVPR  2010.
[17] Dan Zhang  Fei Wang  and Luo Si. Composite hashing with multiple information sources. In SIGIR 

2011.

[18] Jingkuan Song  Yi Yang  Zi Huang  Heng Tao Shen  and Richang Hong. Multiple feature hashing for

real-time large scale near-duplicate video retrieval. In ACM MM  2011.

[19] Wei Liu  Jun Wang  Sanjiv Kumar  and Shih-Fu Chang. Hashing with graphs. In ICML  2011.
[20] Michael M. Bronstein  Alexander M. Bronstein  Fabrice Michel  and Nikos Paragios. Data fusion through

cross-modality metric learning using similarity-sensitive hashing. In CVPR  2010.

[21] Shaishav Kumar and Raghavendra Udupa. Learning hash functions for cross-view similarity search. In

IJCAI  2011.

[22] A. L. Yuille and Anand Rangarajan. The concave-convex procedure (CCCP). In NIPS 14  2001.
[23] Novi Quadrianto and Christoph H. Lampert. Learning multi-view neighborhood preserving projections.

In ICML  2011.

[24] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver

for SVM. In ICML  2007.

[25] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[26] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University

Press  2004.

[27] Bernhard Sch¨olkopf  Ralf Herbrich  and Alex J. Smola. A generalized representer theorem. In COLT 

2001.

[28] David G. Lowe. Distinctive image features from scale-invariant keypoints.

Computer Vision  60(2):91–110  2004.

International Journal of

[29] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. Journal of Machine

Learning Research  3:993–1022  2003.

[30] Tat-Seng Chua  Jinhui Tang  Richang Hong  Haojie Li  Zhiping Luo  and Yan-Tao Zheng. NUS-WIDE:

A real-world web image database from National University of Singapore. In CIVR  2009.

9

,Kumar Avinava Dubey
Qirong Ho
Sinead Williamson
Eric Xing