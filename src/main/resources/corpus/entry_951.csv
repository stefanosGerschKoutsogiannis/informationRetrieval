2018,Clebsch–Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,Recent work by Cohen et al. has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace  but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity  thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.,Clebsch–Gordan Nets: a Fully Fourier Space

Spherical Convolutional Neural Network

Risi Kondor1∗ Zhen Lin1∗ Shubhendu Trivedi2∗

1The University of Chicago
2Toyota Technological Institute
{risi  zlin7}@uchicago.edu  shubhendu@ttic.edu

Abstract

Recent work by Cohen et al. [1] has achieved state-of-the-art results for learning
spherical images in a rotation invariant way by using ideas from group represen-
tation theory and noncommutative harmonic analysis. In this paper we propose
a generalization of this work that generally exhibits improved performace  but
from an implementation point of view is actually simpler. An unusual feature
of the proposed architecture is that it uses the Clebsch–Gordan transform as its
only source of nonlinearity  thus avoiding repeated forward and backward Fourier
transforms. The underlying ideas of the paper generalize to constructing neural
networks that are invariant to the action of other compact groups.

1

Introduction

Despite the many recent breakthroughs in deep learning  we still do not have a satisfactory understand-
ing of how deep neural networks are able to achieve such spectacular perfomance on a wide range of
learning problems. One thing that is clear  however  is that certain architectures pick up on natural
invariances in data  and this is a key component to their success. The classic example is of course
Convolutional Neural Networks (CNNs) for image classiﬁcation [2]. Recall that  fundamentally  each
layer of a CNN realizes two simple operations: a linear one consisting of convolving the previous
layer’s activations with a (typically small) learnable ﬁlter  and a nonlinear but pointwise one  such
as a ReLU operator2. This architecture is sufﬁcient to guarantee translation equivariance  meaning
that if the input image is translated by some vector t  then the activation pattern in each higher layer
of the network will translate by the same amount. Equivariance is crucial to image recognition for
two closely related reasons: (a) It guarantees that exactly the same ﬁlters are applied to each part
the input image regardless of position. (b) Assuming that ﬁnally  at the very top of the network  we
add some layer that is translation invariant  the entire network will be invariant  ensuring that it can
detect any given object equally well regardless of its location.
Recently  a number of papers have appeared that examine equivariance from the theoretical point
of view  motivated by the understanding that the natural way to generalize convolutional networks
to other types of data will likely lead through generalizing the notion of equivariance itself to other
transformation groups [3  4  5  6  7]. Letting f s denote the activations of the neurons in layer s
of a hypothetical generalized convolution-like neural network  mathematically  equivariance to a
group G means that if the inputs to the network are transformed by some transformation g ∈ G 
g }g∈G. s(Note that in
then f s transforms to T s
some contexts this is called “covariance”  the difference between the two words being only one of
emphasis.)

g (f s) for some ﬁxed set of linear transformations {T s

∗Authors are arranged alphabetically
2Real CNNs typically of course have multiple channels  and correspondingly multiple ﬁlters per layer  but

this does not fundamentally change the network’s invariance properties.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

A recent major success of this approach are Spherical CNNs [1][8]  which are an SO(3)–equivariant
neural network architecture for learning images painted on the sphere3. Learning images on the
sphere in a rotation invariant way has applications in a wide range of domains from 360 degree video
through drone navigation to molecular chemistry [9  10  11  12  13  14  15]. The key idea in Spherical
CNNs is to generalize convolutions using the machinery of noncommutative harmonic analysis:
employing a type of generalized SO(3) Fourier transform [16  17]  Spherical CNNs transform the
image to a sequence of matrices  and compute the spherical analog of convolution in Fourier space.
This beautiful construction guarantees equivariance  and the resulting network attains state of the art
results on several benchmark datasets.
One potential drawback of Spherical CNNs of the form proposed in [1]  however  is that the nonlinear
transform in each layer still needs to be computed in “real space”. Consequently  each layer of the
network involves a forward and a backward SO(3) Fourier transform  which is relatively costly  and
is a source of numerical errors  especially since the sphere and the rotation group do not admit any
regular discretization similar to the square grid for Euclidean space.
Spherical CNNs are not the only context in which the idea of Fourier space neural networks has
recently appeared [18  19  5  7]. From a mathematical point of view  the relevance of Fourier theoretic
ideas in all these cases is a direct consequence of equivariance  speciﬁcally  of the fact that the
{T s
g }g∈G operators form a representation of the underlying group  in the algebraic sense of the
word [20]. In particular  it has been shown that whenever there is a compact group G acting on the
inputs of a neural network  there is a natural notion of Fourier transformation with respect to G 
yielding a sequence of Fourier matrices {F s
(cid:96) }(cid:96) at each layer  and the linear operation at layer s will
be equivariant to G if and only if it is equivalent to multiplying each of these matrices from the right
(cid:96) [7]. Any other sort of operation will break equivariance. The
by some (learnable) ﬁlter matrix H s
spherical convolutions employed in [1] are a special case of this general setup for SO(3)  and the
ordinary convolutions employed in classical CNNs are a special case for the integer translation group
Z2. In all of these cases  however  the issue remains that the nonlinearities need to be computed in
“real space”  necessitating repeated forward and backward Fourier transforms.
In the present paper we propose a spherical CNN that differs from [1] in two fundamental ways:
1. While retaining the connection to noncommutative Fourier analysis  we relax the requirement
that the activation of each layer of the network needs to be a (vector valued) function on SO(3) 
requiring only that it be expressible as a collection of some number of SO(3)–covariant vectors
(which we call fragments) corresponding to different irreducible representations of the group. In
this sense  our architecture is strictly more general than [1].

2. Rather than a pointwise nonlinearity in real space  our network takes the tensor (Kronecker)
product of the activations in each layer followed by decomposing the result into irreducible
fragments using the so-called Clebsch–Gordan decomposition. This way  we get a “fully Fourier
space” neural network that avoids repeated forward and backward Fourier transforms.

The resulting architecture is not only more ﬂexible and easier to implement than [1]  but our experi-
ments show that it can also perform better on some standard datasets.
The Clebsch–Gordan transform has recently appeared in two separate preprints discussing neural
networks for learning physical systems [21  22]. However  to the best of our knowledge  it has never
been proposed as a general purpose nonlinearity for covariant neural networks. In fact  any compact
group has a Clebsch–Gordan decomposition (although  due to its connection to angular momentum
in physics  the SO(3) case is by far the best known)  so  in principle  the methods of the present paper
could be applied much broadly  in any situation where one desires to build a neural network that is
equivariant to some class of transformations captured by a compact group.

2 Convolutions on the sphere

The simplest example of a covariant neural network is a classical S + 1 layer CNN for image
recognition. In each layer of a CNN the neurons are arranged in a rectangular grid  so (assuming
for simplicity that the network has just one channel) the activation of layer s can be regarded as
a function f s : Z2 → R  with f 0 being the input image. The neurons compute f s by taking the

3SO(3) denotes the group of three dimensional rotations  i.e.  the group of 3× 3 orthogonal matrices.

2

cross-correlation4 of the previous layer’s output with a small (learnable) ﬁlter hs 

(hs (cid:63) f s−1)(x) =

hs(y− x) f s−1(y) 

(1)

(cid:88)

y

and then applying a nonlinearity σ  such as the Re-LU operator:
f s(x) = σ((hs (cid:63) f s−1)(x)).

(2)
Deﬁning Tx(hs)(y) = hs(y − x)  which is nothing but hs translated by x  allows us to equivalently
write (1) as

(hs (cid:63) f s−1)(x) = (cid:104)f s−1  Tx(hs)(cid:105) 

where the inner product is (cid:104)f s−1  Tx(hs)(cid:105) =(cid:80)

(3)
y f s−1(y) Tx(hs)(y). What this formula tells us is
that fundamentally each layer of the CNN just does pattern matching: f s(x) is an indication of how
well the part of f s−1 around x matches the ﬁlter hs.
Equation 3 is the natural starting point for generalizing convolution to the unit sphere  S2. An
immediate complication that we face  however  is that unlike the plane  S2 cannot be discretized
by any regular (by which we mean rotation invariant) arrangement of points. A number of authors
have addressed this problem in different ways [14  15]. Instead of following one of these approaches 
similarly to recent work on manifold CNNs [23  24]  in the following we simply treat each f s and
the corresponding ﬁlter hs as continuous functions on the sphere  f s(θ  φ) and hs(θ  φ)  where θ and
φ are the polar and azimuthal angles. We allow both these functions to be complex valued  the reason
for which will become clear later.
The inner product of two complex valued functions on the surface of the sphere is given by the
formula

(cid:104)g  h(cid:105)S2

(4)
where ∗ denotes complex conjugation. Further  h (dropping the layer index for clarity) can be
moved to any point (θ0  φ0) on S2 by taking h(cid:48)(θ  φ) = h(θ− θ0  φ− φ0). This suggests that the
generalization of 3 to the sphere should be

−π

=

0

[g(θ  φ)]∗ h(θ  φ) cos θ dθ dφ 

1
4π

(cid:90) 2π

(cid:90) π

(h (cid:63) f )(θ0  φ0) =

1
4π

[h(θ− θ0  φ− φ0)]∗ f (θ  φ) cos θ dθ dφ.

(5)

Unfortunately  this generalization would be wrong  because it does not take into account that h can
also be rotated around a third axis. The correct way to generalize cross-correlations to the sphere is
to deﬁne h (cid:63) f as a function on the rotation group itself  i.e.  to set

(cid:90) 2π

(cid:90) π

0

−π

(cid:90) 2π

(cid:90) π

0

−π

(cid:2)hR(θ  φ)(cid:3)∗

(h (cid:63) f )(R) =

1
4π

f (θ  φ) cos θ dθ dφ

R ∈ SO(3) 

(6)

(7)

where hR is h rotated by R  expressible as

hR(x) = h(R−1x) 

with x being the point on the sphere at position (θ  φ) (c.f. [25][1]).

2.1 Fourier space ﬁlters and activations

Cohen et al.[1] observe that the double integral in (6) would be extremely inconvenient to compute
in a neural network. As mentioned  in the case of the sphere  just ﬁnding the right discretizations
to represent f and h is already problematic. As an alternative  it is natural to represent both these
functions in terms of their spherical harmonic expansions

∞(cid:88)

(cid:96)(cid:88)

(cid:96)=0

m=−(cid:96)

(cid:98)f m

f (θ  φ) =

∞(cid:88)

(cid:96)(cid:88)

(cid:96)=0

m=−(cid:96)

(cid:98)hm

(cid:96) Y m

(cid:96) (θ  φ)

h(θ  φ) =

(cid:96) Y m

(cid:96) (θ  φ).

(8)

4Convolution and cross-correlation are closely related mathematical concepts that are somewhat confounded
in the deep learning literature. In this paper we are going to be a little more precise and focus on cross-correlation 
because  despite their name  that is what CNNs actually compute.

3

(cid:96) (θ  φ) are the well known spherical harmonic functions indexed by (cid:96) = 0  1  2  . . . and
Here  Y m
m ∈ {−(cid:96) −(cid:96) + 1  . . .   (cid:96)}. The spherical harmonics form an orthonormal basis for L2(S2)  so (8)
can be seen as a kind of Fourier series on the sphere  in particular  the elements of the f0  f1  f2  . . .
coefﬁcient vectors can be computed relatively easily by

(cid:98)f m

(cid:96) =

1
4π

(cid:90) 2π

(cid:90) π

0

−π

f (θ  φ) Y m

(cid:96) (θ  φ) cos θ dθ dφ 

and similarly for h. Similarly to usual Fourier series  in practical scenarios spherical harmonic
expansions are computed up to some limiting “frequency” L  which depends on the desired resolution.
Noncommutative harmonic analysis [26  27] tells us that functions on the rotation group also admit a
type of generalized Fourier transform. Given a function g : SO(3) → C  the Fourier transform of g is
deﬁned as the collection of matrices

SO(3)

G(cid:96) =

g(R) ρ(cid:96)(R) dµ(R)

(9)
where ρ(cid:96) : SO(3) → C(2(cid:96)+1)×(2(cid:96)+1) are ﬁxed matrix valued functions called the irreducible repre-
sentations of SO(3)  sometimes also called Wigner D-matrices. Here µ is a ﬁxed measure called the
Haar measure that just hides factors similar to the cos θ appearing in (4). For future reference we also
note that one dimensional irreducible representation ρ0 is the constant representation ρ0(R) = (1).
The inverse Fourier transform is given by

(cid:96) = 0  1  2  . . .  

(cid:90)

1
4π

tr(cid:2)G(cid:96) ρ(cid:96)(R−1)(cid:3)

∞(cid:88)

(cid:96)=0

g(R) =

R ∈ SO(3).

[(cid:100)h (cid:63) f ](cid:96) = (cid:98)f(cid:96) ·(cid:98)h

†
(cid:96)

While the spherical harmonics can be chosen to be real  the ρ(cid:96)(R) representation matrices are
inherently complex valued. This is the reason that we allow all other quantities  including the f s
activations and hs ﬁlters to be complex  too.
Remarkably  the above notions of harmonic analysis on the sphere and the rotation group are closely
related. In particular  it is possible to show that each Fourier component of the spherical cross
correlation (6) that we are interested in computing is given simply by the outer product

(cid:96) = 0  1  2  . . .   L 

(10)
where † denotes the conjugate transpose (Hermitian conjugate) operation. Cohen et al.’s Spherical
CNNs [1] are essentially based on this formula. In particular  they argue that instead of the continuous

function f  it is more expedient to regard the components of the (cid:98)f0 (cid:98)f1  . . .  (cid:98)fL vectors as the
“activations” of their neural network  while the learnable weights or ﬁlters are the(cid:98)h0 (cid:98)h1  . . .  (cid:98)hL

vectors. Computing spherical convolutions in Fourier space then reduces to just computing a few
outer products. Layers s = 2  3  . . .   S of the Spherical CNN operate similarly  except that f s−1 is a
function on SO(3)  so (6) must be replaced by cross-correlation on SO(3) itself  and h must also be
a function on SO(3) rather than just the sphere. Fortuitiously  the resulting cross-correlation formula
is almost exactly the same:

(cid:96) = 0  1  2  . . .   L 

(11)

apart from the fact that now F(cid:96) and H(cid:96) are matrices (see [1] for details).

3 Generalized spherical CNNs

The starting point for our Generalized Spherical CNNs is the Fourier space correlation formula (10).
In contrast to [1]  however  rather than the geometry  we concentrate on its algebraic properties  in
particular  its behavior under rotations. It is well known that if we rotate a spherical function by some
R ∈ SO(3) as in (7)  then each vector of its spherical harmonic expansion just gets multiplied with
the corresponding Wigner D-matrix:

(12)
For functions on SO(3)  the situation is similar. If g : SO(3) → C  and g(cid:48) is the rotated function
g(cid:48)(R(cid:48)) = g(R−1R(cid:48))  then the Fourier matrices of g(cid:48) are G(cid:48)
(cid:96) = ρ(cid:96)(R) G(cid:96). The following proposition
shows that the matrices output by the (10) and (11) cross-correlation formulae behave analogously.

(cid:98)f(cid:96) (cid:55)→ ρ(cid:96)(R) · (cid:98)f(cid:96).

4

[(cid:100)h (cid:63) f ](cid:96) = F(cid:96) · H

†
(cid:96)

Proposition 1 Let f : S2 → C be an activation function that under the action of a rotation R
transforms as (7)  and let h : S2 → C be a ﬁlter. Then  each Fourier component of the cross
correlation (6) transforms as

[(cid:100)h (cid:63) f ](cid:96) (cid:55)→ ρ(cid:96)(R) · [(cid:100)h (cid:63) f ](cid:96).

(13)

Similarly  if f(cid:48)  h(cid:48) : SO(3) → C  then (cid:92)h(cid:48) (cid:63) f(cid:48) (as deﬁned in (11)) transforms the same way.
Equation (12) describes the behavior of spherical harmonic vectors under rotations  while (15)
describes the behavior of Fourier matrices. However  the latter is equivalent to saying that each
column of the matrices separately transforms according to (12). One of the key ideas of the present
paper is to take this property as the basis for the deﬁnition of covariance to rotations in neural nets.
Thus we have the following deﬁnition.
Deﬁnition 1 Let N be an S + 1 layer feed-forward neural network whose input is a spherical
function f 0 : S2 → Cd. We say that N is a generalized SO(3)–covariant spherical CNN if the
output of each layer s can be expressed as a collection of vectors

  . . . . . . . . .   . . .(cid:98)f s
(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)

0 2  . . .  (cid:98)f s
0 1 (cid:98)f s
(cid:98)f s = ((cid:98)f s
 (cid:98)f s
1 1 (cid:98)f s
1 2  . . .  (cid:98)f s
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:123)(cid:122)
where each (cid:98)f s
some rotation R  then (cid:98)f s
(cid:96) j ∈ C2(cid:96)+1 is a ρ(cid:96)–covariant vector in the sense that if the input image is rotated by
(cid:96) j (cid:55)→ ρ(R) · (cid:98)f s
(cid:96) j transforms as(cid:98)f s
(cid:96) j vectors the irreducible fragments of (cid:98)f s  and the integer vector
We call the individual (cid:98)f s
L) counting the number of fragments for each (cid:96) the type of (cid:98)f s.

(14)

τ s = (τ s

0   τ s

1   . . .   τ s

(15)

) 

L τ s
L

(cid:96)=L

(cid:96)=0

(cid:96)=1

0 τ s
0

1 τ s
1

(cid:96) j.

There are a few things worth noting about Deﬁnition 1. First  since the (15) maps are linear  clearly any
SO(3)–covariant spherical CNN is equivariant to rotations  as deﬁned in the introduction. Second 
since in [1] the inputs are functions on the sphere  whereas in higher layers the activations are
functions on SO(3)  their architecture is a special case of Deﬁnition 1 with τ 0 = (1  1  . . .   1) and
τ s = (1  3  5  . . .   2L + 1) for s ≥ 1.
Finally  by the theorem of complete reducibility of representations of compact groups  any f s that
transforms under rotations linearily is reducible into a sequence of irreducible fragments as in (14).
This means that (14) is really the most general possible form for an SO(3) equivariant neural network.
As we remarked in the introduction  technically  the terms “equivariant” and “covariant” map to the
same concept. The difference between them is one of emphasis. We use the term “equivariant” when
we have the same group acting on two objects in a way that is qualitively similar  as in the case of the
rotation group acting on functions on the sphere and on cross-correlation functions on SO(3). We use
the term “covariant” if the actions are qualitively different  as in the case of rotations of functions on
the sphere and the corresonding transformations (15) of the irreducible fragments in a neural network.
To fully deﬁne our neural network  we need to describe three things: 1. The form of the linear
transformations in each layer involving learnable weights  2. The form of the nonlinearity in each
layer  3. The way that the ﬁnal output of the network can be reduced to a vector that is rotation
invariant  since that is our ultimate goal. The following subsections describe each of these components
in turn.

3.1 Covariant linear transformations

In a covariant neural network architecture  the linear operation of each layer must be covariant. As
described in the Introduction  in classical CNNs  convolution automatically satisﬁes this criterion. In
the more general setting of covariance to the action of compact groups  the problem was studied in
[7]. The specialization of their result to our case is the following.

Proposition 2 Let (cid:98)f s be an SO(3)–covariant activation function of the form (14)  and(cid:98)gs = L((cid:98)f s)
be a linear function of (cid:98)f s written in a similar form. Then(cid:98)gs is SO(3)–covariant if and only each(cid:98)gs
fragment is a linear combination of fragments from (cid:98)f s with the same (cid:96).

(cid:96) j

5

Gs

(cid:96) = F s

(cid:96) W s
(cid:96)

(2(cid:96) + 1) × τ s
that

(cid:96) dimensional matrix F s

for some sequence of complex valued martrices W s

Proposition 2 can be made more clear by stacking all fragments of (cid:98)f corresponding to (cid:96) into a
(cid:96)   and doing the same for(cid:98)g. Then the proposition tells us simply
need to be square  i.e.  the number of fragments in (cid:98)f and(cid:98)g corresponding to (cid:96) might be different. In
responding to taking W(cid:96) = (cid:98)h

the context of a neural network  the entries of the W s
Note that the Fourier space cross-correlation formulae (10) and (11) are special cases of (16) cor-
†
(cid:96) . The case of general W(cid:96) does not have such an
intuitive interpretation in terms of cross-correlation. What the (16) lacks in interpretability it makes
up for in terms of generality  since it provides an extremely simple and ﬂexible way of inducing
SO(3)–covariant linear transformations in neural networks.

(16)
(cid:96) does not necessarily

(cid:96) matrices are learnable parameters.

†
(cid:96) or W(cid:96) = H

L. Note that W s

(cid:96) = 0  1  2  . . .   L

0   . . .   W s

3.2 Covariant nonlinearities: the Clebsch–Gordan transform

Differentiable nonlinearities are essential for the operation of multi-layer neural networks. Formu-
lating covariant nonlinearities in Fourier space  however  is more challenging than formulating the
linear operation. This is the reason that most existing group equivariant neural networks perform this
operation in “real space”. However  as discussed above  moving back and forth between real space
and the Fourier domain comes at a signﬁciant cost and leads to a range of complications involving
quadrature on the transformation group and numerical errors.
One of the key contributions of the present paper is to propose a fully Fourier space nonlinearity based
on the Clebsch–Gordan transform. In representation theory  the Clebsch–Gordan decomposition
arises in the context of decomposing the tensor (i.e.  Kronecker) product of irreducible representations
into a direct sum of irreducibles. In the speciﬁc case of SO(3)  it takes form

ρ(cid:96)1(R) ⊗ ρ(cid:96)2(R) = C(cid:96)1 (cid:96)2

ρ(cid:96)(R)

C(cid:62)

(cid:96)1 (cid:96)2

 

R ∈ SO(3) 

(cid:20)

(cid:96)1+(cid:96)2(cid:77)

(cid:96)=|(cid:96)1−(cid:96)2|

(cid:21)

(cid:98)g(cid:96) = C(cid:62)
(cid:71)

|(cid:96)1−(cid:96)2|≤(cid:96)≤(cid:96)1+(cid:96)2

where C(cid:96)1 (cid:96)2 are ﬁxed matrices. Equivalently  letting C(cid:96)1 (cid:96)2 (cid:96) denote the appropriate block of columns
of C(cid:96)1 (cid:96)2 

ρ(cid:96)(R) = C(cid:62)

(cid:96)1 (cid:96)2 (cid:96) [ρ(cid:96)1(R) ⊗ ρ(cid:96)2(R)] C(cid:96)1 (cid:96)2 (cid:96).

The CG-transform is well known in physics  because it is intimately related to the algebra of angular
momentum in quantum mechanics  and the entries of the C(cid:96)1 (cid:96)2 (cid:96) matrices can be computed relatively
easily. The following Lemma explains why this construction is relevant to creating Fourier space
nonlinearities.

Lemma 3 Let (cid:98)f(cid:96)1 and (cid:98)f(cid:96)2 be two ρ(cid:96)1 resp. ρ(cid:96)2 covariant vectors  and (cid:96) be any integer between

|(cid:96)1− (cid:96)2| and (cid:96)1 + (cid:96)2. Then

(cid:2)(cid:98)f(cid:96)1 ⊗ (cid:98)f(cid:96)2

(cid:3)

(cid:96)1 (cid:96)2 (cid:96)

(17)

is a ρ(cid:96)–covariant vector.
Exploiting Lemma 3  the nonlinearity used in our generalized Spherical CNNs consists of computing
(17) between all pairs of fragments. In matrix notation 
C(cid:62)

(cid:2)F s

⊗ F s

(cid:3) 

Gs

(18)

(cid:96)1 (cid:96)2 (cid:96)

(cid:96)1

(cid:96)2

(cid:96) =

where (cid:116) denotes merging matrices horizontally. Note that this operation increases the size of
the activation substantially: the total number of fragments is squared  which can potentially be
problematic  and is addressed in the following subsection.
The Clebsch–Gordan decomposition has recently appeared in two preprints discussing neural net-
works for learning physical systems [21  22]. However  to the best of our knowledge  in the present
context of a general purpose nonlinearity  it has never been proposed before. At ﬁrst sight  the
computational cost it would appear that the computational cost of (17) (assuming that C(cid:96)1 (cid:96)2 (cid:96) has
been precomputed) is (2(cid:96)1 + 1)(2(cid:96)2 + 1)(2(cid:96) + 1). However  C(cid:96)1 (cid:96)2 (cid:96) is actually sparse  in particular
[C(cid:96)1 (cid:96)2 (cid:96)](m1 m2) m = 0 unless m1 +m2 = m. Denoting the total number of scalar entries in the F s
(cid:96) (cid:96)

6

(cid:96) = 0

(cid:96) = 1

F s−1

0

F s−1

1

(cid:96) = 2

F s−1

2

×W0

×W1

×W2

F s
0

F s
1

F s
2

Inputs

Clebsch–Gordan product

Linear

Transform

Outputs

Figure 1: Schematic of a single layer of the Clebsch–Gordan network.

matrices by N  this reduces the complexity of computing (18) to O(N 2L). While the CG transform
is not currently available as a differentiable operator in any of the major deep learning software
frameworks  we have developed and will publicly release a C++ PyTorch extension for it.
A more unusual feature of the CG nonlinearity is that its essentially quadratic nature. Quadratic
nonlinearities are not commonly used in deep neural networks. Nonetheless  our experiments indicate
that the CG nonlinearity is effective in the context of learning spherical images. It is also possible to
use higher CG powers  although the computational cost will obviously increase.

3.3 Limiting the number of channels

In a covariant network  each individual (cid:98)f s

(cid:96) fragment is effecively a separate channel. In this sense 
the quadratic increase in the number of channels after the CG-transform can be seen as a natural
broadening of the network to capture more complicated features. Naturally  allowing the number of
channels to increase quadratically in each layer would be untenable  though.
Following the results of Section 3.1  the natural way to counteract the exponential increase in the
number of channels is follow the CG-transform with another learnable linear transformation that
reduces the number of fragments for each (cid:96) to some ﬁxed maximum number τ (cid:96).
In fact  this
linear transformation can replace the transformation of Section 3.1. Whereas in conventional neural
networks the linear transformation always precedes the nonlinear operation  in Clebsch–Gordan
networks it is natural to design each layer so as to perform the CG-transform ﬁrst  and then the
convolution like step (16)  which will limit the number of fragments.

(cid:55)→ ρ(cid:96)(R) F S−1

of scalars. In our Fourier theoretic language  this simply corresponds to the (cid:98)f S

3.4 Final invariant layer
After the S − 1’th layer  the activations of our network will be a series of matrices F S−1
  . . .   F S−1
 
each transforming under rotations according to F S−1
. Ultimately  however  the
objective of the network is to output a vector that is invariant with respect rotations  i.e.  a collection
0 j fragments  since the
0 are invariant. Thus  the ﬁnal layer
(cid:96) = 0 representation is constant  and therefore the elements of F S
can be similar to the earlier ones  except that it only needs to output this single (single row) matrix.
Note that in contrast to other architectures such as [1] that involve repeated forward and backward
transforms  thanks to their fully Fourier nature  for Clebsch–Gordan nets  in both training and
testing  the elements of F S
0 are guaranteed to be invariant to rotations of arbitrary magnitude not just
approximately  but in the exact sense  up to limitations of ﬁnite precision arithmetic. This is a major
advantage of Clebsch–Gordan networks compared to other covariant architectures.

(cid:96)

0

L

(cid:96)

3.5 Summary of algorithm

In summary  our Spherical Clebsch–Gordan network is an S +1 layer feed-forward neural network in
which apart from the initial spherical harmonic transform  every other operation is a simple matrix
operation. The algorithm is presented in explicit form in the Supplement.

7

RMSE
Method
5.96
MLP/Random CM [28]
LGIKA (RF) [29]
10.82
RBF Kernels/Rand CM [28] 11.42
RBF Kernels/Sorted CM [28] 12.59
MLP/Sorted CM [28]
16.06
8.47
Spherical CNN [1]
7.97
Ours (FFS2CNN)

P@N R@N F1@N mAP NDCG
Method
0.705 0.769 0.719 0.696 0.783
Tatsuma_ReVGG
Furuya_DLAN
0.814 0.683 0.706 0.656 0.754
SHREC16-Bai_GIFT 0.678 0.667 0.661 0.607 0.735
Deng_CM-VGG5-6DB 0.412 0.706 0.472 0.524 0.624
Spherical CNNs [1]
0.701 0.711 0.699 0.676 0.756
0.707 0.722 0.701 0.683 0.756
FFS2CNNs (ours)

Table 1: Results on the QM7 and 3D shape recognition datasets.

4 Experiments

In this section we describe experiments that give a direct comparison with those reported by Cohen
et al. [1]. We choose these experiments as the Spherical CNN proposed in [1] is the only direct
competition to our method.

Rotated MNIST on the Sphere We use a version of MNIST in which the images are painted onto
a sphere and use two instances as in [1]  more details about the data  baseline models  as well as the
detailed architecture of our model and hyperparameters are provided in the appendix. We report three
sets of experiments: For the ﬁrst set both the training and test sets were not rotated (denoted NR/NR) 
for the second  the training set was not rotated while the test was randomly rotated (NR/R) and ﬁnally
when both the training and test sets were rotated (denoted R/R).
NR/NR NR/R
22.18
97.67
95.59
94.62
96.4

Method
Baseline CNN
Cohen et al.
Ours (FFS2CNN)

R/R
12
93.4
96.6

96

We observe that the baseline model’s performance deteriorates in the three cases  more or less
reducing to random chance in the R/R case. While our results are better than those reported in [1] 
they also have another characteristic: they remain roughly the same in the three regimes  while those
of [1] slightly worsen. We think this might be a result of the loss of equivariance in their method.

Atomization Energy Prediction Next  we apply our framework to the QM7 dataset [30  31] 
where the goal is to regress over atomization energies of molecules given atomic positions (pi) and
charges (zi). Each molecule contains up to 23 atoms of 5 types (C  N  O  S  H). More details about
the representations used  baseline models  as well as the architectural parameters are provided in the
appendix. The ﬁnal results are presented in the table  which show that our method outperforms the
Spherical CNN of Cohen et al.. The only method that delivers better performance is a MLP trained
on randomly permuted Coulomb matrices [28]  and as [1] point out  this method is unlikely to scale
to large molecules as it needs a large sample of random permutations  which grows rapidly with N.

3D Shape Recognition Finally  we report results for shape classiﬁcation using the SHREC17
dataset [32]  which is a subset of the larger ShapeNet dataset [33] having roughly 51300 3D models
spread over 55 categories. Architectural details are provided in the appendix. We compare our
results to some of the top performing models on SHREC (which use architectures specialized to the
task) as well as the model of Cohen et al.. Our method  like the model of Cohen et al. is task agnostic
and uses the same representation. Despite this  it is able to consistently come second or third in the
competition  showing that it affords an efﬁcient method to learn from spherical signals.

5 Conclusion

We have presented an SO(3)-equivariant neural network architecture for spherical data that operates
completely in Fourier space  circumventing a major drawback of earlier models that need to switch
back and forth between Fourier space and “real” space. We achieve this by – rather unconventionally
– using the Clebsch-Gordan decomposition as the only source of nonlinearity. While the speciﬁc
focus is on spheres and SO(3)-equivariance  the approach is more widely applicable  suggesting a
general formalism for designing fully Fourier neural networks that are equivariant to the action of
any compact continuous group.

8

References

[1] T. S. Cohen  M. Geiger  J. Köhler  and M. Welling. Spherical CNNs. International Conference on Learning

Representations  2018.

[2] Y LeCun  B Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel. Backpropa-

gation applied to handwritten zip code recognition. Neural Computation  1:541–551  1989.

[3] R. Gens. Deep Symmetry Networks. NIPS 2014  pages 1–9  2014.
[4] T. S. Cohen and M. Welling. Group equivariant convolutional networks. Proceedings of The 33rd

International Conference on Machine Learning  48:2990–2999  2016.

[5] T. S. Cohen and M. Welling. Steerable cnns. In ICLR  2017.
[6] S. Ravanbakhsh  J. Schneider  and B. Poczos. Equivariance through parameter-sharing. In Proceedings of

International Conference on Machine Learning  2017.

[7] R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the

action of compact groups. 2018.

[8] C. Esteves  C. Allen-Blanchette  A. Makadia  and K. Daniilidis. Learning SO(3) Equivariant Representa-

tions with Spherical CNNs. 2017.

[9] L. Zelnik-Manor  G. Peters  and P. Perona. Squaring the Circles in Panoramas. Ieee Iccv  pages 1292–1299 

2005.

[10] J. Cruz-Mota  I. Bogdanova  B. Paquier  M. Bierlaire  and J-P Thiran. Scale invariant feature transform on

the sphere: Theory and applications. International Journal of Computer Vision  98(2):217–241  2012.

[11] Y-C Su  D. Jayaraman  and K. Grauman. Pano2vid: Automatic cinematography for watching 360 videos.
Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture
Notes in Bioinformatics)  10114 LNCS(1):154–171  2017.

[12] W-S Lai  Y. Huang  N. Joshi  C. Buehler  M-H Yang  and S-B Kang. Semantic-driven Generation of

Hyperlapse from 360 @BULLET Video. pages 1–12.

[13] R. Khasanova and P. Frossard. Graph-Based Classiﬁcation of Omnidirectional Images. 2017.
[14] W. Boomsma and J. Frellsen. Spherical convolutions and their application in molecular modelling.

(Nips):3436–3446  2017.

[15] Y-C Su and K. Grauman. Making 360$ˆ{}$ Video Watchable in 2D: Learning Videography for Click Free

Viewing. 2017.

[16] D. M. Healey  D. N. Rockmore  and S. B. Moore. An fft for the 2-sphere and applications. In 1996 IEEE
International Conference on Acoustics  Speech  and Signal Processing Conference Proceedings  volume 3 
pages 1323–1326 vol. 3  May 1996.

[17] P. J. Kostelec and D. N. Rockmore. Ffts on the rotation group. Journal of Fourier Analysis and Applications 

14(2):145–179  Apr 2008.

[18] D. E. Worrall  S. J. Garbin  D. Turmukhambetov  and G. J. Brostow. Harmonic Networks: Deep Translation

and Rotation Equivariance. 2016.

[19] C. Esteves  C. Allen-Blanchette  X. Zhou  and K. Daniilidis. Polar Transformer Networks. 2017.
[20] J-P. Serre. Linear Representations of Finite Groups  volume 42 of Graduate Texts in Mathamatics.

Springer-Verlag  1977.

[21] N. Thomas  T. Smidt  S. Kearnes  L. Yang  L. Li  K. Kohlhoff  and P. Riley. Tensor ﬁeld networks: Rotation-
and translation-equivariant neural networks for 3d point clouds. arXiv:1802.08219 [cs]  Feb 2018. arXiv:
1802.08219.

[22] R. Kondor. N-body networks: a covariant hierarchical neural network architecture for learning atomic

potentials. CoRR  abs/1803.01588  2018.

[23] J. Masci  D. Boscaini  M. M. Bronstein  and P. Vandergheynst. Geodesic convolutional neural networks on

riemannian manifolds. 2015.

[24] F. Monti  D. Boscaini  J. Masci  E. Rodola  J. Svoboda  and M. M. Bronstein. Geometric deep learning on

graphs and manifolds using mixture model cnns. 2016.

[25] B. Gutman  Y. Wang  T. Chan  P. M. Thompson  and A. W. Toga. Shape Registration with Spherical Cross
Correlation. 2nd MICCAI Workshop on Mathematical Foundations of Computational Anatomy  pages
56–67  2008.

[26] A. Terras. Fourier analysis on ﬁnite groups and applications  volume 43 of London Mathematical Society

Student Texts. Cambridge Univ. Press  1999.

[27] P. Diaconis. Group Representation in Probability and Statistics  volume 11 of IMS Lecture Series. Institute

of Mathematical Statistics  1988.

[28] G. Montavon  K. Hansen  S. Fazli  M. Rupp  F. Biegler  A. Ziehe  A. Tkatchenko  O.A. von Lilienfeld  and
K. Müller. Learning invariant representations of molecules for atomization energy prediction. In NIPS 
2012.

9

[29] A. Raj  A. Kumar  Y. Mroueh  and P.T. Fletcher et al. Local group invariant representations via orbit

embeddings. 2016.

[30] L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the chemical

universe database gdb-13. Journal of the American Chemical Society  2009.

[31] M. Rupp  A. Tkatchenko  K.-R. Müller  and O. A. von Lilienfeld. Fast and accurate modeling of molecular

atomization energies with machine learning. Physical Review Letters  2012.

[32] M. Savva  F. Yu  H. Su  A. Kanezaki  T. Furuya  R. Ohbuchi  Z. Zhou  R. Yu  S. Bai  X. Bai  M. Aono 
A. Tatsuma  S. Thermos  A. Axenopoulos  G. Th. Papadopoulos  P. Daras  X. Deng  Z. Lian  B. Li 
H. Johan  Y. Lu  and S. Mk. Large-scale 3d shape retrieval from shapenet core55. Eurographics Workshop
on 3D Object Retrieval  2017.

[33] A. X. Chang  T. Funkhouser  L. Guibas  P. Hanrahan  Q. Huang  Z. Li  S. Savarese  M. Savva  S. Song 

H. Su  J. Xiao  L. Yi  and F. Yu. Shapenet: An information-rich 3d model repository. 2015.

10

,Risi Kondor
Zhen Lin
Shubhendu Trivedi