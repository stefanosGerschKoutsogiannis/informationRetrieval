2019,Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing,SLOPE is a relatively new convex optimization procedure for high-dimensional linear regression via the sorted $\ell_1$ penalty: the larger the rank of the fitted coefficient  the larger the penalty. This non-separable penalty renders many existing techniques invalid or inconclusive in analyzing the SLOPE solution. In this paper  we develop an asymptotically exact characterization of the SLOPE solution under Gaussian random designs through solving the SLOPE problem using approximate message passing (AMP). This algorithmic approach allows us to approximate the SLOPE solution via the much more amenable AMP iterates. Explicitly  we characterize the asymptotic dynamics of the AMP iterates relying on a recently developed state evolution analysis for non-separable penalties  thereby overcoming the difficulty caused by the sorted $\ell_1$ penalty. Moreover  we prove that the AMP iterates converge to the SLOPE solution in an asymptotic sense  and numerical simulations show that the convergence is surprisingly fast. Our proof rests on a novel technique that specifically leverages the SLOPE problem. In contrast to prior literature  our work not only yields an asymptotically sharp analysis but also offers an algorithmic  flexible  and constructive approach to understanding the SLOPE problem.,Algorithmic Analysis and Statistical Estimation of

SLOPE via Approximate Message Passing

Zhiqi Bu∗

Jason M. Klusowski†

Cynthia Rush‡

Weijie Su§

Abstract

SLOPE is a relatively new convex optimization procedure for high-dimensional
linear regression via the sorted (cid:96)1 penalty: the larger the rank of the ﬁtted coefﬁcient 
the larger the penalty. This non-separable penalty renders many existing techniques
invalid or inconclusive in analyzing the SLOPE solution. In this paper  we develop
an asymptotically exact characterization of the SLOPE solution under Gaussian
random designs through solving the SLOPE problem using approximate message
passing (AMP). This algorithmic approach allows us to approximate the SLOPE
solution via the much more amenable AMP iterates. Explicitly  we characterize
the asymptotic dynamics of the AMP iterates relying on a recently developed state
evolution analysis for non-separable penalties  thereby overcoming the difﬁculty
caused by the sorted (cid:96)1 penalty. Moreover  we prove that the AMP iterates converge
to the SLOPE solution in an asymptotic sense  and numerical simulations show
that the convergence is surprisingly fast. Our proof rests on a novel technique that
speciﬁcally leverages the SLOPE problem. In contrast to prior literature  our work
not only yields an asymptotically sharp analysis but also offers an algorithmic 
ﬂexible  and constructive approach to understanding the SLOPE problem.

Introduction

1
Consider observing linear measurements y ∈ Rn that are modeled by the equation

y = Xβ + w 

(1.1)
where X ∈ Rn×p is a known measurement matrix  β ∈ Rp is an unknown signal  and w ∈ Rn is the
measurement noise. Among numerous methods that seek to recover the signal β from the observed
data  especially in the setting where β is sparse and p is larger than n  SLOPE has recently emerged
as a useful procedure that allows for estimation and model selection [9]. This method reconstructs
the signal by solving the minimization problem

(cid:98)β := arg min

b

p(cid:88)

i=1

(cid:107)y − Xb(cid:107)2 +

1
2

λi|b|(i) 

(1.2)

PA 19104  USA. Email: zbu@sas.upenn.edu

where (cid:107) · (cid:107) denotes the (cid:96)2 norm  λ1 ≥ ··· ≥ λp ≥ 0 (with at least one strict inequality) is a
sequence of thresholds  and |b|(1) ≥ ··· ≥ |b|(p) are the order statistics of the ﬁtted coefﬁcients
∗Department of Applied Mathematics and Computational Science  University of Pennsylvania  Philadelphia 
†Department of Statistics  Rutgers University  New Brunswick  NJ 08854  USA. Email:
‡Department
of Statistics  Columbia University  New York  NY 10027  USA. Email:
§Department of Statistics  University of Pennsylvania  Philadelphia  PA 19104  USA. Email:
suw@wharton.upenn.edu Supported in part by NSF DMS CAREER #1847415 and NSF CCF #1763314.

jason.klusowski@rutgers.edu Supported in part by NSF DMS #1915932.

cynthia.rush@columbia.edu Supported in part by NSF CCF #1849883.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Set Diff

ISTA
FISTA
AMP

60
47
30

Optimization errors
10−5
10−3
9007
7326
593
374
13
32

10−4
8569
412
22

10−2
4048
275
6

10−6
9161
604
40

Table 1: First iteration t for which there is zero

set difference or optimization error ||βt − (cid:98)β||2/p

falls below a threshold.

Figure 1: Optimization errors  ||βt − (cid:98)β||2/p  and (sym-
metric) set difference of supp(βt) and supp((cid:98)β).
in absolute value. The regularizer(cid:80) λi|b|(i) is a sorted (cid:96)1-norm (denoted as Jλ(b) henceforth) 

Setting of Figure 1 and Table 1: Design X
is 500 × 1000 and has i.i.d. N (0  1/500) en-
tries. True signal β is elementwise i.i.d. Gaussian-
Bernoulli: N (0  1) with probability 0.1 and 0 oth-
erwise. Noise variance σ2
w = 0. A careful calibra-
tion between the thresholds θt in AMP and λ is
SLOPE is used. Details in Section 2.

which is non-separable due to the sorting operation involved in its calculation. Notably  SLOPE
has two attractive features that are not simultaneously present in other methods for linear regression
including the LASSO [34] and knockoffs [1]. Explicitly  on the estimation side  SLOPE achieves
minimax estimation properties under certain random designs without requiring any knowledge of the
sparsity degree of β [32  7]. On the testing side  SLOPE controls the false discovery rate in the case
of independent predictors [9  12]. For completeness  we remark that [10  35  20] proposed similar
non-separable regularizers to encourage grouping of correlated predictors.
This work is concerned with the algorithmic aspects of SLOPE through the lens of approximate
message passing (AMP) [2  17  22  24  27]. AMP is a class of computationally efﬁcient and easy-
to-implement algorithms for a broad range of statistical estimation problems  including compressed
sensing and the LASSO [3]. When applied to SLOPE  AMP takes the following form: at initial
iteration t = 0  assign β0 = 0  z0 = y  and for t ≥ 0 
(X(cid:62)zt + βt) 

(1.3a)

βt+1 = proxJθt
zt+1 = y − Xβt+1 +

(cid:104)∇ proxJθt

zt
n

(cid:105)

(X(cid:62)zt + βt)

.

(1.3b)

The non-increasing sequence θt is proportional to λ and will be given explicitly in Section 2. Here 
proxJθ

is the proximal operator of the sorted (cid:96)1 norm  that is 

proxJθ

(x) := argmin

b

(cid:107)x − b(cid:107)2 + Jθ(b) 

1
2

and ∇ proxJθ
denotes the divergence of the proximal operator (see an equivalent but more explicit
form of this algorithm in Section 2 and other preliminaries on SLOPE and the prox operator deﬁned
above in Appendix A). Compared to the proximal gradient descent (ISTA) [15  16  26]  AMP has an
extra correction term in its residual step that adjusts the iteration in a non-trivial way and seeks to
provide improved convergence performance [17  11].
The empirical performance of AMP in solving SLOPE is illustrated in Figure 1 and Table 1  which
suggest the superiority of AMP over ISTA and FISTA [6]—perhaps the two most popular proximal
gradient descent methods—in terms of speed of convergence. However  the vast AMP literature thus
far remains silent on whether AMP provably solves SLOPE and  if so  whether one can leverage
AMP to get insights into the statistical properties of SLOPE. This vacuum in the literature is due to
the non-separability of the SLOPE regularizer  making it a major challenge to apply AMP to SLOPE
directly. In stark contrast  AMP theory has been rigorously applied to the LASSO [3]  showing
both good empirical performance and nice theoretical properties of solving the LASSO using AMP.
Moreover  AMP in this setting allows for asymptotically exact statistical characterization of its output 
which converges to the LASSO solution  thereby providing a powerful tool in ﬁne-grained analyses
of the LASSO [4  33  25].
In this work  we prove that the AMP algorithm (1.3) solves the SLOPE problem in an asymptotically
exact sense under independent Gaussian random designs. Our proof uses the recently extended

2

0.000.030.060.09100101102103104Optimization ErrorAMPFISTAISTA0250500750100100.5101101.5102102.5IterationSet DifferenceAMPFISTAISTAAMP theory for non-separable denoisers [8] and applies this tool to derive the state evolution that
describes the asymptotically exact behaviors of the AMP iterates βt in (1.3). The next step  which is
the core of our proof  is to relate the AMP estimates to the SLOPE solution. This presents several
challenges that cannot be resolved only within the AMP framework. In particular  unlike the LASSO 
the number of nonzeros in the SLOPE solution can exceed the number of observations. This fact
imposes substantially more difﬁculties on showing that the distance between the SLOPE solution
and the AMP iterates goes to zero than in the LASSO case due to the possible non-strong convexity
of the SLOPE problem  even restricted to the solution support. To overcome these challenges  we
develop novel techniques that are tailored to the characteristics of the SLOPE solution. For example 
our proof relies on the crucial property of SLOPE that the unique nonzero components of its solution
never outnumber the observation units.
As a byproduct  our analysis gives rise to an exact asymptotic characterization of the SLOPE solution
under independent Gaussian random designs through leveraging the statistical aspect of the AMP
theory. In slightly more detail  the probability distribution of the SLOPE solution is completely
speciﬁed by a few parameters that are the solution to a certain ﬁxed-point equation in an asymptotic
sense. This provides a powerful tool for ﬁne-grained statistical analysis of SLOPE as it was for the
LASSO problem. We note that a recent paper [21]—which takes an entirely different path—gives
an asymptotic characterization of the SLOPE solution that matches our asymptotic analysis that is
deduced from our AMP theory for SLOPE. However  our AMP-based approach is more algorithmic
in nature and offers a more concrete connection between the ﬁnite-sample behaviors of the SLOPE
problem and its asymptotic distribution via the computationally efﬁcient AMP algorithm.

2 Algorithmic Development

In this section we develop an AMP algorithm for ﬁnding the SLOPE estimator in (1.2). Recall the
AMP algorithm we study is (1.3). Speciﬁcally  it is through the threshold values θt that one can ensure
the AMP estimates converge to the SLOPE estimator with parameter λ. In this section we present
how one should calibrate the thresholds of the AMP iterations in (1.3) in order for the algorithm to
solve SLOPE cost in (1.2). Then in Section 3  we prove rigorously that the AMP algorithm solves the
SLOPE optimization asymptotically and we leverage theoretical guarantees for the AMP algorithm to
exactly characterize the mean square error of the SLOPE estimator in the large system limit. This is
done by applying recent theoretical results for AMP algorithms that use a non-separable non-linearity
[8]  like the one in (1.3).
We ﬁrst note that the analysis we pursue in this work makes the following assumptions about the
linear model (1.1) and parameter vector in (A.1):

(A1) The measurement matrix X has independent and identically-distributed (i.i.d.) Gaussian

entries that have mean 0 and variance 1/n.

(A2) The signal β has elements that are i.i.d. B  with E(B2 max{0  log B}) < ∞.
(A3) The noise w is elementwise i.i.d. W   with σ2
(A4) The vector λ(p) = (λ1  . . .   λp) is elementwise i.i.d. Λ  with E(Λ2) < ∞.
(A5) The ratio n/p approaches a constant δ ∈ (0 ∞) in the large system limit  as n  p → ∞.

w := E(W 2) < ∞.

Remark: (A4) can be relaxed as λ1  . . .   λp having an empirical distribution that converges weakly
to probability measure Λ on R with E(Λ2) < ∞ and (cid:107)λ(p)(cid:107)2/p → E(Λ2). A similar relaxation can
be made for assumptions (A2) and (A3).

2.1 SLOPE Preliminaries
For a vector v ∈ Rp  the divergence of the proximal operator  ∇ proxf (v)  is given by the following:

(cid:16) ∂

 

∂
∂v2

  . . .  

∂
∂vp

∂v1

(cid:17) · proxf (v) 

(2.1)

∇ proxf (v) :=

p(cid:88)

i=1

∂
∂vi

[proxf (v)]i =

3

where as given in [32]  proof of Fact 3.4 

(cid:40)

∂[proxJλ
∂vj

(v)]i

=

sign([proxJλ

#{1 ≤ k ≤ p : |[proxJλ
0 

(v)]i)·sign([proxJλ

(v)]k| = |[proxJλ

(v)]j )

(v)]j|}  

if |[proxJλ
otherwise.

(v)]j| = |[proxJλ

(v)]i| 

(2.2)

(2.3)

Hence the divergence is simpliﬁed to

∇ proxJλ

(v) = (cid:107) proxJλ

(v)(cid:107)∗
0 

0 counts the unique non-zero magnitudes in a vector  e.g. (cid:107)(0  1 −2  0  2)(cid:107)∗

where (cid:107) · (cid:107)∗
0 = 2. This
explicit form of divergence not only waives the need to use approximation in calculation but also
speed up the recursion  since it only depends on the proximal operator as a whole instead of on
θt−1  X  zt−1  βt−1. Therefore  we have
Lemma 2.1. In AMP  (1.3b) is equivalent to

zt+1 = y − Xβt+1 +

(cid:107)βt+1(cid:107)∗
0.

zt
δp

Other preliminary ideas and background on SLOPE and the prox operator are found in Appendix A.

2.2 AMP Background

(·)  is usually referred to as a ‘denoiser’ in the AMP literature.

An attractive feature of AMP is that its statistical properties can be exactly characterized at each
iteration t  at least asymptotically  via a one-dimensional recursion known as state evolution [2  8].
Speciﬁcally  it can be shown that the pseudo-data  meaning the input X(cid:62)zt + βt for the estimate of
the unknown signal in (1.3a)  is asymptotically equal in distribution to the true signal plus independent 
Gaussian noise  i.e. β + τtZ  where the noise variance τt is deﬁned by the state evolution. For
this reason  the function used to update the estimate in (1.3a)  in our case  the proximal operator 
proxJθt
This statistical characterization of the pseudo-data was ﬁrst rigorously shown to be true in the case of
‘separable’ denoisers by Bayati and Montanari [2]  and an analysis of the rate of this convergence was
given in [31]. A ‘separable’ denoiser is one that applies the same (possibly non-linear) function to each
element of its input. Recent work  which we make use of in this paper  proves that asymptotically the
pseudo-data has distribution β + τtZ when non-separable ‘denoisers’ are used in the AMP algorithm.
The dynamics of the AMP iterations are tracked by a recursive sequence referred to as the state
evolution  deﬁned below. For B elementwise i.i.d. B independent of Z ∼ N (0  Ip)  let τ 2
0 =
w + E[B2]/δ and for t ≥ 0 
σ2

τ 2
t+1 = σ2

w + lim
p

1
δp

E(cid:107)proxJθt

(B + τtZ) − B(cid:107)2.

(2.4)

Below we make rigorous the way that the recursion in (2.4) relates to the AMP iteration (1.3a)-(1.3b).
We note that throughout  we let N (µ  σ2) denote the Gaussian density with mean µ and variance σ2
and we use Ip to indicate a p × p identity matrix.

2.3 Analysis of the AMP State Evolution

As mentioned previously  it is through the sequence of thresholds θt that one is able to relate the AMP
algorithm to the SLOPE estimator in (1.2) for certain λ. Speciﬁcally  we will choose θt = ατt(p)
for every iteration t where the vector α is ﬁxed via a calibration made explicit below and τ 2
t (p) is
deﬁned using an approximation to the state evolution in (2.4) given in (2.5) below. We can interpret
this to mean that within the AMP algorithm  α plays the role of the regularizer λ.
The calibration is motivated by a careful analysis of the following approximation (when p is large) to
the state evolution iteration in (2.4). Namely 

τ 2
t+1(p) = σ2

w +

1
δp

E(cid:107)proxJατt(p)

(β + τt(p)Z) − β(cid:107)2 

(2.5)

4

where the difference between (2.5) and the state evolution (2.4) is via the large system limit in p.
When we refer to the recursion in (2.5) we will always specify the p dependence explicitly as τt(p).
Before we introduce this calibration  however  we give the following result which motivates why the
AMP iteration should relate at all to the SLOPE estimator.

Lemma 2.2. Any stationary point (cid:98)β (with corresponding(cid:98)z) in the AMP algorithm (1.3a)-(1.3b) with

θ∗ = ατ∗ is a minimizer of the SLOPE cost function in (1.2) with
1 − 1
n

(cid:16)∇ proxJθ∗ ((cid:98)β + X(cid:62)(cid:98)z)

1 − 1
δp

λ = θ∗

(cid:16)

(cid:17)

= θ∗

(cid:17)(cid:17)
(cid:16)
(cid:98)z = y − X(cid:98)β + (cid:98)z

(cid:13)(cid:13)(cid:13)∗
(cid:13)(cid:13)(cid:13)proxJθ∗ ((cid:98)β + X(cid:62)(cid:98)z)
(∇ proxθ∗ ((cid:98)β + X(cid:62)(cid:98)z)).

0

.

(2.6)

Proof of Lemma 2.2. By stationarity 

(cid:98)β = proxθ∗ ((cid:98)β + X(cid:62)(cid:98)z)

and

δp

Denote by ω := 1

δp (∇ proxθ∗ ((cid:98)β + X(cid:62)(cid:98)z)). Then  from (2.6) (cid:98)z = y−X(cid:98)β

Fact A.1  X(cid:62)(cid:98)z ∈ ∂Jθ∗ ((cid:98)β). Clearly  X(cid:62)(cid:98)z = X(cid:62)(y−X(cid:98)β)
∈ Jθ∗ ((cid:98)β)  which tells us X(cid:62)(y−X(cid:98)β) ∈
Jθ∗(1−ω)((cid:98)β) which is exactly the stationary condition of SLOPE with λ = (1− ω)θ∗ as desired.
(cid:111)

Results about the recursion (2.5) are summarized in the following theorem and the theorem’s proof is
given in Appendix C. We ﬁrst introduce some useful notations: let Amin(δ) be the set of solutions to

1−ω   and by (2.6) along with

(cid:17)

1−ω

p(cid:88)

E(cid:110)(cid:16)
1 −(cid:12)(cid:12)[proxJα

δ = f (α)  where f (α) :=

αj

/[D(proxJα

(Z))]i

(Z)]i

(2.7)

(cid:12)(cid:12)(cid:88)

j∈Ii

1
p

i=1

(cid:104)u(cid:105) :=(cid:80)m

Here (cid:12) represents elementwise multiplication of vectors and for a vector v ∈ Rp  D is deﬁned
elementwise as [D(v)]i = #{j : |vj| = |vi|} if vi (cid:54)= 0 and ∞ otherwise. For u ∈ Rm  the notation
i=1 ui/m and we say a vector u is larger than v if ∀i  ui > vi. The expectation in (2.7) is

taken with respect to Z  a p-length vector of i.i.d. standard Gaussians.
Theorem 1. For any α strictly larger than at least one element in the set Amin(δ)  the recursion in
(2.5) has a unique ﬁxed point and denoting this ﬁxed point by τ 2∗ (p). Then τt(p) → τ∗(p) for any
initial condition and monotonically. Moreover  deﬁning a function F : R × Rp → R as

F(τ 2(p)  ατ (p)) := σ2 +

E(cid:107)proxJατ (p)

1
δp

(B + τ (p)Z) − B(cid:107)2 

(2.8)

where B is elementwise i.i.d. B independent of Z ∼ N (0  Ip)  so that τ 2
then | ∂F
f (α) = δ limτ (p)→∞ dF/dτ 2(p).

t (p)  ατt(p)) 
∂τ 2(p) (τ 2(p)  ατ (p))|< 1 at τ (p) = τ∗(p). Moreover  for f (α) deﬁned in (2.7)  we show that

t+1(p) = F(τ 2

Notice that Theorem 1 gives necessary conditions on the calibra-
tion vector α under which recursion in (2.5)  and equivalently  the
calibration given below are well-deﬁned.

2.4 Threshold Calibration

Motivated by Lemma 2.2 and Lemma B.1  we deﬁne a calibration
from the regularization parameter λ  to the corresponding threshold
α used to deﬁne the AMP algorithm. Such calibration is asymptot-
ically exact when p = ∞.
In practice  we will be given ﬁnite-length λ and then we want to
design the AMP iteration to solve the corresponding SLOPE cost.
We do this by choosing α as the vector that solves λ = λ(α) where

(cid:16)

λ(α) := ατ∗(p)

(cid:17)

 
(2.9)

1 − 1
n

E(cid:107) proxJατ∗ (p)

(B + τ∗(p)Z)(cid:107)∗

0

5

Figure 2: Amin (black curve)
when p = 2 and δ = 0.6.

0.00.20.40.60.81.00.00.20.40.60.81.0a2a1FeasibleInfeasible(B + τ∗(p)Z)(cid:107)∗

where B is elementwise i.i.d. B independent of Z ∼ N (0  Ip) and τ∗(p) is the limiting value
deﬁned in Theorem 1. We note the fact that the calibration in (2.9) sets α as a vector in the same
direction as λ  but that is scaled by a constant value (for each p)  where the constant value is given by
τ∗(p)(1 − E(cid:107) proxJατ∗ (p)
We claim that the calibration (2.9) and its inverse λ (cid:55)→ α(λ) are well-deﬁned. In [3  Proposition 1.4
(ﬁrst introduced in [18]) and Corollary 1.7] this is proved rigorously for the LASSO calibration and
we claim that this proof can be adapted to the present case without many difﬁculties  though we don’t
pursue this in the current document.
Proposition 2.3. The function α (cid:55)→ λ(α) deﬁned in (2.9) is continuous on {α : f (α) < δ} for
f (·) deﬁned in (2.7) with λ(Amin) = −∞ and limα→∞ λ(α) = ∞ (where the limit is taken
elementwise). Therefore the inverse function λ (cid:55)→ α(λ) exists and is continuous non-decreasing for
any λ > 0.

0)/n.

This proposition motivates Algorithm 1 which uses bisection method to ﬁnd the unique α for each λ.
It sufﬁces to ﬁnd two guesses of α parallel to λ that  when mapped via (2.9)  sandwich the true λ.
The proof of this proposition can be found in [13  Appendix A.2].
Algorithm 1 Calibration from λ → α

1. Initialize α1 = αmin such that αmin(cid:96) ∈ Amin  where (cid:96) := λ/λ1; Initialize α2 = 2α1
while L(α2) < 0 where L : R → R; α (cid:55)→ sign(λ(α(cid:96)) − λ) do
end while
3. return BISECTION (L(α)  α1  α2)

2. Set α1 = α2  α2 = 2α2

Remark: sign(λ(·) − λ) ∈ R is well-deﬁned since λ(·) (cid:107) λ implies all entries share the same sign.
The function “BISECTION(L  a  b)” ﬁnds the root of L in [a  b] via the bisection method.

(cid:17)

(cid:16)

As noted previously  the calibration in (2.9) is exact when p → ∞  so we study the mapping between
α and λ in this limit. Recall from (A4)  that the sequence of vectors {λ(p)}p≥0 are drawn i.i.d.
from distribution Λ. It follows that the sequence {α(p)}p≥0 deﬁned for each p by the ﬁnite-sample
calibration (2.9) are i.i.d. from a distribution A  where A satisﬁes E(A2) < ∞  and is deﬁned via

p

 

0

1
δp

1 − lim

E|| proxJA(p)τ∗ (B + τ∗Z)||∗

Λ = Aτ∗

(2.10)
where A(p) ∈ Rp are order statistics of p i.i.d. draws from A given by (2.10) and τ∗ is deﬁned as the
large t limit of (2.4). We note that the calibrations presented in this section are well-deﬁned:
Fact 2.4. The limits in (2.4) and (2.10) exist.
This fact is proven in Appendix E. One idea used in the proof is that the prox operator is asymptotically
separable  a result shown by [21  Proposition 1]. Speciﬁcally  for sequences of input  {v(p)}  and
thresholds  {λ(p)}  both having empirical distributions that weakly converge to a distributions V and
Λ  respectively  then there exists a limiting scalar function h(·) := h(v(p); V  Λ) (determined by V
and Λ) of the proximal operator proxJλ
(v(p)). Further details are shown in Appendix E  Lemma E.1.
Using h(·) := h(·; B + τ∗Z  Aτ∗)  this argument implies that (2.4) can be represented as

and if we denote m as the Lebesgue measure  then the limit in (2.10) can be represented as

P(cid:16)

B + τ∗Z ∈(cid:110)

τ 2∗ := σ2 + E(h(B + τ∗Z) − B)2/δ 

(cid:12)(cid:12)(cid:12) h(x) (cid:54)= 0 and m{z | |h(z)| = |h(x)|} = 0
(cid:111)(cid:17)

x

.

In other words  the limit in (2.10) is the Lebesgue measure of the domain of the quantile function of
h for which the quantile of h assumes unique values (i.e.  is not ﬂat).

3 Asymptotic Characterization of SLOPE

3.1 AMP Recovers the SLOPE Estimate

Here we show that the AMP algorithm converges in (cid:96)2 to the SLOPE estimator  implying that the
AMP iterates can be used as a surrogate for the global optimum of the SLOPE cost function. The

6

schema of the proof is similar to [3  Lemma 3.1]  however  major differences lie in the fact that the
proximal operator used in the AMP updates (1.3a)-(1.3b) is non-separable. We sketch the proof here 
and a forthcoming article will be devoted to giving a complete and detailed argument.
Theorem 2. Under assumptions (A1) - (A5)  for the output of the AMP algorithm in (1.3a) and the
SLOPE estimate (1.2) 

lim
t→∞ ct = 0.

(3.1)

(cid:107)(cid:98)β − βt(cid:107)2 = ct  where

plim
p→∞

1
p

Proof. The proof requires dealing carefully with the fact that the SLOPE cost function given in (1.2)
is not necessarily strongly convex  meaning that we could encounter the undesirable situation where

C((cid:98)β) is close to C(β) but (cid:98)β is not close to β  meaning the statistical recovery of β would be poor.

In the LASSO case  one works around this challenge by showing that the (LASSO) cost function
does have nice properties when considering just the elements of the non-zero support of βt at any
(large) iteration t. In the LASSO case  the non-zero support of β has size no larger than n < p.
In the SLOPE problem  however  it is possible that the support set has size exceeding n  and therefore
the LASSO analysis is not immediately applicable. Our proof develops novel techniques that are
tailored to the characteristics of the SLOPE solution. Speciﬁcally  when considering the SLOPE
problem  one can show nice properties (similar to those in the LASSO case) by considering a support-
like set  that being the unique non-zeros in the estimate βt at any (large) iteration t. In other words  if
we deﬁne an equivalence relation x ∼ y when |x| = |y|  then entries of AMP estimate at any iteration
t are partitioned into equivalence classes. Then we observe from (2.9)  and the non-negativity of λ 
that the number of equivalence classes is no larger than n. We see an analogy between SLOPE’s
equivalence class (or ‘maximal atom’ as described in Appendix A) and LASSO’s support set. This
approach allows us to deal with the lack of a strongly convex cost.

3.2 Exact Asymptotic Characterization of the SLOPE Estimate

Theorem 2 ensures that the AMP algorithm solves the SLOPE problem in an asymptotic sense. To
better appreciate the convergence guarantee  it calls for elaboration on (3.1). First  it implies that

(cid:107)(cid:98)β − βt(cid:107)2/p converges in probability to a constant  say ct. Next  (3.1) says that ct → 0 as t → ∞.
A consequence of Theorem B.1  is that the SLOPE estimator (cid:98)β inherits performance guarantees
asymptotic characterization of pseudo-Lipschitz loss between (cid:98)β and the truth β.
d)k−1(cid:17)(cid:16)(cid:107)a − b(cid:107)/

Deﬁnition 3.1. Uniformly pseudo-Lipschitz functions [8]: For k ∈ N>0  a function φ : Rd → R
is pseudo-Lipschitz of order k if there exists a constant L  such that for a  b ∈ Rd 
√

provided by the AMP state evolution  in the sense of Theorem 3 below. Theorem 3 provides as

(3.2)
A sequence (in p) of pseudo-Lipschitz functions {φp}p∈N>0 is uniformly pseudo-Lipschitz of order k if 
denoting by Lp the pseudo-Lipschitz constant of φp  Lp < ∞ for each p and lim supp→∞ Lp < ∞.
Theorem 3. Under assumptions (A1) - (A5)  for any uniformly pseudo-Lipschitz sequence of func-
tions ψp : Rp × Rp → R and for Z ∼ N (0  Ip) 
E
[ψp(proxJα(p)τt
Z

ψp((cid:98)β  β) = lim

(cid:107)φ(a) − φ(b)(cid:107) ≤ L

√

√
d)k−1 + ((cid:107)b(cid:107)/

1 + ((cid:107)a(cid:107)/

(β + τtZ)  β)] 

(cid:16)

(cid:17)

d

.

plim

p

plim

p

t

where τt is deﬁned in (2.4) and the expectation is taken with respect to Z.

Theorem 3 tells us that under uniformly pseudo-Lipschitz loss  in the large system limit  distri-
butionally the SLOPE optimizer acts as a ‘denoised’ version of the truth corrupted by additive
Gaussian noise where the denoising function is given by the proximal operator  i.e. within uniformly

pseudo-Lipschitz loss(cid:98)β can be replaced with proxJα(p)τt

(β + τtZ) for large p  t.

We note that the result [21  Theorem 1] follows by Theorem 3 and their separability result [21 
Proposition 1]. To see this  in Theorem 3 consider a special case where ψp(x  y) = 1
p

(cid:80) ψ(xi  yi)

7

for function ψ : R × R → R that is pseudo-Lipschitz of order k = 2. Then it is easy to show that
ψp(· ·) is uniformly pseudo-Lipschitz of order k = 2. The result of Theorem 3 then says that

p(cid:88)

i=1

ψ((cid:98)βi  βi) = lim

plim

p

1
p

t

p(cid:88)

i=1

E
[ψ([proxJα(p)τt
Z

(β + τtZ)]i  βi)].

plim

p

1
p

p(cid:88)

Then by [21  Proposition 1]  restated in Lemma E.1  which says that the proximal operator becomes
asymptotically separable as p → ∞  the result of [21  Theorem 1] follows by the Law of Large
Numbers and Theorem 1. Namely  for some limiting scalar function ht 

p(cid:88)

i=1

1
p

E
[ψ(ht([β + τtZ]i)  βi)]
Z

lim

t

plim

p

= lim

t

E
Z B

E
[ψ([proxJα(p)τt
Z

1
p
[ψ(ht(B + τtZ)  B)] = E

i=1

Z B

(β + τtZ)]i  βi)]

(a)
= lim

plim

t

p
[ψ(ht(B + τ∗Z)  B)].

We note in step (a) above  we apply Lemma E.1  using that α(p)τt has an empirical distribution that
converges weakly to Aτt for A deﬁned by (2.10). The rigorous argument for justifying step (a) by
Lemma E.1 requires a bit more technical detail. We give such a rigorous argument  for a similar
but different limiting operation  in Appendix D for proving limiting properties of the prox operator
(namely  property (P2) stated in Appendix B).
We highlight that our Theorem 3 allows the consideration of a non-asymptotic case in t. While
Theorem 1 motivates an algorithmic way to ﬁnd a value τt(p) which approximates τ∗(p) well 
Theorem 3 guarantees the accuracy of such approximation for use in practice. One particular use
of Theorem 3 is to design the optimal sequence λ that achieves the minimum τ∗ and equivalently
minimum error [21]  though a concrete algorithm for doing so is still under investigation.
We prove Theorem 3 in Appendix B. We show that Theorem 3 follows from Theorem 2 and Lemma
B.1  which demonstrates that the state evolution given in (2.4) characterizes the performance of the
SLOPE AMP (1.3) via pseudo-Lipschitz loss functions. Finally we show how we use Theorem 3 to
study the asymptotic mean-square error between the SLOPE estimator and the truth.
w).

Corollary 3.2. Under assumptions (A1) − (A5)  plimp(cid:107)(cid:98)β − β(cid:107)2/p = δ(τ 2∗ − σ2
Proof. Applying Theorem 3 to the pseudo-Lipschitz loss function ψ1 : Rp × Rp → R  deﬁned as
ψ1(x  y) = ||x−y||2/p  we ﬁnd plimp
(β +τtZ)−β(cid:107)2].
EZ[(cid:107)proxJατt
The desired result follows since limt plimp
w). To
see this  note that limt δ(τ 2

(β + τtZ) − β(cid:107)2] = δ(τ 2∗ − σ2

p(cid:107)(cid:98)β−β(cid:107)2 = limt plimp
EZ[(cid:107)proxJατt
w) = δ(τ 2∗ − σ2
w) and
E
Z B

t+1 − σ2

1
p

[(cid:107)proxJατt
E
Z

(β + τtZ)− β(cid:107)2] = lim

t+1 − σ2
plim
w) 
for B elementwise i.i.d. B independent of Z ∼ N (0  Ip). A rigorous argument for the above follows
similarly to that used to prove property (P2) stated in Appendix B and proved in Appendix D.

(B + τtZ)− B(cid:107)2] = δ(τ 2

[(cid:107)proxJατt

1
p

1
p

1
p

p

p

1

4 Discussion and Future Work

This work develops and analyzes the dynamics of an approximate message passing (AMP) algorithm
with the purpose of solving the SLOPE convex optimization procedure for high-dimensional linear
regression. By employing recent theoretical analysis of AMP when the non-linearities used in the
algorithm are non-separable [8]  as is the case for the SLOPE problem  we provide a rigorous proof
that the proposed AMP algorithm ﬁnds the SLOPE solution asymptotically. Moreover empirical
evidence suggests that the AMP estimate is already very close to the SLOPE solution even in few
iterations. By leveraging our analysis showing that AMP provably solves SLOPE  we provide an
exact asymptotic characterization of the (cid:96)2 risk of the SLOPE estimator from the underlying truth and
insight into other statistical properties of the SLOPE estimator. Though this asymptotic analysis of
the SLOPE solution has been demonstrated in other recent work [21] using a different proof strategy 
we have a clear  rigorous statement of where it applies. That is  the analysis in [21] applies if the state
evolution has a unique ﬁxed point  whereas our Theorem 1 states precise conditions under which
this is true. Moreover  we believe that our algorithmic approach offers a more concrete connection
between the ﬁnite-sample behavior of the SLOPE estimator and its asymptotic distribution.
We now brieﬂy discuss some potential

improvements and directions for future research.

8

(a) i.i.d. ±1 Bernoulli design ma-
trix (top) and i.i.d. shifted expo-
nential design matrix (bottom)

(b) i.i.d. Gaussian design ma-
trix (top) and non-i.i.d.
right
rotationally-invariant design ma-
trix where AMP diverges (bot-
tom)

i.i.d. Gaussian measurement matrix assumption. A lim-
itation of vanilla AMP is that the theory assumes an i.i.d.
Gaussian measurement matrix  and moreover  the AMP al-
gorithm can become unstable when the measurement matrix
is far from i.i.d.  creating the need for heuristic techniques to
provide convergence in applications where the measurement
matrix is generated by nature (i.e.  a real-world experiment
or observational study). While  in general  AMP theory pro-
vides performance guarantees only for i.i.d. sub-Gaussian
data [2  5]  in practice  favorable performance of AMP seems
to be more universal. For example  in Fig. 3a  we illustrate
the performance of AMP for i.i.d. zero mean  1/n variance
design matrices that are not Gaussian (one i.i.d. ±1 Bernoulli
(top) and one i.i.d. shifted exponential (bottom)). In particular 
we note that the exponential prior is not sub-Gaussian  so the
performance here is not supported by theory. In both cases 
AMP converges very fast  thus demonstrating its robustness
to distributional assumptions.
On the theoretical side  recent work proposes a variant
of AMP  called vector-AMP or VAMP [28]  which is a
computationally-efﬁcient algorithm that provably works for
a wide range of design matrices  namely  those that are right
rotationally-invariant. For example  [23] studies VAMP for
a similar setting as SLOPE. However  the type of nonsep-
arability considered in this work requires the penalty to be
separable on subsets of an afﬁne transformation of its in-
put. As such  the setting does not directly apply to SLOPE.
To address this  we have built a hybrid  ‘SLOPE VAMP’ 
based on code generously shared by the authors of the refer-
enced work [23]  which performs very well in the (non-) i.i.d.
(non-) Gaussian regime (see Fig. 3a and 3b). Motivated by
these promising empirical results  we feel that theoretically
understanding SLOPE dynamics with VAMP is an exciting
direction for future work.
Known signal prior assumption. There is a possibility that 
by using EM- or SURE-based AMP strategies  one can re-
move the known signal prior assumption. Developing such
strategies alongside our SLOPE VAMP would provide a quite
general framework for recovery of the SLOPE estimator.
Comparison to ‘Bayes-AMP’. In general  the (statistical)
motivation for using methods like LASSO or SLOPE is to
perform variable selection  and in addition  for SLOPE  to
control the false discovery rate. Both methods are therefore
biased and  consequently  ‘Bayes-AMP’ strategies that are
designed to be optimal in terms of MSE will outperform if performance is based on MSE. In par-
ticular  [14] proves that ‘Bayes-AMP’ always has smaller MSE than that of methods employing
convex regularization for a wide class of convex penalties and Gaussian design. Nevertheless  Fig. 3c
suggests that SLOPE AMP has MSE that is not too much worse than MMSE AMP.
Sampling regime. The asymptotical regime studied here  n/p → δ ∈ (0 ∞)  requires that the
number of columns of the measurement matrix p grow at the same rate as the number of rows n. It is
of practical interest to extend the results to high-dimensional settings where p grows faster than n.

(c) i.i.d. Gaussian design matrix
Figure 3: Performance of AMP vari-
ants in different settings with Bernoulli-
Gaussian prior  dimension = 1000  and
sample size = 300.

9

0.000.020.040.06100100.5101101.5102Optimization ErrorAMPFISTAISTAVAMP0.000.020.040.06100100.5101101.5102Optimization ErrorAMPFISTAISTAVAMP0.0000.0050.010100100.5101101.5102Optimization ErrorAMPFISTAISTAVAMP0.000.020.040.06100100.5101101.5102Optimization ErrorFISTAISTAVAMP0.400.420.440.46100100.5101101.5102102.5103MSE for true signalFISTAISTALASSO AMPMMSE AMPSLOPE AMP0.4000.4250.4500255075100MSE for true signalLASSO AMP SEMMSE AMP SESLOPE AMP SEReferences
[1] R. F. Barber and E. J. Candès. Controlling the false discovery rate via knockoffs. The Annals of

Statistics  43(5):2055–2085  2015.

[2] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs  with applica-

tions to compressed sensing. IEEE Trans. on Inf. Theory  57(2):764–785  2011.

[3] M. Bayati and A. Montanari. The lasso risk for gaussian matrices. IEEE Transactions on

Information Theory  58(4):1997–2017  2011.

[4] M. Bayati  M. A. Erdogdu  and A. Montanari. Estimating lasso risk and noise level. In Advances

in Neural Information Processing Systems  pages 944–952  2013.

[5] M. Bayati  M. Lelarge  A. Montanari  et al. Universality in polytope phase transitions and

message passing algorithms. The Annals of Applied Probability  25(2):753–822  2015.

[6] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences  2(1):183–202  2009.

[7] P. C. Bellec  G. Lecué  and A. B. Tsybakov. SLOPE meets lasso: improved oracle bounds and

optimality. The Annals of Statistics  46(6B):3603–3642  2018.

[8] R. Berthier  A. Montanari  and P.-M. Nguyen. State evolution for approximate message passing

with non-separable functions. arXiv preprint arXiv:1708.03950  2017.

[9] M. Bogdan  E. Van Den Berg  C. Sabatti  W. Su  and E. J. Candès. SLOPE—adaptive variable

selection via convex optimization. The Annals of Applied Statistics  9(3):1103  2015.

[10] H. D. Bondell and B. J. Reich. Simultaneous regression shrinkage  variable selection  and

supervised clustering of predictors with oscar. Biometrics  64(1):115–123  2008.

[11] M. Borgerding and P. Schniter. Onsager-corrected deep learning for sparse linear inverse
problems. In 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP) 
pages 227–231  2016.

[12] D. Brzyski  A. Gossmann  W. Su  and M. Bogdan. Group SLOPE—adaptive selection of groups

of predictors. Journal of the American Statistical Association  pages 1–15  2018.

[13] Z. Bu  J. Klusowski  C. Rush  and W. Su. Algorithmic analysis and statistical estimation of

slope via approximate message passing. arXiv preprint arXiv:1907.07502  2019.

[14] M. Celentano and A. Montanari. Fundamental barriers to high-dimensional regression with

convex penalties. arXiv preprint arXiv:1903.10603  2019.

[15] A. Chambolle  R. A. De Vore  N.-Y. Lee  and B. J. Lucier. Nonlinear wavelet image process-
ing: variational problems  compression  and noise removal through wavelet shrinkage. IEEE
Transactions on Image Processing  7(3):319–335  1998.

[16] I. Daubechies  M. Defrise  and C. De Mol. An iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Communications on Pure and Applied Mathematics: A
Journal Issued by the Courant Institute of Mathematical Sciences  57(11):1413–1457  2004.

[17] D. L. Donoho  A. Maleki  and A. Montanari. Message-passing algorithms for compressed

sensing. Proceedings of the National Academy of Sciences  106(45):18914–18919  2009.

[18] D. L. Donoho  A. Maleki  and A. Montanari. The noise-sensitivity phase transition in com-

pressed sensing. IEEE Transactions on Information Theory  57(10):6920–6941  2011.

[19] J. L. Doob. Stochastic processes  volume 101. New York Wiley  1953.

[20] M. Figueiredo and R. Nowak. Ordered weighted l1 regularized regression with strongly
correlated covariates: Theoretical aspects. In Artiﬁcial Intelligence and Statistics  pages 930–
938  2016.

10

[21] H. Hu and Y. M. Lu. Asymptotics and optimal designs of SLOPE for sparse linear regression.

arXiv preprint arXiv:1903.11582  2019.

[22] F. Krzakala  M. Mézard  F. Sausset  Y. Sun  and L. Zdeborová. Probabilistic reconstruction
in compressed sensing: algorithms  phase diagrams  and threshold achieving matrices. J. Stat.
Mech. Theory Exp.  (8)  2012.

[23] A. Manoel  F. Krzakala  G. Varoquaux  B. Thirion  and L. Zdeborová. Approximate message-
passing for convex optimization with non-separable penalties. arXiv preprint arXiv:1809.06304 
2018.

[24] A. Montanari. Graphical models concepts in compressed sensing. In Y. C. Eldar and G. Kutyniok 
editors  Compressed Sensing  pages 394–438. Cambridge University Press  2012. URL http:
//dx.doi.org/10.1017/CBO9780511794308.010.

[25] A. Mousavi  A. Maleki  R. G. Baraniuk  et al. Consistent parameter estimation for lasso and

approximate message passing. The Annals of Statistics  46(1):119–148  2018.

[26] N. Parikh  S. Boyd  et al. Proximal algorithms. Foundations and Trends R(cid:13) in Optimization  1

(3):127–239  2014.

[27] S. Rangan. Generalized approximate message passing for estimation with random linear mixing.

In Proc. IEEE Int. Symp. Inf. Theory  pages 2168–2172  2011.

[28] S. Rangan  P. Schniter  and A. K. Fletcher. Vector approximate message passing.

Transactions on Information Theory  2019.

IEEE

[29] R. T. Rockafellar and R. J.-B. Wets. Variational analysis  volume 317. Springer Science &

Business Media  2009.

[30] H. L. Royden. Real analysis. Krishna Prakashan Media  1968.

[31] C. Rush and R. Venkataramanan. Finite sample analysis of approximate message passing

algorithms. IEEE Trans. on Inf. Theory  64(11):7264–7286  2018.

[32] W. Su and E. Candès. SLOPE is adaptive to unknown sparsity and asymptotically minimax.

The Annals of Statistics  44(3):1038–1068  2016.

[33] W. Su  M. Bogdan  and E. Candès. False discoveries occur early on the lasso path. The Annals

of Statistics  45(5):2133–2150  2017.

[34] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B (Statistical Methodology)  58(1):267–288  1996.

[35] X. Zeng and M. A. Figueiredo. Decreasing weighted sorted (cid:96)1 regularization. IEEE Signal

Processing Letters  21(10):1240–1244  2014.

11

,Zhiqi Bu
Jason Klusowski
Cynthia Rush
Weijie Su