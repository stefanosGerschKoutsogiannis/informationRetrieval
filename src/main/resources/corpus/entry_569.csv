2019,Approximate Inference Turns Deep Networks into Gaussian Processes,Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them  but the relationship between their training methods is not well understood. In this paper  we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result  we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly  the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.,Approximate Inference Turns Deep Networks into

Gaussian Processes

Mohammad Emtiyaz Khan
RIKEN Center for AI Project

Tokyo  Japan

emtiyaz.khan@riken.jp

Ehsan Abedi* †

EPFL

Lausanne  Switzerland
ehsan.abedi@epfl.ch

Alexander Immer* †

EPFL

Lausanne  Switzerland

alexander.immer@epfl.ch

Maciej Korzepa* †

Technical University of Denmark

Kgs. Lyngby  Denmark

mjko@dtu.dk

Abstract

Deep neural networks (DNN) and Gaussian processes (GP) are two powerful
models with several theoretical connections relating them  but the relationship
between their training methods is not well understood. In this paper  we show that
certain Gaussian posterior approximations for Bayesian DNNs are equivalent to
GP posteriors. This enables us to relate solutions and iterations of a deep-learning
algorithm to GP inference. As a result  we can obtain a GP kernel and a nonlinear
feature map while training a DNN. Surprisingly  the resulting kernel is the neural
tangent kernel. We show kernels obtained on real datasets and demonstrate the use
of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims
to facilitate further research on combining DNNs and GPs in practical settings.

1

Introduction

Deep neural networks (DNN) and Gaussian processes (GP) models are both powerful models with
complementary strengths and weaknesses. DNNs achieve state-of-the-art results on many real-world
problems providing scalable end-to-end learning  but they can overﬁt on small datasets and be
overconﬁdent. In contrast  GPs are suitable for small datasets and compute conﬁdence estimates 
but they are not scalable and choosing a good kernel in practice is challenging [3]. Combining their
strengths to solve real-world problems is an important problem.
Theoretically  the two models are closely related to each other. Previous work has shown that as the
width of a DNN increases to inﬁnity  the DNN converges to a GP [4  5  13  16  22]. This relationship
is surprising and gives us hope that a practical combination could be possible. Unfortunately  it is not
clear how one can use such connections in practice  e.g.  to perform fast inference in GPs by using
training methods of DNNs  or to reduce overﬁtting in DNNs by using GP inference. We argue that  to
solve such practical problems  we need the relationship not only between the models but also between
their training procedures. The purpose of this paper is to provide such a theoretical relationship.
We present theoretical results aimed at connecting the training methods of deep learning and GP
models. We show that the Gaussian posterior approximations for Bayesian DNNs  such as those
obtained by Laplace approximation and variational inference (VI)  are equivalent to posterior dis-
tributions of GP regression models. This result enables us to relate the solutions and iterations of a
deep-learning algorithm to GP inference. See Fig. 1 for our approach called DNN2GP. In addition 

†Equal contribution. *This work is performed during an internship at the RIKEN Center for AI project.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: A summary of our approach called DNN2GP in three steps.

(c) GP kernel

(a) 2D classiﬁcation problem

(b) GP kernel feature φ(x)

(d) GP posterior mean

Figure 2: Fig. (a) shows a 2D binary-classiﬁcation problem along with the predictive distribution
of a DNN using 513 parameters. The corresponding feature and kernel matrices obtained using
our approach are shown in (b) and (c)  respectively (the two classes are grouped  and marked with
blue and orange color along the axes). Fig. (d) shows the GP posterior mean where we see a clear
separation between the two classes. Surprisingly  the border points A and D in (a) are also at the
boundary in (d).

we can obtain GP kernels and nonlinear feature maps while training a DNN (see Fig. 2). Surprisingly 
a GP kernel we derive is equivalent to the recently proposed neural tangent kernel (NTK) [8].We
present empirical results where we visualize the feature-map obtained on benchmark datasets such
as MNIST and CIFAR  and demonstrate their use for DNN hyperparameter tuning. The code to
reproduce our results is available at https://github.com/team-approx-bayes/dnn2gp. The
work presented in this paper aims to facilitate further research on combining the strengths of DNNs
and GPs in practical settings.

1.1 Related Work

The equivalence between inﬁnitely-wide neural networks and GPs was originally discussed by Neal
[16]. Subsequently  many works derived explicit expressions for the GP kernel corresponding to
neural networks [4  7  16] and their deep variants [5  6  13  18]. These works use a prior distribution
on weights and derive kernels by averaging over the prior. Our work differs from these works in the
fact that we use the posterior approximations to relate DNNs to GPs. Unlike these previous results 
our results hold for DNNs of ﬁnite width.
A GP kernel we derive is equivalent to the recently proposed Neural Tangent Kernel (NTK) [8] 
which is obtained by using the Jacobian of the DNN outputs. For randomly initialized trajectories 
as the DNN width goes to inﬁnity  the NTK converges in probability to a deterministic kernel and
remains asymptotically constant when training with gradient descent. Jacot et al. [8] motivate the
NTK by using kernel gradient descent. Surprisingly  the NTK appears in our work with an entirely
different approach where we consider approximations of the posterior distribution over weights. Due

2

Posterior Approx.DNNGPLinear ModelStep A: Find a Gaussian posterior approximation given DNN weights
BStep C: Find a GP whose predictions are the same as those of the linear model.p(w|y X)⇡N(w|µ ⌃)<latexit sha1_base64="jzWgQqcBtg8KMGroPnGxBT6TCWg=">AAACSXicbZBLSwMxFIUzrc/6qroUJCiCgpQZXehKBDeupKLVQqfUO2mmBpOZkGTUMs4P0R/kxp07/4MbF4q4MtMqPg8EPs7NITcnkJxp47oPTqE4MDg0PDJaGhufmJwqT88c6ThRhNZIzGNVD0BTziJaM8xwWpeKggg4PQ7OdvL58TlVmsXRoelK2hTQiVjICBhrtconctkXYE6DML3Irj6xm61+Yj1bwT5IqeJL3PMI8HQv+y/li+Qr5x+wjoBspVVedCtuT/gveB+wuD1/neum2irf++2YJIJGhnDQuuG50jRTUIYRTrOSn2gqgZxBhzYsRiCobqa9JjK8ZJ02DmNlT2Rwz/2eSEFo3RWBvZnvqX/PcvO/WSMx4WYzZZFMDI1I/6Ew4djEOK8Vt5mixPCuBSCK2V0xOQUFxNjyS7YE7/eX/8LRWsVbr6zt2za2UF8jaA4toGXkoQ20jXZRFdUQQbfoET2jF+fOeXJenbf+1YLzkZlFP1QovgOD9Llf</latexit>ACStep B: Find a linear model with outputs   features   and noise   such that its posterior is equal to the Gaussian approximation  i.e. p(w|˜y X)=N(w|µ ⌃)<latexit sha1_base64="KbGIxqU/jNMkqqWLQ27TimUrlbw=">AAACTHicbVDLSgMxFM3Ud33Vx85NsAgtlDJTBd0IghtXUtHWQmcomTTThiYzQ5JRypgPdOPCnV/hxoUigum0iloPBM499x7uzfFjRqWy7ScrNzM7N7+wuJRfXlldWy9sbDZllAhMGjhikWj5SBJGQ9JQVDHSigVB3Gfk2h+cjvrXN0RIGoVXahgTj6NeSAOKkTJSp4DjksuR6vtBeqvvXEVZl6RfylDrCvwqWroMj8cVRiw917+ME+ryRFe+i0va40iXO4WiXbUzwGniTEjxZDvIUO8UHt1uhBNOQoUZkrLt2LHyUiQUxYzovJtIEiM8QD3SNjREnEgvzcLQcM8oXRhEwrxQwUz96UgRl3LIfTM5ulP+7Y3E/3rtRAVHXkrDOFEkxONFQcKgiuAoWdilgmDFhoYgLKi5FeI+Eggrk3/ehOD8/fI0adaqzn61dmHSOABjLIIdsAtKwAGH4AScgTpoAAzuwTN4BW/Wg/VivVsf49GcNfFsgV/IzX8C4kW5Aw==</latexit>˜y<latexit sha1_base64="yBfnuysYIUTLKB4HtG8+7nkJsCU=">AAAB8HicbVDLSsNAFL2prxofrbp0MyiCbkpSBd1ZEMFlBfuQNpTJZNIOnUzCzEQIoV/hxoUibv0F/8Kdn+BfOH0stPXAhcM593LvPX7CmdKO82UVlpZXVteK6/bG5tZ2qbyz21RxKgltkJjHsu1jRTkTtKGZ5rSdSIojn9OWP7wa+60HKhWLxZ3OEupFuC9YyAjWRrrvasYDmmejXvnQqTgToEXizshhrfR9eWJ/XNd75c9uEJM0okITjpXquE6ivRxLzQinI7ubKppgMsR92jFU4IgqL58cPEJHRglQGEtTQqOJ+nsix5FSWeSbzgjrgZr3xuJ/XifV4YWXM5GkmgoyXRSmHOkYjb9HAZOUaJ4Zgolk5lZEBlhiok1GtgnBnX95kTSrFfe0Ur01aZzBFEXYhwM4BhfOoQY3UIcGEIjgEZ7hxZLWk/VqvU1bC9ZsZg/+wHr/AdZGk08=</latexit>✏<latexit sha1_base64="OqCUuSm6qp/kB7ddxFJoLwEkLqE=">AAAB73icbZDLSgMxFIbPeK31VnXpJlgEV2WmCrqz4MZlBXuBtpRMeqYNzWTGJCOUoS/hRrwgbn0EX8Odb2Om7UJbfwh8/P855Jzjx4Jr47rfztLyyuraem4jv7m1vbNb2Nuv6yhRDGssEpFq+lSj4BJrhhuBzVghDX2BDX94leWNe1SaR/LWjGLshLQvecAZNdZqtjHWXESyWyi6JXcisgjeDIqXn0+Znqvdwle7F7EkRGmYoFq3PDc2nZQqw5nAcb6daIwpG9I+tixKGqLupJN5x+TYOj0SRMo+acjE/d2R0lDrUejbypCagZ7PMvO/rJWY4KKTchknBiWbfhQkgpiIZMuTHlfIjBhZoExxOythA6ooM/ZEeXsEb37lRaiXS95pqXzjFitnMFUODuEITsCDc6jANVShBgwEPMALvDp3zqPz5rxPS5ecWc8B/JHz8QNqhJSh</latexit>˜y=(x)>w+✏<latexit sha1_base64="37/ERAj/mg9GnMTVKBq2NB1FDzE=">AAACLXicbVBNaxsxENWmbeo6TeK0veUiYgouAbPrFNJLIdAecnQh/gCvE7TaWVtYuxLSbBqz7B/qpX+lFHJIKb32b1S7diFfD4Qeb2akeS/SUlj0/Rtv48nTZ5vPGy+aWy+3d3Zbe6+GVuWGw4Arqcw4YhakyGCAAiWMtQGWRhJG0eJTVR9dgrFCZWe41DBN2SwTieAMnXTR+hyikDEUy5J+pGGkZGyXqbuKUM9F2QlThvMoKa7Kd+chKk3/C19LekhD0FbI6pm23/Vr0IckWJP2yZukRv+i9TOMFc9TyJBLZu0k8DVOC2ZQcAllM8wtaMYXbAYTRzOWgp0WtduSvnVKTBNl3MmQ1urtiYKltjLhOqtl7f1aJT5Wm+SYfJgWItM5QsZXHyW5pKhoFR2NhQGOcukI40a4XSmfM8M4uoCbLoTgvuWHZNjrBkfd3heXxnuyQoPskwPSIQE5JifklPTJgHDyjfwgN+SX99279n57f1atG9565jW5A+/vP0kKrDs=</latexit>(x)<latexit sha1_base64="DasG2SWc++rUoz7fq2oSuore9Ak=">AAACCHicbVC7TsMwFHV4lvIKj40BiwqpLFVSkGCsxMJYJPqQmqhyHKe16tiR7SCqKCMLv8LCAEKsfAIbf4PTdoCWI1k+Ovde3XNPkDCqtON8W0vLK6tr66WN8ubW9s6uvbffViKVmLSwYEJ2A6QIo5y0NNWMdBNJUBww0glG10W9c0+kooLf6XFC/BgNOI0oRtpIffvYCwQL1Tg2X+YlQ5pXMy9GehhE2UOen/XtilNzJoCLxJ2RSuMwmqDZt7+8UOA0JlxjhpTquU6i/QxJTTEjedlLFUkQHqEB6RnKUUyUn00OyeGpUUIYCWke13Ci/p7IUKwKr6az8Kjma4X4X62X6ujKzyhPUk04ni6KUga1gEUqMKSSYM3GhiAsqfEK8RBJhLXJrmxCcOdPXiTtes09r9VvTRoXYIoSOAInoApccAka4AY0QQtg8AiewSt4s56sF+vd+pi2LlmzmQPwB9bnD52DnTo=</latexit>˜y<latexit sha1_base64="Q8kwkb7XSPHgft5bzES8V+RW20g=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXRbduKxgH9CEMplM2qGTSZi5EUvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YEqeAaHOfbWlvf2Nzaru3Ud/f2Dw7to0ZPJ5mirEsTkahBQDQTXLIucBBskCpG4kCwfjC9Lf3+I1OaJ/IBZinzYzKWPOKUgJFGdsOLCUyCKPeAi5Dls6IY2U2n5cyBV4lbkSaq0BnZX16Y0CxmEqggWg9dJwU/Jwo4Fayoe5lmKaFTMmZDQyWJmfbzefYCnxklxFGizJOA5+rvjZzEWs/iwEyWSfWyV4r/ecMMoms/5zLNgEm6OBRlAkOCyyJwyBWjIGaGEKq4yYrphChCwdRVNyW4y19eJb2Lluu03PvLZvumqqOGTtApOkcuukJtdIc6qIsoekLP6BW9WYX1Yr1bH4vRNavaOUZ/YH3+APIzlQQ=</latexit><latexit sha1_base64="Q8kwkb7XSPHgft5bzES8V+RW20g=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXRbduKxgH9CEMplM2qGTSZi5EUvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YEqeAaHOfbWlvf2Nzaru3Ud/f2Dw7to0ZPJ5mirEsTkahBQDQTXLIucBBskCpG4kCwfjC9Lf3+I1OaJ/IBZinzYzKWPOKUgJFGdsOLCUyCKPeAi5Dls6IY2U2n5cyBV4lbkSaq0BnZX16Y0CxmEqggWg9dJwU/Jwo4Fayoe5lmKaFTMmZDQyWJmfbzefYCnxklxFGizJOA5+rvjZzEWs/iwEyWSfWyV4r/ecMMoms/5zLNgEm6OBRlAkOCyyJwyBWjIGaGEKq4yYrphChCwdRVNyW4y19eJb2Lluu03PvLZvumqqOGTtApOkcuukJtdIc6qIsoekLP6BW9WYX1Yr1bH4vRNavaOUZ/YH3+APIzlQQ=</latexit><latexit sha1_base64="Q8kwkb7XSPHgft5bzES8V+RW20g=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXRbduKxgH9CEMplM2qGTSZi5EUvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YEqeAaHOfbWlvf2Nzaru3Ud/f2Dw7to0ZPJ5mirEsTkahBQDQTXLIucBBskCpG4kCwfjC9Lf3+I1OaJ/IBZinzYzKWPOKUgJFGdsOLCUyCKPeAi5Dls6IY2U2n5cyBV4lbkSaq0BnZX16Y0CxmEqggWg9dJwU/Jwo4Fayoe5lmKaFTMmZDQyWJmfbzefYCnxklxFGizJOA5+rvjZzEWs/iwEyWSfWyV4r/ecMMoms/5zLNgEm6OBRlAkOCyyJwyBWjIGaGEKq4yYrphChCwdRVNyW4y19eJb2Lluu03PvLZvumqqOGTtApOkcuukJtdIc6qIsoekLP6BW9WYX1Yr1bH4vRNavaOUZ/YH3+APIzlQQ=</latexit><latexit sha1_base64="Q8kwkb7XSPHgft5bzES8V+RW20g=">AAAB+3icbVDLSsNAFJ34rPUV69LNYBFclUQEXRbduKxgH9CEMplM2qGTSZi5EUvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YEqeAaHOfbWlvf2Nzaru3Ud/f2Dw7to0ZPJ5mirEsTkahBQDQTXLIucBBskCpG4kCwfjC9Lf3+I1OaJ/IBZinzYzKWPOKUgJFGdsOLCUyCKPeAi5Dls6IY2U2n5cyBV4lbkSaq0BnZX16Y0CxmEqggWg9dJwU/Jwo4Fayoe5lmKaFTMmZDQyWJmfbzefYCnxklxFGizJOA5+rvjZzEWs/iwEyWSfWyV4r/ecMMoms/5zLNgEm6OBRlAkOCyyJwyBWjIGaGEKq4yYrphChCwdRVNyW4y19eJb2Lluu03PvLZvumqqOGTtApOkcuukJtdIc6qIsoekLP6BW9WYX1Yr1bH4vRNavaOUZ/YH3+APIzlQQ=</latexit>w2<latexit sha1_base64="YBpbs9lJphDaXoUI3POfZtfYqK4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmopTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AypdkcA=</latexit><latexit sha1_base64="YBpbs9lJphDaXoUI3POfZtfYqK4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmopTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AypdkcA=</latexit><latexit sha1_base64="YBpbs9lJphDaXoUI3POfZtfYqK4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmopTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AypdkcA=</latexit><latexit sha1_base64="YBpbs9lJphDaXoUI3POfZtfYqK4=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmopTQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AypdkcA=</latexit>w1<latexit sha1_base64="Ok4f0JTHScObwvQNqmt5BOo/6BE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AyjZkb8=</latexit><latexit sha1_base64="Ok4f0JTHScObwvQNqmt5BOo/6BE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AyjZkb8=</latexit><latexit sha1_base64="Ok4f0JTHScObwvQNqmt5BOo/6BE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AyjZkb8=</latexit><latexit sha1_base64="Ok4f0JTHScObwvQNqmt5BOo/6BE=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AyjZkb8=</latexit>1(x)<latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="hP+6LrUf2d3tZaldqaQQvEKMXyw=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odBu3wMYA6nMMFXEEIN3AHD9CBLghI4BXevYn35n2suqp569LO4I+8zx84xIo4</latexit><latexit sha1_base64="WU379MaQInOjfur8A6jsboh9KFA=">AAAB/XicbVC9TsMwGPzCbykFAisDERVSWaqEBUYkFsYi0R+piSLHcVqrjh3ZDqKKMrLwKiwMIMRrsPE2OG0HaDnJ8unOn3zfRRmjSrvut7W2vrG5tV3bqe829vYP7MNGT4lcYtLFggk5iJAijHLS1VQzMsgkQWnESD+a3FR+/4FIRQW/19OMBCkacZpQjLSRQvvEjwSL1TQ1V+FnY1qGXstPkR5HSfFYnod20227MzirxFuQJizQCe0vPxY4TwnXmCGlhp6b6aBAUlPMSFn3c0UyhCdoRIaGcpQSFRSzRUrnzCixkwhpDtfOTP09UaBUVVnNyyqiWvYq8T9vmOvkKigoz3JNOJ5/lOTM0cKpWnFiKgnWbGoIwpKarA4eI4mwNt3VTQne8sqrpHfR9ty2d+dCDY7hFFrgwSVcwy10oAsYnuAF3uDderZerY95XWvWorcj+APr8we4/Ji1</latexit><latexit sha1_base64="WU379MaQInOjfur8A6jsboh9KFA=">AAAB/XicbVC9TsMwGPzCbykFAisDERVSWaqEBUYkFsYi0R+piSLHcVqrjh3ZDqKKMrLwKiwMIMRrsPE2OG0HaDnJ8unOn3zfRRmjSrvut7W2vrG5tV3bqe829vYP7MNGT4lcYtLFggk5iJAijHLS1VQzMsgkQWnESD+a3FR+/4FIRQW/19OMBCkacZpQjLSRQvvEjwSL1TQ1V+FnY1qGXstPkR5HSfFYnod20227MzirxFuQJizQCe0vPxY4TwnXmCGlhp6b6aBAUlPMSFn3c0UyhCdoRIaGcpQSFRSzRUrnzCixkwhpDtfOTP09UaBUVVnNyyqiWvYq8T9vmOvkKigoz3JNOJ5/lOTM0cKpWnFiKgnWbGoIwpKarA4eI4mwNt3VTQne8sqrpHfR9ty2d+dCDY7hFFrgwSVcwy10oAsYnuAF3uDderZerY95XWvWorcj+APr8we4/Ji1</latexit><latexit sha1_base64="DhzUQeKPrRkb6hDNeWkuZP3/YUU=">AAACCHicbVC7TsMwFHXKq5RXgJEBiwqpLFXCAmMFC2OR6ENqoshxnNaqY0e2g6iijiz8CgsDCLHyCWz8DU6bAVquZPnonHt1zz1hyqjSjvNtVVZW19Y3qpu1re2d3T17/6CrRCYx6WDBhOyHSBFGOeloqhnpp5KgJGSkF46vC713T6Sigt/pSUr8BA05jSlG2lCBfeyFgkVqkpgv99IRnQZuw0uQHoVx/jA9C+y603RmBZeBW4I6KKsd2F9eJHCWEK4xQ0oNXCfVfo6kppiRac3LFEkRHqMhGRjIUUKUn88OmcJTw0QwFtI8ruGM/T2Ro0QVXk1nYVEtagX5nzbIdHzp55SnmSYczxfFGYNawCIVGFFJsGYTAxCW1HiFeIQkwtpkVzMhuIsnL4PuedN1mu6tU29dlXFUwRE4AQ3gggvQAjegDToAg0fwDF7Bm/VkvVjv1se8tWKVM4fgT1mfP1eVmiQ=</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit><latexit sha1_base64="mrNDLu0KRrunj1DNPwKQx++STzY=">AAACCHicbVC7TsMwFHV4lvIKMDJgUSGVpUoQEowVLIxFog+piSLHcVqrjh3ZDqKKOrLwKywMIMTKJ7DxNzhtBmi5kuWjc+7VPfeEKaNKO863tbS8srq2Xtmobm5t7+zae/sdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB0XejdeyIVFfxOj1PiJ2jAaUwx0oYK7CMvFCxS48R8uZcO6SRw616C9DCM84fJaWDXnIYzLbgI3BLUQFmtwP7yIoGzhHCNGVKq7zqp9nMkNcWMTKpepkiK8AgNSN9AjhKi/Hx6yASeGCaCsZDmcQ2n7O+JHCWq8Go6C4tqXivI/7R+puNLP6c8zTTheLYozhjUAhapwIhKgjUbG4CwpMYrxEMkEdYmu6oJwZ0/eRF0zhqu03Bvz2vNqzKOCjgEx6AOXHABmuAGtEAbYPAInsEreLOerBfr3fqYtS5Z5cwB+FPW5w9Y1Zoo</latexit>2(x)<latexit sha1_base64="06UcrCgO743kkBimHm8f9BNZZxo=">AAACCHicbVC7TsMwFHXKq5RXgJEBiwqpLFVSIcFYwcJYJPqQmihyHKe16sSR7SCqKCMLv8LCAEKsfAIbf4PTZoCWK1k+Oude3XOPnzAqlWV9G5WV1bX1jepmbWt7Z3fP3D/oSZ4KTLqYMy4GPpKE0Zh0FVWMDBJBUOQz0vcn14XevydCUh7fqWlC3AiNYhpSjJSmPPPY8TkL5DTSX+YkY5p7rYYTITX2w+whP/PMutW0ZgWXgV2COiir45lfTsBxGpFYYYakHNpWotwMCUUxI3nNSSVJEJ6gERlqGKOISDebHZLDU80EMORCv1jBGft7IkORLLzqzsKiXNQK8j9tmKrw0s1onKSKxHi+KEwZVBwWqcCACoIVm2qAsKDaK8RjJBBWOruaDsFePHkZ9FpN22rat+f19lUZRxUcgRPQADa4AG1wAzqgCzB4BM/gFbwZT8aL8W58zFsrRjlzCP6U8fkDWmWaKQ==</latexit><latexit sha1_base64="06UcrCgO743kkBimHm8f9BNZZxo=">AAACCHicbVC7TsMwFHXKq5RXgJEBiwqpLFVSIcFYwcJYJPqQmihyHKe16sSR7SCqKCMLv8LCAEKsfAIbf4PTZoCWK1k+Oude3XOPnzAqlWV9G5WV1bX1jepmbWt7Z3fP3D/oSZ4KTLqYMy4GPpKE0Zh0FVWMDBJBUOQz0vcn14XevydCUh7fqWlC3AiNYhpSjJSmPPPY8TkL5DTSX+YkY5p7rYYTITX2w+whP/PMutW0ZgWXgV2COiir45lfTsBxGpFYYYakHNpWotwMCUUxI3nNSSVJEJ6gERlqGKOISDebHZLDU80EMORCv1jBGft7IkORLLzqzsKiXNQK8j9tmKrw0s1onKSKxHi+KEwZVBwWqcCACoIVm2qAsKDaK8RjJBBWOruaDsFePHkZ9FpN22rat+f19lUZRxUcgRPQADa4AG1wAzqgCzB4BM/gFbwZT8aL8W58zFsrRjlzCP6U8fkDWmWaKQ==</latexit><latexit sha1_base64="06UcrCgO743kkBimHm8f9BNZZxo=">AAACCHicbVC7TsMwFHXKq5RXgJEBiwqpLFVSIcFYwcJYJPqQmihyHKe16sSR7SCqKCMLv8LCAEKsfAIbf4PTZoCWK1k+Oude3XOPnzAqlWV9G5WV1bX1jepmbWt7Z3fP3D/oSZ4KTLqYMy4GPpKE0Zh0FVWMDBJBUOQz0vcn14XevydCUh7fqWlC3AiNYhpSjJSmPPPY8TkL5DTSX+YkY5p7rYYTITX2w+whP/PMutW0ZgWXgV2COiir45lfTsBxGpFYYYakHNpWotwMCUUxI3nNSSVJEJ6gERlqGKOISDebHZLDU80EMORCv1jBGft7IkORLLzqzsKiXNQK8j9tmKrw0s1onKSKxHi+KEwZVBwWqcCACoIVm2qAsKDaK8RjJBBWOruaDsFePHkZ9FpN22rat+f19lUZRxUcgRPQADa4AG1wAzqgCzB4BM/gFbwZT8aL8W58zFsrRjlzCP6U8fkDWmWaKQ==</latexit><latexit sha1_base64="06UcrCgO743kkBimHm8f9BNZZxo=">AAACCHicbVC7TsMwFHXKq5RXgJEBiwqpLFVSIcFYwcJYJPqQmihyHKe16sSR7SCqKCMLv8LCAEKsfAIbf4PTZoCWK1k+Oude3XOPnzAqlWV9G5WV1bX1jepmbWt7Z3fP3D/oSZ4KTLqYMy4GPpKE0Zh0FVWMDBJBUOQz0vcn14XevydCUh7fqWlC3AiNYhpSjJSmPPPY8TkL5DTSX+YkY5p7rYYTITX2w+whP/PMutW0ZgWXgV2COiir45lfTsBxGpFYYYakHNpWotwMCUUxI3nNSSVJEJ6gERlqGKOISDebHZLDU80EMORCv1jBGft7IkORLLzqzsKiXNQK8j9tmKrw0s1onKSKxHi+KEwZVBwWqcCACoIVm2qAsKDaK8RjJBBWOruaDsFePHkZ9FpN22rat+f19lUZRxUcgRPQADa4AG1wAzqgCzB4BM/gFbwZT8aL8W58zFsrRjlzCP6U8fkDWmWaKQ==</latexit>x1<latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit>x2<latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit>x1<latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit><latexit sha1_base64="2HhTHyaEJkkJenBTAOB/h38YNco=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00g6dTMLMjVhCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJrwfkRHSoSCUbSS70cUx0GYPc0G3qBac+vuHGSVeAWpQYHmoPrlD2OWRlwhk9SYnucm2M+oRsEkn1X81PCEsgkd8Z6likbc9LN55hk5s8qQhLG2TyGZq783MhoZM40CO5lnNMteLv7n9VIMr/uZUEmKXLHFoTCVBGOSF0CGQnOGcmoJZVrYrISNqaYMbU0VW4K3/OVV0r6oe27du7+sNW6KOspwAqdwDh5cQQPuoAktYJDAM7zCm5M6L86787EYLTnFzjH8gfP5AypgkcA=</latexit>x2<latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit><latexit sha1_base64="/xhoKoWR751dxO/nYLz15N6jlLs=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFclaQIuiy6cVnBPqApZTK9aYdOJmFmIpbQ33DjQhG3/ow7/8ZJm4W2Hhg4nHMv98wJEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5zf3OIyrNY/lgpgn2IzqSPOSMGiv5fkTNOAizp9mgPqhU3Zo7B1klXkGqUKA5qHz5w5ilEUrDBNW657mJ6WdUGc4Ezsp+qjGhbEJH2LNU0gh1P5tnnpFzqwxJGCv7pCFz9fdGRiOtp1FgJ/OMetnLxf+8XmrC637GZZIalGxxKEwFMTHJCyBDrpAZMbWEMsVtVsLGVFFmbE1lW4K3/OVV0q7XPLfm3V9WGzdFHSU4hTO4AA+uoAF30IQWMEjgGV7hzUmdF+fd+ViMrjnFzgn8gfP5AyvkkcE=</latexit>ABCDEF00.20.40.60.8150100500400300200100−15−10−5051015data examplesDNN weights2040608010010080604020−1e+3−5e+20e+05e+21e+3data examplesdata examplesABCDEF−200−1000100200to connections to the NTK  we expect similar properties for our kernel. Our approach additionally
shows that we can obtain other types of kernels by using different approximate inference methods.
In a recent work  Lee et al. [14] derive the mean and covariance function corresponding to the GP
induced by the NTK. Unfortunately  the model does not correspond to inference in a GP model (see
Section 2.3.1 in their paper). Our approach does not have this issue and we can express Gaussian
posterior approximations on a Bayesian DNN as inference in a GP regression model.

2 Deep Neural Networks (DNNs) and Gaussian Processes (GPs)

The goal of this paper is to present a theoretical relationship between training methods of DNNs
and GPs. DNNs are typically trained by minimizing an empirical loss between the data and the
i=1 of N examples
predictions. For example  in supervised learning with a dataset D := {(xi  yi)}N
of input xi ∈ RD and output yi ∈ RK  we can minimize a loss of the following form:
2 δw(cid:62)w  where (cid:96)i(w) := (cid:96)(yi  f w(xi)) 

N(cid:88)

(cid:96)i(w) + 1

¯(cid:96)(D  w) :=

i=1

(1)

where f w(x) ∈ RK denotes the DNN outputs with weights w ∈ RP   (cid:96)(y  f (x)) denotes a loss
function between an output y and the function f (x)  and δ is a small L2 regularizer.2. We assume the
loss function to be twice differentiable and strictly convex in f (e.g.  squared loss and cross-entropy
loss). An attractive feature of DNNs is that they can be trained using stochastic-gradient (SG)
methods [11]. Such methods scale well to large data settings.
GP models use an entirely different modeling approach which is based on directly modeling the
functions rather than the parameters. For example  for regression problems with scalar outputs yi ∈ R 
consider the following linear basis-function model with a nonlinear feature-map φ(x) : RD (cid:55)→ RP :
(2)

(cid:62)w +   with  ∼ N (0  σ2)  and w ∼ N (0  δ−1IP ) 

y = φ(x)

))  

where IP is a P × P identity matrix and σ2 is the output noise variance. Deﬁning the function to be
f (x) := φ(x)(cid:62)w  the predictive distribution p(f (x∗)|x∗ D) at a new test input x∗ is equal to that
of the following model directly deﬁned with a GP prior over f (x) [23]:
y = f (x) +   with f (x) ∼ GP (0  κ(x  x(cid:48)

(3)
where κ(x  x(cid:48)) := E[f (x)f (x(cid:48))] = δ−1φ(x)(cid:62)φ(x(cid:48)) is the covariance function or kernel of the GP.
The function-space model is more general in the sense that it can also deal with inﬁnite-dimensional
vector feature maps φ(x)  giving us a nonparametric model. This view has been used to show that as
a DNN becomes inﬁnitely wide it tends to a GP  by essentially showing that averaging over p(w)
with the feature map induced by a DNN leads to a GP covariance function [16].
An attractive property of the function-space formulation as opposed to the weight-space formulation 
such as (1)  is that the posterior distribution has a closed-form expression. Another attractive property
is that the posterior is usually unimodal  unlike the loss ¯l(D  w) which is typically nonconvex.
Unfortunately  the computation of the posterior takes O(N 3) which is infeasible for large datasets.
GPs also require choosing a good kernel [23]. Unlike DNNs  inference in GPs remains much more
difﬁcult.
To summarize  despite the similarities between the two models  their training methods are fundamen-
tally different. While DNNs employ stochastic optimization  GPs use closed-form updates. How can
we relate these seemingly different training procedures in practical settings  e.g.  without assuming
inﬁnite-width DNNs? In this paper  we provide an answer to this question. We derive theoretical
results that relate the solutions and iterations of deep-learning algorithms to GP inference. We do
so by ﬁrst ﬁnding a Gaussian posterior approximation (Step A in Fig. 1)  then use it to ﬁnd a linear
basis-function model (Step B in Fig. 1) and its corresponding GP (Step C in Fig. 1). We start in the
next section with our ﬁrst theoretical result.

2We can assume that δ is small enough that it does not affect the DNN’s generalization.

3

3 Relating Minima of the Loss to GP Inference via Laplace Approximation

In this section  we present theoretical results relating minima of a deep-learning loss (1) to inference
in GP models. A local minimizer w∗ of the loss (1) satisﬁes the following ﬁrst-order and second-
order conditions [17]: ∇w ¯(cid:96)(D  w∗) = 0 and ∇2
¯(cid:96)(D  w∗) (cid:31) 0. Deep-learning optimizers  such as
RMSprop and Adam  aim to ﬁnd such minimizers  and our goal is to relate them to GP inference.
Step A (Laplace Approximation): To do so  we will use an approximate inference method called
the Laplace approximation [1]. The minima of the loss (1) corresponds to a mode of the Bayesian
i=1 e−(cid:96)i(w)p(w) with prior distribution p(w) := N (w|0  δ−1IP )  assuming
that the posterior is well-deﬁned. The posterior distribution p(w|D) = p(D  w)/p(D) is usually
computationally intractable and requires computationally-feasible approximation methods. The
Laplace approximation uses the following Gaussian approximation for the posterior:

model: p(D  w) :=(cid:81)N

ww

N(cid:88)

i=1

p(w|D) ≈ N (w|µ  Σ)  where µ = w∗ and Σ−1 =

∇2
ww(cid:96)i(w∗) + δIP .

(4)

This approximation can be directly built using the solutions found by deep-learning optimizers.
Step B (Linear Model): The next step is to ﬁnd a linear basis-function model whose posterior
distribution is equal to the Gaussian approximation (4). We will now show that this is always possible
whenever the gradient and Hessian of the loss3 can be approximated as follows:

∇w(cid:96)(w) ≈ φw(x)vw(x  y) 

(5)
where φw(x) is a P ×Q feature matrix with Q as a positive integer  vw(x  y) is a Q length vector  and
Dw(x  y) is a Q × Q symmetric positive-deﬁnite matrix. We will now present results for a speciﬁc
choice φw  vw  and Dw. Our proof trivially generalizes to arbitrary choices of these quantities.
For the loss of form (1)  the gradient and Hessian take the following form [15  17]:

∇2
ww(cid:96)(w) ≈ φw(x)Dw(x  y)φw(x)

(cid:62) 

(cid:62)rw(x  y)  ∇2

ww(cid:96)(w) = Jw(x)

∇w(cid:96)(w) = Jw(x)

(6)
where Jw(x) := ∇wf w(x)(cid:62) is a K × P Jacobian matrix  rw(x  y) := ∇f (cid:96)(y  f ) is the residual
f f (cid:96)(y  f )  referred to as the noise precision  is the
vector evaluated at f := f w(x)  Λw(x  y) := ∇2
K × K Hessian matrix of the loss evaluated at f := f w(x)  and Hf := ∇2
wwf w(x). The similarity
ww(cid:96)(w) in (6) 
between (5) and (6) is striking. In fact  if we ignore the second term for the Hessian ∇2
we get the well-known Generalized Gauss-Newton (GGN) approximation [15  17]:

(cid:62)Λw(x  y)Jw(x) + Hf rw(x  y) 

(cid:62)Λw(x  y)Jw(x).

∇2
ww(cid:96)(w) ≈ Jw(x)

(7)
This gives us one choice for the approximation (5) where we can set φw(x) := Jw(x)(cid:62)  vw(x  y) :=
rw(x  y)  and Dw(x  y) := Λw(x  y).
We are now ready to present our ﬁrst theoretical result. Consider a Laplace approximation (4) but
with the GGN approximation (7) for the Hessian. We refer to this as Laplace-GGN  and denote

it by N (w|µ (cid:101)Σ) where (cid:101)Σ is the covariance obtained by using the GGN approximation. We
We construct a transformed dataset (cid:101)D = {(xi  ˜yi)}N
denote the Jacobian  noise-precision  and residual at w = w∗ by J∗(x)  Λ∗(x  y)  and r∗(x  y).
˜yi := J∗(xi)w∗ − Λ∗(xi  yi)−1r∗(xi  yi). We consider the following linear model for(cid:101)D:
i=1 where the outputs ˜yi ∈ RK are equal to
−1) and w ∼ N (0  δ−1IP ).
Theorem 1. The Laplace approximation N (w|µ (cid:101)Σ) is equal to the posterior distribution p(w|(cid:101)D)

˜y = J∗(x)w +   with  ∼ N (0  (Λ∗(x  y))

The following theorem states our result.

(8)

of the linear model (8).

A proof is given in Appendix A.1. The linear model uses J∗(x) as the nonlinear feature map  and the
noise prevision Λ∗(x  y) is obtained using the Hessian of the loss evaluated at f w∗ (x). The model is
constructed such that its posterior is equal to the Laplace approximation and it exploits the quadratic
approximation at w∗. We now describe the ﬁnal step relating the linear model to GPs.

3For notational convenience  we sometime use (cid:96)(w) to denote (cid:96)(y  f w(x)).

4

(cid:62)(cid:1) .

(cid:0)0  δ−1J∗(x)J∗(x(cid:48)

Step C (GP Model): To get a GP model  we use the equivalence between the weight-space view
shown in (2) and the function-space view shown in (3). With this  we get the following GP regression

model whose predictive distribution p(f (x∗)|x∗ (cid:101)D) is equal to that of the linear model (8):

)

˜y = f (x) +   with f (x) ∼ GP

(9)
Note that the kernel here is a multi-dimensional K × K kernel. The steps A  B  and C together
convert a DNN deﬁned in the weight-space to a GP deﬁned in the function-space. We refer to this
approach as “DNN2GP”.
The resulting GP predicts in the space of outputs ˜y and therefore results in different predictions
than the DNN  but it is connected to it through the Laplace approximation as shown in Theorem 1.
In Appendix B  we describe prediction of the outputs y (instead of ˜y) using this GP. Note that
our approach leads to a heteroscedastic GP which could be beneﬁcial. Even though our derivation
assumes a Gaussian prior and DNN model  the approach holds for other types of priors and models.
Relationship to NTK: The GP kernel in (9) is the Neural Tangent Kernel 4 (NTK) [8] which has
desirable theoretical properties. As the width of the DNN is increasing to inﬁnity  the kernel converges
in probability to a deterministic kernel and also remains asymptotically constant during training. Our
kernel is the NTK deﬁned at w∗ and is expected to have similar properties. It is also likely that  as the
DNN width is increased  the Laplace-GGN approximation has similar properties as a GP posterior 
and can be potentially used to improve the performance of DNNs. For example  we can use GPs to
tune hyperparameters of DNNs. The function-space view is also useful to understand relationships
between data examples. Another advantage of our approach is that we can derive kernels other than
the NTK. Any approximation of the form (5) will always result in a linear model similar to (8).
Accuracy of the GGN approximation: This approximation is accurate when the model f w(x) can
ﬁt the data well  in which case the residuals rw(x  y) are close to zero for all training examples and
the second term in (6) goes to zero [2  15  17]. The GGN approximation is a convenient option to
derive DNN2GP  but  as it is clear from (5)  other types of approximations can also be used.

4 Relating Iterations of a Deep-Learning Algorithm to GP Inference via VI

In this section  we present theoretical results relating iterations of an RMSprop-like algorithm to GP
inference. The RMSprop algorithm [21] uses the following updates (all operations are element-wise):

−1 ˆg(wt) 

wt+1 ← wt − αt (√st+1 + ∆)

st+1 ← (1 − βt)st + βt (ˆg(wt))2  

(10)
where t is the iteration  αt > 0 and 0 < βt < 1 are learning rates  ∆ > 0 is a small scalar  and ˆg(w)
is a stochastic-gradient estimate for ¯(cid:96)(D  w) obtained using minibatches. Our goal is to relate the
iterates wt to GP inference using our DNN2GP approach  but this requires a posterior approximation
deﬁned at each wt. We cannot use the Laplace approximation because it is only valid at w∗. We will
instead use a version of RMSprop proposed in [10] for variational inference (VI)  which enables us
to construct a GP inference problem at each wt.
Step A (Variational Inference): The variational online-Newton (VON) algorithm proposed in [10]
optimizes the variational objective  but takes an algorithmic form similar to RMSprop (see a detailed
discussion in [10]). Below  we show a batch version of VON  derived using Eq. (54) in [10]:

µt+1 ← µt − βt(St+1 + δIP )
St+1 ← (1 − βt)St + βt

N(cid:88)

i=1

∇w ¯(cid:96)(D  w)(cid:3)  
(cid:2)
−1Eqt(w)
(cid:2)
ww(cid:96)i(w)(cid:3)  
Eqt(w)
∇2

(11)

(12)

where St is a scaling matrix similar to the scaling vector st in RMSprop  and the Gaussian approxi-
mation at iteration t is deﬁned as qt(w) := N (w|µt  Σt) where Σt := (St + δIP )−1. Since there
are no closed-form expressions for the expectations  the Monte Carlo (MC) approximation is used.
Step B (Linear Model): As before  we assume the choices for (5) obtained by using the GGN
approximation (7). We consider the variant for VON where the GGN approximation is used for the
Hessian and MC approximation is used for the expectations with respect to qt(w). We call this the

4The NTK corrsponds to δ = 1 which implies a standard normal prior on weights.

5

Variational Online GGN or VOGGN algorithm. A similar algorithm has recently been used in [19]
where it shows competitive performance to Adam and SGD.
We now present a theorem relating iterations of VOGGN to linear models. We denote the Gaussian

the GGN approximation. We present theoretical results for VOGGN with 1 MC sample which
is denoted by wt ∼ ˜qt(w). Our proof in Appendix A.2 discusses a more general setting with
multiple MC samples. Similarly to the previous section  we ﬁrst deﬁne a transformed dataset:
i=1 where ˜yi t := Jwt(xi)wt − Λwt(xi  yi)−1rwt(xi  yi)  and then a linear

approximation obtained at iteration t by ˜qt(w) := N (w|µt (cid:101)Σt) where (cid:101)Σt is used to emphasize
(cid:101)Dt := {(xi  ˜yi t)}N
:= (1 − βt)(cid:101)Σ

˜yt = Jwt(x)w +   with  ∼ N (0  (βtΛwt(x  y))

basis-function model:

(13)

−1

t

t + βtδIP and mt := (1 − βt)Vt(cid:101)Σ

−1) and w ∼ N (mt  Vt)
−1
with V−1
t wt. The model is very similar to
the one obtained for Laplace approximation  but is now deﬁned using the iterates wt instead of the
minimum w∗. The prior over w is not the standard Gaussian anymore  rather a correlated Gaussian
derived from qt(w). The theorem below states the result (a proof is given in Appendix A.2).

Theorem 2. The Gaussian approximation N (w|wt+1 (cid:101)Σt+1) at iteration t + 1 of the VOGGN
update is equal to the posterior distribution p(w|(cid:101)Dt) of the linear model (13).
(cid:0)Jwt(x)mt  Jwt(x)VtJwt(x(cid:48)

Step C (GP Model): The linear model (13) has the same predictive distribution as the GP below:

(cid:62)(cid:1) .

˜yt = f t(x) +   with f t(x) ∼ GP

(14)

)

The kernel here is similar to the NTK but now there is a covariance term Vt which incorporates the
effect of the previous qt(w) as a prior. Our DNN2GP approach shows that one iteration of VOGGN
in the weight-space is equivalent to inference in a GP regression model deﬁned in a transformed
function-space with respect to a kernel similar to the NTK. This can be compared with the results
in [8]  where learning by plain gradient descent is shown to be equivalent to kernel gradient descent
in function-space. Similarly to the Laplace case  the resulting GP predicts in the space of outputs ˜yt 
but predictions for yt can be obtained using a method described in Appendix B.
A Deep-Learning Optimizer Derived from VOGGN: The VON algorithm  even though similar to
RMSprop  does not converge to the minimum of the loss. This is because it optimizes the variational
objective. Fortunately  a slight modiﬁcation of this algorithm gives us a deep-learning optimizer
which is similar to RMSprop but is guaranteed to converge to the minimum of the loss. For this  we
approximate the expectations in the updates (11)-(12) at the mean µt. This is called the zeroth-order
delta approximation; see Appendix A.6 in [9] for details of this method. Using this approximation
and denoting the mean µt by wt  we get the following update:

N(cid:88)

i=1

ww(cid:96)i(wt)(cid:3) .

(cid:2)
∇2

wt+1 ← wt − βt(ˆSt+1 + δIP )

−1∇w ¯(cid:96)(D  wt) 

ˆSt+1 ← (1 − βt)ˆSt + βt

We refer to this as Online GGN or OGGN method. A ﬁxed point w∗ of this iteration is also a
minimizer of the loss since we have ∇w ¯(cid:96)(D  w∗) = 0. Unlike RMSprop  at each iteration  we still
posterior of the linear model from Theorem (2) is equivalent to ˆqt when (cid:101)Σt is replaced by ˆΣt (see
get a Gaussian approximation ˆqt(w) := N (w|wt  ˆΣt) with ˆΣt := (ˆSt + δIP )−1. Therefore  the
Appendix A.3). In conclusion  by using VI in our DNN2GP approach  we are able to relate the
iterations of a deep-learning optimizer to GP inference.
Implementation of DNN2GP: In practice  both VOGGN and OGGN are computationally more
expensive than RMSprop because they involve computation of full covariance matrices. To address
this issue  we simply use the diagonal versions of these algorithms discussed in [10  19]. Speciﬁcally 
we use the VOGN and OGN algorithms discussed in [19]. This implies that Vt is a diagonal matrix
and the GP kernel can be obtained without requiring any computation of large matrices. Only
Jacobian computations are required. In our experiments  we also resort to computing the kernel over
a subset of data instead of the whole data  which further reduces the cost.

6

Figure 3: This ﬁgure shows a visualization of the predictive distributions on a modiﬁed version of
the Snelson dataset [20]. The left ﬁgure shows Laplace and the right one shows VI. DNN2GP is
our proposed method  elaborated upon in Appendix B  while DNN refers to a diagonal Gaussian
approximation. We also compare to a GP with RBF kernel (GP-RBF). An MLP is used for DNN2GP
and DNN. We see that  wherever the data is missing  the uncertainties are larger for our method than
the others. For classiﬁcation  we give an example in Fig. 9 in the appendix.
5 Experimental Results

5.1 Comparison of DNN2GP Uncertainty

In this section  we visualize the quality of the uncertainty of the GP obtained with our DNN2GP
approach on a simple regression task. To approximate predicitive uncertainty for our approach  we
use the method described in Appendix B. We use both Laplace and VI approximations  referred to
as ‘DNN2GP-Laplace’ and ‘DNN2GP-VI’  respectively. We compare it to the uncertainty obtained
using an MC approximation in the DNN (referred to as ‘DNN-Laplace’ and ‘DNN-VI’). We also
compare to a standard GP regression model with an RBF kernel (refer to as ‘GP-RBF’)  whose kernel
hyperparameters are chosen by optimizing the GP marginal likelihood.
We consider a version of the Snelson dataset [20] where  to assess the ‘in-between’ uncertainty 
we remove the data points between x = 1.5 and x = 3. We use a single hidden-layer MLP with
32 units and sigmoidal transfer function. Fig. 3 shows the results for Laplace (left) and VI (right)
approximation. For Laplace  we use Adam [11]  and  for VI  we use VOGN [10]. The uncertainty
provided by DNN2GP is bigger than the other methods wherever the data is not observed.

5.2 GP Kernel and Predictive Distribution for Classiﬁcation Datasets

In this section  we visualize the GP kernel and predictive distribution for DNNs trained on CIFAR-10
and MNIST. Our goal is to show that our GP kernel and its predictions enhance our understanding
of a DNN’s performance on classiﬁcation tasks. We consider LeNet-5 [12] and compute both the
Laplace and VI approximations. We show the visualization at the posterior mean.
The K × K GP kernel κ∗(x  x(cid:48)) := J∗(x)J∗(x(cid:48))(cid:62) results in a kernel matrix of dimensionality
N K × N K which makes it difﬁcult to visualize for our datasets. To simplify  we compute the sum of
the diagonal entries of κ∗(x  x(cid:48)) to get an N × N matrix. This corresponds to modelling the output
for each class with an individual GP and then summing the kernels of these GPs. We also visualize the
GP posterior mean: E[f (x)|D] = E[J∗(x)w|D] = J∗(x)w∗ ∈ RK. and use the reparameterization
that allows to predict in the data space y instead of ˜y which is explained in Appendix B.
Fig. 4a shows the GP kernel matrix and the posterior mean for the Laplace approximation on MNIST.
The rows and columns containing 300 data examples are grouped according to the classes. The kernel
matrix clearly shows the correlations learned by the DNN. As expected  each row in the posterior
mean also reﬂects that the classes are correctly classiﬁed (DNN test accuracy is 99%). Fig. 4b shows
the GP posterior mean after reparameterization for CIFAR-10 where we see a more noisy pattern due
to a lower accuracy of around 68% on this task.
Fig. 4d shows the two components of the predictive variances that can be interpreted as “aleatoric”
and “epistemic” uncertainty. As shown in Eq.
(48) in Appendix B.2  for a multiclass clas-
siﬁcation loss  the variance of the prediction of a label at an input x∗ is equal to Λ∗(x∗) +

Λ∗(x∗)J∗(x∗)(cid:101)ΣJ∗(x∗)(cid:62)Λ∗(x∗). Similar to the linear basis function model  the two terms here

have an interpretation (e.g.  see Eq. 3.59 in [1]). The ﬁrst term can be interpreted as the aleatoric
uncertainty (label noise)  while the second term takes a form that resembles the epistemic uncertainty

7

−4−20246810x−2−1012yDNN-LaplaceDNN2GP-LaplaceGP-RBF−4−20246810x−2−1012yDNN-VIDNN2GP-VIGP-RBF(a) MNIST: GP posterior mean (left) and GP kernel matrix (right)

(b) CIFAR: GP posterior mean

(c) Binary-MNIST on digits 0 and 1

(d) Epistemic (left) and aleatoric (right) uncertainties
Figure 4: DNN2GP kernels  posterior means and uncertainties with LeNet5 of 300 samples on
binary MNIST in Fig. (c)  MNIST in Fig. (a)  and CIFAR-10 in Fig. (b d). The colored regions
on the y-axis mark the classes. Fig. (a) shows the kernel and the predictive mean for the Laplace
approximation  which gives 99% test accuracy. We see in the kernel that examples with same class
labels are correlated. Fig. (c) shows the same for binary MNIST trained only on digits 0 and 1 by
using VI. The kernel clearly shows the out-of-class predictive behavior where predictions are not
certain. Fig. (b) and (d) show the Laplace-GP on the more complex CIFAR-10 data set where we
obtain 68% accuracy. Fig. (d) shows the two components of the predictive variance for CIFAR-10
that can be interpreted as epistemic (left) and aleatoric (right) uncertainties. The estimated epistemic
uncertainty is much lower than the aleatoric uncertainty  implying that the model is not ﬂexible
enough. This is plausible since the accuracy of the model is not too high (merely 68%).

(model noise). Fig. 4d shows these for CIFAR-10 where we see that the uncertainty of the model is
low (left) and the label noise rather high (right). This interpretation implies that the model is unable
to ﬂexibly model the data and instead explains it with high label noise.
In Fig. 4c  we study the kernel for classes outside of the training dataset using VI. We train LeNet-5
on digits 0 and 1 with VOGN and visualize the predictive mean and kernel on all 10 classes denoted
by differently colored regions on the y-axis. We can see that there are slight correlations to the
out-of-class samples but no overconﬁdent predictions. In contrast  the pattern between 0 and 1 is
quite strong. The kernel obtained with DNN2GP helps to interpret and visualize such correlations.

5.3 Tuning the Hyperparameters of a DNN Using the GP Marginal Likelihood

In this section  we demonstrate the tuning of DNN hyperparameters by using the GP marginal
likelihood on a real and synthetic regression dataset. In the deep-learning literature  this is usually
done using cross-validation. Our goal is to demonstrate that with DNN2GP we can do this by simply
computing the marginal likelihood on the training set.
We generate a synthetic regression dataset (N = 100; see Fig. 5) where there are a few data points
around x = 0 but plenty away from it. We ﬁt the data by using a neural network with single hidden
layer of 20 units and tanh nonlinearity. Our goal is to tune the regularization parameter δ to trade-off
underﬁtting vs overﬁtting. Fig. 5b and 5c show the train log marginal-likelihood obtained with the GP
obtained by DNN2GP  along with the test and train mean-square error (MSE) obtained using a point
estimate. Black stars indicate the hyperparameters chosen by using the test loss and log marginal

8

012345678930025020015010050−150−100−50050100150classdata examples5010015020025030030025020015010050−3e+4−2e+4−1e+40e+01e+42e+43e+4data examplesdata examples01234567893002502001501005000.20.40.60.81classdata examples0130025020015010050−60−40−20020406080classdata examples5010015020025030030025020015010050−1.5e+4−1e+4−5e+30e+05e+31e+41.5e+4data examplesdata examples01234567893002502001501005000.050.10.150.20.25classdata examples01234567893002502001501005000.050.10.150.20.25classdata examples(a) Model ﬁts

(b) Laplace Approximation

(c) Variational Inference

Figure 5: This ﬁgure demonstrates the use of the GP marginal likelihood to tune hyperparameters of
a DNN. We tune the regularization parameter δ on a synthetic dataset shown in (a). Fig. (b) and (c)
show train and test MSE along with log of the marginal likelihoods on training data obtained with
Laplace and VI respectively. We show the standard error over 10 runs. The optimal hyperparameters
according to test loss and marginal-likelihood (shown with black stars) match well.

Figure 6: This is same as Fig. 5 but on a real dataset: UCI Red Wine Quality. All the plots use
Laplace approximation  and the standard errors are estimated over 20 splits. We tune the following
hyperparameters: the regularization parameter δ (left)  the noise-variance σ (middle)  and the DNN
width (right). The train log marginal-likelihood chooses hyperparameters that give a low test error.

likelihood  respectively. We clearly see that the train marginal-likelihood chooses hyperparameters
that give low test error. The train MSE on the other hand overﬁts as δ is reduced.
Next  we discuss results for a real dataset: UCI Red Wine Quality (N = 1599) with an input-
dimensionality of 12 and a scalar output. We use an MLP with 2 hidden layers 20 units each and
tanh transfer function. We consider tuning the regularizer δ  the noise-variance σ  and the DNN
width. We use the Laplace approximation and tune one parameter at a time while keeping the others
ﬁxed (we use respectively σ = 0.64  δ = 30 and σ = 0.64  δ = 3  1 hidden layer). Similarly to the
synthetic data case  the train marginal-likelihood selects hyperparameters that give low test error.
These experiments show that the DNN2GP framework can be useful to tune DNN hyperparameters 
although this needs to be conﬁrmed for larger networks than we used here.

6 Discussion and Future Work

In this paper  we present theoretical results connecting approximate inference on DNNs to GP
posteriors. Our work enables the extraction of feature maps and GP kernels by simply training DNNs.
It provides a natural way to combine the two different models.
Our hope is that our theoretical results will facilitate further research on combining strengths of
DNNs and GPs. A computational bottleneck is the Jacobian computation which prohibits application
to large problems. There are several ways to reduce this computation  e.g.  by choosing a different
type of GGN approximation that uses gradients instead of the Jacobians. Exploration of such methods
is a future direction that needs to be pursued.
Exact inference on the GP model we derive is still computationally infeasible for large problems.
However  further approximations could enable inference on bigger datasets. Finally  our work opens
many other interesting avenues where a combination of GPs and DNNs can be useful such as model
selection  deep reinforcement learning  Bayesian optimization  active learning  interpretation  etc.
We hope that our work enables the community to conduct further research on such problems.

9

05δ=0.01−2.50.02.5yδ=0.63−202x−2.50.02.5δ=2510−210−1100101102hyperparameterδ0.10.20.3MSEtrainlosstestloss110120130140-logmarginallikelihoodTrainMargLik10−210−1100101102hyperparameterδ0.10.20.30.4MSEtrainlosstestloss110120130140-logmarginallikelihoodTrainMargLik100102hyperparameterδ0.00.20.40.60.81.0MSEtrainlosstestloss2000225025002750-logmarginallikelihoodTrainMargLik10−1100101hyperparameterσ0.00.20.40.60.81.0MSEtrainlosstestloss20004000600080001000012000-logmarginallikelihoodTrainMargLik100101102width0.20.30.40.50.6MSEtrainlosstestloss1800190020002100-logmarginallikelihoodTrainMargLikAcknowledgements
We would like to thank Kazuki Osawa (Tokyo Institute of Technology)  Anirudh Jain (RIKEN) 
and Runa Eschenhagen (RIKEN) for their help with the experiments. We would also like to thank
Matthias Bauer (DeepMind) for discussions and useful feedback. Many thanks to Roman Bachmann
(RIKEN) for helping with the visualization in Fig. 1. We also thank Stephan Mandt (UCI) for
suggesting the marginal likelihood experiment. We thank the reviewers and the area chair for their
feedback as well. We are also thankful for the RAIDEN computing system and its support team at
the RIKEN Center for Advanced Intelligence Project which we used extensively for our experiments.

References
[1] Christopher M Bishop. Pattern recognition and machine learning. springer  2006.

[2] L. Bottou  F. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning.

SIAM Review  60(2):223–311  2018. doi: 10.1137/16M1080173.

[3] John Bradshaw  Alexander G de G Matthews  and Zoubin Ghahramani. Adversarial examples 
uncertainty  and transfer testing robustness in Gaussian process hybrid deep networks. arXiv
preprint arXiv:1707.02476  2017.

[4] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning.

In Y. Bengio 
D. Schuurmans  J. D. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural
Information Processing Systems 22  pages 342–350. Curran Associates  Inc.  2009.

[5] Alexander G. de G. Matthews  Jiri Hron  Mark Rowland  Richard E. Turner  and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. In International
Conference on Learning Representations  2018.

[6] Adrià Garriga-Alonso  Carl Edward Rasmussen  and Laurence Aitchison. Deep convolutional
networks as shallow Gaussian processes. In International Conference on Learning Representa-
tions  2019.

[7] Tamir Hazan and Tommi S. Jaakkola. Steps toward deep kernel methods from inﬁnite neural

networks. CoRR  abs/1508.05133  2015.

[8] Arthur Jacot  Franck Gabriel  and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems
31  pages 8571–8580. Curran Associates  Inc.  2018.

[9] Mohammad Khan. Variational learning for latent Gaussian model of discrete data. PhD thesis 

University of British Columbia  2012.

[10] Mohammad Emtiyaz Khan  Didrik Nielsen  Voot Tangkaratt  Wu Lin  Yarin Gal  and Akash
Srivastava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In
International Conference on Machine Learning  pages 2616–2625  2018.

[11] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[12] Yann LeCun  Léon Bottou  Yoshua Bengio  Patrick Haffner  et al. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[13] Jaehoon Lee  Yasaman Bahri  Roman Novak  Samuel S. Schoenholz  Jeffrey Pennington  and
Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In 6th International
Conference on Learning Representations  ICLR 2018  Vancouver  BC  Canada  April 30 - May
3  2018  Conference Track Proceedings  2018.

[14] Jaehoon Lee  Lechao Xiao  Samuel S. Schoenholz  Yasaman Bahri  Roman Novak  Jascha
Sohl-Dickstein  and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear
Models Under Gradient Descent. arXiv e-prints  art. arXiv:1902.06720  Feb 2019.

[15] James Martens. New perspectives on the natural gradient method. CoRR  abs/1412.1193  2014.

10

[16] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag  Berlin  Heidelberg 

1996. ISBN 0387947248.

[17] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business

Media  2006.

[18] Roman Novak  Lechao Xiao  Yasaman Bahri  Jaehoon Lee  Greg Yang  Daniel A. Abolaﬁa 
Jeffrey Pennington  and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many
channels are Gaussian processes. In International Conference on Learning Representations 
2019.

[19] Kazuki Osawa  Siddharth Swaroop  Anirudh Jain  Runa Eschenhagen  Richard Turner  Rio
Yokota  and Mohammad Emtiyaz Khan. Practical deep learning with Bayesian principles. In
Advances in Neural Information Processing Systems  2019.

[20] Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In
Y. Weiss  B. Schölkopf  and J. C. Platt  editors  Advances in Neural Information Processing
Systems 18  pages 1257–1264. MIT Press  2006.

[21] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4  2012.

[22] Christopher KI Williams. Computing with inﬁnite networks. In Advances in neural information

processing systems  pages 295–301  1997.

[23] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning 

volume 2. MIT Press Cambridge  MA  2006.

11

,Mohammad Emtiyaz Khan
Alexander Immer
Ehsan Abedi
Maciej Korzepa