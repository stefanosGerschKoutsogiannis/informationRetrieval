2015,Non-convex Statistical Optimization for Sparse Tensor Graphical Model,We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor  we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem  we prove that an alternating minimization algorithm  which iteratively estimates each sparse precision matrix while fixing the others  attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably  such an estimator achieves estimation consistency with only one tensor sample  which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.,Non-convex Statistical Optimization for Sparse

Tensor Graphical Model

Wei Sun
Yahoo Labs

Sunnyvale  CA

sunweisurrey@yahoo-inc.com

Han Liu

Department of Operations Research

and Financial Engineering

Princeton University

Princeton  NJ

hanliu@princeton.edu

Zhaoran Wang

Department of Operations Research

and Financial Engineering

Princeton University

Princeton  NJ

zhaoran@princeton.edu

Guang Cheng

Department of Statistics

Purdue University
West Lafayette  IN

chengg@stat.purdue.edu

Abstract

We consider the estimation of sparse graphical models that characterize the depen-
dency structure of high-dimensional tensor-valued data. To facilitate the estimation
of the precision matrix corresponding to each way of the tensor  we assume the
data follow a tensor normal distribution whose covariance has a Kronecker product
structure. The penalized maximum likelihood estimation of this model involves
minimizing a non-convex objective function. In spite of the non-convexity of this
estimation problem  we prove that an alternating minimization algorithm  which
iteratively estimates each sparse precision matrix while ﬁxing the others  attains
an estimator with the optimal statistical rate of convergence as well as consistent
graph recovery. Notably  such an estimator achieves estimation consistency with
only one tensor sample  which is unobserved in previous work. Our theoretical
results are backed by thorough numerical studies.

1

Introduction

High-dimensional tensor-valued data are prevalent in many ﬁelds such as personalized recommen-
dation systems and brain imaging research [1  2]. Traditional recommendation systems are mainly
based on the user-item matrix  whose entry denotes each user’s preference for a particular item. To
incorporate additional information into the analysis  such as the temporal behavior of users  we need
to consider a user-item-time tensor. For another example  functional magnetic resonance imaging
(fMRI) data can be viewed as a three way (third-order) tensor since it contains the brain measurements
taken on different locations over time for various experimental conditions. Also  in the example of
microarray study for aging [3]  thousands of gene expression measurements are recorded on 16 tissue
types on 40 mice with varying ages  which forms a four way gene-tissue-mouse-age tensor.
In this paper  we study the estimation of conditional independence structure within tensor data. For
example  in the microarray study for aging we are interested in the dependency structure across dif-
ferent genes  tissues  ages and even mice. Assuming data are drawn from a tensor normal distribution 
a straightforward way to estimate this structure is to vectorize the tensor and estimate the underlying
Gaussian graphical model associated with the vector. Such an approach ignores the tensor structure

1

and requires estimating a rather high dimensional precision matrix with insufﬁcient sample size. For
instance  in the aforementioned fMRI application the sample size is one if we aim to estimate the
dependency structure across different locations  time and experimental conditions. To address such a
problem  a popular approach is to assume the covariance matrix of the tensor normal distribution is
separable in the sense that it is the Kronecker product of small covariance matrices  each of which
corresponds to one way of the tensor. Under this assumption  our goal is to estimate the precision
matrix corresponding to each way of the tensor. See §1.1 for a detailed survey of previous work.
Despite the fact that the assumption of the Kronecker product structure of covariance makes the
statistical model much more parsimonious  it poses signiﬁcant challenges. In particular  the penalized
negative log-likelihood function is non-convex with respect to the unknown sparse precision matrices.
Consequently  there exists a gap between computational and statistical theory. More speciﬁcally 
as we will show in §1.1  existing literature mostly focuses on establishing the existence of a local
optimum that has desired statistical guarantees  rather than offering efﬁcient algorithmic procedures
that provably achieve the desired local optima. In contrast  we analyze an alternating minimiza-
tion algorithm which iteratively minimizes the non-convex objective function with respect to each
individual precision matrix while ﬁxing the others. The established theoretical guarantees of the
proposed algorithm are as follows. Suppose that we have n observations from a K-th order tensor
normal distribution. We denote by mk  sk  dk (k = 1  . . .   K) the dimension  sparsity  and max
number of non-zero entries in each row of the precision matrix corresponding to the k-th way of the
k=1 mk. The k-th precision matrix estimator from our alternating

tensor. Besides  we deﬁne m =QK
minimization algorithm achieves apmk(mk + sk) log mk/(nm) statistical rate of convergence in
Frobenius norm  which is minimax-optimal since this is the best rate one can obtain even when the
rest K  1 true precision matrices are known [4]. Furthermore  under an extra irrepresentability
condition  we establish apmk log mk/(nm) rate of convergence in max norm  which is also optimal 
and a dkpmk log mk/(nm) rate of convergence in spectral norm. These estimation consistency
results and a sufﬁciently large signal strength condition further imply the model selection consistency
of recovering all the edges. A notable implication of these results is that  when K  3  our alternating
minimization algorithm can achieve estimation consistency in Frobenius norm even if we only have
access to one tensor sample  which is often the case in practice. This phenomenon is unobserved in
previous work. Finally  we conduct extensive experiments to evaluate the numerical performance of
the proposed alternating minimization method. Under the guidance of theory  we propose a way to
signiﬁcantly accelerate the algorithm without sacriﬁcing the statistical accuracy.

1.1 Related work and our contribution

A special case of our sparse tensor graphical model when K = 2 is the sparse matrix graphical
model  which is studied by [5–8]. In particular  [5] and [6] only establish the existence of a local
optima with desired statistical guarantees. Meanwhile  [7] considers an algorithm that is similar to
ours. However  the statistical rates of convergence obtained by [6  7] are much slower than ours
when K = 2. See Remark 3.6 in §3.1 for a detailed comparison. For K = 2  our statistical rate of
convergence in Frobenius norm recovers the result of [5]. In other words  our theory conﬁrms that the
desired local optimum studied by [5] not only exists  but is also attainable by an efﬁcient algorithm. In
addition  for matrix graphical model  [8] establishes the statistical rates of convergence in spectral and
Frobenius norms for the estimator attained by a similar algorithm. Their results achieve estimation
consistency in spectral norm with only one matrix observation. However  their rate is slower than
ours with K = 2. See Remark 3.11 in §3.2 for a detailed discussion. Furthermore  we allow K to
increase and establish estimation consistency even in Frobenius norm for n = 1. Most importantly 
all these results focus on matrix graphical model and can not handle the aforementioned motivating
applications such as the gene-tissue-mouse-age tensor dataset.
In the context of sparse tensor graphical model with a general K  [9] shows the existence of a
local optimum with desired rates  but does not prove whether there exists an efﬁcient algorithm
that provably attains such a local optimum. In contrast  we prove that our alternating minimization
algorithm achieves an estimator with desired statistical rates. To achieve it  we apply a novel theoretical
framework to separately consider the population and sample optimizers  and then establish the one-
step convergence for the population optimizer (Theorem 3.1) and the optimal rate of convergence
for the sample optimizer (Theorem 3.4). A new concentration result (Lemma B.1) is developed for
this purpose  which is also of independent interest. Moreover  we establish additional theoretical

2

guarantees including the optimal rate of convergence in max norm  the estimation consistency in
spectral norm  and the graph recovery consistency of the proposed sparse precision matrix estimator.
In addition to the literature on graphical models  our work is also closely related to a recent line of
research on alternating minimization for non-convex optimization problems [10–13]. These existing
results mostly focus on problems such as dictionary learning  phase retrieval and matrix decomposition.
Hence  our statistical model and analysis are completely different from theirs. Also  our paper is
related to a recent line of work on tensor decomposition. See  e.g.  [14–17] and the references therein.
Compared with them  our work focuses on the graphical model structure within tensor-valued data.
Notation: For a matrix A = (Ai j) 2 Rd⇥d  we denote kAk1 kAk2 kAkF as its max  spectral 
and Frobenius norm  respectively. We deﬁne kAk1 off :=Pi6=j |Ai j| as its off-diagonal `1 norm and
|||A|||1 := maxiPj |Ai j| as the maximum absolute row sum. Denote vec(A) as the vectorization
of A which stacks the columns of A. Let tr(A) be the trace of A. For an index set S = {(i  j)  i  j 2
{1  . . .   d}}  we deﬁne [A]S as the matrix whose entry indexed by (i  j) 2 S is equal to Ai j  and
zero otherwise. We denote 1d as the identity matrix with dimension d ⇥ d. Throughout this paper  we
use C  C1  C2  . . . to denote generic absolute constants  whose values may vary from line to line.

2 Sparse tensor graphical model

2.1 Preliminary

We employ the tensor notations used by [18]. Throughout this paper  higher order tensors are denoted
by boldface Euler script letters  e.g. T . We consider a K-th order tensor T2 Rm1⇥m2⇥···⇥mK .
When K = 1 it reduces to a vector and when K = 2 it reduces to a matrix. The (i1  . . .   iK)-th
element of the tensor T is denoted to be Ti1 ... iK . Meanwhile  we deﬁne the vectorization of T
as vec(T ) := (T1 1 ... 1  . . .  Tm1 1 ... 1  . . .  T1 m2 ... mK  Tm1 m2 ... mK )> 2 Rm with m =Qk mk.
i1 ... iK1/2.
In addition  we deﬁne the Frobenius norm of a tensor T as kT kF :=Pi1 ... iK T 2
For tensors  a ﬁber refers to the higher order analogue of the row and column of matrices. A ﬁber is
obtained by ﬁxing all but one of the indices of the tensor  e.g.  the mode-k ﬁber of T(k) is given by
Ti1 ...  ik1 : ik+1 ... iK . Matricization  also known as unfolding  is the process to transform a tensor
into a matrix. We denote T(k) as the mode-k matricization of a tensor T   which arranges the mode-k
ﬁbers to be the columns of the resulting matrix. Another useful operation in tensors is the k-mode
product. The k-mode product of a tensor T2 Rm1⇥m2⇥···⇥mK with a matrix A 2 RJ⇥mk is denoted
as T⇥ k A and is of the size m1 ⇥···⇥ mk1 ⇥ J ⇥ mk+1 ⇥···⇥ mK. Its entry is deﬁned as (T⇥ k
A)i1 ... ik1 j ik+1 ... iK :=Pmk
ik=1 Ti1 ... iK Aj ik . In addition  for a list of matrices {A1  . . .   AK}
with Ak 2 Rmk⇥mk  k = 1  . . .   K  we deﬁne T⇥{ A1  . . .   AK} := T⇥ 1 A1 ⇥2 ···⇥ K AK.
2.2 Model
A tensor T2 Rm1⇥m2⇥···⇥mK follows the tensor normal distribution with zero mean and covariance
matrices ⌃1  . . .   ⌃K  denoted as T⇠ TN(0; ⌃1  . . .   ⌃K)  if its probability density function is

p(T |⌃1  . . .   ⌃K) = (2⇡)m/2⇢ KYk=1

|⌃k|m/(2mk) exp  kT ⇥ ⌃1/2k2
F /2 

(2.1)

1

  . . .   ⌃1/2

k=1 mk and ⌃1/2 := {⌃1/2

where m = QK
K }. When K = 1  this tensor normal
distribution reduces to the vector normal distribution with zero mean and covariance ⌃1. According
to [9  18]  it can be shown that T⇠ TN(0; ⌃1  . . .   ⌃K) if and only if vec(T ) ⇠ N(vec(0); ⌃K ⌦
···⌦ ⌃1)  where vec(0) 2 Rm and ⌦ is the matrix Kronecker product.
We consider the parameter estimation for the tensor normal model. Assume that we observe in-
dependently and identically distributed tensor samples T1  . . .  Tn from TN(0; ⌃⇤1  . . .   ⌃⇤K). We
aim to estimate the true covariance matrices (⌃⇤1  . . .   ⌃⇤K) and their corresponding true precision
matrices (⌦⇤1  . . .   ⌦⇤K) where ⌦⇤k = ⌃⇤1
(k = 1  . . .   K). To address the identiﬁability issue in
the parameterization of the tensor normal distribution  we assume that k⌦⇤kkF = 1 for k = 1  . . .   K.
This renormalization assumption does not change the graph structure of the original precision matrix.

k

3

qn(⌦1  . . .   ⌦K) :=

is tr[S(⌦K ⌦···⌦ ⌦1)] PK

A standard approach to estimate ⌦⇤k  k = 1  . . .   K  is to use the maximum likelihood method
via (2.1). Up to a constant  the negative log-likelihood function of the tensor normal distribution
nPn
i=1 vec(Ti)vec(Ti)>. To
k=1(m/mk) log |⌦k|  where S := 1
encourage the sparsity of each precision matrix in the high-dimensional scenario  we consider a
penalized log-likelihood estimator  which is obtained by minimizing
KXk=1

tr[S(⌦K ⌦···⌦ ⌦1)] 

where Pk (·) is a penalty function indexed by the tuning parameter k. In this paper  we focus on
the lasso penalty [19]  i.e.  Pk (⌦k) = kk⌦kk1 off. This estimation procedure applies similarly to a
broad family of other penalty functions.
We name the penalized model from (2.2) as the sparse tensor graphical model. It reduces to the sparse
vector graphical model [20  21] when K = 1  and the sparse matrix graphical model [5–8] when
K = 2. Our framework generalizes them to fulﬁll the demand of capturing the graphical structure of
higher order tensor-valued data.

log |⌦k| +

KXk=1

Pk (⌦k) 

1
mk

(2.2)

1
m

2.3 Estimation

This section introduces the estimation procedure for the sparse tensor graphical model. A com-
putationally efﬁcient algorithm is provided to estimate the precision matrix for each way of the
tensor.
Recall that in (2.2)  qn(⌦1  . . .   ⌦K) is jointly non-convex with respect to ⌦1  . . .   ⌦K. Nevertheless 
qn(⌦1  . . .   ⌦K) is a bi-convex problem since qn(⌦1  . . .   ⌦K) is convex in ⌦k when the rest K  1
precision matrices are ﬁxed. The bi-convex property plays a critical role in our algorithm construction
and its theoretical analysis in §3.
According to its bi-convex property  we propose to solve this non-convex problem by alternatively
update one precision matrix with other matrices ﬁxed. Note that  for any k = 1  . . .   K  minimizing
(2.2) with respect to ⌦k while ﬁxing the rest K  1 precision matrices is equivalent to minimizing
(2.3)

L(⌦k) :=

i Vk>i

i=1 Vk

nmPn

K ⇤(k)
Here Sk := mk
with ⇥ the tensor product operation and [·](k) the mode-k matricization operation deﬁned in §2.1. The
1 >
k1⌦···⌦ ⌦1/2
result in (2.3) can be shown by noting that Vk
according to the properties of mode-k matricization shown by [18]. Hereafter  we drop the superscript
k of Vk
i if there is no confusion. Note that minimizing (2.3) corresponds to estimating vector-valued
Gaussian graphical model and can be solved efﬁciently via the glasso algorithm [21].

i :=⇥Ti ⇥⌦1/2
i = [Ti](k)⌦1/2

k1  1mk   ⌦1/2
k+1⌦ ⌦1/2

K ⌦···⌦ ⌦1/2

k+1  . . .   ⌦1/2

  . . .   ⌦1/2

1

tr(Sk⌦k) 

1
mk
  where Vk

1
mk

log |⌦k| + kk⌦kk1 off.

1   . . .   ⌦(t)

1   . . .   ⌦(0)

Algorithm 1 Solve sparse tensor graphical model via Tensor lasso (Tlasso)
1: Input: Tensor samples T1 . . .  Tn  tuning parameters 1  . . .   K  max number of iterations T .
2: Initialize ⌦(0)
K randomly as symmetric and positive deﬁnite matrices and set t = 0.
3: Repeat:
4: t = t + 1.
5: For k = 1  . . .   K:
6:
7:
8: End For
9: Until t = T .

k1  ⌦(t1)
k such that k⌦(t)

Given ⌦(t)
Normalize ⌦(t)

K   solve (2.3) for ⌦(t)

k+1   . . .   ⌦(t1)

k via glasso [21].

10: Output: b⌦k = ⌦(T )
The details of our Tensor lasso (Tlasso) algorithm are shown in Algorithm 1. It starts with a random
initialization and then alternatively updates each precision matrix until it converges. In §3  we will
illustrate that the statistical properties of the obtained estimator are insensitive to the choice of the
initialization (see the discussion following Theorem 3.5).

(k = 1  . . .   K).

k kF = 1.

k

4

3 Theory of statistical optimization

We ﬁrst prove the estimation errors in Frobenius norm  max norm  and spectral norm  and then provide
the model selection consistency of our Tlasso estimator. We defer all the proofs to the appendix.

3.1 Estimation error in Frobenius norm

Based on the penalized log-likelihood in (2.2)  we deﬁne the population log-likelihood function as

q(⌦1  . . .   ⌦K) :=

1

mEtr⇥vec(T )vec(T )>(⌦K ⌦···⌦ ⌦1)⇤ 

1
mk

KXk=1

log |⌦k|.

(3.1)

⌦k

q(⌦1  . . .   ⌦K).

By minimizing q(⌦1  . . .   ⌦K) with respect to ⌦k  k = 1  . . .   K  we obtain the population mini-
mization function with the parameter ⌦[K]k := {⌦1  . . .   ⌦k1  ⌦k+1  . . .   ⌦K}  i.e. 

Mk(⌦[K]k) := argmin

(3.2)
Theorem 3.1. For any k = 1  . . .   K  if ⌦j (j 6= k) satisﬁes tr(⌃⇤j ⌦j) 6= 0  then the population
minimization function in (3.2) satisﬁes Mk(⌦[K]k) = m⇥mkQj6=k tr(⌃⇤j ⌦j)⇤1⌦⇤k.
Theorem 3.1 shows a surprising phenomenon that the population minimization function recovers the
true precision matrix up to a constant in only one iteration. If ⌦j = ⌦⇤j   j 6= k  then Mk(⌦[K]k) =
⌦⇤k. Otherwise  after a normalization such that kMk(⌦[K]k)kF = 1  the normalized population
minimization function still fully recovers ⌦⇤k. This observation suggests that setting T = 1 in
Algorithm 1 is sufﬁcient. Such a suggestion will be further supported by our numeric results.
In practice  when (3.1) is unknown  we can approximate it via its sample version qn(⌦1  . . .   ⌦K)
deﬁned in (2.2)  which gives rise to the statistical error in the estimation procedure. Analogously to
(3.2)  we deﬁne the sample-based minimization function with parameter ⌦[K]k as

qn(⌦1  . . .   ⌦K).

(3.3)

cMk(⌦[K]k) := argmin

⌦k

In order to prove the estimation error  it remains to quantify the statistical error induced from ﬁnite
samples. The following two regularity conditions are assumed for this purpose.
Condition 3.2 (Bounded Eigenvalues). For any k = 1  . . .   K  there is a constant C1 > 0 such that 

0 < C1  min(⌃⇤k)  max(⌃⇤k)  1/C1 < 1 

where min(⌃⇤k) and max(⌃⇤k) refer to the minimal and maximal eigenvalue of ⌃⇤k  respectively.
Condition 3.2 requires the uniform boundedness of the eigenvalues of true covariance matrices ⌃⇤k. It
has been commonly assumed in the graphical model literature [22].
Condition 3.3 (Tuning). For any k = 1  . . .   K and some constant C2 > 0  the tuning parameter k

satisﬁes 1/C2plog mk/(nmmk)  k  C2plog mk/(nmmk).

Condition 3.3 speciﬁes the choice of the tuning parameters. In practice  a data-driven tuning procedure
[23] can be performed to approximate the optimal choice of the tuning parameters.
Before characterizing the statistical error  we deﬁne a sparsity parameter for ⌦⇤k  k = 1  . . .   K. Let
Sk := {(i  j) : [⌦⇤k]i j 6= 0}. Denote the sparsity parameter sk := |Sk| mk  which is the number
of nonzero entries in the off-diagonal component of ⌦⇤k. For each k = 1  . . .   K  we deﬁne B(⌦⇤k) as
the set containing ⌦⇤k and its neighborhood for some sufﬁciently large constant radius ↵> 0  i.e. 
(3.4)
Theorem 3.4. Assume Conditions 3.2 and 3.3 hold. For any k = 1  . . .   K  the statistical error of the
sample-based minimization function deﬁned in (3.3) satisﬁes that  for any ﬁxed ⌦j 2 B(⌦⇤j ) (j 6= k) 

B(⌦⇤k) := {⌦ 2 Rmk⇥mk : ⌦ = ⌦>; ⌦  0;k⌦  ⌦⇤kkF  ↵}.

!  
cMk(⌦[K]k)  Mk(⌦[K]k)F = OP r mk(mk + sk) log mk
where Mk(⌦[K]k) andcMk(⌦[K]k) are deﬁned in (3.2) and (3.3)  and m =QK

nm

k=1 mk.

(3.5)

5

Theorem 3.4 establishes the statistical error associated withcMk(⌦[K]k) for arbitrary ⌦j 2 B(⌦⇤j )
with j 6= k. In comparison  previous work on the existence of a local solution with desired statistical
property only establishes theorems similar to Theorem 3.4 for ⌦j = ⌦⇤j with j 6= k. The extension
to an arbitrary ⌦j 2 B(⌦⇤j ) involves non-trivial technical barriers. Particularly  we ﬁrst establish the
rate of convergence of the difference between a sample-based quadratic form with its expectation
(Lemma B.1) via concentration of Lipschitz functions of Gaussian random variables [24]. This result
is also of independent interest. We then carefully characterize the rate of convergence of Sk deﬁned
in (2.3) (Lemma B.2). Finally  we develop (3.5) using the results for vector-valued graphical models
developed by [25].
According to Theorem 3.1 and Theorem 3.4  we obtain the rate of convergence of the Tlasso estimator
in terms of Frobenius norm  which is our main result.
Theorem 3.5. Assume that Conditions 3.2 and 3.3 hold. For any k = 1  . . .   K  if the initialization
satisﬁes ⌦(0)

j 2 B(⌦⇤j ) for any j 6= k  then the estimator b⌦k from Algorithm 1 with T = 1 satisﬁes 

b⌦k  ⌦⇤kF = OP r mk(mk + sk) log mk

nm

! 

(3.6)

j

k=1 mk and B(⌦⇤j ) is deﬁned in (3.4).

where m =QK
Theorem 3.5 suggests that as long as the initialization is within a constant distance to the truth  our
Tlasso algorithm attains a consistent estimator after only one iteration. This initialization condition
⌦(0)
j 2 B(⌦⇤j ) trivially holds since for any ⌦(0)
that is positive deﬁnite and has unit Frobenius norm 
we have k⌦(0)
j  ⌦⇤kkF  2 by noting that k⌦⇤kkF = 1 (k = 1  . . .   K) for the identiﬁability of the
tensor normal distribution. In literature  [9] shows that there exists a local minimizer of (2.2) whose
convergence rate can achieve (3.6). However  it is unknown if their algorithm can ﬁnd such minimizer
since there could be many other local minimizers.
A notable implication of Theorem 3.5 is that  when K  3  the estimator from our Tlasso algorithm
can achieve estimation consistency even if we only have access to one observation  i.e.  n = 1  which
is often the case in practice. To see it  suppose that K = 3 and n = 1. When the dimensions m1  m2 
and m3 are of the same order of magnitude and sk = O(mk) for k = 1  2  3  all the three error rates
corresponding to k = 1  2  3 in (3.6) converge to zero.
This result indicates that the estimation of the k-th precision matrix takes advantage of the information
from the j-th way (j 6= k) of the tensor data. Consider a simple case that K = 2 and one precision
matrix ⌦⇤1 = 1m1 is known. In this scenario the rows of the matrix data are independent and hence
the effective sample size for estimating ⌦⇤2 is in fact nm1. The optimality result for the vector-valued
graphical model [4] implies that the optimal rate for estimating ⌦⇤2 isp(m2 + s2) log m2/(nm1) 
which matches our result in (3.6). Therefore  the rate in (3.6) obtained by our Tlasso estimator is
minimax-optimal since it is the best rate one can obtain even when ⌦⇤j (j 6= k) are known. As far as
we know  this phenomenon has not been discovered by any previous work in tensor graphical model.
Remark 3.6. For K = 2  our tensor graphical model reduces to matrix graphical model with Kro-

necker product covariance structure [5–8]. In this case  the rate of convergence ofb⌦1 in (3.6) reduces
top(m1 + s1) log m1/(nm2)  which is much faster thanpm2(m1 + s1)(log m1 + log m2)/n es-
tablished by [6] andp(m1 + m2) log[max(m1  m2  n)]/(nm2) established by [7]. In literature  [5]

shows that there exists a local minimizer of the objective function whose estimation errors match ours.
However  it is unknown if their estimator can achieve such convergence rate. On the other hand  our
theorem conﬁrms that our algorithm is able to ﬁnd such estimator with optimal rate of convergence.

3.2 Estimation error in max norm and spectral norm

We next show the estimation error in max norm and spectral norm. Trivially  these estimation errors are
bounded by that in Frobenius norm shown in Theorem 3.5. To develop improved rates of convergence
in max and spectral norms  we need to impose stronger conditions on true parameters.

6

max
e2Sc

k[⇤k]e Sk[⇤k]Sk Sk11  1  ↵k.

  ⇤k are bounded

k on the connected edges in Sk.

Condition 3.7 controls the inﬂuence of the non-connected terms in Sc
This condition has been widely applied in lasso penalized models [26  27].
Condition 3.8 (Bounded Complexity). For each k = 1  . . .   K  the parameters ⌃⇤k

and the parameter dk in (3.7) satisﬁes dk = opnm/(mk log mk).
Theorem 3.9. Suppose Conditions 3.2  3.3  3.7 and 3.8 hold. Assume sk = O(mk) for k = 1  . . .   K
and assume m0ks are in the same order  i.e.  m1 ⇣ m2 ⇣···⇣ mK. For each k  if the initialization
j 2 B(⌦⇤j ) for any j 6= k  then the estimator b⌦k from Algorithm 1 with T = 2 satisﬁes 
satisﬁes ⌦(0)
In addition  the edge set of b⌦k is a subset of the true edge set of ⌦⇤k  that is  supp(b⌦k) ✓ supp(⌦⇤k).

Theorem 3.9 shows that our Tlasso estimator achieves the optimal rate of convergence in max norm
[4]. Here we consider the estimator obtained after two iterations since we require a new concentration
inequality (Lemma B.3) for the sample covariance matrix  which is built upon the estimator in
Theorem 3.5. A direct consequence from Theorem 3.9 is the estimation error in spectral norm.
Corollary 3.10. Suppose the conditions of Theorem 3.9 hold  for any k = 1  . . .   K  we have

= OP r mk log mk
nm ! .

b⌦k  ⌦⇤k1

(3.8)

We ﬁrst introduce some important notations. Denote dk as the maximum number of non-zeros in any
row of the true precision matrices ⌦⇤k  that is 

(3.7)

dk := max

i2{1 ... mk}{j 2{ 1  . . .   mk} : [⌦⇤k]i j 6= 0} 

k ⌦ ⌦⇤1

with | · | the cardinality of the inside set. For each covariance matrix ⌃⇤k  we deﬁne ⌃⇤k
.
:= |||⌃⇤k|||1
Denote the Hessian matrix ⇤k := ⌦⇤1
k  whose entry [⇤k](i j) (s t) corresponds
to the second order partial derivative of the objective function with respect to [⌦k]i j and [⌦k]s t. We
deﬁne its sub-matrix indexed by the index set Sk as [⇤k]Sk Sk = [⌦⇤1
]Sk Sk  which is the
|Sk|⇥| Sk| matrix with rows and columns of ⇤k indexed by Sk and Sk  respectively. Moreover  we
deﬁne ⇤k
. In order to establish the rate of convergence in max norm  we
need to impose an irrepresentability condition on the Hessian matrix.
Condition 3.7 (Irrepresentability). For each k = 1  . . .   K  there exists some ↵k 2 (0  1] such that

:=([⇤k]Sk Sk )11

k ⌦ ⌦⇤1

2 Rm2

k⇥m2

k

k

b⌦k  ⌦⇤k2 = OP dkr mk log mk
nm ! .

(3.9)

Remark 3.11. Now we compare our obtained rate of convergence in spectral norm for K = 2 with
that established in the sparse matrix graphical model literature. In particular  [8] establishes the rate
k  (sk _ 1)  which
holds for example in the bounded degree graphs  our obtained rate is faster. However  our faster rate
comes at the price of assuming the irrepresentability condition. Using recent advance in nonconvex
regularization [28]  we can eliminate the irrepresentability condition. We leave this to future work.

of OPpmk(sk _ 1) log(m1 _ m2)/(nmk) for k = 1  2. Therefore  when d2

3.3 Model selection consistency

Theorem 3.9 ensures that the estimated precision matrix correctly excludes all non-informative edges

Therefore  in order to achieve the model selection consistency  a sufﬁcient condition is to assume that 
for each k = 1  . . .   K  the minimal signal ✓k := min(i j)2supp(⌦⇤k)[⌦⇤k]i j is not too small.

and includes all the true edges (i  j) with |[⌦⇤k]i j| > Cpmk log mk/(nm) for some constant C > 0.
Theorem 3.12. Under the conditions of Theorem 3.9  if ✓k  Cpmk log mk/(nm) for some
constant C > 0  then for any k = 1  . . .   K  signb⌦k = sign(⌦⇤k)  with high probability.

Theorem 3.12 indicates that our Tlasso estimator is able to correctly recover the graphical structure of
each way of the high-dimensional tensor data. To the best of our knowledge  these is the ﬁrst model
selection consistency result in high dimensional tensor graphical model.

7

4 Simulations

k

k b⌦(t1)

FK  0.001.

For simplicity  in our Tlasso algorithm we set the initialization of k-th precision matrix as 1mk for each

We compare the proposed Tlasso estimator with two alternatives. The ﬁrst one is the direct graph-
ical lasso (Glasso) approach [21] which applies the glasso to the vectorized tensor data to es-
timate ⌦⇤1 ⌦···⌦ ⌦⇤K directly. The second alternative method is the iterative penalized max-
imum likelihood method (P-MLE) proposed by [9]  whose termination condition is set to be
k=1b⌦(t)
PK
k = 1  . . .   K and the total iteration T = 1. The tuning parameter k is set as 20plog mk/(nmmk).

For a fair comparison  the same tuning parameter is applied in the P-MLE method. In the direct
Glasso approach  its tuning parameter is chosen by cross-validation via huge package [29].
We consider two simulations with a third order tensor  i.e.  K = 3. In Simulation 1  we construct a
triangle graph  while in Simulation 2  we construct a four nearest neighbor graph for each precision
matrix. An illustration of the generated graphs are shown in Figure 1. In each simulation  we consider
three scenarios  i.e.  s1: n = 10 and (m1  m2  m3) = (10  10  10); s2: n = 50 and (m1  m2  m3) =
(10  10  10); s3: n = 10 and (m1  m2  m3) = (100  5  5). We repeat each example 100 times
and compute the averaged computational time  the averaged estimation error of the Kronecker

rate (TPR)  and the true negative rate (TNR). More speciﬁcally  we denote a⇤i j be the (i  j)-th
1(a⇤i j 6= 0) and

product of precision matrices (m1m2m3)1b⌦1 ⌦···⌦ b⌦K  ⌦⇤1 ⌦···⌦ ⌦⇤KF   the true positive
entry of ⌦⇤1 ⌦···⌦ ⌦⇤K  and deﬁne TPR := Pi j
TNR :=Pi j

As shown in Figure 1  our Tlasso is dramatically faster than both alternative methods. In Scenario
s3  Tlasso takes about ﬁve seconds for each replicate  the P-MLE takes about 500 seconds while
the direct Glasso method takes more than one hour and is omitted in the plot. Tlasso algorithm is
not only computationally efﬁcient but also enjoys superior estimation accuracy. In all examples  the
direct Glasso method has signiﬁcantly larger errors than Tlasso due to ignoring the tensor graphical
structure. Tlasso outperforms P-MLE in Scenarios s1 and s2 and is comparable to it in Scenario s3.

1(bai j 6= 0  a⇤i j 6= 0)/Pi j

1(bai j = 0  a⇤i j = 0)/Pi

1(a⇤i j = 0).

400

300

200

100

)
s
d
n
o
c
e
s
(
 
e
m
T

i

0

Glasso
P-MLE
Tlasso

s1

s2

Scenarios

s3

400

200

)
s
d
n
o
c
e
s
(
 
e
m
T

i

0

0.08

s
r
o
r
r
E

0.06

0.04

0.02

Glasso
P-MLE
Tlasso

Glasso
P-MLE
Tlasso

0.08

0.06

s
r
o
r
r
E

0.04

0.02

Glasso
P-MLE
Tlasso

s1

s2

Scenarios

s3

s1

s2

Scenarios

s3

s1

s2

Scenarios

s3

Figure 1: Left two plots: illustrations of the generated graphs; Middle two plots: computational time;
Right two plots: estimation errors. In each group of two plots  the left (right) is for Simulation 1 (2).
Table 1 shows the variable selection performance. Our Tlasso identiﬁes almost all edges in these six
examples  while the Glasso and P-MLE method miss several true edges. On the other hand  Tlasso
tends to include more non-connected edges than other methods.
Table 1: A comparison of variable selection performance. Here TPR and TNR denote the true positive
rate and true negative rate.

Scenarios
s1
Sim 1 s2
s3
s1
Sim 2 s2
s3

Glasso

TPR

0.27 (0.002)
0.34 (0.000)

TNR

0.96 (0.000)
0.93 (0.000)

0.08 (0.000)
0.15 (0.000)

0.96 (0.000)
0.92 (0.000)

/

/

/

/

P-MLE

TPR
1 (0)
1 (0)
1 (0)

1 (0)

0.93 (0.004)

0.82 (0.001)

TNR

0.89 (0.002)
0.89 (0.002)
0.93 (0.001)
0.88 (0.002)
0.85 (0.002)
0.93 (0.001)

Tlasso

TPR
1(0)
1(0)
1(0)
1(0)
1(0)

0.99(0.001)

TNR

0.76 (0.004)
0.76 (0.004)
0.70 (0.004)
0.65 (0.005)
0.63 (0.005)
0.38 (0.002)

Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. Han Liu is grateful
for the support of NSF CAREER Award DMS1454377  NSF IIS1408910  NSF IIS1332109  NIH
R01MH102339  NIH R01GM083084  and NIH R01HG06841. Guang Cheng’s research is sponsored
by NSF CAREER Award DMS1151692  NSF DMS1418042  Simons Fellowship in Mathematics 
ONR N00014-15-1-2331 and a grant from Indiana Clinical and Translational Sciences Institute.

8

2014.

References
[1] S. Rendle and L. Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag recom-

mendation. In International Conference on Web Search and Data Mining  2010.

[2] G.I. Allen. Sparse higher-order principal components analysis. In International Conference on Artiﬁcial

Intelligence and Statistics  2012.

PLOS Genetics  3:2326–2337  2007.

[3] J. Zahn  S. Poosala  A. Owen  D. Ingram  et al. AGEMAP: A gene expression database for aging in mice.

[4] T. Cai  W. Liu  and H.H. Zhou. Estimating sparse precision matrix: Optimal rates of convergence and

adaptive estimation. Annals of Statistics  2015.

[5] C. Leng and C.Y. Tang. Sparse matrix graphical models. Journal of the American Statistical Association 

107:1187–1200  2012.

[6] J. Yin and H. Li. Model selection and estimation in the matrix normal graphical model. Journal of

Multivariate Analysis  107:119–140  2012.

[7] T. Tsiligkaridis  A. O. Hero  and S. Zhou. On convergence of Kronecker graphical Lasso algorithms. IEEE

Transactions on Signal Processing  61:1743–1755  2013.

[8] S. Zhou. Gemini: Graph estimation with matrix variate normal instances. Annals of Statistics  42:532–562 

[9] S. He  J. Yin  H. Li  and X. Wang. Graphical model selection and estimation for high dimensional tensor

data. Journal of Multivariate Analysis  128:165–185  2014.

[10] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimization. In

Symposium on Theory of Computing  pages 665–674  2013.

[11] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. In Advances in

Neural Information Processing Systems  pages 2796–2804  2013.

[12] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery over the sphere. arXiv:1504.06785  2015.
[13] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for sparse coding.

arXiv:1503.00778  2015.

[14] A. Anandkumar  R. Ge  D. Hsu  S. Kakade  and M. Telgarsky. Tensor decompositions for learning latent

variable models. Journal of Machine Learning Research  15:2773–2832  2014.

[15] W. Sun  J. Lu  H. Liu  and G. Cheng. Provable sparse tensor decomposition. arXiv:1502.01425  2015.
[16] S. Zhe  Z. Xu  X. Chu  Y. Qi  and Y. Park. Scalable nonparametric multiway data analysis. In International

Conference on Artiﬁcial Intelligence and Statistics  2015.

[17] S. Zhe  Z. Xu  Y. Qi  and P. Yu. Sparse bayesian multiview learning for simultaneous association discovery
and diagnosis of alzheimer’s disease. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence  2015.

[18] T. Kolda and B. Bader. Tensor decompositions and applications. SIAM Review  51:455–500  2009.
[19] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society 

Series B  58:267–288  1996.

2007.

Biostatistics  9:432–441  2008.

[20] M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika  94:19–35 

[21] J. Friedman  H. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso.

[22] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation.

Electronic Journal of Statistics  2:494–515  2008.

[23] W. Sun  J. Wang  and Y. Fang. Consistent selection of tuning parameters via variable selection stability.

Journal of Machine Learning Research  14:3419–3440  2013.

[24] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer  2011.
[25] J. Fan  Y. Feng  and Y. Wu. Network exploration via the adaptive Lasso and scad penalties. Annals of

Statistics  3:521–541  2009.

7:2541–2567  2006.

[26] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research 

[27] P. Ravikumar  M.J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by
minimizing `1-penalized log-determinant divergence. Electronic Journal of Statistics  5:935–980  2011.
[28] Z. Wang  H. Liu  and T. Zhang. Optimal computational and statistical rates of convergence for sparse

nonconvex learning problems. Annals of Statistics  42:2164–2201  2014.

[29] T. Zhao  H. Liu  K. Roeder  J. Lafferty  and L. Wasserman. The huge package for high-dimensional

undirected graph estimation in R. Journal of Machine Learning Research  13:1059–1062  2012.

[30] A. Gupta and D. Nagar. Matrix variate distributions. Chapman and Hall/CRC Press  2000.
[31] P. Hoff. Separable covariance arrays via the Tucker product  with applications to multivariate relational

[32] A.P. Dawid. Some matrix-variate distribution theory: Notational considerations and a bayesian application.

data. Bayesian Analysis  6:179–196  2011.

Biometrika  68:265–274  1981.

[33] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional

scaling. Annals of Statistics  39:1069–1097  2011.

9

,Wei Sun
Zhaoran Wang
Han Liu
Guang Cheng