2016,Deep Neural Networks with Inexact Matching for Person Re-Identification,Person Re-Identification is the task of matching images of a person across multiple camera views. Almost all prior approaches address this challenge by attempting to learn the possible transformations that relate the different views of a person from a training corpora. Then  they utilize these transformation patterns for matching a query image to those in a gallery image bank at test time. This necessitates learning good feature representations of the images and having a robust feature matching technique. Deep learning approaches  such as Convolutional Neural Networks (CNN)  simultaneously do both and have shown great promise recently. In this work  we propose two CNN-based architectures for Person Re-Identification. In the first  given a pair of images  we extract feature maps from these images via multiple stages of convolution and pooling. A novel inexact matching technique then matches pixels in the first representation with those of the second. Furthermore  we search across a wider region in the second representation for matching. Our novel matching technique allows us to tackle the challenges posed by large viewpoint variations  illumination changes or partial occlusions. Our approach shows a promising performance and requires only about half the parameters as a current state-of-the-art technique. Nonetheless  it also suffers from false matches at times. In order to mitigate this issue  we propose a fused architecture that combines our inexact matching pipeline with a state-of-the-art exact matching technique. We observe substantial gains with the fused model over the current state-of-the-art on multiple challenging datasets of varying sizes  with gains of up to about 21%.,Deep Neural Networks with Inexact Matching for

Person Re-Identiﬁcation

Arulkumar Subramaniam

Indian Institute of Technology Madras

Chennai  India 600036

aruls@cse.iitm.ac.in

Moitreya Chatterjee

Indian Institute of Technology Madras

Chennai  India 600036

metro.smiles@gmail.com

Anurag Mittal

Indian Institute of Technology Madras

Chennai  India 600036

amittal@cse.iitm.ac.in

Abstract

Person Re-Identiﬁcation is the task of matching images of a person across multiple
camera views. Almost all prior approaches address this challenge by attempting to
learn the possible transformations that relate the different views of a person from a
training corpora. Then  they utilize these transformation patterns for matching a
query image to those in a gallery image bank at test time. This necessitates learning
good feature representations of the images and having a robust feature matching
technique. Deep learning approaches  such as Convolutional Neural Networks
(CNN)  simultaneously do both and have shown great promise recently.
In this work  we propose two CNN-based architectures for Person Re-Identiﬁcation.
In the ﬁrst  given a pair of images  we extract feature maps from these images via
multiple stages of convolution and pooling. A novel inexact matching technique
then matches pixels in the ﬁrst representation with those of the second. Further-
more  we search across a wider region in the second representation for matching.
Our novel matching technique allows us to tackle the challenges posed by large
viewpoint variations  illumination changes or partial occlusions. Our approach
shows a promising performance and requires only about half the parameters as a
current state-of-the-art technique. Nonetheless  it also suffers from false matches at
times. In order to mitigate this issue  we propose a fused architecture that combines
our inexact matching pipeline with a state-of-the-art exact matching technique. We
observe substantial gains with the fused model over the current state-of-the-art on
multiple challenging datasets of varying sizes  with gains of up to about 21%.

1

Introduction

Successful object recognition systems  such as Convolutional Neural Networks (CNN)  extract
“distinctive patterns” that describe an object (e.g. a human) in an image  when “shown” several images
known to contain that object  exploiting Machine Learning techniques [1]. Through successive stages
of convolutions and a host of non-linear operations such as pooling  non-linear activation  etc.  CNNs
extract complex yet discriminative representation of objects that are then classiﬁed into categories
using a classiﬁer  such as softmax.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: Some common challenges in Person Re-Identiﬁcation.

1.1 Problem Deﬁnition

One of the key subproblems of the generic object recognition task is recognizing people. Of
special interest to the surveillance and Human-Computer interaction (HCI) community is the task
of identifying a particular person across multiple images captured from the same/different cameras 
from different viewpoints  at the same/different points in time. This task is also known as Person
Re-Identiﬁcation. Given a pair of images  such systems should be able to decipher if both of them
contain the same person or not. This is a challenging task  since appearances of a person across images
can be very different due to large viewpoint changes  partial occlusions  illumination variations  etc.
Figure 1 highlights some of these challenges. This also leads to a fundamental research question:
Can CNN-based approaches effectively handle the challenges related to Person Re-Identiﬁcation?
CNN-based approaches have recently been applied to this task  with reasonable success [2  3] and are
amongst the most competitive approaches for this task. Inspired by such approaches  we explore a set
of novel CNN-based architectures for this task. We treat the problem as a classiﬁcation task. During
training  for every pair of images  the model is told whether they are from the same person or not.
At test time  the posterior classiﬁcation probabilities obtained from the models are used to rank the
images in a gallery image set in terms of their similarity to a query image (probe).
In this work  we propose two novel CNN-based schemes for Person Re-Identiﬁcation. Our ﬁrst
model hinges on the key observation that due to a wide viewpoint variation  the task of ﬁnding a
match between the pixels of a pair of images needs to be carried out over a larger region  since
“matching pixels” on the object may have been displaced signiﬁcantly. Secondly  illumination
variations might cause the absolute intensities of image regions to vary  rendering exact matching
approaches ineffective. Finally  coupling these two solutions might provide a recipe for taking care
of partial occlusions as well. We call this ﬁrst model of ours  Normalized X-Corr. However  the
ﬂexibility of inexact (soft) matching over a wider search space comes at the cost of occasional false
matches. To remedy this  we propose a second CNN-based model which fuses a state-of-the-art exact
matching technique [2] with Normalized X-Corr. We hypothesize that proper training allows the two
components of the fused network to learn complimentary patterns from the data  thus aiding the ﬁnal
classiﬁcation. Empirical results show Normalized X-Corr to hold promise and the Fused network
outperforming all baseline approaches on multiple challenging datasets  with gains of upto 21% over
the baselines.
In the next section  we touch upon relevant prior work. We present our methodology in Section
3. Sections 4 and 5 deal with the Experiments and the discussion of the obtained Results thereof.
Finally  we conclude in Section 6  outlining some avenues worthy of exploration in the future.

2 Related Work

In broad terms  we categorize the prior work in this ﬁeld into Non-Deep and Deep Learning ap-
proaches.

2.1 Non-Deep Learning Approaches

Person Re-Identiﬁcation systems have two main components: Feature Extraction and Similarity
Metric for matching. Traditional approaches for Person Re-Identiﬁcation either proposed useful
features  or discriminative similarity metrics for comparison  or both.

2

Typical features that have proven useful include color histograms and dense SIFT [4] features
computed over local patches in the image [5  6]. Farenzena et al. represent image patches by
exploiting features that model appearance  chromatic content  etc [7]. Another interesting line of
work on feature representation attempts to learn a bag-of-features (a.k.a. a dictionary)-based approach
for image representation [8  9]. Further  Prosser et al. show the effectiveness of learning a subspace
for representing the data  modeled using a set of standard features [10]. While these approaches show
promise  their performance is bounded by the ability to engineer good features. Our models  based
on deep learning  overcome this handicap by learning a representation from the data.
There is also a substantial body of work that attempts to learn an effective similarity metric for
comparing images [11  12  13  14  15  16]. Here  the objective is to learn a distance measure that
is indicative of the similarity of the images. The Mahalanobis distance has been the most common
metric that has been adopted for matching in person re-identiﬁcation [5  17  18  19]. Some other
metric learning approaches attempt to learn transformations  which when applied to the feature space 
aligns similar images closer together [20]. Yet other successful metric learning approaches are an
ensemble of multiple metrics [21]. In contrast to these approaches  we jointly learn both features and
discriminative metric (using a classiﬁer) in a deep learning framework.
Another interesting line of non-deep approaches for person re-identiﬁcation have claimed novelty
both in terms of features as well as matching metrics [22  23]. Many of them rely on weighing the
hand-engineered image features ﬁrst  based on some measure such as saliency and then performing
matching [6  24  25]. However  this is done in a non-deep framework unlike ours.

2.2 Deep Learning based Approaches

There has been relatively fewer prior work based on deep learning for addressing the challenge of
Person Re-Identiﬁcation. Most deep learning approaches exploit the CNN-framework for the task  i.e.
they ﬁrst extract highly non-linear representations from the images  then they compute some measure
of similarity. Yi et al. propose a Siamese Network that takes as input the image pair that is to be
compared  performs 3 stages of convolution on them (with the kernels sharing weights)  and ﬁnally
uses cosine similarity to judge their extent of match [26]. Both of our models differ by performing a
novel inexact matching of the images after two stages of convolution and then processing the output
of the matching layer to arrive at a decision.
Li et al. also adopt a two-input network architecture [3]. They take the product of the responses
obtained right after the ﬁrst set of convolutions corresponding to the two inputs and process its output
to obtain a measure of similarity. Our models  on the other hand  are signiﬁcantly deeper. Besides 
Normalized X-Corr stands out by retaining the matching outcome corresponding to every candidate in
the search space of a pixel rather than choosing only the maximum match. Ahmed et al. too propose
a very promising architecture for Person Re-Identiﬁcation [2]. Our models are inspired from their
work and does incorporate some of their features  but we substantially differ from their approach by
performing an inexact matching over a wider search space after two stages of convolution. Further 
Normalized X-Corr has fewer parameters than Ahmed et al. [2].
Finally  our Fused model is a one of a kind deep learning architecture for Person Re-Identiﬁcation.
This is because a combination (fusion) of multiple deep frameworks has hitherto remained unexplored
for this task.

3 Proposed Approach

3.1 Our Architecture

In this work  we propose two architectures for Person Re-Identiﬁcation. Both of our architectures
are a type of “Siamese”-CNN model which take as input two images for matching and outputs the
likelihood that the two images contain the same person.

3.1.1 Normalized X-Corr

The following are the principal components of the Normalized X-Corr model  as shown in Fig 2.

3

Figure 2: Architecture of the Normalized X-Corr Model.

Figure 3: Architecture of the Fused Network Model.

Tied Convolution Layers: Convolutional features have been shown to be effective representation of
images [1  2]. In order to measure similarity between the input images  the applied transformations
must be similar. Therefore  along the lines of Ahmed et al.  we perform two stages of convolutions
and pooling on both the input images by passing them through two input pipelines of a “Siamese”
Network  that share weights [2]. The ﬁrst convolution layer takes as input images of size 60×160×3
and applies 20 learned ﬁlters of size 5×5×3  while the second one applies 25 learned ﬁlters of size
5×5×20. Both convolutions are followed by pooling layers  which reduce the output dimension by a
factor of 2  and ReLU (Rectiﬁed Linear Unit) clipping. This gives us 25 maps of dimension 12×37
as output from each branch which are fed to the next layer.
Normalized Correlation Layer: This is the ﬁrst layer that captures the similarity of the two input
images; subsequent layers build on the output of this layer to ﬁnally arrive at a decision as to whether
the two images are of the same person or not. Different from [2  3]  we incorporate both inexact
matching and wider search. Given two corresponding input feature maps X and Y   we compute the
normalized correlation as follows. We start with every pixel of X located at (x  y)  where x is along
the width and y along the height (denoted as X(x  y)). We then create two matrices. The ﬁrst is a
5×5 matrix representing the 5×5 neighborhood of X(x  y)  while the second is the corresponding
5×5 neighborhood of Y centered at (a  b)  where 1 ≤ a ≤ 12 and y − 2 ≤ b ≤ y + 2. Now 
markedly different from Ahmed et al. [2]  we perform inexact matching over a wider search space  by
computing a Normalized Correlation between the two patch matrices. Given two matrices  E and F  
whose elements are arranged as two N-dimensional vectors  the Normalized Correlation is given by:

(cid:80)N
i=1(Ei − µE)(Fi − µF )

(N − 1).σE.σF

normxcorr(E  F ) =

 

where µE  µF denotes the means of the elements of the 2 matrices  E and F respectively  while
σE  σF denotes their respective standard deviations (a small  = 0.01 is added to σE and σF to
avoid division by 0). Interestingly  Normalized Correlation being symmetric  we need to model the
computation only in one-way  thereby cutting down the number of parameters in subsequent layers.
For every pair of 5×5 matrices corresponding to a given pixel in image X  we arrange the normalized
correlation values in different feature maps. These feature maps preserve the spatial ordering of

4

3712NORMALIZEDCORRELATION+RELU 37251x1x1500 ⟶ 251035253x3x25 ⟶ 25525samedifferentFully connected Layer5005x5x3 ⟶ 20CONV + RELU1565620202878MAXPOOL2x25x5x20 ⟶ 25CONV + RELU742425251237MAXPOOL2x25x5x3 ⟶ 20CONV + RELU1565620202878MAXPOOL2x25x5x20 ⟶ 25CONV + RELU7425251237MAXPOOL2x2150012CONV + RELUMAXPOOL2x2CONV 1724Shared weights6060160160Cross Patch Feature AggregationSoftmaxlayer3712NORMALIZEDCORRELATION+RELU 37251x1x1500 ⟶ 251035253x3x25 ⟶ 25525samedifferent5005x5x3 ⟶ 20CONV + RELU1565620202878MAXPOOL2x25x5x20 ⟶ 25CONV + RELU742425251237MAXPOOL2x25x5x3 ⟶ 20CONV + RELU1565620202878MAXPOOL2x25x5x20 ⟶ 25CONV + RELU7425251237MAXPOOL2x2150012CONV + RELUMAXPOOL2x2CONV24Shared weights606016016012CROSS INPUTNEIGHBORHOOD+RELU 37251x1x1250 ⟶ 251035253x3x25 ⟶ 25525125012CONV + RELUMAXPOOL2x2CONV 175003717Cross Patch Feature AggregationPatch summaryFully connected LayerSoftmaxlayerpixels in X and are also 12×37 each  but their pixel values represent normalized correlation. This
gives us 60 feature maps of dimension 12×37 each. Now similarly  we perform the same operation
for all 25 pairs of maps that are input to the Normalized Correlation layer  to obtain an output of
1500  12×37 maps. One subtle but important difference between our approach and that of Li et al.
[3] is that we preserve every correlation output corresponding to the search space of a pixel  X(x  y) 
while they only keep the maximum response. We then pass these set of feature maps through a ReLU 
to discard probable noisy matches.
The mean subtraction and standard deviation normalization step incorporates illumination invariance 
a step unaccounted for in Li et al. [3]. Thus two patches which differ only in absolute intensity values
but are similar in the intensity variation pattern would be treated as similar by our models. The wider
search space  compared to Ahmed et al. [2]  gives invariance to large viewpoint variation. Further 
performing inexact matching (correlation measure) over a wider search space gives us robustness to
partial occlusions. Due to partial occlusions  a part(s) (P) of a person/object visible in one view may
not be visible in others. Using a wider search space  our model looks for a part which is similar to
the missing P within a wider neighborhood of P’s original location. This is justiﬁed since typically
adjacent regions of objects in an image have regularity/similarity in appearance. e.g. Bottom and
upper half of torso. Now  since we are comparing two different parts due to the occlusion of P  we
need to perform ﬂexible matching. Thus  inexact matching is used.
Cross Patch Feature Aggregation Layers: The Normalized Correlation layer incorporates informa-
tion from the local neighborhood of a pixel. We now seek to incorporate greater context information 
to obtain a summarization effect. In order to do so  we perform 2 successive layers of convolution
(with a ReLU ﬁltered output) followed by pooling (by a factor of 2) of the output feature maps from
the previous layer. We use 1×1×1500 convolution kernels for the ﬁrst layer and 3×3×25 convolution
kernels for the second convolution layer. Finally  we get 25 maps of dimension 5×17 each.
Fully Connected Layers: The fully connected layers collate information from pixels that are very
far apart. The feature map outputs obtained from the previous layer are reshaped into one long
2125-dimensional vector. This vector is fed as input to a 500-node fully connected layer  which
connects to another fully connected layer containing 2 softmax units. The ﬁrst unit outputs the
probability that the two images are same and the latter  the probability that the two are different.
One key advantage of the Normalized X-Corr model is that it has about half the number of parameters
(about 1.121 million) as the model proposed by Ahmed et al. [2] (refer supplementary section for
more details).

3.1.2 Fused Model

While the Normalized X-Corr model incorporates inexact matching over a wider search space to
handle important challenges such as illumination variations  partial occlusions  or wide viewpoint
changes  however it also suffers from occasional false matches. Upon investigation  we found that
these false matches tended to recur  especially when the background of the false matches had a similar
appearance to the person being matched (see supplementary). For such cases  an exact matching 
such as taking a difference and constraining the search window might be beneﬁcial. We therefore
fuse the model proposed by Ahmed et al. [2] with Normalized X-Corr to obtain a Fused model  in
anticipation that it incorporates the beneﬁts of both models. Figure 3 shows a representative diagram.
We keep the tied convolution layers unchanged like before  then we fork off two separate pipelines:
one for Normalized X-Corr and the other for Ahmed et. al.’s model [2]. The two separate pipelines
output a 2125-dimensional vector each and then they are fused in a 1000-node fully connected layer.
The outputs from the fully connected layer are then fed into a 2 unit softmax layer as before.

3.2 Training Algorithm

All the proposed architectures are trained using the Stochastic Gradient Descent (SGD) algorithm  as
in Ahmed et al. [2]. The gradient computation is fairly simple except for the Normalized Correlation
layer. Given two matrices  E (from the ﬁrst branch of the Siamese network) and F (from the second
branch of the Siamese network)  represented by a N-dimensional vector each  the gradient pushed
from the Normalized Correlation layer back to the convolution layers on the top branch is given by:

(cid:18) Fi − µF

σF

5

∂normxcorr(E  F )

∂Ei

=

1

(N − 1)σE

− normxcorr(E  F )(Ei − µE)

σE

(cid:19)

 

where Ei is the ith element of the vector representing E and other symbols have their usual meaning. Similar
notation is used for the subnetwork at the bottom. The full derivation is available in the supplementary section.

4 Experiments

Table 1: Performance of different algorithms at ranks 1  10  and 20 on CUHK03 Labeled (left) and
CUHK03 Detected (right) Datasets.

Method

Fused Model (ours)
Norm X-Corr (ours)
Ensembles [21]
LOMO+MLAPG [5]
Ahmed et al. [2]
LOMO+XQDA [22]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

r = 1
72.43
64.73
62.1
57.96
54.74
52.20
20.65
14.17
13.51
8.76

r = 10
95.51
92.77
92.30
94.74
93.88
92.14
68.74
52.57
52.13
38.28

r = 20
98.40
96.78
97.20
98.00
98.10
96.25
83.06
70.03
70.81
53.44

Method

Fused Model (ours)
Norm X-Corr (ours)
LOMO+MLAPG [5]
Ahmed et al. [2]
LOMO+XQDA [22]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

r = 1
72.04
67.13
51.15
44.96
46.25
19.89
11.70
10.92
7.68

r = 10
96.00
94.49
92.05
83.47
88.55
64.79
48.08
47.01
33.38

r = 20
98.26
97.66
96.90
93.15
94.25
81.14
64.86
65.00
50.58

4.1 Datasets  Evaluation Protocol  Baseline Methods

We conducted experiments on the large CUHK03 dataset [3]  the mid-sized CUHK01 Dataset [23]  and the small
QMUL GRID dataset [27]. The datasets are divided into training and test sets for our experiments. The goal of
every algorithm is to rank images in the gallery image bank of the test set by their similarity to a probe image
(which is also from the test set). To do so  they can exploit the training set  consisting of matched and unmatched
image pairs. An oracle would always rank the ground truth match (from the gallery) in the ﬁrst position. All
our experiments are conducted in the single shot setting  i.e. there is exactly one image of every person in the
gallery image bank and the results averaged over 10 test trials are reported using tables and Cumulative Matching
Characteristics (CMC) Curves (see supplementary). For all our experiments  we use a momentum of 0.9  starting
learning rate of 0.05  learning rate decay of 1 × 10−4  weight decay of 5 × 10−4. The implementation was done
in a machine with NVIDIA Titan GPUs and the code was implemented using Torch and is available online 1. We
also conducted an ablation study  to further analyze the contribution of the individual components of our model.
CUHK03 Dataset: The CUHK03 dataset is a large collection of 13 164 images of 1360 people captured from 6
different surveillance cameras  with each person observed by 2 cameras with disjoint views [3]. The dataset
comes with manual and algorithmically labeled pedestrian bounding boxes. In this work  we conduct experiments
on both these sets. For our experiments  we follow the protocol used by Ahmed et al. [2] and randomly pick a
set of 1260 identities for training and 100 for testing. We use 100 identities from the training set for validation.
We compare the performance of both Normalized X-Corr and the Fused model with several baselines for both
labeled [2  3  5  14  18  21  22  25] and detected [2  3  5  14  18  22  25] sets. Of these  the comparison with
Ahmed et al. [2] and with Li et al. [3] is of special interest to us since these are deep learning approaches as well.
For our models  we use mini-batch sizes of 128 and train our models for about 200 000 iterations.
CUHK01 Dataset: The CUHK01 dataset is a mid-sized collection of 3 884 images of 971 people  with each
person observed by 2 cameras with disjoint views [23]. There are 4 images of every identity. For our experiments 
we follow the protocol used by Ahmed et al. [2] and conduct 2 sets of experiments with varying training set
sizes. In the ﬁrst  we randomly pick a set of 871 identities for training and 100 for testing  while in the second 
486 identities are used for testing and the rest for training. We compare the performance of both of our models
with several baselines for both 100 test identities [2  3  14  18  25] and 486 test identities [2  8  9  20  21]. For
our models  we use mini-batch sizes of 128 and train our models for about 50 000 iterations.
QMUL GRID Dataset: The QMUL underGround Re-Identiﬁcation (GRID) dataset is a small and a very
challenging dataset [27]. It is a collection of only 250 people captured from 2 views. Besides  the 2 images
of every identity  there are 775 unmatched images  i.e. for these identities only 1 view is available. For our
experiments  we follow the protocol used by Liao and Li [5]. We randomly pick a set of 125 identities (who
have 2 views each) for training and leave the remaining 125 for testing. Additionally  the gallery image bank of
the test is enriched with the 775 unmatched images. This makes the ranking task even more challenging. We
compare the performance of both of our models with several baselines [11  12  15  19  22  24]. For our models 
we use mini-batch sizes of 128 and train our models for about 20 000 iterations.

1https://github.com/InnovArul/personreid_normxcorr

6

Table 2: Performance of different algorithms at ranks 1  10  and 20 on CUHK01 100 Test Ids (left)
and 486 Test Ids (right) Datasets

Method

Fused Model (ours)
Norm X-Corr (ours)
Ahmed et al. [2]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

r = 1
81.23
77.43
65.00
27.87
29.40
26.45
22.84

r = 10
97.39
96.67
93.12
73.46
72.43
72.04
57.67

r = 20
98.60
98.40
97.20
86.31
86.07
84.69
69.84

Method

Fused Model (ours)
Norm X-Corr (ours)
CPDL [8]
Ensembles [21]
Ahmed et al. [2]
Mirror-KFMA [20]
Mid-Level Filters [9]

r = 1
65.04
60.17
59.5
51.9
47.50
40.40
34.30

r = 10
89.76
86.26
89.70
83.00
80.00
75.3
65.00

r = 20
94.49
91.47
93.10
89.40
87.44
84.10
74.90

4.2 Training Strategies for the Neural Network

The large number of parameters of a deep neural network necessitate special training strategies [2]. In this work 
we adopt 3 main strategies to train our model.
Data Augmentation: For almost all the datasets  the number of negative pairs far outnumbers the number of
positive pairs in the training set. This poses a serious challenge to deep neural nets  which can overﬁt and get
biased in the process. Further  the positive samples may not have all the variations likely to be encountered in a
real scenario. We therefore  hallucinate positive pairs and enrich the training corpus  along the lines of Ahmed
et al. [2]. For every image in the training set of size W×H  we sample 2 images for CUHK03 (5 images for
CUHK01 & QMUL) around the original image center and apply 2D translations chosen from a uniform random
distribution in the range of [−0.05W  0.05W ] × [−0.05H  0.05H]. We also augment the data with images
reﬂected on a vertical mirror.
Fine-Tuning: For small datasets such as QMUL  training parameter-intensive models such as deep neural
networks can be a signiﬁcant challenge. One way to mitigate this issue is to ﬁne-tune the model while training.
We start with a model pre-trained on a large dataset such as CUHK01 with 871 training identities rather than
an untrained model and then reﬁne this pre-trained model by training on the small dataset  QMUL in our case.
During ﬁne-tuning  we use a learning rate of 0.001.
Others: Training deep neural networks is time taking. Therefore  to speed up the training  we implemented our
code such that it spawns threads across multiple GPUs.

5 Results and Discussion

CUHK03 Dataset: Table 1 summarizes the results of the experiments on the CUHK03 Labeled dataset. Our
Fused model outperforms the existing state-of-the-art models by a wide margin  of about 10% (about 72% vs.
62%) on rank-1 accuracy  while the Normalized X-Corr model gives a 3% gain. This serves as a promising
response to our key research endeavor for an effective deep learning model for person re-identiﬁcation. Further 
both models are signiﬁcantly better than Ahmed et al.’s model [2]. We surmise that this is because our models
are more adept at handling variations in illumination  partial occlusion and viewpoint change.
Interestingly  we also note that the existing best performing system is a non-deep approach. This shows that
designing an effective deep learning architecture is a fairly non-trivial task. Our models’ performance  viz.-a-viz.
non-deep methods  once again underscores the beneﬁts of learning representations from the data rather than
using hand-engineered ones. A visualization of some ﬁlter responses of our model  some of the result plots  and
some ranked matching results may be found in the supplementary material.
Table 1 also presents the results on the CUHK03 Detected dataset. Here too  we see the superior performance of
our models over the existing state-of-the-art baselines. Interestingly  here our models take a wider lead over the
existing baselines (about 21%) and our models’ performance rivals its own performance on the Labeled dataset.
We hypothesize that incorporating a wider search space makes our models more robust to the challenges posed
by images in which the person is not centered  such as the CUHK03 Detected dataset.
CUHK01 Dataset: Table 2 summarizes the results of the experiments on the CUHK01 dataset with 100 and
486 test identities. For the 486 test identity setting  our models were pre-trained on the training set of the larger
CUHK03 Labeled dataset and then ﬁne-tuned on the CUHK01-486 training set  owing to the paucity of training
data. As the tables show  our models give us a gain of upto 16% over the existing state-of-the-art on the rank-1
accuracies.
QMUL GRID Dataset: QMUL GRID is a challenging dataset for person re-identiﬁcation due to its small size
and the additional 775 unmatched gallery images in the test set. This is evident from the low performances of

7

Table 3: Performance of different algorithms at ranks 1  5  10  and 20 on the QMUL GRID Dataset

Method

Fused Model (ours)
Norm X-Corr (ours)
KEPLER [24]
LOMO+XQDA [22]
PolyMap [11]
MtMCML [19]
MRank-RankSVM [15]
MRank-PRDC [15]
LCRML [12]
XQDA [22]

r = 1
19.20
16.00
18.40
16.56
16.30
14.08
12.24
11.12
10.68
10.48

r = 5
38.40
32.00
39.12
33.84
35.80
34.64
27.84
26.08
25.76
28.08

r = 10
53.6
40.00
50.24
41.84
46.00
45.84
36.32
35.76
35.04
38.64

r = 20 Deep Learning Model
66.4
55.2
61.44
52.40
57.60
59.84
46.56
46.56
46.48
52.56

Yes
Yes
No
No
No
No
No
No
No
No

existing state-of-the-art algorithms. In order to train our models on this small dataset  we start with a model
trained on CUHK01 dataset with 100 test identities  then we ﬁne-tune the models on the QMUL GRID training
set. Table 3 summarizes the results of the experiments on the QMUL GRID dataset. Here too  our Fused model
performs the best. Even though  our gain in rank-1 accuracy is a modest 1% but we believe this is signiﬁcant for
a challenging dataset like QMUL.
The ablation study across multiple datasets reveals that a wider search and inexact match each buy us at least
6% individually  in terms of performance. The supplementary presents these results in more detail and also
compares the number of parameters across different models. Multi-GPU training  on the other hand gives us a
3x boost to training speed.

6 Conclusions and Future Work

In this work  we address the central research question of proposing simple yet effective deep-learning models for
Person Re-Identiﬁcation by proposing two new models. Our models are capable of handling the key challenges
of illumination variations  partial occlusions and viewpoint invariance by incorporating inexact matching over a
wider search space. Additionally  the proposed Normalized X-Corr model beneﬁts from having fewer parameters
than the state-of-the-art deep learning model. The Fused model  on the other hand  allows us to cut down on
false matches resulting from a wide matching search space  yielding superior performance.
For future work  we intend to use the proposed Siamese architectures for other matching tasks in Vision such
as content-based image retrieval. We also intend to explore the effect of incorporating more feature maps on
performance.

References
[1] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[2] Ejaz Ahmed  Michael Jones  and Tim K Marks. An improved deep learning architecture for person
re-identiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 3908–3916  2015.

[3] Wei Li  Rui Zhao  Tong Xiao  and Xiaogang Wang. Deepreid: Deep ﬁlter pairing neural network for person
re-identiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 152–159  2014.

[4] David G Lowe. Distinctive image features from scale-invariant keypoints.

computer vision  60(2):91–110  2004.

International journal of

[5] Shengcai Liao and Stan Z Li. Efﬁcient psd constrained asymmetric metric learning for person re-
identiﬁcation. In Proceedings of the IEEE International Conference on Computer Vision  pages 3685–3693 
2015.

[6] Rui Zhao  Wanli Ouyang  and Xiaogang Wang. Person re-identiﬁcation by salience matching.

Proceedings of the IEEE International Conference on Computer Vision  pages 2528–2535  2013.

In

[7] Michela Farenzena  Loris Bazzani  Alessandro Perina  Vittorio Murino  and Marco Cristani. Person
re-identiﬁcation by symmetry-driven accumulation of local features. In Computer Vision and Pattern
Recognition (CVPR)  2010 IEEE Conference on  pages 2360–2367. IEEE  2010.

8

[8] Sheng Li  Ming Shao  and Yun Fu. Cross-view projective dictionary learning for person re-identiﬁcation. In
Proceedings of the 24th International Conference on Artiﬁcial Intelligence  AAAI Press  pages 2155–2161 
2015.

[9] Rui Zhao  Wanli Ouyang  and Xiaogang Wang. Learning mid-level ﬁlters for person re-identiﬁcation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 144–151  2014.

[10] Bryan Prosser  Wei-Shi Zheng  Shaogang Gong  Tao Xiang  and Q Mary. Person re-identiﬁcation by

support vector ranking. In BMVC  volume 2  page 6  2010.

[11] Dapeng Chen  Zejian Yuan  Gang Hua  Nanning Zheng  and Jingdong Wang. Similarity learning on an
explicit polynomial kernel feature map for person re-identiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  pages 1565–1573  2015.

[12] Jiaxin Chen  Zhaoxiang Zhang  and Yunhong Wang. Relevance metric learning for person re-identiﬁcation
by exploiting global similarities. In Pattern Recognition (ICPR)  2014 22nd International Conference on 
pages 1657–1662. IEEE  2014.

[13] Jason V Davis  Brian Kulis  Prateek Jain  Suvrit Sra  and Inderjit S Dhillon. Information-theoretic metric
learning. In Proceedings of the 24th international conference on Machine learning  pages 209–216. ACM 
2007.

[14] Matthieu Guillaumin  Jakob Verbeek  and Cordelia Schmid. Is that you? metric learning approaches for
face identiﬁcation. In 2009 IEEE 12th International Conference on Computer Vision  pages 498–505.
IEEE  2009.

[15] Chen Change Loy  Chunxiao Liu  and Shaogang Gong. Person re-identiﬁcation by manifold ranking. In

2013 IEEE International Conference on Image Processing  pages 3567–3571. IEEE  2013.

[16] Wei-Shi Zheng  Shaogang Gong  and Tao Xiang. Reidentiﬁcation by relative distance comparison. IEEE

transactions on pattern analysis and machine intelligence  35(3):653–668  2013.

[17] Martin Hirzer  Peter M Roth  and Horst Bischof. Person re-identiﬁcation by efﬁcient impostor-based
metric learning. In Advanced Video and Signal-Based Surveillance (AVSS)  2012 IEEE Ninth International
Conference on  pages 203–208. IEEE  2012.

[18] Martin Köstinger  Martin Hirzer  Paul Wohlhart  Peter M Roth  and Horst Bischof. Large scale metric
learning from equivalence constraints. In Computer Vision and Pattern Recognition (CVPR)  2012 IEEE
Conference on  pages 2288–2295. IEEE  2012.

[19] Lianyang Ma  Xiaokang Yang  and Dacheng Tao. Person re-identiﬁcation over camera networks using

multi-task distance metric learning. IEEE Transactions on Image Processing  23(8):3656–3670  2014.

[20] Ying-Cong Chen  Wei-Shi Zheng  and Jianhuang Lai. Mirror representation for modeling view-speciﬁc

transform in person re-identiﬁcation. In Proc. IJCAI  pages 3402–3408. Citeseer  2015.

[21] Sakrapee Paisitkriangkrai  Chunhua Shen  and Anton van den Hengel. Learning to rank in person re-
identiﬁcation with metric ensembles. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 1846–1855  2015.

[22] Shengcai Liao  Yang Hu  Xiangyu Zhu  and Stan Z Li. Person re-identiﬁcation by local maximal occurrence
representation and metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 2197–2206  2015.

[23] Wei Li  Rui Zhao  and Xiaogang Wang. Human reidentiﬁcation with transferred metric learning. In Asian

Conference on Computer Vision  pages 31–44. Springer  2012.

[24] Niki Martinel  Christian Micheloni  and Gian Luca Foresti. Kernelized saliency-based person re-
identiﬁcation through multiple metric learning. IEEE Transactions on Image Processing  24(12):5645–
5658  2015.

[25] Rui Zhao  Wanli Ouyang  and Xiaogang Wang. Unsupervised salience learning for person re-identiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 3586–3593 
2013.

[26] Dong Yi  Zhen Lei  Shengcai Liao  Stan Z Li  et al. Deep metric learning for person re-identiﬁcation. In

ICPR  volume 2014  pages 34–39  2014.

[27] Chen Change Loy  Tao Xiang  and Shaogang Gong. Multi-camera activity correlation analysis.

In
Computer Vision and Pattern Recognition  2009. CVPR 2009. IEEE Conference on  pages 1988–1995.
IEEE  2009.

9

,Nicholas Ruozzi
Arulkumar Subramaniam
Moitreya Chatterjee
Anurag Mittal