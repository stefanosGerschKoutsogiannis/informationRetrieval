2018,DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors,Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However  previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available.,DVAE#: Discrete Variational Autoencoders with

Relaxed Boltzmann Priors

Arash Vahdat∗  Evgeny Andriyash∗  William G. Macready

{arash evgeny bill}@quadrant.ai

Quadrant.ai  D-Wave Systems Inc.

Burnaby  BC  Canada

Abstract

Boltzmann machines are powerful distributions that have been shown to be an
effective prior over binary latent variables in variational autoencoders (VAEs).
However  previous methods for training discrete VAEs have used the evidence lower
bound and not the tighter importance-weighted bound. We propose two approaches
for relaxing Boltzmann machines to continuous distributions that permit training
with importance-weighted bounds. These relaxations are based on generalized
overlapping transformations and the Gaussian integral trick. Experiments on the
MNIST and OMNIGLOT datasets show that these relaxations outperform previous
discrete VAEs with Boltzmann priors. An implementation which reproduces these
results is available at https://github.com/QuadrantAI/dvae.

1

Introduction

Advances in amortized variational inference [1  2  3  4] have enabled novel learning methods [4  5  6]
and extended generative learning into complex domains such as molecule design [7  8]  music [9] and
program [10] generation. These advances have been made using continuous latent variable models in
spite of the computational efﬁciency and greater interpretability offered by discrete latent variables.
Further  models such as clustering  semi-supervised learning  and variational memory addressing [11]
all require discrete variables  which makes the training of discrete models an important challenge.
Prior to the deep learning era  Boltzmann machines were widely used for learning with discrete latent
variables. These powerful multivariate binary distributions can represent any distribution deﬁned on
a set of binary random variables [12]  and have seen application in unsupervised learning [13]  super-
vised learning [14  15]  reinforcement learning [16]  dimensionality reduction [17]  and collaborative
ﬁltering [18]. Recently  Boltzmann machines have been used as priors for variational autoencoders
(VAEs) in the discrete variational autoencoder (DVAE) [19] and its successor DVAE++ [20]. It has
been demonstrated that these VAE models can capture discrete aspects of data. However  both these
models assume a particular variational bound and tighter bounds such as the importance weighted
(IW) bound [21] cannot be used for training.
We remove this constraint by introducing two continuous relaxations that convert a Boltzmann ma-
chine to a distribution over continuous random variables. These relaxations are based on overlapping
transformations introduced in [20] and the Gaussian integral trick [22] (known as the Hubbard-
Stratonovich transform [23] in physics). Our relaxations are made tunably sharp by using an inverse
temperature parameter.
VAEs with relaxed Boltzmann priors can be trained using standard techniques developed for continu-
ous latent variable models. In this work  we train discrete VAEs using the same IW bound on the
log-likelihood that has been shown to improve importance weighted autoencoders (IWAEs) [21].

∗Equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

This paper makes two contributions: i) We introduce two continuous relaxations of Boltzmann
machines and use these relaxations to train a discrete VAE with a Boltzmann prior using the IW bound.
ii) We generalize the overlapping transformations of [20] to any pair of distributions with computable
probability density function (PDF) and cumulative density function (CDF). Using these more general
overlapping transformations  we propose new smoothing transformations using mixtures of Gaussian
and power-function [24] distributions. Power-function overlapping transformations provide lower
variance gradient estimates and improved test set log-likelihoods when the inverse temperature is
large. We name our framework DVAE# because the best results are obtained when the power-function
transformations are sharp.2

1.1 Related Work

Previous work on training discrete latent variable models can be grouped into ﬁve main categories:

i) Exhaustive approaches marginalize all discrete variables [25  26] and which are not scalable to

more than a few discrete variables.

ii) Local expectation gradients [27] and reparameterization and marginalization [28] estimators
compute low-variance estimates at the cost of multiple function evaluations per gradient. These
approaches can be applied to problems with a moderate number of latent variables.

iii) Relaxed computation of discrete densities [29] replaces discrete variables with continuous
relaxations for gradient computation. A variation of this approach  known as the straight-through
technique  sets the gradient of binary variables to the gradient of their mean [30  31].

iv) Continuous relaxations of discrete distributions [32] replace discrete distributions with con-
tinuous ones and optimize a consistent objective. This method cannot be applied directly to
Boltzmann distributions. The DVAE [19] solves this problem by pairing each binary variable
with an auxiliary continuous variable. This approach is described in Sec. 2.

v) The REINFORCE estimator [33] (also known as the likelihood ratio [34] or score-function
estimator) replaces the gradient of an expectation with the expectation of the gradient of the
score function. This estimator has high variance  but many increasingly sophisticated methods
provide lower variance estimators. NVIL [3] uses an input-dependent baseline  and MuProp [35]
uses a ﬁrst-order Taylor approximation along with an input-dependent baseline to reduce noise.
VIMCO [36] trains an IWAE with binary latent variables and uses a leave-one-out scheme to
deﬁne the baseline for each sample. REBAR [37] and its generalization RELAX [38] use the
reparameterization of continuous distributions to deﬁne baselines.

The method proposed here is of type iv) and differs from [19  20] in the way that binary latent
variables are marginalized. The resultant relaxed distribution allows for DVAE training with a tighter
bound. Moreover  our proposal encompasses a wider variety of smoothing methods and one of these
empirically provides lower-variance gradient estimates.

2 Background

Let xxx represent observed random variables and ζζζ continuous latent variables. We seek a generative
model p(xxx  ζζζ) = p(ζζζ)p(xxx|ζζζ) where p(ζζζ) denotes the prior distribution and p(xxx|ζζζ) is a probabilistic
decoder. In the VAE [1]  training maximizes a variational lower bound on the marginal log-likelihood:

(cid:2)log p(xxx|ζζζ)(cid:3)

− KL(cid:0)q(ζζζ|xxx)||p(ζζζ)(cid:1).

log p(xxx) ≥ Eq(ζζζ|xxx)

A probabilistic encoder q(ζζζ|xxx) approximates the posterior over latent variables. For continuous ζζζ 
the bound is maximized using the reparameterization trick. With reparameterization  expectations
with respect to q(ζζζ|xxx) are replaced by expectations against a base distribution and a differentiable
function that maps samples from the base distribution to q(ζζζ|xxx). This can always be accomplished
when q(ζζζ|xxx) has an analytic inverse cumulative distribution function (CDF) by mapping uniform
samples through the inverse CDF. However  reparameterization cannot be applied to binary latent
variables because the CDF is not differentiable.

2And not because our model is proposed after DVAE and DVAE++.

2

(cid:26)δ(ζ)

where r(ζζζ|zzz) =(cid:81)

The DVAE [19] resolves this issue by pairing each binary latent variable with a continuous counterpart.
Denoting a binary vector of length D by zzz ∈ {0  1}D  the Boltzmann prior is p(zzz) = e−Eθθθ(zzz)/Zθθθ
where Eθθθ(zzz) = −aaaT zzz − 1
2zzzT WWWzzz is an energy function with parameters θθθ ≡ {WWW   aaa} and partition
function Zθθθ. The joint model over discrete and continuous variables is p(xxx  zzz  ζζζ) = p(zzz)r(ζζζ|zzz)p(xxx|ζζζ)
i r(ζi|zi) is a smoothing transformation that maps each discrete zi to its continuous
analogue ζi.
DVAE [19] and DVAE++ [20] differ in the type of smoothing transformations r(ζ|z): [19] uses
spike-and-exponential transformation (Eq. (1) left)  while [20] uses two overlapping exponential
distributions (Eq. (1) right). Here  δ(ζ) is the (one-sided) Dirac delta distribution  ζ ∈ [0  1]  and Zβ
is the normalization constant:

(cid:26)e−βζ/Zβ
The variational bound for a factorial approximation to the posterior where q(ζζζ|xxx) =(cid:81)
q(zzz|xxx) =(cid:81)
(cid:2)Eq(zzz|xxx ζζζ) log p(zzz))(cid:3)  
Here q(ζi|xxx) =(cid:80)
1) with weights q(zi|xxx). The probability of binary units conditioned on ζi  q(zzz|xxx  ζζζ) =(cid:81)

i q(ζi|xxx) and
(2)
q(zi|xxx)r(ζi|zi) is a mixture distribution combining r(ζi|zi = 0) and r(ζi|zi =
i q(zi|xxx  ζi) 
can be computed analytically. H(q(zzz|xxx)) is the entropy of q(zzz|xxx). The second and third terms in
Eq. (2) have analytic solutions (up to the log normalization constant) that can be differentiated easily
with an automatic differentiation (AD) library. The expectation over q(ζζζ|xxx) is approximated with
reparameterized sampling.
We extend [19  20] to tighten the bound of Eq. (2) by importance weighting [21  39]. These tighter
bounds are shown to improve VAEs. For continuous latent variables  the K-sample IW bound is

i q(zi|xxx) is derived in [20] as
log p(xxx) ≥ Eq(ζζζ|xxx) [log p(xxx|ζζζ)] + H(q(zzz|xxx)) + Eq(ζζζ|xxx)

if z = 0
otherwise  

r(ζ|z) =

eβ(ζ−1)/Zβ

r(ζ|z) =

eβ(ζ−1)/Zβ

if z = 0
otherwise .

(1)

zi

(cid:33)(cid:35)

log p(xxx) ≥ LK(xxx) = E

ζζζ(k)∼q(ζζζ|xxx)

log

1
K

The tightness of the IW bound improves as K increases [21].

3 Model

(cid:34)

(cid:32)

K(cid:88)

k=1

p(ζζζ (k))p(xxx|ζζζ (k))

q(ζζζ (k)|xxx)

.

(3)

We introduce two relaxations of Boltzmann machines to deﬁne the continuous prior distribution
p(ζζζ) in the IW bound of Eq. (3). These relaxations rely on either overlapping transformations
(Sec. 3.1) or the Gaussian integral trick (Sec. 3.2). Sec. 3.3 then generalizes the class of overlapping
transformations that can be used in the approximate posterior q(ζζζ|xxx).
3.1 Overlapping Relaxations

We obtain a continuous relaxation of p(zzz) through the marginal p(ζζζ) = (cid:80)
and ζζζ independently; i.e.  r(ζζζ|zzz) =(cid:81)
r(ζζζ|zzz) approaches δ(ζζζ − zzz) and p(ζζζ) =(cid:80)

z p(zzz)r(ζζζ|zzz) where
r(ζζζ|zzz) is an overlapping smoothing transformation [20] that operates on each component of zzz
i r(ζi|zi). Overlapping transformations such as mixture of
exponential in Eq. (1) may be used for r(ζζζ|zzz). These transformations are equipped with an inverse
temperature hyperparameter β to control the sharpness of the smoothing transformation. As β → ∞ 
z p(zzz)δ(ζζζ − zzz) becomes a mixture of 2D delta function
distributions centered on the vertices of the hypercube in RD. At ﬁnite β  p(ζζζ) provides a continuous
relaxation of the Boltzmann machine.
To train an IWAE using Eq. (3) with p(ζζζ) as a prior  we must compute log p(ζζζ) and its gradient
with respect to the parameters of the Boltzmann distribution and the approximate posterior. This
computation involves marginalization over zzz  which is generally intractable. However  we show that
this marginalization can be approximated accurately using a mean-ﬁeld model.

3.1.1 Computing log p(ζζζ) and its Gradient for Overlapping Relaxations

Since overlapping transformations are factorial  the log marginal distribution of ζζζ is

(cid:17)

(cid:16)(cid:88)

e−Eθθθ(zzz)+bbbβ (ζζζ)T zzz+cccβ (ζζζ)(cid:17)

log p(ζζζ) = log

p(zzz)r(ζζζ|zzz)

= log

− log Zθθθ 

(4)

(cid:16)(cid:88)

zzz

zzz

3

i (ζζζ) = −βζi − log Zβ.

i (ζζζ) = β(2ζi − 1) and cβ

i (ζζζ) = log r(ζi|zi = 0). For the mixture of

i (ζζζ) = log r(ζi|zi = 1) − log r(ζi|zi = 0) and cβ

where bβ
exponential smoothing bβ
The ﬁrst term in Eq. (4) is the log partition function of the Boltzmann machine ˆp(zzz) with augmented
energy function ˆEβ
θθθ ζζζ(zzz) := Eθθθ(zzz)−bbbβ(ζζζ)T zzz−cccβ(ζζζ). Estimating the log partition function accurately
can be expensive  particularly because it has to be done for each ζζζ. However  we note that each ζi
comes from a bimodal distribution centered at zero and one  and that the bias bbbβ(ζζζ) is usually large
for most components i (particularly for large β). In this case  mean ﬁeld is likely to provide a good
approximation of ˆp(zzz)  a fact we demonstrate empirically in Sec. 4.

To compute log p(ζζζ) and its gradient  we ﬁrst ﬁt a mean-ﬁeld distribution m(zzz) =(cid:81)

i mi(zi) by

minimizing KL(m(zzz)||ˆp(zzz)) [40]. The gradient of log p(ζζζ) with respect to β  θθθ or ζζζ is:

(cid:2)
(cid:2)
∇ log p(ζζζ) = −Ezzz∼ ˆp(zzz)
∇ ˆEβ
≈ −Ezzz∼m(zzz)
∇ ˆEβ
= −∇ ˆEβ

(cid:2)
∇Eθθθ(zzz)(cid:3)
θθθ ζζζ(zzz)(cid:3) + Ezzz∼p(zzz)
θθθ ζζζ(zzz)(cid:3) + Ezzz∼p(zzz)
∇Eθθθ(zzz)(cid:3)
(cid:2)
(cid:2)
∇Eθθθ(zzz)(cid:3) 

θθθ ζζζ(mmm) + Ezzz∼p(zzz)

(5)
··· mD(zD = 1)] is the mean-ﬁeld solution and where the gradient
where mmmT = [m1(z1 = 1)
does not act on mmm. The ﬁrst term in Eq. (5) is the result of computing the average energy under
a factorial distribution.3 The second expectation corresponds to the negative phase in training
Boltzmann machines and is approximated by Monte Carlo sampling from p(zzz).
To compute the importance weights for the IW bound of Eq. (3) we must compute the value of
≈ 0
(6)

log p(ζζζ) up to the normalization; i.e. the ﬁrst term in Eq. (4). Assuming that KL(cid:0)m(zzz)||ˆp(zzz)(cid:1)
the ﬁrst term of Eq. (4) is approximated as H(cid:0)m(zzz)(cid:1)

KL(m(zzz)||ˆp(zzz)) = ˆEβ

θθθ ζζζ (zzz)(cid:17)

− H(m(zzz)) 

(cid:16)(cid:88)

θθθ ζζζ(mmm) + log

and using

− ˆEβ

e

z

− ˆEβ

θθθ ζζζ(mmm).

3.2 The Gaussian Integral Trick

The computational complexity of log p(ζζζ) arises from the pairwise interactions zzzT WWWzzz present in
Eθθθ(zzz). Instead of applying mean ﬁeld  we remove these interactions using the Gaussian integral
trick [41]. This is achieved by deﬁning Gaussian smoothing:

r(ζζζ|zzz) = N (ζζζ|AAA(WWW + βIII)zzz  AAA(WWW + βIII)AAAT )

for an invertible matrix AAA and a diagonal matrix βIII with β > 0. Here  β must be large enough so that
WWW + βIII is positive deﬁnite. Common choices for AAA include AAA = III or AAA = ΛΛΛ− 1
2 VVV T where VVV ΛΛΛVVV T is
the eigendecomposition of WWW + βIII [41]. However  neither of these choices places the modes of p(ζζζ)
on the vertices of the hypercube in RD. Instead  we take AAA = (WWW + βIII)−1 giving the smoothing
transformation r(ζζζ|zzz) = N (ζζζ|zzz  (WWW + βIII)−1). The joint density is then
2 β111)T zzz 

2 ζζζT (WWW +βIII)ζζζ+zzzT (WWW +βIII)ζζζ+(aaa− 1

p(zzz  ζζζ) ∝ e− 1

where 111 is the D-vector of all ones. Since p(zzz  ζζζ) no longer contains pairwise interactions zzz can be
marginalized out giving

(cid:12)(cid:12)(cid:12)(cid:12) 1

2π

(cid:12)(cid:12)(cid:12)(cid:12) 1

2

2 ζζζT (WWW +βIII)ζζζ(cid:89)

(cid:104)

e− 1

i

(cid:105)

p(ζζζ) = Z−1

θθθ

(WWW + βIII)

1 + eai+ci− β

2

 

(7)

where ci is the ith element of (WWW + βIII)ζζζ.
The marginal p(ζζζ) in Eq. (7) is a mixture of 2D Gaussian distributions centered on the vertices of
the hypercube in RD with mixing weights given by p(zzz). Each mixture component has covariance
ΣΣΣ = (WWW + βIII)−1 and  as β gets large  the precision matrix becomes diagonally dominant. As
θθθ ζζζ(zzz) is a multi-linear function of {zi} and under the mean-ﬁeld assumption each

3The augmented energy ˆEβ

zi is replaced by its average value m(zi = 1).

4

β → ∞  each mixture component becomes a delta function and p(ζζζ) approaches(cid:80)

z p(zzz)δ(ζζζ − zzz).
This Gaussian smoothing allows for simple evaluation of log p(ζζζ) (up to Zθθθ)  but we note that each
mixture component has a nondiagonal covariance matrix  which should be accommodated when
designing the approximate posterior q(ζζζ|xxx).
The hyperparameter β must be larger than the absolute value of the most negative eigenvalue of WWW to
ensure that WWW + βIII is positive deﬁnite. Setting β to even larger values has the beneﬁt of making
the Gaussian mixture components more isotropic  but this comes at the cost of requiring a sharper
approximate posterior with potentially noisier gradient estimates.

3.3 Generalizing Overlapping Transformations
The previous sections developed two r(ζζζ|zzz) relaxations for Boltzmann priors. Depending on this
choice  compatible q(ζζζ|xxx) parameterizations must be used. For example  if Gaussian smoothing is
used  then a mixture of Gaussian smoothers should be used in the approximate posterior. Unfor-
tunately  the overlapping transformations introduced in DVAE++ [20] are limited to mixtures of
exponential or logistic distributions where the inverse CDF can be computed analytically. Here  we
provide a general approach for reparameterizing overlapping transformations that does not require an-
alytic inverse CDFs. Our approach is a special case of the reparameterization method for multivariate
mixture distributions proposed in [42].
Assume q(ζ|xxx) = (1 − q)r(ζ|z = 0) + qr(ζ|z = 1) is the mixture distribution resulting from an
overlapping transformation deﬁned for one-dimensional z and ζ where q ≡ q(z = 1|xxx). Ancestral
sampling from q(ζ|xxx) is accomplished by ﬁrst sampling from the binary distribution q(z|xxx) and then
sampling ζ from r(ζ|z). This process generates samples but is not differentiable with respect to q.
To compute the gradient (with respect to q) of samples from q(ζ|xxx)  we apply the implicit function
theorem. The inverse CDF of q(ζ|xxx) at ρ is obtained by solving:
(8)
where ρ ∈ [0  1] and R(ζ|z) is the CDF for r(ζ|z). Assuming that ζ is a function of q but ρ is not 
we take the gradient from both sides of Eq. (8) with respect to q giving

CDF(ζ) = (1 − q)R(ζ|z = 0) + qR(ζ|z = 1) = ρ 

∂ζ
∂q

=

R(ζ|z = 0) − R(ζ|z = 1)

(1 − q)r(ζ|z = 0) + qr(ζ|z = 1)

 

(9)

which can be easily computed for a sampled ζ if the PDF and CDF of r(ζ|z) are known. This
generalization allows us to compute gradients of samples generated from a wide range of overlapping
transformations. Further  the gradient of ζ with respect to the parameters of r(ζ|z) (e.g. β) is
computed similarly as

∂ζ
∂β

= −

(1 − q) ∂βR(ζ|z = 0) + q ∂βR(ζ|z = 1)

(1 − q)r(ζ|z = 0) + qr(ζ|z = 1)

.

With this method  we can apply overlapping transformations beyond the mixture of exponentials
considered in [20]. The inverse CDF of exponential mixtures is shown in Fig. 1(a) for several β. As
β increases  the relaxation approaches the original binary variables  but this added ﬁdelity comes at
the cost of noisy gradients. Other overlapping transformations offer alternative tradeoffs:
Uniform+Exp Transformation: We ensure that the gradient remains ﬁnite as β → ∞ by mixing
the exponential with a uniform distribution. This is achieved by deﬁning r(cid:48)(ζ|z) = (1 − )r(ζ|z) + 
where r(ζ|z) is the exponential smoothing and ζ ∈ [0  1]. The inverse CDF resulting from this
smoothing is shown in Fig. 1(b).
Power-Function Transformation: Instead of adding a uniform distribution we substitute the expo-
nential distribution for one with heavier tails. One choice is the power-function distribution [24]:

(cid:40) 1
β −1)
β ζ ( 1
β (1 − ζ)( 1

1

r(ζ|z) =

β −1)

if z = 0
otherwise

for ζ ∈ [0  1] and β > 1.

(10)

The conditionals in Eq. (10) correspond to the Beta distributions B(1/β  1) and B(1  1/β) respec-
tively. The inverse CDF resulted from this smoothing is visualized in Fig. 1(c).

5

(a) Exponential Transformation

Figure 1: In the ﬁrst row  we visualize the inverse CDF of the mixture q(ζ) =(cid:80)

(b) Uniform+Exp Transformation (c) Power-Function Transformation
z q(z)r(ζ|z) for
q = q(z = 1) = 0.5 as a function of the random noise ρ ∈ [0  1]. In the second row  the gradient of
the inverse CDF with respect to q is visualized. Each column corresponds to a different smoothing
transformation. As the transition region sharpens with increasing β  a sampling based estimate of the
gradient becomes noisier; i.e.  the variance of ∂ζ/∂q increases. The uniform+exp exponential has a
very similar inverse CDF (ﬁrst row) to the exponential but has potentially lower variance (bottom
row). In comparison  the power-function smoothing with β = 40 provides a good relaxation of the
discrete variables while its gradient noise is still moderate. See the supplementary material for a
comparison of the gradient noise.

Gaussian Transformations: The transformations introduced above have support ζ ∈ [0  1]. We also
explore Gaussian smoothing r(ζ|z) = N (ζ|z  1
None of these transformations have an analytic inverse CDF for q(ζ|xxx) so we use Eq. (9) to calculate
gradients.

β ) with support ζ ∈ R.

4 Experiments

q(ζζζ|xxx) =(cid:81)G

In this section we compare the various relaxations for training DVAEs with Boltzmann priors on
statically binarized MNIST [43] and OMNIGLOT [44] datasets. For all experiments we use a
generative model of the form p(xxx  ζζζ) = p(ζζζ)p(xxx|ζζζ) where p(ζζζ) is a continuous relaxation obtained
from either the overlapping relaxation of Eq. (4) or the Gaussian integral trick of Eq. (7). The
underlying Boltzmann distribution is a restricted Boltzmann machine (RBM) with bipartite connec-
tivity which allows for parallel Gibbs updates. We use a hierarchical autoregressively-structured
g=1 q(ζζζ g|xxx  ζζζ <g) to approximate the posterior distribution over ζζζ. This structure divides
the components of ζζζ into G equally-sized groups and deﬁnes each conditional using a factorial
distribution conditioned on xxx and all ζζζ from previous groups.
The smoothing transformation used in q(ζζζ|xxx) depends on the type of relaxation used in p(ζζζ). For
overlapping relaxations  we compare exponential  uniform+exp  Gaussian  and power-function. With
the Gaussian integral trick  we use shifted Gaussian smoothing as described below. The decoder
p(xxx|ζζζ) and conditionals q(ζζζ g|xxx  ζζζ <g) are modeled with neural networks. Following [20]  we consider
both linear (—) and nonlinear (∼) versions of these networks. The linear models use a single linear
layer to predict the parameters of the distributions p(xxx|ζζζ) and q(ζζζ g|xxx  ζζζ <g) given their input. The
nonlinear models use two deterministic hidden layers with 200 units  tanh activation and batch-
normalization. We use the same initialization scheme  batch-size  optimizer  number of training
iterations  schedule of learning rate  weight decay and KL warm-up for training that was used in [20]
(See Sec. 7.2 in [20]). For the mean-ﬁeld optimization  we use 5 iterations. To evaluate the trained
models  we estimate the log-likelihood on the discrete graphical model using the importance-weighted

6

01ρ01ζBernoulliβ=14β=12β=801ρ01ζBernoulliβ=14β=12β=801ρ01ζBernoulliβ=40β=20β=1001ρ020406080∂ζ∂qβ=14β=12β=801ρ0481216∂ζ∂qβ=14β=12β=801ρ05101520∂ζ∂qβ=40β=20β=10(cid:0)ζi|µi + ∆µi(ζζζ <i)  σi

(cid:1) where ∆µi(ζζζ <i) is linear in ζζζ <i. Motivated by this observation  we

bound with 4000 samples [21]. At evaluation p(ζζζ) is replaced with the Boltzmann distribution p(zzz) 
and q(ζζζ|xxx) with q(zzz|xxx) (corresponding to β = ∞).
For DVAE  we use the original spike-and-exp smoothing. For DVAE++  in addition to exponential
smoothing  we use a mixture of power-functions. The DVAE# models are trained using the IW bound
in Eq. (3) with K = 1  5  25 samples. To fairly compare DVAE# with DVAE and DVAE++ (which
can only be trained with the variational bound)  we use the same number of samples K ≥ 1 when
estimating the variational bound during DVAE and DVAE++ training.
The smoothing parameter β is ﬁxed throughout training (i.e. β is not annealed). However  since β
acts differently for each smoothing function r  its value is selected by cross validation per smoothing
and structure. We select from β ∈ {4  5  6  8} for spike-and-exp  β ∈ {8  10  12  16} for exponential 
β ∈ {16  20  30  40} with  = 0.05 for uniform+exp  β ∈ {15  20  30  40} for power-function  and
β ∈ {20  25  30  40} for Gaussian smoothing. For models other than the Gaussian integral trick  β is
set to the same value in q(ζζζ|xxx) and p(ζζζ). For the Gaussian integral case  β in the encoder is trained
as discussed next  but is selected in the prior from β ∈ {20  25  30  40}.
With the Gaussian integral trick  each mixture component in the prior contains off-diagonal cor-
(cid:81)
relations and the approximation of the posterior over ζζζ should capture this. We recall that a mul-
tivariate Gaussian N (ζζζ|µµµ  ΣΣΣ) can always be represented as a product of Gaussian conditionals
i N
provide ﬂexibility in the approximate posterior q(ζζζ|xxx) by using shifted Gaussian smoothing where
r(ζi|zi) = N (ζi|zi + ∆µi(ζζζ <i)  1/βi)  and ∆µi(ζζζ <i) is an additional parameter that shifts the
distribution. As the approximate posterior in our model is hierarchical  we generate ∆µi(ζζζ <g) for
the ith element in gth group as the output of the same neural network that generates the parameters
of q(ζζζ g|xxx  ζζζ <g). The parameter βi for each component of ζζζ g is a trainable parameter shared for all xxx.
Training also requires sampling from the discrete RBM to compute the θθθ-gradient of log Zθθθ. We
have used both population annealing [45] with 40 sweeps across variables per parameter update
and persistent contrastive divergence [46] for sampling. Population annealing usually results in a
better generative model (see the supplementary material for a comparison). We use QuPA4  a GPU
implementation of population annealing. To obtain test set log-likelihoods we require log Zθθθ  which
we estimate with annealed importance sampling [47  48]. We use 10 000 temperatures and 1 000
samples to ensure that the standard deviation of the log Zθθθ estimate is small (∼ 0.01).
We compare the performance of DVAE# against DVAE and DVAE++ in Table 1. We consider four
neural net structures when examining the various smoothing models. Each structure is denoted
“G —/∼” where G represent the number of groups in the approximate posterior and —/∼ indicates
linear/nonlinear conditionals. The RBM prior for the structures “1 —/∼” is 100× 100 (i.e. D = 200)
and for structures “2/4 ∼” the RBM is 200 × 200 (i.e. D = 400).
We make several observations based on Table 1: i) Most baselines improve as K increases. The
improvements are generally larger for DVAE# as they optimize the IW bound. ii) Power-function
smoothing improves the performance of DVAE++ over the original exponential smoothing.
iii)
DVAE# and DVAE++ both with power-function smoothing for K = 1 optimizes a similar variational
bound with same smoothing transformation. The main difference here is that DVAE# uses the
marginal p(ζζζ) in the prior whereas DVAE++ has the joint p(zzz  ζζζ) = p(zzz)r(zzz|ζζζ). For this case  it can
be seen that DVAE# usually outperforms DVAE++ . iv) Among the DVAE# variants  the Gaussian
integral trick and Gaussian overlapping relaxation result in similar performance  and both are usually
inferior to the other DVAE# relaxations. v) In DVAE#  the uniform+exp smoothing performs better
than exponential smoothing alone. vi) DVAE# with the power-function smoothing results in the best
generative models  and in most cases outperforms both DVAE and DVAE++.
Given the superior performance of the models obtained using the mean-ﬁeld approximation of
Sec. 3.1.1 to ˆp(ζζζ)  we investigate the accuracy of this approximation. In Fig. 2(a)  we show that the
mean-ﬁeld model converges quickly by plotting the KL divergence of Eq. (6) with the number of
mean-ﬁeld iterations for a single ζζζ. To assess the quality of the mean-ﬁeld approximation  in Fig. 2(b)
we compute the KL divergence for randomly selected ζζζs during training at different iterations for
exponential and power-function smoothings with different βs. As it can be seen  throughout the

4This library is publicly available at https://try.quadrant.ai/qupa

7

(a)

(b)

(c)

Figure 2: (a) The KL divergence between the mean-ﬁeld model and the augmented Boltzmann
machine ˆp(zzz) as a function of the number of optimization iterations of the mean-ﬁeld. The mean-ﬁeld
model converges to KL = 0.007 in three iterations. (b) The KL value is computed for randomly
selected ζs during training at different iterations for exponential and power-function smoothings with
different β. (c) The variance of the gradient of the objective function with respect to the logit of q is
visualized for exponential and power-function smoothing transformations. Power-function smoothing
tends to have lower variance than exponential smoothing. The artifact seen early in training is due to
the warm-up of KL. Models in (b) and (c) are trained for 100K iterations with batch size of 1 000.

DVAE

Exp

Power

1 —

1 ∼

2 ∼

4 ∼

1 —

Struct.

T
S
I
N
M

DVAE++

Power

89.12±0.05
89.09±0.05
89.04±0.07
85.05±0.02
85.29±0.10
85.59±0.10
83.62±0.04
83.57±0.07
83.58±0.15
83.44±0.05
83.17±0.09
83.20±0.08

90.43±0.06
90.13±0.03
89.92±0.07
85.13±0.06
85.13±0.09
86.14±0.18
84.15±0.07
84.85±0.13
85.49±0.12
84.63±0.11
85.41±0.09
85.42±0.07

Table 1: The performance of DVAE# is compared against DVAE and DVAE++ on MNIST and
OMNIGLOT. Mean±standard deviation of the negative log-likelihood for ﬁve runs are reported.
Gaussian
89.35±0.06
91.33±0.13
88.25±0.03
90.15±0.04
87.67±0.07
89.55±0.10
84.93±0.02
86.24±0.05
84.21±0.02
84.91±0.07
83.93±0.06
84.30±0.04
83.37±0.02
84.35±0.04
82.99±0.04
83.61±0.04
82.85±0.03
83.26±0.04
83.18±0.05
84.81±0.19
82.95±0.07
84.20±0.15
83.80±0.04
82.82±0.02
106.81±0.07 107.21±0.14 105.89±0.06 105.47±0.09
106.16±0.11 106.86±0.10 104.94±0.05 104.42±0.09
105.75±0.10 106.88±0.09 104.49±0.07 103.98±0.05
102.74±0.08 102.23±0.08 101.86±0.06 101.70±0.01
102.00±0.09 101.59±0.06 101.22±0.05 101.00±0.02
101.60±0.09 101.48±0.04 100.93±0.07 100.60±0.05
99.75±0.05
102.84±0.23 100.38±0.09 99.84±0.06
101.43±0.11 99.93±0.07
99.57±0.06
99.24±0.05
98.93±0.05
100.45±0.08 100.10±0.28 99.59±0.16
99.65±0.09
103.43±0.10 100.85±0.12 99.92±0.11
99.13±0.10
101.82±0.13 100.32±0.19 99.61±0.07
100.97±0.21 99.92±0.30
99.36±0.09
98.88±0.09

Gauss. Int
Spike-Exp
92.14±0.12
89.00±0.09
91.32±0.09
89.15±0.12
91.18±0.21
89.20±0.13
86.23±0.05
85.48±0.06
84.99±0.03
85.29±0.03
84.36±0.04
85.92±0.10
84.30±0.05
83.97±0.04
83.68±0.02
83.74±0.03
83.39±0.04
84.19±0.21
84.59±0.06
84.38±0.03
83.89±0.09
83.93±0.07
84.12±0.07
83.52±0.06
105.11±0.11 106.71±0.08 105.45±0.08 110.81±0.32
104.68±0.21 106.83±0.09 105.34±0.05 112.26±0.70
104.38±0.15 106.85±0.07 105.38±0.14 111.92±0.30
102.95±0.07 101.84±0.08 101.88±0.06 103.50±0.06
102.45±0.08 102.13±0.11 101.67±0.07 102.15±0.04
102.74±0.05 102.66±0.09 101.80±0.15 101.42±0.04
103.10±0.31 101.34±0.04 100.42±0.03 102.07±0.16
100.88±0.13 100.55±0.09 99.51±0.05
100.85±0.02
100.55±0.08 100.31±0.15 99.49±0.07
100.20±0.02
104.63±0.47 101.58±0.22 100.42±0.08 102.91±0.25
101.79±0.25
101.77±0.20 101.01±0.09 99.52±0.09
100.89±0.13 100.37±0.09 99.43±0.14
100.73±0.08

Un+Exp
89.57±0.08
88.56±0.04
88.02±0.04
85.19±0.05
84.47±0.02
84.22±0.01
83.54±0.06
83.33±0.04
83.30±0.04
83.52±0.06
83.41±0.04
83.39±0.04

K
1
5
25
1
5
25
1
5
25
1
5
25
1
5
25
1
5
25
1
5
25
1
5
25

DVAE#

Exp

90.55±0.11
89.62±0.08
89.27±0.09
85.37±0.05
84.83±0.03
84.69±0.08
83.96±0.06
83.70±0.04
83.76±0.04
84.06±0.06
84.15±0.05
84.22±0.13

T
O
L
G

I
N
M
O

1 ∼

2 ∼

4 ∼

training the KL value is typically < 0.2. For larger βs  the KL value is smaller due to the stronger
bias that bbbβ(ζζζ) imposes on zzz.
Lastly  we demonstrate that the lower variance of power-function smoothing may contribute to its
success. As noted in Fig. 1  power-function smoothing potentially has moderate gradient noise while
still providing a good approximation of binary variables at large β. We validate this hypothesis in
Fig. 2(c) by measuring the variance of the derivative of the variational bound (with K = 1) with
respect to the logit of q during training of a 2-layer nonlinear model on MNIST. When comparing
the exponential (β = 10) to power-function smoothing (β = 30) at the β that performs best for each
smoothing method  we ﬁnd that power-function smoothing has signiﬁcantly lower variance.

8

1510Mean-ﬁeldIterations10−210−1100101KL(m(z)||ˆp(z))20406080100TrainingIterations(x1000)10−510−410−310−210−1100101102KL(m(z)||ˆp(z))powerβ=20powerβ=40exponentialβ=8exponentialβ=1220406080100TrainingIterations(x1000)10−710−610−5GradientVarianceexponentialDVAE#powerDVAE#exponentialDVAE++powerDVAE++5 Conclusions

We have introduced two approaches for relaxing Boltzmann machines to continuous distributions  and
shown that the resulting distributions can be trained as priors in DVAEs using an importance-weighted
bound. We have proposed a generalization of overlapping transformations that removes the need for
computing the inverse CDF analytically. Using this generalization  the mixture of power-function
smoothing provides a good approximation of binary variables while the gradient noise remains
moderate. In the case of sharp power smoothing  our model outperforms previous discrete VAEs.

References
[1] Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In The International Conference on

Learning Representations (ICLR)  2014.

[2] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approxi-

mate inference in deep generative models. In International Conference on Machine Learning  2014.

[3] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In Interna-

tional Conference on Machine Learning  2014.

[4] Karol Gregor  Ivo Danihelka  Andriy Mnih  Charles Blundell  and Daan Wierstra. Deep autoregressive

networks. In International Conference on Machine Learning  2014.

[5] Yuchen Pu  Zhe Gan  Ricardo Henao  Chunyuan Li  Shaobo Han  and Lawrence Carin. VAE learning via

Stein variational gradient descent. In Advances in Neural Information Processing Systems. 2017.

[6] Tim Salimans  Diederik Kingma  and Max Welling. Markov chain Monte Carlo and variational inference:

Bridging the gap. In International Conference on Machine Learning  pages 1218–1226  2015.

[7] Rafael Gómez-Bombarelli  Jennifer N Wei  David Duvenaud  José Miguel Hernández-Lobato  Benjamín
Sánchez-Lengeling  Dennis Sheberla  Jorge Aguilera-Iparraguirre  Timothy D Hirzel  Ryan P Adams 
and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of
molecules. ACS Central Science  2016.

[8] Matt J Kusner  Brooks Paige  and José Miguel Hernández-Lobato. Grammar variational autoencoder. In

International Conference on Machine Learning  2017.

[9] Adam Roberts  Jesse Engel  Colin Raffel  Curtis Hawthorne  and Douglas Eck. A hierarchical latent vector
model for learning long-term structure in music. In International Conference on Machine Learning  2018.
[10] Vijayaraghavan Murali  Letao Qi  Swarat Chaudhuri  and Chris Jermaine. Neural sketch learning for

conditional program generation. In The International Conference on Learning Representations  2018.

[11] Jörg Bornschein  Andriy Mnih  Daniel Zoran  and Danilo Jimenez Rezende. Variational memory addressing

in generative models. In Advances in Neural Information Processing Systems  pages 3923–3932  2017.

[12] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted Boltzmann machines and deep

belief networks. Neural computation  2008.

[13] Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep Boltzmann machines. In International Conference on

Artiﬁcial Intelligence and Statistics  2009.

[14] Hugo Larochelle and Yoshua Bengio. Classiﬁcation using discriminative restricted Boltzmann machines.

In International Conference on Machine Learning (ICML)  2008.

[15] Tu Dinh Nguyen  Dinh Phung  Viet Huynh  and Trung Le. Supervised restricted Boltzmann machines. In

UAI  2017.

[16] Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions. J. Mach.

Learn. Res.  5:1063–1088  December 2004.

[17] Geoffrey E. Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504–507  2006.

[18] Ruslan Salakhutdinov  Andriy Mnih  and Geoffrey E. Hinton. Restricted Boltzmann machines for collab-
orative ﬁltering. In Proceedings of the 24th International Conference on Machine Learning  ICML ’07 
pages 791–798  New York  NY  USA  2007. ACM.

[19] Jason Tyler Rolfe. Discrete variational autoencoders. In International Conference on Learning Representa-

tions (ICLR)  2017.

[20] Arash Vahdat  William G. Macready  Zhengbing Bian  Amir Khoshaman  and Evgeny Andriyash. DVAE++:
In International Conference on

Discrete variational autoencoders with overlapping transformations.
Machine Learning (ICML)  2018.

9

[21] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov.

Importance weighted autoencoders.

International Conference on Learning Representations (ICLR)  2016.

In The

[22] John Hertz  Richard Palmer  and Anders Krogh. Introduction to the theory of neural computation. 1991.
[23] J Hubbard. Calculation of partition functions. Physical Review Letters  3(2):77  1959.
[24] Zakkula Govindarajulu. Characterization of the exponential and power distributions. Scandinavian

Actuarial Journal  1966(3-4):132–136  1966.

[25] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised

learning with deep generative models. In Advances in Neural Information Processing Systems  2014.

[26] Lars Maaløe  Marco Fraccaro  and Ole Winther. Semi-supervised generation with cluster-aware generative

models. arXiv preprint arXiv:1704.00637  2017.

[27] Michalis Titsias RC AUEB and Miguel Lázaro-Gredilla. Local expectation gradients for black box

variational inference. In Advances in neural information processing systems  pages 2638–2646  2015.

[28] Seiya Tokui and Issei Sato. Evaluating the variance of likelihood-ratio gradient estimators. In International

Conference on Machine Learning  pages 3414–3423  2017.

[29] Eric Jang  Shixiang Gu  and Ben Poole. Categorical reparametrization with gumble-softmax. In Interna-

tional Conference on Learning Representations  2017.

[30] Yoshua Bengio  Nicholas Léonard  and Aaron Courville. Estimating or propagating gradients through

stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432  2013.

[31] Tapani Raiko  Mathias Berglund  Guillaume Alain  and Laurent Dinh. Techniques for learning binary

stochastic feedforward neural networks. arXiv preprint arXiv:1406.2989  2014.

[32] Chris J Maddison  Andriy Mnih  and Yee Whye Teh. The concrete distribution: A continuous relaxation of

discrete random variables. In International Conference on Learning Representations (ICLR)  2017.

[33] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. In Reinforcement Learning  pages 5–32. Springer  1992.

[34] Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM 

33(10):75–84  1990.

[35] Shixiang Gu  Sergey Levine  Ilya Sutskever  and Andriy Mnih. MuProp: Unbiased backpropagation for
stochastic neural networks. In The International Conference on Learning Representations (ICLR)  2016.
[36] Andriy Mnih and Danilo Rezende. Variational inference for Monte Carlo objectives. In International

Conference on Machine Learning  pages 2188–2196  2016.

[37] George Tucker  Andriy Mnih  Chris J Maddison  John Lawson  and Jascha Sohl-Dickstein. REBAR:
Low-variance  unbiased gradient estimates for discrete latent variable models. In Advances in Neural
Information Processing Systems  pages 2624–2633  2017.

[38] Will Grathwohl  Dami Choi  Yuhuai Wu  Geoff Roeder  and David Duvenaud. Backpropagation through
the void: Optimizing control variates for black-box gradient estimation. In International Conference on
Learning Representations (ICLR)  2018.

[39] Yingzhen Li and Richard E Turner. Rényi divergence variational inference.

Information Processing Systems  pages 1073–1081  2016.

In Advances in Neural

[40] Max Welling and Geoffrey E Hinton. A new learning algorithm for mean ﬁeld Boltzmann machines. In

International Conference on Artiﬁcial Neural Networks  pages 351–357. Springer  2002.

[41] Yichuan Zhang  Zoubin Ghahramani  Amos J Storkey  and Charles A Sutton. Continuous relaxations
for discrete Hamiltonian Monte Carlo. In Advances in Neural Information Processing Systems  pages
3194–3202  2012.

[42] Alex Graves. Stochastic backpropagation through mixture density distributions.

arXiv:1607.05690  2016.

arXiv preprint

[43] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In Proceedings

of the 25th international conference on Machine learning  pages 872–879. ACM  2008.

[44] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept learning through

probabilistic program induction. Science  350(6266):1332–1338  2015.

[45] K Hukushima and Y Iba. Population annealing and its application to a spin glass. In AIP Conference

Proceedings  volume 690  pages 200–206. AIP  2003.

[46] Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient.
In Proceedings of the 25th international conference on Machine learning  pages 1064–1071. ACM  2008.

[47] Radford M. Neal. Annealed importance sampling. Statistics and computing  11(2):125–139  2001.
[48] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In Proceedings

of the 25th international conference on Machine learning  pages 872–879. ACM  2008.

10

A Population Annealing vs. Persistence Contrastive Divergence

In this section  we compare population annealing (PA) to persistence contrastive divergence (PCD) for sampling
in the negative phase. In Table 2  we train DVAE# with the power-function smoothing on the binarized MNIST
dataset using PA and PCD. As shown  PA results in a comparable generative model when there is one group of
latent variables and better models in other cases.

Table 2: The performance of DVAE# with power-function smoothing for binarized MNIST when
PCD or PA is used in the negative phase.

Struct.

1 —

1 ∼

2 ∼

4 ∼

K
1
5
25
1
5
25
1
5
25
1
5
25

PCD

89.25±0.04
88.18±0.08
87.66±0.09
84.95±0.05
84.25±0.04
83.91±0.05
83.48±0.04
83.12±0.04
83.06±0.03
83.62±0.06
83.34±0.06
83.18±0.05

PA

89.35±0.06
88.25±0.03
87.67±0.07
84.93±0.02
84.21±0.02
83.93±0.06
83.37±0.02
82.99±0.04
82.85±0.03
83.18±0.05
82.95±0.07
82.82±0.02

B On the Gradient Variance of the Power-function Smoothing

Our experiments show that power-function smoothing performs best because it provides a better approximation of
the binary random variables. We demonstrate this qualitatively in Fig. 1 and quantitatively in Fig. 2(c) of the paper.
This is also visualized in Fig. 3. Here  we generate 106 samples from q(ζ) = (1 − q)r(ζ|z = 0) + qr(ζ|z = 1)
for q = 0.5 using both the exponential and power smoothings with different values of β (β ∈ {8  9  10  . . .   15}
for exponential  and β ∈ {10  20  30  . . .   80} for power smoothing). The value of β is increasing from left to
right on each curve. The mean of |ζi − zi| (for zi = 1[ζi>0.5]) vs. the variance of ∂ζi/∂q is visualized in this
ﬁgure. For a given gradient variance  power function smoothing provides a closer approximation to the binary
variables.

Figure 3: Average distance between ζ and its binarized z vs. variance of ∂ζ/∂q measured on 106
samples from q(ζ). For a given gradient variance  power function smoothing provides a closer
approximation to the binary variables.

11

510152025GradientVariance0.020.040.060.080.100.12L1Distanceexponentialpower,Xiangru Huang
Zhenxiao Liang
Chandrajit Bajaj
Qixing Huang
Arash Vahdat
Evgeny Andriyash
William Macready