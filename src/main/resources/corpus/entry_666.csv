2017,Avoiding Discrimination through Causal Reasoning,Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor  protected attribute  features  and outcome. While convenient to work with  observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively.  Going beyond observational criteria  we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about our model of the causal data generating process?" Through the lens of causality  we make several contributions. First  we crisply articulate why and when observational criteria fail  thus formalizing what was before a matter of opinion. Second  our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally  we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.,Avoiding Discrimination through Causal Reasoning

Niki Kilbertus†‡

nkilbertus@tue.mpg.de

Mateo Rojas-Carulla†‡
mrojas@tue.mpg.de

Giambattista Parascandolo†§
gparascandolo@tue.mpg.de

Moritz Hardt∗

Dominik Janzing†

hardt@berkeley.edu

janzing@tue.mpg.de

Bernhard Sch¨olkopf†

bs@tue.mpg.de

†Max Planck Institute for Intelligent Systems
§Max Planck ETH Center for Learning Systems

‡University of Cambridge

∗University of California  Berkeley

Abstract

Recent work on fairness in machine learning has focused on various statistical
discrimination criteria and how they trade off. Most of these criteria are observa-
tional: They depend only on the joint distribution of predictor  protected attribute 
features  and outcome. While convenient to work with  observational criteria have
severe inherent limitations that prevent them from resolving matters of fairness
conclusively.
Going beyond observational criteria  we frame the problem of discrimination
based on protected attributes in the language of causal reasoning. This view-
point shifts attention from “What is the right fairness criterion?” to “What do we
want to assume about our model of the causal data generating process?” Through
the lens of causality  we make several contributions. First  we crisply articulate
why and when observational criteria fail  thus formalizing what was before a mat-
ter of opinion. Second  our approach exposes previously ignored subtleties and
why they are fundamental to the problem. Finally  we put forward natural causal
non-discrimination criteria and develop algorithms that satisfy them.

1

Introduction

As machine learning progresses rapidly  its societal impact has come under scrutiny. An important
concern is potential discrimination based on protected attributes such as gender  race  or religion.
Since learned predictors and risk scores increasingly support or even replace human judgment  there
is an opportunity to formalize what harmful discrimination means and to design algorithms that
avoid it. However  researchers have found it difﬁcult to agree on a single measure of discrimination.
As of now  there are several competing approaches  representing different opinions and striking
different trade-offs. Most of the proposed fairness criteria are observational: They depend only
on the joint distribution of predictor R  protected attribute A  features X  and outcome Y. For
example  the natural requirement that R and A must be statistically independent is referred to as
demographic parity. Some approaches transform the features X to obfuscate the information they
contain about A [1]. The recently proposed equalized odds constraint [2] demands that the predictor
R and the attribute A be independent conditional on the actual outcome Y. All three are examples
of observational approaches.
A growing line of work points at the insufﬁciency of existing deﬁnitions. Hardt  Price and Srebro [2]
construct two scenarios with intuitively different social interpretations that admit identical joint dis-

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

tributions over (R  A  Y  X). Thus  no observational criterion can distinguish them. While there
are non-observational criteria  notably the early work on individual fairness [3]  these have not yet
gained traction. So  it might appear that the community has reached an impasse.

1.1 Our contributions

We assay the problem of discrimination in machine learning in the language of causal reasoning.
This viewpoint supports several contributions:

• Revisiting the two scenarios proposed in [2]  we articulate a natural causal criterion that
formally distinguishes them. In particular  we show that observational criteria are unable
to determine if a protected attribute has direct causal inﬂuence on the predictor that is not
mitigated by resolving variables.

• We point out subtleties in fair decision making that arise naturally from a causal perspec-
tive  but have gone widely overlooked in the past. Speciﬁcally  we formally argue for the
need to distinguish between the underlying concept behind a protected attribute  such as
race or gender  and its proxies available to the algorithm  such as visual features or name.
• We introduce and discuss two natural causal criteria centered around the notion of inter-
ventions (relative to a causal graph) to formally describe speciﬁc forms of discrimination.
• Finally  we initiate the study of algorithms that avoid these forms of discrimination. Under
certain linearity assumptions about the underlying causal model generating the data  an
algorithm to remove a speciﬁc kind of discrimination leads to a simple and natural heuristic.

At a higher level  our work proposes a shift from trying to ﬁnd a single statistical fairness criterion
to arguing about properties of the data and which assumptions about the generating process are
justiﬁed. Causality provides a ﬂexible framework for organizing such assumptions.

1.2 Related work

Demographic parity and its variants have been discussed in numerous papers  e.g.  [1  4–6]. While
demographic parity is easy to work with  the authors of [3] already highlighted its insufﬁciency as
a fairness constraint. In an attempt to remedy the shortcomings of demographic parity [2] proposed
two notions  equal opportunity and equal odds  that were also considered in [7]. A review of various
fairness criteria can be found in [8]  where they are discussed in the context of criminal justice.
In [9  10] it has been shown that imperfect predictors cannot simultaneously satisfy equal odds and
calibration unless the groups have identical base rates  i.e. rates of positive outcomes.
A starting point for our investigation is the unidentiﬁability result of [2]. It shows that observed-
vational criteria are too weak to distinguish two intuitively very different scenarios. However  the
work does not provide a formal mechanism to articulate why and how these scenarios should be
considered different. Inspired by Pearl’s causal interpretation of Simpson’s paradox [11  Section 6] 
we propose causality as a way of coping with this unidentiﬁability result.
An interesting non-observational fairness deﬁnition is the notion of individual fairness [3] that as-
sumes the existence of a similarity measure on individuals  and requires that any two similar individ-
uals should receive a similar distribution over outcomes. More recent work lends additional support
to such a deﬁnition [12]. From the perspective of causality  the idea of a similarity measure is akin
to the method of matching in counterfactual reasoning [13  14]. That is  evaluating approximate
counterfactuals by comparing individuals with similar values of covariates excluding the protected
attribute.
Recently  [15] put forward one possible causal deﬁnition  namely the notion of counterfactual fair-
ness. It requires modeling counterfactuals on a per individual level  which is a delicate task. Even
determining the effect of race at the group level is difﬁcult; see the discussion in [16]. The goal of
our paper is to assay a more general causal framework for reasoning about discrimination in machine
learning without committing to a single fairness criterion  and without committing to evaluating in-
dividual causal effects. In particular  we draw an explicit distinction between the protected attribute
(for which interventions are often impossible in practice) and its proxies (which sometimes can be
intervened upon).

2

Moreover  causality has already been employed for the discovery of discrimination in existing data
sets by [14  17]. Causal graphical conditions to identify meaningful partitions have been proposed
for the discovery and prevention of certain types of discrimination by preprocessing the data [18].
These conditions rely on the evaluation of path speciﬁc effects  which can be traced back all the
way to [11  Section 4.5.3]. The authors of [19] recently picked up this notion and generalized
Pearl’s approach by a constraint based prevention of discriminatory path speciﬁc effects arising
from counterfactual reasoning. Our research was done independently of these works.

1.3 Causal graphs and notation

Causal graphs are a convenient way of organizing assumptions about the data generating process.
We will generally consider causal graphs involving a protected attribute A  a set of proxy variables P 
features X  a predictor R and sometimes an observed outcome Y. For background on causal graphs
see [11]. In the present paper a causal graph is a directed  acyclic graph whose nodes represent
random variables. A directed path is a sequence of distinct nodes V1  . . .   Vk  for k ≥ 2  such
that Vi → Vi+1 for all i ∈ {1  . . .   k − 1}. We say a directed path is blocked by a set of nodes Z 
where V1  Vk /∈ Z  if Vi ∈ Z for some i ∈ {2  . . .   k − 1}.1
A structural equation model is a set of equations Vi = fi(pa(Vi)  Ni)  for i ∈ {1  . . .   n} 
where pa(Vi) are the parents of Vi  i.e. its direct causes  and the Ni are independent noise vari-
ables. We interpret these equations as assignments. Because we assume acyclicity  starting from
the roots of the graph  we can recursively compute the other variables  given the noise variables.
This leads us to view the structural equation model and its corresponding graph as a data gener-
ating model. The predictor R maps inputs  e.g.  the features X  to a predicted output. Hence we
model it as a childless node  whose parents are its input variables. Finally  note that given the noise
variables  a structural equation model entails a unique joint distribution; however  the same joint
distribution can usually be entailed by multiple structural equation models corresponding to distinct
causal structures.

2 Unresolved discrimination and limitations of observational criteria

A

To bear out the limitations of observational criteria  we turn to
Pearl’s commentary on claimed gender discrimination in Berke-
ley college admissions [11  Section 4.5.3]. Bickel [20] had shown
earlier that a lower college-wide admission rate for women than
for men was explained by the fact that women applied in more
competitive departments. When adjusted for department choice 
women experienced a slightly higher acceptance rate compared
with men. From the causal point of view  what matters is the di-
rect effect of the protected attribute (here  gender A) on the deci-
sion (here  college admission R) that cannot be ascribed to a re-
solving variable such as department choice X  see Figure 1. We
shall use the term resolving variable for any variable in the causal
graph that is inﬂuenced by A in a manner that we accept as non-
discriminatory. With this convention  the criterion can be stated as
follows.
Deﬁnition 1 (Unresolved discrimination). A variable V in a causal graph exhibits unresolved dis-
crimination if there exists a directed path from A to V that is not blocked by a resolving variable
and V itself is non-resolving.

Figure 1: The admission de-
cision R does not only di-
rectly depend on gender A  but
also on department choice X 
which in turn is also affected
by gender A.

X

R

Pearl’s commentary is consistent with what we call the skeptic viewpoint. All paths from the pro-
tected attribute A to R are problematic  unless they are justiﬁed by a resolving variable. The pres-
ence of unresolved discrimination in the predictor R is worrisome and demands further scrutiny.
In practice  R is not a priori part of a given graph. Instead it is our objective to construct it as a
function of the features X  some of which might be resolving. Hence we should ﬁrst look for unre-
solved discrimination in the features. A canonical way to avoid unresolved discrimination in R is to
only input the set of features that do not exhibit unresolved discrimination. However  the remaining

1As it is not needed in our work  we do not discuss the graph-theoretic notion of d-separation.

3

Y

A

Y

X2

X1

X2

R∗

A

X1

R∗

features might be affected by non-resolving and resolving variables. In Section 4 we investigate
whether one can exclusively remove unresolved discrimination from such features. A related notion
of “explanatory features” in a non-causal setting was introduced in [21].
The deﬁnition of unresolved discrimination in
a predictor has some interesting special cases
worth highlighting. If we take the set of resolv-
ing variables to be empty  we intuitively get a
causal analog of demographic parity. No di-
rected paths from A to R are allowed  but A
and R can still be statistically dependent. Simi-
larly  if we choose the set of resolving variables
to be the singleton set {Y } containing the true
outcome  we obtain a causal analog of equal-
ized odds where strict independence is not nec-
essary. The causal intuition implied by “the
protected attribute should not affect the predic-
tion”  and “the protected attribute can only af-
fect the prediction when the information comes
through the true label”  is neglected by (con-
ditional) statistical independences A⊥⊥ R  and A⊥⊥ R | Y   but well captured by only considering
dependences mitigated along directed causal paths.
We will next show that observational criteria are fundamentally unable to determine whether a pre-
dictor exhibits unresolved discrimination or not. This is true even if the predictor is Bayes optimal.
In passing  we also note that fairness criteria such as equalized odds may or may not exhibit unre-
solved discrimination  but this is again something an observational criterion cannot determine.
Theorem 1. Given a joint distribution over the protected attribute A  the true label Y   and some
features X1  . . .   Xn  in which we have already speciﬁed the resolving variables  no observational
criterion can generally determine whether the Bayes optimal unconstrained predictor or the Bayes
optimal equal odds predictor exhibit unresolved discrimination.

Figure 2: Two graphs that may generate the same
joint distribution for the Bayes optimal uncon-
strained predictor R∗. If X1 is a resolving vari-
able  R∗ exhibits unresolved discrimination in the
right graph (along the red paths)  but not in the left
one.

All proofs for the statements in this paper are in the supplementary material.
The two graphs in Figure 2 are taken from [2]  which we here reinterpret in the causal context to
prove Theorem 1. We point out that there is an established set of conditions under which unresolved
discrimination can  in fact  be determined from observational data. Note that the two graphs are
not Markov equivalent. Therefore  to obtain the same joint distribution we must violate a condition
called faithfulness.2 We later argue that violation of faithfulness is by no means pathological  but
emerges naturally when designing predictors. In any case  interpreting conditional dependences can
be difﬁcult in practice [22].

3 Proxy discrimination and interventions

We now turn to an important aspect of our framework. Determining causal effects in general requires
Interventions on deeply rooted individual properties such as gender or
modeling interventions.
race are notoriously difﬁcult to conceptualize—especially at an individual level  and impossible to
perform in a randomized trial. VanderWeele et al. [16] discuss the problem comprehensively in an
epidemiological setting. From a machine learning perspective  it thus makes sense to separate the
protected attribute A from its potential proxies  such as name  visual features  languages spoken at
home  etc. Intervention based on proxy variables poses a more manageable problem. By deciding on
a suitable proxy we can ﬁnd an adequate mounting point for determining and removing its inﬂuence
on the prediction. Moreover  in practice we are often limited to imperfect measurements of A in any
case  making the distinction between root concept and proxy prudent.
As was the case with resolving variables  a proxy is a priori nothing more than a descendant of A in
the causal graph that we choose to label as a proxy. Nevertheless in reality we envision the proxy

2If we do assume the Markov condition and faithfulness  then conditional independences determine the

graph up to its so called Markov equivalence class.

4

to be a clearly deﬁned observable quantity that is signiﬁcantly correlated with A  yet in our view
should not affect the prediction.
Deﬁnition 2 (Potential proxy discrimination). A variable V in a causal graph exhibits potential
proxy discrimination  if there exists a directed path from A to V that is blocked by a proxy variable
and V itself is not a proxy.

Potential proxy discrimination articulates a causal criterion that is in a sense dual to unresolved
discrimination. From the benevolent viewpoint  we allow any path from A to R unless it passes
through a proxy variable  which we consider worrisome. This viewpoint acknowledges the fact that
the inﬂuence of A on the graph may be complex and it can be too restraining to rule out all but a few
designated features. In practice  as with unresolved discrimination  we can naively build an uncon-
strained predictor based only on those features that do not exhibit potential proxy discrimination.
Then we must not provide P as input to R; unawareness  i.e. excluding P from the inputs of R 
sufﬁces. However  by granting R access to P   we can carefully tune the function R(P  X) to cancel
the implicit inﬂuence of P on features X that exhibit potential proxy discrimination by the explicit
dependence on P . Due to this possible cancellation of paths  we called the path based criterion po-
tential proxy discrimination. When building predictors that exhibit no overall proxy discrimination 
we precisely aim for such a cancellation.
Fortunately  this idea can be conveniently expressed by an intervention on P   which is denoted
by do(P = p) [11]. Visually  intervening on P amounts to removing all incoming arrows of P in
the graph; algebraically  it consists of replacing the structural equation of P by P = p  i.e. we put
point mass on the value p.
Deﬁnition 3 (Proxy discrimination). A predictor R exhibits no proxy discrimination based on a
proxy P if for all p  p(cid:48)

P(R | do(P = p)) = P(R | do(P = p(cid:48))) .

(1)

The interventional characterization of proxy discrimination leads to a simple procedure to remove
it in causal graphs that we will turn to in the next section. It also leads to several natural variants
of the deﬁnition that we discuss in Section 4.3. We remark that Equation (1) is an equality of
probabilities in the “do-calculus” that cannot in general be inferred by an observational method 
because it depends on an underlying causal graph  see the discussion in [11]. However  in some
cases  we do not need to resort to interventions to avoid proxy discrimination.
Proposition 1. If there is no directed path from a proxy to a feature  unawareness avoids proxy
discrimination.

4 Procedures for avoiding discrimination

Having motivated the two types of discrimination that we distinguish  we now turn to building
predictors that avoid them in a given causal model. First  we remark that a more comprehensive
treatment requires individual judgement of not only variables  but the legitimacy of every existing
path that ends in R  i.e. evaluation of path-speciﬁc effects [18  19]  which is tedious in practice.
The natural concept of proxies and resolving variables covers most relevant scenarios and allows for
natural removal procedures.

4.1 Avoiding proxy discrimination

While presenting the general procedure  we illustrate each step in the example shown in Figure 3.
A protected attribute A affects a proxy P as well as a feature X. Both P and X have additional
unobserved causes NP and NX  where NP   NX   A are pairwise independent. Finally  the proxy also
has an effect on the features X and the predictor R is a function of P and X. Given labeled training
data  our task is to ﬁnd a good predictor that exhibits no proxy discrimination within a hypothesis
class of functions Rθ(P  X) parameterized by a real valued vector θ.
We now work out a formal procedure to solve this task under speciﬁc assumptions and simultane-
ously illustrate it in a fully linear example  i.e. the structural equations are given by

P = αP A + NP  

X = αX A + βP + NX  

Rθ = λP P + λX X .

Note that we choose linear functions parameterized by θ = (λP   λX ) as the hypothesis class
for Rθ(P  X).

5

NP

A

NX

NP

A

NX

NE

A

NX

NE

A

NX

P

X

R˜G

X

P
RG

Figure 3: A template graph ˜G for proxy dis-
crimination (left) with its intervened version G
(right). While from the benevolent viewpoint we
do not generically prohibit any inﬂuence from A
on R  we want to guarantee that the proxy P has
no overall inﬂuence on the prediction  by adjust-
ing P → R to cancel the inﬂuence along P →
X → R in the intervened graph.

E

X

R˜G

X

E
RG

Figure 4: A template graph ˜G for unresolved
discrimination (left) with its intervened ver-
sion G (right). While from the skeptical
viewpoint we generically do not want A to
inﬂuence R  we ﬁrst intervene on E inter-
rupting all paths through E and only cancel
the remaining inﬂuence on A to R.

We will refer to the terminal ancestors of a node V in a causal graph D  denoted by taD(V )  which
are those ancestors of V that are also root nodes of D. Moreover  in the procedure we clarify the
notion of expressibility  which is an assumption about the relation of the given structural equations
and the hypothesis class we choose for Rθ.
Proposition 2. If there is a choice of parameters θ0 such that Rθ0 (P  X) is constant with respect
to its ﬁrst argument and the structural equations are expressible  the following procedure returns a
predictor from the given hypothesis class that exhibits no proxy discrimination and is non-trivial in
the sense that it can make use of features that exhibit potential proxy discrimination.

1. Intervene on P by removing all incoming arrows and replacing the structural equation for P

by P = p. For the example in Figure 3 

P = p 

X = αX A + βP + NX  

Rθ = λP P + λX X .

(2)

2. Iteratively substitute variables in the equation for Rθ from their structural equations until only
root nodes of the intervened graph are left  i.e. write Rθ(P  X) as Rθ(P  g(taG(X))) for some
function g. In the example  ta(X) = {A  P  NX} and

Rθ = (λP + λX β)p + λX (αX A + NX ) .

(3)

3. We now require the distribution of Rθ in (3) to be independent of p  i.e. for all p  p(cid:48)

P((λP + λX β)p + λX (αX A + NX )) = P((λP + λX β)p(cid:48) + λX (αX A + NX )) .

(4)
We seek to write the predictor as a function of P and all the other roots of G separately. If our
hypothesis class is such that there exists ˜θ such that Rθ(P  g(ta(X))) = R˜θ(P  ˜g(ta(X)\{P})) 
we call the structural equation model and hypothesis class speciﬁed in (2) expressible. In our
example  this is possible with ˜θ = (λP + λX β  λX ) and ˜g = αX A + NX. Equation (4) then
yields the non-discrimination constraint ˜θ = θ0. Here  a possible θ0 is θ0 = (0  λX )  which
simply yields λP = −λX β.

4. Given labeled training data  we can optimize the predictor Rθ within the hypothesis class as given

in (2)  subject to the non-discrimination constraint. In the example

Rθ = −λX βP + λX X = λX (X − βP )  

with the free parameter λX ∈ R.

In general  the non-discrimination constraint (4) is by construction just P(R | do(P = p)) =
P(R | do(P = p(cid:48)))  coinciding with Deﬁnition 3. Thus Proposition 2 holds by construction of
the procedure. The choice of θ0 strongly inﬂuences the non-discrimination constraint. However  as
the example shows  it allows Rθ to exploit features that exhibit potential proxy discrimination.

6

A

P

R

X

A

P

R

X

˜G

DAG

G

DAG

Figure 5: Left: A generic graph ˜G to describe proxy discrimination. Right: The graph corresponding
to an intervention on P . The circle labeled “DAG” represents any sub-DAG of ˜G and G containing
an arbitrary number of variables that is compatible with the shown arrows. Dashed arrows can  but
do not have to be present in a given scenario.

4.2 Avoiding unresolved discrimination

We proceed analogously to the previous subsection using the example graph in Figure 4. Instead of
the proxy  we consider a resolving variable E. The causal dependences are equivalent to the ones in
Figure 3 and we again assume linear structural equations

E = αEA + NE 

X = αX A + βE + NX  

Rθ = λEE + λX X .

Let us now try to adjust the previous procedure to the context of avoiding unresolved discrimination.
1. Intervene on E by ﬁxing it to a random variable η with P(η) = P(E)  the marginal distribution

of E in ˜G  see Figure 4. In the example we ﬁnd

(5)
2. By iterative substitution write Rθ(E  X) as Rθ(E  g(taG(X))) for some function g  i.e. in the

X = αX A + βE + NX  

Rθ = λEE + λX X .

E = η 

example

(6)
3. We now demand the distribution of Rθ in (6) be invariant under interventions on A  which coin-

Rθ = (λE + λX β)η + λX αX A + λX NX .

cides with conditioning on A whenever A is a root of ˜G. Hence  in the example  for all a  a(cid:48)
P((λE + λX β)η + λX αX a + λX NX )) = P((λE + λX β)η + λX αX a(cid:48) + λX NX )) .

(7)

Here  the subtle asymmetry between proxy discrimination and unresolved discrimination becomes
apparent. Because Rθ is not explicitly a function of A  we cannot cancel implicit inﬂuences of A
through X. There might still be a θ0 such that Rθ0 indeed fulﬁls (7)  but there is no princi-
pled way for us to construct it. In the example  (7) suggests the obvious non-discrimination con-
straint λX = 0. We can then proceed as before and  given labeled training data  optimize Rθ = λEE
by varying λE. However  by setting λX = 0  we also cancel the path A → E → X → R  even
though it is blocked by a resolving variable. In general  if Rθ does not have access to A  we can not
adjust for unresolved discrimination without also removing resolved inﬂuences from A on Rθ.
If  however  Rθ is a function of A  i.e. we add the term λAA to Rθ in (5)  the non-discrimination
constraint is λA = −λX αX and we can proceed analogously to the procedure for proxies.

4.3 Relating proxy discriminations to other notions of fairness

Motivated by the algorithm to avoid proxy discrimination  we discuss some natural variants of the
notion in this section that connect our interventional approach to individual fairness and other pro-
posed criteria. We consider a generic graph structure as shown on the left in Figure 5. The proxy P
and the features X could be multidimensional. The empty circle in the middle represents any num-
ber of variables forming a DAG that respects the drawn arrows. Figure 3 is an example thereof. All
dashed arrows are optional depending on the speciﬁcs of the situation.
Deﬁnition 4. A predictor R exhibits no individual proxy discrimination  if for all x and all p  p(cid:48)

P(R | do(P = p)  X = x) = P(R | do(P = p(cid:48))  X = x) .

A predictor R exhibits no proxy discrimination in expectation  if for all p  p(cid:48)

E[R | do(P = p)] = E[R | do(P = p(cid:48))] .

7

Individual proxy discrimination aims at comparing examples with the same features X  for different
values of P . Note that this can be individuals with different values for the unobserved non-feature
variables. A true individual-level comparison of the form “What would have happened to me  if I
had always belonged to another group” is captured by counterfactuals and discussed in [15  19].
For an analysis of proxy discrimination  we need the structural equations for P  X  R in Figure 5

P = ˆfP (pa(P ))  
X = ˆfX (pa(X)) = fX (P  taG(X) \ {P})  
R = ˆfR(P  X) = fR(P  taG(R) \ {P}) .

P (X) := taG(X) \ {P}. We can ﬁnd fX   fR
For convenience  we will use the notation taG
from ˆfX   ˆfR by ﬁrst rewriting the functions in terms of root nodes of the intervened graph  shown
on the right side of Figure 5  and then assigning the overall dependence on P to the ﬁrst argument.
We now compare proxy discrimination to other existing notions.
Theorem 2. Let the inﬂuence of P on X be additive and linear  i.e.

X = fX (P  taG

P (X)) = gX (taG

P (X)) + µX P

for some function gX and µX ∈ R. Then any predictor of the form

R = r(X − E[X | do(P )])

for some function r exhibits no proxy discrimination.
Note that in general E[X | do(P )] (cid:54)= E[X | P ]. Since in practice we only have observational data
from ˜G  one cannot simply build a predictor based on the “regressed out features” ˜X := X −
E[X | P ] to avoid proxy discrimination. In the scenario of Figure 3  the direct effect of P on X
along the arrow P → X in the left graph cannot be estimated by E[X | P ]  because of the common
confounder A. The desired interventional expectation E[X | do(P )] coincides with E[X | P ] only
if one of the arrows A → P or A → X is not present. Estimating direct causal effects is a hard
problem  well studied by the causality community and often involves instrumental variables [23].
This cautions against the natural idea of using ˜X as a “fair representation” of X  as it implicitly
neglects that we often want to remove the effect of proxies and not the protected attribute. Never-
theless  the notion agrees with our interventional proxy discrimination in some cases.
Corollary 1. Under the assumptions of Theorem 2  if all directed paths from any ancestor of P
to X in the graph G are blocked by P   then any predictor based on the adjusted features ˜X :=
X − E[X | P ] exhibits no proxy discrimination and can be learned from the observational distribu-
tion P(P  X  Y ) when target labels Y are available.

Our deﬁnition of proxy discrimination in expectation (4) is motivated by a weaker notion proposed
in [24]. It asks for the expected outcome to be the same across the different populations E[R | P =
p] = E[R | P = p(cid:48)]. Again  when talking about proxies  we must be careful to distinguish conditional
and interventional expectations  which is captured by the following proposition and its corollary.
Proposition 3. Any predictor of the form R = λ(X − E[X | do(P )]) + c for λ  c ∈ R exhibits no
proxy discrimination in expectation.

From this and the proof of Corollary 1 we conclude the following Corollary.
Corollary 2. If all directed paths from any ancestor of P to X are blocked by P   any predictor of
the form R = r(X − E[X | P ]) for linear r exhibits no proxy discrimination in expectation and can
be learned from the observational distribution P(P  X  Y ) when target labels Y are available.

5 Conclusion

The goal of our work is to assay fairness in machine learning within the context of causal reasoning.
This perspective naturally addresses shortcomings of earlier statistical approaches. Causal fairness
criteria are suitable whenever we are willing to make assumptions about the (causal) generating

8

process governing the data. Whilst not always feasible  the causal approach naturally creates an
incentive to scrutinize the data more closely and work out plausible assumptions to be discussed
alongside any conclusions regarding fairness.
Key concepts of our conceptual framework are resolving variables and proxy variables that play
a dual role in deﬁning causal discrimination criteria. We develop a practical procedure to remove
proxy discrimination given the structural equation model and analyze a similar approach for un-
resolved discrimination.
In the case of proxy discrimination for linear structural equations  the
procedure has an intuitive form that is similar to heuristics already used in the regression literature.
Our framework is limited by the assumption that we can construct a valid causal graph. The removal
of proxy discrimination moreover depends on the functional form of the causal dependencies. We
have focused on the conceptual and theoretical analysis  and experimental validations are beyond
the scope of the present work.
The causal perspective suggests a number of interesting new directions at the technical  empirical 
and conceptual level. We hope that the framework and language put forward in our work will be a
stepping stone for future investigations.

9

References

[1] Richard S Zemel  Yu Wu  Kevin Swersky  Toniann Pitassi  and Cynthia Dwork. “Learning
Fair Representations.” In: Proceedings of the International Conference of Machine Learning
28 (2013)  pp. 325–333.

[2] Moritz Hardt  Eric Price  Nati Srebro  et al. “Equality of opportunity in supervised learning”.

In: Advances in Neural Information Processing Systems. 2016  pp. 3315–3323.

[3] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. “Fairness
Through Awareness”. In: Proceedings of the 3rd Innovations in Theoretical Computer Science
Conference. 2012  pp. 214–226.

[4] Michael Feldman  Sorelle A Friedler  John Moeller  Carlos Scheidegger  and Suresh Venkata-
subramanian. “Certifying and removing disparate impact”. In: Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2015 
pp. 259–268.

[5] Muhammad Bilal Zafar  Isabel Valera  Manuel G´omez Rogriguez  and Krishna P. Gummadi.
“Fairness Constraints: Mechanisms for Fair Classiﬁcation”. In: Proceedings of the 20th In-
ternational Conference on Artiﬁcial Intelligence and Statistics. 2017  pp. 962–970.

[6] Harrison Edwards and Amos Storkey. “Censoring Representations with an Adversary”. In:

(Nov. 18  2015). arXiv: 1511.05897v3.

[7] Muhammad Bilal Zafar  Isabel Valera  Manuel G´omez Rodriguez  and Krishna P. Gummadi.
“Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classiﬁcation Without
Disparate Mistreatment”. In: Proceedings of the 26th International Conference on World Wide
Web. 2017  pp. 1171–1180.

[8] Richard Berk  Hoda Heidari  Shahin Jabbari  Michael Kearns  and Aaron Roth. “Fairness
in Criminal Justice Risk Assessments: The State of the Art”. In: (Mar. 27  2017). arXiv:
1703.09207v1.
Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. “Inherent Trade-Offs in the Fair
Determination of Risk Scores”. In: (Sept. 19  2016). arXiv: 1609.05807v1.

[9]

[10] Alexandra Chouldechova. “Fair prediction with disparate impact: A study of bias in recidi-

vism prediction instruments”. In: (Oct. 24  2016). arXiv: 1610.07524v1.
Judea Pearl. Causality. Cambridge University Press  2009.

[11]
[12] Sorelle A. Friedler  Carlos Scheidegger  and Suresh Venkatasubramanian. “On the

(im)possibility of fairness”. In: (Sept. 23  2016). arXiv: 1609.07236v1.

[13] Paul R Rosenbaum and Donald B Rubin. “The central role of the propensity score in obser-

vational studies for causal effects”. In: Biometrika (1983)  pp. 41–55.

[14] Bilal Qureshi  Faisal Kamiran  Asim Karim  and Salvatore Ruggieri. “Causal Discrimination

Discovery Through Propensity Score Analysis”. In: (Aug. 12  2016). arXiv: 1608.03735.

[15] Matt J. Kusner  Joshua R. Loftus  Chris Russell  and Ricardo Silva. “Counterfactual Fairness”.

In: (Mar. 20  2017). arXiv: 1703.06856v1.

[16] Tyler J VanderWeele and Whitney R Robinson. “On causal interpretation of race in regres-
sions adjusting for confounding and mediating variables”. In: Epidemiology 25.4 (2014) 
p. 473.

[17] Francesco Bonchi  Sara Hajian  Bud Mishra  and Daniele Ramazzotti. “Exposing the proba-

bilistic causal structure of discrimination”. In: (Mar. 8  2017). arXiv: 1510.00552v3.

[18] Lu Zhang and Xintao Wu. “Anti-discrimination learning: a causal modeling-based frame-

work”. In: International Journal of Data Science and Analytics (2017)  pp. 1–16.

[19] Razieh Nabi and Ilya Shpitser. “Fair Inference On Outcomes”. In: (May 29  2017). arXiv:

1705.10378v1.

[20] Peter J Bickel  Eugene A Hammel  J William O’Connell  et al. “Sex bias in graduate admis-

sions: Data from Berkeley”. In: Science 187.4175 (1975)  pp. 398–404.

[21] Faisal Kamiran  Indr˙e ˇZliobait˙e  and Toon Calders. “Quantifying explainable discrimination
and removing illegal discrimination in automated decision making”. In: Knowledge and in-
formation systems 35.3 (2013)  pp. 613–644.

[22] Nicholas Cornia and Joris M Mooij. “Type-II errors of independence tests can lead to arbi-
trarily large errors in estimated causal effects: An illustrative example”. In: Proceedings of
the Workshop on Causal Inference (UAI). 2014  pp. 35–42.

10

[23]

Joshua Angrist and Alan B Krueger. Instrumental variables and the search for identiﬁcation:
From supply and demand to natural experiments. Tech. rep. National Bureau of Economic
Research  2001.

[24] Toon Calders and Sicco Verwer. “Three naive Bayes approaches for discrimination-free clas-

siﬁcation”. In: Data Mining and Knowledge Discovery 21.2 (2010)  pp. 277–292.

11

,Niki Kilbertus
Mateo Rojas Carulla
Giambattista Parascandolo
Moritz Hardt
Dominik Janzing
Bernhard Schölkopf
Yangyan Li
Rui Bu
Mingchao Sun
Wei Wu
Xinhan Di
Baoquan Chen