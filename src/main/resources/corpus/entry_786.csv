2019,Beyond Alternating Updates for Matrix Factorization with Inertial Bregman Proximal Gradient Algorithms,Matrix Factorization is a popular non-convex optimization problem   for which alternating minimization schemes are mostly used. They usually suffer from the major drawback that the solution is biased towards one of the optimization variables. A remedy is non-alternating schemes. However  due to a lack of Lipschitz continuity of the gradient in matrix factorization problems  convergence cannot be guaranteed. A recently developed approach relies on the concept of Bregman distances  which generalizes the standard Euclidean distance. We exploit this theory by proposing a novel Bregman distance for matrix factorization problems  which  at the same time  allows for simple/closed form update steps. Therefore   for non-alternating schemes  such as the recently introduced Bregman Proximal Gradient (BPG) method and an inertial variant Convex--Concave Inertial BPG (CoCaIn BPG)  convergence of the whole sequence to a stationary point is proved for Matrix Factorization. In several experiments  we observe a superior performance of our non-alternating schemes in terms of speed and objective value at the limit point.,Beyond Alternating Updates for Matrix Factorization
with Inertial Bregman Proximal Gradient Algorithms

Mahesh Chandra Mukkamala
Mathematical Optimization Group

Saarland University  Germany
mukkamala@math.uni-sb.de

Peter Ochs

Mathematical Optimization Group

Saarland University  Germany

ochs@math.uni-sb.de

Abstract

Matrix Factorization is a popular non-convex optimization problem  for which
alternating minimization schemes are mostly used. They usually suffer from
the major drawback that the solution is biased towards one of the optimization
variables. A remedy is non-alternating schemes. However  due to a lack of Lipschitz
continuity of the gradient in matrix factorization problems  convergence cannot
be guaranteed. A recently developed approach relies on the concept of Bregman
distances  which generalizes the standard Euclidean distance. We exploit this theory
by proposing a novel Bregman distance for matrix factorization problems  which 
at the same time  allows for simple/closed form update steps. Therefore  for non-
alternating schemes  such as the recently introduced Bregman Proximal Gradient
(BPG) method and an inertial variant Convex–Concave Inertial BPG (CoCaIn
BPG)  convergence of the whole sequence to a stationary point is proved for Matrix
Factorization. In several experiments  we observe a superior performance of our
non-alternating schemes in terms of speed and objective value at the limit point.

1

Introduction

Matrix factorization has numerous applications in Machine Learning [43  57]  Computer Vision
[17  58  62  28]  Bio-informatics [56  12] and many others. Given a matrix A ∈ RM×N   one is
interested in the factors U ∈ RM×K and Z ∈ RK×N such that A ≈ UZ holds. This is usually cast
into the following non-convex optimization problem

(cid:26)

(cid:27)

min

U∈U  Z∈Z

Ψ ≡

1

2 (cid:107)A − UZ(cid:107)2

F + R1(U) + R2(Z)

 

(1.1)

where U Z are constraint sets and R1 R2 are regularization terms. The most frequently used
techniques for solving matrix factorization problems involve alternating updates (Gauss–Seidel type
methods [26]) like PALM [8]  iPALM [53]  BCD [63]  BC-VMFB [18]  HALS [19] and many
others. A common disadvantage of these schemes is their bias towards one of the optimization
variables. Such alternating schemes involve ﬁxing a subset of variables to do the updates. In order
to guarantee convergence to a stationary point  alternating schemes require the ﬁrst term in (1.1)
to have a Lipschitz continuous gradient only with respect to each subset of variables. However 
in general Lipschitz continuity of the gradient fails to hold for all variables. The same problem
appears in various practical applications such as Quadratic Inverse Problems  Poisson Linear Inverse
Problems  Cubic Regularized Non-convex Quadratic Problems and Robust Denoising Problems with
Non-convex Total Variation Regularization [46  9  4]. They belong to the following broad class of
non-convex additive composite minimization problems

inf(cid:8)Ψ ≡ f (x) + g (x) : x ∈ C(cid:9)  

(P)

(1.2)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where f is potentially a non-convex extended real valued function  g is a smooth (possibly non-
convex) function and C is a nonempty  closed  convex set in Rd. In order to use non-alternating
schemes for (1.1)  the gradient Lipschitz continuity must be generalized. Such a generalization was
initially proposed by [6] and popularized by [4] in convex setting and for non-convex problems in [9].
They are based on a generalized proximity measure known as Bregman distance and have recently
led to new algorithms to solve (1.2): Bregman Proximal Gradient (BPG) method [9] and its inertial
variant Convex–Concave Inertial BPG (CoCaIn BPG) [46].
BPG generalizes the proximal gradient method from Euclidean distances to Bregman distances as
proximity measures. Its convergence theory relies on the generalized Lipschitz assumption  discussed
above  called L-smad property [9]. It involves an upper bound and a lower bound  where the upper
bound involves a convex majorant to control the step-size of BPG. However  the signiﬁcance of lower
bounds for BPG was not clear. In non-convex optimization literature  the lower bounds which involve
concave minorants were largely ignored. Recently  extending on [61  50]  CoCaIn BPG changed
this trend by justifying the usage of lower bounds to incorporate inertia for faster convergence [46].
Moreover  the generated inertia is adaptive  in the sense that it changes according to the function
behavior  i.e.  CoCaIn BPG does not use an inertial parameter depending on the iteration counter
unlike Nesterov Accelerated Gradient (NAG) method [47] (also FISTA [5]) in the convex setting.
In this paper we ask the question: "Can we apply BPG and CoCaIn BPG efﬁciently for Matrix Fac-
torization problems?”. This question is signiﬁcant  since convergence of the Bregman minimization
variants BPG and CoCaIn BPG relies on the L-smad property  which is non-trivial to verify and
an open problem for Matrix Factorization. Another crucial issue is the efﬁcient computability of
the algorithm’s update steps  which is particularly hard due to the coupling between two subsets of
variables. We successfully solve these challenges.
Contributions. We make recently introduced powerful Bregman minimization based algorithms
BPG [9] and CoCaIn BPG [46] and the corresponding convergence results applicable to the matrix
factorization problems. Experiments show a signiﬁcant advantage of BPG and CoCaIn BPG which are
non-alternating by construction  compared to popular alternating minimization schemes in particular
PALM [8] and iPALM [53]. The proposed algorithms require the following non-trivial contributions:
• We propose a novel Bregman distance for Matrix Factorization with the following auxiliary

function (called kernel generating distance) with certain c1  c2 > 0:

(cid:32)

(cid:33)2

(cid:32)

(cid:33)

.

h(U  Z) = c1

(cid:107)U(cid:107)2

F + (cid:107)Z(cid:107)2

F

2

+ c2

(cid:107)U(cid:107)2

F + (cid:107)Z(cid:107)2

F

2

The generated Bregman distance embeds the crucial coupling between the variables U
and Z. We prove the L-smad property with such a kernel generating distance and infer
convergence of BPG and CoCaIn BPG to a stationary point.

• We compute the analytic solution for subproblems of the proposed variants of BPG  for
which the usual analytic solutions based on Euclidean distances cannot be used.

Simple Illustration of BPG for Matrix Factorization. Consider the following simple matrix
factorization optimization problem  where we set R1 := 0 and R2 := 0 in (1.1)

(cid:26)

min

U∈RM×K  Z∈RK×N

Ψ(U  Z) =

1

2 (cid:107)A − UZ(cid:107)2

F

.

(1.3)

(cid:27)

For this problem  the update steps of Bregman Proximal Gradient for Matrix Factorization
(BPG-MF) given in Section 2.1 (also see Section 2.4) with a chosen λ ∈ (0  1) are the following:
F ) + (cid:107)A(cid:107)F and perform the intermediary

In each iteration  compute tk = 3((cid:13)(cid:13)Uk(cid:13)(cid:13)2

F +(cid:13)(cid:13)Zk(cid:13)(cid:13)2

gradient descent steps (non-alternating) for U and Z independently with step-size λ
tk

(cid:2)(UkZk − A)(Zk)T(cid:3)   Qk = Zk −
(cid:16)(cid:13)(cid:13)Pk(cid:13)(cid:13)2

λ
tk

F +(cid:13)(cid:13)Qk(cid:13)(cid:13)2

:

(cid:2)(Uk)T (UkZk − A)(cid:3) .
(cid:17)

Then  the additional scaling steps Uk+1 = rtkPk and Zk+1 = rtkQk are required  where the
scaling factor r ≥ 0 satisﬁes a cubic equation: 3t2
r3 + (cid:107)A(cid:107)F r − 1 = 0.

F

k

Pk = Uk −

λ
tk

2

1.1 Related Work

Alternating Minimization is the go-to strategy for matrix factorization problems due to coupling
between two subsets of variables [24  1  64]. In the context of non-convex and non-smooth optimiza-
tion  recently PALM [8] was proposed and convergence to stationary point was proved. An inertial
variant  iPALM was proposed in [53]. However  such methods require a subset of variables to be
ﬁxed. We remove such a restriction here and take the contrary view by proposing non-alternating
schemes based on a powerful Bregman proximal minimization framework  which we review below.
Bregman Proximal Minimization extends upon the standard proximal minimization  where Breg-
man distances are used as proximity measures. Based on initial works in [6  4  9]  related inertial
variants were proposed in [46  67]. Related line-search methods were proposed in [52] based on
[10  11]. More related works in convex optimization include [49  40  42]. Recently  the symmetric
non-negative matrix factorization problem was solved with a non-alternating Bregman proximal
minimization scheme [21] with the following kernel generating distance

h(U) = (cid:107)U(cid:107)4

F

4

+ (cid:107)U(cid:107)2

F

2

.

However for the following applications  such a h is not suitable  unlike our Bregman distance.
Non-negative Matrix Factorization (NMF) is a variant of the matrix factorization problem which
requires the factors to have non-negative entries [25  37]. Some applications are hyperspectral
unmixing  clustering and others [24  22]. The non-negativity constraints pose new challenges [37]
and only convergence to a stationary point [24  31] is guaranteed  as NMF is NP-hard in general.
Under certain restrictions  NMF can be solved exactly [2  44] but such methods are computationally
infeasible. We give efﬁcient algorithms for NMF and show the superior performance empirically.
Matrix Completion is another variant of Matrix Factorization arising in recommender systems
[35] and bio-informatics [39  60]  which is an active research topic due to the hard non-convex
optimization problem [15  23]. The state-of-the-art methods were proposed in [33  65] and other
recent methods include [66]. Here  our algorithms are either faster or competitive.
Our algorithms are also applicable to Graph Regularized NMF (GNMF) [13]  Sparse NMF [8] 
Nuclear Norm Regularized problems [14  32]  Symmetric NMF via non-symmetric extension [68].

2 Matrix Factorization Problem Setting and Algorithms

Notation. We refer to [55] for standard notation  unless speciﬁed otherwise.
Formally  in a matrix factorization problem  given a matrix A ∈ RM×N   we want to obtain the
factors U ∈ RM×K and ZK×N such that A ≈ UZ  which is captured by the following non-convex
problem
(2.1)

Ψ(U  Z) :=

(cid:26)

(cid:27)

1

 

2 (cid:107)A − UZ(cid:107)2

F + R1(U) + R2(Z)

min

U∈U  Z∈Z

2 (cid:107)A − UZ(cid:107)2

where R1(U) + R2(Z) is the separable regularization term  1
F is the data-ﬁtting term 
and U Z are the constraint sets for U and Z respectively. Here  R1(U) and R2(Z) can be potentially
non-convex extended real valued functions and possibly non-smooth. In this paper  we propose
to make use of BPG and its inertial variant CoCaIn BPG to solve (2.1). The introduction of these
algorithms requires the following preliminary considerations.
Deﬁnition 2.1. (Kernel Generating Distance [9]) Let C be a nonempty  convex and open subset of
Rd. Associated with C  a function h : Rd → (−∞  +∞] is called a kernel generating distance if it
satisﬁes: (i) h is proper  lower semicontinuous and convex  with dom h ⊂ C and dom ∂h = C  and
(ii) h is C 1 on int dom h ≡ C. We denote the class of kernel generating distances by G(C).
For every h ∈ G(C)  the associated Bregman distance is given by Dh : dom h × int dom h → R+:

For examples  consider the following kernel generating distances:

Dh (x  y) := h (x) − [h (y) + (cid:104)∇h (y)   x − y(cid:105)] .

h0(x) =

1

2 (cid:107)x(cid:107)2  

h1(x) =

1

4 (cid:107)x(cid:107)4 +

1

2 (cid:107)x(cid:107)2

and h2(x) =

1

3 (cid:107)x(cid:107)3 +

1

2 (cid:107)x(cid:107)2 .

3

The Bregman distances associated with h0(x) is the Euclidean distance. The Bregman distances
associated with h1 and h2 appear in the context of non-convex quadratic inverse problems [9  46]
and non-convex cubic regularized problems [46] respectively. For a review on the recent literature 
we refer the reader to [59] and for early work on Bregman distances to [16].
These distance measures are key for development of algorithms for the following class of non-convex
additive composite problems

inf(cid:8)Ψ ≡ f (x) + g (x) : x ∈ C(cid:9)  

(P)

which is assumed to satisfy the following standard assumption [9].
Assumption A. (i) h ∈ G(C) with C = dom h. (ii) f : Rd → (−∞  +∞] is a proper and lower
semicontinuous function (potentially non-convex) with dom f ∩ C (cid:54)= ∅. (iii) g : Rd → (−∞  +∞]
is a proper and lower semicontinuous function (potentially non-convex) with dom h ⊂ dom g  which

is continuously differentiable on C. (iv) v(P) := inf(cid:8)Ψ (x) : x ∈ C(cid:9) > −∞.
inf (cid:8)Ψ(U  Z) := f1(U) + f2(Z) + g(U  Z) : (U  Z) ∈ C(cid:9) .

Matrix Factorization Example. A special case of (2.2) is the following problem 

(2.3)
We denote f (U  Z) = f1(U) + f2(Z). Many practical matrix factorization problems can be
cast into the form of (2.1). The choice of f and g is dependent on the problem  for which we
provide some examples in Section 3. Here f1  f2 satisfy the assumptions of f with dimensions
chosen accordingly. Moreover by deﬁnition  f is separable in U and Z  which we assume only
for practical reasons. Also  the choice of f  g may not be unique. For example  in (2.1) when
F the choice of f as in (2.3) can be R1 + R2 and
R1(U) = λ0
g = 1

F and R2(Z) = λ0
F . However  the other choice is to set g = Ψ and f := 0.

2 (cid:107)Z(cid:107)2

2 (cid:107)U(cid:107)2
2 (cid:107)A − UZ(cid:107)2

(2.2)

2.1 BPG-MF: Bregman Proximal Gradient for Matrix Factorization

(cid:27)

(cid:26)

We require the notion of Bregman Proximal Gradient Mapping [9  Section 3.1] given by

Tλ (x) = argmin

f (u) + (cid:104)∇g (x)   u − x(cid:105) +

Dh (u  x) : u ∈ C

.

(2.4)

1
λ

Then  the update step of Bregman Proximal Gradient (BPG) [9] for solving (2.2) is xk+1 ∈ Tλ(xk) 
for some λ > 0 and h ∈ G(C). Convergence of BPG relies on a generalized notion of Lipschitz
continuity  the so-called L-smad property (Deﬁntion 2.2).
Beyond Lipschitz continuity. BPG extends upon the popular proximal gradient methods  for which
convergence relies on Lipschitz continuity of the smooth part of the objective in (2.2). However 
such a notion of Lipschitz continuity is restrictive for many practical applications such as Poisson
linear inverse problems [4]  quadratic inverse problems [9  46]  cubic regularized problems [46] and
robust denoising problems with non-convex total variation regularization [46]. The extensions for
generalized notions of Lipschitz continuity of gradients is an active area of research [6  4  40  9]. We
consider the following from [9].
Deﬁnition 2.2 (L-smad property). The function g is said to be L-smooth adaptable (L-smad) on C
with respect to h  if and only if Lh − g and Lh + g are convex on C.
2 (cid:107)x(cid:107)2  L-smad property is implied by Lipschitz continuous gradient. Consider the
When h(x) = 1
function f (x) = x4  it is L-smad with respect to h(x) = x4 and L ≥ 1  however ∇f is not Lipschitz
continuous.
Now  we are ready to present the BPG algorithm for Matrix Factorization.
BPG-MF: BPG for Matrix Factorization.
Input. Choose h ∈ G(C) with C ≡ int dom h such that g satisﬁes L-smad with respect to h on C.
Initialization. (U1  Z1) ∈ int dom h and let λ > 0.
General Step. For k = 1  2  . . .  compute

− ∇Uh(Uk  Zk)   Qk = λ∇Zg(cid:0)Uk  Zk(cid:1)

(cid:8)λf (U  Z) +(cid:10)Pk  U(cid:11) +(cid:10)Qk  Z(cid:11) + h(U  Z)(cid:9) .

− ∇Zh(Uk  Zk)  

Pk = λ∇Ug(cid:0)Uk  Zk(cid:1)

(2.5)

(Uk+1  Zk+1) ∈ argmin
(U Z)∈C

4

Under Assumption A and the following one (mostly satisﬁed in practice)  BPG is well-deﬁned [9].
Assumption B. The range of Tλ lies in C and  for all λ > 0  the function h + λf is supercoercive.

The update step for BPG-MF is easy to derive from BPG  however convergence of BPG also relies
on the “right” choice of kernel generating distance h and the L-smad condition. Finding h such that
L-smad holds (also see Section 2.2) and that the update step can be given in closed form (also see
Section 2.4) is our main contribution and allows us to invoke the convergence results from [9]. The
convergence result states that the whole sequence of iterates generated by BPG-MF converges to a
stationary point  precisely given in Theorem 2.2. The result depends on the non-smooth KL-property
(see [7  3  8]) which is a mild requirement and is satisﬁed by most practical objectives. We provide
below the convergence result in [9  Theorem 4.1] adapted to BPG-MF.
Theorem 2.1 (Global Convergence of BPG-MF). Let Assumptions A and B hold and let g be L-smad
with respect to h  where h is assumed to be σ-strongly convex with full domain. Assume ∇g ∇h to
k∈N be a bounded sequence
generated by BPG-MF with 0 < λL < 1  and suppose Ψ satisﬁes the KL property  then  such a
sequence has ﬁnite length  and converges to a critical point.

be Lipschitz continuous on any bounded subset. Let(cid:8)(Uk+1  Zk+1)(cid:9)

2.2 New Bregman Distance for Matrix Factorization

We prove the L-smad property for the term g(U  Z) = 1
problem in (2.1). The kernel generating distance is a linear combination of
and h2(U  Z) := (cid:107)U(cid:107)2

2 (cid:107)A − UZ(cid:107)2

h1(U  Z) :=

(cid:107)U(cid:107)2

F + (cid:107)Z(cid:107)2

(cid:33)2

(cid:32)

F

2

F of the matrix factorization

F + (cid:107)Z(cid:107)2

F

2

 

(2.6)

and it is designed to also allow for closed form updates (see Section 2.4).
Proposition 2.1. Let g  h1  h2 be as deﬁned above. Then  for L ≥ 1  the function g satisﬁes the
L-smad property with respect to the following kernel generating distance
(2.7)
ha(U  Z) = 3h1(U  Z) + (cid:107)A(cid:107)F h2(U  Z) .

The proof is given in Section G.1 in the supplementary material. The Bregman distances considered
in previous works [46  9] are separable and not applicable for matrix factorization problems. The
inherent coupling between two subsets of variables U  Z is the main source of non-convexity in the
objective g. The kernel generating distance (in particular h1 in (2.7)) contains the interaction/coupling
terms between U and Z which makes it amenable for matrix factorization problems.

2.3 CoCaIn BPG-MF: An Adaptive Inertial Bregman Proximal Gradient Method

The goal of this section is to introduce an inertial variant of BPG-MF  called CoCaIn BPG-MF. The
effective step-size choice for BPG-MF can be restrictive due to large constant like (cid:107)A(cid:107)F (see (2.7)) 
for which we present a practical example in the numerical experiments. In order to allow for larger
step-sizes  one needs to adapt it locally  which is often done via a backtracking procedure. CoCaIn
BPG-MF combines inertial steps with a novel backtracking procedure proposed in [46].
Inertial algorithms often lead to better convergence [51  53  46]. The classical Nesterov Accelerated
Gradient (NAG) method [47] and the popular Fast Iterative Shrinkage-Thresholding Algorithm
(FISTA) [5] employ an extrapolation based inertial strategy. However  the extrapolation is governed
by a parameter which is typically scheduled to follow certain iteration-dependent scheme [47  29]and
is restricted to the convex setting. Recently with Convex–Concave Inertial Bregman Proximal
Gradient (CoCaIn BPG) [46]  it was shown that one could leverage the upper bound (convexity of
Lh − g) and lower bound (convexity of Lh + g) to incorporate inertia in an adaptive manner.
We recall now the update steps of CoCaIn BPG [46] to solve (2.2). Let h ∈ G(C)  λ > 0  and x0 =
x1 ∈ Rd be an initalization  then in each iteration the extrapolated point yk = xk + γk(xk − xk−1)
is computed followed by a BPG like update (at yk) given by xk+1 ∈ Tτk (yk)  where γk is the
inertial parameter and τk is the step-size parameter. Similar conditions to BPG are required for the
convergence to a stationary point. We use CoCaIn BPG for Matrix Factorization (CoCaIn BPG-MF)
and our proposed novel kernel generating distance h from (2.7) makes the convergence results of
[46] applicable. Along with Assumption B  we require the following assumption.

5

(cid:16)

(cid:17)

2

F

(cid:107)U(cid:107)2

is convex.

Assumption C. (i) There exists α ∈ R such that f (U  Z) − α
F + (cid:107)Z(cid:107)2
(ii) The kernel generating distance h is σ-strongly convex on RM×K × RK×N .
The Assumption C(i) refers to notion of semi-convexity of the function f  (see [50  46]) and seems
to be closely connected to the inertial feature of an algorithm. For notational brevity  we use
Dg (x  y) := g (x) − [g (y) + (cid:104)∇g (y)   x − y(cid:105)] which may also be negative if g is not a kernel
generating distance. Moreover  we use Dh((X1  Y1)  (X2  Y2)) as Dh(X1  Y1  X2  Y2). We
provide CoCaIn BPG-MF below.
CoCaIn BPG-MF: Convex–Concave Inertial BPG for Matrix Factorization.
Input. Choose δ  ε > 0 with 1 > δ >   h ∈ G(C) with C ≡ int dom h  g is L-smad on C w.r.t h.
Initialization. (U1  Z1) = (U0  Z0) ∈ int dom h ∩ dom f  ¯L0 > −α
General Step. For k = 1  2  . . .  compute extrapolated points

Y k
U = Uk + γk

where γk ≥ 0 such that

(δ − ε)Dh

where Lk satisﬁes

and Y k

Z = Zk + γk

≥ (1 + Lkτk−1)Dh

(1−δ)σ and τ0 ≤ ¯L−1
0 .
(cid:0)Zk − Zk−1(cid:1)  
(cid:1)  
(cid:0)Uk  Zk  Y k
(cid:1) .
(cid:1)

(cid:0)Uk − Uk−1(cid:1)
(cid:0)Uk−1  Zk−1  Uk  Zk(cid:1)
(cid:0)Uk  Zk  Y k
(cid:1)
(cid:0)Uk  Zk  Y k
Z )   Qk = τk∇Zg(cid:0)Y k
(cid:1)
(cid:0)Uk+1  Zk+1  Y k
(cid:1)

(cid:8)τkf (U  Z) +(cid:10)Pk  U(cid:11) +(cid:10)Qk  Z(cid:11) + h(U  Z)(cid:9)  
(cid:1) .

(cid:0)Uk+1  Zk+1  Y k

k }. Now  compute

− ∇Zh(Y k

≥ −LkDh

U  Y k
Z

U  Y k
Z

U  Y k
Z

U  Y k
Z

Dg

U  Y k
Z

U  Y k
Z

≤ ¯LkDh

(2.8)

(2.9)

(2.10)

(2.11)

(2.12)

U  Y k

Z )  

Pk = τk∇Ug(cid:0)Y k
Choose ¯Lk ≥ ¯Lk−1  and set τk ≤ min{τk−1  ¯L−1

− ∇Uh(Y k

U  Y k

U  Y k
Z

(Uk+1  Zk+1) ∈ argmin
(U Z)∈C

such that ¯Lk satisﬁes

Dg

The extrapolation step is performed in (2.8)  which is similar to NAG/FISTA. However  the inertia
cannot be arbitrary and the analysis from [46] requires step (2.9) which is governed by the convexity
of lower bound  Lkh + g  however only locally as in (2.10). The update step (2.11) is similar to
BPG-MF  however the step-size is controlled via the convexity of upper bound ¯Lkh − g  but only
locally as in (2.12). The local adaptation of the steps (2.10) and (2.12) is performed via backtracking.
Since  ¯Lk can be potentially very small compared to L  hence potentially large steps can be taken.
There is no restriction on Lk in each iteration  and smaller Lk can result in high value for the inertial
parameter γk. Thus the algorithm in essence aims to detect "local convexity" of the objective. The
update steps of CoCaIn BPG-MF can be executed sequentially without any nested loops for the
backtracking. One can always ﬁnd the inertial parameter γk in (2.9) due to [46  Lemma 4.1]. For
F +(cid:107)Z(cid:107)2
certain cases  (2.9) yields an explicit condition on γk. For example  for h(U  Z) = 1
F ) 
. We now provide below the convergence result from [46  Theorem
we have 0 ≤ γk ≤
5.2] adapted to CoCaIn BPG-MF.
Theorem 2.2 (Global Convergence of CoCaIn BPG-MF). Let Assumptions A  B and C hold  let g
be L-smad with respect to h with full domain. Assume ∇g ∇h to be Lipschitz continuous on any
k∈N be a bounded sequence generated by CoCaIn BPG-MF 
and suppose f  g satisfy the KL property  then  such a sequence has ﬁnite length  and converges to a
critical point.

bounded subset. Let(cid:8)(Uk+1  Zk+1)(cid:9)

(cid:113) δ−ε

2 ((cid:107)U(cid:107)2

1+τk−1Lk

2.4 Closed Form Solutions for Update Steps of BPG-MF and CoCaIn BPG-MF

Our second signiﬁcant contribution is to make BPG-MF and CoCaIn BPG-MF an efﬁcient choice for
solving Matrix Factorization  namely closed form expressions for the main update steps (2.5)  (2.11).
For the derivation  we refer to the supplementary material  here we just state our results.

6

1

For the L2-regularized problem

λ0
2

(cid:16)
F +(cid:13)(cid:13)−Qk(cid:13)(cid:13)2
(cid:0)(cid:13)(cid:13)−Pk(cid:13)(cid:13)2

F

h = ha

(cid:17)
(cid:1)r3 +(c2 +λ0)r−1 = 0 .

g(U  Z) =

f (U  Z) =

2 (cid:107)A − UZ(cid:107)2
F  

(cid:107)U(cid:107)2
with c1 = 3  c2 = (cid:107)A(cid:107)F and 0 < λ < 1 the BPG-MF updates are:
Uk+1 = −rPk   Zk+1 = −rQk with r ≥ 0   c1
For NMF with additional non-negativity constraints  we replace −Pk and −Qk by Π+(−Pk) and
Π+(−Qk) respectively where Π+(.) = max{0  .} and max is applied element wise.
Now consider the following L1-Regularized problem

F + (cid:107)Z(cid:107)2

F

 

1

h = ha .

g(U  Z) =

2 (cid:107)A − UZ(cid:107)2
F  

f (U  Z) = λ1 ((cid:107)U(cid:107)1 + (cid:107)Z(cid:107)1)  

(cid:16)(cid:13)(cid:13)Sλ1λ(−Pk)(cid:13)(cid:13)2

(2.13)
The soft-thresholding operator is deﬁned for any y ∈ Rd by Sθ (y) = max{|y| − θ  0} sgn (y) where
θ > 0. Set c1 = 3  c2 = (cid:107)A(cid:107)F and 0 < λ < 1 the BPG-MF updates with the above given g  f  h are:
(cid:17)
F +(cid:13)(cid:13)Sλ1λ(−Qk)(cid:13)(cid:13)2
Uk+1 = rSλ1λ(−Pk)  Zk+1 = rSλ1λ(−Qk) with r ≥ 0 and
r3 + c2r − 1 = 0 .
(cid:0)
(cid:1)) and Sλ1λ

(cid:0)Qk + λ1λeKeT
(cid:1)).
We denote a vector of ones as eD ∈ RD. For additional non-negativity constraints we need to re-
place Sλ1λ(−Pk) with Π+(−
Excluding the gradient computation  the computational complexity of our updates is O(M K + N K)
only  thanks to linear operations. PALM and iPALM additionally involve calculating Lipschitz
constants with at most O(K 2 max{M  N}2) computations. Examples like Graph Regularized NMF
(GNMF) [13]  Sparse NMF [8]  Matrix Completion [35]  Nuclear Norm Regularization [14  32] 
Symmetric NMF [68] and proofs are given in the supplementary material.

−Qk(cid:1) to Π+(−

(cid:0)Pk + λ1λeM eT

c1

K

N

F

2 (cid:107)Z(cid:107)2

2 (cid:107)U(cid:107)2

F and R2(Z) = λ0

3 Experiments
In this section  we show experiments for (2.1). Denote the regularization settings  R1: with R1 ≡
R2 ≡ 0  R2: with L2 regularization R1(U) = λ0
F for some λ0 > 0 
R3: with L1 Regularization R1(U) = λ0 (cid:107)U(cid:107)1 and R2(Z) = λ0 (cid:107)Z(cid:107)1 for some λ0 > 0.
Algorithms. We compare our ﬁrst order optimization algorithms  BPG-MF and CoCaIn BPG-MF 
and recent state-of-the-art optimization methods iPALM [53] and PALM [8]. We focus on algorithms
that guarantee convergence to a stationary point. We also use BPG-MF-WB  where WB stands for
"with backtracking"  which is equivalent to CoCaIn BPG-MF with γk ≡ 0. We use two settings for
iPALM  where all the extrapolation parameters are set to a single value β set to 0.2 and 0.4. PALM is
equivalent to iPALM if β = 0. We use the same initialization for all methods.
Simple Matrix Factorization. We set U = RM×K and Z = RK×N . We use a randomly generated
synthetic data matrix with A ∈ R200×200 and report performance in terms of function value for three
regularization settings  R1  R2 and R3 with K = 5. Note that this enforces a factorization into at
most rank 5 matrizes U and Z  which yields an additional implicit regularization. For R2 and R3 we
use λ0 = 0.1. CoCaIn BPG-MF is superior1 as shown in Figure 1 .
Statistical Evaluation. We also provide the statistical evaluation of all the algorithms in Figure 2 
for the above problem. The optimization variables are sampled from [0 0.1] and 50 random seeds
are considered. CoCaIn BPG outperforms other methods  however PALM methods are also very
competitive. In L1 regularization setting  the performance of CoCaIn BPG is the best. In all settings 
BPG-MF performance is worst due to a constant step size  which might change in settings where
local adapation with backtracking line search is computationally not feasible.
Matrix Completion. In recommender systems [35] given a matrix A with entries at few index pairs
in set Ω  the goal is to obtain factors U and Z that generalize via following optimization problem

min

U∈RM×K  Z∈RK×N

Ψ(U  Z) :=

1

2 (cid:107)PΩ (A − UZ)(cid:107)2

F +

λ0
2

(cid:107)U(cid:107)2

F + (cid:107)Z(cid:107)2

F

 

(3.1)

(cid:26)

(cid:16)

(cid:17)(cid:27)

1Note that in the y-axis label v(P) is the least objective value attained by any of the methods.

7

where PΩ preserves the given matrix entries and sets others to zero. We use 80% data of MovieLens-
100K  MovieLens-1M and MovieLens-10M [30] datasets and use other 20% to test (details in the
supplementary material). CoCaIn BPG-MF is faster than all methods as given in Figure 3.

(a) No Regularization

(b) L2-Regularization

(c) L1-Regularization

Figure 1: Simple Matrix Factorization on Synthetic Dataset.

(a) No Regularization

(b) L2-Regularization

(c) L1-Regularization

Figure 2: Statistical Evaluation on Simple Matrix Factorization.

(a) MovieLens-100K

(b) MovieLens-1M

(c) MovieLens-10M

Figure 3: Matrix Completion on MovieLens Datasets [30].

As evident from Figures 1  4  3  CoCaIn BPG-MF  BPG-MF-WB can result in better performance
than well known alternating methods. BPG-MF is not better than PALM and iPALM because of
prohibitively small step-sizes (due to (cid:107)A(cid:107)F in (2.7))  which is resolved by CoCaIn BPG-MF and
BPG-MF-WB using backtracking. Time comparisons are provided in the supplementary material 
where we show that our methods are competitive.

Conclusion and Extensions

We proposed non-alternating algorithms to solve matrix factorization problems  contrary to the typical
alternating strategies. We use the Bregman proximal algorithms  BPG [9] and an inertial variant
CoCaIn BPG [46] for matrix factorization problems. We developed a novel Bregman distance  crucial
for proving convergence to a stationary point. Moreover  we also provide non-trivial efﬁcient closed
form update steps for many matrix factorization problems. This line of thinking raises new open
questions  such as extensions to Tensor Factorization [34]  to Robust Matrix Factorization [65] 
stochastic variants [20  27  45  48] and state-of-the-art matrix factorization model [33].

8

100101102103Iterations(logscale)10−410−310−210−1100101102103104Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)100101102103Iterations(logscale)10−410−310−210−1100101102103104Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)100101102103Iterations(logscale)10−410−310−210−1100101102103104Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)1532.51535.01537.51540.01542.51545.01547.51550.0Functionvalue010203040NumberofseedsPALMiPALM(β=0.2)iPALM(β=0.4)CoCaInBPG-MFBPG-MF-WBBPG-MF1545.01547.51550.01552.51555.01557.51560.01562.5Functionvalue0510152025303540NumberofseedsPALMiPALM(β=0.2)iPALM(β=0.4)CoCaInBPG-MFBPG-MF-WBBPG-MF158515901595160016051610Functionvalue0123456NumberofseedsPALMiPALM(β=0.2)iPALM(β=0.4)CoCaInBPG-MFBPG-MF-WBBPG-MF100101102103Iterations(logscale)10−210−1100101102103104105Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)100101102103Iterations(logscale)100101102103104105106Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)100101102103Iterations(logscale)102103104105106107Ψ(Uk Zk)−v(P)(logscale)CoCaInBPG-MFBPG-MF-WBBPG-MFPALMiPALM(β=0.2)iPALM(β=0.4)References
[1] P. Ablin  D. Fagot  H. Wendt  A. Gramfort  and C. Févotte. A quasi-Newton algorithm on the orthogonal
manifold for NMF with transform learning. In IEEE International Conference on Acoustics  Speech and
Signal Processing (ICASSP)  pages 700–704  2019.

[2] S. Arora  R. Ge  R. Kannan  and A. Moitra. Computing a nonnegative matrix factorization–provably. In
Proceedings of the forty-fourth annual ACM symposium on Theory of computing  pages 145–162. ACM 
2012.

[3] H. Attouch and J. Bolte. On the convergence of the proximal algorithm for nonsmooth functions involving

analytic features. Mathematical Programming  116(1-2):5–16  2009.

[4] H. H. Bauschke  J. Bolte  and M. Teboulle. A descent lemma beyond Lipschitz gradient continuity:
ﬁrst-order methods revisited and applications. Mathematics of Operations Research  42(2):330–348  2017.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] B. Birnbaum  N. R. Devanur  and L. Xiao. Distributed algorithms via gradient descent for ﬁsher markets.

In Proceedings of the 12th ACM conference on Electronic commerce  pages 127–136. ACM  2011.

[7] J. Bolte  A. Daniilidis  A.S. Lewis  and M. Shiota. Clarke subgradients of stratiﬁable functions. SIAM

Journal on Optimization  18(2):556–572  2007.

[8] J. Bolte  S. Sabach  and M. Teboulle. Proximal alternating linearized minimization for nonconvex and

nonsmooth problems. Mathematical Programming  146(1-2):459–494  2014.

[9] J. Bolte  S. Sabach  M. Teboulle  and Y. Vaisbourd. First order methods beyond convexity and Lipschitz
gradient continuity with applications to quadratic inverse problems. SIAM Journal on Optimization 
28(3):2131–2151  2018.

[10] S. Bonettini  I. Loris  F. Porta  and M. Prato. Variable metric inexact line-search-based methods for

nonsmooth optimization. SIAM Journal on optimization  26(2):891–921  2016.

[11] S. Bonettini  I. Loris  F. Porta  M. Prato  and S. Rebegoldi. On the convergence of a linesearch based

proximal-gradient method for nonconvex optimization. Inverse Problems  33(5)  2017.

[12] J.-P. Brunet  P. Tamayo  T. R. Golub  and J. P. Mesirov. Metagenes and molecular pattern discovery using

matrix factorization. Proceedings of the National Academy of Sciences  101(12):4164–4169  2004.

[13] D. Cai  X. He  J. Han  and T. S. Huang. Graph regularized nonnegative matrix factorization for data
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence  33(8):1548–1560  2011.

[14] J. F. Cai  E. J. Candès  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[15] E. J. Candès and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational

mathematics  9(6):717  2009.

[16] Y. Censor and A. Lent. An iterative row-action method for interval convex programming. Journal of

Optimization Theory and Applications  34(3):321–353  1981.

[17] S. Chaudhuri  R. Velmurugan  and R. M. Rameshan. Blind image deconvolution. Springer  2016.

[18] E. Chouzenoux  J. C. Pesquet  and A. Repetti. A block coordinate variable metric forward–backward

algorithm. Journal of Global Optimization  66(3):457–485  2016.

[19] A. Cichocki  R. Zdunek  and S. Amari. Hierarchical ALS algorithms for nonnegative matrix and 3D tensor
factorization. In International Conference on Independent Component Analysis and Signal Separation 
pages 169–176. Springer  2007.

[20] D. Davis  D. Drusvyatskiy  and K. J. MacPhee. Stochastic model-based minimization under high-order

growth. ArXiv preprint arXiv:1807.00255  2018.

[21] R. A. Dragomir  A. d’Aspremont  and J. Bolte. ArXiv preprint arXiv:1901.10791  2019.

[22] F. Esposito  N. Gillis  and N. D. Buono. Orthogonal joint sparse NMF for microarray data analysis. Journal

of Mathematical Biology  pages 1–25  2019.

9

[23] H. Fang  Z. Zhang  Y. Shao  and C. J. Hsieh.

Improved bounded matrix completion for large-scale
recommender systems. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages 1654–
1660. AAAI Press  2017.

[24] N. Gillis. The why and how of nonnegative matrix factorization. Regularization  Optimization  Kernels 

and Support Vector Machines  12(257)  2014.

[25] N. Gillis and S. A. Vavasis. Fast and robust recursive algorithms for separable nonnegative matrix

factorization. IEEE Transactions on Pattern Analysis and Machine Intelligence  36(4):698–714  2014.

[26] G. H. Golub and C. F.V. Loan. Matrix computations  volume 3. John Hopkins University Press  2012.

[27] R. M. Gower  N. Loizou  X. Qian  A. Sailanbayev  E. Shulgin  and P. Richtarik. SGD: General analysis

and improved rates. ArXiv preprint arXiv:1901.09401  2019.

[28] B. D. Haeffele and R. Vidal. Structured low-rank matrix factorization: Global optimality  algorithms  and

applications. IEEE Transactions on Pattern Analysis and Machine Intelligence  2019.

[29] F. Hanzely  P. Richtarik  and L. Xiao. Accelerated Bregman proximal gradient methods for relatively

smooth convex optimization. ArXiv preprint arXiv:1808.03045  2018.

[30] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Transactions on Interactive

Intelligent Systems (TIIS)  5(4):19  2016.

[31] C. J. Hsieh and I. S. Dhillon. Fast coordinate descent methods with variable selection for non-negative
matrix factorization. In International Conference on Knowledge Discovery and Data Mining (ICKDDM) 
pages 1064–1072. ACM  2011.

[32] C. J. Hsieh and P. Olsen. Nuclear norm minimization via active subspace selection. In International

Conference on Machine Learning  pages 575–583  2014.

[33] P. Jawanpuria and B. Mishra. A uniﬁed framework for structured low-rank matrix learning. In J. Dy and
A. Krause  editors  Proceedings of the 35th International Conference on Machine Learning  volume 80 
pages 2254–2263. PMLR  2018.

[34] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review  51(3):455–500  2009.

[35] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

42(8):30–37  2009.

[36] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature 

401(6755):788  1999.

[37] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural

Information Processing Systems  pages 556–562  2001.

[38] W. Li and D.-Y. Yeung. Relation regularized matrix factorization. In International Joint Conference on

Artiﬁcal Intelligence (IJCAI)  pages 1126–1131  2009.

[39] C. Lu  M. Yang  F. Luo  F. X. Wu  M. Li  Y. Pan  Y. Li  and J. Wang. Prediction of lncRNA–disease

associations based on inductive matrix completion. Bioinformatics  34(19):3357–3364  2018.

[40] H. Lu  R. M. Freund  and Y. Nesterov. Relatively smooth convex optimization by ﬁrst-order methods  and

applications. SIAM Journal on Optimization  28(1):333–354  2018.

[41] R. Luss and M. Teboulle. Conditional gradient algorithms for rank-one matrix approximations with a

sparsity constraint. SIAM Review  55(1):65–98  2013.

[42] C. J. Maddison  D. Paulin  Y. W. Teh  and A. Doucet. Dual space preconditioning for gradient descent.

ArXiv preprint arXiv:1902.02257  2019.

[43] A. Mnih and R. R. Salakhutdinov. Probabilistic matrix factorization. In Advances in Neural Information

Processing Systems  pages 1257–1264  2008.

[44] A. Moitra. An almost optimal algorithm for computing nonnegative rank. SIAM Journal on Computing 

45(1):156–173  2016.

[45] M. C. Mukkamala and M. Hein. Variants of rmsprop and adagrad with logarithmic regret bounds. In

International Conference on Machine Learning (ICML)  pages 2545–2553  2017.

10

[46] M. C. Mukkamala  P. Ochs  T. Pock  and S. Sabach. Convex-Concave backtracking for inertial Bregman

proximal gradient algorithms in non-convex optimization. ArXiv preprint arXiv:1904.03537  2019.

[47] Y. E. Nesterov. A method for solving the convex programming problem with convergence rate O(1/k2).

Doklady Akademii Nauk SSSR  269(3):543–547  1983.

[48] L. M. Nguyen  P. H. Nguyen  M. V. Dijk  P. Richtárik  K. Scheinberg  and M. Takáˇc. SGD and Hogwild!

convergence without the bounded gradients assumption. ArXiv preprint arXiv:1802.03801  2018.

[49] Q. V. Nguyen. Forward–Backward splitting with Bregman distances. Vietnam Journal of Mathematics 

45(3):519–539  2017.

[50] P. Ochs. Local convergence of the heavy-ball method and ipiano for non-convex optimization. Journal of

Optimization Theory and Applications  177(1):153–180  2018.

[51] P. Ochs  Y. Chen  T. Brox  and T. Pock. iPiano: inertial proximal algorithm for nonconvex optimization.

SIAM Journal on Imaging Sciences  7(2):1388–1419  2014.

[52] P. Ochs  J. Fadili  and T. Brox. Non-smooth non-convex Bregman minimization: Uniﬁcation and new

algorithms. Journal of Optimization Theory and Applications  181(1):244–278  2019.

[53] T. Pock and S. Sabach. Inertial proximal alternating linearized minimization (iPALM) for nonconvex and

nonsmooth problems. SIAM Journal on Imaging Sciences  9(4):1756–1787  2016.

[54] M. Powell. On search directions for minimization algorithms. Mathematical programming  4(1):193–201 

1973.

[55] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis  volume 317 of Fundamental Principles of

Mathematical Sciences. Springer-Verlag  Berlin  1998.

[56] S. Sra and I. S. Dhillon. Generalized nonnegative matrix approximations with Bregman divergences. In

Advances in Neural Information Processing Systems  pages 283–290  2006.

[57] N. Srebro  J. Rennie  and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances in Neural

Information Processing Systems  pages 1329–1336  2005.

[58] J.-L. Starck  F. Murtagh  and J. Fadili. Sparse image and signal processing: wavelets  curvelets  morpho-

logical diversity. Cambridge University Press  2010.

[59] M. Teboulle. A simpliﬁed view of ﬁrst order methods for optimization. Mathematical Programming 

170(1):67–96  2018.

[60] K. Thung  P. T. Yap  E. Adeli  S. W. Lee  D. Shen  and Alzheimer’s Disease Neuroimaging Initiative.
Conversion and time-to-conversion predictions of mild cognitive impairment using low-rank afﬁnity pursuit
denoising and matrix completion. Medical image analysis  45:68–82  2018.

[61] B. Wen  X. Chen  and T. K. Pong. Linear convergence of proximal gradient algorithm with extrapolation for
a class of nonconvex nonsmooth minimization problems. SIAM Journal on Optimization  27(1):124–145 
2017.

[62] Y. Xu  Z. Li  J. Yang  and D. Zhang. A survey of dictionary learning algorithms for face recognition. IEEE

access  5:8502–8514  2017.

[63] Y. Xu and W. Yin. A block coordinate descent method for regularized multiconvex optimization with
applications to nonnegative tensor factorization and completion. SIAM Journal on imaging sciences 
6(3):1758–1789  2013.

[64] Lei Yang  Ting Kei Pong  and Xiaojun Chen. A nonmonotone alternating updating method for a class of

matrix factorization problems. SIAM Journal on Optimization  28(4):3402–3430  2018.

[65] Q. Yao and J. Kwok. Scalable robust matrix factorization with nonconvex loss. In Advances in Neural

Information Processing Systems  pages 5061–5070  2018.

[66] A. W. Yu  W. Ma  Y. Yu  J. Carbonell  and S. Sra. Efﬁcient structured matrix rank minimization. In

Advances in Neural Information Processing Systems  pages 1350–1358  2014.

[67] X. Zhang  R. Barrio  M. Martinez  H. Jiang  and L. Cheng. Bregman proximal gradient algorithm with ex-
trapolation for a class of nonconvex nonsmooth minimization problems. ArXiv preprint arXiv:1904.11295 
2019.

[68] Z. Zhu  X. Li  K. Liu  and Q. Li. Dropping symmetry for fast symmetric nonnegative matrix factorization.

In Advances in Neural Information Processing Systems  pages 5154–5164  2018.

11

,Mahesh Chandra Mukkamala
Peter Ochs