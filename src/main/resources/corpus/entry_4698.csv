2012,Repulsive Mixtures,Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed  finite mixtures and infinite mixtures  relying on Dirichlet processes and modifications  have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular  different components can be introduced that are very similar and hence redundant.   Such redundancy leads to too many clusters that are too similar  degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem  we propose a novel prior that generates components from a repulsive process  automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set.,Repulsive Mixtures

Department of Statistical Science

Gatsby Computational Neuroscience Unit

Francesca Petralia

Duke University

fp12@duke.edu

Vinayak Rao

University College London

vrao@gatsby.ucl.ac.uk

David B. Dunson

Department of Statistical Science

Duke University

dunson@stat.duke.edu

Abstract

Discrete mixtures are used routinely in broad sweeping applications ranging from
unsupervised settings to fully supervised multi-task learning. Indeed  ﬁnite mix-
tures and inﬁnite mixtures  relying on Dirichlet processes and modiﬁcations  have
become a standard tool. One important issue that arises in using discrete mix-
tures is low separation in the components; in particular  different components can
be introduced that are very similar and hence redundant. Such redundancy leads
to too many clusters that are too similar  degrading performance in unsupervised
learning and leading to computational problems and an unnecessarily complex
model in supervised settings. Redundancy can arise in the absence of a penalty on
components placed close together even when a Bayesian approach is used to learn
the number of components. To solve this problem  we propose a novel prior that
generates components from a repulsive process  automatically penalizing redun-
dant components. We characterize this repulsive prior theoretically and propose
a Markov chain Monte Carlo sampling algorithm for posterior computation. The
methods are illustrated using synthetic examples and an iris data set.

Key Words: Bayesian nonparametrics; Dirichlet process; Gaussian mixture model; Model-based
clustering; Repulsive point process; Well separated mixture.

Introduction

1
Discrete mixture models characterize the density of y ∈ Y ⊂ (cid:60)m as

k(cid:88)

f (y) =

phφ(y; γh)

(1)

h=1

where p = (p1  . . .   pk)T is a vector of probabilities summing to one  and φ(·; γ) is a kernel de-
pending on parameters γ ∈ Γ  which may consist of location and scale parameters. In analyses of
ﬁnite mixture models  a common concern is over-ﬁtting in which redundant mixture components
located close together are introduced. Over-ﬁtting can have an adverse impact on predictions and
degrade unsupervised learning. In particular  introducing components located close together can
lead to splitting of well separated clusters into a larger number of closely overlapping clusters. Ide-
ally  the criteria for selecting k in a frequentist analysis and the prior on k and {γh} in a Bayesian
analysis should guard against such over-ﬁtting. However  the impact of the criteria used and prior
chosen can be subtle.

1

Recently  [1] studied the asymptotic behavior of the posterior distribution in over-ﬁtted Bayesian
mixture models having more components than needed. They showed that a carefully chosen prior
will lead to asymptotic emptying of the redundant components. However  several challenging prac-
tical issues arise. For their prior and in standard Bayesian practice  one assumes that γh ∼ P0
independently a priori. For example  if we consider a ﬁnite location-scale mixture of multivariate
Gaussians  one may choose P0 to be multivariate Gaussian-inverse Wishart. However  the behavior
of the posterior can be sensitive to P0 for ﬁnite samples  with higher variance P0 favoring allocation
to fewer clusters. In addition  drawing the component-speciﬁc parameters from a common prior
tends to favor components located close together unless the variance is high.
Sensitivity to P0 is just one of the issues. For ﬁnite samples  the weight assigned to redundant
components is often substantial. This can be attributed to non- or weak identiﬁability. Each mixture
component can potentially be split into multiple components having the same parameters. Even
if exact equivalence is ruled out  it can be difﬁcult to distinguish between models having different
degrees of splitting of well-separated components into components located close together. This
issue can lead to an unnecessarily complex model  and creates difﬁculties in estimating the number
of components and component-speciﬁc parameters. Existing strategies  such as the incorporation
of order constraints  do not adequately address this issue  since it is difﬁcult to choose reasonable
constraints in multivariate problems and even with constraints  the components can be close together.
The problem of separating components has been studied for Gaussian mixture models ([2]; [3]).
Two Gaussians can be separated by placing an arbitrarily chosen lower bound on the distance be-
tween their means. Separated Gaussians have been mainly utilized to speed up convergence of the
Expectation-Maximization (EM) algorithm. In choosing a minimal separation level  it is not clear
how to obtain a good compromise between values that are too low to solve the problem and ones
that are so large that one obtains a poor ﬁt. To avoid such arbitrary hard separation thresholds  we
instead propose a repulsive prior that smoothly pushes components apart.
In contrast to the vast majority of the recent Bayesian literature on discrete mixture models  instead
of drawing the component-speciﬁc parameters {γh} independently from a common prior P0  we
propose a joint prior for {γ1  . . .   γk} that is chosen to assign low density to γhs located close
together. The deviation from independence is speciﬁed a priori by a pair of repulsion parameters.
The proposed class of repulsive mixture models will only place components close together if it
results in a substantial gain in model ﬁt. As we illustrate  the prior will favor a more parsimonious
representation of densities  while improving practical performance in unsupervised learning. We
provide strong theoretical results on rates of posterior convergence and develop Markov chain Monte
Carlo algorithms for posterior computation.

2 Bayesian repulsive mixture models

2.1 Background on Bayesian mixture modeling

Considering the ﬁnite mixture model in expression (1)  a Bayesian speciﬁcation is completed by
choosing priors for the number of components k  the probability weights p  and the component-
speciﬁc parameters γ = (γ1  . . .   γk)T . Typically  k is assigned a Poisson or multinomial prior  p a
Dirichlet(α) prior with α = (α1  . . .   αk)T   and γh ∼ P0 independently  with P0 often chosen to
be conjugate to the kernel φ. Posterior computation can proceed via a reversible jump Markov chain
Monte Carlo algorithm involving moves for adding or deleting mixture components. Unfortunately 
in making a k → k + 1 change in model dimension  efﬁcient moves critically depend on the choice
of proposal density. [4] proposed an alternate Markov chain Monte Carlo method  which treats the
parameters as a marked point process  but does not have clear computational advantages relative to
reversible jump.
It has become popular to use over-ﬁtted mixture models in which k is chosen as a conservative
upper bound on the number of components under the expectation that only relatively few of the
components will be occupied by subjects in the sample. From a practical perspective  the success of
over-ﬁtted mixture models has been largely due to ease in computation.
As motivated in [5]  simply letting αh = c/k for h = 1  . . .   k and a constant c > 0 leads to an
approximation to a Dirichlet process mixture model for the density of y  which is obtained in the

2

limit as k approaches inﬁnity. An alternative ﬁnite approximation to a Dirichlet process mixture is
obtained by truncating the stick-breaking representation of [6]  leading to a similarly simple Gibbs
sampling algorithm [7]. These approaches are now used routinely in practice.

2.2 Repulsive densities

We seek a prior on the component parameters in (1) that automatically favors spread out compo-
nents near the support of the data. Instead of generating the atoms γh independently from P0  one
could generate them from a repulsive process that automatically pushes the atoms apart. This idea
is conceptually related to the literature on repulsive point processes [8]. In the spatial statistics liter-
ature  a variety of repulsive processes have been proposed. One such model assumes that points are
clustered spatially  with the cluster centers having a Strauss density [9]  that is p(k  γ) ∝ βkρr(γ)
where k is the number of clusters  β > 0  0 < ρ ≤ 1 and r(γ) is the number of pairwise centers that
lie within a pre-speciﬁed distance r of each other. A possibly unappealing feature is that repulsion
is not directly dependent on the pairwise distances between the clusters. We propose an alternative
class of priors  which smoothly push apart components based on pairwise distances.
Deﬁnition 1. A density h(γ) is repulsive if for any δ > 0 there is a corresponding  > 0 such that
h(γ) < δ for all γ ∈ Γ \ G  where G = {γ : d(γs  γi) > ; s = 1  . . .   k; i < s} and d is a metric.

Depending on the speciﬁcation of the metric d(γs  γj)  a prior satisfying deﬁnition 1 may limit over-
ﬁtting or favor well separated clusters. When d(γs  γj) is the distance between sub-vectors of γs and
γj corresponding to only locations the proposed prior favors well separated clusters. Instead  when
d(γs  γj) is the distance between the sth and jth kernel  a prior satisfying deﬁnition 1 limits over-
ﬁtting in density estimation. Though both cases can be implemented  in this paper we will focus
exclusively on the clustering problem. As a convenient class of repulsive priors which smoothly
push components apart  we propose

(cid:33)

(cid:32) k(cid:89)

h=1

π(γ) = c1

g0(γh)

h(γ) 

(2)

with c1 being the normalizing constant that depends on the number of components k. The proposed
prior is related to a class of point processes from the statistical physics and spatial statistics
literature referred to as Gibbs processes [10]. We assume g0 : Γ → (cid:60)+ and h : Γk → [0 ∞) are
continuous with respect to Lesbesgue measure  and h is bounded above by a positive constant c2
and is repulsive according to deﬁnition 1. It follows that density π deﬁned in (2) is also repulsive.
A special hardcore repulsion is produced if the repulsion function is zero when at least one pairwise
distance is smaller than a pre-speciﬁed threshold. Such a density implies choosing a minimal
separation level between the atoms. As mentioned in the introduction  we avoid such arbitrary
hard separation thresholds by considering repulsive priors that smoothly push components apart. In
particular  we propose two repulsion functions deﬁned as

(cid:89)

g{d(γs  γj)}

(3)

h(γ) =

{(s j)∈A}

(4)
with A = {(s  j) : s = 1  . . .   k; j < s} and g : (cid:60)+ → [0  M ] a strictly monotone differentiable
function with g(0) = 0  g(x) > 0 for all x > 0 and M < ∞. It is straightforward to show that h
in (3) and (4) is integrable and satisﬁes deﬁnition 1. The two alternative repulsion functions differ
in their dependence on the relative distances between components  with all the pairwise distances
playing a role in (3)  while (4) only depends on the minimal separation. A ﬂexible choice of g
corresponds to

h(γ) = min

{(s j)∈A} g{d(γs  γj)}

g{d(γs  γj)} = exp(cid:2) − τ{d(γs  γj)}−ν(cid:3) 

(5)

where τ > 0 is a scale parameter and ν is a positive integer controlling the rate at which g approaches
zero as d(γs  γj) decreases. Figure 1 shows contour plots of the prior π(γ1  γ2) deﬁned as (2) with
g0 being the standard normal density  the repulsive function deﬁned as (3) or (4) and g deﬁned as
(5) for different values of (τ  ν). As τ and ν increase  the prior increasingly favors well separated
components.

3

Figure 1: Contour plots of the repulsive prior π(γ1  γ2) under (3)  either (4) or (5) and (6) with
hyperparameters (τ  ν) equal to (I)(1  2)  (II)(1  4)  (III)(5  2) and (IV )(5  4)

2.3 Theoretical properties

Let the true density f0 : (cid:60)m → (cid:60)+ be deﬁned as f0 =(cid:80)k0
distance. Let f =(cid:80)k
kernels. Let | · |1 denote the L1 norm and KL(f0  f ) = (cid:82) f0 log(f0/f ) refer to the Kullback-

h=1 p0hφ(γ0h) with γ0h ∈ Γ and γ0js
such that there exists an 1 > 0 such that min{(s j):s<j} d(γ0s  γ0j) ≥ 1 with d being the Euclidean
h=1 phφ(γh) with γh ∈ Γ. Let γ ∼ π with γ = (γ1  . . .   γk)T and π satisfying
deﬁnition 1. Let p ∼ λ with λ = Dirichlet(α) and k ∼ µ with µ(k = k0) > 0. Let θ = (p  γ).
These assumptions on f0 and f will be referred to as condition B0. Let Π be the prior induced on
∪∞
j=1Fk  where Fk is the space of all distributions deﬁned as (1).
We will focus on γ being a location parameter  though the results can be extended to location-scale

Leibler (K-L) divergence between f0 and f. Density f0 belongs to the K-L support of the prior Π
if Π{f : KL(f0  f ) < } > 0 for all  > 0. The next lemma provides sufﬁcient conditions under
which the true density is in the K-L support of the prior.
Lemma 1. Assume condition B0 is satisﬁed with m = 1. Let D0 be a compact set containing
parameters (γ01  . . .   γ0k0 ). Suppose γ ∼ π with π satisfying deﬁnition 1. Let φ and π satisfy the
following conditions:
A1. for any y ∈ Y  the map γ → φ(y; γ) is uniformly continuous
A2. for any y ∈ Y  φ(y; γ) is bounded above by a constant

(cid:12)(cid:12)log(cid:8)supγ∈D0 φ(γ)(cid:9) − log {inf γ∈D0 φ(γ)}(cid:12)(cid:12) < ∞

A3.(cid:82) f0

A4. π is continuous with respect to Lebesgue measure and for any vector x ∈ Γk with
min{(s j):s<j} d(xs  xj) ≥ υ for some υ > 0 there is a δ > 0 such that π(γ) > 0 for all γ
satisfying ||γ − x||1 < δ
Then f0 is in the K-L support of the prior Π.

Lemma 2. The repulsive density in (2) with h deﬁned as either (3) or (4) satisﬁes condition A4 in
lemma 1.

The next lemma formalizes the posterior rate of concentration for univariate location mixtures of
Gaussians.
Lemma 3. Let condition B0 be satisﬁed  let m = 1 and φ be the normal kernel depending on a
location parameter γ and a scale parameter σ. Assume that condition (i)  (ii) and (iii) of theorem
3.1 in [11] and assumption A4 in lemma 1 are satisﬁed. Furthermore  assume that
C1) the joint density π leads to exchangeable random variables and for all k the marginal density

of the location parameter γ1 satisﬁes πm(|γ1| ≥ t) (cid:46) exp(cid:0)−q1t2(cid:1) for a given q1 > 0

4

(I)−505−505(II)−505−505(III)−505−505(IV)−505−505C2) there are constants u1  u2  u3 > 0  possibly depending on f0  such that for any  ≤ u3

π(||γ − γ0||1 ≤ ) ≥ u1 exp(−u2k0 log(1/))

Then the posterior rate of convergence relative to the L1 metric is n = n−1/2 log n.
Lemma 3 is essentially a modiﬁcation of theorem 3.1 in [11] to the proposed repulsive mixture
model. Lemma 4 gives sufﬁcient conditions for π to satisfy condition C1 and C2 in lemma 3.
Lemma 4. Let π be deﬁned as (2) and h be deﬁned as either (3) or (4)  then π satisﬁes condition
C2 in lemma 3. Furthermore  if for a positive constant n1 the function g0 satisﬁes g0(|x| ≥ t) (cid:46)
exp(−n1t2)  π satisﬁes condition C1 in lemma 3.
As motivated above  when the number of mixture components is chosen to be unnecessarily large  it
is appealing for the posterior distribution of the weights of the extra components to be concentrated
near zero. Theorem 1 formalizes the rate of concentration with increasing sample size n. One
of the main assumptions required in theorem 1 is that the posterior rate of convergence relative to
the L1 metric is δn = n−1/2(log n)q with q ≥ 0. We provided the contraction rate  under the
proposed prior speciﬁcation and univariate Gaussian kernel  in lemma 3. However  theorem 1 is a
more general statement and it applies to multivariate mixture density of any kernel.
Theorem 1. Let assumptions B0 − B5 be satisﬁed. Let π be deﬁned as (2) and h be deﬁned as
either (3) or (4). If ¯α = max(α1  . . .   αk) < m/2 and for positive constants r1  r2  r3 the function
g satisﬁes g(x) ≤ r1xr2 for 0 ≤ x < r3 then

(cid:34)

(cid:40)

(cid:32) k(cid:88)

(cid:33)

(cid:41)(cid:35)

M→∞ lim sup
lim
n→∞

E0
n

P

min
{σ∈Sk}

pσ(i)

i=k0+1

> M n−1/2(log n)q(1+s(k0 α)/sr2 )

= 0

with s(k0  α) = k0 − 1 + mk0 + ¯α(k − k0)  sr2 = r2 + m/2 − ¯α and Sk the set of all possible
permutations of {1  . . .   k}.
Assumptions (B1 − B5) can be found in the supplementary material. Theorem 1 is a modiﬁcation
of theorem 1 in [1] to the proposed repulsive mixture model. Theorem 1 implies that the posterior

expectation of weights of the extra components is of order O(cid:8)n−1/2(log n)q(1+s(k0 α)/sr2 )(cid:9). When

g is deﬁned as (5)  parameters r1 and r2 can be chosen such that r1 = τ and r2 = ν.
When the number of components is unknown  with only an upper bound known  the posterior rate
of convergence is equivalent to the parametric rate n−1/2 [12]. In this case  the rate in theorem 1
is n−1/2 under usual priors or the repulsive prior. However  in our experience using usual priors 
the sum of the extra components can be substantial in small to moderate sample sizes  and often
has high variability. As we show in Section 3  for repulsive priors the sum of the extra component
weights is close to zero and has small variance for small as well as large sample sizes. On the
other hand  when an upper bound on the number of components is unknown  the posterior rate of
concentration is n−1/2(log n)q with q > 0. In this case  according to theorem 1  using the proposed
prior speciﬁcation the logarithmic factor in theorem 1 of [1] can be improved.

2.4 Parameter calibration and posterior computation

The parameters involved in the repulsion function h are chosen such that a priori  with high proba-
bility  the clusters will be adequately separated. Consider the case where φ is a location-scale kernel
with location and scale parameters (γ  Σ) and is symmetric about γ. Here  it is natural to relate
the separation of two densities to the distance between their location parameters. The following
deﬁnition introduces the concept of separation level between two densities.
Deﬁnition 2. Let f1 and f2 be two densities having location-scale parameters (γ1  Σ1) and (γ2  Σ2)
respectively  with γ1  γ2 ∈ Γ and Σ1  Σ2 ∈ Ω. Given a metric t(· ·)  a positive constant c and a
function ω : Ω × Ω → (cid:60)+  f1 and f2 are c-separated if

t(γ1  γ2) ≥ cω(Σ1  Σ2)1/2

Deﬁnition 2 is in the spirit of [2] but generalized to any symmetric location-scale kernel. A mixture
of k densities is c-separated if all pairs of densities are c-separated. The parameters of the repulsion

5

Figure 2: (I) Student’s t density  (II) two-components mixture of poorly (solid) and well separated
(dot-dash) Gaussian densities  referred as (IIa  IIb)  (III) mixture of poorly (dot-dash) and well
separated (solid) Gaussian and Pearson densities  referred as (IIIa  IIIb)  (IV ) two-components
mixture of two-dimensional non-spherical Gaussians

function  (τ  ν)  will be chosen such that  for an a priori chosen separation level c  deﬁnition 2
is satisﬁed with high probability. In practice  for a given pair (τ  ν)  we estimate the probability
of pairwise c-separation empirically by simulating N replicates of (γh  Σh) for each component
h = 1  . . .   k from the prior. The appropriate values (τ  ν) are obtained by starting with small values 
and increasing until the pre-speciﬁed pairwise c-separated probability is reached. In practice  only τ
will be calibrated to reach a particular probability value. This is because ν controls the rate at which
the density tends to zero as two components approach but not the separation level across them. In
practice we have found that ν = 2 provides a good default value and we ﬁx ν at this value in all our
applications below.
A possible issue with the proposed repulsive mixture prior is that the full conditionals are nonstan-
dard  complicating posterior computation. To address this  we propose a data augmentation scheme 
introducing auxiliary slice variables to facilitate sampling [13]. This algorithm is straightforward
to implement and is efﬁcient by MCMC standards. Further details can be found in the supplemen-
tary material. It will be interesting in future work to develop fast approximations to MCMC for
implementation of repulsive mixture models  such as variational methods for approximating the full
posterior and optimization methods for obtaining a maximum a posteriori estimate. The latter ap-
proach would provide an alternative to usual maximum likelihood estimation via the EM algorithm 
which provides a penalty on components located close together.

3 Synthetic examples

Synthetic toy examples were considered to assess the performance of the repulsive prior in density
estimation  classiﬁcation and emptying the extra components. Figure 2 plots the true densities in the
various synthetic cases that we considered. For each synthetic dataset  repulsive and non-repulsive
mixture models were compared considering a ﬁxed upper bound on the number of components; extra
components should be assigned small probabilities and hence effectively excluded. The auxiliary
variable sampler was run for 10  000 iterations with a burn-in of 5  000. The chain was thinned by
keeping every 10th simulated draw. To overcome the label switching problem  the samples were
post-processed following the algorithm of [14]. Details on parameters involved in the true densities
and choice of prior distributions can be found in the supplementary material.
Table 1 shows summary statistics of the K-L divergence  the misclassiﬁcation error and the sum of
extra weights under repulsive and non-repulsive mixtures with six mixture components as the upper
bound. Table 1 shows also the misclassiﬁcation error resulting from hierarchical clustering [15]. In
practice  observations drawn from the same mixture component were considered as belonging to the
same category and for each dataset a similarity matrix was constructed. The misclassiﬁcation error
was established in terms of divergence between the true similarity matrix and the posterior similar-

6

−10−5051000.10.20.30.4(I)−20200.20.40.6(II)−3−2−1012300.20.40.60.81(III)(IV)−2−10123−2−10123ity matrix. As shown in table 1  the K-L divergences under repulsive and non-repulsive mixtures
become more similar as the sample size increases. For smaller sample sizes  the results are more
similar when components are very well separated. Since a repulsive prior tends to discourage over-
lapping mixture components  a repulsive model might not estimate the density quite as accurately
when a mixture of closely overlapping components is needed. However  as the sample size increases 
the ﬁtted density approaches the true density regardless of the degree of closeness among clusters.
Again  though repulsive and non-repulsive mixtures perform similarly in estimating the true density 
repulsive mixtures place considerably less probability on extra components leading to more inter-
pretable clusters. In terms of misclassiﬁcation error  the repulsive model outperforms the other two
approaches while  in most cases  the worst performance was obtained by the non-repulsive model.
Potentially  one may favor fewer clusters  and hence possibly better separated clusters  by penalizing
the introduction of new clusters more through modifying the precision in the Dirichlet prior for the
weights; in the supplemental materials  we demonstrate that this cannot solve the problem.

Table 1: Mean and standard deviation of K-L divergence  misclassiﬁcation error and sum of extra
weights resulting from non-repulsive (N-R) and repulsive (R) mixtures with a maximum number of
clusters equal to six under different synthetic data scenarios

n=100

n=1000

I

IIa

IIb

IIIa

IIIb

IV

I

IIa

IIb

IIIa

IIIb

IV

K-L divergence
N-R

R

0·05
0·03
0·03
0·02

0·03
0·01
0·08
0·02

Misclassiﬁcation
HCT 0·12
0·68
N-R
0·09
0·06
0·05

0·11
0·26
0·10
0·09
0·04

R

Sum of extra weights
0·21
N-R
0·11
0·01
0·01

0·30
0·10
0·01
0·01

R

0·07
0·02
0·09
0·03

0·41
0·06
0·05
0·00
0·02

0·09
0·07
0·01
0·01

0·05
0·02
0·07
0·03

0·12
0·17
0·09
0·05
0·03

0·16
0·09
0·01
0·01

0·08
0·03
0·09
0·03

0·78
0·05
0·06
0·00
0·01

0·07
0·07
0·01
0·01

0·22
0·04
0·24
0·04

0·21
0·13
0·05
0·09
0·03

0·13
0·07
0·08
0·05

0·00
0·00
0·01
0·00

0·45
0·65
0·11
0·05
0·05

0·30
0·11
0·01
0·01

0·01
0·00
0·01
0·00

0·42
0·24
0·08
0·08
0·02

0·21
0·11
0·00
0·00

0·01
0·00
0·01
0·00

0·14
0·03
0·04
0·00
0·02

0·03
0·04
0·00
0·00

0·00
0·00
0·01
0·00

0·42
0·14
0·08
0·03
0·03

0·16
0·10
0·00
0·00

0·01
0·00
0·01
0·00

0·09
0·02
0·03
0·00
0·01

0·03
0·03
0·00
0·00

0·02
0·00
0·03
0·00

0·20
0·19
0·02
0·18
0·01

0·29
0·03
0·26
0·03

4 Real data

We assessed the clustering performance of the proposed method on a real dataset. This dataset
consists of 150 observations from three different species of iris each with four measurements. This
dataset was previously analyzed by [16] and [17] proposing new methods to estimate the number of
clusters based on minimizing loss functions. They concluded the optimal number of clusters was
two. This result did not agree with the number of species due to low separation in the data between
two of the species. Such point estimates of the number of clusters do not provide a characterization
of uncertainty in clustering in contrast to Bayesian approaches.
Repulsive and non-repulsive mixtures were ﬁtted under different choices of upper bound on the
number of components. Since the data contains three true biological clusters  with two of these
having similar distributions of the available features  we would expect the posterior to concen-
trate on two or three components. Posterior means and standard deviations of the three highest
weights were (0·30  0·23  0·13) and (0·05  0·04  0·04) for non-repulsive and (0·60  0·30  0·04) and
(0·04  0·03  0·02) for repulsive under six components. Clearly  repulsive priors lead to a posterior
more concentrated on two components  and assign low probability to more than three components.

7

Figure 3: Posterior density of the total probability weight assigned to more than three components
in the Iris data under a max of 6 or 10 components for non-repulsive (6:solid  10:dash-dot) and
repulsive (6:dash  10:dot) mixtures.

Figure 3 shows the density of the total probability assigned to the extra components. This quantity
was computed considering the number of species as the true number of clusters. According to
ﬁgure 3  our repulsive prior speciﬁcation leads to extra component weights very close to zero
regardless of the upper bound on the number of components. The posterior uncertainty is also
small. Non-repulsive mixtures assign large weight to extra components  with posterior uncertainty
increasing considerably as the number of components increases.

Discussions

We have proposed a new repulsive mixture modeling framework  which should lead to substantially
improved unsupervised learning (clustering) performance in general applications. A key aspect is
soft penalization of components located close together to favor  without sharply enforcing  well sep-
arated clusters that should be more likely to correspond to the true missing labels. We have focused
on Bayesian MCMC-based methods  but there are numerous interesting directions for ongoing re-
search  including fast optimization-based approaches for learning mixture models with repulsive
penalties.

Acknowledgments

This research was partially supported by grant 5R01-ES-017436-04 from the National Institute of
Environmental Health Sciences (NIEHS) of the National Institutes of Health (NIH) and DARPA
MSEE.

8

00.10.20.30.40.50.60.70510152025References
[1] J. Rousseau and K. Mengersen. Asymptotic Behaviour of the Posterior Distribution in Over-Fitted Mod-

els. Journal of the Royal Statistical Society B  73:689–710  2011.

[2] S. Dasgupta. Learning Mixtures of Gaussians. Proceedings of the 40th Annual Symposium on Foundations

of Computer Science  pages 633–644  1999.

[3] S. Dasgupta and L. Schulman. A Probabilistic Analysis of EM for Mixtures of Separated  Spherical

Gaussians. The Journal of Machine Learning Research  8:203–226  2007.

[4] M. Stephens. Bayesian Analysis of Mixture Models with an Unknown Number of Components - An

Alternative to Reversible Jump Methods. The Annals of Statistics  28:40–74  2000.

[5] H. Ishwaran and M. Zarepour. Dirichlet Prior Sieves in Finite Normal Mixtures. Statistica Sinica  12:941–

963  2002.

[6] J. Sethuraman. A Constructive Denition of Dirichlet Priors. Statistica Sinica  4:639–650  1994.
[7] H. Ishwaran and L. F. James. Gibbs Sampling Methods for Stick-Breaking Priors. Journal of the American

Statistical Association  96:161–173  2001.

[8] M. L. Huber and R. L. Wolpert. Likelihood-Based Inference for Matern Type-III Repulsive Point Pro-

cesses. Advances in Applied Probability  41:958–977  2009.

[9] A. Lawson and A. Clark. Spatial Cluster Modeling. Chapman & Hall CRC  London  UK  2002.
[10] D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes. Springer  2008.
[11] Catia Scricciolo. Posterior Rates of Convergence for Dirichlet Mixtures of Exponential Power Densities.

Electronic Journal of Statistics  5:270–308  2011.

[12] H. Ishwaran  L. F. James  and J. Sun. Bayesian Model Selection in Finite Mixtures by Marginal Density

Decompositions. Journal of American Statistical Association  96:1316–1332  2001.

[13] Paul Damien  Jon Wakeﬁeld  and Stephen Walker. Gibbs Sampling for Bayesian Non-Conjugate and
Hierarchical Models by Using Auxiliary Variables. Journal of the Royal Statistical Society B  61:331–
344  1999.

[14] M. Stephens. Dealing with label switching in mixture models. Journal of the Roya; statistical society B 

62:795–810  2000.

[15] H. Locarek-Junge and C. Weihs. Classiﬁcation as a Tool for Research. Springer  2009.
[16] C. Sugar and G. James. Finding the number of clusters in a data set: an information theoretic approach.

Journal of the American Statistical Association  98:750–763  2003.

[17] J. Wang. Consistent selection of the number of clusters via crossvalidation. Biometrika  97:893–904 

2010.

9

,Daniel Bartz
Klaus-Robert Müller
Yuanyuan Liu
Fanhua Shang
Wei Fan
James Cheng
Hong Cheng
Kai-Wei Chang
He He
Stephane Ross
Hal Daume III
John Langford