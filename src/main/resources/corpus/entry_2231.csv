2018,Gen-Oja: Simple & Efficient Algorithm for Streaming Generalized Eigenvector Computation,In this paper  we study the problems of principle Generalized Eigenvector computation and Canonical Correlation Analysis in the stochastic setting. We propose a simple and efficient algorithm for these problems. We prove the global convergence of our algorithm  borrowing ideas from the theory of fast-mixing Markov chains and two-Time-Scale Stochastic Approximation  showing that it achieves the optimal rate of convergence. In the process  we develop tools for understanding stochastic processes with Markovian noise which might be of independent interest.,Gen-Oja: A Simple and Efﬁcient Algorithm for
Streaming Generalized Eigenvector Computation

Kush Bhatia∗

University of California  Berkeley

kushbhatia@berkeley.edu

Nicolas Flammarion

University of California  Berkeley

flammarion@berkeley.edu

Aldo Pacchiano∗

University of California  Berkeley

pacchiano@berkeley.edu

Peter L. Bartlett

University of California  Berkeley

peter@berkeley.edu

Michael I. Jordan

University of California  Berkeley

jordan@cs.berkeley.edu

Abstract

In this paper  we study the problems of principal Generalized Eigenvector compu-
tation and Canonical Correlation Analysis in the stochastic setting. We propose
a simple and efﬁcient algorithm  Gen-Oja  for these problems. We prove the
global convergence of our algorithm  borrowing ideas from the theory of fast-
mixing Markov chains and two-time-scale stochastic approximation  showing that
it achieves the optimal rate of convergence. In the process  we develop tools
for understanding stochastic processes with Markovian noise which might be of
independent interest.

1

Introduction

Cannonical Correlation Analysis (CCA) and the Generalized Eigenvalue Problem are two fundamental
problems in machine learning and statistics  widely used for feature extraction in applications
including regression [18]  clustering [9] and classiﬁcation [19].
Originally introduced by Hotelling in [16]  CCA is a statistical tool for the analysis of multi-view
data that can be viewed as a “correlation-aware" version of Principal Component Analysis (PCA).
Given two multidimensional random variables  the objective in CCA is to obtain a pair of linear
transformations that maximize the correlation between the transformed variables.
Given access to samples {(xi  yi)n
i=1} of zero mean random variables X  Y ∈ Rd with an unknown
joint distribution PXY   CCA can be used to discover features expressing similarity or dissimilarity
between X and Y . Formally  CCA aims to ﬁnd a pair of vectors u  v ∈ Rd such that projections of X
onto v and Y onto u are maximally correlated. In the population setting  the corresponding objective
is given by:

max v(cid:62)E[XY (cid:62)]u

s.t.

v(cid:62)E[XX(cid:62)]v = 1 and u(cid:62)E[Y Y (cid:62)]u = 1.

(1)

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In the context of covariance matrices  the objective of the generalized eigenvalue problem is to obtain
the direction u or v ∈ Rd maximizing discrepancy between X and Y and can be formulated as 

arg max
v(cid:54)=0

v(cid:62)E[XX(cid:62)]v
v(cid:62)E[Y Y (cid:62)]v

and arg max
u(cid:54)=0

u(cid:62)E[Y Y (cid:62)]u
u(cid:62)E[XX(cid:62)]u

.

(2)

More generally  given symmetric matrices A  B  with B positive deﬁnite  the objective of the
principal generalized eigenvector problem is to obtain a unit norm vector w such that Aw = λBw
for λ maximal.
CCA and the generalized eigenvalue problem are intimately related. In fact  the CCA problem can be
cast as a special case of the generalized eigenvalue problem by solving for u and v in the following
objective:

(cid:18)
(cid:124)

0

E[Y X(cid:62)]

E[XY (cid:62)]

0

(cid:123)(cid:122)

A

(cid:18)v

(cid:19)

u

(cid:19)
(cid:125)

(cid:18)E[XX(cid:62)]
(cid:123)(cid:122)
(cid:124)

0

B

= λ

(cid:18)v

(cid:19)

u

(cid:19)
(cid:125)

0

E[Y Y (cid:62)]

.

(3)

The optimization problems underlying both CCA and the generalized eigenvector problem are non-
convex in general. While they admit closed-form solutions  even in the ofﬂine setting a direct
computation requires O(d3) ﬂops which is infeasible for large-scale datasets. Recently  there has
been work on solving these problems by leveraging fast linear system solvers [14  2] while requiring
complete knowledge of the matrices A and B.
In the stochastic setting  the difﬁculty increases because the objective is to maximize a ratio of
expectations  in contrast to the standard setting of stochastic optimization [26]  where the objective is
the maximization of an expectation. There has been recent interest in understanding and developing
efﬁcient algorithms with provable convergence guarantees for such non-convex problems. [17] and
[27] recently analyzed the convergence rate of Oja’s algorithm [25]  one of the most commonly used
algorithm for streaming PCA.
In contrast  for the stochastic generalized eigenvalue problem and CCA problem  the focus has
been to translate algorithms from the ofﬂine setting to the online one. For example  [12] proposes a
streaming algorithm for the stochastic CCA problem which utilizes a streaming SVRG method to
solve an online least-squares problem. Despite being streaming in nature  this algorithm requires a
non-trivial initialization and  in contrast to the spirit of streaming algorithms  updates its eigenvector
estimate only after every few samples. This raises the following challenging question:

Is it possible to obtain an efﬁcient and provably convergent counterpart to Oja’s Algorithm for
computing the principal generalized eigenvector in the stochastic setting?

In this paper  we propose a simple  globally convergent  two-line algorithm  Gen-Oja  for the
stochastic principal generalized eigenvector problem and  as a consequence  we obtain a natural
extension of Oja’s algorithm for the streaming CCA problem. Gen-Oja is an iterative algorithm
which works by updating two coupled sequences at every time step. In contrast with existing methods
[17]  at each time step the algorithm can be seen as performing a step of Oja’s method  with a noise
term which is neither zero mean nor conditionally independent  but instead is Markovian in nature.
The analysis of the algorithm borrows tools from the theory of fast mixing of Markov chains [11]
as well as two-time-scale stochastic approximation [6  7  8] to obtain an optimal (up to dimension
dependence) fast convergence rate of ˜O(1/n).
Notation: We denote by λi(M ) and σi(M ) the ith largest eigenvalue and singular value of a square
matrix M. For any positive semi-deﬁnite matrix N  we denote inner product in the N-norm by (cid:104)· ·(cid:105)N
and the corresponding norm by (cid:107) · (cid:107)N . We let κN = λmax(N )
λmin(N ) denote the condition number of N.
We denote the eigenvalues of the matrix B−1A by λ1 > λ2 ≥ . . . ≥ λd with (ui)d
denoting the corresponding right and left eigenvectors of B−1A whose existence is guaranteed by
Lemma G.3 in Appendix G.3. We use ∆λ to denote the eigengap λ1 − λ2.

i=1 and (˜ui)d

i=1

2 Problem Statement
In this section  we focus on the problem of estimating principal generalized eigenvectors in a
stochastic setting. The generalized eigenvector  vi  corresponding to a system of matrices (A  B) 

2

where A ∈ Rd×d is a symmetric matrix and B ∈ Rd×d is a symmetric positive deﬁnite matrix 
satisﬁes

(4)
The principal generalized eigenvector v1 corresponds to the vector with the largest value2 of λi  or 
equivalently  v1 is the principal eigenvector of the non-symmetric matrix B−1A. The vector v1 also
corresponds to the maximizer of the generalized Rayleigh quotient given by

Avi = λiBvi.

v1 = arg max
v∈Rd

v(cid:62)Av
v(cid:62)Bv

.

(5)

In the stochastic setting  we only have access to a sequence of matrices A1  . . .   An ∈ Rd×d and
B1  . . .   Bn ∈ Rd×d assumed to be drawn i.i.d. from an unknown underlying distribution  such that
E[Ai] = A and E[Bi] = B and the objective is to estimate v1 given access to O(d) memory.
In order to quantify the error between a vector and its estimate  we deﬁne the following generalization
of the sine with respect to the B-norm as 

B(v  w) = 1 −(cid:16) v(cid:62)Bw

(cid:107)v(cid:107)B(cid:107)w(cid:107)B

(cid:17)2

sin2

.

(6)

3 Related Work

PCA. There is a vast literature dedicated to the development of computationally efﬁcient algorithms
for the PCA problem in the ofﬂine setting (see [23  13] and references therein). In the stochastic
setting  sharp convergence results were obtained recently by [17] and [27] for the principal eigenvector
computation problem using Oja’s algorithm and later extended to the streaming k-PCA setting by [1].
√
They are able to obtain a O(1/n) convergence rate when the eigengap of the matrix is positive and a
O(1/

n) rate is attained in the gap free setting.

Ofﬂine CCA and generalized eigenvector. Computationally efﬁcient optimization algorithms
with ﬁnite convergence guarantees for CCA and the generalized eigenvector problem based on
Empirical Risk Minimization (ERM) on a ﬁxed dataset have recently been proposed in [14  31  2].
These approaches work by reducing the CCA and generalized eigenvector problem to that of solving
−1
a PCA problem on a modiﬁed matrix M (e.g.  for CCA  M = B
2 ). This reformulation is
then solved by using an approximate version of the Power Method that relies on a linear system
solver to obtain the approximate power method step. [14  2] propose an algorithm for the generalized
eigenvector computation problem and instantiate their results for the CCA problem. [20  21  31]
focus on the CCA problem by optimizing a different objective:

−1
2 AB

min

1
2

ˆE|φ(cid:62)xi − ψ(cid:62)yi|2 + λx(cid:107)φ(cid:107)2

2 + λy(cid:107)ψ(cid:107)2

2

s.t.

(cid:107)φ(cid:107)ˆE[xx(cid:62)] = (cid:107)ψ(cid:107)ˆE[yy(cid:62)] = 1 

where ˆE denotes the empirical expectation. The proposed methods utilize the knowledge of complete
data in order to solve the ERM problem  and hence is unclear how to extend them to the stochastic
setting.

Stochastic CCA and generalized eigenvector. There has been a dearth of work for solving these
problems in the stochastic setting owing to the difﬁculties mentioned in Section 1. Recently  [12]
extend the algorithm of [31] from the ofﬂine to the streaming setting by utilizing a streaming version
of the SVRG algorithm for the least squares system solver. Their algorithm  based on the shift and
invert method  suffers from two drawbacks: a) contrary to the spirit of streaming algorithms  this
method does not update its estimate at each iteration – it requires to use logarithmic samples for
solving an online least squares problem  and  b) their algorithm critically relies on obtaining an
estimate of λ1 to a small accuracy for which it requires to burn a few samples in the process. In
comparison  Gen-Oja takes a single stochastic gradient step for the inner least squares problem and
updates its estimate of the eigenvector after each sample. Perhaps the closest to our approach is [4] 
who propose an online method by solving a convex relaxation of the CCA objective with an inexact
stochastic mirror descent algorithm. Unfortunately  the computational complexity of their method is
O(d2) which renders it infeasible for large-scale problems.

3

Algorithm 1: Gen-Oja for Streaming Av = λBv
Input: Time steps T   step size αt (Least Squares)  βt (Oja)
Initialize: (w0  v0) ← sample uniformly from the unit sphere in Rd  ¯v0 = v0
for t = 1  . . .   T do

Draw sample (At  Bt)
wt ← wt−1 − αt(Btwt−1 − Atvt−1)
t ← vt−1 + βtwt
v(cid:48)
vt ← v(cid:48)
t(cid:107)vt(cid:107)2

Output: Estimate of Principal Generalized Eigenvector: vT

4 Gen-Oja

In this section  we describe our proposed approach for the stochastic generalized eigenvector problem
(see Section 2). Our algorithm Gen-Oja  described in Algorithm 1  is a natural extension of the
popular Oja’s algorithm used for solving the streaming PCA problem. The algorithm proceeds by
iteratively updating two coupled sequences (wt  vt) at the same time: wt is updated using one step of
stochastic gradient descent with constant step-size to minimize w(cid:62)Bw − 2w(cid:62)Avt and vt is updated
using a step of Oja’s algorithm. Gen-Oja has its roots in the theory of two-time-scale stochastic
approximation  by viewing the sequence wt as a fast mixing Markov chain and vt as a slowly evolving
one. In the sequel  we describe the evolution of the Markov chains (wt)t≥0  (vt)t≥0  in the process
outlining the intuition underlying Gen-Oja and understanding the key challenges which arise in the
convergence analysis.
Oja’s algorithm. Gen-Oja is closely related to the Oja’s algorithm [25] for the streaming PCA
problem. Consider a special case of the problem  when each Bt = I. In the ofﬂine setting  this
reduces the generalized eigenvector problem to that of computing the principal eigenvector of A.
With the setting of step-size αt = 1  Gen-Oja recovers the Oja’s algorithm given by

vt =

vt−1 + βtAtvt−1
(cid:107)vt−1 + βtAtvt−1.(cid:107)

This algorithm is exactly a projected stochastic gradient ascent on the Rayleigh quotient v(cid:62)Av (with
a step size βt). Alternatively  it can be interpreted as a randomized power method on the matrix
(I + βtA)[15].
Two-time-scale approximation. The theory of two-time-scale approximation forms the underlying
basis for Gen-Oja. It considers coupled iterative systems where one component changes much faster
than the other [7  8]. More precisely  its objective is to understand classical systems of the type:

(7)
xt = xt−1 + αt
(8)
yt = yt−1 + βt
where g and h are the update functions and (ξ1
t ) correspond to the noise vectors at step t and
typically assumed to be martingale difference sequences.
In the above model  whenever the two step sizes αt and βt satisfy βt/αt → 0  the sequence yt moves
on a slower timescale than xt. For any ﬁxed value of y the dynamical system given by xt 

t   ξ2

t

xt = xt−1 + αt[h (xt−1  y) + ξ1
t ] 

(9)
converges to to a solution x∗(y). In the coupled system  since the state variables xt move at a much
faster time scale  they can be seen as being close to x∗(yt)  and thus  we can alternatively consider:
(10)
If the process given by yt above were to converge to y∗  under certain conditions  we can argue that
the coupled process (xt  yt) converges to (x∗(y∗)  y∗). Intuitively  because xt and yt are evolving at
different time-scales  xt views the process yt as quasi-constant while yt views xt as a process rapidly
converging to x∗(yt).

(cid:2)g (x∗(yt−1)  yt−1) + ξ2

yt = yt−1 + βt

(cid:3) .

t

2Note that we consider here the largest signed value of λi

4

(cid:2)h (xt−1  yt−1) + ξ1
(cid:3)
(cid:2)g (xt−1  yt−1) + ξ2
(cid:3)  

t

Gen-Oja can be seen as a particular instance of the coupled iterative system given by Equations
(7) and (8) where the sequence vt evolves with a step-size βt ≈ 1
t   much slower than the sequence
wt  which has a step-size of αt ≈ 1
log(t). Proceeding as above  the sequence vt views wt as having
converged to B−1Avt + ξt  where ξt is a noise term  and the update step for vt in Gen-Oja can be
viewed as a step of Oja’s algorithm  albeit with Markovian noise.
While previous works on the stochastic CCA problem required to use logarithmic independent
samples to solve the inner least-squares problem in order to perform an approximate power method
(or Oja) step  the theory of two-time-scale stochastic approximation suggests that it is possible to
obtain a similar effect by evolving the sequences wt and vt at two different time scales.
Understanding the Markov Process {wt}.
sequence wt  we consider the homogeneous Markov chain (wv

In order to understand the process described by the

t ) deﬁned by

wv

t = wv

(11)
for a constant vector v and we denote its t-step kernel by πt
v [22]. This Markov process is an iterative
linear model and has been extensively studied by [28  10  5]. It is known that for any step-size
α ≤ 2/R2  the Markov chain (wv
t )t≥0 admits a unique stationary distribution  denoted by νv. In
addition 

t−1 − α(Btwv

W 2

2 (πt

v(w0 ·)  νv) ≤ (1 − 2µα(1 − αR2

B/2))t

(cid:107)w0 − w(cid:107)2

2dνv(w) 

(12)

t−1 − Atv) 
(cid:90)

Rd

where W 2
2 (λ  ν) denotes the Wasserstein distance of order 2 between probability measures λ and ν
(see  e.g.  [30] for more properties of W2). Equation (12) implies that the iterative linear process
described by (11) mixes exponentially fast to the stationary distribution. This forms a crucial
ingredient in our convergence analysis where we use the fast mixing to obtain a bound on the
expected norm of the Markovian noise (see Lemma 6.1).
Moreover  one can compute the mean ¯wv of the process wt under the stationary distribution by taking
expectation under νv on both sides in equation (11). Doing so  we obtain  ¯wv = B−1Av. Thus  in
our setting  since the vt process evolves slowly  we can expect that wt ≈ B−1Avt  allowing Gen-Oja
to mimic Oja’s algorithm.

5 Main Theorem

symmetric matrix B ∈ Rd×d with B (cid:60) µI for µ > 0.

In this section  we present our main convergence guarantee for Gen-Oja when applied to the streaming
generalized eigenvector problem. We begin by listing the key assumptions required by our analysis:
(A1) The matrices (Ai)i≥0 satisfy E[Ai] = A for a symmetric matrix A ∈ Rd×d.
(A2) The matrices (Bi)i≥0 are such that each Bi (cid:60) 0 is symmetric and satisﬁes E[Bi] = B for a
(A3) There exists R ≥ 0 such that max{(cid:107)Ai(cid:107) (cid:107)Bi(cid:107)} ≤ R almost surely.
Under the assumptions stated above  we obtain the following convergence theorem for Gen-Oja with
respect to the sin2
Theorem 5.1 (Main Result). Fix any δ > 0 and 1 > 0. Suppose that the step sizes are set to
αt =

B distance  as described in Section 2.

log(d2β+t) and βt =

γ

c



∆λ(d2β+t) for γ > 1/2   c > 1 and
µ2 + R5

µ + R3

200

20γ2λ2
1

(cid:16) R

(cid:17)

(cid:16)

µ3
δ∆2
λ

β = max

∆2

λd2 log

log

1 + R2

µ + R4

µ2

Suppose that the number of samples n satisfy

d2β + n

1

Then  the output vn of Algorithm 1 satisﬁes 

log

min(1 2γλ1/∆λ ) (d2β + n)

B(u1  vn) ≤ (2 + 1)cd(cid:107)(cid:80)d

i=1 ˜ui ˜u(cid:62)
δ2(cid:107)˜u1(cid:107)2

2

sin2

1+1

(cid:17)  

(cid:16) 1+δ/100
(cid:18)
i (cid:107)2 log(cid:0) 1

≥

cd

δ

δ1 min(1  λ1)

(cid:19)

(cid:32)

(cid:1)

5

(cid:17)

 .
(cid:18) cλ2
(cid:18) d2β + log3(d2β)

(cid:19)

1
d2

d2β + n + 1

(cid:19)2γ(cid:33)

 

1

min(1 2γλ1/∆λ )

(d3β + 1) exp

cγ2 log3(d2β + n)
∆2
λ(d2β + n + 1)

+

cd
∆λ

2(2+1) .

with probability at least 1 − δ with c depending polynomially on parameters of the problem
λ1  κB  R  µ. The parameter δ1 is set as δ1 = 1
The above result shows that with probability at least 1 − δ  Gen-Oja converges in the B-norm to
the right eigenvector  u1  corresponding to the maximum eigenvalue of the matrix B−1A. Further 
Gen-Oja exhibits an ˜O(1/n) rate of convergence  which is known to be optimal for stochastic
approximation algorithms even with convex objectives [24].
Comparison with Streaming PCA. In the setting where B = I  and A (cid:23) 0 is a covariance matrix 
the principal generalized eigenvector problem reduces to performing PCA on the A. When compared
with the results obtained for streaming PCA by [17]  our corresponding results differ by a factor of
dimension d and problem dependent parameters λ1  ∆λ. We believe that such a dependence is not
inherent to Gen-Oja but a consequence of our analysis. We leave this task of obtaining a dimension
free bound for Gen-Oja as future work.
Gap-independent step size: While the step size for the sequence vn in Gen-Oja depends on eigen-
gap  which is a priori unknown  one can leverage recent results as in [29] to get around this issue by
using a streaming average step size.

6 Proof Sketch

In this section  we detail out the two key ideas underlying the analysis of Gen-Oja to obtain the
convergence rate mentioned in Theorem 5.1: a) controlling the non i.i.d. Markovian noise term which
is introduced because of the coupled Markov chains in Gen-Oja and b) proving that a noisy power
method with such Markovian noise converges to the correct solution.

In order to better understand the sequence vt  we rewrite

v(cid:48)
t = vt−1 + βtwt = vt−1 + βt(B−1Avt−1 + ξt) 

Controlling Markovian perturbations.
the update as 
(13)
where ξt = wt − B−1Avt−1 is the prediction error which is a Markovian noise. Note that the
noise term is neither mean zero nor a martingale difference sequence. Instead  the noise term ξt is
dependent on all previous iterates  which makes the analysis of the process more involved. This
framework with Markovian noise has been extensively studied by [6  3].
From the update in Equation (13)  we observe that Gen-Oja is performing an Oja update but with
a controlled Markovian noise. However  we would like to highlight that classical techniques in the
study of stochastic approximation with Markovian noise (as the Poisson Equation [6  22]) were not
enough to provide adequate control on the noise to show convergence.
In order to overcome this difﬁculty  we leverage the fast mixing of the chain wv
t for understanding the
Markovian noise. While it holds that E[(cid:107)ξt(cid:107)2] = O(1) (see Appendix C)  a key part of our analysis
is the following lemma  the proof of which can be found in Appendix B).
)  and assuming that (cid:107)ws(cid:107) ≤ Ws for t ≤ s ≤
Lemma 6.1. . For any choice of k > 4 λ1(B)
t + k we have that

µα log( 1
βt+k

(cid:107)E[t+k|Ft](cid:107)2 = O(βtk2αtWt+k)

Lemma 6.1 uses the fast mixing of wt to show that (cid:107)E[ξt]|Ft−r(cid:107)2 = ˜O(βt) where r = O(log t)  i.e. 
the magnitude of the expected noise is small conditioned on log(t) steps in the past.

Analysis of Oja’s algorithm. The usual proofs of convergence for stochastic approximation deﬁne
a Lyapunov function and show that it decreases sufﬁciently at each iteration. Oftentimes control on
the per step rate of decrease can then be translated into a global convergence result. Unfortunately in
the context of PCA  due to the non-convexity of the Raleigh quotient  the quality of the estimate vt
cannot be related to the previous vt−1. Indeed vt may become orthogonal to the leading eigenvector.
Instead [17] circumvent this issue by leveraging the randomness of the initialization and adopt an
operator view of the problem. We take inspiration from this approach in our analysis of Gen-Oja. Let
Gi = wiv(cid:62)

i=1(I + βiGi)  Gen-Oja’s update can be equivalently written as

i−1 and Ht =(cid:81)t

vt =

Htv0
(cid:107)Htv0(cid:107)2

2

 

6

Tr(HH(cid:62)(cid:80)

j(cid:54)=i ˜uj ˜u(cid:62)
j )

˜u(cid:62)
i HH(cid:62) ˜ui

pushing  for the analysis only  the normalization step at the end. This point of view enables us to
analyze the improvement of Ht over Ht−1 since allows one to interpret Oja’s update as one step
of power method on Ht starting on a random vector v0. We present here an easy adaptation of [17 
Lemma 3.1] that takes into account the special geometry of the generalized eigenvector problem and
the asymmetry of B−1A. The proof can be found in Appendix A.
Lemma 6.2. Let H ∈ Rd×d  (ui)d
i=1 be the corresponding right and left eigenvectors of
B−1A and w ∈ Rd chosen uniformly on the sphere  then with probability 1− δ (over the randomness
in the initial iterate)

i=1 and (˜ui)d

sin2

B(ui  Hw) ≤ C log(1/δ)

δ

 

(14)

for some universal constant C > 0.

algorithm. Indeed we only have to prove that Ht will be close to(cid:81)t

This lemma has the virtue of highly simplifying the challenging proof of convergence of Oja’s
i=1(I + βiB−1A) for t large
enough which can be interpreted as an analogue of the law of large numbers for the multiplication of
i HtH(cid:62)
matrices. This will ensure that Tr(HtH(cid:62)
t ˜ui
and be enough with Lemma 6.2 to prove Theorem 5.1. The proof follows the line of [17] with
two additional tedious difﬁculties: the Markovian noise is neither unbiased nor independent of the
previous iterates  and the matrix B−1A is no longer symmetric  which is precisely why we consider
the left eigenvector ˜ui in the right-hand side of Eq. (14). We highlight two key steps:

j ) is relatively small compared to ˜u(cid:62)

(cid:80)
j(cid:54)=i ˜uj ˜u(cid:62)

t

t

• First we show that E Tr(HtH(cid:62)

plies by Markov’s inequality the same bound on Tr(HtH(cid:62)
probability. See Lemmas E.2 for more details.

j ) grows as O(exp(2λ2

i=1 βi))  which im-
j ) with constant
i HH(cid:62) ˜ui
i=1 βi)) which implies by Chebshev’s inequality the same bound

t ˜ui grows as O(exp(4λ1

• Second we show that Var ˜u(cid:62)

i HH(cid:62) ˜ui with constant probability. See Lemmas E.3 and E.5 for more details.

grows as O(exp(2λ1
for ˜u(cid:62)

i HtH(cid:62)

(cid:80)t

t

(cid:80)t
(cid:80)
(cid:80)t
j(cid:54)=i ˜uj ˜u(cid:62)
i=1 βi)) and E˜u(cid:62)

(cid:80)
j(cid:54)=i ˜uj ˜u(cid:62)

7 Application to Canonical Correlation Analysis
Consider two random vectors X ∈ Rd and Y ∈ Rd with joint distribution PXY . The objective of
canonical correlation analysis in the population setting is to ﬁnd the canonical correlation vectors
φ  ψ ∈ Rd d which maximize the correlation

(cid:112)E[(φ(cid:62)X)2]E[(ψ(cid:62)Y )2]

E[(φ(cid:62)X)(ψ(cid:62)Y )]

.

max
φ ψ

equivalent

problem is

to maximizing
E[XX(cid:62)]−1/2E[XY (cid:62)]E[Y Y (cid:62)]−1/2 

This
the
constraint
E[(φ(cid:62)X)2] = E[(ψ(cid:62)Y )2] = 1 and admits a closed form solution:
if we de-
ﬁne T
=
(E[XX(cid:62)]−1/2a1E[Y Y (cid:62)]−1/2b1) where a1  b1 are the left and right principal singular vec-
tors of T . By the KKT conditions  there exist ν1  ν2 ∈ R such that this solution satisﬁes the
stationarity equation

solution is

then the

(φ∗  ψ∗)

=

φ(cid:62)E[XY (cid:62)]ψ under

E[XY (cid:62)]ψ = ν1E[XX(cid:62)]φ

and E[Y X(cid:62)]φ = ν2E[Y Y (cid:62)]ψ.

Using the constraint conditions we conclude that ν1 = ν2. This condition can be written (for
λ = ν1) in the matrix form of Eq. (3). As a consequence  ﬁnding the largest generalized eigenvector
for the matrices (A  B) will recover the canonical correlation vector (φ  ψ). Solving the associated
generalized streaming eigenvector problem  we obtain the following result for estimating the canonical
correlation vector whose proof easily follows from Theorem 5.1 (setting γ = 6).
Theorem 7.1. Assume that max{(cid:107)X(cid:107) (cid:107)Y (cid:107)} ≤ R a.s.  min{λmin(E[XX(cid:62)])  λmin(E[Y Y (cid:62)])} =
µ > 0 and σ1(T ) − σ2(T ) = ∆ > 0. Fix any δ > 0  let 1 ≥ 0  and suppose the step sizes are set to
αt =

2R2 log(d2β+t) and βt =

∆(d2β+t) and

1

6



β = max

∆2d2 log

(cid:16) R

(cid:16) 1+δ/100

720σ2
1

(cid:17)  

1+1

7

200

µ + R3

µ2 + R5

µ3

δ log(1 + R2

µ + R4
µ2 )

(cid:17) 1

∆2



(cid:18)

≥

(cid:19)

cd

d2β + n

1

1

min(1 12λ1/∆λ )

(d3β + 1) exp

(cid:19)

(cid:18) cλ2

1
d2

Figure 1: Synthetic Generalized Eigenvalue problem. Left: Comparison with two-steps methods.
Middle: Robustness to step size αt. Right: Robustness to step size βt (Streaming averaged Gen-Oja
is dashed).

Suppose that the number of samples n satisfy

log

min(1 12λ1/∆λ ) (d2β + n)

Then the output (φt  ψt) of Algorithm 1 applied to (A  B) deﬁned above satisﬁes 
log3(d2β + n)

B((φ∗  ψ∗)  (φt  ψt)) ≤ (2 + 1)cd2 log(cid:0) 1

sin2

(cid:1)

δ

 

δ1 min(1  λ1)

δ2(cid:107)˜u1(cid:107)2

2

∆2(d2β + n + 1)

with probability at least 1 − δ with c depending on parameters of the problem and independent of d
and ∆ where δ1 = 1

2(2+1) .

We can make the following observations:

• The convergence guarantee are comparable with the sample complexity obtained by the
ERM (t = ˜O(d/(ε∆2) for sub-Gaussian variables and t = ˜O(1/(ε∆2µ2) for bounded
variables)[12].
• The sample complexity in [12] is better in term of the dependence on d. They obtain the
same rates as the ERM. We are unable to explicitly compare our bounds with [4] since they
work in the gap free setting and their computational complexity is O(d2).

8 Simulations
Here we illustrate the practical utility of Gen-Oja on a synthetic  streaming generalized eigenvector
problem. We take d = 20 and T = 106. The streams (At  Bt) ∈ (Rd×d)2 are normally-distributed
with covariance matrix A and B with random eigenvectors and eigenvalues decaying as 1/i  for
i = 1  . . .   d. Here R2 denotes the radius of the streams with R2 = max{Tr A  Tr B}. All results
are averaged over ten repetitions.
Comparison with two-steps methods.
In the left plot of Figure 1 we compare the behavior of
Gen-Oja to different two-steps algorithms. Since the method by [4] is of complexity O(d2)  we
compare Gen-Oja to a method which alternates between one step of Oja’s algorithm and τ steps
of averaged stochastic gradient descent with constant step size 1/2R2. Gen-Oja is converging at
rate O(1/t) whereas the other methods are very slow. For τ = 10  the solution of the inner loop
is too inaccurate and the steps of Oja are inefﬁcient. For τ = 10000  the output of the sgd steps is
very accurate but there are too few Oja iterations to make any progress. τ = 1000 seems an optimal
parameter choice but this method is slower than Gen-Oja by an order of magnitude.
Robustness to incorrect step-size α.
In the middle plot of Figure 1 we compare the behavior
of Gen-Oja for step size α ∈ {α∗  α∗/8  α∗/16} where α∗ = 1/R2. We observe that Gen-Oja
converges at a rate O(1/t) independently of the choice of α.
√
√
Robustness to incorrect step-size βt.
In the right plot of Figure 1 we compare the behavior of
i} where β∗ corresponds to the minimal
Gen-Oja for step size βt ∈ {β∗/t  β∗/16t  β∗/
i  β∗/16
error after one pass over the data. We observe that Gen-Oja is not robust to the choice of the constant

8

0246log10(t)-5-4-3-2-10log10[sinB(vt u1)]=1=10=1000=100000246log10(t)-5-4-3-2-10log10[sinB(vt u1)]=*=*/8=*/160246log10(t)-5-4-3-2-10log10[sinB(vt u1)]=*/t=*/16t=*/t1/2=*/16t1/2for step size βt ∝ 1/t. If the constant is too small  the rate of convergence is arbitrary slow. We
√
observe that considering the streaming average of [29] on Gen-Oja with a step size βt ∝ 1/
t
enables to recover the fast O(1/t) convergence while being robust to constant misspeciﬁcation.

9 Conclusion
We have proposed and analyzed a simple online algorithm to solve the streaming generalized
eigenvector problem and applied it to CCA. This algorithm  inspired by two-time-scale stochastic
√
approximation achieves a fast O(1/t) convergence. Considering recovering the k-principal general-
ized eigenvector (for k > 1) and obtaining a slow convergence rate O(1/
t) in the gap free setting
are promising future directions. Finally  it would be worth considering removing the dimension
dependence in our convergence guarantee.

Acknowledgements

We gratefully acknowledge the support of the NSF through grant IIS-1619362. AP acknowledges
Huawei’s support through a BAIR-Huawei PhD Fellowship. This work was supported in part by the
Mathematical Data Science program of the Ofﬁce of Naval Research under grant number N00014-18-
1-2764. This work was partially supported by AFOSR through grant FA9550-17-1-0308.

References
[1] Z. Allen-Zhu and Y. Li. First efﬁcient convergence for streaming k-PCA: a global  gap-free  and near-
optimal rate. In Foundations of Computer Science (FOCS)  2017 IEEE 58th Annual Symposium on  pages
487–492. IEEE  2017.

[2] Z. Allen-Zhu and Y. Li. Doubly accelerated methods for faster CCA and generalized eigendecomposition.

In International Conference on Machine Learning  2017.

[3] C. Andrieu  E. Moulines  and P. Priouret. Stability of stochastic approximation under veriﬁable conditions.

SIAM Journal on Control and Optimization  44(1):283–312  2005.

[4] R. Arora  T. V. Marinov  P. Mianjy  and N. Srebro. Stochastic approximation for canonical correlation

analysis. In Advances in Neural Information Processing Systems. 2017.

[5] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate

O(1/n). In Advances in Neural Information Processing Systems  2013.

[6] A. Benveniste  M. Métivier  and P. Priouret. Adaptive Algorithms and Stochastic Approximations. Springer

Publishing Company  Incorporated  1990.

[7] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters  29(5):291–294 

1997.

[8] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint  volume 48. Springer  2009.
[9] K. Chaudhuri  S. M. Kakade  K. Livescu  and K. Sridharan. Multi-view clustering via canonical correlation

analysis. In International Conference on Machine Learning  pages 129–136  2009.

[10] P. Diaconis and D. Freedman. Iterated random functions. SIAM Review  41(1):45–76  1999.
[11] A. Dieuleveut  A. Durmus  and F. Bach. Bridging the gap between constant step size stochastic gradient

descent and Markov chains. arXiv preprint arXiv:1707.06386  2017.

[12] C. Gao  D. Garber  N. Srebro  J. Wang  and W. Wang. Stochastic canonical correlation analysis. arXiv

preprint arXiv:1702.06533  2017.

[13] D. Garber  E. Hazan  J. Jin  S. Kakade  C. Musco  P. Netrapalli  and A. Sidford. Faster eigenvector
computation via shift-and-invert preconditioning. In International Conference on Machine Learning  2016.
[14] R. Ge  C. Jin  S. Kakade  P. Netrapalli  and A. Sidford. Efﬁcient algorithms for large-scale generalized
eigenvector computation and canonical correlation analysis. In International Conference on International
Conference on Machine  2016.

[15] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In Advances in

Neural Information Processing Systems  pages 2861–2869  2014.

[16] H. Hotelling. Relations between two sets of variates. Biometrika  28(3/4):321–377  1936.
[17] P. Jain  C. Jin  S. M. Kakade  P. Netrapalli  and A. Sidford. Streaming PCA: Matching matrix Bernstein
and near-optimal ﬁnite sample guarantees for Oja’s algorithm. In Conference on Learning Theory  2016.

9

[18] S. M. Kakade and D. P. Foster. Multi-view regression via canonical correlation analysis. In International

Conference on Computational Learning Theory  pages 82–96. Springer  2007.

[19] N. Karampatziakis and P. Mineiro. Discriminative features via generalized eigenvectors. arXiv preprint

arXiv:1310.1934  2013.

[20] Y. Lu and D. P. Foster. Large scale canonical correlation analysis with iterative least squares. In Advances

in Neural Information Processing Systems  2014.

[21] Z. Ma  Y. Lu  and D. Foster. Finding linear structure in large datasets with scalable canonical correlation

analysis. In International Conference on International Conference on Machine Learning  2015.

[22] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press  2009.
[23] C. Musco and C. Musco. Randomized block Krylov methods for stronger and faster approximate singular

value decomposition. In Advances in Neural Information Processing Systems  2015.

[24] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley-

Interscience Series in Discrete Mathematics. John Wiley & Sons  1983.

[25] E. Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of Mathematical Biology  15

(3):267–273  1982.

[26] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics  22

(3):400–407  1951.

[27] O. Shamir. Convergence of stochastic gradient descent for PCA. In International Conference on Machine

Learning  2016.

[28] D. Steinsaltz. Locally contractive iterated function systems. Annals of Probability  pages 1952–1979 

1999.

[29] N. Tripuraneni  N. Flammarion  F. Bach  and M. I. Jordan. Averaging stochastic gradient descent on

Riemannian manifolds. In Conference on Learning Theory  2018.

[30] C. Villani. Optimal Transport: Old and New  volume 338. Springer-Verlag Berlin Heidelberg  2008.
[31] W. Wang  J. Wang  D. Garber  and N. Srebro. Efﬁcient globally convergent stochastic optimization for

canonical correlation analysis. In Advances in Neural Information Processing Systems  2016.

10

,Kush Bhatia
Aldo Pacchiano
Nicolas Flammarion
Peter Bartlett
Michael Jordan