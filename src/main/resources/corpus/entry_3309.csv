2012,MCMC for continuous-time discrete-state systems,We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system  and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process  while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler  and show its advantages.,MCMC for continuous-time discrete-state systems

Vinayak Rao

Yee Whye Teh

Gatsby Computational Neuroscience Unit

Gatsby Computational Neuroscience Unit

University College London

vrao@gatsby.ucl.ac.uk

University College London

ywteh@gatsby.ucl.ac.uk

Abstract

We propose a simple and novel framework for MCMC inference in continuous-
time discrete-state systems with pure jump trajectories. We construct an exact
MCMC sampler for such systems by alternately sampling a random discretiza-
tion of time given a trajectory of the system  and then a new trajectory given the
discretization. The ﬁrst step can be performed efﬁciently using properties of the
Poisson process  while the second step can avail of discrete-time MCMC tech-
niques based on the forward-backward algorithm. We show the advantage of our
approach compared to particle MCMC and a uniformization-based sampler.

1

Introduction

There has been growing interest in the machine learning community to model dynamical systems in
continuous time. Examples include point processes [1]  Markov processes [2]  structured Markov
processes [3]  inﬁnite state Markov processes [4]  semi-Markov processes [5] etc. However  a major
impediment towards the more widespread use of these models is the problem of inference. A simple
approach is to discretize time  and then run inference on the resulting approximation. This however
has a number of drawbacks  not least of which is that we lose the advantages that motivated the use
of continuous time in the ﬁrst place. Time-discretization introduces a bias into our inferences  and
to control this  one has to work at a time resolution that results in a very large number of discrete
time steps. This can be computationally expensive.
Our focus in this paper is on posterior sampling via Markov chain Monte Carlo (MCMC)  and there
is a huge literature on such techniques for discrete-time models [6]. Here  we construct an exact
MCMC sampler for pure jump processes in continuous time  using a workhorse of the discrete-time
domain  the forward-ﬁltering backward-sampling algorithm [7  8]  to make efﬁcient updates.
The core of our approach is an auxiliary variable Gibbs sampler that repeats two steps. The ﬁrst
step runs the forward-backward algorithm on a random discretization of time to sample a new tra-
jectory. The second step then resamples a new time-discretization given this trajectory. A random
discretization allows a relatively coarse grid  while still keeping inferences unbiased. Such a coarse
discretization allows us to apply the forward-backward algorithm to a Markov chain with relatively
few time steps  resulting in computational savings. Even though the marginal distribution of the
random time-discretization can be quite complicated  we show that conditioned on the system tra-
jectory  it is just distributed as a Poisson process.
While the forward-backward algorithm was developed originally for ﬁnite state hidden Markov mod-
els and linear Gaussian systems  it also forms the core of samplers for more complicated systems
like nonlinear/non-Gaussian [9]  inﬁnite state [10]  and non-Markovian [11] time series. Our ideas
thus apply to essentially any pure jump process  so long as it makes only ﬁnite transitions over ﬁnite
intervals. For concreteness  we focus on semi-Markov processes. We compare our sampler with
two other continuous-time MCMC samplers  a particle MCMC sampler [12]  and a uniformization-
based sampler [13]. The latter turns out to be a special case of ours  corresponding to a random
time-discretization that is marginally distributed as a homogeneous Poisson process.

1

s(cid:48)
0 Ass(cid:48) (u)du) 

P (τs(cid:48) > τ ) = e(−(cid:82) τ

(cid:90) τs(cid:48)

0

2 Semi-Markov processes
A semi-Markov (jump) process (sMJP) is a right-continuous  piecewise-constant stochastic process
on the nonnegative real-line taking values in some state space S [14  15]. For simplicity  we assume
S is ﬁnite  labelling its elements from 1 to N. We also assume the process is stationary. Then  the
sMJP is parametrized by π0  an (arbitrary) initial distribution over states  as well as an N ×N matrix
of hazard functions  Ass(cid:48)(·) ∀s  s(cid:48) ∈ S. For any τ  Ass(cid:48)(τ ) gives the rate of transitioning to state s(cid:48) 
τ time units after entering state s (we allow self-transitions  so s(cid:48) can equal s). Let this transition
occur after a waiting time τs(cid:48). Then τs(cid:48) is distributed according to the density rss(cid:48)(·)  related to
Ass(cid:48)(·) as shown below (see eg. [16]):
rss(cid:48)(τs(cid:48)) = Ass(cid:48)(τs(cid:48))e(−(cid:82) τ
(1)
Sampling an sMJP trajectory proceeds as follows: on entering state s  sample waiting times τs(cid:48) ∼
Ass(cid:48)(·) ∀s(cid:48) ∈ S. The sMJP enters a new state  snew  corresponding to the smallest of these waiting
times. Let this waiting time be τhold (so that τhold = τsnew = mins(cid:48) τs(cid:48)). Then  advance the current
time by τhold  and set the sMJP state to snew. Repeat this procedure  now with the rate functions
Asnews(cid:48)(·) ∀s(cid:48) ∈ S.
(cid:89)
s(cid:48)∈S Ass(cid:48)(·). From the independence of the times τss(cid:48)  equation 1 tells us that

Ass(cid:48)(τs(cid:48)) = rss(cid:48)(τs(cid:48))/(cid:0)1 −

Deﬁne As(·) =(cid:80)

rss(cid:48)(u)du(cid:1)

τhold ∼ rs(τ ) ≡ As(τ )e(−(cid:82) τ

s(cid:48)∈S

0 As(u)du) 

0 As(u)du) (2)
P (τhold > τ ) =
Comparing with equation 1  we see that As(·) gives the rate of any transition out of state s. An
equivalent characterization of many continuous-time processes is to ﬁrst sample the waiting time
τhold  and then draw a new state s(cid:48). For the sMJP  the latter probability is proportional to Ass(cid:48)(τhold).
A special sMJP is the Markov jump process (MJP) where the hazard functions are constant (giving
exponential waiting times). For an MJP  future behaviour is independent of the current waiting time.
By allowing general waiting-time distributions  an sMJP can model memory effects like burstiness
or refractoriness in the system dynamics.
We represent an sMJP trajectory on an interval [tstart  tend] as (S  T )  where T = (t0 ···   t|T|) is
the sequence of jump times (including the endpoints) and S = (s0 ···   s|S|) is the corresponding
sequence of state values. Here |S| = |T|  and si+1 = si implies a self-transition at time ti+1 (except
at the end time t|T| = tend which does not correspond to a jump). The ﬁlled circles in ﬁgure 1(c)
represent (S  T ); since the process is right-continuous  si gives the state after the jump at ti.
2.1 Sampling by dependent thinning
We now describe an alternate thinning-based approach to sampling an sMJP trajectory. Our ap-
proach will produce candidate event times at a rate higher that the actual event rates in the system.
To correct for this  we probabilistically reject (or thin) these events. Deﬁne W as the sequence
of actual event times T   together with the thinned event times (which we call U  these are the
empty circles in ﬁgure 1(c)). W = (w0 ···   w|W|) forms a random discretization of time (with
|W| = |T| + |U|); deﬁne V = (v0 ···   v|W|) as a sequence of state assignments to the times W .
At any wi  let li represent the time since the last sMJP transition (so that  li = wi − maxt∈T t≤wi t) 

and let L =(cid:0)l1 ···   l|W|(cid:1). Figures 1(b) and (c) show these quantities  as well as continuous-time

processes S(t) and L(t) such that li = L(wi) and si = S(wi). (V  L  W ) forms an equivalent
representation of (S  T ) that includes a redundant set of thinned events U. Note that if the ith event
is thinned  vi = vi−1  however this is not a self-transition. L helps distinguish self-transitions (hav-
ing associated l’s equal to 0) from thinned events. We explain the generative process of (V  L  W )
below; a proof of its correctness is included in the supplementary material.
For each hazard function As(τ )  deﬁne another dominating hazard function Bs(τ )  so that Bs(τ ) ≥
As(τ ) ∀s  τ. Suppose we have instantiated the system trajectory until time wi  with the sMJP
having just entered state vi ∈ S (so that li = 0). We sample the next candidate event time wi+1 
with ∆wi = (wi+1− wi) drawn from the hazard function Bvi(·). A larger rate implies faster events 
so that ∆wi will on average be smaller than a waiting time τhold drawn from Avi(·). We correct
for this by treating wi+1 as an actual event with probability Avi (∆wi+li)
Bvi (∆wi+li). If this is the case  we
sample a new state vi+1 with probability proportional to Avivi+1 (∆wi + li)  and set li+1 = 0. On
the other hand  if the event is rejected  we set vi+1 to vi  and li+1 = (∆wi + li). We now sample

2

Figure 1: a) Instantaneous hazard rates given a trajectory b) State holding times  L(t) c) sMJP state values
S(t) d) Graphical model for the randomized time-discretization e) Resampling the sMJP trajectory. In b) and
c)  the ﬁlled and empty circles represent actual and thinned events respectively.
∆wi+1 (and thus wi+2)  such that (∆wi+1 + li+1) ∼ Bvi+1 (·). More simply  we sample a new
waiting time from Bvi+1 (·)  conditioned on it being greater than li+1. Again  accept this point with
probability Avi+1 (∆wi+1+li+1)
Bvi+1 (∆wi+1li+1)   and repeat this process. Proposition 1 conﬁrms that this generative
process (summarized by the graphical model in ﬁgure 1(d)  and algorithm 1) yields a trajectory from
the sMJP. Figure 1(d) also depicts observations X of the sMJP trajectory; we elaborate on this later.
Proposition 1. The path (V  L  W ) returned by the thinning procedure described above is equivalent
to a sample (S  T ) from the sMJP (π0  A).

Algorithm 1 State-dependent thinning for sMJPs
Input:

Dominating hazard functions Bs(τ ) ≥ As(τ ) ∀τ  s  where As(τ ) =(cid:80)

Hazard functions Ass(cid:48)(·) ∀s  s(cid:48) ∈ S  and an initial distribution over states π0.
s(cid:48) Ass(cid:48)(τ ).
A piecewise constant path (V  L  W ) ≡ ((vi  li  wi)) on the interval [tstart  tend].

Sample τhold ∼ Bvi(·)  with τhold > li. Let ∆wi = τhold − li  and wi+1 = wi + ∆wi.
with probability Avi (τhold)
Bvi (τhold)

Output:
1: Draw v0 ∼ π0 and set w0 = tstart. Set l0 = 0 and i = 0.
2: while wi < tend do
3:
4:
5:
6:
7:
8:
9:
10: end while
11: Set w|W| = tend  v|W| = v|W|−1  l|W| = l|W| + w|W| − w|W|−1.

Set li+1 = 0  and sample vi+1  with P (vi+1 = s(cid:48)|vi) ∝ Avis(cid:48)(τhold)  s(cid:48) ∈ S.
Set li+1 = li + ∆wi  and vi+1 = vi.

else

end
Increment i.

3

2.2 Posterior inference via MCMC

We now deﬁne an auxiliary variable Gibbs sampler  setting up a Markov chain that converges to
the posterior distribution over the thinned representation (V  L  W ) given observations X of the
sMJP trajectory. The observations can lie in any space X   and for any time-discretization W   let xi
represent all observations in the interval (wi  wi+1). By construction  the sMJP stays in a single state
vi over this interval; let P (xi|vi) be the corresponding likelihood vector. Given a time discretization
W ≡ (U ∪ T ) and the observations X  we discard the old state labels (V  L)  and sample a new path
( ˜V   ˜L  W ) ≡ ( ˜S  ˜T ) using the forward-backward algorithm. We then discard the thinned events ˜U 
and given the path ( ˜S  ˜T )  resample new thinned events Unew  resulting in a new time discretization
Wnew ≡ ( ˜T ∪ Unew). We describe both operations below.
Resampling the sMJP trajectory given the set of times W :
Given W (and thus all ∆wi)  this involves assigning each element wi ∈ W   a label (vi  li) (see
ﬁgure 1(d)). Note that the system is Markov in the pair (vi  li)  so that this step is a straightforward
application of the forward-backward algorithm to the graphical model shown in ﬁgure 1(d). Observe
from this ﬁgure that the joint distribution factorizes as:

P (V  L  W  X) = P (v0  l0)

P (xi|vi)P (∆wi|vi  li)P (vi+1  li+1|vi  li  ∆wi)

(3)

|W|−1(cid:89)

i=0

From equation 2  (with B instead of A)  P (∆wi|vi  li) = Bvi(li + ∆wi)e
The term P (vi+1  li+1|vi  li  ∆wi) is the thinning/state-transition probability from steps 4 and 5 of
algorithm 1. The forward-ﬁltering stage then moves sequentially through the times in W   succes-
sively calculating the probabilities P (vi  li  w1:i+1  x1:i) using the recursion:
P (vi  li  w1:i+1  x1:i) = P (xi|vi)P (wi+1|vi  li)

P (vi  li|vi−1  li−1  ∆wi)P (vi−1  li−1  w1:i  x1:i−1)

(cid:88)

Bvi (t)dt

(cid:16)−(cid:82) (li+∆wi)

li

(cid:17)

.

vi−1 li−1

The backward sampling stage then returns a new trajectory ( ˜V   ˜L  W ) ≡ ( ˜S  ˜T ). See ﬁgure 1(e).
Observe that li can take (i + 1) values (in the set {0  wi − wi−1 ···   wi − w0})  with the value of
li affecting P (vi+1  li+1|vi  li  ∆wi+1).Thus  the forward-backward algorithm for a general sMJP
scales quadratically with |W|. We can however use ideas from discrete-time MCMC to reduce this
cost (eg. [11] use a slice sampler to limit the maximum holding time of a state  and thus limit li).
Resampling the thinned events given the sMJP trajectory:
Having obtained a new sMJP trajectory (V  L  W )  we discard all thinned events U  so that the
current state of the sampler is now (S  T ). We then resample the thinned events ˜U  recovering a new
thinned representation ( ˜V   ˜L  ˜W )  and with it  a new discretization of time. To simplify notation 
we deﬁne the instantaneous hazard functions A(t) and B(t) (see ﬁgure 1(a)):

A(t) = AS(t)(L(t)) 

and B(t) = BS(t)(L(t))

(4)
These were the event rates relevant at any time t during the generative process. Note that the
sMJP trajectory completely determines these quantities. The events W (whether thinned or not)
were generated from a rate B(·) process  while the probability that an event wi was thinned is
1 − A(wi)/B(wi). The Poisson thinning theorem [17] then suggests that the thinned events U are
distributed as a Poisson process with intensity (B(t) − A(t)). The following proposition (see the
supplementary material for a proof) shows that this is indeed the case.
Proposition 2. Conditioned on a trajectory (S  T ) of the sMJP  the thinned events U are distributed
as a Poisson process with intensity (B(t) − A(t)).
Observe that this is independent of the observations X. We show in section 2.4 how sampling from
such a Poisson process is straightforward for appropriately chosen bounding rates Bs.

2.3 Related work
An increasingly popular approach to inference in continuous-time systems is particle MCMC (pM-
CMC) [12]. At a high level  this uses particle ﬁltering to generate a continuous-time trajectory 
which then serves as a proposal for a Metropolis-Hastings (MH) algorithm. Particle ﬁltering how-
ever cannot propogate back information from future observations  and pMCMC methods can have
difﬁculty in situations where strong observations cause the posterior to deviate from the prior.

4

Recently  [13] proposed a sampler for MJPs that is a special case of ours. This was derived via a
classical idea called uniformization  and constructed the time discretization W from a homogeneous
Poisson process. Our sampler reduces to this when a constant dominating rate B > maxs τ As(τ )
is used to bound all event rates. However  such a ‘uniformizing’ rate does not always exist (we
will discuss two such systems with unbounded rates). Moreover  with a single rate B  the average
number of candidate events |W|  (and thus the computational cost of the algorithm)  scales with the
leaving rate of the most unstable state. Since this state is often the one that the system will spend the
least amount of time in  such a strategy can be wasteful. Under our sampler  the distribution of W is
not a Poisson process. Instead  events rates are coupled via the sMJP state. This allows our sampler
to adapt the granularity of time-discretization to that required by the posterior trajectories  moreover
this granularity can vary over the time interval.
There exists other work on continuous-time models based on the idea of a random discretization
of time [18  1]. Like uniformization  these all are limited to speciﬁc continuous-time models with
speciﬁc thinning constructions  and are not formulated in as general a manner as we have done.
Moreover  none of these exploit the ability to efﬁciently resample the time-discretization from a
Poisson process  or a new trajectory using the forward-backward algorithm.

2.4 Experiments
In this section  we evaluate our sampler on a 3-state sMJP with Weibull hazard rates. Here
rss(cid:48)(τ|αss(cid:48)  λss(cid:48)) = e(−(τ /λss(cid:48) )α

  Ass(cid:48)(τ|αss(cid:48)  λss(cid:48)) =

(cid:19)αss(cid:48)−1

(cid:18) τ

(cid:18) τ

(cid:19)αss(cid:48)−1

ss(cid:48) ) αss(cid:48)
λss(cid:48)

λss(cid:48)

αss(cid:48)
λss(cid:48)

λss(cid:48)

where λss(cid:48) is the scale parameter  and the shape parameter αss(cid:48) controls the stability of a state s.
When αss(cid:48) < 1  on entering state s  the system is likely to quickly jump to state s(cid:48). By contrast 
αss(cid:48) > 1 gives a ‘recovery’ period before transitions to s(cid:48). Note that for αss(cid:48) < 1  the hazard
function tends to inﬁnity as τ → 0. Now  choose an Ω > 1. We use the following simple upper
bound Bss(cid:48)(τ ):

(cid:18) τ

(cid:19)αss(cid:48)−1

(cid:19)αss(cid:48)−1

(cid:18) τ

˜λss(cid:48)

Ω

=

λss(cid:48)

αss(cid:48)
˜λss(cid:48)

Ωαss(cid:48)
λss(cid:48)

Bss(cid:48)(τ ) = ΩAss(cid:48)(τ|αss(cid:48)  λss(cid:48)) =

(5)
√
Ω for any λ and α. Thus  sampling from the dominating hazard function Bss(cid:48)(·)
Here  ˜λ = λ/ α
reduces to straightforward sampling from a Weibull with a smaller scale parameter ˜λss(cid:48). Note from
algorithm 1 that with this construction of the dominating rates  each candidate event is rejected with

(cid:1); this can be a guide to choosing Ω. In our experiments  we set Ω equal to 2.

probability(cid:0)1 − 1
from a Poisson process with intensity (B(t) − A(t)) = (Ω − 1)A(t) = (Ω − 1)(cid:80)
Sampling thinned events on an interval (ti  ti+1) (where the sMJP is in state si) involves sampling
s(cid:48) Asis(cid:48)(t − ti).
This is just the superposition of N independent and shifted Poisson processes on (0  ti+1 − ti) 
the nth having intensity (Ω − 1)Asin(·) ≡ ˆAsin(·). As before  ˆA(·) is a Weibull hazard function
√
obtained by correcting the scale parameter λ of A(·) by α
Ω − 1. A simple way to sample such
(cid:82) (ti+1−ti)
a Poisson process is by ﬁrst drawing the number of events from a Poisson distribution with mean
ˆAsin(u)du  and then drawing that many events i.i.d. from ˆAsin truncated at (ti+1 − ti).
0
Solving the integral for the Poisson mean is straightforward for the Weibull. Call the resulting
Poisson sequence ˜Tn  and deﬁne ˜T = ∪n∈S ˜Tn. Then Wi ≡ ˜T + ti is the set of resampled thinned
events on the interval (ti  ti+1). We repeat this over each segment (ti  ti+1) of the sMJP path.
In the following experiments  the shape parameters for each Weibull hazard (αss(cid:48)) was randomly
drawn from the interval [0.6  3]  while the scale parameter was always set to 1. π0 was set to
the discrete uniform distribution. The unbounded hazards associated with αss(cid:48) < 1 meant that
uniformization is not applicable to this problem  and we only compared our sampler with pMCMC.
We implemented both samplers in Matlab. Our MCMC sampler was set up with Ω = 2  so that the
dominating hazard rate at any instant equalled twice the true hazard rate (i.e. Bss(cid:48)(τ ) = 2Ass(cid:48)(τ )) 
giving a probability of thinning equal to 0.5. For pMCMC  we implemented the particle independent
Metropolis-Hastings sampler from [12]. We tried different values for the number of particles; for
our problems  we found 10 gave best results.
All MCMC runs consisted of 5000 iterations following a burn-in period of 1000. After any MCMC
run  given a sequence of piecewise constant trajectories  we calculated the empirical distribution of

5

time vs

Figure 2: ESS per
unit
the
inverse-temperature
of
the likelihood 
when
trajec-
tories are over an
length
interval of
and
20
2
(right).

the

(left)

Figure 3: ESS per second for increasing interval lengths. Temperature decreases from the left to right subplots.

the time spent in each state as well as the number of state transitions. We then used R-coda [19] to
estimate effective sample sizes (ESS) for these quantities. The ESS of the simulation was set to the
median ESS of all these statistics.
Effect of the observations For our ﬁrst experiment  we distributed 10 observations over an interval
of length 20. Each observation favoured a particular  random state over the other two states by a
factor of 100  giving random likelihood vectors like (1  100  1)(cid:62). We then raised the likelihood
vector P (xi|·) to an ‘inverse-temperature’ ν  so that the effective likelihood at the ith observation
was (P (xi|si))ν. As this parameter varied from 0 to 1  the problem moved from sampling from the
prior to a situation where the trajectory was observed (almost) perfectly at 10 random times.
The left plot in ﬁgure 2 shows the ESS produced per unit time by both samplers as the inverse-
temperature increased  averaging results from 10 random parametrizations of the sMJP. We see (as
one might expect)  that when the effect of the observations is weak  particle MCMC (which uses
the prior distribution to make local proposals)  outperforms our thinning-based sampler. pMCMC
also has the beneﬁt of being simpler implementation-wise  and is about 2-3 times faster (in terms
of raw computation time) for a Weibull sMJP  than our sampler. As the effect of the likelihood
increases  pMCMC starts to have more and more difﬁculty tracking the observations. By contrast 
our sampler is fairly insensitive to the effect of the likelihood  eventually outperforming the particle
MCMC sampler. While there exist techniques to generate more data-driven proposals for the particle
MCMC [12  20]  these compromise the appealing simplicity of the original particle MCMC sampler.
Moreover  none of these really have the ability to propagate information back from the future (like
the forward-backward algorithm)  rather they make more and more local moves (for instance  by
updating the sMJP trajectory on smaller and smaller subsets of the observation interval).
The right plot in ﬁgure 2 shows the ESS per unit time for both samplers  now with the observation
interval set to a smaller length of 2. Here  our sampler comprehensively outperforms pMCMC. There
are two reasons for this. First  more observations per unit time requires rapid switching between
states  a deviation from the prior that particle ﬁltering is unlikely to propose. Additionally  over
short intervals  the quadratic cost of the forward-backward step of our algorithm is less pronounced.
Effect of the observation interval length In the next experiment  we more carefully compare the
two samplers as the interval length varies. For three setting of the inverse temperature parameter
(0.1  0.5 and 0.9)  we calculated the number of effective samples produced per unit time as the
length of the observation interval increased from 2 to 50. Once again  we averaged results from 10
random settings of the sMJP parameters. Figure 3 show the results for the low  medium and high
settings of the the inverse temperature. Again  we clearly see the beneﬁt of the forward-backward
algorithm  especially in the low temperature and short interval regimes where the posterior deviates
from the prior. Of course  the performance of our sampler can be improved further using ideas from
the discrete-time domain; these can help ameliorate effect of the quadratic cost for long intervals.

6

0.10.20.30.40.50.60.70.80.9  1024681012Effective samples per second  Thinningparticle MCMC10particle MCMC200.10.20.30.40.50.60.70.80.9  10204060Effective samples per second  Thinningparticle MCMC10particle MCMC20 2 5102050010203040Effective samples per second  Thinningparticle MCMC10 2 51020500102030Effective samples per second  Thinningparticle MCMC10 2 5102050051015202530Effective samples per second  Thinningparticle MCMC10Figure 4: Effect of increasing the leaving rate of a state. Temperature decreases from the left to right plots.
3 Markov jump processes
In this section  we look at the Markov jump process (MJP)  which we saw has constant hazard
functions Ass(cid:48). MJPs are also deﬁned to disallow self-transitions  so that Ass = 0 ∀s ∈ S. If we
use constant dominating hazard rates Bs  we see from algorithm 1 that all probabilities at time wi
depend only on the current state si  and are independent of the holding time li. Thus  we no longer
need to represent the holding times L. The forward message at time wi needs only to represent the
probability of vi taking different values in S; this completely speciﬁes the state of the MJP. As a
result  the cost of a forward-backward iteration is now linear in |W|.
In the next experiment  we compare Matlab implementations of our thinning-based sampler and the
particle MCMC sampler with the uniformization-based sampler described in section 2.3. Recall
that the latter samples candidate event times W from a homogeneous Poisson process with a state-
independent rate B > maxs As. Following [13]  we set B = 2 maxs As. As in section 2.4  we set
Ω = 2 for our sampler  so that Bs = 2As ∀s. pMCMC was run with 20 particles.
Observe that for uniformization  the rate B is determined by the leaving rate of the most unstable
state; often this is the state the system spends the least time in. To study this  we applied all three
samplers to a 3-state MJP  two of whose states had leaving rates equal to 1. The leaving rate of
the third state  was varied from 1 to 20 (call this rate γ). On leaving any state  the probability of
transitioning to either of the other two was uniformly distributed between 0 and 1. This way  we
constructed 10 random MJPs for each γ. We distributed 5 observation times (again  favouring a
random state by a factor of 100) over the interval [0  10]. Like section 2.4  we looked at the ESS per
unit time for 3 settings of the inverse temperature parameter ν  now as we varied γ.
Figure 4 shows the results. The pMCMC sampler clearly performs worse than the other two. The
Markov structure of the MJP makes the forward-backward algorithm very natural and efﬁcient  by
contrast  running a particle ﬁlter with 20 particles took about twice as long as our sampler. Further 
we see that while both the uniformization and our sampler perform comparably for low values of γ 
our sampler starts to outperform uniformization for γ’s greater than 2. In fact  for weak observations
and large γs  even particle MCMC outperforms uniformization. As we mentioned earlier  this is
because for uniformization  the granularity of time-discretization is determined by the least stable
state  resulting in very long Markov chains for large values of γ.
3.1 The M/M/∞ queue
We ﬁnally apply our ideas to an inﬁnite state MJP from queuing theory  the M/M/∞ queue (also
called an immigration-death process [21]). Here  individuals (customers  messages  jobs etc.) enter
a population according to a homogeneous Poisson process with rate α independent of the population
size. The lifespan of each individual (or the job ‘service time’) is exponentially distributed with rate
β  so that the rate at which a ‘death’ occurs in the population is proportional to the population size.
Let S(t) represent the population size (or the number of ‘busy servers’) at time t. Then  under the
M/M/∞ queue  the stochastic process S(t) evolves according to a simple birth-death Markov jump
process on the space S = {1 ···  ∞}  with rates As s+1 = α and As s−1 = sβ. All other rates
are 0. Observe that since the population size of the M/M/∞ queue is unbounded  we cannot upper
bound the event rates in the system. Thus  uniformization is not directly applicable to this system.
Instead  we have to truncate the maximum value of S(t) to some constant  say c. This is the so-called
M/M/c/c queue; now  when all c servers are busy  any incoming jobs are rejected.
In the following  we considered an M/M/∞ queue with α and β set to 10 and 1 respectively. For
some tend  the state of the system was observed perfectly at three times 0  tend/10 and tend  with
values 10  2 and 15 respectively. Conditioned on these  we sought the posterior distribution over the

7

 1 2 51020020406080100Effective samples per second  UniformizationThinningparticle MCMC 1 2 51020020406080100Effective samples per second  UniformizationThinningparticle MCMC 1 2 51020020406080Effective samples per second  UniformizationThinningparticle MCMCFigure 5: The M/M/∞
queue: a) ESS per unit time
b) ESS per unit time scaled
by interval length.

system trajectory on the interval [0  tend]. Since the state of the system at time 0 is perfectly observed
to be 10  given any time-discretization  the maximum value of si at step i of the Markov chain is
(10 + i). Thus  message dimensions are always ﬁnite  and we can directly apply the forward-
backward algorithm. For noisy observations  we can use a slice sampler [22]. We compared our
sampler with uniformization; for this  we approximated the M/M/∞ system with an M/M/50/50
system. We also applied our sampler to this truncated approximation  labelling it as ‘Thinning
(trunc)’. For both these samplers  the message dimensions were 50. The large state spaces involved
makes pMCMC very inefﬁcient  and we did not include it in our results.
Figure 5(a) shows the ESS per unit time for all three samplers as we varied the interval length tend
from 1 to 20. Sampling a trajectory over a long interval will take more time than over a short one 
and to more clearly distinguish performance for large values of tend  we scale each ESS from the
left plot with tend  the length of the interval  in the right subplot of ﬁgure 5.
We see our sampler always outperforms uniformization  with the difference particularly signiﬁcant
for short intervals. Interestingly  running our thinning-based sampler on the truncated system offers
no signiﬁcant computational beneﬁt over running it on the full model. As the observation interval be-
comes longer and longer  the MJP trajectory can make larger and larger excursions (especially over
the interval [tend/10  tend]). Thus as tend increases  event rates witnessed in posterior trajectories
starts to increase. As our sampler adapts to this  the number of thinned events in all three samplers
start to become comparable  causing the uniformization-based sampler to approach the performance
of the other two samplers. At the same time  we see that the difference between our truncated and
our untruncated sampler starts to widen. Of course  we should remember that over long intervals 
truncating the system size to 50 becomes more likely to introduce biases into our inferences.
4 Discussion
We described a general framework for MCMC inference in continuous-time discrete-state systems.
Each MCMC iteration ﬁrst samples a random discretization of time given the trajectory of the sys-
tem. Given this  we then resample the sMJP trajectory using the forward-backward algorithm. While
we looked only at semi-Markov and Markov jump processes  it is easy to extend our approach to
piecewise-constant stochastic processes with more complicated dependency structures.
For our sampler  a bottleneck in the rate of mixing is that the new and old trajectories share an inter-
mediate discretization W (see ﬁgure 1(e)). Recall that an sMJP trajectory deﬁnes an instantaneous
hazard function B(t); our scheme requires the discretization sampled from the old hazard function
be compatible with the new hazard function. Thus  the forward-backward algorithm is unlikely to
return a trajectory associated with a hazard function that differ signiﬁcantly from the old one. By
contrast  for uniformization  the hazard function is a constant B  independent of the system state.
However  this comes at the cost of a conservatively high discretization of time. An interesting di-
rection for future work is too see how different choices of the dominating hazard function can help
trade-off these factors. For instance  we proposed  using a single Ω  with Bs(·) = ΩAs(·). It is
possible to use a different Ωs for each state s  or even an Ωs(·) that varies with time. Similarly  one
can consider additive (rather than multiplicative) constructions of Bs(·).
For general sMJPs  the forward-backward algorithm scales quadratically with |W|  the number of
candidate jump times. Such scaling is characteristic of sMJPs  though we can avail of discrete-time
MCMC techniques to ameliorate this. For sMJPs whose hazard functions are constant beyond a
‘window of memory’  inference scales quadratically with the memory length  and only linearly with
|W|. One can use such approximations to devise efﬁcient MH proposals for sMJPs trajectories.

8

 1 2 510200102030405060Effective samples per second  UniformizationDependent thinningThinning (trunc) 1 2 5102010203040506070Effective samples per second per unit interval length  UniformizationDependent thinningThinning (trunc)References
[1] Ryan P. Adams  Iain Murray  and David J. C. MacKay. Tractable nonparametric Bayesian inference in
Poisson processes with Gaussian process intensities. In Proceedings of the 26th International Conference
on Machine Learning (ICML)  2009.

[2] Y. W. Teh  C. Blundell  and L. T. Elliott. Modelling genetic variations with fragmentation-coagulation

processes. In Advances In Neural Information Processing Systems  2011.

[3] U. Nodelman  C.R. Shelton  and D. Koller. Continuous time Bayesian networks. In Proceedings of the

Eighteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 378–387  2002.

[4] Ardavan Saeedi and Alexandre Bouchard-Cˆot´e. Priors over Recurrent Continuous Time Processes. In

Advances in Neural Information Processing Systems 24 (NIPS)  volume 24  2011.

[5] Matthias Hoffman  Hendrik Kueck  Nando de Freitas  and Arnaud Doucet. New inference strategies
In Proceedings of the Twenty-
for solving Markov decision processes using reversible jump MCMC.
Fifth Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-09)  pages 223–231 
Corvallis  Oregon  2009. AUAI Press.

[6] A. Doucet  N. de Freitas  and N. J. Gordon. Sequential Monte Carlo Methods in Practice. Statistics for

Engineering and Information Science. New York: Springer-Verlag  May 2001.

[7] Fr¨uwirth-Schnatter. Data augmentation and dynamic linear models. J. Time Ser. Anal.  15:183–202  1994.
[8] C. K. Carter and R. Kohn. Markov chain Monte Carlo in conditionally Gaussian state space models.

Biometrika  83:589–601  1996.

[9] Radford M. Neal  Matthew J. Beal  and Sam T. Roweis.

Inferring state sequences for non-linear sys-
tems with embedded hidden Markov models. In Advances in Neural Information Processing Systems 16
(NIPS)  volume 16  pages 401–408. MIT Press  2004.

[10] J. Van Gael  Y. Saatci  Y. W. Teh  and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov

model. In Proceedings of the International Conference on Machine Learning  volume 25  2008.

[11] M. Dewar  C. Wiggins  and F. Wood. Inference in hidden Markov models with explicit state duration

distributions. IEEE Signal Processing Letters  page To Appear  2012.

[12] Christophe Andrieu  Arnaud Doucet  and Roman Holenstein. Particle Markov chain Monte Carlo meth-

ods. Journal of the Royal Statistical Society Series B  72(3):269–342  2010.

[13] V. Rao and Y. W. Teh. Fast MCMC sampling for Markov jump processes and continuous time Bayesian
networks. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence  2011.
[14] William Feller. On semi-Markov processes. Proceedings of the National Academy of Sciences of the

United States of America  51(4):pp. 653–659  1964.

[15] D. Sonderman. Comparing semi-Markov processes. Mathematics of Operations Research  5(1):110–119 

1980.

[16] D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes. Springer  2008.
[17] J. F. C. Kingman. Poisson processes  volume 3 of Oxford Studies in Probability. The Clarendon Press

Oxford University Press  New York  1993. Oxford Science Publications.

[18] A. Beskos and G.O. Roberts. Exact simulation of diffusions. Annals of applied probability  15(4):2422 –

2444  November 2005.

[19] Martyn Plummer  Nicky Best  Kate Cowles  and Karen Vines. CODA: Convergence diagnosis and output

analysis for MCMC. R News  6(1):7–11  March 2006.

[20] Andrew Golightly and Darren J. Wilkinson. Bayesian parameter inference for stochastic biochemical
Interface Focus  1(6):807–820  December

network models using particle Markov chain Monte Carlo.
2011.

[21] S. Asmussen. Applied Probability and Queues. Applications of Mathematics. Springer  2003.
[22] Stephen G. Walker. Sampling the Dirichlet mixture model with slices. Communications in Statistics -

Simulation and Computation  36:45  2007.

9

,Xiaozhi Chen
Kaustav Kundu
Yukun Zhu
Andrew Berneshawi
Huimin Ma
Sanja Fidler
Raquel Urtasun