2010,Extensions of Generalized Binary Search to Group Identification and Exponential Costs,Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes" or "no" questions posed about that object  and arises in problems such as active learning and active diagnosis. Here  we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First  we consider the case where the objects are partitioned into groups  and the objective is to identify only the group to which the object belongs. Then  we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case  we present an exact formula for the objective function involving Shannon or Renyi entropy  and develop a greedy algorithm for minimizing it.",Extensions of Generalized Binary Search to Group

Identiﬁcation and Exponential Costs

2Institute for Translational Sciences  3Dept. of Preventative Medicine and Community Health 

University of Texas Medical Branch  Galveston  TX 77555

Gowtham Bellala1  Suresh K. Bhavnani2 3 4  Clayton Scott1

1Department of EECS  University of Michigan  Ann Arbor  MI 48109

4School of Biomedical Informatics  University of Texas  Houston  TX 77030

gowtham@umich.edu  skbhavnani@gmail.com  clayscot@umich.edu

Abstract

Generalized Binary Search (GBS) is a well known greedy algorithm for identify-
ing an unknown object while minimizing the number of “yes” or “no” questions
posed about that object  and arises in problems such as active learning and active
diagnosis. Here  we provide a coding-theoretic interpretation for GBS and show
that GBS can be viewed as a top-down algorithm that greedily minimizes the ex-
pected number of queries required to identify an object. This interpretation is then
used to extend GBS in two ways. First  we consider the case where the objects are
partitioned into groups  and the objective is to identify only the group to which
the object belongs. Then  we consider the case where the cost of identifying an
object grows exponentially in the number of queries. In each case  we present an
exact formula for the objective function involving Shannon or R´enyi entropy  and
develop a greedy algorithm for minimizing it.

1

Introduction

In applications such as active learning [1  2  3  4]  disease/fault diagnosis [5  6  7]  toxic chemical
identiﬁcation [8]  computer vision [9  10] or the adaptive traveling salesman problem [11]  one often
encounters the problem of identifying an unknown object while minimizing the number of binary
questions posed about that object. In these problems  there is a set Θ = {θ1 ···   θM} of M different
objects and a set Q = {q1 ···   qN} of N distinct subsets of Θ known as queries. An unknown
object θ is generated from this set Θ with a certain prior probability distribution Π = (π1 ···   πM ) 
i.e.  πi = Pr(θ = θi)  and the goal is to uniquely identify this unknown object through as few queries
from Q as possible  where a query q ∈ Q returns a value 1 if θ ∈ q  and 0 otherwise. For example 
in active learning  the objects are classiﬁers and the queries are the labels for ﬁxed test points. In
active diagnosis  objects may correspond to faults  and queries to alarms. This problem has been
generically referred to as binary testing or object/entity identiﬁcation in the literature [5  12]. We
will refer to this problem as object identiﬁcation. Our attention is restricted to the case where Θ and
Q are ﬁnite  and the queries are noiseless.
The goal in object identiﬁcation is to construct an optimal binary decision tree  where each internal
node in the tree is associated with a query from Q  and each leaf node corresponds to an object
from Θ. Optimality is often with respect to the expected depth of the leaf node corresponding to
the unknown object θ. In general the determination of an optimal tree is NP-complete [13]. Hence 
various greedy algorithms [5  14] have been proposed to obtain a suboptimal binary decision tree. A
well studied algorithm for this problem is known as the splitting algorithm [5] or generalized binary
search (GBS) [1  2]. This is the greedy algorithm which selects a query that most evenly divides the
probability mass of the remaining objects [1  2  5  15].

1

GBS assumes that the end goal is to rapidly identify individual objects. However  in applications
such as disease diagnosis  where Θ is a collection of possible diseases  it may only be necessary
to identify the intervention or response to an object  rather than the object itself. In these prob-
lems  the object set Θ is partitioned into groups and it is only necessary to identify the group to
which the unknown object belongs. We note below that GBS is not necessarily efﬁcient for group
identiﬁcation.
To address this problem  we ﬁrst present a new interpretation of GBS from a coding-theoretic per-
spective by viewing the problem of object identiﬁcation as constrained source coding. Speciﬁcally 
we present an exact formula for the expected number of queries required to identify an unknown
object in terms of Shannon entropy of the prior distribution Π  and show that GBS is a top-down
algorithm that greedily minimizes this cost function. Then  we extend this framework to the problem
of group identiﬁcation and derive a natural extension of GBS for this problem.
We also extend the coding theoretic framework to the problem of object (or group) identiﬁcation
where the cost of identifying an object grows exponentially in the number of queries  i.e.  the cost
of identifying an object using d queries is λd for some ﬁxed λ > 1. Applications where such
a scenario arises have been discussed earlier in the context of source coding [16]  random search
trees [17] and design of alphabetic codes [18]  for which efﬁcient optimal or greedy algorithms
have been presented. In the context of object/group identiﬁcation  the exponential cost function has
certain advantages in terms of avoiding deep trees (which is crucial in time-critical applications)
and being more robust to misspeciﬁcation of the prior probabilities. However  there does not exist
an algorithm to the best of our knowledge that constructs a good suboptimal decision tree for the
problem of object/group identiﬁcation with exponential costs. Once again  we show below that GBS
is not necessarily efﬁcient for minimizing the exponential cost function  and propose an improved
greedy algorithm that generalizes GBS.

1.1 Notation
We denote an object identiﬁcation problem by a pair (B  Π) where B is a known M × N binary
matrix with bij equal to 1 if θi ∈ qj  and 0 otherwise. A decision tree T constructed on (B  Π) has a
query from the set Q at each of its internal nodes  with the leaf nodes terminating in the objects from
Θ. For a decision tree with L leaves  the leaf nodes are indexed by the set L = {1 ···   L} and the
internal nodes are indexed by the set I = {L + 1 ···   2L− 1}. At any node ‘a’  let Qa ⊆ Q denote
the set of queries that have been performed along the path from the root node up to that node. An
object θi reaches node ‘a’ if it agrees with the true θ on all queries in Qa  i.e.  the binary values in B
for the rows corresponding to θi and θ are same over the columns corresponding to queries in Qa.
At any internal node a ∈ I  let l(a)  r(a) denote the “left” and “right” child nodes  and let Θa ⊆ Θ
denote the set of objects that reach node ‘a’. Thus  the sets Θl(a) ⊆ Θa  Θr(a) ⊆ Θa correspond
to the objects in Θa that respond 0 and 1 to the query at node ‘a’  respectively. We denote by
denote the Shannon entropy of a proportion π ∈ [0  1] by H(π) := −π log2 π − (1− π) log2(1− π)
i πi log2 πi  where we use the limit 
lim
π→0

πΘa :=(cid:80){i:θi∈Θa} πi  the probability mass of the objects reaching node ‘a’ in the tree. Finally  we
and that of a vector Π = (π1 ···   πM ) by H(Π) := −(cid:80)

π log2 π = 0  to deﬁne the value of 0 log2 0.

2 GBS Greedily Minimizes the Expected Number of Queries

We begin by noting that object identiﬁcation reduces to the standard source coding problem [19]
in the special case when Q is complete  meaning  for any S ⊆ Θ there exists a query q ∈ Q such
that either q = S or Θ \ q = S. Here  the problem of constructing an optimal binary decision tree
is equivalent to constructing optimal variable length binary preﬁx codes  for which there exists an
efﬁcient optimal algorithm known as the Huffman algorithm [20]. It is also known that the expected
length of any binary preﬁx code (i.e.  expected depth of any binary decision tree) is bounded below
by the Shannon entropy of the prior distribution Π [19].
For the problem of object identiﬁcation  where Q is not complete  the entropy lower bound is still
valid  but Huffman coding cannot be implemented. In this case  GBS is a greedy  top-down al-
gorithm that is analogous to Shannon-Fano coding [21  22]. We now show that GBS is actually
greedily minimizing the expected number of queries required to identify an object.

2

First  we deﬁne a parameter called the reduction factor on the binary matrix/tree combination that
provides a useful quantiﬁcation on the expected number of queries required to identify an object.
Deﬁnition 1 (Reduction factor). Let T be a decision tree constructed on the pair (B  Π). The
reduction factor at any internal node ‘a’ in the tree is deﬁned by ρa = max{πΘl(a)   πΘr(a)}/πΘa.
Note that 0.5 ≤ ρa ≤ 1. Given an object identiﬁcation problem (B  Π)  let T (B  Π) denote the set
of decision trees that can uniquely identify all the objects in the set Θ. We assume that the rows of
B are distinct so that T (B  Π) (cid:54)= ∅. For any decision tree T ∈ T (B  Π)  let {ρa}a∈I denote the
set of reduction factors and let di denote the number of queries required to identify object θi in the
tree. Then the expected number of queries required to identify an unknown object using a tree (or 
i πidi. Note that the cost function depends on both Π
and d = (d1 ···   dM ). However  we do not show the dependence on d explicitly.
Theorem 1. For any T ∈ T (B  Π)  the expected number of queries required to identify an unknown
object is given by

the expected depth of a tree) is L1(Π) =(cid:80)

πΘa[1 − H(ρa)].

(1)

L1(Π) = H(Π) +(cid:88)

a∈I

min

T∈T (B Π)

H(Π) +(cid:80)

Theorems 1  2 and 3 are special cases of Theorem 4  whose proof is sketched in the Appendix.
Complete proofs are given in the Supplemental Material. Since H(ρa) ≤ 1  this theorem recovers
the result that L1(Π) is bounded below by the Shannon entropy H(Π). It presents the exact formula
for the gap in this lower bound. It also follows from the above result that a tree attains the entropy
bound iff the reduction factors are equal to 0.5 at each internal node in the tree. Using this result 
minimizing L1(Π) can be formulated as the following optimization problem:

a∈I πΘa[1 − H(ρa)].

Since Π is ﬁxed  this optimization problem reduces to minimizing (cid:80)

(2)
a∈I πΘa[1 − H(ρa)] over
T (B  Π). As mentioned earlier  ﬁnding a global optimal solution for this optimization problem is
NP-complete [13]. Instead  we may take a top down approach and minimize the objective function
by minimizing the term Ca := πΘa[1 − H(ρa)] at each internal node  starting from the root node.
Note that the only term that depends on the query chosen at node ‘a’ in this cost function is ρa.
Hence the algorithm reduces to minimizing ρa (i.e.  choosing a split as balanced as possible) at each
internal node a ∈ I.
In other words  greedy minimization of (2) is equivalent to GBS. In the next section  we show how
this framework can be extended to derive greedy algorithms for the problems of group identiﬁcation
and object identiﬁcation with exponential costs.
3 Extensions of GBS
3.1 Group Identiﬁcation
In group identiﬁcation1  the goal is not to determine the unknown object θ ∈ Θ  rather the group to
which it belongs  in as few queries as possible. Here  in addition to B and Π  the group labels for
the objects are also provided  where the groups are assumed to be disjoint.
We denote a group identiﬁcation problem by (B  Π  y)  where y = (y1 ···   yM ) denotes the group
labels of the objects  yi ∈ {1 ···   K}. Let {Θk}K
k=1 be the partition of Θ  where Θk = {θi ∈ Θ :
yi = k}. It is important to note here that the group identiﬁcation problem cannot be simply reduced
to an object identiﬁcation problem with groups {Θ1 ···   ΘK} as “meta objects ” since the objects
within a group need not respond the same to each query. For instance  consider the toy example
shown in Figure 1 where the objects θ1  θ2 and θ3 belonging to group 1 cannot be collapsed into a
single meta object as these objects respond differently to queries q1 and q3.
In this context  we also note that GBS can fail to produce a good solution for a group identiﬁcation
problem as it does not take the group labels into consideration while choosing queries. Once again 
consider the toy example shown in Figure 1 where query q2 is sufﬁcient to identify the group of an
unknown object  whereas GBS requires 2 queries to identify the group when the unknown object is
either θ2 or θ4. Here  we propose a natural extension of GBS to the problem of group identiﬁcation.
1Golovin et.al. [23] simultaneously studied the problem of group identiﬁcation in the context of object

identiﬁcation with persistent noise. Their algorithm is an extension of that in [24].

3

q1
0
1
0
1

q2
1
1
1
0

q3 Group label  y
1
0
0
0

1
1
1
2

Π
0.25
0.25
0.25
0.25

θ1
θ2
θ3
θ4

y = 1

q1

1

0

!DDDDD
{wwwww
?>=<
89:;
"FFFF
|xxxx
?>=<
89:;
1 "
0|
y = 2
y = 1

q2

Figure 1: Toy Example

Figure 2: Decision tree constructed using GBS

Note that when constructing a tree for group identiﬁcation  a greedy  top-down algorithm terminates
splitting when all the objects at the node belong to the same group. Hence  a tree constructed in this
fashion can have multiple objects ending in the same leaf node and multiple leaves ending in the
same group. For a tree with L leaves  we denote by Lk ⊂ L = {1 ···   L} the set of leaves that
terminate in group k. Similar to Θk ⊆ Θ  we denote by Θk
a ⊆ Θa the set of objects belonging to
group k that reach node ‘a’ in a tree. Also  in addition to the reduction factor deﬁned in Section 2 
we deﬁne a new parameter called the group reduction factor for each group k ∈ {1 ···   K} at each
internal node.
Deﬁnition 2 (Group reduction factor). Let T be a decision tree constructed on a group identiﬁcation
problem (B  Π  y). The group reduction factor for any group k at an internal node ‘a’ is deﬁned by
a = max{πΘk
ρk
Given (B  Π  y)  let T (B  Π  y) denote the set of decision trees that can uniquely identify the groups
of all objects in the set Θ. For any decision tree T ∈ T (B  Π  y)  let dj denote the depth of leaf
node j ∈ L. Let random variable X denote the number of queries required to identify the group
of an unknown object θ. Then  the expected number of queries required to identify the group of an
unknown object using the given tree is equal to

}/πΘk

  πΘk

r(a)

l(a)

.

a

K(cid:88)

L1(Π) =

Pr(θ ∈ Θk) E[X|θ ∈ Θk] =

k=1

k=1

K(cid:88)

(cid:88)

j∈Lk



πΘj
πΘk

dj

(3)

L1(Π) = H(Πy) +(cid:88)

a∈I

(cid:34)

Theorem 2. For any T ∈ T (B  Π  y)  the expected number of queries required to identify the group
of an unknown object is given by

πΘa

1 − H(ρa) +

a

πΘk
πΘa

H(ρk
a)

(4)

where Πy = (πΘ1 ···   πΘK ) denotes the probability distribution of the object groups induced by
the labels y and H(·) denotes the Shannon entropy.

Note that the term in the summation in (4) is non-negative. Hence  the above result implies that
L1(Π) is bounded below by the Shannon entropy of the probability distribution of the groups. It
also follows from this result that this lower bound is achieved iff the reduction factor ρa is equal to
0.5 and the group reduction factors {ρk
k=1 are equal to 1 at every internal node in the tree. Also 
note that the result in Theorem 1 is a special case of this result where each group is of size 1 leading
to ρk
Using this result  the problem of ﬁnding a decision tree with minimum L1(Π) can be formulated as:

a = 1 for all groups at every internal node.

a}K

(cid:80)

(cid:104)

1 − H(ρa) +(cid:80)K

min

T∈T (B Π y)

a∈I πΘa

πΘk
a
πΘa

H(ρk
a)

k=1

.

(5)

This optimization problem being a generalized version of that in (2) is NP-complete. Hence  we
may take a top-down approach and minimize the objective function greedily by minimizing the term
a)] at each internal node  starting from the root node. Note that
a. Hence the algorithm reduces

πΘa[1 − H(ρa) +(cid:80)K
to minimizing Ca := 1 − H(ρa) +(cid:80)K

the terms that depend on the query chosen at node ‘a’ are ρa and ρk

H(ρk

πΘk
a
πΘa

k=1

πΘk
a
πΘa

k=1

H(ρk

a) at each internal node ‘a’.

4

πΘk

K(cid:88)

k=1

(cid:35)

(cid:105)

{
!
Group-GBS (GGBS)
Initialize: L = {root node}  Qroot = ∅
while some a ∈ L has more than one group
Choose query q∗ = arg minq∈Q\Qa Ca(q)
Form child nodes l(a)  r(a)
Replace ‘a’ with l(a)  r(a) in L
end

Ca = 1 − H(ρa) +(cid:80)K

πΘk
a
πΘa

a)
H(ρk

k=1

λ-GBS
Initialize: L = {root node}  Qroot = ∅
while some a ∈ L has more than one object
Choose query q∗ = arg minq∈Q\Qa Ca(q)
Form child nodes l(a)  r(a)
Replace ‘a’ with l(a)  r(a) in L
end
Ca = πΘl(a)
πΘa

Dα(Θl(a)) + πΘr(a)

Dα(Θr(a))

πΘa

Figure 3: Greedy algorithm for group identiﬁ-
cation

Figure 4: Greedy algorithm for object identiﬁ-
cation with exponential costs

k

πΘk
a
πΘa

H(ρk

group) while the term(cid:80)

Note that this objective function consists of two terms  the ﬁrst term [1 − H(ρa)] favors queries that
evenly distribute the probability mass of the objects at node ‘a’ to its child nodes (regardless of the
a) favors queries that transfer an entire group of objects to one of
its child nodes. This algorithm  which we refer to as Group Generalized Binary Search (GGBS)  is
summarized in Figure 3. Finally  as an interesting connection with greedy decision-tree algorithms
for multi-class classiﬁcation  it can be shown that GGBS is equivalent to the decision-tree splitting
algorithm used in the C4.5 software package  based on the entropy impurity measure [25].
3.2 Exponential Costs

Now assume the cost of identifying an object is deﬁned by Lλ(Π) := logλ((cid:80)

i πiλdi)  where λ > 1
and di corresponds to the depth of object θi in a tree. In the limiting case where λ tends to 1 and ∞ 
this cost function reduces to the average depth and worst case depth  respectively. That is 

L1(Π) = lim
λ→1

Lλ(Π) =

πidi 

L∞(Π) := lim

λ→∞Lλ(Π) = max

i∈{1 ···  M}di.

1−α log2 ((cid:80)

As mentioned in Section 2  GBS is tailored to minimize L1(Π)  and hence may not produce a good
suboptimal solution for the exponential cost function with λ > 1. Thus  we derive an extension
of GBS for the problem of exponential costs. Here  we use a result by Campbell [26] which states
that the exponential cost Lλ(Π) of any tree T is bounded below by the α-R´enyi entropy  given by
Hα(Π) := 1
1+log2 λ. We consider a general object identiﬁcation
problem and derive an explicit formula for the gap in this lower bound. We then use this formula to
derive a family of greedy algorithms that minimize the exponential cost function Lλ(Π) for λ > 1.
Note that the entropy bound reduces to the Shannon entropy H(Π) and log2 M  in the limiting cases
where λ tends to 1 and ∞  respectively.
Theorem 3. For any λ > 1 and any T ∈ T (B  Π)  the exponential cost Lλ(Π) is given by

i )  where α =

i πα

1

(cid:21)

M(cid:88)

i=1

(cid:20)

λLλ(Π) = λHα(Π) +(cid:88)
reach node ‘a’  πΘa = (cid:80)

a∈I

πΘa

where da denotes the depth of any internal node ‘a’ in the tree  Θa denotes the set of objects that

(λ − 1)λda − Dα(Θa) +

πΘl(a)
πΘa

πi  α =

1+log2 λ and Dα(Θa) :=

1

{i:θi∈Θa}

Dα(Θl(a)) +

(cid:104)(cid:80){i:θi∈Θa}

πΘr(a)
πΘa

(cid:16) πi

πΘa

Dα(Θr(a))

(cid:17)α(cid:105)1/α

.

The term in summation over internal nodes I in the above result corresponds to the gap in the
Campbell’s lower bound. This result suggests a top-down greedy approach to minimize Lλ(Π) 
which is to minimize the term (λ − 1)λda − Dα(Θa) + πΘl(a)
Dα(Θr(a)) at
each internal node  starting from the root node. Noting that the terms that depend on the query
chosen at node ‘a’ are πΘl(a)  πΘr(a)  Dα(Θl(a)) and Dα(Θr(a))  this reduces to minimizing Ca :=
Dα(Θr(a)) at each internal node. This algorithm  which we refer to as
πΘl(a)
πΘa
λ-GBS  can be summarized as shown in Figure 4. Also  it can be shown by the application of
L’Hˆopital’s rule that in the limiting case where λ → 1  λ-GBS reduces to GBS  and in the case
where λ → ∞  λ-GBS reduces to GBS with uniform prior πi = 1/M. The latter algorithm is GBS
but with the true prior Π replaced by a uniform distribution.

Dα(Θl(a)) + πΘr(a)

Dα(Θl(a)) + πΘr(a)

πΘa

πΘa

πΘa

5

Figure 5: Beta distribution over the range [0.5  1]
for different values of β when α = 1

Figure 6: Expected number of queries required to identify
the group of an object using GBS and GGBS

3.3 Group Identiﬁcation with Exponential Costs
Finally  we complete our discussion by considering the problem of group identiﬁcation with expo-
nential costs. Here  the cost of identifying the group of an object given a tree T ∈ T (B  Π  y)  is
  which reduces to (3) in the limiting case as λ → 1 
deﬁned to be Lλ(Π) = logλ
and to maxj∈L dj  i.e.  the worst case depth of the tree  in the case where λ → ∞.
Theorem 4. For any λ > 1 and any T ∈ T (B  Π  y)  the exponential cost Lλ(Π) of identifying the
group of an object is given by

j∈L πΘj λdj

(cid:17)

(cid:21)

(cid:16)(cid:80)
(cid:20)

πΘa

λLλ(Π) = λHα(Πy) +(cid:88)
(cid:104)(cid:80)K

πΘa

a∈I

πΘl(a)
πΘa

πΘr(a)
πΘa

k=1

a
πΘa

1

1+log2 λ .

with α =

(cid:16) πΘk

Dα(Θl(a)) +

(cid:17)α(cid:105)1/α

Dα(Θl(a)) + πΘr(a)

(λ − 1)λda − Dα(Θa) +

Dα(Θr(a))
where Πy = (πΘ1 ···   πΘK ) denotes the probability distribution of the object groups induced by
the labels y  Dα(Θa) :=
Note that the deﬁnition of Dα(Θa) in this theorem is a generalization of that in Theorem 3. As
mentioned earlier  Theorems 1-3 are special cases of the above theorem  where Theorem 2 follows
as λ → 1 and Theorem 1 follows when each group is of size one in addition. This result also
implies a top-down  greedy algorithm to minimize Lλ(Π)  which is to choose a query that minimizes
Dα(Θr(a)) at each internal node. Once again  it can be shown by
Ca := πΘl(a)
πΘa
the application of L’Hˆopital’s rule that in the limiting case where λ → 1  this reduces to GGBS  and
in the case where λ → ∞  this reduces to choosing a query that minimizes the maximum number of
groups in the child nodes [27].
4 Performance of the Greedy Algorithms
We compare the performance of the proposed algorithms to that of GBS on synthetic data generated
using different random data models.
4.1 Group Identiﬁcation
For ﬁxed M = |Θ| and N = |Q|  we consider a random data model where each query q ∈ Q is
associated with a pair of parameters (γw(q)  γb(q)) ∈ [0.5  1]2. Here  γw(q) reﬂects the correlation
of the object responses within a group  and γb(q) captures the correlation of object responses between
groups. When γw(q) is close to 0.5  each object within a group is equally likely to exhibit 0 or 1
as its response to query q  whereas  when it is close to 1  most of the objects within a group are
highly likely to exhibit the same query response. Similarly  when γb(q) is close to 0.5  each group
is equally likely to exhibit 0 or 1 as its response to the query  where a group response corresponds
to the majority vote of the object responses within a group  while  as γb(q) tends to 1  most of the

6

0.50.60.70.80.91246810β > 1β < 1β = 10.50.750.9512480.750.951248456789 βbβw Expected # of queriesGBSGGBSEntropy boundFigure 7: Exponential cost incurred in identifying an object using GBS and λ-GBS

groups are highly likely to exhibit the same response. Given these correlation values (γw(q)  γb(q))
for a query q  the object responses to query q (i.e.  the binary column of 0’s and 1’s corresponding
to query q in B) are generated as follows
1. Flip a fair coin to generate a Bernoulli random variable  x
2. For each group k ∈ {1 ···   K}  assign a binary label bk  where bk = x with probability γb(q)
3. For each object in group k  assign bk as the object response to q with probability γw(q)
Given the correlation parameters (γw(q)  γb(q))  ∀q ∈ Q  a random dataset can be created by fol-
lowing the above procedure for each query.
We compare the performances of GBS and GGBS on random datasets generated using the above
model. We demonstrate the results on datasets of size N = 200 (# of queries) and M = 400
(# of objects)  where we randomly partitioned the objects into 15 groups and assumed a uniform
prior on the objects. For each dataset  the correlation parameters are drawn from independent beta
distributions over the range [0.5  1]  i.e.  γw(q) ∼ Beta(1  βw) and γb(q) ∼ Beta(1  βb) where
βw  βb ∈ {0.5  0.75  0.95  1  2  4  8}. Figure 5 shows the density function (pdf) of Beta(1  β) for
different values of β. Note that β = 1 corresponds to a uniform distribution  while  for β < 1 the
distribution is right skewed and for β > 1 the distribution is left skewed.
Figure 6 compares the mean value of the cost function L1(Π) for GBS and GGBS over 100 randomly
generated datasets  for each value of (βw  βb). This shows the improved performance of GGBS over
GBS in group identiﬁcation. Especially  note that GGBS achieves performance close to the entropy
bound as βw decreases. This is due to the increased number of queries with γw(q) close to 1 in the
dataset. As the correlation parameter γw(q) tends to 1  choosing that query keeps the groups intact 
a tend to 1 for these queries. Such queries offer signiﬁcant gains
i.e.  the group reduction factors ρk
in group identiﬁcation  but can be overlooked by GBS.
4.2 Object Identiﬁcation with Exponential Costs
We consider the same random data model as above where we set K = M  i.e.  each group is
comprised of one object. Thus  the only correlation parameter that determines the structure of the
dataset is γb(q)  q ∈ Q. Figure 7 demonstrates the improved performance of λ-GBS over standard
GBS  and GBS with uniform prior  over a range of λ values  for a dataset generated using the above
random data model with γb(q) ∼ Beta(1  1) = unif[0.5  1]. Each curve in the ﬁgure corresponds
to the average value of the cost function Lλ(Π) as a function of λ over 100 repetitions. In each
j=1  δ ≥ 0  after
randomly permuting the objects. Note that in the special case when δ = 0  this reduces to the
uniform distribution and as δ increases  it tends to a skewed distribution with most of the probability
mass concentrated on few objects.
Similar experiments have been performed on datasets generated using γb(q) ∼ Beta(α  β) for differ-
ent values of α  β. In all our experiments  we observed λ-GBS to be consistently performing better
than both the standard GBS  and GBS with uniform prior. In addition  the performance of λ-GBS
has been observed to be very close to that of the entropy bound. Finally  Figure 7 also reﬂects that
λ-GBS converges to GBS as λ → 1  and to GBS with uniform prior as λ → ∞.

repetition  the prior is generated according to Zipf’s law  i.e.  (j−δ/(cid:80)M

i=1 i−δ)M

7

1001011021031041057891011λ Average Exponential cost  Lλ(Π)δ = 1100101102103104105468101214λδ = 2 GBSGBS−Uniformλ−GBSEntropy bound5 Conclusions
In this paper  we show that generalized binary search (GBS) is a top-down algorithm that greedily
minimizes the expected number of queries required to identify an object. We then use this inter-
pretation to extend GBS in two ways. First  we consider the case where the objects are partitioned
into groups  and the goal is to identify only the group of the unknown object. Second  we consider
the problem where the cost of identifying an object grows exponentially in the number of queries.
The algorithms are derived in a common framework. In particular  we prove the exact formulas for
the cost function in each case that close the gap between previously known lower bounds related to
Shannon and R´enyi entropy. These exact formulas are then optimized in a greedy  top-down manner
to construct a decision tree. We demonstrate the improved performance of the proposed algorithms
over GBS through simulations. An important open question and the direction of our future work is
to relate these greedy algorithms to the global optimizer of their respective cost functions.

Acknowledgements

G. Bellala and C. Scott were supported in part by NSF Awards No. 0830490 and 0953135. S.
Bhavnani was supported in part by CDC/NIOSH grant No. R21OH009441.
6 Appendix: Proof Sketch for Theorem 4

Deﬁne two new functions(cid:101)Lλ and (cid:101)Hα as
 =(cid:88)
(cid:101)Lλ :=
where(cid:101)Lλ is related to the cost function Lλ(Π) as λLλ(Π) = (λ− 1)(cid:101)Lλ + 1  and (cid:101)Hα is related to the

dj−1(cid:88)

(cid:88)

πΘj λdj − 1

k=1 πα
Θk

(cid:17) 1

λ − 1

 and (cid:101)Hα := 1 −
(cid:16)(cid:80)K
(cid:32) K(cid:88)

α-R´enyi entropy Hα(Πy) as

(cid:33) 1

j∈L

j∈L

πΘj

λh

h=0

1

1

1

 

α

α

α

1

=

k=1

k=1

k=1

πα
Θk

πα
Θk

log2

Θk =
πα

α log2 λ

Θk = logλ
πα

(cid:32) K(cid:88)

K(cid:88)
K(cid:88)
(cid:33) 1
(cid:33) 1
α (cid:101)Hα + 1
λda πΘa =⇒ λLλ(Π) = 1 +(cid:88)
(cid:2)πΘaDα(Θa) − πΘl(a)Dα(Θl(a)) − πΘr(a)Dα(Θr(a))(cid:3) .

(λ − 1)λda πΘa

a∈I

k=1

1

1+log2 λ in (6a). Now  we note from Lemma 1 that

(6a)

(6b)

(7)

(8)

=⇒ λHα(Πy) =

Hα(Πy) =

log2

πα
Θk

k=1

1 − α

(cid:32) K(cid:88)
(cid:101)Lλ =(cid:88)
λHα(Πy) = 1 +(cid:88)

a∈I

a∈I

where we use the deﬁnition of α  i.e.  α =

where da denotes the depth of internal node ‘a’ in the tree T . Similarly  we note from (6b) and
Lemma 2 that

(cid:101)Hα =

of the objects at that node.

Finally  the result follows from (7) and (8) above.

Lemma 1. The function (cid:101)Lλ can be decomposed over the internal nodes in a tree T   as (cid:101)Lλ =
(cid:80)
a∈I λda πΘa  where da denotes the depth of internal node a ∈ I and πΘa is the probability mass
Lemma 2. The function (cid:101)Hα can be decomposed over the internal nodes in a tree T   as
(cid:88)
(cid:2)πΘaDα(Θa) − πΘl(a)Dα(Θl(a)) − πΘr(a)Dα(Θr(a))(cid:3)
(cid:17) 1
(cid:17)α(cid:105) 1
(cid:16) πΘk

where Dα(Θa) :=
internal node a ∈ I.
The above two lemmas can be proved using induction over subtrees rooted at any internal node ‘a’
in the tree. The details may be found in the Supplemental Material.

α and πΘa denotes the probability mass of the objects at any

(cid:16)(cid:80)K
(cid:104)(cid:80)K

k=1 πα
Θk

a
πΘa

a∈I

k=1

1

α

8

References
[1] S. Dasgupta  “Analysis of a greedy active learning strategy ” Advances in Neural Information Processing

Systems  2004.

[2] R. Nowak  “Generalized binary search ” Proceedings of the 46th Allerton Conference on Communications 

Control and Computing  pp. 568–574  2008.

[3] ——  “Noisy generalized binary search ” Advances in Neural Information Processing Systems  vol. 22 

pp. 1366–1374  2009.

[4] D. Golovin and A. Krause  “Adaptive Submodularity: A new approach to active learning and stochastic

optimization ” In Proceedings of International Conference on Learning Theory (COLT)  2010.

[5] D. W. Loveland  “Performance bounds for binary testing with arbitrary weights ” Acta Informatica  1985.
[6] F. Yu  F. Tu  H. Tu  and K. Pattipati  “Multiple disease (fault) diagnosis with applications to the QMR-DT
problem ” Proceedings of IEEE International Conference on Systems  Man and Cybernetics  vol. 2  pp.
1187–1192  October 2003.

[7] J. Shiozaki  H. Matsuyama  E. O’Shima  and M. Iri  “An improved algorithm for diagnosis of system
failures in the chemical process ” Computational Chemical Engineering  vol. 9  no. 3  pp. 285–293  1985.
[8] S. Bhavnani  A. Abraham  C. Demeniuk  M. Gebrekristos  A. Gong  S. Nainwal  G. Vallabha  and
R. Richardson  “Network analysis of toxic chemicals and symptoms: Implications for designing ﬁrst-
responder systems ” Proceedings of American Medical Informatics Association  2007.

[9] D. Geman and B. Jedynak  “An active testing model for tracking roads in satellite images ” IEEE Trans-

actions on Pattern Analysis and Machine Intelligence  vol. 18  no. 1  pp. 1–14  1996.

[10] M. J. Swain and M. A. Stricker  “Promising directions in active vision ” International Journal of Computer

Vision  vol. 11  no. 2  pp. 109–126  1993.

[11] A. Gupta  R. Krishnaswamy  V. Nagarajan  and R. Ravi  “Approximation algorithms for optimal decision

trees and adaptive TSP problems ” 2010  available online at arXiv.org:1003.0722.

[12] M. Garey  “Optimal binary identiﬁcation procedures ” SIAM Journal on Applied Mathematics  vol. 23(2) 

pp. 173–186  1972.

[13] L. Hyaﬁl and R. Rivest  “Constructing optimal binary decision trees is NP-complete ” Information Pro-

cessing Letters  vol. 5(1)  pp. 15–17  1976.

[14] S. R. Kosaraju  T. M. Przytycka  and R. S. Borgstrom  “On an optimal split tree problem ” Proceedings of

6th International Workshop on Algorithms and Data Structures  WADS  pp. 11–14  1999.

[15] R. M. Goodman and P. Smyth  “Decision tree design from a communication theory standpoint ” IEEE

Transactions on Information Theory  vol. 34  no. 5  1988.

[16] P. A. Humblet  “Generalization of Huffman coding to minimize the probability of buffer overﬂow ” IEEE

Transactions on Information Theory  vol. IT-27  no. 2  pp. 230–232  March 1981.

[17] F. Schulz  “Trees with exponentially growing costs ” Information and Computation  vol. 206  2008.
[18] M. B. Baer  “R´enyi to R´enyi - source coding under seige ” Proceedings of IEEE International Symposium

on Information Theory  pp. 1258–1262  July 2006.

[19] T. M. Cover and J. A. Thomas  Elements of Information Theory.
[20] D. A. Huffman  “A method for the construction of minimum-redundancy codes ” Proceedings of the

John Wiley  1991.

Institute of Radio Engineers  1952.

[21] C. E. Shannon  “A mathematical theory of communication ” Bell Systems Technical Journal  vol. 27  pp.

379 – 423  July 1948.

[22] R. M. Fano  Transmission of Information. MIT Press  1961.
[23] D. Golovin  D. Ray  and A. Krause  “Near-optimal Bayesian active learning with noisy observations ” to

appear in the Proceedings of the Neural Information Processing Systems (NIPS)  2010.

[24] S. Dasgupta  “Coarse sample complexity bounds for active learning ” Advances in Neural Information

Processing Systems  2006.

[25] G. Bellala  S. Bhavnani  and C. Scott  “Group-based query learning for rapid diagnosis in time-critical

situations ” Tech. Rep.  2009  available online at arXiv.org:0911.4511.

[26] L. L. Campbell  “A coding problem and R´enyi’s entropy ” Information and Control  vol. 8  no. 4  pp.

423–429  August 1965.

[27] G. Bellala  S. Bhavnani  and C. Scott  “Query learning with exponential query costs ” Tech. Rep.  2010 

available online at arXiv.org:1002.4019.

9

,Stefanie Jegelka
Francis Bach
Suvrit Sra
Will Hamilton
Zhitao Ying
Jure Leskovec