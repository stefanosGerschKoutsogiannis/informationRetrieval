2013,Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion,We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and  if so  for reconstructing and denoising it; and a priori bounds on the error of each entry  individually.  In the noiseless case our algorithm is exact. For rank-one matrices  the new algorithm is fast  admits a highly-parallel implementation  and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods.,Error-Minimizing Estimates and Universal

Entry-Wise Error Bounds for Low-Rank Matrix

Completion

Franz J. Kir´aly⇤

Department of Statistical Science and

Centre for Inverse Problems
University College London
f.kiraly@ucl.ac.uk

Louis Theran†

Institute of Mathematics
Discrete Geometry Group
Freie Universit¨at Berlin

theran@math.fu-berlin.de

Abstract

We propose a general framework for reconstructing and denoising single entries
of incomplete and noisy entries. We describe: effective algorithms for deciding
if and entry can be reconstructed and  if so  for reconstructing and denoising it;
and a priori bounds on the error of each entry  individually. In the noiseless case
our algorithm is exact. For rank-one matrices  the new algorithm is fast  admits
a highly-parallel implementation  and produces an error minimizing estimate that
is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and
OptSpace methods.

1

Introduction

Matrix Completion is the task to reconstruct low-rank matrices from a subset of its entries and
occurs naturally in many practically relevant problems  such as missing feature imputation  multi-
task learning [2]  transductive learning [4]  or collaborative ﬁltering and link prediction [1  8  9].
Almost all known methods performing matrix completion are optimization methods such as the
max-norm and nuclear norm heuristics [3  9  10]  or OptSpace [5]  to name a few amongst many.
These methods have in common that in general: (a) they reconstruct the whole matrix; (b) error
bounds are given for all of the matrix  not single entries; (c) theoretical guarantees are given based
on the sampling distribution of the observations. These properties are all problematic in scenarios
where: (i) one is interested only in predicting or imputing a speciﬁc set of entries; (ii) the entire data
set is unwieldy to work with; (iii) or there are non-random “holes” in the observations. All of these
possibilities are very natural for the typical “big data” setup.
The recent results of [6] suggest that a method capable of handling challenges (i)–(iii) is within
reach. By analyzing the algebraic-combinatorial structure Matrix Completion  the authors provide
algorithms that identify  for any ﬁxed set of observations  exactly the entries that can be  in principle 
reconstructed from them. Moreover  the theory developed indicates that  when a missing entry can
be determined  it can be found by ﬁrst exposing combinatorially-determined polynomial relations
between the known entries and the unknown ones and then selecting a common solution.
To bridge the gap between the theory of [6] and practice are the following challenges: to efﬁciently
ﬁnd the relevant polynomial relations; and to extend the methodology to the noisy case. In this
paper  we show how to do both of these things in the case of rank one  and discuss how to instantiate
the same scheme for general rank. It will turn out that ﬁnding the right set of polynomials and

⇤Supported by the Mathematisches Forschungsinstitut Oberwolfach
†Supported by the European Research Council under the European Union’s Seventh Framework Programme

(FP7/2007-2013) / ERC grant agreement no 247029-SDModels.

1

noisy estimation are intimately related: we can treat each polynomial as providing an estimate of
the missing entry  and we can then take as our estimate the variance minimizing weighted average.
This technique also gives a priori lower bounds for a broad class of unbiased single-entry estimators
in terms of the combinatorial structure of the observations and the noise model only. In detail  our
contributions include:

of a rank-one-matrix  under the assumption of known noise variances

• the construction of a variance-minimal and unbiased estimator for any ﬁxed missing entry
• an explicit form for the variance of that estimator  being a lower bound for the variance of
any unbiased estimation of any ﬁxed missing entry and thus yielding a quantiative measure
on the trustability of that entry reconstructed from any algorithm
• the description of a strategy to generalize the above to any rank
• comparison of the estimator with two state-of-the-art optimization algorithms (OptSpace
and nuclear norm)  and error assessment of the three matrix completion methods with the
variance bound

As mentioned  the restriction to rank one is not inherent in the overall scheme. We depend on rank
one only in the sense that we understand the combinatorial-algebraic structure of rank-one-matrix
completion exactly  whereas the behavior in higher rank is not yet as well understood. Nonetheless 
it is  in principle accessible  and  once available will can be “plugged in” to the results here without
changing the complexity much. In this sense  the present paper is a proof-of-concept for a new
approach to estimating and denoising in algebraic settings based on combinatorially enumerating a
set of polynomial estimators and then averaging them. For us  computational efﬁciency comes via
a connection to the topology of graphs that is speciﬁc to this problem  but we suspect that this part 
too  can be generalized somewhat.

2 The Algebraic Combinatorics of Matrix Completion

We ﬁrst brieﬂy review facts about Matrix Completion that we require. The exposition is along the
lines of [6].
Deﬁnition 2.1. A matrix M 2 {0  1}m⇥n is called a mask. If A is a partially known matrix  then
the mask of A is the mask which has ones in exactly the positions which are known in A and zeros
otherwise.
Deﬁnition 2.2. Let M be an (m⇥n) mask. We will call the unique bipartite graph G(M ) which has
M as bipartite adjacency matrix the completion graph of M. We will refer to the m vertices of G(M )
corresponding to the rows of M as blue vertices  and to the n vertices of G(M ) corresponding to
the columns as red vertices. If e = (i  j) is an edge in Km n (where Km n is the complete bipartite
graph with m blue and n red vertices)  we will also write Ae instead of Aij and for any (m ⇥ n)
matrix A.

A fundamental result  [6  Theorem 2.3.5]  says that identiﬁability and reconstructability are  up to a
null set  graph properties.
Theorem 2.3. Let A be a generic1 and partially known (m⇥ n) matrix of rank r  let M be the mask
of A  let i  j be integers. Whether Aij is reconstructible (uniquely  or up to ﬁnite choice) depends
only on M and the true rank r; in particular  it does not depend on the true A.

For rank one  as opposed to higher rank  the set of reconstructible entries is easily obtainable from
G(M ) by combinatorial means:
Theorem 2.4 ([6  Theorem 2.5.36 (i)]). Let G ✓ Km n be the completion graph of a partially
known (m ⇥ n) matrix A. Then the set of uniquely reconstructible entries of A is exactly the set
Ae  with e in the transitive closure of G. In particular  all of A is reconstructible if and only if G is
connected.

1In particular  if A is sampled from a continuous density  then the set of non-generic A is a null set.

2

2.1 Reconstruction on the transitive closure

We extend Theorem 2.4’s theoretical reconstruction guarantee by describing an explicit  algebraic
algorithm for actually doing the reconstruction.
Deﬁnition 2.5. Let P ✓ Km n (or  C ✓ Km n) be a path (or  cycle)  with a ﬁxed start and end. We
will denote by E+(P ) be the set of edges in P (resp. E+(C) and C) traversed from blue vertex to
a red one  and by E(P ) the set of edges traversed from a red vertex to a blue one 2. From now
on  when we speak of “oriented paths” or “oriented cycles”  we mean with this sign convention and
some ﬁxed traversal order.
Let A = Aij be a (m ⇥ n) matrix of rank 1  and identify the entries Aij with the edges of Km n.
For an oriented cycle C  we deﬁne the polynomials

PC(A) = Ye2E+(C)
LC(A) = Xe2E+(C)

Ae 

Ae  Ye2E(C)
log Ae  Xe2E(C)

and

log Ae 

where for negative entries of A  we ﬁx a branch of the complex logarithm.
Theorem 2.6. Let A = Aij be a generic (m ⇥ n) matrix of rank 1. Let C ✓ Km n be an oriented
cycle. Then  PC(A) = LC(A) = 0.
Proof: The determinantal ideal of rank one is a binomial ideal generated by the (2 ⇥ 2) minors of A
(where entries of A are considered as variables). The minor equations are exactly PC(A)  where C
is an elementary oriented four-cycle; if C is an elementary 4-cycle  denote its edges by a(C)  b(C) 
c(C)  d(C)  with E+(C) = {a(C)  d(C)}. Let C be the collection of the elementary 4-cycles  and
deﬁne LC(A) = {LC(A) : C 2 C} and PC(A) = {PC(A) : C 2 C}.
By sending the term log Ae to a formal variable xe  we see that the free Z-group generated by the
LC(A) is isomorphic to H1(Km n  Z). With this equivalence  it is straightforward that  for any
oriented cycle D  LD(A) lies in the Z-span of elements of LC(A) and  therefore  formally 

LD(A) =XC2C

↵C · LC(A)

with the ↵C 2 Z. Thus LD(·) vanishes when A is rank one  since the r.h.s. does. Exponentiating
⇤
completes the proof.
Corollary 2.7. Let A = Aij be a (m ⇥ n) matrix of rank 1. Let v  w be two vertices in Km n. Let
P  Q be two oriented paths in Km n starting at v and ending at w. Then  for all A  it holds that
LP (A) = LQ(A).

3 A Combinatorial Algebraic Estimate for Missing Entries and Their Error

We now construct our estimator.

3.1 The sampling model

In all of the following  we will assume that the observations arise from the following sampling
process:
Assumption 3.1. There is an unknown ﬁxed  rank one  matrix A which is generic  and an (m ⇥ n)
mask M 2 {0  1}m⇥n which is known. There is a (stochastic) noise matrix E 2 Rm⇥n whose entries
are uncorrelated and which is multiplicatively centered with ﬁnite variance  non-zero3 variance; i.e. 
E(log Eij) = 0 and 0 < Var(log Eij) < 1 for all i and j.
The observed data is the matrix A  M  E = ⌦(A  E)  where  denotes the Hadamard (i.e. 
component-wise) product. That is  the observation is a matrix with entries Aij · Mij · Eij.

2Any ﬁxed orientation of Km n will give us the same result.
3The zero-variance case corresponds to exact reconstruction  which is handled already by Theorem 2.4.

3

The assumption of multiplicative noise is a necessary precaution in order for the presented estimator
(and in fact  any estimator) for the missing entries to have bounded variance  as shown in Exam-
ple 3.2 below. This is not  in practice  a restriction since an inﬁnitesimal additive error Aij on an
entry of A is equivalent to an inﬁnitesimal multiplicative error  log Aij = Aij/Aij  and additive
variances can be directly translated into multiplicative variances if the density function for the noise
is known4. The previous observation implies that the multiplicative noise model is as powerful as
any additive one that allows bounded variance estimates.
Example 3.2. Consider a (2 ⇥ 2)-matrix A of rank 1. The unique equation between the entries
is then A11A22 = A12A21. Solving for any entry will have another entry in the denominator  for
example A11 = A12A21
. Thus we get an estimator for A11 when substituting observed and noisy
entries for A12  A21  A22. When A22 approaches zero  the estimation error for A11 approaches
inﬁnity. In particular  if the density function of the error E22 of A22 is too dense around the value
A22  then the estimate for A11 given by the equation will have unbounded variance. In such a
case  one can show that no estimator for A11 has bounded variance.

A22

3.2 Estimating entries and error bounds

In this section  we construct the unbiased estimator for the entries of a rank-one-matrix with minimal
variance. First  we deﬁne some notation to ease the exposition:
Notations 3.3. We will denote by aij = log Aij and "ij = log Eij the logarithmic entries and noise.
Thus  for some path P in Km n we obtain

LP (A) = Xe2E+(P )

ae  Xe2E(P )

ae.

Denote by bij = aij + "ij the logarithmic (observed) entries  and B the (incomplete) matrix which
has the (observed) bij as entries. Denote by ij = Var(bij) = Var("ij).

The components of the estimator will be built from the LP :
Lemma 3.4. Let G = G(M ) be the graph of the mask M. Let x = (v  w) 2 Km n be any edge
with v red. Let P be an oriented path in G(M ) starting at v and ending at w. Then 

LP (B) = Xe2E+(P )

be  Xe2E(P )

be

is an unbiased estimator for ax with variance Var(LP (B)) =Pe2P e.

Proof: By linearity of expectation and centeredness of "ij  it follows that
E(be) 

E(LP (B)) = Xe2E+(P )
Var(LP (B)) = Xe2E+(P )

E(be)  Xe2E(P )
Var(be) + Xe2E(P )

Var(be) 

and the statement follows from the deﬁnition of e.

thus LP (B) is unbiased. Since the "e are uncorrelated  the be also are; thus  by Bienaym´e’s formula 
we obtain

In the following  we will consider the following parametric estimator as a candidate for estimating
ae:
Notations 3.5. Fix an edge x = (v  w) 2 Km n. Let P be a basis for the v–w path space and
denote #P by p. For ↵ 2 Rp  set X(↵) =PP2P ↵P LP (B).
Furthermore  we will denote by

the n-vector of ones.

4The multiplicative noise assumption causes the observed entries and the true entries to have the same sign.
The change of sign can be modeled by adding another multiplicative binary random variable in the model which
takes values ±1; this adds an independent combinatorial problem for the estimation of the sign which can be
done by maximum likelihood. In order to keep the exposition short and easy  we did not include this into the
exposition.

4

The following Lemma follows immediately from Lemma 3.4 and Theorem 2.6:
Lemma 3.6. E(X(↵)) = >↵ · bx; in particular  X(↵) is an unbiased estimator for bx if and only
if >↵ = 1.

We will now show that minimizing the variance of X(↵) can be formulated as a quadratic program
with coefﬁcients entirely determined by ax  the measurements be and the graph G(M ). In particular 
we will expose an explicit formula for the ↵ minimizing the variance. The formula will make use of
the following path kernel. For ﬁxed vertices s and t  an s–t path is the sum of a cycle H1(G  Z) and
ast. The s–t path space is the linear span of all the s–t paths. We discuss its relevant properties in
Appendix A.
Deﬁnition 3.7. Let e 2 Km n be an edge. For an edge e and a path P   set ce P = ±1 if e 2 E±(P )
otherwise ce P = 0. Let P  Q 2 P be any ﬁxed oriented paths. Deﬁne the (weighted) path kernel
k : P ⇥ P ! R by

k(P  Q) = Xe2Km n

ce P · ce Q · e.

Under our assumption that Var(be) > 0 for all e 2 Km n  the path kernel is positive deﬁnite  since
it is a sum of p independent positive semi-deﬁnite functions; in particular  its kernel matrix has full
rank. Here is the variance-minimizing unbiased estimator:
Proposition 3.8. Let x = (s  t) be a pair of vertices  and P a basis for the s–t path space in G with
p elements. Let ⌃ be the p ⇥ p kernel matrix of the path kernel with respect to the basis P. For any
↵ 2 Rp  it holds that Var(X(↵)) = ↵>⌃↵. Moreover  under the condition >↵ = 1  the variance
Var(X(↵)) is minimized by ↵ =⌃1  >⌃1 1 .
↵P LP (B) = XP2P

Proof: By inserting deﬁnitions  we obtain

X(↵) = XP2P

↵P Xe2Km n

ce P be.

Writing b = (be) 2 Rmn as vectors  and C = (ce P ) 2 Rp⇥mn as matrices  we obtain X(↵) =
b>C↵. By using that Var(·) = 2 Var(·) for any scalar   and independence of the be  a calculation
yields Var(X(↵)) = ↵>⌃↵. In order to determine the minimum of the variance in ↵  consider the
Lagrangian

L(↵  ) = ↵>⌃↵ +  1  XP2P

↵P!  

where the slack term models the condition >↵ = 1. An straightforward computation yields

@L
@↵

= 2⌃↵  

Due to positive deﬁniteness of ⌃ the function Var(X(↵)) is convex  thus ↵ = ⌃1 / >⌃1 will
⇤
be the unique ↵ minimizing the variance while satisfying >↵ = 1.
Remark 3.9. The above setup works in wider generality: (i) if Var(be) = 0 is allowed and there is
an s–t path of all zero variance edges  the path kernel becomes positive semi-deﬁnite; (ii) similarly
if P is replaced with any set of paths at all  the same may occur. In both cases  we may replace
⌃1 with the Moore-Penrose pseudo-inverse and the proposition still holds: (i) reduces to the exact
reconstruction case of Theorem 2.4; (ii) produces the optimal estimator with respect to P  which is
optimal provided that P is spanning  and adding paths to P does not make the estimate worse.
Our estimator is optimal over a fairly large class.
Theorem 3.10. Let bAij be any estimator for an entry Aij of the true matrix that is: (i) unbiased; (ii)
Let A⇤ij be the estimator from Proposition 3.8. Then Var(A⇤ij)  Var(bAij).

We give a complete proof in the full version. Here  we prove the special case of log-normal noise 
which gives an alternate viewpoint on the path kernel.

a deterministic piecewise smooth function of the observations; (iii) independent of the noise model.

5

Proof: As above  we work with the formal logarithm aij of Aij. For log-normal noise  the "e are
independently distributed normals with variance e. It then follows that  for any P in the i–j path
space 

LP (B) ⇠ N aij Xe2P

e!

and the kernel matrix ⌃ of the path kernel is the covariance matrix for the LP in our path basis.
Thus  the LP have distribution N (aij
  ⌃). It is well-known that any multivariate normal has a
linear repreameterization so that the coordinates are independent; a computation shows that  here 

⌃1  >⌃1 1 is the correct linear map. Thus  the estimator A⇤ij is the sample mean of the

coordinates in the new parameterization. Since this is a sufﬁcient statistic  we are done via the
⇤
Lehmann–Scheff´e Theorem.

3.3 Rank 2 and higher

An estimator for rank 2 and higher  together with a variance analysis  can be constructed similarly
once all the solving polynomials are known. The main difﬁculties lies in the fact that these polyno-
mials are not parameterized by cycles anymore  but speciﬁc subgraphs of G(M )  see [6  Section 2.5]
and that they are not necessarily linear in the missing entry Ae. However  even with approximate
oracles for evaluating these polynomials and estimating their covariances  an estimator similar to
X(↵) can be constructed and analyzed; in particular  we still need only to consider a basis for the
space of “circuits” through the missing entry and not a costly brute force enumeration.

3.4 The algorithms

We now give the algorithms for estimating/denoising entries and computing the variance bounds;
an implementation is available from [7]. Since the the path matrix C  the path kernel matrix ⌃ 
and the optimal ↵ are required for both  we show how to compute them ﬁrst. We can ﬁnd a basis

Algorithm 1 Calculates path kernel ⌃ and ↵.
Input: index (i  j)  an (m ⇥ n) mask M  variances .
Output: path matrix C  path kernel ⌃ and minimizer ↵.
1: Find a linearly independent set of paths P in the graph G(M )  starting from i and ending at j.
2: Determine the matrix C = (ce P ) with e 2 G(M )  P 2 P; set ce P = ±1 if e 2 E±(P ) 
3: Deﬁne a diagonal matrix S = diag()  with See = e for e 2 G(M ).
4: Compute the kernel matrix ⌃ = C>SC.
5: Calculate ↵ =⌃1  >⌃1 1 .

6: Output C  ⌃ and ↵.

otherwise ce P = 0.

for the path space in linear time. To keep the notation manageable  we will conﬂate formal sums
of the xe  cycles in H1(G  Z) and their representations as vectors in Rm. Correctness is proven in
Appendix A.

Output ;.

Algorithm 2 Calculates a basis P of the path space.
Input: index (i  j)  an (m ⇥ n) mask M.
Output: a basis P for the space of oriented i–j paths.
1: If (i  j) is not an edge of M  and i and j are in different connected components  then P is empty.
2: Otherwise  if (i  j) is not an edge  of M  add a “dummy” copy.
3: Compute a spanning forest F of M that does not contain (i  j)  if possible.
4: For each edge e 2 M \ F   compute the fundamental cycle Ce of e in F .
5: If (i  j) is an edge in M  output {x(i j)} [ {Ce  x(i j) : e 2 M \ F}.
6: Otherwise  let P(i j) = C(i j)  x(i j). Output {Ce  P(i j) : e 2 M \ (F [ {(i  j)})}.

6

Algorithms 3 and 4 then can make use of the calculated C  ↵  ⌃ to determine an estimate for any
entry Aij and its minimum variance bound. The algorithms follow the exposition in Section 3.2 
from where correctness follows; Algorithm 3 additionally provides treatment for the sign of the
entries.

Algorithm 3 Estimates the entry aij.
Input: index (i  j)  an (m ⇥ n) mask M  log-variances   the partially observed and noisy matrix
B.
Output: The variance-minimizing estimate for Aij.
1: Calculate C and ↵ with Algorithm 1.
2: Store B as a vector b = (log |Be|) and a sign vector s = (sgn Be) with e 2 G(M ).
3: Calculate bAij = ± expb>C↵ . The sign is + if each column of s>|C| (|.| component-wise)
contains an odd number of entries 1  else .
4: Return bAij.
Algorithm 4 Determines the variance of the entry log(Aij).
Input: index (i  j)  an (m ⇥ n) mask M  log-variances .
Output: The variance lower bound for log(Aij).
1: Calculate ⌃ and ↵ with Algorithm 1.
2: Return ↵>⌃↵.

Algorithm 4 can be used to obtain the variance bound independently of the observations. The
variance bound is relative  due to its multiplicativity  and can be used to approximate absolute
bounds when any (in particular not necessarily the one from Algorithm 3) reconstruction estimate

bAij is available. Namely  if bij is the estimated variance of the logarithm  we obtain an upper
conﬁdence/deviation bound bAij · exppbij for bAij  and a lower conﬁdence/deviation bound
bAij · exppbij  corresponding to the log-conﬁdence log bAij ±pbij. Also note that if Aij

is not reconstructible from the mask M  then the deviation bounds will be inﬁnite.

4 Experiments

4.1 Universal error estimates

For three different masks  we calculated the predicted minimum variance for each entry of the mask.
The mask sizes are all 140⇥ 140. The multiplicative noise was assumed to be e = 1 for each entry.
Figure 1 shows the predicted a-priori minimum variances for each of the masks. The structure of
the mask affects the expected error. Known entries generally have least variance  and it is less than
the initial variance of 1  which implies that the (independent) estimates coming from other paths can
be used to successfully denoise observed data. For unknown entries  the structure of the mask is
mirrored in the pattern of the predicted errors; a diffuse mask gives a similar error on each missing
entry  while the more structured masks have structured error which is determined by combinatorial
properties of the completion graph.

 

3

2.5

2

1.5

1

 

 

 

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 

70

60

50

40

30

20

10

 

Figure 1: The ﬁgure shows three pairs of masks and predicted variances. A pair consists of two
adjacent squares. The left half is the mask which is depicted by red/blue heatmap with red entries
known and blue unknown. The right half is a multicolor heatmap with color scale  showing the pre-
dicted variance of the completion. Variances were calculated by our implementation of Algorithm 4.

7

E
S
M

45

40

35

30

25

20

15

10

5

0

 

Path Kernel
NN
OptSpace

 

r
o
r
r
e

 

d
e
v
r
e
s
b
o
n
a
e
m
e

 

 

 

PathKernel
NN
OptSpace

6

5

4

3

0

.1

.2

.3

.4

.5

Noise Level

.6

.7

.8

.9

l
i
t

n
a
u
q

2

1

 
0
−0.2

0

0.2

0.4
0.6
predicted variance

0.8

1

1.2

(a) mean squared errors

(b) error vs. predicted variance

Figure 2: For 10 randomly chosen masks and 50 ⇥ 50 true matrix  matrix completions were per-
formed with Nuclear Norm (green)  OptSpace (red)  and Algorithm 3 (blue) under multiplicative
noise with variance increasing in increments of 0.1. For each completed entry  minimum variances
were predicted by Algorithm 4. 2(a) shows the mean squared error of the three algorithms for each
noise level  coded by the algorithms’ respective colors. 2(b) shows a bin-plot of errors (y-axis) ver-
sus predicted variances (x-axis) for each of the three algorithms: for each completed entry  a pair
(predicted error  true error) was calculated  predicted error being the predicted variance  and the
actual prediction error being the squared logarithmic error (i.e.  (log |atrue|  log |apredicted|)2 for
an entry a). Then  the points were binned into 11 bins with equal numbers of points. The ﬁgure
shows the mean of the errors (second coordinate) of the value pairs with predicted variance (ﬁrst
coordinate) in each of the bins  the color corresponds to the particular algorithm; each group of bars
is centered on the minimum value of the associated bin.

Inﬂuence of noise level

4.2
We generated 10 random mask of size 50 ⇥ 50 with 200 entries sampled uniformly and a random
(50 ⇥ 50) matrix of rank one. The multiplicative noise was chosen entry-wise independent  with
variance i = (i  1)/10 for each entry. Figure 2(a) compares the Mean Squared Error (MSE) for
three algorithms: Nuclear Norm (using the implementation Tomioka et al. [10])  OptSpace [5]  and
Algorithm 3. It can be seen that on this particular mask  Algorithm 3 is competitive with the other
methods and even outperforms them for low noise.

4.3 Prediction of estimation errors

The data are the same as in Section 4.2  as are the compared algorithm. Figure 2(b) compares the
error of each of the methods with the variance predicted by Algorithm 4 each time the noise level
changed. The ﬁgure shows that for any of the algorithms  the mean of the actual error increases
with the predicted error  showing that the error estimate is useful for a-priori prediction of the actual
error - independently of the particular algorithm. Note that by construction of the data this statement
holds in particular for entry-wise predictions. Furthermore  in quantitative comparison Algorithm 4
also outperforms the other two in each of the bins. The qualitative reversal between the algorithms
in Figures 2(b) (a) and (b) comes from the different error measure and the conditioning on the bins.

5 Conclusion

In this paper  we have introduced an algebraic combinatorics based method for reconstructing and
denoising single entries of an incomplete and noisy matrix  and for calculating conﬁdence bounds
of single entry estimations for arbitrary algorithms. We have evaluated these methods against state-
of-the art matrix completion methods. Our method is competitive and yields the ﬁrst known a
priori variance bounds for reconstruction. These bounds coarsely predict the performance of all the
methods. Furthermore  our method can reconstruct and estimate the error for single entries. It can
be restricted to using only a small number of nearby observations and smoothly improves as more
information is added  making it attractive for applications on large scale data. These results are an
instance of a general algebraic-combinatorial scheme and viewpoint that we argue is crucial for the
future understanding and practical treatment of big data.

8

References
[1] E. Acar  D. Dunlavy  and T. Kolda. Link prediction on evolving data using matrix and tensor factoriza-
tions. In Data Mining Workshops  2009. ICDMW’09. IEEE International Conference on  pages 262–269.
IEEE  2009.

[2] A. Argyriou  C. A. Micchelli  M. Pontil  and Y. Ying. A spectral regularization framework for multi-task
structure learning. In J. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in NIPS 20  pages
25–32. MIT Press  Cambridge  MA  2008.

[3] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math. 
9(6):717–772  2009. ISSN 1615-3375. doi: 10.1007/s10208-009-9045-5. URL http://dx.doi.
org/10.1007/s10208-009-9045-5.

[4] A. Goldberg  X. Zhu  B. Recht  J. Xu  and R. Nowak. Transduction with matrix completion: Three birds
In J. Lafferty  C. K. I. Williams  J. Shawe-Taylor  R. Zemel  and A. Culotta  editors 

with one stone.
Advances in Neural Information Processing Systems 23  pages 757–765. 2010.

[5] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. IEEE Trans. Inform.
ISSN 0018-9448. doi: 10.1109/TIT.2010.2046205. URL http:

Theory  56(6):2980–2998  2010.
//dx.doi.org/10.1109/TIT.2010.2046205.

[6] F. J. Kir´aly  L. Theran  R. Tomioka  and T. Uno. The algebraic combinatorial approach for low-rank matrix

completion. Preprint  arXiv:1211.4116v4  2012. URL http://arxiv.org/abs/1211.4116.
[7] F. J. Kir´aly and L. Theran. AlCoCoMa  2013. http://mloss.org/software/view/524/.
[8] A. Menon and C. Elkan. Link prediction via matrix factorization. Machine Learning and Knowledge

Discovery in Databases  pages 437–452  2011.

[9] N. Srebro  J. D. M. Rennie  and T. S. Jaakkola. Maximum-margin matrix factorization. In L. K. Saul 
Y. Weiss  and L. Bottou  editors  Advances in NIPS 17  pages 1329–1336. MIT Press  Cambridge  MA 
2005.

[10] R. Tomioka  K. Hayashi  and H. Kashima. On the extension of trace norm to tensors. In NIPS Workshop

on Tensors  Kernels  and Machine Learning  2010.

9

,Franz Kiraly
Louis Theran
Xi Chen
Yu Cheng
Bo Tang
Liangpeng Zhang
Ke Tang
Xin Yao