2008,Learning with Consistency between Inductive Functions and Kernels,Regularized Least Squares (RLS) algorithms have the ability to avoid over-fitting problems and to express solutions as kernel expansions. However  we observe that the current RLS algorithms cannot provide a satisfactory interpretation even on a constant function. On the other hand  while kernel-based algorithms have been developed in such a tendency that almost all learning algorithms are kernelized or being kernelized  a basic fact is often ignored: The learned function from the data and the kernel fits the data well  but may not be consistent with the kernel. Based on these considerations and on the intuition that a good kernel-based inductive function should be consistent with both the data and the kernel  a novel learning scheme is proposed. The advantages of this scheme lie in its corresponding Representer Theorem  its strong interpretation ability about what kind of functions should not be penalized  and its promising accuracy improvements shown in a number of experiments. Furthermore  we provide a detailed technical description about heat kernels  which serves as an example for the readers to apply similar techniques for other kernels. Our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions.,Learning with Consistency between Inductive

Functions and Kernels

Haixuan Yang1 2

Irwin King1

Michael R. Lyu1

1Department of Computer Science & Engineering

The Chinese University of Hong Kong

{hxyang king lyu}@cse.cuhk.edu.hk

2Department of Computer Science

Royal Holloway University of London

haixuan@cs.rhul.ac.hk

Abstract

Regularized Least Squares (RLS) algorithms have the ability to avoid over-ﬁtting
problems and to express solutions as kernel expansions. However  we observe
that the current RLS algorithms cannot provide a satisfactory interpretation even
on the penalty of a constant function. Based on the intuition that a good kernel-
based inductive function should be consistent with both the data and the kernel  a
novel learning scheme is proposed. The advantages of this scheme lie in its cor-
responding Representer Theorem  its strong interpretation ability about what kind
of functions should not be penalized  and its promising accuracy improvements
shown in a number of experiments. Furthermore  we provide a detailed techni-
cal description about heat kernels  which serves as an example for the readers to
apply similar techniques for other kernels. Our work provides a preliminary step
in a new direction to explore the varying consistency between inductive functions
and kernels under various distributions.

1 Introduction

Regularized Least Squares (RLS) algorithms have been drawing people’s attention since they were
proposed due to their ability to avoid over-ﬁtting problems and to express solutions as kernel ex-
pansions in terms of the training data [4  9  12  13]. Various modiﬁcations of RLS are made to
improve its performance either from the viewpoint of manifold [1] or in a more generalized form
[7  11]. However  despite these modiﬁcations  problems still remain. We observe that the previous
RLS-related work has the following problem:
Over Penalization. For a constant function f = c  a nonzero term ||f ||K is penalized in both
RLS and LapRLS [1]. As a result  for a distribution generalized by a nonzero constant function 
the resulting regression function by both RLS and LapRLS is not a constant as illustrated in the left
diagram in Fig. 1. For such situations  there is an over-penalization.

In this work  we aim to provide a new viewpoint for supervised or semi-supervised learning prob-
lems. By such a viewpoint we can provide a general condition under which constant functions
should not be penalized. The basic idea is that  if a learning algorithm can learn an inductive func-
tion f (x) from examples generated by a joint probability distribution P on X × R  then the learned
function f (x) and the marginal PX represents a new distribution on X × R  from which there is a
re-learned function r(x). The re-learned function should be consistent with the learned function in
the sense that the expected difference on distribution PX is small. Because the re-learned function
depends on the underlying kernel  the difference f (x) − r(x) depends on f (x) and the kernel  and
from this point of view  we name this work.

RLS

The Re−learned function and the Residual

1.02

1.01

1

y

0.99

0.98

0.97

0.96

0

1

0.5

y

0

−0.5

1

−1

−2

0
x

2

The Ideal Function
Labeled Data
RLS−γ=0.1
RLS−γ=0.01
RLS−γ
RLS−γ=0.005

A=0

0.5
x

1.003

1.002

f(x)
r(x)
f(x)−r(x)

1.001

y

1

0.999

0.998

0

RLS vs PRLS

RLS−γ=0.005
PRLS−γ=1000
PRLS−γ=1
PRLS−γ=0.001
PRLS−γ=0

0.5
x

1

Figure 1: Illustration for over penalization. Left diagram: The training set contains 20 points  whose
x is randomly drawn from the interval [0 1]  whereas the test set contains another 20 points  and
y is generated by 1 + 0.005ε  ε ∼ N (0  1). The over penalized constant functions in the term
||f ||K cause the phenomena that smaller γ can achieve better results. On the other hand  the over-
ﬁtting phenomenon when γ = 0 suggests the necessity of the regularization term. Based on these
observations  an appropriate penalization on a function is expected. Middle diagram: r(x) is very
smooth  and f (x)−r(x) remains the uneven part of f (x); therefore f (x)−r(x) should be penalized
while f is over penalized in ||f ||K. Right diagram: the proposed model has a stable property so that
a large variant of γ results in small changes of the curves  suggesting a right way of penalizing
functions.

2 Background

The RKHS Theory enables us to express solutions of RLS as kernel expansions in terms of the
training data. Here we give a brief description of the concepts. For a complete discussion  see [2].
Let X be a compact domain or manifold  ν be a Borel measure on X  and K : X × X → R be
a Mercer kernel  then there is an associated Hilbert space RKHS HK of functions X → R with
the corresponding norm || · ||K. HK satisﬁes the reproducing property  i.e.  for all f ∈ HK 
f (x) = hKx  f i  where Kx is the function K(x  ·). Moreover  an operator LK can be deﬁned on
ν(X) is the Hilbert space of square integrable

HK as: (LKf )(x) = RX f (y)K(x  y)dν(y)  where L2
functions on X with the scalar product hf  giν = RX f (x)g(x)dν(x).

Given a Mercer kernel and a set of labeled examples (xi  yi) (i = 1  ...  l)  there are two popular
inductive learning algorithms: RLS [12  13] and the Nadaraya-Watson Formula [5  8  14]. By the
standard Tikhonov regularization  RLS is a special case of the following functional extreme problem:

f ∗ = arg min
f ∈HK

1
l

l

Xi=1

V (xi  yi  f ) + γ||f ||2
K

(1)

where V is some loss function.
The Classical Representer Theorem states that the solution to this minimization problem exists in
HK and can be written as

f ∗(x) =

l

Xi=1

αiK(xi  x).

(2)

Such a Representer Theorem is general because it plays an important role in both RLS in the case
when V (x  y  f ) = (y − f (x))2  and SVM in the case when V (x  y  f ) = max(0  1 − yf (x)).

The Nadaraya-Watson Formula is based on local weighted averaging  and it comes with a closed
form:

l

l

r(x) =

Xi=1

yiK(x  xi)/

Xi=1

K(x  xi).

(3)

The formula has a similar appearance as Eq. (2)  but it plays an important role in this paper because
we can write it in an integral form which makes our idea technically feasible as follows. Let p(x) be
a probability density function over X  P (x) be the corresponding cumulative distribution function 
and f (x) be an inductive function. We observe that  if (xi  f (xi))(i = 1  2  . . .   l) are sampled from
the function y = f (x)  then
A Re-learned Function can be expressed as

r(x) = lim

l→∞ Pl
Pl

i=1 f (xi)K(x  xi)

i=1 K(x  xi)

= RX f (α)K(x  α)dP (α)
RX K(x  α)dP (α)

=

LK(f )

RX K(x  α)dP (α)

 

(4)

based on f (x) and P (x). From this form  we show two points: (1) If r(x) = f (x)  then f (x) is
completely predicted by itself through the Nadaraya-Watson Formula  and so f (x) is considered
to be completely consistent with the kernel K(x  y); if r(x) 6= f (x)  then the difference ||f (x) −
r(x)||K can measure how badly f (x) is consistent with the kernel K(x  y) and (2) Intuitively r(x)
can also be understood as the smoothed function of f (x) through a kernel K. Consequently  f (x) −
r(x) represents the intrinsically uneven part of f (x)  which we will penalize. This intuition is
illustrated in the middle diagram in Fig. 1.

Throughout this paper  we assume thatRX K(x  α)dP (α) is a constant  and for simplicity all kernels
are normalized by K/RX K(x  α)dP (α) so that r(x) = LK(f ). Moreover  we assume that X is

compact  and the measure ν is speciﬁed as P (x).

3 Partially-penalized Regularization

For a given kernel K and an inductive function f  LK(f ) is the prediction function produced by K
through the Nadaraya-Watson Formula. Based on Eq. (1)  penalizing the inconsistent part f (x) −
LK(f ) leads to the following Partially-penalized Regularization problem:

f ∗ = arg min
f ∈HK

1
l

l

Xi=1

V (xi  yi  f ) + γ||f − LK(f )||2

K .

(5)

To obtain a Representer Theorem  we need one assumption.

Assumption 1 Let f1  f2 ∈ HK. If hf1  f2iK = 0  then ||f1 − LK(f1) + f2 − LK(f2)||2
||f1 − LK(f1)||2

K + ||f2 − LK(f2)||2

K .

K =

It is well-known that the operator LK is compact  self-adjoint  and positive with respect to L2
ν(X) 
and by the Spectral Theorem [2  3]  its eigenfunctions e1(x)  e2(x)  . . . form an orthogonal basis of
ν(X) and the corresponding eigenvalues λ1 ≥ λ2  . . . are either ﬁnitely many that are nonzero 
L2
or there are inﬁnitely many  in which case λk → 0. Let f1 = Pi aiei(x)  f2 = Pi biei(x)  then
f1−LK(f1) = Pi aiei(x)−LK(Pi aiei(x)) = Pi aiei(x)−Pi λiaiei(x) = Pi(1−λi)aiei(x) 
and similarly  f2 − LK(f2) = Pi(1 − λi)biei(x). By the discussions in [1]  we have hei  ejiν = 0
hf1 − LK(f1)  f2 − LK(f2)iK = Pi(1 − λi)2aibihei(x)  ei(x)iK = 0. Therefore  under some

if i 6= j  and hei  eiiν = 1; hei  ejiK = 0 if i 6= j  and hei  eiiK = 1
. If we consider the situation
λi
that ai  bi ≥ 0 for all i ≥ 1  then hf1  f2iK = 0 implies that aibi = 0 for all i ≥ 1  and consequently

constrains  this assumption is a fact. Under this assumption  we have a Representer Theorem.

Theorem 2 Let µj(x) be a basis in H0 of the operator I − LK  i.e.  H0 = {f ∈ HK|f − LK(f ) =
0}. Under Assumption 1  the minimizer of the optimization problem in Eq. (5) is

f ∗(x) =

o

Xj=1

βjµj(x) +

l

Xi=1

αiK(xi  x)

(6)

Proof of the Representer Theorem. Any function f ∈ HK can be uniquely decomposed into a
component f|| in the linear subspace spanned by the kernel functions {K(xi  ·)}l
i=1  and a compo-

nent f⊥ orthogonal to it. Thus  f = f|| + f⊥ =
and the fact that hf⊥  K(xi  ·)i = 0 for 1 ≤ i ≤ l  we have

Pi=1

l

αiK(xi  ·) + f⊥. By the reproducing property

f (xj) = hf  K(xj  ·)i = h

Xi=1

αiK(xi  ·)  K(xj  ·)i + hf⊥  K(xj  ·)i = h

Xi=1

l

l

αiK(xi  ·)  K(xj  ·)i.

Thus the empirical terms involving the loss function in Eq. (5) depend only on the value of the
coefﬁcients {αi}l

i=1 and the gram matrix of the kernel function. By Assumption 1  we have

||f − LK(f )||2

K = ||

≥ ||

l

l

Pi=1
Pi=1

αiK(xi  ·) − LK(

αiK(xi  ·) − LK(

l

l

Pi=1
Pi=1

αiK(xi  ·))||2

K + ||f⊥ − LK(f⊥)||2
K

αiK(xi  ·))||2

K .

It follows that the minimizer of Eq. (5) must have ||f⊥ − LK(f⊥)||2
Pi=1

representation f ∗(x) = f⊥ +

αiK(xi  x) =

βjµj(x) +

Pj=1

Pi=1

o

l

l

K = 0  and therefore admits a
αiK(xi  x).

3.1 Partially-penalized Regularized Least Squares (PRLS) Algorithm

In this section  we focus our attention in the case that V (xi  yi  f ) = (yi − f (xi))2  i.e  the Regu-
larized Least Squares algorithm. In our setting  we aim to solve:

min
f ∈HK

1

l X(yi − f (xi))2 + γ||f − LK(f )||2

K .

By the Representer Theorem  the solution to Eq. (7) is of the following form:

f ∗(x) =

o

Xj=1

βjµj(x) +

l

Xi=1

αiK(xi  x).

(7)

(8)

By the proof of Theorem 2  we have f⊥ =

βjµj(x) and hf⊥ 

αiK(xi  x)iK = 0. By

Assumption 1 and the fact that f⊥ belongs to the null space H0 of the operator I − LK  we have

o

Pj=1

l

Pi=1

= ||Pl

i=1 αiK(xi  x) −Pl

||f ∗ − LK(f ∗)||2

K = ||f⊥ − LK(f⊥)||2

K + ||Pl

i=1 αiK(xi  x) − LK(Pl
K = αT (K − 2K 0 + K 00)α 

i=1 αiLK(K(xi  x))||2

i=1 αiK(xi  x))||2
K

(9)
where α = [α1  α2  . . .   αl]T   K is the l × l gram matrix Kij = K(xi  xj)  K 0 and
K 00 are reconstructed l × l matrices K 0
ij =
hLK(K(xi  x))  LK (K(xj  x))iK. Substituting Eq. (8) and Eq. (9) to the problem in Eq. (7)  we ar-
rive at the following quadratic objective function of the l-dimensional variable α and o-dimensional
variable β = [β1  β2  . . .   βo]T :

ij = hK(xi  x)  LK (K(xj  x))iK  and K 00

[α∗  β∗] = arg min

1
l

(Y − Kα − Ψβ)T (Y − Kα − Ψβ) + γαT (K − 2K 0 + K 00)α 

(10)

where Ψ is an l × o matrix Ψij = µj(xi)  and Y = [y1  y2  . . .   yl]T . Taking derivatives with respect
to α and β  since the derivative of the objective function vanishes at the minimizer  we obtain

(γl(K − 2K 0 + K 00) + K 2)α + KΨβ = KY  ΨT (Y − Kα − Ψβ) = 0.

(11)

In the term ||f −LK (f )||  f is subtracted by LK(f )  and so it partially penalized. For this reason  the
resulting algorithm is referred as Partially-penalized Regularized Least Squares algorithm (PRLS).

3.2 The PLapRLS Algorithm

The idea in the previous section can also be extended to LapRLS in the manifold regularization
framework [1].
In the manifold setting  the smoothness on the data adjacency graph should be
considered  and Eq. (5) is modiﬁed as

f ∗ = arg min
f ∈HK

1
l

l

Xi=1

V (xi  yi  f )+γA||f −LK (f )||2

K +

γI

(u + l)2

l+u

(f (xi)−f (xj))2Wij  (12)

Xi j=1

where Wij are edge weights in the data adjacency. From W   the graph Laplacian L is given by
j=1 Wij. For this optimization problem 

L = D − W   where D is the diagonal matrix with Dii = Pl+u

the result in Theorem 2 can be modiﬁed slightly as:

Theorem 3 Under Assumption 1  the minimizer of the optimization problem in Eq. (12) admits an
expansion

f ∗(x) =

o

Xj=1

βjµj(x) +

l+u

Xi=1

αiK(xi  x).

(13)

Following Eq.
[α1  α2  . . .   αl+u]α and the o-dimensional variable β = [β1  β2  . . .   βo]T .
the previous section and LapRLS in [1]  α and β are determined by the following linear systems:

(13)  we continue to optimize the (l + u)-dimensional variable α =
In a similar way as

(cid:26) (KJK + λ1(K − 2K 0 + K 00) + λ2KLK)α + (KJΨ + λ2KLΨ)β = KJY 

(Ψ0JK − λ2Ψ0LK)α + (Ψ0Ψ − λ2Ψ0LΨ)β = Ψ0 ∗ Y 

(14)

where K  K 0  K 00 are the (l + u) × (l + u) Gram matrices over labeled and unlabeled points; Y is an
(l + u) dimensional label vector given by: Y = [y1  y2  . . .   yl  0  . . .   0]  J is an (l + u) × (l + u)
diagonal matrix given by J = diag(1  1  . . .   1  0  . . .   0) with the ﬁrst l diagonal entries as 1 and the
rest 0  and Ψ is an (l + u) × o matrix Ψij = µj(xi).

4 Discussions

4.1 Heat Kernels and the Computation of K 0 and K 00

In this section we will illustrate the computation of K 0 and K 00 in the case of heat kernels. The basic
facts about heat kernels are excerpted from [6]  and for more materials  see [10].
Given a manifold M and points x and y  the heat kernel Kt(x  y) is a special solution to the heat
equation with a special initial condition called the delta function δ(x−y). More speciﬁcally  δ(x−y)
describes a unit heat source at position y with no heat in other positions. Namely  δ(x − y) = 0 for
−∞ δ(x − y)dx = 1. If we let f0(x  0) = δ(x − y)  then Kt(x  y) is a solution to the

x 6= y and R +∞

following differential equation on a manifold M:

∂f
∂t

− Lf = 0  f (x  0) = f0(x) 

(15)

where f (x  t) is the temperature at location x at time t  beginning with an initial distribution f0(x) at
time zero  and L is the Laplace-Beltrami operator. Equation (15) describes the heat ﬂow throughout
a geometric manifold with initial conditions.

Theorem 4 Let M be a complete Riemannian manifold. Then there exists a function K ∈
C∞(R+ × M × M)  called the heat kernel  which satisﬁes the following properties for
all x  y ∈ M  with Kt(x  y) = K(t  x  y):
(2)

(1) Kt(x  y) deﬁnes a Mercer kernel.

Kt(x  y) = RM Kt−s(x  z)Ks(z  y)dz for any s > 0. (3) The solution to Eq. (15) is f (x  t) =
(4) 1 = RM Kt(x  y)1dy and (5) When M = Rm  Lf is simpliﬁed as
RM Kt(x  y)f0(y)dy.
Pi

  and the heat kernel takes the Gaussian RBF form Kt(x  y) = (4πt)− m

2 e− ||x−y||2

∂ 2f
∂x2
i

4t

.

K 0 and K 00 can be computed as follows:

K 0

ij = hKt(xi  x)  LK (Kt(xj  x))iK (by deﬁnition)

= LK(Kt(xj  x))|x=xi (by the reproducing property of a Mercer kernel)
= RX Kt(xj  y)Kt(xi  y)dν(y) (by the deﬁnition of LK)

= K2t(xi  xj) (by Property 2 in Theorem 4)

(16)

Based on the fact that LK is self-adjoint  we can similarly derive K 00
kernels  K 0 and K 00 can also be computed.

ij = K3t(xi  xj). For other

4.2 What should not be penalized?

From Theorem 2  we know that the functions in the null space H0 = {f ∈ HK|f − LK(f ) =
0} should not be penalized. Although there may be looser assumptions that can guarantee the
validity of the result in Theorem 2  there are two assumptions in this work: X is compact and

RX K(x  α)dP (α) in Eq. (4) is a constant. Next we discuss the constant functions and the linear

functions.
Should constant functions be penalized? Under the two assumptions  a constant function c should

not be penalized  because c = RX cK(x  α)p(α)dα/RX K(x  α)p(α)dα  i.e.  c ∈ H0. For heat
kernels  if P (x) is uniformly distributed on M  then by Property 4 in Theorem 4 RX K(x  α)dP (α)

is a constant  and so c should not be penalized.
For polynomial kernels  the theory cannot guarantee that constant functions should not be penalized
even with a uniform distribution P (x). For example  considering the polynomial kernel xy +1 in the
0 (xy +1)dy = x/2+1
is not a constant. As a counter example  we will show in Section 5.3 that not penalizing constant
functions in polynomial kernels will result in much worse accuracy. The reason for this phenomenon
is that constant functions may not be smooth in the feature space produced by the polynomial kernel
0 (xy + 1)dP (y)

interval X = [0 1] and the uniform distribution on X RX (xy +1)dP (y) = R 1

under some distributions. The readers can deduce an example for p(x) such that R 1

happens to be a constant.
Should linear function aT x be penalized? In the case when X is a closed ball Br with radius
r when P (x) is uniformly distributed over Br and when K is the Gaussian RBF kernel  then aT x
·dx
Kt(x  y)aT ydy ≈ LK(aT x).
Consequently ||aT x − LK(aT x)||K will be small enough  and so the linear function aT x needs not
be penalized. For other kernels  other spaces  or other PX  the conclusion may not be true.

should not be penalized when r is big enough. 1 Since r is big enough  we have RRn ·dx ≈ RBr
and RBr

Kt(x  y)dy ≈ 1  and so aT x = RRn Kt(x  y)aT ydy ≈ RBr

5 Experiments

In this section  we evaluate the proposed algorithms PRLS and PLapRLS on a toy dataset (size: 40) 
a medium-sized dataset (size: 3 119)  and a large-sized dataset (size: 20 000)  and provide a counter
example for constant functions on another dataset (size: 9 298). We use the Gaussian RBF kernels in
the ﬁrst three datasets  and use polynomial kernels to provide a counter example on the last dataset.
Without any prior knowledge about the data distribution  we assume that the examples are uniformly
distributed  and so constant functions are considered to be in H0 for the Gaussian RBF kernel  but
linear functions are not considered to be in H0 since it is rare for data to be distributed uniformly on
a large ball. The data and results for the toy dataset are illustrated in the left diagram and the right
diagram in Fig. 1.

5.1 UCI Dataset Isolet about Spoken Letter Recognition

We follow the same semi-supervised settings as that in [1] to compare RLS with PRLS  and compare
LapRLS with PLapRLS on the Isolet database. The dataset contains utterances of 150 subjects who

1Note that a subset of Rn is compact if and only if it is closed and bounded. Since Rn is not bounded  it
is not compact  and so the Representer Theorem cannot be established. This is the reason why we cannot talk
about Rn directly.

RLS vs PRLS

LapRLS vs PLapRLS

LapRLS
PLapRLS

25

20

15

l

)
t
e
s
 
d
e
e
b
a
n
u
(
 
s
e
t
a
R

l

 
r
o
r
r

5

10

15

20

25

30

Labeled Speaker #

LapRLS vs PLapRLS

LapRLS
PLapRLS

l

)
t
e
s
 
d
e
e
b
a
n
u
(
 
e
t
a
R

l

 
r
o
r
r

E

)
t
e
s
 
t
s
e
t
(
 
s
e
t
a
R

 
r
o
r
r

E

RLS
PRLS

0

5

10

15

20

25

30

Labeled Speaker #

RLS vs PPLS

RLS
PRLS

28

26

24

22

20

18

16

14

12

10

35

30

25

20

15

0

5

10

15

20

25

30

Labeled Speaker #

E

10

0

)
t
e
s
 
t
s
e
t
(
 
s
e
t
a
R

 
r
o
r
r

E

32

30

28

26

24

22

20

18

16

14

0

5

10

15

20

25

30

Labeled Speaker #

Figure 2: Isolet Experiment

pronounced the name of each letter of the English alphabet twice. The speakers were grouped into
5 sets of 30 speakers each. The data of the ﬁrst 30 speakers forms a training set of 1 560 examples 
and that of the last 29 speakers forms the test set. The task is to distinguish the ﬁrst 13 letters from
the last 13. To simulate a real-world situation  30 binary classiﬁcation problems corresponding to 30
splits of the training data where all 52 utterances of one speaker were labeled and all the rest were
left unlabeled. All the algorithms use Gaussian RBF kernels. For RLS and LapRLS  the results were
obtained with width σ = 10  γl = 0.05  γAl = γI l/(u + l)2 = 0.005. For PRLS and PLapRLS 
the results were obtained with width σ = 4  γl = 0.01  and γAl = γI l/(u + l)2 = 0.01. In Fig. 2 
we can see that both PRLS and PLapRLS make signiﬁcant performance improvements over their
corresponding counterparts on both unlabeled data and test set.

5.2 UCI Dataset Letter about Printed Letter Recognition

In Dataset Letter  there are 16 features for each example  and there are 26 classes representing the
upper case printed letters. The ﬁrst 400 examples were taken to form the training set. The remaining
19 600 examples form the test set. The parameters are set as follows: σ = 1  γl = γA(l+u) = 0.25 
and γI l/(u + l)2 = 0.05. For each of the four algorithms RLS  PRLS  LapRLS  and PLapRLS  for
each of the 26 one-versus-all binary classiﬁcation tasks  and for each of 10 runs  two examples for
each class were randomly labeled. For each algorithm  the averages over all the 260 one-versus-all
binary classiﬁcation error rates for unlabeled 398 examples and test set are listed respectively as
follows: (5.79%  5.23%) for RLS  (5.12%  4.77%) for PRLS  (0%  2.96%) for LapRLS  and (0% 
3.15%) for PLapRLS respectively. From the results  we can see that RLS is improved on both
unlabeled examples and test set. The fact that there is no error in the total 260 tasks for LapRLS
and PLapRLS on unlabeled examples suggests that the data is distributed in a curved manifold. On
a curved manifold  the heat kernels do not take the Gaussian RBF form  and so PLapRLS using the
Gaussian RBF form cannot achieve its best. This is the reason why we can observe that PLapRLS
is slightly worse than LapRLS on the test set. This suggests the need for a vast of investigations on
heat kernels on a manifold.

5.3 A Counter Example in Handwritten Digit Recognition

Note that  polynomial kernels with degree 3 were used on USPS dataset in [1]  and 2 images for each
class were randomly labeled. We follow the same experimental setting as that in [1]. For RLS  if we

use Eq. (2)  then the averages of 45 pairwise binary classiﬁcation error rates are 8.83% and 8.41%
for unlabeled 398 images and 8 898 images in the test set respectively. If constant functions are not
i=1 αiK(xi  x) + a  and the corresponding error rates are
9.75% and 9.09% respectively. By this example  we show that leaving constant functions outside
the regularization term is dangerous; however  it is fortunate that we have a theory to guide this in

penalized  then we should use f ∗(x) = Pl
Section 4: if X is compact and RX K(x  α)dP (α) in Eq. (4) is a constant  then constant functions

should not be penalized.

6 Conclusion

A novel learning scheme is proposed based on a new viewpoint of penalizing the inconsistent part
between inductive functions and kernels. In theoretical aspects  we have three important claims: (1)
On a compact domain or manifold  if the denominator in Eq. (4) is a constant  then there is a new
Representer Theorem; (2) The same conditions become a sufﬁcient condition under which constant
functions should not be penalized; and (3) under the same conditions  a function belongs to the
null space if and only if the function should not be penalized. Empirically  we claim that the novel
learning scheme can achieve accuracy improvement in practical applications.

Acknowledgments

The work described in this paper was supported by two grants from the Research Grants Council of
the Hong Kong Special Administrative Region  China (Project No. CUHK4150/07E) and Project
No. CUHK4235/04E). The ﬁrst author would like to thank Hao Ma for his helpful suggestions 
thank Kun Zhang and Wenye Li for useful discussions  and thank Alberto Paccanaro for his support.

References

[1] GMikhail Belkin  Partha Niyogi  and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of Machine Learning
Research  7:2399–2434  2006.

[2] F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin (New Series)

of the American Mathematical Society  39(1):1–49  2002.

[3] Lokenath Debnath and Piotr Mikusinski.

Introduction to Hilbert Spaces with Applications.

Academic Press  San Diego  second edition  1999.

[4] T. Evgeniou  M. Pontil  and T. Poggio. Regularization networks and support vector machines.

Advances in Computational Mathematics  13:1–50  2000.

[5] T. Hastie and C. Loader. Local regression: Automatic kernel carpentry. Statistical Science 

8(1):120–129  1993.

[6] John Lafferty and Guy Lebanon. Diffusion kernels on statistical manifolds. Journal of Machine

Learning Research  6:129–163  2005.

[7] Wenye Li  Kin-Hong Lee  and Kwong-Sak Leung. Generalized regularized least-squares learn-

ing with predeﬁned features in a Hilbert space. In NIPS  2006.

[8] E. A. Nadaraya. On estimating regression. Theory of Probability and Its Applications 

9(1):141–142  1964.

[9] R.M. Rifkin and R.A. Lippert. Notes on regularized least-squares. Technical Report 2007-019 

Massachusetts Institute of Technology  2007.

[10] S. Rosenberg. The Laplacian on a Riemmannian Manifold. Cambridge University Press  1997.
[11] Bernhard Sch¨olkopf  Ralf Herbrich  and Alex J. Smola. A generalized representer theorem. In

COLT  2001.

[12] I. Sch¨onberg. Spline functions and the problem of graduation. Proc. Nat. Acad. Sci. USA 

52:947–950  1964.

[13] A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. W. H. Winston  1977.
[14] G. S. Watson. Smooth regression analysis. Sankhy´a  Series A  26:359–372  1964.

,Alexandra Carpentier
Michal Valko