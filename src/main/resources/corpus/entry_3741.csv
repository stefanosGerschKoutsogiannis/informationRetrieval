2018,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,At initialization  artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit  thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN  the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object  which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training  in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.

We then focus on the setting of least-squares regression and show that in the infinite-width limit  the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK  hence suggesting a theoretical motivation for early stopping.

Finally we study the NTK numerically  observe its behavior for wide networks  and compare it to the infinite-width limit.,Neural Tangent Kernel:

Convergence and Generalization in Neural Networks

Arthur Jacot

´Ecole Polytechnique F´ed´erale de Lausanne

arthur.jacot@netopera.net

Imperial College London and ´Ecole Polytechnique F´ed´erale de Lausanne

Franck Gabriel

franckrgabriel@gmail.com

Cl´ement Hongler

´Ecole Polytechnique F´ed´erale de Lausanne

clement.hongler@gmail.com

Abstract

At initialization  artiﬁcial neural networks (ANNs) are equivalent to Gaussian
processes in the inﬁnite-width limit (12; 9)  thus connecting them to kernel methods.
We prove that the evolution of an ANN during training can also be described by a
kernel: during gradient descent on the parameters of an ANN  the network function
fθ (which maps input vectors to output vectors) follows the kernel gradient of the
functional cost (which is convex  in contrast to the parameter cost) w.r.t. a new
kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the
generalization features of ANNs. While the NTK is random at initialization and
varies during training  in the inﬁnite-width limit it converges to an explicit limiting
kernel and it stays constant during training. This makes it possible to study the
training of ANNs in function space instead of parameter space. Convergence of
the training can then be related to the positive-deﬁniteness of the limiting NTK.
We then focus on the setting of least-squares regression and show that in the inﬁnite-
width limit  the network function fθ follows a linear differential equation during
training. The convergence is fastest along the largest kernel principal components
of the input data with respect to the NTK  hence suggesting a theoretical motivation
for early stopping.
Finally we study the NTK numerically  observe its behavior for wide networks 
and compare it to the inﬁnite-width limit.

1

Introduction

Artiﬁcial neural networks (ANNs) have achieved impressive results in numerous areas of machine
learning. While it has long been known that ANNs can approximate any function with sufﬁciently
many hidden neurons (7; 10)  it is not known what the optimization of ANNs converges to. Indeed
the loss surface of neural networks optimization problems is highly non-convex: it has a high number
of saddle points which may slow down the convergence (4). A number of results (3; 13; 14) suggest
that for wide enough networks  there are very few “bad” local minima  i.e. local minima with much
higher cost than the global minimum. More recently  the investigation of the geometry of the loss
landscape at initialization has been the subject of a precise study (8). The analysis of the dynamics

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

of training in the large-width limit for shallow networks has seen recent progress as well (11). To
the best of the authors knowledge  the dynamics of deep networks has however remained an open
problem until the present paper: see the contributions section below.
A particularly mysterious feature of ANNs is their good generalization properties in spite of their
usual over-parametrization (16). It seems paradoxical that a reasonably large neural network can ﬁt
random labels  while still obtaining good test accuracy when trained on real data (19). It can be noted
that in this case  kernel methods have the same properties (1).
In the inﬁnite-width limit  ANNs have a Gaussian distribution described by a kernel (12; 9). These
kernels are used in Bayesian inference or Support Vector Machines  yielding results comparable to
ANNs trained with gradient descent (9; 2). We will see that in the same limit  the behavior of ANNs
during training is described by a related kernel  which we call the neural tangent network (NTK).

1.1 Contribution

We study the network function fθ of an ANN  which maps an input vector to an output vector  where
θ is the vector of the parameters of the ANN. In the limit as the widths of the hidden layers tend to
inﬁnity  the network function at initialization  fθ converges to a Gaussian distribution (12; 9).
In this paper  we investigate fully connected networks in this inﬁnite-width limit  and describe the
dynamics of the network function fθ during training:

• During gradient descent  we show that the dynamics of fθ follows that of the so-called kernel
gradient descent in function space with respect to a limiting kernel  which only depends on
the depth of the network  the choice of nonlinearity and the initialization variance.

• The convergence properties of ANNs during training can then be related to the positive-
deﬁniteness of the inﬁnite-width limit NTK. The values of the network function fθ outside
the training set is described by the NTK  which is crucial to understand how ANN generalize.
• For a least-squares regression loss  the network function fθ follows a linear differential
equation in the inﬁnite-width limit  and the eigenfunctions of the Jacobian are the kernel
principal components of the input data. This shows a direct connection to kernel methods
and motivates the use of early stopping to reduce overﬁtting in the training of ANNs.

• Finally we investigate these theoretical results numerically for an artiﬁcial dataset (of points
on the unit circle) and for the MNIST dataset. In particular we observe that the behavior of
wide ANNs is close to the theoretical limit.

2 Neural networks

functions fθ in a space F. The dimension of the parameter space is P =(cid:80)L−1

In this article  we consider fully-connected ANNs with layers numbered from 0 (input) to L (output) 
each containing n0  . . .   nL neurons  and with a Lipschitz  twice differentiable nonlinearity function
σ : R → R  with bounded second derivative 1.
This paper focuses on the ANN realization function F (L) : RP → F  mapping parameters θ to
(cid:96)=0 (n(cid:96) + 1)n(cid:96)+1: the
parameters consist of the connection matrices W ((cid:96)) ∈ Rn(cid:96)×n(cid:96)+1 and bias vectors b((cid:96)) ∈ Rn(cid:96)+1 for
(cid:96) = 0  ...  L − 1. In our setup  the parameters are initialized as iid Gaussians N (0  1).
the function space F is deﬁned as
For a ﬁxed distribution pin on the input space Rn0 
{f : Rn0 → RnL}. On this space  we consider the seminorm || · ||pin  deﬁned in terms of the
bilinear form

(cid:2)f (x)T g(x)(cid:3) .

(cid:104)f  g(cid:105)pin = Ex∼pin
(cid:80)N

In this paper  we assume that the input distribution pin is the empirical distribution on a ﬁnite dataset
x1  ...  xN   i.e the sum of Dirac measures 1
N

i=0 δxi.

1While these smoothness assumptions greatly simplify the proofs of our results  they do not seem to be

strictly needed for the results to hold true.

2

We deﬁne the network function by fθ(x) := ˜α(L)(x; θ)  where the functions ˜α((cid:96))(·; θ) : Rn0 → Rn(cid:96)
(called preactivations) and α((cid:96))(·; θ) : Rn0 → Rn(cid:96) (called activations) are deﬁned from the 0-th to
the L-th layer by:

α(0)(x; θ) = x
1√
n(cid:96)

˜α((cid:96)+1)(x; θ) =

W ((cid:96))α((cid:96))(x; θ) + βb((cid:96))

α((cid:96))(x; θ) = σ(˜α((cid:96))(x; θ)) 

ij ∼ N (0  1

n(cid:96)

) and b((cid:96))

1√
n(cid:96)

where the nonlinearity σ is applied entrywise. The scalar β > 0 is a parameter which allows us to
tune the inﬂuence of the bias on the training.
Remark 1. Our deﬁnition of the realization function F (L) slightly differs from the classical one.
and the parameter β are absent and the parameters are initialized using
Usually  the factors
j ∼ N (0  1) (or
what is sometimes called LeCun initialization  taking W ((cid:96))
j = 0) to compensate. While the set of representable functions F (L)(RP ) is the same
sometimes b((cid:96))
and β)  the derivatives of the realization
for both parametrizations (with or without the factors
function with respect to the connections ∂W ((cid:96))
and β
respectively in comparison to the classical parametrization.
The factors
are key to obtaining a consistent asymptotic behavior of neural networks as the
widths of the hidden layers n1  ...  nL−1 grow to inﬁnity. However a side-effect of these factors is
that they reduce greatly the inﬂuence of the connection weights during training when n(cid:96) is large: the
factor β is introduced to balance the inﬂuence of the bias and connection weights. In our numerical
experiments  we take β = 0.1 and use a learning rate of 1.0  which is larger than usual  see Section 6.
This gives a behaviour similar to that of a classical network of width 100 with a learning rate of 0.01.

F (L) and bias ∂b((cid:96))

F (L) are scaled by

1√
n(cid:96)

1√
n(cid:96)

ij

j

1√
n(cid:96)

3 Kernel gradient
The training of an ANN consists in optimizing fθ in the function space F with respect to a functional
cost C : F → R  such as a regression or cross-entropy cost. Even for a convex functional cost C 
the composite cost C ◦ F (L) : RP → R is in general highly non-convex (3). We will show that
during training  the network function fθ follows a descent along the kernel gradient with respect to
the Neural Tangent Kernel (NTK) which we introduce in Section 4. This makes it possible to study
the training of ANNs in the function space F  on which the cost C is convex.
A multi-dimensional kernel K is a function Rn0 × Rn0 → RnL×nL  which maps any pair (x  x(cid:48)) to
an nL× nL-matrix such that K(x  x(cid:48)) = K(x(cid:48)  x)T (equivalently K is a symmetric tensor in F ⊗F).
Such a kernel deﬁnes a bilinear map on F  taking the expectation over independent x  x(cid:48) ∼ pin:

(cid:104)f  g(cid:105)K := Ex x(cid:48)∼pin

(cid:2)f (x)T K(x  x(cid:48))g(x(cid:48))(cid:3) .

The kernel K is positive deﬁnite with respect to || · ||pin if ||f||pin > 0 =⇒ ||f||K > 0.
We denote by F∗ the dual of F with respect to pin  i.e. the set of linear forms µ : F → R of the form
µ = (cid:104)d ·(cid:105)pin for some d ∈ F. Two elements of F deﬁne the same linear form if and only if they
are equal on the data. The constructions in the paper do not depend on the element d ∈ F chosen in
order to represent µ as (cid:104)d ·(cid:105)pin. Using the fact that the partial application of the kernel Ki ·(x ·) is
a function in F  we can deﬁne a map ΦK : F∗ → F mapping a dual element µ = (cid:104)d ·(cid:105)pin to the
function fµ = ΦK(µ) with values:

fµ i(x) = µKi ·(x ·) = (cid:104)d  Ki ·(x ·)(cid:105)pin .

For our setup  which is that of a ﬁnite dataset x1  . . .   xn ∈ Rn0  the cost functional C only depends
on the values of f ∈ F at the data points. As a result  the (functional) derivative of the cost C at a
point f0 ∈ F can be viewed as an element of F∗  which we write ∂in
f C|f0. We denote by d|f0 ∈ F 
a corresponding dual element  such that ∂in

f C|f0 = (cid:104)d|f0 ·(cid:105)pin.

3

(cid:16)

(cid:17)

The kernel gradient ∇KC|f0 ∈ F is deﬁned as ΦK
f C which is only
N(cid:88)
deﬁned on the dataset  the kernel gradient generalizes to values x outside the dataset thanks to the
kernel K:

. In contrast to ∂in

f C|f0
∂in

∇KC|f0 (x) =

1
N

j=1

K(x  xj)d|f0(xj).

A time-dependent function f (t) follows the kernel gradient descent with respect to K if it satisﬁes
the differential equation

∂tf (t) = −∇KC|f (t).
During kernel gradient descent  the cost C(f (t)) evolves as

∂tC|f (t) = −(cid:10)d|f (t) ∇KC|f (t)

pin = −(cid:13)(cid:13)d|f (t)
(cid:11)

(cid:13)(cid:13)2

K .

Convergence to a critical point of C is hence guaranteed if the kernel K is positive deﬁnite with
respect to || · ||pin: the cost is then strictly decreasing except at points such that ||d|f (t)||pin = 0.
If the cost is convex and bounded from below  the function f (t) therefore converges to a global
minimum as t → ∞.

3.1 Random functions approximation

As a starting point to understand the convergence of ANN gradient descent to kernel gradient descent
in the inﬁnite-width limit  we introduce a simple model  inspired by the approach of (15).
A kernel K can be approximated by a choice of P random functions f (p) sampled independently
from any distribution on F whose (non-centered) covariance is given by the kernel K:

E[f (p)

k (x)f (p)

k(cid:48) (x(cid:48))] = Kkk(cid:48)(x  x(cid:48)).

These functions deﬁne a random linear parametrization F lin : RP → F

θ (cid:55)→ f lin

θ =

1√
P

θpf (p).

P(cid:88)

p=1

The partial derivatives of the parametrization are given by

∂θp F lin(θ) =

1√
P

f (p).

Optimizing the cost C ◦ F lin through gradient descent  the parameters follow the ODE:

(cid:68)
d|f lin

θ(t)

  f (p)(cid:69)

.

pin

f (p) = − 1√
P

  f (p)(cid:69)

f (p) 

pin

θ(t)

f C|f lin
∂in
P(cid:88)

p=1

(cid:68)
d|f lin
P(cid:88)

θ(t)

1
P

p=1

(cid:80)P

where the right-hand side is equal to the kernel gradient −∇ ˜KC with respect to the tangent kernel

˜K =

∂θp F lin(θ) ⊗ ∂θp F lin(θ) =

f (p) ⊗ f (p).

This is a random nL-dimensional kernel with values ˜Kii(cid:48)(x  x(cid:48)) = 1
Performing gradient descent on the cost C ◦ F lin is therefore equivalent to performing kernel gradient
descent with the tangent kernel ˜K in the function space. In the limit as P → ∞  by the law of large
numbers  the (random) tangent kernel ˜K tends to the ﬁxed kernel K  which makes this method an
approximation of kernel gradient descent with respect to the limiting kernel K.

i(cid:48) (x(cid:48)).

p=1 f (p)

(x)f (p)

P

i

∂tθp(t) = −∂θp (C ◦ F lin)(θ(t)) = − 1√
P
θ(t) evolves according to

As a result the function f lin

∂tf lin

θ(t) =

1√
P

∂tθp(t)f (p) = − 1
P

P(cid:88)

p=1

P(cid:88)

p=1

4

4 Neural tangent kernel
For ANNs trained using gradient descent on the composition C ◦ F (L)  the situation is very similar to
that studied in the Section 3.1. During training  the network function fθ evolves along the (negative)
kernel gradient

with respect to the neural tangent kernel (NTK)

∂tfθ(t) = −∇Θ(L) C|fθ(t)
P(cid:88)

Θ(L)(θ) =

∂θp F (L)(θ) ⊗ ∂θp F (L)(θ).

p=1

However  in contrast to F lin  the realization function F (L) of ANNs is not linear. As a consequence 
the derivatives ∂θp F (L)(θ) and the neural tangent kernel depend on the parameters θ. The NTK
is therefore random at initialization and varies during training  which makes the analysis of the
convergence of fθ more delicate.
In the next subsections  we show that  in the inﬁnite-width limit  the NTK becomes deterministic at
initialization and stays constant during training. Since fθ at initialization is Gaussian in the limit  the
asymptotic behavior of fθ during training can be explicited in the function space F.

4.1

Initialization

As observed in (12; 9)  the output functions fθ i for i = 1  ...  nL tend to iid Gaussian processes in
the inﬁnite-width limit (a proof in our setup is given in the appendix):
Proposition 1. For a network of depth L at initialization  with a Lipschitz nonlinearity σ  and in the
limit as n1  ...  nL−1 → ∞  the output functions fθ k  for k = 1  ...  nL  tend (in law) to iid centered
Gaussian processes of covariance Σ(L)  where Σ(L) is deﬁned recursively by:

Σ(1)(x  x(cid:48)) =

xT x(cid:48) + β2
1
n0
Σ(L+1)(x  x(cid:48)) = E
f∼N (0 Σ(L))[σ(f (x))σ(f (x(cid:48)))] + β2 

taking the expectation with respect to a centered Gaussian process f of covariance Σ(L).
Remark 2. Strictly speaking  the existence of a suitable Gaussian measure with covariance Σ(L) is
not needed: we only deal with the values of f at x  x(cid:48) (the joint measure on f (x)  f (x(cid:48)) is simply a
Gaussian vector in 2D). For the same reasons  in the proof of Proposition 1 and Theorem 1  we will
freely speak of Gaussian processes without discussing their existence.

The ﬁrst key result of our paper (proven in the appendix) is the following: in the same limit  the
Neural Tangent Kernel (NTK) converges in probability to an explicit deterministic limit.
Theorem 1. For a network of depth L at initialization  with a Lipschitz nonlinearity σ  and in the
limit as the layers width n1  ...  nL−1 → ∞  the NTK Θ(L) converges in probability to a deterministic
limiting kernel:

The scalar kernel Θ(L)∞ : Rn0 × Rn0 → R is deﬁned recursively by

Θ(L) → Θ(L)∞ ⊗ IdnL.

Θ(1)∞ (x  x(cid:48)) = Σ(1)(x  x(cid:48))

Θ(L+1)∞ (x  x(cid:48)) = Θ(L)∞ (x  x(cid:48)) ˙Σ(L+1)(x  x(cid:48)) + Σ(L+1)(x  x(cid:48)) 

where

˙Σ(L+1) (x  x(cid:48)) = E

f∼N (0 Σ(L)) [ ˙σ (f (x)) ˙σ (f (x(cid:48)))]  

taking the expectation with respect to a centered Gaussian process f of covariance Σ(L)  and where
˙σ denotes the derivative of σ.
Remark 3. By Rademacher’s theorem  ˙σ is deﬁned everywhere  except perhaps on a set of zero
Lebesgue measure.

Note that the limiting Θ(L)∞ only depends on the choice of σ  the depth of the network and the variance
of the parameters at initialization (which is equal to 1 in our setting).

5

4.2 Training

(cid:69)

.

pin

Our second key result is that the NTK stays asymptotically constant during training. This applies
for a slightly more general deﬁnition of training: the parameters are updated according to a training
direction dt ∈ F:

(cid:68)

∂tθp(t) =

∂θp F (L)(θ(t))  dt

the integral(cid:82) T
second derivative. For any T such that the integral(cid:82) T

In the case of gradient descent  dt = −d|fθ(t) (see Section 3)  but the direction may depend on
another network  as is the case for e.g. Generative Adversarial Networks (6). We only assume that
0 (cid:107)dt(cid:107)pindt stays stochastically bounded as the width tends to inﬁnity  which is veriﬁed
for e.g. least-squares regression  see Section 5.
Theorem 2. Assume that σ is a Lipschitz  twice differentiable nonlinearity function  with bounded
0 (cid:107)dt(cid:107)pindt stays stochastically bounded  as
n1  ...  nL−1 → ∞  we have  uniformly for t ∈ [0  T ] 

As a consequence  in this limit  the dynamics of fθ is described by the differential equation

Θ(L)(t) → Θ(L)∞ ⊗ IdnL.

(cid:16)(cid:104)dt ·(cid:105)pin

(cid:17)

.

∂tfθ(t) = ΦΘ(L)∞ ⊗IdnL

Remark 4. As the proof of the theorem (in the appendix) shows  the variation during training of the
individual activations in the hidden layers shrinks as their width grows. However their collective
variation is signiﬁcant  which allows the parameters of the lower layers to learn: in the formula of
the limiting NTK Θ(L+1)∞ (x  x(cid:48)) in Theorem 1  the second summand Σ(L+1) represents the learning
due to the last layer  while the ﬁrst summand represents the learning performed by the lower layers.

As discussed in Section 3  the convergence of kernel gradient descent to a critical point of the cost
C is guaranteed for positive deﬁnite kernels. The limiting NTK is positive deﬁnite if the span of
the derivatives ∂θp F (L)  p = 1  ...  P becomes dense in F w.r.t. the pin-norm as the width grows
to inﬁnity. It seems natural to postulate that the span of the preactivations of the last layer (which
themselves appear in ∂θp F (L)  corresponding to the connection weights of the last layer) becomes
dense in F  for a large family of measures pin and nonlinearities (see e.g. (7; 10) for classical
theorems about ANNs and approximation).

5 Least-squares regression
Given a goal function f∗ and input distribution pin  the least-squares regression cost is

(cid:2)(cid:107)f (x) − f∗(x)(cid:107)2(cid:3) .

C(f ) =

||f − f∗||2

pin =

1
2

Ex∼pin

1
2

Theorems 1 and 2 apply to an ANN trained on such a cost. Indeed the norm of the training direction
(cid:107)d(f )(cid:107)pin = (cid:107)f∗ − f(cid:107)pin is strictly decreasing during training  bounding the integral. We are
therefore interested in the behavior of a function ft during kernel gradient descent with a kernel K
(we are of course especially interested in the case K = Θ(L)∞ ⊗ IdnL):

(cid:16)(cid:104)f∗ − f ·(cid:105)pin

(cid:17)

.

∂tft = ΦK

The solution of this differential equation can be expressed in terms of the map Π : f (cid:55)→
ΦK

:

ft = f∗ + e−tΠ(f0 − f∗)

(−t)k
k! Πk is the exponential of −tΠ. If Π can be diagonalized by eigenfunctions
f (i) with eigenvalues λi  the exponential e−tΠ has the same eigenfunctions with eigenvalues e−tλi.
For a ﬁnite dataset x1  ...  xN of size N  the map Π takes the form

k=0

(cid:17)
(cid:16)(cid:104)f ·(cid:105)pin
where e−tΠ =(cid:80)∞

Π(f )k(x) =

1
N

fk(cid:48)(xi)Kkk(cid:48)(xi  x).

N(cid:88)

nL(cid:88)

i=1

k(cid:48)=1

6

The map Π has at most N nL positive eigenfunctions  and they are the kernel principal components
f (1)  ...  f (N nL) of the data with respect to to the kernel K (17; 18). The corresponding eigenvalues
λi is the variance captured by the component.
Decomposing the difference (f∗ − f0) = ∆0
trajectory of the function ft reads

along the eigenspaces of Π  the

f + ... + ∆N nL

f + ∆1

f

ft = f∗ + ∆0

f +

e−tλi∆i
f  

N nL(cid:88)
f ∝ f (i).

i=1

f is in the kernel (null-space) of Π and ∆i

where ∆0
The above decomposition can be seen as a motivation for the use of early stopping. The convergence
is indeed faster along the eigenspaces corresponding to larger eigenvalues λi. Early stopping hence
focuses the convergence on the most relevant kernel principal components  while avoiding to ﬁt
the ones in eigenspaces with lower eigenvalues (such directions are typically the ‘noisier’ ones: for
instance  in the case of the RBF kernel  lower eigenvalues correspond to high frequency functions).
Note that by the linearity of the map e−tΠ  if f0 is initialized with a Gaussian distribution (as is the
case for ANNs in the inﬁnite-width limit)  then ft is Gaussian for all times t. Assuming that the kernel
is positive deﬁnite on the data (implying that the N nL × N nL Gram marix ˜K = (Kkk(cid:48)(xi  xj))ik jk(cid:48)
is invertible)  as t → ∞ limit  we get that f∞ = f∗ + ∆0

f = f0 −(cid:80)

f takes the form

(cid:16)

(cid:17)

 

i ∆i
˜K−1y0

f∞ k(x) = κT

x k

˜K−1y∗ +

with the N nl-vectors κx k  y∗ and y0 given by

f0(x) − κT

x k

κx k = (Kkk(cid:48)(x  xi))i k(cid:48)
y∗ = (f∗
y0 = (f0 k(xi))i k .

k (xi))i k

The ﬁrst term  the mean  has an important statistical interpretation: it is the maximum-a-posteriori
(MAP) estimate given a Gaussian prior on functions fk ∼ N (0  Θ(L)∞ ) and the conditions fk(xi) =
f∗
k (xi) . Equivalently  it is equal to the kernel ridge regression (18) as the regularization goes to
zero (λ → 0). The second term is a centered Gaussian whose variance vanishes on the points of the
dataset.

6 Numerical experiments

In the following numerical experiments  fully connected ANNs of various widths are compared to the
theoretical inﬁnite-width limit. We choose the size of the hidden layers to all be equal to the same
value n := n1 = ... = nL−1 and we take the ReLU nonlinearity σ(x) = max(0  x).
In the ﬁrst two experiments  we consider the case n0 = 2. Moreover  the input elements are taken on
the unit circle. This can be motivated by the structure of high-dimensional data  where the centered
data points often have roughly the same norm 2.
In all experiments  we took nL = 1 (note that by our results  a network with nL outputs behaves
asymptotically like nL networks with scalar outputs trained independently). Finally  the value of the
parameter β is chosen as 0.1  see Remark 1.

6.1 Convergence of the NTK

The ﬁrst experiment illustrates the convergence of the NTK Θ(L) of a network of depth L = 4 for
two different widths n = 500  10000. The function Θ(4)(x0  x) is plotted for a ﬁxed x0 = (1  0)
and x = (cos(γ)  sin(γ)) on the unit circle in Figure 1. To observe the distribution of the NTK  10
independent initializations are performed for both widths. The kernels are plotted at initialization
2The classical example is for data following a Gaussian distribution N (0  Idn0 ): as the dimension n0 grows 

all data points have approximately the same norm

n0.

√

7

Figure 1: Convergence of the NTK to a ﬁxed limit
for two widths n and two times t.

Figure 2: Networks function fθ near convergence
for two widths n and 10th  50th and 90th per-
centiles of the asymptotic Gaussian distribution.

t = 0 and then after 200 steps of gradient descent with learning rate 1.0 (i.e. at t = 200). We
approximate the function f∗(x) = x1x2 with a least-squares cost on random N (0  1) inputs.
For the wider network  the NTK shows less variance and is smoother. It is interesting to note that
the expectation of the NTK is very close for both networks widths. After 200 steps of training  we
observe that the NTK tends to “inﬂate”. As expected  this effect is much less apparent for the wider
network (n = 10000) where the NTK stays almost ﬁxed  than for the smaller network (n = 500).

6.2 Kernel regression

For a regression cost  the inﬁnite-width limit network function fθ(t) has a Gaussian distribution for
all times t and in particular at convergence t → ∞ (see Section 5). We compared the theoretical
Gaussian distribution at t → ∞ to the distribution of the network function fθ(T ) of a ﬁnite-width
network for a large time T = 1000. For two different widths n = 50  1000 and for 10 random
initializations each  a network is trained on a least-squares cost on 4 points of the unit circle for 1000
steps with learning rate 1.0 and then plotted in Figure 2.
We also approximated the kernels Θ(4)∞ and Σ(4) using a large-width network (n = 10000) and used
them to calculate and plot the 10th  50th and 90-th percentiles of the t → ∞ limiting Gaussian
distribution.
The distributions of the network functions are very similar for both widths: their mean and variance
appear to be close to those of the limiting distribution t → ∞. Even for relatively small widths
(n = 50)  the NTK gives a good indication of the distribution of fθ(t) as t → ∞.

6.3 Convergence along a principal component

We now illustrate our result on the MNIST dataset of handwritten digits made up of grayscale images
of dimension 28 × 28  yielding a dimension of n0 = 784.
We computed the ﬁrst 3 principal components of a batch of N = 512 digits with respect to the NTK
of a high-width network n = 10000 (giving an approximation of the limiting kernel) using a power
iteration method. The respective eigenvalues are λ1 = 0.0457  λ2 = 0.00108 and λ3 = 0.00078.
The kernel PCA is non-centered  the ﬁrst component is therefore almost equal to the constant function 
which explains the large gap between the ﬁrst and second eigenvalues3. The next two components are
much more interesting as can be seen in Figure 3a  where the batch is plotted with x and y coordinates
corresponding to the 2nd and 3rd components.
We have seen in Section 5 how the convergence of kernel gradient descent follows the kernel principal
components. If the difference at initialization f0 − f∗ is equal (or proportional) to one of the principal
3It can be observed numerically  that if we choose β = 1.0 instead of our recommended 0.1  the gap between

the ﬁrst and the second principal component is about ten times bigger  which makes training more difﬁcult.

8

32101230.050.100.150.200.250.300.350.40n=500 t=0n=500 t=20n=10000 t=0n=10000 0n = 500  t = 0n = 500  t = 200n = 10000  t = 0n = 10000  t = 20032101230.40.20.00.20.4f(sin() cos())n=50n=1000n= P50n= {P10 P90}n
i
p
|
|
t
h

|
|

n
i
p
|
|
t
g
|
|

(a) The 2nd and 3rd principal
components of MNIST.

(b) Deviation of the network function
fθ from the straight line.

(c) Convergence of fθ along the 2nd
principal component.

Figure 3

components f (i)  then the function will converge along a straight line (in the function space) to f∗ at
an exponential rate e−λit.
We tested whether ANNs of various widths n = 100  1000  10000 behave in a similar manner. We
set the goal of the regression cost to f∗ = fθ(0) + 0.5f (2) and let the network converge. At each time
step t  we decomposed the difference fθ(t) − f∗ into a component gt proportional to f (2) and another
one ht orthogonal to f (2). In the inﬁnite-width limit  the ﬁrst component decays exponentially fast
||gt||pin = 0.5e−λ2t while the second is null (ht = 0)  as the function converges along a straight line.
As expected  we see in Figure 3b that the wider the network  the less it deviates from the straight line
(for each width n we performed two independent trials). As the width grows  the trajectory along the
2nd principal component (shown in Figure 3c) converges to the theoretical limit shown in blue.
A surprising observation is that smaller networks appear to converge faster than wider ones. This may
be explained by the inﬂation of the NTK observed in our ﬁrst experiment. Indeed  multiplying the
NTK by a factor a is equivalent to multiplying the learning rate by the same factor. However  note
that since the NTK of large-width network is more stable during training  larger learning rates can in
principle be taken. One must hence be careful when comparing the convergence speed in terms of the
number of steps (rather than in terms of the time t): both the inﬂation effect and the learning rate
must be taken into account.

7 Conclusion

This paper introduces a new tool to study ANNs  the Neural Tangent Kernel (NTK)  which describes
the local dynamics of an ANN during gradient descent. This leads to a new connection between ANN
training and kernel methods: in the inﬁnite-width limit  an ANN can be described in the function
space directly by the limit of the NTK  an explicit constant kernel Θ(L)∞   which only depends on
its depth  nonlinearity and parameter initialization variance. More precisely  in this limit  ANN
gradient descent is shown to be equivalent to a kernel gradient descent with respect to Θ(L)∞ . The
limit of the NTK is hence a powerful tool to understand the generalization properties of ANNs  and
it allows one to study the inﬂuence of the depth and nonlinearity on the learning abilities of the
network. The analysis of training using NTK allows one to relate convergence of ANN training with
the positive-deﬁniteness of the limiting NTK and leads to a characterization of the directions favored
by early stopping methods.

Acknowledgements

The authors thank K. Kyt¨ol¨a for many interesting discussions. The second author was supported by
the ERC CG CRITICAL. The last author acknowledges support from the ERC SG Constamis  the
NCCR SwissMAP  the Blavatnik Family Foundation and the Latsis Foundation.

9

321012f(2)(x)21012f(3)(x)05001000150020002500300035004000t0.000.020.040.060.080.100.120.14n=100n=1000n=1000005001000150020002500300035004000t0.00.10.20.30.40.5n=100n=1000n=10000n=References
[1] M. Belkin  S. Ma  and S. Mandal. To understand deep learning we need to understand kernel

learning. arXiv preprint  Feb 2018.

[2] Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information

Processing Systems 22  pages 342–350. Curran Associates  Inc.  2009.

[3] A. Choromanska  M. Henaff  M. Mathieu  G. B. Arous  and Y. LeCun. The Loss Surfaces of

Multilayer Networks. Journal of Machine Learning Research  38:192–204  nov 2015.

[4] Y. N. Dauphin  R. Pascanu  C. Gulcehre  K. Cho  S. Ganguli  and Y. Bengio. Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization.
In
Proceedings of the 27th International Conference on Neural Information Processing Systems -
Volume 2  NIPS’14  pages 2933–2941  Cambridge  MA  USA  2014. MIT Press.

[5] S. S. Dragomir. Some Gronwall Type Inequalities and Applications. Nova Science Publishers 

2003.

[6] I. J. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville 
and Y. Bengio. Generative Adversarial Networks. NIPS’14 Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2  pages 2672–2680  jun 2014.

[7] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal

approximators. Neural Networks  2(5):359 – 366  1989.

[8] R. Karakida  S. Akaho  and S.-i. Amari. Universal Statistics of Fisher Information in Deep

Neural Networks: Mean Field Approach. jun 2018.

[9] J. H. Lee  Y. Bahri  R. Novak  S. S. Schoenholz  J. Pennington  and J. Sohl-Dickstein. Deep

neural networks as gaussian processes. ICLR  2018.

[10] M. Leshno  V. Lin  A. Pinkus  and S. Schocken. Multilayer feedforward networks with a non-
polynomial activation function can approximate any function. Neural Networks  6(6):861–867 
1993.

[11] S. Mei  A. Montanari  and P.-M. Nguyen. A mean ﬁeld view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–E7671 
2018.

[12] R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York  Inc.  Secaucus 

NJ  USA  1996.

[13] R. Pascanu  Y. N. Dauphin  S. Ganguli  and Y. Bengio. On the saddle point problem for

non-convex optimization. arXiv preprint  2014.

[14] J. Pennington and Y. Bahri. Geometry of neural network loss surfaces via random matrix
theory. In Proceedings of the 34th International Conference on Machine Learning  volume 70
of Proceedings of Machine Learning Research  pages 2798–2806  International Convention
Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[15] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20  pages 1177–1184. Curran Associates  Inc.  2008.

[16] L. Sagun  U. Evci  V. U. G¨uney  Y. Dauphin  and L. Bottou. Empirical analysis of the hessian

of over-parametrized neural networks. CoRR  abs/1706.04454  2017.

[17] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue

problem. Neural Computation  10(5):1299–1319  1998.

[18] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University

Press  New York  NY  USA  2004.

[19] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. ICLR 2017 proceedings  Feb 2017.

10

,Arthur Jacot
Franck Gabriel
Clement Hongler