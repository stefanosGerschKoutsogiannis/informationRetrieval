2010,Efficient Optimization for Discriminative Latent Class Models,Dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression; we show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization. While expectation-maximization (EM) algorithm is commonly used to learn these models  its optimization usually leads to local minimum because it relies on a non-convex cost function with many such local minima. To avoid this problem  we introduce a local approximation of this cost function  which leads to a quadratic non-convex optimization problem over a product of simplices. In order to minimize such functions  we propose an efficient algorithm based on convex relaxation and low-rank representation of our data  which allows to deal with large instances. Experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods  while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.,Efﬁcient Optimization for Discriminative

Latent Class Models

Armand Joulin∗

INRIA

23  avenue d’Italie 
75214 Paris  France.

Francis Bach∗

INRIA

23  avenue d’Italie 
75214 Paris  France.

Jean Ponce∗

Ecole Normale Sup´erieure

45  rue d’Ulm

75005 Paris  France.

armand.joulin@inria.fr

francis.bach@inria.fr

jean.ponce@ens.fr

Abstract

Dimensionality reduction is commonly used in the setting of multi-label super-
vised classiﬁcation to control the learning capacity and to provide a meaningful
representation of the data. We introduce a simple forward probabilistic model
which is a multinomial extension of reduced rank regression  and show that this
model provides a probabilistic interpretation of discriminative clustering meth-
ods with added beneﬁts in terms of number of hyperparameters and optimization.
While the expectation-maximization (EM) algorithm is commonly used to learn
these probabilistic models  it usually leads to local maxima because it relies on
a non-convex cost function. To avoid this problem  we introduce a local approx-
imation of this cost function  which in turn leads to a quadratic non-convex op-
timization problem over a product of simplices. In order to maximize quadratic
functions  we propose an efﬁcient algorithm based on convex relaxations and low-
rank representations of the data  capable of handling large-scale problems. Exper-
iments on text document classiﬁcation show that the new model outperforms other
supervised dimensionality reduction methods  while simulations on unsupervised
clustering show that our probabilistic formulation has better properties than exist-
ing discriminative clustering methods.

1

Introduction

Latent representations of data are wide-spread tools in supervised and unsupervised learning. They
are used to reduce the dimensionality of the data for two main reasons: on the one hand  they
provide numerically efﬁcient representations of the data; on the other hand  they may lead to better
predictive performance. In supervised learning  latent models are often used in a generative way 
e.g.  through mixture models on the input variables only  which may not lead to increased predictive
performance. This has led to numerous works on supervised dimension reduction (e.g.  [1  2]) 
where the ﬁnal discriminative goal of prediction is taken explicitly into account during the learning
process.

In this context  various probabilistic models have been proposed  such as mixtures of experts [3] or
discriminative restricted Boltzmann machines [4]  where a layer of hidden variables is used between
the inputs and the outputs of the supervised learning model. Parameters are usually estimated by
expectation-maximization (EM)  a method that is computationally efﬁcient but whose cost function
may have many local maxima in high dimensions.

In this paper  we consider a simple discriminative latent class (DLC) model where inputs and outputs
are independent given the latent representation.We make the following contributions:

∗WILLOW project-team  Laboratoire d’Informatique de l’Ecole Normale Sup´erieure  (ENS/INRIA/CNRS

UMR 8548).

1

– We provide in Section 2 a quadratic (non convex) local approximation of the log-likelihood of
our model based on the EM auxiliary function. This approximation is optimized to obtain robust
initializations for the EM procedure.

– We propose in Section 3.3 a novel probabilistic interpretation of discriminative clustering with

added beneﬁts  such as fewer hyperparameters than previous approaches [5  6  7].

– We design in Section 4 a low-rank optimization method for non-convex quadratic problems over a
product of simplices. This method relies on a convex relaxation over completely positive matrices.
– We perform experiments on text documents in Section 5  where we show that our inference tech-

nique outperforms existing supervised dimension reduction and clustering methods.

2 Probabilistic discriminative latent class models

We consider a set of N observations xn ∈ Rp  and their labels yn ∈ {1  . . .   M }  n ∈ {1  . . .   N }.
We assume that each observation xn has a certain probability to be in one of K latent classes  mod-
eled by introducing hidden variables zn ∈ {1  . . .   K}  and that these classes should be predictive
of the label yn. We model directly the conditional probability of zn given the input data xn and the
probability of the label yn given zn  while making the assumption that yn and xn are independent
given zn (leading to the directed graphical model xn → zn → yn). More precisely  we assume that 
given xn  zn follows a multinomial logit model while  given zn  yn is a multinomial variable:

k xn+bk

ewT
j=1 ewT

j xn+bj

and

p(yn = m|zn = k) = αkm 

(1)

p(zn = k|xn) =

PK
with wk ∈ Rp  bk ∈ R and PM

m=1 αkm = 1. We use the notation w = (w1  . . .   wK )  b =
(b1  . . .   bK ) and α = (αkm)1≤k≤K 1≤m≤M . Note that the model deﬁned by (1) can be kernelized
by replacing implicitly or explicitly x by the image Φ(x) of a non linear mapping.

Related models.
The simple two-layer probabilistic model deﬁned in Eq. (1)  can be interpreted
and compared to other methods in various ways. First  it is an instance of a mixture of experts [3]
where each expert has a constant prediction. It has thus weaker predictive power than general mix-
tures of experts; however  it allows efﬁcient optimization as shown in Section 4. It would be inter-
esting to extend our optimization techniques to the case of experts with non-constant predictions.
This is what is done in [8] where a convex relaxation of EM for a similar mixture of experts is con-
sidered. However  [8] considers the maximization with respect to hidden variables rather than their
marginalization  which is essential in our setting to have a well-deﬁned probabilistic model. Note
also that in [8]  the authors derive a convex relaxation of the softmax regression problems  while we
derive a quadratic approximation. It is worth trying to combine the two approaches in future work.

Another related model is a two-layer neural network.
Indeed  if we marginalize the latent vari-
able z  we get that the probability of y given x is a linear combination of softmax functions of linear
functions of the input variables x. Thus  the only difference with a two-layer neural network with
softmax functions for the last layer is the fact that our last layer considers linear parameterization in
the mean parameters rather than in the natural parameters of the multinomial variable. This change
allows us to provide a convexiﬁcation of two-layer neural networks in Section 4.

Among probabilistic models  a discriminative restricted Boltzmann machine (RBM) [4  9] mod-
els p(y|z) as a softmax function of linear functions of z. Our model assumes instead that p(y|z)
is linear in z. Again  this distinction between mean parameters and natural parameters allows us to
derive a quadratic approximation of our cost function. It would of course be of interest to extend our
optimization technique to the discriminative RBM.

Finally  one may also see our model as a multinomial extension of reduced-rank regression (see 
e.g. [10])  which is commonly used with Gaussian distributions and reduces to singular value de-
composition in the maximum likelihood framework.

2

3

Inference

We consider the negative conditional log-likelihood of yn given xn (regularized in w to avoid over-
ﬁtting) where θ = (α  w  b) and ynm is equal to 1 if yn = m and 0 otherwise:

ℓ(θ) = −

1
N

N

M

Xn=1

Xm=1

ynm log p(ynm = 1|xn) +

λ
2K

kwk2
F .

(2)

3.1 Expectation-maximization

A popular tool for solving maximum likelihood problems is the EM algorithm [10]. A traditional
way of viewing EM is to add auxiliary variables and minimize the following upperbound of the
negative log-likelihood ℓ  obtained by using the concavity of the logarithm:

F (ξ  θ) = −

1
N

N

M

Xn=1

Xm=1

ynm" K
Xk=1

ξnk log

k xn+bk

n αkewT
yT
ξnk

− log(cid:16)

K

Xk=1

ewT

k xn+bk(cid:17)# +

λ
2K

kwk2
F  

where αk = (αk1  . . .   αkm)T ∈ RM and ξ = (ξ1  . . .   ξK )T ∈ RN ×K with ξn =
(ξn1  . . .   ξnK ) ∈ RK . The EM algorithm can be viewed as a two-step block-coordinate descent
procedure [11]  where the ﬁrst step (E-step) consists in ﬁnding the optimal auxiliary variables ξ 
given the parameters of the model θ. In our case  the result of this step is obtained in closed form
as ξnk ∝ yT
n 1K = 1. The second step (M-step) consists of ﬁnding the best set
of parameters θ  given the auxiliary variables ξ. Optimizing the parameters αk leads to the closed
k 1M = 1 while optimizing jointly on w and b leads to a

n=1 ξnkyn with αT

k xn+bk with ξT

n αkewT

softmax regression problem  which we solved with Newton method.

form updates αk ∝ PN

Since F (ξ  θ) is not jointly convex in ξ and θ  this procedure stops when it reaches a local minimum 
and its performance strongly depends on its initialization. We propose in the following section  a
robust initialization for EM given our latent model  based on an approximation of the auxiliary cost
function obtained with the M-step.

3.2

Initialization of EM

Minimizing F w.r.t. ξ leads to the original log-likelihood ℓ(θ) depending on θ alone. Minimizing F
w.r.t. θ gives a function of ξ alone. In this section  we focus on deriving a quadratic approximation
of this function  which will be minimized to obtain an initialization for EM.

We consider second-order Taylor expansions around the value of ξ corresponding to the uniformly
distributed latent variables zn  independent of the observations xn  i.e.  ξ0 = 1
K . This choice
is motivated by the lack of a priori information on the latent classes. We brieﬂy explain the calcu-
lation of the expansion of the terms depending on (w  b). For the rest of the calculation  see the
supplementary material.

K 1N 1T

Second-order Taylor expansion of the terms depending on (w  b). Assuming uniformly dis-
tributed variables zn and independence between zn and xn implies that wT
k xn + bk = 0. There-
k=1 exp(uk))

fore  using the second-order expansion of the log-sum-exp function ϕ(u) = log(PK

around 0 leads to the following approximation of the terms depending on (w  b):

Jwb(ξ) = cst +

tr(ξξT ) −

k(Kξ − Xw − b)ΠKk2

F + λkwk2

K 1K1T

where ΠK = I − 1
K is the usual centering projection matrix  and X = (x1  . . .   xN )T . The
third-order term O(kXw + bk3
F ) can be replaced by third-order terms in kξ − ξ0k  which makes
the minimization with respect to w and b correspond to a multi-label classiﬁcation problem with a
square-loss [7  10  12]. Its solution may be obtained in closed form and leads to:

K
2N

1
2K

min

w b h 1

N

F + O(kXw + bk3)i 

Jwb(ξ) = cst +

K
2N

trhξξT(cid:0)I − A(X  λ)(cid:1)i + O(kξ − ξ0k3) 

where A(X  λ) = ΠN(cid:16)I − X(N λI + X T ΠN )−1X T(cid:17)ΠN .

3

Quadratic approximation. Omitting the terms that are independent of ξ or of an order in ξ higher
than two  the second-order approximation Japp of the function obtained for the M-step is:

Japp(ξ) =

K
2

where B(Y ) = 1

N(cid:16)Y (Y T Y )−1Y T − 1

trhξξT(cid:16)B(Y ) − A(X  λ)(cid:17)i 
N(cid:17) and Y ∈ RN ×M is the matrix with entries ynm.

N 1N 1T

(3)

Link with ridge regression. The ﬁrst term  tr(ξξT B(Y ))  is a concave function in ξ  whose maxi-
mum is obtained for ξξT = I (each variable in a different cluster). The second term  A(X  λ)  is the
matrix obtained in ridge regression [7  10  12]. Since A(x  λ) is a positive semi-deﬁnite matrix such
that A(X  λ)1N = 0  the maximum of the second term is obtained for ξξT = 1N 1T
N (all variables
in the same cluster). Japp(ξ) is thus a combination of a term trying to put every point in the same
cluster and a term trying to spread them equally. Note that in general  Japp is not convex.

Non linear predictions. Using the matrix inversion lemma  A(X  λ) can be expressed in terms
of the Gram matrix K = XX T   which allows us to use any positive deﬁnite kernel in our frame-
work [12]  and tackle problems that are not linearly separable. Moreover  the square loss gives a
natural interpretation of the regularization parameter λ in terms of the implicit number of param-
eters of the learning procedure [10]. Indeed  the degree of freedom deﬁned as df = n(1 − trA)
provides a intuitive method for setting the value of λ [7  10].

Initialization of EM. We optimize Japp(ξ) to get a robust initialization for EM. Since the entries
of each vector ξn sum to 1  we optimize Japp over a set of N simplices in K dimensions  S = {v ∈
RK | v ≥ 0  vT 1K = 1}. However  since this function is not convex  minimizing it directly leads
to local minima. We propose  in Section 4  a general reformulation of any non-convex quadratic
program over a set of N simplices and propose an efﬁcient algorithm to optimize it.

3.3 Discriminative clustering

The goal of clustering is to ﬁnd a low-dimensional representation of unlabeled observations  by
assigning them to K different classes  Xu et al. [5] proposes a discriminative clustering framework
based on the SVM and [7] simpliﬁes it by replacing the hinge loss function by the square loss 
leading to ridge regression. By taking M = N and the labels Y = I  we obtain a formulation
similar to [7] where we are looking for a latent representation that can recover the identity matrix.
However  unlike [5  7]  our discriminative clustering framework is based on a probabilistic model
which may allow natural extensions. Moreover  our formulation naturally avoids putting all variables
in the same cluster  whereas [5  7] need to introduce constraints on the size of each cluster. Also 
our model leads to a soft assignment of the variables  allowing ﬂexibility in the shape of the clusters 
whereas [5  7] is based on hard assignment. Finally  since our formulation is derived from EM  we
obtain a natural rounding by applying the EM algorithm after the optimization whereas [7] uses a
coarse k-means rounding. Comparisons with these algorithms can be found in Section 5.

4 Optimization of quadratic functions over simplices

To initialize the EM algorithm  we must minimize the non-convex quadratic cost function deﬁned by
Eq. (3) over a product of N simplices. More precisely  we are interested in the following problems:

min

V

f (V ) = 1

2 tr (V V T B)

s.t. V = (V1  . . .   VN )T ∈ RN ×K and ∀n  Vn ∈ S 

(4)

where B can be any N ×N symmetric matrix. Denoting v = vec(V ) ∈ RN K the vector obtained by
stacking all the columns of V and deﬁning Q = (BT ⊗IK)T   where ⊗ is the Kronecker product [13] 
the problem (4) is equivalent to:

min

v

1

2 vT Qv

s.t.

v ∈ RN K   v ≥ 0 and (IN ⊗ 1T

K)v = 1N .

(5)

Note that this formulation is general  and that Q could be any N K × N K symmetric matrix. Tradi-
tional convex relaxation methods [14] would rewrite the objective function as vT Qv = tr(QvvT ) =

4

tr(QT ) where T = vvT is a rank-one matrix which satisﬁes the set of constraints:

− T ∈ DN K = {T ∈ RN K×N K | T ≥ 0  T < 0}
− ∀ n  m ∈ {1  . . .   N }  1T
− ∀ n  i  j ∈ {1  . . .   N }  Tni1K = Tnj1K .

K Tnm1K = 1 

(6)

(7)

(8)

We note F the set of matrix T verifying (7-8). With the unit-rank constraint  optimizing over v is
exactly equivalent to optimizing over T . The problem is relaxed into a convex problem by removing
the rank constraint  leading to a semideﬁnite programming problem (SDP) [15].

Relaxation. Optimizing T instead of v is computationally inefﬁcient since the running time com-

plexity of general purpose SDP toolboxes is in this case O(cid:0)(KN )7(cid:1). On the other hand  for prob-

lems without pointwise positivity  [16  17] have considered low-rank representations of matrices T  
of the form T = V V T where V has more than one column. In particular  [17] shows that the non
convex optimization with respect to V leads to the global optimum of the convex problem in T .

In order to apply the same technique here  we need to deal with the pointwise nonnegativity. This
can be done by considering the set of completely positive matrices  i.e. 

CP K = {T ∈ RN K×N K|∃R ∈ N∗  ∃V ∈ RN K×R  V ≥ 0  T = V V T }.

This set is strictly included in the set DN K of doubly non-negative matrices (i.e.  both pointwise
nonnegative and positive semi-deﬁnite). For R ≥ 5  it turns out that the intersection of CP K and F
is the convex hull of the matrices vvT such that v is an element of the product of simplices [16]. This
implies that the convex optimization problem of minimizing tr (QT ) over CP K ∩ F is equivalent to
our original problem (for which no polynomial-time algorithm is known).

However  even if the set CP K ∩ F is convex  optimizing over it is computationally inefﬁcient [18].
We thus follow [17] and consider the problem through the low-rank pointwise nonnegative matrix
V ∈ RN K×R instead of through matrices T = V V T . Note that following arguments from [16]  if R
is large enough  there are no local minima. However  because of the positivity constraint one cannot
ﬁnd in polynomial time a local minimum of a differentiable function. Nevertheless  any gradient
descent algorithm will converge to a stationary point. In Section 5  we compare results with R > 1
than with R = 1  which corresponds to a gradient descent directly on the simplex.

Problem reformulation.
In order to derive a local descent algorithm  we reformulate the con-
straints (7-8) in terms of V (details can be found in the supplementary material). Denoting by Vr
the r-th column of V   V n
R ) 
condition (8) is equivalent to kV m
r k1 for all n and m. Substituting this in (7) yields that
for all n  kV nk2−1 = 1  where kV nk2
r )2 is the squared ℓ2−1 norm. We drop this
condition by using a rescaled cost function which equivalent. Finally  using the notation D:

r the K-vector such as Vr = (V 1

r k1 = kV n

r=1(1T V n

r   . . .   V N

r )T and V n = (V n

1   . . .   V n

2−1 =PR

D = {W ∈ RN K | W ≥ 0  ∀n  m  kW nk1 = kW mk1} 

we obtain a new equivalent formulation:

min

V ∈RNK×R  ∀r  Vr ∈D

1

2 tr(V D−1V T Q) with D = Diag((IN ⊗ 1K)T V V T (IN ⊗ 1K)) 

(9)

where Diag(A) is the matrix with the diagonal of A and 0 elsewhere. Since the set of constraints for
V is convex  we can use a projected gradient method [19] with the projection step we now describe.

Projection on D. Given N K-vectors Z n stacked in a N K vector Z = [Z 1; . . . ; Z N ]  we consider
the projection of Z on D. For a given positive real number a  the projection of Z on the set of
all U ∈ D such that for all n  kU nk1 = a  is equivalent to N independent projections on the ℓ1 ball
with radius a. Thus projecting Z on D is equivalent to ﬁnd the solution of:

N

min
a≥0

L(a) =

Xn=1

max
λn∈R

min
U n≥0

1

2 kU n − Z nk2

2 + λn(1T

K U n − a) 

where (λn)n≤N are Lagrange multipliers. The problem of projecting each Z n on the ℓ1-
ball of radius a is well studied [20]  with known expressions for the optimal Lagrange mul-
tipliers  (λn(a))n≤N and the corresponding projection for a given a. The function L(a) is

5

)

%

(
 
e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

100

80

60

40

 

0

K = 2

 

)

%

(
 
e

t

a
r
 

n
o

avg round
min round
ind − avg round
ind − min round

i
t

a
c
i
f
i
s
s
a
c

l

100

80

60

40

K = 3

K = 5

100

80

60

40

)

%

(
 
e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

5

noise dimension

10

0

5

noise dimension

10

0

5

noise dimension

10

Figure 1: Comparison between our algorithm and R independent optimizations. Also comparison
between two rounding: by summing and by taking the best column. Average results for K = 2  3  5
(Best seen in color).

convex  piecewise-quadratic and differentiable  which yields the ﬁrst-order optimality condi-
n=1 λn(a) = 0 for a. Several algorithms can be used to ﬁnd the optimal value of a. We
n=1 λn(a) on the interval [0  λmax]  where λmax is

tion PN
use a binary search by looking at the sign ofPN

found iteratively. This method was found to be empirically faster than gradient descent.

Overall complexity and running time. We use projected gradient descent  the bottleneck of our
algorithm is the projection with a complexity of O(RN 2K log(K)). We present experiments on
running times in the supplementary material.

5

Implementation and results

We ﬁrst compare our algorithm with others to optimize the problem (4). We show that the per-
formances are equivalent but  our algorithm can scale up to larger database. We also consider the
problem of supervised and unsupervised discriminative clustering. In both cases  we show that our
algorithm outperforms existing methods.

Implementation. For supervised and unsupervised multilabel classiﬁcation  we ﬁrst optimize the
second-order approximation Japp  using the reformulation (9). We use a projected gradient descent
method with Armijo’s rule along the projection arc for backtracking [19].
It is stopped after a
maximum number of iterations (500) or if relative updates are too small (10−8). When the algorithm
r=1 Vr ∈ S as our ﬁnal
solution (“avg round”). We also compare this rounding with another heuristic obtained by taking
f (Vr) (“min round”). v∗ is then used to initialize the EM algorithm described in
v∗ = argminVr
Section 2.

stops  the matrix V has rank greater than 1 and we use the heuristic v∗ =PR

Optimization over simplices. We compare our optimization of the non-convex quadratic prob-
lem (9) in V   to the convex SDP in T = V V T on the set of constraints deﬁned by T ∈ DN K   (7)
and (8). To optimize the SDP  we use generic algorithms  CVX [21] and PPXA [22]. CVX uses
interior points methods whereas PPXA uses proximal methods [22]. Both algorithms are com-
putationally inefﬁcient and do not scale well with either the number of points or the number of
constraints. Thus we set N = 10 and K = 2 on discriminative clustering problems (which are
described later in this section). We compare the performances of these algorithms after rounding.
For the SDP  we take ξ∗ = T 1N K and for our algorithm we report performances obtained for both
rounding discuss above (“avg round” and “min round”). On these small examples  our algorithm
associated with “min round” reaches similar performances than the SDP  whereas  associated with
“avg round”  its performance drops.

Study of rounding procedures. We compare the performances of the two different roundings 
“min round” and “avg round” on discriminative clustering problems. After rounding  we apply the
EM algorithm and look at the classiﬁcation scores. We also compare our algorithm for a given R  to
two baselines where we solve independently problem (4) R times and then apply the same round-
ings (“ind - min round” and “ind - avg round”). Results are shown Figure 1. We consider three

6

1 vs. 20 − K = 3

1 vs. 20 − K = 7

1 vs. 20 − K = 15

90

80

70

60

50

90

80

70

60

50

100

200
N

300

400

100

200
N

300

400

100

200
N

300

400

4 vs. 5 − K = 3

4 vs. 5 − K = 7

4 vs. 5 − K = 15

90

80

70

60

50

90

80

70

60

50

)

%

(
 
e
a
r
 

t

n
o

i
t

a
c
i
f
i
s
s
a
C

l

90

80

70

60

50

)

%

(
 
e
a
r
 

t

n
o

i
t

a
c
i
f
i
s
s
a
C

l

90

80

70

60

50

100

200
N

300

400

100

200
N

300

400

100

200
N

300

400

Figure 2: Classiﬁcation rate for several binary classiﬁcation tasks (from top to bottom) and for
different values of K  from left to right (Best seen in color).

different problems  N = 100 and K = 2  K = 3 and K = 5. We look at the average perfor-
mances as the number of noise dimensions increases in discriminative clustering problems. Our
method outperforms the baseline whatever rounding we use. Figure 1 shows that on problems with
a small number of latent classes (K < 5)  we obtain better performances by taking the column
associated with the lowest value of the cost function (“min round”)  than summing all the columns
(“avg round”). On the other hand  when dealing with a larger number of classes (K ≥ 5)  the per-
formance of “min round’ drops signiﬁcantly while “avg round” maintains good results. A potential
explanation is that summing the columns of V gives a solution close to 1
K in expectation  thus
in the region where our quadratic approximation is valid. Moreover  the best column of V is usually
a local minimum of the quadratic approximation  which we have found to be close to similar local
minima of our original problem  therefore  preventing the EM algorithm from converging to another
solution. In all others experiments  we choose “avg round”.

K 1N 1T

Application to classiﬁcation. We evaluate the optimization performance of our algorithm (DLC)
on text classiﬁcation tasks.
For our experiments  we use the 20 Newsgroups dataset
(http://people.csail.mit.edu/jrennie/)  which contains postings to Usenet newsgroups. The postings
are organized by content into 20 categories. We use the ﬁve binary classiﬁcation tasks considered
in [23  Chapter 4  page 91]. To set the regularization parameter λ  we use the degree of free-
dom df (see Section 3.2). Each document has 13312 entries and we take df = 1000. We use 50
random initializations for our algorithm. We compare our method with classiﬁers such as the lin-
ear SVM and the supervised Latent Dirichlet Allocation (sLDA) classiﬁer of Blei et al. [2]. We
also compare our results to those obtained by an SVM using the features obtained with dimension-
reducing methods such as LDA [1] and PCA. For these models  we select parameters with 5-fold
cross-validation. We also compare to the EM without our initialization (“rand-init”) but also with
50 random initializations  a local descent method which is close to back-propagation in a two-layer
neural network  which in this case strongly suffers from local minima problems. An interesting
result on computational time is that EM without our initialization needs more steps to obtain a local
minimum. It is therefore slower than with our initialization in this particular set of experiments.
We show some results in Figure 2 (others maybe found in the supplementary material) for different
values of K and with an increasing number N of training samples. In the case of topic models  K
represents the number of topics. Our method signiﬁcantly outperforms all the other classiﬁers. The
comparison with “rand-init” shows the importance of our convex initialization. We also note that
our performance increases slowly with K. Indeed  the number of latent classes needed to correctly
separate two classes of text is small. Moreover  the algorithm tends to automatically select K. Em-
pirically  we notice that starting with K = 15 classes  our average ﬁnal number of active classes is
around 3. This explains the relatively small gain in performance as K increases.

7

 

r
o
r
r
e
g
n
i
r
e

t
s
u
c

l

0.4

0.3

0.2

0.1

0

5
noise dimension

10

15

 

r
o
r
r
e
g
n
i
r
e

t
s
u
c

l

0.8

0.6

0.4

0.2

0

20

5
noise dimension

15

10

20

Figure 3: Clustering error when increasing the number of noise dimensions. We have take 50
different problems and 10 random initializations for each of them. K = 2  N = 100 and R = 5  on
the left  and K = 5  N = 250 and R = 10  on the right(Best seen in color).

Figure 4: Comparison between our method (left) and k-means (right). First  circles with RBF
kernels. Second  linearly separable bumps. K = 2  N = 200 and R = 5 in both cases.

Application to discriminative clustering. Figure 3 shows the optimization performance of the
EM algorithm with 10 random starting points with (“DLC”) and without (“rand-init”) our initial-
ization method. We compare their performances to K-means  Gaussian Mixture Model (“GMM”) 
Diffrac [7] and max-margin clustering (“MMC”) [24]. Following [7]  we take linearly separable
bumps in a two-dimensional space and add dimensions containing random independent Gaussian
noise (e.g. “noise dimensions”) to the data. We evaluate the ratio of misclassiﬁed observations over
the total number of observations. For the ﬁrst experiment  we ﬁx K = 2  N = 100  and R = 5  and
for the second K = 5  N = 250  and R = 10. The additional independent noise dimensions are
normally distributed. We use linear kernels for all the methods. We set the regularization parame-
ters λ to 10−2 for all experiments but we have seen that results do not change much as long as λ
is not too small (> 10−8). Note that we do not show results for the MMC algorithm when K = 5
since this algorithm is specially designed for problems with K = 2. It would be interesting to com-
pare to the extension for multi-class problems proposed by Zhang et al. [24]. On both examples  we
are signiﬁcantly better than Diffrac  k-means and MMC. We show in Figure 4 additional examples
which are non linearly separable.

6 Conclusion

We have presented a probabilistic model for supervised dimension reduction  together with associ-
ated optimization tools to improve upon EM. Application to text classiﬁcation has shown that our
model outperforms related ones and we have extended it to unsupervised situations  thus drawing
new links between probabilistic models and discriminative clustering. The techniques presented in
this paper could be extended in different directions: First  in terms of optimization  while the embed-
ding of the problem to higher dimensions has empirically led to ﬁnding better local minima  sharp
statements might be made to characterize the robustness of our approach. In terms of probabilistic
models  such techniques should generalize to other latent variable models. Finally  some additional
structure could be added to the problem to take into account more speciﬁc problems  such as multiple
instance learning [25]  multi-label learning or discriminative clustering for computer vision [26  27].

Acknowledgments. This paper was partially supported by the Agence Nationale de la Recherche
(MGA Project) and the European Research Council (SIERRA Project). We would like to thank Toby
Dylan Hocking  for his help on the comparison with other methods for the classiﬁcation task.

8

References

[1] David M. Blei  Andrew Y. Ng  Michael I. Jordan  and John Lafferty. Latent Dirichlet Allocation. Journal

of Machine Learning Research  3  2003.

[2] David M. Blei and Jon D. Mcauliffe. Supervised topic models.

In Advances in Neural Information

Processing Systems (NIPS)  2007.

[3] R. A. Jacobs  M. I. Jordan  S. J. Nowlan  and G. E. Hinton. Adaptive mixtures of local experts. Neural

Computation  3(1):79–87  1991.

[4] H. Larochelle and Y. Bengio. Classiﬁcation using discriminative restricted boltzmann machines.

In

Proceedings of the international conference on Machine learning (ICML)  2008.

[5] Linli Xu  James Neufeld  Bryce Larson  and Dale Schuurmans. Maximum margin clustering. In Advances

in Neural Information Processing Systems (NIPS)  2004.

[6] Linli Xu. Unsupervised and semi-supervised multi-class support vector machines. In AAAI  2005.

[7] F. Bach and Z. Harchaoui. Diffrac : a discriminative and ﬂexible framework for clustering. In Advances

in Neural Information Processing Systems (NIPS)  2007.

[8] N. Quadrianto  T. Caetano  J. Lim  and D. Schuurmans. Convex relaxation of mixture regression with

efﬁcient algorithms. In Advances in Neural Information Processing Systems (NIPS)  2009.

[9] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504  2006.

[10] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer-Verlag  2001.

[11] David R Hunter and Kenneth Lange. A tutorial on MM algorithms. The American Statistician  58(1):30–

37  February 2004.

[12] J Shawe-Taylor and N Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ Press  2004.

[13] Gene H. Golub and Charles F. Van Loan. Matrix computations. Johns Hopkins University Press  3rd

edition  October 1996.

[14] Kurt Anstreicher and Samuel Burer. D.C. versus copositive bounds for standard QP. Journal of Global

Optimization  33(2):299–312  October 2005.

[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press  2004.

[16] Samuel Burer. Optimizing a polyhedral-semideﬁnite relaxation of completely positive programs. Mathe-

matical Programming Computation  2(1):1–19  March 2010.

[17] M. Journ´ee  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization for semideﬁnite convex

problems. volume 20  pages 2327–2351. SIAM Journal on Optimization  2010.

[18] A. Berman and N. Shaked-Monderer. Completely Positive Matrices. World Scientiﬁc Publishing Com-

pany  2003.

[19] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1995.

[20] P. Brucker. An O(n) algorithm for quadratic knapsack problems. In Journal of Optimization Theory and

Applications  volume 134  pages 549–554  1984.

[21] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming  version 1.21.

http://cvxr.com/cvx  August 2010.

[22] Patrick L. Combettes. Solving monotone inclusions via compositions of nonexpansive averaged operators.

Optimization  53:475–504  2004.

[23] Simon Lacoste-Julien. Discriminative Machine Learning with Structure. PhD thesis  University of Cali-

fornia  Berkeley  2009.

[24] Kai Zhang  Ivor W. Tsang  and James T. Kwok. Maximum margin clustering made practical. In Proceed-

ings of the international conference on Machine learning (ICML)  2007.

[25] Thomas G. Dietterich and Richard H. Lathrop. Solving the multiple-instance problem with axis-parallel

rectangles. Artiﬁcial Intelligence  89:31–71  1997.

[26] P. Felzenszwalb  D. Mcallester  and D. Ramanan. A discriminatively trained  multiscale  deformable part

model. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)  2008.

[27] A. Joulin  F. Bach  and J. Ponce. Discriminative clustering for image co-segmentation. In Proceedings of

the Conference on Computer Vision and Pattern Recognition (CVPR)  2010.

9

,Julien Mairal
Michael Habeck