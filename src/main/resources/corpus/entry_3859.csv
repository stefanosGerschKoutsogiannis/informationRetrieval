2010,Occlusion Detection and Motion Estimation with Convex Optimization,We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that  under standard assumptions of Lambertian reflection and static illumination  the task can be posed as a convex minimization problem. Therefore  the solution  computed using efficient algorithms  is guaranteed to be globally optimal  for any number of independently moving objects  and any number of occlusion layers. We test the proposed algorithm on benchmark datasets  expanded to enable evaluation of occlusion detection performance.,Occlusion Detection and Motion Estimation

with Convex Optimization

Alper Ayvaci  Michalis Raptis  Stefano Soatto

University of California  Los Angeles
{ayvaci  mraptis  soatto}@cs.ucla.edu

Abstract

We tackle the problem of simultaneously detecting occlusions and estimating op-
tical ﬂow. We show that  under standard assumptions of Lambertian reﬂection
and static illumination  the task can be posed as a convex minimization problem.
Therefore  the solution  computed using efﬁcient algorithms  is guaranteed to be
globally optimal  for any number of independently moving objects  and any num-
ber of occlusion layers. We test the proposed algorithm on benchmark datasets 
expanded to enable evaluation of occlusion detection performance.

1

Introduction

Optical ﬂow refers to the deformation of the domain of an image that results from ego- or scene
motion. It is  in general  different from the motion ﬁeld  that is the projection onto the image plane
of the spatial velocity of the scene [28]  unless three conditions are satisﬁed: (a) Lambertian re-
ﬂection  (b) constant illumination  and (c) constant visibility properties of the scene. Most surfaces
with benign reﬂectance properties (diffuse/specular) can be approximated as Lambertian almost ev-
erywhere under sparse illuminants (e.g.  the sun). In any case  widespread violation of Lambertian
reﬂection does not enable correspondence [23]  so we will embrace (a) as customary. Similarly  (b)
constant illumination is a reasonable assumption for ego-motion (the scene is not moving relative to
the light source)  and even for objects moving (slowly) relative to the light source.1 Assumption (c)
is the most critical  as it is needed for the motion ﬁeld to be deﬁned.2 It is often taken for granted in
the optical ﬂow literature  because in the limit where two images are sampled inﬁnitesimally close in
time  there are no occluded regions  and one can focus solely on motion discontinuities. Thus  most
variational motion estimation approaches provide an estimate of a dense ﬂow ﬁeld at each location
on the image domain  including occluded regions. Alas  in occluded regions  the problem is not that
optical ﬂow is discontinuous  or forward-backward inconsistent; it is simply not deﬁned. Motion in
occluded regions can be hallucinated; However  whatever motion is assigned to an occluded region
cannot be validated from the data. In defense of these methods  it can be argued that  even without
taking the limit  for small parallax (slow-enough motion  or far-enough objects  or fast-enough tem-
poral sampling) occluded areas are small. However  small does not mean unimportant  as occlusions
are critical to perception [8] and a key for developing representations for recognition [22].
For this reason  we focus on issues of visibility in optical ﬂow computation. We show that forgoing
assumption (c) and explicitly representing occlusions is not only conceptually correct  but also al-
gorithmically advantageous  for the resulting optimization problem can be shown to become convex
once occlusions are explicitly modeled. Therefore  one can guarantee convergence to a globally

1Assumption (b) is also made for convenience  as modeling illumination changes would require modeling

reﬂectance  which signiﬁcantly complicates the picture.

2If the domain of an image portrays a portion of the scene that is not visible in another image  the two cannot

be put into correspondence.

1

optimal solution regardless of initial conditions (sect. 2). We adapt Nesterov’s efﬁcient optimization
scheme to our problem (sect. 3)  and test the resulting algorithm on benchmark datasets (sect. 4) 
including evaluation of occlusion detection (sect. 1.2).

1.1 Related Work

The most common approach to handling occlusions in the optical ﬂow literature is to deﬁne them as
regions where forward and backwards motion estimates are inconsistent [19  1]. Most approaches
return estimates of motion in the occluded regions  where they cannot be invalidated: As we have
already pointed out  in an occluded region one cannot determine a motion ﬁeld that maps one image
onto another  because the scene is not visible in one of the two. Some approaches [11  4]  while also
exploiting motion symmetry  discount occlusions by weighting the data ﬁdelity with a monotonically
decreasing function. The resulting problem is non-convex  and therefore the proposed alternating
minimization techniques can be prone to local minima. An alternate approach [15  14  25] is to
formulate joint motion estimation and occlusion detection in a discrete setting  where it is NP-
hard. Various approximate solutions using combinatorial optimization require ﬁne quantization and 
therefore  suffer from a large number of labels which results in loose approximation bounds. Another
class of methods uses the motion estimation residual to classify a location as occluded or visible
wither with a direct threshold on the residual [30] or with a more elaborate probabilistic model [24].
In each case  the resulting optimization is non-convex.

1.2 Evaluation

Optical ﬂow estimation is a mature area of computer vision  and benchmark datasets have been de-
veloped  e.g.  [2]. Unfortunately  no existing benchmark provides ground truth for occluded regions 
nor a scoring mechanism to evaluate occlusion detection performance. Motion estimates are scored
even in the occluded regions  where the data does not support them. Since our primary goal is to
detect occlusions  we have produced a new benchmark by taking a subset of the training data in the
Middlebury dataset  and hand-labeled occluded regions. We then use the same evaluation method
of the Middlebury for the (ground truth) regions that are co-visible in at least two images. This
provides a motion estimation score. Then  we provide a separate score for occlusion detection  in
terms of precision-recall curves.

2 Joint Occlusion Detection and Optical Flow Estimation

In this section  we show how the assumptions (a)-(b) can be used to formulate occlusion detection
and optical ﬂow estimation as a joint optimization problem. Let I : D ⊂ R2 × R+ → R+; (x  t) (cid:55)→
I(x  t) be a grayscale time-varying image deﬁned on a domain D. Under the assumptions (a)-(b) 
the relation between two consecutive frames in a video {I(x  t)}T

(cid:26)I(w(x  t)  t + dt) + n(x  t)  x ∈ D\Ω(t; dt)

t=0 is given by

I(x  t) =

ρ(x  t)  x ∈ Ω(t; dt)

(1)

where w : D × R+ → R2; x (cid:55)→ w(x  t)
.
= x + v(x  t) is the domain deformation mapping
I(x  t) onto I(x  t + dt) everywhere except at occluded regions. Usually optical ﬂow denotes the
= w(x  t) − x. The occluded region Ω can change over time
.
incremental displacement v(x  t)
depending on the temporal sampling interval dt and is not necessarily simply-connected; so even if
we call Ω the occluded region (singular)  it is understood that it can be made of several disconnected
portions. Inside Ω  the image can take any value ρ : Ω × R+ → R+ that is in general unrelated to
I(w(x)  t + dt)|x∈Ω. In the limit dt → 0  Ω(t; dt) = ∅. Because of (almost-everywhere) continuity
of the scene and its motion (i)  and because the additive term n(x  t) compounds the effects of a
large number of independent phenomena3 and therefore we can invoke the Law of Large Numbers
(ii)  in general we have that

Ω(t; dt) = ∅  and (ii) n IID∼ N (0  λ)

(i)

lim
dt→0

(2)

3n(x  t) collects all unmodeled phenomena including deviations from Lambertian reﬂection  illumination
changes  quantization error  sensor noise  and later also linearization error. It does not capture occlusions  since
those are explicitly modeled.

2

(cid:26)n(x  t)  x ∈ D\Ω

i.e.  the additive uncertainty is normally distributed in space and time with an isotropic and small
variance λ > 0. We deﬁne the residual e : D → R on the entire image domain x ∈ D  via

= I(x  t) − I(w(x  t)  t + dt) =
.

e(x  t; dt)

(3)
which we can write as the sum of two terms  e1 : D → R and e2 : D → R  also deﬁned on the
entire domain D in such a way that

ρ(x  t) − I(w(x  t)  t + dt) 

x ∈ Ω

(4)
Note that e2 is undeﬁned in Ω  and e1 is undeﬁned in D\Ω  in the sense that they can take any value
there  including zero  which we will assume henceforth. We can then write  for any x ∈ D 

x ∈ D\Ω.

e2(x  t; dt)

= ρ(x  t) − I(w(x  t)  t + dt)  x ∈ Ω
.
.
= n(x  t) 

(cid:26)e1(x  t; dt)

I(x  t) = I(w(x  t)  t + dt) + e1(x  t; dt) + e2(x  t; dt)

(5)
and note that  because of (i) e1 is large but sparse 4 while because of (ii) e2 is small but dense4. We
will use this as an inference criterion for w  seeking to optimize a data ﬁdelity term that minimizes
the number of nonzero elements of e1 (a proxy of the area of Ω)  and the negative log-likelihood of
n.

(6)

= (cid:107)e1(cid:107)L0(D) +
.

(cid:107)e2(cid:107)L2(D)

ψdata(w  e1)

subject to (5)
(cid:107)I(x  t) − I(w(x  t)  t + dt) − e1(cid:107)L2(D) + (cid:107)e1(cid:107)L0(D)

=
= |{x ∈ D|f (x) (cid:54)= 0}| and (cid:107)f(cid:107)L2(D)
.

= (cid:82)

.

1
λ

1
λ

D |f (x)|2dx. Unfortunately  we do
where (cid:107)f(cid:107)L0(D)
not know anything about e1 other than the fact that it is sparse  and that what we are looking for is
χ(Ω) ∝ e1  where χ : D → R+ is the characteristic function that is non-zero when x ∈ Ω  i.e. 
where the occlusion residual is non-zero. So  the data ﬁdelity term depends on w but also on the
characteristic function of the occlusion domain Ω.5 For a sufﬁciently small dt  we can approximate 
for any x ∈ D\Ω 

(9)
where the linearization error has been incorporated into the uncertainty term n(x  t). Therefore 
following the same previous steps  we have

I(x  t + dt) = I(x  t) + ∇I(x  t)v(x  t) + n(x  t)

ψdata(v  e1) = (cid:107)∇Iv + It − e1(cid:107)L2(D) + λ(cid:107)e1(cid:107)L0(D).

(10)

Since we typically do not know the variance λ of the process n  we will treat it as a tuning param-
eter  and because ψdata or λψdata yield the same minimizer  we have attributed the multiplier λ to
the second term. In addition to the data term  because the unknown v is inﬁnite-dimensional and
the problem is ill-posed  we need to impose regularization  for instance by requiring that the total
variation (TV) be small

(11)
where v1 and v2 are the ﬁrst and second components of the optical ﬂow v  µ is a multiplier factor to
weight the strength of the regularizer and the weighted isotropic TV norm is deﬁned by

ψreg(v) = µ(cid:107)v1(cid:107)T V + µ(cid:107)v2(cid:107)T V

(cid:107)f(cid:107)T V (D) =

(g1(x)∇xf (x))2 + (g2(x)∇yf (x))2dx 

(cid:90)

(cid:113)

D

4Sparse stands for almost everywhere zero on D. Similarly  dense stands for almost everywhere non-zero.
5In a digital image  both domains D and Ω are discretized into a lattice  and dt is ﬁxed. Therefore  spatial
and temporal derivative operators are approximated  typically  by ﬁrst-order differences. We use the formal
notation

(cid:18)
(cid:18)

 I

I

x +

x +

(cid:20) 1
(cid:20) 0

0

1

(cid:21)
(cid:21)

(cid:19)
(cid:19)

  t

  t

T

− I(x  t)
− I(x  t)

∇I(x  t)

.
=

It(x  t)

= I(x  t + dt) − I(x  t).
.

(7)

(8)

3

where g1(x) ≈ exp(−β|∇xI(x)|) and g2(x) ≈ exp(−β|∇yI(x)|); β is a normalizing factor. TV
is desirable in the context of occlusion detection because it does not penalize motion discontinuities
signiﬁcantly. The overall problem can then be written as the minimization of the cost functional
ψ = ψdata + ψreg  which is

ˆv1  ˆv2  ˆe1 = arg min
v1 v2 e1

(cid:124)
(cid:125)
(cid:107)∇Iv + It − e1(cid:107)2L2(D) + λ(cid:107)e1(cid:107)L0(D) + µ(cid:107)v1(cid:107)T V (D) + µ(cid:107)v2(cid:107)T V (D)

(cid:123)(cid:122)

ψ(v1 v2 e1)

(12)
In a digital image  the domain D is quantized into an M × N lattice Λ  so we can write (12) in
matrix form as:

ˆv1  ˆv2  ˆe1 = arg min
v1 v2 e1

1
2

(cid:107)A[v1  v2  e1]T + b(cid:107)2

(cid:96)2

+ λ(cid:107)e1(cid:107)(cid:96)0 + µ(cid:107)v1(cid:107)T V + µ(cid:107)v2(cid:107)T V

(13)

where e1 ∈ RM N is the vector obtained from stacking the values of e1(x  t) on the lattice Λ on
top of one another (column-wise)  and similarly with the vector ﬁeld components {v1(x  t)}x∈Λ
and {v2(x  t)}x∈Λ stacked into M N-dimensional vectors v1  v2 ∈ RM N . The spatial deriva-
tive matrix A is given by A = [diag(∇xI) diag(∇yI) − I]  where I is the M N × M N
identity matrix  and the temporal derivative values {It(x  t)}x∈Λ are stacked into b. For ﬁnite-
(cid:54)= 0}| and (cid:107)u(cid:107)T V =

dimensional vectors u ∈ RM N   (cid:107)u(cid:107)(cid:96)2 = (cid:112)(cid:104)u  u(cid:105)  (cid:107)u(cid:107)(cid:96)0 = |{ui|ui
(cid:80)(cid:112)((g1)i(ui+1 − ui))2 + ((g2)i(ui+M − ui))2 where g1 and g2 are the stacked versions of

{g1(x)}x∈Λ and {g2(x)}x∈Λ.
In practice  (13) is NP-hard. Therefore  as customary  we relax it by minimizing the weighted-(cid:96)1
norm of e1  instead of (cid:96)0  such that

(cid:107)A[v1  v2  e1]T + b(cid:107)2

(cid:96)2

1
2

(14)

+ λ(cid:107)W e1(cid:107)(cid:96)1 + µ(cid:107)v1(cid:107)T V + µ(cid:107)v2(cid:107)T V

ˆv1  ˆv2  ˆe1 = arg min
v1 v2 e1

where W is a diagonal weight matrix and (cid:107)u(cid:107)(cid:96)1 =(cid:80)|ui|. When W is the identity  (14) becomes a

standard convex relaxation of (13) and its globally optimal solution can be reached efﬁciently [27].
However  the (cid:96)0 norm can also be approximated by reweighting (cid:96)1  as proposed by Candes et al. [5] 
by setting the diagonal elements of W to wi ≈ 1/(|(e1)i| + )   small  after each iteration of (14).
The data term of the standard (unweighted) relaxation of (13) can be interpreted as a Huber norm
[10]. We favor the more general (14) as the resulting estimate of e1 is more stable and sparse.
The model (9) is valid to the extent in which dt is sufﬁciently small relative to v (or v sufﬁciently
slow relative to dt)  so the linearization error does not alter the statistics of the residual n. When this
is not the case  remedies must be enacted to restore proper sampling conditions [22] and therefore
differentiate contributions to the residual coming from sampling artifacts (aliasing)  rather than oc-
clusions. This can be done by solving (14) in scale-space  as customary  with coarser scales used to
initialize ˆv1  ˆv2 so the increment is properly sampled  and the occlusion term e1 added at the ﬁnest
scale.
The residual term e1 in (5) have been characterized in some literature as modeling illumination
changes [21  16  26  13]. Note that  even if the model (5) appears similar  the priors on e1 are rather
different: Sparsity in our case  smoothness in theirs. While sparsity is clearly motivated by (i)  for
illumination changes to be properly modeled  a reﬂectance function is necessary  which is absent in
all models of the form (5) (see [23].)

3 Optimization with Nesterov’s Algorithm

In this section  we describe an efﬁcient algorithm to solve (14) based on Nesterov’s ﬁrst order scheme
[17] which provides O(1/k2) convergence in k iterations  whereas for standard gradient descent  it
is O(1/k)  a considerable advantage for a large scale problem such as (14). To simplify the notation
we let (e1)i

= [diag(∇xI) diag(∇yI) −W −1]. We then have
.

.
= wi(e1)i  so that A

4

Initialize v0

1  v0

2  e0

1. For k ≥ 0
1   vk

2   ek
1 )

1. Compute ∇ψ(vk
2. Compute αk = 1/2(k + 1)  τk = 2/(k + 3)
1 ]T − (1/L)∇ψ(vk

3. Compute yk = [vk

1   vk

2   ek

1]T − (1/L)(cid:80)k

2  e0

1  v0
1 ]T = τkzk + (1 − τk)yk.

1   vk

1 ) 
2   ek
i=0 αi∇ψ(vi

1  vi

2  ei

1) 

4. Compute zk = [v0

5. Update [vk

1   vk

2   ek

Stop when the solution converges.

ψ(v1  v2  e1) = ψ1(v1  v2  e1) + λψ2(e1) + µψ3(v1) + µψ4(v2) 

In order to implement this scheme  we need to address the nonsmooth nature of (cid:96)1 in the computation
of ∇ψ [18]  a common problem in sparse optimization [3]. We write ψ(v1  v2  e1) as
and compute the gradient of each term separately. ∇v1 v2 e1ψ1(v1  v2  e1) is straightforward:
The other three terms require smoothing. ψ2(e1) = (cid:107)e1(cid:107)(cid:96)1 can be rewritten as ψ2(e1) =
max(cid:107)u(cid:107)∞≤1 (cid:104)u  e1(cid:105) in terms of its conjugate. [18] proposes a smooth approximation

∇v1 v2 e1ψ1(v1  v2  e1) = AT A[v1  v2  e1]T + AT b.

2 (e1) = max
(cid:107)u(cid:107)∞≤1
and shows that (15) is differentiable and ∇e1 ψσ

ψσ

(cid:26)σ−1(e1)i 

uσ
i =

|(e1)i| < σ 
sgn((e1)i)  otherwise.

(cid:104)u  e1(cid:105) − 1
2

σ(cid:107)u(cid:107)2

(cid:96)2

 

2 (e1) = uσ  where uσ is the solution of (15):

(15)

(16)

(17)

(18)

Following [3]  ∇v1ψ3 is given by ∇v1 ψσ
weighted horizontal and vertical differentiation operators   and uσ has the form [u1  u2] where

3 (v1) = GT uσwhere G = [G1  G2]T   G1 and G2 are

(cid:26)σ−1(G1 2v1)i 

u1 2
i =

(cid:107)[(G1v1)i (G2v1)i]T(cid:107)−1

(cid:96)2

(G1 2v1)i 

∇v2 ψ4 can be computed in the same way. Once we have computed each term  ∇ψ(v1  v2  e1) is

∇ψ(v1  v2  e1) = ∇ψ1 + [λ∇e1ψ2  µ∇v1 ψ3  µ∇v2ψ4]T .

(cid:107)[(G1v1)i (G2v1)i]T(cid:107)(cid:96)2 < σ 
otherwise.

We also need the Lipschitz constant L to compute the auxiliary variables yk and zk to minimize ψ.
Since (cid:107)GT G(cid:107)2 is bounded above [7] by 8  given the coefﬁcients λ and µ  L is given by

L = max(λ  8µ)/σ + (cid:107)AT A(cid:107)2.

A crucial element of the scheme is the selection of σ. It trades off accuracy and speed of conver-
gence. A large σ yields a smooth solution  which is undesirable when minimizing the (cid:96)1 norm. A
small σ causes slow convergence. We have chosen σ empirically  although the continuation algo-
rithm proposed in [3] could be employed to adapt σ during convergence.

4 Experiments

To evaluate occlusion detection (Sect. 1.2)  we start from [2] and generate occlusion maps as fol-
lows: for each training sequence  the residual computed from the given ground truth motion is used
as a discriminant to determine ground truth occlusions  ﬁxing obvious errors in the occlusion maps
by hand. We therefore restrict the evaluation of motion to the co-visible regions  and evaluate oc-
clusion detection as a standard binary classiﬁcation task. We compare our algorithm to [29] and
[14]  the former is an example of robust motion estimation and the latter is a representative of the
approaches described in Sect. 1.1.
In our implementation6  we ﬁrst solve (14) with standard relaxation (W is the identity) and then
with reweighted-(cid:96)1. To handle large motion  we use a pyramid with scale factor 0.5 and up to 4
levels; λ and µ are ﬁxed at 0.002 and 0.001 (Flower Garden) and 0.0006 and 0.0003 (Middlebury)
respectively. To make comparison with [29] fair  we modify the code provided online7 to include

6The source code is available at http://vision.ucla.edu/~ayvaci/occlusion-detection/
7http://gpu4vision.icg.tugraz.at

5

anisotropic regularization (Fig. 1). Note that no occlusion is present in the residual of the motion
ﬁeld computed by TV-L1  and subsequently the motion estimates are less precise around occluding
boundaries (top-left corner of the Flower Garden  plane in the left in Venus).

Figure 1: Comparison with TV-L1 [29] on “Venus” from [2] and “Flower Garden.” The ﬁrst column
shows the motion estimates by TV-L1  color-coded as in [29]  the second its residual I(x  t) −
I(w(x)  t + dt); the third shows our motion estimates  and the fourth our residual e1 deﬁned in (14).

Other frames of the Flower Garden sequence are shown in Fig. 2  where we have regularized the
occluded region by minimizing a unilateral energy on e1 with graph-cuts. We have also compared

Figure 2: Motion estimates for more frames of the Flower Garden sequence (left)  residual e (mid-
dle)  and occluded region (right).

motion estimates obtained with our method and [29] in the co-visible regions for the Middlebury
dataset (Table 1). Since occlusions can only be determined at the ﬁnest scale absent proper sam-
pling conditions  in this experiment we minimize the same functional of [29] at coarse scales  and
switch to (14) at the ﬁnest scale. To evaluate occlusion detection performance  we again use the
Middlebury  and compare e1 to ground truth occlusions using precision/recall curves (Fig. 3) and
average precision values (Table 2). We also show the improvement in detection performance when
we use reweighted-(cid:96)1  in Table 2. We have compared our occlusion detection results to [14]  us-
ing the code provided online by the authors (Table 3). Comparing motion estimates gives an unfair

6

Venus RubberWhale Hydrangea Grove2 Grove3 Urban2 Urban3
6.41
4.37
7.12
5.28
0.30
0.84
0.89
0.33

AAE (ours)
AAE (L1TV)
AEPE (ours)
AEPE (L1TV)
Table 1: Quantitative comparison of our algorithm with TV-L1 [29]. Average Angular Error (AAE)
and Average End Point Error (AEPE) of motion estimates in co-visible regions.

2.35
2.44
0.19
0.20

5.42
4.49
0.18
0.13

2.32
3.45
0.16
0.24

5.72
7.66
0.59
0.74

3.60
3.57
0.39
0.46

Figure 3: Left to right: Representative samples of motion estimates from the Middlebury dataset 
labeled ground-truth occlusions  error term estimate e1  and precision-recall curves for our occlusion
detection.

advantage to our algorithm because their approach is based on quantized disparity values  yielding
lower accuracy.

7

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionVenus00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionRubberWhale00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionHydrangea00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionGrove200.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionGrove300.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionUrban2Venus Rubber Whale Hydrangea Grove2 Grove3 Urban2 Urban3
0.67
0.69

0.80
(cid:96)1
reweighted-(cid:96)1
0.80
Table 2: Average precision of our approach on Middlebury data with and without re-weighting.

0.55
0.57

0.70
0.70

0.60
0.61

0.72
0.73

0.48
0.49

It takes 186 seconds for a Matlab/C++ implementation of Nesterov’s algorithm to converge to a
solution on a 288 × 352 frame from Flower Garden sequence. We have also compared Nesterov’s
algorithm to split-Bregman’s method [9] for minimization of (14) in terms of convergence speed and
reported the results in [20].

Venus RubberWhale Hydrangea Grove2 Grove3 Urban2 Urban3
0.61
0.66
0.69

Precision [14]
Recall [14]
Precision(ours)
Table 3: Comparison with [14] on Middlebury. Since Kolmogorov et al. provide a binary output 
we display our precision at their same recall value.

0.79
0.45
0.86

0.68
0.20
0.96

0.26
0.50
0.95

0.56
0.51
0.94

0.72
0.55
0.96

0.46
0.20
0.91

5 Discussion

We have presented an algorithm to detect occlusions and establish correspondence between two im-
ages. It leverages on a formulation that  starting from standard assumptions (Lambertian reﬂection 
constant diffuse illumination)  arrives at a convex optimization problem. Our approach does not as-
sume a rigid scene  nor a single moving object. It also does not assume that the occluded region
is simply connected: Occlusions in natural scenes can be very complex (see Fig. 3) and should
therefore  in general  not be spatially regularized. The fact that occlusion detection reduces to a
two-phase segmentation of the domain into either occluded (Ω) or visible (D\Ω) should not confuse
the reader familiar with the image segmentation literature whereby two-phase segmentation of one
object (foreground) from the background can be posed as a convex optimization problem [6]  but
breaks down in the presence of multiple objects  or “phases.” Note that in [6] the problem can be
made convex only in e1  but not jointly in e1 and v. We focus on inter-frame occlusion detection;
temporal consistency of occlusion “layers” was addressed in [12].
The limitations of our approach stand mostly in its dependency from the regularization coefﬁcients
λ and µ. In the absence of some estimate of the variance coefﬁcient λ  one is left with tuning it by
trial-and-error. Similarly  µ is a parameter that  like in any classiﬁcation problem  trades off missed
detections and false alarms  and therefore no single value is “optimal” in any meaningful sense.
These limitations are shared by most variational optical ﬂow estimation algorithms.
Acknowledgement: This work was supported by AFOSR FA9550-09-1-0427  ARO 56765-CI  and
ONR N00014-08-1-0414.

References
[1] L. Alvarez  R. Deriche  T. Papadopoulo  and J. S´anchez. Symmetrical dense optical ﬂow estimation with

occlusions detection. International Journal of Computer Vision  75(3):371–385  2007.

[2] S. Baker  D. Scharstein  J. Lewis  S. Roth  M. Black  and R. Szeliski. A database and evaluation method-
ology for optical ﬂow. In Proceedings of the International Conference on Computer Vision  volume 5 
2007.

[3] S. Becker  J. Bobin  and E. Candes. Nesta: A fast and accurate ﬁrst-order method for sparse recovery.

Arxiv preprint arXiv  904  2009.

[4] R. Ben-Ari and N. Sochen. Variational stereo vision with sharp discontinuities and occlusion handling.

ICCV. IEEE Computer Society  pages 1–7  2007.

[5] E. Candes  M. Wakin  and S. Boyd. Enhancing sparsity by reweighted 1 minimization. Journal of Fourier

Analysis and Applications  14(5):877–905  2008.

[6] T. Chan  S. Esedoglu  and M. Nikolova. Algorithms for ﬁnding global minimizers of denoising and

segmentation models. SIAM J. Appl. Math  66(1632-1648):1  2006.

8

[7] J. Dahl  P. Hansen  S. Jensen  and T. Jensen. Algorithms and software for total variation image recon-

struction via ﬁrst-order methods. Numerical Algorithms  pages 67–92  2009.

[8] J. J. Gibson. The ecological approach to visual perception. LEA  1984.
[9] T. Goldstein and S. Osher. The split Bregman method for L1 regularized problems. SIAM Journal on

Imaging Sciences  2(2):323–343  2009.

[10] P. Huber and E. Ronchetti. Robust statistics. John Wiley & Sons Inc  2009.
[11] S. Ince and J. Konrad. Occlusion-aware optical ﬂow estimation. IEEE Transactions on Image Processing 

17(8):1443–1451  2008.

[12] J. Jackson  A. J. Yezzi  and S. Soatto. Dynamic shape and appearance modeling via moving and deforming

layers. International Journal of Computer Vision  2008.

[13] Y. Kim  A. Mart´ınez  and A. Kak. Robust motion estimation under varying illumination.

Vision Computing  23(4):365–375  2005.

Image and

[14] V. Kolmogorov and R. Zabih. Computing visual correspondence with occlusions via graph cuts.

International Conference on Computer Vision  volume 2  pages 508–515. Citeseer  2001.

In

[15] K. Lim  A. Das  and M. Chong. Estimation of occlusion and dense motion ﬁelds in a bidirectional
Bayesian framework. IEEE Transactions on Pattern Analysis and Machine Intelligence  pages 712–718 
2002.

[16] S. Negahdaripour. Revised deﬁnition of optical ﬂow: Integration of radiometric and geometric cues for
dynamic scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence  pages 961–
979  1998.

[17] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O

(1/k2). In Doklady AN SSSR  volume 269  pages 543–547  1983.

[18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming  103(1):127–

152  2005.

[19] M. Proesmans  L. Van Gool  and A. Oosterlinck. Determination of optical ﬂow and its discontinuities

using a non-linear diffusion. In European Conference on Computer Vision  1994.

[20] M. Raptis  A. Ayvaci  and S. Soatto. Occlusion Detection and Motion Estimation via Convex Optimiza-

tion. Technical report  UCLA CAM 10-36  June 2010.

[21] D. Shulman and J. Herve. Regularization of discontinuous ﬂow ﬁelds. In Proc. of Workshop on Visual

Motion  pages 81–86  1989.

[22] S. Soatto. Steps Towards a Theory of Visual Information. Technical report  UCLA-CSD100028  Septem-

ber 2010.

[23] S. Soatto  A. J. Yezzi  and H. Jin. Tales of shape and radiance in multiview stereo. In Intl. Conf. on Comp.

Vision  pages 974–981  October 2003.

[24] C. Strecha  R. Fransens  and L. Van Gool. A probabilistic approach to large displacement optical ﬂow

and occlusion detection. In ECCV Workshop SMVP  pages 71–82. Springer  2004.

[25] J. Sun  Y. Li  S. Kang  and H. Shum. Symmetric stereo matching for occlusion handling.

Conference on Computer Vision and Pattern Recognition  volume 2  page 399  2005.

In IEEE

[26] C. Teng  S. Lai  Y. Chen  and W. Hsu. Accurate optical ﬂow computation under non-uniform brightness

variations. Computer vision and image understanding  97(3):315–346  2005.

[27] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  58(1):267–288  1996.

[28] A. Verri and T. Poggio. Motion ﬁeld and optical ﬂow: Qualitative properties.

Pattern Analysis and Machine Intelligence  11(5):490–498  1989.

IEEE Transactions on

[29] A. Wedel  T. Pock  C. Zach  H. Bischof  and D. Cremers. An improved algorithm for TV-L1 optical ﬂow.
In Statistical and Geometrical Approaches to Visual Motion Analysis: International Dagstuhl Seminar.
Springer  2009.

[30] J. Xiao  H. Cheng  H. Sawhney  C. Rao  M. Isnardi  et al. Bilateral ﬁltering-based optical ﬂow estimation

with occlusion detection. Lecture Notes in Computer Science  3951:211  2006.

9

,Ayan Chakrabarti
Mark Bun
Gautam Kamath
Thomas Steinke
Steven Wu