2019,Learning step sizes for unfolded sparse coding,Sparse coding is typically solved by iterative optimization techniques  such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. In this paper  we study the selection of adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However  it is impractical in most large-scale applications. Therefore  we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate that for a large class of unfolded algorithms  if the algorithm converges to the solution of the Lasso  its last layers correspond to ISTA with learned step sizes. Experiments show that our method is competitive with state-of-the-art networks when the solutions are sparse enough.,Learning step sizes for unfolded sparse coding

Pierre Ablin∗   Thomas Moreau∗  Mathurin Massias  Alexandre Gramfort

Inria - CEA

Université Paris-Saclay

{pierre.ablin thomas.moreau mathurin.massias alexandre.gramfort}@inria.fr

Abstract

Sparse coding is typically solved by iterative optimization techniques  such as
the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning
weights of ISTA using neural networks is a practical way to accelerate estimation.
In this paper  we study the selection of adapted step sizes for ISTA. We show
that a simple step size strategy can improve the convergence rate of ISTA by
leveraging the sparsity of the iterates. However  it is impractical in most large-
scale applications. Therefore  we propose a network architecture where only the
step sizes of ISTA are learned. We demonstrate that for a large class of unfolded
algorithms  if the algorithm converges to the solution of the Lasso  its last layers
correspond to ISTA with learned step sizes. Experiments show that our method is
competitive with state-of-the-art networks when the solutions are sparse enough.

1

Introduction

The resolution of convex optimization problems by iterative algorithms has become a key part of
machine learning and signal processing pipelines  in particular with the Generalized Linear Models
for classiﬁcation [Nelder and Wedderburn  1972]. Amongst these problems  special attention has
been devoted to the Lasso [Tibshirani  1996]  due to the attractive sparsity properties of its solution
(see Hastie et al. 2015 for an extensive review). For a given input x ∈ Rn   a dictionary D ∈ Rn×m
and a regularization parameter λ > 0   the Lasso problem is
Fx(z) with Fx(z)   1

z∗(x) ∈ arg min
to solve Problem (1)  e.g. proximal coordinate descent
A variety of algorithms exist
[Tseng  2001  Friedman et al.  2007]  Least Angle Regression [Efron et al.  2004] or proximal
splitting methods [Combettes and Bauschke  2011]. The focus of this paper is on the Iterative
Shrinkage-Thresholding Algorithm (ISTA  Daubechies et al. 2004)  which is a proximal-gradient
method applied to Problem (1). ISTA starts from z(0) = 0 and iterates

2kx − Dzk2 + λkzk1 .

z∈Rm

(1)

z(t+1) = ST(cid:18)z(t) −

1
L

D⊤(Dz(t) − x) 

λ

L(cid:19)  

(2)

where ST is the soft-thresholding operator deﬁned as ST(x  u)   sign(x) max(|x| − u  0)   and L
is the greatest eigenvalue of D⊤D . In the general case  ISTA converges at rate 1/t   which can be
improved to the optimal rate 1/t2 [Nesterov  1983]. However  this optimality stands in the worst
possible case  and linear rates are achievable in practice [Liang et al.  2014].

line of

research to improve the speed of Lasso solvers is to try to iden-
A popular
tify the support of z∗
the optimization prob-
lem [El Ghaoui et al.  2012  Ndiaye et al.  2017  Johnson and Guestrin  2015  Massias et al.  2018].

to diminish the size of

in order

 

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Once the support is identiﬁed  larger steps can also be taken  leading to improved rates for ﬁrst
order algorithms [Liang et al.  2014  Poon et al.  2018  Sun et al.  2019].

However  these techniques only consider the case where a single Lasso problem is solved. When
one wants to solve the Lasso for many samples {xi}N
i=1 – e.g. in dictionary learning [Olshausen and
Field  1997] – it is proposed by Gregor and Le Cun [2010] to learn a T -layers neural network of
parameters Θ   ΦΘ : Rn → Rm such that ΦΘ(x) ≃ z∗(x) . This Learned-ISTA (LISTA) algorithm
yields better solution estimates than ISTA on new samples for the same number of iterations/layers.
This idea has led to a profusion of literature (summarized in Table A.1 in appendix)  and is a popular
approach to solve inverse problems. Recently  it has been hinted by Zhang and Ghanem [2018]  Ito
et al. [2018]  Liu et al. [2019] that only a few well-chosen parameters can be learned while retaining
the performances of LISTA.

In this article  we study strategies for LISTA where only step sizes are learned. In Section 3  we
propose Oracle-ISTA  an analytic strategy to obtain larger step sizes in ISTA. We show that the
proposed algorithm’s convergence rate can be much better than that of ISTA. However  it requires
computing a large number of Lipschitz constants which is a burden in high dimension. This motivates
the introduction of Step-LISTA (SLISTA) networks in Section 4  where only a step size parameter is
learned per layer. As a theoretical justiﬁcation  we show in Theorem 4.4 that the last layers of any
deep LISTA network converging on the Lasso must correspond to ISTA iterations with learned step
sizes. We validate the soundness of this approach with numerical experiments in Section 5.

2 Notation and Framework

Notation The ℓ2 norm on Rn is k · k. For p ∈ [1 ∞]   k · kp is the ℓp norm. The Frobenius matrix
norm is kMkF . The identity matrix of size m is Idm . ST is the soft-thresholding operator. Iterations
are denoted z(t) . λ > 0 is the regularization parameter. The Lasso cost function is Fx . ψα(z  x) is
one iteration of ISTA with step α: ψα(z  x) = ST(z − αD⊤(Dz − x)  αλ) . φθ(z  x) is one iteration
of a LISTA layer with parameters θ = (W  α  β): φθ(z  x) = ST(z − αW ⊤(Dz − x)  βλ) .
The set of integers between 1 and m is J1  mK . Given z ∈ Rm   the support is supp(z) = {j ∈
J1  mK : zj 6= 0} ⊂ J1  mK . For S ⊂ J0  mK  DS ∈ Rn×m is the matrix containing the columns of
D indexed by S. We denote LS  the greatest eigenvalue of D⊤
S DS. The equicorrelation set is E =
j (Dz∗− x)| = λ}. The equiregularization set is B∞ = {x ∈ Rn : kD⊤xk∞ = 1}.
{j ∈ J1  mK : |D⊤
Neural networks parameters are between brackets  e.g. Θ = {α(t)  β(t)}T −1
t=0 . The sign function is
sign(x) = 1 if x > 0  −1 if x < 0 and 0 is x = 0 .
Framework This paragraph recalls some properties of the Lasso. Lemma 2.1 gives the ﬁrst-order
optimality conditions for the Lasso.

Lemma 2.1 (Optimality for the Lasso). The Karush-Kuhn-Tucker (KKT) conditions read

z∗ ∈ arg min Fx ⇔ ∀j ∈ J1  mK  D⊤

j (x − Dz∗) ∈ λ∂|z∗

j | = (cid:26){λ sign z∗
j } 
[−λ  λ] 

if z∗
if z∗

j 6= 0  
j = 0 .

(3)

Deﬁning λmax   kD⊤xk∞   it holds arg min Fx = {0} ⇔ λ ≥ λmax . For some results in Section 3 
we will need the following assumption on the dictionary D:
Assumption 2.2 (Uniqueness assumption). D is such that the solution of Problem (1) is unique for
all λ and x i.e. arg min Fx = {z∗} .
Assumption 2.2 may seem stringent since whenever m > n   Fx is not strictly convex. However  it
was shown in Tibshirani [2013  Lemma 4] – with earlier results from Rosset et al. 2004 – that if D is
sampled from a continuous distribution  Assumption 2.2 holds for D with probability one.
Deﬁnition 2.3 (Equicorrelation set). The KKT conditions motivate the introduction of the equicorre-
lation set E   {j ∈ J1  mK : |D⊤
j = 0   i.e. E contains the
support of any solution z∗ .
When Assumption 2.2 holds  we have E = supp(z∗) [Tibshirani  2013  Lemma 16].

j (Dz∗ − x)| = λ}   since j /∈ E =⇒ z∗

2

We consider samples x in the equiregularization set

(4)
which is the set of x such that λmax(x) = 1 . Therefore  when λ ≥ 1   the solution is z∗(x) = 0 for
all x ∈ B∞   and when λ < 1   z∗(x) 6= 0 for all x ∈ B∞ . For this reason  we assume 0 < λ < 1 in
the following.

B∞   {x ∈ Rn : kD⊤xk∞ = 1}  

3 Better step sizes for ISTA

The Lasso objective is the sum of a L-smooth function  1
2kx − D · k2   and a function with an explicit
proximal operator  λk · k1 . Proximal gradient descent for this problem  with the sequence of step
sizes (α(t)) consists in iterating

z(t+1) = ST(cid:16)z(t) − α(t)D⊤(Dz(t) − x)  λα(t)(cid:17) .

(5)

ISTA follows these iterations with a constant step size α(t) = 1/L . In the following  denote
ψα(z  x)   ST(z − αD⊤(Dz(t) − x)  αλ). One iteration of ISTA can be cast as a majorization-
minimization step [Beck and Teboulle  2009]. Indeed  for all z ∈ Rm  

2kx − Dz(t)k2 + (z − z(t))⊤D⊤(Dz(t) − x) + 1
Fx(z) = 1
2kx − Dz(t)k2 + (z − z(t))⊤D⊤(Dz(t) − x) + L
≤ 1
|

  Qx L(z  z(t))

{z

2kD(z − z(t))k2 + λkzk1
2 kz − z(t)k2 + λkzk1
 
}

(6)

(7)

where we have used the inequality (z − z(t))⊤D⊤D(z − z(t)) ≤ Lkz − z(t)k2 . The minimizer of
Qx L(·  z(t)) is ψ1/L(z(t)  x)  which is the next ISTA step.
Oracle-ISTA: an accelerated ISTA with larger step sizes Since the iterates are sparse  this
approach can be reﬁned. For S ⊂ J1  mK   let us deﬁne the S-smoothness of D as

LS   max

z

z⊤D⊤Dz  s.t. kzk = 1 and supp(z) ⊂ S  

(8)

with the convention L∅ = L . Note that LS is the greatest eigenvalue of D⊤
S DS where DS ∈ Rn×|S|
is the columns of D indexed by S . For all S   LS ≤ L   since L is the solution of Equation (8)
without support constraint. Assume supp(z(t)) ⊂ S . Combining Equations (6) and (8)  we have

∀z s.t. supp(z) ⊂ S  Fx(z) ≤ Qx LS (z  z(t)) .

(9)

The minimizer of the r.h.s is z = ψ1/LS (z(t)  x) . Furthermore  the r.h.s. is a tighter upper bound than
the one given in Equation (7) (see illustration in Figure 1). Therefore  using z(t+1) = ψ1/LS (z(t)  x)
minimizes a tighter upper bound  provided that the following condition holds

supp(z(t+1)) ⊂ S .

(⋆)

Figure 1: Majorization illustration. If z(t) has support
S   Qx LS (·  z(t)) is a tighter upper bound of Fx than
Qx L(·  z(t)) on the set of points of support S .

Oracle-ISTA (OISTA) is an accelerated version of ISTA which leverages the sparsity of the iterates
in order to use larger step sizes. The method is summarized in Algorithm 1. OISTA computes

3

01L1LSStepsizeCostfunctionFxQx L(· z(t))Qx LS(· z(t))Algorithm 1: Oracle-ISTA (OISTA) with larger step sizes
Input: Dictionary D   target x   number of iterations T
z(0) = 0
for t = 0  . . .   T − 1 do

Compute S = supp(z(t)) and LS using an oracle ;
Set y(t+1) = ψ1/LS (z(t)  x) ;
if Condition ⋆ : supp(y(t+1)) ⊂ S then Set z(t+1) = y(t+1) ;
else Set z(t+1) = ψ1/L(z(t)  x) ;

Output: Sparse code z(T )

y(t+1) = ψ1/Ls (z(t)  x)   using the larger step size 1/LS   and checks if it satisﬁes the support
Condition ⋆. When the condition is satisﬁed  the step can be safely accepted. In particular Equation (9)
yields Fx(y(t+1)) ≤ Fx(z(t)) . Otherwise  the algorithm falls back to the regular ISTA iteration
with the smaller step size. Hence  each iteration of the algorithm is guaranteed to decrease Fx . The
following proposition shows that OISTA converges in iterates  achieves ﬁnite support identiﬁcation 
and eventually reaches a safe regime where Condition ⋆ is always true.
Proposition 3.1 (Convergence  ﬁnite-time support identiﬁcation and safe regime). When Assump-
tion 2.2 holds  the sequence (z(t)) generated by the algorithm converges to z∗ = arg min Fx .
Further  there exists an iteration T ∗ such that for t ≥ T ∗   supp(z(t)) = supp(z∗)   S∗ and
Condition ⋆ is always statisﬁed.

Sketch of proof (full proof in Subsection B.1). Using Zangwill’s global convergence theorem [Zang-
will  1969]  we show that all accumulation points of (z(t)) are solutions of Lasso. Since the solution
is assumed unique  (z(t)) converges to z∗ . Then  we show that the algorithm achieves ﬁnite-support
identiﬁcation with a technique inspired by Hale et al. [2008]. The algorithm gets arbitrary close
to z∗   eventually with the same support. We ﬁnally show that in a neighborhood of z∗   the set of
points of support S∗ is stable by ψ1/LS (·  x) . The algorithm eventually reaches this region  and then
Condition ⋆ is true.

It follows that the algorithm enjoys the usual ISTA convergence results replacing L with LS ∗ .
Proposition 3.2 (Rates of convergence). For t > T ∗   Fx(z(t)) − Fx(z∗) ≤ LS ∗
If additionally inf kzk=1 kDS ∗ zk2 = µ∗ > 0   then the convergence rate for t ≥ T ∗ is
Fx(z(t)) − Fx(z∗) ≤ (1 − µ∗
Sketch of proof (full proof in Subsection B.2). After iteration T ∗   OISTA is equivalent to ISTA ap-
plied on Fx(z) restricted to z ∈ S∗ . This function is LS ∗-smooth  and µ∗-strongly convex if µ∗ > 0 .
Therefore  the classical ISTA rates apply with improved condition number.

(Fx(z(T ∗)) − Fx(z∗)) .

LS∗ )t−T ∗

2(t−T ∗)

kz∗−z(T

∗ )k2

.

These two rates are tighter than the usual ISTA rates – in the convex case L kz∗k2
and in the µ-strongly
convex case (1 − µ∗
L )t(Fx(0) − Fx(z∗)) [Beck and Teboulle  2009]. Finally  the same way ISTA
converges in one iteration when D is orthogonal (D⊤D = Idm)  OISTA converges in one iteration if
S∗ is identiﬁed and DS ∗ is orthogonal.
Proposition 3.3. Assume D⊤

S ∗ DS ∗ = LS ∗ Id|S ∗| . Then  z(T ∗+1) = z∗ .

2t

Proof. For z s.t. supp(z) = S∗   Fx(z) = Qx LS (z  z(T ∗)) . Hence  the OISTA step minimizes
Fx .

Quantiﬁcation of the rates improvement in a Gaussian setting The following proposition gives
an asymptotic value for LS

L in a simple setting.

4

Proposition 3.4. Assume that the entries of D ∈ Rn×m are i.i.d centered Gaussian variables with
variance 1 . Assume that S consists of k integers chosen uniformly at random in J1  mK . Assume that
k  m  n → +∞ with linear ratios m/n → γ  k/m → ζ . Then

.

(10)

LS

L → (cid:18) 1 + √ζγ
1 + √γ (cid:19)2

This is a direct application of the Marchenko-Pastur law [Marchenko and Pastur  1967]. The law
is illustrated on a toy dataset in Figure D.1. In Proposition 3.4  γ is the ratio between the number
of atoms and number of dimensions  and the average size of S is described by ζ ≤ 1 . In an
overcomplete setting where we have γ ≫ 1   this yield an approximation of Equation (10) with
LS ≃ ζL . Therefore  if z∗ is very sparse (ζ ≪ 1)  the convergence rates of Proposition 3.2 are much
better than those of ISTA.

Backtracking Line Search A related strategy for ﬁnding good step sizes is the use of backtracking
line search (see for instance Nesterov 2013). The core idea here is to compute iterate candidates for
various step-sizes and choose the one that gives the best cost decrease. This strategy is adaptive to
the actual state of the iterative procedure. However  it requires computing a new step size at each
iteration. At each iteration  BT considers step-sizes of the form (α0βk)k≥0  where α0 is an initial
guess and β < 1 is a shrinking factor. In practice  the hyperparameters α0 and β are critical and hard
to tune. The need to search for a new step-size at each iteration is the main difference with OISTA
which provides a ﬁxed rule (maybe intractable) to set the step size.

Example Figure 2 compares the OISTA  ISTA  FISTA  and backtracking ISTA on a toy problem.
We display two backtracking strategies  with different hyperparameters. We also compare this to a
greedy best step-size approach  where step-sizes are chosen as α(t+1) = arg min Fx(ψα(z(t)  x)).
The improved rate of convergence of OISTA over ISTA and FISTA is illustrated: one can indeed take
greater steps to increase the convergence speed. Further comparisons are displayed in Figure D.2 for
different regularization parameters λ . While this demonstrates a faster rate of convergence  OISTA
requires computing several Lipschitz constants LS   which is cumbersome in high dimension. This
motivates the next section  where we propose to learn those steps.

Figure 2: Convergence curves of OISTA 
ISTA  FISTA  backtracking ISTA and a
greedy best step-size strategy on a toy prob-
lem with n = 10   m = 50   λ = 0.5 . The
bottom ﬁgure displays the (normalized) steps
taken by OISTA and the best steps at each
iteration. Full experimental setup described
in Appendix D.

4 Learning unfolded algorithms

Network architectures At each step  ISTA performs a linear operation to compute an update
in the direction of the gradient D⊤(Dz(t) − x) and then an element-wise non linearity with the
soft-thresholding operator ST . The whole algorithm can be summarized as a recurrent neural network
(RNN)  presented in Figure 3a. Gregor and Le Cun [2010] introduced Learned-ISTA (LISTA)  a
neural network constructed by unfolding this RNN T times and learning the weights associated to each
layer. The unfolded network  presented in Figure 3b  iterates z(t+1) = ST(W (t)
z z(t)  λβ(t)) .
It outputs exactly the same vector as T iterations of ISTA when W (t)
z = Idm − D⊤D
x = D⊤
and β(t) = 1
L . Empirically  this network is able to output a better estimate of the sparse code solution
with fewer operations.

x x+W (t)
L   W (t)

L

5

10−610−12Fx−F∗x050100150NumberofDzcomputations0.02.55.07.5Oraclestep1LISTAFISTAOISTA(proposed)Backtrack1Backtrack2Beststepx

Wx

x

z∗

W (0)

x

W (1)

x

W (2)

x

Wz

W (1)

z

W (2)

z

z(3)

(a) ISTA - Recurrent Neural Network

(b) LISTA - Unfolded network with T = 3

Figure 3: Network architecture for ISTA (left) and LISTA (right).

Due to the expression of the gradient  Chen et al. [2018] proposed to consider only a subclass of
the previous networks  where the weights Wx and Wz are coupled via Wz = Idm −W ⊤
x D . This is
the architecture we consider in the following. A layer of LISTA is a function φθ : Rm × Rn → Rm
parametrized by θ = (W  α  β) ∈ Rn×m × R+

∗ × R+

∗ such that

Given a set of T layer parameters Θ(T ) = {θ(t)}T −1
ΦΘ(T ) (x) = z(T )(x) where z(t)(x) is deﬁned by recursion

φθ(z  x) = ST(z − αW ⊤(Dz − x)  βλ) .

(11)
t=0   the LISTA network ΦΘ(T ) : Rn → Rm is

Taking W = D   α = β = 1

z(0)(x) = 0  and z(t+1)(x) = φθ(t) (z(t)(x)  x) for t ∈ J0  T − 1K .

L yields the same outputs as T iterations of ISTA.

(12)

To alleviate the need to learn the large matrices W (t)  Liu et al. [2019] proposed to use a shared
analytic matrix WALISTA for all layers. The matrix is computed in a preprocessing stage by

WALISTA = arg min

W kW ⊤Dk2

F

s.t.

diag(W ⊤D) = 111m .

(13)

Then  only the parameters (α(t)  β(t)) are learned. This effectively reduces the number of parameters
from (nm + 2) × T to 2 × T . However  we will see that ALISTA fails in our setup.
Step-LISTA With regards to the study on step sizes for ISTA in Section 3  we propose to learn
approximation of ISTA step sizes for the input distribution using the LISTA framework. The resulting
network  dubbed Step-LISTA (SLISTA)  has T parameters ΘSLISTA = {α(t)}T −1
t=0   and follows the
iterations:
z(t+1)(x) = ST(z(t)(x) − α(t)D⊤(Dz(t)(x) − x)  α(t)λ) .

(14)
This is equivalent to a coupling in the LISTA parameters: a LISTA layer θ = (W  α  β) corresponds
to a SLISTA layer if and only if α
β W = D. This network aims at learning good step sizes  like
the ones used in OISTA  without the computational burden of computing Lipschitz constants. The
number of parameters compared to the classical LISTA architecture ΘLISTA is greatly diminished 
making the network easier to train. Learning curves are shown in Figure D.3 in appendix.

Figure 4 displays the learned steps of a SLISTA network on a toy example. The network learns larger
step-sizes as the sparsity (and as a result  1/LS’s) increase. It is interesting to note that the learned
step sizes tends to be larger than 1/LS but smaller than 2/LS. As step sizes in ]0  2/LS[ guarantee
descent of the cost function  SLISTA learns step sizes that are adapted to solve the optimization
problem. Still  steps larger than 2/LS may be suitable depending on the geometry of the problem.
For instance  in Figure 2  the greedy best-steps  that lead to the greatest decrease of the cost function 
are taken larger than 2/LS.

Training the network We consider the framework where the network learns to solve the Lasso on
B∞ in an unsupervised way. Given a distribution p on B∞   the network is trained by solving

˜Θ(T ) ∈ arg min

Θ(T ) L(Θ(T ))   Ex∼p[Fx(ΦΘ(T ) (x))] .

(15)

Most of the literature on learned optimization train the network with a different supervised objective
[Gregor and Le Cun  2010  Xin et al.  2016  Chen et al.  2018  Liu et al.  2019]. Given a set of pairs
(xi  zi)   the supervised approach tries to learn the parameters of the network such that ΦΘ(xi) ≃ zi
e.g. by minimizing kΦΘ(xi)−zik2 . This training procedure differs critically from ours. For instance 
ISTA does not converge for the supervised problem in general while it does for the unsupervised
one. As Proposition 4.1 shows  the unsupervised approach allows to learn to minimize the Lasso cost
function Fx .

6

Figure 4: Steps learned with a 20 layers SLISTA network
on a 10 × 20 problem. For each layer t and each train-
ing sample x  we compute the support S(x  t) of z(t)(x).
The brown (resp.green) curves display the quantiles of the
distribution of 1/LS(x t) (resp. 2/LS(x t)) for each layer
t . Learned steps are mostly in ]0  2/LS[  which guar-
antees the decrease of the surrogate cost function. Full
experimental setup described in Appendix D.

Proposition 4.1 (Pointwise convergence). Let ˜Θ(T ) found by solving Problem (15).
For x ∈ B∞ such that p(x) > 0   Fx(Φ ˜Θ(T ) (x)) −−−−−→T →+∞

x almost everywhere.

F ∗

ISTA

x ] . Hence  Ex∼p[Fx(Φ ˜Θ(T ) (x))− F ∗

x to 0 since it is non-negative.

x ] ≤ Ex∼p[Fx(Φ ˜Θ(T ) (x))] ≤ Ex∼p[Fx(ΦΘ(T )

j ε = 0   the vector (1 − λ)ej minimizes Fx for x = Dj + ε .

ISTA the parameters corresponding to ISTA. For
(x))] . Because ISTA con-
x ] → 0 . This implies

Sketch of proof (full proof in Subsection C.1). Let Θ(T )
all T   we have Ex∼p[F ∗
verges  the right hand term goes to Ex∼p[F ∗
almost sure convergence of Fx(Φ ˜Θ(T ) (x)) − F ∗
Asymptotical weight coupling theorem In this paragraph  we show the main result of this paper:
any LISTA network minimizing Fx on B∞ reduces to SLISTA in its deep layers (Theorem 4.4). It
relies on the following Lemmas.
Lemma 4.2 (Stability of solutions around Dj). Let D ∈ Rn×m be a dictionary with non-duplicated
unit-normed columns. Let c   maxl6=j |D⊤
l Dj| < 1 . Then for all j ∈ J1  mK and ε ∈ Rm such that
kεk < λ(1 − c) and D⊤
It can be proven by verifying the KKT conditions (3) for (1 − λ)ej   detailed in Subsection C.2.
Lemma 4.3 (Weight coupling). Let D ∈ Rn×m be a dictionary with non-duplicated unit-normed
columns. Let θ = (W  α  β) a set of parameters. Assume that all the couples (z∗(x)  x) ∈ Rm × B∞
such that z∗(x) ∈ arg min Fx(z) verify φθ(z∗(x)  x) = z∗(x). Then  α
Sketch of proof (full proof in Subsection C.3). For j ∈ J1  mK   consider x = Dj + ε   with
ε⊤Dj = 0 . For kεk small enough  x ∈ B∞ and ε veriﬁes the hypothesis of Lemma 4.2 
therefore z∗ = (1 − λ)ej ∈ arg min Fx . Writing φθ(z∗  x) = z∗ for the j-th coordinate yields
αW ⊤
j )(λDj + ε) = 0 . This stands for
any ε orthogonal to Dj and of norm small enough. Simple linear algebra shows that this implies
αWj − βDj = 0 .
Lemma 4.3 states that the Lasso solutions are ﬁxed points of a LISTA layer only if this layer
corresponds to a step size for ISTA. The following theorem extends the lemma by continuity  and
shows that the deep layers of any converging LISTA network must tend toward a SLISTA layer.
Theorem 4.4. Let D ∈ Rn×m be a dictionary with non-duplicated unit-normed columns. Let
Θ(T ) = {θ(t)}T
t=0 be the parameters of a sequence of LISTA networks such that the transfer function
of the layer t is z(t+1) = φθ(t) (z(t)  x) . Assume that

j (λDj + ε) = λβ . We can then verify that (αW ⊤

β W = D .

j − βD⊤

(i) the sequence of parameters converges i.e. θ(t) −−−→t→∞
(ii) the output of the network converges toward a solution z∗(x) of the Lasso (1) uniformly over

θ∗ = (W ∗  α∗  β∗)  

the equiregularization set B∞   i.e. supx∈B∞ kΦΘ(T ) (x) − z∗(x)k −−−−→T →∞

0 .

Then α∗

β∗ W ∗ = D .

Sketch of proof (full proof in Subsection C.4). Let ε > 0   and x ∈ B∞ . Using the triangular in-
equality  we have

kφθ∗ (z∗  x) − z∗k ≤ kφθ∗ (z∗  x) − φθ(t) (z(t)  x)k + kφθ(t) (z(t)  x) − z∗k

(16)

7

11020Layer1/L2/L3/L4/LStep1/LLearnedsteps1/LS2/LSSince the z(t) and θ(t) converge  they are valued over a compact set K. The function f : (z  x  θ) 7→
φθ(z  x) is continuous  piecewise-linear. It is therefore Lipschitz on K. Hence  we have kφθ∗ (z∗  x)−
φθ(t) (z(t)  x)k ≤ ε for t large enough. Since φθ(t) (z(t)  x) = z(t+1) and z(t) → z∗   kφθ(t) (z(t)  x)−
z∗k ≤ ε for t large enough. Finally  φθ∗ (z∗  x) = z∗ . Lemma 4.3 allows to conclude.
Theorem 4.4 means that the deep layers of any LISTA network that converges to solutions of the
Lasso correspond to SLISTA iterations: W (t) aligns with D   and α(t)  β(t) get coupled. This is
illustrated in Figure 5  where a 40-layers LISTA network is trained on a 10 × 20 problem with
λ = 0.1 . As predicted by the theorem  α(t)
β(t) W (t) → D : the last layers only learn a step size. This
is consistent with the observation of Moreau and Bruna [2017] which shows that the deep layers
of LISTA stay close to ISTA. Further  Theorem 4.4 also shows that it is hopeless to optimize the
unsupervised objective (15) with WALISTA (13)  since this matrix is not aligned with D .

Figure 5: Illustration of Theorem 4.4: for deep layers
of LISTA  we have α(t)W (t)/β(t) → D   indicating
that the network ultimately only learns a step size. Full
experimental setup described in Appendix D.

5 Numerical Experiments

This section provides numerical arguments to compare SLISTA to LISTA and ISTA. All the experi-
ments were run using Python [Python Software Foundation  2017] and pytorch [Paszke et al.  2017].
The code to reproduce the ﬁgures is available online2.

Network comparisons We compare the proposed approach SLISTA to state-of-the-art learned
methods LISTA [Chen et al.  2018] and ALISTA [Liu et al.  2019] on synthetic and semi-real cases.
In the synthetic case  a dictionary D ∈ Rn×m of Gaussian i.i.d. entries is generated. Each column is
then normalized to unit norm. A set of Gaussian i.i.d. samples (˜xi)N
i=1 ∈ Rn is drawn. The input
samples are obtained as xi = ˜xi/kD⊤ ˜xik∞ ∈ B∞   so that for all i   xi ∈ B∞ . We set m = 256
and n = 64.

For the semi-real case  we used the digits dataset from scikit-learn [Pedregosa et al.  2011] which
consists of 8 × 8 images of handwritten digits from 0 to 9 . We sample m = 256 samples at random
from this dataset and normalize it do generate our dictionary D . Compared to the simulated Gaussian
dictionary  this dictionary has a much richer correlation structure  which is known to imper the
performances of learned algorithms [Moreau and Bruna  2017]. The input distribution also consists
from images from the digits dataset  normalized to lie in B∞.
The networks are trained by minimizing the empirical loss L (15) on a training set of size Ntrain =
10  000 and we report the loss on a test set of size Ntest = 10  000 . Further details on training are in
Appendix D.

Figure 6 shows the test curves for different levels of regularization λ = 0.1 and 0.8. SLISTA performs
best for high λ  even for challenging semi-real dictionary D . In a low regularization setting  LISTA
performs best as SLISTA cannot learn much larger steps due to the low sparsity of the solution. In
this unsupervised setting  ALISTA does not converge in accordance with Theorem 4.4.

6 Conclusion

We showed that using larger step sizes is an efﬁcient strategy to accelerate ISTA for sparse solution
of the Lasso. In order to make this approach practical  we proposed SLISTA  a neural network

2 The code can be found at https://github.com/tomMoral/adopty

8

110203040Layers0123kα(t)W(t)β(t)−DkFLISTAFigure 6: Test loss of ISTA  ALISTA  LISTA and SLISTA on simulated and semi-real data for
different regularization parameters.

architecture which learns such step sizes. Theorem 4.4 shows that the deepest layers of any converging
LISTA architecture must converge to a SLISTA layer. Numerical experiments show that SLISTA
outperforms LISTA in a high sparsity setting. An major beneﬁt of our approach is that it preserves
the dictionary. We plan on leveraging this property to apply SLISTA in convolutional or wavelet
cases  where the structure of the dictionary allows for fast multiplications.

Acknowledgements

We would like to thank the anonymous reviewers for their insightful comments which have improved
the quality of the paper. This project has received funding from the European Research Council
(ERC) under the European Union’s Horizon 2020 research and innovation program (Grant agreement
No. 676943)

References

Jonas Adler  Axel Ringh  Ozan Öktem  and Johan Karlsson. Learning to solve inverse problems

using Wasserstein loss. preprint ArXiv  1710.10898  2017.

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences  2(1):183–202  2009.

Mark Borgerding  Philip Schniter  and Sundeep Rangan. AMP-inspired deep networks for sparse

linear inverse problems. IEEE Transactions on Signal Processing  65(16):4293–4308  2017.

Xiaohan Chen  Jialin Liu  Zhangyang Wang  and Wotao Yin. Theoretical linear convergence of
In Advances in Neural Information

unfolded ISTA and its practical weights and thresholds.
Processing Systems (NIPS)  pages 9061–9071  2018.

Patrick L Combettes and Heinz H. Bauschke. Convex Analysis and Monotone Operator Theory in
Hilbert Spaces. Springer  2011. ISBN 9788578110796. doi: 10.1017/CBO9781107415324.004.

Ingrid Daubechies  Michel Defrise  and Christine De Mol. An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint. Communications on Pure and Applied
Mathematics  57(11):1413–1457  2004.

Bradley Efron  Trevor Hastie  Iain Johnstone  and Robert Tibshirani. Least angle regression. Ann.

Statist.  32(2):407–499  2004.

Laurent El Ghaoui  Vivian Viallon  and Tarek Rabbani. Safe feature elimination in sparse supervised

learning. J. Paciﬁc Optim.  8(4):667–698  2012.

Jerome Friedman  Trevor Hastie  Holger Höﬂing  and Robert Tibshirani. Pathwise coordinate

optimization. The Annals of Applied Statistics  1(2):302–332  2007.

Raja Giryes  Yonina C. Eldar  Alex M. Bronstein  and Guillermo Sapiro. Tradeoffs between con-
vergence speed and reconstruction accuracy in inverse problems. IEEE Transaction on Signal
Processing  66(7):1676–1690  2018.

9

0102030NumberofLayers10−210−1100Fx−F∗xSimulateddataλ=0.10102030NumberofLayers10−610−410−2Simulateddataλ=0.80102030NumberofLayers10−210−1Digitsdataλ=0.10102030NumberofLayers10−410−310−2Digitsdataλ=0.8ISTALISTAALISTASLISTA(proposed)Karol Gregor and Yann Le Cun. Learning Fast Approximations of Sparse Coding. In International

Conference on Machine Learning (ICML)  pages 399–406  2010.

Elaine Hale  Wotao Yin  and Yin Zhang. Fixed-point continuation for ℓ1-minimization: Methodology

and convergence. SIAM J. Optim.  19(3):1107–1130  2008.

Trevor Hastie  Robert Tibshirani  and Martin Wainwright. Statistical Learning with Sparsity: The

Lasso and Generalizations. CRC Press  2015.

John R. Hershey  Jonathan Le Roux  and Felix Weninger. Deep unfolding: Model-based inspiration

of novel deep architectures. preprint ArXiv  1409.2574  2014.

Daisuke Ito  Satoshi Takabe  and Tadashi Wadayama. Trainable ISTA for sparse signal recovery. In

IEEE International Conference on Communications Workshops  pages 1–6  2018.

Tyler Johnson and Carlos Guestrin. Blitz: A principled meta-algorithm for scaling sparse optimization.

In International Conference on Machine Learning (ICML)  pages 1171–1179  2015.

Jingwei Liang  Jalal Fadili  and Gabriel Peyré. Local linear convergence of forward–backward under
partial smoothness. In Advances in Neural Information Processing Systems  pages 1970–1978 
2014.

Jialin Liu  Xiaohan Chen  Zhangyang Wang  and Wotao Yin. ALISTA: Analytic weights are as good
as learned weigths in LISTA. In International Conference on Learning Representation (ICLR) 
2019.

Vladimir A Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of

random matrices. Mathematics of the USSR-Sbornik  1(4):457  1967.

Mathurin Massias  Alexandre Gramfort  and Joseph Salmon. Celer: a Fast Solver for the Lasso with

Dual Extrapolation. In International Conference on Machine Learning (ICML)  2018.

Thomas Moreau and Joan Bruna. Understanding neural sparse coding with matrix factorization. In

International Conference on Learning Representation (ICLR)  2017.

Eugene Ndiaye  Olivier Fercoq  Alexandre Gramfort  and Joseph Salmon. Gap safe screening rules

for sparsity enforcing penalties. J. Mach. Learn. Res.  18(128):1–33  2017.

J. A. Nelder and R. W. M. Wedderburn. Generalized Linear Models. Journal of the Royal Statistical

Society. Series A (General)  135(3):370  1972. ISSN 00359238. doi: 10.2307/2344614.

Yurii Nesterov. A method for solving a convex programming problem with rate of convergence

O(1/k2). Soviet Math. Doklady  269(3):543–547  1983.

Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming 

140(1):125–161  2013.

Bruno A. Olshausen and David J Field. Sparse coding with an incomplete basis set: a strategy

employed by V1  1997.

Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop  2017.

F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Pretten-
hofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 
12:2825–2830  2011.

Clarice Poon  Jingwei Liang  and Carola-Bibiane Schönlieb. Local convergence properties of SAGA
and prox-SVRG and acceleration. In International Conference on Machine Learning (ICML) 
2018.

Python Software Foundation. Python Language Reference  version 3.6. http://python.org/  2017.

10

Saharon Rosset  Ji Zhu  and Trevor Hastie. Boosting as a regularized path to a maximum margin

classiﬁer. J. Mach. Learn. Res.  5:941–973  2004.

Pablo Sprechmann  Alex M. Bronstein  and Guillermo Sapiro. Learning efﬁcient structured sparse

models. In International Conference on Machine Learning (ICML)  pages 615–622  2012.

Pablo Sprechmann  Roee Litman  and TB Yakar. Efﬁcient supervised sparse analysis and synthesis
operators. In Advances in Neural Information Processing Systems (NIPS)  pages 908–916  2013.

Yifan Sun  Halyun Jeong  Julie Nutini  and Mark Schmidt. Are we there yet? manifold identiﬁcation
of gradient-related proximal methods. In Proceedings of Machine Learning Research  volume 89
of Proceedings of Machine Learning Research  pages 1110–1119. PMLR  2019.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B (Methodological)  58(1):267–288  1996.

Ryan Tibshirani. The lasso problem and uniqueness. Electron. J. Stat.  7:1456–1490  2013.

Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.

J. Optim. Theory Appl.  109(3):475–494  2001.

Zhangyang Wang  Qing Ling  and Thomas S. Huang. Learning deep ℓ0 encoders. In AAAI Conference

on Artiﬁcial Intelligence  pages 2194–2200  2015.

Bo Xin  Yizhou Wang  Wen Gao  and David Wipf. Maximal sparsity with deep networks? In

Advances in Neural Information Processing Systems (NIPS)  pages 4340–4348  2016.

Yan Yang  Jian Sun  Huibin Li  and Zongben Xu. Deep ADMM-Net for compressive censing MRI.

In Advances in Neural Information Processing Systems (NIPS)  pages 10–18  2017.

Willard I Zangwill. Convergence conditions for nonlinear programming algorithms. Management

Science  16(1):1–13  1969.

Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network for
image compressive sensing. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition  pages 1828–1837  2018.

11

,Pierre Ablin
Thomas Moreau
Mathurin Massias
Alexandre Gramfort