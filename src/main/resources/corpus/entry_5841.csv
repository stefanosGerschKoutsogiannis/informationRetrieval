2016,Variance Reduction in Stochastic Gradient Langevin Dynamics,Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However  the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper  we present techniques for reducing variance in stochastic gradient Langevin dynamics  yielding novel  stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on  a variety of real world datasets  and on four different machine learning tasks (regression  classification  independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.,Variance Reduction in Stochastic Gradient

Langevin Dynamics

Avinava Dubey∗  Sashank J. Reddi∗  Barnab´as P´oczos  Alexander J. Smola  Eric P. Xing

Department of Machine Learning

Carnegie-Mellon University

{akdubey  sjakkamr  bapoczos  alex  epxing}@cs.cmu.edu

Pittsburgh  PA 15213

Sinead A. Williamson

IROM/Statistics and Data Science

University of Texas at Austin

sinead.williamson@mccombs.utexas.edu

Austin  TX 78712

Abstract

Stochastic gradient-based Monte Carlo methods such as stochastic gradient
Langevin dynamics are useful tools for posterior inference on large scale datasets
in many machine learning applications. These methods scale to large datasets by
using noisy gradients calculated using a mini-batch or subset of the dataset. How-
ever  the high variance inherent in these noisy gradients degrades performance and
leads to slower mixing. In this paper  we present techniques for reducing variance
in stochastic gradient Langevin dynamics  yielding novel stochastic Monte Carlo
methods that improve performance by reducing the variance in the stochastic gra-
dient. We show that our proposed method has better theoretical guarantees on
convergence rate than stochastic Langevin dynamics. This is complemented by
impressive empirical results obtained on a variety of real world datasets  and on
four different machine learning tasks (regression  classiﬁcation  independent com-
ponent analysis and mixture modeling). These theoretical and empirical contribu-
tions combine to make a compelling case for using variance reduction in stochastic
Monte Carlo methods.

1

Introduction

Monte Carlo methods are the gold standard in Bayesian posterior inference due to their asymptotic
convergence properties; however convergence can be slow in large models due to poor mixing.
Gradient-based Monte Carlo methods such as Langevin Dynamics and Hamiltonian Monte Carlo
[10] allow us to use gradient information to more efﬁciently explore posterior distributions over
continuous-valued parameters. By traversing contours of a potential energy function based on the
posterior distribution  these methods allow us to make large moves in the sample space. Although
gradient-based methods are efﬁcient in exploring the posterior distribution  they are limited by the
computational cost of computing the gradient and evaluating the likelihood on large datasets. As a
result  stochastic variants are a popular choice when working with large data sets [15].
Stochastic gradient methods [13] have long been used in the optimization community to decrease
the computational cost of gradient-based optimization algorithms such as gradient descent. These
methods replace the (expensive  but accurate) gradient evaluation with a noisy (but computationally

∗ denotes equal contribution
30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

cheap) gradient evaluation on a random subset of the data. With appropriate scaling  this gradient
evaluated on a random subset of the data acts as a proxy for the true gradient. A carefully designed
schedule of step sizes ensures convergence of the stochastic algorithm.
A similar idea has been employed to design stochastic versions of gradient-based Monte Carlo meth-
ods [15  1  2  9]. By evaluating the derivative of the log likelihood on only a small subset of data
points  we can drastically reduce computational costs. However  using stochastic gradients comes at
a cost: While the resulting estimates are unbiased  they do have very high variance. This leads to an
increased probability of selecting paths with high deviation from the true gradient  leading to slower
convergence.
There have been a number of variations proposed on the basic stochastic gradient Langevin dynamics
(SGLD) model of [15]: [4] incorporates a momentum term to improve posterior exploration; [6]
proposes using additional variables to stabilize ﬂuctuations; [12] proposes modiﬁcations to facilitate
exploration of simplex; [7] provides sampling solutions for correlated data. However  none of these
methods directly tries to reduce the variance in the computed stochastic gradient.
As was the case with the original SGLD algorithm  we look to the optimization community for
inspiration  since high variance is also detrimental in stochastic gradient based optimization. A
plethora of variance reduction techniques have recently been proposed to alleviate this issue for
the stochastic gradient descent (SGD) algorithm [8  5  14]. By incorporating a carefully designed
(usually unbiased) term into the update sequence of SGD  these methods reduce the variance that
arises due to the stochastic gradients in SGD  thereby providing strong theoretical and empirical
performance.
Inspired by these successes in the optimization community  we propose methods for reducing the
variance in stochastic gradient Langevin dynamics. Our approach bridges the gap between the faster
(in terms of iterations) convergence of non-stochastic Langevin dynamics  and the faster per-iteration
speed of SGLD. While our approach draws its motivation from the stochastic optimization literature 
it is to our knowledge the ﬁrst approach that aims to directly reduce variance in a gradient-based
Monte Carlo method. While our focus is on Langevin dynamics  our approach is easily applicable
to other gradient-based Monte Carlo methods.
Main Contributions: We propose a new Langevin algorithm designed to reduce variance in the
stochastic gradient  with minimal additional computational overhead. We also provide a memory
efﬁcient variant of our algorithm. We demonstrate theoretical conversion to the true posterior under
reasonable assumptions  and show that the rate of convergence has a tighter bound than one previ-
ously shown for SGLD. We complement these theoretical results with empirical evaluation showing
impressive speed-ups versus a standard SGLD algorithm  on a variety of machine learning tasks such
as regression  classiﬁcation  independent component analysis and mixture modeling.

i=1 be a set of data items modeled using a likelihood function p(X|θ) =

(cid:2)N
2 Preliminaries
Let X = {xi}N
i=1 p(xi|θ)
(cid:2)N
where the parameter θ has prior distribution p(θ). We are interested in sampling from the posterior
i=1 p(xi|θ). If N is large  standard Langevin Dynamics is not feasible
distribution p(θ|X) ∝ p(θ)
due to the high cost of repeated gradient evaluations; a more scalable approach is to use a stochastic
variant [15]  which we will refer to as stochastic gradient Langevin dynamics  or SGLD. SGLD uses
a classical Robbins-Monro stochastic approximation to the true gradient [13]. At each iteration t of
the algorithm  a subset Xt = {xt1  . . .   xtn} of the data is sampled and the parameters are updated
by using only this subset of data  according to

(cid:4)∞
where ηt ∼ N (0  ht)  and ht is the learning rate. ht is set in such a fashion that

(cid:4)∞
(1)
t=1 ht = ∞ and
t < ∞. This provides an approximation to a ﬁrst order Langevin diffusion  with dynamics
(2)
where U is the unnormalized negative log posterior. Equation 2 has stationary distribution ρ(θ) ∝
exp{−U (θ)}. Let ¯φ =
φ(θ)ρ(θ)dθ where φ represents a test function of interest. For a numerical

2∇θU dt + dW 

(cid:3)∇ log p(θt) + N

n

dθ = − 1

(cid:4)n
i=1 ∇ log p(xti|θt)

(cid:5)

+ ηt

t=1 h2

Δθt = ht
2

(cid:6)

2

method that generates samples {θt}T−1
t=0 φ(θt). Fur-
thermore  let ψ denote the solution to the Poisson equation Lψ = φ − ¯φ  where L is the generator
of the diffusion  given by

i=0   let ˆφ denote the empirical average 1
T

(cid:4)

Lψ = (cid:6)∇θψ ∇θU(cid:7) + 1

2

i ∇2
i ψ.

(3)

(cid:4)T−1

The decreasing step size ht in our approximation (Equation 1) means we do not have to incorporate
a Metropolis-Hastings step to correct for the discretization error relative to Equation 2; however it
comes at the cost of slowing the mixing rate of the algorithm. We note that  while the discretized
Langevin diffusion is Markovian  its convergence guarantees rely on the quality of the approxima-
tion  rather than from standard Markov chain Monte Carlo analyses that rely on this Markovian
property.
A second source of error comes from the use of stochastic approximations to the true gradients. This
is equivalent to using an approximate generator ˜Lt = L + ΔVt  where ΔVt = (∇Ut − ∇U ) · ∇
and ∇Ut is the current stochastic approximation to ∇U. The key contribution of this paper will
be replacing the Robbins-Monro approximation to U with a lower-variance approximation  thus
reducing the error.
To see more clearly the effect of the variance of our stochastic approximation on the estimator error 
we present a result derived for SGLD by [3]:
Theorem 1. [3] Let Ut be an unbiased estimate of U and ht = h for all t ∈ {1  . . .   T}. Then
under certain reasonable assumptions (concretely  assumption [A1] in Section 4)  for a smooth test
function φ  the MSE of SGLD at time K = hT is bounded  for some C > 0 independent of (T  h) in
the following manner:

(cid:4)

⎛
⎜⎜⎝ 1
(cid:10)

T

E( ˆφ − ¯φ)2 ≤ C

t E[(cid:10)ΔVt(cid:10)2]
(cid:11)(cid:12)
(cid:13)

T
T1

+

1
T h

+ h2

⎞
⎟⎟⎠ .

(4)

Here (cid:10).(cid:10) represents the operator norm.
We clearly see that the MSE depends on the variance term E[(cid:10)ΔVt(cid:10)2]  which in turn depends on the
variance of the noisy stochastic gradients. Since  for consistency  we require h → 0 as T → ∞ 1
provided E[(cid:10)ΔVt(cid:10)2] is bounded by a constant τ  the term T1 ceases to dominate as T → ∞ 
meaning that the effect of noise in the stochastic gradient becomes negligible. However outside this
asymptotic regime  the effect of the variance term in Equation 4 remains signiﬁcant. This motivates
our efforts in this paper to decrease the variance of the approximate gradient  while maintaining an
unbiased estimator.
An easy to decrease the variance is by using larger minibatches. However  this comes at a consid-
erable computational cost  undermining the whole beneﬁt of using SGLD. Inspired by the recent
success of variance reduction techniques in stochastic optimization [14  8  5]  we take a rather dif-
ferent approach to reduce the effect of noisy gradients.

3 Variance Reduction for Langevin Dynamics

As we have seen in Section 2  reducing the variance of our stochastic approximation can reduce
our estimation error. In this section  we introduce two approaches for variance reduction  based on
recent variance reduction algorithms for gradient descent [5  8]. The ﬁrst algorithm  SAGA-LD  is
appropriate when our bottleneck is computation; it yields improved convergence with minimal ad-
ditional computational costs over SGLD. The second algorithm  SVRG-LD  is appropriate when our
bottleneck is memory; while the computational cost is generally higher than SAGA-LD  the mem-
ory requirement is lower  with the memory overhead beyond that of stochastic Langevin dynamics
scales as O(d). In practice  we found that computation was a greater bottleneck in the examples
considered  so our experimental section only focuses on SAGA-LD; however on larger datasets with
easily computable gradients  SVRG-LD may be the optimal choice.

1In particular  if h ∝ T −1/3  we obtain the optimal convergence rate for the above upper bound.

3

d for i ∈ {1  . . .   N}  step sizes {ht > 0}T−1

(cid:4)N
Algorithm 1: SAGA-LD
0 = θ0 ∈ R
1: Input: αi
i=1 ∇ log p(xi|αi
2: gα =
0)
3: for t = 0 to T − 1 do
Uniformly randomly pick a set It from {1  . . .   N} (with replacement) such that |It| = b
4:
Randomly draw ηt ∼ N (0  ht)
5:
θt+1 = θt + ht
6:
t+1 = θt for i ∈ It and αi
2
αi
7:
8:
gα = gα +
9: end for
10: Output: Iterates {θt}T−1

(cid:4)
i∈It
t for i /∈ It
t+1) − ∇ log p(xi|αi
t)

(cid:3)∇ log p(xi|θt) − ∇ log p(xi|αi

(cid:3)∇ log p(θt) + N
(cid:3)∇ log p(xi|αi

t+1 = αi

(cid:4)

+ gα

i∈It

+ ηt

(cid:5)

(cid:5)

(cid:5)

i=0

n

t)

t=0

3.1 SAGA-LD
The increased variance in SGLD is due to the fact that we only have information from n (cid:12) N data
points at each iteration. However  inspired by a minibatch version of the SAGA algorithm [5]  we
can include information from the remaining data points via an approximate gradient  and partially
update the average gradient in each operation. We call this approach SAGA-LD.
Under SAGA-LD  we explicitly store N approximate gradients {gαi}N
i=1  corresponding to the N
0 = θ0 for all i ∈
data points. Concretely  let αt = (αi
t)N
[N ]  and initialize gαi = ∇ log p(xi|αi
i=1 gαi. As we iterate through the data 
if a data point is not selected in the current minibatch  we approximate its gradient with gαi. If
It = {i1t  . . . int} is the minibatch selected at iteration t  this means we approximate the gradient as

i=1 be a set of vectors  initialized as αi
0) and gα =

(cid:4)N

(5)
When Equation (5) is used for MAP estimation it corresponds to SAGA[5]. However by injecting
noise into the parameter update in the following manner
Δθt = ht
2

(cid:3)∇ log p(θt) + N

i∈It (∇ log p(xi|θt) − gαi) + gα

+ ηt  where ηt ∼ N (0  ht) (6)

(cid:4)

(cid:5)

n

n

i∈It (∇ log p(xi|θt) − gαi) + gα

(cid:4)N
i=1 ∇ log p(xi|θt) ≈ N

(cid:4)

we can adapt it for sampling from the posterior. After updating θt+1 = θt + Δθt  we let αi
t+1 = θt
for i ∈ It. Note that we do not need to explicitly store the αi
t; instead we just update the correspond-
ing gradients gαi and overall approximate gradient gα. The SAGA-LD algorithm is summarized in
Algorithm 1.
The approximation in Equation (6) gives an unbiased estimate of the true gradient  since the mini-
batch It is sampled uniformly at random from [N ]  and the αt
i are independent of It. SAGA-LD
offers two key properties: (i) As shown in Section 4  SAGA-LD has better convergence properties
than SGLD; (ii) The computational overhead is minimal  since SAGA-LD does not require explicit
calculation of the full gradient. Instead  it simply makes use of gradients that are already being
calculated in the current minibatch. Combined  we end up with a similar computational complexity
to SGLD  with a much better convergence rate.
The only downside of SAGA-LD  when compared with SGLD  is in terms of memory storage. Since
we need to store N individual gradients gαi  we typically have a storage overhead of O(N d) rel-
ative to SGLD. Fortunately  in many applications of interest to machine learning  the cost can be
reduced to O(N ) (please refer to [5] for more details)  and in practice the cost of the higher memory
requirements is typically outweighed by the improved convergence and low computational cost.

3.2 SVRG-LD

If the memory overhead of SAGA-LD is not acceptable  we can use a variant that reduces storage
requirements  at the cost of higher computational demands. The memory complexity for SAGA-LD
is high because the approximate gradient gα is updated at each step. This can be avoided by updating
the approximate gradient every m iterations in a single evaluation  and never storing the individual
gradients gαi. Concretely  after every m passes through the data  we evaluate the gradient on the

4

d  epoch length m  step sizes {ht > 0}T−1

i=0

if (t mod m = 0) then

(cid:4)N
i=1 ∇ log p(xi|˜θ)

Algorithm 2: SVRG-LD
1: Input: ˜θ = θ0 ∈ R
2: for t = 0 to T − 1 do
3:
4:
5:
6:
7:
8:
9:
10: end for
11: Output: Iterates {θt}T−1

(cid:17)

t=0

˜θ = θt
˜g =
end if
Uniformly randomly pick a set It from {1  . . .   N} (with replacement) such that |It| = n
Randomly draw ηt ∼ N (0  ht)
θt+1 = θt + ht
2

∇ log p(xi|θt) − ∇ log p(xi|˜θ)

∇ log p(θt) + N

(cid:4)

(cid:18)

(cid:17)

(cid:18)

+ ηt

+ ˜g

n

i∈It

n

Δθt = ht
2

+ ηt where ηt ∼ N (0  ht)

(cid:4)N
i=1 ˜gi  where ˜gi = ∇ log p(xi|˜θ) is the current local gradient. ˜g
entire data set  obtaining ˜g =
(cid:3)∇ log p(θt) + N
(cid:4)
then serves as an approximate gradient until the next global evaluation. This yields an update of the
form

(cid:5)
i∈It (∇ log p(xi|θt) − ˜gi) + ˜g

(7)
Without adding noise ηt the update sequence in Equation (7) corresponds to the stochastic variance
reduction gradient descent algorithm [8]. Pseudocode for this procedure is given in Algorithm 2.
While the memory requirements are lower  the computational cost is higher  due to the cost of a
full update of ˜g. Further  convergence may be negatively effected due to the fact that  as we move
further from ˜θ  ˜g will be further from the true gradient. In practice  we found SAGA-LD to be a more
effective algorithm on the datasets considered  so in the interest of space we relegate further details
about SVRG-LD to the appendix.
4 Analysis
Our motivation in this paper was to improve the convergence of SGLD  by reducing the variance of
the gradient estimate. As we saw in Theorem 1  a high variance E[||ΔVt||2]  corresponding to noisy
stochastic gradients  leads to a large bound on the MSE of a test function. We expand this analysis
to show that the algorithms introduced in this paper yield a tighter bound.
Theorem 1 required a number of assumptions  given below in [A1]. Discussion of the reasonableness
of these assumptions is provided in [3].
[A1] We assume the functional ψ that solves the Poisson equation Lψ = φ − ¯φ is bounded up to
3rd-order derivatives by some function Γ  i.e.  (cid:10)Dkψ(cid:10) ≤ CkΓpk where D is the kth order derivative
(for k = (0  1  2  3))  and Ck  pk > 0. We also assume that the expectation of Γ on {θt} is bounded
(supt EΓp[θt] < ∞) and that Γ is smooth such that sups∈(0 1) Γp(sθ + (1 − s)θ
) ≤ C(Γp(θ) +
Γp(θ
In our analysis of SAGA-LD and SVRG-LD  we make the assumptions in [A1]  and add the following
further assumptions about the smoothness of our gradients:
[A2] We assume that the functions log p(xi|θ) are Lipschitz smooth with constant L for all i ∈ [N ] 
i.e. (cid:10)∇ log p(xi|θ) − ∇ log p(xi|θ
d. We assume that
(ΔVtψ(θ))2 ≤ C
d  where ψ is the
solution to the Poisson equation for our test function. We also assume that (cid:10)∇ log p(θ)(cid:10) ≤ σ and
(cid:10)∇ log p(xi|θ)(cid:10) ≤ σ for some σ and all i ∈ [N ] and θ ∈ R
The Lipschitz smoothness assumption is very common both in the optimization literature [11] and
when working with Itˆo diffusions [3]. The bound on (ΔVtψ(θ))2 holds when the gradient (cid:10)∇ψ(cid:10) is
bounded.
Loosely  these assumptions encode the idea that the gradients don’t change too quickly  so that we
limit the errors introduced by incorporating gradients based on previous values of θ. With these
assumptions  we state the following key results for SAGA-LD and SVRG-LD  which are proved in
the supplement.

(cid:5)(cid:10)∇Ut(θ) − ∇U (θ)(cid:10)2 for some constant C

(cid:5)  p ≤ max 2pk for some C > 0.

(cid:5) ∈ R
> 0 for all θ ∈ R

(cid:5)(cid:10) for all i ∈ [N ] and θ  θ

)(cid:10) ≤ L(cid:10)θ − θ

))  ∀θ  θ

d.

(cid:5)

(cid:5)

(cid:5)

(cid:5)

5

Theorem 2. Let ht = h for all t ∈ {1  . . .   T}. Under the assumptions [A1] [A2]  for a smooth test
function φ  the MSE of SAGA-LD (in Algorithm 1) at time K = hT is bounded  for some C > 0
independent of (T  h) in the following manner:

E( ˆφ − ¯φ)2 ≤ C

N 2 min{σ2 

n2 (L2h2σ2+hd)}
N 2

nT

+ 1

T h + h2

.

(8)

(cid:19)

(cid:17)

(cid:20)

(cid:18)

.

A similar result can be shown for SVRG-LD in Algorithm 2:
Theorem 3. Let ht = h for all t ∈ {1  . . .   T}. Under the assumptions [A1] [A2]  for a smooth test
function φ  the MSE of SVRG-LD (in Algorithm 2) at time K = hT is bounded  for some C > 0
independent of (T  h) in the following manner:

E( ˆφ − ¯φ)2 ≤ C

N 2 min{σ2 m2(L2h2σ2+hd)}

nT

+ 1

T h + h2

(9)
The result in Theorem 3 is qualitatively equivalent to that in Theorem 2 when m = (cid:15)N/n(cid:16). In
general  such a choice of m is preferable because  in this case  the overall cost of calculation of full
gradient in Algorithm 2 becomes insigniﬁcant.
In order to assess the theoretical convergence of our proposed algorithm  we compare the bounds
for SVRG-LD (Theorem 3) and SAGA-LD (Theorem 2) with those obtained for SGLD (Theorem 1.
Under the assumptions in this section  it is easy to show that the term T1 in Theorem 1 becomes
O(N 2σ2/(T n)). In contrast  both Theorem 2 and 3 show that  due to a reduction in variance 
SVRG-LD and SAGA-LD exhibit a much weaker dependence. More speciﬁcally  this is manifested
in the form of the following bound:

(cid:2)

N 2 min

σ2 

(cid:3)

N 2
n2 (h2σ2+hd)
nT

.

Note that this is tighter than the corresponding bound on SGLD. We also note that  similar to SGLD 
SAGA-LD and SVRG-LD require h → 0 as T → ∞. In such a scenario  the convergence becomes
signiﬁcantly faster relative to SGLD as h → 0.

5 Experiments
We present our empirical results in this section. We focus on applying our stochastic gradient method
to four different machine learning tasks  carried out on benchmark datasets: (i) Bayesian linear re-
gression (ii) Bayesian logistic regression and (iii) Independent component analysis (iv) Mixture
modeling. We focus on SAGA-LD  since in the applications considered  the convergence and com-
putational beneﬁts of SAGA-LD are more beneﬁcial than the memory beneﬁts of SVRG-LD;
In order to reduce the initial computational costs associated with calculating the initial average
gradient  we use a variant of Algorithm 1 that calculates gα (in line 2 of Algorithm 1) in an online
fashion and reweights the updates accordingly. Note that such a heuristic is also commonly used in
the implementation of SAG and SAGA in the context of optimization [14  5].
In all our experiments  we use a decreasing step size for SGLD as suggested by [15]. In particular 
−γ  where the parameters a  b and γ are chosen for each dataset to give the best
we use t = a(b + t)
performance of the algorithm on that particular dataset. For SAGA-LD  due to the beneﬁt of variance
reduction  we use a simple two phase constant step size selection strategy. In each of these phases  a
constant step size is chosen such that SAGA-LD gives the best performance on the particular dataset.
The minibatch size  n  in both SGLD and SAGA-LD is held at a constant value of 10 throughout our
experiments. All algorithms are initialized to the same point and the same sequence of minibatches
is pre-generated and used in both algorithms.

5.1 Regression
We ﬁrst demonstrate the performance of our algorithm on Bayesian regression. Formally  we are
d and yi ∈ R. The distribution of the ith output
provided with inputs Z = {xi  yi}N
yi is given by p(yi|xi) = N (β
−1I). Due to conjugacy  the posterior
distribution over β is also normal  and the gradients of the log-likelihood and the log-prior are given

i=1 where xi ∈ R
xi  σe)  where p(β) = N (0  λ

(cid:6)

6

104

102

E
S
M

 
t
s
e
T

10-1

0

concrete

SGLD
SAGA-LD

1

2

Number of pass through data

104

102

E
S
M

 
t
s
e
T

10-1

0

3

noise

SGLD
SAGA-LD

1

3

Number of pass through data

105

102

E
S
M

 
t
s
e
T

10-1

0

5

parkinsons

SGLD
SAGA-LD

1
Number of pass through data

5

10

103

101

E
S
M

 
t
s
e
T

10-1

0

toms

SGLD
SAGA-LD

1
Number of pass through data

5

10

2

1.5

E
S
M

 
t
s
e
T

1

0

3dRoad

SGLD
SAGA-LD

0.5

Number of pass through data

1

Figure 1: Performance comparison of SGLD and SAGA-LD on a regression task. The x-axis and y-
axis represent the number of passes through the entire data and the average test MSE  respectively.
Additional experiments are provided in the appendix.

d
o
o
h

i
l

e
k

i
l
-
g
o

 

l
 
t
s
e
T
e
g
a
r
e
v
A

-10-1

-100

-101

-102

pima

SGLD
SAGA-LD

1
Number of pass through data

4

8

d
o
o
h

i
l

e
k

i
l
-
g
o

 

l
 
t
s
e
T
e
g
a
r
e
v
A

-10-1

-100

-101

-102

0

diabetic

SGLD
SAGA-LD

2

Number of pass through data

4

d
o
o
h

i
l

e
k

i
l
-
g
o

 

l
 
t
s
e
T
e
g
a
r
e
v
A

eeg

-100

-101

0

SGLD
SAGA-LD

0.5

Number of pass through data

1

d
o
o
h

i
l

e
k

i
l
-
g
o
l
 
t
s
e
T
 
e
g
a
r
e
v
A

-10-1

-100

space

SGLD
SAGA-LD

1
Number of pass through data

5

10

-0.4

-1

-2

d
o
o
h

i
l

e
k

i
l
-
g
o

 

l
 
t
s
e
T
e
g
a
r
e
v
A

susy

0.5

1

Number of pass through data

SGLD
SAGA-LD

2

Figure 2: Comparison of performance of SGLD and SAGA-LD for Bayesian logistic regression.
The x-axes and y-axes represent the number of effective passes through the dataset and the test
log-likelihood  respectively.
by ∇β log(P (yi|xi  β)) = −(yi − βT xi)xi and ∇β log(P (β)) = −λβ. We ran experiments on 11
standard UCI regression datasets  summarized in Table 1.2 In each case  we set the prior precision
λ = 1  and we partitioned our dataset into training (70%)  validation (10%)  and test (20%) sets.
The validation set is used to select the step size parameters  and we report the mean square error
(MSE) evaluated on the test set  using 5-fold cross-validation.
The average test MSE on a subset of datasets is reported in Figure 1. Due to space constraints 
we relegate the remaining experimental results to the appendix. As shown in Figure 1  SAGA-LD
converges much faster than the SGLD method (taking less than one pass through the whole dataset
in many cases). This performance gain is consistent across all the datasets. Furthermore  the step
size selection was much simpler for SAGA-LD than SGLD.

Datasets

N
P

concrete

1030

8

noise
1503

5

parkinson

5875
21

bike
17379

12

toms
45730

96

protein
45730

9

casp
53500

9

kegg
64608

27

Table 1: Summary of datasets used for regression.

3droad
434874

2

music
515345

90

twitter
583250

77

i=1 where xi ∈ R

5.2 Classiﬁcation
We next turn our attention to classiﬁcation  using Bayesian logistic regression. In this case  the
input is the set Z = {xi  yi}N
d  yi ∈ {0  1}. The distribution of the out-
put yi for given sample xi is given by P (yi = 1) = φ(βT xi)  where p(β) = N (0  λ
−1I) and
φ(z) = 1/(1 + exp(−z)). Here  the gradient of the log-likelihood and the log-prior are given by
∇β log(P (yi|xi  β)) = (yi − φ(βT xi))xi and ∇β log(P (β)) = −λβ respectively. Again  λ is set
to 1 for all experiments  and the dataset split and parameter selection method is exactly same as in
our regression experiments. We run experiments on ﬁve binary classiﬁcation datasets in the UCI
repository  summarized in Table 2  and report the the test set log-likelihood for each dataset  using
5-fold cross validation. Figure 2 shows the performance of SGLD and SAGA-LD for the classiﬁca-
tion datasets. As we saw with the regression task  SAGA-LD converges faster that SGLD on all the
datasets  demonstrating the efﬁciency of the our algorithm in this setting.

Datasets

N
d

pima
768
8

diabetic
1151
20

eeg
14980

15

space
58000

9

susy

100000

18

Table 2: Summary of the datasets used for classiﬁcation.

2The datasets can be downloaded from https://archive.ics.uci.edu/ml/index.html

7

d
o
o
h

i
l

e
k

i
l
-
g
o

 

l
 
t
s
e
T
e
g
a
r
e
v
A

1

-1

-2

0

MEG

1010

e
c
n
a
i
r
a
V

105

SGLD
SAGA-LD

Number of pass through data

1

1.5

0

Regression-concrete

Classification-pima

SGLD
SAGA-LD

104

e
c
n
a
i
r
a
V

103

0

6

2

4

Number of pass through data

SGLD
SAGA-LD

1

2

Number of pass through data

(cid:1)106

-0.01

posterior

-1

r
o
i
r
e
t
s
o
p
-
g
o

l

3

-3

-50

Estimated Posterior

15000

7000

t
n
u
o
c
 
e
p
m
a
S

l

0
(cid:1)

50

0
-50

0
(cid:1)

50

Figure 3: The left plot shows the performance of SGLD and SAGA-LD for the ICA task. The next
two plots show the variance of SGLD and SAGA-LD for regression and classiﬁcation. The rightmost
two plot shows true and estimated posteriors using SAGA-LD for the mixture modeling task

5.3 Bayesian Independent Component Analysis
To evaluate performance under a Bayesian Independent Component Analysis (ICA) model  we as-
sume our dataset x = {xi}N

i=1 is distributed according to

p(x|W ) ∝ | det(W )|(cid:2)d

i=1 p(yi)  Wij ∼ N (0  λ) 

(10)

d×d  yi = wT

i x  and p(yi) = 1/(4 cosh2( 1
−1)T − YixT

where W ∈ R
2 yi)). The gradient of the log-likelihood
and the log-prior are ∇W log(p(xi|W )) = (W
2 yij) for all j ∈ [d]
i where Yij = tanh( 1
and ∇W log(p(W )) = −λW respectively. All other parameters are set as before. We used a stan-
dard ICA dataset for our experiment3  comprisein 17730 time-points with 122 channels from which
we extracted the ﬁrst 10 channels. Further experimental details are similar to those for regression
and classiﬁcation. The performance (in terms of test set log likelihood) of SGLD and SAGA-LD for
the ICA task is shown in Figure 3. As seen in Figure 3  similar to the regression and classiﬁcation
tasks  SAGA-LD outperforms SGLD in the ICA task.

5.4 Mixture Model
Finally  we evaluate how well SAGA-LD estimates the true posterior of parameters of mixture mod-
els. We generated 20 000 data points from a mixture of two Gaussians  given by p(x|μ  σ1  σ2  γ) =
2N (x;−μ + γ  σ2)  where μ = −5  γ = 20  and σ = 5. We estimate the posterior
2N (x; μ  σ2) + 1
1
distribution over μ  holding the other variables ﬁxed. The two plots on the right of Figure 3 show
that we are able to estimate the true posterior correctly.
Discussion: Our experiments provide a very compelling reason to use variance reduction techniques
for SGLD  complementing the theoretical justiﬁcation given in Section 4. The hypothesized variance
reduction is demonstrated in Figure 3  where we compare the variances of SGLD and SAGA-LD
with respect to the true gradient on regression and classiﬁcation tasks. As we see from all of the
experimental results in this section  SAGA-LD converges with relatively very few samples compared
with SGLD. This is especially important in hierarchical Bayesian models where  typically  the size
of the model used is proportional to the number of observations. Thus  with SAGA-LD  we can
achieve better performance with very few samples. Another advantage is that  while we require the
step size to tend to zero  we can use a much simpler schedule than SGLD.
6 Discussion and Future Work
SAGA-LD is a new stochastic Langevin method that obtains improved convergence by reducing the
variance in the stochastic gradient. An alternative method  SVRG-LD  can be used when memory is
at a premium. For both SAGA-LD and SVRG-LD  we proved a tighter convergence bound than the
one previously shown for stochastic gradient Langevin dynamics. We also showed  on a variety of
machine learning tasks  that SAGA-LD converges to the true posterior faster than SGLD  suggesting
the widespread use of SAGA-LD in place of SGLD.
We note that  unlike other stochastic Langevin methods  our sampler is non-Markovian. Since our
convergence guarantees are based on bounding the error relative to the full Langevin diffusion rather
than on properties of a Markov chain  this does not impact the validity of our sampler.
While we showed the efﬁcacy of using our proposed variance reduction technique to SGLD  our
proposed strategy is very generic enough and can also be applied to other gradient-based MCMC
techniques such as [1  2  9  6  12]. We leave this as future work.

3The dataset can be downloaded from https://www.cis.hut.fi/projects/ica/eegmeg/

MEG_data.html.

8

References
[1] Sungjin Ahn  Anoop Korattikara  and Max Welling. Bayesian posterior sampling via stochastic

gradient Fisher scoring. In ICML  2012.

[2] Sungjin Ahn  Babak Shahbaba  and Max Welling. Distributed stochastic gradient MCMC. In

ICML  2014.

[3] Changyou Chen  Nan Ding  and Lawrence Carin. On the convergence of stochastic gradient

MCMC algorithms with high-order integrators. In NIPS  2015.

[4] Tianqi Chen  Emily B. Fox  and Carlos Guestrin. Stochastic gradient Hamiltonian Monte

Carlo. In ICML  2014.

[5] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient

method with support for non-strongly convex composite objectives. In NIPS  2014.

[6] Nan Ding  Youhan Fang  Ryan Babbush  Changyou Chen  Robert D. Skeel  and Hartmut

Neven. Bayesian sampling using stochastic gradient thermostats. In NIPS  2014.

[7] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
2011.

[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-

ance reduction. In NIPS  2013.

[9] Yi-An Ma  Tianqi Chen  and Emily Fox. A complete recipe for stochastic gradient MCMC. In

NIPS  2015.

Carlo  2010.

2003.

[10] Radford Neal. Mcmc using hamiltonian dynamics.

In Handbook of Markov Chain Monte

[11] Yurii Nesterov. Introductory Lectures On Convex Optimization: A Basic Course. Springer 

[12] Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the

probability simplex. In NIPS  2013.

[13] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Math-

ematical Statistics  22(3):400–407  sep 1951.

[14] Mark W. Schmidt  Nicolas Le Roux  and Francis R. Bach. Minimizing ﬁnite sums with the

stochastic average gradient. arXiv:1309.2388  2013.

[15] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynam-

ics. In ICML  2011.

9

,Kumar Avinava Dubey
Sashank J. Reddi
Sinead Williamson
Barnabas Poczos
Alexander Smola
Eric Xing