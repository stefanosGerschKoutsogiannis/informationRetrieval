2014,On the Number of Linear Regions of Deep Neural Networks,We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way  deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular  our analysis is not specific to a single family of models  and as an example  we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.,On the Number of Linear Regions of

Deep Neural Networks

Guido Mont´ufar

Max Planck Institute for Mathematics in the Sciences

montufar@mis.mpg.de

Razvan Pascanu

Universit´e de Montr´eal

pascanur@iro.umontreal.ca

Kyunghyun Cho

Universit´e de Montr´eal

kyunghyun.cho@umontreal.ca

Yoshua Bengio

Universit´e de Montr´eal  CIFAR Fellow
yoshua.bengio@umontreal.ca

Abstract

We study the complexity of functions computable by deep feedforward neural networks
with piecewise linear activations in terms of the symmetries and the number of linear
regions that they have. Deep networks are able to sequentially map portions of each
layer’s input-space to the same output. In this way  deep models compute functions
that react equally to complicated patterns of different inputs. The compositional
structure of these functions enables them to re-use pieces of computation exponentially
often in terms of the network’s depth. This paper investigates the complexity of such
compositional maps and contributes new theoretical results regarding the advantage
of depth for neural networks with piecewise linear activation functions. In particular 
our analysis is not specific to a single family of models  and as an example  we employ
it for rectifier and maxout networks. We improve complexity bounds from pre-existing
work and investigate the behavior of units in higher layers.
Keywords: Deep learning  neural network  input space partition  rectifier  maxout

1 Introduction

Artificial neural networks with several hidden layers  called deep neural networks  have become popular
due to their unprecedented success in a variety of machine learning tasks (see  e.g.  Krizhevsky et al.
2012  Ciresan et al. 2012  Goodfellow et al. 2013  Hinton et al. 2012). In view of this empirical evidence 
deep neural networks are becoming increasingly favored over shallow networks (i.e.  with a single layer
of hidden units)  and are often implemented with more than five layers. At the time being  however  the
theory of deep networks still poses many questions. Recently  Delalleau and Bengio (2011) showed that
a shallow network requires exponentially many more sum-product hidden units1 than a deep sum-product
network in order to compute certain families of polynomials. We are interested in extending this kind
of analysis to more popular neural networks  such as those with maxout and rectifier units.
There is a wealth of literature discussing approximation  estimation  and complexity of artificial neural
networks (see  e.g.  Anthony and Bartlett 1999). A well-known result states that a feedforward neural
network with a single  huge  hidden layer is a universal approximator of Borel measurable functions (see
Hornik et al. 1989  Cybenko 1989). Other works have investigated universal approximation of probability
distributions by deep belief networks (Le Roux and Bengio 2010  Mont´ufar and Ay 2011)  as well as
their approximation properties (Mont´ufar 2014  Krause et al. 2013).
These previous theoretical results  however  do not trivially apply to the types of deep neural networks
that have seen success in recent years. Conventional neural networks often employ either hidden units

1A single sum-product hidden layer summarizes a layer of product units followed by a layer of sum units.

1

Figure 1: Binary classification using a shallow model with 20 hidden units (solid line) and a deep model
with two layers of 10 units each (dashed line). The right panel shows a close-up of the left panel. Filled
markers indicate errors made by the shallow model.

with a bounded smooth activation function  or Boolean hidden units. On the other hand  recently it has
become more common to use piecewise linear functions  such as the rectifier activation g(a) = max{0  a}
(Glorot et al. 2011  Nair and Hinton 2010) or the maxout activation g(a1  . . .   ak) = max{a1  . . .   ak}
(Goodfellow et al. 2013). The practical success of deep neural networks with piecewise linear units calls
for the theoretical analysis specific for this type of neural networks.
In this respect  Pascanu et al. (2013) reported a theoretical result on the complexity of functions computable
by deep feedforward networks with rectifier units. They showed that  in the asymptotic limit of many
hidden layers  deep networks are able to separate their input space into exponentially more linear response
regions than their shallow counterparts  despite using the same number of computational units.
Building on the ideas from Pascanu et al. (2013)  we develop a general framework for analyzing deep
models with piecewise linear activations. We describe how the intermediary layers of these models
are able to map several pieces of their inputs into the same output. The layer-wise composition of the
functions computed in this way re-uses low-level computations exponentially often as the number of
layers increases. This key property enables deep networks to compute highly complex and structured
functions. We underpin this idea by estimating the number of linear regions of functions computable by
two important types of piecewise linear networks: with rectifier units and with maxout units. Our results
for the complexity of deep rectifier networks yield a significant improvement over the previous results
on rectifier networks mentioned above  showing a favorable behavior of deep over shallow networks even
with a moderate number of hidden layers. Furthermore  our analysis of deep rectifier and maxout networks
provides a platform to study a broad variety of related networks  such as convolutional networks.
The number of linear regions of the functions that can be computed by a given model is a measure of the
model’s flexibility. An example of this is given in Fig. 1  which compares the learned decision boundary of a
single-layer and a two-layer model with the same number of hidden units (see details in the Supplementary
Material). This illustrates the advantage of depth; the deep model captures the desired boundary more
accurately  approximating it with a larger number of linear pieces. As noted earlier  deep networks are able
to identify an exponential number of input neighborhoods by mapping them to a common output of some
intermediary hidden layer. The computations carried out on the activations of this intermediary layer are
replicated many times  once in each of the identified neighborhoods. This allows the networks to compute
very complex looking functions even when they are defined with relatively few parameters. The number
of parameters is an upper bound for the dimension of the set of functions computable by a network  and
a small number of parameters means that the class of computable functions has a low dimension. The
set of functions computable by a deep feedforward piecewise linear network  although low dimensional 
achieves exponential complexity by re-using and composing features from layer to layer.

2 Feedforward Neural Networks and their Compositional Properties

In this section we discuss the ability of deep feedforward networks to re-map their input-space to create
complex symmetries by using only relatively few computational units. The key observation of our analysis
is that each layer of a deep model is able to map different regions of its input to a common output. This
leads to a compositional structure  where computations on higher layers are effectively replicated in all
input regions that produced the same output at a given layer. The capacity to replicate computations over
the input-space grows exponentially with the number of network layers. Before expanding these ideas  we
introduce basic definitions needed in the rest of the paper. At the end of this section  we give an intuitive
perspective for reasoning about the replicative capacity of deep models.

2

2.1 Definitions

A feedforward neural network is a composition of layers of computational units which defines a function
F : Rn0 → Rout of the form

F (x; θ) = fout ◦ gL ◦ fL ◦ ··· ◦ g1 ◦ f1(x) 

(1)
where fl is a linear preactivation function and gl is a nonlinear activation function. The parameter θ is
composed of input weight matrices Wl ∈ Rk·nl×nl−1 and bias vectors bl ∈ Rk·nl for each layer l ∈ [L].
(cid:62) of activations xl i of the units i ∈ [nl] in
The output of the l-th layer is a vector xl = [xl 1  . . .   xl nl]
that layer. This is computed from the activations of the preceding layer by xl = gl(fl(xl−1)). Given the
activations xl−1 of the units in the (l − 1)-th layer  the preactivation of layer l is given by

fl(xl−1) = Wlxl−1 + bl 

where fl = [fl 1  . . .   fl nl]
of the i-th unit in the l-th layer is given by

(cid:62) is an array composed of nl preactivation vectors fl i ∈ Rk  and the activation

xl i = gl i(fl i(xl−1)).

We will abbreviate gl ◦ fl by hl. When the layer index l is clear  we will drop the corresponding subscript.
We are interested in piecewise linear activations  and will consider the following two important types.

• Rectifier unit:
• Rank-k maxout unit: gi(fi) = max{fi 1  . . .   fi k}  where fi = [fi 1  . . .   fi k] ∈ Rk.

gi(fi) = max{0  fi}  where fi ∈ R and k = 1.

The structure of the network refers to the way its units are arranged. It is specified by the number n0 of
input dimensions  the number of layers L  and the number of units or width nl of each layer.
We will classify the functions computed by different network structures  for different choices of parameters 
in terms of their number of linear regions. A linear region of a piecewise linear function F : Rn0 → Rm
is a maximal connected subset of the input-space Rn0  on which F is linear. For the functions that we
consider  each linear region has full dimension  n0.

2.2 Shallow Neural Networks

Rectifier units have two types of behavior; they can be either constant 0 or linear  depending on their
inputs. The boundary between these two behaviors is given by a hyperplane  and the collection of all
the hyperplanes coming from all units in a rectifier layer forms a hyperplane arrangement. In general 
if the activation function g : R → R has a distinguished (i.e.  irregular) behavior at zero (e.g.  an inflection
point or non-linearity)  then the function Rn0 → Rn1; x (cid:55)→ g(Wx + b) has a distinguished behavior at
all inputs from any of the hyperplanes Hi := {x ∈ Rn0 : Wi :x + bi = 0} for i ∈ [n1]. The hyperplanes
capturing this distinguished behavior also form a hyperplane arrangement (see  e.g.  Pascanu et al. 2013).
The hyperplanes in the arrangement split the input-space into several regions. Formally  a region of a
hyperplane arrangement {H1  . . .   Hn1} is a connected component of the complement Rn0 \ (∪iHi) 
(cid:0)n1
(cid:1)
well-known result by Zaslavsky (1975). An arrangement of n1 hyperplanes in Rn0 has at most(cid:80)n0
i.e.  a set of points delimited by these hyperplanes (possibly open towards infinity). The number of regions
of an arrangement can be given in terms of a characteristic function of the arrangement  as shown in a
(cid:1) (see Pascanu et al. 2013; Proposition 5).
rectifier network with n0 inputs and n1 hidden units is(cid:80)n0

regions. Furthermore  this number of regions is attained if and only if the hyperplanes are in general
position. This implies that the maximal number of linear regions of functions computed by a shallow

(cid:0)n1

j=0

j

j=0

j

2.3 Deep Neural Networks

We start by defining the identification of input neighborhoods mentioned in the introduction more formally:
Definition 1. A map F identifies two neighborhoods S and T of its input domain if it maps them to a com-
mon subset F (S) = F (T ) of its output domain. In this case we also say that S and T are identified by F.
Example 2. The four quadrants of 2-D Euclidean space are regions that are identified by the absolute
(cid:62).
value function g : R2 → R2; (x1  x2) (cid:55)→ [|x1| |x2|]

3

(a)

(b)

(c)

Figure 2: (a) Space folding of 2-D Euclidean space along the two coordinate axes. (b) An illustration of
how the top-level partitioning (on the right) is replicated to the original input space (left). (c) Identification
of regions across the layers of a deep model.

The computation carried out by the l-th layer of a feedforward network on a set of activations from the
(l − 1)-th layer is effectively carried out for all regions of the input space that lead to the same activations
of the (l − 1)-th layer. One can choose the input weights and biases of a given layer in such a way that
the computed function behaves most interestingly on those activation values of the preceding layer which
have the largest number of preimages in the input space  thus replicating the interesting computation many
times in the input space and generating an overall complicated-looking function.
For any given choice of the network parameters  each hidden layer l computes a function hl = gl ◦ fl on
the output activations of the preceding layer. We consider the function Fl : Rn0 → Rnl; Fl := hl ◦···◦ h1
that computes the activations of the l-th hidden layer. We denote the image of Fl by Sl ⊆ Rnl  i.e.  the
set of (vector valued) activations reachable by the l-th layer for all possible inputs. Given a subset R ⊆ Sl 
R the set of subsets ¯R1  . . .   ¯Rk ⊆ Sl−1 that are mapped by hl onto R; that is  subsets
we denote by P l
that satisfy hl( ¯R1) = ··· = hl( ¯Rk) = R. See Fig. 2 for an illustration.
The number of separate input-space neighborhoods that are mapped to a common neighborhood
R ⊆ Sl ⊆ Rnl can be given recursively as
N l−1

R = 1  for each region R ⊆ Rn0.
N 0

(cid:88)

N l
R =

(2)

R(cid:48)

 

R(cid:48)∈P l

R

R is the set of all disjoint input-space neighborhoods whose image by the function

with piecewise linear activations is at least N =(cid:80)

For example  P 1
computed by the first layer  h1 : x (cid:55)→ g(Wx + b)  equals R ⊆ S1 ⊆ Rn1.
The recursive formula (2) counts the number of identified sets by moving along the branches of a tree
rooted at the set R of the j-th layer’s output-space (see Fig. 2 (c)). Based on these observations  we can
estimate the maximal number of linear regions as follows.
Lemma 3. The maximal number of linear regions of the functions computed by an L-layer neural network
is defined by Eq. (2)  and
P L is a set of neighborhoods in distinct linear regions of the function computed by the last hidden layer.
Here  the idea to construct a function with many linear regions is to use the first L − 1 hidden layers to
identify many input-space neighborhoods  mapping all of them to the activation neighborhoods P L of
the (L − 1)-th hidden layer  each of which belongs to a distinct linear region of the last hidden layer. We
will follow this strategy in Secs. 3 and 4  where we analyze rectifier and maxout networks in detail.

R   where N L−1

R∈P L N L−1

R

2.4 Identification of Inputs as Space Foldings

In this section  we discuss an intuition behind Lemma 3 in terms of space folding. A map F that identifies
(cid:48) can be considered as an operator that folds its domain in such a way that the two
two subsets S and S

4

1.Foldalongthe2.Foldalongthehorizontalaxisverticalaxis3.S1S2S3S4S04S01S01S01S01S04S04S04S02S02S02S02S03S03S03S03S01S04S02S03InputSpaceFirstLayerSpaceSecondLayerSpaceFigure 3: Space folding of 2-D space in a non-trivial way. Note how the folding can potentially identify
symmetries in the boundary that it needs to learn.

(cid:48) coincide and are mapped to the same output. For instance  the absolute value function
subsets S and S
g : R2 → R2 from Example 2 folds its domain twice (once along each coordinate axis)  as illustrated
in Fig. 2 (a). This folding identifies the four quadrants of 2-D Euclidean space. By composing such
operations  the same kind of map can be applied again to the output  in order to re-fold the first folding.
Each hidden layer of a deep neural network can be associated with a folding operator. Each hidden layer
folds the space of activations of the previous layer. In turn  a deep neural network effectively folds its
input-space recursively  starting with the first layer. The consequence of this recursive folding is that
any function computed on the final folded space will apply to all the collapsed subsets identified by the
map corresponding to the succession of foldings. This means that in a deep model any partitioning of
the last layer’s image-space is replicated in all input-space regions which are identified by the succession
of foldings. Fig. 2 (b) offers an illustration of this replication property.
Space foldings are not restricted to foldings along coordinate axes and they do not have to preserve lengths.
Instead  the space is folded depending on the orientations and shifts encoded in the input weights W and
biases b and on the nonlinear activation function used at each hidden layer. In particular  this means that the
sizes and orientations of identified input-space regions may differ from each other. See Fig. 3. In the case
of activation functions which are not piece-wise linear  the folding operations may be even more complex.

2.5 Stability to Perturbation

Our bounds on the complexity attainable by deep models (Secs. 3 and 4) are based on suitable choices
of the network weights. However  this does not mean that the indicated complexity is only attainable
in singular cases. The parametrization of the functions computed by a neural network is continuous.
More precisely  the map ψ : RN → C(Rn0; RnL); θ (cid:55)→ Fθ  which maps input weights and biases
i=1 to the continuous functions Fθ : Rn0 → RnL computed by the network  is continuous.
θ = {Wi  bi}L
Our analysis considers the number of linear regions of the functions Fθ. By definition  each linear region
contains an open neighborhood of the input-space Rn0. Given any function Fθ with a finite number
of linear regions  there is an  > 0 such that for each -perturbation of the parameter θ  the resulting
function Fθ+ has at least as many linear regions as Fθ. The linear regions of Fθ are preserved under
small perturbations of the parameters  because they have a finite volume.
If we define a probability density on the space of parameters  what is the probability of the event that
the function represented by the network has a given number of linear regions? By the above discussion 
the probability of getting a number of regions at least as large as the number resulting from any particular
choice of parameters (for a uniform measure within a bounded domain) is nonzero  even though it may be
very small. This is because there exists an epsilon-ball of non-zero volume around that particular choice of
parameters  for which at least the same number of linear regions is attained. For example  shallow rectifier
networks generically attain the maximal number of regions  even if in close vicinity of any parameter
choice there may be parameters corresponding to functions with very few regions.
For future work it would be interesting to study the partitions of parameter space RN into pieces where
the resulting functions partition their input-spaces into isomorphic linear regions  and to investigate how
many of these pieces of parameter space correspond to functions with a given number of linear regions.

2.6 Empirical Evaluation of Folding in Rectifier MLPs

We empirically examined the behavior of a trained MLP to see if it folds the input-space in the way described
above. First  we note that tracing the activation of each hidden unit in this model gives a piecewise linear
map Rn0 → R (from inputs to activation values of that unit). Hence  we can analyze the behavior of each

5

Figure 4: Folding of the real line into equal-length segments by a sum of rectifiers.

unit by visualizing the different weight matrices corresponding to the different linear pieces of this map. The
weight matrix of one piece of this map can be found by tracking the linear piece used in each intermediary
layer  starting from an input example. This visualization technique  a byproduct of our theoretical analysis 
is similar to the one proposed by Zeiler and Fergus (2013)  but is motivated by a different perspective.
After computing the activations of an intermediary hidden unit for each training example  we can  for
instance  inspect two examples that result in similar levels of activation for a hidden unit. With the linear
maps of the hidden unit corresponding to the two examples we perturb one of the examples until it results
in exactly the same activation. These two inputs then can be safely considered as points in two regions
identified by the hidden unit. In the Supplementary Material we provide details and examples of this
visualization technique. We also show inputs identified by a deep MLP.

3 Deep Rectifier Networks

In this section we analyze deep neural networks with rectifier units  based on the general observations
from Sec. 2. We improve upon the results by Pascanu et al. (2013)  with a tighter lower-bound on the
maximal number of linear regions of functions computable by deep rectifier networks. First  let us note the
following upper-bound  which follows directly from the fact that each linear region of a rectifier network
corresponds to a pattern of hidden units being active:
Proposition 4. The maximal number of linear regions of the functions computed by any rectifier network
with a total of N hidden units is bounded from above by 2N.

3.1 Illustration of the Construction
Consider a layer of n rectifiers with n0 input variables  where n ≥ n0. We partition the set of rectifier
units into n0 (non-overlapping) subsets of cardinality p = (cid:98) n/n0(cid:99) and ignore the remainder units. Consider
the units in the j-th subset. We can choose their input weights and biases such that

h1(x) = max{0  wx}  
h2(x) = max{0  2wx − 1}  
h3(x) = max{0  2wx − 2}  

...

hp(x) = max{0  2wx − (p − 1)}  

where w is a row vector with j-th entry equal to 1 and all other entries set to 0. The product wx selects
the j-th coordinate of x. Adding these rectifiers with alternating signs  we obtain following scalar function:

˜hj(x) =(cid:2)1 −1  1  . . .   (−1)p−1(cid:3) [h1(x)  h2(x)  h3(x)  . . .   hp(x)]

Since ˜hj acts only on the j-th input coordinate  we may redefine it to take a scalar input  namely the
j-th coordinate of x. This function has p linear regions given by the intervals (−∞  0]  [0  1]  [1  2] 
. . .   [p − 1 ∞). Each of these intervals has a subset that is mapped by ˜hj onto the interval (0  1)  as
illustrated in Fig. 4. The function ˜hj identifies the input-space strips with j-th coordinate xj restricted to
the intervals (0  1)  (1  2)  . . .   (p− 1  p). Consider now all the n0 subsets of rectifiers and the function ˜h =
. This function is locally symmetric about each hyperplane with a fixed j-th coordinate

(cid:2)˜h1  ˜h2  . . .   ˜hp

(cid:3)(cid:62)

6

(cid:62)

.

(3)

012123h1h2h3h1−h2h1−h2+h3x˜h(x)equal to xj = 1  . . .   xj = p − 1 (vertical lines in Fig. 4)  for all j = 1  . . .   n0. Note the periodic pattern
that emerges. In fact  the function ˜h identifies a total of pn0 hypercubes delimited by these hyperplanes.
Now  note that ˜h arises from h by composition with a linear function (alternating sums). This linear
function can be effectively absorbed in the preactivation function of the next layer. Hence we can treat ˜h as
being the function computed by the current layer. Computations by deeper layers  as functions of the unit
hypercube output of this rectifier layer  are replicated on each of the pn0 identified input-space hypercubes.

3.2 Formal Result

We can generalize the construction described above to the case of a deep rectifier network with n0 inputs
and L hidden layers of widths ni ≥ n0 for all i ∈ [L]. We obtain the following lower bound for the
maximal number of linear regions of deep rectifier networks:
Theorem 5. The maximal number of linear regions of the functions computed by a neural network with
n0 input units and L hidden layers  with ni ≥ n0 rectifiers at the i-th layer  is lower bounded by

(cid:32)L−1(cid:89)

(cid:22) ni

(cid:23)n0(cid:33) n0(cid:88)

(cid:18)nL

(cid:19)

n0

i=1

j

j=0

.

The next corollary gives an expression for the asymptotic behavior of these bounds. Assuming that
n0 = O(1) and ni = n for all i ≥ 1  the number of regions of a single layer model with Ln hidden units
behaves as O(Ln0nn0) (see Pascanu et al. 2013; Proposition 10). For a deep model  Theorem 5 implies:
Corollary 6. A rectifier neural network with n0 input units and L hidden layers of width n ≥ n0 can
compute functions that have Ω

linear regions.

(cid:17)

Thus we see that the number of linear regions of deep models grows exponentially in L and polynomially
in n  which is much faster than that of shallow models with nL hidden units. Our result is a significant
improvement over the bound Ω
obtained by Pascanu et al. (2013). In particular  our
result demonstrates that even for small values of L and n  deep rectifier models are able to produce
substantially more linear regions than shallow rectifier models. Additionally  using the same strategy
as Pascanu et al. (2013)  our result can be reformulated in terms of the number of linear regions per
parameter. This results in a similar behavior  with deep models being exponentially more efficient than
shallow models (see the Supplementary Material).

(cid:16)
( n/n0)(L−1)n0 nn0
(cid:16)
(cid:17)
( n/n0)L−1 nn0

4 Deep Maxout Networks

A maxout network is a feedforward network with layers defined as follows:
Definition 7. A rank-k maxout layer with n input and m output units is defined by a preactivation function
of the form f : Rn → Rm·k; f(x) = Wx+b  with input and bias weights W ∈ Rm·k×n  b ∈ Rm·k  and
activations of the form gj(z) = max{z(j−1)k+1  . . .   zjk} for all j ∈ [m]. The layer computes a function



g ◦ f : Rn → Rm; x (cid:55)→

max{f1(x)  . . .   fk(x)}

...

max{f(m−1)k+1(x)  . . .   fmk(x)}

 .

(4)

Since the maximum of two convex functions is convex  maxout units and maxout layers compute convex
functions. The maximum of a collection of functions is called their upper envelope. We can view the graph
of each linear function fi : Rn → R as a supporting hyperplane of a convex set in (n + 1)-dimensional
space. In particular  if each fi  i ∈ [k] is the unique maximizer fi = max{f(cid:48)
∈ [k]} at some input
neighborhood  then the number of linear regions of the upper envelope g1 ◦ f = max{fi : i ∈ [k]} is
exactly k. This shows that the maximal number of linear regions of a maxout unit is equal to its rank.
The linear regions of the maxout layer are the intersections of the linear regions of the individual maxout
units. In order to obtain the number of linear regions for the layer  we need to describe the structure of
the linear regions of each maxout unit  and study their possible intersections. Voronoi diagrams can be

i : i(cid:48)

7

(cid:0)k2m
(cid:1)  km}.

lifted to upper envelopes of linear functions  and hence they describe input-space partitions generated
by maxout units. Now  how many regions do we obtain by intersecting the regions of m Voronoi diagrams
with k regions each? Computing the intersections of Voronoi diagrams is not easy  in general. A trivial
upper bound for the number of linear regions is km  which corresponds to the case where all intersections
of regions of different units are different from each other. We will give a better bound in Proposition 8.
Now  for the purpose of computing lower bounds  here it will be sufficient to consider certain well-behaved
special cases. One simple example is the division of input-space by k−1 parallel hyperplanes. If m ≤ n  we
can consider the arrangement of hyperplanes Hi = {x ∈ Rn : xj = i} for i = 1  . . .   k − 1  for each max-
(cid:80)n
out unit j ∈ [m]. In this case  the number of regions is km. If m > n  the same arguments yield kn regions.
Proposition 8. The maximal number of regions of a single layer maxout network with n inputs and m
outputs of rank k is lower bounded by kmin{n m} and upper bounded by min{
Now we take a look at the deep maxout model. Note that a rank-2 maxout layer can be simulated by a
rectifier layer with twice as many units. Then  by the results from the last section  a rank-2 maxout network
with L − 1 hidden layers of width n = n0 can identify 2n0(L−1) input-space regions  and  in turn  it can
compute functions with 2n0(L−1)2n0 = 2n0L linear regions. For the rank-k case  we note that a rank-k
maxout unit can identify k cones from its input-domain  whereby each cone is a neighborhood of the
positive half-ray {rWi ∈ Rn : r ∈ R+} corresponding to the gradient Wi of the linear function fi for
all i ∈ [k]. Elaborating this observation  we obtain:
Theorem 9. A maxout network with L layers of width n0 and rank k can compute functions with at least
kL−1kn0 linear regions.
Theorem 9 and Proposition 8 show that deep maxout networks can compute functions with a number of
linear regions that grows exponentially with the number of layers  and exponentially faster than the maximal
number of regions of shallow models with the same number of units. Similarly to the rectifier model  this
exponential behavior can also be established with respect to the number of network parameters. We note
that although certain functions that can be computed by maxout layers can also be computed by rectifier
layers  the rectifier construction from last section leads to functions that are not computable by maxout
networks (except in the rank-2 case). The proof of Theorem 9 is based on the same general arguments
from Sec. 2  but uses a different construction than Theorem 5 (details in the Supplementary Material).

j=0

j

5 Conclusions and Outlook

We studied the complexity of functions computable by deep feedforward neural networks in terms of their
number of linear regions. We specifically focused on deep neural networks having piecewise linear hidden
units which have been found to provide superior performance in many machine learning applications
recently. We discussed the idea that each layer of a deep model is able to identify pieces of its input in
such a way that the composition of layers identifies an exponential number of input regions. This results
in exponentially replicating the complexity of the functions computed in the higher layers. The functions
computed in this way by deep models are complicated  but still they have an intrinsic rigidity caused by
the replications  which may help deep models generalize to unseen samples better than shallow models.
This framework is applicable to any neural network that has a piecewise linear activation function. For
example  if we consider a convolutional network with rectifier units  as the one used in (Krizhevsky et al.
2012)  we can see that the convolution followed by max pooling at each layer identifies all patches of the
input within a pooling region. This will let such a deep convolutional neural network recursively identify
patches of the images of lower layers  resulting in exponentially many linear regions of the input space.
The structure of the linear regions depends on the type of units  e.g.  hyperplane arrangements for shallow
rectifier vs. Voronoi diagrams for shallow maxout networks. The pros and cons of each type of constraint
will likely depend on the task and are not easily quantifiable at this point. As for the number of regions 
in both maxout and rectifier networks we obtain an exponential increase with depth. However  our bounds
are not conclusive about which model is more powerful in this respect. This is an interesting question
that would be worth investigating in more detail.
The parameter space of a given network is partitioned into the regions where the resulting functions have
corresponding linear regions. The combinatorics of such structures is in general hard to compute  even for
simple hyperplane arrangements. One interesting question for future analysis is whether many regions of the
parameter space of a given network correspond to functions which have a given number of linear regions.

8

References
M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University

Press  1999.

D. Ciresan  U. Meier  J. Masci  and J. Schmidhuber. Multi column deep neural network for traffic sign

classification. Neural Networks  32:333–338  2012.

G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control  Signals

and Systems  2(4):303–314  1989.

O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In NIPS  2011.
X. Glorot  A. Bordes  and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS  2011.
I. Goodfellow  D. Warde-Farley  M. Mirza  A. Courville  and Y. Bengio. Maxout networks. In Proc. 30th

International Conference on Machine Learning  pages 1319–1327  2013.

G. Hinton  L. Deng  G. E. Dahl  A. Mohamed  N. Jaitly  A. Senior  V. Vanhoucke  P. Nguyen  T. Sainath 
and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal
Processing Magazine  29(6):82–97  Nov. 2012.

K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal approximators.

Neural Networks  2:359–366  1989.

O. Krause  A. Fischer  T. Glasmachers  and C. Igel. Approximation properties of DBNs with binary hidden
units and real-valued visible units. In Proc. 30th International Conference on Machine Learning  pages
419–426  2013.

A. Krizhevsky  I. Sutskever  and G. Hinton. ImageNet classification with deep convolutional neural

networks. In NIPS  2012.

N. Le Roux and Y. Bengio. Deep belief networks are compact universal approximators. Neural

Computation  22(8):2192–2207  Aug. 2010.

G. Mont´ufar. Universal approximation depth and errors of narrow belief networks with discrete units.

Neural Computation  26  July 2014.

G. Mont´ufar and N. Ay. Refinements of universal approximation results for deep belief networks and

restricted Boltzmann machines. Neural Computation  23(5):1306–1319  May 2011.

V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In Proc. 27th

International Conference on Machine Learning  pages 807–814  2010.

R. Pascanu and Y. Bengio. Revisiting natural gradient for deep networks. In International Conference

on Learning Representations  2014.

R. Pascanu  G. Mont´ufar  and Y. Bengio. On the number of response regions of deep feed forward

networks with piece-wise linear activations. arXiv:1312.6098  Dec. 2013.

R. Stanley. An introduction to hyperplane arrangements. In Lect. notes  IAS/Park City Math. Inst.  2004.
J. Susskind  A. Anderson  and G. E. Hinton. The Toronto face dataset. Technical Report UTML TR

2010-001  U. Toronto  2010.

T. Zaslavsky. Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes.
Number 154 in Memoirs of the American Mathematical Society. American Mathematical Society 
Providence  RI  1975.

M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. arXiv:1311.2901 

2013.

9

,Guido Montufar
Razvan Pascanu
Kyunghyun Cho
Yoshua Bengio