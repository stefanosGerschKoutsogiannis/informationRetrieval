2018,Supervised autoencoders: Improving generalization performance with unsupervised regularizers,Generalization performance is a central goal in machine learning  particularly when learning representations with large neural networks. A common strategy to improve generalization has been through the use of regularizers  typically as a norm constraining the parameters. Regularizing hidden layers in a neural network architecture  however  is not straightforward. There have been a few effective layer-wise suggestions  but without theoretical guarantees for improved performance. In this work  we theoretically and empirically analyze one such model  called a supervised auto-encoder: a neural network that predicts both inputs (reconstruction error) and targets jointly. We provide a novel generalization result for linear auto-encoders  proving uniform stability based on the inclusion of the reconstruction error---particularly as an improvement on simplistic regularization such as norms or even on more advanced regularizations such as the use of auxiliary tasks. Empirically  we then demonstrate that  across an array of architectures with a different number of hidden units and activation functions  the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can significantly improve generalization.,Supervised autoencoders: Improving generalization

performance with unsupervised regularizers

Lei Le

Department of Computer Science

Indiana University
Bloomington  IN
leile@iu.edu

Andrew Patterson and Martha White

Department of Computing Science

University of Alberta

Edmonton  AB T6G 2E8  Canada

{ap3  whitem}@ualberta.ca

Abstract

Generalization performance is a central goal in machine learning  with explicit
generalization strategies needed when training over-parametrized models  like
large neural networks. There is growing interest in using multiple  potentially
auxiliary tasks  as one strategy towards this goal. In this work  we theoretically
and empirically analyze one such model  called a supervised auto-encoder: a
neural network that jointly predicts targets and inputs (reconstruction). We provide
a novel generalization result for linear auto-encoders  proving uniform stability
based on the inclusion of the reconstruction error—particularly as an improvement
on simplistic regularization such as norms. We then demonstrate empirically
that  across an array of architectures with a different number of hidden units and
activation functions  the supervised auto-encoder compared to the corresponding
standard neural network never harms performance and can improve generalization.

1

Introduction

Generalization is a central concept in machine learning: learning functions from a ﬁnite set of
data  that can perform well on new data. Generalization bounds have been characterized for many
functions  including linear functions [1]  and those with low-dimensionality [2  3] and functions from
reproducing kernel Hilbert spaces [4]. Many of these bounds are obtained through some form of
regularization  typically (cid:96)2 regularization [5  6] or from restricting the complexity of the function
class such as by constraining the number of parameters [1].
Understanding generalization performance is particularly critical for powerful function classes  such
as neural networks. Neural networks have well-known overﬁtting issues  with common strategies
to reduce overﬁtting including drop-out [7–9]  early stopping [10] and data augmentation [11  12] 
including adversarial training [13] and label smoothing [14]. Many layer-wise regularization strategies
have also been suggested for neural networks  such as with layer-wise training [15  16]  pre-training
with layer-wise additions of either unsupervised learning or supervised learning [15] and the use of
auxiliary variables for hidden layers [17].
An alternative direction that has begun to be explored is to instead consider regularization with the
addition of tasks. Multi-task learning [18] has been shown to improve generalization performance 
from early work showing learning tasks jointly reduces the required number of samples [19  20]
and later work particularly focused on trace-norm regularization on the weights of a linear  single
hidden-layer neural network for a set of tasks [21–23]. Some theoretical work has also been done
for auxiliary tasks [24]  with the focus of showing that the addition of auxiliary tasks can improve
the representation and so generalization. In parallel  a variety of experiments have demonstrated the
utility of adding layer-wise unsupervised errors as auxiliary tasks [15  16  25–27]. Auxiliary tasks
have also been explored through the use of hints for neural networks [28  18].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this work  we investigate an auxiliary-task model for which we can make generalization guarantees 
called a supervised auto-encoder (SAE). A SAE is a neural network that predicts both inputs and
outputs  and has been previously shown empirically to provide signiﬁcant improvements when used
in a semi-supervised setting [16] and deep neural networks [29]. We provide a novel uniform stability
result  showing that linear SAE—which consists of the addition of reconstruction error to a linear
neural network— provides uniform stability and so a bound on generalization error. We show that the
stability coefﬁcient decays similarly to the stability coefﬁcient under (cid:96)2 regularization [5]  providing
effective generalization performance but avoiding the negative bias from shrinking coefﬁcients. The
reconstruction error may incur some bias  but is related to the prediction task and so is more likely to
prefer a more robust model amongst a set of similarly effective models for prediction. This bound  to
the best of our knowledge  is (a) one of the ﬁrst bounds demonstrating that supervised dimensionality
reduction architectures can provide improved generalization performance and (b) provides a much
tighter bound than is possible from applying generalization results from multi-task learning [21–23]
and learning with auxiliary tasks [24]. Finally  we demonstrate empirically that adding reconstruction
error never harms performance compared to the corresponding neural network model  and in some
cases can signiﬁcantly improve classiﬁcation accuracy.

2 Supervised autoencoders and representation learning

We consider a supervised learning setting  where the goal is to learn a function for a vector of inputs
x ∈ Rd to predict a vector of targets y ∈ Rm. The function is trained on a ﬁnite batch of i.i.d.
data  (x1  y1)  . . .   (xt  yt)  with the aim to predict well on new samples generated from the same
distribution. To do well in prediction  a common goal is representation learning  where the input
xi are ﬁrst transformed into a new representation  for which it is straightforward to learn a simple
predictor—such as a linear predictor.
Auto-encoders (AE) are one strategy to extract a representation. An AE is a neural network  where
the outputs are set to x  the inputs. By learning to reconstruct the input  the AE extracts underlying or
abstract attributes that facilitate accurate prediction of the inputs. Linear auto-encoders with a single
hidden layer are equivalent to principle components analysis [30][31  Theorem 12.1]  which ﬁnds
(orthogonal) explanatory factors for the data. More generally  nonlinear auto-encoders have indeed
been found to extract key attributes  including high-level features [32] and Gabor-ﬁlter features [33].
A supervised auto-encoder (SAE) is an auto-encoder with the addition of a supervised loss on the
representation layer. For a single hidden layer  this simply means that a supervised loss is added to
the output layer  as in Figure 1. For a deeper auto-encoder  the innermost (smallest)1 layer would
have a supervised loss added to it—the layer that would usually be handed off to the supervised
learner after training the AE. More formally  consider a linear SAE  with a single hidden layer of size
k. The weights for the ﬁrst layer are F ∈ Rd×k. The weight for the output layer consist of weights
Wp ∈ Rk×m to predict y and Wr ∈ Rk×d to reconstruct x. Let Lp be the supervised (primary) loss
and Lr the loss for the reconstruction error. For example  in regression  both losses might be the
squared error  resulting in the objective

t(cid:88)

(cid:2)(cid:107)WpFxi − yi(cid:107)2

(cid:3) .

t(cid:88)

1
t

[Lp(WpFxi  yi) + Lr(WrFxi  xi)] = 1
2t

2 + (cid:107)WrFxi − xi(cid:107)2

2

(1)

i=1

i=1

The addition of a supervised loss to the auto-encoder should better direct representation learning
towards representations that are effective for the desired tasks. Conversely  solely training a represen-
tation according to the supervised tasks  like learning hidden layers in an neural network  is likely an
under-constrained problem  and will ﬁnd solutions that can well ﬁt the data but that do not ﬁnd under-
lying patterns in the data and do not generalize well. In this way  the combination of the two losses has
the promise to both balance extracting underlying structure  as well as providing accurate prediction
performance. There have been several empirical papers that have demonstrated the capabilities of
semi-supervised autoencoders [16  27  34]. Those results focus on the semi-supervised component 
where the use of auto-encoders enables the representation to be trained with more unlabeled data. In
this paper  however  we would like to determine if even in the purely supervised setting  the addition
of reconstruction error can have a beneﬁt for generalization.

1The size of the learned representations for deep  nonlinear AEs does not have to be small  but it is common
to learn such a lower-dimensional representations. For linear SAEs  the hidden layer size k < d  as otherwise
trivial solutions like the replication of the input are able to minimize the reconstruction error.

2

(a) (Linear) Supervised Autoencoder

(b) Deep Supervised Autoencoder

Figure 1: Two examples of Supervised Autoencoders  and where the supervised component—the
targets y—are included. We provide generalization performance results for linear SAEs  represented
by (a) assuming a linear activation to produce the hidden layer  with arbitrary convex losses on the
output layer  such as the cross-entropy for classiﬁcation. We investigate more general architectures in
the experiments  including single-hidden layer SAEs  represented by (a) with nonlinear activations to
produce the hidden layer  and deep SAEs  depicted in (b).

3 Uniform stability and generalization bounds for SAE

In this section  we show that including the reconstruction error theoretically improves generalization
performance. We show that linear supervised auto-encoders are uniformly stable  which means that
there is a small difference between models learned for any subsample of data  which differ in only
one instance. Uniformly stable algorithms are known to have good generalization performance [5].
Before showing this result  we discuss a few alternatives to justify why we pursue uniform stability.
There are at least two alternative strategies that could be considered to theoretically analyze these
models: using a multi-task analysis and characterizing the Rademacher complexity of the supervised
auto-encoder function class. The reconstruction error can in-fact be considered as multiple tasks 
where the multiple tasks help regularize or constrain the solution [35]. Previous results for multi-task
learning [21–23] demonstrate improved generalization error bounds when learning multiple tasks
jointly. Unfortunately  these bounds show performance is improved on average across tasks. For
our setting  we only care about the primary tasks  with the reconstruction error simply included
as an auxiliary task to regularize the solution. An average improvement might actually mean that
performance on the primary task degrades with inclusion of these other tasks. Earlier multi-task work
did consider improvement for each task [36]  but assumed different randomly generated features for
each task and all tasks binary classiﬁcation problem  which does not match this setting.
Another strategy is to characterize Rademacher complexity of supervised auto-encoders. There has
been some work characterizing the Rademacher complexity of unsupervised dimensionality reduction
techniques [3  Theorem 3.1]. To the best of our knowledge  however  there as yet does not appear to
be an analysis on complexity of supervised dimensionality reduction techniques. There is some work
on supervised dimension reduction [2  3]; however  this analysis assumes a dimensionality reduction
step followed by a supervised learning step  rather than a joint training procedure.
For these reasons  we pursue a third direction  where we treat the reconstruction error as a regularizer
to promote stability. Uniform stability has mainly been obtained using norm-based regulariza-
tion strategies  such as (cid:96)2. More recently  Liu et al. [24] showed that auxiliary tasks—acting as
regularizers—could also provide uniform stability. Because reconstruction error can be considered
to be an auxiliary task  our analysis resembles this auxiliary-task analysis. However  there are key
differences  as the result by Liu et al. [24] would be uninteresting if simply applied directly to our
setting. In particular  the uniform stability bound would not decay with the number of samples. The
bound decays proportionally to the number of samples for the primary task  but in the numerator
contains the maximum number of samples for an auxiliary task. For us  this maximum number is
exactly the same number of samples as for the primary task  and so they would cancel  making the
bound independent of the number of samples.

3

xhxEncoderDecoderyInputCodeOutputxh1h2h3xEncoderDecoderyInputCodeOutputPrimary Result
We now show that the parameter shared by the primary task and reconstruction error—the forward
model F—does not change signiﬁcantly with the change of one sample. This shows that linear SAEs
have uniform stability  which then immediately provides a generalization bound from [5  Theorem
12]. The proofs are provided in the appendix  for space.
Let Lp corresponds to the primary part (supervised part) of the loss  with weights Wp  and Lr
correspond to the auxiliary tasks that act as regularizers (the reconstruction error)  with weights Wr.
The full loss can be written

Lp (WpFxi  yp i) + Lr (WrFxi  yr i) .

(2)

t(cid:88)

i=1

L(F) =

1
t

For our speciﬁc setting  yr = x. We use more general notation  however  both to clarify the difference
between the inputs and outputs  and for future extensions to this theory for other (auxiliary) targets
yr. The loss where the m-th sample (xm  ym) is replaced by a random new instance (x(cid:48)

m  y(cid:48)

m) is

(cid:104)

(cid:0)WpFx(cid:48)

m  y(cid:48)

p m

Lm(F)=1
t

Lp

(cid:1)+Lr

(cid:0)WrFx(cid:48)

m  y(cid:48)

r m

t(cid:88)

(cid:1)+

i=1 i(cid:54)=m

Lp(WpFxi  yp i)+Lr(WrFxi  yr i)

.

(cid:105)

If we let F  Fm correspond to the optimal forward models for these two losses respectively  then the
algorithm is said to be β-uniformly stable if the difference in loss value for these two models for any
point (x  y) is bounded by β with high-probability

|Lp (WpFmx  yp) − Lp (WpFx  yp)| ≤ β

To obtain uniform stability  we will need to make several assumptions. The ﬁrst common assumption
is to assume bounded spaces  for the data and learned variables.
Assumption 1. The features satisfy (cid:107)x(cid:107)2 ≤ Bx and the primary targets satisfy (cid:107)yp(cid:107)2 ≤ By. The
parameters spaces are bounded 

W = {(Wp  Wr) ∈ Rk×m : (cid:107)Wp(cid:107)F ≤ BWp  (cid:107)Wr(cid:107)F ≤ BWr}
F = {F ∈ Rd×k : (cid:107)F(cid:107)F ≤ BF}

for some positive constants Bx  By  BF  BWp   BWr  where (cid:107) · (cid:107)F denotes Frobenius norm  namely
the square root of the sum of the squares of all elements.
For SAE  yr = x  and so (cid:107)x(cid:107)2 ≤ Bx implies that (cid:107)yr(cid:107)2 ≤ Bx.
Second  we need to ensure that the reconstruction error is both strongly convex and Lipschitz. The
next two assumptions are satisﬁed  for example  by the (cid:96)2 loss  Lr(ˆy  y) = (cid:107)ˆy − y(cid:107)2
2.
Assumption 2. The reconstruction loss Lr(·  y) is σr-admissible  i.e.  for possible predictions ˆy  ˆy(cid:48)

|Lr (ˆy  y) − Lr (ˆy(cid:48)  y)| ≤ σr(cid:107)ˆy − ˆy(cid:48)(cid:107)2.

Assumption 3. Lr(·  y) is c-strongly-convex (cid:104)ˆy − ˆy(cid:48) ∇Lr (ˆy  y) − ∇Lr (ˆy(cid:48)  y)(cid:105) ≥ c(cid:107)ˆy − ˆy(cid:48)(cid:107)2
The growth of the primary loss also needs to be bounded; however  we can use a less stringent
requirement than admissibility.
Assumption 4. For some σp > 0  for any F  Fm ∈ F 

2

|Lp (WpFmx  yp)−Lp (WpFx  yp)| ≤ σp(cid:107)Wr(Fm − F)x(cid:107)2

This requirement should be less stringent because we expect generally that for two forward models
F  Fm  (cid:107)Wp(F − Fm)x(cid:107)2 ≤ (cid:107)Wr(F − Fm)x(cid:107)2. The matrix Wp ∈ Rm×k projects the vector
d = (F − Fm)x into a lower-dimensional space  whereas Wr ∈ Rd×k projects d into a higher-
dimensional space. Because the nullspace of Wp is likely larger  it is more likely that Wp will send
a non-zero d. In fact  if Wr is full rank—which occurs if k is less than or equal to the intrinsic rank
of the data—then we can guarantee this assumption for some σp as long as Lp is σ-admissible  where
likely σp can be smaller than σ. In Corollary 1  we specify the value of σp under a full rank Wr and
σ-admissible Lp.
Finally  we assume that there is a representative set of feature vectors in the sampled data  both in
terms of feature vectors (Assumption 5) as well as loss values (Assumption 6).

4

Assumption 5. There exists a subset

error: x =(cid:80)n

i=1 αibi + η where αi ∈ R (cid:80)n

B = {b1  b2  ...  bn} ⊂ {x1  x2  ...  xt}

i=1 α2

i ≤ r (cid:107)η(cid:107)2 ≤ 
t .

such that with high probability any sampled feature vector x can be reconstructed by B with a small

Assumption 5 is similar to [24  Assumption 1]  except for our setting the features are the same for all
the tasks and the upper bound of (cid:107)η(cid:107) decreases as 1
t . This is a reasonable assumption since more
samples in the training set make it more likely to be able to reconstruct any x that will be observed
with non-negligible probability. In many cases  η = 0 is a mild assumption  as once d independent
vectors bi are observed  η = 0.
This representative set of points also needs to be representative in terms of the reconstruction error. In
particular  we need the average reconstruction error of the representative points to be upper bounded
by some constant factor of the average reconstruction error under the training set.
Assumption 6. For any two datasets S  Sm  where Sm has the m-th sample replaced with a random
new instance  let F  Fm be the corresponding optimal forward models. Let N contain only the
reconstruction errors  without the sample that is replaced

and Nb be the reconstruction error for the representative points

t(cid:88)

n(cid:88)

i=1

N (F) =

1
t

i=1 i(cid:54)=m

Lr (WrFxi  yr i)

Nb(F) =

1
n

Lr (WrFbi  yr bi )

(3)

(4)

where yr bi is the reconstruction target for representative point bi. Then  there exists a > 0 such that
for any small α > 0 

[Nb(F) − Nb((1 − α)F + αFm)] + [Nb(Fm) − Nb((1 − α)Fm + αF)]
≤ a [N (F) − N ((1 − α)F + αFm)] + a [N (Fm) − N ((1 − α)Fm + αF)] .

The above assumption does not require that the difference under N and Nb be small for the two F
and Fm; rather  it only requires that the increase or decrease in error at the two points Fm and F are
similar for N and Nb. Both the right-hand-side and left-hand-side in the assumption are nonnegative 
because of the convexity of N and Nb. Even if N is higher at F than Fm  and Nb is the opposite 
the above bound can hold  because it simply requires that the difference of Nb between Fm and F
be bounded above by the difference of N between F and Fm  up to some constant factor a. This
assumption is key  because we will need to use Nb to ensure that the bound decays with t  where Nb
is only dependent on the number of representative points  unlike N.
We can now provide the key result: SAE has uniform stability wrt the shared parameters F.
Theorem 1. Under Assumptions 1-6  for a randomly sampled x  y  with high probability
|Lp(WpFmx  y) − Lp (WpFx  y)| ≤ a(σr+σp)nσp

+ 2σpBWr BF

(cid:113)

(cid:18)

(cid:19)

(5)

r+

r2 + 4cBWr BFr
a(σr+σp)n

ct

t

Remark: We similarly get O( 1
t ) upper bound on instability from Bousquet and Elisseeff [5]  but
without requiring the (cid:96)2 regularizer. The (cid:96)2 indiscriminately reduces the magnitude of the weights;
the reconstruction error  on the other hand  regularizes  but potentially without strongly biasing the
solution. It can select amongst a set of possible forward models that predict the targets almost equally
well  but that also satisfy reconstruction error. A hidden representation that is useful for reconstructing
the inputs is likely to also be effective for predicting the targets—which are a function of the inputs.
Corollary 1. In Assumption 4  if Wp ∈ Rm×k  Wr ∈ Rd×k  d ≥ k ≥ m  Wr is full rank  Lp is
r (cid:107)F .
σ-admissible  then for W−1
Finally  we provide a few speciﬁc bounds  for particular Lr and Lp  to show how this more general
bound can be used (shown explicitly in Appendix B). For example  for a least-squares reconstruction
loss Lr  c = 2 and σr = 2BWr BFBx + 2Bx.

the inverse matrix of the ﬁrst k rows of Wr  σp = σ(cid:107)Wp(cid:107)F(cid:107)W−1

r

5

4 Experiments with SAE: Utility of reconstruction error

We now empirically test the utility of incorporating the reconstruction error into NNs  as a method
for regularization to improve generalization performance. Our goal is to investigate the impact of the
reconstruction error  and so we use the same architecture for SAE and NN  where the only difference
is the use of reconstruction error. We test several different architectures  namely single-hidden layer
SAEs with different activations  adding non-linearity with kernels before using a linear SAE and a
deep SAE with a bottleneck  namely a hidden layer with smaller size than that of the previous layer.
Experimental setup and Datasets. We used 10-fold cross-validation to choose the best meta-
parameters for each algorithm on each dataset. The meta-parameters providing the highest classiﬁca-
tion accuracy averaged across folds are chosen. Using the meta-parameters chosen by cross-validation 
we report the average accuracy and standard error across 20 runs  each with a different randomly
sampled training-testing splits. A new training-testing split is generated by shufﬂing all data points
together and selecting the ﬁrst samples to be the training set  and the remaining to be the testing set.
SUSY is a high-energy particle physics dataset [37]. The goal is to classify between a process where
supersymmetric particles are produced  and a background process where no detectable particles
are produced. SUSY was generated to discover hidden representations of raw sensor features for
classiﬁcation [37]  and has 8 features and 5 million data points.
Deterding is a vowel dataset [38] containing 11 steady-state vowels of British English spoken by 15
speakers. Every speaker pronounced each of the eleven vowel sounds six times giving 990 labeled
data points. The goal is to classify the vowel sound for each spoken vowel  where each speech signal
is converted into a 10-dimensional feature vector using log area ratios based on linear prediction
coefﬁcients. We normalized each feature between 0 and 1 through Min-Max scaling.
CIFAR-10 is an image dataset [39] with 10 classes and 60000 32x32 color images. The classes
include objects like horses  deer  trucks and airplanes. For each of the training-test splits  we used a
random subset of 50 000 images for training and 10 000 images for testing. We preprocessed the data
by averaging together the three colour channels creating gray-scale images to speed up computation.
MNIST is a dataset [40] of 70000 examples of 28x28 images of handwritten digits from 0 to 9.
We would like to note that for these two benchmark datasets—CIFAR and MNIST—impressive
performance has been achieved  such as with a highly complex  deep neural network model for
CIFAR [41]. Here  however  we use these datasets to investigate a variety of models  rather than to
match performance of the current state-of-the-art. We do not use the provided single training-testing
split  but rather treat these large datasets as an opportunity to generate many (different) training-test
splits for a thorough empirical investigation.
Overall results. Figure 2 shows the performance of SAE versus NN. On the Deterding  SUSY and
MNIST datasets  we compare them in three different architectures. First  we compare linear SAE
with linear NN  where there is no activation function from the input to the hidden layer. Second  we
nonlinearly transform the data with radial basis functions—a Gaussian kernel—and then use linear
SAE and linear NNs. The kernel expansion enables nonlinear functions to be learned  despite the fact
that the learning step can still beneﬁt from the optimality results provided for linear SAE. Third  we
use nonlinear activation functions  sigmoid and ReLu  from the input to the hidden layer. Though this
is outside the scope of the theoretical characterization  it is a relatively small departure and important
to understand the beneﬁts of the reconstruction error for at least simple nonlinear networks. We
investigate only networks with single hidden layers as a ﬁrst step  and to better match the networks
characterized in the theoretical guarantees.
Overall  we ﬁnd that SAE improves performance across settings  in some cases by several percent.
Getting even an additional 1% in classiﬁcation accuracy with just the addition of reconstruction error
to relatively simple models is a notable result. We summarize these results in Figure 2 and Table 1.
SAE and NN with the same architecture have similar sample variances  so we use a t-test for statistical
signiﬁcance. For all pairs but one  the average accuracy of SAE is statistically signiﬁcantly higher
than that of NN  with signiﬁcance level 0.0005  though in some cases the differences are quite small 
particularly on SUSY and MNIST. In other cases  particularly in kernel representations in Deterding 
SAE signiﬁcantly outperformed NN  with a jump by 18% in classiﬁcation accuracy. Because we
attempted to standardize the models  differing only in SAE using reconstruction error  these results
indicate that the reconstruction error has a clear positive impact on generalization performance.

6

(a) Deterding dataset

(b) SUSY dataset

(c) MNIST dataset

Figure 2: Test accuracy of a three layer neural network (NN) and our supervised auto-encoder model
(SAE)  on three datasets. We focus on the impact of using reconstruction error  and compare SAE
and NN with a variety of nonlinear structures  including sigmoid (SAE-Sigmoid and NN-Sigmoid) 
ReLu (SAE-ReLu and NN-ReLu) and Gaussian kernel (SAE-Kernel and NN-Kernel). Though not
showing the results in the ﬁgure  we also tried initializing NN with pre-trained autoencoders and
the performance is similar to NN  thus outperformed by SAE as well. Overall  SAE consistently
outpeforms NNs  though in some cases the advantage is small. Details are shown in Table 1.

MNIST

Training

Deterding

SUSY

Test

Training

63.34 ± 0.17
61.05 ± 0.14
99.38 ± 0.03
97.62 ± 0.05
90.22 ± 0.41
78.76 ± 0.08
93.15 ± 0.11
82.37 ± 0.41

Average Accuracy ± Standard Error Average Accuracy ± Standard Error Average Accuracy ± Standard Error
93.70 ± 0.30
54.98 ± 0.18
92.50 ± 0.22
52.50 ± 0.17
96.35 ± 0.05
90.67 ± 0.12
87.00 ± 0.14
96.20 ± 0.04
85.47 ± 0.52
98.25 ± 0.08
72.29 ± 0.67
98.10 ± 0.09
97.40 ± 0.18
92.52 ± 0.10
74.85 ± 0.20
96.20 ± 0.20

SAE
NN
SAE-Sigmoid
NN-Sigmoid
SAE-ReLu
NN-ReLu
SAE-Kernel
NN-Kernel
Table 1: The percentage accuracy for the results presented in Figure 2. SAE outperforms NNs in
terms of average test accuracy across settings. The only exception is the Gaussian kernel on SUSY 
where the advantage of NN-Kernel is extremely small. We report train accuracies for further insights
and completeness. Note that though there is some amount of overﬁtting occurring  the models were
given the opportunity to select a variety of regularization parameters for (cid:96)2 regularization as well as
dropout using cross-validation.

Training

76.50 ± 0.03
76.42 ± 0.02
77.80 ± 0.01
76.90 ± 0.01
72.04 ± 0.33
75.03 ± 0.11
77.31 ± 0.12
77.42 ± 0.06

Test

92.20 ± 0.40
91.20 ± 0.20
94.50 ± 0.10
92.50 ± 0.10
98.00 ± 0.10
97.30 ± 0.10
96.70 ± 0.20
95.50 ± 0.20

Test

76.48 ± 0.01
76.41 ± 0.02
77.79 ± 0.02
76.90 ± 0.03
71.99 ± 0.58
65.27 ± 0.17
77.27 ± 0.06
77.38 ± 0.06

In the next few sections  we highlight certain properties of interest  in addition to these more general
performance results. We highlight robustness to overﬁtting as model complexity is increased  for both
nonlinear activations and kernel transformations. For these experiments  we choose CIFAR  since
it is a more complex prediction problem with a large amount of data. We then report preliminary
conclusions on the strategy of over-parametrizing and regularizing  rather than using bottleneck layers.
Finally  we demonstrate the structure extracted by SAE  to gain some insight into the representation.
Robustness to overﬁtting. We investigate the impact of increasing the hidden dimension on CIFAR 
with sigmoid and ReLu activation functions from the input to the hidden layer. The results are
summarized in Figures 3a and 3b  where the hidden dimension is increased from 20 to as large as 10
thousand. Both results indicate that SAE can better take advantage of increasing model complexity 
where (a) the NN clearly overﬁt and obtained poor accuracy with a sigmoid transfer and (b) SAE
gained a 2% accuracy improvement over NNs when both used a ReLu transfer.
Results with kernels. The overall conclusion is that SAE can beneﬁt much more from model
complexity given by kernel representations  than NNs. In Table 1  the most striking difference
between SAE and NNs with kernels occurs for the Deterding dataset. SAE outperforms NN by
an entire 18%  going from 75% test accuracy to 92% test accuracy. For SUSY  SAE and NNs
were essentially tied; but for that dataset  all the nonlinear architectures performed very similarly 
suggesting little improvement could be gained.

7

SAENNSAE-SigmoidNN-SigmoidSAE-ReLuNN-ReLuSAE-KernelNN-Kernel020406080100Test AccuracySANNSA-SigmoidNN-SigmoidSA-ReLuNN-ReLuSA-KernelNN-Kernel020406080020406080100(a) Sigmoid activation

(b) ReLu activation

(c) Kernel Representation

Figure 3: Test accuracy of SAE and NN with a variety of nonlinear architectures on CIFAR  with
increasing model complexity. For the sigmoid and relu  the hidden dimension is increased; for
kernels  the number of centers is increased. (a) For the sigmoid activation  the NN suffers noticeably
from overﬁtting as the hidden dimension increases  whereas SAE is robust to the increase in model
complexity. (b) For the ReLu activation  under low model complexity  SAE performed more poorly
than the NN. However  given a larger hidden dimension—about a half as large as the input dimension—
it reaches the same level of performance and then is better able to take advantage of the increase
model complexity. The difference of about 2% accuracy improvement for such a simple addition—the
reconstruction error—is a striking result. (c) The result here is similar to ReLu. Note that the size of
the hidden dimension corresponds to 10% of the number of centers.

On CIFAR  we also investigated the impact of increasing the number of kernel centers  which
correspondingly increases model parameters and model complexity. We ﬁxed the hidden dimension
to 10% of the number of centers  to see if the SAE could still learn an appropriate model even with
an aggressive bottleneck  namely a hidden layer with a relatively very small size  making it hard to
reduce the reconstruction error. This helps to verify the hypothesis that the reconstruction error does
not incur much bias as a regularizer  and test a more practical setting where an aggressive bottleneck
can signiﬁcantly speed up computation and convergence rate. For the NN  because the number of
targets is 10  once the hidden dimension k ≥ 10  the bottleneck should have little to no impact on
performance  which is what we observe. The result is summarized in Figure 3c  which shows that
SAE initially suffers when model complexity is low  but then surpasses the NN with increasing model
complexity. In general  we anticipate the effects with kernels and SAE to be more pronounced with
more powerful selection of kernels and centers.

Demonstration of SAE with a Deep Architecture.
We investigate the effects of adding the
reconstruction loss to deep convolutional models on CIFAR. We use a network with two convolutional
layers of sizes {32  64} and 4 dense layers of sizes {2048  512  128  32} with ReLu activation. Unlike
our previous experiments we do not use grey-scale CIFAR  but instead use all three color channels
for the deep networks to make maximal use of the convolutional layers.
As shown in Figure 4  SAE outperforms NN consistently in both train and test accuracies  suggesting
that SAE is able to ﬁnd a different and better solution than NN in the optimization on the training data
and generalize well on the testing data. We show the performance of SAE with decreasing weight
on the predictive loss  which increases the effect of the reconstruction error. Interestingly  a value
of 0.01 performs the best  but began to degrade with lower values. At the extreme  for a weight of
0 which corresponds to an Autoencoder  performance is signiﬁcantly worse  so the combination of
both is necessary. We discuss other variants we tried in the caption for Figure 4  but the conclusions
remain consistent: SAE improves generalization performance over NNs.

5 Conclusion

In this paper  we systematically investigated supervised auto-encoders (SAEs)  as an approach to
using unsupervised auxiliary tasks to improve generalization performance. We showed theoretically
that the addition of reconstruction error improves generalization performance  for linear SAEs. We
showed empirically  across four different datasets  with a variety of architectures  that SAE never
harms performance but in some cases can signiﬁcantly improve performance  particularly when using
kernels and under ReLu activations  for both shallow and deep architectures.

8

NN test NN train SAE testSAE trainHidden Dimension2K4K6K8K10K0.20.220.240.260.280.30.320.34AccuracyHidden Dimension2K4K6K8K10KSAENN0.360.380.40.420.440.460.480.5Hidden Dimension2K4K6K8K10KSAENN0.360.380.40.420.440.460.480.5(a) Train accuracy

(b) Test accuracy

Figure 4: Train and Test accuracy of SAE and NN with a deep architecture. The numbers 0.01  0.1
and 1.0 denote the weights on the prediction error  with a constant weights of 1.0 on the reconstruction
error. We also compared to Auto-encoders  with a two-stage training strategy where the auto-encoder
is trained ﬁrst  with the representation then used for the supervised learner  but this performed
poorly (about 0.4 testing accuracy). We additionally investigated both dropout and (cid:96)2 regularization.
We ﬁnd that dropout increases the variance of independent runs  and improves each algorithm by
approximately three percentage points over its reported test set accuracy. Using (cid:96)2 regularization did
not improve performance. Under both dropout and (cid:96)2  the advantage of SAE over NN in both train
and test accuracies remained consistent  and so these graphs are representative for those additional
settings. Finally  we additionally compared to the ResNet-18 architecture [42]. For a fair comparison 
we do not use the image augmentation originally used in training ResNet-18. We ﬁnd that ResNet-18 
with nearly double the total learnable parameters  achieved only two percentage points higher on the
test set accuracy than our SAE with reconstructive loss.

References
[1] Sham M Kakade  Karthik Sridharan  and Ambuj Tewari. On the Complexity of Linear Prediction: Risk
Bounds  Margin Bounds  and Regularization. In Advances in Neural Information Processing Systems 
2008.

[2] Mehryar Mohri  Afshin Rostamizadeh  and Dmitry Storcheus. Generalization Bounds for Supervised
Dimensionality Reduction. In NIPS Workshop Feature Extraction Modern Questions and Challenges 
2015.

[3] Lee-Ad Gottlieb  Aryeh Kontorovich  and Robert Krauthgamer. Adaptive metric dimensionality reduction.

Theoretical Computer Science  2016.

[4] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and structural

results. The Journal of Machine Learning Research  2002.

[5] Olivier Bousquet and André Elisseeff. Stability and Generalization. Journal of Machine Learning Research 

2002.

[6] Tong Zhang. Covering Number Bounds of Certain Regularized Linear Function Classes. Journal of

Machine Learning Research  2002.

[7] Stefan Wager  Sida Wang  and Percy S Liang. Dropout Training as Adaptive Regularization. In Advances

in Neural Information Processing Systems  2013.

[8] N Srivastava  G Hinton  A Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout: A Simple Way

to Prevent Neural Networks from Overﬁtting . Journal of Machine Learning Research  2014.

[9] Hyeonwoo Noh  Tackgeun You  Jonghwan Mun  and Bohyung Han. Regularizing Deep Neural Networks
by Noise: Its Interpretation and Optimization. In Advances in Neural Information Processing Systems 
2017.

[10] N Morgan  H Bourlard  and 1990. Generalization and parameter estimation in feedforward nets: Some

experiments. In Advances in Neural Information Processing Systems  1990.

[11] Larry Yaeger  Richard Lyon  and Brandyn Webb. Effective Training of a Neural Network Character

Classiﬁer for Word Recognition. In Advances in Neural Information Processing Systems  1997.

9

NNSAE-0.1SAE-1.0SAE-0.010.50.650.60(cid:17)(cid:24)(cid:24)0.70.750.80.850.90.951.0162468101214Number of EpochsAccuracy0.50.650.60(cid:17)(cid:24)(cid:24)0.70.751713579111315NNSAE-1.0SAE-0.01SAE-0.1Number of Epochs[12] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. ImageNet Classiﬁcation with Deep Convolutional

Neural Networks. In Advances in Neural Information Processing Systems  2012.

[13] Seyed-Mohsen Moosavi-Dezfooli  Alhussein Fawzi  and Pascal Frossard. DeepFool - A Simple and
Accurate Method to Fool Deep Neural Networks. In IEEE Conference on Computer Vision and Pattern
Recognition  2016.

[14] Bin-Bin Gao  Chao Xing  Chen-Wei Xie  Jianxin Wu  and Xin Geng. Deep Label Distribution Learning

With Label Ambiguity. IEEE Transactions on Image Processing  2017.

[15] Yoshua Bengio  Pascal Lamblin  Dan Popovici  and Hugo Larochelle. Greedy layer-wise training of deep

networks. In Advances in Neural Information Processing Systems  2007.

[16] Marc’Aurelio Ranzato and Martin Szummer. Semi-supervised learning of compact document representa-

tions with deep networks. In International Conference on Machine Learning  2008.

[17] Miguel Á Carreira-Perpiñán and Weiran Wang. Distributed optimization of deeply nested systems. In

International Conference on Artiﬁcial Intelligence and Statistics  2014.

[18] Rich Caruana. Multitask Learning. Machine Learning  1997.

[19] Jonathan Baxter. Learning internal representations. In Annual Conference on Learning Theory  1995.

[20] Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research  2000.

[21] Andreas Maurer. Bounds for Linear Multi-Task Learning. Journal of Machine Learning Research  2006.

[22] Andreas Maurer and Massimiliano Pontil. Excess risk bounds for multitask learning with trace norm

regularization. In Annual Conference on Learning Theory  2013.

[23] Andreas Maurer  Massimiliano Pontil  and Bernardino Romera-Paredes. The Beneﬁt of Multitask Repre-

sentation Learning. arXiv:1509.01240v2  2015.

[24] Tongliang Liu  Dacheng Tao  Mingli Song  and Stephen J Maybank. Algorithm-Dependent Generalization
Bounds for Multi-Task Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence  2017.

[25] Jason Weston  Frédéric Ratle  and Ronan Collobert. Deep learning via semi-supervised embedding. In

International Conference on Machine Learning  2008.

[26] Alexander G Ororbia II  C Lee Giles  and David Reitter. Learning a Deep Hybrid Model for Semi-

Supervised Text Classiﬁcation. EMNLP  2015.

[27] Antti Rasmus  Mathias Berglund  Mikko Honkala  Harri Valpola  and Tapani Raiko. Semi-supervised

Learning with Ladder Networks. In Advances in Neural Information Processing Systems  2015.

[28] Yaser S Abu-Mostafa. Learning from hints in neural networks. J. Complexity  1990.

[29] Yuting Zhang  Kibok Lee  and Honglak Lee. Augmenting supervised neural networks with unsupervised
objectives for large-scale image classiﬁcation. In International Conference on Machine Learning  pages
612–621  2016.

[30] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples

without local minima. Neural Networks  1989.

[31] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of Machine Learning. MIT

Press  2012.

[32] Pascal Vincent  Hugo Larochelle  Isabelle Lajoie  Yoshua Bengio  and Pierre-Antoine Manzagol. Stacked
Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising
Criterion. Journal of Machine Learning Research  2010.

[33] Marc’Aurelio Ranzato  Christopher S Poultney  Sumit Chopra  and Yann LeCun. Efﬁcient Learning of
Sparse Representations with an Energy-Based Model. In Advances in Neural Information Processing
Systems  2006.

[34] Anupriya Gogna and Angshul Majumdar. Semi Supervised Autoencoder. In Neural Information Processing.

2016.

[35] R Caruana and V R De Sa. Promoting poor features to supervisors: Some inputs work better as outputs. In

Advances in Neural Information Processing Systems  1997.

10

[36] S Ben-David and R Schuller. Exploiting task relatedness for multiple task learning. Lecture Notes in

Computer Science  2003.

[37] Pierre Baldi  Peter Sadowski  and Daniel Whiteson. Searching for Exotic Particles in High-Energy Physics

with Deep Learning. arXiv:1509.01240v2  2014.

[38] David Henry Deterding. Speaker normalisation for automatic speech recognition. PhD thesis  University

of Cambridge  1990.

[39] A Krizhevsky and G Hinton. Learning Multiple Layers of Features from Tiny Images. Technical report 

University of Toronto  2009.

[40] Y LeCun  L Bottou  Y Bengio  and P Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  1998.

[41] Benjamin Graham. Fractional Max-Pooling. arXiv:1411.4000v2 [cs.LG]  2014.

[42] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

11

,Pedro Felzenszwalb
John Oberlin
Lucas Maystre
Matthias Grossglauser
Lei Le
Andrew Patterson
Martha White