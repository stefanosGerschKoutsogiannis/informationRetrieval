2014,Reducing the Rank in Relational Factorization Models by Including Observable Patterns,Tensor factorizations have become popular methods for learning from multi-relational data. In this context  the rank of a factorization is an important parameter that determines runtime as well as generalization ability. To determine conditions under which factorization is an efficient approach for learning from relational data  we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings  we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization. Experimentally  we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.,Reducing the Rank of Relational Factorization

Models by Including Observable Patterns

Maximilian Nickel1 2

Xueyan Jiang3 4

Volker Tresp3 4

1LCSL  Poggio Lab  Massachusetts Institute of Technology  Cambridge  MA  USA

2Istituto Italiano di Tecnologia  Genova  Italy

3Ludwig Maximilian University  Munich  Germany

4Siemens AG  Corporate Technology  Munich  Germany

mnick@mit.edu  {xueyan.jiang.ext volker.tresp}@siemens.com

Abstract

Tensor factorization has become a popular method for learning from multi-
relational data. In this context  the rank of the factorization is an important parame-
ter that determines runtime as well as generalization ability. To identify conditions
under which factorization is an efﬁcient approach for learning from relational data 
we derive upper and lower bounds on the rank required to recover adjacency tensors.
Based on our ﬁndings  we propose a novel additive tensor factorization model
to learn from latent and observable patterns on multi-relational data and present
a scalable algorithm for computing the factorization. We show experimentally
both that the proposed additive model does improve the predictive performance
over pure latent variable methods and that it also reduces the required rank — and
therefore runtime and memory complexity — signiﬁcantly.

1

Introduction

Relational and graph-structured data has become ubiquitous in many ﬁelds of application such
as social network analysis  bioinformatics  and artiﬁcial intelligence. Moreover  relational data is
generated in unprecedented amounts in projects like the Semantic Web  YAGO [27]  NELL [4]  and
Google’s Knowledge Graph [5] such that learning from relational data  and in particular learning from
large-scale relational data  has become an important subﬁeld of machine learning. Existing approaches
to relational learning can approximately be divided into two groups: First  methods that explain
relationships via observable variables  i.e. via the observed relationships and attributes of entities  and
second  methods that explain relationships via a set of latent variables. The objective of latent variable
models is to infer the states of these hidden variables which  once known  permit the prediction
of unknown relationships. Methods for learning from observable variables cover a wide range of
approaches  e.g. inductive logic programming methods such as FOIL [23]  statistical relational
learning methods such as Probabilistic Relational Models [6] and Markov Logic Networks [24]  and
link prediction heuristics based on the Jaccard’s Coefﬁcient and the Katz Centrality [16]. Important
examples of latent variable models for relational data include the IHRM and the IRM [29  10]  the
Mixed Membership Stochastic Blockmodel [1] and low-rank matrix factorizations [16  26  7]. More
recently  tensor factorization  a generalization of matrix factorization to higher-order data  has shown
state-of-the-art results for relationship prediction on multi-relational data [21  8  2  13]. The number
of latent variables in tensor factorization is determined via the number of latent components used
in the factorization  which in turn is bounded by the factorization rank. While tensor and matrix
factorization algorithms scale typically well with the size of the data — which is one reason for their
appeal — they often do not scale well with respect to the rank of the factorization. For instance 
RESCAL is a state-of-the art relational learning method based on tensor factorization which can be
applied to large knowledge bases consisting of millions of entities and billions of known facts [22].

1

However  while the runtime of the most scalable known algorithm to compute RESCAL scales
linearly with the number of entities  linearly with the number of relations  and linearly with the
number of known facts  it scales cubical with regard to the rank of the factorization [22].1 Moreover 
the memory requirements of tensor factorizations like RESCAL become quickly infeasible on large
data sets if the factorization rank is large and no additional sparsity of the factors is enforced. Hence 
tensor (and matrix) rank is a central parameter of factorization methods that determines generalization
ability as well as scalability. In this paper we study therefore how the rank of factorization methods
can be reduced while maintaining their predictive performance and scalability. We ﬁrst analyze under
which conditions tensor and matrix factorization requires high or low rank on relational data. Based
on our ﬁndings  we then propose an additive tensor decomposition approach to reduce the required
rank of the factorization by combining latent and observable variable approaches.
This paper is organized as follows: In section 2 we develop the main theoretical results of this paper 
where we show that the rank of an adjacency tensor is lower bounded by the maximum number
of strongly connected components of a single relation and upper bounded by the sum of diclique
partition numbers of all relations. Based on our theoretical results  we propose in section 3 a novel
tensor decomposition approach for multi-relational data and present a scalable algorithm to compute
the decomposition. In section 4 we evaluate our model on various multi-relational datasets.

Preliminaries We will model relational data as a directed graph (digraph)  i.e. as an ordered pair
Γ “ pV Eq of a nonempty set of vertices V and a set of directed edges E Ď V ˆ V. An existing
edge between node vi and v j will be denoted by vi (cid:32) v j. By a slight abuse of notation  ΓpYq will
indicate the digraph Γ associated with an adjacency matrix Y P t0 1uNˆN . Next  we will brieﬂy
review further concepts of tensor and graph theory that are important for the course of this paper.
Deﬁnition 1. A strongly connected component of a digraph Γ is a maximal subgraph Ψ for which
every vertex is reachable from any other vertex in Ψ by following the directional edges in the subgraph.
A strongly connected component is trivial if it consists only of a single element  i.e. if it is of the form
Ψ “ ptviu Hq  and nontrivial otherwise.
We will denote the number of strongly connected components in a digraph Γ by sccpΓq. The number
of nontrivially connected components will be denoted by scc`pΓq.
Deﬁnition 2. A digraph Γ “ pV Eq is a diclique if it is an orientation of a complete undirected
bipartite graph with bipartition pV1 V2q such that v1 P V1 and v2 P V2 for every edge v1 (cid:32) v2 P E.
Figure 3 in supplementary material A shows an example of a diclique. Please note that dicliques
consist only of trivially strongly connected components  as there cannot exist any cycles in a diclique.
Given the concept of a diclique  the diclique partitioning number of a digraph is deﬁned as:
Deﬁnition 3. The diclique partition number dppΓq of a digraph Γ “ pV Eq is the minimum number
of dicliques such that each edge e P E is contained in exactly one diclique.
Tensors can be regarded as higher-order generalizations of vectors and matrices. In the following  we
will only consider third-order tensors of the form X P RIˆJˆK   although many concepts generalize
to higher-order tensors. The mode-n unfolding (or matricization) of X arranges the mode-n ﬁbers
of X as the columns of a newly formed matrix and will be denoted by Xpnq. The tensor-matrix
product A “ X ˆn B multiplies the tensor X with the matrix B along the n-th mode of X such
that Apkq “ BXpkq. For a detailed introduction to tensors and these operations we refer the reader
to Kolda et al. [12]. The k-th frontal slice of a third-order tensor X P RIˆJˆK will be denoted by
Xk P RIˆJ . The outer product of vectors will be denoted by a ˝ b. In contrast to matrices  there exist
two non-equivalent notions of the rank of a tensor:
Deﬁnition 4. Let X P RIˆJˆK be a third-order tensor. The tensor rank t-rankpXq of X is deﬁned as
t-rankpXq “ mintr | X “
i“1 ai ˝ bi ˝ ciu where ai P RI   bi P RJ   and ci P RK . The multilinear
rank n-rankpXq of X is deﬁned as the tuple pr1 r2 r3q  where ri “ rank
To model multi-relational data as tensors  we use the following concept of an adjacency tensor:
Deﬁnition 5. Let G “ tpV EkquK
k“1 be a set of digraphs over the same set of vertices V  where
|V| “ N. The adjacency tensor of G is a third-order tensor X P t0 1uNˆNˆK with entries xi j k “ 1
if vi (cid:32) v j P Ek and xi j k “ 0 otherwise.

ř

`

r

˘

Xpiq

.

1Similar results can be obtained for state-of-the-art algorithms to compute the well-known CP and Tucker

decompositions. Please see the supplementary material A.3 for the respective derivations.

2

For a single digraph  an adjacency tensor is equivalent to the digraph’s adjacency matrix. Note that K
would correspond to the number of relation types in a domain.

2 On the Algebraic Complexity of Graph-Structured Data

In this section  we want to identify conditions under which tensor factorization can be considered
efﬁcient for relational learning. Let X denote an observed adjacency tensor with missing or noisy
entries from which we seek to recover the true adjacency tensor Y. Rank affects both the predictive
as well as the runtime performance of a factorization: A high factorization rank will lead to poor
runtime performance while a low factorization rank might not be sufﬁcient to model Y. We are
therefore interested in identifying upper and lower bounds on the minimal rank — either tensor rank
or multilinear rank — that is required such that a factorization can model the true adjacency tensor
Y. Please note that we are not concerned with bounds on the generalization error or the sample
complexity that is needed to learn a good model  but on bounds on the algebraic complexity that is
needed to express the true underlying data via factorizations. For sign-matrices Y P t˘1uNˆN   this
(
question has been discussed in combinatorics and communication complexity via their sign-rank
rank˘pYq  which is the minimal rank needed to recover the sign-pattern of Y:

ˇˇ@i  j : sgnpmi jq “ yi j

(cid:32)
rankpMq

(1)

.

rank˘pYq “ min
MPRNˆN

Although the concept of sign-rank can be extended to adjacency tensors  bounds based on the sign-
rank would have only limited signiﬁcance for our purpose  as no practical algorithms exist to ﬁnd
the solution to equation (1). Instead  we provide upper and lower bounds on tensor and multilinear
rank  i.e. bounds on the exact recovery of Y  for the following reasons: It follows immediately
from (1) that any upper-bound on rankpYq will also hold for rank˘pYq since it has to hold that
rank˘pYq ď rankpYq. Upper bounds on rankpYq can therefore provide insight under what conditions
factorizations can be efﬁcient on relational data — regardless whether we seek to recover exact values
or sign patterns. Lower bounds on rankpYq provide insight under what conditions the exact recovery
of Y can be inefﬁcient. Furthermore  it can be observed empirically that lower bounds on the rank are
more informative for existing factorization approaches to relational learning like [21  13  16] than
bounds on sign-rank. For instance  let Sn “ 2In ´ Jn be the “signed identity matrix” of size n  where
In denotes the n ˆ n identity matrix and Jn denotes the n ˆ n matrix of all ones. While it is known
that rank˘pSnq “ Op1q for any size n [17]  it can be checked empirically that SVD requires a rank
larger than n
Based on these considerations  we state now the main theorem of this paper  which bounds the
different notions of the rank of an adjacency tensor by the diclique partition number and the number
of strongly connected components of the involved relations:
Theorem 1. Tensor rank t-rankpYq and multilinear rank n-rankpYq “ pr1 r2 r3q of any adjacency
tensor Y P t0 1uNˆNˆK representing K relations tΓkpYkquK

2   i.e. a rank of Opnq  to recover the sign pattern of Sn.

ÿ

K
k“1

dppΓkq ě θ ě max

k

k“1 are bounded as
scc`pΓkq 

where θ is any of the quantities t-rankpYq  r1  or r2.
To prove theorem 1 we will ﬁrst derive upper and lower bounds on adjacency matrices and then show
how these bounds generalize to adjacency tensors.
Lemma 1. For any adjacency matrix Y P t0 1uNˆN it holds that dppΓq ě rankpYq ě scc`pΓq.
Proof. The upper bound of lemma 1 follows directly from the fact that dppΓpYqq “ rankNpYq and the
fact that rankNpYq ě rankpYq  where rankNpYq denotes the non-negative integer rank of the binary
(cid:3)
matrix Y [19  see eq. 1.6.5 and eq. 1.7.1].
Next we will prove the lower bound of lemma 1. Let λipYq denote the i-th (complex) eigenvalue
of Y and let ΛpYq denote the spectrum of Y P RNˆN   i.e. the multiset of (complex) eigenvalues of
Y. Furthermore  let ρpYq “ maxi |λipYq| be the spectral radius of Y. Now  recall the celebrated
Perron-Frobenius theorem:
Theorem 2 ([25  Theorem 8.2]). Let Y P RNˆN with yi j ě 0 be a non-negative irreducible matrix.
Then ρpYq ą 0 is a simple eigenvalue of Y associated with a positive eigenvector.

3

k

Ť

Please note that a nontrivial digraph is strongly connected iff its adjacency matrix is irreducible [3 
Theorem 3.2.1]. Furthermore  an adjacency matrix is nilpotent iff the associated digraph is acyclic [3 
Section 9.8]. Hence  the adjacency matrix of a strongly connected component Ψ is nilpotent iff Ψ is
trivial. Given these considerations  we can now prove the lower bound of lemma 1:
Lemma 2. For any non-negative adjacency matrix Y P RNˆN with yi j ě 0 of a weighted digraph Γ
it holds that rankpYq ě scc`pΓq.
Proof. Let Γ consist of k nontrivial strongly connected components. The Frobenius normal form B
of its associated adjacency matrix Y consists then of k irreducible matrices Bi on its block diagonal.
It follows from theorem 2 that each irreducible Bi has at least one nonzero eigenvalue. Since B is
block upper triangular  it holds also that ΛpBq “
i“1 ΛpBiq. As the rank of a square matrix is
larger or equal to the number of its nonzero eigenvalues  it follows that rankpBq ě k. Lemma 2
(cid:3)
follows from the fact that B is similar to Y and that matrix similarity preserves rank.
So far  we have shown that rankpYq of an adjacency matrix Y is bounded by the diclique covering
number and the number of nontrivial strongly connected components of the associated digraph. To
complete the proof of theorem 1 we will now show that these bounds for unirelational data translate
directly to multi-relational data and to the different notions of the rank of an adjacency tensor. In
particular we will show that both notions of tensor rank are lower bounded by the maximum rank of
a single frontal slice in the tensor and upper bounded by the sum of the ranks of all frontal slices:
Lemma 3. The tensor rank t-rankpYq and multilinear rank n-rankpYq “ pr1 r2 r3q of any third-order
tensor Y P RIˆJˆK with frontal slices Yk are bounded as
rankpYkq ě θ ě max
rankpYkq 
ř

ÿ

K
k“1

k

`ř

rmax “ rank

`
i“1 ck r ar bJ

i and all other entries are 0. It can be easily veriﬁed that

where θ is any of the quantities t-rankpYq  r1  or r2.
Proof. Due to space constraints  we will include only the proof for tensor rank. The proof for
multilinear rank can be found in supplementary material A.1. Let t-rankpYq “ r and rankpYkq “ rmax.
It can be seen from the deﬁnition of tensor rank that Yk “
r q. Consequently  it follows
˘
˘
ř
from the subadditivity of matrix rank  i.e. rankpA ` Bq ď rankpAq ` rankpBq  that
˘
ď
ck r ar bJ

i“1 ck rpar bJ
`
ck r ar bJ
ř
i“1 rank
ď 1. Now we will derive the upper bound
where the last inequality follows from rank
k rankpYkq that recovers Y exactly.
of lemma 3 by providing a decomposition of Y with rank r “
ř
k be the SVD of Yk with Sk “ diagpskq. Furthermore  let U “ rU1 U2 ¨¨¨ UKs 
Let Yk “ Uk SkVJ
V “ rV1 V2 ¨¨¨ VKs  and let S be a block-diagonal matrix where the i-th block on the diagonal is
i“1 ˆui ˝ ˆvi ˝ ˆsi provides an exact
equal to sJ
decomposition of Y  where r “
k rankpYkq and ˆui  ˆvi  and ˆsi are the i-th columns of the matrices
(cid:3)
U  V  and S. The inequality in lemma 3 follows since r is not necessarily minimal.
ř
Theorem 1 can now be derived by combining lemmas 1 and 3 what concludes the proof.

Discussion It can be seen from theorem 1 that factorizations can be computationally efﬁcient when
k dppΓkq is small. However  factorizations can potentially be inefﬁcient when scc`pΓkq is large
for any Γk in the data. For instance  consider an idealized marriedTo relation  where each person is
married to exactly one person. Evidently  for m marriages  the associated digraph would consist of
m strongly connected components  i.e. one component for each marriage. According to lemma 2 
a factorization model would at least require m latent components to recover this adjacency matrix
exactly. Consequently  an algorithm with cubic runtime complexity in the rank would only be able
to recover Y for this relation when the number of marriages is small  what limits its applicability
to these relations. A second important observation for multi-relational learning is that the lower
bound in theorem 1 depends only on the largest rank of a single frontal slice (i.e. a single adjacency
matrix) in Y. For multi-relational learning this means that regularities between different relations
can not decrease tensor or multilinear rank below the largest matrix rank of a single relation. For
instance  consider an N ˆ N ˆ 2 tensor Y where Y1 “ Y2. Clearly it holds that rankpYp3qq “ 1  such
that Y1 could easily be predicted from Y2 when Y2 is known. However  theorem 1 states that the rank
of the factorization must be at least rankpY1q — which can be arbitrarily large up to N — when

ď r

r

r

r

r

r

r

r

ř

4

the ﬁrst two modes of Y are also factorized. Please note that this is not a statement about sample
complexity or generalization error which can be reduced when factorizing all modes of a tensor  but
a statement about the minimal rank that is required to express the data. A last observation from the
previous discussion is that factorizations and observable variable methods excel at different aspects
of relationship prediction. For instance  predicting relationships in the idealized marriedTo relation
can be done easily with Horn clauses and link predication heuristics as listed in supplementary
material A.2. In contrast  factorization methods would be inefﬁcient in predicting links in this relation
as they would require at least one latent component for each marriage. At the same time  links in a
diclique of any size can trivially be modeled with a rank-2 factorization that indicates the partition
memberships  while standard neighborhood-based methods will fail on dicliques since — by the
deﬁnition of a diclique — there do not exist links within one partition yet the only vertices that share
neighbors are located in the same partition.

3 An Additive Relational Effects Model

RESCAL is a state-of-the-art relational learning method that is based on a constrained Tucker-
decomposition and as such is subject to bounds as in theorem 1. Motivated by the results of
section 2  we propose an additive tensor decomposition approach to combine the strengths of latent
and observable variable methods to reduce the rank requirements of RESCAL on multi-relational
data. To include the information of observable pattern methods in the factorization  we augment the
RESCAL model with an additive term that holds the predictions of observable pattern methods. In
particular  let X P t0 1uNˆNˆK be a third-order adjacency tensor and M P RNˆNˆP be a third-order
tensor that holds the predictions of an arbitrary number of relational learning methods. The proposed
additive relational effects model (ARE) decomposes X into

i Rka j `

P

p“1 wk pmi j p.

X « R ˆ1 A ˆ2 A ` M ˆ3 W 

single relationship is calculated in ARE viapxi j k “ aT

(2)
where A P RNˆr   R P RrˆrˆK and W P RKˆP. The ﬁrst term of equation (2) corresponds to
the RESCAL model which can be interpreted as following: The matrix A holds the latent variable
representations of the entities  while each frontal slice Rk of R is an asymmetric r ˆ r matrix that
models the interactions of the latent components for the k-th relation. The variable r denotes the
number of latent components of the factorization. An important aspect of RESCAL for relational
learning is that entities have a unique latent representation via the matrix A. This enables a relational
learning effect via the propagation of information over different relations and the occurrences of
entities as a subject or objects in relationships. For a detailed description of RESCAL we refer the
reader to Nickel et al. [21  22]. After computing the factorization (2)  the score for the existence of a

ř
The construction of the tensor M is of the following: Let F “ t f puP
p“1 be a set of given real-valued
functions f p : V ˆ V Ñ R which assign scores to each pair of entities in V. Examples of such score
functions include link prediction heuristics such as Common Neighbors  Katz Centrality  or Horn
clauses. Depending on the underlying model these scores can be interpreted as conﬁdences value or as
probabilities that a relationship exists between two entities. We collect these real-valued predictions
of P score functions in the tensor M P RNˆNˆP by setting mi j p “ f ppvi   v jq. Supplementary
material A.2 provides a detailed description of the construction of M for typical score functions. The
tensor M acts in the factorization as an independent source of information that predicts the existence
of relationships. The term Mˆ3 W can be interpreted as learning a set of weights wk p which indicate
how much the p-th score function in M correlates with the k-th relation in X. For this reason we refer
to M also as the oracle tensor. If M is composed of relation path features as proposed by Lao et al.
[15]  the term MW is closely related to the Path Ranking Algorithm (PRA) [15].
The main idea of equation (2) is the following: The term R ˆ1 A ˆ2 A is equivalent to the RESCAL
model and provides an efﬁcient approach to learn from latent patterns on relational data. The oracle
tensor M on the other hand is not factorized  such that it can hold information that is difﬁcult to
predict via latent variable methods. As it is not clear a priori which score functions are good predictors
for which relations  the term M ˆ3 W learns a weighting of how predictive any score function is for
any relation. By integrating both terms in an additive model  the term M ˆ3 W can potentially reduce
the required rank for the RESCAL term by explaining links that  for instance  reduce the diclique
partition number of a digraph. Rules and operations that are likely to reduce the diclique partition

5

ř

number of slices in X are therefore good candidates to be included in M. For instance  by including a
copy of the observed adjacency tensor X in M (or some selected frontal slices Xk)  the term M ˆ3 W
can easily model common multi-relational patterns where the existence of a relationship in one
relation correlates with the existence of a relationship between the same entities in another relation
via xi j k “
p‰k wk p xi j p. Since wk p is allowed to be negative  anti-correlations can be modeled
efﬁciently. ARE is similar in spirit to the model of Koren [14]  which extends SVD with additive
terms to include local neighborhood information in an uni-relational recommendation setting and
Jiang et al. [9] which uses an additive matrix factorization model for link prediction. Furthermore  the
recently proposed Google Knowledge Vault (KV) [5] considers a combination of PRA and a neural
network model related to RESCAL for learning from large multi-relational datasets. However  in KV
both models are trained separately and combined only later in a separate fusion step  whereas ARE
learns both models jointly what leads to the desired rank-reduction effect.
To compute ARE  we pursue a similar optimization scheme as used for RESCAL which has been
shown to scale to large datasets [22]. In particular  we solve the regularized optimization problem

}X ´ pR ˆ1 A ˆ2 A ` M ˆ3 Wq}2

F ` λ A}A}2

F ` λ R}R}2

F ` λW}W}2
F .

(3)

min
A R W

via alternating least-squares  which is a block-coordinate optimization method in which blocks of
variables are updated alternatingly until convergence. For equation (3) the variable blocks are given
naturally by the factors A  R  and W.
Updates for W Let E “ pX ´ R ˆ1 A ˆ2 Aq and I be the identity matrix. We rewrite equation (2)
as Ep3q « W Mp3q such that equation (3) becomes a regularized least-squares problem when solving
for W. It follows that updates for W can be computed via W Ð pMp3qMJ
p3q ` λW Iq´1Mp3qEJ
p3q.
However  performing the updates in this way would be very inefﬁcient as it involves the computation
of the dense N ˆ N ˆ K tensor R ˆ1 A ˆ2 A. This would quickly lead to scalability issues with
regard to runtime and memory requirements. To overcome this issue  we rewrite Mp3qEJ
p3q using the
equality pR ˆ1 A ˆ2 Aqp3qMJ
p3q. Updates for W can then be computed
efﬁciently as

p3q “ Rp3qpM ˆ1 AJ ˆ2 AJqJ

WJ Ð

Xp3qMJ

p3q ´ Rp3qpM ˆ1 AJ ˆ2 AJqJ
p3q

(4)
In equation (4) the dense tensor R ˆ1 A ˆ2 A is never computed explicitly and the computational
complexity with regard to the parameters N  K  and r is reduced from OpN 2Krq to OpN Kr3q.
Furthermore  all terms in equation (4) except Rp3qpM ˆ1 AJ ˆ2 AJqJ
p3q are constant and have only to
p3q and Mp3qMJ
be computed once at the beginning of the algorithm. Finally  Xp3qMJ
p3q are the products
of sparse matrices such that their computational complexity depends only on the number of nonzeros
in X or M. A full derivation of equation (4) can be found in the supplementary material A.4.

pMp3qMJ

p3q ` λW Iq´1.

”

ı

A Ð

´ÿ

¯´ÿ

Updates for A and R The updates for A and R can be derived directly from the RESCAL-ALS
algorithm by setting E “ X ´ M ˆ3 W and computing the RESCAL factorization of E. The updates
for A can therefore be computed by:
k ` EJ

k AJ ARk ` λI
Rk AJ ARJ
where Ek “ Xk ´ M ˆ3 wk and wk denotes the k-th row of W.
`
˘
The updates of R can be computed in the following way: Let A “ UΣVJ be the SVD of A  where σi
is the i-th singular value of A. Furthemore  let S be a matrix with entries si j “ σi σ j{pσ2
j ` λ Rq.
VJ  where “˚”
S ˚ pUJpXk ´ M ˆ3 wkqUq
An update of Rk can then be computed via Rk Ð V
denotes the Hadamard product. For a full derivation of these updates please see [20].

k ` RJ

¯´1

Ek ARJ

k ARk

K
k“1

K
k“1

i σ2

4 Evaluation

We evaluated ARE on various multi-relational datasets where we were in particular interested in its
generalization ability relative to the factorization rank. For comparison  we included the well-known

6

(a) Kinships

(b) PoliticalDiscussant

(c) CloseFriend

(d) BlogLiveJournalTwitter

(e) SocializeTwicePerWeek

(f) FacebookAllTaggedPhotos

Figure 1: Evaluation results for AUC-PR on the Kinships (1a) and Social Evolution data sets (1b-1f).

CP and Tucker tensor factorizations in the evaluation  as well as RESCAL and the non-latent model
X « Mˆ3 W (in the following denoted by MW). In all experiments  the oracle tensor M used in MW
and ARE is identical  such that the results of MW can be regarded as a baseline for the contribution
of the heuristic methods to ARE. Following [10  11  28  21] we used k-fold cross-validation for the
evaluation  partitioning the entries of the adjacency tensor into training  validation  and test sets. In
the test and validation folds all entries are set to 0. Due to the large imbalance of true and false
relationships  we used the area under the precision-recall curve (AUC-PR) to measure predictive
performance  which is known to behave better with imbalanced classes then AUC-ROC. All AUC-PR
results are averaged over the different test-folds. Links and references for the datasets used in the
evaluation are provided in the supplementary material A.5.

Social Evolution First  we evaluated ARE on a dataset consisting of multiple relations of persons
living in an undergraduate dormitory. From the relational data  we constructed a 84ˆ84ˆ5 adjacency
tensor where two modes correspond to persons and the third mode represents the relations between
these persons such as friendship (CloseFriend)  social media interaction (BlogLivejournalTwitter
and FacebookAllTaggedPhotos)  political discussion (PoliticalDiscussant)  and social interaction
(SocializeTwicePerWeek). For each relation  we performed link prediction via 5-fold cross validation.
The oracle tensor M consisted only of a copy of the observed tensor X. Including X in M allows
ARE to efﬁciently exploit patterns where the existence of a social relationship for a particular pair
of persons is predictive for other social interactions between exactly this pair of persons (e.g. close
friends are more likely to socialize twice per week). It can be seen from the results in ﬁgure 1(b ´ f )
that ARE achieves better performance than all competing approaches and already achieves excellent
performance at a very low rank  what supports our theoretical considerations.

Kinship The Kinship dataset describes the kinship relations in the Australian Alyawarra tribe
in terms of 26 kinship relations between 104 persons. The task in the experiment was to predict
unknown kinship relations via 10-fold cross validation in the same manner as in [21]. Table 1 shows
the improvement of ARE over state-of-the-art relational learning methods. Figure 1a shows the
predictive performance compared to the rank of multiple factorization methods. It can be seen that
ARE outperforms all other methods signiﬁcantly for lower rank. Moreover  starting from rank 40
ARE gives already comparable results to the best results in table 1. As in the previous experiments 
M consisted only of a copy of X. On this dataset  the copy of X allows ARE to model efﬁciently that
the relations in the data are mutually exclusive by setting wii ą 0 and wi j ă 0 for all i ‰ j. This
also explains the large improvement of ARE over RESCAL for small ranks.

7

102030405060708090100102030405060708090100RankAera under Precision−Recall Curve CPTuckerMWRESCALARE51015202530556065707580859095Rank CPTuckerMWRESCALARE510152025304550556065707580Rank CPTuckerMWRESCALARE5101520253080859095100RankAera under Precision−Recall Curve CPTuckerMWRESCALARE51015202530707580859095Rank CPTuckerMWRESCALARE510152025309095100Rank CPTuckerMWRESCALARELink Prediction on Semantic Web Data The SWRC ontology models a research group in terms
of people  publications  projects  and research interests. The task in our experiments was to predict
the afﬁliation relation  i.e. to map persons to research groups. We followed the experimental setting
in [18]: From the raw data  we created a 12058 ˆ 12058 ˆ 85 tensor by considering all directly
connected entities of persons and research groups. In total  168 persons and 5 research groups are
considered in the evaluation data. The oracle tensor M consisted again of a copy of X and of the
common neighbor heuristics Xi Xi and XJ
i . These heuristics were included to model patterns like
people who share the same research interest are likely in the same afﬁliation or a person is related
to a department if the person belongs to a group in the department. We also imposed a sparsity
penalty on W to prune away inactive heuristics during iterations. Table 2 shows that ARE improved
the results signiﬁcantly over three state-of-the-art link prediction methods for Semantic Web data.
Moreover  whereas RESCAL required a rank of 45  ARE required only a small rank of 15.

i XJ

Figure 2: Runtime on Cora

AUC
Rank

Table 1: Evaluation Results on Kinships.
MRC BCTF
[28]
[11]
90
86
-
-

LFM
[8]
94.6

(50 50 500)

96
100

RESCAL ARE

96.9
90

Table 2: Evaluation results on SWRC.

SVD Subtrees [18]
0.8

0.95

nDCG

RESCAL MW ARE
0.99

0.96

0.59

i XJ

Runtime Performance To evaluate the trade-off between runtime and predictive performance
we recorded the nDCG values of RESCAL and ARE after each iteration of the respective ALS
algorithms on the Cora citation database. We used the variant of Cora in which all publications are
organized in a hierarchy of topics with two to three levels and 68 leaves. The relational data consists
of information about paper citations  authors and topics from which a tensor of size 28073ˆ28073ˆ3
is constructed. The oracle tensor consisted of a copy of X and the common neighbor patterns Xi X j
and XJ
to model patterns such that a cited paper shares the same topic  a cited paper shares
the same author etc. The task of the experiment was to predict the leaf topic of papers by 5-fold
cross-validation on a moderate PC with Intel(R) Core i5 @3.1GHz  4G RAM. The optimal rank 220
for RESCAL was determined out of the range r10 300s via parameter selection. For ARE we used a
signiﬁcantly smaller rank 20. Figure 2 shows the runtime of RESCAL and ARE compared to their
predictive performance. It is evident that ARE outperforms RESCAL after a few iterations although
the rank of the factorization is decreased by an order of magnitude. Moreover  ARE surpasses
the best prediction results of RESCAL in terms of total runtime even before the ﬁrst iteration of
RESCAL-ALS has terminated.

j

5 Concluding Remarks

In this paper we considered learning from latent and observable patterns on multi-relational data.
We showed analytically that the rank of adjacency tensors is upper bounded by the sum of diclique
partition numbers and lower bounded by the maximum number of strongly connected components of
any relation in the data. Based on our theoretical results  we proposed an additive tensor factorization
approach for learning from multi-relational data which combines strengths from latent and observable
variable methods. Furthermore we presented an efﬁcient and scalable algorithm to compute the
factorization. Experimentally we showed that the proposed approach does not only increase the
predictive performance but is also very successful in reducing the required rank — and therefore also
the required runtime — of the factorization. The proposed additive model is one option to overcome
the rank-scalability problem outlined in section 2  however not the only one. In future work we intend
to investigate to what extent sparse or hierarchical models can be used to the same effect.

Acknowledgements Maximilian Nickel acknowledges support by the Center for Brains  Minds and Ma-
chines (CBMM)  funded by NSF STC award CCF-1231216. We thank Youssef Mroueh and Lorenzo Rosasco
for clarifying discussions on the theoretical part of this paper.

8

10−1100101102Time(s)0.700.720.740.760.780.800.820.84nDCGRESCALAREReferences

[1] E. M. Airoldi  D. M. Blei  S. E. Fienberg  and E. P. Xing. “Mixed Membership Stochastic Blockmodels”.

In: Journal of Machine Learning Research 9 (2008)  pp. 1981–2014.

[2] A. Bordes  J. Weston  R. Collobert  and Y. Bengio. “Learning Structured Embeddings of Knowledge

Bases”. In: Proceedings of the 25th Conference on Artiﬁcial Intelligence. 2011.

[3] R. A. Brualdi and H. J. Ryser. Combinatorial Matrix Theory. 1991.
[4] A. Carlson  J. Betteridge  B. Kisiel  B. Settles  Jr  and T. Mitchell. “Toward an Architecture for Never-

Ending Language Learning”. In: AAAI. 2010  pp. 1306–1313.

[5] X. L. Dong  K. Murphy  E. Gabrilovich  G. Heitz  W. Horn  N. Lao  T. Strohmann  S. Sun  and W. Zhang.
“Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion”. In: Proceedings of the
20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2014.

[6] L. Getoor  N. Friedman  D. Koller  A. Pfeffer  and B. Taskar. “Probabilistic Relational Models”. In:

Introduction to statistical relational learning. 2007  pp. 129–174.
P. D. Hoff. “Modeling homophily and stochastic equivalence in symmetric relational data”. In: Advances
in Neural Information Processing Systems. Vol. 20. 2008  pp. 657–664.

[7]

[8] R. Jenatton  N. Le Roux  A. Bordes  and G. Obozinski. “A latent factor model for highly multi-relational

data”. In: Advances in Neural Information Processing Systems. Vol. 25. 2012  pp. 3176–3184.

[9] X. Jiang  V. Tresp  Y. Huang  and M. Nickel. “Link Prediction in Multi-relational Graphs using Additive
Models.” In: Proceedings of International Workshop on Semantic Technologies meet Recommender
Systems & Big Data at the ISWC. Vol. 919. 2012  pp. 1–12.

[10] C. Kemp  J. B. Tenenbaum  T. L. Grifﬁths  T. Yamada  and N. Ueda. “Learning systems of concepts with

an inﬁnite relational model”. In: AAAI. Vol. 3. 2006  p. 5.

[11] S. Kok and P. Domingos. “Statistical Predicate Invention”. In: Proceedings of the 24th International

Conference on Machine Learning. 2007  pp. 433–440.

[12] T. G. Kolda and B. W. Bader. “Tensor Decompositions and Applications”. In: SIAM Review 51.3 (2009) 

pp. 455–500.

[13] T. G. Kolda  B. W. Bader  and J. P. Kenny. “Higher-order web link analysis using multilinear algebra”. In:

Proceedings of the Fifth International Conference on Data Mining. 2005  pp. 242–249.

[14] Y. Koren. “Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model”. In:
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. 2008  pp. 426–434.

[15] N. Lao and W. W. Cohen. “Relational retrieval using a combination of path-constrained random walks”.

In: Machine learning 81.1 (2010)  pp. 53–67.

[16] D. Liben-Nowell and J. Kleinberg. “The link-prediction problem for social networks”. In: Journal of the

American society for information science and technology 58.7 (2007)  pp. 1019–1031.

[17] N. Linial  S. Mendelson  G. Schechtman  and A. Shraibman. “Complexity measures of sign matrices”. In:

Combinatorica 27.4 (2007)  pp. 439–463.

[18] U. Lösch  S. Bloehdorn  and A. Rettinger. “Graph Kernels for RDF Data”. In: The Semantic Web:
Research and Applications - 9th Extended Semantic Web Conference  ESWC 2012. Vol. 7295. 2012 
pp. 134–148.
S. D. Monson  N. J. Pullman  and R. Rees. “A survey of clique and biclique coverings and factorizations
of (0 1)-matrices”. In: Bulletin of the ICA 14 (1995)  pp. 17–86.

[19]

[20] M. Nickel. “Tensor factorization for relational learning”. PhD thesis. LMU München  2013.
[21] M. Nickel  V. Tresp  and H.-P. Kriegel. “A Three-Way Model for Collective Learning on Multi-Relational
Data”. In: Proceedings of the 28th International Conference on Machine Learning. 2011  pp. 809–816.
[22] M. Nickel  V. Tresp  and H.-P. Kriegel. “Factorizing YAGO: scalable machine learning for linked data”.

In: Proceedings of the 21st international conference on World Wide Web. 2012  pp. 271–280.
[23]
J. R. Quinlan. “Learning logical deﬁnitions from relations”. In: Machine Learning 5 (1990)  pp. 239–266.
[24] M. Richardson and P. Domingos. “Markov logic networks”. In: Machine Learning 62.1 (2006)  pp. 107–

136.

[27]

[25] D. Serre. Matrices: Theory and applications. Vol. 216. 2010.
[26] A. P. Singh and G. J. Gordon. “Relational learning via collective matrix factorization”. In: Proc. of the
14th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining. 2008  pp. 650–658.
F. M. Suchanek  G. Kasneci  and G. Weikum. “Yago: A Core of Semantic Knowledge”. In: Proceedings
of the 16th international conference on World Wide Web. 2007  pp. 697–706.
I. Sutskever  R. Salakhutdinov  and J. Tenenbaum. “Modelling Relational Data using Bayesian Clustered
Tensor Factorization”. In: Advances in Neural Information Processing Systems 22. 2009  pp. 1821–1828.
[29] Z. Xu  V. Tresp  K. Yu  and H.-P. Kriegel. “Inﬁnite Hidden Relational Models”. In: Proc. of the Twenty-

[28]

Second Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence. 2006  pp. 544–551.

9

,Maximilian Nickel
Xueyan Jiang
Volker Tresp
Chen Huang
Chen Change Loy
Xiaoou Tang
Songbai Yan
Chicheng Zhang