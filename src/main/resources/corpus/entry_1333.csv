2017,Deep Dynamic Poisson Factorization Model,A new model  named as deep dynamic poisson factorization model  is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks  representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution  and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.,Deep Dynamic Poisson Factorization Model

Department of Information Management

Department of Information Management

Chengyue Gong

Peking University

cygong@pku.edu.cn

Win-bin Huang

Peking University

huangwb@pku.edu.cn

Abstract

A new model  named as deep dynamic poisson factorization model  is proposed
in this paper for analyzing sequential count vectors. The model based on the
Poisson Factor Analysis method captures dependence among time steps by neural
networks  representing the implicit distributions. Local complicated relationship
is obtained from local implicit distribution  and deep latent structure is exploited
to get the long-time dependence. Variational inference on latent variables and
gradient descent based on the loss functions derived from variational distribution is
performed in our inference. Synthetic datasets and real-world datasets are applied
to the proposed model and our results show good predicting and ﬁtting performance
with interpretable latent structure.

1

Introduction

+

There has been growing interest in analyzing sequentially observed count vectors x1  x2 . . .   xT . Such
data appears in many real world applications  such as recommend systems  text analysis  network
analysis and time series analysis. Analyzing such data should conquer the computational or statistical
challenges  since they are often high-dimensional  sparse  and with complex dependence across the
time steps. For example  when analyzing the dynamic word count matrix of research papers  the
amount of words used is large and many words appear only few times. Although we know the trend
that one topic may encourage researchers to write papers about related topics in the following year 
the relationship among each time step and each topic is still hard to analyze completely.
Bayesian factor analysis model has recently reached success in modeling sequentially observed
count matrix. They assume the data is Poisson distributed  and model the data under Poisson
Factorize Analysis (PFA). PFA factorizes a count matrix  where Φ ∈ RV ×K
is the loading matrix and
Θ ∈ RT×K
is the factor score matrix. The assumption that θt ∼ Gamma(θt−1  βt) is then included
[1  2] to smooth the transition through time. With property of the Gamma-Poisson distribution and
Gamma-NB process  inference via MCMC is used in these models. Considering the lack of ability to
capture the relationship between factors  a transition matrix is included in Poisson-Gamma Dynamical
System (PGDS) [2]. However  these models may still have some shortcomings in exploring the
long-time dependence among the time steps  as the independence assumption is made on θt−1 and
θt+1 if θt is given. In text analysis problem  temporal Dirichlet process [3] is used to catch the time
dependence on each topic using a given decay rate. This method may have weak points in analyzing
other data with different pattern long-time dependence  such as fanatical data and disaster data [3].
Deep models  which are also called hierarchical models in Bayesian learning ﬁeld  are widely used in
Bayesian models to ﬁt the deep relationship between latent variables. Examples of this include the
nested Chinese Restaurant Process [4]  nest hierarchical Dirichlet process [5]  deep Gaussian process
[6  7] and so on. Some models based on neural network structure or recurrent structure is also used 
such as the Deep Exponential Families [8]  the Deep Poisson Factor Analysis based on RBM or SBN
[9  10]  the Neural Autoregressive Density Estimator based on neural networks [11]  Deep Poisson

+

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: The visual representation of our model. In (a)  the structure of one-layer model is shown.
(b) shows the transmission of the posterior information. The prior and posterior distributions between

interfacing layers are shown in (c) and (d).

Factor Modeling with a recurrent structure based on PFA using a Bernoulli-Poisson link [12]  Deep
Latent Dirichlet Allocation uses stochastic gradient MCMC [23]. These models capture the deep
relationship among the shallow models  and often outperform shallow models.
In this paper  we present the Deep Dynamic Poisson Factor Analysis (DDPFA) model. Based on PFA 
our model includes recurrent neural networks to represent implicit distributions  in order to learn
complicated relationship between different factors among short time. Deep structure is included in
order to capture the long-time dependence. An inference algorithm based on variational inference is
used for inferring the latent variables. Parameters in the neural networks are learnt according to a
loss function based on the variational distributions. Finally  the DDPFA model is used on several
synthetic and real-world datasets  and excellent results are obtained in prediction and ﬁtting tasks.

2 Deep Dynamic Poisson Factorization Model

Assume that V -dimensional sequentially observed count data x1  x2 . . .   xT are represented as a
V × T count matrix X  a count data xvt ∈ {0  1  . . .} is generated by the proposed DDPFA model as
follows:

xvt ∼ P oisson((cid:80)K

k=1 θtkφvkλkψv)

(1)

(2)

where the latent variables θtk  φvk  λk and ψv are all positive variables. φk represents the strength of
the kth component and is treated as factor. θtk represents the strength of the kth component at the tth
time step. Feature-wise variable ψv captures the sparsity of the vth feature and λk recognizes the
importance of the kth component. According to the regular setting in [2  13-16]  the factorization is
regarded as X ∼ P oisson(ΦΘT ). Λ and Ψ can be absorbed into Θ. In this paper  in order to extract
the sparsity of vth feature or kth component and impose a feature-wise or temporal smoothness
constraint  Ψ and Λ are included in our model. The additive property of the Poisson distribution is
used to decompose the observed count of xvt as K latent counts xvtk  k ∈ {0  . . .   K}. In this way 
the model is rewritten as:

xvt =(cid:80)K

k=1 xvtk and xvtk ∼ P oisson(θtkφvkλkψv)

Capturing the complicated temporal dependence of Θ is the major purpose in this paper. In the
previous work  transition via Gamma-Gamma-Poisson distribution structure is used  where θt ∼
Gamma(θt−1  βt) [1]. Non-homogeneous Poisson process over time to model the stochastic
transition over different features is exploited in Poisson process models [17-19]. These models are
then trained via MCMC or variational inference. However  it is rough for these models to catch
complicated time dependence because of the weak points in their shallow structure in time dimension.
In order to capture the complex time dependence over Θ  a deep and long-time dependence model
with a dynamic structure over time steps is proposed. The ﬁrst layer over Θ is as follows:

θt ∼ p(θt|h(0)

t−c  ...  h(0)
t )

(3)

2

	θ0	θ1	θ2	θ3	θ4	θ5(a) 	θ0	θ1	θ2	θ3	θ4	θ5(b) 	θ0	θ1	θ2	θ3	θ4	θ5(d) 		h0(0)		h1(0)		h2(0)		h3(0)		h4(0)		h5(0)	θ0	θ1	θ2	θ3	θ4	θ5		h0(0)		h1(0)		h2(0)		h3(0)		h4(0)		h5(0)(c) t ∼ p(h(n)
h(n)

t

|h(n+1)
t−c

t ∼ p(h(N )

t

|h(N )

where c is the size of a window for analysis  and the latent variables in the nth layer  n ≤ N  are
indicated as follows:

t

t

t

t+c

  ...  h(n+1)

t

  . . .   h(n−1)

t−c−1  ...  h(N )
t−1)

t−c−1:t−1 as the prior information. This structure is illustrated in Figure 1.

(4)
) and h(N )
|·) is modeled as a recurrent neural network. Proba-
|h(n−1)
)  also modeled
t
is a K-dimensional latent variable in
is generated from a Gamma

where the implicit probability distribution p(h(n)
bility AutoEncoder with an auxiliary posterior distribution p(h(n)
as a neural network  is exploited in our training phase. h(n)
the nth layer at the tth time step. Specially  in the nth layer  h(N )
distribution with h(N )
Finally  prior parameters are placed over other latent variables for Bayesian inference. These variables
are generated as: φvk ∼ Gamma(αφ  βφ) andλk ∼ Gamma(αλ  βλ) and ψv ∼ Gamma(αψ  βψ).
Although Dirichlet distribution is often used as prior distribution [13  14  15] over φvk in previous
works  a Gamma distribution is exploited in our model due to the including of feature-wise parameter
ψv and the purpose for obtaining feasible factor strength of φk.
In real world applications  like recommendation systems  the observed binary count data can be
formulated by the proposed DDPFA model with a Bernoulli-Poisson link [1]. The distribution of b
given λ is called Bernoulli-Poisson distribution as: b = 1(x > 1)  x ∼ P oisson(λ) and the linking
distribution is rewritten as: f (b|x  λ) = e−λ(1−b)(1 − e−λ)
b. The conditional posterior distribution
is then (x|b  λ) ∼ b · P oisson+(λ)  where P oisson+(λ) is a truncated Poisson distribution  so the
MCMC or VI methods can be used to do inference. Non-count real-valued matrix can also be linked
to a latent count matrix via a compound Poisson distribution or a Gamma belief network [20].

t

3

Inference

q(Θ  Φ  Ψ  Λ  H) =(cid:81)

There are many classical inference approaches for Bayesian probabilistic model  such as Monte Carlo
methods and variational inference. In the proposed method  variational inference is exploited because
the implicit distribution is regarded as prior distribution over Θ. Two stages of inference in our model
are adopted: the ﬁrst stage updates latent variables by the coordinate-ascent method with a ﬁxed
implicit distribution  and the parameters in neural networks are learned in the second one.
Mean-ﬁeld Approximation: In order to obtain mean-ﬁeld VI  all variables are independent and
governed by its own variational distribution. The joint distribution of the variational distribution is
written as:

v t n k q(φvk|φ∗

vk)q(ψk|ψ∗

k)q(θtk|θ∗

tk)q(λk|λ∗

)

k)q(h(n)

(5)
where y∗ represents the prior variational parameter of the variable y. The variational parameters ν
are ﬁtted to minimize the KL divergence:

ν = argminν∗ KL(p(Θ  Φ  Ψ  Λ  H|X)||q(Θ  Φ  Ψ  Λ  H|ν))

(6)
The variational distribution q(·|ν∗) is then used as a proxy for the posterior. The objective actually is
equal to maximize the evidence low bound (ELBO) [19]. The optimization can be performed by a
coordinate-ascent method or a variational-EM method. As a result  each variational parameter can be
optimized iteratively while the remaining parameters of the model are set to ﬁxed value. Due to Eq.
2  the conditional distribution of (xvt1  . . .   xvtk) is a multinomial while its parameter is normalized
set of rates [19] and formulated as:

(xvt1  . . .   xvtk)|θt  φv  λ  ψv ∼ M ult(xvt·; θtφvλφv/(cid:80)

k θtkφvkλkψv)

(7)

tk |h(n)∗

tk

Given the auxiliary variables xvtk  the Poisson factorization model is a conditional conjugate model.
The complete conditional of the latent variables is Gamma distribution and shown as:

φvk|Θ  Λ  Ψ  α  β  X ∼ Gamma(αφ + xv·k  βφ + λkψvθ·k)
λk|Θ  Φ  Φ  α  β  X ∼ Gamma(αλ + x··k  βθ + θ·k

ψv|Θ  Λ  Φ  α  β  X ∼ Gamma(αψ + xv··  βψ +(cid:80)

v ψvφvk)
k λkφvkθ·k)

(cid:80)

(8)

3

Generally  these distributions are derived from conjugate properties between Poisson and Gamma
distribution. The posterior distribution of θtk described in Eq. 3 can be a Gamma distribution while
the prior h(0)

t−c:t is given as:

θtk|Ψ  Λ  Φ  h(0)  β  X ∼ Gamma(αθtk + xv·k  βθ + λk

v ψvφvk)

(9)

(cid:80)

where αθtk is calculated through a recurrent neural network with (h(0)
the posterior distribution of h(0)

tk described in Eq. 4 is given as:
tk |Θ  h(1)  β  X ∼ Gamma(αh(0)
h(0)

t−c  ...  h(0)

t ) as its inputs. Then

+ γh(0)

tk

tk

  βh)

(10)

tk

tk

tk

t−c

is the prior information given by the (n + 1)th layer  γh(n)

where αh(n)
given by the (n − 1)th layer. Here  the notation h(−1)
  ...  h(n+1)
a recurrent neural network using (h(n+1)
a recurrent neural network using (h(n−1)
  ...  h(n−1)
mentioned in Eq. 9 can be regarded as an implicit conditional distribution of θtk given (h(0)
And the distribution in Eq. 10 is an implicit distribution of αh(n)
(h(n−1)
Variational Inference: Mean ﬁeld variational inference can approximate the latent variables while
all parameters of a neural network are given. If the observed data satisﬁes xvt > 0  the auxiliary
variables xvtk can be updated by:

is the posterior information
is calculated through
is equal to θtk. αh(n)
) as its inputs. γh(n)
is calculated through
) as its inputs. Therefore  the distribution
t−c  ...  h(0)
t ).
) and

  ...  h(n−1)

given (h(n+1)

  ...  h(n+1)

t−c

t+c

t+c

).

tk

tk

tk

t

t

t

t

xvtk ∝ exp{Ψ (θshp

tk ) − logθrte

tk + Ψ (λshp

k

) − logλrte

k

+ Ψ (φshp

vk ) − logφrte

vk + Ψ (ψshp

v

) − logψrte
v }

(11)

where Ψ (·) is the digamma function. Variables with the superscript “shp” indicate the shape parameter
of Gamma distribution  and those with the superscript “rte” are the rate parameter of it. This update
comes from the expectation of the logarithm of a Gamma variable as (cid:104)logθ(cid:105) = Ψ (θshp) − log(θrte).
Here  θ is generated from a Gamma distribution and (cid:104)·(cid:105) represents the expectation of the variable.
Calculation of the expectation of the variable  obeyed Gamma distribution  is noted as (cid:104)θ(cid:105) =
θshp/θrte. Variables can be updated by mean-ﬁeld method as:

φvk ∼ Gamma(αφ + (cid:104)xv·k(cid:105)  βφ + (cid:104)λk(cid:105)(cid:104)ψv(cid:105)(cid:104)θ·k(cid:105))

λk ∼ Gamma(αλ + (cid:104)x··k(cid:105)  βθ + (cid:104)θ·k(cid:105)(cid:80)
ψv ∼ Gamma(αψ + (cid:104)xv··(cid:105)  βψ +(cid:80)
θtk ∼ Gamma(αθtk + (cid:104)xv·k(cid:105)  βθ + (cid:104)λk(cid:105)(cid:80)

v (cid:104)ψv(cid:105)(cid:104)φvk(cid:105))
k (cid:104)λk(cid:105)(cid:104)φvk(cid:105)(cid:104)θ·k(cid:105))

v (cid:104)ψv(cid:105)(cid:104)φvk(cid:105))

The latent variables in the deep structure can also be updated by mean-ﬁeld method:

tk ∼ Gamma(αh(n)
h(n)
= ff eed((cid:104)hn+1(cid:105))  αh(N )

= ff eed((cid:104)hN

tk

+ γh(n)

tk

  βh)

t−c−1:t−1(cid:105)) and γh(n)

= fback((cid:104)hn−1(cid:105))  γh(N )

t

t

=
t+c+1:t+1(cid:105)). ff eed(·) is the neural network constructing the prior distribution and fback(·)

where αh(n)
fback((cid:104)hN
is the neural network constructing the posterior distribution.
Probability AutoEncoder: This stage of the inference is to update the parameters of the neural
networks. The bottom layer is used by us as an example. Given all latent variables  these parameters
can be approximated by p(θt|h(0)
t ) =
Gamma(αθt  βh) is modeled by a RNN with the inputs (h(0)
t ) and the outputs  αθt. The

|θt+c  ...  θt). p(θ(n)
t−c  ...  h(0)

t ) and p(h(0)

t−c  ...  h(0)

t−c  ...  h(0)

|h(0)

t

t

t

t

(12)

(13)

(14)

4

t

|θt+c  ...  θt) is also modeled as a RNN with the inputs (θt+c  ...  θt) and the outputs  γh(0)
p(h(0)
. With the posterior distribution from Θ to H (0) and the prior distribution from H (0) to Θ  the
probability of Θ should be maximized. The loss function of these two neural networks is as follows:
(15)

{(cid:82) p(Θ|H (0))p(H (0)|Θ)dH (0)}

t

max
W

is generated by Θ 

where W represents the parameters in neural networks. Because the integration in Eq. 15
is intractable  a new loss function should include auxiliary variational variables H (0)(cid:48). As-
sume that H (0)(cid:48)
the optimization can be regarded as maximizing the
{p(Θ|H (0))} and
probability of Θ with minimal difference between H (0)(cid:48) and H (0) as max
min
W
Then approximating the variables generated from a distribution by its expectation  the loss function 
similar to variational AutoEncoder [21]  can be simpliﬁed to:

{KL(p(H (0)(cid:48)|Θ)||p(H (0)|H (1))}

W

{(cid:107)(cid:104)p(H (0)(cid:48)|Θ)(cid:105) − (cid:104)p(H (0)|H (1))(cid:105)(cid:107)2 + (cid:107)Θ − (cid:104)p(Θ|H (0))(cid:105)(cid:107)2}

(16)

min
W

Since only a few samples are drawn from one certain distrbution  which means sampling all latent
variables is high-cost and useless  differentiable variational Bayes is not suitable. As a result  we
focus more on ﬁtting data than generating data. In our objective  the ﬁrst term  a regularization 
encourages the data to be reconstructed from the latent variables  and the second term encourages the
decoder to ﬁt the data.
The parameters in the networks for nth and (n + 1)th layer are trained by the loss function:

min
W

{(cid:107)(cid:104)p(H (n+1)(cid:48)|H (n))(cid:105) − (cid:104)p(H (n)|H (n+1))(cid:105)(cid:107)2
+ (cid:107)H (n) − (cid:104)p(H (n)|H (n+1))(cid:105)(cid:107)2}

(17)

In order to make the convergence more stable  the term of Θ in the ﬁrst layer is collapsed into X by
using the ﬁxed latent variables approximated by mean-ﬁeld VI  and the loss function is as follows:
(18)

{(cid:107)(cid:104)p(H (0)(cid:48)|Θ)(cid:105) − (cid:104)p(H (0)|H (1))(cid:105)(cid:107)2 + (cid:107)X − (cid:104)Ψ(cid:105)(cid:104)Λ(cid:105)(cid:104)Φ(cid:105)(cid:104)p(Θ|H (0))(cid:105)(cid:107)2}

min
W

After the layer-wise training  all the parameters in neural networks are jointly trained by the ﬁne-tuning
trick in stacked AutoEncoder [22].

4 Experiments

In this section  four multi-dimensional synthetic datasets and ﬁve real-world datasets are exploited to
examine the performance of the proposed model. Besides  the results of three existed methods  PGDS 
LSTM  and PFA  are compared with results of our model. PGDS is a dynamic Poisson-Gamma
system mentioned in Section 1  and LSTM is a classical time sequence model. In order to prove the
deep relationship learnt by the deep structure can improve the performance  a simple PFA model is
also included as a baseline.
All hyperparameters of PGDS set in [2] are used in this paper. 1000 times gibbs sampling iterations
for PGDS is performed  100 iterations used mean-ﬁeld VI for PFA is performed  and 400 epochs is
executed for LSTM. The parameters in the proposed DDPFA model are set as follows:α(λ φ ψ) =
1  β(λ φ ψ) = 2  α(θ h) = 1  β(θ h) = 1. The iterations is set to 100. The stochastic gradient
descent for the neural networks is executed 10 epochs in each iteration. The size of the window is 4.
Hyperparameters of PFA are set as the same to our model. Data in the last time step is exploited as
the predicting target in a prediction task. Mean squared error (MSE) between the ground truth and
the estimated value and the predicted mean squared error (PMSE) between the ground truth and the
predicted value in next time step are exploited to evaluate the performance of each model.

4.1 Synthetic Datasets

The multi-dimensional synthetic datasets are obtained by using the following functions where the
subscript stands for the index of dimension:

5

Data Measure

SDS1

SDS2

SDS3

SDS4

MSE
PMSE
MSE
PMSE
MSE
PMSE
MSE
PMSE

Table 1: The result on the synthetic data
LSTM

DDPFA

PGDS

0.15 ± 0.01
2.07 ± 0.02
0.06 ± 0.01
2.01 ± 0.02
0.10 ± 0.02
2.14 ± 0.04
0.15 ± 0.03
1.48 ± 0.04

1.48 ± 0.00
5.96 ± 0.00
3.38 ± 0.00
3.50 ± 0.01
1.62 ± 0.00
4.33 ± 0.01
2.92 ± 0.00
6.41 ± 0.01

2.02 ± 0.23
2.94 ± 0.31
1.83 ± 0.04
2.41 ± 0.06
1.13 ± 0.06
3.03 ± 0.05
4.30 ± 0.26
4.67 ± 0.24

PFA

-

1.61 ± 0.00
4.42 ± 0.00
1.34 ± 0.00
0.25 ± 0.00

-

-

-

(mod 2)  f2(t) = 2t

(mod 2)  f2(t) = 2t

SDS1:f1(t) = f2(t) = t  f3(t) = f4(t) = t + 1 on the interval t = [1 : 1 : 6].
SDS2:f1(t) = t
(mod 2) + 2  f3(t) = t on the interval t = [1 : 1 : 20].
SDS3:f1(t) = f2(t) = t  f3(t) = f4(t) = t + 1  f5(t) = I(4|t) on the interval t = [1 : 1 : 20] 
where I is an indicator function.
SDS4:f1(t) = t
t = [1 : 1 : 100].
The number of factor is set to K = 3  and the number of the layers is 2. Both ﬁtting and predicting
tasks are performed in each model. The hidden layer of LSTM is 4 and the size in each layer is 20. In
Table 1  it is obviously that DDPFA has the best performance in ﬁtting and prediction task of all the
datasets. Note that the complex relationship learnt from the time steps helps the model catch more
time patterns according to the results of DDPFA  PGDS and PFA. LSTM performs worse in SDS4
because the noise in the synthetic data and the long time steps make the neural network difﬁcult to
memorize enough information.

(mod 10) on the interval

(mod 2) + 2  f3(t) = t

4.2 Real-world Datasets

Five real-world datasets are used as follows:
Integrated Crisis Early Warning System (ICEWS): ICEWS is an international relations event data set
extracted from news corpora used in [2]. We therefore treated undirected pairs of countries i ↔ j as
features and created a count matrix for the year 2003. The number of events for each pair during each
day time step is counted  and all pairs with fewer than twenty-ﬁve total events is discarded  leaving
T = 365  V = 6197  and 475646 events for the matrix.
NIPS corpus (NIPS): NIPS corpus contains the text of every NIPS conference paper from the year
1987 to 2003. We created a single count matrix with one column per year. The dataset is downloaded
from Gal’s page 1  with T = 17  V = 14036  with 3280697 events for the matrix.
Ebola corpus (EBOLA)2 : EBOLA corpus contains the data for the 2014 Ebola outbreak in West
Africa every day from Mar 22th  2014 to Jan 5th 2015  each column represents the cases or deaths in
a West Africa country. After data cleaning  the dataset is with T = 122  V = 16.
International Disaster(ID)3 : The International Disaster dataset contains essential core data on the
occurrence and effects of over 22 000 mass disasters in the world from 1900 to the present day. A
count matrix with T = 115 and V = 12 is built from the events of disasters occurred in Europe from
the year 1902 to 2016  classiﬁed according to their disaster types.
Annual Sheep Population(ASP)4 : The Annual Sheep Population contains the sheep population in
England & Wales from the year 1867 to 1939 yearly. The data matrix is with T = 73  V = 1.

1http://ai.stanford.edu/gal/data.html
2https://github.com/cmrivers/ebola/blob/master/country_timeseries.csv
3http://www.emdat.be/
4https://datamarket.com/data/list/?q=provider:tsdl

6

Data

Measure

ICEWS

NIPS

EBOLA

ID

ASP

MSE
PMSE
MSE
PMSE
MSE
PMSE
MSE
PMSE
MSE
PMSE

3.05 ± 0.02
0.96 ± 0.03
51.14 ± 0.03
289.21 ± 0.02
381.82 ± 0.13
490.32 ± 0.12
1.59 ± 0.01
5.18 ± 0.01
14.17 ± 0.02
21.23 ± 0.04

3.21 ± 0.01
0.97 ± 0.02
54.71 ± 0.08
337.60 ± 0.10
516.57 ± 0.01
1071.01 ± 0.01
3.45 ± 0.00
10.44 ± 0.00
2128.47 ± 0.02
760.42 ± 0.02

Table 2: The result on the real-world data
DDPFA

PGDS

LSTM

4.53 ± 0.04
6.30 ± 0.03

1053.12 ± 39.01
1728.04 ± 38.42
4892.34 ± 10.21
5839.26 ± 11.92
11.19 ± 1.32
10.37 ± 1.54

17962.47 ± 14.12
21324.72 ± 17.48

PFA

-

-

3.70 ± 0.01
69.05 ± 0.43
1493.32 ± 0.21
4.41 ± 0.01
388.02 ± 0.01

-

-

-

(a) PGDS

(b) DDPFA

Figure 2: The visual of the factor strength in each time step of the ICEWS data  the data is

normalized each time step. In (a)  the result of PGDS shows the factors are shrunk to some local time

steps. In (b)  the result of DDPFA shows the factors are not taking effects locally.

We set K = 3 for ID and ASP datasets  while set K = 10 for the others. The size of the hidden
layers of the LSTM is 40. The settings of remainder parameters here are the same as those in the
above experiment. The results of the experiment are shown in Table 2.
Table 2 shows the results of four different models on the ﬁve datasets  and the proposed model
DDPFA has satisfying performance in most experiments although the DDPFA’s result in ICEWS
prediction task is not good enough. While smoothed data obtained from the transition matrix in
PGDS performs well in this prediction task. However  In EBOLA and ASP datasets  PGDS fails in
catching complicated time dependence. And it is a tough challenge for LSTM network to memorize
enough useful patterns while its input data includes long-time patterns or the dimension of the data is
particular high.
According to the observation in Figure 2  it can be shown that the factors learnt by our model are
not activated locally compared to PGDS. Natrually  in real-world data  it is impossible that only
one factor happens in one time step. For example  in the ICEWS dataset  the connection between
Israel and Occupied Palestinian Territory still remains strong during the Iraq War or other accidents.
Figure 2(a) reveals that several factors at a certain time step are not captured by PGDS. In Figure 3 
the changes of two meaningful factors in ICEWS is shown. These two factor  respectively  indicate
Israel-Palestinian conﬂict and six-party talks. The long-time activation of factors is shown in thi
ﬁgure  since DDPFA model can capture weak strength along time.
In Table 3  we show the performance of our model with different sizes. From the table  performance
cannot be improved distinctly by adding more layers or adding more variables in upper layer. It is
also noticed that expanding the dimension in bottom layer is more useful than in upper layers. The
results reveal two problems of proposed DDPFA: "pruning" and uselessness of adding network layers.

7

(a)

(b)

Figure 3: The visual of the top two factors of the ICEWS data generated by DDPFA method. In (a) 

’Japan–Russian Federation’  ’North Korea–United States’  ’Russian Federation–United States’ 
’South Korea–United States’  and ’China–Russian Federation’ are the largest features due to their

loading weights. This factor stands for six-party talks and other accidents about it. In (b) 

’Israel–Occupied Palestinian Territory’  ’Israel–United States’  ’Occupied Palestinian

Territory–United States’ are the largest features and it stands for the Israeli-Palestinian conﬂict.

Table 3: MSE on real datasets with different sizes.

Size

10-10-10

10-10-10 (ladder structure)

10-10

32-32-32

32-32-32 (ladder structure)

32-64-64
64-32-32

ICEWS NIPS
51.24
49.81
51.14
50.12
49.26
50.18
50.04

2.94
2.88
3.05
2.95
2.86
2.93
2.90

EBOLA
382.17
379.08
381.82
379.64
377.81
380.01
378.87

[25] notices hierarchical latent variable models do not take advantage of the structure  and gives
such a conclusion that only using the bottom latent layer of hierarchical variational autoencoders
should be enough. In order to solve this problem  the ladder-like architecture  in which each layer
combines independent variables with latent variables depend on the upper layers  is used in our model.
It is noticed that using ladder architecture could reach much better results from Table 3. Another
problem  "pruning"  is a phenomenon where the optimizer severs connections between most of the
latent variables and the data [24]. In our experiments  it is noticed that some dimenisions in the latent
layers only contain data noise. This problem is also found in differentiable variational Bayes and
solved by using auxiliary MCMC strcuture [24]. Therefore  we believe this problem is caused by
MF-variational inference used in our model and we hope it can be solved if we try other inference
methods.

5 Summary

A new model  called DDPFA  is proposed to obtain long-time and complicated dependence in time
series count data. Inference in DDPFA is based on variational method for estimating the latent
variables and approximating parameters in neural networks. In order to show the performance of the
proposed model  four multi-dimensional synthetic datasets and ﬁve real-world datasets  ICEWS  NIPS
corpus  EBOLA  International Disaster and Annual Sheep Population  are used  and the performance
of three existed methods  PGDS  LSTM  and PFA  are compared. According to our experimental
results  DDPFA has better effectivity and interpretability in sequential count analysis.

8

References

[1] A. Ayan  J. Ghosh  & M. Zhou. Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices.
AISTATS  2015.
[2] A. Schein  M. Zhou  & H. Wallach. Poisson–Gamma Dynamical Systems. NIPS  2016.
[3] A. Ahmed  & E. Xing. Dynamic Non-Parametric Mixture Models and The Recurrent Chinese Restaurant
Process. SDM  2008.
[4] D. M. Blei  D. M. Grifﬁths  M. I. Jordan  & J. B. Tenenbaum. Hierarchical topic models and the nested
Chinese restaurant process. NIPS  2004.
[5] J. Paisley  C. Wang  D. M. Blei  & M. I. Jordan. Nested hierarchical Dirichlet processes. PAMI  2015.
[6] T. D. Bui  D. Hernándezlobato  Y. Li  & et al. Deep Gaussian Processes for Regression using Approximate
Expectation Propagation. ICML  2016.
[7] T. D. Bui  D. Thang  E. Richard  & et al. A unifying framework for sparse Gaussian process approximation
using Power Expectation Propagation. arXiv:1605.07066.
[8] R. Ranganath  L. Tang  L. Charlin  & D. M. Blei. Deep exponential families. AISTATS  2014.
[9] Z. Gan  C. Chen  R. Henao  D. Carlson  & L. Carin. Scalable deep Poisson factor analysis for topic modeling.
ICML  2015.
[10] Z. Gan  R. Henao  D. Carlson  & L. Carin. Learning deep sigmoid belief networks with data augmentation.
AISTATS  2015.
[11] H. Larochelle & S. Lauly. A neural autoregressive topic model. NIPS  2012.
[12] H. Ricardo  Z. Gan  J. Lu & L. Carin. Deep Poisson Factor Modeling. NIPS  2015.
[13] M. Zhou & L. Carin. Augment-and-conquer negative binomial processes. NIPS  pages 2546–2554  2012.
[14] M. Zhou  L. Hannah  D. Dunson  & L. Carin. Beta-negative binomial process and Poisson factor analysis.
AISTATS  2012.
[15] M. Zhou & L. Carin. Negative binomial process count and mixture modeling. IEEE Transactions on Pattern
Analysis and Machine Intelligence  37(2):307–320  2015.
[16] M. Zhou. Nonparametric Bayesian negative binomial factor analysis. arXiv:1604.07464.
[17] S. A. Hosseini  K. Alizadeh  A. Khodadadi  & et al. Recurrent Poisson Factorization for Temporal
Recommendation. KDD  2017.
[18] P. Gopalan  J. M. Hofman  & D. M. Blei. Scalable Recommendation with Hierarchical Poisson Factorization.
UAI  2015.
[19] P. Gopalan  J. M. Hofman  & D. M. Blei. Scalable Recommendation with Poisson Factorization.
arXiv:1311.1704.
[20] M. Zhou  Y. Cong  & B. Chen. Augmentable gamma belief networks. Journal of Machine Learning
Research  17(163):1–44  2016.
[21] D. P. Kingma & W. Max. Auto-encoding variational Bayes. ICLR  2014.
[22] Y. Bengio  P. Lamblin  D. Popovici  & H. Larochelle. Greedy layer-wise training of deep networks. NIPS 
2006.
[23] Y. Cong  B. Chen  H. Liu  and M. Zhou  Deep latent Dirichlet allocation with topic-layer-adaptive stochastic
gradient Riemannian MCMC. ICML  2017.
[24] S. Zhao  J. Song  S. Ermon. Learning Hierarchical Features from Generative Models. ICML  2017.
[25] M. Hoffman. Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo. ICML  2017.

9

,Chengyue Gong
win-bin huang