2014,Just-In-Time Learning for Fast and Flexible Inference,Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model  we typically only need to perform a small subset of all possible inference computations. Motivated by this  we introduce just-in-time learning  a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments  we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.,Just-In-Time Learning for Fast and Flexible Inference

S. M. Ali Eslami  Daniel Tarlow  Pushmeet Kohli and John Winn

{alie dtarlow pkohli jwinn}@microsoft.com

Microsoft Research

Abstract

Much of research in machine learning has centered around the search for inference
algorithms that are both general-purpose and efﬁcient. The problem is extremely
challenging and general inference remains computationally expensive. We seek to
address this problem by observing that in most speciﬁc applications of a model 
we typically only need to perform a small subset of all possible inference com-
putations. Motivated by this  we introduce just-in-time learning  a framework for
fast and ﬂexible inference that learns to speed up inference at run-time. Through
a series of experiments  we show how this framework can allow us to combine the
ﬂexibility of sampling with the efﬁciency of deterministic message-passing.

1

Introduction

We would like to live in a world where we can deﬁne a probabilistic model  press a button  and
get accurate inference results within a matter of seconds or minutes. Probabilistic programming
languages allow for the rapid deﬁnition of rich probabilistic models to this end  but they also raise a
crucial question: what algorithms can we use to efﬁciently perform inference for the largest possible
set of programs in the language? Much of recent research in machine learning has centered around
the search for inference algorithms that are both ﬂexible and efﬁcient.
The general inference problem is extremely challenging and remains computationally expensive.
Sampling based approaches (e.g. [5  19]) can require many evaluations of the probabilistic program
to obtain accurate inference results. Message-passing based approaches (e.g. [12]) are typically
faster  but require the program to be expressed in terms of functions for which efﬁcient message-
passing operators have been implemented. However  implementing a message-passing operator for
a new function either requires technical expertise  or is computationally expensive  or both.
In this paper we propose a solution to this problem that is automatic (it doesn’t require the user
to build message passing operators) and efﬁcient (it learns from past experience to make future
computations faster). The approach is motivated by the observation that general algorithms are
solving problems that are harder than they need to be: in most speciﬁc inference problems  we only
ever need to perform a small subset of all possible message-passing computations. For example 
in Expectation Propagation (EP) the range of input messages to a logistic factor  for which it needs
to compute output messages  is highly problem speciﬁc (see Fig. 1a). This observation raises the
central question of our work: can we automatically speed up the computations required for general
message-passing  at run-time  by learning about the statistics of the speciﬁc problems at hand?
Our proposed framework  which we call just-in-time learning (JIT learning)  initially uses highly
general algorithms for inference. It does so by computing messages in a message-passing algorithm
using Monte Carlo sampling  freeing us from having to implement hand-crafted message update
operators. However  it also gradually learns to increase the speed of these computations by regress-
ing from input to output messages (in a similar way to [7]) at run-time. JIT learning enables us
to combine the ﬂexibility of sampling (by allowing arbitrary factors) and the speed of hand-crafted
message-passing operators (by using regressors)  without having to do any pre-training. This con-
stitutes our main contribution and we describe the details of our approach in Sec. 3.

1

GP

f soil

xi

Eval

ai

+

ti

GP

f seed

Eval

ymax
i

Yield

topt
i

yavg
i

Noise

yi

(a) Problem-speciﬁc variation

(b) Random forest uncertainty

(c)

Figure 1: (a) Parameters of Gaussian messages input to a logistic factor in logistic regression vary
signiﬁcantly in four random UCI datasets.
(b) Figure for Sec. 4: A regression forest performs
1D regression (1 000 trees  2 feature samples per node  maximum depth 4  regressor polynomial
degree 2). The red shaded area indicates one standard deviation of the predictions made by the
different trees in the forest  indicating its uncertainty. (c) Figure for Sec. 6: The yield factor relates
temperatures and yields recorded at farms to the optimal temperatures of their planted grain. JIT
learning enables us to incorporate arbitrary factors with ease  whilst maintaining inference speed.

Our implementation relies heavily on the use of regressors that are aware of their own uncertainty.
Their awareness about the limits of their knowledge allows them to decide when to trust their pre-
dictions and when to fall back to computationally intensive Monte Carlo sampling (similar to [8]
and [9]). We show that random regression forests [4] form a natural and efﬁcient basis for this
class of ‘uncertainty aware’ regressors and we describe how they can be modiﬁed for this purpose in
Sec. 4. To the best of our knowledge this is the ﬁrst application of regression forests to the self-aware
learning setting and it constitutes our second contribution.
To demonstrate the efﬁcacy of the JIT framework  we employ it for inference in a variety of graphical
models. Experimental results in Sec. 6 show that for general graphical models  our approach leads
to signiﬁcant improvements in inference speed (often several orders of magnitude) over importance
sampling whilst maintaining overall accuracy  even boosting performance for models where hand
designed EP message-passing operators are available. Although we demonstrate JIT learning in the
context of expectation propagation  the underlying ideas are general and the framework can be used
for arbitrary inference problems.

2 Background

ables x = {x1  ...  xV } via non-negative factors ψ1  ...  ψF given by p(x) = (cid:81)

A wide class of probabilistic models can be represented using the framework of factor graphs. In this
context a factor graph represents the factorization of the joint distribution over a set of random vari-
f ψf (xne(ψf ))/Z 
where xne(ψf ) is the set of variables that factor ψf is deﬁned over. We will focus on directed factors
of the form ψ(xout|xin) which directly specify the conditional density over the output variables xout
as a function of the inputs xin  although our approach can be extended to factors of arbitrary form.
Belief propagation (or sum-product) is a message-passing algorithm for performing inference in fac-
tor graphs with discrete and real-valued variables  and it includes sub-routines that compute variable-
to-factor and factor-to-variable messages. The bottleneck is mainly in computing the latter kind  as
they often involve intractable integrals. The message from factor ψ to variable i is:

mψ→i(xi) =

ψ(xout|xin)

x−i

k∈ne(ψ)\i

mk→ψ(xk) 

(1)

(cid:89)

(cid:90)

(cid:104)(cid:82)

where x−i denotes all random variables in xne(ψ) except i. To further complicate matters  the
messages are often not even representable in a compact form. Expectation Propagation [11] extends
the applicability of message-passing algorithms by projecting messages back to a pre-determined 
tractable family distribution:

(cid:105)

proj

mψ→i(xi) =

x−i

k∈ne(ψ) mk→ψ(xk)

.

(2)

ψ(xout|xin)(cid:81)

mi→ψ(xi)

2

−40−30−20−10010−4−202468MeanLog precision banknote_authenticationblood_transfusionionospherefertility_diagnosis−10−50510−0.500.51 Training datapointsForest predictionsThe proj[·] operator ensures that the message is a distribution of the correct type and only has an
effect if its argument is outside the approximating family used for the target message.
The integral in the numerator of Eq. 2 can be computed using Monte Carlo methods [2  7]  e.g. by
using the generally applicable technique of importance sampling. After multiplying and dividing by
a proposal distribution q(xin) we get:
mψ→i(xi) ≡ proj

v(xin  xout) · w(xin  xout)

/mi→ψ(xi) 

(cid:34)(cid:90)

(cid:35)

(3)

where v(xin  xout) = q(xin)ψ(xout|xin) and w(xin  xout) =(cid:81)

x−i

k∈ne(ψ) mk→ψ(xk)/q(xin). Therefore

mψ→i(xi) (cid:39) proj

s w(xs

in  xs
s w(xs

out)δ(xi)
in  xs

out)

/mi→ψ(xi) 

(4)

in and xs

out are samples from v(xin  xout). To sample from v  we ﬁrst draw values xs
where xs
in from q
then pass them through the forward-sampling procedure deﬁned by ψ to get a value for xs
out.
Crucially  note that we require no knowledge of ψ other than the ability to sample from ψ(xout|xin).
This allows the model designer to incorporate arbitrary factors simply by providing an implemen-
tation of this forward sampler  which could be anything from a single line of deterministic code to
a large stochastic image renderer. However  drawing a single sample from ψ can itself be a time-
consuming operation  and the complexity of ψ and the arity of xin can both have a dramatic effect
on the number of samples required to compute messages accurately.

3 Just-in-time learning of message mappings

(cid:20)(cid:80)

(cid:80)

(cid:21)

Monte Carlo methods (as deﬁned above) are computationally expensive and can lead to slow infer-
ence. In this paper  we adopt an approach in which we learn a direct mapping  parameterized by θ 
from variable-to-factor messages {mk→ψ}k∈ne(ψ) to a factor-to-variable message mψ→i:

mψ→i(xi) ≡ f ({mk→ψ}k∈ne(ψ)|θ).

(5)
Using this direct mapping function f  factor-to-variable messages can be computed in a fraction
of the time required to perform full Monte Carlo estimation. Heess et al. [7] recently used neural
networks to learn this mapping ofﬂine for a broad range of input message combinations.
Motivated by the observation that the distribution of input messages that a factor sees is often prob-
lem speciﬁc (Fig. 1a)  we consider learning the direct mapping just-in-time in the context of a spe-
ciﬁc model. For this we employ ‘uncertainty aware’ regressors. Along with each prediction m  the
regressor produces a scalar measure u of its uncertainty about that prediction:

(6)
We adopt a framework similar to that of uncertainty sampling [8] (also [9]) and use these uncertain-
ties at run-time to choose between the regressor’s estimate and slower ‘oracle’ computations:

uψ→i ≡ u({mk→ψ}k∈ne(ψ)|θ).

mψ→i(xi) =

mψ→i(xi) uψ→i < umax
moracle

ψ→i (xi) otherwise

(7)

where umax is the maximum tolerated uncertainty for a prediction. In this paper we consider impor-
tance sampling or hand-implemented Infer.NET operators as oracles however other methods such as
MCMC-based samplers could be used. The regressor is updated after every oracle consultation in
order to incorporate the newly acquired information.
An appropriate value for umax can be found by collecting a small number of Monte Carlo mes-
sages for the target model ofﬂine: the uncertainty aware regressor is trained on some portion of the
collected messages  and evaluated on the held out portion  producing predictions mψ→i and conﬁ-
dences uψ→i for every held out message. We then set umax such that no held out prediction has an
error above a user-speciﬁed  problem-speciﬁc maximum tolerated value Dmax.
A natural choice for this error measure is mean squared error of the parameters of the messages (e.g.
natural parameters for the exponential family)  however this is sensitive to the particular parameteri-
zation chosen for the target distribution type. Instead  for each pair of predicted and oracle messages

3

(cid:40)

from factor ψ to variable i  we calculate the marginals bi and boracle
random variable  and compute the Kullback-Leibler (KL) divergence between the two:
ψ→i ) ≡ DKL(bi(cid:107)boracle

(8)
ψ→i · mi→ψ  using the fact that beliefs can be computed
where bi = mψ→i · mi→ψ and boracle
as the product of incoming and outgoing messages on any edge. We refer to the error measure Dmar
KL
as marginal KL and use it throughout the JIT framework  as it encourages the system to focus efforts
on the quantity that is ultimately of interest: the accuracy of the posterior marginals.

KL (mψ→i(cid:107)moracle
Dmar

they each induce on the target

= moracle

) 

i

i

i

4 Random decision forests for JIT learning
We wish to learn a mapping from a set of incoming messages {mk→ψ}k∈ne(ψ) to the outgoing
message mψ→i. Note that separate regressors are trained for each outgoing message. We require
that the regressor: 1) trains and predicts efﬁciently  2) can model arbitrarily complex mappings 
3) can adapt dynamically  and 4) produces uncertainty estimates. Here we describe how decision
forests can be modiﬁed to satisfy these requirements. For a review of decision forests see [4].
In EP  each incoming and outgoing message can be represented using only a few numbers  e.g. a
Gaussian message can be represented by its natural parameters. We refer to the outgoing message by
mout and to the set of incoming messages by min. Each set of incoming messages min is represented
in two ways: the ﬁrst  a concatenation of the parameters of its constituent messages which we call the
‘regression parameterization’ and denote by rin; and the second  a vector of features computed on the
set which we call the ‘tree parameterization’ and denote by tin. This tree parametrization typically
contains values for a larger number of properties of each constituent message (e.g. parameters and
moments)  and also properties of the set as a whole (e.g. ψ evaluated at the mode of min). We
represent the outgoing message mout by a vector of real valued numbers rout. Note that din and dout 
the number of elements in rin and rout respectively  need not be equal.
Weak learner model. Data arriving at a split node j is separated into the node’s two children
according to a binary weak learner h(tin  τ j) ∈ {0  1}  where τ j parameterizes the split criterion.
We use weak learners of the generic oriented hyperplane type throughout (see [4] for details).
Prediction model. Each leaf node is associated with a subset of the labelled training data. During
testing  a previously unseen set of incoming messages traverses the tree until it reaches a leaf which
by construction is likely to contain similar training examples. We therefore use the statistics of the
data gathered in that leaf to predict outgoing messages with a multivariate polynomial regression
) +   where φn(·) is the n-th degree polynomial basis
model of the form: rtrain
function  and  is the dout-dimensional vector of normal error terms. We use the learned dout × din-
dimensional matrix of coefﬁcients W at test time to make predictions rout for each rin. To recap  tin
is used to traverse message sets down to leaves  and rin is used by the linear regressor to predict rout.
Training objective function. The optimization of the split functions proceeds in a greedy man-
ner. At each node j  depending on the subset of the incoming training set Sj we learn the
function that ‘best’ splits Sj into the training sets corresponding to each child  SL
j   i.e.
τ j = argmaxτ∈Tj I(Sj  τ ). This optimization is performed as a search over a discrete set Tj of a
random sample of possible parameter settings. The number of elements in Tj is typically kept small 
introducing random variation in the different trees in the forest. The objective function I is:

out = W · φn(rtrain

j and SR

in

I(Sj  τ ) = −E(SL

j   WL) − E(SR

(9)
where WL and WR are the parameters of the polynomial regression models corresponding to the
left and right training sets SL
E(S  W) =

j   and the ‘ﬁt residual’ E is:

(cid:88)
j and SR

Dmar

(10)

min ) + Dmar

KL (moracle

min (cid:107)mW
min ).

min(cid:107)moracle

KL (mW

j   WR) 

Here min is a set of incoming messages in S  moracle
min is the
estimate produced by the regression model speciﬁed by W and Dmar
KL is the marginal KL. In simple
terms  this objective function splits the training data at each node in a way that the relationship
between the incoming and outgoing messages is well captured by the polynomial regression in each
child  as measured by symmetrized marginal KL.

is the oracle outgoing message  mW

min

1
2

min∈S

4

out}  m) where U ({mt

t DKL(mt

out(cid:107)m).

out}  m) =(cid:80)

out of the predicted outgoing messages mt

Instead  we compute the moment average mout of the distributions {mt

Ensemble model. A key aspect of forests is that their trees are randomly different from each other.
This is due to the relatively small number of weak learner candidates considered in the optimization
of the weak learners. During testing  each test point min simultaneously traverses all trees from
their roots until it reaches their leaves. Combining the predictions into a single forest prediction
may be done by averaging the parameters rt
out by each
tree t  however again this would be sensitive to the parameterizations of the output distribution
out} by averaging
types.
the ﬁrst few moments of each predicted distribution across trees  and solving for the distribution
parameters which match the averaged moments. Grosse et al. [6] study the characteristics of the
moment average in detail  and have showed that it can be interpreted as minimizing an objective
function mout = argminm U ({mt
Intuitively  the level of agreement between the predictions of the different trees can be used as a
proxy of the forest’s uncertainty about that prediction (we choose not to use uncertainty within
leaves in order to maintain high prediction speed). If all the trees in the forest predict the same output
distribution  it means that their knowledge about the function f is similar despite the randomness in
their structures. We therefore set uout ≡ U ({mt
out}  mout). A similar notion is used for classiﬁcation
forests  where the entropy of the aggregate output histogram is used as a proxy of the classiﬁcation’s
uncertainty [4]. We illustrate how this idea extends to simple regression forests in Fig. 1b  and in
Sec. 6 we also show empirically that this uncertainty measure works well in practice.
Online training. During learning  the trees periodically obtain new information in the form of
) pairs. The forest makes use of this by pushing min down a portion 0 < ρ ≤ 1 of the
(min  moracle
trees to their leaf nodes and retraining the regressors at those leaves. Typically ρ = 1  however we
use values smaller than 1 when the trees are shallow (due to the mapping function being captured
well by the regressors at the leaves) and the forest’s randomness is too low to produce reliable
uncertainty estimates. If the regressor’s ﬁt residual E at a leaf (Eq. 10) is above a user-speciﬁed
threshold value Emax

leaf   a split is triggered on that node. Note that no depth limit is ever speciﬁed.

out

5 Related work

There are a number of works in the literature that consider using regressors to speed up general
purpose inference algorithms. For example  the Inverse MCMC algorithm [20] uses discriminative
estimates of local conditional distributions to make proposals for a Metropolis-Hastings sampler 
however these predictors are not aware of their own uncertainty. Therefore the decision of when the
sampler can start to rely on them needs to be made manually and the user has to explicitly separate
ofﬂine training and test-time inference computations.
A related line of work is that of inference machines [14  15  17  13]. Here  message-passing is
performed by a sequence of predictions  where the sequence itself is deﬁned by the graphical model.
The predictors are jointly trained to ensure that the system produces correct labellings  however the
resulting inference procedure no longer corresponds to the original (or perhaps to any) graphical
model and therefore the method is unsuitable if we care about querying the model’s latent variables.
The closest work to ours is [7]  in which Heess et al. use neural networks to learn to pass EP
messages. However  their method requires the user to anticipate the set of messages that will ever be
sent by the factor ahead of time (itself a highly non-trivial task)  and it has no notion of conﬁdence in
its predictions and therefore it will silently fail when it sees unfamiliar input messages. In contrast
the JIT learner trains in the context of a speciﬁc model thereby allocating resources more efﬁciently 
and because it knows what it knows  it buys generality without having to do extensive pre-training.

6 Experiments

We ﬁrst analyze the behaviour of JIT learning with diagnostic experiments on two factors: logistic
and compound gamma  which were also considered by [7]. We then demonstrate its application to
a challenging model of US corn yield data. The experiments were performed using the extensible
factor API in Infer.NET [12]. Unless stated otherwise  we use default Infer.NET settings (e.g. for
message schedules and other factor implementations). We set the number of trees in each forest to
64 and use quadratic regressors. Message parameterizations and graphical models  experiments on
a product factor and a quantitative comparison with [7] can be found in the supplementary material.

5

(a) Inference error

(b) Worst predicted messages

(c) Awareness of uncertainty

Figure 2: Uncertainty aware regression. All plots for the Gaussian forest.
(a) Histogram of
marginal KLs of outgoing messages  which are typically very small. (b) The forest’s most inaccurate
predictions (black: moracle  red: m  dashed black: boracle  purple: b). (c) The regressor’s uncertainty
increases in tandem with marginal KL  i.e. it does not make conﬁdent but inaccurate predictions.

(a) Oracle consultation rate

(b) Inference time

(c) Inference error

Figure 3: Logistic JIT learning. (a) The factor consults the oracle for only a fraction of messages 
(b) leading to signiﬁcant savings in time  (c) whilst maintaining (or even decreasing) inference error.

Logistic. We have access to a hand-crafted EP implementation of this factor  allowing us to perform
quantitative analysis of the JIT framework’s performance. The logistic deterministically computes
xout = σ(xin) = 1/(1+exp{−xin}). Sensible choices for the incoming and outgoing message types
are Gaussian and Beta respectively. We study the logistic factor in the context of Bayesian logistic
regression models  where the relationship between an input vector x and a binary output observation
y is modeled as p(y = 1) = σ(wT x). We place zero-mean  unit-variance Gaussian priors on the
entries of regression parameters w  and run EP inference for 10 iterations.
We ﬁrst demonstrate that the forests described in Sec. 4 are fast and accurate uncertainty aware
regressors by applying them to ﬁve synthetic logistic regression ‘problems’ as follows: for each
problem  we sample a groundtruth w and training xs from N (0  1) and then sample their corre-
sponding ys. We use a Bayesian logistic regression model to infer ws using the training datasets
and make predictions on the test datasets  whilst recording the messages that the factor receives and
sends during both kinds of inference. We split the observed message sets into training (70%) and
hold out (30%)  and train and evaluate the random forests using the two datasets. In Fig. 2 we show
that the regressor is accurate and that it is uncertain whenever it makes predictions with higher error.
One useful diagnostic for choosing the various parameters of the forests (including choice of
parametrization for rin and tin  as well leaf tolerance Emax
leaf ) is the average utilization of its leaves
during held out prediction  i.e. what fraction of leaves are visited at test time. In this experiment the
forests obtain an average utilization of 1  meaning that every leaf contributes to the predictions of the
30% held out data  thereby indicating that the forests have learned a highly compact representation
of the underlying function. As described in Sec. 3  we also use the data gathered in this experiment
to ﬁnd an appropriate value of umax for use in just-in-time learning.
Next we evaluate the uncertainty aware regressor in the context of JIT learning. We present several
related regression problems to a JIT logistic factor  i.e. we keep w ﬁxed and generate multiple new
{(x  y)} sets. This is a natural setting since often in practice we observe multiple datasets which
we believe to have been generated by the same underlying process. For each problem  using the JIT
factor we infer the regression weights and make predictions on test inputs  comparing wall-clock
time and accuracy with non-JIT implementations of the factor. We consider two kinds of oracles:

6

−20−18−16−14−12−10−80510152025Log marginal KLCount−1001000.20.40.6Hold out worst 1Groundtruth − µ: −3.4  σ2: 6.8Predicted − µ: −3.3  σ2: 6.5Log marginal KL: −8.2Log uncertainty: −7.8−1001000.20.40.6Hold out worst 2Groundtruth − µ: −3.4  σ2: 6.8Predicted − µ: −3.3  σ2: 6.6Log marginal KL: −8.6Log uncertainty: −8.2−25−20−15−10−5−18−16−14−12−10−8−6Log marginal KLLog uncertainty TrainHold out5010015020025030035040045050000.050.10.150.20.25Problems seenOracle consultation rate Infer.NET + KNNInfer.NET + JITSampling + KNNSampling + JIT501001502002503003504004505006789101112Problems seenLog time (ms) Infer.NETInfer.NET + KNNInfer.NET + JITSamplingSampling + KNNSampling + JIT50100150200250300350400450500−18−16−14−12−10Problems seenLog KL of inferred weight posterior Infer.NET + KNNInfer.NET + JITSamplingSampling + KNNSampling + JITthose that consult Infer.NET’s message operators and those that use importance sampling (Eq. 4).
As a baseline  we also implemented a K-nearest neighbour (KNN) uncertainty aware regressor.
Here  messages are represented using their natural parameters  the uncertainty associated with each
prediction is the mean distance from the K-closest points in this space  and the outgoing message’s
parameters are found by taking the average of the parameters of the K-closest output messages. We
use the same procedure as the one described in Sec. 3 to choose umax for KNN.
We observe that the JIT factor does indeed learn about the inference problem over time. Fig. 3a
shows that the rate at which the factor consults the oracle decreases over the course of the experi-
ment  reaching zero at times (i.e. for these problems the factor relies entirely on its predictions). On
average  the factor sends 97.7% of its messages without consulting the sampling oracle (a higher rate
of 99.2% when using Infer.NET as the oracle  due to lack of sampling noise)  which leads to several
orders of magnitude savings in inference time (from around 8 minutes for sampling to around 800
ms for sampling + JIT)  even increasing the speed of our Infer.NET implementation (from around
1300 ms to around 800 ms on average  Fig. 3b). Note that the forests are not merely memorising a
mapping from input to output messages  as evidenced by the difference in the consultation rates of
JIT and KNN  and that KNN speed deteriorates as the database grows. Surprisingly  we observe that
the JIT regressors in fact decrease the KL between the results produced by importance sampling and
Infer.NET  thereby increasing overall inference accuracy (Fig. 3c  this could be due to the fact that
the regressors at the leaves of the forests smooth out the noise of the sampled messages). Reducing
the number of importance samples to reach speed parity with JIT drastically degrades the accuracy
of the outgoing messages  increasing overall log KL error from around −11 to around −4.
Compound gamma. The second factor we investigate is the compound gamma factor. The com-
pound gamma construction is used as a heavy-tailed prior over precisions of Gaussian random vari-
ables  where ﬁrst r2 is drawn from a gamma with rate r1 and shape s1 and the precision of the
Gaussian is set to be a draw from a gamma with rate r2 and shape s2. Here  we have access to
closed-form implementations of the two gamma factors in the construction  however we use the JIT
framework to collapse the two into a single factor for increased speed.
We study the compound gamma factor in the context of Gaussian ﬁtting  where we sample a ran-
dom number of points from multiple Gaussians with a wide range of precisions  and then infer the
precision of the generating Gaussians via Bayesian inference using a compound gamma prior. The
number of samples varies between 10 and 100 and the precision varies between 10−4 and 104 in
each problem. The compound factor learns the message mapping after around 20 problems (see
Fig. 4a). Note that only a single message is sent by the factor in each episode  hence the abrupt drop
in inference time. This increase in performance comes at negligible loss of accuracy (Figs. 4b  4c).
Yield. We also consider a more realistic application to scientiﬁc modelling. This is an example
of a scenario for which our framework is particularly suited: scientists often need to build large
models with factors that directly take knowledge about certain components of the problem into
account. We use JIT learning to implement a factor that relates agriculture yields to temperature in
the context of an ecological climate model. Ecologists have strong empirical beliefs about the form
of the relationship between temperature and yield (that yield increases gradually up to some optimal
temperature but drops sharply after that point; see Fig 5a and [16  10]) and it is imperative that this
relationship is modelled faithfully. Deriving closed form message-operators is a non-trivial task  and
therefore current state-of-the-art is sampling-based (e.g. [3]) and highly computationally intensive.

(a) Inference time

(b) Inference error

(c) Accuracy (1 dot per problem)

Figure 4: Compound gamma JIT learning. (a) JIT reduces inference time for sampling from ∼11
seconds to ∼1 ms. (b) JIT s posteriors agree highly with Infer.NET. Using fewer samples to match
JIT speed leads to degradation of accuracy. (c) Increased speed comes at negligible loss of accuracy.

7

10203040506070809010002468Problems seenLog time (ms) Infer.NETInfer.NET + KNNInfer.NET + JITSamplingSampling + KNNSampling + JIT00.10.20.30.40.50.60.70.800.20.40.60.81Distance d of inferred log precision from groundtruthRatio of inferred precisions with error < d Infer.NETInfer.NET + JITSamplingSampling (matching JIT speed)Sampling + JIT−10−50510−10−50510Sampling inferred log precisionSampling + JIT inferred log precision(cid:122)

(cid:125)(cid:124)

2011

(cid:123)

(cid:122)

(cid:125)(cid:124)

2012

(cid:123)

(cid:122)

(cid:125)(cid:124)

2013

(cid:123)

(a) The yield factor

(b) Oracle consultation rate

(c) Accuracy (1 dot per county)

Figure 5: A probabilistic model of corn yield. (a) Ecologists believe that yield increases gradually
up to some optimal temperature but drops sharply after that point [16  10]  and they wish to incor-
porate this knowledge into their models faithfully. (b) Average consultation rate per 1 000 messages
over the course of inference on the three datasets. Notice decrease within and across datasets. (c)
Signiﬁcant savings in inference time (Table 1) come at a small cost in inference accuracy.

We obtain yield data for 10% of US counties for 2011–2013 from the USDA National Agricultural
Statistics Service [1] and corresponding temperature data using [18]. We ﬁrst demonstrate that it
is possible to perform inference in a large-scale ecological model of this kind with EP (graphical
model shown in Fig. 1c; derived in collaboration with computational ecologists; see supplementary
material for a description)  using importance sampling to compute messages for the yield factor
for which we lack message-passing operators. In addition to the difﬁculty of computing messages
for the multidimensional yield factor  inference in the model is challenging as it includes multiple
Gaussian processes  separate topt and ymax variables for each location  many copies of the yield
factor  and its graph is loopy. Results of inference are shown in the supplementary material.
We ﬁnd that with around 100 000 samples the message for the yield factor can be computed ac-
curately  making these by far the slowest computations in the inference procedure. We apply JIT
learning by regressing these messages instead. The high arity of the factor makes the task particu-
larly challenging as it increases the complexity of the mapping function being learned. Despite this 
we ﬁnd that when performing inference on the 2011 data the factor can learn to accurately send up
to 54% of messages without having to consult the oracle  resulting in a speedup of 195%.
A common scenario is one in which we collect more data and
wish to repeat inference. We use the forests learned at the
end of inference on 2011 data to perform inference on 2012
data  and the forests learned at the end of this to do inference
on 2013 data  and compare to JIT learning from scratch for
each dataset. The factor transfers its knowledge across the
problems  increasing inference speedup from 195% to 289%
and 317% in the latter two experiments respectively (Table 1) 
whilst maintaining overall inference accuracy (Fig. 5c).
7 Discussion
The success of JIT learning depends heavily on the accuracy of the regressor and its knowledge
about its uncertainty. Random forests have shown to be adequate however alternatives may exist 
and a more sophisticated estimate of uncertainty (e.g. using Gaussian processes) is likely to lead to
an increased rate of learning. A second critical ingredient is an appropriate choice of umax  which
currently requires a certain amount of manual tuning.
In this paper we showed that it is possible to speed up inference by combining EP  importance
sampling and JIT learning  however it will be of interest to study other inference settings where JIT
ideas might be applicable. Surprisingly  our experiments also showed that JIT learning can increase
the accuracy of sampling or accelerate hand-coded message operators  suggesting that it will be
fruitful to use JIT to remove bottlenecks even in existing  optimized inference code.

11 451s 54% 195% — —
12 449s 54% 192% 60% 288%
13 451s 54% 191% 64% 318%

Table 1: FR is fraction of regres-
sions with no oracle consultation.

JIT fresh

IS
JIT continued
Time FR Speedup FR Speedup

Acknowledgments

Thanks to Tom Minka and Alex Spengler for valuable discussions  and to Silvia Caldararu and Drew
Purves for introducing us to the corn yield datasets and models.

8

0510tOpt2025303540050100150200yMaxYield (bushels/acre)Temperature (celcius)02000400060008000100000.20.30.40.50.60.70.80.9Message numberOracle consultation rate−60−40−2002040−60−40−2002040Sampling inferred county ability (ai)Sampling + JIT inferred county ability (ai)References
[1] National Agricultural Statistics Service  2013. United States Department of Agriculture.

http://quickstats.nass.usda.gov/.

[2] Simon Barthelm´e and Nicolas Chopin. ABC-EP: Expectation Propagation for Likelihood-
free Bayesian Computation. In Proceedings of the 28th International Conference on Machine
Learning  pages 289–296  2011.

[3] Silvia Caldararu  Vassily Lyutsarev  Christopher McEwan  and Drew Purves.

Filzbach 
2013. Microsoft Research Cambridge. Website URL: http://research.microsoft.com/en-
us/projects/ﬁlzbach/.

[4] Antonio Criminisi and Jamie Shotton. Decision Forests for Computer Vision and Medical

Image Analysis. Springer Publishing Company  Incorporated  2013.

[5] Noah D. Goodman  Vikash K. Mansinghka  Daniel Roy  Keith Bonawitz  and Joshua B. Tenen-
baum. Church: a language for generative models. In Uncertainty in Artiﬁcial Intelligence 
2008.

[6] Roger B Grosse  Chris J Maddison  and Ruslan Salakhutdinov. Annealing between distribu-
tions by averaging moments. In Advances in Neural Information Processing Systems 26  pages
2769–2777. 2013.

[7] Nicolas Heess  Daniel Tarlow  and John Winn. Learning to Pass Expectation Propagation
Messages. In Advances in Neural Information Processing Systems 26  pages 3219–3227. 2013.
[8] David D. Lewis and William A. Gale. A Sequential Algorithm for Training Text Classiﬁers.

In Special Interest Group on Information Retrieval  pages 3–12. Springer London  1994.

[9] Lihong Li  Michael L. Littman  and Thomas J. Walsh. Knows what it knows: a framework for
self-aware learning. In Proceedings of the 25th International Conference on Machine learning 
pages 568–575  New York  NY  USA  2008. ACM.

[10] David B. Lobell  Marianne Banziger  Cosmos Magorokosho  and Bindiganavile Vivek. Non-
linear heat effects on African maize as evidenced by historical yield trials. Nature Climate
Change  1:42–45  2011.

[11] Thomas Minka. Expectation Propagation for approximate Bayesian inference. PhD thesis 

Massachusetts Institute of Technology  2001.

[12] Thomas Minka  John Winn  John Guiver  and David Knowles. Infer.NET 2.5  2012. Microsoft

Research Cambridge. Website URL: http://research.microsoft.com/infernet.

[13] Daniel Munoz. Inference Machines: Parsing Scenes via Iterated Predictions. PhD thesis  The

Robotics Institute  Carnegie Mellon University  June 2013.

[14] Daniel Munoz  J. Andrew Bagnell  and Martial Hebert. Stacked Hierarchical Labeling.

European Conference on Computer Vision  2010.

In

[15] Stephane Ross  Daniel Munoz  Martial Hebert  and J. Andrew Bagnell. Learning Message-
Passing Inference Machines for Structured Prediction. In Conference on Computer Vision and
Pattern Recognition  2011.

[16] Wolfram Schlenker and Michael J. Roberts. Nonlinear temperature effects indicate severe
damages to U.S. crop yields under climate change. Proceedings of the National Academy of
Sciences  106(37):15594–15598  2009.

[17] Roman Shapovalov  Dmitry Vetrov  and Pushmeet Kohli. Spatial Inference Machines.

Conference on Computer Vision and Pattern Recognition  pages 2985–2992  2013.

In

[18] Matthew J. Smith  Paul I. Palmer  Drew W. Purves  Mark C. Vanderwel  Vassily Lyutsarev 
Ben Calderhead  Lucas N. Joppa  Christopher M. Bishop  and Stephen Emmott. Changing
how Earth System Modelling is done to provide more useful information for decision making 
science and society. Bulletin of the American Meteorological Society  2014.

[19] Stan Development Team. Stan: A C++ Library for Probability and Sampling  2014.
[20] Andreas Stuhlm¨uller  Jessica Taylor  and Noah D. Goodman. Learning Stochastic Inverses. In

Advances in Neural Information Processing Systems 27  2013.

9

,S. M. Ali Eslami
Daniel Tarlow
Pushmeet Kohli
John Winn
Yuanjun Gao
Evan Archer
Liam Paninski
John Cunningham