2013,Pass-efficient unsupervised feature selection,The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm  a modification of the classical pivoted QR algorithm of Businger and Golub  that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass  and skipping calculations that can be shown not to affect the final selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm  and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of {\em several orders of magnitude} over the classical algorithm. These results appear to be competitive with  recently proposed randomized algorithms in terms of pass efficiency and run time. On the other hand  the randomized algorithms may produce better features  at the cost of small probability of failure.,Pass-Efﬁcient Unsupervised Feature Selection

Crystal Maung

Department of Computer Science
The University of Texas at Dallas
Crystal.Maung@gmail.com

Haim Schweitzer

Department of Computer Science
The University of Texas at Dallas
HSchweitzer@utdallas.edu

Abstract

The goal of unsupervised feature selection is to identify a small number of impor-
tant features that can represent the data. We propose a new algorithm  a modiﬁca-
tion of the classical pivoted QR algorithm of Businger and Golub  that requires a
small number of passes over the data. The improvements are based on two ideas:
keeping track of multiple features in each pass  and skipping calculations that can
be shown not to affect the ﬁnal selection. Our algorithm selects the exact same
features as the classical pivoted QR algorithm  and has the same favorable numer-
ical stability. We describe experiments on real-world datasets which sometimes
show improvements of several orders of magnitude over the classical algorithm.
These results appear to be competitive with recently proposed randomized algo-
rithms in terms of pass efﬁciency and run time. On the other hand  the randomized
algorithms may produce more accurate features  at the cost of small probability of
failure.

1

Introduction

Work on unsupervised feature selection has received considerable attention. See  e.g.  [1  2  3  4 
In numerical linear algebra unsupervised feature selection is known as the column
5  6  7  8] .
subset selection problem  where one attempts to identify a small subset of matrix columns that can
approximate the entire column space of the matrix. See  e.g.  [9  Chapter 12]. The distinction
between supervised and unsupervised feature selection is as follows. In the supervised case one
is given labeled objects as training data and features are selected to help predict that label; in the
unsupervised case nothing is known about the labels.
We describe an improvement to the classical Businger and Golub pivoted QR algorithm [9  10]. We
refer to the original algorithm as the QRP  and to our improved algorithm as the IQRP. The QRP
selects features one by one  using k passes in order to select k features. In each pass the selected
feature is the one that is the hardest to approximate by the previously selected features. We achieve
improvements to the algorithm run time and pass efﬁciency without affecting the selection and the
excellent numerical stability of the original algorithm. Our algorithm is deterministic  and runs in a
small number of passes over the data. It is based on the following two ideas:

1. In each pass we identify multiple features that are hard to approximate with the previously
selected features. A second selection step among these features uses an upper bound on
unselected features that enables identifying multiple features that are guaranteed to have
been selected by the QRP. See Section 4 for details.

2. Since the error of approximating a feature can only decrease when additional features are
added to the selection  there is no need to evaluate candidates with error that is already “too
small”. This allows a signiﬁcant reduction in the number of candidate features that need to
be considered in each pass. See Section 4 for details.

1

2 Algorithms for unsupervised feature selection

The algorithms that we consider take as input large matrices of numeric values. We denote by m
the number of rows  by n the number of columns (features)  and by k the number of features to be
selected. Criteria for evaluating algorithms include their run time and memory requirements  the
number of passes over the data  and the algorithm accuracy. The accuracy is a measure of the error
of approximating the entire data matrix as a linear combination of the selection. We review some
classical and recent algorithms for unsupervised feature selection.

2.1 Related work in numerical linear algebra

Businger and Golub QRP was established by Businger and Golub [9  10]. We discuss it in detail
in Section 3. It requires k passes for selecting k features  and its run time is 4kmn − 2k2(m + n) +
4k3/3. A recent study [11] compares experimentally the accuracy of the QRP as a feature selection
algorithm to some recently proposed state-of-the-art algorithms. Even though the accuracy of the
QRP is somewhat below the other algorithms  the results are quite similar. (The only exception was
the performance on the Kahan matrix  where the QRP was much less accurate.)

Gu and Eisenstat algorithm [1] was considered the most accurate prior to the work on randomized
algorithms that had started with [12]. It computes an initial selection (typically by using the QRP) 
and then repeatedly swaps selected columns with unselected column. The swapping is done so that
the product of singular values of the matrix formed by the selected columns is increased with each
swapping. The algorithm requires random access memory  and it is not clear how to implement it
by a series of passes over the data. Its run time is O(m2n).

2.2 Randomized algorithms

Randomized algorithms come with a small probability of failure  but otherwise appear to be more
accurate than the classical deterministic algorithms. Frieze et al [12  13] have proposed a randomized
algorithm that requires only two passes over the data. This assumes that the norms of all matrix
columns are known in advance  and guarantees only an additive approximation error. We discuss
the run time and the accuracy of several generalizations that followed their studies.

Volume sampling Deshpande et al [14] have studied a randomized algorithm that samples k-tuples
of columns with probability proportional to their “volume”. The volume is the square of the product
of the singular values of the submatrix formed by these columns. They show that this sampling
scheme gives rise to a randomized algorithm that computes the best possible solution in the Frobe-
nius norm. They describe an efﬁcient O(kmn) randomized algorithm that can be implemented in k
passes and approximates this sampling scheme. These results were improved (in terms of accuracy)
in [15]  by computing the exact volume sampling. The resulting algorithm is slower but much more
accurate. Further improvements to the speed of volume sampling in [6] have reduced the run time
complexity to O(km2n). As shown in [15  6]  this optimal (in terms of accuracy) algorithm can also
be derandomized  with a deterministic run time of O(km3n).

Leverage sampling The idea behind leverage sampling is to randomly select features with prob-
ability proportional to their “leverage”. Leverage values are norms of the rows of the n × k right
eigenvector matrix in the truncated SVD expansion of the data matrix. See [16  2]. In particular 
the “two stage” algorithm described in [2] requires only 2 passes if the leverage values are known.
Its run time is dominated by the calculation of the leverage values. To the best of our knowledge
the currently best algorithms for estimating leverage values are randomized [17  18]. One run takes
2 passes and O(mn log n + m3) time. This is dominated by the mn term  and [18] show that it
can be further reduced to the number of nonzero values. We note that these algorithms do not com-
pute reliable leverage in 2 passes  since they may fail at a relatively high (e.g.  1/3) probability. As
stated in [18] “the success probability can be ampliﬁed by independent repetition and taking the
coordinate-wise median”. Therefore  accurate estimates of leverage can be computed in constant
number of passes. But the constant would be larger than 2.

2

Input: The features (matrix columns) x1  . . .   xn  and an integer k ≤ n.
Output: An ordered list S of k indices.
1.

In the initial pass compute:
1.1. For i = 1  . . .   n set ˜xi = xi  vi = |˜xi|2.
At the end of the pass set z1 = arg max
For each pass j = 2  . . .   k:
2.1. For i = 1  . . .   n set vi to the square error of

approximating xi by a linear combination of the columns in S.)
vi  and initialize S = (z1).

(˜xi is the error vector of

2.

i

approximating xi by a linear combination of the columns in S.

At the end of pass j set zj = arg max

i

vi  and add zj to S.

Figure 1: The main steps of the QRP algorithm.

2.3 Randomized ID

In a recent survey [19] Halko et.al. describe how to compute QR factorization using their random-
ized Interpolative Decomposition. Their approach produces an accurate Q as a basis of the data
matrix column space. They propose an efﬁcient “row extraction” method for computing R  that
works when k  the desired rank  is similar to the rank of the data matrix. Otherwise the row extrac-
tion introduces unacceptable inaccuracies  which led Halko et.al to recommend using an alternative
O(kmn) technique in such cases.

2.4 Our result  the complexity of the IQRP

The savings that the IQRP achieves depend on the data. The algorithm takes as input an integer value
l  the length of a temporary buffer. As explained in Section 4 our implementation requires temporary
storage of l + 1  which takes (l + 1)m ﬂoats. The following values depend on the data: the number
of passes p  the number of IO-passes q (explained below)  and a unit cost of orthogonalization c (see
Section 4.3).
In terms of l and c the run time is 2mn + 4mnc + 4mlk. Our experiments show that for typical
datasets the value of c is below k. For l ≈ k our experiments show that the number of passes is
typically much smaller than k. The number of passes is even smaller if one considers IO-passes. To
explain what we mean by IO-passes consider as an example a situation where the algorithm runs
three passes over the data. In the ﬁrst pass all n features are being accessed. In the second  only two
features are being accessed. In the third  only one feature is being accessed. In this case we take
n. We believe that q is a relevant measure of the algorithm pass
the number of IO-passes to be q=1+ 3
complexity when skipping is cheap  so that the cost of a pass over the data is the amount of data that
needs to be read.

3 The Businger Golub algorithm (QRP)

In this section we describe the QRP [9  10] which forms the basis to the IQRP. The main steps
are described in Figure 1. There are two standard implementations for Step 2.1 in Figure 1. The
ﬁrst is by means of the “Modiﬁed Gram-Schmidt” (e.g.  [9])  and the second is by Householder
orthogonalization (e.g.  [9]). Both methods require approximately the same number of ﬂops  but
error analysis (see [9]) shows that the Householder approach is signiﬁcantly more stable.

3.1 Memory-efﬁcient implementations

The implementations shown in Figure 2 update the memory where the matrix A is stored. Speciﬁ-
cally  A is overwritten by the R component of the QR factorization. Since we are not interested in
R  overwriting A may not be acceptable. The procedure shown in Figure 3 does not overwrite A 
but it is more costly. The ﬂops count is dominated by Steps 1 and 2  which cost at most 4(j − 1)mn
at pass j. Summing up for j = 1  . . .   k this gives a total ﬂops count of approximately 2k2mn ﬂops.

3

Compute zj  qj  Qj
for i = 1  . . .   n
1. wi = qT
j−1 ˜xi.
2. ˜xi ← ˜xi − wiqj−1.
3. vi ← vi − w2
i .
At the end of the pass:
4. zj = arg max
vi.
5. qj = xzj /|xzj|.
6. Qj = (Qj−1  qj).

i

Compute zj  hj  Hj
for i = 1  . . .   n
1. ˜xi ← hj−1 ˜xi.
2. wi = ˜xi(j) (the j’th coordinate of ˜xi).
3. vi ← vi − w2
i .
At the end of the pass:
4. zj = arg max
5. Create the Householder matrix hj from ˜xj.
6. Hj = Hj−1hj.

vi.

i

Modiﬁed Gram-Schmidt

Householder orthogonalization

Figure 2: Standard implementations of Step 2.1 of the QRP

Compute zj  qj  Qj
for i = 1  . . .   n
1. wi = QT
j−1xi.
2. vi = |xi|2 − |wi|2.
At the end of the pass:
3. zj = arg max
4. ˜qj = xzj − Qj−1wzj   qj = ˜qj/|˜qj|.
5. Qj = (Qj−1  qj).

vi.

i

Compute zj  hj  Hj
for i = 1  . . .   n
1. yi = Hj−1xi.

2. vi =(cid:80)m

t=j+1 y2

i (t).
At the end of the pass:
3. zj = arg max
4. Create hj from yzj .
5. Hj = Hj−1hj.

vi.

i

Modiﬁed Gram-Schmidt

Householder orthogonalization

Figure 3: Memory-efﬁcient implementations of Step 2.1 of the QRP

4 The IQRP algorithm

In this section we describe our main result: the improved QRP. The algorithm maintains three or-
dered lists of columns: The list F is the input list containing all columns. The list S contains
columns that have already been selected. The list L is of size l  where l is a user deﬁned parameter.
For each column xi in F the algorithm maintains an integer value ri and a real value vi. These
values can be kept in core or a secondary memory. They are deﬁned as follows:

ri ≤ |S| 

vi = vi(ri) = (cid:107)xi − QriQT

ri

xi(cid:107)2

(1)

where Qri = (q1  . . .   qri) is an orthonormal basis to the ﬁrst ri columns in S. Thus  vi(ri) is
the (squared) error of approximating xi with the ﬁrst ri columns in S. In each pass the algorithm
identiﬁes the l candidate columns xi corresponding to the l largest values of vi(|S|). That is  the vi
values are computed as the error of predicting each candidate by all columns currently in S. The
identiﬁed l columns with the largest vi(|S|) are stored in the list L. In addition  the value of the
l+1’th largest vi(|S|) is kept as the constant BF . Thus  after a pass is terminated the following
condition holds:

vα(rα) ≤ BF

for all xα ∈ F \ L.

(2)

The list L and the value BF can be calculated in one pass using a binary heap data structure  with
the cost of at most n log(l + 1) comparisons. See [20  Chapter 9]. The main steps of the algorithm
are described in Figure 4.

Details of Steps 2.0  2.1 of the IQRP. The threshold value T is deﬁned by:

(cid:26)−∞

T =

top of the heap

4

if the heap is not full.
if the heap is full.

(3)

Input: The matrix columns (features) x1  . . .   xn  and an integer k ≤ n.
Output: An ordered list S of k indices.
1.

(The initial pass over F .)
1.0. Create a min-heap of size l+1.
1.1. Set vi(0) = |xi|2  ri = 0.

In one pass go over xi  i = 1  . . .   n:

Fill the heap with the candidates corresponding to the l+1 largest vi(0).

1.2. At the end of the pass:

Set BF to the value at the top of the heap.
Set L to heap content excluding the top element.
Add to S as many candidates from L as possible using BF .

2.

Repeat until S has k candidates:
2.0. Create a min-heap of size l+1.

Let T be deﬁned by (3).
In one pass go over xi  i = 1  . . .   n:

2.1. Skip xi if vi(ri) ≤ T . Otherwise update vi  ri  heap.
2.2. At the end of the pass:

Set BF = T .
Set L to heap content excluding the top element.
Add to S as many candidates from L as possible using BF .

Figure 4: The main steps of the IQRP algorithm.

Thus  when the heap is full  T is the value of v associated with the l+1’th largest candidate encoun-
tered so far. The details of Step 2.1 are shown in Figure 5. Step A.2.2.1 can be computed using
either Gram-Schmidt or Householder  as shown in Figures 3 and 4.

A.1. If vi(ri) ≤ T skip xi.
A.2. Otherwise check ri:
A.2.1. If ri = |S| conditionally insert xi into the heap.
A.2.2. If ri < |S|:

A.2.2.1. Compute vi(|S|). Set ri = |S|.
A.2.2.2. Conditionally insert xi into the heap.

Figure 5: Details of Step 2.1

Details of Steps 1.2 and 2.2 of the IQRP. Here we are given the list L and the value of BF
satisfying (2). To move candidates from L to S run the QRP on L as long as the pivot value is above
BF . (The pivot value is the largest value of vi(|S|) in L.) The details are shown in Figure 6.

vi(|S|)

B.1. z = arg max
i∈L
B.2. If vz(|S|) < BF   we are done exploiting L.
B.3. Otherwise:

B.3.1. Move z from L to S.
B.3.2. Update the remaining candidates in L using either Gram-Schmidt or

the Householder procedure.
For example  with Householder:
B.3.2.1. Create the Householder matrix hj from xz.
B.3.2.2. for all x in L replace x with hjx.

Figure 6: Details of Steps 1.2 and 2.2

5

4.1 Correctness

i

|xi|2. The IQRP selects v(cid:48)

In this section we show that the IQRP computes the same selection as the QRP. The proof
the number of columns in S. For j = 0 the QRP selects xj with
is by induction on j 
vj = |xj|2 = max
j as the largest among the l largest values in F . There-
j = maxxi∈L |xi|2 = maxxi∈F |xi|2 = vj. Now assume that for j = |S| the QRP and the
fore v(cid:48)
IQRP select the same columns in S (this is the inductive assumption). Let vj(|S|) be the value of
j(|S|) be the value of the j+1’th selection by the IQRP. We
the j+1’th selection by the QRP  and let v(cid:48)
j(|S|) = vj(|S|). The QRP selection of j satisﬁes: vj(|S|) = maxxi∈F vi(|S|).
need to show that v(cid:48)
Observe that if xi ∈ L then ri = |S|. (Initially L is created from the heap elements that have
ri = |S|. Once S is increased in Step B.3.1 the columns in L are updated according to B.3.2 so that
they all satisfy ri = |S|.) The IQRP selection satisﬁes:

j(|S|) = max
v(cid:48)
xi∈L

vi(|S|)

and v(cid:48)

j(|S|) ≥ BF .

(4)

Additionally for all xα ∈ F \ L:

(5)
This follows from (2)  the observation that vα(r) is monotonically decreasing in r  and rα ≤ |S|.
Therefore  combining (4)  and (5) we get

BF ≥ vα(rα) ≥ vα(|S|).

j(|S|) = max
v(cid:48)
xi∈F

vi(|S|) = vj(|S|).

This completes the proof by induction.

4.2 Termination

To see that the algorithm terminates it is enough to observe that at least one column is selected in
each pass. The condition at Step B.2 in Figure 6 cannot hold at the ﬁrst time in a new L. The value
of BF is the l+1 largest vi(|S|)  while the maximum at B.1 is among the l largest vi(|S|).

4.3 Complexity

the number of selected features
number of passes
a unit cost of orthogonalizing F

The formulas in this section describe the complexity of the IQRP in terms of the following:
n the number of features (matrix columns)
k
p
c
The value of c depends on the implementation of Step A.2.2.1 in Figure 5. We write cmemory for the
value of c in the memory-efﬁcient implementation  and cﬂops for the faster implementation (in terms
of ﬂops). We use the following notation. At pass j the number of selected columns is kj  and the
number of columns that were not skipped in Step 2.1 of the IQRP (same as Step A.1) is nj.
The number of ﬂops in the memory-efﬁcient implementation can be shown to be

m the number of objects (matrix rows)
user provided parameter. 1 ≤ l ≤ n
l
number of IO-passes
q

p(cid:88)

j=2

nj
n

j−1(cid:88)

j(cid:48)=1

ﬂopsmemory = 2mn + 4mnc + 4mlk  where c =

kj(cid:48)

(6)

Observe that c ≤ k2/2  so that for l < n the worst case behavior is the same as the memory-
optimized QRP algorithm  which is O(k2mn). We show in Section 5 that the typical run time is
much faster. In particular  the dependency on k appears to be linear and not quadratic.
For the faster implementation that overwrites the input it can be shown that:

ﬂopstime = 2mn + 4m

(7)
Since ˜ri ≤ k − 1 it follows that ﬂopstime ≤ 4kmn. Thus  the worst case behavior is the same as the
ﬂops-efﬁcient QRP algorithm.

˜ri  where ˜ri is the value of ri at termination.

i=1

n(cid:88)

6

Memory in the memory-efﬁcient implementation requires km in-core ﬂoats  and additional memory
for the heap  that can be reused for the list L. Additional memory to store and manipulate vi  ri
for i = 1  . . .   n is roughly 2n ﬂoats. Observe that these memory locations are being accessed
consecutively  and can be efﬁciently stored and manipulated out-of-core. The data itself  the matrix
A  is stored out-of-core. When the method of Figure 3 is used in A.2.2.1  these matrix values are
read-only.
IO-passes. We wish to distinguish between a pass where the entire data is accessed and a pass where
most of the data is skipped. This suggests the following deﬁnition for the number of IO-passes:

q =(cid:80)p

j=1

n = 1 +(cid:80)p

nj

nj
n .

j=2

Number of ﬂoating point comparisons. Testing for the skipping and manipulating the heap requires
ﬂoating point comparisons. The number of comparisons is n(p − 1 + (q − 1) log2(l + 1)). This
does not affect the asymptotic complexity since the number of ﬂops is much larger.

5 Experimental results

We describe results on several commonly used datasets. “Day1”  with m = 20  000 and n =
3  231  957 is part of the ”URL reputation” collection at the UCI Repository. “thrombin”  with
m = 1  909 and n = 139  351 is the data used in KDD Cup 2001. “Amazon”  with m = 1  500
and n = 10  000 is part of the “Amazon Commerce reviews set” and was obtained from the UCI
Repository. “gisette”  with m = 6  000 and n = 5  000 was used in NIPS 2003 selection challenge.

Measurements. We vary k  and report the following: ﬂopsmemory  ﬂopstime are the ratios between
the number of ﬂops used by the IQRP and kmn  for the memory-efﬁcient orthogonalization and
the time-efﬁcient orthogonalization. # passes is the number of passes needed to select k features.
# IO-passes is discussed in sections 2.4 and 4.3. It is the number of times that the entire data is read.
Thus  the ratio between the number of IO-passes and the number of passes is the fraction of the data
that was not skipped.

Run time. The number of ﬂops of the QRP is between 2kmn and 4kmn. We describe experiments
with the list size l taken as l = k. For Day1 the number of ﬂops beats the QRP by a factor of more
than 100. For the other datasets the results are not as impressive. There are still signiﬁcant savings
for small and moderate values of k (say up to k = 600)  but for larger values the savings are smaller.
Most interesting is the observation that the memory-efﬁcient implementation of Step 2.1 is not much
slower than the optimization for time. Recall that the memory-optimized QRP is k times slower than
the time-optimized QRP. In our experiments they differ by no more than a factor of 4.

Number of passes. We describe experiments with the list size l taken as l = k  and also with
l = 100 regardless of the value of k. The QRP takes k passes for selecting k features. For the
Day1 dataset we observed a reduction by a factor of between 50 to 250 in the number of passes. For
IO-passes  the reduction goes up to a factor of almost 1000. Similar improvements are observed for
the Amazon and the gisette datasets. For the thrombin it is slightly worse  typically a reduction by
a factor of about 70. The number of IO-passes is always signiﬁcantly below the number of passes 
giving a reduction by factors up to 1000. For the recommended setting of l = k we observed the
following. In absolute terms the number of passes was below 10 for most of the data; the number of
IO-passes was below 2 for most of the data.

6 Concluding remarks

This paper describes a new algorithm for unsupervised feature selection. Based on the experiments
we recommend using the memory-efﬁcient implementation and setting the parameter l = k. As
explained earlier the algorithm maintains 2 numbers for each column  and these can also be kept
in-core. This gives a 2(km + n) memory footprint.
Our experiments show that for typical datasets the number of passes is signiﬁcantly smaller than
k. In situations where memory can be skipped the notion of IO-passes may be more accurate than
passes. IO-passes indicate the amount of data that was actually read and not skipped.

7

·10−2

ﬂopsmemory
ﬂopstime

Day1  m = 20  000  n = 3  231  957

s
e
s
s
a
p

f
o

r
e
b
m
u
n

5

4

3

2

1

#passes

#IO-passes

#passes

#IO-passes

s
e
s
s
a
p

f
o

r
e
b
m
u
n

20

15

10

5

0

0

200

400

600
k (l = k)

800

1 000

0

200

400
600
k (l = k)

800

1 000

0

200

400
600
k (l = 100)

800

1 000

thrombin  m = 1  909  n = 139  351

ﬂopsmemory
ﬂopstime

s
e
s
s
a
p

f
o

r
e
b
m
u
n

15

10

5

0

#passes

#IO-passes

#passes

#IO-passes

s
e
s
s
a
p

f
o

r
e
b
m
u
n

40

20

0

0

200

400
600
k (l = k)

800

1 000

0

200

600
400
k (l = k)

800

1 000

0

200

400
600
k (l = 100)

800

1 000

ﬂopsmemory
ﬂopstime

Amazon  m = 1  500  n = 10  000

#passes

#IO-passes

s
e
s
s
a
p

f
o

r
e
b
m
u
n

5

4

3

2

1

#passes

#IO-passes

s
e
s
s
a
p
f
o

r
e
b
m
u
n

15

10

5

0

0

200

400
600
k (l = k)

800

1 000

0

200

400
600
k (l = k)

800

1 000

0

200

400
k (l = 100)

600

ﬂopsmemory
ﬂopstime

gisette  m = 6  000  n = 5  000

s
e
s
s
a
p
f
o

r
e
b
m
u
n

5

4

3

2

1

#passes

#IO-passes

#passes

#IO-passes

s
e
s
s
a
p
f
o

r
e
b
m
u
n

15

10

5

0

4

3

2

1

3

2

1

0

4

3

2

1

0

3

2.5

2

1.5

1

0.5

n
m
k
/
s
p
o
ﬂ

n
m
k
/
s
p
o
ﬂ

n
m
k
/
s
p
o
ﬂ

n
m
k
/
s
p
o
ﬂ

0

200

600
400
k (l = k)

800

1 000

0

200

600
400
k (l = k)

800

1 000

0

200

600
400
k (l = 100)

800

1 000

Figure 7: Results of applying the IQRP to several datasets with varying k  and l = k.

The performance of the IQRP depends on the data. Therefore  the improvements that we observe
can also be viewed as an indication that typical datasets are “easy”. This appears to suggest that
worst case analysis should not be considered as the only criterion for evaluating feature selection
algorithms. Comparing the IQRP to the current state-of-the-art randomized algorithms that were
reviewed in Section 2.2 we observe that the IQRP is competitive in terms of the number of passes
and appears to outperform these algorithms in terms of the number of IO-passes. On the other hand 
it may be less accurate.

8

References
[1] M. Gu and S. C. Eisenstat. Efﬁcient algorithms for computing a strong rank-revealing QR factorization.

SIAM J. Computing  17(4):848–869  1996.

[2] C. Boutsidis  M. W. Mahoney  and P. Drineas. An improved approximation algorithm for the column
subset selection problem. In Claire Mathieu  editor  Proceedings of the Twentieth Annual ACM-SIAM
Symposium on Discrete Algorithms  SODA 2009  New York  NY  USA  January 4-6  2009  pages 968–
977. SIAM  2009.

[3] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Near-optimal column-based matrix reconstruction 

February 2011. arXiv e-print (arXiv:1103.0995).

[4] A. Dasgupta  P. Drineas  B. Harb  V. Josifovski  and M. W. Mahoney. Feature selection methods for text
classiﬁcation. In Pavel Berkhin  Rich Caruana  and Xindong Wu  editors  KDD  pages 230–239. ACM 
2007.

[5] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Sparse features for PCA-like linear regression. In John
Shawe-Taylor  Richard S. Zemel  Peter L. Bartlett  Fernando C. N. Pereira  and Kilian Q. Weinberger 
editors  NIPS  pages 2285–2293  2011.

[6] V. Guruswami and A. K. Sinop. Optimal column-based low-rank matrix reconstruction. In Yuval Rabani 
editor  Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms  SODA
2012  Kyoto  Japan  January 17-19  2012  pages 1207–1214. SIAM  2012.

[7] Z. Li  Y. Yang  J. Liu  X. Zhou  and H. Lu. Unsupervised feature selection using nonnegative spectral
analysis. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence  July 22-26  2012 
Toronto  Ontario  Canada. AAAI Press  2012.

[8] S. Zhang  H.S. Wong  Y. Shen  and D. Xie. A new unsupervised feature ranking method for gene expres-
sion data based on consensus afﬁnity. IEEE/ACM Transactions on Computational Biology and Bioinfor-
matics  9(4):1257–1263  July 2012.

[9] G. H. Golub and C. F. Van-Loan. Matrix computations. The Johns Hopkins University Press  third edition 

1996.

[10] P. Businger and G. H. Golub. Linear least squares solutions by Householder transformations. Numer.

Math.  7:269–276  1965.

[11] A. C¸ ivril and M. Magdon-Ismail. Column subset selection via sparse approximation of SVD. Theoretical

Computer Science  421:1–14  March 2012.

[12] A. M. Frieze  R. Kannan  and S. Vempala. Fast Monte-Carlo algorithms for ﬁnding low-rank approxima-

tions. In IEEE Symposium on Foundations of Computer Science  pages 370–378  1998.

[13] A. M. Frieze  R. Kannan  and S. Vempala. Fast Monte-Carlo algorithms for ﬁnding low-rank approxima-

tions. Journal of the ACM  51(6):1025–1041  2004.

[14] A. Deshpande  L. Rademacher  S. Vempala  and G. Wang. Matrix approximation and projective clustering

via volume sampling. Theory of Computing  2(12):225–247  2006.

[15] A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection. In FOCS 

pages 329–338. IEEE Computer Society Press  2010.

[16] M. W. Mahoney and P. Drineas. CU R matrix decompositions for improved data analysis. Proceedings

of the National Academy of Sciences  106(3):697–702  2009.

[17] P. Drineas  M. Magdon-Ismail  M. W. Mahoney  and D. P. Woodruff. Fast approximation of matrix

coherence and statistical leverage. Journal of Machine Learning Research  13:3441–3472  2012.

[18] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. arXiv

e-print (arXiv:1207.6365v4)  April 2013.

[19] N. Halko  P. G. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms

for constructing approximate matrix decompositions. SIAM Review  53(2):217–288  2011.

[20] T. H. Cormen  C. E. Leiserson  and R. L. Rivest. Introduction to algorithms. MIT Press and McGraw-Hill

Book Company  third edition  2009.

9

,Crystal Maung
Haim Schweitzer