2018,Occam's razor is insufficient to infer the preferences of irrational agents,Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality  several approaches have been tried to account for specific human shortcomings. 
However  the general problem of inferring the reward function of an agent of unknown rationality has received little attention.
Unlike the well-known ambiguity problems in IRL  this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments.
This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function  and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions  we cannot distinguish between the true decomposition and others that lead to high regret.
To address this  we need simple `normative' assumptions  which cannot be deduced exclusively from observations.,Occam’s razor is insufﬁcient to infer the preferences

of irrational agents

S¨oren Mindermann ∗* †

Vector Institute

University of Toronto

Stuart Armstrong* ‡

Future of Humanity Institute

University of Oxford

soeren.mindermann@gmail.com

stuart.armstrong@philosophy.ox.ac.uk

Abstract

Inverse reinforcement learning (IRL) attempts to infer human rewards or pref-
erences from observed behavior. Since human planning systematically deviates
from rationality  several approaches have been tried to account for speciﬁc human
shortcomings. However  the general problem of inferring the reward function of an
agent of unknown rationality has received little attention. Unlike the well-known
ambiguity problems in IRL  this one is practically relevant but cannot be resolved
by observing the agent’s policy in enough environments. This paper shows (1) that
a No Free Lunch result implies it is impossible to uniquely decompose a policy
into a planning algorithm and reward function  and (2) that even with a reasonable
simplicity prior/Occam’s razor on the set of decompositions  we cannot distinguish
between the true decomposition and others that lead to high regret. To address this 
we need simple ‘normative’ assumptions  which cannot be deduced exclusively
from observations.

1

Introduction

In today’s reinforcement learning systems  a simple reward function is often hand-crafted  and still
sometimes leads to undesired behaviors on the part of RL agent  as the reward function is not well
aligned with the operator’s true goals4. As AI systems become more powerful and autonomous  these
failures will become more frequent and grave as RL agents exceed human performance  operate at
time-scales that forbid constant oversight  and are given increasingly complex tasks — from driving
cars to planning cities to eventually evaluating policies or helping run companies. Ensuring that
the agents behave in alignment with human values is known  appropriately  as the value alignment
problem [Amodei et al.  2016  Hadﬁeld-Menell et al.  2016  Russell et al.  2015  Bostrom  2014 
Leike et al.  2017].
One way of resolving this problem is to infer the correct reward function by observing human
behaviour. This is known as Inverse reinforcement learning (IRL) [Ng and Russell  2000  Abbeel and
Ng  2004  Ziebart et al.  2008]. Often  learning a reward function is preferred over imitating a policy:
when the agent must outperform humans  transfer to new environments  or be interpretable. The
reward function is also usually a (much) more succinct and robust task representation than the policy 
especially in planning tasks [Abbeel and Ng  2004]. Moreover  supervised learning of long-range and
goal-directed behavior is often difﬁcult without the reward function [Ratliff et al.  2006].

∗Equal contribution.
†Work performed at Future of Humanity Institute.
‡Further afﬁliation: Machine Intelligence Research Institute  Berkeley  USA.
4See for example the game CoastRunners  where an RL agent didn’t ﬁnish the course  but instead
found a bug allowing it to get a high score by crashing round in circles https://blog.openai.com/
faulty-reward-functions/.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Usually  the reward function is inferred based on the assumption that human behavior is optimal
or noisily optimal. However  it is well-known that humans deviate from rationality in systematic 
non-random ways [Tversky and Kahneman  1975]. This can be due to speciﬁc biases such as time-
inconsistency  loss aversion and hundreds of others  but also limited cognitive capacity  which leads
to forgetfulness  limited planning and false beliefs. This limits the use of IRL methods for tasks that
humans don’t ﬁnd trivial.
Some IRL approaches address speciﬁc biases [Evans et al.  2015b a]  and others assume noisy
rationality [Ziebart et al.  2008  Boularias et al.  2011]. But a general framework for inferring the
reward function from suboptimal behavior does not exist to our knowledge. Such a framework needs
to infer two unobserved variables simultaneously: the human reward function and their planning
algorithm5 which connects the reward function with behaviour  henceforth called a planner.
The task of observing human behaviour (or the human policy) and inferring from it the human reward
function and planner will be termed decomposing the human policy. This paper will show there is a
No Free Lunch theorem in this area: it is impossible to get a unique decomposition of human policy
and hence get a unique human reward function. Indeed  any reward function is possible. And hence 
if an IRL agent acts on what it believes is the human policy  the potential regret is near-maximal.
This is another form of unidentiﬁability of the reward function  beyond the well-known ones [Ng and
Russell  2000  Amin and Singh  2016].
The main result of this paper is that  unlike other No Free Lunch theorems  this unidentiﬁability
does not disappear when regularising with a general simplicity prior that formalizes Occam’s razor
[Vitanyi and Li  1997]. This result will be shown in two steps: ﬁrst  that the simplest decompositions
include degenerate ones  and secondly  that the most ‘reasonable’ decompositions according to human
judgement are of high complexity.
So  although current IRL methods can perform well on many well-speciﬁed problems  they are
fundamentally and philosophically incapable of establishing a ‘reasonable’ reward function for the
human  no matter how powerful they become. In order to do this  they will need to build in ‘normative
assumptions’: key assumptions about the reward function and/or planner  that cannot be deduced
from observations  and allow the algorithm to focus on good ways of decomposing the human policy.
Future work will sketch out some potential normative assumptions that can be used in this area 
making use of the fact that humans assess each other to be irrational  and often these assessments
agree. In view of the No Free Lunch result  this shows that humans must share normative assumptions.
One of these ‘normative assumption’ approaches is brieﬂy illustrated in an appendix  while another
appendix demonstrates how to use the planner-reward formalism to deﬁne when an agent might be
manipulating or overriding human preferences. This happens when the agent pushes the human
towards situations where their policy is very suboptimal according to their reward function.

2 Related Work

In the ﬁrst IRL papers from Ng and Russell [2000] and Abbeel and Ng [2004] a max-margin algorithm
was used to ﬁnd the reward function under which the observed policy most outperforms other policies.
Suboptimal behavior was ﬁrst addressed explicitly by Ratliff et al. [2006] who added slack variables to
allow for suboptimal behavior. This ﬁnds reward functions such that the observed policy outperforms
most other policies and the biggest margin by which another policy outperforms it is minimal  i.e. the
observed policy has low regret. Shiarlis et al. [2017] introduce a modern max-margin technique with
an approximate planner in the optimisation.
However  the max-margin approach has mostly been replaced by the max entropy IRL [Ziebart et al. 
2008]. Here  the assumption is that observed actions or trajectories are chosen with probability
proportional to the exponent of their value. This assumes a speciﬁc suboptimal planning algorithm
which is noisily rational (also known as Boltzmann-rational). Noisy rationality explains human
behavior on various data sets better [Hula et al.  2015]. However  Evans et al. [2015b] and Evans et al.
[2015a] showed that this can fail since humans deviate from rationality in systematic  non-random
ways. If noisy rationality is assumed  repeated suboptimal actions throw off the inference.

5 Technically we only need to infer the human reward function  but inferring that from behaviour requires

some knowledge of the planning algorithm.

2

Literature on inferring the reasoning capabilities of an agent is scarce. Evans et al. [2015b] and Evans
et al. [2015a] use Bayesian inference to identify speciﬁc planning biases such as myopic planning and
hyperbolic time-discounting. They simultaneously infer the agent’s preferences. Cundy and Filan
[2018] adds bias resulting from hierarchical planning. Hula et al. [2015] similarly let agents infer
features of their opponent’s reasoning such as planning depth and impulsivity in simple economic
games. Recent work learns the planning algorithm with two assumptions: being close to noisily
rational in a high-dimensional planner space and supervised planner-learning [Anonymous  2019].
The related ideas of meta-reasoning [Russell  2016]  computational rationality [Lewis et al.  2014] and
resource rationality [Grifﬁths et al.  2015] may create the possibility to redeﬁne irrational behavior
as rational in an ‘ancestral’ distribution of environments where the agent optimises its rewards by
choosing among the limited computations it is able to perform or jointly minimising the cost of
computation and maximising reward. This could in theory redeﬁne many biases as computationally
optimal in some distribution of environments and provide priors on human planning algorithms.
Unfortunately the problem of doing this in practice seems to be extremely difﬁcult — and it assumes
that human goals are roughly the same as evolution’s goals  which is certainly not the case.

3 Problem setup and background

A human will be performing a series of actions  and from these  an agent will attempt to estimate
both the human’s reward function and their planning algorithm.
The environment M in which the human operates is an MDP/R  a Markov Decision Process without
reward function (a world-model [Hadﬁeld-Menell et al.  2017]). An MDP/R is deﬁned as a tuple 
(cid:104)S A  T  ˆs(cid:105) consisting of a discrete state space S  a ﬁnite action space A  a ﬁxed starting state ˆs  and
a probabilistic transition function T : S × A × S → [0  1] to the next state (also called the dynamics).
At each step  the human is in a certain state s  takes a certain action a  and ends up in a new state s(cid:48)
as given by T (s(cid:48) | s  a).
Let R = {R : S × A → [−1  1]} = [−1  1]S×A be the space of candidate reward functions; a given
R will map any state-reward pair to a reward value in the interval [−1  1].
Let Π be the space of deterministic  Markovian policies. So Π is the space of functions S → A. The
human will be following the policy ˙π ∈ Π.
The results of this paper apply to both discounted rewards and episodic environments settings6.

3.1 Planners and reward functions: decomposing the policy

The human has their reward function  and then follows a policy that presumably attempts to maximise
it. Therefore there is something that bridges between the reward function and the policy: a piece of
greater or lesser rationality that transforms knowledge of the reward function into a plan of action.
This bridge will be modeled as a planner p : R → Π  a function that takes a reward and outputs a
policy. This planner encodes all the rationality  irrationality  and biases of the human. Let P be the
set of planners. The human is therefore deﬁned by a planner-reward pair (p  R) ∈ P × R. Similarly 
(p  R) with p(R) = π is a decomposition of the policy π. The task of the agent is to ﬁnd a ‘good’
decomposition of the human policy ˙π.

3.2 Compatible pairs and evidence

The agent can observe the human’s behaviour and infer their policy from that. In order to simplify
the problem and separate out the effect of the agent’s learning  we will assume the agent has perfect
knowledge of the human policy ˙π and of the environment M. At this point  the agent cannot learn
anything by observing the human’s actions  as it can already perfectly predict these.
Then a pair (p  R) is deﬁned to be compatible with ˙π  if p(R) = ˙π — thus that pair is a possible
candidate for decomposing the human policy into the human’s planner and reward function.

6The setting is only chosen for notational convenience:

it also emulates discrete POMDPs  non-

Markovianness (eg by encoding the whole history in the state) and pseudo-random policies.

3

4

Irrationality-based unidentiﬁability

Unidentiﬁability of the reward is a well-known problem in IRL [Ng and Russell  2000]. Amin and
Singh [2016] categorise the problem into representational and experimental unidentiﬁability. The
former means that adding a constant to a reward function or multiplying it with a positive scalar
does not change what is optimal behavior. This is unproblematic as rescaling the reward function
doesn’t change the preference ordering. The latter can be resolved by observing optimal policies in a
whole class of MDPs which contains all possible transition dynamics. We complete this framework
with a third kind of identiﬁability  which arises when we observe suboptimal agents. This kind of
unidentiﬁability is worse as it cannot necessarily be resolved by observing the agent in many tasks.
In fact  it can lead to almost arbitrary regret.

4.1 Weak No Free Lunch: unidentiﬁable reward function and half-maximal regret

The results in this section show that without assumptions about the rationality of the human  all
attempts to optimise their reward function are essentially futile. Everitt et al. [2017] work in a similar
setting as we do: in their case  a corrupted version of the reward function is observed. The problem
our case is that a ‘corrupted’ version ˙π of an optimal policy π∗
is observed and used as information
˙R
to optimise for the ideal reward ˙R. A No Free Lunch result analogous to theirs applies in our case;
both resemble the No Free Lunch theorems for optimisation [Wolpert and Macready  1997].
More philosophically  this result is as an instance of the well-known is-ought problem from meta-
ethics. Hume [1888] argued that what ought to be (here  the human’s reward function) can never be
concluded from what is (here  behavior) without extra assumptions. Equivalently  the human reward
function cannot be inferred from behavior without assumptions about the planning algorithm p. In
p∈P P (π | R  p)P (p) is undeﬁned without P (p). As

probabilistic terms  the likelihood P (π|R) =(cid:80)

shown in Section 5 and Section 5.2  even a simplicity prior on p and R will not help.

4.1.1 Unidentiﬁable reward functions

Firstly  we note that compatibility (p(R) = ˙π)  puts no restriction on R  and few restrictions on p:
Theorem 1. For all π ∈ Π and R ∈ R  there exists a p ∈ P such that p(R) = π.
For all p ∈ P and π ∈ Π in the image of p  there exists an R such that p(R) = π.
Proof. Trivial proof: deﬁne the planner7 p as mapping all of R to π; then p(R) = π. The second
statement is even more trivial  as π is in the image of p  so there must exist R with p(R) = π.

4.1.2 Half-maximal regret

The above shows that the reward function cannot be constrained by observation of the human  but
what about the expected long-term value? Suppose that an agent is unsure what the actual human
reward function is; if the agent itself is acting in an MDP/R  can it follow a policy that minimises the
possible downside of its ignorance?
This is prevented by a recent No Free Lunch theorem. Being ignorant of the reward function one
should maximise is equivalent of having a corrupted reward channel with arbitrary corruption. In that
case  Everitt et al. [2017] demonstrated that whatever policy π the agent follows  there is a R ∈ R
for which π is half as bad as the worst policy the agent could have followed. Speciﬁcally  let V π
R (s)
be the expected return of reward function R from state s  given that the agent follows policy π. If π
was the optimal policy for R  then this can be written as V ∗
R(s). The regret of π for R at s is given by
the difference:

Reg(π  R)(s) = V ∗
Then Everitt et al. [2017] demonstrates that for any π 

(cid:18)

R (s).

R(s) − V π

(cid:19)
π(cid:48)∈Π R∈R Reg(π(cid:48)  R)(s)

R∈R Reg(π  R)(s) ≥ 1

max

2

max

.

So for any compatible (p  R) = ˙π  we cannot rule out that maximizing R leads to at least half of the
worst-case regret.

7This is the ‘indifferent’ planner pπ of subsubsection 5.1.1.

4

5 Simplicity of degenerate decompositions

Like many No Free Lunch theorems  the result of the previous section is not surprising given there are
no assumptions about the planning algorithm. No Free Lunch results are generally avoided by placing
a simplicity prior on the algorithm  dataset  function class or other object [Everitt et al.  2014]. This
amounts to saying algorithms can beneﬁt from regularisation. This section is dedicated to showing
that  surprisingly  simplicity does not solve the No Free Lunch result.
Our simplicity measure is minimum description length of an object  deﬁned as Kolmogorov com-
plexity [Kolmogorov  1965]  the length of the shortest program that outputs a string describing the
object. This is the most general formalization of Occam’s razor we know of [Vitanyi and Li  1997].
Appendix A explores how the results extend to other measures of complexity  such as those that
include computation time. We start with informal versions of our main results.
Theorem 2 (Informal simplicity theorem). Let ( ˙p  ˙R) be a ‘reasonable’ planner-reward pair that
captures our judgements about the biases and rationality of a human with policy ˙π = ˙p( ˙R). Then
there are degenerate planner-reward pairs  compatible with ˙π  of lower complexity than ( ˙p  ˙R)  and
a pair ( ˙p(cid:48) − ˙R) of similar complexity to ( ˙p  ˙R)  but with opposite reward function.
There are a few issues with this theorem as it stands. Firstly  simplicity in algorithmic information
theory is relative to the computer language (or equivalently Universal Turing Machine) L used [Ming
and Vit´anyi  2014  Calude  2002]  and there exists languages in which the theorem is clearly false:
one could choose a degenerate language in which ( ˙p  ˙R) is encoded by the string ‘0’  for example 
and all other planner-reward pairs are of extremely long length. What constitutes a ‘reasonable’
language is a long-standing open problem  see Leike et al. [2017] and M¨uller [2010]. For any pair of
languages  complexities differ only by a constant  the amount required for one language to describe
the other  but this constant can be arbitrarily large.
Nevertheless  this section will provide grounds for the following two semi-formal results:
Proposition 3. If ˙π is a human policy  and L is a ‘reasonable’ computer language  then there exists
degenerate planner-reward pairs amongst the pairs of lowest complexity compatible with ˙π.
Proposition 4. If ˙π is a human policy  and L is a ‘reasonable’ computer language with ( ˙p  ˙R) a
compatible planner-reward pair  then there exist a pair ( ˙p(cid:48) − ˙R) of comparable complexity to ( ˙p  ˙R) 
but opposite reward function.

The last part of Theorem 2  the fact that any ‘reasonable’ ( ˙p  ˙R) is expected to be of higher complexity 
will be addressed in Section 6.

5.1 Simple degenerate pairs

The argument in this subsection will be that 1) the complexity of ˙π is close to a lower bound on
any pair compatible with it and 2) degenerate decompositions are themselves close to this bound.
The ﬁrst statement follows because for any decomposition (p  R) compatible with ˙π  the map
(p  R) (cid:55)→ p(R) = ˙π will be a simple one  adding little complexity. And if a compatible pair (p(cid:48)  R(cid:48))
can be from ˙π with little extra complexity  then it too will have a complexity close to the minimal
complexity of any other pair compatible with it. Therefore we will ﬁrst produce three degenerate
pairs that can be simply constructed from ˙π.

5.1.1 The degenerate pairs

We can deﬁne the trivial constant reward function 0  and the greedy planner pg. The greedy planner
pg acts by taking the action that maximises the immediate reward in the current state and the next
action. Thus8 pg(R)(s) = argmaxa R(s  a). We can also deﬁne the anti-greedy planner −pg  with
−pg(R)(s) = argmina R(s  a). In general  it will be useful to deﬁne the negative of a planner:
Deﬁnition 5. If p : R → Π is a planner  the planner −p is deﬁned by −p(R) = p(−R).
For any given policy π  we can deﬁne the indifferent planner pπ  which maps any reward function to
π. We can also deﬁne the reward function Rπ  so that Rπ(s  a) = 1 if π(s) = a  and Rπ(s  a) = 0
otherwise. The reward function −Rπ is deﬁned to be the negative of Rπ. Then:

8Recall that pg is a planner  pg(R) is a policy  so pg(R) can be applied to states  and pg(R)(s) is an action.

5

Lemma 6. The pairs (pπ  0)  (pg  Rπ)  and (−pg −Rπ) are all compatible with π.

Proof. Since the image pπ is π  pπ(0) = π. Now  Rπ(s  a) > 0 iff π(s) = a  hence for all s:

so pg(Rπ) = π. Then −pg(−Rπ) = pg(−(−Rπ)) = pg(Rπ) = π  by Deﬁnition 5.

pg(Rπ)(s) = argmax

Rπ(s  a) = π(s) 

a

5.1.2 Complexity of basic operations

We will look the operations that build the degenerate planner-reward pairs from any compatible pair:

1. For any planner p  f1(p) = (p  0) as a planner-reward pair.
2. For any reward function R  f2(R) = (pg  R).
3. For any planner-reward pair (p  R)  f3(p  R) = p(R).
4. For any planner-reward pair (p  R)  f4(p  R) = (−p −R).
5. For any policy π  f5(π) = pπ.
6. For any policy π  f6(π) = Rπ.

These will be called the basic operations  and there are strong arguments that reasonable computer
languages should be able to express them with short programs. The operation f1  for instance  is
simply appending the ﬂat trivial 0  f2 appends a planner deﬁned by the simple9 search operator
argmax  f3 applies a planner to the object — a reward function — that the planner naturally acts on 
f4 is a double negation  while f5 and f6 are simply described in subsubsection 5.1.1.
From these basic operations  we can deﬁne three composite operations that map any compatible
planner-reward pair to one of the degenerate pairs (the element F4 = f4 is useful for later deﬁnitions).
Thus deﬁne

F = {F1 = f1 ◦ f5 ◦ f3  F2 = f2 ◦ f6 ◦ f3  F3 = f4 ◦ f2 ◦ f6 ◦ f3  F4 = f4}.

For any ˙π-compatible pair (p  R) we have F1(p  R) = (p ˙π  0)  F2(p  R) = (pg  R ˙π)  and F3(p  R) =
(−pg −R ˙π) (see the proof of Proposition 7).
Let KL denote Kolmogorov complexity in the language L: the shortest algorithm in L that generates
a particular object. We deﬁne the F -complexity of L as

max

(p R) Fi∈F

KL(Fi(p  R)) − KL(p  R).

Thus the F -complexity of L is how much the Fi potentially increase10 the complexity of pairs.
For a constant c ≥ 0  this allows us to formalise what we mean by L being a c-reasonable language
for F : that the F -complexity of L is at most c. A reasonable language is a c-reasonable language for
a c that we feel is intuitively low enough.

5.1.3 Low complexity of degenerate planner-reward pairs

To formalise the concepts ‘of lowest complexity’  and ‘of comparable complexity’  choose a constant
c ≥ 0  then (p  R) and (p(cid:48)  R(cid:48)) are of ‘comparable complexity’ if
||KL(p  R) − KL(p(cid:48)  R(cid:48))|| ≤ c.

For a set S ⊂ P × R  the pair (p  R) ∈ S is amongst the lowest complexity in S if

||KL(p  R) − min
(p(cid:48) R(cid:48))∈S

KL(p(cid:48)  R(cid:48))|| ≤ c 

thus KL is within distance c of the minimum complexity element of S. Now formalize Proposition 3:
9 In most standard computer languages  argmax just requires a for-loop  a reference to R  a comparison
with a previously stored value  and possibly the storage of a new value and the current action.
10F -complexity is non-negative: F4 ◦ F4 is the identity  so that KL(F4(p  R)) − KL(p  R) =
−(KL(F4(F4(p  R)) − KL(F4(p  R))  meaning that max(p R) F4 KL(F4(p  R)) − KF (p  R) must be non-
negative; this is a reason to include F4 in the deﬁnition of F .

6

Proposition 7. If ˙π is the human policy  c deﬁnes a reasonable measure of comparable complexity 
and L is a c-reasonable language for F   then the degenerate planner-reward pairs (p ˙π  0)  (pg  R ˙π) 
and (−pg −R ˙π) are amongst the pairs of lowest complexity among the pairs compatible with ˙π.
Proof. By Lemma 6  (p ˙π  0)  (pg  R ˙π)  and (−pg −R ˙π) are compatible with ˙π. By the deﬁnitions
of the fi and Fi  for(p  R) compatible with ˙π  f3((p  R)) = p(R) = ˙π and hence

F1(p  R) = f1 ◦ f5( ˙π) = f1(p ˙π) = (p ˙π  0) 
F2(p  R) = f2 ◦ f6( ˙π) = f2(R ˙π) = (pg  R ˙π) 
F3(p  R) = f4 ◦ F2(p  R) = (−pg −R ˙π).

Now pick (p  R) to be the simplest pair compatible with ˙π. Since L is c-reasonable for F  
KL(p ˙π  0) ≤ c + KL(p  R). Hence (p ˙π  0) is of lowest complexity among the pairs compatible with
˙π; the same argument applies for the other two degenerate pairs.

5.2 Negative reward
If ( ˙p  ˙R) is compatible with ˙π  then so is (− ˙p − ˙R) = f4( ˙p  ˙R) = F4( ˙p  ˙R). This immediately
implies the formalisation of Proposition 4:
Proposition 8. If ˙π is a human policy  c deﬁnes a reasonable measure of comparable complexity  L
is a c-reasonable language for F   and ( ˙p  ˙R) is compatible with ˙π  then (− ˙p − ˙R) is of comparable
complexity to ( ˙p  ˙R).

So complexity fails to distinguish between a reasonable human reward function and its negative.

6 The high complexity of the genuine human reward function

Section 5 demonstrated that there are degenerate planner-reward pairs close to the minimum com-
plexity among all pairs compatible with ˙π. This section will argue that any reasonable pair ( ˙p  ˙R)
is unlikely to be close to this minimum  and is therefore of higher complexity than the degenerate
pairs. Unlike simplicity  reasonable decomposition cannot easily be formalised. Indeed  a formaliza-
tion would likely already solve the problem  yielding an algorithm to maximize it. Therefore  the
arguments in this section are mostly qualitative.
We use reasonable to mean ‘compatible with human judgements about rationality’. Since we do
not have direct access to such a decomposition  the complexity argument will be about showing the
complexity of these human judgements. This argument will proceed in three stages:

1. Any reasonable ( ˙p  ˙R) is of high complexity  higher than it may intuitively seem to us.
2. Even given ˙π  any reasonable ( ˙p  ˙R) involves a high number of contingent choices. Hence

any given ( ˙p  ˙R) has high information (and thus high complexity)  even given ˙π.
3. Past failures to ﬁnd a simple ( ˙p  ˙R) derived from ˙π are evidence that this is tricky.

6.1 The complexity of human (ir)rationality

Humans make noisy and biased decisions all the time. Though noise is important [Kahneman et al. 
2016]  many biases  such as anchoring bias  overconﬁdence  planning fallacies  and so on  affect
humans in a highly systematic way; see Kahneman and Egan [2011] for many examples.
Many people may feel that they have a good understanding of rationality  and therefore assume that
assessing the (ir)rationality of any particular decision is not a complicated process. But an intuition
for bias does not translate into a process for establishing a ( ˙p  ˙R).
Consider the anchoring bias deﬁned in Ariely et al. [2004]  where irrelevant information — the last
digits of social security numbers — changed how much people were willing to pay for goods. When
deﬁning a reasonable ( ˙p  ˙R)  it does not sufﬁce to be aware of the existence of anchoring bias11  but

11 The fact that many cognitive biases have only been discovered recently argue against people having a good

intuitive grasp of bias and rationality  as do people’s persistent bias blind spots [Scopelliti et al.  2015].

7

one has to precisely quantify the extent of the bias — why does anchoring bias seem to be stronger
for chocolate than for wine  for instance? And why these precise percentages and correlations  and
not others? And can people’s judgment tell which people are more or less susceptible to anchoring
bias? And can one quantify the bias for a single individual  rather than over a sample?
Any given ( ˙p  ˙R) can quantify the form and extent of these biases by computing objects like the regret
function Reg( ˙p  ˙R)(s) := Reg( ˙p( ˙R)  ˙R)(s) = V ∗
(s)  which measures the divergence
˙R
between the expected value of the actual and optimal human policies12. Thus any given ( ˙p  ˙R)
— which contains the information to compute quantities like Reg( ˙p  ˙R)(s) or similar measures of
bias13  in every state — carries a high amount of numerical information about bias  and hence a high
complexity.
Since humans do not easily have access to this information  this implies that human judgement of
irrationality is subject to Moravec’s paradox [Moravec  1988]. It is similar to  for example  social
skills: though it seems intuitively simple to us  it is highly complex to deﬁne in algorithmic terms.
Other authors have argued directly for the complexity of human values  from ﬁelds as diverse as
computer science  philosophy  neuroscience  and economics [Minsky  1984  Bostrom  2014  Glimcher
et al.  2009  Muehlhauser and Helm  2012  Yudkowsky  2011].

(s) − V ˙p( ˙R)

˙R

6.2 The contingency of human judgement

The previous section showed that reasonable ( ˙p  ˙R) carry large amounts of information/complexity 
but the key question is whether it requires information additional to that in ˙π. This section will show
that even when ˙π is known  there are many contingent choices that need to be made to deﬁne any
speciﬁc reasonable ( ˙p  ˙R). Hence any given ( ˙p  ˙R) contains a large amount of information beyond
that in ˙π  and hence is of higher complexity.
Reasons to believe that human judgement about reasonable ( ˙p  ˙R) contains many contingent choices:
• There is a variability of human judgement between cultures. When Miller [1984] compared
American and Indian assessments of the same behaviours  they found systematically different
explanations for them14 Basic intuitions about rationality also vary between cultures [Nisbett
et al.  2001  Br¨uck  1999].
• There is a variability of human judgement within a single culture. When Slovic and Tversky
[1974] analysed the “Allais Paradox”  they found that different people gave different answers
as to what the rational behaviour was in their experiments.
• There is evidence of variability of human judgement within the same person. Slovic and
Tversky [1974] further attempted to argue for the rationality of one of the answers. This
sometimes resulted in the participant sometimes changing their minds  and contradicting
their previous assessment of rationality.
• There is a variability of human judgement for the same person assessing their own values 
caused by differences as trivial as question ordering [Schuman and Ludwig  1983]. So
human meta-judgement  of own values and rationality  is also contingent and variable.
• People have partial bias blind spots around their own biases [Scopelliti et al.  2015].

Thus if a human is following policy ˙π  a decomposition ( ˙p  ˙R) would provide additional information
about the cultural background of the decomposer  their personality within their culture  and even
about the past history of the decomposer and how the issue is being presented to them. Those last
pieces prevents us from ‘simply’ using the human’s own assessment of their own rationality  as that
assessment is subject to change and re-interpretation depending on their possible histories.

12To exactly quantify the anchoring bias above  we could use a regret function that contrasts ˙π with the same

policy  but where the decision is optimal for one turn only (rather than for all turns  as in standard regret).

13In constrast  regret for the degenerate planner-reward pairs is trivial. Reg(p ˙π  0) and Reg(pg  R ˙π) are
identically zero — in the second case  since pg(R ˙π) is actually optimal for R ˙π  getting the maximal possible
reward — while (−pg −R ˙π) has a regret that is identically −1 at each step.

14“Results show that there were cross-cultural and developmental differences related to contrasting cultural

conceptions of the person [...] rather than from cognitive  experiential  and informational differences [...].”

8

6.3 The search for human rationality models

One ﬁnal argument that there is no simple algorithm for going from ˙π to ( ˙p  ˙R): many have tried
and failed to ﬁnd such an algorithm. Since the subject of human rationality has been a major one for
several thousands of years  the ongoing failure is indicative — though not a proof — of the difﬁculties
involved. There have been many suggested philosophical avenues for ﬁnding such a reward (such as
reﬂective equilibrium [Rawls  1971])  but all have been underdeﬁned and disputed.
The economic concept of revealed preferences [Samuelson  1948] is the most explicit  using the
assumption of rational behaviour to derive human preferences. This is an often acceptable approx-
imation  but can be taken too far: failure to take achieve an achievable goal does not imply that
failure was desired. Even within the conﬁnes of economics  it has been criticised by behavioural
economics approaches  such as prospect theory [Kahneman and Tversky  2013] — and there are
counter-criticisms to these.
Using machine learning to deduce the intentions and preferences of humans is in its infancy  but we
can see non-trivial real-world examples  even in settings as simple as car-driving [Lazar et al.  2018].
Thus to date  neither humans nor machine learning have been able to ﬁnd simple ways of going from
˙π to ( ˙p  ˙R)  nor any simple and explicit theory for how such a decomposition could be achieved. This
suggests that ( ˙p  ˙R) is a complicated object  even if ˙π is known. In conclusion:
Conjecture 9 (Informal complexity proposition). If ˙π is a human policy  and L is a ‘reasonable’
computer language with ( ˙p  ˙R) a ‘reasonable’ compatible planner-reward pair  then the complexity of
( ˙p  ˙R) is not close to minimal amongst the pairs compatible with ˙π.

7 Conclusion

We have shown that some degenerate planner-reward decompositions of a human policy have near-
minimal description length and argued that decompositions we would endorse do not. Hence  under
the Kolmogorov-complexity simplicity prior  a formalization of Occam’s Razor  the posterior would
endorse degenerate solutions. Previous work has shown that noisy rationality is too strong an
assumption as it does not account for bias; we tried the weaker assumption of simplicity  strong
enough to avoid typical No Free Lunch results  but it is insufﬁcient here.
This is no reason for despair: there is a large space to explore between these two extremes. Our hope
is that with some minimal assumptions about planner and reward we can infer the rest with enough
data. Staying close to agnostic is desirable in some settings: for example  a misspeciﬁed model of
the human reward function can lead to disastrous decisions with high conﬁdence [Milli et al.  2017].
Anonymous [2019] makes a promising ﬁrst try — a high-dimensional parametric planner is initialized
to noisy rationality and then adapts to ﬁt the behavior of a systematically irrational agent.
How can we reconcile our results with the fact that humans routinely make judgments about the
preferences and irrationality of others? And  that these judgments are often correlated from human
to human? After all  No Free Lunch applies to human as well as artiﬁcial agents. Our result shows
that they must be using shared priors  beyond simplicity  that are not learned from observations.
We call these normative assumptions because they encode beliefs about which reward functions are
more likely and what constitutes approximately rational behavior. Uncovering minimal normative
assumptions would be an ideal way to build on this paper; Appendix C shows one possible approach.

Acknowledgments.

We wish to thank Laurent Orseau  Xavier O’Rourke  Jan Leike  Shane Legg  Nick Bostrom  Owain
Evans  Jelena Luketina  Tom Everrit  Jessica Taylor  Paul Christiano  Eliezer Yudkowsky  Stuart
Russell  Dylan Hadﬁeld-Menell  and Anders Sandberg  Adam Gleave  Rohin Shah  among many
others. This work was supported by the Alexander Tamas programme on AI safety research  the
Leverhulme Trust  and the Machine Intelligence Research Institute.

9

References
Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning.

2004.

Eric Allender. When worlds collide: Derandomization  lower bounds  and kolmogorov complexity.
In International Conference on Foundations of Software Technology and Theoretical Computer
Science  pages 1–15. Springer  2001.

Kareem Amin and Satinder Singh. Towards Resolving Unidentiﬁability in Inverse Reinforcement

Learning. 2016.

Dario Amodei  Chris Olah  Jacob Steinhardt  Paul Christiano  John Schulman  and Dan Man´e.

Concrete Problems in AI Safety. 2016.

Anonymous. Inferring reward functions from demonstrators with unknown biases. In Submitted to
International Conference on Learning Representations  2019. URL https://openreview.net/
forum?id=rkgqCiRqKQ. under review.

Dan Ariely  George Loewenstein  and Drazen Prelec. Arbitrarily coherent preferences. The psychology

of economic decisions  2:131–161  2004.

Nick Bostrom. Superintelligence: Paths  dangers  strategies. Oxford University Press  2014.

Abdeslam Boularias  Jens Kober  and Jan Peters. Relative Entropy Inverse Reinforcement Learning 

2011.

Joanna Br¨uck. Ritual and rationality: some problems of interpretation in european archaeology.

European journal of archaeology  2(3):313–344  1999.

Cristian Calude. Information and randomness : an algorithmic perspective. Springer  2002.

Chris Cundy and Daniel Filan. Exploring hierarchy-aware inverse reinforcement learning. arXiv

preprint arXiv:1807.05037  2018.

Owain Evans  Andreas Stuhlmueller  and Noah D. Goodman. Learning the Preferences of Ignorant 

Inconsistent Agents. Thirtieth AAAI Conference on Artiﬁcial Intelligence  2015a.

Owain Evans  Andreas Stuhlm¨uller  and Noah D Goodman. Learning the preferences of bounded

agents. NIPS Workshop on Bounded Optimality  pages 16–22  2015b.

Tom Everitt and Marcus Hutter. Avoiding wireheading with value reinforcement learning.

International Conference on Artiﬁcial General Intelligence  pages 12–22. Springer  2016.

In

Tom Everitt  Tor Lattimore  and Marcus Hutter. Free Lunch for optimisation under the universal
distribution. In Proceedings of the 2014 IEEE Congress on Evolutionary Computation  CEC 2014 
pages 167–174  2014.

Tom Everitt  Victoria Krakovna  Laurent Orseau  Marcus Hutter  and Shane Legg. Reinforcement

Learning with a Corrupted Reward Channel. 2017.

Paul W Glimcher  Colin F Camerer  Ernst Fehr  and Russell A Poldrack. Neuroeconomics: Decision

making and the brain  2009.

Thomas L. Grifﬁths  Falk Lieder  and Noah D. Goodman. Rational Use of Cognitive Resources:
Levels of Analysis Between the Computational and the Algorithmic. Topics in Cognitive Science 
7(2):217–229  2015.

Dylan Hadﬁeld-Menell  Anca Dragan  Pieter Abbeel  and Stuart Russell. Cooperative Inverse

Reinforcement Learning. arXiv:1606.03137 [cs]  2016.

Dylan Hadﬁeld-Menell  Smitha Milli  Stuart J Russell  Pieter Abbeel  and Anca Dragan. Inverse
reward design. In Advances in Neural Information Processing Systems  pages 6749–6758  2017.

10

Andreas Hula  P. Read Montague  and Peter Dayan. Monte Carlo Planning Method Estimates
Planning Horizons during Interactive Social Exchange. PLOS Computational Biology  11(6):
e1004254  2015.

David Hume. Treatise on Human Nature Ed Selby-bigge  L a. 1888.

Daniel Kahneman and Patrick Egan. Thinking  fast and slow  volume 1. Farrar  Straus and Giroux

New York  2011.

Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In
Handbook of the fundamentals of ﬁnancial decision making: Part I  pages 99–127. World Scientiﬁc 
2013.

Daniel Kahneman  Andrew M Rosenﬁeld  Linnea Gandhi  and Tom Blaser. Noise: How to overcome
the high  hidden cost of inconsistent decision making. Harvard business review  94(10):38–46 
2016.

Andrei N Kolmogorov. Three approaches to the quantitative deﬁnition oﬁnformation’. Problems of

information transmission  1(1):1–7  1965.

Daniel A Lazar  Kabir Chandrasekher  Ramtin Pedarsani  and Dorsa Sadigh. Maximizing road

capacity using cars that inﬂuence people. arXiv preprint arXiv:1807.04414  2018.

Jan Leike  Miljan Martic  Victoria Krakovna  Pedro Ortega  Tom Everitt  Andrew Lefrancq  Laurent

Orseau  and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883  2017.

Leonid A Levin. Randomness conservation inequalities; information and independence in mathemat-

ical theories. Information and Control  61(1):15–37  1984.

Richard L Lewis  Andrew Howes  and Satinder Singh. Computational Rationality: Linking Mech-
anism and Behavior Through Bounded Utility Maximization. Topics in Cognitive Science  6:
279–311  2014.

Joan G Miller. Culture and the development of everyday social explanation. Journal of personality

and social psychology  46(5):961  1984.

Smitha Milli  Dylan Hadﬁeld-Menell  Anca Dragan  and Stuart Russell. Should robots be obedient?

arXiv preprint arXiv:1705.09990  2017.

LI Ming and Paul MB Vit´anyi. Kolmogorov complexity and its applications. Algorithms and

Complexity  1:187  2014.

Marvin Minsky. Afterword to Vernor Vinge’s novel  “True names.” Unpublished manuscript. 1984.

URL http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html.

Hans Moravec. Mind children: The future of robot and human intelligence. Harvard University Press 

1988.

Luke Muehlhauser and Louie Helm. The singularity and machine ethics. In Singularity Hypotheses 

pages 101–126. Springer  2012.

Markus M¨uller. Stationary algorithmic probability. Theoretical Computer Science  411(1):113–130 

2010.

Andrew Ng and Stuart Russell. Algorithms for inverse reinforcement learning. Proceedings of the

Seventeenth International Conference on Machine Learning  pages 663–670  2000.

Richard E Nisbett  Kaiping Peng  Incheol Choi  and Ara Norenzayan. Culture and systems of thought:

holistic versus analytic cognition. Psychological review  108(2):291  2001.

Nathan D Ratliff  J Andrew Bagnell  and Martin A Zinkevich. Maximum margin planning. In
Proceedings of the 23rd international conference on Machine learning - ICML ’06  pages 729–736 
2006.

11

John Rawls. A Theory of Justice. Cambridge  Massachusetts: Belknap Press  1971. ISBN 0-674-

00078-1.

Stuart Russell. Rationality and Intelligence: A Brief Update. In Fundamental Issues of Artiﬁcial

Intelligence  pages 7–28. 2016.

Stuart Russell  Daniel Dewey  and Max Tegmark. Research Priorities for Robust and Beneﬁcial

Artiﬁcial Intelligence. AI Magazine  36(4):105  2015.

Paul A Samuelson. Consumption theory in terms of revealed preference. Economica  15(60):243–253 

1948.

J¨urgen Schmidhuber. The speed prior: a new simplicity measure yielding near-optimal computable
predictions. In International Conference on Computational Learning Theory  pages 216–228.
Springer  2002.

Howard Schuman and Jacob Ludwig. The norm of even-handedness in surveys as in life. American

Sociological Review  pages 112–120  1983.

Irene Scopelliti  Carey K Morewedge  Erin McCormick  H Lauren Min  Sophie Lebrecht  and
Karim S Kassam. Bias blind spot: Structure  measurement  and consequences. Management
Science  61(10):2468–2486  2015.

Kyriacos Shiarlis  Joao Messias  and Shimon Whiteson. Rapidly exploring learning trees.

In
Proceedings - IEEE International Conference on Robotics and Automation  pages 1541–1548 
2017.

Paul Slovic and Amos Tversky. Who accepts savage’s axiom? Behavioral science  19(6):368–373 

1974.

Amos Tversky and Daniel Kahneman. Judgment under Uncertainty: Heuristics and Biases. In Utility 
Probability  and Human Decision Making  pages 141–162. Springer Netherlands  Dordrecht  1975.

Paul MB Vitanyi and Ming Li. An introduction to Kolmogorov complexity and its applications 

volume 34. Springer Heidelberg  1997.

David H. Wolpert and William G. Macready. No free lunch theorems for optimization.

Transactions on Evolutionary Computation  1(1):67–82  1997.

IEEE

Eliezer Yudkowsky. Complex value systems in friendly ai. In International Conference on Artiﬁcial

General Intelligence  pages 388–393. Springer  2011.

Brian D Ziebart  Andrew Maas  J Andrew Bagnell  and Anind K Dey. Maximum Entropy Inverse
Reinforcement Learning. In AAAI Conference on Artiﬁcial Intelligence  pages 1433–1438  2008.

12

,Stuart Armstrong
Sören Mindermann