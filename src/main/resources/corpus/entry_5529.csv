2019,The continuous Bernoulli: fixing a pervasive error in variational autoencoders,Variational autoencoders (VAE) have quickly become a central tool in machine learning  applicable to a broad range of data types and latent variable models.  By far the most common first step  taken by seminal papers and by core software libraries alike  is to model MNIST data using a deep network parameterizing a Bernoulli likelihood.  This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0 1] valued  not {0 1} as supported by the Bernoulli likelihood.  Here we show that  far from being a triviality or nuisance that is convenient to ignore  this error has profound importance to VAE  both qualitative and quantitative.  We introduce and fully characterize a new [0 1]-supported  single parameter distribution: the continuous Bernoulli  which patches this pervasive bug in VAE.  This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets  including sharper image samples  and suggests a broader class of performant VAE.,The continuous Bernoulli: ﬁxing a pervasive error in

variational autoencoders

Gabriel Loaiza-Ganem
Department of Statistics

Columbia University

gl2480@columbia.edu

John P. Cunningham
Department of Statistics

Columbia University

jpc2181@columbia.edu

Abstract

Variational autoencoders (VAE) have quickly become a central tool in machine
learning  applicable to a broad range of data types and latent variable models. By far
the most common ﬁrst step  taken by seminal papers and by core software libraries
alike  is to model MNIST data using a deep network parameterizing a Bernoulli
likelihood. This practice contains what appears to be and what is often set aside as a
minor inconvenience: the pixel data is [0  1] valued  not {0  1} as supported by the
Bernoulli likelihood. Here we show that  far from being a triviality or nuisance that
is convenient to ignore  this error has profound importance to VAE  both qualitative
and quantitative. We introduce and fully characterize a new [0  1]-supported  single
parameter distribution: the continuous Bernoulli  which patches this pervasive bug
in VAE. This distribution is not nitpicking; it produces meaningful performance
improvements across a range of metrics and datasets  including sharper image
samples  and suggests a broader class of performant VAE.1

1

Introduction

Variational autoencoders (VAE) have become a central tool for probabilistic modeling of complex 
high dimensional data  and have been applied across image generation [10]  text generation [14] 
neuroscience [8]  chemistry [9]  and more. One critical choice in the design of any VAE is the choice
of likelihood (decoder) distribution  which stipulates the stochastic relationship between latents and
observables. Consider then using a VAE to model the MNIST dataset  by far the most common ﬁrst
step for introducing and implementing VAE. An apparently innocuous practice is to use a Bernoulli
likelihood to model this [0  1]-valued data (grayscale pixel values)  in disagreement with the {0  1}
support of the Bernoulli distribution. Though doing so will not throw an obvious type error  the
implied object is no longer a coherent probabilistic model  due to a neglected normalizing constant.
This practice is extremely pervasive in the VAE literature  including the seminal work of Kingma
and Welling [20] (who  while aware of it  set it aside as an inconvenience)  highly-cited follow up
work (for example [25  37  17  6] to name but a few)  VAE tutorials [7  1]  including those in hugely
popular deep learning frameworks such as PyTorch [32] and Keras [3]  and more.
Here we introduce and fully characterize the continuous Bernoulli distribution (§3)  both as a means
to study the impact of this widespread modeling error  and to provide a proper VAE for [0  1]-valued
data. Before these details  let us ask the central question: who cares?
First  theoretically  ignoring normalizing constants is unthinkable throughout most of probabilistic
machine learning: these objects serve a central role in restricted Boltzmann machines [36  13] 
graphical models [23  33  31  38]  maximum entropy modeling [16  29  26]  the “Occam’s razor”
nature of Bayesian models [27]  and much more.

1Our code is available at https://github.com/cunningham-lab/cb.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Second  one might suppose this error can be interpreted or ﬁxed via data augmentation  binarizing
data (which is also a common practice)  stipulating a different lower bound  or as a nonprobabilistic
model with a “negative binary cross-entropy” objective. §4 explores these possibilities and ﬁnds them
wanting. Also  one might be tempted to call the Bernoulli VAE a toy model or a minor point. Let us
avoid that trap: MNIST is likely the single most widely used dataset in machine learning  and VAE is
quickly becoming one of our most popular probabilistic models.
Third  and most importantly  empiricism; §5 shows three key results: (i) as a result of this error 
we show that the Bernoulli VAE signiﬁcantly underperforms the continuous Bernoulli VAE across
a range of evaluation metrics  models  and datasets; (ii) a further unexpected ﬁnding is that this
performance loss is signiﬁcant even when the data is close to binary  a result that becomes clear by
consideration of continuous Bernoulli limits; and (iii) we further compare the continuous Bernoulli
to beta likelihood and Gaussian likelihood VAE  again ﬁnding the continuous Bernoulli performant.
All together this work suggests that careful treatment of data type – neither ignoring normalizing
constants nor defaulting immediately to a Gaussian likelihood – can produce optimal results when
modeling some of the most core datasets in machine learning.

2 Variational autoencoders

Autoencoding variational Bayes [20] is a technique to perform inference in the model:

Zn ∼ p0(z)

and Xn|Zn ∼ pθ(x|zn)   for n = 1  . . .   N 

(1)
where each Zn ∈ RM is a local hidden variable  and θ are parameters for the likelihood of observables
Xn. The prior p0(z) is conventionally a Gaussian N (0  IM ). When the data is binary  i.e. xn ∈
{0  1}D  the conditional likelihood pθ(xn|zn) is chosen to be B(λθ(zn))  where λθ : RM → RD is a
neural network with parameters θ. B(λ) denotes the product of D independent Bernoulli distributions 
with parameters λ ∈ [0  1]D (in the standard way we overload B(·) to be the univariate Bernoulli or the
product of independent Bernoullis). Since direct maximum likelihood estimation of θ is intractable 
variational autoencoders use VBEM [18]  ﬁrst positing a now-standard variational posterior family:

N(cid:89)

qφ(zn|xn)  with qφ(zn|xn) = N(cid:16)

mφ(xn)  diag(cid:0)s2

φ(xn)(cid:1)(cid:17)

(cid:0)(z1  ...  zn)|(x1  ...  xn)(cid:1) =

qφ

n=1

(2)
where mφ : RD → RM   sφ : RD → RM
+ are neural networks parameterized by φ. Then  using
stochastic gradient descent and the reparameterization trick  the evidence lower bound (ELBO)
E(θ  φ) is maximized over both generative and posterior (decoder and encoder) parameters (θ  φ):

N(cid:88)

E(θ  φ) =

Eqφ(zn|xn)[log pθ(xn|zn)]− KL(qφ(zn|xn)||p0(zn)) ≤ log pθ

n=1

2.1 The pervasive error in Bernoulli VAE

In the Bernoulli case  the reconstruction term in the ELBO is:

(cid:0)(x1  . . .   xN )(cid:1). (3)

(cid:105)

(4)

(cid:104) D(cid:88)

d=1

Eqφ(zn|xn)[log pθ(xn|zn)] = Eqφ(zn|xn)

xn d log λθ d(zn)+(1−xn d) log(1−λθ d(zn))

where xn d and λθ d(zn) are the d-th coordinates of xn and λθ(zn)  respectively. However  critically 
Bernoulli likelihoods and the reconstruction term of equation 4 are commonly used for [0  1]-valued
data  which loses the interpretation of probabilistic inference. To clarify  hereafter we denote the
Bernoulli distribution as ˜p(x|λ) = λx(1 − λ)1−x to emphasize the fact that it is an unnormalized
distribution (when evaluated over [0  1]). We will also make this explicit in the ELBO  writing
E(˜p  θ  φ) to denote that the reconstruction term of equation 4 is being used. When analyzing [0  1]-
valued data such as MNIST  the Bernoulli VAE has optimal parameter values θ∗(˜p) and φ∗(˜p); that
is:

(θ∗(˜p)  φ∗(˜p)) = argmax

E(˜p  θ  φ).

(5)

(θ φ)

2

3 CB: the continuous Bernoulli distribution
In order to analyze the implications of this modeling error  we introduce the continuous Bernoulli  a
novel distribution on [0  1]  which is parameterized by λ ∈ (0  1) and deﬁned by:

X ∼ CB(λ) ⇐⇒ p(x|λ) ∝ ˜p(x|λ) = λx(1 − λ)1−x.

(6)

0 <(cid:82) 1

We fully characterize this distribution  deferring proofs and secondary properties to appendices.
Proposition 1 (CB density and mean): The continuous Bernoulli distribution is well deﬁned  that is 
0 ˜p(x|λ)dx < ∞ for every λ ∈ (0  1). Furthermore  if X ∼ CB(λ)  then the density function

of X and its expected value are:

 2tanh−1(1 − 2λ)

1 − 2λ

2

if λ (cid:54)= 0.5
otherwise

(7)

(8)

p(x|λ) = C(λ)λx(1 − λ)1−x  where C(λ) =

 λ

2λ − 1
0.5

µ(λ)

:= E[X] =

+

1

2tanh−1(1 − 2λ)

if λ (cid:54)= 0.5
otherwise

Figure 1 shows log C(λ)  p(x|λ)  and µ(λ). Some notes warrant mention: (i) unlike the Bernoulli 
the mean of the continuous Bernoulli is not λ; (ii) however  like for the Bernoulli  µ(λ) is increasing
on λ and goes to 0 or 1 when λ goes to 0 or 1; (iii) the continuous Bernoulli is not a beta distribution
(the main difference between these two distributions is how they concentrate mass around the
extrema  see appendix 1 for details)  nor any other [0  1]-supported distribution we are aware of
(including continuous relaxations such as the Gumbel-Softmax [28  15]  see appendix 1 for details);
(iv) C(λ) and µ(λ) are continuous functions of λ; and (v) the log normalizing constant log C(λ) is
well characterized but numerically unstable close to λ = 0.5  so our implementation uses a Taylor
approximation near that critical point to calculate log C(λ) to high numerical precision. Proof: See
appendix 3.

Figure 1: continuous Bernoulli log normalizing constant (left)  pdf (middle) and mean (right).

Proposition 2 (CB additional properties): The continuous Bernoulli forms an exponential family 
has closed form variance  CDF  inverse CDF (which importantly enables easy sampling and the use
of the reparameterization trick)  characteristic function (and thus moment generating function too)
and entropy. Also  the KL between two continuous Bernoulli distributions also has closed form and
C(λ) is convex. Finally  the continuous Bernoulli admits a conjugate prior which we call the C-Beta
distribution. See appendix 2 for details. Proof: See appendix 3.
Proposition 3 (CB Bernoulli limit): CB(λ) λ→0−−−→ δ(0) and CB(λ) λ→1−−−→ δ(1) in distribution; that
is  the continuous Bernoulli becomes a Bernoulli in the limit. Proof: See appendix 3.
This proposition might at a ﬁrst glance suggest that  as long as the estimated parameters are close
to 0 or 1 (which should happen when the data is close to binary)  then the practice of erroneously
applying a Bernoulli VAE should be of little consequence. However  the above reasoning is wrong  as
it ignores the shape of log C(λ): as λ → 0 or λ → 1  log C(λ) → ∞ (Figure 1  left). Thus  if data is
close to binary  the term neglected by the Bernoulli VAE becomes even more important  the exact
opposite conclusion than the naive one presented above.
Proposition 4 (CB normalizing constant bound): C(λ) ≥ 2  with equality if and only if λ = 0.5.
And thus it follows that  for any x  λ  we have log p(x|λ) > log ˜p(x|λ). Proof: See appendix 3.

3

0.00.20.40.60.81.0parameter 0.751.001.251.501.752.002.252.50log C() log normalizing constant0.00.20.40.60.81.0x0.51.01.52.02.5p(x|) density0.10.20.30.40.50.60.70.80.9parameter 0.00.20.40.60.81.0parameter 0.00.20.40.60.81.0() meancontinuous BernoulliBernoulliThis proposition allows us to interpret E(˜p  θ  φ) as a lower lower bound of the log likelihood (§4).
Proposition 5 (CB maximum likelihood): For an observed sample x1  . . .   xN ∼iid CB(λ)  the
maximum likelihood estimator ˆλ of λ is such that µ(ˆλ) = 1
N
Beyond characterizing a novel and interesting distribution  these propositions now allow full analysis
of the error in applying a Bernoulli VAE to the wrong data type.

n xn. Proof: See appendix 3.

(cid:80)

4 The continuous Bernoulli VAE  and why the normalizing constant matters

We deﬁne the continuous Bernoulli VAE analogously to the Bernoulli VAE:

Zn ∼ N (0  IM )

and Xn|Zn ∼ CB (λθ(zn))   for n = 1  . . .   N

(9)
where again λθ : RM → RD is a neural network with parameters θ  and CB(λ) now denotes the
product of D independent continuous Bernoulli distributions. Operationally  this modiﬁcation results
only in a change to the optimized objective; for clarity we compare the ELBO of the continuous
Bernoulli VAE (top)  E(p  θ  φ)  to the Bernoulli VAE (bottom):

xn d log λθ d(zn) + (1 − xn d) log(1 − λθ d(zn)) + log C(λθ d(zn))

(cid:35)

(cid:35)

N(cid:88)
N(cid:88)

n=1

E(p  θ  φ) =

E( ˜p  θ  φ) =

−KL(qφ||p0) + Eqφ

−KL(qφ||p0) + Eqφ

(cid:34) D(cid:88)
(cid:34) D(cid:88)

d=1

xn d log λθ d(zn) + (1 − xn d) log(1 − λθ d(zn))

 

n=1

d=1

Analogously  we denote θ∗(p) and φ∗(p) as the maximizers of the continuous Bernoulli ELBO:

(θ∗(p)  φ∗(p)) = argmax

E(p  θ  φ).

(θ φ)

(10)

Immediately  a number of potential interpretations for the Bernoulli VAE come to mind  some of
which have appeared in literature. We analyze each in turn.

4.1 Changing the data  model or training objective

One obvious workaround is to simply binarize any [0  1]-valued data (MNIST pixel values or oth-
erwise)  so that it accords with the Bernoulli likelihood [24]  a practice that is commonly followed
(e.g. [34  2  28  15  12]). First  modifying data to ﬁt a model  particularly an unsupervised model  is
fundamentally problematic. Second  many [0  1]-valued datasets are heavily degraded by binarization
(see appendices for CIFAR-10 samples)  indicating major practical limitations. Another workaround
is to change the likelihood of the model to a proper [0  1]-supported distribution  such as a beta or a
truncated Gaussian. In §5 we include comparisons against a VAE with a beta distribution likelihood
(we also made comparisons against a truncated Gaussian but found this to severely underperform all
the alternatives). Gulrajani et al. [11] use a discrete distribution over the 256 possible pixel values.
Knoblauch et al. [22] study changing the reconstruction and/or the KL term in the ELBO. While
their main focus is to obtain more robust inference  they provide a framework in which the Bernoulli
VAE corresponds simply to a different (nonprobabilistic) loss. In this perspective  empirical results
must determine the adequacy of this objective; §5 shows the Bernoulli VAE to underperform its
proper probabilistic counterpart across a range of settings. Finally  note that none of these alternatives
provide a way to understand the effect of using Bernoulli likelihoods on [0  1]-valued data.

4.2 Data augmentation

Since the expectation of a Bernoulli random variable is precisely its parameter  the Bernoulli VAE
might (erroneously) be assumed to be equivalent to a continuous Bernoulli VAE on an inﬁnitely
augmented dataset  obtained by sampling binary data whose mean is given by the observed data;
indeed this idea is suggested by Kingma and Welling [20]2. This interpretation does not hold3; it
would result in a reconstruction term as in the ﬁrst line in the equation below  while a correct Bernoulli

2see the comments in https://openreview.net/forum?id=33X9fd2-9FyZd
3see http://ruishu.io/2018/03/19/bernoulli-vae/ for a looser lower bound interpretation

4

VAE on the augmented dataset would have a reconstruction term given by the second line (not equal 
as the order of expectation can not be switched since qφ depends on Xr on the second line):

Ezn∼qφ(zn|xn)

EXn∼B(xn)

Xn d log λθ d(zn) + (1 − Xn d) log λθ d(zn)

(cid:104)
(cid:104)

(cid:104) D(cid:88)
(cid:104) D(cid:88)

d=1

(cid:105)(cid:105)

(cid:105)(cid:105)

(11)

.

(cid:54)= EXn∼B(xn)

Ezn∼qφ(zn|Xn)

Xn d log λθ d(zn) + (1 − Xn d) log λθ d(zn)

4.3 Bernoulli VAE as a lower lower bound

d=1

N(cid:88)

D(cid:88)

Because the continuous Bernoulli ELBO and the Bernoulli ELBO are related by:

E(˜p  θ  φ) +

Ezn∼qφ(zn|xn)[log C(λθ d(zn))] = E(p  θ  φ)

(12)

n=1

d=1

and recalling Proposition 4  since log 2 > 0  we get that E(˜p  θ  φ) < E(p  θ  φ). That is  using
the Bernoulli VAE results in optimizing an even-lower bound of the log likelihood than using the
continuous Bernoulli ELBO. Note however that unlike the ELBO  E(˜p  θ  φ) is not tight even if the
approximate posterior matches the true posterior.

(cid:80)

4.4 Mean parameterization

The conventional maximum likelihood estimator for a Bernoulli  namely ˆλB = 1
n xn  maximizes
˜p(x1  ...  xN|λ) regardless of whether data is {0  1} and [0  1]. As a thought experiment  consider
N
(cid:80)
x1  . . .   xN ∼iid CB(λ). Proposition 5 tells us that the correct maximum likelihood estimator  ˆλCB
n xn  where µ is the CB mean of equation 8. Thus  while using ˆλB is
is such that µ(ˆλCB) = 1
N
incorrect  one can (surprisingly) still recover the correct maximum likelihood estimator via ˆλCB =
µ−1(ˆλB). One might then (wrongly) think that training a Bernoulli VAE  and then subsequently
transforming the decoder parameters with µ−1  would be equivalent to training a continuous Bernoulli
VAE; that is  λθ∗(p) might be equal to µ−1(λθ∗( ˜p)). This reasoning is incorrect: the KL term in
the ELBO implies that λθ∗(p)(zn) (cid:54)= µ−1(xn)  and so too λθ∗( ˜p)(zn) (cid:54)= xn  and as such λθ∗(p) (cid:54)=
µ−1(λθ∗( ˜p)). In fact  our experiments will show that despite this ﬂawed reasoning  applying this
transformation can recover some  but not all  of the performance loss from using a Bernoulli VAE.

5 Experiments

We have introduced the continuous Bernoulli distribution to give a proper probabilistic VAE for
[0  1]-valued data. The essential question that we now address is how much  if any  improvement we
achieve by making this choice.

5.1 MNIST

One frequently noted shortcoming of VAE (and Bernoulli VAE on MNIST in particular) is that
samples from this model are blurry. As noted  the convexity of log C(λ) can be seen as regularizing
sample values from the VAE to be more extremal; that is  sharper. As such we ﬁrst compared samples
from a trained continuous Bernoulli VAE against samples from the MNIST dataset itself  from a
trained Bernoulli VAE  and from a trained Gaussian VAE  that is  the usual VAE model with a decoder
likelihood pθ(x|z) = N (ηθ(z)  σ2
θ (z))  where we use η to avoid confusion with µ of equation 8.
These samples are shown in Figure 2. In each case  as is standard  we show the parameter output
by the generative/decoder network for a given latent draw: λθ∗(p)(z) for the CB VAE  λθ∗( ˜p)(z)
for the B VAE  and ηθ∗ (z) for the N VAE. Qualitatively  the continuous Bernoulli VAE achieves
considerably superior samples vs the Bernoulli or Gaussian VAE  owing to the effect of log C(λ)
pushing the decoder toward sharper images. Further samples are in the appendix. Dai and Wipf [4]
consider why Gaussian VAE produce blurry images; we point out that our work (considering the
likelihood) is orthogonal to theirs (considering the data manifold).

5

data

CB VAE

B VAE

N VAE

Figure 2: Samples from MNIST  continuous Bernoulli VAE  Bernoulli VAE  and Gaussian VAE.

5.2 Warped MNIST datasets

The most common justiﬁcation for the Bernoulli VAE is that MNIST pixel values are ‘close’ to
binary. An important study is thus to ask how the performance of continuous Bernoulli VAE vs the
Bernoulli VAE changes as a function of this ‘closeness.’ We formalize this concept by introducing
a warping function fγ(x) that  depending on the warping parameter γ  transforms individual pixel
values to produce a dataset that is anywhere from fully binarized (every pixel becomes {0  1}) to
fully degraded (every pixel becomes 0.5). Figure 3 shows fγ for different values of γ  and the (rather
uninformative) warping equation appears next to Figure 3.
Importantly  γ = 0 corresponds to an unwarped dataset  i.e.  the original MNIST dataset. Further  note
that negative values of γ warp pixel values towards being more binarized  completely binarizing it in
the case where γ = −0.5  whereas positive values of γ push the pixel values towards 0.5  recovering
constant data at γ = 0.5. We then train our competing VAE models on the datasets induced by
different values of γ and compare the difference in performance as γ changes. Note importantly that 
because γ induces different datasets  performance values should primarily be compared between
VAE models at each γ value; the trend as a function of γ is not of particular interest.



(cid:17)(cid:17)

1(x ≥ 0.5)  if γ = −0.5
x + γ
min
1 + 2γ
γ + (1 − 2γ)x  if γ ∈ [0  0.5]

1  max

(cid:16)

(cid:16)

0 

  if γ ∈ (−0.5  0)

fγ(x) =

Figure 3: fγ for different γ values.

Figure 4 shows the results of various models applied to these different datasets (all values are an
average of 10 separate training runs). The same neural network architectures are used across this
ﬁgure  with architectural choices that are quite standard (detailed in appendix 4  along with training
hyperparameters). The left panel shows ELBO values. In dark blue is the continuous Bernoulli
VAE ELBO  namely E(p  θ∗(p)  φ∗(p)).
In light blue is the same ELBO when evaluated on a
trained Bernoulli VAE: E(p  θ∗(˜p)  φ∗(˜p)). Most importantly  note the γ = 0 values; the continuous
Bernoulli VAE achieves a 300 nat improvement over the Bernoulli VAE. This ﬁnding supports the
previous qualitative ﬁnding and the theoretical motivation for this work: signiﬁcant quantitative gains
are achieved via the continuous Bernoulli model. This ﬁnding remains true across a range of γ (dark
blue above light blue in Figure 4)  indicating that regardless of how ‘close’ to binary a dataset is  the
continuous Bernoulli is a superior VAE model.
One might then wonder if the continuous Bernoulli is outperforming simply because the Bernoulli
needs a mean correction. We thus apply µ−1  namely the map from Bernoulli to continuous Bernoulli
maximum likelihood estimators (equation 8 and §4.4)  and evaluate the same ELBO on µ−1(λθ∗( ˜p)) as
the decoder shown in light red (Figure 4  left). This result  which is only achieved via the introduction
of the continuous Bernoulli  shows two important ﬁndings: ﬁrst  that indeed some improvement over

6

0.00.20.40.60.81.0x0.00.20.40.60.81.0f(x)warping functions0.50.40.30.20.10.00.10.20.30.40.5warping the Bernoulli VAE is achieved by post hoc correction to a continuous Bernoulli parameterization; but
second  that this transformation is still inadequate to achieve the full performance of the continuous
Bernoulli VAE.

Figure 4: Continuous Bernoulli comparisons against Bernoulli VAE. See text for details.

We also note that we ran the same experiment with log likelihood instead of ELBO (using importance
weighted estimates with k = 100 samples; see Burda et al. [2])  and the same results held (up to small
numerical differences; these traces are omitted for ﬁgure clarity). We also ran the same experiment
for the β-VAE [12]  sweeping a range of β values  and the same results held (see appendix 5.1).
To make sure that the discrepancy between the continuous Bernoulli and Bernoulli is not due to
the approximate posterior not being able to adequately approximate the true posterior  we ran the
same experiments with ﬂexible posteriors using normalizing ﬂows [34] and found the discrepancy to
become even larger (see appendix 5.1).
It is natural to then wonder if this performance is an artifact of ELBO and log likelihood; thus  we
also evaluated the same datasets and models using different evaluation metrics. In the middle panel
of Figure 4  we use the inception score [35] to measure sample quality produced by the different
models (higher is better). Once again  we see that including the normalizing constant produces
better samples (dark traces / continuous Bernoulli lie above light traces / Bernoulli). We include
that comparison on both the decoder parameters λ (dark and light green) and also samples from
distributions indexed by those parameters (dark and light orange). One can imagine a variety of other
parameter transformations that may be of interest; we include several in appendix 5.1  where again
we ﬁnd that the continuous Bernoulli VAE outperforms its Bernoulli counterpart.
In the right panel of Figure 4  to measure usefulness of the latent representations of these models  we
compute mφ∗(p)(xn) and mφ∗( ˜p)(xn) (note that m is the variational posterior mean from equation
2 and not the continuous Bernoulli mean) for training data and use a 15-nearest neighbor classiﬁer
to predict the test labels. The right panel of Figure 4 shows the accuracy of the classiﬁers (denoted
K(φ)) as a function of γ. Once again  the continuous Bernoulli VAE outperforms the Bernoulli VAE.
Now that the continuous Bernoulli VAE gives us a proper model on [0  1]  we can also propose
other natural models. Here we introduce and compare against the beta distribution VAE (not β-VAE
[12]); as the name implies  the generative likelihood is Beta(αθ(z)  βθ(z)). We repeated the same
warped MNIST experiments using Gaussian VAE and beta distribution VAE  both including and
ignoring the normalizing constants of those distributions  as an analogy to the continuous Bernoulli
and Bernoulli distributions. First  Figure 5 shows again that that ignoring the normalizing constant
hurts performance in every metric and model (dark above light). Second  interestingly  we ﬁnd that
the continuous Bernoulli VAE outperforms both the beta distribution VAE and the Gaussian VAE in
terms of inception scores  and that the beta distribution VAE dominates in terms of ELBO. This ﬁgure
clariﬁes that the continuous Bernoulli and beta distribution are to be preferred over the Gaussian for
VAE applied to [0  1] valued data  and that ignoring normalizing constants is indeed unwise.
A few additional notes warrant mention on Figure 5. Unlike with the continuous Bernoulli  we should
not expect the Gaussian and beta normalizing constants to go to inﬁnity as extrema are reached  so
we do not observe the same patterns with respect to γ as we did with the continuous Bernoulli. Note
also that the lower lower bound property of ignoring normalizing constants does not hold in general 
as it is a direct consequence of the continuous Bernoulli log normalizing constant being nonnegative.

7

0.40.20.00.20.4warping 02004006008001000120014001600ELBO147011391416ELBO for VAE(p *(p) *(p))(p *(p) *(p))(p *(p) *(p)) with 10.40.20.00.20.4warping 246810inception scoreinception scores of VAEIS dataIS *(p)(z)IS *(p)(z)IS (*(p)(z))IS (*(p)(z))0.40.20.00.20.4warping 0.20.40.60.81.0accuracyknn accuracy of VAE latents(p)(p)Figure 5: Gaussian (top panels) and beta (bottom panels) distributions comparisons between including
and excluding the normalizing constants. Left panels show ELBOs  middle panels inceptions scores 
and right panels 15-nearest neighbors accuracy.

Table 1: Comparisons of training with and without normalizing constants for CIFAR-10. For
connection to the panels in Figures 4 and 5  column headers are colored accordingly.

distribution

CB/B

Gaussian

beta

objective map
E(p  θ  φ)
·
E(˜p  θ  φ)
µ−1
·
E(˜p  θ  φ)
E(p  θ  φ)
·
E(˜p  θ  φ)
·
·
E(p  θ  φ)
E(˜p  θ  φ)
·

E(p  θ∗  φ∗)

1007
916
475
1891
-42411
3121
-38913

IS w/ samples

1.15
1.49
1.41
1.86
1.24
2.98
1.39

IS w/ parameters K(φ∗)
0.43
0.42
0.42
0.42
0.1
0.47
0.1

2.31
4.55
1.39
3.04
1.00
4.07
1.00

5.3 CIFAR-10

We repeat the same experiments as in the previous section on the CIFAR-10 dataset (without common
preprocessing that takes the data outside [0  1] support)  a dataset often considered to be a bad ﬁt
for Bernoulli VAE. For brevity we evaluated only the non-warped data γ = 0  leading to the results
shown in Table 1 (note the colored column headers  to connect to the panels in Figures 4 5). The top
section shows results for the continuous Bernoulli VAE (ﬁrst row  top)  the Bernoulli VAE (third row 
top)  and the Bernoulli VAE with a continuous Bernoulli inverse map µ−1 (second row  top). Across
all metrics – ELBO  inception score with parameters λ  inception score with continuous Bernoulli
samples given λ  and k nearest neighbors – the Bernoulli VAE is suboptimal. Interestingly  unlike
in MNIST  here we see that using the continuous Bernoulli parameter correction µ−1 (§4.4) to a
Bernoulli VAE is optimal under some metrics. Again we note that this is a result belonging to the
continuous Bernoulli (the parameter correction is derived from equation 8)  so even these results
emphasize the importance of the continuous Bernoulli.
We then repeat the same set of experiments for Gaussian and beta distribution VAE (middle and
bottom sections of Table 1). Again  ignoring normalizing constants produces signiﬁcant performance
loss across all metrics. Comparing metrics across the continuous Bernoulli  Gaussian  and beta
sections  we see again that the Gaussian VAE is suboptimal across all metrics  with the optimal
distribution being the continuous Bernoulli or beta distribution VAE  depending on the metric.

8

0.40.20.00.20.4warping 900800700600500ELBOELBO for Gaussian VAE(p *(p) *(p))(p *(p) *(p))0.40.20.00.20.4warping 246810inception scoreinception scores of Gaussian VAEIS dataIS *(p)(z)IS *(p)(z)IS (*(p)(z) 2*(p)(z))IS (*(p)(z) 2*(p)(z))0.40.20.00.20.4warping 0.20.40.60.81.0accuracyknn accuracy of Gaussian VAE latents(p)(p)0.40.20.00.20.4warping 0500100015002000ELBOELBO for beta distribution VAE(p *(p) *(p))(p *(p) *(p))0.40.20.00.20.4warping 246810inception scoreinception scores of beta distribution VAEIS dataIS mean(*(p)(z) *(p)(z))IS mean(*(p)(z) *(p)(z))IS Beta(*(p)(z) *(p)(z))IS Beta(*(p)(z) *(p)(z))0.40.20.00.20.4warping 0.20.40.60.8accuracyknn accuracy of beta distribution VAE latents(p)(p)5.4 Parameter estimation with EM

Finally  one might wonder if the performance improvements of the continuous Bernoulli VAE over
the Bernoulli VAE and its corrected version with µ−1 are merely an artifact of not having access to
the log likelihood and having to optimize the ELBO instead. In this section we show  empirically 
that the answer is no. We consider estimating the parameters of a mixture of continuous Bernoulli
distributions  of which the VAE can be thought of as a generalization with inﬁnitely many components.
We proceed as follows: We randomly set a mixture of continuous Bernoulli distributions  ptrue  with
K components in 50 dimensions (independent of each other) and sample from this mixture 10000
times in order to generate a simulated dataset. We then use the EM algorithm [5] to estimate the
mixture coefﬁcients and the corresponding continuous Bernoulli parameters  ﬁrst using a continuous
Bernoulli likelihood (correct)  and second using a Bernoulli likelihood (incorrect). We then measure
how close the estimated parameters are from ground truth. To avoid a (hard) optimization problem
over permutations  we measure closeness with KL divergence between the true distribution ptrue and
the estimated pest.
The results of performing the procedure described above 10 times and averaging the KL values
(over these 10 repetitions)  along with standard errors  are shown in Figure 6. First  we can see that
when using the correct continuous Bernoulli likelihood  the EM algorithm correctly recovers the true
distribution. We can also see that  as the number of mixture components K gets larger  ignoring the
normalizing constant results in worse performance  even after correcting with µ−1  which helps but
does not fully remedy the situation (except at K = 1  as noted in §4.4).

Figure 6: Bias of the EM algorithm to estimate CB parameters when using a CB likelihood (dark
blue)  B likelihood (light blue) and a B likelihood plus a µ−1 correction (light red).

6 Conclusions

In this paper we introduce and characterize a novel probability distribution – the continuous Bernoulli
– to study the effect of using a Bernoulli VAE on [0  1]-valued intensity data  a pervasive error in
highly cited papers  publicly available implementations  and core software tutorials alike. We show
that this practice is equivalent to ignoring the normalizing constant of a continuous Bernoulli  and
that doing so results in signiﬁcant performance decrease in the qualitative appearance of samples
from these models  the ELBO (approximately 300 nats)  the inception score  and in terms of the latent
representation (via k nearest neighbors). Several surprising ﬁndings are shown  including: (i) that
some plausible interpretations of ignoring a normalizing constant are in fact wrong; (ii) the (possibly
counterintuitive) fact that this normalizing constant is most critical when data is near binary; and (iii)
that the Gaussian VAE often underperforms VAE models with the appropriate data type (continuous
Bernoulli or beta distributions).
Taken together  these ﬁndings suggest an important potential role for the continuous Bernoulli
distribution going forward. On this point  we note that our characterization of the continuous
Bernoulli properties (such as its ease of reparameterization  likelihood evaluation  and sampling)
make it compatible with the vast array of VAE improvements that have been proposed in the literature 
including ﬂexible posterior approximations [34  21]  disentangling [12]  discrete codes [28  15] 
variance control strategies [30]  and more.

9

051015202530mixture components K02468101214KL(ptrue||pest)bias of likelihood for mixture plus 1Acknowledgments

We thank Yixin Wang  Aaron Schein  Andy Miller  and Keyon Vafa for helpful conversations  and the
Simons Foundation  Sloan Foundation  McKnight Endowment Fund  NIH NINDS 5R01NS100066 
NSF 1707398  and the Gatsby Charitable Foundation for support.

References
[1] Pytorch VAE turotial:

Keras VAE turotial:
html.

https://github.com/pytorch/examples/tree/master/vae 
https://blog.keras.io/building-autoencoders-in-keras.

[2] Y. Burda  R. Grosse  and R. Salakhutdinov. Importance weighted autoencoders. arXiv preprint

arXiv:1509.00519  2015.

[3] F. Chollet et al. Keras. https://keras.io  2015.
[4] B. Dai and D. Wipf. Diagnosing and enhancing vae models. In International Conference on

Learning Representations  2019.

[5] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological)  39(1):
1–22  1977.

[6] N. Dilokthanakul  P. A. Mediano  M. Garnelo  M. C. Lee  H. Salimbeni  K. Arulkumaran  and
M. Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders.
arXiv preprint arXiv:1611.02648  2016.

[7] C. Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908  2016.
[8] Y. Gao  E. W. Archer  L. Paninski  and J. P. Cunningham. Linear dynamical neural population
models through nonlinear embeddings. In Advances in neural information processing systems 
pages 163–171  2016.

[9] R. Gómez-Bombarelli  J. N. Wei  D. Duvenaud  J. M. Hernández-Lobato  B. Sánchez-Lengeling 
D. Sheberla  J. Aguilera-Iparraguirre  T. D. Hirzel  R. P. Adams  and A. Aspuru-Guzik. Auto-
matic chemical design using a data-driven continuous representation of molecules. ACS central
science  4(2):268–276  2018.

[10] K. Gregor  I. Danihelka  A. Graves  D. Rezende  and D. Wierstra. Draw: A recurrent neural
In International Conference on Machine Learning  pages

network for image generation.
1462–1471  2015.

[11] I. Gulrajani  K. Kumar  F. Ahmed  A. A. Taiga  F. Visin  D. Vazquez  and A. Courville.
Pixelvae: A latent variable model for natural images. In International Conference on Learning
Representations  2017.

[12] I. Higgins  L. Matthey  A. Pal  C. Burgess  X. Glorot  M. Botvinick  S. Mohamed  and A. Ler-
chner. beta-vae: Learning basic visual concepts with a constrained variational framework. In
International Conference on Learning Representations  volume 3  2017.

[13] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

computation  14(8):1771–1800  2002.

[14] Z. Hu  Z. Yang  X. Liang  R. Salakhutdinov  and E. P. Xing. Toward controlled generation of
text. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 1587–1596. JMLR. org  2017.

[15] E. Jang  S. Gu  and B. Poole. Categorical reparameterization with gumbel-softmax.

International Conference on Learning Representations  2017.

In

[16] E. T. Jaynes. Information theory and statistical mechanics. Physical review  106(4):620  1957.
[17] Z. Jiang  Y. Zheng  H. Tan  B. Tang  and H. Zhou. Variational deep embedding: an unsupervised
and generative approach to clustering. In Proceedings of the 26th International Joint Conference
on Artiﬁcial Intelligence  pages 1965–1972. AAAI Press  2017.

[18] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul. An introduction to variational

methods for graphical models. Machine learning  37(2):183–233  1999.

10

[19] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

Conference on Learning Representations  2015.

In International

[20] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference

on Learning Representations  2014.

[21] D. P. Kingma  T. Salimans  R. Jozefowicz  X. Chen  I. Sutskever  and M. Welling. Improved
In Advances in neural information

variational inference with inverse autoregressive ﬂow.
processing systems  pages 4743–4751  2016.

[22] J. Knoblauch  J. Jewson  and T. Damoulas. Generalized variational inference. arXiv preprint

arXiv:1904.02063  2019.

[23] D. Koller  N. Friedman  and F. Bach. Probabilistic graphical models: principles and techniques.

MIT press  2009.

[24] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In Proceedings of
the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  pages 29–37 
2011.

[25] A. B. L. Larsen  S. K. Sønderby  H. Larochelle  and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In International Conference on Machine Learning  pages
1558–1566  2016.

[26] G. Loaiza-Ganem  Y. Gao  and J. P. Cunningham. Maximum entropy ﬂow networks.

International Conference on Learning Representations  2017.

In

[27] D. J. MacKay. Information theory  inference and learning algorithms. Cambridge university

press  2003.

[28] C. J. Maddison  A. Mnih  and Y. W. Teh. The concrete distribution: A continuous relaxation of

discrete random variables. In International Conference on Learning Representations  2017.

[29] R. Malouf. A comparison of algorithms for maximum entropy parameter estimation. In pro-
ceedings of the 6th conference on Natural language learning-Volume 20  pages 1–7. Association
for Computational Linguistics  2002.

[30] A. Miller  N. Foti  A. D’Amour  and R. P. Adams. Reducing reparameterization gradient

variance. In Advances in Neural Information Processing Systems  pages 3708–3718  2017.

[31] K. P. Murphy  Y. Weiss  and M. I. Jordan. Loopy belief propagation for approximate inference:
An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artiﬁcial
intelligence  pages 467–475. Morgan Kaufmann Publishers Inc.  1999.

[32] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 

L. Antiga  and A. Lerer. Automatic differentiation in pytorch. In NIPS-W  2017.

[33] J. Pearl. Reverend Bayes on inference engines: A distributed hierarchical approach. Cognitive
Systems Laboratory  School of Engineering and Applied Science  University of California  Los
Angeles  1982.

[34] D. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In International

Conference on Machine Learning  pages 1530–1538  2015.

[35] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved
techniques for training gans. In Advances in neural information processing systems  pages
2234–2242  2016.

[36] P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory.

Colorado Univ at Boulder Dept of Computer Science  1986.

[37] C. K. Sønderby  T. Raiko  L. Maaløe  S. K. Sønderby  and O. Winther. Ladder variational
autoencoders. In Advances in neural information processing systems  pages 3738–3746  2016.
[38] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

11

,Gabriel Loaiza-Ganem
John Cunningham