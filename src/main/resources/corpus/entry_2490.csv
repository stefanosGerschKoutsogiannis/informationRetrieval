2011,How biased are maximum entropy models?,Maximum entropy models have become popular statistical models in neuroscience and other areas in biology  and can be useful tools for obtaining estimates of mu- tual information in biological systems. However  maximum entropy models fit to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class  the bias is equal to the number of parameters divided by twice the number of observations. However  in practice  the true distribution is usually outside the model class  and we show here that this misspecification can lead to much larger bias. We provide a perturba- tive approximation of the maximally expected bias when the true model is out of model class  and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data.,How biased are maximum entropy models?

Jakob H. Macke

Gatsby Computational Neuroscience Unit

University College London  UK
jakob@gatsby.ucl.ac.uk

Iain Murray

School of Informatics

University of Edinburgh  UK

i.murray@ed.ac.uk

Peter E. Latham

Gatsby Computational Neuroscience Unit

University College London  UK
pel@gatsby.ucl.ac.uk

Abstract

Maximum entropy models have become popular statistical models in neuroscience
and other areas in biology  and can be useful tools for obtaining estimates of mu-
tual information in biological systems. However  maximum entropy models ﬁt to
small data sets can be subject to sampling bias; i.e. the true entropy of the data can
be severely underestimated. Here we study the sampling properties of estimates
of the entropy obtained from maximum entropy models. We show that if the data
is generated by a distribution that lies in the model class  the bias is equal to the
number of parameters divided by twice the number of observations. However  in
practice  the true distribution is usually outside the model class  and we show here
that this misspeciﬁcation can lead to much larger bias. We provide a perturba-
tive approximation of the maximally expected bias when the true model is out of
model class  and we illustrate our results using numerical simulations of an Ising
model; i.e. the second-order maximum entropy distribution on binary data.

1

Introduction

Over the last several decades  information theory [1  2] has played a major role in our effort to
understand the neural code in the brain [3  4]. Its usefulness  however  is limited by the fact that
the quantity of interest  mutual information (typically between stimuli and neuronal responses) is
hard to compute from data [5]. Consequently  although this approach has led to a relatively deep
understanding of neural coding in single neurons [4]  it has told us far less about populations [6  7].
In essence  the brute-force approaches to measuring mutual information that have worked so well on
single spike trains simply do not work on populations. This is because the key-ingredient of mutual
information is the entropy  and in general  estimation of the entropy from ﬁnite data sets suffers
from a severe downward bias [8  9]: on average  the entropy estimated on the data set will be lower
than the actual entropy of the underlying model. While a number of improved estimators have been
developed (see [5  10] for an overview)  the amount of data one needs is  ultimately  exponential in
the number of neurons  so even modest populations (tens of neurons) are out of reach.
To apply information-theoretic techniques to populations  then  our only hope is to develop models in
which the number of unconstrained parameters grows (relatively) slowly with the number of neurons
[11]. For such models  estimating information requires much less data than brute force methods.
Still  the amount of data is nontrivial  and naive estimators of information can be badly biased. Here
we consider one class of models – maximum entropy models subject to linear constraints – and
compute the bias in the entropy. We show that if the true distribution lies in the parametric model
class  then the bias is equal to the number of parameters divided by twice the number of observations.
When the true distribution is outside the model class  however  the bias can be much larger.

1

We illustrate our results using a very popular model in neuroscience  the Ising model [12]  which
is the second-order maximum entropy distribution on binary data. Recently  this model has become
a popular means of characterizing the distribution of ﬁring patterns in multi-electrode recordings 
and has been used extensively in a wide range of applications  including recordings in the retina
[13  14] and visual cortex [15]. In addition  several recent studies [16  17  18] have used numerical
simulations of large Ising models to understand the scaling of the entropy of the model with popula-
tion size. And  ﬁnally  Ising models have been used in other ﬁelds in biology  for example to model
gene-regulation networks [19].

2 Theory

2.1 Maximum entropy models

Our starting point is an underlying true distribution  denoted p(x) where x is a (typically real valued)
vector; the goal is to model it with a maximum entropy distribution. For simplicity  when developing
the formalism we take x to be discrete; however  all our results apply to continuous variables.
The maximum entropy distribution is the distribution with the highest entropy subject to a set of
constraints  where the entropy is given by

p(x) log p(x) .

(1)

Speciﬁcally  suppose that under the true distributions a set of m functions  denoted gi(x)  i =
1  ...  m  average to µi 

p(x)gi(x) .

(2)

S = −￿x
µi =￿x
￿x

(4)

(5)

If we use q(x|µ) to denote the maximum entropy distribution (with µ ≡ (µ1  µ2  ...  µm))  the
constraints (here taken to be linear in the probability) are of the form
(3)

q(x|µ)gi(x) = µi .

Finding an explicit expression for q(x|µ) is a straightforward optimization problem (see  e.g.  [2]).
It can be shown that the maximum entropy distribution is in the exponential family 

where the parameters  λi (the Lagrange multipliers of the optimization problem)  are chosen such
that the constraints in Eq. (2) are satisﬁed. The partition function  Z(µ)  ensures that the probabili-
ties normalize to one 

q(x|µ) =

exp [￿m

i=1 λi(µ)gi(x)]
Z(µ)

Z(µ) =￿x

exp￿ m￿i=1

λi(µ)gi(x)￿ .

Once we have identiﬁed the parameters of this model  we can insert Eq. (4) into Eq. (1)  which
allows us to write the entropy in the form

Sq(µ) = log Z(µ) −

λi(µ)µi .

m￿i=1

(6)

2.2 Estimation bias in maximum entropy models

So far we have assumed that the true µi are known. In general  though  we have to estimate the
µi from data. Speciﬁcally  if we have K observations of x  denoted x(k)  k = 1  ...  K  then the
estimate of µi  denoted ˆµi  is given by

ˆµi =

1
K

K￿k=1

gi￿x(k)￿ .

2

(7)

We can still use the maximum entropy formulation described above; the only difference is that we
replace µ by ˆµ. Thus  the maximum entropy distribution is given by q(x| ˆµ) (Eq. (4)) and the
entropy by Sq( ˆµ) (Eq. (6)).
Because of sampling error  the ˆµi are not equal to their true values  µi; consequently  neither is
Sq( ˆµ). This leads to variability  in the sense that different sets of x(k) lead to different entropies
and  because the entropy is concave  to bias. Thus  the entropy estimated from a ﬁnite data set will
be lower  on average  than the entropy obtained from the true underlying model. In the large K limit 
so that ˆµi is close to µi  the bias can be computed by Taylor expanding around Sq(µ) and averaging
over the true distribution  p(x). Anticipating somewhat our result  we use −b/2K to denote the
bias  and we have

b
2K ≡ ￿Sq( ˆµ) − Sq(µ)￿p(x) =

−

∂Sq(µ)

∂µi

￿δµi￿p(x) +

m￿i=1

1
2

m￿i j=1

∂2Sq(µ)
∂µi∂µj ￿δµiδµj￿p(x) + ...
(8)

where

δµi ≡ ˆµi − µi =

1
K

K￿k=1

gi￿x(k)￿ − µi .

The angle brackets with subscript p(x) indicate an average with respect to the true distribution 
p(x). The quantity we focus on is b  the normalized bias (as it is independent of K in the large K
limit). Computing the averages and derivatives in Eq. (8) is straightforward (see Appendix A in the
supplementary material for details)  and we ﬁnd that  through second order in δµ 

(9)

(10)

(11a)
(11b)

(12)

b =￿ij

Cq−1
ij Cp
ji 

where

Here Cq−1

ij

Cq
ij ≡ ￿δgi(x)δgj(x)￿q(x|µ)
Cp
ij ≡ ￿δgi(x)δgj(x)￿p(x).

denotes the ijth entry of Cq−1 and

δgi(x) ≡ gi(x) − µi .

2.3 Bias when the true model is in the model class

Equation (10) tells us the normalized bias (to ﬁrst order in 1/K). Evaluating it is  typically  hard  but
there is one case in which we can write down an explicit expression for it: when the true distribution
lies in the model class  so that p(x) = q(x|µ). In that case  Cq = Cp  the normalized bias is
the trace of the identity matrix  and we have b = m (recall that m is the number of constraints);
alternatively  Bias[S] = −m/2K.
An important within-model-class case arises when x is discrete and the “parametrized” model is a
direct histogram of the data. If x can take on D values  then there are D − 1 parameters (the “−1”
comes from the fact that p(x) must sum to 1) and the normalized bias is (D − 1)/2K. We thus re-
cover a general version of the Miller–Madow [8] or Panzeri & Treves bias correction [9]  which was
derived for a multinomial distribution. (Note that our expression differs from theirs by a factor of
log 2; that’s because they use base 2 logarithms whereas we use natural logarithms.) Alternatively 
one can exploit the relationship between entropy-maximization and maximum-likelihood estima-
tion in the exponential family to deduce this result from the asymptotic distribution of maximum
likelihood estimators [20]. For details see Appendix B in the supplementary material.

2.4 Bias when the true model is not in the model class

In practice  it is rare for the true distribution to lie in the model class  so it is important to know
how the normalized bias behaves in general. In this section  we investigate how quickly it changes
when we leave the model class. We concentrate on the worst case scenario and determine the largest
normalized bias that is consistent with a given “distance” from the true model class. For cases in
which we are close to the true model class  we provide a perturbative expression for this quantity.

3

To assess the normalized bias out of model class  we assume that p(x)  the distribution from which
the data was generated  can be written as

(13)

(14)

p(x) = q(x|µ) + δp(x)

in turn implies that

with δp(x) chosen so that it is orthogonal to all the constraints; that is￿x δp(x)gi(x) = 0  which

p(x)gi(x) =￿x
(and both  of course  are equal to µi). We then ask how the normalized bias behaves as δp(x) varies.
Because q(x|µ) is independent of δp(x)  so is Cq
ij  and the normalized bias  b  that appears in
Eq. (10) can be written (using Eq. (11b))

q(x|µ)gi(x)

￿x

where

b = ￿B(x)￿p(x)

B(x) ≡￿ij

δgi(x)Cq−1

ij δgj(x) .

(15)

(16)

It’s not possible to say anything deﬁnitive about the normalized bias in general  but what we can
do is compute its maximum as a function of the distance between p(x) and q(x|µ)  with “distance”
measured by the Kullback–Leibler divergence. The latter quantity  denoted ∆S  is given by

∆S =￿x

p(x) log

p(x)
q(x|µ)

= Sq(µ) − Sp

(17)

where Sp is the entropy of p(x). The second equality follows from the deﬁnition of q(x|µ)  Eq. (4) 
and the fact that ￿gi(x)￿p(x) = ￿gi(x)￿q(x|µ)  which comes from Eq. (14).
We are interested in ﬁnding the maximal normalized bias that is consistent with a given ∆S. Rather
than maximizing the normalized bias at ﬁxed ∆S  we take the complementary approach: For each
possible bias  we ﬁnd the minimal possible ∆S. This gives us a relationship between bias and
minimal ∆S  which we can invert to obtain the maximal bias for a given ∆S. Since Sq(µ) is
independent of p(x)  minimizing ∆S is equivalent to maximizing Sp (see Eq. (17)). Thus  again we
have a maximum entropy problem. Now  though  we have an additional constraint on the normalized
bias  which gives us an additional Lagrange multiplier in addition to the λi we had for the original
optimization problem. This leads to (in analogy to Eq. (4))

p(x|µ β ) =

exp [βB(x) +￿i λi(µ β )gi(x)]

Z(µ β )

(18)

where Z(µ β ) is the partition function and the λi(µ β ) are chosen to satisfy Eq. (2)  but with p(x)
replaced by p(x|µ β ). Amongst all models that satisfy the moments constraints and have the same
normalized bias  this is the one that is closest (in KL–divergence) to the maximum entropy model.
Note that we have slightly abused notation: whereas in the previous sections the λi and Z depended
only on µ  they now depend on both µ and β. However  the previous variables are closely related to
the new ones: when β = 0 the constraint associated with b disappears  and we recover q(x|µ); that
is  p(x|µ  0) = q(x|µ). Consequently  λi(µ  0) = λi(µ)  and Z(µ  0) = Z(µ).
Relating ∆S to b is now a purely numerical task: choose a set of µi and a normalized bias  b 
determine the Lagrange multipliers  λi(µ β ) and β  that appear in Eq. (18)  then compute Sp the
entropy of p(x|µ β )  and subtract that from Sq(µ) to ﬁnd ∆S (see Eq. (17)). In section 3.2 we do
exactly that. First  however  to gain some intuition into how the normalized bias depends on ∆S  we
compute the relationship between the two perturbatively. This can be done by considering the small
β limit. In this limit we can expand both ∆S and b as a Taylor series in β. Deﬁning

(19)
where Sp(β) is the entropy of p(x|µ β )  and using primes to denote derivatives with respect to β 
we have  through second order in β 

∆S(β) ≡ Sq(µ) − Sp(β)

∆S(β) = Sq(µ) − Sp(0) − βS￿p(0) −

b(β) = b(0) + βb￿(0) .

β2
2

S￿￿p (0)

(20a)
(20b)

4

We expand ∆S(β) to second order in β because S￿p(0) = 0  which follows from the fact that when
β ￿= 0 there is an additional constraint on the normalized bias  and so any β ￿= 0 can only lower
the entropy; therefore  β = 0 must be a local maximum. Alternatively  a straightforward calculation
in which we write down the entropy of p(x|µ β ) using Eq. (18) (which results in an expression
analogous to Eq. (6)) and differentiate with respect to β  yields
(21)
From this it follows that S￿p(0) = 0; in addition  we see that S￿￿p (0) = −b￿(0). Thus  using the fact
that when β = 0  p(x|µ  0) is within the model class  so Sp(0) = Sq(µ)  Eq. (20) tells us that when
β is sufﬁciently small 

S￿p(β) = −βb￿(β) .

∆S =

(b − m)2
2b￿(0)

.

(22)

The term in the denominator  b￿(0)  is relatively easy to compute  and we show in Appendix C (in
the supplementary material) that it is given by

b￿(0) = Var[B]q(x|µ) −

m￿i j=1

￿B(x)δgi(x)￿q(x|µ)Cq−1

ij

￿δgj(x)B(x)￿q(x|µ) .

(23)

The key result of the perturbative analysis is that when the true distribution is out of the model class 
the normalized bias can be increased by a term proportional to b￿(0)1/2. Thus  the size of b￿(0) is
crucial for telling us how big the bias really is. In the next section we investigate this numerically
for a particular model  the Ising model.

3 Numerical Results: Estimation bias in Ising models

For our numerical simulations  we consider the second order maximum entropy model on n binary
variables  also known as the Ising model [12] (see [13  14] for an application of Ising models to
neuroscience). In this section  we use numerical studies to verify that the asymptotic bias gives
an accurate characterization of the expected bias for relevant sample-sizes K  investigate the size
of the normalized bias when the true model is not in the model class  and study the scaling of the
normalized bias with the number of parameters. We show numerically that  for the Ising model  the
model-misspeciﬁcation can result in the normalized bias increasing rapidly with population size.

3.1 Estimation in a binary maximum entropy model
We consider n interacting spins si  i = 1  ...  n with si ∈{ 0  1}. We put constraints on the ﬁrst and
second moments only  so m  the number of constraints  is n(n + 1)/2: gi(s) = si and gij(s) =
sisj  i < j. The maximum entropy model (with the λi’s replaced by hi and Jij and the gi written
explicitly) has the form

(24)

q(s|h  J) =

1

Z(h  J)

exp￿i

hisi +￿i<j

siJijsj .

To illustrate our results for the asymptotic bias  and to investigate how large K has to be for the
asymptotic calculation to be relevant  we performed the following simulations: For different values
of K (ranging from 10 to 104) and different values of the model-size n ∈{ 2  3  5  10  15}  we
generated 104 data sets of size K each from an independent binary model with n variables and
mean µ = 0.1 or µ = 0.5  i.e. sampling from the distribution given in Eq. (24) with Jij = 0 and
hi = log(µ/(1 − µ)). For each such data set  we ﬁt a pairwise binary maximum entropy model
to the data by gradient-ascent on the (log-concave) likelihood. By calculating the entropy of the
resulting model (via Eq. (6)) and averaging over the 104 data sets  we obtained a numerical estimate
of the difference between the true entropy and the expected estimated entropy; i.e. the bias.
Figure 1 shows (aside from the reassuring fact that our asymptotic calculations are consistent with
the numerical simulations) that the asymptotic solution gives surprisingly accurate results even for
relatively low values of K. From ﬁgures 1B and D  we can see that  for values of K of around
100  the numerical biases already lie very close to the asymptotic prediction. Since the asymptotics
are accurate for large K  we expect this ﬁt to remain close. While we did observe some deviations

5

for very large data sets for which the bias is very small (K > 103)  such deviations could be a
consequence of numerical errors in the ﬁtting-procedure.
We note that our choice of Jij = 0 is merely for concreteness  and that the validity of our formulation
is not dependent on the values of Jij. We also performed simulations with models in which Jij is
non-zero and drawn from a Gaussian distribution  which yielded qualitatively similar results.

A)

100

10−2

s
a
b

i

 
)
e
v
i
t

a
g
e
N

(

10−4

 
101

C)

100

10−2

i

s
a
b
 
)
e
v
i
t
a
g
e
N

(

10−4

101

n=2
3
5
10
15
Asymp

B)

 

i

 

s
a
B
d
e
a
c
s
e
R

l

102
103
Sample size K

104

D)

i

 

s
a
B
d
e
a
c
s
e
R

l

1
0.8
0.6
0.4
0.2
0
10

1.5

1

0.5

25

50 100
Sample size K

250

1000

102
103
Sample size K

104

0
10

25

50

100
Sample size K

250

Figure 1: Asymptotic bias in Ising models. A) Comparison of asymptotic bias with expected bias
calculated via simulations of an independent model with a mean of 0.5 (see text). The thin-black
lines correspond to the bias as predicted by our asymptotic calculation. We have here inverted the
sign of the bias  the actual biases are negative numbers. B) Same data as in A  but on a semi-
log plot to illustrate how many samples are necessary for the asymptotic bias to be an accurate
representation of the actual bias: For the parameters used here  the bias seems to be accurate even
for small (< 100) values of K. We rescaled the estimated biases of each population size n such that
the predicted asymptotic biases (thin black lines) are on top of each other  and such that the biases
are positive. C and D) Same as in A and B  but for an independent model with mean 0.1. Error bars
show standard errors on the mean estimates from 104 simulated data sets.

3.2 Estimation bias when the data has higher-order correlations

What happens when the true model is not in the model class? To investigate this question  we
ﬁrst consider homogeneous pairwise maximum entropy models (hi = h and Jij = J) of sizes
n ∈{ 5  10  15}  common means ￿si￿ = 0.5 or 0.1  and pairwise correlation-coefﬁcient ρi j = 0.1
for each pair i  j. For a range of normalized biases  we calculated ∆S  the maximum entropy
difference between g(x|µ) and an out of model class distribution as a function of normalized bias 
b. For very small or large normalized biases  the optimization did not converge to values moment
constraints  indicating that such an extreme normalized bias would be inconsistent with the speciﬁed
second order moments. The results are shown in Fig. 2  along with the perturbative predictions. For
these choices of parameters  the maximum and minimum normalized bias did not deviate much from
the within-model-class case. In the next example  we illustrate that the deviation can be very large.
To get a better understanding of the additional bias (or  potentially  reduction in bias) due to model
misspeciﬁcation  we studied the bias of the Dichotomized Gaussian distribution  which can be in-
terpreted as a very simple model of neural population activity in which correlations among neurons
are induced by common  Gaussian inputs into threshold neurons [21  22]. In this case we simply
set p(x) to a Dichotomized Gaussian  and numerically computed the bias and the KL–divergence
between p(x) and the maximum entropy model with the same ﬁrst and second moments. We did

6

 

 

30

20

10

0

30

20

10

12
16
Normalized bias

14

N=5

Predicted
Exact

40

30
60
Normalized bias

50

N=10

n
i
(
 

2

S

 
/

S
Δ

 

)
t
n
e
c
r
e
p
 
n
i
(
 

S

 
/

S
Δ

 

2

5

0

 

15

10

5

0

 

N=5

Predicted
Exact

15

10

)
t

n
e
c
r
e
p

 

N=10

N=15

25
20
15
10
5
0

20

15

10

5

0

60 80 100 120 140
Normalized bias

N=15

118 120 122 124 126

Normalized bias

14.8

15

15.2

Normalized bias

0

54

55

56

Normalized bias

57

Figure 2: Bias in the case of model misspeciﬁcation. Top row: ∆S/S2  where S2 is the entropy of
the second order model  as a function of the normalized bias for a model with means ￿si￿ = 0.5 and
correlation-coefﬁcient 0.1. The red (dashed) lines show the exact ∆S calculated by using equation
(18)  and the green (solid) lines using the perturbative expansion in equation (22). The curves end
because for normalized biases too large or too small the optimization does not converge to values
which satisfy the moment constraints. Bottom row: Same as top row  but using means of ￿si￿ = 0.1.
this for means set to ￿si￿ = 0.02  a realistic value for applications of maximum entropy models
in neuroscience  and different values of the pairwise correlation coefﬁcient ρ ∈{ 0.02  0.1  0.5}.
We also included  for comparison  the normalized bias for a within model class distribution (i.e. a
maximum entropy model with matched ﬁrst and second moments)  which is just n(n + 1)/2.
For the Dichotomized Gaussian  the normalized bias was substantially larger than the within model
class bias. For example  for population size n = 15  its bias is 2.3 times larger for ρ = 0.1  and 6.8
times larger for ρ = 0.5. Figure 3B shows ∆S versus population size for the models in Fig. 3A 
and the corresponding “maximally biased” model; i.e. the model which has the same normalized
bias as the Dichotomized Gaussian  but minimal ∆S. Interestingly  ∆S for the maximally biased
models (equation (18)) is very similar to ∆S for the Dichotomized Gaussian. This suggests that our
extremal calculation of the bias is relevant for a reasonably mechanistic model of neural population
activity.

4 Conclusions

In recent years  there has been a resurgence of interest in maximum entropy models in neuroscience
and related ﬁelds [13  14  15]. In particular  maximum entropy models can be useful for model-based
estimation of the information content of neural populations [11]  as direct information-estimates do
not scale well for large population sizes. In this paper  we studied estimation biases in the entropy of
maximum entropy models. We focused on “naive” estimators  i.e. estimators of the entropy which
simply calculate it from the empirical estimates of the probabilities of the model  and do not attempt
to do any bias reduction.
We found that if the true model is in the model class  the (downward) bias in a maximum entropy
estimate from ﬁnite observations is proportional to the ratio of the number of parameters to the
number of observations  a relationship which is identical to that of the (naive) histogram estimators
[8  9]. However  we also show that if the model is misspeciﬁed (i.e. if the true data do not come

7

A)

1000

i

s
a
b
 
d
e
z

i
l

a
m
r
o
N

800

600

400

200

0

 

MaxEnt model
DG ρ= .02
DG ρ= .1
DG ρ= .5

5

10

Population size

15

B)

 

0.2

0.15

S
Δ

 

0.1

0.05

0

 

 

DG ρ= 0.02
Min ρ= 0.02
DG ρ= 0.1
Min ρ= 0.1
DG ρ= 0.5
Min ρ= 0.5

5

10

Population size

15

Figure 3: Bias in the case of model misspeciﬁcation  using the Dichotomized Gaussian. A) Scal-
ing of the normalized bias with population size. The normalized bias of the Dichotomized Gaussian
(DG) is much larger than that of the maximum entropy model. B) Distance from model class  ∆S 
versus population size for the Dichotomized Gaussian and maximum entropy models. They are
about the same  indicating that the Dichotomized Gaussian model has close to maximum bias.

from the speciﬁed exponential family model)  then the bias can be much larger. We numerically
investigated the bias in second-order binary maximum entropy models (also known as Ising models) 
and showed that in this case  model misspeciﬁcation can lead to substantially bigger biases.
Non-parametric estimation of entropy is a well researched subject  and various estimators with op-
timized properties have been proposed (see e.g. [5  23]). A number of studies have looked at the
entropy estimation for the multivariate normal distribution [24  25  26  27] and other continuous
distributions  and improved estimators for the Gaussian distribution have been described [28]. As
the (differential) entropy of a Gaussian distribution is essentially its log-determinant  the bias of
this model can be related to results about the eigenvalues of random matrices [29]. An overview of
estimators of the entropy of continuous-valued distributions is given in [30].
However  to our knowledge  the entropy bias of maximum entropy models in the presence of model-
misspeciﬁcation has not be characterized or studied numerically. We provided here an asymptotic
derivation of this bias  and studied it numerically for the pairwise binary maximum entropy model 
the Ising model. Our characterization of the bias relates the (worst case) bias in the case of model-
misspeciﬁcation to the distance (as measured by KL–divergence) between the model and the actual
data. This characterization does not yield a precise estimate of the bias on a given data-set which
could simply be ‘subtracted-off’– thus  our derivation does not directly yield an improved estimator
of the bias for such data-sets. However  importantly  our results show that model-misspeciﬁcation
can indeed lead to additional bias which can be much larger than generally appreciated. Using
numerical simulations  we showed that this also happens for a realistic model which shares many
properties with neural recordings. In addition  our results could be useful for deriving general guide-
line for how many samples a neurophysiological data-set needs to contain to achieve a bias which is
less than some desired accuracy.

Acknowledgements

We acknowledge support from the Gatsby Charitable Foundation. JHM is supported by an EC
Marie Curie Fellowship  and IM in part by the IST Programme of the European Community  under
the PASCAL2 Network of Excellence  IST-2007-216886. This publication only reﬂects the authors’
views.

References
[1] C.E. Shannon and W. Weaver. The mathematical theory of communication. University of Illinois Press 

1949.

[2] T.M. Cover  J.A. Thomas  J. Wiley  et al. Elements of information theory  volume 6. Wiley Online

Library  1991.

[3] F. Rieke  D. Warland  R. de R uytervansteveninck  and W. Bialek. Spikes: exploring the neural code

(computational neuroscience). The MIT Press  1999.

8

[4] A. Borst and F. E. Theunissen. Information theory and neural coding. Nat Neurosci  2(11):947–957 

1999 Nov.

[5] L. Paninski. Estimation of entropy and mutual information. Neural Computation  15(6):1191–1253 

2003.

[6] B. B. Averbeck  P. E. Latham  and A. Pouget. Neural correlations  population coding and computation.

Nature Reviews Neuroscience  7(5):358–66  2006.

[7] R. Quian Quiroga and S. Panzeri. Extracting information from neuronal populations: information theory

and decoding approaches. Nat Rev Neurosci  10(3):173–185  2009.

[8] G. Miller. Note on the bias of information estimates. In Information Theory in Psychology II-B  chapter

95-100. Free Press  Glencole  IL  1955.

[9] A. Treves and S. Panzeri. The upward bias in measures of information derived from limited data samples.

Neural Computation  7(2):399–407  1995.

[10] S. Panzeri  R. Senatore  M. A. Montemurro  and R. S. Petersen. Correcting for the sampling bias problem

in spike train information measures. J Neurophysiol  98(3):1064–1072  2007.

[11] Robin A A Ince  Alberto Mazzoni  Rasmus S Petersen  and Stefano Panzeri. Open source tools for the

information theoretic analysis of neural data. Front Neurosci  4  2010.

[12] E. Ising. Beitrag zur Theorie des Ferromagnetismus. Z. Phys  31:253  1925.
[13] E. Schneidman  M. J. 2nd Berry  R. Segev  and W. Bialek. Weak pairwise correlations imply strongly

correlated network states in a neural population. Nature  440(7087):1007–12  2006.

[14] J. Shlens  G. D. Field  J. L. Gauthier  M. I. Grivich  D. Petrusca  A. Sher  A. M. Litke  and E. J.
Chichilnisky. The structure of multi-neuron ﬁring patterns in primate retina. J Neurosci  26(32):8254–66 
2006.

[15] I. E. Ohiorhenuan  F. Mechler  K. P. Purpura  A. M. Schmid  Q. Hu  and J. D. Victor. Sparse coding and

high-order correlations in ﬁne-scale cortical networks. Nature  466(7306):617–621  2010.

[16] G. Tkacik  E. Schneidman  M. J. Berry  II  and W. Bialek. Spin glass models for a network of real

neurons. arXiv:q-bio/0611072v2  2009.

[17] Y. Roudi  J. Tyrcha  and J. Hertz. Ising model for neural data: model quality and approximate methods
for extracting functional connectivity. Phys Rev E Stat Nonlin Soft Matter Phys  79(5 Pt 1):051915  May
2009.

[18] Y. Roudi  E. Aurell  and J. A. Hertz. Statistical physics of pairwise probability models. Front Comput

Neurosci  3:22  2009.

[19] T. Mora  A. M. Walczak  W. Bialek  and C. G. Jr Callan. Maximum entropy models for antibody diversity.

Proc Natl Acad Sci U S A  107(12):5405–5410  2010.

[20] A.W. Van der Vaart. Asymptotic statistics. Cambridge Univ Pr  2000.
[21] J.H. Macke  P. Berens  A.S. Ecker  A.S. Tolias  and M. Bethge. Generating spike trains with speciﬁed

correlation coefﬁcients. Neural Computation  21(2):397–423  2009.

[22] J.H. Macke  M. Opper  and M. Bethge. Common input explains higher-order correlations and entropy in

a simple model of neural population activity. Physical Review Letters  106(20):208102  2011.

[23] I. Nemenman  W. Bialek  and R.D.R. Van Steveninck. Entropy and information in neural spike trains:

Progress on the sampling problem. Physical Review E  69(5):056111  2004.

[24] N.A. Ahmed and D. V. Gokhale. Entropy expressions and their estimators for multivariate distributions.

Information Theory  IEEE Transactions on  35(3):688–692  1989.

[25] O. Oyman  R. U. Nabar  H. Bolcskei  and A. J. Paulraj. Characterizing the statistical properties of mutual
information in MIMO channels: insights into diversity-multiplexing tradeoff. In Signals  Systems and
Computers  2002. Conference Record of the Thirty-Sixth Asilomar Conference on  volume 1  pages 521–
525. IEEE  2002.

[26] N. Misra  H. Singh  and E. Demchuk. Estimation of the entropy of a multivariate normal distribution.

Journal of multivariate analysis  92(2):324–342  2005.

[27] G. Marrelec and H. Benali. Large-sample asymptotic approximations for the sampling and posterior
distributions of differential entropy for multivariate normal distributions. Entropy  13(4):805–819  2011.
In
Information Theory  2008. ISIT 2008. IEEE International Symposium on  pages 1103–1107. IEEE  2008.
[29] N.R. Goodman. The distribution of the determinant of a complex Wishart distributed matrix. The Annals

[28] S. Srivastava and M.R. Gupta. Bayesian estimation of the entropy of the multivariate Gaussian.

of mathematical statistics  34(1):178–180  1963.

[30] M. Gupta and S. Srivastava. Parametric Bayesian estimation of differential entropy and relative entropy.

Entropy  12(4):818–843  2010.

9

,Omer Levy
Yoav Goldberg
Kohei Hayashi
Yuichi Yoshida
Giacomo De Palma
Bobak Kiani
Seth Lloyd