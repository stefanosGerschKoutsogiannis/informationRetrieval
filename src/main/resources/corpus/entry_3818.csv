2012,Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions,We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function  but this assumption is hard to be met in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efficient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions  and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains.,Nonparametric Bayesian

Inverse Reinforcement Learning
for Multiple Reward Functions

Jaedeug Choi and Kee-Eung Kim
Department of Computer Science

Korea Advanced Institute of Science and Technology

Daejeon 305-701  Korea

jdchoi@ai.kaist.ac.kr  kekim@cs.kaist.ac.kr

Abstract

We present a nonparametric Bayesian approach to inverse reinforcement learning
(IRL) for multiple reward functions. Most previous IRL algorithms assume that
the behaviour data is obtained from an agent who is optimizing a single reward
function  but this assumption is hard to guarantee in practice. Our approach is
based on integrating the Dirichlet process mixture model into Bayesian IRL. We
provide an efﬁcient Metropolis-Hastings sampling algorithm utilizing the gradient
of the posterior to estimate the underlying reward functions  and demonstrate that
our approach outperforms previous ones via experiments on a number of problem
domains.

1

Introduction

Inverse reinforcement learning (IRL) aims to ﬁnd the agent’s underlying reward function given the
behaviour data and the model of environment [1]. IRL algorithms often assume that the behaviour
data is from an agent who behaves optimally without mistakes with respect to a single reward func-
tion. From the Markov decision process (MDP) perspective  the IRL can be deﬁned as the problem
of ﬁnding the reward function given the trajectory data of an optimal policy  consisting of state-
action histories. Under this assumption  a number of studies on IRL have appeared in the liter-
ature [2  3  4  5].
In addition  IRL has been applied to various practical problems that includes
inferring taxi drivers’ route preferences from their GPS data [6]  estimating patients’ preferences to
determine the optimal timing of living-donor liver transplants [7]  and implementing simulated users
to assess the quality of dialogue management systems [8].

In practice  the behaviour data is often gathered collectively from multiple agents whose reward
functions are potentially different from each other. The amount of data generated from a single
agent may be severely limited  and hence we may suffer from the sparsity of data if we try to infer
the reward function individually. Moreover  even when we have enough data from a single agent 
the reward function may change depending on the situation.

However  most of the previous IRL algorithms assume that the behaviour data is generated by a
single agent optimizing a ﬁxed reward function  although there are a few exceptions that address
IRL for multiple reward functions. Dimitrakakis and Rothkopf [9] proposed a multi-task learning
approach  generalizing the Bayesian approach to IRL [4]. In this work  the reward functions are
individually estimated for each trajectory  which are assumed to share a common prior. Other than
the common prior assumption  there is no effort to group trajectories that are likely to be generated
from the same or similar reward functions. On the other hand  Babes¸-Vroman et al. [10] took a more
direct approach that combines EM clustering with IRL algorithm. The behaviour data are clustered

1

based on the inferred reward functions  where the reward functions are deﬁned per cluster. However 
the number of clusters (hence the number of reward functions) has to be speciﬁed as a parameter in
order to use the approach.

In this paper  we present a nonparametric Bayesian approach using the Dirichlet process mixture
model in order to address the IRL problem with multiple reward functions. We develop an efﬁcient
Metropolis-Hastings (MH) sampler utilizing the gradient of the reward function posterior to infer
reward functions from the behaviour data. In addition  after completing IRL on the behaviour data 
we can efﬁciently estimate the reward function for a new trajectory by computing the mean of the
reward function posterior given the pre-learned results.

2 Preliminaries

We assume that the environment is modeled as an MDP hS  A  T  R  γ  b0i where: S is the ﬁnite set
of states; A is the ﬁnite set of actions; T (s  a  s′) is the state transition probability of changing to
state s′ from state s when action a is taken; R(s  a) is the immediate reward of executing action a
in state s; γ ∈ [0  1) is the discount factor; b0(s) denotes the probability of starting in state s. For
notational convenience  we use the vector r = [r1  . . .   rD] to denote the reward function.1
A policy is a mapping π : S → A. The value of policy π is the expected discounted return of
executing the policy  deﬁned as V π = E [P∞
t=0 γtR(st  at)|b0  π]. The value function of policy π
for each state s is computed by V π(s) = R(s  π(s)) + γPs′∈S T (s  π(s)  s′)V π(s′) so that the
value is calculated by V π = Ps∈S b0(s)V π(s). Similarly  the Q-function is deﬁned as Qπ(s  a) =
R(s  a) + γPs′∈S T (s  a  s′)V π(s′). Given an MDP  the agent’s objective is to execute an optimal
optimality equation: V ∗(s) = maxa∈A(cid:2)R(s  a) + γPs′∈S T (s  a  s′)V ∗(s′)(cid:3).

policy π∗ that maximizes the value function for all the states  which should satisfy the Bellman

We assume that the agent’s behavior data is generated by executing an optimal policy with some
unknown reward function(s) R  given as the set X of M trajectories where the m-th trajectory is an
H-step sequence of state-action pairs: Xm = {(sm 1  am 1)  (sm 2  am 2)  . . .   (sm H   am H )}.2

2.1 Bayesian Inverse Reinforcement Learning (BIRL)

Ramachandran and Amir [4] proposed a Bayesian approach to IRL with the assumption that the
behaviour data is generated from a single reward function. The prior encodes the the reward function
preference and the likelihood measures the compatibility of the reward function with the data.

P (r) = QD

Assuming that the reward function entries are independently distributed  the prior is deﬁned as
d=1 P (rd). We can use various distributions for the reward prior. For instance  the
uniform distribution can be used if we have no knowledge or preference on rewards other than its
range  and the normal or Laplace distributions can be used if we prefer rewards to be close to some
speciﬁc values. The Beta distribution can also be used if we treat rewards as the parameter of the
Bernoulli distribution  i.e. P (ξd = 1) = rd with auxiliary binary random variable ξd [11].
The likelihood is deﬁned as an independent exponential distribution  analogous to the softmax dis-
tribution over actions:

P (X |r  η) = QM

m=1QH

h=1 P (am h|sm hr  η) = QM

m=1QH

exp(ηQ∗(sm h am h;r))
Pa′ exp(ηQ∗(sm h a′;r))

(1)

h=1

where η is the conﬁdence parameter of choosing optimal actions and Q∗(·  ·; r) denotes the optimal
Q-function computed using reward function r.
For the sake of exposition  we assume that the reward function entries are independently and
normally distributed with mean µ and variance σ2 so that the prior is deﬁned as P (r|µ  σ) =
QD
d=1 N (rd; µ  σ)  but our approach to be presented in later sections can be generalized to use
many other distributions for the prior. The posterior over the reward functions is then formulated by

1D denotes the number of features. Note that we can assign individual reward values to every state-action

pair by using |S||A| indicator functions for features.

2Although we assume that all trajectories are of length H for notational brevity  our formulation trivially

extends to different lengths.

2

Figure 1: Graphical model for BIRL.

Algorithm 1: MH algorithm for DPM-BIRL
Initialize c and {rk}K
k=1
for t = 1 to MaxIter do
for m = 1 to M do

c∗
m ∼ P (c|c−m  α)
m /∈ c−m then rc∗
if c∗
hcm  rcm i ← hc∗
m  rc∗
P (Xm|r c∗
P (Xm|r cm  η) }
min{1 
m

 η)

m ∼ P (r|µ  σ)
m i with prob. of

for k = 1 to K do

ǫ ∼ N (0  1)
k ← rk + τ 2
r∗
rk ← r∗

2 ∇ log f (rk) + τ ǫ
k with prob. of min{1  f (r

∗

k)g(r

∗

f (r k)g(r k  r

k  r k)
k) }

∗

Figure 2: Graphical model for DPM-BIRL.

Bayes rule as follows:

P (r|X   η  µ  σ) ∝ P (X |r  η)P (r|µ  σ).

(2)

We can infer the reward function from the model by computing the posterior mean using a Markov
chain Monte Carlo (MCMC) algorithm [4] or the maximum-a-posteriori (MAP) estimates using a
gradient method [12]. Fig. 1 shows the graphical model used in BIRL.

3 Nonparametric Bayesian IRL for Multiple Reward Functions

In this section  we present our approach to IRL for multiple reward functions. We assume that each
trajectory in the behaviour data is generated by an agent with a ﬁxed reward function.
In other
words  we assume that the reward function does not change within a trajectory. However  the whole
trajectories are assumed be generated by one or more agents whose reward functions are distinct
from each other. We do not assume any information regarding which trajectory is generated by
which agent as well as the number of agents. Hence  the goal is to infer an unknown number of
reward functions from the unlabeled behaviour data.

A naive approach to this problem setting would be solving M separate and independent IRL prob-
lems by treating each trajectory as the sole behaviour data and employing one of the well-known
IRL algorithms designed for a single reward function. We can then use an unsupervised learning
method with the M reward functions as data points. However  this approach would suffer from the
sparsity of data  since each trajectory may not contain a sufﬁcient amount of data to infer the reward
function reliably  or the number of trajectories may not be enough for the unsupervised learning
method to yield a meaningful result. Babes¸-Vroman et al. [10] proposed an algorithm that combines
EM clustering with IRL algorithm. It clusters trajectories and assumes that all the trajectories in a
cluster are generated by a single reward function. However  as a consequence of using EM clus-
tering  we need to specify the number of clusters (i.e. the number of distinct reward functions) as a
parameter.

We take a nonparametric Bayesian approach to IRL using the Dirichlet process mixture model. Our
approach has three main advantages. First  we do not need to specify the number of distinct reward
functions due to the nonparametric nature of our model. Second  we can encode our preference
or domain knowledge on the reward function into the prior since it is a Bayesian approach to IRL.
Third  we can acquire rich information from the behaviour data such as the distribution over the
reward functions.

3.1 Dirichlet Process Mixture Models

The Dirichlet process mixture (DPM) model [13] provides a nonparametric Bayesian framework for
clustering using mixture models with a countably inﬁnite number of mixture components. The prior
of the mixing distribution is given by the Dirichlet process  which is a distribution over distributions

3

parameterized by base distribution G0 and concentration parameter α. The DPM model for a data
{xm}M

m=1 using a set of latent parameters {θm}M

m=1 can be deﬁned as:

G|α  G0 ∼ DP (α  G0) 

θm|G ∼ G

xm|θm ∼ F (θm)

where G is the prior used to draw each θm and F (θm) is the parameterized distribution for data xm.
This is equivalent to the following form with K → ∞:

p|α ∼ Dirichlet(α/K  . . .   α/K)
cm|p ∼ Multinomial(p1  . . .   pK )

φk ∼ G0

xm|cm  φ ∼ F (φcm)

(3)

where p = {pk}K
k=1 is the mixing proportion for the latent classes  cm ∈ {1  . . .   K} is the class
assignment of xm so that cm = k when xm is assigned to class k  φk is the parameter of the data
distribution for class k  and φ = {φk}K

k=1.

3.2 DPM-BIRL for Multiple Reward Functions

We address the IRL for multiple reward functions by extending BIRL with the DPM model. We
place a Dirichlet process prior on the reward functions rk. The base distribution G0 is deﬁned
as the reward function prior  i.e. the product of the normal distribution for each reward entry
QD
d=1 N (rk d; µ  σ). The cluster assignment cm = k indicates that the trajectory Xm belongs to
the cluster k  which represents that the trajectory is generated by the agent with the reward function
rk. We can thus regard the behavior data X = {X1  . . .   XM } as being drawn from the following
generative process:

1. The cluster assignment cm is drawn by the ﬁrst two equations in Eqn. (3).

2. The reward function rk is drawn from QD

d=1 N (rk d; µ  σ).
3. The trajectory Xm is drawn from P (Xm|rcm  η) in Eqn. (1).

Fig. 2 shows the graphical model of DPM-BIRL. The joint posterior of the cluster assignment c =
{cm}M

m=1 and the set of reward functions {rk}K

k=1 is deﬁned as:

P (c  {rk}K

k=1|X   η  µ  σ  α) = P (c|α)QK

k=1 P (rk|Xc(k)  η  µ  σ)

(4)

where Xc(k) = {Xm|cm = k for m = 1  . . .   M } and P (rk|X   η  µ  σ) are taken from Eqn. (2).
The inference in DPM-BIRL can be done using the Metropolis-Hastings (MH) algorithm that sam-
ples each hidden variable in turn. First  note that we can safely assume that there are K distinct
values of cm’s so that cm ∈ {1  . . .   K} without loss of generality. The conditional distribution to
sample cm for the MH update can be deﬁned as

P (cm|c−m  {rk}K

k=1  X   η  α) ∝ P (Xm|rcm  η)P (cm|c−m  α)

P (cm|c−m  α) ∝ (cid:26)n−m cj  

α 

if cm = cj for some j
if cm 6= cj for all j

(5)

where c−m = {ci|i 6= m for i = 1  . . .   M }  P (Xm|rcm  η) is the likelihood deﬁned in Eqn. (1) 
and n−m cj = |{ci = cj|i 6= m for i = 1  . . .   M }| is the number of trajectories  excluding Xm 
assigned to the cluster cj. Note that if the sampled cm 6= cj for all j then Xm is assigned to a new
cluster. The conditional distribution to sample rk for the MH update is deﬁned as

P (rk|c  r−k  X   η  µ  σ) ∝ P (Xc(k)|rk  η)P (rk|µ  σ)

where P (Xc(k)|rk  η) is again the likelihood deﬁned in Eqn.

(1) and P (rk|µ  σ) =

QD
d=1 N (rk d; µ  σ).

In Alg. 1  we present the MH algorithm for DPM-BIRL that uses the above MH updates. The
algorithm consists of two steps. The ﬁrst step updates the cluster assignment c. We sample new

4

assignment c∗
function rc∗
of min{1 
the reward functions {rk}K

 η)

m from Eqn. (5). If c∗
from the reward prior P (r|µ  σ). We then set cm = c∗

m 6= cj for all j  we draw new reward
m with the acceptance probability
m
P (Xm|rc∗
P (Xm|rcm  η) }  since we are using a non-conjugate prior [13]. The second step updates
m

m is not in c−m  i.e.  c∗

k=1. We sample a new reward function r∗

k using the equation

k = rk + τ 2
r∗

2 ∇ log f (rk) + τ ǫ

where ǫ is a sample from the standard normal distribution N (0  1)  τ is a non-negative scalar for the
scaling parameter  and f (rk) is the target distribution of the MH update P (Xc(k)|rk  η)P (rk|µ  σ)
which is the unnormalized posterior of the reward function rk. We then set rk = r∗
k with the
acceptance probability of min{1  f (r

k)g(r

k rk)

∗

∗

f (rk)g(rk r

k) } where

∗

g(x  y) =

1

(2πτ 2)D/2 exp(cid:0)− 1

2τ 2 ||x − y − 1

2 τ 2∇ log f (x)||2
2(cid:1) .

This step is motivated by the Langevin algorithm [14] which exploits local information (i.e. gradient)
of f in order to efﬁciently move towards the high probability region. This algorithm is known to
be more efﬁcient than random walk MH algorithms. We can compute the gradient of f using the
results of Choi and Kim [12].

3.3

Information Transfer to a New Trajectory

Suppose that we would like to infer the reward function of a new trajectory after we ﬁnish IRL on the
behaviour data consisting of M trajectories. A naive approach would be running IRL from scratch
using all of the M + 1 trajectories. However  it would be more desirable to transfer the relevant
information from the pre-computed IRL results. In order to do so  Babes¸-Vroman et al. [10] use the
weighted average of cluster reward functions assuming that the new trajectory is generated from the
same population of the behaviour data. Note that we can relax this assumption and allow the new
trajectory generated by a novel reward function  as a direct result of using DPM model.
Given the cluster assignment c and the reward functions {rk}K
data  the conditional prior of the reward function r for the new trajectory can be deﬁned as:

k=1 computed from the behaviour

P (r|c  {rk}K

k=1  µ  σ  α) = α

(6)
where nk = |{Xm|cm = k for m = 1  . . .   M }| is the number of trajectories assigned to cluster k
and δ(x) is the Dirac delta function. Running Alg. 1 on the behaviour data X   we already have a set
n=1 drawn from the joint posterior. The conditional posterior of r
of N samples {c(n)  {r
k=1 }N
for the new trajectory Xnew is then:

α+M P (r|µ  σ) + 1

k=1 nkδ(r − rk)

α+M PK

k }K(n)

(n)

P (r|Xnew  X   Θ) ∝ P (Xnew|r  η)P (r|X   Θ)

= P (Xnew|r  η)Z P (r|c  {rk}K
N PN
≈ P (Xnew|r  η) 1
= P (Xnew|r  η)(cid:20) α
α+M P (r|µ  σ) + 1

n=1 P (r|{c(n)  {r

k=1  µ  σ  α)dP (c  {rk}K

k=1|X   Θ)

k=1 }N

n=1  µ  σ  α)

(n)

k }K(n)
α+M PN

n=1PK(n)

k=1

n(n)
N δ(r − r

k

(n)

k )(cid:21)

where Θ = {η  µ  σ  α}.
We can then re-draw samples of r using the approximated posterior and take the sample average
as the inferred reward function. However  we present a more efﬁcient way of calculating the pos-
terior mean of r without re-drawing the samples. Note that Eqn. (6) is a mixture of a continuous
k=1. If we approximate
distribution P (r|µ  σ) with a number of point mass distributions on {rk}K
the continuous one by a point mass distribution  i.e.  P (r|µ  σ) ≈ δ(ˆr)  the posterior mean is ana-
lytically computable using the above approximation:

E[r|Xnew  X   Θ] = R rdP (r|Xnew  X   Θ)

≈ 1

Z (cid:20)αP (Xnew|ˆr  η)ˆr +PN

n=1PK(n)

k=1

n(n)
N P (Xnew|r

k

(n)
k   η)r

(n)

k (cid:21)

(7)

where Z is the normalizing constant. We choose ˆr = argmaxr P (Xnew|r  η)P (r|µ  σ)  which is
the MAP estimate of the reward function for the new trajectory Xnew only  ignoring the previous
behaviour data X .

5

1.5

1

0.5

 

D
V
E
e
g
a
r
e
v
A

0

2

1

e
r
o
c
s
−
F

0.9

0.8

12

0.7

2

1

0.9

0.8

I

M
N

12

0.7

2

4

6

8

10

# of trajectories per agent

5

4

3

s
r
e

t
s
u
c
 
f

l

o
#

 

12

2

 
2

BIRL
EM−MLIRL(3)
EM−MLIRL(6)
EM−MLIRL(9)
DPM−BIRL(U)
DPM−BIRL(G)

4

6

8

10

# of trajectories per agent

 
y
r
o

j

t
c
e
a
r
t
 

w
e
n

 

e
h

t
 
r
o

f
 

D
V
E

12

1.5

1

0.5

0

2

4

6

8

10

12

# of trajectories per agent

4

6

8

10

# of trajectories per agent

4

6

8

10

# of trajectories per agent

Figure 3: Results with increasing number of trajectories per agent in the gridworld problem. DPM-
BIRL uses the uniform (U) and the standard normal (N) priors.

4 Experimental Results

We compared the performance of DPM-BIRL to the EM-MLIRL algorithm [10] and the baseline
algorithm which runs BIRL separately on each trajectory. The experiments consisted of two tasks:
The ﬁrst task was ﬁnding multiple reward functions from the behaviour data with a number of
trajectories. The second task was inferring the reward function underlying a new trajectory  while
exploiting the results learned in the ﬁrst task.

The performance of each algorithm was evaluated by the expected value difference (EVD)
L)(rA)| where rA is the agent’s ground truth reward function  rL is the learned
|V ∗(rA) − V π∗(r
reward function  π∗(r) is the optimal policy induced by reward function r  and V π(r) is the value of
policy π measured using r. The EVD thus measures the performance difference between the agent’s
optimal policy and the optimal policy induced by the learned reward function. In the ﬁrst task  we
evaluated the EVD for the true and learned reward functions of each trajectory and computed the
average EVD over the trajectories in the behaviour data. In the second task  we evaluated the EVD
for the new trajectory. The clustering quality on the behaviour data was evaluated by F-score and
normalized mutual information (NMI).

In all the experiments  we assumed that the reward function was linearly parameterized such that

R(s  a) = PD

d=1 rdφd(s  a) with feature functions φd : S × A → R  hence r = [r1  . . .   rD].

4.1 Gridworld Problem

In order to extensively evaluate our approach  we ﬁrst performed experiments on a small toy domain 
8×8 gridworld  where each of the 64 cells corresponds to the state. The agent can move north  south 
east  or west  but with probability of 0.2  it fails and moves in a random direction. The initial state is
randomly chosen from the states. The grid is partitioned into non-overlapping regions of size 2 × 2 
and the feature function is deﬁned by a binary indicator function for each region. Random instances
of IRL with three reward functions were generated as follows: each element of r was sampled to
have a non-zero value with probability of 0.2 and the value is drawn from the uniform distribution
between -1 and 1. We obtained the trajectories of 40 time steps and measured the performance as
we increased the number of trajectories per reward function.

Fig. 3 shows the averages and standard errors of the performance results over 10 problem instances.
The left four panels in the ﬁgure present the results for the ﬁrst task of learning multiple reward
functions from the behaviour data. When the size of the behaviour data is small  the clustering
performances of both DPM-BIRL and EM-MLIRL were not good enough due to the sparsity of
data  hence their EVD results were similar to that of the baseline algorithm that independently runs
BIRL on each trajectory. However  as we increased the size of the data  both DPM-BIRL and EM-
MLIRL achieved better EVD results than the baseline since they could utilize more information by
grouping the trajectories to infer the reward functions. As for EM-MLIRL  we set the parameter
K used for the maximum number of clusters to 3 (ground truth)  6 (2x)  and 9 (3x). DPM-BIRL
achieved signiﬁcantly better results than EM-MLIRL with all of the parameter settings  in terms of
EVD and clustering quality. The rightmost panel in the ﬁgure present the results for the second task
of inferring the reward function for a new trajectory. DPM-BIRL clearly outperformed EM-MLIRL
since it exploits the rich information from the reward function posterior. The relatively large error
bars of the EM-MLIRL results are due to the local convergence inherent to EM clustering.

6

3

2

1

 

D
V
E
e
g
a
r
e
v
A

0

 
0

20

 

EM−MLIRL(3)
EM−MLIRL(6)
EM−MLIRL(9)
DPM−BIRL(U)
DPM−BIRL(G)

Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79
Time step: 79

40

60

Cpu time (sec)

80

100

Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high
Speed: high

Figure 4: CPU timing results in the
gridworld problem.

Figure 5: Screenshots of Simulated-highway problem
(left) and Mario Bros (right).

BIRL

EM-MLIRL(3)
EM-MLIRL(6)
DPM-BIRL(U)
DPM-BIRL(N)

0.52±0.05
4.53±0.96
0.89±0.57
0.35±0.04
0.36±0.05

n.a.

0.80±0.05
0.96±0.02
0.98±0.01
0.99±0.01

NMI
n.a.

0.74±0.09
0.96±0.03
0.97±0.01
0.99±0.01

Table 1: Results in Simulated-highway problem.
Average EVD

F-score

# of clusters

n.a.

2.20±0.20
3.10±0.18
3.30±0.15
3.10±0.10

EVD for Xnew

0.41±0.00
4.14±0.88
0.82±0.53
0.32±0.04
0.30±0.04

Fig. 4 compares the average CPU timing results of DPM-BIRL and EM-MLIRL with 10 trajectories
per reward function. DPM-BIRL using Alg. 1 took much shorter time to converge than EM-MLIRL.
This is mainly due to the fact that  whereas EM-MLIRL performs full single-reward IRL multiple
times in each iteration  DPM-BIRL takes a sample from the posterior leveraging the gradient that
does not involve a full IRL.

4.2 Simulated-highway Problem

The second set of experiments was conducted in Simulated-highway problem [15] where the agent
drives on a three lane road. The left panel in Fig. 5 shows a screenshot of the problem. The agent
can move one lane left or right and drive at speeds 2 through 3  but it fails to change the lane with
probability of 0.2 and 0.4 respectively in speed 2 and 3. All the other cars on the road constantly
drive at speed 1 and do not change the lane. The reward function is deﬁned by using 6 binary
feature functions: one function for indicating the agent’s collision with other cars  3 functions for
indicating the agent’s current lane  2 functions for indicating the agent’s current speed. We generated
three agents having different driving styles. The ﬁrst one prefers driving at speed 3 in the left-most
lane and avoiding collisions. The second one prefers driving at speed 3 in the right-most lane and
avoiding collisions. The third one prefers driving at speed 2 and colliding with other cars. We
prepared 3 trajectories of 40 time steps per driver agent for the ﬁrst task and 20 trajectories of 40
time steps yielded by a driver randomly chosen among the three for the second task.

Tbl. 1 presents the averages and standard errors of the results over 10 sets of the behaviour data.
DPM-BIRL signiﬁcantly outperformed the others while EM-MLIRL suffered from the convergence
to a local optimum.

4.3 Mario Bros.

For the third set of experiments  we used the open source simulator of the game Mario Bros  which
is a challenging problem due to its huge state space. The right panel in Fig. 5 is a screenshot of the
game. Mario can move left  move right  or jump. Mario’s goal is to reach the end of the level by
traversing from left to right while collecting coins and avoiding or killing enemies. We used 8 binary
feature functions  each being an indicator for: Mario successfully reaching the end of the level;
Mario getting killed; Mario killing an enemy; Mario collecting a coin; Mario receiving damage by
an enemy; existence of a wall preventing Mario from moving in the current direction; Mario moving
to the right; Mario moving to the left. We collected the behaviour data from 4 players: The expert
player is good at both collecting coins and killing enemies. The coin collector likes to collect coins
but avoids killing enemies. The enemy killer likes to kill enemies but avoids collecting coins. The

7

Expert player

c

DPM-BIRL

EM-MLIRL(4)
EM-MLIRL(8)

Coin collector

Table 2: Cluster assignments in Mario Bros.
Enemy killer
4
3
2
1
1
3

2
2
2

3
2
3

1
1
1

1
1
1

1
1
1

1
1
1

2
1
2

Speedy Gonzales

5
3
3

5
3
3

5
3
3

Table 3: Results of DPM-BIRL in Mario Bros.

Reward function entry (rk d)

k from DPM-BIRL

φenemy-killed
φcoin-collected

1
1.00
1.00

2
-0.81
1.00

3
1.00
-1.00

4
1.00
-0.42

5
-1.00
-1.00

1
3.10
21.60

Average feature counts
4
1.90
7.85

2
1.60
21.55

3
2.80
7.55

5
0.55
6.75

speedy Gonzales avoids both collecting coins and killing enemies. All the players commonly try
to reach the end of the level while acting according to their own preferences. The behaviour data
consisted of 3 trajectories per player. Since only the simulator of the environment is available instead
of the complete model  we used the relative entropy IRL [16] which is a model-free IRL algorithm.

Tbl. 2 presents the cluster assignment results. Each column represents each trajectory and the num-
ber denotes the cluster assignment cm of trajectory Xm. For example  DPM-BIRL produced 5
clusters and trajectories X1  . . .   X4 are assigned to the cluster 1 representing the expert player. EM-
MLIRL failed to group the trajectories that align well with the players  even though we restarted it
100 times in order to mitigate the convergence to bad local optima. On the other hand  DPM-BIRL
was incorrect on only one trajectory  assigning a coin collector’s trajectory to the expert player clus-
ter. Tbl. 3 presents the reward function entries (rk d) learned from DPM-BIRL and the average
feature counts acquired by the players with the learned reward functions. For the sake of brevity 
we present only two important features (d=enemy-killed  coin-collected) that determine the playing
style. To compute each player’s feature counts  we executed an n-step lookahead policy yielded by
each reward function rk on the simulator in 20 randomly chosen levels. The reward function entries
align well with each playing style. For example  the cluster 2 represents the coin collector  and its
reward function entry for killing an enemy is negative but that for collecting a coin is positive.

As a demonstration  we implemented a small piece of software that visualizes the posterior proba-
bility of a gamer’s behavior belonging to one of the clusters including a new one. A demo video is
provided as supplementary material.

5 Conclusion

We proposed a nonparametric Bayesian approach to IRL for multiple reward functions using the
Dirichlet process mixture model  which extends the previous Bayesian approach to IRL assuming
a single reward function. We can learn an appropriate number of reward functions from the be-
havior data due to the nonparametric nature and facilitates incorporating domain knowledge on the
reward function by utilizing a Bayesian approach. We presented an efﬁcient Metropolis-Hastings
sampling algorithm that draws samples from the posterior of DPM-BIRL  leveraging the gradient
of the posterior. We also provided an analytical way to compute the approximate posterior mean
for the information transfer task. In addition  we showed that DPM-BIRL outperforms the previous
approach in various problem domains.

Acknowledgments

This work was supported by National Research Foundation of Korea (Grant# 2012-007881)  the
Defense Acquisition Program Administration and Agency for Defense Development of Korea (Con-
tract# UD080042AD)  and the SW Computing R&D Program of KEIT (2011-10041313) funded by
the Ministry of Knowledge Economy of Korea.

8

References
[1] Stuart Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of COLT 

1998.

[2] Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning.

ICML  2000.

In Proceedings of

[3] Gergely Neu and Csaba Szepesv´ari. Apprenticeship learning using inverse reinforcement learning and

gradient methods. In Proceedings of UAI  2007.

[4] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of IJCAI 

2007.

[5] Brian D. Ziebart  Andrew L. Maas  J. Andrew Bagnell  and Anind K. Dey. Maximum entropy inverse

reinforcement learning. In Proceedings of AAAI  2008.

[6] Brian D. Ziebart  Andrew L. Maas  Anind K. Dey  and J. Andrew Bagnell. Navigate like a cabbie: proba-
bilistic reasoning from observed context-aware behavior. In Proceedings of the international conference
on Ubiquitous computing  2008.

[7] Zeynep Erkin  Matthew D. Bailey  Lisa M. Maillart  Andrew J. Schaefer  and Mark S. Roberts. Eliciting
patients’ revealed preferences: An inverse Markov decision process approach. Decision Analysis  7(4) 
2010.

[8] Senthilkumar Chandramohan  Matthieu Geist  Fabrice Lefevre  and Olivier Pietquin. User simulation in

dialogue systems using inverse reinforcement learning. In Proceedings of Interspeech  2011.

[9] Christos Dimitrakakis and Constantin A. Rothkopf. Bayesian multitask inverse reinforcement learning.

In Proceedings of the European Workshop on Reinforcement Learning  2011.

[10] Monica Babes¸-Vroman  Vukosi Marivate  Kaushik Subramanian  and Michael Littman. Apprenticeship

learning about multiple intentions. In Proceedings of ICML  2011.

[11] Peter Dayan and Geoffrey E. Hinton. Using expectation-maximization for reinforcement learning. Neural

Computation  9(2)  1997.

[12] Jaedeug Choi and Kee-Eung Kim. MAP inference for Bayesian inverse reinforcement learning. In Pro-

ceedings of NIPS  2011.

[13] Radford M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of

Computational and Graphical Statistics  9(2)  2000.

[14] Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to langevin

diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  60(1)  1998.

[15] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Pro-

ceedings of ICML  2004.

[16] Abdeslam Boularias  Jens Kober  and Jan Peters. Relative entropy inverse reinforcement learning. In

Proceedings of AISTATS  2011.

9

,Zhenyao Zhu
Ping Luo
Xiaogang Wang
Xiaoou Tang
Yu-Xiong Wang
Martial Hebert
Xueting Li
Sifei Liu
Shalini De Mello
Xiaolong Wang
Jan Kautz
Ming-Hsuan Yang