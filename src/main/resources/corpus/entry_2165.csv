2009,From PAC-Bayes Bounds to KL Regularization,We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors  that we call quasi uniform  we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ell_p-regularized objective functions currently used  such as ridge regression and ell_p-regularized boosting  are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost.,From PAC-Bayes Bounds to KL Regularization

Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  Mario Marchand  Sara Shanian

Department of Computer Science and Software Engineering

Laval University  Qu´ebec (QC)  Canada

firstname.secondname@ift.ulaval.ca

Abstract

We show that convex KL-regularized objective functions are obtained from a
PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs
classiﬁer that upper-bound the standard zero-one loss used for the weighted ma-
jority vote. By restricting ourselves to a class of posteriors  that we call quasi
uniform  we propose a simple coordinate descent learning algorithm to minimize
the proposed KL-regularized cost function. We show that standard (cid:96)p-regularized
objective functions currently used  such as ridge regression and (cid:96)p-regularized
boosting  are obtained from a relaxation of the KL divergence between the quasi
uniform posterior and the uniform prior. We present numerical experiments where
the proposed learning algorithm generally outperforms ridge regression and Ada-
Boost.

1 Introduction

What should a learning algorithm optimize on the training data in order to give classiﬁers having the
smallest possible true risk? Many different speciﬁcations of what should be optimized on the train-
ing data have been provided by using different inductive principles. But the universally accepted
guarantee on the true risk  however  always comes with a so-called risk bound that holds uniformly
over a set of classiﬁers. Since a risk bound can be computed from what a classiﬁer achieves on the
training data  it automatically suggests that learning algorithms should ﬁnd a classiﬁer that mini-
mizes a tight risk (upper) bound.
Among the data-dependent bounds that have been proposed recently  the PAC-Bayes bounds [6  8 
4  1  3] seem to be especially tight. These bounds thus appear to be a good starting point for the
design of a bound-minimizing learning algorithm. In that respect  [4  5  3] have proposed to use
isotropic Gaussian posteriors over the space of linear classiﬁers. But a computational drawback of
this approach is the fact the Gibbs empirical risk is not a quasi-convex function of the parameters
of the posterior. Consequently  the resultant PAC-Bayes bound may have several local minima for
certain data sets—thus giving an intractable optimization problem in the general case. To avoid
such computational problems  we propose here to use convex loss functions for stochastic Gibbs
classiﬁers that upper-bound the standard zero-one loss used for the weighted majority vote. By
restricting ourselves to a class of posteriors  that we call quasi uniform  we propose a simple coor-
dinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show
that there are no loss of discriminative power by restricting the posterior to be quasi uniform. We
also show that standard (cid:96)p-regularized objective functions currently used  such as ridge regression
and (cid:96)p-regularized boosting  are obtained from a relaxation of the KL divergence between the quasi
uniform posterior and the uniform prior. We present numerical experiments where the proposed
learning algorithm generally outperforms ridge regression and AdaBoost [7].

1

2 Basic Deﬁnitions
We consider binary classiﬁcation problems where the input space X consists of an arbitrary subset
of Rd and the output space Y = {−1  +1}. An example is an input-output (x  y) pair where x ∈ X
and y ∈ Y. Throughout the paper  we adopt the PAC setting where each example (x  y) is drawn
according to a ﬁxed  but unknown  distribution D on X × Y.
The risk R(h) of any classiﬁer h: X → Y is deﬁned as the probability that h misclassiﬁes an
example drawn according to D. Given a training set S of m examples  the empirical risk RS(h) of
any classiﬁer h is deﬁned by the frequency of training errors of h on S. Hence

R(h) def=

E

(x y)∼D

I(h(x) (cid:54)= y)

; RS(h) def=

1
m

I(h(xi) (cid:54)= yi)  

m(cid:88)

i=1

where I(a) = 1 if predicate a is true and 0 otherwise.
After observing the training set S  the task of the learner is to choose a posterior distribution Q
over a space H of classiﬁers such that the Q-weighted majority vote classiﬁer BQ will have the
smallest possible risk. On any input example x  the output BQ(x) of the majority vote classiﬁer BQ
(sometimes called the Bayes classiﬁer) is given by

(cid:20)

(cid:21)

BQ(x) def= sgn

E
h∼Q

h(x)

 

where sgn(s) = +1 if s > 0 and sgn(s) = −1 otherwise. The output of the deterministic majority
vote classiﬁer BQ is closely related to the output of a stochastic classiﬁer called the Gibbs classiﬁer
GQ. To classify an input example x  the Gibbs classiﬁer GQ chooses randomly a (deterministic)
classiﬁer h according to Q to classify x. The true risk R(GQ) and the empirical risk RS(GQ) of the
Gibbs classiﬁer are thus given by

R(GQ) = E
h∼Q

R(h)

; RS(GQ) = E
h∼Q

RS(h) .

Any bound for R(GQ) can straightforwardly be turned into a bound for the risk of the majority vote
R(BQ). Indeed  whenever BQ misclassiﬁes x  at least half of the classiﬁers (under measure Q)
misclassiﬁes x. It follows that the error rate of GQ is at least half of the error rate of BQ. Hence
R(BQ) ≤ 2R(GQ). As shown in [5]  this factor of 2 can sometimes be reduced to (1 + ).

3 PAC-Bayes Bounds and General Loss Functions

(cid:20)

(cid:104)

1
m

(cid:105)(cid:21)(cid:19)

1
δ

(cid:18)

In this paper  we use the following PAC-Bayes bound which is obtained directly from Theorem 1.2.1
of [1] and Corollary 2.2 of [3] by using 1 − exp(−x) ≤ x ∀x ∈ R.
Theorem 3.1. For any distribution D  any set H of classiﬁers  any distribution P of support H 
any δ ∈ (0  1]  and any positive real number C(cid:48)  we have

Pr

S∼Dm

∀ Q onH: R(GQ) ≤

1

1 − e−C(cid:48)

C(cid:48)·RS(GQ) +

KL(Q(cid:107)P ) + ln

≥ 1 − δ  

where KL(Q(cid:107)P ) def= E
h∼Q

ln Q(h)

P (h) is the Kullback-Leibler divergence between Q and P .

Note that the dependence on Q of the upper bound on R(GQ) is realized via Gibbs’ empirical risk
RS(GQ) and the PAC-Bayes regularizer KL(Q(cid:107)P ). As in boosting  we focus on the case where
the a priori deﬁned class H consists (mostly) of “weak” classiﬁers having large risk R(h) . In this
case  R(GQ) is (almost) always large (near 1/2) for any Q even if the majority vote BQ has null
risk. In this case the disparity between R(BQ) and R(GQ) is enormous and the upper-bound on
R(GQ) has very little relevance with R(BQ). On way to obtain a more relevant bound on R(BQ)
from PAC-Bayes theory is to use a loss function ζQ(x  y) for stochastic classiﬁers which is distinct
from the loss used for the deterministic classiﬁers (the zero-one loss in our case). In order to obtain
a tractable optimization problem for a learning algorithm to solve  we propose here to use a loss
ζQ(x  y) which is convex in Q and that upper-bounds as closely as possible the zero-one loss of the
deterministic majority vote BQ.

2

Consider WQ(x  y) def= Eh∼QI(h(x) (cid:54)= y)  the Q-fraction of binary classiﬁers that err on exam-
ple (x  y). Then  R(GQ) = E(x y)∼D WQ(x  y). Following [2]  we consider any non-negative
convex loss ζQ(x  y) that can be expanded in a Taylor series around WQ(x  y) = 1/2:

ζQ(x  y) def= 1 +

ak (2WQ(x  y) − 1)k = 1 +

ak

E
h∼Q

− yh(x)

 

that upper bounds the risk of the majority vote BQ  i.e. 

(cid:18)

(cid:19)

1
2

ζQ(x  y) ≥ I

WQ(x  y) >

∀Q  x  y .

It has been shown [2] that ζQ(x  y) can be expressed in terms of the risk on example (x  y) of a
Gibbs classiﬁer described by a transformed posterior Q on N × H∞  i.e. 

(cid:18)

∞(cid:88)

k=1

def= (cid:80)∞

where ca

k=1 |ak| and where

ζQ(x  y) = 1 + ca

(cid:104)
(cid:105)
2WQ(x  y) − 1
(cid:16)

 

(cid:19)k

(cid:17)

∞(cid:88)

k=1

∞(cid:88)

k=1

WQ(x  y) def=

1
ca

|ak| E
h1∼Q

. . . E

hk∼Q

I

(−y)kh1(x) . . . hk(x) = −sgn(ak)

.

Since WQ(x  y) is the expectation of boolean random variable  Theorem 3.1 holds if we replace
(P  Q) by (P   Q) with R(GQ) def=
i=1 WQ(xi  yi). More-
over  it has been shown [2] that

WQ(x  y) and RS(GQ) def= 1

(x y)∼D

E

m

KL(Q(cid:107)P ) = k · KL(Q(cid:107)P )   where k def=

1
ca

|ak| · k .

(cid:80)m

∞(cid:88)

k=1

If we deﬁne

ζQ

(cid:99)ζQ

def=

def=

ζ(x  y) = 1 + ca[2R(GQ) − 1]

ζ(xi  yi) = 1 + ca[2RS(GQ) − 1]  

E

m(cid:88)

(x y)∼D
1
m

i=1

Theorem 3.1 gives an upper bound on ζQ and  consequently  on the true risk R(BQ) of the majority
vote. More precisely  we have the following theorem.
Theorem 3.2. For any D  any H  any P of support H  any δ ∈ (0  1]  any positive real number C(cid:48) 
any loss function ζQ(x  y) deﬁned above  we have

(cid:20)(cid:99)ζQ +

(cid:104)

2ca
mC(cid:48)

k · KL(Q(cid:107)P ) + ln

1
δ

≥ 1 − δ  

(cid:105)(cid:21)(cid:19)

(cid:18)

Pr

∀ Q onH: ζQ ≤ g(ca  C(cid:48)) +

S∼Dm
where g(ca  C(cid:48)) def= 1 − ca + C(cid:48)

1 − e−C(cid:48)
1−e−C(cid:48) · (ca − 1).

C(cid:48)

4 Bound Minimization Learning Algorithms

The task of the learner is to ﬁnd the posterior Q that minimizes the upper bound on ζQ for a ﬁxed
loss function given by the coefﬁcients {ak}∞
k=1 of the Taylor series expansion for ζQ(x  y). Finding
Q that minimizes the upper bound given by Theorem 3.2 is equivalent to ﬁnding Q that minimizes

f(Q) def= C

where C def= C(cid:48)/(2cak) .

ζQ(xi  yi) + KL(Q(cid:107)P )  

m(cid:88)

i=1

3

exp

y

Q(h)h(x)

= exp

[2WQ(x  y) − 1]

.

(cid:32)
− 1
γ

(cid:88)

h∈H

(cid:32)

(cid:88)

h∈H

1
γ

y

(cid:33)

(cid:33)2

(cid:18) 1

γ

(cid:18) 1

γ

(cid:19)

(cid:19)2

To compare the proposed learning algorithms with AdaBoost  we will consider  for ζQ(x  y)  the
exponential loss given by

For this choice of loss  we have ca = eγ−1 −1 and k = γ−1/(1− e−γ−1). Because of its simplicity 
we will also consider  for ζQ(x  y)  the quadratic loss given by

Q(h)h(x) − 1

=

[1 − 2WQ(x  y)] − 1

.

has the minimum value of zero for examples having a margin y(cid:80)

For this choice of loss  we have ca = 2γ−1 + γ−2 and k = (2γ + 2)/(2γ + 1). Note that this loss

h∈H Q(h)h(x) = γ.

With these two choices of loss functions  ζQ(x  y) is convex in Q. Moreover  KL(Q(cid:107)P ) is also
convex in Q. Since a sum of convex functions is also convex  it follows that objective function f
is convex in Q (which has a convex domain). Consequently  f has a single local minimum which
coincides with the global minimum. We therefore propose to minimize f coordinate-wise  similarly
h∈H Q(h) =
1)  each coordinate minimization will consist of a transfer of weight from one classiﬁer to another.

as it is done for AdaBoost [7]. However  to ensure that Q is a distribution (i.e.  that(cid:80)

4.1 Quasi Uniform Posteriors
We consider learning algorithms that work in a space H of binary classiﬁers such that for
each h ∈ H  the boolean complement of h is also in H. More speciﬁcally  we have H =
{h1  . . .   hn  hn+1  . . .   h2n} where hi(x) = −hn+i(x) ∀x ∈ X and ∀i ∈ {1  . . .   n}. We thus
say that (hi  hn+i) constitutes a boolean complement pair of classiﬁers.
We consider a uniform prior distribution P over H  i.e.  Pi = 1
The posterior distribution Q over H is constrained to be quasi uniform. By this  we mean that
n ∀i ∈ {1  . . .   n}  i.e.  the total weight assigned to each boolean complement pair of
Qi + Qi+n = 1
def= Qi − Qi+n ∀i ∈ {1  . . .   n}. Then wi ∈ [−1/n  +1/n] ∀i ∈
classiﬁers is ﬁxed to 1/n. Let wi
{1  . . .   n} whereas Qi ∈ [0  1/n] ∀i ∈ {1  . . .   2n}.
For any quasi uniform Q  the output BQ(x) of the majority vote on any example x is given by

2n ∀i ∈ {1  . . .   2n}.

(cid:16) 2n(cid:88)

(cid:17)

(cid:16) n(cid:88)

(cid:17) def= sgn

(cid:16)

(cid:17)

w · h(x)

.

BQ(x) = sgn

Qihi(x)

= sgn

wihi(x)

i=1

i=1

Consequently  the set of majority votes BQ over quasi uniform posteriors is isomorphic to the set
of linear separators with real weights. There is thus no loss of discriminative power if we restrict
ourselves to quasi uniform posteriors.

Since all loss functions that we consider are functions of 2WQ(x  y) − 1 = −y(cid:80)

are thus functions of yw · h(x). Hence we will often write ζ(yw · h(x)) for ζQ(x  y).
The basic iteration for the learning algorithm consists of choosing (at random) a boolean com-
plement pair of classiﬁers  call it (h1  hn+1)  and then attempting to change only Q1  Qn+1  w1
according to:

i Qihi(x)  they

Q1 ← Q1 + δ
2

; Qn+1 ← Qn+1 − δ
2

; w1 ← w1 + δ  

(1)

for some optimally chosen value of δ.
Let Qδ and wδ be  respectively  the new posterior and the new weight vector obtained with such a
change. The above-mentioned convex properties of objective function f imply that we only need to
look for the value of δ∗ satisfying

= 0 .

(2)

df(Qδ)

dδ

4

If w1 + δ∗ > 1/n  then w1 ← 1/n  Q1 ← 1/n  Qn+1 ← 0. If w1 + δ∗ < −1/n  then w1 ←
−1/n  Q1 ← 0  Qn+1 ← 1/n. Otherwise  we accept the change described by Equation 1 with
δ = δ∗.
For objective function f we simply have

+ dKL(Qδ(cid:107)P )

 

dδ

(cid:18)

Q1 + δ
2

+

Qn+1 − δ
2

(cid:35)

(cid:19)

ln

Qn+1 − δ

2

1
2n

d(cid:100)ζQδ

dδ

df(Qδ)

dδ

= Cm

(cid:34)(cid:18)
(cid:19)
(cid:20) Q1 + δ/2

Q1 + δ
2

ln

(cid:21)

Qn+1 − δ/2

.

1
2n

m(cid:88)

i=1

where

dKL(Qδ(cid:107)P )

dδ

= d
dδ
1
2
For the quadratic loss  we ﬁnd

=

m

where

=

2mδ
γ2 +

2
γ2

Dql

w(i)yih1(xi)  

(3)

(4)

(5)

(6)

(7)

(8)

(9)

ln

d(cid:100)ζQδ

dδ

m(cid:88)
m(cid:88)

Consequently  for the quadratic loss case  the optimal value δ∗ satisﬁes

Dql

w(i)

def= yiw · h(xi) − γ .

2Cmδ
γ2 +

2C
γ2

Dql

w(i)yih1(xi) +

1
2

ln

(cid:21)

= 0 .

i=1
For the exponential loss  we ﬁnd

d(cid:100)ζQδ

dδ

m

where

= eδ/γ
γ

w(i)I(h1(xi) (cid:54)= yi) − e−δ/γ

Del

i=1

Del

w(i)

def= exp

(cid:18)
− 1
γ

Consequently  for the exponential loss case  the optimal value δ∗ satisﬁes

Qn+1 − δ/2

(cid:20) Q1 + δ/2
m(cid:88)
(cid:19)

Del

i=1

γ

yiw · h(xi)

.

w(i)I(h1(xi) = yi)  

m(cid:88)

i=1

Ceδ/γ

γ

w(i)I(h1(xi) (cid:54)= yi)
Del
m(cid:88)
− Ce−δ/γ

γ

i=1

Del

w(i)I(h1(xi) = yi) +

(cid:20) Q1 + δ/2

Qn+1 − δ/2

(cid:21)

1
2

ln

= 0 .

(10)

− 1

w(i) + yih1(xi)δ
w(i)e
γ yih1(xi)δ

(quadratic loss case)
(exponential loss case) .

After changing w1  we need to recompute1 Dw(i) ∀i ∈ {1  . . .   m}. This can be done with the
following update rules.
w(i) ← Dql
Dql
w(i) ← Del
Del
Since  initially we have
w(i) = −γ ∀i ∈ {1  . . .   m} (quadratic loss case)
Dql
w(i) = 1 ∀i ∈ {1  . . .   m} (exponential loss case)  
Del

(13)
(14)
the dot product present in Equations 6 and 9 never needs to be computed. Consequently  updating
Dw takes Θ(m) time.
The computation of the summations over the m examples in Equation 7 or 10 takes Θ(m) time.
Once these summations are computed  solving Equation 7 or 10 takes Θ(1) time. Consequently 
it takes Θ(m) time to perform one basic iteration of the learning algorithm which consist of (1)
solving Equation 7 or 10 to ﬁnd δ∗  (2) modifying w1  Q1  Qn+1  and (3) updating Dw according to
Equation 11 or 12. The complete algorithm  called f minimization  is described by the pseudo code
of Algorithm 1.

(11)
(12)

1Dw(i) stands for either Dql

w(i) or Del

w(i).

5

Algorithm 1 : f minimization
1: Initialization: Let Qi = Qn+i = 1

2n   wi = 0  ∀i ∈ {1  . . .   n}.

Initialize Dw according to Equation 13 or 14.

2: repeat
3:

4:

5:

6:

7:

n < w1 + δ∗ < 1

Choose at random h ∈ H and call it h1 (hn+1 is then the boolean complement of h1).
Find δ∗ that solves Equation 7 or 10.
If [−1
If [w1 + δ∗ ≥ 1
n] then Q1 ← 1
If [w1 + δ∗ ≤ −1
n ] then Q1 ← 0; Qn+1 ← 1
Update Dw according to Equation 11 or 12.

n] then Q1 ← Q1 + δ/2; Qn+1 ← Qn+1 − δ/2; w1 ← w1 + δ.

n; Qn+1 ← 0; w1 ← 1
n.
n; w1 ← −1
n .

8:
9: until Convergence

The repeat-until loop in Algorithm 1 was implemented as follows. We ﬁrst mix at random the n
boolean complement pairs of classiﬁers and then go sequentially over each pair (hi  hn+i) to update
wi and Dw. We repeat this sequence until no weight change exceeds a speciﬁed small number .
4.2 From KL(Q(cid:107)P ) to (cid:96)p Regularization
We can recover (cid:96)2 regularization if we upper-bound KL(Q(cid:107)P ) by a quadratic function. Indeed  if
we use

(cid:18) 1

n

(cid:19)

(cid:18) 1

n

q ln q +

− q

ln

− q

ln

1
2n

+ 4n

we obtain  for the uniform prior Pi = 1/(2n) 

KL(Q(cid:107)P ) = ln(2n) +

Qi ln Qi +

With this approximation  the objective function to minimize becomes

i=1

≤ 4n

n(cid:88)

(cid:18)
f(cid:96)2(w) = C(cid:48)(cid:48) m(cid:88)

i=1

Qi − 1
2n

(cid:19)2
(cid:18) 1

ζ

γ

i=1

= n

w2
i .

yiw · h(xi)

+ (cid:107)w(cid:107)2
2  

(cid:19)
≤ 1
n
(cid:20)
n(cid:88)

(cid:19)2 ∀q ∈ [0  1/n]  
(cid:18) 1
(cid:19)(cid:21)

− Qi

ln

n

q − 1
2n
(cid:19)

− Qi

(cid:18)
(cid:18) 1
n(cid:88)

n

i=1

(cid:19)

(15)

(16)

(17)

subject to the (cid:96)∞ constraint |wj| ≤ 1/n ∀j ∈ {1  . . .   n}. Here (cid:107)w(cid:107)2 denotes the Euclidean norm
of w and ζ(x) = (x − 1)2 for the quadratic loss and e−x for the exponential loss.
If  instead  we minimize f(cid:96)2 for v def= w/γ and remove the (cid:96)∞ constraint  we recover exactly ridge
regression for the quadratic loss case and (cid:96)2-regularized boosting for the exponential loss case.
We can obtain a (cid:96)1-regularized version of Equation 17 by repeating the above steps and us-

(cid:12)(cid:12) ∀q ∈ [0  1/n] since  in that case  we ﬁnd that KL(Q(cid:107)P ) ≤

(cid:1)2 ≤ 2(cid:12)(cid:12)q − 1

2n

ing 4n(cid:0)q − 1
(cid:80)n
i=1 |wi| def= (cid:107)w(cid:107)1.

2n

To sum up  the KL-regularized objective function f immediately follows from PAC-Bayes theory
and (cid:96)p regularization is obtained from a relaxation of f. Consequently  PAC-Bayes theory favors
the use of KL regularization if the goal of the learner is to produce a weighted majority vote with
good generalization.2

2Interestingly  [9] has recently proposed a KL-regularized version of LPBoost but their objective function

was not derived from a uniform risk bound.

6

5 Empirical Results

For the sake of comparison  all learning algorithms of this subsection are producing a weighted
majority vote classiﬁer on the set of basis functions {h1  . . .   hn} known as decision stumps. Each
decision stump hi is a threshold classiﬁer that depends on a single attribute: its output is +b if
the tested attribute exceeds a threshold value t  and −b otherwise  where b ∈ {−1  +1}. For each
attribute  at most ten equally-spaced possible values for t were determined a priori. Recall that 
although Algorithm 1 needs a set H of 2n classiﬁers containing n boolean complement pairs  it
outputs a majority vote with n real-valued weights deﬁned on {h1  . . .   hn}.
The results obtained for all tested algorithms are summarized in Table 1. We have compared Al-
gorithm 1 with quadratic loss (KL-QL) and exponential loss (KL-EL) to AdaBoost [7] (AdB) and
ridge regression (RR).
Except for MNIST  all data sets were taken from the UCI repository. Each data set was randomly
split into a training set S of |S| examples and a testing set T of |T| examples. The number a
of attributes for each data set is also speciﬁed in Table 1. For AdaBoost  the number of boosting
rounds was ﬁxed to 200. For all algorithms  RT refers to the frequency of errors  measured on the
testing set T .
In addition to this  the “C and “γ” columns in Table 1 refer  respectively  to the C value of the
objective function f and to the γ parameter present in the loss functions. These hyperparameters
were determined from the training set only by performing the 10-fold cross validation (CV) method.
The hyperparameters that gave the smallest 10-fold CV error were then used to train the Algorithms
on the whole training set and the resulting classiﬁers were then run on the testing set.

Table 1: Summary of results.

(1) AdB (2) RR

C

1

(3) KL-EL
γ
0.1
0.360 0.5 0.02
0.227 0.1
0.2
0.187 500 0.01
0.2
0.1

(4) KL-QL
RT
γ
RT C RT C
0.047 0.02 0.4
0.050 10 0.047 0.1
0.286 0.02 0.3
0.309 5
0.157 2
0.183 0.02 0.05
0.196 0.02 0.01
0.206 5
0.273 100 0.253 500
0.260 0.02 0.5
0.177 0.05 0.2
0.197 1
0.211 0.2
0.131 0.05 0.120 20 0.0001 0.097 0.2 0.1
0.004 0.5 0.006 0.1 0.02
0.006 1000 0.1
0.026 0.05 0.019 500 0.01
0.020 0.02 0.05
0.045 0.5 0.043 10 0.0001 0.047 0.1 0.05
0.015 0.05 0.006 500 0.001 0.015 0.2 0.02
0.012 1
0.014 1000 0.1
0.024 0.2 0.016 0.2 0.001 0.031
0.02
0.033 0.2 0.035 500 0.0001 0.029 0.02 0.05
0.001 0.5 0.000 10 0.001 0.000 1000 0.02
0.037 0.05 0.025 500 0.01
0.039 0.05 0.05
0.115 1000 0.1
0.192 0.05 0.135 500 0.05
0.055 1000 0.05
0.060 2
0.1
0.079 0.02 0.080 0.2 0.05
0.080 0.02 0.05
0.049 0.2 0.039 500 0.02
0.046 1000 0.1

0.014 500 0.02

0.060 0.5

Dataset
|S|

|T|
Name
a
9
BreastCancer 343 340
170 175
Liver
6
353 300 15
Credit-A
Glass
107 107
9
144 150
Haberman
3
150 147 13
Heart
176 175 34
Ionosphere
500 1055 16
Letter:AB
Letter:DO
500 1058 16
Letter:OQ
500 1036 16
MNIST:0vs8 500 1916 784
MNIST:1vs7 500 1922 784
MNIST:1vs8 500 1936 784
MNIST:2vs3 500 1905 784
Mushroom 4062 4062 22
3700 3700 20
Ringnorm
104 104 60
Sonar
Usvotes
235 200 16
4000 4000 21
Waveform
Wdbc
285 284 30

RT
0.053
0.320
0.170
0.178
0.260
0.252
0.120
0.010
0.036
0.038
0.008
0.013
0.025
0.047
0.000
0.043
0.231
0.055
0.085
0.049

SSB

(3) < (2  4)

(3) < (4)
(4) < (1)

(3) < (1  2  4)

We clearly see that the cross-validation method generally chooses very small values for γ. This  in
turn  gives a risk bound (computed from Theorem 3.2) having very large values (results not shown
here). We have also tried to choose C and γ from the risk bound values.3 This method for selecting
hyperparameters turned out to produce classiﬁers having larger testing errors (results not shown
here).
To determine whether or not a difference of empirical risk measured on the testing set T is statisti-
cally signiﬁcant  we have used the test set bound method of [4] (based on the binomial tail inversion)
3From the standard union bound argument  the bound of Theorem 3.2 holds simultaneously for k different

choices of (γ  C) if we replace δ by δ/k.

7

with a conﬁdence level of 95%. It turns out that no algorithm has succeeded in choosing a majority
vote classiﬁer which was statistically signiﬁcantly better (SSB) than the one chosen by another al-
gorithm except for the 4 cases that are listed in the column “SSB” of Table 1. We see that on these
cases  Algorithm 1 turned out to be statistically signiﬁcantly better.

6 Conclusion

Our numerical results indicate that Algorithm 1 generally outperforms AdaBoost and ridge regres-
sion when the hyperparameters C and γ are chosen by cross-validation. This indicates that the

empirical loss(cid:99)ζQ and the KL(Q(cid:107)P ) regularizer that are present in the PAC-Bayes bound of Theo-
PAC-Bayes theory does not yet capture quantitatively the proper tradeoff between(cid:99)ζQ and KL(Q(cid:107)P )

rem 3.2 are key ingredients for learning algorithms to focus on. The fact that cross-validation turns
out to be more efﬁcient than Theorem 3.2 at selecting good values for hyperparameters indicates that

that learners should optimize on the trading data. However  we feel that it is important to pursue
this research direction since it could potentially eliminate the need to perform the time-consuming
cross-validation method for selecting hyperparameters and provide better guarantees on the gener-
alization error of classiﬁers output by learning algorithms. In short  it could perhaps yield the best
generic optimization problem for learning.

Acknowledgments

Work supported by NSERC discovery grants 122405 (M.M.) and 262067 (F.L.).

References
[1] Olivier Catoni.

tistical
Monograph series of
http://arxiv.org/abs/0712.0248  December 2007.

learning.

PAC-Bayesian surpevised classiﬁcation:

the thermodynamics of sta-
the Institute of Mathematical Statistics 

[2] Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  and Mario Marchand. A pac-bayes
risk bound for general loss functions. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Ad-
vances in Neural Information Processing Systems 19  pages 449–456. MIT Press  Cambridge 
MA  2007.

[3] Pascal Germain  Alexandre Lacasse  Franc¸ois Laviolette  and Mario Marchand. PAC-Bayesian
In L´eon Bottou and Michael Littman  editors  Proceedings of
learning of linear classiﬁers.
the 26th International Conference on Machine Learning  pages 353–360  Montreal  June 2009.
Omnipress.

[4] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine

Learning Research  6:273–306  2005.

[5] John Langford and John Shawe-Taylor. PAC-Bayes & margins.

In S. Thrun S. Becker and
K. Obermayer  editors  Advances in Neural Information Processing Systems 15  pages 423–430.
MIT Press  Cambridge  MA  2003.

[6] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning  51:5–21 

2003.

[7] Robert E. Schapire  Yoav Freund  Peter Bartlett  and Wee Sun Lee. Boosting the margin: A new
explanation for the effectiveness of voting methods. The Annals of Statistics  26:1651–1686 
1998.

[8] Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of Ma-

chine Learning Research  3:233–269  2002.

[9] Manfred K. Warmuth  Karen A. Glocer  and S.V.N. Vishwanathan. Entropy regularized LP-
Boost. In Proceedings of the 2008 conference on Algorithmic Learning Theory  Springer LNAI
5254   pages 256–271  2008.

8

,Xiang Zhang
Junbo Zhao
Yann LeCun
Novi Quadrianto
Viktoriia Sharmanska