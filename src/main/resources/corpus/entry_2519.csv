2018,Hyperbolic Neural Networks,Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However  the representational power of hyperbolic geometry is not yet on par with Euclidean geometry  firstly because of the absence of corresponding hyperbolic neural network layers. Here  we bridge this gap in a principled manner by combining the formalism of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model of hyperbolic spaces. As a result  we derive hyperbolic versions of important deep learning tools: multinomial logistic regression  feed-forward and recurrent neural networks. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically  we show that  even if hyperbolic optimization tools are limited  hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.,Hyperbolic Neural Networks

Octavian-Eugen Ganea∗
Dept. of Computer Science

ETH Zürich

Zurich  Switzerland

Dept. of Computer Science

Dept. of Computer Science

Thomas Hofmann

ETH Zürich

Zurich  Switzerland

Gary Bécigneul∗

ETH Zürich

Zurich  Switzerland

Abstract

Hyperbolic spaces have recently gained momentum in the context of machine
learning due to their high capacity and tree-likeliness properties. However  the
representational power of hyperbolic geometry is not yet on par with Euclidean
geometry  mostly because of the absence of corresponding hyperbolic neural
network layers. This makes it hard to use hyperbolic embeddings in downstream
tasks. Here  we bridge this gap in a principled manner by combining the formalism
of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model
of hyperbolic spaces. As a result  we derive hyperbolic versions of important deep
learning tools: multinomial logistic regression  feed-forward and recurrent neural
networks such as gated recurrent units. This allows to embed sequential data and
perform classiﬁcation in the hyperbolic space. Empirically  we show that  even if
hyperbolic optimization tools are limited  hyperbolic sentence embeddings either
outperform or are on par with their Euclidean variants on textual entailment and
noisy-preﬁx recognition tasks.

1

Introduction

It is common in machine learning to represent data as being embedded in the Euclidean space Rn. The
main reason for such a choice is simply convenience  as this space has a vectorial structure  closed-
form formulas of distance and inner-product  and is the natural generalization of our intuition-friendly 
visual three-dimensional space. Moreover  embedding entities in such a continuous space allows to
feed them as input to neural networks  which has led to unprecedented performance on a broad range
of problems  including sentiment detection [15]  machine translation [3]  textual entailment [22] or
knowledge base link prediction [20  6].
Despite the success of Euclidean embeddings  recent research has proven that many types of com-
plex data (e.g. graph data) from a multitude of ﬁelds (e.g. Biology  Network Science  Computer
Graphics or Computer Vision) exhibit a highly non-Euclidean latent anatomy [8]. In such cases  the
Euclidean space does not provide the most powerful or meaningful geometrical representations. For
example  [10] shows that arbitrary tree structures cannot be embedded with arbitrary low distortion
(i.e. almost preserving their metric) in the Euclidean space with unbounded number of dimensions 
but this task becomes surprisingly easy in the hyperbolic space with only 2 dimensions where the
exponential growth of distances matches the exponential growth of nodes with the tree depth.
The adoption of neural networks and deep learning in these non-Euclidean settings has been rather
limited until very recently  the main reason being the non-trivial or impossible principled general-
izations of basic operations (e.g. vector addition  matrix-vector multiplication  vector translation 
vector inner product) as well as  in more complex geometries  the lack of closed form expressions for
basic objects (e.g. distances  geodesics  parallel transport). Thus  classic tools such as multinomial

∗Equal contribution  correspondence at {octavian.ganea gary.becigneul}@inf.ethz.ch

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

logistic regression (MLR)  feed forward (FFNN) or recurrent neural networks (RNN) did not have a
correspondence in these geometries.
How should one generalize deep neural models to non-Euclidean domains ? In this paper we address
this question for one of the simplest  yet useful  non-Euclidean domains: spaces of constant negative
curvature  i.e. hyperbolic. Their tree-likeness properties have been extensively studied [12  13  26]
and used to visualize large taxonomies [18] or to embed heterogeneous complex networks [17]. In
machine learning  recently  hyperbolic representations greatly outperformed Euclidean embeddings
for hierarchical  taxonomic or entailment data [21  10  11]. Disjoint subtrees from the latent hierar-
chical structure surprisingly disentangle and cluster in the embedding space as a simple reﬂection of
the space’s negative curvature. However  appropriate deep learning tools are needed to embed feature
data in this space and use it in downstream tasks. For example  implicitly hierarchical sequence data
(e.g. textual entailment data  phylogenetic trees of DNA sequences or hierarchial captions of images)
would beneﬁt from suitable hyperbolic RNNs.
The main contribution of this paper is to bridge the gap between hyperbolic and Euclidean geometry
in the context of neural networks and deep learning by generalizing in a principled manner both the
basic operations as well as multinomial logistic regression (MLR)  feed-forward (FFNN)  simple and
gated (GRU) recurrent neural networks (RNN) to the Poincaré model of the hyperbolic geometry.
We do it by connecting the theory of gyrovector spaces and generalized Möbius transformations
introduced by [2  26] with the Riemannian geometry properties of the manifold. We smoothly
parametrize basic operations and objects in all spaces of constant negative curvature using a uniﬁed
framework that depends only on the curvature value. Thus  we show how Euclidean and hyperbolic
spaces can be continuously deformed into each other. On a series of experiments and datasets we
showcase the effectiveness of our hyperbolic neural network layers compared to their "classic"
Euclidean variants on textual entailment and noisy-preﬁx recognition tasks. We hope that this paper
will open exciting future directions in the nascent ﬁeld of Geometric Deep Learning.

2 The Geometry of the Poincaré Ball

Basics of differential geometry are presented in appendix A.

2.1 Hyperbolic space: the Poincaré ball

The hyperbolic space has ﬁve isometric models that one can work with [9]. Similarly as in [21] and
[11]  we choose to work in the Poincaré ball. The Poincaré ball model (Dn  g
) is deﬁned by the
manifold Dn = {x ∈ Rn : (cid:107)x(cid:107) < 1} equipped with the following Riemannian metric:

D

D
x = λ2
g

xgE  where λx :=

2

1 − (cid:107)x(cid:107)2  

(1)

(3)

gE = In being the Euclidean metric tensor. Note that the hyperbolic metric tensor is conformal to
the Euclidean one. The induced distance between two points x  y ∈ Dn is known to be given by

dD(x  y) = cosh

(2)
Since the Poincaré ball is conformal to Euclidean space  the angle between two vectors u  v ∈
TxDn \ {0} is given by

(1 − (cid:107)x(cid:107)2)(1 − (cid:107)y(cid:107)2)

1 + 2

(cid:18)

−1

(cid:107)x − y(cid:107)2

(cid:19)

.

cos(∠(u  v)) =

2.2 Gyrovector spaces

(cid:112)gD
x (u  u)(cid:112)gD

D
g
x (u  v)

x (v  v)

(cid:104)u  v(cid:105)
(cid:107)u(cid:107)(cid:107)v(cid:107) .

=

In Euclidean space  natural operations inherited from the vectorial structure  such as vector addition 
subtraction and scalar multiplication are often useful. The framework of gyrovector spaces provides
an elegant non-associative algebraic formalism for hyperbolic geometry just as vector spaces provide
the algebraic setting for Euclidean geometry [2  25  26].
In particular  these operations are used in special relativity  allowing to add speed vectors belonging
to the Poincaré ball of radius c (the celerity  i.e. the speed of light) so that they remain in the ball 
hence not exceeding the speed of light.

2

We will make extensive use of these operations in our deﬁnitions of hyperbolic neural networks.
For c ≥ 0  denote2 by Dn
then Dn
c  c(cid:48) > 0  Dn
Möbius addition. The Möbius addition of x and y in Dn

c = Rn; if c > 0 
c. If c = 1 then we recover the usual ball Dn. Note that for

√
c is the open ball of radius 1/
c(cid:48) are isometric.

c := {x ∈ Rn | c(cid:107)x(cid:107)2 < 1}. Note that if c = 0  then Dn

c and Dn

c is deﬁned as

x ⊕c y :=

(1 + 2c(cid:104)x  y(cid:105) + c(cid:107)y(cid:107)2)x + (1 − c(cid:107)x(cid:107)2)y

1 + 2c(cid:104)x  y(cid:105) + c2(cid:107)x(cid:107)2(cid:107)y(cid:107)2

.

(4)

In particular  when c = 0  one recovers the Euclidean addition of two vectors in Rn. Note that
without loss of generality  the case c > 0 can be reduced to c = 1. Unless stated otherwise  we
will use ⊕ as ⊕1 to simplify notations. For general c > 0  this operation is not commutative nor
associative. However  it satisﬁes x ⊕c 0 = 0 ⊕c x = x. Moreover  for any x  y ∈ Dn
c   we have
(−x) ⊕c x = x ⊕c (−x) = 0 and (−x) ⊕c (x ⊕c y) = y (left-cancellation law). The Möbius
substraction is then deﬁned by the use of the following notation: x (cid:9)c y := x ⊕c (−y). See [29 
section 2.1] for a geometric interpretation of the Möbius addition.
Möbius scalar multiplication. For c > 0  the Möbius scalar multiplication of x ∈ Dn
r ∈ R is deﬁned as

c \ {0} by

(5)
and r ⊗c 0 := 0. Note that similarly as for the Möbius addition  one recovers the Euclidean scalar
multiplication when c goes to zero: limc→0 r ⊗c x = rx. This operation satisﬁes desirable properties
such as n⊗c x = x⊕c···⊕c x (n additions)  (r + r(cid:48))⊗c x = r⊗c x⊕c r(cid:48)⊗c x (scalar distributivity3) 
(rr(cid:48)) ⊗c x = r ⊗c (r(cid:48) ⊗c x) (scalar associativity) and |r| ⊗c x/(cid:107)r ⊗c x(cid:107) = x/(cid:107)x(cid:107) (scaling property).

c) tanh(r tanh

√
r ⊗c x := (1/

√
−1(

c(cid:107)x(cid:107)))

x
(cid:107)x(cid:107)  

c   gc) is given by4

Distance.
Euclidean one  with conformal factor λc
(Dn

If one deﬁnes the generalized hyperbolic metric tensor gc as the metric conformal to the
x := 2/(1 − c(cid:107)x(cid:107)2)  then the induced distance function on
√
dc(x  y) = (2/

(6)
Again  observe that limc→0 dc(x  y) = 2(cid:107)x − y(cid:107)  i.e. we recover Euclidean geometry in the limit5.
Moreover  for c = 1 we recover dD of Eq. (2).

c(cid:107) − x ⊕c y(cid:107)(cid:1) .

−1(cid:0)√

c) tanh

Hyperbolic trigonometry. Similarly as in the Euclidean space  one can deﬁne the notions of
hyperbolic angles or gyroangles (when using the ⊕c)  as well as hyperbolic law of sines in the
generalized Poincaré ball (Dn
c   gc). We make use of these notions in our proofs. See Appendix B.

2.3 Connecting Gyrovector spaces and Riemannian geometry of the Poincaré ball

In this subsection  we present how geodesics in the Poincaré ball model are usually described with
Möbius operations  and push one step further the existing connection between gyrovector spaces and
the Poincaré ball by ﬁnding new identities involving the exponential map  and parallel transport.
In particular  these ﬁndings provide us with a simpler formulation of Möbius scalar multiplication 
yielding a natural deﬁnition of matrix-vector multiplication in the Poincaré ball.

Riemannian gyroline element. The Riemannian gyroline element is deﬁned for an inﬁnitesimal
dx as ds := (x + dx) (cid:9)c x  and its size is given by [26  section 3.7]:

(cid:107)ds(cid:107) = (cid:107)(x + dx) (cid:9)c x(cid:107) = (cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2).

(7)
What is remarkable is that it turns out to be identical  up to a scaling factor of 2  to the usual line
element 2(cid:107)dx(cid:107)/(1 − c(cid:107)x(cid:107)2) of the Riemannian manifold (Dn

c   gc).
√
2We take different notations as in [25] where the author uses s = 1/
c.
3⊗c has priority over ⊕c in the sense that a ⊗c b ⊕c c := (a ⊗c b) ⊕c c and a ⊕c b ⊗c c := a ⊕c (b ⊗c c).
4The notation −x ⊕c y should always be read as (−x) ⊕c y and not −(x ⊕c y).
5The factor 2 comes from the conformal factor λx = 2/(1 − (cid:107)x(cid:107)2)  which is a convention setting the

curvature to −1.

3

Geodesics. The geodesic connecting points x  y ∈ Dn
γx→y(t) := x ⊕c (−x ⊕c y) ⊗c t  with γx→y : R → Dn

c is shown in [2  26] to be given by:

c s.t. γx→y(0) = x and γx→y(1) = y.

(cid:18)

(8)
Note that when c goes to 0  geodesics become straight-lines  recovering Euclidean geometry. In the
remainder of this subsection  we connect the gyrospace framework with Riemannian geometry.
Lemma 1. For any x ∈ Dn and v ∈ TxDn
x with direction v is given by:
γx v(t) = x ⊕c
t
2

  where γx v : R → Dn s.t. γx v(0) = x and ˙γx v(0) = v.
(9)
Proof. One can use Eq. (8) and reparametrize it to unit-speed using Eq. (6). Alternatively  direct
computation and identiﬁcation with the formula in [11  Thm. 1] would give the same result. Using
Eq. (6) and Eq. (9)  one can sanity-check that dc(γ(0)  γ(t)) = t ∀t ∈ [0  1].

x(v  v) = 1  the unit-speed geodesic starting from

(cid:19) v√

(cid:18)√

c s.t. gc

c(cid:107)v(cid:107)

(cid:19)

tanh

c

Exponential and logarithmic maps. The following lemma gives the closed-form derivation of
exponential and logarithmic maps.
Lemma 2. For any point x ∈ Dn
x : Dn
map logc
x(cid:107)v(cid:107)
λc
x(v) = x ⊕c
2

c are given for v (cid:54)= 0 and y (cid:54)= x by:
x(y) =

(cid:18)
c → TxDn

c   the exponential map expc

c → Dn
√

(cid:19) v√

c and the logarithmic

c(cid:107) − x ⊕c y(cid:107))

x : TxDn

(cid:18)√

c(cid:107)v(cid:107)

  logc

(cid:19)

−1(

tanh

tanh

c

2√
cλc
x

−x ⊕c y
(cid:107) − x ⊕c y(cid:107) .

expc

Proof. Following the proof of [11  Cor. 1.1]  one gets expc
gives the formula for expc
The above maps have more appealing forms when x = 0  namely for v ∈ T0Dn

x. Algebraic check of the identity logc

x(v) = γx 
x(expc

x(cid:107)v(cid:107) (λc
λc

v

x(v)) = v concludes.

c \ {0}  y ∈ Dn

c \ {0}:

(10)
x(cid:107)v(cid:107)). Using Eq. (9)

expc

0(v) = tanh(

√

c(cid:107)v(cid:107))

v√
c(cid:107)v(cid:107)   logc

0(y) = tanh

√
−1(

c(cid:107)y(cid:107))

y√
c(cid:107)y(cid:107) .
x(v) = x + v is the

(11)

Moreover  we still recover Euclidean geometry in the limit c → 0  as limc→0 expc
Euclidean exponential map  and limc→0 logc

x(y) = y − x is the Euclidean logarithmic map.

Möbius scalar multiplication using exponential and logarithmic maps. We studied the expo-
nential and logarithmic maps in order to gain a better understanding of the Möbius scalar multiplica-
tion (Eq. (5)). We found the following:
Lemma 3. The quantity r ⊗ x can actually be obtained by projecting x in the tangent space at 0
with the logarithmic map  multiplying this projection by the scalar r in T0Dn
c   and then projecting it
back on the manifold with the exponential map:
0(r logc

(12)
In addition  we recover the well-known relation between geodesics connecting two points and the
exponential map:

∀r ∈ R  x ∈ Dn
c .

r ⊗c x = expc

0(x)) 

γx→y(t) = x ⊕c (−x ⊕c y) ⊗c t = expc

x(t logc

x(y)) 

t ∈ [0  1].

(13)

This last result enables us to generalize scalar multiplication in order to deﬁne matrix-vector multipli-
cation between Poincaré balls  one of the essential building blocks of hyperbolic neural networks.

Parallel transport. Finally  we connect parallel transport along the unique geodesic from 0 to x to
gyrovector spaces with the following theorem  which we prove in appendix C.
Theorem 4. In the manifold (Dn
vector v ∈ T0Dn

c   gc)  the parallel transport w.r.t. the Levi-Civita connection of a

c to another tangent space TxDn

c is given by the following isometry:

0→x(v) = logc
P c

x(x ⊕c expc

0(v)) =

λc
0
λc
x

v.

(14)

As we’ll see later  this result is crucial in order to deﬁne and optimize parameters shared between
different tangent spaces  such as biases in hyperbolic neural layers or parameters of hyperbolic MLR.

4

3 Hyperbolic Neural Networks

Neural networks can be seen as being made of compositions of basic operations  such as linear
maps  bias translations  pointwise non-linearities and a ﬁnal sigmoid or softmax layer. We ﬁrst
explain how to construct a softmax layer for logits lying in a Poincaré ball. Then  we explain how
to transform a mapping between two Euclidean spaces as one between Poincaré balls  yielding
matrix-vector multiplication and pointwise non-linearities in the Poincaré ball. Finally  we present
possible adaptations of various recurrent neural networks to the hyperbolic domain.

3.1 Hyperbolic multiclass logistic regression

In order to perform multi-class classiﬁcation on the Poincaré ball  one needs to generalize multinomial
logistic regression (MLR) − also called softmax regression − to the Poincaré ball.

Reformulating Euclidean MLR. Let’s ﬁrst reformulate Euclidean MLR from the perspective of
distances to margin hyperplanes  as in [19  Section 5]. This will allow us to easily generalize it.
Given K classes  one learns a margin hyperplane for each such class using softmax probabilities:

∀k ∈ {1  ...  K} 

p(y = k|x) ∝ exp (((cid:104)ak  x(cid:105) − bk))   where bk ∈ R  x  ak ∈ Rn.

(15)

Note that any afﬁne hyperplane in Rn can be written with a normal vector a and a scalar shift b:

Ha b = {x ∈ Rn : (cid:104)a  x(cid:105) − b = 0}  where a ∈ Rn \ {0}  and b ∈ R.

As in [19  Section 5]  we note that (cid:104)a  x(cid:105) − b = sign((cid:104)a  x(cid:105) − b)(cid:107)a(cid:107)d(x  Ha b). Using Eq. (15):

p(y = k|x) ∝ exp(sign((cid:104)ak  x(cid:105) − bk)(cid:107)ak(cid:107)d(x  Hak bk ))  bk ∈ R  x  ak ∈ Rn.

(16)

(17)

As it is not immediately obvious how to generalize the Euclidean hyperplane of Eq. (16) to other
spaces such as the Poincaré ball  we reformulate it as follows:

˜Ha p = {x ∈ Rn : (cid:104)−p + x  a(cid:105) = 0} = p + {a}⊥  where p ∈ Rn  a ∈ Rn \ {0}.

(18)
This new deﬁnition relates to the previous one as ˜Ha p = Ha (cid:104)a p(cid:105). Rewriting Eq. (17) with b = (cid:104)a  p(cid:105):
(19)

p(y = k|x) ∝ exp(sign((cid:104)−pk + x  ak(cid:105))(cid:107)ak(cid:107)d(x  ˜Hak pk ))  with pk  x  ak ∈ Rn.

It is now natural to adapt the previous deﬁnition to the hyperbolic setting by replacing + by ⊕c:
Deﬁnition 3.1 (Poincaré hyperplanes). For p ∈ Dn
c \ {0}  let {a}⊥ := {z ∈ TpDn
c :
p(z  a) = 0} = {z ∈ TpDn
gc
c : (cid:104)logc
a p := {x ∈ Dn
˜H c

c : (cid:104)z  a(cid:105) = 0}. Then  we deﬁne6 Poincaré hyperplanes as
p(x)  a(cid:105)p = 0} = expc

c : (cid:104)−p ⊕c x  a(cid:105) = 0}.

p({a}⊥) = {x ∈ Dn

c   a ∈ TpDn

(20)

a p can also be described as the union of images of
c orthogonal to a and containing p. Notice that our deﬁnition matches that of

The last equality is shown appendix D. ˜H c
all geodesics in Dn
hypergyroplanes  see [27  deﬁnition 5.8]. A 3D hyperplane example is depicted in Fig. 1.
Next  we need the following theorem  proved in appendix E:
Theorem 5.

dc(x  ˜H c

a p) := inf
w∈ ˜H c

a p

dc(x  w) =

−1

sinh

1√
c

c|(cid:104)−p ⊕c x  a(cid:105)|

(1 − c(cid:107) − p ⊕c x(cid:107)2)(cid:107)a(cid:107)

.

(21)

(cid:18) 2

√

(cid:19)

Final formula for MLR in the Poincaré ball. Putting together Eq. (19) and Thm. 5  we get the
hyperbolic MLR formulation. Given K classes and k ∈ {1  . . .   K}  pk ∈ Dn
c \ {0}:
Dn
(22)

p(y = k|x) ∝ exp(sign((cid:104)−pk ⊕c x  ak(cid:105))

c   ak ∈ Tpk
∀x ∈ Dn
c  

(ak  ak)dc(x  ˜H c
6where (cid:104)· ·(cid:105) denotes the (Euclidean) inner-product of the ambient space.

(cid:113)

gc
pk

)) 

ak pk

5

√
2

c(cid:104)−pk ⊕c x  ak(cid:105)

(cid:19)(cid:19)

or  equivalently

p(y = k|x) ∝ exp

(cid:18) λc

(cid:107)ak(cid:107)√

pk

c

(cid:18)

−1

sinh

(1 − c(cid:107) − pk ⊕c x(cid:107)2)(cid:107)ak(cid:107)

(23)
this goes to p(y = k|x) ∝ exp(4(cid:104)−pk + x  ak(cid:105)) =

 

∀x ∈ Dn
c .

Notice that when c goes to zero 
)2(cid:104)−pk + x  ak(cid:105)) = exp((cid:104)−pk + x  ak(cid:105)0)  recovering the usual Euclidean softmax.
exp((λ0
pk
Dn
However  at this point it is unclear how to perform optimization over ak  since it lives in Tpk
c and
)a(cid:48)
k  where
hence depends on pk. The solution is that one should write ak = P c
k ∈ T0Dn
a(cid:48)
3.2 Hyperbolic feed-forward layers

c = Rn  and optimize a(cid:48)

k as a Euclidean parameter.

k) = (λc

0/λc
pk

0→pk

(a(cid:48)

In order to deﬁne hyperbolic neural networks  it is crucial to de-
ﬁne a canonically simple parametric family of transformations 
playing the role of linear mappings in usual Euclidean neural
networks  and to know how to apply pointwise non-linearities.
Inspiring ourselves from our reformulation of Möbius scalar
multiplication in Eq. (12)  we deﬁne:
Deﬁnition 3.2 (Möbius version). For f : Rn → Rm  we deﬁne
the Möbius version of f as the map from Dn
0(x))) 
0 : Dn

f⊗c(x) := expc
c → Dm
Dm

c → T0n

c and logc

c to Dm

where expc

0(f (logc

c by:

0 : T0m

Dn
c .

(24)

Figure 1: An example of a hyper-
bolic hyperplane in D3
1 plotted us-
ing sampling. The red point is p.
The shown normal axis to the hy-
perplane through p is parallel to a.

Note that similarly as for other Möbius operations  we recover
the Euclidean mapping in the limit c → 0 if f is continuous  as limc→0 f⊗c(x) = f (x). This
deﬁnition satisﬁes a few desirable properties too  such as: (f ◦ g)⊗c = f⊗c ◦ g⊗c for f : Rm → Rl
and g : Rn → Rm (morphism property)  and f⊗c(x)/(cid:107)f⊗c(x)(cid:107) = f (x)/(cid:107)f (x)(cid:107) for f (x) (cid:54)= 0
(direction preserving). It is then straight-forward to prove the following result:
Lemma 6 (Möbius matrix-vector multiplication). If M : Rn → Rm is a linear map  which we
identify with its matrix representation  then ∀x ∈ Dn

c   if M x (cid:54)= 0 we have

√
M⊗c(x) = (1/

c) tanh

(cid:18)(cid:107)M x(cid:107)

(cid:107)x(cid:107) tanh

√
−1(

c(cid:107)x(cid:107))

(cid:19) M x

(cid:107)M x(cid:107)  

(25)

and M⊗c(x) = 0 if M x = 0. Moreover  if we deﬁne the Möbius matrix-vector multiplication of
M ∈ Mm n(R) and x ∈ Dn
c by M ⊗c x := M⊗c(x)  then we have (M M(cid:48))⊗c x = M ⊗c (M(cid:48)⊗c x)
for M ∈ Ml m(R) and M(cid:48) ∈ Mm n(R) (matrix associativity)  (rM ) ⊗c x = r ⊗c (M ⊗c x) for
r ∈ R and M ∈ Mm n(R) (scalar-matrix associativity) and M ⊗c x = M x for all M ∈ On(R)
(rotations are preserved).

Pointwise non-linearity.
ϕ⊗c can be applied to elements of the Poincaré ball.

If ϕ : Rn → Rn is a pointwise non-linearity  then its Möbius version

Bias translation. The generalization of a translation in the Poincaré ball is naturally given by
moving along geodesics. But should we use the Möbius sum x ⊕c b with a hyperbolic bias b or the
x(b(cid:48)) with a Euclidean bias b(cid:48)? These views are uniﬁed with parallel transport
exponential map expc
(see Thm 4). Möbius translation of a point x ∈ Dn

c by a bias b ∈ Dn

c is given by

x ← x ⊕c b = expc

x(P c

0→x(logc

0(b))) = expc
x

(26)
We recover Euclidean translations in the limit c → 0. Note that bias translations play a particular
Indeed  consider multiple layers of the form fk(x) = ϕk(Mkx)  each of
role in this model.
k (Mk ⊗c x). Then their composition can be re-written
⊗c
⊗c
which having Möbius version f
k (x) = ϕ
0 ◦fk ◦ ··· ◦ f1 ◦ logc
k ◦ ··· ◦ f
⊗c
0. This means that these operations can essentially be
f
performed in Euclidean space. Therefore  it is the interposition between those with the bias translation
of Eq. (26) which differentiates this model from its Euclidean counterpart.

⊗c
1 = expc

logc

0(b)

.

(cid:19)

(cid:18) λc

0
λc
x

6

If a vector x ∈ Rn+p is the (vertical) concatenation
Concatenation of multiple input vectors.
of two vectors x1 ∈ Rn  x2 ∈ Rp  and M ∈ Mm n+p(R) can be written as the (horizontal)
concatenation of two matrices M1 ∈ Mm n(R) and M2 ∈ Mm p(R)  then M x = M1x1 + M2x2.
We generalize this to hyperbolic spaces: if we are given x1 ∈ Dn
c ×Dp
c 
and M  M1  M2 as before  then we deﬁne M ⊗c x := M1 ⊗c x1 ⊕c M2 ⊗c x2. Note that when c goes
to zero  we recover the Euclidean formulation  as limc→0 M ⊗c x = limc→0 M1⊗c x1⊕c M2⊗c x2 =
M1x1 + M2x2 = M x. Moreover  hyperbolic vectors x ∈ Dn
c can also be "concatenated" with real
features y ∈ R by doing: M ⊗c x ⊕c y ⊗c b with learnable b ∈ Dm

c  x = (x1 x2)T ∈ Dn

c and M ∈ Mm n(R).

c   x2 ∈ Dp

3.3 Hyperbolic RNN

Naive RNN. A simple RNN can be deﬁned by ht+1 = ϕ(W ht + U xt + b) where ϕ is a pointwise
non-linearity  typically tanh  sigmoid  ReLU  etc. This formula can be naturally generalized to the
hyperbolic space as follows. For parameters W ∈ Mm n(R)  U ∈ Mm d(R)  b ∈ Dm
c   we deﬁne:
ht ∈ Dn
(27)
0(xt) and use the above formula  since

ht+1 = ϕ⊗c (W ⊗c ht ⊕c U ⊗c xt ⊕c b) 

Note that if inputs xt’s are Euclidean  one can write ˜xt := expc
expc

c   xt ∈ Dd
c .
0(U xt) = W ⊗c ht ⊕c U ⊗c ˜xt.

(U xt)) = W ⊗c ht ⊕c expc

0→W⊗cht

W⊗cht

(P c

GRU architecture. One can also adapt the GRU architecture:

rt = σ(W rht−1 + U rxt + br) 
zt = σ(W zht−1 + U zxt + bz) 
˜ht = ϕ(W (rt (cid:12) ht−1) + U xt + b)  ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˜ht 

(28)
where (cid:12) denotes pointwise product. First  how should we adapt the pointwise multiplication by a
scaling gate? Note that the deﬁnition of the Möbius version (see Eq. (24)) can be naturally extended
to maps f : Rn × Rp → Rm as f⊗c : (h  h(cid:48)) ∈ Dn
c (cid:55)→ expc
0(h(cid:48)))). In
particular  choosing f (h  h(cid:48)) := σ(h) (cid:12) h(cid:48) yields7 f⊗c(h  h(cid:48)) = expc
0(h(cid:48))) =
diag(σ(logc

0(h))) ⊗c h(cid:48). Hence we adapt rt (cid:12) ht−1 to diag(rt) ⊗c ht−1 and the reset gate rt to:

0(h)  logc
0(h)) (cid:12) logc

0(f (logc
0(σ(logc

c × Dp

(29)
and similarly for the update gate zt. Note that as the argument of σ in the above is unbounded  rt and
zt can a priori take values onto the full range (0  1). Now the intermediate hidden state becomes:

0(W r ⊗c ht−1 ⊕c U r ⊗c xt ⊕c br) 

rt = σ logc

˜ht = ϕ⊗c ((W diag(rt)) ⊗c ht−1 ⊕c U ⊗c xt ⊕ b) 

(30)
where Möbius matrix associativity simpliﬁes W ⊗c (diag(rt) ⊗c ht−1) into (W diag(rt)) ⊗c ht−1.
Finally  we propose to adapt the update-gate equation as

ht = ht−1 ⊕c diag(zt) ⊗c (−ht−1 ⊕c

(31)
Note that when c goes to zero  one recovers the usual GRU. Moreover  if zt = 0 or zt = 1  then ht
becomes ht−1 or ˜ht respectively  similarly as in the usual GRU. This adaptation was obtained by
adapting [24]: in this work  the authors re-derive the update-gate mechanism from a ﬁrst principle
called time-warping invariance. We adapted their derivation to the hyperbolic setting by using the
notion of gyroderivative [4] and proving a gyro-chain-rule (see appendix F).

˜ht).

4 Experiments

SNLI task and dataset. We evaluate our method on two tasks. The ﬁrst is natural language
inference  or textual entailment. Given two sentences  a premise (e.g. "Little kids A. and B. are
playing soccer.") and a hypothesis (e.g. "Two children are playing outdoors.")  the binary classiﬁcation
task is to predict whether the second sentence can be inferred from the ﬁrst one. This deﬁnes a partial
order in the sentence space. We test hyperbolic networks on the biggest real dataset for this task 
SNLI [7]. It consists of 570K training  10K validation and 10K test sentence pairs. Following [28] 
we merge the "contradiction" and "neutral" classes into a single class of negative sentence pairs  while
the "entailment" class gives the positive pairs.

7If x has n coordinates  then diag(x) denotes the diagonal matrix of size n with xi’s on its diagonal.

7

FULLY EUCLIDEAN RNN

HYP RNN+FFNN  EUCL MLR

FULLY HYPERBOLIC RNN
FULLY EUCLIDEAN GRU

HYP GRU+FFNN  EUCL MLR

FULLY HYPERBOLIC GRU

SNLI
79.34 %
79.18 %
78.21 %
81.52 %
79.76 %
81.19 %

PREFIX-10% PREFIX-30% PREFIX-50%

89.62 %
96.36 %
96.91 %
95.96 %
97.36 %
97.14 %

81.71 %
87.83 %
87.25 %
86.47 %
88.47 %
88.26 %

72.10 %
76.50 %
62.94 %
75.04 %
76.87 %
76.44 %

Table 1: Test accuracies for various models and four datasets. “Eucl” denotes Euclidean  “Hyp”
denotes hyperbolic. All word and sentence embeddings have dimension 5. We highlight in bold the
best baseline (or baselines  if the difference is less than 0.5%).

PREFIX task and datasets. We conjecture that the improvements of hyperbolic neural networks
are more signiﬁcant when the underlying data structure is closer to a tree. To test this  we design a
proof-of-concept task of detection of noisy preﬁxes  i.e. given two sentences  one has to decide if the
second sentence is a noisy preﬁx of the ﬁrst  or a random sentence. We thus build synthetic datasets
PREFIX-Z% (for Z being 10  30 or 50) as follows: for each random ﬁrst sentence of random length
at most 20 and one random preﬁx of it  a second positive sentence is generated by randomly replacing
Z% of the words of the preﬁx  and a second negative sentence of same length is randomly generated.
Word vocabulary size is 100  and we generate 500K training  10K validation and 10K test pairs.
Experimental details are presented in appendix G.

Models architecture. Our neural network layers can be used in a plug-n-play manner exactly like
standard Euclidean layers. They can also be combined with Euclidean layers. However  optimization
w.r.t. hyperbolic parameters is different (see below) and based on Riemannian gradients which
are just rescaled Euclidean gradients when working in the conformal Poincaré model [21]. Thus 
back-propagation can be applied in the standard way.
In our setting  we embed the two sentences using two distinct hyperbolic RNNs or GRUs. The
sentence embeddings are then fed together with their squared distance (hyperbolic or Euclidean 
depending on their geometry) to a FFNN (Euclidean or hyperbolic  see Sec. 3.2) which is further
fed to an MLR (Euclidean or hyperbolic  see Sec. 3.1) that gives probabilities of the two classes
(entailment vs neutral). We use cross-entropy loss on top. Note that hyperbolic and Euclidean layers
can be mixed  e.g. the full network can be hyperbolic and only the last layer be Euclidean  in which
case one has to use log0 and exp0 functions to move between the two manifolds in a correct manner
as explained for Eq. 24. For the results shown in Tab. 1  we run each model (baseline or ours) exactly
3 times and report the test result corresponding to the best validation result from these 3 runs. We
do this because the highly non-convex spectrum of hyperbolic neural networks sometimes results in
convergence to poor local minima  suggesting that initialization is very important.

Results. Results are shown in Tab. 1. Note that the fully Euclidean baseline models might have
an advantage over hyperbolic baselines because more sophisticated optimization algorithms such
as Adam do not have a hyperbolic analogue at the moment. We ﬁrst observe that all GRU models
overpass their RNN variants. Hyperbolic RNNs and GRUs have the most signiﬁcant improvement
over their Euclidean variants when the underlying data structure is more tree-like  e.g. for PREFIX-
10% − for which the tree relation between sentences and their preﬁxes is more prominent − we
reduce the error by a factor of 3.35 for hyperbolic vs Euclidean RNN  and by a factor of 1.5 for
hyperbolic vs Euclidean GRU. As soon as the underlying structure diverges more and more from
a tree  the accuracy gap decreases − for example  for PREFIX-50% the noise heavily affects the
representational power of hyperbolic networks. Also  note that on SNLI our methods perform
similarly as with their Euclidean variants. Moreover  hyperbolic and Euclidean MLR are on par when
used in conjunction with hyperbolic sentence embeddings  suggesting further empirical investigation
is needed for this direction (see below).

MLR classiﬁcation experiments. For the sentence entailment classiﬁcation task we do not see
a clear advantage of hyperbolic MLR compared to its Euclidean variant. A possible reason is
that  when trained end-to-end  the model might decide to place positive and negative embed-
dings in a manner that is already well separated with a classic MLR. As a consequence  we

8

WORDNET
SUBTREE

ANIMAL.N.01
3218 / 798

GROUP.N.01
6649 / 1727

WORKER.N.01

861 / 254

MAMMAL.N.01

953 / 228

MODEL

D = 2

D = 3

D = 5

D = 10

HYP
EUCL
log0
HYP
EUCL
log0
HYP
EUCL
log0
HYP
EUCL
log0

47.43 ± 1.07
41.69 ± 0.19
38.89 ± 0.01
81.72 ± 0.17
61.13 ± 0.42
60.75 ± 0.24
12.68 ± 0.82
10.86 ± 0.01
9.04 ± 0.06
32.01 ± 17.14
15.58 ± 0.04
13.10 ± 0.13

91.92 ± 0.61
68.43 ± 3.90
62.57 ± 0.61
89.87 ± 2.73
63.56 ± 1.22
61.98 ± 0.57
24.09 ± 1.49
22.39 ± 0.04
22.57 ± 0.20
87.54 ± 4.55
44.68 ± 1.87
44.89 ± 1.18

98.07 ± 0.55
95.59 ± 1.18
89.21 ± 1.34
87.89 ± 0.80
67.82 ± 0.81
67.92 ± 0.74
55.46 ± 5.49
35.23 ± 3.16
26.47 ± 0.78
88.73 ± 3.22
59.35 ± 1.31
52.51 ± 0.85

99.26 ± 0.59
99.36 ± 0.18
98.27 ± 0.70
91.91 ± 3.07
91.38 ± 1.19
91.41 ± 0.18
66.83 ± 11.38
47.29 ± 3.93
36.66 ± 2.74
91.37 ± 6.09
77.76 ± 5.08
56.11 ± 2.21

Table 2: Test F1 classiﬁcation scores (%) for four different subtrees of WordNet noun tree. 95% con-
ﬁdence intervals for 3 different runs are shown for each method and each dimension. “Hyp” denotes
our hyperbolic MLR  “Eucl” denotes directly applying Euclidean MLR to hyperbolic embeddings in
their Euclidean parametrization  and log0 denotes applying Euclidean MLR in the tangent space at 0 
after projecting all hyperbolic embeddings there with log0.

further investigate MLR for the task of subtree classiﬁcation. Using an open source implemen-
tation8 of [21]  we pre-trained Poincaré embeddings of the WordNet noun hierarchy (82 115
nodes). We then choose one node in this tree (see Table 2) and classify all other nodes (solely
based on their embeddings) as being part of the subtree rooted at this node. All nodes in
such a subtree are divided into positive training nodes (80%) and positive test nodes (20%).
The same splitting procedure is applied for the
remaining WordNet nodes that are divided into
a negative training and negative test set respec-
tively. Three variants of MLR are then trained
on top of pre-trained Poincaré embeddings[21]
to solve this binary classiﬁcation task: hyper-
bolic MLR  Euclidean MLR applied directly
on the hyperbolic embeddings (even if mathe-
matically this is not respecting the hyperbolic
geometry) and Euclidean MLR applied after
mapping all embeddings in the tangent space
at 0 using the log0 map. More experimental de-
tails in appendix G.2. Quantitative results are
presented in Table 2. We can see that the hyper-
bolic MLR overpasses its Euclidean variants in
almost all settings  sometimes by a large mar-
gin. Moreover  to provide further understand-
ing  we plot the 2-dimensional embeddings and
the trained separation hyperplanes (geodesics
in this case) in Figure 2.

Figure 2: Hyperbolic (left) vs Direct Euclidean
(right) binary MLR used to classify nodes as be-
ing part in the GROUP.N.01 subtree of the WordNet
noun hierarchy solely based on their Poincaré em-
beddings. The positive points (from the subtree) are
in red  the negative points (the rest) are in yellow
and the trained positive separation hyperplane is
depicted in green.

5 Conclusion

We showed how classic Euclidean deep learning tools such as MLR  FFNNs  RNNs or GRUs can be
generalized in a principled manner to all spaces of constant negative curvature combining Riemannian
geometry with the elegant theory of gyrovector spaces. Empirically we found that our models
outperform or are on par with corresponding Euclidean architectures on sequential data with implicit
hierarchical structure. We hope to trigger exciting future research related to better understanding
of the hyperbolic non-convexity spectrum and development of other non-Euclidean deep learning
methods. Our data and Tensorﬂow [1] code are publicly available9.

8https://github.com/dalab/hyperbolic_cones
9https://github.com/dalab/hyperbolic_nn

9

Acknowledgements

We thank Igor Petrovski for useful pointers regarding the implementation.
This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement
number 167176. Gary Bécigneul is also funded by the Max Planck ETH Center for Learning
Systems.

References
[1] Martín Abadi  Paul Barham  Jianmin Chen  Zhifeng Chen  Andy Davis  Jeffrey Dean  Matthieu
Devin  Sanjay Ghemawat  Geoffrey Irving  Michael Isard  et al. Tensorﬂow: A system for
large-scale machine learning. 2016.

[2] Ungar Abraham Albert. Analytic hyperbolic geometry and Albert Einstein’s special theory of

relativity. World scientiﬁc  2008.

[3] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning
Representations (ICLR)  2015.

[4] Graciela S Birman and Abraham A Ungar. The hyperbolic derivative in the poincaré ball model
of hyperbolic geometry. Journal of mathematical analysis and applications  254(1):321–333 
2001.

[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on

Automatic Control  58(9):2217–2229  Sept 2013.

[6] Antoine Bordes  Nicolas Usunier  Alberto Garcia-Duran  Jason Weston  and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems (NIPS)  pages 2787–2795  2013.

[7] Samuel R. Bowman  Gabor Angeli  Christopher Potts  and Christopher D. Manning. A large
annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing (EMNLP)  pages 632–642. Association
for Computational Linguistics  2015.

[8] Michael M Bronstein  Joan Bruna  Yann LeCun  Arthur Szlam  and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 
34(4):18–42  2017.

[9] James W Cannon  William J Floyd  Richard Kenyon  Walter R Parry  et al. Hyperbolic geometry.

Flavors of geometry  31:59–115  1997.

[10] Christopher De Sa  Albert Gu  Christopher Ré  and Frederic Sala. Representation tradeoffs for

hyperbolic embeddings. arXiv preprint arXiv:1804.03329  2018.

[11] Octavian-Eugen Ganea  Gary Bécigneul  and Thomas Hofmann. Hyperbolic entailment cones
for learning hierarchical embeddings. In Proceedings of the thirty-ﬁfth international conference
on machine learning (ICML)  2018.

[12] Mikhael Gromov. Hyperbolic groups. In Essays in group theory  pages 75–263. Springer  1987.

[13] Matthias Hamann. On the tree-likeness of hyperbolic spaces. Mathematical Proceedings of the

Cambridge Philosophical Society  page 1–17  2017.

[14] Christopher Hopper and Ben Andrews. The Ricci ﬂow in Riemannian geometry. Springer  2010.

[15] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)  pages
1746–1751. Association for Computational Linguistics  2014.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations (ICLR)  2015.

10

[17] Dmitri Krioukov  Fragkiskos Papadopoulos  Maksim Kitsak  Amin Vahdat  and Marián Boguná.

Hyperbolic geometry of complex networks. Physical Review E  82(3):036106  2010.

[18] John Lamping  Ramana Rao  and Peter Pirolli. A focus+ context technique based on hyperbolic
geometry for visualizing large hierarchies. In Proceedings of the SIGCHI conference on Human
factors in computing systems  pages 401–408. ACM Press/Addison-Wesley Publishing Co. 
1995.

[19] Guy Lebanon and John Lafferty. Hyperplane margin classiﬁers on the multinomial manifold. In
Proceedings of the international conference on machine learning (ICML)  page 66. ACM  2004.

[20] Maximilian Nickel  Volker Tresp  and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Proceedings of the international conference on machine
learning (ICML)  volume 11  pages 809–816  2011.

[21] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical repre-
sentations. In Advances in Neural Information Processing Systems (NIPS)  pages 6341–6350 
2017.

[22] Tim Rocktäschel  Edward Grefenstette  Karl Moritz Hermann  Tomáš Koˇcisk`y  and Phil Blun-
som. Reasoning about entailment with neural attention. In Proceedings of the International
Conference on Learning Representations (ICLR)  2015.

[23] Michael Spivak. A comprehensive introduction to differential geometry. Publish or perish  1979.

[24] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In Proceedings of

the International Conference on Learning Representations (ICLR)  2018.

[25] Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of

hyperbolic geometry. Computers & Mathematics with Applications  41(1-2):135–147  2001.

[26] Abraham Albert Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis

Lectures on Mathematics and Statistics  1(1):1–194  2008.

[27] Abraham Albert Ungar. Analytic hyperbolic geometry in n dimensions: An introduction. CRC

Press  2014.

[28] Ivan Vendrov  Ryan Kiros  Sanja Fidler  and Raquel Urtasun. Order-embeddings of images and
language. In Proceedings of the International Conference on Learning Representations (ICLR) 
2016.

[29] J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic

plane. Topology and its Applications  152(3):226–242  2005.

11

,Octavian Ganea
Gary Becigneul
Thomas Hofmann