2011,Ranking annotators for crowdsourced labeling tasks,With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers--annotators who assign labels randomly (e.g.  without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.,Ranking annotators for crowdsourced labeling tasks

Vikas C. Raykar

Siemens Healthcare  Malvern  PA  USA

Shipeng Yu

Siemens Healthcare  Malvern  PA  USA

vikas.raykar@siemens.com

shipeng.yu@siemens.com

Abstract

With the advent of crowdsourcing services it has become quite cheap and reason-
ably effective to get a dataset labeled by multiple annotators in a short amount of
time. Various methods have been proposed to estimate the consensus labels by
correcting for the bias of annotators with different kinds of expertise. Often we
have low quality annotators or spammers–annotators who assign labels randomly
(e.g.  without actually looking at the instance). Spammers can make the cost of
acquiring labels very expensive and can potentially degrade the quality of the con-
sensus labels.
In this paper we formalize the notion of a spammer and deﬁne
a score which can be used to rank the annotators—with the spammers having a
score close to zero and the good annotators having a high score close to one.

1 Spammers in crowdsourced labeling tasks

Annotating an unlabeled dataset is one of the bottlenecks in using supervised learning to build good
predictive models. Getting a dataset labeled by experts can be expensive and time consuming. With
the advent of crowdsourcing services (Amazon’s Mechanical Turk being a prime example) it has
become quite easy and inexpensive to acquire labels from a large number of annotators in a short
amount of time (see [8]  [10]  and [11] for some computer vision and natural language processing
case studies). One drawback of most crowdsourcing services is that we do not have tight control
over the quality of the annotators. The annotators can come from a diverse pool including genuine
experts  novices  biased annotators  malicious annotators  and spammers. Hence in order to get good
quality labels requestors typically get each instance labeled by multiple annotators and these multiple
annotations are then consolidated either using a simple majority voting or more sophisticated meth-
ods that model and correct for the annotator biases [3  9  6  7  14] and/or task complexity [2  13  12].

In this paper we are interested in ranking annotators based on how spammer like each annotator is.
In our context a spammer is a low quality annotator who assigns random labels (maybe because the
annotator does not understand the labeling criteria  does not look at the instances when labeling  or
maybe a bot pretending to be a human annotator). Spammers can signiﬁcantly increase the cost of
acquiring annotations (since they need to be paid) and at the same time decrease the accuracy of the
ﬁnal consensus labels. A mechanism to detect and eliminate spammers is a desirable feature for any
crowdsourcing market place. For example one can give monetary bonuses to good annotators and
deny payments to spammers.

The main contribution of this paper is to formalize the notion of a spammer for binary  categorical 
and ordinal labeling tasks. More speciﬁcally we deﬁne a scalar metric which can be used to rank the
annotators—with the spammers having a score close to zero and the good annotators having a score
close to one (see Figure 4). We summarize the multiple parameters corresponding to each annotator
into a single score indicative of how spammer like the annotator is. While this spammer score was
implicit for binary labels in earlier works [3  9  2  6] the extension to categorical and ordinal labels is
novel and is quite different from the accuracy computed from the confusion rate matrix. An attempt
to quantify the quality of the workers based on the confusion matrix was recently made by [4] where
they transformed the observed labels into posterior soft labels based on the estimated confusion

1

matrix. While we obtain somewhat similar annotator rankings  we differ from this work in that our
score is directly deﬁned in terms of the annotator parameters (see § 5 for more details).
The rest of the paper is organized as follows. For ease of exposition we start with binary labels
(§ 2) and later extend it to categorical (§ 3) and ordinal labels (§ 4). We ﬁrst specify the annotator
model used  formalize the notion of a spammer  and propose an appropriate score in terms of the
annotator model parameters. We do not dwell too much on the estimation of the annotator model
parameters. These parameters can either be estimated directly using known gold standard 1 or the
iterative algorithms that estimate the annotator model parameters without actually knowing the gold
standard [3  9  2  6  7]. In the experimental section (§ 6) we obtain rankings for the annotators using
the proposed spammer scores on some publicly available data from different domains.

2 Spammer score for crowdsourced binary labels

i ∈ {0  1} be the label assigned to the ith instance by the jth annotator  and
Annotator model Let yj
let yi ∈ {0  1} be the actual (unobserved) binary label. We model the accuracy of the annotator
separately on the positive and the negative examples. If the true label is one  the sensitivity (true
positive rate) αj for the jth annotator is deﬁned as the probability that the annotator labels it as one.

αj := Pr[yj

i = 1|yi = 1].

On the other hand  if the true label is zero  the speciﬁcity (1−false positive rate) βj is deﬁned as the
probability that annotator labels it as zero.

βj := Pr[yj

i = 0|yi = 0].

Extensions of this basic model have been proposed to include item level difﬁculty [2  13] and also
to model the annotator performance based on the feature vector [14]. For simplicity we use the
basic model proposed in [7] in our formulation. Based on many instances labeled by multiple
annotators the maximum likelihood estimator for the annotator parameters (αj  βj) and also the
consensus ground truth (yi) can be estimated iteratively [3  7] via the Expectation Maximization
(EM) algorithm. The EM algorithm iteratively establishes a particular gold standard (initialized via
majority voting)  measures the performance of the annotators given that gold standard (M-step)  and
reﬁnes the gold standard based on the performance measures (E-step).
Who is a spammer? Intuitively  a spammer assigns labels randomly—maybe because the annotator
does not understand the labeling criteria  does not look at the instances when labeling  or maybe a
bot pretending to be a human annotator. More precisely an annotator is a spammer if the probability
of observed label yj

i being one given the true label yi is independent of the true label  i.e. 

This means that the annotator is assigning labels randomly by ﬂipping a coin with bias Pr[yj
without actually looking at the data. Equivalently (1) can be written as

i = 1]

Pr[yj

i = 1|yi] = Pr[yj

i = 1].

(1)

Pr[yj

i = 1|yi = 1] = Pr[yj

i = 1|yi = 0] which implies αj = 1 − βj .

(2)
Hence in the context of the annotator model deﬁned earlier a perfect spammer is an annotator for
whom αj + βj − 1 = 0. This corresponds to the diagonal line on the Receiver Operating Character-
istic (ROC) plot (see Figure 1(a)) 2. If αj + βj − 1 < 0 then the annotators lies below the diagonal
line and is a malicious annotator who ﬂips the labels. Note that a malicious annotator has discrimi-
natory power if we can detect them and ﬂip their labels. In fact the methods proposed in [3  7] can
automatically ﬂip the labels for the malicious annotators. Hence we deﬁne the spammer score for
an annotator as

S j = (αj + βj − 1)2

(3)

An annotator is a spammer if S j is close to zero. Good annotators have S j > 0 while a perfect
annotator has S j = 1.

1One of the commonly used strategy to ﬁlter out spammers is to inject some items into the annotations with

known labels. This is the strategy used by CrowdFlower (http://crowdflower.com/docs/gold).

2Also note that (αj + βj )/2 is equal to the area shown in the plot and can be considered as a non-parametric
approximation to the area under the ROC curve (AUC) based on one observed point. It is also equal to the
Balanced Classiﬁcation Rate (BCR). So a spammer can also be deﬁned as having BCR or AUC equal to 0.5.

2

Equal accuracy contours (prevalence=0.5)

Equal spammer score contours

)
 

j

α
 
(
 
y
t
i
v
i
t
i
s
n
e
S

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

Good
Annotators

Biased
Annotators

[ 1−βj  αj ]

Spammers

Area = (αj+βj)/2

Biased
Annotators

0.2

0.6
0.4
1−Specificity ( βj )

Malicious
Annotators

0.8

1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
t
i
v
i
t
i
s
n
e
S

0.9

0.8

0.7

0.5

0.6

0
0

0.2

0.8

0.7

0.5

0.4

0.6

0.7

0.6

0.5

0.4

0.3

0.2

0.3
0.4
0.6
1−Specificity

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.1

y
t
i
v
i
t
i
s
n
e
S

0.3

0.2

0.8

0.7

0.6

0.5

0.4

0.3

0.1

0.4

0.3

0.2

0.1

0.2

0.1

0.1

0.2

0.1

0.2

0.3

0.4

0.8

1

0
0

0.2

0.3

0.1
0.4
0.6
1−Specificity

0.4

0.5

0.6

0.7

0.8

0.8

1

(a) Binary annotator model

(b) Accuracy

(c) Spammer score

Figure 1: (a) For binary labels an annotator is modeled by his/her sensitivity and speciﬁcity. A perfect spammer
lies on the diagonal line on the ROC plot. (b) Contours of equal accuracy (4) and (c) equal spammer score (3).

Accuracy This notion of a spammer is quite different for that of the accuracy of an annotator. An
annotator with high accuracy is a good annotator but one with low accuracy is not necessarily a
spammer. The accuracy is computed as

1

Accuracyj = Pr[yj

i = yi] =

Pr[yj

i = 1|yi = k]Pr[yi = k] = αjp + βj (1 − p) 

(4)

Xk=0

where p := Pr[yi = 1] is the prevalence of the positive class. Note that accuracy depends on
prevalence. Our proposed spammer score does not depend on prevalence and essentially quantiﬁes
the annotator’s inherent discriminatory power. Figure 1(b) shows the contours of equal accuracy
on the ROC plot. Note that annotators below the diagonal line (malicious annotators) have low
accuracy. The malicious annotators are good annotators but they ﬂip their labels and as such are not
spammers if we can detect them and then correct for the ﬂipping. In fact the EM algorithms [3  7]
can correctly ﬂip the labels for the malicious annotators and hence they should not be treated as
spammers. Figure 1(c) also shows the contours of equal score for our proposed score and it can be
seen that the malicious annotators have a high score and only annotators along the diagonal have a
low score (spammers).
Log-odds Another interpretation of a spammer can be seen from the log odds. Using Bayes’ rule
the posterior log-odds can be written as

log

Pr[yi = 1|yj
i ]
Pr[yi = 0|yj
i ]

= log

Pr[yj
Pr[yj

i |yi = 1]
i |yi = 0]

+ log

p

1 − p

.

1−p . Essentially the annotator
If an annotator is a spammer (i.e.  (2) holds) then log
provides no information in updating the posterior log-odds and hence does not contribute to the
estimation of the actual true label.

= log p

Pr[yi=1|yj
i ]
Pr[yi=0|yj
i ]

3 Spammer score for categorical labels

Annotator model Suppose there are K ≥ 2 categories. We introduce a multinomial parameter
αj

c = (αj

c1  . . .   αj

cK) for each annotator  where

αj
ck := Pr[yj

i = k|yi = c]

and

αj

ck = 1.

K

Xk=1

ck denotes the probability that annotator j assigns class k to an instance given that the

The term αj
true class is c. When K = 2  αj
Who is a spammer? As earlier a spammer assigns labels randomly  i.e. 

11 and αj

00 are sensitivity and speciﬁcity  respectively.

Pr[yj

i = k|yi] = Pr[yj

i = k]  ∀k.

3

This is equivalent to Pr[yj
i = k|yi = c0]  ∀c  c0  k = 1  . . .   K— which means
knowing the true class label being c or c0 does not change the probability of the annotator’s assigned
label. This indicates that the annotator j is a spammer if

i = k|yi = c] = Pr[yj

Let Aj be the K × K confusion rate matrix with entries [Aj]ck = αck—a spammer would have

αj
ck = αj

c0k  ∀c  c0  k = 1  . . .   K.

(5)

all the rows of Aj equal  for example  Aj = (cid:20) 0.50

0.50
0.50

0.25
0.25
0.25

annotation problem. Essentially Aj is a rank one matrix of the form Aj = ev>
vector vj ∈ RK that satisﬁes v>

j e = 1  where e is column vector of ones.

0.25
0.25

0.25 (cid:21)  for a three class categorical

j   for some column

In the binary case we had this natural notion of spammer as an annotator for whom αj + βj − 1 was
close to zero. One natural way to summarize (5) would be in terms of the distance (Frobenius norm)
of the confusion matrix to the closest rank one approximation  i.e 

S j := kAj − eˆv>

j k2
F  

where ˆvj solves

ˆvj = arg min
vj

kAj − ev>

j k2

F

s.t. v>

j e = 1.

(6)

(7)

Solving (7) yields ˆvj = (1/K)Aj >e  which is the mean of the rows of Aj. Then from (6) we have

So a spammer is an annotator for whom S j is close to zero. A perfect annotator has S j = K − 1.
We normalize this score to lie between 0 and 1.

S j =

(αj

ck − αj

c0k)2

(8)

(αj

ck − αj

c0k)2.

K Xc<c0Xk

1
K

S j =(cid:13)(cid:13)(cid:13)(cid:13)
(cid:18)I −

2

F

1

=

ee>(cid:19) Aj(cid:13)(cid:13)(cid:13)(cid:13)
K(K − 1) Xc<c0Xk

1

When K = 2 this is equivalent to the score proposed earlier for binary labels. As earlier this
notion of a spammer is different than the accuracy computed from the confusion rate matrix and
the prevalence. The accuracy is computed as Accuracyj = Pr[yj
i = k|yi =

k=1 Pr[yj

i = yi] = PK

k]Pr[yi = k] =PK

k=1 αj

kkPr[yi = k].

4 Spammer score for ordinal labels

A commonly used paradigm to annotate instances is to use ordinal scales where an annotator is
asked to rate an instance on a certain ordinal scale  say {1  . . .   K}. For example  rating a restaurant
on a scale of 1 to 5 or assessing the malignancy of a lesion on a BIRADS scale of 1 to 5 for
mammography. This differs from categorical labels where there is no order among the multiple
class labels. An ordinal variable expresses rank and there is an implicit ordering 1 < . . . < K.
Annotator model It is conceptually easier to think of the true label to be binary  that is  yi ∈ {0  1}.
For example in mammography a lesion is either malignant (1) or benign (0) (which can be conﬁrmed
by biopsy) and the BIRADS ordinal scale is a means for the radiologist to quantify the uncertainty
based on the digital mammogram. The radiologist assigns a higher value of the label if he/she
thinks the true label is closer to one. As earlier we characterize each annotator by the sensitivity
and the speciﬁcity  but the main difference is that we now deﬁne the sensitivity and speciﬁcity for
each ordinal label (or threshold) k ∈ {1  . . .   K}. Let αj
k be the sensitivity and speciﬁcity
respectively of the jth annotator corresponding to the threshold k  that is 

k and βj

αj
k = Pr[yj
1 = 0 and αj
1 = 1  βj

i ≥ k | yi = 1]
K+1 = 0  βj

Note that αj
is parameterized by a set of 2(K − 1) parameters [αj
empirical ROC curve for the annotator (Figure 2).

and βj

k = Pr[yj

i < k | yi = 0].

K+1 = 1 from this deﬁnition. Hence each annotator
K]. This corresponds to an

2  . . .   αj

K  βj

2  βj

4

k+1 and Pr[yj

Who is a spammer? As earlier we deﬁne an an-
notator j to be a spammer if Pr[yj
i = k|yi = 1] =
Pr[yj
i = k|yi = 0] ∀k = 1  . . .   K. Note that from
the annotation model we have 3 Pr[yj
i = k | yi =
k − αj
1] = αj
i = k | yi = 0] =
βj
k+1 − βj
k. This implies that annotator j is a spam-
mer if αj
k − αj
k  ∀k = 1  . . .   K 
which leads to αj
1 = 1  ∀k. This
means that for every k  the point (1 − βj
k) lies on
the diagonal line in the ROC plot shown in Figure 2.
The area under the empirical ROC curve can be com-
puted as (see Figure 2) AUCj = 1
k+1 +
αj
k)(βj
k)  and can be used to deﬁne the fol-
lowing spammer score as (2AUCj − 1)2 to rank the
different annotators.

k+1 − βj
k = αj

k+1 = βj
k + βj

2PK

k+1 − βj

k=1(αj

1 + βj

k  αj

k=1

k=2

k=3 [ 1−β3  α3 ]

k=4

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
 

j

α
 
(
 
y
t
i
v
i
t
i
s
n
e
S

0
0

0.2

0.6
0.4
1−Specificity ( βj )

0.8

1

Figure 2: Ordinal labels: An annotator is mod-
eled by sensitivity/speciﬁcity for each threshold.

S j = " K
Xk=1

(αj

k+1 + αj

k)(βj

k+1 − βj

2

k)# − 1!

(9)

With two levels this expression defaults to the binary case. An annotator is a spammer if S j is close
to zero. Good annotators have S j > 0 while a perfect annotator has S j = 1.

5 Previous work

i ] for c = 1  . . .   K  where yj

Recently Ipeirotis et.al. [4] proposed a score for categorical labels based on the expected cost of
the posterior label. In this section we brieﬂy describe their approach and compare it with our pro-
posed score. For each instance labeled by the annotator they ﬁrst compute the posterior (soft) label
Pr[yi = c|yj
i is the label assigned to the ith instance by the jth
annotator and yi is the true unknown label. The posterior label is computed via Bayes’ rule as
Pr[yi = c|yj
i  k)pc  where pc = Pr[yi = c] is the preva-
lence of class c. The score for a spammer is based on the intuition that the posterior label vector
(Pr[yi = 1|yj
i ]) for a good annotator will have all the probability mass concen-
trated on single class. For example for a three class problem (with equal prevalence)  a posterior label
vector of (1  0  0) (certain that the class is one) comes from a good annotator while a (1/3  1/3  1/3)
(complete uncertainty about the class label) comes from spammer. Based on this they deﬁne the
following score for each annotator

i |yi = c]Pr[yi = c] = (αj

i ]  . . .   Pr[yi = K|yj

i ] ∝ Pr[yj

ck)δ(yj

Scorej =

1
N

K

N

Xi=1" K
Xc=1

Xk=1(cid:16)costckPr[yi = k|yj

i ](cid:17)# .

i ]Pr[yi = c|yj

(10)

where costck is the misclassiﬁcation cost when an instance of class c is classiﬁed as k. Essentially
this is capturing some sort of uncertainty of the posterior label averaged over all the instances.
Perfect workers have a score Scorej = 0 while spammers will have high score. An entropic version
of this score based on similar ideas has also been recently proposed in [5]. Our proposed spammer
score differs from this approach in the following aspects: (1) Implicit in the score deﬁned above (10)
is the assumption that an annotator is a spammer when Pr[yi = c|yj
i ] = Pr[yi = c]  i.e.  the estimated
posterior labels are simply based on the prevalence and do not depend on the observed labels. By
Bayes’ rule this is equivalent to Pr[yj
i ] which is what we have used to deﬁne
our spammer score. (2) While both notions of a spammer are equivalent  the approach of [4] ﬁrst
computes the posterior labels based on the observed data  the class prevalence and the annotator

i |yi = c] = Pr[yj

3This can be seen as follows: Pr[yj

i < k + 1 | yi = 1] − Pr[(yj

i = k | yi = 1] = Pr[(yj
i ≥ k) OR (yj

i ≥ k) AND (yj
i < k + 1) | yi = 1] = Pr[yj

i < k + 1) | yi = 1] = Pr[yj

i ≥
i ≥ k | yi =

i ≥ k + 1 | yi = 1] = αj

k − αj

k+1. Here we used the fact that Pr[(yj

i ≥ k) OR (yj

i < k + 1)] = 1.

k | yi = 1] + Pr[yj
1] − Pr[yj

5

simulated | 500 instances | 30 annotators

simulated | 500 instances | 30 annotators 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

30

25

20

15

10

y
t
i
v
i
t
i
s
n
e
S

y
c
a
r
u
c
c
a

 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o
a
o
n
n
A

t

t

9
78

6

2
10
5

1

4

3

20

16

14

15

11

17

12

18

22

24
23

29

25

26

21

30

28

27

19

13

0.2

0.4
0.6
1−Specificity

0.8

1

(a) Simulation setup

simulated | 500 instances | 30 annotators
27

30

28

2223
26

24

21

29

25

14

17

18

11

12

15

16
20

19

13

e
r
o
c
S

 
r
e
m
m
a
p
S

]

4

[
.
l

a

.
t

e

 
s
i
t

o
r
i
e
p

I
 

i

a
v
 
)
n
a
d
e
m

i

5

0
0

9

4

1

5

2

10
7

6

3

8

5

10

15

20

25

Annotator rank (median) via spammer score
(c) Comparison with accuracy

(
 
k
n
a
r
 
r
o
a
o
n
n
A

t

t

30

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0
0

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

0
0
5

49

78
2

063
3

87
2

02
1

3
2

2
2

6
2

451
2

1
2

9
2

5
2

4
1

2
1

7
1

1
1

8
1

0
2

9
1

5
1

6
1

3
1

Annotator

(b) Annotator ranking

simulated | 500 instances | 30 annotators

18

1516
20
19

13

12

17

11

14

25

29

21

5

1

24

26
22
23

2

10
7

28
3

6

8
30

27

9

4

5

10

15

20

25

30

Annotator rank (median) via spammer score
(d) Comparison with Ipeirotis et. al. [4]

Figure 3: (a) The simulation setup consisting of 10 good annotators (annotators 1 to 10)  10 spammers (11 to
20)  and 10 malicious annotators (21 to 30). (b) The ranking of annotators obtained using the proposed spammer
score. The spammer score ranges from 0 to 1  the lower the score  the more spammy the annotator. The mean
spammer score and the 95% conﬁdence intervals (CI) are shown—obtained from 100 bootstrap replications.
The annotators are ranked based on the lower limit of the 95% CI. The number at the top of the CI bar shows the
number of instances annotated by that annotator. (c) and (d) Comparison of the median rank obtained via the
spammer score with the rank obtained using (c) accuracy and (d) the method proposed by Ipeirotis et. al. [4].

parameters and then computes the expected cost. Our proposed spammer score does not depend
on the prevalence of the class. Our score is also directly deﬁned only in terms of the annotator
confusion matrix and does not need the observed labels. (3) For the score deﬁned in (10) while
perfect annotators have a score of 0 it is not clear what should be a good baseline for a spammer.
The authors suggest to compute the baseline by assuming that a worker assigns as label the class
with maximum prevalence. Our proposed score has a natural scale with a perfect annotator having
a score of 1 and a spammer having a score of 0. (4) However one advantage of the approach in [4]
is that they can directly incorporate varied misclassiﬁcation costs.

6 Experiments

Ranking annotators based on the conﬁdence interval As mentioned earlier the annotator model
parameters can be estimated using the iterative EM algorithms [3  7] and these estimated annotator
parameters can then be used to compute the spammer score. The spammer score can then be used
to rank the annotators. However one commonly observed phenomenon when working with crowd-
sourced data is that we have a lot of annotators who label only a very few instances. As a result the
annotator parameters cannot be reliably estimated for these annotators. In order to factor this uncer-
tainty in the estimation of the model parameters we compute the spammer score for 100 bootstrap
replications. Based on this we compute the 95% conﬁdence intervals (CI) for the spammer score for
each annotator. We rank the annotators based on the lower limit of the 95% CI. The CIs are wider

6

Table 1: Datasets N is the number of instances. M is the number of annotators. M ∗ is the mean/median
number of annotators per instance. N ∗ is the mean/median number of instances labeled by each annotator.

Dataset

Type

bluebird

binary

temp

binary

wsd

categorical/3

sentiment

categorical/3

wosi
valence

ordinal/[0 10]
ordinal[-100 100]

N M M ∗

N ∗

Brief Description

108

462

177

1660

30
100

39

76

34

33

10
38

39/39

108/108

10/10

61/16

10/10

52/20

6/6

291/175

10/10
10/10

30/30
26/20

bird identiﬁcation [12] The annotator had to identify whether there was an Indigo
Bunting or Blue Grosbeak in the image.
event annotation [10] Given a dialogue and a pair of verbs annotators need to label
whether the event described by the ﬁrst verb occurs before or after the second.

word sense disambiguation [10] The labeler is given a paragraph of text containing
the word ”president” and asked to label one of the three appropriate senses.
irish economic sentiment analysis [1] Articles from three Irish online news sources
were annotated by volunteer users as positive  negative  or irrelevant.

word similarity [10] Numeric judgements of word similarity.
affect recognition [10] Each annotator is presented with a short headline and asked
to rate it on a scale [-100 100] to denote the overall positive or negative valence.

bluebird | 108 instances | 39 annotators 

wsd | 177 instances | 34 annotators 

wosi | 30 instances | 10 annotators 

1

8
0
1

e
r
o
c
S

 
r
e
m
m
a
p
S

e
r
o
c
S

 
r
e
m
m
a
p
S

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
81
0
1

8
0
81
0
1

8
0
1

8
0
1

8
0
1

8
0
1

8
0
1

78
1

7
2

0
3

5
2

51
3

2
1

2
3

7
3

8
3

6
1

29
2

9
2

5
1

0
2

95
1

93
3

1
2

3
2

42
1

0
1

47
2

3
3

3
1

6
3

14
3

4
3

8
2

8
1

16
1

6
2

e
r
o
c
S

 
r
e
m
m
a
p
S

1

0.8

0.6

0.4

0.2

0

7
7
1

7
5
1

7
7
1

7
5
1

0
8

0
4

0
4

0
2

7
7

0
0
1

0
2

0
2

0
2

7
1
1

7
7

0
2

0
2

0
2

0
2

0
2

0
2

7
7

0
2

0
2

0
6

7
1

0
4

7
1

0
2

0
2

0
2

0
2

0
2

0
2

3
1

1
3

0
1

3
2

9124689
2

4
1

5
1

7
1

2
2

25
3

8
1

6
1

9
1

1
1

2
1

0
2

1
2

4
2

5
2

6
2

7
2

8
2

0
3

3
3

473
3

Annotator

Annotator

temp | 462 instances | 76 annotators 

sentiment | 1660 instances | 33 annotators 

0
8

2
0
4

0
3

2
5

0
3

0
6

0
3

0
1

0
2

0
1

2
3
1

0
1

0
6
3

0
1

e
r
o
c
S

 
r
e
m
m
a
p
S

0.8

0.6

0.4

0.2

0

2
4
4

2
6
4

2
5
4

0
1

0
1

9
9
0
1

1
1
2
1

2
7
5

7
3
4

5
2
5

1
4
5

5
7
1

9
1
1

3
4

7
7

7
6

2
1

6
4
3

9
2
2

3
5
4

8
2
4

4
7
3

9
4
2

4
8
2

4
0
1

7
1
9

5
7

4
5
6

1
7
1

8
3
2

626
2

15
1

43
1

09
2

2
2

1
3

0
1

2
1

88
1

3
1

041
3

9
2

9
1

7
1

7
2

8
2

1
2

5
1

5
2

37
2

3
3

6
1

4
2

2
3

e
r
o
c
S

 
r
e
m
m
a
p
S

e
r
o
c
S

 
r
e
m
m
a
p
S

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

0
3

0
3

0
3

0
3

0
3

0
3

0
3

0
3

0
3

0
3

2 4 1 3 5 6 8 9 7

0
1

Annotator

valence | 100 instances | 38 annotators 

0
4

0
4

0
0
1

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
6

0
2

0
2

0
2

0
4

0
1

0
5

0
1

0
1

0
4

0
1

0
7

0
0
1

0
8

0
4

2
9
1

0
9
1

0
5
3

0
4

2
3

0
6

0
7

0
2

0
2

0
4

0
2

0
5

0
5

0
5

0
3

0
1

0
1

0
3

0
2

0
1

0
2

2
2

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

0
1

2
1

9
2

1

7
8

2
1

5
3

1
1

5
1

77

0
2

0
2

0
2

0
2

0
4

0
2

0
6

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

0
2

3
1

8
1

2
5

5
7

3
3

2
3

2
1

4
7

1
3

1
5

1
4

57
5

4
1

0
7

2
4

8
5

5
6

31
4

0
1

7
4

1
6

3
7

5
2

7
3

6
7

7
6

4
2

6
4

4
5

8
4

9
3

6
5

5
1

2
6

8
6

4
4

3
5

4
6

09
4

862
2

73458
5

1
1

6
1

7
1

9
1

0
2

1
2

2
2

3
2

6
2

7
2

9
2

0
3

4
3

5
3

6
3

8
3

5
4

9
4

0
5

9
5

0
6

3
6

6
6

9
6

1
7

2
7

1

6
2

0
1

8
1

8
2

55
1

6
3

3
2

28
1

2
3

1
3

8
3

3
1

7
1

7
2

12
1

5
3

4
2

996
1

0
3

3
3

7
3

4
1

943
2

0
2

4
3

2
2

57
2

6
1

1
2

Annotator

Annotator

Annotator

Figure 4: Annotator Rankings The rankings obtained for the datasets in Table 1. The spammer score ranges
from 0 to 1  the lower the score  the more spammy the annotator. The mean spammer score and the 95%
conﬁdence intervals (CI) are shown—obtained from 100 bootstrap replications. The annotators are ranked
based on the lower limit of the 95% CI. The number at the top of the CI bar shows the number of instances
annotated by that annotator. Note that the CIs are wider when the annotator labels only a few instances.

when the annotator labels only a few instances. For a crowdsourced labeling task the annotator has
to be good and also label a reasonable number of instances in order to be reliably identiﬁed.
Simulated data We ﬁrst illustrate our proposed spammer score on simulated binary data (with equal
prevalence for both classes) consisting of 500 instances labeled by 30 annotators of varying sensitiv-
ity and speciﬁcity (see Figure 3(a) for the simulation setup). Of the 30 annotators we have 10 good
annotators (annotators 1 to 10 who lie above the diagonal in Figure 3(a))  10 spammers (annotators
11 to 20 who lie around the diagonal)  and 10 malicious annotators (annotators 21 to 30 who lie be-
low the diagonal). Figure 3(b) plots the ranking of annotators obtained using the proposed spammer
score with the annotator model parameters estimated via the EM algorithm [3  7]. The spammer
score ranges from 0 to 1  the lower the score  the more spammy the annotator. The mean spammer
score and the 95% conﬁdence interval (CI) obtained via bootstrapping are shown. The annotators
are ranked based on the lower limit of the 95% CI. As can be seen all the spammers (annotators 11
to 20) have a low spammer score and appear at the bottom of the list. The malicious annotators have
higher score than the spammers since we can correct for their ﬂipping. The malicious annotators
are good annotators but they ﬂip their labels and as such are not spammers if we detect that they are
malicious. Figure 3(c) compares the (median) rank obtained via the spammer score with the (me-
dian) rank obtained using accuracy as the score to rank the annotators. While the good annotators
are ranked high by both methods the accuracy score gives a low rank to the malicious annotators.
Accuracy does not capture the notion of a spammer. Figure 3(d) compares the ranking with the
method proposed by Ipeirotis et. al. [4] which gives almost similar rankings as our proposed score.

7

bluebird | 108 instances | 39 annotators

bluebird | 108 instances | 39 annotators

y
c
a
r
u
c
c
a

 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o
a

t

t

o
n
n
A

40

35

30

25

20

15

10

8
17

5

0
0

21

23

10

6

4
34
11

26

18

7
14

28

31

13
33
36

24

3

5

2

19
39

15

20

9
29

12

16

22
37
38

1
32

25
35

27
30

5
35
Annotator rank (median) via spammer score

10

15

20

25

30

]

4

[
.
l

a

.
t

e

 
s
i
t

o
r
i
e
p

I
 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o

t

t

a
o
n
n
A

40

40

35

30

25

20

15

10

8
17

5

0
0

31
7

13

10

2

23

24

33
14
36

3

21
5

20

15

39
19

22
37

38

16

12

9
29

1
32

25
35

27
30

6
18
26

11
34
4

28

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
t
i
v
i
t
i
s
n
e
S

8

27

17

30
25

1

35
32

38
29
9

19
39

36

16

22
20

37

15

24

33
28

26

18

5
35
Annotator rank (median) via spammer score

10

15

20

25

30

40

0
0

0.2

(a)

(b)

21

23

0.4
0.6
1−Specificity
(c)

0.8

1

bluebird | 108 instances | 39 annotators
14

12

3

7

6

4

5

34

10

2

13

31

11

Figure 5: Comparison of the rank obtained via the spammer score with the rank obtained using (a) accuracy
and (b) the method proposed by Ipeirotis et. al. [4] for the bluebird binary dataset. (c) The annotator model
parameters as estimated by the EM algorithm [3  7].

wsd | 177 instances | 34 annotators

wsd | 177 instances | 34 annotators

sentiment | 1660 instances | 33 annotators

sentiment | 1660 instances | 33 annotators

y
c
a
r
u
c
c
a

 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o

t

a

t

o
n
n
A

35

30

25

20

15

10

5

0
0

19

18
7

24

20

12

3
14

5
32

9

8

1

34

21

28

22

17

26

29
31

23

15

13

10

6

4

2

5

30
Annotator rank (median) via spammer score

10

15

20

25

27

33
11
30

25

16

]

4

[
.
l

a

.
t

e

 
s
i
t

o
r
i
e
p

35

30

25

20

15

10

19
7

3
14

9

8

25

16

28

1

18

4

10

5
32
29
31

15

23

26

34

17

22

21

20
24

12

5

2

6

13

0
0

5

30
Annotator rank (median) via spammer score

25

20

10

15

27

33
11
30

y
c
a
r
u
c
c
a

 

30

25

20

15

10

5

0
0

24

33

15
17

28

27

19

8

1
4

18

12
29

13

10

20

3

30
31

9

2223

32

5

2
6

16

11
14

26

5

30
Annotator rank (median) via spammer score

10

15

20

25

7

25

21

]

4

[
.
l

a

.
t

e

 
s
i
t

o
r
i
e
p

I
 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o

t

a

t

o
n
n
A

30

25

20

15

10

5

0
0

7
21

25

19

15
17
27

28

8
12

4

18

13

29

1

24

20

33
10

3

9

23

5

2
6

16

32

3031

11
14

22

26

5

30
Annotator rank (median) via spammer score

20

15

10

25

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o

t

a

t

o
n
n
A

35

I
 

i

a
v
 
)
n
a
d
e
m

i

(
 
k
n
a
r
 
r
o

t

t

a
o
n
n
A

35

Figure 6: Comparison of the median rank obtained via the spammer score with the rank obtained using
accuracy and he method proposed by Ipeirotis et. al. [4] for the two categorial datasets in Table 1.

Mechanical Turk data We report results on some publicly available linguistic and image annotation
data collected using the Amazon’s Mechanical Turk (AMT) and other sources. Table 1 summarizes
the datasets. Figure 4 plots the spammer scores and rankings obtained. The mean and the 95% CI
obtained via bootstrapping are also shown. The number at the top of the CI bar shows the number
of instances annotated by that annotator. The rankings are based on the lower limit of the 95% CI
which factors the number of instances labeled by the annotator into the ranking. An annotator who
labels only a few instances will have very wide CI. Some annotators who label only a few instances
may have a high mean spammer score but the CI will be wide and hence ranked lower. Ideally we
would like to have annotators with a high score and at the same time label a lot of instances so that
we can reliablly identify them. The authors [1] for the sentiment dataset shared with us some of the
qualitative observations regarding the annotators and they somewhat agree with our rankings. For
example the authors made the following comments about Annotator 7 ”Quirky annotator - had a lot
of debate about what was the meaning of the annotation question. I’d say he changed his labeling
strategy at least once during the process”. Our proposed score gave a low rank to this annotator.
Comparison with other approaches Figure 5 and 6 compares the proposed ranking with the rank
obtained using accuracy and the method proposed by Ipeirotis et. al. [4] for some binary and cate-
gorical datasets in Table 1. Our proposed ranking is somewhat similar to that obtained by Ipeirotis
et. al. [4] but accuracy does not quite capture the notion of spammer. For example for the bluebird
dataset for annotator 21 (see Figure 5(a)) accuracy ranks it at the bottom of the list while the pro-
posed score puts is in the middle of the list. From the estimated model parameters it can be seen that
annotator 21 actually ﬂips the labels (below the diagonal in Figure 5(c)) but is a good annotator.

7 Conclusions

We proposed a score to rank annotators for crowdsourced binary  categorical  and ordinal labeling
tasks. The obtained rankings and the scores can be used to allocate monetary bonuses to be paid
to different annotators and also to eliminate spammers from further labeling tasks. A mechanism
to rank annotators should be desirable feature of any crowdsourcing service. The proposed score
should also be useful to specify the prior for Bayesian approaches to consolidate annotations.

8

References
[1] A. Brew  D. Greene  and P. Cunningham. Using crowdsourcing and active learning to track
sentiment in online media. In Proceedings of the 6th Conference on Prestigious Applications
of Intelligent Systems (PAIS’10)  2010.

[2] B. Carpenter. Multilevel bayesian models of categorical data annotation. Technical Report

available at http://lingpipe-blog.com/lingpipe-white-papers/  2008.

[3] A. P. Dawid and A. M. Skene. Maximum likeihood estimation of observer error-rates using

the EM algorithm. Applied Statistics  28(1):20–28  1979.

[4] P. G. Ipeirotis  F. Provost  and J. Wang. Quality management on Amazon Mechanical Turk.
In Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP’10)  pages
64–67  2010.

[5] V. C. Raykar and S. Yu. An entropic score to rank annotators for crowdsourced labelling tasks.
In Proceedings of the Third National Conference on Computer Vision  Pattern Recognition 
Image Processing and Graphics (NCVPRIPG)  2011.

[6] V. C. Raykar  S. Yu  L .H. Zhao  A. Jerebko  C. Florin  G. H. Valadez  L. Bogoni  and L. Moy.
Supervised learning from multiple experts: Whom to trust when everyone lies a bit. In Pro-
ceedings of the 26th International Conference on Machine Learning (ICML 2009)  pages 889–
896  2009.

[7] V. C. Raykar  S. Yu  L. H. Zhao  G. H. Valadez  C. Florin  L. Bogoni  and L. Moy. Learning

from crowds. Journal of Machine Learning Research  11:1297–1322  April 2010.

[8] V. S. Sheng  F. Provost  and P. G. Ipeirotis. Get another label? Improving data quality and data
mining using multiple  noisy labelers. In Proceedings of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  pages 614–622  2008.

[9] P. Smyth  U. Fayyad  M. Burl  P. Perona  and P. Baldi. Inferring ground truth from subjective
labelling of venus images. In Advances in Neural Information Processing Systems 7  pages
1085–1092. 1995.

[10] R. Snow  B. O’Connor  D. Jurafsky  and A. Y. Ng. Cheap and Fast—but is it good? Evaluating
Non-Expert Annotations for Natural Language Tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP ’08)  pages 254–263  2008.

[11] A. Sorokin and D. Forsyth. Utility data annotation with Amazon Mechanical Turk. In Pro-

ceedings of the First IEEE Workshop on Internet Vision at CVPR 08  pages 1–8  2008.

[12] P. Welinder  S. Branson  S. Belongie  and P. Perona. The multidimensional wisdom of crowds.

In Advances in Neural Information Processing Systems 23  pages 2424–2432. 2010.

[13] J. Whitehill  P. Ruvolo  T. Wu  J. Bergsma  and J. Movellan. Whose vote should count more:
In Advances in Neural

Optimal integration of labels from labelers of unknown expertise.
Information Processing Systems 22  pages 2035–2043. 2009.

[14] Y. Yan  R. Rosales  G. Fung  M. Schmidt  G. Hermosillo  L. Bogoni  L. Moy  and J. Dy. Mod-
eling annotator expertise: Learning when everybody knows a bit of something. In Proceedings
of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS
2010)  pages 932–939  2010.

9

,Supasorn Suwajanakorn
Noah Snavely
Jonathan Tompson
Mohammad Norouzi