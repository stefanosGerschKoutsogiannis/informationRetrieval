2019,The Option Keyboard: Combining Skills in Reinforcement Learning,The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or "cumulants"). Based on this premise  we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning  we show how to approximate options whose cumulants are linear combinations of the cumulants of known options. This means that  once we have learned options associated with a set of cumulants  we can instantaneously synthesise options induced by any linear combination of them  without any learning involved. We describe how this framework provides a hierarchical interface to the environment whose abstract actions correspond to combinations of basic skills. We demonstrate the practical benefits of our approach in a resource management problem and a navigation task involving a quadrupedal simulated robot.,The Option Keyboard

Combining Skills in Reinforcement Learning

André Barreto  Diana Borsa  Shaobo Hou  Gheorghe Comanici  Eser Aygün 

Philippe Hamel  Daniel Toyama  Jonathan Hunt  Shibl Mourad  David Silver  Doina Precup

{andrebarreto borsa shaobohou gcomanici eser}@google.com

{hamelphi kenjitoyama jjhunt shibl davidsilver doinap}@google.com

DeepMind

Abstract

The ability to combine known skills to create new ones may be crucial in the
solution of complex reinforcement learning problems that unfold over extended
periods. We argue that a robust way of combining skills is to deﬁne and manipulate
them in the space of pseudo-rewards (or “cumulants”). Based on this premise  we
propose a framework for combining skills using the formalism of options. We show
that every deterministic option can be unambiguously represented as a cumulant
deﬁned in an extended domain. Building on this insight and on previous results
on transfer learning  we show how to approximate options whose cumulants are
linear combinations of the cumulants of known options. This means that  once we
have learned options associated with a set of cumulants  we can instantaneously
synthesise options induced by any linear combination of them  without any learning
involved. We describe how this framework provides a hierarchical interface to the
environment whose abstract actions correspond to combinations of basic skills.
We demonstrate the practical beneﬁts of our approach in a resource management
problem and a navigation task involving a quadrupedal simulated robot.

1

Introduction

In reinforcement learning (RL) an agent takes actions in an environment in order to maximise the
amount of reward received in the long run [25]. This textbook deﬁnition of RL treats actions as
atomic decisions made by the agent at every time step. Recently  Sutton [23] proposed a new view
on action selection. In order to illustrate the potential beneﬁts of his proposal Sutton resorts to the
following analogy. Imagine that the interface between agent and environment is a piano keyboard 
with each key corresponding to a possible action. Conventionally the agent plays one key at a time
and each note lasts exactly one unit of time. If we expect our agents to do something akin to playing
music  we must generalise this interface in two ways. First  we ought to allow notes to be arbitrarily
long—that is  we must replace actions with skills. Second  we should be able to also play chords.
The argument in favour of temporally-extended courses of actions has repeatedly been made in the
literature: in fact  the notion that agents should be able to reason at multiple temporal scales is one of
the pillars of hierarchical RL [7  18  26  8  17]. The insight that the agent should have the ability to
combine the resulting skills is a far less explored idea. This is the focus of the current work.
The possibility of combining skills replaces a monolithic action set with a combinatorial counterpart:
by learning a small set of basic skills (“keys”) the agent should be able to perform a potentially very
large number of combined skills (“chords”). For example  an agent that can both walk and grasp an
object should be able to walk while grasping an object without having to learn a new skill. According

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

to Sutton [23]  this combinatorial action selection process “could be the key to generating behaviour
with a good mix of preplanned coherence and sensitivity to the current situation.”
But how exactly should one combine skills? One possibility is to combine them in the space of
policies: for example  if we look at policies as distribution over actions  a combination of skills can
be deﬁned as a mixture of the corresponding distributions. One can also combine parametric policies
by manipulating the corresponding parameters. Although these are feasible solutions  they fail to
capture possible intentions behind the skills. Suppose the agent is able to perform two skills that can
be associated with the same objective—distinct ways of grasping an object  say. It is not difﬁcult
to see how combinations of the corresponding behaviours can completely fail to accomplish the
common goal. We argue that a more robust way of combining skills is to do so directly in the goal
space  using pseudo-rewards or cumulants [25]. If we associate each skill with a cumulant  we can
combine the former by manipulating the latter. This allows us to go beyond the direct prescription of
behaviours  working instead in the space of intentions.
Combining skills in the space of cumulants poses two challenges. First  we must establish a well-
deﬁned mapping between cumulants and skills. Second  once a combined cumulant is deﬁned  we
must be able to perform the associated skill without having to go through the slow process of learning
it. We propose to tackle the former by adopting options as our formalism to deﬁne skills [26]. We
show that there is a large subset of the space of options  composed of deterministic options  in which
every element can be unambiguously represented as a cumulant deﬁned in an extended domain.
Building on this insight  we extend Barreto et al.’s [3  4] previous results on transfer learning to
show how to approximate options whose cumulants are linear combinations of the cumulants of
known options. This means that  once the agent has learned options associated with a collection
of cumulants  it can instantaneously synthesise options induced by any linear combination of them 
without any learning involved. Thus  by learning a small set of options  the agent instantaneously
has at its disposal a potentially enormous number of combined options. Since we are combining
cumulants  and not policies  the resulting options will be truly novel  meaning that they cannot  in
general  be directly implemented as a simple alternation of their constituents.
We describe how our framework provides a ﬂexible interface with the environment whose abstract
actions correspond to combinations of basic skills. As a reference to the motivating analogy described
above  we call this interface the option keyboard. We discuss the merits of the option keyboard at the
conceptual level and demonstrate its practical beneﬁts in two experiments: a resource management
problem and a realistic navigation task involving a quadrupedal robot simulated in MuJoCo [30  21].

2 Background

As usual  we assume the interaction between agent and environment can be modelled as a Markov
decision process (MDP) [19]. An MDP is a tuple M ≡ (S A  p  r  γ)  where S and A are the state and
action spaces  p(·|s  a) gives the next-state distribution upon taking action a in s  r : S ×A×S (cid:55)→ R
speciﬁes the reward associated with the transition s
The objective of the agent is to ﬁnd a policy π : S (cid:55)→ A that maximises the expected return

Gt ≡(cid:80)∞i=0 γiRt+i  where Rt = r(St  At  St+1). A principled way to address this problem is to

a−→ s(cid:48)  and γ ∈ [0  1) is the discount factor.

use methods derived from dynamic programming  which usually compute the action-value function
of a policy π as: Qπ(s  a) ≡ Eπ [Gt|St = s  At = a]   where Eπ[·] denotes expectation over the
transitions induced by π [19]. The computation of Qπ(s  a) is called policy evaluation. Once π has
been evaluated  we can compute a greedy policy

π(cid:48)(s) ∈ argmaxaQπ(s  a) for all s ∈ S.

(1)
(s  a) ≥ Qπ(s  a) for all (s  a) ∈ S × A  and hence the computation of π(cid:48) is
It can be shown that Qπ(cid:48)
referred to as policy improvement. The alternation between policy evaluation and policy improvement
is at the core of many RL algorithms  which usually carry out these steps approximately. Here we will
use a tilde over a symbol to indicate that the associated quantity is an approximation (e.g.  ˜Qπ ≈ Qπ).

2.1 Generalising policy evaluation and policy improvement
Following Sutton and Barto [25]  we call any signal deﬁned as c : S × A × S (cid:55)→ R a cumulant.
Analogously to the conventional value function Qπ  we deﬁne Qπ
c as the expected discounted sum of

2

cumulant c under policy π [27]. Given a policy π and a set of cumulants C  we call the evaluation of
π under all c ∈ C generalised policy evaluation (GPE) [2]. Barreto et al. [3  4] propose an efﬁcient
form of GPE based on successor features: they show that  given cumulants c1  c2  ...  cd  for any

c =(cid:80)

i wici  with w ∈ Rd 
c (s  a) ≡ Eπ
Qπ

(cid:34) ∞(cid:88)

d(cid:88)

(cid:35)

d(cid:88)

γk

wiCi t+k|St = s  At = a

=

wiQπ

ci(s  a) 

(2)

where Ci t ≡ ci(St  At  Rt). Thus  once we have computed Qπ

evaluate π under any cumulant in the set C ≡ {c =(cid:80)

k=0

i=1

c1  Qπ

i wici | w ∈ Rd}.

i=1
c1   ...  Qπ

cd  we can instantaneously

Policy improvement can also be generalised. In Barreto et al.’s [3] generalised policy improvement
(GPI) the improved policy is computed based on a set of value functions. Let Qπ1
c be
the action-value functions of n policies πi under cumulant c  and let Qmax
c (s  a)
for all (s  a) ∈ S × A. If we deﬁne

(s  a) = maxi Qπi

c   ...Qπn

c   Qπ2

c

then Qπ
improvement (1). The guarantee extends to the case in which GPI uses approximations ˜Qπi
c

(3)
(s  a) for all (s  a) ∈ S × A. This is a strict generalisation of standard policy

c (s  a) ≥ Qmax

[3].

c

c

π(s) ∈ argmaxaQmax

(s  a) for all s ∈ S 

2.2 Temporal abstraction via options

As discussed in the introduction  one way to get temporal abstraction is through the concept of
options [26]. Options are temporally-extended courses of actions. In their more general formulation 
options can depend on the entire history between the time t when they were initiated and the current
time step t + k  ht:t+k ≡ statst+1...at+k−1st+k. Let H be the space of all possible histories; a
semi-Markov option is a tuple o ≡ (Io  πo  βo) where Io ⊂ S is the set of states where the option can
be initiated  πo : H (cid:55)→ A is a policy over histories  and βo : H (cid:55)→ [0  1] gives the probability that the
option terminates after history h has been observed [26]. It is worth emphasising that semi-Markov
options depend on the history since their initiation  but not before.

3 Combining options
In the previous section we discussed how several key concepts in RL can be generalised: rewards
with cumulants  policy evaluation with GPE  policy improvement with GPI  and actions with options.
In this section we discuss how these concepts can be used to combine skills.

3.1 The relation between options and cumulants

We start by showing that there is a subset of the space of options in which every option can be
unequivocally represented as a cumulant deﬁned in an extended domain.
First we look at the relation between policies and cumulants. Given an MDP (S A  p ·  γ)  we say
that a cumulant cπ : S × A × S (cid:55)→ R induces a policy π : S (cid:55)→ A if π is optimal for the MDP
(S A  p  cπ  γ). We can always deﬁne a cumulant cπ that induces a given policy π. For instance  if
we make

(cid:26) 0 if a = π(s);

z otherwise 

cπ(s  a ·) =

(4)

where z < 0  it is clear that π is the only policy that achieves the maximum possible value Qπ(s  a) =
Q∗(s  a) = 0 on all (s  a) ∈ S × A. In general  the relation between policies and cumulants is a
many-to-many mapping. First  there is more than one cumulant that induces the same policy: for
example  any z < 0 in (4) will clearly lead to the same policy π. There is thus an inﬁnite set of
cumulants Cπ associated with π. Conversely  although this is not the case in (4)  the same cumulant
can give rise to multiple policies if more than one action achieves the maximum in (1).
Given the above  we can use any cumulant cπ ∈ Cπ to refer to policy π. In order to extend this
possibility to options o = (Io  πo  βo) we need two things. First  we must deﬁne cumulants in the
space of histories H. This will allow us to induce semi-Markov policies πo : H (cid:55)→ A in a way that is
analogous to (4). Second  we need cumulants that also induce the initiation set Io and the termination
function βo. We propose to accomplish this by augmenting the action space.

3

Let τ be a termination action that terminates option o much like the termination function βo. We can
think of τ as a ﬁctitious action and model it by deﬁning an augmented action space A+ ≡ A ∪ {τ}.
When the agent is executing an option o  selecting action τ immediately terminates it. We now
show that if we extend the deﬁnition of cumulants to also include τ we can have the resulting
cumulant induce not only the option’s policy but also its initiation set and termination function. Let
e : H × A+ × S (cid:55)→ R be an extended cumulant. Since e is deﬁned over the augmented action space 
for each h ∈ H we now have a termination bonus e(h  τ  s) = e(h  τ ) that determines the value of
interrupting option o after having observed h. The extended cumulant e induces an augmented policy
ωe : H (cid:55)→ A+ in the same sense that a standard cumulant induces a policy (that is  ωe is an optimal
policy for the derived MDP whose state space is H and the action space is A+; see Appendix A for
details). We argue that ωe is equivalent to an option oe ≡ (Ie  πe  βe) whose components are deﬁned
as follows. The policy πe : H (cid:55)→ A coincides with ωe whenever the latter selects an action in A. The
termination function is given by

(cid:26) 1 if e(h  τ ) > maxa(cid:54)=τ Qωe

0 otherwise.

e (h  a) 

βe(h) =

(5)

In words  the agent will terminate after h if the instantaneous termination bonus e(h  τ ) is larger than
the maximum expected discounted sum of cumulant e under policy ωe. Note that when h is a single
state s  no concrete action has been executed by the option yet  hence it terminates with τ immediately
after its initiation. This is precisely the deﬁnition of the initialisation set Ie ≡ {s| βe(s) = 0}.
Termination functions like (5) are always deterministic. This means that extended cumulants e can
only represent options oe in which βe is a mapping H (cid:55)→ {0  1}. In fact  it is possible to show that
all options of this type  which we will call deterministic options  are representable as an extended
cumulant e  as formalised in the following proposition (proof in Appendix A):
Proposition 1. Every extended cumulant induces at least one deterministic option  and every deter-
ministic option can be unambiguously induced by an inﬁnite number of extended cumulants.

3.2 Synthesising options using GPE and GPI

ei can be linearly combined; it then follows that  for any w ∈ Rd  e = (cid:80)
termination bonuses deﬁned as e(h  τ ) =(cid:80)

In the previous section we looked at the relation between extended cumulants and deterministic
options; we now build on this connection to use GPE and GPI to combine options.
Let E ≡ {e1  e2  ...  ed} be a set of extended cumulants. We know that ei : H × A+ × S (cid:55)→ R is
associated with deterministic option oei ≡ ωei. As with any other cumulant  the extended cumulants
i wiei deﬁnes a new
deterministic option oe ≡ ωe. Interestingly  the termination function of oe has the form (5) with
i wiei(h  τ ). This means that the combined option oe
“inherits” its termination function from its constituents oei. Since any w ∈ Rd deﬁnes an option oe 
the set E can give rise to a very large number of combined options.
The problem is of course that for each w ∈ Rd we have to actually compute the resulting option ωe.
This is where GPE and GPI come to the rescue. Suppose we have the values of options ωei under all
the cumulants e1  e2  ...  ed. With this information  and analogously to (2)  we can use the fast form
of GPE provided by successor features to compute the value of ωej with respect to e:

(cid:88)

ωej
Q
e

(h  a) =

wiQ

ωej
ei (h  a).

(6)

Now that we have all the options ωej evaluated under e  we can merge them to generate a new option
that does at least as well as  and in general better than  all of them. This is done by applying GPI over
ωej
the value functions Q
e

:

i

˜ωe(h) ∈ argmaxa∈A+ maxj Q

(h  a).

ωej
e
(h  a) ≤ Q˜ωe

ωej
From previous theoretical results we know that maxj Q
e (h  a) for all
e
(h  a) ∈ H × A+ [3]. In words  this means that  even though the GPI option ˜ωe is not necessarily
optimal  following it will in general result in a higher return in terms of cumulant e than if the agent
were to execute any of the known options ωej . Thus  we can use ˜ωe as an approximation to ωe that
requires no additional learning. It is worth mentioning that the action selected by the combined
option in (7)  ˜ωe(h)  can be different from ωei(h) for all i—that is  the resulting policy cannot  in

e (h  a) ≤ Qωe

(7)

4

the resulting cumulant e = (cid:80)

general  be implemented as an alternation of its constituents. This highlights the fact that combining
cumulants is not the same as deﬁning a higher-level policy over the associated options.
In summary  given a set of cumulants E  we can combine them by picking weights w and computing
i wiei. This can be interpreted as determining how desirable or
undesirable each cumulant is. Going back to the example in the introduction  suppose that e1 is
associated with walking and e2 is associated with grasping an object. Then  cumulant e1 + e2 will
reinforce both behaviours  and will be particularly rewarding when they are executed together. In
contrast  cumulant e1 − e2 will induce an option that avoids grasping objects  favouring the walking
behaviour in isolation and even possibly inhibiting it. Since the resulting option aims at maximising a
combination of the cumulants ei  it can itself be seen as a combination of the options oei.

Figure 1: OK mediates the interaction between player
and environment. The exchange of information between
OK and the environment happens at every time step.
The interaction between player and OK only happens
“inside” the agent when the termination action τ is se-
lected by GPE and GPI (see Algorithms 1 and 2).

4 Learning with combined options
Given a set of extended cumulants E  in
order to be able to combine the asso-
ciated options using GPE and GPI one
only needs the value functions Q
≡
E
{ ˜Q
| ∀(i  j) ∈ {1  2  ...  d}2}. The set
ωei
ej
Q
E can be constructed using standard RL
methods; for an illustration of how to do it
with Q-learning see Algorithm 3 in App. B.
As discussed  once Q
E has been computed
we can use GPE and GPI to synthesise op-
tions on the ﬂy.
In this case the newly-
generated options are fully determined by
the vector of weights w ∈ Rd. Concep-
tually  we can think of this process as an
interface between an RL algorithm and the environment: the algorithm selects a vector w  hands it
over to GPE and GPI  and “waits” until the action returned by (7) is the termination action τ. Once τ
has been selected  the algorithm picks a new w  and so on. The RL method is thus interacting with
the environment at a higher level of abstraction in which actions are combined skills deﬁned by the
vectors of weights w. Returning to the analogy with a piano keyboard described in the introduction 
we can think of each option ωei as a “key” that can be activated by an instantiation of w whose only
non-zero entry is wi > 0. Combined options associated with more general instantiations of w would
correspond to “chords”. We will thus call the layer of temporal abstraction between algorithm and
environment the option keyboard (OK). We will also generically refer to the RL method interacting
with OK as the “player”. Figure 1 shows how an RL agent can be broken into a player and an OK.
Algorithm 1 shows a generic implementation of OK. Given a set of value functions Q
E and a vector
of weights w  OK will execute the actions selected by GPE and GPI until the termination action is
picked or a terminal state is reached. During this process OK keeps track of the discounted reward
accumulated in the interaction with the environment (line 6)  which will be returned to the player
when the interaction terminates (line 10). As the options ωei may depend on the entire trajectory
since their initiation  OK uses an update function u(h  a  s(cid:48)) that retains the parts of the history that
are actually relevant for decision making (line 8). For example  if OK is based on Markov options
only  one can simply use the update function u(h  a  s(cid:48)) = s(cid:48).
The set Q
E deﬁnes a speciﬁc instantiation of OK; once an OK is in place any conventional RL
method can interact with it as if it were the environment. As an illustration  Algorithm 2 shows
how a keyboard player that uses a ﬁnite set of combined options W ≡ {w1  w2  ...  wn} can be
implemented using standard Q-learning by simply replacing the environment with OK. It is worth
pointing out that if we substitute any other set of weight vectors W(cid:48) for W we can still use the same
OK  without the need to relearn the value functions in Q
E. We can even use sets of abstract actions
W that are inﬁnite—as long as the OK player can deal with continuous action spaces [33  24  22].
Although the clear separation between OK and its player is instructive  in practice the boundary
between the two may be more blurry. For example  if the player is allowed to intervene in all interac-
tions between OK and environment  one can implement useful strategies like option interruption [26].
Finally  note that although we have been treating the construction of OK (Algorithm 3) and its use

5

playerenvironmentwar sr′ s′ γ′agentOKtemporalabstraction(Algorithms 1 and 2) as events that do not overlap in time  nothing keeps us from carrying out the
two procedures in parallel  like in similar methods in the literature [1  32].
Algorithm 1 Option Keyboard (OK)

Algorithm 2 Q-learning keyboard player

 OK

Require:

option keyboard
W
combined options
Q
value functions
α    γ ∈ R hyper-parameters
E

1: create ˜Q(s  w) parametrised by θQ
2: select initial state s ∈ S
3: repeat forever
4:
5:
6:
7:
8:
9:
10:

if Bernoulli()=1 then w ← Uniform(W)
else w ← argmaxw(cid:48)∈W
(s(cid:48)  r(cid:48)  γ(cid:48)) ← OK(s  w  Q
δ ← r(cid:48) + γ(cid:48) maxw(cid:48) ˜Q(s(cid:48)  w(cid:48)) − ˜Q(s  w)
θQ ← θQ + αδ∇θQ
˜Q(s  w) // update ˜Q
if s(cid:48) is terminal then select initial s ∈ S
else s ← s(cid:48)

˜Q(s  w(cid:48))
E  γ)

current state
vector of weights
value functions

w ∈ Rd
Q
γ ∈ [0  1) discount rate
E

Require:

 s ∈ S
a ← argmaxa(cid:48) maxi[(cid:80)

1: h ← s; r(cid:48) ← 0; γ(cid:48) ← 1
2: repeat
3:
4:
5:
6:
7:
8:
9: until a = τ or s(cid:48) is terminal
10: return s(cid:48)  r(cid:48)  γ(cid:48)

if a (cid:54)= τ then

j wj ˜Q

ej (h  a(cid:48))]
ωei

execute action a and observe r and s(cid:48)
r(cid:48) ← r(cid:48) + γ(cid:48)r
if s(cid:48) is terminal γ(cid:48) ← 0 else γ(cid:48) ← γ(cid:48)γ
h ← u(h  a  s(cid:48))

5 Experiments

We now present our experimental results illustrating the beneﬁts of OK in practice. Additional details 
along with further results and analysis  can be found in Appendix C.
5.1 Foraging world
The goal in the foraging world is to manage a set of resources by navigating in a grid world and
picking up items containing the resources in different proportions. For illustrative purposes we will
consider that the resources are nutrients and the items are food. The agent’s challenge is to stay
healthy by keeping its nutrients within certain bounds. The agent navigates in the grid world using
the four usual actions: up  down  left  and right. Upon collecting a food item the agent’s nutrients are
increased according to the type of food ingested. Importantly  the quantity of each nutrient decreases
by a ﬁxed amount at every step  so the desirability of different types of food changes even if no food is
consumed. Observations are images representing the conﬁguration of the grid plus a vector indicating
how much of each nutrient the agent currently has (see Appendix C.1 for a technical description).
What makes the foraging world particularly challenging is the fact that the agent has to travel towards
the items to pick them up  adding a spatial aspect to an already complex management problem. The
dual nature of the problem also makes it potentially amenable to be tackled with options  since we
can design skills that seek speciﬁc nutrients and then treat the problem as a management task in
which actions are preferences over nutrients. However  the number of options needed can increase
exponentially fast. If at any given moment the agent wants  does not want  or does not care about
each nutrient  we need 3m options to cover the entire space of preferences  where m is the number of
nutrients. This is a typical situation where being able to combine skills can be invaluable.
As an illustration  in our experiments we used m = 2 nutrients and 3 types of food. We deﬁned
a cumulant ei ∈ E associated with each nutrient as follows: ei(h  a  s) = 0 until a food item is
consumed  when it becomes the increase in the associated nutrient. After a food item is consumed we
have that ei(h  a  s) = −1{a (cid:54)= τ}  where 1{·} is the indicator function—this forces the induced
option to terminate  and also illustrates how the deﬁnition of cumulants over histories h can be useful
(since single states would not be enough to determine whether the agent has consumed a food item).
We used Algorithm 3 in Appendix B to compute the 4 value functions in Q
E. We then deﬁned a
8-dimensional abstract action space covering the space of preferences  W ≡ {−1  0  1}2 − {[0  0]} 
and used it with the Q-learning player in Algorithm 2. We also consider Q-learning using only the 2
options maximizing each nutrient and a “ﬂat” Q-learning agent that does not use options at all.
By modifying the target range of each nutrient we can create distinct scenarios with very different
dynamics. Figure 2 shows results in two such scenarios. Note how the relative performance of the
two baselines changes dramatically from one scenario to the other  illustrating how the usefulness
of options is highly context-dependent. Importantly  as shown by the results of the OK player  the

6

Figure 2: Results on the foraging world. The two plots correspond to different conﬁgurations of the
environment (see Appendix C.1). Shaded regions are one standard deviation over 10 runs.

ability to combine options in cumulant space makes it possible to synthesise useful behaviour from a
given set of options even when they are not useful in isolation.
5.2 Moving-target arena
As the name suggests  in the moving-target arena the goal is to get to a target region whose location
changes every time the agent reaches it. The arena is implemented as a square room with realistic
dynamics deﬁned in the MuJoCo physics engine [30]. The agent is a quadrupedal simulated robot
with 8 actuated degrees of freedom; actions are thus vectors in [−1  1]8 indicating the torque applied to
each joint [21]. Observations are 29-dimensional vectors with spatial and proprioception information
(Appendix C.2). The reward is always 0 except when the agent reaches the target  when it is 1.
We deﬁned cumulants in order to encourage the agent’s displacement in certain directions. Let v(h)
be the vector of (x  y) velocities of the agent after observing history h (the velocity is part of the
observation). Then  if we want the agent to travel at a certain direction w for k steps  we can deﬁne:

(8)

(cid:26) w(cid:62)v(h) if length(h) ≤ k;

−1{a (cid:54)= τ} otherwise.

ew(h  a ·) =

ex(h  a) + w2Qω

ew (h  a) = w1Qω

The induced option will terminate after k = 8 steps as a negative reward is incurred for all histories
of length greater than k and actions other than τ. It turns out that even if a larger number of directions
w is to be learned  we only need to compute two value functions for each cumulant ew. Since for
all ew ∈ E we have that ew = w1ex + w2ey  where x = [1  0] and y = [0  1]  we can use (2) to
decompose the value function of any option ω as Qω
ey (h  a). Hence 
|Q
| = 2|E|  resulting in a 2-dimensional space W in which w ∈ R2 indicates the intended direction
E
of locomotion. Thus  by learning a few options that move along speciﬁc directions  the agent is
potentially able to synthesise options that travel in any direction.
For our experiments  we deﬁned cumulants ew corresponding to the directions 0o  120o  and 240o.
To compute the set of value functions Q
E we used Algorithm 3 with Q-learning replaced by the
deterministic policy gradient (DPG) algorithm [22]. We then used the resulting OK with both discrete
and continuous abstract-action spaces W. For ﬁnite W we adopted a Q-learning player (Algorithm 2);
in this case the abstract actions wi correspond to n ∈ {4  6  8} directions evenly-spaced in the unit
circle. For continuous W we used a DPG player. We compare OK’s results with that of DPG applied
directly in the original action space and also with Q-learning using only the three basic options.
Figure 3 shows our results on the moving-target arena. As one can see by DPG’s results  solving
the problem in the original action space is difﬁcult because the occurrence of non-zero rewards may
depend on a long sequence of lucky actions. When we replace actions with options we see a clear
speed up in learning  even if we take into account the training of the options. If in addition we allow
for combined options  we observe a signiﬁcant boost in performance  as shown by the OK players’
results. Here we see the expected trend: as we increase |W| the OK player takes longer to learn but
achieves better ﬁnal performance  as larger numbers of directional options allow for ﬁner control.
These results clearly illustrate the beneﬁts of being able to combine skills  but how much is the agent
actually using this ability? In Figure 3 we show a histogram indicating how often combined options
are used by OK to implement directions w ∈ R2 across the state space (details in App. C.2). As
shown  for abstract actions w close to 0o  120o and 240o the agent relies mostly on the 3 options
trained to navigate along these directions  but as the intended direction of locomotion gets farther from

7

02000004000006000008000001000000steps020406080100120140160Average Episode ReturnQ-Learning PlayerQ-Learning Simple OptionsQ-Learning02000004000006000008000001000000steps4020020406080100Average Episode ReturnQ-Learning PlayerQ-Learning Simple OptionsQ-LearningFigure 3: Left: Results on the moving-target arena. All players used the same keyboard  so they
share the same OK training phase. Shaded regions are one standard deviation over 10 runs. Right:
Histogram of options used by OK to implement directions w. Black lines are the three basic options.

(cid:80)

j wj ˆQ

these reference points combined options become crucial. This shows how the ability to combine skills
can extend the range of behaviours available to an agent without the need for additional learning.1
Even if one accepts the premise of this paper that skills should be combined in the space of cumulants 
it is natural to ask whether other strategies could be used instead of GPE and GPI. Although we are
not aware of any other algorithm that explicitly attempts to combine skills in the space of cumulants 
there are methods that do so in the space of value functions [29  6  13  16]. Haarnoja et al. [13]
propose a way of combining skills based on entropy-regularised value functions. Given a set of
i wiei as follows:
ωej
ej (h  a) are entropy-regularised value functions

cumulants e1  e2  ...  ed  they propose to compute a skill associated with e =(cid:80)
ˆωe(h) ∈ argmaxa∈A+
and wj ∈ [−1  1]. We will refer to this method as additive value composition (AVC).
How well does AVC perform as compared to GPE and GPI? In order to answer this question we
reran the previous experiments but now using ˆωe(h) as deﬁned above instead of the option ˜ωe(h)
computed through (6) and (7). In order to adhere more closely to the assumptions underlying AVC 
we also repeated the experiment using an entropy-regularised OK [14] (App. C.2). Figure 4 shows
the results. As indicated in the ﬁgure  GPE and GPI outperform AVC both with the standard and the
entropy-regularised OK. A possible explanation for this is given in the accompanying polar scatter
chart in Figure 4  which illustrates how much progress each method makes  over the state space  in all
directions w (App. C.2). The plot suggests that  in this domain  the directional options implemented
through GPI and GPE are more effective in navigating along the desired directions (also see [16]).

ωej
ej (h  a)  where ˆQ

be well approximated as r(s  a  s(cid:48)) ≈(cid:80)

6 Related work
Previous work has used GPI and successor features  the linear form of GPE considered here  in the
context of transfer [3  4  5]. A crucial assumption underlying these works is that the reward can
i wici(s  a  s(cid:48)). By solving a regression problem  the agent
ﬁnds a w ∈ Rd that leads to a good approximation of r(s  a  s(cid:48)) and uses it to apply GPE and GPI
(equations (2) and (3)  respectively). In terms of the current work  this is equivalent to having a
keyboard player that is only allowed to play one endless “chord”. Through the introduction of a
termination action  in this work we replace policies with options that may eventually halt. Since
policies are options that never terminate  the previous framework is a special case of OK. Unlike in
the previous framework  with OK we can also chain a sequence of options  resulting in more ﬂexible
behaviour. Importantly  this allows us to completely remove the linearity assumption on the rewards.
We now turn our attention to previous attempts to combine skills with no additional learning. As
discussed  one way to do so is to work directly in the space of policies. Many policy-based methods
ﬁrst learn a parametric representation of a lower-level policy  π(·| s; θ)  and then use θ ∈ Rd as the
actions for a higher-level policy µ : S (cid:55)→ Rd [15  10  12]. One of the central arguments of this paper
1A video of the quadrupedal simulated robot being controlled by the DPG player can be found on the

following link: https://youtu.be/39Ye8cMyelQ.

8

012345Steps1e705101520253035Average return per episodeOK trainingDPG PlayerQ-Learning Player (8)Q-Learning Player (6)Q-Learning Player (4)Q-Learning + OptionsDPGFigure 4: Left: Comparison of GPE and GPI with AVC on the moving-target arena. Results were
obtained by a DPG player using a standard OK and an entropy-regularised counterpart (ENT-OK).
We trained several ENT-OK with different regularisation parameters and picked the one leading to the
best AVC performance. The same player and keyboards were used for both methods. Shaded regions
are one standard deviation over 10 runs. Right: Polar scatter chart showing the average distance
travelled by the agent along directions w when combining options using the two competing methods.

is that combining skills in the space of cumulants may be advantageous because it corresponds to
manipulating the goals underlying the skills. This can be seen if we think of w ∈ Rd as a way of
encoding skills and compare its effect on behaviour with that of θ: although the option induced by
w1 + w2 through (6) and (7) will seek a combination of both its constituent’s goals  the same cannot
be said about a skill analogously deﬁned as π(·| s; θ1 + θ2). More generally  though  one should
expect both policy- and cumulant-based approaches to have advantages and disadvantages.
Interestingly  most of the previous attempts to combine skills in the space of value functions are based
on entropy-regularised RL  like the already discussed AVC [34  9  11  13]. Hunt et al. [16] propose a
way of combining skills which can in principle lead to optimal performance if one knows in advance
the weights of the intended combinations. They also extend GPE and GPI to entropy-regularised
RL. Todorov [28] focuses on entropy-regularised RL on linearly solvable MDPs. Todorov [29] and
da Silva et al. [6] have shown how  in this scenario  one can compute optimal skills corresponding
to linear combinations of other optimal skills—a property later explored by Saxe et al. [20] to
propose a hierarchical approach. Along similar lines  Van Niekerk et al. [31] have shown how
optimal value function composition can be obtained in entropy-regularised shortest-path problems
with deterministic dynamics  with the non-regularised setup as a limiting case.

7 Conclusion
The ability to combine skills makes it possible for an RL agent to learn a small set of skills and
then use them to generate a potentially very large number of distinct behaviours. A robust way of
combining skills is to do so in the space of cumulants  but in order to accomplish this one needs
to solve two problems: (1) establish a well-deﬁned mapping between cumulants and skills and (2)
deﬁne a mechanism to implement the combined skills without having to learn them.
The two main technical contributions of this paper are solutions for these challenging problems. First 
we have shown that every deterministic option can be induced by a cumulant deﬁned in an extended
domain. This novel theoretical result provides a way of thinking about options whose interest may
go beyond the current work. Second  we have described how to use GPE and GPI to synthesise
combined options on-the-ﬂy  with no learning involved. To the best of our knowledge  this is the only
method to do so in general MDPs with performance guarantees for the combined options.
We used the above formalism to introduce OK  an interface to an RL problem in which actions
correspond to combined skills. Since OK is compatible with essentially any RL method  it can be
readily used to endow our agents with the ability to combine skills. In describing the analogy with a
keyboard that inspired our work  Sutton [23] calls for the need of “something larger than actions  but
more combinatorial than the conventional notion of options.” We believe OK provides exactly that.

9

012345Steps1e705101520253035Average return per episodeOK trainingGPE and GPI + OKGPE and GPI + ENT-OKAVC + OKAVC + ENT-OKDPGAcknowledgements

We thank Joseph Modayil for ﬁrst bringing the subgoal keyboard idea to our attention  and also
for the subsequent discussions on the subject. We are also grateful to Richard Sutton  Tom Schaul 
Daniel Mankowitz  Steven Hansen  and Tuomas Haarnoja for the invaluable conversations that helped
us develop our ideas and improve the paper. Finally  we thank the anonymous reviewers for their
comments and suggestions.

References
[1] P. Bacon  J. Harb  and D. Precup. The option-critic architecture. In Proceedings of the AAAI

Conference on Artiﬁcial Intelligence (AAAI)  2017.

[2] A. Barreto  S. Hou  D. Borsa  D. Silver  and D. Precup. Fast reinforcement learning with

generalized policy updates. Manuscript in preparation.

[3] A. Barreto  W. Dabney  R. Munos  J. Hunt  T. Schaul  H. van Hasselt  and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
Systems (NIPS)  2017.

[4] A. Barreto  D. Borsa  J. Quan  T. Schaul  D. Silver  M. Hessel  D. Mankowitz  A. Zidek  and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised
policy improvement. In Proceedings of the International Conference on Machine Learning
(ICML)  2018.

[5] D. Borsa  A. Barreto  J. Quan  D. J. Mankowitz  H. van Hasselt  R. Munos  D. Silver  and
T. Schaul. Universal successor features approximators. In International Conference on Learning
Representations (ICLR)  2019.

[6] M. da Silva  F. Durand  and J. Popovi´c. Linear Bellman combination for control of character

animation. ACM Transactions on Graphics  28(3):82:1–82:10  2009.

[7] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In Advances in Neural Information

Processing Systems (NIPS)  1993.

[8] T. G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decompo-

sition. Journal of Artiﬁcial Intelligence Research  13:227–303  2000.

[9] R. Fox  A. Pakman  and N. Tishby. Taming the noise in reinforcement learning via soft updates.

In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2016.

[10] K. Frans  J. Ho  X. Chen  P. Abbeel  and J. Schulman. Meta learning shared hierarchies. In

International Conference on Learning Representations (ICLR)  2018.

[11] T. Haarnoja  H. Tang  P. Abbeel  and S. Levine. Reinforcement learning with deep energy-based
policies. In Proceedings of the International Conference on Machine Learning (ICML)  2017.

[12] T. Haarnoja  K. Hartikainen  P. Abbeel  and S. Levine. Latent space policies for hierarchical
reinforcement learning. In Proceedings of the International Conference on Machine Learning
(ICML)  2018.

[13] T. Haarnoja  V. Pong  A. Zhou  M. Dalal  P. Abbeel  and S. Levine. Composable deep reinforce-
ment learning for robotic manipulation. In IEEE International Conference on Robotics and
Automation (ICRA)  2018.

[14] T. Haarnoja  A. Zhou  P. Abbeel  and S. Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Proceedings of the International
Conference on Machine Learning (ICML)  2018.

[15] N. Heess  G. Wayne  Y. Tassa  T. P. Lillicrap  M. A. Riedmiller  and D. Silver. Learning
and transfer of modulated locomotor controllers. CoRR  abs/1610.05182  2016. URL http:
//arxiv.org/abs/1610.05182.

10

[16] J. J. Hunt  A. Barreto  T. P. Lillicrap  and N. Heess. Entropic policy composition with generalized
policy improvement and divergence correction. In Proceedings of the International Conference
on Machine Learning (ICML)  2019.

[17] L. P. Kaelbling. Hierarchical learning in stochastic domains: Preliminary results. In Proceedings

of the International Conference on Machine Learning (ICML)  2014.

[18] R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. In Proceedings of

the Conference on Advances in Neural Information Processing Systems (NIPS)  1997.

[19] M. L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  1994.

[20] A. M. Saxe  A. C. Earle  and B. Rosman. Hierarchy through composition with multitask
LMDPS. In Proceedings of the International Conference on Machine Learning (ICML)  2017.

[21] J. Schulman  P. Moritz  S. Levine  M. Jordan  and P. Abbeel. High-dimensional continuous
control using generalized advantage estimation. In Proceedings of the International Conference
on Learning Representations (ICLR)  2016.

[22] D. Silver  G. Lever  N. Heess  T. Degris  D. Wierstra  and M. Riedmiller. Deterministic policy
gradient algorithms. In Proceedings of the International Conference on Machine Learning
(ICML)  2014.

[23] R. Sutton. Toward a new view of action selection: The subgoal keyboard. Slides presented
at the Barbados Workshop on Reinforcement Learning  2016. URL http://barbados2016.
rl-community.org/RichSutton2016.pdf?attredirects=0&d=1.

[24] R. Sutton  D. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems
(NIPS)  2000.

[25] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  2018.

[26] R. S. Sutton  D. Precup  and S. Singh. Between MDPs and semi-MDPs: a framework for

temporal abstraction in reinforcement learning. Artiﬁcial Intelligence  112:181–211  1999.

[27] R. S. Sutton  J. Modayil  M. Delp  T. Degris  P. M. Pilarski  A. White  and D. Precup. Horde:
A scalable real-time architecture for learning knowledge from unsupervised sensorimotor
In International Conference on Autonomous Agents and Multiagent Systems
interaction.
(AMAS)  2011.

[28] E. Todorov. Linearly-solvable Markov decision problems. In Advances in Neural Information

Processing Systems (NIPS)  2007.

[29] E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information

Processing Systems (NIPS)  2009.

[30] E. Todorov  T. Erez  and Y. Tassa. MuJoCo: A physics engine for model-based control. In

Intelligent Robots and Systems (IROS)  2012.

[31] B. Van Niekerk  S. James  A. Earle  and B. Rosman. Composing value functions in reinforcement
learning. In Proceedings of the International Conference on Machine Learning (ICML)  2019.

[32] A. S. Vezhnevets  S. Osindero  T. Schaul  N. Heess  M. Jaderberg  D. Silver  and K. Kavukcuoglu.
FeUdal networks for hierarchical reinforcement learning. In Proceedings of the International
Conference on Machine Learning (ICML)  pages 3540–3549  2017.

[33] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning  8:229–256  1992.

[34] B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal

Entropy. PhD thesis  Carnegie Mellon University  2010.

11

,Andre Barreto
Diana Borsa
Shaobo Hou
Gheorghe Comanici
Eser Aygün
Philippe Hamel
Daniel Toyama
Jonathan hunt
Shibl Mourad
David Silver
Doina Precup