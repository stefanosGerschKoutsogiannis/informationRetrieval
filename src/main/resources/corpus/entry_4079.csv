2019,Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function,We  consider  online  learning  in  episodic loop-free Markov decision processes (MDPs)  where the loss function can change arbitrarily between episodes. 
The transition function is fixed but unknown to the learner  and the learner only observes bandit feedback (not the entire loss function).
For this problem we develop no-regret algorithms that perform asymptotically as well as the best stationary policy in hindsight.
Assuming that all states are reachable with probability $\beta > 0$ under any policy  we give a regret bound of $\tilde{O} ( L|X|\sqrt{|A|T} / \beta )$  where $T$ is the number of episodes  $X$ is the state space  $A$ is the action space  and $L$ is the length of each episode.
When this assumption is removed we give a regret bound of $\tilde{O} ( L^{3/2} |X| |A|^{1/4} T^{3/4})$  that holds for an arbitrary transition function. 
To our knowledge these are the first algorithms that in our setting handle both bandit feedback and an unknown transition function.,Online Stochastic Shortest Path with

Bandit Feedback and Unknown Transition Function

Aviv Rosenberg

Tel Aviv University  Israel
avivros007@gmail.com

Yishay Mansour

Tel Aviv University  Israel
and Google Research  Israel

mansour.yishay@gmail.com

Abstract

We consider online learning in episodic loop-free Markov decision processes
(MDPs)  where the loss function can change arbitrarily between episodes. The
transition function is ﬁxed but unknown to the learner  and the learner only observes
bandit feedback (not the entire loss function). For this problem we develop no-
regret algorithms that perform asymptotically as well as the best stationary policy
in hindsight. Assuming that all states are reachable with probability β > 0

under any policy  we give a regret bound of ˜O(L|X|(cid:112)|A|T /β)  where T is the

number of episodes  X is the state space  A is the action space  and L is the
length of each episode. When this assumption is removed we give a regret bound
of ˜O(L3/2|X||A|1/4T 3/4)  that holds for an arbitrary transition function. To our
knowledge these are the ﬁrst algorithms that in our setting handle both bandit
feedback and an unknown transition function.

1

Introduction

Reinforcement learning is the study of problems involving sequential decision making in an unknown
stochastic environment  and Markov decision process [1] is the most common model used in this
ﬁeld. In this model both the losses and dynamics of the environment are assumed to be stationary
over time. However  in real world applications  the losses might change over time  even throughout
the learning process.
To address this issue the adversarial MDP model [2] was proposed. In this model the loss function
can change arbitrarily  while still assuming a ﬁxed stochastic transition function. Since the absolute
total expected loss of the learner becomes meaningless  the learner’s performance is measured by the
regret - comparing to the best stationary policy in hindsight.
The adversarial MDP model is actually a combination of the original MDP model with online learning
[3]  which considers decision making against an adversary but in a stateless environment. Online
learning problems are usually divided into two types of feedback. The ﬁrst is full information
feedback  in which the learner observes the entire loss function after it made its decision. The second
is the more challenging bandit feedback  in which the learner only observes the losses related to the
actions it chose.
In this paper we propose the ﬁrst algorithms for the adversarial MDP model with bandit feedback
and an unknown transition function. Our algorithms are based on the recently proposed UC-O-REPS
algorithm [4]  that assumes unknown transition function but full information feedback. Our ﬁrst
algorithm  "Bounded Bandit UC-O-REPS"  assumes that any state is reachable under any policy with
Bandit UC-O-REPS"  removes this assumption and achieves a regret bound of ˜O(L3/2|X||A|1/4T 3/4).

probability β > 0 and achieves a regret bound of ˜O(L|X|(cid:112)|A|T /β). Our second algorithm  "Shifted

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The algorithms are fairly simple  and the main challenge is the analysis of the regret and computational
complexity.

1.1 Related Work

enough T (which is optimal [5])  and some more recent papers achieve similar optimal results [8  9].

tight.
The work of [2]  which presented the adversarial MDP model  assumes full knowledge of the transition
function and full information feedback about the losses. They propose an algorithm  MDP-E  which

The works of [5] and [6] assume an unknown ﬁxed MDP  and achieve a ˜O(L|X|(cid:112)|A|T ) regret
compared to the optimal policy. A recent work by [7] achieves ˜O((cid:112)L|X||A|T ) regret for large
We remark that the lower bound of Ω((cid:112)L|X||A|T ) by [5] also holds in our setting but might not be
uses an experts algorithm in each state and achieves O(τ 2(cid:112)T ln|A|) regret  where τ is a bound on
achieves an O(L2(cid:112)|A|T /β) regret  where β is deﬁned similarly to our deﬁnition. Later [12]
O-REPS algorithm which guarantees an ˜O((cid:112)L|X||A|T ) regret.
˜O(L|X|(cid:112)|A|T ) regret.
algorithm [14] achieves ˜O(L|X||A|√

The setting where the transition function is unknown is much more challenging and only two
algorithms were previously presented for it  both assume full information feedback. The FPOP
T ) regret  and the recent UC-O-REPS algorithm [4] achieves

the mixing time of the MDP. Another early work in this setting  by [10]  achieves an O(T 2/3) regret.
In the bandit setting  but still assuming full knowledge of the transition function  the work of [11]

eliminate the dependence on β but achieve only ˜O(T 2/3) regret. A later work  by [13]  proposed the

The rest of the paper is organized as follows. Section 2 presents the formal model and problem.
Section 3 presents the concept of occupancy measures  which is the foundation of the UC-O-REPS
algorithm presented in Section 4. Section 5 presents our algorithms  and sections 6 and 7 prove their
regret bounds.

2 The Model

(cid:16)
(cid:111)L−1

k=0

2

t=1

(cid:1)  where X and A are the ﬁnite state and

MDP which is deﬁned by a tuple M =(cid:0)X  A  P {(cid:96)t}T

The Online Stochastic Shortest Path problem (OSSP) considers an episodic loop-free adversarial
action spaces  and P : X × A × X → [0  1] is the transition function such that P (x(cid:48)|x  a) is the
probability to move to state x(cid:48) after performing action a in state x.
We assume that the state space can be decomposed into L non-intersecting layers X0  . . .   XL such
that the ﬁrst and the last layers are singletons  i.e.  X0 = {x0} and XL = {xL}. Furthermore  the
loop-free assumption means that transitions are only possible between consecutive layers.
t=1 be a sequence of loss functions describing the losses at each episode  i.e.  (cid:96)t : X × A →
Let {(cid:96)t}T
[0  1]. We do not make any statistical assumption on the loss functions  i.e.  they can be chosen
arbitrarily.
The interaction between the learner and the environment is described in Algorithm 1. It proceeds
in episodes  where in each episode the learner starts in state x0 and moves forward across the
consecutive layers until it reaches state xL. The learner’s task is to select an action at each state it
visits. Alternatively  we can say that its task at each episode is to choose a stationary (stochastic)
policy  which is a mapping π : X × A → [0  1]  where π(a|x) gives the probability that action a is
selected in state x.

x(t)
0   a(t)
In episode t  the learner traverses a trajectory U (t) =
using the policy πt it chose for this episode. That is  action a(t)
(cid:110)
k+1 is drawn from distribution P (·|x(t)
x(t)
feedback  i.e.  it observes (cid:96)t(U (t)) =

L−1  x(t)
k ) and state
k ). At the end of the episode the learner observes bandit
k   a(t)
k )

0   x(t)
L−1  a(t)
k is chosen using πt(·|x(t)

and not the entire loss function (cid:96)t.

k   a(t)
(cid:96)t(x(t)

1   a(t)

1   . . .   x(t)

L

(cid:17)

Algorithm 1 Learner-Environment Interaction

Parameters: MDP M =(cid:0)X  A  P {(cid:96)t}T

(cid:1)

t=1

for t = 1 to T do

learner starts in state x(t)
for k = 0 to L − 1 do

0 = x0

k ∈ A
learner chooses action a(t)
environment draws new state x(t)
learner observes state x(t)
k+1

k+1 ∼ P (·|x(t)

k   a(t)
k )

learner observes suffered losses (cid:96)t(x(t)

0   a(t)

0 )  (cid:96)t(x(t)

1   a(t)

1 )  . . .   (cid:96)t(x(t)

L−1  a(t)

L−1)

For a policy π we deﬁne its total expected loss with respect to loss function (cid:96) and transition function
P as

L(P  π  (cid:96)) = E

(cid:96)(xk  ak)

where action ak is chosen using π(·|xk) and state xk+1 is drawn from distribution P (·|xk  ak).
At the beginning of each episode t the learner picks a policy πt  and its goal is to minimize its total
expected loss. Its performance will be measured by comparison to the best stationary policy. This is
deﬁned using the regret 

ˆR1:T (P {(cid:96)t}T

t=1) =

L(P  πt  (cid:96)t) − min

π

L(P  π  (cid:96)t)

t=1
where the minimum is taken over all stationary stochastic policies.

t=1

(cid:34)L−1(cid:88)

k=0

T(cid:88)

(cid:35)

(cid:12)(cid:12)(cid:12)P  π

T(cid:88)

3 Occupancy Measures

The O-REPS [13] and UC-O-REPS [4] algorithms showed that the OSSP problem can be reformulated
as an online convex optimization problem  using occupancy measures on the space X × A × X. For
a policy π and a transition function P the occupancy measure qP π is deﬁned as follows:

qP π(x  a  x(cid:48)) = Pr [xk = x  ak = a  xk+1 = x(cid:48)|P  π]

where x ∈ Xk and x(cid:48) ∈ Xk+1. We also introduce the notation k(x) for the index of the layer that x
belongs to  and the two following notations 

qP π(x  a) = Pr [xk = x  ak = a|P  π] =

qP π(x  a  x(cid:48))

(cid:88)

(cid:88)

a∈A

x(cid:48)∈Xk+1
qP π(x  a).

qP π(x) = Pr [xk = x|P  π] =

Notice that every occupancy measure q induces a transition function P q and a policy πq  that can be
computed as follows:

P q(x(cid:48)|x  a) =

q(x  a  x(cid:48))
q(x  a)

πq(a|x) =

;

q(x  a)
q(x)

.

The set of all occupancy measures of an MDP M is denoted as ∆(M )  and can be characterized with
the following lemma from [4].
Lemma 3.1. For every q ∈ [0  1]|X|×|A|×|X| it holds that q ∈ ∆(M ) if and only if:

1. (cid:80)
2. (cid:80)

(cid:80)

(cid:80)
(cid:80)
a∈A q(x  a  x(cid:48)) =(cid:80)

x(cid:48)∈Xk+1

a∈A

x∈Xk

x(cid:48)∈Xk+1

q(x  a  x(cid:48)) = 1 ∀k = 0  . . .   L − 1

(cid:80)
a∈A q(x(cid:48)  a  x) ∀k = 1  . . .   L−1∀x ∈ Xk

x(cid:48)∈Xk−1

3. P q = P (where P is the transition function of M)

3

Thus  the task of the learner becomes selecting an occupancy measure qP πt at the beginning of each
episode  and the regret with respect to an occupancy measures q can be easily reformulated as 

where (cid:104)q  (cid:96)(cid:105) def

= (cid:80)

(cid:80)

x∈X
ˆR1:T (P {(cid:96)t}T

ˆR1:T (q  P {(cid:96)t}T

t=1) =

(cid:104)qP πt − q  (cid:96)t(cid:105)

a∈A q(x  a)(cid:96)(x  a). Therefore the regret of the algorithm is 

t=1) = max
q∈∆(M )

ˆR1:T (q  P {(cid:96)t}T

t=1) = max
q∈∆(M )

(cid:104)qP πt − q  (cid:96)t(cid:105).

T(cid:88)

t=1

T(cid:88)

t=1

4 The UC-O-REPS Algorithm

Our algorithms are based on the recently proposed UC-O-REPS algorithm [4]. The full details of the
algorithm can be found in the original paper  but here we give a brief description.
UC-O-REPS uses the framework of UCRL2 [5] that maintains conﬁdence sets of occupancy measures
that contain ∆(M ) with high probability of at least 1 − δ  for some parameter 0 < δ < 1. It
proceeds in epochs such that an epoch ends every time the number of visits to some state-action
pair is doubled  and the conﬁdence set is recomputed in the beginning of every epoch. For every
(x  a  x(cid:48)) ∈ X × A × Xk(x)+1 it keeps counters Ni(x  a  x(cid:48)) and Ni(x  a) that count the number of
visits up to epoch i  and uses these counters to compute the empirical transition function in epoch i
deﬁned as follows 

¯Pi(x(cid:48)|x  a) =

Ni(x  a  x(cid:48))

max{1  Ni(x  a)} .

The conﬁdence set of epoch i is denoted as ∆(M  i)  and it contains all the occupancy measures
whose induced transition function has L1-distance of at most i from the empirical transition function 
where i is a parameter that determines the size of the conﬁdence set and is deﬁned as follows 

(cid:115)

i(x  a) =

2|Xk(x)+1| ln T|X||A|
max{1  Ni(x  a)} .

δ

Formally  ∆(M  i) contains all occupancy measures qP (cid:48) π such that for every (x  a) 

(cid:107)P (cid:48)(·|x  a) − ¯Pi(·|x  a)(cid:107)1 ≤ i(x  a).

In each episode UC-O-REPS chooses an occupancy measure  from within the current conﬁdence set 
that minimizes a trade-off between the current loss function and the distance to the previously chosen
occupancy measure. Formally  it performs the following steps in each episode given some parameter
η > 0 

˜qt+1 = arg min

q

η(cid:104)q  (cid:96)t(cid:105) + D(q(cid:107)qt)

qt+1 = qPt+1 πt+1 = arg min

q∈∆(M i(t))

where i(t) is the epoch that time step t belongs to  and D(q(cid:107)q(cid:48)) is the unnormalized KL divergence be-
q(cid:48)(x a x(cid:48)) − q(x  a  x(cid:48)) +
q(cid:48)(x  a  x(cid:48)).
In [4] it is shown that all the conﬁdence sets contain ∆(M ) with probability at least 1 − δ  that the

tween two occupancy measures deﬁned as D(q(cid:107)q(cid:48)) =(cid:80)
algorithm can be implemented efﬁciently  and that it achieves a regret bound of O(L|X|(cid:112)|A|T ln T ).

x a x(cid:48) q(x  a  x(cid:48)) ln q(x a x(cid:48))

D(q(cid:107)˜qt+1)

5 Our Algorithms

We deﬁne β(M ) as the minimum probability to visit some state under the worst exploratory policy 
i.e.  β(M ) = minπ minx∈X qP π(x). Moreover  we deﬁne pmin(M ) as the minimal transition
probability  that is  pmin(M ) = minx a x(cid:48) P (x(cid:48)|x  a) where x(cid:48) ∈ Xk(x)+1.
Our ﬁrst algorithm  "Bounded Bandit UC-O-REPS"  is aimed for MDPs where there is a known
positive lower bound on β(M ). Our second algorithm  "Shifted Bandit UC-O-REPS"  works in
general episodic loop-free MDPs and makes use of the ﬁrst algorithm.

4

5.1 Bounded Bandit UC-O-REPS

The "Bounded Bandit UC-O-REPS" algorithm runs UC-O-REPS but with two crucial changes.
Firstly  instead of using (cid:96)t (which we do not have) we use ˆ(cid:96)t which is our estimate of (cid:96)t deﬁned as
follows 

ˆ(cid:96)t(x  a) =

(cid:40) (cid:96)t(x a)
(cid:12)(cid:12)(cid:12)U (1)  . . .   U (t−1)(cid:105)

qt(x a)  
0 

if (x  a) ∈ U (t)
otherwise

.

Notice that this is a biased estimator since Pt may be different from P  
(cid:96)t(x  a)

= qP πt(x  a)

qPt πt(x  a)

E(cid:104)ˆ(cid:96)t(x  a)

= qP πt(x)πt(a|x)

(cid:96)t(x  a)

qPt πt(x)πt(a|x)

(1)

= qP πt(x)

(cid:96)t(x  a)
qPt πt(x)

.

Secondly  because of the bandit feedback we want to ensure that our algorithm performs enough
exploration. For this purpose we constrain the conﬁdence sets to contain only occupancy measures
that visit every state with probability of at least α  where 0 < α < 1 is a parameter. That is  we deﬁne
our conﬁdence set for epoch i as ∆α(M  i) = ∆(M  i) ∩ {q : q(x) ≥ α ∀x}.
Thus our algorithm performs the following steps in each episode 

˜qt+1 = arg min

q

η(cid:104)q  ˆ(cid:96)t(cid:105) + D(q(cid:107)qt)

qt+1 = qPt+1 πt+1 = arg

min

q∈∆α(M i(t))

D(q(cid:107)˜qt+1).

If ∆α(M  i(t)) = ∅  then qt+1 is chosen to be an arbitrary occupancy measure. The efﬁcient
implementation of this algorithm is similar to the one of the original UC-O-REPS algorithm  and is
described in details in the supplementary material (together with full pseudo-code).

5.2 Shifted Bandit UC-O-REPS

The "Shifted Bandit UC-O-REPS" algorithm runs "Bounded Bandit UC-O-REPS" with α = ρ|X|
(where 0 < ρ < 1 is a parameter) but it makes the following change in order to handle the unknown
β(M ) (which may be zero). It shifts the conﬁdence sets by changing the empirical transition function.
That is  instead of using ¯Pi as the empirical transition function for epoch i it uses ¯P (cid:63)
i which is deﬁned
as follows for every k = 0  . . .   L − 1 and for every (x  a  x(cid:48)) ∈ Xk × A × Xk+1 

i (x(cid:48)|x  a) = (1 − ρ) ¯Pi(x(cid:48)|x  a) +
¯P (cid:63)

ρ

|Xk+1| .

To sum up  the new conﬁdence sets are denoted as ∆(cid:63)
qP (cid:48) π such that qP (cid:48) π(x) ≥ α for every x  and for every (x  a) 

α(M  i) and they contain all occupancy measures

(cid:107)P (cid:48)(·|x  a) − ¯P (cid:63)

i (·|x  a)(cid:107)1 ≤ i(x  a).

Clearly this algorithm can be implemented efﬁciently  given the efﬁcient implementation of "Bounded
Bandit UC-O-REPS" (full pseudo-code can be found in the supplementary material for completeness).

6 Regret Analysis - Bounded Bandit UC-O-REPS

In this case we assume that β(M ) > 0 and it is known to the learner (or some positive lower bound
on it). This assumption is quite strong but it holds if  for example  the minimum transition probability
is not zero  i.e.  pmin(M ) > 0. In this case β(M ) ≥ pmin(M ).
def
Notice that if we run "Bounded Bandit UC-O-REPS" with α = β(M )  then ∆(M ) = ∆α(M )
=
∆(M ) ∩ {q : q(x) ≥ α ∀x}. Therefore  using the proof of UC-O-REPS  we have that all the
conﬁdence sets contain ∆(M ) with probability at least 1 − δ.

5

Let q ∈ ∆(M ) = ∆α(M )  and partition the regret into two terms as follows 

T(cid:88)

(cid:32) T(cid:88)

(cid:33)

(cid:32) T(cid:88)

(cid:33)

ˆR1:T (q  P {(cid:96)t}T

t=1) =

(cid:104)qP πt − q  (cid:96)t(cid:105) =

(cid:104)qP πt − qPt πt  (cid:96)t(cid:105)

+

(cid:104)qPt πt − q  (cid:96)t(cid:105)

t=1

t=1

t=1

The ﬁrst term includes the error that comes from the estimation of the unknown transition function and
will be denoted as ˆRAP P
1:T . The second term includes the error that comes from choosing sub-optimal
policies and will be denoted as ˆRON
1:T .
Sections 6.1 and 6.2 bound these two terms and give us the following regret bound.

Theorem 6.1. Let M =(cid:0)X  A  P {(cid:96)t}T
E(cid:104) ˆR1:T (P {(cid:96)t}T

t=1

(cid:1) be an episodic loop-free adversarial MDP  and assume
(cid:105) ≤ O

L|X|(cid:112)|A|T ln T

(cid:32)

(cid:33)

that β(M ) > 0. Then  "Bounded Bandit UC-O-REPS" with α = β(M ) obtains the following regret
bound 

t=1)

β(M )

6.1 Bounding ˆRAP P
1:T

Recall that ˆRAP P
is the difference between the loss of the learner’s chosen policies in M and the
loss of these policies in the “optimistic” MDPs (the ones induced by the occupancy measures qt).
The algorithm minimizes this difference through shrinking of the conﬁdence sets. Notice that 

1:T

T(cid:88)

(cid:104)qP πt − qPt πt  (cid:96)t(cid:105) ≤ T(cid:88)

(cid:107)qP πt − qPt πt(cid:107)1(cid:107)(cid:96)t(cid:107)∞ ≤ T(cid:88)

(cid:107)qP πt − qPt πt(cid:107)1.

ˆRAP P

1:T =

t=1

t=1

t=1

Since the algorithm uses the same framework of conﬁdence sets as the original UC-O-REPS (and all
the conﬁdence sets contain ∆(M ) with high probability)  we can use the following theorem from [4]
to bound this difference.
t=1 be transition functions such that qPt πt ∈
t=1 be policies and let {Pt}T
Theorem 6.2. Let {πt}T
(cid:35)
|X||A|
 
∆(M  i(t)) for every t. Then  when setting δ =

T

E(cid:104) ˆRAP P

1:T

(cid:105) ≤ E

(cid:34) T(cid:88)

(cid:16)

L|X|(cid:112)|A|T ln T

(cid:17)

(cid:107)qP πt − qPt πt(cid:107)1

≤ O

t=1

6.2 Bounding ˆRON
1:T

Lemma 6.3. Let M = (cid:0)X  A  P {(cid:96)t}T

Recall that ˆRON
1:T is the regret for the performance of the online algorithm’s chosen occupancy
measures. Notice that the learner performs the original UC-O-REPS algorithm with respect to the
sequence of loss functions {ˆ(cid:96)t}T
t=1 and the set of occupancy measures ∆α(M ). Therefore  we can
use the regret analysis of the original algorithm to obtain the following result (full proof in the
supplementary material).
every q ∈ ∆α(M )  "Bounded Bandit UC-O-REPS" obtains 
ηL|A|T

(cid:1) be an episodic loop-free adversarial MDP. Then  for
(cid:35)

(cid:34) T(cid:88)

(cid:32)

(cid:33)

|X||A|

L ln

t=1

E

(cid:104)qPt πt − q  ˆ(cid:96)t(cid:105)

≤ O

+

α

L

η

t=1

t=1 and {(cid:96)t}T

t=1 in expectation  and therefore we can derive a bound on ˆRON
1:T .

Now we show that the sequence of occupancy measures chosen by the algorithm performs similarly
on {ˆ(cid:96)t}T

Lemma 6.4. Let M = (cid:0)X  A  P {(cid:96)t}T
(cid:1) be an episodic loop-free adversarial MDP. Then  for
(cid:34) T(cid:88)

every q ∈ ∆α(M )  "Bounded Bandit UC-O-REPS" obtains 

L|X|(cid:112)|A|T ln T

(cid:34) T(cid:88)

(cid:32)

(cid:33)

(cid:35)

t=1

(cid:104)qPt πt − q  ˆ(cid:96)t(cid:105)

− E

(cid:104)qPt πt − q  (cid:96)t(cid:105)

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ O

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E

α

t=1

t=1

6

t=1

=

(2)

(cid:35)

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t=1

t=1

x a

t=1

t=1

(cid:96)t(x  a)
qPt πt(x)

− (cid:96)t(x  a)).

(cid:96)t(x  a)
qPt πt(x)

− (cid:96)t(x  a))

(cid:104)qPt πt − q  (cid:96)t(cid:105)

t=1

(cid:88)

x a

(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)

(cid:104)qPt πt − q  ˆ(cid:96)t(cid:105)

− E

Substituting this back into (2) we get 

(cid:96)t(x  a)(qt(x  a) − q(x  a))

From the law of total expectation we have 

(qt(x  a) − q(x  a))(qP πt(x)

(qt(x  a) − q(x  a))(qP πt(x)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E

(cid:34) T(cid:88)

(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
E(cid:104)(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)(cid:12)(cid:12)(cid:12)U (1)  . . .   U (t−1)(cid:105)(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

t=1

Now for every t we can use the deﬁnition of ˆ(cid:96)t and (1) to obtain 

(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)(cid:105)(cid:12)(cid:12)(cid:12)=

Proof. First we use the linearity of expectation and the fact that qt = qPt πt to obtain 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34) T(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34) T(cid:88)

(cid:34) T(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:34) T(cid:88)
E(cid:104)(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)(cid:12)(cid:12)(cid:12)U (1)  . . .   U (t−1)(cid:105)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34) T(cid:88)
(cid:12)(cid:12)(cid:12)E(cid:104) T(cid:88)
(cid:88)
(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88)
(cid:88)
(cid:34) T(cid:88)
(cid:88)
(cid:34) T(cid:88)
(cid:88)
 T(cid:88)
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1
 T(cid:88)
(cid:34) T(cid:88)
(cid:1) be an episodic loop-free adversarial MDP. Then  when
Corollary 6.1. Let M =(cid:0)X  A  P {(cid:96)t}T
(cid:32)
L|X|(cid:112)|A|T ln T
(cid:105) ≤ O
E(cid:104) ˆRON

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:35)
where the last inequality follows because qPt πt(x) ≥ α  0 ≤ (cid:80)
(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
a q(x  a)(cid:96)t(x  a) ≤ 1. Finally  we use Theorem 6.2 to conclude that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


L|X|(cid:112)|A|T ln T

x a x(cid:48)
(cid:107)qP πt − qPt πt(cid:107)1

a qt(x  a)(cid:96)t(x  a) ≤ 1 and 0 ≤

|qP πt(x  a  x(cid:48)) − qPt πt(x  a  x(cid:48))|

qP πt(x  a  x(cid:48)) − qPt πt(x  a  x(cid:48))

(cid:96)t(x  a)(qt(x  a) − q(x  a))

  "Bounded Bandit UC-O-REPS" obtains 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)
(cid:88)

x

|qP πt(x) − qPt πt(x)|

|qP πt(x) − qPt πt(x)|

qP πt(x) − qPt πt(x)

(cid:104)qt − q  ˆ(cid:96)t − (cid:96)t(cid:105)

(cid:34) T(cid:88)

t=1

≤ E

≤ E

ln

|X||A|
L|A|T

and δ =

|X||A|

T

E

≤ 1
α

=

E

1
α

(cid:88)

a x(cid:48)

t=1

t=1

t=1

E

≤ 1
α

t=1

x

qPt πt(x)

a

(cid:33)

.

(cid:35)

(cid:32)

≤ O

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t=1

x

E

α

(cid:114)

setting η =

t=1

x a

qPt πt(x)

t=1

1:T

α

(cid:33)

α

7 Regret Analysis - Shifted Bandit UC-O-REPS

We remove the assumption that β(M ) > 0  and for this case will use the "Shifted Bandit UC-O-
REPS" algorithm. Notice that the key insight for the regret analysis of "Bounded Bandit UC-O-REPS"

7

By the deﬁnition of ¯P (cid:63)
(cid:107) ¯P (cid:63)

i (·|x  a) − P (cid:63)(·|x  a)(cid:107)1 =

i and P (cid:63) we have that 

(cid:88)
(cid:88)

x(cid:48)

x(cid:48)

=

i (x(cid:48)|x  a) − P (cid:63)(x(cid:48)|x  a)|
| ¯P (cid:63)
|(1 − ρ) ¯Pi(x(cid:48)|x  a) +
ρ
(cid:88)

| ¯Pi(x(cid:48)|x  a) − P (x(cid:48)|x  a)|

= (1 − ρ)
= (1 − ρ)(cid:107) ¯Pi(·|x  a) − P (·|x  a)(cid:107)1 ≤ i(x  a)

x(cid:48)

|Xk+1| − (1 − ρ)P (x(cid:48)|x  a) − ρ

|Xk+1||

is that by setting α = β(M )  we get that all the conﬁdence sets contain ∆(M ) with high probability.
The idea behind "Shifted Bandit UC-O-REPS" is to work on an imaginary MDP M (cid:63) that is close to
M but has the property β(M (cid:63)) > 0.

The transition function for the MDP M (cid:63) = (cid:0)X  A  P (cid:63) {(cid:96)t}T

(cid:1) is deﬁned as follows for every

t=1

k = 0  . . .   L − 1 and for every (x  a  x(cid:48)) ∈ Xk × A × Xk+1 
P (cid:63)(x(cid:48)|x  a) = (1 − ρ)P (x(cid:48)|x  a) +

This means that the minimal transition probability is positive  i.e.  pmin(M (cid:63)) ≥ ρ|X| > 0. Therefore 
β(M (cid:63)) ≥ ρ|X| > 0 and we can run "Bounded Bandit UC-O-REPS" on M (cid:63). The problem is that our
data is sampled from M  but we need to build conﬁdence sets that contain ∆(M (cid:63)) and not ∆(M ).
The following lemma shows that shifting the conﬁdence sets obtains this desired property: all the
conﬁdence sets contain ∆(M (cid:63)) with probability at least 1 − δ.
Lemma 7.1. If ∆(M ) ⊆ ∆(M  i)  then ∆(M (cid:63)) ⊆ ∆(cid:63)
α(M  i).
Proof. Let qP (cid:63) π ∈ ∆(M (cid:63)). First of all  since β(M (cid:63)) ≥ ρ|X| = α we have that qP (cid:63) π(x) ≥ α for
every x. Now  Since ∆(M ) ⊆ ∆(M  i) we have that for every (x  a) 
(cid:107) ¯Pi(·|x  a) − P (·|x  a)(cid:107)1 ≤ i(x  a).

ρ

|Xk+1| .

and therefore qP (cid:63) π ∈ ∆(cid:63)

α(M  i) and ∆(M (cid:63)) ⊆ ∆(cid:63)

α(M  i).

(cid:32) T(cid:88)

=

t=1

(cid:104)qP πt − qP (cid:63) πt  (cid:96)t(cid:105)

+

(cid:33)

(cid:32) T(cid:88)

(cid:33)

(cid:32) T(cid:88)

(cid:33)

Now we divide the regret into two parts: the regret of "Bounded Bandit UC-O-REPS" in M (cid:63) and the
difference in the performance of policies in M and M (cid:63). Formally  the regret of any q = qP π ∈ ∆(M )
is partitioned as follows 
ˆR1:T (qP π  P {(cid:96)t}T

(cid:104)qP πt − qP π  (cid:96)t(cid:105)

T(cid:88)

t=1) =

(cid:104)qP (cid:63) πt − qP (cid:63) π  (cid:96)t(cid:105)

+

(cid:104)qP (cid:63) π − qP π  (cid:96)t(cid:105)

.

t=1

t=1

t=1

O

√

(cid:18)

(cid:19)

Since (cid:107)P (·|x  a) − P (cid:63)(·|x  a)(cid:107)1 ≤ 2ρ for every (x  a)  we can use Corollary E.2 in the supple-
mentary material to bound the ﬁrst and third terms as O(ρL2T ). The second term includes the
regret of "Bounded Bandit UC-O-REPS" in M (cid:63) so according to Theorem 6.1 we can bound it as

|A|T ln T

√
L|X|
ρ/|X|

(cid:18)
Theorem 7.2. Let M = (cid:0)X  A  P {(cid:96)t}T
E(cid:104) ˆR1:T (P {(cid:96)t}T

"Shifted Bandit UC-O-REPS" with ρ =

|A|T ln T
ρ

. Thus we get the following regret bound.

(cid:19)
(cid:1) be an episodic loop-free adversarial MDP. Then 
4(cid:113)|A| ln T
(cid:16)
(cid:105) ≤ O

L3/2|X||A|1/4T 3/4 ln1/4 T

obtains the following regret bound 

(cid:17)

|X|√

L

L|X|2

= O

t=1

T

t=1)

Remark 7.1. One might wonder if a regret of O(T 2/3) is achievable by an algorithm that ﬁrst explores
to estimate the transition function and then runs a known algorithm that assumes full knowledge of
the transition function. While this approach is possible in the classic online learning setting  it is
unclear how to implement it in our setting since estimating the transition function properly should
take about T 2/3/β(M ) episodes. This becomes even more complicated when β(M ) = 0.

8

8 Conclusions and Future Work

In this paper we considered online learning in episodic loop-free adversarial MDPs where the losses
can change arbitrarily between episodes. We assumed the transition function is completely unknown
to the learner and that it only observes bandit feedback. Our algorithms are based on the recently
proposed UC-O-REPS algorithm and achieve ˜O(L3/2|X||A|1/4T 3/4) regret for the general case  and

˜O(L|X|(cid:112)|A|T /β) regret for the case where any state is reachable under any policy with probability

at least β > 0.
Throughout the paper we assumed that the MDP is loop-free. However  it is important to mention
that any episodic MDP can be transformed into a loop-free MDP by duplicating the state space L
times  i.e.  a state x becomes states (x  k) where k = 0  . . .   L. Therefore  our algorithms work in
any episodic MDP but the regret bound has a worse dependence on L.
The algorithms we proposed are the ﬁrst to handle the setting of both an unknown transition function

and a bandit feedback  but our results are still far from the known lower bound of Ω((cid:112)L|X||A|T ).

Future work should be done to ﬁnd algorithms with stronger regret bounds and speciﬁcally ones that
remove the loop-free assumption and still get good dependence on L.
Another line of work to be considered is ﬁnding tighter lower bounds for this speciﬁc problem. Since
this problem is much more difﬁcult than the usual reinforcement learning setting and techniques that
involve the value function cannot be implemented here naturally  it might be the case that the lower
bound of [5] cannot be achieved in this setting.

Acknowledgements

This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Tel
Aviv University Yandex Initiative in Machine Learning.

References
[1] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  New York  NY  USA  1st edition  1994.

[2] Eyal Even-Dar  Sham M. Kakade  and Yishay Mansour. Online Markov Decision Processes.

Math. Oper. Res.  34(3):726–736  2009. (preliminary version NIPS 2004).

[3] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge University

Press  2006.

[4] Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov
decision processes. In Proceedings of the 36th International Conference on Machine Learning 
ICML 2019  9-15 June 2019  Long Beach  California  USA  pages 5478–5486  2019.

[5] Peter Auer  Thomas Jaksch  and Ronald Ortner. Near-optimal regret bounds for reinforcement

learning. In Advances in Neural Information Processing Systems  pages 89–96  2008.

[6] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence (UAI)  pages 35–42  2009.

[7] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for
In Proceedings of the 34th International Conference on Machine

reinforcement learning.
Learning (ICML)  pages 263–272  2017.

[8] Chi Jin  Zeyuan Allen-Zhu  Sébastien Bubeck  and Michael I. Jordan. Is q-learning provably
efﬁcient? In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal 
Canada.  pages 4868–4878  2018.

9

[9] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In Proceedings of the 36th
International Conference on Machine Learning  ICML 2019  9-15 June 2019  Long Beach 
California  USA  pages 7304–7312  2019.

[10] Jia Yuan Yu  Shie Mannor  and Nahum Shimkin. Markov Decision Processes with arbitrary

reward processes. Math. Oper. Res.  34(3):737–757  2009.

[11] Gergely Neu  András György  and Csaba Szepesvári. The online loop-free stochastic shortest-

path problem. In Conference on Learning Theory (COLT)  pages 231–243  2010.

[12] Gergely Neu  András György  Csaba Szepesvári  and András Antos. Online Markov Decision

Processes under bandit feedback. IEEE Trans. Automat. Contr.  59(3):676–691  2014.

[13] Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes
by relative entropy policy search. In Advances in Neural Information Processing Systems  pages
1583–1591  2013.

[14] Gergely Neu  András György  and Csaba Szepesvári. The adversarial stochastic shortest path
problem with unknown transition probabilities. In Proceedings of the Fifteenth International
Conference on Artiﬁcial Intelligence and Statistics  (AISTATS)  pages 805–813  2012.

10

,Mikhail Yurochkin
XuanLong Nguyen
Aviv Rosenberg
Yishay Mansour