2018,PCA of high dimensional random walks with comparison to neural network training,One technique to visualize the training of neural networks is to perform PCA on the parameters over the course of training and to project to the subspace spanned by the first few PCA components.  In this paper we compare this technique to the PCA of a high dimensional random walk.  We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the first few PCA components  and that the projection of the trajectory onto any subspace spanned by PCA components is a Lissajous curve.  We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e.  a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting  but will instead be trapped at a fixed distance from the minimum.  We finally analyze PCA projected training trajectories for: a linear model trained on CIFAR-10; a fully connected model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases  both the distribution of PCA eigenvalues and the projected trajectories resemble those of a random walk with drift.,PCA of high dimensional random walks with

comparison to neural network training

Joseph M. Antognini∗

Whisper AI

Jascha Sohl-Dickstein

Google Brain

joe.antognini@gmail.com

jaschasd@google.com

Abstract

One technique to visualize the training of neural networks is to perform PCA on
the parameters over the course of training and to project to the subspace spanned by
the ﬁrst few PCA components. In this paper we compare this technique to the PCA
of a high dimensional random walk. We compute the eigenvalues and eigenvectors
of the covariance of the trajectory and prove that in the long trajectory and high
dimensional limit most of the variance is in the ﬁrst few PCA components  and that
the projection of the trajectory onto any subspace spanned by PCA components is a
Lissajous curve. We generalize these results to a random walk with momentum and
to an Ornstein-Uhlenbeck processes (i.e.  a random walk in a quadratic potential)
and show that in high dimensions the walk is not mean reverting  but will instead be
trapped at a ﬁxed distance from the minimum. We ﬁnally analyze PCA projected
training trajectories for: a linear model trained on CIFAR-10; a fully connected
model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases 
both the distribution of PCA eigenvalues and the projected trajectories resemble
those of a random walk with drift.

1

Introduction

Deep neural networks (NNs) are extremely high dimensional objects. A popular deep NN for image
recognition tasks  ResNet-50 (He et al.  2016)  has ∼25 million parameters for example  and it is
common for language models to have more than one billion parameters (Jozefowicz et al.  2016). This
overparameterization may be responsible for NN’s impressive generalization performance (Novak
et al.  2018). Simultaneously  the high dimensional nature of NNs makes them difﬁcult to reason
about.

Over the decades of NN research  the common lore about the geometry of the loss landscape of NNs
has changed dramatically. In the early days of NN research it was believed that NNs were difﬁcult
to train because they tended to get stuck in suboptimal local minima. Later  Dauphin et al. (2014)
argued that the true scourge of NN optimization was saddle points  not local minima. Choromanska
et al. (2015) further used a spherical spin-glass model to conjecture that local minima of NNs are
not much worse than global minima. Baity-Jesi et al. (2018) showed that in the typical case of an
over-parameterized NN the dynamics of NN optimization are different from glassy systems  and
claimed that the difﬁculties with NN optimization were instead due to vast plateaus where the gradient
is very small. There has also been active debate as to whether the geometry of the loss landscape
around minima can inform the NN’s ability to generalize. Hochreiter & Schmidhuber (1997) and
Keskar et al. (2017) have claimed that NNs that generalize better tend to ﬁnd ﬂatter minima  though
Dinh et al. (2017) countered that due to the scale-free nature of NNs  there always exist sharp minima
that generalize equally well.

∗Work done as a Google AI Resident.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

To help resolve these questions  we would ideally like to be able to visualize the loss landscapes
of NNs  but this is a difﬁcult  perhaps even futile  task because it involves embedding an extremely
high dimensional space into very few dimensions — typically one or two. Goodfellow et al. (2015)
introduced a visualization technique in which the loss is plotted along a straight line from the initial
point to the ﬁnal point of training (the “royal road”). The authors found that the loss often decreased
monotonically along this path. They further considered the loss in the space from the residuals
between the NN’s trajectory to this royal road (note that while this is a two-dimensional manifold  it
is not a linear subspace). Lorch (2016) and Lipton (2016) proposed another visualization technique
in which principal component analysis (PCA) is performed on the NN trajectory and the trajectory is
projected into the subspace spanned by the lowest PCA components. Lipton (2016) noted that most
of the variance was in a small number of PCA components. Li et al. (2018) explored this technique
in more depth by plotting 2-dimensional cross-sections of the loss landscape spanned by the ﬁrst two
PCA components.

In this paper we consider the theory behind this visualization technique. We show that PCA projections
of random walks in ﬂat space qualitatively have many of the same properties as projections of NN
training trajectories. We then generalize these results to a random walk with momentum and a random
walk in a quadratic potential  also known as an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein 
1930). This process is more similar to NN optimization since it consists of a deterministic component
(the true gradient) plus a stochastic component. In fact  recent work has suggested that stochastic
gradient descent (SGD) approximates a random walk in a quadratic potential (Ahn et al.  2012; Mandt
et al.  2016; Smith & Le  2018). Finally  we perform experiments on linear models and large NNs to
show how closely they match this simpliﬁed model.

The approach we take to study the properties of the PCA of high dimensional random walks in ﬂat
space follows that of Moore et al. (2018)  but we correct several errors in their argument  notably in the
values of the matrix ST S and the trace of (ST S)−1 in Eq. 10. We also ﬁll in some critical omissions 
particularly the connection between banded Toeplitz matrices and circulant matrices. We extend their
contribution by proving that the trajectories of high dimensional random walk in PCA subspaces
are Lissajous curves and generalizing to random walks with momentum and Ornstein-Uhlenbeck
processes.

2 PCA of random walks in ﬂat space

2.1 Preliminaries

Let us consider a random walk in d-dimensional space consisting of n steps where every step is
equal to the previous step plus a sample from an arbitrary probability distribution  P  with zero mean
and a ﬁnite covariance matrix.2 For simplicity we shall assume that the covariance matrix has been
normalized so that its trace is 1. This process can be written in the form

(1)
where xt is a d-dimensional vector and x0 = 0. If we collect the xts together in an n × d dimensional
design matrix X  we can then write this entire process in matrix form as

xt = xt−1 + ξt 

ξt ∼ P 

(2)
where the matrix S is an n× n matrix consisting of 1 along the diagonal and -1 along the subdiagonal 

SX = R 

 

(3)

S ≡





0

1
−1
1
0 −1
...
. . .
0
···

0

0

···
. . .
. . .
. . .

0

1
. . .
0
0 −1 1

0
...





2The case of a constant non-zero mean corresponds to a random walk with a constant drift term. This is not
an especially interesting extension from the perspective of PCA because in the limit of a large number of steps
the ﬁrst PCA component will simply pick out the direction of the drift (i.e.  the mean)  and the remaining PCA
components will behave as a random walk without a drift term.

2

and the matrix R is an n × d matrix where every column is a sample from P. Thus X = S−1R.

To perform PCA  we need to compute the eigenvalues and eigenvectors of the covariance matrix
T ˆX  where ˆX is the matrix X with the mean of every dimension across all steps subtracted. ˆX can
ˆX
be found by applying the n × n centering matrix  C:

ˆX = CX  C ≡ I −

1
n

11T .

(4)

We now note that the analysis is simpliﬁed considerably by instead ﬁnding the eigenvalues and
T
T
eigenvectors of the matrix ˆX ˆX
.
The eigenvectors are similarly related by vk = XT uk  where vk is a (non-normalized) eigenvector of
T ˆX  and uk is the corresponding eigenvector of ˆX ˆX
ˆX

T ˆX are the same as those of ˆX ˆX

. The non-zero eigenvalues of ˆX

T

.

We therefore would like to ﬁnd the eigenvalues and eigenvectors of the matrix

T
ˆX ˆX

= CS−1RRT S−T C 

(5)

where we note that CT = C. Consider the middle term  RRT . In the limit d ≫ n we will have
RRT → I because the off diagonal terms will be E[ξi]2 = 0  whereas the diagonal terms will be
E[ξ2] =Pd
V[ξi] = 1. (Recall that we have assumed that the covariance of the noise distribution
is normalized; if the covariance is not normalized  this simply introduces an overall scale factor given
by the trace of the covariance.) We therefore have the simpliﬁcation

i=0

T
ˆX ˆX

= CS−1S−T C.

(6)

2.2 Asymptotic convergence to circulant matrices

Let us consider the new middle term  S−1S−T = (ST S)−1. The matrix S is a banded Toeplitz matrix.
Gray et al. (2006) has shown that banded Toeplitz matrices asymptotically approach circulant matrices
as the size of the matrix grows. In particular  Gray et al. (2006) showed that banded Toeplitz matrices
have the same inverses  distribution of eigenvalues  and eigenvectors as their corresponding circulant
matrices in this asymptotic limit (see especially theorem 4.1 and subsequent material from Gray
et al. 2006). Zhu & Wakin (2017) has furthermore proved a stronger result that under some weak
conditions all eigenvalues of a banded Toeplitz matrix are equal to all eigenvalues of a corresponding
circulant matrix in the limit of large matrices. Thus in our case  if we consider the limit of a large

number of steps  S asymptotically approaches a circulant matrixeS that is equal to S in every entry
except the top right  where there appears a −1 instead of a 0.3
With the limiting circulant behavior of S in mind  the problem simpliﬁes considerably. We note that
C is also a circulant matrix  and furthermore the product of two circulant matrices is circulant  the
transpose of a circulant matrix is circulant  and the inverse of a circulant matrix is circulant. Thus
T
the matrix ˆX ˆX
is asymptotically circulant as n → ∞. Finding the eigenvectors is trivial because
the eigenvectors of all circulant matrices are the Fourier modes. To ﬁnd the eigenvalues we must
explicitly consider the values of ˆX ˆX
. The matrix ST S consists of a 2 along the diagonal  -1 along
the subdiagonal and superdiagonal  and 0 elsewhere  with the exception of the bottom right corner
where there appears a 1 instead of a 2.

T

While this matrix is not a banded Toeplitz  it is asymptotically equivalent to a banded Toeplitz matrix
because it differs from a banded Toeplitz matrix by a ﬁnite amount in a single location (Böttcher et al. 
2003). We now note that multiplication of the centering matrix does not change either the eigenvector
or the eigenvalues of this matrix since all vectors with zero mean are eigenvectors of the centering
matrix with eigenvalue 1  and all Fourier modes but the ﬁrst have zero mean. Thus the eigenvalues of
T
ˆX ˆX
can be determined by the inverse of the non-zero eigenvalues of ST S  which is an asymptotic
circulant matrix. The kth eigenvalue of a circulant matrix with entries c0  c1  . . . in the ﬁrst row is

λcirc k = c0 + cn−1ωk + cn−2ω2

k + . . . + c1ωn−1

k

 

(7)

3We note in passing that eS is the exact representation of a closed random walk.

3

where ωk is the kth root of unity. The imaginary parts of the roots of unity cancel out  leaving the kth
eigenvalue of ST S to be

and the kth eigenvalue of ˆX ˆX

T

to be

λST S k

= 2(cid:20)1 − cos(cid:18) πk

n (cid:19)(cid:21)  

λ ˆX ˆX

T

 k

=

1

2(cid:20)1 − cos(cid:18) πk

n (cid:19)(cid:21)−1

(8)

(9)

.

The sum of the eigenvalues is given by the trace of (ST S)−1 = S−1S−T   and S−1 is given by a lower
triangular matrix with ones everywhere on and below the diagonal. The trace of (ST S)−1 is therefore
given by

and so the explained variance ratio from the kth PCA component  ρk in the limit n → ∞ is

Tr(cid:16)S−1S−T(cid:17) =

1
2

n(n + 1) 

ρk ≡

λk

Tr(cid:16)S−1S−T(cid:17) =

1

2(cid:2)1 − cos(cid:0) πk

2 n(n + 1)

n (cid:1)(cid:3)−1

1

.

(10)

(11)

(12)

(15)

(16)

If we let n → ∞ we can consider only the ﬁrst term in a Taylor expansion of the cosine term.
Requiring thatP∞

k=1 ρk = 1  the explained variance ratio is

ρk =

6
π2k2 .

We test Eq. 12 empirically in Fig. 5 in the supplementary material.

We pause here to marvel that the explained variance ratio of a random walk in the limit of inﬁnite
dimensions is highly skewed towards the ﬁrst few PCA components. Roughly 60% of the variance
is explained by the ﬁrst component  ∼80% by the ﬁrst two components  ∼95% by the ﬁrst 12
components  and ∼99% by the ﬁrst 66 components.
2.3 Projection of the trajectory onto PCA components

Let us now turn to the trajectory of the random walk when projected onto the PCA components. The
trajectory projected onto the kth PCA component is

(13)
where ˆvk is the normalized vk. We ignore the centering operation from here on because it changes
neither the eigenvectors nor the eigenvalues. From above  we then have

XPCA k = Xˆvk 

XPCA k =

uk.

(14)

By the symmetry of the eigenvalue equations XXT u = λu and XT Xv = λv  it can be shown that

Since uk is simply the kth Fourier mode  we therefore have

λk
kvkk

Xvk =

XXT uk =

1
kvkk

1
kvkk
kvkk = kXT ukk = √λ.
XPCA k =r 2λk
n (cid:19) .
cos(cid:18) πkt

n

This implies that the random walk trajectory projected into the subspace spanned by two PCA
components will be a Lissajous curve. In Fig. 1 we plot the trajectories of a high dimensional random
walk projected to various PCA components and compare to the corresponding Lissajous curves. We
perform 1000 steps of a random walk in 10 000 dimensions and ﬁnd an excellent correspondence
between the empirical and analytic trajectories. We additionally show the projection onto the ﬁrst
few PCA components over time in Fig. 6 in the supplementary material.

While our experiments thus far have used an isotropic Gaussian distribution for ease of computation 
we emphasize that these results are completely general for any probability distribution with zero
mean and a ﬁnite covariance matrix with rank much larger than the number of steps. We include
the PCA projections and eigenvalue distributions of random walks using non-isotropic multivariate
Gaussian distributions in Figs. 7 and 8 in the supplementary material.

4

2

A
C
P

3

A
C
P

4

A
C
P

5

A
C
P

8
6
4
2
0
−2
−4
−6
−8

6

4

2

0

−2

−4

−6

4
3
2
1
0
−1
−2
−3
−4

4

3

2

1

0

−1

−2

−3

−10

0

10

PCA1

−10

0

10

PCA1

−10

0

10

PCA1

−10

0

10

PCA1

3

A
C
P

4

A
C
P

5

A
C
P

6

4

2

0

−2

−4

−6

4
3
2
1
0
−1
−2
−3
−4

4

3

2

1

0

−1

−2

−3

R

a

n

d

o

m

 

w

a

lk

−4

0

4

PCA3

−4

0

4

PCA3

5

A
C
P

4

3

2

1

0

−1

−2

−3

−3 −1

1
PCA4

3

−6 −2

2
PCA2

6

−6 −2

2
PCA2

6

−6 −2

2
PCA2

6

4

A
C
P

5

A
C
P

4
3
2
1
0
−1
−2
−3
−4

4

3

2

1

0

−1

−2

−3

y

y

y

y

8
6
4
2
0
−2
−4
−6
−8

6

4

2

0

−2

−4

−6

4
3
2
1
0
−1
−2
−3
−4

3

2

1

0

−1

−2

−3

−10

10

0
x

−10

10

0
x

−10

10

0
x

−10

10

0
x

y

y

y

6

4

2

0

−2

−4

−6

4
3
2
1
0
−1
−2
−3
−4

3

2

1

0

−1

−2

−3

Lis

s

a
j
o

u

s 
c

u

r

v

−6 −2

2

6

x

e

s

y

y

4
3
2
1
0
−1
−2
−3
−4

3

2

1

0

−1

−2

−3

−6 −2

2

6

x

−6 −2

2

6

x

−4

0
x

−4

0
x

4

4

y

3

2

1

0

−1

−2

−3

−3 −1

1

3

x

Figure 1: The PCA projections of the trajectories of high dimensional random walks are Lissajous
curves. Left tableau: Projections of a 10 000-dimensional random walk onto various PCA components.
Right tableau: Corresponding Lissajous curves from Eq. 16.

3 Generalizations

3.1 Random walk with momentum

It is a common practice to train neural networks using stochastic gradient descent with momentum.
It is therefore interesting to examine the case of a random walk with momentum. In this case  the
process is governed by the following set of updates:

vt = γvt−1 + ξt
xt = xt−1 + vt.

It can be seen that this modiﬁes Eq. 2 to instead read

SX = MR

(17)

(18)

(19)

where M is a lower triangular Toeplitz matrix with 1 on the diagonal and γk on the kth subdiagonal.
The analysis from Section 2 is unchanged  except that now instead of considering the matrix S−1S−T
we have the matrix S−1MMT S−T . Although M is not a banded Toeplitz matrix  its terms decay
exponentially to zero for terms very far from the main diagonal. It is therefore asymptotically
circulant as well  and the eigenvectors remain Fourier modes. To ﬁnd the eigenvalues consider the

product (ST M−T M−1S)−1  noting that M−1 is a matrix with 1s along the main diagonal and −γs
subdiagonal. With some tedious calculation it can be seen that the matrix ST M−T M−1S is given by

(SM−1M−T ST )ij =


2 + 2γ + γ 2 
−(1 + γ)2 
γ 
0 

i = j
i = j ± 1
i = j ± 2
otherwise

(20)

with the exception that Snn = 1  and Sn n−1 = Sn−1 n = −(1 + γ). As before  this matrix is
asymptotically circulant  so the eigenvalues of its inverse are

λk =

1

2(cid:20)1 + γ + γ 2 − (1 + γ)2 cos(cid:18) πk

n (cid:19) + γ cos(cid:18) 2πk

n (cid:19)(cid:21)−1

.

(21)

In the limit of n → ∞  the distribution of eigenvalues is identical to that of a random walk in
ﬂat space  however for ﬁnite n  it has the effect of shifting the distribution towards the lower PCA
components. We empirically test Eq. 21 in Fig. 9 in the supplementary material.

5

3.2 Discrete Ornstein-Uhlenbeck processes

A useful generalization of the above analysis of random walks in ﬂat space is to consider random
walks in a quadratic potential  also known as an AR(1) process or a discrete Ornstein-Uhlenbeck
process. For simplicity we will assume that the potential has its minimum at the origin. Now every
step consists of a stochastic component and a deterministic component which points toward the origin
and is proportional in magnitude to the distance from the origin. In this case the update equation can
be written

(22)
where α measures the strength of the potential. In the limit α → 0 the potential disappears and we
recover a random walk in ﬂat space. In the limit α → 1 the potential becomes inﬁnitely strong and
we recover independent samples from a multivariate Gaussian distribution. For 1 < α < 2 the steps
will oscillate across the origin. For α outside [0  2] the updates diverge exponentially.

xt = (1 − α)xt−1 + ξt 

3.2.1 Analysis of eigenvectors and eigenvalues

This analysis proceeds similarly to the analysis in Section 2 except that instead of S we now have
the matrix SOU which has 1s along the diagonal and −(1 − α) along the subdiagonal. SOU remains a
T
banded Toeplitz matrix and so the arguments from Sec. 2 that ˆX ˆX
is asymptotically circulant hold
and its eigenvectors are remain Fourier modes. The eigenvalues will differ  however  because we now
have that the components of ST

OUSOU are given by

(cid:16)ST
OUSOU(cid:17)ij

1 + (1 − α)2 
−(1 − α) 
1 
0 

i < n  i = j
i = j ± 1
i = j = n
otherwise.

=


(23)

(24)

From Eq. 7 we have that the kth eigenvalue of ST

OUSOU is

λOU k =(cid:20)1 + (1 − α)2 − 2(1 − α) cos(cid:18) 2πk

n (cid:19)(cid:21)−1

≃(cid:20) 4π2k2(1 − α)

n2

+ α2(cid:21)−1

.

We show in Fig. 2 a comparison between the eigenvalue distribution predicted from Eq. 24 and the
observed distribution from a 3000 step Ornstein-Uhlenbeck process in 30 000 dimensions for several
values of α. There is generally a tight correspondence between the two. The exception is in the limit
of α → 1  where there is a catch which we have hitherto neglected. While it is true that the mean
eigenvalue of any eigenvector approaches the same constant  there is nevertheless going to be some
distribution of eigenvalues for any ﬁnite walk. Because PCA sorts the eigenvalues  there will be a
characteristic deviation from a ﬂat distribution.

3.2.2 Critical distance and mixing time

While we might be tempted to take the limit n → ∞ as we did in the case of a random walk in
ﬂat space  doing so would obscure interesting dynamics early in the walk. (A random walk in ﬂat
space is self-similar so we lose no information by taking this limit. This is no longer the case in an
Ornstein-Uhlenbeck process because the parameter α sets a characteristic scale in the system.) In
fact there will be two distinct phases of a high dimensional Ornstein-Uhlenbeck process initialized
at the origin. First phase the process will behave as a random walk in ﬂat space and the distance
from the origin will increase proportionally to √n and the variance of the kth PCA component will
be proportional to k−2. But once the distance from the origin reaches a critical value  the gradient
toward the origin will become large enough to balance the tendency of the random walk to drift away
from the origin.4 At this point the trajectory will wander indeﬁnitely around a sphere centered at
the origin with radius given by this critical distance. Thus  while an Ornstein-Uhlenbeck process
is mean-reverting in low dimensions  in the limit of inﬁnite dimensions the Ornstein-Uhlenbeck
process is no longer mean-reverting — an inﬁnite dimensional Ornstein-Uhlenbeck process will
never return to its mean.5 This critical distance can be calculated by noting that each dimension is

4Assuming we start close to the origin.

If we start sufﬁciently far from the origin the trajectory will

exponentially decay to this critical value.

5Speciﬁcally  since the limiting distribution is a d-dimensional Gaussian  the probability that the process will
return to within ǫ of the origin is P (d/2  ǫ2/2)  where P is the regularized gamma function. For small ǫ this
decays exponentially with d.

6

e
c
n
a
i
r
a
v
 
A
C
P

103
102
101
100
10-1
10-2
10-3
10-4

100

α = 10−4
α = 10−3
α = 10−2
α = 10−1
α = 100

101
102
PCA component

103

i

n
g
i
r
o
 
e
h
t
 

m
o
r
f
 
e
c
n
a
t
s
i
D

102

101

100

100

α = 10−4
α = 10−3
α = 10−2
α = 10−1
α = 100

101

102

Step

103

Figure 2: Left panel: The variance of the PCA components for several choices of α. The empirical
distribution is shown in solid and the predicted distribution with a dotted line. The predicted
distribution generally matches the observed distribution closely  but there is a systematic deviation
for α near 1. This is due to the fact that when the mean distribution is ﬂat  there will nevertheless be
a distribution around this mean when these eigenvalues are sampled from real data. Because PCA
sorts these eigenvalues  this will always lead to a deviation from the ﬂat distribution. Right panel:
Distance from the origin for discrete Ornstein-Uhlenbeck processes with several choices of α (solid
lines) with the predicted asymptote from Eq. 25 (dotted lines).

independent of every other and it is well known that the asymptotic distribution of an AR(1) process

with Gaussian noise is Gaussian with a mean of zero and a standard deviation ofpV /(1 − (1 − α)2) 
where V is the variance of the stochastic component of the process. In high dimensions the asymptotic
distribution as n → ∞ is simply a multidimensional isotropic Gaussian. Because we are assuming
V = 1/d  the overwhelming majority of points sampled from this distribution will be in a narrow
annulus at a distance

rc =

(25)

1

pα(2 − α)

from the origin. Since the distance from the origin during the initial random walk phase grows as

√n  the process will start to deviate from a random walk after nc ∼ (α(2 − α))−1 steps. We show in

the right panel of Fig. 2 the distance from the origin over time for 3000 steps of Ornstein-Uhlenbeck
processes in 30 000 dimensions with several different choices of α. We compare to the prediction of
Eq. 25 and ﬁnd a good match.

3.2.3

Iterate averages converge slowly

We ﬁnally note that if the location of the minimum is unknown  then iterate (or Polyak) averaging
can be used to provide a better estimate. But the number of steps must be much greater than nc
before iterate averaging will improve the estimate. Only then will the location on the sphere be
approximately orthogonal to its original location on the sphere and the variance on the estimate of the
minimum will decrease as 1/√n. We compute the mean of converged Ornstein-Uhlenbeck processes
with various choices of α in Fig. 10 in the supplementary material.

3.2.4 Random walks in non-isotropic potential are dominated by low curvature directions

While our analysis has been focused on the special case of a quadratic potential with equal curvature
in all dimensions  a more realistic quadratic potential will have a distribution of curvatures and the
axes of the potential may not be aligned with the coordinate basis. Fortunately these complications
do not change the overall picture much. For a general quadratic potential described by a positive
semi-deﬁnite matrix A  we can decompose A into its eigenvalues and eigenvectors. We then apply
a coordinate transformation to align the parameter space with the eigenvectors of A. At this point
we have a distribution of curvatures  each one given by an eigenvalue of A. However  because we
are considering the limit of inﬁnite dimensions  we can assume that there will be a large number of
dimensions that fall in any bin [αi  αi + dα]. Each of these bins can be treated as an independent
high-dimensional Ornstein-Uhlenbeck process with curvature αi. After n steps  PCA will then be
dominated by dimensions for which αi is small enough that n ≪ nc i. Thus  even if relatively few

7

o
i
t
a
r
 
e
c
n
a
i
r
a
v
 
d
e
n
a
p
x
E

l

i

100
10-1
10-2
10-3
10-4
10-5
10-6
10-7
10-8

100

Linear model

Start of training
Middle of training
End of training
Random walk
101
102
PCA component

103

o
i
t
a
r
 
e
c
n
a
i
r
a
v
 
d
e
n
a
p
x
E

l

i

10-1010-910-810-710-610-510-410-310-210-1100

10-11
10-12
10-13
10-14

100

ResNet-50-v2

101

102
PCA component

103

Figure 3: Left panel: The distribution of PCA variances at various points in training for a linear
model trained on CIFAR-10. At the beginning of training the model’s trajectory is more directed
than a random walk  as exhibited by the steep distribution in the lower PCA components. By the
middle of training this distribution has ﬂattened (apart from the ﬁrst PCA component) and more
closely resembles that of an Ornstein-Uhlenbeck process. Right panel: The distribution of PCA
variances of the parameters of ResNet-50-v2 at various points in training. The distribution of PCA
variances generally matches that of a random walk with the exception of the ﬁrst PCA component 
which dominates the distribution  particularly at the end of training.

dimensions have small curvature they will come to dominate the PCA projected trajectory after
enough steps.

4 Comparison to linear models and neural networks

While random walks and Ornstein-Uhlenbeck processes are analytically tractable  there are several
important differences between these simple processes and optimization of even linear models. In
particular  the statistics of the noise will depend on the location in parameter space and so will change
over the course of training. Furthermore  there may be ﬁnite data or ﬁnite trajectory length effects.

To get a sense for the effect of these differences we now compare the distribution of the variances
in the PCA components between two models and a random walk. For our ﬁrst model we train a
linear model without biases on CIFAR-10 using a learning rate of 10−5 for 10 000 steps. For our
second model we train ResNet-50-v2 on Imagenet without batch normalization for 150 000 steps
using SGD with momentum and linear learning rate decay. We collect the value of all parameters at
every step for the ﬁrst 1500 steps  the middle 1500 steps  and the last 1500 steps of training  along
with collecting the parameters every 100 steps throughout the entirety of training. Further details of
both models and the training procedures can be found in the supplementary material. While PCA is
tractable on a linear model of CIFAR-10  ResNet-50-v2 has ∼25 million parameters and performing
PCA directly on the parameters is infeasible  so we instead perform a random Gaussian projection
into a subspace of 30 000 dimensions. We show in Fig. 3 the distribution of the PCA variances at the
beginning  middle  and end of training for both models and compare to the distribution of variances
from an inﬁnite dimensional random walk. We show tableaux of the PCA projected trajectories from
the middle of training for the linear model and ResNet-50-v2 in Fig. 4. Tableaux of the other training
trajectories in various PCA subspaces are shown in the supplementary material  along with results
from a small fully connected neural network trained on MNIST.

The distribution of eigenvalues of the linear model resembles an OU process  whereas the distribution
of eigenvalues of ResNet-50-v2 resembles a random walk with a large drift term. The trajectories
appear almost identical to those of random walks shown in Fig. 1  with the exception that there is
more variance along the ﬁrst PCA component than in the random walk case  particularly at the start
and end points. This manifests itself in a small outward turn of the edges of the parabola in the PCA2
vs. PCA1 projection. This suggests that ResNet-50-v2 generally moves in a consistent direction over
relatively long spans of training  similarly to an Ornstein-Uhlenbeck process initialized beyond rc.

8

1e−2

2

A
C
P

2.0
1.5
1.0
0.5
0.0
−0.5
−1.0
−1.5
−2.0

−0.06

1e−2

4

3

2

1

0

3

A
C
P

−1

−2
−0.06

0.00

0.06

PCA1

1e−2

4

3

2

1

0

−1

3

A
C
P

0.00

0.06

−2
−0.020 −0.005 0.010

PCA2

1e−2

PCA1

1e−2

3

2

1

0

−1

−2

4

A
C
P

3

2

1

0

−1

−2

4

A
C
P

1e3

1.0

2

A
C
P

0.5

0.0

−0.5

−1.0

−80000 −20000 40000

PCA1

3

A
C
P

1e2

8
6
4
2
0
−2
−4
−6
−8
−80000 −20000 40000

PCA1

1e2

6

4

2

0

−2

−4

4

A
C
P

3

A
C
P

1e2

8
6
4
2
0
−2
−4
−6
−8
−1000

1e2

6

4

2

0

−2

−4

4

A
C
P

500

PCA2

1e2

6

4

2

0

−2

−4

4

A
C
P

Li

n

e

a

r 

m

o

d

e
l

1e−2

3

2

1

0

−1

−2

4

A
C
P

R

e

s

N

e

t-

5

0
-
v

2

−3
−0.06

0.00

0.06

−3
−0.020 −0.005 0.010

−3
−0.02

0.01

0.04

−6
−80000 −20000 40000

−6
−1000

PCA2

1e−2

PCA1

1e−2

3

2

1

0

−1

−2

5

A
C
P

3

2

1

0

−1

−2

5

A
C
P

3

2

1

0

−1

−2

5

A
C
P

PCA3

1e−2

1e−2

3

2

1

0

−1

−2

5

A
C
P

−3
−0.06

0.00

0.06

−3
−0.020 −0.005 0.010

−3
−0.02

0.01

0.04

−3
−0.03

0.00

0.03

PCA1

PCA2

PCA3

PCA4

PCA1

5

A
C
P

1e2

5
4
3
2
1
0
−1
−2
−3
−4
−80000 −20000 40000

PCA1

5

A
C
P

1e2

5
4
3
2
1
0
−1
−2
−3
−4
−1000

500

PCA2

−6
−800 −200

400

PCA3

5

A
C
P

1e2

5
4
3
2
1
0
−1
−2
−3
−4
−800 −200

400

PCA3

5

A
C
P

1e2

5
4
3
2
1
0
−1
−2
−3
−4
−600

0

600

PCA4

500

PCA2

Figure 4: Left tableau: PCA projected trajectories from the middle of training a linear model on
CIFAR-10. Training has largely converged at this point  producing an approximately Gaussian
distribution in the higher PCA components. Right tableau: PCA projected trajectories from the
middle of training ResNet-50-v2 on Imagenet. These trajectories strongly resemble those of a random
walk. See Figs. 12 and 13 in the supplementary material for PCA projected trajectories at other
phases of training.

5 Random walks with decaying step sizes

We ﬁnally note that the PCA projected trajectories of the linear model and ResNet-50-v2 over
the entire course of training qualitatively resemble those of a high dimensional random walk with
exponentially decaying step sizes. To show this we train a linear regression model y = Wx  where W
is a ﬁxed  unknown vector of dimension 10 000. We sample x from a 10 000 dimensional isotropic
Gaussian and calculate the loss

L =

(y − y′)2 

(26)

1
2

where y′ is the correct output. We show in Fig. 15 that the step size decays exponentially. We ﬁt the
decay rate to this data and then perform a random walk in 10 000 dimensions but decay the variance
of the stochastic term ξi by this rate. We compare in Fig. 16 of the supplementary material the PCA
projected trajectories of the linear model trained on synthetic data to the decayed random walk. We
note that these trajectories resemble the PCA trajectories over the entire course of training observed
in Figs. 12 and 13 for the linear model trained on CIFAR-10 and ResNet-50-v2 trained on Imagenet.

6 Conclusions

We have derived the distribution of the variances of the PCA components of a random walk both with
and without momentum in the limit of inﬁnite dimensions  and proved that the PCA projections of the
trajectory are Lissajous curves. We have argued that the PCA projected trajectory of a random walk in
a general quadratic potential will be dominated by the dimensions with the smallest curvatures where
they will appear similar to a random walk in ﬂat space. Finally  we ﬁnd that the PCA projections of
the training trajectory of a layer in ResNet-50-v2 qualitatively resemble those of a high dimensional
random walk despite the many differences between the optimization of a large NN and a high
dimensional random walk.

Acknowledgments

The authors thank Matthew Hoffman  Martin Wattenberg  Jeffrey Pennington  Roy Frostig  and Niru
Maheswaranathan for helpful discussions and comments on drafts of the manuscript.

9

References

Ahn  S.  Korattikara  A.  and Welling  M. Bayesian posterior sampling via stochastic gradient ﬁsher

scoring. In International Conference on Machine Learning  2012.

Baity-Jesi  M.  Sagun  L.  Geiger  M.  Spigler  S.  Arous  G. B.  Cammarota  C.  LeCun  Y.  Wyart 
M.  and Biroli  G. Comparing dynamics: Deep neural networks versus glassy systems. arXiv
preprint arXiv:1803.06969  2018.

Böttcher  A.  Embree  M.  and Sokolov  V. The spectra of large toeplitz band matrices with a randomly

perturbed entry. Mathematics of computation  72(243):1329–1348  2003.

Choromanska  A.  Henaff  M.  Mathieu  M.  Arous  G. B.  and LeCun  Y. The loss surfaces of

multilayer networks. In Artiﬁcial Intelligence and Statistics  pp. 192–204  2015.

Dauphin  Y. N.  Pascanu  R.  Gulcehre  C.  Cho  K.  Ganguli  S.  and Bengio  Y. Identifying and
attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in
neural information processing systems  pp. 2933–2941  2014.

Dinh  L.  Pascanu  R.  Bengio  S.  and Bengio  Y. Sharp minima can generalize for deep nets. arXiv

preprint arXiv:1703.04933  2017.

Goodfellow  I. J.  Vinyals  O.  and Saxe  A. M. Qualitatively characterizing neural network optimiza-

tion problems. In International Conference on Learning Representations  2015.

Gray  R. M. et al. Toeplitz and circulant matrices: A review. Foundations and Trends R(cid:13) in Communi-

cations and Information Theory  2(3):155–239  2006.

He  K.  Zhang  X.  Ren  S.  and Sun  J. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on computer vision and pattern recognition  pp. 770–778  2016.

Hochreiter  S. and Schmidhuber  J. Flat minima. Neural Computation  9(1):1–42  1997.

Jozefowicz  R.  Vinyals  O.  Schuster  M.  Shazeer  N.  and Wu  Y. Exploring the limits of language

modeling. arXiv preprint arXiv:1602.02410  2016.

Keskar  N. S.  Mudigere  D.  Nocedal  J.  Smelyanskiy  M.  and Tang  P. T. P. On large-batch training
for deep learning: Generalization gap and sharp minima. In International Conference on Learning
Representations  2017.

Li  H.  Xu  Z.  Taylor  G.  and Goldstein  T. Visualizing the loss landscape of neural nets.

In

International Conference on Learning Representations  2018.

Lipton  Z. C. Stuck in a what? adventures in weight space. arXiv preprint arXiv:1602.07320  2016.

Lorch  E. Visualizing deep network training trajectories with pca.
Conference on Machine Learning JMLR volume  volume 48  2016.

In The 33rd International

Mandt  S.  Hoffman  M.  and Blei  D. A variational analysis of stochastic gradient algorithms. In

International Conference on Machine Learning  pp. 354–363  2016.

Moore  J.  Ahmed  H.  and Antia  R. High dimensional random walks can appear low dimensional:

Application to inﬂuenza h3n2 evolution. Journal of theoretical biology  447:56–64  2018.

Novak  R.  Bahri  Y.  Abolaﬁa  D. A.  Pennington  J.  and Sohl-Dickstein  J. Sensitivity and
generalization in neural networks: an empirical study. In International Conference on Learning
Representations  2018.

Rump  S. M. Eigenvalues  pseudospectrum and structured perturbations. Linear algebra and its

applications  413(2-3):567–593  2006.

Smith  S. L. and Le  Q. V. A bayesian perspective on generalization and stochastic gradient descent.

In International Conference on Learning Representations  2018.

Uhlenbeck  G. E. and Ornstein  L. S. On the theory of the brownian motion. Physical review  36(5):

823  1930.

Zhu  Z. and Wakin  M. B. On the asymptotic equivalence of circulant and toeplitz matrices. IEEE

Trans. Information Theory  63(5):2975–2992  2017.

10

,Joseph Antognini
Jascha Sohl-Dickstein