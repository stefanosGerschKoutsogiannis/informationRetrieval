2011,Convergent Bounds on the Euclidean Distance,Given a set V of n vectors in d-dimensional space  we provide an efficient method  for computing quality upper and lower bounds of the Euclidean distances between  a pair of the vectors in V . For this purpose  we define a distance measure  called  the MS-distance  by using the mean and the standard deviation values of vectors in  V . Once we compute the mean and the standard deviation values of vectors in V in  O(dn) time  the MS-distance between them provides upper and lower bounds of  Euclidean distance between a pair of vectors in V in constant time. Furthermore   these bounds can be refined further such that they converge monotonically to the  exact Euclidean distance within d refinement steps. We also provide an analysis on  a random sequence of refinement steps which can justify why MS-distance should  be refined to provide very tight bounds in a few steps of a typical sequence. The  MS-distance can be used to various problems where the Euclidean distance is used  to measure the proximity or similarity between objects. We provide experimental  results on the nearest and the farthest neighbor searches.,Convergent Bounds on the Euclidean Distance

Yoonho Hwang

Hee-Kap Ahn

Department of Computer Science and Engineering

Pohang University of Science and Technology
POSTECH  Pohang  Gyungbuk  Korea(ROK)
{cypher heekap}@postech.ac.kr

Abstract

Given a set V of n vectors in d-dimensional space  we provide an efﬁcient method
for computing quality upper and lower bounds of the Euclidean distances between
a pair of vectors in V . For this purpose  we deﬁne a distance measure  called the
MS-distance  by using the mean and the standard deviation values of vectors in
V . Once we compute the mean and the standard deviation values of vectors in V
in O(dn) time  the MS-distance provides upper and lower bounds of Euclidean
distance between any pair of vectors in V in constant time. Furthermore  these
bounds can be reﬁned further in such a way to converge monotonically to the
exact Euclidean distance within d reﬁnement steps. An analysis on a random se-
quence of reﬁnement steps shows that the MS-distance provides very tight bounds
in only a few reﬁnement steps. The MS-distance can be used to various applica-
tions where the Euclidean distance is used to measure the proximity or similarity
between objects. We provide experimental results on the nearest and the farthest
neighbor searches.

1

Introduction

The Euclidean distance between two vectors x and y in d-dimensional space is a typical distance
measure that reﬂects their proximity in the space. Measuring the Euclidean distance is a fundamental
operation in computer science  including the areas of database  computational geometry  computer
vision and computer graphics. In machine learning  the Euclidean distance  denoted by dist(x  y) 
or it’s variations(for example  e||x−y||) are widely used to measure data similarity for clustering [1] 
classiﬁcation [2] and so on.
A typical problem is as follows. Given two sets X and Y of vectors in d-dimensional space  our
goal is to ﬁnd a pair (x  y)  for x ∈ X and y ∈ Y   such that dist(x  y) is the optimum (minimum or
maximum) over all such pairs. For the nearest or farthest neighbor searches  X is the set consisting
of a single query point while Y consists of all candidate data points. If the dimension is low  a
brute-force computation would be fast enough. However  data sets in areas such as optimization 
computer vision  machine learning or statistics often live in spaces of dimensionality in the order
of thousands or millions. In d dimensional space  a single distance computation already takes O(d)
time  thus the cost for ﬁnding the nearest or farthest neighbor becomes O(dnm) time  where n and
m are the cardinalities of X and Y   respectively.
Several techniques have been proposed to reduce computation cost for computing distance. Probably
PCA (principal component analysis) is the most frequently used technique for this purpose [3]  in
which we use an orthogonal transformation based on PCA to convert a set of given data so that
the dimensionality of the transformed data is reduced. Then it computes distances between pairs of
transformed data efﬁciently. However  this transformation does not preserve the pairwise distances
of data in general  therefore there is no guarantee on the computation results.

1

If we restrict ourselves to the nearest neighbor search  some methods using space partitioning trees
such as KD-tree [4]  R-tree [5]  or their variations have been widely used. However  they become
impractical for high dimensions because of their poor performance in constructing data structures
for queries. Recently  cover tree [6] has been used for high dimensional nearest neighbor search  but
its construction time increases drastically as the dimension increases [7].
Another approach that has attracted some attention is to compute a good bound of the exact Eu-
clidean distance efﬁciently such that it can be used to ﬁlter off some unnecessary computation  for
example  the distance computation between two vectors that are far apart from each other in near-
est neighbor search. One of such methods is to compute a distance bound using the inner product
approximation [8]. This method  however  requires the distribution of the input data to be known in
advance  and works only on data in some predetermined distribution. Another method is to com-
pute a distance bound using bitwise operations [9]. But this method works well only on uniformly
distributed vectors  and requires O(2d) bitwise operations in d dimension. A method using an index
structure [10] provides an effective ﬁltering method based on the triangle inequality. But this works
well only when data are well clustered.
In this paper  we deﬁne a distance measure  called the MS-distance  by using the mean and the
standard deviation values of vectors in V . Once we compute the mean and the standard deviation
values of vectors in V in O(dn) time  the MS-distance provides tight upper and lower bounds of
Euclidean distance between any pair of vectors in V in constant time. Furthermore  these bounds can
be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d
reﬁnement steps. Each reﬁnement step takes constant time.
We provide an analysis on a random sequence of k reﬁnement steps for 0 ≤ k ≤ d  which shows
a good expectation on the lower and upper bounds. This can justify that the MS-distance provides
very tight bounds in a few reﬁnement steps of a typical sequence. We also show that the MS-distance
can be used in fast ﬁltering. Note that we do not use any assumption on data distribution.
The MS-distance can be used to various applications where the Euclidean distance is a measure
for proximity or similarity between objects. Among them  we provide experimental results on the
nearest and the farthest neighbor searches.

2 An Upper and A Lower Bounds of the Euclidean Distance

(cid:80)d
For a d-dimensional vector x = [x1  x2  . . .   xd]  we denote its mean by µx = 1
i=1 xi and its
d
i=1(xi − µx)2. For a pair of vectors x and y  we can reformulate the squared
variance by σ2
Euclidean distance between x and y as follows. Let a = [a1  a2  . . .   ad] and b = [b1  b2  . . .   bd]
such that ai = xi − µx and bi = yi − µy.

x = 1
d

(cid:80)d

dist(x  y)2 =

i=1

i=1

i=1

i=1

=

=

=

(xi − yi)2

((µx + ai) − (µy + bi))2

d(cid:88)
d(cid:88)
d(cid:88)
d(cid:88)
= d(cid:0)(µx − µy)2 + (σx + σy)2(cid:1) − 2dσxσy − 2
= d(cid:0)(µx − µy)2 + (σx − σy)2(cid:1) + 2dσxσy − 2

x − 2µxµy + µ2

x + 2aiµx + a2

i + µ2

y + 2biµy + b2

i − 2aibi)

y + a2

i + b2

(µ2

(µ2

2

i − 2(µxµy + aiµy + biµx + aibi)) (1)

d(cid:88)
d(cid:88)

i=1

i=1

aibi

aibi.

(2)

(3)

(4)

(cid:80)d

By the deﬁnitions of ai and bi  we have(cid:80)d

i=1 ai =(cid:80)d

i=1 bi = 0  and 1

x. By the ﬁrst
properties  equation (1) is simpliﬁed to (2)  and by the second property  equations (2) becomes (3)
and (4).
Note that equations (3) and (4) are composed of the mean and variance values (their products and
squared values  multiplied by d) of x and y  except the last summations. Thus  once we preprocess
V of n vectors such that both µx and σx for all x ∈ V are computed in O(dn) time and stored in a
table of size O(n)  this sum can be computed in constant time for any pair of vectors  regardless of
the dimension.

i = σ2

i=1 a2

d

i aibi  is the inner product (cid:104)a  b(cid:105)  and therefore by applying the Cauchy-

The last summation (cid:80)d

Schwarz inequality we get

|(cid:104)a  b(cid:105)| = | d(cid:88)

aibi| ≤

(cid:118)(cid:117)(cid:117)(cid:116)(
d(cid:88)

d(cid:88)

i=1

i=1

i=1

a2
i )(

b2
i ) = dσxσy.

(5)

This gives us the following upper and lower bounds of the squared Euclidean distance from equa-
tions (3) and (4).

Lemma 1 For two d-dimensional vectors x  y  the followings hold.

dist(x  y)2 ≥ d(cid:0)(µx − µy)2 + (σx − σy)2(cid:1)
dist(x  y)2 ≤ d(cid:0)(µx − µy)2 + (σx + σy)2(cid:1)

(6)
(7)

3 The MS-distance

The lower and upper bounds in inequalities (6) and (7) can be computed in constant time once we
compute the mean and standard variance values of each vector in V in the preprocessing. However 
in some applications these bounds may not be tight enough. In this section  we introduce the MS-
distance which not only provides lower and upper bounds of the Euclidean distance in constant time 
but also could be reﬁned further in such a way to converge to the exact Euclidean distance within d
steps.
To do this  we reformulate the last term of equations (3) and (4)  that is  the inner product (cid:104)a  b(cid:105). If
the norms ||a|| =
i=1 aibi = 0  thus the upper
and lower bounds become the same. This implies that we can compute the exact Euclidean distance
in constant time. So from now on  we assume that both ||a|| and ||b|| are non-zero. We reformulate
the inner product (cid:104)a  b(cid:105).

i are zero  then(cid:80)d

(cid:113)(cid:80)d

(cid:113)(cid:80)d

i or ||b|| =

i=1 a2

i=1 b2

= −dσxσy +

σxσy

2

Equation (8) is because of(cid:80)d

switching the roles of the term −dσxσy and the term dσxσy in the above equations.

i=1 b2

i = dσ2

y. We can also get equation (10) by

i=1 a2

i = dσ2

d(cid:88)

aibi = dσxσy − dσxσy +

i=1

= dσxσy − σxσy
2

= dσxσy − σxσy
2

= dσxσy − σxσy
2

(cid:33)
d(cid:88)

i=1

i=1

i=1

aibi

d(cid:88)
(cid:32)
2d − d(cid:88)
(cid:32) d(cid:88)
(cid:19)2
(cid:18) ai
d(cid:88)
d(cid:88)
x and(cid:80)d

− ai
σx

bi
σy

bi
σy

σx

i=1

i=1

i=1

+

(

(

2aibi
σxσy

+

)2

ai
σx

)2

3

(cid:18) bi

σy

(cid:19)2 − d(cid:88)

i=1

2aibi
σxσy

(cid:33)

(8)

(9)

(10)

Deﬁnition. Now we deﬁne the MS-distance between x and y in its lower bound form  denoted by
MSL(x  y  k)  by replacing the last term of equation (3) with equation (9)  and in its upper bound
form  denoted by MSU(x  y  k) by replacing the last term of equation (4) with equation (10). The
MS-distance makes use of the nonincreasing intermediate values for its upper bound and the nonde-
creasing intermediate values for its lower bound. We let a0 = b0 = 0.

(cid:19)2
(cid:19)2

− ai
σx

+

ai
σx

(11)

(12)

MSL(x  y  k) = d(cid:0)(µx − µy)2 + (σx − σy)2(cid:1) + σxσy
MSU(x  y  k) = d(cid:0)(µx − µy)2 + (σx + σy)2(cid:1) − σxσy

k(cid:88)
k(cid:88)

i=0

(cid:18) bi
(cid:18) bi

σy

σy

i=0

Properties. Note that equation (11) is nondecreasing and equation (12) is nonincreasing while i
)2 are
increases from 0 to d  because d  σx  and σy are all nonnegative  and ( bi
σy
also nonnegative for all i. This is very useful because  in equation (11)  the ﬁrst term  MSL(x  y  0) 
is already a lower bound of dist(x  y)2 by inequality (6)   and the lower bound can be reﬁned further
nondecreasingly over the summation in the second term. If we stop the summation at i = k  for
k < d  the intermediate result is also a reﬁned lower bounds of dist(x  y)2. Similarly  in equation
(12)  the ﬁrst term  MSU(x  y  0)  is already an upper bound of dist(x  y)2 by inequality (7)   and
the upper bound can be reﬁned further nonincreasingly over the summation in the second term. This
means we can stop the summation as soon as we ﬁnd a bound good enough for the application
under consideration. If we need the exact Euclidean distance  we can get it by continuing to the full
summation. We summarize the above properties in the following.

)2 and ( bi
σy

− ai

+ ai
σx

σx

Lemma 2 (Monotone Convergence) Let MSL(x  y  k) and MSU(x  y  k) be the lower and upper
bounds of MS-distance as deﬁned above  respectively. Then the following properties hold.

• MSL(x  y  0) ≤ MSL(x  y  1) ≤ ··· ≤ MSL(x  y  d − 1) ≤ MSL(x  y  d) = dist(x  y)2.
• MSU(x  y  0) ≥ MSU(x  y  1) ≥ ··· ≥ MSU(x  y  d − 1) ≥ MSU(x  y  d) = dist(x  y)2.
• MSL(x  y  k) = MSL(x  y  k + 1) if and only if bk+1/σy = ak+1/σx.
• MSU(x  y  k) = MSU(x  y  k + 1) if and only if bk+1/σy = −ak+1/σx.

Lemma 3 For 0 ≤ k < d  we can update MSL(x  y  k) to MSL(x  y  k + 1)  and MSU(x  y  k) to
MSU(x  y  k + 1) in constant time.

Fast Filtering. We must emphasize that MSL(x  y  0) and MSU(x  y  0) can be used for fast ﬁlter-
ing. Let φ denote a threshold for ﬁltering deﬁned in some proximity search problem under consider-
ation. If φ < MSL(x  y  0) in case of nearest search or φ > MSL(x  y  0) in case of farthest search 
we do not need to consider this pair (x  y) as a candidate  thus we can save time from computing
their exact Euclidean distance.
Precisely speaking  we map each d-dimensional vector x = [x1  x2  . . .   xd] into a pair of points 
ˆx+ and ˆx−  in the 2-dimensional plane such that ˆx+ = [µx  σx] and ˆx− = [µx −σx]. Then

dist(ˆx+  ˆy+)2 = MSL(x  y  0)/d
dist(ˆx+  ˆy−)2 = MSU(x  y  0)/d.

(13)
(14)

To see why it is useful in fast ﬁltering  consider the case of ﬁnding the nearest vector. For d-
dimensional vectors in V of size n  we have n pairs of points in the plane as in Figure 1. Since σx
is nonnegative  exactly n points lie on or below µ-axis. Let q be a query vector  and let ˆq+ denote
the point mapped in the plane as deﬁned above. Among these n points lying on or below µ-axis  let
ˆx−i be the point that is nearest to ˆq+. Note that the closest point from the query can be computed
efﬁciently in 2-dimensional space  for example  after constructing some space partitioning structures
such as kd-trees or R-trees  each query can be answered in poly-logarithmic search time.

4

Then we can ignore all d-dimensional vectors x whose mapped point ˆx+ lies outside the circle
centered at ˆq+ and of radius dist(ˆq+  ˆx−i ) in the plane  because they are strictly farther than xi
from q.

Figure 1: Fast ﬁltering using MSL(x  y  0) and MSU(x  y  0). All d-dimensional vectors x whose
mapped point ˆx+ lies outside the circle are strictly farther than xi from q.

4 Estimating the Expected Difference Between Two Bounds

We now turn to estimating the expected difference between MSL(x  y  k) and MSU(x  y  k). Observe
that MSL(x  y  k) is almost the same as MSL(x  y  k − 1) if bk/σy ≈ ak/σx. Hence  in the worst
case  MSL(x  y  0) = MSL(x  y  d − 1) < MSL(x  y  d) = dist(x  y)2 when bk/σy = ak/σx for
all k = 0  1  . . .   d − 1  except k = d. Therefore  if we need a lower bound strictly better than
MSL(x  y  0)  then we need to go through all d reﬁnement steps  which takes O(d) time. It is not
difﬁcult to see that this also applies to the case of MSU(x  y  k).
However  this is unlikely to happen. Consider a random order for the last term in equation
MSL(x  y  k) and for the last term in equation MSU(x  y  k). We show below that their expected val-
ues increase and decrease linearly  respectively  as k increases from 0 to d. Formally  let (aγ(i)  bγ(i))
denote the ith pair in the random order. We measure the expected quality of the bounds by the dif-
ference between the bounds  that is  MSU(x  y  k) − MSL(x  y  k) as follows.

(cid:32)(cid:18) aγ(i)
k(cid:88)
(cid:32)(cid:18) ai
d(cid:88)

σx

i=0

(cid:19)2
(cid:19)2

σx

i=0

+

+

(cid:19)2(cid:33)
(cid:18) bγ(i)
(cid:19)2(cid:33)
(cid:18) bi

σy

σy

(15)

(16)

(17)
(18)

MSU(x  y  k) − MSL(x  y  k) = 4dσxσy − 2σxσy

= 4dσxσy − 2σxσy
k
d
= 4dσxσy − 4kσxσy
= 4σxσy(d − k)

Let us explain how we get Equation (16) from (15). Let N denote the set of all pairs  and let N k
denote the set of ﬁrst k pairs in the random order. Since each pair in N is treated equally  N k is a
i=1(aγ(i)/σx)2 is equivalent to take the total sum of
i=1(bγ(i)/σy)2 by a

random subset of N of size k. Therefore (cid:80)k
(ai/σx)2 with i from 1 to d and divide it by d/k. We can also show this for(cid:80)k
Equations (17) and (18) are because(cid:80)d
x and(cid:80)d
i=1(ai/σx)2 =(cid:80)d
By replacing each squared sum with d  that is   by applying(cid:80)d

y by deﬁnitions of ai and bi.
i=1(bi/σy)2 = d 

similar augment.

i = dσ2

i=1 b2

i=1 a2

i = dσ2

we have Equation (18).
Lemma 4 The expected value of MSU(x  y  k) − MSL(x  y  k) is 4σxσy(d − k).

5

µσY+Y−X+2X−2X+1X−1Because dist(x  y)2 always lies in between the two bounds  the following also holds.
Corollary 1 Both expected values of MSU(x  y  k) − dist(x  y)2 and dist(x  y)2 − MSL(x  y  k)
are at most 4σxσy(d − k).
This shows a good theoretical expectation on the lower and upper bounds. This can justify that the
MS-distance provides very tight bounds in a few reﬁnement steps of a typical sequence.

5 Applications : Proximity Searches

The MS-distance can be used to application problems where the Euclidean distance is a measure
for proximity or similarity of objects. As a case study  we implemented the nearest neighbor search
(NNS) and the farthest neighbor search (FNS) using the MS-distance.
Given a set X of d-dimensional vectors xi  for i = 1  . . .   n  and a d-dimensional query vector
q  we use the following simple randomized algorithm for NNS. Initially  we set φ to the threshold
given from the application under consideration or computed from the fast ﬁltering in 2-dimension in
Section 3.

1. Consider the vectors in X one at a time according to this sequence. At the ith stage  we do

the followings.

if MSL(q  xi  0) < φ :
for j = 1  2  ...  d :

if MSL(q  xi  j) > φ :

break;

if j = d:

φ = MSL(q  xi  d);
NN = i;

2. return NN as the nearest neighbor of q with the squared Euclidean distance φ.

Note that the ﬁrst line of the pseudocodes ﬁlters out the vectors whose distance to q is larger than φ
as in the fast ﬁltering in Section 3. In the for loop  we compute MSL(q  xi  j) from MSL(q  xi  j−1)
in constant time. From the last two lines of the pseudocodes  we update φ to the exact Euclidean
distance between q and xi and store the index as the current nearest neighbor (NN). The algorithm
for the farthest neighbor search is similar to this one  except that it uses MSU(xi  y  j) and maintains
the maximum distance.
For empirical comparison  we implemented a linear search algorithm that simply computes distances
from q to every xi and chooses the one with the minimum distance. We also used the implementation
of the cover tree [6]. A cover tree is a data structure that supports fast nearest neighbor queries given
a ﬁxed intrinsic dimensionality [7].
We tested these implementations on data sets from UCI machine learning archive [11]. We selected
data sets D from various dimensions (from 10 to 100  000)  and randomly selected 30 queries points
Q ⊂ D  and queried them on D \ Q. We labelled the data set on d-dimension as “Dd”. The
data sets D500  D5000  D10000  D20000  D100000 were used in NIPS 2003 challenge on feature
selection [12]. The test machine has one CPU  Intel Q6600 with 2.4GHz  3GB memory  and 32bit
Ubuntu 10 operating system running on the machine.
Figure 2 shows the percentage of data ﬁltered off. For the data sets on relaxed dimensions  the
MS-distance ﬁltered off over 95% of data without lose of accuracy. For high dimensional data 
MS-distance failed to ﬁlter off many data. Probably this is because the distances from queries to
their nearest vectors tend to converge to the distances to their farthest vectors as described in [13].
This makes it hard to decrease (or increase in FNS) the threshold φ for the MS-distance enough to
ﬁlter off many data. However  on such high dimensions  both the linear search and the cover tree
algorithm also show poor performance.
Figure 3 shows the preprocessing time of the MS-distance and the cover tree for NNS. The time axis
is log-scaled second. This shows that the preprocessing time of the MS-distance is up to 1000 times

6

Figure 2: Data ﬁltered off in percentage.

Figure 3: Preprocessing time for nearest neigh-
bor search in log-scaled second.

faster than the one in the cover tree. This is because for the MS-distance it requires only O(dn) time
to compute the mean and the standard deviation values.

Figure 4: Relative running time for the nearest
neighbor search queries  normalized by linear
search time.

Figure 5: Relative running time for the farthest
neighbor search queries  normalized by linear
search time.

Figure 4 shows the time spent for NNS queries. The graph shows the query time that is normalized
by the linear search time. It is clear that the ﬁltering algorithm based on the MS-distance beats
the linear search algorithm  even on high dimensional data in the results. The cover tree  which is
designed exclusively for NNS  shows slightly better query performance than ours. However  the
MS-distance is more general and ﬂexible: it supports addition of a new vector to the data set (our
data structure) in O(d) time for computing the mean and the standard deviation values of the vector.
Deletion of a vector from the data set can be done in constant time. Furthermore  the data structure
for NNS can also be used for FNS.
Figure 5 shows the time spent for FNS queries. This is outstanding compared to the linear search
algorithm. We hardly know any other previous work achieving better performance than this.

6 Conclusion

We introduce a fast distance bounding technique  called the MS-distance  by using the mean and the
standard deviation values. The MS-distance between two vectors provides upper and lower bounds
of Euclidean distance between them in constant time  and these bounds converge monotonically to
the exact Euclidean distance over iteration. The MS-distance can be used to application problems
where the Euclidean distance is a measure for proximity or similarity of objects. The experimental
results show that our method is efﬁcient enough even to replace the best known algorithms for
proximity searches.

7

020406080100D10D11D16D19D22D27D37D50D55D57D61D64D86D90D167D255D500D617D5000D10000D20000D10^5Percent(%)NNSFNS0.0010.010.1110100100010000D10D11D16D19D22D27D37D50D55D57D61D64D86D90D167D255D500D617D5000D10000D20000D10^5Time(Second)MS-distCover00.20.40.60.811.2D10D11D16D19D22D27D37D50D55D57D61D64D86D90D167D255D500D617D5000D10000D20000D10^5Relative TimeMS-distCoverLinear00.20.40.60.811.2D10D11D16D19D22D27D37D50D55D57D61D64D86D90D167D255D500D617D5000D10000D20000D10^5Relative TimeMS-distLinearTable 1: Data sets

Data Label Name

# of vectors Data Label Name

# of vectors

D10
D11
D16
D19
D22
D27
D37
D50
D55
D57
D61

Page Blocks
Wine Quality
Letter Recognition
Image Segmentation
Parkinsons Tel
Steel Plates Faults
Statlog Satellite
MiniBooNE
Covertype
Spambase
IPUMS Census

5473 D64
6497 D86
20000 D90
2310 D167
5875 D255
1941 D500
6435 D617
130064 D5000
581012 D10000
4601 D20000
233584 D100000

Optical Recognition
Insurance Company
YearPredictionMSD
Musk2
Semeion
Madelon
ISOLET
Gisette
Arcene
Dexter
Dorothea

5620
5822
515345
6597
1593
4400
7795
13500
900
2600
1950

Acknowledgments

This work was supported by the National Research Foundation of Korea Grant funded by the Korean
Government (MEST) (NRF-2010-0009857).

References
[1] J. B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman  editors  Proceedings of the ﬁfth Berkeley Symposium on Mathematical Statistics and
Probability  volume 1  pages 281–297. University of California Press  1967.

[2] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer  New York  NY  USA  1995.
[3] K. Pearson. On lines and planes of closest ﬁt to systems of points in space. Philosophical Magazine 

2:559–572  1901.

[4] J. L. Bentley. Multidimensional binary search trees used for associative searching. Communications of

ACM  18:509–517  September 1975.

[5] A. Guttman. R-trees: A dynamic index structure for spatial searching.

In Beatrice Yormark  editor 
Proceedings of the ACM SIGMOD International Conference on Management of Data  SIGMOD ’84 
pages 47–57. ACM  1984.

[6] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbor. In Proceedings of the 23rd
international conference on Machine learning  ICML ’06  pages 97–104  New York  NY  USA  2006.
ACM.

[7] D. R. Karger and M. Ruhl. Finding nearest neighbors in growth-restricted metrics. In Proceedings of the
34th annual ACM symposium on Theory of computing  STOC ’02  pages 741–750  New York  NY  USA 
2002. ACM.
¨O. E˘gecio˘glu and H. Ferhatosmano˘glu. Dimensionality reduction and similarity computation by inner
product approximations. In Proceedings of the ninth international conference on Information and knowl-
edge management  CIKM ’00  pages 219–226  New York  NY  USA  2000. ACM.

[8]

[9] R. Weber  H. J. Schek  and S. Blott. A quantitative analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proceedings of the 24rd International Conference on Very Large
Data Bases  VLDB ’98  pages 194–205  San Francisco  CA  USA  1998. Morgan Kaufmann Publishers
Inc.

[10] H. V. Jagadish  B. C. Ooi  K.L. Tan  C. Yu  and R. Zhang. idistance: An adaptive b+-tree based indexing

method for nearest neighbor search. ACM Transactions on Database Systems  30:364–397  June 2005.

[11] UCI machine learning archive. http://archive.ics.uci.edu/ml/.

8

[12] NIPS 2003 challenge on feature selection. http://clopinet.com/isabelle/projects/nips2003/.
[13] K. S. Beyer  J. Goldstein  R. Ramakrishnan  and U. Shaft. When is ”nearest neighbor” meaningful? In
Proceedings of the 7th International Conference on Database Theory  ICDT ’99  pages 217–235  London 
UK  1999. Springer.

9

,Wei-Sheng Lai
Jia-Bin Huang
Ming-Hsuan Yang