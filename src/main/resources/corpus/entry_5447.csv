2018,The Effect of Network Width on the Performance of  Large-batch Training,Distributed implementations of mini-batch stochastic gradient descent (SGD)  suffer from communication overheads  attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance.

In this work  we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down  unlike their deeper variants.,The Effect of Network Width on the Performance of

Large-batch Training

1Department of Computer Sciences 

2Department of Electrical and Computer Engineering

Lingjiao Chen1   Hongyi Wang1   Jinman Zhao1 
Paraschos Koutris  1 Dimitris Papailiopoulos2

University of Wisconsin-Madison

Abstract

Distributed implementations of mini-batch stochastic gradient descent (SGD) suf-
fer from communication overheads  attributed to the high frequency of gradient
updates inherent in small-batch training. Training with large batches can reduce
these overheads; however it besets the convergence of the algorithm and the gen-
eralization performance. In this work  we take a ﬁrst step towards analyzing how
the structure (width and depth) of a neural network affects the performance of
large-batch training. We present new theoretical results which suggest that–for a
ﬁxed number of parameters–wider networks are more amenable to fast large-batch
training compared to deeper ones. We provide extensive experiments on residual
and fully-connected neural networks which suggest that wider networks can be
trained using larger batches without incurring a convergence slow-down  unlike
their deeper variants.

1

Introduction

Distributed implementations of stochastic optimization algorithms have become the standard in large-
scale model training [1  2  3  4  5  6  7]. Most machine learning frameworks  including Tensorﬂow
[1]  MxNet [4]  and Caffe2 [7]  implement variants of mini-batch SGD as their default distributed
training algorithm. During a distributed iteration of mini-batch SGD a parameter server (PS) stores
the global model  and P compute nodes evaluate a total of B gradients; B is commonly referred
to as the batch size. Once the PS receives the sum of these B gradients from every compute node 
it applies them to the global model and sends the model back to the compute nodes  where a new
distributed iteration begins.
The main premise of a distributed implementation is speedup gains  i.e.  how much faster training
takes on P vs 1 compute node. In practice  these gains usually saturate beyond a few 10s of compute
nodes [6  8  9]. This is because communication becomes the bottleneck  i.e.  for a ﬁxed batch of B
examples  as the number of compute nodes increases  these nodes will eventually spend more time
communicating gradients to the PS rather than computing them. To mitigate this bottleneck  a plethora
of recent work has studied low-precision training and gradient sparsiﬁcation  e.g.  [10  11  12].
An alternative approach to alleviate these overheads is to increase the batch size B  since B directly
controls the communication-computation ratio. Recent work develops sophisticated methods that
enable large-batch training on state-of-the-art models and data sets [13  14  15]. At the same time 
several studies suggest that large-batch training can affect the generalizability of the models [16]  can
slow down convergence [17  18  19]  and is more sensitive to hyperparameter mis-tuning [20].
Several theoretical results [21  18  22  19  17] suggest that  when the batch size B becomes larger
than a problem-dependent threshold B∗  the total number of iterations to converge signiﬁcantly

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

increases  rendering the use of larger B a less viable option. Some of these studies  implicitly or
explicitly  indicate that the threshold B∗ is controlled by the similarity of the gradients in the batch.
In particular  [19] shows that the measure of gradient
diversity directly controls the relationship of B and the
convergence speed of mini-batch SGD. Gradient diversity
measures the similarity of concurrently processed gradi-
ents  and [19] shows theoretically and experimentally that
the higher the diversity  the more amenable a problem
is to fast large-batch training  and by extent to speedup
gains in a distributed setting.
A large volume of work has focused on how the structure
of neural networks can affect the complexity or capacity
[23  24  25] of the model  its representation efﬁciency
[26]  and its prediction accuracy [27  28]. However  there
is little work towards understanding how the structure
of a neural network affects its amenability to distributed
speedup gains.
In this work  through analyzing the gradient diversity of
different network architectures  we take a step towards
addressing the following question: How does the struc-
ture of a neural network affect its amenability to fast
large-batch training?
Our contribution We establish a theoretical connection
between the structure (depth and width) of neural net-
works and their gradient diversity  which is an indicator of how large batch size can become  without
slowing down the speed of convergence [19]. In particular  we prove how gradient diversity varies as
a function of width and depth for two types of networks: 2-layer fully-connected linear and non-linear
neural networks  and multi-layer fully-connected linear neural networks. Our theoretical analysis
indicates that  perhaps surprisingly  gradient diversity increases monotonically as width increases and
depth decreases. On a high-level  wider networks provide more space for the gradients to become
diverse. This result suggests that wider and shallower networks are more amenable to fast large-batch
training compared to deeper ones. Figure 1 provides an illustrative example of this phenomenon.
We provide extensive experimental results that support our theoretical ﬁndings. We present ex-
periments on fully-connected and residual networks on CIFAR10  MNIST  EMNIST  Gisette  and
synthetic datasets. In our experimental setting  we ﬁx the number of network parameters  vary the
depth and width  and measure (after tuning the step size) how many passes over the data it takes to
reach an accuracy of  with batch size B. We observe that for all networks there exists a threshold
B∗  and setting the batch size larger than the threshold leads to slower convergence. The observed
threshold B∗ becomes smaller when the network becomes deeper  validating our theoretical result
that deeper networks are less amenable to fast large-batch training.
To summarize the main message of our work  communication bottlenecks in distributed mini-batch
SGD can be partially overcome not only by designing communication-efﬁcient algorithms  but also
by optimizing the architecture of the neural network at hand in order to enable large-batch training.

Figure 1: Impact of neural network structure
on amenability to large-batch training. This is
for fully-connected models with ReLUs on M-
NIST. For each fully-connected network  we
vary the batch size and measure the number
of epochs to converge to 96% accuracy on M-
NIST. Wider and shallower networks require
less epochs to converge than narrower and
deeper ones  which suggests that the former
are more suitable to scale out to more compute
nodes.

2 Related Work

Mini-batch The choice of an optimal batch size has been studied for non-strongly convex models [21] 
least square regression [22]  and SVMs [29]. Other works propose methods that automatically choose
the batch size on the ﬂy [30  31]. Mini-batch algorithms can be combined with accelerated gradient
descent algorithms [32]  or using dual coordinate descent [33  34]. Mini-batch proximal algorithms
are presented in [35]. While previous work mainly focuses on (strongly) convex models  or speciﬁc
models (e.g.  least square regression  SVMs)  our work studies how neural network structure can
affect the optimal batch size.
Gradient Diversity Previous work indicates that mini-batch can achieve better convergence rates
by increasing the diversity of gradient batches  e.g.  using stratiﬁed sampling [36]  Determinantal

2

102103Batch Size02040Number of Epochsto ConvergeWidth=21  Depth=1Width=19  Depth=5Width=17  Depth=10Point Processes [37]  or active sampling [38]. The notion of similarity between gradients and how it
affects convergence performance has been studied in several papers [17  18  19]. A formal deﬁnition
and analysis of gradient diversity is given in [19]  which establishes the connection between gradient
diversity and maximum batch size for convex and nonconvex models. To the best of our knowledge 
none of the existing works relates gradient diversity (and thus the optimal batch size) with the
structure of a neural network.
Width vs Depth in Artiﬁcial Neural Networks There has been an increasing interest and debate
on the qualities of deep versus wide neural networks. [23] suggests that deep networks have larger
complexity than wide networks and thus may be able to obtain better models. [26] proves that deep
networks can approximate sum products more efﬁciently than wide networks. Meanwhile  [39] shows
that a class of wide ResNets can achieve at least as high accuracy as deep ResNets. [40] presents
two classes of networks  one shallow and one deep  that achieve similar prediction error for saliency
prediction. In fact  [41] shows that well-designed shallow neural networks can outperform many
deep neural networks. More recently  [27] shows that using a dense structure  wider yet shallower
networks can signiﬁcantly improve the accuracy compared to deeper networks. In addition  [42]
shows that larger widths leads to better optimization landscape. While previous work has mainly
studied the effect of network structure on prediction accuracy  we focus on its effect on the optimal
choice of batch size for distributed computation.

3 Setup and Preliminaries

In this section  we present the necessary background and problem setup.
Mini-batch SGD The process of training a model from data can be cast as an optimization problem
known as empirical risk minimization (ERM):

n(cid:88)

i=1

min

w

1
n

(cid:96)(w; (xi  yi))

(k+1)B−1(cid:88)

(cid:96)=kB

where xi ∈ Rm represents the ith data point  n is the total number of data points  w ∈ Rd is a
parameter vector or model  and (cid:96)(·;·) is a loss function that measures the prediction accuracy of the
model on each data point. One way to approximately solve the above ERM is through mini-batch
stochastic gradient descent (SGD)  which operates as follows:

w(k+1)B = wkB − γ

∇fs(cid:96)(wkB) 

(3.1)

where each index s(cid:96) is drawn uniformly at random from [n] with replacement. We use w with
subscript kB to denote the model we obtain after k distributed iterations  i.e.  a total number of
kB gradient updates. In related studies there is often a normalization factor included in the batch
computation  but here we subsume that in the step size γ.
Gradient diversity and speed of convergence Gradient diversity measures the degree to which
individual gradients of the loss function are different from each other.
Deﬁnition 1 (Gradient Diversity [19]). We refer to the following ratio as gradient diversity

(cid:80)n
(cid:107)(cid:80)n
i=1 (cid:107)∇fi(w)(cid:107)2
i=1 ∇fi(w)(cid:107)2

2

2

(cid:80)n
2 +(cid:80)
(cid:80)n
i=1 (cid:107)∇fi(w)(cid:107)2
i=1 (cid:107)∇fi(w)(cid:107)2

2

=

i(cid:54)=j(cid:104)∇fi(w) ∇fj(w)(cid:105) .

∆S (w) : =

The gradient diversity ∆S (w) is large when the inner products between the gradients taken with
respect to different data points are small. Equipped with the notion of gradient diversity  we deﬁne a
batch size bound BS (w) for each data set S and each w as follows:

BS (w) := n · ∆S (w).

The following result [19] uses the notion of gradient diversity to capture the convergence rate of
mini-batch SGD.
Lemma 1. [Theorem 3 in [19] Informal] Suppose B ≤ δ · n∆S (w) + 1 ∀w in each iteration. If
serial SGD achieves an -suboptimal solution after T gradient updates  then using the same step-size
as serial SGD  mini-batch SGD with batch-size B can achieve a (1 + δ
2 )-suboptimal solution after
the same number of gradient updates/data pass ( i.e.  T /B iterations).

3

The above result is true for both convex and non-convex problems  and its main message is that
mini-batch SGD does not suffer from speedup saturation as long as the batch size is smaller than
n · ∆S (w) (up to a constant factor). Moreover  [19] also shows that this is a worst-case optimal
bound  i.e.  (roughly) if the batch size is larger than n times the gradient diversity  there exists some
model such that the convergence rate of mini-batch SGD is slower than that of serial SGD.
The main theoretical question that we study in this work is the following: how does gradient diversity
change as neural networks’ structure (depth and width) varies?
Fully-connected Neural Networks We consider both linear and non-linear fully connected net-
works  with L ≥ 2 layers. We denote by K(cid:96) the width (number of nodes) of the (cid:96)-th layer  where
(cid:96) ∈ {0  . . .   L}. The ﬁrst layer corresponds to the input of dimension d  hence K0 = d. The last
layer corresponds to the single output of the neural network  hence KL = 1. The weights of the
edges that connect the (cid:96) and (cid:96) − 1 layers  where l ∈ {1  . . .   L}  are represented by the matrix
W(cid:96) ∈ RK(cid:96)×K(cid:96)−1. For the sake of simplicity  we will express the collection of weights (i.e.  the
model) as w = (W1  W2  . . .   WL).
A general neural network (NN) with L ≥ 2 layers can be described as a collection of matrices
W1  . . .   WL  where W(cid:96) ∈ RK(cid:96)×K(cid:96)−1  together with a (generally nonlinear) activation function σ(·).
The output of a NN (or LNN) on input data point xi is then deﬁned as ˆyi = WL · σ(··· σ(W2 ·
σ(W1 · xi))). There are different types of activation that we study i.e.  tanh(x)  the softsign function
1+|x|  arctan(x)  and the ReLU function max{0  x}. For linear neural networks (LNNs)  we denote
(cid:96)=1 W(cid:96) = WL · WL−1 ··· W1. We will also write W(cid:96) p q to denote the element in the p-th

W =(cid:81)L

x

row and q-th column of matrix W(cid:96).
The output of the neural network with input xi is deﬁned as ˆyi. Throughout the theory part of this
paper  we will use the square loss function to measure the error  which we denote for the i-th data
point as fi = 1
such that the loss function is 0 on each data point when W(cid:96) p q = W ∗

2 (ˆyi − yi)2. Further  we assume that the data is achievable  i.e.  there exists W ∗

(cid:96) p q

(cid:96) p q.

4 Main Results

In this section  we present a theoretical analysis on how structural properties of a neural network  and
in particular the depth and width  inﬂuence the gradient diversity  and hence the convergence rate of
mini-batch SGD for varying batch size B. All proofs are left to the Appendix.
In the following derivations  we will assume that the labels {y1  . . .   yn} of the n data points are
realizable  i.e.  there exist a network of L layers that on input xi outputs yi. Our results are presented
as probabilistic statements  and for almost all weight matrices.
Warmup: 2-Layer Linear Neural Networks Our ﬁrst result concerns the case of a simple 2-layer
linear neural network with one hidden layer. To simplify notation  we will denote the width of the
hidden layer with K = K1. Further  Θ(·) and Ω(·) are used in their standard meaning. The main
result can be stated as follows:
l p q for l ∈ {1  2} and xi be independently
Theorem 1. Consider a 2 LNN. Let the weights Wl p q  W ∗
drawn random variables  such that their k-th order moments for k ≤ 4 are bounded in a postive
interval. Then  with arbitrary constant probability  the following holds:

BS (w) ≥

Θ(nKd)

Θ(Kn + dn + Kd)

For sufﬁciently large n  the above ratio on the batch size scales like Θ(Kd)
Θ(K+d). This ratio is always
increasing as a function of the width of the hidden layer  which implies that larger width allows for a
larger batch size.
2-Layer Nonlinear Neural Networks As a next step in our theoretical analysis  we analyze general
2-layer NNs with a nonlinear activation function σ.
Theorem 2. Consider a 2-layer NN with a monotone activation function σ such that for every
x we have: −σ(x) = σ(−x)  and both |σ(x)| and supx{xσ(cid:48)(x)} are bounded. Let the weights

4

E[n(cid:80)n
E[||(cid:80)n

Wl p q  W ∗
probability  the following holds:

l p q for l ∈ {1  2} and xi be i.i.d. random variables from N (0  1). Then  with high

≥ Ω(

Kd2

).

Kd + K + d

i=1 ||∇fi||2
2]
i=1 ∇fi||2
2]
where the expectation is over W2  W ∗
2 .
We should remark here that the above bound is weaker than the one obtained for the case of 2-layer
LNNs  since it bounds the ratio of the expectations  and not the expectation of the ratio (the batch
size bound). Nevertheless  we conjecture that the batch size bound concentrates  and thus the above
theorem can approximate the batch size bound well.
Another remark is that several commonly used activation functions in NNs  such as tanh  arctan 
and the softsign function satisfy the assumptions of the above theorem. The same trends can be
observed here as in the case of 2-layer LNNs: (i) larger width leads to a larger gradient diversity  and
thus faster convergence of distributed mini-batch SGD  and (ii) the ratio can never exceed Ω(d).
Multilayer Linear Neural Networks We generalize here our result for 2-layer LNNs to general
multilayer LNNs of arbitrary depth L ≥ 2. Below is our main result.
Theorem 3. Let the weight values Wl p q for l ∈ {1  . . .   L} and xi be independently drawn
2 (W xi − W ∗xi)2 =
random variables from N (0  1). Consider a multilayer LNN where fi = 1
(cid:96) xi)2. Assuming that K(cid:96) ≥ 2 for every (cid:96) ∈ {0  . . .   L − 1}  and that n is

2 ((cid:81)L
(cid:96)=1 W(cid:96)xi −(cid:81)L

1
sufﬁciently large  then we have:

(cid:96)=1 W ∗

E[n(cid:80)n
E[||(cid:80)n

ρ =

i=1 ||∇fi||2
2]
i=1 ∇fi||2
2]

≥

(cid:80)L−1

φ=1

L
L−φ
Kφ−1 + 2L
d−1

.

(4.1)

Again  note that the above bound is weaker than the one obtained for the case of 2-layer LNNs  since
it bounds the ratio of the expectations  and not the expectation of the ratio. It is believed that the
denominator and numerator should concentrate around their expectations (as was the case in Theorem
1) and thus ratio of the expectation reﬂects the expectation of the ratio. Whether this can be proved
remains an interesting open question.
We next discuss the implications of Theorem 3 on the convergence rate of mini-batch SGD. To analyze
the behavior of the bound  consider the simple case where all the hidden layers (l = 1  . . .   L − 1)
have exactly the same width K. In this case  the ratio in Eq. (4.1) becomes:

ρ ≥

1

L−1
2(K−1) + 2
d−1

= Θ

(cid:18) dK

(cid:19)

dL + K

There are three takeaways from the above bound. First  by increasing the width K of the LNN  the
ratio increases as well  which implies that the convergence rate increases. Second  the effect of the
depth L is the opposite: by increasing the depth  the ratio decreases. Third  the ratio can never exceed
Θ(d)  but it can be arbitrarily small. Suppose now that we ﬁx the total number of weights in the LNN 
and then start increasing the width of each layer (which means that the depth will decrease). In this
case  the ratio will also increase.
We conclude by noting that the same behavior of the bound w.r.t. width and depth can be observed if
we drop the simplifying assumption that all layers have the same width.

5 Experiments

In this section  we provide empirical results on how the structure of a neural network (width and
depth) impacts its amenability to large-batch training using various datasets and network architectures.
Our main ﬁndings are three-fold:

1. For all neural networks we used  there exists a threshold B∗  such that using batch size

larger than this threshold induces slower convergence;

2. The threshold of wider neural networks is often larger than that of deeper ones;

5

Dataset

# datapoints

Model
# Classes

# Parameters

Synthetic
10 000
linear FC

+∞
16k

MNIST
70 000

Cifar10
60 000

FC/LeNet

ResNet-18/34

10

16k / 431k
96% / 98%

10

11m / 21m

EMNIST Gisette
6 000
131 600
FC
2

FC
47
16k
65%

262k
95%

Converged Accuracy
Table 1: The datasets used and their associated learning models and hyper-parameters.

95%

10−12 (loss)

3. When using the same large batch size  almost all wider neural networks need much fewer

epochs to converge compared to their deeper counterparts.

Those ﬁndings validate our theoretical analysis and suggest that wider neural networks are indeed
more amenable to large-batch training and thus more suitable to scale out.
Implementation and Setup We implemented our experimental pipeline in Keras [43]  and conducted
all experiments on p2.xlarge instances on Amazon EC2. All results reported are averaged from 5
independent runs.

Datasets and Networks Table 1 summarizes the datasets and networks used in the experiments.
In the synthetic dataset  all data points were independently drawn from N (0  1) as described by our
theory results. A deep linear fully connected neural network (FC) whose weights were generated
from N (0  1) independently was used to produce the true labels. The task on the synthetic data is a
regression task. We train linear FCs on the synthetic dataset. The real-world datasets we used include
MNIST [44]  EMNIST[45]  Gisette [46]  and CIFAR-10 [47]  with appropriate networks ranging
from linear  to non-linear fully connected ones  and to LeNet [48] and ResNet [28].
For each network  we ﬁx the total number of parameters and vary its depth/number of layers L and
width K. For fully connected networks and LeNet  we vary depth L from 1 to 10 and change K
accordingly to ensure the total number of parameters are approximately ﬁxed. More precisely  we ﬁx
the total number of parameters p  and solve the following equations

din × K + (L − 1) × K 2 + K × dout = p.

where din is the dimension of the data and dout is the size of output. For ResNet  we vary two
parameters separately. We ﬁrst vary the width and depth of the fully connected layers without
changing the residual blocks. Next we ﬁx the fully connected layers and change the number of blocks
and convolution ﬁlters in each chunk. We refer to the building block in a residual function described
in [28] as chunk. For ResNet-18/34 architecture  we use [s1  s2  s3  s4] to denote a particular structure 
where s1 represents the number of blocks stacked in the ﬁrst chunk  s2 is the number of blocks
stacked in the second chunk  etc. For varying depths  we incrementally increase or decrease one
block in each chunk and adjust the number of convolutional ﬁlters in each block to meet the ﬁxed
number of parameters requirement.
For each combination of depth and width of a NN architecture  we train the model by setting a
constant threshold on training accuracy for classiﬁcation tasks  or loss for regression tasks. We then
train the NN for a variety of batch sizes  in range of 2i  for i ∈ {5 ···   12}. We tune the step size in
the following way: (i) for all learning rates η from a grid of candidate values  we run the training
process with η for 2 passes over the data; and then (ii) we choose ˆη which leads to the lowest training
loss after two epochs. An epoch represents a full pass over the data.
Experimental Results We ﬁrst verify whether gradient diversity reﬂects the amenability to large
batch training. For each linear FC network with ﬁxed width and depth  we measure its gradient
diversity every ten epochs and compute the average. Figure 2(a) shows how the averaged gradient
diversity varies as depth/width changes  while Figure 2(b) presents the largest batch to converge
for each network within a pre-set number of epochs. Both of them increase as the width K of the
networks increases. In fact  as shown in Figure 2(c)  the largest batch size that does not impact the
convergence rate grows monotonically w.r.t the gradient diversity. This validates our theoretical
analysis that gradient diversity can be used to capture the amenability to large batch training.

6

(a) Gradient Diversity

(b) Largest Batch Size

(c) Diversity vs Batch Size

Figure 2: The effect of gradient diversity for linear FCs trained on the synthetic dataset for a regression task. (a)
Gradient diversity for different width/depth (b) Largest batch size to converge to loss 10−12  within a pre-set
number (i.e.  14) of epochs. (c) Largest batch size v.s. gradient diversity.

(a) Synthetic  Linear FC

(b) MNIST  FC

(c) EMNIST  FC

(d) Gisette  FC

(e) MNIST  LeNet

(f) Cifar10  ResNet18  FC (g) Cifar10  ResNet18  Res (h) Cifar10  ResNet34  Res
Figure 3: Number of epochs needed to converge to the same loss / accuracy given in Table 1. K represents
width  and L depth. In (f) We ﬁx the residual blocks of ResNet 18 and only vary the fully-connected parts. In (g)
and (h)  we ﬁx the fully connected layers and vary the residual blocks of ResNet 18 and ResNet 34.

Next  we study the number of epochs needed to converge when different batch sizes are used for
real-world datasets. First  for almost all network architectures  there exists a batch size threshold 
such that using a batch size larger than this  requires more epochs for convergence  consistent with
the observations in [19]. For example  in Figure 3(b)  when the batch size is smaller than 256  the FC
network with width K = 17 and depth L = 10 needs a small number (2 to 3) of epochs to converge.
But when the batch size becomes larger than 256  the number of epochs necessary for convergence
increases signiﬁcantly  e.g.  it takes 50 epochs to converge when batch size is 4096. Moreover  we
observe that the threshold increases as width increases. Again as shown in Figure 3(b)  the batch-size
threshold for the FC network with L = 10 is 256  but goes up to 1024 with L = 1. Furthermore 
when using the same large batch size  wider networks tend to require fewer epochs to converge than
the deeper ones. In Figure 3(c)  for instance  using the same batch size of 4096  the required epochs
to converge decreases from 211 to 9 as width K increases from 17 to 21. Those trends are similar for
all FC networks we used in the experiments.
When it comes to ResNets and LeNet  the trends are not always as sharp. This is expected since our
theoretical analysis does not cover such cases  but the main trend can still be observed. For example 
as shown in Figures 3(e) and 3(f)  for a ﬁxed batch size  increasing the width almost always leads
to a decrease in number of epochs for convergence. Figure 4  depicts the exact number of epochs
to converge for each network architecture  and plots them as a heatmap. It is interesting to see that
for ResNet  there is a small fraction of cases where increase of depth can also reduce the number of
epochs for convergence.
In many practical applications  only a reasonable and limited number of data passes is performed
due to time and resources constraints. Thus  we also study how the structure of a network affects the
largest possible batch size to converge within a ﬁxed number of epochs/data passes to a pre-speciﬁed
accuracy. As shown in Figure 5  neural networks with larger width K usually allow much larger
batch sizes to converge within a small  pre-set number of total epochs. This is especially beneﬁcial in

7

K:16L:11621631641551561571581491410Structure of Network0.000.020.040.060.080.10Gradient DiversityK:16L:11621631641551561571581491410Structure of Network05001000150020002500300035004000Largest Batch Sizefor Fixed Epochs0.0000.0250.0500.0750.100Gradient Diversity01000200030004000Largest Batch Size102103Batch Size0.00.51.0Number ofEpochs1e4K:16  L:1K:16  L:4K:15  L:7K:14  L:10102103Batch Size02040K:21  L:1K:19  L:4K:18  L:7K:17  L:10102103Batch Size0100200K:21  L:1K:19  L:4K:18  L:7K:17  L:10102103Batch Size01020K:52  L:1K:51  L:4K:49  L:7K:48  L:10102103Batch Size050100K:143  L:1K:88  L:4K:71  L:7K:61  L:10102103Batch Size204060K:143  L:1K:88  L:4K:71  L:7K:61  L:10102103Batch Size255075Num Blocks:[3  3  3  3]Num Blocks:[5  5  5  5]Num Blocks:[6  6  6  6]102103Batch Size2040Num Blocks:[3  4  6  3]Num Blocks:[5  6  8  5]Num Blocks:[6  7  9  6](a) Synthetic  Linear FC

(b) MNIST  Linear FC

(c) EMNIST  FC

(d) Gisette  FC

(e) MNIST  LeNet

(f) Cifar10  ResNet18  FC (g) Cifar10  ResNet18  Res (h) Cifar10  ResNet34  Res

Figure 4: Heatmap on number of epochs needed to converge to loss / accuracy deﬁned in Table 1. We report
the log10 of the epochs for (a) and the real epochs for the others.

(a) Synthetic  Linear FC

(b) MNIST  FC

(c) EMNIST  FC

(d) Gisette  FC

(e) MNIST on LeNet

(f) Cifar10  ResNet18  FC (g) Cifar10  ResNet18  Res (h) Cifar10  ResNet34  Res

Figure 5: Largest possible batch size to converge within a ﬁxed number of epochs.

the scenarios of large-scale distributed learning  since increasing the batch size can result in more
speedup gains due to a reduction in the total amount of communication. Finally  we should note that
the largest batch size differs among different networks  as well as different datasets. This is because
gradient diversity is both data-dependent and model-dependent.
6 Conclusion

In this paper  we study how the structure of a neural network affects the performance of large-batch
training. Through the lens of gradient diversity  we quantitatively connect a network’s amenability to
larger batches during training with its depth and width. Extensive experimental results  along with
theoretical analysis  demonstrate that for a large class of neural networks  increasing width leads
to larger gradient diversity and thus allows for a larger batch training that is always beneﬁcial for
distributed computation.
In the future  we plan to explore how a particular structure  e.g.  convolutional ﬁlters  residual blocks 
etc  affects gradient diversity. From a practical perspective  we argue that it is important to consider
the architecture of a network with regards to its amenability for speedups in a distributed setting.
Hence  we plan to explore how one can ﬁne-tune a network so that large-batch training is enabled 
and communication bottlenecks are minimized. Another direction is to quantitatively study how the
generalization error is affected.

8

12345678910Depth of Hidden Layers3264128256512102420484096Batch Size0.50.60.70.70.81.01.11.31.51.90.60.70.80.81.01.21.41.61.81.90.60.80.91.01.21.51.71.92.12.20.61.01.11.21.61.72.02.22.42.50.71.01.61.51.82.12.32.52.72.80.81.31.81.62.12.42.62.83.03.11.01.52.01.92.42.73.03.13.43.41.11.72.22.22.82.83.33.43.64.10.81.62.43.24.012345678910Depth of Hidden Layers326412825651210242048409611111111121111111122111112222311122223441223234559223346789153457891114192666910141518242550102030405012345678910Depth of Hidden Layers3264128256512102420484096112234699112323568111415245799141520242235781115182334571213242927475813172226435162909132328445670921151409152436486376113130211408012016020012345678910Depth of Hidden Layers3264128256512102420484096111111111111111111111111111122121222222322333344454445676778789101113131416131112121618181921212851015202512345678910Depth of Hidden Layers326412825651210242048409611111111111111111111111111222212222333454446786981291416181521201828283341364541484346445282818390828388961041142040608010012345678910Depth of Hidden Layers32641282565121024204840961718202021232324272813161617171819202122141516171818192021221214161617181919212312151617181921212326151719212324262729312724262830343538404445394245515254596468203040506012345Number of Blocksin Every Chunk326412825651210242048409681013151571013152079121418781115148911151711131318162421224253414868779415304560759012345Number of Blocksin Every Chunk3264128256512102420484096810121314810121415899121478910138101112121214161718201829293258363845571020304050K:16L:11621641561410Structure of Network102103Largest Batch Sizefor Fixed EpochsK:21L:11941961881710Structure of Network103K:21L:11941961871710Structure of Network102103K:52L:15145064974710Structure of Network103K:143L:111326786496110Structure of Network102103K:143L:19737566496110Structure of Network102103K:64L:2523444405366Structure of Network102103K:100L:2773644575516Structure of Network102103,Lingjiao Chen
Hongyi Wang
Jinman Zhao
Dimitris Papailiopoulos
Paraschos Koutris
Tengyu Xu
Shaofeng Zou
Yingbin Liang