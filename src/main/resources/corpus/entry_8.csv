2013,(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings,We provide a general technique for making online learning algorithms differentially private  in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a \emph{convex} loss function which is a sum of smaller convex loss terms  one for each data point. We modify the popular \emph{mirror descent} approach  or rather a variant called \emph{follow the approximate leader}.    The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting  our algorithms improve over the regret bounds of previous work.  In many cases  our algorithms (in both settings) matching the dependence on the input length  $T$  of the \emph{optimal nonprivate} regret bounds up to logarithmic factors in $T$. Our algorithms require logarithmic space and update time.,(Nearly) Optimal Algorithms for Private Online
Learning in Full-information and Bandit Settings

Adam Smith⇤

Pennsylvania State University
asmith@cse.psu.edu

Abhradeep Thakurta†
Stanford University and

Microsoft Research Silicon Valley Campus

b-abhrag@microsoft.com

Abstract

We give differentially private algorithms for a large class of online learning al-
gorithms  in both the full information and bandit settings. Our algorithms aim
to minimize a convex loss function which is a sum of smaller convex loss terms 
one for each data point. To design our algorithms  we modify the popular mirror
descent approach  or rather a variant called follow the approximate leader.
The technique leads to the ﬁrst nonprivate algorithms for private online learning in
the bandit setting. In the full information setting  our algorithms improve over the
regret bounds of previous work (due to Dwork  Naor  Pitassi and Rothblum (2010)
and Jain  Kothari and Thakurta (2012)). In many cases  our algorithms (in both
settings) match the dependence on the input length  T   of the optimal nonprivate
regret bounds up to logarithmic factors in T . Our algorithms require logarithmic
space and update time.

1

Introduction

This paper looks at the information leaked by online learning algorithms  and seeks to design ac-
curate learning algorithms with rigorous privacy guarantees – that is  algorithms that provably leak
very little about individual inputs.
Even the output of ofﬂine (batch) learning algorithms can leak private information. The dual form
of a support vector machine’s solution  for example  is described in terms of a small number of exact
data points  revealing these individuals’ data in the clear. Considerable effort has been devoted to
designing batch learning algorithms satisfying differential privacy (a rigorous notion of privacy that
emerged from the cryptography literature [DMNS06  Dwo06])  for example [BDMN05  KLN+08 
CM08  CMS11  Smi11  KST12  JT13  DJW13].
In this work we provide a general technique for making a large class of online learning algorithms
differentially private  in both the full information and bandit settings. Our technique applies to
algorithms that aim to minimize a convex loss function which is a sum of smaller convex loss terms 
one for each data point. We modify the popular mirror descent approach (or rather a variant called
follow the approximate leader) [Sha11  HAK07].
In most cases  the modiﬁed algorithms provide similar accuracy guarantees to their nonprivate coun-
terparts  with a small (logarithmic in the stream length) blowup in space and time complexity.

Online (Convex) Learning: We begin with the full information setting. Consider an algorithm
that receives a stream of inputs F = hf1  ....  fTi  each corresponding to one individual’s data. We
interpret each input as a loss function on a parameter space C (for example  it might be one term

⇤Supported by NSF awards #0941553 and #0747294.
†Supported by Sloan Foundation fellowship and Microsoft Research.

1

in a convex program such as the one for logistic regression). The algorithm’s goal is to output a
sequence of parameter estimates w1  w2  ...  with each wt in C  that roughly minimizes the errors
Pt ft(wt). The difﬁculty for the algorithm is that it computes wt based only on f1  ...  ft1. We
seek to minimize the a posteriori regret 

Regret(T ) =

ft(w)

(1)

TXt=1

ft(wt)  min
w2C

TXt=1

In the bandit setting  the input to the algorithms consists only of f1(w1)  f2(w2)  .... That is  at each
time step t  the algorithm learns only the cost ft1(wt1) of the choice wt1 it made at the previous
time step  rather than the full cost function ft1.
We consider three types of adversarial input selection: An oblivious adversary selects the input
stream f1  ...  fT ahead of time  based on knowledge of the algorithm but not of the algorithm’s
random coins. A (strongly) adaptive adversary selects ft based on the output so far w1  w2  ...  wt
(but not on the algorithm’s internal random coins).
Both the full-information and bandit settings are extensively studied in the literature (see  e.g. 
[Sha11  BCB12] for recent surveys). Most of this effort has been spent on online learning prob-
lems are convex  meaning that the loss functions ft are convex (in w) and the parameter set C✓ Rp
is a convex set (note that one can typically “convexify” the parameter space by randomization). The
problem dimension p is the dimension of the ambient space containing C.
We consider various restrictions on the cost functions  such as Lipschitz continuity and strong con-
vexity. A function f : C! R is L-Lipschitz with respect to the `2 metric if |f (x)  f (y)|
Lkx  yk2 for all x  y 2C . Equivalently  for every x 2C 0 (the interior of C) and every subgra-
dient z 2 @f (x)  we have kzk2  L. (Recall that z is a subgradient of f at x if the function
˜f (y) = f (x) + hz  y  xi is a lower bound for f on all of C. If f is convex  then a subgradient
exists at every point  and the subgradient is unique if and only if f is differentiable at that point.)
The function f is H-strongly convex w.r.t. `2 if for every y 2C   we can bound f below on C by a
quadratic function of the form ˜f (y) = f (x) + hz  y  xi + H
2. If f is twice differentiable 
H-strong convexity is equivalent to the requirement that all eigenvalues of r2f (w) be at least H
for all w 2C .
We denote by D the set of allowable cost functions; the input sequence thus lies in DT .
Differential Privacy  and Challenges for Privacy in the Online Setting: We seek to design on-
line learning algorithms that satisfy differential privacy [DMNS06  Dwo06]  which ensures that the
amount of information an adversary learns about a particular cost function ft in the function se-
quence F is almost independent of its presence or absence in F . Each ft can be thought as private
information belonging to an individual. The appropriate notion of privacy here is when the entire
sequence of outputs of the algorithms ( ˆw1  ...  ˆwT ) is revealed to an attacker (the continual observa-
tion setting [DNPR10]). Formally  we say two input sequences F  F 0 2D T are neighbors if they
differ only in one entry (say  replacing ft by f0t).
Deﬁnition 2 (Differential privacy [DMNS06  Dwo06  DNPR10]). A randomized algorithm A is
(✏  )-differentially private if for every two neighboring sequences F  F 0 2D T   and for every event
O in the output space CT  
(2)
If  is zero  then we simply say A is ✏-differentially private.
Here A(F ) refers to the entire sequence of outputs produced by the algorithm during its execution.1
Our protocols all satisfy ✏-differential privacy (that is  with  = 0). We include  in the deﬁnition
for comparison with previous work.

Pr[A(F ) 2O ]  e✏ Pr[A(F 0) 2O ] + .

2 ky  xk2

1As deﬁned  differential privacy requires indistinguishable outputs only for nonadaptively chosen sequences
(that is  sequences where the inputs at time t are ﬁxed ahead of time and do not depend on the outputs at times
1  ...  t  1). The algorithms in our paper (and in previous work) in fact satisfy a stronger adaptive variant 
in which an adversary selects the input online as the computation proceeds. When  = 0  the nonadaptive
and adaptive variants are equivalent [DNPR10]. Moreover  protocols based on “randomized response” or the
“tree-based sum” protocol of [DNPR10  CSS10] are adaptively secure  even when > 0. We do not deﬁne the
adaptive variant here explicitly  but we use it implicitly when proving privacy.

2

Differential privacy provides meaningful guarantees in against an attacker who has access to con-
siderable side information: the attacker learns the same things about someone whether or not their
data were actually used (see [KS08  DN10  KM12] for further discussion).
Differential privacy is particularly challenging to analyze for online learning algorithms  since a
change in a single input at the beginning of the sequence may affect outputs at all future times in
ways that are hard to predict. For example  a popular algorithm for online learning is online gradient
descent: at each time step  the parameter is updated as wt+1 =⇧ C(wt1  ⌘trft1(wt1)) 
where ⇧C(x) the nearest point to x in C  and ⌘t > 0 is a parameter called the learning rate. A
change in an input fi (replacing it with f0i) leads to changes in all subsequent outputs wi+1  wi+2  ... 
roughly pushing them in the direction of rfi(wi)  rf0i(wi). The effect is ampliﬁed by the fact
that the gradient of subsequent functions fi+1  fi+2  ... will be evaluated at different points in the
two streams.

Previous Approaches: Despite the challenges  there are several results on differentially private
online learning. A special case  “learning from experts” in the full information setting  was discussed
in the seminal paper of Dwork  Naor  Pitassi and Rothblum [DNPR10] on privacy under continual
observation. In this case  the set of available actions is the simplex ({1  ...  p}) and the functions fi
are linear with coefﬁcients in {0  1} (that is  ft(w) = hw  cti where ct 2{ 0  1}p). Their algorithm
guarantees a weaker notion of privacy than the one we consider2 but  when adapted to our stronger
setting  it yields a regret bound of O(ppT /✏).
Jain  Kothari and Thakurta [JKT12] deﬁned the general problem of private online learning  and gave
algorithms for learning convex functions over convex domains in the full information setting. They
gave algorithms that satisfy (✏  )-differential privacy with > 0 (our algorithms satisfy the stronger
variant with  = 0). Speciﬁcally  their algorithms have regret ˜O(pT log(1/)/✏) for Lipshitz-
bounded  strongly convex cost functions and ˜O(T 2/3 log(1/)/✏) for general Lipshitz convex costs.
The idea of [JKT12] for learning strongly convex functions is to bound the sensitivity of the entire
vector of outputs w1  w2  ... to a change in one input (roughly  they show that when fi is changed  a
subsequent output wj changes by O(1/|j  i|)).
Unfortunately  the regret bounds obtained by previous work remain far from the best nonprivate
bounds. [Zin03] gave an algorithm with regret O(pT ) for general Lipshitz functions  assuming L
and the diameter kCk2 of C are constants. ⌦(pT ) regret is necessary (see  e.g.  [HAK07])  so the

dependence on T of [Zin03] is tight. When cost functions in F are H-strongly convex for constant
H  then the regret can be improved to O(log T ) [HAK07]  which is also tight. In this work  we give
new algorithms that match these nonprivate bounds’ dependence on T   up to (poly log T )/✏ factors.
We note that [JKT12] give one algorithm for a speciﬁc strongly convex problem  online linear re-
gression  with regret poly(log T ). One can view that algorithm as a special case of our results.
We are not aware of any previous work on privacy in the bandit setting. One might expect that bandit
learning algorithms are easier to make private  since they access data in a much more limited way.
However  even nonprivate algorithms for bandit learning are very delicate  and private versions had
until now proved elusive.

Our Results: In this work we provide a technique for making a large class of online learning algo-
rithms differentially private  in both the full information and bandit settings. In both cases  the idea is
to search for algorithms whose decisions at time t depend only on previous time steps through a sum
of observations made at times 1  2  ...  t. Speciﬁcally  our algorithms work by measuring the gradient
rft(wt) when ft is learned  and maintaining a differentially private running sum of the gradients
observed so far. We maintain this sum using the tree-based sum protocol of [DNPR10  CSS10]. We
then show that a class of learning algorithms known collectively as follow the approximate leader
(the version we use is due to [HAK07]) can be run given only these noisy sums  and that their regret
can be bounded even when these sums are inaccurate.
Our algorithms can be run with space O(log T )  and require O(log T ) running time at each step.

2Speciﬁcally  Dwork et al. [DNPR10] provide single-entry-level privacy  in the sense that a neighboring
data set may only differ in one entry of the cost vector for one round. In contrast  we allow the entire cost
vector to change at one round. Hiding that larger set of possible changes is more difﬁcult  so our algorithms
also satisfy the weaker notion of Dwork et al.

3

✏

Our contributions for the full information setting and their relation to previous work is summarized
in Table 1. Our main algorithm  for strongly convex functions  achieves regret O( log2.5 T
)  ignoring
factors of the dimension p  Lipschitz continuity L and strong convexity H. When strong convexity
is not guaranteed  we use regularization to ensure it (similar to what is done in nonprivate settings 
). These bounds essen-

e.g. [Sha11]). Setting parameters carefully  we get regret of O(q T log2.5 T
tially match the nonprivate lower bounds of ⌦(log T ) and ⌦(pT )  respectively.
The results in the full information setting apply even when the input stream is chosen adaptively as
a function of the algorithm’s choices at previous time steps. In the bandit setting  we distinguish
between oblivious and adaptive adversaries.
Furthermore  in the bandit setting  we assume that C is sandwiched between two concentric L2-balls
of radii r and R (where r < R). We also assume that for all w 2C   |ft(w)| B for all t 2 [T ].
Similar assumption were made in [FKM05  ADX10].
Our results are summarized in Table 2. For most of the settings we consider  we match the depen-
dence on T of the best nonprivate algorithm  though generally not the dependence on the dimension
p.

✏

Function class

Learning with ex-
perts (linear func-
tions over C =
({1  ...  p})
Lipshitz

and

Lipshitz and
strongly convex

Previous
bound.
˜O(ppT /✏)

private

upper

Our algorithm

Nonprivate
lower bound

[DNPR10] O(ppT log2.5 T /✏) ⌦(pT log p)

˜O(ppT 2/3 log(1/)/✏)
[JKT12]
˜O(ppT log2(1/)/✏)
[JKT12]

O(qpT log2.5 T /✏) ⌦(pT )

O(p log2.5 T /✏)

⌦(log T )

Table 1: Regret bounds for online learning in the full information setting. Bounds in lines 2 and 3
hide the (polynomial) dependencies on parameters L  H. Notation ˜O(·) hides poly(log(T )) factors.

Function class
Learning with experts (linear func-
tions over C =( {1  ...  p})
Lipschitz
Lipschitz
strongly
(Adaptive)
Lipschitz
(Oblivious)

strongly

convex

convex

and

and

Our result
˜O(pT 3/4/✏) O(pT )

Best nonprivate bound
[AHR08]

˜O(pT 3/4/✏) O(pT 3/4)
[FKM05]
˜O(pT 3/4/✏) O(p2/3T 3/4)[ADX10]

˜O(pT 2/3/✏) O(p2/3T 2/3)[ADX10]

Table 2: Regret bounds for online learning in the bandit setting.

known nonprivate lower bound is pT . The ˜O(·) notation hides poly log factors in T . Bounds hide

In all these settings  the best

polynomial dependencies on L  H  r and R.

In the remainder of the text  we refer to appendices for many of the details of algorithms and proofs.
The appendices can be found in the “Supplementary Materials” associated to this paper.

2 Private Online Learning: Full-information Setting

In this section we adapt the Follow The Approximate Leader (FTAL) algorithm of [HAK07] to
design a differentially private variant. Our modiﬁed algorithm  which we call Private Follow The

4

Approximate Leader (PFTAL)  needs a new regret analysis as we have to deal with randomness due
to differential privacy.

2.1 Private Follow The Approximate Leader (PFTAL) with Strongly Convex Costs

schitz constant: L  convex set: C✓ Rp and privacy parameter: ✏.

Algorithm 1 Differentially Private Follow the Approximate Leader (PFTAL)
Input: Cost functions: hf1 ···   fTi (in an online sequence)  strong convexity parameter: H  Lip-
1: ˆw1 Any vector from C. Output ˆw1.
2: Pass 5f1( ˆw1)  L2-bound L and privacy parameter ✏ to the tree based aggregation protocol and
receive the current partial sum in ˆv1.
3: for time steps t 2{ 1 ···   T  1} do
4:

ˆwt+1 arg min
Pass 5ft+1( ˆwt+1)  L2-bound L and privacy parameter ✏ to the tree-based protocol (Algo-
rithm 2) and receive the current partial sum in ˆvt+1.

tP⌧ =1kw  ˆw⌧k2

w2C hˆvt  wi + H

2. Output ˆwt.

5:

2

6: end for

The main idea in PFTAL algorithm is to execute the well-known Follow The Leader algorithm (FTL)
algorithm [Han57] using quadratic approximations ˜f1 ···   ˜fT of the cost functions f1 ···   fT .
Roughly  at every time step (t + 1)  PFTAL outputs a vector w that approximately minimizes the
sum of the approximations ˜f1 ···   ˜ft over the convex set C.
Let ˆw1 ···   ˆwt be the sequence of outputs produced in the ﬁrst t time steps  and let ft be the cost-
function at step t. Consider the following quadratic approximation to ft (as in [HAK07]). Deﬁne

2 kw  ˆwtk2

˜ft(w) = ft( ˆwt) + h5ft( ˆwt)  w  ˆwti + H

(3)
where H is the strong convexity parameter. Notice that ft and ˜ft have the same value and gradient
at ˆwt (that is  ft( ˆwt) = ˜ft( ˆwt) and 5ft( ˆwt) = 5 ˜ft( ˆwt)). Moreover  ˜ft is a lower bound for ft
everywhere on C.
˜f⌧ (w) be the “leader” corresponding to the cost functions ˜f1 ···   ˜ft.
Let ˜wt+1 = arg min
w2C
Minimizing the sum of ˜ft(w) is the same as minimizing the sum of ˜ft(w)ft( ˆwt)  since subtracting
a constant term won’t change the minimizer. We can thus write ˜wt+1 as

tP⌧ =1

2

2

2

(4)

tX⌧ =1

kw  ˆw⌧k2

5ft( ˆw⌧ )  wi + H

tX⌧ =1
w2C h
˜wt+1 = arg min
Suppose  ˆw1 ···   ˆwt have been released so far. To release a private approximation to ˜wt+1  it
sufﬁces to approximate vt+1 = Pt
⌧ =1 5ft( ˆw⌧ ) while ensuring differential privacy. If we ﬁx the
previously released information ˆw⌧   then changing any one cost function will only change one of
the summands in vt+1.
With the above observation  we abstract out the following problem: Given a set of vectors
z1 ···   zT 2 Rp  compute all the partial sums vt =
z⌧   while preserving privacy. This problem
is well studied in the privacy literature. Assuming each zt has L2-norm of at most L0  the following
tree-based aggregation scheme will ensure that in expectation  the noise (in terms of L2-error) in
each of vt is OpL0 log1.5 T /✏ and the whole sequence v1 ···   vT is ✏-differentially private. We
Tree-based Aggregation [DNPR10  CSS10]: Consider a complete binary tree. The leaf nodes are
the vectors z1 ···   zT . (For the ease of exposition  assume T to be a power of two. In general 
we can work with the smallest power of two greater than T ). Each internal node in the tree stores
the sum of all the leaves in its sub-tree. In a differentially private version of this tree  we ensure
that each node’s sub-tree sum is (✏/log2T )-differentially private  by adding a noise vector b 2 Rp

now describe the tree-based scheme.

tP⌧ =1

5

whose L2-norm is Gamma distributed and has standard deviation O(
). Since each zt only
affects log2T nodes in the tree  by the composition property [DMNS06]  the complete tree will be
⌧ =1 z⌧
)  since one can compute vt from at most log T nodes in the tree. A formal

✏-differentially private. Moreover  the algorithm’s error in estimating any partial sum vt =Pt

grows as O(
description of the tree based aggregation scheme in given in Appendix A.
Now we complete the PFTAL algorithm by computing the private version ˆwt+1 of ˜wt+1 in (4) as
the minimizer of the perturbed loss function:

ppL0 log2 T

✏

ppL0 log T

✏

tX⌧ =1

ˆwt+1 = arg min

w2C hˆvt  wi + H

2

kw  ˆw⌧k2

2

(5)

Here ˆvt is the noisy version of vt  computed using the tree-based aggregation scheme. A formal
description of the algorithm is given in Algorithm 1.

Note on space complexity: For simplicity  in the description of tree based aggregation scheme
(Algorithm 2 in Appendix A) we maintain the complete binary tree. However  it is not hard to show
at any time step t  it sufﬁces to keep track of the vectors (of partial sums) in the path from zt to the
root of the tree. So  the amount of space required by the algorithm is O(log T ).

2.1.1 Privacy and Utility Guarantees for PFTAL (Algorithm 1)
In this section we provide the privacy and regret guarantees for the PFTAL algorithm (Algorithm 1).
For detailed proofs of the theorem statements  see Appendix B.
Theorem 3 (Privacy guarantee). Algorithm 1 is ✏-differentially private.

Proof Sketch. Given the binary tree  the sequence ˆw2 ···   ˆwT is completely determined. Hence 
it sufﬁces to argue privacy for the collection of noisy sums associated to nodes in the binary tree.
At ﬁrst glance  it seems that each loss function affects only one leaf in the tree  and hence at most
log T of the nodes’ partial sums. If it were true  that statement would make the analysis simple.
The analysis is delicate  however  since the value (gradient z⌧ ) at a leaf ⌧ in the tree depends on the
partial sums that are released before time ⌧. Hence  changing one loss function ft actually affects
all subsequent partial sums. One can get around this by using the fact that differential privacy
composes adaptively [DMNS06]: we can write the computations done on a particular loss function
ft as a sequence of log T smaller differentially private computations  where the each computation
in the sequence depends on the outcome of previous ones. See Appendix B for details.

In terms of regret guarantee  we show that our algorithm enjoys regret of O(p log2.5 T ) (assuming
other parameters to be constants). Compared to the non-private regret bound of O(log T )  our regret
bound has an extra log1.5 T factor and an explicit dependence on the dimensionality (p). A formal
regret bound for PFTAL algorithm is given in Theorem 4.
Theorem 4 (Regret guarantee). Let f1 ···   fT be L-Lipschitz  H-strongly convex functions and let
C✓ Rp be a ﬁxed convex set. For adaptive adversaries  the expected regret satisﬁes:

E [Regret(T )] = O✓ p(L + HkCk2)2 log2.5 T

✏H

◆ .

Here expectation is taken over the random coins of the algorithm and adversary.

Results for Lipschitz Convex Costs: Our algorithm for strongly convex costs can be adapted to
arbitrary Lipschitz convex costs by executing Algorithm 1 on functions ht(w) = ft(w) + H
2 kwk2
2
instead of the ft’s. Setting H = O(p log2.5 T /(✏pT )) will give us a regret bound of ˜O(ppT /✏).
See Appendix C for details.

3 Private Online Learning: Bandit Setting

In this section we adapt the Private Follow the Approximate Leader (PFTAL) from Section 2 to
the bandit setting. Existing (nonprivate) bandit algorithms for online convex optimization follow

6

a generic reduction to the full-information setting [FKM05  ADX10]  called the “one-point” (or
“one-shot”) gradient trick. Our adaptation of PFTAL to the bandit setting also uses this technique.
Speciﬁcally  to deﬁne the quadratic lower bounds to the input cost functions (as in (3))  we replace
the exact gradient of ft at ˆwt with a one-point approximation.
In this section we describe our results for strongly convex costs. Speciﬁcally  to deﬁne the quadratic
lower bounds to the input cost functions (as in (3))  we replace the exact gradient of ft at ˆwt with
a one-point approximation. As in the full information setting  one may obtain regret bounds for
general convex functions in the bandit setting by adding a strongly convex regularizer to the cost
functions.

One-point Gradient Estimates [FKM05]: Suppose one has to estimate the gradient of a function
f : Rp ! R at a point w 2 Rp via a single query access to f. [FKM05] showed that one can
approximate 5f (w) by p
 f (w + u)u  where > 0 is a small real parameter and u is a uniformly
random vector from the p-dimensional unit sphere Sp1 = {a 2 Rp : kak2 = 1}. More precisely 
Euh p
5f (w) = lim
!0
For ﬁnite  nonzero values of   one can view this technique as estimating the gradient of a smoothed
version of f. Given > 0  deﬁne ˆf (w) = Ev⇠Bp [f (w + v)] where Bp is the unit ball in Rp. That
is  ˆf = f ⇤ UBp is the convolution of f with the uniform distribution on the ball Bp of radius .
By Stokes’ theorem  we have Eu⇠Sp1h p

 f (w + u)ui = 5 ˆf (w).

 f (w + u)ui.

3.1 Follow the Approximate Leader (Bandit version): Non-private Algorithm
Let ˜W = h ˜w1 ···   ˜wTi be a sequence of vectors in C (the outputs of the algorithm). Corresponding
to the smoothed function ˆft = f ⇤ UBp  we deﬁne a quadratic lower bound ˆgt:
2 kw  ˜wtk2

(6)
Notice that ˆgt is a uniform lower bound on ˆft satisfying ˆgt( ˜wt) = ˆft( ˜wt) and 5ˆgt( ˜wt) = 5 ˆft( ˜wt).
To deﬁne ˆgt  one needs access to 5 ˆft( ˜wt). As suggested above  we replace the true gradient with
the one-point estimate. Consider the following proxy ˜gt for ˆgt:

ˆgt(w) = ˆft( ˜wt) + h5 ˆft( ˜wt)  w  ˜wti + H

2

p


+h

ft( ˜wt + ut)ut  wi +

H
2 kw  ˜wtk2

2

(7)

˜gt(w) = ˆft( ˜wt)  h5 ˆft( ˜wt)  ˜wti
}

{z

|

A

where uT is drawn uniformly from the unit sphere Sp1. Note that in (7) we replaced the gradient
of ˆft with its one-point approximation only in one of its two occurrences (the inner product with w).
We would like to deﬁne ˜wt+1 as the minimizer of the sum of proxiesPt
⌧ =1 ˜g⌧ (w). One difﬁculty
remains: because ft is only assumed to be deﬁned on C  the approximation p
 ft( ˜wt + ut)ut is only
deﬁned when ˜wt is sufﬁciently far inside C. Recall from the introduction that we assume C contains
rBp (the ball of radius r). To ensure that we only evaluate f on C  we actually minimize over a
smaller set (1  ⇠)C  where ⇠ = 
˜wt+1 = arg min

˜g⌧ (w) = arg min

r . We obtain:

tX⌧ =1✓ p



ft( ˜wt + ut)ut◆  wi+ H

2

tX⌧ =1

kw ˜w⌧k2
(8)

2

w2(1⇠)C

tX⌧ =1

w2(1⇠)C h

(We have use the fact that to minimize ˜gt  one can ignore the constant term A in (7).)
We can now state the bandit version of FTAL. At each step t = 1  ...  T :

1. Compute ˜wt+1 using (8).
2. Output ˆwt = ˜wt + ut.

Theorem 12 (in Appendix D) gives the precise regret guarantees for this algorithm. For adaptive
adversaries the regret is bounded by ˜O(p2/3T 3/4) and for oblivious adversaries the regret is bounded
by ˜O(p2/3T 2/3).

7

3.2 Follow the Approximate Leader (Bandit version): Private Algorithm

To make the bandit version of FTAL ✏-differentially private  we replace the value vt =

 ft(w†t + ut)ut⌘ with a private approximation v†t computed using the tree-based sum

protocol. Speciﬁcally  at each time step t we output

⌧ =1⇣ p
Pt

w†t+1 = arg min

w2(1⇠)C hv†t   wi +

H
2

tX⌧ =1

kw  w†⌧k2
2 .

(9)

See Algorithm 3 (Appendix E.1) for details.
Theorem 5 (Privacy guarantee). The bandit version of Private Follow The Approximate Leader
(Algorithm 3) is ✏-differentially private.

The proof of Theorem 5 is exactly the same as of Theorem 3  and hence we omit the details.
In the following theorem we provide the regret guarantee of the Private FTAL (bandit version). For
a complete proof  see Appendix E.2.
Theorem 6 (Regret guarantee). Let Bp be the p-dimensional unit ball centered at the origin and
C✓ Rp be a convex set such that rBp ✓C✓ RBp (where 0 < r < R). Let f1 ···   fT be L-
Lipschitz  H-strongly convex functions such that for all w 2C   |fi(w)| B. Setting ⇠ = /r in the
bandit version of Private Follow The Approximate Leader (Algorithm 3 in Appendix E.1)  we obtain
the following regret guarantees.

1. (Oblivious adversary) With  = p

2. (Adaptive adversary) With  = p

T 1/3   E [Regret(T )]  ˜OpT 2/3
T 1/4   E [Regret(T )]  ˜OpT 3/4
1 + B

H

✏⌘. The expectations are taken over the ran-

Here  =⇣BR + (1 + R/r)L + (HkCk2+B)2
domness of the algorithm and the adversary.
One can remove the dependence on r in Thm. 6 by rescaling C to isotropic position. This increases
the expected regret bound by a factor of (LR + kCk2). See [FKM05] for details.
Bound for general convex functions: Our results in this section can be extended to the setting of
arbitrary Lipshitz convex costs via regularization  as in Section C (by adding H
2 to each cost
function ft) . With the appropriate choice of H the regret scales as ˜O(T 3/4/✏) for both oblivious
and adaptive adversaries. See Appendix E.3 for details.

2 kwk2

4 Open Questions

Our work raises several interesting open questions: First  our regret bounds with general convex
functions have the form ˜O(pT /✏). We would like to have a regret bound where the parameter 1/✏
is factored out with lower order terms in the regret  i.e.  we would like to have regret bound of the
form O(pT ) + o(pT /✏).
Second  our regret bounds for convex bandits are worse than the non-private bounds for linear and
multi-arm bandits. For multi-arm bandits [ACBF02] and for linear bandits [AHR08]  the non-private
regret bound is known to be O(pT ). If we use our private algorithm in this setting  we will incur a
regret of ˜O(T 2/3). Can we get O(pT ) regret for multi-arm or linear bandits?
Finally  bandit algorithms require internal randomness to get reasonable regret guarantees. Can we
harness the randomness of non-private bandit algorithms in the design private bandit algorithms?
Our current privacy analysis ignores this additional source of randomness.

8

References
[ACBF02] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning  2002.

[ADX10] Alekh Agarwal  Ofer Dekel  and Lin Xiao. Optimal algorithms for online convex optimization

[AHR08]

[BCB12]

with multi-point bandit feedback. In COLT  2010.
Jacob Abernethy  Elad Hazan  and Alexander Rakhlin. Competing in the dark: An efﬁcient algo-
rithm for bandit linear optimization. In COLT  2008.
S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721  2012.

[BDMN05] Avrim Blum  Cynthia Dwork  Frank McSherry  and Kobbi Nissim. Practical privacy: The SuLQ

framework. In PODS  2005.
Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression.
2008.

In NIPS 

[CM08]

[CMS11] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empirical

[CSS10]

[DJW13]

risk minimization. Journal of Machine Learning Research  12:1069–1109  2011.
TH Hubert Chan  Elaine Shi  and Dawn Song. Private and continual release of statistics.
ICALP  2010.
John C. Duchi  Michael I. Jordan  and Martin J. Wainwright.
tical minimax rates.
http://arxiv.org/abs/1302.3203.

Local privacy and statis-
In IEEE Symp. on Foundations of Computer Science (FOCS)  2013.

In

[DMNS06] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to sensitivity

in private data analysis. In TCC  2006.
Cynthia Dwork and Moni Naor. On the difﬁculties of disclosure prevention in statistical databases
or the case for differential privacy. J. Privacy and Conﬁdentiality  2(1)  2010.

[DN10]

[DNPR10] Cynthia Dwork  Moni Naor  Toniann Pitassi  and Guy N Rothblum. Differential privacy under
In Proceedings of the 42nd ACM symposium on Theory of computing 

continual observation.
2010.
Cynthia Dwork. Differential privacy. In ICALP  2006.

[HAK07]

[Dwo06]
[FKM05] Abraham D Flaxman  Adam Tauman Kalai  and H Brendan McMahan. Online convex optimiza-

tion in the bandit setting: gradient descent without a gradient. In SODA  2005.
Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Journal of Machine Learning Research  2007.
James Hannan. Approximation to bayes risk in repeated play. 1957.
Prateek Jain  Pravesh Kothari  and Abhradeep Thakurta. Differentially private online learning. In
COLT  2012.
[JT13]
Prateek Jain and Abhradeep Thakurta. Differentially private learning with kernels. In ICML  2013.
[KLN+08] Shiva Prasad Kasiviswanathan  Homin K. Lee  Kobbi Nissim  Sofya Raskhodnikova  and Adam

[Han57]
[JKT12]

[KM12]

[KS08]

[KST12]

[Sha11]

[Smi11]

[Zin03]

Smith. What can we learn privately? In FOCS  2008.
Daniel Kifer and Ashwin Machanavajjhala. A rigorous and customizable framework for privacy.
In PODS  2012.
Shiva Prasad Kasiviswanathan and Adam Smith. A note on differential privacy: Deﬁning resis-
tance to arbitrary side information. CoRR  arXiv:0803.39461 [cs.CR]  2008.
Daniel Kifer  Adam Smith  and Abhradeep Thakurta. Private convex empirical risk minimization
and high-dimensional regression. In COLT  2012.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R
in Machine Learning  2011.
Adam Smith. Privacy-preserving statistical estimators with optimal convergence rates. In STOC 
2011.
Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
ICML  2003.

9

,Abhradeep Guha Thakurta
Adam Smith
Noga Alon
Daniel Reichman
Igor Shinkar
Tal Wagner
Jonathan Cohen
Tom Griffiths
Biswadip dey
Kayhan Ozcimder