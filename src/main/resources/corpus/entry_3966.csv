2008,On the Design of Loss Functions for Classification: theory  robustness to outliers  and SavageBoost,The machine learning problem of classifier design is studied from the perspective of probability elicitation  in statistics. This shows that the standard approach of proceeding from the specification of a loss  to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk  and derive the loss function. This has various consequences of practical interest  such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary  and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex  but does not compromise the computational tractability of classifier design  and is robust to the contamination of data with outliers. A new boosting algorithm  SavageBoost  is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods  such as Ada  Real  or LogitBoost  and converges in fewer iterations.,On the Design of Loss Functions for Classiﬁcation:

theory  robustness to outliers  and SavageBoost

Hamed Masnadi-Shirazi

Statistical Visual Computing Laboratory 

University of California  San Diego

La Jolla  CA 92039

hmasnadi@ucsd.edu

Abstract

Nuno Vasconcelos

Statistical Visual Computing Laboratory 

University of California  San Diego

La Jolla  CA 92039
nuno@ucsd.edu

The machine learning problem of classiﬁer design is studied from the perspective
of probability elicitation  in statistics. This shows that the standard approach of
proceeding from the speciﬁcation of a loss  to the minimization of conditional
risk is overly restrictive. It is shown that a better alternative is to start from the
speciﬁcation of a functional form for the minimum conditional risk  and derive
the loss function. This has various consequences of practical interest  such as
showing that 1) the widely adopted practice of relying on convex loss functions is
unnecessary  and 2) many new losses can be derived for classiﬁcation problems.
These points are illustrated by the derivation of a new loss which is not convex 
but does not compromise the computational tractability of classiﬁer design  and
is robust to the contamination of data with outliers. A new boosting algorithm 
SavageBoost  is derived for the minimization of this loss. Experimental results
show that it is indeed less sensitive to outliers than conventional methods  such as
Ada  Real  or LogitBoost  and converges in fewer iterations.

1 Introduction

The binary classiﬁcation of examples x is usually performed with recourse to the mapping ˆy =
sign[f (x)]  where f is a function from a pre-deﬁned class F  and ˆy the predicted class label. Most
state-of-the-art classiﬁer design algorithms  including SVMs  boosting  and logistic regression  de-
termine the optimal function f ∗ by a three step procedure: 1) deﬁne a loss function φ(yf (x))  where
y is the class label of x  2) select a function class F  and 3) search within F for the function f ∗ which
minimizes the expected value of the loss  known as minimum conditional risk. Although tremen-
dously successful  these methods have been known to suffer from some limitations  such as slow
convergence  or too much sensitivity to the presence of outliers in the data [1  2]. Such limitations
can be attributed to the loss functions φ(·) on which the algorithms are based. These are convex
bounds on the so-called 0-1 loss  which produces classiﬁers of minimum probability of error  but is
too difﬁcult to handle from a computational point of view.

In this work  we analyze the problem of classiﬁer design from a different perspective  that has long
been used to study the problem of probability elicitation  in the statistics literature. We show that the
two problems are identical  and probability elicitation can be seen as a reverse procedure for solving
the classiﬁcation problem: 1) deﬁne the functional form of expected elicitation loss  2) select a
function class F  and 3) derive a loss function φ. Both probability elicitation and classiﬁer design
reduce to the problem of minimizing a Bregman divergence. We derive equivalence results  which
allow the representation of the classiﬁer design procedures in “probability elicitation form”  and the
representation of the probability elicitation procedures in “machine learning form”. This equivalence
is useful in two ways. From the elicitation point of view  the risk functions used in machine learning
can be used as new elicitation losses. From the machine learning point of view  new insights on the
relationship between loss φ  optimal function f ∗  and minimum risk are obtained. In particular  it is
shown that the classical progression from loss to risk is overly restrictive: once a loss φ is speciﬁed 

1

both the optimal f ∗  and the functional form of the minimum risk are immediately pined down.
This is  however  not the case for the reverse progression: it is shown that any functional form of
the minimum conditional risk  which satisﬁes some mild constraints  supports many (φ  f ∗) pairs.
Hence  once the risk is selected  one degree of freedom remains: by selecting a class of f ∗  it is
possible to tailor the loss φ  so as to guarantee classiﬁers with desirable traits. In addition to this 
the elicitation view reveals that the machine learning emphasis on convex losses φ is misguided. In
particular  it is shown that what matters is the convexity of the minimum conditional risk. Once a
functional form is selected for this quantity  the convexity of the loss φ does not affect the convexity
of the Bregman divergence to be optimized.

These results suggest that many new loss functions can be derived for classiﬁer design. We illustrate
this  by deriving a new loss that trades convexity for boundedness. Unlike all previous φ  the one
now proposed remains constant for strongly negative values of its argument. This is akin to robust
loss functions proposed in the statistics literature to reduce the impact of outliers. We derive a new
boosting algorithm  denoted SavageBoost  by combination of the new loss and the procedure used
by Friedman to derive RealBoost [3]. Experimental results show that the new boosting algorithm is
indeed more outlier resistant than classical methods  such as AdaBoost  RealBoost  and LogitBoost.

2 Classiﬁcation and risk minimization

A classiﬁer is a mapping g : X → {−1  1} that assigns a class label y ∈ {−1  1} to a feature
vector x ∈ X   where X is some feature space. If feature vectors are drawn with probability density
PX(x)  PY (y) is the probability distribution of the labels y ∈ {−1  1}  and L(x  y) a loss function 
the classiﬁcation risk is R(f ) = EX Y [L(g(x)  y)]. Under the 0-1 loss  L0/1(x  y) = 1 if g(x) 6= y
and 0 otherwise  this risk is the expected probability of classiﬁcation error  and is well known to be
minimized by the Bayes decision rule. Denoting by η(x) = PY |X(1|x) this can be written as

g∗(x) = sign[2η(x) − 1].

(1)

Classiﬁers are usually implemented with mappings of the form g(x) = sign[f (x)]  where f is some
mapping from X to R. The minimization of the 0-1 loss requires that
sign[f ∗(x)] = sign[2η(x) − 1]  ∀x

(2)

When the classes are separable  any f (x) such that yf (x) ≥ 0  ∀x has zero classiﬁcation error. The
0-1 loss can be written as a function of this quantity

L0/1(x  y) = φ0/1[yf (x)] = sign[−yf (x)].

This motivates the minimization of the expected value of this loss as a goal for machine learning.
However  this minimization is usually difﬁcult. Many algorithms have been proposed to minimize
alternative risks  based on convex upper-bounds of the 0-1 loss. These risks are of the form

(3)
where φ(·) is a convex upper bound of φ0/1(·). Some examples of φ(·) functions in the literature are
given in Table 1. Since these functions are non-negative  the risk is minimized by minimizing the
conditional risk EY |X[φ(yf (x))|X = x] for every x ∈ X . This conditional risk can be written as

Rφ(f ) = EX Y [φ(yf (x))]

Cφ(η  f ) = ηφ(f ) + (1 − η)φ(−f ) 

where we have omitted the dependence of η and f on x for notational convenience.
Various authors have shown that  for the φ(·) of Table 1  the function f ∗

φ which minimizes (4)

f ∗
φ(η) = arg min

f

Cφ(η  f )

(4)

(5)

satisﬁes (2) [3  4  5]. These functions are also presented in Table 1. It can  in fact  be shown that (2)
holds for any f ∗
φ(·) which minimizes (4) whenever φ(·) is convex  differentiable at the origin  and
has derivative φ′(0) = 0 [5].
While learning algorithms based on the minimization of (4)  such as SVMs  boosting  or logistic
regression  can perform quite well  they are known to be overly sensitive to outliers [1  2]. These
are points for which yf (x) < 0. As can be seen from Figure 1  the sensitivity stems from the large

2

Table 1: Machine learning algorithms progress from loss φ  to inverse link function f ∗
conditional risk C ∗

φ(η)  and minimum

φ(v)

(1 − v)2

max(1 − v  0)2
max(1 − v  0)

exp(−v)

f ∗
φ(η)
2η − 1
2η − 1

sign(2η − 1)

2 log η
1
log η
1−η

1−η

C ∗

φ(η)

4η(1 − η)
4η(1 − η)
1 − |2η − 1|

2pη(1 − η)

-η log η − (1 − η) log(1 − η)

φ(η).
Algorithm

Least squares
Modiﬁed LS

SVM

Boosting

Logistic Regression

log(1 + e−v)

(inﬁnite) weight given to these points by the φ(·) functions when yf (x) → −∞. In this work  we
show that this problem can be eliminated by allowing non-convex φ(·). This may  at ﬁrst thought 
seem like a bad idea  given the widely held belief that the success of the aforementioned algorithms
is precisely due to the convexity of these functions. We will see  however  that the convexity of φ(·)
is not important. What really matters is the fact  noted by [4]  that the minimum conditional risk

C ∗

φ(η) = inf
f

Cφ(η  f ) = Cφ(η  f ∗
φ)

(6)

satisﬁes two properties. First  it is a concave function of η (η ∈ [0  1])1. Second  if f ∗
tiable  then C ∗

φ(η) is differentiable and  for any pair (v  ˆη) such that v = f ∗

φ(ˆη) 

φ is differen-

Cφ(η  v) − C ∗

φ(η) = B−C ∗

φ

(η  ˆη) 

(7)

where

BF (η  ˆη) = F (η) − F (ˆη) − (η − ˆη)F ′(ˆη).

(8)
is the Bregman divergence of the convex function F . The second property provides an interesting
interpretation of the learning algorithms as methods for the estimation of the class posterior proba-
bility η(x): the search for the f (x) which minimizes (4) is equivalent to a search for the probability
estimate ˆη(x) which minimizes (7). This raises the question of whether minimizing a cost of the
form of (4) is the best way to elicit the posterior probability η(x).

3 Probability elicitation

This question has been extensively studied in statistics. In particular  Savage studied the problem of
designing reward functions that encourage probability forecasters to make accurate predictions [6].
The problem is formulated as follows.

• let I1(ˆη) be the reward for the prediction ˆη when the event y = 1 holds.
• let I−1(ˆη) be the reward for the prediction ˆη when the event y = −1 holds.

The expected reward is

(9)
Savage asked the question of which functions I1(·)  I−1(·) make the expected reward maximal when
ˆη = η  ∀η. These are the functions such that

I(η  ˆη) = ηI1(ˆη) + (1 − η)I−1(ˆη).

(10)
with equality if and only if ˆη = η. Using the linearity of I(η  ˆη) on η  and the fact that J(η) is
supported by I(η  ˆη) at  and only at  η = ˆη  this implies that J(η) is strictly convex [6  7]. Savage
then showed that (10) holds if and only if

I(η  ˆη) ≤ I(η  η) = J(η)  ∀η

I1(η) = J(η) + (1 − η)J ′(η)
I−1(η) = J(η) − ηJ ′(η).

(11)
(12)

Deﬁning the loss of the prediction of η by ˆη as the difference to the maximum reward

L(η  ˆη) = I(η  η) − I(η  ˆη)

1Here  and throughout the paper  we omit the dependence of η on x  whenever we are referring to functions

of η  i.e. mappings whose range is [0  1].

3

Table 2: Probability elicitation form for various machine learning algorithms  and Savage’s procedure. In
Savage 1 and 2 m′ = m + k.

I1(η)

−4(1 − η)2
−4(1 − η)2

I−1(η)
−4η2
−4η2

sign[2η − 1] − 1

sign[2η − 1] + 1

J(η)

−4η(1 − η)
−4η(1 − η)
|2η − 1| − 1

−q 1−η

η
log η

−k(1 − η)2 + m′ + l

−k(1/η + log η) + m′ + l

−q η

1−η

log(1 − η)
−kη2 + m

−k log η + m′

−2pη(1 − η)

η log η + (1 − η) log(1 − η)

kη2 + lη + m
m + lη − k log η

Algorithm

Least squares
Modiﬁed LS

SVM

Boosting

Log. Regression

Savage 1
Savage 2

it follows that

L(η  ˆη) = BJ (η  ˆη) 

(13)
i.e. the loss is the Bregman divergence of J. Hence  for any probability η  the best prediction ˆη is the
one of minimum Bregman divergence with η. Savage went on to investigate which functions J(η)
are admissible. He showed that for losses of the form L(η  ˆη) = H(h(η) − h(ˆη))  with H(0) = 0
and H(v) > 0  v 6= 0  and h(v) any function  only two cases are possible. In the ﬁrst h(v) = v  i.e.
the loss only depends on the difference η − ˆη  and the admissible J are

(14)
for some integers (k  l  m). In the second h(v) = log(v)  i.e. the loss only depends on the ratio η/ˆη 
and the admissible J are of the form

J1(η) = kη2 + lη + m 

J2(η) = m + lη − k log η.

(15)

4 Classiﬁcation vs. probability elicitation

The discussion above shows that the optimization carried out by the learning algorithms is identical
to Savage’s procedure for probability elicitation. Both procedures reduce to the search for

ˆη∗ = arg min
ˆη

BF (η  ˆη) 

(16)

where F (η) is a convex function. In both cases  this is done indirectly. Savage starts from the speci-
ﬁcation of F (η) = J(η)  from which the conditional rewards I1(η) and I2(η) are derived  using (11)
and (12). ˆη∗ is then found by maximizing the expected reward I(η  ˆη) of (9) with respect to ˆη. The
learning algorithms start from the loss φ(·). The conditional risk Cφ(η  f ) is then minimized with
respect to f  so as to obtain the minimum conditional risk C ∗
φ(ˆη). This
is identical to solving (16) with F (η) = −C ∗
φ(η) it is possible
to express the learning algorithms in “Savage form”  i.e. as procedures for the maximization of (9) 
φ(η) in Table 1. This is
by deriving the conditional reward functions associated with each of the C ∗
done with (11) and (12) and the results are shown in Table 2. In all cases I1(η) = −φ(f ∗
φ(η)) and
I−1(η) = −φ(−f ∗
The opposite question of whether Savage’s algorithms be expressed in “machine learning form”  i.e.
as the minimization of (4)  is more difﬁcult. It requires that the Ii(η) satisfy

φ(η). Using the relation J(η) = −C ∗

φ(η) and the corresponding f ∗

φ(η)).

I1(η) = −φ(f (η))

(17)
(18)
φ it
φ is invertible  to think of
φ)−1(v) as a link function  which maps a real v into a probability η. Under this interpretation 

for some f (η)  and therefore constrains J(η). To understand the relationship between J  φ  and f ∗
helps to think of the latter as an inverse link function. Or  assuming that f ∗
η = (f ∗
it is natural to consider link functions which exhibit the following symmetry

I−1(η) = −φ(−f (η))

(19)
Note that this implies that f −1(0) = 1/2  i.e. f maps v = 0 to η = 1/2. We refer to such link
functions as symmetric  and show that they impose a special symmetry on J(η).

f −1(−v) = 1 − f −1(v).

4

Table 3: Probability elicitation form progresses from minimum conditional risk  and link function (f ∗
to loss φ. f ∗

φ)−1(η) 

Algorithm

φ(η) is not invertible for the SVM and modiﬁed LS methods.
φ)−1(v)
(f ∗
1
2 (v + 1)

J(η)

φ(v)

(1 − v)2

max(1 − v  0)2
max(1 − v  0)

exp(−v)

log(1 + e−v)

NA
N/A
e2v

1+e2v

ev

1+ev

Least squares
Modiﬁed LS

SVM

−4η(1 − η)
−4η(1 − η)
|2η − 1| − 1

Boosting

Logistic Regression

−2pη(1 − η)

η log η + (1 − η) log(1 − η)

Theorem 1. Let I1(η) and I−1(η) be two functions derived from a continuously differentiable
function J(η) according to (11) and (12)  and f (η) be an invertible function which satisﬁes (19).
Then (17) and (18) hold if and only if

In this case 

φ(v) = −J[f −1(v)] − (1 − f −1(v))J ′[f −1(v)].

J(η) = J(1 − η).

(20)

(21)

The theorem shows that for any pair J(η)  f (η)  such that J(η) has the symmetry of (20) and f (η)
the symmetry of (19)  the expected reward of (9) can be written in the “machine learning form”
of (4)  using (17) and (18) with the φ(v) given by (21). The following corollary specializes this
result to the case where J(η) = −C ∗
Corollary 2. Let I1(η) and I−1(η) be two functions derived with (11) and (12) from any continu-
ously differentiable J(η) = −C ∗

φ(η).

φ(η)  such that

C ∗

φ(η) = C ∗

φ(1 − η) 

and fφ(η) be any invertible function which satisﬁes (19). Then

I1(η) = −φ(fφ(η))

I−1(η) = −φ(−fφ(η))

with

φ(v) = C ∗

φ[f −1

φ (v)] + (1 − f −1

φ (v))(C ∗

φ)′[f −1

φ (v)].

(22)

(23)
(24)

(25)

Note that there could be many pairs φ  fφ for which the corollary holds2. Selecting a particular fφ
“pins down” φ  according to (25). This is the case of the algorithms in Table 1  for which C ∗
φ(η)
and f ∗
φ have the symmetries required by the corollary. The link functions associated with these
algorithms are presented in Table 3. From these and (25) it is possible to recover φ(v)  also shown
in the table.

5 New loss functions

The discussion above provides an integrated picture of the “machine learning” and “probability elic-
itation” view of the classiﬁcation problem. Table 1 summarizes the steps of the “machine learning
view”: start from the loss φ(v)  and ﬁnd 1) the inverse link function f ∗
φ(η) of minimum condi-
tional risk  and 2) the value of this risk C ∗
φ(η). Table 3 summarizes the steps of the “probability
elicitation view”: start from 1) the expected maximum reward function J(η) and 2) the link func-
tion (f ∗
φ(η)  the two procedures are
equivalent  since they both reduce to the search for the probability estimate ˆη∗ of (16).
Comparing to Table 2  it is clear that the least squares procedures are special cases of Savage 1  with
k = −l = 4 and m = 0  and the link function η = (v + 1)/2. The constraint k = −l is necessary

φ)−1(v)  and determine the loss function φ(v). If J(η) = −C ∗

2This makes the notation fφ and C ∗

φ technically inaccurate. C ∗

f φ would be more suitable. We  nevertheless 

retain the C ∗

φ notation for the sake of consistency with the literature.

5

)
v
(
φ

4

3.5

3

2.5

2

1.5

1

0.5

0

Least squares
Modified LS
SVM
Boosting
Logistic Reg.
Savage Loss
Zero−One

−6

−5

−4

−3

−2
v

−1

0

1

2

)
η
(

*φ
C

1

0.8

0.6

0.4

0.2

0
0

Least squares
Modified LS
SVM
Boosting
Logistic Reg.
Savage Loss
Zero−One

0.2

0.4

η

0.6

0.8

1

Figure 1: Loss function φ(v) (left) and minimum conditional risk C ∗
methods discussed in the text.

φ(η) (right) associated with the different

for (22) to hold  but not the others. For Savage 2  a “machine learning form” is not possible (at
this point)  because J(η) 6= J(1 − η). We currently do not know if such a form can be derived
in cases like this  i.e. where the symmetries of (19) and/or (22) are absent. From the probability
elicitation point of view  an important contribution of the machine learning research (in addition
to the algorithms themselves) has been to identify new J functions  namely those associated with
the techniques other than least squares. From the machine learning point of view  the elicitation
perspective is interesting because it enables the derivation of new φ functions.
The main observation is that  under the customary speciﬁcation of φ  both C ∗
φ(η) are
immediately set  leaving no open degrees of freedom. In fact  the selection of φ can be seen as the
indirect selection of a link function (f ∗
φ(η). The latter is an
approximation to the minimum conditional risk of the 0-1 loss  C ∗
(η) = 1 − max(η  1 − η). The
approximations associated with the existing algorithms are shown in Figure 1. The approximation
error is smallest for the SVM  followed by least squares  logistic regression  and boosting  but all
approximations are comparable. The alternative  suggested by the probability elicitation view  is
to start with the selection of the approximation directly. In addition to allowing direct control over
the quantity that is usually of interest (the minimum expected risk of the classiﬁer)  the selection of
φ(η) (which is equivalent to the selection of J(η)) has the added advantage of leaving one degree
C ∗
of freedom open. As stated by Corollary 2 it is further possible to select across φ functions  by
controlling the link function fφ. This allows tailoring properties of detail of the classiﬁer  while
maintaining its performance constant  in terms of the expected risk.

φ)−1 and a minimum conditional risk C ∗

φ(η) and f ∗

φ0/1

We demonstrate this point  by proposing a new loss function φ. We start by selecting the minimum
conditional risk of least squares (using Savage’s version with k = −l = 1  m = 0) C ∗
φ(η) =
η(1 − η)  because it provides the best approximation to the Bayes error  while avoiding the lack of
differentiability of the SVM. We next replace the traditional link function of least squares by the
logistic link function (classically used with logistic regression) f ∗
1−η . When used in the
context of boosting (LogitBoost [3])  this link function has been found less sensitive to outliers than
other variants [8]. We then resort to (25) to ﬁnd the φ function  which we denote by Savage loss 

2 log η

φ = 1

φ(v) =

1

(1 + e2v)2 .

(26)

A plot of this function is presented in Figure 1  along with those associated with all the algorithms
of Table 1. Note that the proposed loss is very similar to that of least squares in the region where |v|
is small (the margin)  but quickly becomes constant as v → −∞. This is unlike all other previous φ
functions  and suggests that classiﬁers designed with the new loss should be more robust to outliers.

It is also interesting to note that the new loss function is not convex  violating what has been an
hallmark of the φ functions used in the literature. The convexity of φ is  however  not important 
a fact that is made clear by the elicitation view. Note that the convexity of the expected reward
of (9) only depends on the convexity of the functions I1(η) and I−1(η). These  in turn  only depend
on the choice of J(η)  as shown by (11) and (12). From Corollary 2 it follows that  as long as
the symmetries of (22) and (19) hold  and φ is selected according to (25)  the selection of C ∗
φ(η)

6

Algorithm 1 SavageBoost

Input: Training set D = {(x1  y1)  . . .   (xn  yn)}  where y ∈ {1  −1} is the class label of
example x  and number M of weak learners in the ﬁnal decision rule.
Initialization: Select uniform weights w(1)
for m = {1  . . .   M } do

i = 1

|D|   ∀i.

end for

compute the gradient step Gm(x) with (30).
update weights wi according to w(m+1)
Output: decision rule h(x) = sgn[PM

i

= w(m)

i × eyiGm(xi).

m=1 Gm(x)].

completely determines the convexity of the conditional risk of (4). Whether φ is itself convex does
not matter.

6 SavageBoost

We have hypothesized that classiﬁers designed with (26) should be more robust than those derived
from the previous φ functions. To test this we designed a boosting algorithm based in the new loss 
using the procedure proposed by Friedman to derive RealBoost [3]. At each iteration the algorithm
searches for the weak learner G(x) which further reduces the conditional risk EY |X[φ(y(f (x) +
G(x)))|X = x] of the current f (x)  for every x ∈ X . The optimal weak learner is

G∗(x) = arg min

G(x)(cid:8)η(x)φw(G(x)) + (1 − η(x))φw(−G(x))(cid:9)

where

and

φw(yG(x)) =

1

(1 + w(x  y)2e2y(G(x)))2

w(x  y) = eyf (x)

(27)

(28)

(29)

The minimization is by gradient descent. Setting the gradient with respect to G(x) to zero results in

G∗(x) =

1

2 (cid:18)log

Pw(y = 1|x)

Pw(y = −1|x)(cid:19)

(30)

where Pw(y = i|x) are probability estimates obtained from the re-weighted training set. At each
iteration the optimal weak learner is found from (30) and reweighing is performed according to (29).
We refer to the algorithm as SavageBoost  and summarize it in the inset.

7 Experimental results

We compared SavageBoost to AdaBoost [9]  RealBoost [3]  and LogitBoost [3]. The latter is gen-
erally considered more robust to outliers [8] and thus a good candidate for comparison. Ten binary
UCI data sets were used: Pima-diabetes  breast cancer diagnostic  breast cancer prognostic  original
Wisconsin breast cancer  liver disorder  sonar  echo-cardiogram  Cleveland heart disease  tic-tac-toe
and Haberman’s survival. We followed the training/testing procedure outlined in [2] to explore the
robustness of the algorithms to outliers. In all cases  ﬁve fold validation was used with varying
levels of outlier contamination. Figure 2 shows the average error of the four methods on the Liver-
Disorder set. Table 4 shows the number of times each method produced the smallest error (#wins)
over the ten data sets at a given contamination level  as well as the average error% over all data
sets (at that contamination level). Our results conﬁrm previous studies that have noted AdaBoost’s
sensitivity to outliers [1]. Among the previous methods AdaBoost indeed performed the worst  fol-
lowed by RealBoost  with LogistBoost producing the best results. This conﬁrms previous reports
that LogitBoost is less sensitive to outliers [8]. SavageBoost produced generally better results than
Ada and RealBoost at all contamination levels  including 0% contamination. LogitBoost achieves

7

Sav. Loss (SavageBoost)
Exp Loss (RealBoost)
Log Loss (LogitBoost)
Exp Loss (AdaBoost)

48

46

44

42

40

38

36

34

32

30

r
o
r
r

E
%

28
0

5

10

15
25
Outlier Percentage

20

30

35

40

Figure 2: Average error for four boosting methods at different contamination levels.

Table 4: (number of wins  average error%) for each method and outlier percentage.

Method

Savage Loss (SavageBoost)

Log Loss(LogitBoost)
Exp Loss(RealBoost)
Exp Loss(AdaBoost)

0% outliers
(4  19.22%)
(4  20.96%)
(2  23.99%)
(0  24.58%)

5% outliers
(4  19.91%)
(4  22.04%)
(2  25.34%)
(0  26.45%)

40% outliers
(6  25.9%)
(3  31.73%)
(0  33.18%)
(1  38.22%)

comparable results at low contamination levels (0%  5%) but has higher error when contamination
is signiﬁcant. With 40% contamination SavageBoost has 6 wins  compared to 3 for LogitBoost
and  on average  about 6% less error. Although  in all experiments  each algorithm was allowed
50 iterations  SavageBoost converged much faster than the others  requiring an average of 25 itera-
tions at 0% cantamination. This is in contrast to 50 iterations for LogitBoost and 45 iterations for
RealBoost. We attribute fast convergence to the bounded nature of the new loss  that prevents so
called ”early stopping” problems [10]. Fast convergence is  of course  a great beneﬁt in terms of the
computational efﬁciency of training and testing. This issue will be studied in greater detail in the
future.

References

[1] T. G. Dietterich  “An experimental comparison of three methods for constructing ensembles of decision

trees: Bagging  boosting  and randomization ” Machine Learning  2000.

[2] Y. Wu and Y. Liu  “Robust truncated-hinge-loss support vector machines ” JASA  2007.

[3] J. Friedman  T. Hastie  and R. Tibshirani  “Additive logistic regression: A statistical view of boosting ”

Annals of Statistics  2000.

[4] T. Zhang  “Statistical behavior and consistency of classiﬁcation methods based on convex risk minimiza-

tion ” Annals of Statistics  2004.

[5] P. Bartlett  M. Jordan  and J. D. McAuliffe  “Convexity  classiﬁcation  and risk bounds ” JASA  2006.

[6] L. J. Savage  “The elicitation of personal probabilities and expectations ” JASA  vol. 66  pp. 783–801 

1971.

[7] S. Boyd and L. Vandenberghe  Convex Optimization. Cambridge: Cambridge University Press  2004.

[8] R. McDonald  D. Hand  and I. Eckley  “An empirical comparison of three boosting algorithms on real

data sets with artiﬁcial class noise ” in International Workshop on Multiple Classiﬁer Systems  2003.

[9] Y. Freund and R. Schapire  “A decision-theoretic generalization of on-line learning and an application to

boosting ” Journal of Computer and System Sciences  1997.

[10] T. Zhang and B. Yu  “Boosting with early stopping: Convergence and consistency ” Annals of Statistics 

2005.

8

,Viet-An Nguyen
Jordan Ying
Philip Resnik