2012,A Geometric take on Metric Learning,Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach  even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is  however  non-metric  which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that  with appropriate changes  multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically  we provide the first practical algorithm for computing geodesics according to the learned metrics  as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together  these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.,A Geometric take on Metric Learning

Søren Hauberg

MPI for Intelligent Systems

T¨ubingen  Germany

Oren Freifeld

Brown University
Providence  US

soren.hauberg@tue.mpg.de

freifeld@dam.brown.edu

Michael J. Black

MPI for Intelligent Systems

T¨ubingen  Germany
black@tue.mpg.de

Abstract

Multi-metric learning techniques learn local metric tensors in different parts of a
feature space. With such an approach  even simple classiﬁers can be competitive
with the state-of-the-art because the distance measure locally adapts to the struc-
ture of the data. The learned distance measure is  however  non-metric  which has
prevented multi-metric learning from generalizing to tasks such as dimensional-
ity reduction and regression in a principled way. We prove that  with appropriate
changes  multi-metric learning corresponds to learning the structure of a Rieman-
nian manifold. We then show that this structure gives us a principled way to
perform dimensionality reduction and regression according to the learned metrics.
Algorithmically  we provide the ﬁrst practical algorithm for computing geodesics
according to the learned metrics  as well as algorithms for computing exponential
and logarithmic maps on the Riemannian manifold. Together  these tools let many
Euclidean algorithms take advantage of multi-metric learning. We illustrate the
approach on regression and dimensionality reduction tasks that involve predicting
measurements of the human body from shape data.

1 Learning and Computing Distances

Statistics relies on measuring distances. When the Euclidean metric is insufﬁcient  as is the case in
many real problems  standard methods break down. This is a key motivation behind metric learning 
which strives to learn good distance measures from data.
In the most simple scenarios a single
metric tensor is learned  but in recent years  several methods have proposed learning multiple metric
tensors  such that different distance measures are applied in different parts of the feature space. This
has proven to be a very powerful approach for classiﬁcation tasks [1  2]  but the approach has not
generalized to other tasks. Here we consider the generalization of Principal Component Analysis
(PCA) and linear regression; see Fig. 1 for an illustration of our approach. The main problem with
generalizing multi-metric learning is that it is based on assumptions that make the feature space both
non-smooth and non-metric. Speciﬁcally  it is often assumed that straight lines form geodesic curves
and that the metric tensor stays constant along these lines. These assumptions are made because it
is believed that computing the actual geodesics is intractable  requiring a discretization of the entire
feature space [3]. We solve these problems by smoothing the transitions between different metric
tensors  which ensures a metric space where geodesics can be computed.
In this paper  we consider the scenario where the metric tensor at a given point in feature space is
deﬁned as the weighted average of a set of learned metric tensors. In this model  we prove that the
feature space becomes a chart for a Riemannian manifold. This ensures a metric feature space  i.e.

dist(x  y) = 0 ⇔ x = y  
dist(x  y) = dist(y  x)
dist(x  z) ≤ dist(x  y) + dist(y  z)

(symmetry) 
(triangle inequality).

(1)

To compute statistics according to the learned metric  we need to be able to compute distances 
which implies that we need to compute geodesics. Based on the observation that geodesics are

1

(a) Local Metrics & Geodesics

(b) Tangent Space Representation

(c) First Principal Geodesic

Figure 1: Illustration of Principal Geodesic Analysis.
(a) Geodesics are computed between the
mean and each data point. (b) Data is mapped to the Euclidean tangent space and the ﬁrst principal
component is computed. (c) The principal component is mapped back to the feature space.

smooth curves in Riemannian spaces  we derive an algorithm for computing geodesics that only
requires a discretization of the geodesic rather than the entire feature space. Furthermore  we show
how to compute the exponential and logarithmic maps of the manifold. With this we can map any
point back and forth between a Euclidean tangent space and the manifold. This gives us a general
strategy for incorporating the learned metric tensors in many Euclidean algorithms: map the data to
the tangent of the manifold  perform the Euclidean analysis and map the results back to the manifold.
Before deriving the algorithms (Sec. 3) we set the scene by an analysis of the shortcomings of current
state-of-the-art methods (Sec. 2)  which motivate our ﬁnal model. The model is general and can be
used for many problems. Here we illustrate it with several challenging problems in 3D body shape
modeling and analysis (Sec. 4). All proofs can be found in the supplementary material along with
algorithmic details and further experimental results.

2 Background and Related Work

Single-metric learning learns a metric tensor  M  such that distances are measured as

dist2(xi  xj) = (cid:107)xi − xj(cid:107)2

M ≡ (xi − xj)T M(xi − xj)  

W SBS−1

(2)
where M is a symmetric and positive deﬁnite D × D matrix. Classic approaches for ﬁnding such a
metric tensor include PCA  where the metric is given by the inverse covariance matrix of the training
data; and linear discriminant analysis (LDA)  where the metric tensor is M = S−1
W   with Sw
and SB being the within class scatter and the between class scatter respectively [9].
A more recent approach tries to learn a metric tensor from triplets of data points (xi  xj  xk)  where
the metric should obey the constraint that dist(xi  xj) < dist(xi  xk). Here the constraints are often
chosen such that xi and xj belong to the same class  while xi and xk do not. Various relaxed ver-
sions of this idea have been suggested such that the metric can be learned by solving a semi-deﬁnite
or a quadratic program [1  2  4–8]. Among the most popular approaches is the Large Margin Near-
est Neighbor (LMNN) classiﬁer [5]  which ﬁnds a linear transformation that satisﬁes local distance
constraints  making the approach suitable for multi-modal classes.
For many problems  a single global metric tensor is not enough  which motivates learning several
local metric tensors. The classic work by Hastie and Tibshirani [9] advocates locally learning metric
tensors according to LDA and using these as part of a kNN classiﬁer. In a somewhat similar fashion 
Weinberger and Saul [5] cluster the training data and learn a separate metric tensor for each cluster
using LMNN. A more extreme point of view was taken by Frome et al. [1  2]  who learn a diagonal
metric tensor for every point in the training set  such that distance rankings are preserved. Similarly 
Malisiewicz and Efros [6] ﬁnd a diagonal metric tensor for each training point such that the distance
to a subset of the training data from the same class is kept small.
Once a set of metric tensors {M1  . . .   MR} has been learned  the distance dist(a  b) is measured
according to (2) where “the nearest” metric tensor is used  i.e.

(cid:26) 1

0

(cid:107)x − xr(cid:107)2
otherwise

Mr

≤ (cid:107)x − xj(cid:107)2

Mj

 ∀j

 

(3)

R(cid:88)

r=1

˜wr(x)(cid:80)

j ˜wj(x)

M(x) =

Mr   where ˜wr(x) =

where x is either a or b depending on the algorithm. Note that this gives a non-metric distance
function as it is not symmetric. To derive this equation  it is necessary to assume that 1) geodesics

2

(a)

(b)

(c)

(d)

Figure 2: (a)–(b) An illustrative example where straight lines do not form geodesics and where
the metric tensor does not stay constant along lines; see text for details. The background color
is proportional to the trace of the metric tensor  such that light grey corresponds to regions where
paths are short (M1)  and dark grey corresponds to regions they are long (M2). (c) The suggested
geometric model along with the geodesics. Again  background colour is proportional to the trace
of the metric tensor; the colour scale is the same is used in (a) and (b). (d) An illustration of the
exponential and logarithmic maps.

form straight lines  and 2) the metric tensor stays constant along these lines [3]. Both assumptions
are problematic  which we illustrate with a simple example in Fig. 2a–c.
Assume we are given two metric tensors M1 = 2I and M2 = I positioned at x1 = (2  2)T and
x2 = (4  4)T respectively. This gives rise to two regions in feature space in which x1 is nearest
in the ﬁrst and x2 is nearest in the second  according to (3). This is illustrated in Fig. 2a. In the
same ﬁgure  we also show the assumed straight-line geodesics between selected points in space. As
can be seen  two of the lines goes through both regions  such that the assumption of constant metric
tensors along the line is violated. Hence  it would seem natural to measure the length of the line 
by adding the length of the line segments which pass through the different regions of feature space.
This was suggested by Ramanan and Baker [3] who also proposed a polynomial time algorithm for
measuring these line lengths. This gives a symmetric distance function.
Properly computing line lengths according to the local metrics is  however  not enough to ensure that
the distance function is metric. As can be seen in Fig. 2a the straight line does not form a geodesic
as a shorter path can be found by circumventing the region with the “expensive” metric tensor M1
as illustrated in Fig. 2b. This issue makes it trivial to construct cases where the triangle inequality is
violated  which again makes the line length measure non-metric.
In summary  if we want a metric feature space  we can neither assume that geodesics are straight
lines nor that the metric tensor stays constant along such lines. In practice  good results have been
reported using (3) [1 3 5]  so it seems obvious to ask: is metricity required? For kNN classiﬁers this
does not appear to be the case  with many successes based on dissimilarities rather than distances
[10]. We  however  want to generalize PCA and linear regression  which both seek to minimize the
reconstruction error of points projected onto a subspace. As the notion of projection is hard to deﬁne
sensibly in non-metric spaces  we consider metricity essential.
In order to build a model with a metric feature space  we change the weights in (3) to be smooth
functions. This impose a well-behaved geometric structure on the feature space  which we take
advantage of in order to perform statistical analysis according to the learned metrics. However  ﬁrst
we review the basics of Riemannian geometry as this provides the theoretical foundation of our
work.

2.1 Geodesics and Riemannian Geometry

We start by deﬁning Riemannian manifolds  which intuitively are smoothly curved spaces equipped
with an inner product. Formally  they are smooth manifolds endowed with a Riemannian metric [11]:
Deﬁnition A Riemannian metric M on a manifold M is a smoothly varying inner product
< a  b >x= aT M(x)b in the tangent space TxM of each point x ∈ M .

3

  −8−6−4−20246−8−6−4−20246Assumed GeodesicsLocation of Metric TensorsTest Points  −8−6−4−20246−8−6−4−20246Actual GeodesicsLocation of Metric TensorsTest Points  −8−6−4−20246−8−6−4−20246Riemannian GeodesicsLocation of Metric TensorsTest PointsOften Riemannian manifolds are represented by a chart; i.e. a parameter space for the curved sur-
face. An example chart is the spherical coordinate system often used to represent spheres. While
such charts are often ﬂat spaces  the curvature of the manifold arises from the smooth changes in the
metric.
On a Riemannian manifold M  the length of a smooth curve c : [0  1] → M is deﬁned as the
integral of the norm of the tangent vector (interpreted as speed) along the curve:

(cid:107)c(cid:48)(λ)(cid:107)M(c(λ))dλ =

Length(c) =

(4)
where c(cid:48) denotes the derivative of c and M(c(λ)) is the metric tensor at c(λ). A geodesic curve is
then a length-minimizing curve connecting two given points x and y  i.e.

(cid:0)Length(c)(cid:1) with c(0) = x and c(1) = y .

cgeo = arg min

c(cid:48)(λ)T M(c(λ))c(cid:48)(λ)dλ  

(5)

0

0

(cid:90) 1

(cid:113)

(cid:90) 1

c

The distance between x and y is deﬁned as the length of the geodesic.
Given a tangent vector v ∈ TxM  there exists a unique geodesic cv(t) with initial velocity v at x.
The Riemannian exponential map  Expx  maps v to a point on the manifold along the geodesic cv
at t = 1. This mapping preserves distances such that dist(cv(0)  cv(1)) = (cid:107)v(cid:107). The inverse of
the exponential map is the Riemannian logarithmic map denoted Logx. Informally  the exponential
and logarithmic maps move points back and forth between the manifold and the tangent space while
preserving distances (see Fig. 2d for an illustration). This provides a general strategy for generalizing
many Euclidean techniques to Riemannian domains: data points are mapped to the tangent space 
where ordinary Euclidean techniques are applied and the results are mapped back to the manifold.

3 A Metric Feature Space
With the preliminaries settled we deﬁne the new model. Let C = RD denote the feature space. We
endow C with a metric tensor in every point x  which we deﬁne akin to (3) 
˜wr(x)
j=1 ˜wj(x)

wr(x)Mr   where wr(x) =

(cid:80)R

R(cid:88)

M(x) =

(6)

r=1

 

with ˜wr > 0. The only difference from (3) is that we shall not restrict ourselves to binary weight
functions ˜wr. We assume the metric tensors Mr have already been learned; Sec. 4 contain examples
where they have been learned using LMNN [5] and LDA [9].
From the deﬁnition of a Riemannian metric  we trivially have the following result:
Lemma 1 The space C = RD endowed with the metric tensor from (6) is a chart of a Riemannian
manifold  iff the weights wr(x) change smoothly with x.
Hence  by only considering smooth weight functions ˜wr we get a well-studied geometric structure
on the feature space  which ensures us that it is metric. To illustrate the implications we return to the
example in Fig. 2. We change the weight functions from binary to squared exponentials  which gives
the feature space shown in Fig. 2c. As can be seen  the metric tensor now changes smoothly  which
also makes the geodesics smooth curves (a property we will use when computing the geodesics).
It is worth noting that Ramanan and Baker [3] also consider the idea of smoothly averaging the
metric tensor. They  however  only evaluate the metric tensor at the test point of their classiﬁer
and then assume straight line geodesics with a constant metric tensor. Such assumptions violate the
premise of a smoothly changing metric tensor and  again  the distance measure becomes non-metric.
Lemma 1 shows that metric learning can be viewed as manifold learning. The main difference be-
tween our approach and techniques such as Isomap [12] is that  while Isomap learns an embedding
of the data points  we learn the actual manifold structure. This gives us the beneﬁt that we can
compute geodesics as well as the exponential and logarithmic maps. These provide us with map-
pings back and forth between the manifold and Euclidean representation of the data  which preserve
distances as well as possible. The availability of such mappings is in stark contrast to e.g. Isomap.
In the next section we will derive a system of ordinary differential equations (ODE’s) that geodesics
in C have to satisfy  which provides us with algorithms for computing geodesics as well as exponen-
tial and logarithmic maps. With these we can generalize many Euclidean techniques.

4

3.1 Computing Geodesics  Maps and Statistics

At minima of (4) we know that the Euler-Lagrange equation must hold [11]  i.e.
  where L(λ  c  c(cid:48)) = c(cid:48)(λ)T M(c(λ))c(cid:48)(λ) .

=

∂L
∂c

d
dλ

∂L
∂c(cid:48)

(7)

As we have an explicit expression for the metric tensor we can compute (7) in closed form:
Theorem 2 Geodesic curves in C satisfy the following system of 2nd order ODE’s
(c(cid:48)(λ) ⊗ c(cid:48)(λ))  

(cid:20) ∂vec [M(c(λ))]

(8)
where ⊗ denotes the Kronecker product and vec [·] stacks the columns of a matrix into a vector [13].

M(c(λ))c(cid:48)(cid:48)(λ) = − 1
2

(cid:21)T

∂c(λ)

Proof See supplementary material.

(cid:3)

This result holds for any smooth weight functions ˜wr. We  however  still need to compute ∂vec[M]
 
which depends on the speciﬁc choice of ˜wr. Any smooth weighting scheme is applicable  but we
restrict ourselves to the obvious smooth generalization of (3) and use squared exponentials. From
this assumption  we get the following result

∂c

Theorem 3 For ˜wr(x) = exp(cid:0)− ρ
2(cid:107)x − xr(cid:107)2
R(cid:88)
(cid:17)2

∂vec [M(c)]

(cid:16)(cid:80)R

∂c

=

ρ

˜wrvec [Mr]

Mr

(cid:1) the derivative of the metric tensor from (6) is
(cid:17)
R(cid:88)

(c − xj)T Mj − (c − xr)T Mr

(cid:16)

˜wj

.

j=1 ˜wj

r=1

j=1

(9)

(cid:3)

Proof See supplementary material.

Computing Geodesics. Any geodesic curve must be a solution to (8). Hence  to compute a
geodesic between x and y  we can solve (8) subject to the constraints

c(0) = x and c(1) = y .

(10)
This is a boundary value problem  which has a smooth solution. This allows us to solve the prob-
lem numerically using a standard three-stage Lobatto IIIa formula  which provides a fourth-order
accurate C 1–continuous solution [14].
Ramanan and Baker [3] discuss the possibility of computing geodesics  but arrive at the conclusion
that this is intractable based on the assumption that it requires discretizing the entire feature space.
Our solution avoids discretizing the feature space by discretizing the geodesic curve instead. As this
is always one-dimensional the approach remains tractable in high-dimensional feature spaces.

Computing Logarithmic Maps. Once a geodesic c is found  it follows from the deﬁnition of the
logarithmic map  Logx(y)  that it can be computed as
c(cid:48)(0)
(cid:107)c(cid:48)(0)(cid:107) Length(c) .

v = Logx(y) =

(11)

In practice  we solve (8) by rewriting it as a system of ﬁrst order ODE’s  such that we compute both
c and c(cid:48) simultaneously (see supplementary material for details).

Computing Exponential Maps. Given a starting point x on the manifold and a vector v in the
tangent space  the exponential map  Expx(v)  ﬁnds the unique geodesic starting at x with initial
velocity v. As the geodesic must fulﬁll (8)  we can compute the exponential map by solving this
system of ODE’s with the initial conditions

(12)
This initial value problem has a unique solution  which we ﬁnd numerically using a standard Runge-
Kutta scheme [15].

c(0) = x and c(cid:48)(0) = v .

5

3.1.1 Generalizing PCA and Regression

At this stage  we know that the feature space is Riemannian and we know how to compute geodesics
and exponential and logarithmic maps. We now seek to generalize PCA and linear regression 
which becomes straightforward since solutions are available in Riemannian spaces [16  17]. These
generalizations can be summarized as mapping the data to the tangent space at the mean  performing
standard Euclidean analysis in the tangent and mapping the results back.
The ﬁrst step is to compute the mean value on the manifold  which is deﬁned as the point that
minimizes the sum-of-squares distances to the data points. Pennec [18] provides an efﬁcient gradient
descent approach for computing this point  which we also summarize in the supplementary material.
The empirical covariance of a set of points is deﬁned as the ordinary Euclidean covariance in the
tangent space at the mean value [18]. With this in mind  it is not surprising that the principal
components of a dataset have been generalized as the geodesics starting at the mean with initial
velocity corresponding to the eigenvectors of the covariance [16] 

γvd (t) = Expµ(tvd)  

(13)

where vd denotes the dth eigenvector of the covariance. This approach is called Principal Geodesic
Analysis (PGA)  and the geodesic curve γvd is called the principal geodesic. An illustration of the
approach can be seen in Fig. 1 and more algorithmic details are in the supplementary material.
Linear regression has been generalized in a similar way [17] by performing regression in the tangent
of the mean and mapping the resulting line back to the manifold using the exponential map.
The idea of working in the tangent space is both efﬁcient and convenient  but comes with an element
of approximation as the logarithmic map is only guarantied to preserve distances to the origin of the
tangent and not between all pairs of data points. Practical experience  however  indicates that this is
a good tradeoff; see [19] for a more in-depth discussion of when the approximation is suitable.

4 Experiments

To illustrate the framework1 we consider an example in human body analysis  and then we analyze
the scalability of the approach. But ﬁrst  to build intuition  Fig. 3a show synthetically generated
data samples from two classes. We sample random points xr and learn a local LDA metric [9] by
considering all data points within a radius; this locally pushes the two classes apart. We combine the
local metrics using (6) and Fig. 3b show the data in the tangent space of the resulting manifold. As
can be seen the two classes are now globally further apart  which shows the effect of local metrics.

4.1 Human Body Shape

We consider a regression example concerning human body shape analysis. We study 986 female
body laser scans from the CAESAR [20] data set; each shape is represented using the leading 35
principal components of the data learned using a SCAPE-like model [21  22]. Each shape is asso-
ciated with anthropometric measurements such as body height  shoe size  etc. We show results for
shoulder to wrist distance and shoulder breadth  but results for more measurements are in the sup-
plementary material. To predict the measurements from shape coefﬁcients  we learn local metrics
and perform linear regression according to these. As a further experiment  we use PGA to reduce
the dimensionality of the shape coefﬁcients according to the local metrics  and measure the quality
of the reduction by performing linear regression to predict the measurements. As a baseline we use
the corresponding Euclidean techniques.
To learn the local metric we do the following. First we whiten the data such that the variance
captured by PGA will only be due to the change of metric; this allows easy visualization of the
impact of the learned metrics. We then cluster the body shapes into equal-sized clusters according
to the measurement and learn a LMNN metric for each cluster [5]  which we associate with the
mean of each class. These push the clusters apart  which introduces variance along the directions
where the measurement changes. From this we construct a Riemannian manifold according to (6) 

1Our software implementation for computing geodesics and performing manifold statistics is available at

http://ps.is.tue.mpg.de/project/Smooth Metric Learning

6

(a)

(b)

Figure 3: Left panels: Synthetic data.
(a) Samples from two classes along with illustratively
sampled metric tensors from (6). (b) The data represented in the tangent of a manifold constructed
from local LDA metrics learned at random positions. Right panels: Real data.
(c) Average error
of linearly predicted body measurements (mm). (d) Running time (sec) of the geodesic computation
as a function of dimensionality.

(c)

(d)

e
c
n
a
t
s
i

d
t
s
i
r
w
o
t

l

r
e
d
u
o
h
S

h
t
d
a
e
r
b
r
e
d

l

u
o
h
S

Euclidean PCA

Tangent Space PCA (PGA)

Regression Error

Figure 4: Left: body shape data in the ﬁrst two principal components according to the Euclidean
metric. Point color indicates cluster membership. Center: As on the left  but according to the
Riemannian model. Right: regression error as a function of the dimensionality of the shape space;
again the Euclidean metric and the Riemannian metric are compared.

compute the mean value on the manifold  map the data to the tangent space at the mean and perform
linear regression in the tangent space.
As a ﬁrst visualization we plot the data expressed in the leading two dimensions of PGA in Fig. 4; as
can be seen the learned metrics provide principal geodesics  which are more strongly related with the
measurements than the Euclidean model. In order to predict the measurements from the body shape 
we perform linear regression  both directly in the shape space according to the Euclidean metric
and in the tangent space of the manifold corresponding to the learned metrics (using the logarithmic
map from (11)). We measure the prediction error using leave-one-out cross-validation. To further
illustrate the power of the PGA model  we repeat this experiment for different dimensionalities of
the data. The results are plotted in Fig. 4  showing that regression according to the learned metrics
outperforms the Euclidean model.
To verify that the learned metrics improve accuracy  we average the prediction errors over all mil-
limeter measurements. The result in Fig. 3c shows that much can be gained in lower dimensions by
using the local metrics.
To provide visual insights into the behavior of the learned metrics  we uniformly sample body shape
along the ﬁrst principal geodesic (in the range ±7 times the standard deviation) according to the
different metrics. The results are available as a movie in the supplementary material  but are also
shown in Fig. 5. As can be seen  the learned metrics pick up intuitive relationships between body
shape and the measurements  e.g. shoulder to wrist distance is related to overall body size  while
shoulder breadth is related to body weight.

7

01020301214161820222426DimensionalityAverage Prediction Error  Euclidean ModelRiemannian Model050100150051015202530DimensionalityRunning Time (sec.)−4−3−2−10123−4−3−2−101234−505−4−3−2−1012340510152025303510152025DimensionalityPrediction Error  Euclidean ModelRiemannian Model−4−3−2−10123−4−3−2−101234−4−20246−6−5−4−3−2−1012340510152025303591011121314151617DimensionalityPrediction Error  Euclidean ModelRiemannian ModelShoulder to wrist distance

Shoulder breadth

Figure 5: Shapes corresponding to the mean (center) and ±7 times the standard deviations along the
principal geodesics (left and right). Movies are available in the supplementary material.

4.2 Scalability

The human body data set is small enough (986 samples in 35 dimensions) that computing a geodesic
only takes a few seconds. To show that the current unoptimized Matlab implementation can handle
somewhat larger datasets  we brieﬂy consider a dimensionality reduction task on the classic MNIST
handwritten digit data set. We use the preprocessed data available with [3] where the original 28×28
gray scale images were deskewed and projected onto their leading 164 Euclidean principal compo-
nents (which captures 95% of the variance in the original data).
We learn one diagonal LMNN metric per class  which we associate with the mean of the class. From
this we construct a Riemannian manifold from (6)  compute the mean value on the manifold and
compute geodesics between the mean and each data point; this is the computationally expensive part
of performing PGA. Fig. 3d plots the average running time (sec) for the computation of geodesics as
a function of the dimensionality of the training data. A geodesic can be computed in 100 dimensions
in approximately 5 sec.  whereas in 150 dimensions it takes about 30 sec.
In this experiment  we train a PGA model on 60 000 data points  and test a nearest neighbor classiﬁer
in the tangent space as we decrease the dimensionality of the model. Compared to a Euclidean
model  this gives a modest improvement in classiﬁcation accuracy of 2.3 percent  when averaged
across different dimensionalities. Plots of the results can be found in the supplementary material.

5 Discussion

This work shows that multi-metric learning techniques are indeed applicable outside the realm of
kNN classiﬁers. The idea of deﬁning the metric tensor at any given point as the weighted average of
a ﬁnite set of learned metrics is quite natural from a modeling point of view  which is also validated
by the Riemannian structure of the resulting space. This opens both a theoretical and a practical
toolbox for analyzing and developing algorithms that use local metric tensors. Speciﬁcally  we
show how to use local metric tensors for both regression and dimensionality reduction tasks.
Others have attempted to solve non-classiﬁcation problems using local metrics  but we feel that our
approach is the ﬁrst to have a solid theoretical backing. For example  Hastie and Tibshirani [9] use
local LDA metrics for dimensionality reduction by averaging the local metrics and using the result-
ing metric as part of a Euclidean PCA  which essentially is a linear approach. Another approach
was suggested by Hong et al. [23] who simply compute the principal components according to each
metric separately  such that one low dimensional model is learned per metric.
The suggested approach is  however  not difﬁculty-free in its current implementation. Currently  we
are using off-the-shelf numerical solvers for computing geodesics  which can be computationally
demanding. While we managed to analyze medium-sized datasets  we believe that the run-time can
be drastically improved by developing specialized numerical solvers.
In the experiments  we learned local metrics using techniques specialized for classiﬁcation tasks as
this is all the current literature provides. We expect improvements by learning the metrics speciﬁ-
cally for regression and dimensionality reduction  but doing so is currently an open problem.

Acknowledgments: Søren Hauberg is supported in part by the Villum Foundation  and Oren Freifeld is
supported in part by NIH-NINDS EUREKA (R01-NS066311).

8

References
[1] Andrea Frome  Yoram Singer  and Jitendra Malik. Image retrieval and classiﬁcation using local distance
functions. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information Processing
Systems 19 (NIPS)  pages 417–424  Cambridge  MA  2007. MIT Press.

[2] Andrea Frome  Fei Sha  Yoram Singer  and Jitendra Malik. Learning globally-consistent local distance
functions for shape-based image retrieval and classiﬁcation. In International Conference on Computer
Vision (ICCV)  pages 1–8  2007.

[3] Deva Ramanan and Simon Baker. Local distance functions: A taxonomy  new algorithms  and an evalua-

tion. IEEE Transactions on Pattern Analysis and Machine Intelligence  33(4):794–806  2011.

[4] Shai Shalev-Shwartz  Yoram Singer  and Andrew Y. Ng. Online and batch learning of pseudo-metrics. In
Proceedings of the twenty-ﬁrst international conference on Machine learning  ICML ’04  pages 94–101.
ACM  2004.

[5] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. The Journal of Machine Learning Research  10:207–244  2009.

[6] Tomasz Malisiewicz and Alexei A. Efros. Recognition by association via learning per-exemplar distances.

In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 1–8  2008.

[7] Yiming Ying and Peng Li. Distance metric learning with eigenvalue optimization. The Journal of Machine

Learning Research  13:1–26  2012.

[8] Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative comparisons.

Advances in Neural Information Processing Systems 16 (NIPS)  2004.

In

[9] Trevor Hastie and Robert Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Trans-

actions on Pattern Analysis and Machine Intelligence  18(6):607–616  June 1996.

[10] Elzbieta Pekalska  Pavel Paclik  and Robert P. W. Duin. A generalized kernel approach to dissimilarity-

based classiﬁcation. Journal of Machine Learning Research  2:175–211  2002.

[11] Manfredo Perdigao do Carmo. Riemannian Geometry. Birkh¨auser Boston  January 1992.
[12] Joshua B. Tenenbaum  Vin De Silva  and John C. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  2000.

[13] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and

Econometrics. John Wiley & Sons  2007.

[14] Jacek Kierzenka and Lawrence F. Shampine. A BVP solver based on residual control and the Matlab

PSE. ACM Transactions on Mathematical Software  27(3):299–316  2001.

[15] John R. Dormand and P. J. Prince. A family of embedded Runge-Kutta formulae. Journal of Computa-

tional and Applied Mathematics  6:19–26  1980.

[16] P. Thomas Fletcher  Conglin Lu  Stephen M. Pizer  and Sarang Joshi. Principal Geodesic Analysis for the
study of Nonlinear Statistics of Shape. IEEE Transactions on Medical Imaging  23(8):995–1005  2004.
[17] Peter E. Jupp and John T. Kent. Fitting smooth paths to spherical data. Applied Statistics  36(1):34–46 

1987.

[18] Xavier Pennec. Probabilities and statistics on Riemannian manifolds: Basic tools for geometric measure-

ments. In Proceedings of Nonlinear Signal and Image Processing  pages 194–198  1999.

[19] Stefan Sommer  Franc¸ois Lauze  Søren Hauberg  and Mads Nielsen. Manifold valued statistics  exact
principal geodesic analysis and the effect of linear approximations. In European Conference on Computer
Vision (ECCV)  pages 43–56  2010.

[20] Kathleen M. Robinette  Hein Daanen  and Eric Paquet. The CAESAR project: a 3-D surface anthropom-

etry survey. In 3-D Digital Imaging and Modeling  pages 380–386  1999.

[21] Dragomir Anguelov  Praveen Srinivasan  Daphne Koller  Sebastian Thrun  Jim Rodgers  and James Davis.
Scape: shape completion and animation of people. ACM Transactions on Graphics  24(3):408–416  2005.
[22] Oren Freifeld and Michael J. Black. Lie bodies: A manifold representation of 3D human shape. In A.
Fitzgibbon et al. (Eds.)  editor  European Conference on Computer Vision (ECCV)  Part I  LNCS 7572 
pages 1–14. Springer-Verlag  oct 2012.

[23] Yi Hong  Quannan Li  Jiayan Jiang  and Zhuowen Tu. Learning a mixture of sparse distance metrics
for classiﬁcation and dimensionality reduction. In International Conference on Computer Vision (ICCV) 
pages 906–913  2011.

9

,Tengyu Ma
Avi Wigderson
Pan Zhou
Xiaotong Yuan
Jiashi Feng