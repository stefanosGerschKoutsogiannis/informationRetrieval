2014,On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures,We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points  such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures  there is limited understanding of the theoretical properties of these algorithms. Recently  Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure  but their results apply only to an idealized setting  where precise knowledge of the underlying probability distribution (in the form of the `true' posterior class probability) is available to a learning algorithm. In this work  we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable `estimate' of the class probability  and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR)  and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge  these are the first such results for these measures. In addition  for continuous distributions  we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.,On the Statistical Consistency of Plug-in Classiﬁers

for Non-decomposable Performance Measures

Harikrishna Narasimhan†  Rohit Vaish†  Shivani Agarwal

Department of Computer Science and Automation

{harikrishna  rohit.vaish  shivani}@csa.iisc.ernet.in

Indian Institute of Science  Bangalore – 560012  India

Abstract

We study consistency properties of algorithms for non-decomposable performance
measures that cannot be expressed as a sum of losses on individual data points 
such as the F-measure used in text retrieval and several other performance mea-
sures used in class imbalanced settings. While there has been much work on
designing algorithms for such performance measures  there is limited understand-
ing of the theoretical properties of these algorithms. Recently  Ye et al. (2012)
showed consistency results for two algorithms that optimize the F-measure  but
their results apply only to an idealized setting  where precise knowledge of the
underlying probability distribution (in the form of the ‘true’ posterior class prob-
ability) is available to a learning algorithm.
In this work  we consider plug-in
algorithms that learn a classiﬁer by applying an empirically determined threshold
to a suitable ‘estimate’ of the class probability  and provide a general methodology
to show consistency of these methods for any non-decomposable measure that can
be expressed as a continuous function of true positive rate (TPR) and true nega-
tive rate (TNR)  and for which the Bayes optimal classiﬁer is the class probability
function thresholded suitably. We use this template to derive consistency results
for plug-in algorithms for the F-measure and for the geometric mean of TPR and
precision; to our knowledge  these are the ﬁrst such results for these measures. In
addition  for continuous distributions  we show consistency of plug-in algorithms
for any performance measure that is a continuous and monotonically increasing
function of TPR and TNR. Experimental results conﬁrm our theoretical ﬁndings.

1

Introduction

In many real-world applications  the performance measure used to evaluate a learning model is
non-decomposable and cannot be expressed as a summation or expectation of losses on individual
data points; this includes  for example  the F-measure used in information retrieval [1]  and several
combinations of the true positive rate (TPR) and true negative rate (TNR) used in class imbalanced
classiﬁcation settings [2–5] (see Table 1). While there has been much work in the last two decades
in designing learning algorithms for such performance measures [6–14]  our understanding of the
statistical consistency of these methods is rather limited. Recently  Ye et al. (2012) showed consis-
tency results for two algorithms for the F-measure [15] that use the ‘true’ posterior class probability
to make predictions on instances. These results implicitly assume that the given learning algorithm
has precise knowledge of the underlying probability distribution (in the form of the true posterior
class probability); this assumption does not however hold in most real-world settings.
In this paper  we consider a family of methods that construct a plug-in classiﬁer by applying an
empirically determined threshold to a suitable ‘estimate’ of the class probability (obtained using a
model learned from a sample drawn from the underlying distribution). We provide a general method-

†Both authors contributed equally to this paper.

1

Table 1: Performance measures considered in our study. Here β ∈ (0 ∞) and p = P(y = 1).
Each performance measure here can be expressed as P Ψ
D[h] = Ψ(TPRD[h]  TNRD[h]  p). The last
column contains the assumption on the distribution D under which the plug-in algorithm considered
in this work is statistically consistent w.r.t. a performance measure (details in Sections 3 and 5).
Measure
AM (1-BER)
Fβ-measure
G-TP/PR
G-Mean (GM)
H-Mean (HM)
Q-Mean (QM)

Ref.
[17–19] u+v
2
[1  19]
[3]
[2  3]
[4]
1 − ((1 − TPR)2 + (1 − TNR)2)/2 [5]

Assumption on D
Assumption A
p+β2(pu+(1−p)(1−v)) Assumption A
Assumption A
√
Assumption B
Assumption B
Assumption B

pu+(1−p)(1−v)
uv
2uv
u+v

1 − (1−u)2+(1−v)2

Prec + 1
TPR

Ψ(u  v  p)

Deﬁnition
(TPR + TNR)/2

(1 + β2)/(cid:0) β2
2/(cid:0) 1

TPR · Prec
TPR · TNR
TPR + 1
TNR

√
√

(cid:1)

(cid:1)

(cid:113)

(1+β2)pu

pu2

2

ology to show statistical consistency of these methods (under a mild assumption on the underlying
distribution) for any performance measure that can be expressed as a continuous function of the TPR
and TNR and the class proportion  and for which the Bayes optimal classiﬁer is the class probability
function thresholded at a suitable point. We use our proof template to derive consistency results for
the F-measure (using a recent result by [15] on the Bayes optimal classiﬁer for F-measure)  and the
geometric mean of TPR and precision; to our knowledge  these are the ﬁrst such results for these
performance measures. Using our template  we also obtain a recent consistency result by Menon et
al. [16] for the arithmetic mean of TPR and TNR. In addition  we show that for continuous distri-
butions  the optimal classiﬁer for any performance measure that is a continuous and monotonically
increasing function of TPR and TNR is necessarily of the requisite thresholded form  thus establish-
ing consistency of the plug-in algorithms for all such performance measures. Experiments on real
and synthetic data conﬁrm our theoretical ﬁndings  and show that the plug-in methods considered
here are competitive with the state-of-the-art SVMperf method [12] for non-decomposable measures.
Related Work. Much of the work on non-decomposable performance measures in binary classiﬁ-
cation settings has focused on the F-measure; these include the empirical plug-in algorithm consid-
ered here [6]  cost-weighted versions of SVM [9]  methods that optimize convex and non-convex
approximations to F-measure [10–14]  and decision-theoretic methods that learn a class probability
estimate and compute predictions that maximize the expected F-measure on a test set [7–9]. While
there has been considerable amount of work on consistency of algorithms for univariate performance
measures [16  20–22]  theoretical results on non-decomposable measures have been limited to char-
acterizing the Bayes optimal classiﬁer for F-measure [15  23  24]  and some consistency results for
F-measure for certain idealized versions of the empirical plug-in and decision theoretic methods
that have access to the true class probability [15]. There has also been some work on algorithms that
optimize F-measure in multi-label classiﬁcation settings [25  26] and consistency results for these
methods [26  27]  but these results do not apply to the binary classiﬁcation setting that we consider
here; in particular  in a binary classiﬁcation setting  the F-measure that one seeks to optimize is a
single number computed over the entire training set  while in a multi-label setting  the goal is to
optimize the mean F-measure computed over multiple labels on individual instances.
Organization. We start with some preliminaries in Section 2. Section 3 presents our main result
on consistency of plug-in algorithms for non-decomposable performance measures that are func-
tions of TPR and TNR. Section 4 contains application of our proof template to the AM  Fβ and
G-TP/PR measures  and Section 5 contains results under continuous distributions for performance
measures that are monotonic in TPR and TNR. Section 6 describes our experimental results on real
and synthetic data sets. Proofs not provided in the main text can be found in the Appendix.

Let X be any instance space.

2 Preliminaries
Problem Setup.
Given a training sample S =
to make predictions for new instances drawn from X . Assume all examples (both training and
test) are drawn iid according to some unknown probability distribution D on X × {±1}. Let
η(x) = P(y = 1|x) and p = P(y = 1) (both under D). We will be interested in settings where the

((x1  y1)  . . .   (xn  yn)) ∈ (X × {±1})n  our goal is to learn a binary classiﬁer(cid:98)hS : X →{±1}
performance of(cid:98)hS is measured via a non-decomposable performance measure P : {±1}X → R+ 

which cannot be expressed as a sum or expectation of losses on individual examples.

2

pTPRD[h]

Non-decomposable performance measures. Let us ﬁrst deﬁne the following quantities associated
with a binary classiﬁer h : X →{±1}:
True Positive Rate / Recall
True Negative Rate
Precision

TPRD[h] = P(cid:0)h(x) = 1| y = 1(cid:1)
TNRD[h] = P(cid:0)h(x) = −1| y = −1(cid:1)
PrecD[h] = P(cid:0)y = 1| h(x) = 1(cid:1) =

pTPRD[h]+(1−p)(1−TNRD[h]).
In this paper  we will consider non-decomposable performance measures that can be expressed as a
function of the TPR and TNR and the class proportion p. Speciﬁcally  let Ψ : [0  1]3 → R+; then the
Ψ-performance of h w.r.t. D  which we will denote as P Ψ

D[h]  will be deﬁned as:

(cid:16) β2

P Ψ
D[h] = Ψ(TPRD[h]  TNRD[h]  p).
for β > 0 
[0  1]3 → R+ given by ΨFβ (u  v  p) =

the Fβ-measure of h can be deﬁned through the func-
For example 
p+β2(pu+(1−p)(1−v))  which gives
tion ΨFβ
:
P Fβ
. Table 1 gives several examples of non-decomposable
D [h] = (1 + β2)/
performance measures that are used in practice. We will also ﬁnd it useful to consider empirical ver-
S [h]:
(1)

sions of these performance measures calculated from a sample S  which we will denote as (cid:98)P Ψ
(cid:80)n
where(cid:98)pS = 1
n(cid:88)
(cid:100)TPRS[h] =
1(cid:98)pSn

(cid:98)P Ψ
S [h] = Ψ((cid:100)TPRS[h]  (cid:100)TNRS[h]  (cid:98)pS) 
(1 −(cid:98)pS)n

i=1 1(yi = 1) is an empirical estimate of p  and
1

1(h(xi) = 1  yi = 1); (cid:100)TNRS[h] =

1(h(xi) = −1  yi = −1)

n(cid:88)

PrecD[h] +

(1+β2)pu

1

TPRD[h]

(cid:17)

n

i=1

i=1

are the empirical TPR and TNR respectively.1
Ψ-consistency. We will be interested in the optimum value of P Ψ

D over all classiﬁers:

P Ψ ∗
D =

sup

h:X → {±1}

P Ψ
D[h].

regretΨ

D[h] = P Ψ ∗

A learning algorithm is then said to be Ψ-consistent if the Ψ-regret of the classiﬁer(cid:98)hS output by the

In particular  one can deﬁne the Ψ-regret of a classiﬁer h as:
D − P Ψ
D[(cid:98)hS] P−→ 0.

algorithm on seeing training sample S converges in probability to 0:2

Class of Threshold Classiﬁers. We will ﬁnd it useful to deﬁne for any function f : X → [0  1] 
the set of classiﬁers obtained by assigning a threshold to f: Tf = {sign ◦ (f − t)| t ∈ [0  1]} 
where sign(u) = 1 if u > 0 and −1 otherwise. For a given f  we shall also deﬁne the thresholds
corresponding to maximum population and empirical measures respectively (when they exist) as:

regretΨ

D[h].

(cid:98)tS f Ψ ∈ argmax

t∈[0 1]

(cid:98)P Ψ
S [sign ◦ (f − t)].

D f Ψ ∈ argmax
t∗
t∈[0 1]

P Ψ
D[sign ◦ (f − t)];

Plug-in Algorithms and Result of Ye et al. (2012). In this work  we consider a family of plug-in
algorithms  which divide the input sample S into samples (S1  S2)  use a suitable class probability

estimation (CPE) algorithm to learn a class probability estimator (cid:98)ηS1 : X → [0  1] from S1  and
output a classiﬁer(cid:98)hS(x) = sign((cid:98)ηS1(x)−(cid:98)tS2 (cid:98)ηS1  Ψ)  where(cid:98)tS2 (cid:98)ηS1  Ψ is a threshold that maximizes

the empirical performance measure on S2 (see Algorithm 1). We note that this approach is differ-
ent from the idealized plug-in method analyzed by Ye et al. (2012) in the context of F-measure
optimization  where a classiﬁer is learned by assigning an empirical threshold to the ‘true’ class
probability function η [15]; the consistency result therein is useful only if precise knowledge of η is
available to a learning algorithm  which is not the case in most practical settings.
L1-consistency of a CPE algorithm. Let C be a CPE algorithm  and for any sample S  denote

(cid:98)ηS = C(S). We will say C is L1-consistent w.r.t. a distribution D if Ex

(cid:2)(cid:12)(cid:12)(cid:98)ηS(x) − η(x)(cid:12)(cid:12)(cid:3) P−→ 0.

1In the setting considered here  the goal is to maximize a (non-decomposable) function of expectations; we
note that this is different from the decision-theoretic setting in [15]  where one looks at the expectation of a
non-decomposable performance measure on n examples  and seeks to maximize its limiting value as n→∞.
if ∀ > 0 
PS∼Dn (|φ(S) − a| ≥ ) → 0 as n → ∞.

2We say φ(S) converges in probability to a ∈ R  written as φ(S)

P−→ a 

3

Algorithm 1 Plug-in with Empirical Threshold for Performance Measure P Ψ : 2X → R+
1: Input: S = ((x1  y1)  . . .   (xn  yn)) ∈ (X × {±1})n
2: Parameter: α ∈ (0  1)
4: Learn(cid:98)ηS1 = C(S1)  where C : ∪∞
3: Let S1 = ((x1  y1)  . . .   (xn1   yn1))  S2 = ((xn1+1  yn1+1)  . . .   (xn  yn))  where n1 = (cid:100)nα(cid:101)
5: (cid:98)tS2 (cid:98)ηS1  Ψ ∈ argmax
6: Output: Classiﬁer(cid:98)hS(x) = sign((cid:98)ηS1(x) −(cid:98)tS2 (cid:98)ηS1  Ψ)

n=1(X × {±1})n → [0  1]X is a suitable CPE algorithm

[sign ◦ ((cid:98)ηS1 − t)]

(cid:98)P Ψ

t∈[0 1]

S2

3 A Generic Proof Template for Ψ-consistency of Plug-in Algorithms

D η Ψ.3

D η Ψ for P Ψ exists and is not a point of discontinuity.

We now give a general result for showing consistency of the plug-in method in Algorithm 1 for any
performance measure that can be expressed as a continuous function of TPR and TNR  and for which
the Bayes optimal classiﬁer is obtained by suitably thresholding the class probability function.
Assumption A. We will say that a probability distribution D on X × {±1} satisﬁes Assumption A
w.r.t. Ψ if t∗
D η Ψ exists and is in (0  1)  and the cumulative distribution functions of the random vari-
able η(x) conditioned on y = 1 and on y = −1  P(η(x) ≤ z | y = 1) and P(η(x) ≤ z | y = −1) 
are continuous at z = t∗
Note that this assumption holds for any distribution D for which η(x) conditioned on y = 1 and on
y = −1 is continuous  and also for any D for which η(x) conditioned on y = 1 and on y = −1 is
mixed  provided the optimum threshold t∗
Under the above assumption  and assuming that the CPE algorithm used in Algorithm 1 is L1-
consistent (which holds for any algorithm that uses a regularized empirical risk minimization of a
proper loss [16  28])  we have our main consistency result.
Theorem 1 (Ψ-consistency of Algorithm 1). Let Ψ : [0  1]3 → R+ be continuous in each argument.
Let D be a probability distribution on X ×{±1} that satisﬁes Assumption A w.r.t. Ψ  and for which
the Bayes optimal classiﬁer is of the form hΨ ∗(x) = sign ◦ (η(x) − t∗
D η Ψ). If the CPE algorithm
C in Algorithm 1 is L1-consistent  then Algorithm 1 is Ψ-consistent w.r.t. D.
Before we prove the above theorem  we will ﬁnd it useful to state the following lemmas. In our ﬁrst
lemma  we state that the TPR and TNR of a classiﬁer constructed by thresholding a suitable class
probability estimate at a ﬁxed c ∈ (0  1) converge respectively to the TPR and TNR of the classiﬁer
obtained by thresholding the true class probability function η at c.
Lemma 2 (Convergence of TPR and TNR for ﬁxed threshold). Let D be a distribution on X ×
an apriori ﬁxed constant such that the cumulative distribution functions P(η(x) ≤ z | y = 1) and
P(η(x) ≤ z | y = −1) are continuous at z = c. We then have

{±1}. Let (cid:98)ηS : X → [0  1] be generated by an L1-consistent CPE algorithm. Let c ∈ (0  1) be

TPRD[sign ◦ ((cid:98)ηS − c)] P−→ TPRD[sign ◦ (η − c)];
TNRD[sign ◦ ((cid:98)ηS − c)] P−→ TNRD[sign ◦ (η − c)].

As a corollary to the above lemma  we have a similar result for P Ψ.
Lemma 3 (Convergence of P Ψ for ﬁxed threshold). Let Ψ : [0  1]3 → R+ be continuous in each
argument. Under the statement of Lemma 2  we have

We next state a result showing convergence of the empirical performance measure to its population
value for a ﬁxed classiﬁer  and a uniform convergence result over a class of thresholded classiﬁers.
Lemma 4 (Concentration result for P Ψ). Let Ψ : [0  1]3 → R+ be continuous in each argument.
Then for any ﬁxed h : X →{±1}  and  > 0 

P Ψ

D[sign ◦ ((cid:98)ηS − c)] P−→ P Ψ
(cid:12)(cid:12)(cid:12) ≥ 
(cid:16)(cid:12)(cid:12)(cid:12)P Ψ
D[h] − (cid:98)P Ψ

S [h]

PS∼Dn

D[sign ◦ (η − c)].

(cid:17) → 0 as n → ∞.

3For simplicity  we assume that t∗

D η Ψ is in (0  1); our results easily extend to the case when t∗

D η Ψ ∈ [0  1].

4

Lemma 5 (Uniform convergence of P Ψ over threshold classiﬁers). Let Ψ : [0  1]3 → R+ be
continuous in each argument. For any f : X → [0  1] and  > 0 

(cid:32) (cid:91)

θ∈Tf

PS∼Dn

(cid:110)(cid:12)(cid:12)(cid:12)P Ψ
D[θ] − (cid:98)P Ψ

S [θ]

(cid:111)(cid:33)
(cid:12)(cid:12)(cid:12) ≥ 

→ 0 as n → ∞.

regretΨ

D[hS] = regretΨ

= P Ψ ∗
= P Ψ

We are now ready to prove our main theorem.
D η Ψ ∈ argmax
Proof of Theorem 1. Recall t∗
t∈[0 1]
following  we shall use t∗ in the place of t∗

P Ψ
D[sign ◦ (η − t)] exists by Assumption A. In the

which follows from the assumption on the Bayes optimal classiﬁer for P Ψ. Adding and subtracting
empirical and population versions of P Ψ computed on certain classiﬁers 
regretΨ

D η Ψ and(cid:98)tS2 S1 in the place of(cid:98)tS2 (cid:98)ηS1  Ψ. We have
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1)]
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1)]
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1 )] 
D − P Ψ
D[sign ◦ (η − t∗)] − P Ψ
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1 )] = P Ψ
D[sign ◦ ((cid:98)ηS1 − t∗)]
(cid:123)(cid:122)
(cid:125)
(cid:124)
D[sign ◦ (η − t∗)] − P Ψ
D[sign ◦ ((cid:98)ηS1 − t∗)] − (cid:98)P Ψ
(cid:123)(cid:122)
(cid:124)
+ (cid:98)P Ψ
[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1 )] − P Ψ
(cid:123)(cid:122)
(cid:124)
For term2  from the deﬁnition of threshold(cid:98)tS2 S1 (see Algorithm 1)  we have
[sign ◦ ((cid:98)ηS1 − t∗)].

We now show convergence for each of the above terms. Applying Lemma 3 with c = t∗ (by
Assumption A  t∗ ∈ (0  1) and satisﬁes the necessary continuity assumption)  we have term1
P−→ 0.

[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1)]
(cid:125)
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1)]
(cid:125)

term2 ≤ P Ψ

+ P Ψ

term1

term2

term3

(2)

S2

S2

.

S2

(cid:104)
(cid:104)

PS2|S1

PS2|S1

= ES1
≤ ES1
→ 0

Then for any  > 0 
PS∼Dn

D[sign ◦ ((cid:98)ηS1 − t∗)] − (cid:98)P Ψ
(cid:0)term2 ≥ (cid:1) = PS1∼Dn1   S2∼Dn−n1
(cid:0)term2 ≥ (cid:1)
(cid:0)term2 ≥ (cid:1)(cid:105)
(cid:16)(cid:12)(cid:12)P Ψ
D[sign ◦ ((cid:98)ηS1 − t∗)] − (cid:98)P Ψ

as n→∞  where the third step follows from Eq. (2)  and the last step follows by applying  for a

(cid:17)(cid:105)
[sign ◦ ((cid:98)ηS1 − t∗)](cid:12)(cid:12) ≥ 
ﬁxed S1  the concentration result in Lemma 4 with h = sign ◦ ((cid:98)ηS1 − t∗) (given continuity of Ψ).
(cid:16)(cid:98)P Ψ
(cid:17)(cid:105)
(cid:0)term3 ≥ (cid:1) = ES1
[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1 )] − P Ψ
D[sign ◦ ((cid:98)ηS1 −(cid:98)tS2 S1)] ≥ 
(cid:32) (cid:91)
(cid:111)(cid:33)(cid:35)
(cid:110)(cid:12)(cid:12)(cid:98)P Ψ
D[θ](cid:12)(cid:12) ≥ 
θ∈T(cid:98)ηS1
= {sign ◦ ((cid:98)ηS1 − t)| t ∈ [0  1]} (for a ﬁxed S1).

as n→∞  where the last step follows by applying the uniform convergence result in Lemma 5 over
the class of thresholded classiﬁers T(cid:98)ηS1
4 Consistency of Plug-in Algorithms for AM  Fβ  and G-TP/PR

Finally  for term3  we have for any  > 0 

≤ ES1
→ 0

[θ] − P Ψ

PS2|S1

PS2|S1

(cid:104)
(cid:34)

PS

S2

S2

S2

We now use the result in Theorem 1 to establish consistency of the plug-in algorithms for the arith-
metic mean of TPR and TNR  the Fβ-measure  and the geometric mean of TPR and precision.

5

4.1 Consistency for AM-measure

The arithmetic mean of TPR and TNR (AM) or one minus the balanced error rate (BER) is a widely-
used performance measure in class imbalanced binary classiﬁcation settings [17–19]:

PAM
D [h] =

TPRD[h] + TNRD[h]

.

for

the AM-measure is of

2
It can be shown that Bayes optimal classiﬁer
the form
hAM ∗(x) = sign ◦ (η(x) − p) (see for example [16])  and that the threshold chosen by the plug-
in method in Algorithm 1 for the AM-measure is an empirical estimate of p. In recent work  Menon
et al. show that this plug-in method is consistent w.r.t. the AM-measure [16]; their proof makes use
of a decomposition of the AM-measure in terms of a certain cost-sensitive error and a result of [22]
on regret bounds for cost-sensitive classiﬁcation. We now use our result in Theorem 1 to give an
alternate route for showing AM-consistency of this plug-in method.4
Theorem 6 (Consistency of Algorithm 1 w.r.t. AM-measure). Let Ψ = ΨAM. Let D be a
distribution on X × {±1} that satisﬁes Assumption A w.r.t. ΨAM.
If the CPE algorithm C in
Algorithm 1 is L1-consistent  then Algorithm 1 is AM-consistent w.r.t. D.
Proof. We apply Theorem 1 noting that ΨAM(u  v  p) = (u+v)/2 is continuous in all its arguments 
and that the Bayes optimal classiﬁer for PAM is of the requisite thresholded form.

4.2 Consistency for Fβ-measure

The Fβ-measure or the (weighted) harmonic mean of TPR and precision is a popular performance
measure used in information retrieval [1]:
(1 + β2)TPRD[h]PrecD[h]

p + β2(cid:0)pTPRD[h] + (1 − p)(1 − TNRD[h])(cid:1)  

β2TPRD[h] + PrecD[h]

(1 + β2)pTPRD[h]

PFβ
D [h] =

=

where β ∈ (0  1) controls the trade-off between TPR and precision. In a recent work  Ye et al. [15]
show that the optimal classiﬁer for the Fβ-measure is the class probability η thresholded suitably.
Lemma 7 (Optimality of threshold classiﬁers for Fβ-measure; Ye et al. (2012) [15]). For any
distribution D over X × {±1} that satisﬁes Assumption A w.r.t. Ψ  the Bayes optimal classiﬁer for
PFβ is of the form hFβ  ∗(x) = sign ◦ (η(x) − t∗
As noted earlier  the authors in [15] show that an idealized plug-in method that applies an empirically
determined threshold to the ‘true’ class probability η is consistent w.r.t. the Fβ-measure . This result
is however useful only when the ‘true’ class probability is available to a learning algorithm  which
is not the case in most practical settings. On the other hand  the plug-in method considered in our
work constructs a classiﬁer by assigning an empirical threshold to a suitable ‘estimate’ of the class
probability. Using Theorem 1  we now show that this method is consistent w.r.t. the Fβ-measure.
Theorem 8 (Consistency of Algorithm 1 w.r.t. Fβ-measure). Let Ψ = ΨFβ in Algorithm 1. Let
D be a distribution on X × {±1} that satisﬁes Assumption A w.r.t. ΨFβ . If the CPE algorithm C in
Algorithm 1 is L1-consistent  then Algorithm 1 is Fβ-consistent w.r.t. D.

D η Fβ

).

Proof. We apply Theorem 1 noting that ΨFβ (u  v  p) =
argument  and that (by Lemma 7) the Bayes optimal classiﬁer for PFβ is of the requisite form.

p+β2(pu+(1−p)(1−v)) is continuous in each

(1+β2)pu

4.3 Consistency for G-TP/PR

The geometric mean of TPR and precision (G-TP/PR) is another performance measure proposed for
class imbalanced classiﬁcation problems [3]:

[h] = (cid:112)TPRD[h]PrecD[h] =

PG-TP/PR

D

(cid:115)

pTPRD[h]2

pTPRD[h] + (1 − p)(1 − TNRD[h])

.

4Note that the plug-in classiﬁcation threshold chosen for the AM-measure is the same independent of the
class probability estimate used; our consistency results will therefore apply in this case even if one uses  as
in [16]  the same sample for both learning a class probability estimate  and estimating the plug-in threshold.

6

We ﬁrst show that the optimal classiﬁer for G-TP/PR is obtained by thresholding the class probability
function η at a suitable point; our proof uses a technique similar to the one for the Fβ-measure in [15].
Lemma 9 (Optimality of threshold classiﬁers for G-TP/PR). For any distribution D on
X × {±1} that satisﬁes Assumption A w.r.t. Ψ  the Bayes optimal classiﬁer for PG-TP/PR is of
the form hG-TP/PR ∗(x) = sign(η(x) − t∗
Theorem 10 (Consistency of Algorithm 1 w.r.t. G-TP/PR). Let Ψ = ΨG-TP/PR. Let D be a
distribution on X × {±1} that satisﬁes Assumption A w.r.t. ΨG-TP/PR. If the CPE algorithm C in
Algorithm 1 is L1-consistent  then Algorithm 1 is G-TP/PR-consistent w.r.t. D.

D η G-TP/PR).

(cid:113)

Proof. We apply Theorem 1 noting that ΨG-TP/PR(u  v  p) =
pu+(1−p)(1−v) is continuous in each
argument  and that (by Lemma 9) the Bayes optimal classiﬁer for PG-TP/PR is of the requisite form.

pu2

5 Consistency of Plug-in Algorithms for Non-decomposable Performance

Measures that are Monotonic in TPR and TNR

The consistency results seen so far apply to any distribution that satisﬁes a mild continuity condi-
tion at the optimal threshold for a performance measure  and have crucially relied on the speciﬁc
functional form of the measure. In this section  we shall see that under a stricter continuity assump-
tion on the distribution  the empirical plug-in algorithm can be shown to be consistent w.r.t. any
performance measure that is a continuous and monotonically increasing function of TPR and TNR.
Assumption B. We will say that a probability distribution D on X × {±1} satisﬁes Assumption
B w.r.t. Ψ if t∗
D η Ψ exists and is in (0  1)  and the cumulative distribution function of the random
variable η(x)  P(η(x) ≤ z)  is continuous at all z ∈ (0  1).
Distributions that satisfy the above assumption also satisfy Assumption A. We show that under this
assumption  the optimal classiﬁer for any performance measure that is monotonically increasing in
TPR and TNR is obtained by thresholding η  and this holds irrespective of the speciﬁc functional
form of the measure. An application of Theorem 1 then gives us the desired consistency result.
Lemma 11 (Optimality of threshold classiﬁers for monotonic Ψ under distributional assump-
tion). Let Ψ : [0  1]3 → R+ be monotonically increasing in its ﬁrst two arguments. Then for any
distribution D on X × {±1} that satisﬁes Assumption B  the Bayes optimal classiﬁer for P Ψ is of
the form hΨ ∗(x) = sign(η(x) − t∗
Theorem 12 (Consistency of Algorithm 1 for monotonic Ψ under distributional assumption).
Let Ψ : [0  1]3 → R+ be continuous in each argument  and monotonically increasing in its ﬁrst two
arguments. Let D be a distribution on X × {±1} that satisﬁes Assumption B. If the CPE algorithm
C in Algorithm 1 is L1-consistent  then Algorithm 1 is Ψ-consistent w.r.t. D.
Proof. We apply Theorem 1 by using the continuity assumption on Ψ  and noting that  by Lemma 11
and monotonicity of Ψ  the Bayes optimal classiﬁer for P Ψ is of the requisite form.
The above result applies to all performance measures listed in Table 1  and in particular  to the
geometric  harmonic  and quadratic means of TPR and TNR [2–5]  for which the Bayes optimal
classiﬁer need not be of the requisite thresholded form for a general distribution (see Appendix C).

D η Ψ).

6 Experiments

We performed two types of experiments. The ﬁrst involved synthetic data  where we demonstrate
diminishing regret of the plug-in method in Algorithm 1 with growing sample size for different
performance measures; since the data is generated from a known distribution  exact calculation of
regret is possible here. The second involved real data  where we show that the plug-in algorithm is
competitive with the state-of-the-art SVMperf algorithm for non-decomposable measures (SVMPerf)
[12]; we also include for comparison a plug-in method with a ﬁxed threshold of 0.5 (Plug-in (0-1)).
We consider three performance measures here: F1-measure  G-TP/PR and G-Mean (see Table 1).
Synthetic data. We generated data from a known distribution (class conditionals are multivariate
Gaussians with mixing ratio p and equal covariance matrices) for which the optimal classiﬁer for

7

Figure 1: Experiments on synthetic data with p = 0.5: regret as a function of number of training
examples using various methods for the F1  G-TP/PR and G-mean performance measures.

Figure 2: Experiments on synthetic data with p = 0.1: regret as a function of number of training
examples using various methods for the F1  G-TP/PR and G-Mean performance measures.

Figure 3: Experiments on real data: results for various methods (using linear models) on four data
sets in terms of F1  G-TP/PR and G-Mean performance measures. Here N  d  p refer to the number
of instances  number of features and fraction of positives in the data set respectively.
each performance measure considered here is linear  making it sufﬁcient to learn a linear model; the
distribution satisﬁes Assumption B w.r.t. each performance measure. We used regularized logistic
regression as the CPE method in Algorithm 1 in order to satisfy the L1-consistency condition in
Theorem 1 (see Appendix A.1 and A.4 for details). The experimental results are shown in Figures 1
and 2 for p = 0.5 and p = 0.1 respectively. In each case  the regret for the empirical plug-in method
(Plug-in (F1)  Plug-in (G-TP/PR) and Plug-in (GM)) goes to zero with increasing training set size 
validating our consistency results; SVMperf often fails to exhibit diminishing regret for p = 0.1; and
as expected  Plug-in (0-1)  with its apriori ﬁxed threshold  fails to be consistent in most cases.
Real data. We ran the three algorithms described earlier over data sets drawn from the UCI ML
repository [29] and a cheminformatics data set obtained from [30]  and report their performance on
held-out test sets. Figure 3 contains results for four data sets averaged over 10 random train-test
splits of the original data (also see Appendix A.2 and A.3). Clearly  in most cases  the empirical
plug-in performs comparable to SVMperf and outperforms Plug-in (0-1). Moreover  the empirical
plug-in was found to exhibit faster run-times than the SVMperf method (see Figure 5 in Appendix).
7 Conclusions
We have presented a general method for proving consistency of plug-in algorithms that assign an
empirical threshold to a suitable class probability estimate for a variety of non-decomposable per-
formance measures for binary classiﬁcation that can be expressed as a continuous function of TPR
and TNR  and for which the Bayes optimal classiﬁer is the class probability function thresholded
suitably. We use our template to show consistency for the AM  Fβ and G-TP/PR measures  and
under a continuous distribution  for any performance measure that is continuous and monotonic in
TPR and TNR. Our experiments suggest that these algorithms yield performance comparable to the
state-of-the-art SVMperf method  while being faster than this method in practice.
Acknowledgments
HN thanks support from a Google India PhD Fellowship. SA gratefully acknowledges support from
DST  Indo-US Science and Technology Forum  and an unrestricted gift from Yahoo.

8

10210310410500.050.10.15No. of training examplesF1 RegretF1−measure Plug−in (F1)SVMPerf (F1)Plug−in (0−1)10210310410500.050.10.15No. of training examplesG−TP/PR RegretG−TP/PR Plug−in (G−TP/PR)SVMPerf (G−TP/PR)Plug−in (0−1)10210310410500.020.040.060.080.1No. of training examplesGM RegretG−Mean Plug−in (GM)SVMPerf (GM)Plug−in (0−1)10210310410500.050.10.150.20.250.3No. of training examplesF1 RegretF1−measure Plug−in (F1)SVMPerf (F1)Plug−in (0−1)10210310410500.050.10.150.20.250.3No. of training examplesG−TP/PR RegretG−TP/PR Plug−in (G−TP/PR)SVMPerf (G−TP/PR)Plug−in (0−1)10210310410500.10.20.30.40.5No. of training examplesGM RegretG−Mean Plug−in (GM)SVMPerf (GM)Plug−in (0−1)F1G−TP/PRG−Mean00.51Performance on test setcar (N = 1728  d = 21  p = 0.038) Emp. Plug−inSVMPerfPlug−in (0−1)F1G−TP/PRG−Mean00.51Performance on test setchemo (N = 2111  d = 1021  p = 0.024) Emp. Plug−inSVMPerfPlug−in (0−1)F1G−TP/PRG−Mean00.51Performance on test setnursery (N = 12960  d = 27  p = 0.025) Emp. Plug−inSVMPerfPlug−in (0−1)F1G−TP/PRG−Mean00.51Performance on test setletter (N = 18668  d = 16  p = 0.034) Emp. Plug−inSVMPerfPlug−in (0−1)References
[1] C. D. Manning  P. Raghavan  and H. Sch¨utze. Introduction to Information Retrieval. Cambridge Univer-

sity Press  2008.

[2] M. Kubat and S. Matwin. Addressing the curse of imbalanced training sets: One-sided selection.

ICML  1997.

In

[3] S. Daskalaki  I. Kopanas  and N. Avouris. Evaluation of classiﬁers for an uneven class distribution prob-

lem. Applied Artiﬁcial Intelligence  20:381–417  2006.

[4] K. Kennedy  B.M. Namee  and S.J. Delany. Learning without default: a study of one-class classiﬁcation

and the low-default portfolio problem. In ICAICS  2009.

[5] S. Lawrence  I. Burns  A. Back  A-C. Tsoi  and C.L. Giles. Neural network classiﬁcation and prior class

probabilities. In Neural Networks: Tricks of the Trade  pages 1524:299–313. 1998.
[6] Y. Yang. A study of thresholding strategies for text categorization. In SIGIR  2001.
[7] D.D. Lewis. Evaluating and optimizing autonomous text classiﬁcation systems. In SIGIR  1995.
[8] K.M.A. Chai. Expectation of F-measures: Tractable exact computation and some empirical observations

of its properties. In SIGIR  2005.

[9] D.R. Musicant  V. Kumar  and A. Ozgur. Optimizing F-measure with support vector machines. In FLAIRS 

2003.

[10] S. Gao  W. Wu  C-H. Lee  and T-S. Chua. A maximal ﬁgure-of-merit learning approach to text catego-

rization. In SIGIR  2003.

[11] M. Jansche. Maximum expected F-measure training of logistic regression models. In HLT  2005.
[12] T. Joachims. A support vector method for multivariate performance measures. In ICML  2005.
[13] Z. Liu  M. Tan  and F. Jiang. Regularized F-measure maximization for feature selection and classiﬁcation.

BioMed Research International  2009  2009.

[14] P.M. Chinta  P. Balamurugan  S. Shevade  and M.N. Murty. Optimizing F-measure with non-convex loss

and sparse linear classiﬁers. In IJCNN  2013.

[15] N. Ye  K.M.A. Chai  W.S. Lee  and H.L. Chieu. Optimizing F-measures: A tale of two approaches. In

ICML  2012.

[16] A.K. Menon  H. Narasimhan  S. Agarwal  and S. Chawla. On the statistical consistency of algorithms for

binary classiﬁcation under class imbalance. In ICML  2013.

[17] J. Cheng  C. Hatzis  H. Hayashi  M-A. Krogel  S. Morishita  D. Page  and J. Sese. KDD Cup 2001 report.

ACM SIGKDD Explorations Newsletter  3(2):47–64  2002.

[18] R. Powers  M. Goldszmidt  and I. Cohen. Short term performance forecasting in enterprise systems. In

KDD  2005.

[19] Q. Gu  L. Zhu  and Z. Cai. Evaluation measures of the classiﬁcation performance of imbalanced data sets.

In Computational Intelligence and Intelligent Systems  volume 51  pages 461–471. 2009.

[20] T. Zhang. Statistical behaviour and consistency of classiﬁcation methods based on convex risk minimiza-

tion. Annals of Mathematical Statistics  32:56–134  2004.

[21] P.L. Bartlett  M.I. Jordan  and J.D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal of the

American Statistical Association  101(473):138–156  2006.

[22] C. Scott. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics  6:958–992  2012.
[23] M. Zhao  N. Edakunni  A. Pocock  and G. Brown. Beyond Fano’s inequality: Bounds on the optimal
F-score  BER  and cost-sensitive risk and their implications. Journal of Machine Learning Research 
14(1):1033–1090  2013.

[24] Z.C. Lipton  C. Elkan  and B. Naryanaswamy. Optimal thresholding of classiﬁers to maximize F1 mea-

sure. In ECML/PKDD  2014.

[25] J. Petterson and T. Caetano. Reverse multi-label learning. In NIPS  2010.
[26] K. Dembczynski  W. Waegeman  W. Cheng  and E. H¨ullermeier. An exact algorithm for F-measure

maximization. In NIPS  2011.

[27] K. Dembczynski  A. Jachnik  W. Kotlowski  W. Waegeman  and E. Huellermeier. Optimizing the F-mea-
sure in multi-label classiﬁcation: Plug-in rule approach versus structured loss minimization. In ICML  13.
[28] S. Agarwal. Surrogate regret bounds for the area under the ROC curve via strongly proper losses. In

COLT  2013.

[29] A. Frank and A. Asuncion. UCI machine learning repository  2010. URL: http://archive.ics.uci.edu/ml.
[30] Robert N. Jorissen and Michael K. Gilson. Virtual screening of molecular databases using a support

vector machine. Journal of Chemical Information and Modeling  45:549–561  2005.

9

,Tetsuro Morimura
Takayuki Osogami
Tsuyoshi Ide
Harikrishna Narasimhan
Rohit Vaish
Shivani Agarwal