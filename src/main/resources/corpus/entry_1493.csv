2017,Diffusion Approximations for Online Principal Component Estimation and Global Convergence,In this paper  we propose to adopt the diffusion approximation tools to study the dynamics of Oja's iteration which is an online stochastic gradient method for the principal component analysis. Oja's iteration maintains a running estimate of the true principal component from streaming data and enjoys less temporal and spatial complexities. We show that the Oja's iteration for the top eigenvector generates a continuous-state discrete-time Markov chain over the unit sphere. We characterize the Oja's iteration in three phases using diffusion approximation and weak convergence tools. Our three-phase analysis further provides a finite-sample error bound for the running estimate  which matches the minimax information lower bound for PCA under the additional assumption of bounded samples.,Diffusion Approximations for Online Principal
Component Estimation and Global Convergence

Chris Junchi Li

Mengdi Wang

Princeton University

Han Liu

Department of Operations Research and Financial Engineering  Princeton  NJ 08544

{junchil mengdiw hanliu}@princeton.edu

Tong Zhang
Tencent AI Lab

Shennan Ave  Nanshan District  Shenzhen  Guangdong Province 518057  China

tongzhang@tongzhang-ml.org

Abstract

In this paper  we propose to adopt the diffusion approximation tools to study the
dynamics of Oja’s iteration which is an online stochastic gradient descent method
for the principal component analysis. Oja’s iteration maintains a running estimate
of the true principal component from streaming data and enjoys less temporal
and spatial complexities. We show that the Oja’s iteration for the top eigenvector
generates a continuous-state discrete-time Markov chain over the unit sphere. We
characterize the Oja’s iteration in three phases using diffusion approximation and
weak convergence tools. Our three-phase analysis further provides a ﬁnite-sample
error bound for the running estimate  which matches the minimax information
lower bound for principal component analysis under the additional assumption of
bounded samples.

1

Introduction

In the procedure of Principal Component Analysis (PCA) we aim at learning the principal leading
eigenvector of the covariance matrix of a d-dimensional random vector Z from its independent
and identically distributed realizations Z1  . . .   Zn. Let E[Z] = 0  and let the eigenvalues of Σ be
λ1 > λ2 ≥ ··· ≥ λd > 0  then the PCA problem can be formulated as minimizing the expectation of
a nonconvex function:

minimize − w(cid:62)E(cid:2)ZZ(cid:62)(cid:3) w 

(1.1)
where (cid:107) · (cid:107) denotes the Euclidean norm. Since the eigengap λ1 − λ2 is nonzero  the solution to
(1.1) is unique  denoted by w∗. The classical method of ﬁnding the estimator of the ﬁrst leading
eigenvector w∗ can be formulated as the solution to the empirical covariance problem as

subject to (cid:107)w(cid:107) = 1  w ∈ Rd 

(cid:107)w(cid:107)=1

In words  (cid:98)Σ(n) denotes the empirical covariance matrix for the ﬁrst n samples. The estimator (cid:98)w(n)
produced via this process provides a statistical optimal solution (cid:98)w(n). Precisely  [43] shows that the
angle between any estimator (cid:101)w(n) that is a function of the ﬁrst n samples and w∗ has the following

i=1

n

(cid:98)w(n) = argmin

−w(cid:62)(cid:98)Σ(n)w 

where (cid:98)Σ(n) ≡ 1

n(cid:88)

Z(i)(cid:16)

Z(i)(cid:17)(cid:62)

.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Left: an objective function for the top-1 PCA  where we use both the radius and heatmap to
represent the function value at each point of the unit sphere. Right: A quiver plot on the unit sphere
denoting the directions of negative gradient of the PCA objective.

E(cid:104)

sin2 ∠((cid:101)w(n)  w∗)

(cid:105) ≥ c · σ2∗ · d − 1

 

minimax lower bound

inf(cid:101)w(n)

sup

n

Z∈M(σ2∗ d)

where c is some positive constant. Here the inﬁmum of (cid:101)w(n) is taken over all principal eigenvector
estimators  and M(σ2∗  d) is the collection of all d-dimensional subgaussian distributions with mean
zero and eigengap λ1 − λ2 > 0 satisfying λ1λ2/(λ1 − λ2)2 ≤ σ2∗. Classical PCA method has time
complexity O(nd2) and space complexity O(d2). The drawback of this method is that  when the
data samples are high-dimensional  computing and storage of a large empirical covariance matrix can
be costly.
In this paper we concentrate on the streaming or online method for PCA that processes online data
and estimates the principal component sequentially without explicitly computing and storing the

empirical covariance matrix (cid:98)Σ. Over thirty years ago  Oja [30] proposed an online PCA iteration that

(1.2)

can be regarded as a projected stochastic gradient descent method as

(cid:104)
w(n−1) + βZ(n)(Z(n))(cid:62)w(n−1)(cid:105)

.

w(n) = Π

(1.3)
Here β is some positive learning rule or stepsize  and Π is deﬁned as Πw = (cid:107)w(cid:107)−1w for each
nonzero vector w  namely  Π projects any vector onto the unit sphere S d−1 = {w ∈ Rd | (cid:107)w(cid:107) = 1}.
Oja’s iteration enjoys a less expensive time complexity O(nd) and space complexity O(d) and
thereby has been used as an alternative method for PCA when both the dimension d and number of
samples n are large.
In this paper  we adopt the diffusion approximation method to characterize the stochastic algorithm
using Markov processes and its differential equation approximations. The diffusion process approxi-
mation is a fundamental and powerful analytic tool for analyzing complicated stochastic process. By
leveraging the tool of weak convergence  we are able to conduct a heuristic ﬁnite-sample analysis of
the Oja’s iteration and obtain a convergence rate which  by carefully choosing the stepsize β  matches
the PCA minimax information lower bound. Our analysis involves the weak convergence theory for
Markov processes [11]  which is believed to have a potential for a broader class of stochastic algo-
rithms for nonconvex optimization  such as tensor decomposition  phase retrieval  matrix completion 
neural network  etc.

Our Contributions We provide a Markov chain characterization of the stochastic process {w(n)}
generated by the Oja’s iteration with constant stepsize. We show that upon appropriate scalings  the
iterates as a Markov process weakly converges to the solution of an ordinary differential equation
system  which is a multi-dimensional analogue to the logistic equations. Also locally around the
neighborhood of a stationary point  upon a different scaling the process weakly converges to the
multidimensional Ornstein-Uhlenbeck processes. Moreover  we identify from differential equation
approximations that the global convergence dynamics of the Oja’s iteration has three distinct phases:

2

Figure 2: A simulation plot of Oja’s method  marked with the three phases.

(i) The initial phase corresponds to escaping from unstable stationary points;
(ii) The second phase corresponds to fast deterministic crossing period;
(iii) The third phase corresponds to stable oscillation around the true principal component.

Lastly  this is the ﬁrst work that analyze the global rate of convergence analysis of Oja’s iteration 
i.e.  the convergence rate does not have any initialization requirements.

Related Literatures This paper is a natural companion to paper by the authors’ recent work [23]
that gives explicit rate analysis using a discrete-time martingale-based approach. In this paper  we
provide a much simpler and more insightful heuristic analysis based on diffusion approximation
method under the additional assumption of bounded samples.
The idea of stochastic approximation for PCA problem can be traced back to Krasulina [19] published
almost ﬁfty years ago. His work proposed an algorithm that is regarded as the stochastic gradient
descent method for the Rayleigh quotient. In contrast  Oja’s iteration can be regarded as a projected
stochastic gradient descent method. The method of using differential equation tools for PCA appeared
in the ﬁrst papers [19  31] to prove convergence result to the principal component  among which  [31]
also analyze the subspace learning for PCA. See also [16  Chap. 1] for a gradient ﬂow dynamical
system perspective of Oja’s iteration.
The convergence rate analysis of the online PCA iteration has been very few until the recent big data
tsunami  when the need to handle massive amounts of data emerges. Recent works by [6  10  17  34]
study the convergence of online PCA from different perspectives  and obtain some useful rate results.
Our analysis using the tools of diffusion approximations suggests a rate that is sharper than all existing
results  and our global convergence rate result poses no requirement for initialization.

More Literatures Our work is related to a very recent line of work [3  13  21  33  38–41] on
the global dynamics of nonconvex optimization with statistical structures. These works carefully
characterize the global geometry of the objective functions  and in special  around the unstable
stationary points including saddle points and local maximizers. To solve the optimization problem
various algorithms were used  including (stochastic) gradient method with random initialization or
noise injection as well as variants of Newton’s method. The unstable stationary points can hence be
avoided  enabling the global convergence to desirable local minimizers.
Our diffusion process-based characterization of SGD is also related to another line of work [8  10  24 
26  37]. Among them  [10] uses techniques based on martingales in discrete time to quantify the global

3

convergence of SGD on matrix decomposition problems. In comparison  our techniques are based on
Stroock and Varadhan’s weak convergence of Markov chains to diffusion processes  which yield the
continuous-time dynamics of SGD. The rest of these results mostly focus on analyzing continuous-
time dynamics of gradient descent or SGD on convex optimization problems. In comparison  we are
the ﬁrst to characterize the global dynamics for nonconvex statistical optimization. In particular  the
ﬁrst and second phases of our characterization  especially the unstable Ornstein-Uhlenbeck process 
are unique to nonconvex problems. Also  it is worth noting that  using the arguments of [26]  we can
show that the diffusion process-based characterization admits a variational Bayesian interpretation of
nonconvex statistical optimization. However  we do not pursue this direction in this paper.
In the mathematical programming and statistics communities  the computational and statistical
aspects of PCA are often studied separately. From the statistical perspective  recent developments
have focused on estimating principal components for very high-dimensional data. When the data
dimension is much larger than the sample size  i.e.  d (cid:29) n  classical method using decomposition of
the empirical convariance matrix produces inconsistent estimates [18  29]. Sparsity-based methods
have been studied  such as the truncated power method studied by [45] and [44]. Other sparsity
regularization methods for high dimensional PCA has been studied in [2  7  9  18  25  42  43  46]  etc.
Note that in this paper we do not consider the high-dimensional regime and sparsity regularization.
From the computational perspective  power iterations or the Lanczos method are well studied. These
iterative methods require performing multiple products between vectors and empirical covariance
matrices. Such operation usually involves multiple passes over the data  whose complexity may scale
with the eigengap and dimensions [20  28]. Recently  randomized algorithms have been developed to
reduce the computation complexity [12  35  36]. A critical trend today is to combine the computational
and statistical aspects and to develop algorithmic estimator that admits fast computation as well as
good estimation properties. Related literatures include [4  5  10  14  27].

Organization §2 introduces the settings and distributional assumptions. §3 brieﬂy discusses the
Oja’s iteration from the Markov processes perspective and characterizes that it globally admits
ordinary differential equation approximation upon appropriate scaling  and also stochastic differential
equation approximation locally in the neighborhood of each stationary point. §4 utilizes the weak
convergence results and provides a three-phase argument for the global convergence rate analysis 
which is near-optimal for the Oja’s iteration. Concluding remarks are provided in §5.

2 Settings

In this section  we present the basic settings for the Oja’s iteration. The algorithm maintains a running
estimate w(n) of the true principal component w∗  and updates it while receiving streaming samples
from exterior data source. We summarize our distributional assumptions.
Assumption 2.1. The random vectors Z ≡ Z(1)  . . .   Z(n) ∈ Rd are independent and identically
distributed and have the following properties:

(i) E[Z] = 0 and E(cid:2)ZZ(cid:62)(cid:3) = Σ;

(ii) λ1 > λ2 ≥ ··· ≥ λd > 0;
(iii) There is a constant B such that (cid:107)Z(cid:107)2 ≤ B.

For the easiness of presentation  we transform the iterates w(n) and deﬁne the rescaled samples  as
follows. First we let the eigendecomposition of the covariance matrix be

Σ = E(cid:2)ZZ(cid:62)(cid:3) = UΛU(cid:62) 

where Λ = diag(λ1  λ2  . . .   λd) is a diagonal matrix with diagonal entries λ1  λ2  . . .   λd  and U is
an orthogonal matrix consisting of column eigenvectors of Σ. Clearly the ﬁrst column of U is equal
to the principal component w∗. Note that the diagonal decomposition might not be unique  in which
case we work with an arbitrary one. Second  let

Y (n) = U(cid:62)Z(n)  v(n) = U(cid:62)w(n)  v∗ = U(cid:62)w∗.

(2.1)

One can easily verify that

E[Y ] = 0 

E(cid:2)Y Y (cid:62)(cid:3) = Λ;

4

The principal component of the rescaled random variable Y   which we denote by v∗  is equal to e1 
where {e1  . . .   ed} is the canonical basis of Rd. By applying the orthonormal transformation U(cid:62)
to the stochastic process {w(n)}  we obtain an iterative process {v(n) = U(cid:62)w(n)} in the rescaled
space:

(cid:26)
(cid:26)

U(cid:62)w(n−1) + βU(cid:62)Z(n)(cid:16)
Y (n)(cid:17)(cid:62)
v(n−1) + βY (n)(cid:16)

Z(n)(cid:17)(cid:62)
(cid:27)

v(n−1)

v(n) = U(cid:62)w(n) = Π

= Π

UU(cid:62)w(n−1)

(2.2)

.

(cid:27)

Moreover  the angle processes associated with {w(n)} and {v(n)} are equivalent  i.e. 

(2.3)
Therefore it would be sufﬁcient to study the rescaled iteration v(n) in (2.2) and the transformed
iteration Y (n) throughout the rest of this paper.

∠(w(n)  w∗) = ∠(v(n)  v∗).

3 A Theory of Diffusion Approximation for PCA

In this section we show that the stochastic iterates generated by the Oja’s iteration can be approximated
by the solution of an ODE system upon appropriate scaling  as long as β is small. To work on the
approximation we ﬁrst observe that the iteration v(n)  n = 0  1  . . . generated by (2.2) forms a
discrete-time  time-homogeneous Markov process that takes values on S d−1. Furthermore  v(n)
holds strong Markov property.

3.1 Global ODE Approximation

To state our results on differential equation approximations  let us deﬁne a new process  which is
obtained by rescaling the time index n according to the stepsize β

(3.1)
We add the superscript β in the notation to emphasize the dependence of the process on β. We will

show that (cid:101)V β(t) converges weakly to a deterministic function V (t)  as β → 0+.

Furthermore  we can identify the limit V (t) as the closed-form solution to an ODE system. Under

Assumption 2.1 and using an inﬁnitesimal generator analysis we have

(cid:101)V β(t) ≡ vβ ((cid:98)tβ−1(cid:99)).

(cid:12)(cid:12)(cid:101)V β(t + β) − (cid:101)V β(t)(cid:12)(cid:12) = O(Bβ).
(cid:105)
(cid:104)(cid:101)V β(t + β) − (cid:101)V β(t)(cid:12)(cid:12)(cid:101)V β(t) = v

It follows that  as β → 0+  the inﬁnitesimal conditional variance tends to 0:
= O(Bβ) 

β−1var
and the inﬁnitesimal mean is

β−1E(cid:104)(cid:101)V β(t + β) − (cid:101)V β(t)(cid:12)(cid:12)(cid:101)V β(t) = v

=(cid:0)Λ − V (cid:62)ΛV(cid:1) V + O(B2β2).

(cid:105)

Using the classical weak convergence to diffusion argument [11  Corollary 4.2 in §7.4]  we obtain
the following result.
Theorem 3.1. If vβ (0) converges weakly to some constant vector V o ∈ S d−1 as β → 0+ then the
Markov process vβ ((cid:98)tβ−1(cid:99)) converges weakly to the solution V = V (t) to the following ordinary
differential equation system

=(cid:0)Λ − V (cid:62)ΛV(cid:1) V  

(3.2)

with initial values V (0) = V o.

dV
dt

We can straightforwardly check for sanity that the solution vector V (t) lies on the unit sphere
S d−1  i.e.  (cid:107)V (t)(cid:107) = 1 for all t ≥ 0. Written in coordinates V (t) = (V1(t)  . . .   Vd(t))(cid:62)  the ODE
is expressed for k = 1  . . .   d

dVk
dt

= Vk

(λk − λi)V 2
i .

d(cid:88)

i=1

5

One can straightforwardly verify that the solution to (3.2) has

Vk(t) = (Z(t))

−1/2 Vk(0) exp(λkt) 

(3.3)

where Z(t) is the normalization function

Z(t) =

d(cid:88)

i=1

(V o

i )2 exp(2λit).

(cid:16)

(cid:16)

1 )2(cid:17)
1 )2(cid:17)

To understand the limit function given by (3.3)  we note that in the special case where λ2 = ··· = λd

Z(t) = (V o

1 )2 exp(2λ1t) +

1 − (V o

exp(2λ2t) 

and

(V1(t))2 =

(V o

1 )2 exp(2λ1t)
1 − (V o

(V o

1 )2 exp(2λ1t) +

.

exp(2λ2t)

(3.4)

This is the formula of the logistic curve. Hence analogously  V (t) in (3.3) is namely the generalized
logistic curves.

3.2 Local Approximation by Diffusion Processes

The weak convergence to ODE theorem introduced in §3.1 characterizes the global dynamics of the
Oja’s iteration. Such approximation explains many behaviors  but neglected the presence of noise that
plays a role in the algorithm. In this section we aim at understanding the Oja’s iteration via stochastic
differential equations (SDE). We refer the readers to [32] for more on basic concepts of SDE.

In this section  we instead show that under some scaling  the process admits an approximation
of multidimensional Ornstein-Uhlenbeck process within a neighborhood of each of the unstable
stationary points  both stable and unstable. Afterwards  we develop some weak convergence results
to give a rough estimate on the rate of convergence of the Oja’s iteration. For purposes of illustration
and brevity  we restrict ourselves to the case of starting point v(0) being the stationary point ek for
some k = 1  . . .   d  and denote an arbitrary vector xk to be a (d − 1)-dimensional vector that keeps
all but the kth coordinate of x. Using theory from [11] we conclude the following theorem.
Theorem 3.2. Let k = 1  . . .   d be arbitrary. If β−1/2vβ (0)
as β → 0+  then the Markov process

converges weakly to some Uo

k ∈ Rd−1

k

β−1/2vβ ((cid:98)tβ−1(cid:99))

dUk(t) = −(λkId−1 − Λk)Uk dt +(cid:0)λkΛk

k

(cid:1)1/2

converges weakly to the solution of the multidimensional stochastic differential equation

with initial values Uk(0) = Uo

(3.5)
k. Here Bk(t) is a standard (d − 1)-dimensional Brownian motion. 1
The solution to (3.5) can be solved explicitly. We let for a matrix A ∈ Rn×n the matrix expo-
for the

  . . .   λ1/2
positive semideﬁnite diagonal matrix Λ = diag(λ1  . . .   λd). The solution to (3.5) is hence

nentiation exp(A) as exp(A) =(cid:80)∞
Uk(t) = exp(cid:2)−t(λkId−1 − Λk)(cid:3) U o

exp(cid:2)(s − t)(λkId−1 − Λk)(cid:3) dBk(s) 

n=0(1/n!)An. Also  let Λ1/2 = diag

k +(cid:0)λkΛk

(cid:1)1/2(cid:90) t

dBk(t) 

λ1/2
1

(cid:17)

(cid:16)

d

0

which is known as the multidimensional Ornstein-Uhlenbeck process  whose behavior depends on
the matrix −(λkId−1 − Λk) and is discussed in details in §4.

Before concluding this section  we emphasize that the weak convergence to diffusions results in
§3.1 and §3.2 should be distinguished from the convergence of the Oja’s iteration. From a random
process theoretical perspective  the former one treats the weak convergence of ﬁnite dimensional
distributions of a sequence of rescaled processes as β tends to 0  while the latter one charaterizes the
long-time behavior of a single realization of iterates generated by algorithm for a ﬁxed β > 0.

1 The reason we have a (d − 1)-dimensional Ornstein-Uhlenbeck process is because the objective function

of PCA is deﬁned on a (d − 1)-dimensional manifold S d−1 and has d − 1 independent variables.

6

4 Global Three-Phase Analysis of Oja’s Iteration

Previously §3.1 and §3.2 develop the tools of weak convergence to diffusion under global and local
scalings. In this section  we apply these tools to analyze the dynamics of online PCA iteration in
three phases in sequel. For purposes of illustration and brevity  we restrict ourselves to the case of
starting point v(0) that is near a saddle point ek. Let Aβ (cid:46) Bβ denotes lim supβ→0+ Aβ/Bβ ≤ 1 
a.s.  and Aβ (cid:16) Bβ when both Aβ (cid:46) Bβ and Bβ (cid:46) Aβ hold.

4.1 Phase I: Noise Initialization

In consideration of global convergence  we analyze the initial phase where the iteration starts at a
point on or around Se and eventually escapes an O(1)-neighborhood of the set

Se =(cid:8)v ∈ S d−1 : v1 = 0(cid:9) .

When thinking the sphere S d−1 as the globe with ±e1 being the north and south poles  Se corresponds
to the equator of the globe. Therefore  all unstable stationary points (including saddle points and
local maximizers) lie on the equator Se.

4.2 Phase II: Deterministic Crossing
In Phase II  the iteration escapes from the neighborhood of equator Se and converges to a basin of
attraction of the local minimizer v∗. From strong Markov property of the Oja’s iteration introduced
in the beginning of §3  one can forget the iteration steps in Phase I and analyze the iteration from
1 )2 (cid:16) δ  where
the ﬁnal iterate of Phase I. Suppose we have an initial point v(0) that satisﬁes (v(0)
δ is a ﬁxed constant in (0  1/2)  Theorem 3.1 concludes that the iteration moves in a deterministic
pattern and quickly evolves into a small neighborhood of the principal component e1 such that
(v(n)

)2 (cid:16) 1 − δ.

1

4.3 Phase III: Convergence to Principal Component

E(cid:107)U1(∞)(cid:107)2 = tr E(cid:0)(cid:2)U1(t)U1(t)(cid:62)(cid:3)(cid:1) = (λ1/2) tr(cid:0)Λ1(λ1Id−1 − Λ1)−1(cid:1) . Rescaling the Markov

In Phase III  the iteration quickly converges to and ﬂuctuates around the true principal component
v∗ = e1. We start our iteration from a neighborhood around the principal component  where
1 )2 = 1 − δ. Letting k = 1 in (3.5) and taking the limit t → ∞  we have the limit
v(0) has (v(0)
process along with some calculations gives as n → ∞  in very rough sense 

lim
n→∞

E sin2 ∠(v(n)  v∗) (cid:16) β · E(cid:107)U1(∞)(cid:107)2 = β · λ1
2

tr(cid:0)Λ1(λ1Id−1 − Λ1)−1(cid:1)

λ1λk

2(λ1 − λk)

.

(4.1)

= β · d(cid:88)

k=2

The above display implies that there will be some nondiminishing ﬂuctuations  variance being
proportional to the constant stepsize β  as time goes to inﬁnity or at stationarity. Therefore in terms of
angle  at stationarity the Markov process concentrates within a O(β1/2)-radius neighborhood of zero.

4.4 Crossing Time Estimate

We turn to estimate the running time  namely the crossing time  which is the number of iterates
required for the iteration to cross the corresponding regions in different phases. We will use the
relation v(n) ≈ V (nβ) to bridge the discrete-time algorithm and its continuous-time approximation.
Phase I. For illustrative purposes we only consider the special case where v is close to ek the kth
coordinate vector  which is a saddle point that has a negative Hessian eigenvalue. In this situation  the
SDE (3.5) in terms of the ﬁrst coordinate U (t) of Uk reduces to

dU (t) = (λ1 − λk)U (t) dt + (λ1λk)1/2 dB(t) 

(4.2)

7

(cid:90) t

(4.3)

with initial value U (0) = 0. Solution to (4.2) is known as unstable Ornstein-Uhlenbeck process [1]
and can be expressed explicitly in closed-form  as
U (t) = W β(t) exp ((λ1 − λk)t)   where W β(t) ≡ (λ1λk)1/2
Rescaling the time back to the discrete-time iteration  we let n = tβ−1 and obtain

exp (−(λ1 − λk)s) dB(s).

0

(cid:19)1/2

W β(nβ) (cid:16)

2(λ1 − λk)

(cid:18) λ1λk

where χ stands for a standard normal variable. We have

1 (cid:16) β1/2W β(nβ) exp (β(λ1 − λk)n) .
v(n)
In (4.3)  the term W β(nβ) is approximately distributed as t = nβ → ∞

(cid:18) λ1λk
(cid:19)1/2
(cid:19)
−1 β−1 log(cid:0)δ|χ|−1(cid:1) + (λ1 − λk)
−1 β−1 log(cid:0)β−1(cid:1) . This suggests that the noise helps the iteration to move away

1 (cid:16) β1/2
v(n)
)2 = δ in (4.4)  we have as β → 0+ the crossing time is approximately
β−1/2

(cid:18)(cid:18) λ1λd

.
(4.5)
Therefore we have whenever the smallest eigenvalue λd is bounded away from 0  then asymptotically
1 (cid:16) 0.5 (λ1 − λk)
N β
from ek rapidly.
Phase II. We turn to estimate the crossing time N β
ensures the existence of a constant T   that depends only on δ such that V 2
T has the following bounds:

2 in Phase II. (3.3) together with simple calculation
1 (T ) ≥ 1 − δ. Furthermore

In order to have (v(n)
1 (cid:16) (λ1 − λk)
N β

χ exp (β(λ1 − λk)n) .

(cid:19)−1/2

2(λ1 − λk)

2(λ1 − λd)

−1 β−1 log

(4.4)

χ 

1

(λ1 − λd)−1 log ((1 − δ)/δ) (cid:46) T (cid:46) (λ1 − λ2)−1 log ((1 − δ)/δ) .

(4.6)

Translating back to the timescale of the iteration  it takes asymptotically

(cid:46) (λ1 − λ2)−1β−1 log ((1 − δ)/δ)

N β
2

1

)2 ≥ 1 − δ. Theorem 3.1 indicates that when β is positively small  the
iterates to achieve (v(N β
2 )
iterates needed for the ﬁrst coordinate squared to cross from δ to 1−δ is O(β−1). This is substantiated
by simulation results [4] suggesting that the Oja’s iteration moves fast from the warm initialization.
Phase III. To estimate the crossing time N β
3 or the number of iterates needed in Phase III  we restart
our counter and have from the approximation in Theorem 3.2 and (3.5) that

E(v(n)

k )2 = (v(0)

k )2 exp (−2(λ1 − λk)βn) + βλ1λk

exp (−2(λ1 − λk)(t − s)) ds

(cid:90) βn

(cid:19)

0

λ1λk

2(λ1 − λk)

(cid:18)

d(cid:88)

k=2

λ1λk

2(λ1 − λk)

+

k )2 − β ·
(v(0)

exp (−2β(λ1 − λk)n)

= β · d(cid:88)
(cid:16) β · d(cid:88)

k=2

In terms of the iterations v(n)  note the relationship E sin2 ∠(v  e1) =(cid:80)d

+ δ exp (−2β(λ1 − λ2)n) .

2(λ1 − λk)

λ1λk

k=2

of Phase II implies that E sin2 ∠(v(0)  e1) = 1 − (v(0)

1 )2 = δ  and hence by setting

k=2 v2

k = 1 − v2

1. The end

3 )  e1) = β · d(cid:88)

E sin2 ∠(v(N β

λ1λk

2(λ1 − λk)

+ o(β) 

k=2

3 (cid:16) 0.5(λ1 − λ2)−1β−1 log(cid:0)δβ−1(cid:1) .

N β

we conclude that as β → 0+

(4.7)

8

4.5 Finite-Sample Rate Bound

In this subsection we establish the global ﬁnite-sample convergence rate using the crossing time
estimates in the previous subsection. Starting from v(0) = ek where k = 2  . . .   d is arbitrary  the
3 as β → 0+ such that  by choosing δ ∈ (0  1/2) as a
global convergence time N β = N β
small ﬁxed constant 

2 + N β
1 + N β
N β (cid:16) (λ1 − λ2)

−1 β−1 log(cid:0)β−1(cid:1)  

with the following estimation on global convergence rate as in (4.1)

sin2 ∠(v(N β )  v∗) = β · d(cid:88)

λ1λk

2(λ1 − λk)

.

¯β(T ))  v∗) ≤ d(cid:88)

k=2

Given a ﬁxed number of samples T   by choosing β as

k=2

we have T (cid:16) (λ1 − λ2)−1 ¯β(T )−1 log(cid:0) ¯β(T )(cid:1)−1

β = ¯β(T ) ≡

log T

(λ1 − λ2)T
= N ¯β(T ). Plugging in β as in (4.8) we have  by

(4.8)

the angle-preserving property of coordinate transformation (2.3)  that

E sin2 ∠(w(N

¯β(T ))  w∗) = E sin2 ∠(v(N

λ1λk

2(λ1 − λk)

·

log T

(λ1 − λ2)T

.

(4.9)

The ﬁnite sample bound in (4.9) is sharper than any existing results and matches the information lower
bound. Moreover  (4.9) implies that the rate in terms of sine-squared angle is sin2 ∠(w(T )  w∗) ≤
C · λ1λ2/(λ1 − λ2)2 · d log T /T  which matches the minimax information lower bound (up to a log T
factor)  see for example  Theorem 3.1 of [43]. Limited by space  details about the rate comparison is
provided in the supplementary material.

5 Concluding Remarks

(i) As β → 0+ we have N β

We make several concluding remarks on the global convergence rate estimations  as follows.
Crossing Time Comparison. From the crossing time estimates in (4.5)  (4.6)  (4.7) we conclude
1 → 0. This implies that the algorithm demonstrates the cutoff
phenomenon which frequently occur in discrete-time Markov processes [22]. In words 
the Phase II where the objective value in Rayleigh quotient drops from 1 − δ to δ is an
asymptotically a phase of short time  compared to Phases I and III  so the convergence curve
occurs instead of an exponentially decaying curve.
1 (cid:16) 1. This suggests that for the high-d case that Phase I of

(ii) As β → 0+ we have N β

2 /N β

3 /N β

escaping from the equator consumes roughly the same iterations as in Phase III.

To summarize from above  the cold initialization iteration roughly takes twice the number of steps

than the warm initialization version which is consistent with the simulation discussions in [31].
Subspace Learning. In this work we primarily concentrates on the problem of ﬁnding the top-
1 eigenvector. It is believed that the problem of ﬁnding top-k eigenvectors  a.k.a. the subspace
PCA problem  can be analyzed using our approximation methods. This will involve a careful
characterization of subspace angles and is hence more complex. We leave this for future investigations.

References
[1] Aldous  D. (1989). Probability Approximations via the Poisson Clumping Heuristic  volume 77. Springer.

[2] Amini  A. & Wainwright  M. (2009). High-dimensional analysis of semideﬁnite relaxations for sparse

principal components. The Annals of Statistics  37(5B)  2877–2921.

[3] Anandkumar  A. & Ge  R. (2016). Efﬁcient approaches for escaping higher order saddle points in non-convex

optimization. arXiv preprint arXiv:1602.05908.

[4] Arora  R.  Cotter  A.  Livescu  K.  & Srebro  N. (2012). Stochastic optimization for PCA and PLS. In 50th

Annual Allerton Conference on Communication  Control  and Computing (pp. 861–868).

9

[5] Arora  R.  Cotter  A.  & Srebro  N. (2013). Stochastic optimization of PCA with capped msg. In Advances

in Neural Information Processing Systems (pp. 1815–1823).

[6] Balsubramani  A.  Dasgupta  S.  & Freund  Y. (2013). The fast convergence of incremental PCA. In

Advances in Neural Information Processing Systems (pp. 3174–3182).

[7] Cai  T. T.  Ma  Z.  & Wu  Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. The Annals of

Statistics  41(6)  3074–3110.

[8] Darken  C. & Moody  J. (1991). Towards faster stochastic gradient search. In Advances in Neural Information

Processing Systems (pp. 1009–1016).

[9] d’Aspremont  A.  Bach  F.  & El Ghaoui  L. (2008). Optimal solutions for sparse principal component

analysis. Journal of Machine Learning Research  9  1269–1294.

[10] De Sa  C.  Olukotun  K.  & Ré  C. (2015). Global convergence of stochastic gradient descent for some
non-convex matrix problems. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15) (pp. 2332–2341).

[11] Ethier  S. N. & Kurtz  T. G. (2005). Markov Processes: Characterization and Convergence  volume 282.

John Wiley & Sons.

[12] Garber  D. & Hazan  E. (2015). Fast and simple PCA via convex optimization. arXiv preprint

arXiv:1509.05647.

[13] Ge  R.  Huang  F.  Jin  C.  & Yuan  Y. (2015). Escaping from saddle points – online stochastic gradient for

tensor decomposition. In Proceedings of The 28th Conference on Learning Theory (pp. 797–842).

[14] Hardt  M. & Price  E. (2014). The noisy power method: A meta algorithm with applications. In Advances

in Neural Information Processing Systems (pp. 2861–2869).

[15] Hardt  Moritz & Price  Eric (2014). The Noisy Power Method: A Meta Algorithm with Applications.

NIPS  (pp. 2861–2869).

[16] Helmke  U. & Moore  J. B. (1994). Optimization and Dynamical Systems. Springer.

[17] Jain  P.  Jin  C.  Kakade  S. M.  Netrapalli  P.  & Sidford  A. (2016). Matching matrix bernstein with little

memory: Near-optimal ﬁnite sample guarantees for oja’s algorithm. arXiv preprint arXiv:1602.06929.

[18] Johnstone  I. M. & Lu  A. Y. (2009). On Consistency and Sparsity for Principal Components Analysis in

High Dimensions. Journal of the American Statistical Association  104(486)  682–693.

[19] Krasulina  T. (1969). The method of stochastic approximation for the determination of the least eigenvalue

of a symmetrical matrix. USSR Computational Mathematics and Mathematical Physics  9(6)  189–195.

[20] Kuczynski  J. & Wozniakowski  H. (1992). Estimating the largest eigenvalue by the power and lanczos

algorithms with a random start. SIAM journal on matrix analysis and applications  13(4)  1094–1122.

[21] Lee  J. D.  Simchowitz  M.  Jordan  M. I.  & Recht  B. (2016). Gradient descent only converges to

minimizers. In Conference on Learning Theory (pp. 1246–1257).

[22] Levin  D. A.  Peres  Y.  & Wilmer  E. L. (2009). Markov chains and mixing times. American Mathematical

Society.

[23] Li  C. J.  Wang  M.  Liu  H.  & Zhang  T. (2016). Near-optimal stochastic approximation for online

principal component estimation. arXiv preprint arXiv:1603.05305.

[24] Li  Q.  Tai  C.  & E  W. (2015). Dynamics of stochastic gradient algorithms.

arXiv:1511.06251.

arXiv preprint

[25] Ma  Z. (2013). Sparse principal component analysis and iterative thresholding. The Annals of Statistics 

41(2)  772–801.

[26] Mandt  S.  Hoffman  M. D.  & Blei  D. M. (2016). A variational analysis of stochastic gradient algorithms.

arXiv preprint arXiv:1602.02666.

[27] Mitliagkas  I.  Caramanis  C.  & Jain  P. (2013). Memory limited  streaming PCA. In Advances in Neural

Information Processing Systems (pp. 2886–2894).

10

[28] Musco  C. & Musco  C. (2015). Stronger approximate singular value decomposition via the block lanczos

and power methods. arXiv preprint arXiv:1504.05477.

[29] Nadler  B. (2008). Finite sample approximation results for principal component analysis: A matrix

perturbation approach. The Annals of Statistics  41(2)  2791–2817.

[30] Oja  E. (1982). Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical

biology  15(3)  267–273.

[31] Oja  E. & Karhunen  J. (1985). On stochastic approximation of the eigenvectors and eigenvalues of the

expectation of a random matrix. Journal of mathematical analysis and applications  106(1)  69–84.

[32] Oksendal  B. (2003). Stochastic Differential Equations. Springer.

[33] Panageas  I. & Piliouras  G. (2016). Gradient descent converges to minimizers: The case of non-isolated

critical points. arXiv preprint arXiv:1605.00405.

[34] Shamir  O. (2015a). Convergence of stochastic gradient descent for PCA. arXiv preprint arXiv:1509.09002.

[35] Shamir  O. (2015b). Fast stochastic algorithms for svd and PCA: Convergence properties and convexity.

arXiv preprint arXiv:1507.08788.

[36] Shamir  O. (2015c). A stochastic PCA and svd algorithm with an exponential convergence rate. In

Proceedings of the 32nd International Conference on Machine Learning (ICML-15) (pp. 144–152).

[37] Su  W.  Boyd  S.  & Candes  E. J. (2016). A differential equation for modeling Nesterov’s accelerated

gradient method: theory and insights. Journal of Machine Learning Research  17(153)  1–43.

[38] Sun  J.  Qu  Q.  & Wright  J. (2015a). Complete dictionary recovery over the sphere i: Overview and the

geometric picture. arXiv preprint arXiv:1511.03607.

[39] Sun  J.  Qu  Q.  & Wright  J. (2015b). Complete dictionary recovery over the sphere ii: Recovery by

Riemannian trust-region method. arXiv preprint arXiv:1511.04777.

[40] Sun  J.  Qu  Q.  & Wright  J. (2015c). When are nonconvex problems not scary?

arXiv:1510.06096.

[41] Sun  J.  Qu  Q.  & Wright  J. (2016). A geometric analysis of phase retrieval.

arXiv:1602.06664.

arXiv preprint

arXiv preprint

[42] Vu  V. Q. & Lei  J. (2012). Minimax Rates of Estimation for Sparse PCA in High Dimensions. AISTATS 

(pp. 1278–1286).

[43] Vu  V. Q. & Lei  J. (2013). Minimax sparse principal subspace estimation in high dimensions. The Annals

of Statistics  41(6)  2905–2947.

[44] Wang  Z.  Lu  H.  & Liu  H. (2014). Nonconvex statistical optimization: Minimax-optimal sparse pca in

polynomial time. arXiv preprint arXiv:1408.5352.

[45] Yuan  X.-T. & Zhang  T. (2013). Truncated power method for sparse eigenvalue problems. Journal of

Machine Learning Research  14(Apr)  899–925.

[46] Zou  H. (2006). The adaptive lasso and its oracle properties. Journal of the American statistical association 

101(476)  1418–1429.

11

,Marylou Gabrie
Eric Tramel
Florent Krzakala
Chris Junchi Li
Mengdi Wang
Han Liu
Tong Zhang
Shupeng Su
Chao Zhang
Kai Han
Yonghong Tian