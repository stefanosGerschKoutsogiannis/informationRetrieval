2010,Graph-Valued Regression,Undirected graphical models encode in a graph $G$ the dependency structure of a random vector $Y$. In many applications  it is of interest to model $Y$ given another random vector $X$ as input. We refer to the problem of estimating the graph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression''. In this paper  we propose a semiparametric method for estimating $G(x)$ that builds a tree on the $X$ space just as in CART (classification and regression trees)  but at each leaf of the tree estimates a graph. We call the method ``Graph-optimized CART''  or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees  establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset  showing how graph-valued regression can provide a useful tool for analyzing complex data.,Graph-Valued Regression

Han Liu Xi Chen John Lafferty Larry Wasserman

Carnegie Mellon University

Pittsburgh  PA 15213

Abstract

Undirected graphical models encode in a graph G the dependency structure of a
random vector Y . In many applications  it is of interest to model Y given an-
other random vector X as input. We refer to the problem of estimating the graph
G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper 
we propose a semiparametric method for estimating G(x) that builds a tree on the
X space just as in CART (classiﬁcation and regression trees)  but at each leaf of
the tree estimates a graph. We call the method “Graph-optimized CART”  or Go-
CART. We study the theoretical properties of Go-CART using dyadic partitioning
trees  establishing oracle inequalities on risk minimization and tree partition con-
sistency. We also demonstrate the application of Go-CART to a meteorological
dataset  showing how graph-valued regression can provide a useful tool for ana-
lyzing complex data.

1

Introduction

Let Y be a p-dimensional random vector with distribution P . A common way to study the structure
of P is to construct the undirected graph G = (V  E)  where the vertex set V corresponds to the
p components of the vector Y . The edge set E is a subset of the pairs of vertices  where an edge
between Yj and Yk is absent if and only if Yj is conditionally independent of Yk given all the
other variables. Suppose now that Y and X are both random vectors  and let P (·| X) denote the
conditional distribution of Y given X. In a typical regression problem  we are interested in the
conditional mean µ(x) = E (Y | X = x). But if Y is multivariate  we may be also interested in
how the structure of P (·| X) varies as a function of X. In particular  let G(x) be the undirected
graph corresponding to P (·| X = x). We refer to the problem of estimating G(x) as graph-valued
regression.
Let G = {G(x) : x ∈ X} be a set of graphs indexed by x ∈ X   where X is the domain of X.
Then G induces a partition of X   denoted as X1  . . .  Xm  where x1 and x2 lie in the same partition
element if and only if G(x1) = G(x2). Graph-valued regression is thus the problem of estimating
the partition and estimating the graph within each partition element.

We present three different partition-based graph estimators; two that use global optimization  and
one based on a greedy splitting procedure. One of the optimization based schemes uses penalized
empirical risk minimization  the other uses held-out risk minimization. As we show  both methods
enjoy strong theoretical properties under relatively weak assumptions; in particular  we establish
oracle inequalities on the excess risk of the estimators  and tree partition consistency (under stronger
assumptions) in Section 4. While the optimization based estimates are attractive  they do not scale
well computationally when the input dimension is large. An alternative is to adapt the greedy algo-
rithms of classical CART  as we describe in Section 3. In Section 5 we present experimental results
on both synthetic data and a meteorological dataset  demonstrating how graph-valued regression can
be an effective tool for analyzing high dimensional data with covariates.

1

(cid:3)

(cid:4)

(cid:2)
(cid:1)Ω = arg min

2 Graph-Valued Regression
Let y1  . . .   yn be a random sample of vectors from P   where each yi ∈ R
p. We are interested in
the case where p is large and  in fact  may diverge with n asymptotically. One way to estimate G
from the sample is the graphical lasso or glasso [13  5  1]  where one assumes that P is Gaussian
with mean µ and covariance matrix Σ. Missing edges in the graph correspond to zero elements in
the precision matrix Ω = Σ−1 [12  4  7]. A sparse estimate of Ω is obtained by solving

tr(SΩ) − log |Ω| + λ(cid:2)Ω(cid:2)1

(1)
j k |Ωjk| is the

Ω(cid:1)0

where Ω is positive deﬁnite  S is the sample covariance matrix  and (cid:2)Ω(cid:2)1 =

elementwise (cid:2)1-norm of Ω. A fast algorithm for ﬁnding(cid:1)Ω was given by Friedman et al. [5]  which
The theoretical properties of(cid:1)Ω have been studied by Rothman et al. [10] and Ravikumar et al. [9].

involves estimating a single row (and column) of Ω in each iteration by solving a lasso regression.

In practice  it seems that the glasso yields reasonable graph estimators even if Y is not Gaussian;
however  proving conditions under which this happens is an open problem.
We brieﬂy mention three different strategies for estimating G(x)  the graph of Y conditioned on
X = x  each of which builds upon the glasso.
Parametric Estimators. Assume that Z = (X  Y ) is jointly multivariate Gaussian with covariance
matrix Σ =
. We can estimate ΣX  ΣY   and ΣXY by their corresponding sample

quantities (cid:1)ΣX  (cid:1)ΣY   and (cid:1)ΣXY   and the marginal precision matrix of X  denoted as ΩX  can be
Gaussian formulas. In particular  the conditional covariance matrix of Y | X is (cid:1)ΣY |X = (cid:1)ΣY −
(cid:1)ΣXY and a sparse estimate of(cid:1)ΩY |X can be obtained by directly plugging(cid:1)ΣY |X into glasso.
(cid:1)ΣY X

estimated using the glasso. The conditional distribution of Y given X = x is obtained by standard

ΣX ΣXY
ΣY X
ΣY

(cid:1)ΩX

However  the estimated graph does not vary with different values of X.
Kernel Smoothing Estimators. We assume that Y given X is Gaussian  but without making any
assumption about the marginal distribution of X. Thus Y | X = x ∼ N(µ(x)  Σ(x)). Under
the assumption that both µ(x) and Σ(x) are smooth functions of x  we estimate Σ(x) via kernel

 

!

(cid:1)µ(x) =

smoothing:(cid:1)Σ(x) =

n(cid:5)

i=1

K

(cid:6)(cid:2)x − xi(cid:2)
n(cid:5)

h

(cid:7)
(yi −(cid:1)µ(x)) (yi −(cid:1)µ(x))T
(cid:6)(cid:2)x − xi(cid:2)

(cid:8) n(cid:5)

(cid:8) n(cid:5)
(cid:6)(cid:2)x − xi(cid:2)

(cid:7)

i=1

K

(cid:6)(cid:2)x − xi(cid:2)
(cid:7)

h

(cid:7)

where K is a kernel (e.g. the probability density function of the standard Gaussian distribution)  (cid:2)·(cid:2)
is the Euclidean norm  h > 0 is a bandwidth and

.

h

h

K

K

yi

i=1

Now we apply glasso in (1) with S =(cid:1)Σ(x) to obtain an estimate of G(x). This method is appealing
X1  . . .  Xm. Within each Xj  we apply the glasso to get an estimated graph (cid:1)Gj. We then take
(cid:1)G(x) = (cid:1)Gj for all x ∈ Xj. To ﬁnd the partition  we appeal to the idea used in CART (classiﬁcation

because it is simple and very similar to nonparametric regression smoothing; the method was ana-
lyzed for one-dimensional X in [14]. However  while it is easy to estimate G(x) at any given x  it
requires global smoothness of the mean and covariance functions.
In this approach  we partition X into ﬁnitely many connected regions
Partition Estimators.

i=1

and regression trees) [3]. We take the partition elements to be recursively deﬁned hyperrectangles.
As is well-known  we can then represent the partition by a tree  where each leaf node corresponds to
a single partition element. In CART  the leaves are associated with the means within each partition
element; while in our case  there will be an estimated undirected graph for each leaf node. We refer
to this method as Graph-optimized CART  or Go-CART. The remainder of this paper is devoted to
the details of this method.

3 Graph-Optimized CART
Let X ∈ R
p be two random vectors  and let {(x1  y1)  . . .   (xn  yn)} be n i.i.d. samples
from the joint distribution of (X  Y ). The domains of X and Y are denoted by X and Y respectively;

d and Y ∈ R

2

d → R

d → R

p is a vector-valued mean function and Σ : R

p×p is a matrix-valued
where µ : R
covariance function. We also assume that for each x  Ω(x) = Σ(x)−1 is a sparse matrix  i.e.  many
elements of Ω(x) are zero. In addition  Ω(x) may also be a sparse function of x  i.e.  Ω(x) = Ω(xR)
for some R ⊂ {1  . . .   d} with cardinality |R| (cid:6) d. The task of graph-valued regression is to ﬁnd

a sparse inverse covariance (cid:1)Ω(x) to estimate Ω(x) for any x ∈ X ; in some situations the graph of
nected regions X1  . . .  Xm  and within each Xj we apply the glasso to estimate a graph (cid:1)Gj. We
then take (cid:1)G(x) = (cid:1)Gj for all x ∈ Xj. To ﬁnd the partition  we restrict ourselves to dyadic splits 

Ω(x) is of greater interest than the entries of Ω(x) themselves.
Go-CART is a partition based conditional graph estimator. We partition X into ﬁnitely many con-

as studied by [11  2]. The primary reason for such a choice is the computational and theoretical
tractability of dyadic partition based estimators.
Let T denote the set of dyadic partitioning trees (DPTs) deﬁned over X = [0  1]d  where each
(cid:9)d
DPT T ∈ T is constructed by recursively dividing X by means of axis-orthogonal dyadic splits.
Each node of a DPT corresponds to a hyperrectangle in [0  1]d. If a node is associated to the hyper-
rectangle A =
l=1[al  bl]  then after being dyadically split along dimension k  the two children
are associated with the sub-hyperrectangles A(k)
l>k[al  bl] and
A(k)
L . Given a DPT T   we denote by Π(T ) = {X1  . . .  XmT
R = A\A(k)
} the partition of X induced
by the leaf nodes of T . For a dyadic integer N = 2K  we deﬁne TN to be the collection of all DPTs
such that no partition has a side length smaller than 2−K. Let I(·) denote the indicator function. We
denote µT (x) and ΩT (x) as the piecewise constant mean and precision functions associated with T :

l<k[al  bl] × [ak  ak+bk

] ×(cid:9)

(cid:9)

L =

2

µT (x) =
∈ R

j=1
p and ΩXj

µXj
∈ R

· I (x ∈ Xj)  
· I (x ∈ Xj) and ΩT (x) =
p×p are the mean vector and precision matrix for Xj.

ΩXj

j=1

R(T  µT   ΩT ) =

where µXj
Before formally deﬁning our graph-valued regression estimators  we require some further deﬁni-
tions. Given a DPT T with an induced partition Π(T ) = {Xj}mT
j=1 and corresponding mean and
precision functions µT (x) and ΩT (x)  the negative conditional log-likelihood risk R(T  µT   ΩT )

and its sample version (cid:1)R(T  µT   ΩT ) are deﬁned as follows:
(cid:1)R(T  µT   ΩT ) =

(cid:13)
(Y − µXj )(Y − µXj )T
(cid:13)

− log |ΩXj
(cid:4)
− log |ΩXj
|
(3)
Let [[T ]] > 0 denote a preﬁx code over all DPTs T ∈ TN satisfying
T∈TN 2−[[T ]] ≤ 1. One
such preﬁx code [[T ]] is proposed in [11]  and takes the form [[T ]] = 3|Π(T )| − 1 + (|Π(T )| −
1) log d/ log 2. A simple upper bound for [[T ]] is

(cid:17)
(cid:17)
· I (xi ∈ Xj)

(cid:10)(cid:11)
mT(cid:5)
(cid:10)(cid:11)
mT(cid:5)
n(cid:5)

(yi − µXj )(yi − µXj )T

· I (X ∈ Xj)

(cid:14)(cid:15)
(cid:14)(cid:15)

(cid:16)
(cid:16)

(cid:12)
(cid:12)

ΩXj

ΩXj

1
n

(2)

i=1

j=1

j=1

tr

tr

|

 

.

E

[[T ]] ≤ (3 + log d/ log 2)|Π(T )|.

(4)
Our analysis will assume that the conditional means and precision matrices are bounded in the
(cid:2) · (cid:2)∞ and (cid:2) · (cid:2)1 norms; speciﬁcally we suppose there is a positive constant B and a sequence
L1 n  . . .   LmT  n  where each Lj n ∈ R+ is a function of the sample size n  and we deﬁne the
domains of each µXj and ΩXj as

mT(cid:5)

mT(cid:5)

Mj = {µ ∈ R
Ω ∈ R
Λj =

(cid:2)
(cid:19)mbT

(cid:3)
p : (cid:2)µ(cid:2)∞ ≤ B}  
p×p : Ω is positive deﬁnite  symmetric  and (cid:2)Ω(cid:2)1 ≤ Lj n
(cid:21)
(cid:20)(cid:1)R(T  µT   ΩT ) + pen(T )

∈Mj  ΩXj

∈Λj

With this notation in place  we can now deﬁne two estimators.
Deﬁnition 1. The penalized empirical risk minimization Go-CART estimator is deﬁned as

(cid:18)(cid:1)µbXj

(cid:1)T  

 (cid:1)ΩbXj

.

(5)

and for simplicity we take X = [0  1]d. We assume that

Y | X = x ∼ Np(µ(x)  Σ(x))

where (cid:1)R is deﬁned in (3) and pen(T ) = γn · mT

= argminT∈TN  µXj

j=1

(cid:22)

[[T ]] log 2+2 log(np)

n

.

3

Empirically  we may always set the dyadic integer N to be a reasonably large value; the regulariza-
tion parameter γn is responsible for selecting a suitable DPT T ∈ TN .
We also formulate an estimator that minimizes held-out risk. Practically  we could split the data into
two partitions: D1 = {(x1  y1)  . . .   (xn1   yn1)} for training and D2 = {((x(cid:4)
n2))}
1)  . . .   (x(cid:4)
  y(cid:4)
  y(cid:4)
n2
for validation with n1 + n2 = n. The held-out negative log-likelihood risk is then given by
(cid:19)
(cid:16)
i ∈ Xj)

i − µXj )(y(cid:4)
(y(cid:4)

− log |ΩXj

i − µXj )T

· I (x(cid:4)

ΩXj

(6)

tr

|

1

.

(cid:13)

1
n2

mT(cid:5)

n2(cid:5)

(cid:1)Rout(T  µT   ΩT ) =
(cid:18)(cid:11)
(cid:12)
(cid:1)µT  (cid:1)ΩT = argminµXj
(cid:1)T = argminT∈TN

j=1

i=1

(cid:14)(cid:15)
(cid:1)R(T  µT   ΩT )

∈Λj

Deﬁnition 2. For each DPT T deﬁne

∈Mj  ΩXj

minimization Go-CART estimator is

where (cid:1)R is deﬁned in (3) but only evaluated on D1 = {(x1  y1)  . . .   (xn1   yn1)}. The held-out risk
where (cid:1)Rout is deﬁned in (6) but only evaluated on D2.
((cid:1)T  (cid:1)µT  (cid:1)ΩT ). We focus on the held-out risk minimization form as in Deﬁnition 2  due to its superior

The above procedures require us to ﬁnd an optimal dyadic partitioning tree within TN . Although
dynamic programming can be applied  as in [2]  the computation does not scale to large input dimen-
sions d. We now propose a simple yet effective greedy algorithm to ﬁnd an approximate solution

(cid:1)Rout(T (cid:1)µT  (cid:1)ΩT ).

i=1

i=1

i=1

i=1

(cid:13)

n1(cid:5)

(cid:4)n1

empirical performance. But note that our greedy approach is generic and can easily be adapted to
the penalized empirical risk minimization form.
First  consider the simple case that we are given a dyadic tree structure T which induces a partition
Π(T )={X1  . . .  XmT
} on X . For any partition element Xj  we estimate the sample mean using D1:
(cid:1)µXj =
(cid:4)n1

The glasso is then used to estimate a sparse precision matrix (cid:1)ΩXj . More precisely  let (cid:1)ΣXj be the
(cid:14)T · I (xi ∈ Xj) .
yi −(cid:1)µXj
The estimator(cid:1)ΩXj is obtained by optimizing(cid:1)ΩXj = arg minΩ(cid:1)0
{tr((cid:1)ΣXj Ω) − log |Ω| + λj(cid:2)Ω(cid:2)1} 

sample covariance matrix for the partition element Xj  given by

yi · I (xi ∈ Xj) .
(cid:14)(cid:13)

1
I (xi ∈ Xj)
n1(cid:5)

yi −(cid:1)µXj

1
I (xi ∈ Xj)

(cid:1)ΣXj =

where λj is in one-to-one correspondence with Lj n in (5). In practice  we run the full regularization
path of the glasso  from large λj  which yields very sparse graph  to small λj  and select the graph
that minimizes the held-out negative log-likelihood risk. To further improve the model selection per-
formance  we reﬁt the parameters of the precision matrix after the graph has been selected. That is 
to reduce the bias of the glasso  we ﬁrst estimate the sparse precision matrix using (cid:2)1-regularization 
and then we reﬁt the Gaussian model without (cid:2)1-regularization  but enforcing the sparsity pattern
obtained in the ﬁrst step.
The natural  standard greedy procedure starts from the coarsest partition X = [0  1]d and then
computes the decrease in the held-out risk by dyadically splitting each hyperrectangle A along
dimension k ∈ {1  . . . d}. The dimension k∗
that results in the largest decrease in held-out risk is
selected  where the change in risk is given by

∆(cid:1)R(k)
out(A (cid:1)µA (cid:1)ΩA) = (cid:1)Rout(A (cid:1)µA (cid:1)ΩA) − (cid:1)Rout(A(k)

).
If splitting any dimension k of A leads to an increase in the held-out risk  the element A should no
longer be split and hence becomes a partition element of Π(T ). The details and pseudo code are
provided in the supplementary materials.

) − (cid:1)Rout(A(k)

R  (cid:1)µA(k)

L  (cid:1)µA(k)

 (cid:1)ΩA(k)

 (cid:1)ΩA(k)

R

R

L

L

This greedy partitioning method parallels the classical algorithms for classiﬁcation and regression
that have been used in statistical learning for decades. However  the strength of the procedures given
in Deﬁnitions 1 and 2 is that they lend themselves to a theoretical analysis under relatively weak
assumptions  as we show in the following section. The theoretical properties of greedy Go-CART
are left to future work.

4

4 Theoretical Properties
We deﬁne the oracle risk R∗

over TN as
∗
T   Ω
T ) =

= R(T ∗  µ∗

R∗

T∈TN  µXj

inf
∈Mj  ΩXj

∈Λj

R(T  µT   ΩT ).

  µ∗

T ∗  and Ω∗

Note that T ∗
T ∗ might not be unique  since the ﬁnest partition always achieves the oracle
risk. To obtain oracle inequalities  we make the following two technical assumptions.
Assumption 1. Let T ∈ TN be an arbitrary DPT which induces a partition Π(T ) =
{X1  . . .  XmT

} on X   we assume that there exists a constant B  such that

where Λj is deﬁned in (5) and Ln = max1≤j≤mT
assume that

Assumption 2. Let Y = (Y1  . . .   Yp)T ∈ R

max
1≤j≤mT

sup
Ω∈Λj

(cid:2)µXj

log |Ω| ≤ Ln

(cid:2)∞ ≤ B and max
1≤j≤mT
Lj n  where Lj n is the same as in (5). We also
√
n).
Ln = o(
p. For any A ⊂ X   we deﬁne
Zk(cid:1)(A) = YkY(cid:1) · I(X ∈ A) − E(YkY(cid:1) · I(X ∈ A))
Zj(A) = Yj · I(X ∈ A) − E(Yj · I(X ∈ A)).

sup

We assume there exist constants M1  M2  v1  and v2  such that

k (cid:1) A E|Zk(cid:1)(A)|m ≤ m!M m−2

j A E|Zj(A)|m ≤ m!M m−2
for all m ≥ 2.
Theorem 1. Let T ∈ TN be a DPT that induces a partition Π(T ) = {X1  . . .  XmT

δ ∈ (0  1/4)  let (cid:1)T  (cid:1)µbT  (cid:1)ΩbT be the estimator obtained using the penalized empirical risk minimiza-

} on X . For any

and sup

v1

v2

2
2

1
2

tion Go-CART in Deﬁnition 1  with a penalty term pen(T ) of the form

(cid:23)

pen(T ) = (C1 + 1)LnmT
√
v2 + 8B
where C1 = 8

√
v1 + B2. Then for sufﬁciently large n  the excess risk inequality

n

[[T ]] log 2 + 2 log p + log(48/δ)

(cid:25)

(cid:24)

2pen(T ) +

inf

∈Mj  ΩXj

∈Λj

µXj

(R(T  µT   ΩT ) − R∗

)

R((cid:1)T  (cid:1)µbT  (cid:1)ΩbT ) − R∗ ≤ inf

T∈TN
holds with probability at least 1 − δ.

√

2v1 +

(cid:23)

} on X . We

A similar oracle inequality holds when using the held-out risk minimization Go-CART.
Theorem 2. Let T ∈ TN be a DPT which induces a partition Π(T ) = {X1  . . .  XmT
deﬁne φn(T ) to be a function of n and T such that

[[T ]] log 2 + 2 log p + log(384/δ)

√
2)LnmT
√
2B2 and Ln = max1≤j≤mT
Lj n. Partition the data into
n2)} with sizes n1 = n2 =
  y(cid:4)

n
  y(cid:4)
1)  . . .   (x(cid:4)

φn(T ) = (C2 +
√
where C2 = 8
2v2 + 8B
D1 = {(x1  y1)  . . .   (xn1   yn1)} and D2 = {(x(cid:4)

Deﬁnition 2. Then  for sufﬁciently large n  the excess risk inequality

n/2. Let (cid:1)T  (cid:1)µbT  (cid:1)ΩbT be the estimator constructed using the held-out risk minimization criterion of
R((cid:1)T  (cid:1)µbT  (cid:1)ΩbT ) − R∗ ≤ inf
due to the extra φn((cid:1)T ) term  which depends on the complexity of the ﬁnal estimate (cid:1)T . Due to space

Note that in contrast to the statement in Theorem 1  Theorem 2 results in a stochastic upper bound

T∈TN
with probability at least 1 − δ.

(R(T  µT   ΩT ) − R∗

+ φn((cid:1)T )

3φn(T ) +

∈Mj  ΩXj

(cid:25)

(cid:24)

∈Λj

inf

µXj

n2

limitations  the proofs of both theorems are detailed in the supplementary materials.
We now temporarily make the strong assumption that the model is correct  so that Y given X is
conditionally Gaussian  with a partition structure that is given by a dyadic tree. We show that with
high probability  the true dyadic partition structure can be correctly recovered.

)

1

5

Under this assumption  clearly
R(T ∗  µ∗

mT(cid:5)
where MT is given by

(cid:18)

MT =

µ(x) =

∗
T ∗   Ω
T ∗) =

T∈TN  µT  ΩT ∈MT

R(T  µT   ΩT ) 

inf

mT(cid:5)

(7)

(cid:19)

.

Assumption 3. The true model is
∗
T ∗(x))
T ∗(x)  Ω
mT ∗(cid:5)
where T ∗ ∈ TN is a DPT with induced partition Π(T ∗) = {X ∗
j }mT ∗
j I(x ∈ X ∗
∗
j ).
Ω

Y | X = x ∼ Np(µ∗
mT ∗(cid:5)

∗
T ∗(x) =
j )  Ω

j I(x ∈ X ∗
µ∗

µ∗
T ∗(x) =

j=1 and

j=1

j=1

I(x ∈ Xj)  Ω(x) =

µXj

I(x ∈ Xj) : µXj

∈ Mj  ΩXj

∈ Λj

ΩXj

j=1

j=1

Let T1 and T2 be two DPTs  if Π(T1) can be obtained by further split the hyperrectangles within
Π(T2)  we say Π(T2) ⊂ Π(T1). We then have the following deﬁnitions:

Deﬁnition 3. A tree estimation procedure (cid:1)T is tree partition consistent in case

(cid:11)

Π(T ∗

P

(cid:16)
) ⊂ Π((cid:1)T )

→ 1 as n → ∞.

Note that the estimated partition may be ﬁner than the true partition. Establishing a tree parti-
tion consistency result requires further technical assumptions. The following assumption speciﬁes
that for arbitrary adjacent subregions of the true dyadic partition  either the means or the variances
should be sufﬁciently different. Without such an assumption  of course  it is impossible to detect the
boundaries of the true partition.
Assumption 4. Let X ∗
i and X ∗
j be adjacent partition elements of T ∗
= (Ω∗
. Let Σ∗
parent node within T ∗
X ∗
X ∗
such that either
+ Σ∗
X ∗
2

  so that they have a common
)−1. We assume there exist positive constants c1  c2  c3  c4 

(cid:26)(cid:26)(cid:26)(cid:26)(cid:26) − log |Σ

| − log |Σ
∗
X ∗

(cid:26)(cid:26)(cid:26)(cid:26)(cid:26)Σ∗

| ≥ c4

2 log

∗
X ∗

X ∗

j

j

i

i

i

i

or (cid:2)µ∗
X ∗

i

− µ∗
X ∗

j

(cid:2)2

2

≥ c3. We also assume

∗
ρmin(Ω
X ∗

) ≥ c1 

∀j = 1  . . .   mT ∗  

where ρmin(·) denotes the smallest eigenvalue. Furthermore  for any T ∈ TN and any A ∈ Π(T ) 
we have P (X ∈ A) ≥ c2.

j

Theorem 3. Under the above assumptions  we have

inf

T∈TN   Π(T ∗)(cid:1)Π(T )

inf

µT   ΩT ∈MT

R(T  µT   ΩT ) − R(T ∗  µ∗

T ∗) > min{ c1c2c3
∗
T ∗   Ω

  c2c4}

2

where c1  c2  c3  c4 are deﬁned in Assumption 4. Moreover  the Go-CART estimator in both the
penalized risk minimization and held-out risk minimization form is tree partition consistent.
This result shows that  with high probability  we obtain a ﬁner partition than T ∗
; the assumptions
do not  however  control the size of the resulting partition. The proof of this result appears in the
supplementary material.

5 Experiments

We now present the performance of the greedy partitioning algorithm of Section 3 on both synthetic
data and a real meteorological dataset. In the experiment  we always set the dyadic integer N = 210
to ensure that we can obtain ﬁne-tuned partitions of the input space X .
5.1 Synthetic Data
We generate n data points x1  . . .   xn ∈ R
d with n = 10  000 and d = 10 uniformly distributed on
the unit hypercube [0  1]d. We split the square [0  1]2 deﬁned by the ﬁrst two dimension of the unit

6

20

1

2

3

19

18

17

16

15

14

13

12

11

9

10

4

8

5

6

7

 1

20

1

2

19

18

3

4

5

6

7

8

 X1<
 0.5

17

16

15

1

20

2

14

3

4

13

12

11

9

10

5

6

7

8

12

11

9

10

 2
 X2<
 0.5

 4

17

16

15

19

18

14

13

 X1>
 0.5

20

1

2

3

5

6

7

4

8

12

11

9

10

19

18

17

16

15

14

13

 3

 X2>
 0.5

 7

 X2<
 0.25

 X2>
 0.25

 X1<
 0.75

 X1>
 0.75

 8
 X1<
 0.25

 9

 X1>
 0.25

 10

 X2<
 0.75

 11

 X2>
 0.75

 X2>
 0.5

 X2<
 0.5

19

18

17

16

15

14

3

13

4

5

6

7

8

9

1

20

2

19

13

2

12

11

10

17

16

15

18

14

1

20

19

3

9

4

8

5

6

7

5

6

7

4

8

18

17

16

15

14

13

19

18

17

16

15

12

11

10

20

1

2

3

14

13

12

11

9

10

20

1

2

3

9

5

6

7

4

8

12

10

11

1

20

2

19

18

3

4

17

16

15

5

6

7

14

13

8

9

12

11

10

20

1

2

17

16

15

19

18

14

13

12

10

11

1

20

19

18

17

16

15

3

9

2

5

6

7

4

8

3

4

5

6

7

8

5

41

40

43

42

18

17

38 39

36 37

33

32

35

34

13

14

30 31

28 29

6

(b)

k
s
R

i

 
t

u
o
−
d
e
H

l

21.3

21.2

21.1

21

20.9

20.8
0

5

10

15

20

25

Splitting Sequence No.

(c)

 12

 X2<
 0.125

 X2>
 0.125

 X1>
 0.25

 X1<
 0.25

 15

 X1<
 0.375

 X1>
 0.375

 16

 X2<
 0.625

 X2>
 0.625

 X2>
 0.75

 X2<
 0.75

 19

 X1<
 0.875

 X1>
 0.875

 20

 21

 X1<
 0.125

 X1>
 0.125

 X1<
 0.125

 X1>
 0.125

 22

 23

 X2<
 0.375

 X2>
 0.375

 X2<
 0.375

 X2>
 0.375

 24

 25

 X1<
 0.625

 X1>
 0.625

 X1<
 0.625

 X1>
 0.625

 26

 27

 X2<
 0.875

 X2>
 0.875

 X2<
 0.875

 X2>
 0.875

 28

 29

 30

 31

 13

 14

 32

 33

 34

 35

 5

 6

 36

 37

 38

 39

 17

 18

 40

 41

 42

 43

14

13

9

12

11

10

20

1

2

1

20

2

20

1

2

17

16

15

19

18

14

13

3

4

19

18

5

6

7

8

17

16

15

14

13

12

11

9

10

3

9

5

6

7

4

8

17

16

15

19

18

14

13

12

11

10

12

11

10

20

1

2

3

19

18

14

13

12

11

9

10

4

8

5

6

7

20

19

1

2

3

18

17

16

15

14

13

9

12

10

11

4

8

5

6

7

17

16

15

19

18

14

13

20

1

2

3

4

5

6

7

8

9

12

10

11

3

4

8

9

5

6

7

17

16

15

(a)

Figure 1: Analysis of synthetic data. (a) Estimated dyadic tree structure; (b) Ground true partition. The hori-
zontal axis corresponds to the ﬁrst dimension denoted as X1 while the vertical axis corresponds to the second
dimension denoted by X2. The bottom left point corresponds to [0  0] and the upper right point corresponds to
[1  1]. It is also the induced partition on [0  1]2. The number labeled on each subregion corresponds to each leaf
node ID of the tree in (a); (c) The held-out negative log-likelihood risk for each split. The order of the splits
corresponds the ID of the tree node (from small to large).
hypercube into 22 subregions as shown in Figure 1 (b). For the t-th subregion where 1 ≤ t ≤ 22 
we generate an Erd¨os-R´enyi random graph Gt = (V t  Et) with the number of vertices p = 20 
the number of edges |E| = 10 and the maximum node degree is four. Based on Gt  we generate
i j = I(i = j) + 0.245 · I((i  j) ∈ Et)  where
the inverse covariance matrix Ωt according to Ωt
0.245 guarantees the positive deﬁniteness of Ωt when the maximum node degree is 4. For each data
point xi in the t-th subregion  we sample a 20-dimensional response vector yi from a multivariate
Gaussian distribution N20
. We also create an equally-sized held-out dataset in the same
manner based on {Ωt}22
t=1.
The learned dyadic tree structure and its induced partition are presented in Figure 1. We also provide
the estimated graphs for some nodes. We conduct 100 monte-carlo simulations and ﬁnd that 82 times
out of 100 runs our algorithm perfectly recover the ground true partitions on the X1-X2 plane and
never wrongly split any irrelevant dimensions ranging from X3 to X10. Moreover  the estimated
graphs have interesting patterns. Even though the graphs within each subregion are sparse  the
estimated graph obtained by pooling all the data together is highly dense. As the greedy algorithm
proceeds  the estimated graphs become sparser and sparser. However  for the immediate parent
of the leaf nodes  the graphs become denser again. Out of the 82 simulations where we correctly
identify the tree structure  we list the graph estimation performance for subregions 28  29  13  14  5 
6 in terms of precision  recall  and F1-score in Table 1.

(cid:14)−1

(cid:13)

(cid:13)

(cid:14)

Ωt

0 

Table 1: The graph estimation performance over different subregions

Mean values over 100 runs (Standard deviation)

subregion

region 28

region 29

region 13

region 14

region 5

region 6

Precision
Recall
F1 − score

0.8327 (0.15)
0.7890 (0.16)
0.7880 (0.11)

0.8429 (0.15)
0.7990 (0.18)
0.7923 (0.12)

0.9853 (0.04)
1.0000 (0.00)
0.9921 (0.02)

0.9821 (0.05)
1.0000 (0.00)
0.9904 (0.03)

0.9906 (0.04)
1.0000 (0.00)
0.9949 (0.02)

0.9899 (0.05)
1.0000 (0.00)
0.9913 (0.02)

We see that for a larger subregion (e.g. 13  14  5  6)  it is easier to obtain better recovery perfor-
mance; while good recovery for a very small region (e.g. 28  29) becomes more challenging. We
also plot the held-out risk in the subplot (c). As can be seen  the ﬁrst few splits lead to the most
signiﬁcant decreases of the held-out risk. The whole risk curve illustrates a diminishing return be-
havior. Correctly splitting the large rectangle leads to a signiﬁcant decrease in the risk; in contrast 
splitting the middle rectangles does not reduce the risk as much. We also conducted simulations
where the true conditional covariance matrix is a continuous function of x; these are presented in
the supplementary materials.

7

15

5

13

17

16

42

44

41

43

18 19

21

20

60

62

9

8

59

61

6

39 40

46

48

45

47

51 52

56

58

49 50

55

57

24

23

26

25

66

28

65

64

27

63

34

36

14

38

31

32

33

35

37

4

3

10

12

29

30

11

53

54

22

1

2

7

CO2

DIR

CH4

CO2

DIR

CH4

CO2

DIR

CH4

GLO

CO

GLO

CO

GLO

CO

TMX

TMP

TMN

DTR

TMX

H2

WET

TMP

CLD

TMN

H2

WET

CLD

TMX

TMP

TMN

H2

WET

CLD

VAP

DTR

VAP

FRS

PRE

FRS

PRE

DTR

VAP

FRS

PRE

(a)

CO2

DIR

CH4

GLO

CO

TMX

TMP

TMN

DTR

H2

WET

CLD

VAP

FRS

PRE

(b)

42

44

16

17

18

19

41

43

21

20

8

60

62

59

61

6

46

48

51

52

56

58

24

15

39

40

45

47

49

50

55

57

23

66

65

27

28

64

63

9

26

25

5

34

36

13

31

32

33

35

14

38

37

12

29

30

4

3

1

2

10

11

53

54

22

7

(c)

Figure 2: Analysis of climate data. (a) Learned partitions for the 100 locations and projected to the US map 
with the estimated graphs for subregions 3  10  and 33; (b) Estimated graph with data pooled from all 100
locations; (c) the re-scaled partition pattern induced by the learned dyadic tree structure.

5.2 Climate Data Analysis
In this section  we apply Go-CART on a meteorology dataset collected in a similar approach as in
[8]. The data contains monthly observations of 15 different meteorological factors from 1990 to
2002. We use the data from 1990 to 1995 as the training data and data from 1996 to 2002 as the
held-out validation data. The observations span 100 locations in the US between latitudes 30.475 to
47.975 and longitudes -119.75 to -82.25. The 15 meteorological factors measured for each month
include levels of CO2  CH4  H2  CO  average temperature (TMP) and diurnal temperature range
(DTR)  minimum temperate (TMN)  maximum temperature (TMX)  precipitation (PRE)  vapor (VAP) 
cloud cover (CLD)  wet days (WET)  frost days (FRS)  global solar radiation (GLO)  and direct solar
radiation (DIR).

As a baseline  we estimate a sparse graph on the data pooled from all 100 locations  using the glasso
algorithm; the estimated graph is shown in Figure 2 (b). It is seen that the greenhouse gas factor
CO2 is isolated from all the other factors. This apparently contradicts the basic domain knowledge
that CO2 should be correlated with the solar radiation factors (including GLO  DIR)  according to
the IPCC report [6] which is one of the most authoritative reports in the ﬁeld of meteorology. The
reason for the missing edges in the pooled data may be that positive correlations at one location are
canceled by negative correlations at other locations.
Treating the longitude and latitude of each site as two-dimensional covariate X  and the meteorology
data of the p = 15 factors as the response Y   we estimate a dyadic tree structure using the greedy
algorithm. The result is a partition with 66 subregions  shown in Figure 2. The graphs for subregions
3 and 10 (corresponding to the coast of California and Arizona states) are shown in subplot (a)
of Figure 2. The graphs for these two adjacent subregions are quite similar  suggesting spatial
smoothness of the learned graphs. Moreover  for both graphs  CO2 is connected to the solar radiation
factor GLO through CH4.
In contrast  for subregion 33  which corresponds to the north part of
Arizona  the estimated graph is quite different. In general  it is found that the graphs corresponding
to the locations along the coasts are sparser than those corresponding to the locations in the mainland.

Such observations  which require validation and interpretation by domain experts  are examples of
the capability of graph-valued regression to provide a useful tool for high dimensional data analysis.

8

References

[1] O. Banerjee  L. E. Ghaoui  and A. d’Aspremont. Model selection through sparse maximum

likelihood estimation. Journal of Machine Learning Research  9:485–516  March 2008.

[2] G. Blanchard  C. Sch¨afer  Y. Rozenholc  and K.-R. M¨uller. Optimal dyadic decision trees.

Mach. Learn.  66(2-3):209–241  2007.

[3] L. Breiman  J. Friedman  C. J. Stone  and R. Olshen. Classiﬁcation and regression trees.

Wadsworth Publishing Co Inc  1984.

[4] D. Edwards. Introduction to graphical modelling. Springer-Verlag Inc  1995.
[5] J. H. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the

graphical lasso. Biostatistics  9(3):432–441  2007.

[6] IPCC. Climate Change 2007–The Physical Science Basis IPCC Fourth Assessment Report.
[7] S. L. Lauritzen. Graphical Models. Oxford University Press  1996.
[8] A. C. Lozano  H. Li  A. Niculescu-Mizil  Y. Liu  C. Perlich  J. Hosking  and N. Abe. Spatial-

temporal causal modeling for climate change attribution. In ACM SIGKDD  2009.

[9] P. Ravikumar  M. Wainwright  G. Raskutti  and B. Yu. Model selection in Gaussian graph-
In Advances in Neural

ical models: High-dimensional consistency of (cid:2)1-regularized MLE.
Information Processing Systems 22  Cambridge  MA  2009. MIT Press.

[10] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance

estimation. Electronic Journal of Statistics  2:494–515  2008.

[11] C. Scott and R. Nowak. Minimax-optimal classiﬁcation with dyadic decision trees. Information

Theory  IEEE Transactions on  52(4):1335–1353  2006.

[12] J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley  1990.
[13] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model.

Biometrika  94(1):19–35  2007.

[14] S. Zhou  J. Lafferty  and L. Wasserman. Time varying undirected graphs. Machine Learning 

78(4)  2010.

9

,Ilja Kuzborskij
Nicolò Cesa-Bianchi