2019,Selecting the independent coordinates of manifolds with large aspect ratios,Many manifold embedding algorithms fail apparently when the data manifold has a large aspect ratio (such as a long  thin strip). Here  we formulate success and failure in terms of finding a smooth embedding  showing also that the problem is pervasive and more complex than previously recognized. Mathematically  success is possible under very broad conditions  provided that embedding is done by carefully selected eigenfunctions of the Laplace-Beltrami operator $\Delta_\M$. Hence  we propose a bicriterial Independent Eigencoordinate Selection (IES) algorithm that selects smooth embeddings with few eigenvectors. The algorithm is grounded in theory  has low computational overhead  and is successful on synthetic and large real data.,Selecting the independent coordinates of manifolds

with large aspect ratios

Department of Electrical & Computer Engineering

Yu-Chia Chen

University of Washington

Seattle  WA 98195
yuchaz@uw.edu

Marina Meil˘a

Department of Statistics
University of Washington

Seattle  WA 98195
mmp2@uw.edu

Abstract

Many manifold embedding algorithms fail apparently when the data manifold has
a large aspect ratio (such as a long  thin strip). Here  we formulate success and
failure in terms of ﬁnding a smooth embedding  showing also that the problem is
pervasive and more complex than previously recognized. Mathematically  success
is possible under very broad conditions  provided that embedding is done by care-
fully selected eigenfunctions of the Laplace-Beltrami operator M. Hence  we
propose a bicriterial Independent Eigencoordinate Selection (IES) algorithm that
selects smooth embeddings with few eigenvectors. The algorithm is grounded in
theory  has low computational overhead  and is successful on synthetic and large
real data.

1 Motivation

We study a well-documented deﬁciency of manifold learning algorithms. Namely  as shown in
[GZKR08]  algorithms such as Laplacian Eigenmaps (LE)  Local Tangent Space Alignment (LTSA) 
Hessian Eigenmaps (HLLE)  and Diffusion Maps (DM) fail spectacularly when the data has a large
aspect ratio  that is  it extends much more in one geodesic direction than in others. This problem 
illustrated by the strip in Figure 1  was studied in [GZKR08] from a linear algebraic perspective;
[GZKR08] show that  especially when noise is present  the problem is pervasive.
In the present paper  we revisit the problem from a differential geometric perspective. First  we de-
ﬁne failure not as distortion  but as drop in the rank of the mapping  represented by the embedding
algorithm. In other words  the algorithm fails when the map  is not invertible  or  equivalently 
when the dimension dim (M) < dimM = d  where M represents the idealized data manifold 
and dim denotes the intrinsic dimension. Figure 1 demonstrates that the problem is ﬁxed by choos-
ing the eigenvectors with care. We call this problem the Independent Eigencoordinate Selection
(IES) problem  formulate it and explain its challenges in Section 3.
Our second main contribution (Section 4) is to design a bicriterial method that will select from a set
of coordinate functions 1  . . .  m  a subset S of small size that provides a smooth full-dimensional
embedding of the data. The IES problem requires searching over a combinatorial number of sets. We
show (Section 4) how to drastically reduce the computational burden per set for our algorithm. Third 
we analyze the proposed criterion under asymptotic limit (Section 5). Finally (Section 6)  we show
examples of successful selection on real and synthetic data. The experiments also demonstrate that
users of manifold learning for other than toy data must be aware of the IES problem and have tools
for handling it. Notations table  proofs  a library of hard examples  extra experiments and analyses
are in Supplements A–H; Figure/Table/Equation references with preﬁx S are in the Supplement.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2 Background on manifold learning

Manifold learning (ML) and intrinsic geometry Suppose we observe data X 2 Rn⇥D  with data
points denoted by xi 2 RD 8 i 2 [n]  that are sampled from a smooth1 d-dimensional submanifold
M⇢ RD. Manifold Learning algorithms map xi  i 2 [n] to yi = (xi) 2 Rs  where d  s ⌧ D 
thus reducing the dimension of the data X while preserving (some of) its properties. Here we present
the LE/DM algorithm  but our results can be applied to other ML methods with slight modiﬁcation.
The DM [CL06  NLCK06] algorithm embeds the data by solving the minimum eigen-problem of the
renormalized graph Laplacian [CL06] matrix L. The desired m dimensional embedding coordinates
are obtained from the second to m + 1-th principal eigenvectors of graph Laplacian L  with 0 =
0 < 1  . . .  m  i.e.  yi = (1(xi)  . . . m(xi)) (see also Supplement B).
To analyze ML algorithms  it is useful to consider the limit of the mapping  when the data is the
entire manifold M. We denote this limit also by   and its image by (M) 2 Rm. For standard
algorithms such as LE/DM  it is known that this limit exists [CL06  BN07  HAvL05  HAvL07 
THJ10]. One of the fundamental requirements of ML is to preserve the neighborhood relations in
the original data. In mathematical terms  we require that  : M! (M) is a smooth embedding 
i.e.  that  is a smooth function (i.e. does not break existing neighborhood relations) whose Jacobian
D(x) is full rank d at each x 2M (i.e. does not create new neighborhood relations).
The pushforward Riemannian metric A smooth  does not typically preserve geometric quanti-
ties such as distances along curves in M. These concepts are captured by Riemannian geometry  and
we additionally assume that (M  g) is a Riemannian manifold  with the metric g induced from RD.
One can always associate with (M) a Riemannian metric g⇤  called the pushforward Riemannian
metric [Lee03]  which preserves the geometry of (M  g); g⇤ is deﬁned by

(1)

intrinsic dimension d

hu  vig⇤(x) =⌦D1(x)u  D1(x)v↵g(x) for all u  v 2T (x)(M)
Algorithm 1: RMETRIC
Input : Embedding Y 2 Rn⇥m  Laplacian L 
1 for all yi 2 Y  k = 1 ! m  l = 1 ! m do
[ ˜H(i)]kl =Pj6=i Lij(yjl  yil)(yjk  yik)
2
3 end
4 for i = 1 ! n do
U(i)  ⌃(i) REDUCEDRANKSVD( ˜H(i)  d)
H(i) = U(i)⌃(i)U(i)>
G(i) = U(i)⌃1(i)U(i)>

In the above  TxM  T(x)(M) are
tangent subspaces  D1(x) maps vec-
tors from T(x)(M) to TxM  and
h i is the Euclidean scalar product.
For each (xi)  the associated push-
forward Riemannian metric expressed
in the coordinates of Rm  is a sym-
metric  semi-positive deﬁnite m ⇥ m
matrix G(i) of rank d. The scalar
product hu  vig⇤(xi) takes the form
u>G(i)v. Given an embedding Y =
(X)  G(i) can be estimated by Algo-
rithm 1 (RMETRIC) of [PM13]. The
RMETRIC also returns the co-metric
H(i)  which is the pseudo-inverse of
the metric G(i)  and its Singular Value
Decomposition ⌃(i)  U(i) 2 Rm⇥d. The latter represents an orthogonal basis of T(x)((M)).
3

5
6
7
8 end
Return: G(i)  H(i) 2 Rm⇥m  U(i) 2 Rm⇥d 

IES problem  related work  and challenges

⌃(i) 2 Rd⇥d  for i 2 [n]

An example Consider a continuous two dimensional strip with width W   height H  and as-
pect ratio W/H  1  parametrized by coordinates w 2 [0  W ]  h 2 [0  H]. The eigenval-
ues and eigenfunctions of the Laplace-Beltrami operator  with von Neumann boundary condi-
tions [Str07] are k1 k2 = k1⇡
W 2
W  cos k2⇡h
H .

Eigenfunctions 1 0  0 1 are in bijection with the w  h coordinates (and give a full rank em-
bedding)  while the mapping by 1 0  2 0 provides no extra information regarding the second
dimension h in the underlying manifold (and is rank 1). Theoretically  one can choose as
coordinates eigenfunctions indexed by (k1  0)  (0  k2)  but  in practice  k1  and k2 are usually

H 2  respectively k1 k2(w  h) = cos k1⇡w

+ k2⇡

1In this paper  a smooth function or manifold will be assumed to be of class at least C3.

2

unknown  as the eigenvalues are index by their
rank 0 = 0 < 1  2 ···
. For a two
dimensional strip  it is known [Str07] that 1 0
always corresponds to 1 and 0 1 corresponds
to (dW/He). Therefore  when W/H > 2  the
mapping of the strip to R2 by 1  2 is low
rank  while the mapping by 1  dW/He is full
rank. Note that other mappings of rank 2 exist 
e.g.  1  dW/He+2 (k1 = k2 = 1 in Figure 1b).
These embeddings reﬂect progressively higher
frequencies  as the corresponding eigenvalues
grow larger.

(b)

(a)
Figure 1:
(a) Eigenfunction 1 0 versus 2 0
(curve) or 0 1 (two dimensional manifold). (b)
Eigenfunction 1 0 versus 1 1. All three mani-
folds are colored by the parameterization h.

Prior work [GZKR08] is the ﬁrst work to give the IES problem a rigurous analysis. Their paper
focuses on rectangles  and the failure illustrated in Figure 1a is deﬁned as obtaining a mapping Y =
(X) that is not afﬁnely equivalent with the original data. They call this the Price of Normalization
and explain it in terms of the variances along w and h. [DTCK18] is the ﬁrst to frame the failure
in terms of the rank of S = {k : k 2 S ✓ [m]}  calling it the repeated eigendirection problem.
They propose a heuristic  LLRCOORDSEARCH  based on the observation that if k is a repeated
eigendirection of 1 ···   k1  one can ﬁt k with local linear regression on predictors [k1] with
low leave-one-out errors rk. A sequential algorithm [BM17] with an unpredictability constraint in
the eigenproblem has also been proposed. Under their framework  the k-th coordinate k is obtained
from the top eigenvector of the modiﬁed kernel matrix ˜Kk  which is constructed by the original
kernel K and 1 ···   k1.
Existence of solution Before trying to ﬁnd an algorithmic solution to the IES problem  we ask
the question whether this is even possible  in the smooth manifold setting. Positive answers are
given in [Por16]  which proves that isometric embeddings by DM with ﬁnite m are possible  and
more recently in [Bat14]  which proves that any closed  connected Riemannian manifold M can be
smoothly embedded by its Laplacian eigenfunctions [m] into Rm for some m  which depends only
on the intrinsic dimension d of M  the volume of M  and lower bounds for injectivity radius and
Ricci curvature. The example in Figure 1a demonstrates that  typically  not all m eigenfunctions
are needed. I.e.  there exists a set S ⇢ [m]  so that S is also a smooth embedding. We follow
It is not known how to ﬁnd an independent S
[DTCK18] in calling such a set S independent.
analytically for a given M  except in special cases such as the strip. In this paper  we propose a
ﬁnite sample and algorithmic solution  and we support it with asymptotic theoretical analysis.

The IES Problem We are given data X  and the output of an embedding algorithm (DM for sim-
plicity) Y = (X) = [1 ···   m] 2 Rn⇥m. We assume that X is sampled from a d-dimensional
manifold M  with known d  and that m is sufﬁciently large so that (M) is a smooth embedding.
Further  we assume that there is a set S ✓ [m]  with |S| = s  m  so that S is also a smooth
embedding of M. We propose to ﬁnd such set S so that the rank of S is d on M and S varies as
slowly as possible.
Challenges (1) Numerically  and on a ﬁnite sample  distiguishing between a full rank mapping and a
rank-defective one is imprecise. Therefore  we substitute for rank the volume of a unit parallelogram
in T(xi)(M). (2) Since  is not an isometry  we must separate the local distortions introduced
by  from the estimated rank of  at x. (3) Finding the optimal balance between the above desired
properties. (4) In [Bat14] it is strongly suggested that s the number of eigenfunctions needed may
exceed the Whitney embedding dimension ( 2d)  and that this number may depend on injectivity
radius  aspect ratio  and so on. Supplement G shows an example of a ﬂat 2-manifold  the strip with
cavity  for which s > 2. In this paper  we assume that s and m are given and focus on selecting S
with |S| = s; for completeness  in Supplement G we present a heuristic to select s.
(Global) functional dependencies  knots and crossings Before we proceed  we describe three
different ways a mapping (M) can fail to be invertible. The ﬁrst  (global) functional dependency
is the case when rank D< d on an open subset of M  or on all of M (yellow curve in Figure
1a); this is the case most widely recognized in the literature (e.g.  [GZKR08  DTCK18]). The knot
is the case when rank D< d at an isolated point (Figure 1b). Third  the crossing (Figure S8 in

3

1011 01012 0/0 11011 01011 1Supplement H) is the case when  : M! (M) is not invertible at x  but M can be covered with
open sets U such that the restriction  : U ! (U ) has full rank d. Combinations of these three
exemplary cases can occur. The criteria and approach we deﬁne are based on the (surrogate) rank
of   therefore they will not rule out all crossings. We leave the problem of crossings in manifold
embeddings to future work  as we believe that it requires an entirely separate approach (based  e.g. 
or the injectivity radius or density in the co-tangent bundle rather than differential structure).

4 Criteria and algorithm

A geometric criterion We start with the main idea in evaluating the quality of a subset S of
coordinate functions. At each data point i  we consider the orthogonal basis U(i) 2 Rm⇥d of
the d dimensional tangent subspace T(xi)(M). The projection of the columns of U(i) onto the
subspace T(xi)S(M) is U(i)[S  :] ⌘ US(i). The following Lemma connects US(i) and the
co-metric HS(i) deﬁned by S  with the full H(i).
Lemma 1. Let H(i) = U(i)⌃(i)U(i)> be the co-metric deﬁned by embedding   S ✓ [m]  HS(i)
and US(i) deﬁned above. Then HS(i) = US(i)⌃(i)US(i)> = H(i)[S  S].
The proof is straightforward and left to the reader. Note that Lemma 1 is responsible for the efﬁ-
ciency of the search over sets S  given that the push-forward co-metric HS can be readily obtained
k (i) the k-th column of US(i). We further normalize each uS
as a submatrix of H. Denote by uS
k
to length 1 and deﬁne the normalized projected volume Volnorm(S  i) =
. Con-
ceptually  Volnorm(S  i) is the volume spanned by a (non-orthonormal) “basis” of unit vectors in
TS (xi)S(M); Volnorm(S  i) = 1 when US(i) is orthogonal  and it is 0 when rank HS(i) < d.
In Figure 1a  the Volnorm({1  2}) with {1 2} = {1 0  2 0} is close to zero  since the projec-
tion of the two tangent vectors is parallel to the yellow curve; however Volnorm({1 dw/he}  i)
is almost 1  because the projections of the tangent vectors U(i) will be (approximately) orthogo-
nal. Hence  Volnorm(S  i) away from 0 indicates a non-singular S at i  and we use the average
log Volnorm(S  i)  which penalizes values near 0 highly  as the rank quality R(S) of S.
Higher frequency S maps with high R(S) may exist  being either smooth  such as the embeddings
of the strip mentioned previously  or containing knots involving only small fraction of points  such
as 1 0 1 1 in Figure 1a. To choose the lowest frequency  slowest varying smooth map  a regular-
ization term consisting of the eigenvalues k  k 2 S  of the graph Laplacian L is added  obtaining
the criterion

pdet(US (i)>US (i))
Qd

k=1 kuS

k (i)k2

1
n

nXi=1

logqdet (US(i)>US(i))
}

i=1 R1(S;i)

1
n



nXi=1

|

dXk=1
log kuS
{z
nPn

k (i)k2

⇣Xk2S

k

(2)

L(S; ⇣) =

|

R1(S)= 1

{z
nPn
Search algorithm With this criterion 
the IES problem turns into a subset selec-
tion problem parametrized by ⇣

S⇤(⇣) =

argmax

L(S; ⇣)

(3)

S✓[m];|S|=s;12S

Note that we force the ﬁrst coordinate 1
to always be chosen  since this coordinate
cannot be functionally dependent on pre-
vious ones  and  in the case of DM  it also
has lowest frequency. Note also that R1
and R2 are both submodular set function
(proof in Supplement C.3). For large s
and d  algorithms for optimizing over the
difference of submodular functions can
be used (e.g.  see [IB12]). For the experi-
ments in this paper  we have m = 20 and

}

R2(S)= 1

i=1 R2(S;i)
Algorithm 2: INDEIGENSEARCH
Input : Data X  bandwith "  intrinsic dimension d 

embedding dimension s  regularizer ⇣
1 Y 2 Rn⇥m  L   2 Rm DIFFMAP(X " )
2 U(i) ···   U(n) RMETRIC(Y  L  d)
3 for S 2{ S0 ✓ [m] : |S0| = s  1 2 S0} do

4
5
6
7

R1(S) 0; R2(S) 0
for i = 1 ···   n do
US(i) U(i)[S  :]
2n · log detUS(i)>US(i)
R1(S) += 1
n ·Pd
R2(S) += 1
L(S; ⇣) = R1(S)  R2(S)  ⇣Pk2S k

8
9
10
11 end
12 S⇤ = argmaxS L(S; ⇣)
Return: Independent eigencoordinates set S⇤

k=1 log kuS

k (i)k2

end

4

d  s = 2 ⇠ 4  which enables us to use exhaustive search to handle (3). The exact search algorithm
is summarized in Algorithm 2 INDEIGENSEARCH. A greedy variant is also proposed and analyzed
in Supplement D. Note that one might be able to search in the continuous space of all s-projections.
We conjecture the objective function (2) will be a difference of convex function and leave the details
as future work2.

multiple lines (each correspond to a set S) with slopes Pk2S k and intercepts R(S). The larger

Regularization path and choosing ⇣ According to (2)  the optimal subset S⇤ depends on the
parameter ⇣. The regularization path `(⇣) = maxS✓[m];|S|=s;12S L(S; ⇣) is the upper envelope of
⇣ is  the more the lower frequency subset penalty prevails  and for sufﬁciently large ⇣ the algorithm
will output [s]. In the supervised learning framework  the regularization parameters are often chosen
by cross validation. Here we propose a second criterion  that effectively limits how much R(S) may
be ignored  or alternatively  bounds ⇣ by a data dependent quantity. Deﬁne the leave-one-out regret
of point i as follows

(4)

D(S  i) = R(Si

⇤; [n]\{i})  R(S; [n]\{i}) with Si

⇤ = argmaxS✓[m];|S|=s;12SR(S; i)

|T|Pi2T R1(S; i)  R2(S; i) for some subset T ✓ [n]. The
In the above  we denote R(S; T ) = 1
quantity D(S  i) in (4) measures the gain in R if all the other points [n]\{i} choose the optimal
. If the regret D(S  i) is larger than zero  it indicates that the alternative choice might
subset Si
nPi D(S  i)
⇤
be better compared to original choice S. Note that the mean value for all i  i.e.  1
. Therefore  it might not fa-
depends also on the variability of the optimal choice of points i  Si
⇤
Instead  we propose to inspect the distribution of
vor an S  if S is optimal for every i 2 [n].
D(S  i)  and remove the sets S for which ↵’s percentile are larger than zero  e.g.  ↵ = 75% 
recursively from ⇣ = 1 in decreasing order. Namely  the chosen set is S⇤ = S⇤(⇣0) with
⇣0 = max⇣0 PERCENTILE({D(S⇤(⇣)  i)}n
i=1 ↵ )  0. The optimal ⇣⇤ value is simply chosen to be
the midpoint of all the ⇣’s that outputs set S⇤ i.e.  ⇣⇤ = 1
2 (⇣0 + ⇣00)  where ⇣00 = min⇣0 S⇤(⇣) =
S⇤(⇣0). The procedure REGUPARAMSEARCH is summarized in Algorithm S5.
5 R as Kullbach-Leibler divergence

In this section we analyze R in its population version  and show that it is reminiscent of a Kullbach-
Leibler divergence between unnormalized measures on S(M). The population version of the reg-
ularization term takes the form of a well-known smoothness penalty on the embedding coordinates
S. Proofs of the theorems can be found in Supplement C.
Volume element and the Riemannian metric Consider a Riemannian manifold (M  g) mapped
by a smooth embedding S into (S(M)  g⇤S )  S : M! Rs  where g⇤S is the push-forward
metric deﬁned in (1). A Riemannian metric g induces a Riemannian measure on M  with volume
element pdet g. Denote now by µM  respectively µS (M) the Riemannian measures corresponding
to the metrics induced on M  S(M) by the ambient spaces RD  Rs; let g be the former metric.
Lemma 2. Let S    S  HS(x)  US(x)  ⌃(x) be deﬁned as in Section 4 and Lemma 1. For sim-
plicity  we denote by HS(y) ⌘ HS(1
S (y))  and similarly for US(y)  ⌃(y). Assume that S is a
smooth embedding. Then  for any measurable function f : M! R 

ZM

f (x)dµM(x) =ZS (M)

f (1

S (y))jS(y)dµS (M)(y) 

(5)

(6)

with

jS(y) = 1/ Vol(US(y)⌃1/2

S (y)).

Asymptotic limit of R We now study the ﬁrst term of our criterion in the limit of inﬁnite sample
size. We make the following assumptions.
Assumption 1. The manifold M is compact of class C3  and there exists a set S  with |S| = s so
that S is a smooth embedding of M in Rs.

2We thank the anonymous reviewer who made this suggestion.

5

Assumption 2. The data are sampled from a distribution on M continuous with respect to µM 
whose density is denoted by p.
Assumption 3. The estimate of HS in Algorithm 1 computed w.r.t. the embedding S is consistent.

We know from [Bat14] that Assumption 1 is satisﬁed for the DM/LE embedding. The remaining
assumptions are minimal requirements ensuring that limits of our quantities exist. Now consider the
setting in Sections 3  in which we have a larger set of eigenfunctions  [m] so that [m] contains the

set S of Assumption 1. Denote by ˜|S(y) =Qd

here k = [⌃]kk.
Theorem 3 (Limit of R). Under Assumptions 1–3 

k=1||uS

k (y)||k(y))1/21 a new volume element 

and

lim
n!1

1

nXi

ln R(S  xi) = R(S M) 

R(S M) = ZS (M)

ln

jS(y)
˜|S(y)

jS(y)p(1

S (y))dµS (M)(y)

def

= D(pjSkp˜|S)

(7)

(8)

The expression D(·k·) represents a Kullbach-Leibler divergence. Note that jS  ˜|S  which implies
that D is always positive  and that the measures deﬁned by pjS  p˜|S normalize to different values.
By deﬁnition  local injectivity is related to the volume element j. Intuitively  pjS is the observation
and p˜jS  where ˜jS is the minimum attainable for jS  is the model; the objective itself is looking for
a view S of the data that agrees with the model.
It is known that k  the k-th eigenvalue of the Laplacian  converges under certain technical condi-
tions [BN07] to an eigenvalue of the Laplace-Beltrami operator M and that
2dµ(M).

k(M) = hk  Mki =ZM k grad k(x)k2

Hence  a smaller value for the regularization term encourages the use of slow varying coordinate
functions  as measured by the squared norm of their gradients  as in equation (9). Hence  under
Assumptions 1  2  3  L converges to

(9)

L(S M) = D(pjSkp˜|S) ✓ ⇣

1(M)◆Xk2S

k(M).

(10)

Since eigenvalues scale with the volume of M  the rescaling of ⇣ in comparison with equation (2)
makes the ⇣ above adimensional.

6 Experiments

We demonstrate the proposed algorithm on three synthetic datasets  one where the minimum em-
bedding dimension s equals d (D1 long strip)  and two (D7 high torus and D13 three torus) where
s > d. The complete list of synthetic manifolds (transformations of 2 dimensional strips  3 di-
mensional cubes  two and three tori  etc.) investigated can be found in Supplement H and Table
S2. The examples have (i) aspect ratio of at least 4 (ii) points sampled non-uniformly from the un-
derlying manifold M  and (iii) Gaussian noise added. The sample size of the synthetic datasets is
n = 10  000 unless otherwise stated. Additionally  we analyze several real datasets from chemistry
and astronomy. All embeddings are computed with the DM algorithm  which outputs m = 20 eigen-
vectors. Hence  we examine 171 sets for s = 3 and 969 sets for s = 4. No more than 2 to 5 of these
sets appear on the regularization path. Detailed experimental results are in Table S3. In this section 
we show the original dataset X  the embedding S⇤  with S⇤ selected by INDEIGENSEARCH and
⇣⇤ from REGUPARAMSEARCH  and the maximizer sets on the regularization path with box plots of
D(S  i) as discussed in Section 4. The ↵ threshold for REGUPARAMSEARCH is set to 75%. The
kernel bandwidth " for synthetic datasets is chosen manually. For real datasets  " is optimized as
in [JMM17]. All the experiments are replicated for more than 5 times  and the outputs are similar
because of the large sample size n.

6

Original data X

Embedding S⇤

Regularization path

1

D

7

D

3
1

D

Figure 2: Experimental result for synthetic datasets. Rows correspond to different synthetic datasets
(please refer to Table S2). Optimal subset S⇤ is selected by INDEIGENSEARCH.

(a) Embedding [3]

(b) L({1  i  j})

(c) S1 = {1 4 6}

(d) S2 = {1 5 7}

(f) L({1  3}) = 0.39

(h) rk vs. k

(e) L({1  2}) = 1.24
Figure 3: First row: Chloromethane dataset; second row: SDSS dataset in (e)  (f) and (g)  (h) show
the example when LLR failed.
(c) and (d) are embeddings with top two ranked subsets S1 and S2 
colored by the distances between C and two different Cl –   respectively. (e) and (f) are embeddings
of {1 2} (suboptimal set) and {1 3} (maximizer of L)  respectively (values shown in caption).

(g) Subset {1 2 5} by LLR

Synthetic manifolds The results of synthetic manifolds are in Figure 2. (i) Manifold with s = d.
The ﬁrst synthetic dataset we considered  D1  is a two dimensional strip with aspect ratio W/H =
2⇡. Left panel of the top row shows the scatter plot of such dataset. From the theoretical analysis
in Section 3  the coordinate set that corresponds to slowest varying unique eigendirection is S =
{1 dW/He} = {1  7}. Middle panel  with S⇤ = {1  7} selected by INDEIGENSEARCH with ⇣ cho-
sen by REGUPARAMSEARCH  conﬁrms this. The right panel shows the box plot of {D(S  i)}n
i=1.
According to the proposed procedure  we eliminate S0 = {1  2} since D(S0  i)  0 for almost all
the points. (ii) Manifold with s > d. The second data D7 is displayed in the left panel of the second
row. Due to the mechanism we used to generate the data  the resultant torus is non-uniformly dis-
tributed along the z axis. Middle panel is the embedding of the optimal coordinate set S⇤ = {1  4  5}
selected by INDEIGENSEARCH. Note that the middle region (in red) is indeed a two dimensional
narrow tube when zoomed in. The right panel indicates that both {1  2  3} and {1  2  4} (median

7

102100102104⇣{1 2}{1 7}202D(S i)1010107104101102⇣{1 2 3}{1 2 4}{1 4 5}101D(S i)102101100101102⇣{1 2 3 4}{1 2 3 5}{1 2 5 10}{1 5 6 10}2024D(S i)is around zero) should be removed. The optimal regularization parameter is ⇣⇤ ⇡ 7. The result of
the third dataset D13  three torus  is in the third row of the ﬁgure. We displayed only projections
of the penultimate and the last coordinate of original data X and embedding S⇤ (which is {5  10})
colored by ↵1 of (S15) in the left and middle panel to conserve space. A full combinations of coor-
dinates can be found in Figure S5. The right panel implies one should eliminate the set {1  2  3  4}
and {1  2  3  5} since both of them have more than 75% of the points such that D(S  i)  0. The
ﬁrst remaining subset is {1  2  5  10}  which yields an optimal regularization parameter ⇣⇤ ⇡ 5.
Molecular dynamics dataset [FTP16] The dataset has size n ⇡ 30  000 and ambient dimension
D = 40  with the intrinsic dimension estimate be ˆd = 2 (see Supplement H.1 for details). The
embedding with coordinate set S = [3] is shown in Figure 3a. The ﬁrst three eigenvectors pa-
rameterize the same directions  which yields a one dimensional manifold in the ﬁgure. Top view
(S = [2]) of the ﬁgure is a u-shaped structure similar to the yellow curve in Figure 1a. The heat map
of L({1  i  j}) for different combinations of coordinates in Figure 3b conﬁrms that L for S = [3] is
low and that 1  2 and 3 give a low rank mapping. The heat map also shows high L values for
S1 = {1  4  6} or S2 = {1  5  7}  which correspond to the top two ranked subsets. The embeddings
with S1  S2 are in Figures 3c and 3d  respectively. In this case  we obtain two optimal S sets due to
the data symmetry.

3 [AAMA+09]  preprocessed as in
Galaxy spectra from the Sloan Digital Sky survey (SDSS)
[MMVZ16]. We display a sample of n = 50  000 points from the ﬁrst 0.3 million points which
correspond to closer galaxies. Figures 3e and 3f show that the ﬁrst two coordinates are almost
dependent; the embedding with S⇤ = {1  3} is selected by INDEIGENSEARCH with d = 2. Both
plots are colored by the blue spectrum magnitude  which is correlated to the number of young stars
in the galaxy  showing that this galaxy property varies smoothly and non-linearly with 1  3  but is
not smooth w.r.t. 1  2.

Comparison with [DTCK18] The LLRCOORDSEARCH method outputs similar candidate coor-
dinates as our proposed algorithm most of the time (see Table S3). However  the results differ for
high torus as in Figure 3. Figure 3h is the leave one out (LOO) error rk versus coordinates. The
coordinates chosen by LLRCOORDSEARCH was S = {1  2  5}  as in Figure 3g. The embedding is
clearly shown to be suboptimal  for it failed to capture the cavity within the torus. This is because the
algorithm searches in a sequential fashion; the noise eigenvector 2 in this example appears before
the signal eigenvectors e.g.  4 and 5.

Additional experiments with real data are shown in Table 1. Not surprisingly  for most real data
sets we examined  the independent coordinates are not the ﬁrst s. They also show that the algorithm
scales well and is robust to the noise present in real data.

Table 1: Results for other real datasets. Columns from left to right are sample size n  ambient
dimension of data D  average degree of neighbor graph degavg  (s  d) and runtime for IES  and the
chosen set S⇤  respectively. Last three datasets are from [CTS+17].

SDSS (full)
Aspirin
Ethanol
Malondialdehyde

n
298 511
211 762
555 092
993 237

D degavg
144.91
101.03
107.27
106.51

3750
244
102
96

(s  d)
(2  2)
(4  3)
(3  2)
(3  2)

t (sec)
106.05
85.11
233.16
459.53

S⇤
(1  3)
(1  2  3  7)
(1  2  4)
(1  2  3)

The asymptotic runtime of LLRCOORDSEARCH has quadratic dependency on n  while for our
algorithm is linear in n. Details of runtime analysis are Supplement F. LLRCOORDSEARCH was
too slow to be tested on the four larger datasets (see also Figure S1).

3The Sloan Digital Sky Survey data can be downloaded from https://www.sdss.org

8

7 Conclusion

Algorithms that use eigenvectors  such as DM  are among the most promising and well studied in
ML. It is known since [GZKR08] that when the aspect ratio of a low dimensional manifold exceeds
a threshold  the choice of eigenvectors becomes non-trivial  and that this threshold can be as low
as 2. Our experimental results conﬁrm the need to augment ML algorithms with IES methods in
order to successfully apply ML to real world problems. Surprisingly  the IES problem has received
little attention in the ML literature  to the extent that the difﬁculty and complexity of the problem
have not been recognized. Our paper advances the state of the art by (i) introducing for the ﬁrst
time a differential geometric deﬁnition of the problem  (ii) highlighting geometric factors such as
injectivity radius that  in addition to aspect ratio  inﬂuence the number of eigenfunctions needed for
a smooth embedding  (iii) constructing selection criteria based on intrinsic manifold quantities  (iv)
which have analyzable asymptotic limits  (v) can be computed efﬁciently  and (vi) are also robust to
the noise present in real scientiﬁc data. The library of hard synthetic examples we constructed will
be made available along with the python software implementation of our algorithms.

Acknowledgements

The authors acknowledge partial support from the U.S. Department of Energy  Solar Energy Tech-
nology Ofﬁce award DE-EE0008563 and from the NSF DMS PD 08-1269 and NSF IIS-0313339
awards. They are grateful to the Tkatchenko and Pfaendtner labs and in particular to Stefan Chmiela
and Chris Fu for providing the molecular dynamics data and for many hours of brainstorming and
advice.

References
[AAMA+09] Kevork N Abazajian  Jennifer K Adelman-McCarthy  Marcel A Ag¨ueros  Sahar S
Allam  Carlos Allende Prieto  Deokkeun An  Kurt SJ Anderson  Scott F Anderson 
James Annis  Neta A Bahcall  et al. The seventh data release of the sloan digital sky
survey. The Astrophysical Journal Supplement Series  182(2):543  2009.

[Bat14] Jonathan Bates. The embedding dimension of laplacian eigenfunction maps. Applied

and Computational Harmonic Analysis  37(3):516–530  2014.

[BM17] Yochai Blau and Tomer Michaeli. Non-redundant spectral dimensionality reduction.
In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases  pages 256–271. Springer  2017.

[BN07] Mikhail Belkin and Partha Niyogi. Convergence of laplacian eigenmaps.

In
B. Sch¨olkopf  J. C. Platt  and T. Hoffman  editors  Advances in Neural Information
Processing Systems 19  pages 129–136. MIT Press  2007.

[CL06] R. R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic

Analysis  30(1):5–30  2006.

[CTS+17] Stefan Chmiela  Alexandre Tkatchenko  Huziel E Sauceda  Igor Poltavsky  Kristof T
Sch¨utt  and Klaus-Robert M¨uller. Machine learning of accurate energy-conserving
molecular force ﬁelds. Science advances  3(5):e1603015  2017.

[Dry16] I. L. (Ian L.) Dryden. Statistical shape analysis : with applications in R. Wiley
series in probability and statistics. Wiley  Chichester  West Sussex  England  2nd ed.
edition  2016.

[DS13] Sanjoy Dasgupta and Kaushik Sinha. Randomized partition trees for exact nearest

neighbor search. In Conference on Learning Theory  pages 317–337  2013.

[DTCK18] Carmeline J Dsilva  Ronen Talmon  Ronald R Coifman  and Ioannis G Kevrekidis.
Parsimonious representation of nonlinear dynamical systems through manifold learn-
ing: A chemotaxis case study. Applied and Computational Harmonic Analysis 
44(3):759–773  2018.

9

[FTP16] Kelly L. Fleming  Pratyush Tiwary  and Jim Pfaendtner. New approach for inves-
tigating reaction dynamics and rates with ab initio calculations. Jornal of Physical
Chemistry A  120(2):299–305  2016.

[GZKR08] Yair Goldberg  Alon Zakai  Dan Kushnir  and Yaacov Ritov. Manifold learning: The
price of normalization. Journal of Machine Learning Research  9(Aug):1909–1939 
2008.

[Har98] David A Harville. Matrix algebra from a statistician’s perspective  1998.

[HAvL05] Matthias Hein  Jean-Yves Audibert  and Ulrike von Luxburg. From graphs to man-
ifolds - weak and strong pointwise consistency of graph laplacians.
In Learning
Theory  18th Annual Conference on Learning Theory  COLT 2005  Bertinoro  Italy 
June 27-30  2005  Proceedings  pages 470–485  2005.

[HAvL07] Matthias Hein  Jean-Yves Audibert  and Ulrike von Luxburg. Graph laplacians and
their convergence on random neighborhood graphs. Journal of Machine Learning
Research  8:1325–1368  2007.

[HHJ90] Roger A Horn  Roger A Horn  and Charles R Johnson. Matrix analysis. Cambridge

university press  1990.

[IB12] Rishabh Iyer and Jeff Bilmes. Algorithms for approximate minimization of the differ-
ence between submodular functions  with applications. In Proceedings of the Twenty-
Eighth Conference on Uncertainty in Artiﬁcial Intelligence  UAI’12  pages 407–417 
Arlington  Virginia  United States  2012. AUAI Press.

[JMM17] Dominique Joncas  Marina Meila  and James McQueen. Improved graph laplacian
via geometric self-consistency. In Advances in Neural Information Processing Sys-
tems  pages 4457–4466  2017.

[LB05] Elizaveta Levina and Peter J Bickel. Maximum likelihood estimation of intrinsic
dimension. In Advances in neural information processing systems  pages 777–784 
2005.

[Lee03] John M. Lee. Introduction to smooth manifolds  2003.

[MMVZ16] James McQueen  Marina Meil˘a  Jacob VanderPlas  and Zhongyue Zhang. Mega-
man: Scalable manifold learning in python. Journal of Machine Learning Research 
17(148):1–5  2016.

[NLCK06] Boaz Nadler  Stephane Lafon  Ronald Coifman  and Ioannis Kevrekidis. Diffusion
maps  spectral clustering and eigenfunctions of Fokker-Planck operators. In Y. Weiss 
B. Sch¨olkopf  and J. Platt  editors  Advances in Neural Information Processing Sys-
tems 18  pages 955–962  Cambridge  MA  2006. MIT Press.

[NWF78] George L Nemhauser  Laurence A Wolsey  and Marshall L Fisher. An analysis of
approximations for maximizing submodular set functionsi. Mathematical program-
ming  14(1):265–294  1978.

[PM13] D. Perraul-Joncas and M. Meila. Non-linear dimensionality reduction: Riemannian
metric estimation and the problem of geometric discovery. ArXiv e-prints  May 2013.
[Por16] Jacobus W Portegies. Embeddings of riemannian manifolds with heat kernels and
eigenfunctions. Communications on Pure and Applied Mathematics  69(3):478–518 
2016.

[Str07] Walter A Strauss. Partial differential equations: An introduction. Wiley  2007.
[THJ10] Daniel Ting  Ling Huang  and Michael I. Jordan. An analysis of the convergence of
graph laplacians. In Proceedings of the 27th International Conference on Machine
Learning (ICML-10)  pages 1079–1086  2010.

10

,Jie Liu
David Page
Prateek Jain
Nagarajan Natarajan
Ambuj Tewari
Timothy Rubin
Oluwasanmi Koyejo
Michael Jones
Tal Yarkoni
Yu-Chia Chen
Marina Meila