2019,Globally optimal score-based learning of directed acyclic graphs in high-dimensions,We prove that $\Omega(s\log p)$ samples suffice to learn a sparse Gaussian directed acyclic graph (DAG) from data  where $s$ is the maximum Markov blanket size. This improves upon recent results that require $\Omega(s^{4}\log p)$ samples in the equal variance case. To prove this  we analyze a popular score-based estimator that has been the subject of extensive empirical inquiry in recent years and is known to achieve state-of-the-art results. Furthermore  the approach we study does not require strong assumptions such as faithfulness that existing theory for score-based learning crucially relies on.  The resulting estimator is based around a difficult nonconvex optimization problem  and its analysis may be of independent interest given recent interest in nonconvex optimization in machine learning. Our analysis overcomes the drawbacks of existing theoretical analyses  which either fail to guarantee structure consistency in high-dimensions (i.e. learning the correct graph with high probability)  or rely on restrictive assumptions. In contrast  we give explicit finite-sample bounds that are valid in the important $p\gg n$ regime.,Globally optimal score-based learning of directed

acyclic graphs in high-dimensions

Bryon Aragam1 Arash A. Amini2 Qing Zhou2
1University of Chicago
bryon@chicagobooth.edu

2University of California  Los Angeles

{aaamini zhou}@stat.ucla.edu

Abstract

We prove that ⌦(s log p) samples sufﬁce to learn a sparse Gaussian directed acyclic
graph (DAG) from data  where s is the maximum Markov blanket size. This
improves upon recent results that require ⌦(s4 log p) samples in the equal variance
case. To prove this  we analyze a popular score-based estimator that has been
the subject of extensive empirical inquiry in recent years and is known to achieve
state-of-the-art results. Furthermore  the approach we study does not require strong
assumptions such as faithfulness that existing theory for score-based learning
crucially relies on. The resulting estimator is based around a difﬁcult nonconvex
optimization problem  and its analysis may be of independent interest given recent
interest in nonconvex optimization in machine learning. Our analysis overcomes
the drawbacks of existing theoretical analyses  which either fail to guarantee
structure consistency in high-dimensions (i.e.
learning the correct graph with
high probability)  or rely on restrictive assumptions. In contrast  we give explicit
ﬁnite-sample bounds that are valid in the important p  n regime.

1

Introduction

With the growing importance of explainability and interpretability in modern machine learning
[11  64  65]  graphical models continue to play an important role in applications including genomics
[72]  health care [41]  and ﬁnance [50] owing to their natural interpretability and simplicity. For this
reason  rigorous theoretical understanding of graphical models is an important challenge in modern
machine learning. Although estimating undirected graphical models can be formulated as a convex
program  DAG models cannot be [15]  which has limited our understanding of their ﬁnite-sample
properties. Despite impressive progress in our understanding of nonconvex models across a spectrum
of problems including dictionary learning [58]  tensor decomposition [16  18]  deep neural networks
[13  14]  and regression [36  37]  learning DAGs remains an important problem with many open
questions  particularly in the high-dimensional (p  n) setting.
Among the many strategies for learning DAGs from data  score-based learning is a classical approach
that is popular in practice. While much is known about greedy search algorithms [6  40]  much
less is known regarding the statistical properties of methods that ﬁnd a global minimizer of a score
function. One of the advantages of the latter approach is a potential relaxation of assumptions such
as faithfulness [61]. In this paper  we prove that a score-based method requires only O(s log p)
samples  where s is the maximum Markov blanket size  at the cost of being difﬁcult to compute since
it requires solving a nonconvex  NP-hard optimization problem. This is a well-known drawback
of score-based methods  although recent work has demonstrated that approximate methods can
outperform state-of-the-art methods [1  25  70]  and even come close to ﬁnding the global minimum
in practice [77].
More speciﬁcally  we characterize the ﬁnite-sample  high-dimensional behaviour of the following
score-based DAG estimator  formulated as the solution of a constrained  nonsmooth  nonconvex

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

optimization problem:

bB 2 arg min

B 2 D

Q(B)  Q(B) =

1
2n kX  XBk2

F + ⇢(B) 

(1)

where D is the set of p⇥ p matrices representing the weighted adjacency matrix of a DAG  X 2 Rn⇥p
is the data  and ⇢ is a suitably chosen regularizer (Section 2.3). In the literature on learning DAGs 
Q is called a score function. This estimator has been the subject of extensive empirical inquiry [e.g.
1  26  51  54  68  77]  and outperforms classical approaches such as the PC algorithm [57] and greedy

equivalence search [GES  6] on high-dimensional data. Moreover  although computation of bB is

NP-hard [7]  it can be computed exactly using dynamic programming [43  44  55  56] and mixed
integer programs [9  10]  and approximate algorithms for computing this estimator scale to modern
problem sizes with tens of thousands of variables [1  3].

Contributions
ing much needed justiﬁcation—and caution—for its use in applications. Speciﬁcally  our main
contributions are as follows:

In this paper we provide a comprehensive portrait of the behaviour of bB  provid-
1. We provide explicit  ﬁnite-sample structure recovery guarantees for the score-based estimator (1)
that are valid when p  n. This is in contrast to recent work on score-based methods that either
studies asymptotic properties of speciﬁc algorithms under faithfulness [40]  or does not prove exact
structure recovery [35  62].
2. We develop a new proof technique in order to simplify the analysis of score-based estimators 
based on a novel lattice construction and a reduction to neighbourhood regression. This construction
allows us to provide uniform control over the superexponential family of neighbourhood regression
problems that deﬁne (1)  a result that is potentially interesting in its own right.

3. We use this construction to prove an ⌦(s log p) sample complexity under which bB recovers the

true DAG with high probability  which improves upon existing results. We also generalize existing
results on estimating identiﬁable DAGs with equal error variances to what we call minimum-trace
DAGs.
4. We discuss the more general  nonidentiﬁable case. In this setting  there is no “truth” to approxi-

distribution.

mate  however  we show that bB still estimates a sufﬁciently sparse representative of the underlying

We anticipate these results will be of interest not only to the graphical modeling community  but also
to the broader machine learning community in the way it analyzes a difﬁcult nonconvex optimization
problem head on.

Previous work It was recently shown that it is possible to learn DAGs in high-dimensions [21–
23  67]. These papers prove a lower bound of ⌦(k log p) on the sample complexity where k is the
maximum number of parents in the true DAG  and provide a polynomial-time algorithm that requires
O(s4 log p) samples to recover this DAG. These papers are based on a new approach—distinct from
traditional score-based or constraint-based learning—that uses second-order information to ﬁnd a
node ordering. Once this ordering is found  estimation is straightforward. Earlier work on the linear
non-Gaussian case uses independent component analysis to identify the true DAG model [52  53] but
requires n > p and as such is not high-dimensional.
Perhaps surprisingly  despite score-based methods being very popular in practice  none of these
papers consider score-based methods. Asymptotically  consistency of the score-based GES algorithm
is well-known [6  40]  however  to the best of our knowledge ﬁnite-sample complexity results are
not available for GES. Furthermore  these results assume strong faithfulness  which—as the name
suggests—is an even stronger version of faithfulness that is known to be very stringent and may
not hold in practice [34  61]. By assuming faithfulness  the Markov equivalence class—and hence
CPDAG—of a distribution becomes identiﬁed  which greatly simpliﬁes the theoretical analysis. Only
a few recent papers have studied ﬁnite-sample properties of score-based estimators: van de Geer and
Bühlmann [62] establish `2-consistency of a restricted `0-regularized MLE  Loh and Bühlmann [35]
analyze the empirical score of DAGs that are consistent with an estimated moral graph  and Yuan
et al. [71] analyze a constrained MLE. Unfortunately  the practical implications of these interesting
theoretical results have been limited by certain aspects of their analysis. Although van de Geer and

2

Bühlmann [62] and Yuan et al. [71] avoid the faithfulness assumption  their structure consistency
results require p  n and thus do not provide a direct theory for the high-dimensional structure
learning problem. Loh and Bühlmann [35] do not consider the problem of structure recovery  and one
of our contributions is to show that by properly regularizing the score in high-dimensions  structure
recovery is possible when p  n.
Perhaps surprisingly  proving consistency for the global minimizer of (1) turns out to be a unique
challenge: Despite a growing literature on theory for nonconvex problems [5  8  13  14  16  18  19  28–
30  33  38  58  60]  existing techniques from the graphical modeling literature fail to capture the
essence of the program (1). Classical arguments such as the basic inequality can be used to prove `2-
rates of convergence as in [63]  but translating these rates into structure learning (e.g. by thresholding)
requires n =⌦( p). By assuming strong faithfulness  one can simplify the problem substantially
by reducing it to a constraint-based method as in [40]. The latter work in particular sidesteps all
of the difﬁculties in analyzing the nonconvex program (1)  which constitute arguably some of the
most interesting theoretical aspects of this problem. More discussion on these points can be found in
Section 6.

2 Background

Our approach is based on the structural equation model (SEM) interpretation of Gaussian DAGs.
Suppose X = (X1  . . .   Xp) is a random vector satisfying

(2)

X = eBT X +e" 

e" ⇠N p(0 e⌦) 

where eB 2 D ande⌦ is a p ⇥ p positive diagonal matrix of variances. One can interpret eB as the
weighted adjacency matrix of a graph. Given an n ⇥ p random matrix X whose rows are i.i.d. drawn
according to the model (2)  we deﬁne a penalized least-squares (PLS) score function by (1). It follows
from (2) that X ⇠N p(0  ⌃(eB e⌦))  where
(3)
We will assume that ⌃  0  and moreover that rmin(⌃) ⇣ rmax(⌃) ⇣ 1  i.e. the eigenvalues of ⌃ are
bounded away from 0 and 1. This is purely to simplify the theorem statements; explicit constants
depending on ⌃ and its eigenvalues can be found in the supplement.
Notation We write a & b (resp. a . b) to mean that a  C · b (resp. a  C · b) for some constant
C > 0. In all cases  exact values for these constants can be found in the supplement.

⌃(eB e⌦) := (I  eB)Te⌦(I  eB)1.

2.1

Identiﬁability

nonidentiﬁable. Recent work [12  21  62] assumes equivariance  i.e. ⌃=⌃(

The map (eB e⌦) 7! ⌃(eB e⌦) is not one-to-one  i.e. without further assumptions the model (2) is
eB e!2I) for some
e!2 > 0  which ensures that eB is identiﬁable [47]. We generalize this condition as follows: Let Rp
denote the space of p ⇥ p positive diagonal matrices and deﬁne the equivalence class of ⌃ by
and call eBmin a minimum-trace DAG if (eBmin e⌦min) 2 arg min{tre⌦: ( eB e⌦) 2 D(⌃)}. In other
words  eBmin minimizes the total conditional variance amongst all of the DAGs that represent ⌃. We
will sometimes abuse notation by writing eB 2 D(⌃) ore⌦ 2 D(⌃) for short. The following lemma
eB e!2I) for some e!2 > 0. Then eB is the unique

connects equivariant DAGs to minimum-trace DAGs:
Lemma 2.1. Suppose ⌃ is given and ⌃=⌃(
minimum-trace DAG in D(⌃).

D(⌃) =(eB e⌦) 2 D ⇥ Rp

In general  minimum-trace DAGs are not unique  so this lemma shows that the concept of minimum-
trace provides a convenient generalization of known identiﬁability results for equivariant DAGs.
Beyond their connection with equivariance DAGs  it is important to address why minimum-trace
DAGs should be of interest in the sequel. As discussed previously  despite a lack of theoretical

+

(4)

+ :⌃=⌃(

eB e⌦)  

3

former question is surprisingly tricky; see Section 4. The results presented in this paper will show

justiﬁcation  the estimator bB is popular in practice [e.g. 1  26  51  54  68  77]. Our motivation is to
answer fundamental questions such as does bB converge  and if so  to what? We note that even the
that not only does bB converge  we can pinpoint what it converges to  namely a minimum-trace DAG.

The importance of this result lies not in the fact that we might be interested in minimum-trace DAGs 
but perhaps that we might not be: Whether or not one would be interested in a minimum-trace (or
equivariance) DAG depends on the application.

2.2 Superstructures

In addition to (1)  we will also study a restricted version of bB deﬁned as follows: Given an undirected
graph G = (V  E)  deﬁne DG = {B 2 D : B ⇢ G}  i.e. the subset of D that are subgraphs of G  and
(5)

Q(B) 

where Q(B) is deﬁned as in (1). The graph G is called a superstructure  and reduces both the
computational and statistical complexity of score-based methods [42  46]. We recall here also the
moral graph m(B) of a DAG B  deﬁned as the undirected graph that results from ignoring edge
orientation in B and adding an undirected edge between the parents of each node in B. Clearly  m(B)
is a superstructure of B.

bB(G) 2 arg min

B 2 DG

2.3 Regularizer

This penalty leads to good theoretical properties but is difﬁcult to optimize due to its combinatorial

Traditionally  score functions use `0-regularization  i.e. ⇢(B) = 2Pi j 1(ij 6= 0) [6  20  62].
nature. For this reason  we consider the `1-regularizer  ⇢(B) = Pi j |ij|  which is a convex

surrogate of the `0-regularizer that is easier to optimize [77]  as well as the minimax concave
penalty (MCP) [73]  which is a continuous  nonconvex interpolant between `0 and `1 regularization.
Although easier to compute with  `1 regularization is known to require strong incoherence conditions
for consistent variable selection [39  66  76]  whereas the MCP does not require these conditions.
More details can be found in Appendix A.2 of the supplement.
The following condition formalizes the assumptions we place on ⇢. Let Nj(G) denote the neigh-
bourhood of Xj in G  i.e. the set of all vertices adjacent to Xj.
Condition 2.1 (Regularizer). The regularizer ⇢ is either `1 or the MCP. If `1 regularization is used 
then additionally assume that ⇣(G) < 1  where
sup

(6)

⇣(G) := sup
1jp

S⇢Nj (G)k⌃ScS(⌃SS)1k1 1.

Here  kAk1 1 = maxiPj |aij|. Crucially  if ⇢ is the MCP  then we are left with a continuous

optimization problem without requiring any incoherence conditions.

3 The identiﬁable case: Recovery of minimum-trace DAGs

3.1 Assumptions

We begin with the identiﬁable case  i.e. eBmin is unique.
Given a minimum-trace DAG eBmin  deﬁne for ⌘> 0

(⌘) := inf

Given a superstructure G  let s = s(G) denote the maximum degree of G  and deﬁne

e⌦2D(⌃)h(1  ⌘) tre⌦  (1 + ⌘) tre⌦min  ⇢(eBmin)i.
e⌦6=e⌦min
1 = 1(G) := 4r s log[3ep/s] + log p
2 = 2(G) :=⇣1 + 3p2r s log(ep/s)

⌘2

n

n

 

.

4

(7)

(8)

(9)

Condition 3.1 (Identiﬁability). ⌃  0  and
(a) There exists a unique minimum-trace DAG eBmin 2 D(⌃);
(b) 1(G)  1 and (⌘) > 0  where ⌘ := 1[1 + 6(⌃; s)2] and (⌃; s) is a constant that depends
on ⌃ and s.

See (47) in the supplement for an exact expression of (⌃; s)  which is roughly the maximum
condition number of the principal submatrices of ⌃ of size O(s). Condition 3.1(a) is an identiﬁa-

Lemma 2.1  Condition 3.1(a) is strictly weaker than equivariance. Under this condition  we can speak

bility condition on eBmin  and Condition 3.1(b) is needed to recover eBmin from ﬁnite samples. By
of “the” minimum-trace DAG  which will be denoted in the sequel by (eBmin e⌦min). Condition 3.1(b)

is closely related to gap conditions that have appeared previously [35  62]  and is discussed in detail
in Section 3.2.

3.2 First main result: Identiﬁable DAGs

A standard approach is to deﬁne G by the support of a consistent estimate of the precision matrix

continue to hold for some minimum-trace DAG. In the next section  we consider the nonidentiﬁable
case in even greater detail (see Theorem 4.1).

For any A 2 Rp⇥p  let ⌧⇤(A) := min{|aij| : aij 6= 0}. The quantity ⌧⇤(eBmin) measures the smallest
nonzero weight in eBmin  which is a measure of the signal strength in the problem.
Theorem 3.1. Suppose that Conditions 2.1 and 3.1 hold and that eBmin ⇢ G. If n & s log p 
 &plog p/n  and ⌧⇤(eBmin) &   then
with probability 1  O(ek log p)  where k is the maximum in-degree of eBmin.
In fact  even if Condition 3.1(a) fails—i.e. eBmin is not identiﬁable—the conclusions of Theorem 3.1
The previous theorem assumes that a consistent superstructure G is known  i.e. that eBmin ⇢ G.
=⌃ 1. The following assumption encodes the minimal requirement we need on ⌃ and eBmin:
Condition 3.2 (Superstructure). If (i  j) is an edge in m(eBmin)  then ij 6= 0.
The results in Loh and Bühlmann [35] show that as long as the entries of eBmin are drawn from a
the support of   which can be estimated using known results [39]. Letb denote such an estimate
and with some abuse of notation  denote the resulting DAG estimator by bB(b).
Corollary 3.1. Suppose that Conditions 2.1  3.1  and 3.2 hold. If n & s log p   & plog p/n 
⌧⇤() &   and ⌧⇤(eBmin) &   then
with probability 1  O(ek log p).
Corollary 3.1 implies that there is a score-based estimator with sample complexity ⌦(s log p). In
contrast to Ghoshal and Honorio [21]  who require an element-wise consistent estimate of  (i.e.
in `1-norm)  our result only requires the support of . The former approach leads to a ⌦(s4 log p)
sample complexity  whereas our approach requires only ⌦(s log p) samples. Both of these results are
a signiﬁcant improvement over existing results on score-based methods  e.g. Theorem 5.1 in [62] 
which requires p . n/ log n and hence n & p.

continuous distribution  Condition 3.2 is satisﬁed except on a set of measure zero. For details  see
Theorem 2 and Assumption 1 therein. Under Condition 3.2  it sufﬁces to use a consistent estimate of

supp(bB(G)) = supp(eBmin)

supp(bB(b)) = supp(eBmin)

Faithfulness and the beta-min condition Theorem 3.1 does not require the faithfulness assump-
tion  which is a standard assumption in the literature on learning DAGs for both score-based [6  40]
and constraint-based methods [31]  and is known to be very strong in practice [34  61]. Assuming

5

faithfulness  the Markov equivalence class becomes identiﬁed  which simpliﬁes the problem by
restricting the number of equivalent DAGs that must be controlled. Recent work has also relaxed this
assumption [21  23  45  62]  however  to the best of our knowledge  our result is the ﬁrst such result
for score-based estimators in high-dimensions. Instead  we require a beta-min condition on the true

DAG eBmin  which is typical in the statistical literature on model selection.
Gap condition Condition 3.1(b) imposes an implicit assumption on the degree of G through the
requirement 1(G)  1 which roughly translates to s log(p/s) + log p . n. The assumption on (⌘) 
on the other hand  is a type of identiﬁability condition on eBmin. Whereas Condition 3.1(a) requires
eBmin to be identiﬁable in the inﬁnite sample limit  Condition 3.1(b) requires that there is a “gap” on
the orderps log p/n between the expected loss of eBmin and the expected loss of any other DAG in
D(⌃). To see this  note that EkX  XeBk2
(10)
When ⇢ is the MCP and  & ⌘  a straightforward calculation shows that the following two conditions
are sufﬁcient to guarantee Condition 3.1(b)  in addition to 1  1: There exists a  0 such that

gap(⌃) := inftre⌦  tre⌦min :e⌦ 6=e⌦min  e⌦ 2 D(⌃) .

F /n = tre⌦ for any (eB e⌦) 2 D(⌃) and deﬁne

gap(⌃) &h s log(p/s) + log p
keBmink0 .h

ia
s log(p/s) + log pi1 a

n
n

p 

2 p.

(11)

(12)

Thus  Condition 3.1(b) allows one to trade off the size of the “gap” in (11) with a sparsity condition

(12) on eBmin. For example  taking a 2 (0  2) and s log(p/s) + log p ⌧ n allows gap(⌃) = o(p)
while simultaneously tolerating an average degree keBmink0/p that grows without bound (cf. (12)).

Since the problem considered here is at least as hard as p separate regression problems  this scaling
in terms of p is expected. Similar conditions with a similar scaling have appeared in previous work
[35  62].

4 The general case: Recovery of sparse representations

In the previous section  we leveraged strong prior information—namely identiﬁability and a consistent
superstructure—in order to analyze the sample complexity of learning a minimum-trace DAG. In
practice  such prior information may not be available  and in general it is well-known that Gaussian
DAGs are not identiﬁable [2  62]. The estimator (1)  of course  is well-deﬁned whether or not

Condition 3.1 holds  and in practice  one typically computes bB and “hopes for the best”. Is it possible
to say more in the general setting? Surprisingly  even if there is no DAG eB 2 D(⌃) that is identiﬁable 
we can still provide guarantees. The idea is to ﬁrst show that bB converges to some DAG eB 2 D(⌃) 
and then show that eB is well-behaved compared to other representative DAGs in D(⌃). Speciﬁcally 
we will show that eB is roughly as sparse as a minimum-trace DAG.
Deﬁnition 4.1. Letej denote the jth column of eB 2 D. For any ⌃  let
⇤(D(⌃)) := inf
eB2D(⌃)

4.1 Assumptions

⌧⇤(eB).

d(D(⌃)) := sup

We will write d = d(D(⌃)) to simplify the notation in the sequel.

(13)

Condition 4.1 (Minimum-trace DAG). ⌃  0  and there is a minimum-trace DAG eBmin such that
Condition 4.1 can be interpreted as putting a soft lower bound on the weights in eBmin  as measured
by the regularizer ⇢ ande⌦min. For comparison  recall that the usual beta-min condition in regression
is minj |j| & plog p/n.

for some a2 > 0.

eB2D(⌃)keBk0 ⌧
tre⌦min  a2r (d + 1) log p
⇢(eBmin)

n

6

4.2 Second main result: The nonidentiﬁable case
Our second result shows that even in the absence of identiﬁability assumptions  we can still guarantee

approaches any particular member of D(⌃).
Theorem 4.1. Suppose that Conditions 2.1 (with G the complete graph in the case of `1) and 4.1

that bB recovers the support of a DAG eB 2 D(⌃)  and that eB must also be sparse. In fact  we note
that even without the sparsity conclusion  it is not obvious (and indeed nontrivial to show) that bB
hold. If n & d log p   &p(d + 1) log p/n  and ⌧⇤(D(⌃)) &  then there exists eB 2 D(⌃) and a
minimum-trace DAG eBmin 2 D(⌃) such that
supp(bB) = supp(eB)
with probability at least 1  O(ed log p).
This is similar to the approach taken in van de Geer and Bühlmann [62] with some key differences:
1) Their Theorem 3.1 does not establish structure consistency  and 2) Their `0-regularized MLE
involves a thresholded parameter space that is much more difﬁcult to compute in practice  whereas
our estimator (1) is deﬁned over the full parameter space and involves continuous optimization.
In contrast to Theorem 3.1  Theorem 4.1 no longer requires the identiﬁability condition (Condi-

and ⇢(eB) . ⇢(bB) . ⇢(eBmin)

tion 3.1)  which is replaced by Condition 4.1 on eBmin. The tradeoffs are 1) The estimator bB is no

longer guaranteed to recover an exact minimum-trace DAG  and 2) The beta-min condition and
sample complexity now depend on the sparsity parameter d  which may be larger than s and can
be large for general covariance matrices. This result also emphasizes the advantages of noncon-
vex regularization: When `1-regularization is used  the incoherence condition (6) is imposed over
every neighbourhood  which is a very severe restriction. With the MCP  there are no incoherence
assumptions whatsoever.

interpreted as a “soft” notion of sparsity.

Strong faithfulness and the beta-min condition In contrast to Theorem 3.1  which only requires

Sparsity A key conclusion in Theorem 4.1 is that ⇢(eB) . ⇢(bB) . ⇢(eBmin): This says that bB
is consistent with a parsimonious DAG. It is easy to show that this implies keBk0 . kbBk0 . keBmink0
for the MCP regularizer. For the `1 penalty  we have keBk1 . kbBk1 . keBmink1  which can be
a beta-min condition on the true DAG eBmin  Theorem 4.1 requires a much stronger condition on the

smallest weight of any DAG in the equivalence class D(⌃) (cf. (13)). This is reminiscent of—but
not the same as—the strong faithfulness condition  which roughly asserts that the minimum partial
correlation between any pair of d-separated variables in the true DAG is bounded away from zero. We
leave it to future work to study this connection more carefully  however  we note here that previous
work on this problem [61] has noted the difﬁculty of establishing such an explicit relationship  and to
the best of our knowledge this remains an open problem. Nonetheless  the novelty of Theorem 4.1 is
in establishing ﬁnite-sample structure recovery without imposing any identiﬁability requirement  so
it is natural to expect that stronger assumptions will be needed.

5 Proof outline

Our basic strategy is to reduce the analysis of bB to a family of neighbourhood regression problems 
using a similar approach as in our preprint [2]. This is similar to undirected models  for which the
analysis can be reduced to p different regression problems  namely the regression of Xj onto Xj
[39  69]. Unfortunately  for DAGs  there are p2p possible regression problems (the regression of Xj
onto any subset S ⇢ [p]j)  which quickly become intractable to control uniformly. The manner in
which these problems are controlled highlights the main technical difference between the proofs of
Theorems 3.1 and 4.1.
To prove Theorem 3.1  we ﬁrst prove a uniform concentration result for the score Q(B). Speciﬁcally 
letting `(B) = kX  XBk2
F /(2n)  we show that the following upper bound holds with high
probability over DG (Proposition B.7):

|`(B)  E`(B)| 1⇥1 + 6(⌃; s)2⇤E`(B)

7

for all B 2 DG.

(14)

bj(S) 2

arg min

✓2Rm  supp(✓) ⇢ S

1
2nkxj  X✓k2

2 + ⇢(✓).

Based on this result  we show that bB has the same topological sort as eBmin. This topological sort
identiﬁes candidate parent sets for each node Xj  and reduces the problem to p regression problems.
The main technical device here is uniform score concentration via (14)  which is an interesting result
in its own right due to its uniform control of an unbounded  subexponential empirical process. We
note here that the requirement that 1(G)  1 in Condition 3.1(b) is precisely the condition needed
to ensure uniform concentration is possible over the restricted space DG.
The proof of Theorem 4.1 is more subtle and involved. Since we no longer assume we can restrict to
a superstructure  uniform score concentration (i.e. over the full space D) is no longer readily viable.
As a result  we must obtain uniform control over all p2p neighbourhood regression problems. Let
j(S) =⌃ 1
SS⌃Sj denote the population regression coefﬁcients of Xj onto XS  where S ⇢ [p]j. It
is not hard to show that bB reduces to estimating j(S) for p random sets S that depend on X with

the penalized least-squares estimator

It turns out that these estimators have a great deal of redundancy  and in order to control all p2p such
estimators  it sufﬁces to control at most O(pd) of them. In order to prove this  we show that the
following set system has a largest element Mj(S) (Lemma B.2):

Tj(S) = {T ⇢ [p]j : j(T ) = j(S)}.

Let Mj(S) be this largest element  i.e. T 2T j(S) =) T ⇢ Mj(S). Then there are at most O(pd)
such sets  and we show that in order to control j(S) for all S  it sufﬁces to control each j(Mj(S))
(Corollary B.4). The ﬁnal piece of the proof is to establish control over ⇢(bB); this follows from a

somewhat lengthy but straightforward Gaussian concentration argument.

6 Discussion

We have established that a score-based estimator achieves ⌦(s log p) sample complexity for learning
a sparse  minimum-trace DAG  and extended these results to the nonidentiﬁable setting. The proof
technique is novel  leveraging the lattice structure of Gaussian conditional independence. Compared
to recent theoretical work on DAG learning that sidesteps optimization altogether  our approach
directly attacks a difﬁcult nonconvex optimization problem. To conclude this paper  we discuss some
limitations  extensions  and directions for future research.

NP-hard [7]. Fortunately  there are fast algorithms via dynamic programming for ﬁnding globally
optimal Bayesian networks [43  55  56]. For example  by combining dynamic programming with A*

Computation Since (1) is a nonconvex program  computation of bB is challenging and in fact
search  Xiang and Kim [68] propose an exact algorithm to compute the `1-regularized version of bB
true global minimum in practice. Given the NP-hardness of computing bB  an important direction

that is tractable on problems with hundreds of nodes. More recently  a mixed-integer formulation
has also been proposed [9  10]. Recent work [77] has also shown that the program (1) can be solved
approximately with second-order methods  and the resulting solutions are often very close to the

for future work is to determine whether or not there exists a polynomial-time estimator that can
achieve s log p sample complexity or better. As such  the current work provides important theoretical
justiﬁcation for this inquiry.

Comparison to existing methods Despite the long history of score-based methods for learning
DAGs  very little is known about the explicit  ﬁnite-sample behaviour of these methods. We have
already acknowledged that the estimator (1) has appeared previously in the literature without a
rigourous theoretical analysis [e.g. 26  51  54  68  77]. The well-known GES algorithm  on the other
hand  has asymptotic consistency guarantees in both the low- [6] and high-dimensional [40] settings.
We do not pursue a detailed experimental comparison of these two popular approaches here for the
simple reason that this has already been done  see e.g. [1  68  70  77]. These papers indicate that even

on a wide variety of settings and graphs.

approximate algorithms for bB outperform GES (along with other algorithms such as PC and MMHC)

8

Comparison to nonconvex models in ML Much of the interest in the current work stems not
only from providing explicit ﬁnite-sample guarantees for the DAG learning problem  but also from
its analysis of a highly nonconvex optimization problem. For this reason  it is worth comparing
our results with recent work on nonconvex models in the ML literature [5  8  13  14  16  18  19 
28–30  33  38  58  60].
In particular  we note the spate of recent papers on so-called “benign
nonconvexity”  which is the idea that although a problem may be nonconvex  its geometry is such that
the nonconvexity is not a practical issue. Conditions ensuring this include the Polyak-Lojasiewicz
condition [32]  restricted strong convexity [36]  and “strict” or “rideable” saddle points [17  59].
Unfortunately  this approach of benign nonconvexity does not apply to optimizing (1) since this
problem is easily shown to violate these properties  and in particular  there exist local minima that
are not global. While this may seem discouraging  we note that recent work [77] has shown that
second-order algorithms often ﬁnd the global minimum in practice. We leave it to future work to
study this behaviour in more detail.

Acknowledgments
We thank the anonymous reviewers for their feedback. The authors acknowledge the support of the
NSF via IIS-1546098.

References
[1] B. Aragam and Q. Zhou. Concave penalized estimation of sparse Gaussian Bayesian networks.

Journal of Machine Learning Research  16:2273–2328  2015.

[2] B. Aragam  A. A. Amini  and Q. Zhou. Learning directed acyclic graphs with penalized
neighbourhood regression. arXiv:1511.08963  2015. URL https://arxiv.org/abs/1511.
08963.

[3] B. Aragam  J. Gu  and Q. Zhou. Learning large-scale bayesian networks with the sparsebn

package. To appear  Journal of Statistical Software  arXiv:1703.04025  2017.

[4] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

Annals of Statistics  37(4):1705–1732  2009.

[5] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian

inputs. arXiv preprint arXiv:1702.07966  2017.

[6] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine

Learning Research  3:507–554  2003.

[7] D. M. Chickering  D. Heckerman  and C. Meek. Large-sample learning of Bayesian networks

is NP-hard. Journal of Machine Learning Research  5:1287–1330  2004.

[8] A. Choromanska  M. Henaff  M. Mathieu  G. B. Arous  and Y. LeCun. The loss surfaces of

multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204  2015.

[9] J. Cussens. Bayesian network learning with cutting planes. arXiv preprint arXiv:1202.3713 

2012.

[10] J. Cussens  D. Haws  and M. Studen`y. Polyhedral aspects of score equivalence in bayesian

network structure learning. Mathematical Programming  164(1-2):285–324  2017.

[11] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. arXiv

preprint arXiv:1702.08608  2017.

[12] M. Drton  W. Chen  and Y. S. Wang. On causal discovery with equal variance assumption.

arXiv preprint arXiv:1807.03419  2018.

[13] S. S. Du  J. D. Lee  and Y. Tian. When is a convolutional ﬁlter easy to learn? arXiv preprint

arXiv:1709.06129  2017.

[14] S. S. Du  J. D. Lee  Y. Tian  B. Poczos  and A. Singh. Gradient descent learns one-hidden-layer

cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779  2017.

9

[15] R. J. Evans. Model selection and local geometry. arXiv preprint arXiv:1801.08364  2018.

[16] R. Ge and T. Ma. On the optimization landscape of tensor decompositions. In Advances in

Neural Information Processing Systems  pages 3653–3663  2017.

[17] R. Ge  F. Huang  C. Jin  and Y. Yuan. Escaping from saddle points—online stochastic gradient

for tensor decomposition. In Conference on Learning Theory  pages 797–842  2015.

[18] R. Ge  F. Huang  C. Jin  and Y. Yuan. Escaping from saddle points — online stochastic gradient
for tensor decomposition. In P. Grünwald  E. Hazan  and S. Kale  editors  Proceedings of The
28th Conference on Learning Theory  volume 40 of Proceedings of Machine Learning Research 
pages 797–842  Paris  France  03–06 Jul 2015. PMLR. URL http://proceedings.mlr.
press/v40/Ge15.html.

[19] R. Ge  J. D. Lee  and T. Ma. Matrix completion has no spurious local minimum. In Advances in

Neural Information Processing Systems  pages 2973–2981  2016.

[20] D. Geiger and D. Heckerman. Parameter priors for directed acyclic graphical models and the
characterization of several probability distributions. Annals of Statistics  30:1412–1440  2002.

[21] A. Ghoshal and J. Honorio. Learning identiﬁable gaussian bayesian networks in polynomial

time and sample complexity. 03 2017. URL https://arxiv.org/abs/1703.01196.

[22] A. Ghoshal and J. Honorio. Information-theoretic limits of Bayesian network structure learning.
In A. Singh and J. Zhu  editors  Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics  volume 54 of Proceedings of Machine Learning Research  pages
767–775  Fort Lauderdale  FL  USA  20–22 Apr 2017. PMLR. URL http://proceedings.
mlr.press/v54/ghoshal17a.html.

[23] A. Ghoshal and J. Honorio. Learning linear structural equation models in polynomial time and

sample complexity. 07 2017. URL https://arxiv.org/abs/1707.04673.

[24] Y. Gordon. On Milman’s inequality and random subspaces which escape through a mesh in Rn.

Geometric Aspects of Functional Analysis  pages 84–106  1988.

[25] J. Gu  F. Fu  and Q. Zhou. Penalized estimation of directed acyclic graphs from discrete data.

Statistics and Computing  DOI: 10.1007/s11222-018-9801-y  2018.

[26] S. W. Han  G. Chen  M.-S. Cheon  and H. Zhong. Estimation of directed acyclic graphs through
two-stage adaptive lasso for gene network inference. Journal of the American Statistical
Association  111(515):1004–1019  2016.

[27] J. Huang  P. Breheny  and S. Ma. A selective review of group selection in high-dimensional
models. Statistical science: a review journal of the Institute of Mathematical Statistics  27(4) 
2012.

[28] C. Jin  Y. Zhang  S. Balakrishnan  M. Wainwright  and M. Jordan. Local maxima in the likeli-
hood of gaussian mixture models: Structural results and algorithmic consequences. pages
4123–4131  2016. URL https://www.scopus.com/inward/record.uri?eid=2-s2.
0-85019182259&partnerID=40&md5=ffefd6d85c791c4ea475d149cb372d52. cited By
3.

[29] C. Jin  R. Ge  P. Netrapalli  S. M. Kakade  and M. I. Jordan. How to escape saddle points

efﬁciently. arXiv preprint arXiv:1703.00887  2017.

[30] C. Jin  L. T. Liu  R. Ge  and M. I. Jordan. Minimizing Nonconvex Population Risk from Rough

Empirical Risk. ArXiv e-prints  Mar. 2018.

[31] M. Kalisch and P. Bühlmann. Estimating high-dimensional directed acyclic graphs with the

PC-algorithm. Journal of Machine Learning Research  8:613–636  2007.

[32] H. Karimi  J. Nutini  and M. Schmidt. Linear convergence of gradient and proximal-gradient

methods under the Polyak-Łojasiewicz condition. ECML  2016.

10

[33] K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  pages 586–594  2016.

[34] S. Lin  C. Uhler  B. Sturmfels  and P. Bühlmann. Hypersurfaces and their singularities in partial

correlation testing. Foundations of Computational Mathematics  14(5):1079–1116  2014.

[35] P.-L. Loh and P. Bühlmann. High-dimensional learning of linear causal networks via inverse

covariance estimation. Journal of Machine Learning Research  15:3065–3105  2014.

[36] P.-L. Loh and M. J. Wainwright. Support recovery without incoherence: A case for nonconvex

regularization. arXiv preprint arXiv:1412.5632  2014.

[37] P.-L. Loh and M. J. Wainwright. Regularized M-estimators with nonconvexity: Statistical and
algorithmic theory for local optima. Journal of Machine Learning Research  16:559–616  2015.

[38] S. Mei  Y. Bai  and A. Montanari. The landscape of empirical risk for nonconvex losses. The

Annals of Statistics  46(6A):2747–2774  2018.

[39] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with the

Lasso. Annals of Statistics  34(3):1436–1462  2006.

[40] P. Nandy  A. Hauser  and M. H. Maathuis. High-dimensional consistency in score-based and

hybrid structure learning. arXiv preprint arXiv:1507.02608  2018.

[41] A. Nicholson  F. Cozman  M. Velikova  J. T. van Scheltinga  P. J. Lucas  and M. Spaanderman.
Applications of bayesian networks exploiting causal functional relationships in bayesian network
modelling for personalised healthcare. International Journal of Approximate Reasoning  55
(1):59 – 73  2014. ISSN 0888-613X. doi: http://dx.doi.org/10.1016/j.ijar.2013.03.016. URL
http://www.sciencedirect.com/science/article/pii/S0888613X13000777.

[42] S. Ordyniak and S. Szeider. Parameterized complexity results for exact bayesian network

structure learning. Journal of Artiﬁcial Intelligence Research  46:263–302  2013.

[43] S. Ott and S. Miyano. Finding optimal gene networks using biological constraints. Genome

Informatics  14:124–133  2003.

[44] S. Ott  S. Imoto  and S. Miyano. Finding optimal models for small gene networks. In Paciﬁc

symposium on biocomputing  volume 9  pages 557–567. Citeseer  2004.

[45] G. Park and G. Raskutti. Learning quadratic variance function (qvf) dag models via overdisper-

sion scoring (ods). 04 2017. URL https://arxiv.org/abs/1704.08783.

[46] E. Perrier  S. Imoto  and S. Miyano. Finding optimal bayesian network given a super-structure.

Journal of Machine Learning Research  9(Oct):2251–2286  2008.

[47] J. Peters and P. Bühlmann. Identiﬁability of Gaussian structural equation models with equal

error variances. Biometrika  101(1):219–228  2013.

[48] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue properties for correlated

Gaussian designs. Journal of Machine Learning Research  11:2241–2259  2010.

[49] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax rates of estimation for high-dimensional
linear regression over `q-balls. Information Theory  IEEE Transactions on  57(10):6976–6994 
2011.

[50] A. D. Sanford and I. A. Moosa. A bayesian network structure for operational risk modelling
in structured ﬁnance operations. Journal of the Operational Research Society  63(4):431–444 
2012.

[51] M. Schmidt  A. Niculescu-Mizil  and K. Murphy. Learning graphical model structure using

L1-regularization paths. In AAAI  volume 7  pages 1278–1283  2007.

[52] S. Shimizu  P. O. Hoyer  A. Hyvärinen  and A. Kerminen. A linear non-Gaussian acyclic model

for causal discovery. Journal of Machine Learning Research  7:2003–2030  2006.

11

[53] S. Shimizu  T. Inazumi  Y. Sogawa  A. Hyvärinen  Y. Kawahara  T. Washio  P. O. Hoyer  and
K. Bollen. Directlingam: A direct method for learning a linear non-gaussian structural equation
model. Journal of Machine Learning Research  12:1225–1248  2011.

[54] A. Shojaie and G. Michailidis. Penalized likelihood methods for estimation of sparse high-

dimensional directed acyclic graphs. Biometrika  97(3):519–538  2010.

[55] T. Silander and P. Myllymaki. A simple approach for ﬁnding the globally optimal bayesian net-
work structure. In Proceedings of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence 
2006.

[56] A. P. Singh and A. W. Moore. Finding optimal bayesian networks by dynamic programming.

2005.

[57] P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social

Science Computer Review  9(1):62–72  1991.

[58] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery using nonconvex optimization.
In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages
2351–2360  2015.

[59] J. Sun  Q. Qu  and J. Wright. When are nonconvex problems not scary? arXiv preprint

arXiv:1510.06096  2015.

[60] J. Sun  Q. Qu  and J. Wright. A geometric analysis of phase retrieval. In Information Theory

(ISIT)  2016 IEEE International Symposium on  pages 2379–2383. IEEE  2016.

[61] C. Uhler  G. Raskutti  P. Bühlmann  and B. Yu. Geometry of the faithfulness assumption in

causal inference. Annals of Statistics  41(2):436–463  2013.

[62] S. van de Geer and P. Bühlmann. `0-penalized maximum likelihood for sparse directed acyclic

graphs. Annals of Statistics  41(2):536–567  2013.

[63] S. van de Geer et al. On the uniform convergence of empirical norms and inner products  with

application to causal inference. Electronic Journal of Statistics  8:543–574  2014.

[64] K. R. Varshney  P. Khanduri  P. Sharma  S. Zhang  and P. K. Varshney. Why interpretability in
machine learning? an answer using distributed detection and data fusion theory. arXiv preprint
arXiv:1806.09710  2018.

[65] S. Wachter  B. Mittelstadt  and C. Russell. Counterfactual explanations without opening the

black box: Automated decisions and the gdpr. 2017.

[66] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using-
constrained quadratic programming (Lasso). Information Theory  IEEE Transactions on  55(5):
2183–2202  2009.

[67] Y. S. Wang and M. Drton. High-dimensional causal discovery under non-gaussianity. 03 2018.

URL https://arxiv.org/pdf/1803.11273.

[68] J. Xiang and S. Kim. A* Lasso for learning a sparse Bayesian network structure for continuous

variables. In Advances in Neural Information Processing Systems  pages 2418–2426  2013.

[69] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. Graphical models via univariate exponential

family distributions. Journal of Machine Learning Research  16:3813–3847  2015.

[70] Q. Ye  A. A. Amini  and Q. Zhou. Optimizing regularized cholesky score for order-based

learning of bayesian networks. arXiv preprint arXiv:1904.12360  2019.

[71] Y. Yuan  X. Shen  W. Pan  and Z. Wang. Constrained likelihood for reconstructing a directed

acyclic gaussian graph. Biometrika  106(1):109–125  2018.

12

[72] B. Zhang  C. Gaiteri  L.-G. Bodea  Z. Wang  J. McElwee  A. A. Podtelezhnikov  C. Zhang 
T. Xie  L. Tran  R. Dobrin  E. Fluder  B. Clurman  S. Melquist  M. Narayanan  C. Suver  H. Shah 
M. Mahajan  T. Gillis  J. Mysore  M. E. MacDonald  J. R. Lamb  D. A. Bennett  C. Molony 
D. J. Stone  V. Gudnason  A. J. Myers  E. E. Schadt  H. Neumann  J. Zhu  and V. Emilsson.
Integrated systems approach identiﬁes genetic nodes and networks in late-onset alzheimer’s
disease. Cell  153(3):707—720  April 2013. ISSN 0092-8674. doi: 10.1016/j.cell.2013.03.030.
URL http://europepmc.org/articles/PMC3677161.

[73] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of

Statistics  38(2):894–942  2010.

[74] C.-H. Zhang and J. Huang. The sparsity and bias of the Lasso selection in high-dimensional

linear regression. Annals of Statistics  pages 1567–1594  2008.

[75] C.-H. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional

sparse estimation problems. Statistical Science  27(4):576–593  2012.

[76] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2563  2006.

[77] X. Zheng  B. Aragam  P. K. Ravikumar  and E. P. Xing. DAGs with NO TEARS: Continuous
optimization for structure learning. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems
31  pages 9472–9483. Curran Associates  Inc.  2018. URL http://papers.nips.cc/paper/
8157-dags-with-no-tears-continuous-optimization-for-structure-learning.
pdf.

13

,Bryon Aragam
Arash Amini
Qing Zhou