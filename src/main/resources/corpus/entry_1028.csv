2018,Community Exploration: From Offline Optimization to Online Learning,We introduce the community exploration problem that has various real-world applications such as online advertising. In the problem  an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem  from offline optimization to online learning. For the offline setting where the sizes of communities are known  we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations  we propose an ``upper confidence'' like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds  we can achieve a constant regret bound.,Community Exploration: From Ofﬂine Optimization

to Online Learning

Xiaowei Chen1  Weiran Huang2  Wei Chen3  John C.S. Lui1

1The Chinese University of Hong Kong

2Huawei Noah’s Ark Lab  3Microsoft Research

1{xwchen  cslui}@cse.cuhk.edu.hk  2huang.inbox@outlook.com

3weic@microsoft.com

Abstract

We introduce the community exploration problem that has many real-world appli-
cations such as online advertising. In the problem  an explorer allocates limited
budget to explore communities so as to maximize the number of members he could
meet. We provide a systematic study of the community exploration problem  from
ofﬂine optimization to online learning. For the ofﬂine setting where the sizes of
communities are known  we prove that the greedy methods for both of non-adaptive
exploration and adaptive exploration are optimal. For the online setting where the
sizes of communities are not known and need to be learned from the multi-round
explorations  we propose an “upper conﬁdence” like algorithm that achieves the
logarithmic regret bounds. By combining the feedback from different rounds  we
can achieve a constant regret bound.

1

Introduction

In this paper  we introduce the community exploration problem  which is abstracted from many
real-world applications. Consider the following hypothetical scenario. Suppose that John just entered
the university as a freshman. He wants to explore different student communities or study groups at
the university to meet as many new friends as possible. But he only has a limited time to spend on
exploring different communities  so his problem is how to allocate his time and energy to explore
different student communities to maximize the number of people he would meet.
The above hypothetical community exploration scenario can also ﬁnd similar counterparts in serious
business and social applications. One example is online advertising. In this application  an advertiser
wants to promote his products via placing advertisements on different online websites. The website
would show the advertisements on webpages  and visitors to the websites may click on the advertise-
ments when they view these webpages. The advertiser wants to reach as many unique customers as
possible  but he only has a limited budget to spend. Moreover  website visitors come randomly  so it
is not guaranteed that all visitors to the same website are unique customers. So the advertiser needs
to decide how to spend the budget on each website to reach his customers. Of course  intuitively he
should spend more budget on larger communities  but how much? And what if he does not know the
user size of every website? In this case  each website is a community  consisting of all visitors to this
website  and the problem can be modeled as a community exploration problem. Another example
could be a social worker who wants to reach a large number of people from different communities to
do social studies or improve the social welfare for a large population  while he also needs to face the
budget constraint and uncertainty about the community.
In this paper  we abstract the common features of these applications and deﬁne the following
community exploration problem that reﬂects the common core of the problem. We model the problem
with m disjoint communities C1  . . .   Cm with C = ∪m
i=1Ci  where each community Ci has di

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

+ with(cid:80)m

members. Each time when one explores (or visit) a community Ci  he would meet one member of
Ci uniformly at random.1 Given a budget K  the goal of community exploration is to determine the
budget allocation k = (k1  . . .   km) ∈ Zm
i=1 ki ≤ K  such that the total number of distinct
members met is maximized when each community Ci is explored ki times.
We provide a systematic study of the above community exploration problem  from ofﬂine optimization
to online learning. First  we consider the ofﬂine setting where the community sizes are known. In
this setting  we further study two problem variants — the non-adaptive version and the adaptive
version. The non-adaptive version requires that the complete budget allocation k is decided before
the exploration is started  while the adaptive version allows the algorithm to use the feedback from
the exploration results of the previous steps to determine the exploration target of the next step. In
both cases  we prove that the greedy algorithm provides the optimal solution. While the proof for
the non-adaptive case is simple  the proof that the adaptive greedy policy is optimal is much more
involved and relies on a careful analysis of transitions between system statuses. The proof techniques
may be applicable in the analysis of other related problems.
Second  we consider the online setting where the community sizes are unknown in advance  which
models the uncertainty about the communities in real applications. We apply the multi-armed bandit
(MAB) framework to this task  in which community explorations proceed in multiple rounds  and
in each round we explore communities with a budget of K  use the feedback to learn about the
community size  and adjust the exploration strategy in future rounds. The reward of a round is the
the expected number of unique people met in the round. The goal is to maximize the cumulative
reward from all rounds  or minimizing the regret  which is deﬁned as the difference in cumulative
reward between always using the optimal ofﬂine algorithm when knowing the community sizes and
using the online learning algorithm. Similar to the ofﬂine case  we also consider the non-adaptive and
adaptive version of exploration within each round. We provide theoretical regret bounds of O(log T )
for both versions  where T is the number of rounds  which is asymptotically tight. Our analysis uses
the special feature of the community exploration problem  which leads to improved coefﬁcients in the
regret bounds compared with a simple application of some existing results on combinatorial MABs.
Moreover  we also discuss the possibility of using the feedback in previous round to turn the problem
into the full information feedback model  which allows us to provide constant regret in this case.
In summary  our contributions include: (a) proposing the study of the community exploration problem
to reﬂect the core of a number of real-world applications; and (b) a systematic study of the problem
with rigorous theoretical analysis that covers ofﬂine non-adaptive  ofﬂine adaptive  online non-
adaptive and online adaptive cases  which model the real-world situations of adapting to feedback
and handling uncertainty.

2 Problem Deﬁnition
We model the problem with m disjoint communities C1  . . .   Cm with C = ∪m
i=1Ci  where each
community Ci has di members. Each exploration (or visit) of one community Ci returns a member
of Ci uniformly at random  and we have a total budget of K for explorations. Since we can trivially
explore each community once when K ≤ m  we assume that K > m.
We consider both the ofﬂine setting where the sizes of the communities d1  . . .   dm are known  and
the online setting where the sizes of the communities are unknown. For the ofﬂine setting  we further
consider two different problems: (1) non-adaptive exploration and (2) adaptive exploration. For
the non-adaptive exploration  the explorer needs to predetermine the budget allocation k before the
exploration starts  while for the adaptive exploration  she can sequentially select the next community
to explore based on previous observations (the members met in the previous community visits).
Formally  we use pair (i  τ ) to represent the τ-th exploration of community Ci  called an item. Let
E = [m] × [K] be the set of all possible items. A realization is a function φ : E → C mapping
every possible item (i  τ ) to a member in the corresponding community Ci  and φ(i  τ ) represents the
member met in the exploration (i  τ ). We use Φ to denote a random realization  and the randomness
comes from the exploration results. From the description above  Φ follows the distribution such that
Φ(i  τ ) ∈ Ci is selected uniformly at random from Ci and is independent of all other Φ(i(cid:48)  τ(cid:48))’s.

1The model can be extended to meet multiple members per visit  but for simplicity  we consider meeting one

member per visit in this paper.

2

of distinct members met  i.e.  R(k  φ) =(cid:80)m

For a budget allocation k = (k1  . . .   km) and a realization φ  we deﬁne the reward R as the number
τ =1{φ(i  τ )}|  where |·| is the cardinality of the set.
The goal of the non-adaptive exploration is to ﬁnd an optimal budget allocation k∗ = (k∗
1  . . .   k∗
m)
with given budget K  which maximizes the expected reward taken over all possible realizations  i.e. 
(1)

EΦ [R(k  Φ)] .

i=1 |∪ki

k∗ ∈ arg max
k : (cid:107)k(cid:107)1≤K

For the adaptive exploration  the explorer sequentially picks a community to explore  meets a random
member of the chosen community  then picks the next community  meets another random member
of that community  and so on  until the budget is used up. After each selection  the observations so
far can be represented as a partial realization ψ  a function from the subset of E to C = ∪m
i=1Ci.
Suppose that each community Ci has been explored ki times. Then the partial realization ψ is a
function mapping items in ∪m
i=1{(i  1)  . . .   (i  ki)} (which is also called the domain of ψ  denoted
as dom(ψ)) to members in communities. The partial realization ψ records the observation on the
sequence of explored communities and the members met in this sequence. We say that a partial
realization ψ is consistent with realization φ  denoted as φ ∼ ψ  if for all item (i  τ ) in the domain
of ψ  we have ψ(i  τ ) = φ(i  τ ). The strategy to explore the communities adaptively is encoded
as a policy. The policy  denoted as π  is a function mapping ψ to an item in E  specifying which
community to explore next under the partial realization. Deﬁne πK(φ) = (k1  . . .   km)  where ki
is the times the community Ci is explored via policy π under realization φ with budget K. More
speciﬁcally  starting from the partial realization ψ0 with empty domain  for every current partial
realization ψs at step s  policy π determines the next community π(ψs) to explore  meet the member
φ(π(ψs))  such that the new partial realization ψs+1 is adding the mapping from π(ψs) to φ(π(ψs))
on top of ψs. This iteration continues until the communities have been explored K times  and
πK(φ) = (k1  . . .   km) denotes the resulting exploration vector. The goal of the adaptive exploration
is to ﬁnd an optimal policy π∗ to maximize the expected adaptive reward  i.e. 

π∗ ∈ arg max

π

EΦ [R(πK(Φ)  Φ)] .

(2)

We next consider the online setting of community exploration. The learning process proceeds in
discrete rounds. Initially  the size of communities d = (d1  . . .   dm) is unknown. In each round
t ≥ 1  the learner needs to determine an allocation or a policy (called an “action”) based on the
previous-round observations to explore communities (non-adaptively or adaptively). When an action
is played  the sets of encountered members for every community are observed as the feedback to the
player. A learning algorithm A aims to cumulate as much reward (i.e.  number of distinct members)
as possible by selecting actions properly at each round. The performance of a learning algorithm is
measured by the cumulative regret. Let Φt be the realization at round t. If we explore the communities
with predetermined budget allocation in each round  the T -round (non-adaptive) regret of a learning
algorithm A is deﬁned as

RegA

µ(T ) = EΦ1 ... ΦT

∗

  Φt) − R(kA

t   Φt)

R(k

 

(3)

where the budget allocation kA
is selected by algorithm A in round t. If we explore the communities
t
adaptively in each round  then the T -round (adaptive) regret of a learning algorithm A is deﬁned as

RegA

µ(T ) = EΦ1 ... ΦT

K (Φt)  Φt) − R(πA t
∗

R(π

K (Φt)  Φt)

 

(4)

where πA t is a policy selected by algorithm A in round t. The goal of the learning problem is to
design a learning algorithm A which minimizes the regret deﬁned in (3) and (4).

t=1

3 Ofﬂine Optimization for Community Exploration

3.1 Non-adaptive Exploration Algorithms
If Ci is explored ki times  each member in Ci is encountered at least once with probability 1 − (1 −
1/di)ki . Thus we have EΦ[|{Φ(i  1)  . . .   Φ(i  ki)}|] = di(1 − (1 − 1/di)ki). Hence EΦ [R(k  Φ)]
is a function of only the budget allocation k and the size d = (d1  . . .   dm) of all communities.

3

(cid:34) T(cid:88)

t=1

(cid:34) T(cid:88)

(cid:35)

(cid:35)

For i ∈ [m]  ki ← 0
for s = 1  . . .   K do

Algorithm 1 Non-Adaptive community exploration with optimal budget allocation
1: procedure CommunityExplore({µ1  . . .   µm}  K  non-adaptive)
2:
3:
4:
5:
6:
7: end procedure

i∗ ← a random elements in arg maxi(1 − µi)ki
ki∗ ← ki∗ + 1

For i ∈ [m]  explore Ci for ki times  and put the uniformly met members in multi-set Si

(cid:46) O(log m) via using priority queue

(cid:46) Line 2-5: budget allocation

For i ∈ [m]  Si ← ∅  ci ← 0
for s = 1  . . .   K do

Algorithm 2 Adaptive community exploration with greedy policy
1: procedure CommunityExplore({µ1  . . .   µm}  K  adaptive)
2:
3:
4:
5:
6:
7:
8: end procedure

i∗ ← a random elements in arg maxi 1 − µici
v ← a random member met when Ci∗ is explored
if v /∈ Si∗ then ci∗ ← ci∗ + 1
Si∗ ← Si∗ ∪ {v}

(cid:46) Line 2-7: adaptively explore communities with policy πg

(cid:46) v is not met before

Let µi = 1/di  and vector µ = (1/d1  . . .   1/dm). Henceforth  we treat µ as the parameter of the
problem instance  since it is bounded with µ ∈ [0  1]m. Let rk(µ) = EΦ[R(k  Φ)] be the expected
reward for the budget allocation k. Based on the above discussion  we have

m(cid:88)

m(cid:88)

rk(µ) =

di(1 − (1 − 1/di)ki) =

(1 − (1 − µi)ki)/µi.

(5)

i=1

i=1

Since ki must be integers  a traditional method like Lagrange Multipliers cannot be applied to solve
the optimization problem deﬁned in Eq. (1). We propose a greedy method consisting of K steps to
compute the feasible k∗. The greedy method is described in Line 2-5 of Algo. 1.
Theorem 1. The greedy method obtains an optimal budget allocation.

The time complexity of the greedy method is O(K log m)  which is not efﬁcient for large K. We
ﬁnd that starting from the initial allocation ki =
  the greedy method can ﬁnd the
optimal budget allocation in O(m log m)2. (See supplementary materials.)

(cid:108) (K−m)/ ln(1−µi)
(cid:80)m
j=1 1/ ln(1−µj )

(cid:109)

3.2 Adaptive Exploration Algorithms
With a slight abuse of notations  we also deﬁne rπ(µ) = EΦ [R(πK(Φ)  Φ)]  since the expected
reward is the function of the policy π and the vector µ. Deﬁne ci(ψ) as the number of distinct
members we met in community Ci under partial realization ψ. Then 1 − ci(ψ)/di is the probability
that we can meet a new member in the community Ci if we explore community Ci one more time. A
natural approach is to explore community Ci∗ such that i∗ ∈ arg maxi∈[m] 1 − ci(ψ)/di when we
have partial realization ψ. We call such policy as the greedy policy πg. The adaptive community
exploration with greedy policy is described in Algo. 2. One could show that our reward function is
actually an adaptive submodular function  for which the greedy policy is guaranteed to achieve at
least (1−1/e) of the maximized expected reward [13]. However  the following theorem shows that
for our community exploration problem  our greedy policy is in fact optimal.
Theorem 2. Greedy policy is the optimal policy for our adaptive exploration problem.

Proof sketch. Note that the greedy policy chooses the next community only based on the fraction
of unseen members. It does not care which members are already met. Thus  we deﬁne si as the
percentage of members we have not met in a community Ci. We introduce the concept of status 
denoted as s = (s1  . . .   sm). The greedy policy chooses next community based on the current

2We thank Jing Yu from School of Mathematical Sciences at Fudan University for her method to ﬁnd a good

initial allocation  which leads to a faster greedy method.

4

2Ti

Algorithm 3 Combinatorial Lower Conﬁdence Bound (CLCB) algorithm
Input budget for each round K  method (non-adaptive or adaptive)
1: For i ∈ [m]  Ti ← 0 (number of pairs)  Xi ← 0 (collision counting)  ˆµi ← 0 (empirical mean)
2: for t = 1  2  3  . . . do
(cid:46) Line 2-8: online learning
(cid:46) conﬁdence radius
3:
(cid:46) lower conﬁdence bound
4:
µm}  K  method) (cid:46) Si: set of met members
5:
¯
(cid:46) update number of (member) pairs we observe
6:
(cid:46) Si[x]: x-th element in Si
7:
(cid:46) update empirical mean
8:

For i ∈ [m] 
{S1  . . .  Sm} ← CommunityExplore({
For i ∈ [m]  Ti ← Ti + (cid:98)|Si| /2(cid:99)
For i ∈ [m] and |Si| > 1  ˆµi ← Xi/Ti

µi ← max{0  ˆµi − ρi}
¯

1{Si[2x − 1] = Si[2x]}

(ρi = 0 if Ti = 0)

µ1  . . .  
¯

For i ∈ [m]  ρi ←(cid:113) 3 ln t
For i ∈ [m]  Xi ← Xi +(cid:80)(cid:98)|Si|(cid:99)/2
τ =1{φ(i  τ )}(cid:12)(cid:12)(cid:12)(cid:17)
(cid:12)(cid:12)(cid:12)(cid:83)ki

(cid:16)(cid:80)m

x=1

i=1

status. In the proof  we further extend the deﬁnition of reward with a non-decreasing function f as
. Note that the reward function corresponding to the original
R(k  φ) = f
community exploration problem is simply the identity function f (x) = x. Let Fπ(ψ  t) denote the
expected marginal gain when we further explore communities for t steps with policy π starting from
a partial realization ψ. We want to prove that for all ψ  t and π  Fπg (ψ  t) ≥ Fπ(ψ  t)  where πg is
the greedy policy and π is an arbitrary policy. If so  we simply take ψ = ∅  and Fπg (∅  t) ≥ Fπ(∅  t)
for every π and t exactly shows that πg is optimal. We prove the above result by an induction on t.

Let Ci be the community chosen by π based on the partial realization ψ. Deﬁne c(ψ) =(cid:80)

i ci(ψ)
and ∆ψ f = f (c(ψ) + 1) − f (c(ψ)). We ﬁrst claim that Fπg (ψ  1) ≥ Fπ(ψ  1) holds for all ψ and π
with the fact that Fπ(ψ  1) = (1 − µici(ψ))∆ψ f . Note that the greedy policy πg chooses Ci∗ with
i∗ ∈ arg maxi(1 − µici(ψ)). Hence  Fπg (ψ  1) ≥ Fπ(ψ  1).
Next we prove that Fπg (ψ  t+1) ≥ Fπ(ψ  t+1) based on the assumption that Fπg (ψ  t(cid:48)) ≥ Fπ(ψ  t(cid:48))
holds for all ψ  π  and t(cid:48) ≤ t. An important observation is that Fπg (ψ  t) has equal value for any
partial realization ψ associated with the same status s since the status is enough for the greedy
policy to determine the choice of next community. Formally  we deﬁne Fg(s  t) = Fπg (ψ  t) for
any partial realization that satisﬁes s = (1 − c1(ψ)/d1  . . .   1 − cm(ψ)/dm). Let Ci∗ denote the
community chosen by policy πg under realization ψ  i.e.  i∗ ∈ arg maxi∈[m] 1 − µici(ψ). Let I i be
the m-dimensional unit vector with one in the i-th entry and zeros in all other entries. We show that
Fπ(ψ  t + 1) ≤ ci(ψ) · µiFg(s  t) + (di − ci(ψ)) · µiFg(s − µiI i  t) + (1 − µici(ψ))∆ψ f

≤ µi∗ ci∗ (ψ)Fg(s  t) + (1 − µi∗ ci∗ (ψ))Fg(s − µi∗ I i∗   t) + (1 − µi∗ ci∗ (ψ))∆ψ f
= Fg(s  t + 1) = Fπg (ψ  t + 1).

The ﬁrst line is derived directly from the deﬁnition and the assumption. The key is to prove the
correctness of Line 2 in above inequality. It indicates that if we choose a sub-optimal community at
ﬁrst  and then we switch back to the greedy policy  the expected reward would be smaller. The proof
is nontrivial and relies on a careful analysis based on the stochastic transitions among status vectors.
We leave detailed analysis in the supplementary materials. Note that the reward function rπ(µ) is not
necessary adaptive submodular if we extend the reward with the non-decreasing function f. Hence 
a (1 − 1/e) guarantee for adaptive submodular function [13] is not applicable in this scenario. Our
analysis scheme can be applied to any adaptive problems with similar structures.

4 Online Learning for Community Exploration

The key of the learning algorithm is to estimate the community sizes. The size estimation problem
is deﬁned as inferring unknown set size di from random samples obtained via uniformly sampling
with replacement from the set Ci. Various estimators have been proposed [3  8  10  16] for the
estimation of di. The core idea of estimators in [3  16] are based on “collision counting”. Let (u  v)
be an unordered pair of two random elements from Ci and Yu v be a pair collision random variable
that takes value 1 if u = v (i.e.  (u  v) is a collision) and 0 otherwise. It is easy to verify that
E[Yu v] = 1/di = µi. Suppose we independently take Ti pairs of elements from Ci and Xi of them
are collisions. Then E[Xi/Ti] = 1/di = µi. The size di can be estimated by Ti/Xi (the estimator is
valid when Xi > 0).

5

we need at least (1+(cid:112)8di ln 1/δ + 1)/2 uniformly sampled elements in Ci to make sure that Xi > 0

We present our CLCB algorithm in Algorithm 3. In the algorithm  we maintain an unbiased estimation
of µi instead of di for each community Ci for the following reasons. Firstly  Ti/Xi is not an unbiased
estimator of di since E[Ti/Xi] ≥ di according to the Jensen’s inequality. Secondly  the upper
conﬁdence bound of Ti/Xi depends on di  which is unknown in our online learning problem. Thirdly 
with probability at least 1 − δ. We feed the lower conﬁdence bound
µi to the exploration process
¯
since our reward function increases as µi decreases. The idea is similar to CUCB algorithm [7].
The lower conﬁdence bound is small if community Ci is not explored often (Ti is small). Small
µi
¯
motivates us to explore Ci more times. The feedbacks after the exploration process at each round
are the sets of encountered members S1  . . .  Sm in communities C1  . . .   Cm respectively. Note that
for each i ∈ [m]  all pairs of elements in Si  namely {(x  y) | x ≤ y  x ∈ Si  y ∈ Si\{x}} are not
mutually independent. Thus  we only use (cid:98)|Si| /2(cid:99) independent pairs. Therefore  Ti is updated as
Ti + (cid:98)|Si| /2(cid:99) at each round. In each round  the community exploration could either be non-adaptive
or adaptive  and the following regret analysis separately discuss these two cases.

4.1 Regret Analysis for the Non-adaptive Version

The non-adaptive bandit learning model ﬁts into the general combinatorial multi-armed bandit
(CMAB) framework of [7  20] that deals with nonlinear reward functions. In particular  we can treat
the pair collision variable in each community Ci as a base arm  and our expected reward in Eq. (5) is
non-linear  and it satisﬁes the monotonicity and bounded smoothness properties (See Properties 1
and 2). However  directly applying the regret result from [7  20] will give us an inferior regret bound
for two reasons. First  in our setting  in each round we could have multiple sample feedback for
each community  meaning that each base arm could be observed multiple times  which is not directly
covered by CMAB. Second  to use the regret result in [7  20]  the bounded smoothness property
needs to have a bounded smoothness constant independent of the actions  but we can have a better
result by using a tighter form of bounded smoothness with action-related coefﬁcients. Therefore  in
this section  we provide a better regret result by adapting the regret analysis in [20].

We deﬁne the gap ∆k = rk∗ (µ) − rk(µ) for all action k satisfying(cid:80)m

min = min∆k>0 ki>1 ∆k and ∆i

community Ci  we deﬁne ∆i
convention  if there is no action k with ki > 1 such that ∆k > 0  we deﬁne ∆i
max = 0. Furthermore  deﬁne ∆min = mini∈[m] ∆i
∆i
K(cid:48) = K − m + 1. We have the regret for Algo. 3 as follows.
Theorem 3. Algo. 3 with non-adaptive exploration method has regret as follows.

min and ∆max = maxi∈[m] ∆i

i=1 ki = K. For each
max = max∆k>0 ki>1 ∆k. As a
min = ∞ and
max. Let

(cid:32) m(cid:88)

i=1

K(cid:48)3 log T

∆i

min

(cid:33)

.

(6)

Regµ(T ) ≤ m(cid:88)

i=1

48(cid:0)K(cid:48)

(cid:1)K ln T

2
∆i

min

(cid:18)K(cid:48)

(cid:19)

2

(cid:106) K(cid:48)

(cid:107)

2

3

π2

+ 2

m +

m∆max = O

The proof of the above theorem is an adaption of the proof of Theorem 4 in [20]  and the full proof
details as well as the detailed comparison with the original CMAB framework result are included in
the supplementary materials. We brieﬂy explain our adaption that leads to the regret improvement.
We rely on the following monotonicity and 1-norm bounded smoothness properties of our expected
reward function rk(µ)  similar to the ones in [7  20].
Property 1 (Monotonicity). The reward function rk(µ) is monotonically decreasing  i.e.  for any two
i ∀i ∈ [m].
vectors µ = (µ1  . . .   µm) and µ(cid:48) = (µ(cid:48)
Property 2 (1-Norm Bounded Smoothness). The reward function rk(µ) satisﬁes the 1-norm bounded
smoothness property  i.e.  for any two vectors µ = (µ1 ···   µm) and µ(cid:48) = (µ(cid:48)
m)  we have

m)  we have rk(µ) ≥ rk(µ(cid:48)) if µi ≤ µ(cid:48)
(cid:1)(cid:80)m
1 ···   µ(cid:48)
i=1 |µi − µ(cid:48)
i|.

|rk(µ) − rk(µ(cid:48))| ≤(cid:80)m

(cid:1)|µi − µ(cid:48)

i| ≤(cid:0)K(cid:48)

1  . . .   µ(cid:48)

(cid:0)ki

i=1

2

2

We remark that if we directly apply the CMAB regret bound of Theorem 4 in [20]  we need to revise
the update procedure in Lines 6-8 of Algo. 3 so that each round we only update one observation for
each community Ci if |Si| > 1. Then we would obtain a regret bound O
  which
means that our regret bound in Eq. (6) has an improvement of O(K(cid:48)m). This improvement is exactly
due to the reason we give earlier  as we now explain with more details.
For all the random variables introduced in Algo. 3  we add the subscript t to denote their value at the
end of round t. For example  Ti t is the value of Ti at the end of round t. First  the improvement of

K(cid:48)4m log T

∆i

min

i

(cid:16)(cid:80)

(cid:17)

6

2

2

i=1

i=1

i=1

i=1

2

2

i=1

2

2

i=1

i=1

2

(cid:80)

c(ki t−1)

(cid:0)ki

2

t=1 ∆kt = c(cid:80)m

is the form of a right Riemann summation  which achieves the maximum value when ki t = K(cid:48).
Here Li(T ) is a ln T function with some constants related with community Ci. Hence the regret
use the original CMAB framework  we need to set Ti t = Ti t−1 + 1{ki t > 1}. In this case 

t≥1 Ti t≤Li(T )(cid:98) ki t

the factor m comes from the use of a tighter bounded smoothness in Property 2  namely  we use the
i|. The CMAB framework in [20] requires the
bounded smoothness constant to be independent of actions. So to apply Theorem 4 in [20]  we have
i| 
improvement of the O(K(cid:48)) factor  more precisely a factor of (K(cid:48) − 1)/2  is achieved by utilizing
multiple feedback in a single round and a more careful analysis of the regret utilizing the property
of the right Riemann summation. Speciﬁcally  let ∆kt = rk∗ (µ) − rkt(µ) be the reward gap.

i| instead of(cid:0)K(cid:48)
(cid:1)|µi−µ(cid:48)
(cid:0)ki
(cid:1)(cid:80)m
bound(cid:80)m
i=1 |µi−µ(cid:48)
(cid:1)(cid:80)m
to use the bound(cid:0)K(cid:48)
(cid:1)|µi − µ(cid:48)
i|. However  in our case  when using bound(cid:80)m
(cid:1) ≤(cid:0)K(cid:48)
(cid:0)ki
(cid:1) to improve the result by a factor of m. Second  the
we are able to utilize the fact(cid:80)m
i=1 |µi − µ(cid:48)
/(cid:112)Ti t−1 ≤
When the estimate is within the conﬁdence radius  we have ∆kt ≤ (cid:80)m
i=1(cid:98)ki t/2(cid:99)/(cid:112)Ti t−1  where c is a constant. In Algo. 3  we have Ti t = Ti t−1 + (cid:98)ki t/2(cid:99)
c(cid:80)m
t≥1 Ti t≤Li(T )(cid:98)ki t/2(cid:99)/(cid:112)Ti t−1
because we allow multiple feedback in a single round. Then(cid:80)
(cid:112)Li(T ). However  if we
2 (cid:99)/(cid:112)Ti t−1 ≤ 2c(cid:80)m
t=1 ∆kt ≤ c(cid:80)m
bound(cid:80)T
t≥1 Ti t≤Li(T )(ki t − 1)/2(cid:112)Ti t−1 ≤
(cid:80)
we can only bound the regret as (cid:80)T
(cid:112)Li(T )  leading to an extra factor of (K(cid:48) − 1)/2.
(cid:80)m

2c K(cid:48)−1
Justiﬁcation for Algo. 3. In Algo. 3  we only use the members in current round to update the estima-
tor. This is practical for the situation where the member identiﬁers are changing in different rounds
for privacy protection. Privacy gains much attention these days. Consider the online advertising
scenario we explain in the introduction. Whenever a user clicks an advertisement  the advertiser
would store the user information (e.g. Facebook ID  IP address etc.) to identify the user and correlated
with past visits of the user. If such user identiﬁers are ﬁxed and do not change  the advertiser could
easily track user behavior  which may result in privacy leak. A reasonable protection for users is to
periodically change user IDs (e.g. Facebook can periodically change user hash IDs  or users adopt
dynamic IP addresses  etc.)  so that it is difﬁcult for the advertiser to track the same user over a long
period of time. Under such situation  it may be likely that our learning algorithm can still detect ID
collisions within the short period of each learning round  but cross different rounds  collisions may
not be detectable due to ID changes.
Full information feedback. Now we consider the scenario where the member identiﬁers are ﬁxed
over all rounds  and design an algorithm with a constant regret bound. Our idea is to ensure that
we can observe at least one pair of members in every community Ci in each round t. We call such
guarantee as full information feedback. If we only use members revealed in current round  we cannot
achieve this goal since we have no observation of new pairs for a community Ci when ki = 1. To
achieve full information feedback  we use at least one sample from the previous round to form a
pair with a sample in the current round to generate a valid pair collision observation. In particular 
we revise the Line 3  6  and 7 as follows. Here we use u0 to represent the last member in Si in the
previous round (let u0 = null when t = 1) and ux(x > 0) to represent the x-th members in Si in the
current round. The revision of Line 3 implies that we use the empirical mean ˆµi = Xi/Ti instead of
the lower conﬁdence bound in the function CommunityExplore.

i=1

Line 3: For i ∈ [m]  ρi = 0; Line 6: For i ∈ [m]  Ti ← Ti + |Si| − 1{t = 1} 
Line 7: For i ∈ [m]  Xi ← Xi +

1{ux = ux+1}.

(cid:88)|Si|−1

x=0

(7)

Theorem 4. With the full information feedback revision in Eq. (7)  Algo. 3 with non-adaptive
exploration method has a constant regret bound. Speciﬁcally 

Regµ(T ) ≤(cid:0)2 + 2me2K(cid:48)2(K(cid:48) − 1)2/∆2

min

(cid:1) ∆max.

Note that we cannot apply the Hoeffding bound in [14] directly since the random variables 1{ux =
ux+1} we obtain during the online learning process are not mutually independent. Instead  we apply
a concentration bound in [9] that is applicable to variables that have local dependence relationship.

7

4.2 Regret Analysis for the Adaptive Version

min and ∆(k)

µt into the adaptive community
For the adaptive version  we feed the lower conﬁdence bound
exploration procedure  namely CommunityExplore({
µm}  K  adaptive) in round t. We
¯
µ1  . . .  
¯
¯
denote the policy implemented by this procedure as πt. Note that both πg and πt are based on
the greedy procedure CommunityExplore(·  K  adaptive). The difference is that πg uses the true
µt. More speciﬁcally  given a partial realization
parameter µ while πt uses the lower bound parameter
ψ  the community chosen by πt is Ci∗ where i∗ ∈ arg maxi∈[m] 1 − ci(ψ)
¯
µi t. Recall that ci(ψ) is
¯
the number of distinct encountered members in community Ci under partial realization ψ.
We ﬁrst properly deﬁne the metrics ∆i k
max used in the regret bound as follows. Consider
a speciﬁc full realization φ where {φ(i  1)  . . .   φ(i  di)} are di distinct members in Ci for i ∈
[m]. The realization φ indicates that we will obtain a new member in the ﬁrst di exploration of
community Ci. Let Ui k denote the number of times community Ci is selected by policy πg in
the ﬁrst k − 1(k > m) steps under the special full realization φ we deﬁne previously. We deﬁne
min = (µiUi k − minj∈[m] µjUj k)/Ui k. Conceptually  the value µiUi k − minj∈[m] µjUj k is gap
∆i k
in the expected reward of the next step between selecting a community by πg (the optimal policy)
and selecting community Ci  when we already meet Uj k distinct members in Cj for j ∈ [m]. When
min = ∞. Let π be another policy that chooses the same
µiUi k = minj∈[m] µjUj k  we deﬁne ∆i k
sequence of communities as πg when the number of met members in Ci is no more than Ui k for all
i ∈ [m]. Note that policy π chooses the same communities as πg in the ﬁrst k − 1 steps under the
special full realization φ. Actually  the policy π is the same as πg for at least k − 1 steps. We use Πk
to denote the set of all such policies. We deﬁne ∆(k)
max as the maximum reward gap between the policy
π ∈ Πk and the optimal policy πg  i.e.  ∆(k)
Theorem 5. Algo. 3 with adaptive exploration method has regret as follows.

max = maxπ∈Πk rπg (µ) − rπ(µ). Let D =(cid:80)m

i=1 di.

min{K D}(cid:88)

i=1

k=m+1

∆(k)

max.

(8)

i=1

k=m+1

(cid:98) K(cid:48)
2 (cid:99)π2
3

6∆(k)
max
(∆i k
min)2

 m(cid:88)
min{K D}(cid:88)
Regµ(T ) ≤(cid:88)m

 ln T +
m(cid:88)
(cid:88)min{K D}
(cid:0)2/ε4
i k + 1(cid:1) ∆(k)
k ∈ arg mini∈[m] µiUi k)
Ui∗

k k)/(Ui k + Ui∗

i=1

k=m+1

max.

k k) for i (cid:54)= i∗

k and εi k = ∞ for i = i∗
k.

Theorem 6. With the full information feedback revision in Eq. (7)  Algo. 3 with adaptive exploration
method has a constant regret bound. Speciﬁcally 

Regµ(T ) ≤

where εi k is deﬁned as (here i∗
εi k (cid:44) (µiUi k − µi∗

k

max. Their version of ∆i k

Gabillon et al. [11] analyzes a general adaptive submodular function maximization in bandit setting.
We have a regret bound in similar form as (8) if we directly apply Theorem 1 in [11]. However  their
version of ∆(k)
max is an upper bound on the expected reward of policy πg from k steps forward  which
min is the minimum (µici(ψ)− minj∈[m] µjcj(ψ))/ci(ψ)
is larger than our ∆(k)
for all partial realization ψ obtained after policy πg is executed for k steps  which is smaller than
our ∆i k
min. Our regret analysis is based on counting how many times πg and πt choose different
communities under the special full realization φ  while the analysis in [11] is based on counting how
many times πg and πt choose different communities under all possible full realizations.
Discussion. In this paper  we consider the online learning problem that consists of T rounds  and
during each round  we explore the communities with a budget K. Our goal is to maximize the
cumulative reward in T rounds. Another important and natural setting is described as follows. We
start to explore communities with unknown sizes  and update the parameters every time we explore
the community for one step (or for a few steps). Different from the setting deﬁned in this paper  here
a member will not contribute to the reward if it has been met in previous rounds. To differentiate
the two settings  let’s call the latter one the “interactive community exploration”  while the former
one the “repeated community exploration”. Both the repeated community exploration deﬁned in this
paper and the interactive community exploration we will study as the future work have corresponding
applications. The former is suitable for online advertising where in each round the advertiser promotes
different products. Hence the rewards in different rounds are additive. The latter corresponds to
the adaptive online advertising for the same product  and thus the rewards in different rounds are
dependent.

8

5 Related Work
Golovin and Krause [13] show that a greedy policy could achieve at least (1 − 1/e) approximation
for the adaptive submodular function. The result could be applied to our ofﬂine adaptive problem  but
by an independent analysis we show the better result that the greedy policy is optimal. Multi-armed
bandit (MAB) problem is initiated by Robbins [18] and extensively studied in [2  4  19]. Our online
learning algorithm is based on the extensively studied Upper Conﬁdence Bound approach [1]. The
non-adaptive community exploration problem in the online setting ﬁts into the general combinatorial
multi-armed bandit (CMAB) framework [6  7  12  17  20]  where the reward is a set function of base
arms. The CMAB problem is ﬁrst studied in [12]  and its regret bound is improved by [7  17]. We
leverage the analysis framework in [7  20] and prove a tighter bound for our algorithm. Gabillon et al.
[11] deﬁne an adaptive submodular maximization problem in bandit setting. Our online adaptive
exploration problem is a instance of the problem deﬁned in [11]. We prove a tighter bound than the
one in [11] by using the properties of our problem.
Our model bears similarities to the optimal discovery problem proposed in [5] such as we both have
disjoint assumption  and both try to maximize the number of target elements. However  there are also
some differences: (a) We use different estimators for our critical parameters  because our problem
setting is different. (b) Their online model is closer to the interactive community exploration we
explained in 4.2   while our online model is on repeated community exploration. As explained in 4.2 
the two online models serve different applications and have different algorithms and analyses. (c) We
also have more comprehensive studies on the ofﬂine cases.

6 Future Work

In this paper  we systematically study the community exploration problems. In the ofﬂine setting 
we propose the greedy methods for both of non-adaptive and adaptive exploration problems. The
optimality of the greedy methods are rigorously proved. We also analyze the online setting where the
community sizes are unknown initially. We provide a CLCB algorithm for the online community
exploration. The algorithm has O(log T ) regret bound. If we further allow the full information
feedback  the CLCB algorithm with some minor revisions has a constant regret.
Our study opens up a number of possible future directions. For example  we can consider various
extensions to the problem model  such as more complicated distributions of member meeting prob-
abilities  overlapping communities  or even graph structures between communities. We could also
study the gap between non-adaptive and adaptive solutions.

Acknowledgments

We thank Jing Yu from School of Mathematical Sciences at Fudan University for her insightful
discussion on the ofﬂine problems  especially  we thank Jing Yu for her method to ﬁnd a good initial
allocation  which leads to a faster greedy method. Wei Chen is partially supported by the National
Natural Science Foundation of China (Grant No. 61433014). The work of John C.S. Lui is supported
in part by the GRF Grant 14208816.

References
[1] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine learning  47(2-3):235–256  2002.

[2] Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments.

Chapman and Hall  5:71–87  1985.

[3] Marco Bressan  Enoch Peserico  and Luca Pretto. Simple set cardinality estimation through

random sampling. arXiv preprint arXiv:1512.07901  2015.

[4] Sébastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122 
2012.

9

[5] Sébastien Bubeck  Damien Ernst  and Aurélien Garivier. Optimal discovery with probabilistic
expert advice: ﬁnite time analysis and macroscopic optimality. JMLR  14(Feb):601–623  2013.

[6] Wei Chen  Wei Hu  Fu Li  Jian Li  Yu Liu  and Pinyan Lu. Combinatorial multi-armed bandit

with general reward functions. In NIPS  pages 1659–1667  2016.

[7] Wei Chen  Yajun Wang  Yang Yuan  and Qinshi Wang. Combinatorial multi-armed bandit and
its extension to probabilistically triggered arms. Journal of Machine Learning Research  17
(50):1–33  2016. A preliminary version appeared as Chen  Wang  and Yuan  “Combinatorial
multi-armed bandit: General framework  results and applications”  ICML’2013.

[8] Mary C Christman and Tapan K Nayak. Sequential unbiased estimation of the number of classes

in a population. Statistica Sinica  pages 335–352  1994.

[9] Devdatt Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis of

Randomized Algorithms. Cambridge University Press  1st edition  2009.

[10] Mark Finkelstein  Howard G. Tucker  and Jerry Alan Veeh. Conﬁdence intervals for the number

of unseen types. Statistics & Probability Letters  pages 423 – 430  1998.

[11] Victor Gabillon  Branislav Kveton  Zheng Wen  Brian Eriksson  and S Muthukrishnan. Adaptive

submodular maximization in bandit setting. In NIPS  pages 2697–2705  2013.

[12] Yi Gai  Bhaskar Krishnamachari  and Rahul Jain. Combinatorial network optimization with
unknown variables: Multi-armed bandits with linear rewards and individual observations.
IEEE/ACM Trans. Netw.  20(5):1466–1478  2012.

[13] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in
active learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research  42:
427–486  2011.

[14] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of

the American statistical association  58(301):13–30  1963.

[15] Svante Janson. Large deviations for sums of partly dependent random variables. Random

Structures & Algorithms  24(3):234–248  2004.

[16] Liran Katzir  Edo Liberty  and Oren Somekh. Estimating sizes of social networks via biased

sampling. In WWW  2011.

[17] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvari. Tight regret bounds for
stochastic combinatorial semi-bandits. In Artiﬁcial Intelligence and Statistics  pages 535–543 
2015.

[18] Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins

Selected Papers  pages 169–177. Springer  1985.

[19] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction  volume 1.

MIT press Cambridge  1998.

[20] Qinshi Wang and Wei Chen. Improving regret bounds for combinatorial semi-bandits with

probabilistically triggered arms and its applications. In NIPS  pages 1161–1171  2017.

10

,Xiaowei Chen
Weiran Huang
Wei Chen
John C. S. Lui