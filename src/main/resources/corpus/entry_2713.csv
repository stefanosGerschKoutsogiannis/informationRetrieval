2012,Inverse Reinforcement Learning through Structured Classification,This paper adresses the inverse reinforcement learning (IRL) problem  that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm  SCIRL  whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multi-class classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms  SCIRL does not require solving the direct RL problem. Moreover  with an appropriate heuristic  it can succeed with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator.,Inverse Reinforcement Learning
through Structured Classiﬁcation

Edouard Klein1 2
1LORIA – team ABC

Nancy  France

edouard.klein@supelec.fr

Matthieu Geist2

Metz  France

2Supélec – IMS-MaLIS Research Group

matthieu.geist@supelec.fr

Bilal Piot2 3  Olivier Pietquin2 3
3 UMI 2958 (GeorgiaTech-CNRS)

{bilal.piot olivier.pietquin}@supelec.fr

Metz  France

Abstract

This paper adresses the inverse reinforcement learning (IRL) problem  that is in-
ferring a reward for which a demonstrated expert behavior is optimal. We in-
troduce a new algorithm  SCIRL  whose principle is to use the so-called feature
expectation of the expert as the parameterization of the score function of a multi-
class classiﬁer. This approach produces a reward function for which the expert
policy is provably near-optimal. Contrary to most of existing IRL algorithms 
SCIRL does not require solving the direct RL problem. Moreover  with an ap-
propriate heuristic  it can succeed with only trajectories sampled according to the
expert behavior. This is illustrated on a car driving simulator.

1 Introduction

Inverse reinforcement learning (IRL) [14] consists in ﬁnding a reward function such that a demon-
strated expert behavior is optimal. Many IRL algorithms (to be brieﬂy reviewed in Sec. 5) search
for a reward function such that the associated optimal policy induces a distribution over trajectories
(or some measure of this distribution) which matches the one induced by the expert. Often  this
distribution is characterized by the so-called feature expectation (see Sec. 2.1): given a reward func-
tion linearly parameterized by some feature vector  it is the expected discounted cumulative feature
vector for starting in a given state  applying a given action and following the related policy.
In this paper  we take a different route. The expert behavior could be mimicked by a supervised
learning algorithm generalizing the mapping from states to actions. Here  we consider generally
multi-class classiﬁers which compute from a training set the parameters of a linearly parameterized
score function; the decision rule for a given state is the argument (the action) which maximizes the
score function for this state (see Sec. 2.2). The basic idea of our SCIRL (Structured Classiﬁcation-
based IRL) algorithm is simply to take an estimate of the expert feature expectation as the param-
eterization of the score function (see Sec. 3.1). The computed parameter vector actually deﬁnes a
reward function for which we show the expert policy to be near-optimal (Sec. 3.2).
Contrary to most existing IRL algorithms  a clear advantage of SCIRL is that it does not require
solving repeatedly the direct reinforcement learning (RL) problem. It requires estimating the expert
feature expectation  but this is roughly a policy evaluation problem (for an observed policy  so is less
involved than repeated policy optimization problems)  see Sec. 4. Moreover  up to the use of some
heuristic  SCIRL may be trained solely from transitions sampled from the expert policy (no need to
sample the whole dynamic). We illustrate this on a car driving simulator in Sec. 6.

1

2 Background and Notations

R = T ∗

Rv∗

(Inverse) Reinforcement Learning

R (according to the reward function R) is a policy of associated value function v∗

this state and following the policy π afterwards: vπR(s) = E[P

2.1
A Markov Decision process (MDP) [12] is a tuple {S A P R  γ} where S is the ﬁnite state space1 
A the ﬁnite actions space  P = {Pa = (p(s0|s  a))1≤s s0≤|S|  a ∈ A} the set of Markovian transition
probabilities  R ∈ RS the state-dependent reward function and γ the discount factor. A deterministic
policy π ∈ SA deﬁnes the behavior of an agent. The quality of this control is quantiﬁed by the
value function vπR ∈ RS  associating to each state the cumulative discounted reward for starting in
t≥0 γtR(St)|S0 = s  π]. An optimal
policy π∗
R satisfying
R ≥ vπR  for any policy π and componentwise.
v∗
Let Pπ be the stochastic matrix Pπ = (p(s0|s  π(s)))1≤s s0≤|S|. With a slight abuse of notation 
we may write a the policy which associates the action a to each state s. The Bellman evaluation
R) : RS → RS are deﬁned as T πRv = R + γPπv and
(resp. optimality) operators T πR (resp. T ∗
T ∗
Rv = maxπ T πRv. These operators are contractions and vπR and v∗
R are their respective ﬁxed-
R. The action-value function Qπ ∈ RS×A adds a degree of
points: vπR = T πRvπR and v∗
freedom on the choice of the ﬁrst action  it is formally deﬁned as QπR(s  a) = [T aRvπR](s). We also
write ρπ the stationary distribution of the policy π (satisfying ρ>
Reinforcement learning and approximate dynamic programming aim at estimating the optimal con-
trol policy π∗
R when the model (transition probabilities and the reward function) is unknown (but
observed through interactions with the system to be controlled) and when the state space is too large
to allow exact representations of the objects of interest (as value functions or policies) [2  15  17].
We refer to this as the direct problem. On the contrary  (approximate) inverse reinforcement learn-
ing [11] aim at estimating a reward function for which an observed policy is (nearly) optimal. Let
us call this policy the expert policy  denoted πE. We may assume that it optimizes some unknown
reward function RE. The aim of IRL is to compute some reward ˆR such that the expert policy is
(close to be) optimal  that is such that v∗
Similarly to the direct problem  the state space may be too large for the reward function to admit a
practical exact representation. Therefore  we restrict our search of a good reward among linearly pa-
rameterized functions. Let φ(s) = (φ1(s) . . . φp(s))> be a feature vector composed of p basis func-
i=1 θiφi(s).
Searching a good reward thus reduces to searching a good parameter vector θ ∈ Rp. Notice that we
will use interchangeably Rθ and θ as subscripts (e.g.  vπ
). Parameterizing the reward this
way implies a related parameterization for the action-value function:

tion φi ∈ RS  we deﬁne the parameterized reward functions as Rθ(s) = θ>φ(s) =Pp

ˆR ≈ vπE

ˆR . We refer to this as the inverse problem.

π Pπ = ρ>
π ).

θ (s  a) = θ>µπ(s  a) with µπ(s  a) = E[X

Qπ

θ for vπRθ

t≥0

γtφ(St)|S0 = s  A0 = a  π].

(1)

Therefore  the action-value function shares the parameter vector of the reward function  with an as-
sociated feature vector µπ called the feature expectation. This notion will be of primary importance
i of this feature vector is actually the
for the contribution of this paper. Notice that each component µπ
(s  a). Therefore 
action-value function of the policy π assuming the reward is φi: µπ
any algorithm designed for estimating an action-value function may be used to estimate the feature
expectation  such as Monte-Carlo rollouts or temporal difference learning [7].

i (s  a) = Qπ
φi

2.2 Classiﬁcation with Linearly Parameterized Score Functions
Let X be a compact or a ﬁnite set (of inputs to be classiﬁed) and let Y be a ﬁnite set (of labels).
Assume that inputs x ∈ X are drawn according to some unknown distribution P(x) and that there
exists some oracle which associates to each of these inputs a label y ∈ Y drawn according to the
unknown conditional distribution P(y|x). Generally speaking  the goal of multi-class classiﬁcation
is  given a training set {(xi  yi)1≤i≤N} drawn according to P(x  y)  to produce a decision rule
g ∈ YX which aims at minimizing the classiﬁcation error E[χ{g(x)6=y}] = P(g(x) 6= y)  where χ
denotes the indicator function.

1This work can be extended to compact state spaces  up to some technical aspects.

2

Here  we consider a more restrictive set of classiﬁcation algorithms. We assume that the decision
rule associates to an input the argument which maximizes a related score function  this score func-
tion being linearly parameterized and the associated parameters being learnt by the algorithm. More
formally  let ψ(s  a) = (ψ1(x  y) . . . ψd(x  y))> ∈ Rd be a feature vector whose components are d
basis functions ψi ∈ RX×Y. The linearly parameterized score function sw ∈ RX×Y of parameter
vector w ∈ Rd is deﬁned as sw(x  y) = w>ψ(x  y). The associated decision rule gw ∈ YX is de-
ﬁned as gw(x) ∈ argmaxy∈Y sw(x  y). Using a training set {(xi  yi)1≤i≤N}  a linearly parameter-
ized score function-based multi-class classiﬁcation (MC2 for short) algorithm computes a parameter
vector θc. The quality of the solution is quantiﬁed by the classiﬁcation error c = P(gθc(x) 6= y).
We do not consider a speciﬁc MC2 algorithm  as long as it classiﬁes inputs by maximizing the
argument of a linearly parameterized score function. For example  one may choose a multi-class
support vector machine [6] (taking the kernel induced by the feature vector) or a structured large
margin approach [18]. Other choices may be possible  one can choose its preferred algorithm.

3 Structured Classiﬁcation for Inverse Reinforcement Learning

3.1 General Algorithm

Consider the classiﬁcation framework of Sec. 2.2. The input x may be seen as a state and the label y
as an action. Then  the decision rule gw(x) can be interpreted as a policy which is greedy according
to the score function w>ψ(x  y)  which may itself be seen as an action-value function. Making the
parallel with Eq. (1)  if ψ(x  y) is the feature expectation of some policy π which produces labels
of the training set  and if the classiﬁcation error is small  then w will be the parameter vector of a
reward function for which we may hope the policy π to be near optimal. Based on these remarks 
we’re ready to present the proposed Structured Classiﬁcation-based IRL (SCIRL) algorithm.
Let πE be the expert policy from which we would like to recover a reward function. Assume that
we have a training set D = {(si  ai = πE(si))1≤i≤N} where states are sampled according to the
expert stationary distribution2 ρE = ρπE . Assume also that we have an estimate ˆµπE of the expert
feature expectation µπE deﬁned in Eq. (1). How to practically estimate this quantity is postponed to
Sec. 4.1; however  recall that estimating µπE is simply a policy evaluation problem (estimating the
action-value function of a given policy)  as noted in Sec. 2.1. Assume also that an MC2 algorithm
has been chosen. The proposed algorithm simply consists in choosing θ> ˆµπE (s  a) as the linearly
parameterized score function  training the classiﬁer on D which produces a parameter vector θc  and
outputting the reward function Rθc(s) = θ>

c φ(s).

Algorithm 1: SCIRL algorithm
Given a training set D = {(si  ai = πE(si))1≤i≤N}  an estimate ˆµπE of the expert feature
expectation µπE and an MC2 algorithm;
Compute the parameter vector θc using the MC2 algorithm fed with the training set D and
considering the parameterized score function θ> ˆµπE (s  a);
Output the reward function Rθc(s) = θ>

c φ(s) ;

The proposed approach is summarized in Alg. 1. We call this Structured Classiﬁcation-based IRL
because using the (estimated) expert feature expectation as the feature vector for the classiﬁer some-
how implies taking into account the MDP structure into the classiﬁcation problem and allows out-
putting a reward vector. Notice that contrary to most of existing IRL algorithms  SCIRL does not
require solving the direct problem. If it possibly requires estimating the expert feature expectation 
it is just a policy evaluation problem  less difﬁcult than the policy optimization issue involved by the
direct problem. This is further discussed in Sec. 5.

2For example  if the Markov chain induced by the expert policy is fast-mixing  sampling a trajectory will

quickly lead to sample states according to this distribution.

3

3.2 Analysis

In this section  we show that the expert policy πE is close to be optimal according to the reward
function Rθc  more precisely that Es∼ρE [v∗
(s)] is small. Before stating our main result 
we need to introduce some notations and to deﬁne some objects.
We will use the ﬁrst order discounted future state distribution concentration coefﬁcient Cf [9]:

(s) − vπE

θc

θc

Cf = (1 − γ)X

t≥0

γtc(t) with c(t) = max

π1 ... πt s∈S

(ρ>

EPπ1 . . . Pπt)(s)

ρE(s)

.

We note πc the decision rule of the classiﬁer: πc(s) ∈ argmaxa∈A θ>
c ˆµπE (s  a). The classiﬁca-
tion error is therefore c = Es∼ρE [χ{πc(s)6=πE (s)}] ∈ [0  1]. We write ˆQπE
c ˆµπE the score
function computed from the training set D (which can be interpreted as an approximate action-value
function). Let also µ = ˆµπE − µπE : S × A → Rp be the feature expectation error. Conse-
c (ˆµπE − µπE ) =
quently  we deﬁne the action-value function error as Q = ˆQπE
c µ : S × A → R. We ﬁnally deﬁne the mean delta-max action-value function error as
θ>
¯Q = Es∼ρE [maxa∈A Q(s  a) − mina∈A Q(s  a)] ≥ 0.
Theorem 1. Let Rθc be the reward function outputted by Alg. 1. Let also the quantities Cf   c and
(cid:18)
¯Q be deﬁned as above. We have
0 ≤ Es∼ρE [v∗
Rθc

2γkRθck∞

− QπE

− vπERθc

¯Q + c

= θ>

= θ>

(cid:19)

θc

θc

θc

.

] ≤ Cf
1 − γ

1 − γ

Proof. As the proof only relies on the reward Rθc  we omit the related subscripts to keep the nota-
or R for Rθc). First  we link the error Es∼ρE [v∗(s) − vπE (s)]
tions simple (e.g.  vπ for vπ
θc
to the Bellman residual Es∼ρE [[T ∗vπE ](s) − vπE (s)]. Componentwise  we have that:

= vπRθc

v∗ − vπE = T ∗v∗ − T π∗

vπE + T π∗

vπE − T ∗vπE + T ∗vπE − vπE

(a)≤ γPπ∗(v∗ − vπE ) + T ∗vπE − vπE

(b)≤ (I − γPπ∗)−1(T ∗vπE − vπE ).

ator  we have T ∗vπE ≥ T πE vπE = vπE . Additionally  remark that (I − γPπ∗)−1 =P

vπE ≤ T ∗vπE and inequality (b) holds thanks to [9  Lemma 4.2].
Inequality (a) holds because T π∗
Moreover  v∗ being optimal we have that v∗ − vπE ≥ 0 and T ∗ being the Bellman optimality oper-
π∗.
t≥0 γtP t

Therefore  using the deﬁnition of the concentration coefﬁcient Cf   we have that:

0 ≤ Es∼ρE [v∗(s) − vπE (s)] ≤ Cf
1 − γ

Es∼ρE [[T ∗vπE ](s) − vπE (s)] .

(2)

This results actually follows closely the one of [9  Theorem 4.2]. There remains to bound the
Bellman residual Es∼ρE [[T ∗vπE ](s) − vπE (s)]. Considering the following decomposition 

T ∗vπE − vπE = T ∗vπE − T πc vπE + T πc vπE − vπE  

we will bound Es∼ρE [[T ∗vπE ](s) − [T πc vπE ](s)] and Es∼ρE [[T πc vπE ](s) − vπE (s)].
The policy πc (the decision rule of the classiﬁer) is greedy with respect to ˆQπE = θ>
for any state-action couple (s  a) ∈ S × A we have:

c ˆµπE . Therefore 

ˆQπE (s  πc(s)) ≥ ˆQπE (s  a) ⇔ QπE (s  a) ≤ QπE (s  πc(s)) + Q(s  πc(s)) − Q(s  a).

By deﬁnition  QπE (s  a) = [T avπE ](s) and QπE (s  πc(s)) = [T πc vπE ](s). Therefore  for s ∈ S:

∀a ∈ A  [T avπE ](s) ≤ [T πc vπE ](s) + Q(s  πc(s)) − Q(s  a)

⇒ [T ∗vπE ](s) ≤ [T πc vπE ](s) + max

a∈A Q(s  a) − min

a∈A Q(s  a).

Taking the expectation according to ρE and noticing that T ∗vπE ≥ vπE   we bound the ﬁrst term:

0 ≤ Es∼ρE [[T ∗vπE ](s) − [T πc vπE ](s)] ≤ ¯Q.
There ﬁnally remains to bound the term Es∼ρE [[T πc vπE ](s) − vπE (s)].

(3)

4

Let us write M ∈ R|S|×|S| the diagonal matrix deﬁned as M = diag(χ{πc(s)6=πE (s)}). Using this 
the Bellman operator T πc may be written as  for any v ∈ RS:

T πc v = R + γM Pπc v + γ(I − M)PπE v = R + γPπE v + γM(Pπc − PπE )v.
Applying this operator to vπE and recalling that R + γPπE vπE = T πE vπE = vπE   we get:
T πc vπE − vπE = γM(Pπc − PπE )vπE ⇒ |ρ>
One can easily see that k(Pπc − PπE )vπEk∞ ≤ 2

EM(Pπc − PπE )vπE|.
E(T πc vπE − vπE )| = γ|ρ>
1−γkRk∞  which allows bounding the last term:

|Es∼ρE [[T πc vπE ](s) − vπE (s)]| ≤ c

2γ
1 − γ

kRk∞.

(4)

Injecting bounds of Eqs. (3) and (4) into Eq. (2) gives the stated result.

This result shows that if the expert feature expectation is well estimated (in the sense that the estima-
tion error µ is small for states sampled according to the expert stationary policy and for all actions)
and if the classiﬁcation error c is small  then the proposed generic algorithm outputs a reward func-
tion Rθc for which the expert policy will be near optimal. A direct corollary of Th. 1 is that given
the true expert feature expectation µπE and a perfect classiﬁer (c = 0)  πE is the unique optimal
policy for Rθc.
One may argue that this bounds trivially holds for the null reward function (a reward often exhibited
to show that IRL is an ill-posed problem)  obtained if θc = 0. However  recall that the parameter
vector θc is computed by the classiﬁer. With θc = 0  the decision rule would be a random policy
and we would have c = |A|−1
  the worst possible classiﬁcation error. This case is really unlikely.
|A|
Therefore  we advocate that the proposed approach somehow allows disambiguating the IRL prob-
lem (at least  it does not output trivial reward functions such as the null vector). Also  this bound is
scale-invariant: one could impose kθck = 1 or normalize (action-) value functions by kRθck−1∞ .
One should notice that there is a hidden dependency of the classiﬁcation error c to the estimated
expert feature expectation ˆµπE . Indeed  the minimum classiﬁcation error depends on the hypothesis
space spanned by the chosen score function basis functions for the MC2 algorithm (here ˆµπE ).
Nevertheless  provided a good representation for the reward function (that is a good choice of basis
functions φi) and a small estimation error  this should not be a practical problem.
Finally  if our bound relies on the generalization errors c and ¯Q  the classiﬁer will only use
(ˆµπE (si  a))1≤i≤N a∈A in the training phase  where si are the states from the set D.
It out-
puts θc  seen as a reward function  thus the estimated feature expectation ˆµπE is no longer re-
quired. Therefore  practically it should be sufﬁcient to estimate well ˆµπE on state-action couples
(si  a)1≤i≤N a∈A  which allows envisioning Monte-Carlo rollouts for example.

4 A Practical Approach

4.1 Estimating the Expert Feature Expectation

(s  a) = [T a
φi

vπE
φi

i

(s  a) = QπE
φi

SCIRL relies on an estimate ˆµπE of the expert feature expectation. Basically  this is a policy evalu-
ation problem. An already made key observation is that each component of µπE is the action-value
](s). We brieﬂy review
function of πE for a reward function φi: µπE
its exact computation and possible estimation approaches  and consider possible heuristics.
If the model is known  the feature expectation can be computed explicitly. Let Φ ∈ R|S|×p be the
feature matrix whose rows contain the feature vectors φ(s)> for all s ∈ S. For a ﬁxed a ∈ A 
a ∈ R|S|×p be the feature expectation matrix whose rows are the expert feature vectors  that
let µπE
is (µπE (s  a))> for any s ∈ S. With these notations  we have µπE
a = Φ + γPa(I − γPπE )−1Φ.
Moreover  the related computational cost is the same order of magnitude as evaluating a single policy
(as the costly part  computing (I − γPπE )−1  is shared by all components).
If the model is unknown  any temporal difference learning algorithm can be used to estimate the
expert feature expectation [7]  as LSTD (Least-Squares Temporal Differences) [4]. Let ψ : S×A →
Rd be a feature vector composed of d basis functions ψi ∈ RS×A. Each component µπE
of the

5

i

i

. . .

i  a0

i = πE(s0

i  a0

(s  a) ≈ ξ>

expert feature expectation is parameterized by a vector ξi ∈ Rd: µπE
i ψ(s  a). Assume
that we have a training set {(si  ai  s0
i))1≤i≤M} with actions ai not necessarily sampled
according to policy πE (e.g.  this may be obtained by sampling trajectories according to an expert-
based -greedy policy)  the aim being to have a better variability of tuples (non-expert actions should
be tried). Let ˜Ψ ∈ RM×d (resp. ˜Ψ0) be the feature matrix whose rows are the feature vectors
i)>). Let also ˜Φ ∈ RM×p be the feature matrix whose rows are the
ψ(si  ai)> (resp. ψ(s0
ξp] ∈ Rd×p be the matrix of all
reward’s feature vectors φ(si)>. Finally  let Ξ = [ξ1
parameter vectors. Applying LSTD to each component of the feature expectation gives the LSTD-µ
algorithm [7]: Ξ = ( ˜Ψ>( ˜Ψ − γ ˜Ψ0))−1 ˜Ψ> ˜Φ and ˆµπE (s  a) = Ξ>ψ(s  a). As for the exact case 
the costly part (computing the inverse matrix) is shared by all feature expectation components  the
computational cost is reasonable (same order as LSTD).
Provided a simulator and the ability to sample according to the expert policy  the expert feature
expectation may also be estimated using Monte-Carlo rollouts for a given state-action pair (as noted
in Sec. 3.2  ˆµπE need only be known on (si  a)1≤i≤N a∈A). Assuming that K trajectories are
sampled for each required state-action pair  this method would require KN|A| rollouts.
In order to have a small error ¯Q  one may learn using transitions whose starting state is sampled
according to ρE and whose actions are uniformly distributed. However  it may happen that only
i)1≤i≤N}. If the state-action cou-
transitions of the expert are available: T = {(si  ai = πE(si)  s0
ples (si  ai) may be used to feed the classiﬁer  the transitions (si  ai  s0
i) are not enough to provide
an accurate estimate of the feature expectation. In this case  we can still expect an accurate estimate
of µπE (s  πE(s))  but there is little hope for µπE (s  a 6= πE(s)). However  one can still rely on
some heuristic; this does not ﬁt the analysis of Sec. 3.2  but it can still provide good experimental
results  as illustrated in Sec. 6.
We propose such a heuristic. Assume that only data T is available and that we use it to provide an
(accurate) estimate ˆµπE (s  πE(s)) (this basically means estimating a value function instead of an
action-value function as described above). We may adopt an optimistic point of view by assuming
that applying a non-expert action just delays the effect of the expert action. More formally  we
associate to each state s a virtual state sv for which p(.|sv  a) = p(.|s  πE(s)) for any action a
and for which the reward feature expectation is the null vector  φ(sv) = 0. In this case  we have
µπE (s  a 6= πE(s)) = γµπE (s  πE(s)). Applying this idea to the available estimate (recalling that
the classiﬁers only requires evaluating ˆµπE on (si  a)1≤i≤N a∈A) provides the proposed heuristic:
for 1 ≤ i ≤ N  ˆµπE (si  a 6= ai) = γ ˆµπE (si  ai).
We may even push this idea further  to get the simpler estimate of the expert feature expectation (but
with the weakest guarantees). Assume that the set T consists of one long trajectory  that is s0
i = si+1
(thus T = {s1  a1  s2  . . .   sN−1  aN−1  sN   aN}). We may estimate µπE (si  ai) using the single
rollout available in the training set and use the proposed heuristic for other actions:

NX

∀1 ≤ i ≤ N  ˆµπE (si  ai) =

γj−iφ(sj) and ˆµπE (si  a 6= ai) = γ ˆµπE (si  ai).

(5)

j=i

To sum up  the expert feature expectation may be seen as a vector of action-value functions (for
the same policy πE and different reward functions φi). Consequently  any action-value function
evaluation algorithm may be used to estimate µπ(s  a). Depending on the available data  one may
have to rely on some heuristic to assess the feature expectation for a unexperienced (non-expert)
action. Also  this expert feature expectation estimate is only required for training the classiﬁer  so
it is sufﬁcient to estimate on state-action couples (si  a)1≤i≤N a∈A. In any case  estimating µπE is
not harder than estimating the action-value function of a given policy in the on-policy case  which is
much easier than computing an optimal policy for an arbitrary reward function (as required by most
of existing IRL algorithms  see Sec. 5).

4.2 An Instantiation

As stated before  any MC2 algorithm may be used. Here  we choose the structured large margin
approach [18]. Let L : S × A → R+ be a user-deﬁned margin function satisfying L(s  πE(s)) ≤

6

min
θ ζ

1
2

kθk2 + η
N

ζi

s.t. ∀i  θ> ˆµπE (si  ai) + ζi ≥ max

a

θ> ˆµπE (si  a) + L(si  a).

Following [13]  we express the equivalent hinge-loss form (noting that the slack variables ζi are
tight  which allows moving the constraints in the objective function):

NX

i=1

NX

i=1

L(s  a) (here  L(si  ai) = 0 and L(si  a 6= ai) = 1). The MC2 algorithm solves:

J(θ) =

1
N

max

a

θ> ˆµπE (si  a) + L(si  a) − θ> ˆµπE (si  ai) + λ
2

kθk2.

This objective function is minimized using a subgradient descent. The expert feature expectation is
estimated using the scheme described in Eq. (5).

5 Related Works

The notion of IRL has ﬁrst been introduced in [14] and ﬁrst been formalized in [11]. A classic ap-
proach to IRL  initiated in [1]  consists in ﬁnding a policy (through some reward function) such that
its feature expectation (or more generally some measure of the underlying trajectories’ distribution)
matches the one of the expert policy. See [10] for a review. Notice that related algorithms are not
always able to output a reward function  even if they may make use of IRL as an intermediate step.
In such case  they are usually refereed to as apprenticeship learning algorithms.
Closer to our contribution  some approaches also somehow introduce a structure in a classiﬁcation
procedure [8][13]. In [8]  a metric induced by the MDP is used to build a kernel which is used in
a classiﬁcation algorithm  showing improvements compared to a non-structured kernel. However 
this approach is not an IRL algorithm  and more important assessing the metric of an MDP is a
quite involved problem. In [13]  a classiﬁcation algorithm is also used to produce a reward function.
However  instead of associating actions to states  as we do  it associates optimal policies (labels) to
MDPs (inputs)  which is how the structure is incorporated. This involves solving many MDPs.
As far as we know  all IRL algorithms require solving the direct RL problem repeatedly  except [5  3].
[5] applies to linearly-solvable MDPs (where the control is done by imposing any dynamic to the
system). In [3]  based on a relative entropy argument  some utility function is maximized using a
subgradient ascent. Estimating the subgradient requires sampling trajectories according to the policy
being optimal for the current estimated reward. This is avoided thanks to the use of importance
sampling. Still  this requires sampling trajectories according to a non-expert policy and the direct
problem remains at the core of the approach (even if solving it is avoided).
SCIRL does not require solving the direct problem  just estimating the feature expectation of the
expert policy. In other words  instead of solving multiple policy optimization problems  we only
solve one policy evaluation problem. This comes with theoretical guarantees (which is not the case
of all IRL algorithms  e.g. [3]). Moreover  using heuristics which go beyond our analysis  SCIRL
may rely solely on data provided by expert trajectories. We demonstrate this empirically in the next
section. To the best of our knowledge  no other IRL algorithm can work in such a restrictive case.

6 Experiments

We illustrate the proposed approach on a car driving simulator  similar to [1  16]. The goal si to drive
a car on a busy three-lane highway with randomly generated trafﬁc (driving off-road is allowed on
both sides). The car can move left and right  accelerate  decelerate and keep a constant speed. The
expert optimizes a handcrafted reward RE which favours speed  punish off-road  punish collisions
even more and is neutral otherwise.
We compare SCIRL as instantiated in Sec. 4.2 to the unstructured classiﬁer (using the same classiﬁ-
cation algorithm) and to the algorithm of [1] (called here PIRL for Projection IRL). We also consider
the optimal behavior according to a randomly sampled reward function as a baseline (using the same
reward feature vector as SCIRL and PIRL  the associated parameter vector is randomly sampled).
For SCIRL and PIRL we use a discretization of the state space as the reward feature vector  φ ∈
R729: 9 horizontal positions for the user’s car  3 horizontal and 9 vertical positions for the closest

7

Figure 1: Highway problem. The highest line is the expert value. For each curves  we show the
mean (plain line)  the standard deviation (dark color) and the min-max values (light color). The
policy corresponding to the random reward is in blue  the policy outputted by the classiﬁer is in
yellow and the optimal policy according the SCIRL’s reward is in red. PIRL is the dark blue line.

trafﬁc’s car and 3 speeds. Notice that these features are much less informative than the ones used
in [1  16]. Actually  in [16] features are so informative that sampling a random positive parameter
vector θ already gives an acceptable behavior. The discount factor is γ = 0.9. The classiﬁer uses
the same feature vector reproduced for each action.
SCIRL is fed with n trajectories of length n (started in a random state) with n varying from 3 to
20 (so fed with 9 to 400 transitions). Each experiment is repeated 50 times. The classiﬁer uses the
same data. PIRL is an iterative algorithm  each iteration requiring to solve the MDP for some reward
function. It is run for 70 iterations  all required objects (a feature expectations for a non-expert policy
and an optimal policy according to some reward function at each iteration) are computed exactly
(s)]  where U is
using the model. We measure the performance of each approach with Es∼U[vπRE
the uniform distribution (this allows measuring the generalization capability of each approach for
states infrequently encountered)  RE is the expert reward and π is one of the following polices: the
optimal policy for RE (upper baseline)  the optimal policy for a random reward (lower baseline) 
the optimal policy for Rθc (SCIRL)  the policy produced by PIRL and the classiﬁer decision rule.
Fig. 1 shows the performance of each approach as a number of used expert transitions (except PIRL
which uses the model). We can see that the classiﬁer does not work well on this example. Increasing
the number of samples would improve its performance  but after 400 transitions it does not work as
well as SCIRL with only a ten of transitions. SCIRL works pretty well here: after only a hundred
of transitions it reaches the performance of PIRL  both being close to the expert value. We do not
report exact computational times  but running SCIRL one time with 400 transitions is approximately
hundred time faster than running PIRL for 70 iteration.

7 Conclusion

We have introduced a new way to perform IRL by structuring a linearly parameterized score
function-based multi-class classiﬁcation algorithm with an estimate of the expert feature expecta-
tion. This outputs a reward function for which we have shown the expert to be near optimal  provided
a small classiﬁcation error and a good expert feature expectation estimate. How to practically es-
timate this quantity has been discussed and we have introduced a heuristic for the case where only
transitions from the expert are available  along with a speciﬁc instantiation of the SCIRL algorithm.
We have shown on a car driving simulator benchmark that the proposed approach works well (even
combined with the introduced heuristic)  much better than the unstructured classiﬁer and as well as
a state-of-the-art algorithm making use of the model (and with a much lower computational time).
In the future  we plan to deepen the theoretical properties of SCIRL (notably regarding possible
heuristics) and to apply it to real-world robotic problems.

Acknowledgments. This research was partly funded by the EU FP7 project ILHAIRE (grant
n◦270780)  by the EU INTERREG IVa project ALLEGRO and by the Région Lorraine (France).

8

50100150200250300350400Numberofsamplesfromtheexpert−4−20246810Es∼U[VπRE(s)]50100150200250300350400Numberofsamplesfromtheexpert−4−20246810Es∼U[VπRE(s)]References
[1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning.

In Proceedings of the 21st International Conference on Machine learning (ICML)  2004.

[2] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and

Neural Computation Series  3). Athena Scientiﬁc  1996.

[3] Abdeslam Boularias  Jens Kober  and Jan Peters. Relative entropy inverse reinforcement learn-

ing. In JMLR Workshop and Conference Proceedings Volume 15: AISTATS 2011  2011.

[4] Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares algorithms for temporal differ-

ence learning. Machine Learning  22(1-3):33–57  1996.

[5] Krishnamurthy Dvijotham and Emanuel Todorov.

Inverse Optimal Control with Linearly-
Solvable MDPs. In Proceedings of the 27th International Conference on Machine Learning
(ICML)  2010.

[6] Yann Guermeur. VC thoery of large margin multi-category classiﬁers. Journal of Machine

Learning Research  8:2551–2594  2007.

[7] Edouard Klein  Matthieu Geist  and Olivier Pietquin. Batch  Off-policy and Model-free Ap-
prenticeship Learning. In Proceedings of the European Workshop on Reinforcement Learning
(EWRL)  2011.

[8] Francisco S. Melo and Manuel Lopes. Learning from demonstration using MDP induced

metrics. In Proceedings of the European Conference on Machine Learning (ECML)  2010.

[9] Rémi Munos. Performance bounds in Lp norm for approximate value iteration. SIAM journal

on control and optimization  46(2):541–561  2007.

[10] Gergely Neu and Czaba Szepesvari. Training Parsers by Inverse Reinforcement Learning.

Machine Learning  77(2-3):303–337  2009.

[11] Andrew Y. Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. In Pro-

ceedings of 17th International Conference on Machine Learning (ICML)  2000.

[12] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience  1994.

[13] Nathan Ratliff  Andrew D. Bagnell  and Martin Zinkevich. Maximum Margin Planning. In

Proceedings of the 23rd International Conference on Machine Learning (ICML)  2006.

[14] Stuart Russell. Learning agents for uncertain environments (extended abstract). In Proceedings

of the 11th annual Conference on Computational Learning Theory (COLT)  1998.

[15] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT

Press  3rd edition  March 1998.

[16] Umar Syed and Robert Schapire. A game-theoretic approach to apprenticeship learning. In

Advances in Neural Information Processing Systems 20 (NIPS)  2008.

[17] Csaba Szepesvári. Algorithms for Reinforcement Learning. Morgan and Claypool  2010.
[18] Ben Taskar  Vassil Chatalbashev  Daphne Koller  and Carlos Guestrin. Learning Structured
Prediction Models: a Large Margin Approach. In Proceedings of 22nd International Confer-
ence on Machine Learning (ICML)  2005.

9

,Yoshua Bengio
Li Yao
Guillaume Alain
Pascal Vincent