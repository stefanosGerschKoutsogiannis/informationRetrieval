2018,Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation,We consider the off-policy estimation problem of estimating the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique to derive (nearly) unbiased estimators  but is known to suffer from an excessively high variance in long-horizon problems.  In the extreme case of in infinite-horizon problems  the variance of an IS-based estimator may even be unbounded. In this paper  we propose a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators.Our key contribution is a novel approach to estimating the density ratio of two stationary distributions  with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem  and derive a closed-form solution for the case of RKHS. We support our method with both theoretical  and empirical analyses.,Breaking the Curse of Horizon:

Inﬁnite-Horizon Off-Policy Estimation

Qiang Liu

The University of Texas at Austin

Austin  TX  78712

lqiang@cs.utexas.edu

Ziyang Tang

The University of Texas at Austin

Austin  TX  78712

ztang@cs.utexas.edu

Lihong Li

Google Brain

Kirkland  WA  98033
lihong@google.com

Dengyong Zhou
Google Brain

Kirkland  WA  98033

dennyzhou@google.com

Abstract

We consider off-policy estimation of the expected reward of a target policy using
samples collected by a different behavior policy. Importance sampling (IS) has
been a key technique for deriving (nearly) unbiased estimators  but is known to
suffer from an excessively high variance in long-horizon problems. In the extreme
case of inﬁnite-horizon problems  the variance of an IS-based estimator may even
be unbounded. In this paper  we propose a new off-policy estimator that applies
IS directly on the stationary state-visitation distributions to avoid the exploding
variance faced by existing methods. Our key contribution is a novel approach to
estimating the density ratio of two stationary state distributions  with trajectories
sampled from only the behavior distribution. We develop a mini-max loss function
for the estimation problem  and derive a closed-form solution for the case of RKHS.
We support our method with both theoretical and empirical analyses.

1

Introduction

Reinforcement learning (RL) [36] is one of the most successful approaches to artiﬁcial intelligence 
and has found successful applications in robotics  games  dialogue systems  and recommendation
systems  among others. One of the key problems in RL is policy evaluation: given a ﬁxed policy 
estimate the average reward garnered by an agent that runs this policy in the environment. In this
paper  we consider the off-policy estimation problem  in which we want to estimate the expected
reward of a given target policy with samples collected by a different behavior policy. This problem
is of great practical importance in many application domains where deploying a new policy can
be costly or risky  such as medical treatments [26]  econometrics [13]  recommender systems [19] 
education [23]  Web search [18]  advertising and marketing [4  5  38  40]. It can also be used as a key
component for developing efﬁcient off-policy policy optimization algorithms [7  14  18  39].
Most state-of-the-art off-policy estimation methods are based on importance sampling (IS) [e.g.  22].
A major limitation  however  is that this approach can become inaccurate due to the high variance
introduced by the importance weights  especially when the trajectory is long. Indeed  most existing
IS-based estimators compute the weight as the product of the importance ratios of many steps in the
trajectory. Variances in individual steps accumulate multiplicatively  so that the overall IS weight of a
random trajectory can have an exponentially high variance to result in an unreliable estimator. In the
extreme case when the trajectory length is inﬁnite  as in inﬁnite-horizon average-reward problems 
some of these estimators are not even well-deﬁned. Ad hoc approaches can be used  such as truncating

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

the trajectories  but often lead to a hard-to-control bias in the ﬁnal estimation. Analogous to the
well-known “curse of dimensionality” in dynamic programming [2]  we call this problem the “curse
of horizon” in off-policy learning.
In this work  we develop a new approach that tackles the curse of horizon. The key idea is to apply
importance sampling on the average visitation distribution of single steps of state-action pairs  instead
of the much higher dimensional distribution of whole trajectories. This avoids the cumulative product
across time in the density ratio  substantially decreasing its variance and eliminating the estimator’s
dependence on the horizon.
Our key challenge  of course  is to estimate the importance ratios of average visitation distributions.
In practice  we often have access to both the target and behavior policies to compute their importance
ratio of an action conditioned on a given state. But we typically have no access to transition
probabilities of the environment  so estimating importance ratios of state visitation distributions has
been very difﬁcult  especially when only off-policy samples are available. In this paper  we develop a
mini-max loss function for estimating the true stationary density ratio  which yields a closed-form
representation similar to maximum mean discrepancy [9] when combined with a reproducing kernel
Hilbert space (RKHS). We study the theoretical properties of our loss function  and demonstrate its
empirical effectiveness on long-horizon problems.

2 Background
Problem Deﬁnition Consider a Markov decision process (MDP) [31] M = hS A  r  Ti with state
space S  action space A  reward function r  and transition probability function T . Assume the
environment is initialized at state s0 2S   drawn from an unknown distribution d0(·). At each time
step t  an agent observes the current state st  takes an action at according to a possibly stochastic
policy ⇡(·|st)  receives a reward rt whose expectation is r(st  at)  and transitions to a next state
st+1 according to transition probabilities T (·|st  at). To simplify exposition and avoid unnecessary
technicalities  we assume S and A are ﬁnite unless otherwise speciﬁed  although our method extends
to continuous spaces straightforwardly  as demonstrated in experiments.
We consider the inﬁnite horizon problem in which the MDP continues without termination. Let p⇡(·)
be the distribution of trajectory ⌧ = {st  at  rt}1t=0 under policy ⇡. The expected reward of ⇡ is

R⇡ := lim
T!1

E⌧⇠p⇡ [RT (⌧ )] 

RT (⌧ ) := (

trt)/(

t)  

TXt=0

TXt=0

where RT
distinguish two reward criteria  the average reward ( = 1) and discounted reward (0 << 1):

⇡ (⌧ ) is the reward of trajectory ⌧ up to time T . Here   2 (0  1] is a discount factor. We

Average: R(⌧ ) := lim
T!1

1

T + 1

TXt=0

rt 

Discounted: R(⌧ ) := (1  )

trt .

1Xt=0

where (1  ) = 1/P1t=0 t is a normalization factor. The problem of off-policy value estimation
is to estimate the expected reward R⇡ of a given target policy ⇡  when we only observe a set of
trajectories ⌧ i = {si
Bellman Equation We brieﬂy review the Bellman equation and the notation of value functions 
for both average and discounted reward criteria. In the discounted case (0 << 1)  the value
V ⇡(s) is the expected total discounted reward when the initial state s0 is ﬁxed to be s: V ⇡(s) =

t=0 generated by following a different behavior policy ⇡0.
t}T

t  ai

t  ri

E⌧⇠p⇡ [P1t=0 trt | s0 = s]. Note that we do not normalize V ⇡ by (1  ) in our notation. For the
average reward ( = 1) case  the expected average reward does not depend on the initial state if the
Markov process is ergodic [31]. Instead  the value function V ⇡(s) in the average case measures the
average adjusted sum of reward: V ⇡(s) = limT!1 E⌧⇠p⇡ [PT
t=0(rt  R⇡)|s0 = s]. It represents

the relative difference in total reward gained from starting in state s0 = s as opposed to R⇡.
Under these deﬁnitions  V ⇡ is the ﬁxed-point solution to the respective Bellman equations:
V ⇡(s)  Es0 a|s⇠d⇡ [V ⇡(s0)] = Ea|s⇠⇡[r(s  a)  R⇡]  
V ⇡(s)  Es0 a|s⇠d⇡ [V ⇡(s0)] = Ea|s⇠⇡[r(s  a)] .

Average:
Discounted:

(1)
(2)

2

Importance Sampling IS represents a major class of approaches to off-policy estimation  which 
in principle  only applies to the ﬁnite-horizon reward RT
⇡ when the trajectory is truncated at a ﬁnite
time step T < 1. IS-based estimators are based on the following change-of-measure equality:
⇡/⇡0(at|st)  

⇡ = E⌧⇠p⇡0
RT

[w0:T (⌧ )RT (⌧ )]  

p⇡(⌧ 0:T )
p⇡0(⌧ 0:T )

w0:T (⌧ ) :=

with

(3)

where ⇡/⇡0(a|s) := ⇡(a|s)/⇡0(a|s) is the single-step density ratio of policies ⇡ and ⇡0 evaluated
at a particular state-action pair (s  a)  and w0:T is the density ratio of the trajectory ⌧ up to time
T . Methods based on (3) are called trajectory-wise IS  or weighted IS (WIS) when the weights are
self-normalized [22  30]. It is possible to improve trajectory-wise IS with the so called step-wise  or
per-decision  IS/WIS  which uses weight w0:t for reward rt at time t  yielding smaller variance [30].
More details about these estimators are given in Appendix A.

=

TYt=0

The Curse of Horizon The importance weight w0:T is a product of T density ratios  whose variance
can grow exponentially with T . Thus  IS-based estimators have not been widely successful in long-
horizon problems  let alone inﬁnite-horizon ones where w0:1 may not even be well-deﬁned. While
WIS estimators often have reduced variance  the exponential dependence on horizon is unavoidable
in general. We call this phenomenon in IS/WIS-based estimators the curse of horizon.
Not all hope is lost  however. To see this  consider an MDP with n
states and 2 actions  where states are arranged on a circle (see ﬁgure
on the right). The two actions deterministically move the agent from
the current state to the neighboring state counterclockwise and clock-
wise  respectively. Suppose we are given two policies with opposite
effects: the behavior policy ⇡0 moves the agent clockwise with prob-
ability ⇢  and the target policy ⇡ moves the agent counterclockwise
with probability ⇢  for some constant ⇢ 2 (0  1). As shown in Ap-
pendix B  IS and WIS estimators suffer from exponentially large
variance when estimating the average reward of ⇡. However  a keen
reader will realize that the two policies are symmetric  and thus their stationary state visitation
distributions are identical. As we show in the sequel  this allows us to estimate the expected reward
using a much more efﬁcient importance sampling  whose importance weight equals the single-step
density ratio ⇡/⇡0(at|st)  instead of the cumulative product weight w0:T in (3)  allowing us to
signiﬁcantly reduce the variance. Such an observation inspired the approach developed in this paper.

3 Off-Policy Estimation via Stationary State Density Ratio Estimation

As shown in the example above  signiﬁcant decrease in estimation variance is possible when we
apply importance weighting on the state space  rather than the trajectory space. It eliminates the
dependency on the trajectory length and is much more suited for long- or inﬁnite-horizon problems.
To realize this  we need to introduce an alternative representation of the expected reward. Denote by
d⇡ t(·) the distribution of state st when we execute policy ⇡ starting from an initial state s0 drawn
from an initial distribution d0(·). We deﬁne the average visitation distribution to be

d⇡(s) = lim

T!1 TXt=0

td⇡ t(s)! / TXt=0

t! .

(4)

We always assume the limit T ! 1 exists in this work. When  2 (0  1) in the discounted case 
d⇡ is a discounted average of d⇡ t  that is  d⇡(s) = (1  )P1t=0 td⇡ t(s) ; when  = 1 in
the average reward case  d⇡ is the stationary distribution of st as t ! 1 under policy ⇡  that is 
d⇡(s) = limT!1
Following Deﬁnition 4  it can be veriﬁed that R⇡ can be expressed alternatively as

t=0 d⇡ t(s) = limt!1 d⇡ t(s).

T +1PT

1

R⇡ =Xs a

d⇡(s)⇡(a|s)r(s  a) = E(s a)⇠d⇡ [r(s  a)] 

(5)

where  abusing notation slightly  we use (s  a) ⇠ d⇡ to denote draws from distribution d⇡(s  a) :=
d⇡(s)⇡(a|s). Our idea is to construct an IS estimator based on (5)  where the importance ratio is

3

computed on state-action pairs rather than on trajectories:

t  ai

t  ri

R⇡ = E(s a)⇠d⇡0⇥w⇡/⇡0(s)⇡/⇡0(a  s)r(s  a)⇤  

i=1 obtained when running policy ⇡0 
t}m
wi

(6)
where ⇡/⇡0(a  s) = ⇡(a|s)/⇡0(a|s) and w⇡/⇡0(s) := d⇡(s)/d⇡0(s) is the density ratio of the
visitation distributions d⇡ and d⇡0; here  w⇡/⇡0(s) is not known directly but can be estimated  as
shown later. Eq 5 allows us to construct a (weighted-)IS estimator by approximating E(s a)⇠d⇡0
[·]
with data {si
ˆR⇡ =

t|si
t)
t0|si0
t0)⇡/⇡0(ai0
t0)
This IS estimator works in the space of (s  a)  instead of trajectoris ⌧ = {st  at}T
t=0  leading
to a potentially signiﬁcant variance reduction. Returning to the example in Section 2 (see also
Appendix B)  since the two policies are symmetric and lead to the same stationary distributions  that is 
w⇡/⇡0(s) = 1  the importance weight in (6) is simply ⇡(a|s)/⇡0(a|s)  independent of the trajectory
length. This avoids the excessive variance in long horizon problems. In Appendix A  we provide a
further discussion  showing that our estimator can be viewed as a type of Rao-Backwellization of the
trajectory-wise and step-wise estimators.

Pt0 i0 t0w⇡/⇡0(si0

tw⇡/⇡0(si

t)⇡/⇡0(ai

TXt=0

mXi=1

wi

t :=

.

(7)

where

tri
t 

3.1 Average Reward Case
The key technical challenge remaining is estimating the density ratio w⇡/⇡0(s)  which we address in
this section. For simplifying the presentation  we start with estimating d⇡(s) for the average reward
case and discuss the discounted case in Section 3.2.

Let T ⇡(s0|s) :=Pa T (s0|s  a)⇡(a|s) be the transition probability from s to s0 following policy ⇡.

In the average reward case  d⇡ equals the stationary distribution of T ⇡  satisfying

d⇡(s0) =Xs

T ⇡(s0|s)d⇡(s)  8s0.

Assume the Markov chain of T ⇡ is ﬁnite state and ergodic  d⇡ is also the unique distribution that
satisﬁes (8). This simple fact can be leveraged to derive the following key property of w⇡/⇡0(s).
Theorem 1. In the average reward case ( = 1)  assume d⇡ is the unique invariant distribution of
T ⇡ and d⇡0(s) > 0  8s. Then a function w(s) equals w⇡/⇡0(s) := d⇡(s)/d⇡0(s) (up to a constant
factor) if and only if it satisﬁes

E(s a)|s0⇠d⇡0
with

[(w; s  a  s0) | s0] = 0 

8 s0 

(w; s  a  s0) := w(s)⇡/⇡0(a|s)  w(s0) 

where ⇡/⇡0(a|s) = ⇡(a|s)/⇡0(a|s) and (s  a)|s0 ⇠ d⇡0 denote the conditional distribution
d⇡0(s  a|s0) related to joint distribution d⇡0(s  a  s0) := d⇡0(s)⇡0(a|s)T (s0|s  a). Note that this
is a time-reserved conditional probability  since it is the conditional distribution of (s  a) given that
their next state is s0 following policy ⇡0.

Because the conditional distribution is time reversed  it is difﬁcult to directly estimate the conditional
expectation E(s a)|s0[·] for a given s0. This is because we usually can observe only a single data
point from d⇡0(s  a|s0) of a ﬁxed s0  given that it is difﬁcult to see by chance two different (s  a)
pairs transit to the same s0. This problem can be addressed by introducing a discriminator function
and constructing a mini-max loss function. Speciﬁcally  multiplying (9) with a function f (s0) and
averaging under s0 ⇠ d⇡0 gives

(8)

(9)

(10)

(11)

Following Theorem 1  we have w / w⇡/⇡0 if and only if L(w  f ) = 0 for any function f. This
motivates us to estimate w⇡/⇡0 with a mini-max problem:

L(w  f ) := E(s a s0)⇠d⇡0

[(w; s  a  s0)f (s0)]

= E(s a s0)⇠d⇡0⇥w(s)⇡/⇡0(a|s)  w(s0) f (s0)⇤ .
w D(w) := max

L (w/zw  f )2  

f2F

min

4

where F is a set of discriminator functions and zw := Es⇠d⇡0
[w(s)] normalizes w to avoid the trivial
solution w ⌘ 0. We shall assume F to be rich enough following the conditions to be discussed in
Section 3.3. A promising choice of a rich function class is neural networks  for which the mini-max
problem (11) can be solved numerically in a fashion similar to generative adversarial networks
(GANs) [8]. Alternatively  we can take F to be a ball of a reproducing kernel Hilbert space (RKHS) 
which enables a closed form representation of D(w) as we show in the following.
Theorem 2. Assume H is a RKHS of functions f (s) with a positive deﬁnite kernel k(s  ¯s)  and deﬁne
F := {f 2H : ||f||H  1} to be the unit ball of H. We have

L(w  f )2 = Ed⇡0

[(w; s  a  s0)(w; ¯s  ¯a  ¯s0)k(s0  ¯s0)]  

(12)

max
f2F

where (s  a  s0) and (¯s  ¯a  ¯s0) are independent transition pairs obtained when running policy ⇡0  and
(w; s  a  s0) is deﬁned in (10). See Appendix C for more background on RKHS.

In practice  we approximate the expectation in (12) using empirical distribution of the transition pairs 
yielding consistent estimates following standard results on V-statistics [33].

3.2 Discounted Reward Case
We now discuss the extension to the discount case of  2 (0  1). Similar to the average reward case 
we start with a recursive equation that characterizes d⇡(s) in the discounted case.
Lemma 3. Following the deﬁnition of d⇡ in (4)  for any  2 (0  1]  we have

Denote by (s  a  s0) ⇠ d⇡ draws from d⇡(s)⇡(a|s)T (s0|s  a). For any function f  we have

T ⇡(s0|s)d⇡(s)  d⇡(s0) + (1  )d0(s0) = 0 

Xs
E(s a s0)⇠d⇡ [f (s0)  f (s)] + (1  )Es⇠d0[f (s)] = 0.

8s0.

(13)

(14)

One may view d⇡ as the invariant distribution of an induced Markov chain with transition probability
of (1  )d0(s0) + T ⇡(s0|s)  which follows T ⇡ with probability   and restarts from initial
distribution d0(s0) with probability 1  . We can show that d⇡ exists and is unique under mild
conditions [31].
Theorem 4. Assume d⇡ is the unique solution of (13)  and d⇡0(s) > 0  8s. Deﬁne

L(w  f ) = E(s a s0)⇠d⇡0

[(w; s  a  s0)f (s0)] + (1  )Es⇠d0[(1  w(s))f (s)].

Assume 0 << 1  then w(s) = w⇡/⇡0(s) if and only if L(w  f ) = 0 for any test function f.
When  = 1  the deﬁnition in (15) reduces to the average reward case in (10). A subtle difference is
that L(w  f ) = 0 only ensures w / w⇡/⇡0 when  = 1  while w = w⇡/⇡0 when  2 (0  1). This is
because the additional term Es⇠d0[(1  w(s))f (s)] in (15) forces w to be normalized properly. In
practice  however  we still ﬁnd it works better to pre-normalize w to ˜w = w/Ed⇡0
[w]  and optimize
the objective L( ˜w  f ).

(15)

3.3 Further Theoretical Analysis
In this section  we develop further theoretical understanding on the loss function L(w  f ). Lemma 5
below reveals an interesting connection between L(w  f ) and the Bellman equation  allowing us to
bound the estimation error of density ratio and expected reward with the mini-max loss when the
discriminator space F is chosen properly (Theorems 6 and 7). The results in this section apply to
both discounted and average reward cases.
Lemma 5. Given L(w  f ) in (15)  and assuming Ed⇡0

L(w  f ) = Es⇠d⇡0
where

[(w⇡/⇡0(s)  w(s))⇧f (s)]  

[w] = 1 in the average reward case  we have
(16)
(17)

⇧f (s) := f (s)  E(s0 a)|s⇠d⇡ [f (s0)] .

Note that ⇧f equals the left hand side of the Bellman equations (1) and (2)  when f = V ⇡.

5

Lemma 5 represents L(w  f ) as an inner product between w⇡/⇡0  w and ⇧f (under base measure
d⇡0). This provides an alternative proof of Theorem 4  since L(w  f ) = 0  8f 2F implies that
w⇡/⇡0  w is orthogonal with all ⇧f and hence w⇡/⇡0 = w when {⇧f : f 2F} is sufﬁciently rich.
In order to make (w⇡/⇡0  w) orthogonal to a given function g  it requires “reversing” operator ⇧:
ﬁnding a function fg which solves g =⇧ fg for given g. Observing that g =⇧ fg can be viewed
as a Bellman equation (Eqs. (1)–(2)) when taking g and fg to be the reward and value functions 
respectively  we can derive an explicit representation of fg (Lemma 10 in Appendix). This allows
one to gain insights into what discriminator set F would be a good choice  so that minimizing
maxf2F L(w  f ) yields good estimation with desirable properties. In the following  by taking
g(s) /± 1(s = ˜s)  8˜s  we can characterize the conditions on F under which the mini-max loss
upper bounds the estimation error of w⇡/⇡0 or d⇡.

Theorem 6. Let T t

Assume Lemma 5 holds. We have

(18)

when 0 << 1 

⇡(˜s|s)
⇡(˜s|s)  d⇡(˜s)) when  = 1 

⇡(s0|s) be the t-step transition probability of T ⇡(s0|s). For 8˜s 2S   deﬁne
f˜s(s) =(P1t=0 tT t
P1t=0(T t
L(w  f ) w⇡/⇡0  w1

{±f˜s : 8˜s 2S}✓F  
{±f˜s/d⇡0(˜s) : 8˜s 2S}✓F .

L(w  f )  kd⇡(s)  w(s)d⇡0(s)k1  

if

if

 

max
f2F
max
f2F

Since our main goal is to estimate the expected total reward R⇡ instead of the density ratio w⇡/⇡0  it
is of interest to select F to directly bound the estimation error of the total reward. Interestingly  this
can be achieved once F includes the true value function V ⇡.
Theorem 7. Deﬁne R⇡[w] to be the reward estimate using estimated density ratio w(s) (which may
not equal the true ratio w⇡/⇡0) and inﬁnite number of trajectories from d⇡0  that is 

R⇡[w] := E(s a s0)⇠d⇡0

[w(s)⇡/⇡0(a|s)r(s  a)] .

Assume w is properly normalized such that Es⇠d⇡0
Therefore  if ±V ⇡ 2F   we have |R⇡[w]  R⇡| maxf2F L(w  f ).
4 Related Work

[w(s)] = 1  we have L(w  V ⇡) = R⇡  R⇡[w].

Our off-policy setting is related to  but different from  off-policy value-function learning [30  29 
37  12  25  21]. Our goal is to estimate a single scalar that summarizes the quality of a policy (a.k.a.
off-policy value estimation as called by some authors [20]). However  our idea can be extended to
estimating value functions as well  by using estimated density ratios to weight observed transitions
(c.f.  the distribution µ in LSTDQ [16]). We leave this as future work.
IS-based off-policy value estimation has seen a lot of interest recently for short-horizon problems 
including contextual bandits [26  13  7  42]  and achieved many empirical successes [7  34]. When
extended to long-horizon problems  it faces an exponential blowup of variance  and variance-reduction
techniques are used to improve the estimator [14  39  10  42]. However  it can be proved that
in the worst case  the mean squared error of any estimator has to depend exponentially on the
horizon [20  10]. Fortunately  many problems encountered in practical applications may present
structures that enable more efﬁcient off-policy estimation  as tackled by the present paper. An
interesting open direction is to characterize theoretical conditions that can ensure tractable estimation
for long horizon problems.
Few prior work directly target inﬁnite-horizon problems. There exists approaches that use simulated
samples to estimate stationary state distributions [1  Chapter IV]. However  they need a reliable model
to draw such simulations  a requirement that is not satisﬁed in many real-world applications. To the
best of our knowledge  the recently developed COP-TD algorithm [11] is the only work that attempts
to estimate w⇡/⇡0 as an intermediate step of estimating the value function of a target policy ⇡. They
take a stochastic-approximation approach and show asymptotic consistence. However  extending
their approach to continuous state/action spaces appears challenging.

6

-5

-6

-7

-8

-9

-10

-11

E
S
M
g
o
l

-5

-6

-7

-8

-9

-4

-6

-8

-10

Naive Average
On Policy (oracle)
WIS Trajectory-wise
WIS Step-wise
Model-based
Our Method

e
c
n
a
t
s
i
D
V
T

30 50 

100

200

400

800

0.2

0.4

0.6

0.8

1.0

200 

600 

1000

1500

# of Trajectories (n)

Different Behavior Policies

Truncated Length T

(a)

(b)

(c)

ˆd⇡ (s) vs. d⇡ (s) Plot

Training Iteration

(d)

(e)

Figure 1: Results on Taxi environment with average reward ( = 1). (a)-(b) show the performance of various
methods as the number of trajectory (a) and the difference between behavior and target policies (b) vary. (c)
shows the change of truncated length T . (d) shows that scatter plot of pairs ( ˆd⇡(s)  d⇡(s))  8s. The diagonal
lines means exact estimation. (e) shows the weighted total variation distance between ˆd⇡ := ˆwd⇡0 and d⇡ along
the training iteration of the ratio estimator ˆw. The number of trajectory is ﬁxed to be 100 in (b c d). The potential
behavior policy ⇡+ (the right most points in (b)) is used in (a c d e).

Finally  there is a comprehensive literature of two-sample density ratio estimation [e.g.  27  35] 
which estimates the density ratio of two distributions from pairs of their samples. Our problem setting
is different in that we only have data from d⇡0  but not from d⇡; this makes the traditional density
ratio estimators inapplicable to our problem. Our method is made possible by taking the special
temporal structure of MDP into consideration.

5 Experiment

In this section  we conduct experiments on different environmental settings to compare our method
with existing off-policy evaluation methods. We compare with the standard trajectory-wise and
step-wise IS and WIS methods. We do not report the results of unnormalized IS because they are
generally signiﬁcantly worse than WIS methods [30  22]. In all the cases  we also compare with an
on-policy oracle and a naive averaging baseline  which estimates the reward using direct averaging
over the trajectories generated by the target policy and behavior policy  respectively. For problems
with discrete action and state spaces  we also compare with a standard model-based method  which
estimates the transition and reward model and then calculates expected reward explicitly using the
model up to the desired truncation length. When applying our method on problems with ﬁnite and
discrete state space  we optimize w and f in the space of all possible functions (corresponding to using
a delta kernel in terms of RKHS). For continuous state space  we assume w is a standard feed-forward
neural network  and F is a RKHS with a standard Gaussian RBF kernel whose bandwidth equals the
median of the pairwise distances between the observed data points.
Because we cannot simulate truly inﬁnite steps in practice  we use the behavior policy to generate
trajectories of length T   and evaluate the algorithms based on the mean square error (MSE) w.r.t. the
T -step rewards of a large number of trajectories of length T from the target policy. We expect that
our method gets better as T increases  since it is designed for inﬁnite horizon problems  while the
IS/WIS methods receive large variance and deteriorate as T increases.

Taxi Environment Taxi [6] is a 2D grid world simulating taxi movement along the grids. A taxi
moves North  East  South  West or attends to pick up or drop off a passenger. It receives a reward
of 20 when it successfully picks up a passenger or drops her off at the right place  and otherwise a
reward of -1 every time step. The original taxi environment would stop when the taxi successfully
picks up a passenger and drops her off at the right place. We modify the environment to make it
inﬁnite horizon  by allowing passengers to randomly appear and disappear at every corner of the
map at each time step. We use a grid size of 5 ⇥ 5  which yields 2000 states in total (25 ⇥ 24 ⇥ 5 
corresponding to 25 taxi locations  24 passenger appearance status and 5 taxi status (empty or with
one of 4 destinations)).
To construct target and behavior policies for testing our algorithm  we set our target policy to be
the ﬁnal policy ⇡⇤ after running Q-learning for 1000 iterations  and set another policy ⇡+ after 950
iterations. The behavior policy is ⇡ = (1 ↵)⇡⇤ + ↵⇡+  where ↵ is a mixing ratio that can be varied.
Results in Taxi Environment Figure 1(a)–(b) show results with average reward. We can see our
method performs almost as well as the on-policy oracle  outperforming all the other methods. To
evaluate the approximation error of the estimated density ratio ˆw  we plot in Figure 1(c) the weighted

7

E
S
M
g
o
l

-4

-6

-8

-10

-12

-5

-6

-7

-8

-9

-2

-4

-6

-8

-5

-6

-7

-8

-9

-10

Naive Average
On Policy (oracle)
WIS Trajectory-wise
WIS Step-wise
Model-based
Our Method

30 50 

100

200

400

800

0.2

0.4

0.6

0.8

1.0

50 

200 

600 

1500

0.95

0.97 0.98 0.99

1 

# of Trajectories (n)

Different Behavior Policies

Truncated length T

Discounted Factor 

(a)

(b)

(c)

(d)

Figure 2: Results on Taxi with discounted reward (0 << 1)  as we vary the number of trajectory n (a)  the
difference between target and behavior policies (b)  the truncated length T (c)  the discount factor  (d). The
default values of the parameters  unless it is varying  are  = 0.99  n = 200  T = 400. The potential behavior
policy ⇡+ (the right most points in (b)) is used in (a c d).

E
S
M
g
o
l

2

0

-2

-4

-6

2

0

-2

-4

2

0

-2

-4

2

0

-2

-4

-6

-8

Naive Average
On Policy (oracle)
WIS Trajectory-wise
WIS Step-wise
Our Method

0 

0.2

0.4

0.6

0.8

1 

100 

300 

500 

800 

1000

0 

0.2

0.4

0.6

0.8

1 

0.97

0.98

0.99

1 

Mixing Ratio ↵

Truncated Length T

Mixing Ratio ↵

Discount Factor 

(a) Average Reward Case

(b) Average Reward Case

(c) Discounted Reward Case

(d) Discounted Reward Case

Figure 3: Results on Pendulum. (a)-(b) show the results in the average reward case when we vary the mixing
ratio ↵ in the behavior policies and the truncated length T   respectively. (c)-(d) show the results of the discounted
reward case when we vary mixing ratio ↵ in the behavior policies and discount factor   respectively. The default
parameters are n = 150  T = 1000   = 0.99  ↵ = 1.

total variation distance between ˆd⇡ = ˆwd⇡0 with the true d⇡ with TV distance as we optimize the loss
function. Figure 1(d) shows scatter plot of {( ˆd⇡(s)  d⇡(s)) : 8s 2S} at convergence  indicating
our method correctly estimates the true density ratio over the state space.
Figure 2 shows similar results for discounted reward. From Figure 2(c) and (d)  we can see that
typical IS methods deteriorate as the trajectory length T and discount factor  increase  respectively 
which is expected since their variance grows exponentially with T . In contrast  our density ratio
method performs better as trajectory length T increases  and is robust as  increases.

Pendulum Environment The Taxi environment features discrete action and state spaces. We
now test Pendulum  which has a continuous state space of R3 and action space of [2  2]. In this
environment  we want to control the pendulum to make it stand up as long as possible (for the average
case)  or as fast as possible (for small discounted case). The policy is taken to be a truncated Gaussian
whose mean is a neural network of the states and variance a constant.
We train a near-optimal policy ⇡⇤ using REINFORCE and set it to be the target policy. The behavior
policy is set to be ⇡ = (1  ↵)⇡⇤ + ↵⇡+  where ↵ is a mixing ratio  and ⇡+ is another policy from
REINFORCE when it has not converged. Our results are shown in Figure 3  where we again ﬁnd
that our method generally outperforms the standard trajectory-wise and step-wise WIS  and works
favorably in long-horizon problems (Figure 3(b)).
SUMO Trafﬁc Simulator SUMO [15] is an open source trafﬁc simulator; see Figure 4(a) for an
illustration. We consider the task of reducing trafﬁc congestion by modelling trafﬁc light control as a
reinforcement learning problem [41]. We use TraCI  a built-in “Trafﬁc Control Interface”  to interact

0

-2

E
S
M
-4
g
o
l

-6

-8

0

E
-2
S
M
g
-4
o
l

-6

-8

0

-2
E
S
M
-4
g
o
-6
l

-8

Naive Average
On Policy (oracle)
WIS Trajectory-wise
WIS Step-wise
Our Method

(a) Environment

(b) # of Trajectories (n)

(c) Different Behavior Policies

(d) Truncated Length T

30 

50 

100

200

1

2

3

4

5

200 

400 

600 

800 1000

Figure 4: Results on SUMO (a) with average reward  as we vary the number of trajectories (b)  choose different
behavior policies (c)  and truncated size (d). When being ﬁxed  the default parameters are n = 250  T = 400.
The behavior policy in (c) with x-tick 2 is used in (b) and (d).

8

with the SUMO simulator. Full details of our environmental settings can be found in Appendix E.
Our results are shown in Figure 4  where we again ﬁnd that our method is consistently better than
standard IS methods.

6 Conclusions

We study the off-policy estimation problem in inﬁnite-horizon problems and develop a new algorithm
based on direct estimation of the stationary state density ratio between the target and behavior policies.
Our mini-max objective function enjoys nice theoretical properties and yields an intriguing connection
with Bellman equations that is worth further investigation. Future directions include scaling our
method to larger scale problems and extending it to estimate value functions and leverage off-policy
data in policy optimization.

Acknowledgement

This work is supported in part by NSF CRII 1830161. We would like to acknowledge Google Cloud
for their support.

References
[1] Søren Asmussen and Peter W. Glynn. Stochastic Simulation: Algorithms and Analysis  vol-

ume 57 of Probability Theory and Stochastic Processes. Springer-Verlag  2007.
[2] Richard E. Bellman. Dynamic Programming. Princeton University Press  1957.
[3] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability

and statistics. Springer Science & Business Media  2011.

[4] Léon Bottou  Jonas Peters  Joaquin Quiñonero-Candela  Denis Xavier Charles  D. Max Chicker-
ing  Elon Portugaly  Dipankar Ray  Patrice Simard  and Ed Snelson. Counterfactual reasoning
and learning systems: The example of computational advertising. Journal of Machine Learning
Research  14:3207–3260  2013.

[5] Olivier Chapelle  Eren Manavoglu  and Romer Rosales. Simple and scalable response prediction
for display advertising. ACM Transactions on Intelligent Systems and Technology  5(4):61:1–
61:34  2014.

[6] Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function

decomposition. Journal of Artiﬁcial Intelligence Research  13:227–303  2000.

[7] Miroslav Dudík  John Langford  and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on Machine Learning (ICML)  pages
1097–1104  2011.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems 27 (NIPS)  pages 2672–2680  2014.

[9] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. The Journal of Machine Learning Research  13(1):723–773 
2012.

[10] Zhaohan Guo  Philip S. Thomas  and Emma Brunskill. Using options and covariance testing
for long horizon off-policy policy evaluation. In Advances in Neural Information Processing
Systems 30 (NIPS)  pages 2489–2498  2017.

[11] Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In Proceedings of the

34th International Conference on Machine Learning (ICML)  pages 1372–1383  2017.

[12] Assaf Hallak  Aviv Tamar  Remi Munos  and Shie Mannor. Generalized emphatic temporal
difference learning: Bias-variance analysis. In Proceedings of the 30th AAAI Conference on
Artiﬁcial Intelligence  pages 1631–1637  2016.

9

[13] Keisuke Hirano  Guido W Imbens  and Geert Ridder. Efﬁcient estimation of average treatment

effects using the estimated propensity score. Econometrica  71(4):1161–1189  2003.

[14] Nan Jiang and Lihong Li. Doubly robust off-policy evaluation for reinforcement learning. In
Proceedings of the 23rd International Conference on Machine Learning (ICML)  pages 652–661 
2016.

[15] Daniel Krajzewicz  Jakob Erdmann  Michael Behrisch  and Laura Bieker. Recent development
and applications of sumo-simulation of urban mobility. International Journal On Advances in
Systems and Measurements  5(3&4)  2012.

[16] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine

Learning Research  4:1107–1149  2003.

[17] David A Levin and Yuval Peres. Markov chains and mixing times  volume 107. American

Mathematical Soc.  2017.

[18] Lihong Li  Shunbao Chen  Ankur Gupta  and Jim Kleban. Counterfactual analysis of click
metrics for search engine optimization: A case study. In Proceedings of the 24th International
World Wide Web Conference (WWW)  Companion Volume  pages 929–934  2015.

[19] Lihong Li  Wei Chu  John Langford  and Xuanhui Wang. Unbiased ofﬂine evaluation of
contextual-bandit-based news article recommendation algorithms. In Proceedings of the 4th
International Conference on Web Search and Data Mining (WSDM)  pages 297–306  2011.

[20] Lihong Li  Rémi Munos  and Csaba Szepesvári. Toward minimax off-policy value estimation.
In Proceedings of the 18th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS)  pages 608–616  2015.

[21] Hao Liu  Yihao Feng  Yi Mao  Dengyong Zhou  Jian Peng  and Qiang Liu. Action-dependent
control variates for policy optimization via stein identity. In Proceedings of the 6th International
Conference on Learning Representations (ICLR)  2018.

[22] Jun S. Liu. Monte Carlo Strategies in Scientiﬁc Computing. Springer Series in Statistics.

Springer-Verlag  2001.

[23] Travis Mandel  Yun-En Liu  Sergey Levine  Emma Brunskill  and Zoran Popovic. Ofﬂine policy
evaluation across representations with applications to educational games. In Proceedings of
the 13th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) 
pages 1077–1084  2014.

[24] Krikamol Muandet  Kenji Fukumizu  Bharath Sriperumbudur  Bernhard Schölkopf  et al. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends R in Machine
Learning  10(1-2):1–141  2017.

[25] Rémi Munos  Tom Stepleton  Anna Harutyunyan  and Marc G. Bellemare. Safe and efﬁcient
off-policy reinforcement learning. In Advances in Neural Information Processing Systems 29
(NIPS)  pages 1046–1054  2016.

[26] Susan A. Murphy  Mark van der Laan  and James M. Robins. Marginal mean models for
dynamic regimes. Journal of the American Statistical Association  96(456):1410–1423  2001.

[27] XuanLong Nguyen  Martin J Wainwright  and Michael Jordan. Estimating divergence function-
als and the likelihood ratio by convex risk minimization. Information Theory  IEEE Transactions
on  56(11):5847–5861  2010.

[28] Art B. Owen. Monte Carlo Theory  Methods and Examples. 2013. http://statweb.

stanford.edu/~owen/mc.

[29] Doina Precup  Richard S. Sutton  and Sanjoy Dasgupta. Off-policy temporal-difference learning
with funtion approximation. In Proceedings of the 18th Conference on Machine Learning
(ICML)  pages 417–424  2001.

10

[30] Doina Precup  Richard S. Sutton  and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the 17th International Conference on Machine Learning (ICML) 
pages 759–766  2000.

[31] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience  New York  1994.

[32] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2001.

[33] Robert J Serﬂing. Approximation theorems of mathematical statistics  volume 162. John Wiley

& Sons  2009.

[34] Alexander L. Strehl  John Langford  Lihong Li  and Sham M. Kakade. Learning from logged
implicit exploration data. In Advances in Neural Information Processing Systems 23 (NIPS-10) 
pages 2217–2225  2010.

[35] Masashi Sugiyama  Taiji Suzuki  and Takafumi Kanamori. Density ratio estimation in machine

learning. Cambridge University Press  2012.

[36] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press 

Cambridge  MA  March 1998.

[37] Richard S. Sutton  A. Rupam Mahmood  and Martha White. An emphatic approach to the
problem of off-policy temporal-difference learning. Journal of Machine Learning Research 
17(73):1–29  2016.

[38] Liang Tang  Romer Rosales  Ajit Singh  and Deepak Agarwal. Automatic ad format selection via
contextual bandits. In Proceedings of the 22nd ACM International Conference on Information
& Knowledge Management (CIKM)  pages 1587–1594  2013.

[39] Philip S. Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for rein-
forcement learning. In Proceedings of the 33rd International Conference on Machine Learning
(ICML)  pages 2139–2148  2016.

[40] Philip S. Thomas  Georgios Theocharous  Mohammad Ghavamzadeh  Ishan Durugkar  and
Emma Brunskill. Predictive off-policy policy evaluation for nonstationary decision problems 
with applications to digital marketing. In Proceedings of the 31st AAAI Conference on Artiﬁcial
Intelligence (AAAI)  pages 4740–4745  2017.

[41] Elise Van der Pol and Frans A Oliehoek. Coordinated deep reinforcement learners for trafﬁc
light control. In NIPS Workshop on Learning  Inference and Control of Multi-Agent Systems 
2016.

[42] Yu-Xiang Wang  Alekh Agarwal  and Miroslav Dudík. Optimal and adaptive off-policy evalua-
tion in contextual bandits. In Proceedings of the 34th International Conference on Machine
Learning (ICML)  pages 3589–3597  2017.

11

,Qiang Liu
Lihong Li
Ziyang Tang
Dengyong Zhou