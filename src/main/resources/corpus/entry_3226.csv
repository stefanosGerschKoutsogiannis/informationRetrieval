2015,Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes,Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However  a well known failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family  LRVB has a simple  analytic form  even for non-conjugate models. Indeed  we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.,Linear Response Methods for Accurate Covariance

Estimates from Mean Field Variational Bayes

Ryan Giordano
UC Berkeley

Tamara Broderick

MIT

Michael Jordan
UC Berkeley

rgiordano@berkeley.edu

tbroderick@csail.mit.edu

jordan@cs.berkeley.edu

Abstract

Mean ﬁeld variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However  a well known ma-
jor failing of MFVB is that it underestimates the uncertainty of model variables
(sometimes severely) and provides no information about model variable covari-
ance. We generalize linear response methods from statistical physics to deliver
accurate uncertainty estimates for model variables—both for individual variables
and coherently across variables. We call our method linear response variational
Bayes (LRVB). When the MFVB posterior approximation is in the exponential
family  LRVB has a simple  analytic form  even for non-conjugate models. In-
deed  we make no assumptions about the form of the true posterior. We demon-
strate the accuracy and scalability of our method on a range of models for both
simulated and real data.

1

Introduction

With increasingly efﬁcient data collection methods  scientists are interested in quickly analyzing
ever larger data sets. In particular  the promise of these large data sets is not simply to ﬁt old models
but instead to learn more nuanced patterns from data than has been possible in the past. In theory 
the Bayesian paradigm yields exactly these desiderata. Hierarchical modeling allows practitioners
to capture complex relationships between variables of interest. Moreover  Bayesian analysis allows
practitioners to quantify the uncertainty in any model estimates—and to do so coherently across all
of the model variables.
Mean ﬁeld variational Bayes (MFVB)  a method for approximating a Bayesian posterior distribu-
tion  has grown in popularity due to its fast runtime on large-scale data sets [1–3]. But a well known
major failing of MFVB is that it gives underestimates of the uncertainty of model variables that
can be arbitrarily bad  even when approximating a simple multivariate Gaussian distribution [4–
6]. Also  MFVB provides no information about how the uncertainties in different model variables
interact [5–8].
By generalizing linear response methods from statistical physics [9–12] to exponential family vari-
ational posteriors  we develop a methodology that augments MFVB to deliver accurate uncertainty
estimates for model variables—both for individual variables and coherently across variables.
In
particular  as we elaborate in Section 2  when the approximating posterior in MFVB is in the expo-
nential family  MFVB deﬁnes a ﬁxed-point equation in the means of the approximating posterior 

1

and our approach yields a covariance estimate by perturbing this ﬁxed point. We call our method
linear response variational Bayes (LRVB).
We provide a simple  intuitive formula for calculating the linear response correction by solving a
linear system based on the MFVB solution (Section 2.2). We show how the sparsity of this system
for many common statistical models may be exploited for scalable computation (Section 2.3). We
demonstrate the wide applicability of LRVB by working through a diverse set of models to show that
the LRVB covariance estimates are nearly identical to those produced by a Markov Chain Monte
Carlo (MCMC) sampler  even when MFVB variance is dramatically underestimated (Section 3).
Finally  we focus in more depth on models for ﬁnite mixtures of multivariate Gaussians (Section 3.3) 
which have historically been a sticking point for MFVB covariance estimates [5  6]. We show that
LRVB can give accurate covariance estimates orders of magnitude faster than MCMC (Section 3.3).
We demonstrate both theoretically and empirically that  for this Gaussian mixture model  LRVB
scales linearly in the number of data points and approximately cubically in the dimension of the
parameter space (Section 3.4).

Previous Work. Linear response methods originated in the statistical physics literature [10–13].
These methods have been applied to ﬁnd new learning algorithms for Boltzmann machines [13] 
covariance estimates for discrete factor graphs [14]  and independent component analysis [15]. [16]
states that linear response methods could be applied to general exponential family models but works
out details only for Boltzmann machines. [10]  which is closest in spirit to the present work  derives
general linear response corrections to variational approximations; indeed  the authors go further to
formulate linear response as the ﬁrst term in a functional Taylor expansion to calculate full pairwise
joint marginals. However  it may not be obvious to the practitioner how to apply the general formulas
of [10]. Our contributions in the present work are (1) the provision of concrete  straightforward
formulas for covariance correction that are fast and easy to compute  (2) demonstrations of the
success of our method on a wide range of new models  and (3) an accompanying suite of code.

2 Linear response covariance estimation

2.1 Variational Inference

Suppose we observe N data points  denoted by the N-long column vector x  and denote our un-
observed model parameters by θ. Here  θ is a column vector residing in some space Θ; it has J
subgroups and total dimension D. Our model is speciﬁed by a distribution of the observed data
given the model parameters—the likelihood p(x|θ)—and a prior distributional belief on the model
parameters p(θ). Bayes’ Theorem yields the posterior p(θ|x).
Mean-ﬁeld variational Bayes (MFVB) approximates p(θ|x) by a factorized distribution of the form
q(θ) =�J
j=1 q(θj). q is chosen so that the Kullback-Liebler divergence KL(q||p) between q and p
is minimized. Equivalently  q is chosen so that E := L + S  for L := Eq[log p(θ|x)] (the expected
log posterior) and S := −Eq[log q(θ)] (the entropy of the variational distribution)  is maximized:
(1)

q∗ := arg min

Eq [log q(θ) − log p(θ|x)] = arg max

E.

KL(q||p) = arg min

q

q

q

Up to a constant in θ  the objective E is sometimes called the “evidence lower bound”  or the ELBO
[5]. In what follows  we further assume that our variational distribution  q (θ)  is in the exponential
family with natural parameter η and log partition function A: log q (θ|η) = ηT θ − A (η) (expressed
with respect to some base measure in θ). We assume that p (θ|x) is expressed with respect to the
same base measure in θ as for q. Below  we will make only mild regularity assumptions about the
true posterior p(θ|x) and no assumptions about its form.
If we assume additionally that the parameters η∗ at the optimum q∗(θ) = q(θ|η∗) are in the interior
of the feasible space  then q(θ|η) may instead be described by the mean parameterization: m := Eqθ

2

with m∗ := Eq∗ θ. Thus  the objective E can be expressed as a function of m  and the ﬁrst-order
condition for the optimality of q∗ becomes the ﬁxed point equation

= m∗ ⇔ M (m∗) = m∗ for M (m) :=

∂E
∂m

+ m. (2)

∂E

∂m����m=m∗

= 0 ⇔ � ∂E

∂m

+ m�����m=m∗

2.2 Linear Response

Let V denote the covariance matrix of θ under the variational distribution q∗(θ)  and let Σ denote
the covariance matrix of θ under the true posterior  p(θ|x):

V := Covq∗ θ 

Σ := Covpθ.

In MFVB  V may be a poor estimator of Σ  even when m∗ ≈ Epθ  i.e.  when the marginal estimated
means match well [5–7]. Our goal is to use the MFVB solution and linear response methods to
construct an improved estimator for Σ. We will focus on the covariance of the natural sufﬁcient
statistic θ  though the covariance of functions of θ can be estimated similarly (see Appendix A).
The essential idea of linear response is to perturb the ﬁrst-order condition M (m∗) = m∗ around its
optimum. In particular  deﬁne the distribution pt (θ|x) as a log-linear perturbation of the posterior:
(3)
where C (t) is a constant in θ. We assume that pt(θ|x) is a well-deﬁned distribution for any t in an
open ball around 0. Since C (t) normalizes pt(θ|x)  it is in fact the cumulant-generating function
of p(θ|x)  so the derivatives of C (t) evaluated at t = 0 give the cumulants of θ. To see why this
perturbation may be useful  recall that the second cumulant of a distribution is the covariance matrix 
our desired estimand:

:= log p (θ|x) + tT θ − C (t)  

log pt (θ|x)

Σ = Covp(θ) =

d

dtT dt

=

C(t)����t=0

d

dtT Ept θ����t=0

.

The practical success of MFVB relies on the fact that its estimates of the mean are often good in
practice. So we assume that m∗t ≈ Ept θ  where m∗t is the mean parameter characterizing q∗t and
q∗t is the MFVB approximation to pt. (We examine this assumption further in Section 3.) Taking
derivatives with respect to t on both sides of this mean approximation and setting t = 0 yields

Σ = Covp(θ) ≈

=: ˆΣ 

(4)

dm∗t

dtT ����t=0

where we call ˆΣ the linear response variational Bayes (LRVB) estimate of the posterior covariance
of θ.
We next show that there exists a simple formula for ˆΣ. Recalling the form of the KL divergence
(see Eq. (1))  we have that −KL(q||pt) = E +tT m =: Et. Then by Eq. (2)  we have m∗t = Mt(m∗t )
for Mt(m) := M (m) + t. It follows from the chain rule that

dm∗t
dt

=

+

∂Mt
∂t

=

dm∗t
dt

+ I 

(5)

where I is the identity matrix. If we assume that we are at a strict local optimum and so can invert
the Hessian of E  then evaluating at t = 0 yields

∂Mt

∂mT����m=m∗t

dm∗t
dt

∂Mt

∂mT����m=m∗t
ˆΣ + I =� ∂2E

3

ˆΣ =

dm∗t

dtT ����t=0

=

∂M
∂m

∂m∂mT + I� ˆΣ + I ⇒ ˆΣ = −� ∂2E

∂m∂mT�−1

 

(6)

where we have used the form for M in Eq. (2). So the LRVB estimator ˆΣ is the negative inverse
Hessian of the optimization objective  E  as a function of the mean parameters. It follows from
Eq. (6) that ˆΣ is both symmetric and positive deﬁnite when the variational distribution q∗ is at least
a local maximum of E.
We can further simplify Eq. (6) by using the exponential family form of the variational approximat-
ing distribution q. For q in exponential family form as above  the negative entropy −S is dual to the
log partition function A [17]  so S = −ηT m + A(η); hence 
∂η − m� dη

dm − η(m) = −η(m).

=� ∂A

∂S
∂m

∂S
∂ηT

dη
dm

+

dS
dm

=

Recall that for exponential families  ∂η(m)/∂m = V −1. So Eq. (6) becomes1

ˆΣ = −� ∂2L

∂m∂mT +

∂2S

∂m∂mT�−1

= −(H − V −1)−1  for H :=

∂2L

∂m∂mT . ⇒

ˆΣ = (I − V H)−1V.

(7)

When the true posterior p(θ|x) is in the exponential family and contains no products of the vari-
ational moment parameters  then H = 0 and ˆΣ = V . In this case  the mean ﬁeld assumption is
correct  and the LRVB and MFVB covariances coincide at the true posterior covariance. Further-
more  even when the variational assumptions fail  as long as certain mean parameters are estimated
exactly  then this formula is also exact for covariances. E.g.  notably  MFVB is well-known to pro-
vide arbitrarily bad estimates of the covariance of a multivariate normal posterior [4–7]  but since
MFVB estimates the means exactly  LRVB estimates the covariance exactly (see Appendix B).

2.3 Scaling the matrix inverse
Eq. (7) requires the inverse of a matrix as large as the parameter dimension of the posterior p(θ|x) 
which may be computationally prohibitive. Suppose we are interested in the covariance of parameter
sub-vector α  and let z denote the remaining parameters: θ = (α  z)T . We can partition Σ =
(Σα  Σαz; Σzα  Σz) . Similar partitions exist for V and H. If we assume a mean-ﬁeld factorization
q(α  z) = q(α)q(z)  then Vαz = 0. (The variational distributions may factor further as well.) We
calculate the Schur complement of ˆΣ in Eq. (7) with respect to its zth component to ﬁnd that

ˆΣα = (Iα − VαHα − VαHαz�Iz − VzHz)−1VzHzα�−1

(8)
Here  Iα and Iz refer to α- and z-sized identity matrices 
In cases where
(Iz − VzHz)−1 can be efﬁciently calculated (e.g.  all the experiments in Section 3; see Fig. (5)
in Appendix D)  Eq. (8) requires only an α-sized inverse.

Vα.
respectively.

3 Experiments

We compare the covariance estimates from LRVB and MFVB in a range of models  including models
both with and without conjugacy 2. We demonstrate the superiority of the LRVB estimate to MFVB
in all models before focusing in on Gaussian mixture models for a more detailed scalability analysis.
For each model  we simulate datasets with a range of parameters. In the graphs  each point represents
the outcome from a single simulation. The horizontal axis is always the result from an MCMC

1For a comparison of this formula with the frequentist “supplemented expectation-maximization” procedure

see Appendix C.

2All the code is available on our Github repository  rgiordan/LinearResponseVariationalBayesNIPS2015 

4

zn|β  τ

indep

∼ N�zn|βxn  τ−1�  
β ∼ N (β|0  σ2
β) 

∼ Poisson (yn| exp(zn))  

indep

yn|zn
τ ∼ Γ(τ|ατ   βτ ).

(9)

procedure  which we take as the ground truth. As discussed in Section 2.2  the accuracy of the
LRVB covariance for a sufﬁcient statistic depends on the approximation m∗t ≈ Ept θ. In the models
to follow  we focus on regimes of moderate dependence where this is a reasonable assumption for
most of the parameters (see Section 3.2 for an exception). Except where explicitly mentioned 
the MFVB means of the parameters of interest coincided well with the MCMC means  so our key
assumption in the LRVB derivations of Section 2 appears to hold.

3.1 Normal-Poisson model

Model. First consider a Poisson generalized linear mixed model  exhibiting non-conjugacy. We
observe Poisson draws yn and a design vector xn  for n = 1  ...  N.
Implicitly below  we will
everywhere condition on the xn  which we consider to be a ﬁxed design matrix. The generative
model is:

For MFVB  we factorize q (β  τ  z) = q (β) q (τ )�N
n=1 q (zn). Inspection reveals that the optimal
q (β) will be Gaussian  and the optimal q (τ ) will be gamma (see Appendix D). Since the optimal
q (zn) does not take a standard exponential family form  we restrict further to Gaussian q (zn). There
are product terms in L (for example  the term Eq [τ ] Eq [β] Eq [zn])  so H �= 0  and the mean ﬁeld
approximation does not hold; we expect LRVB to improve on the MFVB covariance estimate. A
detailed description of how to calculate the LRVB estimate can be found in Appendix D.

Results. We simulated 100 datasets  each with 500 data points and a randomly chosen value for
µ and τ. We drew the design matrix x from a normal distribution and held it ﬁxed throughout. We
β = 10  ατ = 1  and βτ = 1. To get the “ground truth” covariance
set prior hyperparameters σ2
matrix  we took 20000 draws from the posterior with the R MCMCglmm package [18]  which
used a combination of Gibbs and Metropolis Hastings sampling. Our LRVB estimates used the
autodifferentiation software JuMP [19].
Results are shown in Fig. (1). Since τ is high in many of the simulations  z and β are correlated 
and MFVB underestimates the standard deviation of β and τ. LRVB matches the MCMC standard
deviation for all β  and matches for τ in all but the most correlated simulations. When τ gets very
high  the MFVB assumption starts to bias the point estimates of τ  and the LRVB standard deviations
start to differ from MCMC. Even in that case  however  the LRVB standard deviations are much more
accurate than the MFVB estimates  which underestimate the uncertainty dramatically. The ﬁnal plot
shows that LRVB estimates the covariances of z with β  τ  and log τ reasonably well  while MFVB
considers them independent.

Figure 1: Posterior mean and covariance estimates on normal-Poisson simulation data.

3.2 Linear random effects

Model. Next  we consider a simple random slope linear model  with full details in Appendix E. We
observe scalars yn and rn and a vector xn  for n = 1  ...  N. Implicitly below  we will everywhere

5

condition on all the xn and rn  which we consider to be ﬁxed design matrices. In general  each
random effect may appear in multiple observations  and the index k(n) indicates which random
effect  zk  affects which observation  yn. The full generative model is:

yn|β  z  τ

indep

∼ N�yn|βT xn + rnzk(n)  τ−1�  

ν ∼ Γ(ν|αν  βν) 

β ∼ N (β|0  Σβ) 

zk|ν iid∼ N�zk|0  ν−1�  

τ ∼ Γ(τ|ατ   βτ ).

k=1 q (zn). Since this is
a conjugate model  the optimal q will be in the exponential family with no additional assumptions.

We assume the mean-ﬁeld factorization q (β  ν  τ  z) = q (β) q (τ ) q (ν)�K

Results. We simulated 100 datasets of 300 datapoints each and 30 distinct random effects. We
set prior hyperparameters to αν = 2  βν = 2  ατ = 2   βτ = 2  and Σβ = 0.1−1I. Our xn was
2-dimensional. As in Section 3.1  we implemented the variational solution using the autodifferenti-
ation software JuMP [19]. The MCMC ﬁt was performed with using MCMCglmm [18].
Intuitively  when the random effect explanatory variables rn are highly correlated with the ﬁxed
effects xn  then the posteriors for z and β will also be correlated  leading to a violation of the
mean ﬁeld assumption and an underestimated MFVB covariance. In our simulation  we used rn =
x1n + N (0  0.4)  so that rn is correlated with x1n but not x2n. The result  as seen in Fig. (2) 
is that β1 is underestimated by MFVB  but β2 is not. The ν parameter  in contrast  is not well-
estimated by the MFVB approximation in many of the simulations. Since the LRVB depends on the
approximation m∗t ≈ Ept θ  its LRVB covariance is not accurate either (Fig. (2)). However  LRVB
still improves on the MFVB standard deviation.

Figure 2: Posterior mean and covariance estimates on linear random effects simulation data.

3.3 Mixture of normals

Model. Mixture models constitute some of the most popular models for MFVB application [1  2]
and are often used as an example of where MFVB covariance estimates may go awry [5  6]. Thus  we
will consider in detail a Gaussian mixture model (GMM) consisting of a K-component mixture of
P -dimensional multivariate normals with unknown component means  covariances  and weights. In
what follows  the weight πk is the probability of the kth component  µk is the P -dimensional mean
of the kth component  and Λk is the P × P precision matrix of the kth component (so Λ−1
is the
covariance parameter). N is the number of data points  and xn is the nth observed P -dimensional
data point. We employ the standard trick of augmenting the data generating process with the latent
indicator variables znk  for n = 1  ...  N and k = 1  ...  K  such that znk = 1 implies xn ∼
N (µk  Λ−1

k ). So the generative model is:

k

6

P (znk = 1) = πk  p(x|π  µ  Λ  z) = �n=1:N �k=1:K

N (xn|µk  Λ−1

k )znk

(10)

We used diffuse conditionally conjugate priors (see Appendix F for details). We make the variational
n=1 q (zn). We compare the accuracy and

assumption q (µ  π  Λ  z) =�K

k=1 q (µk) q (Λk) q (πk)�N

speed of our estimates to Gibbs sampling on the augmented model (Eq. (10)) using the function
rnmixGibbs from the R package bayesm. We implemented LRVB in C++  making extensive use
of RcppEigen [20]. We evaluate our results both on simulated data and on the MNIST data set [21].

Results. For simulations  we generated N = 10000 data points from K = 2 multivariate normal
components in P = 2 dimensions. MFVB is expected to underestimate the marginal variance of µ 
Λ  and log(π) when the components overlap since that induces correlation in the posteriors due to
the uncertain classiﬁcation of points between the clusters. We check the covariances estimated with
Eq. (7) against a Gibbs sampler  which we treat as the ground truth.3
We performed 198 simulations  each of which had at least 500 effective Gibbs samples in each
variable—calculated with the R tool eﬀectiveSize from the coda package [22]. The ﬁrst three plots
show the diagonal standard deviations  and the third plot shows the off-diagonal covariances. Note
that the off-diagonal covariance plot excludes the MFVB estimates since most of the values are
zero. Fig. (3) shows that the raw MFVB covariance estimates are often quite different from the
Gibbs sampler results  while the LRVB estimates match the Gibbs sampler closely.
For a real-world example  we ﬁt a K = 2 GMM to the N = 12665 instances of handwritten 0s
and 1s in the MNIST data set. We used PCA to reduce the pixel intensities to P = 25 dimensions.
Full details are provided in Appendix G. In this MNIST analysis  the Λ standard deviations were
under-estimated by MFVB but correctly estimated by LRVB (Fig. (3)); the other parameter standard
deviations were estimated correctly by both and are not shown.

Figure 3: Posterior mean and covariance estimates on GMM simulation and MNIST data.

3.4 Scaling experiments

We here explore the computational scaling of LRVB in more depth for the ﬁnite Gaussian mixture
model (Section 3.3). In the terms of Section 2.3  α includes the sufﬁcient statistics from µ  π  and Λ 
and grows as O(KP 2). The sufﬁcient statistics for the variational posterior of µ contain the P -length
vectors µk  for each k  and the (P + 1)P/2 second-order products in the covariance matrix µkµT
k .
Similarly  for each k  the variational posterior of Λ involves the (P + 1)P/2 sufﬁcient statistics
in the symmetric matrix Λk as well as the term log |Λk|. The sufﬁcient statistics for the posterior
of πk are the K terms log πk.4 So  minimally  Eq. (7) will require the inverse of a matrix of size

3The likelihood described in Section 3.3 is symmetric under relabeling. When the component locations
and shapes have a real-life interpretation  the researcher is generally interested in the uncertainty of µ  Λ  and
π for a particular labeling  not the marginal uncertainty over all possible re-labelings. This poses a problem
for standard MCMC methods  and we restrict our simulations to regimes where label switching did not occur
in our Gibbs sampler. The MFVB solution conveniently avoids this problem since the mean ﬁeld assumption
prevents it from representing more than one mode of the joint posterior.

4Since�K
k=1 πk = 1  using K sufﬁcient statistics involves one redundant parameter. However  this does
not violate any of the necessary assumptions for Eq. (7)  and it considerably simpliﬁes the calculations. Note
that though the perturbation argument of Section 2 requires the parameters of p(θ|x) to be in the interior of the
feasible space  it does not require that the parameters of p(x|θ) be interior.

7

O(KP 2). The sufﬁcient statistics for z have dimension K × N. Though the number of parameters
thus grows with the number of data points  Hz = 0 for the multivariate normal (see Appendix F) 
so we can apply Eq. (8) to replace the inverse of an O(KN )-sized matrix with multiplication by
the same matrix. Since a matrix inverse is cubic in the size of the matrix  the worst-case scaling for
LRVB is then O(K 2) in K  O(P 6) in P   and O(N ) in N.
In our simulations (Fig. (4)) we can see that  in practice  LRVB scales linearly5 in N and approxi-
mately cubically in P across the dimensions considered.6 The P scaling is presumably better than
the theoretical worst case of O(P 6) due to extra efﬁciency in the numerical linear algebra. Note that
the vertical axis of the leftmost plot is on the log scale. At all the values of N  K and P considered
here  LRVB was at least as fast as Gibbs sampling and often orders of magnitude faster.

Figure 4: Scaling of LRVB and Gibbs on simulation data in both log and linear scales. Before taking
logs  the line in the two lefthand (N) graphs is y ∝ x  and in the righthand (P) graph  it is y ∝ x3.

4 Conclusion

The lack of accurate covariance estimates from the widely used mean-ﬁeld variational Bayes
(MFVB) methodology has been a longstanding shortcoming of MFVB. We have demonstrated that
in sparse models  our method  linear response variational Bayes (LRVB)  can correct MFVB to de-
liver these covariance estimates in time that scales linearly with the number of data points. Further-
more  we provide an easy-to-use formula for applying LRVB to a wide range of inference problems.
Our experiments on a diverse set of models have demonstrated the efﬁcacy of LRVB  and our de-
tailed study of scaling of mixtures of multivariate Gaussians shows that LRVB can be considerably
faster than traditional MCMC methods. We hope that in future work our results can be extended
to more complex models  including Bayesian nonparametric models  where MFVB has proven its
practical success.

Acknowledgments. The authors thank Alex Blocker for helpful comments. R. Giordano and
T. Broderick were funded by Berkeley Fellowships.

5The Gibbs sampling time was linearly rescaled to the amount of time necessary to achieve 1000 effective
samples in the slowest-mixing component of any parameter. Interestingly  this rescaling leads to increasing
efﬁciency in the Gibbs sampling at low P due to improved mixing  though the beneﬁts cease to accrue at
moderate dimensions.

6For numeric stability we started the optimization procedures for MFVB at the true values  so the time to
compute the optimum in our simulations was very fast and not representative of practice. On real data  the
optimization time will depend on the quality of the starting point. Consequently  the times shown for LRVB
are only the times to compute the LRVB estimate. The optimization times were on the same order.

8

References
[1] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[2] D. M. Blei and M. I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian Analysis 

1(1):121–143  2006.

[3] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. Stochastic variational inference. Journal of Machine

Learning Research  14(1):1303–1347  2013.

[4] D. J. C. MacKay. Information Theory  Inference  and Learning Algorithms. Cambridge University Press 

2003. Chapter 33.

[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  New York  2006. Chapter 10.
[6] R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series

models. In D. Barber  A. T. Cemgil  and S. Chiappa  editors  Bayesian Time Series Models. 2011.

[7] B. Wang and M. Titterington.

Inadequacy of interval estimates corresponding to variational Bayesian

approximations. In Workshop on Artiﬁcial Intelligence and Statistics  pages 373–380  2004.

[8] H. Rue  S. Martino  and N. Chopin. Approximate Bayesian inference for latent Gaussian models by using
integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (statistical
methodology)  71(2):319–392  2009.

[9] G. Parisi. Statistical Field Theory  volume 4. Addison-Wesley New York  1988.
[10] M. Opper and O. Winther. Variational linear response. In Advances in Neural Information Processing

Systems  2003.

[11] M. Opper and D. Saad. Advanced mean ﬁeld methods: Theory and practice. MIT press  2001.
[12] T. Tanaka. Information geometry of mean-ﬁeld approximation. Neural Computation  12(8):1951–1968 

2000.

[13] H. J. Kappen and F. B. Rodriguez. Efﬁcient learning in Boltzmann machines using linear response theory.

Neural Computation  10(5):1137–1156  1998.

[14] M. Welling and Y. W. Teh. Linear response algorithms for approximate inference in graphical models.

Neural Computation  16(1):197–221  2004.

[15] P. A. d. F. R. Højen-Sørensen  O. Winther  and L. K. Hansen. Mean-ﬁeld approaches to independent

component analysis. Neural Computation  14(4):889–918  2002.

[16] T. Tanaka. Mean-ﬁeld theory of Boltzmann machine learning. Physical Review E  58(2):2302  1998.
[17] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends® in Machine Learning  1(1-2):1–305  2008.

[18] J. D. Hadﬁeld. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm

R package. Journal of Statistical Software  33(2):1–22  2010.

[19] M. Lubin and I. Dunning. Computing in operations research using Julia. INFORMS Journal on Comput-

ing  27(2):238–248  2015.

[20] D. Bates and D. Eddelbuettel. Fast and elegant numerical linear algebra using the RcppEigen package.

Journal of Statistical Software  52(5):1–24  2013.

[21] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[22] M. Plummer  N. Best  K. Cowles  and K. Vines. CODA: Convergence diagnosis and output analysis for

MCMC. R News  6(1):7–11  2006.

[23] X. L. Meng and D. B. Rubin. Using EM to obtain asymptotic variance-covariance matrices: The SEM

algorithm. Journal of the American Statistical Association  86(416):899–909  1991.

[24] A. W¨achter and L. T. Biegler. On the implementation of an interior-point ﬁlter line-search algorithm for

large-scale nonlinear programming. Mathematical Programming  106(1):25–57  2006.

9

,Ryan Giordano
Tamara Broderick
Michael Jordan