2014,Active Regression by Stratification,We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification)  in regression the passive learning rate of O(1/epsilon) cannot in general be improved upon. Nonetheless  the so-called `constant' in the rate of convergence  which is characterized by a distribution-dependent risk  can be improved in many cases. For a given distribution  achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration  our active learner approaches a the optimal risk using piecewise constant approximations.,Active Regression by Stratiﬁcation

Sivan Sabato

Department of Computer Science

Ben Gurion University  Beer Sheva  Israel

sabatos@cs.bgu.ac.il

Remi Munos∗

INRIA

Lille  France

remi.munos@inria.fr

Abstract

We propose a new active learning algorithm for parametric linear regression with
random design. We provide ﬁnite sample convergence guarantees for general dis-
tributions in the misspeciﬁed model. This is the ﬁrst active learner for this setting
that provably can improve over passive learning. Unlike other learning settings
(such as classiﬁcation)  in regression the passive learning rate of O(1/) cannot
in general be improved upon. Nonetheless  the so-called ‘constant’ in the rate
of convergence  which is characterized by a distribution-dependent risk  can be
improved in many cases. For a given distribution  achieving the optimal risk re-
quires prior knowledge of the distribution. Following the stratiﬁcation technique
advocated in Monte-Carlo function integration  our active learner approaches the
optimal risk using piecewise constant approximations.

1

Introduction

In linear regression  the goal is to predict the real-valued labels of data points in Euclidean space
using a linear function. The quality of the predictor is measured by the expected squared error of
its predictions. In the standard regression setting with random design  the input is a labeled sample
drawn i.i.d. from the joint distribution of data points and labels  and the cost of data is measured by
the size of the sample. This model  which we refer to here as passive learning  is useful when both
data and labels are costly to obtain. However  in domains where raw data is very cheap to obtain  a
more suitable model is that of active learning (see  e.g.  Cohn et al.  1994). In this model we assume
that random data points are essentially free to obtain  and the learner can choose  for any observed
data point  whether to ask also for its label. The cost of data here is the total number of requested
labels.
In this work we propose a new active learning algorithm for linear regression. We provide ﬁnite
sample convergence guarantees for general distributions  under a possibly misspeciﬁed model. For
parametric linear regression  the sample complexity of passive learning as a function of the excess
error  is of the order O(1/). This rate cannot in general be improved by active learning  unlike
in the case of classiﬁcation (Balcan et al.  2009). Nonetheless  the so-called ‘constant’ in this rate
of convergence depends on the distribution  and this is where the potential improvement by active
learning lies.
Finite sample convergence of parametric linear regression in the passive setting has been studied by
several (see  e.g.  Gy¨orﬁ et al.  2002; Hsu et al.  2012). The standard approach is Ordinary Least
Squares (OLS)  where the output predictor is simply the minimizer of the mean squared error on the
sample. Recently  a new algorithm for linear regression has been proposed (Hsu and Sabato  2014).
This algorithm obtains an improved convergence guarantee under less restrictive assumptions. An
appealing property of this guarantee is that it provides a direct and tight relationship between the
point-wise error of the optimal predictor and the convergence rate of the predictor. We exploit this to

∗Current Afﬁliation: Google DeepMind.

1

allow our active learner to adapt to the underlying distribution. Our approach employs a stratiﬁcation
technique  common in Monte-Carlo function integration (see  e.g.  Glasserman  2004). For any ﬁnite
partition of the data domain  an optimal oracle risk can be deﬁned  and the convergence rate of our
active learner approaches the rate deﬁned by this risk. By constructing an inﬁnite sequence of
partitions that become increasingly reﬁned  one can approach the globally optimal oracle risk.
Active learning for parametric regression has been investigated in several works  some of them in
the context of statistical experimental design. One of the earliest works is Cohn et al. (1996)  which
proposes an active learning algorithm for locally weighted regression  assuming a well-speciﬁed
model and an unbiased learning function. Wiens (1998  2000) calculates a minimax optimal de-
sign for regression given the marginal data distribution  assuming that the model is approximately
well-speciﬁed. Kanamori (2002) and Kanamori and Shimodaira (2003) propose an active learning
algorithm that ﬁrst calculates a maximum likelihood estimator and then uses this estimator to come
up with an optimal design. Asymptotic convergence rates are provided under asymptotic normal-
ity assumptions. Sugiyama (2006) assumes an approximately well-speciﬁed model and i.i.d. label
noise  and selects a design from a ﬁnite set of possibilities. The approach is adapted to pool-based
active learning by Sugiyama and Nakajima (2009). Burbidge et al. (2007) propose an adaptation
of Query By Committee. Cai et al. (2013) propose guessing the potential of an example to change
the current model. Ganti and Gray (2012) propose a consistent pool-based active learner for the
squared loss. A different line of research  which we do not discuss here  focuses on active learning
for non-parameteric regression  e.g. Efromovich (2007).
Outline In Section 2 the formal setting and preliminaries are introduced. In Section 3 the notion of
an oracle risk for a given distribution is presented. The stratiﬁcation technique is detailed in Section
4. The new active learner algorithm and its analysis are provided in Section 5  with the main result
stated in Theorem 5.1. In Section 6 we show via a simple example that in some cases the active
learner approaches the maximal possible improvement over passive learning.

2 Setting and Preliminaries

We assume a data space in Rd and labels in R. For a distribution P over Rd × R  denote by
suppX (P ) the support of the marginal of P over Rd. Denote the strictly positive reals by R∗
+.
We assume that labeled examples are distributed according to a distribution D. A random labeled
example is (X  Y ) ∼ D  where X ∈ Rd is the example and Y ∈ R is the label. Throughout this
work  whenever P[·] or E[·] appear without a subscript  they are taken with respect to D. DX is
the marginal distribution of X in pairs draws from D. The conditional distribution of Y when the
example is X = x is denoted DY |x. The function x (cid:55)→ DY |x is denoted DY |X.
A predictor is a function from Rd to R that predicts a label for every possible example. Linear
predictors are functions of the form x (cid:55)→ x(cid:62)w for some w ∈ Rd. The squared loss of w ∈ Rd
for an example x ∈ Rd with a true label y ∈ R is (cid:96)((x  y)  w) = (x(cid:62)w − y)2. The expected
squared loss of w with respect to D is L(w  D) = E(X Y )∼D[(X(cid:62)w − Y )2]. The goal of the
learner is to ﬁnd a w such that L(w) is small. The optimal loss achievable by a linear predictor is
L(cid:63)(D) = minw∈Rd L(w  D). We denote by w(cid:63)(D) a minimizer of L(w  D) such that L(cid:63)(D) =
L(w(cid:63)(D)  D). In all these notations the parameter D is dropped when clear from context.
In the passive learning setting  the learner draws random i.i.d. pairs (X  Y ) ∼ D. The sample
complexity of the learner is the number of drawn pairs. In the active learning setting  the learner
draws i.i.d. examples X ∼ DX. For any drawn example  the learner may draw a label according to
the distribution DY |X. The label complexity of the learner is the number of drawn labels. In this
setting it is easy to approximate various properties of DX to any accuracy  with zero label cost. Thus
we assume for simplicity direct access to some properties of DX  such as the covariance matrix of
DX  denoted ΣD = EX∼DX [XX(cid:62)]  and expectations of some other functions of X. We assume
w.l.o.g. that ΣD is not singular. For a matrix A ∈ Rd×d  and x ∈ Rd  denote (cid:107)x(cid:107)A =
x(cid:62)Ax. Let
D = maxx∈suppX (D) (cid:107)x(cid:107)2
. This is the condition number of the marginal distribution DX. We
R2
have

√

−1
D

Σ

E[(cid:107)X(cid:107)2

−1
D

Σ

] = E[tr(X(cid:62)Σ−1

D X)] = tr(Σ−1

D

E[XX(cid:62)]) = d.

(1)

2

Hsu and Sabato (2014) provide a passive learning algorithm for least squares linear regression with a
minimax optimal sample complexity (up to logarithmic factors). The algorithm is based on splitting
the labeled sample into several subsamples  performing OLS on each of the subsamples  and then
choosing one of the resulting predictors via a generalized median procedure. We give here a useful
version of the result.1
Theorem 2.1 (Hsu and Sabato  2014). There are universal constants C  c  c(cid:48)  c(cid:48)(cid:48) > 0 such that the
following holds. Let D be a distribution over Rd×R. There exists an efﬁcient algorithm that accepts
as input a conﬁdence δ ∈ (0  1) and a labeled sample of size n drawn i.i.d. from D  and returns
ˆw ∈ Rd  such that if n ≥ cR2

D log(c(cid:48)n) log(c(cid:48)(cid:48)/δ)  with probability 1 − δ 

L( ˆw  D) − L(cid:63)(D) = (cid:107)w(cid:63)(D) − ˆw(cid:107)2

ΣD

≤ C log(1/δ)

n

· ED[(cid:107)X(cid:107)2

−1
D

Σ

(Y − X(cid:62)w(cid:63)(D))2].

(2)

This result is particularly useful in the context of active learning  since it provides an explicit de-
pendence on the point-wise errors of the labels  including in heteroscedastic settings  where this
error is not uniform. As we see below  in such cases active learning can potentially gain over passive
learning. We denote an execution of the algorithm on a labeled sample S by ˆw ← REG(S  δ). The al-
gorithm is used a black box  thus any other algorithm with similar guarantees could be used instead.
For instance  similar guarantees might hold for OLS for a more restricted class of distributions.
Throughout the analysis we omit for readability details of integer rounding  whenever the effects are
negligible. We use the notation O(exp)  where exp is a mathematical expression  as a short hand
for ¯c · exp + ¯C for some universal constants ¯c  ¯C ≥ 0  whose values can vary between statements.

3 An Oracle Bound for Active Regression

The bound in Theorem 2.1 crucially depends on the input distribution D.
In an active learning
framework  rejection sampling (Von Neumann  1951) can be used to simulate random draws of
labeled examples according to a different distribution  without additional label costs. By selecting a
suitable distribution  it might be possible to improve over Eq. (2). Rejection sampling for regression
has been explored in Kanamori (2002); Kanamori and Shimodaira (2003); Sugiyama (2006) and
others  mostly in an asymptotic regime. Here we use the explicit bound in Eq. (2) to obtain new
ﬁnite sample guarantees that hold for general distributions.
Let φ : Rd → R∗
+ be a strictly positive weight function such that E[φ(X)] = 1. We deﬁne the
distribution Pφ over Rd × R as follows: For x ∈ Rd  y ∈ R  let Γφ(x  y) = {(˜x  ˜y) ∈ Rd × R | x =
˜x√
}  and deﬁne Pφ by

  y = ˜y√

φ(˜x)

φ(˜x)

∀(X  Y ) ∈ Rd × R 

Pφ(X  Y ) =

( ˜X  ˜Y )∈Γφ(X Y )

φ( ˜X)dD( ˜X  ˜Y ).

A labeled i.i.d. sample drawn according to Pφ can be simulated using rejection sampling without
additional label costs (see Alg. 2 in Appendix B). We denote drawing m random labeled examples
according to P by S ← SAMPLE(P  m). For the squared loss on Pφ we have

(cid:90)

(cid:90)
˜Y(cid:113)

(cid:90)
(cid:90)
(cid:90)
(cid:90)

L(w  Pφ) =

(∗)
=

=

=

(cid:96)((X  Y )  w) dPφ(X  Y )

(cid:96)((X  Y )  w)

φ( ˜X) dD( ˜X  ˜Y )

( ˜X  ˜Y )∈Γφ(X Y )

(X Y )∈Rd

(X Y )∈Rd

˜X(cid:113)

(cid:96)((

 

)  w) φ( ˜X) dD( ˜X  ˜Y )

( ˜X  ˜Y )∈Rd

φ( ˜X)

φ( ˜X)

(cid:96)((X  Y )  w) dD(X  Y ) = L(w  D).

(X Y )∈Rd

The equality (∗) can be rigorously derived from the deﬁnition of Lebesgue integration. It follows
that also L(cid:63)(D) = L(cid:63)(Pφ) and that w(cid:63)(D) = w(cid:63)(Pφ). We thus denote these by L(cid:63) and w(cid:63). In

1This is a slight variation of the original result of Hsu and Sabato (2014)  see Appendix A.

3

a similar manner  we have ΣPφ =(cid:82) XX(cid:62) dPφ(X  Y ) =(cid:82) XX(cid:62) dD(X  Y ) = ΣD. From now on

we denote this matrix simply Σ. We denote (cid:107) · (cid:107)Σ by (cid:107) · (cid:107)  and (cid:107) · (cid:107)Σ−1 by (cid:107) · (cid:107)∗. The condition
number of Pφ is R2
Pφ
If the regression algorithm is applied to n labeled examples drawn from the simulated Pφ  then by
Eq. (2) and the equalities above  with probability 1 − δ  if n ≥ cR2

log(c(cid:48)n) log(c(cid:48)(cid:48)/δ)) 

= maxx∈suppX (D)

(cid:107)x(cid:107)2∗
φ(x) .

Pφ

L( ˆw) − L(cid:63) ≤ C · log(1/δ)
C · log(1/δ)

n

=

n

· EPφ[(cid:107)X(cid:107)2∗(X(cid:62)w(cid:63) − Y )2]
· ED[(cid:107)X(cid:107)2∗(X(cid:62)w(cid:63) − Y )2/φ(X)].

Denote ψ2(x) := (cid:107)x(cid:107)2∗ · ED[(X(cid:62)w(cid:63) − Y )2 | X = x]. Further denote ρ(φ) := ED[ψ2(X)/φ(X)] 
which we term the risk of φ. Then  if n ≥ cR2

log(c(cid:48)n) log(c(cid:48)(cid:48)/δ)  with probability 1 − δ 

Pφ

L( ˆw) − L(cid:63) ≤ C · ρ(φ) log(1/δ)

.

(3)
A passive learner essentially uses the default φ  which is constantly 1  for a risk of ρ(1) = E[ψ2(X)].
But the φ that minimizes the bound is the solution to the following minimization problem:

n

Minimizeφ
subject to

E[ψ2(X)/φ(X)]
E[φ(X)] = 1 
φ(x) ≥ c log(c(cid:48)n) log(c(cid:48)(cid:48)/δ)

n

(cid:107)x(cid:107)2∗ 

∀x ∈ suppX (D).

(4)

Pφ

log(c(cid:48)n) log(c(cid:48)(cid:48)/δ). The following lemma

The second constraint is due to the requirement n ≥ cR2
bounds the risk of the optimal φ. Its proof is provided in Appendix C.
Lemma 3.1. Let φ(cid:63) be the solution to the minimization problem in Eq. (4). Then for n ≥
O(d log(d) log(1/δ))  E2[ψ(X)] ≤ ρ(φ(cid:63)) ≤ E2[ψ(X)](1 + O(d log(n) log(1/δ)/n)).
The ratio between the risk of φ(cid:63) and the risk of the default φ thus approaches E[ψ2(X)]/E2[ψ(X)] 
and this is also the optimal factor of label complexity reduction. The ratio is 1 for highly symmetric
distributions  where the support of DX is on a sphere and all the noise variances are identical. In
these cases  active learning is not helpful  even asymptotically. However  in the general case  this
ratio is unbounded  and so is the potential for improvement from using active learning. The crucial
challenge is that without access to the conditional distribution DY |X  Eq. (4) cannot be solved
directly. We consider the oracle risk ρ(cid:63) = E2[ψ(X)]  which can be approached if an oracle divulges
the optimal φ and n → ∞. The goal of the active learner is to approach the oracle guarantee without
prior knowledge of DY |X.

4 Approaching the Oracle Bound with Strata

To approximate the oracle guarantee  we borrow the stratiﬁcation approach used in Monte-Carlo
function integration (e.g.  Glasserman  2004). Partition suppX (D) into K disjoint subsets A =
{A1  . . .   AK}  and consider for φ only functions that are constant on each Ai and such that
E[φ(X)] = 1. Each of the functions in this class can be described by a vector a = (a1  . . .   aK) ∈
  where pj := P[X ∈ Aj]. Let φa denote
(R∗
a function deﬁned by a  leaving the dependence on the partition A implicit. To calculate the risk of
φa  denote µi := E[(cid:107)X(cid:107)2∗(X(cid:62)w(cid:63) − Y )2 | X ∈ Ai]. From the deﬁnition of ρ(φ) 

ai(cid:80)
+)K. The value of the function on x ∈ Ai is
(cid:88)

(cid:88)

j∈[K] pj aj

ρ(φa) =
j∈[K]
√

pjaj

i∈[K]

µi minimizes ρ(φa)  and
√

ρ(φa) = ρ(φa(cid:63) ) = (

pi

(5)

(6)

µi)2.

pi
ai

µi.

(cid:88)

i∈[K]

ρ(cid:63)A := inf
a∈RK

+

It is easy to verify that a(cid:63) such that a(cid:63)

i =

4

ρ(φ1) =(cid:80)

ρ(cid:63)A is the oracle risk for the ﬁxed partition A. In comparison  the standard passive learner has risk
i∈[K] piµi. Thus  the ratio between the optimal risk and the default risk can be as large as
1/ mini pi. Note that here  as in the deﬁnition of ρ(cid:63) above  ρ(cid:63)A might not be achievable for samples
up to a certain size  because of the additional requirement that φ not be too small (see Eq. (4)).
Nonetheless  this optimistic value is useful as a comparison.
Consider an inﬁnite sequence of partitions: for j ∈ N  Aj = {Aj
}  with Kj → ∞.
Similarly to Carpentier and Munos (2012)  under mild regularity assumptions  if the partitions have
diameters and probabilities that approach zero  then ρ(cid:63)Aj → ρ(φ(cid:63))  achieving the optimal upper
bound for Eq. (3). For a ﬁxed partition A  the challenge is then to approach ρ∗
A without prior
knowledge of the true µi’s  using relatively few extra labeled examples.
In the next section we
describe our active learning algorithm that does just that.

1  . . .   Aj
Kj

5 Active Learning for Regression
A  we need a good estimate of µi for i ∈ [K]. Note that µi depends on
To approach the optimal risk ρ∗
the optimal predictor w(cid:63)  therefore its value depends on the entire distribution. We assume that the
error of the label relative to the optimal predictor is bounded as follows: There exists a b ≥ 0 such
that (x(cid:62)w(cid:63) − y)2 ≤ b2(cid:107)x(cid:107)2∗ for all (x  y) in the support of D. This boundedness assumption can be
replaced by an assumption on sub-Gaussian tails with similar results. Our assumption implies also
L(cid:63) = E[(x(cid:62)w(cid:63) − y)2] ≤ b2E[(cid:107)X(cid:107)2∗] = b2d  where the last equality follows from Eq. (1).
Algorithm 1 Active Regression
input Conﬁdence δ ∈ (0  1)  label budget m  partition A.
output ˆw ∈ Rd
1: m1 ← m4/5/2  m2 ← m4/5/2  m3 ← m − (m1 + m2).
2: δ1 ← δ/4  δ2 ← δ/4  δ3 ← δ/2.
3: S1 ← SAMPLE(Pφ[Σ]  m1)
4: ˆv ← REG(S1  δ1)

; γ ← (b + 2∆)2(cid:112)K log(2K/δ2)/m2;

5: ∆ ←(cid:113) Cd2b2 log(1/δ1)
˜µi ← Θi ·(cid:16) 1
m1
(cid:80)
6: for i = 1 to K do
Ti ← SAMPLE(Qi  t).
7:
8:
ˆai ← √
9:
10: end for
11: ξ ← c log(c(cid:48)m3) log(c(cid:48)(cid:48)/δ3)
12: Set ˆφ such that for x ∈ Ai  ˆφ(x) := (cid:107)x(cid:107)2∗ · ξ + (1 − dξ)
13: S3 ← SAMPLE(P ˆφ  m3).
14: ˆw ← REG(S3  δ3).

(|x(cid:62) ˆv − y| + ∆)2 + γ

ˆai(cid:80)

j pj ˆaj

.

(x y)∈Ti

(cid:17)

.

m3

t

˜µi.

t ← m2/K.

Our active regression algorithm  listed in Alg. 1  operates in three stages. In the ﬁrst stage  the goal is
to ﬁnd a crude loss optimizer ˆv  so as to later estimate µi. To ﬁnd this optimizer  the algorithm draws
d(cid:107)x(cid:107)2∗.
a labeled sample of size m1 from the distribution Pφ[Σ]  where φ[Σ](x) := 1
Note that ρ(φ[Σ]) = d · E[(Xw(cid:63) − Y )2] = dL(cid:63). In addition  R2
= d. Consequently  by Eq. (3) 
applying REG to m1 ≥ O(d log(d) log(1/δ1)) random draws from Pφ[Σ] gets  with probability 1−δ1

d x(cid:62)Σ−1x = 1

Pφ[Σ]

L(ˆv) − L(cid:63) = (cid:107)ˆv − w(cid:63)(cid:107)2 ≤ CdL(cid:63) log(1/δ1)

≤ Cd2b2 log(1/δ1)

.

(7)

In Needell et al. (2013) a similar distribution is used to speed up gradient descent for convex losses.
Here  we make use of φ[Σ] as a stepping stone in order to approach the optimal φ at a rate that does
not depend on the condition number of D. Denote by E the event that Eq. (7) holds.
In the second stage  estimates for µi  denoted ˜µi  are calculated from labeled samples that are drawn
from another set of probability distributions  Qi for i ∈ [K]. These distributions are deﬁned as
follows. Denote Θi = E[(cid:107)X(cid:107)4∗ | X ∈ Ai]. For x ∈ Rd  y ∈ R  let Γi(x  y) = {(˜x  ˜y) ∈ Ai ×

m1

m1

5

(cid:82)

Θi

R | x = ˜x(cid:107)˜x(cid:107)∗   y = ˜y(cid:107)˜x(cid:107)∗}  and deﬁne Qi by dQi(X  Y ) = 1
( ˜X  ˜Y )∈Γi(X Y ) (cid:107) ˜X(cid:107)4∗ dD( ˜X  ˜Y ).
Clearly  for all x ∈ suppX (Qi)  (cid:107)x(cid:107)∗ = 1. Drawing labeled examples from Qi can be done using
rejection sampling  similarly to Pφ. The use of the Qi distributions in the second stage again helps
avoid a dependence on the condition number of D in the convergence rates.
In the last stage  a weight function ˆφ is determined based on the estimated ˜µi. A labeled sample is
drawn from P ˆφ  and the algorithm returns the predictor resulting from running REG on this sample.
The following theorem gives our main result  a ﬁnite sample convergence rate guarantee.
Theorem 5.1. Let b ≥ 0 such that (x(cid:62)w(cid:63) − y)2 ≤ b2(cid:107)x(cid:107)2∗ for all (x  y) in the support of D. Let
ΛD = E[(cid:107)X(cid:107)4∗]. If Alg. 1 is executed with δ and m such that m ≥ O(d log(d) log(1/δ))5/4  then it
draws m labels  and with probability 1 − δ 
(cid:33)
(cid:32)
L( ˆw) − L(cid:63) ≤ Cρ(cid:63)A log(3/δ)
m
d1/2Λ1/4

D K 1/4 log1/4(K/δ) log(1/δ)

D log5/4(1/δ)

log(1/δ)

dΛ1/2

+

O

ρ(cid:63)A +

m6/5

m6/5

b1/2ρ(cid:63)A3/4 +

m6/5

bρ(cid:63)A1/2

.

The theorem shows that the learning rate of the active learner approaches the oracle rate for the given
partition. With an inﬁnite sequence of partitions with K an increasing function of m  the optimal
oracle risk can also be approached. The rate of convergence to the oracle rate does not depend on the
condition number of D  unlike the passive learning rate. In addition  m = O(d log(d) log(1/δ))5/4
sufﬁces to approach the optimal rate  whereas m = Ω(d) is obviously necessary for any learner. It
is interesting that also in active learning for classiﬁcation  it has been observed that active learning
in a non-realizable setting requires a super-linear dependence on d (See  e.g.  Dasgupta et al.  2008).
Whether this dependence is unavoidable for active regression is an open question. Theorem 5.1 is
be proved via a series of lemmas. First  we show that if ˜µi is a good approximation of µi then ρA( ˆφ)
can be bounded as a function of the oracle risk for A.
Lemma 5.2. Suppose m3 ≥ O(d log(d) log(1/δ3))  and let ˆφ as in Alg. 1. If  for some α  β ≥ 0 

µi ≤ ˜µi ≤ µi + αi

µi + βi 

√

(cid:88)

(8)

piβi)1/2ρ(cid:63)A1/2).

(cid:88)

i

i

piαi)1/2ρ(cid:63)A3/4 + (

Proof. We have ∀x ∈ Ai  ˆφ(x) ≥ (1 − dξ)
ρ( ˆφ) ≡ E[ψ2(X)/ ˆφ(X)] ≤ 1

then
ρA( ˆφ) ≤ (1 + O(d log(m3) log(1/δ3)/m3))(ρ(cid:63)A + (
(cid:88)
(cid:88)
piµi/ˆai = (1 +
1−dξ ≤ 2dξ. It follows
ρ( ˆφ) ≤ (1 + O(d log(m3) log(1/δ3)/m3))ρ(φˆa).

For m3 ≥ O(d log(d) log(1/δ3))  dξ ≤ 1

ˆai(cid:80)
(cid:88)
(cid:88)

2 2 therefore dξ

1 − dξ

1 − dξ

pj ˆaj

pj ˆaj

j pj ˆaj

m3

=

1

j

j

i

i

  where ξ = c log(c(cid:48)m3) log(c(cid:48)(cid:48)/δ)

. Therefore

pi · E[ψ2(X)/ ˆai | X ∈ Ai]

dξ
1 − dξ

)ρ(φˆa).

(9)

By Eq. (8) 

ρA(φˆa) =

(cid:88)
≤(cid:88)
(cid:88)

j

j

= (

pj

√
pj(
√

pi

i

= ρ(cid:63)A + (

(cid:88)

(cid:112)˜µj

i

µj +

√

piµi/(cid:112)˜µi
j +(cid:112)βj)
(cid:88)

αjµ1/4
√

pj

αjµ1/4

j

)(

(cid:88)

µi)2 + (
√

j

pj

αjµ1/4

j

)ρ(cid:63)A1/2 + (

pj

(cid:88)
(cid:88)
(cid:88)

i

i

√

pi

µi
√

(cid:88)
(cid:112)βj)ρ(cid:63)A1/2.

µi) + (

j

pi

(cid:112)βj)(

(cid:88)

pj

i

√

pi

µi).

2Using the fact that m ≥ O(d log(d) log(1/δ3)) implies m ≥ O(d log(m) log(1/δ3)).

j

j

6

The last equality is since ρ(cid:63)A = ((cid:80)
((cid:80)
i piαi)1/2ρ(cid:63)A3/4. By Jensen’s inequality (cid:80)

i pi

√

and Eq. (9)  the lemma directly follows.

µi)2. By Cauchy-Schwartz  ((cid:80)

(cid:112)βj ≤ ((cid:80)

) ≤
j pjβj)1/2. Combined with Eq. (6)

αjµ1/4

j pj

j

j pj

√

We now show that Eq. (8) holds and provide explicit values for α and β. Deﬁne

νi := Θi · EQi[(|X(cid:62) ˆw − Y | + ∆)2] 

and

ˆνi :=

Θi
t

(|x(cid:62) ˆw − y| + ∆)2.

(cid:88)

(x y)∈Ti

Note that ˜µi = ˆνi + Θiγ. We will relate ˆνi to νi  and then νi to µi  to conclude a bound of the
form in Eq. (8) for ˜µi. First  note that if m1 ≥ O(d log(d) log(1/δ1) and E holds  then for any
x ∈ ∪i∈[K]suppX (Qi) 

|x(cid:62) ˆv − x(cid:62)w(cid:63)| ≤ (cid:107)x(cid:107)∗(cid:107)ˆv − w(cid:63)(cid:107) ≤

(10)
The second inequality stems from (cid:107)x(cid:107)∗ = 1 for x ∈ ∪i∈[K]suppX (Qi)  and Eq. (7). This is useful
in the following lemma  which relates ˆνi with νi.
Lemma 5.3. Suppose that m1 ≥ O(d log(d) log(1/δ1)) and E holds. Then with probability 1 − δ2

over the draw of T1  . . .   TK  for all i ∈ [K]  |ˆνi − νi| ≤ Θi(b + 2∆)2(cid:112)K log(2K/δ2)/m2 ≡ Θiγ.

Cd2b2 log(1/δ1)

≡ ∆.

m1

(cid:115)

Proof. For a ﬁxed ˆv  ˆνi/Θi is the empirical average of i.i.d. samples of the random variable Z =
(|X(cid:62) ˆv − Y | + ∆)2  where (X  Y ) is drawn according to Qi. We now give an upper bound for Z
with probability 1. Let ( ˜X  ˜Y ) in the support of D such that X = ˜X/(cid:107) ˜X(cid:107)∗ and Y = ˜Y /(cid:107) ˜X(cid:107)∗.
Then |X(cid:62)w(cid:63) − Y | = | ˜X(cid:62)w(cid:63) − ˜Y |/(cid:107) ˜X(cid:107)∗ ≤ b. If E holds and m1 ≥ O(d log(d) log(1/δ1)) 

Z ≤ (|X(cid:62) ˆv − X(cid:62)w(cid:63)| + |X(cid:62)w(cid:63) − Y | + ∆)2 ≤ (b + 2∆)2 

bility 1 − δ2  |ˆνi − νi| ≤ Θi(b + 2∆)2(cid:112)log(2/δ2)/t. The statement of the lemma follows from a

where the last inequality follows from Eq. (10). By Hoeffding’s inequality  for every i  with proba-
union bound over i ∈ [K] and t = m2/K.

The following lemma  proved in Appendix D  provides the desired relationship between νi and µi.
√
Lemma 5.4. If m1 ≥ O(d log(d) log(1/δ1)) and E holds  then µi ≤ νi ≤ µi +4∆
Θiµi +4∆2Θi.
We are now ready to prove Theorem 5.1.

Proof of Theorem 5.1. From the condition on m and the deﬁnition of m1  m3 in Alg. 1 we have
m1 ≥ O(d log(d/δ1)) and m3 ≥ O(d log(d/δ3)). Therefore the inequalities in Lemma 5.4  Lemma
5.3 and Eq. (3) (with n  δ  φ substituted with m3  δ3  ˆφ) hold simultaneously with probability 1 −
δ1 − δ2 − δ3. For Eq. (3)  note that (cid:107)x(cid:107)∗
Combining Lemma 5.4 and Lemma 5.3  and noting that ˜µi = ˆνi + Θiγ  we conclude that

log(c(cid:48)n) log(c(cid:48)(cid:48)/δ3) as required.

≥ ξ  thus m3 ≥ cR2

ˆφ(x)

P ˆφ

µi ≤ ˜µi ≤ µi + 4∆(cid:112)Θiµi + Θi(4∆2 + 2γ).
(cid:88)
(cid:88)
(cid:112)
i∈[K]
The last inequality follows since(cid:80)
≤ ρ(cid:63)A + 2∆1/2Λ1/4
D ρ(cid:63)A3/4 +

By Lemma 5.2  it follows that
ρA( ˆφ) ≤ ρ(cid:63)A + 2

4∆2 + 2γ · Λ1/2

4∆2 + 2γ · (

Θi)1/2ρ(cid:63)A3/4 +

(cid:112)

(cid:112)

i∈[K]

√

∆(

pi

appear in the other terms of the bound. Combining this with Eq. (3) 

piΘi)1/2ρ(cid:63)A1/2 + ¯O(

log(m3)

m3

)

D ρ(cid:63)A1/2 + ¯O(log(m3)/m3).

i∈[K] piΘi = ΛD. We use ¯O to absorb parameters that already

L( ˆw) − L(cid:63) ≤ Cρ(cid:63)A log(1/δ3)

(cid:16)

+

D ρ(cid:63)A3/4 + (2∆ +(cid:112)2γ) · Λ1/2

D ρ(cid:63)A1/2(cid:17)

m3

2∆1/2Λ1/4

C log(1/δ3)

m3

+ ¯O(

log(m3)

m2
3

).

7

√
∆ ≤ b

We have γ = (b+2∆)2(cid:112)K log(2K/δ2)/m2  and ∆ =

√

(cid:113) Cd2b2 log(1/δ1)
(cid:19)1/4
(cid:18) 16Cd2b2 log(1/δ1)

d + 1)2(cid:112)K log(2K/δ2)/m2. Substituting for ∆ and γ  we have
(cid:32)(cid:18) 4Cd2b2 log(1/δ1)

Λ1/4
D ρ(cid:63)A3/4

C log(1/δ3)

m3

m1

m3

m1

+

L( ˆw) − L(cid:63) ≤ Cρ(cid:63)A log(1/δ3)

d  thus γ ≤ b2(2

. For m1 ≥ Cd log(1/δ1) 

+

C log(1/δ3)

m3

√

+

2b(2

m1
√

d + 1)

(cid:19)1/2
(cid:18) K log(2K/δ2)

m2

(cid:19)1/4(cid:33)

· Λ1/2

D ρ(cid:63)A1/2 + ¯O(

log(m3)

m2
3

).

To get the theorem  set m3 = m − m4/5  m2 = m1 = m4/5/2  δ1 = δ2 = δ/4  and δ3 = δ/2.

6

Improvement over Passive Learning

2

  p = 1

(cid:113) 1−pα2

2α2   and η ∈ R such that |η| ≤ σ

Theorem 5.1 shows that our active learner approaches the oracle rate  which can be strictly faster than
the rate implied by Theorem 2.1 for passive learning. To complete the picture  observe that this better
rate cannot be achieved by any passive learner. This can be seen by the following 1-dimensional
α. Let Dη over R × R such
example. Let σ > 0  α > 1√
that with probability p  X = α and Y = αη +   where  ∼ N (0  σ2)  and with probability 1 − p 
1−p and Y = 0. Then E[X 2] = 1 and w(cid:63) = pα2η. Consider a partition of R such
X = β :=
that α ∈ A1 and β ∈ A2. Then p1 = p  µ1 = E[α2( + αη− αw(cid:63))2] = α2(σ2 + α2η2(1− pα2)) ≤
2 α2σ2. In addition  p2 = 1 − p and µ2 = β4w2
4(1−p)2 . The oracle risk is
)2 ≤ 2pσ2.

1−p )2p2α4η2 ≤ p2α2σ2

(cid:63) = ( 1−pα2
pασ

ασ + (1 − p)

(cid:114) 3

(cid:114) 3

)2 = p2α2σ2(

µ2)2 ≤ (p

µ1 + p2

√

√

ρ(cid:63)A = (p1

3

2

2(1 − p)

1
2

+

2

Therefore  for the active learner  with probability 1 − δ 

L( ˆw) − L(cid:63) ≤ 2Cpσ2 log(1/δ)

m

+ o(

1
m

).

(11)

In contrast  consider any passive learner that receives m labeled examples and outputs a predictor
ˆw. Consider the estimator for η deﬁned by ˆη = ˆw
pα2 . ˆη estimates the mean of a Gaussian distribution
with variance σ2/α2. The minimax optimal rate for such an estimator is σ2
α2n  where n is the number
of examples with X = α.3 With probability at least 1/2  n ≤ 2mp. Therefore  EDm [(ˆη − η)2] ≥
4α2mp. It follows that EDm[L( ˆw) − L(cid:63)] = EDm [( ˆw − w)2] = p2α4 · E[(ˆη − η)2] ≥ pα2σ2
4m = σ2
4m .
Comparing this to Eq. (11)  one can see that the ratio between the rate of the best passive learner
and the rate of the active learner approaches O(1/p) for large m.

σ2

7 Discussion

Many questions remain open for active regression. For instance  it is of particular interest whether
the convergence rates provided here are the best possible for this model. Second  we consider here
only the plain vanilla ﬁnite-dimensional regression  however we believe that the approach can be
extended to ridge regression in a general Hilbert space. Lastly  the algorithm uses static allocation
of samples to stages and to partitions. In Monte-Carlo estimation Carpentier and Munos (2012) 
dynamic allocation has been used to provide convergence to a pseudo-risk with better constants. It
is an open question whether this type of approach can be useful in the case of active regression.

References
M. F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. Journal of Computer and

System Sciences  75(1):78–89  2009.
3Since |η| ≤ σ
α   this rate holds when σ2

n (cid:28) σ2

α2   that is n (cid:29) α2. (Casella and Strawderman  1981)

8

R. Burbidge  J. J. Rowland  and R. D. King. Active learning for regression based on query by
committee. In Intelligent Data Engineering and Automated Learning-IDEAL 2007  pages 209–
218. Springer  2007.

W. Cai  Y. Zhang  and J. Zhou. Maximizing expected model change for active learning in regression.
In Data Mining (ICDM)  2013 IEEE 13th International Conference on  pages 51–60. IEEE  2013.
A. Carpentier and R. Munos. Minimax number of strata for online stratiﬁed sampling given noisy
samples. In N. H. Bshouty  G. Stoltz  N. Vayatis  and T. Zeugmann  editors  Algorithmic Learning
Theory  volume 7568 of Lecture Notes in Computer Science  pages 229–244. Springer Berlin
Heidelberg  2012.

G. Casella and W. E. Strawderman. Estimating a bounded normal mean. The Annals of Statistics  9

(4):870–878  1981.

D. Cohn  L. Atlas  and R. Ladner. Improving generalization with active learning. Machine Learning 

15:201–221  1994.

D. A. Cohn  Z. Ghahramani  and M. I. Jordan. Active learning with statistical models. Journal of

Artiﬁcial Intelligence Research  4:129–145  1996.

S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In J. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems
20  pages 353–360. MIT Press  2008.

S. Efromovich. Sequential design and estimation in heteroscedastic nonparametric regression. Se-

quential Analysis  26(1):3–25  2007.

R. Ganti and A. G. Gray. Upal: Unbiased pool based active learning. In International Conference

on Artiﬁcial Intelligence and Statistics  pages 422–431  2012.

P. Glasserman. Monte Carlo methods in ﬁnancial engineering  volume 53. Springer  2004.
L. Gy¨orﬁ  M. Kohler  A. Krzyzak  and H. Walk. A distribution-free theory of nonparametric regres-

sion. Springer  2002.

D. Hsu and S. Sabato. Heavy-tailed regression with a generalized median-of-means. In Proceed-
ings of the 31st International Conference on Machine Learning  volume 32  pages 37–45. JMLR
Workshop and Conference Proceedings  2014.

D. Hsu  S. M. Kakade  and T. Zhang. Random design analysis of ridge regression. In Twenty-Fifth

Conference on Learning Theory  2012.

T. Kanamori. Statistical asymptotic theory of active learning. Annals of the Institute of Statistical

Mathematics  54(3):459–475  2002.

T. Kanamori and H. Shimodaira. Active learning algorithm using the maximum weighted log-

likelihood estimator. Journal of Statistical Planning and Inference  116(1):149–162  2003.

D. Needell  N. Srebro  and R. Ward. Stochastic gradient descent and the randomized kaczmarz

algorithm. arXiv preprint arXiv:1310.5715  2013.

M. Sugiyama. Active learning in approximately linear regression based on conditional expectation

of generalization error. The Journal of Machine Learning Research  7:141–166  2006.

M. Sugiyama and S. Nakajima. Pool-based active learning in approximate linear regression. Ma-

chine Learning  75(3):249–274  2009.

J. Von Neumann. Various techniques used in connection with random digits. Applied Math Series 

12(36-38):1  1951.

D. P. Wiens. Minimax robust designs and weights for approximately speciﬁed regression models
with heteroscedastic errors. Journal of the American Statistical Association  93(444):1440–1450 
1998.

D. P. Wiens. Robust weights and designs for biased regression models: Least squares and general-

ized m-estimation. Journal of Statistical Planning and Inference  83(2):395–412  2000.

9

,Sivan Sabato
Remi Munos
Yitian Yuan
Lin Ma
Jingwen Wang
Wei Liu
Wenwu Zhu