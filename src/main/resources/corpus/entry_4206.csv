2018,PacGAN: The power of two samples in generative adversarial networks,Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images  a major shortcoming of GANs is the fact that they tend to produce samples with little diversity  even when trained on diverse datasets. This phenomenon  known as mode collapse  has been the focus of much recent work. We study a principled approach to handling mode collapse  which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class  either real or artificially generated. We draw analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse  thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements.,PacGAN:ThepoweroftwosamplesingenerativeadversarialnetworksZinanLinECEDepartmentCarnegieMellonUniversityzinanl@andrew.cmu.eduAshishKhetanIESEDepartmentUniversityofIllinoisatUrbana-Champaignashish.khetan09@gmail.comGiuliaFantiECEDepartmentCarnegieMellonUniversitygfanti@andrew.cmu.eduSewoongOhIESEDepartmentUniversityofIllinoisatUrbana-Champaignswoh@illinois.eduAbstractGenerativeadversarialnetworks(GANs)areatechniqueforlearninggenerativemodelsofcomplexdatadistributionsfromsamples.Despiteremarkableadvancesingeneratingrealisticimages amajorshortcomingofGANsisthefactthattheytendtoproducesampleswithlittlediversity evenwhentrainedondiversedatasets.Thisphenomenon knownasmodecollapse hasbeenthefocusofmuchrecentwork.Westudyaprincipledapproachtohandlingmodecollapse whichwecallpacking.Themainideaistomodifythediscriminatortomakedecisionsbasedonmultiplesamplesfromthesameclass eitherrealorartiﬁciallygenerated.Wedrawanalysistoolsfrombinaryhypothesistesting—inparticulartheseminalresultofBlackwell[4]—toproveafundamentalconnectionbetweenpackingandmodecollapse.Weshowthatpackingnaturallypenalizesgeneratorswithmodecollapse therebyfavoringgeneratordistributionswithlessmodecollapseduringthetrainingprocess.Numericalexperimentsonbenchmarkdatasetssuggestthatpackingprovidessigniﬁcantimprovements.1IntroductionGenerativeadversarialnetworks(GANs)areatechniquefortraininggenerativemodelstoproducerealisticexamplesfromanunknowndatadistribution[10].SupposewearegivenNi.i.d.samplesX1 ... XNfromanunknownprobabilitydistributionPoversomehigh-dimensionalspaceRp(e.g. images).ThegoalofgenerativemodelingistolearnamodelthatcandrawsamplesfromdistributionP.Indata-drivengenerativemodeling thismodelistypicallyformulatedasafunctionG:Rd→Rpthatmapsalow-dimensionalcodevectorZ∈Rddrawnfromastandarddistribution(e.g.sphericalGaussian)toahigh-dimensionaldomainofinterest.AbreakthroughintrainingsuchgenerativemodelswasachievedbytheinnovativeideaofGANs.GANstraintwoneuralnetworkscalledthegeneratorG(Z)anddiscriminatorD(X).Theroleofthegeneratoristoproducerealisticsamples andtheroleofthediscriminatoristodistinguishgeneratedsamplesfromrealdata.Thesetwoneuralnetworksplayadynamicminimaxgameagainsteachother.Iftrainedlongenough eventuallythegeneratorlearnstoproducesamplesthatareindistinguishablefromrealdata(butpreferablydifferentfromthetrainingsamples).Concretely GANssearchforthe32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.parametersofneuralnetworksGandDthatoptimizethefollowingminimaxobjective:G∗∈argminGmaxDV(G D)=argminGmaxDEX∼P[log(D(X))]+EZ∼PZ[log(1−D(G(Z)))] (1)wherePisthedistributionoftherealdata andPZisthedistributionoftheinputcodevectorZ.Critically [10]showsthattheglobaloptimumof(1)isachievedifandonlyifP=Q whereQisthegenerateddistributionofG(Z).Thesolutiontotheminimaxproblem(1)canbeapproximatedbyiterativelytrainingtwo“competing"neuralnetworks thegeneratorGanddiscriminatorD.Eachmodelcanbeupdatedbybackpropagatingthegradientofthelossfunctiontoitsparameters.AmajorchallengeintrainingGANsisaphenomenonknownasmodecollapse whichreferstoalackofdiversityingeneratedsamples.Indeed GANscommonlymissmodeswhentrainedonmultimodaldistributions.Forinstance whentrainedonhand-writtendigitswithtenmodes thegeneratormightfailtoproducesomeofthedigits[24].Severalapproacheshavebeenproposedtoﬁghtmodecollapse e.g.[7 8].Proposedsolutionsrelyonmodiﬁedarchitectures lossfunctions andoptimizationalgorithms.Althougheachoftheseproposedmethodsempiricallymitigatesmodecollapse welackrigorousexplanationsofwhytheempiricalgainsareachieved—especiallywhenthosegainsaresensitivetohyperparameters.OurContributions.Inthiswork weexamineGANsthroughthelensofhypothesistesting.Byviewingthediscriminatorasperformingabinaryhypothesistestonsamples(i.e. whethertheyweredrawnfromdistributionPorQ) wecanapplyclassicalhypothesistestingresultstotheanalysisofGANs.Thisviewleadstothreecontributions:(1)Conceptual:Weproposeaformaldeﬁnitionofmodecollapsethatabstractsawaythegeometricpropertiesoftheunderlyingdatadistributions(Section3).ThisdeﬁnitioniscloselyrelatedtothenotionofROCcurvesinbinaryhypothesistesting.Giventhisdeﬁnition weprovideanewinterpretationofthepairofdistributions(P Q)asatwo-dimensionalregioncalledthemodecollapseregion wherePisthetruedatadistributionandQthegeneratedone.Themodecollapseregionprovidesnewinsightsonhowtoreasonabouttherelationshipbetweenthosetwodistributions.(2)Analytical:Throughthelensofhypothesistestingandmodecollapseregions weshowthatifthediscriminatorisallowedtoseesamplesfromthem-thorderproductdistributionsPmandQminsteadoftheusualtargetdistributionPandgeneratordistributionQ thenthecorrespondinglosswhentrainingthegeneratornaturallypenalizesgeneratordistributionswithstrongmodecollapse(Section3).Hence ageneratortrainedwiththistypeofdiscriminatorwillchoosedistributionsthatexhibitlessmodecollapse.Theregioninterpretationofmodecollapseandcorrespondingdataprocessinginequalitiesprovidenovelanalysistoolsforprovingstrongandsharpresultswithsimpleproofs.Technically thisleadstoanovelgeometricanalysistechniquetoﬁndtheoptimalsolutionsofinﬁnitedimensionalnon-convexoptimizationproblemsofinterestinEqs.(2)and(3).(3)Algorithmic:WeproposeanewGANframeworktomitigatemodecollapse whichwecallPacGAN.PacGANcanbeseamlesslyappliedtoexistingGANs requiringonlyasmallmodiﬁcationtothediscriminatorarchitecture(Section2).Thekeyideaistopassm“packed"orconcatenatedsamplestothediscriminator whicharejointlyclassiﬁedaseitherrealorgenerated.Thisallowsthediscriminatortodobinaryhypothesistestingbasedontheproductdistributions(Pm Qm) whichnaturallypenalizesmodecollapse(Section3).WedemonstrateonbenchmarkdatasetsthatPacGANsigniﬁcantlyimprovesuponcompetingapproachesinmitigatingmodecollapse(Section4) notablyminibatchdiscrimination[24].RelatedWorkThreeprimarychallengesappearintheGANliterature:(i)theyareunstabletotrain (ii)theyarechallengingtoevaluate and(iii)theyexhibitmodecollapse(morebroadly theydonotgeneralize).Ourworkexplicitlyaddresseschallenge(iii) whichisthefocusofthissection.Modecollapseisabyproductofpoorgeneralization—i.e. thegeneratordoesnotlearnthetruedatadistribution;thisphenomenonisofsigniﬁcantinterest[2 3 18 1 2].Priorworkhasobservedtwotypesofmodecollapse:entiremodesfromtheinputdataarenevergenerated orthegeneratoronlycreatesimageswithinasubsetofaparticularmode[9 27 3 7 20 23].Thesephenomenaarenotwell-understood butanumberofexplanatoryhypotheseshavebeenproposed includingimproperobjectivefunctions[1 2]andweakdiscriminators[20 24 2 17].Buildingonthesecondhypothesis 2weshowthatapackeddiscriminatorcansigniﬁcantlyreducemodecollapse boththeoreticallyandinpractice.Wecomparepackingtothreemainapproachesformitigatingmodecollapse:(1)JointArchitectures.Inencoder-decoderarchitectures theGANlearnsanencodingG−1(X)fromthedataspacetoalower-dimensionallatentspace inadditiontotheusualdecodingG(Z)fromthelatentspacetothedataspace(e.g. BiGAN[8] adversariallylearnedinference[7] VEEGAN[25]).Despiteempiricalgainsinsuchjointarchitectures weﬁndthatpackingcapturesmoremodesforaﬁxedgeneratoranddiscriminatorarchitecture withlessarchitecturalandcomputationaloverhead.Also recentworksuggeststhatsucharchitecturesmaybeunabletopreventmodecollapse[2].(2)AugmentedDiscriminators.Severalproposalshavestrengthenedthediscriminatorbyprovidingitwithimagelabels[5]and/ormoresamples.Alatterapproach minibatchdiscrimination[24] feedsanarrayofdatasamplestothediscriminator whichusestheminibatchassideinformationtoclassifyeachsampleindividually.Recentworkimprovedminibatchdiscriminationbyprogressivelytrainingdiscriminatorsonlargerminibatches withimpressivevisualresults[13].Whilepackingandminibatchdiscriminationstartfromthesameintuitionthatshowingmultiplesamplesatthediscriminatorhelpsmitigatemodecollapse howthisideaisimplementedinthediscriminatorarchitecturesarecompletelydifferent.PacGANiseasiertoimplement empiricallyeffective andourtheoreticalanalysisshowsthatpackingisaprincipledwaytousebatchedsamples.Forexample intheexperimentinAppendixB.2(leftcolumnofTable6) thedefaultDCGANdiscriminatorhas585weightsintotalintheUnrolledGANimplementation theproposedPacDCGAN4onlyadds162moreweightstothediscriminator whileminibatchdiscriminatoradds1 225 732moreweights.(3)Optimization-basedsolutions.GANsaretypicallytrainedwithiterativegenerator-discriminatorparameterupdates whichcanleadtonon-convergence[17]—aworseproblemthanmodecollapse.UnrolledGANs[20]proposeanoptimizationthataccountsforkgradientstepswhencomputinggradients.Weobservethatpackingachievesbetterempiricalperformancewithlessoverhead.2PacGANFrameworkTherearemanywaystoimplementtheideaofpacking eachwithtradeoffs.Inthissection wepresentasimplepackingframeworkthatservesasthebasisforourempiricalexperimentsandaconcreteexampleofpacking.Aprimaryreasonforthisarchitecturalchoiceistoemphasizeonlytheeffectofpackinginnumericalexperiments andisolateitfromanyothereffectsthatmightresultfromother(moresophisticated)changestothearchitecture.However ouranalysisinSection3isagnostictothepackingimplementation andwediscusspotentialalternativepackingarchitecturesinSection5 especiallythosethatexplicitlyimposepermutationinvariance.WestartwithanexistingGAN deﬁnedbyageneratorarchitecture adiscriminatorarchitecture andalossfunction.Wecallthistripletthemotherarchitecture.ThePacGANframeworkmaintainsthesamegeneratorarchitecture lossfunction andhyperparametersasthemotherarchitecture.However insteadofusingadiscriminatorD(X)thatmapsasinglesample(eitherrealorgenerated)toa(soft)label weuseanaugmenteddiscriminatorD(X1 X2 ... Xm)thatmapsmsamplestoasingle(soft)label.Thesemsamplesaredrawnindependentlyfromthesamedistribution—eitherreal(jointlylabelledY=1)orgenerated(Y=0).Werefertotheconcatenationofsampleswiththesamelabelaspacking theresultingdiscriminatorasapackeddiscriminator andthenumbermofconcatenatedsamplesasthedegreeofpacking.TheproposedapproachcanbeappliedtoanyexistingGANarchitectureandanylossfunction aslongasitusesadiscriminatorD(X)thatclassiﬁesasingleinputsample.Weusethenotation“Pac(X)(m)”where(X)isthenameofthemotherarchitecture and(m)isisthepackingdegree.Forexample ifwetakeanoriginalGANandfeedthediscriminatorthreepackedsamples wecallthis“PacGAN3”.Weimplementpackingbykeepingallhiddenlayersofthediscriminatoridenticaltothemotherarchitecture andincreasingthenumberofnodesintheinputlayerbyafactorofm.Forexample inFigure1 westartwithafully-connected feed-forwarddiscriminator.EachsampleXistwo-dimensional sotheinputlayerhastwonodes.UnderPacGAN2 wemultiplythesizeoftheinputlayerbythepackingdegreem=2 andtheconnectionstotheﬁrsthiddenlayerareadjustedsothattheﬁrsttwolayersremainfully-connected asinthemotherarchitecture.Thegrid-patternednodesinFigure1representinputnodesforthesecondsample.Similarly whenpackingaDCGAN whichusesconvolutionalneuralnetworksforboththegeneratorandthediscriminator wesimplystacktheimagesintoatensorofdepthm.Forinstance thediscriminatorforPacDCGAN4onthe3Figure1:PacGAN(m)augmentstheinputlayerbyafactorofm.Thenumberofweightsbetweentheﬁrsttwolayersareincreasedtopreservethemotherarchitecture’sconnectivity.Packedsamplesareconcatenatedandfedtotheinputlayer;grid-patternednodesareinputnodesforthesecondsample.MNISTdatasetofhandwrittenimages[16]wouldtakeaninputofsize28×28×4 sinceeachindividualblack-and-whiteMNISTimageis28×28pixels.Onlytheinputlayerandthenumberofweightsinthecorrespondingﬁrstconvolutionallayerwillincreaseindepthbyafactorof4.AsinstandardGANs wetrainthepackeddiscriminatorwithabagofsamplesfromtherealdataandthegenerator.However eachminibatchinthestochasticgradientdescentnowconsistsofpackedsamples(X1 X2 ... Xm Y) whichthediscriminatorjointlyclassiﬁes.Intuitively packinghelpsthediscriminatordetectmodecollapsebecauselackofdiversityismoreobviousinasetofsamplesthaninasinglesample.3TheoreticalAnalysisofPacGANInthissection weshowafundamentalconnectionbetweentheprincipleofpackingandmodecollapseinGAN.Weprovideacompleteunderstandingofhowpackingchangesthelossasseenbythegenerator byfocusingon(a)theoptimaldiscriminatoroverafamilyofallmeasurablefunctions;(b)thepopulationexpectation;and(c)the0-1lossfunctionoftheformmaxDEX∼P[I(D(X))]+EG(Z)∼Q[1−I(D(G(Z)))] subjecttoD(X)∈{0 1}.Thisdiscriminatorprovides(anapproximationof)thetotalvariationdistance andthegeneratortriestominimizethetotalvariationdistancedTV(P Q) aswidelyknownintheGANliterature[10].Thereasonwemakethisassumptionisprimarilyforclarityandanalyticaltractability:totalvariationdistancehighlightstheeffectofpackinginawaythatiscleanerandeasiertounderstandthanifweweretoanalyzeJensen-Shannondivergence.Wewanttounderstandhowthis0-1loss asprovidedbysuchadiscriminator changeswiththedegreeofpackingm.Aspackeddiscriminatorsseempackedsamples eachdrawni.i.d.fromonejointclass(i.e.eitherrealorgenerated) wecanconsiderthesepackedsamplesasasinglesamplethatisdrawnfromtheproductdistribution:PmforrealandQmforgenerated.TheresultinglossprovidedbythepackeddiscriminatoristhereforedTV(Pm Qm).Weﬁrstprovideaformalmathematicaldeﬁnitionofmodecollapse whichleadstoatwo-dimensionalrepresentationofanypairofdistributions(P Q)asamode-collapseregion.Thisregionrepresentationprovidesnotonlyconceptualclarityregardingmodecollapse butalsoprooftechniquesthatareessentialtoprovingourmainresults.WedeferalltheproofstotheAppendix.InAppendixE weshowtheproposedmodecollapseregionisequivalenttotheROCcurveforbinaryhypothesistesting.Thisallowsustousepowerfulmathematicaltechniquesfrombinaryhypothesistestingincludingthedataprocessinginequality.Deﬁnition1.AtargetdistributionPandageneratorQexhibit(ε δ)-modecollapsefor0≤ε<δ≤1ifthereexistsasetS⊆XsuchthatP(S)≥δandQ(S)≤ε.Intuitively largerδandsmallerεindicatemoreseveremodecollapse.Thatis ifalargeportionofthetargetP(S)≥δinsomesetSinthedomainXismissinginthegeneratorQ(S)≤ε wedeclare(ε δ)-modecollapse.Similarly whenwechangetheroleofPandQ andhaveP(S)≤εandQ(S)≥δ wesayPandQexhibit( δ)-modeaugmentation.ThisdeﬁnitionhasafundamentalconnectiontotheROCregionindetectiontheoryandbinaryhypothesistesting—aconnectionthatiscriticalforourprooftechniques;thisconnectionisdetailedinAppendixDandE.Akeyobservationisthattwopairsofdistributionscanhavethesametotalvariationdistancewhileexhibitingverydifferentmodecollapsepatterns.Toseethis consideratoyexampleinFigure2 withauniformtargetdistributionP=U([0 1])andamodecollapsinggeneratorQ1=U([0.2 1])4andanonmodecollapsinggeneratorQ2=0.6U([0 0.5])+1.4U([0.5 1]).Theappropriateway111.250.211110.61.40.5PQ1Q2 0 0.5 1 0 0.5 1R(P Q1)εδ 0 0.5 1 0 0.5 1R(P Q2)εδFigure2:Aformaldeﬁnitionof(ε δ)-modecollapseanditsaccompanyingregionrepresentationcapturestheintensityofmodecollapseforgeneratorsQ1withmodecollapseandQ2whichdoesnothavemodecollapse foratoyexampledistributionsP Q1 andQ2shownontheleft.Theregionof(ε δ)-modecollapsethatisachievableisshowningrey.topreciselyrepresentmodecollapseistovisualizeitthroughatwo-dimensionalregionwecallthemodecollapseregion.Foragivenpair(P Q) thecorrespondingmodecollapseregionR(P Q)isdeﬁnedastheconvexhulloftheregionofpoints(ε δ)suchthat(P Q)exhibit(ε δ)-modecollapse asshowninFigure2:R(P Q) conv(cid:0)(cid:8)(ε δ)(cid:12)(cid:12)δ>εand(P Q)has(ε δ)-modecollapse(cid:9)(cid:1).ThereisafundamentalconnectionbetweenthemodecollapseregionandtheROCcurveinhypothesistesting(AppendixE).Anunpackeddiscriminator observingonlytheTVdistancebetweengeneratordistributionsQandthetruedistributionP cannotdistinguishbetweentwocandidategeneratorsQ1andQ2withdTV(P Q1)=dTV(P Q2) butdifferentmodecollapseregions.Thekeyinsightofthisworkisthatbyinsteadconsideringproductdistributions thetotalvariationdistancedTV(Pm Qm)variesinawaythatiscloselytiedtothemodecollapseregionsfor(P Q).Forinstance Figure3(left)showsanachievablerangeofdTV(Pm Qm)conditionedonthatdTV(P Q)=τforτ=1.1.Withinthisachievablerange somepairs(P Q)haverapidlyincreasingtotalvariation occupyingtheupperpartoftheregion(showninred middlepanelofFigure3) andothershaveslowlyincreasingtotalvariation occupyingthelowerpart(showninblue)intherightpanelofFigure3.Weformallyshowinthefollowingthatthereisafundamentalconnectionbetweentotalvariationdistanceevolutionanddegreeofmodecollapse.Namely distributionswithstrongmodecollapseoccupytheupperregion andhencewillbepenalizedbyapackeddiscriminator.Evolutionoftotalvariationdistanceswithmodecollapse.Weanalyzehowthetotalvariationevolvesforthesetofallpairs(P Q)thathavethesametotalvariationdistancesτwhenunpacked withm=1 andhave(ε δ)-modecollapseforsome0≤ε<δ≤1.Thesolutionofthefollowingoptimizationproblemgivesthedesiredrange:minP QormaxP QdTV(Pm Qm)(2)subjecttodTV(P Q)=τ(P Q)has(ε δ)-modecollapse wherethemaximizationandminimizationareoverallprobabilitymeasuresPandQ andthemodecollapseconstraintisdeﬁnedinDeﬁnition1.Weprovidetheoptimalsolutionanalyticallyandestablishthatmode-collapsingpairsoccupytheupperpartofthetotalvariationregion;thatis totalvariationincreasesrapidlyaswepackmoresamplestogether(Figure3 middlepanel).Theorem2.Forall0≤ε<δ≤1andanintegerm if1≥τ≥δ−εthenthesolutiontothemaximizationin(2)is1−(1−τ)m andthesolutiontotheminimizationisminnmin0≤α≤1−τδδ−εdTV(cid:16)Pinner1(α)m Qinner1(α)m(cid:17) min1−τδδ−ε≤α≤1−τdTV(cid:16)Pinner2(α)m Qinner2(α)m(cid:17)o 5 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 3 5 7 9 11dTV(Pm Qm)degreeofpackingm 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 3 5 7 9 11(0.00 0.1)-modecollapsedegreeofpackingm 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 3 5 7 9 11no(0.07 0.1)-modecollapsedegreeofpackingmFigure3:TherangeofdTV(Pm Qm)achievablebypairswithdTV(P Q)=τ forachoiceofτ=0.11 deﬁnedbythesolutionsoftheoptimization(4)providedinTheorem4intheAppendix(leftpanel).TherangeofdTV(Pm Qm)achievablebythosepairsthatalsohave(ε=0.00 δ=0.1)-modecollapse(middlepanel).Asimilarrangeachievablebypairsofdistributionsthatdonothave(ε=0.07 δ=0.1)-modecollapseor(ε=0.07 δ=0.1)-modeaugmentation(rightpanel).Pairs(P Q)withstrongmodecollapseoccupythetopregion(neartheupperbound)andthepairswithweakmodecollapseoccupythebottomregion(nearthelowerbound).wherePinner1(α)m Qinner1(α)m Pinner2(α)m andQinner2(α)marethem-thorderproductdistri-butionsofdiscreterandomvariablesdistributedasPinner1(α)=[δ 1−α−δ α] Qinner1(α)=[ε 1−α−τ−ε α+τ] Pinner2(α)=[1−α α] andQinner2(α)=[1−α−τ α+τ].Ifτ<δ−ε thentheoptimizationin(2)hasnosolutionandthefeasiblesetisanemptyset.Oneimplicationisthatdistributionpairs(P Q)atthetopofthetotalvariationevolutionregionarethosewiththestrongestmodecollapse.Anotherimplicationisthatapair(P Q)withstrongmodecollapse(i.e. withlargerδandsmallerεintheconstraint)willbepenalizedmoreunderpacking andhenceageneratorminimizinganapproximationofdTV(Pm Qm)willbeunlikelytoselectadistributionthatexhibitssuchstrongmodecollapse.Evolutionoftotalvariationdistanceswithoutmodecollapse.Wenextanalyzehowthetotalvariationevolvesforthesetofallpairs(P Q)thathavethesametotalvariationsdistancesτwhenunpacked withm=1 anddonothave(ε δ)-modecollapseforsome0≤ε<δ≤1.Becauseofthesymmetryofthetotalvariationdistance modecollapsefor(Q P)isequallydamagingasmodecollapseof(P Q) whenitcomestohowfasttotalvariationdistancesevolve.Hence wecharacterizethisevolutionforthosefamilyofpairsofdistributionsthatdonothaveeithermodecollapses.Thesolutionofthefollowingoptimizationproblemgivesthedesiredrangeoftotalvariationdistances:minP QormaxP QdTV(Pm Qm)(3)subjecttodTV(P Q)=τ (P Q)doesnothave(ε δ)-modecollapse (Q P)doesnothave(ε δ)-modecollapse Weprovidethteoptimalsolutionanalyticallyandestablishthatthepairs(P Q)withweakmodecollapsewilloccupythebottompartoftheevolutionofthetotalvariationdistances(seeFigure3rightpanel) andalsowillbepenalizedlessunderpacking.Henceageneratorminimizing(approximate)dTV(Pm Qm)islikelytogeneratedistributionswithweakmodecollapse.Theorem3.Ifδ+ε≤1andδ−ε≤τ≤(δ−ε)/(δ+ε)thenthesolutiontothemaximizationin(3)ismaxα+β≤1−τ ετδ−ε≤α βdTV(cid:16)Pouter1(α β)m Qouter1(α β)m(cid:17) wherePouter1(α β)mandQouter1(α β)marethem-thorderproductdistributionsofdiscreterandomvariablesdistributedasPouter1(α β)=[α(δ−ε)−ετα−ε α(α+τ−δ)α−ε 1−τ−α−β β 0]andQouter1(α β)=[0 α 1−τ−α−β β(β+τ−δ)β−ε β(δ−ε)−ετβ−ε].Thesolutiontotheminimizationin(3)isminετδ−ε≤α≤1−δτδ−εdTV(cid:16)Pinner(α)m Qinner(α τ)m(cid:17) 6wherePinner(α)andQinner(α τ)aredeﬁnedasinTheorem4intheAppendix.Wecanprovetheexactsolutionoftheoptimizationforallvaluesofεandδ whichweprovideintheAppendix.Wereferalsototheappendixofmoreillustrationsofregionsoccupiedbyvariouschoicesofεandδformodecollapsingdistributions andnonmodecollapsingregions.4ExperimentsOnstandardbenchmarkdatasets wecomparePacGANtoseveralbaselineGANarchitectures someexplicitlydesignedtomitigatemodecollapse:GAN[10] minibatchdiscrimination(MD)[24] DCGAN[22] VEEGAN[25] UnrolledGANs[20] andALI[8].WealsoimplicitlycompareagainstBIGAN[7] whichisconceptuallyidenticaltoALI.Toisolatetheeffectsofpacking wemakeminimalchoicesinthearchitectureandhyperparametersofourpackingimplementation.Ourgoalistoreproduceexperimentsfromtheliterature applypackingtothesimplestbaselineGAN andobservehowpackingaffectsperformance.Wheneverpossible weusetheexactlysamechoiceofarchitecture hyperparameters andlossfunctionasabaselineineachexperiment;wechangeonlythediscriminatortoacceptpackedsamples.Allcodetoreproduceourexperimentscanbefoundathttps://github.com/fjxmlzn/PacGAN.Metrics.Wemeasureseveralpreviously-usedmetrics.Theﬁrstisnumberofmodesthatareproducedbyagenerator[7 20 25].Inlabelleddatasets thisnumbercanbeevaluatedusingathird-partytrainedclassiﬁerthatclassiﬁesthegeneratedsamples[25].Asecondmetricusedin[25]isthenumberofhigh-qualitysamples whichistheproportionofthesamplesthatarewithinxstandarddeviationsfromthecenterofamode.Finally wemeasurethereverseKullback-Leiblerdivergencebetweentheinduceddistributionfromgeneratedsamplesandtheinduceddistributionfromtherealsamples.Eachofthesemetricshasshortcomings—forexample thenumberofobservedmodesignoresclassimbalance andallofthemetricsassumethemodesareknownapriori.Datasets.Weusesyntheticandrealdatasets.The2D-ring[25]isamixtureofeighttwo-dimensionalsphericalGaussianswithmeans(cos((2π/8)i) sin((2π/8)i))andvariances10−4ineachdimensionfori∈{1 ... 8}.The2D-grid[25]isamixtureof25two-dimensionalsphericalGaussianswithmeans(−4+2i −4+2j)andvariances0.0025ineachdimensionfori j∈{0 1 2 3 4}.TheMNISTdataset[16]consistsof70Kimagesofhandwrittendigits each28×28pixels.Unmodiﬁed thisdatasethas10modes(digits).Asin[20 25] weaugmentthenumberofmodesbystackingtheimages:wegenerateanewdatasetof128KimageswhereeachimageconsistsofthreerandomMNISTimagesstackedintoa28×28×3RGBimage.ThisnewstackedMNISTdatasethas(withhighprobability)1000=10×10×10modes.Finally weincludeexperimentsontheCelebAdataset whichisacollectionof200Kfacialimagesofcelebrities[19].4.1SyntheticdataexperimentsOurﬁrstexperimentmeasurestheeffectofthenumberofdiscriminatorparametersonmodecollapse.Packedarchitectureshavemorediscriminatornodes(andparameters)thanthemotherarchitecture whichcouldartiﬁciallymitigatemodecollapsebygivingthediscriminatormorecapacity.Wecomparethiseffecttotheeffectofpackingonthe2Dgriddataset.InFigure4 weevaluatethenumberofmodesrecoveredandreverseKLdivergenceforALI GAN MD andPacGAN whilevaryingthenumberoftotalparametersineacharchitecture(discriminatorandencoderifoneexists).TheexperimentaldetailsareincludedinAppendixA.2.ForMD themetricsﬁrstimproveandthendegradewiththenumberofparameters.WesuspectthatthismaybecauseMDisverysensitivetoexperimentsettings asthesamearchitectureofMDhasverydifferentperformanceon2d-gridand2d-ringdataset(AppendixA.1).ForALI GANandPacGAN despitevaryingthenumberofparametersbyanorderofmagnitude wedonotseesigniﬁcantevidenceofthemetricsimprovingwiththenumberofparameters.ThissuggeststhattheadvantagesofPacGANandALIcomparedtoGANdonotstemfromhavingmoreparameters.PackingalsoseemstoincreasethenumberofmodesrecoveredanddecreasethereverseKLdivergence;weexplorethisphenomenonmoreinsubsequentexperiments.701000002000003000004000005000006000007000008000001516171819202122232425GANPacGAN2PacGAN3PacGAN4Minibatch DiscriminationALIModesrecovered(higherisbetter)ParameterCount01000002000003000004000005000006000007000008000000.00.20.40.60.81.0GANPacGAN2PacGAN3PacGAN4Minibatch DiscriminationALIReverseKLdivergence(lowerisbetter)ParameterCountFigure4:Effectofnumberofparametersonevaluationmetrics.4.2StackedMNISTexperimentsForourstackedMNISTexperiments wegeneratesamples.Eachofthethreechannelsineachsampleisclassiﬁedbyapre-trainedthird-partyMNISTclassiﬁer andtheresultingthreedigitsdeterminewhichofthe1000modesthesamplebelongsto.Wemeasurethenumberofmodescaptured aswellastheKLdivergencebetweenthegenerateddistributionovermodesandtheexpected(uniform)one.Intheﬁrstexperiment wereplicateTable2from[25] whichmeasuredthenumberofobservedmodesinageneratortrainedonthestackedMNISTdataset aswellastheKLdivergenceofthegeneratedmodedistribution.Inlinewith[25] weusedaDCGAN-likearchitecturefortheseexperiments1(detailsinAppendixB.1).OurresultsareshowninTable1.Theﬁrstfourrowsarecopieddirectlyfrom[25].ThelastthreerowsarecomputedusingabasicDCGAN withpackinginthediscriminator.Weﬁndthatpackinggivesgoodmodecoverage reachingall1 000modesineverytrial.Again packingthesimplestDCGANfullycapturesallthemodesinthebenchmarktest sowedonotpursuepackingmorecomplexbaselinearchitectures.WealsoobservethatMDisveryunstablethroughouttraining whichmakesitcaptureevenlessmodesthanGAN.OnefactorthatcontributestoMD’sinstabilitymaybethatMDrequirestoomanyparameters.ThenumberofdiscriminatorparametersinMDis47 976 773 whereasGANhas4 310 401andPacGAN4onlyneeds4 324 801.StackedMNISTModesKLDCGAN[22]99.03.40ALI[8]16.05.40UnrolledGAN[20]48.74.32VEEGAN[25]150.02.95MinibatchDiscrimination[24]24.5±7.675.49±0.418DCGAN(ourimplementation)78.9±6.464.50±0.127PacDCGAN2(ours)1000.0±0.000.06±0.003PacDCGAN3(ours)1000.0±0.000.06±0.003PacDCGAN4(ours)1000.0±0.000.07±0.005Table1:Twomeasuresofmodecollapseproposedin[25]forthestackedMNISTdataset:numberofmodescapturedbythegeneratorandreverseKLdivergenceoverthegeneratedmodedistribution.TheDCGAN PacDCGAN andMDresultsareaveragedover10trials withstandarderrorreported.4.3CelebAexperimentsFinally wemeasurethediversityofimagesgeneratedfromtheCelebAdatasetasin[3]byestimatingtheprobabilityofcollisioninabatchofgeneratedimages.Ifthereexistsatleastonepairofnear-duplicateimagesinthebatch acollisionisdeclared whichindicateslackofdiversity.Thedetailsof1https://github.com/carpedm20/DCGAN-tensorflow8howwedetermineduplicatesandourarchitecturearedeferredtoAppendixC.Weﬁndthatpackingsigniﬁcantlyimprovesthediversityofsamples andifthesizeofthediscriminatorissmall packingalsoimprovessamplequality.SeeAppendixCforgeneratedsamples.discriminatorsizeprobabilityofcollisionDCGANPacDCGAN2273K10.334×273K0.42016×273K0.86025×273K0.650.17Table2:Probabilityof≥1pairofnear-duplicateimagesappearinginabatchof1024imagesgeneratedfromDCGANandPacDCGAN2oncelebAdataset.5DiscussionInthiswork weproposeapackingframeworkthattheoreticallyandempiricallymitigatesmodecollapsewithlowoverhead.Ouranalysisleadstoseveralinterestingopenquestions includinghowtoapplytheseanalysistechniquestomoregeneralclassesoflossfunctionssuchasJensen-ShannondivergenceandWassersteindistances.ThiswillcompletetheunderstandingofthesuperiorityofourapproachobservedinexperimentswithJSdivergenceinSection4andwithWassersteindistanceinAppendixB.3.Anotherimportantquestioniswhatpackingarchitecturetouse.Forinstance aframeworkthatprovidespermutationinvariancemaygivebetterresultssuchasgraphneuralnetworks[6 26 15]ordeepsets[28].AcknowledgementTheauthorswouldliketothankSreeramKannanandAlexDimakisfortheinitialdiscussionsthatleadtotheinceptionofthepackingidea andVyasSekarforvaluablediscussionsaboutGANs.WethankSrivastavaAkash LukeMetz TuNguyen andYingyuLiangforprovidinginsightsand/ortheimplementationdetailsontheirproposedarchitecturesforVEEGAN[25] UnrolledGAN[20] D2GAN[21] andMIX+GAN[2].Wethanktheanonymousreviewersfortheirconstructivefeedback.ThisworkissupportedbyNSFawardsCNS-1527754 CCF-1553452 CCF-1705007 andRI-1815535andGoogleFacultyResearchAward.ThisworkusedtheExtremeScienceandEngineeringDiscoveryEnvironment(XSEDE) whichissupportedbyNationalScienceFoundationgrantnumberOCI-1053575.Speciﬁcally itusedtheBridgessystem whichissupportedbyNSFawardnumberACI-1445606 atthePittsburghSupercomputingCenter(PSC).ThisworkispartiallysupportedbythegenerousresearchcreditsonAWScloudcomputingresourcesfromAmazon.References[1]M.Arjovsky S.Chintala andL.Bottou.WassersteinGAN.arXivpreprintarXiv:1701.07875 2017.[2]S.Arora R.Ge Y.Liang T.Ma andY.Zhang.Generalizationandequilibriumingenerativeadversarialnets(GANs).arXivpreprintarXiv:1703.00573 2017.[3]S.AroraandY.Zhang.Dogansactuallylearnthedistribution?anempiricalstudy.arXivpreprintarXiv:1706.08224 2017.[4]D.Blackwell.Equivalentcomparisonsofexperiments.TheAnnalsofMathematicalStatistics 24(2):265–272 1953.[5]T.Che Y.Li A.P.Jacob Y.Bengio andW.Li.Moderegularizedgenerativeadversarialnetworks.arXivpreprintarXiv:1612.02136 2016.[6]M.Defferrard X.Bresson andP.Vandergheynst.Convolutionalneuralnetworksongraphswithfastlocalizedspectralﬁltering.InAdvancesinNeuralInformationProcessingSystems pages3844–3852 2016.9[7]J.Donahue P.Krähenbühl andT.Darrell.Adversarialfeaturelearning.arXivpreprintarXiv:1605.09782 2016.[8]V.Dumoulin I.Belghazi B.Poole A.Lamb M.Arjovsky O.Mastropietro andA.Courville.Adversariallylearnedinference.arXivpreprintarXiv:1606.00704 2016.[9]I.Goodfellow.Nips2016tutorial:Generativeadversarialnetworks.arXivpreprintarXiv:1701.00160 2016.[10]I.Goodfellow J.Pouget-Abadie M.Mirza B.Xu D.Warde-Farley S.Ozair A.Courville andY.Bengio.Generativeadversarialnets.InAdvancesinneuralinformationprocessingsystems pages2672–2680 2014.[11]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.InInternationalConferenceonMachineLearning pages448–456 2015.[12]P.Kairouz S.Oh andP.Viswanath.Thecompositiontheoremfordifferentialprivacy.IEEETransactionsonInformationTheory 63(6):4037–4049 June2017.[13]T.Karras T.Aila S.Laine andJ.Lehtinen.ProgressivegrowingofGANsforimprovedquality stability andvariation.arXivpreprintarXiv:1710.10196 2017.[14]D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980 2014.[15]T.N.KipfandM.Welling.Semi-supervisedclassiﬁcationwithgraphconvolutionalnetworks.arXivpreprintarXiv:1609.02907 2016.[16]Y.LeCun.Themnistdatabaseofhandwrittendigits.http://yann.lecun.com/exdb/mnist/ 1998.[17]J.Li A.Madry J.Peebles andL.Schmidt.Towardsunderstandingthedynamicsofgenerativeadversarialnetworks.arXivpreprintarXiv:1706.09884 2017.[18]S.Liu O.Bousquet andK.Chaudhuri.Approximationandconvergencepropertiesofgenerativeadversariallearning.arXivpreprintarXiv:1705.08991 2017.[19]Z.Liu P.Luo X.Wang andX.Tang.Deeplearningfaceattributesinthewild.InProceedingsofInternationalConferenceonComputerVision(ICCV) 2015.[20]L.Metz B.Poole D.Pfau andJ.Sohl-Dickstein.Unrolledgenerativeadversarialnetworks.arXivpreprintarXiv:1611.02163 2016.[21]T.Nguyen T.Le H.Vu andD.Phung.Dualdiscriminatorgenerativeadversarialnets.InAdvancesinNeuralInformationProcessingSystems pages2667–2677 2017.[22]A.Radford L.Metz andS.Chintala.Unsupervisedrepresentationlearningwithdeepconvolu-tionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434 2015.[23]S.Reed Z.Akata X.Yan L.Logeswaran B.Schiele andH.Lee.Generativeadversarialtexttoimagesynthesis.arXivpreprintarXiv:1605.05396 2016.[24]T.Salimans I.Goodfellow W.Zaremba V.Cheung A.Radford andX.Chen.Improvedtechniquesfortraininggans.InAdvancesinNeuralInformationProcessingSystems pages2234–2242 2016.[25]A.Srivastava L.Valkov C.Russell M.Gutmann andC.Sutton.Veegan:Reducingmodecollapseingansusingimplicitvariationallearning.arXivpreprintarXiv:1705.07761 2017.[26]K.K.Thekumparampil C.Wang S.Oh andL.-J.Li.Attention-basedgraphneuralnetworkforsemi-supervisedlearning.arXivpreprintarXiv:1803.03735 2018.[27]I.Tolstikhin S.Gelly O.Bousquet C.-J.Simon-Gabriel andB.Schölkopf.Adagan:Boostinggenerativemodels.arXivpreprintarXiv:1701.02386 2017.[28]M.Zaheer S.Kottur S.Ravanbakhsh B.Poczos R.R.Salakhutdinov andA.J.Smola.Deepsets.InAdvancesinNeuralInformationProcessingSystems pages3391–3401 2017.10,Zinan Lin
Ashish Khetan
Giulia Fanti
Sewoong Oh