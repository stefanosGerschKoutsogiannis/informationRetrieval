2013,Regret based Robust Solutions for Uncertain Markov Decision Processes,In this paper  we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of {\em maximin} policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed {\em minimax} regret as a suitable alternative to the {\em maximin} objective for robust optimization.  However  existing algorithms for handling {\em minimax} regret are restricted to models with uncertainty over rewards only.  We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state  action pairs and decision epochs; (c) Scalability and quality bounds. Finally  to demonstrate the empirical effectiveness of our sampling approaches  we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.,Regret based Robust Solutions for

Uncertain Markov Decision Processes

Asrar Ahmed

Singapore Management University

masrara@smu.edu.sg

Pradeep Varakantham

Singapore Management University

pradeepv@smu.edu.sg

Yossiri Adulyasak

Massachusetts Institute of Technology

yossiri@smart.mit.edu

Patrick Jaillet

Massachusetts Institute of Technology

jaillet@mit.edu

Abstract

In this paper  we seek robust policies for uncertain Markov Decision Processes (MDPs). Most
robust optimization approaches for these problems have focussed on the computation of maximin
policies which maximize the value corresponding to the worst realization of the uncertainty. Recent
work has proposed minimax regret as a suitable alternative to the maximin objective for robust op-
timization. However  existing algorithms for handling minimax regret are restricted to models with
uncertainty over rewards only. We provide algorithms that employ sampling to improve across mul-
tiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence
of model uncertainties across state  action pairs and decision epochs; (c) Scalability and quality
bounds. Finally  to demonstrate the empirical effectiveness of our sampling approaches  we pro-
vide comparisons against benchmark algorithms on two domains from literature. We also provide a
Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.

Introduction

Motivated by the difﬁculty in exact speciﬁcation of reward and transition models  researchers have
proposed the uncertain Markov Decision Process (MDP) model and robustness objectives in solving
these models. Given the uncertainty over the reward and transition models  a robust solution can
typically provide some guarantees on the worst case performance. Most of the research in comput-
ing robust solutions has assumed a maximin objective  where one computes a policy that maximizes
the value corresponding to the worst case realization [8  4  3  1  7]. This line of work has devel-
oped scalable algorithms by exploiting independence of uncertainties across states and convexity of
uncertainty sets. Recently  techniques have been proposed to deal with dependence of uncertain-
ties [15  6].
Regan et al. [11] and Xu et al. [16] have proposed minimax regret criterion [13] as a suitable alterna-
tive to maximin objective for uncertain MDPs. We also focus on this minimax notion of robustness
and also provide a new myopic variant of regret called Cumulative Expected Regret (CER) that
allows for development of scalable algorithms.
Due to the complexity of computing optimal minimax regret policies [16]   existing algorithms [12]
are restricted to handling uncertainty only in reward models and the uncertainties are independent
across states. Recent research has shown that sampling-based techniques [5  9] are not only efﬁcient
but also provide a priori (Chernoff-Hoefﬁding bounds) and a posteriori [14] quality bounds for
planning under uncertainty.
In this paper  we also employ sampling-based approaches to address restrictions of existing ap-
proaches for obtaining regret-based solutions for uncertain MDPs . More speciﬁcally  we make the

1

following contributions: (a) An approximate Mixed Integer Linear Programming (MILP) formu-
lation with error bounds for computing minimum regret solutions for uncertain MDPs  where the
uncertainties across states are dependent. We further provide enhancements and error bounds to
improve applicability. (b) We introduce a new myopic concept of regret  referred to as Cumulative
Expected Regret (CER) that is intuitive and that allows for development of scalable approaches.
(c) Finally  we perform a Sample Average Approximation (SAA) analysis to provide experimental
bounds for our approaches on benchmark problems from literature.

Preliminaries

We now formally deﬁne the two regret criterion that will be employed in this paper. In the deﬁnitions
below  we assume an underlying MDP  M = (cid:104)S A  T  R  H(cid:105) where a policy is represented as:
(cid:126)πt = {πt  πt+1  . . .   πH−1}  the optimal policy as (cid:126)π∗ and the optimal expected value as v0((cid:126)π∗).
The maximum reward in any state s is denoted as R∗(s) = maxa R(s  a). Throughout the paper 
we use α(s) to denote the starting state distribution in state s and γ to represent the discount factor.
Deﬁnition 1 Regret for any policy (cid:126)π0 is denoted by reg((cid:126)π0) and is deﬁned as:

reg((cid:126)π0) = v0((cid:126)π∗) − v0((cid:126)π0)  where v0((cid:126)π0) =

α(s) · v0(s  (cid:126)π0) 

(cid:88)

(cid:88)

πt(s  a) ·(cid:104)

vt(s  (cid:126)πt) =

R(s  a) + γ

a

s(cid:48)

(cid:88)

s

T (s  a  s(cid:48)) · vt+1(s(cid:48)  (cid:126)πt+1)

(cid:105)

Extending the deﬁnitions of simple and cumulative regret in stochastic multi-armed bandit prob-
lems [2]  we now deﬁne a new variant of regret called Cumulative Expected Regret (CER).
Deﬁnition 2 CER for policy (cid:126)π0 is denoted by creg((cid:126)π0) and is deﬁned as:

(cid:88)
(cid:88)

s

a

creg((cid:126)π0) =

cregt(s  (cid:126)πt) =

α(s) · creg0(s  (cid:126)π0)  where

(cid:88)
πt(s  a) ·(cid:2)R∗(s) − R(s  a) + γ
Proposition 1 For a policy (cid:126)π0 : 0 ≤ reg((cid:126)π0)− creg((cid:126)π0) ≤(cid:104)

s(cid:48)

The following properties highlight the dependencies between regret and CER.

T (s  a  s(cid:48)) · cregt+1(s(cid:48)  (cid:126)πt+1)(cid:3)

(1)

maxs R∗(s)− mins R∗(s)

(cid:105)· (1−γH )

1−γ

Proof Sketch1 By rewriting Equation (1) as creg((cid:126)π0) = v0 #((cid:126)π0) − v0((cid:126)π0)  we provide the proof.
Corollary 1 If ∀s  s(cid:48) ∈ S : R∗(s) = R∗(s(cid:48))  then ∀(cid:126)π0 : creg((cid:126)π0) = reg((cid:126)π0).
Proof. Substituting maxs R∗(s) = mins R∗(s) in the result of Proposition 1  we have creg((cid:126)π0) =
reg((cid:126)π0). (cid:4)

Uncertain MDP
A ﬁnite horizon uncertain MDP is deﬁned as the tuple of (cid:104)S A  T  R  H(cid:105). S denotes the set of
states and A denotes the set of actions. T = ∆τ (T ) denotes a distribution over the set of transition
functions T   where T t
k (s  a  s(cid:48)) denotes the probability of transitioning from state s ∈ S to state s(cid:48) ∈
S on taking action a ∈ A at time step t according to the kth element in T . Similarly  R = ∆ρ(R)
denotes the distribution over the set of reward functions R  where Rt
k(s  a  s(cid:48)) is the reinforcement
obtained on taking action a in state s and transitioning to state s(cid:48) at time t according to kth element
in R. Both T and R sets can have inﬁnite elements. Finally  H is the time horizon.
In the above representation  every element of T and R represent uncertainty over the entire horizon
and hence this representation captures dependence in uncertainty distributions across states. We now
provide a formal deﬁnition for the independence of uncertainty distributions that is equivalent to the
rectangularity property introduced in Iyengar et al. [4].

1Detailed proof provided in supplement under Proposition 1.

2

Deﬁnition 3 An uncertainty distribution ∆τ over the set of transition functions  T is independent
over state-action pairs at various decision epochs if

∆τ (T ) = ×s∈S a∈A t≤H ∆τ t

s a(T t

s a)  i.e. ∀k  P r∆τ (T k) =

P r∆τ t

s a

(T t

s a)

(cid:89)

where T = ×s a tT t
the set T t

s a t
s a is the set of transition functions for s  a  t; ∆τ t

s a is the distribution over
s a and P r∆τ (T k) is the probability of the transition function T k given the distribution ∆τ .

s a  T t

We can provide a similar deﬁnition for the independence of uncertainty distributions over the re-
ward functions.
In the following deﬁnitions  we include transition  T and reward  R models as
subscripts to indicate value (v)  regret (reg) and CER (creg) functions corresponding to a speciﬁc
MDP. Existing works on computation of maximin policies have the following objective:

πmaximin = arg max
(cid:126)π0

min

T∈T  R∈R

α(s) · v0

T R(s  (cid:126)π0)

(cid:88)

s

Our goal is to compute policies that minimize the maximum regret or cumulative regret over possible
models of transitional and reward uncertainty.

πreg = arg min
(cid:126)π0

max

T∈T  R∈R regT R((cid:126)π0); πcreg = arg min

(cid:126)π0

max

T∈T  R∈R cregT R((cid:126)π0)

Regret Minimizing Solution

We will ﬁrst consider the more general case of dependent uncertainty distributions. Our approach
to obtaining regret minimizing solution relies on sampling the uncertainty distributions over the
transition and reward models. We formulate the regret minimization problem over the sample set as
an optimization problem and then approximate it as a Mixed Integer Linear Program (MILP).
We now describe the representation of a sample and the deﬁnition of optimal expected value for
a sample  a key component in the computation of regret. Since there are dependencies amongst
uncertainties  we can only sample from ∆τ   ∆ρ and not from ∆τ t

s a. Thus  a sample is:

ξq = {(cid:10)T 0

q  R0

q

(cid:11)  (cid:10)T 1

q  R1

q

(cid:11)  ···(cid:10)T H−1

q

s a   ∆ρ t
 RH−1

q

(cid:11)}

q and Rt

where T t
q refer to the transition and reward model respectively at time step t in sample q .
Let (cid:126)πt represent the policy for each time step from t to H − 1 and the set of samples be denoted
by ξ. Intuitively  that corresponds to |ξ| number of discrete MDPs and our goal is to compute one
policy that minimizes the regret over all the |ξ| MDPs  i.e.
α(s) · [v∗

(s) − v0

(cid:88)

(s  (cid:126)π0)]

ξq

ξq

πreg = arg min
(cid:126)π0

max
ξq∈ξ

s

ξq

and v0
ξq

(s  (cid:126)π0) denote the optimal expected value and expected value for policy (cid:126)π0 re-

where v∗
spectively of the sample ξq.
Let  (cid:126)π0 be any policy corresponding to the sample ξq  then the expected value is deﬁned as follows:
(cid:48)
vt
ξq (s  (cid:126)πt) =
q (s  a  s

ξq (s  a  (cid:126)πt) = Rt

ξq (s  a  (cid:126)πt)  where vt

πt(s  a) · vt

  (cid:126)πt+1) · T t

(cid:88)

(cid:88)

q(s  a) + γ

(cid:48)
vt+1
ξq (s

)

a

s(cid:48)

The optimization problem for computing the regret minimizing policy corresponding to sample set
ξ is then deﬁned as follows:

(s  a  (cid:126)πt) = Rt

vt
ξq

q(s  a) + γ

(s(cid:48)  (cid:126)πt+1) · T t

q (s  a  s(cid:48))

vt+1
ξq

The value function expression in Equation (3) is a product of two variables  πt(s  a) and
vt
ξq

(s  a  (cid:126)πt)  which hampers scalability signiﬁcantly. We now linearize these nonlinear terms.

3

reg((cid:126)π0)

min
(cid:126)π0
s.t. reg((cid:126)π0) ≥ v∗

α(s) · v0

ξq

(s  (cid:126)π0)

vt
ξq

(s  (cid:126)πt) =

πt(s  a) · vt

(s  a  (cid:126)πt)

−(cid:88)
(cid:88)

s

ξq

a

ξq

(cid:88)

s(cid:48)

∀ξq
∀s  ξq  t
∀s  a  ξq  t

(2)

(3)

(4)

Mixed Integer Linear Program

The optimal policy for minimizing maximum regret in the general case is randomized. However  to
account for domains which only allow for deterministic policies  we provide linearization separately
for the two cases of deterministic and randomized policies.
Deterministic Policy: In case of deterministic policies  we replace Equation (3) with the following
equivalent integer linear constraints:

(s  (cid:126)πt) ≤ vt

vt
ξq

(s  a  (cid:126)πt) ; vt
ξq

ξq

(s  (cid:126)πt) ≤ πt(s  a) · M

(s  (cid:126)πt) ≥ vt

(s  a  (cid:126)πt) − (1 − πt(s  a)) · M ∀s  a  ξq  t

ξq

vt
ξq

M is a large positive constant that is an upper bound on vt
ξq
terms in Equation (3) can be veriﬁed by considering all values of πt(s  a).
Randomized Policy: When (cid:126)π0 is a randomized policy  we have a product of two continuous vari-
ables. We provide a mixed integer linear approximation to address the product terms above. Let 

(5)
(s  a  (cid:126)πt). Equivalence to the product

vt
ξq

(s  a  (cid:126)πt) + πt(s  a)

; Bt
ξq

(s  a  (cid:126)πt) =

vt
ξq

(s  a  (cid:126)πt) − πt(s  a)

2

At
ξq

(s  a  (cid:126)πt) =

2
Equation (3) can then be rewritten as:
(s  (cid:126)πt) =

vt
ξq

(cid:88)

a

[At
ξq

(s  a  (cid:126)πt)2 − Bt

ξq

(s  a  (cid:126)πt)2]

(6)

(s  a  (cid:126)πt) and hence for At
ξq

As discussed in the next subsection on “Pruning dominated actions”  we can compute upper and
lower bounds for vt
(s  a  (cid:126)πt). We approximate the
ξq
squared terms by using piecewise linear components that provide an upper bound on the squared
terms. We employ a standard method from literature of dividing the variable range into multiple
break points. More speciﬁcally  we divide the overall range of At
(s  a  (cid:126)πt))  say
ξq
[br0  brr] into r intervals by using r+1 points namely (cid:104)br0  br1  . . .   brr(cid:105). We associate a linear vari-
(s  a  (cid:126)πt)2)
able  λt
ξq
as follows:

(s  a  w) with each break point w and then approximate At
ξq

(s  a  (cid:126)πt)2 (and Bt
ξq

(s  a  (cid:126)πt) and Bt
ξq

(s  a  (cid:126)πt) (or Bt
ξq

(cid:88)
(cid:88)

w

At
ξq

(s  a  (cid:126)πt) =

λt
ξq

At
ξq

(cid:88)

(s  a  (cid:126)πt)2 =

w

λt
ξq

(s  a  w) = 1 

w
SOS2s a t

ξq

({λt

ξq

(s  a  w)}w≤r) 

(s  a  w) · brw 
(s  a  w) · (brw)2 

λt
ξq

∀s  a  ξq  t
∀s  a  ξq  t
∀s  a  ξq  t
∀s  a  ξq  t

(7)

(8)

(9)

where SOS2 is a construct which is associated with a set of variables of which at most two variables
can be non-zero and if two variables are non-zero they must be adjacent. Since any number in the
range lies between at most two adjacent points  we have the above constructs for the λt
(s  a  w)
ξq
variables. We implement the above adjacency constraints on λt
(s  a  w) using the CPLEX Special
ξq
Ordered Sets (SOS) type 22.

Proposition 2 Let [c d] denote the range of values for At
ξq
points that divide At
ξq
error δ < 
4 .

(s  a  (cid:126)πt)2 into r equal intervals of size  = d2−c2

r

(s  a  (cid:126)πt) and assume we have r + 1
then the approximation

Proof: Let the r + 1 points be br0  . . .   brr. By deﬁnition  we have (brw)2 = (brw−1)2 + . Because
of the convexity of x2 function  the maximum approximation error in any interval [brw−1  brw]
occurs at its mid-point3. Hence  approximation error δ is given by:

δ ≤ (brw)2 + (brw−1)2

2

−

 + 2 · brw−1 · (brw−1 − brw)

4

<


4

(cid:4)

(cid:20) brw + brw−1

(cid:21)2

2

=

2Using CPLEX SOS-2 considerably improves runtime compared to a binary variables formulation.
3Proposition and proof provided in supplement as footnote 3

4

Proposition 3 Let ˆvt
ξq

(s  (cid:126)πt) − |A| ·  · (1 − γH−1)

(s  (cid:126)πt) denote the approximation of vt
ξq
(s  (cid:126)πt) ≤ vt

≤ ˆvt

vt
ξq

ξq

(s  (cid:126)πt) +

ξq

(s  (cid:126)πt). Then

4 · (1 − γ)

|A| ·  · (1 − γH−1)

4 · (1 − γ)

Proof Sketch4: We use the approximation error provided in Proposition 2 and propagate it through
the value function update. (cid:4)
Corollary 2 The positive and negative errors in regret are bounded by |A|··(1−γH−1)
Proof. From Equation (2) and Proposition 3  we have the proof. (cid:4)
Since the break points are ﬁxed before hand  we can ﬁnd tighter bounds (refer to Proof of Proposi-
tion 2). Also  we can further improve on the performance (both run-time and solution quality) of the
MILP by pruning out dominated actions and adopting clever sampling strategies as discussed in the
next subsections.

4·(1−γ)

Pruning dominated actions

We now introduce a pruning approach5 to remove actions that will never be assigned a positive
probability in a regret minimization strategy. For every state-action pair at each time step  we deﬁne
a minimum and maximum value function as follows:

(cid:110)
(cid:110)
vt max
ξq
An action a(cid:48) is pruned if there exists the same action a over all samples ξq  such that

q(s  a) + γ(cid:80)
q(s  a) + γ(cid:80)

q (s  a  s(cid:48)) · vt+1 min
q (s  a  s(cid:48)) · vt+1 max

(s  a) = Rt
(s  a) = Rt

(s(cid:48)) ; vt min
(s(cid:48)) ; vt max

s(cid:48) T t
s(cid:48) T t

(cid:111)
(cid:111)

(s) = maxa

(s) = mina

vt max
ξq

vt min
ξq

vt min
ξq

(s  a)

(s  a)

ξq

ξq

ξq

ξq

vt min
ξq

(s  a) ≥ vt max

(s  a(cid:48)) ∃a  ∀ξq

ξq

The above pruning step follows from the observation that an action whose best case payoff is less
than the worst case payoff of another action a cannot be part of the regret optimal strategy  since we
could switch from a(cid:48) to a without increasing the regret value. It should be noted that an action that
is not optimal for any of the samples cannot be pruned.

Greedy sampling

The scalability of the MILP formulation above is constrained by the number of samples Q. So 
instead of generating only the ﬁxed set of Q samples from the uncertainty distribution over models 
we generate more than Q samples and then pick a set of size Q so that samples are “as far apart”
as possible. The key intuition in selecting the samples is to consider distance among samples as
being equivalent to entropy in the optimal policies for the MDPs in the samples. For each decision
epoch  t  each state s and action a  we deﬁne P rs a t
ξ (s  a) = 1) to be the probability that a is
the optimal action in state s at time t. Similarly  we deﬁne P rs a t

(π∗t

(π∗t

ξ

ξ (s  a) = 0):

ξ

P rs a t

ξ

(π∗t

ξ (s  a) = 1) =

∆S(ξ) = −(cid:88)

ξq

π∗t
ξq
Q

(s  a)

; P rs a t

ξ

(π∗t

ξ (s  a) = 0) =

ξ (s  a) = z) · ln(cid:0)P rs a t

ξ

(π∗t

P rs a t

ξ

Let the total entropy of sample set  ξ (|ξ| = Q) be represented as ∆S(ξ)  then

(cid:80)
(cid:88)

t s a

z∈{0 1}

(cid:17)

(s  a)

ξq

1 − π∗t

(cid:16)
(cid:80)
ξ (s  a) = z)(cid:1)

Q

ξq

(π∗t

We use a greedy strategy to select the Q samples  i.e. we iteratively add samples that maximize
entropy of the sample set in that iteration.
It is possible to provide bounds on the number of samples required for a given error using the
methods suggested by Shapiro et al. [14]. However these bounds are conservative and as we show
in the experimental results section  typically  we only require a small number of samples.

4Detailed proof in supplement under Proposition 3
5Pseudo code provided in the supplement under ”Pruning dominated actions” section.

5

CER Minimizing Solution

The MILP based approach mentioned in the previous section can easily be adapted to minimize the
maximum cumulative regret over all samples when uncertainties across states are dependent:

min
(cid:126)π0

creg((cid:126)π0)

s.t. creg((cid:126)π0) ≥(cid:88)

α(s) · creg0

ξq (s  (cid:126)πt) 
πt(s  a) · cregt

s

(cid:88)
ξq (s  (cid:126)πt) =
ξq (s  a  (cid:126)πt) = R∗ t

a

cregt

cregt

q (s) − Rt

q(s  a) + γ

ξq (s  a  (cid:126)πt) 

∀ξq

∀s  t  ξq

T t
q (s  a  s

(cid:48)

) · cregt+1

(cid:48)
ξq (s

  (cid:126)πt+1)  ∀s  a  t  ξq

(10)

(11)

(cid:88)

s(cid:48)

ξq

(s  a  (cid:126)πt) is approximated as described earlier.

where the product term πt(s  a) · cregt
While we were unable to exploit the independence of uncertainty distributions across states with
minimax regret  we are able to exploit the independence with minimax CER. In fact  a key advantage
of the CER robustness concept in the context of independent uncertainties is that it has the optimal
substructure over time steps and hence a Dynamic Programming(DP) algorithm can be used to solve
it.
In the case of independent uncertainties  samples at each time step can be drawn independently and
we now introduce a formal notation to account for samples drawn at each time step. Let ξt denote
the set of samples at time step t  then ξ = ×t≤H−1ξt. Further  we use (cid:126)ξt to indicate cross product
of samples from t to H − 1  i.e. (cid:126)ξt = ×t≤e≤H−1ξe. Thus  (cid:126)ξ0 = ξ. To indicate the entire horizon
samples corresponding to a sample p from time step t  we have (cid:126)ξt
(s  a) = R∗ t−1
For notational compactness  we use ∆Rt−1
(cid:104)
dence in uncertainties across time steps  for a sample set (cid:126)ξt−1

p × (cid:126)ξt+1.
p = ξt
(s) − Rt−1
(s  a). Because of indepen-
(cid:105)
p × (cid:126)ξt  we have the following:
p = ξt−1
(cid:88)
T t
(cid:48)
(cid:105)
(cid:88)
p (s  a  s

(s  (cid:126)πt−1) = max
ξt−1
p ×ξt

(cid:88)
(cid:88)

cregt−1
(cid:126)ξt−1

πt−1(s  a)

) · cregt

∆Rt−1

(s  a) + γ

(cid:48)
(cid:126)ξt (s

max
(cid:126)ξt−1
p

  (cid:126)πt)

(cid:104)

p

a

p

p

p

p

p

πt−1(s  a)

∆Rt−1

p

(s  a) + γ

s(cid:48)
T t
(cid:48)
p (s  a  s

) · max
q∈(cid:126)ξt
(cid:126)ξt

(cid:48)
(s

  (cid:126)πt)

cregt
(cid:126)ξt
q

= max
ξt−1

p

a

(12)
Proposition 4 At time step t − 1  the CER corresponding to any policy πt−1 will have least regret
if it includes the CER minimizing policy from t. Formally  if (cid:126)π∗ t represents the CER minimizing
policy from t and (cid:126)πt represents any arbitrary policy  then:

s(cid:48)

∀s : max

(cid:126)ξt−1
p ∈(cid:126)ξt−1

(cid:16)
s (cid:10)πt−1  (cid:126)π∗ t(cid:11)(cid:17) ≤ max

(cid:126)ξt−1
p ∈(cid:126)ξt−1
(s  (cid:126)π∗ t) ≤ max
q∈(cid:126)ξt
(cid:126)ξt

cregt
(cid:126)ξt
q

cregt
(cid:126)ξt
q

p

cregt−1
(cid:126)ξt−1
if  ∀s : max
q∈(cid:126)ξt
(cid:126)ξt

cregt−1
(cid:126)ξt−1

p

(s  (cid:126)πt)

(cid:16)

s (cid:10)πt−1  (cid:126)πt(cid:11)(cid:17)

(13)

(14)

(15)

Proof Sketch6 We prove this by using Equations (14) and (12) in LHS of Equation (13). (cid:4)
It is easy to show that minimizing CER also has an optimal substructure:

(cid:88)

s

min
(cid:126)π0

max

(cid:126)ξ0
p

α(s) · creg0
(cid:126)ξ0
p

(s  (cid:126)π0) =⇒ min

(cid:126)π0

α(s) ·(cid:2) max

(cid:88)

s

creg0
(cid:126)ξ0
p

(cid:126)ξ0
p

(s  (cid:126)π0)(cid:3)

In Proposition 4 (extending the reasoning to t = 1)  we have already shown that max(cid:126)ξ0
creg0
(cid:126)ξ0
p
has an optimal substructure. Thus  Equation (15) can also exploit the optimal substructure.
MINIMIZECER function below provides the pseudo code for a DP algorithm that exploits this struc-
ture. At each stage  t we calculate the creg for each state-action pair corresponding to each of the

(s  (cid:126)π0)

p

6Detailed proof in supplement under Proposition 4.

6

samples at that stage  i.e. ξt (lines 6-9). Once these are computed  we obtain the maximum creg and
the policy corresponding to it (line 10) using the GETCER() function. In the next iteration  creg
computed at t is then used in the computation of creg at t− 1 using the same update step (lines 6-9).
MINIMIZECER()
1: for all t ≤ H − 1 do
ξt ← GENSAMPLES(T  R)
2:
3: for all s ∈ S do
cregH (s) ← 0
4:
5: while t >= 0 do
for all s ∈ S do
6:
7:
8:
9:
10:
11:

GETCER (s {cregt
cregt(s) ≥(cid:88)
(cid:88)

q ∈ ξt  a ∈ A do
(s  a) ← ∆Rt

0 ≤ πt(s  a) ≤ 1 ∀a

for all ξt
cregt
ξt
q

(cid:10)πt  cregt(s)(cid:11) ← GETCER (s  {cregt

q (s  a  s(cid:48)) · cregt+1(s(cid:48))

s(cid:48) T t

q(s  a)+

πt(s  a) · cregt

(s  a)})

γ(cid:80)

(s  a)  ∀ξt

q

ξt
q

(s  a)})

ξt
q

cregt(s)

min

π

t ← t − 1

πt(s  a) = 1

ξt
q

a

a

return ( (cid:126)creg0  (cid:126)π0)

It can be noted that MINIMIZECER() makes only H · |S| calls to the LP in GETCER() function 
each of which has only |A| continuous variables and at most [1 + maxt |ξt|] number of constraints.
Thus  the overall complexity of MinimizeCER() is polynomial in the number of samples given ﬁxed
values of other attributes.
Let creg∗ H−1(s  a) denote the optimal cumulative regret at time step H − 1 for taking action a in
∗ H−1
(s  a) denote the optimal cumulative regret over the sample set ξ. Let indicator
state s and creg
ξ
(s  a) ≤ λ

(cid:40)

random variable  X be deﬁned as follows: X =

if creg∗ H−1(s  a) − creg
otherwise

∗ H−1
ξ

1
0

By using Chernoff and Hoeffding bounds on X  it is possible to provide bounds on deviation from
mean and on the number of samples at H − 1. This can then be propagated to H − 2 and so on.
However  these bounds can be very loose and they do not exploit the properties of creg functions.
Bounds developed on spacings of order statistics can help exploit the properties of creg functions.
We will leave this for future work.

Experimental Results

In this section  we provide performance comparison of various algorithms introduced in previous
sections over two domains. MILP-Regret refers to the randomized policy variant of the MILP ap-
proximation algorithm for solving uncertain MDPs with dependent uncertainties. Similar one for
minimizing CER is referred to as MILP-CER. We refer to the dynamic programming algorithm
for minimizing CER in the independent uncertainty case as DP-CER and ﬁnally  we refer to the
maximin value algorithm as “Maximin”. All the algorithms ﬁnished within 15 minutes on all the
problems. DP-CER was much faster than other algorithms and ﬁnished within a minute on the
largest problems.
We provide the following results in this section:

(1) Performance comparison of Greedy sampling and Random sampling strategies in the context of

MILP-Regret as we increase the number of samples.

(2) SAA analysis of the results obtained using MILP-Regret.
(3) Comparison of MILP-Regret and MILP-CER policies with respect to simulated regret.
(4) Comparison of DP-CER and Maximin.

The ﬁrst three comparisons correspond to the dependent uncertainties case and the results are based
on a path planning problem that is motivated by disaster rescue and is a modiﬁcation of the one
employed in Bagnell et al. [1]. On top of normal transitional uncertainty  we have uncertainty over
transition and reward models due to random obstacles and random reward cells. Furthermore  these
uncertainties are dependent on each other due to patterns in terrains. Each sample of the various
uncertainties represents an individual map and can be modelled as an MDP. We experimented with

7

(a)

(b)

(c)

Figure 1: In (a) (b) we have 4 × 4 grid  H = 5. In (c)  the maximum inventory size (X) = 50 
H = 20  |ξt| = 50. The normal distribution mean µ = {0.3  0.4  0.5} · X and σ ≤ min{µ X−µ}

3

a grid world of size 4x4 while varying numbers of obstacles  reward cells  horizon and the number
of break points employed (3-6).
In Figure 1a  we show the effect of using greedy sampling strategy on the MILP-Regret policy. On
X-axis  we represent the number of samples used for computation of policy (learning set). The test
set from which the samples were selected consisted of 250 samples. We then obtained the policies
using MILP-Regret corresponding to the sample sets (referred to as learning set) generated by using
the two sampling strategies. On Y-axis  we show the percentage difference between simulated regret
values on test and learning sample sets. We observe that for a ﬁxed difference  the number of samples
required by greedy is signiﬁcantly lower in comparison to random. Furthermore  the variance in
difference is also much lower for greedy. A key result from this graph is that even with just 15
samples  the difference with actual regret is less than 10%.
Figure 1b shows that even the gap obtained using SAA analysis7 is near zero (< 0.1) with 15
samples. We have shown the gap and variance on the gap over three different settings of uncertainty
labeled 1 2 and 3. Setting 3 has the highest uncertainty over the models and Setting 1 has the least
uncertainty. The variance over the gap was higher for higher uncertainty settings.
While MILP-CER obtained a simulated regret value (over 250 samples) within the bound provided
in Proposition 1  we were unable to ﬁnd any correlation in the simulated regret values of MILP-
Regret and MILP-CER policies as the samples were increased. We have not yet ascertained a reason
for there being no correlation in performance.
In the last result shown in Figure 1c  we employ the well known single product ﬁnite horizon stochas-
tic inventory control problem [10]. We compare DP-CER against the widely used benchmark algo-
rithm on this domain  Maximin. The demand values at each decision epoch were taken from a
normal distribution. We considered three different settings of mean and variance of the demand.
As expected  the DP-CER approach provides much higher values than maximin and the difference
between the two reduced as the cost to revenue ratio increased. We obtained similar results when
the demands were taken from other distributions (uniform and bi-modal).

Conclusions

We have introduced scalable sampling-based mechanisms for optimizing regret and a new variant of
regret called CER in uncertain MDPs with dependent and independent uncertainties across states.
We have provided a variety of theoretical results that indicate the connection between regret and
CER  quality bounds on regret in case of MILP-Regret  optimal substructure in optimizing CER for
independent uncertainty case and run-time performance for MinimizeCER. In the future  we hope to
better understand the correlation between regret and CER  while also understanding the properties
of CER policies.
Acknowledgement This research was supported in part by the National Research Foundation Sin-
gapore through the Singapore MIT Alliance for Research and Technologys Future Urban Mobility
research programme. The last author was also supported by ONR grant N00014-12-1-0999.

7We have provided the method for performing SAA analysis in the supplement.

8

510152025Samples00.10.20.30.40.50.60.7DiﬀerenceRandomGreedySampling Strategies (MILP-Regret)510152025Samples−1−0.500.511.522.5Gap123SAA Analysis0.10.20.30.40.50.60.70.80.91Cost-to-revenue ratio0100200300400PerformanceDP-CER (0.3)Maximin (0.3)DP-CER (0.5)Maximin (0.5)DP-CER (0.7)Maximin (0.7)DP-CER AnalysisReferences
[1] J. Andrew Bagnell  Andrew Y. Ng  and Jeff G. Schneider. Solving uncertain markov decision

processes. Technical report  Carnegie Mellon University  2001.

[2] S´ebastien Bubeck  R´emi Munos  and Gilles Stoltz. Pure exploration in multi-armed bandits
problems. In Proceedings of the 20th international conference on Algorithmic learning theory 
Algorithmic Learning Theory  2009.

[3] Robert Givan  Sonia Leach  and Thomas Dean. Bounded-parameter markov decision pro-

cesses. Artiﬁcial Intelligence  122  2000.

[4] G. Iyengar. Robust dynamic programming. Mathematics of Operations Research  30  2004.
[5] Michael Kearns  Yishay Mansour  and Andrew Y. Ng. A sparse sampling algorithm for near-

optimal planning in large markov decision processes. Machine Learning  49  2002.

[6] Shie Mannor  Oﬁr Mebel  and Huan Xu. Lightning does not strike twice: Robust MDPs with

coupled uncertainty. In International Conference on Machine Learning (ICML)  2012.

[7] Andrew Mastin and Patrick Jaillet. Loss bounds for uncertain transition probabilities in markov
decision processes. In IEEE Annual Conference on Decision and Control (CDC)  2012  2012.
[8] Arnab Nilim and Laurent El Ghaoui. Ghaoui  l.: Robust control of markov decision processes

with uncertain transition matrices. Operations Research  2005.

[9] Joelle Pineau  Geoffrey J. Gordon  and Sebastian Thrun. Point-based value iteration: An
anytime algorithm for POMDPs. In International Joint Conference on Artiﬁcial Intelligence 
2003.

[10] Martin Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley and Sons  1994.

[11] Kevin Regan and Craig Boutilier. Regret-based reward elicitation for markov decision pro-

cesses. In Uncertainty in Artiﬁcial Intelligence  2009.

[12] Kevin Regan and Craig Boutilier. Robust policy computation in reward-uncertain MDPs using

nondominated policies. In National Conference on Artiﬁcial Intelligence (AAAI)  2010.

[13] Leonard Savage. The Foundations of Statistics. Wiley  1954.
[14] A. Shapiro. Monte carlo sampling methods. In Stochastic Programming  volume 10 of Hand-

books in Operations Research and Management Science. Elsevier  2003.

[15] Wolfram Wiesemann  Daniel Kuhn  and Ber Rustem. Robust markov decision processes.

Mathematics of Operations Research  38(1)  2013.

[16] Huan Xu and Shie Mannor. Parametric regret in uncertain markov decision processes. In IEEE

Conference on Decision and Control  CDC  2009.

9

,Asrar Ahmed
Pradeep Varakantham
Yossiri Adulyasak
Patrick Jaillet
Guobin Chen
Wongun Choi
Xiang Yu
Tony Han
Manmohan Chandraker