2015,Local Smoothness in Variance Reduced Optimization,Abstract We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems  our methods exploit data dependent local smoothness of the loss functions near the optimum  while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically  we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways  which perform even better in practice.,LocalSmoothnessinVarianceReducedOptimizationDanielVainsencher HanLiuTongZhangDept.ofOperationsResearch&FinancialEngineeringDept.ofStatisticsPrincetonUniversityRutgersUniversityPrinceton NJ08544Piscataway NJ 08854{daniel.vainsencher han.liu}@princeton.edutzhang@stat.rutgers.eduAbstractWeproposeafamilyofnon-uniformsamplingstrategiestoprovablyspeedupaclassofstochasticoptimizationalgorithmswithlinearconvergenceincludingStochasticVarianceReducedGradient(SVRG)andStochasticDualCoordinateAscent(SDCA).Foralargefamilyofpenalizedempiricalriskminimizationprob-lems ourmethodsexploitdatadependentlocalsmoothnessofthelossfunctionsneartheoptimum whilemaintainingconvergenceguarantees.Ourboundsaretheﬁrsttoquantifytheadvantagegainedfromlocalsmoothnesswhicharesigniﬁcantforsomeproblemssigniﬁcantlybetter.Empirically weprovidethoroughnumer-icalresultstobackupourtheory.Additionallywepresentalgorithmsexploitinglocalsmoothnessinmoreaggressiveways whichperformevenbetterinpractice.1IntroductionWeconsiderminimizationoffunctionsofformP(w)=n−1nXi=1φi(cid:0)x⊤iw(cid:1)+R(w)wheretheconvexφicorrespondstoalossofwonsomedataxi RisaconvexregularizerandPisµstronglyconvex sothatP(w′)≥P(w)+hw′−w ▽P(w)i+µ2kw′−wk2.Inaddition weassumeeachφiissmoothingeneralandnearﬂatinsomeregion;examplesincludeSVM regressionwiththeabsoluteerrororεinsensitiveloss smoothapproximationsofthose andalsologisticregression.Stochasticoptimizationalgorithmsconsideronelossφiatatime chosenatrandomaccordingtoadistributionptwhichmaychangeovertime.Recentalgorithmscombineφiwithinformationaboutpreviouslyseenlossestoacceleratetheprocess achievinglinearconvergencerate includingStochasticVarianceReducedGradient(SVRG)[2] StochasticAveragedGradient(SAG)[4] andStochasticDualCoordinateAscent(SDCA)[6].TheexpectednumberofiterationsrequiredbythesealgorithmsisofformO(cid:0)(n+L/µ)log(cid:0)ε−1(cid:1)(cid:1)whereLisaLipschitzconstantofalllossgradients▽φi measuringtheirsmoothness.Difﬁcultproblems havingaconditionnumberL/µmuchlargerthann arecalledillconditioned andhavemotivatedthedevelopmentofacceleratedalgorithms[5 8 3].Someofthesealgorithmshavebeenadaptedtoallowimportancesamplingwhereptisnonuniform;theeffectonconvergenceboundsistoreplacetheuniformboundLdescribedabovebyLavg theaverageoverLi lossspeciﬁcLipschitzbounds.Inpractice foranimportantclassofproblems alargeproportionofφineedtobesampledonlyveryfewtimes andothersindeﬁnitely.AsanexamplewetakeaninstanceofsmoothSVM withµ=n−1andL≈30 solvedviastandardSDCA.InFigure1weobservethedecayofanupperboundontheupdatespossiblefordifferentsamples wherechoosingasamplethatiswhiteproducesnoupdate.Thelargemajorityoftheﬁgureiswhite indicatingwastedeffort.For95%oflosses thealgorithmcapturedallrelevantinformationafterjust3visits.Sincethenonwhitezoneisnearlyconstantovertime detectingandfocusingonthefewimportantlossesshouldbepossible.This1representsbothasuccessofSDCAandasigniﬁcantroomforimprovement asfocusingjusthalftheeffortontheactivelosseswouldincreaseeffectivenessbyafactorof10.SimilarphenomenaoccurundertheSVRGandSAGalgorithmsaswell.Butisthephenomenonspeciﬁctoasingleproblem orgeneral?forwhatproblemscanweexpectthesetofusefullossestobesmallandnearconstant?Figure1:SDCAonsmoothedSVM.DualresidualsupperboundtheSDCAupdatesize;whiteindicateszerohencewastedeffort.Thedualresidualsquicklybecomesparse;thesupportisstable.Allowingpttochangeovertime thephenomenondescribedindeedcanbeexploited;Figure2showssigniﬁcantspeedupsobtainedbyourvariantsofSVRGandSDCA.ComparisonsonotherdatasetsaregiveninSection4.Themechanismbywhichspeedupisobtainedisspeciﬁctoeachal-gorithm buttheunderlyingphenomenonweexploitisthesame:manyproblemsaremuchsmootherlocallythanglobally.Firstconsiderasinglesmoothedhingelossφi asusedinsmoothedSVMwithsmoothingparameterγ.Thenon-smoothnessofthehingelossisspreadinφioveranintervaloflengthγ asillustratedinFigure3andgivenbyφi(a)=0a>11−a−γ/2a<1−γ(a−1)2/(2γ)otherwise.TheLipschitzconstantofddaφi(a)isγ−1 henceitentersintotheglobalestimateofconditionnum-berLavgasLi=kxik/γ;henceapproximatingthehingelossmoreprecisely withasmallerγ makestheproblemsstrictlymoreillconditioned.Butoutsidethatintervaloflengthγ φicanbelocallyapproximatedasafﬁne havingaconstantgradient;intoacorrectexpressionoflocalcondi-tioning sayonintervalBintheﬁgure itshouldcontributenothing.Sosmallerγcansometimesmaketheproblem(locally)betterconditioned.AsetIoflosseshavingconstantgradientsoverasubsetofthehypothesisspacecanbesummarizedforpurposesofoptimizationbyasingleafﬁne05001000150020002500300035004000Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySVRG solving smoothed hinge loss SVM on MNIST 0/1. Loss gradient is 3.33e+01 Lip. smooth. 6.77e-05 strong convexity.Uniform sampling ([2])Global smoothness sampling ([7])Local SVRG (Alg. 1)Empirical Affinity SVRG (Alg. 4)050100150200250300350Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySDCA solving smoothed hinge loss SVM on MNIST 0/1. Loss gradient is 3.33e+01 Lip. smooth. 6.77e-05 strong convexity.Uniform sampling ([6])Global smoothness sampling ([10])Affine-SDCA (Alg. 2)Empirical ¢ SDCA (Alg. 3)Figure2:OntheleftweseevariantsofSVRGwithη=1/(8L) ontherightvariantsofSDCA.2Figure3:Alossφithatisnearﬂat(Hessianvanishes nearconstantgradient)ona“ball”B⊂R.Bwithradius2rkxikisinducedbythe(Euclidean)ballofhypothesesB(wt r) thatweproveincludesw∗.Thenthelossφidoesnotcontributetocurvatureintheregionofinterest andanafﬁnemodelofthesumofsuchφionBcanreplacesamplingfromthem.Weﬁndrinalgorithmsbycombiningstrongconvexitywithquantitiessuchasdualitygaporgradientnorm.function sosamplingfromIshouldnotbenecessary.ItsohappensthatSAG SVRGandSDCAnaturallydosuchmodeling henceneedonlylightmodiﬁcationstorealizesigniﬁcantgains.WeprovidethedetailsforSVRGinSection2(theSAGcaseissimilar)andforSDCAinSection3.Otherlosses whilenowhereafﬁne arelocallysmooth:thelogisticregressionlosshasgradientswithlocalLipschitzconstantsthatdecayexponentiallywithdistancefromahyperplanedependentonxi.Forsuchlosseswecannotforgosamplinganyφipermanently butwecanstillobtainboundsbeneﬁttingfromlocalsmoothnessforanSVRGvariant.Nextwedeﬁneformallytherelevantgeometricpropertiesoftheoptimizationproblemandrelatethemtoprovableconvergenceimprovementsoverexistinggenericbounds;wegivedetailedboundsinthesequel.ThroughoutB(c r)isaEuclideanballofradiusraroundc.Deﬁnition1.WeshalldenoteLi r=maxw∈B(w∗ r)(cid:13)(cid:13)▽2φi(cid:0)x⊤i·(cid:1)(cid:13)(cid:13)2whichisalsotheuniformLipschitzcoefﬁcientof▽φithatholdatdistanceatmostrfromw∗.Remark2.Algorithmswillusesimilarquantitiesnotdependentonknowingw∗suchas˜Li raroundaknown˜w.Deﬁnition3.WedeﬁnetheaverageballsmoothnessfunctionS:R→Rofaproblemby:S(r)=nXi=1Li ∞/nXi=1Li r.InTheorem5weseethatAlgorithm1requiresfewerstochasticgradientsamplestoreducelosssub-optimalitybyaconstantfactorthanSVRGwithimportancesamplingaccordingtoglobalsmooth-ness.Onceithascertiﬁedthattheoptimumw∗iswithinrofthecurrentiteratew0itusesS(2r)timeslessstochasticgradientsteps.Thenextmeasuresimilarlyincreaseswhenmanylossesareafﬁneonaballaroundtheoptimum.Deﬁnition4.WedeﬁnetheballafﬁnityfunctionS:R→[0 n]ofaproblemby:A(r)= n−1nXi=11{Li r>0}!−1.InTheorem10weseesimilarlythatAlgorithm2requiresfeweraccessesofφitoreducethedualitygaptoanyε>0thanSDCAwithimportancesamplingaccordingtoglobalsmoothness.Onceithascertiﬁedthattheoptimumiswithindistancerofthecurrentprimaliteratew=w(cid:0)α0(cid:1)itaccessesA(2r)timesfewerφi.Inbothcases localsmoothnessandafﬁnityenableustofocusaconstantportionofsamplingeffortonthefewerlossesstillchallengingneartheoptimum;whenthesearefew theratios(andhence3algorithmicadvantage)arelarge.Weobtaintheseprovablespeedupsoveralreadyfastalgorithmsbyusingthatlocalsmoothnesswhichwecancertify.FornonsmoothlossessuchasSVMandandabsolutelossregression wecansimilarlyignoreirrelevantlosses leadingtosigniﬁcantpracticalimprovements;thecurrenttheoryforsuchlossesisinsufﬁcienttoquantifythespeedupsaswedoforsmoothlosses.Weobtainalgorithmsthataresimplerandsometimesmuchfasterbyusingthemorequalitativeobservationthatasiteratestendtoanoptimum thesetofrelevantlossesisgenerallystableandshrinking.Thenalgorithmscanestimatethesetofrelevantlossesdirectlyfromquantitiesobservedinperformingstochasticiterations sidesteppingtheloosenessofestimatingr.Therearetwopreviousworksinthisgeneraldirection.Theﬁrstpaperworkcombiningnon-uniformsamplingandempiricalestimationoflosssmoothnessis[4].TheynoteexcellentempiricalperformanceonavariantofSAG butwithouttheoryensuringconvergence.Weprovidesimilarlyfast(andboundfree)variantsofSDCA(Section3.2)andSVRG(Section2.2).AdynamicimportancesamplingvariantofSDCAwasreportedin[1]withoutrelationtolocalsmoothness;wediscusstheconnectioninSection3.2LocalsmoothnessandgradientdescentalgorithmsInthissectionwedescribehowSVRG incontrasttotheclassicalstochasticgradientdescent(SGD) naturallyexposeslocalsmoothnessinlosses.ThenwepresenttwovariantsofSVRGthatrealizethesegains.WebeginbyconsideringasinglelosswhenclosetotheoptimumandforsimplicityassumeR≡0.AssumeasmallballB=B(w r)aroundourcurrentestimatewincludesaroundtheoptimumw∗ andBiscontainedinaﬂatregionofφi andthisholdsforalargeproportionofthenlosses.SGDanditsdescendentSVRG(withimportancesampling)useupdatesofformwt+1=wt−ηvti/(pin) whereEi∼pvti/(pin)=▽F(wt)isanunbiasedestimatorofthefullgradientofthelosstermF(w)=n−1Pni=1φi(cid:0)x⊤iw(cid:1).SVRGusesvti=(cid:0)▽φi(cid:0)x⊤iwt(cid:1)−▽φi(cid:0)x⊤i˜w(cid:1)(cid:1)/(pin)+▽F(˜w)where˜wissomereferencepoint withtheadvantagethatvtihasvariancethatvanishesaswt ˜w→w∗.Wepointoutinadditionthatwhen˜w wt∈Band▽φi(cid:0)x⊤i·(cid:1)isconstantonBtheeffectsofsamplingφicancelsoutandvti=▽F(˜w).Inparticular wecansetpti=0withnolossofinfor-mation.Moregenerallywhen▽φi(cid:0)x⊤i·(cid:1)isnearconstantonB(smallLi r)thedifferencebetweenthesampledvaluesof▽φiinvtiisverysmallandpticanbesimilarlysmall.Weformalizethisinthenextsection wherewelocalizeexistingtheorythatappliedimportancesamplingtoadaptSVRGstaticallytolosseswithvariedglobalsmoothness.2.1TheLocalSVRGalgorithmHalvingthesuboptimalityofasolutionusingSVRGhastwoparts:computinganexactgradientatareferencepoint andperformingmanystochasticgradientdescentsteps.Thesamplingdistribution stepsizeandnumberofiterationsinthelatteraredeterminedbysmoothnessofthelosses.Algorithm1 Local-SVRG replacestheglobalboundsongradientchangeLiwithlocalonesLi r madevalidbyrestrictingiterationstoasmallballcertiﬁedtocontaintheoptimum.Thisallowsustoleveragepreviousalgorithmsandanalysis maintainingpreviousguaranteesandimprovingonthemwhenS(r)islarge.ForthissectionweassumeP=F;asintheinitialversionofSVRG[2] wemayincorporateasmoothregularizer(thoughinadifferentway explainedlater).Thisallowsustoapplytheex-istingProx-SVRGalgorithm[7]anditstheory;insteadofusingtheproximaloperatorforﬁxedregularization weuseittolocalize(byprojections)thestochasticdescenttoaballBaroundtheref-erencepoint˜wseeAlgorithm1.ThenthetheorydevelopedaroundimportancesamplingandglobalsmoothnessappliestosharperlocalsmoothnessestimatesthatholdonB(ignoringφiwhichareafﬁneonBisaspecialcase).Thisallowsforfewerstochasticiterationsandusingalargerstepsize obtainingspeedupsthatareproblemdependentbutoftenlargeinlatestages;seeFigure2.Thisisformalizedinthefollowingtheorem.4Algorithm1LocalSVRGisanapplicationofProxSVRGwith˜wdependentregularization.Thisportionreducessuboptimalitybyaconstantfactor applyiterativelytominimizeloss.1.Compute˜v=▽F(˜w)2.Deﬁner=2µk˜vk R(w)=iB(˜w r)=(cid:26)0w∈B(˜w r)∞otherwise(byµstrongconvexity w∗∈B(˜w r))3.Foreachi compute˜Li r=maxw∈B(˜w r)▽2φi(cid:0)x⊤iw(cid:1)4.Deﬁneaprobabilitydistribution:pi∝˜Li r weightedLipschitzconstant˜Lp=maxi˜Li r/(npi)andstepsizeη=116˜Lp.5.ApplytheinnerloopofProx-SVRG:(a)Setw0=˜w(b)Fort∈{1 ... m}:i.Chooseit∼pii.Computevt=(cid:0)▽φit(cid:0)wt−1(cid:1)−▽φit(˜x)(cid:1)/(npit)+˜viii.wt=proxηR(cid:0)wt−1−ηvt(cid:1)(c)Returnˆw=m−1Pt∈[m]wtTheorem5.Let˜wbeaninitialsolutionsuchthat▽F(˜w)certiﬁesthatw∗∈B=B(˜w r).Algorithm1ﬁndsˆwwithEF(ˆw)−F(w∗)≤(F(˜w)−F(w∗))/2usingO(d(n+m))time wherem=128µn−1Pni=1Li 2r+3.Remark6.Inthedifﬁcultcasethatisillconditionedevenlocallysothat128n−1Pni=1Li 2r≫nµ thetermnisnegligibleandtheratiobetweencomplexitiesofAlgorithm1andanSVRGusingglobalsmoothnessapproachesS(2r).Proof.Intheinitialpassonthedata compute▽F(˜w) rand˜Li r≤Li 2r.WethenapplyasingleroundofAlgorithmProx-SVRGof[7] withtheregularizerR(x)=χB(˜w r)localizingaroundthereferencepoint.ThenwemayapplyTheorem1of[7]withlocal˜Li rinsteadoftheglobalLirequiredthereforgeneralproximaloperators.Thisallowsustousethecorrespondinglargerstepsizeη=116Lp=116n−1Pni=1˜Li r.Remark7.Theuseofprojections(hencetherestrictiontosmoothregularization)isnecessarybe-causethelocalsmoothnessisrestrictedtoB andventuringoutsideBwithalargestepsizemaycompromiseconvergenceentirely.WhileexcursionsoutsideBaredifﬁculttocontrolintheory inpracticeskippingtheprojectionentirelydoesnotseemtohurtconvergence.Informally steppingfarfromBrequiresmovingconsistentlyagainst▽F whichisanunlikelyevent.Remark8.Thetheoryrequiresmstochasticstepsperexactgradienttoguaranteeanyimprovementatall butforillconditionedproblemsthisisoftenverypessimistic.Inpractice theﬁrstO(n)stochasticstepsafteranexactgradientprovidemostofthebeneﬁt.Inthisheuristicscenario thecomputationalbeneﬁtofTheorem5isthroughthesamplingdistributionandthelargerstepsize.Enlargingthestepsizewithoutaccompanyingtheoryoftengainsacorrespondingspeeduptoacertainprecisionbuttheriskofnonconvergencematerializesfrequently.While[2]incorporatedasmoothRbyaddingittoeverylossfunction thiscouldreducethesmooth-ness(increase˜Li r)inherentinthelosseshencereducingthebeneﬁtsofourapproach.WeinsteadproposetoaddasinglelossfunctiondeﬁnedasnR;thatthisisnotofformφi(cid:0)x⊤iw(cid:1)posesnorealdifﬁcultybecauseLocal-SVRGdependsonlossesonlythroughtheirgradientsandsmoothness.Themaindifﬁcultywiththeapproachofthissectionisthatinearlystagesrislarge inpartbecauseµisoftenverysmall(µ=n−αforα∈{0.5 1}arecommonchoices) leadingtoloosebounds5on˜Li r.Insomecasesthespeedupisonlyobtainedwhentheprecisionisalreadysatisfactory;weconsideralessconservativeschemeinthenextsection.2.2TheEmpiricalAfﬁnitySVRGalgorithmLocal-SVRGreliesonlocalsmoothnesstocertifythatsome∆ti=(cid:13)(cid:13)▽φi(cid:0)x⊤iwt(cid:1)−▽φi(cid:0)x⊤i˜w(cid:1)(cid:13)(cid:13)aresmall.Incontrast EmpiricalAfﬁnitySVRG(Algorithm4)takes∆ti>ttobeevidencethatalossisactive;when∆ti=0severaltimes thatisevidenceoflocalafﬁnityoftheloss henceitcanbesampledlessoften.Thisstrategydeemphasizeslocallyafﬁnelossesevenwhenristoolargetocertifyit therebyfocusesworkontherelevantlossesmuchearlier.HalfofthetimewesampleproportionaltotheglobalboundsLiwhichkeepsestimatesof∆ticurrent andalsoboundsthevariancewhensome∆tiincreasesfromzerotopositive.Abeneﬁtofusing∆tiisthatitisobservedateverysampleofiwithoutadditionalwork.PseudocodefortheslightlylongAlgorithm4isinthesupplementarymaterialforspacereasons.3StochasticDualCoordinateAscent(SDCA)TheSDCAalgorithmsolvesPthroughthedualproblemD(α)=−n−1nXi=1φ∗i(−αi)+R∗(w(α))wherew(α)=▽R∗(cid:0)1λnPni=1xiαi(cid:1).Ateachiteration SDCAchoosesiatrandomaccordingtopt andupdatestheαicorrespondingtothelossφitoincreaseD.Thisschemehasbeenusedforparticularlossesbefore andwasanalyzedin[6]obtaininglinearratesforgeneralsmoothlosses uniformsamplingandl2regularization andrecentlygeneralizedin[10]tootherregularizersandgeneralsamplingdistributions.Inparticular [10]showimprovedboundsandperformancebystati-callyadaptingtotheglobalsmoothnesspropertiesoflosses;usingadistributionpi∝1+Li(nµ)−1 itsufﬁcestoperformO(cid:16)(cid:16)n+Lavgµ(cid:17)log(cid:16)(cid:16)n+Lavgµ(cid:17)ε−1(cid:17)(cid:17)iterationstoobtainanexpecteddu-alitygapofatmostε.WhileSDCAisverydifferentfromgradientdescentmethods itsharesthepropertythatwhenthecurrentstateofthealgorithm(intheformofαi)alreadymatchesthederiva-tiveinformationforφi theupdatedoesnotrequireφiandcanbeskipped.Aswe’veseeninFigure1 manylossesconvergeαi→α∗iveryquickly;wewillshowthatlocalafﬁnityisasufﬁcientcondition.3.1TheAfﬁne-SDCAalgorithmThealgorithmicapproachforexploitinglocallyafﬁnelossesinSDCAisverydifferentfromthatforgradientdescentstylealgorithms;forsomeafﬁnelosseswecertifyearlythatsomeαiareintheirﬁnalform(seeLemma9)andhenceforthignorethem.Thisappliesonlytolocallyafﬁne(notjustsmooth)losses butunlikeLocal-SVRG doesnotrequiremodifyingthealgorithmforexplicitlocalization.Weuseareductiontoobtainimprovedrateswhilereusingthetheoryof[9]fortheremainingpoints.TheseresultsarestatedforsquaredEuclideanregularization butholdforstronglyconvexRasin[10].Lemma9.Letwt=w(αt)∈B(w∗ r) andlet{gi}=Sw∈B(wt r)φ′i(cid:0)x⊤iw(cid:1);inotherwords φi(cid:0)x⊤i·(cid:1)isafﬁneonB(wt r)whichincludesw∗.Thenwecancomputetheoptimalvalueα∗i=−gi.Proof.AsstatedinSection7of[6] foreachi wehave−α∗i=φ′i(cid:0)x⊤iw∗(cid:1).Thenifφ′i(cid:0)x⊤iw(cid:1)isaconstantsingletononB(wt r)containingw∗ theninparticularthatis−α∗i.ThelemmaenablesAlgorithm2toignoreagrowingproportionoflosses.Theoverallconvergencethisenablesisgivenbythefollowing.6Algorithm2Afﬁne-SDCA:adaptingtolocallyafﬁneφi withspeedupapproximatelyA(r).1.α0=0∈Rn I0=∅.2.Forτ∈{1 ...}:(a)˜wτ=w(cid:0)α(τ−1)m(cid:1);Computerτ=q2(cid:0)P(˜wτ)−D(cid:0)α(τ−1)m(cid:1)(cid:1)/µ(b)ComputeIτ=ni:(cid:12)(cid:12)(cid:12)Sw∈B(wτ r)φ′i(cid:0)x⊤i˜wτ(cid:1)(cid:12)(cid:12)(cid:12)=1o(c)Fori∈Iτ\Iτ−1:α(τ−1)ni=−φ′i(cid:0)x⊤i˜wτ(cid:1)(d)pτi∝(0i∈Iτ1+Li(nµ)−1otherwise si=(cid:26)0i∈Iτs/pτiotherwise(e)Fort∈[(τ−1)m+1 τm]:i.Chooseit∼pτii.Compute∆αtit=sit·(cid:0)φ′it(cid:0)x⊤itw(αt)(cid:1)−αt−1it(cid:1)iii.αtj=(αt−1j+∆αtjj=itαt−1jotherwiseTheorem10.IfatepochτAlgorithm2isatdualitygapετ itwillachieveexpecteddualitygapεinatmost(cid:16)n′+A−1(2r)L′avgµ(cid:17)log(cid:16)(cid:16)n′+A−1(2r)L′avgµ(cid:17)ετε(cid:17)iterations wheren′=n−|Iτ|andL′avg=n′−1Pi∈[n]\IτLiµ.Remark11.AssumingLi=Lforsimplicity andrecallingA(2r)≤n/n′ weﬁndthenumberofiterationsisreducedbyafactorofatleastA(2r) comparedtousingpi∝1+Li(nµ)−1.Incontrast thecostofthesteps2ato2daddedbyAlgorithm2isatmostafactorofO((m+n)/m) whichmaybedriventowardsonebythechoiceofm.Recentwork[1]modiﬁedSDCAfordynamicimportancesamplingdependentonthesocalleddualresidual:κi=αi+φ′i(cid:0)x⊤iw(α)(cid:1)(wherebyφ′i(w)werefertothederivativeofφiatw)whichis0atα∗.Theyexhibitpracticalimprovementinconvergence especiallyforsmoothSVM andtheoreticalspeedupswhenκissparse(foranimpracticalversionofthealgorithm) but[1]doesnottelluswhenthispre-conditionholds northemagnitudeoftheexpectedbeneﬁtintermsofpropertiesoftheproblem(asopposedtoalgorithmstatesuchasκ).InthecontextoflocallyﬂatlossessuchassmoothSVM weanswerthesequestionsthroughlocalsmoothness:Lemma9showsκitendstozeroforlossesthatarelocallyafﬁneonaballaroundtheoptimum andthepracticalAlgorithm2realizesthebeneﬁtwhenthiscertiﬁcationcomesintoplay asquantiﬁedintermsofA(r).3.2TheEmpirical∆SDCAalgorithmAlgorithm2useslocalafﬁnityandasmalldualitygaptocertifytheoptimalityofsomeαi avoidingcalculating∆αithatarezerooruseless;naturallyrissmallenoughonlylateintheprocess.Algo-rithm3insteaddedicateshalfofsamplesinproportiontothemagnitudeofrecent∆αi(theotherhalfchosenuniformly).AsFigure2illustrates thisapproachleadstosigniﬁcantspeedupmuchearlierthantheapproachbasedondualitygapcertiﬁcationoflocalafﬁnity.WhileweitisnotclearthatwecanproveforAlgorithm3aboundthatstrictlyimprovesonAlgorithm2 itisworthnotingthatexceptfor(probablyrare)updatestoi∈Iτ andafactorof2 theempiricalalgorithmshouldquicklydetectalllocallyafﬁnelosseshenceobtainatleastthespeedupofthecertifyingalgorithm.Inaddition itnaturallyadaptstotheexpectedsmallupdatesoflocallysmoothlosses.Notethat∆αiiscloselyrelatedto(andmightbereplacableby)κ butthecurrentalgorithmdifferssigniﬁcantlyfromthosein[1]inhowthesequantitiesareusedtoguidesampling.7Algorithm3Empirical∆SDCA1.α0=0∈Rn Ati=0.2.Forτ∈{1 ...}:(a)pτ=0.5pτ 1+0.5p2wherepτ 1i∝A(τ−1)miandp2i=n−1(b)Fort∈[(τ−1)m+1 τm]:i.Chooseit∼pτii.Compute∆αtit=sit·(cid:0)φ′it(cid:0)x⊤itw(αt)(cid:1)−αt−1it(cid:1)iii.Atj=(0.5At−1j+0.5(cid:12)(cid:12)∆αtj(cid:12)(cid:12)j=itAt−1jotherwiseiv.αtj=(αt−1j+∆αtjj=itαt−1jotherwise4EmpiricalevaluationWeappliedthesamealgorithmswithalmost1thesameparametersto4additionalclassiﬁcationdatasetstodemonstratetheimpactofouralgorithmvariantsmorewidely.TheresultsforSDCAareinFigure4 thoseforSVRGinFigure5inSection7inthesupplementarymaterialforlackofspace.0100200300400Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySDCA solving smoothed hinge loss SVM on Mushroom. Loss gradient is 3.33e+01 Lip. smooth. 1.23e-04 strong convexity.Uniform sampling ([6])Global smoothness sampling ([10])Affine-SDCA (Alg. 2)Empirical ¢ SDCA (Alg. 3)020406080Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySDCA solving smoothed hinge loss SVM on w8a. Loss gradient is 3.33e+01 Lip. smooth. 2.01e-05 strong convexity.Uniform sampling ([6])Global smoothness sampling ([10])Affine-SDCA (Alg. 2)Empirical ¢ SDCA (Alg. 3)051015202530Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySDCA solving smoothed hinge loss SVM on Dorothea. Loss gradient is 3.33e+01 Lip. smooth. 1.25e-03 strong convexity.Uniform sampling ([6])Global smoothness sampling ([10])Affine-SDCA (Alg. 2)Empirical ¢ SDCA (Alg. 3)0100200300400500Effective passes over data10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100Duality gap/suboptimalitySDCA solving smoothed hinge loss SVM on ijcnn1. Loss gradient is 3.33e+01 Lip. smooth. 5.22e-06 strong convexity.Uniform sampling ([6])Global smoothness sampling ([10])Affine-SDCA (Alg. 2)Empirical ¢ SDCA (Alg. 3)Figure4:SDCAvariantresultsonfouradditionaldatasets.Theadvantagesofusinglocalsmoothnessaresigniﬁcantontheharderdatasets.References[1]DominikCsiba ZhengQu andPeterRicht´arik.Stochasticdualcoordinateascentwithadaptiveprobabilities.arXivpreprintarXiv:1502.08053 2015.[2]RieJohnsonandTongZhang.Acceleratingstochasticgradientdescentusingpredictivevari-ancereduction.InAdvancesinNeuralInformationProcessingSystems pages315–323 2013.1Ononeofthenewdatasets SVRGwitharatioofstep-sizetoLavgmoreaggressivethantheorysuggestsstoppedconverging;hencewechangedallrunstousethepermissible1/8.Nootherparameterswerechangedadaptedtothedataset.8[3]QihangLin ZhaosongLu andLinXiao.Anacceleratedproximalcoordinategradientmethodanditsapplicationtoregularizedempiricalriskminimization.arXivpreprintarXiv:1407.1296 2014.[4]MarkSchmidt NicolasLeRoux andFrancisBach.Minimizingﬁnitesumswiththestochasticaveragegradient.arXivpreprintarXiv:1309.2388 2013.[5]ShaiShalev-ShwartzandTongZhang.Acceleratedproximalstochasticdualcoordinateascentforregularizedlossminimization.MathematicalProgramming pages1–41 2013.[6]ShaiShalev-ShwartzandTongZhang.Stochasticdualcoordinateascentmethodsforregular-izedloss.TheJournalofMachineLearningResearch 14(1):567–599 2013.[7]LinXiaoandTongZhang.Aproximalstochasticgradientmethodwithprogressivevariancereduction.SIAMJournalonOptimization 24(4):2057–2075 2014.[8]YuchenZhangandLinXiao.Stochasticprimal-dualcoordinatemethodforregularizedempir-icalriskminimization.arXivpreprintarXiv:1409.3257 2014.[9]PeilinZhaoandTongZhang.Stochasticoptimizationwithimportancesampling.arXivpreprintarXiv:1401.2753 2014.[10]PeilinZhaoandTongZhang.Stochasticoptimizationwithimportancesamplingforregularizedlossminimization.ProceedingsofThe32ndInternationalConferenceonMachineLearning 2015.9,Josh Merel
Roy Fox
Tony Jebara
Liam Paninski
Daniel Vainsencher
Han Liu
Tong Zhang
Yali Wan
Marina Meila