2011,Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning,In this paper  we give a new generalization error bound of Multiple Kernel Learning  (MKL) for a general class of regularizations. Our main target in this paper is  dense type regularizations including ℓp-MKL that imposes ℓp-mixed-norm regularization  instead of ℓ1-mixed-norm regularization. According to the recent numerical  experiments  the sparse regularization does not necessarily show a good  performance compared with dense type regularizations. Motivated by this fact   this paper gives a general theoretical tool to derive fast learning rates that is applicable  to arbitrary monotone norm-type regularizations in a unifying manner. As  a by-product of our general result  we show a fast learning rate of ℓp-MKL that  is tightest among existing bounds. We also show that our general learning rate  achieves the minimax lower bound. Finally  we show that  when the complexities  of candidate reproducing kernel Hilbert spaces are inhomogeneous  dense type  regularization shows better learning rate compared with sparse ℓ1 regularization.,Unifying Framework for Fast Learning Rate of

Non-Sparse Multiple Kernel Learning

Taiji Suzuki

Department of Mathematical Informatics

The University of Tokyo
Tokyo 113-8656  Japan

s-taiji@stat.t.u-tokyo.ac.jp

Abstract

In this paper  we give a new generalization error bound of Multiple Kernel Learn-
ing (MKL) for a general class of regularizations. Our main target in this paper is
dense type regularizations including ℓp-MKL that imposes ℓp-mixed-norm regu-
larization instead of ℓ1-mixed-norm regularization. According to the recent nu-
merical experiments  the sparse regularization does not necessarily show a good
performance compared with dense type regularizations. Motivated by this fact 
this paper gives a general theoretical tool to derive fast learning rates that is ap-
plicable to arbitrary mixed-norm-type regularizations in a unifying manner. As
a by-product of our general result  we show a fast learning rate of ℓp-MKL that
is tightest among existing bounds. We also show that our general learning rate
achieves the minimax lower bound. Finally  we show that  when the complexities
of candidate reproducing kernel Hilbert spaces are inhomogeneous  dense type
regularization shows better learning rate compared with sparse ℓ1 regularization.

1 Introduction

Multiple Kernel Learning (MKL) proposed by [20] is one of the most promising methods that adap-
tively select the kernel function in supervised kernel learning. A kernel method is widely used and
several studies have supported its usefulness [25]. However the performance of kernel methods
critically relies on the choice of the kernel function. Many methods have been proposed to deal
with the issue of kernel selection. [23] studied hyperkrenels as a kernel of kernel functions. [2]
considered DC programming approach to learn a mixture of kernels with continuous parameters.
Some studies tackled a problem to learn non-linear combination of kernels as in [4  9  34]. Among
them  learning a linear combination of ﬁnite candidate kernels with non-negative coefﬁcients is the
most basic  fundamental and commonly used approach. The seminal work of MKL by [20] con-
sidered learning convex combination of candidate kernels. This work opened up the sequence of
the MKL studies. [5] showed that MKL can be reformulated as a kernel version of the group lasso
[36]. This formulation gives an insight that MKL can be described as a ℓ1-mixed-norm regularized
method. As a generalization of MKL  ℓp-MKL that imposes ℓp-mixed-norm regularization has been
proposed [22  14]. ℓp-MKL includes the original MKL as a special case as ℓ1-MKL. Another direc-
tion of generalizing MKL is elasticnet-MKL [26  31] that imposes a mixture of ℓ1-mixed-norm and
ℓ2-mixed-norm regularizations. Recently numerical studies have shown that ℓp-MKL with p > 1
and elasticnet-MKL show better performances than ℓ1-MKL in several situations [14  8  31]. An
interesting perception here is that both ℓp-MKL and elasticnet-MKL produce denser estimator than
the original ℓ1-MKL while they show favorable performances. One motivation of this paper is to
give a theoretical justiﬁcation to these generalized dense type MKL methods in a unifying manner.

1

√

M
In the pioneering paper of [20]  a convergence rate of MKL is given as
n   where M is the number
of given kernels and n is the number of samples. [27] gave improved learning bound utilizing the
pseudo-dimension of the given kernel class. [35] gave a convergence bound utilizing Rademacher
chaos and gave some upper bounds of the Rademacher chaos utilizing the pseudo-dimension of the
kernel class. [8] presented a convergence bound for a learning method with L2 regularization on the
for 1 ≤ p ≤ 2. [15]
kernel weight. [10] gave the convergence rate of ℓp-MKL as M
gave a similar convergence bound with improved constants. [16] generalized this bound to a variant
of the elasticnet type regularization and widened the effective range of p to all range of p ≥ 1 while
in the existing bounds 1 ≤ p ≤ 2 was imposed. One concern about these bounds is that all bounds
√
introduced above are “global” bounds in a sense that the bounds are applicable to all candidates of
estimators. Consequently all convergence rate presented above are of order 1/
n with respect to
the number n of samples. However  by utilizing the localization techniques including so-called local
Rademacher complexity [6  17] and peeling device [32]  we can derive a faster learning rate. Instead
of uniformly bounding all candidates of estimators  the localized inequality focuses on a particular
estimator such as empirical risk minimizer  thus can gives a sharp convergence rate.

√
p ∨
√
n

log(M )

1− 1

Localized bounds of MKL have been given mainly in sparse learning settings [18  21  19]  and there
are only few studies for non-sparse settings in which the sparsity of the ground truth is not assumed.
Recently [13] gave a localized convergence bound of ℓp-MKL. However  their analysis assumed a
strong condition where RKHSs have no-correlation to each other.

In this paper  we show a uniﬁed framework to derive fast convergence rates of MKL with various
regularization types. The framework is applicable to arbitrary mixed-norm regularizations includ-
ing ℓp-MKL and elasticnet-MKL. Our learning rate utilizes the localization technique  thus is tighter
than global type learning rates. Moreover our analysis does not require no-correlation assumption
as in [13]. We apply our general framework to some examples and show our bound achieves the
minimax-optimal rate. As a by-product  we obtain a tighter convergence rate of ℓp-MKL than exist-
ing results. Finally  we show that dense type regularizations can outperforms sparse ℓ1 regularization
when the complexities of the RKHSs are not uniformly same.

2 Preliminary

In this section  we give the problem formulation  the notations and the assumptions required for the
convergence analysis.

2.1 Problem Formulation
Suppose that we are given n i.i.d. samples {(xi  yi)}n
i=1 distributed from a probability distribution
P on X × R where X is an input space. We denote by (cid:5) the marginal distribution of P on X . We
are given M reproducing kernel Hilbert spaces (RKHS) {Hm}M
m=1 each of which is associated with
a kernel km. We consider a mixed-norm type regularization with respect to an arbitrary given norm
∥ψ of the vector (∥fm∥Hm)M
∥·∥ψ  that is  the regularization is given by the norm ∥(∥fm∥Hm)M
∥ψ
for fm ∈ Hm (m = 1  . . .   M)
m=1
m=1 fm (fm ∈ Hm).
for f =

. For notational simplicity  we write ∥f∥ψ = ∥(∥fm∥Hm)M

∑

m=1

M

∗

The general formulation of MKL that we consider in this paper ﬁts a function f =
Hm) to the data by solving the following optimization problem:

m=1

∑
m=1 fm (fm ∈

M

M∑

m=1

n∑

(
yi − M∑

i=1

m=1

)2

^f =

^fm =

arg min

fm∈Hm (m=1 ... M )

1
n

fm(xi)

+ λ(n)

1

∥f∥2
ψ.

(1)

We call this “ψ-norm MKL”. This formulation covers many practically used MKL methods (e.g. 
ℓp-MKL  elasticnet-MKL  variable sparsity kernel learning (see later for their deﬁnitions))  and is
solvable by a ﬁnite dimensional optimization procedure due to the representer theorem [12]. In this
m=1∥ satisﬁes the triangular inequality with respect to
m=1∥ . To satisfy this

m∥Hm )M
′
(fm)M
condition  it is sufﬁcient if the norm is monotone  i.e.  ∥a∥ ≤ ∥a + b∥ for all a  b ≥ 0.

We assume that the mixed-norm ∥(∥fm∥Hm )M
m=1  that is  ∥(∥fm + f

m=1∥ ≤ ∥(∥fm∥Hm )M

m=1∥ + ∥(∥f

m∥Hm )M
′

∗

2

∑

∑

∑

paper  we focus on the regression problem (the squared loss). However the discussion presented
here can be generalized to Lipschitz continuous and strongly convex losses [6].

Example 1: ℓp-MKL The ﬁrst motivating example of ψ-norm MKL is ℓp-MKL [14] that employs
ℓp-norm for 1 ≤ p ≤ ∞ as the regularizer: ∥f∥ψ = ∥(∥fm∥Hm)M
p .
If p is strictly greater than 1 (p > 1)  the solution of ℓp-MKL becomes dense. In particular  p = 2
corresponds to averaging candidate kernels with uniform weight [22]. It is reported that ℓp-MKL
with p greater than 1  say p = 4
3   often shows better performance than the original sparse ℓ1-MKL
[10].

∥fm∥pHm

∥ℓp = (

M
m=1

m=1

)

1

Example 2: Elasticnet-MKL The second example is elasticnet-MKL [26  31] that employs mix-
ture of ℓ1 and ℓ2 norms as the regularizer: ∥f∥ψ = τ∥f∥ℓ1 + (1 − τ )∥f∥ℓ2 = τ
∥fm∥Hm +
(1 − τ )(
2 with τ ∈ [0  1]. Elasticnet-MKL shares the same spirit with ℓp-MKL in
) 1
a sense that it bridges sparse ℓ1-regularization and dense ℓ2-regularization. An efﬁcient optimization
method for elasticnet-MKL is proposed by [30].

∥fm∥2Hm

M
m=1

M
m=1

} 1
Example 3: Variable Sparsity Kernel Learning Variable Sparsity Kernel Learning (VSKL) pro-
′
′
posed by [1] divides the RKHSs into M
) and imposes a mixed
norm regularization ∥f∥ψ = ∥f∥(p q) =
q where 1 ≤ p  q  and
fj k ∈ Hj k. An advantageous point of VSKL is that by adjusting the parameters p and q  various
levels of sparsity can be introduced  that is  the parameters can control the level of sparsity within
group and between groups. This point is beneﬁcial especially for multi-modal tasks like object
categorization.

groups {Hj k}Mj

k=1  (j = 1  . . .   M

∥fj k∥pHj k

∑Mj

{∑

M
j=1(

k=1

q
p

)

′

2.2 Notations and Assumptions
Here  we prepare notations and assumptions that are used in the analysis. Let H⊕M = H1 ⊕ ··· ⊕
HM . Throughout the paper  we assume the following technical conditions (see also [3]).
∑
Assumption 1. (Basic Assumptions)

∗
m(X) 
(A2) For each m = 1  . . .   M  Hm is separable (with respect to the RKHS norm) and

M ) ∈ H⊕M such that E[Y |X] = f
∗

(X) is bounded as |ϵ| ≤ L.

(A1) There exists f

∗
1   . . .   f

and the noise ϵ := Y − f
supX∈X |km(X  X)| < 1.

M
m=1 f

(X) =

= (f

∗

∗

∗

The ﬁrst assumption in (A1) ensures the model H⊕M is correctly speciﬁed  and the technical as-
sumption |ϵ| ≤ L allows ϵf to be Lipschitz continuous with respect to f. The noise boundedness
can be relaxed to unbounded situation as in [24]  but we don’t pursue that direction for simplicity.
Let an integral operator Tkm : L2((cid:5)) → L2((cid:5)) corresponding to a kernel function km be

∫

Tkm f =

km(·  x)f (x)d(cid:5)(x).

It is known that this operator is compact  positive  and self-adjoint (see Theorem 4.27 of [28]). Thus
it has at most countably many non-negative eigenvalues. We denote by µℓ m be the ℓ-th largest
eigenvalue (with possible multiplicity) of the integral operator Tkm. Then we assume the following
assumption on the decreasing rate of µℓ m.
Assumption 2. (Spectral Assumption) There exist 0 < sm < 1 and 0 < c such that

µℓ m ≤ cℓ

− 1

(∀ℓ ≥ 1  1 ≤ ∀m ≤ M ) 

sm  

ℓ=1 is the spectrum of the operator Tkm corresponding to the kernel km.

(A3)
where {µℓ m}∞
It was shown that the spectral assumption (A3) is equivalent to the classical covering number as-
sumption [29]. Recall that the ϵ-covering number N (ϵ BHm  L2((cid:5))) with respect to L2((cid:5)) is the
minimal number of balls with radius ϵ needed to cover the unit ball BHm in Hm [33]. If the spectral
assumption (A3) holds  there exists a constant C that depends only on s and c such that

log N (ε BHm   L2((cid:5))) ≤ Cε

−2sm 

(2)

3

Table 1: Summary of the constants we use in this article.

The number of samples.

n
M The number of candidate kernels.
sm The spectral decay coefﬁcient; see (A3).
κM The smallest eigenvalue of the design matrix (see Eq. (3)).

and the converse is also true (see [29  Theorem 15] and [28] for details). Therefore  if sm is large 
the RKHSs are regarded as “complex”  and if sm is small  the RKHSs are “simple”.
An important class of RKHSs where sm is known is Sobolev space. (A3) holds with sm = d
2α for
Sobolev space of α-times continuously differentiability on the Euclidean ball of Rd [11]. Moreover 
for α-times continuously differentiable kernels on a closed Euclidean ball in Rd  that holds for sm =
d
2α [28  Theorem 6.26]. According to Theorem 7.34 of [28]  for Gaussian kernels with compact
support distribution  that holds for arbitrary small 0 < sm. The covering number of Gaussian
{
kernels with unbounded support distribution is also described in Theorem 7.34 of [28].
Let κM be deﬁned as follows:
κ ≥ 0

  ∀fm ∈ Hm (m = 1  . . .   M )

(cid:12)(cid:12)(cid:12) κ ≤ ∥∑
∑

κM := sup

}

(3)

L2((cid:5))

M

.

L2((cid:5))

m=1 fm∥2
∥fm∥2

M
m=1

κM represents the correlation of RKHSs. We assume all RKHSs are not completely correlated to
each other.
Assumption 3. (Incoherence Assumption) κM is strictly bounded from below; there exists a con-
stant C0 > 0 such that
(A4)

∑
This condition is motivated by the incoherence condition [18  21] considered in sparse MKL settings.
∗
m of the ground truth. [3] also
This ensures the uniqueness of the decomposition f
assumed this condition to show the consistency of ℓ1-MKL.
Finally we give a technical assumption with respect to ∞-norm.
Assumption 4. (Embedded Assumption) Under the Spectral Assumption  there exists a constant
C1 > 0 such that
(A5)

∥fm∥∞ ≤ C1∥fm∥1−smHm

−1
0 < κM .

∥fm∥sm

M
m=1 f

0 < C

L2((cid:5)).

=

∗

This condition is met when the input distribution (cid:5) has a density with respect to the uniform distri-
bution on X that is bounded away from 0 and the RKHSs are continuously embedded in a Sobolev
space W α 2(X ) where sm = d
2α   d is the dimension of the input space X and α is the “smoothness”
of the Sobolev space. Many practically used kernels satisfy this condition (A5). For example  the
RKHSs of Gaussian kernels can be embedded in all Sobolev spaces. Therefore the condition (A5)
seems rather common and practical. More generally  there is a clear characterization of the condi-
tion (A5) in terms of real interpolation of spaces. One can ﬁnd detailed and formal discussions of
interpolations in [29]  and Proposition 2.10 of [7] gives the necessary and sufﬁcient condition for
the assumption (A5).

Constants we use later are summarized in Table 1.

3 Convergence Rate Analysis of -norm MKL
Here we derive the learning rate of ψ-norm MKL in a most general setting. We suppose that the
number of kernels M can increase along with the number of samples n. The motivation of our
analysis is summarized as follows:

• Give a unifying frame work to derive a sharp convergence rate of ψ-norm MKL.
• (homogeneous complexity) Show the convergence rate of some examples using our general
frame work  and prove its minimax-optimality under conditions that the complexities sm
of all RKHSs are same.

4

)M

(cid:13)(cid:13)(cid:13)(cid:13)

• (inhomogeneous complexity) Discuss how the dense type regularization outperforms the
sparse type regularization  when the complexities sm of all RKHSs are not uniformly same.
√
n) for t > 0  and  for given positive reals {rm}M

√
t  t/

m=1

(
Now we deﬁne η(t) := ηn(t) = max(1 
and given n  we deﬁne α1  α2  β1  β2 as follows:
(
α1 := α1({rm}) = 3

) 1

−2sm
n

m=1

m

r

2

− 2sm (3−sm )

M∑
M∑

) 1
  α2 := α2({rm}) = 3

(cid:13)(cid:13)(cid:13)(cid:13)(

smr

2

2

r

m

n

1+sm

1+sm

m=1

  β2 := β2({rm}) = 3

β1 := β1({rm}) = 3
(note that α1  α2  β1  β2 implicitly depends on the reals {rm}M
(
)2
gives the general form of the learning rate of ψ-norm MKL.
Theorem 1. Suppose Assumptions 1-4 are satisﬁed. Let {rm}M
can depend on n  and assume λ(n)
[(
)2
12 and for all t ≥ 1  we have
1   M log(M )
and 4ϕ
1  β2
≤ 24η(t)2ϕ2

(
) ≤ 1

max{α2
∗∥2

1 =
(
}η(t

M log(M )

)2

)

α2
α1

β2
β1

κM

√

+

n

n

′

. Then for all n and t

∥ ^f − f

+ 4

α2
1 + β2

1 +

L2((cid:5))

κM

n

smr1−sm

m√
n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(

m=1

(1−sm )2
1+sm
1

m

n

1+sm

 

)M

ψ∗

m=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

ψ∗

  (4)

m=1). Then the following theorem

m=1 be arbitrary positive reals that
≤ 1

that satisfy log(M )√

′

n

(

]

)2

α2
α1

+

β2
β1

∥f

∗∥2
ψ.

(5)

).

with probability 1 − exp(−t) − exp(−t
′
The proof will be given in Appendix D in the supplementary material. One can also ﬁnd an outline
of the proof in Appendix A in the supplementary material.
The statement of Theorem 1 itself is complicated. Thus we will show later concrete learning rates on
some examples such as ℓp-MKL. The convergence rate (5) depends on the positive reals {rm}M
m=1 
(
})
but the choice of {rm}M
m=1 are arbitrary. Thus by minimizing the right hand side of Eq. (5)  we
obtain tight convergence bound as follows:
∥ ^f − f

There is a trade-off between the ﬁrst two terms (a) := α2

[(
1 and the third term (b) :=
∗∥2
ψ  that is  if we take {rm}m large  then the term (a) becomes small and
the term (b) becomes large  on the other hand  if we take {rm}m small  then it results in large (a)
and small (b). Therefore we need to balance the two terms (a) and (b) to obtain the minimum in
Eq. (6).

L2((cid:5)) =Op
∗∥2
]
)2
(

min
{rm}M
rm>0

[(

)2

)2

α2
1 + β2

M log(M )

)2

1 + β2

{

(

α2
α1

β2
β1

∗∥2

]

∥f

∥f

ψ +

1 +

m=1:

α2
α1

(6)

β2
β1

+

+

n

.

We discuss the obtained learning rate in two situations  (i) homogeneous complexity situation  and
(ii) inhomogeneous complexity situation:
(i) (homogeneous) All sms are same: there exists 0 < s < 1 such that sm = s (∀m) (Sec.3.1).
such that sm ̸= sm′ (Sec.3.2).
(ii) (inhomogeneous) All sms are not same: there exist m  m

′

3.1 Analysis on Homogeneous Settings

Here we assume all sms are same  say sm = s for all m (homogeneous setting). If we further restrict
the situation as all rms are same (rm = r (∀m) for some r)  then the minimization in Eq. (6) can
be easily carried out as in the following lemma. Let 1 be the M-dimensional vector each element of
which is 1: 1 := (1  . . .   1)
Lemma 2. When sm = s (∀m) with some 0 < s < 1 and n ≥ (∥1∥ψ∗∥f
(6) indicates that
∥ ^f − f

⊤ ∈ RM   and ∥ · ∥ψ∗ be the dual norm of the ψ-norm

.
∗∥ψ/M )

1+s (∥1∥ψ∗∥f
− 1

1−s   the bound

M 1− 2s

∗∥2
L2((cid:5)) = Op

M log(M )

(

)

1+s n

(7)

†

4s

.

2s

1+s +

∗∥ψ)
a | ∥a∥ ≤ 1}.

n

†

The dual of the norm ∥ · ∥ is deﬁned as ∥b∥ ∗ := supa

{b
⊤

5

The proof is given in Appendix G.1 in the supplementary material. Lemma 2 is derived by assuming
rm = r (∀m)  which might make the bound loose. However  when the norm ∥ · ∥ψ is isotropic
(whose deﬁnition will appear later)  that restriction (rm = r (∀m)) does not make the bound loose 
that is  the upper bound obtained in Lemma 2 is tight and achieves the minimax optimal rate (the
minimax optimal rate is the one that cannot be improved by any estimator). In the following  we
investigate the general result of Lemma 2 through some important examples.
Convergence Rate of ℓp-MKL Here we derive the convergence rate of ℓp-MKL (1 ≤ p ≤ ∞)
m=1(∥fm∥pHm
p (for p = ∞  it is deﬁned as maxm ∥fm∥Hm). It is well known
where ∥f∥ψ =
(∑
that the dual norm of ℓp-norm is given as ℓq-norm where q is the real satisfying 1
q = 1. For
∗∥ψ = Rp and ∥1∥ψ∗ =
)

) 1
p . Then substituting ∥f

notational simplicity  let Rp :=
∥1∥ℓq = M

p into the bound (7)  the learning rate of ℓp-MKL is given as

∥pHm

∑

p + 1

M
m=1

(

∥f

∗
m

M

)

1

1

q = M 1− 1
∥ ^f − f

∗∥2
L2((cid:5)) =Op

1+s M 1− 2s
− 1
(
If we further assume n is sufﬁciently large so that n ≥ M
ﬁrst term  and thus we have

n

2s

M log(M )

.

p(1+s) R

1+s

p +
−2
p (log M )

2

p R

n
s   the leading term is the

1+s

(8)

)

∥ ^f − f

∗∥2
L2((cid:5)) = Op

1+s M 1− 2s
− 1

p(1+s) R

n

2s

1+s
p

.

(9)

− 1

Note that as the complexity s of RKHSs becomes small the convergence rate becomes fast. It is
known that n
1+s is the minimax optimal learning rate for single kernel learning. The derived
rate of ℓp-MKL is obtained by multiplying a coefﬁcient depending on M and Rp to the optimal
rate of single kernel learning. To investigate the dependency of Rp to the learning rate  let us
consider two extreme settings  i.e.  sparse setting (∥f
m=1 = (1  0  . . .   0) and dense setting
(∥f

∥Hm )M

∗
m

∗
m

m=1 = (1  . . .   1) as in [15].

∥Hm)M
∥Hm )M
• (∥f
∗
m
1+s M 1− 2s
− 1
n
that ℓ1 regularization is preferred for sparse truth.
∥Hm )M
• (∥f

m=1 = (1  . . .   1): Rp = M

∗
m

1

m=1 = (1  0  . . .   0): Rp = 1 for all p. Therefore the convergence rate
p(1+s) is fast for small p and the minimum is achieved at p = 1. This means

p   thus the convergence rate is M n

1+s for all
p. Interestingly for dense ground truth  there is no dependency of the convergence rate
on the parameter p (later we will show that this is not the case in inhomogeneous settings
(Sec.3.2)). That is  the convergence rate is M times the optimal learning rate of single
kernel learning (n
1+s ) for all p. This means that for the dense settings  the complexity of
solving MKL problem is equivalent to that of solving M single kernel learning problems.

− 1

− 1

1

)

M

M

1− 1

log(M )

M
m=1

√
p ∨
√
n

R for all f ∈ Hℓp(R) 

∥fm∥pHm
R(f ) ≤ bR(f ) + C

∑
∑
Comparison with Existing Bounds Here we compare the bound for ℓp-MKL we derived above
with the existing bounds. Let Hℓp (R) be the ℓp-mixed norm ball with radius R: Hℓp(R) := {f =
m=1 fm | (
where R(f ) and bR(f ) is the population risk and the empirical risk. First observation is that the

p ≤ R}. [10  16  15] gave “global” type bounds for ℓp-MKL as

bounds by [10] and [15] are restricted to the situation 1 ≤ p ≤ 2. On the other hand  our analysis
and that of [16] covers all p ≥ 1. Second  since our bound is specialized to the regularized risk
minimizer ^f deﬁned at Eq. (1) while the existing bound (10) is applicable to all f ∈ Hℓp (R)  our
bound is sharper than theirs for sufﬁciently large n. To see this  suppose n ≥ M
−2
p   then we
have n
p . Moreover we should note that s can be large as long as
Spectral Assumption (A3) is satisﬁed. Thus the bound (10) is formally recovered by our analysis by
(
approaching s to 1.
Recently [13] gave a tighter convergence rate utilizing the localization technique as ∥ ^f−f
Op

L2((cid:5)) =
  under a strong condition κM = 1 that imposes all

1+s M 1− 2s
− 1

1+s M 1− 2s
− 1

p(1+s) ≤ n

2 M 1− 1
− 1

p′ (1+s) R

})

minp′≥p

{

∗∥2

(10)

p R

1+s

2s

′

2

p
p′−1 n

p′

6

′

RKHSs are completely uncorrelated to each other. Comparing our bound with their result  there are
not minp′≥p and p
p′−1   then the minimum of minp′≥p
p′−1 in our bound (if there is not the term p
′
is attained at p
= p  thus our bound is tighter)  moreover our analysis doesn’t need the strong
assumption κM = 1.

′

(∥a∥ℓ∞

)}

Convergence Rate of Elasticnet-MKL Elasticnet-MKL employs a mixture of ℓ1 and ℓ2 norm as
the regularizer: ∥f∥ψ = τ∥f∥ℓ1 + (1 − τ )∥f∥ℓ2 where τ ∈ [0  1]. Then its dual norm is given
by ∥b∥ψ∗ = mina∈RM
. Therefore by a simple calculation  we have
)
∥1∥ψ∗ =
∥ ^f − f

. Hence Eq. (7) gives the convergence rate of elasticnet-MKL as

√
√
M
1−τ +τ
L2((cid:5)) = Op
∗∥2

∗∥ℓ1 + (1 − τ )∥f

1+s + M log(M )

∥a−b∥ℓ2

∗∥ℓ2)

1− s
√
1+s

(τ∥f

1−τ

max

− 1

n

1+s

M

n

2s

2s

 

.

τ

M
(1−τ +τ

M )

1+s

{
(

Note that  when τ = 0 or τ = 1  this rate is identical to that of ℓ2-MKL or ℓ1-MKL obtained in
Eq. (8) respectively.

3.1.1 Minimax Lower Bound

In this section  we show that the derived learning rate (7) achieves the minimax-learning rate on the
ψ-norm ball

{

∑

Hψ(R) :=

f =

M
m=1 fm

}

(cid:12)(cid:12) ∥f∥ψ ≤ R

 

∥b∥ψ ≤ ∥b

(cid:22)cM = (cid:22)c∥1∥ℓ1

≥ ∥1∥ψ∗∥1∥ψ 

m (∀m)) 
′∥ψ (if 0 ≤ bm ≤ b
′

when the norm is isotropic. We say the ψ-norm ∥ · ∥ψ is isotropic when there exits a universal
constant (cid:22)c such that
(11)
(note that the inverse inequality M ≤ ∥1∥ψ∗∥1∥ψ of the ﬁrst condition always holds by the deﬁ-
nition of the dual norm). Practically used regularizations usually satisfy this isotropic property. In
fact  ℓp-MKL  elasticnet-MKL and VSKL satisfy the isotropic property with (cid:22)c = 1.
We derive the minimax learning rate in a simpler situation. First we assume that each RKHS is same
as others. That is  the input vector is decomposed into M components like x = (x(1)  . . .   x(M ))
m=1 are M i.i.d. copies of a random variable ~X  and Hm = {fm | fm(x) =
where {x(m)}M
f ∈ H⊕M is decomposed as f (x) = f (x(1)  . . .   x(M )) =

fm(x(1)  . . .   x(M )) = ~fm(x(m))  ~fm ∈ eH} where eH is an RKHS shared by all Hm. Thus
a member of the common RKHS eH. We denote byek the kernel associated with the RKHS eH.

~fm(x(m)) where each ~fm is

∑

M
m=1

′

c

− 1
s  

(1 ≤ ∀ℓ) 

In addition to the condition about the upper bound of spectrum (Spectral Assumption (A3))  we
assume that the spectrum of all the RKHSs Hm have the same lower bound of polynomial rate.
′
Assumption 5. (Strong Spectral Assumption) There exist 0 < s < 1 and 0 < c  c
such that
(A6)
where {~µℓ}∞
ular  the spectrum of Tkm also satisﬁes µℓ m ∼ ℓ

Without loss of generality  we may assume that E[f ( ~X)] = 0 (∀f ∈ eH). Since each fm receives

ℓ=1 is the spectrum of the integral operator T~k corresponding to the kernel ~k. In partic-

s ≤ ~µℓ ≤ cℓ
− 1
ℓ

i.i.d. copy of ~X  Hms are orthogonal to each other:
E[fm(X)fm′(X)] = E[ ~fm(X (m)) ~fm′(X (m
We also assume that the noise {ϵi}n
Under the assumptions described above  we have the following minimax L2((cid:5))-error.
[
Theorem 3. Suppose R > 0 is given and n > (cid:22)c2M 2
R2∥1∥2
on Hψ(R) for isotropic norm ∥ · ∥ψ is lower bounded as

).
i=1 is an i.i.d. normal sequence with standard deviation σ > 0.

))] = 0 (∀fm ∈ Hm  ∀fm′ ∈ Hm′   ∀m ̸= m

ψ∗ is satisﬁed. Then the minimax-learning rate

s (∀ℓ  m).
− 1

]

′

′

∥ ^f − f

∗∥2

L2((cid:5))

≥ CM 1− 2s

1+s n

1+s (∥1∥ψ∗ R)
− 1

2s

1+s  

(12)

min

^f

max

f∗∈Hψ(R)

E

where inf is taken over all measurable functions of n samples {(xi  yi)}n

i=1.

7

The proof will be given in Appendix F in the supplementary material. One can see that the con-
vergence rate derived in Eq. (7) achieves the minimax rate on the ψ-norm ball (Theorem 3) up
to M log(M )
that is negligible when the number of samples is large. This means that the ψ-norm
regularization is well suited to make the estimator included in the ψ-norm ball.

n

3.2 Analysis on Inhomogeneous Settings
In the previous section (analysis on homogeneous settings)  we have not seen any theoretical justiﬁ-
cation supporting the fact that dense MKL methods like ℓ 4
-MKL can outperform the sparse ℓ1-MKL
[10]. In this section  we show dense type regularizations can outperform the sparse regularization
such that sm ̸= sm′). For simplicity  we focus on
in inhomogeneous settings (there exists m  m
ℓp-MKL  and discuss the relation between the learning rate and the norm parameter p.

′

3

‡

. In

Let us consider an extreme situation where s1 = s for some 0 < s < 1 and sm = 0 (m > 1)
this situation  we have

α1 = 3

r

1 +M−1
−2s

n

  α2 = 3 sr1−s
1√

n   β1 = 3

− 2s(3−s)

1+s

r

1

+M−1

2

1+s

n

(1−s)2
1+s

  β2 = 3 sr
n

1

1

1+s

.

(

) 1

2

(

) 1

2

2s

∗
m

∥Hm)M

∗∥ℓ∞ ≤ ∥f

∗∥ℓ1 = M∥f

m=1 = 1  we have ∥f

for all p. Note that these α1  α2  β1 and β2 have no dependency on p. Therefore the learning bound
(6) is smallest when p = ∞ because ∥f
∗∥ℓp for all 1 ≤ p < ∞. In particular  when
∗∥ℓ∞ and thus obviously the learning rate of ℓ∞-MKL
(∥f
given by Eq. (6) is faster than that of ℓ1-MKL. In fact  through a bit cumbersome calculation  one
can check that ℓ∞-MKL can be M
1+s times faster than ℓ1-MKL in a worst case. This indicates
that  when the complexities of RKHSs are inhomogeneous  the generalization abilities of dense type
regularizations (e.g.  ℓ∞-MKL) can be better than the sparse type regularization (ℓ1-MKL). In real
settings  it is likely that one uses various types of kernels and the complexities of RKHSs become
inhomogeneous. As mentioned above  it has been often reported that ℓ1-MKL is outperformed by
dense type MKL such as ℓ 4
-MKL in numerical experiments [10]. Our theoretical analysis explains
well this experimental results.
4 Conclusion
We have shown a uniﬁed framework to derive the learning rate of MKL with arbitrary mixed-norm-
type regularization. To analyze the general result  we considered two situations: homogeneous
settings and inhomogeneous settings. We have seen that the convergence rate of ℓp-MKL obtained in
homogeneous settings is tighter and require less restrictive condition than existing results. We have
also shown the convergence rate of elasticnet-MKL  and proved the derived learning rate is minimax
optimal. Furthermore  we observed that our bound well explains the favorable experimental results
for dense type MKL by considering the inhomogeneous settings. This is the ﬁrst result that strongly
justiﬁes the effectiveness of dense type regularizations in MKL.
Acknowledgement This work was partially supported by MEXT Kakenhi 22700289 and the Ai-
hara Project  the FIRST program from JSPS  initiated by CSTP.

3

References

[1] J. Aﬂalo  A. Ben-Tal  C. Bhattacharyya  J. S. Nath  and S. Raman. Variable sparsity kernel learning.

Journal of Machine Learning Research  12:565–592  2011.

[2] A. Argyriou  R. Hauser  C. A. Micchelli  and M. Pontil. A DC-programming algorithm for kernel selec-

tion. In the 23st ICML  pages 41–48  2006.

[3] F. R. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning

Research  9:1179–1225  2008.

[4] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Advances in

Neural Information Processing Systems 21  pages 105–112  2009.

[5] F. R. Bach  G. Lanckriet  and M. Jordan. Multiple kernel learning  conic duality  and the SMO algorithm.

In the 21st ICML  pages 41–48  2004.

[6] P. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. The Annals of Statistics 

33:1487–1537  2005.

‡

In our assumption sm should be greater than 0. However we formally put sm = 0 (m > 1) for simplicity

of discussion. For rigorous discussion  one might consider arbitrary small sm ≪ s.

8

[7] C. Bennett and R. Sharpley. Interpolation of Operators. Academic Press  Boston  1988.
[8] C. Cortes  M. Mohri  and A. Rostamizadeh. L2 regularization for learning kernels. In UAI 2009  2009.
[9] C. Cortes  M. Mohri  and A. Rostamizadeh. Learning non-linear combinations of kernels. In Advances

in Neural Information Processing Systems 22  pages 396–404  2009.

[10] C. Cortes  M. Mohri  and A. Rostamizadeh. Generalization bounds for learning kernels.

ICML  pages 247–254  2010.

In the 27th

[11] D. E. Edmunds and H. Triebel. Function Spaces  Entropy Numbers  Differential Operators. Cambridge

University Press  Cambridge  1996.

[12] G. S. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathemati-

cal Analysis and Applications  33:82–95  1971.

[13] M. Kloft and G. Blanchard. The local rademacher complexity of ℓp-norm multiple kernel learning  2011.

arXiv:1103.0790.

[14] M. Kloft  U. Brefeld  S. Sonnenburg  P. Laskov  K.-R. M¨uller  and A. Zien. Efﬁcient and accurate ℓp-norm
multiple kernel learning. In Advances in Neural Information Processing Systems 22  pages 997–1005 
2009.

[15] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien. lp-norm multiple kernel learning. Journal of Machine

Learning Research  12:953–997  2011.

[16] M. Kloft  U. R¨uckert  and P. L. Bartlett. A unifying view of multiple kernel learning. In ECML/PKDD 

2010.

[17] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals

of Statistics  34:2593–2656  2006.

[18] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In COLT  pages

229–238  2008.

[19] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics  38(6):3660–

3695  2010.

[20] G. Lanckriet  N. Cristianini  L. E. Ghaoui  P. Bartlett  and M. Jordan. Learning the kernel matrix with

semi-deﬁnite programming. Journal of Machine Learning Research  5:27–72  2004.

[21] L. Meier  S. van de Geer  and P. B¨uhlmann. High-dimensional additive modeling. The Annals of Statistics 

37(6B):3779–3821  2009.

[22] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine

Learning Research  6:1099–1125  2005.

[23] C. S. Ong  A. J. Smola  and R. C. Williamson. Learning the kernel with hyperkernels. Journal of Machine

Learning Research  6:1043–1071  2005.

[24] G. Raskutti  M. Wainwright  and B. Yu. Minimax-optimal rates for sparse additive models over kernel

classes via convex programming. Technical report  2010. arXiv:1008.3654.

[25] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[26] J. Shawe-Taylor. Kernel learning for novelty detection. In NIPS 2008 Workshop on Kernel Learning:

Automatic Selection of Optimal Kernels  Whistler  2008.

[27] N. Srebro and S. Ben-David. Learning bounds for support vector machines with learned kernels.

COLT  pages 169–183  2006.

In

[28] I. Steinwart. Support Vector Machines. Springer  2008.
[29] I. Steinwart  D. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In COLT 

2009.

[30] T. Suzuki and R. Tomioka. Spicymkl: A fast algorithm for multiple kernel learning with thousands of

kernels. Machine Learning  85(1):77–108  2011.

[31] R. Tomioka and T. Suzuki. Sparsity-accuracy trade-off in MKL. In NIPS 2009 Workshop: Understanding

Multiple Kernel Learning Methods  Whistler  2009.

[32] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press  2000.
[33] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to

Statistics. Springer  New York  1996.

[34] M. Varma and B. R. Babu. More generality in efﬁcient multiple kernel learning. In the 26th ICML  pages

1065–1072  2009.

[35] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In COLT  2009.
[36] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of The

Royal Statistical Society Series B  68(1):49–67  2006.

9

,Emily Denton
Soumith Chintala
arthur szlam
Rob Fergus
Wei Chen
Wei Hu
Fu Li
Jian Li
Yu Liu
Pinyan Lu