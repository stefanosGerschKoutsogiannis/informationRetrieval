2014,PAC-Bayesian AUC classification and scoring,We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior  and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm  as an efficient method which may be used as a gold standard  and an Expectation-Propagation algorithm  as a much faster but approximate method. We also extend our method to a class of non-linear score functions  essentially leading to a nonparametric procedure  by considering a Gaussian process prior.,PAC-Bayesian AUC classiﬁcation and scoring

James Ridgway∗

CREST and CEREMADE University Dauphine

james.ridgway@ensae.fr

Pierre Alquier
CREST (ENSAE)

pierre.alquier@ucd.ie

Nicolas Chopin

CREST (ENSAE) and HEC Paris
nicolas.chopin@ensae.fr

Feng Liang

University of Illinois at Urbana-Champaign

liangf@illinois.edu

Abstract

We develop a scoring and classiﬁcation procedure based on the PAC-Bayesian ap-
proach and the AUC (Area Under Curve) criterion. We focus initially on the class
of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for
two types of prior for the score parameters: a Gaussian prior  and a spike-and-slab
prior; the latter makes it possible to perform feature selection. One important ad-
vantage of our approach is that it is amenable to powerful Bayesian computational
tools. We derive in particular a Sequential Monte Carlo algorithm  as an efﬁcient
method which may be used as a gold standard  and an Expectation-Propagation
algorithm  as a much faster but approximate method. We also extend our method
to a class of non-linear score functions  essentially leading to a nonparametric
procedure  by considering a Gaussian process prior.

1

Introduction

Bipartite ranking (scoring) amounts to rank (score) data from binary labels. An important problem
in its own right  bipartite ranking is also an elegant way to formalise classiﬁcation: once a score
function has been estimated from the data  classiﬁcation reduces to chooses a particular threshold 
which determine to which class is assigned each data-point  according to whether its score is above
or below that threshold. It is convenient to choose that threshold only once the score has been esti-
mated  so as to get ﬁner control of the false negative and false positive rates; this is easily achieved
by plotting the ROC (Receiver operating characteristic) curve.
A standard optimality criterion for scoring is AUC (Area Under Curve)  which measures the area
under the ROC curve. AUC is appealing for at least two reasons. First  maximising AUC is equiv-
alent to minimising the L1 distance between the estimated score and the optimal score. Second 
under mild conditions  Cortes and Mohri [2003] show that AUC for a score s equals the probability
that s(X−) < s(X +) for X− (resp. X +) a random draw from the negative (resp. positive class).
Yan et al. [2003] observed AUC-based classiﬁcation handles much better skewed classes (say the
positive class is much larger than the other) than standard classiﬁers  because it enforces a small
score for all members of the negative class (again assuming the negative class is the smaller one).
One practical issue with AUC maximisation is that the empirical version of AUC is not a continuous
function. One way to address this problem is to ”convexify” this function  and study the properties of
so-obtained estimators [Cl´emenc¸on et al.  2008a]. We follow instead the PAC-Bayesian approach in
this paper  which consists of using a random estimator sampled from a pseudo-posterior distribution
that penalises exponentially the (in our case) AUC risk. It is well known [see e.g. the monograph
of Catoni  2007] that the PAC-Bayesian approach comes with a set of powerful technical tools to

∗http://www.crest.fr/pagesperso.php?user=3328

1

establish non-asymptotic bounds; the ﬁrst part of the paper derive such bounds. A second advantage
however of this approach  as we show in the second part of the paper  is that it is amenable to pow-
erful Bayesian computational tools  such as Sequential Monte Carlo and Expectation Propagation.

2 Theoretical bounds from the PAC-Bayesian Approach

with distribution P   and taking values in Rd×{−1  1}. Let n+ =(cid:80)n

2.1 Notations
The data D consist in the realisation of n IID (independent and identically distributed) pairs (Xi  Yi)
1{Yi = +1}  n− = n−n+.
For a score function s : Rd → R  the AUC risk and its empirical counter-part may be deﬁned as:

i=1

R(s) = P(X Y ) (X(cid:48) Y (cid:48))∼P [{s(X) − s(X(cid:48))}(Y − Y (cid:48)) < 0]  
1 [{s(Xi) − s(Xj)}(Yi − Yj) < 0] .

Rn(s) =

1

n(n − 1)

(cid:88)

i(cid:54)=j

Let σ(x) = E(Y |X = x)  ¯R = R(σ) and ¯Rn = Rn(σ). It is well known that σ is the score that
minimise R(s)  i.e. R(s) ≥ ¯R = R(σ) for any score s.
The results of this section apply to the class of linear scores  sθ(x) = (cid:104)θ  x(cid:105)  where (cid:104)θ  x(cid:105) = θT x
denotes the inner product. Abusing notations  let R(θ) = R(sθ)  Rn(θ) = Rn(sθ)  and  for a given
prior density πξ(θ) that may depend on some hyperparameter ξ ∈ Ξ  deﬁne the Gibbs posterior
density (or pseudo-posterior) as

πξ γ(θ|D) :=

πξ(θ) exp{−γRn(θ)}

Zξ γ(D)

  Zξ γ(D) =

πξ(˜θ) exp

Rd

(cid:90)

(cid:110)−γRn(˜θ)

(cid:111)

d˜θ

for γ > 0. Both the prior and posterior densities are deﬁned with respect to the Lebesgue measure
over Rd.

2.2 Assumptions and general results

Our general results require the following assumptions.

Deﬁnition 2.1 We say that Assumption Dens(c) is satisﬁed for c > 0 if

P((cid:104)X1 − X2  θ(cid:105) ≥ 0 (cid:104)X1 − X2  θ(cid:48)(cid:105) ≤ 0) ≤ c(cid:107)θ − θ(cid:48)(cid:107)

for any θ and θ(cid:48) ∈ Rd such that (cid:107)θ(cid:107) = (cid:107)θ(cid:48)(cid:107) = 1.
This is a mild Assumption  which holds for instance as soon as (X1 − X2)/(cid:107)X1 − X2(cid:107) admits a
bounded probability density; see the supplement.

Deﬁnition 2.2 (Mammen & Tsybakov margin assumption) We say that Assumption MA(κ  C)
is satisﬁed for κ ∈ [1  +∞] and C ≥ 1 if

1 2)2(cid:3) ≤ C(cid:2)R(θ) − R(cid:3) 1
E(cid:2)(qθ

κ

where qθ

i j = 1{(cid:104)θ  Xi − Xj(cid:105) (Yi − Yj) < 0} − 1{[σ(Xi) − σ(Xj)](Yi − Yj) < 0} − R(θ) + R.
This assumption was introduced for classiﬁcation by Mammen and Tsybakov [1999]  and used for
ranking by Cl´emenc¸on et al. [2008b] and Robbiano [2013] (see also a nice discussion in Lecu´e
[2007]). The larger κ  the less restrictive MA(κ  C). In fact  MA(∞  C) is always satisﬁed for
C = 4. For a noiseless classiﬁcation task (i.e. σ(Xi)Yi ≥ 0 almost surely)  R = 0 

E((qθ

1 2)2) = Var(qθ

1 2) = E[1{(cid:104)θ  X1 − X2(cid:105) (Yi − Yj) < 0}] = R(θ) − R

and MA(1  1) holds. More generally  MA(1  C) is satisﬁed as soon as the noise is small; see the
discussion in Robiano 2013 (Proposition 5 p. 1256) for a formal statement. From now  we focus
on either MA(1  C) or MA(∞  C)  C ≥ 1. It is possible to prove convergence under MA(κ  1)

2

(cid:90)

(cid:90)

the

use

for a general κ ≥ 1  but at the price of complications regarding the choice of γ; see Catoni [2007] 
Alquier [2008] and Robbiano [2013].
We
[1998];
Shawe-Taylor and Williamson [1997] (see Alquier [2008]; Catoni [2007] for a complete sur-
vey and more recent advances) to get the following results. Proof of these and forthcoming
results may be found in the supplement. Let K(ρ  π) denotes the Kullback-Liebler divergence 
+ the set of probability

K(ρ  π) = (cid:82) ρ(dθ) log{ dρ

dπ (θ)} if ρ << π  ∞ otherwise  and denote M1

classical PAC-Bayesian methodology

distributions ρ(dθ).
Lemma 2.1 Assume that MA(1  C) holds with C ≥ 1. For any ﬁxed γ with 0 < γ ≤ (n− 1)/(8C) 
for any ε > 0  with probability at least 1 − ε on the drawing of the data D 

by McAllester

initiated

R(θ)πξ γ(θ|D)dθ − R ≤ 2 inf
ρ∈M1

+

R(θ)ρ(dθ) − R + 2

Lemma 2.2 Assume MA(∞  C) with C ≥ 1. For any ﬁxed γ with 0 < γ ≤ (n − 1)/8  for any
 > 0 with probability 1 −  on the drawing of D 

R(θ)πξ γ(θ|D)dθ − ¯R ≤ inf
ρ∈M1

+

R(θ)ρ(dθ) − ¯R + 2

K(ρ  π) + log 2



γ

+

16γ
n − 1

.

(cid:27)

Both lemmas bound the expected risk excess  for a random estimator of θ generated from πξ γ(θ|D).

2.3

Independent Gaussian Prior

We now specialise these results to the prior density πξ(θ) = (cid:81)d

independent Gaussian distributions N (0  ϑ); ξ = ϑ in this case.
Theorem 2.3 Assume MA(1  C)  C ≥ 1  Dens(c)  c > 0  and take ϑ = 2
n2d )  γ =
(n − 1)/8C  then there exists a constant α = α(c  C  d) such that for any  > 0  with probability
1 −  

d (1 + 1

i=1 ϕ(θi; 0  ϑ)  i.e. a product of

K(ρ  π) + log(cid:0) 4

ε

(cid:41)

(cid:1)

.

γ

(cid:40)(cid:90)

(cid:26)(cid:90)

(cid:90)

(cid:90)

R(θ)πγ(θ|D)dθ − ¯R ≤ 2 inf

θ0

Theorem 2.4 Assume MA(∞  C)  C ≥ 1  Dens(c) c > 0  and take ϑ = 2

C(cid:112)dn log(n)  there exists a constant α = α(c  C  d) such that for any  > 0  with probability 1−  

n2d )  γ =

d (1 + 1

(cid:8)R(θ0) − ¯R(cid:9) + α

d log(n) + log 4


n − 1

.

(cid:8)R(θ0) − ¯R(cid:9) + α

(cid:112)d log(n) + log 2



.

√

n

R(θ)πγ(θ|D)dθ − ¯R ≤ inf

θ0

The proof of these results is provided in the supplementary material.
MA(κ  C)  the rate (d/n)
Following Robbiano [2013] we conjecturate that this rate is also optimal for ranking problems.

It is known that  under
2κ−1 is minimax-optimal for classiﬁcation problems  see Lecu´e [2007].

κ

2.4 Spike and slab prior for feature selection

The independent Gaussian prior considered in the previous section is a natural choice  but it does
not accommodate sparsity  that is  the possibility that only a small subset of the components of Xi
actually determine the membership to either class. For sparse scenarios  one may use the spike and
slab prior of Mitchell and Beauchamp [1988]  George and McCulloch [1993] 

d(cid:89)

πξ(θ) =

[pϕ(θi; 0  v1) + (1 − p)ϕ(θi; 0  v0)]

with ξ = (p  v0  v1) ∈ [0  1] × (R+)2  and v0 (cid:28) v1  for which we obtain the following result. Note
(cid:107)θ(cid:107)0 is the number of non-zero coordinates for θ ∈ Rd.

i=1

3

Theorem 2.5 Assume MA(1  C) holds with C ≥ 1  Dens(c) holds with c > 0  and take p = 1 −
exp(−1/d)  v0 ≤ 1/(2nd log(d))  and γ = (n− 1)/(8C). Then there is a constant α = α(C  v1  c)
such that for any ε > 0  with probability at least 1 − ε on the drawing of the data D 

(cid:40)

(cid:107)θ0(cid:107)0 log(nd) + log(cid:0) 4

ε

2(n − 1)

(cid:41)

(cid:1)

.

R(θ)πγ(dθ|D) − R ≤ 2 inf

θ0

R(θ0) − R + α

(cid:90)

Compared to Theorem 2.3  the bound above increases logarithmically rather than linearly in d  and
depends explicitly on (cid:107)θ(cid:107)0  the sparsity of θ. This suggests that the spike and slab prior should lead
to better performance than the Gaussian prior in sparse scenarios. The rate (cid:107)θ(cid:107)0 log(d)/n is the
same as the one obtained in sparse regression  see e.g. B¨uhlmann and van de Geer [2011].
Finally  note that if v0 → 0  we recover the more standard prior which assigns a point mass at zero
for every component. However this leads to a pseudo-posterior which is a mixture of 2d components
that mix Dirac masses and continuous distributions  and thus which is more difﬁcult to approximate
(although see the related remark in Section 3.4 for Expectation-Propagation).

3 Practical implementation of the PAC-Bayesian approach

3.1 Choice of hyper-parameters

Theorems 2.3  2.4  and 2.5 propose speciﬁc values for hyper-parameters γ and ξ  but these values de-
pend on some unknown constant C. Two data-driven ways to choose γ and ξ are (i) cross-validation
(which we will use for γ)  and (ii) (pseudo-)evidence maximisation (which we will use for ξ).
The latter may be justiﬁed from intermediate results of our proofs in the supplement  which provide
an empirical bound on the expected risk:

R(θ)πξ γ(θ|D)dθ − ¯R ≤ Ψγ n inf
ρ∈M1

+

Rn(θ)ρ(dθ) − ¯Rn + 2

with Ψγ n ≤ 2. The right-hand side is minimised at ρ(dθ) = πξ γ(θ|D)dθ  and the so-obtained
bound is −Ψγ n log(Zξ γ(D))/γ plus constants. Minimising the upper bound with respect to hyper-
parameter ξ is therefore equivalent to maximising log Zξ γ(D) with respect to ξ. This is of course
akin to the empirical Bayes approach that is commonly used in probabilistic machine learning. Re-
garding γ the minimization is more cumbersome because the dependence with the log(2/) term
and Ψn γ  which is why we recommend cross-validation instead.
It seems noteworthy that  beside Alquier and Biau [2013]  very few papers discuss the practical im-
plementation of PAC-Bayes  beyond some brief mention of MCMC (Markov chain Monte Carlo).
However  estimating the normalising constant of a target density simulated with MCMC is notori-
ously difﬁcult. In addition  even if one decides to ﬁx the hyperparameters to some arbitrary value 
MCMC may become slow and difﬁcult to calibrate if the dimension of the sampling space becomes
large. This is particularly true if the target does not (as in our case) have some speciﬁc structure
that make it possible to implement Gibbs sampling. The two next sections discuss two efﬁcient
approaches that make it possible to approximate both the pseudo-posterior πξ γ(θ|D) and its nor-
malising constant  and also to perform cross-validation with little overhead.

(cid:19)

K(ρ  π) + log 2



γ

(cid:18)(cid:90)

(cid:90)

3.2 Sequential Monte Carlo
Given the particular structure of the pseudo-posterior πξ γ(θ|D)  a natural approach to simulate
from πξ γ(θ|D) is to use tempering SMC [Sequential Monte Carlo Del Moral et al.  2006] that is 
deﬁne a certain sequence γ0 = 0 < γ1 < . . . < γT   start by sampling from the prior πξ(θ) 
then applies successive importance sampling steps  from πξ γt−1 (θ|D) to πξ γt(θ|D)  leading to
importance weights proportional to:

πξ γt(θ|D)
πξ γt−1(θ|D)

∝ exp{−(γt − γt−1)Rn(θ)} .

When the importance weights become too skewed  one rejuvenates the particles through a resam-
pling step (draw particles randomly with replacement  with probability proportional to the weights)
and a move step (move particles according to a certain MCMC kernel).

4

One big advantage of SMC is that it is very easy to make it fully adaptive. For the choice of
the successive γt  we follow Jasra et al. [2007] in solving numerically (1) in order to impose that
the Effective sample size has a ﬁxed value. This ensures that the degeneracy of the weights always
remain under a certain threshold. For the MCMC kernel  we use a Gaussian random walk Metropolis
step  calibrated on the covariance matrix of the resampled particles. See Algorithm 1 for a summary.

Algorithm 1 Tempering SMC
Input N (number of particles)  τ ∈ (0  1) (ESS threshold)  κ > 0 (random walk tuning parameter)
Init. Sample θi
Loop a. Solve in γt the equation

0 ∼ πξ(θ) for i = 1 to N  set t ← 1  γ0 = 0  Z0 = 1.

t−1)}2
t−1))2} = τ N  wt(θ) = exp[−(γt − γt−1)Rn(θ)]
(cid:111)

(cid:80)N

(1)

t−1)
in 1  . . .   N so that P(Ai

i=1 wt(θi

N

  and stop.

t = j) =

{(cid:80)N
(cid:80)N
i=1 wt(θi
using bisection search. If γt ≥ γT   set ZT = Zt−1 ×(cid:110) 1
i=1{wt(θi
t−1)/(cid:80)N
d. Set Zt = Zt−1 ×(cid:110) 1

for i = 1 to N  draw Ai
t

t ∼ Mt(θAi

b. Resample:

c. Sample θi

(cid:80)N

k=1 wt(θk

i=1 wt(θi

t−1)

wt(θj

(cid:111)

.

N

t−1); see Algorithm 1 in the supplement.

t

t−1  dθ) for i = 1 to N where Mt is a MCMC kernel that leaves
invariant πt; see Algorithm 2 in the supplement for an instance of such a MCMC
kernel  which takes as an input S = κ ˆΣ  where ˆΣ is the covariance matrix of the θAi
t−1.

t

In our context  tempering SMC brings two extra advantages: it makes it possible to obtain sam-
ples from πξ γ(θ|D) for a whole range of values of γ  rather than a single value. And it provides
an approximation of Zξ γ(D) for the same range of γ values  through the quantity Zt deﬁned in
Algorithm 1.

3.3 Expectation-Propagation (Gaussian prior)

The SMC sampler outlined in the previous section works fairly well  and we will use it as gold
standard in our simulations. However  as any other Monte Carlo method  it may be too slow for
large datasets. We now turn our attention to EP [Expectation-Propagation Minka  2001]  a general
framework to derive fast approximations to target distributions (and their normalising constants).
First note that the pseudo-posterior may be rewritten as:

πξ γ(θ|D) =

1

Zξ γ(D)

fij(θ) 

fij(θ) = exp [−γ(cid:48)1{(cid:104)θ  Xi − Xj(cid:105) < 0}]

where γ(cid:48) = γ/n+n−  and the product is over all (i  j) such that Yi = 1  Yj = −1. EP generates an
approximation of this target distribution based on the same factorisation:

πξ(θ) ×(cid:89)
(cid:89)

i j

qij(θ) 

i j

q(θ) ∝ q0(θ)

qij(θ) = exp{− 1
2

θT Qijθ + rT

ijθ}.

We consider in the section the case where the prior is Gaussian  as in Section 2.3. Then one may set
q0(θ) = πξ(θ). The approximating factors are un-normalised Gaussian densities (under a natural
parametrisation)  leading to an overall approximation that is also Gaussian  but other types of expo-
nential family parametrisations may be considered; see next section and Seeger [2005]. EP updates
iteratively each site qij (that is  it updates the parameters Qij and rij)  conditional on all the sites 
by matching the moments of q with those of the hybrid distribution

(cid:89)

fkl(θ)

(k l)(cid:54)=(i j)

hij(θ) ∝ q(θ)

fij(θ)
qij(θ)

∝ q0(θ)fij(θ)

5

θT Qh

(k l)(cid:54)=(i j) rkl  Qh

ij = (cid:80)

ij = (cid:80)

hij(θ) ∝ exp{θT rh

where again the product is over all (k  l) such that Yk = 1  Yl = −1  and (k  l) (cid:54)= (i  j).
We refer to the supplement for a precise algorithmic description of our EP implementation. We
highlight the following points. First  the site update is particularly simple in our case:
ijθ} exp [−γ(cid:48)1{(cid:104)θ  Xi − Xj(cid:105) < 0}]  

ij − 1
2
with rh
(k l)(cid:54)=(i j) Qkl  which may be interpreted as: θ conditional
on T (θ) = (cid:104)θ  Xi − Xj(cid:105) has a d − 1-dimensional Gaussian distribution  and the distribution of
T (θ) is that of a one-dimensional Gaussian penalised by a step function. The two ﬁrst moments
of this particular hybrid may therefore be computed exactly  and in O(d2) time  as explained in
the supplement. The updates can be performed efﬁciently using the fact that the linear combination
(Xi− Xj)θ is a one dimensional Gaussian. For our numerical experiment we used a parallel version
of EP Van Gerven et al. [2010]. The complexity of our EP implementation is O(n+n−d2 + d3).
Second  EP offers at no extra cost an approximation of the normalising constant Zξ γ(D) of the
target πξ γ(θ|D); in fact  one may even obtain derivatives of this approximated quantity with respect
to hyper-parameters. See again the supplement for more details.
Third  in the EP framework  cross-validation may be interpreted as dropping all the factors qij that
depend on a given data-point Xi in the global approximation q. This makes it possible to implement
cross-validation at little extra cost [Opper and Winther  2000].

3.4 Expectation-Propagation (spike and slab prior)

To adapt our EP algorithm to the spike and slab prior of Section 2.4  we introduce latent variables
Zk = 0/1 which ”choose” for each component θk whether it comes from a slab  or from a spike 
and we consider the joint target

πξ γ(θ  z|D) ∝

B(zk; p)N (θk; 0  vzk )

exp

1{(cid:104)θ  Xi − Xj(cid:105) > 0}

On top of the n+n− Gaussian sites deﬁned in the previous section  we add a product of d sites to
approximate the prior. Following Hernandez-Lobato et al. [2013]  we use

(cid:40) d(cid:89)

k=1

(cid:41)

(cid:88)

ij

n+n−

− γ
(cid:19)

− 1
2

(cid:26)

(cid:18) pk

1 − pk

(cid:27)

qk(θk  zk) = exp

zk log

θ2
kuk + vkθk

 .

that is a (un-normalised) product of an independent Bernoulli distribution for zk  times a Gaussian
distribution for θk. Again that the site update is fairly straightforward  and may be implemented in
O(d2) time. See the supplement for more details. Another advantage of this formulation is that we
obtain a Bernoulli approximation of the marginal pseudo-posterior πξ γ(zi = 1|D) to use in feature
selection. Interestingly taking v0 to be exactly zero also yield stable results corresponding to the
case where the spike is a Dirac mass.

4 Extension to non-linear scores

To extend our methodology to non-linear score functions  we consider the pseudo-posterior

− γ

(cid:88)

πξ γ(ds|D) ∝ πξ(ds) exp

1{s(Xi) − s(Xj) > 0}

n+n−

i∈D+  j∈D−



where πξ(ds) is some prior probability measure with respect to an inﬁnite-dimensional functional
class. Let si = s(Xi)  s1:n = (s1  . . .   sn) ∈ Rn  and assume that πξ(ds) is a GP (Gaus-
sian process) associated to some kernel kξ(x  x(cid:48))  then using a standard trick in the GP literature
[Rasmussen and Williams  2006]  one may derive the marginal (posterior) density (with respect to

6

the n-dimensional Lebesgue measure) of s1:n as
πξ γ(s1:n|D) ∝ Nd (s1:n; 0  Kξ) exp

− γ

n+n−

(cid:88)

i∈D+  j∈D−

1{si − sj > 0}



where Nd (s1:n; 0  Kξ) denotes the probability density of the N (0  Kξ) distribution  and Kξ is the
n × n matrix (kξ(Xi  Xj))n
This marginal pseudo-posterior retains essentially the structure of the pseudo-posterior πξ γ(θ|D)
for linear scores  except that the “parameter” s1:n is now of dimension n. We can apply straightfor-
wardly the SMC sampler of Section 3.2  and the EP algorithm of 3.3  to this new target distribution.
In fact  for the EP implementation  the particular simple structure of a single site:

i j=1.

exp [−γ(cid:48)1{si − sj > 0}]

makes it possible to implement a site update in O(1) time  leading to an overall complexity
O(n+n− + n3) for the EP algorithm.
Theoretical
van der Vaart and van Zanten [2009]  but we leave this for future study.

this approach could be obtained by applying lemmas from e.g.

results for

5 Numerical Illustration

Figure 1 compares the EP approximation with the output of our SMC sampler  on the well-known
Pima Indians dataset and a Gaussian prior. Marginal ﬁrst and second order moments essentially
match; see the supplement for further details. The subsequent results are obtained with EP.

(a) θ1

(b) θ2

(c) θ3

Figure 1: EP Approximation (green)  compared to SMC (blue) of the marginal posterior of the ﬁrst
three coefﬁcients  for Pima dataset (see the supplement for additional analysis).

We now compare our PAC-Bayesian approach (computed with EP) with Bayesian logistic regression
(to deal with non-identiﬁable cases)  and with the rankboost algorithm [Freund et al.  2003] on dif-
ferent datasets1; note that Cortes and Mohri [2003] showed that the function optimised by rankbook
is AUC.
As mentioned in Section 3  we set the prior hyperparameters by maximizing the evidence  and we
use cross-validation to choose γ. To ensure convergence of EP  when dealing with difﬁcult sites 
we use damping [Seeger  2005]. The GP version of the algorithm is based on a squared exponential
kernel. Table 1 summarises the results; balance refers to the size of the smaller class in the data
(recall that the AUC criterion is particularly relevant for unbalanced classiﬁcation tasks)  EP-AUC
(resp. GPEP-AUC) refers to the EP approximation of the pseudo-posterior based on our Gaussian
prior (resp. Gaussian process prior). See also Figure 2 for ROC curve comparisons  and Table 1 in
the supplement for a CPU time comparison.
Note how the GP approach performs better for the colon data  where the number of covariates
(2000) is very large  but the number of observations is only 40. It seems also that EP gives a better
approximation in this case because of the lower dimensionality of the pseudo-posterior (Figure 2b).

1All available at http://archive.ics.uci.edu/ml/

7

0.00.51.0−2−100.000.250.500.75−4−3−2−100.00.51.01.5−101Dataset Covariates Balance EP-AUC GPEP-AUC Logit Rankboost

Pima
Credit
DNA
SPECTF
Colon
Glass

7
60
180
22
2000
10

34%
28%
22%
50%
40%
1%

0.8617
0.7952
0.9814
0.8684
0.7034
0.9843

0.8557
0.7922
0.9812
0.8545
0.75
0.9629

0.8646
0.7561
0.9696
0.8715
0.73
0.9029

0.8224
0.788
0.9814
0.8684
0.5935
0.9436

The Glass dataset has originally more than two classes. We compare the “silicon” class against all others.

Table 1: Comparison of AUC.

(a) Rankboost vs EP-AUC
on Pima

(b) Rankboost vs GPEP-
AUC on Colon

(c) Logistic vs EP-AUC on
Glass

Figure 2: Some ROC curves associated to the example described in a more systematic manner in
table 1. In black is always the PAC version.

Finally  we also investigate feature selection for the DNA dataset (180 covariates) using a spike and
slab prior. The regularization plot (3a) shows how certain coefﬁcients shrink to zero as the spike’s
variance v0 goes to zero  allowing for some sparsity. The aim of a positive variance in the spike is
to absorb negligible effects into it [Roˇckov´a and George  2013]. We observe this effect on ﬁgure 3a
where one of the covariates becomes positive when v0 decreases.

Figure 3: Regularization plot for v0 ∈(cid:2)10−6  0.1(cid:3) and estimation for v0 = 10−6 for DNA dataset;

(a) Regularization plot

blue circles denote posterior probabilities ≥ 0.5.

(b) Estimate

6 Conclusion

The combination of the PAC-Bayesian theory and Expectation-Propagation leads to fast and efﬁcient
AUC classiﬁcation algorithms  as observed on a variety of datasets  some of them very unbalanced.
Future work may include extending our approach to more general ranking problems (e.g. multi-
class)  establishing non-asymptotic bounds in the nonparametric case  and reducing the CPU time
by considering only a subset of all the pairs of datapoints.

8

0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00−0.3−0.2−0.10.00.11e−041e−02v0qllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−0.10.0050100150V1qBibliography

P. Alquier. Pac-bayesian bounds for randomized empirical risk minimizers. Mathematical Methods

of Statistics  17(4):279–304  2008.

P. Alquier and G. Biau. Sparse single-index model. J. Mach. Learn. Res.  14(1):243–280  2013.
P. B¨uhlmann and S. van de Geer. Statistics for High-Dimensionnal Data. Springer  2011.
O. Catoni. PAC-Bayesian Supervised Classiﬁcation  volume 56. IMS Lecture Notes & Monograph

Series  2007.

S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical minimization of U-statistics. Ann.

Stat.  36(2):844–874  04 2008a.

S. Cl´emenc¸on  V.C. Tran  and H. De Arazoza. A stochastic SIR model with contact-tracincing: large
population limits and statistical inference. Journal of Biological Dynamics  2(4):392–414  2008b.

C. Cortes and M. Mohri. Auc optimization vs. error rate minimization. In NIPS  volume 9  2003.
P. Del Moral  A. Doucet  and A. Jasra. Sequential Monte Carlo samplers. J. R. Statist. Soc. B  68

(3):411–436  2006. ISSN 1467-9868.

Y. Freund  R. Iyer  R.E Schapire  and Y. Singer. An efﬁcient boosting algorithm for combining

preferences. J. Mach. Learn. Res.  4:933–969  2003.

E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. J. Am. Statist. Assoc.  88

(423):pp. 881–889  1993.

D. Hernandez-Lobato  J. Hernandez-Lobato  and P. Dupont. Generalized Spike-and-Slab Priors for
Bayesian Group Feature Selection Using Expectation Propagation . J. Mach. Learn. Res.  14:
1891–1945  2013.

A. Jasra  D. Stephens  and C. Holmes. On population-based simulation for static inference. Statist.

Comput.  17(3):263–279  2007.

G. Lecu´e. M´ethodes d’agr´egation: optimalit´e et vitesses rapides. Ph.D. thesis  Universit´e Paris 6 

2007.

E. Mammen and A. Tsybakov. Smooth discrimination analysis. Ann. Stat.  27(6):1808–1829  12

1999.

D.A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference

on Computational learning theory  pages 230–234. ACM  1998.

T. Minka. Expectation Propagation for approximate Bayesian inference. In Proc. 17th Conf. Uncer-
tainty Artiﬁcial Intelligence  UAI ’01  pages 362–369. Morgan Kaufmann Publishers Inc.  2001.
T. J Mitchell and J. Beauchamp. Bayesian variable selection in linear regression. J. Am. Statist.

Assoc.  83(404):1023–1032  1988.

M. Opper and O. Winther. Gaussian Processes for Classiﬁcation: Mean-ﬁeld Algorithms. Neural

Computation  12(11):2655–2684  November 2000.

C. Rasmussen and C. Williams. Gaussian processes for Machine Learning. MIT press  2006.
S. Robbiano. Upper bounds and aggregation in bipartite ranking. Elec. J. of Stat.  7:1249–1271 

2013.

V. Roˇckov´a and E. George. EMVS: The EM Approach to Bayesian Variable Selection. J. Am.

Statist. Assoc.  2013.

M. Seeger. Expectation propagation for exponential families. Technical report  U. of California 

2005.

J. Shawe-Taylor and R.C. Williamson. A PAC analysis of a Bayesian estimator.

Computat. learn. theory  pages 2–9. ACM  1997.

In Proc. conf.

A.W. van der Vaart and J.H. van Zanten. Adaptive Bayesian estimation using a Gaussian random

ﬁeld with inverse Gamma bandwidth. Ann. Stat.  pages 2655–2675  2009.

M. A.J. Van Gerven  B. Cseke  F. P. de Lange  and T. Heskes. Efﬁcient Bayesian multivariate fMRI

analysis using a sparsifying spatio-temporal prior. NeuroImage  50:150–161  2010.

L. Yan  R. Dodier  M. Mozer  and R. Wolniewicz. Optimizing classiﬁer performance via an ap-
proximation to the Wilcoxon-Mann-Whitney statistic. Proc. 20th Int. Conf. Mach. Learn.  pages
848–855  2003.

9

,Francis Bach
Eric Moulines
James Ridgway
Pierre Alquier
Nicolas Chopin
Feng Liang
Noam Brown
Tuomas Sandholm