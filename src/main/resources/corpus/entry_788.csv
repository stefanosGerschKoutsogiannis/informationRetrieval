2019,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs,Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge  our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method  and the empirical results demonstrate its practical promise.,Beyond Conﬁdence Regions: Tight Bayesian

Ambiguity Sets for Robust MDPs

Reazul Hasan Russel

Department of Computer Science

University of New Hampshire

rrussel@cs.unh.edu

Marek Petrik

Department of Computer Science

University of New Hampshire

mpetrik@cs.unh.edu

Abstract

Robust MDPs (RMDPs) can be used to compute policies with provable worst-
case guarantees in reinforcement learning. The quality and robustness of an
RMDP solution are determined by the ambiguity set—the set of plausible transition
probabilities—which is usually constructed as a multi-dimensional conﬁdence
region. Existing methods construct ambiguity sets as conﬁdence regions using
concentration inequalities which leads to overly conservative solutions. This paper
proposes a new paradigm that can achieve better solutions with the same robustness
guarantees without using conﬁdence regions as ambiguity sets. To incorporate
prior knowledge  our algorithms optimize the size and position of ambiguity sets
using Bayesian inference. Our theoretical analysis shows the safety of the proposed
method  and the empirical results demonstrate its practical promise.

1

Introduction

Markov decision processes (MDPs) provide a versatile framework for modeling reinforcement
learning problems [4  33  38]. However  they assume that transition probabilities and rewards
are known exactly which is rarely the case. Limited data sets  modeling errors  value function
approximation  and noisy data are common reasons for errors in transition probabilities [16  30  45].
This results in policies that are brittle and fail when implemented. This is particularly true in the case
of batch reinforcement learning [18  20  23  32  42].
A promising framework for computing robust policies is based on Robust MDPs (RMDPs). RMDPs
relax the need for precisely known transition probabilities. Instead  transition probabilities can
take on any value from a so-called ambiguity set which represents a set of plausible transition
probabilities [9  14  24  29  32  40  46  47]. RMDPs are also reminiscent of dynamic zero-sum games:
the decision maker chooses the best actions  while the adversarial nature chooses the worst transition
probabilities from the ambiguity set.
The practical utility of using RMDPs has been hindered by the lack of good ways of constructing ambi-
guity sets that lead to solutions that are robust without being too conservative. The standard approach
to constructing ambiguity sets from concentration inequalities [1  32  42  44] leads to theoretical
guarantees but provides solutions that hopelessly conservative. Many problem-speciﬁc methods have
been proposed too  but they are hard to use and typically lack ﬁnite-sample guarantees [3  5  16  28].
The main contribution of this work is to introduce a new method for constructing ambiguity sets that
are both signiﬁcantly less conservative than existing ones [21  32  42] and also provide strong ﬁnite-
sample guarantees. Similarly to some prior work on robust reinforcement learning and optimization 
we use Bayesian assumptions to take advantage of domain knowledge which is often available [7 
8  13  47]. Our main innovation is to realize that the natural approach to building ambiguity sets
as conﬁdence intervals is unnecessarily conservative. Surprisingly  in the Bayesian setting  using a

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

95% conﬁdence region for the transition probabilities is unnecessarily conservative to achieve 95%
conﬁdence in the robustness of the solution. We also derive new L1 concentration inequalities of
possible independent interest.
The remainder of the paper is organized as follows. Section 2 formally describes the framework and
goals of the paper. Section 3 describes our main contribution  RSVF  a new method for constructing
tight ambiguity sets from Bayesian models that are adapted to the optimal policy. We provide
theoretical justiﬁcation for the robustness of RSVF  but detailed theoretical analysis of its performance
guarantees is beyond the scope of this work. Then  Section 4 overviews related work and outlines
methods that build ambiguity sets as frequentist conﬁdence regions or Bayesian credible sets. Finally 
Section 5 presents empirical results on several problem domains.

2 Problem Statement: Data-driven RMDPs

This section formalizes our goals and reviews relevant results for robust Markov decision pro-
cesses (RMDPs). Throughout the paper  we use the symbol ∆S to denote the probability simplex in
RS
+. The symbols 1 and 0 denote vectors of all ones and zeros  respectively  of an appropriate size.
The symbol I represents the identity matrix.

2.1 Safe Return Estimate: VaR

The underlying reinforcement learning problem is a Markov decision process with states S =
{1  . . .   S} and actions A = {1  . . .   A}. The rewards r : S × A → R are known but the true
transition probabilities P (cid:63) : S × A → ∆S are unknown. The transition probability vector for a state
s a. As this is a batch reinforcement learning setting  a ﬁxed dataset
s and an action a is denoted by p(cid:63)
D of transition samples is provided: D = (si ∈ S  ai ∈ A  s(cid:48)
i ∈ S)i=1 ... m. The only assumption
about D is that the state s(cid:48) in (s  a  s(cid:48)) ∈ S is distributed according to the true transition probabilities
s(cid:48) ∼ P (cid:63)(s  a ·)  no assumptions are made on the sampling policy. Note that in the Bayesian approach 
P (cid:63) is a random variable and we assume to have a prior distribution available.
The objective is to maximize the standard γ-discounted inﬁnite horizon return [33]. Because this
paper analyzes the impact of using different transition probabilities  we use a subscript to indicate
which ones are used. The optimal value function for some transition probabilities P is  therefore 
P : S → R  and the value function for a deterministic policy π : S → A is denoted as vπ
denoted as v(cid:63)
P .
The set of all deterministic stationary policies is denoted by Π. The total return ρ(π  P ) of a policy π
under transition probabilities P is:

ρ(π  P ) = pT

0 vπ
P  

where p0 is the initial distribution.
Ideally  we could compute a policy π : S → A that maximizes the return ρ(π  P (cid:63))  but P (cid:63) is
unknown. Ignoring the uncertainty in P (cid:63) completely leads to brittle policies. Instead  a common
objective in robust reinforcement learning is to maximize a plausible lower-bound on the return.
Having a safe return estimate is very important since it can inform the stakeholder that the policy
may not be good enough when deployed. The objective of computing a policy π that maximizes a
high-conﬁdence lower bound on the return can be expressed as [8  21  31  42]:

max
π∈Π

V@Rδ

P (cid:63) [ρ(π  P (cid:63))] 

(1)

where V@Rδ is the popular value-at-risk measure at a risk level δ [35]. This objective is also
sometimes known as percentile optimization [8]. It is important to note that the risk metric is applied
over possible values of the uncertain parameter and not over the distribution of returns. For example 
P (cid:63) [ρ(π  P (cid:63))] = −1 then for 5% of uncertain transition probabilities P (cid:63)  the return is −1 or
if V@R0.05
smaller.
Because solving the optimization problem in (1) is NP-hard [8]  we instead maximize a lower bound
˜ρ(π). We call this lower bound a safe return estimate and it is deﬁned as follows.
Deﬁnition 2.1 (Safe Return Estimate). The estimate ˜ρ : Π → R of return is called safe for a policy
π with probability 1 − δ if ˜ρ(π) ≤ V@Rδ

P (cid:63) [ρ(π  P (cid:63))]  or in other words if it satisﬁes:

(cid:105) ≥ 1 − δ .

(cid:104)

PP (cid:63)

˜ρ(π) ≤ ρ(π  P (cid:63)) D

2

Recall that under Bayesian assumptions  P (cid:63) is a random variable and the guarantees are conditional
on the dataset D. This is different from the frequentist approach  in which the random variable
is D and the guarantees are conditional on P (cid:63). The relative merits of Bayesian versus frequentist
approaches to robust optimization have been discussed in earlier work [8  47]  but we emphasize
that each approach presents a different set of advantages. An insightful discussion of the differences
between the two approaches can be found  for example  in Sections 5.2.2 and 6.1.1 of Murphy (2012).
The following example will be used throughout the paper to demonstrate the proposed methods and
visualize simple ambiguity sets.
Example 2.1. Consider an MDP with 3 states: s1  s2  s3 and a single action a1. Assume that the true 
but unknown  transition probability is P (cid:63)(s1  a1 ·) = [0.3  0.2  0.5]. The known prior distribution
over p(cid:63)
s1 a1 is Dirichlet with concentration parameters α = (1  1  1). The dataset D is comprised of 3
occurrences of transitions (s1  a1  s1)  2 of transitions (s1  a1  s2)  and 5 of transitions (s1  a1  s3).
The posterior distribution over p(cid:63)
sa a1 is also Dirichlet with α = (4  3  6). Note that this is a probability
distribution over transition probability distributions. Fig. 1 depicts the posterior distribution projected
onto the probability simplex along with a 90% conﬁdence region centered on the posterior mean.

2.2 Robust MDPs

Robust Markov Decision Processes (RMDPs) are a convenient model and tractable model that
generalizes MDPs. We will use RMDPs to maximize a tractable lower bound on V@R objective in
(1) and compute a safe return estimate. Our RMDP model has the same states S  actions A  rewards
rs a as the MDP. The transition probabilities for each state s and action a  denoted as ps a ∈ ∆S  are
assumed chosen adversarialy from an ambiguity set Ps a. We use P to refer cumulatively to Ps a for
all states s and actions a.
We restrict our attention to sa-rectangular ambiguity sets  which allow the adversarial nature to
choose the worst transition probability independently for each state and action [22  45]. Limitations
of rectangular ambiguity sets are known well [12  25  43] but they represent a simple  tractable  and
practical model. A convenient way of deﬁning ambiguity sets is to use a norm-distance from a given
nominal transition probability ¯ps a:

Ps a =(cid:8)p ∈ ∆S : (cid:107)p − ¯ps a(cid:107)1 ≤ ψs a

(cid:9)

(2)
for a given ψs a ≥ 0 and a nominal point ¯ps a. We focus on ambiguity sets deﬁned by the L1 norm
because they give rise to RMDPs that can be solved very efﬁciently [15].
RMDPs have properties that are similar to regular MDPs (see  for example  [2  19  22  28  45]). The

robust Bellman operator (cid:98)TP for an ambiguity set P for a state s computes the best action with respect

to the worst-case realization of the transition probabilities:

(rs a + γ · pTv)

min
p∈Ps a

(3)

((cid:98)TPv)(s) := max
The symbol (cid:98)T π
to MDPs  satisfy ˆv(cid:63) = (cid:98)TPˆv(cid:63) and ˆvπ = (cid:98)T π

a∈A

P denotes a robust Bellman update for a given stationary policy π. The optimal robust
value function ˆv(cid:63)  and the robust value function ˆvπ for a policy π are unique and must  similarly
P ˆvπ. In general  we use a hat to denote quantities in the
RMDP and omit it for the MDP. When the ambiguity set P is not obvious from the context  we use it
as a subscript ˆv(cid:63)

P. The robust return ˆρ is deﬁned as [16]:

ˆρ(π  P) = min
P∈P

ρ(π  P ) = pT

0 ˆvπ
P  

where p0 ∈ ∆S is the initial distribution. In the remainder of the paper  we describe methods that
construct P from D in order to guarantee that ˆρ is a tight lower bound on V@R of the returns.

3 Optimized Bayesian Ambiguity Sets

In this section  we describe the new algorithm for constructing Bayesian ambiguity sets that can
compute less-conservative lower bounds on the return. RSVF (robustiﬁcation with sensible value
functions) is a Bayesian method that uses samples from the posterior distribution over P (cid:63) to construct
tight ambiguity sets.

3

Figure 1: Contours of the
posterior distribution and the
90%-conﬁdence region.

Figure 2: Optimal Bayesian
ambiguity set (red) for a value
function v = (0  0  1).

Sets Ks1 a1 (vi)
Figure 3:
(dashed red) for i = 1  2 and
Ls1 a1 ({v1  v2}) (black).

Before describing the algorithm  we use the setting of Example 2.1 to motivate our approach. To
minimize distractions by technicalities  assume that the goal is to compute the return for a single
time step starting from state s1. Assume also that the value function v = (1  0  0) is known  all
rewards from s1 are 0  and γ = 1. Recall that our goal is to construct a safe return estimate ˜ρ(π) of
V@R0.1
P (cid:63) [ρ(π  P (cid:63))] at the 90% level. When the value function is known  it is possible to construct the
optimal ambiguity set P(cid:63) such that ˆρ(π) = minp∈P(cid:63) pTv = V@R0.1

P (cid:63) [ρ(π  P (cid:63))] as:

(cid:110)

P(cid:63) =

p ∈ ∆3 : pTv ≥ V@R0.1

P (cid:63) [ρ(π  P (cid:63))]

.

(cid:111)

It can be shown readily that this ambiguity set is optimal in the sense that any set for which ˜ρ(π) is
exact must be a subset of P(cid:63) [13]. Fig. 2 depicts the optimal ambiguity set along with the arrow that
indicates the direction along which v increases.
The optimal ambiguity set described above cannot be used directly  unfortunately  because the value
function is unknown. It would be tempting to construct the ambiguity set as the intersection of
optimal sets for all possible value functions; a polyhedral approximation of this set is shown in Fig. 2
using a blue color. Unfortunately  this approach is not (usually) correct and will not lead to a safe
return estimate. This can be shown from the fact that support functions to convex sets are convex and
V@R is not a convex (concave) function [6  34]; see Gupta (2015) for a more detailed discussion.
Since it is not possible  in general  to simply consider the intersection of optimal ambiguity sets for
all possible value functions  we approximate the optimal ambiguity set for a few reasonable value
functions. For this purpose  we use a set Ks a(v) deﬁned as follows:

s a)Tv(cid:3)(cid:111)
(cid:2)(p(cid:63)
(cid:2)(p(cid:63)
s a)Tv(cid:3). See Lemma B.2 for the formal statement.

where ζ = 1 − δ/(SA). The bottom dashed set in Fig. 3 depicts this set K for v = (0  0  1)
in Example 2.1. The intuition behind this construction is as follows. If any ambiguity set Ps a
P is safe: maxp∈Ks a(v) pTv ≤
intersects Ks a(ˆvπ
V@Rζ
The set Ks a(v) is sufﬁcient  when the value function is known  but we need to generalize the
approach to unknown value functions. The set Ls a(V) provides such a guarantee for a set of possible
value functions (POV) V. Its center is chosen to minimize its size while intersecting Ks a(v) for each
v in V and is constructed as follows.

P) for each state s  a then the value function ˆvπ

Ls a(V) =(cid:8)p ∈ ∆S : (cid:107)p − θs a(V)(cid:107)1 ≤ ψs a(V)(cid:9)

Ks a(v) =

p ∈ ∆S : pTv ≤ V@Rζ

P (cid:63)

(cid:110)

P (cid:63)

 

ψs a(V) = min
p∈∆S

f (p) 

θs a(V) ∈ arg min
p∈∆S

f (p) 

f (p) = max
v∈V

(cid:107)q − p(cid:107)1

min

q∈Ks a(v)

(4)

The optimization in (4) can be represented and solved as a linear program. Fig. 3 shows the set L
in black solid color. It is the smallest L1-constrained set that intersects the two K sets for value
functions v1 = (0  0  1) and v2 = (2  1  0) in Example 2.1.
We are now ready to describe RSVF  which is outlined in Algorithm 1. RSVF takes an optimistic
approach to approximating the optimal ambiguity set. It starts with a small set of potential optimal
value functions (POV) and constructs an ambiguity set that is safe for these value functions. It
keeps increasing the POV set until ˆv(cid:63) is in the set and the policy is safe. To simplify presentation 

4

s1s2s3l0.000.250.500.750.000.250.500.751.00s1s2s30.000.250.500.750.000.250.500.751.00s1s2s3+l0.000.250.500.750.000.250.500.751.00Algorithm 1: RSVF: Adapted Ambiguity Sets
Input: Conﬁdence 1 − δ and posterior PP (cid:63) [· | D]
Output: Policy π and lower bound ˜ρ(π)
1 k ← 0;
2 Pick some initial value function ˆv0;
3 Initialize POV: V0 ← ∅ ;
4 repeat
5
6
7
8

Augment POV: Vk+1 ← Vk ∪ {vk} ;
s a ← Ls a(Vk+1) ;
For all s  a update Pk+1
Solve ˆvk+1 ← ˆv(cid:63)
and ˆπk+1 ← ˆπ(cid:63)
k ← k + 1 ;

Pk+1

Pk+1

;

9 until safe for all s  a: Ks a(ˆvk) ∩ Pk
10 return (ˆπk  pT

0 ˆvk) ;

s a (cid:54)= ∅;

Algorithm 1 is not guaranteed to terminate in ﬁnite time; the actual implementation switches to BCI
described in Section 4.2 after 100 iterations  which guarantees its termination.
The following theorem states that Algorithm 1 produces a safe estimate of the true return.
Theorem 3.1. Suppose that Algorithm 1 terminates with a policy ˆπk and a value function ˆvk in the
iteration k. Then  the return estimate ˜ρ(ˆπ) = pT

(cid:105) ≥ 1 − δ.

0 ˆvk is safe: PP (cid:63)

0 ˆvk ≤ pT
pT

(cid:104)

0 v ˆπk
P (cid:63)

(cid:12)(cid:12)(cid:12) D

Before discussing the proof of Theorem 3.1  it is important to mention its limitations. This result
shows only that the return estimate ˆρ is safe; it does not show that it is good. There are  of course 
naive safe estimates such as ˜ρ(π) = (1 − γ)−1 mins a rs a. Since RSVF tightly approximates the
optimal ambiguity sets  we expect it to perform signiﬁcantly better. The theoretical analysis of this of
the approximation error of ˆρ is beyond the scope of this work and we present empirical evidence in
Section 5 instead.
All proofs can be found in Appendix B. The proof is technical but conceptually simple. It is based
on two main properties. The ﬁrst one is the construction of optimal ambiguity sets for the known
value function as outlined above. The second is the fact that the ambiguity set needs to be robust
with only with respect to the robust value function ˆv and not the optimal value function v(cid:63). This is
subtle  but crucial since ˆv is a constant while v(cid:63) is a random variable in the Bayesian setting. The
RSVF approach  therefore  does not work when frequentist guarantees are required. Conﬁdence
regions  described in Section 4  are designed for situations when robustness is required with respect
to a random variable  and are therefore overly conservative in our setting. See Appendix E for more
in-depth discussion.

4 Ambiguity Sets as Conﬁdence Regions

In this section  we describe the standard approach to constructing ambiguity sets as multidimensional
conﬁdence regions and propose its extension to the Bayesian setting. Conﬁdence regions derived
from concentration inequalities have been used previously to compute bounds on the true return in
off-policy policy evaluation [41  42]. These methods  unfortunately  do not readily generalize to the
policy optimization setting  which we target. Other work has focused on reducing variance rather than
on high-probability bounds [18  23  26]. Methods for exploration in reinforcement learning  such as
MBIE or UCRL2  also construct ambiguity sets using concentration inequalities [10  17  37  37  39]
and compute optimistic (upper) bounds to guide exploration.

4.1 Distribution-free (Frequentist) Conﬁdence Interval

Distribution-free conﬁdence regions are used widely in reinforcement learning to achieve robust-
ness [32  42] and to guide exploration [36  39]. The conﬁdence region is constructed around the
mean transition probability by combining the Hoeffding inequality with the union bound [32  44].

5

We refer to this set as a Hoeffding conﬁdence region and deﬁne it as follows for each s and a:

PH

s a =

p ∈ ∆S : (cid:107)p − ¯ps a(cid:107)1 ≤

2

ns a

log

SA2S

δ

(cid:115)

(cid:41)

 

(cid:40)

(cid:40)

where ¯ps a is the mean transition probability computed from D and ns a is the number of transitions
in D originating from state s and an action a.
Theorem 4.1. The robust value function ˆvPH for the ambiguity set PH satisﬁes:

PD [ˆvπ

PH ≤ vπ

P (cid:63)   ∀π ∈ Π | P (cid:63)] ≥ 1 − δ .

(5)

In addition  if ˆπ(cid:63)

PH is the optimal solution to the RMDP  then pT

0 ˆv(cid:63)

PH is a safe return estimate of ˆπ(cid:63)

PH .

To better understand the limitations of using concentration inequalities  we compare with new  and
signiﬁcantly tighter  frequentist ambiguity sets. The size of PH grows as a square root of the number
of states because of the 2S term. This means that the size of D must scale about quadratically with the
number of states to achieve the same conﬁdence. Under some restrictive assumptions  the ambiguity
set can be shown to be:

(cid:115)

(cid:41)

PM

s a =

p ∈ ∆S : (cid:107)p − ¯ps a(cid:107)1 ≤

2

ns a

log

S2A

δ

.

This auxiliary result is proved in Appendix C.1. We emphasize that the aim of this bound is
to understand the limitations of distribution free bounds  and we use it even when the necessary
assumptions are violated.

4.2 Bayesian Credible Region (BCI)

We now describe how to construct ambiguity sets from Bayesian credible (or conﬁdence) regions. To
the best of our knowledge  this approach has not been studied explicitly. The construction starts with
a (hierarchical) Bayesian model that can be used to sample from the posterior probability of P (cid:63) given
data D. The implementation of the Bayesian model is irrelevant as long as it generates posterior
samples efﬁciently. For example  one may use a Dirichlet posterior  or use MCMC sampling libraries
like JAGS  Stan  or others [11].
The posterior distribution is used to optimize for the smallest ambiguity set around the mean transition
probability. Smaller sets  for a ﬁxed nominal point  are likely to result in less conservative robust
estimates. The BCI ambiguity set is deﬁned as follows:

s a =(cid:8)p ∈ ∆S : (cid:107)p − ¯ps a(cid:107)1 ≤ ψB

s a

(cid:9)  

PB

¯ps a = EP (cid:63) [p(cid:63)

There is no closed-form expression for the Bayesian ambiguity set size. It must be computed by
solving the following optimization problem for each state s and action a:

(cid:26)

ψ : P(cid:2)(cid:107)p(cid:63)

s a − ¯ps a(cid:107)1 > ψ | D(cid:3) <

ψB

s a = min
ψ∈R+

The nominal point ¯ps a is ﬁxed (not optimized) to preserve tractability. This optimization problem can
be solved by the Sample Average Approximation (SAA) algorithm [35]. Algorithm 2  in the appendix 
summarizes the sort-based method. The main idea is to sample from the posterior distribution and
then choose the minimal size ψs a that satisﬁes the constraint. We assume that it is possible to draw
enough samples from P (cid:63) that the sampling error becomes negligible. Because the ﬁnite-sample
analysis of SAA is simple but tedious  we omit it.
Theorem 4.2. The robust value function ˆvPB for the ambiguity set PB satisﬁes:

PP (cid:63) [ˆvπ

PB ≤ vπ

P (cid:63)   ∀π ∈ Π | D] ≥ 1 − δ .

In addition  if ˆπ(cid:63)

PB is the optimal solution to the RMDP  then pT

0 ˆv(cid:63)

PB is a safe return estimate of ˆπ(cid:63)

PB .

The proof is provided in Appendix B. Similar to other results  this theorem only proves that the
constructed lower bound on the return is safe. It does not address the tightness of the bound.

6

s a | D] .
(cid:27)

.

δ
SA

Figure 4: Expected regret of safe estimates
with 95% conﬁdence regions for the Bellman
update with an uninformative prior.

Figure 5: Expected regret of safe estimates
with 95% conﬁdence regions for the Bellman
update with an informative prior.

5 Empirical Evaluation

In this section  we empirically evaluate the safe estimates computed using Hoeffding  BCI  and
RSVF ambiguity sets. We start by assuming a true model and generate simulated datasets from it.
Each dataset is then used to construct an ambiguity set and a safe estimate of policy return. The
performance of the methods is measured using the average of the absolute errors of the estimates
compared with the true returns of the optimal policies. All of our experiments use a 95% conﬁdence
for the safety of the estimates.
We compare ambiguity sets constructed using BCI  RSVF  with the Hoeffding sets. To reduce the
conservativeness of Hoeffding sets when transition probabilities are sparse  we use a modiﬁcation
inspired by the Good-Turing bounds [39]. That is that any transitions from s  a to s(cid:48) are impossible
if they are not in D. We also compare with the “Hoeffding Monotone” formulation PM even when
there is no guarantee that the value function is really monotone. Finally  we compare the results with
the “Mean Transition” which solves the expected model ¯ps a with no safety guarantees.
Next in Section 5.1  we compare the methods in a simpliﬁed setting in which we consider the problem
of estimating the value of a single state from a Bellman update. Then  Section 5.2  evaluates the
approach on an MDP with an informative prior.
We do not evaluate the computational complexity of the methods since they target problems con-
strained by data and not computation. The Bayesian methods are generally more computationally
demanding but the scale depends signiﬁcantly on the type of the prior model used. All Bayesian
methods draw 1  000 samples from the posterior for each state and action.

5.1 Bellman Update

In this section  we consider a transition from a single state s0 and action a0 to 5 states s1  . . .   s5. The
value function for the states s1  . . .   s5 is ﬁxed to be [1  2  3  4  5]. RSVF is run for a single iteration
with the given value function. The single iteration of RSVF in this simplistic setting helps to quantify
the possible beneﬁt of using RSVF-style methods over BCI. The ground truth is generated from the
corresponding prior for each one of the problems.

Uninformative Dirichlet Priors This setting considers a uniform Dirichlet distribution with α =
[1  1  1  1  1] as the prior. This prior provides little information. Figure 4 compares the computed
robust return errors. The value ξ represents the regret of predicted returns  which is the absolute
difference between the true optimal value and the robust estimate: ξ = |ρ(π(cid:63)
P (cid:63)   P (cid:63)) − ˜ρ(ˆπ(cid:63))|. Here 
˜ρ is the robust estimate and ˆπ(cid:63) is the optimal robust solution. The smaller the value  the tighter and
less conservative the safe estimate is. The number of samples is the size of dataset D. All results are
computed by averaging over 200 simulated datasets of the given size generated from the true P (cid:63). The
results show that BCI improves on both types of Hoeffding bounds and RSVF further improves on
BCI. The mean estimate provides the tightest bounds  but it does not provide any meaningful safety
guarantees.

7

20406080100Number of samples10−1100Calculated return error: [ξ]Mean TransitionHoeffdingHoeffding MonotoneBCIRSVF20406080100Number of samples10−1100Calculated return error: [ξ]Mean TransitionHoeffdingHoeffding MonotoneBCIRSVFFigure 6: Expected regret of safe estimates
with 95% conﬁdence regions for the River-
Swim: an MDP with an uninformative prior.

Figure 7: Expected regret of safe estimates
with 90% conﬁdence regions for the ExpPop-
ulation: an MDP with an informative prior.

Informative Gaussian Priors To evaluate the effect of using an informative prior  we use a problem
inspired by inventory optimization. The states s1  . . .   s5 represent inventory levels. The inventory
level corresponds to the state index (1 in the state s1) except that the inventory in the current state
s0 is 5. The demand is assumed to be Normally distributed with an unknown mean µ and a known
standard deviation σ = 1. The prior over µ is Normal with the mean µ0 = 3 and  therefore  the
posterior over µ is also Normal. The current action assumes that no product is ordered and  therefore 
only the demand is subtracted from s0.

5.2 Full MDP

In this section  we evaluate the methods using MDPs with relatively small state-spaces. They can be
used with certain types of value function approximation  like aggregation [30]  but we evaluate them
only on tabular problems to prevent approximation errors from skewing the results. To prevent the
sampling policy from inﬂuencing the results  each dataset D has the same number of samples from
each state.

Uninformative Prior We ﬁrst use the standard RiverSwim domain for the evaluation [36]. The
methods are evaluated identically to the Bellman update above. That is  we generate synthetic datasets
from the ground truth and then compare the expected regret of the robust estimate with respect to the
true return of the optimal policy for the ground truth. As the prior distribution  we use the uniform
Dirichlet distribution over all states. Figure 6 shows the expected robust regret over 100 repetitions.
The x-axis represents the number of samples in D for each state. It is apparent that BCI improves
only slightly on the Hoeffding sets since the prior is not informative. RSVF  on the other hand  shows
a signiﬁcant improvement over BCI. All robust methods have safety violations of 0% indicating that
even RSVF is unnecessarily conservative here.

Informative Prior Next  we evaluate RSVF on the MDP model of a simple exponential population
model [43]. Robustness plays an important role in ecological models because they are often complex 
stochastic  and data collection is expensive. Yet  it is important that the decisions are robust due to
their long term impacts. Figure 7 shows the average regret of safe predictions. BCI can leverage the
prior information to compute tighter bounds  but RSVF further improves on BCI. The rate of safety
violations is again 0% for all robust methods.

6 Summary and Conclusion

This paper proposes new Bayesian algorithms for constructing ambiguity sets in RMDPs  improving
over standard distribution-free methods. BCI makes it possible to ﬂexibly incorporate prior domains
knowledge and is easy to generalize to other shapes of ambiguity sets (like L2) without having
to prove new concentration inequalities. Finally  RSVF improves on BCI by constructing tighter
ambiguity sets that are not conﬁdence regions. Our experimental results and theoretical analysis
indicate that the new ambiguity sets provide much tighter safe return estimates. The only drawbacks
of the Bayesian methods are that they need priors and may increase the computational complexity.

8

20406080100Number of samples05101520253035Calculated return error: [ξ]Mean TransitionHoeffdingHoeffding MonotoneBCIRSVF20406080100Number of samples020406080100Calculated return error: [ξ]Mean TransitionHoeffdingHoeffding MonotoneBCIRSVFAcknowledgments

We would like to thank Vishal Gupta and the anonymous referees for their insightful comments and
suggestions. This work was supported by NSF under grants number 1815275 and 1717368.

References
[1] Auer  P.  Jaksch  T.  and Ortner  R. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research  11(1):1563–1600  2010.

[2] Bagnell  J. A.  Ng  A. Y.  and Schneider  J. G. Solving Uncertain Markov Decision Processes.

Carnegie Mellon Research Showcase  pp. 948–957  2001.

[3] Ben-Tal  A.  El Ghaoui  L.  and Nemirovski  A. Robust Optimization. Princeton University

Press  2009.

[4] Bertsekas  D. P. and Tsitsiklis  J. N. Neuro-dynamic programming. 1996.
[5] Bertsimas  D.  Kallus  N.  and Gupta  V. Data-driven robust optimization. Springer Berlin

Heidelberg  2017.

[6] Boyd  S. and Vandenberghe  L. Convex Optimization. Cambridge University Press  Cambridge 

2004.

[7] Castro  P. S. and Precup  D. Smarter Sampling in Model-Based Bayesian Reinforcement
Learning. Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2010.
LNAI  6321:200–2014  2010.

[8] Delage  E. and Mannor  S. Percentile Optimization for Markov Decision Processes with

Parameter Uncertainty. Operations Research  58(1):203–213  2010.

[9] Delgado  K. V.  De Barros  L. N.  Dias  D. B.  and Sanner  S. Real-time dynamic programming
for Markov decision processes with imprecise probabilities. Artiﬁcial Intelligence  230:192–223 
2016.

[10] Dietterich  T.  Taleghan  M.  and Crowley  M. PAC optimal planning for invasive species
management: Improved exploration for reinforcement learning from simulator-deﬁned MDPs.
AAAI  2013.

[11] Gelman  A.  Carlin  J. B.  Stern  H. S.  and Rubin  D. B. Bayesian Data Analysis. Chapman and

Hall/CRC  3rd edition  2014.

[12] Goyal  V. and Grand-Clement  J. Robust Markov Decision Process: Beyond Rectangularity.

Technical report  2018.

[13] Gupta  V. Near-Optimal Bayesian Ambiguity Sets for Distributionally Robust Optimization.

2015.

[14] Hanasusanto  G. and Kuhn  D. Robust Data-Driven Dynamic Programming. In Advances in

Neural Information Processing Systems (NIPS)  2013.

[15] Ho  C. P.  Petrik  M.  and Wiesemann  W. Fast Bellman Updates for Robust MDPs.
International Conference on Machine Learning (ICML)  volume 80  pp. 1979–1988  2018.

In

[16] Iyengar  G. N. Robust dynamic programming. Mathematics of Operations Research  30(2):

257–280  2005.

[17] Jaksch  T.  Ortner  R.  and Auer  P. Near-optimal Regret Bounds for Reinforcement Learning.

Journal of Machine Learning Research  11(1):1563–1600  2010.

[18] Jiang  N. and Li  L. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning.

In International Conference on Machine Learning (ICML)  2015.

[19] Kalyanasundaram  S.  Chong  E. K. P.  and Shroff  N. B. Markov decision processes with
uncertain transition rates: Sensitivity and robust control. In IEEE Conference on Decision and
Control  pp. 3799–3804  2002.

[20] Lange  S.  Gabel  T.  and Riedmiller  M. Batch Reinforcement Learning. In Reinforcement

Learning  pp. 45–73. 2012.

[21] Laroche  R. and Trichelair  P. Safe Policy Improvement with Baseline Bootstrapping  2018.

9

[22] Le Tallec  Y. Robust  Risk-Sensitive  and Data-driven Control of Markov Decision Processes.

PhD thesis  MIT  2007.

[23] Li  L.  Munos  R.  and Szepesvári  C. Toward Minimax Off-policy Value Estimation.

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

In

[24] Mannor  S.  Mebel  O.  and Xu  H. Lightning does not strike twice: Robust MDPs with coupled

uncertainty. In International Conference on Machine Learning (ICML)  2012.

[25] Mannor  S.  Mebel  O.  and Xu  H. Robust MDPs with k-rectangular uncertainty. Mathematics

of Operations Research  41(4):1484–1509  2016.

[26] Munos  R.  Stepleton  T.  Harutyunyan  A.  and Bellemare  M. G. Safe and Efﬁcient Off-Policy
Reinforcement Learning. In Conference on Neural Information Processing Systems (NIPS) 
2016.

[27] Murphy  K. Machine Learning: A Probabilistic Perspective. 2012.
[28] Nilim  A. and El Ghaoui  L. Robust control of Markov decision processes with uncertain

transition matrices. Operations Research  53(5):780–798  2005.

[29] Petrik  M. Approximate dynamic programming by minimizing distributionally robust bounds.

In International Conference of Machine Learning (ICML)  2012.

[30] Petrik  M. and Subramanian  D. RAAM : The beneﬁts of robustness in approximating aggregated

MDPs in reinforcement learning. In Neural Information Processing Systems (NIPS)  2014.

[31] Petrik  M.  Chow  Y.  and Ghavamzadeh  M. Safe Policy Improvement by Minimizing Robust
Baseline Regret. In ICML Workshop on Reliable Machine Learning in the Wild  pp. 1–25  2016.
[32] Petrik  M.  Mohammad Ghavamzadeh  and Chow  Y. Safe Policy Improvement by Minimizing
Robust Baseline Regret. In Advances in Neural Information Processing Systems (NIPS)  2016.
[33] Puterman  M. L. Markov decision processes: Discrete stochastic dynamic programming. 2005.
[34] Shapiro  A.  Dentcheva  D.  and Ruszczynski  A. Lectures on Stochastic Programming. SIAM 

2009.

[35] Shapiro  A.  Dentcheva  D.  and Ruszczynski  A. Lectures on stochastic programming: Modeling

and theory. 2014.

[36] Strehl  A. and Littman  M. An analysis of model-based Interval Estimation for Markov Decision

Processes. Journal of Computer and System Sciences  74:1309–1331  2008.

[37] Strehl  A. L. Probably Approximately Correct (PAC) Exploration in Reinforcement Learning.

PhD thesis  Rutgers University  2007.

[38] Sutton  R. S. and Barto  A. Reinforcement learning. 1998.
[39] Taleghan  M. A.  Dietterich  T. G.  Crowley  M.  Hall  K.  and Albers  H. J. PAC Optimal MDP
Planning with Application to Invasive Species Management. Journal of Machine Learning
Research  16:3877–3903  2015.

[40] Tamar  A.  Mannor  S.  and Xu  H. Scaling up Robust MDPs Using Function Approximation.

In International Conference of Machine Learning (ICML)  2014.

[41] Thomas  P. S. and Brunskill  E. Data-efﬁcient off-policy policy evaluation for reinforcement

learning. In International Conference of Machine Learning (ICML)  2016.

[42] Thomas  P. S.  Teocharous  G.  and Ghavamzadeh  M. High Conﬁdence Off-Policy Evaluation.

In Annual Conference of the AAAI  2015.

[43] Tirinzoni  A.  Milano  P.  Chen  X.  and Ziebart  B. D. Policy-Conditioned Uncertainty Sets for
Robust Markov Decision Processes. In Neural Information Processing Systems (NIPS)  2018.
[44] Weissman  T.  Ordentlich  E.  Seroussi  G.  Verdu  S.  and Weinberger  M. J. Inequalities for the

L1 deviation of the empirical distribution. 2003.

[45] Wiesemann  W.  Kuhn  D.  and Rustem  B. Robust Markov decision processes. Mathematics of

Operations Research  38(1):153–183  2013.

[46] Xu  H. and Mannor  S. The robustness-performance tradeoff in Markov decision processes.

Advances in Neural Information Processing Systems (NIPS)  2006.

[47] Xu  H. and Mannor  S. Parametric regret in uncertain Markov decision processes. In IEEE

Conference on Decision and Control (CDC)  pp. 3606–3613  2009.

10

,Guanhong Tao
Shiqing Ma
Yingqi Liu
Xiangyu Zhang
Marek Petrik
Reazul Hasan Russel