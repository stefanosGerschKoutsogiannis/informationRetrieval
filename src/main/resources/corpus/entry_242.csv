2014,A Unified Semantic Embedding: Relating Taxonomies and Attributes,We propose a method that learns a discriminative yet semantic space for object categorization  where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information  we explicitly embed the semantic entities into the same space where we embed categories  which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics  we enforce each category to be generated as a sparse combination of a supercategory + attributes  with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a better generalizing model  as well as generates compact semantic description of each category  which enables humans to analyze what has been learned.,A Uniﬁed Semantic Embedding:

Relating Taxonomies and Attributes

Sung Ju Hwang∗
Disney Research
Pittsburgh  PA

sungju.hwang@disneyresearch.com

Abstract

Leonid Sigal

Disney Research
Pittsburgh  PA

lsigal@disneyresearch.com

We propose a method that learns a discriminative yet semantic space for object
categorization  where we also embed auxiliary semantic entities such as supercat-
egories and attributes. Contrary to prior work  which only utilized them as side in-
formation  we explicitly embed these semantic entities into the same space where
we embed categories  which enables us to represent a category as their linear com-
bination. By exploiting such a uniﬁed model for semantics  we enforce each cate-
gory to be generated as a supercategory + a sparse combination of attributes  with
an additional exclusive regularization to learn discriminative composition. The
proposed reconstructive regularization guides the discriminative learning process
to learn a model with better generalization. This model also generates compact se-
mantic description of each category  which enhances interoperability and enables
humans to analyze what has been learned.

1

Introduction

Object categorization is a challenging problem that requires drawing boundaries between groups of
objects in a seemingly continuous space. Semantic approaches have gained a lot of attention recently
as object categorization became more focused on large-scale and ﬁne-grained recognition tasks and
datasets. Attributes [1  2  3  4] and semantic taxonomies [5  6  7  8] are two popular semantic sources
which impose certain relations between the category models  including a more recently introduced
analogies [9] that induce even higher-order relations between them. While many techniques have
been introduced to utilize each of the individual semantic sources for object categorization  no uni-
ﬁed model has been proposed to relate them.
We propose a uniﬁed semantic model where we can learn to place categories  supercategories  and
attributes as points (or vectors) in a hypothetical common semantic space  and taxonomies provide
speciﬁc topological relationships between these semantic entities. Further  we propose a discrimi-
native learning framework  based on dictionary learning and large margin embedding  to learn each
of these semantic entities to be well separated and pseudo-orthogonal  such that we can use them to
improve visual recognition tasks such as category or attribute recognition.
However  having semantic entities embedded into a common space is not enough to utilize the
vast number of relations that exist between the semantic entities. Thus  we impose a graph-based
regularization between the semantic embeddings  such that each semantic embedding is regularized
by sparse combination of auxiliary semantic embeddings. This additional requirement imposed on
the discriminative learning model would guide the learning such that we obtain not just the optimal
model for class discrimination  but to learn a semantically plausible model which has a potential to
be more robust and human-interpretable; we call this model Uniﬁed Semantic Embedding (USE).

∗Now at Ulsan National Institute of Science and Technology in Ulsan  South Korea

1

Figure 1: Concept: We regularize each category
to be represented by its supercategory + a sparse
combination of attributes  where the regularization
parameters are learned. The resulting embedding
model improves the generalization ability by the
speciﬁc relations between the semantic entities  and
also is able to compactly represent a novel category
in this manner. For example  given a novel category
tiger  our model can describe it as a striped feline.

The observation we make to draw the relation between the categories and attributes  is that a category
can be represented as the sum of its supercategory + the category-speciﬁc modiﬁer  which in many
cases can be represented by a combination of attributes. Further  we want the representation to be
compact. Instead of describing a dalmatian as a domestic animal with a lean body  four legs  a
long tail  and spots  it is more efﬁcient to say it is a spotted dog (Figure 1). It is also more exact
since the higher-level category dog contains all general properties of different dog breeds  including
indescribable dog-speciﬁc properties  such as the shape of the head  and its posture.
This exempliﬁes how a human would describe an object  to efﬁciently communicate and understand
the concept. Such decomposition of a category into attributes+supercategory can hold for categories
at any level. For example  supercategory feline can be described as a stalking carnivore.
With the addition of this new generative objective  our goal is to learn a discriminative model that
can be compactly represented as a combination of semantic entities  which helps learn a model that is
semantically more reasonable. We want to balance between these two discriminative and generative
objectives when learning a model for each object category. For object categories that have scarce
training examples  we can put more weight on the generative part of the model.
Contributions: Our contributions are threefold: (1) We show a multitask learning formulation for
object categorization that learns a uniﬁed semantic space for supercategories and attributes  while
drawing relations between them. (2) We propose a novel sparse-coding based regularization that
enforces the object category representation to be reconstructed as the sum of a supercategory and a
sparse combination of attributes. (3) We show from the experiments that the generative learning with
the sparse-coding based regularization helps improve object categorization performance  especially
in the one or few-shot learning case  by generating semantically plausible predictions.

2 Related Work
Semantic methods for object recognition. For many years  vision researchers have sought to
exploit external semantic knowledge about the object to incorporate semantics into learning of the
model. Taxonomies  or class hierarchies were the ﬁrst to be explored by vision researchers [5  6]  and
were mostly used to efﬁciently rule out irrelevant category hypotheses leveraging class hierarchical
structure [8  10]. Attributes are visual or semantic properties of an object that are common across
multiple categories  mostly regarded as describable mid-level representations. They have been used
to directly infer categories [1  2]  or as additional supervision to aid the main categorization problem
in the multitask learning framework [3]. While many methods have been proposed to leverage either
of these two popular types of semantic knowledge  little work has been done to relate the two  which
our paper aims to address.

Discriminative embedding for object categorization. Since the conventional kernel-based mul-
ticlass SVM does not scale due to its memory and computational requirements for today’s large-scale
classiﬁcation tasks  embedding-based methods have gained recent popularity. Embedding-based
methods perform classiﬁcation on a low dimensional shared space optimized for class discrimina-
tion. Most methods learn two linear projections  for data instances and class labels  to a common
lower-dimensional space optimized by ranking loss. Bengio et al. [10] solves the problem using
stochastic gradient  and also provides a way to learn a tree structure which enables one to efﬁciently
predict the class label at the test time. Mensink et al. [11] eliminated the need of class embedding by
replacing them with the class mean  which enabled generalization to new classes at near zero cost.
There are also efforts in incorporating semantic information into the learned embedding space.
Weinberger et al. [7] used the taxonomies to preserve the inter-class similarities in the learned space 

2

in terms of distance. Akata et al. [4] used attributes and taxonomy information as labels  replacing
the conventional unit-vector based class representation with more structured labels to improve on
zero-shot performance. One most recent work in this direction is DEVISE [12]  which learns em-
beddings that maximize the ranking loss  as an additional layer on top of the deep network for both
images and labels. However  these models impose structure only on the output space  and structure
on the learned space is not explicitly enforced  which is our goal.
Recently  Hwang et al. [9] introduced one such model  which regularizes the category quadruplets 
that form an analogy  to form a parallelogram. Our goal is similar  but we explore a more general
compositional relationship  which we learn without any manual supervision.
Multitask learning. Our work can be viewed as a multitask learning method  since we relate
each model for different semantic entities by learning both the joint semantic space and enforcing
geometric constraints between them. Perhaps the most similar work is [13]  where the parameter
of each model is regularized while ﬁxing the parameter for its parent-level models. We use similar
strategy but instead of enforcing sharing between the models  we simply learn each model to be
close to its approximation obtained using higher-level (more abstract) concepts.
Sparse coding. Our method to approximate each category embedding as a sum of its direct super-
category plus a sparse combination of attributes  is similar to the objective of sparse coding. One
work that is speciﬁcally relevant to ours is Mairal et al. [14]  where the learning objective is to re-
duce both the classiﬁcation and reconstruction error  given class labels. In our model  however  the
dictionary atoms are also discriminatively learned with supervision  and are assembled to be a se-
mantically meaningful combination of a supercategory + attributes  while [14] learns the dictionary
atoms in an unsupervised way.
3 Approach

We now explain our uniﬁed semantic embedding model  which learns a discriminative common
low-dimensional space to embed both the images and semantic concepts including object categories 
while enforcing relationships between them using semantic reconstruction.
Suppose that we have a d-dimensional image descriptor and m-dimensional vector describing labels
associated with the instances  including category labels at different semantic granularities and at-
tributes. Our goal then is to embed both images and the labels onto a single uniﬁed semantic space 
where the images are associated with their corresponding semantic labels.
To formally state the problem  given a training set D that has N labeled examples  i.e. D =
{xi  yi}N
i=1  where xi ∈ Rd denotes image descriptors and yi ∈ {1  . . .   m} are their labels as-
sociated with m unique concepts  we want to embed each xi as zi  and each label yi as uyi in the
de-dimensional space  such that the similarity between zi and uyi  S(zi  uyi) is maximized.
One way to solve the above problem is to use regression  using S(zi  uyi) = −(cid:107)zi− uyi(cid:107)2
2. That is 
we estimate the data embedding zi as zi = W xi  and minimize their distances to the correct label
embeddings uyi ∈ Rm where the dimension for yi is set to 1 and every other dimension is set to 0:
(1)

(cid:107)W xi − uyi(cid:107)2

2 + λ(cid:107)W(cid:107)2
F .

N(cid:88)

m(cid:88)

min
W

c=1

i=1

The above ridge regression will project each instance close to its correct embedding. However  it
does not guarantee that the resulting embeddings are well separated. Therefore  most embedding
methods for categorization add in discriminative constraints which ensure that the projected in-
stances have higher similarity to their own category embedding than to others. One way to enforce
2 +ξic  yi (cid:54)= c
this is to use large-margin constraints on distance: (cid:107)W xi−uyi(cid:107)2
which can be translated into to the following discriminative loss:

2 +1 ≤ (cid:107)W xi−uc(cid:107)2

LC(W   U   xi  yi) =

2 − (cid:107)W xi − uc(cid:107)2

2]+ ∀c (cid:54)= yi 

(2)

(cid:88)
[1 + (cid:107)W xi − uyi(cid:107)2

c

where U is the columwise concatenation of each label embedding vector  such that uj denotes jth
column of U. After replacing the generative loss in the ridge regression formula with the discrimi-
native loss  we get the following discriminative learning problem:
F + λ(cid:107)U(cid:107)2

LC(W   U   xi  yi) + λ(cid:107)W(cid:107)2

F   yi ∈ {1  . . .   m} 

N(cid:88)

(3)

min
W  U

i

3

where λ regularizes W and U from shooting to inﬁnity. This is one of the most common objective
used for learning discriminative category embeddings for multi-class classiﬁcation [10  7]  while
ranking loss-based [15] models have been also explored for LC. Bilinear model on a single variable
W has been also used in Akata et al. [4]  which uses structured labels (attributes) as uyi.

3.1 Embedding auxiliary semantic entities.

Now we describe how we embed the supercategories and attributes onto the learned shared space.
Supercategories. While our objective is to better categorize entry level categories  categories in
general can appear at different semantic granularities. For example  a zebra could be both an equus 
and an odd-toed ungulate. To learn the embeddings for the supercategories  we map each data
instance to be closer to its correct supercategory embedding than to its siblings: (cid:107)W xi−us(cid:107)2
2 +1 ≤
(cid:107)W xi − uc(cid:107)2
2 + ξsc ∀s ∈ Pyi and c ∈ Ss where Pyi denotes the set of superclasses at all levels
for class yi  and Ss is the set of its siblings. The constraints can be translated into the following loss
term:

[1 + (cid:107)W xi − us(cid:107)2

2 − (cid:107)W xi − uc(cid:107)2

2]+.

(4)

LS(W   U   xi  yi) =

(cid:88)

(cid:88)

s∈Pyi

c∈Ss

Attributes. Attributes can be considered normalized basis vectors for the semantic space  whose
combination represents a category. Basically  we want to maximize the correlation between the
projected instance that possess the attribute  and its correct attribute embedding  as follows:

(5)
where Ac is the set of all attributes for class c  σ is the margin (can be empirically determined; we
simply use a ﬁxed value of σ = 1)  ya
i is the label indicating presence/absence of each attribute a
for the ith training instance  and ua is its embedding vector for attribute a.

a∈Ayi

i ua]+ (cid:107)ua(cid:107)2 ≤ 1  ya

i ∈ {0  1}

LA(W   U   xi  yi) =

[σ − (W xi)Tya

(cid:88)

3.2 Relationship between the categories  supercategories  and attributes
Simply summing up all previously deﬁned loss functions while adding {us} and {ua} as addi-
tional columns of U will result in a multi-task formulation that implicitly associate the semantic
entities  through the shared data embedding W . However  we want to further utilize the relation-
ships between the semantic entities  to explicitly impose structural regularization on the semantic
embeddings U. One simple and intuitive relation is that an object class can be represented as the
combination of its parent level category plus a sparse combination of attributes  which translates into
the following constraint:

uc = up + U Aβc  c ∈ Cp (cid:107)βc(cid:107)0 (cid:22) γ1  βc (cid:23) 0 ∀c  p ∈ {1  . . .   C + S} 

(6)
where U A is the aggregation of all attribute embeddings {ua}  Cp is the set of children classes for
class p  γ1 is the sparsity parameter  C is the number of leaf level categories  and S is the number
of supercategories. We require β to be non-negative  since it makes more sense and more efﬁcient
to describe an object with attributes that it might have  rather than describing it by attributes that it
might not have.
We rewrite Eq. 6 into a regularization term as follows  replacing the (cid:96)0-norm constraints with (cid:96)1-
norm regularizations for tractable optimization:

c

(cid:107)uc − up − U Aβc(cid:107)2

R(U   B) =
2 + γ2(cid:107)βc + βo(cid:107)2
2 
c ∈ Cp  o ∈ Pc ∪ Sc  0 (cid:22) βc (cid:22) γ1 ∀c  p ∈ {1  . . .   C + S} 

(7)
where B is the matrix whose jth column vector βj is the reconstruction weight for class j  Sc is the
set of all sibling classes for class c  and γ2 is the parameters to enforce exclusivity.
The exclusive regularization term is used to prevent the semantic reconstruction βc for class c from
ﬁtting to the same attributes ﬁtted by its parents and siblings. This is because attributes common
across parent and child  and between siblings  are less discriminative. This regularization is es-
pecially useful for discrimination between siblings  which belong to the same superclass and only
differ by the category-speciﬁc modiﬁer. By generating unique semantic decomposition for each
class  we can better discriminate between any two categories using a semantic combination of dis-
criminatively learned auxiliary entities.

C(cid:88)

4

With the sparsity regularization enforced by γ1  the simple sum of the two weights will prevent the
two (super)categories from having high weight for a single attribute  which will let each category
embedding to ﬁt to exclusive attribute set. This  in fact  is the exclusive lasso regularizer introduced
in [16]  except for the nonnegativity constraint on βc  which makes the problem easier to solve.
3.3 Uniﬁed semantic embeddings for object categorization

After augmenting the categorization objective in Eq. 3 with the superclass and attributes loss and the
sparse-coding based regularization in Eq. 6  we obtain the following multitask learning formulation
that jointly learns all the semantic entities along with the sparse-coding based regularization:
LC (W   U   xi  yi) + µ1 (LS(W   U   xi  yi) + LA(W   U   xi  yi)) + µ2R(U   B);
2 ≤ λ  0 (cid:22) βc (cid:22) γ1 ∀j ∈ {1  . . .   d} ∀k ∈ {1  . . .   m} ∀c  p ∈ {1  . . .   C + S} 

min
2 ≤ λ (cid:107)uk(cid:107)2

(cid:107)wj(cid:107)2

(8)

N(cid:88)

i=1

W  U  B

where wj is W ’s jth column  and µ1 and µ2 are parameters to balance between the main and
auxiliary tasks  and discriminative and generative objective.
Eq. 8 could be also used for knowledge transfer when learning a model for a novel set of categories 
by replacing U A in R(U   B) with US  learned on class set S to transfer the knowledge from.

3.4 Numerical optimization

Eq. 8 is not jointly convex in all variables  and has both discriminative and generative terms. This
problem is similar to the problem in [14]  where the objective is to learn the dictionary  sparse
coefﬁcients  and classiﬁer parameters together  and can be optimized using a similar alternating
optimization  while each subproblem differs. We ﬁrst describe how we optimize for each variable.
Learning of W and U. The optimization of both embedding models are similar  except for the
reconstructive regularization on U. and the main bottleneck lies in the minimization of the O(N m)
large-margin losses. Since the losses are non-differentiable  we solve the problems using stochastic
subgradient method. Speciﬁcally  we implement the proximal gradient algorithm in [17]  handling
the (cid:96)-2 norm constraints with proximal operators.
Learning B. This is similar to the sparse coding problem  but simpler. We use projected gradient
method  where at each iteration t  we project the solution of the objective βt+ 1
for category c to (cid:96)-1
norm ball and nonnegative orthant  to obtain βt
Alternating optimization. We decompose Eq. 8 to two convex problems: 1) Optimization of the
data embedding W and approximation parameter B (Since the two variable do not have direct link
between them)   and 2) Optimization of the category embedding U. We alternate the process of
optimizing each of the convex problems while ﬁxing the remaining variables  until the convergence
criterion 1 is met  or the maximum number of iteration is reached.
Run-time complexity. Training: Optimization of W and U using proximal stochastic gradi-
ent [17]  have time complexities of O(ded(k + 1)) and O(de(dk + m)) respectively. Both terms are
dominated by the gradient computation for k(k (cid:28) N ) sampled constraints  that is O(dedk). Outer
loop for alternation converges within 5-10 iterations depending on . Test: Test time complexity is
exactly the same as in LME  which is O(de(C + d)).
4 Results

c
c that satisﬁes the constraints.

2

We validate our method for multiclass categorization performance on two different datasets gener-
ated from a public image collection  and also test for knowledge transfer on few-shot learning.

4.1 Datasets

We use Animals with Attributes dataset [1]  which consists of 30  475 images of 50 animal classes 
with 85 class-level attributes 2. We use the Wordnet hierarchy to generate supercategories. Since

1(cid:107)W t+1 − W t(cid:107)2 + (cid:107)U t+1 − U t(cid:107)2 + (cid:107)Bt+1 − Bt(cid:107)2 < 
2Attributes are deﬁned on color (black  orange)  texture (stripes  spots)  parts (longneck  hooves)  and other

high-level behavioral properties (slow  hibernate  domestic) of the animals

5

there is no ﬁxed training/test split  we use {30 30 30} random split for training/validation/test. We
generate the following two datasets using the provided features. 1) AWA-PCA: We compose a 300-
dimensional feature vectors by performing PCA on each of 6 types of features provided  including
SIFT  rgSIFT  SURF  HoG  LSS  and CQ to have 50 dimensions per each feature type  and concate-
nating them. 2) AWA-DeCAF: For the second dataset  we use the provided 4096-D DeCAF features
[18] obtained from the layer just before the output layer of a deep convolutional neural network.
4.2 Baselines

We compare our proposed method against multiple existing embedding-based categorization ap-
proaches  that either do not use any semantic information  or use semantic information but do not
explicitly embed semantic entities. For non-semantics baselines  we use the following: 1)Ridge
Regression: A linear regression with (cid:96)-2 norm (Eq. 1). 2) NCM: Nearest mean classiﬁer from [11] 
which uses the class mean as category embeddings (uc = xµ
c ). We use the code provided by the
authors3. 3) LME: A base large-margin embedding (Eq. 3) solved using alternating optimization.

For implicit semantic baselines  we consider two different methods. 4) LMTE: Our implementation
of the Weinberger et al. [7]  which enforces the semantic similarity between class embeddings as
distance constraints [7]  where U is regularized to preserve the pairwise class similarities from a
given taxonomy. 5-7) ALE  HLE  AHLE: Our implementation of the attribute label embedding in
Akata et al. [4]  which encodes the semantic information by representing each class with structured
labels that indicate the class’ association with superclasses and attributes. We implement variants
that use attributes (ALE)  leaf level + superclass labels (HLE)  and both (AHLE) labels.
For our models  we implement multiple variants to analyze the impact of each semantic entity and
the proposed regularization. 1) LME-MTL-S: The multitask semantic embedding model learned
with supercategories. 2) LME-MTL-A: The multitask embedding model learned with attributes. 3)
USE-No Reg.: The uniﬁed semantic embedding model learned using both attributes and supercate-
gories  without semantic regularization. 4) USE-Reg: USE with the sparse coding regularization.
As for the parameters  we use the projection dimensionality of de = 50 for all our models. 4 For
other parameters  we ﬁnd the optimal value by cross-validation on the validation set. We set µ1 = 1
that balances the main and auxiliary task equally  and search for µ2 for discriminative/generative
tradeoff  in the range of {0.01  0.1  0.2 . . .   1  10}  and set (cid:96)-2 norm regularization parameter λ = 1.
For sparsity parameter γ1  we set it to select on average several (3 or 4) attributes per class  and for
disjoint parameter γ2  we use 10γ1  without tuning for performance.

No
semantics

Implicit
semantics

Explicit
semantics
USE

Method

Ridge Regression

NCM [11]

LME

LMTE [7]
ALE [4]
HLE [4]
AHLE [4]

LME-MTL-S
LME-MTL-A
USE-No Reg.

USE-Reg.

1

19.31 ± 1.15
18.93 ± 1.71
19.87 ± 1.56
20.76 ± 1.64
15.72 ± 1.14
17.09 ± 1.09
16.65 ± 0.47
20.77 ± 1.41
20.65 ± 0.83
21.07 ± 1.53
21.64 ± 1.02

2

Flat hit @ k (%)
28.34 ± 1.53
29.75 ± 0.92
30.47 ± 1.56
30.71 ± 1.35
25.63 ± 1.44
27.52 ± 1.20
26.55 ± 0.77
32.09 ± 1.67
31.51 ± 0.72
31.59 ± 1.57
32.69 ± 0.83

5

44.17 ± 2.33
47.33 ± 1.60
48.07 ± 1.06
47.76 ± 2.25
43.42 ± 1.67
45.49 ± 0.61
43.05 ± 1.22
50.94 ± 1.21
49.40 ± 0.62
50.11 ± 1.51
52.04 ± 1.02

5

2

Hierarchical precision @ k (%)
39.39 ± 0.17
28.95 ± 0.54
43.43 ± 0.53
30.81 ± 0.53
42.63 ± 0.56
30.98 ± 0.62
31.05 ± 0.71
43.13 ± 0.29
29.26 ± 0.50
43.71 ± 0.34
30.51 ± 0.48
44.76 ± 0.20
43.41 ± 0.65
29.49 ± 0.89
45.73 ± 0.71
33.71 ± 0.94
43.47 ± 0.23
31.69 ± 0.49
33.67 ± 0.55
45.41 ± 0.43
33.37 ± 0.74
47.17 ± 0.91

Table 1: Multiclass classiﬁcation performance on AWA-PCA dataset (300-D PCA features).

4.3 Multiclass categorization

We ﬁrst evaluate the suggested multitask learning framework for categorization performance. We
report the average classiﬁcation performance and standard error over 5 random training/test splits
in Table 1 and 2  using both ﬂat hit@k  which is the accuracy for the top-k predictions made  and
hierarchical precision@k from [12]  which is a precision the given label is correct at k  at all levels.
Non-semantic baselines  ridge regression and NCM  were outperformed by our most basic LME
model. For implicit semantic baselines  ALE-variants underperformed even the ridge regression

3http://staff.science.uva.nl/˜tmensink/code.php
4Except for ALE variants where de=m  the number of semantic entities.

6

No
semantics

Implicit
semantics

Explicit
semantics
USE

Method

Ridge Regression

NCM [11]

LME

LMTE [7]
ALE [4]
HLE [4]
AHLE [4]

LME-MTL-S
LME-MTL-A
USE-No Reg.

USE-Reg.

1

38.39 ± 1.48
43.49 ± 1.23
44.76 ± 1.77
38.92 ± 1.12
36.40 ± 1.03
33.56 ± 1.64
38.01 ± 1.69
45.03 ± 1.32
45.55 ± 1.71
45.93 ± 1.76
46.42 ± 1.33

2

Flat hit @ k (%)
48.61 ± 1.29
57.45 ± 0.91
58.08 ± 2.05
49.97 ± 1.16
50.43 ± 1.92
45.93 ± 2.56
52.07 ± 1.19
57.73 ± 1.75
58.60 ± 1.76
59.37 ± 1.32
59.54 ± 0.73

5

62.12 ± 1.20
75.48 ± 0.58
75.11 ± 1.48
63.35 ± 1.38
70.25 ± 1.97
64.66 ± 1.77
71.53 ± 1.41
74.43 ± 1.26
74.67 ± 0.93
74.97 ± 1.15
76.62 ± 1.45

5

2

Hierarchical precision @ k (%)
41.73 ± 0.54
38.51 ± 0.61
50.32 ± 0.47
45.25 ± 0.52
49.87 ± 0.39
44.84 ± 0.98
38.67 ± 0.46
41.72 ± 0.45
52.46 ± 0.37
42.52 ± 1.17
56.79 ± 2.05
46.11 ± 2.65
44.43 ± 0.66
54.39 ± 0.55
46.05 ± 0.89
51.08 ± 0.36
44.23 ± 0.95
48.52 ± 0.29
51.04 ± 0.46
47.13 ± 0.62
47.39 ± 0.82
53.35 ± 0.30

Table 2: Multiclass classiﬁcation performance on AWA-DeCAF dataset (4096-D DeCAF features).

baseline with regard to the top-1 classiﬁcation accuracy 5  while they improve upon the top-2 recog-
nition accuracy and hierarchical precision. This shows that hard-encoding structures in the label
space do not necessarily improve the discrimination performance  while it helps to learn a more
semantic space. LMTE makes substantial improvement on 300-D features  but not on DeCAF fea-
tures.
Explicit embedding of semantic entities using our method improved both the top-1 accuracy and
the hierarchical precision  with USE variants achieving the best performance in both. Speciﬁcally 
adding superclass embeddings as auxiliary entities improves the hierarchical precision  while using
attributes improves the ﬂat top-k classiﬁcation accuracy. USE-Reg  especially  made substantial
improvements on ﬂat hit and hierarchical precision @ 5  which shows the proposed regularization’s
effectiveness in learning a semantic space that also discriminates well.

Category Ground-truth attributes

Supercategory + learned attributes

An animal that swims  ﬁsh  water  new world  small  ﬂippers 
furry  black  brown  tail  . . .

A musteline mammal that is quadrapedal  ﬂippers  furry 
ocean

An animal that is smelly  black  stripes  white  tail  furry 
ground  quadrapedal  new world  walks  . . .

A musteline mammal that has stripes

An animal
quadrapedal  vegetation  timid  hooves  walks  . . .

fast  horns  grazer 

is brown 

that

forest 

A deer that has spots  nestspot  longneck  yellow  hooves

An animal that has horns  brown  big  quadrapedal  new
world  vegetation  grazer  hooves  strong  ground . . .

A deer that is arctic  stripes  black

N/A
N/A

An odd-toed ungulate  that is lean and active
An animal  that has hands and bipedal

Otter

Skunk

Deer

Moose
Equine
Primate

Table 3: Semantic description generated using ground truth attributes labels and learned semantic decomposi-
tion of each categorys. For ground truth labels  we show top-10 ranked by their human-ranked relevance. For
our method  we rank the attributes by their learned weights. Incorrect attributes are colored in red.
4.3.1 Qualitative analysis

Besides learning a space that is both discriminative and generalizes well  our method’s main ad-
vantage  over existing methods  is its ability to generate compact  semantic descriptions for each
category it has learned. This is a great caveat  since in most models  including the state-of-the
art deep convolutional networks  humans cannot understand what has been learned; by generating
human-understandable explanation  our model can communicate with the human  allowing under-
standing of the rationale behind the categorization decisions  and to possibly allow feedback for
correction.
To show the effectiveness of using supercategory+attributes in the description  we report the learned
reconstruction for our model  compared against the description generated by its ground-truth at-
tributes in Table 3. The results show that our method generates compact description of each cat-
egory  focusing on its discriminative attributes. For example  our method select attributes such as
ﬂippers for otter  and stripes for skunk  instead of attributes common and nondescriminative such as
tail. Note that some attributes that are ranked less relevant by humans were selected for their dis-
criminativity  e.g.  yellow for dear and black for moose  both of which human annotators regarded

5We did extensive parameter search for the ALE variants.

7

Figure 2: Learned discriminative attributes association on the AWA-PCA dataset.
colored in red.

Incorrect attributes are

Learned decomposition
Class
A baleen whale  with plankton  ﬂip-
Humpback
pers  blue  skimmer  arctic
whale
Leopard
A big cat that is orange  claws  black
Hippopotamus An even-toed ungulate  that is gray 
bulbous  water  smelly  hands
A primate  that is mountains  strong 
stalker  black
A domestic cat  that is arctic  nestspot 
ﬁsh  bush

Chimpanzee

Persian Cat

Figure 3: Few-shot experiment result on the AWA dataset  and generated semantic decompositions.

as brown. Further  our method selects discriminative attributes for each supercategory  while there
is no provided attribute label for supercategories.
Figure 2 shows the discriminative attributes disjointly selected at each node on the class hierarchy.
We observe that coarser grained categories ﬁt to attributes that are common throughout all its chil-
dren (e.g. pads  stalker and paws for carnivore)  while the ﬁner grained categories ﬁt to attributes
that help for ﬁner-grained distinctions (e.g. orange for tiger  spots for leopard  and desert for lion).
4.4 One-shot/Few-shot learning

Our method is expected to be especially useful for few-shot learning  by generating a richer descrip-
tion than existing methods  that approximate the new input category using either trained categories
or attributes. For this experiment  we divide the 50 categories into predeﬁned 40/10 training/test
split  and compare with the knowledge transfer using AHLE. We assume that no attribute label is
provided for test set. For AHLE  and USE  we regularize the learning of W with W S learned
on training class set S by adding λ2(cid:107)W − W S(cid:107)2
2  to LME (Eq. 3). For USE-Reg we use the
reconstructive regularizer to regularize the model to generate semantic decomposition using US.
Figure 3 shows the result  and the learned semantic decomposition of each novel category. While all
methods make improvements over the no-transfer baseline  USE-Reg achieves the most improve-
ment  improving two-shot result on AWA-DeCAF from 38.93% to 49.87%  where USE comes in
second with 44.87%. Most learned reconstructions look reasonable  and ﬁt to discriminative traits
that help to discriminate between the test classes  which in this case are colors; orange for leopard 
gray for hippopotamus  blue for humpback whale  and arctic (white) for Persian cat.
5 Conclusion

We propose a uniﬁed semantic space model that learns a discriminative space for object categoriza-
tion  with the help of auxiliary semantic entities such as supercategories and attributes. The auxiliary
entities aid object categorization both indirectly  by sharing a common data embedding  and directly 
by a sparse-coding based regularizer that enforces the category to be generated by its supercategory
+ a sparse combination of attributes. Our USE model improves both the ﬂat-hit accuracy and hier-
archical precision on the AWA dataset  and also generates semantically meaningful decomposition
of categories  that provides human-interpretable rationale.

8

antelope: lean agility activegrizzly bear: cave big mountainsk. whale: meatteeth meat leanbeaver: swims pads stripesdalmatian: spots longleg hairlessPersian cat: domestic pads clawshorse: toughskin brown plainsG. shepherd: longleg gray stalkerblue whale: inactiveSiamese: inactive stalker meatteethskunk: horns slow hoovesmole: plankton tunnelstiger: group orangehippopotamus: strainteeth fish swimsleopard: spots fishmoose: inactivespider monkey: horns grazerhumpback: tailelephant: plankton tusks bushgorilla: bipedal bulbousox: bush bulbous hairlessfox: horns orange fishsheep: fish domestic padsseal: mountains small walkschimpanzee: toughskin insects handshamster: patches walks inactivesquirrel: pads stalker bipedalrhinoceros: jungle tusksrabbit: plankton bushbat: plankton flys hairlessgiraffe: yellow orange longneckwolf: arctic muscleChihuahua: weak domestic grayrat: fierce fields meatteethweasel: longneck grazerotter: tusks group coastalbuffalo: slow toughskinzebra: stripes longneck bushgiant panda: tusks plankton slowdeer: spots lean hoovesbobcat: strong yellow spotspig: weak tunnels whitelion: desert bulbous smellymouse: forest group domesticpolar bear: ocean smelly arcticcollie: domestic meatteethwalrus: tusks grazer buckteethraccoon: stripes spots fastcow: hornsc. dolphin: active domestic swimsbearplanktonlongneckstrainteethdolphinplanktonlongneckrodentplanktondomesticplanktonlongnecktoughskinequinehuntermeatteethsmallsheperdbaleenmustelnstrainteethlongneckplanktonbig cattoughskinlongneckbigdeermuscleg.apebovinesmallmeatteethpinnpdwalksgroundstalkerprocyonidlongnecktoughskinstrainteethbovidhooveshornsgrazerwhalelonglegplainsfieldsdogstrainteethtoughskinlongneckcatstrainteethtoughskinhairlessodd−toed ungulateplanktonmeatteethhunterprimateplanktonhandsbipedalruminantplanktonmeatteethhunteraquaticplanktonoceanswimscaninelongneckfelineplanktonyelloworangeeven−toedhuntercarnivorepadsstalkerpawsungulatehornshooveslongneckplacental024681015202530354045Number of training examplesAccuracy (%)AWA−PCA No transferAHLEUSEUSE−Reg.0246810203040506070Number of training examplesAccuracy (%)AWA−DeCAF No transferAHLEUSEUSE−Reg.References
[1] Christoph Lampert  Hannes Nickisch  and Stefan Harmeling. Learning to Detect Unseen Ob-
ject Classes by Between-Class Attribute Transfer. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)  2009.

[2] Ali Farhadi  Ian Endres  Derek Hoiem  and David Forsyth. Describing Objects by their At-

tributes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2009.

[3] Sung Ju Hwang  Fei Sha  and Kristen Grauman. Sharing features between objects and their
attributes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages
1761–1768  2011.

[4] Zeynep Akata  Florent Perronnin  Zaid Harchaoui  and Cordelia Schmid. Label-Embedding for
Attribute-Based Classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR)  pages 819–826  2013.

[5] Marcin Marszalek and Cordelia Schmid. Constructing category hierarchies for visual recogni-

tion. In European Conference on Computer Vision (ECCV)  2008.

[6] Gregory Grifﬁn and Pietro Perona. Learning and using taxonomies for fast visual categoriza-
tion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 1–8 
2008.

[7] Kilian Q. Weinberger and Olivier Chapelle. Large margin taxonomy embedding for document

categorization. In Neural Information Processing Systems (NIPS)  pages 1737–1744  2009.

[8] Tianshi Gao and Daphne Koller. Discriminative learning of relaxed hierarchy for large-scale
visual recognition. International Conference on Computer Vision (ICCV)  pages 2072–2079 
2011.

[9] Sung Ju Hwang  Kristen Grauman  and Fei Sha. Analogy-preserving semantic embedding for
visual object categorization. In International Conference on Machine Learning (ICML)  pages
639–647  2013.

[10] Samy Bengio  Jason Weston  and David Grangier. Label Embedding Trees for Large Multi-

Class Task. In Neural Information Processing Systems (NIPS)  2010.

[11] Thomas Mensink  Jakov Verbeek  Florent Perronnin  and Gabriela Csurka. Distance-based
IEEE Transactions on

image classiﬁcation: Generalizing to new classes at near zero cost.
Pattern Analysis and Machine Intelligence (TPAMI)  35(11)  2013.

[12] Andrea Frome  Greg Corrado  Jon Shlens  Samy Bengio  Jeffrey Dean  Marc’Aurelio Ranzato 
and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Neural Information
Processing Systems (NIPS)  2013.

[13] Alon Zweig and Daphna Weinshall. Hierarchical regularization cascade for joint learning. In

International Conference on Machine Learning (ICML)  volume 28  pages 37–45  2013.

[14] Julien Mairal  Francis Bach  Jean Ponce  Guillermo Sapiro  and Andrew Zisserman. Super-
In Neural Information Processing Systems (NIPS)  pages 1033–

vised dictionary learning.
1040  2008.

[15] Jason Weston  Samy Bengio  and Nicolas Usunier. Wsabie: Scaling up to large vocabulary
image annotation. In International Joint Conferences on Artiﬁcial Intelligence (IJCAI)  2011.
[16] Yang Zhou  Rong Jin  and Steven C. H. Hoi. Exclusive lasso for multi-task feature selection.

Journal of Machine Learning Research  9:988–995  2010.

[17] John Duchi and Yoram Singer. Efﬁcient online and batch learning using forward backward

splitting. Journal of Machine Learning Research  10  2009.

[18] Jeff Donahue  Yangqing Jia  Oriol Vinyals  Judy Hoffman  Ning Zhang  Eric Tzeng  and Trevor
Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In
International Conference on Machine Learning (ICML)  2014.

9

,Ulrike Von Luxburg
Morteza Alamgir
Sung Ju Hwang
Leonid Sigal
Megasthenis Asteris
Dimitris Papailiopoulos
Alexandros Dimakis
Hisham Husain
Richard Nock
Robert Williamson