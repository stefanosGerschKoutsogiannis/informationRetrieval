2018,Training Neural Networks Using Features Replay,Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network.
The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources.  Recently  there are several works trying to decouple and parallelize the backpropagation algorithm. However  all of them suffer from severe accuracy loss or memory explosion when the neural network is deep.  To address these challenging issues  we propose a novel parallel-objective formulation for the objective function of the neural network. After that   we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally  we apply our method to training deep convolutional neural networks  and the experimental results show that the proposed method achieves {faster} convergence  {lower} memory consumption  and {better} generalization error than compared methods.,Training Neural Networks Using Features Replay

Zhouyuan Huo1 2  Bin Gu2  Heng Huang1 2∗

1Electrical and Computer Engineering  University of Pittsburgh  2 JDDGlobal.com

zhouyuan.huo@pitt.edu  jsgubin@gmail.com

heng.huang@pitt.edu

Abstract

Training a neural network using backpropagation algorithm requires passing error
gradients sequentially through the network. The backward locking prevents us from
updating network layers in parallel and fully leveraging the computing resources.
Recently  there are several works trying to decouple and parallelize the backpropa-
gation algorithm. However  all of them suffer from severe accuracy loss or memory
explosion when the neural network is deep. To address these challenging issues 
we propose a novel parallel-objective formulation for the objective function of the
neural network. After that  we introduce features replay algorithm and prove that
it is guaranteed to converge to critical points for the non-convex problem under
certain conditions. Finally  we apply our method to training deep convolutional
neural networks  and the experimental results show that the proposed method
achieves faster convergence  lower memory consumption  and better generalization
error than compared methods.

1

Introduction

In recent years  the deep convolutional neural networks have made great breakthroughs in computer
vision [8  10  19  20  32  33]  natural language processing [15  16  31  36]  and reinforcement learning
[21  23  24  25]. The growth of the depths of the neural networks is one of the most critical factors
contributing to the success of deep learning  which has been veriﬁed both in practice [8  10] and
in theory [2  7  35]. Gradient-based methods are the major methods to train deep neural networks 
such as stochastic gradient descent (SGD) [29]  ADAGRAD [6]  RMSPROP [9] and ADAM [17].
As long as the loss functions are differentiable  we can compute the gradients of the networks using
backpropagation algorithm [30]. The backpropagation algorithm requires two passes of the neural
network  the forward pass to compute activations and the backward pass to compute gradients. As
shown in Figure 1 (BP)  error gradients are repeatedly propagated from the top (output layer) all
the way back to the bottom (input layer) in the backward pass. The sequential propagation of the
error gradients is called backward locking because all layers of the network are locked until their
dependencies have executed. According to the benchmark report in [14]  the computational time of
the backward pass is about twice of the computational time of the forward pass. When networks are
quite deep  backward locking becomes the bottleneck of making good use of computing resources 
preventing us from updating layers in parallel.
There are several works trying to break the backward locking in the backpropagation algorithm.
[4] and [34] avoid the backward locking by removing the backpropagation algorithm completely.
In [4]  the authors proposed the method of auxiliary coordinates (MAC) and simpliﬁed the nested
functions by imposing quadratic penalties. Similarly  [34] used Lagrange multipliers to enforce
equality constraints between auxiliary variables and activations. Both of the reformulated problems
do not require backpropagation algorithm at all and are easy to be parallelized. However  neither of

∗Corresponding Author.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Illustrations of the backward pass of the backpropagation algorithm (BP) [30]  decoupled
neural interface (DNI) [13] and decoupled parallel backpropagation (DDG) [11]. DNI breaks the
backward locking by synthesizing error gradients. DDG breaks the backward locking by storing stale
gradients.

them have been applied to training convolutional neural networks yet. There are also several works
breaking the dependencies between groups of layers or modules in the backpropagation algorithm.
In [13]  the authors proposed to remove the backward locking by employing the decoupled neural
interface to approximate error gradients (Figure 1 DNI). [1  27] broke the local dependencies between
successive layers and made all hidden layers receive error information from the output layer directly.
In the backward pass  we can use the synthetic gradients or the direct feedbacks to update the weights
of all modules without incurring any delay. However  these methods work poorly when the neural
networks use very deep architecture. In [11]  the authors proposed decoupled parallel backpropagation
by using stale gradients  where modules are updated with the gradients from different timestamps
(Figure 1 DDG). However  it requires large amounts of memory to store the stale gradients and suffers
from the loss of accuracy.
In this paper  we propose feature replay algorithm which is free of the above three issues: backward
locking  memory explosion and accuracy loss. The main contributions of our work are summarized
as follows:
• Firstly  we propose a novel parallel-objective formulation for the objective function of the neural
networks in Section 3. Using this new formulation  we break the backward locking by introducing
features replay algorithm  which is easy to be parallelized.
• Secondly  we provide the theoretical analysis in Section 4 and prove that the proposed method is
guaranteed to converge to critical points for the non-convex problem under certain conditions.
• Finally  we validate our method with experiments on training deep convolutional neural net-
works in Section 5. Experimental results demonstrate that the proposed method achieves faster
convergence  lower memory consumption  and better generalization error than compared methods.

2 Background
We assume there is a feedforward neural network with L layers  where w = [w1  w2  ...  wL] ∈ Rd
denotes the weights of all layers. The computation in each layer can be represented as taking an input
hl−1 and producing an activation hl = Fl(hl−1; wl) using weight wl. Given a loss function f and
target y  we can formulate the objective function of the neural network f (w) as follows:

f (hL  y)

min

w
s.t.

(1)
where h0 denotes the input data x. By using stochastic gradient descent  the weights of the network
are updated in the direction of their negative gradients of the loss function following:

hl = Fl(hl−1; wl)

for all

l ∈ {1  2  ...  L}

wt+1

l

= wt

l − γt · gt

l

l ∈ {1  2  ...  L}

(2)

for all

2

layer 1layer 2layer 3hδtActivationError gradientht0layer 4loss layer 1layer 2layer 3layer 4loss layer sht1ht2ht3ht4ht0ht1ht2ht3ht4δt−1layer 1layer 2layer 3layer 4loss ht−10ht−11ht−12ht3ht4ht0ht1ht2Forward passBackward passδlayer Network layerMethodBPDNIDDGBackward  lockingYesNoNo LSˆδtLS=||δt−ˆδt||22Figure 2: Backward pass of Features Replay Algorithm. We divide a 12-layer neural network into
four modules  where each module stores its input history and a stale error gradient from the upper
module. At each iteration  all modules compute the activations by inputting features from the history
and compute the gradients by applying the chain rule. After that  they receive the error gradients
from the upper modules for the next iteration.

:= ∂fxt (wt)

denotes the gradient of the loss function (1)
where γt denotes the stepsize and gt
l
regarding wt
l with input samples xt. The backpropagation algorithm [30] is utilized to compute the
gradients for the neural networks. At iteration t  it requires two passes over the network: in the
forward pass  the activations of all layers are computed from the bottom layer l = 1 to the top layer
l = L following: ht
l ); in the backward pass  it applies the chain rule and propagates
error gradients through the network from the top layer l = L to the bottom layer l = 1 following:

l = Fl(ht

l−1; wt

∂wt
l

∂fxt(wt)

∂wt
l

=

∂ht
l
∂wt
l

× ∂fxt(wt)

∂ht
l

and

∂fxt (wt)

∂ht

l−1

=

∂ht
l
∂ht
l−1

× ∂fxt(wt)

∂ht
l

.

(3)

According to (3)  computing gradients for the weights wl of the layer l is dependent on the error
gradient ∂fxt (wt)
from the layer l + 1  which is known as backward locking. Therefore  the backward
locking prevents all layers from updating before receiving error gradients from dependent layers.
When the networks are deep  the backward locking becomes the bottleneck in the training process.

∂ht
l

3 Features Replay

In this section  we propose a novel parallel-objective formulation for the objective function of the
neural networks. Using our new formulation  we break the backward locking in the backpropagation
algorithm by using features replay algorithm.

3.1 Problem Reformulation

As shown in Figure 2  we assume to divide an L-layer feedforward neural network into K modules
where K (cid:28) L  such that w = [wG(1)  wG(2)  ...  wG(K)] ∈ Rd and G(k) denotes the layers in the
module k. Let Lk represent the last layer of the module k  the output of this module can be written
as hLk. The error gradient variable is denoted as δt
k   which is used for the gradient computation of
the module k. We can split the problem (1) into K subproblems. The task of the module k (except
(wt)

k = K) is minimizing the least square error between the error gradient variable δt
into the module k + 1  and
which is the gradient of the loss function regarding ht
the task of the module K is minimizing the loss between the prediction ht
and the real label yt.
From this point of view  we propose a novel parallel-objective loss function at iteration t as follows:

with input ht

k and

Lk
∂ht

∂fht

LK

Lk

Lk

Lk

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)δt

K−1(cid:88)

k=1
ht
Lk

min
w δ

s.t.

(wt)

k − ∂fht
Lk
∂ht

Lk

= FG(k)(ht

Lk−1

; wtG(k))

3

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

+ f(cid:0)ht

LK

  yt(cid:1)

for all k ∈ {1  ...  K} 

(4)

layer 1layer 2layer 3δt1Module 1Forward passBackward passhActivationError gradientht−30ht−20ht−10layer 4layer 5layer 6δt2Module 2ht−23ht−13ht3˜ht1˜ht2˜ht3ht0˜ht4˜ht6˜ht5layer 7layer 8layer 9δt3Module 3ht−16ht6˜ht7˜ht9˜ht8layer 10layer 11layer 12Module 4ht9ht10ht12ht11loss δAlgorithm 1 Features Replay Algorithm
1: Initialize: weights w0 = [w0G(1)  ...  w0G(K)] ∈ Rd and stepsize sequence {γt};
2: for t = 0  1  2  . . .   T − 1 do
3:
4:
5:

Sample mini-batch (xt  yt) from the dataset and let ht
L0
for k = 1  . . .   K do

= xt;

(cid:16)

(cid:17)

Lk−1

; wtG(k)

; ← Play

6:
7:
8:
9:
10:
11:
12:
13:

14:

Store ht
Lk−1
Compute ht
Send ht

in the memory;
following ht

ht
Lk
to the module k + 1 if k < K;

= FG(k)

Lk

Lk

Compute loss f (wt) = f(cid:0)ht

end for

LK
for k = 1  . . .   K in parallel do

  yt(cid:1);

= FG(k)(ht+k−K

following ˜ht
Lk

Compute ˜ht
Lk
Compute gradient gtG(k) following (7);
Update weights: wt+1G(k) = wtG(k) − γt · gtG(k);
Send
end for

t+k−K
h
Lk−1
∂ht+k−K
Lk−1

to the module k − 1 if k > 1;

Lk−1

(wt)

∂f

15:
16: end for

; wtG(k)); ← Replay

pass

 Forward
 Backward

pass

∂fht

where ht
L0

denotes the input data xt. It is obvious that the optimal solution for the left term of the
  for all k ∈ {1  ...  K − 1}. In other words  the optimal solution of
problem (4) is δt
the module k is dependent on the output of the upper modules. Therefore  minimizing the problem
(1) with the backpropagation algorithm is equivalent to minimizing the problem (4) with the ﬁrst
K − 1 subproblems obtaining optimal solutions.

k =

Lk
∂ht

(wt)

Lk

3.2 Breaking Dependencies by Replaying Features

Features replay algorithm is introduced in Algorithm 1. In the forward pass  immediate features are
generated and passed through the network  and the module k keeps a history of its input with size
K − k + 1. To break the dependencies between modules in the backward pass  we propose to compute
the gradients of the modules using immediate features from different timestamps. Features replay
denotes that immediate feature ht+k−K
is input into the module k for the ﬁrst time in the forward
Lk−1
pass at iteration t + k − K  and it is input into the module k for the second time in the backward pass
at iteration t. If t + k − K < 0  we set ht+k−K
= 0 . Therefore  the new problem can be written as:
(wt)

Lk−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)δt

K−1(cid:88)

k=1
˜ht
Lk

k − ∂f˜ht
Lk
∂˜ht
Lk
= FG(k)(ht+k−K

Lk−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

min
w δ

s.t.

∂f˜ht

(wt)

+ f (˜ht

LK

  yt)

; wtG(k))

for all k ∈ {1  ...  K}.

(5)

Lk

Lk
∂˜ht

denotes the gradient of the loss f (wt) regarding ˜ht
Lk

where
into the
module k + 1. It is important to note that it is not necessary to get the optimal solutions for the
ﬁrst K − 1 subproblems while we do not compute the optimal solution for the last subproblem. To
avoid the tedious computation  we make a trade-off between the error of the left term in (5) and the
computational time by making:

with input ˜ht
Lk

δt
k =

∂fht+k−K

(wt−1)

Lk

∂ht+k−K

Lk

for all k ∈ {1  ...  K − 1} 

(wt−1)

∂f

t+k−K
h
Lk
∂ht+k−K

where
put ht+k−K

Lk

denotes the gradient of the loss f (wt−1) regarding ht+k−K

Lk

Lk
into the module k + 1 at the previous iteration. Assuming the algorithm has

(6)

with in-

4

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ∂f

(wt)

(wt−1)

− ∂f˜ht
Lk
∂˜ht

t+k−K
h
Lk
∂ht+k−K

converged as t → ∞  we have wt ≈ wt−1 ≈ wt+k−K such that ˜ht

and
≈ 0 for all k ∈ {1  ...  K − 1}. Therefore  (6) is a reasonable
approximation of the optimal solutions to the ﬁrst K − 1 subproblems in (5). In this way  we break
k can
the backward locking in the backpropagation algorithm because the error gradient variable δt
be determined at the previous iteration t − 1 such that all modules are independent of each other at
iteration t. Additionally  we compute the gradients inside each module following:

≈ ht+k−K

Lk

Lk

Lk

Lk

(wt)

∂fht+k−K
Lk−1
∂wt
l

=

∂˜ht
Lk
∂wt
l

× δt

k

and

(wt)

∂fht+k−K
Lk−1
∂˜ht
l

=

∂˜ht
Lk
∂˜ht
l

× δt
k 

(7)

where l ∈ G(k). At the end of each iteration  the module k sends
the computation of the next iteration.

4 Convergence Analysis

(wt)

∂f

t+k−K
h
Lk−1
∂ht+k−K
Lk−1

to module k − 1 for

In this section  we provide theoretical analysis for Algorithm 1. Analyzing the convergence of the
problem (5) directly is difﬁcult  as it involves the variables of different timestamps. Instead  we solve
this problem by building a connection between the gradients of Algorithm 1 and stochastic gradient
descent in Assumption 1  and prove that the proposed method is guaranteed to converge to critical
points for the non-convex problem (1).

Assumption 1 (Sufﬁcient direction) We assume that the expectation of the descent direction
E
in Algorithm 1 is a sufﬁcient descent direction of the loss f (wt) regarding wt. Let
∇f (wt) denote the full gradient of the loss  there exists a constant σ > 0 such that 

gtG(k)

k=1

(cid:20) K(cid:80)

(cid:21)

(cid:42)

(cid:34) K(cid:88)

(cid:35)(cid:43)

∇f (wt)  E

gtG(k)

≥ σ(cid:107)∇f (wt)(cid:107)2
2.

(8)

k=1

Sufﬁcient direction assumption guarantees that the model is moving towards the descending direction
of the loss function.

Assumption 2 Throughout this paper  we make two assumptions following [3]:
• (Lipschitz-continuous gradient) The gradient of f is Lipschitz continuous with a constant L > 0 
such that for any w1  w2 ∈ Rd  it is satisﬁed that (cid:107)∇f (w1) − ∇f (w2)(cid:107)2 ≤ L(cid:107)w1 − w2(cid:107)2.
• (Bounded variance) We assume that the second moment of the descent direction in Algorithm 1 is
upper bounded. There exists a constant M ≥ 0 such that E

≤ M.

gtG(k)

(cid:13)(cid:13)(cid:13)(cid:13) K(cid:80)

k=1

(cid:13)(cid:13)(cid:13)(cid:13)2

2

(cid:20) K(cid:80)

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

k=1

gtG(k)

gtG(k) − E

2 = E(cid:107)ξ(cid:107)2

2−(cid:107)E [ξ](cid:107)2

2   the variance of the
is guaranteed to be less than M. According to the

According to the equation regarding variance E(cid:107)ξ − E [ξ](cid:107)2
descent direction E
above assumptions  we prove the convergence rate for the proposed method under two circumstances
of γt. Firstly  we analyze the convergence for Algorithm 1 when γt is ﬁxed and prove that the
learned model will converge sub-linearly to the neighborhood of the critical points for the non-convex
problem.
Theorem 1 Assume that Assumptions 1 and 2 hold  and the ﬁxed stepsize sequence {γt} satisﬁes
γt = γ for all t ∈ {0  1  ...  T − 1}. In addition  we assume w∗ to be the optimal solution to f (w).
Then  the output of Algorithm 1 satisﬁes that:

k=1

2

(cid:13)(cid:13)(cid:13)(cid:13) K(cid:80)

T−1(cid:88)

t=0

1
T

E(cid:13)(cid:13)∇f (wt)(cid:13)(cid:13)2

2

≤ f (w0) − f (w∗)

σγT

+

γLM

2σ

.

(9)

5

Figure 3: Sufﬁcient direction constant σ for ResNet164 and ResNet101 on CIFAR-10.

Therefore  the best solution we can obtain is controlled by γLM
2σ . We also prove that Algorithm 1 can
guarantee the convergence to critical points for the non-convex problem  as long as the diminishing
stepsizes satisfy the requirements in [29] such that:

lim
T→∞

γt = ∞ and

lim
T→∞

t < ∞.
γ2

(10)

Theorem 2 Assume that Assumptions 1 and 2 hold and the diminishing stepsize sequence {γt}
satisﬁes (10). In addition  we assume w∗ to be the optimal solution to f (w). Setting ΓT =
γt 
then the output of Algorithm 1 satisﬁes that:

t=0

T−1(cid:80)

T−1(cid:88)

t=0

T−1(cid:88)

t=0

T−1(cid:88)

t=0

1
ΓT

γtE(cid:13)(cid:13)∇f (wt)(cid:13)(cid:13)2

2 ≤ f (w0) − f (w∗)

σΓT

+

LM
2σ

.

(11)

T−1(cid:80)

t=0
ΓT

γ2
t

Remark 1 Suppose ws is chosen randomly from {wt}T−1
{γt}T−1
critical points for the non-convex problem:

t=0 with probabilities proportional to
t=0 . According to Theorem 2  we can prove that Algorithm 1 guarantees convergence to

E(cid:107)∇f (ws)(cid:107)2

2 = 0 .

(12)

lim
s→∞

5 Experiments

In this section  we validate our method with experiments training deep convolutional neural networks.
Experimental results show that the proposed method achieves faster convergence  lower memory
consumption and better generalization error than compared methods.

5.1 Experimental Setting

Implementations: We implement our method in PyTorch [28]  and evaluate it with ResNet models
[8] on two image classiﬁcation benchmark datasets: CIFAR-10 and CIFAR-100 [18]. We adopt
the standard data augmentation techniques in [8  10  22] for training these two datasets: random
cropping  random horizontal ﬂipping and normalizing. We use SGD with the momentum of 0.9  and
the stepsize is initialized to 0.01. Each model is trained using batch size 128 for 300 epochs and
the stepsize is divided by a factor of 10 at 150 and 225 epochs. The weight decay constant is set to
5 × 10−4. In the experiment  a neural network with K modules is sequentially distributed across K
GPUs. All experiments are performed on a server with four Titan X GPUs.
Compared Methods: We compare the performance of four methods in the experiments  including:
• BP: we use the backpropagation algorithm [30] in PyTorch Library.
• DNI: we implement the decoupled neural interface in [13]. Following [13]  the synthetic network

6

Sufficient Direction Constant Module 1Module 2Module 3Module 4ResNet1640.85 0.90.95 1102030405060708090100110120130140150160170180190200210220230240250260270280290EpochModule 1Module 2Module 3Module 4ResNet1010.85 0.90.95 1Figure 4: Training and testing curves for ResNet-164  ResNet101 and ResNet152 on CIFAR-10. Row
1 and row 2 present the convergence of the loss function regrading epochs and computational time
respectively. Because DNI diverges for all models  we only plot the result of DNI for ResNet164.

has two hidden convolutional layers with 5 × 5 ﬁlters  padding of size 2  batch-normalization [12]
and ReLU [26]. The output layer is a convolutional layer with 5 × 5 ﬁlters and padding size of 2.
• DDG: we implement the decoupled parallel backpropagation in [11].
• FR: features replay algorithm in Algorithm 1.

5.2 Sufﬁcient Direction

We demonstrate that the proposed method satisﬁes Assumption 1 empirically. In the experiment 
we divide ResNet164 and ResNet 101 into 4 modules and visualize the variations of the sufﬁcient
direction constant σ during the training period in Figure 3. Firstly  it is obvious that the values of
σ of these modules are larger than 0 all the time. Therefore  Assumption 1 is satisﬁed such that
Algorithm 1 is guaranteed to converge to the critical points for the non-convex problem. Secondly 
we can observe that the values of σ of the lower modules are relatively small at the ﬁrst half epochs 
and become close to 1 afterwards. The variation of σ indicates the difference between the descent
direction of FR and the steepest descent direction. Small σ at early epochs can help the method
escape from saddle points and ﬁnd better local minimum; large σ at the ﬁnal epochs can prevent the
method from diverging. In the following context  we will show that our method has better generation
error than compared methods.

5.3

Performance Comparisons

To evaluate the performance of the compared methods  we utilize three criterion in the experiment
including convergence speed  memory consumption  and generalization error.
Faster Convergence: In the experiments  we evaluate the compared methods with three ResNet
models: ResNet164 with the basic building block  ResNet101 and ResNet152 with the bottleneck
building block [8]. The performances of the compared methods on CIFAR-10 are shown in Figure
4. There are several nontrivial observations as follows: Firstly  DNI cannot converge for all models.
The synthesizer network in [13] is so small that it cannot learn an accurate approximation of the
error gradient when the network is deep. Secondly  DDG cannot converge for the model ResNet152
when we set K = 4. The stale gradients can impose noise in the optimization and lead to divergence.
Thirdly  our method converges much faster than BP when we increase the number of modules. In the
experiment  the proposed method FR can achieve a speedup of up to 2 times compared to BP. We do
not consider data parallelism for BP in this section. In the supplementary material  we show that our
method also converges faster than BP with data parallelism.

7

050100150200250300Epoch 10-310-210-1100101LossCIFAR-10 (ResNet164)BP TrainDDG Train K=4FR Train K=2FR Train K=3FR Train K=4DNI Train K=4BP TestDDG Test K=4FR Test K=2FR Test K=3FR Test K=4DNI Test K=4050100150200250300Epoch 10-310-210-1100101LossCIFAR-10 (ResNet101)BP TrainDDG Train K=4FR Train K=2FR Train K=3FR Train K=4BP TestDDG Test K=4FR Test K=2FR Test K=3FR Test K=4050100150200250300Epoch 10-310-210-1100101LossCIFAR-10 (ResNet152)BP TrainDDG Train K=4FR Train K=2FR Train K=3FR Train K=4BP TestDDG Test K=4FR Test K=2FR Test K=3FR Test K=4Algorithm Backward
Locking
BP [30]
DNI [13]
DDG [11]

yes
no
no
no

FR

Memory
O(L)

(Activations)
O(L + KLs)
O(LK + K 2)
O(L + K 2)

Figure 5:
Memory consumption for
ResNet164  ResNet101 and ResNet152.
We do not report the memory consumption of
DNI because it does not converge. DDG also
diverges when K = 3  4 for ResNet152.

Table 1: Comparisons of memory consumption
of the neural network with L layers  which is
divided into K modules and L (cid:29) K. We use
O(L) to represent the memory consumption
of the activations. For DNI  each gradient syn-
thesizer has Ls layers. From the experiments 
it is reasonable to assume that Ls (cid:29) K to
make the algorithm converge. The memory
consumed by the weights is negligible com-
pared to the activations.

Lower Memory Consumption: In Figure 5  we present the memory consumption of the compared
methods for three models when we vary the number of modules K. We do not consider DNI because
it does not converge for all models. It is evident that the memory consumptions of FR and BP are
very close. On the contrary  when K = 4  the memory consumption of DDG is more than two times
of the memory consumption of BP. The observations in the experiment are also consistent with the
analysis in Table 1. For DNI  since a three-layer synthesizer network cannot converge  it is reasonable
to assume that Ls should be large if the network is very deep. We do not explore it because it is out
of the scope of this paper. We always set K very small such that K (cid:28) L and K (cid:28) Ls. FR can still
obtain a good speedup when K is very small according to the second row in Figure 4.
Better Generalization Error:
Table 2 shows the best testing er-
ror rates for the compared meth-
ods. We do not report the result
of DNI because it does not con-
verge. We can observe that FR
always obtains better testing error
rates than other two methods BP
and DDG by a large margin. We
think it is related to the variation
of the sufﬁcient descent constant
σ. Small σ at the early epochs
help FR escape saddle points and
ﬁnd better local minimum  large
σ at the ﬁnal epochs prevent FR from diverging. DDG usually performs worse than BP because
the stale gradients impose noise in the optimization  which is commonly observed in asynchronous
algorithms with stale gradients [5].

Table 2: Best testing error rates (%) of the compared methods on
CIFAR-10 and CIFAR-100 datasets. For DDG and FR  we set
K = 2 in the experiment.

FR
6.03
27.34
4.97
23.10
4.91
23.61

CIFAR [18] BP [30]
6.40
28.53
5.25
23.48
5.26
25.20

C-10
C-100
C-10
C-100
C-10
C-100

6.45
28.51
5.35
24.25
5.72
26.39

ResNet101

ResNet152

Model

ResNet164

DDG [11]

6 Conclusion

In this paper  we proposed a novel parallel-objective formulation for the objective function of the
neural network and broke the backward locking using a new features replay algorithm. Besides
the new algorithms  our theoretical contributions include analyzing the convergence property of
the proposed method and proving that our new algorithm is guaranteed to converge to critical
points for the non-convex problem under certain conditions. We conducted experiments with deep
convolutional neural networks on two image classiﬁcation datasets  and all experimental results verify
that the proposed method can achieve faster convergence  lower memory consumption  and better
generalization error than compared methods.

8

K=2K=3K=4K=2K=3K=4K=2K=3K=400.511.52Memory (MiB)104Memory ConsumptionBPDDGFRResNet164ResNet101ResNet152References
[1] David Balduzzi  Hastagiri Vanchinathan  and Joachim M Buhmann. Kickback cuts backprop’s
red-tape: Biologically plausible credit assignment in neural networks. In AAAI  pages 485–491 
2015.

[2] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine

Learning  2(1):1–127  2009.

[3] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv preprint arXiv:1606.04838  2016.

[4] Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems.

In Artiﬁcial Intelligence and Statistics  pages 10–19  2014.

[5] Jianmin Chen  Xinghao Pan  Rajat Monga  Samy Bengio  and Rafal Jozefowicz. Revisiting

distributed synchronous sgd. arXiv preprint arXiv:1604.00981  2016.

[6] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

[7] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In

Conference on Learning Theory  pages 907–940  2016.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[9] Geoffrey Hinton  Nitish Srivastava  and Kevin Swersky. Lecture 6a overview of mini–
batch gradient descent. Coursera Lecture slides https://class. coursera. org/neuralnets-2012-
001/lecture [Online  2012.

[10] Gao Huang  Zhuang Liu  Kilian Q Weinberger  and Laurens van der Maaten. Densely connected

convolutional networks. arXiv preprint arXiv:1608.06993  2016.

[11] Zhouyuan Huo  Bin Gu  Qian Yang  and Heng Huang. Decoupled parallel backpropagation

with convergence guarantee. arXiv preprint arXiv:1804.10574  2018.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[13] Max Jaderberg  Wojciech Marian Czarnecki  Simon Osindero  Oriol Vinyals  Alex Graves  and
Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint
arXiv:1608.05343  2016.

[14] Justin Johnson.

Benchmarks for popular cnn models.

jcjohnson/cnn-benchmarks  2017.

https://github.com/

[15] Nal Kalchbrenner  Edward Grefenstette  and Phil Blunsom. A convolutional neural network for

modelling sentences. arXiv preprint arXiv:1404.2188  2014.

[16] Yoon Kim. Convolutional neural networks for sentence classiﬁcation.

arXiv:1408.5882  2014.

arXiv preprint

[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[19] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

9

[20] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[21] Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971  2015.

[22] Min Lin  Qiang Chen  and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400 

2013.

[23] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap 
Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep rein-
forcement learning. In International Conference on Machine Learning  pages 1928–1937 
2016.

[24] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602  2013.

[25] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[26] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference on machine learning (ICML-10)  pages
807–814  2010.

[27] Arild Nøkland. Direct feedback alignment provides learning in deep neural networks. In

Advances in Neural Information Processing Systems  pages 1037–1045  2016.

[28] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W  2017.

[29] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[30] David E Rumelhart  Geoffrey E Hinton  Ronald J Williams  et al. Learning representations by

back-propagating errors. Cognitive modeling  5(3):1  1988.

[31] Cicero D Santos and Bianca Zadrozny. Learning character-level representations for part-of-
speech tagging. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14)  pages 1818–1826  2014.

[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[33] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov 
Dumitru Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 1–9 
2015.

[34] Gavin Taylor  Ryan Burmeister  Zheng Xu  Bharat Singh  Ankit Patel  and Tom Goldstein.
Training neural networks without gradients: A scalable admm approach. In International
Conference on Machine Learning  pages 2722–2731  2016.

[35] Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485  2016.

[36] Xiang Zhang  Junbo Zhao  and Yann LeCun. Character-level convolutional networks for text

classiﬁcation. In Advances in neural information processing systems  pages 649–657  2015.

10

,Zhouyuan Huo
Bin Gu
Heng Huang