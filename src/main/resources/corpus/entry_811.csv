2014,Biclustering Using Message Passing,Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships  and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function  separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework  our approach excels at resolving the overlaps between biclusters  which are important features of biclusters in practice. Moreover  Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations  we find that our method outperforms two of the best existing biclustering algorithms  ISA and LAS  when the planted clusters overlap. Applied to three gene expression datasets  our method finds coregulated gene clusters that have high quality in terms of cluster size and density.,Biclustering Using Message Passing

Luke O’Connor

Bioinformatics and Integrative Genomics

Harvard University

Cambridge  MA 02138

loconnor@g.harvard.edu

Soheil Feizi

Electrical Engineering and Computer Science

Massachusetts Institute of Technology

Cambridge  MA 02139
sfeizi@mit.edu

Abstract

Biclustering is the analog of clustering on a bipartite graph. Existent methods infer
biclusters through local search strategies that ﬁnd one cluster at a time; a common
technique is to update the row memberships based on the current column member-
ships  and vice versa. We propose a biclustering algorithm that maximizes a global
objective function using message passing. Our objective function closely approx-
imates a general likelihood function  separating a cluster size penalty term into
row- and column-count penalties. Because we use a global optimization frame-
work  our approach excels at resolving the overlaps between biclusters  which are
important features of biclusters in practice. Moreover  Expectation-Maximization
can be used to learn the model parameters if they are unknown. In simulations  we
ﬁnd that our method outperforms two of the best existing biclustering algorithms 
ISA and LAS  when the planted clusters overlap. Applied to three gene expres-
sion datasets  our method ﬁnds coregulated gene clusters that have high quality in
terms of cluster size and density.

1

Introduction

The term biclustering has been used to describe several distinct problems variants. In this paper 
we consider the problem of biclustering as a bipartite analogue of clustering: Given an N × M
matrix  a bicluster is a subset of rows that are heavily connected to a subset of columns. In this
framework  biclustering methods are data mining techniques allowing simultaneous clustering of
the rows and columns of a matrix. We suppose there are two possible distributions for edge weights
in the bipartite graph: a within-cluster distribution and a background distribution. Unlike in the
traditional clustering problem  in our setup  biclusters may overlap  and a node may not belong to
any cluster. We emphasize the distinction between biclustering and the bipartite analog of graph
partitioning  which might be called bipartitioning.
Biclustering has several noteworthy applications. It has been used to ﬁnd modules of coregulated
genes using microarray gene expression data [1] and to predict tumor phenotypes from their geno-
types [2]. It has been used for document classiﬁcation  clustering both documents and related words
simultaneously [3]. In all of these applications  biclusters are expected to overlap with each other 
and these overlaps themselves are often of interest (e.g.  if one wishes to explore the relationships
between document topics).
The biclustering problem is NP-hard (see Proposition 1). However  owing to its practical impor-
tance  several heuristic methods using local search strategies have been developed. A popular ap-
proach is to search for one bicluster at a time by iteratively assigning rows to a bicluster based on the
columns  and vice versa. Two algorithms based on this approach are ISA [4] and LAS [5]. Another
approach is an exhaustive search for complete bicliques used by Bimax [6]. This approach fragments
large noisy clusters into small complete ones. SAMBA [7] uses a heuristic combinatorial search for
locally optimal biclusters  motivated by an exhaustive search algorithm that is exponential in the

1

maximum degree of the nodes. FABIA [8] uses a variational approach to ﬁt a model with fuzzy bi-
clusters. For more details about existent biclustering algorithms  and performance comparisons  see
references [6]  [9] and [10]. Most of these methods have two shortcomings: ﬁrst  they apply a local
optimality criterion to each bicluster individually. Because a collection of locally optimal biclusters
might not be globally optimal  these local methods struggle to resolve overlapping clusters  which
arise frequently in many applications. Second  the lack of a well-deﬁned global objective function
precludes an analytical characterization of their expected results.
Global optimization methods have been developed for problems closely related to biclustering  in-
cluding clustering. Unlike most biclustering problem formulations  these are mostly partitioning
problems: each node is assigned to one cluster or category. Major recent progress has been made in
the development of spectral clustering methods (see references [11] and [12]) and message-passing
algorithms (see [13]  [14] and [15]). In particular  Afﬁnity Propagation [14] maximizes the sum of
similarities to one central exemplar instead of overall cluster density. Reference [16] uses variational
expectation-maximization to ﬁt the latent block model  which is a binary model in which each row
or column is assigned to a row or column cluster  and the probability of an edge is dictated by the
respective cluster memberships. Row and column clusters that are not paired to form biclusters.
In this paper  we propose a message-passing algorithm that searches for a globally optimal col-
lection of possibly overlapping biclusters. Our method maximizes a likelihood function using an
approximation that separates a cluster-size penalty term into a row-count penalty and a column-
count penalty. This decoupling enables the messages of the max-sum algorithm to be computed
efﬁciently  effectively breaking an intractable optimization into a pair of tractable ones that can be
solved in nearly linear time. When the underlying model parameters are unknown  they can be
learned using an expectation-maximization approach.
Our approach has several advantages over existing biclustering algorithms: the objective function
of our biclustering method has the ﬂexibility to handle diverse statistical models; the max-sum al-
gorithm is a more robust optimization strategy than commonly used iterative approaches; and in
particular  our global optimization technique excels at resolving overlapping biclusters. In simula-
tions  our method outperforms two of the best existing biclustering algorithms  ISA and LAS  when
the planted clusters overlap. Applied to three gene expression datasets  our method found biclusters
of high quality in terms of cluster size and density.

2 Methods

2.1 Problem statement

Let G = (V  W  E) be a weighted bipartite graph  with vertices V = (1  ...  N ) and W = (1  ...  M ) 
connected by edges with non-negative weights: E : V × W → [0 ∞). Let V1  ...  VK ⊂ V and
W1  ...  WK ⊂ W . Let (Vk  Wk) = {(i  j) : i ∈ Vk  j ∈ Wk} be a bicluster: Graph edge weights
eij are drawn independently from either a within-cluster distribution or a background distribution
depending on whether  for some k  i ∈ Vk and j ∈ Wk. In this paper  we assume that the within-
cluster and background distributions are homogenous. However  our formulation can be extended
to a general case in which the distributions are row- or column-dependent.

ij be the indicator for i ∈ Vk and j ∈ Wk. Let cij (cid:44) min(1 (cid:80)

ij) and let c (cid:44) (ck
ij).

Let ck
Deﬁnition 1 (Biclustering Problem). Let G = (V  W  E) be a bipartite graph with biclusters
(V1  W1)  ...  (VK  WK)  within-cluster distribution f1 and background distribution f0. The problem
is to ﬁnd the maximum likelihood cluster assignments (up to reordering):

k ck

(cid:88)
rs = 1 ⇒ ck

(i j)

c

ˆc = arg max

cij log

f1(eij)
f0(eij)

 

(1)

ck
ij = ck

is = ck

rj = 1 

∀i  r ∈ V ∀j  s ∈ W.

Figure 1 demonstrates the problem qualitatively for an unweighted bipartite graph. In general  the
combinatorial nature of a biclustering problem makes it computationally challenging.
Proposition 1. The clique problem can be reduced to the maximum likelihood problem of Deﬁnition
(1). Thus  the biclustering problem is NP-hard.

2

Figure 1: Biclustering is the analogue of clustering on a bipartite graph. (a) Biclustering allows
nodes to be reordered in a manner that reveals modular structures in the bipartite graph. (b) The
rows and columns of an adjacency matrix are similarly biclustered and reordered.

Proof. Proof is provided in Supplementary Note 1.

2.2 BCMP objective function

In this section  we introduce the global objective function considered in the proposed biclustering
algorithm called Biclustering using Message Passing (BCMP). This objective function approximates
the likelihood function of Deﬁnition 1. Let lij = log f1(eij )
f0(eij ) be the log-likelihood ratio score of

tuple (i  j). Thus  the likelihood function of Deﬁnition 1 can be written as(cid:80) cijlij. If there were

no consistency constraints in the Optimization (1)  an optimal maximum likelihood biclustering
solution would be to set cij = 1 for all tuples with positive lij. Our key idea is to enforce the
consistency constraints by introducing a cluster-size penalty function and shifting the log-likelihood
ratios lij to recoup this penalty. Let Nk and Mk be the number of rows and columns  respectively 
assigned to cluster k. We have 

(cid:88)

(i j)

cijlij

(i j)

(a)≈ (cid:88)
(cid:88)
(c)≈ (cid:88)

(b)
=

(i j)

(i j)

(cid:88)
(cid:88)
(cid:88)

(i j)

cij max(0  lij + δ) − δ

cij max(0  lij + δ) + δ

cij max(0  lij + δ) + δ

cij

max(0 −1 +

max(0 −1 +

(cid:88)
(cid:88)

k

k

ij) − δ
ck

ij) − δ
ck
2

(cid:88)
(cid:88)

k

k

NkMk

rkN 2

k + r−1

k M 2
k .

(i j)

(i j)

(2)
The approximation (a) holds when δ is large enough that thresholding lij at −δ has little effect on
the resulting objective function. In equation (b)  we have expressed the second term of (a) in terms
of a cluster size penalty −δNkMk  and we have added back a term corresponding to the overlap
between clusters. Because a cluster-size penalty function of the form NkMk leads to an intractable
optimization in the max-sum framework  we approximate it using a decoupling approximation (c)
where rk is a cluster shape parameter:

2NkMk ≈ rkN 2

k + r−1

(3)
when rk ≈ Mk/Nk. The cluster-shape parameter can be iteratively tuned to ﬁt the estimated biclus-
ters.
Following equation (2)  the BCMP objective function can be separated into three terms as follows:

k M 2
k  

3

(a)(b)column variablesrow variablescolumn variablesrow variablesBiclusteringBiclusteringF (c) =

ηk +

τij +

(cid:88)
(cid:88)
ij) + δ max(0 (cid:80)

i j

k

k ck

τij = (cid:96)ij min(1 (cid:80)

ηk = − δ
µk = − δ

2 rkN 2
2 r−1
k M 2
k

k

(cid:88)

k

µk 

ij − 1) ∀(i  j) ∈ V × W 

k ck

∀1 ≤ k ≤ K 
∀1 ≤ k ≤ K

(4)

(5)

Here τij  the tuple function  encourages heavier edges of the bipartite graph to be clustered. Its
second term compensates for the fact that when biclusters overlap  the cluster-size penalty functions
double-count the overlapping regions. (cid:96)ij (cid:44) max(0  lij − δ) is the shifted log-likelihood ratio for
observed edge weight eij. ηk and µk penalize the number of rows and columns of cluster k  Nk
and Mk  respectively. Note that by introducing a penalty for each nonempty cluster  the number of
clusters can be learned  and ﬁnding weak  spurious clusters can be avoided (see Supplementary Note
3.3).
Now  we analyze BCMP over the following model for a binary or unweighted bipartite graph:
Deﬁnition 2. The binary biclustering model is a generative model for N × M bipartite graph
(V  W  E) with K biclusters distributed by uniform sampling with replacement  allowing for over-
lapping clusters. Within a bicluster  edges are drawn independently with probability p  and outside
of a bicluster  they are drawn independently with probability q < p.

In the following  we assume that p  q  and K are given. We discuss the case that the model pa-
rameters are unknown in Section 2.4. The following proposition shows that optimizing the BCMP
objective function solves the problem of Deﬁnition 1 in the case of the binary model:
Proposition 2. Let (eij) be a matrix generated by the binary model described in Deﬁnition 2.
Suppose p  q and K are given. Suppose the maximum likelihood assignment of edges to biclusters 
arg max(P (data|c))  is unique up to reordering. Let rk = M(cid:48)
k be the cluster shape ratio for
the k-th maximum likelihood cluster. Then  by using these values of rk  setting (cid:96)ij = eij  for all
(i  j)  with cluster size penalty

k/N(cid:48)

= − log( 1−p
1−q )
2 log( p(1−q)
q(1−p) )

 

δ
2

we have 

arg max

c

(P (data|c)) = arg max

c

(6)

(7)

(F (c)).

It is presented in Supplementary Note

Proof. The proof follows the derivation of equation (2).
2.
Remark 1. In the special case when q = 1 − p ∈ (0  1/2)  according to equation (6)  we have
2 = 1/4. This is suggested as a reasonable initial value to choose when the true values of p and q
δ
are unknown; see Section 2.4 for a discussion of learning the model parameters.
The assumption that rk = N(cid:48)
k may seem rather strong. However  it is essential as it justiﬁes the
decoupling equation (3) that enables a linear-time algorithm. In practice  if the initial choice of rk
is close enough to the actual ratio that a cluster is detected corresponding to the real cluster  rk can
be tuned to ﬁnd the true value by iteratively updating it to ﬁt the estimated bicluster. This iterative
strategy works well in our simulations. For more details about automatically tuning the parameter
rk  see Supplementary Note 3.1.
In a more general statistical setting  log-likelihood ratios lij may be unbounded below  and the ﬁrst
step (a) of derivation (2) is an approximation; setting δ arbitrarily large will eventually lead to
instability in the message updates.

k/M(cid:48)

4

2.3 Biclustering Using Message Passing

In this section  we use the max-sum algorithm to optimize the objective function of equation (4).
For a review of the max-sum message update rules  see Supplementary Note 4. There are N M
function nodes for the functions τij  K function nodes for the functions ηk  and K function nodes
for the functions µk. There are N M K binary variables  each attached to three function nodes: ck
ij
is attached to τij  ηk  and µk (see Supplementary Figure 1). The incoming messages from these
function nodes are named tk
ij  respectively. In the following  we describe messages for
ck
ij = c1
First  we compute t1

12; other messages can be computed similarly.

ij  and mk

ij  nk

12:

(cid:88)

k(cid:54)=1

(cid:88)

(cid:88)

ck
12) + δ max(0 

k

k

12 − 1) +
ck

(8)

ck
12(mk

12 + nk

12)] + d1

(cid:88)

k(cid:54)=1

t1
12(x)

(a)
= max

c2
12 ... cK
12

[τ12(x  c2

12  . . .   cK

12) +

mk

12(ck

12) + nk

12(ck

12)]

(b)
= max

c2
12 ... cK
12

[(cid:96)12 min(1 

12(0)+nk

where d1 =(cid:80)
t1
= (cid:96)12 +(cid:80)
= (cid:96)12 − δ +(cid:80)
12 = 1  we have min(1 (cid:80)

12(1) − d1
12(0) − d1
t1
12(0) − d1
t1

(d)

(c)

k(cid:54)=1 mk

12(0) is a constant. Equality (a) comes from the deﬁnition of messages
according to equation (6) in the Supplement. Equality (b) uses the deﬁnition of τ12 of equation (5)
and the deﬁnition of the scalar message of equation (8) in the Supplement. We can further simplify
t12 as follows:

(e)
= max(0  (cid:96)12 + maxk(cid:54)=1(mk

k(cid:54)=1 max(0  δ + mk

12 + nk

12) 

k(cid:54)=1 max(0  δ + mk
12 + nk

12) = 1  and max(0 (cid:80)

12 + nk
12)) 

12) 

if ∃k  nk
otherwise .

12 − 1) = (cid:80)

12 + mk

12 + δ > 0 

(9)

k ck

If c1
equality (c). A similar argument can be made if c1
0. This leads to equality (d). If c1
the increase obtained by letting ck
This leads to equality (e).
Remark 2. Computation of t1
ij  ...  tk
mation need only be computed once.

12 = 0 and there is no k such that nk
12 + mk
12 = 1 (i.e.  (cid:96)12) with the penalty (i.e.  mk

12. These lead to
12+δ >
12 + δ > 0  we compare
12)  for the best k.
12 + nk
ij using equality (d) costs O(K)  and not O(K 2)  as the sum-

12 = 0 but there exists a k such that nk

12+mk

k(cid:54)=1 ck

k ck

ij(c1
ij(c1

ij) + n1
ij) + m1

(i j)(cid:54)=(1 2) t1
(i j)(cid:54)=(1 2) t1
12 in constant time  we perform a preliminary

ij(c1
ij(c1

ij)] 
ij)] 

(10)

(cid:40)

12 and n1

Messages m1

12 are computed as follows:

12=x [µ1(c1) +(cid:80)
12=x [η1(c1) +(cid:80)
m1
12(x) = maxc1|c1
n1
12(x) = maxc1|c1
ij : i ∈ V  j ∈ W}. To compute n1
where c1 = {c1
(cid:88)
optimization  ignoring the effect of edge (1  2):

arg max

c1

− δ
2

Let si =(cid:80)M

N 2

1 +

t1
ij(c1

ij) + m1

ij(c1

ij).

(i j)

(11)

ij + t1

j=1 max(0  m1

ij) be the sum of positive incoming messages of row i. The function
η1 penalizes the number of rows containing some nonzero c1
ij: if any message along that row is
included  there is no additional penalty for including every positive message along that row. Thus 
optimization (11) is computed by deciding which rows to include. This can be done efﬁciently
through sorting: we sort row sums s(1)  ...  s(N ) at a cost of O(N log N ). Then we proceed from
largest to smallest  including row (N + 1 − i) if the marginal penalty δ
2 (2i − 1)
N 2 can be computed
is less than s(N +1−i). After solving optimization (11)  the messages n1
in linear time  as we explain in Supplementary Note 5.
Remark 3. Computation of nk
Proposition 3 (Computational Complexity of BCMP). The computational complexity of BCMP
over a bipartite graph with N rows  M columns  and K clusters is O(K(N + log M )(M + log N )).

ij through sorting costs O(N log N ).

2 (i2 − (i − 1)2) = δ
12  ...  n1

5

Proof. For each iteration  there are N M messages tij to be computed at cost O(K) each. Before
ij)  there are K sorting steps at a cost of O(M log M )  after which each message may
computing (nk
be computed in constant time. Likewise  there are K sorting steps at a cost of O(N log N ) each
before computing (mk

ij).

We provide an empirical runtime example of the algorithm in Supplementary Figure 3.

2.4 Parameter learning using Expectation-Maximization

In the BCMP objective function described in Section 2.2  the parameters of the generative model
were used to compute the log-likelihood ratios (lij). In practice  however  these parameters may
be unknown. Expectation-Maximization (EM) can be used to estimate these parameters. The use
of EM in this setting is slightly unorthodox  as we estimate the hidden labels (cluster assignments)
in the M step instead of the E step. However  the distinction between parameters and labels is not
intrinsic in the deﬁnition of EM [17] and the true ML solution is still guaranteed to be a ﬁxed point
of the iterative process. Note that it is possible that the EM iterative procedure leads to a locally
optimal solution and therefore it is recommended to use several random re-initializations for the
method.
The EM algorithm has three steps:

• Initialization: We choose initial values for the underlying model parameters θ and compute
the log-likelihood ratios (lij) based on these values  denoting by F0 the initial objective
function.

• M step: We run BCMP to maximize the objective Fi(c). We denote the estimated cluster

assignments by by ˆci .

• E step: We compute the expected-log-likelihood function as follows:

(cid:88)

Fi+1(c) = Eθ[log P ((eij)|θ)|c = ˆci] =

Eθ[log P (eij|θ)|c = ˆci].

(12)

(i j)

Conveniently  the expected-likelihood function takes the same form as the original likelihood func-
tion  with an input matrix of expected log-likelihood ratios. These can be computed efﬁciently if
conjugate priors are available for the parameters. Therefore  BCMP can be used to maximize Fi+1.
The algorithm terminates upon failure to improve the estimated likelihood Fi( ˆci).
For a discussion of the application of EM to the binary and Gaussian models  see Supplementary
Note 6.
In the case of the binary model  we use uniform Beta distributions as conjugate priors
for p and q  and in the case of the Gaussian model  we use inverse-gamma-normal distributions as
the priors for the variances and means. Even when convenient priors are not available  EM is still
tractable as long as one can sample from the posterior distributions.

3 Evaluation results

We compared the performance of our biclustering algorithm with two methods  ISA and LAS  in
simulations and in real gene expression datasets (Supplementary Note 8). ISA was chosen because
it performed well in comparison studies [6] [9]  and LAS was chosen because it outperformed ISA
in preliminary simulations. Both ISA and LAS search for biclusters using iterative reﬁnement. ISA
assigns rows iteratively to clusters fractionally in proportion to the sum of their entries over columns.
It repeats the same for column-cluster assignments  and this process is iterated until convergence.
LAS uses a similar greedy iterative search without fractional memberships  and it masks already-
detected clusters by mean subtraction.
In our simulations  we generate simulated bipartite graphs of size 100x100. We planted (possibly
overlapping) biclusters as full blocks with two noise models:

• Bernoulli noise: we drew edges according to the binary model of Deﬁnition 2 with varying
noise level q = 1 − p.

6

Figure 2: Performance comparison of the proposed method (BCMP) with ISA and LAS  for
Bernoulli and Gaussian models  and for overlapping and non-overlapping biclusters. On the y axis
is the total number of misclassiﬁed row-column pairs. Either the noise level or the amount of overlap
is on the x axis.

• Gaussian noise: we drew edge weights within and outside of biclusters from normal distri-

butions N (1  σ2) and N (0  σ2)  respectively  for different values of σ.

For each of these cases  we ran simulations on three setups (see Figure 2):

• Non-overlapping clusters: three non-overlapping biclusters were planted in a 100 × 100
matrix with sizes 20 × 20  15 × 20  and 15 × 10. We varied the noise level.
• Overlapping clusters with ﬁxed overlap: Three overlapping biclusters with ﬁxed overlaps
were planted in a 100 × 100 matrix with sizes 20 × 20  20 × 10  and 10 × 30. We varied
the noise level.
• Overlapping clusters with variable overlap: we planted two 30 × 30 biclusters in a 100 ×
100 matrix with variable amount of overlap between them  where the amount of overlap
is deﬁned as the fraction of rows and columns shared between the two clusters. We used
Bernoulli noise level q = 1 − p = 0.15  and Gaussian noise level σ = 0.7.

The methods used have some parameters to set. Pseudocode for BCMP is presented in Supplemen-
tary Note 10. Here are the parameters that we used to run each method:

• BCMP method with underlying parameters given: We computed the input matrix of shifted
log-likelihood ratios following the discussion in Section 2.2. The number of biclusters
K was given. We initialized the cluster-shape parameters rk at 1 and updated them as
discussed in Supplementary Note 3.1. In the case of Bernoulli noise  following Proposition
2 and Remark 1  we set (cid:96)ij = eij and δ
2 = 1/4. In the case of Gaussian noise  we chose a
threshold δ to maximize the unthresholded likelihood (see Supplementary Note 3.2).

• BCMP - EM method: Instead of taking the underlying model parameters as given  we
estimated them using the procedure described in Section 2.4 and Supplementary Note 6.

7

column variablesrow variablescolumn variablesrow variablescolumn variablesrow variablesGaussian noiseBernoulli noiseoverlapping biclusters(fixed overlap)non-overlapping biclustersoverlapping biclusters(variable overlap)(a3)(a2)(b3)(b2)(b1)(a1)00.050.10.150.20.250.30.350200400600800100012001400(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLASISAnoise levelaverage number of misclassified tuplestotal number of clustered tuples is 85000.050.10.150.20.250.30.35020040060080010001200140016001800average number of misclassified tuplesnoise leveltotal number of clustered tuples is 900(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLASISA00.10.20.30.40.50.60100200300400500600average number of misclassified tuplesoverlap(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLAS00.20.40.60.8102004006008001000noise levelaverage number of misclassified tuplestotal number of clustered tuples is 850(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLASISA00.20.40.60.8102004006008001000noise levelaverage number of misclassified tuplestotal number of clustered tuples is 900(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLASISA00.10.20.30.40.50100200300400500600700800overlapaverage number of misclassified tuples(cid:37)(cid:38)(cid:48)(cid:51)(cid:3)(cid:237)(cid:3)(cid:40)(cid:48)BCMPLASWe used identical  uninformative priors on the parameters of the within-cluster and null
distributions.

• ISA method: We used the same threshold ranges for both rows and columns  attempting
to ﬁnd best-performing threshold values for each noise level. These values were mostly
around 1.5 for both noise types and for all three dataset types. We found positive biclusters 
and used 20 reinitializations. Out of these 20 runs  we selected the best-performing run.

• LAS method: There were no parameters to set. Since K was given  we selected the ﬁrst K

biclusters discovered by LAS  which marginally increased its performance.

Evaluation results of both noise models and non-overlapping and overlapping biclusters are shown
in Figure 2. In the non-overlapping case  BCMP and LAS performed similarly well  better than
ISA. Both of these methods made few or no errors up until noise levels q = 0.2 and σ = .6 in
Bernoulli and Gaussian cases  respectively. When the parameters had to be estimated using EM 
BCMP performed worse for higher levels of Gaussian noise but well otherwise. ISA outperformed
BCMP and LAS at very high levels of Bernoulli noise; at such a high noise level  however  the
results of all three algorithms are comparable to a random guess.
In the presence of overlap between biclusters  BCMP outperformed both ISA and LAS except at very
high noise levels. Whereas LAS and ISA struggled to resolve these clusters even in the absence of
noise  BCMP made few or no errors up until noise levels q = 0.2 and σ = .6 in Bernoulli and Gaus-
sian cases  respectively. Notably  the overlapping clusters were more asymmetrical  demonstrating
the robustness of the strategy of iteratively tuning rk in our method. In simulations with variable
overlaps between biclusters  for both noise models  BCMP outperformed LAS signiﬁcantly  while
the results for the ISA method were very poor (data not shown). These results demonstrate that
BCMP excels at inferring overlapping biclusters.

4 Discussion and future directions

In this paper  we have proposed a new biclustering technique called Biclustering Using Message
Passing that  unlike existent methods  infers a globally optimal collection of biclusters rather than a
collection of locally optimal ones. This distinction is especially relevant in the presence of overlap-
ping clusters  which are common in most applications. Such overlaps can be of importance if one is
interested in the relationships among biclusters. We showed through simulations that our proposed
method outperforms two popular existent methods  ISA and LAS  in both Bernoulli and Gaussian
noise models  when the planted biclusters were overlapping. We also found that BCMP performed
well when applied to gene expression datasets.
Biclustering is a problem that arises naturally in many applications. Often  a natural statistical model
for the data is available; for example  a Poisson model can be used for document classiﬁcation (see
Supplementary Note 9). Even when no such statistical model will be available  BCMP can be used
to maximize a heuristic objective function such as the modularity function [19]. This heuristic is
preferable to clustering the original adjacency matrix when the degrees of the nodes vary widely;
see Supplementary Note 7.
The same optimization strategy used in this paper for biclustering can also be applied to perform
clustering  generalizing the graph-partitioning problem by allowing nodes to be in zero or several
clusters. We believe that the ﬂexibility of our framework to ﬁt various statistical and heuristic models
will allow BCMP to be used in diverse clustering and biclustering applications.

Acknowledgments

We would like to thank Professor Manolis Kellis and Professor Muriel Médard for their advice
and support. We would like to thank the Harvard Division of Medical Sciences for supporting this
project.

8

References
[1] Cheng  Yizong  and George M. Church. "Biclustering of expression data." Ismb. Vol. 8. 2000.
[2] Dao  Phuong  et al. "Inferring cancer subnetwork markers using density-constrained bicluster-

ing." Bioinformatics 26.18 (2010): i625-i631.

[3] Bisson  Gilles  and Fawad Hussain. "Chi-sim: A new similarity measure for the co-clustering
task." Machine Learning and Applications  2008. ICMLA’08. Seventh International Conference
on. IEEE  2008.

[4] Bergmann  Sven  Jan Ihmels  and Naama Barkai. "Iterative signature algorithm for the analysis

of large-scale gene expression data." Physical review E 67.3 (2003): 031902.

[5] Shabalin  Andrey A.  et al. "Finding large average submatrices in high dimensional data." The

Annals of Applied Statistics (2009): 985-1012.

[6] Prelic  Amela  et al. "A systematic comparison and evaluation of biclustering methods for gene

expression data." Bioinformatics 22.9 (2006): 1122-1129.

[7] Tanay  Amos  Roded Sharan  and Ron Shamir. "Discovering statistically signiﬁcant biclusters

in gene expression data." Bioinformatics 18.suppl 1 (2002): S136-S144.

[8] Hochreiter  Sepp  et al. "FABIA: factor analysis for bicluster acquisition." Bioinformatics 26.12

(2010): 1520-1527.

[9] Li  Li  et al. "A comparison and evaluation of ﬁve biclustering algorithms by quantifying good-

ness of biclusters for gene expression data." BioData mining 5.1 (2012): 1-10.

[10] Eren  Kemal  et al. "A comparative analysis of biclustering algorithms for gene expression

data." Brieﬁngs in bioinformatics 14.3 (2013): 279-292.

[11] Nadakuditi  Raj Rao  and Mark EJ Newman. "Graph spectra and the detectability of commu-

nity structure in networks." Physical review letters 108.18 (2012): 188701.

[12] Krzakala  Florent  et al. "Spectral redemption in clustering sparse networks." Proceedings of

the National Academy of Sciences 110.52 (2013): 20935-20940.

[13] Decelle  Aurelien  et al. "Asymptotic analysis of the stochastic block model for modular net-

works and its algorithmic applications." Physical Review E 84.6 (2011): 066106.

[14] Frey  Brendan J.  and Delbert Dueck. "Clustering by passing messages between data points."

Science 315.5814 (2007): 972-976.

[15] Dueck  Delbert  et al. "Constructing treatment portfolios using afﬁnity propagation." Research

in Computational Molecular Biology. Springer Berlin Heidelberg  2008.

[16] Govaert  G. and Nadif  M. "Block clustering with bernoulli mixture models: Comparison of

different approaches." Computational Statistics and Data Analysis  52 (2008): 3233-3245.

[17] Dempster  Arthur P.  Nan M. Laird  and Donald B. Rubin. "Maximum likelihood from incom-
plete data via the EM algorithm." Journal of the Royal Statistical Society. Series B (Method-
ological) (1977): 1-38.

[18] Marbach  Daniel  et al. "Wisdom of crowds for robust gene network inference." Nature meth-

ods 9.8 (2012): 796-804.

[19] Newman  Mark EJ. "Modularity and community structure in networks." Proceedings of the

National Academy of Sciences 103.23 (2006): 8577-8582.

[20] Yedidia  Jonathan S.  William T. Freeman  and Yair Weiss. "Constructing free-energy approxi-
mations and generalized belief propagation algorithms." Information Theory  IEEE Transactions
on 51.7 (2005): 2282-2312.

[21] Caldas  José  and Samuel Kaski. "Bayesian biclustering with the plaid model." Machine Learn-

ing for Signal Processing  2008. MLSP 2008. IEEE Workshop on. IEEE  2008.

9

,Luke O'Connor
Soheil Feizi
Tengyao Wang
Quentin Berthet
Yaniv Plan
Aladin Virmaux
Kevin Scaman
Nathan Kallus
Angela Zhou