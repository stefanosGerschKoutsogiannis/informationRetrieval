2012,A Linear Time Active Learning Algorithm for Link Classification,We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model  showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $G = (V E)$ such that $|E|$ is at least order of $|V|^{3/2}$ by querying at most order of $|V|^{3/2}$ edge labels. More generally  we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|V| + (|V|/k)^{3/2}$ edge labels. The running time of this algorithm is at most of order $|E| + |V|\log|V|$.,A Linear Time Active Learning Algorithm

for Link Classiﬁcation∗

Nicol`o Cesa-Bianchi

Claudio Gentile

Dipartimento di Informatica

Dipartimento di Scienze Teoriche ed Applicate

Universit`a degli Studi di Milano  Italy

Universit`a dell’Insubria  Italy

Fabio Vitale

Dipartimento di Informatica

Giovanni Zappella

Dipartimento di Matematica

Universit`a degli Studi di Milano  Italy

Universit`a degli Studi di Milano  Italy

Abstract

We present very eﬃcient active learning algorithms for link classiﬁcation
in signed networks. Our algorithms are motivated by a stochastic model
in which edge labels are obtained through perturbations of a initial sign
assignment consistent with a two-clustering of the nodes. We provide a the-
oretical analysis within this model  showing that we can achieve an optimal
(to whithin a constant factor) number of mistakes on any graph G = (V  E)
such that |E| = Ω(|V |3/2) by querying O(|V |3/2) edge labels. More gen-
erally  we show an algorithm that achieves optimality to within a factor
of O(k) by querying at most order of |V | + (|V |/k)3/2 edge labels. The
running time of this algorithm is at most of order |E| + |V | log |V |.

1 Introduction
A rapidly emerging theme in the analysis of networked data is the study of signed networks.
From a mathematical point of view  signed networks are graphs whose edges carry a sign
representing the positive or negative nature of the relationship between the incident nodes.
For example  in a protein network two proteins may interact in an excitatory or inhibitory
fashion. The domain of social networks and e-commerce oﬀers several examples of signed
relationships: Slashdot users can tag other users as friends or foes  Epinions users can rate
other users positively or negatively  Ebay users develop trust and distrust towards sellers
in the network. More generally  two individuals that are related because they rate similar
products in a recommendation website may agree or disagree in their ratings.
The availability of signed networks has stimulated the design of link classiﬁcation algorithms 
especially in the domain of social networks. Early studies of signed social networks are from
the Fifties. E.g.  [8] and [1] model dislike and distrust relationships among individuals as
(signed) weighted edges in a graph. The conceptual underpinning is provided by the theory
of social balance  formulated as a way to understand the structure of conﬂicts in a network
of individuals whose mutual relationships can be classiﬁed as friendship or hostility [9]. The
advent of online social networks has revamped the interest in these theories  and spurred a
signiﬁcant amount of recent work —see  e.g.  [7  11  14  3  5  2]  and references therein.
Many heuristics for link classiﬁcation in social networks are based on a form of social balance
summarized by the motto “the enemy of my enemy is my friend”. This is equivalent to
saying that the signs on the edges of a social graph tend to be consistent with some two-
clustering of the nodes. By consistency we mean the following: The nodes of the graph can
be partitioned into two sets (the two clusters) in such a way that edges connecting nodes
∗This work was supported in part by the PASCAL2 Network of Excellence under EC grant
216886 and by “Dote Ricerca”  FSE  Regione Lombardia. This publication only reﬂects the authors’
views.

1

from the same set are positive  and edges connecting nodes from diﬀerent sets are negative.
Although two-clustering heuristics do not require strict consistency to work  this is admittely
a rather strong inductive bias. Despite that  social network theorists and practitioners
found this to be a reasonable bias in many social contexts  and recent experiments with
online social networks reported a good predictive power for algorithms based on the two-
clustering assumption [11  13  14  3]. Finally  this assumption is also fairly convenient from
the viewpoint of algorithmic design.
In the case of undirected signed graphs G = (V  E)  the best performing heuristics exploiting
the two-clustering bias are based on spectral decompositions of the signed adiacency matrix.

Noticeably  these heuristics run in time Ω(cid:0)|V |2(cid:1)  and often require a similar amount of

memory storage even on sparse networks  which makes them impractical on large graphs.
In order to obtain scalable algorithms with formal performance guarantees  we focus on the
active learning protocol  where training labels are obtained by querying a desired subset
of edges. Since the allocation of queries can match the graph topology  a wide range of
graph-theoretic techniques can be applied to the analysis of active learning algorithms. In
the recent work [2]  a simple stochastic model for generating edge labels by perturbing some
unknown two-clustering of the graph nodes was introduced. For this model  the authors
proved that querying the edges of a low-stretch spanning tree of the input graph G = (V  E)
is suﬃcient to predict the remaining edge labels making a number of mistakes within a
factor of order (log |V |)2 log log |V | from the theoretical optimum. The overall running time
is O(|E| ln|V |). This result leaves two main problems open: First  low-stretch trees are a
powerful structure  but the algorithm to construct them is not easy to implement. Second 
the tree-based analysis of [2] does not generalize to query budgets larger than |V | − 1 (the
edge set size of a spanning tree).
In this paper we introduce a diﬀerent active learning
approach for link classiﬁcation that can accomodate a large spectrum of query budgets.
We show that on any graph with Ω(|V |3/2) edges  a query budget of O(|V |3/2) is suﬃcient
to predict the remaining edge labels within a constant factor from the optimum. More in
queries is suﬃcient to
make a number of mistakes within a factor of O(k) from the optimum with a running time
of order |E| + (|V |/k) log(|V |/k). Hence  a query budget of Θ(|V |)  of the same order as the
algorithm based on low-strech trees  achieves an optimality factor O(|V |1/3) with a running
time of just O(|E|).
At the end of the paper we also report on a preliminary set of experiments on medium-sized
synthetic and real-world datasets  where a simpliﬁed algorithm suggested by our theoretical
ﬁndings is compared against the best performing spectral heuristics based on the same
inductive bias. Our algorithm seems to perform similarly or better than these heuristics.

general  we show that a budget of at most order of |V | +(cid:0)|V |

(cid:1)3/2

k

2 Preliminaries and notation

We consider undirected and connected graphs G = (V  E) with unknown edge labeling
Yi j ∈ {−1  +1} for each (i  j) ∈ E. Edge labels can collectively be represented by the
associated signed adjacency matrix Y   where Yi j = 0 whenever (i  j) (cid:54)∈ E. In the sequel 
the edge-labeled graph G will be denoted by (G  Y ).
We deﬁne a simple stochastic model for assigning binary labels Y to the edges of G. This
is used as a basis and motivation for the design of our link classiﬁcation strategies. As
we mentioned in the introduction  a good trade-oﬀ between accuracy and eﬃciency in link
classiﬁcation is achieved by assuming that the labeling is well approximated by a two-
clustering of the nodes. Hence  our stochastic labeling model assumes that edge labels are
obtained by perturbing an underlying labeling which is initially consistent with an arbitrary
(and unknown) two-clustering. More formally  given an undirected and connected graph
G = (V  E)  the labels Yi j ∈ {−1  +1}  for (i  j) ∈ E  are assigned as follows. First  the
nodes in V are arbitrarily partitioned into two sets  and labels Yi j are initially assigned
consistently with this partition (within-cluster edges are positive and between-cluster edges
are negative). Note that the consistency is equivalent to the following multiplicative rule:
For any (i  j) ∈ E  the label Yi j is equal to the product of signs on the edges of any path
connecting i to j in G. This is in turn equivalent to say that any simple cycle within the
graph contains an even number of negative edges. Then  given a nonnegative constant p < 1
2  

labels are randomly ﬂipped in such a way that P(cid:0)Yi j is ﬂipped(cid:1) ≤ p for each (i  j) ∈ E.

2

We call this a p-stochastic assignment. Note that this model allows for correlations between
ﬂipped labels.
A learning algorithm in the link classiﬁcation setting receives a training set of signed edges
and  out of this information  builds a prediction model for the labels of the remaining edges.
It is quite easy to prove a lower bound on the number of mistakes that any learning algorithm
makes in this model.
Fact 1. For any undirected graph G = (V  E)  any training set E0 ⊂ E of edges  and any
learning algorithm that is given the labels of the edges in E0  the number M of mistakes

made by A on the remaining E \ E0 edges satisﬁes E M ≥ p(cid:12)(cid:12)E \ E0

(cid:12)(cid:12)  where the expectation

is with respect to a p-stochastic assignment of the labels Y .

Proof. Let Y be the following randomized labeling: ﬁrst  edge labels are set consistently
with an arbitrary two-clustering of V . Then  a set of 2p|E| edges is selected uniformly at
random and the labels of these edges are set randomly (i.e.  ﬂipped or not ﬂipped with equal
probability). Clearly  P(Yi j is ﬂipped) = p for each (i  j) ∈ E. Hence this is a p-stochastic

assignment of the labels. Moreover  E \ E0 contains in expectation 2p(cid:12)(cid:12)E \ E0
labeled edges  on which A makes p(cid:12)(cid:12)E \ E0

(cid:12)(cid:12) mistakes in expectation.

(cid:12)(cid:12) randomly

In this paper we focus on active learning algorithms. An active learner for link classiﬁcation
ﬁrst constructs a query set E0 of edges  and then receives the labels of all edges in the query
set. Based on this training information  the learner builds a prediction model for the labels
of the remaining edges E \ E0. We assume that the only labels ever revealed to the learner
are those in the query set. In particular  no labels are revealed during the prediction phase.
It is clear from Fact 1 that any active learning algorithm that queries the labels of at most
a constant fraction of the total number of edges will make on average Ω(p|E|) mistakes.
We often write VG and EG to denote  respectively  the node set and the edge set of some
underlying graph G. For any two nodes i  j ∈ VG  Path(i  j) is any path in G having i
and j as terminals  and |Path(i  j)| is its length (number of edges). The diameter DG of a
graph G is the maximum over pairs i  j ∈ VG of the shortest path between i and j. Given
a tree T = (VT   ET ) in G  and two nodes i  j ∈ VT   we denote by dT (i  j) the distance
of i and j within T   i.e.  the length of the (unique) path PathT (i  j) connecting the two
nodes in T . Moreover  πT (i  j) denotes the parity of this path  i.e.  the product of edge
signs along it. When T is a rooted tree  we denote by ChildrenT (i) the set of children of
i in T . Finally  given two disjoint subtrees T (cid:48)  T (cid:48)(cid:48) ⊆ G such that VT (cid:48) ∩ VT (cid:48)(cid:48) ≡ ∅  we let

EG(T (cid:48)  T (cid:48)(cid:48)) ≡(cid:8)(i  j) ∈ EG : i ∈ VT (cid:48)  j ∈ VT (cid:48)(cid:48)(cid:9) .

3 Algorithms and their analysis

In this section  we introduce and analyze a family of active learning algorithms for link
classiﬁcation. The analysis is carried out under the p-stochastic assumption. As a warm
up  we start oﬀ recalling the connection to the theory of low-stretch spanning trees (e.g. 
[4])  which turns out to be useful in the important special case when the active learner is
aﬀorded to query only |V | − 1 labels.
Let Eﬂip ⊂ E denote the (random) subset of edges whose labels have been ﬂipped in a
p-stochastic assignment  and consider the following class of active learning algorithms pa-
rameterized by an arbitrary spanning tree T = (VT   ET ) of G. The algorithms in this class
use E0 = ET as query set. The label of any test edge e(cid:48) = (i  j) (cid:54)∈ ET is predicted as the
parity πT (e(cid:48)). Clearly enough  if a test edge e(cid:48) is predicted wrongly  then either e(cid:48) ∈ Eﬂip
or PathT (e(cid:48)) contains at least one ﬂipped edge. Hence  the number of mistakes MT made
by our active learner on the set of test edges E \ ET can be deterministically bounded by

(cid:88)

(cid:88)

I(cid:8)e ∈ PathT (e(cid:48))(cid:9)I(cid:8)e ∈ Eﬂip

(cid:9)

MT ≤ |Eﬂip| +

where I(cid:8)·(cid:9) denotes the indicator of the Boolean predicate at argument. A quantity which

e(cid:48)∈E\ET

e∈E

can be related to MT is the average stretch of a spanning tree T which  for our purposes 
reduces to

(1)

(cid:104)|V | − 1 +(cid:80)

1|E|

(cid:12)(cid:12)PathT (e(cid:48))(cid:12)(cid:12)(cid:105)

.

e(cid:48)∈E\ET

3

A stunning result of [4] shows that every connected  undirected and unweighted graph has

uses a spanning tree with the same low stretch  then the following result holds.
Theorem 1 ([2]). Let (G  Y ) = ((V  E)  Y ) be a labeled graph with p-stochastic assigned
If the active learner queries the edges of a spanning tree T = (VT   ET ) with
labels Y .

a spanning tree with an average stretch of just O(cid:0)log2 |V | log log |V |(cid:1). If our active learner
average stretch O(cid:0)log2 |V | log log |V |(cid:1)  then E MT ≤ p|E| × O(cid:0)log2 |V | log log |V |(cid:1).
Although low-stretch trees can be constructed in time O(cid:0)|E| ln|V |(cid:1)  the algorithms are fairly

We call the quantity multiplying p|E| in the upper bound the optimality factor of the
algorithm. Recall that Fact 1 implies that this factor cannot be smaller than a constant
when the query set size is a constant fraction of |E|.

complicated (we are not aware of available implementations)  and the constants hidden in
the asymptotics can be high. Another disadvantage is that we are forced to use a query set
of small and ﬁxed size |V |− 1. In what follows we introduce algorithms that overcome both
limitations.
A key aspect in the analysis of prediction performance is the ability to select a query set
so that each test edge creates a short circuit with a training path. This is quantiﬁed by

I(cid:8)e ∈ PathT (e(cid:48))(cid:9) in (1). We make this explicit as follows. Given a test edge (i  j)

(cid:80)

e∈E

and a path Path(i  j) whose edges are queried edges  we say that we are predicting label Yi j
using path Path(i  j) Since (i  j) closes Path(i  j) into a circuit  in this case we also say that
(i  j) is predicted using the circuit.
Fact 2. Let (G  Y ) = ((V  E)  Y ) be a labeled graph with p-stochastic assigned labels Y .
Given query set E0 ⊆ E  the number M of mistakes made when predicting test edges (i  j) ∈
E\ E0 using training paths Path(i  j) whose length is uniformly bounded by (cid:96) satisﬁes EM ≤
(cid:96) p|E \ E0| .

(cid:0)1 − (1 − p)|Path(i j)|(cid:1) ≤

(i j)∈E\E0

(cid:80)

Proof. We have the chain of

(cid:0)1 − (1 − p)(cid:96)(cid:1) ≤(cid:80)

(i j)∈E\E0

inequalities EM ≤(cid:80)

(cid:96) p ≤ (cid:96) p|E \ E0| .

(i j)∈E\E0

For instance  if the input graph G = (V  E) has diameter DG and the queried edges are
those of a breadth-ﬁrst spanning tree  which can be generated in O(|E|) time  then the
above fact holds with |E0| = |V | − 1  and (cid:96) = 2 DG. Comparing to Fact 1 shows that this
simple breadth-ﬁrst strategy is optimal up to constants factors whenever G has a constant
diameter. This simple observation is especially relevant in the light of the typical graph
topologies encountered in practice  whose diameters are often small. This argument is at
the basis of our experimental comparison —see Section 4 .
Yet  this mistake bound can be vacuous on graph having a larger diameter. Hence  one may
think of adding to the training spanning tree new edges so as to reduce the length of the
circuits used for prediction  at the cost of increasing the size of the query set. A similar
technique based on short circuits has been used in [2]  the goal there being to solve the link
classiﬁcation problem in a harder adversarial environment. The precise tradeoﬀ between
prediction accuracy (as measured by the expected number of mistakes) and fraction of
queried edges is the main theoretical concern of this paper.
We now introduce an intermediate (and simpler) algorithm  called treeCutter  which
improves on the optimality factor when the diameter DG is not small. In particular  we
demonstrate that treeCutter achieves a good upper bound on the number of mistakes

on any graph such that |E| ≥ 3|V | +(cid:112)|V |. This algorithm is especially eﬀective when the
input graph is dense  with an optimality factor between O(1) and O((cid:112)|V |). Moreover  the

total time for predicting the test edges scales linearly with the number of such edges  i.e. 
treeCutter predicts edges in constant amortized time. Also  the space is linear in the size
of the input graph.
The algorithm (pseudocode given in Figure 1) is parametrized by a positive integer k ranging
from 2 to |V |. The actual setting of k depends on the graph topology and the desired fraction
of query set edges  and plays a crucial role in determining the prediction performance.
Setting k ≤ DG makes treeCutter reduce to querying only the edges of a breadth-ﬁrst
spanning tree of G  otherwise it operates in a more involved way by splitting G into smaller
node-disjoint subtrees.

4

In a preliminary step (Line 1 in Figure 1)  treeCutter draws an arbitrary breadth-ﬁrst
spanning tree T = (VT   ET ). Then subroutine extractTreelet(T  k) is used in a do-while
loop to split T into vertex-disjoint subtrees T (cid:48) whose height is k (one of them might have a
smaller height). extractTreelet(T  k) is a very simple procedure that performs a depth-
ﬁrst visit of the tree T at argument. During this visit  each internal node may be visited
several times (during backtracking steps). We assign each node i a tag hT (i) representing
the height of the subtree of T rooted at i. hT (i) can be recursively computed during the
visit. After this assignment  if we have hT (i) = k (or i is the root of T ) we return the
subtree Ti of T rooted at i. Then treeCutter removes (Line 6) Ti from T along with
all edges of ET which are incident to nodes of Ti  and then iterates until VT gets empty.
By construction  the diameter of the generated subtrees will not be larger than 2k. Let T
denote the set of these subtrees. For each T (cid:48) ∈ T   the algorithm queries all the labels of
ET (cid:48)  each edge (i  j) ∈ EG \ ET (cid:48) such that i  j ∈ VT (cid:48) is set to be a test edge  and label Yi j is
predicted using PathT (cid:48)(i  j) (note that this coincides with PathT (cid:48)(i  j)  since T (cid:48) ⊆ T )  that
is  ˆYi j = πT (i  j). Finally  for each pair of distinct subtrees T (cid:48)  T (cid:48)(cid:48) ∈ T such that there exists
a node of VT (cid:48) adjacent to a node of VT (cid:48)(cid:48)   i.e.  such that EG(T (cid:48)  T (cid:48)(cid:48)) is not empty  we query the
label of an arbitrarily selected edge (i(cid:48)  i(cid:48)(cid:48)) ∈ EG(T (cid:48)  T (cid:48)(cid:48)) (Lines 8 and 9 in Figure 1). Each
edge (u  v) ∈ EG(T (cid:48)  T (cid:48)(cid:48)) whose label has not been previously queried is then part of the
test set  and its label will be predicted as ˆYu v ← πT (u  i(cid:48)) · Yi(cid:48) i(cid:48)(cid:48) · πT (i(cid:48)(cid:48)  v) (Line 11). That
is  using the path obtained by concatenating PathT (cid:48)(u  i(cid:48)) to edge (i(cid:48)  i(cid:48)(cid:48)) to PathT (cid:48)(i(cid:48)(cid:48)  v).
The following theorem1 quantiﬁes the number of mistakes made by treeCutter. The

Parameter: k ≥ 2

treeCutter(k)
Initialization: T ← ∅.
1. Draw an arbitrary breadth-ﬁrst spanning tree T of G
2. Do
T (cid:48) ← extractTreelet(T  k)  and query all labels in ET (cid:48)
3.
T ← T ∪ {T (cid:48)}
4.
For each i  j ∈ VT (cid:48)  set predict ˆYi j ← πT (i  j)
5.
T ← T \ T (cid:48)
6.
7. While (VT (cid:54)≡ ∅)
8. For each T (cid:48)  T (cid:48)(cid:48) ∈ T : T (cid:48) (cid:54)≡ T (cid:48)(cid:48)
9.
10.
11.

predict ˆYu v ← πT (cid:48)(u  i(cid:48)) · Yi(cid:48) i(cid:48)(cid:48) · πT (cid:48)(cid:48) (i(cid:48)(cid:48)  v)

If EG(T (cid:48)  T (cid:48)(cid:48)) (cid:54)≡ ∅ query the label of an arbitrary edge (i(cid:48)  i(cid:48)(cid:48)) ∈ EG(T (cid:48)  T (cid:48)(cid:48))
For each (u  v) ∈ EG(T (cid:48)  T (cid:48)(cid:48)) \ {(i(cid:48)  i(cid:48)(cid:48))}  with i(cid:48)  u ∈ VT (cid:48) and v  i(cid:48)(cid:48) ∈ VT (cid:48)(cid:48)

Figure 1: treeCutter pseudocode.

Parameters: tree T   k ≥ 2.

extractTreelet(T  k)
1. Perform a depth-ﬁrst visit of T starting from the root.
2. During the visit
3.
4.
5.
6.

If i is a leaf set hT (i) ← 0
Else set hT (i) ← 1 + max{hT (j) : j ∈ ChildrenT (i)}
If hT (i) = k or i ≡ T ’s root return subtree rooted at i

For each i ∈ VT visited for the |1 + ChildrenT (i)|-th time (i.e.  the last visit of i)

Figure 2: extractTreelet pseudocode.

requirement on the graph density in the statement  i.e.  |V | − 1 +
implies
that the test set is not larger than the query set. This is a plausible assumption in active
learning scenarios  and a way of adding meaning to the bounds.
Theorem 2. For any integer k ≥ 2  the number M of mistakes made by treeCutter on
|V |
|V |2
any graph G(V  E) with |E| ≥ 2|V | − 2 +
k satisﬁes EM ≤ min{4k + 1  2DG}p|E| 
k2 +
|V |2
while the query set size is bounded by |V | − 1 +
2k2 +
We now reﬁne the simple argument leading to treeCutter  and present our active link
classiﬁer. The pseudocode of our reﬁned algorithm  called starMaker  follows that of

|V |
2k ≤ |E|
2 .

|V |
2k ≤ |E|

|V |2
2k2 +

2

1 Due to space limitations long proofs are presented in the supplementary material.

5

2 ≤ |E|
2 .

Figure 1 with the following diﬀerences: Line 1 is dropped (i.e.  starMaker does not draw
an initial spanning tree)  and the call to extractTreelet in Line 3 is replaced by a call
to extractStar. This new subroutine just selects the star T (cid:48) centered on the node of G
having largest degree  and queries all labels of the edges in ET (cid:48). The next result shows that
this algorithm gets a constant optimality factor while using a query set of size O(|V |3/2).
Theorem 3. The number M of mistakes made by starMaker on any given graph G(V  E)
with |E| ≥ 2|V |− 2 + 2|V | 3
2 satisﬁes EM ≤ 5 p|E|  while the query set size is upper bounded
by |V | − 1 + |V | 3
Finally  we combine starMaker with treeCutter so as to obtain an algorithm  called
treeletStar  that can work with query sets smaller than |V |− 1 +|V | 3
2 labels. treelet-
Star is parameterized by an integer k and follows Lines 1–6 of Figure 1 creating a set
T of trees through repeated calls to extractTreelet. Lines 7–11 are instead replaced
by the following procedure: a graph G(cid:48) = (VG(cid:48)  EG(cid:48)) is created such that: (1) each node
in VG(cid:48) corresponds to a tree in T   (2) there exists an edge in EG(cid:48) if and only if the two
corresponding trees of T are connected by at least one edge of EG. Then  extractStar
is used to generate a set S of stars of vertices of G(cid:48)  i.e.  stars of trees of T . Finally  for
each pair of distinct stars S(cid:48)  S(cid:48)(cid:48) ∈ S connected by at least one edge in EG  the label of an
arbitrary edge in EG(S(cid:48)  S(cid:48)(cid:48)) is queried. The remaining edges are all predicted.
Theorem 4. For any integer k ≥ 2 and for any graph G = (V  E) with |E| ≥ 2|V | − 2 +
2   the number M of mistakes made by treeletStar(k) on G satisﬁes EM =
2 ≤ |E|
2 .
Hence  even if DG is large  setting k = |V |1/3 yields a O(|V |1/3) optimality factor just by
querying O(|V |) edges. On the other hand  a truly constant optimality factor is obtained
by querying as few as O(|V |3/2) edges (provided the graph has suﬃciently many edges). As
a direct consequence (and surprisingly enough)  on graphs which are only moderately dense
we need not observe too many edges in order to achieve a constant optimality factor. It is
instructive to compare the bounds obtained by treeletStar to the ones we can achieve
by using the cccc algorithm of [2]  or the low-stretch spanning trees given in Theorem 1.
Because cccc operates within a harder adversarial setting  it is easy to show that Theorem
9 in [2] extends to the p-stochastic assignment model by replacing ∆2(Y ) with p|E| therein.2

k + 1(cid:1) 3
2(cid:0)|V |−1
O(min{k  DG}) p|E|  while the query set size is bounded by |V | − 1 +(cid:0)|V |−1

2(cid:112)|V |  where α ∈ (0  1] is the fraction of
(cid:1) 3
The resulting optimality factor is of order(cid:0) 1−α
order to obtain an optimality factor which is lower than (cid:112)|V |  cccc has to query in the

queried edges out of the total number of edges. A quick comparison to Theorem 4 reveals
that treeletStar achieves a sharper mistake bound for any value of α. For instance  in
worst case a fraction of edges that goes to one as |V | → ∞. On top of this  our algorithms
are faster and easier to implement —see Section 3.1.
Next  we compare to query sets produced by low-stretch spanning trees. A low-stretch
spanning tree achieves a polylogarithmic optimality factor by querying |V | − 1 edge labels.
The results in [4] show that we cannot hope to get a better optimality factor using a single
low-stretch spanning tree combined by the analysis in (1). For a comparable amount Θ(|V |)
of queried labels  Theorem 4 oﬀers the larger optimality factor |V |1/3. However  we can get
a constant optimality factor by increasing the query set size to O(|V |3/2). It is not clear
how multiple low-stretch trees could be combined to get a similar scaling.

k + 1(cid:1) 3

α

3.1 Complexity analysis and implementation

We now compute bounds on time and space requirements for our three algorithms. Recall
the diﬀerent lower bound conditions on the graph density that must hold to ensure that the
|V |
query set size is not larger than the test set size. These were |E| ≥ 2|V | − 2 +
k for
treeCutter(k) in Theorem 2  |E| ≥ 2|V | − 2 + 2|V | 3
2 for starMaker in Theorem 3  and
|E| ≥ 2|V | − 2 + 2

for treeletStar(k) in Theorem 4.

(cid:16)|V |−1

|V |2
k2 +

(cid:17) 3

2

k + 1

2 This theoretical comparison is admittedly unfair  as cccc has been designed to work in a
harder setting than p-stochastic. Unfortunately  we are not aware of any other general active
learning scheme for link classiﬁcation to compare with.

6

O(cid:0)|E| + |V | log |V |(cid:1)
(cid:18)
(cid:19)

O(|E|)

O

|E| +

|V |
k

log

|V |
k

for treeCutter(k) and for all k
for starMaker

for treeletStar(k) and for all k.

Theorem 5. For any input graph G = (V  E) which is dense enough to ensure that the
query set size is no larger than the test set size  the total time needed for predicting all test
labels is:

In particular  whenever k|E| = Ω(|V | log |V |) we have that treeletStar(k) works in con-
stant amortized time. For all three algorithms  the space required is always linear in the
input graph size |E|.

4 Experiments

In this preliminary set of experiments we only tested the predictive performance of
treeCutter(|V |). This corresponds to querying only the edges of the initial spanning
tree T and predicting all remaining edges (i  j) via the parity of PathT (i  j). The spanning
tree T used by treeCutter is a shortest-path spanning tree generated by a breadth-ﬁrst
visit of the graph (assuming all edges have unit length). As the choice of the starting node
in the visit is arbitrary  we picked the highest degree node in the graph. Finally  we run
through the adiacency list of each node in random order  which we empirically observed to
improve performance.
Our baseline is the heuristic ASymExp from [11] which  among the many spectral heuristics
proposed there  turned out to perform best on all our datasets. With integer parameter
z  ASymExp(z) predicts using a spectral transformation of the training sign matrix Ytrain 
whose only non-zero entries are the signs of the training edges. The label of edge (i  j) is
z is
the spectral decomposition of Ytrain containing only the z largest eigenvalues and their corre-
sponding eigenvectors. Following [11]  we ran ASymExp(z) with the values z = 1  5  10  15.
This heuristic uses the two-clustering bias as follows : expand exp(Ytrain) in a series of
powers Y n
train)i j is a sum of values of paths of length n between i and
j. Each path has value 0 if it contains at least one test edge  otherwise its value equals the
product of queried labels on the path edges. Hence  the sign of exp(Ytrain) is the sign of a
linear combination of path values  each corresponding to a prediction consistent with the
two-clustering bias —compare this to the multiplicative rule used by treeCutter. Note
that ASymExp and the other spectral heuristics from [11] have all running times of order

predicted using(cid:0)exp(Ytrain(z))(cid:1)
train. Then each (cid:0)Y n

i j. Here exp(cid:0)Ytrain(z)(cid:1) = Uz exp(Dz)U(cid:62)

z   where UzDzU(cid:62)

Ω(cid:0)|V |2(cid:1).

We performed a ﬁrst set of experiments on synthetic signed graphs created from a subset
of the USPS digit recognition dataset. We randomly selected 500 examples labeled “1” and
500 examples labeled “7” (these two classes are not straightforward to tell apart). Then 
we created a graph using a k-NN rule with k = 100. The edges were labeled as follows:
all edges incident to nodes with the same USPS label were labeled +1; all edges incident
to nodes with diﬀerent USPS labels were labeled −1. Finally  we randomly pruned the
positive edges so to achieve an unbalance of about 20% between the two classes.3 Starting
from this edge label assignment  which is consistent with the two-clustering associated with
the USPS labels  we generated a p-stochastic label assignment by ﬂipping the labels of a
random subset of the edges. Speciﬁcally  we used the three following synthetic datasets:
DELTA0: No ﬂippings (p = 0)  1 000 nodes and 9 138 edges;
DELTA100: 100 randomly chosen labels of DELTA0 are ﬂipped;
DELTA250: 250 randomly chosen labels of DELTA0 are ﬂipped.
We also used three real-world datasets:
MOVIELENS: A signed graph we created using Movielens ratings.4 We ﬁrst normalized
the ratings by subtracting from each user rating the average rating of that user. Then 
we created a user-user matrix of cosine distance similarities. This matrix was sparsiﬁed by

3 This is similar to the class unbalance of real-world signed networks —see below.
4 www.grouplens.org/system/files/ml-1m.zip.

7

Figure 3: F-measure against training set size for treeCutter(|V |) and ASymExp(z) with diﬀerent values of z
on both synthetic and real-world datasets. By construction  treeCutter never makes a mistake when the labeling
is consistent with a two-clustering. So on DELTA0 treeCutter does not make mistakes whenever the training set
contains at least one spanning tree. With the exception of EPINIONS  treeCutter outperforms ASymExp using
a much smaller training set. We conjecture that ASymExp responds to the bias not as well as treeCutter  which
on the other hand is less robust than ASymExp to bias violations (supposedly  the labeling of EPINIONS).

zeroing each entry smaller than 0.1 and removing all self-loops. Finally  we took the sign
of each non-zero entry. The resulting graph has 6 040 nodes and 824 818 edges (12.6% of
which are negative).
SLASHDOT: The biggest strongly connected component of a snapshot of the Slashdot
social network 5 similar to the one used in [11]. This graph has 26 996 nodes and 290 509
edges (24.7% of which are negative).
EPINIONS: The biggest strongly connected component of a snapshot of the Epinions
signed network 6 similar to the one used in [13  12]. This graph has 41 441 nodes and
565 900 edges (26.2% of which are negative).
Slashdot and Epinions are originally directed graphs. We removed the reciprocal edges with
mismatching labels (which turned out to be only a few)  and considered the remaining edges
as undirected.
The following table summarizes the key statistics of each dataset: Neg. is the fraction of
negative edges  |V |/|E| is the fraction of edges queried by treeCutter(|V |)  and Avgdeg
is the average degree of the nodes of the network.

Dataset
DELTA0
DELTA100
DELTA250
SLASHDOT
EPINIONS
MOVIELENS

|V |
1000
1000
1000
26996
41441
6040

|E|
9138
9138
9138
290509
565900
824818

Neg.
21.9%
22.7%
23.5%
24.7%
26.2%
12.6%

|V |/|E|
10.9%
10.9%
10.9%
9.2%
7.3%
0.7%

Avgdeg
18.2
18.2
18.2
21.6
27.4
273.2

Our results are summarized in Figure 3  where we plot F-measure (preferable to accuracy
due to the class unbalance) against the fraction of training (or query) set size. On all
datasets  but MOVIELENS  the training set size for ASymExp ranges across the values 5% 
10%  25%  and 50%. Since MOVIELENS has a higher density  we decided to reduce those
fractions to 1%  3%  5% and 10%. treeCutter(|V |) uses a single spanning tree  and thus
we only have a single query set size value. All results are averaged over ten runs of the
algorithms. The randomness in ASymExp is due to the random draw of the training set.
The randomness in treeCutter(|V |) is caused by the randomized breadth-ﬁrst visit.

5 snap.stanford.edu/data/soc-sign-Slashdot081106.html.
6 snap.stanford.edu/data/soc-sign-epinions.html.

8

 0.4 0.6 0.8 1 10 20 30 40 50F-MEASURE (%)TRAINING SET SIZE (%)DELTA0ASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutter 0.4 0.6 0.8 1 10 20 30 40 50F-MEASURE (%)TRAINING SET SIZE (%)DELTA100ASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutter 0.4 0.6 0.8 1 10 20 30 40 50F-MEASURE (%)TRAINING SET SIZE (%)DELTA250ASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutter 0.2 0.4 0.6 1 2 3 4 5 6 7 8 9 10F-MEASURE (%)TRAINING SET SIZE (%)MOVIELENSASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutter 0.2 0.4 0.6 10 20 30 40 50F-MEASURE (%)TRAINING SET SIZE (%)SLASHDOTASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutter 0.2 0.4 0.6 0.8 10 20 30 40 50F-MEASURE (%)TRAINING SET SIZE (%)EPINIONSASymExp z=1ASymExp z=5ASymExp z=10ASymExp z=15TreeCutterReferences

[1] Cartwright  D. and Harary  F. Structure balance: A generalization of Heider’s theory.

Psychological review  63(5):277–293  1956.

[2] Cesa-Bianchi  N.  Gentile  C.  Vitale  F.  Zappella  G. A correlation clustering approach
In Proceedings of the 25th conference on

to link classiﬁcation in signed networks.
learning theory (COLT 2012). To appear  2012.

[3] Chiang  K.  Natarajan  N.  Tewari  A.  and Dhillon  I. Exploiting longer cycles for
In Proceedings of the 20th ACM Conference on

link prediction in signed networks.
Information and Knowledge Management (CIKM). ACM  2011.

[4] Elkin  M.  Emek  Y.  Spielman  D.A.  and Teng  S.-H. Lower-stretch spanning trees.

SIAM Journal on Computing  38(2):608–628  2010.

[5] Facchetti  G.  Iacono  G.  and Altaﬁni  C. Computing global structural balance in

large-scale signed social networks. PNAS  2011.

[6] Giotis  I. and Guruswami  V. Correlation clustering with a ﬁxed number of clusters. In
Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms 
pp. 1167–1176. ACM  2006.

[7] Guha  R.  Kumar  R.  Raghavan  P.  and Tomkins  A. Propagation of trust and distrust.
In Proceedings of the 13th international conference on World Wide Web  pp. 403–412.
ACM  2004.

[8] Harary  F. On the notion of balance of a signed graph. Michigan Mathematical Journal 

2(2):143–146  1953.

[9] Heider  F. Attitude and cognitive organization. J. Psychol  21:107–122  1946.

[10] Hou  Y.P. Bounds for the least Laplacian eigenvalue of a signed graph. Acta Mathe-

matica Sinica  21(4):955–960  2005.

[11] Kunegis  J.  Lommatzsch  A.  and Bauckhage  C. The Slashdot Zoo: Mining a social
network with negative edges. In Proceedings of the 18th International Conference on
World Wide Web  pp. 741–750. ACM  2009.

[12] Leskovec  J.  Huttenlocher  D.  and Kleinberg  J. Trust-aware bootstrapping of recom-
mender systems. In Proceedings of ECAI 2006 Workshop on Recommender Systems 
pp. 29–33. ECAI  2006.

[13] Leskovec  J.  Huttenlocher  D.  and Kleinberg  J. Signed networks in social media.
In Proceedings of the 28th International Conference on Human Factors in Computing
Systems  pp. 1361–1370. ACM  2010.

[14] Leskovec  J.  Huttenlocher  D.  and Kleinberg  J. Predicting positive and negative links
in online social networks. In Proceedings of the 19th International Conference on World
Wide Web  pp. 641–650. ACM  2010.

[15] Von Luxburg  U. A tutorial on spectral clustering. Statistics and Computing  17(4):

395–416  2007.

9

,Damien Scieur
Vincent Roulet
Francis Bach
Alexandre d'Aspremont
Yuxin Chen
Adish Singla
Oisin Mac Aodha
Pietro Perona
Yisong Yue