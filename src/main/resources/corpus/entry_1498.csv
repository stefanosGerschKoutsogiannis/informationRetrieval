2017,Dilated Recurrent Neural Networks,Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task.  There are three major challenges: 1) complex dependencies  2) vanishing and exploding gradients  and 3) efficient parallelization. In this paper  we introduce a simple yet effective RNN connection structure  the DilatedRNN  which simultaneously tackles all of these challenges.  The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with diverse RNN cells.  Moreover  the DilatedRNN reduces the number of parameters needed and enhances training efficiency significantly  while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies.  To provide a theory-based quantification of the architecture's advantages  we introduce a memory capacity measure  the mean recurrent length  which is more suitable for RNNs with long skip connections than existing measures.  We rigorously prove the advantages of the DilatedRNN over other recurrent neural architectures.  The code for our method is publicly available at https://github.com/code-terminator/DilatedRNN.,Dilated Recurrent Neural Networks

Shiyu Chang1⇤  Yang Zhang1⇤  Wei Han2⇤  Mo Yu1  Xiaoxiao Guo1  Wei Tan1 

Xiaodong Cui1  Michael Witbrock1  Mark Hasegawa-Johnson2  Thomas S. Huang2

{yum  wtan  cuix  witbrock}@us.ibm.com 

{weihan3  jhasegaw  t-huang1}@illinois.edu

1IBM Thomas J. Watson Research Center  Yorktown  NY 10598  USA
2University of Illinois at Urbana-Champaign  Urbana  IL 61801  USA

{shiyu.chang  yang.zhang2  xiaoxiao.guo}@ibm.com 

Abstract

Learning with recurrent neural networks (RNNs) on long sequences is a notori-
ously difﬁcult task. There are three major challenges: 1) complex dependencies  2)
vanishing and exploding gradients  and 3) efﬁcient parallelization. In this paper 
we introduce a simple yet effective RNN connection structure  the DILATEDRNN 
which simultaneously tackles all of these challenges. The proposed architecture is
characterized by multi-resolution dilated recurrent skip connections  and can be
combined ﬂexibly with diverse RNN cells. Moreover  the DILATEDRNN reduces
the number of parameters needed and enhances training efﬁciency signiﬁcantly 
while matching state-of-the-art performance (even with standard RNN cells) in
tasks involving very long-term dependencies. To provide a theory-based quantiﬁ-
cation of the architecture’s advantages  we introduce a memory capacity measure 
the mean recurrent length  which is more suitable for RNNs with long skip
connections than existing measures. We rigorously prove the advantages of the
DILATEDRNN over other recurrent neural architectures. The code for our method
is publicly available1.

1

Introduction

Recurrent neural networks (RNNs) have been shown to have remarkable performance on many
sequential learning problems. However  long sequence learning with RNNs remains a challenging
problem for the following reasons: ﬁrst  memorizing extremely long-term dependencies while
maintaining mid- and short-term memory is difﬁcult; second  training RNNs using back-propagation-
through-time is impeded by vanishing and exploding gradients; And lastly  both forward- and
back-propagation are performed in a sequential manner  which makes the training time-consuming.
Many attempts have been made to overcome these difﬁculties using specialized neural structures  cells 
and optimization techniques. Long short-term memory (LSTM) [10] and gated recurrent units (GRU)
[6] powerfully model complex data dependencies. Recent attempts have focused on multi-timescale
designs  including clockwork RNNs [12]  phased LSTM [17]  hierarchical multi-scale RNNs [5]  etc.
The problem of vanishing and exploding gradients is mitigated by LSTM and GRU memory gates;
other partial solutions include gradient clipping [18]  orthogonal and unitary weight optimization
[2  14  24]  and skip connections across multiple timestamps [8  30]. For efﬁcient sequential training 
WaveNet [22] abandoned RNN structures  proposing instead the dilated causal convolutional neural
network (CNN) architecture  which provides signiﬁcant advantages in working directly with raw
audio waveforms. However  the length of dependencies captured by a dilated CNN is limited by its
kernel size  whereas an RNN’s autoregressive modeling can  in theory  capture potentially inﬁnitely

⇤Denotes equal contribution.
1https://github.com/code-terminator/DilatedRNN

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

*+*+5

 /

 .

 3

 0

 1

 2

 -

 4

!"

*+

!#

!(

!$

!%

!&

!'

!)

*+

!%!&!'
!)

!"!#!$
!(

Figure 1: (left) A single-layer RNN with recurrent skip connections. (mid) A single-layer RNN with
dilated recurrent skip connections. (right) A computation structure equivalent to the second graph 
which reduces the sequence length by four times.

long dependencies with a small number of parameters. Recently  Yu et al. [27] proposed learning-
based RNNs with the ability to jump (skim input text) after seeing a few timestamps worth of data;
although the authors showed that the modiﬁed LSTM with jumping provides up to a six-fold speed
increase  the efﬁciency gain is mainly in the testing phase.
In this paper  we introduce the DILATEDRNN  a neural connection architecture analogous to the
dilated CNN [22  28]  but under a recurrent setting. Our approach provides a simple yet useful
solution that tries to alleviate all challenges simultaneously. The DILATEDRNN is a multi-layer  and
cell-independent architecture characterized by multi-resolution dilated recurrent skip connections.
The main contributions of this work are as follows. 1) We introduce a new dilated recurrent skip
connection as the key building block of the proposed architecture. These alleviate gradient problems
and extend the range of temporal dependencies like conventional recurrent skip connections  but in
the dilated version require fewer parameters and signiﬁcantly enhance computational efﬁciency. 2)
We stack multiple dilated recurrent layers with hierarchical dilations to construct a DILATEDRNN 
which learns temporal dependencies of different scales at different layers. 3) We present the mean
recurrent length as a new neural memory capacity measure that reveals the performance difference
between the previously developed recurrent skip-connections and the dilated version. We also verify
the optimality of the exponentially increasing dilation distribution used in the proposed architecture.
It is worth mentioning that  the recent proposed Dilated LSTM [23] can be viewed as a special case
of our model  which contains only one dilated recurrent layer with ﬁxed dilation. The main purpose
of their model is to reduce the temporal resolution on time-sensitive tasks. Thus  the Dilated LSTM
is not a general solution for modeling at multiple temporal resolutions.
We empirically validate the DILATEDRNN in multiple RNN settings on a variety of sequential
learning tasks  including long-term memorization  pixel-by-pixel classiﬁcation of handwritten digits
(with permutation and noise)  character-level language modeling  and speaker identiﬁcation with
raw audio waveforms. The DILATEDRNN improves signiﬁcantly on the performance of a regular
RNN  LSTM  or GRU with far fewer parameters. Many studies [6  14] have shown that vanilla RNN
cells perform poorly in these learning tasks. However  within the proposed structure  even vanilla
RNN cells outperform more sophisticated designs  and match the state-of-the-art. We believe that the
DILATEDRNN provides a simple and generic approach to learning on very long sequences.

2 Dilated Recurrent Neural Networks

The main ingredients of the DILATEDRNN are its dilated recurrent skip connection and its use of
exponentially increasing dilation; these will be discussed in the following two subsections respectively.

2.1 Dilated Recurrent Skip Connection

Denote c(l)

t as the cell in layer l at time t. The dilated skip connection can be represented as

This is similar to the regular skip connection[8  30]  which can be represented as

s(l) is referred to as the skip length  or dilation of layer l; x(l)
t as the input to layer l at time t; and
f (·) denotes any RNN cell and output operations  e.g. Vanilla RNN cell  LSTM  GRU etc. Both
skip connections allow information to travel along fewer edges. The difference between dilated and

c(l)

  c(l)

t = f⇣x(l)
t = f⇣x(l)

ts(l)⌘ .
ts(l)⌘ .
  c(l)
t1  c(l)

c(l)

t

t

2

(1)

(2)

Output

Hidden	Layer
Dilation	=	4

Hidden	Layer
Dilation	=	2

Hidden	Layer
Dilation	=	1

Input

Figure 2: (left) An example of a three-layer DILATEDRNN with dilation 1  2  and 4. (right) An
example of a two-layer DILATEDRNN  with dilation 2 in the ﬁrst layer. In such a case  extra
embedding connections are required (red arrows) to compensate missing data dependencies.

regular skip connection is that the dependency on c(l)
t1 is removed in dilated skip connection. The left
and middle graphs in ﬁgure 1 illustrate the differences between two architectures with dilation or skip
length s(l) = 4  where W 0r is removed in the middle graph. This reduces the number of parameters.
More importantly  computational efﬁciency of a parallel implementation (e.g.  using GPUs) can be
greatly improved by parallelizing operations that  in a regular RNN  would be impossible. The middle
and right graphs in ﬁgure 1 illustrate the idea with s(l) = 4 as an example. The input subsequences
4t } 
4t }  {x(l)
4t+3} are given four different colors. The four cell chains  {c(l)
{x(l)
4t+3}  can be computed in parallel by feeding the four subsequences into a
4t+1}  {c(l)
{c(l)
regular RNN  as shown in the right of ﬁgure 1. The output can then be obtained by interweaving
among the four output chains. The degree of parallelization is increased by s(l) times.

4t+1}  {x(l)
4t+2} and {c(l)

4t+2} and {x(l)

2.2 Exponentially Increasing Dilation
To extract complex data dependencies  we stack dilated recurrent layers to construct DILATEDRNN.
Similar to settings that were introduced in WaveNet [22]  the dilation increases exponentially across
layers. Denote s(l) as the dilation of the l-th layer. Then 

s(l) = M l1  l = 1 ···   L.

(3)
The left side of ﬁgure 2 depicts an example of DILATEDRNN with L = 3 and M = 2. On one
hand  stacking multiple dilated recurrent layers increases the model capacity. On the other hand 
exponentially increasing dilation brings two beneﬁts. First  it makes different layers focus on different
temporal resolutions. Second  it reduces the average length of paths between nodes at different
timestamps  which improves the ability of RNNs to extract long-term dependencies and prevents
vanishing and exploding gradients. A formal proof of this statement will be given in section 3.
To improve overall computational efﬁciency  a generalization of our standard DILATEDRNN is also
proposed. The dilation in the generalized DILATEDRNN does not start at one  but M l0. Formally 
(4)
where M l
0 is called the starting dilation. To compensate for the missing dependencies shorter than
M l0  a 1-by-M (l0) convolutional layer is appended as the ﬁnal layer. The right side of ﬁgure 2
illustrates an example of l0 = 1  i.e. dilations start at two. Without the red edges  there would be
no edges connecting nodes at odd and even time stamps. As discussed in section 2.1  the computa-
tional efﬁciency can be increased by M l0 by breaking the input sequence into M l0 downsampled
subsequences  and feeding each into a L  l0-layer regular DILATEDRNN with shared weights.
3 The Memory Capacity of DILATEDRNN

s(l) = M (l1+l0)  l = 1 ···   L and l0  0 

In this section  we extend the analysis framework in [30] to establish better measures of memory
capacity and parameter efﬁciency  which will be discussed in the following two sections respectively.

3.1 Memory Capacity
To facilitate theoretical analysis  we apply the cyclic graph Gc notation introduced in [30].
Deﬁnition 3.1 (Cyclic Graph). The cyclic graph representation of an RNN structure is a directed
multi-graph  GC = (VC  EC). Each edge is labeled as e = (u  v  ) 2 EC  where u is the origin

3

node  v is the destination node  and  is the number of time steps the edge travels. Each node is
labeled as v = (i  p) 2 VC  where i is the time index of the node modulo m  m is the period of the
graph  and p is the node index. GC must contain at least one directed cycle. Along the edges of any
directed cycle  the summation of  must not be zero.
Deﬁne di(n) as the length of the shortest path from any input node at time i to any output node at
time i + n. In [30]  a measure of the memory capacity is proposed that essentially only looks at
di(m)  where m is the period of the graph. This is reasonable when the period is small. However 
when the period is large  the entire distribution of di(n) 8n  m makes a difference  not just the
one at span m. As a concrete example  suppose there is an RNN architecture of period m = 10  000 
implemented using equation (2) with skip length s(l) = m  so that di(n) = n for n = 1 ···   9  999
and di(m) = 1. This network rapidly memorizes the dependence on inputs at time i of the outputs
at time i + m = i + 10  000  but shorter dependencies 2  n  9  999 are much harder to learn.
Motivated by this  we proposed the following additional measure of memory capacity.
Deﬁnition 3.2 (Mean Recurrent Length). For an RNN with cycle m  the mean recurrent length is

¯d =

1
m

mXn=1

di(n).

max
i2V

(5)

Mean recurrent length studies the average dilation across different time spans within a cycle. An
architecture with good memory capacity should generally have a small recurrent length for all
time spans. Otherwise the network can only selectively memorize information at a few time spans.
Also  we take the maximum over i  so as to punish networks that have good length only for a few
starting times  which can only well memorize information originating from those speciﬁc times. The
proposed mean recurrent length has an interesting reciprocal relation with the short-term memory
(STM) measure proposed in [11]  but mean recurrent length emphasizes more on long-term memory
capacity  which is more suitable for our intended applications.
With this  we are ready to illustrate the memory advantage of DILATEDRNN . Consider two RNN
architectures. One is the proposed DILATEDRNN structure with d layers and M = 2 (equation (1)).
The other is a regular d-layer RNN with skip connections (equation (2)). If the skip connections are
of skip s(l) = 2l1  then connections in the RNN are a strict superset of those in the DILATEDRNN  
and the RNN accomplishes exactly the same ¯d as the DILATEDRNN   but with twice the number of
trainable parameters (see section 3.2). Suppose one were to give every layer in the RNN the largest
possible skip for any graph with a period of m = 2d1: set s(l) = 2d1 in every layer  which is the
regular skip RNN setting. This apparent advantage turns out to be a disadvantage  because time spans
of 2  n < m suffer from increased path lengths  and therefore
which grows linearly with m. On the other hand  for the proposed DILATEDRNN 

(6)

(7)
where ¯d only grows logarithmically with m  which is much smaller than that of regular skip RNN.
It implies that the information in the past on average travels along much fewer edges  and thus
undergoes far less attenuation. The derivation is given in appendix A in the supplementary materials.

¯d = (m  1)/2 + log2 m + 1/m + 1 
¯d = (3m  1)/2m log2 m + 1/m + 1 

3.2 Parameter Efﬁciency
The advantage of DILATEDRNN lies not only in the memory capacity but also the number of
parameters that achieves such memory capacity. To quantify the analysis  the following measure is
introduced.
Deﬁnition 3.3 (Number of Recurrent Edges per Node). Denote Card{·} as the set cardinality. For
an RNN represented as GC = (VC  EC)  the number of recurrent edges per node  Nr  is deﬁned as
(8)

Nr = Card{e = (u  v  ) 2 EC :  6= 0} / Card{VC}.

Ideally  we would want a network that has large recurrent skips while maintaining a small number of
recurrent weights. It is easy to show that Nr for DILATEDRNN is 1 and that for RNNs with regular
skip connections is 2. The DILATEDRNN has half the recurrent complexity as the RNN with regular
skip RNN because of the removal of the direct recurrent edge. The following theorem states that the
DILATEDRNN is able to achieve the best memory capacity among a class of connection structures
with Nr = 1  and thus is among the most parameter efﬁcient RNN architectures.

4

Theorem 3.1 (Parameter Efﬁciency of DILATEDRNN). Consider a subset of d-layer RNNs with
period m = M d1 that consists purely of dilated skip connections (hence Nr = 1). For the RNNs in
this subset  there are d different dilations  1 = s1  s2 ··· sd = m  and

(9)
where ni is any arbitrary positive integer. Among this subset  the d-layer DILATEDRNN with dilation
rate {M 0 ···   M d1} achieves the smallest ¯d.
The proof is motivated by [4]  and is given in appendix B.

si = nisi1 

3.3 Comparing with Dilated CNN

Since DILATEDRNN is motivated by dilated CNN [22  28]  it is useful to compare their memory
capacities. Although cyclic graph  mean recurrent length and number of recurrent edges per node
are designed for recurrent structures  they happen to be applicable to dilated CNN as well. What’s
more  it can be easily shown that  compared to a DILATEDRNN with the same number of layers and
dilation rate of each layer  a dilated CNN has exactly the same number of recurrent edges per node 
and a slightly smaller (by log2 m) mean recurrent length. Hence both architectures have the same
model complexity  and it looks like a dilated CNN has a slightly better memory capacity.
However  mean recurrent length only measures the memory capacity within a cycle. When going
beyond a cycle  it is already shown that the recurrent length grows linearly with the number of cycles
[30] for RNN structures  including DILATEDRNN  whereas for a dilated CNN  the receptive ﬁeld
size is always ﬁnite (thus mean recurrent length goes to inﬁnity beyond the receptive ﬁeld size). For
example  with dilation rate M = 2l1 and d layers l = 1 ···   d  a dilated CNN has a receptive
ﬁeld size of 2d  which is two cycles. On the other hand  the memory of a DILATEDRNN can go far
beyond two cycles  particularly with the sophisticated units like GRU and LSTM. Hence the memory
capacity advantage of DILATEDRNN over a dilated CNN is obvious.

4 Experiments

In this section  we evaluate the performance of DILATEDRNN on four different tasks  which include
long-term memorization  pixel-by-pixel MNIST classiﬁcation [15]  character-level language modeling
on the Penn Treebank [16]  and speaker identiﬁcation with raw waveforms on VCTK [26]. We also
investigate the effect of dilation on performance and computational efﬁciency.
Unless speciﬁed otherwise  all the models are implemented with Tensorﬂow [1]. We use the default
nonlinearities and RMSProp optimizer [21] with learning rate 0.001 and decay rate of 0.9. All
weight matrices are initialized by the standard normal distribution. The batch size is set to 128.
Furthermore  in all the experiments  we apply the sequence classiﬁcation setting [25]  where the
output layer only adds at the end of the sequence. Results are reported for trained models that achieve
the best validation loss. Unless stated otherwise  no tricks  such as gradient clipping [18]  learning
rate annealing  recurrent weight dropout [20]  recurrent batch norm [20]  layer norm [3]  etc.  are
applied. All the tasks are sequence level classiﬁcation tasks  and therefore the “gridding” problem
[29] is irrelevant. No “degridded” module is needed.
Three RNN cells  Vanilla  LSTM and GRU cells  are combined with the DILATEDRNN   which
we refer to as dilated Vanilla  dilated LSTM and dilated GRU  respectively. The common baselines
include single-layer RNNs (denoted as Vanilla RNN  LSTM  and GRU)  multi-layer RNNs (denoted
as stack Vanilla  stack LSTM  and stack GRU)  and Vanilla RNN with regular skip connections
(denoted as Skip Vanilla). Additional baselines will be speciﬁed in the corresponding subsections.

4.1 Copy memory problem

This task tests the ability of recurrent models in memorizing long-term information. We follow
a similar setup in [2  24  10]. Each input sequence is of length T + 20. The ﬁrst ten values are
randomly generated from integers 0 to 7; the next T  1 values are all 8; the last 11 values are all
9  where the ﬁrst 9 signals that the model needs to start to output the ﬁrst 10 values of the inputs.
Different from the settings in [2  24]  the average cross-entropy loss is only measured at the last 10
timestamps. Therefore  the random guess yields an expected average cross entropy of ln(8) ⇡ 2.079.

5

Figure 3: Results of the copy memory problem with T = 500 (left) and T = 1000 (right). The dilated-
RNN converges quickly to the perfect solutions. Except for RNNs with dilated skip connections  all
other methods are unable to improve over random guesses.

The DILATEDRNN uses 9 layers with hidden state size of 10. The dilation starts from one to 256 at
the last hidden layer. The single-layer baselines have 256 hidden units. The multi-layer baselines use
the same number of layers and hidden state size as the DILATEDRNN . The skip Vanilla has 9 layers 
and the skip length at each layer is 256  which matches the maximum dilation of the DILATEDRNN.
The convergence curves in two settings  T = 500 and 1  000  are shown in ﬁgure 3. In both settings 
the DILATEDRNN with vanilla cells converges to a good optimum after about 1 000 training iterations 
whereas dilated LSTM and GRU converge slower. It might be because the LSTM and GRU cells are
much more complex than the vanilla unit. Except for the proposed models  all the other models are
unable to do better than the random guess  including the skip Vanilla. These results suggest that the
proposed structure as a simple renovation is very useful for problems requiring very long memory.

4.2 Pixel-by-pixel MNIST

Sequential classiﬁcation on the MNIST digits [15] is commonly used to test the performance of RNNs.
We ﬁrst implement two settings. In the ﬁrst setting  called the unpermuted setting  we follow the same
setups in [2  13  14  24  30] by serializing each image into a 784 x 1 sequence. The second setting 
called permuted setting  rearranges the input sequence with a ﬁxed permutations. Training  validation
and testing sets are the default ones in Tensorﬂow. Hyperparameters and results are reported in table
1. In addition to the baselines already described  we also implement the dilated CNN. However  the
receptive ﬁelds size of a nine-layer dilated CNN is 512  and is insufﬁcient to cover the sequence
length of 784. Therefore  we added one more layer to the dilated CNN  which enlarges its receptive
ﬁeld size to 1 024. It also forms a slight advantage of dilated CNN over the DILATEDRNN structures.
In the unpermuted setting  the dilated GRU achieves the best evaluation accuracy of 99.2. However 
the performance improvements of dilated GRU and LSTM over both the single- and multi-layer
ones are marginal  which might be because the task is too simple. Further  we observe signiﬁcant
performance differences between stack Vanilla and skip vanilla  which is consistent with the ﬁndings
in [30] that RNNs can better model long-term dependencies and achieves good results when recurrent
skip connections added. Nevertheless  the dilated vanilla has yet another signiﬁcant performance gain
over the skip Vanilla  which is consistent with our argument in section 3  that the DILATEDRNN has
a much more balanced memory over a wide range of time periods than RNNs with the regular skips.
The performance of the dilated CNN is dominated by dilated LSTM and GRU  even when the latter
have fewer parameters (in the 20 hidden units case) than the former (in the 50 hidden units case).
In the permuted setting  almost all performances are lower. However  the DILATEDRNN models
maintain very high evaluation accuracies. In particular  dilated Vanilla outperforms the previous
RNN-based state-of-the-art Zoneout [13] with a comparable number of parameters. It achieves
test accuracy of 96.1 with only 44k parameters. Note that the previous state-of-the-art utilizes the
recurrent batch normalization. The version without it has a much lower performance compared to
all the dilated models. We believe the consistently high performance of the DILATEDRNN across
different permutations is due to its hierarchical multi-resolution dilations. In addition  the dilated
CNN is able the achieve the best performance  which is in accordance with our claim in section 3.3
that dilated CNN has a slightly shorter mean recurrent length than DILATEDRNN architectures  when
sequence length fall within its receptive ﬁeld size. However  note that this is achieved by adding one
additional layer to expand its receptive ﬁeld size compared to the RNN counterparts. When the useful
information lies outside its receptive ﬁeld  the dilated CNN might fail completely.

6

Figure 4: Results of the noisy MNIST task with T = 1000 (left) and 2000 (right). RNN models
without skip connections fail. DILATEDRNN signiﬁcant outperforms regular recurrent skips and
on-pars with the dilated CNN.

Table 1: Results for unpermuted and permuted pixel-by-pixel MNIST. Italic numbers indicate the
results copied from the original paper. The best results are bold.
Max

# parameters

Method

#

Vanilla RNN
LSTM [24]
GRU
IRNN [14]
Full uRNN [24]
Skipped RNN [30]
Zoneout [13]
Dilated CNN [22]
Dilated Vanilla
Dilated LSTM
Dilated GRU

layers
1 / 9
1 / 9
1 / 9

1 / 9

1
1

1
10
9
9
9

hidden /
layer

256 / 20
256 / 20
256 / 20

100
512

95 / 20

100

20 / 50
20 / 50
20 / 50
20 / 50

(⇡  k)
68 / 7
270 / 28
200 / 21

12
270

16 / 11

42

7 / 46
7 / 44
28 / 173
21 / 130

dilations

1
1
1
1
1

1
512
256
256
256

Unpermuted
test accuracy

- / 49.1

98.2 / 98.7
99.1 / 98.8

97.0
97.5

-

98.0 / 98.3
97.7 / 98.0
98. 9 / 98.9
99.0 / 99.2

Permunted
test accuracy
71.6 / 88.5
91.7 / 89.5
94.1 / 91.3
⇡82.0
94.1
94.0 / 91.8
93.1 / 95.92
95.7 / 96.7
95.5 / 96.1
94.2 / 95.4
94.4 / 94.6

21 / 256

98.1 / 85.4

In addition to these two settings  we propose a more challenging task called the noisy MNIST  where
we pad the unpermuted pixel sequences with [0  1] uniform random noise to the length of T . The
results with two setups T = 1  000 and T = 2  000 are shown in ﬁgure 4. The dilated recurrent
models and skip RNN have 9 layers and 20 hidden units per layer. The number of skips at each layer
of skip RNN is 256. The dilated CNN has 10 layers and 11 layers for T = 1  000 and T = 2  000 
respectively. This expands the receptive ﬁeld size of the dilated CNN to the entire input sequence.
The number of ﬁlters per layer is 20. It is worth mentioning that  in the case of T = 2  000  if
we use a 10-layer dilated CNN instead  it will only produce random guesses. This is because the
output node only sees the last 1  024 input samples which do not contain any informative data. All
the other reported models have the same hyperparameters as shown in the ﬁrst three row of table 1.
We found that none of the models without skip connections is able to learn. Although skip Vanilla
remains learning  its performance drops compared to the ﬁrst unpermuted setup. On the contrary 
the DILATEDRNN and dilated CNN models obtain almost the same performances as before. It is
also worth mentioning that in all three experiments  the DILATEDRNN models are able to achieve
comparable results with an extremely small number of parameters.

4.3 Language modeling

We further investigate the task of predicting the next character on the Penn Treebank dataset [16]. We
follow the data splitting rule with the sequence length of 100 that are commonly used in previous
studies. This corpus contains 1 million words  which is small and prone to over-ﬁtting. Therefore
model regularization methods have been shown effective on the validation and test set performances.
Unlike many existing approaches  we apply no regularization other than a dropout on the input layer.
Instead  we focus on investigating the regularization effect of the dilated structure itself. Results are
shown in table 2. Although Zoneout  LayerNorm HM-LSTM and HyperNetowrks outperform the
DILATEDRNN models  they apply batch or layer normalizations as regularization. To the best of our
knowledge  the dilated GRU with 1.27 BPC achieves the best result among models of similar sizes

2with recurrent batch norm [20].

7

Table 2: Character-level language modeling on the Penn Tree Bank dataset.

layers
1 / 5
1 / 5

hidden
/ layer
1k / 256
1k / 256

# parameters

(⇡  M)
4.25 / 1.9
3.19 / 1.42

Max

dilations

Evaluation

BPC

1.31 / 1.33
1.32 / 1.33

Method

LSTM
GRU
Recurrent BN-LSTM [7]
Recurrent dropout LSTM [20]
Zoneout [13]
LayerNorm HM-LSTM [5]
HyperNetworks [9]
Dilated Vanilla
Dilated LSTM
Dilated GRU

#

1
1
1
3

5
5
5

1k
1k
1k
512
1k
256
256
256

4.25
4.25

-

-

0.6
1.9
1.42

1
1
1
1
1
1
1
64
64
64

1.32
1.30
1.27
1.24

1.37
1.31
1.27

1 / 2

4.91 / 14.41

1.26 / 1.223

Table 3: Speaker identiﬁcation on the VCTK dataset.

Method

MFCC GRU
Raw

Fused GRU
Dilated GRU

layers
5 / 1

#

1

6 / 8

hidden
/ layer
20 / 128

256
50

# parameters

(⇡  k)
16 / 68
225

103 / 133

Min

dilations

Max

dilations

1

32 / 8
32 / 8

1

32 /8
1024

Evaluation
accuracy
0.66 / 0.77
0.45 / 0.65
0.64 / 0.74

without layer normalizations. Also  the dilated models outperform their regular counterparts  Vanilla
(didn’t converge  omitted)  LSTM and GRU  without increasing the model complexity.

4.4 Speaker identiﬁcation from raw waveform
We also perform the speaker identiﬁcation task using the corpus from VCTK [26]. Learning audio
models directly from the raw waveform poses a difﬁcult challenge for recurrent models because of
the vastly long-term dependency. Recently the CLDNN family of models [19] managed to match
or surpass the log mel-frequency features in several speech problems using waveform. However 
CLDNNs coarsen the temporal granularity by pooling the ﬁrst-layer CNN output before feeding it
into the subsequent RNN layers  so as to solve the memory challenge. Instead  the DILATEDRNN
directly works on the raw waveform without pooling  which is considered more difﬁcult.
To achieve a feasible training time  we adopt the efﬁcient generalization of the DILATEDRNN as
proposed in equation (4) with l0 = 3 and l0 = 5 . As mentioned before  if the dilations do not start
at one  the model is equivalent to multiple shared-weight networks  each working on partial inputs 
and the predictions are made by fusing the information using a 1-by-M l0 convolutional layer. Our
baseline GRU model follows the same setting with various resolutions (referred to as fused-GRU) 
with dilation starting at 8. This baseline has 8 share-weight GRU networks  and each subnetwork
works on 1/8 of the subsampled sequences. The same fusion layer is used to obtain the ﬁnal prediction.
Since most other regular baselines failed to converge  we also implemented the MFCC-based models
on the same task setting for reference. The 13-dimensional log-mel frequency features are computed
with 25ms window and 5ms shift. The inputs of MFCC models are of length 100 to match the
input duration in the waveform-based models. The MFCC feature has two natural advantages: 1)
no information loss from operating on subsequences; 2) shorter sequence length. Nevertheless  our
dilated models operating directly on the waveform still offer a competitive performance (Table 3).

4.5 Discussion
In this subsection  we ﬁrst investigate the relationship between performance and the number of
dilations. We compare the DILATEDRNN models with different numbers of layers on the noisy
MNIST T = 1  000 task. All models use vanilla RNN cells with hidden state size 20. The number of
dilations starts at one. In ﬁgure 5  we observe that the classiﬁcation accuracy and rate of convergence
increases as the models become deeper. Recall the maximum skip is exponential in the number of
layers. Thus  the deeper model has a larger maximum skip and mean recurrent length.
Second  we consider maintaining a large maximum skip with a smaller number of layers  by increasing
the dilation at the bottom layer of DILATEDRNN . First  we construct a nine-layer DILATEDRNN

3with layer normalization [3].

8

Figure 5: Results for dilated vanilla with different numbers of layers on the noisy MNIST dataset.
The performance and convergent speed increase as the number of layers increases.

Figure 6: Training time (left) and evaluation performance (right) for dilated vanilla that starts at
different numbers of dilations at the bottom layer. The maximum dilations for all models are 256.

model with vanilla RNN cells. The number of dilations starts at 1  and hidden state size is 20. This
architecture is referred to as “starts at 1” in ﬁgure 6. Then  we remove the bottom hidden layers
one-by-one to construct seven new models. The last created model has three layers  and the number
of dilations starts at 64. Figure 6 demonstrates both the wall time and evaluation accuracy for 50 000
training iterations of noisy MNIST dataset. The training time reduces by roughly 50% for every
dropped layer (for every doubling of the minimum dilation). Although the testing performance
decreases when the dilation does not start at one  the effect is marginal with s(0) = 2  and small with
4  s(0)  16. Notably  the model with dilation starting at 64 is able to train within 17 minutes by
using a single Nvidia P-100 GPU while maintaining a 93.5% test accuracy.

5 Conclusion

Our experiments with DILATEDRNN provide strong evidence that this simple multi-timescale
architectural choice can reliably improve the ability of recurrent models to learn long-term dependency
in problems from different domains. We found that the DILATEDRNN trains faster  requires less
hyperparameter tuning  and needs fewer parameters to achieve the state-of-the-arts. In complement
to the experimental results  we have provided a theoretical analysis showing the advantages of
DILATEDRNN and proved its optimality under a meaningful architectural measure of RNNs.

Acknowledgement

Authors would like to thank Tom Le Paine (paine1@illinois.edu) and Ryan Musa
(ramusa@us.ibm.com) for their insightful discussions.

9

References
[1] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467  2016.

[2] Martin Arjovsky  Amar Shah  and Yoshua Bengio. Unitary evolution recurrent neural networks.

International Conference on Machine Learning  pages 1120–1128  2016.

In

[3] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450  2016.

[4] Eduardo R Caianiello  Gaetano Scarpetta  and Giovanna Simoncelli. A systemic study of monetary systems.

International Journal Of General System  8(2):81–92  1982.

[5] Junyoung Chung  Sungjin Ahn  and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.

arXiv preprint arXiv:1609.01704  2016.

[6] Junyoung Chung  Caglar Gulcehre  KyungHyun Cho  and Yoshua Bengio. Empirical evaluation of gated

recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555  2014.

[7] Tim Cooijmans  Nicolas Ballas  César Laurent  Ça˘glar Gülçehre  and Aaron Courville. Recurrent batch

normalization. arXiv preprint arXiv:1603.09025  2016.

[8] Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In

Nips  volume 409  1995.

[9] David Ha  Andrew Dai  and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106  2016.

[10] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[11] Herbert Jaeger. Short term memory in echo state networks  volume 5. GMD-Forschungszentrum Informa-

tionstechnik  2001.

[12] Jan Koutnik  Klaus Greff  Faustino Gomez  and Juergen Schmidhuber. A clockwork rnn. arXiv preprint

arXiv:1402.3511  2014.

[13] David Krueger  Tegan Maharaj  János Kramár  Mohammad Pezeshki  Nicolas Ballas  Nan Rosemary Ke 
Anirudh Goyal  Yoshua Bengio  Hugo Larochelle  Aaron Courville  et al. Zoneout: Regularizing rnns by
randomly preserving hidden activations. arXiv preprint arXiv:1606.01305  2016.

[14] Quoc V Le  Navdeep Jaitly  and Geoffrey E Hinton. A simple way to initialize recurrent networks of

rectiﬁed linear units. arXiv preprint arXiv:1504.00941  2015.

[15] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[16] Mitchell P Marcus  Mary Ann Marcinkiewicz  and Beatrice Santorini. Building a large annotated corpus

of english: The penn treebank. Computational linguistics  19(2):313–330  1993.

[17] Daniel Neil  Michael Pfeiffer  and Shih-Chii Liu. Phased LSTM: accelerating recurrent network training

for long or event-based sequences. arXiv preprint arXiv:1610.09513  2016.

[18] Razvan Pascanu  Tomas Mikolov  and Yoshua Bengio. On the difﬁculty of training recurrent neural

networks. ICML (3)  28:1310–1318  2013.

[19] Tara N Sainath  Ron J Weiss  Andrew Senior  Kevin W Wilson  and Oriol Vinyals. Learning the speech front-
end with raw waveform cldnns. In Sixteenth Annual Conference of the International Speech Communication
Association  2015.

[20] Stanislau Semeniuta  Aliaksei Severyn  and Erhardt Barth. Recurrent dropout without memory loss. arXiv

preprint arXiv:1603.05118  2016.

[21] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of

its recent magnitude. COURSERA: Neural networks for machine learning  4(2)  2012.

[22] Aäron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex Graves  Nal
Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
CoRR abs/1609.03499  2016.

10

[23] Alexander Sasha Vezhnevets  Simon Osindero  Tom Schaul  Nicolas Heess  Max Jaderberg  David Silver 
and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv preprint
arXiv:1703.01161  2017.

[24] Scott Wisdom  Thomas Powers  John Hershey  Jonathan Le Roux  and Les Atlas. Full-capacity unitary
recurrent neural networks. In Advances in Neural Information Processing Systems  pages 4880–4888 
2016.

[25] Zhengzheng Xing  Jian Pei  and Eamonn Keogh. A brief survey on sequence classiﬁcation. ACM Sigkdd

Explorations Newsletter  12(1):40–48  2010.

[26] Junichi Yamagishi. English multi-speaker corpus for cstr voice cloning toolkit. http://homepages.inf.

ed.ac.uk/jyamagis/page3/page58/page58.html  2012.

[27] Adams W Yu  Hongrae Lee  and Quoc V Le. Learning to skim text. arXiv preprint arXiv:1704.06877 

2017.

[28] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint

arXiv:1511.07122  2015.

[29] Fisher Yu  Vladlen Koltun  and Thomas Funkhouser. Dilated residual networks. arXiv preprint

arXiv:1705.09914  2017.

[30] Saizheng Zhang  Yuhuai Wu  Tong Che  Zhouhan Lin  Roland Memisevic  Ruslan R Salakhutdinov  and
Yoshua Bengio. Architectural complexity measures of recurrent neural networks. In Advances in Neural
Information Processing Systems  pages 1822–1830  2016.

11

,Qinqing Zheng
John Lafferty
Shiyu Chang
Yang Zhang
Wei Han
Mo Yu
Xiaoxiao Guo
Wei Tan
Xiaodong Cui
Michael Witbrock
Mark Hasegawa-Johnson
Thomas Huang