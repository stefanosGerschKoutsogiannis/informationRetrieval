2019,Recurrent Kernel Networks,Substring kernels are classical tools for representing biological sequences or text. However  when large amounts of annotated data is available  models that allow end-to-end training such as neural networks are often prefered.  Links between recurrent neural networks (RNNs) and substring kernels have recently been drawn  by formally showing that RNNs with specific activation functions were points in a reproducing kernel Hilbert space (RKHS).  In this paper  we revisit this link by generalizing convolutional kernel networks---originally related to a relaxation of the mismatch kernel---to model gaps in sequences. It results in a new type of recurrent neural network which can be trained end-to-end with backpropagation  or without supervision by using kernel approximation techniques.  We experimentally show that our approach is well suited to biological sequences  where it outperforms existing methods for protein classification tasks.,Recurrent Kernel Networks

Dexiong Chen

Inria∗

dexiong.chen@inria.fr

Laurent Jacob

CNRS†

laurent.jacob@univ-lyon1.fr

Julien Mairal

Inria∗

julien.mairal@inria.fr

Abstract

Substring kernels are classical tools for representing biological sequences or text.
However  when large amounts of annotated data are available  models that allow
end-to-end training such as neural networks are often preferred. Links between
recurrent neural networks (RNNs) and substring kernels have recently been drawn 
by formally showing that RNNs with speciﬁc activation functions were points
in a reproducing kernel Hilbert space (RKHS). In this paper  we revisit this link
by generalizing convolutional kernel networks—originally related to a relaxation
of the mismatch kernel—to model gaps in sequences. It results in a new type of
recurrent neural network which can be trained end-to-end with backpropagation  or
without supervision by using kernel approximation techniques. We experimentally
show that our approach is well suited to biological sequences  where it outperforms
existing methods for protein classiﬁcation tasks.

1

Introduction

Learning from biological sequences is important for a variety of scientiﬁc ﬁelds such as evolution [8]
or human health [16]. In order to use classical statistical models  a ﬁrst step is often to map sequences
to vectors of ﬁxed size  while retaining relevant features for the considered learning task. For a long
time  such features have been extracted from sequence alignment  either against a reference or between
each others [3]. The resulting features are appropriate for sequences that are similar enough  but they
become ill-deﬁned when sequences are not suited to alignment. This includes important cases such as
microbial genomes  distant species  or human diseases  and calls for alternative representations [7].
String kernels provide generic representations for biological sequences  most of which do not require
global alignment [34]. In particular  a classical approach maps sequences to a huge-dimensional
feature space by enumerating statistics about all occuring subsequences. These subsequences may be
simple classical k-mers leading to the spectrum kernel [21]  k-mers up to mismatches [22]  or gap-
allowing subsequences [24]. Other approaches involve kernels based on a generative model [17  35] 
or based on local alignments between sequences [36] inspired by convolution kernels [11  37].
The goal of kernel design is then to encode prior knowledge in the learning process. For instance 
modeling gaps in biological sequences is important since it allows taking into account short insertion
and deletion events  a common source of genetic variation. However  even though kernel methods are
good at encoding prior knowledge  they provide ﬁxed task-independent representations. When large
amounts of data are available  approaches that optimize the data representation for the prediction task

∗Univ. Grenoble Alpes  Inria  CNRS  Grenoble INP  LJK  38000 Grenoble  France.
†Univ. Lyon  Université Lyon 1  CNRS  Laboratoire de Biométrie et Biologie Evolutive UMR 5558  69000

Lyon  France

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

are now often preferred. For instance  convolutional neural networks [19] are commonly used for DNA
sequence modeling [1  2  41]  and have been successful for natural language processing [18]. While
convolution ﬁlters learned over images are interpreted as image patches  those learned over sequences
are viewed as sequence motifs. RNNs such as long short-term memory networks (LSTMs) [14] are
also commonly used in both biological [13] and natural language processing contexts [5  26].
Motivated by the regularization mechanisms of kernel methods  which are useful when the amount
of data is small and are yet imperfect in neural networks  hybrid approaches have been developed
between the kernel and neural networks paradigms [6  27  40]. Closely related to our work  the
convolutional kernel network (CKN) model originally developed for images [25] was successfully
adapted to biological sequences in [4]. CKNs for sequences consist in a continuous relaxation of the
mismatch kernel: while the latter represents a sequence by its content in k-mers up to a few discrete
errors  the former considers a continuous relaxation  leading to an inﬁnite-dimensional sequence
representation. Finally  a kernel approximation relying on the Nyström method [38] projects the
mapped sequences to a linear subspace of the RKHS  spanned by a ﬁnite number of motifs. When
these motifs are learned end-to-end with backpropagation  learning with CKNs can also be thought
of as performing feature selection in the—inﬁnite dimensional—RKHS.
In this paper  we generalize CKNs for sequences by allowing gaps in motifs  motivated by genomics
applications. The kernel map retains the convolutional structure of CKNs but the kernel approx-
imation that we introduce can be computed using a recurrent network  which we call recurrent
kernel network (RKN). This RNN arises from the dynamic programming structure used to compute
efﬁciently the substring kernel of [24]  a link already exploited by [20] to derive their sequence neural
network  which was a source of inspiration for our work. Both our kernels rely on a RNN to build
a representation of an input sequence by computing a string kernel between this sequence and a
set of learnable ﬁlters. Yet  our model exhibits several differences with [20]  who use the regular
substring kernel of [24] and compose this representation with another non-linear map—by applying
an activation function to the output of the RNN. By contrast  we obtain a different RKHS directly by
relaxing the substring kernel to allow for inexact matching at the compared positions  and embed
the Nyström approximation within the RNN. The resulting feature space can be interpreted as a
continuous neighborhood around all substrings (with gaps) of the described sequence. Furthermore 
our RNN provides a ﬁnite-dimensional approximation of the relaxed kernel  relying on the Nyström
approximation method [38]. As a consequence  RKNs may be learned in an unsupervised manner (in
such a case  the goal is to approximate the kernel map)  and with supervision with backpropagation 
which may be interpreted as performing feature selection in the RKHS.

In this paper  we make the following contributions:

Contributions.
• We generalize convolutional kernel networks for sequences [4] to allow gaps  an important option
for biological data. As in [4]  we observe that the kernel formulation brings practical beneﬁts over
traditional CNNs or RNNs [13] when the amount of labeled data is small or moderate.
• We provide a kernel point of view on recurrent neural networks with new unsupervised and
supervised learning algorithms. The resulting feature map can be interpreted in terms of gappy motifs 
and end-to-end learning amounts to performing feature selection.
• Based on [28]  we propose a new way to simulate max pooling in RKHSs  thus solving a classical
discrepancy between theory and practice in the literature of string kernels  where sums are often
replaced by a maximum operator that does not ensure positive deﬁniteness [36].

2 Background on Kernel Methods and String Kernels

Kernel methods consist in mapping data points living in a set X to a possibly inﬁnite-dimensional
Hilbert space H  through a mapping function Φ : X → H  before learning a simple predictive model
in H [33]. The so-called kernel trick allows to perform learning without explicitly computing this
mapping  as long as the inner-product K(x  x(cid:48)) = (cid:104)Φ(x)  Φ(x(cid:48))(cid:105)H between two points x  x(cid:48) can
be efﬁciently computed. Whereas kernel methods traditionally lack scalability since they require
computing an n × n Gram matrix  where n is the amount of training data  recent approaches based
on approximations have managed to make kernel methods work at large scale in many cases [30  38].
For sequences in X = A∗  which is the set of sequences of any possible length over an alphabet A 
the mapping Φ often enumerates subsequence content. For instance  the spectrum kernel maps
sequences to a ﬁxed-length vector Φ(x) = (φu(x))u∈Ak  where Ak is the set of k-mers—length-k

2

sequence of characters in A for some k in N  and φu(x) counts the number of occurrences of u
in x [21]. The mismatch kernel [22] operates similarly  but φu(x) counts the occurrences of u up to a
few mismatched letters  which is useful when k is large and exact occurrences are rare.

2.1 Substring kernels

As [20]  we consider the substring kernel introduced in [24]  which allows to model the presence
of gaps when trying to match a substring u to a sequence x. Modeling gaps requires introducing
the following notation: Ix k denotes the set of indices of sequence x with k elements (i1  . . .   ik)
satisfying 1 ≤ i1 < ··· < ik ≤ |x|  where |x| is the length of x. For an index set i in Ix k  we may
now consider the subsequence xi = (xi1  . . .   xik ) of x indexed by i. Then  the substring kernel
takes the same form as the mismatch and spectrum kernels  but φu(x) counts all—consecutive or
not—subsequences of x equal to u  and weights them by the number of gaps. Formally  we consider a
parameter λ in [0  1]  and φu(x) =(cid:80)i∈Ix k
λgaps(i)δ(u  xi)  where δ(u  v) = 1 if and only if u = v 
and 0 otherwise  and gaps(i) := ik − i1 − k + 1 is the number of gaps in the index set i. When λ is
small  gaps are heavily penalized  whereas a value close to 1 gives similar weights to all occurrences.
Ultimately  the resulting kernel between two sequences x and x(cid:48) is

Ks(x  x(cid:48)) := (cid:88)i∈Ix k (cid:88)j∈Ix(cid:48) k

λgaps(i)λgaps(j)δ(cid:0)xi  x(cid:48)j(cid:1) .

(1)

As we will see in Section 3  our RKN model relies on (1)  but unlike [20]  we replace the quantity
δ(xi  x(cid:48)j) that matches exact occurrences by a relaxation  allowing more subtle comparisons. Then 
we will show that the model can be interpreted as a gap-allowed extension of CKNs for sequences.
We also note that even though Ks seems computationally expensive at ﬁrst sight  it was shown in [24]
that (1) admits a dynamic programming structure leading to efﬁcient computations.

2.2 The Nyström method

When computing the Gram matrix is infeasible  it is typical to use kernel approximations [30  38] 
consisting in ﬁnding a q-dimensional mapping ψ : X → Rq such that the kernel K(x  x(cid:48)) can be
approximated by a Euclidean inner-product (cid:104)ψ(x)  ψ(x(cid:48))(cid:105)Rq. Then  kernel methods can be simulated
by a linear model operating on ψ(x)  which does not raise scalability issues if q is reasonably small.
Among kernel approximations  the Nyström method consists in projecting points of the RKHS onto a
q-dimensional subspace  allowing to represent points into a q-dimensional coordinate system.
Speciﬁcally  consider a collection of Z = {z1  . . .   zq} points in X and consider the subspace

E = Span(Φ(z1)  . . .   Φ(zq)) and deﬁne ψ(x) = K− 1

2

ZZ KZ(x) 

where KZZ is the q×q Gram matrix of K restricted to the samples z1  . . .   zq and KZ(x) in Rq carries
the kernel values K(x  zj)  j = 1  . . .   q. This approximation only requires q kernel evaluations and
often retains good performance for learning. Interestingly as noted in [25]  (cid:104)ψ(x)  ψ(x(cid:48))(cid:105)Rq is exactly
the inner-product in H between the projections of Φ(x) and Φ(x(cid:48)) onto E  which remain in H.
When X is a Euclidean space—this can be the case for sequences when using a one-hot encoding
representation  as discussed later— a good set of anchor points zj can be obtained by simply clustering
the data and choosing the centroids as anchor points [39]. The goal is then to obtain a subspace E
that spans data as best as possible. Otherwise  previous works on kernel networks [4  25] have also
developed procedures to learn the set of anchor points end-to-end by optimizing over the learning
objective. This approach can then be seen as performing feature selection in the RKHS.

3 Recurrent Kernel Networks

With the previous tools in hand  we now introduce RKNs. We show that it admits variants of CKNs 
substring and local alignment kernels as special cases  and we discuss its relation with RNNs.

3.1 A continuous relaxation of the substring kernel allowing mismatches
From now on  and with an abuse of notation  we represent characters in A as vectors in Rd. For
instance  when using one-hot encoding  a DNA sequence x = (x1  . . .   xm) of length m can be seen

3

eα((cid:104)xi x(cid:48)j(cid:105)−k) =(cid:81)k

as a 4-dimensional sequence where each xj in {0  1}4 has a unique non-zero entry indicating which
of {A  C  G  T} is present at the j-th position  and we denote by X the set of such sequences. We
now deﬁne the single-layer RKN as a generalized substring kernel (1) in which the indicator function
δ(xi  x(cid:48)j) is replaced by a kernel for k-mers:

Kk(x  x(cid:48)) := (cid:88)i∈Ix k (cid:88)j∈Ix(cid:48)  k

λx iλx je− α

2 (cid:107)xi−x(cid:48)j(cid:107)2

 

(2)

where we assume that the vectors representing characters have unit (cid:96)2-norm  such that e− α

2 (cid:107)xi−x(cid:48)j(cid:107)2
t=1 eα((cid:104)xit  x(cid:48)jt(cid:105)−1) is a dot-product kernel  and λx i = λgaps(i) if we follow (1).

=

For λ = 0 and using the convention 00 = 1  all the terms in these sums are zero except those
for k-mers with no gap  and we recover the kernel of the CKN model of [4] with a convolutional
structure—up to the normalization  which is done k-mer-wise in CKN instead of position-wise.
Compared to (1)  the relaxed version (2) accommodates inexact k-mer matching. This is important
for protein sequences  where it is common to consider different similarities between amino acids in
terms of substitution frequency along evolution [12]. This is also reﬂected in the underlying sequence
representation in the RKHS illustrated in Figure 1: by considering ϕ(.) the kernel mapping and
RKHS H such that K(xi  x(cid:48)j) = e− α

= (cid:104)ϕ(xi)  ϕ(x(cid:48)j)(cid:105)H  we have
λx jϕ(x(cid:48)j)(cid:43)
λx iϕ(xi)  (cid:88)j∈Ix(cid:48)  k
H
A natural feature map for a sequence x is therefore Φk(x) =(cid:80)i∈Ix k
λx iϕ(xi): using the RKN
2 (cid:107)xi−z(cid:107)2
amounts to representing x by a mixture of continuous neighborhoods ϕ(xi) : z (cid:55)→ e− α
centered on all its k-subsequences xi   each weighted by the corresponding λx i (e.g.  λx i = λgaps(i)).
As a particular case  a feature map of CKN [4] is the sum of the kernel mapping of all the k-mers
without gap.

Kk(x  x(cid:48)) =(cid:42) (cid:88)i∈Ix k

2 (cid:107)xi−x(cid:48)j(cid:107)2

(3)

.

Figure 1: Representation of sequences in a RKHS based on Kk with k = 4 and λx i = λgaps(i).

3.2 Extension to all k-mers and relation to the local alignment kernel
Dependency in the hyperparameter k can be removed by summing Kk over all possible values:

Ksum(x  x(cid:48)) :=

Kk(x  x(cid:48)) =

Kk(x  x(cid:48)).

∞(cid:88)k=1

max(|x| |x(cid:48)|)(cid:88)k=1

Interestingly  we note that Ksum admits the local alignment kernel of [36] as a special case. More
precisely  local alignments are deﬁned via the tensor product set Ak(x  x(cid:48)) := Ix k × Ix(cid:48) k  which
contains all possible alignments of k positions between a pair of sequences (x  x(cid:48)). The local
alignment score of each such alignment π = (i  j) in Ak(x  x(cid:48)) is deﬁned  by [36]  as S(x  x(cid:48)  π) :=
(cid:80)k
t=1 s(xit  x(cid:48)jt) −(cid:80)k−1
t=1 [g(it+1 − it − 1) + g(jt+1 − jt − 1)]  where s is a symmetric substitution

4

k-merkernelembeddingone4-merofxi1i2λi3λi4xii1i2i3i4λ2ϕ(xi)one-layerk-subsequencekernelxi1i2λi3λikallembeddedk-mersλgap(i)ϕ(xi)poolingPiλgap(i)ϕ(xi)function and g is a gap penalty function. The local alignment kernel in [36] can then be expressed in
terms of the above local alignment scores (Thrm. 1.7 in [36]):

KLA(x  x(cid:48)) =

K k

LA(x  x(cid:48)) :=

∞(cid:88)k=1

∞(cid:88)k=1 (cid:88)π∈Ak(x x(cid:48))

exp(βS(x  x(cid:48)  π)) for some β > 0.

(4)

When the gap penalty function is linear—that is  g(x) = cx with c > 0  K k

LA(x  x(cid:48)) =
(cid:80)π∈Ak(x x(cid:48)) exp(βS(x  x(cid:48)  π)) = (cid:80)(i j)∈Ak(x x(cid:48)) e−cβgaps(i)e−cβgaps(j)(cid:81)k
). When
s(xit  x(cid:48)jt) can be written as an inner-product (cid:104)ψs(xit)  ψs(x(cid:48)jt)(cid:105) between normalized vectors  we see
that KLA becomes a special case of (2)—up to a constant factor—with λx i = e−cβgaps(i)  α = β.
This observation sheds new lights on the relation between the substring and local alignment kernels 
which will inspire new algorithms in the sequel. To the best of our knowledge  the link we will
provide between RNNs and local alignment kernels is also new.

LA becomes K k
t=1 eβs(xit  x(cid:48)jt

3.3 Nyström approximation and recurrent neural networks

As in CKNs  we now use the Nyström approximation method as a building block to make the above
kernels tractable. According to (3)  we may ﬁrst use the Nyström method described in Section 2.2 to
ﬁnd an approximate embedding for the quantities ϕ(xi)  where xi is one of the k-mers represented
as a matrix in Rk×d. This is achieved by choosing a set Z = {z1  . . .   zq} of anchor points in Rk×d 
and by encoding ϕ(xi) as K−1/2
ZZ KZ(xi)—where K is the kernel of H. Such an approximation for
k-mers yields the q-dimensional embedding for the sequence x:

ψk(x) = (cid:88)i∈Ix k

2

ZZ (cid:88)i∈Ix k

λx iK− 1

ZZ KZ(xi) = K− 1

2

λx iKZ(xi).

(5)

Then  an approximate feature map ψsum(x) for the kernel Ksum can be obtained by concatenating the
embeddings ψ1(x)  . . .   ψk(x) for k large enough.

The anchor points as motifs. The continuous relaxation of the substring kernel presented in (2)
allows us to learn anchor points that can be interpreted as sequence motifs  where each position can
encode a mixture of letters. This can lead to more relevant representations than k-mers for learning on
biological sequences. For example  the fact that a DNA sequence is bound by a particular transcription
factor can be associated with the presence of a T followed by either a G or an A  followed by another
T  would require two k-mers but a single motif [4]. Our kernel is able to perform such a comparison.
Efﬁcient computations of Kk and Ksum approximation via RNNs. A naive computation of ψk(x)
would require enumerating all substrings present in the sequence  which may be exponentially large
when allowing gaps. For this reason  we use the classical dynamic programming approach of substring
kernels [20  24]. Consider then the computation of ψj(x) deﬁned in (5) for j = 1  . . .   k as well as a
set of anchor points Zk = {z1  . . .   zq} with the zi’s in Rd×k. We also denote by Zj the set obtained
when keeping only j-th ﬁrst positions (columns) of the zj’s  leading to Zj = {[z1]1:j  . . .   [zq]1:j} 
i in Rd
which will serve as anchor points for the kernel Kj to compute ψj(x). Finally  we denote by zj
the j-th column of zi such that zi = [z1
i ]. Then  the embeddings ψ1(x)  . . .   ψk(x) can be
computed recursively by using the following theorem:
Theorem 1. For any j ∈ {1  . . .   k} and t ∈ {1  . . .  |x|} 

i   . . .   zk

ψj(x1:t) = K− 1

2

Zj Zj(cid:26)cj[t]

hj[t]

if λx i = λ|x|−i1−j+1 
if λx i = λgaps(i) 

(6)

where cj[t] and hj[t] form a sequence of vectors in Rq indexed by t such that cj[0] = hj[0] = 0  and
c0[t] is a vector that contains only ones  while the sequence obeys the recursion
1 ≤ j ≤ k 
1 ≤ j ≤ k 

(7)
where (cid:12) is the elementwise multiplication operator and bj[t] is a vector in Rq whose entry i in
{1  . . .   q} is e− α

cj[t] = λcj[t − 1] + cj−1[t − 1] (cid:12) bj[t]
hj[t] = hj[t − 1] + cj−1[t − 1] (cid:12) bj[t]

j(cid:105)−1) and xt is the t-th character of x.

= eα((cid:104)xt zi

2 (cid:107)xt−zi

j(cid:107)2

5

A proof is provided in Appendix A and is based on classical recursions for computing the substring
kernel  which were interpreted as RNNs by [20]. The main difference in the RNN structure we
obtain is that their non-linearity is applied over the outcome of the network  leading to a feature map
formed by composing the feature map of the substring kernel of [24] and another one from a RKHS
that contains their non-linearity. By contrast  our non-linearities are built explicitly in the substring
kernel  by relaxing the indicator function used to compare characters. The resulting feature map is a
continuous neighborhood around all substrings of the described sequence. In addition  the Nyström
method yields an orthogonalization factor K−1/2
ZZ to the output KZ(x) of the network to compute our
approximation  which is perhaps the only non-standard component of our RNN. This factor provides
an interpretation of ψ(x) as a kernel approximation. As discussed next  it makes it possible to learn
the anchor points by k-means  see [4]  which also makes the initialization of the supervised learning
procedure simple without having to deal with the scaling of the initial motifs/ﬁlters zj.

Learning the anchor points Z. We now turn to the application of RKNs to supervised learning.
Given n sequences x1  . . .   xn in X and their associated labels y1  . . .   yn in Y  e.g.  Y = {−1  1}
for binary classiﬁcation or Y = R for regression  our objective is to learn a function in the RKHS H
of Kk by minimizing

min
f∈H

1
n

n(cid:88)i=1

L(f (xi)  yi) +

µ
2(cid:107)f(cid:107)2
H 

where L : R × R → R is a convex loss function that measures the ﬁtness of a prediction f (xi) to
the true label yi and µ controls the smoothness of the predictive function. After injecting our kernel
approximation Kk(x  x(cid:48)) (cid:39) (cid:104)ψk(x)  ψk(x(cid:48))(cid:105)Rq  the problem becomes
µ
2(cid:107)w(cid:107)2.

1
n

(8)

n(cid:88)i=1

L(cid:0)(cid:104)ψk(xi)  w(cid:105)  yi(cid:1) +

min
w∈Rq

Following [4  25]  we can learn the anchor points Z without exploiting training labels  by applying
a k-means algorithm to all (or a subset of) the k-mers extracted from the database and using the
obtained centroids as anchor points. Importantly  once Z has been obtained  the linear function
parametrized by w is still optimized with respect to the supervised objective (8). This procedure can
be thought of as learning a general representation of the sequences disregarding the supervised task 
which can lead to a relevant description while limiting overﬁtting.
Another strategy consists in optimizing (8) jointly over (Z  w)  after observing that ψk(x) =
K−1/2
λx iKZ(xi) is a smooth function of Z. Learning can be achieved by using backprop-
agation over (Z  w)  or by using an alternating minimization strategy between Z and w. It leads to
an end-to-end scheme where both the representation and the function deﬁned over this representation
are learned with respect to the supervised objective (8). Backpropagation rules for most operations
are classical  except for the matrix inverse square root function  which is detailed in Appendix B.
Initialization is also parameter-free since the unsupervised learning approach may be used for that.

ZZ (cid:80)i∈Ix k

3.4 Extensions

Multilayer construction.
struct a multilayer model based on kernel compositions similar to [20]. Assume that K(n)
layer kernel and Φ(n)

In order to account for long-range dependencies  it is possible to con-
is the n-th
its mapping function. The corresponding (n + 1)-th layer kernel is deﬁned as

k

k

K(n+1)

k

(x  x(cid:48)) = (cid:88)i∈Ix k j∈Ix(cid:48)  k

λ(n+1)
x i

λ(n+1)
x(cid:48) j

k(cid:89)t=1

Kn+1(Φ(n)

k (x1:it)  Φ(n)

k (x(cid:48)1:jt)) 

(9)

where Kn+1 will be deﬁned in the sequel and the choice of weights λ(n)
x i slightly differs from the
x i = λgaps(i) only for the last layer N of the kernel  which
single-layer model. We choose indeed λ(N )
depends on the number of gaps in the index set i but not on the index positions. Since (9) involves
a kernel Kn+1 operating on the representation of preﬁx sequences Φ(n)
k (x1:t) from layer n  the
representation makes sense only if Φ(n)
k (x1:t) carries mostly local information close to position t.

6

Otherwise  information from the beginning of the sequence would be overrepresented. Ideally  we
would like the range-dependency of Φ(n)
k (x1:t) (the size of the window of indices before t that
inﬂuences the representation  akin to receptive ﬁelds in CNNs) to grow with the number of layers
in a controllable manner. This can be achieved by choosing λ(n)
x i = λ|x|−i1−k+1 for n < N  which
assigns exponentially more weights to the k-mers close to the end of the sequence.
For the ﬁrst layer  we recover the single-layer network Kk deﬁned in (2) by deﬁning Φ(0)
k (x1:ik ) = xik
and K1(xik   x(cid:48)jk ) = eα((cid:104)xik  x(cid:48)jk(cid:105)−1). For n > 1  it remains to deﬁne Kn+1 to be a homogeneous
dot-product kernel  as used for instance in CKNs [25]:

Kn+1(u  u(cid:48)) = (cid:107)u(cid:107)Hn(cid:107)u(cid:107)Hnκn(cid:32)(cid:28) u

(cid:107)u(cid:107)Hn

 

u(cid:48)

(cid:107)u(cid:48)(cid:107)Hn(cid:29)Hn(cid:33) with κn(t) = eαn(t−1).

(10)

k

at each layer  allowing to replace the inner-products (cid:104)Φ(n)

Note that the Gaussian kernel K1 used for 1st layer may also be written as (10) since characters are
normalized. As for CKNs  the goal of homogenization is to prevent norms to grow/vanish exponen-
tially fast with n  while dot-product kernels lend themselves well to neural network interpretations.
As detailed in Appendix C  extending the Nyström approximation scheme for the multilayer con-
struction may be achieved in the same manner as with CKNs—that is  we learn one approximate
embedding ψ(n)
k (x(cid:48)1:jt)(cid:105) by
their approximations (cid:104)ψ(n)
k (x(cid:48)1:jt)(cid:105)  and it is easy to show that the interpretation in terms
of RNNs is still valid since K(n)
Max pooling in RKHS. Alignment scores (e.g. Smith-Waterman) in molecular biology rely on
a max operation—over the scores of all possible alignments—to compute similarities between
sequences. However  using max in a string kernel usually breaks positive deﬁniteness  even though it
seems to perform well in practice. To solve such an issue  sum-exponential is used as a proxy in [32] 
but it leads to diagonal dominance issue and makes SVM solvers unable to learn. For RKN  the sum
in (3) can also be replaced by a max

has the same sum structure as (2).

k (x1:it)  ψ(n)

k (x1:it)  Φ(n)

k

k (x  x(cid:48)) =(cid:28) max
Kmax

i∈Ix k

λx iψk(xi)  max
j∈Ix(cid:48)  k

λx jψk(x(cid:48)j)(cid:29)  

(11)

which empirically seems to perform well  but breaks the kernel interpretation  as in [32]. The
corresponding recursion amounts to replacing all the sum in (7) by a max.
An alternative way to aggregate local features is the generalized max pooling (GMP) introduced in
[28]  which can be adapted to the context of RKHSs. Assuming that before pooling x is embedded
to a set of N local features (ϕ1  . . .   ϕN ) ∈ HN   GMP builds a representation ϕgmp whose inner-
product with all the local features ϕi is one: (cid:104)ϕi  ϕgmp(cid:105)H = 1  for i = 1  . . .   N. ϕgmp coincides
with the regular max when each ϕ is an element of the canonical basis of a ﬁnite representation—i.e. 
assuming that at each position  a single feature has value 1 and all others are 0.
Since GMP is deﬁned by a set of inner-products constraints  it can be applied to our approximate
kernel embeddings by solving a linear system. This is compatible with CKN but becomes intractable
for RKN which pools across |Ix k| positions. Instead  we heuristically apply GMP over the set
ψk(x1:t) for all t with λx i = λ|x|−i1−k+1  which can be obtained from the RNN described in
Theorem 1. This amounts to composing GMP with mean poolings obtained over each preﬁx of x.
We observe that it performs well in our experiments. More details are provided in Appendix D.

4 Experiments

We evaluate RKN and compare it to typical string kernels and RNN for protein fold recognition.
Pytorch code is provided with the submission and additional details given in Appendix E.

4.1 Protein fold recognition on SCOP 1.67

Sequencing technologies provide access to gene and  indirectly  protein sequences for yet poorly
studied species. In order to predict the 3D structure and function from the linear sequence of these

7

proteins  it is common to search for evolutionary related ones  a problem known as homology
detection. When no evolutionary related protein with known structure is available  a—more difﬁcult—
alternative is to resort to protein fold recognition. We evaluate our RKN on such a task  where the
objective is to predict which proteins share a 3D structure with the query [31].
Here we consider the Structural Classiﬁcation Of Proteins (SCOP) version 1.67 [29]. We follow
the preprocessing procedures of [10] and remove the sequences that are more than 95% similar 
yielding 85 fold recognition tasks. Each positive training set is then extended with Uniref50 to
make the dataset more balanced  as proposed in [13]. The resulting dataset can be downloaded
from http://www.bioinf.jku.at/software/LSTM_protein. The number of training samples
for each task is typically around 9 000 proteins  whose length varies from tens to thousands of
amino-acids. In all our experiments we use logistic loss. We measure classiﬁcation performances
using auROC and auROC50 scores (area under the ROC curve and up to 50% false positives).
For CKN and RKN  we evaluate both one-hot encoding of amino-acids by 20-dimensional binary
vectors and an alternative representation relying on the BLOSUM62 substitution matrix [12]. Specif-
ically in the latter case  we represent each amino-acid by the centered and normalized vector of
its corresponding substitution probabilities with other amino-acids. The local alignment kernel (4) 
which we include in our comparison  natively uses BLOSUM62.

Hyperparameters. We follow the training procedure of CKN presented in [4]. Speciﬁcally  for
each of the 85 tasks  we hold out one quarter of the training samples as a validation set  use it to
tune α  gap penalty λ and the regularization parameter µ in the prediction layer. These parameters are
then ﬁxed across datasets. RKN training also relies on the alternating strategy used for CKN: we use
an Adam algorithm to update anchor points  and the L-BFGS algorithm to optimize the prediction
layer. We train 100 epochs for each dataset: the initial learning rate for Adam is ﬁxed to 0.05 and is
halved as long as there is no decrease of the validation loss for 5 successive epochs. We ﬁx k to 10 
the number of anchor points q to 128 and use single layer CKN and RKN throughout the experiments.

Implementation details for unsupervised models. The anchor points for CKN and RKN are
learned by k-means on 30 000 extracted k-mers from each dataset. The resulting sequence represen-
tations are standardized by removing mean and dividing by standard deviation and are used within a
logistic regression classiﬁer. α in Gaussian kernel and the parameter λ are chosen based on validation
loss and are ﬁxed across the datasets. µ for regularization is chosen by a 5-fold cross validation on
each dataset. As before  we ﬁx k to 10 and the number of anchor points q to 1024. Note that the
performance could be improved with larger q as observed in [4]  at a higher computational cost.

Comparisons and results. The results are shown in Table 1. The blosum62 version of CKN and
RKN outperform all other methods. Improvement against the mismatch and LA kernels is likely
caused by end-to-end trained kernel networks learning a task-speciﬁc representation in the form of a
sparse set of motifs  whereas data-independent kernels lead to learning a dense function over the set
of descriptors. This difference can have a regularizing effect akin to the (cid:96)1-norm in the parametric
world  by reducing the dimension of the learned linear function w while retaining relevant features
for the prediction task. GPkernel also learns motifs  but relies on the exact presence of discrete motifs.
Finally  both LSTM and [20] are based on RNNs but are outperformed by kernel networks. The latter
was designed and optimized for NLP tasks and yields a 0.4 auROC50 on this task.
RKNs outperform CKNs  albeit not by a large margin. Interestingly  as the two kernels only differ
by their allowing gaps when comparing sequences  this results suggests that this aspect is not the
most important for identifying common foldings in a one versus all setting: as the learned function
discriminates on fold from all others  it may rely on coarser features and not exploit more subtle ones
such as gappy motifs. In particular  the advantage of the LA-kernel against its mismatch counterpart
is more likely caused by other differences than gap modelling  namely using a max rather than a
mean pooling of k-mer similarities across the sequence  and a general substitution matrix rather than
a Dirac function to quantify mismatches. Consistently  within kernel networks GMP systematically
outperforms mean pooling  while being slightly behind max pooling.
Additional details and results  scatter plots  and pairwise tests between methods to assess the statistical
signiﬁcance of our conclusions are provided in Appendix E. Note that when k = 14  the auROC and
auROC50 further increase to 0.877 and 0.636 respectively.

8

Table 1: Average auROC and auROC50 for SCOP fold recognition benchmark. LA-kernel uses
BLOSUM62 to compare amino acids which is a little different from our encoding approach. Details
about pairwise statistical tests between methods can be found in Appendix E.

Method

pooling

one-hot

GPkernel [10]
SVM-pairwise [23]
Mismatch [22]
LA-kernel [32]
LSTM [13]
CKN-seq [4]
CKN-seq [4]
CKN-seq
CKN-seq (unsup)[4]
RKN (λ = 0)
RKN
RKN (λ = 0)
RKN
RKN (λ = 0)
RKN
RKN (unsup)

auROC auROC50
0.844
0.724
0.814

0.514
0.359
0.467

BLOSUM62

auROC auROC50

–

–

0.834

0.504

–

–

0.830
0.827
0.837
0.838
0.804
0.829
0.829
0.840
0.844
0.840
0.848
0.805

mean
max
GMP
mean
mean
mean
max
max
GMP
GMP
mean

0.566
0.536
0.572
0.561
0.493
0.542
0.541
0.575
0.587
0.563
0.570
0.504

–

0.843
0.866
0.856
0.827
0.838
0.840
0.862
0.871
0.855
0.852
0.833

–

0.563
0.621
0.608
0.548
0.563
0.571
0.618
0.629
0.598
0.609
0.570

Table 2: Classiﬁcation accuracy for SCOP 2.06. The complete table with error bars can be found in
Appendix E.

Method

PSI-BLAST
DeepSF
CKN (128 ﬁlters)
CKN (512 ﬁlters)
RKN (128 ﬁlters)
RKN (512 ﬁlters)

(cid:93)Params Accuracy on SCOP 2.06
top 10
87.34
94.51
95.27
96.36
95.51
96.54

top 5
86.48
90.25
92.17
94.29
92.89
94.95

top 1
84.53
73.00
76.30
84.11
77.82
85.29

-

920k
211k
843k
211k
843k

Level-stratiﬁed accuracy (top1/top5/top10)

family

superfamily

fold

82.20/84.50/85.30
75.87/91.77/95.14
83.30/94.22/96.00
90.24/95.77/97.21
76.91/93.13/95.70
84.31/94.80/96.74

86.90/88.40/89.30
72.23/90.08/94.70
74.03/91.83/95.34
82.33/94.20/96.35
78.56/92.98/95.53
85.99/95.22/96.60

18.90/35.10/35.10
51.35/67.57/72.97
43.78/67.03/77.57
45.41/69.19/79.73
60.54/83.78/90.54
71.35/84.86/89.73

4.2 Protein fold classiﬁcation on SCOP 2.06

We further benchmark RKN in a fold classiﬁcation task  following the protocols used in [15].
Speciﬁcally  the training and validation datasets are composed of 14699 and 2013 sequences from
SCOP 1.75  belonging to 1195 different folds. The test set consists of 2533 sequences from SCOP
2.06  after removing the sequences with similarity greater than 40% with SCOP 1.75. The input
sequence feature is represented by a vector of 45 dimensions  consisting of a 20-dimensional one-hot
encoding of the sequence  a 20-dimensional position-speciﬁc scoring matrix (PSSM) representing
the proﬁle of amino acids  a 3-class secondary structure represented by a one-hot vector and a
2-class solvent accessibility. We further normalize each type of the feature vectors to have unit
(cid:96)2-norm  which is done for each sequence position. More dataset details can be found in [15]. We
use mean pooling for both CKN and RKN models  as it is more stable during training for multi-class
classiﬁcation. The other hyperparameters are chosen in the same way as previously. More details
about hyperparameter search grid can be found in Appendix E.
The accuracy results are obtained by averaging 10 different runs and are shown in Table 2  stratiﬁed
by prediction difﬁculty (family/superfamily/fold  more details can be found in [15]). By contrast
to what we observed on SCOP 1.67  RKN sometimes yields a large improvement on CKN for fold
classiﬁcation  especially for detecting distant homologies. This suggests that accounting for gaps
does help in some fold prediction tasks  at least in a multi-class context where a single function is
learned for each fold.

9

Acknowledgments

We thank the anonymous reviewers for their insightful comments and suggestions. This work has
been supported by the grants from ANR (FAST-BIG project ANR-17-CE23-0011-01)  by the ERC
grant number 714381 (SOLARIS)  and ANR 3IA MIAI@Grenoble Alpes.

References
[1] B. Alipanahi  A. Delong  M. T. Weirauch  and B. J. Frey. Predicting the sequence speciﬁcities
of DNA-and RNA-binding proteins by deep learning. Nature biotechnology  33(8):831–838 
2015.

[2] C. Angermueller  T. Pärnamaa  L. Parts  and O. Stegle. Deep learning for computational biology.

Molecular Systems Biology  12(7):878  2016.

[3] A. Auton  L. D. Brooks  R. M. Durbin  E. Garrison  H. M. Kang  J. O. Korbel  J. Marchini 
S. McCarthy  G. McVean  and G. R. Abecasis. A global reference for human genetic variation.
Nature  526:68–74  2015.

[4] D. Chen  L. Jacob  and J. Mairal. Biological sequence modeling with convolutional kernel

networks. Bioinformatics  35(18):3294–3302  02 2019.

[5] K. Cho  B. Van Merriënboer  C. Gulcehre  D. Bahdanau  F. Bougares  H. Schwenk  and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) 
2014.

[6] Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information

Processing Systems (NIPS)  2009.

[7] T. C. P.-G. Consortium. Computational pan-genomics: status  promises and challenges. Brieﬁngs

in Bioinformatics  19(1):118–135  10 2016.

[8] L. Flagel  Y. Brandvain  and D. R. Schrider. The Unreasonable Effectiveness of Convolutional
Neural Networks in Population Genetic Inference. Molecular Biology and Evolution  36(2):220–
238  12 2018.

[9] M. B. Giles. Collected matrix derivative results for forward and reverse mode algorithmic

differentiation. In Advances in Automatic Differentiation  pages 35–44. Springer  2008.

[10] T. Håndstad  A. J. Hestnes  and P. Sætrom. Motif kernel generated by genetic programming

improves remote homology and fold detection. BMC bioinformatics  8(1):23  2007.

[11] D. Haussler. Convolution kernels on discrete structures. Technical report  Department of

Computer Science  University of California  1999.

[12] S. Henikoff and J. G. Henikoff. Amino acid substitution matrices from protein blocks. Proceed-

ings of the National Academy of Sciences  89(22):10915–10919  1992.

[13] S. Hochreiter  M. Heusel  and K. Obermayer. Fast model-based protein homology detection

without alignment. Bioinformatics  23(14):1728–1736  2007.

[14] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–

1780  1997.

[15] J. Hou  B. Adhikari  and J. Cheng. DeepSF: deep convolutional neural network for mapping

protein sequences to folds. Bioinformatics  34(8):1295–1303  12 2017.

[16] E. J. Topol. High-performance medicine: the convergence of human and artiﬁcial intelligence.

Nature Medicine  25  01 2019.

[17] T. S. Jaakkola  M. Diekhans  and D. Haussler. Using the ﬁsher kernel method to detect remote
protein homologies. In Conference on Intelligent Systems for Molecular Biology (ISMB)  1999.

10

[18] N. Kalchbrenner  E. Grefenstette  and P. Blunsom. A convolutional neural network for modelling

sentences. In Association for Computational Linguistics (ACL)  2014.

[19] Y. LeCun  B. Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D.
Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation 
1(4):541–551  1989.

[20] T. Lei  W. Jin  R. Barzilay  and T. Jaakkola. Deriving neural architectures from sequence and

graph kernels. In International Conference on Machine Learning (ICML)  2017.

[21] C. Leslie  E. Eskin  and W. S. Noble. The spectrum kernel: A string kernel for svm protein

classiﬁcation. In Biocomputing  pages 564–575. World Scientiﬁc  2001.

[22] C. S. Leslie  E. Eskin  A. Cohen  J. Weston  and W. S. Noble. Mismatch string kernels for

discriminative protein classiﬁcation. Bioinformatics  20(4):467–476  2004.

[23] L. Liao and W. S. Noble. Combining pairwise sequence similarity and support vector machines
for detecting remote protein evolutionary and structural relationships. Journal of computational
biology  10(6):857–868  2003.

[24] H. Lodhi  C. Saunders  J. Shawe-Taylor  N. Cristianini  and C. Watkins. Text classiﬁcation

using string kernels. Journal of Machine Learning Research (JMLR)  2:419–444  2002.

[25] J. Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. In

Advances in Neural Information Processing Systems (NIPS)  2016.

[26] S. Merity  N. S. Keskar  and R. Socher. Regularizing and optimizing lstm language models. In

International Conference on Learning Representations (ICLR)  2018.

[27] A. Morrow  V. Shankar  D. Petersohn  A. Joseph  B. Recht  and N. Yosef. Convolutional kitchen
sinks for transcription factor binding site prediction. arXiv preprint arXiv:1706.00125  2017.

[28] N. Murray and F. Perronnin. Generalized max pooling. In Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition (CVPR)  2014.

[29] A. G. Murzin  S. E. Brenner  T. Hubbard  and C. Chothia. Scop: a structural classiﬁcation
of proteins database for the investigation of sequences and structures. Journal of molecular
biology  247(4):536–540  1995.

[30] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Adv. in Neural

Information Processing Systems (NIPS)  2008.

[31] H. Rangwala and G. Karypis. Proﬁle-based direct kernels for remote homology detection and

fold recognition. Bioinformatics  21(23):4239–4247  2005.

[32] H. Saigo  J.-P. Vert  N. Ueda  and T. Akutsu. Protein homology detection using string alignment

kernels. Bioinformatics  20(11):1682–1689  2004.

[33] B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines  regularization 

optimization  and beyond. MIT press  2002.

[34] B. Schölkopf  K. Tsuda  and J.-P. Vert. Kernel methods in computational biology. MIT Press 

Cambridge  Mass.  2004.

[35] K. Tsuda  T. Kin  and K. Asai. Marginalized kernels for biological sequences. Bioinformatics 

18(suppl_1):S268–S275  07 2002.

[36] J.-P. Vert  H. Saigo  and T. Akutsu. Convolution and local alignment kernels. Kernel methods in

computational biology  pages 131–154  2004.

[37] C. Watkins. Dynamic alignment kernels. In Advances in Neural Information Processing Systems

(NIPS)  1999.

[38] C. K. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. In

Advances in Neural Information Processing Systems (NIPS)  2001.

11

[39] K. Zhang  I. W. Tsang  and J. T. Kwok. Improved nyström low-rank approximation and error

analysis. In International Conference on Machine Learning (ICML)  2008.

[40] Y. Zhang  P. Liang  and M. J. Wainwright. Convexiﬁed convolutional neural networks. In

International Conference on Machine Learning (ICML)  2017.

[41] J. Zhou and O. Troyanskaya. Predicting effects of noncoding variants with deep learning-based

sequence model. Nature Methods  12(10):931–934  2015.

12

,Dexiong Chen
Laurent Jacob
Julien Mairal