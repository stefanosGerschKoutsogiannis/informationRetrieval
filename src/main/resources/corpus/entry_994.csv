2016,Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks,Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However  most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem  we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this  we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that  by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings  CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.,Collaborative Recurrent Autoencoder:

Recommend while Learning to Fill in the Blanks

Hao Wang  Xingjian Shi  Dit-Yan Yeung

Hong Kong University of Science and Technology

{hwangaz xshiab dyyeung}@cse.ust.hk

Abstract

Hybrid methods that utilize both content and rating information are commonly
used in many recommender systems. However  most of them use either handcrafted
features or the bag-of-words representation as a surrogate for the content infor-
mation but they are neither effective nor natural enough. To address this problem 
we develop a collaborative recurrent autoencoder (CRAE) which is a denoising
recurrent autoencoder (DRAE) that models the generation of content sequences in
the collaborative ﬁltering (CF) setting. The model generalizes recent advances in
recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides
a new denoising scheme along with a novel learnable pooling scheme for the recur-
rent autoencoder. To do this  we ﬁrst develop a hierarchical Bayesian model for the
DRAE and then generalize it to the CF setting. The synergy between denoising
and CF enables CRAE to make accurate recommendations while learning to ﬁll
in the blanks in sequences. Experiments on real-world datasets from different
domains (CiteULike and Netﬂix) show that  by jointly modeling the order-aware
generation of sequences for the content information and performing CF for the
ratings  CRAE is able to signiﬁcantly outperform the state of the art on both the
recommendation task based on ratings and the sequence generation task based on
content information.

1

Introduction

With the high prevalence and abundance of Internet services  recommender systems are becoming
increasingly important to attract users because they can help users make effective use of the informa-
tion available. Companies like Netﬂix have been using recommender systems extensively to target
users and promote products. Existing methods for recommender systems can be roughly categorized
into three classes [13]: content-based methods that use the user proﬁles or product descriptions only 
collaborative ﬁltering (CF) based methods that use the ratings only  and hybrid methods that make
use of both. Hybrid methods using both types of information can get the best of both worlds and  as a
result  usually outperform content-based and CF-based methods.
Among the hybrid methods  collaborative topic regression (CTR) [20] was proposed to integrate a
topic model and probabilistic matrix factorization (PMF) [15]. CTR is an appealing method in that it
produces both promising and interpretable results. However  CTR uses a bag-of-words representation
and ignores the order of words and the local context around each word  which can provide valuable
information when learning article representation and word embeddings. Deep learning models like
convolutional neural networks (CNN) which use layers of sliding windows (kernels) have the potential
of capturing the order and local context of words. However  the kernel size in a CNN is ﬁxed during
training. To achieve good enough performance  sometimes an ensemble of multiple CNNs with
different kernel sizes has to be used. A more natural and adaptive way of modeling text sequences
would be to use gated recurrent neural network (RNN) models [8  3  18]. A gated RNN takes in one

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

word (or multiple words) at a time and lets the learned gates decide whether to incorporate or to
forget the word. Intuitively  if we can generalize gated RNNs to the CF setting (non-i.i.d.) to jointly
model the generation of sequences and the relationship between items and users (rating matrices)  the
recommendation performance could be signiﬁcantly boosted.
Nevertheless  very few attempts have been made to develop feedforward deep learning models for CF 
let alone recurrent ones. This is due partially to the fact that deep learning models  like many machine
learning models  assume i.i.d. inputs. [16  6  7] use restricted Boltzmann machines and RNN instead
of the conventional matrix factorization (MF) formulation to perform CF. Although these methods
involve both deep learning and CF  they actually belong to CF-based methods because they do not
incorporate the content information like CTR  which is crucial for accurate recommendation. [14]
uses low-rank MF in the last weight layer of a deep network to reduce the number of parameters  but
it is for classiﬁcation instead of recommendation tasks. There have also been nice explorations on
music recommendation [10  25] in which a CNN or deep belief network (DBN) is directly used for
content-based recommendation. However  the models are deterministic and less robust since the noise
is not explicitly modeled. Besides  the CNN is directly linked to the ratings making the performance
suffer greatly when the ratings are sparse  as will be shown later in our experiments. Very recently 
collaborative deep learning (CDL) [23] is proposed as a probabilistic model for joint learning of
a probabilistic stacked denoising autoencoder (SDAE) [19] and collaborative ﬁltering. However 
CDL is a feedforward model that uses bag-of-words as input and it does not model the order-aware
generation of sequences. Consequently  the model would have inferior recommendation performance
and is not capable of generating sequences at all  which will be shown in our experiments. Besides
order-awareness  another drawback of CDL is its lack of robustness (see Section 3.1 and 3.5 for
details). To address these problems  we propose a hierarchical Bayesian generative model called
collaborative recurrent autoencoder (CRAE) to jointly model the order-aware generation of sequences
(in the content information) and the rating information in a CF setting. Our main contributions are:
• By exploiting recurrent deep learning collaboratively  CRAE is able to sophisticatedly model
the generation of items (sequences) while extracting the implicit relationship between items
(and users). We design a novel pooling scheme for pooling variable-length sequences into
ﬁxed-length vectors and also propose a new denoising scheme to effectively avoid overﬁtting.
Besides for recommendation  CRAE can also be used to generate sequences on the ﬂy.
• To the best of our knowledge  CRAE is the ﬁrst model that bridges the gap between RNN
and CF  especially with respect to hybrid methods for recommender systems. Besides  the
Bayesian nature also enables CRAE to seamlessly incorporate other auxiliary information
to further boost the performance.
• Extensive experiments on real-world datasets from different domains show that CRAE can

substantially improve on the state of the art.

2 Problem Statement and Notation

Similar to [20]  the recommendation task considered in this paper takes implicit feedback [9] as the
training and test data. There are J items (e.g.  articles or movies) in the dataset. For item j  there is a
corresponding sequence consisting of Tj words where the vector e(j)
speciﬁes the t-th word using the
1-of-S representation  i.e.  a vector of length S with the value 1 in only one element corresponding
to the word and 0 in all other elements. Here S is the vocabulary size of the dataset. We deﬁne an
I-by-J binary rating matrix R = [Rij]I×J where I denotes the number of users. For example  in the
CiteULike dataset  Rij = 1 if user i has article j in his or her personal library and Rij = 0 otherwise.
Given some of the ratings in R and the corresponding sequences of words e(j)
(e.g.  titles of articles
or plots of movies)  the problem is to predict the other ratings in R.

t

t

denotes the noise-corrupted version of e(j)

(cid:48)(j)
t ) refers to
In the following sections  e
t
e)
the concatenation of the two KW -dimensional column vectors. All input weights (like Ye and Yi
e) are of dimensionality KW -by-KW . The output state h(j)
and recurrent weights (like We and Wi
 
(j))  and cell state s(j)
gate units (e.g.  ho
are of dimensionality KW . K is the dimensionality of the
t
ﬁnal representation γj  middle-layer units θj  and latent vectors vj and ui. IK or IKW denotes a
K-by-K or KW -by-KW identity matrix. For convenience we use W+ to denote the collection of all
weights and biases. Similarly h+
t

is used to denote the collection of ht  hi

t  hf

t   and ho
t .

t

t

and (h(j)

; s(j)

t

t

2

Figure 1: On the left is the graphical model for an example CRAE where Tj = 2 for all j. To
prevent clutter  the hyperparameters for beta-pooling  all weights  biases  and links between ht and γ
are omitted. On the right is the graphical model for the degenerated CRAE. An example recurrent
autoencoder with Tj = 3 is shown. ‘(cid:104)?(cid:105)’ is the (cid:104)wildcard(cid:105) and ‘$’ marks the end of a sentence. E(cid:48)
and E are used in place of [e
3 Collaborative Recurrent Autoencoder

]Tj
t=1 respectively.

]Tj
t=1 and [e(j)

(cid:48)(j)
t

t

In this section we will ﬁrst propose a generalization of the RNN called robust recurrent networks
(RRN)  followed by the introduction of two key concepts  wildcard denoising and beta-pooling  in
our model. After that  the generative process of CRAE is provided to show how to generalize the
RRN as a hierarchical Bayesian model from an i.i.d. setting to a CF (non-i.i.d.) setting.

3.1 Robust Recurrent Networks

One problem with RNN models like long short-term memory networks (LSTM) is that the computa-
tion is deterministic without taking the noise into account  which means it is not robust especially
with insufﬁcient training data. To address this robustness problem  we propose RRN as a type of
noisy gated RNN. In RRN  the gates and other latent variables are designed to incorporate noise 
making the model more robust. Note that unlike [4  5]  the noise in RRN is directly propagated back
and forth in the network  without the need for using separate neural networks to approximate the
distributions of the latent variables. This is much more efﬁcient and easier to implement. Here we
provide the generative process of RRN. Using t = 1 . . . Tj to index the words in the sequence  we
have (we drop the index j for items for notational simplicity):

st ∼ N (σ(hf

t−1) (cid:12) st−1 + σ(hi

xt−1 ∼ N (Wwet−1  λ−1

s IKW )  at−1 ∼ N (Yxt−1 + Wht−1 + b  λ−1

(1)
(2)
where xt is the word embedding of the t-th word  Ww is a KW -by-S word embedding matrix  et is
the 1-of-S representation mentioned above  (cid:12) stands for the element-wise product operation between
two vectors  σ(·) denotes the sigmoid function  st is the cell state of the t-th word  and b  Y  and
W denote the biases  input weights  and recurrent weights respectively. The forget gate units hf
t
and the input gate units hi
t in Equation (2) are drawn from Gaussian distributions depending on their
corresponding weights and biases Yf   Wf   Yi  Wi  bf   and bi:

t−1) (cid:12) σ(at−1)  λ−1

s IKW ) 

s IKW )

s IKW )  hi

t ∼ N (Yixt + Wiht + bi  λ−1

t ∼ N (Yf xt + Wf ht + bf   λ−1
hf
The output ht depends on the output gate ho
t ∼ N (Yoxt + Woht + bo  λ−1
ho

t which has its own weights and biases Yo  Wo  and bo:
(3)
In the RRN  information of the processed sequence is contained in the cell states st and the output
states ht  both of which are column vectors of length KW . Note that RRN can be seen as a generalized
and Bayesian version of LSTM [1]. Similar to [18  3]  two RRNs can be concatenated to form an
encoder-decoder architecture.

s IKW )  ht ∼ N (tanh(st) (cid:12) σ(ho

t−1)  λ−1

s IKW ).

s IKW ).

3.2 Wildcard Denoising

Since the input and output are identical here  unlike [18  3] where the input is from the source
language and the output is from the target language  this naive RRN autoencoder can suffer from
serious overﬁtting  even after taking noise into account and reversing sequence order (we ﬁnd that

3

h1h1h2h2h3h3h4h4h5h5s1s1s2s2s3s3s4s4s5s5e01e01e02e02e1e1e2e2µµ°°vvJIRRuu¸u¸u¸v¸vJIEEE0E0¸w¸wW+W+vvRRuu¸u¸uµµA$AB$¸v¸v<?>reversing sequence order in the decoder [18] does not improve the recommendation performance).
One natural way of handling it is to borrow ideas from the denoising autoencoder [19] by randomly
dropping some of the words in the encoder. Unfortunately  directly dropping words may mislead
the learning of transition between words. For example  if we drop the word ‘is’ in the sentence
‘this is a good idea’  the encoder will wrongly learn the subsequence ‘this a’  which never appears
in a grammatically correct sentence. Here we propose another denoising scheme  called wildcard
denoising  where a special word ‘(cid:104)wildcard(cid:105)’ is added to the vocabulary and we randomly select
some of the words and replace them with ‘(cid:104)wildcard(cid:105)’. This way  the encoder RRN will take ‘this
(cid:104)wildcard(cid:105) a good idea’ as input and successfully avoid learning wrong subsequences. We call this
denoising recurrent autoencoder (DRAE). Note that the word ‘(cid:104)wildcard(cid:105)’ also has a corresponding
word embedding. Intuitively this wildcard denoising RRN autoencoder learns to ﬁll in the blanks in
sentences automatically. We ﬁnd this denoising scheme much better than the naive one. For example 
in dataset CiteULike wildcard denoising can provide a relative accuracy boost of about 20%.

3.3 Beta-Pooling

The RRN autoencoders would produce a representation vector for each input word. In order to
facilitate the factorization of the rating matrix  we need to pool the sequence of vectors into one
single vector of ﬁxed length 2KW before it is further encoded into a K-dimensional vector. A natural
way is to use a weighted average of the vectors. Unfortunately different sequences may need weights
of different size. For example  pooling a sequence of 8 vectors needs a weight vector with 8 entries
while pooling a sequence of 50 vectors needs one with 50 entries. In other words  we need a weight
vector of variable length for our pooling scheme. To tackle this problem  we propose to use a beta
distribution. If six vectors are to be pooled into one single vector (using weighted average)  we
can use the area wp in the range ( p−1
6 ) of the x-axis of the probability density function (PDF)
for the beta distribution Beta(a  b) as the pooling weight. Then the resulting pooling weight vector
becomes y = (w1  . . .   w6)T . Since the total area is always 1 and the x-axis is bounded  the beta
distribution is perfect for this type of variable-length pooling (hence the name beta-pooling). If we
set the hyperparameters a = b = 1  it will be equivalent to average pooling. If a is set large enough
and b > a the PDF will peak slightly to the left of x = 0.5  which means that the last time step of the
encoder RRN is directly used as the pooling result. With only two parameters  beta-pooling is able to
pool vectors ﬂexibly enough without having the risk of overﬁtting the data.

6   p

3.4 CRAE as a Hierarchical Bayesian Model

Following the notation in Section 2 and using the DRAE in Section 3.2 as a component  we then
provide the generative process of the CRAE (note that t indexes words or time steps  j indexes
sentences or documents  and Tj is the number of words in document j):
(cid:48)(j)
Encoding (t = 1  2  . . .   Tj): Generate x
t−1  a(j)
Compression and decompression (t = Tj + 1):

according to Equation (1)-(2).

t−1  and s(j)

t

θj ∼ N (W1(h(j)

Tj

; s(j)
Tj

) + b1  λ

−1
s IK )  (h(j)

Tj +1; s(j)

Tj +1) ∼ N (W2 tanh(θj) + b2  λ

−1
s I2KW ).

(4)

Decoding (t = Tj + 2  Tj + 3  . . .   2Tj + 1): Generate a(j)
tion (1)-(3)  after which generate:

t−1  s(j)

t

  and h(j)

t

according to Equa-

t−Tj−2 ∼ Mult(softmax(Wgh(j)
e(j)

t + bg)).

Beta-pooling and recommendation:

γj ∼ N (tanh(W1fa b({(h(j)
vj ∼ N (γj  λ−1

v IK)  ui ∼ N (0  λ−1

t

u IK)  Rij ∼ N (uT

i vj  C−1
ij ).

t )}t) + b1)  λ−1
; s(j)

s IK)

(5)

Note that each column of the weights and biases in W+ is drawn from N (0  λ−1
N (0  λ−1
can be drawn as described in Section 3.1. e(cid:48)(j)

w IK). In the generative process above  the input gate hi

denotes the corrupted word (with the embedding

(j) and the forget gate hf

w IKW ) or
(j)

t−1

t−1

t

4

t

t

) and e(j)

denotes the original word (with the embedding x(j)

(cid:48)(j)
). λw  λu  λs  and λv are hy-
x
t
perparameters and Cij is a conﬁdence parameter (Cij = α if Rij = 1 and Cij = β otherwise).
Note that if λs goes to inﬁnity  the Gaussian distribution (e.g.  in Equation (4)) will become a Dirac
delta distribution centered at the mean. The compression and decompression act like a bottleneck
between two Bayesian RRNs. The purpose is to reduce overﬁtting  provide necessary nonlinear
transformation  and perform dimensionality reduction to obtain a more compact ﬁnal representa-
tion γj for CF. The graphical model for an example CRAE where Tj = 2 for all j is shown in
Figure 1(left). fa b({(h(j)
t )}t) in Equation (5) is the result of beta-pooling with hyperparameters
; s(j)
a and b. If we denote the cumulative distribution function of the beta distribution as F (x; a  b) 
φ(j)
t = (h(j)
; s(j)
t+1) for t = Tj + 1  . . .   2Tj  then
we have fa b({(h(j)
  a  b))φt. Please see Section 3 of the
supplementary materials for details (including hyperparameter learning) of beta-pooling. From the
generative process  we can see that both CRAE and CDL are Bayesian deep learning (BDL) models
(as described in [24]) with a perception component (DRAE in CRAE) and a task-speciﬁc component.

t )}t) =(cid:80)2Tj

t ) for t = 1  . . .   Tj  and φ(j)

t = (h(j)
  a  b) − F ( t−1

t=1(F ( t
2Tj

t+1; s(j)

; s(j)

2Tj

t

t

t

3.5 Learning

According to the CRAE model above  all parameters like h(j)
and vj can be treated as random
variables so that a full Bayesian treatment such as methods based on variational approximation can
be used. However  due to the extreme nonlinearity and the CF setting  this kind of treatment is
non-trivial. Besides  with CDL [23] and CTR [20] as our primary baselines  it would be fairer to use
maximum a posteriori (MAP) estimates  which is what CDL and CTR do.
End-to-end joint learning: Maximization of the posterior probability is equivalent to maximizing
the joint log-likelihood of {ui}  {vj}  W+  {θj}  {γj}  {e(j)
t }  and R
given λu  λv  λw  and λs:

(cid:48)(j)
t }  {h+

(j)}  {s(j)

t }  {e

t

t

L = log p(DRAE|λs  λw) − λu
2

(cid:107)ui(cid:107)2

2 − λv
2

(cid:107)vj − γj(cid:107)2

(Rij − uT

i vj)2

Cij
2

j

i j

(cid:88)

2 −(cid:88)

(cid:88)

i

(cid:88)

j

− λs
2

(cid:107) tanh(W1fa b({(h(j)

t

t )}t) + b1) − γj(cid:107)2
; s(j)
2 

t

t }  {e

(cid:48)(j)
t }  {h+

(j)}  and {s(j)

where log p(DRAE|λs  λw) corresponds to the prior and likelihood terms for DRAE (including
the encoding  compression  decompression  and decoding in Section 3.4) involving W+  {θj} 
{e(j)
t }. For simplicity and computational efﬁciency  we can ﬁx the
hyperparameters of beta-pooling so that Beta(a  b) peaks slightly to the left of x = 0.5 (e.g. 
a = 9.8 × 107  b = 1 × 108)  which leads to γj = tanh(θj) (a treatment for the more general case
with learnable a or b is provided in the supplementary materials). Further  if λs approaches inﬁnity 
the terms with λs in log p(DRAE|λs  λw) will vanish and γj will become tanh(W1(h(j)
)+b1).
Figure 1(right) shows the graphical model of a degenerated CRAE when λs approaches positive
inﬁnity and b > a (with very large a and b). Learning this degenerated version of CRAE is equivalent
to jointly training a wildcard denoising RRN and an encoding RRN coupled with the rating matrix. If
λv (cid:28) 1  CRAE will further degenerate to a two-step model where the representation θj learned by
the DRAE is directly used for CF. On the contrary if λv (cid:29) 1  the decoder RRN essentially vanishes.
Both extreme cases can greatly degrade the predictive performance  as shown in the experiments.
Robust nonlinearity on distributions: Different from [23  22]  nonlinear transformation is per-
formed after adding the noise with precision λs (e.g. a(j)
in Equation (1)). In this case  the input of
the nonlinear transformation is a distribution rather than a deterministic value  making the nonlinearity
more robust than in [23  22] and leading to more efﬁcient and direct learning algorithms than CDL.
Consider a univariate Gaussian distribution N (x|µ  λ−1
s ) and the sigmoid function σ(x) =
1+exp(−x)  the expectation (see Section 6 of the supplementary materials for details):

  s(j)
Tj

Tj

1

t

E(x) =

N (x|µ  λ−1

s )σ(x)dx = σ(κ(λs)µ) 

(6)

(cid:90)

Equation (6) holds because the convolution of a sigmoid function with a Gaussian distribution can be
approximated by another sigmoid function. Similarly  we can approximate σ(x)2 with σ(ρ1(x + ρ0)) 

5

√
where ρ1 = 4 − 2

(cid:90)

√
2 and ρ0 = − log(
s ) ◦ Φ(ξρ1(x + ρ0))dx − E(x)2 = σ(
−1

2 + 1). Hence the variance

D(x) ≈

N (x|µ  λ

ρ1(µ + ρ0)
−1
s )1/2

(1 + ξ2ρ2

1λ

) − E(x)2 ≈ λ

−1
s  

(7)

N (σ(hf
≈ N (σ(κ(λs)h

to approximate D(x) for computational efﬁciency. Using Equation (6) and (7) 

where we use λ−1
the Gaussian distribution in Equation (2) can be computed as:
t−1) (cid:12) σ(at−1)  λ−1

t−1) (cid:12) st−1 + σ(hi

s

s IKW )

f

t−1) (cid:12) st−1 + σ(κ(λs)h

i

t−1) (cid:12) σ(κ(λs)at−1)  λ−1

s IKW ) 

(8)
where the superscript (j) is dropped. We use overlines (e.g.  at−1 = Yext−1 + Weht−1 + be) to
denote the mean of the distribution from which a hidden variable is drawn. By applying Equation (8)
recursively  we can compute st for any t. Similar approximation is used for tanh(x) in Equation (3)
since tanh(x) = 2σ(2x) − 1. This way the feedforward computation of DRAE would be seamlessly
chained together  leading to more efﬁcient learning algorithms than the layer-wise algorithms in
[23  22] (see Section 6 of the supplementary materials for more details).
Learning parameters: To learn ui and vj  block coordinate ascent can be used. Given the current
W+  we can compute γ as γ = tanh(W1fa b({(h(j)
t )}t) + b1) and get the following update
; s(j)
rules:

t

ui ← (VCiVT + λuIK)−1VCiRi
vj ← (UCiUT + λvIK)−1(UCjRj + λv tanh(W1fa b({(h(j)

t

t )}t) + b1)T ) 
; s(j)

i=1  V = (vj)J

j=1  Ci = diag(Ci1  . . .   CiJ ) is a diagonal matrix  and Ri =

where U = (ui)I
(Ri1  . . .   RiJ )T is a column vector containing all the ratings of user i.
Given U and V  W+ can be learned using the back-propagation algorithm according to Equation
(6)-(8) and the generative process in Section 3.4. Alternating the update of U  V  and W+ gives a
local optimum of L . After U and V are learned  we can predict the ratings as Rij = uT

i vj.

4 Experiments

In this section  we report some experiments on real-world datasets from different domains to evaluate
the capabilities of recommendation and automatic generation of missing sequences.

4.1 Datasets

We use two datasets from different real-world domains. CiteULike is from [20] with 5 551 users and
16 980 items (articles with text). Netﬂix consists of 407 261 users  9 228 movies  and 15 348 808
ratings after removing users with less than 3 positive ratings (following [23]  ratings larger than 3 are
regarded as positive ratings). Please see Section 7 of the supplementary materials for details.

4.2 Evaluation Schemes

Recommendation: For the recommendation task  similar to [21  23]  P items associated with each
user are randomly selected to form the training set and the rest is used as the test set. We evaluate the
models when the ratings are in different degrees of density (P ∈ {1  2  . . .   5}). For each value of P  
we repeat the evaluation ﬁve times with different training sets and report the average performance.
Following [20  21]  we use recall as the performance measure since the ratings are in the form of
implicit feedback [9  12]. Speciﬁcally  a zero entry may be due to the fact that the user is not interested
in the item  or that the user is not aware of its existence. Thus precision is not a suitable performance
measure. We sort the predicted ratings of the candidate items and recommend the top M items for
the target user. The recall@M for each user is then deﬁned as:

recall@M =

# items that the user likes among the top M

# items that the user likes

.

The average recall over all users is reported.

6

Figure 2: Performance comparison of CRAE  CDL  CTR  DeepMusic  CMF  and SVDFeature based
on recall@M for datasets CiteULike and Netﬂix. P is varied from 1 to 5 in the ﬁrst two ﬁgures.
We also use another evaluation metric  mean average precision (mAP)  in the experiments. Exactly
the same as [10]  the cutoff point is set at 500 for each user.
Sequence generation on the ﬂy: For the sequence generation task  we set P = 5. In terms of
content information (e.g.  movie plots)  we randomly select 80% of the items to include their content
in the training set. The trained models are then used to predict (generate) the content sequences for
the other 20% items. The BLEU score [11] is used to evaluate the quality of generation. To compute
the BLEU score in CiteULike we use the titles as training sentences (sequences). Both the titles
and sentences in the abstracts of the articles (items) are used as reference sentences. For Netﬂix  the
ﬁrst sentences of the plots are used as training sentences. The movie names and sentences in the
plots are used as reference sentences. A higher BLEU score indicates higher quality of sequence
generation. Since CDL  CTR  and PMF cannot generate sequences directly  a nearest neighborhood
based approach is used with the resulting vj. Note that this task is extremely difﬁcult because the
sequences of the test set are unknown during both the training and testing phases. For this reason 
this task is impossible for existing machine translation models like [18  3].

4.3 Baselines and Experimental Settings

The models for comparison are listed as follows:

information by simultaneously factorizing multiple matrices.

paper we use the bag-of-words as raw features to feed into SVDFeature.

• CMF: Collective Matrix Factorization [17] is a model incorporating different sources of
• SVDFeature: SVDFeature [2] is a model for feature-based collaborative ﬁltering. In this
• DeepMusic: DeepMusic [10] is a feedforward model for music recommendation mentioned
• CTR: Collaborative Topic Regression [20] is a model performing topic modeling and
• CDL: Collaborative Deep Learning (CDL) [23] is proposed as a probabilistic feedforward
• CRAE: Collaborative Recurrent Autoencoder is our proposed recurrent model. It jointly

collaborative ﬁltering simultaneously as mentioned in the previous section.

in Section 1. We use the best performing variant as our baseline.

model for joint learning of a probabilistic SDAE [19] and CF.

performs collaborative ﬁltering and learns the generation of content (sequences).

In the experiments  we use 5-fold cross validation to ﬁnd the optimal hyperparameters for CRAE and
the baselines. For CRAE  we set α = 1  β = 0.01  K = 50  KW = 100. The wildcard denoising
rate is set to 0.4. See Section 5.1 of the supplementary materials for details.

4.4 Quantitative Comparison

Recommendation: The ﬁrst two plots of Figure 2 show the recall@M for the two datasets when P
is varied from 1 to 5. As we can see  CTR outperforms the other baselines except for CDL. Note that
as previously mentioned  in both datasets DeepMusic suffers badly from overﬁtting when the rating
matrix is extremely sparse (P = 1) and achieves comparable performance with CTR when the rating
matrix is dense (P = 5). CDL as the strongest baseline consistently outperforms other baselines.
By jointly learning the order-aware generation of content (sequences) and performing collaborative
ﬁltering  CRAE is able to outperform all the baselines by a margin of 0.7% ∼ 1.9% (a relative boost
of 2.0% ∼ 16.7%) in CiteULike and 3.5% ∼ 6.0% (a relative boost of 5.7% ∼ 22.5%) in Netﬂix.
Note that since the standard deviation is minimal (3.38 × 10−5 ∼ 2.56 × 10−3)  it is not included in
the ﬁgures and tables to avoid clutter.
The last two plots of Figure 2 show the recall@M for CiteULike and Netﬂix when M varies from 50
to 300 and P = 1. As shown in the plots  the performance of DeepMusic  CMF  and SVDFeature is

7

123450.10.150.20.250.30.35PRecall CRAECDLCTRDeepMusicCMFSVDFeature123450.150.20.250.30.350.40.450.50.550.60.65PRecall CRAECDLCTRDeepMusicCMFSVDFeature501001502002503000.020.030.040.050.060.070.080.090.10.110.12MRecall CRAECDLCTRDeepMusicCMFSVDFeature501001502002503000.050.10.150.20.250.3MRecall CRAECDLCTRDeepMusicCMFSVDFeature(a)

(b)

(c)

(d)

(e)

Figure 3: The shape of the beta distribution for different a and b (corresponding to Table 1).

(f)

(g)

(h)

Table 1: Recall@300 for beta-pooling with different hyperparameters

a
b

Recall

31112
40000
12.17

1
1

1
10

311
400
12.54
10.72
Table 2: mAP for two datasets

0.4
0.4
11.08

10.48

11.62

10
1

CiteULike
Netﬂix

CRAE
0.0123
0.0301

DeepMusic

CDL
0.0091
0.0275

CMF
0.0061
0.0144
Table 3: BLEU score for two datasets

CTR
0.0071
0.0211

0.0058
0.0156

400
311
12.71

40000
31112
12.22

SVDFeature

0.0056
0.0173

CiteULike
Netﬂix

CRAE
46.60
48.69

CDL
21.14
6.90

CTR
31.47
17.17

PMF
17.85
11.74

similar in this setting. Again CRAE is able to outperform the baselines by a large margin and the
margin gets larger with the increase of M.
As shown in Figure 3 and Table 1  we also investigate the effect of a and b in beta-pooling and ﬁnd
that in DRAE: (1) temporal average pooling performs poorly (a = b = 1); (2) most information
concentrates near the bottleneck; (3) the right of the bottleneck contains more information than the
left. Please see Section 4 of the supplementary materials for more details.
As another evaluation metric  Table 2 compares different models based on mAP. As we can see 
compared with CDL  CRAE can provide a relative boost of 35% and 10% for CiteULike and
Netﬂix  respectively. Besides quantitative comparison  qualitative comparison of CRAE and CDL is
provided in Section 2 of the supplementary materials. In terms of time cost  CDL needs 200 epochs
(40s/epoch) while CRAE needs about 80 epochs (150s/epoch) for optimal performance.
Sequence generation on the ﬂy: To evaluate the ability of sequence generation  we compute the
BLEU score of the sequences (titles for CiteULike and plots for Netﬂix) generated by different models.
As mentioned in Section 4.2  this task is impossible for existing machine translation models like
[18  3] due to the lack of source sequences. As we can see in Table 3  CRAE achieves a BLEU
score of 46.60 for CiteULike and 48.69 for Netﬂix  which is much higher than CDL  CTR and PMF.
Incorporating the content information when learning user and item latent vectors  CTR is able to
outperform other baselines and CRAE can further boost the BLEU score by sophisticatedly and jointly
modeling the generation of sequences and ratings. Note that although CDL is able to outperform
other baselines in the recommendation task  it performs poorly when generating sequences on the ﬂy 
which demonstrates the importance of modeling each sequence recurrently as a whole rather than as
separate words.

5 Conclusions and Future Work

We develop a collaborative recurrent autoencoder which can sophisticatedly model the generation of
item sequences while extracting the implicit relationship between items (and users). We design a new
pooling scheme for pooling variable-length sequences and propose a wildcard denoising scheme to
effectively avoid overﬁtting. To the best of our knowledge  CRAE is the ﬁrst model to bridge the
gap between RNN and CF. Extensive experiments show that CRAE can signiﬁcantly outperform the
state-of-the-art methods on both the recommendation and sequence generation tasks.
With its Bayesian nature  CRAE can easily be generalized to seamlessly incorporate auxiliary
information (e.g.  the citation network for CiteULike and the co-director network for Netﬂix) for
further accuracy boost. Moreover  multiple Bayesian recurrent layers may be stacked together to
increase its representation power. Besides making recommendations and guessing sequences on
the ﬂy  the wildcard denoising recurrent autoencoder also has potential to solve other challenging
problems such as recovering the blurred words in ancient documents.

8

00.5105010015020025000.51051015202500.51024681000.5100.511.5200.5105101500.51024681000.51051015202500.51050100150200250References
[1] Y. Bengio  I. J. Goodfellow  and A. Courville. Deep learning. Book in preparation for MIT Press  2015.

[2] T. Chen  W. Zhang  Q. Lu  K. Chen  Z. Zheng  and Y. Yu. SVDFeature: a toolkit for feature-based

collaborative ﬁltering. JMLR  13:3619–3622  2012.

[3] K. Cho  B. van Merrienboer  C. Gulcehre  D. Bahdanau  F. Bougares  H. Schwenk  and Y. Bengio. Learning
phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP  2014.

[4] J. Chung  K. Kastner  L. Dinh  K. Goel  A. C. Courville  and Y. Bengio. A recurrent latent variable model

for sequential data. In NIPS  2015.

[5] O. Fabius and J. R. van Amersfoort. Variational recurrent auto-encoders. arXiv preprint arXiv:1412.6581 

2014.

[6] K. Georgiev and P. Nakov. A non-iid framework for collaborative ﬁltering with restricted Boltzmann

machines. In ICML  2013.

[7] B. Hidasi  A. Karatzoglou  L. Baltrunas  and D. Tikk. Session-based recommendations with recurrent

neural networks. arXiv preprint arXiv:1511.06939  2015.

[8] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):1735–1780  1997.

[9] Y. Hu  Y. Koren  and C. Volinsky. Collaborative ﬁltering for implicit feedback datasets. In ICDM  2008.

[10] A. V. D. Oord  S. Dieleman  and B. Schrauwen. Deep content-based music recommendation. In NIPS 

2013.

[11] K. Papineni  S. Roukos  T. Ward  and W.-J. Zhu. BLEU: a method for automatic evaluation of machine

translation. In ACL  2002.

[12] S. Rendle  C. Freudenthaler  Z. Gantner  and L. Schmidt-Thieme. BPR: Bayesian personalized ranking

from implicit feedback. In UAI  2009.

[13] F. Ricci  L. Rokach  and B. Shapira. Introduction to Recommender Systems Handbook. Springer  2011.

[14] T. N. Sainath  B. Kingsbury  V. Sindhwani  E. Arisoy  and B. Ramabhadran. Low-rank matrix factorization

for deep neural network training with high-dimensional output targets. In ICASSP  2013.

[15] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In NIPS  2007.

[16] R. Salakhutdinov  A. Mnih  and G. E. Hinton. Restricted Boltzmann machines for collaborative ﬁltering.

In ICML  2007.

[17] A. P. Singh and G. J. Gordon. Relational learning via collective matrix factorization. In KDD  2008.

[18] I. Sutskever  O. Vinyals  and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS 

2014.

[19] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. JMLR  11:3371–3408 
2010.

[20] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientiﬁc articles. In KDD 

2011.

[21] H. Wang  B. Chen  and W.-J. Li. Collaborative topic regression with social regularization for tag recom-

mendation. In IJCAI  2013.

[22] H. Wang  X. Shi  and D. Yeung. Relational stacked denoising autoencoder for tag recommendation. In

AAAI  2015.

[23] H. Wang  N. Wang  and D. Yeung. Collaborative deep learning for recommender systems. In KDD  2015.

[24] H. Wang and D. Yeung. Towards Bayesian deep learning: A framework and some existing methods. TKDE 

2016  to appear.

[25] X. Wang and Y. Wang. Improving content-based and hybrid music recommendation using deep learning.

In ACM MM  2014.

9

,Jason Lee
Yuekai Sun
Jonathan Taylor
Tian Lin
Jian Li
Wei Chen
Hao Wang
Xingjian SHI
Dit-Yan Yeung
Maxime Bucher
Tuan-Hung VU
Matthieu Cord
Patrick Pérez