2009,Which graphical models are difficult to learn?,We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task  their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples  we show that low-complexity  algorithms systematically fail when the Markov random field  develops long-range correlations. More precisely  this phenomenon  appears to be related to the Ising model phase transition  (although it does not coincide with it).,Which graphical models are difﬁcult to learn?

Department of Electrical Engineering

Department of Electrical Engineering and

Andrea Montanari

Department of Statistics

Stanford University

Jos´e Bento

Stanford University

jbento@stanford.edu

montanari@stanford.edu

Abstract

We consider the problem of learning the structure of Ising models (pairwise bi-
nary Markov random ﬁelds) from i.i.d. samples. While several methods have
been proposed to accomplish this task  their relative merits and limitations remain
somewhat obscure. By analyzing a number of concrete examples  we show that
low-complexity algorithms systematically fail when the Markov random ﬁeld de-
velops long-range correlations. More precisely  this phenomenon appears to be
related to the Ising model phase transition (although it does not coincide with it).

1

Introduction and main results

Given a graph G = (V = [p]  E)  and a positive parameter θ > 0 the ferromagnetic Ising model on
G is the pairwise Markov random ﬁeld

µG θ(x) =

1

ZG θ Y(i j)∈E

eθxixj

(1)

over binary variables x = (x1  x2  . . .   xp). Apart from being one of the most studied models in
statistical mechanics  the Ising model is a prototypical undirected graphical model  with applications
in computer vision  clustering and spatial statistics. Its obvious generalization to edge-dependent
parameters θij  (i  j) ∈ E is of interest as well  and will be introduced in Section 1.2.2. (Let us
stress that we follow the statistical mechanics convention of calling (1) an Ising model for any graph
G.)
In this paper we study the following structural learning problem: Given n i.i.d. samples x(1) 
x(2) . . .   x(n) with distribution µG θ(· )  reconstruct the graph G. For the sake of simplicity  we
assume that the parameter θ is known  and that G has no double edges (it is a ‘simple’ graph).
The graph learning problem is solvable with unbounded sample complexity  and computational re-
sources [1]. The question we address is: for which classes of graphs and values of the parameter θ is
the problem solvable under appropriate complexity constraints? More precisely  given an algorithm
Alg  a graph G  a value θ of the model parameter  and a small δ > 0  the sample complexity is
deﬁned as

nAlg(G  θ) ≡ infnn ∈ N : Pn G θ{Alg(x(1)  . . .   x(n)) = G} ≥ 1 − δo  

(2)

where Pn G θ denotes probability with respect to n i.i.d. samples with distribution µG θ. Further 
we let χAlg(G  θ) denote the number of operations of the algorithm Alg  when run on nAlg(G  θ)
samples.1

1For the algorithms analyzed in this paper  the behavior of nAlg and χAlg does not change signiﬁcantly if we

require only ‘approximate’ reconstruction (e.g. in graph distance).

1

The general problem is therefore to characterize the functions nAlg(G  θ) and χAlg(G  θ)  in par-
ticular for an optimal choice of the algorithm. General bounds on nAlg(G  θ) have been given in
[2  3]  under the assumption of unbounded computational resources. A general charactrization of
how well low complexity algorithms can perform is therefore lacking. Although we cannot prove
such a general characterization  in this paper we estimate nAlg and χAlg for a number of graph mod-
els  as a function of θ  and unveil a fascinating universal pattern: when the model (1) develops long
range correlations  low-complexity algorithms fail. Under the Ising model  the variables {xi}i∈V
become strongly correlated for θ large. For a large class of graphs with degree bounded by ∆  this
phenomenon corresponds to a phase transition beyond some critical value of θ uniformly bounded in
p  with typically θcrit ≤ const./∆. In the examples discussed below  the failure of low-complexity
algorithms appears to be related to this phase transition (although it does not coincide with it).

1.1 A toy example: the thresholding algorithm

In order to illustrate the interplay between graph structure  sample complexity and interaction
strength θ  it is instructive to consider a warmup example. The thresholding algorithm reconstructs
G by thresholding the empirical correlations

1
n

nXℓ=1

bCij ≡

x(ℓ)
i x(ℓ)

j

for i  j ∈ V .

(3)

THRESHOLDING( samples {x(ℓ)}  threshold τ )
1: Compute the empirical correlations {bCij}(i j)∈V ×V ;
2:
3:

For each (i  j) ∈ V × V
If bCij ≥ τ   set (i  j) ∈ E;

We will denote this algorithm by Thr(τ ). Notice that its complexity is dominated by the computation
of the empirical correlations  i.e. χThr(τ ) = O(p2n). The sample complexity nThr(τ ) can be bounded
for speciﬁc classes of graphs as follows (the proofs are straightforward and omitted from this paper).
Theorem 1.1. If G has maximum degree ∆ > 1 and if θ < atanh(1/(2∆)) then there exists
τ = τ (θ) such that

nThr(τ )(G  θ) ≤

(tanh θ − 1

2∆ )2

8

log

2p
δ

.

(4)

Further  the choice τ (θ) = (tanh θ + (1/2∆))/2 achieves this bound.
Theorem 1.2. There exists a numerical constant K such that the following is true. If ∆ > 3 and
the
θ > K/∆  there are graphs of bounded degree ∆ such that for any τ   nThr(τ ) = ∞  i.e.
thresholding algorithm always fails with high probability.

These results conﬁrm the idea that the failure of low-complexity algorithms is related to long-range
correlations in the underlying graphical model. If the graph G is a tree  then correlations between far
apart variables xi  xj decay exponentially with the distance between vertices i  j. The same happens
on bounded-degree graphs if θ ≤ const./∆. However  for θ > const./∆  there exists families of
bounded degree graphs with long-range correlations.

1.2 More sophisticated algorithms

In this section we characterize χAlg(G  θ) and nAlg(G  θ) for more advanced algorithms. We again
obtain very distinct behaviors of these algorithms depending on long range correlations. Due to
space limitations  we focus on two type of algorithms and only outline the proof of our most chal-
lenging result  namely Theorem 1.6.
In the following we denote by ∂i the neighborhood of a node i ∈ G (i /∈ ∂i)  and assume the degree
to be bounded: |∂i| ≤ ∆.
1.2.1 Local Independence Test

A recurring approach to structural learning consists in exploiting the conditional independence struc-
ture encoded by the graph [1  4  5  6].

2

Let us consider  to be deﬁnite  the approach of [4]  specializing it to the model (1). Fix a vertex r 
whose neighborhood we want to reconstruct  and consider the conditional distribution of xr given its
neighbors2: µG θ(xr|x∂r). Any change of xi  i ∈ ∂r  produces a change in this distribution which
is bounded away from 0. Let U be a candidate neighborhood  and assume U ⊆ ∂r. Then changing
the value of xj  j ∈ U will produce a noticeable change in the marginal of Xr  even if we condition
on the remaining values in U and in any W   |W| ≤ ∆. On the other hand  if U * ∂r  then it is
possible to ﬁnd W (with |W| ≤ ∆) and a node i ∈ U such that  changing its value after ﬁxing all
other values in U ∪ W will produce no noticeable change in the conditional marginal. (Just choose
i ∈ U\∂r and W = ∂r\U). This procedure allows us to distinguish subsets of ∂r from other sets
of vertices  thus motivating the following algorithm.

LOCAL INDEPENDENCE TEST( samples {x(ℓ)}  thresholds (ǫ  γ) )
1:
2:

Select a node r ∈ V ;
Set as its neighborhood the largest candidate neighbor U of
size at most ∆ for which the score function SCORE(U ) > ǫ/2;

3: Repeat for all nodes r ∈ V ;
The score function SCORE(· ) depends on ({x(ℓ)}  ∆  γ) and is deﬁned as follows 

min
W j

max

xi xW  xU  xj|bPn G θ{Xi = xi|X W = xW   X U = xU}−

bPn G θ{Xi = xi|X W = xW   X U \j = xU \j  Xj = xj}| .
In the minimum  |W| ≤ ∆ and j ∈ U. In the maximum  the values must be such that
bPn G θ{X W = xW   X U = xU} > γ/2 
bPn G θ{X W = xW   X U \j = xU \j  Xj = xj} > γ/2
bPn G θ is the empirical distribution calculated from the samples {x(ℓ)}. We denote this algorithm
computation of the SCORE(U ) and the computation ofbPn G θ all contribute for χInd(G  θ).
Both theorems that follow are consequences of the analysis of [4].
Theorem 1.3. Let G be a graph of bounded degree ∆ ≥ 1. For every θ there exists (ǫ  γ)  and a
numerical constant K  such that

by Ind(ǫ  γ). The search over candidate neighbors U  the search for minima and maxima in the

(5)

nInd(ǫ γ)(G  θ) ≤

100∆
ǫ2γ4 log

2p
δ

 

χInd(ǫ γ) (G  θ) ≤ K (2p)2∆+1 log p .

More speciﬁcally  one can take ǫ = 1

4 sinh(2θ)  γ = e−4∆θ 2−2∆.

This ﬁrst result implies in particular that G can be reconstructed with polynomial complexity for
any bounded ∆. However  the degree of such polynomial is pretty high and non-uniform in ∆. This
makes the above approach impractical.

A way out was proposed in [4]. The idea is to identify a set of ‘potential neighbors’ of vertex r via
thresholding:

B(r) = {i ∈ V : bCri > κ/2}  

(6)
For each node r ∈ V   we evaluate SCORE(U ) by restricting the minimum in Eq. (5) over W ⊆ B(r) 
and search only over U ⊆ B(r). We call this algorithm IndD(ǫ  γ  κ). The basic intuition here is
that Cri decreases rapidly with the graph distance between vertices r and i. As mentioned above 
this is true at small θ.
Theorem 1.4. Let G be a graph of bounded degree ∆ ≥ 1. Assume that θ < K/∆ for some small
enough constant K. Then there exists ǫ  γ  κ such that

nIndD(ǫ γ κ)(G  θ) ≤ 8(κ2 + 8∆) log
More speciﬁcally  we can take κ = tanh θ  ǫ = 1

 

χIndD(ǫ γ κ)(G  θ) ≤ K ′p∆∆ log(4/κ)
4 sinh(2θ) and γ = e−4∆θ 2−2∆.

4p
δ

α + K ′∆p2 log p .

2If a is a vector and R is a set of indices then we denote by aR the vector formed by the components of a

with index in R.

3

1.2.2 Regularized Pseudo-Likelihoods

A different approach to the learning problem consists in maximizing an appropriate empirical likeli-
hood function [7  8  9  10  13]. To control the ﬂuctuations caused by the limited number of samples 
and select sparse graphs a regularization term is often added [7  8  9  10  11  12  13].
As a speciﬁc low complexity implementation of this idea  we consider the ℓ1-regularized pseudo-
likelihood method of [7]. For each node r  the following likelihood function is considered

L(θ;{x(ℓ)}) = −

1
n

nXℓ=1

log Pn G θ(x(ℓ)

r |x(ℓ)
\r )

(7)

where x\r = xV \r = {xi : i ∈ V \ r} is the vector of all variables except xr and Pn G θ is deﬁned
from the following extension of (1) 

µG θ(x) =

1

ZG θ Yi j∈V

eθij xixj

(8)

where θ = {θij}i j∈V is a vector of real parameters. Model (1) corresponds to θij = 0  ∀(i  j) /∈ E
and θij = θ  ∀(i  j) ∈ E.
The function L(θ;{x(ℓ)}) depends only on θr · = {θrj  j ∈ ∂r} and is used to estimate the neigh-
borhood of each node by the following algorithm  Rlr(λ) 

REGULARIZED LOGISTIC REGRESSION( samples {x(ℓ)}  regularization (λ))
1:
2: Calculate ˆθr · = arg min
If ˆθrj > 0  set (r  j) ∈ E;

θr ·∈Rp−1{L(θr ·;{x(ℓ)}) + λ||θr ·||1};

Select a node r ∈ V ;

3:

Our ﬁrst result shows that Rlr(λ) indeed reconstructs G if θ is sufﬁciently small.
Theorem 1.5. There exists numerical constants K1  K2  K3  such that the following is true. Let G
be a graph with degree bounded by ∆ ≥ 3. If θ ≤ K1/∆  then there exist λ such that

nRlr(λ)(G  θ) ≤ K2 θ−2 ∆ log

8p2
δ

.

(9)

Further  the above holds with λ = K3 θ ∆−1/2.
This theorem is proved by noting that for θ ≤ K1/∆ correlations decay exponentially  which makes
all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold  and then computing the
probability of success as a function of n  while strenghtening the error bounds of [7].
In order to prove a converse to the above result  we need to make some assumptions on λ. Given
θ > 0  we say that λ is ‘reasonable for that value of θ if the following conditions old: (i) Rlr(λ)
is successful with probability larger than 1/2 on any star graph (a graph composed by a vertex r
connected to ∆ neighbors  plus isolated vertices); (ii) λ ≤ δ(n) for some sequence δ(n) ↓ 0.
Theorem 1.6. There exists a numerical constant K such that the following happens. If ∆ > 3 
θ > K/∆  then there exists graphs G of degree bounded by ∆ such that for all reasonable λ 
nRlr(λ)(G) = ∞  i.e. regularized logistic regression fails with high probability.
The graphs for which regularized logistic regression fails are not contrived examples. Indeed we will
prove that the claim in the last theorem holds with high probability when G is a uniformly random
graph of regular degree ∆.
The proof Theorem 1.6 is based on showing that an appropriate incoherence condition is necessary
for Rlr to successfully reconstruct G. The analogous result was proven in [14] for model selection
using the Lasso. In this paper we show that such a condition is also necessary when the underlying
model is an Ising model. Notice that  given the graph G  checking the incoherence condition is
NP-hard for general (non-ferromagnetic) Ising model  and requires signiﬁcant computational effort

4

 20

 15

λ0

 10

 5

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

θ

 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

 1.2

 1

 0.8

 0.6

Psucc

 0.4

 0.2

 0

 0

 0.2

 0.4

θ

 0.6

θcrit

 0.8

 1

Figure 1: Learning random subgraphs of a 7 × 7 (p = 49) two-dimensional grid from n = 4500
Ising models samples  using regularized logistic regression. Left: success probability as a function
of the model parameter θ and of the regularization parameter λ0 (darker corresponds to highest
probability). Right:
the same data plotted for several choices of λ versus θ. The vertical line
corresponds to the model critical temperature. The thick line is an envelope of the curves obtained
for different λ  and should correspond to optimal regularization.

even in the ferromagnetic case. Hence the incoherence condition does not provide  by itself  a clear
picture of which graph structure are difﬁcult to learn. We will instead show how to evaluate it on
speciﬁc graph families.
Under the restriction λ → 0 the solutions given by Rlr converge to θ∗ with n [7]. Thus  for large
n we can expand L around θ∗ to second order in (θ − θ∗). When we add the regularization term
to L we obtain a quadratic model analogous the Lasso plus the error term due to the quadratic
approximation. It is thus not surprising that  when λ → 0 the incoherence condition introduced for
the Lasso in [14] is also relevant for the Ising model.

2 Numerical experiments

In order to explore the practical relevance of the above results  we carried out extensive numerical
simulations using the regularized logistic regression algorithm Rlr(λ). Among other learning algo-
rithms  Rlr(λ) strikes a good balance of complexity and performance. Samples from the Ising model
(1) where generated using Gibbs sampling (a.k.a. Glauber dynamics). Mixing time can be very large
for θ ≥ θcrit  and was estimated using the time required for the overall bias to change sign (this is a
quite conservative estimate at low temperature). Generating the samples {x(ℓ)} was indeed the bulk
of our computational effort and took about 50 days CPU time on Pentium Dual Core processors (we
show here only part of these data). Notice that Rlr(λ) had been tested in [7] only on tree graphs G 
or in the weakly coupled regime θ < θcrit. In these cases sampling from the Ising model is easy  but
structural learning is also intrinsically easier.
Figure reports the success probability of Rlr(λ) when applied to random subgraphs of a 7 × 7
two-dimensional grid. Each such graphs was obtained by removing each edge independently with
probability ρ = 0.3. Success probability was estimated by applying Rlr(λ) to each vertex of 8
graphs (thus averaging over 392 runs of Rlr(λ))  using n = 4500 samples. We scaled the regular-
ization parameter as λ = 2λ0θ(log p/n)1/2 (this choice is motivated by the algorithm analysis and
is empirically the most satisfactory)  and searched over λ0.
The data clearly illustrate the phenomenon discussed. Despite the large number of samples
n ≫ log p  when θ crosses a threshold  the algorithm starts performing poorly irrespective of λ.
Intriguingly  this threshold is not far from the critical point of the Ising model on a randomly diluted
grid θcrit(ρ = 0.3) ≈ 0.7 [15  16].

5

 1.2

 1

 0.8

 0.6
Psucc
 0.4

 0.2

 0

 0

θ = 0.35  0.40

θ = 0.25

θ = 0.20

θ = 0.10

θ = 0.45

θ = 0.50

θ = 0.65  0.60  0.55

 2000

 4000

 6000

 8000

 10000

n

 1.2

 1

 0.8

 0.6
Psucc
 0.4

 0.2

 0

 0

 0.1

 0.2

 0.3

θthr

 0.5

 0.6

 0.7

 0.8

 0.4
θ

Figure 2: Learning uniformly random graphs of degree 4 from Ising models samples  using Rlr.
Left: success probability as a function of the number of samples n for several values of θ. Right:
the same data plotted for several choices of λ versus θ as in Fig. 1  right panel.

Figure 2 presents similar data when G is a uniformly random graph of degree ∆ = 4  over p = 50
vertices. The evolution of the success probability with n clearly shows a dichotomy. When θ is
below a threshold  a small number of samples is sufﬁcient to reconstruct G with high probability.
Above the threshold even n = 104 samples are to few. In this case we can predict the threshold
analytically  cf. Lemma 3.3 below  and get θthr(∆ = 4) ≈ 0.4203  which compares favorably with
the data.

3 Proofs

In order to prove Theorem 1.6  we need a few auxiliary results. It is convenient to introduce some
notations. If M is a matrix and R  P are index sets then MR P denotes the submatrix with row
indices in R and column indices in P . As above  we let r be the vertex whose neighborhood we are
trying to reconstruct and deﬁne S = ∂r  Sc = V \ ∂r ∪ r. Since the cost function L(θ;{x(ℓ)}) +
λ||θ||1 only depend on θ through its components θr · = {θrj}  we will hereafter neglect all the other
parameters and write θ as a shorthand of θr ·.
Let ˆz∗ be a subgradient of ||θ||1 evaluated at the true parameters values  θ∗ = {θrj : θij = 0  ∀j /∈
n be the parameter estimate returned by Rlr(λ) when the number
∂r  θrj = θ  ∀j ∈ ∂r}. Let ˆθ
of samples is n. Note that  since we assumed θ∗ ≥ 0  ˆz∗
S = 1. Deﬁne Qn(θ  ;{x(ℓ)}) to be the
Hessian of L(θ;{x(ℓ)}) and Q(θ) = limn→∞ Qn(θ  ;{x(ℓ)}). By the law of large numbers Q(θ) is
the Hessian of EG θ log PG θ(Xr|X\r) where EG θ is the expectation with respect to (8) and X is a
random variable distributed according to (8). We will denote the maximum and minimum eigenvalue
of a symmetric matrix M by σmax(M ) and σmin(M ) respectively.
We will omit arguments whenever clear from the context. Any quantity evaluated at the true pa-
rameter values will be represented with a ∗  e.g. Q∗ = Q(θ∗). Quantities under a ∧ depend on n.
Throughout this section G is a graph of maximum degree ∆.

3.1 Proof of Theorem 1.6

Our ﬁrst auxiliary results establishes that  if λ is small  then ||Q∗
condition for the failure of Rlr(λ).
Lemma 3.1. Assume [Q∗

−1 ˆz∗

ScSQ∗
SS

−1 ˆz∗

S||∞ > 1 is a sufﬁcient

S]i ≥ 1 + ǫ for some ǫ > 0 and some row i ∈ V   σmin(Q∗
ScSQ∗
SS
minǫ/29∆4. Then the success probability of Rlr(λ) is upper bounded as

SS) ≥

Cmin > 0  and λ <pC 3

where δA = (C 2

Psucc ≤ 4∆2e−nδ2
min/100∆2)ǫ and δB = (Cmin/8∆)ǫ.

A + 2∆ e−nλ2δ2

B

6

(10)

The next Lemma implies that  for λ to be ‘reasonable’ (in the sense introduced in Section 1.2.2) 
nλ2 must be unbounded.
Lemma 3.2. There exist M = M (K  θ) > 0 for θ > 0 such that the following is true: If G is the
graph with only one edge between nodes r and i and nλ2 ≤ K  then
Psucc ≤ e−M (K θ)p + e−n(1−tanh θ)2/32 .

(11)

−1 ˆz∗

ScSQ∗
SS

S||∞ ≤ 1 is violated with high
Finally  our key result shows that the condition ||Q∗
probability for large random graphs. The proof of this result relies on a local weak convergence
result for ferromagnetic Ising models on random graphs proved in [17].
Lemma 3.3. Let G be a uniformly random regular graph of degree ∆ > 3  and ǫ > 0 be sufﬁciently
small. Then  there exists θthr(∆  ǫ) such that  for θ > θthr(∆  ǫ)  ||Q∗
S||∞ ≥ 1 + ǫ with
probability converging to 1 as p → ∞.
Furthermore  for large ∆  θthr(∆  0+) = ˜θ ∆−1(1 + o(1)). The constant ˜θ is given by ˜θ =
tanh ¯h)/¯h and ¯h is the unique positive solution of ¯h tanh ¯h = (1 − tanh2 ¯h)2. Finally  there exist
Cmin > 0 dependent only on ∆ and θ such that σmin(Q∗
SS) ≥ Cmin with probability converging to
1 as p → ∞.
The proofs of Lemmas 3.1 and 3.3 are sketched in the next subsection. Lemma 3.2 is more straight-
forward and we omit its proof for space reasons.

ScSQ∗
SS

−1 ˆz∗

Proof. (Theorem 1.6) Fix ∆ > 3  θ > K/∆ (where K is a large enough constant independent of
∆)  and ǫ  Cmin > 0 and both small enough. By Lemma 3.3  for any p large enough we can choose
−11S|i > 1 + ǫ for
a ∆-regular graph Gp = (V = [p]  Ep) and a vertex r ∈ V such that |Q∗
some i ∈ V \ r.
By Theorem 1 in [4] we can assume  without loss of generality n > K ′∆ log p for some small
constant K ′. Further by Lemma 3.2  nλ2 ≥ F (p) for some F (p) ↑ ∞ as p → ∞ and the condition
of Lemma 3.1 on λ is satisﬁed since by the ”reasonable” assumption λ → 0 with n. Using these
results in Eq. (10) of Lemma 3.1 we get the following upper bound on the success probability

ScSQ∗
SS

Psucc(Gp) ≤ 4∆2p−δ2

AK ′∆ + 2∆ e−nF (p)δ2

B .

In particular Psucc(Gp) → 0 as p → ∞.
3.2 Proofs of auxiliary lemmas

(12)

Proof. (Lemma 3.1) We will show that under the assumptions of the lemma and if ˆθ = (ˆθS  ˆθSC ) =
(ˆθS  0) then the probability that the i component of any subgradient of L(θ;{x(ℓ)})+λ||θ||1 vanishes
for any ˆθS > 0 (component wise) is upper bounded as in Eq. (10). To simplify notation we will omit
{x(ℓ)} in all the expression derived from L.
Let ˆz be a subgradient of ||θ|| at ˆθ and assume ∇L(ˆθ) + λˆz = 0. An application of the mean value
theorem yields
(13)
(j) a point in the line
where W n = −∇L(θ∗) and [Rn]j = [∇2L(¯θ
from ˆθ to θ∗. Notice that by deﬁnition ∇2L(θ∗) = Qn∗ = Qn(θ∗). To simplify notation we will
omit the ∗ in all Qn∗. All Qn in this proof are thus evaluated at θ∗.
Breaking this expression into its S and Sc components and since ˆθSC = θ∗
ˆθS − θ∗

∇2L(θ∗)[ˆθ − θ∗] = W n − λˆz + Rn  

SC = 0 we can eliminate

j (ˆθ − θ∗) with ¯θ

)−∇2L(θ∗)]T

S from the two expressions obtained and write
S − Rn

SC ] − Qn

SC − Rn

SS)−1[W n

SC S(Qn

SS)−1 ˆzS = λˆzSC .

S] + λQn

SC S(Qn

[W n

(14)

(j)

Now notice that Qn

SC S(Qn

SS)−1 = T1 + T2 + T3 + T4 where

T1 = Q∗
T3 = [Qn

SC S[(Qn
SC S − Q∗

SS)−1 − (Q∗
SC S][(Qn

SS)−1]  
SS)−1 − (Q∗

SS)−1]  

T2 = [Qn
T4 = Q∗

SC S]Q∗
SC S − Q∗
−1 .
SC SQ∗

SS

SS

−1  

7

(15)

S /λ||∞ < ξC}  

We will assume that the samples {x(ℓ)} are such that the following event holds

E ≡ {||Qn

SS − Q∗

SS||∞ < ξA ||Qn

SC S − Q∗

SC S||∞ < ξB ||W n

SC SQ∗

SS) > σmin(Q∗

minǫ/(16∆)  ξB ≡ Cminǫ/(8√∆) and ξC ≡ Cminǫ/(8∆). Since EG θ(Qn) = Q∗
where ξA ≡ C 2
and EG θ(W n) = 0 and noticing that both Qn and W n are sums of bounded i.i.d. random variables 
a simple application of Azuma-Hoeffding inequality upper bounds the probability of E as in (10).
From E it follows that σmin(Qn
SS) − Cmin/2 > Cmin/2. We can therefore lower
bound the absolute value of the ith component of ˆzSC by
|[Q∗
where the subscript i denotes the i-th row of a matrix.
The proof is completed by showing that the event E and the assumptions of the theorem imply that
each of last 7 terms in this expression is smaller than ǫ/8. Since |[Q∗
S| ≥ 1 + ǫ by
assumption  this implies |ˆzi| ≥ 1 + ǫ/8 > 1 which cannot be since any subgradient of the 1-norm
has components of magnitude at most 1.
The last condition on E immediately bounds all terms involving W by ǫ/8. Some straightforward
manipulations imply (See Lemma 7 from [7])

−11S]i|−||T1 i||∞−||T2 i||∞−||T3 i||∞−(cid:12)(cid:12)(cid:12)

Cmin (cid:18)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

λ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞

λ (cid:12)(cid:12)(cid:12)−(cid:12)(cid:12)(cid:12)

λ (cid:12)(cid:12)(cid:12)−

+(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

SC SQ∗

−1]T

i ˆzn

W n
S

W n
i

Rn
S

Rn
i

∆

SS

SS

λ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞(cid:19)  

||T2 i||∞ ≤

√∆
Cmin||[Qn

SC S − Q∗

SC S]i||∞  

||T1 i||∞ ≤

||T3 i||∞ ≤

∆
min||Qn
C 2
2∆
min||Qn
C 2

SS − Q∗
SS − Q∗

SS||∞  
SS||∞||[Qn

SC S − Q∗

SC S]i||∞  

and thus all will be bounded by ǫ/8 when E holds. The upper bound of Rn follows along similar
lines via an mean value theorem  and is deferred to a longer version of this paper.

Proof. (Lemma 3.3.) Let us state explicitly the local weak convergence result mentioned in Sec. 3.1.
For t ∈ N  let T(t) = (VT  ET) be the regular rooted tree of t generations and deﬁne the associated
Ising measure as
(16)

eh∗xi .

µ+
T θ(x) =

1

ZT θ Y(i j)∈ET

eθxixj Yi∈∂T(t)

T θ(x)  as p → ∞.

Here ∂T(t) is the set of leaves of T(t) and h∗ is the unique positive solution of h = (∆ −
1) atanh{tanh θ tanh h}. It can be proved using [17] and uniform continuity with respect to the
‘external ﬁeld’ that non-trivial local expectations with respect to µG θ(x) converge to local expecta-
tions with respect to µ+
More precisely  let Br(t) denote a ball of radius t around node r ∈ G (the node whose neighborhood
we are trying to reconstruct). For any ﬁxed t  the probability that Br(t) is not isomorphic to T(t)
goes to 0 as p → ∞. Let g(xBr(t)) be any function of the variables in Br(t) such that g(xBr(t)) =
g(−xBr(t)). Then almost surely over graph sequences Gp of uniformly random regular graphs with
p nodes (expectations here are taken with respect to the measures (1) and (16))
(17)

EG θ{g(X Br(t))} = ET(t) θ +{g(X T(t))} .

lim
p→∞

−1 ˆz∗

Br (t)

−1 ˆz∗

ScSQ∗
SS

)} and (Q∗

ScS)il = E{gi l(X

SS)lk = E{gl k(X
ScSQ∗
SS

S]i for t = dist(r  i) ﬁnite. We then write
The proof consists in considering [Q∗
) and
(Q∗
apply the weak convergence result (17) to these expectations. We thus reduced the calculation of
S]i to the calculation of expectations with respect to the tree measure (16). The latter
[Q∗
can be implemented explicitly through a recursive procedure  with simpliﬁcations arising thanks to
the tree symmetry and by taking t ≫ 1. The actual calculations consist in a (very) long exercise in
calculus and we omit them from this outline.
The lower bound on σmin(Q∗

)} for some functions g· ·(X

SS) is proved by a similar calculation.

Br (t)

Br (t)

Acknowledgments

This work was partially supported by a Terman fellowship  the NSF CAREER award CCF-0743978
and the NSF grant DMS-0806211 and by a Portuguese Doctoral FCT fellowship.

8

References

[1] P. Abbeel  D. Koller and A. Ng  “Learning factor graphs in polynomial time and sample com-

plexity”. Journal of Machine Learning Research.  2006  Vol. 7  1743–1788.

[2] M. Wainwright  “Information-theoretic limits on sparsity recovery in the high-dimensional and

noisy setting”  arXiv:math/0702301v2 [math.ST]  2007.

[3] N. Santhanam  M. Wainwright  “Information-theoretic limits of selecting binary graphical

models in high dimensions”  arXiv:0905.2639v1 [cs.IT]  2009.

[4] G. Bresler  E. Mossel and A. Sly  “Reconstruction of Markov Random Fields from Sam-
ples: Some Observations and Algorithms” Proceedings of the 11th international workshop 
APPROX 2008  and 12th international workshop RANDOM 2008  2008  343–356.

[5] Csiszar and Z. Talata  “Consistent estimation of the basic neighborhood structure of Markov

random ﬁelds”  The Annals of Statistics  2006  34  Vol. 1  123-145.

[6] N. Friedman  I. Nachman  and D. Peer  “Learning Bayesian network structure from massive

datasets: The sparse candidate algorithm”. In UAI  1999.

[7] P. Ravikumar  M. Wainwright and J. Lafferty  “High-Dimensional Ising Model Selection Using

l1-Regularized Logistic Regression”  arXiv:0804.4202v1 [math.ST]  2008.

[8] M.Wainwright  P. Ravikumar  and J. Lafferty  “Inferring graphical model structure using l1-

regularized pseudolikelihood“  In NIPS  2006.

[9] H. H¨oﬂing and R. Tibshirani  “Estimation of Sparse Binary Pairwise Markov Networks using

Pseudo-likelihoods”   Journal of Machine Learning Research  2009  Vol. 10  883–906.

[10] O.Banerjee  L. El Ghaoui and A. d’Aspremont  “Model Selection Through Sparse Maximum
Likelihood Estimation for Multivariate Gaussian or Binary Data”  Journal of Machine Learning
Research  March 2008  Vol. 9  485–516.

[11] M. Yuan and Y. Lin  “Model Selection and Estimation in Regression with Grouped Variables” 

J. Royal. Statist. Soc B  2006  68  Vol. 19 49–67.

[12] N. Meinshausen and P. B¨uuhlmann  “High dimensional graphs and variable selection with the

lasso”  Annals of Statistics  2006  34  Vol. 3.

[13] R. Tibshirani  “Regression shrinkage and selection via the lasso”  Journal of the Royal Statis-

tical Society  Series B  1994  Vol. 58  267–288.

[14] P. Zhao  B. Yu  “On model selection consistency of Lasso”  Journal of Machine. Learning

Research 7  25412563  2006.

[15] D. Zobin  ”Critical behavior of the bond-dilute two-dimensional Ising model“  Phys. Rev. 

1978  5  Vol. 18  2387 – 2390.

[16] M. Fisher  ”Critical Temperatures of Anisotropic Ising Lattices. II. General Upper Bounds” 

Phys. Rev. 162  Oct. 1967  Vol. 2  480–485.

[17] A. Dembo and A. Montanari  “Ising Models on Locally Tree Like Graphs”  Ann. Appl. Prob.

(2008)  to appear  arXiv:0804.4726v2 [math.PR]

9

,Christian Albers
Maren Westkott
Klaus Pawelzik
Kartik Ahuja
William Zame
Mihaela van der Schaar
Hongteng Xu
Wenlin Wang
Wei Liu
Lawrence Carin