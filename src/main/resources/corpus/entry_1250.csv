2019,Cross-Domain Transferability of Adversarial Perturbations,Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings  where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods  whether learning an instance-specific or an instance-agnostic perturbation  is the direct or indirect reliance on the original domain-specific data distribution. In this work  for the first time  we demonstrate the existence of domain-invariant adversaries  thereby showing common adversarial space among different datasets and models. To this end  we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance  an adversarial function learned on Paintings  Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier  with success rates as high as $\sim$99\% ($\ell_{\infty} \le 10$). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates  both under the white-box and black-box scenarios. Furthermore  despite being an instance-agnostic perturbation function  our attack outperforms the conventionally much stronger instance-specific attack methods.,Cross-Domain Transferability of Adversarial

Perturbations

Muzammal Naseer1 2  Salman Khan2 1  Muhammad Haris Khan2 

Fahad Shahbaz Khan2 3  Fatih Porikli1

1Australian National University  Canberra  Australia

2Inception Institute of Artiﬁcial Intelligence  Abu Dhabi  UAE

3CVL  Department of Electrical Engineering  Linköping University  Sweden

{muzammal.naseer fatih.porikli}@anu.edu.au

{salman.khan muhammad.haris fahad.khan}@inceptioniai.org

Abstract

Adversarial examples reveal the blind spots of deep neural networks (DNNs) and
represent a major concern for security-critical applications. The transferability
of adversarial examples makes real-world attacks possible in black-box settings 
where the attacker is forbidden to access the internal parameters of the model. The
underlying assumption in most adversary generation methods  whether learning
an instance-speciﬁc or an instance-agnostic perturbation  is the direct or indirect
reliance on the original domain-speciﬁc data distribution. In this work  for the ﬁrst
time  we demonstrate the existence of domain-invariant adversaries  thereby show-
ing common adversarial space among different datasets and models. To this end 
we propose a framework capable of launching highly transferable attacks that crafts
adversarial patterns to mislead networks trained on entirely different domains. For
instance  an adversarial function learned on Paintings  Cartoons or Medical images
can successfully perturb ImageNet samples to fool the classiﬁer  with success rates
as high as ∼99% ((cid:96)∞ ≤ 10). The core of our proposed adversarial function is a
generative network that is trained using a relativistic supervisory signal that enables
domain-invariant perturbations. Our approach sets the new state-of-the-art for fool-
ing rates  both under the white-box and black-box scenarios. Furthermore  despite
being an instance-agnostic perturbation function  our attack outperforms the con-
ventionally much stronger instance-speciﬁc attack methods. Code is available at:
https://github.com/Muzammal-Naseer/Cross-domain-perturbations

1

Introduction

Albeit displaying remarkable performance across a range of tasks  Deep Neural Networks (DNNs)
are highly vulnerable to adversarial examples  which are carefully crafted examples generated by
adding a certain degree of noise (a.k.a. perturbations) to the corresponding original images  typically
appearing quasi-imperceptible to humans [1]. Importantly  these adversarial examples are transferable
from one network to another  even when the other network fashions a different architecture and
possibly trained on a different subset of training data [2  3]. Transferability permits an adversarial
attack  without knowing the internals of the target network  posing serious security concerns on the
practical deployment of these models.
Adversarial perturbations are either instance-speciﬁc or instance-agnostic. The instance-speciﬁc
attacks iteratively optimize a perturbation pattern speciﬁc to an input sample (e.g.  [4  5  6  7  8  9 
10  11]). In comparison  the instance-agnostic attacks learn a universal perturbation or a function
that ﬁnds adversarial patterns on a data distribution instead of a single sample. For example  [12]

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Transferable Generative Adversarial Perturbation: We demonstrate that common adversaries
exist across different image domains and introduce a highly transferable attack approach that carefully crafts
adversarial patterns to fool classiﬁers trained on totally different domains. Our generative scheme learns to
reconstruct adversaries on paintings or comics (left) that can successfully fool natural image classiﬁers with high
fooling rates at the inference time (right).

proposed universal adversarial perturbations that can fool a model on the majority of the source
dataset images. To reduce dependency on the input data samples  [13] maximizes layer activations of
the source network while [14] extracts deluding perturbations using class impressions relying on the
source label space. To enhance the transferability of instance-agnostic approaches  recent generative
models attempt to directly craft perturbations using an adversarially trained function [15  16].
We observe that most prior works on crafting adversarial attacks suffer from two pivotal limitations
that restrict their transferability to real-world scenarios. (a) Existing attacks rely directly or indirectly
on the source (training) data  which hampers their transferability to other domains. From a practical
standpoint  source domain can be unknown  or the domain-speciﬁc data may be unavailable to the
attacker. Therefore  a true "black-box" attack must be able to fool learned models across different
target domains without ever being explicitly trained on those data domains. (b) Instance-agnostic
attacks  compared with their counterparts  are far more scalable to large datasets as they avoid
expensive per-instance iterative optimization. However  they demonstrate weaker transferability rates
than the instance-speciﬁc attacks. Consequently  the design of highly transferable instance-agnostic
attacks that also generalize across unseen domains is largely an unsolved problem.
In this work  we introduce ‘domain-agnostic’ generation of adversarial examples  with the aim of
relaxing the source data reliance assumption. In particular  we propose a ﬂexible framework capable
of launching vastly transferable adversarial attacks  e.g.  perturbations found on paintings  comics
or medical images are shown to trick natural image classiﬁers trained on ImageNet dataset with
high fooling rates. A distinguishing feature of our approach is the introduction of relativistic loss
that explicitly enforces learning of domain-invariant adversarial patterns. Our attack algorithm is
highly scalable to large-scale datasets since it learns a universal adversarial function that avoids
expensive iterative optimization from instance-speciﬁc attacks. While enjoying the efﬁcient inference
time of instance-agnostic methods  our algorithm outperforms all existing attack methods (both
instance-speciﬁc and agnostic) by a signiﬁcant margin (∼ 86.46% average increase in fooling rate
from naturally trained Inception-v3 to adversarially trained models in comparison to state-of-the-art
[10]) and sets the new state-of-the-art under both white-box and black-box settings. Figure 1 provides
an overview of our approach.

2 Related Work

Image-dependent Perturbations: Several approaches target creation of image-dependent perturba-
tions. [17] noticed that despite exhibiting impressive performance  neural networks can be fooled
through maliciously crafted perturbations that appear quasi-imperceptible to humans. Following this

2

ﬁnding  many approaches [4  5  6  7  8  9] investigate the existence of these perturbations. They either
apply gradient ascent in the pixel space or solve complex optimizations. Recently  a few methods
[18  10] propose input or gradient transformation modules to improve the transferability of adversarial
examples. A common characteristic of the aforementioned approaches is their data-dependence; the
perturbations are computed for each data-point separately in a mutually exclusive way. Further  these
approaches render inefﬁciently at inference time since they iterate on the input multiple times. In
contrast  we resort to a data-independent approach based on a generator  demonstrating improved
inference-time efﬁciency along with high transferability rates.
Universal Adversarial Perturbation: Seminal work of [12] introduces the existence of Universal
Adversarial Perturbation (UAP). It is a single noise vector which when added to a data-point can
fool a pretrained model. [12] crafts UAP in an iterative fashion utilizing target data-points that is
capable of ﬂipping their labels. Though it can generate image-agnostic UAP  the success ratio of
their attack is proportional to the number of training samples used for crafting UAP. [13] proposes
a so-called data-independent algorithm by maximizing the product of mean activations at multiple
layers given a universal perturbation as input. This method crafts a so-called data-independent
perturbation  however  the attack success ratio is not comparable to [12]. Instead  we propose a fully
distribution-agnostic approach that crafts adversarial examples directly from a learned generator  as
opposed to ﬁrst generating perturbations followed by their addition to images.
Generator-oriented Perturbations: Another branch of attacks leverage generative models to craft
adversaries. [15] learns a generator network to perturb images  however  the unbounded perturbation
magnitude in their case might render perceptible perturbations at test time. [19] trains conditional
generators to learn original data manifold and searches the latent space conditioned on the human
recognizable target class that is mis-classiﬁed by a target classier. [20] apply generative adversarial
networks to craft visually realistic perturbations and build distilled network to perform black-box
attack. Similarly  [16  14] train generators to create adversaries to launch attacks; the former uses
target data directly and the latter relies on class impressions.
A common trait of prior work is that they either rely directly (or indirectly) upon the data distribution
and/or entail access to its label space for creating adversarial examples (Table 1). In contrast  we
propose a ﬂexible  distribution-agnostic approach - inculcating relativistic loss - to craft adversarial
examples that achieves state-of-the-art results both under white-box and black-box attack settings.

Method

FFF [13]
AAA [14]
UAP [12]
GAP [16]
RHP [11]

Data Type

Pretrained-net/data
Class Impressions

ImageNet
ImageNet
ImageNet

Ours

Arbitrary (Paintings  Comics  Medical scans etc.)

Transfer Label Cross-domain
Strength Agnostic

Attack

Low

Medium

Low

Medium
Medium

High

























Table 1: A comparison of different attack methods based on their dependency on data distribution and labels.

3 Cross-Domain Transferable Perturbations

Our proposed approach is based on a generative model that is trained using an adversarial mechanism.
Assume we have an input image xs belonging to a source domain Xs ∈ (cid:82)s. We aim to train
a universal function that learns to add a perturbation pattern δ on the source domain which can
successfully fool a network trained on source Xs as well as any target domain Xt ⊂ (cid:82)t when fed
with perturbed inputs x(cid:48)
t = xt + δ. Importantly  our training is only performed on the unlabelled
source domain dataset with ns samples: {xi
s}ns
i=1 and the target domain is not used at all during
training. For brevity  in the following discussion  we will only refer the input and perturbed images
using x and x(cid:48) respectively and the domain will be clear from the context.
The proposed framework consists of a generator Gθ(x) and a discriminator Dψ(x) parameterized by
θ and ψ. In our case  we initialize discriminator with a pretrained network and the parameters ψ are
remained ﬁxed while the Gθ is learned. The output of Gθ is scaled to have a ﬁxed norm and it lies

within a bound; x(cid:48) = clip(cid:0) min(x +   max(Gθ(x)  x − ))(cid:1). The perturbed images x(cid:48) as well as

3

Figure 2: The proposed generative framework seeks to maximize the ‘fooling gap’ that helps in achieving very
high transferability rates across domains. The orange dashed line shows the ﬂow of gradients  notably only the
generator is tuned in the whole pipeline to fool the pretrained discriminator.

the real images x are passed through the discriminator. The output of the discriminator denotes the
class probabilities Dψ(x  x(cid:48)) ∈ [0  1]c  where c is the number of classes. This is different from the
traditional GAN framework where a discriminator only estimate whether an input is real or fake. For
an adversarial attack  the goal is to fool a network on most examples by making minor changes to its
inputs  i.e. 

s.t.  (cid:80)(cid:0)argmaxj(Dψ(x(cid:48))j) (cid:54)= argmaxj(Dψ(x)j)(cid:1) > fr 

(1)
where  fr is the fooling ratio  y is the ground-truth label for the example x and the predictions on
clean images x are given by  y = argmaxj(Dψ(x)j). Note that we do not necessarily require the
ground-truth labels of source domain images to craft a successful attack. In the case of adversarial
attacks based on a traditional GAN framework  the following objective is maximized for the generator
to achieve the maximal fooling rate:

(cid:107) δ (cid:107)∞≤  

θ∗ ← argmax

CROSSENTROPY(Dψ(x(cid:48))  (cid:49)y) 

(2)

θ

where (cid:49)y is the one-hot encoded label vector for an input example x. The above objective seeks to
maximize the discriminator error on the perturbed images that are output from the generator network.
We argue that the objective given by Eq. 2 does not directly enforce transferability for the generated
perturbations δ. This is primarily due to the reason that the discriminator’s response for clean
examples is totally ignored in the conventional generative attacks. Here  inspired by the generative
adversarial network in [21]  we propose a relativistic adversarial perturbation (RAP) generation
approach that explicitly takes in to account the discriminator’s predictions on clean images. Along-
side reducing the classiﬁer’s conﬁdence on perturbed images  the attack algorithm also forces the
discriminator to maintain a high conﬁdence scores for the clean samples. The proposed relativistic
objective is given by:

θ∗ ← argmax

CROSSENTROPY(Dψ(x(cid:48)) − Dψ(x)  (cid:49)y).

(3)

θ

The cross entropy loss would be higher when the perturbed image is scored signiﬁcantly lower than
the clean image response for the ground-truth class i.e.  Dψ(x(cid:48))y << Dψ(x)y. The discriminator
basically seeks to increase the ‘fooling gap’ (Dψ(x(cid:48))y − Dψ(x)y) between the true and perturbed
samples. Through such relative discrimination  we not only report better transferability rates across
networks trained on the same domain  but most importantly show excellent cross-domain transfer rates
for the instance-agnostic perturbations. We attribute this behaviour to the fact that once a perturbation
pattern is optimized using the proposed loss on a source distribution (e.g.  paintings  cartoon images) 
the generator learns a "contrastive" signal that is agnostic to the underlying distribution. As a result 
when the same perturbation pattern is applied to networks trained on totally different domain (e.g. 
natural images)  it still achieves the state-of-the-art attack transferability rates. Table 2 shows the gain
in transferability when using relativistic cross-entropy (Eq. 3) in comparison to simple cross-entropy
loss (Eq. 2).
For an untargeted attack  the above mentioned objective in Eq. 2 and 3 sufﬁces  however  for a
targeted adversarial attack  the prediction for the perturbed image must match a given target class y(cid:48)
i.e.  argmaxj(Dψ(x(cid:48))j) = y(cid:48) (cid:54)= y. For such a case  we employ the following loss function:
CROSSENTROPY(Dψ(x(cid:48))  (cid:49)y(cid:48)) + CROSSENTROPY(Dψ(x)  (cid:49)y).

θ∗ ← argmin

(4)

The overall training scheme for the generative network is given in Algorithm 1.

θ

4

Algorithm 1 Generator Training for Relativistic Adversarial Perturbations
1: A pretrained classiﬁer Dψ  arbitrary training data distribution X   perturbation budget   loss criteria L.
2: Randomly initialize generator network Gθ
3: repeat
4:
5:
6:
7:
8:
9: until model convergence.

Sample mini-batch of data from the training set.
Use the current state of the generator  Gθ  to generate unbounded adversaries.
Project adversaries  Gθ(x)  within a valid perturbation budget to obtain x(cid:48) such that (cid:107)x(cid:48) − x(cid:107)∞ ≤ .
Forward pass x(cid:48) to Dψ and compute loss given in Eq. (3)/Eq. (4) for targeted/untargeted attack.
Backward pass and update the generator  Gθ  parameters to maximize the loss.

Figure 3: Loss and gradients trend for CE and RCE loss functions. Results are reported with VGG16 network
on 100 random images for MI-FGSM attack. Trends are shown in log scale.

4 Gradient Perspective of Relativistic Cross-Entropy

Adversarial perturbations are crafted via loss function gradients. An effective loss function helps
in the generation of perturbations by back-propagating stronger gradients. Below  we show that
Relativistic Cross-Entropy (RCE) ensures this requisite and thus leads to better performance than
regular Cross-Entropy (CE) loss.
Suppose  the logit-space outputs from the discriminator (pretrained classiﬁer) corresponding to
a clean image (x) and a perturbed image (x’) are denoted by a and a(cid:48)  respectively. Then 

k(cid:1) is the cross-entropy loss for a perturbed input x(cid:48). For clarity  we

CE(a(cid:48)  y)=− log(cid:0)ea(cid:48)
y /(cid:80)
y /(cid:80)

k ea(cid:48)
k. The derivative of p(cid:48)

deﬁne p(cid:48)
rule  the derivative of cross-entropy loss is given by:

y = ea(cid:48)

y w.r.t a(cid:48)

k ea(cid:48)

y([[i=y]] − p(cid:48)

i). Using chain

i is ∂p(cid:48)

y/∂a(cid:48)

i = p(cid:48)

∂CE
∂a(cid:48)

For the relativistic loss formulated as RCE(a(cid:48)  a  y)=− log(cid:0)ea(cid:48)
ry=(cid:0)ea(cid:48)

k−ak(cid:1). The derivative of ry w.r.t a(cid:48)

y−ay /(cid:80)

i is ∂ry/∂a(cid:48)

k ea(cid:48)

y−ay /(cid:80)

i

chain rule  RCE derivative w.r.t to a(cid:48)

= p(cid:48)

i − [[i=y]].

i is given by:

(5)

k−ak(cid:1)  we deﬁne

k ea(cid:48)

i = ri([[i=y]] − ry). From

= ri − [[i=y]].
In light of above relation  RCE has three important properties:

i

∂RCE
∂a(cid:48)

(6)

as opposed to only scores a(cid:48)
as an explicit objective during optimization.

1. Comparing (Eq.5) with (Eq.6) shows that RCE gradient is a function of ‘difference’ (a(cid:48)

y−ay)
y in CE loss. Thus  it measures the relative change in prediction
2. RCE loss back-propagates larger gradients compared to CE  resulting in efﬁcient training and
stronger adversaries (see Figure 3 for empirical evidence). Sketch Proof: We can factorize
the denominator in (Eq. 6) as follows: ∂RCE/∂a(cid:48)
[[i=y]]. Consider the fact that maximization of RCE is only possible when e(a(cid:48)

k(cid:54)=y ea(cid:48)
y−ay) decreases

y−ay +(cid:80)

k−ak )(cid:1)−

i =(cid:0)ea(cid:48)

y−ay /(ea(cid:48)

5

12345678910Number of iterations012345LossLoss Trend Over Iterations12345678910Number of iterations012345Taxicab NormGradients Trend Over Iterationsand(cid:80)

k(cid:54)=y e(a(cid:48)

k−ak) increases. Generally 

trained model and a(cid:48)
∂RCE/∂a(cid:48)
simple words  the gradient strength of RCE is higher than CE.

ay (cid:29) ak(cid:54)=y for the score generated by a pre-
k(cid:54)=y (here k denotes an incorrectly predicted class). Thus 
k). In

k−ak) > (cid:80)

y) and(cid:80)

i > ∂CE/∂a(cid:48)

y (cid:28) a(cid:48)

i since e(a(cid:48)

y−ay) < e(a(cid:48)

k(cid:54)=y e(a(cid:48)

k(cid:54)=y e(a(cid:48)

3. In case x is misclassiﬁed by F(·)  the gradient strength of RCE is still higher than CE (here
noise update with the CE loss will be weaker since adversary’s goal is already achieved i.e. 
x is misclassiﬁed).

Loss
Cross Entropy (CE)
Relativistic CE

VGG-16 VGG-19 Squeeze-v1.1 Dense-121

79.21
86.95

78.96
85.88

69.32
77.81

66.45
75.21

Table 2: Effect of Relativistic loss on trans-
ferability in terms of fooling rate (%) on Im-
ageNet val-set. Generator is trained against
ResNet-152 on Paintings dataset.

5 Experiments

5.1 Rules of the Game

We report results using following three different attack settings in our experiments: (a) White-box.
Attacker has access to the original model (both architecture and parameters) and the training data
distribution. (b) Black-box. Attacker has access to a pretrained model on the same distribution but
without any knowledge of the target architecture and target data distribution. (c) Cross-domain
Black-box. Attacker has neither access to (any) pretrained model  nor to its label space and its
training data distribution. It then has to seek a transferable adversarial function that is learned from a
model pretrained on a possibly different distribution than the original. Hence  this setting is relatively
far more challenging than the plain black-box setting.

Perturbation Attack

l∞ ≤ 10

l∞ ≤ 16

l∞ ≤ 32

Gaussian Noise
Ours-Paintings
Ours-Comics
Ours-ChestX
Gaussian Noise
Ours-Paintings
Ours-Comics
Ours-ChestX
Gaussian Noise
Ours-Paintings
Ours-Comics
Ours-ChestX

VGG-19

ResNet-50

Dense-121

Fool Rate (↑) Top-1 (↓) Fool Rate (↑) Top-1 (↓) Fool Rate (↑) Top-1 (↓)
70.30
62.0
60.40
67.63
66.70
49.76
45.17
59.75
54.37
33.46
26.18
36.98

70.74
60.77
59.26
67.72
66.07
47.62
43.91
58.6
48.40
28.77
26.12
34.85

17.05
29.00
31.81
20.53
23.30
44.50
50.37
31.81
39.90
63.78
71.85
59.49

18.06
31.52
33.69
22.00
25.76
47.51
51.78
34.49
47.21
69.05
71.91
62.17

23.59
47.12
48.47
40.81
33.80
66.52
67.75
62.14
61.07
87.08
87.90
88.12

64.65
46.68
45.78
50.11
57.92
30.21
29.25
33.95
35.48
11.96
11.17
10.92

Table 3: Cross-Domain Black-box: Untargeted attack success (%) in terms of fooling rate on ImageNet val-set.
Adversarial generators are trained against ChexNet on Paintings  Comics and ChestX datasets. Perturbation
budget  l∞ ≤ 10/16/32  is chosen as per the standard practice. Even without the knowledge of targeted model 
its label space and its training data distribution  the transferability rate is much higher than the Gaussian noise.

5.2 Experimental Settings

Generator Architecture. We chose ResNet architecture introduced in [22] as the generator network
Gθ; it consists of downsampling  residual and upsampling blocks. For training  we used Adam
optimizer [23] with a learning rate of 1e-4 and values of exponential decay rate for ﬁrst and second
moments set to 0.5 and 0.999  respectively. Generators are learned against the four pretrained
ImageNet models including VGG-16  VGG-19 [24]  Inception (Inc-v3) [25]  ResNet-152 [26] and
ChexNet (which is a Dense-121 [27] network trained to diagnose pneumonia) [28].
Datasets. We consider the following datasets for generator training namely Paintings [29]  Comics
[30]  ImageNet and a subset of ChestX-ray (ChestX) [28]. There are approximately 80k samples in
Paintings  50k in Comics  1.2 million in ImageNet training set and 10k in ChestX.

6

Bee Eater

Cardoon

Impala

Anemone Fish

Crane

Jigsaw Puzzle

Jigsaw Puzzle

Jigsaw Puzzle

Jigsaw Puzzle

Jigsaw Puzzle

Figure 4: Untargeted adversaries produced by generator trained against Inception-v3 on Paintings dataset. 1st
row shows original images while 2nd row shows unrestricted outputs of adversarial generator and 3rd row are
adversaries after valid projection. Perturbation budget is set to l∞ ≤ 10.

Figure 5: Illustration of attention shift. We use [32] to visualize attention maps of clean (1st row) and adversarial
(2nd row) images. Adversarial images are obtained by training generator against VGG-16 on Paintings dataset.

Inference: Inference is performed on ImageNet validation set (val-set) (50k samples)  a subset (5k
samples) of ImageNet proposed by [11] and ImageNet-NeurIPS [31] (1k samples) dataset.
Evaluation Metrics: We use the fooling rate (percentage of input samples for which predicted label
is ﬂipped after adding adversarial perturbations)  top-1 accuracy and % increase in error rate (the
difference between error rate of adversarial and clean images) to evaluate our proposed approach.

5.2.1 Results

Table 3 shows the cross-domain black-box setting results  where attacker have no access to model
architecture  parameters  its training distribution or label space. Note that ChestX [28] does not have
much texture  an important feature to deceive ImageNet models [33]  yet the transferability rate of
perturbations learned against ChexNet is much better than the Gaussian noise.
Tables 4 and 5 show the comparison of our method against different universal methods on both
naturally and adversarially trained models [34] (Inc-v3  Inc-v4 and IncRes-v2). Our attack success rate
is much higher both in white-box and black-box settings. Notably  for the case of adversarially trained
models  Gaussian smoothing on top of our approach leads to signiﬁcant increase in transferability.
We provide further comparison with GAP [16] in the supplementary material. Figures 4 and 5 show
the model’s output and attention shift on example adversaries.

7

Model Attack

VGG-16 VGG-19 ResNet-152
47.10∗
FFF
71.59∗
AAA
78.30∗
UAP
99.58∗
Ours-Paintings
99.83∗
Ours-Comics
Ours-ImageNet 99.75∗
38.19
FFF
69.45
AAA
73.50
UAP
98.90
Ours-Paintings
99.29
Ours-Comics
Ours-ImageNet
99.19
19.23
FFF
47.21
AAA
47.00
UAP
86.95
Ours-Paintings
88.94
Ours-Comics
95.40
Ours-ImageNet

27.82
45.33
63.40
47.90
58.18
52.64
26.34
51.74
58.00
40.98
42.61
53.02
29.78∗
60.72∗
84.0∗
98.03∗
94.18∗
99.02∗

41.98
65.64
73.10
98.97
99.56
99.44
43.60∗
72.84∗
77.80∗
99.61∗
99.76∗
99.80∗
17.15
48.78
45.5
85.88
88.84
93.26

6
1
-
G
G
V

9
1
-
G
G
V

2
5
1
-
t
e
N
s
e
R

Table 4: White & Black-box Setting: Fool rate (%)
of untargeted attack on ImageNet val-set. Perturba-
tion budget is l∞≤10. * indicates white-box attack.
Our attack’s transferability from ResNet-152 to VGG-
16/19 is even higher than other white-box attacks.

5.2.2 Comparison with State-of-the-Art

Inc-v4

Inc-v3

Model

Attack
UAP
GAP
RHP
UAP
RHP
IncRes-v2 UAP
RHP

Inc-v3ens3
1.00/7.82
5.48/33.3
32.5/60.8
2.08/7.68
27.5/60.3
1.88/8.28
29.7/62.3
33.92/72.46
47.78/73.06
21.06/67.5
34.52/70.3
28.34/71.3
41.06/71.96

Inc-v3ens4
1.80/5.60
4.14/29.4
31.6/58.7
1.94/6.92
26.7/62.5
1.74/7.22
29.8/63.3
38.94/71.4
48.18/72.68
24.1/68.72
56.54/69.9
29.9/66.72
42.68/71.58

IncRes-v2ens
1.88/5.60
3.76/22.5
24.6/57.0
2.34/6.78
21.2/58.5
1.96/8.18
26.8/62.8
Ours-Paintings
33.24/69.66
42.86/73.3
Ours-gs-Paintings
Ours-Comics
12.82/54.72
Ours-gs-Comics
23.58/68.02
Ours-ImageNet
19.84/60.88
Ours-gs-ImageNet
37.4/72.86
Table 5: Black-box Setting: Transferability compar-
ison in terms of % increase in error rate after attack.
Results are reported on subset of ImageNet (5k) with
perturbation budget of l∞ ≤ 16/32. Our generators are
trained against naturally trained Inc-v3 only. ‘gs’ repre-
sents Gaussian smoothing applied to generator output
before projection that enhances our attack strength.

Finally  we compare our method with recently proposed instance-speciﬁc attack method [10] that
exhibits high transferability to adversarially trained models. For the very ﬁrst time in literature  we
showed that a universal function like ours can attain much higher transferability rate  outperforming
the state-of-the-art instance-speciﬁc translation invariant method [10] by a large average absolute gain
of 46.6% and 86.5% (in fooling rates) on both naturally and adversarially trained models  respectively 
as reported in Table 6. The naturally trained models are Inception-v3 (Inc-v3) [25]  Inception-v4
(Inc-v4)  Inception Resnet v2 (IncRes-v2) [35] and Resnet v2-152 (Res-152) [36]). The adversarially
trained models are from [34].

Attack

Naturally Trained

Adversarially Trained

3
v
-
c
n
I

2
v
-
s
e
R
c
n
I

2
5
1
-
s
e
R

Inc-v3 Inc-v4 IncRes-v2 Res-152 Inc-v3ens3 Inc-v3ens4 IncRes-v2ens
79.6∗
35.9
FGSM
75.5∗
37.3
TI-FGSM
97.8∗
47.1
MI-FGSM
TI-MI-FGSM 97.9∗
52.4
98.3∗
73.8
DIM
98.5∗
75.2
TI-DIM
36.1
FGSM
44.3
41.5
49.7
TI-FGSM
64.8
MI-FGSM
74.8
69.5
TI-MI-FGSM 76.1
83.5
86.1
DIM
85.5
TI-DIM
86.4
34.0
40.1
FGSM
39.3
46.4
TI-FGSM
48.1
MI-FGSM
54.2
50.9
TI-MI-FGSM 55.6
77.8
DIM
77.0
77.0
73.9
TI-DIM
100.0∗ 99.7
98.5
99.8
97.0
99.1
95.4

30.6
32.1
46.4
47.9
67.8
69.2
64.3∗
63.7∗
100.0∗
100.0∗
99.1∗
98.8∗
30.3
33.4
44.3
45.1
73.5
73.2
99.8
97.6
99.8
93.4
97.5
90.5

30.2
34.1
38.7
41.1
58.4
59.2
31.9
40.1
54.5
59.6
73.5
76.3
81.3∗
78.9∗
97.5∗
97.4∗
97.4∗
97.2∗
98.9
93.6
98.7
87.7
98.1
91.8

14.7
28.9
17.4
35.1
24.3
47.1
17.2
34.5
23.7
51.7
40.0
60.1
17.7
34.5
23.7
37.7
36.0
58.8
74.6
83.9
46.8
58.8
60.5
78.4

7.0
22.3
9.5
25.8
13.0
37.4
10.2
27.8
13.3
49.3
27.9
59.5
9.9
27.8
13.3
32.8
24.1
42.8
64.8
75.9
23.3
42.8
36.4
68.9

15.6
28.2
20.5
35.8
24.2
46.9
18.0
34.6
25.1
50.7
41.2
61.3
20.2
34.6
25.1
39.9
40.5
60.3
69.3
85.2
39.3
60.3
55.4
78.6

Ours-Paintings
Ours-gs-Paintings 99.9∗
99.9∗
Ours-Comics
99.9∗
Ours-gs-Comics
99.8∗
Ours-ImageNet
Ours-gs-ImageNet 98.9∗

Table 6: White-box and
Black-box: Transferabil-
ity comparisons.
Suc-
cess rate on ImageNet-
NeurIPS validation set
(1k images) is reported
by creating adversaries
within the perturbation
budget of l∞ ≤ 16 
as per the standard prac-
tice [10]. Our generators
are learned against nat-
urally trained Inception-
∗ indicates
v3 only.
white-box attack.
‘gs’
is Gaussian smoothing
applied to the generator
output before projection.
Smoothing leads to slight
decrease in transferabil-
ity on naturally trained
but shows signiﬁcant in-
crease against adversari-
ally trained models.

8

(a) Naturally Trained IncRes-v2

(b) Adversarially Trained IncRes-v2

(c) Naturally Trained IncRes-v2

(d) Adversarially Trained IncRes-v2

Figure 6: Effect of Gaussian kernel size and number of training epochs is shown on the transferability (in %age
fool rate) of adversarial examples. Generator is trained against Inception-v3 on Paintings  while the inference is
performed on ImageNet-NeurIPS. Firstly  as number of epochs increases  transferability against naturally trained
IncRes-v2 increases while decreases against its adversarially trained version. Secondly  as the size of Gaussian
kernel increases  transferability against naturally as well as adversarially trained IncRes-v2 decreases. Applying
kernel of size 3 leads to optimal results against adversarially trained model. Perturbation is set to l∞ ≤ 16.

5.3 Transferability: Naturally Trained vs. Adversarially Trained

Furthermore  we study the impact of training iterations and Gaussian smoothing [10] on the transfer-
ability of our generative adversarial examples. We report results using naturally and adversarially
trained IncRes-v2 model [35] as other models exhibit similar behaviour. Figure 6 displays the
transferability (in %age accuracy) as a function of the number of training epochs (a-b) and various
kernel sizes for Gaussian smoothing (c-d).
Firstly  we observe a gradual increase in the transferability of generator against the naturally trained
model as the training epochs advance. In contrast the transferability deteriorates against the adver-
sarially trained model. Therefore  when targeting naturally trained models  we train for ten epochs
on Paintings  Comics  and ChestX datasets (although we anticipate better performance for higher
epochs). When targeting adversarially trained models  we deploy an early stopping criterion to
obtain the best trained generator since the performance drops on such models as epochs are increased.
This fundamentally shows the reliance of naturally and adversarially trained models on different
set of features. Our results clearly demonstrate that the adversarial solution space is shared across
different architectures and even across distinct data domains. Since we train our generator against
naturally trained models only  therefore it converges to a solution space on which an adversarially
trained model has already been trained. As a result  our perturbations gradually become weaker
against adversarially trained models as the training progress. A visual demonstration is provided in
supplementary material.
Secondly  the application of Gaussian smoothing reveals different results on naturally trained and
adversarially trained models. After applying smoothing  adversaries become stronger for adversarially
trained models and get weaker for naturally trained models. We achieve optimal results with the
kernel size of 3 and σ = 1 for adversarially trained models and use these settings consistently in our
experiments. We apply Gaussian kernel on the unrestricted generator’s output  therefore as the kernel
size is increased  generator’s output becomes very smooth and after projection within valid l∞ range 
adversaries become weaker.

6 Conclusion

Adversarial examples have been shown to be transferable across different models trained on the same
domain. For the ﬁrst time in literature  we show that the cross-domain transferable adversaries exists
that can fool the target domain networks with high success rates. We propose a novel generative
framework that learns to generate strong adversaries using a relativistic discriminator. Surprisingly 
our proposed universal adversarial function can beat the instance-speciﬁc attack methods that were
previously found to be much stronger compared to the universal perturbations. Our generative attack
model trained on Chest X-ray and Comics images  can fool VGG-16  ResNet50 and Dense-121
models with a success rate of ∼ 88% and ∼ 72%  respectively  without having any knowledge of
data distribution or label space.

9

13579020406080100Fool Rate (%)Transferability Vs. Epochs13579010203040506070Fool Rate (%)Transferability Vs. EpochsNo Smoothing3x35x57x79x911x11020406080100Fool Rate (%)Transferability Vs. Kernel SizeNo Smoothing3x35x57x79x911x110102030405060Fool Rate (%)Transferability Vs. Kernel SizeReferences
[1] Nicolas Papernot  Patrick McDaniel  and Ian Goodfellow. Transferability in machine learning: from

phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277  2016.

[2] Yanpei Liu  Xinyun Chen  Chang Liu  and Dawn Song. Delving into transferable adversarial examples and

black-box attacks. Proceedings of 5th International Conference on Learning Representations  2017.

[3] Florian Tramèr  Nicolas Papernot  Ian Goodfellow  Dan Boneh  and Patrick McDaniel. The space of

transferable adversarial examples. arXiv preprint arXiv:1704.03453  2017.

[4] Alhussein Fawzi  Omar Fawzi  and Pascal Frossard. Analysis of classiﬁers’ robustness to adversarial

perturbations. arXiv preprint arXiv:1502.02590  2015.

[5] Alhussein Fawzi  Seyed-Mohsen Moosavi Dezfooli  and Pascal Frossard. Robustness of classiﬁers: from
adversarial to random noise. In Advances in Neural Information Processing Systems  pages 1632–1640 
2016.

[6] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial examples.

arXiv preprint arXiv:1412.6572  2014.

[7] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial machine learning at scale. arXiv preprint

arXiv:1611.01236  2016.

[8] Seyed-Mohsen Moosavi Dezfooli  Alhussein Fawzi  and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 2574–2582  2016.

[9] Anh Nguyen  Jason Yosinski  and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 427–436  2015.

[10] Yinpeng Dong  Tianyu Pang  Hang Su  and Jun Zhu. Evading defenses to transferable adversarial examples
by translation-invariant attacks. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition  2019.

[11] Yingwei Li  Song Bai  Cihang Xie  Zhenyu Liao  Xiaohui Shen  and Alan L Yuille. Regional homogene-
ity: Towards learning transferable universal adversarial perturbations against defenses. arXiv preprint
arXiv:1904.00979  2019.

[12] Seyed-Mohsen Moosavi Dezfooli  Alhussein Fawzi  Omar Fawzi  and Pascal Frossard. Universal adver-
sarial perturbations. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages
86–94  2017.

[13] Konda Reddy Mopuri  Utsav Garg  and R Venkatesh Babu. Fast feature fool: A data independent approach
to universal adversarial perturbations. In Proceedings of the British Machine Vision Conference (BMVC) 
2017.

[14] Konda Reddy Mopuri  Phani Krishna Uppala  and R. Venkatesh Babu. Ask  acquire  and attack: Data-free

uap generation using class impressions. In ECCV  2018.

[15] Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adversarial

examples. arXiv preprint arXiv:1703.09387  2017.

[16] Omid Poursaeed  Isay Katsman  Bicheng Gao  and Serge J. Belongie. Generative adversarial perturbations.

2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition  pages 4422–4431  2018.

[17] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow 
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICRL)  2014.

[18] Cihang Xie  Zhishuai Zhang  Jianyu Wang  Yuyin Zhou  Zhou Ren  and Alan Loddon Yuille. Improving

transferability of adversarial examples with input diversity. CoRR  abs/1803.06978  2018.

[19] Yang Song  Rui Shu  Nate Kushman  and Stefano Ermon. Constructing unrestricted adversarial examples
with generative models. In Advances in Neural Information Processing Systems  pages 8312–8323  2018.

[20] Chaowei Xiao  Bo Li  Jun-Yan Zhu  Warren He  Mingyan Liu  and Dawn Song. Generating adversarial
examples with adversarial networks. In Proceedings of the 27th International Joint Conference on Artiﬁcial
Intelligence  pages 3905–3911. AAAI Press  2018.

10

[21] Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv

preprint arXiv:1807.00734  2018.

[22] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In ECCV  2016.

[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[24] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556  2014.

[25] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 2818–2826  2016.

[26] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

[27] Gao Huang  Zhuang Liu  and Kilian Q. Weinberger. Densely connected convolutional networks. 2017

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2261–2269  2017.

[28] Pranav Rajpurkar  Jeremy Irvin  Kaylie Zhu  Brandon Yang  Hershel Mehta  Tony Duan  Daisy Ding 
Aarti Bagul  Curtis P. Langlotz  Katie Shpanskaya  Matthew P. Lungren  and Andrew Y. Ng. Chexnet:
Radiologist-level pneumonia detection on chest x-rays with deep learning. CoRR  abs/1711.05225  2017.

[29] Painter by Number. https://www.kaggle.com/c/painter-by-numbers/data. Kaggle  2017.

[30] Cenk Bircano˘glu. https://www.kaggle.com/cenkbircanoglu/comic-books-classification.

Kaggle  2017.

[31] NeurIPS.

https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/

data. Kaggle  2017.

[32] Ramprasaath R. Selvaraju  Michael Cogswell  Abhishek Das  Ramakrishna Vedantam  Devi Parikh  and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. 2017
IEEE International Conference on Computer Vision (ICCV)  pages 618–626  2017.

[33] Robert Geirhos  Patricia Rubisch  Claudio Michaelis  Matthias Bethge  Felix A. Wichmann  and Wieland
Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and
robustness. In International Conference on Learning Representations  2019.

[34] Florian Tramèr  Alexey Kurakin  Nicolas Papernot  Dan Boneh  and Patrick McDaniel. Ensemble adver-
sarial training: Attacks and defenses. In International Conference on Learning Representations (ICRL) 
2018.

[35] Christian Szegedy  Sergey Ioffe  Vincent Vanhoucke  and Alexander A Alemi. Inception-v4  inception-

resnet and the impact of residual connections on learning. In AAAI  volume 4  page 12  2017.

[36] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual networks.

In European conference on computer vision  pages 630–645. Springer  2016.

11

,Muhammad Muzammal Naseer
Salman Khan
Muhammad Haris Khan
Fahad Shahbaz Khan
Fatih Porikli