2018,Manifold Structured Prediction,Structured prediction provides a general framework to deal with supervised problems where the outputs have semantically rich structure. While classical approaches consider finite  albeit potentially huge  output spaces  in this paper we discuss how structured prediction can be extended to a continuous scenario. Specifically  we study a structured prediction approach to manifold-valued regression. We characterize a class of problems for which the considered approach is statistically consistent and study how geometric optimization can be used to compute the corresponding estimator. Promising experimental results on both simulated and real data complete our study.,Manifold Structured Prediction

Alessandro Rudi • 1 Carlo Ciliberto • ∗  2 Gian Maria Marconi 3 Lorenzo Rosasco 3 4

alessandro.rudi@inria.fr

c.ciliberto@imperial.ac.uk

gian.maria.marconi@iit.it

lrosasco@mit.edu

1INRIA - Département d’informatique  École Normale Supérieure - PSL Research University  Paris  France.

2Department of Electrical and Electronic Engineering  Imperial College  London  UK.

3Università degli studi di Genova & Istituto Italiano di Tecnologia  Genova  Italy.

4Massachusetts Institute of Technology  Cambridge  USA.

• Equal Contribution

Abstract

Structured prediction provides a general framework to deal with supervised prob-
lems where the outputs have semantically rich structure. While classical approaches
consider ﬁnite  albeit potentially huge  output spaces  in this paper we discuss how
structured prediction can be extended to a continuous scenario. Speciﬁcally  we
study a structured prediction approach to manifold valued regression. We character-
ize a class of problems for which the considered approach is statistically consistent
and study how geometric optimization can be used to compute the corresponding
estimator. Promising experimental results on both simulated and real data complete
our study.

1

Introduction

Regression and classiﬁcation are probably the most classical machine learning problems and cor-
respond to estimating a function with scalar and binary values  respectively. In practice  it is often
interesting to estimate functions with more structured outputs. When the output space can be assumed
to be a vector space  many ideas from regression can be extended  think for example to multivariate
[20] or functional regression [32]. However  a lack of a natural vector structure is a feature of many
practically interesting problems  such as ranking [18]  quantile estimation [26] or graph prediction
[38]. In this latter case  the outputs are typically provided only with some distance or similarity func-
tion that can be used to design appropriate loss function. Knowledge of the loss is sufﬁcient to analyze
an abstract empirical risk minimization approach within the framework of statistical learning theory 
but deriving approaches that are at the same time statistically sound and computationally feasible
is a key challenge. While ad-hoc solutions are available for many speciﬁc problems [15  37  24  7] 
structured prediction [5] provides a unifying framework where a variety of problems can be tackled
as special cases.
Classically  structured prediction considers problems with ﬁnite  albeit potentially huge  output spaces.
In this paper  we study how these ideas can be applied to non-discrete output spaces. In particular  we
consider the case where the output space is a Riemannian manifold  that is the problem of manifold
structured prediction (also called manifold valued regression [46]). While also in this case ad-hoc
methods are available [47]  in this paper we adopt and study a structured prediction approach starting
from a framework proposed in [13]. Within this framework  it is possible to derive a statistically
sound  and yet computationally feasible  structured prediction approach as long as the loss function
satisﬁes suitable structural assumptions [14  17  29  25  12  36]. Moreover we can guarantee that the
computed prediction is always an element of the manifold.
Our main technical contribution is a characterization of loss functions for manifold structured
prediction satisfying such a structural assumption. In particular  we consider the case where the

∗Work performed while C.C. was at the University College London.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Riemannian metric is chosen as a loss function. As a byproduct of these results  we derive a manifold
structured learning algorithm that is universally consistent and corresponding ﬁnite sample bounds.
From a computational point of view  the proposed algorithm requires solving a linear system (at
training time) and a minimization problem over the output manifold (at test time). To tackle this
latter problem  we investigate the application of geometric optimization methods  and in particular
Riemannian gradient descent [1]. We consider both numerical simulations and benchmark datasets
reporting promising performances. The rest of the paper is organized as follows. In Section 2  we
deﬁne the problem and explain the proposed algorithm. In Section 3 we state and prove the theoretical
results of this work. In Section 4 we explain how to compute the proposed algorithm and we show
the performance of our method on synthetic and real data.

(cid:90)

E(f )

with

2 Structured Prediction for Manifold Valued Regression
The goal of supervised learning is to ﬁnd a functional relation between an input space X and an
output space Y given a ﬁnite set of observations. Traditionally  the output space is either a linear
space (e.g. Y = RM ) or a discrete set (e.g. Y = {0  1} in binary classiﬁcation). In this paper  we
consider the problem of manifold structured prediction [47]  in which output data lies on a manifold
M ⊂ Rd. In this context  statistical learning corresponds to solving

E(f ) =

(cid:52)(f (x)  y) ρ(x  y)

X×Y

argmin
f∈X→Y

(1)
where Y is a subset of the manifold M and ρ is an unknown distribution on X ×Y. Here  (cid:52) : Y×Y →
R is a loss function that measures prediction errors for points estimated on the manifold. The
minimization is meant over the set of all measurable functions from X to Y. The distribution is ﬁxed

but unknown and a learning algorithm seeks an estimator (cid:98)f : X → Y that approximately solves

Eq. (1)  given a set of training points (xi  yi)n
A concrete example of loss function that we will consider in this paper is (cid:52) = d2 the squared
geodesic distance d : Y × Y → R [27]. The geodesic distance is the natural metric on a Riemannian
manifold (it corresponds to the Euclidean distance when M = Rd) and is a natural loss function in
the context of manifold regression [46  47  19  23  21].

i=1 sampled independently from ρ.

2.1 Manifold Valued Regression via Structured Prediction

In this paper we consider a structured prediction approach to manifold valued regression following
ideas in [13]. Given a training set (xi  yi)n

i=1  an estimator for problem Eq. (1) is deﬁned by

(cid:98)f (x) = argmin

y∈Y

n(cid:88)

i=1

αi(x) (cid:52) (y  yi)

(2)

for any x ∈ X . The coefﬁcients α(x) = (α1(x)  . . .   αn(x))(cid:62) ∈ Rn are obtained solving a linear
system for a problem akin to kernel ridge regression (see Section 2.2): given a positive deﬁnite kernel
k : X × X → R [4] over X   we have

α(x) = (α1(x)  . . .   αn(x))(cid:62) = (K + nλI)−1Kx

(3)
where K ∈ Rn×n is the empirical kernel matrix with Ki j = k(xi  xj)  and Kx ∈ Rn the vector
whose i-th entry corresponds to (Kx)i = k(x  xi). Here  λ ∈ R+ is a regularization parameter and
I ∈ Rn×n denotes identity matrix.
Computing the estimator in Eq. (2) can be divided into two steps. During a training step the score
on a new test point x ∈ X . This last step requires minimizing the linear combination of distances
(cid:52)(y  yi) between a candidate y ∈ Y and the training outputs (yi)n
i=1  weighted by the corresponding
scores αi(x). Next  we recall the derivation of the above estimator following [13].

function α : X → Rn is learned  while during the prediction step  the output (cid:98)f (x) ∈ Y is estimated

2.2 Derivation of the Proposed Estimator

The derivation of (cid:98)f in Eq. (2) is based on the following key structural assumption on the loss.

2

(cid:90)

(cid:29)

(cid:90)

(cid:28)

E(f ) =

Deﬁnition 1 (Structure Encoding Loss Function (SELF)). Let Y be a compact set. A function
(cid:52) : Y × Y → R is a Structure Encoding Loss Function if there exist a separable Hilbert space H  a
continuous feature map ψ : Y → H and a continuous linear operator V : H → H such that for all
y  y(cid:48) ∈ Y

(cid:52)(y  y(cid:48)) = (cid:104)ψ(y)  V ψ(y(cid:48))(cid:105)H.

(4)
Intuitively  the SELF deﬁnition requires a loss function to be “bi-linearizable” over the space H. This
is similar  but more general  than requiring the loss to be a kernel since it allows also to consider
distances (which are not positive deﬁnite) or even non-symmetric loss functions. As observed in [13] 
a wide range of loss functions often used in machine learning are SELF. In Section 3 we study how
the above assumption applies to manifold structured loss functions  including the squared geodesic
distance.
We ﬁrst recall how the estimator Eq. (2) can be obtained assuming (cid:52) to be SELF. We begin by
rewriting the expected risk in Eq. (1) as

ψ(y) dρ(y|x)

(cid:90)

(5)
where we have conditioned ρ(y  x) = ρ(y|x)ρX (x) and used the linearity of the integral and the
inner product. Therefore  any function f∗ : X → Y minimizing the above functional must satisfy the
following condition

ψ(f (x))  V

dρ(x)

H

X

Y

(cid:104)ψ(y)  V g∗(x)(cid:105)H

f∗(x) = argmin
y∈Y

(6)
where we have introduced the function g∗ : X → H that maps each point x ∈ X to the conditional
expectation of ψ(y) given x. However we cannot compute explicitly g∗  but noting that it minimizes
the expected least squares error

where

Y

g∗(x) =

ψ(y) dρ(y|x)

(cid:90)

(cid:98)g(x) =(cid:99)W (cid:62)x

(cid:107)ψ(y) − g(x)(cid:107)2Hdρ(x  y)
(cid:99)W = argmin

(7)
suggests that a least squares estimator can be considered. We ﬁrst illustrate this idea for X = Rd and
H = Rk. In this case we can consider a ridge regression estimator

1
n

(cid:107)XW − ψ(Y )(cid:107)2

F + λ(cid:107)W(cid:107)2

with

W∈Rd×k

(8)
where X = (x1  . . .   xn)(cid:62) ∈ Rn×d and ψ(Y ) = (ψ(y1)  . . .   ψ(yn))(cid:62) ∈ Rn×k are the matrices
whose i-th row correspond respectively to the training sample xi ∈ X and the (mapped) training
output ψ(yi) ∈ H. We have denoted (cid:107) · (cid:107)2
F the squared Frobenius norm of a matrix  namely the
sum of all its squared entries. The ridge regression solution can be obtained in closed form as

(cid:99)W = (X(cid:62)X + nλI)−1X(cid:62)ψ(Y ). For any x ∈ X we have
where we have introduced the coefﬁcients α(x) = X(X(cid:62)X + nλI)−1x ∈ Rn. By substituting(cid:98)g to

(cid:98)g(x) = ψ(Y )(cid:62)X(X(cid:62)X + nλI)−1x = ψ(Y )(cid:62)α(x) =

n(cid:88)

αi(x)ψ(yi)

(9)

i=1

F

ψ(y)  V

αi(x)ψ(yi)

= argmin
y∈M

αi(x) (cid:52) (y  yi)

(10)

where we have used the linearity of the sum and the inner product to move the coefﬁcients αi outside
of the inner product. Since the loss is SELF  we then obtain (cid:104)ψ(y)  V ψ(yi)(cid:105) = (cid:52)(y  yi) for any yi in

the training set. This recovers the estimator (cid:98)f introduced in Eq. (2)  as desired.
k : X × X → R a positive deﬁnite kernel. Then(cid:98)g can be computed by kernel ridge regression (see
applies if H is inﬁnite dimensional. Indeed  thanks to the SELF assumption  (cid:98)f does not depend on

We end noting how the above idea can be extended. First  we can consider X to be a set and
e.g. [43]) to obtain the scores α(x) = (K + nλI)−1Kx  see Eq. (3). Second  the above discussion
explicit knowledge of the space H but only on the loss function.
We next discuss the main results of the paper  showing that a large class of loss functions for manifold
structured prediction are SELF. This will allow us to prove consistency and learning rates for the
manifold structured estimator considered in this work.

g∗ in Eq. (6) we have

(cid:98)f (x) = argmin

y∈M

(cid:42)

(cid:32) n(cid:88)

i=1

(cid:33)(cid:43)

n(cid:88)

i=1

3

3 Characterization of SELF Function on Manifolds

In this section we provide sufﬁcient conditions for a wide class of functions on manifolds to satisfy
the deﬁnition of SELF. A key example will be the case of the squared geodesic distance. To this end
we will make the following assumptions on the manifold M and the output space Y ⊆ M where the
learning problem takes place.
Assumption 1. M is a complete d-dimensional smooth connected Riemannian manifold  without
boundary  with Ricci curvature bounded below and positive injectivity radius.

The assumption above imposes basic regularity conditions on the output manifold. In particular
we require the manifold to be locally diffeomorphic to Rd and that the tangent space of M at any
p ∈ M varies smoothly with respect to p. This assumption avoids pathological manifolds and is
satisﬁed for instance by any smooth compact manifold (e.g. the sphere  torus  etc.) [27]. Other
notable examples are the statistical manifold (without boundary) [3] any open bounded sub-manifold
of the cone of positive deﬁnite matrices  which is often studied in geometric optimization settings [1].
This assumption will be instrumental to guarantee the existence of a space of functions H on M rich
enough to contain the squared geodesic distance.
Assumption 2. Y is a compact geodesically convex subset of the manifold M.
A subset Y of a manifold is geodesically convex if for any two points in Y there exists one and only
one minimizing geodesic curve connecting them. The effect of Asm. 2 is twofold. On one hand it
guarantees a generalized notion of convexity for the space Y on which we will solve the optimization
problem in Eq. (2). On the other hand it avoids the geodesic distance to have singularities on Y
(which is key to our main result below). For a detailed introduction to most deﬁnitions and results
reviewed in this section we refer the interested reader to standard references for differential and
Riemannian geometry (see e.g. [27]). We are ready to prove the main result of this work.
Theorem 1 (Smooth Functions are SELF). Let M satisfy Asm. 1 and Y ⊆ M satisfy Asm. 2. Then 
any smooth function h : Y × Y → R is SELF on Y.

0 (M) ⊗ C∞

Sketch of the proof (Thm. 1). The complete proof of Thm. 1 is reported in ??. The proof hinges
around the following key steps:
Step 1 If there exists an RKHS H on M  then any h ∈ H ⊗ H is SELF. Let H be a reproducing
kernel Hilbert space (RKHS) [4] of functions on M with associated bounded kernel k : M×M → R.
Let H ⊗ H denote the RKHS of functions h : M × M → R with associated kernel ¯k such that
¯k((y  z)  (y(cid:48)  z(cid:48))) = k(y  y(cid:48))k(z  z(cid:48)) for any y  y(cid:48)  z  z(cid:48) ∈ M. Let  h : M × M → R be such that
h ∈ H ⊗ H. Recall that H ⊗ H is isometric to the space of Hilbert-Schmidt operators from H to
itself. Let Vh : H → H be the operator corresponding to h via such isometry. We show that the
SELF deﬁnition is satisﬁed with V = Vh and ψ(y) = k(y ·) ∈ H for any y ∈ M. In particular  we
have (cid:107)V (cid:107) ≤ (cid:107)V (cid:107)HS = (cid:107)h(cid:107)H⊗H  with (cid:107)V (cid:107)HS denoting the Hilbert-Schmidt norm of V .
0 (M) “contains” C∞(Y × Y). If Y is compact and geodesi-
Step 2: Under Asm. 2  C∞
cally convex  then it is diffeomorphic to a compact set of Rd. By using this fact  we prove that any
0 (M×M)
function in C∞(Y×Y)  the space of smooth functions on Y×Y  admits an extension in C∞
the space of smooth functions on M×M vanishing at inﬁnity (this is well deﬁned since M is diffeo-
0 (M) 
morphic to Rd  see ?? in the supplementary material)  and that C∞
with ⊗ the canonical topological tensor product [50].
0 (M). Under Asm. 1  the
Step 3: Under Asm. 1  there exists an RKHS on M containing C∞
Sobolev space H = H 2
s (M) of square integrable functions with smoothness s is an RKHS for any
s > d/2 (see [22] for a deﬁnition of Sobolev spaces on Riemannian manifolds).
The proof proceeds as follows: from Step 1  we see that to guarantee h to be SELF it is sufﬁcient to
prove the existence of an RKHS H such that h ∈ H ⊗H. The rest of the proof is therefore devoted to
showing that for smooth functions this is satisﬁed for H = H 2
s (M). Since h is smooth  by Step 2 we
have that under Asm. 2  there exists a ¯h ∈ C∞
0 (M) whose restriction ¯h|Y×Y to Y × Y
s (M) the Sobolev space of squared integrable functions on M
corresponds to h. Now  denote by H 2
0 (M)|Y ⊆
with smoothness index s > 0. By construction  (see [22]) for any s > 0  we have C∞
s (M).
s (M)|Y  namely for any function. In particular  ¯h ∈ C∞
H 2
Finally  Step 3 guarantees that under Asm. 1  H = H 2
s (M) with s > d/2 is an RKHS  showing that
h ∈ H ⊗ H as desired.

0 (M×M) = C∞

0 (M) ⊗ C∞

0 (M)⊗C∞

0 (M) ⊗ C∞

0 (M) ⊆ H 2

s (M) ⊗ H 2

4

Interestingly  Thm. 1 shows that the SELF estimator proposed in Eq. (2) can tackle any manifold
valued learning problem in the form of Eq. (1) with smooth loss function. In the following we study
the speciﬁc case of the squared geodesic distance.
Theorem 2 (d2 is SELF). Let M satisfy Asm. 1 and Y ⊆ M satisfy Asm. 2. Then  the squared
geodesic distance (cid:52) = d2 : M × M → R is smooth on Y. Therefore (cid:52) is SELF on Y.
The proof of the result above is reported in the supplementary material. The main technical aspect is
to show that regularity provided by Asm. 2 guarantees the squared geodesic distance to be smooth.
The fact that (cid:52) is SELF is then an immediate corollary of Thm. 1.

3.1 Statistical Properties of Manifold Structured Prediction

In this section  we characterize the generalization properties of the manifold structured estimator
Eq. (2) in light of Thm. 1 and Thm. 2.
Theorem 3 (Universal Consistency). Let M satisfy Asm. 1 and Y ⊆ M satisfy Asm. 2. Let X be
a compact set and k : X × X → R be a bounded continuous universal kernel2 For any n ∈ N
for a learning problem with smooth loss function (cid:52)  with (xi  yi)n
sampled from ρ and λn = n−1/4. Then

and any distribution ρ on X × Y let (cid:98)fn : X → Y be the manifold structured estimator in Eq. (2)

i=1 training points independently

n→∞E((cid:98)fn) = E(f∗) with probability 1.

lim

(11)

The result above follows from Thm. 4 in [13] combined with our result in Thm. 1. It guarantees
that the algorithm considered in this work ﬁnds a consistent estimator for the manifold structured
problem  in order to derive also generalization bounds for (cid:98)f. In particular  if we denote by F the
problem  when the loss function is smooth (thus also in the case of the squared geodesic distance).
As it is standard in statistical learning theory  we can impose regularity conditions on the learning
estimator(cid:98)g introduced in Eq. (9) is learned. In the simpliﬁed case discussed in Section 2.2  with
RKHS associated to the kernel k  we will require g∗ to belong to the same space H ⊗ F where the
linear kernel on X = Rd and H = Rk ﬁnite dimensional  we have F = Rd and this assumption
∗ ∈ Rk×d = H ⊗ F  such that g∗(x) = W (cid:62)
corresponds to require the existence of a matrix W (cid:62)
∗ x
for any x ∈ X . In the general case  the space H ⊗ F extends to the notion of reproducing kernel
Hilbert space for vector-valued functions (see e.g. [30  2]) but the same intuition applies [28  10  49].
Theorem 4 (Generalization Bounds). Let M satisfy Asm. 1 and Y ⊆ M satisfy Asm. 2. Let
associated RKHS F. For any n ∈ N  let (cid:98)fn denote the manifold structured estimator in Eq. (2) for a
s (M) with s > d/2 and k : X × X → R be a bounded continuous reproducing kernel with
H = H 2
learning problem with smooth loss (cid:52) : Y × Y → R and λn = n−1/2. If the conditional mean g∗
belongs to H ⊗ F  then

E((cid:98)fn) − E(f∗) ≤ c(cid:52)q τ 2 n− 1

4

(12)
holds with probability not less than 1 − 8e−τ for any τ > 0  with c(cid:52) = (cid:107) (cid:52) (cid:107)H⊗H and q a constant
not depending on n  τ or the loss (cid:52).
The generalization bound of Thm. 4 is obtained by adapting Thm. 5 of [13] to our results in Thm. 1
as detailed in the supplementary material. To our knowledge these are the ﬁrst results characterizing
in such generality the generalization properties of an estimator for manifold structured learning with
generic smooth loss function. We conclude with a remark on a key quantity in the bound of Thm. 4.
Remark 1 (The constant c(cid:52)). We comment on the role played in the learning rate by c(cid:52)  the norm of
the loss function (cid:52) seen an element of the Hilbert space H⊗H. Indeed  from the discussion of Thm. 1
we have seen that any smooth function on Y is SELF and belongs to the set H⊗H with H = H 2
s (M) 
the Sobolev space of squared integrable functions for s > d/2. Following this interpretation  we
see that the bound in Thm. 4 can improve signiﬁcantly (in terms of the constants) depending on the
regularity of the loss function: smoother loss functions will result in “simpler” learning problems and
vice-versa. In particular  when (cid:52) corresponds to the squared geodesic distance  the more “regular”
is the manifold M  the learning problem will be. A reﬁned quantitative characterization of c(cid:52) in
terms of the Ricci curvature and the injective radius of the manifold is left to future work.

2This is standard assumption for universal consistency (see [48]). An example of continuous universal kernel

on X = Rd is the Gaussian k(x  x(cid:48)) = exp(−(cid:107)x − x(cid:48)(cid:107)2/σ)  for σ > 0.

5

Table 1: Structured loss  gradient of the structured loss and retraction for P m
zi ∈ Sd−1 are the training set points. I ∈ Rd×d is the identity matrix.

++ and Sd−1. Zi ∈ P m

++ and

Positive deﬁnite matrix manifold (P m

++)

Sphere (Sd−1)
αi arccos ((cid:104)zi  y(cid:105))2

n(cid:80)
i=1 αi(yyT − I) arccos((cid:104)zi y(cid:105))
1−(cid:104)zi y(cid:105) zi

√

i=1

4(cid:80)n

v(cid:107)v(cid:107)

n(cid:80)
n(cid:80)

i=1

i=1

F (y)
∇MF (y)
Ry(v)

2

αi(cid:107) log(Y − 1

2 log(Y 1
αiY 1
2 exp(Y − 1

Y 1

2 )(cid:107)2

F

2 ZiY − 1
2 Z−1
2 vY − 1

i Y 1
2 )Y 1

2

2 )Y 1

2

4 Manifold Structured Prediction Algorithm and Experiments

In this section we recall geometric optimization algorithms that can be adopted to perform the

estimation of (cid:98)f on a novel test point x. We then evaluate the performance of the proposed method in

practice  reporting numerical results on simulated and real data.
The algorithm presented in this paper consists in two steps. In the training phase  the matrix C =
(K + λnI)−1 is computed  requiring a computational cost of essentially O(n3) in time and O(n2) in
space (see Eq. (3)). In the inference phase  given a test point x  the coefﬁcients in Eq. (3) are computed 
α(x) := Cv(x)  requiring essentially O(n) in time  and then the optimization problem in Eq. (2) is
solved (see next subsection for more details). Note that it is possible to reduce the computational
complexity of the training and evaluation of the coefﬁcients  while retaining the statistical guarantees
of the proposed method. Indeed the computation of the coefﬁcients consists essentially in solving
√
a kernel ridge regression problem [10] as analyzed in [13]  for which methods based on random
projection  as Nyström [44] or random features [39]  reduce the complexity up to O(n
n) in time
and O(n) in space  while guaranteeing the same statistical properties [40  42  9  11  41].

4.1 Optimization on Manifolds

We begin discussing the computational aspects related to evaluating the manifold structured estimator.
In particular  we discuss how to address the optimization problem in Eq. (2) in speciﬁc settings.
Given a test point x ∈ X   this process consists in solving a minimization over Y  namely

(13)
where F (y) corresponds to the linear combination of (cid:52)(y  yi) weighted by the scores αi(x) computed
according to Eq. (3). If Y is a linear manifold or a subset of M = Rd  this problem can be solved by
means of gradient-based minimization algorithms  such as Gradient Descent (GD):

min
y∈Y F (y)

(14)
for a step size ηt ∈ R. This algorithm can be extended to Riemannian gradient descent (RGD) [52]
on manifolds  as

yt+1 = yt − ηt∇F (yt)

yt+1 = Expyt(ηt∇MF (yt))

(15)
Where ∇MF is the gradient deﬁned with respect to the Riemannian metric (see [1]) and Expy :
TyM → M denotes the exponential map on y ∈ Y  mapping a vector from the tangent space TyM
to the associated point on the manifold according to the Riemannian metric [27]. For completeness 
the algorithm is recalled in ??. For this family of gradient-based algorithms it is possible to substitute
the exponential map with a retraction Ry : TyM → M  which is a ﬁrst order approximation to the
exponential map. Retractions are often faster to compute and still offer convergence guarantees [1].
In the following experiments we will use both retractions and exponential maps. We mention that the
step size ηt can be found with a line search over the validation set  for more see [1]. Note that also
stochastic optimization algorithms have been generalized to the Riemannian setting such as R-SGD
[8]. Interestingly  methods such as R-SGD can be advantageous in our setting. Indeed  solving the
inference in Eq. (2) requires solving the minimization of a sum over n elements  it might be favorable
from the computational perspective to adopt a stocastic method that at each iteration minimizes the
functional with respect to a single (or a mini-batch) of randomly sampled elements of the entire sum.

6

Table 2: Simulation experiment: average squared loss (First two columns) and (cid:52)PD (Last two columns) error of
the proposed structured prediction (SP) approach and the KRLS baseline on learning the inverse of a PD matrix
for increasing matrix dimension.

Squared loss
SP

KRLS

0.72±0.08
0.81±0.03
0.83±0.03
0.85±0.02
0.87±0.01
0.88±0.01

0.89±0.08
0.92±0.05
0.91±0.06
0.91±0.03
0.91±0.02
0.91±0.02

Dim
5
10
15
20
25
30

KRLS
111±64
44±8.3
56±10
59±12
72±9
67±7.2

(cid:52)PD loss
SP

0.94±0.06
1.24±0.06
1.25±0.05
1.33±0.03
1.44±0.03
1.55±0.03

Table 1 reports gradients and retraction maps for the geodesic distance of two problems of interest
considered in this work: positive deﬁnite manifold and the sphere. See Sections 4.2 and 4.3 for more
details on the related manifolds.
We point out that using optimization algorithms that comply with the geometry of the manifold 
such as RGD  guarantees that the computed value is an element of the manifold. This is in contrast
with algorithms that compute a solution in a linear space that contains M and then need to project
the computed solution onto M. We next discuss empirical evaluations of the proposed manifold
structured estimator on both synthetic and real datasets.

4.2 Synthetic Experiments: Learning Positive Deﬁnite Matrices
We consider the problem of learning a function f : Rd → Y = P m
positive deﬁnite (PD) m × m matrices. Note that P m
(cid:52)PD between any two PD matrices Z  Y ∈ P m

++ deﬁned as

++ denotes the cone of
++ is a manifold with squared geodesic distance

++  where P m

(cid:52)PD(Z  Y ) = (cid:107) log(Y − 1

F

2 Z Y − 1

2 )(cid:107)2

(16)

++  we have that M 1

where  for any M ∈ P m
2 and log(M ) correspond to the matrices with same
eigenvectors of M but with respectively the square root and logarithm of the eigenvalues of M.
In Table 1 we show the computation of the structured loss  the gradient of the structured loss and
the exponential map of the PD matrix manifold. We refer the reader to [31  6] for a more detailed
introduction on the manifold of positive deﬁnite matrices.
For the experiments reported in the following we compared the performance of the manifold struc-
tured estimator minimizing the loss (cid:52)PD and a Kernel Regularized Least Squares classiﬁer (KRLS)
baseline (see Appendix ??)  both trained using the Gaussian kernel k(x  x(cid:48)) = exp(−(cid:107)x−x(cid:48)(cid:107)2/2σ2).
The matrices predicted by the KRLS estimator are projected on the PD manifold by setting to a
small positive constant (1e − 12) the negative eigenvalues. For the manifold structured estimator 
the optimization problem at Eq. (2) was performed with the Riemannian Gradient Descent (RGD)
algorithm [1]. We refer to [52] regarding the implementation of the RGD in the case of the geodesic
distance on the PD cone.

Learning the Inverse of a Positive Deﬁnite Matrix. We consider the problem of learning the
++ → P m
++ such that f (X) = X−1 for any X ∈ P m
function f : P m
++. Input matrices are generated
as Xi = U ΣU(cid:62) ∈ P m
++ with U a random orthonormal matrix sampled from the Haar distribution
[16] and S ∈ P m
++ a diagonal matrix with entries randomly sampled from the uniform distribution
on [0  10]. We generated datasets of increasing dimension m from 5 to 50  each with 1000 points
for training  100 for validation and 100 for testing. The kernel bandwidth σ was chosen and the
regularization parameter λ were selected by cross-validation respectively in the ranges 0.1 to 1000
and 10−6 to 1 (logarithmically spaced).
Table 2 reports the performance of the manifold structured estimator (SP) and the KRLS baseline
with respect to both the (cid:52)PD loss and the least squares loss (normalized with respect to the number
of dimensions). Note that the KRLS estimator target is to minimize the least squares (Frobenius) loss
and is not designed to capture the geometry of the PD cone. We notice that the proposed approach
signiﬁcantly outperforms the KRLS baseline with respect to the (cid:52)PD loss. This is expected: (cid:52)PD

7

KRLS
MR[47]
SP (ours)

∆ Deg.
26.9 ± 5.4
22 ± 6
18.8 ± 3.9

Figure 1: (Left) Fingerprints reconstruction: Average absolute error (in degrees) for the manifold structured
estimator (SP)  the manifold regression (MR) approach in [47] and the KRLS baseline. (Right) Fingerprint
reconstruction of a single image where the structured predictor achieves 15.7 of average error while KRLS 25.3.

penalizes especially matrices with very different eigenvalues and our method cannot predict matrices
with non-positive eigenvalues  as opposed to KRLS which computes a linear solution in Rd2 and
then projects it onto the manifold. However the two methods perform comparably with respect to the
squared loss. This is consistent with the fact that our estimator is aware of the natural structure of the
output space and uses it proﬁtably during learning.

4.3 Fingerprint Reconstruction

We consider the ﬁngerprint reconstruction application in [47] in the context of manifold regression.
Given a partial image of a ﬁngerprint  the goal is to reconstruct the contour lines in output. Each
ﬁngerprint image is interpreted as a separate structured prediction problem where training input points
correspond to the 2D position x ∈ R2 of valid contour lines and the output is the local orientation
of the contour line  interpreted as a point on the circumference S1. The space S1 is a manifold with
squared geodesic distance (cid:52)S1 between two points z  y ∈ S1 corresponding to

(cid:52)S1 (z  y) = arccos ((cid:104)z  y(cid:105))2

(17)
where arccos is the inverse cosine function. In Table 1 we show the computation of the structured loss 
the gradient of the structured loss and the chosen retraction for the sphere manifold. We compared the
performance of the manifold structured estimator proposed in this paper with the manifold regression
approach in [47] on the FVC ﬁngerprint veriﬁcation challenge dataset3. The dataset consists of 48
ﬁngerprint pictures  each with ∼ 1400 points for training  ∼ 1000 points for validation and the rest
(∼ 25000) for test.
Fig. 1 reports the average absolute error (in degrees) between the true contour orientation and the one
estimated by our structured prediction approach (SP)  the manifold regression (MR) in [47] and the
KRLS baseline. Our method outperforms the MR competitor by a signiﬁcant margin. As expected 
the KRLS baseline is not able to capture the geometry of the output space and has a signiﬁcantly
larger error of the two other approaches. This is also observed on the qualitative plot in Fig. 1 (Left)
where the predictions of our SP approach and the KRLS baseline are compared with the ground truth
on a single ﬁngerprint. Output orientations are reported for each pixel with a color depending on their
orientation (from 0 to π). While the KRLS predictions are quite inconsistent  it can be noticed that
our estimator is very accurate and even “smoother” than the ground truth.

4.4 Multilabel Classiﬁcation on the Statistical Manifold

We evaluated our algorithm on multilabel prediction problems. In this context the output is an
m-dimensional histogram  i.e. a discrete probability distribution over m points. We consider as
manifold the space of probability distributions over m points  that is the m-dimensional simplex
∆m endowed with the Fisher information metric [3]. We will consider Y = ∆m
 where we require
y1  . . .   ym ≥   for  > 0. In the experiment we considered  = 1e− 5. The geodesic induced by the

3http://bias.csr.unibo.it/fvc2004

8

Table 3: Area under the curve (AUC) on multilabel benchmark datasets [51] for KRLS and SP.

Emotions
CAL500
Scene

KRLS
0.63
0.92
0.62

SP (Ours)

0.73
0.92
0.73

Fisher metric is  d(y  y(cid:48)) = arccos(cid:0)(cid:80)m

√

√

i=1

(cid:112)yiy(cid:48)

i

(cid:1)[35]. This geodesic comes from applying the map

y1  . . .  

ym+1) to the points {yi}n

π : ∆m → Sm−1  π(y) = (
i=1 ∈ ∆m. This results in points
++ and the sphere Sm−1. We can therefore
that belong to the intersection of the positive quadrant Rm
use the geodetic distance on the Sphere and gradient and retraction map described in Table 1. We
test our approach on some of the benchmark multilabel datasets described in [51] and we compare
the results with the KRLS baseline. We cross-validate λ and σ taking values  respectively  from the
intervals [10−6  10−1] and [0.1  10]. We compute the area under curve (AUC) [45] metric to evaluate
the quality of the predictions  results are shown in Table 3.

4.5 Additional Example

We conclude this section with a further relevant example of our proposed approach to the setting of
applications to relational knowledge  which we plan to investigate in future work. In particular  we
consider settings where the output space M corresponds to the Hyperbolic space (or Poincaré disk) 
which has recently been adopted by the knowledge representation community to learn embeddings
of relational data to encode discrete semantic/hierarchical information [34  33]. The embedding is
such that symbolic objects (e.g. words  entities  concepts) with high semantic or functional similarity
are mapped into points with small hyperbolic geodesic distance. Typically  learning the embedding
is a time consuming process that requires training from scratch on the whole dataset whenever a
new example is provided. To address this issue  with our manifold regression approach we could
learn f : X → M with x ∈ X the observed entity and f (x) its predicted embedding. This would
allow to transfer the embedding learned using techniques such as those in [34] to new points  without
retraining the entire system from scratch. Interestingly  our theory is applicable to this setting since
M satisﬁes Asm. 1.

5 Conclusions

In this paper we studied a structured prediction approach for manifold valued learning problems.
In particular we characterized a wide class of loss functions (including the geodesic distance) for
which we proved the considered algorithm to be statistically consistent  additionally providing ﬁnite
sample bounds under standard regularity assumptions. Our experiments show promising results on
synthetic and real data using two common manifolds: the positive deﬁnite matrices cone and the
sphere. With the latter we considered applications on ﬁngerprint reconstruction and multi-labeling.
The proposed method leads to some open questions. From a statistical point of view it is of interest
how invariants of the manifold explicitly affect the learning rates  see Remark 1. From a more
computational perspective  even if experimentally our algorithm achieves good results we did not
investigate convergence guarantees in terms of optimization.
Acknowledgments.
A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063). L. R. acknowl-
edges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European
Ofﬁce of Aerospace Research and Development)  and the EU H2020-MSCA-RISE project NoMADS - DLV-
777826. The work of L. R. and G. M. M. is supported by the Center for Brains  Minds and Machines (CBMM) 
funded by NSF STC award CCF-1231216  and the Italian Institute of Technology. We gratefully acknowledge
the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this
research. This work was supported in part by EPSRC grant EP/P009069/1.

9

References
[1] P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press  2009.

[2] Mauricio A Alvarez  Lorenzo Rosasco  Neil D Lawrence  et al. Kernels for vector-valued functions: A

review. Foundations and Trends R(cid:13) in Machine Learning  4(3):195–266  2012.

[3] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry  volume 191. American

Mathematical Soc.  2007.

[4] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society 

68(3):337–404  1950.

[5] GH Bakir  T Hofmann  B Schölkopf  AJ Smola  B Taskar  and SVN Vishwanathan. Predicting structured

data. neural information processing  2007.

[6] Rajendra Bhatia. Positive deﬁnite matrices. Princeton university press  2009.

[7] Veli Bicer  Thanh Tran  and Anna Gossen. Relational kernel machines for learning from graph-structured

rdf data. In Extended Semantic Web Conference  pages 47–62. Springer  2011.

[8] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic

Control  58(9):2217–2229  2013.

[9] Raffaello Camoriano  Tomás Angles  Alessandro Rudi  and Lorenzo Rosasco. Nytro: When subsampling

meets early stopping. In Artiﬁcial Intelligence and Statistics  pages 1403–1411  2016.

[10] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.

Foundations of Computational Mathematics  7(3):331–368  2007.

[11] Luigi Carratino  Alessandro Rudi  and Lorenzo Rosasco. Learning with sgd and random features. Advances

in Neural Information Processing Systems  2018.

[12] Carlo Ciliberto  Francis Bach  and Alessandro Rudi. Localized structured prediction. arXiv preprint

arXiv:1806.02402  2018.

[13] Carlo Ciliberto  Lorenzo Rosasco  and Alessandro Rudi. A consistent regularization approach for structured

prediction. Advances in Neural Information Processing Systems 29 (NIPS)  pages 4412–4420  2016.

[14] Carlo Ciliberto  Alessandro Rudi  Lorenzo Rosasco  and Massimiliano Pontil. Consistent multitask learning
with nonlinear output relations. In Advances in Neural Information Processing Systems  pages 1986–1996 
2017.

[15] Harold Charles Daume and Daniel Marcu. Practical structured learning techniques for natural language

processing. Citeseer  2006.

[16] Joe Diestel and Angela Spalsbury. The joys of Haar measure. American Mathematical Soc.  2014.

[17] Moussab Djerrab  Alexandre Garcia  Maxime Sangnier  and Florence d’Alché Buc. Output ﬁsher embed-

ding regression. Machine Learning  pages 1–28  2018.

[18] John C Duchi  Lester W Mackey  and Michael I Jordan. On the consistency of ranking algorithms. In

ICML  pages 327–334  2010.

[19] P Thomas Fletcher. Geodesic regression and the theory of least squares on riemannian manifolds. Interna-

tional journal of computer vision  105(2):171–185  2013.

[20] Wolfgang Härdle and Léopold Simar. Applied multivariate statistical analysis  volume 22007. Springer 

2007.

[21] Søren Hauberg  Oren Freifeld  and Michael J Black. A geometric take on metric learning. In Advances in

Neural Information Processing Systems  pages 2024–2032  2012.

[22] Emmanuel Hebey. Nonlinear analysis on manifolds: Sobolev spaces and inequalities  volume 5. American

Mathematical Soc.  2000.

[23] Jacob Hinkle  Prasanna Muralidharan  P Thomas Fletcher  and Sarang Joshi. Polynomial regression on

riemannian manifolds. In European Conference on Computer Vision  pages 1–14. Springer  2012.

10

[24] Mohammed Waleed Kadous and Claude Sammut. Classiﬁcation of multivariate time series and structured

data using constructive induction. Machine learning  58(2):179–216  2005.

[25] Anna Korba  Alexandre Garcia  and Florence d’Alché Buc. A structured prediction approach for label

ranking. Advances in Neural Information Processing Systems  2018.

[26] Quoc V Le  Tim Sears  and Alexander J Smola. Nonparametric quantile estimation. 2005.

[27] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds  pages 1–29. Springer  2003.

[28] Junhong Lin  Alessandro Rudi  Lorenzo Rosasco  and Volkan Cevher. Optimal rates for spectral algorithms
with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis  2018.

[29] Giulia Luise  Alessandro Rudi  Massimiliano Pontil  and Carlo Ciliberto. Differential properties of sinkhorn
approximation for learning with wasserstein distance. Advances in Neural Information Processing Systems 
2018.

[30] Charles A Micchelli and Massimiliano Pontil. On learning vector-valued functions. Neural computation 

17(1):177–204  2005.

[31] Maher Moakher and Philipp G Batchelor. Symmetric positive-deﬁnite matrices: From geometry to
applications and visualization. In Visualization and Processing of Tensor Fields  pages 285–298. Springer 
2006.

[32] Jeffrey S Morris. Functional regression. Annual Review of Statistics and Its Application  2:321–359  2015.

[33] Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic

geometry. International Conference on Machine Learning  2018.

[34] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In

Advances in neural information processing systems  pages 6338–6347  2017.

[35] Frank Nielsen and Ke Sun. Clustering in hilbert simplex geometry. arXiv preprint arXiv:1704.00454 

2017.

[36] Alex Nowak-Vila  Francis Bach  and Alessandro Rudi. Sharp analysis of learning with discrete losses.

arXiv preprint arXiv:1810.06839  2018.

[37] Sebastian Nowozin  Christoph H Lampert  et al. Structured learning and prediction in computer vision.

Foundations and Trends R(cid:13) in Computer Graphics and Vision  6(3–4):185–365  2011.

[38] Benjamin Paaßen  Christina Göpfert  and Barbara Hammer. Time series prediction for graphs in kernel

and dissimilarity spaces. Neural Processing Letters  pages 1–21  2017.

[39] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural

information processing systems  pages 1177–1184  2008.

[40] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström computational

regularization. In Advances in Neural Information Processing Systems  pages 1657–1665  2015.

[41] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. Falkon: An optimal large scale kernel method.

In Advances in Neural Information Processing Systems  pages 3888–3898  2017.

[42] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In

Advances in Neural Information Processing Systems  pages 3215–3225  2017.

[43] John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university

press  2004.

[44] Alex J Smola and Bernhard Schölkopf. Sparse greedy matrix approximation for machine learning. 2000.

[45] Ashwin Srinivasan. Note on the location of optimal classiﬁers in n-dimensional roc space. 1999.

[46] Florian Steinke and Matthias Hein. Non-parametric regression between manifolds. In Advances in Neural

Information Processing Systems  pages 1561–1568  2009.

[47] Florian Steinke  Matthias Hein  and Bernhard Schölkopf. Nonparametric regression between general

riemannian manifolds. SIAM Journal on Imaging Sciences  3(3):527–563  2010.

[48] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media 

2008.

11

[49] Ingo Steinwart  Don R Hush  Clint Scovel  et al. Optimal rates for regularized least squares regression. In

COLT  2009.

[50] François Treves. Topological Vector Spaces  Distributions and Kernels: Pure and Applied Mathematics 

volume 25. Elsevier  2016.

[51] Grigorios Tsoumakas  Ioannis Katakis  and Ioannis Vlahavas. Mining multi-label data. In Data mining

and knowledge discovery handbook  pages 667–685. Springer  2009.

[52] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on

Learning Theory  pages 1617–1638  2016.

12

,Alessandro Rudi
Carlo Ciliberto
GianMaria Marconi
Lorenzo Rosasco