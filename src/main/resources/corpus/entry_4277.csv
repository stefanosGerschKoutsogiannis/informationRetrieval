2008,Phase transitions for high-dimensional joint support recovery,We consider the following instance of transfer learning: given a pair of regression problems  suppose that the regression coefficients share a partially common support  parameterized by the overlap fraction $\overlap$ between the two supports. This set-up suggests the use of $1  \infty$-regularized linear regression for recovering the support sets of both regression vectors. Our main contribution is to provide a sharp characterization of the sample complexity of this $1 \infty$ relaxation  exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\pdim$  support size $\spindex$ and overlap $\overlap \in [0 1]$. For measurement matrices drawn from standard Gaussian ensembles  we prove that the joint $1 \infty$-regularized method undergoes a phase transition characterized by order parameter $\orpar(\numobs  \pdim  \spindex  \overlap) = \numobs{(4 - 3 \overlap) s \log(p-(2-\overlap)s)}$. More precisely  the probability of successfully recovering both supports converges to $1$ for scalings such that $\orpar > 1$  and converges to $0$ to scalings for which $\orpar < 1$. An implication of this threshold is that use of $1  \infty$-regularization leads to gains in sample complexity if the overlap parameter is large enough ($\overlap > 2/3$)  but performs worse than a naive approach if $\overlap < 2/3$. We illustrate the close agreement between these theoretical predictions  and the actual behavior in simulations. Thus  our results illustrate both the benefits and dangers associated with block-$1 \infty$ regularization in high-dimensional inference.,Joint support recovery under high-dimensional scaling:

Beneﬁts and perils of `1 ∞-regularization

Department of Electrical Engineering and Computer Sciences

Sahand Negahban

University of California  Berkeley

Berkeley  CA 94720-1770

sahand n@eecs.berkeley.edu

Department of Statistics  and Department of Electrical Engineering and Computer Sciences

Martin J. Wainwright

University of California  Berkeley

Berkeley  CA 94720-1770

wainwrig@eecs.berkeley.edu

Abstract

Given a collection of r ≥ 2 linear regression problems in p dimensions  suppose that the
regression coefﬁcients share partially common supports. This set-up suggests the use of
`1/`∞-regularized regression for joint estimation of the p × r matrix of regression coefﬁ-
cients. We analyze the high-dimensional scaling of `1/`∞-regularized quadratic program-
ming  considering both consistency rates in `∞-norm  and also how the minimal sample size
n required for performing variable selection grows as a function of the model dimension 
sparsity  and overlap between the supports. We begin by establishing bounds on the `∞-
error as well sufﬁcient conditions for exact variable selection for ﬁxed design matrices  as
well as designs drawn randomly from general Gaussian matrices. These results show that the
high-dimensional scaling of `1/`∞-regularization is qualitatively similar to that of ordinary
`1-regularization. Our second set of results applies to design matrices drawn from standard
Gaussian ensembles  for which we provide a sharp set of necessary and sufﬁcient conditions:
the `1/`∞-regularized method undergoes a phase transition characterized by the rescaled sam-
ple size θ1 ∞(n  p  s  α) = n/{(4 − 3α)s log(p − (2 − α) s)}. More precisely  for any δ > 0 
the probability of successfully recovering both supports converges to 1 for scalings such that
θ1 ∞ ≥ 1 + δ  and converges to 0 for scalings for which θ1 ∞ ≤ 1− δ. An implication of this
threshold is that use of `1 ∞-regularization yields improved statistical efﬁciency if the overlap
parameter is large enough (α > 2/3)  but performs worse than a naive Lasso-based approach
for moderate to small overlap (α < 2/3). We illustrate the close agreement between these
theoretical predictions  and the actual behavior in simulations.

1 Introduction

The area of high-dimensional statistical inference is concerned with the behavior of models and algorithms in
which the dimension p is comparable to  or possibly even larger than the sample size n. In the absence of addi-
tional structure  it is well-known that many standard procedures—among them linear regression and principal
component analysis—are not consistent unless the ratio p/n converges to zero. Since this scaling precludes hav-
ing p comparable to or larger than n  an active line of research is based on imposing structural conditions on the
data—for instance  sparsity  manifold constraints  or graphical model structure—and then studying conditions
under which various polynomial-time methods are either consistent  or conversely inconsistent.

1

This paper deals with high-dimensional scaling in the context of solving multiple regression problems  where
the regression vectors are assumed to have shared sparse structure. More speciﬁcally  suppose that we are
given a collection of r different linear regression models in p dimensions  with regression vectors β i ∈ Rp  for
i = 1  . . .   r. We let S(β i) = {j | β i
j 6= 0} denote the support set of β i. In many applications—among them
sparse approximation  graphical model selection  and image reconstruction—it is natural to impose a sparsity
constraint  corresponding to restricting the cardinality |S(β i)| of each support set. Moreover  one might expect
some amount of overlap between the sets S(β i) and S(β j) for indices i 6= j since they correspond to the
sets of active regression coefﬁcients in each problem. For instance  consider the problem of image denoising
or reconstruction  using wavelets or some other type of multiresolution basis. It is well known that natural
images tend to have sparse representations in such bases. Moreover  similar images—say the same scene taken
from multiple cameras—would be expected to share a similar subset of active features in the reconstruction.
Similarly  in analyzing the genetic underpinnings of a given disease  one might have results from different
subjects and/or experiments  meaning that the covariate realizations and regression vectors would differ in their
numerical values  but one expects the same subsets of genes to be active in controlling the disease  which
translates to a condition of shared support in the regression coefﬁcients. Given these structural conditions of
shared sparsity in these and other applications  it is reasonable to consider how this common structure can be
exploited so as to increase the statistical efﬁciency of estimation procedures.
In this paper  we study the high-dimensional scaling of block `1/`∞ regularization. Our main contribution is
to obtain some precise—and arguably surprising—insights into the beneﬁts and dangers of using block `1/`∞
regularization  as compared to simpler `1-regularization (separate Lasso for each regression problem). We
begin by providing a general set of sufﬁcient conditions for consistent support recovery for both ﬁxed design
matrices  and random Gaussian design matrices. In addition to these basic consistency results  we then seek to
characterize rates  for the particular case of standard Gaussian designs  in a manner precise enough to address
the following questions.

(a) First  under what structural assumptions on the data does the use of `1/`∞ block-regularization provide
a quantiﬁable reduction in the scaling of the sample size n  as a function of the problem dimension p
and other structural parameters  required for consistency?

(b) Second  are there any settings in which `1/`∞ block-regularization can be harmful relative to compu-

tationally less expensive procedures?

Answers to these questions yield useful insight into the tradeoff between computational and statistical efﬁ-
ciency.
Indeed  the convex programs that arise from using block-regularization typically require a greater
computational cost to solve. Accordingly  it is important to understand under what conditions this increased
computational cost guarantees that fewer samples are required for achieving a ﬁxed level of statistical accuracy.

As a representative instance of our theory  consider the special case of standard Gaussian design matrices and
two regression problems (r = 2)  with the supports S(β 1) and S(β 2) each of size s and overlapping in a
fraction α ∈ [0  1] of their entries. For this problem  we prove that block `1/`∞ regularization undergoes a
phase transition in terms of the rescaled sample size

θ1 ∞(n  p  s  α)

: =

n

(4 − 3α)s log(p − (2 − α)s)

.

(1)

In words  for any δ > 0 and for scalings of the quadruple (n  p  s  α) such that θ1 ∞ ≥ 1 + δ  the probability of
successfully recovering both S(β 1) and S(β 2) converges to one  whereas for scalings such that θ1 ∞ ≤ 1 − δ 
the probability of success converges to zero. By comparison to previous theory on the behavior of the Lasso
(ordinary `1-regularized quadratic programming)  the scaling (1) has two interesting implications. For the s-
sparse regression problem with standard Gaussian designs  the Lasso has been shown [10] to undergo a phase
transition as a function of the rescaled sample size

θLas(n  p  s)

: =

 

(2)

so that solving two separate Lasso problems  one for each regression problem  would recover both supports for
problem sequences (n  p  s) such that θLas > 1. Thus  one consequence of our analysis is to provide a precise
conﬁrmation of the natural intuition: if the data is well-aligned with the regularizer  then block-regularization
increases statistical efﬁciency. On the other hand  our analysis also conveys a cautionary message: if the overlap
is too small—more precisely  if α < 2/3—then block `1 ∞ is actually harmful relative to the naive Lasso-based
approach. This fact illustrates that some care is required in the application of block regularization schemes.

n

2s log(p − s)

2

In Section 2  we provide a precise description of the
The remainder of this paper is organized as follows.
problem. Section 3 is devoted to the statement of our main result  some discussion of its consequences  and
illustration by comparison to empirical simulations.

2 Problem set-up

We begin by setting up the problem to be studied in this paper  including multivariate regression and family of
block-regularized programs for estimating sparse vectors.

2.1 Multivariate regression

In this problem  we consider the following form of multivariate regression. For each i = 1  . . .   r  let βi ∈ Rp
be a regression vector  and consider the family of linear observation models
(3)
i = 1  2  . . .   r.
Here each X i ∈ Rn×p is a design matrix  possibly different for each vector βi  and wi ∈ Rn is a noise vector.
We assume that the noise vectors wi and wj are independent for different regression problems i 6= j. In this
paper  we assume that each wi has a multivariate Gaussian N (0  σ2In×n) distribution. However  we note that
qualitatively similar results will hold for any noise distribution with sub-Gaussian tails (see the book [1] for
more background).

yi = X iβi + wi 

2.2 Block-regularization schemes

For compactness in notation  we frequently use B to denote the p × r matrix with β i ∈ Rp as the ith column.
Given a parameter q ∈ [1 ∞]  we deﬁne the `1/`q block-norm as follows:
k  . . .   βr

k  β2

(4)

: =

kBk`1/`q

k)kq 

corresponding to applying the `q norm to each row of B  and the `1-norm across all of these blocks. We note
that all of these block norms are special cases of the CAP family of penalties [12].

This family of block-regularizers (4) suggests a natural family of M-estimators for estimating B  based on
solving the block-`1/`q-regularized quadratic program

pXk=1

k(β1

(5)

bB ∈ arg min

B∈Rp×r(cid:8) 1

2n

rXi=1

kyi − X iβik2

2 + λnkBk`1/`q(cid:9) 

where λn > 0 is a user-deﬁned regularization parameter. Note that the data term is separable across the different
regression problems i = 1  . . .   r  due to our assumption of independence on the noise vectors. Any coupling
between the different regression problems is induced by the block-norm regularization.

In the special case of univariate regression (r = 1)  the parameter q plays no role  and the block-regularized
scheme (6) reduces to the Lasso [7  3]. If q = 1 and r ≥ 2  the block-regularization function (like the data
term) is separable across the different regression problems i = 1  . . .   r  and so the scheme (6) reduces to
solving r separate Lasso problems. For r ≥ 2 and q = 2  the program (6) is frequently referred to as the group
Lasso [11  6]. Another important case [9  8]  and the focus of this paper  is block `1/`∞ regularization.
The motivation for using block `1/`∞ regularization is to encourage shared sparsity among the columns of the
regression matrix B. Geometrically  like the `1 norm that underlies the ordinary Lasso  the `1/`∞ block norm
has a polyhedral unit ball. However  the block norm captures potential interactions between the columns βi
k) in any given row
in the matrix B. Intuitively  taking the maximum encourages the elements (β1
k = 1  . . .   p to be zero simultaneously  or to both be non-zero simultaneously. Indeed  if βi
k 6= 0 for at least
one i ∈ {1  . . .   r}  then there is no additional penalty to have βj
k 6= 0 as well  as long as |βj
k|.
k| ≤ |βi
2.3 Estimation in `∞ norm and support recovery
For a given λn > 0  suppose that we solve the block `1/`∞ program  thereby obtaining an estimate

k . . .   βr

k  β2

bB ∈ arg min

B∈Rp×r(cid:8) 1

2n

rXi=1

kyi − X iβik2

2 + λnkBk`1/`∞(cid:9) 

3

(6)

We note that under high-dimensional scaling (p (cid:29) n)  this convex program (6) is not necessarily strictly
convex  since the quadratic term is rank deﬁcient and the block `1/`∞ norm is polyhedral  which implies that
the program is not strictly convex. However  a consequence of our analysis is that under appropriate conditions 
the optimal solution bB is in fact unique.
In this paper  we study the accuracy of the estimate bB  as a function of the sample size n  regression dimensions
p and r  and the sparsity index s = maxi=1 ... r |S(β i)|. There are various metrics with which to assess the
“closeness” of the estimate bB to the truth B  including predictive risk  various types of norm-based bounds on
the difference bB − B  and variable selection consistency. In this paper  we prove results bounding the `∞/`∞
In addition  we prove results on support recovery criteria. Recall that for each vector β i ∈ Rp  we use S(β i) =
k 6= 0} to denote its support set. The problem of union support recovery corresponds to recovering the
{k | β i
set

kbB − Bk`∞/`∞ : = max

i=1 ... r |bBi

k − Bi
k|.

difference

k=1 ... p

max

J : =

S(β i) 

(7)

r[i=1

corresponding to the subset J ⊆ {1  . . .   p} of indices that are active in at least one regression problem. Note
that the cardinality of |J| is upper bounded by rs  but can be substantially smaller (as small as s) if there is
overlap among the different supports.

In some results  we also study the more reﬁned criterion of recovering the individual signed supports  meaning
the signed quantities sign(β i

k)  where the sign function is given by

sign(t) = 

+1 if t > 0
if t = 0
0
−1 if t < 0

(8)

(10)

There are multiple ways in which the support (or signed support) can be estimated  depending on whether we
use primal or dual information from an optimal solution.

`1/`∞ primal recovery: Solve the block-regularized program (6)  thereby obtaining a (primal) optimal solu-

tion bB ∈ Rp×r  and estimate the signed support vectors
`1/`∞ dual recovery: Solve the block-regularized program (6)  thereby obtaining an primal solution bB ∈
k|. Estimate the signed support via:

[Spri(bβ i)]k = sign(bβ i

Rp×r. For each row k = 1  . . .   p  compute the set Mk : = arg max

k).

(9)

i=1 ... r |bβ i
if i ∈ Mk
otherwise.

k)

k)] = (sign(bβ i

0

[Sdua(bβ i

As our development will clarify  this procedure corresponds to estimating the signed support on the basis of a
dual optimal solution associated with the optimal primal solution.

2.4 Notational conventions

Throughout this paper  we use the index p ∈ {1  . . .   r} as a superscript in indexing the different regression
problems  or equivalently the columns of the matrix B ∈ Rp×r. Given a design matrix X ∈ Rn×p and a subset
S ⊆ {1  . . .   p}  we use XS to denote the n × |S| sub-matrix obtained by extracting those columns indexed by
S. For a pair of matrices A ∈ Rm×` and B ∈ Rm×n  we use the notation(cid:10)A  B(cid:11) : = AT B for the resulting
` × n matrix.
We use the following standard asymptotic notation: for functions f  g  the notation f (n) = O(g(n)) means that
there exists a ﬁxed constant 0 < C < +∞ such that f (n) ≤ Cg(n); the notation f (n) = Ω(g(n)) means that
f (n) ≥ Cg(n)  and f (n) = Θ(g(n)) means that f (n) = O(g(n)) and f (n) = Ω(g(n)).

4

3 Main results and their consequences

In this section  we provide precise statements of the main results of this paper. Our ﬁrst main result (Theorem 1)
provides sufﬁcient conditions for deterministic design matrices X 1  . . .   X r. This result allows for an arbitrary
number r of regression problems. Not surprisingly  these results show that the high-dimensional scaling of block
`1/`∞ is qualitiatively similar to that of ordinary `1-regularization: for instance  in the case of random Gaussian
designs and bounded r  our sufﬁcient conditions in [5] ensure that n = Ω(s log p) samples are sufﬁcient to
recover the union of supports correctly with high probability  which matches known results on the Lasso [10].

As discussed in the introduction  we are also interested in the more reﬁned question: can we provide nec-
essary and sufﬁcient conditions that are sharp enough to reveal quantitative differences between ordinary `1-
regularization and block regularization? In order to provide precise answers to this question  our ﬁnal two results
concern the special case of r = 2 regression problems  both with supports of size s that overlap in a fraction α
of their entries  and with design matrices drawn randomly from the standard Gaussian ensemble. In this setting 
our ﬁnal two results (Theorem 2 and 3) show that block `1/`∞ regularization undergoes a phase transition
speciﬁed by the rescaled sample size. We then discuss some consequences of these results  and illustrate their
sharpness with some simulation results.

3.1 Sufﬁcient conditions for deterministic designs

In addition to the sample size n  problem dimensions p and r  and sparsity index s  our results are stated in
terms of the minimum eigenvalue Cmin of the |J| × |J| matrices 1

Ji—that is 

nhX i

J   X i

as well as an `∞-operator norm of their inverses:

J   X i

Ji(cid:1) ≥ Cmin

λmin(cid:0) 1
nhX i
|||(cid:0) 1
nhX i

J   X i

Ji(cid:1)−1

for all i = 1  . . .   r 

|||∞ ≤ Dmax

for all i = 1  . . .   r.

(11)

(12)

(14)

It is natural to think of these quantites as being constants (independent of p and s)  although our results do allow
them to scale.

We assume that the columns of each design matrix X i  i = 1  . . .   r are normalized so that

kX i

kk2

2 ≤ 2n

for all k = 1  2  . . . p.

(13)

The choice of the factor 2 in this bound is for later technical convenience. We also require the following
incoherence condition on the design matrix is satisiﬁed: there exists some γ ∈ (0  1] such that

Ji)−1(cid:11)k1 ≤ (1 − γ) 
and we also deﬁne the support minimum value Bmin = mink∈J maxi=1 ... r |β i
k| 
For a parameter ξ > 1 (to be chosen by the user)  we deﬁne the probability

k(cid:10)X i

J (hX i

`=1 ... |J c|

J   X i

`  X i

max

rXi=1

(15)

γ2

n

r2+r log(p)

φ1(ξ  p  s)

n ≥ 4ξσ2

: = 1 − 2 exp(−(ξ − 1)[r + log p]) − 2 exp(−(ξ2 − 1) log(rs))
which speciﬁes the precise rate with which the “high probability” statements in Theorem 1 hold.
Theorem 1. Consider the observation model (3) with design matrices X i satisfying the column bound (13) and
incoherence condition (14). Suppose that we solve the block-regularized `1/`∞ convex program (6) with regu-
larization parameter ρ2
for some ξ > 1. Then with probability greater than φ1(ξ  p  s) → 1 
we are guaranteed that:
i=1 S(bβ i) ⊆ J  and it satisﬁes the

(a) The block-regularized program has a unique solution bB such thatSr
{z

k| ≤ ξs 4σ2

k=1 ... p|bβ i

+ Dmax ρn

.

(16)

elementwise bound

b1(ξ  ρn  n  s)

k − β i

max
i=1 ... r

log(rs)

Cmin

max

|

}

n

5

the union of supports J.

(b) If in addition Bmin ≥ b1(ξ  ρn  n  s)  thenSr

i=1 S(bβ i) = J  so that the solution bB correctly speciﬁes
Remarks: To clarify the scope of the claims  part (a) guarantees that the estimator recovers the union support
J correctly  whereas part (b) guarantees that for any given i = 1  . . .   r and k ∈ S(β i)  the sign sign(bβ i
k) is
correct. Note that we are guaranteed that bβ i
k = 0 for all k /∈ J. However  within the union support J  when
using primal recovery method  it is possible to have false non-zeros—i.e.  there may be an index k ∈ J\S(β i)
such that bβ i
k 6= 0. Of course  this cannot occur if the support sets S(β i) are all equal. This phenomenon is
related to geometric properties of the block `1/`∞ norm: in particular  for any given index k  when bβ j
k 6= 0 for
some j ∈ {1  . . .   r}  then there is no further penalty to havingbβ i
k 6= 0 for other column indices i 6= j.
The dual signed support recovery method (10) is more conservative in estimating the individual support sets.
In particular  for any given i ∈ {1  . . .   r}  it only allows an index k to enter the signed support estimate
Sdua(bβ i) when |bβ i
k| achieves the maximum magnitude (possibly non-unique) across all indices i = 1  . . .   r.
Consequently  Theorem 1 guarantees that the dual signed support method will never include an index in the
individual supports. However  it may incorrectly exclude indices of some supports  but like the primal support
estimator  it is always guaranteed to correctly recover the union of supports J.
We note that it is possible to ensure that under some conditions that the dual support method will correctly
recover each of the individual signed supports  without any incorrect exclusions. However  as illustrated by
Theorem 2  doing so requires additional assumptions on the size of the gap |β i
k | for indices k ∈ B : =
S(β i) ∩ S(β j).
3.2 Sharp results for standard Gaussian ensembles

k| − |β j

Our results thus far show under standard mutual incoherence or irrepresentability conditions  the block `1/`∞
method produces consistent estimators for n = Ω(s log(p− s)). In qualitative terms  these results match known
scaling for the Lasso  or ordinary `1-regularization. In order to provide keener insight into the (dis)advantages
associated with using `1/`∞ block regularization  we specialize the remainder of our analysis to the case of
r = 2 regression problems  where the corresponding design matrices X i  i = 1  2 are sampled from the standard
Gaussian ensemble [2  4]—i.e.  with i.i.d. rows N (0  Ip×p). Our goal in studying this special case is to be able
to make quantiative comparisons with the Lasso.

n

ρnkT (Bgap)k∞  where T (Bgap) = ρn ∧ Bgap.

We consider a sequence of models indexed by the triplet (p  s  α)  corresponding to the problem dimension
p  support sizes s. and overlap parameter α ∈ [0  1]. We assume that s ≤ p/2  capturing the intuition of a
(relatively) sparse model. Suppose that for a given model  we take n = n(p  s  α) observations. according to
equation (3). We can then study the probability of successful recovery as a function of the model triplet  and
the sample size n.
In order to state our main result  we deﬁne the order parameter or rescaled sample size θ1 ∞(n  p  s  α) : =

(4−3α)s log(p−(2−α)s) . We also deﬁne the support gap value as well as c∞-gap Bgap = (cid:12)(cid:12)|β 1

c∞ = 1
3.2.1 Sufﬁcient conditions
We begin with a result that provides sufﬁcient conditions for support recovery using block `1/`∞ regularization.
Theorem 2 (Achievability). Given the observation model (3) with random design X drawn with i.i.d. standard
Gaussian entries  and consider problem sequences (n  p  s  α) for which θ1 ∞(n  p  s  α) > 1 + δ for some
n and c∞ → 0   then with probability
(i) The block `1 ∞-program (6) has a unique solution (bβ 1 bβ 2)  with supports S(bβ 1) ⊆ J and S(bβ 2) ⊆

δ > 0. If we solve the block-regularized program (6) with ρn = ξq log p
greater than 1 − c1 exp(−c2 log(p − (2 − α)s))  the following properties hold:

J. Moreover  we have the elementwise bound

B|(cid:12)(cid:12)  and

B| − |β 2

(17)

max
i=1 2

k − β i

max

k=1 ... p|bβ i

k| ≤ ξr 100 log(s)

n

√n

+ ρn(cid:2) 4s
{z

+ 1(cid:3) 
}

b3(ξ  ρn  n  s)

|

6

(ii) If the support minimum Bmin > 2b3(ξ  ρn  n  s)  then the primal support method successfully recovers
the support union J = S(β 1)∪S(β 2). Moreover  using the primal signed support recovery method (9) 
we have
(18)

for all k ∈ S(β i).

[Spri(bβ i)]k = sign(β i

k)

3.2.2 Necessary conditions

We now turn to the question of ﬁnding matching necessary conditions for support recovery.
Theorem 3 (Lower bounds). Given the observation model (3) with random design X drawn with i.i.d. standard
Gaussian entries.

(a) For problem sequences (n  p  s  α) such that θ1 ∞(n  p  s  α) < 1 − δ for some δ > 0 and for any
non-increasing regularization sequence ρn > 0  no solution bB = (bβ 1 bβ 2) to the block-regularized
program (6) has the correct support union S(bβ 1) ∪ S(bβ 2).

(b) Recalling

c2(ρn  Bgap)

of Bgap 

deﬁnition

deﬁne

limit

the

rescaled

gap

the

: =

lim sup(n p s) kT (Bgap)k2

ρn√s

. If the sample size n is bounded as

for some δ > 0  then the dual recovery method (10) fails to recover the individual signed supports.

n < (1 − δ)(cid:2)(4 − 3α) + (c2(ρn  Bgap))2(cid:3)s log[p − (2 − α)s]

It is important to note that c∞ ≥ c2  which implies then that as long as c∞ → 0  then c2 → 0  so that the
conditions of Theorem 3(a) and (b) are equivalent. However  note that if c2 does not go to 0  then in fact  the
method could fail to recover the correct support even if θ1 ∞ > 1 + δ. This result is key to understanding the
`1 ∞-regularization term. The gap between the vectors plays a fundamental role in in reducing the sampling
complexity. Namely  if the gap is too large  then the sampling efﬁciency is greatly reduced as compared to if
the gap is very small. In summary  while (a) and (b) seem equivalent on the surface  the requirement in (b) is in
fact stronger than that in (a) and demonstrates the importance of condition (iii) in Theorem 2. It shows that if
the gap is too large  then correct joint support recovery is not possible.

3.3

Illustrative simulations and some consequences

In this section  we provide some illustrative simulations that illustrate the phase transitions predicted by The-
orems 2 and 3  and show that the theory provides an accurate description of practice even for relatively small
problem sizes (e.g.  p = 128). Figure 1 plots the probability of successful recovery of the individual signed sup-

ports using dual support recovery (10)—namely  P[Sdua(bβ i) = S±(β i)  Sdua(bβ 2) = S±(β 2)] for i = 1  2—
versus the order parameter θ1 ∞(n  p  s  α). The plot contains four sets of “stacked” curves  each corresponding
to a different choice of the overlap parameter  ranging from α = 1 (left-most stack)  to α = 0.1 (right-most
stack). Each stack contains three curves  corresponding to the problem sizes p ∈ {128  256  512}. In all cases 
we ﬁxed the support size s = 0.1p. The stacking behavior of these curves demonstrates that we have isolated
the correct order parameter  and the step-function behavior is consistent with the theoretical predictions of a
sharp threshold.

Theorems 2 and 3 have some interesting consequences  particularly in comparison to the behavior of the “naive”
Lasso-based individual decoding of signed supports—that is  the method that simply applies the Lasso (ordinary
`1-regularization) to each column i = 1  2 separately. By known results [10] on the Lasso  the performance of
this naive approach is governed by the order parameter

θLas(n  p  s) =

 

(19)

n

2s log(p − s)

meaning that for any δ > 0  it succeeds for sequences such that θLas > 1 + δ  and conversely fails for sequences
such that θLas < 1−δ. To compare the two methods  we deﬁne the relative efﬁciency coefﬁcient R(θ1 ∞  θLas) :
= θLas(n  p  s)/θ1 ∞(n  p  s  α). A value of R < 1 implies that the block method is more efﬁcient  while R > 1
implies that the naive method is more efﬁcient.

With this notation  we have the following:
Corollary 1. The relative efﬁciency of the block `1 ∞ program (6) compared to the Lasso is given by
R(θ1 ∞  θLas) = 4−3α
. Thus  for sublinear sparsity s/p → 0  the block scheme has greater
statistical efﬁciency for all overlaps α ∈ (2/3  1]  but lower statistical efﬁciency for overlaps α ∈ [0  2/3).

log(p−(2−α)s)

log(p−s)

2

7

1

0.8

s
s
e
c
c
u
s
 
.

b
o
r
P

0.6

0.4

0.2

 

0
0

`1 ∞ relaxation for s = 0.1*p and α = 1  0.7  0.4  0.1

 

α = 1
α = 1
α = 1

α = 0.7
α = 0.7
α = 0.7

α = 0.4
α = 0.4
α = 0.4

α = 0.1
α = 0.1
α = 0.1

1

2

Control parameter q

3

p = 128
p = 256
p = 512
5

4

Figure 1. Probability of success in recovering the joint signed supports plotted against the control parameter θ1
∞ =
n/[2s log(p − (2 − α)s))] for linear sparsity s = 0.1p. Each stack of graphs corresponds to a ﬁxed overlap α  as
labeled on the ﬁgure. The three curves within each stack correspond to problem sizes p{128  256  512}; note how
they all align with each other and exhibit step-like behavior  consistent with Theorems 2 and 3. The vertical lines
correspond to the thresholds θ∗
∞(α) predicted by Theorems 2 and 3; note the close agreement between theory and
simulation.

1

 

 

References

[1] V. V. Buldygin and Y. V. Kozachenko. Metric characterization of random variables and random processes.

American Mathematical Society  Providence  RI  2000.

[2] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals

of Statistics  2006.

[3] S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci.

Computing  20(1):33–61  1998.

[4] D. L. Donoho and J. M. Tanner. Counting faces of randomly-projected polytopes when the projection
radically lowers dimension. Technical report  Stanford University  2006. Submitted to Journal of the
AMS.

[5] S. Negahban and M. J. Wainwright. Joint support recovery under high-dimensional scaling: Beneﬁts and

perils of `1 ∞-regularization. Technical report  Department of Statistics  UC Berkeley  January 2009.

[6] G. Obozinski  B. Taskar  and M. Jordan. Joint covariate selection for grouped classiﬁcation. Technical

report  Statistics Department  UC Berkeley  2007.

[7] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[8] J. A. Tropp  A. C. Gilbert  and M. J. Strauss. Algorithms for simultaneous sparse approximation. Sig-
nal Processing  86:572–602  April 2006. Special issue on ”Sparse approximations in signal and image
processing”.

[9] B. Turlach  W.N. Venables  and S.J. Wright. Simultaneous variable selection. Technometrics  27:349–363 

2005.

[10] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity using using `1-

constrained quadratic programs. Technical Report 709  Department of Statistics  UC Berkeley  2006.

[11] Kim Y.  Kim J.  and Y. Kim. Blockwise sparse regression. Statistica Sinica  16(2)  2006.
[12] P. Zhao  G. Rocha  and B. Yu. Grouped and hierarchical model selection through composite absolute

penalties. Technical report  Statistics Department  UC Berkeley  2007.

8

,Emily Denton
Soumith Chintala
arthur szlam
Rob Fergus
Wei Chen
Wei Hu
Fu Li
Jian Li
Yu Liu
Pinyan Lu