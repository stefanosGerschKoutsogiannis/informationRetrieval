2015,Combinatorial Bandits Revisited,This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback  we derive a problem-specific regret lower bound  and discuss its scaling with the dimension of the decision space. We propose ESCB  an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms  and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback  we propose CombEXP  an algorithm with the same regret scaling as state-of-the-art algorithms  but with lower computational complexity for some combinatorial problems.,Combinatorial Bandits Revisited

Richard Combes∗

M. Sadegh Talebi†

Alexandre Proutiere†
∗ Centrale-Supelec  L2S  Gif-sur-Yvette  FRANCE

Marc Lelarge‡

† Department of Automatic Control  KTH  Stockholm  SWEDEN

‡ INRIA & ENS  Paris  FRANCE

richard.combes@supelec.fr {mstms alepro}@kth.se marc.lelarge@ens.fr

Abstract

This paper investigates stochastic and adversarial combinatorial multi-armed ban-
dit problems.
In the stochastic setting under semi-bandit feedback  we derive
a problem-speciﬁc regret lower bound  and discuss its scaling with the dimen-
sion of the decision space. We propose ESCB  an algorithm that efﬁciently ex-
ploits the structure of the problem and provide a ﬁnite-time analysis of its regret.
ESCB has better performance guarantees than existing algorithms  and signiﬁ-
cantly outperforms these algorithms in practice. In the adversarial setting under
bandit feedback  we propose COMBEXP  an algorithm with the same regret scal-
ing as state-of-the-art algorithms  but with lower computational complexity for
some combinatorial problems.

1

Introduction

receives a reward M(cid:62)X(n) = (cid:80)d

Multi-Armed Bandit (MAB) problems [1] constitute the most fundamental sequential decision prob-
lems with an exploration vs. exploitation trade-off. In such problems  the decision maker selects
an arm in each round  and observes a realization of the corresponding unknown reward distribution.
Each decision is based on past decisions and observed rewards. The objective is to maximize the
expected cumulative reward over some time horizon by balancing exploitation (arms with higher
observed rewards should be selected often) and exploration (all arms should be explored to learn
their average rewards). Equivalently  the performance of a decision rule or algorithm can be mea-
sured through its expected regret  deﬁned as the gap between the expected reward achieved by the
algorithm and that achieved by an oracle algorithm always selecting the best arm. MAB problems
have found applications in many ﬁelds  including sequential clinical trials  communication systems 
economics  see e.g. [2  3].
In this paper  we investigate generic combinatorial MAB problems with linear rewards  as introduced
in [4]. In each round n ≥ 1  a decision maker selects an arm M from a ﬁnite set M ⊂ {0  1}d and
+ is unknown.
We focus here on the case where all arms consist of the same number m of basic actions in the
sense that (cid:107)M(cid:107)1 = m  ∀M ∈ M. After selecting an arm M in round n  the decision maker
receives some feedback. We consider both (i) semi-bandit feedback under which after round n  for
all i ∈ {1  . . .   d}  the component Xi(n) of the reward vector is revealed if and only if Mi = 1; (ii)
bandit feedback under which only the reward M(cid:62)X(n) is revealed. Based on the feedback received
up to round n − 1  the decision maker selects an arm for the next round n  and her objective is to
maximize her cumulative reward over a given time horizon consisting of T rounds. The challenge in
these problems resides in the very large number of arms  i.e.  in its combinatorial structure: the size
of M could well grow as dm. Fortunately  one may hope to exploit the problem structure to speed
up the exploration of sub-optimal arms.
We consider two instances of combinatorial bandit problems  depending on how the sequence
of reward vectors is generated. We ﬁrst analyze the case of stochastic rewards  where for all

i=1 MiXi(n). The reward vector X(n) ∈ Rd

1

Algorithm

Regret

O(cid:16) m3d∆max

LLR
[9]

∆2

min

(cid:17) O(cid:16) m2d

CUCB

[10]

∆min

log(T )

(cid:17) O(cid:16) md

CUCB

[11]

∆min

log(T )

(cid:17) O(cid:16) √

ESCB

(Theorem 5)

md
∆min

log(T )

(cid:17)

log(T )

Table 1: Regret upper bounds for stochastic combinatorial optimization under semi-bandit feedback.

i ∈ {1  . . .   d}  (Xi(n))n≥1 are i.i.d. with Bernoulli distribution of unknown mean. The reward
sequences are also independent across i. We then address the problem in the adversarial setting
where the sequence of vectors X(n) is arbitrary and selected by an adversary at the beginning of
the experiment. In the stochastic setting  we provide sequential arm selection algorithms whose per-
formance exceeds that of existing algorithms  whereas in the adversarial setting  we devise simple
algorithms whose regret have the same scaling as that of state-of-the-art algorithms  but with lower
computational complexity.

2 Contribution and Related Work

2.1 Stochastic combinatorial bandits under semi-bandit feedback

Contribution. (a) We derive an asymptotic (as the time horizon T grows large) regret lower bound
satisﬁed by any algorithm (Theorem 1). This lower bound is problem-speciﬁc and tight:
there
exists an algorithm that attains the bound on all problem instances  although the algorithm might
be computationally expensive. To our knowledge  such lower bounds have not been proposed in
the case of stochastic combinatorial bandits. The dependency in m and d of the lower bound is
unfortunately not explicit. We further provide a simpliﬁed lower bound (Theorem 2) and derive its
scaling in (m  d) in speciﬁc examples.
√
(b) We propose ESCB (Efﬁcient Sampling for Combinatorial Bandits)  an algorithm whose regret
scales at most as O(
min log(T )) (Theorem 5)  where ∆min denotes the expected reward dif-
ference between the best and the second-best arm. ESCB assigns an index to each arm. The index
of given arm can be interpreted as performing likelihood tests with vanishing risk on its average re-
ward. Our indexes are the natural extension of KL-UCB and UCB1 indexes deﬁned for unstructured
bandits [5  21]. Numerical experiments for some speciﬁc combinatorial problems are presented in
the supplementary material  and show that ESCB signiﬁcantly outperforms existing algorithms.
Related work. Previous contributions on stochastic combinatorial bandits focused on speciﬁc com-
binatorial structures  e.g. m-sets [6]  matroids [7]  or permutations [8]. Generic combinatorial prob-
lems were investigated in [9  10  11  12]. The proposed algorithms  LLR and CUCB are variants
of the UCB algorithm  and their performance guarantees are presented in Table 1. Our algorithms
improve over LLR and CUCB by a multiplicative factor of

md∆−1

√

m.

min

We

(cid:19)

1

m|M|

whose

present

algorithm

COMBEXP 

m3T (d + m1/2λ

−1) log µ−1

  where µmin = mini∈[d]

2.2 Adversarial combinatorial problems under bandit feedback

(cid:18)(cid:113)
so that COMBEXP has O((cid:112)m3dT log(d/m)) regret. A known regret lower bound is Ω(m

(cid:80)
Contribution.
is
regret
O
and λ is
the smallest nonzero eigenvalue of the matrix E[M M(cid:62)] when M is uniformly distributed over M
(Theorem 6). For most problems of interest m(dλ)−1 = O(1) [4] and µ−1
min = O(poly(d/m)) 
dT )
[13]  so the regret gap between COMBEXP and this lower bound scales at most as m1/2 up to a
logarithmic factor.
Related work. Adversarial combinatorial bandits have been extensively investigated recently 
see [13] and references therein. Some papers consider speciﬁc instances of these problems  e.g. 
shortest-path routing [14]  m-sets [15]  and permutations [16]. For generic combinatorial problems 
(if d ≥ 2m) in the case of semi-
known regret lower bounds scale as Ω
bandit and bandit feedback  respectively [13]. In the case of semi-bandit feedback  [13] proposes

(cid:16)√

M∈M Mi

√
m

mdT

and Ω

(cid:17)

(cid:16)

(cid:17)

√

dT

2

Algorithm

Lower Bound [13]

COMBAND [4]

EXP2 WITH JOHN’S EXPLORATION [18]

COMBEXP (Theorem 6)

O

Ω

(cid:16)
(cid:18)(cid:114)
(cid:18)(cid:113)
(cid:18)(cid:114)
(cid:16)

O

m3dT

Regret
√
dT
m

  if d ≥ 2m

(cid:16)

1 + 2m
dλ

m3dT log d
m
O

m3dT log d
m

(cid:17)(cid:19)

(cid:19)

(cid:19)

(cid:17)

(cid:17)

1 + m1/2
dλ

log µ

−1
min

Table 2: Regret of various algorithms for adversarial combinatorial bandits with bandit feedback.
Note that for most combinatorial classes of interests  m(dλ)−1 = O(1) and µ−1
min = O(poly(d/m)).

T log(d/m)) regret where L(cid:63)

OSMD  an algorithm whose regret upper bound matches the lower bound. [17] presents an algorithm

with O(m(cid:112)dL(cid:63)
is upper-bounded by O((cid:112)m3dT log(d/m)). [18] addresses generic linear optimization with bandit
regret scaling at most as O((cid:112)m3dT log(d/m)) in the case of combinatorial structure. As we show

For problems with bandit feedback  [4] proposes COMBAND and derives a regret upper bound which
depends on the structure of arm set M. For most problems of interest  the regret under COMBAND

feedback and the proposed algorithm  referred to as EXP2 WITH JOHN’S EXPLORATION  has a

T is the total reward of the best arm after T rounds.

d and λ = m(d−m)

next  for many combinatorial structures of interest (e.g. m-sets  matchings  spanning trees)  COMB-
EXP yields the same regret as COMBAND and EXP2 WITH JOHN’S EXPLORATION  with lower
computational complexity for a large class of problems. Table 2 summarises known regret bounds.
Example 1: m-sets. M is the set of all d-dimensional binary vectors with m non-zero coordinates.
d(d−1) (refer to the supplementary material for details). Hence when
We have µmin = m

same as that of COMBAND and EXP2 WITH JOHN’S EXPLORATION.
Example 2: matchings. The set of arms M is the set of perfect matchings in Km m. d = m2 and
|M| = m!. We have µmin = 1
m−1. Hence the regret upper bound of COMBEXP is

m = o(d)  the regret upper bound of COMBEXP becomes O((cid:112)m3dT log(d/m))  which is the
O((cid:112)m5T log(m))  the same as for COMBAND and EXP2 WITH JOHN’S EXPLORATION.
case  d = (cid:0)N
(cid:1)  m = N − 1  and by Cayley’s formula M has N N−2 arms. log µ−1
EXPLORATION becomes O((cid:112)N 5T log(N )). As for COMBEXP  we get the same regret upper
bound O((cid:112)N 5T log(N )).

Example 3: spanning trees. M is the set of spanning trees in the complete graph KN . In this
min ≤ 2N for
dλ < 7 when N ≥ 6  The regret upper bound of COMBAND and EXP2 WITH JOHN’S
N ≥ 2 and m

m  and λ = 1

2

3 Models and Objectives

We consider MAB problems where each arm M is a subset of m basic actions taken from [d] =
{1  . . .   d}. For i ∈ [d]  Xi(n) denotes the reward of basic action i in round n. In the stochastic
setting  for each i  the sequence of rewards (Xi(n))n≥1 is i.i.d. with Bernoulli distribution with
mean θi. Rewards are assumed to be independent across actions. We denote by θ = (θ1  . . .   θd)(cid:62) ∈
Θ = [0  1]d the vector of unknown expected rewards of the various basic actions. In the adversarial
setting  the reward vector X(n) = (X1(n)  . . .   Xd(n))(cid:62) ∈ [0  1]d is arbitrary  and the sequence
(X(n)  n ≥ 1) is decided (but unknown) at the beginning of the experiment.
The set of arms M is an arbitrary subset of {0  1}d  such that each of its elements M has m basic
actions. Arm M is identiﬁed with a binary column vector (M1  . . .   Md)(cid:62)  and we have (cid:107)M(cid:107)1 =
m  ∀M ∈ M. At the beginning of each round n  a policy π  selects an arm M π(n) ∈ M based on
the arms chosen in previous rounds and their observed rewards. The reward of arm M π(n) selected

in round n is(cid:80)

i∈[d] M π

i (n)Xi(n) = M π(n)(cid:62)X(n).

3

We consider both semi-bandit and bandit feedbacks. Under semi-bandit feedback and policy π 
at the end of round n  the outcome of basic actions Xi(n) for all i ∈ M π(n) are revealed to the
decision maker  whereas under bandit feedback  M π(n)(cid:62)X(n) only can be observed.
Let Π be the set of all feasible policies. The objective is to identify a policy in Π maximizing the
cumulative expected reward over a ﬁnite time horizon T . The expectation is here taken with respect
to possible randomness in the rewards (in the stochastic setting) and the possible randomization in
the policy. Equivalently  we aim at designing a policy that minimizes regret  where the regret of
policy π ∈ Π is deﬁned by:

(cid:34) T(cid:88)

(cid:35)

(cid:34) T(cid:88)

(cid:35)

Rπ(T ) = max
M∈M

E

M(cid:62)X(n)

− E

M π(n)(cid:62)X(n)

.

n=1

n=1

Finally  for the stochastic setting  we denote by µM (θ) = M(cid:62)θ the expected reward of arm M 
and let M (cid:63)(θ) ∈ M  or M (cid:63) for short  be any arm with maximum expected reward: M (cid:63)(θ) ∈
arg maxM∈M µM (θ). In what follows  to simplify the presentation  we assume that the optimal
M (cid:63) is unique. We further deﬁne: µ(cid:63)(θ) = M (cid:63)(cid:62)θ  ∆min = minM(cid:54)=M (cid:63) ∆M where ∆M =
µ(cid:63)(θ) − µM (θ)  and ∆max = maxM (µ(cid:63)(θ) − µM (θ)).

4 Stochastic Combinatorial Bandits under Semi-bandit Feedback

4.1 Regret Lower Bound

Given θ  deﬁne the set of parameters that cannot be distinguished from θ when selecting action
M (cid:63)(θ)  and for which arm M (cid:63)(θ) is suboptimal:

B(θ) = {λ ∈ Θ : M (cid:63)

i (θ)(θi − λi) = 0  ∀i  µ(cid:63)(λ) > µ(cid:63)(θ)}.

We deﬁne X = (R+)|M| and kl(u  v) the Kullback-Leibler divergence between Bernoulli distri-
butions of respective means u and v  i.e.  kl(u  v) = u log(u/v) + (1 − u) log((1 − u)/(1 − v)).
Finally  for (θ  λ) ∈ Θ2  we deﬁne the vector kl(θ  λ) = (kl(θi  λi))i∈[d].
We derive a regret lower bound valid for any uniformly good algorithm. An algorithm π is uniformly
good iff Rπ(T ) = o(T α) for all α > 0 and all parameters θ ∈ Θ. The proof of this result relies on
a general result on controlled Markov chains [19].
Theorem 1 For all θ ∈ Θ  for any uniformly good policy π ∈ Π 
where c(θ) is the optimal value of the optimization problem:

log(T ) ≥ c(θ) 

lim inf T→∞ Rπ(T )

xM (M (cid:63)(θ) − M )(cid:62)θ

s.t.

xM M

kl(θ  λ) ≥ 1   ∀λ ∈ B(θ).

(1)

(cid:16) (cid:88)

M∈M

(cid:17)(cid:62)

(cid:88)

inf
x∈X

M∈M

Observe ﬁrst that optimization problem (1) is a semi-inﬁnite linear program which can be solved for
any ﬁxed θ  but its optimal value is difﬁcult to compute explicitly. Determining how c(θ) scales as
a function of the problem dimensions d and m is not obvious. Also note that (1) has the following
interpretation: assume that (1) has a unique solution x(cid:63). Then any uniformly good algorithm must
select action M at least x(cid:63)
M log(T ) times over the T ﬁrst rounds. From [19]  we know that there
exists an algorithm which is asymptotically optimal  so that its regret matches the lower bound of
Theorem 1. However this algorithm suffers from two problems: it is computationally infeasible
for large problems since it involves solving (1) T times  furthermore the algorithm has no ﬁnite
time performance guarantees  and numerical experiments suggest that its ﬁnite time performance
on typical problems is rather poor. Further remark that if M is the set of singletons (classical
bandit)  Theorem 1 reduces to the Lai-Robbins bound [20] and if M is the set of m-sets (bandit
with multiple plays)  Theorem 1 reduces to the lower bound derived in [6]. Finally  Theorem 1 can
be generalized in a straightforward manner for when rewards belong to a one-parameter exponential
family of distributions (e.g.  Gaussian  Exponential  Gamma etc.) by replacing kl by the appropriate
divergence measure.

4

A Simpliﬁed Lower Bound We now study how the regret c(θ) scales as a function of the problem
dimensions d and m. To this aim  we present a simpliﬁed regret lower bound. Given θ  we say that
a set H ⊂ M \ M (cid:63) has property P (θ) iff  for all (M  M(cid:48)) ∈ H2  M (cid:54)= M(cid:48) we have MiM(cid:48)
i (1 −
M (cid:63)
Theorem 2 Let H be a maximal (inclusion-wise) subset of M with property P (θ). Deﬁne β(θ) =
minM(cid:54)=M (cid:63)

i (θ)) = 0 for all i. We may now state Theorem 2.

∆M|M\M (cid:63)| . Then:

c(θ) ≥ (cid:88)

(cid:16)

(cid:80)

β(θ)

θi 

1

|M\M (cid:63)|

(cid:17) .

M∈H

maxi∈M\M (cid:63) kl

j∈M (cid:63)\M θj

Corollary 1 Let θ ∈ [a  1]d for some constant a > 0 and M be such that each arm M ∈ M  M (cid:54)=
M (cid:63) has at most k suboptimal basic actions. Then c(θ) = Ω(|H|/k).

Theorem 2 provides an explicit regret lower bound. Corollary 1 states that c(θ) scales at least
with the size of H. For most combinatorial sets  |H| is proportional to d − m (see supplementary
material for some examples)  which implies that in these cases  one cannot obtain a regret smaller
min log(T )). This result is intuitive since d − m is the number of parameters
than O((d − m)∆−1
√
not observed when selecting the optimal arm. The algorithms proposed below have a regret of
O(d

min log(T ))  which is acceptable since typically 

m is much smaller than d.

m∆−1

√

4.2 Algorithms

Next we present ESCB  an algorithm for stochastic combinatorial bandits that relies on arm indexes
as in UCB1 [21] and KL-UCB [5]. We derive ﬁnite-time regret upper bounds for ESCB that hold
even if we assume that (cid:107)M(cid:107)1 ≤ m  ∀M ∈ M  instead of (cid:107)M(cid:107)1 = m  so that arms may have
different numbers of basic actions.

4.2.1 Indexes

ESCB relies on arm indexes. In general  an index of arm M in round n  say bM (n)  should be
deﬁned so that bM (n) ≥ M(cid:62)θ with high probability. Then as for UCB1 and KL-UCB  applying the
principle of optimism in face of uncertainty  a natural way to devise algorithms based on indexes is
to select in each round the arm with the highest index. Under a given algorithm  at time n  we deﬁne
s=1 Mi(s) the number of times basic action i has been sampled. The empirical mean
s=1 Xi(s)Mi(s) if ti(n) > 0 and ˆθi(n) =

ti(n) = (cid:80)n
reward of action i is then deﬁned as ˆθi(n) = (1/ti(n))(cid:80)n

0 otherwise. We deﬁne the corresponding vectors t(n) = (ti(n))i∈[d] and ˆθ(n) = (ˆθi(n))i∈[d].
The indexes we propose are functions of the round n and of ˆθ(n). Our ﬁrst index for arm M 
referred to as bM (n  ˆθ(n)) or bM (n) for short  is an extension of KL-UCB index. Let f (n) =
log(n) + 4m log(log(n)). bM (n  ˆθ(n)) is the optimal value of the following optimization problem:

M(cid:62)q

(M t(n))(cid:62)kl(ˆθ(n)  q) ≤ f (n) 

s.t.

max
q∈Θ

(2)
where we use the convention that for v  u ∈ Rd  vu = (viui)i∈[d]. As we show later  bM (n) may be
computed efﬁciently using a line search procedure similar to that used to determine KL-UCB index.
Our second index cM (n  ˆθ(n)) or cM (n) for short is a generalization of the UCB1 and UCB-tuned
indexes:

cM (n) = M(cid:62) ˆθ(n) +

(cid:118)(cid:117)(cid:117)(cid:116) f (n)

2

(cid:32) d(cid:88)

i=1

(cid:33)

Mi
ti(n)

Note that  in the classical bandit problems with independent arms  i.e.  when m = 1  bM (n) re-
duces to the KL-UCB index (which yields an asymptotically optimal algorithm) and cM (n) reduces
to the UCB-tuned index. The next theorem provides generic properties of our indexes. An impor-
tant consequence of these properties is that the expected number of times where bM (cid:63) (n  ˆθ(n)) or
cM (cid:63) (n  ˆθ(n)) underestimate µ(cid:63)(θ) is ﬁnite  as stated in the corollary below.

5

Theorem 3 (i) For all n ≥ 1  M ∈ M and τ ∈ [0  1]d  we have bM (n  τ ) ≤ cM (n  τ ).
(ii) There exists Cm > 0 depending on m only such that  for all M ∈ M and n ≥ 2:

P[bM (n  ˆθ(n)) ≤ M(cid:62)θ] ≤ Cmn−1(log(n))−2.

P[bM (cid:63) (n  ˆθ(n)) ≤ µ(cid:63)] ≤ 1 + Cm

n≥1

n≥2 n−1(log(n))−2 < ∞.

Corollary 2 (cid:80)

(cid:80)

Statement (i) in the above theorem is obtained combining Pinsker and Cauchy-Schwarz inequalities.
The proof of statement (ii) is based on a concentration inequality on sums of empirical KL diver-
gences proven in [22]. It enables to control the ﬂuctuations of multivariate empirical distributions
for exponential families. It should also be observed that indexes bM (n) and cM (n) can be extended
in a straightforward manner to the case of continuous linear bandit problems  where the set of arms
is the unit sphere and one wants to maximize the dot product between the arm and an unknown
vector. bM (n) can also be extended to the case where reward distributions are not Bernoulli but
lie in an exponential family (e.g. Gaussian  Exponential  Gamma  etc.)  replacing kl by a suitably
chosen divergence measure. A close look at cM (n) reveals that the indexes proposed in [10]  [11] 
Mi
ti(n)

and [9] are too conservative to be optimal in our setting: there the “conﬁdence bonus”(cid:80)d
was replaced by (at least) m(cid:80)d

ti(n). Note that [10]  [11] assume that the various basic actions
are arbitrarily correlated  while we assume independence among basic actions. When independence
log(T )). This
does not hold  [11] provides a problem instance where the regret is at least Ω( md
√
∆min
does not contradict our regret upper bound (scaling as O( d
log(T )))  since we have added the
independence assumption.

m
∆min

i=1

i=1

Mi

4.2.2 Index computation

While the index cM (n) is explicit  bM (n) is deﬁned as the solution to an optimization problem. We
show that it may be computed by a simple line search. For λ ≥ 0  w ∈ [0  1] and v ∈ N  deﬁne:

(cid:17)
1 − λv +(cid:112)(1 − λv)2 + 4wvλ

/2.

g(λ  w  v) =

Fix n  M  ˆθ(n) and t(n). Deﬁne I = {i : Mi = 1  ˆθi(n) (cid:54)= 1}  and for λ > 0  deﬁne:

F (λ) =

ti(n)kl(ˆθi(n)  g(λ  ˆθi(n)  ti(n))).

(cid:16)
(cid:88)

i∈I

Theorem 4 If I = ∅  bM (n) = ||M||1. Otherwise: (i) λ (cid:55)→ F (λ) is strictly increasing  and
F (R+) = R+. (ii) Deﬁne λ(cid:63) as the unique solution to F (λ) = f (n). Then bM (n) = ||M||1 −|I| +

(cid:80)

i∈I g(λ(cid:63)  ˆθi(n)  ti(n)).

Theorem 4 shows that bM (n) can be computed using a line search procedure such as bisection 
as this computation amounts to solving the nonlinear equation F (λ) = f (n)  where F is strictly
increasing. The proof of Theorem 4 follows from KKT conditions and the convexity of the KL
divergence.

4.2.3 The ESCB Algorithm

The pseudo-code of ESCB is presented in Algorithm 1. We consider two variants of the algorithm
based on the choice of the index ξM (n): ESCB-1 when ξM (n) = bM (n) and ESCB-2 if ξM (n) =
cM (n). In practice  ESCB-1 outperforms ESCB-2. Introducing ESCB-2 is however instrumental
in the regret analysis of ESCB-1 (in view of Theorem 3 (i)). The following theorem provides a
ﬁnite time analysis of our ESCB algorithms. The proof of this theorem borrows some ideas from
the proof of [11  Theorem 3].
Theorem 5 The regret under algorithms π ∈ {ESCB-1  ESCB-2} satisﬁes for all T ≥ 1:

√
Rπ(T ) ≤ 16d

m∆−1

minf (T ) + 4dm3∆−2

min + C(cid:48)
m 

m ≥ 0 does not depend on θ  d and T . As a consequence Rπ(T ) = O(d

√

m∆−1

min log(T ))

where C(cid:48)
when T → ∞.

6

Select arm M (n) ∈ arg maxM∈M ξM (n).
Observe the rewards  and update ti(n) and ˆθi(n) ∀i ∈ M (n).

Algorithm 1 ESCB

for n ≥ 1 do

end for

Algorithm 2 COMBEXP

(cid:113)

(cid:113)

−1
min

m log µ

√
−1
min+

and η = γC  with C = λ

m3/2 .

Initialization: Set q0 = µ0  γ =
for n ≥ 1 do

m log µ

C(Cm2d+m)T

Mixing: Let q(cid:48)

n−1 = (1 − γ)qn−1 + γµ0.

Decomposition: Select a distribution pn−1 over M such that(cid:80)
Sampling: Select a random arm M (n) with distribution pn−1 and incur a reward Yn =(cid:80)
Estimation: Let Σn−1 = E(cid:2)M M(cid:62)(cid:3)  where M has law pn−1. Set ˜X(n) = YnΣ+

M pn−1(M )M = mq(cid:48)

n−1.

is the pseudo-inverse of Σn−1.
Update: Set ˜qn(i) ∝ qn−1(i) exp(η ˜Xi(n))  ∀i ∈ [d].
Projection: Set qn to be the projection of ˜qn onto the set P using the KL divergence.

end for

i Xi(n)Mi(n).
n−1

n−1M (n)  where Σ+

ESCB with time horizon T has a complexity of O(|M|T ) as neither bM nor cM can be writ-
ten as M(cid:62)y for some vector y ∈ Rd. Assuming that the ofﬂine (static) combinatorial prob-
lem is solvable in O(V (M)) time  the complexity of the CUCB algorithm in [10] and [11]
after T rounds is O(V (M)T ). Thus  if the ofﬂine problem is efﬁciently implementable  i.e. 
V (M) = O(poly(d/m))  CUCB is efﬁcient  whereas ESCB is not since |M| may have expo-
In §2.5 of the supplement  we provide an extension of ESCB called
nentially many elements.
EPOCH-ESCB  that attains almost the same regret as ESCB while enjoying much better computa-
tional complexity.

5 Adversarial Combinatorial Bandits under Bandit Feedback

We now consider adversarial combinatorial bandits with bandit feedback. We start with the follow-
ing observation:

M∈M M(cid:62)X = max
µ∈Co(M)

max

µ(cid:62)X 

with Co(M) the convex hull of M. We embed M in the d-dimensional simplex by dividing its
elements by m. Let P be this scaled version of Co(M).
Inspired by OSMD [13  18]  we propose the COMBEXP algorithm  where the KL divergence
is the Bregman divergence used to project onto P.
Projection using the KL divergence is
addressed in [23]. We denote the KL divergence between distributions q and p in P by
q(i) . The projection of distribution q onto a closed convex set Ξ of

KL(p  q) =(cid:80)

i∈[d] p(i) log p(i)

distributions is p(cid:63) = arg minp∈Ξ KL(p  q).
Let λ be the smallest nonzero eigenvalue of E[M M(cid:62)]  where M is uniformly distributed over M.
We deﬁne the exploration-inducing distribution µ0 ∈ P: µ0
∀i ∈ [d]  and
i . µ0 is the distribution over basic actions [d] induced by the uniform distri-
let µmin = mini mµ0
bution over M. The pseudo-code for COMBEXP is shown in Algorithm 2. The KL projection
in COMBEXP ensures that mqn−1 ∈ Co(M). There exists λ  a distribution over M such that
M λ(M )M. This guarantees that the system of linear equations in the decomposition
step is consistent. We propose to perform the projection step (the KL projection of ˜q onto P) using
interior-point methods [24]. We provide a simpler method in §3.4 of the supplement. The decom-
position step can be efﬁciently implemented using the algorithm of [25]. The following theorem
provides a regret upper bound for COMBEXP.

mqn−1 =(cid:80)

M∈M Mi 

i = 1

(cid:80)

m|M|

Theorem 6 For all T ≥ 1: RCOMBEXP(T ) ≤ 2

d + m1/2

λ

log µ−1

min + m5/2

λ

log µ−1
min.

(cid:17)

(cid:114)

(cid:16)

m3T

7

For most classes of M  we have µ−1

classes  COMBEXP has a regret of O((cid:112)m3dT log(d/m))  which is a factor(cid:112)m log(d/m) off the

min = O(poly(d/m)) and m(dλ)−1 = O(1) [4]. For these

lower bound (see Table 2).
It might not be possible to compute the projection step exactly  and this step can be solved up
to accuracy n in round n. Namely we ﬁnd qn such that KL(qn  ˜qn) − minp∈Ξ KL(p  ˜qn) ≤ n.
Proposition 1 shows that for n = O(n−2 log
−3(n))  the approximate projection gives the same
regret as when the projection is computed exactly. Theorem 7 gives the computational complexity of
COMBEXP with approximate projection. When Co(M) is described by polynomially (in d) many
linear equalities/inequalities  COMBEXP is efﬁciently implementable and its running time scales
(almost) linearly in T . Proposition 1 and Theorem 7 easily extend to other OSMD-type algorithms
and thus might be of independent interest.

Proposition 1 If
n = O(n−2 log

the

projection

−3(n))  we have:

step

of

COMBEXP

is

solved

up

to

accuracy

(cid:115)

(cid:18)

(cid:19)

RCOMBEXP(T ) ≤ 2

2m3T

d +

m1/2

λ

log µ−1

min +

2m5/2

λ

log µ−1
min.

Theorem 7 Assume that Co(M) is deﬁned by c linear equalities and s linear inequalities. If the
projection step is solved up to accuracy n = O(n−2 log
−3(n))  then COMBEXP has time com-
plexity.
The time complexity of COMBEXP can be reduced by exploiting the structure of M (See [24 
page 545]). In particular  if inequality constraints describing Co(M) are box constraints  the time
complexity of COMBEXP is O(T [c2√
The computational complexity of COMBEXP is determined by the structure of Co(M) and COMB-
EXP has O(T log(T )) time complexity due to the efﬁciency of interior-point methods. In con-
trast  the computational complexity of COMBAND depends on the complexity of sampling from M.
COMBAND may have a time complexity that is super-linear in T (see [16  page 217]). For instance 
consider the matching problem described in Section 2. We have c = 2m equality constraints and
s = m2 box constraints  so that the time complexity of COMBEXP is: O(m5T log(T )). It is noted
that using [26  Algorithm 1]  the cost of decomposition in this case is O(m4). On the other hand 
COMBBAND has a time complexity of O(m10F (T ))  with F a super-linear function  as it requires
to approximate a permanent  requiring O(m10) operations per round. Thus  COMBEXP has much
lower complexity than COMBAND and achieves the same regret.

s(c + d) log(T ) + d4]).

6 Conclusion

We have investigated stochastic and adversarial combinatorial bandits. For stochastic combinatorial
bandits with semi-bandit feedback  we have provided a tight  problem-dependent regret lower bound
that  in most cases  scales at least as O((d − m)∆−1
√
min log(T )). We proposed ESCB  an algorithm
with O(d
min log(T )) regret. We plan to reduce the gap between this regret guarantee and
the regret lower bound  as well as investigate the performance of EPOCH-ESCB. For adversarial
combinatorial bandits with bandit feedback  we proposed the COMBEXP algorithm. There is a gap
between the regret of COMBEXP and the known regret lower bound in this setting  and we plan to
reduce it as much as possible.

m∆−1

Acknowledgments

A. Proutiere’s research is supported by the ERC FSA grant  and the SSF ICT-Psi project.

8

References
[1] Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected

Papers  pages 169–177. Springer  1985.

[2] S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–222  2012.

[3] Nicol`o Cesa-Bianchi and G´abor Lugosi. Prediction  learning  and games  volume 1. Cambridge Univer-

sity Press Cambridge  2006.

[4] Nicol`o Cesa-Bianchi and G´abor Lugosi. Combinatorial bandits. Journal of Computer and System Sci-

ences  78(5):1404–1422  2012.

[5] Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and beyond.

In Proc. of COLT  2011.

[6] Venkatachalam Anantharam  Pravin Varaiya  and Jean Walrand. Asymptotically efﬁcient allocation rules
for the multiarmed bandit problem with multiple plays-part i: iid rewards. Automatic Control  IEEE
Transactions on  32(11):968–976  1987.

[7] Branislav Kveton  Zheng Wen  Azin Ashkan  Hoda Eydgahi  and Brian Eriksson. Matroid bandits: Fast

combinatorial optimization with learning. In Proc. of UAI  2014.

[8] Yi Gai  Bhaskar Krishnamachari  and Rahul Jain. Learning multiuser channel allocations in cognitive

radio networks: A combinatorial multi-armed bandit formulation. In Proc. of IEEE DySpan  2010.

[9] Yi Gai  Bhaskar Krishnamachari  and Rahul Jain. Combinatorial network optimization with unknown
variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Trans. on
Networking  20(5):1466–1478  2012.

[10] Wei Chen  Yajun Wang  and Yang Yuan. Combinatorial multi-armed bandit: General framework and

applications. In Proc. of ICML  2013.

[11] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvari. Tight regret bounds for stochastic

combinatorial semi-bandits. In Proc. of AISTATS  2015.

[12] Zheng Wen  Azin Ashkan  Hoda Eydgahi  and Branislav Kveton. Efﬁcient learning in large-scale combi-

natorial semi-bandits. In Proc. of ICML  2015.

[13] Jean-Yves Audibert  S´ebastien Bubeck  and G´abor Lugosi. Regret in online combinatorial optimization.

Mathematics of Operations Research  39(1):31–45  2013.

[14] Andr´as Gy¨orgy  Tam´as Linder  G´abor Lugosi  and Gy¨orgy Ottucs´ak. The on-line shortest path problem

under partial monitoring. Journal of Machine Learning Research  8(10)  2007.

[15] Satyen Kale  Lev Reyzin  and Robert Schapire. Non-stochastic bandit slate problems. Advances in Neural

Information Processing Systems  pages 1054–1062  2010.

[16] Nir Ailon  Kohei Hatano  and Eiji Takimoto. Bandit online optimization over the permutahedron.

Algorithmic Learning Theory  pages 215–229. Springer  2014.

In

[17] Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Proc. of COLT  2015.
[18] S´ebastien Bubeck  Nicol`o Cesa-Bianchi  and Sham M. Kakade. Towards minimax policies for online

linear optimization with bandit feedback. Proc. of COLT  2012.

[19] Todd L. Graves and Tze Leung Lai. Asymptotically efﬁcient adaptive choice of control laws in controlled

markov chains. SIAM J. Control and Optimization  35(3):715–743  1997.

[20] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[21] Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

[22] Stefan Magureanu  Richard Combes  and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds

and optimal algorithms. Proc. of COLT  2014.

[23] I. Csisz´ar and P.C. Shields. Information theory and statistics: A tutorial. Now Publishers Inc  2004.
[24] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press  2004.
[25] H. D. Sherali. A constructive proof of the representation theorem for polyhedral sets based on fundamental

deﬁnitions. American Journal of Mathematical and Management Sciences  7(3-4):253–270  1987.

[26] David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights. Journal

of Machine Learning Research  10:1705–1736  2009.

9

,Richard Combes
Mohammad Sadegh Talebi Mazraeh Shahi
marc lelarge