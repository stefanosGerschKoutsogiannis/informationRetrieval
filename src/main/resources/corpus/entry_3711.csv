2017,Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee,We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models  overfitting and model redundancy negatively affects the prediction accuracy and model variance.  Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise  removing connections at each layer by solving a convex optimization program.  This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model.  The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm.  While the latter can achieve slightly simpler models with the same generalization performance  the former can be computed in a distributed manner.  In both cases  Net-Trim significantly reduces the number of connections in the network  while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model.  To analyze the model sample complexity  we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length $N$ as inputs   we show that if the network response can be described using a maximum number of $s$ non-zero weights per node  these weights can be learned from $\mathcal{O}(s\log N)$ samples.,Net-Trim: Convex Pruning of Deep Neural Networks

with Performance Guarantee

Alireza Aghasi∗

Institute for Insight

Georgia State University

IBM TJ Watson

aaghasi@gsu.edu

Afshin Abdi

Department of ECE

Georgia Tech

abdi@gatech.edu

Nam Nguyen
IBM TJ Watson

nnguyen@us.ibm.com

Justin Romberg
Department of ECE

Georgia Tech

jrom@ece.gatech.edu

Abstract

We introduce and analyze a new technique for model reduction for deep neural
networks. While large networks are theoretically capable of learning arbitrarily
complex models  overﬁtting and model redundancy negatively affects the prediction
accuracy and model variance. Our Net-Trim algorithm prunes (sparsiﬁes) a trained
network layer-wise  removing connections at each layer by solving a convex
optimization program. This program seeks a sparse set of weights at each layer
that keeps the layer inputs and outputs consistent with the originally trained model.
The algorithms and associated analysis are applicable to neural networks operating
with the rectiﬁed linear unit (ReLU) as the nonlinear activation. We present both
parallel and cascade versions of the algorithm. While the latter can achieve slightly
simpler models with the same generalization performance  the former can be
computed in a distributed manner. In both cases  Net-Trim signiﬁcantly reduces the
number of connections in the network  while also providing enough regularization
to slightly reduce the generalization error. We also provide a mathematical analysis
of the consistency between the initial network and the retrained model. To analyze
the model sample complexity  we derive the general sufﬁcient conditions for the
recovery of a sparse transform matrix. For a single layer taking independent
Gaussian random vectors of length N as inputs  we show that if the network
response can be described using a maximum number of s non-zero weights per

node  these weights can be learned fromO(s log N) samples.

1

Introduction

With enough layers  neurons in each layer  and a sufﬁciently large set of training data  neural networks
can learn structure of arbitrary complexity [1]. This model ﬂexibility has made the deep neural
network a pioneer machine learning tool over the past decade (see [2] for a comprehensive overview).
In practice  multi-layer networks often have more parameters than can be reliably estimated from the
amount of data available. This gives the training procedure a certain ambiguity – many different sets
of parameter values can model the data equally well  and we risk instabilities due to overﬁtting. In
this paper  we introduce a framework for sparisfying networks that have already been trained using
standard techniques. This reduction in the number of parameters needed to specify the network makes
it more robust and more computationally efﬁcient to implement without sacriﬁcing performance.

∗Corresponding Author

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In recent years there has been increasing interest in the mathematical understanding of deep networks.
These efforts are mainly in the context of characterizing the minimizers of the underlying cost
function [3  4] and the geometry of the loss function [5]. Recently  the analysis of deep neural
networks using compressed sensing tools has been considered in [6]  where the distance preservability
of feedforward networks at each layer is studied. There are also works on formulating the training of
feedforward networks as an optimization problem [7  8  9]  where the majority of the works approach
their understanding of neural networks by sequentially studying individual layers.
Various methods have been proposed to reduce overﬁtting via regularizing techniques and pruning
strategies. These include explicit regularization using (cid:96)1 and (cid:96)2 penalties during training [10  11] 
and techniques that randomly remove active connections in the training phase (e.g. Dropout [12] and
DropConnect [13]) making them more likely to produce sparse networks. There has also been recent
works on explicit network compression (e.g.  [14  15  16]) to remove the inherent redundancies. In
what is perhaps the most closely related work to what is presented below  [14] proposes a pruning
scheme that simply truncates small weights of an already trained network  and then re-adjusts the
remaining active weights using another round of training. These aforementioned techniques are based
on heuristics  and lack general performance guarantees that help understand when and how well they
work.
We present a framework  called Net-Trim  for pruning the network layer-by-layer that is based on
convex optimization. Each layer of the net consists of a linear map followed by a nonlinearity;
the algorithms and theory presented below use a rectiﬁed linear unit (ReLU) applied point-wise
to each output of the linear map. Net-trim works by taking a trained network  and then ﬁnding
the sparsest set of weights for each layer that keeps the output responses consistent with the initial

training. More concisely  if Y((cid:96)−1) is the input (across the training examples) to layer (cid:96)  and
Y((cid:96)) is the output following the ReLU operator  Net-Trim searches for a sparse W such that
Y((cid:96)) ≈ ReLU(W
Y((cid:96)−1)). Using the standard (cid:96)1 relaxation for sparsity and the fact that the



ReLU function is piecewise linear allows us to perform this search by solving a convex program. In
contrast to techniques based on thresholding (such as [14])  Net-Trim does not require multiple other
time-consuming training steps after the initial pruning.
Along with making the computations tractable  Net-Trim’s convex formulation also allows us to derive
theoretical guarantees on how far the retrained model is from the initial model  and establish sample
complexity arguments about the number of random samples required to retrain a presumably sparse
layer. To the best of our knowledge  Net-Trim is the ﬁrst pruning scheme with such performance
guarantees. In addition  it is easy to modify and adapt to other structural constraints on the weights
by adding additional penalty terms or introducing additional convex constraints.
An illustrative example is shown in Figure 1. Here  200 points in the 2D plane are used to train
a binary classiﬁer. The regions corresponding to each class are nested spirals. We ﬁt a classiﬁer
using a simple neural network with two hidden layers with fully connected weights  each consisting
200 neurons. Figure 1(b) shows the weighted adjacency matrix between the layers after training 
and then again after Net-Trim is applied. With only a negligible change to the overall network
response (panel (a) vs panel (d))  Net-Trim is able to prune more than 93% of the links among the
neurons  representing a signiﬁcant model reduction. Even when the neural network is trained using
sparsifying weight regularizers (here  Dropout [12] and (cid:96)1 penalty)  Net-Trim produces a model
which is over 7 times sparser than the initial one  as presented in panel (c). The numerical experiments
in Section 6 show that these kinds of results are not limited to toy examples; Net-Trim achieves
signiﬁcant compression ratios on large networks trained on real data sets.
The remainder of the paper is structured as follows. In Section 2  we formally present the network
model used in the paper. The proposed pruning schemes  both the parallel and cascade Net-Trim are
presented and discussed in Section 3. Section 4 is devoted to the convex analysis of the proposed
framework and its sample complexity. The implementation details of the proposed convex scheme
are presented in Section 5. Finally  in Section 6  we report some retraining experiments using the
Net-Trim and conclude the paper by presenting some general remarks. Along with some extended
discussions  the proofs of all of the theoretical statements in the paper are presented as a supplementary
note (speciﬁcally  §4 of the notes is devoted to the technical proofs).

We very brieﬂy summarize the notation used below. For a matrix A  we use AΓ1 ∶ to denote the
submatrix formed by restricting the rows of A to the index set Γ1. Similarly  A∶ Γ2 is the submatrix

of columns indexed by Γ2  and AΓ1 Γ2 is formed by extracting both rows and columns. For an

2

(b)

(c)

(a)

(d)

Figure 1: Net-Trim pruning performance; (a) initial trained model; (b) the weighted adjacency matrix
relating the two hidden layers before (left) and after (right) the application of Net-Trim; (c) left: the
adjacency matrix after training the network with Dropout and (cid:96)1 regularization; right: after retraining
via Net-Trim; (d) the retrained classiﬁer

n=1xm n andXF as the Frobenius
M× N matrix X with entries xm n  we use2X1á∑M
norm. For a vector x x0 is the cardinality of x  supp x is the set of indexes with non-zero entries 
and suppc x is the complement set. We will use the notation x+ as shorthand max(x  0)  where
max(.  0) is applied to vectors and matrices component-wise. Finally  the vertical concatenation of
two vectors a and b is denoted by[a; b].

m=1∑N

2 Feedforward Network Model

previous layer Y

The network activations are taken to be rectiﬁed linear units. The output of the (cid:96)-th layer is Y

In this section  we introduce some notational conventions related to a feedforward network model. We

assume that we have P training samples xp  p= 1   P   where xp∈ RN is an input to the network.
We stack up these samples into a matrix X∈ RN×P   structured as X=[x1   xP]. Considering
L layers for the network  the output of the network at the ﬁnal layer is denoted by Y(L)∈ RNL×P  
where each column in Y(L) is a response to the corresponding training column in X.
((cid:96))∈
RN(cid:96)×P   generated by applying the adjoint of the weight matrix W(cid:96)∈ RN(cid:96)−1×N(cid:96) to the output of the
((cid:96)−1) and then applying a component-wise max(.  0) operation:
(0) = X and N0 = N. A trained neural network as outlined in (1) is represented by
(cid:96)=1  X).
TN({W(cid:96)}L
networks  whereW(cid:96)1= 1 for every layer (cid:96)= 1   L. Such presentation is with no loss of generality 
j=0Wj1. Since max(αx  0)= α max(x  0) for α> 0  any
W(cid:96)~W(cid:96)1  and Y((cid:96)+1) with Y((cid:96)+1)~∏(cid:96)

as any network in the form of (1) can be converted to its link-normalized version by replacing W(cid:96) with

For the sake of theoretical analysis  all the results presented in this paper are stated for link-normalized

(cid:96)= 1   L 

weight processing on a network of the form (1) can be applied to the link-normalized version and
later transferred to the original domain via a suitable scaling.

((cid:96))= maxW



(cid:96) Y

((cid:96)−1)

  0  

Y

where Y

(1)

3 Convex Pruning of the Network

Our pruning strategy relies on redesigning the network so that for the same training data each layer
outcomes stay more or less close to the initial trained model  while the weights associated with each
layer are replaced with sparser versions to reduce the model complexity. Figure 2 presents the main
idea  where the complex paths between the layer outcomes are replaced with simple paths. In a sense 
if we consider each layer response to the transmitted data as a checkpoint  Net-Trim assures the
checkpoints remain roughly the same  while a simpler path between the checkpoints is discovered.

2The notationX1 should not be confused with the matrix induced (cid:96)1 norm

3

-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.5W1

X

(1)

Y



(L−1)

Y

WL

(L)

Y

⇒ X

ˆW1

(1)

ˆY



(L−1)

ˆY

ˆWL

(L)

ˆY

Figure 2: The main retraining idea: keeping the layer outcomes close to the initial trained model
while ﬁnding a simpler path relating each layer input to the output

Consider the ﬁrst layer  where X =[x1   xP] is the layer input  W =[w1   wM] the layer
coefﬁcient matrix  and Y =[ym p] the layer outcome. We require the new coefﬁcient matrix ˆW to

be sparse and the new response to be close to Y . Using the sum of absolute entries as a proxy to
promote sparsity  a natural strategy to retrain the layer is addressing the nonlinear program

ˆW = arg min

U1

s.t.

maxU

X  0− Y



≤ .

U

Despite the convex objective  the constraint set in (2) is non-convex. However  we may approximate

it with a convex set by imposing Y and ˆY = max( ˆWX  0) to have similar activation patterns.
More speciﬁcally  knowing that ym p is either zero or positive  we enforce the max(.  0) argument
to be negative when ym p= 0  and close to ym p elsewhere. To present the convex formulation  for
V =[vm p]  throughout the paper we use the notation U∈C(X  Y   V) to present the constraint set

F

.

(3)

(2)

(4)



(u
mxp− ym p)2≤ 2
∑
mxp≤ vm p
m p∶ ym p>0
u
U1
ˆW = arg min

m  p∶ ym p= 0
s.t. U∈C(X  Y   0).

Based on this deﬁnition  a convex proxy to (2) is

U

mxp to emulate the
ReLU operation. As a ﬁrst observation towards establishing a retraining framework  we show that the
solution of (4) is consistent with the desired constraint in (2)  as follows.

Basically  depending on the value of ym p  a different constraint is imposed on u
Proposition 1. Let ˆW be the solution to (4). For ˆY = max( ˆWX  0) being the retrained layer
response  ˆY − YF ≤ .

3.1 Parallel and Cascade Net-Trim

((cid:96)−1) and Y

Based on the above exploratory  we propose two schemes to retrain a neural network; one explores
a computationally distributable nature and the other proposes a cascading scheme to retrain the
layers sequentially. The general idea which originates from the relaxation in (4) is referred to as the
Net-Trim  speciﬁed by the parallel or cascade nature.
The parallel Net-Trim is a straightforward application of the convex program (4) to each layer in
the network. Basically  each layer is processed independently based on the initial model input and
output  without taking into account the retraining result from the previous layer. Speciﬁcally  denoting
Y
equation (1))  we propose to relearn the coefﬁcient matrix W(cid:96) via the convex program

((cid:96)) as the input and output of the (cid:96)-th layer of the initial trained neural network (see

U1
ˆW(cid:96)= arg min
pseudocode  we use TRIM(X  Y   V   ) as a function which returns the solution to a program like (4)
with the constraint U∈C(X  Y   V).

The optimization in (5) can be independently applied to every layer in the network and hence
computationally distributable. Algorithm 1 presents the pseudocode for the parallel Net-Trim. In this

s.t. U∈CY

  0 .

((cid:96)−1)

((cid:96))

(5)

  Y

U

With reference to the constraint in (5)  if we only retrain the (cid:96)-th layer  the output of the retrained
layer is in the -neighborhood of that before retraining. However  when all the layers are retrained
through (5)  an immediate question would be whether the retrained network produces an output
which is controllably close to the initially trained model. In the following theorem  we show that the
retrained error does not blow up across the layers and remains a multiple of .

4

((cid:96)−1)  

(6)

(7)

2 .

matrix  ˆW(cid:96)  is obtained via

min
U

2

((cid:96)−1)

  Y

((cid:96)−1)

(cid:96) m ˆy

p

ˆY

  W
(cid:96)

((cid:96))

− y
m p2
((cid:96))

.

the supplementary note  such program is not necessarily feasible and needs to be sufﬁciently slacked

When all the layers are retrained with a ﬁxed parameter  (as in Algorithm 1)  a corollary of the

s.t. U∈C(cid:96)ˆY
w


of the resulting matrices. In the following theorem  we prove that the outcome of the retrained
network produced by Algorithm 2 is close to that of the network before retraining.

In a cascade Net-Trim  unlike the parallel scheme where each layer is retrained independently  the
outcome of a retrained layer is probed into the program retraining the next layer. More speciﬁcally 
having the ﬁrst layer processed via (4)  one would ideally seek to address (5) with the modiﬁed

(cid:96)=1  X) be a link-normalized trained network with layer outcomes Y
((cid:96))
(cid:96)=1  X) by solving the convex programs
 ˆY((cid:96)−1)  0) obey

Theorem 1. LetTN({W(cid:96)}L
described by (1). Form the retrained networkTN({ ˆW(cid:96)}L
(5)  with = (cid:96) at each layer. Then the retrained layer outcomes ˆY((cid:96))= max( ˆW(cid:96)
 ˆY((cid:96))− Y((cid:96))F ≤∑(cid:96)
j=1 j.
(L)F ≤ L.
theorem above would bound the overall discrepancy as ˆY(L)− Y
constraint U∈C( ˆY((cid:96)−1)  Y((cid:96))  0) to retrain the subsequent layers. However  as detailed in §1 of
to warrant feasibility. In this regard  for every subsequent layer  (cid:96)= 2   L  the retrained weighting
U1
where for W(cid:96)=[w(cid:96) 1   w(cid:96) N(cid:96)] and γ(cid:96)≥ 1 
(cid:96)= γ(cid:96) Q
m p>0
m p∶ y
((cid:96))
The constants γ(cid:96)≥ 1 (referred to as the inﬂation rates) are free parameters  which control the sparsity
Theorem 2. LetTN({W(cid:96)}L
(cid:96)=1  X) be a link-normalized trained network with layer outcomes Y
((cid:96)).
(cid:96)=1  X) by solving (5) for the ﬁrst layer and (6) for the
Form the retrained networkTN({ ˆW(cid:96)}L
X  0) and γ(cid:96)≥ 1.
 ˆY((cid:96)−1)  0)  ˆY(1)= max( ˆW1
subsequent layers with (cid:96) as in (7)  ˆY((cid:96))= max( ˆW(cid:96)
j=2 γj) 1
Then the outputs ˆY((cid:96)) of the retrained network will obey ˆY((cid:96))− Y((cid:96))F ≤ 1(∏(cid:96)
network with 1=  and a constant inﬂation rate  γ  across all the layers. In such case  a corollary of
Theorem 2 bounds the network overall discrepancy as ˆY(L)− Y
((cid:96))F and use
necessary and to retrain layer (cid:96) in the parallel Net-Trim we can take  = rY
= rY
(1)F for the cascade case  where r plays a similar role as  for a link-normalized network.
practical networks that follow (1) for the ﬁrst L− 1 layers and skip an activation at the last layer.
1: Input: X  > 0  γ> 1 and normalized W1   WL
1 X  0
2: Y ← maxW

3: ˆW1← TRIM(X  Y   0  )
4: ˆY ← max( ˆW1
X  0)

5: for (cid:96)= 2   L do
Y ← max(W
(cid:96) Y   0)

←(γ∑m p∶ym p>0(w
(cid:96) m ˆyp− ym p)2)1~2

ˆW(cid:96)← TRIM( ˆY   Y   W
ˆY   )

ˆY ← max( ˆW
ˆY   0)

11: Output: ˆW1   ˆWL

1: Input: X  > 0  and normalized W1   WL
(0)← X
3: for (cid:96)= 1   L do
((cid:96))← maxW

6: for all (cid:96)= 1   L do
ˆW(cid:96)← TRIMY
((cid:96)−1)
9: Output: ˆW1   ˆWL

We would like to note that focusing on a link-normalized network is only for the sake of
In practice  such conversion is not
presenting the theoretical results in a more compact form.

Algorithm 2 presents the pseudo-code to implement the cascade Net-Trim for a link normalized

Moreover  as detailed in §2 of the supplementary note  Theorems 1 and 2 identically apply to the

  0
((cid:96)−1)
  0  

((cid:96))

(L)F ≤ γ

(L−1)

2

.

% generating initial layer outcomes:

% w(cid:96) m is the m-th column of W(cid:96)

Algorithm 2 Cascade Net-Trim

Y

4:
5: end for

% retraining:

Algorithm 1 Parallel Net-Trim

8:
9:
10: end for

(cid:96)

(cid:96)

2: Y

7:
8: end for

(cid:96) Y

  Y

6:
7:

5

4 Convex Analysis and Sample Complexity

In this section  we derive a sampling theorem for a single-layer  redundant network. Here  there are
many sets of weights that can induce the observed outputs given then input vectors. This scenario
might arise when the number of training samples used to train a (large) network is small (smaller than
the network degrees of freedom). We will show that when the inputs into the layers are independent
Gaussian random vectors  if there are sparse set of weights that can generate the output  then with
high probability  the Net-Trim program in (4) will ﬁnd them.
As noted above  in the case of a redundant layer  for a given input X and output Y   the relation

Y = max(W
X  0) can be established via more than one W . In this case  we hope to ﬁnd a sparse
W by setting = 0 in (4). For this value of   our central convex program decouples into M convex



programs  each searching for the m-th column in ˆW :

w

w1 s.t. wxp= ym p
ˆwm= arg min
wxp≤ 0
˜Xw
w1
= y 
   y=yΩ
˜X= X

∶ Ω
∶ Ωc −I

min
w s

s.t.

X

0

s

p∶ ym p> 0
p∶ ym p= 0
s 0 

.

  

(8)

(9)

By dropping the m index and introducing the slack variable s  program (8) can be cast as

where

0

(cid:96)∈ supp w∗
(cid:96)∈ supp s∗

 Λ(cid:96)=sign(w∗
(cid:96))
Λn1+(cid:96)=0

−1<Λ(cid:96)<1 (cid:96)∈ suppc w∗
0< Λn1+(cid:96)
(cid:96)∈ suppc s∗

and Ω={p∶ yp> 0}. For a general ˜X  not necessarily structured as above  the following result states
the sufﬁcient conditions under which a sparse pair(w∗  s∗) is the unique minimizer to (9).
Proposition 2. Consider a pair(w∗  s∗)∈(Rn1   Rn2)  which is feasible for the convex program
(9). If there exists a vector Λ=[Λ(cid:96)]∈ Rn1+n2 in the range of ˜X with entries satisfying
and for ˜Γ= supp w∗∪{n1+ supp s∗} the restricted matrix ˜X∶ ˜Γ is full column rank  then the pair
(w∗  s∗) is the unique solution to (9).
The proposed optimality result can be related to the unique identiﬁcation of a sparse w∗ from rectiﬁed
observations of the form y= max(X
w∗  0). Clearly  the structure of the feature matrix X plays
dual certiﬁcate can be constructed. As a result  we can warrant that learning w∗ can be performed
Theorem 3. Let w∗∈ RN be an arbitrary s-sparse vector  X∈ RN×P a Gaussian matrix repre-
senting the samples and µ> 1 a ﬁxed value. Given P =(11s+ 7)µ log N observations of the type
y= max(X
w∗  0)  with probability exceeding 1− N 1−µ the vector w∗ can be learned exactly

the key role here  and the construction of the dual certiﬁcate stated in Proposition 2 entirely relies on
this. As an insightful case  we show that when X is a Gaussian matrix (that is  the elements of X are
i.i.d values drawn from a standard normal distribution)  for sufﬁciently large number of samples  the

with much fewer samples than the layer degrees of freedom.

(10)





through (8).

The standard Gaussian assumption for the feature matrix X allows us to relate the number of training
samples to the number of active links in a layer. Such feature structure could be a realistic assumption
for the ﬁrst layer of the neural network. As reﬂected in the proof of Theorem 3  because of the
dependence of the set Ω to the entries in X  we need to take a totally nontrivial analysis path than the
standard concentration of measure arguments for the sum of independent random matrices. In fact 
the proof requires establishing concentration bounds for the sum of dependent random matrices.

X  0) 
∗ individually  for the observations Y = max(W
∗
∗ can be warranted as a corollary of Theorem 3.
M]∈ RN×M   where sm=w∗
m0  and
1   w∗
0< sm≤ smax for m= 1   M. For X∈ RN×P being a Gaussian matrix  set Y = max(W
X  0).
∗
If µ>(1+ logN M) and P=(11smax+ 7)µ log N  for = 0  W
∗ can be accurately learned through
(4) with probability exceeding 1−∑M

While we focused on each column of W
using the union bound  an exact identiﬁcation of W
Corollary 1. Consider an arbitrary matrix W

∗=[w∗
m=1 N 1−µ 11smax+7
11sm+7 .

6

It can be shown that for the network model in (1)  probing the network with an i.i.d sample matrix
X would generate subgaussian random matrices with independent columns as the subsequent layer
outcomes. Under certain well conditioning of the input covariance matrix of each layer  results similar
to Theorem 3 are extendable to the subsequent layers. While such results are left for a more extended
presentation of the work  Theorem 3 is brought here as a good reference for the general performance
of the proposed retraining scheme and the associated analysis theme.

5

Implementing the Convex Program

If the quadratic constraint in (3) is brought to the objective via a regularization parameter λ  the
resulting convex program decouples into M smaller programs of the form



xp≤ vm p  for p∶ ym p= 0  (11)

s.t. u

ˆwm= arg min

u

u1+ λ Q
p∶ ym p>0

u



xp− ym p2

each recovering a column of ˆW . Such decoupling of the regularized form is computationally
attractive  since it makes the trimming task extremely distributable among parallel processing units
by recovering each column of ˆW on a separate unit. Addressing the original constrained form (4) in
a fast and scalable way requires using more complicated techniques  which is left to a more extended
presentation of the work.
We can formulate the program in a standard form by introducing the index sets

 



min

u

where

∶ Ωm

in terms of u as

Ωm={p∶ ym p> 0}  Ωc
m={p∶ ym p= 0}.
Denoting the m-th row of Y by y
m and the m-th row of V by v
u1+ u
Qmu+ 2q
mu s.t. P mu cm 

Qm= λX∶ ΩmX
=−λXym  P m= X
qm=−λX∶ Ωm ymΩm
∶ Ωc
˜u=[u+;−u−]  where u−= min(u  0). This variable change naturally yields
u1= 1
u=[I −I]˜u 

 ˜P m−I
 ˜Qm ˜u+(1+ 2˜qm)
 ˜ucm
  
−1
˜P m=[P m −P m] .
˜qm= qm−qm
⊗ Qm 

The convex program (13) is now cast as the standard quadratic program

˜Qm= 1
−1

  

˜u s.t.

where

min

˜u

 

m

˜u.

˜u

0

1

m  the solution to (14) is found  the solution to (11) can be recovered via ˆwm=[I −I]˜u
∗

Once ˜u
m.
Aside from the variety of convex solvers that can be used to address (14)  we are speciﬁcally interested
in using the alternating direction method of multipliers (ADMM). In fact the main motivation to
translate (11) into (14) is the availability of ADMM implementations for problems in the form of
(14) that are reasonably fast and scalable (e.g.  see [17]). The authors have made the implementation
publicly available online3.

∗

The (cid:96)1 term in the objective of (12) can be converted into a linear term by deﬁning a new vector

m  one can equivalently rewrite (11)

cm= vmΩc

m

(12)

. (13)

(14)

6 Experiments and Discussions

Aside from the major technical contribution of the paper in providing a theoretical understanding of the
Net-Trim pruning process  in this section we present some experiments to highlight its performance
against the state of the art techniques.

3The code for the regularized Net-Trim implementation using the ADMM scheme can be accessed online at:

https://github.com/DNNToolBox/Net-Trim-v1

7

The ﬁrst set of experiments associated with the example presented in the introduction (classiﬁcation
of 2D points on nested spirals) compares the Net-Trim pruning power against the standard pruning
strategies of (cid:96)1 regularization and Dropout. The experiments demonstrate how Net-Trim can signiﬁ-
cantly improve the pruning level of a given network and produce simpler and more understandable
networks. We also compare the cascade Net-Trim against the parallel scheme. As could be expected 
for a ﬁxed level of discrepancy between the initial and retrained models  the cascade scheme is
capable of producing sparser networks. However  the computational distributability of the parallel
scheme makes it a more favorable approach for large scale and big data problems. Due to the space
limitation  these experiments are moved to §3 of the supplementary note.
We next apply Net-Trim to the problem of classifying hand-written digits of the mixed national
institute of standards and technology (MNIST) dataset. The set contains 60 000 training samples and
10 000 test instances. To examine different settings  we consider 6 networks: NN2-10K  which is a

784⋅300⋅300⋅10 network (two hidden layers of 300 nodes) and trained with 10 000 samples; NN3-30K 
a 784⋅300⋅500⋅300⋅10 network trained with 30 000 samples; and NN3-60K  a 784⋅300⋅1000⋅300⋅10
size 5× 5× 1 for the ﬁrst layer and 5× 5× 32 for the second layer  both followed by max pooling

network trained with 60 000 samples. We also consider CNN-10K  CNN-30K and CNN-60K which
are topologically identical convolutional networks trained with 10 000  30 000 and 60 000 samples 
respectively. The convolutional networks contain two convolutional layers composed of 32 ﬁlters of

and a fully connected layer of 512 neurons. While the linearity of the convolution allows using the
Net-Trim for the associated layers  here we merely consider retraining the fully connected layers.
To address the Net-Trim convex program  we use the regularized form outlined in Section 5  which is
fully capable of parallel processing. For our largest problem (associated with the fully connected layer
in CNN-60K)  retraining each column takes less than 20 seconds and distributing the independent
jobs among a cluster of processing units (in our case 64) or using a GPU reduces the overall retraining
of a layer to few minutes.
Table 1 summarize the retraining experiments. Panel (a) corresponds to the Net-Trim operating in a
low discrepancy mode (smaller )  while in panel (b) we explore more sparsity by allowing larger
discrepancies. Each neural network is trained three times with different initialization seeds and
average quantities are reported. In these tables  the ﬁrst row corresponds to the test accuracy of the
initial models. The second row reports the overall pruning rate and the third row reports the overall
discrepancy between the initial and Net-Trim retrained models. We also compare the results with
the work by Han  Pool  Tran and Dally (HPTD) [14]. The basic idea in [14] is to truncate the small
weights across the network and perform another round of training on the active weights. The forth
row reports the test accuracy after applying Net-Trim. To make a fair comparison in applying the
HPTD  we impose the same number of weights to be truncated in the HPTD technique. The accuracy
of the model after this truncation is presented on the ﬁfth row. Rows six and seven present the test
accuracy of Net-Trim and HPTD after a ﬁne training process (optional for Net-Trim).
An immediate observation is the close test error of Net-Trim compared to the initial trained models
(row four vs row one). We can observe from the second and third rows of the two tables that allowing
more discrepancy (larger ) increases the pruning level. We can also observe that the basic Net-Trim
process (row four) in many scenarios beats the HPTD (row seven)  and if we allow a ﬁne training
step after the Net-Trim (row six)  in all the scenarios a better test accuracy is achieved.
A serious problem with the HPTD is the early minima trapping (EMT). When we simply truncate the
layer transfer matrices  ignoring their actual contribution to the network  the error introduced can
be very large (row ﬁve)  and using this biased pattern as an initialization for the ﬁne training can
produce poor local minima solutions with large errors. The EMT blocks in the table correspond to
the scenarios where all three random seeds failed to generate acceptable results for this approach. In
the experiments where Net-Trim was followed by an additional ﬁne training step  this was never an
issue  since the Net-Trim outcome is already a good model solution.
In Figure 3(a)  we visualize ˆW1 after the Net-Trim process. We observe 28 bands (MNIST images are

28×28)  where the zero columns represent the boundary pixels with the least image information. It is

noteworthy that such interpretable result is achieved using the Net-Trim with no post or pre-processes.
A similar outcome of HPTD is depicted in panel (b). As a matter of fact  the authors present a similar
visualization as panel (a) in [14]  which is the result of applying the HPTD process iteratively and
going through the retraining step many times. Such path certainly produces a lot of processing load
and lacks any type of conﬁdence on being a convergent procedure.

8

Table 1: The test accuracy of different models before and after Net-Trim (NT) and HPTD [14].
Without a ﬁne training (FT) step  Net-Trim produces pruned networks in the majority of cases more
accurate than HPTD and with no risk of poor local minima. Adding an additional FT step makes
Net-Trim consistently prominent

K
0
6
-
N
N
C

K
0
3
-
N
N
C

K
0
1
-
N
N
C

K
0
3
-
3
N
N

K
0
6
-
3
N
N

Init. Mod. Acc. (%)
Total Pruning (%)

K
0
1
-
2
N
N
95.59 97.58 98.18 98.37 99.11 99.25
40.86 30.69 29.38 43.91 39.11 45.74
1.98
0.55
95.47 97.55 98.1 98.31 99.15 99.25
9.3
10.34 8.92 19.17 55.92 30.17
95.85 97.67 98.12 98.35 99.21 99.33
HPTD + FT Acc. (%) 93.56 97.32 EMT 98.16 EMT EMT

NT Overall Disc. (%)
NT No FT Acc. (%)

NT + FT Acc. (%)

HPTD No FT Acc. (%)

1.77

1.31

1.22

0.75

K
0
6
-
N
N
C

K
0
3
-
N
N
C

K
0
1
-
N
N
C

K
0
6
-
3
N
N

K
0
3
-
3
N
N

K
0
1
-
2
N
N
95.59 97.58 98.18 98.37 99.11 99.25
75.87 75.82 77.40 76.18 77.63 81.62
4.95 11.01 11.47 3.65
8.93
94.92 95.97 97.35 97.91 99.08 98.96
8.97
8.92 31.18 73.36 46.84
95.89 97.69 98.19 98.40 99.17 99.26
95.61 EMT 97.96 EMT 99.01 99.06

5.32

10.1

Init. Mod. Acc. (%)
Total Pruning (%)

NT Overall Disc. (%)
NT No FT Acc. (%)

HPTD No FT Acc. (%)

NT + FT Acc. (%)

HPTD + FT Acc. (%)

(a)

(b)

Figure 3: Visualization of ˆW1 in NN3-60K; (a) Net-Trim output; (b) standard HPTD

(a)

(b)

y
c
a
r
u
c
c
A

t
s
e
T

y
c
a
r
u
c
c
A

t
s
e
T

Noise (%)
(a)

Noise (%)
(b)

Figure 4: Noise robustness of initial and retrained networks; (a) NN2-10K; (b) NN3-30K

Also  for a deeper understanding of the robustness Net-Trim adds to the models  in Figure 4 we
have plotted the classiﬁcation accuracy of the initial and retrained models against the level of added
noise to the test data (ranging from 0 to 160%). The Net-Trim improvement in accuracy becomes
more noticeable as the noise level in the data increases. Basically  as expected  reducing the model
complexity makes the network more robust to outliers and noisy samples. It is also interesting to note
that the NN3-30K initial model in panel (b)  which is trained with more data  presents robustness to a
larger level of noise compared to NN2-10K in panel (a). However  the retrained models behave rather
similarly (blue curves) indicating the saving that can be achieved in the number of training samples
via Net-Trim.
In fact  Net-Trim can be particularly useful when the number of training samples is limited. While
overﬁtting is likely to occur in such scenarios  Net-Trim reduces the complexity of the model by
setting a signiﬁcant portion of weights at each layer to zero  yet maintaining the model consistency.
This capability can also be viewed from a different perspective  that Net-Trim simpliﬁes the process
of determining the network size. In other words  if the network used at the training phase is oversized 
Net-Trim can reduce its size to an order matching the data. Finally  aside from the theoretical and
practical contribution that Net-Trim brings to the understanding of deep neural network  the idea can
be easily generalized to retraining schemes with other regularizers (e.g.  the use of ridge or elastic net
type regularizers) or other structural constraint about the network.

9

0100200300400500600700050100150200250300010020030040050060070005010015020025030002040608010012014016065707580859095100Net-Trim Retrained ModelInitial Model02040608010012014016065707580859095100Net-Trim Retrained ModelInitial ModelReferences
[1] K. Hornik  M. Stinchcombe  H. White D. Achlioptas  and F. McSherry. Multilayer feedforward networks

are universal approximators. Neural networks  2(5):359–366  1989.

[2] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press  2016.

[3] S. Arora  A. Bhaskara  R. Ge  and T. Ma. Provable bounds for learning some deep representations. In

Proceedings of the 31st International Conference on Machine Learning  2014.

[4] K. Kawaguchi. Deep learning without poor local minima. In Preprint  2016.

[5] A. Choromanska  M. Henaff  M. Mathieu  G.B. Arous  and Y. LeCun. The loss surfaces of multilayer
networks. In Proceedings of the 18th International Conference on Artiﬁcial Intelligence and Statistics 
2015.

[6] R. Giryes  G. Sapiro  and A.M. Bronstein. Deep neural networks with random gaussian weights: A

universal classiﬁcation strategy? IEEE Transactions on Signal Processing  64(13):3444–3457  2016.

[7] Y. Bengio  N. Le Roux  P. Vincent  O. Delalleau  and P. Marcotte. Convex neural networks. In Proceedings

of the 18th International Conference on Neural Information Processing Systems  pages 123–130  2005.

[8] F. Bach. Breaking the curse of dimensionality with convex neural networks. Technical report  2014.

[9] O. Aslan  X. Zhang  and D. Schuurmans. Convex deep learning via normalized kernels. In Proceedings of

the 27th International Conference on Neural Information Processing Systems  pages 3275–3283  2014.

[10] S. Nowlan and G. Hinton. Simplifying neural networks by soft weight-sharing. Neural computation 

4(4):473–493  1992.

[11] F. Girosi  M. Jones  and T. Poggio. Regularization theory and neural networks architectures. Neural

computation  7(2):219–269  1995.

[12] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: a simple way to
prevent neural networks from overﬁtting. The Journal of Machine Learning Research  15(1):1929–1958 
2014.

[13] L. Wan  M. Zeiler  S. Zhang  Y. LeCun  and R. Fergus. Regularization of neural networks using dropconnect.

In Proceedings of the 33rd International Conference on Machine Learning  2016.

[14] S. Han  J. Pool  J. Tran  and W. Dally. Learning both weights and connections for efﬁcient neural network.

In Advances in Neural Information Processing Systems  pages 1135–1143  2015.

[15] W. Chen  J. Wilson  S. Tyree  K. Weinberger  and Y. Chen. Compressing neural networks with the hashing

trick. In International Conference on Machine Learning  pages 2285–2294  2015.

[16] S. Han  H. Mao  and W. J Dally. Deep compression: Compressing deep neural networks with pruning 

trained quantization and huffman coding. arXiv preprint arXiv:1510.00149  2015.

[17] E. Ghadimi  A. Teixeira  I. Shames  and M. Johansson. Optimal parameter selection for the alternating
direction method of multipliers (admm): quadratic problems. IEEE Transactions on Automatic Control 
60(3):644–658  2015.

10

,Mingyuan Zhou
Jack Rae
Jonathan Hunt
Ivo Danihelka
Timothy Harley
Andrew Senior
Gregory Wayne
Alex Graves
Timothy Lillicrap
Alireza Aghasi
Afshin Abdi
Nam Nguyen
Justin Romberg
David Zoltowski
Jonathan Pillow