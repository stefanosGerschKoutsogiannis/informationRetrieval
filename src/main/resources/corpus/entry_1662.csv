2014,Dynamic Rank Factor Model for Text Streams,We propose a semi-parametric and dynamic rank factor model for topic modeling  capable of (1) discovering topic prevalence over time  and (2) learning contemporary multi-scale dependence structures  providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts)  after an arbitrary monotone transformation  are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations  capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling  based on the forward-filtering backward-sampling algorithm. Moreover  an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.,Dynamic Rank Factor Model for Text Streams

Shaobo Han∗  Lin Du∗  Esther Salazar and Lawrence Carin

{shaobo.han  lin.du  esther.salazar  lcarin}@duke.edu

Duke University  Durham  NC 27708

Abstract

We propose a semi-parametric and dynamic rank factor model for topic model-
ing  capable of (i) discovering topic prevalence over time  and (ii) learning con-
temporary multi-scale dependence structures  providing topic and word correla-
tions as a byproduct. The high-dimensional and time-evolving ordinal/rank ob-
servations (such as word counts)  after an arbitrary monotone transformation  are
well accommodated through an underlying dynamic sparse factor model. The
framework naturally admits heavy-tailed innovations  capable of inferring abrupt
temporal jumps in the importance of topics. Posterior inference is performed
through straightforward Gibbs sampling  based on the forward-ﬁltering backward-
sampling algorithm. Moreover  an efﬁcient data subsampling scheme is leveraged
to speed up inference on massive datasets. The modeling framework is illustrated
on two real datasets: the US State of the Union Address and the JSTOR collection
from Science.

Introduction

1
Multivariate longitudinal ordinal/count data arise in many areas  including economics  opinion polls 
text mining  and social science research. Due to the lack of discrete multivariate distributions sup-
porting a rich enough correlation structure  one popular choice in modeling correlated categorical
data employs the multivariate normal mixture of independent exponential family distributions  after
appropriate transformations. Examples include the logistic-normal model for compositional data
[1]  the Poisson log-normal model for correlated count data [2]  and the ordered probit model for
multivariate ordinal data [3]. Moreover  a dynamic Bayesian extension of the generalized linear
model [4] may be considered  for capturing the temporal dependencies of non-Gaussian data (such
as ordinal data). In this general framework  the observations are assumed to follow an exponen-
tial family distribution  with natural parameter related to a conditionally Gaussian dynamic model
[5]  via a nonlinear transformation. However  these model speciﬁcations may still be too restrictive
in practice  for the following reasons: (i) Observations are usually discrete  non-negative and with
a massive number of zero values and  unfortunately  far from any standard parametric distributions
(e.g.  multinomial  Poisson  negative binomial and even their zero-inﬂated variants). (ii) The number
of contemporaneous series can be large  bringing difﬁculties in sharing/learning statistical strength
and in performing efﬁcient computations. (iii) The linear state evolution is not truly manifested after
a nonlinear transformation  where positive shocks (such as outliers and jumps) are magniﬁed and
negative shocks are suppressed; hence  handling temporal jumps (up and down) is a challenge for
the above models.
We present a ﬂexible semi-parametric Bayesian model  termed dynamic rank factor model (DRFM) 
that does not suffer these drawbacks. We ﬁrst reduce the effect of model misspeciﬁcation by mod-
eling the sampling distribution non-parametrically. To do so  we ﬁt the observed data only after
some implicit monotone transformation  learned automatically via the extended rank likelihood [6].
Second  instead of treating panels of time series as independent collections of variables  we analyze
them jointly  with the high-dimensional cross-sectional dependencies estimated via a latent factor

∗contributed equally

1

model. Finally  by avoiding nonlinear transformations  both smooth transitions and sudden changes
(“jumps”) are better preserved in the state-space model  using heavy-tailed innovations.
The proposed model offers an alternative to both dynamic and correlated topic models [7  8  9] 
with additional modeling facility of word dependencies  and improved ability to handle jumps. It
also provides a semi-parametric Bayesian treatment of dynamic sparse factor model. Further  our
proposed framework is applicable in the analysis of multiple ordinal time series  where the innova-
tions follow either stationary Gaussian or heavy-tailed distributions.

2 Dynamic Rank Factor Model
We perform analysis of multivariate ordinal time series. In the most general sense  such ordinal
variables indicate a ranking of responses in the sample space  rather than a cardinal measure [10].
Examples include real continuous variables  discrete ordered variables with or without numerical
scales or  more specially  counts  which can be viewed as discrete variables with integer numeric
scales. Our goal is twofold: (i) discover the common trends that govern variations in observations 
and (ii) extract interpretable patterns from the cross-sectional dependencies.
Dependencies among multivariate non-normal variables may be induced through normally dis-
tributed latent variables. Suppose we have P ordinal-valued time series yp t  p = 1  . . .   P  
t = 1  . . .   T . The general framework contains three components:

yp t ∼ g(zp t) 

zp t ∼ p(θt) 

θt ∼ q(θt−1) 

(1)
where g(·) is the sampling distribution  or marginal likelihood for the observations  the latent vari-
able zp t is modeled by p(·) (assumed to be Gaussian) with underlying system parameters θt  and
q(·) is the system equation representing Markovian dynamics for the time-evolving parameter θt.
In order to gain more model ﬂexibility and robustness against misspeciﬁcation  we propose a semi-
parametric Bayesian dynamic factor model for multiple ordinal time series analysis. The model is
based on the extended rank likelihood [6]  allowing the transformation from the latent conditionally
Gaussian dynamic model to the multivariate observations  treated non-parametrically.
Extended rank likelihood (ERL): There exist many approaches for dealing with ordinal data  how-
ever  they all have some restrictions. For continuous variables  the underlying normality assumption
could be easily violated without a carefully chosen deterministic transformation. For discrete ordi-
nal variables  an ordered probit model  with cut points  becomes computationally expensive if the
number of categories is large. For count variables  a multinomial model requires ﬁnite support on
the integer values. Poisson and negative binomial models lack ﬂexibility from a practical viewpoint 
and often lead to non-conjugacy when employing log-normal priors.
Being aware of these issues  a natural candidate for consideration is the ERL [6]. With appropriate
monotone transformations learned automatically from data  it offers a uniﬁed framework for han-
dling both continuous [11] and discrete ordinal variables. The ERL depends only on the ranks of the
observations (zero values in observations are further restricted to have negative latent variables) 

zp t ∈ D(Y ) ≡ {zp t ∈ R : yp t < yp(cid:48) t(cid:48) ⇒ zp t < zp(cid:48) t(cid:48)  and zp t ≤ 0 if yp t = 0}.

(2)
In particular  this offers a distribution-free approach  with relaxed assumptions compared to para-
metric models  such as Poisson log-normal [12]. It also avoids the burden of computing nuisance
parameters in the ordered probit model (cut points). The ERL has been utilized in Bayesian Gaussian
copula modeling  to characterize the dependence of mixed data [6]. In [13] a low-rank decompo-
sition of the covariance matrix is further employed and efﬁcient posterior sampling is developed in
[14]. The proposed work herein can be viewed as a dynamic extension of that framework.

2.1 Latent sparse dynamic factor model
In the forthcoming text  G(α  β) denotes a gamma distribution with shape parameter α and rate
parameter β  TN(l u)(µ  σ2) denotes a univariate truncated normal distribution within the interval
(l  u)  and N+(0  σ2) is the half-normal distribution that only has non-negative support.
Assume zt ∼ N (0  Ωt)  where Ωt is usually a high-dimensional (P × P ) covariance matrix.
To reduce the number of parameters  we assume a low rank factor model decomposition of the
covariance matrix Ωt = ΛV tΛT + R such that

zt = Λst + t 

t ∼ N (0  R)  R = I P .

(3)

2

0 < ρk < 1 

sk t = ρksk t−1 + δk t 

ν1/2 ∼ C+(0  h) 

Common trends (importance of topics) are captured by a low-dimensional factor score parameter
st. We assume autoregressive dynamics on sk t ← AR(1|(ρk  δk t)) with heavy-tailed innovations 
(4)
where δk t follows the three-parameter beta mixture of normal TPBN(e  f  ν) distribution [15]. Pa-
rameter e controls the peak around zero  f controls the heaviness on the tails  and ν controls the
global sparsity with a half-Cauchy prior [16]. This prior encourages smooth transitions in general 
while jumps are captured by the heavy tails. The conjugate hierarchy may be equivalently repre-
sented as

δk t ∼ TPBN(e  f  ν) 

ζ ∼ G(1/2  h2).
0)  and assume s0 k ∼ N (0  σ2
Truncated normal priors are employed on ρk  ρk ∼ TN(0 1)(µ0  σ2
s ).
Note that the extended rank likelihood is scale-free; therefore  we do not need to include a redundant
intercept parameter in (3). For the same reason  we set R = I P .
Model identiﬁability issues: Although the covariance matrix Ωt is not identiﬁable [10]  the related

correlation matrix Ct = Ω[i j] t/(cid:112)Ω[i i] tΩ[j j] t  (i  j = 1  . . .   P ) may be identiﬁed  using the

δk t ∼ N (0  τk t) 

τk t ∼ G(e  ηk t) 

ηk t ∼ G(f  ν)

ν ∼ G(1/2  ζ) 

parameter expansion technique [3  13]. Further  the rank K in the low-rank decomposition of Ωt is
also not unique. For the purpose of brevity  we do not explore this uncertainty here  but the tools
developed in the Bayesian factor analysis literature [17  18  19] can be easily adopted.
Identiﬁability is a key concern for factor analysis. Conventionally  for ﬁxed K  a full-rank  lower-
triangular structure in Λ ensures identiﬁability [20]. Unfortunately  this assumption depends on the
ordering of variables. As a solution  we add nonnegative and sparseness constraints on the factor
loadings  to alleviate the inherit ambiguity  while also improving interpretability. Also  we add a
Procrustes post-processing step [21] on the posterior samples  to reduce this indeterminacy.
The nonnegative and (near) sparseness constraints are imposed by the following hierarchy 
k ∼ C+(0  d).

(5)
Integrating out lp k and up k  we obtain a half-TPBN prior λp k ∼ TPBN+(a  b  φk). The column-
wise shrinkage parameters φk enable factors to be of different sparsity levels [22]. We set hyperpa-
rameters a = b = e = f = 0.5  d = P   h = 1  σ2
s = 1. For weakly informative priors  we set
α = β = 0.01; µ0 = 0.5  σ2
2.2 Extension to handle multiple documents
At each time point t we may have a corpus of documents {ynt
is a P -dimensional
observation vector  and Nt denotes the number of documents at time t. The model presented in
Section 2.1 is readily extended to handle this situation. Speciﬁcally  at each time point t  for each
document nt  the ERL representation for word count p  denoted by ynt

up k ∼ G(b  φk)  φ1/2

λp k ∼ N+(0  lp k)

lp k ∼ G(a  up k) 

nt=1  where ynt

t }Nt

0 = 10.

t

p t  is

p = 1  . . .   P 

t = 1  . . .   T  nt = 1  . . .   Nt 

+

where znt

t ∼ N (st  Γ)  Γ = diag(γ) 
bnt

t ∼ N (0  I P ) 
nt
t ∈ RK is the topic usage for each document ynt

t ∈ RP and P is the vocabulary size. We assume a latent factor model for znt
such that
k ∼ G(α  β) 
γ−1
t = Λbnt
znt
t + nt
t  
where Λ ∈ RP×K
is the topic-word loading matrix  representing the K topics as columns of Λ.
The factor score vector bnt
t   corresponding to loca-
tions in a low-dimensional RK space. The other parts of the model remain unchanged. The latent
trajectory s1:T represents the common trends for the K topics. Moreover  through the forward ﬁl-
tering backward sampling (FFBS) algorithm [23  24]  we also obtain time-evolving topic correlation
matrices Φt ∈ RK×K and word dependencies matrices Ct ∈ RP×P   offering a multi-scale graph
representation  a useful tool for document visualization.

t

p t = g(cid:0)znt

p t

(cid:1)  

ynt

2.3 Comparison with admixture topic models
Many topic models are uniﬁed in the admixture framework [25] 

P

Admix

(yn|w  Φ) = P

Base

yn

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)φn =

K(cid:88)

k=1

(cid:33)

wk nφk

 

(6)

where yn is the P -dimensional observation vector of word counts in the n th document  and P de-
notes the vocabulary size. Traditionally  yn is generated from an admixture of base distributions  wn
is the admixture weight (topic proportion for document n)  and φk is the canonical parameter (word

3

(cid:32)

distribution for topic k)  which denotes the location of the kth topic on the P -1 dimensional simplex.
For example  latent Dirichlet allocation (LDA) [26] assumes the base distribution to be multinomial 
with φk ∼ Dir(α0)  wn ∼ Dir(β0). The correlated topic model (CTM) [8] modiﬁes the topic dis-
tribution  with wn ∼ Logistic Normal(µ  Σ). The dynamic topic model (DTM) [7] analyzes docu-
ment collections in a known chronological order. In order to incorporate the state space model  both
the topic proportion and the word distribution are changed to logistic normal  with isotropic covari-
ance matrices wt ∼ Logistic Normal(wt−1  σ2I K) and φk t ∼ Logistic Normal(φk t−1  vI P ) 
respectively. To overcome the drawbacks of multinomial base  spherical topic models [27] assume
the von Mises-Fisher (vMF) distribution as its base distribution  with φk ∼ vMF(µ  ξ) lying on a
unit P -1 dimensional sphere. Recently in [25] the base and word distribution are both replaced with
Poisson Markov random ﬁelds (MRFs)  which characterizes word dependencies.
We present here a semi-parametric factor model formulation 

 

(7)

sk nλk

zn ∈ D(Y )

P(yn|s  Λ) (cid:44) P
with yn deﬁned as above  λk ∈ RP
+ is a vector of nonnegative weights  indicating the P vocab-
ulary usage in each individual topics k  and sn ∈ RK is the topic usage. Note that the extended
rank likelihood does not depend on any assumptions about the data marginal distribution  making it
appropriate for a broad class of ordinal-valued observations  e.g.  term frequency-inverse document
frequency (tf-idf) or rankings  beyond word counts. However  the proposed model here is not an
admixture model  as the topic usage is allowed to be either positive or negative.
The DRFM framework has some appealing advantages: (i) It is more natural and convenient to in-
corporate with sparsity  rank selection  and state-space model; (ii) it provides topic-correlations and
word-dependences as a byproduct; and (iii) computationally  this model is tractable and often leads
to locally conjugate posterior inference. DRFM has limitations. Since the marginal distributions
are of unspeciﬁed types  objective criteria (e.g. perplexity) is not directly computable. This makes
quantitative comparisons to other parametric baselines developed in the literature very difﬁcult.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)λn =

K(cid:88)

k=1

(cid:33)

3 Conjugate Posterior Inference
Let Θ = {Λ  S  L  U   φ  ω  ρ  τ   η  ν  ζ} denote the set of parameters in basic model  and let Z be
the augmented data (from the ERL). We use Gibbs sampling to approximate the joint posterior dis-
tribution p(Z  Θ|Z ∈ R(Y )). The algorithm alternates between sampling p(Z|Θ  Z ∈ R(Y )) and
p(Θ|Z  Z ∈ R(Y )) (reduced to p(Θ|Z)). The derivation of the Gibbs sampler is straightforward 
and for brevity here we only highlight the sampling steps for Z  and the forward ﬁltering backward
sampling (FFBS) steps for the trajectory s1:T . The Supplementary Material contains further details
for the inference.

• Sampling zp t: p(zp t|Θ  Z ∈ R(Y )  Z−p −t) ∼ TN[zp t zp t]((cid:80)K

k=1 λp ksk t  1)  where zp t =

max{zp(cid:48) t(cid:48) : yp(cid:48) t(cid:48) < yp t} and zp t = min{zp(cid:48) t(cid:48) : yp(cid:48) t(cid:48) > yp t}.

This conditional sampling scheme is widely used in [6  10  13]. In [14] a novel Hamiltonian Monte
Carlo (HMC) approach has been developed recently  for a Gaussian copula extended rank likelihood
model  where ranking is only within each row of Z. This method simultaneously samples a column
vector of zi conditioned on other columns Z−i  with higher computation but better mixing.
• Sampling st: we have the state model st|st−1 ∼ N (Ast−1  Qt)  and the observation model
zt|st ∼ N (Λst  R) 1 where A = diag(ρ)  Qt = diag(τ t)  R = I P . for t = 1  . . .   T
1. Forward Filtering: beginning at t = 0 with s0 ∼ N (0  σ2

s I K)  for all t = 1  . . .   T   we
ﬁnd the on-line posteriors at t  p(st|z1:t) = N (mt  V t)  where mt = V t{ΛT R−1zt +
H−1

2. Backward Sampling: starting from N ((cid:102)mt (cid:101)V t)  the backward smoothing density  i.e.  the
conditional distribution of st−1 given st  is p(st−1|st  z1:(t−1)) = N ((cid:101)µt−1 (cid:101)Σt−1)  where
(cid:101)µt−1 = (cid:101)Σt−1{AT Q−1

t−1mt−1}  (cid:101)Σt−1 = (V −1

t + ΛT R−1Λ]−1  and H t = Qt + AV t−1AT .

t Amt−1}  V t = [H−1

t st + V −1

t−1 + AT Q−1

t A)−1.

There exist different variants of FFBS schemes (see [28] for a detailed comparison); the method we
choose here enjoys fast decay in autocorrelation and reduced computation time.

1For brevity  we omit the dependencies on Θ in notation

4

t (cid:102)mt + V −1

3.1 Time-evolving topic and word dependencies

t−1mt−1) and (cid:101)V t−1 = (cid:101)Σt−1 + (cid:101)Σt−1AT Q−1

We also have the backward recursion density at t − 1  p(st−1|z1:T ) = N ((cid:102)mt−1 (cid:101)V t−1)  where
(cid:102)mt−1 = (cid:101)Σt−1(AT Q−1
t A(cid:101)Σt−1.
covariances {(cid:101)V 1:T} (with topic correlation matrices Φ1:T   Φ[r s] t = V[r s] t/(cid:112)V[r r] tV[s s] t  r  s =
with Ωt = Λ(cid:101)V tΛT + I P . Essentially  this can be viewed as a dynamic Gaussian copula model 
yp t = g((cid:101)zp t) (cid:101)zt ∼ N (0  Ct)  where g(·) is a non-decreasing function of a univariate marginal

We perform inference on the K × K time-evolving topic dependences in s1:T   using the posterior
1  . . .   K)  and further obtain the P × P time-evolving word dependencies capsuled in {Ω1:T}

likelihood and Ct (t = 1  . . .   T ) is the correlation matrix capturing the multivariate dependence.
We obtain a posterior distribution for C1:T as a byproduct  without having to estimate the nuisance
parameters in marginal likelihoods g(·). This decoupling strategy resembles the idea of copula
models.

t (cid:101)V tQ−1

3.2 Accelerated MCMC via document subsampling
For large-scale datasets  recent approaches efﬁciently reduce the computational load of Monte Carlo
Markov chain (MCMC) by data subsampling [29  30]. We borrow this idea of subsampling docu-
ments when considering a large corpora (e.g.  in our experiments  we consider analysis of articles
in the magazine Science  composed of 139379 articles from years 1880 to 2002  and a vocabulary
size 5855). In our model  the augmented data znt
(nt = 1  . . .   Nt) for each document is relatively
t
expensive to sample. One simple method is random document sampling without replacement. How-
ever  by treating all likelihood contributions symmetrically  this method leads to a highly inefﬁcient
MCMC chain with poor mixing [29].
Alternatively  we adopt the probability proportional-to-size (PSS) sampling scheme in [30]  i.e. 
sampling the documents with inclusion probability proportional to the likelihood contributions. For
each MCMC iteration  the sub-sampling procedure for documents at time t is designed as follows:
t} for all
• Step 1: Given a small subset Vt ⊂ {1  . . .   Nt} of chosen documents  only sample {zd
N (Λst (cid:101)R)  where (cid:101)R = ΛΓΛT + I P . Note that  only a K-dimensional matrix inversion is
d ∈ Vt and compute the augment log-likelihood contributions (with Bt integrated out) (cid:96)Vt(zd
t ) =
required  by using the Woodbury matrix inversion formula (cid:101)R

= I P − Λ(Γ−1 + ΛT Λ)T ΛT .
• Step 2: Similar to [30]  we use a Gaussian process [31] to predict the log-likelihood for
t )  where K is a Nt ×
t ) =

the remaining documents (cid:96)V c
Nt squared-exponential kernel  which denotes the similarity of documents: K(yi
σ2
f exp

t  Vt)K(Vt Vt)−1(cid:96)Vt(zd

(cid:16)−||yi

  i  j = 1  . . .   Nt  σ2

t ) = K(V c

f = 1  s = 1.

t − yj

t||2/(2s2)

• Step 3: Calculate the inclusion probability wd ∝ exp [(cid:96)(zd

• Step 4: Sampling the next subset Vt of pre-speciﬁed size |Vt| with inclusion probability (cid:101)wd  and

d(cid:48) wd(cid:48).

(zd

t

(cid:17)

t  yj

t )]  d = 1  . . .   Nt  (cid:101)wd = wd/(cid:80)

−1

store it for the use of the next MCMC iteration.

In practice  this adaptive design allows MCMC to run more efﬁciently on a full dataset of large
scale  often mitigating the need to do parallel MCMC implementation. Future work could also con-
sider nonparametric function estimation subject to monotonicity constraint  e.g. Gaussian process
projections recently developed in [32].

4 Experiments
Different from DTM [7]   the proposed model has the jumps directly at the level of the factor scores
(no exponentiation or normalization needed)  and therefore it proved more effective in uncovering
jumps in factor scores over time. Demonstrations of this phenomenon in a synthetic experiment are
detailed in the Supplementary Material. In the following  we present exploratory data analysis on
two real examples  demonstrating the ability of the proposed model to infer temporal jumps in topic
importance  and to infer correlations across topics and words.

4.1 Case Study I: State of the Union dataset
The State of the Union dataset contains the transcripts of T = 225 US State of the Union addresses 
from 1790 to 2014. We take each transcript as a document  i.e.  we have one document per year.

5

After removing stop words  and removing terms that occur fewer than 3 times in one document and
less than 10 times overall  we have P = 7518 unique words. The observation yp t corresponds to
the frequency of word p of the State of the Union transcript from year t.
We apply the proposed DRFM setting and learned K = 25 topics. To better understand the temporal
dynamic per topic  six topics are selected and the posterior mean of their latent trajectories sk 1:T
are shown in Figure 1 (with also the top 12 most probable words associated with each of the topics).
A complete table with all 25 learned topics and top 12 words is provided in the Supplementary
Material. The learned trajectory associated with every topic indicates different temporal patterns
across all the topics. Clearly  we can identify jumps associated with some key historical events. For
instance  for Topic 10  we observe a positive jump in 1846 associated with the Mexican-American
war. Topic 13 is related with the Spanish-American war of 1898  with a positive jump in that year.
In Topic 24  we observe a positive jump in 1914  when the Panama Canal was ofﬁcially opened
(words Panana and canal are included). In Topic 18  the positive jumps observed from 1997 to
1999 seem to be associated with the creation of the State Children’s Health Insurance Program in
1997. We note that the words for this topic are explicitly related with this issue. Topic 25 appears to
be related to banking; the signiﬁcant spike around 1836 appears to correspond to the Second Bank
of the United States  which was allowed to go out of existence  and end national banking that year.
In 1863 Congress passed the National Banking Act  which ended the “free-banking” period from
1836-1863; note the spike around 1863 in Topic 25.

Topic#10
Mexico
Government
Texas
United
War
Mexican
Army
Territory
Country
Peace
Policy
Lands

Topic#13
Government
United
Islands
Commission
Island
Cuba
Spain
Act
General
Military
International
Ofﬁciers

Topic#24
United
Treaty
Isthmus
Public
Panama
Law
Territory
America
Canal
Service
Banks
Colombia

Topic#17
Jobs
Country
Tax
American
Economy
Deﬁcit
Americans
Energy
Businesses
Health
Plan
Care

Topic#18
Children
America
Americans
Care
Tonight
Support
Century
Health
Working
Challenge
Security
Families

Topic#25
Government
Public
Banks
Bank
Currency
Money
United
Federal
American
National
Duty
Institutions

Figure 1: (State of the Union dataset) Above: Time evolving from 1790 to 2014 for six selected
topics. The plotted values represent the posterior means. Below: Top 12 most probable words
associated with the above topics.

Our modeling framework is able to capture dynamic patterns of topics and word correlations. To
illustrate this  we select three years (associated with some meaningful historical events) and analyze
their corresponding topic and word correlations. Figure 2 (ﬁrst row) shows graphs of the topic
correlation matrices  in which the nodes represent topics and the edges indicate positive (green) and
negative (red) correlations (we show correlations with absolute value larger than 0.01). We notice
that Topics 11 and 22 are positively correlated with those years. Some of the most probable words
increase  united  law and legislation (for Topic 11) and war 
associated with each of them are:
Mexico  peace  army  enemy and military (for Topic 22). We also are interested in understanding
the time-varying correlation between words. To do so  and for the same years as before  in Figure 2
(second row) we plot the dendrogram associated with the learned correlation matrix for words. In
the plots  different colors indicate highly correlated word clusters deﬁned by cutting the branches off
the dendrogram. Those ﬁgures reveal different sets of highly correlated words for different years. By

6

024 Topic 100246 Topic 13180018501900195020000246 Topic 24024 Topic 170246 Topic 1818001850190019502000-50510 Topic 251846

Mexican-American War

1929

Economic Depression

2003

Iraq War

Figure 2: (State of the Union dataset) First row: Inferred correlations between topics for some
speciﬁc years associated with some meaningful historical events. Green edges indicate positive
correlations and red edges indicate negative correlations. Second row: Learned dendrogram based
upon the correlation matrix between the top 10 words associated with each topic (we display 80
unique words in total).
inspecting all the words correlation  we noticed that the set of words {government  federal  public 
power  authority  general  country} are highly correlated across the whole period.

4.2 Case Study II: Analysis of Science dataset
We analyze a collection of scientiﬁc documents from the JSTOR Science journal [7]. This dataset
contains a collection of 139379 documents from 1880 to 2002 (T = 123)  with approximately 1100
documents per year. After removing terms that occurred fewer than 25 times  the total vocabulary
size is P = 5855. We learn K = 50 topics from the inferred posterior distribution  for brevity and
simplicity  we only show 20 of them. We handle about 2700 documents per iteration (subsampling
rate: 2%). Table 1 shows the 20 selected topics and the top 10 most probable words associated with
each of them. By inspection  we notice that those topics are related with speciﬁc ﬁelds in science.
For instance  Topic 2 is more related to “scientiﬁc research”  Topic 10 to “natural resources”  and

Topic 15 to “genetics”. Figure 3 shows the time-varying trend for some speciﬁc words (cid:98)zp 1:T   which

reveals the importance of those words across time. Finally  Figure 4 shows the correlation between
the selected 20 topics. For instance  in 1950 and 2000  topic 9 (related to mouse  cells  human 
transgenic) and topic 17 (related to virus  rna  tumor  infection) are highly correlated.

Figure 3: (Science dataset) the inferred latent trend for variable(cid:98)zp 1:T associated with words.

7

T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20T21T22T23T24T25T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20T21T22T23T24T25T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20T21T22T23T24T25actadministrationamericaamericanamericansarmyattentionauthoritybanksbusinessbillionbondscanalcarechildrencountrycourtcurrencycitizensconstitutionconventioncubadepartmentdevelopmentdollarseconomicenergyexpendituresfederalfiscalforcesforeignfreedomfreegeneralgovernmentgoldhealthincreaseislandsislandjobsjunelaborlawmexicanmexicomilitarymillionnationalnationsnationnotesnumberorderpeacepolicypowerpresidentprogrampublicpresentprogramsreservereportsecretaryservicesilverspainsubjecttaxtradetreasurytreatyterritorytexastonightunionunitedwaractadministrationamericaamericanamericansarmyattentionauthoritybanksbusinessbillionbondscanalcarechildrencountrycourtcurrencycitizensconstitutionconventioncubadepartmentdevelopmentdollarseconomicenergyexpendituresfederalfiscalforcesforeignfreedomfreegeneralgovernmentgoldhealthincreaseislandsislandjobsjunelaborlawmexicanmexicomilitarymillionnationalnationsnationnotesnumberorderpeacepolicypowerpresidentprogrampublicpresentprogramsreservereportsecretaryservicesilverspainsubjecttaxtradetreasurytreatyterritorytexastonightunionunitedwaractadministrationamericaamericanamericansarmyattentionauthoritybanksbusinessbillionbondscanalcarechildrencountrycourtcurrencycitizensconstitutionconventioncubadepartmentdevelopmentdollarseconomicenergyexpendituresfederalfiscalforcesforeignfreedomfreegeneralgovernmentgoldhealthincreaseislandsislandjobsjunelaborlawmexicanmexicomilitarymillionnationalnationsnationnotesnumberorderpeacepolicypowerpresidentprogrampublicpresentprogramsreservereportsecretaryservicesilverspainsubjecttaxtradetreasurytreatyterritorytexastonightunionunitedwar−1.0−0.50.00.51.01880190019201940196019802000−1−0.500.511.522.5 DNARNAGene188019001920194019601980200000.20.40.60.81 CancerPatientsNuclear188019001920194019601980200000.10.20.30.40.50.60.7 AstronomyPsychologyBrain1900

1950

2000

Figure 4: (Science dataset) Inferred correlations between topics for some speciﬁc years. Green
edges indicate positive correlations and red edges indicate negative correlations.

Table 1: Selected 20 topics associated with the analysis of the Science dataset and top 10 most
probable words.
Topic#1
cells
cell
normal
two
growth
development development
tissue
body
egg
blood
Topic#11
system
nuclear
new
systems
power
cost
computer
fuel
coal
plant

Topic#9
Topic#8
mice
work
mouse
research
type
scientiﬁc
wild
laboratory
ﬁg
made
cells
university
human
results
transgenic
science
survey
animals
department mutant
Topic#19
Topic#18
stars
energy
mass
electron
star
state
temperature protein
ﬁg
solar
two
gas
structure
data
reaction
density
laser
high
surface
temperature galaxies

Topic#3
ﬁeld
magnetic
solar
energy
spin
state
electron
new
quantum
program
temperature
scientiﬁc
current
basic
Topic#13
Topic#12
association
energy
theory
science
temperature meeting
radiation
atoms
surface
atomic
mass
atom
time

Topic#7
science
scientiﬁc
new
scientists
human
men
sciences
knowledge
meeting
work
Topic#17
virus
rna
viruses
particles
tumor
mice
disease
viral
human
infection

Topic#6
university
professor
college
president
department
research
institute
director
society
school
Topic#16
professor
university
society

Topic#5
energy
oil
percent
production
fuel
total
growth
states
electricity
coal
Topic#15
human
genome
sequence
chromosome department
gene
genes
map
data
sequences
genetic

college
president
director
american
appointed
medical

Topic#10
water
surface
temperature
soil
pressure
sea
plants
solution
plant
air
Topic#20
rna
ﬁg
mrna

site
sequence
splicing
synthesis
trna
rnas

Topic#2
research
national
government
support
federal

Topic#4
animals
brain
neurons
activity
response
rats
control
ﬁg
effects
days
Topic#14
protein
proteins
cell
membrane
amino
sequence
binding
acid
residues
sequences

university
american
society
section
president
committee
secretary

5 Discussion

We have proposed a DRFM framework that could be applied to a broad class of applications such
as: (i) dynamic topic model for the analysis of time-stamped document collections; (ii) joint analy-
sis of multiple time series  with ordinal valued observations; and (iii) multivariate ordinal dynamic
factor analysis or dynamic copula analysis for mixed type of data. The proposed model is a semi-
parametric methodology  which offers modeling ﬂexibilities and reduces the effect of model mis-
speciﬁcation. However  as the marginal likelihood is distribution-free  we could not calculate the
model evidence or other evaluation metrics based on it (e.g. held-out likelihood). As a consequence 
we are lack of objective evaluation criteria  which allow us to perform formal model comparisons.
In our proposed setting  we are able to perform either retrospective analysis or multi-step ahead
forecasting (using the recursive equations derived in the FFBS algorithm). Finally  our inference
framework is easily adaptable for using sequential Monte Carlo (SMC) methods [33] allowing on-
line learning.

Acknowledgments

The research reported here was funded in part by ARO  DARPA  DOE  NGA and ONR. The authors
are grateful to Jonas Wallin  Lund University  Sweden  for providing efﬁcient package on simulation
of the GIG distribution.

8

T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20T1T2T3T4T5T6T7T8T9T10T11T12T13T14T15T16T17T18T19T20−1.0−0.50.00.51.0References
[1] J. Aitchison. The statistical analysis of compositional data. J. Roy. Stat. Soc. Ser. B  44(2):139–177  1982.
[2] S. Chib and R. Winkelmann. Markov chain Monte Carlo analysis of correlated count data. Journal of

Business & Economic Statistics  19(4)  2001.

[3] E. Lawrence  D. Bingham  C. Liu  and V. N. Nair. Bayesian inference for multivariate ordinal data using

parameter expansion. Technometrics  50(2)  2008.

[4] M. West  P. J. Harrison  and H. S. Migon. Dynamic generalized linear models and Bayesian forecasting.

J. Am. Statist. Assoc.  80(389):73–83  1985.

[5] C. Cargnoni  P. M¨uller  and M. West. Bayesian forecasting of multinomial time series through condition-

ally Gaussian dynamic models. J. Am. Statist. Assoc.  92(438):640–647  1997.

[6] P. D. Hoff. Extending the rank likelihood for semiparametric copula estimation. Ann. Appl. Statist. 

1(1):265–283  2007.

[7] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Int. Conf. Machine Learning  2006.
[8] D. M. Blei and J. D. Lafferty. Correlated topic models. In Adv. Neural Inform. Processing Systems  2006.
[9] A. Ahmed and E. P. Xing. Timeline: A dynamic hierarchical dirichlet process model for recovering

birth/death and evolution of topics in text stream. 2010.

[10] P. D. Hoff. A ﬁrst course in Bayesian statistical methods. Springer  2009.
[11] A. N. Pettitt. Inference for the linear model using a likelihood based on ranks. J. Roy. Stat. Soc. Ser. B 

44(2):234–243  1982.

[12] J. Aitchison and C. H. Ho. The multivariate Poisson-log normal distribution. Biometrika  76(4):643–653 

1989.

[13] J. S. Murray  D. B. Dunson  L. Carin  and J. E. Lucas. Bayesian Gaussian copula factor models for mixed

data. J. Am. Statist. Assoc.  108(502):656–665  2013.

[14] A. Kalaitzis and R. Silva. Flexible sampling of discrete data correlations without the marginal distribu-

tions. In Adv. Neural Inform. Processing Systems  2013.

[15] A. Armagan  M. Clyde  and D. B. Dunson. Generalized Beta mixtures of Gaussians.

Inform. Processing Systems  2011.

In Adv. Neural

[16] N. G. Polson and J. G. Scott. On the half-Cauchy prior for a global scale parameter. Bayesian Analysis 

7(4):887–902  2012.

[17] H. F. Lopes and M. West. Bayesian model assessment in factor analysis. Statistica Sinica  14(1):41–68 

2004.

[18] J. Ghosh and D. B. Dunson. Default prior distributions and efﬁcient posterior computation in Bayesian

factor analysis. Journal of Computational and Graphical Statistics  18(2):306–320  2009.

[19] A. Bhattacharya and D. B. Dunson. Sparse Bayesian inﬁnite factor models. Biometrika  98(2):291–306 

2011.

[20] J. Geweke and G. Zhou. Measuring the pricing error of the arbitrage pricing theory. Review of Financial

Studies  9(2):557–587  1996.

[21] A. Christian  B. Jens  and P. Markus. Bayesian analysis of dynamic factor models: an ex-post approach

towards the rotation problem. Kiel Working Papers 1902  Kiel Institute for the World Economy  2014.

[22] C. Gao and B. E. Engelhardt. A sparse factor analysis model for high dimensional latent spaces. In NIPS:
Workshop on Analysis Operator Learning vs. Dictionary Learning: Fraternal Twins in Sparse Modeling 
2012.

[23] C. K. Carter and R. Kohn. On Gibbs sampling for state space models. Biometrika  81:541–553  1994.
[24] S. Fr¨uhwirth-Schnatter. Data augmentation and dynamic linear models. Journal of Times Series Analysis 

15:183–202  1994.

[25] D. Inouye  P. Ravikumar  and I. Dhillon. Admixture of Poisson MRFs: A topic model with word depen-

dencies. In Int. Conf. Machine Learning  2014.

[26] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. J. Machine Learn. Res.  3:993–1022 

2003.

[27] J. Reisinger  A. Waters  B. Silverthorn  and R. J. Mooney. Spherical topic models. In Int. Conf. Machine

Learning  2010.

[28] E. A. Reis  E. Salazar  and D. Gamerman. Comparison of sampling schemes for dynamic linear models.

International Statistical Review  74(2):203–214  2006.

[29] A. Korattikara  Y. Chen  and M. Welling. Austerity in MCMC land: cutting the Metropolis-Hastings

budget. In Int. Conf. Machine Learning  pages 181–189  2014.

[30] M. Quiroz  M. Villani  and R. Kohn.

arXiv:1404.4178  2014.

Speeding up MCMC by efﬁcient data subsampling.

[31] C. E. Rasmussen. Gaussian processes in machine learning. Springer  2004.
[32] L. Lin and D. B. Dunson. Bayesian monotone regression using Gaussian process projection. Biometrika 

101(2):303–317  2014.

[33] A. Doucet  D. F. Nando  and N. Gordon. Sequential Monte Carlo methods in practice. Springer  2001.

9

,Shaobo Han
Lin Du
Esther Salazar
Lawrence Carin