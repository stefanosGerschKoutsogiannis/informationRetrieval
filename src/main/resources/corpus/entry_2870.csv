2007,Learning Bounds for Domain Adaptation,Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world  though  we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains  each of which may have a different number of instances  and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.,LearningBoundsforDomainAdaptationJohnBlitzer KobyCrammer AlexKulesza FernandoPereira andJenniferWortmanDepartmentofComputerandInformationScienceUniversityofPennsylvania Philadelphia PA19146{blitzer crammer kulesza pereira wortmanj}@cis.upenn.eduAbstractEmpiricalriskminimizationofferswell-knownlearningguaranteeswhentrainingandtestdatacomefromthesamedomain.Intherealworld though weoftenwishtoadaptaclassiﬁerfromasourcedomainwithalargeamountoftrainingdatatodifferenttargetdomainwithverylittletrainingdata.Inthisworkwegiveuniformconvergenceboundsforalgorithmsthatminimizeaconvexcombinationofsourceandtargetempiricalrisk.Theboundsexplicitlymodeltheinherenttrade-offbetweentrainingonalargebutinaccuratesourcedatasetandasmallbutaccuratetargettrainingset.Ourtheoryalsogivesresultswhenwehavemultiplesourcedomains eachofwhichmayhaveadifferentnumberofinstances andweexhibitcasesinwhichminimizinganon-uniformcombinationofsourceriskscanachievemuchlowertargeterrorthanstandardempiricalriskminimization.1IntroductionDomainadaptationaddressesacommonsituationthatariseswhenapplyingmachinelearningtodi-versedata.Wehaveampledatadrawnfromasourcedomaintotrainamodel butlittleornotrainingdatafromthetargetdomainwherewewishtousethemodel[17 3 10 5 9].Domainadaptationquestionsariseinnearlyeveryapplicationofmachinelearning.Infacerecognitionsystems trainingimagesareobtainedunderonesetoflightingorocclusionconditionswhiletherecognizerwillbeusedunderdifferentconditions[14].Inspeechrecognition acousticmodelstrainedbyonespeakerneedtobeusedbyanother[12].Innaturallanguageprocessing part-of-speechtaggers parsers anddocumentclassiﬁersaretrainedoncarefullyannotatedtrainingsets butappliedtotextsfromdifferentgenresorstyles[7 6].Whilemanydomain-adaptationalgorithmshavebeenproposed thereareonlyafewtheoreticalstudiesoftheproblem[3 10].Thosestudiesfocusonthecasewheretrainingdataisdrawnfromasourcedomainandtestdataisdrawnfromadifferenttargetdomain.Wegeneralizethisapproachtothecasewherewehavesomelabeleddatafromthetargetdomaininadditiontoalargeamountoflabeledsourcedata.Ourmainresultisauniformconvergenceboundonthetruetargetriskofamodeltrainedtominimizeaconvexcombinationofempiricalsourceandtargetrisks.Thebounddescribesanintuitivetradeoffbetweenthequantityofthesourcedataandtheaccuracyofthetargetdata andunderrelativelyweakassumptionswecancomputeitfromﬁnitelabeledandunlabeledsamplesofthesourceandtargetdistributions.Weusethetaskofsentimentclassiﬁcationtodemonstratethatourboundmakescorrectpredictionsaboutmodelerrorwithrespecttoadistancemeasurebetweensourceandtargetdomainsandthenumberoftraininginstances.Finally weextendourtheorytothecaseinwhichwehavemultiplesourcesoftrainingdata eachofwhichmaybedrawnaccordingtoadifferentdistributionandmaycontainadifferentnumberofinstances.Severalauthorshaveempiricallystudiedaspecialcaseofthisinwhicheachinstanceisweightedseparatelyinthelossfunction andinstanceweightsaresettoapproximatethetargetdomaindistribution[10 5 9 11].Wegiveauniformconvergenceboundforalgorithmsthatmin-1imizeaconvexcombinationofmultipleempiricalsourcerisksandweshowthatthesealgorithmscanoutperformstandardempiricalriskminimization.2ARigorousModelofDomainAdaptationWeformalizedomainadaptationforbinaryclassiﬁcationasfollows.AdomainisapairconsistingofadistributionDonXandalabelingfunctionf:X→[0 1].1Initiallyweconsidertwodomains asourcedomainhDS fSiandatargetdomainhDT fTi.Ahypothesisisafunctionh:X→{0 1}.TheprobabilityaccordingthedistributionDSthatahypothesishdisagreeswithalabelingfunctionf(whichcanalsobeahypothesis)isdeﬁnedasǫS(h f)=Ex∼DS[|h(x)−f(x)|].Whenwewanttorefertotheriskofahypothesis weusetheshorthandǫS(h)=ǫS(h fS).WewritetheempiricalriskofahypothesisonthesourcedomainasˆǫS(h).WeusetheparallelnotationǫT(h f) ǫT(h) andˆǫT(h)forthetargetdomain.WemeasurethedistancebetweentwodistributionsDandD′usingahypothesisclass-speciﬁcdis-tancemeasure.LetHbeahypothesisclassforinstancespaceX andAHbethesetofsubsetsofXthatarethesupportofsomehypothesisinH.Inotherwords foreveryhypothesish∈H {x:x∈X h(x)=1}∈AH.Wedeﬁnethedistancebetweentwodistributionsas:dH(D D′)=2supA∈AH|PrD[A]−PrD′[A]|.Forourpurposes thedistancedHhasanimportantadvantageovermorecommonmeansforcom-paringdistributionssuchasL1distanceortheKLdivergence:wecancomputedHfromﬁniteunlabeledsamplesofthedistributionsDandD′whenHhasﬁniteVCdimension[4].Furthermore wecancomputeaﬁnite-sampleapproximationtodHbyﬁndingaclassiﬁerh∈Hthatmaximallydiscriminatesbetween(unlabeled)instancesfromDandD′[3].ForahypothesisspaceH wedeﬁnethesymmetricdifferencehypothesisspaceH∆HasH∆H={h(x)⊕h′(x):h h′∈H} where⊕istheXORoperator.Eachhypothesisg∈H∆HlabelsaspositiveallpointsxonwhichagivenpairofhypothesesinHdisagree.WecanthendeﬁneAH∆HinthenaturalwayasthesetofallsetsAsuchthatA={x:x∈X h(x)6=h′(x)}forsomeh h′∈H.ThisallowsustodeﬁneasaboveadistancedH∆Hthatsatisﬁesthefollowingusefulinequalityforanyhypothesesh h′∈H whichisstraight-forwardtoprove:|ǫS(h h′)−ǫT(h h′)|≤12dH∆H(DS DT).Weformalizethedifferencebetweenlabelingfunctionsbymeasuringerrorrelativetootherhypothe-sesinourclass.Theidealhypothesisminimizescombinedsourceandtargetrisk:h∗=argminh∈HǫS(h)+ǫT(h).Wedenotethecombinedriskoftheidealhypothesisbyλ=ǫS(h∗)+ǫT(h∗).Theidealhypothesisexplicitlyembodiesournotionofadaptability.Whentheidealhypothesisperformspoorly wecannotexpecttolearnagoodtargetclassiﬁerbyminimizingsourceerror.2Ontheotherhand forthekindsoftasksmentionedinSection1 weexpectλtobesmall.Ifthisisthecase wecanreasonablyapproximatetargetriskusingsourceriskandthedistancebetweenDSandDT.Weillustratethekindofresultavailableinthissettingwiththefollowingboundonthetargetriskintermsofthesourcerisk thedifferencebetweenlabelingfunctionsfSandfT andthedistancebetweenthedistributionsDSandDT.ThisboundisessentiallyarestatementofthemaintheoremofBen-Davidetal.[3] withasmallcorrectiontothestatementoftheirtheorem.1Thisnotionofdomainisnotthedomainofafunction.Toavoidconfusion wewillalwaysmeanaspeciﬁcdistributionandfunctionpairwhenwesaydomain.2Ofcourseitisstillpossiblethatthesourcedatacontainsrelevantinformationaboutthetargetfunctionevenwhentheidealhypothesisperformspoorly—suppose forexample thatfS(x)=1ifandonlyiffT(x)=0—butaclassiﬁertrainedusingsourcedatawillperformpoorlyondatafromthetargetdomaininthiscase.2Theorem1LetHbeahypothesisspaceofVC-dimensiondandUS UTbeunlabeledsamplesofsizem′each drawnfromDSandDT respectively.LetˆdH∆HbetheempiricaldistanceonUS UT inducedbythesymmetricdifferencehypothesisspace.Withprobabilityatleast1−δ(overthechoiceofthesamples) foreveryh∈H ǫT(h)≤ǫS(h)+12ˆdH∆H(US UT)+4s2dlog(2m′)+log(4δ)m′+λ.ThecorrectedproofofthisresultcanbefoundAppendixA.3Themainstepintheproofisavariantofthetriangleinequalityinwhichthesidesofthetrianglerepresenterrorsbetweendifferentdecisionrules[3 8].Theboundisrelativetoλ.Whenthecombinederroroftheidealhypothesisislarge thereisnoclassiﬁerthatperformswellonboththesourceandtargetdomains sowecannothopetoﬁndagoodtargethypothesisbytrainingonlyonthesourcedomain.Ontheotherhand forsmallλ(themostrelevantcasefordomainadaptation) Theorem1showsthatsourceerrorandunlabeledH∆H-distanceareimportantquantitiesforcomputingtargeterror.3ALearningBoundCombiningSourceandTargetDataTheorem1showshowtorelatesourceandtargetrisk.Wenowproceedtogivealearningboundforempiricalriskminimizationusingcombinedsourceandtargettrainingdata.Inordertosimplifythepresentationofthetrade-offsthatariseinthisscenario westatetheboundintermsofVCdimension.Similar tighterboundscouldbederivedusingmoresophisticatedmeasuresofcomplexitysuchasPAC-Bayes[15]orRademachercomplexity[2]inananalogousway.AttraintimealearnerreceivesasampleS=(ST SS)ofminstances whereSTconsistsofβminstancesdrawnindependentlyfromDTandSSconsistsof(1−β)minstancesdrawnindependentlyfromDS.ThegoalofalearneristoﬁndahypothesisthatminimizestargetriskǫT(h).Whenβissmall asindomainadaptation minimizingempiricaltargetriskmaynotbethebestchoice.Weanalyzelearnersthatinsteadminimizeaconvexcombinationofempiricalsourceandtargetrisk:ˆǫα(h)=αˆǫT(h)+(1−α)ˆǫS(h)Wedenoteasǫα(h)thecorrespondingweightedcombinationoftruesourceandtargetrisks mea-suredwithrespecttoDSandDT.Weboundthetargetriskofadomainadaptationalgorithmthatminimizesˆǫα(h).Theproofoftheboundhastwomaincomponents whichwestateaslemmasbelow.FirstweboundthedifferencebetweenthetargetriskǫT(h)andweightedriskǫα(h).Thenweboundthedifferencebetweenthetrueandempiricalweightedrisksǫα(h)andˆǫα(h).Theproofsoftheselemmas aswellastheproofofTheorem2 areinAppendixB.Lemma1LethbeahypothesisinclassH.Then|ǫα(h)−ǫT(h)|≤(1−α)(cid:18)12dH∆H(DS DT)+λ(cid:19).Thelemmashowsthatasαapproaches1 werelyincreasinglyonthetargetdata andthedistancebetweendomainsmatterslessandless.TheproofusesasimilartechniquetothatofTheorem1.Lemma2LetHbeahypothesisspaceofVC-dimensiond.IfarandomlabeledsampleofsizemisgeneratedbydrawingβmpointsfromDTand(1−β)mpointsfromDS andlabelingthemaccordingtofSandfTrespectively thenwithprobabilityatleast1−δ(overthechoiceofthesamples) foreveryh∈H|ˆǫα(h)−ǫα(h)|<sα2β+(1−α)21−βrdlog(2m)−logδ2m.3Alongerversionofthispaperthatincludestheomittedappendixcanbefoundontheauthors’websites.3Theproofissimilartostandarduniformconvergenceproofs[16 1] butitusesHoeffding’sin-equalityinadifferentwaybecausetheboundontherangeoftherandomvariablesunderlyingtheinequalityvarieswithαandβ.Thelemmashowsthatasαmovesawayfromβ(whereeachinstanceisweightedequally) ourﬁnitesampleapproximationtoǫα(h)becomeslessreliable.Theorem2LetHbeahypothesisspaceofVC-dimensiond.LetUSandUTbeunlabeledsamplesofsizem′each drawnfromDSandDTrespectively.LetSbealabeledsampleofsizemgeneratedbydrawingβmpointsfromDTand(1−β)mpointsfromDS labelingthemaccordingtofSandfT respectively.Ifˆh∈Histheempiricalminimizerofˆǫα(h)onSandh∗T=minh∈HǫT(h)isthetargetriskminimizer thenwithprobabilityatleast1−δ(overthechoiceofthesamples) ǫT(ˆh)≤ǫT(h∗T)+2sα2β+(1−α)21−βrdlog(2m)−logδ2m+2(1−α)12ˆdH∆H(US UT)+4s2dlog(2m′)+log(4δ)m′+λ.Whenα=0(thatis weignoretargetdata) theboundisidenticaltothatofTheorem1 butwithanempiricalestimateforthesourceerror.Similarlywhenα=1(thatis weuseonlytargetdata) theboundisthestandardlearningboundusingonlytargetdata.Attheoptimalα(whichminimizestherighthandside) theboundisalwaysatleastastightaseitherofthesetwosettings.Finallynotethatbychoosingdifferentvaluesofα theboundallowsustoeffectivelytradeoffthesmallamountoftargetdataagainstthelargeamountoflessrelevantsourcedata.Weremarkthatwhenitisknownthatλ=0 thedependenceonminTheorem2canbeimproved;thiscorrespondstotherestrictedorrealizablesetting.4ExperimentalResultsWeevaluateourtheorybycomparingitspredictionstoempiricalresults.WhileideallyTheorem2couldbedirectlycomparedwithtesterror thisisnotpracticalbecauseλisunknown dH∆Hiscomputationallyintractable[3] andtheVCdimensiondistoolargetobeausefulmeasureofcomplexity.Instead wedevelopasimpleapproximationofTheorem2thatwecancomputefromunlabeleddata.Formanyadaptationtasks λissmall(thereexistsaclassiﬁerwhichissimultane-ouslygoodforbothdomains) soweignoreithere.WeapproximatedH∆Hbytrainingalinearclassiﬁertodiscriminatebetweenthetwodomains.Weuseastandardhingeloss(normalizedbydividingbythenumberofinstances)andapplythequantity1−(cid:0)hingeloss(cid:1)inplaceoftheactualdH∆H.Letζ(US UT)beourapproximationtodH∆H computedfromsourceandtargetunlabeleddata.Fordomainsthatcanbeperfectlyseparatedwithmargin ζ(US UT)=1.Fordomainsthatareindistinguishable ζ(US UT)=0.FinallywereplacetheVCdimensionsamplecomplexitytermwithatighterconstantC.TheresultingapproximationtotheboundofTheorem2isf(α)=sCm(cid:18)α2β+(1−α)21−β(cid:19)+(1−α)ζ(US UT).(1)Ourexperimentalresultsareforthetaskofsentimentclassiﬁcation.Sentimentclassiﬁcationsystemshaverecentlygainedpopularitybecauseoftheirpotentialapplicabilitytoawiderangeofdocumentsinmanygenres fromcongressionalrecordstoﬁnancialnews.Becauseofthelargenumberofpotentialgenres sentimentclassiﬁcationisanidealareafordomainadaptation.WeusethedataprovidedbyBlitzeretal.[6] whichconsistsofreviewsofeighttypesofproductsfromAmazon.com:apparel books DVDs electronics kitchenappliances music video andacatchallcategory“other”.Thetaskisbinaryclassiﬁcation:givenareview predictwhetheritispositive(4or5outof5stars)ornegative(1or2stars).Wechosethe“apparel”domainasourtargetdomain andalloftheplotsontheright-handsideofFigure1areforthisdomain.Weobtainempiricalcurvesfortheerrorasafunctionofαbytrainingaclassiﬁerusingaweightedhingeloss.Supposethetargetdomainhasweightαandthereareβmtargettraininginstances.Thenwescalethelossoftargettraininginstancebyα/βandthelossofasourcetraininginstanceby(1−α)/(1−β).4(a)varydistance mS=2500 (c)ζ(US UT)=0.715 (e)ζ(US UT)=0.715 mT=1000mS=2500 varymTvarymS mT=250000.20.40.60.81Dist: 0.780Dist: 0.715Dist: 0.447Dist: 0.33600.20.40.60.81mT: 250mT: 500mT: 1000mT: 200000.20.40.60.81mS: 250mS: 500mS: 1000mS: 2500(b)varysources mS=2500 (d)source=dvd mS=2500 (f)source=dvd mT=1000varymTvarymS mT=250000.10.40.60.81books: 0.78dvd: 0.715electronics: 0.447kitchen: 0.33600.20.40.60.81mT: 250mT: 500mT: 1000mT: 20000  0.20.40.60.81  mS: 250mS: 500mS: 1000mS: 2500Figure1:Comparingtheboundwithtesterrorforsentimentclassiﬁcation.Thex-axisofeachﬁgureshowsα.They-axisshowsthevalueoftheboundortestseterror.(a) (c) and(e)depictthebound (b) (d) and(f)thetesterror.Eachcurvein(a)and(b)representsadifferentdistance.Curvesin(c)and(d)representdifferentnumbersoftargetinstances.Curvesin(e)and(f)representdifferentnumbersofsourceinstances.Figure1showsaseriesofplotsofequation1(onthetop)coupledwithcorrespondingplotsoftesterror(onthebottom)asafunctionofαfordifferentamountsofsourceandtargetdataanddifferentdistancesbetweendomains.Ineachpairofplots asingleparameter(distance numberoftargetinstancesmT ornumberofsourceinstancesmS)isvariedwhiletheothertwoareheldconstant.Notethatβ=mT/(mT+mS).TheplotsonthetoppartofFigure1arenotmeanttobenumericalproxiesforthetrueerror(Forthesourcedomains“books”and“dvd” thedistancealoneiswellabove12).Instead theyarescaledtoillustratethattheboundissimilarinshapetothetrueerrorcurveandthatrelativerelationshipsarepreserved.BychoosingadifferentCinequation1foreachcurve onecanachievecompletecontrolovertheirminima.Inordertoavoidthis weonlyuseasinglevalueofC=1600forall12curvesonthetoppartofFigure1.Firstnotethatineverypairofplots theempiricalerrorcurveshavearoughlyconvexshapethatmimicstheshapeofthebounds.Furthermorethevalueofαwhichminimizestheboundalsohasalowempiricalerrorforeachcorrespondingcurve.ThissuggeststhatchoosingαtominimizetheboundofTheorem2andsubsequentlytrainingaclassiﬁertominimizetheempiricalerrorˆǫα(h)canworkwellinpractice providedwehaveareasonablemeasureofcomplexity.4Figures1aand1bshowthatmoredistantsourcedomainsresultinhighertargeterror.Figures1cand1dillustratethatformoretargetdata wehavenotonlylowererroringeneral butalsoahigherminimizingα.Finally ﬁgures1eand1fdepictthelimitationofdistantsourcedata.Withenoughtargetdata nomatterhowmuchsourcedataweinclude wealwaysprefertouseonlythetargetdata.Thisisreﬂectedinourboundasaphasetransitioninthevalueoftheoptimalα(governingthetradeoffbetweensourceandtargetdata).ThephasetransitionoccurswhenmT=C/ζ(US UT)2(SeeFigure2).4AlthoughTheorem2doesnotholduniformlyforallαasstated thisiseasilyremediedviaanapplicationoftheunionbound.Theresultingboundwillcontainanadditionallogarithmicfactorinthecomplexityterm.5SourceTarget ×102  5 00050 000722 00011 million167 million323028262400.51Figure2:Anexampleofthephasetransitionintheoptimalα.Thevalueofαwhichminimizestheboundisindicatedbytheintensity whereblackmeansα=1(correspondingtoignoringsourceandlearningonlyfromtargetdata).WeﬁxC=1600andζ(US UT)=0.715 asinoursentimentresults.Thex-axisshowsthenumberofsourceinstances(log-scale).They-axisshowsthenumberoftargetinstances.Aphasetransitionoccursat3 130targetinstances.Withmoretargetinstancesthanthis itismoreeffectivetoignoreevenaninﬁniteamountofsourcedata.5LearningfromMultipleSourcesWenowexploreanextensionofourtheorytothecaseofmultiplesourcedomains.Wearepre-sentedwithdatafromNdistinctsources.EachsourceSjisassociatedwithanunknownunderlyingdistributionDjoverinputpointsandanunknownlabelingfunctionfj.FromeachsourceSj wearegivenmjlabeledtraininginstances andourgoalistousetheseinstancestotrainamodeltoperformwellonatargetdomainhDT fTi whichmayormaynotbeoneofthesources.Thissettingismotivatedbyseveralnewdomainadaptationalgorithms[10 5 11 9]thatweighthelossfromtraininginstancesdependingonhow“far”theyarefromthetargetdomain.Thatis eachtraininginstanceisitsownsourcedomain.Asintheprevioussections wewillexaminealgorithmsthatminimizeconvexcombinationsoftrainingerrorsoverthelabeledexamplesfromeachsourcedomain.Asbefore weletmj=βjmwithPNj=1βj=1.Givenavectorα=(α1 ··· αN)ofdomainweightswithPjαj=1 wedeﬁnetheempiricalα-weightederroroffunctionhasˆǫα(h)=NXj=1αjˆǫj(h)=NXj=1αjmjXx∈Sj|h(x)−fj(x)|.Thetrueα-weightederrorǫα(h)isdeﬁnedanalogously.LetDαbeamixtureoftheNsourcedistributionswithmixingweightsequaltothecomponentsofα.Finally analogoustoλinthesingle-sourcesetting wedeﬁnetheerrorofthemulti-sourceidealhypothesisforaweightingαasγα=minh{ǫT(h)+ǫα(h)}=minh{ǫT(h)+NXj=1αjǫj(h)}.Thefollowingtheoremgivesalearningboundforempiricalriskminimizationusingtheempiricalα-weightederror.Theorem3SupposewearegivenmjlabeledinstancesfromsourceSjforj=1...N.Foraﬁxedvectorofweightsα letˆh=argminh∈Hˆǫα(h) andleth∗T=argminh∈HǫT(h).Thenforanyδ∈(0 1) withprobabilityatleast1−δ(overthechoiceofsamplesfromeachsource) ǫT(ˆh)≤ǫT(h∗T)+2vuutNXj=1α2jβjrdlog2m−logδ2m+2(cid:18)γα+12dH∆H(Dα DT)(cid:19).6(a)Source.Moregirlsthanboys(b)Target.Separatorfrom(c)WeightingsourcestomatchuniformmixtureissuboptimaltargetisoptimalFemalesMaleslearnedseparatoroptimalseparatorlearnedseparatorerrorsFemalesMalesTargetoptimal &learnedseparatorFigure3:A1-dimensionalexampleillustratinghownon-uniformmixtureweightingcanresultinoptimalerror.Weobserveonefeature whichweusetopredictgender.(a)Attraintimeweobservemorefemalesthanmales.(b)Learningbyuniformlyweightingthetrainingdatacausesustolearnasuboptimaldecisionboundary (c)butbyweightingthemalesmorehighly wecanmatchthetargetdataandlearnanoptimalclassiﬁer.ThefullproofisinappendixC.LiketheproofofTheorem2 itissplitintotwoparts.Theﬁrstpartboundsthedifferencebetweentheα-weightederrorandthetargeterrorsimilartolemma1.Thesecondisauniformconvergenceboundforˆǫα(h)similartolemma2.Theorem3reducestoTheorem2whenwehaveonlytwosources oneofwhichisthetargetdomain(thatis wehavesomesmallnumberoftargetinstances).Itismoregeneral though becausebymanipulatingαwecaneffectivelychangethesourcedomain.Thishastwoconsequences.First wedemandthatthereexistsahypothesish∗whichhaslowerroronboththeα-weightedconvexcombinationofsourcesandthetargetdomain.Second wemeasuredistancebetweenthetargetandamixtureofsources ratherthanbetweenthetargetandasinglesource.Onequestionwemightaskiswhetherthereexistsettingswhereanon-uniformweightingcanleadtoasigniﬁcantlylowervalueoftheboundthanauniformweighting.Thiscanhappenifsomenon-uniformweightingofsourcesaccuratelyapproximatesthetargetdomain.Asahypotheticalexample supposewearetryingtopredictgenderfromheight(Figure3).Eachinstanceisdrawnfromagender-speciﬁcGaussian.Inthisexample wecanﬁndtheoptimalclassiﬁerbyweightingthe“males”and“females”componentsofthesourcetomatchthetarget.6RelatedWorkDomainadaptationisawidely-studiedarea andwecannothopetocovereveryaspectandap-plicationofithere5.Instead inthissectionwefocusonothertheoreticalapproachestodomainadaptation.Whilewedonotexplicitlyaddresstherelationshipinthispaper wenotethatdomainadaptationiscloselyrelatedtothesettingofcovariateshift whichhasbeenstudiedinstatistics.InadditiontotheworkofHuangetal.[10] severalotherauthorshaveconsideredlearningbyassigningseparateweightstothecomponentsofthelossfunctioncorrespondingtoseparateinstances.Bickelatal.[5]andJiangandZhai[11]suggestpromisingempiricalalgorithmsthatinpartinspireourTheorem3.Wehopethatourworkcanhelptoexplainwhenthesealgorithmsareeffective.Daietal.[9]consideredweightinginstancesusingatransfer-awarevariantofboosting butthelearningboundstheygivearenostrongerthanboundswhichcompletelyignorethesourcedata.Crammeretal.[8]considerlearningwhenthemarginaldistributiononinstancesisthesameacrosssourcesbutthelabelingfunctionmaychange.ThiscorrespondsinourtheorytocaseswheredH∆H=0butλislarge.Likeustheyconsidermultiplesources buttheirnotionofweightingislessgeneral.Theyconsideronlyincludingordiscardingasourceentirely.LiandBilmes[13]givePAC-Bayesianlearningboundsforadaptationusing“divergencepriors”.Theyplacesource-centeredpriorontheparametersofamodellearnedinthetargetdomain.Like5TheNIPS2006WorkshoponLearningWhenTestandTrainingInputshaveDifferentDistributions(http://ida.first.fraunhofer.de/projects/different06/)containsagoodsetofrefer-encesondomainadaptationandrelatedtopics.7ourmodel thedivergenceprioralsoemphasizesthetradeoffbetweensourceandtarget.Inourmodel though wemeasurethedivergence(andconsequentlythebias)ofthesourcedomainfromunlabeleddata.Thisallowsustochoosethebesttradeoffbetweensourceandtargetlabeleddata.7ConclusionInthisworkweinvestigatethetaskofdomainadaptationwhenwehavealargeamountoftrain-ingdatafromasourcedomainbutwishtoapplyamodelinatargetdomainwithamuchsmalleramountoftrainingdata.Ourmainresultisauniformconvergencelearningboundforalgorithmswhichminimizeconvexcombinationsofsourceandtargetempiricalrisk.Ourboundreﬂectsthetrade-offbetweenthesizeofthesourcedataandtheaccuracyofthetargetdata andwegiveasimpleapproximationtoitthatiscomputablefromﬁnitelabeledandunlabeledsamples.Thisap-proximationmakescorrectpredictionsaboutmodeltesterrorforasentimentclassiﬁcationtask.Ourtheoryalsoextendsinastraightforwardmannertoamulti-sourcesetting whichwebelievehelpstoexplainthesuccessofrecentempiricalworkindomainadaptation.Ourfutureworkhastworelateddirections.First wewishtotightenourbounds bothbyconsideringmoresophisticatedmeasuresofcomplexity[15 2]andbyfocusingourdistancemeasureonthemostrelevantfeatures ratherthanallthefeatures.WealsoplantoinvestigatealgorithmsthatchooseaconvexcombinationofmultiplesourcestominimizetheboundinTheorem3.8AcknowledgementsThismaterialisbaseduponworkpartiallysupportedbytheDefenseAdvancedResearchProjectsAgency(DARPA)underContractNo.NBCHD030010.Anyopinions ﬁndings andconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheDARPAorDepartmentofInterior-NationalBusinessCenter(DOI-NBC).References[1]M.AnthonyandP.Bartlett.NeuralNetworkLearning:TheoreticalFoundations.CambridgeUniversityPress Cambridge 1999.[2]P.BarlettandS.Mendelson.Rademacherandgaussiancomplexities:Riskboundsandstructuralresults.JMLR 3:463–482 2002.[3]S.Ben-David J.Blitzer K.Crammer andF.Pereira.Analysisofrepresentationsfordomainadaptation.InNIPS 2007.[4]S.Ben-David J.Gehrke andD.Kifer.Detectingchangeindatastreams.InVLDB 2004.[5]S.Bickel M.Br¨uckner andT.Scheffer.Discriminativelearningfordifferingtrainingandtestdistribu-tions.InICML 2007.[6]J.Blitzer M.Dredze andF.Pereira.Biographies bollywood boomboxesandblenders:Domainadapta-tionforsentimentclassiﬁcation.InACL 2007.[7]C.ChelbaandA.Acero.Empiricalmethodsinnaturallanguageprocessing.InEMNLP 2004.[8]K.Crammer M.Kearns andJ.Wortman.Learningfrommultiplesources.InNIPS 2007.[9]W.Dai Q.Yang G.Xue andY.Yu.Boostingfortransferlearning.InICML 2007.[10]J.Huang A.Smola A.Gretton K.Borgwardt andB.Schoelkopf.Correctingsampleselectionbiasbyunlabeleddata.InNIPS 2007.[11]J.JiangandC.Zhai.Instanceweightingfordomainadaptation.InACL 2007.[12]C.LegetterandP.Woodland.Maximumlikelihoodlinearregressionforspeakeradaptationofcontinuousdensityhiddenmarkovmodels.ComputerSpeechandLanguage 9:171–185 1995.[13]X.LiandJ.Bilmes.Abayesiandivergencepriorforclassiﬁcationadaptation.InAISTATS 2007.[14]A.Martinez.Recognitionofpartiallyoccludedand/orimpreciselylocalizedfacesusingaprobabilisticapproach.InCVPR 2007.[15]D.McAllester.SimpliﬁedPAC-Bayesianmarginbounds.InCOLT 2003.[16]V.Vapnik.StatisticalLearningTheory.JohnWiley NewYork 1998.[17]P.WuandT.Dietterich.Improvingsvmaccuracybytrainingonauxiliarydatasources.InICML 2004.8,Sven Eberhardt
Jonah Cader
Thomas Serre