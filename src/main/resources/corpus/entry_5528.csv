2018,The committee machine: Computational to statistical gaps in learning a two-layers neural network,Heuristic tools from statistical physics have been used in the past to compute the optimal learning and generalization errors in the teacher-student scenario in multi- layer neural networks. In this contribution  we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases  and unveiling a large computational gap.,The committee machine: Computational to statistical

gaps in learning a two-layers neural network

Benjamin Aubin(cid:63)†  Antoine Maillard†  Jean Barbier⊗♦†
Florent Krzakala†  Nicolas Macris⊗  Lenka Zdeborová(cid:63)

Abstract

Heuristic tools from statistical physics have been used in the past to locate the
phase transitions and compute the optimal learning and generalization errors in
the teacher-student scenario in multi-layer neural networks. In this contribution 
we provide a rigorous justiﬁcation of these approaches for a two-layers neural
network model called the committee machine. We also introduce a version of
the approximate message passing (AMP) algorithm for the committee machine
that allows to perform optimal learning in polynomial time for a large set of
parameters. We ﬁnd that there are regimes in which a low generalization error is
information-theoretically achievable while the AMP algorithm fails to deliver it;
strongly suggesting that no efﬁcient algorithm exists for those cases  and unveiling
a large computational gap.

While the traditional approach to learning and generalization follows the Vapnik-Chervonenkis [1]
and Rademacher [2] worst-case type bounds  there has been a considerable body of theoretical work
on calculating the generalization ability of neural networks for data arising from a probabilistic model
within the framework of statistical mechanics [3  4  5  6  7]. In the wake of the need to understand
the effectiveness of neural networks and also the limitations of the classical approaches [8]  it is of
interest to revisit the results that have emerged thanks to the physics perspective. This direction is
currently experiencing a strong revival  see e.g. [9  10  11  12].
Of particular interest is the so-called teacher-student approach  where labels are generated by feeding
i.i.d. random samples to a neural network architecture (the teacher) and are then presented to another
neural network (the student) that is trained using these data. Early studies computed the information
theoretic limitations of the supervised learning abilities of the teacher weights by a student who is
given m independent n-dimensional examples with α≡ m/n = Θ(1) and n → ∞ [3  4  7]. These
works relied on non-rigorous heuristic approaches  such as the replica and cavity methods [13  14].
Additionally no provably efﬁcient algorithm was provided to achieve the predicted learning abilities 
and it was thus difﬁcult to test those predictions  or to assess the computational difﬁculty.
Recent developments in statistical estimation and information theory —in particular of approximate
message passing algorithms (AMP) [15  16  17  18]  and a rigorous proof of the replica formula for
the optimal generalization error [11]— allowed to settle these two missing points for single-layer
neural networks (i.e. without any hidden variables). In the present paper  we leverage on these works 
and provide rigorous asymptotic predictions and corresponding message passing algorithm for a class
of two-layers networks.
(cid:63) Institut de Physique Théorique  CNRS & CEA & Université Paris-Saclay  Saclay  France.
† Laboratoire de Physique Statistique  CNRS & Sorbonnes Universités & École Normale Supérieure  PSL
University  Paris  France.
⊗ Laboratoire de Théorie des Communications  Faculté Informatique et Communications  Ecole Polytechnique
Fédérale de Lausanne  Suisse.
♦ International Center for Theoretical Physics  Trieste  Italy.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

1 Summary of contributions and related works

While our results hold for a rather large class of non-linear activation functions  we illustrate our
ﬁndings on a case considered most commonly in the early literature: The committee machine. This is
possibly the simplest version of a two-layers neural network where all the weights in the second layer
are ﬁxed to unity. Denoting Yµ the label associated with a n-dimensional sample Xµ  and W ∗
il the
weight connecting the i-th coordinate of the input to the l-th node of the hidden layer  it is deﬁned by:

Yµ = sign(cid:104) K(cid:88)l=1

sign(cid:16) n(cid:88)i=1

XµiW

∗

il(cid:17)(cid:105) .

(1)

il}K

il from the available examples (Xµ  Yµ)m

µ=1 and knows the distribution P0 used to generate W ∗

We concentrate here on the teacher-student scenario: The teacher generates i.i.d. data samples with
i.i.d. standard Gaussian coordinates Xµi ∼ N (0  1)  then she/he generates the associated labels
Yµ using a committee machine as in (1)  with i.i.d. weights W ∗
il unknown to the student (in the
proof section we will consider the more general case of a distribution for the weights of the form
(cid:81)n
i=1 P0({W ∗
l=1)  but in practice we consider the fully separable case). The student is then given
the m input-output pairs (Xµ  Yµ)m
il. The
goal of the student is to learn the weights W ∗
µ=1 in order to
reach the smallest possible generalization error (i.e. to be able to predict the label the teacher would
generate for a new sample not present in the training set).
There have been several studies of this model within the non-rigorous statistical physics approach
in the limit where α ≡ m/n = Θ(1)  K = Θ(1) and n → ∞ [19  20  21  22  6  7]. A particularly
interesting result in the teacher-student setting is the specialization of hidden neurons (see sec. 12.6
of [7]  or [23] in the context of online learning): For α < αspec (where αspec is a certain critical value
of the sample complexity)  the permutational symmetry between hidden neurons remains conserved
even after an optimal learning  and the learned weights of each of the hidden neurons are identical.
For α > αspec  however  this symmetry gets broken as each of the hidden units correlates strongly
with one of the hidden units of the teacher. Another remarkable result is the calculation of the optimal
generalization error as a function of α.
Our ﬁrst contribution consists in a proof of the replica formula conjectured in the statistical physics
literature  using the adaptive interpolation method of [24  11]  that allows to put several of these
results on a ﬁrm rigorous basis. Our second contribution is the design of an AMP-type of algorithm
that is able to achieve the optimal learning error in the above limit of large dimensions for a wide range
of parameters. The study of AMP —that is widely believed to be optimal between all polynomial
algorithms in the above setting [25  26  27  28]— unveils  in the case of the committee machine with
a large number of hidden neurons  the existence a large hard phase in which learning is information-
theoretically possible  leading to a good generalization error decaying asymptotically as 1.25K/α (in
the α = Θ(K) regime)  but where AMP fails and provides only a poor generalization that does not
decay when increasing α. This strongly suggests that no efﬁcient algorithm exists in this hard region
and therefore there is a computational gap in learning in such neural networks. In other problems
where a hard phase was identiﬁed  its study boosted the development of algorithms that are able to
match the predicted thresholds and we anticipate this will translate to the present model.
We also want to comment on a related line of work that studies the loss-function landscape of neural
networks. While a range of works show under various assumptions that spurious local minima are
absent in neural networks  others show under different conditions that they do exist  see e.g. [29].
The regime of parameters that is hard for AMP must have spurious local minima  but the converse is
not true in general. It might be that there are spurious local minima  yet the AMP approach succeeds.
Moreover  in all previously studied models in the Bayes-optimal setting the (generalization) error
obtained with the AMP is the best known and other approaches  e.g. (noisy) gradient based  spectral
algorithms or semideﬁnite programming  are not better in generalizing even in cases where the
“student” models are overparametrized. Of course in order to be in the Bayes-optimal setting one
needs to know the model used by the teacher which is not the case in practice.

2 Main technical results

A general model — While in the illustration of our results we shall focus on the model (1)  all our
µ i=1  we denote W ∗
formulas are valid for a broader class of models: Given m input samples (Xµi)m n
il

2

∗

∗

l=1

(2)

√n

√n

XµiW

XµiW

n(cid:88)i=1

il(cid:111)K

il(cid:111)K
l=1(cid:17)  

the teacher-weight connecting the i-th input (i.e. visible unit) to the l-th node of the hidden layer. For
a generic function ϕout : RK × R → R one can formally write the output as
Yµ = ϕout(cid:16)(cid:110) 1
n(cid:88)i=1

  Aµ(cid:17) or Yµ ∼ Pout(cid:16) ·(cid:12)(cid:12)(cid:12)(cid:110) 1

where (Aµ)m
µ=1 are i.i.d. real valued random variables with known distribution PA  that form the
probabilistic part of the model  generally accounting for noise. For deterministic models the second
argument is simply absent (or is a Dirac mass). We can view alternatively (2) as a channel where
the transition kernel Pout is directly related to ϕout. As discussed above  we focus on the teacher-
student scenario where the teacher generates Gaussian i.i.d. data Xµi ∼ N (0  1)  and i.i.d. weights
il ∼ P0. The student then learns W ∗ from the data (Xµ  Yµ)m
W ∗
µ=1 by computing marginal means of
the posterior probability distribution (5).
Different scenarii ﬁt into this general framework. Among those  the committee machine is obtained
when choosing ϕout(h) = sign((cid:80)K
l=1 sign(hl)) while another model considered previously is given
by the parity machine  when ϕout(h) =(cid:81)K
l=1 sign(hl)  see e.g. [7]. A number of layers beyond two
has also been considered  see [22]. Other activation functions can be used  and many more problems
can be described  e.g. compressed pooling [30  31] or multi-vector compressed sensing [32].
Two auxiliary inference problems — Denote SK the ﬁnite dimensional vector space of K × K
matrices  S +
K for positive
deﬁnite K × K matrices  and ∀ N ∈ S +
Stating our results requires introducing two simpler auxiliary K-dimensional estimation problems:
• The ﬁrst one consists in retrieving a K-dimensional input vector W0 ∼ P0 from the output of a
Gaussian vector channel with K-dimensional observations Y0 = r1/2W0 + Z0  Z0 ∼ N (0  IK×K)
and the “channel gain” matrix r ∈ S +
P (w|Y0) =

K the convex and compact set of semi-deﬁnite positive K × K matrices  S ++
K s.t. N − M ∈ S +
K}.

K. The associated posterior distribution on w = (wl)K

K(N ) ≡ {M ∈ S+

K we set S+

(cid:124)
0 r1/2w− 1

P0(w)eY

l=1 is

rw  

(3)

(cid:124)

2 w

1
ZP0

u

1
ZPout

(cid:124)
0 ] is in S +

P (u|(cid:101)Y0  V ) =

and the associated free entropy (or minus free energy) is given by the expectation over Y0 of the
log-partition function ψP0(r) ≡ E lnZP0 and involves K dimensional integrals.
• The second problem considers K-dimensional i.i.d. vectors V  U∗
considered to be known and one has to retrieve U∗ from a scalar observation obtained as (cid:101)Y0 ∼
Pout(·|q1/2V + (ρ− q)1/2U∗) where the second moment matrix ρ ≡ E[W0W
and the so-called “overlap matrix” q is in S+
K(ρ). The associated posterior is
(cid:124)
e− 1
2 u
(2π)K/2 Pout(cid:0)(cid:101)Y0|q1/2V + (ρ − q)1/2u(cid:1)  

∼ N (0  IK×K) where V is
K (W0 ∼ P0)

and also involves K dimensional integrals.
The free entropy — The central object of study leading to the optimal learning and generalization
errors in the present setting is the posterior distribution of the weights:

and the free entropy reads this time ΨPout (q; ρ) ≡ E lnZPout (with the expectation over(cid:101)Y0 and V )
l=1(cid:17)  
Xµiwil(cid:111)K

P ({wil}n K
where the normalization factor is nothing else than a partition function  i.e.
the integral of the
i l=1. The expected 2 free entropy is by deﬁnition fn ≡ E lnZn/n. The replica
numerator over {wil}n K
formula gives an explicit (conjectural) expression of fn in the high-dimensional limit n  m → ∞ with
α = m/n ﬁxed. We discuss in the long version of this paper [33] how the heuristic replica method
[13  14] yields the formula. This computation was ﬁrst performed  to the best of our knowledge 
by [19] in the case of the committee machine. Our ﬁrst contribution is a rigorous proof of the
corresponding free entropy formula using an interpolation method [34  35  24].
2 The symbol E will generally denote an expectation over all random variables in the ensuing expression (here
{Xµi  Yµ}). Subscripts will be used only when we take partial expectations or if there is an ambiguity.

Pout(cid:16)Yµ(cid:12)(cid:12)(cid:12)(cid:110) 1

i l=1|{Xµi  Yµ}m n

P0({wil}K

m(cid:89)µ=1

n(cid:88)i=1

n(cid:89)i=1

µ i=1) =

1
Zn

√n

l=1)

(5)

(4)

3

In order to formulate our rigorous results  we add an (arbitrarily small) Gaussian regularization noise
Zµ√∆ to the ﬁrst expression of the model (2)  where ∆ > 0  Zµ ∼ N (0  1)  so that the channel
kernel is (u ∈ RK)

dPA(a)e

− 1

2∆ (y−ϕ(u a))2

.

(6)

Pout(y|u) =

1

√2π∆(cid:90)R

Theorem 2.1 (Replica formula) Suppose (H1): The prior P0 has bounded support in RK; (H2):
The activation ϕout : RK × R → R is a bounded C2 function with bounded ﬁrst and second
derivatives w.r.t. its ﬁrst argument (in RK-space); and (H3): For all µ = 1  . . .   m and i = 1  . . .   n
we have i.i.d. Xµi ∼ N (0  1). Then for the model (2) with kernel (6) the limit of the free entropy is:
(7)

K (ρ)(cid:110)ψP0 (r) + αΨPout (q; ρ) −

1
2

Tr(rq)(cid:111)  

1
n

E lnZn = sup
r∈S+

K

n→∞ fn ≡ lim
lim
n→∞

inf
q∈S+

where α ≡ m/n and where ΨPout(q; ρ) and ψP0(r) are the free entropies of two simpler K-
dimensional estimation problems (3) and (4).

This theorem extends the recent progress for generalized linear models of [11]  which includes the
case K = 1 of the present contribution  to the phenomenologically richer case of two-layers problems
such as the committee machine. The proof sketch based on an adaptive interpolation method recently
developed in [24] is outlined and the details can be found in [33]. Note that  following similar
approximation arguments as in [11]  the hypothesis (H1) can be relaxed to the existence of the
second moment of the prior; thus covering the Gaussian case  (H2) can be dropped (and thus include
model (1) and its sign(·) activation) and (H3) extended to data matrices X with i.i.d. entries of zero
mean  unit variance and ﬁnite third moment. Moreover  the case ∆ = 0 can be considered when
the outputs are discrete  as in the committee machine (1)  see [11]. The channel kernel becomes
Pout(y|u) =(cid:82) dPA(a)1(y− ϕ(u  a)) and the replica formula is the limit ∆ → 0 of the one provided

in Theorem 2.1. In general this regularizing noise is needed for the free entropy limit to exist.
Learning the teacher weights and optimal generalization error — A classical result in Bayesian
estimation is that the estimator ˆW that minimizes the mean-square error with the ground-truth W ∗ is
given by the expected mean of the posterior distribution. Denoting q∗ the extremizer in the replica
formula (7)  we expect from the replica method that in the limit n → ∞  m/n = α  that with high
W ∗/n → q∗. We refer to proposition 4.2 and to the proof in [33] for the precise
probablity ˆW
statement  that remains rigorously valid only in the presence of an additional (possibly inﬁnitesimal)
side-information. From the overlap matrix q∗  one can compute the Bayes-optimal generalization
error when the student tries to classify a new  yet unseen  sample Xnew. The estimator of the new label
ˆYnew that minimizes the mean-square error with the true label is given by computing the posterior
mean of ϕout(Xneww) (Xnew is a row vector). Given the new sample  the optimal generalization
error is then

(cid:124)

∗

)

(8)

1
2

E(cid:104)(cid:0)Ew|X Y(cid:2)ϕout(Xneww)(cid:3) − ϕout(XnewW

∗

)(cid:1)2(cid:105) −−−−→n→∞ g(q

(cid:124)

where w is distributed according to the posterior measure (5) (note that this Bayes-optimal compu-
tation differs from the so-called Gibbs estimator by a factor 2  see [33]). In particular  when the
data X is drawn from the standard Gaussian distribution on Rm×n  and is thus rotationally invariant 
W ∗  which converges to q∗. Then a direct algebraic
it follows that this error only depends on w
computation gives a lengthy but explicit formula for g(q∗)  as shown in [33].
Approximate message passing  and its state evolution — Our next result is based on an adaptation
of a popular algorithm to solve random instances of generalized linear models  the AMP algorithm
[15  16]  for the case of the committee machine and models described by (2).
The AMP algorithm can be obtained as a Taylor expansion of loopy belief-propagation (see [33]) and
also originates in earlier statistical physics works [36  37  38  39  40  26]. It is conjectured to perform
the best among all polynomial algorithms in the framework of these models. It thus gives us a tool
to evaluate both the intrinsic algorithmic hardness of the learning and the performance of existing
algorithms with respect to the optimal one in this model.
The AMP algorithm is summarized by its pseudo-code in Algorithm 1  where the update functions
gout  ∂ωgout  fW and fC are related  again  to the two auxiliary problems (3) and (4). The functions

4

ωt

µ =

Algorithm 1 Approximate Message Passing for the committee machine
Input: vector Y ∈ Rm and matrix X ∈ Rm×n:
Initialize: ˆWi  gout µ ∈ RK and ˆCi  ∂ωgout µ ∈ S +
K for 1 ≤ i ≤ n and 1 ≤ µ ≤ m at t = 0.
repeat
Update of the mean ωµ ∈ RK and covariance Vµ ∈ S +
K:
(cid:1)−1 ˆC t
out µ(cid:1)
gt−1
i Σt−1
Update of gout µ ∈ RK and ∂ωgout µ ∈ S +
K:
∂ωgt
out µ = ∂ωgout(ωt
Update of the mean Ti ∈ RK and covariance Σi ∈ S +
K:
| Σt
Update of the estimated marginals ˆWi ∈ RK and ˆCi ∈ S +
K:

i = −(cid:16) m(cid:80)µ=1

n (cid:0)Σt−1

out µ(cid:17)−1

n(cid:80)i=1(cid:0) Xµi√

i(cid:16) m(cid:80)µ=1

gt
out µ = gout(ωt

X 2
n ∂ωgt
µi

X 2
n ∂ωgt
µi

µ  Yµ  V t
µ)

µ  Yµ  V t
µ)

out µ −

n(cid:80)i=1

ˆW t

i −

T t
i = Σt

i(cid:17)

Xµi√

n gt

X 2
µi
n

ˆC t
i

ˆW t

out µ

V t
µ =

X 2
µi

i

|

|

n

i

ˆW t+1
t = t + 1

i = fW (Σt

i  T t
i )

ˆC t+1

i = fC(Σt

i  T t
i )

|

until Convergence on ˆW   ˆC.
Output: ˆW and ˆC.

fW (Σ  T ) and fC(Σ  T ) are respectively the mean and variance under the posterior distribution (3)
when r → Σ−1 and Y0 → Σ1/2T   while gout(ωµ  Yµ  Vµ) is given by the product of V
and the
mean of u under the posterior (4) using(cid:101)Y0 → Yµ  ρ − q → Vµ and q1/2V → ωµ (see [33] for more
details). After convergence  ˆW estimates the weights of the teacher-neural network. The label of a
sample Xnew not seen in the training set is estimated by the AMP algorithm as

−1/2
µ

Y t

new =(cid:90) dy(cid:0) K(cid:89)l=1

dzl(cid:1) yPout(y|{zl}K

l=1)N (z; ωt

new  V t

new)  

(9)

i=1 Xnew i ˆW t

new = (cid:80)n

AMP is the K × K covariance matrix (see below for the deﬁnition of qt

i is the mean of the normally distributed variable z ∈ RK  and
AMP). We

where ωt
new = ρ − qt
V t
provide a demo of the algorithm on github [41].
AMP is particularly interesting because its performance can be tracked rigorously  again in the
asymptotic limit when n → ∞  via a procedure known as state evolution (a rigorous version of the
cavity method in physics [14])  see [18]. State evolution tracks the value of the overlap between the
W ∗/n  via:
hidden ground truth W ∗ and the AMP estimate ˆWt  deﬁned as qt

(cid:124)

qt+1
AMP = 2

∂ψP0
∂r

(rt

AMP)  

rt+1
AMP = 2α

∂q

AMP ≡ limn→∞( ˆW t)
∂ΨPout

(qt

AMP; ρ) .

(10)

The ﬁxed points of these equations correspond to the critical points of the replica free entropy (7).
Let us comment further on the convergence of the algorithm. In the large n limit  and if the integrals
are performed without errors  then the algorithm is guaranteed to converge. This is a consequence of
the state evolution combined with the Bayes-optimal setting. In practice  of course  n is ﬁnite and
integrals are approximated. In that case convergence is not guaranteed  but is robustly achieved in all
the cases presented in this paper. We also expect (by experience with the single layer case) that if the
input-data matrix is not random (i.e. beyond our assumptions) then we will encounter convergence
issues  which could be ﬁxed by moving to some variant of the algorithm such as VAMP [42].

3 From two to more hidden neurons  and the specialization phase transition

Two neurons — Let us now discuss how the above results can be used to study the optimal learning
in the simplest non-trivial case of a two-layers neural network with two hidden neurons  i.e. when
model (1) is simply Yµ = sign[ sign((cid:80)n
i=1 XµiW ∗
i2)]  with the convention
that sign(0) = 0. We remind that the input-data matrix X has i.i.d. N (0  1) entries  and the
teacher-weights W ∗ used to generate the labels Y are taken i.i.d. from P0.

i1) + sign((cid:80)n

i=1 XµiW ∗

5

In Fig. 1 we plot the optimal generalization error as a function of the sample complexity α = m/n.
In the left panel the weights are Gaussian (for both the teacher and the student)  while in the center
panel they are binary/Rademacher. The full line is obtained from the ﬁxed point of the state evolution
(SE) of the AMP algorithm (10)  corresponding to the extremizer of the replica free entropy (7).
The points are results of the AMP algorithm run till convergence averaged over 10 instances of size
n = 104. In this case and with random initial conditions the AMP algorithm did converge in all our
trials. As expected we observe excellent agreement between the SE and AMP.
In both left and center panels of Fig. 1 we observe the so-called specialization phase transition. Indeed
(10) has two types of ﬁxed points: A non-specialized ﬁxed point where every element of the K × K
order parameter q is the same (so that both hidden neurons learn the same function) and a specialized
ﬁxed point where the diagonal elements of the order parameter are different from the non-diagonal
ones. We checked for other types of ﬁxed points for K = 2 (one where the two diagonal elements are
not the same)  but have not found any. In terms of weight-learning  this means for the non-specialized
ﬁxed point that the estimators for both W1 and W2 are the same  whereas in the specialized ﬁxed
point the estimators of the weights corresponding to the two hidden neurons are different  and that
the network “ﬁgured out” that the data are better described by a non-linearly separable model. The
specialized ﬁxed point is associated with lower error than the non-specialized one (as one can see in
Fig. 1). The existence of this phase transition was discussed in statistical physics literature on the
committee machine  see e.g. [20  23].
For Gaussian weights (Fig. 1 left)  the specialization phase transition arises continuously at
spec(K = 2) the number of samples is too
spec(K = 2) (cid:39) 2.04. This means that for α < αG
αG
small  and the student-neural network is not able to learn that two different teacher-vectors W1 and
W2 were used to generate the observed labels. For α > αG
spec(K = 2)  however  it is able to distin-
guish the two different weight-vectors and the generalization error decreases fast to low values (see
Fig. 1). For completeness we remind that in the case of K = 1 corresponding to single-layer neural
network no such specialization transition exists. We show [33] that it is absent also in multi-layer
neural networks as long as the activations remain linear. The non-linearity of the activation function
is therefore an essential ingredient in order to observe a specialization phase transition.
The center part of Fig. 1 depicts the ﬁxed point reached by the state evolution of AMP for the case of
binary weights. We observe two phase transitions in the performance of AMP in this case: (a) the
specialization phase transition at αB
spec(K = 2) (cid:39) 1.58  and for slightly larger sample complexity a
transition towards perfect generalization (beyond which the generalization error is asymptotically
zero) at αB
perf (K = 2) (cid:39) 1.99. The binary case with K = 2 differs from the Gaussian one in the fact
that perfect generalization is achievable at ﬁnite α. While the specialization transition is continuous
here  the error has a discontinuity at the transition of perfect generalization. This discontinuity is
associated with the 1st order phase transition (in the physics nomenclature)  leading to a gap between
algorithmic (AMP in our case) performance and information-theoretically optimal performance
reachable by exponential algorithms. To quantify the optimal performance we need to evaluate the
global extremum of the replica free entropy (not the local one reached by the state evolution). In
doing so that we get that information theoretically there is a single discontinuous phase transition
towards perfect generalization at αB
While the information-theoretic and specialization phase transitions were identiﬁed in the physics
literature on the committee machine [20  21  3  4]  the gap between the information-theoretic per-
formance and the performance of AMP —that is conjectured to be optimal among polynomial
algorithms— was not yet discussed in the context of this model. Indeed  even its understanding in
simpler models than those discussed here  such as the single layer case  is more recent [15  26  25].
More is different — It becomes more difﬁcult to study the replica formula for larger values of K
as it involves (at least) K-dimensional integrals. Quite interestingly  it is possible to work out the
solution of the replica formula in the large K limit (thus taken after the large n limit  so that K/n
vanishes). It is indeed natural to look for solutions of the replica formula  as suggested in [19]  of
the form q = qdIK×K + (qa/K)1K1(cid:124)
l=1. Since both q and ρ are
assumed to be positive  this scaling implies [33] that 0 ≤ qd ≤ 1 and 0 ≤ qa + qd ≤ 1  as it should.
We also detail in [33] the corresponding large K expansion of the free entropy for the teacher-student
scenario with Gaussian weights. Only the information-theoretically reachable generalization error
was computed [19]  thus we concentrated on the analysis of performance of AMP by tracking the
state evolution equations. In doing so  we unveil a large computational gap.

K  with the unit vector 1K = (1)K

IT(K = 2) (cid:39) 1.54.

6

Figure 1: Order parameter and optimal generalization error for a committee machine with two
hidden neurons with Gaussian weights (left)  binary/Rademacher weights (center)  and for Gaussian
weights in the large number of hidden units limit (right). These are shown as a function of the ratio
α = m/n between the number of samples m and the dimensionality n. Lines are obtained from
the state evolution equations (dominating solution is shown in full line)  data-points from the AMP
algorithm averaged over 10 instances of the problem of size n = 104. q00 and q01 denote diagonal
and off-diagonal overlaps  and their values are given by the labels on the far-right of the ﬁgure.
In the right plot of Fig. 1 we show the ﬁxed point values of the two overlaps q00 = qd + qa/K
and q01 = qa/K and the resulting generalization error. As discussed in [19] it can be written in
a closed form as g = arccos [2 (qa + arcsin qd) /π] /π. The specialization transition arises for
spinodal (cid:39) 7.17 but the free entropy global
spec (cid:39) 7.65. This has interesting
implications for the optimal generalization error that gets towards a plateau of value εplateau (cid:39) 0.28
AMP is conjectured to be optimal among all polynomial algorithms (in the considered limit) and thus
analyzing its state evolution sheds light on possible computational-to-statistical gaps that come hand
in hand with 1st order phase transitions. In the regime of α = Θ(K) for large K the non-specialized
ﬁxed point is always stable implying that AMP will not be able to give a lower generalization error
than εplateau. Analyzing the replica formula for large K in more details [33]  we concluded that
AMP will not reach the optimal generalization for any α < Θ(K 2). This implies a rather sizable
gap between the performance that can be reached information-theoretically and the one reachable
tractably. Such large computational gaps have been previously identiﬁed in a range of inference
problems —most famously in the planted clique problem [27]— but the committee machine is the
ﬁrst model of a multi-layer neural network with realistic non-linearities (the parity machine is another
example but use a very peculiar non-linearity) that presents such large gap.

α = Θ(K) so we deﬁne(cid:101)α ≡ α/K. The specialization is now a 1st order phase transition  meaning
that the specialization ﬁxed point ﬁrst appears at (cid:101)αG
extremizer remains the one of the non-specialized ﬁxed point until(cid:101)αG
spec and then jumps discontinuously down to reach a decay aymptotically as 1.25/(cid:101)α.
for(cid:101)α <(cid:101)αG

4 Sketch of proof of Theorem 2.1

∗
i = (W ∗

il)K

l=1. For µ = 1  . . . m  let V µ  U

l=1  wi =
We denote K-dimensional column vectors by underlined letters. In particular W
∗
µ be K-dimensional vectors with i.i.d. N (0  1) components.
(wil)K
Let sn ∈ (0  1/2] a sequence that goes to 0 as n increases  and let M be the compact subset of
K with eigenvalues in the interval [1  2]. For all M ∈ snM  2snIK×K − M ∈ S +
matrices in S++
K.
Let  = (1  2) ∈ (snM)2. Let then q : [0  1] → S +
K(ρ) and r : [0  1] → S +
K be two “interpolation
functions” (that will later on depend on )  and R1(t) ≡ 1 +(cid:82) t
0 q(v)dv.
i +(cid:112)R2(t) V µ +
For t ∈ [0  1]  deﬁne the K-dimensional vector: St µ ≡(cid:112)(1 − t)/n(cid:80)n
(cid:112)tρ − R2(t) + 2snIK×K U
t i =(cid:112)R1(t) W

∗
(cid:48)
(11)
i 
i + Z
(cid:48)
(cid:48)
where Z
i is (for each i) a K-vector with i.i.d. N (0  1) components  and Y
t i is a K-vector as well.
Recall that in our notation the ∗-variables have to be retrieved  while the other random variables are

∗
µ where matrix square-roots are well deﬁned. We interpolate with

auxiliary problems related to those discussed in sec. 2:

0 r(v)dv  R2(t) ≡ 2 +(cid:82) t

i=1 XµiW

∗

1 ≤ i ≤ n 

Yt µ ∼ Pout( · | St µ) 

1 ≤ µ ≤ m 

(cid:48)

Y

7

024α0.000.050.100.150.200.25Generalizationerrorǫg(α)012α0.000.050.100.150.200.250510eα≡α/K0.00.10.20.30.40.50.00.20.40.60.81.0OverlapqAMPq00AMPq01SEq00SEq01SEǫg(α)AMPǫg(α)SpecializationSpinodal1stordertransitionTr

+

1
2

n t 

i=1 W ∗

(cid:107)uµ(cid:107)2

2

e

the

equals

dt

2 + On(1)  

(cid:104)(cid:16) 1

n

m(cid:88)

µ=1

2(cid:81)µ

0 r(v)dv) − K

2 + On(1) .

0 q(t)dt; ρ) − 1

2Tr(ρ(cid:82) 1

Tr [r(t)(q(t) − ρ)]+On(1)  

∇uYt µ (st µ)∇uYt µ (St µ)

(cid:124) − r(t)

(cid:17)(cid:0)Q − q(t)(cid:1)(cid:105)(cid:69)

∗
i and uµ replacing U
2(cid:107)Y (cid:48)

− 1

0 r(t)dt) + αΨPout((cid:82) 1

t   X  V ) = 1Zn(t )(cid:81)i P0(wi)e

assumed to be known (except for the noise variables obviously). Deﬁne now st µ by the expression
∗
µ. We introduce the interpolating posterior:
of St µ but with wi replacing W
t i−√R1(t)wi(cid:107)2
− 1
Pt (w  u|Yt  Y (cid:48)
2
(2π)K/2 Pout(Yt µ|st µ)
where the normalization factor Zn(t  ) equals the numerator integrated over all components of w
and u. The average free entropy at time t is by deﬁnition fn (t) ≡ E lnZn(t  )/n. One veriﬁes 
using in particular continuity and boundedness properties of ψP0 and ΨPout(see [33] for details):
(cid:26) fn (0) = fn − K
fn (1) = ψP0((cid:82) 1
Here On(1) → 0 in the n  m→∞ limit uniformly in t  q  r  . The next step is to compute the free
entropy variation along the interpolation path [33]:
Proposition 4.1 (Free entropy variation) Denote by (cid:104)−(cid:105)n t  the (Gibbs) expectation w.r.t.
E(cid:68)
posterior Pt . Set uy(x) ≡ ln Pout(y|x). For all t ∈ [0  1] the derivative dfn (t)
− 1
2
where ∇ is the K-dimensional gradient w.r.t. the argument of uYt µ(·)  and On(1) → 0 in the
n  m→∞ limit uniformly in t  q  r  . Here  Qll(cid:48) ≡(cid:80)n
ilwil(cid:48)/n is a K×K overlap matrix .
A crucial step of the adaptive interpolation method is to show that the overlap concentrates (see [33]):
Proposition 4.2 (Overlap concentration) Assume that for any t ∈ (0  1) the transformation  ∈
(snM)2 (cid:55)→ (R1(t  )  R2(t  )) is a C1 diffeomorphism with a Jacobian greater or equal to 1. Then
one can ﬁnd a sequence sn going to 0 slowly enough such that there exists a constant C > 0 and a
γ > 0  that only depend on the support and moments of P0 and on the activation ϕout and α  such that
F(cid:11)n t  ≤ Cn−γ.
((cid:107) − (cid:107)F is the Frobenius norm): Vol(snM)−2(cid:82)(snM)2 d(cid:82) 1
Let fRS(q  r) ≡ ψP0 (r) + αΨPout (q; ρ) − Tr(rq)/2 the replica symmetric (RS) potential. We have:
Proposition 4.3 (Lower bound) lim inf n→∞ fn ≥ supr∈S+
Proof: Choose ﬁrst r(t) = r ∈ S +
K a ﬁxed matrix. Then R(t) = (R1(t)  R2(t)) can be ﬁxed
as the solution to the ﬁrst order differential equation: ∂tR1(t  ) = r  ∂tR2(t  ) = E(cid:104)Q(cid:105)n t 
and R(0  ) = . We denote it R(t  ) = (rt + 1 (cid:82) t
It is possible to
check (see [33]) that this ODE satisﬁes the hypotheses of the parametric Cauchy-Lipschitz
theorem  and that by the Liouville formula the determinant of the Jacobian of  (cid:55)→ R(  t)
satisﬁes Jn (t) = exp{(cid:82) t
0(cid:80)K
l≥l(cid:48) ∂(R2)ll(cid:48) E(cid:104)Qll(cid:48)(cid:105)n s (s  R(s  ))ds} ≥ 1; indeed  this sum of
partials is always positive  see [33]. Using then Prop. 4.1 and Prop. 4.2  we obtain fn =
Vol(snM)−2(cid:82)(snM)2 dfRS((cid:82) 1
0 q(v  ; r)dv  r) + On(1). This implies the lower bound.
Proposition 4.4 (Upper bound) lim supn→∞ fn ≤ supr∈S+
Proof : We now ﬁx R(t) = (R1(t)  R2(t)) as the solution R(t  ) = ((cid:82) t
0 r(v  )dv +
1 (cid:82) t
0 q(v  )dv + 2) to the following Cauchy problem: ∂tR1(t  ) = 2α∇ΨPout (E(cid:104)Q(cid:105)n t ) 
∂tR2(t  ) = E(cid:104)Q(cid:105)n t   and R(0  ) = . We denote this equation as ∂tR(t  ) = Fn (R(t  )  t). It is
then possible to verify that Fn(R(t  )  t) is a bounded C1 function of R(t  )  and thus a direct appli-
cation of the Cauchy-Lipschitz theorem implies that R(t  ) is a C1 function of t and  and by unicity
of the solution  the function  (cid:55)→ R(t  ) is injective for any t. Since E(cid:104)Q(cid:105)n t  and ρ − E(cid:104)Q(cid:105)n t  are
positive matrices (see [33]) we also have that q(t  ) ∈ S +
K(ρ) and since by the differential equation
we have r(t  ) = 2α∇ΨPout (q(t  )) and as ∇ΨPout(q) ∈ S +
K. More-
over the Liouville formula for the Jacobian of the map  ∈ (snM)2 (cid:55)→ R(t  ) ∈ R(t  (snM)2)

0 dt E(cid:10)(cid:107)Q − E(cid:104)Q(cid:105)n t (cid:107)2

K (see [33])  then r(t  ) ∈ S +

inf q∈S+

K (ρ) fRS(q  r).

K

inf q∈S+

K (ρ) fRS(q  r).

K

0 q(v  ; r)dv + 2).

8

fn =

1

1

0 q(  v)dv; ρ)− 1

yields Jn (t) = exp{(cid:82) t
0(cid:80)K
l≥l(cid:48)[∂(R1)ll(cid:48) (Fn 1)ll(cid:48) +∂(R2)ll(cid:48) (Fn 2)ll(cid:48)](s  R(s  ))ds}. For all s ∈ [0  1]
the integrand(cid:80)l≥l(cid:48)[. . .] ≥ 0 (see [33]). We can again apply Prop. 4.2  and obtain
Vol(snM)2(cid:82) d{ψP0((cid:82) 1
0 r(  v)dv)+αΨPout((cid:82) 1
By convexity of ψP0 and ΨPout  fn ≤
K (ρ) fRS(q  r(  v)). Indeed  for every r ∈ S +
now remark that fRS(q(  v)  r(  v)) = inf q∈S+
function gr : q ∈ S +
is ∇gr(q) = α∇ΨPout (q) − r/2. Since ∇gr( v)(q(  v)) = 0 by deﬁnition of r(  v)  and S +
convex  the minimum of gr( v)(q) is necessarily achieved at q = q(  v). Therefore
fn ≤
which concludes the proof of Prop. 4.4. Combined with Prop. 4.3 we obtain Thm. 2.1.

0 q(  v)r(  v)dv}+On(1).
0 dvfRS(q(  v)  r(  v))+On(1). We
K  the
K(ρ) (cid:55)→ fRS(q  r) ∈ R can be shown to be convex (see [33]) and its q-derivative
K(ρ) is

Vol(snM)2(cid:82)(snM)2 d(cid:82) 1

1

Vol(snM)2(cid:82) d(cid:82) 1

inf
q∈S+
K (ρ)

fRS (q  r(  v)) + On(1) ≤ sup
r∈S+

fRS(q  r) + On(1) 

2 Tr(cid:82) 1

0 dv

inf
q∈S+

K (ρ)

K

5 Discussion

One of the contributions of this paper is the design of an AMP-type algorithm that is able to achieve
the Bayes-optimal learning error in the limit of large dimensions for a range of parameters out of
the so-called hard phase. The hard phase is associated with ﬁrst order phase transitions appearing
in the solution of the model. In the case of the committee machine with a large number of hidden
neurons we identify a large hard phase in which learning is possible information-theoretically but
not efﬁciently. In other problems where such a hard phase was identiﬁed  its study boosted the
development of algorithms that are able to match the predicted threshold. We anticipate this will also
be the same for the present model. We should  however  note that for larger K > 2 the present AMP
algorithm includes higher-dimensional integrals that hamper the speed of the algorithm. Our current
strategy to tackle this is to combine the large-K expansion and use it in the algorithm. Detailed
account of the corresponding results are left for future work.
We studied the Bayes-optimal setting where the student-network is the same as the teacher-network 
for which the replica method can be readily applied. The method still applies when the number
of hidden units in the student and teacher are different  while our proof does not generalize easily
to this case. It is an interesting subject for future work to see how the hard phase evolves under
over-parametrization and what is the interplay between the simplicity of the loss-landscape and the
achievable generalization error. We conjecture that in the present model over-parametrization will
not improve the generalization error achieved by AMP in the Bayes-optimal case.
Even though we focused in this paper on a two-layers neural network  the analysis and algorithm can
be readily extended to a multi-layer setting  see [22]  as long as the number of layers as well as the
number of hidden neurons in each layer is held constant  and as long as one learns only weights of the
ﬁrst layer  for which the proof already applies. The numerical evaluation of the phase diagram would
be more challenging than the cases presented in this paper as multiple integrals would appear in the
corresponding formulas. In future works  we also plan to analyze the case where the weights of the
second and subsequent layers (including the biases of the activation functions) are also learned. This
could be done for instance with a combination of EM and AMP along the lines of [43  44] where this
is done for the simpler single layer case.
Concerning extensions of the present work  an important open case is the one where the number of
samples per dimension α = Θ(1) and also the size of the hidden layer per dimension K/n = Θ(1)
as n → ∞  while in this paper we treated the case K = Θ(1) and n → ∞. This other scaling where
K/n = Θ(1) is challenging even for the non-rigorous replica method.

6 Acknowledgements

This work is supported by the ERC under the European Union’s Horizon 2020 Research and Innova-
tion Program 714608-SMiLe  as well as by the French Agence Nationale de la Recherche under grant
ANR-17-CE23-0023-01 PAIL. We gratefully acknowledge the support of NVIDIA Corporation with
the donation of the Titan Xp GPU used for this research. Jean Barbier was supported by the Swiss
National Foundation grant no 200021-156672. We also thank Léo Miolane for fruitful discussions.

9

References
[1] Vladimir Vapnik. Statistical learning theory. 1998. Wiley  New York  1998.

[2] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[3] Sebastian Seung  Haim Sompolinsky  and Naftali Tishby. Statistical mechanics of learning from

examples. Physical Review A  45(8):6056  1992.

[4] Timothy LH Watkin  Albrecht Rau  and Michael Biehl. The statistical mechanics of learning a

rule. Reviews of Modern Physics  65(2):499  1993.

[5] Rémi Monasson and Riccardo Zecchina. Learning and generalization theories of large

committee-machines. Modern Physics Letters B  9(30):1887–1897  1995.

[6] Rémi Monasson and Riccardo Zecchina. Weight space structure and internal representations: a
direct approach to learning and generalization in multilayer neural networks. Physical review
letters  75(12):2432  1995.

[7] Andreas Engel and Christian PL Van den Broeck. Statistical Mechanics of Learning. Cambridge

University Press  2001.

[8] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016. in
ICLR 2017.

[9] Pratik Chaudhari  Anna Choromanska  Stefano Soatto  Yann LeCun  Carlo Baldassi  Christian
Borgs  Jennifer Chayes  Levent Sagun  and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. arXiv preprint arXiv:1611.01838  2016. in ICLR 2017.

[10] Charles H Martin and Michael W Mahoney. Rethinking generalization requires revisiting
old ideas: statistical mechanics approaches and complex learning behavior. arXiv preprint
arXiv:1710.09553  2017.

[11] Jean Barbier  Florent Krzakala  Nicolas Macris  Léo Miolane  and Lenka Zdeborová. Phase
transitions  optimal errors and optimality of message-passing in generalized linear models.
arXiv preprint arXiv:1708.03395  2017.

[12] Marco Baity-Jesi  Levent Sagun  Mario Geiger  Stefano Spigler  Gérard Ben-Arous  Chiara
Cammarota  Yann LeCun  Matthieu Wyart  and Giulio Biroli. Comparing dynamics: Deep
neural networks versus glassy systems. arXiv preprint arXiv:1803.06969  2018.

[13] Marc Mézard  Giorgio Parisi  and Miguel Virasoro. Spin glass theory and beyond: An Intro-
duction to the Replica Method and Its Applications  volume 9. World Scientiﬁc Publishing
Company  1987.

[14] Marc Mézard and Andrea Montanari. Information  physics  and computation. Oxford University

Press  2009.

[15] David L Donoho  Arian Maleki  and Andrea Montanari. Message-passing algorithms for
compressed sensing. Proceedings of the National Academy of Sciences  106(45):18914–18919 
2009.

[16] Sundeep Rangan. Generalized approximate message passing for estimation with random linear
mixing. In Information Theory Proceedings (ISIT)  2011 IEEE International Symposium on 
pages 2168–2172. IEEE  2011.

[17] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs  with
applications to compressed sensing. IEEE Transactions on Information Theory  57(2):764–785 
2011.

[18] Adel Javanmard and Andrea Montanari. State evolution for general approximate message
passing algorithms  with applications to spatial coupling. Information and Inference: A Journal
of the IMA  2(2):115–144  2013.

10

[19] Henry Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A:

Mathematical and General  26(21):5781  1993.

[20] Henry Schwarze and John Hertz. Generalization in a large committee machine. EPL (Euro-

physics Letters)  20(4):375  1992.

[21] Henry Schwarze and John Hertz. Generalization in fully connected committee machines. EPL

(Europhysics Letters)  21(7):785  1993.

[22] German Mato and Nestor Parga. Generalization properties of multilayered neural networks.

Journal of Physics A: Mathematical and General  25(19):5047  1992.

[23] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E 

52(4):4225  1995.

[24] Jean Barbier and Nicolas Macris. The adaptive interpolation method: A simple scheme to prove
replica formulas in bayesian inference. To appear in Probability Theory and Related Fields 
arXiv preprint http://arxiv.org/abs/1705.02780 [cs.IT]  2017.

[25] David L Donoho  Iain Johnstone  and Andrea Montanari. Accurate prediction of phase tran-
sitions in compressed sensing via a connection to minimax denoising. IEEE transactions on
information theory  59(6):3396–3433  2013.

[26] Lenka Zdeborová and Florent Krzakala. Statistical physics of inference:

algorithms. Advances in Physics  65(5):453–552  2016.

thresholds and

[27] Yash Deshpande and Andrea Montanari. Finding hidden cliques of size \sqrt {N/e} n/e in

nearly linear time. Foundations of Computational Mathematics  15(4):1069–1128  2015.

[28] Afonso S Bandeira  Amelia Perry  and Alexander S Wein. Notes on computational-to-statistical

gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132  2018.

[29] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural

networks. arXiv preprint arXiv:1712.08968  2017.

[30] Ahmed El Alaoui  Aaditya Ramdas  Florent Krzakala  Lenka Zdeborová  and Michael I
Jordan. Decoding from pooled data: Sharp information-theoretic bounds. arXiv preprint
arXiv:1611.09981  2016.

[31] Ahmed El Alaoui  Aaditya Ramdas  Florent Krzakala  Lenka Zdeborová  and Michael I Jordan.
Decoding from pooled data: Phase transitions of message passing. In Information Theory (ISIT) 
2017 IEEE International Symposium on  pages 2780–2784. IEEE  2017.

[32] Junan Zhu  Dror Baron  and Florent Krzakala. Performance limits for noisy multimeasurement

vector problems. IEEE Transactions on Signal Processing  65(9):2444–2454  2017.

[33] Benjamin Aubin  Antoine Maillard  Jean Barbier  Florent Krzakala  Nicolas Macris  and Lenka
Zdeborová. The committee machine: Computational to statistical gaps in learning a two-layers
neural network. arXiv:1806.05451  2018.

[34] Francesco Guerra. Broken replica symmetry bounds in the mean ﬁeld spin glass model.

Communications in mathematical physics  233(1):1–12  2003.

[35] Michel Talagrand. Spin glasses: a challenge for mathematicians: cavity and mean ﬁeld models 

volume 46. Springer Science & Business Media  2003.

[36] David J Thouless  Philip W Anderson  and Robert G Palmer. Solution of’solvable model of a

spin glass’. Philosophical Magazine  35(3):593–601  1977.

[37] Marc Mézard. The space of interactions in neural networks: Gardner’s computation with the

cavity method. Journal of Physics A: Mathematical and General  22(12):2181–2190  1989.

[38] Manfred Opper and Ole Winther. Mean ﬁeld approach to bayes learning in feed-forward neural

networks. Physical review letters  76(11):1964  1996.

11

[39] Yoshiyuki Kabashima. Inference from correlated patterns: a uniﬁed theory for perceptron
learning and linear vector channels. Journal of Physics: Conference Series  95(1):012001  2008.

[40] Carlo Baldassi  Alfredo Braunstein  Nicolas Brunel  and Riccardo Zecchina. Efﬁcient supervised
learning in networks with binary synapses. Proceedings of the National Academy of Sciences 
104(26):11079–11084  2007.

[41] Benjamin Aubin  Antoine Maillard  Jean Barbier  Florent Krzakala  Nicolas Macris  and
Lenka Zdeborová. AMP implementation of the committee machine. https://github.com/
benjaminaubin/TheCommitteeMachine  2018.

[42] Philip Schniter  Sundeep Rangan  and Alyson K Fletcher. Vector approximate message passing
for the generalized linear model. In Signals  Systems and Computers  2016 50th Asilomar
Conference on  pages 1525–1529. IEEE  2016.

[43] Florent Krzakala  Marc Mézard  Francois Sausset  Yifan Sun  and Lenka Zdeborová. Probabilis-
tic reconstruction in compressed sensing: algorithms  phase diagrams  and threshold achieving
matrices. Journal of Statistical Mechanics: Theory and Experiment  2012(08):P08009  2012.

[44] Ulugbek Kamilov  Sundeep Rangan  Michael Unser  and Alyson K Fletcher. Approximate
message passing with consistent parameter estimation and applications to sparse learning. In
Advances in Neural Information Processing Systems  pages 2438–2446  2012.

12

,Benjamin Aubin
Antoine Maillard
jean barbier
Florent Krzakala
Nicolas Macris
Lenka Zdeborová