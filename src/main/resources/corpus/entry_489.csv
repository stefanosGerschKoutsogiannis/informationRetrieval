2018,Alternating optimization of decision trees  with application to learning sparse oblique trees,Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice  dating to the 1980s  is based on a greedy growth of the tree structure by recursively splitting nodes  and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that  given an input tree (its structure and the parameter values at its nodes)  produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further  the same algorithm can handle a sparsity penalty  so it can learn sparse oblique trees  having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data  low generalization error  fast inference and interpretable nodes that involve only a few features in their decision.,Alternating Optimization of Decision Trees 

with Application to Learning Sparse Oblique Trees

Miguel ´A. Carreira-Perpi ˜n´an

Pooya Tavallali

Dept. EECS  University of California  Merced

Dept. EECS  University of California  Merced

mcarreira-perpinan@ucmerced.edu

ptavallali@ucmerced.edu

Abstract

Learning a decision tree from data is a difﬁcult optimization problem. The most
widespread algorithm in practice  dating to the 1980s  is based on a greedy growth
of the tree structure by recursively splitting nodes  and possibly pruning back the
ﬁnal tree. The parameters (decision function) of an internal node are approxi-
mately estimated by minimizing an impurity measure. We give an algorithm that 
given an input tree (its structure and the parameter values at its nodes)  produces
a new tree with the same or smaller structure but new parameter values that prov-
ably lower or leave unchanged the misclassiﬁcation error. This can be applied
to both axis-aligned and oblique trees and our experiments show it consistently
outperforms various other algorithms while being highly scalable to large datasets
and trees. Further  the same algorithm can handle a sparsity penalty  so it can
learn sparse oblique trees  having a structure that is a subset of the original tree
and few nonzero parameters. This combines the best of axis-aligned and oblique
trees: ﬂexibility to model correlated data  low generalization error  fast inference
and interpretable nodes that involve only a few features in their decision.

1 Introduction

Decision trees are among the most widely used statistical models in practice. They are routinely at
the top of the list in the KDnuggets.com annual polls of top machine learning algorithms and other
top-10 lists [36]. Many statistical or mathematical packages such as SAS or Matlab implement them.
Apart from being able to model nonlinear data well in the ﬁrst place  they enjoy several unique ad-
vantages. The prediction made by the tree is a path from the root to a leaf consisting of a sequence
of decisions  each involving a question of the type “xj > bi” (is feature j bigger than threshold bi?)
for axis-aligned trees  or “wT
x > bi” for oblique trees. This makes inference very fast  and may not
i
even need to use all input features to make a prediction (with axis-aligned trees). The path can be
understood as a sequence of IF-THEN rules  which is intuitive to humans  and indeed one can equiv-
alently turn the tree into a database of rules. This can make decision trees preferable over more ac-
curate models such as neural nets in some applications  such as medical diagnosis or legal analysis.

However  trees pose one crucial problem that is only partly solved to date: learning the tree from
data is a very difﬁcult optimization problem  involving a search over a complex  large set of tree
structures  and over the parameters at each node. For simplicity  in this paper we focus on classiﬁca-
tion trees with a binary tree (having a binary split at each node) where the bipartition in each node is
either an axis-aligned hyperplane or an arbitrary hyperplane (oblique trees). However  many of our
arguments carry over beyond this case.

Ideally  the objective function we would like to optimize has the usual form of a regularized loss:

(1)
where L is the misclassiﬁcation error on the training set  given below in eq. (2)  and C is the com-
plexity of the tree  e.g. its depth or number of nodes. This is necessary to avoid large trees that ﬁnely

E(T ) = L(T ) + α C(T )

α > 0

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

split the space so the dataset is perfectly classiﬁed  but would likely overﬁt. Finding an optimal
decision tree is NP-hard [22] even if we ﬁx its number of splits [26].

How does one learn a tree in practice (also called “tree induction”)? After many decades of research 
the algorithms that have stood the test of time are  in spite of their obvious suboptimality  (variations
of) greedy growing and pruning  such as CART [8] or C4.5 [31]. First  a tree is grown by recursively
splitting each node into two children  using an impurity measure. One can stop growing and return
the tree when the impurity of each leaf falls below a set threshold. Even better trees are produced by
growing a large tree and pruning it back one node at a time. At each growing step  the parameters
at the node are learned by minimizing an impurity measure such as the Gini index  cross-entropy or
misclassiﬁcation error. The goal is to ﬁnd a bipartition where each class is as pure (single-class) as
possible. Gini or cross-entropy are preferable to misclassiﬁcation error because the former are more
sensitive to changes in the node probabilities than the misclassiﬁcation rate [20  p. 309]. Minimizing
the impurity over the parameters at the node depends on the node type:

• Axis-aligned trees: the exact solution can be found by enumeration over all (feature  thresh-
old) combinations. For a given feature (= axis)  the possible thresholds are the midpoints
between consecutive training point values along that axis. For a node i containing Ni
training points in RD  an efﬁcient implementation can do this in O(DNi) time.

• Oblique trees: minimizing the impurity is much harder because it is a non-differentiable
function of the weights. As these change continuously  points change side of the hyperplane
discontinuously  and so does the impurity. The standard approach is coordinate descent
over the weights at the node  but this tends to get stuck in poor local optima [8  28].

The optimization over the node parameters (exact for axis-aligned trees  approximate for oblique
trees) assumes the rest of the tree (structure and parameters) is ﬁxed. The greedy nature of the
algorithm means that once a node is optimized  it its ﬁxed forever.

Note that it is only in the leaves where an actual predictive model is ﬁt. The internal nodes do not
do prediction  they partition the space ever more ﬁnely into boxes (axis-aligned trees) or polyhedra
(oblique trees). Each internal node bipartitions its region. Each leaf ﬁts a local model to its region
(for classiﬁcation  the model is often the majority label of the training points in its region). Hence 
the tree is really a partition of the space into disjoint regions with a local predictor at each region
and a fast access to the region for a given input point (by propagating it through the tree).

The majority of trees used in practice are axis-aligned  not oblique. Two possible reasons for this
are 1) an oblique tree is slower at inference and less interpretable because each node involves all
features. And 2) as noted above and conﬁrmed in our experiments  coordinate descent for oblique
trees does not work as well  and often an axis-aligned tree will outperform in test error an oblique
tree of similar size. This is disappointing because an axis-aligned tree imposes an arbitrary region
geometry that is unsuitable for many problems and results in larger trees than would be needed.

In this paper we improve both of these problems with oblique trees. We focus on a restricted set-
ting: we assume a given tree structure  given by an initial tree (CART or random). We propose
an optimization algorithm for the tree parameters that considerably decreases its misclassiﬁcation
error. Further  this allows us to introduce a new type of trees that we call sparse oblique trees  where
each node is a hyperplane involving only a small subset of features  and whose structure is a pruned
version of the original tree. Our algorithm is based on directly optimizing the quantity of interest 
the misclassiﬁcation error  using alternating optimization over separable subsets of nodes. After a
section 2 on related work  we describe our algorithm in section 3 and evaluate it in sections 4 and 5.

2 Related work

The CART book [8] is a good summary of work on decision trees up to the 80s  including the basic
algorithms to learn the tree structure (greedy growing and pruning)  and to optimize the impurity
measure at each node (by enumeration for the axis-aligned case and coordinate descent over the
weights for the oblique case). OC1 [28] is a minor variation of the coordinate descent algorithm of
CART for oblique trees that uses multiple restarts and random perturbations after convergence to try
to ﬁnd a better local optimum  but its practical improvement is marginal. See [31  32] for reviews
of more recent work  including tree induction and applications. There is also a large literature on

2

constructing ensembles of trees  such as random forests [7  13] or boosting [33]  but we focus here
on learning a single tree.

Much research has focused on optimizing the parameters of a tree given a initial tree (obtained with
greedy growing and pruning) whose structure is kept ﬁxed. Bennett [2  3] casts the problem of
optimizing a ﬁxed tree as a linear programming problem  in which the global optimum could be
found. However  the linear program is so large that the procedure is only practical for very small
trees (4 internal nodes in her experiments); also  it applies only to binary classiﬁcation. Norouzi
et al. [29  30] introduce a framework based on optimizing an upper bound over the tree loss using
stochastic gradient descent (initialized from an already induced tree). Their method is scalable to
large datasets  however it is not guaranteed to decrease the real loss function of a decision tree and
may even marginally worsen an already induced tree.

Bertsimas and Dunn [4] formulate the optimization over tree structures (limited to a given depth)
and node parameters as a mixed-integer optimization (MIO) by introducing auxiliary binary vari-
ables that encode the tree structure. Then  one can apply state-of-the-art MIO solvers (based on
branch-and-bound) that are guaranteed to ﬁnd the globally optimum tree (unlike the classical  greedy
approach). However  this has a worst-case exponential cost and again is not practical unless the tree
is very small (depth 2 to 4 in their paper).

Methods such as T2 [1]  T3 [34] and T3C [35]  introduce a family of efﬁcient enumeration ap-
proaches constructing optimal non-binary decision trees of depths up to 3. These trees may not be
as interpretable as binary ones and do not outperform existing approaches of inducing trees [34  35].

Finally  soft decision trees assign a probability to every root-leaf path of a ﬁxed tree structure  such
as the hierarchical mixtures of experts [23]. The parameters can be learned by maximum likelihood
with an EM or gradient-based algorithm. However  this loses the fast inference and interpretability
advantages of regular decision trees  since now an instance must follow each root-leaf path.

3 Alternating optimization over node sets

Problem deﬁnition We want to optimize eq. (1) but assuming a given  ﬁxed tree structure (ob-
tained e.g. from the CART algorithm  i.e.  greedy growing and pruning for axis-aligned or oblique
trees with impurity minimization at each node). Equivalently  since the tree complexity is ﬁxed  we
minimize the misclassiﬁcation cost jointly over the parameters Θ = {θi} of all nodes i of the tree:

L(Θ) =

N

X

n=1

L(yn  T (xn; Θ))

(2)

n=1 ⊂ RD × {1  . . .   K} is a training set of D-dimensional real-valued instances
where {(xn  yn)}N
and their labels (in K classes)  L(·  ·) is the 0/1 loss  and T (x; Θ): RD → {1  . . .   K} is the
predictive function of the tree. This is obtained by propagating x along a path from the root down to
a leaf  computing a binary decision fi(x; θi): RD → {left  right} at each internal node i along the
path  and outputing the leaf’s label. Hence  the parameters θi at a node i are:

• If i is a leaf  θi = {yi} ⊂ {1  . . .   K} contains the label at that leaf.
• If i is an internal (decision) node  θi = {wi  bi} where wi ∈ RD is the weight vector and
bi ∈ R the threshold (or bias) for the decision hyperplane “wT
x ≥ bi”. For axis-aligned
i
trees  the decision hyperplane is “xk(i) ≥ bi” where k(i) ∈ {1  . . .   D}  i.e.  we threshold
the feature k(i)  hence θi = {k(i)  bi}.

Separability condition The key to design a good optimization algorithm for (2) is the following
separability condition. Assume the parameters are not shared across nodes (i 6= j ⇒ θi ∩ θj = ∅).
Theorem 3.1 (separability condition). Let T (x; Θ) be the predictive function of a rooted directed
binary decision tree. If nodes i and j (internal or leaves) are not descendants of each other  then  as
a function of θi and θj (i.e.  ﬁxing all other parameters Θ \ {θi  θj})  the function L(Θ) of eq. (2)
can be written as L(θi  θj) = Li(θi) + Lj(θj) + constant  where the “constant” does not depend
on θi or θj .

Proof. Each training point xn for n ∈ {1  . . .   N } follows a unique path from the root to one leaf
of the tree. Hence  a node i receives a subset {(xn  yn): n ∈ Si} of the training set {(xn  yn)}N
n=1 

3

on which its bisector (with parameters θi) will operate. If i and j are not descendants of each other 
then their subsets are disjoint. Since L(Θ) is a separable sum over the N points  then the theorem
follows  with Li(θi) summing those training points in Si  Lj(θj) summing those in Sj   and the
remaining points being summed in the “constant” term. That is  the respective terms are:
L(Θ) = X

L(yn  T (xn; Θ))

L(yn  T (xn; Θ))

L(yn  T (xn; Θ))

.

+ X

+

X

n∈Si

|

{z

Li(θi)

n∈Sj

}

|

n∈{1 ... N }\(Si∪Sj )

{z

Li(θj )

}

|

{z

constant

}

Note that Li depends on the parameters θk of other nodes k besides i but it does not depend on θj .
Likewise  Lj depends on other nodes’ parameters besides j’s but it does not depend on θi; and the
“constant” term depends on other nodes’ parameters but it does not depend on θi or θj .

The separability condition allows us to optimize separately (and in parallel) over the parameters
of any set of nodes that are not descendants of each other (ﬁxing the parameters of the remaining
nodes). This has two advantages. First  we expect a deeper decrease of the loss  because we optimize
over a large set of parameters exactly. This is because optimizing over each node can be done exactly
(see some caveats later) and the nodes separate. Second  the computation is fast: the joint problem
over the set becomes a collection of smaller independent problems over the nodes that can be solved
in parallel (if so desired). There are many possible choices of such node sets  and it is of interest to
make those sets as big as possible  so that we make large  fast moves in search space. One example
of set is “all nodes at the same depth” (distance from the root)  and we will focus on it.

TAO: alternating optimization over depth levels of the tree We cycle over depth levels from the
bottom (leaves) to the top (root) and iterate bottom-top  bottom-top  etc. (i.e.  reverse breadth-ﬁrst
order). We experimented with other orders (top-bottom  or alternating bottom-top and top-bottom)
but found little difference in the results for both axis-aligned and oblique trees. At a given depth
level  we optimize in parallel over all the nodes. We call this algorithm Tree Alternating Optimization
(TAO). The optimization over each node is described below. Before  we make some observations.

As TAO iterates  the root-leaf path followed by each training point changes and so does the set
of points that reach a particular node. This can cause dead branches and pure subtrees  which we
remove after convergence. Dead branches arise if  after optimizing over a node  some of its subtrees
(a child or other descendants) become empty because they receive no training points from their
parent (which sends all its points to the other child). The subtree of a node with one empty child can
be replaced with the non-empty child’s subtree. We do not do this as soon as they become empty
in case they might become non-empty again. Pure subtrees arise if  after optimizing over a node 
some of its subtrees become pure (i.e.  all their points have the same label). A pure subtree can be
replaced with a leaf but  as with dead branches  we do this after convergence  in case they become
impure again. During each pass  only non-empty  impure nodes are processed  so dead branches
and pure subtrees are ignored  which accelerates the algorithm. This means that TAO can actually
modify the tree structure  by reducing the size of the tree; we call this indirect pruning  and it is very
signiﬁcant with sparse oblique trees (described later). It is a good thing because we achieve (while
always decreasing the training loss) a smaller tree that is faster  takes less space and—having fewer
parameters—probably generalizes better. TAO pseudocode appears in the supplementary material.

Optimizing the misclassiﬁcation error at a single node As we show below  optimizing eq. (2) 
the K-class misclassiﬁcation error of the tree over a node’s parameters (decision function at an
internal node or predictor model at a leaf)  reduces to optimizing a misclassiﬁcation error of a
simpler classiﬁer. In some important special cases this “reduced problem” can be solved exactly 
but in general it is an NP-hard problem [17  21]. In the latter case  we can approximate it by a
surrogate loss (e.g. logistic or hinge loss with a support vector machine). We consider each type of
node next (leaf or internal).

Optimizing (2) over a leaf is equivalent to minimizing the K-class misclassiﬁcation error over the
subset of training points that reach the leaf. If the classiﬁer at the leaf is a constant label  this is
solved exactly by majority vote (i.e.  setting the leaf label to the most frequent label in the leaf’s
subset of points). If the classiﬁer at the leaf is some other model  we can train it using a surrogate of
the misclassiﬁcation error.

4

Optimizing (2) over an internal node i is exactly equivalent to a reduced problem: a binary misclassi-
ﬁcation loss for a certain subset Ci (deﬁned below) of the training points over the parameters θi of i’s
decision function fi(x; θi). The argument is subtle; we show it step by step. Firstly  optimizing the
misclassiﬁcation error over θi in (2)  where is summed over the whole training set  is equivalent to
optimizing it over the subset of training points Si that reach node i. Next  the fate of a point xn ∈ Si
(i.e.  the label the tree will predict for it) depends only on which of i’s children it follows  because
the decision functions and leaf predictor models in the subtree rooted at i are ﬁxed (in other words 
the subtree of each child of i is a ﬁxed decision tree that classiﬁes whatever is inputted to it). Hence 
call zj ∈ {1  . . .   K} the label predicted for xn if following child j (where j is left or right). Now 
comparing the ground-truth label yn of xn in the training set with the predicted label zj for child j 
they can either be equal (yn = zj   correct classiﬁcation) or not (yn 6= zj  incorrect classiﬁcation).
Hence  if xn is correctly classiﬁed for all children j ∈ {left  right}  or incorrectly classiﬁed for all
children j ∈ {left  right}  the fate for this point cannot be altered by changing the decision function
at i  and we call such a point “don’t-care”. It can be removed from the loss since it contributes an ad-
ditive constant. Therefore  the only points (“care” points) whose fate does depend on the parameters
of i’s decision function are those for which zleft = yn 6= zright or zright = yn 6= zleft. Then  we can
deﬁne a new  binary classiﬁcation problem over the parameters θi of the decision function fi(x; θi)
on the “care” points Ci ⊆ Si where xn ∈ Ci has a label yn ∈ {left  right}  according to which child
classiﬁes it correctly. The misclassiﬁcation error in this problem equals the misclassiﬁcation error
in eq. (2) for each “care” point. In summary  we have proven the following theorem.

Theorem 3.2 (reduced problem). Let T (x; Θ) be the predictive function of a rooted directed binary
decision tree and i be any internal node in the tree with decision function fi(x; θi). The tree’s
K-class misclassiﬁcation error (2) can be written as:

L(Θ) =

N

X

n=1

L(yn  T (xn; Θ)) = X

n∈Ci

L(yn  fi(xn; θi)) + constant

(3)

where the “constant” does not depend on θi  Ci ⊂ {1  . . .   N } is the set of “care” training points
for i deﬁned above  and yn ∈ {left  right} is the child that leads to a correct classiﬁcation for xn
under i’s current subtree.

All is left now is how to solve this binary misclassiﬁcation loss problem:

Reduced problem:

min
θi

X

n∈Ci

L(yn  fi(xn; θi)).

(4)

For axis-aligned trees  it can be solved exactly by enumeration over features and splits  just as in the
CART algorithm to minimize the impurity. For oblique trees  we approximate it by a surrogate loss.

The reduced problem is  of course  much easier to solve than the original loss over the tree. In
particular for the oblique case (where the node decision function is a hyperplane  for which enu-
meration is intractable)  the reduced problem introduces a crucial advantage over the traditional
impurity minimization in CART. The latter is a non-differentiable  unsupervised problem  which is
solved rather inaccurately via coordinate descent over the hyperplane weights. The reduced prob-
lem is non-differentiable but supervised and can be conveniently approximated by a surrogate binary
classiﬁcation loss  much easier to solve accurately. This improved optimization translates into much
better trees using TAO  as seen in our experiments.

Sparse oblique trees The equivalence of optimizing (2) over one oblique node to the reduced
problem (4) makes it computationally easy to introduce constraints over the weight vector and hence
learn more ﬂexible types of oblique trees than was possible before. In this paper we propose to learn
oblique nodes involving few features. We can do this by adding an ℓ1 penalty “λPnodes i kwik1” to
the misclassiﬁcation error (2) where λ ≥ 0 controls the sparsity: from no sparsity for λ = 0 to all
weight vectors in all nodes being zero if λ is large enough. Since this penalty separates over nodes 
the only change in TAO is that the optimization over node i in eq. (4) has a penalty “λkwik1”.
This can be easily combined with the formulation above  resulting in an ℓ1-regularized linear SVM
or logistic regression [19  sections 3.2 and 3.6]  a convex problem for which well-developed code
exists  such as LIBLINEAR [15]. Alternatively  one can use an ℓ0 penalty or constraint on the
weights  for which good optimization algorithms also exist [14].

5

Convergence and computational complexity Optimizing the misclassiﬁcation loss L is NP-hard
in general [17  21  26] and we have no approximation guarantees for TAO at present. TAO does
converge to a local optimum in the sense of alternating optimization (as in k-means)  i.e.  when no
more progress can be made by optimizing one subset of nodes given the rest. For oblique trees 
the complexity of one TAO iteration (pass over all nodes) is upper bounded by the tree depth times
the cost of solving an SVM on the whole training set  and is typically quite smaller than that. For
axis-aligned trees  one TAO iteration is comparable to running CART to grow a tree of the same
size  since in both cases the nodes run an enumeration procedure over features and thresholds. See
details in the supplementary material.

4 Experiments: sparse oblique trees for MNIST digits

The supplementary material gives additional experiments using TAO to optimize axis-aligned and
oblique trees on over 10 datasets and comparing with other methods for optimizing trees.
In a
nutshell  TAO produces trees that signiﬁcantly improve over the CART baseline for axis-aligned
and especially oblique trees  and also consistently beat the other methods. But where TAO is truly
remarkable is with sparse oblique trees  and we explore this here in the MNIST benchmark [27].

We induce an initial tree for TAO using the CART algorithm1 (greedy growing and pruning) either
for axis-aligned trees (enumeration over features/thresholds) or oblique trees (coordinate descent
over weights  picking the best of several random restarts [28]). The node optimization uses an ℓ1-
regularized linear SVM with slack hyperparameter C ≥ 0 (so the TAO sparsity hyperparameter in
section 3 is λ = 1/C)  implemented with LIBLINEAR [15]. The rest of our code is in Matlab. We
stop TAO when the training misclassiﬁcation loss decreases but by less than 0.5%  or the number of
iterations (passes over all nodes) reaches 14 (in practice TAO stops after around 7 iterations).

Sparse oblique trees behave like the LASSO [20]: we have a path of trees as a function of the
sparsity hyperparameter C  from ∞ (no ℓ1 penalty) to 0 (all parameters zero). We estimate this path
by inducing an initial CART tree and running TAO for a sequence of decreasing C values  where the
tree at the current C value is used to initialize TAO for the next  smaller C value. We constructed
paths using initial CART trees of depths 4 to 12 (both axis-aligned and oblique) on the MNIST
dataset  splitting its training set of 60k points into 48k training and 12k validation (to determine an
optimal C or depth)  and reporting generalization error on its 10k test points. The training time for
each C value is roughly between 1 minute (for depth 4) and 4 minutes (for depth 12).

Path of trees The resulting paths are best viewed in our supplementary animations. Fig. 1 shows
three representative trees from the path for depth 12: the initial CART tree (which was oblique)  the
tree with optimal validation error and an oversparse tree. It also plots various measures as a function
of C. Several observations are obvious  as follows.

As soon as TAO runs on the initial CART tree (for the largest C value  which imposes little sparsity) 
the improvement is drastic: from a training/test error of 1.95%/11.03% down to 0.09%/5.66%. The
tree is pruned from 410 to 230 internal nodes (the number of leaves for a binary tree equals the
number of internal nodes plus 1). The TAO tree is more balanced: the samples are distributed more
evenly over the tree branches and the training subset that a node receives is more pure. This can
be seen explicitly from the node histograms (tree as binary heap animations in the supplementary
material) or indirectly from the sample mean of the node.

Further decreasing C imposes more sparsity and this leads to progressive pruning of the tree and
ever sparser weight vectors at the internal nodes. The large changes in topology are caused by
postprocessing the tree to remove dead branches. The number of nonzero weights and the number
of nodes (internal and leaves) decreases monotonically with C. The training error slowly increases
but the test error remains about constant. In general  depending on the initial tree size  we ﬁnd the
validation error (not shown) and the test error are minimal for some range of C); trees there will
generalize best. Further decreasing C (beyond 0.01 in the ﬁgure) increases both training and test
error and produces smaller trees with sparser weight vectors that underﬁt.

1During the review period we found out that TAO performs about as well on random initial trees (having
random parameters at the nodes) as on trees induced by CART. This would make TAO a stand-alone learning
algorithm rather than a postprocessing step over a CART tree. We will report on this in a separate publication.

6

Inference runtime For inference (prediction)  each point follows a different root-leaf path. We
report its mean/min/max path length (number of nodes) and runtime (number of operations  here
scalar multiplications) over the training set  for each C. It decreases mostly monotonically with C.
The inference time is extremely fast because the root-leaf path involves a handful of nodes and each
node’s decision function looks at a few pixels of the image. This is orders-of-magnitude faster than
kernel machines  deep nets  random forests or nearest-neighbor classiﬁers  and is a major advantage
of trees. The same can be said of the model storage required. This is especially important at present
given the need to deploy classiﬁers on resource-constrained IoT devices [9–11  18  24].

Classiﬁcation accuracy The best test error for the TAO trees we obtained (having initial depth up
to 12) is around 5%. To put this in perspective  we plot the error reported for MNIST for a range of
models [27] vs. the number of parameters in ﬁg. 1. The tree error is much better than than of linear
models (≈12%) and boosted stumps (7.7%) and is comparable to that of a 2-layer neural net and a
3-nearest-neighbor classiﬁer. Better errors can of course be achieved by many-parameter  complex
models such as kernel SVMs or convolutional nets (not shown)  or using image-speciﬁc transfor-
mations and features. Our trees operate directly on the image pixels with no special transformation
and are astoundingly small and fast given their accuracy. For example  our tree achieves about
the same test error as a 3-nearest-neighbor classiﬁer  but the tree compares the input image with
at most ≈6 sparse “images” (weight vectors)  rather than with the 60 000 dense training images.

Interpretable trees By using a high sparsity penalty  TAO allows us to obtain trees of suboptimal
but reasonable test error that have a really small number of nodes and nonzero weights and are
eminently interpretable. Fig. 1 shows an example for C = 0.01 (test error 10.19%  0.22% nonzeros 
17 leaves). Examining the nodes’ weight vectors shows that the few weights that are not zero are
strategically located to discriminate between speciﬁc classes or groups of classes  and essentially
detect patterns characterized by the presence or absence of strokes in certain locations. Nodes use
minimal features to separate large groups of digits  and leaf parents often separate very similar digits
that differ on just one or two strokes. We mention some examples (referring to the nodes by their
index in the ﬁgure). Node 53 separates 3s from 5s by detecting the small vertical stroke that precisely
differentiates these digits (blue = negative  red = positive). Node 5 separates 4s from 9s by detecting
the presence of a top horizontal stroke. Node 2 separates 7s from {4 9} by detecting ink in the
left-middle of the image. Node 6 separates 0s from {1 2 3 5 8} by detecting the presence of ink in
the center of the image but not in its sides. Node 1 (the root) separates {4 7 9} from the remaining
digits. Also  once the tree is sparse enough  several of these weight vectors (such as nodes 2 and 5)
tend to appear in the tree regardless of the initial tree and depth (see supplementary animations).

In a sense  each decision node pays attention to a simple but high-level concept so the tree classiﬁes
digits by asking a few conceptual questions about the relative spatial presence of strokes or ink. A
root-leaf path can then be seen as a sequence of conceptual questions that “deﬁne” a class. This is
very different from the way convolutional neural networks operate  by constructing a large number
of “features” that start very simple (e.g. edge detectors) and are combined into progressively more
abstract features. While deep neural nets get very accurate predictions (so they are able to classify
correctly even unusual digit shapes)  this is achieved by very complex models that are not easy to
interpret. Our trees do not reach such high accuracy  but arguably they are able to learn the more
high-level  conceptual structure of each digit class.

5 Experiments: comparison with forest-based nearest-neighbor classiﬁers

As requested by a reviewer  we compared CART+TAO with fast  forest-based algorithms that ap-
proximate a nearest-neighbor classiﬁer (see [25] and references therein). Roughly speaking  these
algorithms construct a tree that can approximate a nearest neighbor search and have a controllable
tradeoff between approximation error and search speed. Thus  they can be used to approximate a
nearest-neighbor classiﬁer fast. On top of that  they can be ensembled into a forest. Although the
comparison is not apples-to-apples (since the latter classiﬁers are not of the decision-tree type  and
also are forests rather than a single tree)  it is still very interesting.

We followed the protocol of [25]  which lists results for cover trees (CT) [5]  random kd-trees
(forest of 4 trees) [16] and boundary forest (BF) (50 trees) [25]. Table 1 shows our results for TAO
on oblique trees (we initialized TAO from both axis-aligned and oblique CART trees and picked the

7

initial tree:

CART  oblique

1 (b=-1.19e-03)

train err= 1.95%

%nonzero= 6.65%

test err=11.03%

#splits= 410

2 (b=9.68e-09)

3 (b=7.16e-03)

4 (b=2.09e-05)

5 (b=1.38e-10)

6 (b=1.02e-03)

7 (b=1.28e-03)

-1

 0

 1

8 (b=9.24e-04)

9 (b=3.50e-05)

10 (b=-1.45e-04)

11 (b=6.93e-06)

12 (b=1.56e-03)

13 (b=6.27e-02)

14 (b=2.57e-01)

15 (b=3.46e-04)

16 (b=2.18e-02)

17 (b=6.68e-03)

18 (b=1.04e-02)

19 (b=6.40e-06)

20 (b=4.56e-04)

21 (b=1.41e-08)

22 (b=7.28e-03)

23 (b=1.39e-05)

24(19  1 ) 25 (b=6.48e-02)

26 (b=3.48e-03)

27(9  3 )

28 (b=6.45e-01)

29 (b=6.28e-03)

30 (b=1.35e-01)

31 (b=5.14e-02)

32 (b=5.84e-02)

33 (b=3.54e-02)

34 (b=2.56e-01)

35 (b=4.55e-02)

36 (b=2.67e-03)

37 (b=5.35e-03)

38 (b=1.80e-03)

39 (b=5.72e-04)

40 (b=2.61e-05)

41 (b=6.01e-03)

42 (b=4.12e-09)

43 (b=2.70e-06)

44 (b=3.50e-02)

45 (b=3.65e-03)

46 (b=4.45e-03)

47 (b=3.28e-05)

50(1  8 )

51(15  2 )

52 (b=5.08e-04)

53(22  8 )

56 (b=7.20e-02)

57(2  1 )

58 (b=1.18e-01)

59(2  2 )

60 (b=4.11e-02)

61(3  5 )

62(1  6 )

63(31  8 )

64 (b=1.82e-03)

65 (b=2.35e-02)

66 (b=5.88e-03)

67(23  2 )

68(8  2 ) 69 (b=1.96e-02)

70(45  9 ) 71 (b=2.79e-02)

72 (b=4.45e-03)

73 (b=4.11e-02)

74 (b=6.72e-03)

75 (b=5.71e-02)

76 (b=3.72e-02)

77 (b=2.03e-04)

78 (b=7.74e-04)

79 (b=3.68e-05)

80 (b=2.08e-01)

81 (b=6.31e-01)

82 (b=6.49e-03)

83(33  2 )

84 (b=6.67e-03)

85 (b=2.14e-04)

86 (b=2.03e-06)

87 (b=2.00e-06)

88 (b=4.35e-04)

89 (b=8.66e-03)

90 (b=1.29e-04)

91 (b=9.98e-01)

92 (b=6.35e-01)

93 (b=1.84e-03)

94 (b=2.93e-03)

95 (b=8.16e-02)

104 (b=1.77e-01)

105(6  5 )

112 (b=9.64e-01)

113(1  8 )

116 (b=3.33e-02)

117(2  3 )

120(2  1 )

121(2  3 )

158(46  7 )

159 (b=4.04e-02)

160 (b=4.59e-02)

161(14  6 )

162 (b=8.18e-01)

163 (b=9.90e-01)

164 (b=2.40e-02)165 (b=7.93e-01)

319 (b=1.77e-02)

320 (b=9.62e-02)

324 (b=9.91e-01)

330 (b=4.87e-01)

288 (b=2.29e-02)

289 (b=2.16e-02)

290 (b=6.60e-03)

291 (b=4.48e-04)

294 (b=1.52e-02)

296 (b=6.85e-04)

301 (b=7.37e-01)

303 (b=9.90e-01)

308 (b=1.90e-02)

310 (b=1.49e-06)

311 (b=9.86e-01)

313 (b=1.29e-01)

314 (b=3.86e-04)

315 (b=2.81e-03)

336 (b=8.84e-04)

337 (b=4.00e-03)

338 (b=4.03e-03)

339 (b=4.26e-05)

340 (b=2.72e-02)

341 (b=1.96e-03)

342 (b=9.48e-02)

343 (b=1.43e-03)

344 (b=1.56e-02)

345 (b=1.10e-01)

346 (b=1.26e-03)

347 (b=2.09e-03)

348 (b=7.67e-01)

349 (b=4.39e-03)

350 (b=3.60e-03)

351 (b=3.94e-05)

352 (b=5.26e-03)

353 (b=1.25e-03)

256 (b=4.19e-02)

257 (b=4.43e-01)

258 (b=7.58e-03)

259 (b=9.76e-01)

512 (b=1.45e-01)

513 (b=1.47e-02)

514 (b=9.31e-01)

515 (b=1.21e-01)

517 (b=3.41e-01)

263 (b=5.25e-01)

267 (b=2.92e-02)

526 (b=1.36e-02)

535 (b=3.31e-02)

128 (b=4.79e-05)

129 (b=3.93e-02)

130(5 

4 )

131 (b=9.89e-02)

132 (b=9.92e-01)133 (b=6.31e-01)

138 (b=1.76e-02)

139(2 

3 )

142 (b=3.14e-02)

143(4 

1 )

144 (b=6.23e-03)

145 (b=3.43e-03)

146(32  5 )

147 (b=1.25e-02)

148 (b=6.67e-04)

149(31  5 )

150 (b=9.17e-03)

151 (b=9.02e-01)

152 (b=9.76e-01)

153(2 

2 )

154 (b=2.73e-04)

155 (b=1.36e-03)

156 (b=4.47e-02)

157 (b=6.05e-04)

168 (b=2.40e-02)

169 (b=8.31e-05)

170 (b=4.62e-03)

171 (b=6.61e-05)

172 (b=3.34e-02)

173 (b=5.11e-05)

174 (b=4.37e-03)

175 (b=3.84e-05)

176 (b=1.51e-06)

177 (b=1.05e-02)

178(9 

10 )

179 (b=1.60e-01)

180 (b=5.34e-03)

181 (b=2.88e-02)

182 (b=9.54e-01)

183(1 

10 )

184(85  4 )

185(2 

5 )

186 (b=5.94e-01)187 (b=1.03e-03)

188(31  5 )

189 (b=3.97e-02)

190 (b=5.62e-04)

191(2 

5 )

208 (b=5.59e-02)

209(10  6 )

224 (b=3.02e-01)

225(2 

1 )

232(2 

5 )

233(2 

8 )

576 (b=5.87e-01)

577 (b=9.51e-02)

579 (b=2.68e-02)

580 (b=9.98e-01)

581 (b=5.75e-03)

582 (b=9.14e-02)

583 (b=2.04e-02)

588 (b=9.98e-01)

589 (b=2.57e-02)

592 (b=7.95e-03)

593 (b=8.95e-04)

616 (b=9.94e-01)

617 (b=2.72e-02)

621 (b=1.55e-01)

626 (b=7.97e-04)

629 (b=2.28e-01)

630 (b=3.52e-02)

631 (b=1.27e-04)

638 (b=1.45e-01)

639 (b=2.61e-01)

641 (b=6.55e-01)

648 (b=5.79e-03)

661 (b=1.17e-01)

672 (b=4.15e-02)

673 (b=2.19e-01)

674 (b=1.25e-02)

675 (b=5.70e-04)

676 (b=9.98e-01)

677 (b=2.55e-02)

678 (b=9.94e-01)

679 (b=8.03e-04)

680 (b=8.67e-03)

681 (b=1.18e-01)

683 (b=1.90e-02)

684 (b=1.65e-01)

685 (b=7.91e-01)

686 (b=3.22e-06)

687 (b=2.76e-03)

688 (b=9.71e-01)

689 (b=8.63e-02)

692 (b=4.04e-02)

693 (b=9.30e-01)

694 (b=2.96e-03)

695 (b=2.94e-04)

697 (b=1.25e-01)

698 (b=2.59e-03)

699 (b=1.62e-01)

700 (b=2.39e-02)

701 (b=9.30e-04)

702 (b=1.59e-03)

703 (b=9.35e-03)

704 (b=3.32e-01)

705 (b=9.94e-01)

706 (b=6.32e-04)

707 (b=2.18e-01)

355 (b=1.42e-01)

358 (b=6.47e-03)

710 (b=2.42e-01)

716 (b=4.01e-02)

360 (b=9.82e-01)361 (b=4.83e-04)

362 (b=9.95e-01)

363 (b=2.40e-02)

722 (b=2.60e-03)

723 (b=6.42e-01)

724 (b=9.04e-01)

726 (b=5.91e-01)

727 (b=7.14e-01)

375 (b=2.85e-02)

750 (b=4.25e-02)

379 (b=1.78e-02)

380 (b=2.85e-03)

381 (b=1.06e-03)

758 (b=4.76e-01)

760 (b=5.90e-02)

762 (b=1.14e-01)

763 (b=1.28e-03)

417 (b=1.74e-01)

448 (b=9.79e-01)

449 (b=2.14e-02)

834 (b=5.88e-03)

896 (b=1.35e-02)

train err= 4.27%

%nonzero= 0.95%

test err= 5.69%

#splits= 37

C=0.090

1 (b=-7.13e-02)

2 (b=-1.20e-01)

3 (b=1.57e-01)

-1

 0

 1

4 (b=7.23e-01)

5 (b=-6.22e-01)

6 (b=-4.20e-01)

7 (b=4.34e-01)

10

8

6

4

2

10 1

10

9

8

7

6

5

4

3

2

1

training error
test error

10 0

10 -1

10 -2

%nonzero
# splits

400

350

300

250

200

150

100

50

8 (b=3.52e-02)

9(5  2 )

10 (b=5.96e-01)

11 (b=-2.14e-01)

12 (b=2.70e-01)

13 (b=2.07e-01)

14 (b=-2.61e-02)

15 (b=-3.04e+00)

10 1

10 0

10 -1

10 -2

16 (b=4.95e-01)

17(289  2 )

20 (b=-4.20e-01)

21(986  6 )

22 (b=1.52e+00)

23 (b=-1.84e-01)

24 (b=-3.48e-01)

25(230  3 )

26 (b=-2.11e-01)

27 (b=7.47e-02)

28(3621  2 ) 29 (b=-2.95e-01)

30(340  10 ) 31 (b=7.62e-01)

32 (b=7.24e-01)

33(316  1 )

40(1435  4 )

41(180  7 )

44(1427  5 )45 (b=3.58e+00)

46(2596  4 ) 47 (b=-1.13e-01)

48(57  6 ) 49 (b=-1.09e+00)

52 (b=-4.81e-01)

53 (b=4.89e-01)

54(518  5 ) 55 (b=7.34e-02)

58(980  10 )

59(1022  6 )

62(2038  6 )

63(112  10 )

64(2820  7 )

65(526  3 )

90(158  5 )

91(1552  8 )

94(1846  7 )95 (b=-1.42e+00)

98(3408  10 )

99(172  2 )

104 (b=3.50e-01)

105 (b=-2.46e-01)

106(1865  5 ) 107(2220  3 )

110 (b=-1.55e-01)

111 (b=-5.03e-01)

190(277  8 )

191 (b=-3.44e-02)

208(1099 

3 )

209(785  5 )

210(1694 

1 )

211(629  3 )

220(1428 

1 )

221(694  2 )

222(794  6 )

223 (b=9.52e-01)

train err=10.61%

%nonzero= 0.22%

test err=10.19%

#splits= 16

C=0.010

1 (b=-4.01e-02)

2 (b=-4.95e-02)

-1

 0

 1

3 (b=-7.90e-02)

4(4658  7 )

5 (b=-1.38e-01)

6 (b=-4.66e-01)

7 (b=-1.21e-01)

10(4886  4 )

11(4884  9 )

12(2112  10 )

13 (b=9.90e-01)

14 (b=-1.10e-01)

15 (b=-2.41e-01)

26 (b=-4.98e-02)

27 (b=-1.38e-01)

28(4171  2 )

29 (b=-2.30e-01)

30(1315  10 )

31(2866  6 )

52 (b=4.82e-01)

53 (b=1.37e-01)

54 (b=8.53e-02)

55(4624  8 )

58(1586  10 )

59(2117  6 )

104 (b=-3.06e-02)

105(2214  1 )

106(2911  5 )107(2859  3 )

108(2830  1 ) 109(588  2 )

208(1923  3 )

209(1456  5 )

12

10

8

6

4

2
10 1

15

10

5

0

r
o
r
r
e

t
s
e
t

4500

4000

path length
inference time
mean
minimum
maximum

3500

3000

2500

2000

1500

1000

500

10 0

C

10 -1

10 -2

1

depth 4
depth 6
depth 8
depth 10
depth 12
TAO axis-aligned
TAO oblique
CART axis-aligned
CART oblique

6
2

3

10 3
number of parameters

10 4

5

4
7

10 5

Figure 1: Sparse oblique trees for MNIST. Left plots: initial CART tree and sparse oblique trees for
C = 0.09 and 0.01. For each internal node  we show its index and bias value and plot its weight
vector (red = positive  blue = negative  white = zero); you may need to zoom into the image. For
each leaf  we plot the mean of its training points and show something like “4(4658 7)” where 4
is its index  4658 is the number of training points it receives  and 7 is its digit class. Right plots:
several measures of the tree as a function of C ≥ 0:
training/test error; proportion of nonzero
weights and number of internal nodes; and length of root-leaf path and inference time (in scalar
multiplications) for an input sample. The bottom plot shows test error vs. number of parameters for
sparse oblique trees of different depths (color-coded)  initialized from a CART tree that is either axis-
aligned (dotted line) or oblique (solid line). The markers correspond to the initial CART trees (◦  +)
or to models from [27]  numbered as follows: 1) linear classiﬁers  2) one-vs-all linear classiﬁers  3)
2-layer neural net with 300 hidden units  4) 2-layer neural net with 1 000 hidden units  5) 3-nearest-
neighbor classiﬁer  6) one-vs-all classiﬁers where each classiﬁer consists of 50 000 boosted decision
stumps (each operating over a feature and threshold)  7) 3-layer neural net with 500+100 hidden
units. Values outside the axes limits are projected on the boundary of the plots.

8

Table 1: Comparison with forest-based algorithms that approximate a nearest-neighbor classiﬁer.

dataset (N × D  K)

TAO

BF

R-kd

CT

Test error (%)

Inference time

on entire test set (seconds)
CT

R-kd

TAO

BF

MNIST (60 000×784  10)

letter (10 500×16  26)

pendigits (7 494×16  10)
protein (17 766×357  3)
seismic (78 823×50  3)

5.69
7.94
3.14
31.70
27.81

2.24
5.40
2.62
44.20
40.60

3.08
5.50
2.26
53.60
30.80

2.99
5.60
2.80
52.00
38.90

0.18
0.05
0.01
0.05
0.09

23.90
1.16
0.34
35.47
16.20

89.20
1.67
0.75
11.50
65.70

417.60
0.91
0.02
51.40
172.5

C

TAO

0.09
9.11
0.03
0.14
3.28

best result)  ran on a laptop with 2 core i5 CPUs and 12GB RAM (pretty similar to the system of
[25]). TAO’s test error is somewhat bigger (ﬁrst 3 datasets) or quite smaller (last 2 datasets) than
other forest classiﬁers  but it always has faster inference time by at least one order of magnitude. We
reiterate that TAO produces a single tree with sparse decision nodes.

6 Discussion

The way TAO works is very simple: TAO repeatedly trains a simple classiﬁer (binary at the decision
nodes  K-class at the leaves) while monotonically decreasing the objective function. The only thing
that changes over iterations is the subset of training instances on which each classiﬁer is trained. In
order to optimize the misclassiﬁcation error  TAO fundamentally relies on alternating optimization.
This is most effective when two circumstances apply. 1) Some separability into blocks exists in the
problem  as e.g. in matrix factorization  or is created via auxiliary variables  as e.g. with consensus
problems [6] or nested functions [12]. And 2) the step over each block is easy and ideally exact. All
this applies here thanks to the separability condition and the reduced problem.

Two important remarks. First  note TAO is very different from coordinate descent in CART [8 
28]. The latter optimizes the impurity of a single node; each step updates a single weight of its
hyperplane. TAO optimizes the misclassiﬁcation error of the entire tree; each step updates one
entire set of nodes (i.e.  all the weights of all the hyperplanes in those nodes). Second  what we
really want to minimize is the misclassiﬁcation error on the data  not the impurity in each node. The
latter  while useful to construct a good tree structure and initial node parameters  is only indirectly
related to the classiﬁcation accuracy.

The quality of the TAO result naturally depends on the initial tree it is run on. A good strategy
appears to be to grow a large tree with CART that overﬁts the data (or a large tree with random
parameters) and let TAO prune it  particularly if using a sparsity penalty with oblique trees. TAO
also depends on the choice of surrogate loss in the node (decision or leaf) optimization. In our
experience with the logistic or hinge loss the TAO trees considerably improve over the initial CART
or random tree.

7 Conclusion

We have presented Tree Alternating Optimization (TAO)  a scalable algorithm that can ﬁnd a local
optimum of oblique trees given a ﬁxed structure  in the sense of repeatedly decreasing the mis-
classiﬁcation loss until no more progress can be done. A critical difference with the standard tree
induction algorithm is that we do not optimize a proxy measure (the impurity) greedily one node at
a time  but the misclassiﬁcation error itself  jointly and iteratively over all nodes. We suggest to use
TAO as postprocessing after the usual greedy tree induction in CART  or to run TAO directly on a
random initial tree.

TAO could make oblique trees widespread in practice and replace to some extent the considerably
less ﬂexible axis-aligned trees. Even more interesting are the sparse oblique trees we propose.
These can strike a good compromise between ﬂexible modeling of features (involving complex local
correlations  as with image data) and using few features in each node  hence producing a relatively
accurate tree that is very small  fast and interpretable. For MNIST  we believe this is the ﬁrst time
that a single decision tree achieves such high accuracy  comparable to that of much larger models.

Our work opens up important extensions  among others: to other types of trees  such as regression
trees; to other types of nodes beyond linear bisectors or constant-class leaves; to ensembles of trees;
to using TAO with a search over tree structures; and to combining trees with other models.

9

Acknowledgements

Work funded in part by NSF award IIS–1423515.

References

[1] P. Auer  R. C. Holte  and W. Maass. Theory and applications of agnostic PAC–learning with
In A. Prieditis and S. J. Russell  editors  Proc. of the 12th Int. Conf.

small decision trees.
Machine Learning (ICML’95)  pages 21–29  Tahoe City  CA  July 9–12 1995.

[2] K. P. Bennett. Decision tree construction via linear programming. In Proc. 4th Midwest Artiﬁ-

cial Intelligence and Cognitive Sience Society Conference  pages 97–101  1992.

[3] K. P. Bennett. Global tree optimization: A non-greedy decision tree algorithm. Computing

Science and Statistics  26:156–160  1994.

[4] D. Bertsimas and J. Dunn. Optimal classiﬁcation trees. Machine Learning  106(7):1039–1082 

July 2017.

[5] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbor.

In W. W.
Cohen and A. Moore  editors  Proc. of the 23rd Int. Conf. Machine Learning (ICML’06)  pages
97–104  Pittsburgh  PA  June 25–29 2006.

[6] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Foundations and Trends in
Machine Learning  3(1):1–122  2011.

[7] L. Breiman. Random forests. Machine Learning  45(1):5–32  Oct. 2001.

[8] L. J. Breiman  J. H. Friedman  R. A. Olshen  and C. J. Stone. Classiﬁcation and Regression

Trees. Wadsworth  Belmont  Calif.  1984.

[9] M. ´A. Carreira-Perpi˜n´an. Model compression as constrained optimization  with application to

neural nets. Part I: General framework. arXiv:1707.01209  July 5 2017.

[10] M. ´A. Carreira-Perpi˜n´an and Y. Idelbayev. Model compression as constrained optimization 

with application to neural nets. Part II: Quantization. arXiv:1707.04319  July 13 2017.

[11] M. ´A. Carreira-Perpi˜n´an and Y. Idelbayev. “Learning-compression” algorithms for neural net
In Proc. of the 2018 IEEE Computer Society Conf. Computer Vision and Pattern

pruning.
Recognition (CVPR’18)  pages 8532–8541  Salt Lake City  UT  June 18–22 2018.

[12] M. ´A. Carreira-Perpi˜n´an and W. Wang. Distributed optimization of deeply nested systems.
In S. Kaski and J. Corander  editors  Proc. of the 17th Int. Conf. Artiﬁcial Intelligence and
Statistics (AISTATS 2014)  pages 10–19  Reykjavik  Iceland  Apr. 22–25 2014.

[13] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analy-

sis. Advances in Computer Vision and Pattern Recognition. Springer-Verlag  2013.

[14] Y. C. Eldar and G. Kutyniok  editors. Compressed Sensing: Theory and Applications. Cam-

bridge University Press  2012.

[15] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. LIBLINEAR: A library for

large linear classiﬁcation. J. Machine Learning Research  9:1871–1874  Aug. 2008.

[16] J. H. Friedman  J. L. Bentley  and R. A. Finkel. An algorithm for ﬁnding best matches in

logarithmic expected time. ACM Trans. Mathematical Software  3(3):209–226  1977.

[17] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. SIAM J.

Comp.  39(2):742–765  2009.

[18] S. Han  J. Pool  J. Tran  and W. Dally. Learning both weights and connections for efﬁcient
neural network.
In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett 
editors  Advances in Neural Information Processing Systems (NIPS)  volume 28  pages 1135–
1143. MIT Press  Cambridge  MA  2015.

10

[19] T. Hastie  R. Tibshirani  and M. Wainwright. Statistical Learning with Sparsity: The Lasso and
Generalizations. Monographs on Statistics and Applied Probability. Chapman & Hall/CRC 
2015.

[20] T. J. Hastie  R. J. Tibshirani  and J. H. Friedman. The Elements of Statistical Learning—
Data Mining  Inference and Prediction. Springer Series in Statistics. Springer-Verlag  second
edition  2009.

[21] K.-U. Hoffgen  H. U. Simon  and K. S. Vanhorn. Robust trainability of single neurons. J.

Computer and System Sciences  50(1):114–125  Feb. 1995.

[22] L. Hyaﬁl and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. Infor-

mation Processing Letters  5(1):15–17  1975.

[23] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural

Computation  6(2):181–214  Mar. 1994.

[24] A. Kumar  S. Goyal  and M. Varma. Resource-efﬁcient machine learning in 2 KB RAM for
the internet of things. In D. Precup and Y. W. Teh  editors  Proc. of the 34th Int. Conf. Machine
Learning (ICML 2017)  pages 1935–1944  Sydney  Australia  Aug. 6–11 2017.

[25] C. Mathy  N. Derbinsky  J. Bento  J. Rosenthal  and J. Yedidia. The boundary forest algorithm
for online supervised and unsupervised learning. In Proc. of the 29th National Conference on
Artiﬁcial Intelligence (AAAI 2015)  pages 2864–2870  Austin  TX  Jan. 25–29 2015.

[26] N. Megiddo. On the complexity of polyhedral separability. Discrete & Computational Geom-

etry  3(4):325–337  Dec. 1988.

[27] MNIST.

MNIST
http://yann.lecun.com/exdb/mnist.

The

database

of

handwritten

digits.

[28] S. K. Murthy  S. Kasif  and S. Salzberg. A system for induction of oblique decision trees. J.

Artiﬁcial Intelligence Research  2:1–32  1994.

[29] M. Norouzi  M. Collins  D. J. Fleet  and P. Kohli. CO2 forest: Improved random forest by

continuous optimization of oblique splits. arXiv:1506.06155  June 24 2015.

[30] M. Norouzi  M. Collins  M. A. Johnson  D. J. Fleet  and P. Kohli. Efﬁcient non-greedy op-
timization of decision trees.
In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and
R. Garnett  editors  Advances in Neural Information Processing Systems (NIPS)  volume 28 
pages 1720–1728. MIT Press  Cambridge  MA  2015.

[31] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann  1993.

[32] L. Rokach and O. Maimon. Data Mining With Decision Trees. Theory and Applications. Num-

ber 69 in Series in Machine Perception and Artiﬁcial Intelligence. World Scientiﬁc  2007.

[33] R. E. Schapire and Y. Freund. Boosting. Foundations and Algorithms. Adaptive Computation

and Machine Learning Series. MIT Press  2012.

[34] C. Tjortjis and J. Keane. T3: A classiﬁcation algorithm for data mining. In H. Yin  N. Allinson 
R. Freeman  J. Keane  and S. Hubbard  editors  Proc. of the 6th Int. Conf. Intelligent Data
Engineering and Automated Learning (IDEAL’02)  pages 50–55  Manchester  UK  Aug. 12–
14 2002.

[35] P. Tzirakis and C. Tjortjis. T3C: Improving a decision tree classiﬁcation algorithm’s interval
splits on continuous attributes. Advances in Data Analysis and Classiﬁcation  11(2):353–370 
June 2017.

[36] X. Wu  V. Kumar  J. R. Quinlan  J. Ghosh  Q. Yang  H. Motoda  G. J. McLachlan  A. Ng 
B. Liu  P. S. Yu  Z.-H. Zhou  M. Steinbach  D. J. Hand  and D. Steinberg. Top 10 algorithms
in data mining. Knowledge and Information Systems  14(1):1–37  Jan. 2008.

11

,Jie Wang
Jiayu Zhou
Jun Liu
Peter Wonka
Jieping Ye
Dangna Li
Kun Yang
Wing Hung Wong
Miguel Carreira-Perpinan
Pooya Tavallali