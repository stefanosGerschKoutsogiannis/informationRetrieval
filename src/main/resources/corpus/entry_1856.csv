2013,Estimating LASSO Risk and Noise Level,We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular  we consider the problem of learning a coefficient vector $\theta_0\in R^p$ from noisy linear observation $y=X\theta_0+w\in R^n$ and the popular estimation procedure of solving an $\ell_1$-penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context  we develop new estimators for the $\ell_2$ estimation risk $\|\hat{\theta}-\theta_0\|_2$ and the variance of the noise. These can be used to select the regularization parameter optimally. Our approach combines Stein unbiased risk estimate (Stein'81) and recent results of (Bayati and Montanari'11-12) on the analysis of approximate message passing and risk of LASSO.  We establish high-dimensional consistency of our estimators for sequences of matrices $X$ of increasing dimensions  with independent Gaussian entries. We establish validity for a broader class of Gaussian designs  conditional on the validity of a certain conjecture from statistical physics.  Our approach is the first that provides an asymptotically consistent risk estimator. In addition  we demonstrate through simulation that our variance estimation outperforms several existing methods in the literature.,Estimating LASSO Risk and Noise Level

Mohsen Bayati
Stanford University

bayati@stanford.edu

Murat A. Erdogdu
Stanford University

erdogdu@stanford.edu

Andrea Montanari
Stanford University

montanar@stanford.edu

Abstract

We study the fundamental problems of variance and risk estimation in high di-
mensional statistical modeling. In particular  we consider the problem of learning
a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn
(p > n) and the popular estimation procedure of solving the (cid:96)1-penalized least
squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In

this context  we develop new estimators for the (cid:96)2 estimation risk (cid:107)(cid:98)θ − θ0(cid:107)2 and

the variance of the noise when distributions of θ0 and w are unknown. These can
be used to select the regularization parameter optimally. Our approach combines
Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b]
on the analysis of approximate message passing and the risk of LASSO.
We establish high-dimensional consistency of our estimators for sequences of ma-
trices X of increasing dimensions  with independent Gaussian entries. We es-
tablish validity for a broader class of Gaussian designs  conditional on a certain
conjecture from statistical physics.
To the best of our knowledge  this result is the ﬁrst that provides an asymptotically
consistent risk estimator for the LASSO solely based on data. In addition  we
demonstrate through simulations that our variance estimation outperforms several
existing methods in the literature.

1

Introduction

In Gaussian random design model for the linear regression  we seek to reconstruct an unknown
coefﬁcient vector θ0 ∈ Rp from a vector of noisy linear measurements y ∈ Rn:

y = Xθ0 + w 

(1.1)
where X ∈ Rn×p is a measurement (or feature) matrix with iid rows generated through a multivari-
ate normal density. The noise vector  w  has iid entries with mean 0 and variance σ2. While this
problem is well understood in the low dimensional regime p (cid:28) n  a growing corpus of research
addresses the more challenging high-dimensional scenario in which p > n. The Basis Pursuit De-
noising (BPDN) or LASSO [CD95  Tib96] is an extremely popular approach in this regime  that
ﬁnds an estimate for θ0 by minimizing the following cost function

with λ > 0. In particular  θ0 is estimated by (cid:98)θ(λ; X  y) = argminθ CX y(λ  θ). This method is

CX y(λ  θ) ≡ (2n)−1 (cid:107)y − Xθ(cid:107)2

well suited for the ubiquitous case in which θ0 is sparse  i.e. a small number of features effectively
predict the outcome. Since this optimization problem is convex  it can be solved efﬁciently  and fast
specialized algorithms have been developed for this purpose [BT09].
Research has established a number of important properties of LASSO estimator under suitable con-
ditions on the design matrix X  and for sufﬁciently sparse vectors θ0. Under irrepresentability
conditions  the LASSO correctly recovers the support of θ0 [ZY06  MB06  Wai09]. Under weaker

2 + λ(cid:107)θ(cid:107)1  

(1.2)

1

√

√

conditions such as restricted isometry or compatibility properties the correct recovery of support fails

however  the (cid:96)2 estimation error (cid:107)(cid:98)θ− θ0(cid:107)2 is of the same order as the one achieved by an oracle esti-
provided asymptotic formulas for MSE or other operating characteristics of(cid:98)θ  for Gaussian design

mator that knows the support [CRT06  CT07  BRT09  BdG11]. Finally  [DMM09  RFG09  BM12b]

2 ≤ C kλ2  with k = (cid:107)θ0(cid:107)0 the number of nonzero entries in θ0  as long as λ ≥ cσ

question of estimating accurately the (cid:96)2 error (cid:107)(cid:98)θ − θ0(cid:107)2
(cid:107)(cid:98)θ − θ0(cid:107)2

matrices X.
While the aforementioned research provides solid justiﬁcation for using the LASSO estimator  it is
of limited guidance to the practitioner. For instance  a crucial question is how to set the regularization
parameter λ. This question becomes even more urgent for high-dimensional methods with multiple
regularization terms. The oracle bounds of [CRT06  CT07  BRT09  BdG11] suggest to take λ =
log p with c a dimension-independent constant (say c = 1 or 2). However  in practice a factor
c σ
two in λ can make a substantial difference for statistical applications. Related to this issue is the
2. The above oracle bounds have the form
log p.
As a consequence  minimizing the bound does not yield a recipe for setting λ. Finally  estimating
the noise level is necessary for applying these formulae  and this is in itself a challenging question.
The results of [DMM09  BM12b] provide exact asymptotic formulae for the risk  and its dependence
on the regularization parameter λ. This might appear promising for choosing the optimal value of
λ  but has one serious drawback. The formulae of [DMM09  BM12b] depend on the empirical
distribution1 of the entries of θ0  which is of course unknown  as well as on the noise level2. A step
towards the resolution of this problem was taken in [DMM11]  which determined the least favorable
noise level and distribution of entries  and hence suggested a prescription for λ  and a predicted risk
in this case. While this settles the question (in an asymptotic sense) from a minimax point of view 
it would be preferable to have a prescription that is adaptive to the distribution of the entries of θ0
and to the noise level.
Our starting point is the asymptotic results of [DMM09  DMM11  BM12a  BM12b]. These provide

a construction of an unbiased pseudo-data (cid:98)θu that is asymptotically Gaussian with mean θ0. The
LASSO estimator(cid:98)θ is obtained by applying a denoiser function to(cid:98)θu. We then use Stein’s Unbiased

Risk Estimate (SURE) [Ste81] to derive an expression for the (cid:96)2 risk (mean squared error) of this
operation. What results is an expression for the mean squared error of the LASSO that only depends
on the observed data y and X. Finally  by modifying this formula we obtain an estimator for the
noise level.
We prove that these estimators are asymptotically consistent for sequences of design matrices X
with converging aspect ratio and iid Gaussian entries. We expect that the consistency holds far
beyond this case. In particular  for the case of general Gaussian design matrices  consistency holds
conditionally on a conjectured formula stated in [JM13] on the basis of the “replica method” from
statistical physics.
For the sake of concreteness  let us brieﬂy describe our method in the case of standard Gaussian
design that is when the design matrix X has iid Gaussian entries. We construct the unbiased pseudo-
data vector by

(1.3)
Our estimator of the mean squared error is derived from applying SURE to unbiased pseudo-data.

(cid:98)R(y  X  λ  τ ) ≡ τ 2(cid:16)

(cid:98)θu =(cid:98)θ + X T (y − X(cid:98)θ)/[n − (cid:107)(cid:98)θ(cid:107)0] .
In particular  our estimator is (cid:98)R(y  X  λ (cid:98)τ ) where
(cid:17)
+ (cid:107)X T (y − X(cid:98)θ)(cid:107)2
2(cid:107)(cid:98)θ(cid:107)0/p − 1
Here(cid:98)θ(λ; X  y) is the LASSO estimator and(cid:98)τ = (cid:107)y − X(cid:98)θ(cid:107)2/[n − (cid:107)(cid:98)θ(cid:107)0].
Our estimator of the noise level is(cid:98)σ2/n =(cid:98)τ 2 − (cid:98)R(y  X  λ (cid:98)τ )/δ

2

(cid:46)(cid:2)p(n − (cid:107)(cid:98)θ(cid:107)0)2(cid:3)

(1.4)

where δ = n/p. Although our rigorous results are asymptotic in the problem dimensions  we show
through numerical simulations that they are accurate already on problems with a few thousands of

1The probability distribution that puts a point mass 1/p at each of the p entries of the vector.
2Note that our deﬁnition of noise level σ corresponds to σ

√
n in most of the compressed sensing literature.

2

Figure 1: Red color represents the estimated values by our estimators and green color represents the
true values to be estimated. Left: MSE versus regularization parameter λ. Here  δ = 0.5  σ2/n =
0.2  X ∈ Rn×p with iid N1(0  1) entries where n = 4000. Right: ˆσ2/n versus λ. Comparison of
different estimators of σ2 under the same model parameters. Scaled Lasso’s prescribed choice of
(λ  ˆσ2/n) is marked with a bold x.

variables. To the best of our knowledge  this is the ﬁrst method for estimating the LASSO mean
square error solely based on data. We compare our approach with earlier work on the estimation of
the noise level. The authors of [NSvdG10] target this problem by using a (cid:96)1-penalized maximum
log-likelihood estimator (PMLE) and a related method called “Scaled Lasso” [SZ12] (also studied by
[BC13]) considers an iterative algorithm to jointly estimate the noise level and θ0. Moreover  authors
of [FGH12] developed a reﬁtted cross-validation (RCV) procedure for the same task. Under some
conditions  the aforementioned studies provide consistency results for their noise level estimators.
We compare our estimator with these methods through extensive numerical simulations.
The rest of the paper is organized as follows. In order to motivate our theoretical work  we start with
numerical simulations in Section 2. The necessary background on SURE and asymptotic distribu-
tional characterization of the LASSO is presented in Section 3. Finally  our main theoretical results
can be found in Section 4.

2 Simulation Results

In this section  we validate the accuracy of our estimators through numerical simulations. We also
analyze the behavior of our variance estimator as λ varies  along with four other methods. Two of
these methods rely on the minimization problem 

((cid:98)θ (cid:98)σ) = argminθ σ

(cid:26)(cid:107)y − Xθ(cid:107)2

2

2nh1(σ)

+ h2(σ) + λ

(cid:107)θ(cid:107)1
23 h3(σ)

(cid:27)

 

where for PMLE h1(σ) = σ2  h2(σ) = log(σ)  h3(σ) = σ and for the Scaled Lasso h1(σ) = σ 
h2(σ) = σ/2  and h3(σ) = 1. The third method is a na¨ıve procedure that estimates the variance in
two steps: (i) use the LASSO to determine the relevant variables; (ii) apply ordinary least squares
on the selected variables to estimate the variance. The fourth method is Reﬁtted Cross-Validation
(RCV) by [FGH12] which also has two-stages. RCV requires sure screening property that is the
model selected in its ﬁrst stage includes all the relevant variables. Note that this requirement may
not be satisﬁed for many values of λ.
In our implementation of RCV  we used the LASSO for
variable selection.
In our simulation studies  we used the LASSO solver l1 ls [SJKG07]. We simulated across 50
replications within each  we generated a new Gaussian design matrix X. We solved for LASSO
over 20 equidistant λ’s in the interval [0.1  2]. For each λ  a new signal θ0 and noise independent
from X were generated.

3

llllllllllllllllllll0.00.10.20.30.00.51.01.52.0lMSEResults in a Single RunlEstimated MSETrue MSE90% Confidence BandsEstimated MSETrue MSEAsymptoticsAsymptotic MSEMSE Estimationllllllllllllllllllllxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx0.00.20.40.60.00.51.01.52.0ls^2nlAMP.LASSON.LASSOPMLERCV.LASSOSCALED.LASSOTRUENoise Level EstimationFigure 2: Red color represents the estimated values by our estimators and green color represents
the true values to be estimated. Left: MSE versus regularization parameter λ. Here  δ = 0.5 
σ2/n = 0.2  rows of X ∈ Rn×p are iid from Np(0  Σ) where n = 5000 and Σ has entries 1
on the main diagonal  0.4 on above and below the main diagonal. Right: Comparison of different
estimators of σ2/n. Parameter values are the same as in Figure 1. Scaled Lasso’s prescribed choice
of (λ  ˆσ2/n) is marked with a bold x.

The results are demonstrated in Figures 1 and 2. Figure 1 is obtained using n = 4000  δ = 0.5 and
σ2/n = 0.2. The coordinates of true signal independently get values 0  1  −1 with probabilities 0.9 
iid∼ N1(0  1).
0.05  0.05 respectively. For each replication  we used a design matrix X where Xi j
Figure 2 is obtained with n = 5000 and same values of δ and σ2 as in Figure 1. The coordinates
of true signal independently get values 0  1  −1 with probabilities 0.9  0.05  0.05 respectively. For
each replication  we used a design matrix X where each row is independently generated through
Np(0  Σ) where Σ has 1 on the main diagonal and 0.4 above and below the diagonal.
As can be seen from the ﬁgures  the asymptotic theory applies quite well to the ﬁnite dimensional
data. We refer reader to [BEM13] for a more detailed simulation analysis.

3 Background and Notations

3.1 Preliminaries and Deﬁnitions

t = y − Xθt +

t−1(cid:10)η(cid:48)

t(yt−1)(cid:11)  

1
δ

First  we need to provide a brief introduction to approximate message passing (AMP) algorithm
suggested by [DMM09] and its connection to LASSO (see [DMM09  BM12b] for more details).
For an appropriate sequence of non-linear denoisers {ηt}t≥0  the AMP algorithm constructs a se-
quence of estimates {θt}t≥0  pseudo-data {yt}t≥0  and residuals {t}t≥0 where θt  yt ∈ Rp and
t ∈ Rn. These sequences are generated according to the iteration

θt+1 = ηt(yt)  

yt = θt + X T t/n  

(3.1)
where δ ≡ n/p and the algorithm is initialized with θ0 = 0 = 0 ∈ Rp. In addition  each denoiser
ηt(·) is a separable function and its derivative is denoted by η(cid:48)
t(· ). Given a scalar function f and a
vector u ∈ Rm  we let f (u) denote the vector (f (u1)  . . .   f (um)) ∈ Rm obtained by applying f

component-wise and (cid:104)u(cid:105) ≡ m−1(cid:80)m

i=1 ui is the average of the vector u ∈ Rm.

Next  consider the state evolution for the AMP algorithm. For the random variable Θ0 ∼ pθ0 
a positive constant σ2 and a given sequence of non-linear denoisers {ηt}t≥0  deﬁne the sequence
{τ 2
t }t≥0 iteratively by

τ 2
t+1 = Ft(τ 2

(3.2)
0}/δ and Z ∼ N1(0  1) is independent of Θ0. From Eq. 3.2  it is apparent
where τ 2
that the function Ft depends on the distribution of Θ0. It is shown in [BM12a] that the pseudo-data

0 = σ2 + E{Θ2

t )  

E{ [ηt(Θ0 + τ Z) − Θ0]2}  

Ft(τ 2) ≡ σ2 +

1
δ

4

llllllllllllllllllll0.00.10.20.30.00.51.01.52.0lMSEResults in a Single RunlEstimated MSETrue MSE90% Confidence BandsEstimated MSETrue MSEAsymptoticsAsymptotic MSEMSE Estimationllllllllllllllllllllxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx0.00.20.40.60.00.51.01.52.0ls^2nlAMP.LASSON.LASSOPMLERCV.LASSOSCALED.LASSOTRUENoise Level Estimationyt has the same asymptotic distribution as Θ0 + τtZ. This result can be roughly interpreted as the
pseudo-data generated by AMP is the summation of the true signal and a normally distributed noise
which has zero mean. Its variance is determined by the state evolution. In other words  each iteration
i ≈ θ0 i +N1(0  τ 2
produces a pseudo-data that is distributed normally around the true signal  i.e. yt
t ).
The importance of this result will appear later when we use Stein’s method in order to obtain an
estimator for the MSE and the variance of the noise.
We will use state evolution in order to describe the behavior of a speciﬁc type of converging sequence
deﬁned as the following:
Deﬁnition 1. The sequence of instances {θ0(n)  X(n)  σ2(n)}n∈N indexed by n is said to be a
converging sequence if θ0(n) ∈ Rp  X(n) ∈ Rn×p  σ2(n) ∈ R and p = p(n) is such that n/p →
δ ∈ (0 ∞)  σ2(n)/n → σ2
(a) The empirical distribution of {θ0 i(n)}p

pθ0 on R with bounded 2nd moment. Further  as n → ∞  p−1(cid:80)p

0 for some σ0 ∈ R and in addition the following conditions hold:

i=1  converges in distribution to a probability measure

i=1 θ0 i(n)2 → Epθ0

{Θ2
0}.

then n−1/2 maxi∈[p] (cid:107)X(n)ei(cid:107)2 → 1 

(b) If {ei}1≤i≤p ⊂ Rp denotes the standard basis 
n−1/2 mini∈[p] (cid:107)X(n)ei(cid:107)2 → 1  as n → ∞ with [p] ≡ {1  . . .   p}.
We provide rigorous results for the special class of converging sequences when entries of X are iid
N1(0  1) (i.e.  standard gaussian design model). We also provide results (assuming Conjecture 4.4 is
correct) when rows of X are iid multivariate normal Np(0  Σ) (i.e.  general gaussian design model).
In order to discuss the LASSO connection for the AMP algorithm  we need to use a speciﬁc class of
denoisers and apply an appropriate calibration to the state evolution. Here  we provide brieﬂy how
this can be done and we refer the reader to [BEM13] for a detailed discussion.
Denote by η : R × R+ → R the soft thresholding denoiser where

(cid:40) x − ξ

η(x; ξ) =

0
x + ξ

if x > ξ
if −ξ ≤ x ≤ ξ .
if x < −ξ

Also  denote by η(cid:48)(· ; · )  the derivative of the soft-thresholding function with respect to its ﬁrst
argument. We will use the AMP algorithm with the soft-thresholding denoiser ηt(· ) = η(· ; ξt )
along with a suitable sequence of thresholds {ξt}t≥0 in order to obtain a connection to the LASSO.
Let α > 0 be a constant and at every iteration t  choose the threshold ξt = ατt. It was shown in
[DMM09] and [BM12b] that the state evolution has a unique ﬁxed point τ∗ = limt→∞ τt  and there
exists a mapping α (cid:55)→ τ∗(α)  between those two parameters. Further  it was shown that a function
α (cid:55)→ λ(α) with domain (αmin(δ) ∞) for some constant αmin  and given by

λ(α) ≡ ατ∗(cid:0)1 − 1

E(cid:2)η(cid:48)(Θ0 + τ∗Z; ατ∗)(cid:3)(cid:1)  

δ

admits a well-deﬁned continuous and non-decreasing inverse α : (0 ∞) → (αmin ∞). In particu-
lar  the functions λ (cid:55)→ α(λ) and α (cid:55)→ τ∗(α) provide a calibration between the AMP algorithm and
the LASSO where λ is the regularization parameter.

3.2 Distributional Results for the LASSO

We will proceed by stating a distributional result on LASSO which was established in [BM12b].
Theorem 3.1. Let {θ0(n)  X(n)  σ2(n)}n∈N be a converging sequence of instances of the standard

Gaussian design model. Denote the LASSO estimator of θ0(n) by(cid:98)θ(n  λ) and the unbiased pseudo-
data generated by LASSO by(cid:98)θu(n  λ) ≡(cid:98)θ + X T (y − X(cid:98)θ)/[n − (cid:107)(cid:98)θ(cid:107)0].
Then  as n → ∞  the empirical distribution of {(cid:98)θu
i=1 converges weakly to the joint distri-
bution of (Θ0 + τ∗Z  Θ0) where Θ0 ∼ pθ0  τ∗ = τ∗(α(λ))  Z ∼ N1(0  1) and Θ0 and Z are
independent random variables.
(cid:1)
cal distribution of {(cid:98)θi  θ0 i}p

i=1 converges weakly to the joint distribution of(cid:0)η(Θ0 + τ∗Z; ξ∗)  Θ0

The above theorem combined with the stationarity condition of the LASSO implies that the empiri-

i   θ0 i}p

5

where ξ∗ = α(λ)τ∗(α(λ)). It is also important to emphasize a relation between the asymptotic
MSE  τ 2∗ and the model variance. By Theorem 3.1 and the state evolution recursion  almost surely 
(3.3)

[η(Θ0 + τ∗Z; ξ∗) − Θ0]2(cid:105)

2/p = E(cid:104)

p→∞(cid:107)(cid:98)θ − θ0(cid:107)2

= δ(τ 2∗ − σ2
0)  

lim

which will be helpful to get an estimator for the noise level.

3.3 Stein’s Unbiased Risk Estimator

In [Ste81]  Stein proposed a method to estimate the risk of an almost arbitrary estimator of the mean
of a multivariate normal vector. A generalized form of his method can be stated as the following.
Proposition 3.2. [Ste81]&[Joh12] Let x  µ ∈ Rn and V ∈ Rn×n be such that x ∼ Nn(µ  V ).
Suppose that ˆµ(x) ∈ Rn is an estimator of µ for which ˆµ(x) = x + g(x) and that g : Rn → Rn is
weakly differentiable and that ∀i  j ∈ [n]  Eν[|xigi(x)| + |xjgj(x)|] < ∞ where ν is the measure
corresponding to the multivariate Gaussian distribution Nn(µ  V ). Deﬁne the functional

S(x  ˆµ) ≡ Tr(V ) + 2Tr(V Dg(x)) + (cid:107)g(x)(cid:107)2
2  

2 = Eν[S(x  ˆµ)].

S(x  ˆµ) is an unbiased estimator of

where Dg is the vector derivative.
Eν(cid:107)ˆµ(x) − µ(cid:107)2
In the literature of statistics  the above estimator is called “Stein’s Unbiased Risk Estimator” or
SURE. The following remark will be helpful to build intuition about our approach.
Remark 1. If we consider the risk of soft thresholding estimator η(xi; ξ) for µi when xi ∼
N1(µi  σ2) for i ∈ [m]  the above formula suggests the functional

the risk 

i.e.

m(cid:88)

i=1

m(cid:88)

i=1

S(x  η( · ; ξ))

= σ2 − 2σ2
m
as an estimator of the corresponding MSE.

m

1{|xi|≤ξ} +

1
m

[min{|xi|  ξ}]2  

4 Main Results

4.1 Standard Gaussian Design Model

We start by deﬁning two estimators that are motivated by Proposition 3.2.
Deﬁnition 2. Deﬁne

where x ∈ Rm  τ ∈ R+  and ψ : R → R is a suitable non-linear function. Also for y ∈ Rn and

(cid:98)Rψ(x  τ ) ≡ −τ 2 + 2τ 2(cid:104)ψ(cid:48)(x)(cid:105) + (cid:104) (ψ(x) − x)2(cid:105)  
(cid:107)X T (y − X(cid:98)θ)(cid:107)2
(cid:98)R(y  X  λ  τ ) ≡ τ 2
p(n − (cid:107)(cid:98)θ(cid:107)0)2

X ∈ Rn×p denote by (cid:98)R(y  X  λ  τ )  the estimator of the mean squared error of LASSO where
Remark 2. Note that (cid:98)R(y  X  λ  τ ) is just a special case of (cid:98)Rψ(x  τ ) when x = (cid:98)θu and ψ(· ) =
η(· ; ξ ) for ξ = λ/(1 − (cid:107)(cid:98)θ(cid:107)0/p).

(2(cid:107)(cid:98)θ(cid:107)0 − p) +

p

2

.

We are now ready to state the following theorem on the asymptotic MSE of the AMP:
Theorem 4.1. Let {θ0(n)  X(n)  σ2(n)}n∈N be a converging sequence of instances of the standard
Gaussian design model. Denote the sequence of estimators of θ0(n) by {θt(n)}t≥0  the pseudo-
data by {yt(n)}t≥0  and residuals by {t(n)}t≥0 produced by AMP algorithm using the sequence
of Lipschitz continuous functions {ηt}t≥0 as in Eq. 3.1.
Then  as n → ∞  the mean squared error of the AMP algorithm at iteration t + 1 has the same limit

as (cid:98)Rηt(yt (cid:98)τ ) where(cid:98)τt = (cid:107)t(cid:107)2/
In other words  (cid:98)Rηt(yt (cid:98)τt) is a consistent estimator of the asymptotic mean squared error of the

n→∞(cid:98)Rηt(yt (cid:98)τt) .

n. More precisely  with probability one 

n→∞(cid:107)θt+1 − θ0(cid:107)2

2/p(n) = lim

(4.1)

√

lim

AMP algorithm at iteration t + 1.

6

The above theorem allows us to accurately predict how far the AMP estimate is from the true signal
at iteration t + 1 and this can be utilized as a stopping rule for the AMP algorithm. Note that it was
shown in [BM12b] that the left hand side of Eq. (4.1) is E[(ηt(Θ0 + τtZ) − Θ0)2]. Combining this
with the above theorem  we easily obtain 

n→∞(cid:98)Rηt(yt (cid:98)τt) = E[(ηt(Θ0 + τtZ) − Θ0)2] .

lim

one 

We state the following version of Theorem 4.1 for the LASSO.
Theorem 4.2. Let {θ0(n)  X(n)  σ2(n)}n∈N be a converging sequence of instances of the standard

Gaussian design model. Denote the LASSO estimator of θ0(n) by(cid:98)θ(n  λ). Then with probability
where(cid:98)τ = (cid:107)y − X(cid:98)θ(cid:107)2/[n − (cid:107)(cid:98)θ(cid:107)0]. In other words  (cid:98)R(y  X  λ (cid:98)τ ) is a consistent estimator of the

n→∞(cid:98)R(y  X  λ (cid:98)τ )  

n→∞(cid:107)(cid:98)θ − θ0(cid:107)2

lim

2/p(n) = lim

asymptotic mean squared error of the LASSO.

Note that Theorem 4.2 enables us to assess the quality of the LASSO estimation without knowing
the true signal itself or the noise (or their distribution). The following corollary can be shown using
the above theorem and Eq. 3.3.
Corollary 4.3. In the standard Gaussian design model  the variance of the noise can be accurately

estimated by(cid:98)σ2/n ≡ (cid:98)τ 2 − (cid:98)R(y  X  λ (cid:98)τ )/δ where δ = n/p and other variables are deﬁned as in

Theorem 4.2. In other words  we have

n→∞ ˆσ2/n = σ2
lim
0  

(4.2)

almost surely  providing us a consistent estimator for the variance of the noise in the LASSO.
Remark 3. Theorems 4.1 and 4.2 provide a rigorous method for selecting the regularization pa-
rameter optimally. Also  note that obtaining the expression in Theorem 4.2 only requires solving
one solution path to LASSO problem versus k solution paths required by k-fold cross-validation
methods. Additionally  using the exponential convergence of AMP algorithm for the standard gaus-
sian design model  proved by [BM12b]  one can use O(log(1/)) iterations of AMP algorithm and
Theorem 4.1 to obtain the solution path with an additional error up to O().

4.2 General Gaussian Design Model

In Section 4.1  we devised our estimators based on the standard Gaussian design model. Motivated
by Theorem 4.2  we state the following conjecture of [JM13].
Let {Ω(n)}n∈N be a sequence of inverse covariance matrices. Deﬁne the general Gaussian design
model by the converging sequence of instances {θ0(n)  X(n)  σ2(n)}n∈N where for each n  rows
of design matrix X(n) are iid multivariate Gaussian  i.e. Np(0  Ω(n)−1).
Conjecture 4.4 ([JM13]). Let {θ0(n)  X(n)  σ2(n)}n∈N be a converging sequence of instances
under the general Gaussian design model with a sequence of proper inverse covariance matri-
ces {Ω(n)}n∈N. Assume that the empirical distribution of {(θ0 i  Ωii}p
i=1 converges weakly to

the distribution of a random vector (Θ0  Υ). Denote the LASSO estimator of θ0(n) by (cid:98)θ(n  λ)
and the LASSO pseudo-data by (cid:98)θu(n  λ) ≡ (cid:98)θ + ΩX T (y − X(cid:98)θ)/[n − (cid:107)(cid:98)θ(cid:107)0]. Then  for some
τ ∈ R  the empirical distribution of {θ0 i (cid:98)θu
i   Ωii} converges weakly to the joint distribution of
ther  the empirical distribution of (y − X(cid:98)θ)/[n − (cid:107)(cid:98)θ(cid:107)0] converges weakly to N(0  τ 2).
(Θ0  Θ0 + τ Υ1/2Z  Υ)  where Z ∼ N1(0  1)  and (Θ0  Υ) are independent random variables. Fur-

A heuristic justiﬁcation of this conjecture using the replica method from statistical physics is offered
in [JM13]. Using the above conjecture  we deﬁne the following generalized estimator of the linearly
transformed risk under the general Gaussian design model. The construction of the estimator is
essentially the same as before i.e. apply SURE to unbiased pseudo-data.

7

(cid:16)

τ 2
p

2/p as

Deﬁnition 3. For an inverse covariance matrix Ω and a suitable matrix V ∈ Rp×p  let W = V ΩV T

and deﬁne an estimator of (cid:107)V ((cid:98)θ − θ)(cid:107)2
(cid:107)V ΩX T (y − X(cid:98)θ)(cid:107)2
(cid:98)ΓΩ(y  X  τ  λ  V ) =
p(n − (cid:107)(cid:98)θ(cid:107)0)2
Further (cid:98)θ(n  λ) is the LASSO solution for penalty level λ and τ is a real number. S ⊂ [p] is the
support of(cid:98)θ and ˜S is [p] \ S. Finally  for a p × p matrix M and subsets D  E of [p] the notation

where y ∈ Rn and X ∈ Rn×p denote the linear observations and the design matrix  respectively.

MDE refers to the |D| × |E| sub-matrix of M obtained by intersection of rows with indices from D
and columns with indices from E.

Tr (WSS) − Tr (W ˜S ˜S) − 2Tr

W ˜SSΩS ˜SΩ−1

(cid:17)(cid:17)

+

˜S ˜S

(cid:16)

2

Derivation of the above formula is rather complicated and we refer the reader to [BEM13] for a
detailed argument. A notable case  when V = I  corresponds to the mean squared error of LASSO

for the general Gaussian design and the estimator (cid:98)R(y  X  λ  τ ) is just a special case of the estimator
(cid:98)ΓΩ(y  X  τ  λ  V ). That is  when V = Ω = I  we have(cid:98)ΓI (y  X  τ  λ  I) = (cid:98)R(y  X  λ  τ ).

Now  we state the following analog of Theorem 4.2.
Theorem 4.5. Let {θ0(n)  X(n)  σ2(n)}n∈N be a converging sequence of instances of the general
Gaussian design model with the inverse covariance matrices {Ω(n)}n∈N. Denote the LASSO esti-

mator of θ0(n) by(cid:98)θ(n  λ). If Conjecture 4.4 holds  then  with probability one 
n→∞(cid:98)ΓΩ(y  X (cid:98)τ   λ  I)
where(cid:98)τ = (cid:107)y − X(cid:98)θ(cid:107)2/[n−(cid:107)(cid:98)θ(cid:107)0]. In other words (cid:98)ΓΩ(y  X (cid:98)τ   λ  I) is a consistent estimator of the

n→∞(cid:107)(cid:98)θ − θ0(cid:107)2

2/p(n) = lim

lim

asymptotic MSE of the LASSO.

We will assume that a similar state evolution holds for the general design. In fact  for the general
case  replica method suggests the relation
n→∞(cid:107)Ω− 1

2/p(n) = δ(τ 2 − σ2
0).

2 ((cid:98)θ − θ)(cid:107)2

lim

Hence motivated by the Corollary 4.3  we state the following result on the general Gaussian design
model.
Corollary 4.6. Assume that Conjecture 4.4 holds. In the general Gaussian design model  the vari-
ance of the noise can be accurately estimated by

ˆσ2(n  Ω)/n ≡(cid:98)τ 2 −(cid:98)ΓΩ(y  X (cid:98)τ   λ  Ω− 1

2 )/δ  

where δ = n/p and other variables are deﬁned as in Theorem 4.5. Also  we have

n→∞ ˆσ2/n = σ2
lim
0  

almost surely  providing us a consistent estimator for the noise level in LASSO.

Corollary 4.6  extends the results stated in Corollary 4.3 to the general Gaussian design matrices.
The derivation of formulas in Theorem 4.5 and Corollary 4.6 follows similar arguments as in the
standard Gaussian design model. In particular  they are obtained by applying SURE to the distri-
butional result of Conjecture 4.4 and using the stationary condition of the LASSO. Details of this
derivation can be found in [BEM13].

8

References
[BC13]

[BdG11]

A. Belloni and V. Chernozhukov  Least Squares after Model Selection in High-Dimensional Sparse
Models  Bernoulli (2013).
P. B¨uhlmann and S. Van de Geer  Statistics for high-dimensional data  Springer-Verlag Berlin
Heidelberg  2011.

[BEM13] M. Bayati  M. A. Erdogdu  and A. Montanari  Estimating LASSO Risk and Noise Level  long

version (in preparation)  2013.

[BM12a] M. Bayati and A. Montanari  The dynamics of message passing on dense graphs  with applications

to compressed sensing  IEEE Trans. on Inform. Theory 57 (2012)  764–785.

[BM12b]
[BRT09]

[BS05]

[BT09]

[BY93]

[CD95]

[CRT06]

[CT07]

  The LASSO risk for gaussian matrices  IEEE Trans. on Inform. Theory 58 (2012).

P. Bickel  Y. Ritov  and A. Tsybakov  Simultaneous Analysis of Lasso and Dantzig Selector  The
Annals of Statistics 37 (2009)  1705–1732.
Z. Bai and J. Silverstein  Spectral Analysis of Large Dimensional Random Matrices  Springer 
2005.
A. Beck and M. Teboulle  A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse
Problems  SIAM J. Imaging Sciences 2 (2009)  183–202.
Z. D. Bai and Y. Q. Yin  Limit of the Smallest Eigenvalue of a Large Dimensional Sample Covari-
ance Matrix  The Annals of Probability 21 (1993)  1275–1294.
S.S. Chen and D.L. Donoho  Examples of basis pursuit  Proceedings of Wavelet Applications in
Signal and Image Processing III (San Diego  CA)  1995.
E. C`andes  J. K. Romberg  and T. Tao  Stable signal recovery from incomplete and inaccurate
measurements  Communications on Pure and Applied Mathematics 59 (2006)  1207–1223.
E. C`andes and T. Tao  The Dantzig selector: statistical estimation when p is much larger than n 
Annals of Statistics 35 (2007)  2313–2351.

[DMM09] D. L. Donoho  A. Maleki  and A. Montanari  Message Passing Algorithms for Compressed Sens-

ing  Proceedings of the National Academy of Sciences 106 (2009)  18914–18919.

[DMM11]

[FGH12]

[JM13]

[Joh12]
[MB06]

  The noise-sensitivity phase transition in compressed sensing  Information Theory  IEEE

Transactions on 57 (2011)  no. 10  6920–6941.
J. Fan  S. Guo  and N. Hao  Variance estimation using reﬁtted cross-validation in ultrahigh di-
mensional regression  Journal of the Royal Statistical Society: Series B (Statistical Methodology)
74 (2012)  1467–9868.
A. Javanmard and A. Montanari  Hypothesis testing in high-dimensional regression under the
gaussian random design model: Asymptotic theory  preprint available in arxiv:1301.4240  2013.
I. Johnstone  Gaussian estimation: Sequence and wavelet models  Book draft  2012.
N. Meinshausen and P. B¨uhlmann  High-dimensional graphs and variable selection with the lasso 
The Annals of Statistics 34 (2006)  no. 3  1436–1462.

[NSvdG10] P. B¨uhlmann N. St¨adler and S. van de Geer  (cid:96)1-penalization for Mixture Regression Models (with

discussion)  Test 19 (2010)  209–285.
S. Rangan  A. K. Fletcher  and V. K. Goyal  Asymptotic analysis of map estimation via the replica
method and applications to compressed sensing  2009.

[RFG09]

[Ste81]

[SZ12]
[Tib96]

[SJKG07] M. Lustig S. Boyd S. J. Kim  K. Koh and D. Gorinevsky  An Interior-Point Method for Large-Scale
l1-Regularized Least Squares  IEEE Journal on Selected Topics in Signal Processing 4 (2007) 
606–617.
C. Stein  Estimation of the mean of a multivariate normal distribution  The Annals of Statistics 9
(1981)  1135–1151.
T. Sun and C. H. Zhang  Scaled sparse linear regression  Biometrika (2012)  1–20.
R. Tibshirani  Regression shrinkage and selection with the lasso  J. Royal. Statist. Soc B 58 (1996) 
267–288.
M. J. Wainwright  Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1
constrained quadratic programming  Information Theory  IEEE Transactions on 55 (2009)  no. 5 
2183–2202.
P. Zhao and B. Yu  On model selection consistency of Lasso  The Journal of Machine Learning
Research 7 (2006)  2541–2563.

[Wai09]

[ZY06]

9

,Mohsen Bayati
Murat Erdogdu
Andrea Montanari
Jan Drugowitsch
Ruben Moreno-Bote
Alexandre Pouget
Megasthenis Asteris
Dimitris Papailiopoulos
Anastasios Kyrillidis
Alexandros Dimakis