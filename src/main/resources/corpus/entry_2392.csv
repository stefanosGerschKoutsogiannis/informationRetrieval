2010,Group Sparse Coding with a Laplacian Scale Mixture Prior,We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefficients. Each coefficient is modeled as a Laplacian distribution with a variable scale parameter  with a Gamma distribution prior over the scale parameter. We show that  due to the conjugacy of the Gamma prior  it is possible to derive efficient inference procedures for both the coefficients and the scale parameter. When the scale parameters of a group of coefficients are combined into a single variable  it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients  which have been shown to constitute a large fraction of the redundancy in natural images. We show that  as a consequence of this group sparse coding  the resulting inference of the coefficients follows a divisive normalization rule  and that this may be efficiently implemented a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model.,Group Sparse Coding with a Laplacian Scale Mixture

Prior

Pierre J. Garrigues

IQ Engines  Inc.

Berkeley  CA 94704

pierre.garrigues@gmail.com

Bruno A. Olshausen

Helen Wills Neuroscience Institute

School of Optometry

University of California  Berkeley

Berkeley  CA 94720

baolshausen@berkeley.edu

Abstract

We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture
(LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is mod-
eled as a Laplacian distribution with a variable scale parameter  with a Gamma
distribution prior over the scale parameter. We show that  due to the conjugacy
of the Gamma prior  it is possible to derive efﬁcient inference procedures for both
the coefﬁcients and the scale parameter. When the scale parameters of a group of
coefﬁcients are combined into a single variable  it is possible to describe the de-
pendencies that occur due to common amplitude ﬂuctuations among coefﬁcients 
which have been shown to constitute a large fraction of the redundancy in natu-
ral images [1]. We show that  as a consequence of this group sparse coding  the
resulting inference of the coefﬁcients follows a divisive normalization rule  and
that this may be efﬁciently implemented in a network architecture similar to that
which has been proposed to occur in primary visual cortex. We also demonstrate
improvements in image coding and compressive sensing recovery using the LSM
model.

1

Introduction

The concept of sparsity is widely used in the signal processing  machine learning and statistics
communities for model ﬁtting and solving inverse problems. It is also important in neuroscience as
it is thought to underlie the neural representations used by the brain. The operation to compute the
sparse representation of a signal x ∈ Rn with respect to a dictionary of basis functions Φ ∈ Rn×m
can be implemented via an (cid:96)1-penalized least-square problem commonly referred to as Basis Pursuit
Denoising (BPDN) [2] or Lasso [3]

min

s

1
2

(cid:107)x − Φs(cid:107)2

2 + µ(cid:107)s(cid:107)1 

(1)

where µ is a regularization parameter that controls the tradeoff between the quality of the reconstruc-
tion and the sparsity. This approach has been applied to problems such as image coding  compressive
sensing [4]  or classiﬁcation [5]. The (cid:96)1 penalty leads to solutions where typically a large number
of coefﬁcients are exactly zero  which is a desirable property to achieve model selection or data
compression  or for obtaining interpretable results. The cost function of BPDN is convex  and many
efﬁcient algorithms have been recently developed to solve this problem [6  7  8  9].
Minimizing the cost function of BPDN corresponds to MAP inference in a probabilistic model
2 e−λ|si|. Hence  the
where the coefﬁcients are independent and have Laplacian priors p(si) = λ
signal model assumed by BPDN is linear  generative  and the basis function coefﬁcients are inde-
pendent. In the context of analysis-based models of natural images (for a review on analysis-based

1

and synthesis-based or generative models see [10])  it has been shown that the linear responses of
natural images to Gabor-like ﬁlters have kurtotic histograms  and that there can be strong dependen-
cies among these responses in the form of common amplitude ﬂuctuations [11  12  13  14]. It has
also been observed in the context of generative image models that the inferred sparse coefﬁcients
exhibit pronounced statistical dependencies [15  16]  and therefore the independence assumption is
violated. It has been proposed in block-(cid:96)1 methods to account for dependencies among the coefﬁ-
cients by dividing them into subspaces such that dependencies within the subspaces are allowed  but
not across the subspaces [17] . This approach can produce blocking artifacts and has recently been
generalized to overlapping subspaces in [18]. Another approach is to only allow certain conﬁgura-
tions of active coefﬁcients [19].
We propose in this paper a new class of prior on the basis function coefﬁcients that makes it possible
to model their statistical dependencies in a probabilistic generative model  whose inferred represen-
tations are more sparse than those obtained with the factorial Laplacian prior  and for which we have
efﬁcient inference algorithms. Our approach consists of introducing for each coefﬁcient a hyperprior
on the inverse scale parameter λi of the Laplacian distribution. The coefﬁcient prior is thus a mixture
of Laplacian distributions which we denote “Laplacian Scale Mixture” (LSM)  which is an analogy
to the Gaussian scale mixture (GSM) [12]. Higher-order dependencies of feedforward responses of
wavelet coefﬁcients [12] or basis functions learned using independent component analysis [14] have
been captured using GSMs  and we extend this approach to a generative sparse coding model using
LSMs.
We deﬁne the Laplacian scale mixture in Section 2  and we describe the inference algorithms in the
resulting sparse coding models with an LSM prior on the coefﬁcients in Section 3. We present an
example of a factorial LSM model in Section 4  and of a non-factorial LSM model in Section 5 that
is particularly well suited to signals having the “group sparsity” property. We show that the non-
factorial LSM results in a divisive normalization rule for inferring the coefﬁcients. When the groups
are organized topographically and the basis is trained on natural images  the resulting model resem-
bles the neighborhood divisive normalization that has been hypothesized to occur in visual cortex.
We also demonstrate that the proposed LSM inference algorithm provides superior performance in
image coding and compressive sensing recovery.

2 The Laplacian Scale Mixture distribution
A random variable si is a Laplacian scale mixture if it can be written si = λ−1
i ui  where ui has
2 e−|ui|  and the multiplier variable λi is a
a Laplacian distribution with scale 1  i.e. p(ui) = 1
positive random variable with probability p(λi). We also suppose that λi and ui are independent.
Conditioned on the parameter λi  the coefﬁcient si has a Laplacian distribution with inverse scale λi 
i.e. p(si|λi) = λi
2 e−λi|si|. The distribution over si is therefore a continuous mixture of Laplacian
distributions with different inverse scales  and it can be computed by integrating out λi

(cid:90) ∞

p(si) =

(cid:90) ∞

p(si| λi)p(λi)dλi =

0

0

e−λi|si|p(λi)dλi.

λi
2

Note that for most choices of p(λi) we do not have an analytical expression for p(si). We denote
such a distribution a Laplacian Scale Mixture (LSM). It is a special case of the Gaussian Scale
Mixture (GSM) [12] as the Laplacian distribution can be written as a GSM.

3

Inference in a sparse coding model with LSM prior

We propose the linear generative model

x = Φs + ν =

m(cid:88)

siϕi + ν 

(2)

where x ∈ Rn  Φ = [ϕ1  . . .   ϕm] ∈ Rn×m is an overcomplete transform or basis set  and the
columns ϕi are its basis functions. ν ∼ N (0  σ2In) is small Gaussian noise. The coefﬁcients are
endowed with LSM distributions. They can be used to reconstruct x and are called the synthesis
coefﬁcients.

i=1

2

Given a signal x  we wish to infer its sparse representation s in the dictionary Φ. We consider in this
section the computation of the maximum a posteriori (MAP) estimate of the coefﬁcients s given the
input signal x. Using Bayes’ rule we have p(s | x) ∝ p(x | s)p(s)  and therefore the MAP estimate
ˆs is given by

{− log p(s | x)} = arg min

{− log p(x | s) − log p(s)}.

(3)

ˆs = arg min

s

s

In general it is difﬁcult to compute the MAP estimate with an LSM prior on s since we do not
necessarily have an analytical expression for the log-likelihood log p(s). However  we can compute
the complete log-likelihood log p(s  λ) analytically

log p(s  λ) = log p(s | λ) + log p(λ) = −λi|si| + log

+ log p(λ).

λi
2

Hence  if we also observed the latent variable λ  we would have an objective function that can be
maximized with respect to s. The standard approach in machine learning when confronted with
such a problem is the Expectation-Maximization (EM) algorithm  and we derive in this Section an
EM algorithm for the MAP estimation of the coefﬁcients. We use Jensen’s inequality and obtain the
following upper bound on the posterior likelihood
− log p(s | x) ≤ − log p(x | s) −

dλ := L(q  s) 

q(λ) log

(cid:90)

(4)

p(s  λ)
q(λ)

λ

which is true for any probability distribution q(λ). Performing coordinate descent in the auxiliary
function L(q  s) leads to the following updates that are usually called the E step and the M step.

(5)

(6)

(7)

E Step

M Step

q(t+1) = arg min

q

s(t+1) = arg min

L(q  s(t))
L(q(t+1)  s)

Let < . >q denote the expectation with respect to q(λ). The M Step (6) simpliﬁes to

s(t+1) = arg min

s

1

2σ2(cid:107)x − Φs(cid:107)2

2 +

(cid:104)λi(cid:105)q(t+1) |si| 

s

m(cid:88)

i=1

which is a least-square problem regularized by a weighted sum of the absolute values of the coefﬁ-
cients. It is a quadratic program very similar to BPDN  and we can therefore use efﬁcient algorithms
developed for BPDN that take advantage of the sparsity of the solution. This presents a signiﬁcant
computational advantage over the GSM prior where the inferred coefﬁcients are not exactly sparse.
We have equality in the Jensen inequality if q(λ) = p(λ | s). The inequality (4) is therefore tight for
this particular choice of q  which implies that the E step reduces to q(t+1)(λ) = p(λ | s(t)). Note
that in the M step we only need to compute the expectation of λi with respect to the maximizing
distribution in the E step. Hence we only need to compute the sufﬁcient statistics

(cid:104)λi(cid:105)p(λ|s(t)) =

λi p(λ | s(t))dλ.

(8)
Note that the posterior of the multiplier given the coefﬁcient p(λ | s) might be hard to compute. We
will see in Section 4.1 that it is tractable if the prior on λ is factorial and each λi has a Gamma dis-
tribution  as the Laplacian distribution and the Gamma distribution are conjugate. We can apply the
efﬁcient algorithms developed for BPDN to solve (7). Furthermore  warm-start capable algorithms
are particularly interesting in this context as we can initialize the algorithm with s(t)  and we do not
expect the solution to change much after a few iterations of EM.

λ

(cid:90)

4 Sparse coding with a factorial LSM prior

We propose in this Section a sparse coding model where the distribution of the multipliers is facto-
rial  and each multiplier has a Gamma distribution  i.e. p(λi) = (βα/Γ(α))λα−1
e−βλi  where α is
the shape parameter and β is the inverse scale parameter. With this particular choice of a prior on
the multiplier  we can compute the probability distribution of si analytically:

i

This distribution has heavier tails than the Laplacian distribution. The graphical model correspond-
ing to this generative model is shown in Figure 1.

p(si) =

αβα

2(β + |si|)α+1 .

3

4.1 Conjugacy

The Gamma distribution and Laplacian distribution are conjugate  i.e. the posterior probability of
λi given si is also a Gamma distribution when the prior over λi is a Gamma distribution and the
conditional probability of si given λi is a Laplace distribution with inverse scale λi. Hence  the
posterior of λi given si is a Gamma distribution with parameters α + 1 and β + |si|.
The conjugacy is a key property that we can use in our EM algorithm proposed in Section 3. We saw
that the solution of the E step is given by q(t+1)(λ) = p(λ | s(t)). In the factorial model we have
i ). The solution of the E step is therefore a product of Gamma distributions

p(λ | s) =(cid:81)

i p(λi | s(t)

with parameters α + 1 and β + |s(t)

i

|  and the sufﬁcient statistics (8) are given by
(cid:104)λi(cid:105)p(λi|s(t)

i ) =

α + 1
β + |s(t)

| .

i

(9)

i

i

A coefﬁcient that has a small value after t iterations but is not exactly zero will have in the next
iteration a large reweighting factor λ(t+1)
  which increases the chance that it will be set to zero
in the next iteration  resulting in a sparser representation. On the other hand  a coefﬁcient having
a large value after t iterations corresponds to a feature that is very salient in the signal x.
It is
therefore beneﬁcial to reduce its corresponding inverse scale λ(t+1)
such that it is not penalized and
can account for as much information as possible.
We saw that with the Gamma prior we can compute the distribution of si analytically  and therefore
we can compute the gradient of log p(s | x) with respect to s. Hence another inference algorithm
is to descend the cost function in (3) directly using a method such as conjugate gradient  or the
method proposed in [20] where the authors also exploit the conjugacy of the Laplacian and Gamma
priors. We argue here that the EM algorithm is in fact more efﬁcient. The solution of (7) indeed has
typically few elements that are non-zero  and the computational complexity scales with the number
of non-zero coefﬁcients [6  7]. On the other hand  a gradient-based method will have a harder time
identifying the support of the solution  and therefore the required computations will involve all the
coefﬁcients  which is computationally expensive.
The update formula (9) is coincidentally equivalent to the reweighted L1 minimization scheme pro-
posed by Cand`es et al. [21]. They solve the following sequence of problems

|si| subject to (cid:107)x − Φs(cid:107)2 ≤ δ

λ(t)
i

(10)

m(cid:88)

s(t+1) = arg min

s

i=1

i

= 1/(β + |s(t)

|) (which is identical to our rule when α = 0). The authors show
with update λ(t+1)
that the solutions achieved by their algorithm are more sparse than the solution where λi = 1 for
all i. Whereas they derive this rule from mathematical intuitions regarding the L1 ball  we show
that this update rule follows from from Bayesian inference assuming a Gamma prior over λ. It
was also shown that evidence maximization in a sparse coding model with an automatic relevance
determination prior can also be solved via a sequence of reweighted (cid:96)1 optimization problems [22].

i

4.2 Application to image coding

It has been shown that the convex relaxation consisting of replacing the (cid:96)0 norm with the (cid:96)1 norm is
able to identify the sparsest solution under some conditions on the dictionary of basis functions [23].
However  these conditions are typically not veriﬁed for the dictionaries learned from the statistics
of natural images [24]. For instance  it was observed in [16] that it is possible to infer sparser
representations with a prior over the coefﬁcients that is a mixture of a delta function at zero and a
Gaussian distribution than with the Laplacian prior. We show that our proposed inference algorithm
also leads to representations that are more sparse  as the LSM prior with Gamma hyperprior has
heavier tails than the Laplacian distribution. We selected 1000 16 × 16 image patches at random 
and computed their sparse representations in a dictionary with 256 basis functions using both the
conventional Laplacian prior and our LSM prior. The dictionary is learned from the statistics of
natural images [24] using a Laplacian prior over the coefﬁcients. To ensure that the reconstruction
error is the same in both cases  we solve the constrained version of the problem as in [21]  where we
require that the signal to noise ratio of the reconstruction is equal to 10. We choose β = 0.01 and 5

4

EM iterations. We can see in Figure 2 that the representations using the LSM prior are indeed more
sparse by approximately a factor of 2. Note that the computational complexity to compute these
sparse representations is much lower than that of [16].

Figure 1: Graphical model representation of our
proposed generative model where the multipli-
ers distribution is factorial.

Figure 2: Sparsity comparison. On the x-axis
(resp. y-axis) is the (cid:96)0 norm of the represen-
tation inferred with the Laplacian prior (resp.
LSM prior).

5 Sparse coding with a non-factorial model

It has been shown that many natural signals such as sound or images have a particular type of
higher-order  sparse structure in which active coefﬁcients occur in groups corresponding to basis
functions having similar properties (position  orientation  or frequency tuning) [25  1]. We focus in
this Section on a class of signals that has a particular type of higher-order structure where the active
coefﬁcients occur in groups. We show here that the LSM prior can be used to capture this group
structure in natural images  and we propose an efﬁcient inference algorithm for this case.

5.1 Group sparsity

or neighborhoods indexed by Nk  i.e. {1  . . .   m} = (cid:83)
indices of the nonzero coefﬁcients are given by(cid:83)

We consider a dictionary Φ such that the basis functions can be divided in a set of disjoint groups
k∈Λ Nk  and Ni ∩ Nj = ∅ if i (cid:54)= j. A
signal having the group sparsity property is such that the sparse coefﬁcients occur in groups  i.e. the

k∈Γ Nk  where Γ is a subset of Λ.

The group sparsity structure can be captured with the LSM prior by having all the coefﬁcients in a
group share the same inverse scale parameter  i.e. for all i ∈ Nk  λi = λ(k). The corresponding
graphical model is shown in Figure 3. This addresses the case where dependencies are allowed
within groups  but not across groups as in the block-(cid:96)1 method [17]. Note that for some types of
dictionaries it is more natural to consider overlapping groups to avoid blocking artifacts. We propose
in Section 5.2 inference algorithms for both overlapping and non-overlapping cases.

Figure 3: The two groups N(k) = {i − 2  i −
1  i} and N(l) = {i + 1  i + 2  i + 3} are non-
overlapping.

Figure 4: The basis function coefﬁcients in the
neighborhood deﬁned by N (i) = {i−1  i  i+1}
share the same multiplier λi.

5.2

Inference

In the EM algorithm we proposed in Section 3  the sufﬁcient statistics that are computed in the E
step are (cid:104)λi(cid:105)p(λi|s(t)) for all i. We suppose as in Section 4.1 that the prior on λ(k) is Gamma with

5

s1s2smsjx1xnxiλ1λ2λmλjφij020406080100120140Laplacian prior020406080100120140LSM priorSparsity of the representationsi-1λ(k)si-2sisi+1si+2λ(l)si+3si-1λi-1si-2siλisi+1si+2λi+2si+3λi+1(cid:104)λi(cid:105)p(λi|s(t)) =(cid:10)λ(k)

parameters α and β. Using the structure of the dependencies in the probabilistic model shown in
Figure 3  we have

(11)
where the index i is in the group Nk  and sNk = (sj)j∈Nk is the vector containing all the coefﬁcients
in the group. Using the conjugacy of the Laplacian and Gamma distributions  the distribution of λ(k)
given all the coefﬁcients in the neighborhood is a Gamma distribution with parameters α +|Nk| and
|sj|  where |Nk| denotes the size of the neighborhood. Hence (11) can be rewritten as

β +(cid:80)

p(λ(k)|s(t)Nk

(cid:11)

)

j∈Nk

follows

λ(t+1)
(k) =

β +(cid:80)

α + |Nk|
j∈Nk

j | .
|s(t)

/(β +(cid:80)

The resulting update rule is a form of divisive normalization. We saw in Section 2 that we can write
sk = λ−1
(k)uk  where uk is a Laplacian random variable with scale 1  and thus after convergence we
have u(∞)
k = (α + |Nk|)s(∞)
|). Such rescaling operations are also thought to
j
play an important role in the visual system. [25]
Now let us consider the case where coefﬁcient neighborhoods are allowed to overlap. Let N (i)
denote the indices of the neighborhood that is centered around si (see Figure 4 for an example). We
propose to estimate the scale parameter λi by only considering the coefﬁcients in N (i)  and suppose
that they all share the same multiplier λi. In this case the EM update is given by

|s(∞)

j∈Nk

k

(12)

(13)

λ(t+1)
i

=

β +(cid:80)

α + |N (i)|

j | .
j∈N (i) |s(t)

Note that we have not derived this rule from a proper probabilistic model. A coefﬁcient is indeed a
member of many neighborhoods as shown in Figure 4  and the structure of the dependencies implies
p(λi | s) (cid:54)= p(λi | sN (i)). However  we show experimentally that estimating the multiplier using
(13) gives good performance. A similar approximation is used in the GSM analysis-based model
[26]. Note that the noise shaping algorithm  which bears similarities with the iterative thresholding
algorithm developed for BPDN [7]  is modiﬁed in [27] using an update that is essentially inversely
proportional to ours. The authors show improved coding efﬁciency in the context of natural images.

5.3 Compressive sensing recovery
In compressed sensing  we observe a number n of random projections of a signal s0 ∈ Rm  and
it is in principle impossible to recover s0 if n < m. However  if s0 has p non-zero coefﬁcients  it
has been shown in [28] that it is sufﬁcient to use n ∝ p log m such measurements. We denote by
W ∈ Rn×m the measurement matrix and let y = W s0 be the observations. A standard method to
obtain the reconstruction is to use the solution of the Basis Pursuit (BP) problem

ˆs = arg min

s

(cid:107)s(cid:107)1

subject to W s = y.

(14)

Note that the solution of BP is the solution of BPDN as µ converges to zero in (1)  or δ = 0 in (10).
If the signal has structure beyond sparsity  one can in principle recover the signal with even fewer
measurements using an algorithm that exploits this structure [19  29]. We therefore compare the
performance of BP with the performance of our proposed LSM inference algorithms

|si|

λ(t)
i

subject to W s = y.

(15)

m(cid:88)

s(t+1) = arg min

s

i=1

We denote by RWBP the algorithm with the factorial update (9)  and RW3BP (resp. RW5BP) the
algorithm with our proposed divisive normalization update (13) with group size 3 (resp. 5). We
consider 50-dimensional signals that are sparse in the canonical basis and where the neighborhood
size is 3. To sample such a signal s ∈ R50  we draw a number d of “centroids” i  and we sample three
values for si−1  si and si+1 using a normal distribution of variance 1. The groups are thus allowed
to overlap. A compressive sensing recovery problem is parameterized by (m  n  d). To explore the
problem space we display the results using phase plots as in [30]  which plots performance as a
function of different parameter settings. We ﬁx m = 50 and parameterize the phase plots using
the indeterminacy of the system indexed by δ = n/m  and the approximate sparsity of the system

6

indexed by ρ = 3d/m. We vary δ and ρ in the range [.1  .9] using a 30 by 30 grid. For a given
value (δ  ρ) on the grid  we sample 10 sparse signals using the corresponding (m  n  d) parameters.
The underlying sparse signal is recovered using the three algorithms and we average the recovery
error (cid:107)ˆs − s0(cid:107)2/(cid:107)s0(cid:107)2 for each of them. We show in Figure 5 that RW3BP clearly outperforms
RWBP. There is a slight improvement by going from BP to RWBP (see supplementary material) 
but this improvement is rather small as compared with going from RWBP to RW3BP and RW5BP.
This illustrates the importance of using the higher-order structure of the signals in the inference
algorithm. The performance of RW3BP and RW5BP is comparable (see supplementary material) 
which shows that our algorithm is not very sensitive to the choice of the neighborhood size.

Figure 5: Compressive sensing recovery results using synthetic data. Shown are the phase plots for
a sequence of BP problems with the factorial update (RWBP)  and a sequence of BP problems with
the divisive normalization update with neighborhood size 3 (RW3BP). On the x-axis is the sparsity
of the system indexed by ρ = 3d/m  and on the y-axis is the indeterminacy of the system indexed
by δ = n/m. At each point (ρ  δ) in the phase plot we display the average recovery error.

5.4 Application to natural images

It has been shown that adapting a dictionary of basis functions to the statistics of natural images so
as to maximize sparsity in the coefﬁcients results in a set of dictionary elements whose spatial prop-
erties match those of V1 (primary visual cortex) receptive ﬁelds [24]. However  the basis functions
are learned under a probabilistic model where the probability density over the basis functions coef-
ﬁcients is factorial  whereas the sparse coefﬁcients exhibit statistical dependencies [15  16]. Hence 
a generative model with factorial LSM is not rich enough to capture the complex statistics of natural
images. We propose here to model these dependencies using a non-factorial LSM model. We ﬁx
a topography where the basis functions coefﬁcients are arranged on a 2D grid  and with overlap-
ping neighborhoods of ﬁxed size 3 × 3. The corresponding inference algorithm uses the divisive
normalization update (13).

We learn the optimal dictionary of basis functions Φ using the learning rule ∆Φ = η(cid:10)(x − Φˆs)ˆsT(cid:11)

as in [24]  where η is the learning rate  ˆs are the basis functions coefﬁcients inferred under the model
(13)  and the average is taken over a batch of size 100. We ﬁx n = m = 256  and sample 16 × 16
image patches from a set of whitened images  using a total of 100000 batches. The learned basis
functions are shown in Figure 6. We see here that the neighborhoods of size 3 × 3 group basis
functions at a similar position  scale and orientation. The topography is similar to how neurons are
arranged in the visual cortex  and is reminiscent of the results obtained in topographic ICA [13] and
topographic mixture of experts models [31]. An important difference is that our model is based on a
generative sparse coding model in which both inference and learning can be implemented via local
network interactions [7]. Because of the topographic organization  we also obtain a neighborhood-
based divisive normalization rule.
Does the proposed non-factorial model represent image structure more efﬁciently than those with
factorial priors? To answer this question we measured the models’ ability to recover sparse struc-
ture in the compressed sensing setting. We note that the basis functions are learned such that they
represent the sparse structure in images  as opposed to representing the images exactly (there is a
noise term in the generative model (2)). Hence  we design our experiment such that we measure
the recovery of this sparse structure. Using the basis functions shown in Figure 6  we ﬁrst infer the

7

0.10.20.30.40.50.60.70.80.9ρ0.10.20.30.40.50.60.70.80.9δRWBP0.00.10.20.30.40.50.60.70.80.91.00.10.20.30.40.50.60.70.80.9ρ0.10.20.30.40.50.60.70.80.9δRW3BP0.00.10.20.30.40.50.60.70.80.91.0sparse coefﬁcients s0 of an image patch x such that (cid:107)x − Φs0(cid:107)2 < δ using the inference algorithm
corresponding to the model. We ﬁx δ such that the SNR is 10  and thus the three sparse approxi-
mations for the three models contain the same amount of signal power. We then compute random
projections y = ˜W Φs0 where ˜W is the random measurements matrix. We attempt to recover the
sparse coefﬁcients as in Section 5.3 by substituting W := Φ ˜W   and y := Φs0. We compare the
recovery performance (cid:107)Φˆs− Φs0(cid:107)2/(cid:107)Φs0(cid:107)0 for 100 16× 16 image patches selected at random  and
we use 110 random projections. We can see in Figure 7 that the model with non-factorial LSM prior
outperforms the other models as it is able to capture the group sparsity structure in natural images.

Figure 7: Compressive sensing recovery. On the
x-axis is the recovery performance for the fac-
torial LSM model (RWBP)  and on the y-axis
the recovery performance for the non-factorial
LSM model with 3 × 3 overlapping groups
(RW3×3BP). RW3×3BP outperforms RWBP.
See supplementary material for the comparison
between RW3×3BP and BP as well as between
RWBP and BP.

Figure 6: Basis functions learned in a non-
factorial LSM model with overlapping groups of
size 3 × 3
6 Conclusion

We introduced a new class of probability densities that can be used as a prior for the coefﬁcients in a
generative sparse coding model of images. By exploiting the conjugacy of the Gamma and Laplacian
prior  we were able to derive an efﬁcient inference algorithm that consists of solving a sequence of
reweighted (cid:96)1 least-square problems  thus leveraging the multitude of algorithms already developed
for BPDN. Our framework also makes it possible to capture higher-order dependencies through
group sparsity. When applied to natural images  the learned basis functions of the model may be
topographically organized according to the speciﬁed group structure. We also showed that exploiting
the group sparsity results in performance gains for compressive sensing recovery on natural images.
An open question is the learning of group structure  which is a topic of ongoing work.
We wish to acknowledge support from NSF grant IIS-0705939.
References
[1] S. Lyu and E. P. Simoncelli. Statistical modeling of images with ﬁelds of gaussian scale mixtures. In

Advances in Neural Computation Systems (NIPS)  Vancouver  Canada  2006.

[2] S.S. Chen  D.L. Donoho  and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on

Scientiﬁc Computing  20(1):33–61  1999.

[3] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B  58(1):267–288  1996.

[4] Y. Tsaig and D.L. Donoho. Extensions of compressed sensing. Signal Processing  86(3):549–571  2006.
[5] R. Raina  A. Battle  H. Lee  B. Packer  and A.Y. Ng. Self-taught learning: Transfer learning from unla-

beled data. Proceedings of the Twenty-fourth International Conference on Machine Learning  2007.

8

[6] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics 

32(2):407–499  2004.

[7] C.J. Rozell  D.H Johnson  R.G. Baraniuk  and B.A. Olshausen. Sparse coding via thresholding and local

competition in neural circuits. Neural Computation  20(10):2526–2563  October 2008.

[8] J. Friedman  T. Hastie  H. Hoeﬂing  and R. Tibshirani. Pathwise coordinate optimization. The Annals of

Applied Statistics  1(2):302–332  2007.

[9] M. Figueiredo  R. Nowak  and S. Wright. Gradient projection for sparse reconstruction: Application to
compressed sensing and other inverse problems. IEEE Journal of Selected Topics in Signal Processing 
1(4):586–597  2007.

[10] M. Elad  P. Milanfar  and R. Rubinstein. Analysis vs synthesis in signal priors.

23(3):947–968  June 2007.

Inverse Problems 

[11] C. Zetzsche  G. Krieger  and B. Wegmann. The atoms of vision: Cartesian or polar?

Optical Society of America A  16(7):1554–1565  1999.

Journal of the

[12] M.J. Wainwright  E.P. Simoncelli  and A.S. Willsky. Random cascades on wavelet trees and their use
in modeling and analyzing natural imagery. Applied and Computational Harmonic Analysis  11(1)  July
2001.

[13] A. Hyv¨arinen  P.O. Hoyer  and M. Inki. Topographic independent component analysis. Neural Computa-

tion  13(7):1527–1558  2001.

[14] Y. Karklin and M.S. Lewicki. A hierarchical bayesian model for learning nonlinear statistical regularities

in nonstationary natural signals. Neural Computation  17(2):397–423  February 2005.

[15] P. Hoyer and A. Hyv¨arinen. A multi-layer sparse coding network learns contour coding from natural

images. Vision Research  42:1593–1605  2002.

[16] P.J. Garrigues and B.A. Olshausen. Learning horizontal connections in a sparse coding model of natural

images. In Advances in Neural Computation Systems (NIPS)  Vancouver  Canada  2007.

[17] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  February 2006.

[18] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso.

Conference on Machine Learning (ICML)  2009.

In International

[19] R.G. Baraniuk  V. Cevher  M.F. Duarte  and C. Hegde. Model-based compressive sensing. Preprint 

August 2008.

[20] I. Ramirez  F. Lecumberry  and G. Sapiro. Universal priors for sparse modeling. CAMPSAP  December

2009.

[21] E.J. Cand`es  M.B. Wakin  and S.P. Boyd. Enhancing sparsity by reweighted l1 minimization. J. Fourier

Anal. Appl.  to appear  2008.

[22] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In Advances in Neural

Information Processing Systems 20  2008.

[23] J.A. Tropp.

Just relax: convex programming methods for identifying sparse signals in noise.

Transactions on Information Theory  52(3):1030–1051  2006.

IEEE

[24] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381(6583):607–609  June 1996.

[25] M.J. Wainwright  O. Schwartz  and E.P. Simoncelli. Natural image statistics and divisive normalization:
Modeling nonlinearity and adaptation in cortical neurons. In R. Rao  B.A. Olshausen  and M.S. Lewicki 
editors  Statistical Theories of the Brain. MIT Press  2001.

[26] J. Portilla  V. Strela  M.J Wainwright  and E.P. Simoncelli.

Image denoising using scale mixtures of

gaussians in the wavelet domain. IEEE Transactions on Image Processing  12(11):1338–1351  2003.

[27] R.M. Figueras and E.P. Simoncelli. Statistically driven sparse image representation. In Proc 14th IEEE

Int’l Conf on Image Proc  volume 6  pages 29–32  September 2007.

[28] E. Cand`es. Compressive sampling. Proceedings of the International Congress of Mathematicians  2006.
[29] V. Cevher    M. F. Duarte  C. Hegde  and R. G. Baraniuk. Sparse signal recovery using markov random

ﬁelds. In Advances in Neural Computation Systems (NIPS)  Vancouver  B.C.  Canada  2008.

[30] D. Donoho and Y. Tsaig. Fast solution of l 1-norm minimization problems when the solution may be

sparse. preprint  2006.

[31] S. Osindero  M. Welling  and G.E. Hinton. Topographic product models applied to natural scene statistics.

Neural Computation  18(2):381–414  2006.

9

,Richard Socher
Milind Ganjoo
Christopher Manning
Andrew Ng
Florian Stimberg
Andreas Ruttor
Manfred Opper