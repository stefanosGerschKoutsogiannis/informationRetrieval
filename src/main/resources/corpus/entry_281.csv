2010,A novel family of non-parametric cumulative based divergences for point processes,Hypothesis testing on point processes has several applications such as model fitting  plasticity detection  and non-stationarity detection. Standard tools for hypothesis testing include tests on mean firing rate and time varying rate function. However  these statistics do not fully describe a point process and thus the tests can be misleading. In this paper  we introduce a family of non-parametric divergence measures for hypothesis testing. We extend the traditional Kolmogorov--Smirnov and Cramer--von-Mises tests for point process via stratification. The proposed divergence measures compare the underlying probability structure and  thus  is zero if and only if the point processes are the same. This leads to a more robust test of hypothesis. We prove consistency and show that these measures can be efficiently estimated from data. We demonstrate an application of using the proposed divergence as a cost function to find optimally matched spike trains.,A novel family of non-parametric cumulative based

divergences for point processes

Sohan Seth

University of Florida

Il “Memming” Park

University of Texas at Austin

Austin J. Brockmeier
University of Florida

Mulugeta Semework

SUNY Downstate Medical Center

John Choi  Joseph T. Francis

SUNY Downstate Medical Center & NYU-Poly

Jos´e C. Pr´ıncipe

University of Florida

Abstract

Hypothesis testing on point processes has several applications such as model ﬁt-
ting  plasticity detection  and non-stationarity detection. Standard tools for hy-
pothesis testing include tests on mean ﬁring rate and time varying rate function.
However  these statistics do not fully describe a point process  and therefore  the
conclusions drawn by these tests can be misleading. In this paper  we introduce
a family of non-parametric divergence measures for hypothesis testing. A diver-
gence measure compares the full probability structure and  therefore  leads to a
more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov
and Cram´er–von-Mises tests to the space of spike trains via stratiﬁcation  and
show that these statistics can be consistently estimated from data without any free
parameter. We demonstrate an application of the proposed divergences as a cost
function to ﬁnd optimally matched point processes.

1

Introduction

Neurons communicate mostly through noisy sequences of action potentials  also known as spike
trains. A point process captures the stochastic properties of such sequences of events [1]. Many
neuroscience problems such as model ﬁtting (goodness-of-ﬁt)  plasticity detection  change point
detection  non-stationarity detection  and neural code analysis can be formulated as statistical infer-
ence on point processes [2  3]. To avoid the complication of dealing with spike train observations 
neuroscientists often use summarizing statistics such as mean ﬁring rate to compare two point pro-
cesses. However  this approach implicitly assumes a model for the underlying point process  and
therefore  the choice of the summarizing statistic fundamentally restricts the validity of the inference
procedure.

One alternative to mean ﬁring rate is to use the distance between the inhomogeneous rate functions 

i.e. R |λ1(t) − λ2(t)| dt  as a test statistic  which is sensitive to the temporal ﬂuctuation of the

means of the point processes. In general the rate function does not fully specify a point process 
and therefore  ambiguity occurs when two distinct point processes have the same rate function.
Although physiologically meaningful change is often accompanied by the change in rate  there has
been evidence that the higher order statistics can change without a corresponding change of rate [4 
5]. Therefore  statistical tools that capture higher order statistics  such as divergences  can improve
the state-of-the-art hypothesis testing framework for spike train observations  and may encourage
new scientiﬁc discoveries.

1

In this paper  we present a novel family of divergence measures between two point processes. Un-
like ﬁring rate function based measures  a divergence measure is zero if and only if the two point
processes are identical. Applying a divergence measure for hypothesis testing is  therefore  more
appropriate in a statistical sense. We show that the proposed measures can be estimated from
data without any assumption on the underlying probability structure. However  a distribution-free
(non-parametric) approach often suffers from having free parameters  e.g. choice of kernel in non-
parametric density estimation  and these free parameters often need to be chosen using computa-
tionally expensive methods such as cross validation [6]. We show that the proposed measures can
be consistently estimated in a parameter free manner  making them particularly useful in practice.

One of the difﬁculties of dealing with continuous-time point process is the lack of well structured
space on which the corresponding probability laws can be described. In this paper we follow a rather
unconventional approach for describing the point process by a direct sum of Euclidean spaces of
varying dimensionality  and show that the proposed divergence measures can be expressed in terms
of cumulative distribution functions (CDFs) in these disjoint spaces. To be speciﬁc  we represent
the point process by the probability of having a ﬁnite number of spikes and the probability of spike
times given that number of spikes  and since these time values are reals  we can represent them in
a Euclidean space using a CDF. We follow this particular approach since  ﬁrst  CDFs can be easily
estimated consistently using empirical CDFs without any free parameter  and second  standard tests
on CDFs such as Kolmogorov–Smirnov (K-S) test [7] and Cram´er–von-Mises (C-M) test [8] are
well studied in the literature. Our work extends the conventional K-S test and C-M test on the real
line to the space of spike trains.

The rest of the paper is organized as follows; in section 2 we introduce the measure space where
the point process is deﬁned as probability measures  in section 3 and section 4 we introduce the
extended K-S and C-M divergences  and derive their respective estimators. Here we also prove the
consistency of the proposed estimators. In section 5  we compare various point process statistics in
a hypothesis testing framework. In section 6 we show an application of the proposed measures in
selecting the optimal stimulus parameter. In section 7  we conclude the paper with some relevant
discussion and future work guidelines.

2 Basic point process

We deﬁne a point process to be a probability measure over all possible spike trains. Let Ω be the
set of all ﬁnite spike trains  that is  each ω ∈ Ω can be represented by a ﬁnite set of action potential
timings ω = {t1 ≤ t2 ≤ . . . ≤ tn} ∈ Rn where n is the number of spikes. Let Ω0  Ω1  · · · denote
the partitions of Ω such that Ωn contains all possible spike trains with exactly n events (spikes) 
n=0 Ωn is a disjoint union  and that Ω0 has only one element

representing the empty spike train (no action potential). See Figure 1 for an illustration.

hence Ωn = Rn. Note that Ω = S∞
clidean spaces; F = σ (S∞

Deﬁne a σ-algebra on Ω by the σ-algebra generated by the union of Borel sets deﬁned on the Eu-
n=0 B (Ωn)). Note that any measurable set A ∈ F can be partitioned
n=0  such that each An is measurable in corresponding measurable space
into {An = A ∩ Ωn}∞
(Ωn  B (Ωn)). Here A denotes a collection of spike trains involving varying number of action po-
tentials and corresponding action potential timings  whereas An denotes a subset of these spike
trains involving only n action potentials each.
A (ﬁnite) point process is deﬁned as a probability measure P on the measurable space (Ω  F) [1].
Let P and Q be two probability measures on (Ω  F)  then we are interested in ﬁnding the diver-
gence d(P  Q) between P and Q  where a divergence measure is characterized by d(P  Q) ≥ 0 and
d(P  Q) = 0 ⇐⇒ P = Q.

3 Extended K-S divergence

A Kolmogorov-Smirnov (K-S) type divergence between P and Q can be derived from the L1 dis-
tance between the probability measures  following the equivalent representation 

d1(P  Q) =ZΩ

d |P − Q| ≥ sup
A∈F

|P (A) − Q(A)| .

(1)

2

Inhomogeneous Poisson Firing

0

2

3

4

5

6
8

Figure 1: (Left) Illustration of how the point process space is stratiﬁed. (Right) Example of spike
trains stratiﬁed by their respective spike count.

time

Since (1) is difﬁcult and perhaps impossible to estimate directly without a model  our strategy is to
use the stratiﬁed spaces (Ω0  Ω1  . . .) deﬁned in the previous section  and take the supremum only in
the corresponding conditioned probability measures. Let Fi = F ∩ Ωi := {F ∩ Ωi|F ∈ F}. Since
∪iFi ⊂ F 

d1(P  Q) ≥ Xn∈N

sup
A∈Fn

|P (A) − Q(A)| = Xn∈N

sup
A∈Fn

|P (Ωn)P (A|Ωn) − Q(Ωn)Q(A|Ωn)| .

Since each Ωn is a Euclidean space  we can induce the traditional K-S test statistic by further reduc-
ing the search space to ˜Fn = {×i(−∞  ti]|t = (t1  . . .   tn) ∈ Rn}. This results in the following
inequality 

sup
A∈Fn

|P (A) − Q(A)| ≥ sup
A∈ ˜Fn

|P (A) − Q(A)| = sup

P (t) − F (n)
F (n)

 

(2)

t∈Rn(cid:12)(cid:12)(cid:12)

Q (t)(cid:12)(cid:12)(cid:12)

where F (n)
corresponding to the probability measure P in Ωn. Hence  we deﬁne the K-S divergence as

P (t) = P [T1 ≤ t1 ∧ . . . ∧ Tn ≤ tn] is the cumulative distribution function (CDF)

P (Ωn)F (n)

P (t) − Q(Ωn)F (n)

.

(3)

i=1 and Y = {yj}NQ

j=1 from P and Q respectively  we

ˆP (Ωn) ˆF (n)

P (t) − ˆQ(Ωn) ˆF (n)

ˆP (Ωn) ˆF (n)

P (t) − ˆQ(Ωn) ˆF (n)

(4)

where Xn = X ∩ Ωn  and ˆP and ˆFP are the empirical probability and empirical CDF  respectively.
Notice that we only search the supremum over the locations of the realizations Xn ∪ Yn and not
only changes

P (t) − ˆQ(Ωn) ˆF (n)

ˆP (Ωn) ˆF (n)

dKS(P  Q) = Xn∈N

sup

t∈Rn(cid:12)(cid:12)(cid:12)

Given a ﬁnite number of samples X = {xi}NP
have the following estimator for equation (3).

sup

ˆdKS(P  Q) = Xn∈N
= Xn∈N

t∈Rn(cid:12)(cid:12)(cid:12)
t∈Xn∪Yn(cid:12)(cid:12)(cid:12)
the whole Rn  since the empirical CDF difference(cid:12)(cid:12)(cid:12)

values at those locations.
Theorem 1 (dKS is a divergence).

sup

Q (t)(cid:12)(cid:12)(cid:12)

 

Q (t)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)

d1(P  Q) ≥ dKS(P  Q) ≥ 0
dKS(P  Q) = 0 ⇐⇒ P = Q

(5)
(6)

3

Proof. The ﬁrst property and the ⇐ proof for the second property are trivial. From the deﬁnition
of dKS and properties of CDF  dKS(P  Q) = 0 implies that P (Ωn) = Q(Ωn) and F (n)
P = F (n)
for all n ∈ N. Given probability measures for each (Ωn  Fn) denoted as Pn and Qn  there exist
corresponding unique extended measures P and Q for (Ω  F) such that their restrictions to (Ωn  Fn)
coincide with Pn and Qn  hence P = Q.
Theorem 2 (Consistency of K-S divergence estimator). As the sample size approaches inﬁnity 

Q

(7)

(cid:12)(cid:12)(cid:12)(cid:12)

sup

t∈Rn(cid:12)(cid:12)(cid:12)

Proof. Note that |P sup · −P sup ·| ≤ P |sup · − sup ·|. Due to the triangle inequality of the

supremum norm 

P (Ωn)F (n)

P (t) − Q(Ωn)F (n)

ˆP (Ωn) ˆF (n)

P (t) − ˆQ(Ωn) ˆF (n)

P (Ωn)F (n)

P (t) − Q(Ωn)F (n)

ˆP (Ωn) ˆF (n)

P (t) − ˆQ(Ωn) ˆF (n)

.

Again  using the triangle inequality we can show the following:
ˆP (Ωn) ˆF (n)

P (Ωn)F (n)

P (t) − ˆQ(Ωn) ˆF (n)

a.u.−−→ 0

− sup

(cid:12)(cid:12)(cid:12)
dKS − ˆdKS(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
t∈Rn(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
−(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
−(cid:12)(cid:12)(cid:12)

Q (t) − ˆP (Ωn) ˆF (n)
P (t) − Q(Ωn)F (n)
P (t) + ˆQ(Ωn) ˆF (n)

P (t) + ˆQ(Ωn) ˆF (n)
Q (t) + Q(Ωn) ˆF (n)
Q (t)
Q (t) − Q(Ωn) ˆF (n)

(cid:12)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)

P (Ωn)F (n)

P (Ωn)F (n)

≤ sup

t∈Rn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)
≤P (Ωn)(cid:12)(cid:12)(cid:12)
P (t)(cid:12)(cid:12)(cid:12)

+ ˆF (n)

+P (Ωn) ˆF (n)

P (t) − Q(Ωn)F (n)
P (t) − Q(Ωn)F (n)
P (t) − P (Ωn) ˆF (n)
P (t) − ˆP (Ωn) ˆF (n)
F (n)
P (t) − ˆF (n)

P (t)(cid:12)(cid:12)(cid:12)
P (Ωn) − ˆP (Ωn)(cid:12)(cid:12)(cid:12)

+ Q(Ωn)(cid:12)(cid:12)(cid:12)
Q (t)(cid:12)(cid:12)(cid:12)

+ ˆF (n)

F (n)
Q (t) − ˆF (n)

Q (t)(cid:12)(cid:12)(cid:12)
Q(Ωn) − ˆQ(Ωn)(cid:12)(cid:12)(cid:12)

.

Then the theorem follows from the Glivenko-Cantelli theorem  and ˆP   ˆQ a.s.−−→ P  Q.

Notice that the inequality in (2) can be made stricter by considering the supremum over not just the
product of the segments (−∞  ti] but over the all 2n − 1 possible products of the segments (−∞  ti]
and [ti  ∞) in n dimensions [7]. However  the latter approach is computationally more expensive 
and therefore  in this paper we only explore the former approach.

4 Extended C-M divergence

We can extend equation (3) to derive a Cram´er–von-Mises (C-M) type divergence for point pro-
cesses. Let µ = P + Q/2  then P  Q are absolutely continuous with respect to µ. Note that 
F (n)
P   F (n)
Q ∈ L2(Ωn  µ|n) where |n denotes the restriction on Ωn  i.e. the CDFs are L2 integrable 
since they are bounded. Analogous to the relation between K-S test and C-M test  we would like to
use the integrated squared deviation statistics in place of the maximal deviation statistic. By inte-
grating over the probability measure µ instead of the supremum operation  and using L2 instead of
L∞ distance  we deﬁne

dCM (P  Q) = Xn∈NZRn(cid:16)P (Ωn)F (n)

Q (t)(cid:17)2

P (t) − Q(Ωn)F (n)

dµ|n(t).

(8)

This can be seen as a direct extension of the C-M criterion. The corresponding estimator can be
derived using the strong law of large numbers 

ˆdCM (P  Q) = Xn∈N" 1

2Xi (cid:16) ˆP (Ωn) ˆF (n)

P (x(n)

i

) − ˆQ(Ωn) ˆF (n)

Q (x(n)

i

)(cid:17)2

+

1

2Xi (cid:16) ˆP (Ωn) ˆF (n)

P (y(n)

i

) − ˆQ(Ωn) ˆF (n)

Q (y(n)

i

4

)(cid:17)2# .

(9)

Theorem 3 (dCM is a divergence). For P and Q with square integrable CDFs 

dCM (P  Q) ≥ 0

dCM (P  Q) = 0 ⇐⇒ P = Q.

(10)
(11)

Proof. Similar to theorem 1.

Theorem 4 (Consistency of C-M divergence estimator). As the sample size approaches inﬁnity 

a.u.−−→ 0

(12)

(cid:12)(cid:12)(cid:12)

dCM − ˆdCM(cid:12)(cid:12)(cid:12)

Proof. Similar to (7)  we ﬁnd an upper bound and show that the bound uniformly converges to
zero. To simplify the notation  we deﬁne gn(x) = P (Ωn)F (n)
Q (x)  and ˆgn(x) =
a.u.−−→ g by the Glivenko-Cantelli theorem and
ˆP (Ωn) ˆF (n)
ˆP a.s.−−→ P by the strong law of large numbers.

P (x(n)) − ˆQ(Ωn) ˆF (n)

P (x) − Q(Ωn)F (n)

Q (x(n)). Note that ˆgn

1

=

(cid:12)(cid:12)(cid:12)

ˆgn(yi)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ndQ|n −Xn∈NXi

dCM − ˆdCM(cid:12)(cid:12)(cid:12)

2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xn∈NZ g2
ndP |n +Xn∈NZ g2
ˆgn(xi)2 −Xn∈NXi
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
nd ˆQ|n(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xn∈N(cid:20)Z g2
nd ˆP |n +Z g2
ndQ|n −Z ˆg2
ndP |n −Z ˆg2
≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xn∈N(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)
nd ˆQ|n(cid:12)(cid:12)(cid:12)(cid:12)
nd ˆP |n(cid:12)(cid:12)(cid:12)(cid:12)
+(cid:12)(cid:12)(cid:12)(cid:12)
ndQ|n −Z ˆg2
Z g2
ndP |n −Z ˆg2
Z g2
where ˆP = Pi δ(xi) and ˆQ = Pi δ(yi) are the corresponding empirical measures. Without loss
of generality  we only ﬁnd the bound on(cid:12)(cid:12)(cid:12)R g2
nd ˆP |n(cid:12)(cid:12)(cid:12)
ndP |n −R ˆg2
nd ˆP |n(cid:12)(cid:12)(cid:12)(cid:12)
ndP |n −Z ˆg2
ndP |n +Z ˆg2
ndP |n −Z ˆg2
Z g2
(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:1) dP |n(cid:12)(cid:12)(cid:12)(cid:12)
−(cid:12)(cid:12)(cid:12)(cid:12)
nd(cid:16)P |n − ˆP |n(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
Z (cid:0)g2
Z ˆg2

Applying Glivenko-Cantelli theorem and strong law of large numbers  these two terms converges
since ˆg2

n is bounded. Hence  we show that the C-M test estimator is consistent.

Z g2

ndP |n −Z ˆg2

  then the rest is bounded similarly

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

nd ˆP |n(cid:12)(cid:12)(cid:12)(cid:12)

=(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)(cid:12)

for Q.

(cid:12)(cid:12)(cid:12)(cid:12)

n − ˆg2

5 Results

We present a set of two-sample problems and apply various statistics to perform hypothesis test-
ing. As a baseline measure  we consider the widely used Wilcoxon rank-sum test (or equiva-
lently  the Mann-Whitney U test) on the count distribution (e.g. [9])  which is a non-parametric
median test for the total number of action potentials  and the integrated squared deviation statistic

λL2 = R (λ1(t) − λ2(t))2 dt  where λ(t) is estimated by smoothing spike timing with a Gaussian

kernel  evaluated at a uniform grid at least an order of magnitude smaller than the standard deviation
of the kernel. We report the performance of the test with varying kernel sizes.

All tests are quantiﬁed by the power of the test given a signiﬁcance threshold (type-I error) at 0.05.
The null hypothesis distribution is empirically computed by either generating independent samples
or by permuting the data to create at least 1000 values.

5.1 Stationary renewal processes

Renewal process is a widely used point process model that compensates the deviation from Poisson
process [10]. We consider two stationary renewal processes with gamma interval distributions. Since
the mean rate of the two processes are the same  the rate function statistic and Wilcoxon test does

5

20

15

10

5

0

0

20

15

10

5

0

0

H
0

r
e
w
o
p

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.2

0.4

0.6

0.8

1

H
1

0.2

0.4

0.6

0.8

1

time (sec)

 

82 111 150

K−S
C−M
λ
L2
λ
L2
λ
L2
N

 10 ms
 100 ms
 1 ms

 
10

14 18

33

25
61
Number of samples

45

Figure 2: Gamma distributed renewal process with shape parameter θ = 3 (H0) and θ = 0.5 (H1).
The mean number of action potential is ﬁxed to 10. (Left) Spike trains from the null and alternate
hypothesis. (Right) Comparison of the power of each method. The error bars are standard deviation
over 20 Monte Carlo runs.

not yield consistent result  while the proposed measures obtain high power with a small number of
samples. The C-M test is more powerful than K-S in this case; this can be interpreted by the fact
that the difference in the cumulative is not concentrated but spread out over time because of the
stationarity.

5.2 Precisely timed spike trains

When the same stimulation is presented to a neuronal system  the observed spike trains some-
times show a highly repeatable spatio-temporal pattern at the millisecond time scale. Recently
these precisely timed spike trains (PTST) are abundantly reported both in vivo and in vitro prepa-
rations [11  12  13]. Despite being highly reproducible  different forms of trial-to-trial variability
have also been observed [14]. It is crucial to understand this variability since for a system to utilize
PTSTs as a temporal code  it should presumably be robust to its variability structure  and possibly
learn to reduce it [15].

A precisely timed spike train in an interval is modeled by L number of probability density and
i=1. Each fi(t) corresponds to the temporal jitter  and pi corresponds
probability pairs {(fi(t)  pi)}L
to the probability of generating the spike. Each realization of the PTST model produces at most

L spikes. The equi-intensity Poisson process has the rate function λ(t) = Pi pifi(t). We test if

the methods can differentiate between the PTST (H0) and equi-intensity Poisson process (H1) for
L = 1  2  3  4 (see Figure 3 for the L = 4 case). Note that L determines the maximum dimension for
the PTST. fi(t) were equal variance Gaussian distribution on a grid sampled from a uniform random
variable  and pi = 0.9.
As shown in Figure 3  only the proposed methods perform well. Since the rate function proﬁle is
identical for both models  the rate function statistic λL2 fails to differentiate. The Wilcoxon test does
work for intermediate dimensions  however its performance is highly variable and unpredictable. In
contrast to the previous example  the K-S test is consistently better than the C-M statistic in this
problem.

6 Optimal stimulation parameter selection

Given a set of point processes  we can ﬁnd the one which is closest to a target point process in terms
of the proposed divergence. Here we use this method on a real dataset obtained from the somatosen-
sory system of an anesthetized rat (see supplement for procedure). Speciﬁcally  we address ﬁnding

6

H
0

0.2

0.6
0.4
time (ms)

0.8

1

 

r
e
w
o
p

d
CM
d
CM
d
CM
d
CM
d
KS
d
KS
d
KS
d
KS

 L=1
 L=2
 L=3
 L=4
 L=1
 L=2
 L=3
 L=4

20

15

10

5

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

20

15

10

5

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

r
e
w
o
p

0

 

19

37
136
Number of samples

71

261

500

0

 

19

H
1

0.2

0.6
0.4
time (ms)

0.8

1

 

N L=1
N L=2
N L=3
N L=4

261

500

37

71

136
number of samples

Figure 3: [Top] Precisely timed spike train model (H0) versus equi-intensity Poisson process (H1).
Spike trains from the null and alternate hypothesis for L = 4. [Bottom] Comparison of the power
of each method for L = 1  2  3  4 on precisely timed spike train model (H0) versus equi-intensity
Poisson process (H1). (Left) Power comparison for methods except for N. The rate statistic λL2 are
not labeled  since they are not able to detect the difference. (Right) Wilcoxon test on the number of
action potentials. The error bars are standard deviation over 10 Monte Carlo runs.

optimal electrical stimulation settings to produce cortical spiking patterns similar to those observed
with tactile stimuli.

The target process has 240 realizations elicited by tactile stimulation of the ventral side of the ﬁrst
digit with a mechanical tactor. We seek the closest out of 19 processes elicited by electrical stim-
ulation in the thalamus. Each process has 140 realizations that correspond to a particular setting
of electrical stimulation. The settings correspond to combinations of duration and amplitude for
biphasic current injection on two adjacent channels in the thalamus. The channel of interest and the
stimulating channels were chosen to have signiﬁcant response to tactile stimulation.
The results from applying the C-M  K-S  and λL2 measures between the tactile responses and the sets
from each electrical stimulation setting are shown Figure 4. The overall trend among the measures
is consistent  but the location of the minima does not coincide for λL2.

7 Conclusion

In this paper  we have proposed two novel measures of divergence between point processes. The
proposed measures have been derived from the basic probability law of a point process and we have
shown that these measures can be efﬁciently estimated consistently from data. Using divergences for
statistical inference transcends ﬁrst and second order statistics  and enables distribution-free spike
train analysis.

The time complexity of both methods is O(cid:0)Pn n(cid:2)NP (n)NQ(n) + N 2

NP (n) is the number of spike trains from P that has n spikes. In practice this is often faster than

Q(n)(cid:3)(cid:1) where

P (n) + N 2

7

Tactile

#15 (100uA 125µs)

#17 (100uA 175µs)

1

0.8

0.6

0.4

0.2

0

K−S
C−M

λ

L2

t

n
u
o
c
 
y
b

 

d
e

t
r
o
s
 
s
a
i
r
T

l

i

e
k
p
s
 
t
s
1

 

n
e
h

t

n
b

i

i

 
r
e
p
 
s
e
k
p
s
 
e
g
a
r
e
v
A

0.4

0.3

0.2

0.1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

0

0.02

0.04

0

Parameter index (sorted by duration then amplitude)

0.02

Time (s)

0.04

0

0.02

0.04

Figure 4: (Left) Dissimilarity/divergences from tactile response across parameter sets. The values
of each measure are shifted and scaled to be in the range of 0 to 1. λL2 uses 2.5 ms bins with
no smoothing. (Right) Responses from the tactile response (left)  stimulation settings selected by
λL2 (center)  and the realizations selected by K-S and C-M (right). Top row shows the spike trains
stratiﬁed into number of spikes and then sorted by spike times. Bottom row shows the average
response binned at 2.5 ms; the variance is shown as a thin green line.

the binned rate function estimation which has time complexity O(BN ) where B is the number of

bins and N = Pn n(NP (n) + NQ(n)) is the total number of spikes in all the samples. Although 

we have observed that the statistic based on the L2 distance between the rate functions often outper-
forms the proposed method  this approach involves the search for the smoothing kernel size and bin
size which can make the process slow and prohibitive. In addition  it brings the danger of multiple
testing  since some smoothing kernel sizes may pickup spurious patterns that are only ﬂuctuations
due to ﬁnite samples size.

A similar approach based on stratiﬁcation has also been addressed in [16]  where the authors have
discussed the problem of estimating Hellinger distance between two point processes. Although
conceptually similar  the advantage of the proposed approach is that it is parameter free  whereas the
other approach requires selecting appropriate kernels and the corresponding kernel sizes for each
Euclidean partitions. However  a stratiﬁcation-based approach suffers in estimation when the count
distributions of the point processes under consideration are ﬂat  since in this situation the spike
train realizations tend to exist in separate Euclidean partitions  and given a ﬁnite set of realizations 
it becomes difﬁcult to populate each partition sufﬁciently. Therefore  other methods should be
investigated that allow two spike trains to interact irrespective of their spike counts. Other possible
approaches include the kernel-based divergence measures as proposed in [17]  since the measures
can be applied to any abstract space. However  it requires desinging an appropriate strictly positive
deﬁnite kernel on the space of spike trains.

In this literature  we have presented the divergences in the context of spike trains generated by
neurons. However  the proposed methods can be used for general point processes  and can be
applied to other areas. Although we have proved consistency of the proposed measures  further
statistical analysis such as small sample power analysis  rate of convergence  and asymptotic prop-
erties would be interesting to address. A MATLAB implementation is freely available on the web
(http://code.google.com/p/iocane) with BSD-license.

Acknowledgment

This work is partially funded by NSF Grant ECCS-0856441 and DARPA Contract N66001-10-C-
2008.

8

References

[1] D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes. Springer 

1988.

[2] D. H. Johnson  C. M. Gruner  K. Baggerly  and C. Seshagiri. Information-theoretic analysis of

neural coding. Journal of Computational Neuroscience  10(1):47–69  2001.

[3] J. D. Victor. Spike train metrics. Current Opinion in Neurobiology  15:585–592  2005.
[4] A. Kuhn  A. Aertsen  and S. Rotter. Higher-order statistics of input ensembles and the response

of simple model neurons. Neural Computation  15(1):67–101  2003.

[5] F. Rieke  D. Warland  R. de Ruyter van Steveninck  and W. Bialek. Spikes: exploring the

neural code. MIT Press  Cambridge  MA  USA  1999.

[6] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall 

New York  1986.

[7] G. Fasano and A. Franceschini. A multidimensional version of the Kolmogorov–Smirnov test.

Royal Astronomical Society  Monthly Notices  225:155–170  1987.

[8] T. W. Anderson. On the distribution of the two-sample Cram´er–von-Mises criterion. Annals

of Mathematical Statistics  33(3):1148–1159  1962.

[9] A. Kepecs  N. Uchida  H. A. Zariwala  and Z. F. Mainen. Neural correlates  computation and

behavioural impact of decision conﬁdence. Nature  455(7210):227–231  2008.

[10] M. P. P. Nawrot  C. Boucsein  V. R. Molina  A. Riehle  A. Aertsen  and S. Rotter. Measurement
of variability dynamics in cortical spike trains. Journal of Neuroscience Methods  169(2):374–
390  2008.

[11] P. Reinagel and R. Clay Reid. Precise ﬁring events are conserved across neurons. Journal of

Neuroscience  22(16):6837–6841  2002.

[12] M. R. DeWeese  M. Wehr  and A. M. Zador. Binary spiking in auditory cortex. Journal of

Neuroscience  23(21):7940–7949  2003.

[13] R. S. Johansson and I. Birznieks. First spikes in ensembles of human tactile afferents code

complex spatial ﬁngertip events. Nature Neuroscience  7(2):170–177  2004.

[14] P. Tiesinga  J. M. Fellous  and T. J. Sejnowski. Regulation of spike timing in visual cortical

circuits. Nature Reviews Neuroscience  9:97–107  2008.

[15] S. M. Bohte and M. C. Mozer. Reducing the variability of neural responses: A computational

theory of spike-timing-dependent plasticity. Neural Computation  19(2):371–403  2007.

[16] I. Park and J. C. Pr´ıncipe. Quantiﬁcation of inter-trial non-stationarity in spike trains from
periodically stimulated neural cultures. In Proceedings of IEEE International Conference on
Acoustics  Speech and Signal Processing  2010. Special session on Multivariate Analysis of
Brain Signals: Methods and Applications.

[17] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Sch¨olkopf  and A. J. Smola. A kernel method

for the two-sample problem. CoRR  abs/0805.2368  2008.

9

,Adams Wei Yu
Wanli Ma
Yaoliang Yu
Jaime Carbonell
Suvrit Sra
Jack Serrino
Max Kleiman-Weiner
David Parkes
Josh Tenenbaum