2015,Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze–like Environments,Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations  often after a single trial. While the mechanism for rapid path planning is unknown  the CA3 region in the hippocampus plays an important role  and emerging evidence suggests that place cell activity during hippocampal preplay periods may trace out future goal-directed trajectories. Here  we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment  based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model  resulting in bump--like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers  while inhibiting other neurons with distant place field centers  such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment  then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed  in networks with large place fields  we show that the network properties cause the bump to move smoothly from its initial location to the goal  around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.,Attractor Network Dynamics Enable Preplay and
Rapid Path Planning in Maze–like Environments

Dane Corneil

Wulfram Gerstner

Laboratory of Computational Neuroscience
´Ecole Polytechnique F´ed´erale de Lausanne

Laboratory of Computational Neuroscience
´Ecole Polytechnique F´ed´erale de Lausanne

CH-1015 Lausanne  Switzerland
dane.corneil@epfl.ch

CH-1015 Lausanne  Switzerland

wulfram.gerstner@epfl.ch

Abstract

Rodents navigating in a well–known environment can rapidly learn and revisit ob-
served reward locations  often after a single trial. While the mechanism for rapid
path planning is unknown  the CA3 region in the hippocampus plays an important
role  and emerging evidence suggests that place cell activity during hippocam-
pal “preplay” periods may trace out future goal–directed trajectories. Here  we
show how a particular mapping of space allows for the immediate generation of
trajectories between arbitrary start and goal locations in an environment  based
only on the mapped representation of the goal. We show that this representation
can be implemented in a neural attractor network model  resulting in bump–like
activity proﬁles resembling those of the CA3 region of hippocampus. Neurons
tend to locally excite neurons with similar place ﬁeld centers  while inhibiting
other neurons with distant place ﬁeld centers  such that stable bumps of activity
can form at arbitrary locations in the environment. The network is initialized to
represent a point in the environment  then weakly stimulated with an input cor-
responding to an arbitrary goal location. We show that the resulting activity can
be interpreted as a gradient ascent on the value function induced by a reward at
the goal location. Indeed  in networks with large place ﬁelds  we show that the
network properties cause the bump to move smoothly from its initial location to
the goal  around obstacles or walls. Our results illustrate that an attractor network
with hippocampal–like attributes may be important for rapid path planning.

1

Introduction

While early human case studies revealed the importance of the hippocampus in episodic memory [1 
2]  the discovery of “place cells” in rats [3] established its role for spatial representation. Recent
results have further suggested that  along with these functions  the hippocampus is involved in active
spatial planning: experiments in “one–shot learning” have revealed the critical role of the CA3
region [4  5] and the intermediate hippocampus [6] in returning to goal locations that the animal has
seen only once. This poses the question of whether and how hippocampal dynamics could support
a representation of the current location  a representation of a goal  and the relation between the two.
In this article  we propose that a model of CA3 as a “bump attractor” [7] can be be used for path
planning. The attractor map represents not only locations within the environment  but also the spatial
relationship between locations. In particular  broad activity proﬁles (like those found in intermediate
and ventral hippocampus [8]) can be viewed as a condensed map of a particular environment. The
planned path presents as rapid sequential activity from the current position to the goal location 
similar to the “preplay” observed experimentally in hippocampal activity during navigation tasks [9 
10]  including paths that require navigating around obstacles. In the model  the activity is produced
by supplying input to the network consistent with the sensory input that would be provided at the

1

goal site. Unlike other recent models of rapid goal learning and path planning [11  12]  there is
no backwards diffusion of a value signal from the goal to the current state during the learning or
planning process. Instead  the sequential activity results from the representation of space in the
attractor network  even in the presence of obstacles.
The recurrent structure in our model is derived from the “successor representation” [13]  which
represents space according to the number and length of paths connecting different locations. The
resulting network can be interpreted as an attractor manifold in a low–dimensional space  where the
dimensions correspond to weighted version of the most relevant eigenvectors of the environment’s
transition matrix. Such low–frequency functions have recently found support as a viable basis for
place cell activity [14–16]. We show that  when the attractor network operates in this basis and is
stimulated with a goal location  the network activity traces out a path to that goal. Thus  the bump
attractor network can act as a spatial path planning system as well as a spatial memory system.

2 The successor representation and path–ﬁnding

A key problem in reinforcement learning is assessing the value of a particular state  given the ex-
pected returns from that state in both the immediate and distant future. Several model–free algo-
rithms exist for solving this task [17]  but they are slow to adjust when the reward landscape is
rapidly changing. The successor representation  proposed by Dayan [13]  addresses this issue.
Given a Markov chain described by the transition matrix P  where each element P (s  s(cid:48)) gives the
probability of transitioning from state s to state s(cid:48) in a single time step; a reward vector r  where
each element r(s(cid:48)) gives the expected immediate returns from state s(cid:48); and a discount factor γ  the
expected returns v from each state can be described by

v = r + γPr + γ2P2r + γ3P3r + . . .

= (I − γP)−1r
= Lr.

(1)

The successor representation L provides an efﬁcient means of representing the state space according
to the expected (discounted) future occupancy of each state s(cid:48)  given that the chain is initialized from
state s. An agent employing a policy described by the matrix P can immediately update the value
function when the reward landscape r changes  without any further exploration.
The successor representation is particularly useful for representing many reward landscapes in the
same state space. Here we consider the set of reward functions where returns are conﬁned to a single
state s(cid:48); i.e. r(s(cid:48)) = δs(cid:48)g where δ denotes the Kronecker delta function and the index g denotes a
particular goal state. From Eq. 1  we see that the value function is then given by the column s(cid:48)
of the matrix L. Indeed  when we consider only a single goal  we can see the elements of L as
L(s  s(cid:48)) = v(s|s(cid:48) = g). We will use this property to generate a spatial mapping that allows for a
rapid approximation of the shortest path between any two points in an environment.

2.1 Representing space using the successor representation

In the spatial navigation problems considered here  we assume that the animal has explored the en-
vironment sufﬁciently to learn its natural topology. We represent the relationship between locations
with a Gaussian afﬁnity metric a: given states s(x  y) and s(cid:48)(x  y) in the 2D plane  their afﬁnity is

a(s(x  y)  s(cid:48)(x  y)) = a(s(cid:48)(x  y)  s(x  y)) = exp

(2)

where d is the length of the shortest traversable path between s and s(cid:48)  respecting walls and obstacles.
We deﬁne σ to be small enough that the metric is localized (Fig. 1) such that a(s(x  y) ·) resembles
a small bump in space  truncated by walls. Normalizing the afﬁnity metric gives

(cid:18)−d2

(cid:19)

2σ2
s

p(s  s(cid:48)) =

a(s  s(cid:48))
s(cid:48) a(s  s(cid:48))

(cid:80)

2

.

(3)

The normalized metric can be interpreted as a transition probability for an agent exploring the envi-
ronment randomly. In this case  a spectral analysis of the successor representation [14  18] gives

v(s|s(cid:48) = g) = π(s(cid:48))

(1 − γλl)−1ψl(s)ψl(s(cid:48))

(4)

where ψl are the right eigenvectors of the transition matrix P  1 = |λ0| ≥ |λ1| ≥ |λ2|··· ≥
|λn| are the eigenvalues [18]  and π(s(cid:48)) denotes the steady–state occupancy of state s(cid:48) resulting
from P. Although the afﬁnity metric is deﬁned locally  large–scale features of the environment are
represented in the eigenvectors associated with the largest eigenvalues (Fig. 1).
We now express the position in the 2D space using a set of “successor coordinates”  such that

n(cid:88)

l=0

(cid:113)

s(x  y) (cid:55)→ ˘s =

(1 − γλ0)

−1ψ0(s) 

(1 − γλ1)

−1ψ1(s)  . . .  

(1 − γλq)

−1ψq(s)

(cid:113)

(cid:19)

(5)

(cid:18)(cid:113)

(cid:113)

= (ξ0(s)  ξ1(s)  . . .   ξq(s))

(1 − γλl)

−1ψl. This is similar to the “diffusion map” framework by Coifman and
where ξl =
Lafon [18]; with the useful property that  if q = n  the value of a given state when considering
a given goal is proportional to the scalar product of their respective mappings: v(s|s(cid:48) = g) =
π(s(cid:48))(cid:104)˘s  ˘s(cid:48)(cid:105). We will use this property to show how a network operating in the successor coordinate
space can rapidly generate prospective trajectories between arbitrary locations.
Note that the mapping can also be deﬁned using the eigenvectors φl of a related measure of the
space  the normalized graph Laplacian [19]. The eigenvectors φl serve as the objective functions for
slow feature analysis [20]  and approximations have been extracted through hierarchical slow feature
analysis on visual data [15  16]  where they have been used to generate place cell–like behaviour.

2.2 Path–ﬁnding using the successor coordinate mapping

Successor coordinates provide a means of mapping a set of locations in a 2D environment to a new
space based on the topology of the environment. In the new representation  the value landscape
is particularly simple. To move from a location ˘s towards a goal position ˘s(cid:48)  we can consider a
constrained gradient ascent procedure on the value landscape:

(cid:104)
(cid:104)

(˘s − (˘st + α∇v(˘st)))2(cid:105)
(˘s − (˘st + ˜α˘s(cid:48)))2(cid:105)

˘st+1 = arg min

˘s∈ ˘S

= arg min

˘s∈ ˘S

(6)

where π(s(cid:48)) has been absorbed into the parameter ˜α. At each time step  the state closest to an
incremental ascent of the value gradient is selected amongst all states in the environment ˘S. In the
following  we will consider how the step ˘st + ˜α˘s(cid:48) can be approximated by a neural attractor network
acting in successor coordinate space.
Due to the properties of the transition matrix  ψ0 is constant across the state space and does not
contribute to the value gradient in Eq. 6. As such  we substituted a free parameter for the coefﬁcient

(cid:112)(1 − γλ0)−1  which controlled the overall level of activity in the network simulations.

3 Encoding successor coordinates in an attractor network

The bump attractor network is a common model of place cell activity in the hippocampus [7  21].
Neurons in the attractor network strongly excite other neurons with similar place ﬁeld centers  and
weakly inhibit the neurons within the network with distant place ﬁeld centers. As a result  the
network allows a stable bump of activity to form at an arbitrary location within the environment.

3

[Left] A rat explores a maze–like environment and passively learns its topology. We as-
Figure 1:
sume a process such as hierarchical slow feature analysis  that preliminarily extracts slowly changing
functions in the environment (here  the vectors ξ1 . . . ξq). The vector ξ1 for the maze is shown in
the top left. In practice  we extracted the vectors directly from a localized Gaussian transition func-
tion (bottom center  for an arbitrary location). [Right] This basis can be used to generate a value
map approximation over the environment for a given reward (goal) position and discount factor γ
(inset). Due to the walls  the function is highly discontinuous in the xy spatial dimensions. The
goal position is circled in white. In the scatter plot  the same array of states and value function are
shown in the ﬁrst two non–trivial successor coordinate dimensions. In this space  the value function
is proportional to the scalar product between the states and the goal location. The grey and black
dots show corresponding states between the inset and the scatter plot.

Such networks typically represent a periodic (toroidal) environment [7  21]  using a local excitatory
weight proﬁle that falls off exponentially. Here  we show how the spatial mapping of Eq. 5 can be
used to represent bounded environments with arbitrary obstacles. The resulting recurrent weights
induce stable ﬁring ﬁelds that decrease with distance from the place ﬁeld center  around walls and
obstacles  in a manner consistent with experimental observations [22].
In addition  the network
dynamics can be used to perform rapid path planning in the environment.
We will use the techniques introduced in the attractor network models by Eliasmith and Anderson
[23] to generalize the bump attractor. We ﬁrst consider a purely feed–forward network  composed of
a population of neurons with place ﬁeld centers scattered randomly throughout the environment. We
assume that the input is highly preprocessed  potentially by several layers of neuronal processing
(Fig. 1)  and given directly by units k whose activities ˘sin
k (t) = ξk(sin(t)) represent the input in the
successor coordinate dimensions introduced above. The activity ai of neuron i in response to the m
inputs ˘sin

k (t) can be described by

(cid:34) m(cid:88)

(cid:35)

τ

dai(t)

dt

= −ai(t) + g

wf f

ik ˘sin

k (t)

(7)

k=1

+

where g is a gain factor  [·]+ represents a rectiﬁed linear function  and wf f
ik are the feed–forward
weights. Each neuron is particularly responsive to a “bump” in the environment given by its encod-
ing vector ei = ˘si
||˘si||  the normalized successor coordinates of a particular point in space  which
corresponds to its place ﬁeld center. The input to neuron i in the network is then given by

m(cid:88)

k=1

wf f
ik = [ei]k 
k (t) = ei · ˘sin(t).

wf f

ik ˘sin

(8)

A neuron is therefore maximally active when the input coordinates are nearly parallel to its encoding
vector. Although we assume the input is given directly in the basis vectors ξl for convenience  a
neural encoding using an (over)complete basis based on a linear combination of the eigenvectors ψl
or φl is also possible given a corresponding transformation in the feed–forward weights.

4

3020100-10-20-30-40-40-30-20-100102030-50Figure 2: [Left] The attractor network structure for the maze–like environment in Fig. 1. The inputs
give a low–dimensional approximation of the successor coordinates of a point in space. The network
is composed of 500 neurons with encoding vectors representing states scattered randomly through-
out the environment. Each neuron’s activation is proportional to the scalar product of its encoding
vector and the input  resulting in a large “bump” of activity. Recurrent weights are generated using a
least–squares error decoding of the successor coordinates from the neural activities  projected back
on to the neural encoding vectors. [Right] The generated recurrent weights for the network. The
plot shows the incoming weights from each neuron to the unit at the circled position  where neurons
are plotted according to their place ﬁeld centers.

n(cid:88)

If the input ˘sin(t) represents a location in the environment  a bump of activity forms in the network
(Fig. 2). These activities give a (non–linear) encoding of the input. Given the response properties of
the neurons  we can ﬁnd a set of linear decoding weights dj that recovers an approximation of the
input given to the network from the neural activities [23]:

˘srec(t) =

dj · aj(t).

(9)

j=1

These decoding weights dj were derived by minimizing the least–squares estimation error of a set
of example inputs from their resulting steady–state activities  where the example inputs correspond
to the successor coordinates of points evenly spaced throughout the environment. The minimization
can be performed by taking the Moore–Penrose pseudoinverse of the matrix of neural activities in
response to the example inputs (with singular values below a certain tolerance removed to avoid
overﬁtting). The vector dj therefore gives the contribution of aj(t) to a linear population code for
the input location.
We now introduce the recurrent weights wrec
to allow the network to maintain a memory of past
ij
input in persistent activity. The recurrent weights are determined by projecting the decoded location
back on to the neuron encoding vectors such that

ij = (1 − ) · ei · dj 
wrec

ij aj(t) = (1 − ) · ei · ˘srec(t).
wrec

n(cid:88)

j=1

(10)

Here  the factor  (cid:28) 1 determines the timescale on which the network activity fades. Since the
encoding and decoding vectors for the same neuron tend to be similar  recurrent weights are highest
between neurons representing similar successor coordinates  and the weight proﬁle decreases with
the distance between place ﬁeld centers (Fig. 2). The full neuron–level description is given by

τ

dai(t)

dt

 n(cid:88)


= −ai(t) + g(cid:2)ei ·(cid:0)(1 − ) · ˘srec(t) + α · ˘sin(t)(cid:1)(cid:3)

= −ai(t) + g

ij aj(t) + α

wf f

ik ˘sin

k (t)

wrec

m(cid:88)

k=1

+

+

j=1

5

(11)

where the α parameter corresponds to the input strength. If we consider the estimate of ˘srec(t)
recovered from decoding the activities of the network  we arrive at the update equation

τ

d˘srec(t)

dt

≈ α · ˘sin(t) −  · ˘srec(t).

(12)

Given a location ˘sin(t) as an initial input  the recovered representation ˘srec(t) approximates the
input and reinforces it  allowing a persistent bump of activity to form. When ˘sin(t) then changes
to a new (goal) location  the input and recovered coordinates conﬂict. By Eq. 12  the recovered
location moves in the direction of the new input  giving us an approximation of the initial gradient
ascent step in Eq. 6 with the addition of a decay controlled by . As we will show  the attractor
dynamics typically cause the network activity to manifest as a movement of the bump towards the
goal location  through locations intermediate to the starting position and the goal (as observed in
experiments [9  10]). After a short stimulation period  the network activity can be decoded to give a
state nearby the starting position that is closer to the goal. Note that  with no decay   the network
activity will tend to grow over time. To induce stable activity when the network representation
matches the goal position (˘srec(t) ≈ ˘sin(t))  we balanced the decay and input strength ( = α).
In the following  we consider networks where the successor coordinate representation was truncated
to the ﬁrst q dimensions  where q (cid:28) n. This was done because the network is composed of a limited
number of neurons  representing only the portion of the successor coordinate space corresponding
to actual locations in the environment. In a very high–dimensional space  the network can rapidly
move into a regime far from any actual locations  and the integration accuracy suffers. In effect  the
weight proﬁles and feed–forward activation proﬁle become very narrow  and as a result the bump
of activity simply disappears from the original position and reappears at the goal. Conversely  low–
dimensional representations tend to result in broad excitatory weight proﬁles and activity proﬁles
(Fig. 2). The high degree of excitatory overlap across the network causes the activity proﬁle to move
smoothly between distant points  as we will show.

4 Results

We generated attractor networks according to the layout of multiple environments containing walls
and obstacles  and stimulated them successively with arbitrary startpoints and goals. We used
n = 500 neurons to represent each environment  with place ﬁeld centers selected randomly through-
out the environment. The successor coordinates were generated using γ = 1. We adjusted q to
control the dimensionality of the representation. The network activity resembles a bump across a
portion of the environment (Fig. 3). Low–dimensional representations (low q) produced large activ-
ity bumps across signiﬁcant portions of the environment; when a weak stimulus was provided at the
goal  the overall activity decreased while the center of the bump moved towards the goal through
the intervening areas of the environment. With a high–dimensional representation  activity bumps
became more localized  and shifted discontinuously to the goal (Fig. 3  bottom row).
For several networks representing different environments  we initialized the activity at points evenly
spaced throughout the environment and provided weak feed–forward stimulation corresponding to
a ﬁxed goal location (Fig. 4). After a short delay (5τ)  we decoded the successor coordinates from
the network activity to determine the closest state (Eq. 6). The shifts in the network representation
are shown by the arrows in Fig. 4. For two networks  we show the effect of different feed–forward
stimuli representing different goal locations. The movement of the activity proﬁle was similar to the
shortest path towards the goal (Fig. 4  bottom left)  including reversals at equidistant points (center
bottom of the maze). Irregularities were still present  however  particularly near the edges of the
environment and in the immediate vicinity of the goal (where high–frequency components play a
larger role in determining the value gradient).

5 Discussion

We have presented a spatial bump attractor model generalized to represent environments with arbi-
trary obstacles  and shown how  with large activity proﬁles relative to the size of the environment  the
network dynamics can be used for path–ﬁnding. This provides a possible correlate for goal–directed

6

Figure 3: Attractor network activities illustrated over time for different inputs and networks  in
multiples of the membrane time constant τ. Purple boxes indicate the most active unit at each point
in time. [First row] Activities are shown for a network representing a maze–like environment in
a low–dimensional space (q = 5). The network was initially stimulated with a bump of activa-
tion representing the successor coordinates of the state at the black circle; recurrent connections
maintain a similar yet fading proﬁle over time. [Second row] For the same network and initial con-
ditions  a weak constant stimulus was provided representing the successor coordinates at the grey
circle; the activities transiently decrease and the center of the proﬁle shifts over time through the
environment. [Third row] Two positions (black and grey circles) were sequentially activated in a
network representing a second environment in a low–dimensional space (q = 4). [Bottom row] For
a higher–dimensional representation (q = 50)  the activity proﬁle fades rapidly and reappears at the
stimulated position.

activity observed in the hippocampus [9  10] and an hypothesis for the role that the hippocampus
and the CA3 region play in rapid goal–directed navigation [4–6]  as a complement to an additional
(e.g. model–free) system enabling incremental goal learning in unfamiliar environments [4].
Recent theoretical work has linked the bump–like ﬁring behaviour of place cells to an encoding of the
environment based on its natural topology  including obstacles [22]  and speciﬁcally to the successor
representation [14]. As well  recent work has proposed that place cell behaviour can be learned by
processing visual data using hierarchical slow feature analysis [15  16]  a process which can extract
the lowest frequency eigenvectors of the graph Laplacian generated by the environment [20] and
therefore provide a potential input for successor representation–based activity. We provide the ﬁrst
link between these theoretical analyses and attractor–based models of CA3.
Slow feature analysis has been proposed as a natural outcome of a plasticity rule based on Spike–
Timing–Dependent Plasticity (STDP) [24]  albeit on the timescale of a standard postsynaptic po-

7

0.04.09.013.018.00.04.09.013.018.00.02.04.06.08.00.02.755.58.2511.0Figure 4: Large–scale  low–dimensional attractor network activities can be decoded to determine
local trajectories to long–distance goals. Arrows show the initial change in the location of the
activity proﬁle by determining the state closest to the decoded network activity (at t = 5τ) after
weakly stimulating with the successor coordinates at the black dot (α =  = 0.05). Pixels show the
place ﬁeld centers of the 500 neurons representing each environment  coloured according to their
activity at the stimulated goal site. [Top left] Change in position of the activity proﬁle in a maze–
like environment with low–dimensional activity (q = 5) compared to [Bottom left] the true shortest
path towards the goal at each point in the environment. [Additional plots] Various environments
and stimulated goal sites using low–dimensional successor coordinate representations.

tential rather than the behavioural timescale we consider here. However  STDP can be extended to
behavioural timescales when combined with sustained ﬁring and slowly decaying potentials [25] of
the type observed on the single–neuron level in the input pathway to CA3 [26]  or as a result of
network effects. Within the attractor network  learning could potentially be addressed by a rule that
trains recurrent synapses to reproduce feed–forward inputs during exploration (e.g. [27]).
Our model assigns a key role to neurons with large place ﬁelds in generating long–distance goal–
directed trajectories. This suggests that such trajectories in dorsal hippocampus (where place ﬁelds
are much smaller [8]) must be inherited from dynamics in ventral or intermediate hippocampus.
The model predicts that ablating the intermediate/ventral hippocampus [6] will result in a signiﬁcant
reduction in goal–directed preplay activity in the remaining dorsal region. In an intact hippocampus 
the model predicts that long–distance goal–directed preplay in the dorsal hippocampus is preceded
by preplay tracing a similar path in intermediate hippocampus. However  these large–scale networks
lack the speciﬁcity to consistently generate useful trajectories in the immediate vicinity of the goal.
Therefore  higher–dimensional (dorsal) representations may prove useful in generating trajectories
close to the goal location  or alternative methods of navigation may become more important.
If an assembly of neurons projecting to the attractor network is active while the animal searches the
environment  reward–modulated Hebbian plasticity provides a potential mechanism for reactivating
a goal location. In particular  the presence of a reward–induced neuromodulator could allow for
potentiation between the assembly and the attractor network neurons active when the animal receives
a reward at a particular location. Activating the assembly would then provide stimulation to the goal
location in the network; the same mechanism could allow an arbitrary number of assemblies to
become selective for different goal locations in the same environment. Unlike traditional model–
free methods of learning which generate a static value map  this would give a highly conﬁgurable
means of navigating the environment (e.g. visiting different goal locations based on thirst vs. hunger
needs)  providing a link between spatial navigation and higher cognitive functioning.
Acknowledgements
This research was supported by the Swiss National Science Foundation (grant agreement no. 200020 147200).
We thank Laureline Logiaco and Johanni Brea for valuable discussions.

8

References
[1] William Beecher Scoville and Brenda Milner. Loss of recent memory after bilateral hippocampal lesions. Journal of

neurology  neurosurgery  and psychiatry  20(1):11  1957.

[2] Howard Eichenbaum. Memory  amnesia  and the hippocampal system. MIT press  1993.
[3] John O’Keefe and Jonathan Dostrovsky. The hippocampus as a spatial map. preliminary evidence from unit activity in

the freely-moving rat. Brain research  34(1):171–175  1971.

[4] Kazu Nakazawa  Linus D Sun  Michael C Quirk  Laure Rondi-Reig  Matthew A Wilson  and Susumu Tonegawa.
Hippocampal CA3 NMDA receptors are crucial for memory acquisition of one-time experience. Neuron  38(2):305–
315  2003.

[5] Toshiaki Nakashiba  Jennie Z Young  Thomas J McHugh  Derek L Buhl  and Susumu Tonegawa. Transgenic inhibition

of synaptic transmission reveals role of ca3 output in hippocampal learning. Science  319(5867):1260–1264  2008.

[6] Tobias Bast  Iain A Wilson  Menno P Witter  and Richard GM Morris. From rapid place learning to behavioral perfor-

mance: a key role for the intermediate hippocampus. PLoS biology  7(4):e1000089  2009.

[7] Alexei Samsonovich and Bruce L McNaughton. Path integration and cognitive mapping in a continuous attractor neural

network model. The Journal of Neuroscience  17(15):5900–5920  1997.

[8] Kirsten Brun Kjelstrup  Trygve Solstad  Vegard Heimly Brun  Torkel Hafting  Stefan Leutgeb  Menno P Witter  Edvard I
Moser  and May-Britt Moser. Finite scale of spatial representation in the hippocampus. Science  321(5885):140–143 
2008.

[9] Brad E Pfeiffer and David J Foster. Hippocampal place-cell sequences depict future paths to remembered goals. Nature 

497(7447):74–79  2013.

[10] Andrew M Wikenheiser and A David Redish. Hippocampal theta sequences reﬂect current goals. Nature neuroscience 

2015.

[11] Louis-Emmanuel Martinet  Denis Sheynikhovich  Karim Benchenane  and Angelo Arleo. Spatial learning and action

planning in a prefrontal cortical network model. PLoS computational biology  7(5):e1002045  2011.

[12] Filip Ponulak and John J Hopﬁeld. Rapid  parallel path planning by propagating wavefronts of spiking neural activity.

Frontiers in computational neuroscience  7  2013.

[13] Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Com-

putation  5(4):613–624  1993.

[14] Kimberly L Stachenfeld  Matthew Botvinick  and Samuel J Gershman. Design principles of the hippocampal cognitive
map. In Z. Ghahramani  M. Welling  C. Cortes  N.D. Lawrence  and K.Q. Weinberger  editors  Advances in Neural
Information Processing Systems 27  pages 2528–2536. Curran Associates  Inc.  2014.

[15] Mathias Franzius  Henning Sprekeler  and Laurenz Wiskott. Slowness and sparseness lead to place  head-direction  and

spatial-view cells. PLoS Computational Biology  3(8):e166  2007.

[16] Fabian Schoenfeld and Laurenz Wiskott. Modeling place ﬁeld activity with hierarchical slow feature analysis. Frontiers

in Computational Neuroscience  9:51  2015.

[17] Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press  1998.
[18] Ronald R Coifman and St´ephane Lafon. Diffusion maps. Applied and computational harmonic analysis  21(1):5–30 

2006.

[19] Sridhar Mahadevan. Learning Representation and Control in Markov Decision Processes  volume 3. Now Publishers

Inc  2009.

[20] Henning Sprekeler. On the relation of slow feature analysis and laplacian eigenmaps. Neural computation  23(12):

3287–3302  2011.

[21] John Conklin and Chris Eliasmith. A controlled attractor network model of path integration in the rat. Journal of

computational neuroscience  18(2):183–203  2005.

[22] Nicholas J Gustafson and Nathaniel D Daw. Grid cells  place cells  and geodesic generalization for spatial reinforcement

learning. PLoS computational biology  7(10):e1002235  2011.

[23] Chris Eliasmith and C Charles H Anderson. Neural engineering: Computation  representation  and dynamics in neu-

robiological systems. MIT Press  2004.

[24] Henning Sprekeler  Christian Michaelis  and Laurenz Wiskott. Slowness: an objective for spike-timing-dependent

plasticity. PLoS Comput Biol  3(6):e112  2007.

[25] Patrick J Drew and LF Abbott. Extending the effects of spike-timing-dependent plasticity to behavioral timescales.

Proceedings of the National Academy of Sciences  103(23):8876–8881  2006.

[26] Phillip Larimer and Ben W Strowbridge. Representing information in cell assemblies: persistent activity mediated by

semilunar granule cells. Nature neuroscience  13(2):213–222  2010.

[27] Robert Urbanczik and Walter Senn. Learning by the dendritic prediction of somatic spiking. Neuron  81(3):521–528 

2014.

9

,Dane Corneil
Wulfram Gerstner
Aaron Defazio