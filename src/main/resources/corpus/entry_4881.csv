2018,Scalable Robust Matrix Factorization with Nonconvex Loss,Robust matrix factorization (RMF)  which uses the $\ell_1$-loss  often outperforms standard matrix factorization using the $\ell_2$-loss  particularly when outliers are present. The state-of-the-art RMF solver is the RMF-MM algorithm  which  however  cannot utilize data sparsity. Moreover  sometimes even the (convex) $\ell_1$-loss is not robust enough. In this paper  we propose the use of nonconvex loss to enhance robustness. To address the resultant difficult optimization problem  we use majorization-minimization (MM) optimization and propose a new MM surrogate. To improve scalability  we exploit data sparsity and optimize the surrogate via its dual with the accelerated proximal gradient algorithm. The resultant algorithm has low time and space complexities and is guaranteed to converge to a critical point. Extensive experiments demonstrate its superiority over the state-of-the-art in terms of both accuracy and scalability.,Scalable Robust Matrix Factorization with

Nonconvex Loss

Quanming Yao1 2  James T. Kwok2

14Paradigm Inc. Beijing  China

yaoquanming@4paradigm.com  jamesk@cse.ust.hk

2Department of Computer Science and Engineering 

Hong Kong University of Science and Technology  Hong Kong

Abstract

Matrix factorization (MF)  which uses the (cid:96)2-loss  and robust matrix factorization
(RMF)  which uses the (cid:96)1-loss  are sometimes not robust enough for outliers.
Moreover  even the state-of-the-art RMF solver (RMF-MM) is slow and cannot
utilize data sparsity. In this paper  we propose to improve robustness by using
nonconvex loss functions. The resultant optimization problem is difﬁcult. To
improve efﬁciency and scalability  we use majorization-minimization (MM) and
optimize the MM surrogate by using the accelerated proximal gradient algorithm
on its dual problem. Data sparsity can also be exploited. The resultant algorithm
has low time and space complexities  and is guaranteed to converge to a critical
point. Extensive experiments show that it outperforms the state-of-the-art in terms
of both accuracy and speed.

1

Introduction

Matrix factorization (MF) is a fundamental tool in machine learning  and an important component
in many applications such as computer vision [1  38]  social networks [37] and recommender
systems [30]. The square loss has been commonly used in MF [8  30]. This implicitly assumes
the Gaussian noise  and is sensitive to outliers. Eriksson and van den Hengel [12] proposed robust
matrix factorization (RMF)  which uses the (cid:96)1-loss  and obtains much better empirical performance.
However  the resultant nonconvex nonsmooth optimization problem is much more difﬁcult.
Most RMF solvers are not scalable [6  12  22  27  40]. The current state-of-the-art solver is RMF-MM
[26]  which is based on majorization minimization (MM) [20  24]. In each iteration  a convex
nonsmooth surrogate is optimized. RMF-MM is advantageous in that it has theoretical convergence
guarantees  and demonstrates fast empirical convergence [26]. However  it cannot utilize data sparsity.
This is problematic in applications such as structure from motion [23] and recommender system [30] 
where the data matrices  though large  are often sparse.
Though the (cid:96)1-loss used in RMF is more robust than the (cid:96)2  still it may not be robust enough for
outliers. Recently  better empirical performance is obtained in total-variation image denosing by
using the (cid:96)0-loss instead [35]  and in sparse coding the capped-(cid:96)1 loss [21]. A similar observation
is also made on the (cid:96)1-regularizer in sparse learning and low-rank matrix learning [16  38  41]. To
alleviate this problem  various nonconvex regularizers have been introduced. Examples include the
Geman penalty [14]  Laplace penalty [34]  log-sum penalty (LSP) [9] minimax concave penalty
(MCP) [39]  and the smooth-capped-absolute-deviation (SCAD) penalty [13]. These regularizers are
similar in shape to Tukey’s biweight function in robust statistics [19]  which ﬂattens for large values.
Empirically  they achieve much better performance than (cid:96)1.
In this paper  we propose to improve the robustness of RMF by using these nonconvex functions
(instead of (cid:96)1 or (cid:96)2) as the loss function. The resultant optimization problem is difﬁcult  and existing

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

RMF solvers cannot be used. As in RMF-MM  we rely on the more ﬂexible MM optimization
technique  and a new MM surrogate is proposed. To improve scalability  we transform the surrogate
to its dual and then solve it with the accelerated proximal gradient (APG) algorithm [2  32]. Data
sparsity can also be exploited in the design of the APG algorithm. As for its convergence analysis 
proof techniques in RMF-MM cannot be used as the loss is no longer convex. Instead  we develop
new proof techniques based on the Clarke subdifferential [10]  and show that convergence to a
critical point can be guaranteed. Extensive experiments on both synthetic and real-world data sets
demonstrate superiority of the proposed algorithm over the state-of-the-art in terms of both accuracy
and scalability.
Notation. For scalar x  sign (x) = 1 if x > 0  0 if x = 0  and −1 otherwise. For a vector x 
ij)1/2
i j |Xij| is its (cid:96)1-norm  and nnz(X) is the number of nonzero
i Xii is its trace. For two matrices X  Y   X (cid:12) Y
denotes their element-wise product. For a smooth function f  ∇f is its gradient. For a convex f 
G ∈ ∂f (X) = {U : f (Y ) ≥ f (X) + tr(U(cid:62)(Y − X))} is a subgradient.

Diag(x) constructs a diagonal matrix X with Xii = xi. For a matrix X  (cid:107)X(cid:107)F = ((cid:80)
is its Frobenius norm  (cid:107)X(cid:107)1 = (cid:80)
elements in X. For a square matrix X  tr(X) =(cid:80)

i j X 2

2 Related Work

2.1 Majorization Minimization
Majorization minimization (MM) is a general technique to make difﬁcult optimization problems
easier [20  24]. Consider a function h(X) which is hard to optimize. Let the iterate at the kth MM
iteration be X k. The next iterate is generated as X k+1 = X k + arg minX f k(X; X k)  where f k is a
surrogate that is being optimized instead of h. A good surrogate should have the following properties
[24]: (i) h(X k + X) ≤ f k(X; X k) for any X; (ii) 0 = arg minX
h(X k) = f k(0; X k); and (iii) f k is convex on X. MM only guarantees that the objectives obtained
in successive iterations are non-increasing  but does not guarantee convergence of X k [20  24].

(cid:0)f k(X; X k) − h(X k + X)(cid:1) and

2.2 Robust Matrix Factorization (RMF)
In matrix factorization (MF)  the data matrix M ∈ Rm×n is approximated by U V (cid:62)  where U ∈
Rm×r  V ∈ Rn×r and r (cid:28) min(m  n) is the rank. In applications such as structure from motion
(SfM) [1] and recommender systems [30]  some entries of M may be missing. In general  the
MF problem can be formulated as: minU V
F )  where
W ∈ {0  1}m×n contains indices to the observed entries in M (with Wij = 1 if Mij is observed 
and 0 otherwise)  and λ ≥ 0 is a regularization parameter. The (cid:96)2-loss is sensitive to outliers. In [11] 
it is replaced by the (cid:96)1-loss  leading to robust matrix factorization (RMF):
F + (cid:107)V (cid:107)2
F ).

2(cid:107)W (cid:12) (M − U V (cid:62))(cid:107)2

(cid:107)W (cid:12) (M − U V (cid:62))(cid:107)1 +

F + (cid:107)V (cid:107)2

2 ((cid:107)U(cid:107)2

((cid:107)U(cid:107)2

F + λ

(1)

1

min
U V

λ
2

Many RMF solvers have been developed [6  7  12  18  22  26  27  40]. However  as the objective
in (1) is neither convex nor smooth  these solvers lack scalability  robustness and/or convergence
guarantees. Interested readers are referred to Section 2 of [26] for details.
Recently  the RMF-MM algorithm [26] solves (1) using MM. Let the kth iterate be (U k  V k).
RMF-MM tries to ﬁnd increments ( ¯U   ¯V ) that should be added to obtain the target (U  V ):

U = U k + ¯U  

V = V k + ¯V .

2(cid:107)U k + ¯U(cid:107)2

(2)
Substituting into (1)  the objective can be rewritten as H k( ¯U   ¯V ) ≡ (cid:107)W (cid:12) (M −(U k + ¯U )(V k +
¯V )(cid:62))(cid:107)1 + λ
F . The following Proposition constructs a surrogate F k of
H k that satisﬁes properties (i) and (ii) in Section 2.1. Unlike H k  F k is jointly convex in ( ¯U   ¯V ).
Proposition 2.1. [26] Let nnz(W(i :)) (resp. nnz(W(: j))) be the number of nonzero elements in
√
the ith row (resp. jth column) of W   Λr = Diag(
nnz(W(m :)))  and Λc =
Diag(

nnz(W(: n))). Then  H k( ¯U   ¯V ) ≤ F k( ¯U   ¯V )  where

2(cid:107)V k + ¯V (cid:107)2

nnz(W(1 :))  . . .  

nnz(W(: 1))  . . .  

F + λ

√

√

√

F k( ¯U   ¯V ) ≡(cid:107)W (cid:12)(M − U k(V k)(cid:62) − ¯U (V k)(cid:62) − U k ¯V (cid:62))(cid:107)1
(cid:107)V k + ¯V (cid:107)2

(cid:107)U k + ¯U(cid:107)2

(cid:107)Λr ¯U(cid:107)2

+

F +

F +

λ
2

λ
2

F +

(cid:107)Λc ¯V (cid:107)2
F .

1
2

(3)

1
2

2

Equality holds iff ( ¯U   ¯V ) = (0  0).
Because of the coupling of ¯U   V k (resp. U k  ¯V ) in ¯U (V k)(cid:62) (resp. U k ¯V (cid:62)) in (3)  F k is still difﬁcult
to optimize. To address this problem  RMF-MM uses the LADMPSAP algorithm [25]  which is a
multi-block variant of the alternating direction method of multipliers (ADMM) [3].
RMF-MM has a space complexity of O(mn)  and a time complexity of O(mnrIK)  where I is the
number of (inner) LADMPSAP iterations and K is the number of (outer) RMF-MM iterations. These
grow linearly with the matrix size  and can be expensive on large data sets. Besides  as discussed in
Section 1  the (cid:96)1-loss may still be sensitive to outliers.

3 Proposed Algorithm
3.1 Use a More Robust Nonconvex Loss
In this paper  we improve robustness of RMF by using a general nonconvex loss instead of the (cid:96)1-loss.
Problem (1) is then changed to:

˙H(U  V ) ≡ m(cid:88)

n(cid:88)

Wijφ(cid:0)|Mij − [U V (cid:62)]ij|(cid:1) +

min
U V

i=1

j=1

((cid:107)U(cid:107)2

F + (cid:107)V (cid:107)2
F ) 

λ
2

(4)

where φ is nonconvex. We assume the following on φ:
Assumption 1. φ(α) is concave  smooth and strictly increasing on α ≥ 0.

Assumption 1 is satisﬁed by many nonconvex functions  including the Geman  Laplace and LSP
penalties mentioned in Section 1  and slightly modiﬁed variants of the MCP and SCAD penalties.
Details can be found in Appendix A. Unlike previous papers [16  38  41]  we use these nonconvex
functions as the loss  not as regularizer. The (cid:96)1 also satisﬁes Assumption 1  and thus (4) includes (1).
When the ith row of W is zero  the ith row of U obtained is zero because of the (cid:107)U(cid:107)2
F regularizer.
Similarly  when the ith column of W is zero  the corresponding column in V is zero. To avoid this
trivial solution  we assume the following  as in matrix completion [8] and RMF-MM.
Assumption 2. W has no zero row or column.

3.2 Constructing the Surrogate
Problem (4) is difﬁcult to solve  and existing RMF solvers cannot be used as they rely crucially on
the (cid:96)1-norm. In this Section  we use the more ﬂexible MM technique as in RMF-MM. However  its
surrogate construction scheme cannot be used here. RMF-MM uses the convex (cid:96)1 loss  and only
needs to handle nonconvexity resulting from the product U V (cid:62) in (1). Here  nonconvexity in (4)
comes from both from the loss and U V (cid:62).
The following Proposition ﬁrst obtains a convex upper bound of the nonconvex φ using Taylor
expansion. An illustration is shown in Figure 1. Note that this upper bound is simply a re-weighted
(cid:96)1  with scaling factor φ(cid:48)(|β|) and offset φ(|β|) − φ(cid:48)(|β|)|β|. As one may expect  recovery of the (cid:96)1
makes optimization easier. It is known that the LSP  when used as a regularizer  can be interpreted as
re-weighted (cid:96)1 regularization [8]. Thus  Proposition 3.1 includes this as a special case.
Proposition 3.1. For any given β ∈ R  φ(|α|) ≤ φ(cid:48)(|β|)|α| + (φ(|β|)− φ(cid:48)(|β|)|β|)  and the equality
holds iff α = ±β.

(a) Geman.

(b) Laplace.

(c) LSP.

(d) modiﬁed MCP.

(e) modiﬁed SCAD.

Figure 1: Upper bounds for the various nonconvex penalties (see Table 5 in Appendix A.2) β = 1 
θ = 2.5 for SCAD and θ = 0.5 for the others; and δ = 0.05 for MCP and SCAD.

3

i=1

i=1

2(cid:107)V k + ¯V (cid:107)2

˙H k( ¯U   ¯V ) ≤ bk + λ

˙H in (4) can be
2(cid:107)U k + ¯U(cid:107)2
F +

˙H k( ¯U   ¯V ) ≡(cid:80)m

F + (cid:107) ˙W k (cid:12) (M − U k(V k)(cid:62) −
ij|[U k(V k)(cid:62)]ij|) 

F . Using Proposition 3.1  we obtain the following convex upper bound for ˙H k.

¯U (V k)(cid:62)−U k ¯V (cid:62)− ¯U ¯V (cid:62))(cid:107)1  where bk =(cid:80)m

Given the current iterate (U k  V k)  we want to ﬁnd increments ( ¯U   ¯V ) as in (2).
rewritten as:
2(cid:107)V k + ¯V (cid:107)2
λ
Corollary 3.2.

(cid:80)n
j=1 Wijφ(|Mij − [(U k+ ¯U )(V k + ¯V )(cid:62)]ij|) + λ
(cid:80)n
2(cid:107)U k + ¯U(cid:107)2
F + λ
j=1 Wij(φ(|[U k(V k)(cid:62)]ij|)−Ak
ij = φ(cid:48)(|[U k(V k)(cid:62)]ij|).
(cid:112)

˙W k = Ak (cid:12) W   and Ak
The product ¯U ¯V (cid:62) still couples ¯U and ¯V together. As ˙H k is similar to H k in Section 2.2  one
may want to reuse Proposition 2.1. However  Proposition 2.1 holds only when W is a binary
matrix  while ˙W k here is real-valued. Let Λk
(m :))) and
(: n))). The following Proposition shows that ˙F k( ¯U   ¯V ) ≡
c = Diag(
Λk
(cid:107) ˙W k (cid:12) (M − U k(V k)(cid:62) − ¯U (V k)(cid:62) − U k ¯V (cid:62))(cid:107)1 + λ
F + λ
F +
2(cid:107)Λk
¯V (cid:107)2
F +bk  can be used as a surrogate. Moreover  it can be easily seen that ˙F k qualiﬁes as a good
surrogate in Section 2.1: (a) ˙H( ¯U +U k  ¯V +V k) ≤ ˙F k( ¯U   ¯V ); (b) (0  0) = arg min ¯U   ¯V
˙F k( ¯U   ¯V )−
˙H k( ¯U   ¯V ) and ˙F k(0  0) = ˙H(0  0); and (c) ˙F k is jointly convex in ¯U   ¯V .
Proposition 3.3.
Remark 3.1. In the special case where the (cid:96)1-loss is used 
c = Λc.
The surrogate ˙F k( ¯U   ¯V ) then reduces to that in (3)  and Proposition 3.3 becomes Proposition 2.1.

˙H k( ¯U   ¯V ) ≤ ˙F k( ¯U   ¯V )  with equality holds iff ( ¯U   ¯V ) = (0  0).

(1 :))  . . .  
2(cid:107)Λk
¯U(cid:107)2

˙W k = W   bk = 0 Λk

2(cid:107)V k + ¯V (cid:107)2

2(cid:107)U k + ¯U(cid:107)2

F + 1

sum( ˙W k

(: 1))  . . .  

r = Λr  and Λk

sum( ˙W k

r = Diag(

sum( ˙W k

(cid:112)

(cid:112)

sum( ˙W k

(cid:112)

1

c

r

3.3 Optimizing the Surrogate via APG on the Dual
LADMPSAP  which is used in RMF-MM  can also be used to optimize ˙F k. However  the dual
variable in LADMPSAP is a dense matrix  and cannot utilize possible sparsity of W . Moreover 
LADMPSAP converges at a rate of O(1/T ) [25]  which is slow. In the following  we propose a time-
and space-efﬁcient optimization procedure based on running the accelerated proximal gradient (APG)
algorithm on the surrogate optimization problem’s dual. Note that while the primal problem has
O(mn) variables  the dual problem has only nnz(W ) variables.
Problem Reformulation. Let Ω ≡ {(i1  j1)  . . .   (innz(W )  jnnz(W ))} be the set containing indices
of the observed elements in W   HΩ(·) be the linear operator which maps a nnz(W )-dimensional
vector x to the sparse matrix X ∈ Rm×n with nonzero positions indicated by Ω (i.e.  Xitjt = xt
where (it  jt) is the tth element in Ω)  and H−1
Proposition 3.4. The dual problem of min ¯U   ¯V

Ω (·) be the inverse operator of HΩ.

˙F k( ¯U   ¯V ) is

r (HΩ(x)V k − λU k)) − tr(HΩ(x)(cid:62)M )

min
x∈W k

Dk(x) ≡ 1
2

tr((HΩ(x)V k − λU k)(cid:62)Ak
tr((HΩ(x)(cid:62)U k − λV k)(cid:62)Ak
1
2

+

(5)
r )2)−1  and
c )2)−1. From the obtained x  the primal ( ¯U   ¯V ) solution can be recovered as

where W k ≡ {x ∈ Rnnz(W ) : |xi| ≤ [ ˙wk]−1
i } 
Ak
c = (λI + (Λk
¯U = Ak

r (HΩ(x)V k − λU k) and ¯V = Ak

˙wk = H−1
c (HΩ(x)(cid:62)U k − λV k).

r = (λI + (Λk

Ω ( ˙W k)  Ak

c (HΩ(x)(cid:62)U k − λV k)) 

Problem (5) can be solved by the APG algorithm  which has a convergence rate of O(1/T 2) [2  32]
and is faster than LADMPSAP. As W k involves only (cid:96)1 constraints  the proximal step can be easily
computed with closed-form (details are in Appendix B.3) and takes only O(nnz(W )) time.
The complete procedure  which will be called Robust Matrix Factorization with Nonconvex Loss
(RMFNL) algorithm  is shown in Algorithm 1. The surrogate is optimized via its dual in step 4. The
primal solution is recovered in step 5  and (U k  V k) are updated in step 6.
Exploiting Sparsity. A direct implementation of APG takes O(mn) space and O(mnr) time per
iteration. In the following  we show how these can be reduced by exploiting sparsity of W .
c and W k  which are all related to ˙W k. Recall that
The objective in (5) involves Ak
Corollary 3.2 is sparse (as W is sparse). Thus  by exploiting sparsity  constructing Ak
r   Ak
only take O(nnz(W )) time and space.

˙W k in
c and W k

r   Ak

4

Algorithm 1 Robust matrix factorization using nonconvex loss (RMFNL) algorithm.
1: initialize U 1 ∈ Rm×r and V 1 ∈ Rm×r;
2: for k = 1  2  . . .   K do
3:
4:
5:
6:
7: end for
8: return U K+1 and V K+1.

compute ˙W k in Corollary 3.2 (only on the observed positions)  and Λk
compute xk = arg minx∈W k Dk(x) in Proposition 3.4 using APG;
c (HΩ(xk)(cid:62)U k − λV k);
¯U k = Ak
r
U k+1 = U k + ¯U k  V k+1 = V k + ¯V k;

(cid:0)HΩ(xk)V k − λU k(cid:1)  ¯V k = Ak

c ;
r   Λk

In each APG iteration  one has to compute the gradient  objective  and proximal step. First  consider
the gradient ∇Dk(x) of the objective  which is equal to

H−1
Ω (Ak

r (HΩ(x)V k − λU k)(V k)(cid:62)) + H−1

Ω (U k[(U k)(cid:62)HΩ(x) − λ(V k)(cid:62)]Ak

c ) − H−1

Ω (M ).

Ω (·)  we have ˆgk

t =(cid:80)r

r (HΩ(x)V k)−λ(Ak

Ω (Qk(V k)(cid:62))  where Qk = Ak

(6)
r (HΩ(x)V k − λU k). As Ak
The ﬁrst term can be rewritten as ˆgk = H−1
r
is diagonal and HΩ(x) is sparse  Qk can be computed as Ak
r U k) in O(nnz(W )r+
mr) time  where r is the number of columns in U k and V k. Let the tth element in Ω be (it  jt). By the
deﬁnition of H−1
jtq  and this takes O(nnz(W )r+mr) time. Similarly 
computing the second term in (6) takes O(nnz(W )r + nr) time. Hence  computing ∇Dk(x) takes a
total of O(nnz(W )r + (m + n)r) time and O(nnz(W ) + (m + n)r) space (the Algorithm is shown
in Appendix B.1). Similarly  the objective can be obtained in O(nnz(W )r + (m + n)r) time and
O(nnz(W ) + (m + n)r) space (details are in Appendix B.2). The proximal step takes O(nnz(W ))
time and space  as x ∈ Rnnz(W ). Thus  by exploiting sparsity  the APG algorithm has a space
complexity of O(nnz(W ) + (m + n)r) and iteration time complexity of O(nnz(W )r + (m + n)r).
In comparison  LADMPSAP needs O(mn) space and iteration time complexity of O(mnr). A
summary of the complexity results is shown in Figure 2(a).

q=1 Qk

itqV k

3.4 Convergence Analysis
In this section  we study the convergence of RMFNL. Note that the proof technique in RMF-MM
cannot be used  as it relies on convexity of the (cid:96)1-loss while φ in (4) is nonconvex (in particular 
Proposition 1 in [26] fails). Moreover  the proof of RMF-MM uses the subgradient. Here  as φ is
nonconvex  we will use the Clarke subdifferential [10]  which generalizes subgradients to nonconvex
functions (a brief introduction is in Appendix C). For the iterates {X k} generated by RMF-MM  it is
guaranteed to have a sufﬁcient decrease on the objective f in the following sense [26]: There exists a
F  ∀k. The following Proposition
constant γ > 0 such that f (X k) − f (X k+1) ≥ γ(cid:107)X k − X k+1(cid:107)2
shows that RMFNL also achieves a sufﬁcient decrease on its objective. Moreover  the {(U k  V k)}
sequence generated is bounded  which has at least one limit point.
Proposition 3.5. For Algorithm 1  {(U k  V k)} is bounded  and has a sufﬁcient decrease on ˙H.
Theorem 3.6. The limit points of the sequence generated by Algorithm 1 are critical points of (4).

4 Experiments
In this section  we compare the proposed RMFNL with state-of-the-art MF algorithms. Experiments
are performed on a PC with Intel i7 CPU and 32GB RAM. All the codes are in Matlab  with sparse
matrix operations implemented in C++. We use the nonconvex loss functions of LSP  Geman and
Laplace in Table 5 of Appendix A  with θ = 1; and ﬁx λ = 20/(m + n) in (1) as suggested in [26].

4.1 Synthetic Data
We ﬁrst perform experiments on synthetic data  which is generated as X = U V (cid:62) with U ∈ Rm×5 
V ∈ Rm×5  and m = {250  500  1000}. Elements of U and V are sampled i.i.d. from the standard
normal distribution N (0  1). This is then corrupted to form M = X + N + S  where N is
the noise matrix from N (0  0.1)  and S is a sparse matrix modeling outliers with 5% nonzero
elements randomly sampled from {±5}. We randomly draw 10 log(m)/m% of the elements from

5

√

(cid:107) ¯W (cid:12) (X − ¯U ¯V T )(cid:107)2

M as observations  with half of them for training and the other half for validation. The remaining
unobserved elements are for testing. Note that the larger the m  the sparser is the observed matrix.
The iterate (U 1  V 1) is initialized as Gaussian random matrices  and the iterative procedure is stopped
when the relative change in objective values between successive iterations is smaller than 10−4. For
the subproblems in RMF-MM and RMFNL  iteration is stopped when the relative change in objective
value is smaller than 10−6 or a maximum of 300 iterations is used. Rank r is set to the ground truth
(i.e.  5). For performance evaluation  we follow [26] and use the (i) testing root mean square error 
F /nnz( ¯W )  where ¯W is a binary matrix indicating positions of the
RMSE =
testing elements; and (ii) CPU time. To reduce statistical variability  results are averaged over ﬁve
repetitions.
Solvers for Surrogate Optimization. Here  we compare three solvers for surrogate optimization
in each RMFNL iteration (with the LSP loss and m = 1000): (i) LADMPSAP in RMF-MM; (ii)
APG(dense)  which uses APG but without utilizing data sparsity; and (iii) APG in Algorithm 1  which
utilizes data sparsity as in Section 3.3. The APG stepsize is determined by line-search  and adaptive
restart is used for further speedup [32]. Figure 2 shows convergence in the ﬁrst RMFNL iteration
(results for the other iterations are similar). As can be seen  LADMPSAP is the slowest w.r.t. the
number of iterations  as its convergence rate is inferior to both variants of APG (whose rates are the
same). In terms of CPU time  APG is the fastest as it can also utilize data sparsity.

(a) Complexities of surrogate optimizers.

(b) Number of iterations.

(c) CPU time.

Figure 2: Convergence of the objective on the synthetic data set (with the LSP loss and m = 1000).
Note that the curves for APG-dense and APG overlap in Figure 2(b).

Table 1 shows performance of the whole RMFNL algorithm with different surrogate optimizers.1 As
can be seen  the various nonconvex losses (LSP  Geman and Laplace) lead to similar RMSE’s  as has
been similarly observed in [16  38]. Moreover  the different optimizers all obtain the same RMSE. In
terms of speed  APG is the fastest  then followed by APG(dense)  and LADMPSAP is the slowest.
Hence  in the sequel  we will only use APG to optimize the surrogate.

Table 1: Performance of RMFNL with different surrogate optimizers.

loss

LSP

Geman

solver

LADMPSAP
APG(dense)

APG

LADMPSAP
APG(dense)

APG

LADMPSAP
Laplace APG(dense)

APG

RMSE

m = 250 (nnz: 11.04%)
CPU time
0.110±0.004
17.0±1.4
12.1±0.6
0.110±0.004
3.2±0.6
0.110±0.004
20.4±0.8
0.115±0.014
0.115±0.011
13.9±1.6
0.114±0.009
3.1±0.5
0.110±0.004
17.1±1.5
12.1±2.1
0.110±0.004
0.111±0.004
2.8±0.4

m = 500 (nnz: 6.21%)
RMSE

0.072±0.001
0.073±0.001
0.073±0.001
0.074±0.006
0.073±0.002
0.073±0.002
0.072±0.001
0.073±0.003
0.074±0.001

CPU time
195.7±34.7
114.4±18.8
5.5±1.0
231.0±36.9
146.9±24.8
8.3±1.1
203.4±22.7
120.9±28.9
5.6±1.0

m = 1000 (nnz: 3.45%)
RMSE
CPU time
0.45±0.007
950.8±138.8
490.1±91.9
0.45±0.007
24.6±3.2
0.45±0.006
950.8±138.8
0.45±0.007
0.45±0.007
490.1±91.9
0.45±0.006
24.6±3.2
0.45±0.007
950.8±138.8
490.1±91.9
0.45±0.007
0.45±0.006
24.6±3.2

Comparison with State-of-the-Art Matrix Factorization Algorithms. The (cid:96)2-loss-based MF
algorithms that will be compared include alternating gradient descent (AltGrad) [30]  Riemannian
preconditioning (RP) [29]  scaled alternating steepest descent (ScaledASD) [33]  alternative
minimization for large scale matrix imputing (ALT-Impute) [17] and online massive dictionary
learning (OMDL) [28]. The (cid:96)1-loss-based RMF algorithms being compared include RMF-MM [26] 
robust matrix completion (RMC) [7] and Grassmannian robust adaptive subspace tracking algorithm

1For all tables in the sequel  the best and comparable results according to the pairwise t-test with 95%

conﬁdence are highlighted.

6

(GRASTA) [18]. Codes are provided by the respective authors. We do not compare with AOPMC
[36]  which has been shown to be slower than RMC [7].
As can be seen from Table 2  RMFNL produces much lower RMSE than the MF/RMF algorithms 
and the RMSEs from different nonconvex losses are similar. AltGrad  RP  ScaledASD  ALT-Impute
and OMDL are very fast because they use the simple (cid:96)2 loss. However  their RMSEs are much higher
than RMFNL and RMF algorithms. A more detailed convergence comparison is shown in Figure 3.
As can be seen  RMF-MM is the slowest. RMFNL with different nonconvex losses have similar
convergence behavior  and they all converge to a lower testing RMSE much faster than the others.

Table 2: Performance of the various matrix factorization algorithms on synthetic data.

loss
(cid:96)2

(cid:96)1

LSP
Geman
Laplace

RP

RMC

RMSE

m = 250 (nnz: 11.04%)
algorithm
CPU time
1.0±0.6
1.062±0.040
AltGrad
0.1±0.1
1.048±0.071
ScaledASD 1.042±0.066
0.2±0.1
1.030±0.060
0.2±0.1
ALT-Impute
0.1±0.1
1.089±0.055
OMDL
1.5±0.1
0.338±0.033
GRASTA
0.226±0.040
2.8±1.0
RMF-MM 0.194±0.032
13.4±0.6
0.110±0.004
3.2±0.6
RMFNL
3.1±0.5
0.114±0.004
RMFNL
0.111±0.004
2.8±0.4
RMFNL

m = 500 (nnz: 6.21%)
CPU time
RMSE
1.8±0.3
0.4±0.2
0.4±0.3
0.3±0.1
0.2±0.1
2.9±0.3
2.7±0.5
154.9±12.5
5.5±1.0
8.3±1.1
5.6±1.0

0.950±0.005
0.953±0.012
0.950±0.009
0.937±0.010
0.945±0.018
0.306±0.002
0.201±0.001
0.145±0.009
0.073±0.001
0.073±0.001
0.074±0.001

m = 1000 (nnz: 3.45%)
CPU time
RMSE
6.0±4.2
1.1±0.1
1.2±0.5
1.0±0.2
0.5±0.2
6.1±0.4
4.2±2.5
827.7±116.3
14.0±5.2
19.0±4.9
15.9±6.1

0.853±0.010
0.848±0.009
0.847±0.009
0.838±0.009
0.847±0.009
0.244±0.009
0.195±0.006
0.122±0.004
0.047±0.002
0.047±0.001
0.047±0.002

(a) m = 250.

(b) m = 500.

(c) m = 1000.

Figure 3: Convergence of testing RMSE for the various algorithms on synthetic data.

4.2 Robust Collaborative Recommendation
In a recommender system  the love/hate attack changes the ratings of selected items to the minimum
(hate) or maximum (love) [5]. The love/hate attack is very simple  but can signiﬁcantly bias overall
prediction. As no love/hate attack data sets are publicly available  we follow [5  31] and manually
add permutations. Experiments are performed on the popular MovieLens recommender data sets:
MovieLens-100K  MovieLens-1M  and MovieLens-10M (Some statistics on these data sets are in
Appendix E.1). We randomly select 3% of the items from each data set. For each selected item 
all its observed ratings are set to either the minimum or maximum with equal possibilities. 50% of
the observed ratings are used for training  25% for validation  and the rest for testing. Algorithms
in Section 4.1 will be compared. To reduce statistical variability  results are averaged over ﬁve
repetitions. As in Section 4.1  the testing RMSE and CPU time are used for performance evaluation.
Results are shown in Table 3  and Figure 4 shows convergence of the RMSE. Again  RMFNL
with different nonconvex losses have similar performance and achieve the lowest RMSE. The MF
algorithms are fast  but have high RMSEs. GRASTA is not stable  with large RMSE and variance.

4.3 Afﬁne Rigid Structure-from-Motion (SfM)
SfM reconstructs the 3D scene from sparse feature points tracked in m images of a moving camera
[23]. Each feature point is projected to every image plane  and is thus represented by a 2m-
dimensional vector. With n feature points  this leads to a 2m × n matrix. Often  this matrix
has missing data (e.g.  some feature points may not be always visible) and outliers (arising from
feature mismatch). We use the Oxford Dinosaur sequence  which has 36 images and 4  983 feature
points. As in [26]  we extract three data subsets using feature points observed in at least 5  6 and 7

7

Table 3: Performance on the MovieLens data sets. CPU time is in seconds. RMF-MM cannot converge
in 104 seconds on the MovieLens-1M and MovieLens-10M data sets  and thus is not reported.
MovieLens-10M

MovieLens-100K

MovieLens-1M

loss
(cid:96)2

(cid:96)1

LSP
Geman
Laplace

RP

RMSE

algorithm
0.954±0.004
AltGrad
0.968±0.008
ScaledASD 0.951±0.004
0.942±0.021
ALT-Impute
0.958±0.003
OMDL
1.057±0.218
GRASTA
0.920±0.001
RMF-MM 0.901±0.003
0.885±0.006
RMFNL
0.885±0.005
RMFNL
0.885±0.005
RMFNL

RMC

CPU time
1.0±0.2
0.2±0.1
0.3±0.1
0.2±0.1
0.1±0.1
4.6±0.3
1.4±0.2
402.3±80.0
5.9±1.5
6.6±1.2
4.9±1.1

RMSE

0.856±0.005
0.867±0.002
0.878±0.003
0.859±0.001
0.873±0.008
0.842±0.011
0.849±0.001
0.828±0.001
0.829±0.005
0.828±0.001

—

CPU time
30.6±2.5
4.4±0.4
8.7±0.2
10.7±0.2
2.6±0.5
31.1±0.6
40.6±2.2
34.9±1.0
35.3±0.3
35.1±0.2

—

RMSE

0.872±0.003
0.948±0.011
0.884±0.001
0.872±0.001
0.881±0.003
0.876±0.047
0.855±0.001
0.817±0.004
0.817±0.004
0.817±0.005

—

CPU time
1130.4±9.6
199.9±39.0
230.2±7.7
198.9±2.6
63.4±4.2
1304.3±18.0
526.0±29.5
1508.2±69.1
1478.5±72.8
1513.4±12.2

—

(a) MovieLens-100K.

(b) MovieLens-1M.

(c) MovieLens-10M.

Figure 4: Convergence of testing RMSE on the recommendation data sets.

images. These are denoted “D1" (with size 72×932)  “D2" (72×557) and “D3" (72×336). The fully
observed data matrix can be recovered by rank-4 matrix factorization [12]  and so we set r = 4.
We compare RMFNL with RMF-MM and its variant (denoted RMF-MM(heuristic)) in Section 4.2
of [26]. In this variant  the diagonal entries of Λr and Λc are initialized with small values and
then increased gradually. It is claimed in [26] that this leads to faster convergence. However  our
experimental results show that this heuristic leads to more accurate  but not faster  results. Moreover 
its key pitfall is that Proposition 2.1 and the convergence guarantee for RMF-MM no longer holds.
For performance evaluation  as there is no ground-truth  we follow [26] and use the (i) mean absolute
error (MAE) (cid:107) ¯W (cid:12) ( ¯U ¯V (cid:62) − X)(cid:107)1/nnz( ¯W )  where ¯U and ¯V are outputs from the algorithm  X is
the data matrix with observed positions indicated by the binary ¯W ; and (ii) CPU time. As the various
nonconvex penalties have been shown to have similar performance  we will only report the LSP here.
Results are shown in Table 4. As can be seen  RMF-MM(heuristic) obtains a lower MAE than
RMF-MM  but is still outperformed by RMFNL. RMFNL is the fastest  though the speedup is not
as signiﬁcant as in previous sections. This is because the Dinosaur subsets are not very sparse (the
percentages of nonzero entries in “D1"  “D2" and “D3" are 17.9%  20.5% and 23.1%  respectively).

Table 4: Performance on the Dinosaur data subsets. CPU time is in seconds.

MAE

D1
0.374±0.031
0.442±0.096
0.323±0.012

CPU time
43.9±3.3
26.9±3.4
8.3±1.9

MAE

D2
0.381±0.022
0.458±0.043
0.332±0.005

CPU time
25.9±3.1
14.9±2.2
6.8±1.3

MAE

D3
0.382±0.034
0.466±0.072
0.316±0.006

CPU time
10.8±3.4
9.2±2.1
3.4±1.0

RMF-MM(heuristic)

RMF-MM
RMFNL

5 Conclusion

In this paper  we improved the robustness of matrix factorization by using a nonconvex loss instead
of the commonly used (convex) (cid:96)1 and (cid:96)2-losses. Second  we improved its scalability by exploiting
data sparsity (which RMF-MM cannot) and using the accelerated proximal gradient algorithm (which
is faster than the commonly used ADMM). The space and iteration time complexities are greatly
reduced. Theoretical analysis shows that the proposed RMFNL algorithm generates a critical point.
Extensive experiments on both synthetic and real-world data sets demonstrate that RMFNL is more
accurate and more scalable than the state-of-the-art.

8

References
[1] R. Basri  D. Jacobs  and I. Kemelmacher. Photometric stereo with general  unknown lighting. International

Journal of Computer Vision  72(3):239–257  2007.

[2] M. Beck  A.and Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Foundations and Trends in Machine Learning  3(1):1–122 
2011.

[4] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[5] R. Burke  M. O’Mahony  and N. Hurley. Recommender Systems Handbook. Springer  2015.

[6] R. Cabral  F. De la Torre  J. Costeira  and A. Bernardino. Unifying nuclear norm and bilinear factorization
approaches for low-rank matrix decomposition. In International Conference on Computer Vision  pages
2488–2495  2013.

[7] L. Cambier and P. Absil. Robust low-rank matrix completion by Riemannian optimization. SIAM Journal

on Scientiﬁc Computing  38(5):S440–S460  2016.

[8] E.J. Candès and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational

Mathematics  9(6):717–772  2009.

[9] E.J. Candès  M.B. Wakin  and S. Boyd. Enhancing sparsity by reweighted (cid:96)1 minimization. Journal of

Fourier Analysis and Applications  14(5-6):877–905  2008.

[10] F. Clarke. Optimization and Nonsmooth Analysis. SIAM  1990.

[11] F. De La Torre and M. Black. A framework for robust subspace learning. International Journal of Computer

Vision  54(1):117–142  2003.

[12] A. Eriksson and A. Van Den Hengel. Efﬁcient computation of robust low-rank matrix approximations
in the presence of missing data using the (cid:96)1-norm. In International Conference on Computer Vision and
Pattern Recognition  pages 771–778  2010.

[13] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal

of the American Statistical Association  96(456):1348–1360  2001.

[14] D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regularization. IEEE Transactions

on Image Processing  4(7):932–946  1995.

[15] P. Gong and J. Ye. HONOR: Hybrid optimization for non-convex regularized problems. In Advance in

Neural Information Processing Systems  pages 415–423  2015.

[16] P. Gong  C. Zhang  Z. Lu  J. Huang  and J. Ye. A general iterative shrinkage and thresholding algorithm
for non-convex regularized optimization problems. In International Conference on Machine Learning 
pages 37–45  2013.

[17] T. Hastie  R. Mazumder  J. Lee  and R. Zadeh. Matrix completion and low-rank SVD via fast alternating

least squares. Journal of Machine Learning Research  16:3367–3402  2015.

[18] J. He  L. Balzano  and A. Szlam.

Incremental gradient on the Grassmannian for online foreground
and background separation in subsampled video. In Computer Vision and Pattern Recognition  pages
1568–1575  2012.

[19] P. Huber. Robust Statistics. Springer  2011.

[20] D. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician  58(1):30–37  2004.

[21] W. Jiang  F. Nie  and H. Huang. Robust dictionary learning with capped (cid:96)1-norm. In International Joint

Conference on Artiﬁcial Intelligence  pages 3590–3596  2015.

[22] E. Kim  M. Lee  C. Choi  N. Kwak  and S. Oh. Efﬁcient (cid:96)1-norm-based low-rank matrix approximations for
large-scale problems using alternating rectiﬁed gradient method. IEEE Transactions on Neural Networks
and Learning Systems  26(2):237–251  2015.

9

[23] J. Koenderink and A. Van Doorn. Afﬁne structure from motion. Journal of the Optical Society of America 

8(2):377–385  1991.

[24] K. Lange  R. Hunter  and I. Yang. Optimization transfer using surrogate objective functions. Journal of

Computational and Graphical Statistics  9(1):1–20  2000.

[25] Z. Lin  R. Liu  and H. Li. Linearized alternating direction method with parallel splitting and adaptive

penalty for separable convex programs in machine learning. Machine Learning  2(99):287–325  2015.

[26] Z. Lin  C. Xu  and H. Zha. Robust matrix factorization by majorization minimization. IEEE Transactions

on Pattern Analysis and Machine Intelligence  (99)  2017.

[27] D. Meng  Z. Xu  L. Zhang  and J. Zhao. A cyclic weighted median method for (cid:96)1 low-rank matrix

factorization with missing entries. In AAAI Conference on Artiﬁcial Intelligence  pages 704–710  2013.

[28] A. Mensch  J. Mairal  B. Thirion  and G. Varoquaux. Dictionary learning for massive matrix factorization.

In International Conference on Machine Learning  pages 1737–1746  2016.

[29] B. Mishra and R. Sepulchre. Riemannian preconditioning. SIAM Journal on Optimization  26(1):635–660 

2016.

[30] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advance in Neural Information

Processing Systems  pages 1257–1264  2008.

[31] B. Mobasher  R. Burke  R. Bhaumik  and C. Williams. Toward trustworthy recommender systems: An
analysis of attack models and algorithm robustness. ACM Transactions on Internet Technology  7(4):23 
2007.

[32] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming 

140(1):125–161  2013.

[33] J. Tanner and K. Wei. Low rank matrix completion by alternating steepest descent methods. Applied and

Computational Harmonic Analysis  40(2):417–429  2016.

[34] J. Trzasko and A. Manduca. Highly undersampled magnetic resonance image reconstruction via homotopic-

minimization. IEEE Transactions on Medical Imaging  28(1):106–121  2009.

[35] M. Yan. Restoration of images corrupted by impulse noise and mixed Gaussian impulse noise using blind

inpainting. SIAM Journal on Imaging Sciences  6(3):1227–1245  2013.

[36] M. Yan  Y. Yang  and S. Osher. Exact low-rank matrix completion from sparsely corrupted entries via

adaptive outlier pursuit. Journal of Scientiﬁc Computing  56(3):433–449  2013.

[37] J. Yang and J. Leskovec. Overlapping community detection at scale: a nonnegative matrix factorization

approach. In Web Search and Data Mining  pages 587–596  2013.

[38] Q. Yao  J. Kwok  T. Wang  and T. Liu. Large-scale low-rank matrix learning with nonconvex regularizers.

IEEE Transactions on Pattern Analysis and Machine Intelligence  2018.

[39] C. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics 

38(2):894–942  2010.

[40] Y. Zheng  G. Liu  S. Sugimoto  S. Yan  and M. Okutomi. Practical low-rank matrix approximation
under robust (cid:96)1-norm. In International Conference on Computer Vision and Pattern Recognition  pages
1410–1417  2012.

[41] W. Zuo  D. Meng  L. Zhang  X. Feng  and D. Zhang. A generalized iterated shrinkage algorithm for

non-convex sparse coding. In International Conference on Computer Vision  pages 217–224  2013.

10

,Quanming Yao
James Kwok
Hongjoon Ahn
Sungmin Cha
Donggyu Lee
Taesup Moon