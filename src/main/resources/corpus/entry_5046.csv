2013,Sketching Structured Matrices for Faster Nonlinear Regression,Motivated by the desire to extend fast randomized techniques to nonlinear $l_p$ regression  we consider a class of structured regression problems. These problems involve Vandermonde matrices which arise naturally in various statistical modeling settings  including classical polynomial fitting problems and recently developed randomized techniques for scalable kernel methods. We show that this structure can be exploited to further accelerate the solution of the regression problem  achieving running times that are faster than input sparsity''. We present empirical results confirming both the practical value of our modeling framework  as well as speedup benefits of randomized regression.",Sketching Structured Matrices for

Faster Nonlinear Regression

Haim Avron
Vikas Sindhwani
IBM T.J. Watson Research Center

Yorktown Heights  NY 10598

{haimav vsindhw}@us.ibm.com

David P. Woodruff

IBM Almaden Research Center

San Jose  CA 95120

dpwoodru@us.ibm.com

Abstract

Motivated by the desire to extend fast randomized techniques to nonlinear lp re-
gression  we consider a class of structured regression problems. These problems
involve Vandermonde matrices which arise naturally in various statistical model-
ing settings  including classical polynomial ﬁtting problems  additive models and
approximations to recently developed randomized techniques for scalable kernel
methods. We show that this structure can be exploited to further accelerate the
solution of the regression problem  achieving running times that are faster than
“input sparsity”. We present empirical results conﬁrming both the practical value
of our modeling framework  as well as speedup beneﬁts of randomized regression.

1

Introduction

Recent literature has advocated the use of randomization as a key algorithmic device with which
to dramatically accelerate statistical learning with lp regression or low-rank matrix approximation
techniques [12  6  8  10]. Consider the following class of regression problems 

arg min

x∈C (cid:107)Zx − b(cid:107)p  where p = 1  2

(1)
where C is a convex constraint set  Z ∈ Rn×k is a sample-by-feature design matrix  and b ∈ Rn
is the target vector. We assume henceforth that the number of samples is large relative to data
dimensionality (n (cid:29) k). The setting p = 2 corresponds to classical least squares regression  while
p = 1 leads to least absolute deviations ﬁt  which is of signiﬁcant interest due to its robustness
properties. The constraint set C can incorporate regularization. When C = Rk and p = 2  an -
optimal solution can be obtained in time O(nk log k) + poly(k −1) using randomization [6  19] 
which is much faster than an O(nk2) deterministic solver when  is not too small (dependence on 
can be improved to O(log(1/)) if higher accuracy is needed [17]). Similarly  a randomized solver
for l1 regression runs in time O(nk log n) + poly(k −1) [5].
In many settings  what makes such acceleration possible is the existence of a suitable oblivious
subspace embedding (OSE). An OSE can be thought of as a data-independent random “sketching”
matrix S ∈ Rt×n whose approximate isometry properties over a subspace (e.g.  over the column
space of Z  b) imply that 

(cid:107)S(Zx − b)(cid:107)p ≈ (cid:107)Zx − b(cid:107)p for all x ∈ C  

which in turn allows x to be optimized over a “sketched” dataset of much smaller size without losing
solution quality. Sketching matrices include Gaussian random matrices  structured random matrices
which admit fast matrix multiplication via FFT-like operations  and others.
This paper is motivated by two questions which in our context turn out to be complimentary:

1

◦ Can additional structure in Z be non-trivially exploited to further accelerate runtime? Clarkson
and Woodruff have recently shown that when Z is sparse and has nnz(Z) (cid:28) nk non-zeros  it
is possible to achieve much faster “input-sparsity” runtime using hashing-based sketching matri-
ces [7]. Is it possible to further beat this time in the presence of additional structure on Z?
◦ Can faster and more accurate sketching techniques be designed for nonlinear and nonparametric
regression? To see that this is intertwined with the previous question  consider the basic problem
i=1 βizi to a set of samples (zi  bi) ∈ R × R  i = 1  . . .   n.
Then  the design matrix Z has Vandermonde structure which can potentially be exploited in a
regression solver. It is particularly appealing to estimate non-parametric models on large datasets.
Sketching algorithms have recently been explored in the context of kernel methods for non-
parametric function estimation [16  11].

of ﬁtting a polynomial model  b =(cid:80)q

To be able to precisely describe the structure on Z that we consider in this paper  and outline our
contributions  we need the following deﬁnitions.

Deﬁnition 1 (Vandermonde Matrix) Let x0  x1  . . .   xn−1 be real numbers. The Vandermonde ma-
trix  denoted Vq n(x0  x1  . . .   xn−1)  has the form:

Vq n(x1  x1  . . .   xn−1) =

 1

x0
. . .
xq−1

0



1
x1
. . .
xq−1

1

1

. . .
. . . xn−1
. . .
. . .
. . . xq−1
n−1

Vandermonde matrices of dimension q × n require only O(n) implicit storage and admit O((n +
q) log2 q) matrix-vector multiplication time. We also deﬁne the following matrix operator Tq which
maps a matrix A to a block-Vandermonde structured matrix.
Deﬁnition 2 (Matrix Operator) Given a matrix A ∈ Rn×d  we deﬁne the following matrix:
Tq(A) =

Vq n(A1 1  . . .   An 1)T Vq n(A1 2  . . .   An 2)T

Vq n(A1 d  . . .   An d)T

···

(cid:20)

(cid:21)

In this paper  we consider regression problems  Eqn. 1  where Z can be written as

Z = Tq(A)

(2)
for an n × d matrix A  so that k = dq. The operator Tq expands each feature (column) of the
original dataset A to q columns of Z by applying monomial transformations upto degree q − 1. This
lends a block-Vandermonde structure to Z. Such structure naturally arises in polynomial regression
problems  but also applies more broadly to non-parametric additive models and kernel methods as
we discuss below. With this setup  the goal is to solve the following problem:
Structured Regression: Given A and b  with constant probability output a vector x(cid:48) ∈ C for which

(cid:107)Tq(A)x(cid:48) − b(cid:107)p ≤ (1 + ε)(cid:107)Tq(A)x(cid:63) − b(cid:107)p 

for an accuracy parameter ε > 0  where x(cid:63) = arg minx∈C (cid:107)Tq(A)x − b(cid:107)p.
Our contributions in this paper are as follows:
◦ For p = 2  we provide an algorithm that solves the structured regression problem above in time
O(nnz(A) log2 q) + poly(dq−1). By combining our sketching methods with preconditioned
iterative solvers  we can also obtain logarithmic dependence on . For p = 1  we provide an
algorithm with runtime O(nnz(A) log n log2 q) + poly(dq−1 log n). This implies that moving
from linear (i.e  Z = A) to nonlinear regression (Z = Tq(A))) incurs only a mild additional log2 q
runtime cost  while requiring no extra storage! Since nnz(Tq(A)) = q nnz(A)  this provides - to
our knowledge - the ﬁrst sketching approach that operates faster than “input-sparsity” time  i.e.
we sketch Tq(A) in time faster than nnz(Tq(A)).
◦ Our algorithms apply to a broad class of nonlinear models for both least squares regression and
their robust l1 regression counterparts. While polynomial regression and additive models with
monomial basis functions are immediately covered by our methods  we also show that under a
suitable choice of the constraint set C  the structured regression problem with Z = Tq(AG) for
a Gaussian random matrix G approximates non-parametric regression using the Gaussian kernel.
We argue that our approach provides a more ﬂexible modeling framework when compared to
randomized Fourier maps for kernel methods [16  11].

2

◦ Empirical results conﬁrm both the practical value of our modeling framework  as well as speedup

beneﬁts of sketching.

2 Polynomial Fitting  Additive Models and Random Fourier Maps

z ∈ Rd are related through the model y = µ +(cid:80)d
to expand each function as fi(·) = (cid:80)q

Our primary goal in this section is to motivate sketching approaches for a versatile class of Block-
Vandermonde structured regression problems by showing that these problems arise naturally in var-
ious statistical modeling settings.
The most basic application is the one-dimensional (d = 1) polynomial regression.
In multivariate additive regression models  a continuous target variable y ∈ R and input variables
i=1 fi(zi) + i where µ is an intercept term 
i are zero-mean Gaussian error terms and fi are smooth univariate functions. The basic idea is
t=1 βi thi t(·) using basis functions hi t(·) and estimate the
unknown parameter vector x = [β11 . . . β1q . . . βdq]T typically by a constrained or penalized least
2 where b = (y1 . . . yn)T and Z = [H1 . . . Hq] ∈ Rn×dq for
squares model  argminx∈C(cid:107)Zx − b(cid:107)2
(Hi)j t = hi t(zj) on a training sample (zi  yi)  i = 1 . . . n. The constraint set C typically imposes
smoothing  sparsity or group sparsity constraints [2]. It is easy to see that choosing a monomial basis
hi s(u) = us immediately maps the design matrix Z to the structured regression form of Eqn. 2.
For p = 1  our algorithms also provide fast solvers for robust polynomial additive models.
Additive models impose a restricted form of univariate nonlinearity which ignores interactions
between covariates. Let us denote an interaction term as zα = zα1
d   α = (α1 . . . αd)
i αi = q  αi ∈ {0 . . . q}. A degree-q multivariate polynomial function space Pq is
i αi ≤ q}. Pq admits all possible degree-q interactions
but has dimensionality dq which is computationally infeasible to explicitly work with except for
low-degrees and low-dimensional or sparse datasets [3]. Kernel methods with polynomial kernels
α zαz(cid:48)α provide an implicit mechanism to compute inner products in the
feature space associated with Pq. However  they require O(n3) computation for solving associated
kernelized (ridge) regression problems and O(n2) storage of dense n × n Gram matrices K (given
by Kij = k(zi  zj))  and therefore do not scale well.
For a d × D matrix G let SG be the subspace spanned by

1 . . . zαd

where (cid:80)
spanned by {zα  α ∈ {0  . . . q}d (cid:80)
k(z  z(cid:48)) =(cid:0)zT z(cid:48)(cid:1)q

=(cid:80)

 .

(cid:33)t


(cid:32) d(cid:88)

i=1

Gijzi

  t = 1 . . . q  j = 1 . . . s

Assuming D = dq and that G is a random matrix of i.i.d Gaussian variables  then almost surely we
have SG = Pq. An intuitively appealing explicit scalable approach is then to use D (cid:28) dq. In that
case SG essentially spans a random subspace of Pq. The design matrix for solving the multivariate
polynomial regression restricted to SG has the form Z = Tq(AG) where A = [zT
This scheme can be in fact related to the idea of random Fourier features introduced by Rahimi
and Recht [16] in the context of approximating shift-invariant kernel functions  with the Gaussian
Kernel k(z  z(cid:48)) = exp (−(cid:107)z − z(cid:48)(cid:107)2
2/2σ2) as the primary example. By appealing to Bochner’s The-
orem [18]  it is shown that the Gaussian kernel is the Fourier transform of a zero-mean multivariate
Gaussian distribution with covariance matrix σ−1Id where Id denotes the d-dimensional identity
matrix 

1 . . . zT

n ]T .

2/2σ2) = Eω∼N (0d σ−1Id)[φω(z)φω(z(cid:48))∗]

(cid:80)D
where φω(z) = eiω(cid:48)z. An empirical approximation to this expectation can be obtained by sampling
D frequencies ω ∼ N (0d  σ−1Id) and setting k(z  z(cid:48)) = 1
i=1 φωi(z)φωi(z)∗. This implies
that the Gram matrix of the Gaussian kernel  Kij = exp (−(cid:107)zi − zj(cid:107)2
2/2σ2) may be approximated
with high concentration as K ≈ RRT where R = [cos(AG) sin(AG)] ∈ Rn×2D (sine and co-
sine are applied elementwise as scalar functions). This randomized explicit feature mapping for
the Gaussian kernel implies that standard linear regression  with R as the design matrix  can then
be used to obtain a solution in time O(nD2). By taking the Maclaurin series expansion of sine
and cosine upto degree q  we can see that a restricted structured regression problem of the form 

k(z  z(cid:48)) = exp (−(cid:107)z − z(cid:48)(cid:107)2

D

3

argminx∈range(Q)(cid:107)Tq(AG)x − b(cid:107)p  where the matrix Q ∈ R2Dq×2D contains appropriate coefﬁ-
cients of the Maclaurin series  will closely approximate the randomized Fourier features construction
of [16]. By dropping or modifying the constraint set x ∈ range(Q)  the setup above  in principle 
can deﬁne a richer class of models. A full error analysis of this approach is the subject of a separate
paper.

3 Fast Structured Regression with Sketching

We now develop our randomized solvers for block-Vandermonde structured lp regression problems.
In the theoretical developments below  we consider unconstrained regression though our results
generalize straightforwardly to convex constraint sets C. For simplicity  we state all our results
for constant failure probability. One can always repeat the regression procedure O(log(1/δ)) times 
each time with independent randomness  and choose the best solution found. This reduces the failure
probability to δ.

3.1 Background

of M. Deﬁne (cid:107)M(cid:107)1 to be the element-wise (cid:96)1 norm of M. That is  (cid:107)M(cid:107)1 =(cid:80)

We begin by giving some notation and then provide necessary technical background.
Given a matrix M ∈ Rn×d  let M1  . . .   Md be the columns of M  and M 1  . . .   M n be the rows
i∈[d] (cid:107)Mi(cid:107)1. Let

(cid:107)M(cid:107)F =

i∈[n] j∈[d] M 2
i j

be the Frobenius norm of M. Let [n] = {1  . . .   n}.

(cid:16)(cid:80)

(cid:17)1/2

3.1.1 Well-Conditioning and Sampling of A Matrix
Deﬁnition 3 ((α  β  1)-well-conditioning [8]) Given a matrix M ∈ Rn×d  we say M is (α  β  1)-
well-conditioned if (1) (cid:107)x(cid:107)∞ ≤ β (cid:107)M x(cid:107)1 for any x ∈ Rd  and (2) (cid:107)M(cid:107)1 ≤ α.
Lemma 4 (Implicit in [20]) Suppose S is an r × n matrix so that for all x ∈ Rd 

(cid:107)M x(cid:107)1 ≤ (cid:107)SM x(cid:107)1 ≤ κ(cid:107)M x(cid:107)1.

√

r  κ  1)-well-conditioned.

Let Q · R be a QR-decomposition of SM  so that QR = SM and Q has orthonormal columns.
Then M R−1 is (d
Theorem 5 (Theorem 3.2 of [8]) Suppose U is an (α  β  1)-well-conditioned basis of an n × d
matrix A. For each i ∈ [n]  let pi ≥ min
Suppose we independently sample each row with probability pi  and create a diagonal matrix S
where Si i = 0 if i is not sampled  and Si i = 1/pi if i is sampled. Then with probability at least
1 − δ  simultaneously for all x ∈ Rd we have:

(cid:17)
  where t ≥ 32αβ(d ln(cid:0) 12

(cid:1) + ln(cid:0) 2

(cid:1))/(ε2).

(cid:107)Ui(cid:107)1
t(cid:107)U(cid:107)1

(cid:16)

ε

δ

1 

|(cid:107)SAx(cid:107)1 − (cid:107)Ax(cid:107)1| ≤ ε(cid:107)Ax(cid:107)1.

We also need the following method of quickly obtaining approximations to the pi’s in Theorem 5 
which was originally given in Mahoney et al. [13].
Theorem 6 Let U ∈ Rn×d be an (α  β  1)-well-conditioned basis of an n × d matrix A. Suppose
G is a d × O(log n) matrix of i.i.d. Gaussians. Let pi = min
for all i  where t is
as in Theorem 5. Then with probability 1 − 1/n  over the choice of G  the following occurs. If we
sample each row with probability pi  and create S as in Theorem 5  then with probability at least
1 − δ  over our choice of sampled rows  simultaneously for all x ∈ Rd we have:

(cid:107)UiG(cid:107)1
√
d(cid:107)U G(cid:107)1

(cid:17)

(cid:16)

1 

t2

|(cid:107)SAx(cid:107)1 − (cid:107)Ax(cid:107)1| ≤ ε(cid:107)Ax(cid:107)1.

3.1.2 Oblivious Subspace Embeddings
Let A ∈ Rn×d. We assume that n > d. Let nnz(A) denote the number of non-zero entries of A.
We can assume nnz(A) ≥ n and that there are no all-zero rows or columns in A.

4

(cid:96)2 Norm The following family of matrices is due to Charikar et al. [4] (see also [9]): For a param-
eter t  deﬁne a random linear map ΦD : Rn → Rt as follows:
• h : [n] (cid:55)→ [t] is a random map so that for each i ∈ [n]  h(i) = t(cid:48) for t(cid:48) ∈ [t] with probability 1/t.
• Φ ∈ {0  1}t×n is a t × n binary matrix with Φh(i) i = 1  and all remaining entries 0.
• D is an n × n random diagonal matrix  with each diagonal entry independently chosen to be +1

or −1 with equal probability.

We will refer to Π = ΦD as a sparse embedding matrix.
For certain t  it was recently shown [7] that with probability at least .99 over the choice of Φ and D 
for any ﬁxed A ∈ Rn×d  we have simultaneously for all x ∈ Rd 

(1 − ε) · (cid:107)Ax(cid:107)2 ≤ (cid:107)ΠAx(cid:107)2 ≤ (1 + ε) · (cid:107)Ax(cid:107)2  

that is  the entire column space of A is preserved [7]. The best known value of t is t = O(d2/ε2)
[14  15] .
We will also use an oblivious subspace embedding known as the subsampled randomized Hadamard
transform  or SRHT. See Boutsidis and Gittens’s recent article for a state-the-art analysis [1].
Theorem 7 (Lemma 6 in [1]) There is a distribution over linear maps Π(cid:48) such that with probability
.99 over the choice of Π(cid:48)  for any ﬁxed A ∈ Rn×d  we have simultaneously for all x ∈ Rd 

(1 − ε) · (cid:107)Ax(cid:107)2 ≤ (cid:107)Π(cid:48)Ax(cid:107)2 ≤ (1 + ε) · (cid:107)Ax(cid:107)2  

√
where the number of rows of Π(cid:48) is t(cid:48) = O(ε−2(log d)(
Π(cid:48)A is O(nd log t(cid:48)).

√

d +

log n)2)  and the time to compute

(cid:96)1 Norm The results can be generalized to subspace embeddings with respect to the (cid:96)1-norm
[7  14  21]. The best known bounds are due to Woodruff and Zhang [21]  so we use their family of
embedding matrices in what follows. Here the goal is to design a distribution over matrices Ψ  so
that with probability at least .99  for any ﬁxed A ∈ Rn×d  simultaneously for all x ∈ Rd 

(cid:107)Ax(cid:107)1 ≤ (cid:107)ΨAx(cid:107)1 ≤ κ(cid:107)Ax(cid:107)1  

where κ > 1 is a distortion parameter. The best known value of κ  independent of n  for which
ΨA can be computed in O(nnz(A)) time is κ = O(d2 log2 d) [21]. Their family of matrices Ψ is
chosen to be of the form Π · E  where Π is as above with parameter t = d1+γ for arbitrarily small
constant γ > 0  and E is a diagonal matrix with Ei i = 1/ui  where u1  . . .   un are independent
standard exponentially distributed random variables.
Recall that an exponential distribution has support x ∈ [0 ∞)  probability density function (PDF)
f (x) = e−x and cumulative distribution function (CDF) F (x) = 1−e−x. We say a random variable
X is exponential if X is chosen from the exponential distribution.

3.1.3 Fast Vandermonde Multipication
Lemma 8 Let x0  . . .   xn−1 ∈ R and V = Vq n(x0  . . .   xn−1). For any y ∈ Rn and z ∈ Rq  the
matrix-vector products V y and V T z can be computed in O((n + q) log2 q) time.

3.2 Main Lemmas

We handle (cid:96)2 and (cid:96)1 separately. Our algorithms uses the subroutines given by the next lemmas.
Lemma 9 (Efﬁcient Multiplication of a Sparse Sketch and Tq(A)) Let A ∈ Rn×d. Let Π = ΦD
be a sparse embedding matrix for the (cid:96)2 norm with associated hash function h : [n] → [t] for an
arbitrary value of t  and let E be any diagonal matrix. There is a deterministic algorithm to compute
the product Φ · D · E · Tq(A) in O((nnz(A) + dtq) log2 q) time.
Proof: By deﬁnition of Tq(A)  it sufﬁces to prove this when d = 1. Indeed  if we can prove for a
column vector a that the product Φ· D · E · Tq(a) can be computed in O((nnz(a) + tq) log2 q) time 
then by linearity if will follow that the product Φ · D · E · Tq(A) can be computed in O((nnz(A +

5

Algorithm 1 StructRegression-2
1: Input: An n × d matrix A with nnz(A) non-zero entries  an n × 1 vector b  an integer degree q  and an accuracy parameter ε > 0.
2: Output: With probability at least .98  a vector x(cid:48) ∈ Rd for which (cid:107)Tq(A)x(cid:48) − b(cid:107)2 ≤ (1 + ε) minx (cid:107)Tq(A)x − b(cid:107)2.
3: Let Π = ΦD be a sparse embedding matrix for the (cid:96)2 norm with t = O((dq)2/ε2).
4: Compute ΠTq(A) using the efﬁcient algorithm of Lemma 9 with E set to the identity matrix.
5: Compute Πb.
6: Compute Π(cid:48)(ΠTq(A)) and Π(cid:48)Πb  where Π(cid:48) is a subsampled randomized Hadamard transform of Theorem 7 with t(cid:48) =

√

√

O(ε−2(log(dq))(

dq +

log t)2) rows.

7: Output the minimizer x(cid:48) of (cid:107)Π(cid:48)ΠTq(A)x(cid:48) − Π(cid:48)Πb(cid:107)2.

dtq) log2 q) time for general d. Hence  in what follows  we assume that d = 1 and our matrix A is a
column vector a. Notice that if a is just a column vector  then Tq(A) is equal to Vq n(a1  . . .   an)T .
For each k ∈ [t]  deﬁne the ordered list Lk = i such that ai (cid:54)= 0 and h(i) = k. Let (cid:96)k = |Lk|.
We deﬁne an (cid:96)k-dimensional vector σk as follows. If pk(i) is the i-th element of Lk  we set σk
i =
Dpk(i) pk(i) · Epk(i) pk(i). Let V k be the submatrix of Vq n(a1  . . .   an)T whose rows are in the set
Lk. Notice that V k is itself the transpose of a Vandermonde matrix  where the number of rows of
V k is (cid:96)k. By Lemma 8  the product σkV k can be computed in O(((cid:96)k + q) log2 q) time. Notice that
σkV k is equal to the k-th row of the product ΦDETq(a). Therefore  the entire product ΦDETq(a)

k (cid:96)k log2 q(cid:1) = O((nnz(a) + tq) log2 q) time.

can be computed in O(cid:0)(cid:80)

Lemma 10 (Efﬁcient Multiplication of Tq(A) on the Right) Let A ∈ Rn×d. For any vector z 
there is a deterministic algorithm to compute the matrix vector product Tq(A) · z in O((nnz(A) +
dq) log2 q) time.

The proof is provided in the supplementary material.
Lemma 11 (Efﬁcient Multiplication of Tq(A) on the Left) Let A ∈ Rn×d. For any vector z 
there is a deterministic algorithm to compute the matrix vector product z · Tq(A) in O((nnz(A) +
dq) log2 q) time.

The proof is provided in the supplementary material.

3.3 Fast (cid:96)2-regression

We start by considering the structured regression problem in the case p = 2. We give an algorithm
for this problem in Algorithm 1.

Theorem 12 Algorithm STRUCTREGRESSION-2 solves w.h.p the structured regression with p = 2
in time

O(nnz(A) log2 q) + poly(dq/ε).

Proof: By the properties of a sparse embedding matrix (see Section 3.1.2)  with probability at least
.99  for t = O((dq)2/ε2)  we have simultaneously for all y in the span of the columns of Tq(A)
adjoined with b 

(1 − ε)(cid:107)y(cid:107)2 ≤ (cid:107)Πy(cid:107)2 ≤ (1 + ε)(cid:107)y(cid:107)2 

since the span of this space has dimension at most dq + 1. By Theorem 7  we further have that with
probability .99  for all vectors z in the span of the columns of Π(Tq(A) ◦ b) 

(1 − ε)(cid:107)z(cid:107)2 ≤ (cid:107)Π(cid:48)z(cid:107)2 ≤ (1 + ε)(cid:107)z(cid:107)2.

It follows that for all vectors x ∈ Rd 

(1 − O(ε))(cid:107)Tq(A)x − b(cid:107)2 ≤ (cid:107)Π(cid:48)Π(Tq(A)x − B)(cid:107)2 ≤ (1 + O(ε))(cid:107)Tq(A)x − b(cid:107)2.

It follows by a union bound that with probability at least .98  the output of STRUCTREGRESSION-2
is a (1 + ε)-approximation.
For the time complexity  ΠTq(A) can be computed in O((nnz(A) + dtq) log2 q) by Lemma 9  while
Πb can be computed in O(n) time. The remaining steps can be performed in poly(dq/ε) time  and
therefore the overall time is O(nnz(A) log2 q) + poly(dq/ε).

6

γ > 0.

Algorithm 2 StructRegression-1
1: Input: An n × d matrix A with nnz(A) non-zero entries  an n × 1 vector b  an integer degree q  and an accuracy parameter ε > 0.
2: Output: With probability at least .98  a vector x(cid:48) ∈ Rd for which (cid:107)Tq(A)x(cid:48) − b(cid:107)1 ≤ (1 + ε) minx (cid:107)Tq(A)x − b(cid:107)1.
3: Let Ψ = ΠE = ΦDE be a subspace embedding matrix for the (cid:96)1 norm with t = (dq + 1)1+γ for an arbitrarily small constant
4: Compute ΨTq(A) = ΠETq(A) using the efﬁcient algorithm of Lemma 9.
5: Compute Ψb = ΠEb.
6: Compute a QR-decomposition of Ψ(Tq(A) ◦ b)  where ◦ denotes the adjoining of column vector b to Tq(A).
7: Let G be a (dq + 1) × O(log n) matrix of i.i.d. Gaussians.
8: Compute R−1 · G.
9: Compute (Tq(A) ◦ b) · (R−1G) using the efﬁcient algorithm of Lemma 10 applied to each of the columns of R−1G.
10: Let S be the diagonal matrix of Theorem 6 formed by sampling ˜O(q1+γ/2d4+γ/2ε−2) rows of Tq(A) and corresponding entries of
11: Output the minimizer x(cid:48) of (cid:107)STq(A)x(cid:48) − Sb(cid:107)1.

b using the scheme of Theorem 6.

3.3.1 Logarithmic Dependence on 1/ε

The STRUCTREGRESSION-2 algorithm can be modiﬁed to obtain a running time with a logarithmic
dependence on ε by combining sketching-based methods with iterative ones.

Theorem 13 There is an algorithm which solves the structured regression problem with p = 2 in
time O((nnz(A) + dq) log(1/ε)) + poly(dq) w.h.p.

Due to space limitations the proof is provided in Supplementary material.

3.4 Fast (cid:96)1-regression

We now consider the structured regression in the case p = 1. The algorithm in this case is more
complicated than that for p = 2  and is given in Algorithm 2.

Theorem 14 Algorithm STRUCTREGRESSION-1 solves w.h.p the structured regression in problem
with p = 1 in time

O(nnz(A) log n log2 q) + poly(dqε−1 log n).

The proof is provided in supplementary material.
We note when there is a convex constraint set C the only change in the above algorithms is to
optimize over x(cid:48) ∈ C.
4 Experiments
We report two sets of experiments on classiﬁcation and regression datasets. The ﬁrst set of ex-
periments compares generalization performance of our structured nonlinear least squares regres-
sion models against standard linear regression  and nonlinear regression with random fourier fea-
tures [16]. The second set of experiments focus on scalability beneﬁts of sketching. We used
Regularized Least Squares Classiﬁcation (RLSC) for classiﬁcation.
Generalization performance is reported in Table 1. As expected  ordinary (cid:96)2 linear regression is
very fast  especially if the matrix is sparse. However  it delivers only mediocre results. The results
improve somewhat with additive polynomial regression. Additive polynomial regression maintains
the sparsity structure so it can exploit fast sparse solvers. Once we introduce random features 
thereby introducing interaction terms  results improve considerably. When compared with random
Fourier features  for the same number of random features D  additive polynomial regression with
random features get better results than regression with random Fourier features. If the number of
random features is not the same  then if DF ourier = DP oly · q (where DF ourier is the number of
Fourier features  and DP oly is the number of random features in the additive polynomial regression)
then regression with random Fourier features seems to outperform additive polynomial regression
with random features. However  computing the random features is one of the most expensive steps 
so computing better approximations with fewer random features is desirable.
Figure 1 reports the beneﬁt of sketching in terms of running times  and the trade-off in terms of
accuracy. In this experiment we use a larger sample of the MNIST dataset with 300 000 examples.

7

Dataset

Ord. Reg.

Add. Poly. Reg.

MNIST
classiﬁcation
n = 60  000  d = 784
k = 10  000
CPU
regression
n = 6  554  d = 21
k = 819
ADULT
classiﬁcation
n = 32  561  d = 123
k = 16  281
CENSUS
regression
n = 18  186  d = 119
k = 2  273
FOREST COVER
classiﬁcation
n = 522  910  d = 54
k = 58  102

14%
3.9 sec

12%
0.01 sec

15.5%
0.17 sec

7.1%
0.3 sec

25.7%
3.3 sec

11%
19.1 sec
q = 4

3.3%
0.07 sec
q = 4

15.5%
0.55 sec
q = 4

7.0%
1.4 sec
q = 4
λ = 0.2
23.7%
7.8 sec
q = 4

Add. Poly. Reg.
w/ Random Features
6.9%
5.5 sec
D = 300  q = 4

Ord. Reg.
w/ Fourier Features
7.8%
6.8 sec
D = 500

2.8%
0.13 sec
D = 60  q = 4

15.0%
3.9 sec
D = 500  q = 4

6.85%
1.9 sec
D = 500  q = 4 
λ = 0.1
20.0%
14.0 sec
D = 200  q = 4

2.8%
0.14 sec
D = 180

15.1%
3.6 sec
D = 1000

6.5%
2.1 sec
D = 500
λ = 0.1
21.3%
15.5 sec
D = 400

Table 1: Comparison of testing error and training time of the different methods. In the table  n is number of training instances  d is the
number of features per instance and k is the number of instances in the test set. “Ord. Reg.” stands for ordinary (cid:96)2 regression. “Add. Poly.
Reg.” stands for additive polynomial (cid:96)2 regression. For classiﬁcation tasks  the percent of testing points incorrectly predicted is reported. For
regression tasks  we report (cid:107)yp − y(cid:107)2 / (cid:107)y(cid:107) where yp is the predicted values and y is the ground truth.

(a)

(b)

(c)

Figure 1: Examining the performance of sketching.

We compute 1 500 random features  and then solve the corresponding additive polynomial regres-
sion problem with q = 4  both exactly and with sketching to different number of rows. We also
tested a sampling based approach which simply randomly samples a subset of the rows (no sketch-
ing). Figure 1 (a) plots the speedup of the sketched method relative to the exact solution. In these
experiments we use a non-optimized straightforward implementation that does not exploit fast Van-
dermonde multiplication or parallel processing. Therefore  running times were measured using a
sequential execution. We measured only the time required to solve the regression problem. For
this experiment we use a machine with two quad-core Intel E5410 @ 2.33GHz  and 32GB DDR2
800MHz RAM. Figure 1 (b) explores the sub-optimality in solving the regression problem. More
p is
the best approximation (exact solution)  and Yp is the sketched solution. We see that indeed the error
decreases as the size of the sampled matrix grows  and that with a sketch size that is not too big we
get to about a 10% larger objective. In Figure 1 (c) we see that this translates to an increase in error
rate. Encouragingly  a sketch as small as 15% of the number of examples is enough to have a very
small increase in error rate  while still solving the regression problem more than 5 times faster (the
speedup is expected to grow for larger datasets).

speciﬁcally  we plot ((cid:107)Yp − Y (cid:107)F −(cid:13)(cid:13)Y (cid:63)

where Y is the labels matrix  Y (cid:63)

p − Y(cid:13)(cid:13)F

)/(cid:13)(cid:13)Y (cid:63)
p − Y(cid:13)(cid:13)F

Acknowledgements

The authors acknowledge the support from XDATA program of the Defense Advanced Research
Projects Agency (DARPA)  administered through Air Force Research Laboratory contract FA8750-
12-C-0323.

8

 0%10%20%30%40%50%05101520Sketch Size (% of Examples)Speedup  SketchingSampling 0%10%20%30%40%50%10−2100102Sketch Size (% of Examples)Suboptimality of residual  SketchingSampling 0%10%20%30%40%50%34567Sketch Size (% of Examples)Classification Error on Test Set (%)  SketchingSamplingExactReferences
[1] C. Boutsidis and A. Gittens.

Improved matrix algorithms via the Subsampled Randomized
Hadamard Transform. ArXiv e-prints  Mar. 2012. To appear in the SIAM Journal on Matrix
Analysis and Applications.

[2] P. Buhlmann and S. v. d. Geer. Statistics for High-dimensional Data. Springer  2011.
[3] Y. Chang  C. Hsieh  K. Chang  M. Ringgaard  and C. Lin. Low-degree polynomial mapping

of data for svm. JMLR  11  2010.

[4] M. Charikar  K. Chen  and M. Farach-Colton. Finding frequent items in data streams. Theo-
retical Computer Science  312(1):3 – 15  2004. ¡ce:title¿Automata  Languages and Program-
ming¡/ce:title¿.

[5] K. L. Clarkson  P. Drineas  M. Magdon-Ismail  M. W. Mahoney  X. Meng  and D. P. Woodruff.
The Fast Cauchy Transform and faster faster robust regression. CoRR  abs/1207.4684  2012.
Also in SODA 2013.

[6] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model.

In
Proceedings of the 41st annual ACM Symposium on Theory of Computing  STOC ’09  pages
205–214  New York  NY  USA  2009. ACM.

[7] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity
time. In Proceedings of the 45th annual ACM Symposium on Theory of Computing  STOC ’13 
pages 81–90  New York  NY  USA  2013. ACM.

[8] A. Dasgupta  P. Drineas  B. Harb  R. Kumar  and M. Mahoney. Sampling algorithms and

coresets for (cid:96)p regression. SIAM Journal on Computing  38(5):2060–2078  2009.

[9] A. Gilbert and P. Indyk. Sparse recovery using sparse matrices. Proceedings of the IEEE 

98(6):937–947  2010.

[10] N. Halko  P. G. Martinsson  and J. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review  53(2):217–
288  2011.

[11] Q. Le  T. Sarl´os  and A. Smola. Fastfood computing hilbert space expansions in loglinear

time. In Proceedings of International Conference on Machine Learning  ICML ’13  2013.

[12] M. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in

Machine Learning  3(2):123–224  2011.

[13] M. W. Mahoney  P. Drineas  M. Magdon-Ismail  and D. P. Woodruff. Fast approximation of
matrix coherence and statistical leverage. In Proceedings of the 29th International Conference
on Machine Learning  ICML ’12  2012.

[14] X. Meng and M. W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and
applications to robust linear regression. In Proceedings of the 45th annual ACM Symposium
on Theory of Computing  STOC ’13  pages 91–100  New York  NY  USA  2013. ACM.

[15] J. Nelson and H. L. Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser

subspace embeddings. CoRR  abs/1211.1002  2012.

[16] R. Rahimi and B. Recht. Random features for large-scale kernel machines. In Proceedings of

Neural Information Processing Systems  NIPS ’07  2007.

[17] V. Rokhlin and M. Tygert. A fast randomized algorithm for overdetermined linear least-squares

regression. Proceedings of the National Academy of Sciences  105(36):13212  2008.

[18] W. Rudin. Fourier Analysis on Groups. Wiley Classics Library. Wiley-Interscience  New York 

1994.

[19] T. Sarl´os. Improved approximation algorithms for large matrices via random projections. In
Proceeding of IEEE Symposium on Foundations of Computer Science  FOCS ’06  pages 143–
152  2006.

[20] C. Sohler and D. P. Woodruff. Subspace embeddings for the l1-norm with applications. In
Proceedings of the 43rd annual ACM Symposium on Theory of Computing  STOC ’11  pages
755–764  2011.

[21] D. P. Woodruff and Q. Zhang. Subspace embeddings and lp regression using exponential

random variables. In COLT  2013.

9

,Haim Avron
Vikas Sindhwani
David Woodruff
Pranjal Awasthi
Avrim Blum
Or Sheffet
Aravindan Vijayaraghavan