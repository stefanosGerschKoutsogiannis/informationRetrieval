2018,Tight Bounds for Collaborative PAC Learning via Multiplicative Weights,We study the collaborative PAC learning problem recently proposed in Blum  et al.~\cite{BHPQ17}  in which we have $k$ players and they want to learn a target function collaboratively  such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead).  We obtain a collaborative learning algorithm with overhead $O(\ln k)$  improving the one with overhead $O(\ln^2 k)$ in \cite{BHPQ17}.  We also show that an $\Omega(\ln k)$ overhead is inevitable when $k$ is polynomial bounded by the VC dimension of the hypothesis class.  Finally  our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum  et al.~\cite{BHPQ17} on real-world datasets.,Tight Bounds for Collaborative PAC Learning via

Multiplicative Weights∗

Jiecao Chen

Computer Science Department

Indiana University at Bloomington

jiecchen@iu.edu

Qin Zhang

Computer Science Department

Indiana University at Bloomington

qzhangcs@indiana.edu

Yuan Zhou

Computer Science Department

Indiana University at Bloomington

and

Department of Industrial and Enterprise Systems Engineering

University of Illinois at Urbana-Champaign

yuanz@illinois.edu

Abstract

We study the collaborative PAC learning problem recently proposed in Blum
et al. [3]  in which we have k players and they want to learn a target function
collaboratively  such that the learned function approximates the target function
well on all players’ distributions simultaneously. The quality of the collaborative
learning algorithm is measured by the ratio between the sample complexity of the
algorithm and that of the learning algorithm for a single distribution (called the
overhead). We obtain a collaborative learning algorithm with overhead O(ln k) 
improving the one with overhead O(ln2 k) in [3]. We also show that an Ω(ln k)
overhead is inevitable when k is polynomial bounded by the VC dimension of the
hypothesis class. Finally  our experimental study has demonstrated the superiority
of our algorithm compared with the one in Blum et al. [3] on real-world datasets.

1

Introduction

In this paper we study the collaborative PAC learning problem recently proposed in Blum et al. [3].
In this problem we have an instance space X   a label space Y  and an unknown target function f∗ :
X → Y chosen from the hypothesis class F. We have k players with distributions D1  D2  . . .   Dk
labeled by the target function f∗. Our goal is to probably approximately correct (PAC) learn the
target function f∗ for every distribution Di. That is  for any given parameters   δ > 0  we need to
return a function f so that with probability 1 − δ  f agrees with the target f∗ on instances of at least
1 −  probability mass in Di for every player i.
As a motivating example  consider a scenario of personalized medicine where a pharmaceutical
company wants to obtain a prediction model for dose-response relationship of a certain drug based
on the genomic proﬁles of individual patients. While existing machine learning methods are efﬁcient
to learn the model with good accuracy for the whole population  for fairness consideration  it is
also desirable to ensure the model accuracies among demographic subgroups  e.g. deﬁned by
gender  ethnicity  age  social-economic status and etc.  where each of them is associated with a label
distribution.

∗A full version of this paper is available at https://arxiv.org/abs/1805.09217

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

We will be interested in the ratio between the sample complexity required by the best collaborative
learning algorithm and that of the learning algorithm for a single distribution  which is called the
overhead ratio. A naïve approach for collaborative learning is to allocate a uniform sample budget for
each player distribution  and learn the model using all collected samples. In this method  the players
do minimal collaboration with each other and it leads to an Ω(k) overhead for many hypothesis
classes (which is particularly true for the classes with ﬁxed VC dimension – the ones we will focus
on in this paper). In this paper we aim to develop a collaborative learning algorithm with the optimal
overhead ratio.

(cid:16) ln2 k
(cid:16) (ln k+ln δ−1)(d+k)



(cid:17)





(cid:0)d + ln δ−1(cid:1)(cid:1) [10]. We

(cid:0)(d + k) ln −1 + k ln δ−1(cid:1)(cid:17)

that there exists an (  δ)-PAC learning algorithm L δ F with S δ = O(cid:0) 1

Our Results. We will focus on the hypothesis class F = {f : X → Y} with VC dimension d. For
every   δ > 0  let S δ be the sample complexity needed to (  δ)-PAC learn the class F. It is known
remark that we will use the algorithm L as a blackbox  and therefore our algorithms can be easily
extended to other hypothesis classes given their single-distribution learning algorithms.
Given a function g and a set of samples T   let errT (g) = Pr(x y)∈T [g(x) (cid:54)= y] be the error of g on
T . Given a distribution D over X × Y  deﬁne errD(g) = Pr(x y)∼D[g(x) (cid:54)= y] to be the error of
g on D. The (  δ)-PAC k-player collaborative learning problem can be rephrased as follows: For
player distributions D1  D2  . . .   Dk and a target function f∗ ∈ F  our goal is to learn a function
g : X → Y so that Pr[∀i = 1  2  . . . k  errDi (f∗  g) ≤ ] ≥ 1 − δ. Here we allow the learning
algorithm to be improper  that is  the learned function g does not have to be a member of F.
Blum et al. [3] showed an algorithm with sample complexity O
.
When k = O(d)  this leads to an overhead ratio of O(ln2 k) (assuming   δ are constants). In this
paper we propose an algorithm with sample complexity O
(Theorem 4)  which
gives an overhead ratio of O(ln k) when k = O(d) and for constant δ  matching the Ω(ln k) lower
bound proved in Blum et al. [3].
Similarly to the algorithm in Blum et al. [3]  our algorithm runs in rounds and return the plurality
of the functions computed in each round as the learned function g. In each round  the algorithm
adaptively decides the number of samples to be taken from each player distribution  and calls L to
learn a function. While the algorithm in Blum et al. [3] uses a grouping idea and evenly takes samples
from the distribution in each group  our algorithm adopts the multiplicative weight method. In our
algorithm  each player distribution is associated with a weight which helps to direct the algorithm to
distribute the sample budget among all player distributions. After each round  the weight for a player
distribution increases if the function learned in the round is not accurate on the distribution  letting the
algorithm pay more attention to it in the future rounds. We will ﬁrst present a direct application of the
multiplicative weight method which leads to a slightly worse sample complexity bound (Theorem 3) 
and then prove Theorem 4 with more reﬁned algorithmic ideas.
On the lower bound side  the lower bound result in Blum et al. [3] is only for the special case when
k = d. We extend their result to every k and d. In particular  we show that the sample complexity for
collaborative learning has to be Ω(max{d ln k  k ln d}/) for constant δ (Theorem 6). Therefore  the
sample complexity of our algorithm is optimal when k = dO(1). 2
Finally  we have implemented our algorithms and compared with the one in Blum et al. [3] and the
naïve method on several real-world datasets. Our experimental results demonstrate the superiority of
our algorithm in terms of the sample complexity.

Related Work. As mentioned  collaborative PAC learning was ﬁrst studied in Blum et al. [3].
Besides the problem of learning one hypothesis that is good for all players’ distributions (called
the centralized collaborative learning in [3])  the authors also studied the case in which we can use
different hypotheses for different distributions (called personalized collaborative learning). For the
personalized version they obtained an O(ln k) overhead in sample complexity. Our results show that

2We note that this is a stronger statement than the earlier one on the “the optimal overhead ratio of O(ln k)
for k = O(d)” in several aspects. First  the showing the optimal overhead ratio only needs a minimax lower
bound; while in the latter statement we claim the optimal sample complexity for every k and d in the range.
Second  our latter statement works for a much wider parameter range for k and d.

2

we can obtain the same overhead for the (more difﬁcult) centralized version. In a concurrent work
[15]  the authors showed the similar results as in our paper.
Both our algorithms and Adaboost [7] use the multiplicative weights method. While Adaboost places
weights on the samples in the preﬁxed training set  our algorithms place weights on the distributions
of data points  and adaptively acquire new samples to achieve better accuracy. Another important
feature of our improved algorithm is that it tolerates a few “failed rounds” in the multiplicative
weights method  which requires more efforts in its analysis and is crucial to shaving the extra ln k
factor when k = Θ(d).
Balcan et al. [1] studied the problem of ﬁnding a hypothesis that approximates the target function well
on the joint mixture of k distributions of k players. They focused on minimizing the communication
between the players  and allow players to exchange not only samples but also hypothesis and
other information. Daume et al. [11  12] studied the problem of computing linear separators in a
similar distributed communication model. The communication complexity of distributed learning
has also been studied for a number of other problems  including principal component analysis [13] 
clustering [2  9]  multi-task learning [16]  etc.
Another related direction of research is the multi-source domain adaption problem [14]  where we
have k distributions  and a hypothesis with error at most  on each of the k distributions. The task
is to combine the k hypotheses to a single one which has error at most k on any mixture of the k
distribution. This problem is different from our setting in that we want to learn the “global” hypothesis
from scratch instead of combine the existing ones.

2 The Basic Algorithm

In this section we propose an algorithm for collaborative learning using the multiplicative weight
method. The algorithm is described in Algorithm 1  using Algorithm 2 as a subroutine.

We brieﬂy describe Algorithm 1 in words. We
start by giving a unit weight to each of the k
player. The algorithm runs in T = O(ln k)
rounds  and players’ weights will change at each
round. At round t  we take a set of samples S(t)
from the average distribution of the k players
weighted by their weights. We then learn a clas-
siﬁer g(t) for samples in S(t)  and test for each
player i whether g(t) agrees with the target func-
tion f∗ with probability mass at least 1 − /6 on
distribution Di. If yes then we keep the weight of
the i-th player; otherwise we multiply its weight
by a factor of 2  so that Di will attract more at-
tention in the future learning process. Finally  we
return a classiﬁer g which takes the plurality vote3
of the T classiﬁers g(0)  g(1)  . . .   g(T−1) that we
have constructed. We note that we make no ef-
fort to optimize the constants in the algorithms
and their theoretical analysis; while in the experi-
ment section  we will tune the constants for better
empirical performance.
The following lemma shows that TEST returns 
with high probability  the desired set of players
where g is an accurate hypothesis for its own dis-
tribution. We say a call to TEST successful if
its returning set has the properties described in
Lemma 1. The omitted proofs in this section can
be found in Appendix ??.

i ← 1 for each

Algorithm 1 BASICMW
1: Let the initial weight w(0)
player i ∈ {1  2  . . .   k}.
2: Let T ← 10 ln k.
3: for t ← 0 to T − 1 do
i(cid:80)k
Let p(t)(i) ← w(t)
4:
Let D(t) ←(cid:80)K

for each i ∈
{1  2  . . .   k} so that p(t)(·) deﬁnes a prob-
ability distribution.

i=1 w(t)

i=1 p(t)(i)Di.

i

Let S(t) be a set of S 
120  
from D(t). Let g(t) ← L 
120  

δ

4(t+1)2

samples
4(t+1)2  F (S(t)).

δ

5:
6:

if i ∈ Z (t) then

Let Z (t) ← TEST(g(t)  k  t    δ).
for each i ∈ {1  2  . . .   k} do

7:
8:
9:
10:
11:
12:
13: return g = Plurality(g(0)  . . .   g(T−1)).

← w(t)
← 2 · w(t)

w(t+1)

w(t+1)

else

.

i

i

i

i

(cid:16) k·4(t+1)2

Algorithm 2 Accuracy Test (TEST(g  k  t    δ))
1: for each i ∈ {1  2  . . .   k} do Let Ti be a set

(cid:17)
samples from Di.
2: return {i | errTi(g) ≤ 
6}.

of 432


ln

δ

3I.e. the most frequent value  where ties broken arbitrarily.

3

Lemma 1 With probability at least 1 −
includes 1) each i such that errDi(g) ≤ 
Given a function g and a distribution D  we say that g is a good candidate for D if errD(g) ≤ 
4. The
following lemma shows that if we have a set of functions where most of them are good candidates for
D  then the plurality vote of these functions also has good accuracy for D.

4(t+1)2   TEST(g  k  t    δ) returns a set of players that

12   2) none of the i such that errDi(g) > 
4 .

δ

Lemma 2 Let g1  g2  . . .   gm be a set of functions such that more than 70% of them are good
candidates for D. Let g = Plurality(g1  g2  . . .   gm)  we have that errD(g) ≤ .
We let the E be the event that every call of the learner L and TEST is successful. It is straightforward
to see that

Pr[E] ≥ 1 − +∞(cid:88)

t=0

δ

4(t + 1)2 · 2 = 1 − δ · π2

24

> 1 − δ.

(1)

Now we are ready to prove the main theorem for Algorithm 1.

Theorem 3 Algorithm 1 has the following properties.

1. With probability at least 1 − δ  it returns a function g such that errDi(g) ≤  for all

i ∈ {1  2  . . .   k}.

2. Its sample complexity is O

(d + k ln δ−1 + k ln k)

.

(cid:18) ln k



(cid:19)

Proof. While the sample complexity is easy to verify  we focus on the proof of the ﬁrst property. In
particular  we show that when E happens (which is with probability at least 1 − δ by (1))  we have
errDi(g) ≤  for all i ∈ {1  2  . . .   k}.
For now till the end of the proof  we assume that E happens.
For each round t  we have that
inequality  we have that Pri∼p(t)(·)

120 ≥ errD(t)(g(t)) = Ei∼p(t)(·)[errDi(g(t))] . Therefore  by Markov

(cid:2)errDi(g(t)) > 
(cid:3) ≤ .1 . In other words 
1(cid:80)k

(cid:88)

p(t)(i) =

w(t)

12

.



i

i

i:errDi (g(t))> 

12

i

12

  we have

i=1 w(t)

i:errDi (g(t))> 

i=1 w(t+1)

.1 ≥ (cid:88)
Now consider the total weight(cid:80)k
k(cid:88)
By Lemma 1 and E  we have that(cid:88)
≤ 1.1(cid:80)k
Combining (2)  (3)  and (4)  we have(cid:80)k
have the following inequality holds for every t = 0  1  2  . . . :(cid:80)k

k(cid:88)
≤ (cid:88)

i=1 w(t+1)

i:errDi (g(t))> 

(cid:88)

w(t+1)

w(t+1)

i(cid:54)∈Z(t)

w(t)

i +

i(cid:54)∈Z(t)

i=1

i=1

=

12

i

i

i

i

w(t+1)

.

i

w(t+1)

.

. Since(cid:80)k

i=1 w(t)

i

i=1 w(t)

i ≤ 1.1t · k .

i=1 w(0)

i = k  we

(2)

(3)

(4)

4  and this will conclude the proof of this theorem thanks to Lemma 2.

Now let us focus on an arbitrary player i. We will show that for at least 70% of the rounds t  we have
errDi(g(t)) ≤ 
Suppose the contrary: for more than 30% of the rounds  we have errDi(g(t)) > 
round t  we have i (cid:54)∈ Z (t) because of Lemma 1 and E  and therefore w(t+1)
we have w(T )
i=1 w(T )
contradiction for T = 10 ln k.

4. At each of such
. Therefore 
i ≤ 1.1T · k  which is a
(cid:117)(cid:116)

i ≥ 2.3T . Together with (4)  we have 2.3T ≤ wT

i ≤(cid:80)k

= 2 · w(t)

i

i

4

3 The Quest for Optimality via Robust Multiplicative Weights

In this section we improve the result in Theorem 3 to get an optimal algorithm when k is polynomially
bounded by d (see Theorem 4; the optimality will be shown in Section 4). In fact  our improved
algorithm (Algorithm 3 using Algorithm 4 as a subroutine)  is almost the same as Algorithm 1 (using
Algorithm 2 as a subroutine). We highlight the differences as follows.

1. The total number of iterations at Line 2 of Algorithm 1 is changed to ˜T = 2000 ln(k/δ).
2. The failure probability for the single-distribution learning algorithm L at Line 6 of Algo-

rithm 1 is increased to a constant 1/100.

3. The number of times that each distribution is sampled at Line 1 of Algorithm 2 is reduced to

432



ln(100).

Although these changes seem minor  it requires substantial technical efforts to establish Theorem 4.
We describe the challenge and sketch our solution as follows.
While the 2nd and 3rd items lead to the key reduction of the sample complexity  they make it
impossible to use the union bound and claim that with high probability “every call of L and TEST is
successful” (see Inequality (1) in the analysis for Algorithm 1).
To address this problem  we will make our multiplicative weight analysis robust against occasionally
failed rounds so that it works when “most calls of L and WEAKTEST are successful”.

i

i

i

i=1 w(t)

i=1 w(t+1)

≤ 1.1(cid:80)k

weights W (t) = (cid:80)k
istic statement (cid:80)k

In more details  we will ﬁrst work on the total
at the t-th round 
and show that conditioned on the t-th round 
E[W (t+1)] is upper bounded by 1.13W (t) (where
in contrast we had a stronger and determin-
i=1 w(t)
in the analysis for the basic algorithm). Us-
ing Jensen’s inequality we will be able to de-
rive that E[ln W (t+1)]
is upper bounded by
(ln 1.13+ln W (t)). Then  using Azuma’s inequal-
ity for supermartingale random variables  we
will show that with high probability  ln W ( ˜T ) ≤
˜T (ln 1.18) + ln W (0)  i.e. W ( ˜T ) ≤ 1.18 ˜T · k 
i ≤ 1.1t · k in
the basic proof. On the other hand  recall that in
the basic proof we had to show that if for more
than 30% of the rounds  the g(t) function is not a
good candidate for a player distribution Di  then
i ≥ 2.3T . In the analysis for the im-
we have w(T )
proved algorithm  because the WEAKTEST pro-
cedure fails with much higher probability  we
need to use concentration inequalities and derive
i ≥ 2.25 ˜T ). Fi-
a slightly weaker statement (w( ˜T )
nally  we will put everything together using the
same proof via contradiction argument  and prove
the following theorem.

which corresponds to(cid:80)k

i=1 w(t)

i ← 1 for each

Algorithm 3 MWEIGHTS
1: Let the initial weight w(0)
player i ∈ {1  2  3  . . .   k}.
2: Let ˜T ← 2000 ln(k/δ).
3: for t ← 0 to ˜T − 1 do
i(cid:80)k
Let p(t)(i) ← w(t)
4:
Let D(t) ←(cid:80)K

i

for each i ∈
{1  2  3  . . .   k} so that p(t)(·) deﬁnes a prob-
ability distribution.

i=1 w(t)

i=1 p(t)(i)Di.

samples from

Let S(t) be a set of S 

100

120   1

D(t). Let g(t) ← L 

120   1
100  F (S(t)).
Let Z (t) ← WEAKTEST(k  g(t)    δ).
for each i ∈ {1  2  3  . . .   k} do

7:
8:
9:
10:
11:
12:
13: return g = Plurality(g(0)  . . .   g( ˜T−1)).

← w(t)
← 2 · w(t)

if i ∈ Z (t) then

w(t+1)

w(t+1)

else

.

i

i

i

i

5:
6:

Accuracy

Algorithm 4 Weak
(WEAKTEST(g  k    δ))
1: for each i ∈ {1  2  3  . . .   k} do Let Ti be a
2: return {i | errTi(g) ≤ 
6}.

ln (100) samples from Di.

set of 432


Test

Theorem 4 Algorithm 3 has the following properties.

1. With probability at least 1 − δ  it returns a function g such that errDi(g) ≤  for all

i ∈ {1  2  . . .   k}.

2. Its sample complexity is O

(cid:18) (ln k + ln δ−1)(d + k)

(cid:19)

.



5

Now we prove Theorem 4.
Similarly to Lemma 1  applying Proposition ?? (but without the union bound)  we have the following
lemma for WEAKTEST.
Lemma 5 For each player i  with probability at least 1− 1
then i ∈ WEAKTEST(g  k    δ); 2) if errDi (g) > 

100   the following hold  1) if errDi (g) ≤ 
12  

4   then i (cid:54)∈ WEAKTEST(g  k    δ).

Let the indicator variable ψ(t)
not happen; and let ψ(t)
for each player i  we have Pr

(cid:104)(cid:80) ˜T−1
Now let J1 be the event that(cid:80) ˜T−1

i = 1 if the desired event described in Lemma 5 for i and time t does
100. By Proposition ?? 
k5 .

i = 0 otherwise. By Lemma 5  we have E[ψ(t)
3 · 42 ·

i > .05 ˜T
i ≤ .05 ˜T for every i. Via a union bound  we have that

(cid:17) ≤ exp

(cid:105) ≤ exp

(cid:17) ≤ δ

(cid:16)− 5 ˜T

] ≤ 1
˜T
100

(cid:16)− 1

t=0 ψ(t)

t=0 ψ(t)

100

i

Pr[J1] ≥ 1 − δ
k4 .

(5)

Let the indicator variable χ(t) = 1 if the learner L fails at time t; and let χ(t) = 0 otherwise. We have

Let W (t) =(cid:80)k

i=1 w(t)

i be the total weights at time t. For each t  similarly to (3)  we have

E(cid:104)

χ(t) | time 0  1  . . .   t − 1

(cid:105) ≤ 1

.

100

W (t+1) = W (t) +

w(t)

i

.

(cid:88)

i(cid:54)∈Z(t)

(6)

(7)

(cid:2)errDi(g(t)) > 

12

(8)

(cid:3) ≤ .1 

(9)

For each i such that errDi (g(t)) ≤ 
we take the expectation over the randomness of WEAKTEST at time t  we have 

12  by Lemma 5  we know that Pr[i (cid:54)∈ Z (t)] ≤ 1

100. Therefore  if

 (cid:88)

i(cid:54)∈Z(t)

E

 ≤

w(t)

i

(cid:88)
(cid:88)

≤

i:errDi (g(t))> 

12

i:errDi (g(t))> 

12



w(t)

i

 (cid:88)
· k(cid:88)

i

i=1

w(t)

.

i:errDi (g(t))≤ 

12

i + E
w(t)

w(t)

i +

1
100

When χ(t) = 0  similarly to the proof of Theorem 3  we have Pri∼p(t)(·)
and

.1 ≥ (cid:88)
E(cid:104)
W (t+1)(cid:12)(cid:12) χ(t) = 0 and W (0)  . . .   W (t)(cid:105) ≤ 1.11 · W (t).

1(cid:80)k

i:errDi (g(t))> 

i:errDi (g(t))> 

(cid:88)

i=1 w(t)

p(t)(i) =

12

12

i

i

w(t)

.

Combining (7)  (8)  and (9)  we have (when χ(t) = 0)

(10)

(6)  we

Together with
ln

have E(cid:2)W (t+1)(cid:12)(cid:12) W (0)  . . .   W (t)(cid:3)
(cid:16) E(cid:2)W (t+1)(cid:12)(cid:12) χ(t) = 0 and W (0)  . . .   W (t)(cid:3) · Pr(cid:2)χ(t) = 0 |W (0)  . . .   W (t)(cid:3) + 2W (t)
Pr(cid:2)χ(t) = 1 |W (0)  . . .   W (t)(cid:3)(cid:17) ≤ (1.11 + 0.02)W (t) = 1.13W (t).
Let Q(t) = ln W (t+1)/W (t)  and by Jensen’s inequality  we have E(cid:2)Q(t)(cid:12)(cid:12) W (0)  . . .   W (t)(cid:3) ≤
ln E(cid:2)W (t+1)/W (t)(cid:12)(cid:12) W (0)  . . .   W (t)(cid:3).
Therefore  we have E(cid:2)Q(t)(cid:12)(cid:12) Q(0)  . . .   Q(t−1)(cid:3) =
E(cid:2)Q(t)(cid:12)(cid:12) W (0)  . . .   W (t)(cid:3) ≤ ln E(cid:2)W (t+1)/W (t)(cid:12)(cid:12) W (0)  . . .   W (t)(cid:3) ≤ ln(1.11 + .02) = ln 1.13.
Now let ˜Q(t) = (cid:80)t−1

z=0 Q(z) − t · ln 1.13 for all t = 0  1  2  . . . . We have that { ˜Q(t)} is a super-
martingale and | ˜Q(t+1) − ˜Q(t)| ≤ ln 2 for all t = 0  1  2  . . . . By Proposition ?? and noticing that

1.11 · W (t)

=
·

≤

6

ln 1.18 − ln 1.13 > .04  we have Pr

(cid:17) ≤ δ
k2 . Let J2 be the event that W ( ˜T ) ≤ 1.18 ˜T · k ⇔(cid:80) ˜T−1

t=0 Q(t) > (ln 1.18) ˜T

(cid:16)− .042· ˜T

2·(ln 2)2

exp
we have that

(cid:104)(cid:80) ˜T−1

(cid:105) ≤ Pr

(cid:104) ˜Q( ˜T ) − ˜Q(0) > .04 ˜T

(cid:105) ≤

t=0 Q(t) ≤ (ln 1.18) ˜T  

Now let J = J1 ∩ J2  combining (5) and (11)  for k ≥ 2  we have

Pr[J2] ≥ 1 − δ
k2 .

Pr[J ] ≥ 1 − δ
k

.

(11)

(12)

Now we are ready to prove Theorem 4 for Algorithm 3.
Proof. [of Theorem 4] While the sample complexity is easy to verify  we focus on the proof of the
ﬁrst property. In particular  we show that when J happens (which is with probability at least 1 − δ
by (12))  we have errDi(g) ≤  for all i ∈ {1  2  3  . . .   k}.
Let us consider an arbitrary player i. We will show that when J happens  for at least 70% the times t 
we have errDi(g(t)) ≤ 
Suppose the contrary: for more than 30% of the times  we have errDi(g(t)) > 
more than 30%− 5% = 25% of the times t  we have i (cid:54)∈ Z (t). Therefore  we have w( ˜T )
the other hand  by J2 we have W ( ˜T ) ≤ 1.2 ˜T . Therefore  we reach 2.25 ˜T ≤ w( ˜T )
which is a contradiction to ˜T = 2000 ln(k/δ).

4. Because of J1  for
i ≥ 2.25 ˜T . On
i ≤ W ( ˜T ) ≤ 1.18 ˜T·k 
(cid:117)(cid:116)

4  and this will conclude the proof of this theorem thanks to Lemma 2.

4 Lower Bound

We show the following lower bound result  which matches our upper bound (Theorem 3) when
k = (1/δ)Ω(1) and k = dO(1).

Theorem 6 In collaborative PAC learning with k players and a hypothesis class of VC-dimension d 
for any   δ ∈ (0  0.01)  there exists a hard input distribution on which any (  δ)-learning algorithm
A needs Ω(max{d ln k  k ln d}/) samples in expectation  where the expectation is taken over the
randomness used in obtaining the samples and the randomness used in drawing the input from the
input distribution.

The proof of Theorem 6 is similar to that for the lower bound result in [3]; however  we need to
generalize the hard instance provided in [3] in two different cases. We brieﬂy discuss the high level
ideas of our generalization here  and leave the full proof to Appendix ?? due to space constraints.
The lower bound proof in [3] (for k = d) performs a reduction from a simple player problem to a
k-player problem  such that if we can (  δ)-PAC learn the k-party problem using m samples in total 
then we can (  10δ/(9k))-PAC learn the single player problem using O(m/k) samples. Now for the
case when d > k  we need to change the single player problem used in [3] whose hypothesis class
is of VC-dimension Θ(1) to one whose hypothesis class is of VC-dimension Θ(d/k). For the case
when d ≤ k  we essentially duplicate the hard instance for a d-player problem k/d times  getting a
hard instance for a k-player problem  and then perform the random embedding reduction from the
single player problem to the k-player problem. See Appendix ?? for details.

5 Experiments

We present in this section a set of experimental results which demonstrate the effectiveness of our
proposed algorithms.
Our algorithms are based on the assumption that given a hypothesis class  we are able to compute its
VC dimension d and access an oracle to compute an (  δ)-classiﬁer with sample complexity S δ. In
practice  however  it is usually computationally difﬁcult to compute the exact VC dimension for a

7

given hypothesis class. Also  the VC dimension usually only proves to be a very loose upper bound
for the sample complexity needed for an (  δ)-classiﬁer.
To address these practical difﬁculties  in our experiment  we treat the VC dimension d as a parameter
to control the sample budget. More speciﬁcally  we will ﬁrst choose a concrete model as the oracle;
in our implementation  we choose the decision tree. We then set the parameter δ = 0.9 and gradually
increase d to determine the sample budget. For each ﬁxed sample budget (i.e.  each ﬁxed d)  we run
the algorithm for 100 times and test whether the following happens 
errDi (g) ≤  for all i] ≥ 0.9.

(cid:99)Pr[max

(13)

i

Here  is a parameter we choose and g is the classiﬁer returned by the collaborative learning algorithm

to be tested. The empirical probability (cid:99)Pr[·] in (13) is calculated over the 100 runs. We ﬁnally report

the minimum number of samples consumed by the algorithm to achieve (13).
Note that in our theoretical analysis  we did not try to optimize the constants. Instead  we tune
the constants for both CENLEARN and MWEIGHTS for better performance. Please ﬁnd more
implementation details in the appendix.

Datasets. We will test the collaborative learning algorithms using the following data sets.
MAGIC-EVEN [4]. This data set is generated to simulate registration of high energy gamma particles
in an atmospheric Cherenkov telescope. There are 19  020 instances and each belongs to one of the
two classes (gamma and hadron). There are 11 attributes in each data point. We randomly partition
this data set into k = 10 subsets (namely  D1  . . .   Dk).
MAGIC-1. The raw data set is the same as we have in MAGIC-EVEN. Instead of random partitioning 
we partition the data set into D1 and D2 based on the two different classes  and make k − 2 more
copies of D2 so that D2  D3  . . .   Dk are identical. In our case we set k = 10.
MAGIC-2. This data set differs from MAGIC-1 in the way of constructing D1 and D2: we partition
the original data set into D1 and D2 based on the ﬁrst dimension of the feature vectors; we then make
duplicates for D2. Here we again set k = 10.
WINE [5]. This data set contains physicochemical tests for white wine  and the scores of the wine
range from 0 to 10. There are 4  898 instances and there are 12 attributes in the feature vectors. We
partition the data set into D1  . . .   D4 based on the ﬁrst two dimensions.
EYE. This data set consists of 14 EEG values and a value indicating the eye state. There are 14  980
instances in this data set. We partition it into D1  . . .   D4 based on the ﬁrst two dimensions.
LETTER [8]. This data set has 20  000 instances  each in R16. There are 26 classes  each representing
one of 26 capital letters. We partition this data set into k = 12 subsets based on the ﬁrst 4 dimensions
of the feature vectors.

Tested Algorithms. We compare our algorithms with the following two baseline algorithms 
NAIVE. In this algorithm we treat all distributions D1  . . .   Dk equally. That is  given a budget z 
we sample z training samples from D = 1
i=1 Di. We then train a classiﬁer (decision tree) using
k
those samples.
CENLEARN  this is the implementation of the algorithm proposed by Blum et al. [3].
Since our Algorithm 1 and Algorithm 3 are very similar  and Algorithm 3 has better theoretical
guarantee  we will only test Algorithm 3  denoted as MWEIGHTS  in our experiments.

(cid:80)k

Experimental Results and Discussion. The experimental results are presented in Figure 1. We
test the algorithms for each data set using multiple values of the error threshold   and report the
sample complexity for NAIVE  MWEIGHTS and CENLEARN.
In Figure 1a  we notice that NAIVE uses less samples than its competitors. This phenomenon is
predictable because in MAGIC-EVEN  D1  . . .   Dk are constructed via random partitioning  which is
the easiest case for NAIVE. Since MWEIGHTS and CENLEARN need to train multiple classiﬁers 
each classiﬁer will get fewer training samples than NAIVE when the total budgets are the same.

8

(a) MAGIC-EVEN

(b) MAGIC-1

(c) MAGIC-2

(d) WINE

(e) EYE

(f) LETTER

Figure 1: Sample complexity versus error threshold .

(cid:80)k

In Figure 1b and Figure 1c  D1  . . .   Dk are constructed in a way that D2  D3  . . .   Dk are identical 
and D1 is very different from other distributions. Thus the overall distribution (i.e.  D = 1
i=1 Di)
k
used to train NAIVE is quite different from the original data set. One can observe from those two
ﬁgures that MWEIGHTS still works quite well while NAIVE suffers.
In Figure 1b-Figure 1f  one can observe that MWEIGHTS uses fewer samples than its competitors in
almost all cases  which shows the superiority of our proposed algorithm. CENLEARN outperforms
NAIVE in general. However  NAIVE uses slightly fewer samples than CENLEARN in some cases
(e.g.  Figure 1d). This may due to the fact that the distributions D1  . . .   Dk in those cases are not
hard enough to show the superiority of CENLEARN over NAIVE.
To summarize  our experimental results show that MWEIGHTS and CENLEARN need fewer samples
than NAIVE when the input distributions D1  . . .   Dk are sufﬁciently different. MWEIGHTS consis-
tently outperforms CENLEARN  which may due to the facts that MWEIGHTS has better theoretical
guarantees and is more straightforward to implement.

6 Conclusion

In this paper we consider the collaborative PAC learning problem. We have proved the optimal
overhead ratio and sample complexity  and conducted experimental studies to show the superior
performance of our proposed algorithms.
One open question is to consider the balance of the numbers of queries made to each player  which
can be measured by the ratio between the largest number of queries made to a player and the average
number of queries made to the k players. The proposed algorithms in this paper may attain a balance
ratio of Ω(k) in the worst case. It will be interesting to investigate:

1. Whether there is an algorithm with the same sample complexity but better balance ratio?
2. What is the optimal trade-off between sample complexity and balance ratio?

9

Acknowledgments

Jiecao Chen and Qin Zhang are supported in part by NSF CCF-1525024  CCF-1844234 and IIS-
1633215. Part of the work was done when Yuan Zhou was visiting the Shanghai University of Finance
and Economics.

References
[1] M. Balcan  A. Blum  S. Fine  and Y. Mansour. Distributed learning  communication complexity

and privacy. In COLT  pages 26.1–26.22  2012.

[2] M. Balcan  S. Ehrlich  and Y. Liang. Distributed k-means and k-median clustering on general

communication topologies. In NIPS  pages 1995–2003  2013.

[3] A. Blum  N. Haghtalab  A. D. Procaccia  and M. Qiao. Collaborative PAC learning. In NIPS 

pages 2389–2398  2017.

[4] R. Bock  A. Chilingarian  M. Gaug  F. Hakl  T. Hengstebeck  M. Jirina  J. Klaschka  E. Kotrc 
P. Savick`y  S. Towers  et al. Methods for multidimensional event classiﬁcation: a case study. as
Internal Note in CERN  2003.

[5] P. Cortez  A. Cerdeira  F. Almeida  T. Matos  and J. Reis. Modeling wine preferences by data

mining from physicochemical properties. Decision Support Systems  47(4):547–553  2009.

[6] A. Ehrenfeucht  D. Haussler  M. J. Kearns  and L. G. Valiant. A general lower bound on the

number of examples needed for learning. Inf. Comput.  82(3):247–261  1989.

[7] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of computer and system sciences  55(1):119–139  1997.

[8] P. W. Frey and D. J. Slate. Letter recognition using holland-style adaptive classiﬁers. Machine

Learning  6:161–182  1991.

[9] S. Guha  Y. Li  and Q. Zhang. Distributed partial clustering. In SPAA  pages 143–152  2017.

[10] S. Hanneke. The optimal sample complexity of pac learning. The Journal of Machine Learning

Research  17(1):1319–1333  2016.

[11] H. D. III  J. M. Phillips  A. Saha  and S. Venkatasubramanian. Efﬁcient protocols for distributed

classiﬁcation and optimization. In ALT  pages 154–168  2012.

[12] H. D. III  J. M. Phillips  A. Saha  and S. Venkatasubramanian. Protocols for learning classiﬁers

on distributed data. In AISTATS  pages 282–290  2012.

[13] Y. Liang  M. Balcan  V. Kanchanapally  and D. P. Woodruff. Improved distributed principal

component analysis. In NIPS  pages 3113–3121  2014.

[14] Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation with multiple sources. In

NIPS  pages 1041–1048  2008.

[15] H. L. Nguyen and L. Zakynthinou. Improved Algorithms for Collaborative PAC Learning.

arXiv preprint arXiv:1805.08356  2018.

[16] J. Wang  M. Kolar  and N. Srebro. Distributed multi-task learning. In AISTATS  pages 751–760 

2016.

10

,Jiecao Chen
Qin Zhang
Yuan Zhou