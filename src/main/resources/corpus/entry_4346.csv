2019,Powerset Convolutional Neural Networks,We present a novel class of convolutional neural networks (CNNs) for set functions  i.e.  data indexed with the powerset of a finite set. The convolutions are derived as linear  shift-equivariant functions	for various notions of shifts on set functions. The framework is fundamentally different from graph convolutions based on the Laplacian  as it provides not one but several basic shifts  one for each element in the ground set. Prototypical experiments with several set function classification tasks on synthetic datasets and on datasets derived from real-world hypergraphs demonstrate the potential of our new powerset CNNs.,Powerset Convolutional Neural Networks

Chris Wendler

Department of Computer Science

ETH Zurich  Switzerland

chris.wendler@inf.ethz.ch

Dan Alistarh
IST Austria

dan.alistarh@ist.ac.at

Markus Püschel

Department of Computer Science

ETH Zurich  Switzerland
pueschel@inf.ethz.ch

Abstract

We present a novel class of convolutional neural networks (CNNs) for set functions 
i.e.  data indexed with the powerset of a ﬁnite set. The convolutions are derived
as linear  shift-equivariant functions for various notions of shifts on set functions.
The framework is fundamentally different from graph convolutions based on the
Laplacian  as it provides not one but several basic shifts  one for each element in
the ground set. Prototypical experiments with several set function classiﬁcation
tasks on synthetic datasets and on datasets derived from real-world hypergraphs
demonstrate the potential of our new powerset CNNs.

1

Introduction

Deep learning-based methods are providing state-of-the-art approaches for various image learning
and natural language processing tasks  such as image classiﬁcation [22  28]  object detection [41] 
semantic image segmentation [42]  image synthesis [20]  language translation / understanding [23  62]
and speech synthesis [58]. However  an artifact of many of these models is that regularity priors
are hidden in their fundamental neural building blocks  which makes it impossible to apply them
directly to irregular data domains. For instance  image convolutional neural networks (CNNs) are
based on parametrized 2D convolutional ﬁlters with local support  while recurrent neural networks
share model parameters across different time stamps. Both architectures share parameters in a way
that exploits the symmetries of the underlying data domains.
In order to port deep learners to novel domains  the according parameter sharing schemes reﬂecting
the symmetries in the target data have to be developed [40]. An example are neural architectures for
graph data  i.e.  data indexed by the vertices of a graph. Graph CNNs deﬁne graph convolutional
layers by utilizing results from algebraic graph theory for the graph Laplacian [9  51] and message
passing neural networks [18  47] generalize recurrent neural architectures from chain graphs to
general graphs. With these building blocks in place  neural architectures for supervised [16  18  50] 
semi-supervised [25] and generative learning [52  59] on graphs have been deployed. These research
endeavors fall under the umbrella term of geometric deep learning (GDL) [10].
In this work  we want to open the door for deep learning on set functions  i.e.  data indexed by
the powerset of a ﬁnite set. There are (at least) three ways to do so. First  set functions can be
viewed as data indexed by a hypercube graph  which makes graph neural nets applicable. Second 
results from the Fourier analysis of set functions based on the Walsh-Hadamard-transform (WHT)
[15  33  54] can be utilized to formulate a convolution for set functions in a similar way to [51].
Third  [36] introduces several novel notions of convolution for set functions (powerset convolution)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

as linear  equivariant functions for different notions of shift on set functions. This derivation parallels
the standard 2D-convolution (equivariant to translations) and graph convolutions (equivariant to
the Laplacian or adjacency shift) [34]. A general theory for deriving new forms of convolutions 
associated Fourier transforms and other signal processing tools is outlined in [38].
Contributions Motivated by the work on generalized convolutions and by the potential utility of
deep learning on novel domains  we propose a method-driven approach for deep learning on irregular
data domains and  in particular  set functions:

• We formulate novel powerset CNN architectures by integrating recent convolutions [36] and

proposing novel pooling layers for set functions.

• As a protoypical application  we consider the set function classiﬁcation task. Since there is
little prior work in this area  we evaluate our powerset CNNs on three synthetic classiﬁcation
tasks (submodularity and spectral properties) and two classiﬁcation tasks on data derived
from real-world hypergraphs [5]. For the latter  we design classiﬁers to identify the origin
of the extracted subhypergraph. To deal with hypergraph data  we introduce several set-
function-based hypergraph representations.

• We validate our architectures experimentally  and show that they generally outperform
the natural fully-connected and graph-convolutional baselines on a range of scenarios and
hyperparameter values.

2 Convolutions on Set Functions

We introduce background and deﬁnitions for set functions and associated convolutions. For context
and analogy  we ﬁrst brieﬂy review prior convolutions for 2D and graph data. From the signal
processing perspective  2D convolutions are linear  shift-invariant (or equivariant) functions on
images s : Z2 → R; (i  j) (cid:55)→ si j  where the shifts are the translations T(k l)s = (si−k j−l)i j∈Z2.
The 2D convolution thus becomes

(h ∗ s)i j =

hk lsi−k j−l.

(1)

(cid:88)

k l∈Z2

Equivariance means that all convolutions commute with all shifts: h ∗ (T(k l)s) = T(k l)(h ∗ s).
Convolutions on vertex-indexed graph signals s : V → R; v (cid:55)→ sv are linear and equivariant with
respect to the Laplacian shifts Tks = Lks  where L is the graph Laplacian [51].
Set functions With this intuition in place  we now consider set functions. We ﬁx a ﬁnite set
N = {x1  . . .   xn}. An associated set function is a signal on the powerset of N:

s : 2N → R; A (cid:55)→ sA.

(2)

Powerset convolution A convolution for set functions is obtained by specifying the shifts to which
it is equivariant. The work in [36] speciﬁes TQs = (sA\Q)A⊆N as one possible choice of shifts for
Q ⊆ N. Note that in this case the shift operators are parametrized by the monoid (2N  ∪)  since for
all s

TQ(TRs) = (sA\R\Q)A⊆N = (sA\(R∪Q))A⊆N = TQ∪Rs 

which implies TQTR = TQ∪R. The corresponding linear  shift-equivariant powerset convolution is
given by [36] as

(h ∗ s)A =

hQsA\Q.

(3)

(cid:88)

Q⊆N

Note that the ﬁlter h is itself a set function. Table 1 contains an overview of generalized convolutions
and the associated shift operations to which they are equivariant to.
Fourier transform Each ﬁlter h deﬁnes a linear operator Φh := (h ∗ ·) obtained by ﬁxing h in (3).
It is diagonalized by the powerset Fourier transform

(cid:18)1

(cid:19)⊗n

(cid:18)1

(cid:19)

(cid:18)1

(cid:19)

F =

0
1 −1

=

0
1 −1

⊗ ··· ⊗

0
1 −1

 

(4)

2

image
graph Laplacian
graph adjacency
group
group spherical
powerset

signal

(si j)i j
(sv)v∈V
(sv)v∈V
(sg)g∈G
(sR)R∈SO(3)
(sA)A⊆N

shifted signal
(si−k j−l)i j∈Z
(Lks)v∈V
(Aks)v∈V
(sq−1g)g∈G
(sQ−1R)R∈SO(3)
(sA\Q)A⊆N

convolution

(cid:80)
((cid:80)
((cid:80)
(cid:80)
(cid:82) hQsQ−1Rdµ(Q)
(cid:80)

k l hk lsi−k j−l
k hkLks)v
k hkAks)v
q hqsq−1g

Q hQsA\Q

reference CNN
standard
[51]
[44]
[53]
[12]
[36]

standard
[9]
[55]
[13]
[12]
this paper

Table 1: Generalized convolutions and their shifts.

where ⊗ denotes the Kronecker product. Note that F −1 = F in this case and that the spectrum is
also indexed by subsets B ⊆ N. In particular  we have

F ΦhF −1 = diag((˜hB)B⊆N ) 

(5)
in which ˜h denotes the frequency response of the ﬁlter h [36]. We denote the linear mapping from h
to its frequency response ˜h by ¯F   i.e.  ˜h = ¯F h.
Other shifts and convolutions There are several other possible deﬁnitions of set shifts  each com-
ing with its respective convolutions and Fourier transforms [36]. Two additional examples are
T (cid:5)
Qs = (sA∪Q)A⊆N and the symmetric difference T •
Qs = (s(A\Q)∪(Q\A))A⊆N [54]. The associated
convolutions are  respectively 

(h ∗ s)A =

hQsA∪Q and

(h ∗ s)A =

hQs(A\Q)∪(Q\A).

(6)

(cid:88)

Q⊆N

(cid:88)

Q⊆N

particular  1-localized ﬁlters (h ∗ s)A = h∅sA +(cid:80)

Localized ﬁlters Filters h with hQ = 0 for |Q| > k are k-localized in the sense that the evaluation
of (h ∗ s)A only depends on evaluations of s on sets differing by at most k elements from A. In
x∈N h{x}sA\{x} are the counterpart of one-hop
ﬁlters that are typically used in graph CNNs [25]. In contrast to the omnidirectional one-hop graph
ﬁlters  these one-hop ﬁlters have one direction per element in N.

2.1 Applications of Set Functions

Set functions are of practical importance across a range of research ﬁelds. Several optimization tasks 
such as cost effective sensor placement [27]  optimal ad placement [19] and tasks such as semantic
image segmentation [35]  can be reduced to subset selection tasks  in which a set function determines
the value of every subset and has to be maximized to ﬁnd the best one. In combinatorial auctions 
set functions can be used to describe bidding behavior. Each bidder is represented as a valuation
function that maps each subset of goods to its subjective value to the customer [14]. Cooperative
games are set functions [8]. A coalition is a subset of players and a coalition game assigns a value to
every subset of players. In the simplest case the value one is assigned to winning and the value zero
to losing coalitions. Further  graphs and hypergraphs also admit set function representations:
Deﬁnition 1. (Hypergraph) A hypergraph is a triple H = (V  E  w)  where V = {v1  . . .   vn} is a
set of vertices  E ⊆ (P(V ) \ ∅) is a set of hyperedges and w : E → R is a weight function.
The weight function of a hypergraph is a set function on V by setting sA = wA if A ∈ E and
sA = 0 otherwise. Additionally  hypergraphs induce two set functions  namely the hypergraph cut
and association score function:

cutA =

wB and

assocA =

wB.

(7)

(cid:88)

B∈E B⊆A

(cid:88)

B∈E B∩A(cid:54)=∅ 
B∩(V \A)(cid:54)=∅

2.2 Convolutional Pattern Matching

The powerset convolution in (3) raises the question of which patterns are “detected” by a ﬁlter
(hQ)Q⊆N . In other words  to which signal does the ﬁlter h respond strongest when evaluated at a

3

given subset A? We call this signal pA (the pattern matched at position A). Formally 

pA = arg max
s:(cid:107)s(cid:107)=1

(h ∗ s)A.

(8)

For pN   the answer is pN = (1/(cid:107)h(cid:107))(hN\B)B⊆N . This is because the dot product (cid:104)h  s∗(cid:105)  with
s∗
A = sN\A  is maximal if h and s∗ are aligned. Slightly rewriting (3) yields the answer for the
general case A ⊆ N:

(h ∗ s)A =

hQsA\Q =

hQ1∪Q2

sA\Q1.

(9)

(cid:88)

Q⊆N

(cid:88)

Q1⊆A

 (cid:88)
(cid:124)

Q2⊆N\A
=:h(cid:48)

(cid:123)(cid:122)

Q1


(cid:125)

Namely  (9) shows that the powerset convolution evaluated at position A can be seen as the convolution
of a new ﬁlter h(cid:48) with s restricted to the powerset 2A evaluated at position A  the case for which we
know the answer: pA

Example 1. (One-hop patterns) For a one-hop ﬁlter h  i.e.  (h ∗ s)A = h∅sA +(cid:80)

A\B if B ⊆ A and pA

B = (1/(cid:107)h(cid:48)(cid:107))h(cid:48)

B = 0 otherwise.

x∈N h{x}sA\{x}

the pattern matched at position A takes the form

 1(cid:107)h(cid:48)(cid:107) (h∅ +(cid:80)

1(cid:107)h(cid:48)(cid:107) h{x}
0

pA
B =

x∈N\A h{x})

if B = A 
if B = A \ {x} with x ∈ A 
else.

(10)

Here  h(cid:48) corresponds to the ﬁlter restricted to the powerset 2A as in (9).

Notice that this behavior is different from 1D and 2D convolutions: there the underlying shifts
(translations) are invertible and thus the detected patterns are again shifted versions of each other. For
example  the 1D convolutional ﬁlter (hq)q∈Z matches p0 = (h−q)q∈Z at position 0 and pt = T−tp0 =
(h−q+t)q∈Z at position t  and  the group convolutional ﬁlter (hq)q∈G matches pe = (hq−1)q∈G at the
unit element e and pg = Tg−1pe = (hgq−1)q∈G at position g. Since powerset shifts are not invertible 
the detected patterns by a ﬁlter are not just (set-)shifted versions of each other as shown above.
A similar behavior can be expected with graph convolutions since the Laplacian shift is never
invertible and the adjacency shift is not always invertible.

3 Powerset Convolutional Neural Networks

Convolutional layers We deﬁne a convolutional layer by extending the convolution to multiple
channels  summing up the feature maps obtained by channel-wise convolution as in [10]:
Deﬁnition 2. (Powerset convolutional layer) A powerset convolutional layer is deﬁned as follows:

1. The input is given by nc set functions s = (s(1)  . . .   s(nc)) ∈ R2N×nc ;
2. The output is given by nf set functions t = LΓ(s) = (t(1)  . . .   t(nf )) ∈ R2N×nf ;
3. The layer applies a bank of set function ﬁlters Γ = (h(i j))i j  with i ∈ {1  . . .   nc} and

j ∈ {1  . . .   nf}  and a point-wise non-linearity σ resulting in

t(j)
A = σ(

(h(i j) ∗ s(i))A).

(11)

i=1

Pooling layers As in conventional CNNs  we deﬁne powerset pooling layers to gain additional
robustness with respect to input perturbations  and to control the number of features extracted by the
convolutional part of the powerset CNN. From a signal processing perspective  the crucial aspect of
the pooling operation is that the pooled signal lives on a valid signal domain  i.e.  a powerset. One
way to achieve this is by combining elements of the ground set.

4

nc(cid:88)

Figure 1: Forward pass of a simple powerset CNN with two convolutional and two pooling layers.
Set functions are depicted as signals on the powerset lattice.

Deﬁnition 3. (Powerset pooling) Let N(cid:48)(X) be the ground set of size n − |X| + 1 obtained by
combining all the elements in X ⊆ N into a single element. E.g.  for X = {x1  x2} we get
N(cid:48)(X) = {{x1  x2}  x3  . . .   xn}. Therefore every subset X ⊆ N deﬁnes a pooling operation

P X : R2N → R2N(cid:48) (X)

: (sA)A⊆N (cid:55)→ (sB)B:B∩X=X or B∩X=∅.

(12)

In our experiments we always use P := P {x1 x2}. It is also possible to pool a set function by
combining elements of the powerset as in [48] or by the simple rule sB = max(sB  sB∪{x}) for
B ⊆ N \ {x}. Then  a pooling layer is obtained by applying our pooling strategy to every channel.
Deﬁnition 4. (Powerset pooling layer) A powerset pooling layer takes nc set functions as
input s = (s(1)  . . .   s(nc)) ∈ R2N×nc and outputs nc pooled set functions t = LP (s) =
(t(1)  . . .   t(nc)) ∈ R2N(cid:48)×nc  with |N(cid:48)| = |N| − 1  by applying the pooling operation to every
channel

t(i) = P (s(i)).

(13)

Powerset CNN A powerset CNN is a composition of several powerset convolutional and pooling
layers. Depending on the task  the outputs of the convolutional component can be fed into a multi-layer
perceptron  e.g.  for classiﬁcation.
Fig. 1 illustrates a forward pass of a powerset CNN with two convolutional layers  each of which is
followed by a pooling layer. The ﬁrst convolutional layer is parameterized by three one-hop ﬁlters
and the second one is parameterized by ﬁfteen (three times ﬁve) one-hop ﬁlters. The ﬁlter coefﬁcients
were initialized with random weights for this illustration.
Implementation1 We implemented the powerset convolutional and pooling layers in Tensorﬂow [1].
Our implementation supports various deﬁnitions of powerset shifts  and utilizes the respective Fourier
transforms to compute the convolutions in the frequency domain.

4 Experimental Evaluation

Our powerset CNN is built on the premise that the successful components of conventional CNNs
are domain independent and only rely on the underlying concepts of shift and shift-equivariant
convolutions. In particular  if we use only one-hop ﬁlters  our powerset CNN satisﬁes locality and
compositionality. Thus  similar to image CNNs  it should be able to learn localized hierarchical
features. To understand whether this is useful when applied to set function classiﬁcation problems 
we evaluate our powerset CNN architectures on three synthetic tasks and on two tasks based on
real-world hypergraph data.
Problem formulation Intuitively  our set function classiﬁcation task will require the models to learn
to classify a collection of set functions sampled from some natural distributions. One such example
would be to classify (hyper-)graphs coming from some underlying data distributions. Formally  the
set function classiﬁcation problem is characterized by a training set {(s(i)  t(i))}m
i=1 ⊆ (R2N × C)
composed of pairs (set function  label)  as well as a test set. The learning task is to utilize the training
set to learn a mapping from the space of set functions R2N to the label space C = {1  . . .   k}.
1Sample implementations are provided at https://github.com/chrislybaer/Powerset-CNN.

5

inputconv1pool1conv2pool2MLP22x 524x 123x 523x 324x 34.1 Synthetic Datasets
Unless stated otherwise  we consider the ground set N = {x1  . . .   xn} with n = 10  and sample
10  000 set functions per class. We use 80% of the samples for training  and the remaining 20% for
testing. We only use one random split per dataset. Given this  we generated the following three
synthetic datasets  meant to illustrate speciﬁc applications of our framework.
Spectral patterns In order to obtain non-trivial classes of set functions  we deﬁne a sampling proce-
dure based on the Fourier expansion associated with the shift TQs = (sA\Q)A⊆N . In particular  we
sample Fourier sparse set functions  s = F −1ˆs with ˆs sparse. We implement this by associating each
target “class” with a collection of frequencies  and sample normally distributed Fourier coefﬁcients
for these frequencies. In our example  we deﬁned four classes  where the Fourier support of the ﬁrst
and second class is obtained by randomly selecting roughly half of the frequencies. For the third
class we use the entire spectrum  while for the fourth we use the frequencies that are either in both of
class one’s and class two’s Fourier support  or in neither of them.
k-junta classiﬁcation A k-junta [33] is a boolean function deﬁned on n variables x1  . . .   xn that
only depends on k of the variables: xi1  . . .   xik. In the same spirit  we call a set function a k-junta if
its evaluations only depend on the presence or absence of k of the n elements of the ground set:
Deﬁnition 5. (k-junta) A set function s on the ground set N is called a k-junta if there exists a subset
N(cid:48) ⊆ N  with |N(cid:48)| = k  such that s(A) = s(A ∩ N(cid:48))  for all A ⊆ N.
We generate a k-junta classiﬁcation dataset by sampling random k-juntas for k ∈ {3  . . .   7}. We do
so by utilizing the fact that shifting a set function by {x} eliminates its dependency on x  i.e.  for
A with x ∈ A we have (T{x}s)A = sA\{x} = (T{x}s)A\{x} because (A \ {x}) \ {x} = A \ {x}.
Therefore  sampling a random k-junta amounts to ﬁrst sampling a random value for every subset
A ⊆ N and performing n − k set shifts by randomly selected singleton sets.
Submodularity classiﬁcation A set function s is submodular if it satisﬁes the diminishing returns
property

∀A  B ⊆ N with A ⊆ B and ∀x ∈ N \ B : sA∪{x} − sA ≥ sB∪{x} − sB.

(14)

In words  adding an element to a small subset increases the value of the set function at least as
much as adding it to a larger subset. We construct a dataset comprised of submodular and "almost
submodular" set functions. As examples of submodular functions we utilize coverage functions [26]
(a subclass of submodular functions that allows for easy random generation). As examples of what
we informally call "almost submodular" set functions here  we sample coverage functions and perturb
them slightly to destroy the coverage property.

4.2 Real Datasets

Finally  we construct two classiﬁcation tasks based on real hypergraph data. Reference [5] provides
19 real-world hypergraph datasets. Each dataset is a hypergraph evolving over time. An example is
the DBLP coauthorship hypergraph in which vertices are authors and hyperedges are publications.
In the following  we consider classiﬁcation problems on subhypergraphs induced by vertex subsets
of size ten. Each hypergraph is represented by its weight set function sA = 1 if A ∈ E and sA = 0
otherwise.
Deﬁnition 6. (Induced Subhypergraph [6]) Let H = (V  E) be a hypergraph. The subset of vertices
V (cid:48) ⊆ V induces a subhypergraph H(cid:48) = (V (cid:48)  E(cid:48)) with E(cid:48) = {A∩ V (cid:48) : for A ∈ E and A∩ V (cid:48) (cid:54)= ∅}.
Domain classiﬁcation As we have multiple hypergraphs  an interesting question is whether it is
possible to identify from which hypergraph a given subhypergraph of size ten was sampled  i.e. 
whether it is possible to distinguish the hypergraphs by considering only local interactions. Therefore 
among the publicly available hypergraphs in [5] we only consider those containing at least 500
hyperedges of cardinality ten (namely  DAWN: 1159  threads-stack-overﬂow: 3070  coauth-DBLP:
6599  coauth-MAG-History: 1057  coauth-MAG-Geology: 7704  congress-bills: 2952). The coauth-
hypergraphs are coauthorship hypergraphs  in DAWN the vertices are drugs and the hyperedges
patients  in threads-stack-overﬂow the vertices are users and the hyperedges questions on threads
on stackoverflow.com and in congress-bills the vertices are congresspersons and the hyperedges
cosponsored bills. From those hypergraphs we sample all the subhypergraphs induced by the

6

hyperedges of size ten and assign the respective hypergraph of origin as class label. In addition to this
dataset (DOM6)  we create an easier version (DOM4) in which we only keep one of the coauthorship
hypergraphs  namely coauth-DBLP.
Simplicial closure Reference [5] distinguishes between open and closed hyperedges (the latter are
called simplices). A hyperedge is called open if its vertices in the 2-section (the graph obtained
by making the vertices of every hyperedge a clique) of the hypergraph form a clique and it is
not contained in any hyperedge in the hypergraph. On the other hand  a hyperedge is closed if
it is contained in one or is one of the hyperedges of the hypergraph. We consider the following
classiﬁcation problem: For a given subhypergraph of ten vertices  determine whether its vertices form
a closed hyperedge in the original hypergraph or not.
In order to obtain examples for closed hyperedges  we sample the subhypergraphs induced by the
vertices of hyperedges of size ten and for open hyperedges we sample subhypergraphs induced by
vertices of hyperedges of size nine extended by an additional vertex. In this way we construct two
learning tasks. First  CON10 in which we extend the nine-hyperedge by choosing the additional
vertex such that the resulting hyperedge is open (2952 closed and 4000 open examples). Second 
COAUTH10 in which we randomly extend the size nine hyperedges (as many as there are closed ones)
and use coauth-DBLP for training and coauth-MAG-History & coauth-MAG-Geology for testing.

4.3 Experimental Setup

(h (cid:5) s)A = h∅sA +(cid:80)

(h ∗ s)A = h∅sA + (cid:80)

Baselines As baselines we consider a multi-layer perceptron (MLP) [43] with two hidden layers
of size 4096 and an appropriately chosen last layer and graph CNNs (GCNs) on the undirected
n-dimensional hypercube. Every vertex of the hypercube corresponds to a subset and vertices are
connected by an edge if their subsets only differ by one element. We evaluate graph convolutional
layers based on the Laplacian shift [25] and based on the adjacency shift [44]. In both cases one layer
does at most one hop.
Our models For our powerset CNNs (PCNs) we consider convolutional
one-hop ﬁlters of two different convolutions:

layers based on
x∈N h{x}sA\{x} and
x∈N h{x}sA∪{x}. For all types of convolutional layers we consider the follow-
ing models: three convolutional layers followed by an MLP with one hidden layer of size 512 as
illustrated before  a pooling layer after each convolutional layer followed by the MLP  and a pooling
layer after each convolutional layer followed by an accumulation step (average of the features over all
subsets) as in [18] followed by the MLP. For all models we use 32 output channels per convolutional
layer and ReLU [32] non-linearities.
Training We train all models for 100 epochs (passes through the training data) using the Adam
optimizer [24] with initial learning rate 0.001 and an exponential learning rate decay factor of 0.95.
The learning rate decays after every epoch. We use batches of size 128 and the cross entropy loss. All
our experiments were run on a server with an Intel(R) Xeon(R) CPU @ 2.00GHz with four NVIDIA
Tesla T4 GPUs. Mean and standard deviation are obtained by running each experiment 20 times.

4.4 Results

Our results are summarized in Table 2. We report the test classiﬁcation accuracy in percentages (for
models that converged).
Discussion Table 2 shows that in the synthetic tasks the powerset convolutional models (∗-PCNs)
tend to outperform the baselines with the exception of A-GCNs  which are based on the adjacency
graph shift on the undirected hypercube. In fact  the set of A-convolutional ﬁlters parametrized
by our A-GCNs is the subset of the powerset convolutional ﬁlters associated with the symmetric
difference shift (6) obtained by constraining all ﬁlter coefﬁcients for one-element sets to be equal:
h{xi} = c with c ∈ R  for all i ∈ {1  . . .   n}. Therefore  it is no surprise that the A-GCNs perform
well. In contrast  the restrictions placed on the ﬁlters of L-GCN are stronger  since [25] replaces the
one-hop Laplacian convolution (θ0I + θ1(L − I))x (in Chebyshev basis) with θ(2I − L)x by setting
θ = θ0 = −θ1.
An analogous trend is not as clearly visible in the tasks derived from real hypergraph data. In
these tasks  the graph CNNs seem to be either more robust to noisy data  or  to beneﬁt from their
permutation equivariance properties. The robustness as well as the permutation equivariance can

7

Baselines
MLP
L-GCN
L-GCN pool
L-GCN pool avg.
A-GCN
A-GCN pool
A-GCN pool avg.
Proposed models
∗-PCN
∗-PCN pool
∗-PCN pool avg.
(cid:5)-PCN
(cid:5)-PCN pool
(cid:5)-PCN pool avg.

Patterns

k-Junta

Submod.

COAUTH10

CON10

DOM4

DOM6

46.8 ± 3.9
52.5 ± 0.9
45.0 ± 1.0
42.1 ± 0.3
65.5 ± 0.9
56.9 ± 2.2
54.8 ± 0.9
88.5 ± 4.3
80.9 ± 0.9
75.9 ± 1.9
-
-
54.8 ± 1.9

43.2 ± 2.5
69.3 ± 2.8
60.9 ± 1.5
64.3 ± 2.2
95.8 ± 2.7
91.9 ± 2.1
95.8 ± 1.1
97.2 ± 2.3
96.0 ± 1.6
96.5 ± 0.6
97.5 ± 1.4
96.4 ± 1.7
96.6 ± 0.7

-
-
-
82.2 ± 0.4
-
89.8 ± 1.8
84.8 ± 1.9
88.6 ± 0.4
85.1 ± 1.8
87.0 ± 1.6
-
-
80.9 ± 2.9

80.7 ± 0.2
84.7 ± 0.9
83.2 ± 0.7
56.8 ± 1.1
80.5 ± 0.7
84.1 ± 0.6
64.8 ± 1.1
80.6 ± 0.7
82.6 ± 0.4
80.6 ± 0.5
83.6 ± 0.4
84.8 ± 0.3
83.3 ± 0.5

66.1 ± 1.8
67.2 ± 1.8
65.7 ± 1.0
64.1 ± 1.7
64.9 ± 1.8
66.0 ± 1.6
65.4 ± 0.7
62.8 ± 2.9
62.9 ± 2.0
63.4 ± 3.5
68.7 ± 1.3
68.2 ± 0.8
67.0 ± 2.0

93.6 ± 0.2
96.0 ± 0.2
93.2 ± 1.1
88.4 ± 0.3
93.9 ± 0.3
93.8 ± 0.3
92.7 ± 0.6
94.1 ± 0.3
94.0 ± 0.3
94.4 ± 0.3
93.7 ± 0.2
93.6 ± 0.3
94.8 ± 0.3

71.1 ± 0.3
73.7 ± 0.4
71.7 ± 0.5
62.8 ± 0.4
69.1 ± 0.5
70.7 ± 0.4
67.9 ± 0.3
70.5 ± 0.3
70.2 ± 0.5
73.0 ± 0.3
69.9 ± 0.3
70.3 ± 0.4
73.5 ± 0.5

Table 2: Results of the experimental evaluation in terms of test classiﬁcation accuracy (percentage).
The ﬁrst three columns contain the results from the synthetic experiments and the last four columns
the results from the hypergraph experiments. The best-performing model from the corresponding
category is in bold.

be attributed to the graph one-hop ﬁlters being omnidirectional. On the other hand  the powerset
one-hop ﬁlters are n-directional. Thus  they are sensitive to hypergraph isomorphy  i.e.  hypergraphs
with same connectivity structure but different vertex ordering are being processed differently.
Pooling Interestingly  while reducing the hidden state by a factor of two after every convolutional
layer  pooling in most cases only slightly decreases the accuracy of the PCNs in the synthetic tasks
and has no impact in the other tasks. Also the inﬂuence of pooling on the A-GCN is more similar to
the behavior of PCNs than the one for the L-GCN.
Equivariance Finally  we compare models having a shift-invariant convolutional part (sufﬁx "pool
avg.") with models having a shift-equivariant convolutional part (sufﬁx "pool") models. The difference
between these models is that the invariant ones have an accumulation step before the MLP resulting
in (a) the inputs to the MLP being invariant w.r.t. the shift corresponding to the speciﬁc convolutions
used and (b) the MLP having much fewer parameters in its hidden layer (32 · 512 instead of
210 · 32 · 512). For the PCNs the effect of the accumulation step appears to be task dependent.
For instance  in k-Junta  Submod.  DOM4 and DOM6 it is largely beneﬁcial  and in the others it
slightly disadvantageous. Similarly  for the GCNs the accumulation step is beneﬁcial in k-Junta and
disadvantageous in COAUTH10. A possible cause is that the resulting models are not expressive
enough due to the lack of parameters.
Complexity analysis Consider a powerset convolutional layer (11) with nc input channels and nf
output channels. Using k-hop ﬁlters  the layer is parametrized by np = nf + ncnf
parameters (nf bias terms plus ncnf
in the Fourier domain  i.e.  h∗ s = F −1(diag( ¯F h)F s)  which requires 3
2 n2n + 2n operations and 2n
ﬂoats of memory [36]. Thus  forward as well as backward pass require Θ(ncnf n2n) operations and
Θ(nc2n + nf 2n + np) ﬂoats of memory2. The hypercube graph convolutional layers are a special
case of powerset convolutional layers. Hence  they are in the same complexity class. A k-hop graph
convolutional layer requires nf + ncnf (k + 1) parameters.

(cid:1)
(cid:0)n
(cid:1) ﬁltering coefﬁcients). Convolution is done efﬁciently

(cid:80)k

(cid:80)k

i=0

i

(cid:0)n

i

i=0

5 Related Work

Our work is at the intersection of geometric deep learning  generalized signal processing and set
function learning. Since each of these areas is broad  due to space limitations  we will only review
the work that is most closely related to ours.
Deep learning Geometric deep learners [10] can be broadly categorized into convolution-based
approaches [9  12  13  16  25  55] and message-passing-based approaches [18  47  50]. The latter
assign a hidden state to each element of the index domain (e.g.  to each vertex in a graph) and make
use of a message passing protocol to learn representations in a ﬁnite amount of communication steps.

2The derivation of these results is provided in the supplementary material.

8

Reference [18] points out that graph CNNs are a subclass of message passing / graph neural networks
(MPNNs). References [9  16  25] utilize the spectral analysis of the graph Laplacian [51] to deﬁne
graph convolutions  while [55] makes use of the adjacency shift based convolution [44]. Similarly 
[12  13] utilize group convolutions [53] with desirable equivariances.
In a similar vein  in this work we utilize the recently proposed powerset convolutions [36] as the
foundation of a generalized CNN. With respect to the latter reference  which provides the theoretical
foundation for powerset convolutions  our contributions are an analysis of the resulting ﬁlters from
a pattern matching perspective  to deﬁne its exact instantiations and applications in the context of
neural networks  as well as to show that these operations are practically relevant for various tasks.
Signal processing Set function signal processing [36] is an instantiation of algebraic signal processing
(ASP) [38] on the powerset domain. ASP provides a theoretical framework for deriving a complete
set of basic signal processing concepts  including convolution  for novel index domains  using as
starting point a chosen shift to which convolutions should be equivariant. To date the approach
was used for index domains including graphs [34  44  45]  powersets (set functions) [36]  meet/join
lattices [37  61]  and a collection of more regular domains  e.g.  [39  46  49].
Additionally  there are spectral approaches such as [51] for graphs and [15  33] for set functions
(or  equivalently  pseudo-boolean functions)  that utilize analogues of the Fourier transform to port
spectral analysis and other signal processing methods to novel domains.
Set function learning In contrast to the set function classiﬁcation problems considered in this work 
most of existing set function learning is concerned with completing a single partially observed set
function [2–4  7  11  30  54  56  63]. In this context  traditional methods [2–4  11  30  56] mainly
differ in the way how the class of considered set functions is restricted in order to be manageable.
E.g.  [54] does this by considering Walsh-Hadamard-sparse (= Fourier sparse) set functions. Recent
approaches [7  17  31  57  60  63] leverage deep learning. Reference [7] proposes a neural architecture
for learning submodular functions and [31  63] propose architectures for learning multi-set functions
(i.e.  permutation-invariant sequence functions). References [17  57] introduce differentiable layers
that allow for backpropagation through the minimizer or maximizer of a submodular optimization
problem respectively and  thus  for learning submodular set functions. Similarly  [60] proposes a
differentiable layer for learning boolean functions.

6 Conclusion

We introduced a convolutional neural network architecture for powerset data. We did so by utilizing
novel powerset convolutions and introducing powerset pooling layers. The powerset convolutions
used stem from algebraic signal processing theory [38]  a theoretical framework for porting signal
processing to novel domains. Therefore  we hope that our method-driven approach can be used to
specialize deep learning to other domains as well. We conclude with challenges and future directions.
Lack of data We argue that certain success components of deep learning are domain independent
and our experimental results empirically support this claim to a certain degree. However  one cannot
neglect the fact that data abundance is one of these success components and  for the supervised
learning problems on set functions considered in this paper  one that is currently lacking.
Computational complexity As evident from our complexity analysis and [29]  the proposed method-
ology is feasible only up to about n = 30 using modern multicore systems. This is caused by the fact
that set functions are exponentially large objects. If one would like to scale our approach to larger
ground sets  e.g.  to support semisupervised learning on graphs or hypergraphs where there is enough
data available  one should either devise methods to preserve the sparsity of the respective set function
representations while ﬁltering  pooling and applying non-linear functions  or  leverage techniques for
NN dimension reduction like [21].

Acknowledgements

We thank Max Horn for insightful discussions and his extensive feedback  and Razvan Pascanu
for feedback on an earlier draft. This project has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant
agreement No 805223).

9

References
[1] M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving 
M. Isard  et al. Tensorﬂow: A system for large-scale machine learning. In Symp. Operating
Systems Design and Implementation (OSDI)  pages 265–283  2016.

[2] A. Badanidiyuru  S. Dobzinski  H. Fu  R. Kleinberg  N. Nisan  and T. Roughgarden. Sketching

valuation functions. In Proc. Discrete Algorithms  pages 1025–1035. SIAM  2012.

[3] M. F. Balcan and N. J. A. Harvey. Learning submodular functions. In Proc. Theory of computing 

pages 793–802. ACM  2011.

[4] M. F. Balcan  F. Constantin  S. Iwata  and L. Wang. Learning valuation functions. 2012.
[5] A. R. Benson  R. Abebe  M. T. Schaub  A. Jadbabaie  and J. Kleinberg. Simplicial closure and
higher-order link prediction. Proc. National Academy of Sciences  115(48):E11221–E11230 
2018.

[6] C. Berge. Graphs and hypergraphs. North-Holland Pub. Co.  1973.
[7] J. Bilmes and W. Bai. Deep submodular functions. arXiv preprint arXiv:1701.08939  2017.
[8] R. Branzei  D. Dimitrov  and S. Tijs. Models in cooperative game theory  volume 556. Springer

Science & Business Media  2008.

[9] M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep learning:

going beyond euclidean data. IEEE Signal Processing Magazine  34(4):18–42  2017.

[10] M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep learning:

going beyond Euclidean data. IEEE Signal Processing Magazine  34(4):18–42  2017.

[11] S.-S. Choi  K. Jung  and J. H. Kim. Almost tight upper bound for ﬁnding Fourier coefﬁcients
of bounded pseudo-Boolean functions. Journal of Computer and System Sciences  77(6):
1039–1053  2011.

[12] T. Cohen  M. Geiger  J. Köhler  and M. Welling. Convolutional networks for spherical signals.

arXiv preprint arXiv:1709.04893  2017.

[13] T. S. Cohen and M. Welling. Group equivariant convolutional networks. In Proc. International
Conference on International Conference on Machine Learning (ICML)  pages 2990–2999  2016.
INFORMS Journal on

[14] S. De Vries and R. V. Vohra. Combinatorial auctions: A survey.

computing  15(3):284–309  2003.

[15] R. De Wolf. A brief introduction to Fourier analysis on the Boolean cube. Theory of Computing 

pages 1–20  2008.

[16] M. Defferrard  X. Bresson  and P. Vandergheynst. Convolutional neural networks on graphs

with fast localized spectral ﬁltering. pages 3844–3852  2016.

[17] J. Djolonga and A. Krause. Differentiable learning of submodular models. In Advances in

neural information processing systems (NIPS)  pages 1013–1023  2017.

[18] J. Gilmer  S. S. Schoenholz  P. F. Riley  O. Vinyals  and G. E. Dahl. Neural message passing for

quantum chemistry. pages 1263–1272  2017.

[19] D. Golovin  A. Krause  and M. Streeter. Online submodular maximization under a matroid

constraint with application to learning assignments. arXiv preprint arXiv:1407.1082  2014.

[20] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems
(NIPS)  pages 2672–2680  2014.

[21] T. Hackel  M. Usvyatsov  S. Galliani  J. D. Wegner  and K. Schindler. Inference  learning and
attention mechanisms that exploit and preserve sparsity in CNNs. In German Conference on
Pattern Recognition  pages 597–611. Springer  2018.

[22] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition.
Proc. IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

In

[23] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

10

[25] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

2017.

[26] A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical

Approaches to Hard Problems  pages 71–104. Cambridge University Press  2014.

[27] A. Krause and C. Guestrin. Near-optimal observation selection using submodular functions. In

AAAI  volume 7  pages 1650–1654  2007.

[28] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems (NIPS)  pages 1097–
1105  2012.

[29] Y. Lu. Practical tera-scale Walsh-Hadamard transform. In Future Technologies Conference

(FTC)  pages 1230–1236. IEEE  2016.

[30] E. Mossel  R. O’Donnell  and R. P. Servedio. Learning juntas. In Proc. Theory of computing 

pages 206–212. ACM  2003.

[31] R. L. Murphy  B. Srinivasan  V. Rao  and B. Ribeiro.

Janossy pooling: Learning deep
permutation-invariant functions for variable-size inputs. arXiv preprint arXiv:1811.01900 
2018.

[32] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. In

Proc. International conference on machine learning (ICML)  pages 807–814  2010.

[33] R. O’Donnell. Analysis of Boolean functions. Cambridge University Press  2014.
[34] A. Ortega  P. Frossard  J. Kovaˇcevi´c  J. M. F. Moura  and P. Vandergheynst. Graph signal

processing: Overview  challenges  and applications. Proc. IEEE  106(5):808–828  2018.

[35] A. Osokin and D. P. Vetrov. Submodular relaxation for inference in Markov random ﬁelds.

IEEE Trans. pattern analysis and machine intelligence  37(7):1347–1359  2014.

[36] M. Püschel. A discrete signal processing framework for set functions. In Proc. International

Conference on Acoustics  Speech and Signal Processing (ICASSP)  pages 4359–4363  2018.

[37] M. Püschel. A discrete signal processing framework for meet/join lattices with applications
to hypergraphs and trees. In Proc. International Conference on Acoustics  Speech and Signal
Processing (ICASSP)  pages 5371–5375  2019.

[38] M. Püschel and J. M. F. Moura. Algebraic signal processing theory: Foundation and 1-D time.

IEEE Trans. Signal Processing  56(8):3572–3585  2008.

[39] M. Püschel and M. Rötteler. Algebraic signal processing theory: 2-D hexagonal spatial lattice.

IEEE Trans. Image Processing  16(6):1506–1521  2007.

[40] S. Ravanbakhsh  J. Schneider  and B. Poczos. Equivariance through parameter-sharing. In

Proc. International Conference on Machine Learning (ICML)  pages 2892–2901  2017.

[41] S. Ren  K. He  R. Girshick  and J. Sun. Faster r-cnn: Towards real-time object detection with
region proposal networks. In Advances in neural information processing systems (NIPS)  pages
91–99  2015.

[42] O. Ronneberger  P. Fischer  and T. Brox. U-net: Convolutional networks for biomedical
image segmentation. In Proc. International Conference on Medical image computing and
computer-assisted intervention  pages 234–241  2015.

[43] F. Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mechanisms.

Technical report  Cornell Aeronautical Lab Inc Buffalo NY  1961.

[44] A. Sandryhaila and J. M. F. Moura. Discrete signal processing on graphs. IEEE Trans. Signal

Processing  61(7):1644–1656  2013.

[45] A. Sandryhaila and J. M. F. Moura. Discrete signal processing on graphs: Frequency analysis.

IEEE Trans. Signal Processing  62(12):3042–3054  2014.

[46] A. Sandryhaila  J. Kovacevic  and M. Püschel. Algebraic signal processing theory: 1-D

nearest-neighbor models. IEEE Trans. on Signal Processing  60(5):2247–2259  2012.

[47] F. Scarselli  M. Gori  A. C. Tsoi  M. Hagenbuchner  and G. Monfardini. The graph neural

network model. IEEE Trans. Neural Networks  20(1):61–80  2009.

11

[48] R. Scheibler  S. Haghighatshoar  and M. Vetterli. A fast Hadamard transform for signals with
sublinear sparsity in the transform domain. IEEE Trans. Information Theory  61(4):2115–2132 
2015.

[49] B. Seifert and K. Hüper. The discrete cosine transform on triangles. In Proc. International

Conference on Acoustics  Speech and Signal Processing (ICASSP)  pages 5023–5026  2019.

[50] D. Selsam  M. Lamm  B. Bünz  P. Liang  L. de Moura  and D. L. Dill. Learning a SAT solver

from single-bit supervision. arXiv preprint arXiv:1802.03685  2018.

[51] D. I. Shuman  S. K. Narang  P. Frossard  A. Ortega  and P. Vandergheynst. The emerging ﬁeld
of signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains. IEEE Trans. Signal Processing  30(3):83–98  2013.

[52] M. Simonovsky and N. Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. In International Conference on Artiﬁcial Neural Networks  pages
412–422  2018.

[53] R. S. Stankovic  C. Moraga  and J. Astola. Fourier analysis on ﬁnite groups with applications

in signal processing and system design. John Wiley & Sons  2005.

[54] P. Stobbe and A. Krause. Learning Fourier sparse set functions. In Artiﬁcial Intelligence and

Statistics  pages 1125–1133  2012.

[55] F. P. Such  S. Sah  M. A. Dominguez  S. Pillai  C. Zhang  A. Michael  N. D. Cahill  and
R. Ptucha. Robust spatial ﬁltering with graph convolutional neural networks. IEEE Journal of
Selected Topics in Signal Processing  11(6):884–896  2017.

[56] A. M. Sutton  L. D. Whitley  and A. E. Howe. Computing the moments of k-bounded pseudo-
Boolean functions over Hamming spheres of arbitrary radius in polynomial time. Theoretical
Computer Science  425:58–74  2012.

[57] S. Tschiatschek  A. Sahin  and A. Krause. Differentiable submodular maximization.

In
Proc. International Joint Conference on Artiﬁcial Intelligence  pages 2731–2738. AAAI Press 
2018.

[58] A. Van Den Oord  S. Dieleman  H. Zen  K. Simonyan  O. Vinyals  A. Graves  N. Kalchbrenner 
A. W. Senior  and K. Kavukcuoglu. WaveNet: A generative model for raw audio. SSW  125 
2016.

[59] H. Wang  J. Wang  J. Wang  M. Zhao  W. Zhang  F. Zhang  X. Xie  and M. Guo. Graphgan:
Graph representation learning with generative adversarial nets. In Conference on Artiﬁcial
Intelligence  2018.

[60] P.-W. Wang  P. Donti  B. Wilder  and Z. Kolter. SATNet: Bridging deep learning and logical
reasoning using a differentiable satisﬁability solver. In International Conference on Machine
Learning  pages 6545–6554  2019.

[61] C. Wendler and M. Püschel. Sampling signals on Meet/Join lattices. In Proc. Global Conference

on Signal and Information Processing (GlobalSIP)  2019.

[62] T. Young  D. Hazarika  S. Poria  and E. Cambria. Recent trends in deep learning based natural

language processing. IEEE Computational intelligence magazine  13(3):55–75  2018.

[63] M. Zaheer  S. Kottur  S. Ravanbakhsh  B. Poczos  R. R. Salakhutdinov  and A. J. Smola. Deep
sets. In Advances in neural information processing systems (NIPS)  pages 3391–3401  2017.

12

,Hsiang-Fu Yu
Nikhil Rao
Inderjit Dhillon
YAN ZHENG
Zhaopeng Meng
Jianye Hao
Zongzhang Zhang
Tianpei Yang
Changjie Fan
Chris Wendler
Markus Püschel
Dan Alistarh