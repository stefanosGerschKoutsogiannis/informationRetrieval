2016,Efficient Nonparametric Smoothness Estimation,Sobolev quantities (norms  inner products  and distances) of probability density functions are important in the theory of nonparametric statistics  but have rarely been used in practice  partly due to a lack of practical estimators. They also include  as special cases  L^2 quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the finite-sample bias and variance of our estimators  finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators  and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing and empirically validate our estimators on synthetic data.,Efﬁcient Nonparametric Smoothness Estimation

Shashank Singh

Carnegie Mellon University
sss1@andrew.cmu.edu

Simon S. Du

Carnegie Mellon University

ssdu@cs.cmu.edu

Barnabás Póczos

Carnegie Mellon University
bapoczos@cs.cmu.edu

Abstract

Sobolev quantities (norms  inner products  and distances) of probability density
functions are important in the theory of nonparametric statistics  but have rarely
been used in practice  due to a lack of practical estimators. They also include 
as special cases  L2 quantities which are used in many applications. We propose
and analyze a family of estimators for Sobolev quantities of unknown probability
density functions. We bound the ﬁnite-sample bias and variance of our estimators 
ﬁnding that they are generally minimax rate-optimal. Our estimators are signif-
icantly more computationally tractable than previous estimators  and exhibit a
statistical/computational trade-off allowing them to adapt to computational con-
straints. We also draw theoretical connections to recent work on fast two-sample
testing and empirically validate our estimators on synthetic data.

Introduction

1
L2 quantities (i.e.  inner products  norms  and distances) of continuous probability density functions
are important information theoretic quantities with many applications in machine learning and signal
processing. For example  L2 norm estimates can be used for goodness-of-ﬁt testing [7]  image
registration and texture classiﬁcation [12]  and parameter estimation in semi-parametric models [36].
L2 inner products estimates can generalize linear or polynomial kernel methods to inputs which are
distributions rather than numerical vectors. [28] L2 distance estimators are used for two-sample
testing [1  25]  transduction learning [30]  and machine learning on distributional inputs [27]. [29]
gives applications of L2 quantities to adaptive information ﬁltering  classiﬁcation  and clustering.
L2 quantities are a special case of less-well-known Sobolev quantities. Sobolev norms measure
global smoothness of a function in terms of integrals of squared derivatives. For example  for a
non-negative integer s and a function f : R → R with an sth derivative f (s)  the s-order Sobolev
dx (when this quantity is ﬁnite). See Section 2 for

norm (cid:107) · (cid:107)H s is given by (cid:107)f(cid:107)H s =(cid:82)

(cid:0)f (s)(x)(cid:1)2

R

more general deﬁnitions  and see [21] for an introduction to Sobolev spaces.
Estimation of general Sobolev norms has a long history in nonparametric statistics (e.g.  [32  13  10 
2]) This line of work was motivated by the role of Sobolev norms in many semi- and non-parametric
problems  including density estimation  density functional estimation  and regression  (see [35] 
Section 1.7.1) where they dictate the convergence rates of estimators. Despite this  to our knowledge 
these quantities have never been studied in real data  leaving an important gap between the theory
and practice of nonparametric statistics. We suggest this is in part due a lack of practical estimators
for these quantities. For example  the only one of the above estimators that is statistically minimax-
optimal [2] is extremely difﬁcult to compute in practice  requiring numerical integration over each of
O(n2) different kernel density estimates  where n denotes the sample size. We know of no estimators
previously proposed for Sobolev inner products and distances.
The main goal of this paper is to propose and analyze a family of computationally and statistically
efﬁcient estimators for Sobolev inner products  norms  and distances. Our speciﬁc contributions are:

1. We propose families of nonparametric estimators for all Sobolev quantities (Section 4).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

2. We analyze the estimators’ bias and variance. Assuming the underlying density functions lie in
a Sobolev class of smoothness parametrized by s(cid:48)  we show the estimator for Sobolev quantities
of order s < s(cid:48) converges to the true value at the “parametric” rate of O(n−1) in mean squared
error when s(cid:48) ≥ 2s + D/4  and at a slower rate of O

otherwise. (Section 5).

8(s−s(cid:48) )
4s(cid:48)+D

(cid:18)

(cid:19)

n

3. We validate our theoretical results on simulated data. (Section 8).
4. We derive asymptotic distributions for our estimators  and we use these to derive tests for the
general statistical problem of two-sample testing. We also draw theoretical connections between
our test and the recent work on nonparametric two-sample testing. (Section 9).

In terms of mean squared error  minimax lower bounds matching our convergence rates over Sobolev
or Hölder smoothness classes have been shown by [16] for s = 0 (i.e.  L2 quantities)  and [3] for
Sobolev norms with integer s. Since these lower bounds intuitively “span” the space of relevant
quantities  it is a small step to conjecture that our estimators are minimax rate-optimal for all Sobolev
quantities and s ∈ [0 ∞).
As described in Section 7  our estimators are computable in O(n1+ε) time using only basic matrix
operations  where n is the sample size and ε ∈ (0  1) is a tunable parameter trading statistical and
computational efﬁciency; the smallest value of ε at which the estimator continues to be minimax
rate-optimal approaches 0 as we assume more smoothness of the true density.

2 Problem setup and notation
Let X = [−π  π]D and let µ denote the Lebesgue measure on X . For D-tuples z ∈ ZD of integers 
let ψz ∈ L2 = L2(X ) 1 deﬁned by ψz(x) = e−i(cid:104)z x(cid:105) for all x ∈ X denote the zth element of the L2-
X ψz(x)f (x) dµ(x) denote
the zth Fourier coefﬁcient of f. 2 For any s ∈ [0 ∞)  deﬁne the Sobolev space H s = H s(X ) ⊆ L2
of order s on X by 3

orthonormal Fourier basis  and  for f ∈ L2  let (cid:101)f (z) := (cid:104)ψz  f(cid:105)L2 =(cid:82)
(cid:41)

(cid:40)

H s =

(1)
Fix a known s ∈ [0 ∞) and a unknown probability density functions p  q ∈ H s  and suppose we
have n IID samples X1  ...  Xn ∼ p and Y1  . . .   Yn ∼ q from each of p and q. We are interested in
estimating the inner product

z∈ZD

.

f ∈ L2 :

(cid:88)

z2s(cid:12)(cid:12)(cid:12)(cid:101)f (z)

(cid:12)(cid:12)(cid:12)2

< ∞

(cid:104)p  q(cid:105)H s :=

deﬁned for all

p  q ∈ H s.

(2)

(cid:88)

z∈ZD

z2s(cid:101)p(z)(cid:101)q(z)

(cid:88)

z∈ZD

Estimating the inner product gives an estimate for the (squared) induced norm and distance  since 4
(cid:107)p(cid:107)2

z2s |(cid:101)p(z)|2 = (cid:104)p  p(cid:105)H s

H s − 2(cid:104)p  q(cid:105)H s + (cid:107)q(cid:107)2

H s = (cid:107)p(cid:107)2

(cid:107)p − q(cid:107)2

H s :=

and

H s .

(3)

Since our theoretical results assume the samples from p and q are independent  when estimating
(cid:107)p(cid:107)2
may not be optimal in practice.
For a more classical intuition  we note that  in the case D = 1 and s ∈ {0  1  2  . . .}  (via Parseval’s

H s  we split the sample from p in half to compute two independent estimates of(cid:101)p  although this
identity and the identity (cid:103)f (s)(z) = (iz)s(cid:101)f (z))  that one can show the following: H s includes the
3When D > 1  z2s = (cid:81)D

1We suppress dependence on X ; all function spaces are over X except as discussed in Section 2.1.
2Here  (cid:104)· ·(cid:105) denotes the dot product on RD. For a complex number c = a + bi  c = a − bi denotes the
√
cc =
j . For z < 0  z2s should be read as (z2)s  so that z2s ∈ R even when
2s /∈ Z. In the L2 case  we use the convention that 00 = 1.
4(cid:107)p(cid:107)Hs is pseudonorm on H s because it fails to distinguish functions identical almost everywhere up to
additive constants; a combination of (cid:107)p(cid:107)L2 and (cid:107)p(cid:107)Hs is used when a proper norm is needed. However  since
probability densities integrate to 1  (cid:107)·−·(cid:107)Hs is a proper metric on the subset of (almost-everywhere equivalence
classes of) probability density functions in H s  which is important for two-sample testing (see Section 9). For
simplicity  we use the terms “norm”  “inner product”  and “distance” for the remainder of the paper.

complex conjugate of c  and |c| =

a2 + b2 denotes the modulus of c.

j=1 z2s

√

2

Functional Name

L2 norms

(Integer) Sobolev norms

Density functionals
Derivative functionals

L2 inner products

Multivariate functionals

Functional Form

L2 =(cid:82) (p(x))2 dx
H k =(cid:82)(cid:0)p(k)(x)(cid:1)2
(cid:82) ϕ(x  p(x)) dx

(cid:107)p(cid:107)2
(cid:107)p(cid:107)2

dx

(cid:82) ϕ(x  p(x)  p(cid:48)(x)  . . .   p(k)(x)) dx
(cid:104)p1  p2(cid:105)L2 =(cid:82) p1(x)p2(x) dx
(cid:82) ϕ(x  p1(x)  . . .   pk(x)) dx

References

[32  6]

[2]

[18  19]

[3]

[16  17]
[34  14]

Table 1: Some related functional forms for which estimators for which nonparametric estimators have
been developed and analyzed. p  p1  ...  pk are unknown probability densities  from each of which we
draw n IID samples  ϕ is a known real-valued measurable function  and k is a non-negative integer.

(cid:16)

(cid:17)2

(cid:90)

X

(cid:13)(cid:13)(cid:13)f (s)(cid:13)(cid:13)(cid:13)2

subspace of L2 functions with at least s derivatives in L2 and  if f (s) denotes the sth derivative of f

(cid:107)f(cid:107)2

H s = 2π

(4)
In particular  when s = 0  H s = L2  (cid:107) · (cid:107)H s = (cid:107) · (cid:107)L2  and (cid:104)· ·(cid:105)H s = (cid:104)· ·(cid:105)L2. As we describe in
the supplement  equation (4) and our results generalizes trivially to weak derivatives  as well as to
non-integer s ∈ [0 ∞) via a notion of fractional derivative.

f (s)(x)

dx = 2π

 

L2

∀f ∈ H s.

p2π(x) :=(cid:80)

2.1 Unbounded domains
A notable restriction above is that p and q are supported in X := [−π  π]D. In fact  our estimators
and tests are well-deﬁned and valid for densities supported on arbitrary subsets of RD. In this
case  they act on the 2π-periodic summation p2π : [−π  π]D → [0 ∞] deﬁned for x ∈ X by
z∈ZD p(x + 2πz)  which is itself a probability density function on X . For example  the
estimator for (cid:107)p(cid:107)H s will instead estimate (cid:107)p2π(cid:107)H s  and the two-sample test for distributions p and q
will attempt to distinguish p2π from q2π. Typically  this is not problematic; for example  for most
realistic probability densities  p and p2π have similar orders of smoothness  and p2π = q2π if and
only if p = q. However  there are (meagre) sets of exceptions; for example  if q is a translation of p
by exactly 2π  then p2π = q2π  and one can craft a highly discontinuous function p such that p2π is
uniform on X . [39] These exceptions make it difﬁcult to extend theoretical results to densities with
arbitrary support  but they are ﬁxed  in practice  by randomly rescaling the data (as in [4]). If the
densities have (known) bounded support  they can simply be shifted and scaled to be supported on X .

3 Related work
There is a large body of work on estimating nonlinear functionals of probability densities  with
various generalizations in terms of the class of functionals considered. Table 1 gives a subset of such
work  for functionals related to Sobolev quantities. As shown in Section 2  the functional form we
consider is a strict generalization of L2 norms  Sobolev norms  and L2 inner products. It overlaps
with  but is neither a special case nor a generalization of the remaining functional forms in the table.
Nearly all of the above approaches compute an optimally smoothed kernel density estimate and
then perform bias corrections based on Taylor series expansions of the functional of interest. They
typically consider distributions with densities that are β-Hölder continuous and satisfy periodicity
assumptions of order β on the boundary of their support  for some constant β > 0 (see  for example 
Section 4 of [16] for details of these assumptions). The Sobolev class we consider is a strict superset
of this Hölder class  permitting  for example  certain “small” discontinuities. In this regard  our
results are slightly more general than most of these prior works.
Finally  there is much recent work on estimating entropies  divergences  and mutual informations 
using methods based on kernel density estimates [14  17  16  24  33  34] or k-nearest neighbor
statistics [20  23  22  26]. In contrast  our estimators are more similar to orthogonal series density
estimators  which are computationally attractive because they require no pairwise operations between
samples. However  they require quite different theoretical analysis; unlike prior work  our estimator

3

is constructed and analyzed entirely in the frequency domain  and then related to the data domain via
Parseval’s identity. We hope our analysis can be adapted to analyze new  computationally efﬁcient
information theoretic estimators.

4 Motivation and construction of our estimator
For a non-negative integer parameter Zn (to be speciﬁed later)  let

(cid:88)

(cid:107)z(cid:107)∞≤Zn

(cid:101)q(z)ψz where

and

qn :=

(cid:88)

(cid:101)p(z)ψz

pn :=

(cid:107)z(cid:107)∞≤Zn

(cid:107)z(cid:107)∞ := max

j∈{1 ... D} zj

(5)

orthonormal family Fn := {ψz : z ∈ ZD |z| ≤ Zn}. Note that  since(cid:102)ψz(y) = 0 whenever y (cid:54)= z 

denote the L2 projections of p and q  respectively  onto the linear subspace spanned by the L2-
the Fourier basis has the special property that it is orthogonal in (cid:104)· ·(cid:105)H s as well. Hence  since
pn and qn lie in the span of Fn while p − pn and q − qn lie in the span of {ψz : z ∈ Z}\Fn 
(cid:104)p − pn  qn(cid:105)H s = (cid:104)pn  q − qn(cid:105)H s = 0. Therefore 

(cid:104)p  q(cid:105)H s = (cid:104)pn  qn(cid:105)H s + (cid:104)p − pn  qn(cid:105)H s + (cid:104)pn  q − qn(cid:105)H s + (cid:104)p − pn  q − qn(cid:105)H s

= (cid:104)pn  qn(cid:105)H s + (cid:104)p − pn  q − qn(cid:105)H s .

We propose an unbiased estimate of Sn := (cid:104)pn  qn(cid:105)H s =(cid:80)(cid:107)z(cid:107)∞≤Zn
z2s(cid:101)pn(z)(cid:101)qn(z). Notice that
Fourier coefﬁcients of p are the expectations(cid:101)p(z) = EX∼p [ψz(X)]. Thus  ˆp(z) := 1
(cid:80)n
j=1 ψz(Yj) are independent unbiased estimates of(cid:101)p and(cid:101)q  respectively. Since Sn
is bilinear in(cid:101)p and(cid:101)q  the plug-in estimator for Sn is unbiased. That is  our estimator for (cid:104)p  q(cid:105)H s is

and ˆq(z) := 1
n

(cid:80)n

j=1 ψz(Xj)

(6)

n

z2s ˆp(z)ˆq(z).

(7)

(cid:88)

ˆSn :=

(cid:107)z(cid:107)∞≤Zn

z2s(cid:101)pn(z)(cid:101)qn(z) = Sn.

5 Finite sample bounds
Here  we present our main theoretical results  bounding the bias  variance  and mean squared error of
our estimator for ﬁnite n.
By construction  our estimator satisﬁes

=

(cid:105)
E(cid:104) ˆSn
(cid:88)
(cid:12)(cid:12)(cid:12)E(cid:104) ˆSn
(cid:105) − (cid:104)p  q(cid:105)H s

(cid:107)z(cid:107)∞≤Zn

z2s E [ˆp(z)] E [ˆq(z)] =

(cid:88)
(cid:113)
(cid:12)(cid:12)(cid:12) = |(cid:104)p − pn  q − qn(cid:105)H s| ≤

(cid:107)z(cid:107)∞≤Zn

Thus  via (6) and Cauchy-Schwarz  the bias of the estimator ˆSn satisﬁes
(cid:107)p − pn(cid:107)2

(8)
(cid:107)p − pn(cid:107)H s is the error of approximating p by an order-Zn trigonometric polynomial  a classic
problem in approximation theory  for which Theorem 2.2 of [15] shows:

H s (cid:107)q − qn(cid:107)2

H s.

if p ∈ H s(cid:48)

for some s(cid:48) > s 

then

(cid:107)p − pn(cid:107)H s ≤ (cid:107)p(cid:107)H s(cid:48) Z s−s(cid:48)

n

.

In combination with (8)  this implies the following bound on the bias of our estimator:
Theorem 1. (Bias bound) If p  q ∈ H s(cid:48)

for some s(cid:48) > s  then  for CB := (cid:107)p(cid:107)H s(cid:48)(cid:107)q(cid:107)H s(cid:48)  

(cid:12)(cid:12)(cid:12)E(cid:104) ˆSn

(cid:105) − (cid:104)p  q(cid:105)H s

(cid:12)(cid:12)(cid:12) ≤ CBZ 2(s−s(cid:48))

n

(9)

(10)

Hence  the bias of ˆSn decays polynomially in Zn  with a power depending on the “extra” s(cid:48) − s
orders of smoothness available. On the other hand  as we increase Zn  the number of frequencies at
which we estimate ˆp increases  suggesting that the variance of the estimator will increase with Zn.
Indeed  this is expressed in the following bound on the variance of the estimator.

4

Theorem 2. (Variance bound) If p  q ∈ H s(cid:48)

for some s(cid:48) ≥ s  then

V(cid:104) ˆSn

(cid:105) ≤ 2C1

Z 4s+D

n

n2 +

C2
n

 

where C1 :=

(cid:107)p(cid:107)L2(cid:107)q(cid:107)L2

2DΓ(4s + 1)
Γ(4s + D + 1)
H s are the constants (in n)

(11)

H s(cid:107)q(cid:107)4

and C2 := ((cid:107)p(cid:107)H s + (cid:107)q(cid:107)H s)(cid:107)p(cid:107)W 2s 4(cid:107)q(cid:107)W 2s 4 + (cid:107)p(cid:107)4
The proof of Theorem 2 is perhaps the most signiﬁcant theoretical contribution of this work. Due to
space constraints  the proof is given in the supplement. Combining Theorems 1 and 2 gives a bound
on the mean squared error (MSE) of ˆSn via the usual decomposition into squared bias and variance:
Corollary 3. (Mean squared error bound) If p  q ∈ H s(cid:48)
BZ 4(s−s(cid:48))

(cid:20)(cid:16) ˆSn − (cid:104)p  q(cid:105)H s

for some s(cid:48) > s  then
C2
n

≤ C 2

Z 4s+D

+ 2C1

(12)

E

n

n

.

If  furthermore  we choose Zn (cid:16) n

4s(cid:48)+D (optimizing the rate in inequality 12)  then

(cid:17)2(cid:21)

(cid:110) 8(s−s(cid:48) )

4s(cid:48)+D

(cid:16) nmax

n2 +
(cid:111)

 −1

.

(cid:17)2(cid:21)
(cid:20)(cid:16) ˆSn − (cid:104)p  q(cid:105)H 2

2

E

Corollary 3 recovers the phenomenon discovered by [2]: when s(cid:48) ≥ 2s + D

MSE decays at the “semi-parametric” n−1 rate  whereas  when s(cid:48) ∈(cid:0)s  2s + D

a slower rate. Also  the estimator is L2-consistent if Zn → ∞ and Znn− 2
is useful in practice  since s is known but s(cid:48) is not.
Finally  it is worth reiterating that  by (3)  these ﬁnite sample rates extend  with additional constant
factors  to estimating Sobolev norms and distances.

(13)

(cid:1)  the MSE decays at

4   the minimax optimal
4s+D → 0 as n → ∞. This

4

6 Asymptotic distributions
In this section  we derive the asymptotic distributions of our estimator in two cases: (1) the inner
product estimator and (2) the distance estimator in the case p = q. These results provide conﬁdence
intervals and two-sample tests without computationally intensive resampling. While (1) is more
general in that it can be used with (3) to bound the asymptotic distributions of the norm and distance
estimators  (2) provides a more precise result leading to a more computationally and statistically
efﬁcient two-sample test. Proofs are given in the supplementary material.
Theorem 4 shows that our estimator has a normal asymptotic distribution  assuming Zn → ∞ slowly
enough as n → ∞  and also gives a consistent estimator for its asymptotic variance. From this  one
can easily estimate asymptotic conﬁdence intervals for inner products  and hence also for norms.
Theorem 4. (Asymptotic normality) Suppose that  for some s(cid:48) > 2s + D
  and
4s+D → 0 as n → ∞. Then  ˆSn is asymptotically normal
suppose Znn
with mean (cid:104)p  q(cid:105)H s. In particular  for j ∈ {1  . . .   n} and z ∈ ZD with (cid:107)z(cid:107)∞ ≤ Zn  deﬁne
Wj z := zseizXj and Vj z := zseizYj   so that Wj and Vj are column vectors in R(2Zn)D. Let
W := 1
n

(cid:80)n
j=1 Vj ∈ R(2Zn)D

4(s−s(cid:48) ) → ∞ and Znn− 1

4   p  q ∈ H s(cid:48)

j=1 Wj  V := 1
n

 

1

ΣW :=

1
n

(Wj−W )(Wj−W )T  

and ΣV :=

1
n

(Vj−V )(Vj−V )T ∈ R(2Zn)D×(2Zn)D

(cid:80)n
n(cid:88)

j=1

denote the empirical means and covariances of W and V   respectively. Then  for

n(cid:88)
(cid:32) ˆSn − (cid:104)p  q(cid:105)H s

j=1

√

n

ˆσn

(cid:33)

(cid:20) V

(cid:21)T(cid:20)ΣW

W

0

(cid:21)(cid:20) V

(cid:21)

W

0
ΣV

ˆσ2
n :=

 

we have

D→ N (0  1) 

where D→ denotes convergence in distribution.
Since distances can be written as a sum of three inner products (Eq. (3))  Theorem 4 might suggest
an asymptotic normal distribution for Sobolev distances. However  extending asymptotic normality

5

from inner products to their sum requires that the three estimates be independent  and hence that we
split data between the three estimates. This is inefﬁcient in practice and somewhat unnatural  as we
know  for example  that distances should be non-negative. For the particular case p = q (as in the
null hypothesis of two-sample testing)  the following theorem 5 provides a more precise asymptotic
(χ2) distribution of our Sobolev distance estimator  after an extra decorrelation step. This gives  for
example  a more powerful two-sample test statistic (see Section 9 for details).
Theorem 5. (Asymptotic null distribution) Suppose that  for some s(cid:48) > 2s + D
suppose Znn

(cid:107)z(cid:107)∞ ≤ Zn  deﬁne Wj z := zs(cid:0)e−izXj − e−izYj(cid:1)  so that Wj is a column vector in R(2Zn)D. Let
(cid:0)Wj − W(cid:1)(cid:0)Wj − W(cid:1)T ∈ R(2Zn)D×(2Zn)D

  and
4s+D → 0 as n → ∞. For j ∈ {1  . . .   n} and z ∈ ZD with

4(s−s(cid:48) ) → ∞ and Znn− 1

Wj ∈ R(2Zn)D

4   p  q ∈ H s(cid:48)

and Σ :=

n(cid:88)

n(cid:88)

W :=

1

1
n

j=1

1
n

j=1

denote the empirical mean and covariance of W   and deﬁne T := nW

T

Σ−1W . Then  if p = q  then

Qχ2((2Zn)D)(T ) D→ Uniform([0  1])

as

n → ∞ 

where Qχ2(d) : [0 ∞) → [0  1] denotes the quantile function (inverse CDF) of the χ2 distribution
χ2(d) with d degrees of freedom.
Let ˆM denote our estimator for (cid:107)p− q(cid:107)H s (i.e.  plugging ˆSn into (3)). While Theorem 5 immediately
provides a valid two-sample test of desired level  it is not immediately clear how this relates to
ˆM  nor is there any suggestion of why the test statistic ought to be a good (i.e.  consistent) one.
Some intuition is as follows. Notice that ˆM = W
W . Since  by the central limit theorem  W
has a normal asymptotic distribution  if the components of W were uncorrelated (and Zn were
ﬁxed)  we would expect n ˆM to have an asymptotic χ2 distribution with (2Zn)D degrees of freedom.
However  because we use the same data to compute each component of ˆM  they are not typically
uncorrelated  and so the asymptotic distribution of ˆM is difﬁcult to derive. This motivates the statistic

T

Σ−1
W W

Σ−1
W W   since the components of

Σ−1
W W are (asymptotically) uncorrelated.

(cid:113)

(cid:18)(cid:113)

T =

(cid:19)T(cid:113)

2

7 Parameter selection and statistical/computational trade-off
Here  we give statistical and computational considerations for choosing the smoothing parameter Zn.
Statistical perspective: In practice  of course  we do not typically know s(cid:48)  so we cannot simply
set Zn (cid:16) n
4s(cid:48)+D   as suggested by the mean squared error bound (13). Fortunately (at least for ease
of parameter selection)  when s(cid:48) ≥ 2s + D
4   the dominant term of (13) is C2/n for Zn (cid:16) n− 1
4s+D .
Hence if we are willing to assume that the density has at least 2s + D
4 orders of smoothness (which
may be a mild assumption in practice)  then we achieve statistical optimality (in rate) by setting
Zn (cid:16) n− 1
4s+D   which depends only on known parameters. On the other hand  the estimator can
continue to beneﬁt from additional smoothness computationally.
Computational perspective An attractive property of the estimator discussed is its computational
simplicity and efﬁciency  in low dimensions. Most competing nonparametric estimators  such
as kernel-based or nearest-neighbor methods  either take O(n2) time or rely on complex data
structures such as k-d trees or cover trees [31] for O(2Dn log n) time performance. Since computing
the estimator takes O(nZ D
n ) memory (that is  the cost of estimating each of
(2Zn)D Fourier coefﬁcients by an average)  a statistically optimal choice of Zn gives a runtime of
. Since the estimate requires only a vector outer product  exponentiation  and averaging 

O
constants involved are small and computations parallelize trivially over frequencies and data.
Under severe computational constraints  for very large data sets  or if D is large relative to s(cid:48)  we can
reduce Zn to trade off statistical for computational efﬁciency. For example  if we want an estimator
5This result is closely related to Proposition 4 of [4]. However  in their situation  s = 0 and the set of test

n ) time and O(Z D

4s(cid:48)+2D
4s(cid:48)+D

(cid:18)

(cid:19)

n

frequencies is ﬁxed as n → ∞  whereas our set is increasing.

6

(a) 1D Gaussians with
different means.

(b) 1D Gaussians with
different variance.

(c) Uniform distributions
with different range.

(d) One uniform and one
triangular distribution.

(a) 3D Gaussians with
different means.

with runtime O(n1+θ) and space requirement O(nθ) for some θ ∈(cid:16)

(b) 3D Gaussians with
different variance.

(c) Estimation of H 0
norm of N (0  1).

still gives a consistent estimator  with mean squared error of the order O

(d) Estimation of H 1
norm of N (0  1).

0 

2D

4s(cid:48)+D

  setting Zn (cid:16) nθ/D

(cid:17)
nmax{ 4θ(s−s(cid:48) )

D

(cid:16)

 −1}(cid:17)

.

Kernel- or nearest-neighbor-based methods  including nearly all of the methods described in Section
3  tend to require storing previously observed data  resulting in O(n) space requirements.
In
contrast  orthogonal basis estimation requires storing only O(Z D
n ) estimated Fourier coefﬁcients.
The estimated coefﬁcients can be incrementally updated with each new data point  which may make
the estimator or close approximations feasible in streaming settings.
8 Experimental results
In this section  we use synthetic data to demonstrate effectiveness of our methods. 6 All experiments
use 10  102  . . .   105 samples for estimation.
We ﬁrst test our estimators on 1D L2 distances. Figure 1a shows estimated distance between N (0  1)
and N (1  1); Figure 1b shows estimated distance between N (0  1) and N (0  4); Figure 1c shows
estimated distance between Unif [0  1] and Unif[0.5  1.5]; Figure 1d shows estimated distance between
[0  1] and a triangular distribution whose density is highest at x = 0.5. Error bars indicate asymptotic
95% conﬁdence intervals based on Theorem 4. These experiments suggest 105 samples is sufﬁcient
to estimate L2 distances with high conﬁdence. Note that we need fewer samples to estimate Sobolev
quantities of Gaussians than  say  of uniform distributions  consistent with our theory  since (inﬁnitely
differentiable) Gaussians are smoothier than (discontinuous) uniform distributions.
Next  we test our estimators on L2 distances of multivariate distributions. Figure 2a shows estimated
distance between N ([0  0  0]   I) and N ([1  1  1]   I); Figure 2b shows estimated distance between
N ([0  0  0]   I) and N ([0  0  0]   4I). These experiments show that our estimators can also handle
multivariate distributions. Lastly  we test our estimators for H s norms. Figure 2c shows estimated
H 0 norm of N (0  1) and Figure 2d shows H 1 norm of N (0  1). Additional experiments with other
distributions and larger values of s are given in the supplement.
9 Connections to two-sample testing
We now discuss the use of our estimator in two-sample testing. From the large literature on nonpara-
metric two-sample testing  we discuss only some recent approaches closely related to ours.
Let ˆM denote our estimate of the Sobolev distance  consisting of plugging ˆS into equation (3).
Since (cid:107) · − · (cid:107)H s is a metric on the space of probability density functions in H s  computing ˆM
leads naturally to a two-sample test on this space. Theorem 5 suggests an asymptotic test  which
is computationally preferable to a permutation test. In particular  for a desired Type I error rate
α ∈ (0  1) our test rejects the null hypothesis p = q if and only if Qχ2(2ZD

n )(T ) < α.

6MATLAB code for these experiments is available at https://github.com/sss1/SobolevEstimation.

7

1011021031041050.050.10.150.20.250.3number of samplesL22 Estimated DistanceTrue Distance1011021031041050.050.10.150.20.25number of samplesL22 Estimated DistanceTrue Distance1011021031041050.20.40.60.81number of samplesL22 Estimated DistanceTrue Distance10110210310410500.10.20.30.4number of samplesL22 Estimated DistanceTrue Distance1050.020.0250.030.0350.040.045number of samplesL22 Estimated DistanceTrue Distance1050.010.0150.020.0250.030.0350.04number of samplesL22 Estimated DistanceTrue Distance1011021031041050.10.20.30.40.5number of samples||⋅||2H0 Estimated DistanceTrue Distance101102103104105−0.4−0.200.20.40.6number of samples||⋅||H12 Estimated DistanceTrue DistanceWhen s = 0  this approach is closely related to several two-sample tests in the literature based on
comparing empirical characteristic functions (CFs). Originally  these tests [11  5] computed the same
statistic T with a ﬁxed number of random RD-valued frequencies instead of deterministic ZD-valued
frequencies. This test runs in linear time  but is not generally consistent  since the two CFs need
not differ almost everywhere. Recently  [4] suggested using smoothed CFs  i.e.  the convolution of
the CF with a universal smoothing kernel k. This is computationally easy (due to the convolution

theorem) and  when p (cid:54)= q  ((cid:101)p ∗ k)(z) (cid:54)= ((cid:101)q ∗ k)(z) for almost all z ∈ RD  reducing the need for

carefully choosing test frequencies. Furthermore  this test is almost-surely consistent under very
general alternatives. However  it is not clear what sort of assumptions would allow ﬁnite sample
analysis of the power of their test. Indeed  the convergence as n → ∞ can be arbitrarily slow 
depending on the random test frequencies used. Our analysis instead uses the assumption p  q ∈ H s(cid:48)

to ensure that small  ZD-valued frequencies contain most of the power of(cid:101)p. 7
At the other extreme (θ = 1) are MMD-based tests of [8  9]  which use the entire spectrum(cid:101)p. These

These ﬁxed-frequency approaches can be thought of as the extreme point θ = 0 of the compu-
tational/statistical trade-off described in section 7: they are computable in linear time and (with
smoothing) are strongly consistent  but do not satisfy ﬁnite-sample bounds under general conditions.

tests are statistically powerful and have strong guarantees for densities in an RKHS  but have O(n2)
computational complexity. 8 The computational/statistical trade-off discussed in Section 7 can be
thought of as an interpolation (controlled by θ) of these approaches  with runtime in the case θ = 1
approaching quadratic for large D and small s(cid:48).

10 Conclusions and future work
In this paper  we proposed nonparametric estimators for Sobolev inner products  norms and distances
of probability densities  for which we derived ﬁnite-sample bounds and asymptotic distributions.
A natural follow-up question to our work is whether estimating smoothness of a density can guide the
choice of smoothing parameters in nonparametric estimation. When analyzing many nonparametric
estimators  Sobolev norms appear as the key unknown term in error bounds. While theoretically
optimal smoothing parameter values are often suggested based on optimizing these error bounds 
our work may suggest a practical way of mimicking this procedure by plugging estimated Sobolev
norms into these bounds. For some problems  such as estimating functionals of a density  this may
be especially useful  since no error metric is typically available for cross-validation. Even when
cross-validation is an option  as in density estimation or regression  estimating smoothness may be
faster  or may suggest an appropriate range of parameter values.
Acknowledgments
This material is based upon work supported by a National Science Foundation Graduate Research
Fellowship to the ﬁrst author under Grant No. DGE-1252522.
References
[1] N. H. Anderson  P. Hall  and D. M. Titterington. Two-sample test statistics for measuring discrepancies
between two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis  50(1):41–54  1994.

[2] P. J. Bickel and Y. Ritov. Estimating integrated squared density derivatives: sharp best order of convergence

estimates. Sankhy¯a: The Indian Journal of Statistics  Series A  pages 381–393  1988.

[3] L. Birgé and P. Massart. Estimation of integral functionals of a density. The Annals of Statistics  pages

11–29  1995.

[4] K. P. Chwialkowski  A. Ramdas  D. Sejdinovic  and A. Gretton. Fast two-sample testing with analytic

representations of probability measures. In NIPS  pages 1972–1980  2015.

[5] T. Epps and K. J. Singleton. An omnibus test for the two-sample problem using the empirical characteristic

function. Journal of Statistical Computation and Simulation  26(3-4):177–203  1986.

47–61  2008.

[6] E. Giné and R. Nickl. A simple adaptive estimator of the integrated square of a density. Bernoulli  pages

7Note that smooth CFs can be used in our test by replacing ˆp(z) with 1
n

(cid:80)n
j=1 e−izXj k(x)  where k is the
inverse Fourier transform of a characteristic kernel. However  smoothing seems less desirable under Sobolev
assumptions  as it spreads the power of the CF away from small ZD-valued frequencies where our test focuses.
√
n

8Fast MMD approximations have been proposed  including the Block MMD  [37] FastMMD  [38] and

sub-sampled MMD  but these lack the statistical guarantees of MMD.

8

[7] M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. L. N. Inverardi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. J. Nonparametric Stat.  17:277–297  2005.
[8] A. Gretton  K. M. Borgwardt  M. Rasch  B. Schölkopf  and A. J. Smola. A kernel method for the

two-sample-problem. In Advances in neural information processing systems  pages 513–520  2006.

[9] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. Smola. A kernel two-sample test. The

Journal of Machine Learning Research  13(1):723–773  2012.

[10] P. Hall and J. S. Marron. Estimation of integrated squared density derivatives. Statistics & Probability

[11] C. Heathcote. A test of goodness of ﬁt for symmetric random variables. Australian Journal of Statistics 

[12] A. O. Hero  B. Ma  O. J. J. Michel  and J. Gorman. Applications of entropic spanning graphs. IEEE Signal

[13] I. Ibragimov and R. Khasminskii. On the nonparametric estimation of functionals. In Symposium in

Letters  6(2):109–115  1987.

14(2):172–181  1972.

Processing Magazine  19(5):85–95  2002.

Asymptotic Statistics  pages 41–52  1978.

[14] K. Kandasamy  A. Krishnamurthy  B. Poczos  L. Wasserman  et al. Nonparametric von Mises estimators

for entropies  divergences and mutual informations. In NIPS  pages 397–405  2015.

[15] H.-O. Kreiss and J. Oliger. Stability of the Fourier method. SIAM Journal on Numerical Analysis  16(3):

421–433  1979.

AISTATS  2015.

de mathématiques  1992.

659–681  1996.

[16] A. Krishnamurthy  K. Kandasamy  B. Poczos  and L. A. Wasserman. Nonparametric estimation of renyi

divergence and friends. In ICML  pages 919–927  2014.

[17] A. Krishnamurthy  K. Kandasamy  B. Poczos  and L. A. Wasserman. On estimating L2

2 divergence. In

[18] B. Laurent. Efﬁcient estimation of integral functionals of a density. Université de Paris-sud  Département

[19] B. Laurent et al. Efﬁcient estimation of integral functionals of a density. The Annals of Statistics  24(2):

[20] N. Leonenko  L. Pronzato  V. Savani  et al. A class of rényi information estimators for multidimensional

densities. The Annals of Statistics  36(5):2153–2182  2008.

[21] G. Leoni. A ﬁrst course in Sobolev spaces  volume 105. American Math. Society  Providence  RI  2009.
[22] K. Moon and A. Hero. Multivariate f-divergence estimation with conﬁdence. In Advances in Neural

Information Processing Systems  pages 2420–2428  2014.

[23] K. R. Moon and A. O. Hero. Ensemble estimation of multivariate f-divergence. In Information Theory

(ISIT)  2014 IEEE International Symposium on  pages 356–360. IEEE  2014.

[24] K. R. Moon  K. Sricharan  K. Greenewald  and A. O. Hero III. Improving convergence of divergence

functional ensemble estimators. arXiv preprint arXiv:1601.06884  2016.

[25] L. Pardo. Statistical inference based on divergence measures. CRC Press  2005.
[26] B. Póczos and J. G. Schneider. On the estimation of alpha-divergences. In International Conference on

Artiﬁcial Intelligence and Statistics  pages 609–617  2011.

[27] B. Póczos  L. Xiong  and J. Schneider. Nonparametric divergence estimation with applications to machine

learning on distributions. arXiv preprint arXiv:1202.3758  2012.

[28] B. Póczos  L. Xiong  D. J. Sutherland  and J. Schneider. Nonparametric kernel estimators for image
classiﬁcation. In Computer Vision and Pattern Recognition (CVPR)  2012 IEEE Conference on  pages
2989–2996. IEEE  2012.

[29] J. C. Principe. Information theoretic learning: Renyi’s entropy and kernel perspectives. Springer Science

& Business Media  2010.

[30] N. Quadrianto  J. Petterson  and A. J. Smola. Distribution matching for transduction. In Advances in

Neural Information Processing Systems  pages 1500–1508  2009.

[31] P. Ram  D. Lee  W. March  and A. G. Gray. Linear-time algorithms for pairwise statistical problems. In

Advances in Neural Information Processing Systems  pages 1527–1535  2009.

[32] T. Schweder. Window estimation of the asymptotic variance of rank estimators of location. Scandinavian

Journal of Statistics  pages 113–126  1975.

[33] S. Singh and B. Póczos. Generalized exponential concentration inequality for renyi divergence estimation.

In Proceedings of The 31st International Conference on Machine Learning  pages 333–341  2014.

[34] S. Singh and B. Póczos. Exponential concentration of a density functional estimator. In Advances in

Neural Information Processing Systems  pages 3032–3040  2014.

[35] A. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company  Incorporated  1st

edition  2008. ISBN 0387790519  9780387790510.

[36] E. Wolsztynski  E. Thierry  and L. Pronzato. Minimum-entropy estimation in semi-parametric models.

Signal Processing  85(5):937–949  2005.

[37] W. Zaremba  A. Gretton  and M. Blaschko. B-test: A non-parametric  low variance kernel two-sample test.

In Advances in neural information processing systems  pages 755–763  2013.

[38] J. Zhao and D. Meng. Fastmmd: Ensemble of circular discrepancy for efﬁcient two-sample test. Neural

computation  27(6):1345–1372  2015.

[39] A. Zygmund. Trigonometric series  volume 1. Cambridge university press  2002.

9

,Rie Johnson
Tong Zhang
Shashank Singh
Simon Du
Barnabas Poczos