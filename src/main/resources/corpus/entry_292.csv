2013,A simple example of Dirichlet process mixture inconsistency for the number of components,For data assumed to come from a finite mixture with an unknown number of components  it has become common to use Dirichlet process mixtures (DPMs) not only for density estimation  but also for inferences about the number of components. The typical approach is to use the posterior distribution on the number of components occurring so far --- that is  the posterior on the number of clusters in the observed data. However  it turns out that this posterior is not consistent --- it does not converge to the true number of components. In this note  we give an elementary demonstration of this inconsistency in what is perhaps the simplest possible setting: a DPM with normal components of unit variance  applied to data from a mixture" with one standard normal component. Further  we find that this example exhibits severe inconsistency: instead of going to 1  the posterior probability that there is one cluster goes to 0.",A simple example of Dirichlet process mixture
inconsistency for the number of components

Jeffrey W. Miller

Division of Applied Mathematics

Brown University

Providence  RI 02912

jeffrey miller@brown.edu

matthew harrison@brown.edu

Matthew T. Harrison

Division of Applied Mathematics

Brown University

Providence  RI 02912

Abstract

For data assumed to come from a ﬁnite mixture with an unknown number of com-
ponents  it has become common to use Dirichlet process mixtures (DPMs) not
only for density estimation  but also for inferences about the number of compo-
nents. The typical approach is to use the posterior distribution on the number of
clusters — that is  the posterior on the number of components represented in the
observed data. However  it turns out that this posterior is not consistent — it does
not concentrate at the true number of components. In this note  we give an elemen-
tary proof of this inconsistency in what is perhaps the simplest possible setting: a
DPM with normal components of unit variance  applied to data from a “mixture”
with one standard normal component. Further  we show that this example exhibits
severe inconsistency: instead of going to 1  the posterior probability that there is
one cluster converges (in probability) to 0.

1

Introduction

It is well-known that Dirichlet process mixtures (DPMs) of normals are consistent for the density —
that is  given data from a sufﬁciently regular density p0 the posterior converges to the point mass at
p0 (see [1] for details and references). However  it is easy to see that this does not necessarily imply
consistency for the number of components  since for example  a good estimate of the density might
include superﬂuous components having vanishingly small weight.
Despite the fact that a DPM has inﬁnitely many components with probability 1  it has become
common to apply DPMs to data assumed to come from ﬁnitely many components or “populations” 
and to apply the posterior on the number of clusters (in other words  the number of components used
in the process of generating the observed data) for inferences about the true number of components;
see [2  3  4  5  6] for a few prominent examples. Of course  if the data-generating process very
closely resembles the DPM model  then it is ﬁne to use this posterior for inferences about the number
of clusters (but beware of misspeciﬁcation; see Section 2). However  in the examples cited  the
authors evaluated the performance of their methods on data simulated from a ﬁxed ﬁnite number of
components or populations  suggesting that they found this to be more realistic than a DPM for their
applications.
Therefore  it is important to understand the behavior of this posterior when the data comes from a
ﬁnite mixture — in particular  does it concentrate at the true number of components? In this note 
we give a simple example in which a DPM is applied to data from a ﬁnite mixture and the posterior
distribution on the number of clusters does not concentrate at the true number of components. In
fact  DPMs exhibit this type of inconsistency under very general conditions [7] — however  the aim
of this note is brevity and clarity. To that end  we focus our attention on a special case that is as

1

data  for a univariate normal DPM on n i.i.d. samples from (a) N (0  1)  and (b)(cid:80)2

Figure 1: Prior (red x) and estimated posterior (blue o) of the number of clusters in the observed
5N (4k  1
2 ).
The DPM had concentration parameter α = 1 and a Normal–Gamma base measure on the mean and
precision: N (µ | 0  1/cλ)Gamma(λ | a  b) with a = 1  b = 0.1  and c = 0.001. Estimates were
made using a collapsed Gibbs sampler  with 104 burn-in sweeps and 105 sample sweeps; traceplots
and running averages were used as convergence diagnostics. Each plot shown is an average over 5
independent runs.

1

k=−2

simple as possible: a “standard normal DPM”  that is  a DPM using univariate normal components
of unit variance  with a standard normal base measure (prior on component means).
The rest of the paper is organized as follows. In Section 2  we address several pertinent questions
and consider some suggestive experimental evidence. In Section 3  we formally deﬁne the DPM
model under consideration. In Section 4  we give an elementary proof of inconsistency in the case
of a standard normal DPM on data from one component  and in Section 5  we show that on standard
normal data  a standard normal DPM is in fact severely inconsistent.

2 Discussion

It should be emphasized that these results do not diminish  in any way  the utility of Dirichlet process
mixtures as a ﬂexible prior on densities  i.e.  for Bayesian density estimation. In addition to their
widespread success in empirical studies  DPMs are backed by theoretical guarantees showing that
in many cases the posterior on the density concentrates at the true density at the minimax-optimal
rate  up to a logarithmic factor (see [1] and references therein).
Many researchers (e.g. [8  9]  among others) have empirically observed that the DPM posterior on
the number of clusters tends to overestimate the number of components  in the sense that it tends to
put its mass on a range of values greater or equal to the true number. Figure 1 illustrates this effect for
univariate normals  and similar experiments with different families of component distributions yield
similar results. Thus  while our theoretical results in Sections 4 and 5 (and in [7]) are asymptotic in
nature  experimental evidence suggests that the issue is present even in small samples.
It is natural to think that this overestimation is due to the fact that the prior on the number of clusters
diverges as n → ∞  at a log n rate. However  this does not seem to be the main issue — rather 
the problem is that DPMs strongly prefer having some tiny clusters and will introduce extra clusters
even when they are not needed (see [7] for an intuitive explanation of why this is the case).

2

In fact  many researchers have observed the presence of tiny extra clusters (e.g. [8  9])  but the reason
for this has not previously been well understood  often being incorrectly attributed to the difﬁculty of
detecting components with small weight. These tiny extra clusters are rather inconvenient  especially
in clustering applications  and are often dealt with in an ad hoc way by simply removing them. It
might be possible to consistently estimate the number of components in this way  but this remains
an open question.
A more natural solution is the following: if the number of components is unknown  put a prior on
the number of components. For example  draw the number of components s from a probability
mass function p(s) on {1  2  . . .} with p(s) > 0 for all s  draw mixing weights π = (π1  . . .   πs)
(given s)  draw component parameters θ1  . . .   θs i.i.d. (given s and π) from an appropriate prior 
and draw X1  X2  . . . i.i.d. (given s  π  and θ1:s) from the resulting mixture. This approach has been
widely used [10  11  12  13]. Under certain conditions  the posterior on the density has been shown
to concentrate at the true density at the minimax-optimal rate  up to a logarithmic factor  for any
sufﬁciently regular true density [14]. Strictly speaking  as deﬁned  such a model is not identiﬁable 
but it is fairly straightforward to modify it to be identiﬁable by choosing one representative from
each equivalence class. Subject to a modiﬁcation of this sort  it can be shown (see [10]) that under
very general conditions  when the data is from a ﬁnite mixture of the chosen family  such models are
(a.e.) consistent for the number of components  the mixing weights  the component parameters  and
the density. Also see [15] for an interesting discussion about estimating the number of components.
However  as a practical matter  when dealing with real-world data  one would not expect to ﬁnd data
coming exactly from a ﬁnite mixture of a known family (except  perhaps  in rare circumstances).
Unfortunately  even for a model as in the preceding paragraph  the posterior on the number of com-
ponents will typically be highly sensitive to misspeciﬁcation  and it seems likely that in order to
obtain robust estimators  the problem itself may need to be reformulated. We urge researchers inter-
ested in the number of components to be wary of this robustness issue  and to think carefully about
whether they really need to estimate the number of components  or whether some other measure of
heterogeneity will sufﬁce.

3 Setup

In this section  we deﬁne the Dirichlet process mixture model under consideration.

3.1 Dirichlet process mixture model

The DPM model was introduced by Ferguson [16] and Lo [17] for the purpose of Bayesian den-
sity estimation  and was made practical through the efforts of several authors (see [18] and ref-
erences therein). We will use p(·) to denote probabilities under the DPM model (as opposed to
other probability distributions that will be considered in what follows). The core of the DPM is the
so-called Chinese restaurant process (CRP)  which deﬁnes a certain probability distribution on par-
titions. Given n ∈ {1  2  . . .} and t ∈ {1  . . .   n}  let At(n) denote the set of all ordered partitions
(A1  . . .   At) of {1  . . .   n} into t nonempty sets. In other words 

At(n) =

(A1  . . .   At) : A1  . . .   At are disjoint 

Ai = {1  . . .   n}  |Ai| ≥ 1 ∀i

(cid:83)n
The CRP with concentration parameter α > 0 deﬁnes a probability mass function on A(n) =
t=1 At(n) by setting

i=1

p(A) =

αt

α(n) t!

(|Ai| − 1)!

t(cid:89)

i=1

(cid:110)

t(cid:91)

(cid:111)

.

for A ∈ At(n)  where α(n) = α(α + 1)··· (α + n − 1). Note that since t is a function of A 
we have p(A) = p(A  t). (It is more common to see this distribution deﬁned in terms of unordered
partitions {A1  . . .   At}  in which case the t! does not appear in the denominator — however  for our
purposes it is more convenient to use the distribution on ordered partitions (A1  . . .   At) obtained
by uniformly permuting the parts. This does not affect the prior or posterior on t.)

3

Consider the hierarchical model

p(A  t) = p(A) =

(|Ai| − 1)! 

(3.1)

t(cid:89)

αt

α(n) t!

i=1

t(cid:89)

p(θ1:t | A  t) =

i=1

p(x1:n | θ1:t  A  t) =

π(θi)  and

t(cid:89)

(cid:89)

i=1

j∈Ai

pθi(xj) 

where π(θ) is a prior on component parameters θ ∈ Θ  and {pθ : θ ∈ Θ} is a parametrized family
of distributions on x ∈ X for the components. Typically  X ⊂ Rd and Θ ⊂ Rk for some d and k.
Here  x1:n = (x1  . . .   xn) with xi ∈ X   and θ1:t = (θ1  . . .   θt) with θi ∈ Θ. This hierarchical
model is referred to as a Dirichlet process mixture (DPM) model.

The prior on the number of clusters t under this model is pn(t) = (cid:80)
“cluster”: a component is part of a mixture distribution (e.g. a mixture(cid:80)∞

A∈At(n) p(A  t). We use Tn
(rather than T ) to denote the random variable representing the number of clusters  as a reminder
that its distribution depends on n. Note that we distinguish between the terms “component” and
i=1 πipθi has components
pθ1   pθ2  . . . )  while a cluster is the set of indices of data points coming from a given component
(e.g. in the DPM model above  A1  . . .   At are the clusters).
Since we are concerned with the posterior distribution p(Tn = t | x1:n) on the number of clusters 
we will be especially interested in the marginal distribution on (x1:n  t)  given by

p(x1:n  Tn = t) =

=

=

(cid:90)

(cid:88)
(cid:88)
(cid:88)

A∈At(n)

p(A)

p(A)

A∈At(n)

A∈At(n)

p(x1:n  θ1:t  A  t) dθ1:t

(cid:17)

pθi(xj)

(cid:90) (cid:16) (cid:89)

j∈Ai

m(xAi)

t(cid:89)
t(cid:89)

i=1

i=1

π(θi) dθi

(3.2)

where for any subset of indices S ⊂ {1  . . .   n}  we denote xS = (xj : j ∈ S) and let m(xS)
denote the single-cluster marginal of xS 

m(xS) =

pθ(xj)

π(θ) dθ.

(3.3)

(cid:90) (cid:16)(cid:89)

j∈S

(cid:17)

3.2 Specialization to the standard normal case

In this note  for brevity and clarity  we focus on the univariate normal case with unit variance  with
a standard normal prior on means — that is  for x ∈ R and θ ∈ R 

pθ(x) = N (x | θ  1) =
π(θ) = N (θ | 0  1) =

1√
2π

exp(− 1

2 (x − θ)2) 

and

1√
2π

exp(− 1

2 θ2).

It is a straightforward calculation to show that the single-cluster marginal is then

m(x1:n) =

1√
n + 1

p0(x1:n) exp

xj

 

(3.4)

where p0(x1:n) = p0(x1)··· p0(xn) (and p0 is the N (0  1) density). When pθ(x) and π(θ) are as
above  we refer to the resulting DPM as a standard normal DPM.

4

(cid:16) 1

(cid:16) n(cid:88)

(cid:17)2(cid:17)

1

2

n + 1

j=1

4 Simple example of inconsistency

In this section  we prove the following result  exhibiting a simple example in which a DPM is
inconsistent for the number of components: even when the true number of components is 1 (e.g.
N (µ  1) data)  the posterior probability of Tn = 1 does not converge to 1. Interestingly  the result
applies even when X1  X2  . . . are identically equal to a constant c ∈ R. To keep it simple  we set
α = 1; for more general results  see [7].
Theorem 4.1. If X1  X2  . . . ∈ R are i.i.d. from any distribution with E|Xi| < ∞  then with
probability 1  under the standard normal DPM with α = 1 as deﬁned above  p(Tn = 1 | X1:n) does
not converge to 1 as n → ∞.
Proof. Let n ∈ {2  3  . . .}. Let x1  . . .   xn ∈ R  A ∈ A2(n)  and ai = |Ai| for i = 1  2.
xj for i = 1  2. Using Equation 3.4 and noting that
1/(n + 1) ≤ 1/(n + 2) + 1/n2  we have

j=1 xj and sAi = (cid:80)

Deﬁne sn = (cid:80)n

j∈Ai

2

s2
n

= exp

n + 1
n)  where xn = 1
n

(cid:16) 1
(cid:16) sA1
(cid:17)2 ≤ a1 + 1
(cid:17)

√

a1 + 1

s2
A2

+

1
2

(cid:16) sn
(cid:16) 1

s2
A1

√

m(x1:n)
p0(x1:n)
The second factor equals exp( 1
2 x2

n + 1

n + 2
and thus  the ﬁrst factor is less or equal to

n + 2

exp

Hence 

2

a1 + 1

a2 + 1

s2
n

(cid:17)

(cid:16) 1
(cid:17) ≤ exp
(cid:80)n
(cid:17)2
j=1 xj. By the convexity of x (cid:55)→ x2 

(cid:17)
(cid:16) sA2

(cid:16) 1
(cid:17)2

s2
n
n2

n + 2

exp

2

2

.

+

a2 + 1
n + 2

 

a2 + 1

=

a1 + 1

√

a2 + 1

m(xA1 ) m(xA2 )

p0(x1:n)

.

m(x1:n)

m(xA1) m(xA2 )

√

≤

√

a2 + 1

√
a1 + 1

n + 1

exp( 1

2 x2

n).

(4.1)

Consequently  we have

p(x1:n  Tn = 2)
p(x1:n  Tn = 1)

n p(A)

n p(A)

m(xA1) m(xA2 )

m(x1:n)

√

(cid:112)|A1| + 1(cid:112)|A2| + 1

n + 1

(n − 2)!
n! 2!

n

√
n + 1√
√
n
2

exp(− 1

2 x2
n)

exp(− 1

2 x2
n)

A∈A2(n)

(a)
=

(cid:88)
(b)≥ (cid:88)
(c)≥ (cid:88)

A∈A2(n)

A∈A2(n):
|A1|=1
√
(d)≥ 1
2

2

exp(− 1

2 x2

n) 

where step (a) follows from applying Equation 3.2 to both numerator and denominator  plus using
Equation 3.1 (with α = 1) to see that p(A) = 1/n when A = ({1  . . .   n})  step (b) follows from
Equation 4.1 above  step (c) follows since all the terms in the sum are nonnegative and p(A) =
(n − 2)!/n! 2! when |A1| = 1 (by Equation 3.1  with α = 1)  and step (d) follows since there are n
partitions A ∈ A2(n) such that |A1| = 1.
If X1  X2  . . . ∈ R are i.i.d. with µ = EXj ﬁnite  then by the law of large numbers  X n =

1
n

(cid:80)n
j=1 Xj → µ almost surely as n → ∞. Therefore 
p(Tn = 1 | X1:n) =
≤
≤

p(X1:n  Tn = 1)
t=1 p(X1:n  Tn = t)

(cid:80)∞

a.s.−−→

1
exp(− 1

√
1 + 1
2
Hence  almost surely  p(Tn = 1 | X1:n) does not converge to 1.

√
1 + 1
2

2 X

2
n)

2

p(X1:n  Tn = 1)

p(X1:n  Tn = 1) + p(X1:n  Tn = 2)

1
exp(− 1

2 µ2)

2

< 1.

5

5 Severe inconsistency
In the previous section  we showed that p(Tn = 1 | X1:n) does not converge to 1 for a standard
normal DPM on any data with ﬁnite mean. In this section  we prove that in fact  it converges to 0  at
least on standard normal data. This vividly illustrates that improperly using DPMs in this way can
lead to entirely misleading results. The key step in the proof is an application of Hoeffding’s strong
law of large numbers for U-statistics.
Theorem 5.1. If X1  X2  . . . ∼ N (0  1) i.i.d. then
p(Tn = 1 | X1:n)

as n → ∞

Pr−→ 0

under the standard normal DPM with concentration parameter α = 1.

Proof. For t = 1 and t = 2 deﬁne

Rt(X1:n) = n3/2 p(X1:n  Tn = t)

p0(X1:n)

.

Our method of proof is as follows. We will show that

R2(X1:n)

Pr−−−−→
n→∞ ∞

(or in other words  for any B > 0 we have P(R2(X1:n) > B) → 1 as n → ∞)  and we will show
that R1(X1:n) is bounded in probability:

R1(X1:n) = OP (1)

(or in other words  for any ε > 0 there exists Bε > 0 such that P(R1(X1:n) > Bε) ≤ ε for all
n ∈ {1  2  . . .}). Putting these two together  we will have

p(Tn = 1 | X1:n) =

p(X1:n  Tn = 1)
t=1 p(X1:n  Tn = t)

≤ p(X1:n  Tn = 1)
p(X1:n  Tn = 2)

=

R1(X1:n)
R2(X1:n)

Pr−−−−→
n→∞ 0.

First  let’s show that R2(X1:n) → ∞ in probability. For S ⊂ {1  . . .   n} with |S| ≥ 1  deﬁne h(xS)
by

h(xS) =

m(xS)
p0(xS)

=

exp

1

|S| + 1

1(cid:112)|S| + 1

(cid:17)2(cid:17)

xj

 

(cid:16)(cid:88)

j∈S

where m is the single-cluster marginal as in Equations 3.3 and 3.4. Note that when 1 ≤ |S| ≤ n− 1 
we have

n h(xS) ≥ 1. Note also that Eh(XS) = 1 since

√

(cid:80)∞

(cid:90)

(cid:16) 1

2

(cid:90)

Eh(XS) =

h(xS) p0(xS) dxS =

m(xS) dxS = 1 

using the fact that m(xS) is a density with respect to Lebesgue measure. For k ∈ {1  . . .   n}  deﬁne
the U-statistics

(cid:1) (cid:88)
1(cid:0)n

k

|S|=k

Uk(X1:n) =

h(XS)

where the sum is over all S ⊂ {1  . . .   n} such that |S| = k. By Hoeffding’s strong law of large
numbers for U-statistics [19] 

Uk(X1:n)

a.s.−−−−→
n→∞

Eh(X1:k) = 1

6

for any k ∈ {1  2  . . .}. Therefore  using Equations 3.1 and 3.2 we have that for any K ∈ {1  2  . . .}
and any n > K 

m(XA1 ) m(XA2 )

p0(X1:n)

A∈A2(n)

p(A)
√

p(A)

n h(XA1) h(XA2)

p(A) h(XA1)

(k − 1)! (n − k − 1)!

h(XS)

= n

≥ n

A∈A2(n)

A∈A2(n)

R2(X1:n) = n3/2 (cid:88)
(cid:88)
(cid:88)
(cid:88)
n−1(cid:88)
n−1(cid:88)
n−1(cid:88)
≥ K(cid:88)

= n

k=1

k=1

k=1

=

=

n

n

n

|S|=k

2k(n − k)

2k(n − k)

2k(n − k)

k=1

a.s.−−−−→
n→∞

K(cid:88)

k=1

n! 2!

(cid:1) (cid:88)
1(cid:0)n

k

|S|=k

h(XS)

Uk(X1:n)

Uk(X1:n)

1
2k

=

HK
2

>

log K

2

where HK is the K th harmonic number  and the last inequality follows from the standard bounds
[20] on harmonic numbers: log K < HK ≤ log K + 1. Hence  for any K 

and it follows easily that

lim inf
n→∞ R2(X1:n) >

log K

2

almost surely 

R2(X1:n)

a.s.−−−−→
n→∞ ∞.

Convergence in probability is implied by almost sure convergence.
Now  let’s show that R1(X1:n) = OP (1). By Equations 3.1  3.2  and 3.4  we have

R1(X1:n) = n3/2 p(X1:n  Tn = 1)

p0(X1:n)

(cid:16) 1

n

√
n√
n + 1

n

=

√

(cid:16) 1√

m(X1:n)
p0(X1:n)

n(cid:88)

(cid:17)2(cid:17) ≤ exp(Z 2

√
where Zn = (1/
conclude that R1(X1:n) = OP (1). This completes the proof.

=
i=1 Xi ∼ N (0  1) for each n ∈ {1  2  . . .}. Since Zn = OP (1) then we

n + 1

n/2)

exp

Xi

i=1

n

2

n)(cid:80)n

Acknowledgments

We would like to thank Stu Geman for raising this question  and the anonymous referees for several
helpful suggestions that improved the quality of this manuscript. This research was supported in part
by the National Science Foundation under grant DMS-1007593 and the Defense Advanced Research
Projects Agency under contract FA8650-11-1-715.

References
[1] S. Ghosal. The Dirichlet process  related priors and posterior asymptotics.

In N.L. Hjort 
C. Holmes  P. M¨uller  and S.G. Walker  editors  Bayesian Nonparametrics  pages 36–83. Cam-
bridge University Press  2010.

7

[2] J.P. Huelsenbeck and P. Andolfatto. Inference of population structure under a Dirichlet process

model. Genetics  175(4):1787–1802  2007.

[3] M. Medvedovic and S. Sivaganesan. Bayesian inﬁnite mixture model based clustering of gene

expression proﬁles. Bioinformatics  18(9):1194–1206  2002.

[4] E. Otranto and G.M. Gallo. A nonparametric Bayesian approach to detect the number of

regimes in Markov switching models. Econometric Reviews  21(4):477–496  2002.

[5] E.P. Xing  K.A. Sohn  M.I. Jordan  and Y.W. Teh. Bayesian multi-population haplotype in-
ference via a hierarchical Dirichlet process mixture. In Proceedings of the 23rd International
Conference on Machine Learning  pages 1049–1056  2006.

[6] P. Fearnhead. Particle ﬁlters for mixture models with an unknown number of components.

Statistics and Computing  14(1):11–21  2004.

[7] J. W. Miller and M. T. Harrison. Inconsistency of Pitman–Yor process mixtures for the number

of components. arXiv:1309.0024  2013.

[8] M. West  P. M¨uller  and M.D. Escobar. Hierarchical priors and mixture models  with applica-
tion in regression and density estimation. Institute of Statistics and Decision Sciences  Duke
University  1994.

[9] A. Onogi  M. Nurimoto  and M. Morita. Characterization of a Bayesian genetic clustering
algorithm based on a Dirichlet process prior and comparison among Bayesian clustering meth-
ods. BMC Bioinformatics  12(1):263  2011.

[10] A. Nobile. Bayesian Analysis of Finite Mixture Distributions. PhD thesis  Department of

Statistics  Carnegie Mellon University  Pittsburgh  PA  1994.

[11] S. Richardson and P.J. Green. On Bayesian analysis of mixtures with an unknown number of

components. Journal of the Royal Statistical Society. Series B  59(4):731–792  1997.

[12] P.J. Green and S. Richardson. Modeling heterogeneity with and without the Dirichlet process.

Scandinavian Journal of Statistics  28(2):355–375  June 2001.

[13] A. Nobile and A.T. Fearnside. Bayesian ﬁnite mixtures with an unknown number of compo-

nents: The allocation sampler. Statistics and Computing  17(2):147–162  2007.

[14] W. Kruijer  J. Rousseau  and A. Van der Vaart. Adaptive Bayesian density estimation with

location-scale mixtures. Electronic Journal of Statistics  4:1225–1257  2010.

[15] P. McCullagh and J. Yang. How many clusters? Bayesian Analysis  3(1):101–120  2008.
[16] T.S. Ferguson. Bayesian density estimation by mixtures of normal distributions.

In M. H.
Rizvi  J. Rustagi  and D. Siegmund  editors  Recent Advances in Statistics  pages 287–302.
Academic Press  1983.

[17] A. Y. Lo. On a class of Bayesian nonparametric estimates: I. Density estimates. The Annals of

Statistics  12(1):351–357  1984.

[18] M.D. Escobar and M. West. Computing nonparametric hierarchical models.

In D. Dey 
P. M¨uller  and D. Sinha  editors  Practical Nonparametric and Semiparametric Bayesian Statis-
tics  pages 1–22. Springer-Verlag  New York  1998.

[19] W. Hoeffding. The strong law of large numbers for U-statistics. Institute of Statistics  Univ. of

N. Carolina  Mimeograph Series  302  1961.

[20] R.L. Graham  D.E. Knuth  and O. Patashnik. Concrete Mathematics. Addison-Wesley  1989.

8

,Jeffrey Miller
Matthew Harrison