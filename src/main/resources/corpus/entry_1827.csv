2009,Ensemble Nystrom Method,A crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices. We introduce a new family of algorithms based on mixtures of Nystrom approximations  ensemble Nystrom algorithms  that yield more accurate low-rank approximations than the standard Nystrom method. We give a detailed study of multiple variants of these algorithms based on simple averaging  an exponential weight method  or regression-based methods. We also present a theoretical analysis of these algorithms  including novel error bounds guaranteeing a better convergence rate than the standard Nystrom method. Finally  we report the results of extensive experiments with several data sets containing up to 1M points demonstrating the signiﬁcant performance improvements gained over the standard Nystrom approximation.,Ensemble Nystr¨om Method

Sanjiv Kumar
Google Research
New York  NY

Courant Institute and Google Research

Mehryar Mohri

New York  NY

sanjivk@google.com

mohri@cs.nyu.edu

Courant Institute of Mathematical Sciences

Ameet Talwalkar

New York  NY

ameet@cs.nyu.edu

Abstract

A crucial technique for scaling kernel methods to very large data sets reaching
or exceeding millions of instances is based on low-rank approximation of kernel
matrices. We introduce a new family of algorithms based on mixtures of Nystr¨om
approximations  ensemble Nystr¨om algorithms  that yield more accurate low-rank
approximations than the standard Nystr¨om method. We give a detailed study of
variants of these algorithms based on simple averaging  an exponential weight
method  or regression-based methods. We also present a theoretical analysis of
these algorithms  including novel error bounds guaranteeing a better convergence
rate than the standard Nystr¨om method. Finally  we report results of extensive
experiments with several data sets containing up to 1M points demonstrating the
signiﬁcant improvement over the standard Nystr¨om approximation.

1 Introduction

Modern learning problems in computer vision  natural language processing  computational biology 
and other areas are often based on large data sets of tens of thousands to millions of training in-
stances. But  several standard learning algorithms such as support vector machines (SVMs) [2  4] 
kernel ridge regression (KRR) [14]  kernel principal component analysis (KPCA) [15]  manifold
learning [13]  or other kernel-based algorithms do not scale to such orders of magnitude. Even the
storage of the kernel matrix is an issue at this scale since it is often not sparse and the number of
entries is extremely large. One solution to deal with such large data sets is to use an approximation
of the kernel matrix. As shown by [18]  later by [6  17  19]  low-rank approximations of the kernel
matrix using the Nystr¨om method can provide an effective technique for tackling large-scale scale
data sets with no signiﬁcant decrease in performance.

This paper deals with very large-scale applications where the sample size can reach millions of in-
stances. This motivates our search for further improved low-rank approximations that can scale to
such orders of magnitude and generate accurate approximations. We show that a new family of al-
gorithms based on mixtures of Nystr¨om approximations  ensemble Nystr¨om algorithms  yields more
accurate low-rank approximations than the standard Nystr¨om method. Moreover  these ensemble al-
gorithms naturally ﬁt distributed computing environment where their computational cost is roughly
the same as that of the standard Nystr¨om method. This issue is of great practical signiﬁcance given
the prevalence of distributed computing frameworks to handle large-scale learning problems.

The remainder of this paper is organized as follows. Section 2 gives an overview of the Nystr¨om
low-rank approximation method and describes our ensemble Nystr¨om algorithms. We describe sev-
eral variants of these algorithms  including one based on simple averaging of p Nystr¨om solutions 

1

an exponential weight method  and a regression method which consists of estimating the mixture pa-
rameters of the ensemble using a few columns sampled from the matrix. In Section 3  we present a
theoretical analysis of ensemble Nystr¨om algorithms  namely bounds on the reconstruction error for
both the Frobenius norm and the spectral norm. These novel generalization bounds guarantee a bet-
ter convergence rate for these algorithms in comparison to the standard Nystr¨om method. Section 4
reports the results of extensive experiments with these algorithms on several data sets containing up
to 1M points  comparing different variants of our ensemble Nystr¨om algorithms and demonstrating
the performance improvements gained over the standard Nystr¨om method.

2 Algorithm

We ﬁrst give a brief overview of the Nystr¨om low-rank approximation method  introduce the notation
used in the following sections  and then describe our ensemble Nystr¨om algorithms.

2.1 Standard Nystr¨om method

We adopt a notation similar to that of [5  9] and other previous work. The Nystr¨om approximation
of a symmetric positive semideﬁnite (SPSD) matrix K is based on a sample of m ≪ n columns
of K [5  18]. Let C denote the n× m matrix formed by these columns and W the m× m matrix
consisting of the intersection of these m columns with the corresponding m rows of K. The columns
and rows of K can be rearranged based on this sampling so that K and C be written as follows:

K =(cid:20) W K⊤21
K21 K22(cid:21)

Note that W is also SPSD since K is SPSD. For a uniform sampling of the columns  the Nystr¨om

and C =(cid:20) W
K21(cid:21) .
method generates a rank-k approximation eK of K for k≤ m deﬁned by:
(2)
that is Wk =
where Wk is the best k-rank approximation of W for the Frobenius norm 
argminrank(V)=k kW − VkF and W+
k can be de-
rived from the singular value decomposition (SVD) of W  W = UΣU⊤  where U is orthonormal
and Σ = diag(σ1  . . .   σm) is a real diagonal matrix with σ1 ≥···≥ σm ≥ 0. For k ≤ rank(W)  it
UiUi⊤  where Ui denotes the ith column of U. Since the running
is given by W+
time complexity of SVD is O(m3) and O(nmk) is required for multiplication with C  the total
complexity of the Nystr¨om approximation computation is O(m3 +nmk).

k denotes the pseudo-inverse of Wk [7]. W+

eK = CW+

k

k =Pk

i=1 σ−1

i

C⊤ ≈ K 

(1)

2.2 Ensemble Nystr¨om algorithm

The main idea behind our ensemble Nystr¨om algorithm is to treat each approximation generated by
the Nystr¨om method for a sample of m columns as an expert and to combine p≥ 1 such experts to
derive an improved hypothesis  typically more accurate than any of the original experts.
The learning set-up is deﬁned as follows. We assume a ﬁxed kernel function K : X ×X → R that
can be used to generate the entries of a kernel matrix K. The learner receives a sample S of mp
columns randomly selected from matrix K uniformly without replacement. S is decomposed into
p subsamples S1 . . .  Sp. Each subsample Sr  r ∈ [1  p]  contains m columns and is used to deﬁne
a rank-k Nystr¨om approximation eKr. Dropping the rank subscript k in favor of the sample index
r  eKr can be written as eKr = CrW+
C⊤r   where Cr and Wr denote the matrices formed from
r is the pseudo-inverse of the rank-k approximation of Wr. The learner
the columns of Sr and W+
further receives a sample V of s columns used to determine the weight µr ∈ R attributed to each
expert eKr. Thus  the general form of the approximation of K generated by the ensemble Nystr¨om
algorithm is

r

eKens =

pXr=1

µreKr.

2

The mixture weights µr can be deﬁned in many ways. The most straightforward choice consists of
assigning equal weight to each expert  µr = 1/p  r ∈ [1  p]. This choice does not require the addi-
tional sample V   but it ignores the relative quality of each Nystr¨om approximation. Nevertheless 

(3)

this simple uniform method already generates a solution superior to any one of the approximations

Another method  the exponential weight method  consists of measuring the reconstruction error ˆǫr of

eKr used in the combination  as we shall see in the experimental section.
each expert eKr over the validation sample V and deﬁning the mixture weight as µr = exp(−ηˆǫr)/Z 
where η > 0 is a parameter of the algorithm and Z a normalization factor ensuring that the vector
µ = (µ1  . . .   µp) belongs to the simplex ∆ of Rp: ∆ = {µ ∈ Rp : µ ≥ 0 ∧Pp
r=1 µr = 1}. The
choice of the mixture weights here is similar to those used in the weighted-majority algorithm [11].
Let KV denote the matrix formed by using the samples from V as its columns and let eKV
r denote
the submatrix of eKr containing the columns corresponding to the columns in V . The reconstruction
error ˆǫr =keKV

A more general class of methods consists of using the sample V to train the mixture weights µr to
optimize a regression objective function such as the following:

r − KV k can be directly computed from these matrices.

min

µ

λkµk2

2 + k

pXr=1

r − KV k2
F  

µreKV

(4)

where KV denotes the matrix formed by the columns of the samples S and V and λ > 0. This can
be viewed as a ridge regression objective function and admits a closed form solution. We will refer
to this method as the ridge regression method.
The total complexity of the ensemble Nystr¨om algorithm is O(pm3 + pmkn + Cµ)  where Cµ is
the cost of computing the mixture weights  µ  used to combine the p Nystr¨om approximations. In
general  the cubic term dominates the complexity since the mixture weights can be computed in
constant time for the uniform method  in O(psn) for the exponential weight method  or in O(p3 +
pms) for the ridge regression method. Furthermore  although the ensemble Nystr¨om algorithm
requires p times more space and CPU cycles than the standard Nystr¨om method  these additional
requirements are quite reasonable in practice. The space requirement is still manageable for even
large-scale applications given that p is typically O(1) and m is usually a very small percentage of
n (see Section 4 for further details). In terms of CPU requirements  we note that our algorithm
can be easily parallelized  as all p experts can be computed simultaneously. Thus  with a cluster
of p machines  the running time complexity of this algorithm is nearly equal to that of the standard
Nystr¨om algorithm with m samples.

3 Theoretical analysis

We now present a theoretical analysis of the ensemble Nystr¨om method for which we use as tools
some results previously shown by [5] and [9]. As in [9]  we shall use the following generalization
of McDiarmid’s concentration bound to sampling without replacement [3].
Theorem 1. Let Z1  . . .   Zm be a sequence of random variables sampled uniformly without re-
placement from a ﬁxed set of m + u elements Z  and let φ : Z m → R be a symmetric function
such that for all i ∈ [1  m] and for all z1  . . .   zm ∈ Z and z′1  . . .   z′m ∈ Z  |φ(z1  . . .   zm)−
φ(z1  . . .   zi−1  z′i  zi+1  . . .   zm)|≤ c. Then  for all ǫ > 0  the following inequality holds:

where α(m  u) =

mu

m+u−1/2

Pr(cid:2)φ − E[φ] ≥ ǫ(cid:3) ≤ exp(cid:2) −2ǫ2
1−1/(2 max{m u}) .

α(m u)c2(cid:3) 

1

(5)

We deﬁne the selection matrix corresponding to a sample of m columns as the matrix S ∈ Rn×m
deﬁned by Sii = 1 if the ith column of K is among those sampled  Sij = 0 otherwise. Thus  C = KS
is the matrix formed by the columns sampled. Since K is SPSD  there exists X ∈ RN×n such that
K = X⊤X. We shall denote by Kmax the maximum diagonal entry of K  Kmax = maxi Kii  and
by dK

max the distance maxijpKii + Kjj − 2Kij.

3.1 Error bounds for the standard Nystr¨om method

The following theorem gives an upper bound on the norm-2 error of the Nystr¨om approximation of

the form kK−eKk2/kKk2 ≤ kK− Kkk2/kKk2 + O(1/√m) and an upper bound on the Frobenius

3

1

error of the Nystr¨om approximation of the form kK − eKkF /kKkF ≤ kK − KkkF /kKkF +

4 ). Note that these bounds are similar to the bounds in Theorem 3 in [9]  though in this
O(1/m
work we give new results for the spectral norm and present a tighter Lipschitz condition (9)  the
latter of which is needed to derive tighter bounds in Section 3.2.

Theorem 2. Let eK denote the rank-k Nystr¨om approximation of K based on m columns sampled
uniformly at random without replacement from K  and Kk the best rank-k approximation of K.
Then  with probability at least 1 − δ  the following inequalities hold for any sample of size m:

Kmaxh1 +q n−m
kK − eKk2 ≤ kK − Kkk2 + 2n√m
4 nKmaxh1 +q n−m
kK − eKkF ≤ kK − KkkF +(cid:2) 64k
m (cid:3) 1

n−1/2

n−1/2

1

1

β(m n) log 1

δ dK

max/K

1

2

maxi

1

β(m n) log 1

δ dK

max/K

2

2

1

maxi 1

 

where β(m  n) = 1−
Proof. To bound the norm-2 error of the Nystr¨om method in the scenario of sampling without re-
placement  we start with the following general inequality given by [5][proof of Lemma 4]:

2 max{m n−m}

.

m

kK − eKk2 ≤ kK − Kkk2 + 2kXX⊤ − ZZ⊤k2 

(6)
where Z = p n
XS. We then apply the McDiarmid-type inequality of Theorem 1 to φ(S) =
kXX⊤−ZZ⊤k2. Let S′ be a sampling matrix selecting the same columns as S except for one  and
let Z′ denotep n

XS′. Let z and z′ denote the only differing columns of Z and Z′  then

|φ(S′) − φ(S)| ≤ kz′z′⊤ − zz⊤k2 = k(z′ − z)z′⊤ + z(z′ − z)⊤k2

m

≤ 2kz′ − zk2 max{kzk2 kz′k2}.

Columns of Z are those of X scaled bypn/m. The norm of the difference of two columns of X

can be viewed as the norm of the difference of two feature vectors associated to K and thus can be
bounded by dK. Similarly  the norm of a single column of X is bounded by K
max. This leads to the
following inequality:

1

2

|φ(S′) − φ(S)| ≤
The expectation of φ can be bounded as follows:

2n
m

dK
max

1

K

2

max.

(7)
(8)

(9)

(10)

E[Φ] = E[kXX⊤ − ZZ⊤k2] ≤ E[kXX⊤ − ZZ⊤kF ] ≤

n
√m

Kmax 

where the last inequality follows Corollary 2 of [9]. The inequalities (9) and (10) combined with
Theorem 1 give a bound on kXX⊤ − ZZ⊤k2 and yield the statement of the theorem.
The following general inequality holds for the Frobenius error of the Nystr¨om method [5]:

F ≤ kK − Kkk2
Bounding the term kXX⊤− ZZ⊤k2
Theorem 1 yields the result of the theorem.

kK − eKk2

F + √64k kXX⊤ − ZZ⊤k2

(11)
F as in the norm-2 case and using the concentration bound of

F nKmax

ii

.

3.2 Error bounds for the ensemble Nystr¨om method

The following error bounds hold for ensemble Nystr¨om methods based on a convex combination of
Nystr¨om approximations.
Theorem 3. Let S be a sample of pm columns drawn uniformly at random without replacement

from K  decomposed into p subsamples of size m  S1  . . .   Sp. For r ∈ [1  p]  let eKr denote the
rank-k Nystr¨om approximation of K based on the sample Sr  and let Kk denote the best rank-k
approximation of K. Then  with probability at least 1 − δ  the following inequalities hold for any
sample S of size pm and for any µ in the simplex ∆ and eKens =Pp
2q n−pm
Kmaxh1 + µmaxp
kK − eKensk2 ≤ kK − Kkk2 + 2n√m
2q n−pm
4 nKmaxh1 + µmaxp
m (cid:3) 1
kK − eKenskF ≤ kK − KkkF +(cid:2) 64k
where β(pm  n) = 1−

r=1 µreKr:

and µmax = maxp

β(pm n) log 1

β(pm n) log 1

maxi

2 max{pm n−pm}

r=1 µr.

max/K

max/K

δ dK

δ dK

n−1/2

n−1/2

maxi 1

1

1

1

2

2

1

1

2

1

1

 

4

Proof. For r ∈ [1  p]  let Zr = pn/m XSr  where Sr denotes the selection matrix corresponding
to the sample Sr. By deﬁnition of eKens and the upper bound on kK − eKrk2 already used in the

proof of theorem 2  the following holds:

pXr=1

kK − eKensk2 =(cid:13)(cid:13)(cid:13)
µr(K − eKr)(cid:13)(cid:13)(cid:13)2 ≤
pXr=1
µrkK − eKrk2
pXr=1
µr(cid:0)kK − Kkk2 + 2kXX⊤ − Zr Z⊤r k2(cid:1)
µrkXX⊤ − Zr Z⊤r k2.
= kK − Kkk2 + 2

≤

pXr=1

(12)

(13)

(14)

We apply Theorem 1 to φ(S) =Pp
r=1 µrkXX⊤ − Zr Z⊤r k2. Let S′ be a sample differing from
S by only one column. Observe that changing one column of the full sample S changes only one
subsample Sr and thus only one term µrkXX⊤ − Zr Z⊤r k2. Thus  in view of the bound (9) on the
change to kXX⊤ − Zr Z⊤r k2  the following holds:
2n
m

|φ(S′) − φ(S)| ≤

µmaxdK

max 

(15)

n√m

r=1 µr

The expectation of Φ can be straightforwardly bounded by E[Φ(S)] = Pp
Zr Z⊤r k2] ≤ Pp

r=1 µr E[kXX⊤ −
Kmax using the bound (10) for a single expert. Plugging
in this upper bound and the Lipschitz bound (15) in Theorem 1 yields our norm-2 bound for the
ensemble Nystr¨om method.
For the Frobenius error bound  using the convexity of the Frobenius norm square k·k2
general inequality (11)  we can write

Kmax = n√m

F and the

max

K

1

2

kK − eKensk2

pXr=1

2

F =(cid:13)(cid:13)(cid:13)
µr(K − eKr)(cid:13)(cid:13)(cid:13)
pXr=1
µrhkK − Kkk2
pXr=1
F + √64k
= kK − Kkk2

≤

F

µrkK − eKrk2

F ≤
F + √64k kXX⊤ − Zr Z⊤r kF nKmax
pXr=1

µrkXX⊤ − Zr Z⊤r kF nKmax

ii

ii

.

i.

(16)

(17)

(18)

The result follows by the application of Theorem 1 to ψ(S) =Pp

similar to the norm-2 case.

r=1 µrkXX⊤ − Zr Z⊤r kF in a way

The bounds of Theorem 3 are similar in form to those of Theorem 2. However  the bounds for the
ensemble Nystr¨om are tighter than those for any Nystr¨om expert based on a single sample of size
m even for a uniform weighting. In particular  for µ = 1/p  the last term of the ensemble bound for
norm-2 is smaller by a factor larger than µmaxp

2 = 1/√p.

1

4 Experiments

In this section  we present experimental results that illustrate the performance of the ensemble
Nystr¨om method. We work with the datasets listed in Table 1. In Section 4.1  we compare the
performance of various methods for calculating the mixture weights (µr). In Section 4.2  we show
the effectiveness of our technique on large-scale datasets. Throughout our experiments  we mea-

sure the accuracy of a low-rank approximation eK by calculating the relative error in Frobenius and
spectral norms  that is  if we let ξ = {2  F}  then we calculate the following quantity:

% error = kK − eKkξ

kKkξ × 100.

5

(19)

Dataset
PIE-2.7K [16]
MNIST [10]
ESS [8]
AB-S [1]
DEXT [1]
SIFT-1M [12]

Type of data
face images
digit images

proteins
abalones

bag of words
Image features

2731

4000

4728

4177

2304

# Points (n) # Features (d) Kernel
linear
linear
RBF
RBF
linear
RBF

2000
1M

20000

16

8

784

128

Table 1: A summary of the datasets used in the experiments.

4.1 Ensemble Nystr¨om with various mixture weights

In this set of experiments  we show results for our ensemble Nystr¨om method using different tech-
niques to choose the mixture weights as discussed in Section 2.2. We ﬁrst experimented with the
ﬁrst ﬁve datasets shown in Table 1. For each dataset  we ﬁxed the reduced rank to k = 50  and set the
number of sampled columns to m = 3% n.1 Furthermore  for the exponential and the ridge regres-
sion variants  we sampled an additional set of s = 20 columns and used an additional 20 columns
(s′) as a hold-out set for selecting the optimal values of η and λ. The number of approximations  p 
was varied from 2 to 30. As a baseline  we also measured the minimal and mean percent error across

the p Nystr¨om approximations used to construct eKens. For the Frobenius norm  we also calculated

the performance when using the optimal µ  that is  we used least-square regression to ﬁnd the best
possible choice of combination weights for a ﬁxed set of p approximations by setting s = n.
The results of these experiments are presented in Figure 1 for the Frobenius norm and in Figure 2
for the spectral norm. These results clearly show that the ensemble Nystr¨om performance is signiﬁ-
cantly better than any of the individual Nystr¨om approximations. Furthermore  the ridge regression
technique is the best of the proposed techniques and generates nearly the optimal solution in terms of
the percent error in Frobenius norm. We also observed that when s is increased to approximately 5%
to 10% of n  linear regression without any regularization performs about as well as ridge regression
for both the Frobenius and spectral norm. Figure 3 shows this comparison between linear regression
and ridge regression for varying values of s using a ﬁxed number of experts (p = 10). Finally we
note that the ensemble Nystr¨om method tends to converge very quickly  and the most signiﬁcant
gain in performance occurs as p increases from 2 to 10.

4.2 Large-scale experiments

Next  we present an empirical study of the effectiveness of the ensemble Nystr¨om method on the
SIFT-1M dataset in Table 1 containing 1 million data points. As is common practice with large-scale
datasets  we worked on a cluster of several machines for this dataset. We present results comparing
the performance of the ensemble Nystr¨om method  using both uniform and ridge regression mixture
weights  with that of the best and mean performance across the p Nystr¨om approximations used to

construct eKens. We also make comparisons with a recently proposed k-means based sampling tech-

nique for the Nystr¨om method [19]. Although the k-means technique is quite effective at generating
informative columns by exploiting the data distribution  the cost of performing k-means becomes
expensive for even moderately sized datasets  making it difﬁcult to use in large-scale settings. Nev-
ertheless  in this work  we include the k-means method in our comparison  and we present results
for various subsamples of the SIFT-1M dataset  with n ranging from 5K to 1M.
To fairly compare these techniques  we performed ‘ﬁxed-time’ experiments. To do this  we ﬁrst
searched for an appropriate m such that the percent error for the ensemble Nystr¨om method with
ridge weights was approximately 10%  and measured the time required by the cluster to construct
this approximation. We then alloted an equal amount of time (within 1 second) for the other tech-
niques  and measured the quality of the resulting approximations. For these experiments  we set
k = 50 and p = 10  based on the results from the previous section. Furthermore  in order to speed up
computation on this large dataset  we decreased the size of the validation and hold-out sets to s = 2
and s′ = 2  respectively.

1Similar results (not reported here) were observed for other values of k and m as well.

6

4.5

4

3.5

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

3
 
0

5

 Ensemble Method − PIE−2.7K

 Ensemble Method − MNIST

 

mean b.l.
best b.l.
uni
exp
ridge
optimal

16

15

14

13

12

11

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

10
 
0

5

 Ensemble Method − AB−S

10

15

20

Number of base learners (p) 

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge
optimal

0.65

0.6

0.55

0.5

0.45

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

0.4
 
0

5

 Ensemble Method − ESS

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge
optimal

25

30

40

38

36

34

32

30

28

26

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

24
 
0

5

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge
optimal

70

68

66

64

62

60

58

56

54

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

52
 
0

5

 Ensemble Method − DEXT

 

mean b.l.
best b.l.
uni
exp
ridge
optimal

25

30

10

15

20

Number of base learners (p) 

Figure 1: Percent error in Frobenius norm for ensemble Nystr¨om method using uniform (‘uni’)  ex-
ponential (‘exp’)  ridge (‘ridge’) and optimal (‘optimal’) mixture weights as well as the best (‘best
b.l.’) and mean (‘mean b.l.’) performance of the p base learners used to create the ensemble approx-
imation.

 Ensemble Method − PIE−2.7K

 Ensemble Method − MNIST

2

1.8

1.6

1.4

1.2

1

)
l
a
r
t
c
e
p
S

(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

0.8
 
0

5

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge

10

9

8

7

6

5

4

3

)
l
a
r
t
c
e
p
S

(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

2
 
0

5

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge

0.28

0.26

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.1

)
l
a
r
t
c
e
p
S

(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

0.08
 
0

5

 Ensemble Method − ESS

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge

25

30

45

40

35

30

25

20

15

)
l
a
r
t
c
e
p
S

(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

10
 
0

5

 Ensemble Method − AB−S

 Ensemble Method − DEXT

 

mean b.l.
best b.l.
uni
exp
ridge

45

40

35

30

25

20

15

)
l
a
r
t
c
e
p
S

(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

25

30

10
 
0

5

10

15

20

Number of base learners (p) 

 

mean b.l.
best b.l.
uni
exp
ridge

25

30

10

15

20

Number of base learners (p) 

Figure 2: Percent error in spectral norm for ensemble Nystr¨om method using various mixture
weights as well as the best and mean performance of the p approximations used to create the ensem-
ble approximation. Legend entries are the same as in Figure 1.

The results of this experiment  presented in Figure 4  clearly show that the ensemble Nystr¨om
method is the most effective technique given a ﬁxed amount of time. Furthermore  even with
the small values of s and s′  ensemble Nystr¨om with ridge-regression weighting outperforms the
uniform ensemble Nystr¨om method. We also observe that due to the high computational cost of
k-means for large datasets  the k-means approximation does not perform well in this ‘ﬁxed-time’
experiment. It generates an approximation that is worse than the mean standard Nystr¨om approxi-
mation and its performance increasingly deteriorates as n approaches 1M. Finally  we note that al-

7

 Effect of Ridge − PIE−2.7K

 Effect of Ridge − MNIST

 Effect of Ridge − ESS

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

3.5

3.45

3.4

3.35

 

 

no−ridge
ridge
optimal

5

10

15

20

25

 Relative size of validation set

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

10.525

10.52

10.515

10.51

10.505

10.5

10.495

 

 

no−ridge
ridge
optimal

5

10

15

20

25

 Relative size of validation set

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

0.455

0.45

0.445

 
0

 

no−ridge
ridge
optimal

5

10

15

20

25

 Relative size of validation set

 Effect of Ridge − AB−S

 Effect of Ridge − DEXT

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

28.5

28

27.5

27

26.5

26

 
0

 

no−ridge
ridge
optimal

5

10

15

20

25

 Relative size of validation set

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t

n
e
c
r
e
P

56

55.5

55

54.5

 

 

no−ridge
ridge
optimal

5

10

15

20

25

 Relative size of validation set

Figure 3: Comparison of percent error in Frobenius norm for the ensemble Nystr¨om method with p =
10 experts with weights derived from linear regression (‘no-ridge’) and ridge regression (‘ridge’).
The dotted line indicates the optimal combination. The relative size of the validation set equals
s/n×100%.

i

)
s
u
n
e
b
o
r
F
(
 
r
o
r
r

E

 
t
n
e
c
r
e
P

17

16

15

14

13

12

11

10

9

 

 Large Scale Ensemble Study

 

mean b.l.
best b.l.
uni
ridge
kmeans

104

105

 Size of dataset (n) 

106

Figure 4: Large-scale performance comparison with SIFT-1M dataset. Given ﬁxed computational
time  ensemble Nystr¨om with ridge weights tends to outperform other techniques.

though the space requirements are 10 times greater for ensemble Nystr¨om in comparison to standard
Nystr¨om (since p = 10 in this experiment)  the space constraints are nonetheless quite reasonable.
For instance  when working with the full 1M points  the ensemble Nystr¨om method with ridge re-
gression weights only required approximately 1% of the columns of K to achieve a percent error of
10%.

5 Conclusion

We presented a novel family of algorithms  ensemble Nystr¨om algorithms  for accurate low-rank ap-
proximations in large-scale applications. The consistent and signiﬁcant performance improvement
across a number of different data sets  along with the fact that these algorithms can be easily par-
allelized  suggests that these algorithms can beneﬁt a variety of applications where kernel methods
are used. Interestingly  the algorithmic solution we have proposed for scaling these kernel learning
algorithms to larger scales is itself derived from the machine learning idea of ensemble methods.
We also gave the ﬁrst theoretical analysis of these methods. We expect that ﬁner error bounds and
theoretical guarantees will further guide the design of the ensemble algorithms and help us gain a
better insight about the convergence properties of our algorithms.

8

References

[1] A. Asuncion and D. Newman. UCI machine learning repository  2007.
[2] B. E. Boser  I. Guyon  and V. N. Vapnik. A training algorithm for optimal margin classiﬁers.

In COLT  volume 5  pages 144–152  1992.

[3] C. Cortes  M. Mohri  D. Pechyony  and A. Rastogi. Stability of transductive regression algo-

rithms. In ICML  2008.

[4] C. Cortes and V. N. Vapnik. Support-Vector Networks. Machine Learning  20(3):273–297 

1995.

[5] P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a Gram matrix for

improved kernel-based learning. JMLR  6:2153–2175  2005.

[6] C. Fowlkes  S. Belongie  F. Chung  and J. Malik. Spectral grouping using the Nystr¨om method.

IEEE Transactions on Pattern Analysis and Machine Intelligence  26(2)  2004.

[7] G. Golub and C. V. Loan. Matrix Computations. Johns Hopkins University Press  Baltimore 

2nd edition  1983.

[8] A. Gustafson  E. Snitkin  S. Parker  C. DeLisi  and S. Kasif. Towards the identiﬁcation of
essential genes using targeted genome sequencing and comparative analysis. BMC:Genomics 
7:265  2006.

[9] S. Kumar  M. Mohri  and A. Talwalkar. Sampling techniques for the Nystr¨om method.

AISTATS  pages 304–311  2009.

In

[10] Y. LeCun and C. Cortes. The MNIST database of handwritten digits  2009.
[11] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-

putation  108(2):212261  1994.

[12] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal

of Computer Vision  60:91–110  2004.

[13] J. C. Platt. Fast embedding of sparse similarity graphs. In NIPS  2004.
[14] C. Saunders  A. Gammerman  and V. Vovk. Ridge Regression Learning Algorithm in Dual

Variables. In Proceedings of the ICML ’98  pages 515–521  1998.

[15] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigen-

value problem. Neural Computation  10(5):1299–1319  1998.

[16] T. Sim  S. Baker  and M. Bsat. The CMU PIE database. In Conference on Automatic Face and

Gesture Recognition  2002.

[17] A. Talwalkar  S. Kumar  and H. Rowley. Large-scale manifold learning. In CVPR  2008.
[18] C. K. I. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In

NIPS  pages 682–688  2000.

[19] K. Zhang  I. Tsang  and J. Kwok. Improved Nystr¨om low-rank approximation and error anal-

ysis. In ICML  pages 273–297  2008.

9

,Wojciech Zaremba
Arthur Gretton
Matthew Blaschko
Dan Garber