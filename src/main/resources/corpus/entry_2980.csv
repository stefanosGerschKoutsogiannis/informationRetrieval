2018,Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,Neural networks have many successful applications  while much less theoretical understanding has been gained. Towards bridging this gap  we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting  when the data comes from mixtures of well-separated distributions  we prove that SGD learns a network with a small generalization error  albeit the network has enough capacity to fit arbitrary labels. Furthermore  the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.,LearningOverparameterizedNeuralNetworksviaStochasticGradientDescentonStructuredDataYuanzhiLiComputerScienceDepartmentStanfordUniversityStanford CA94305yuanzhil@stanford.eduYingyuLiangDepartmentofComputerSciencesUniversityofWisconsin-MadisonMadison WI53706yliang@cs.wisc.eduAbstractNeuralnetworkshavemanysuccessfulapplications whilemuchlesstheoreticalunderstandinghasbeengained.Towardsbridgingthisgap westudytheproblemoflearningatwo-layeroverparameterizedReLUneuralnetworkformulti-classclas-siﬁcationviastochasticgradientdescent(SGD)fromrandominitialization.Intheoverparameterizedsetting whenthedatacomesfrommixturesofwell-separateddistributions weprovethatSGDlearnsanetworkwithasmallgeneralizationerror albeitthenetworkhasenoughcapacitytoﬁtarbitrarylabels.Furthermore theanalysisprovidesinterestinginsightsintoseveralaspectsoflearningneuralnetworksandcanbeveriﬁedbasedonempiricalstudiesonsyntheticdataandontheMNISTdataset.1IntroductionNeuralnetworkshaveachievedgreatsuccessinmanyapplications butdespitearecentincreaseoftheoreticalstudies muchremainstobeexplained.Forexample itisempiricallyobservedthatlearningwithstochasticgradientdescent(SGD)intheoverparameterizedsetting(i.e. learningalargenetworkwithnumberofparameterslargerthanthenumberoftrainingdatapoints)doesnotleadtooverﬁtting[24 31].Somerecentstudiesusethelowcomplexityofthelearnedsolutiontoexplainthegeneralization butusuallydonotexplainhowtheSGDoritsvariantsfavorslowcomplexitysolutions(i.e. theinductivebiasorimplicitregularization)[3 23].Itisalsoobservedthatoverparameterizationandproperrandominitializationcanhelptheoptimization[28 12 26 18] butitisalsonotwellunderstoodwhyaparticularinitializationcanimprovelearning.Moreover mostoftheexistingworkstryingtoexplainthesephenomenonsingeneralrelyonunrealisticassumptionsaboutthedatadistribution suchasGaussian-nessand/orlinearseparability[32 25 10 17 7].Thispaperthusproposestostudytheproblemoflearningatwo-layeroverparameterizedneuralnetworkusingSGDforclassiﬁcation ondatawithamorerealisticstructure.Inparticular thedataineachclassisamixtureofseveralcomponents andcomponentsfromdifferentclassesarewellseparatedindistance(butthecomponentsineachclasscanbeclosetoeachother).Thisismotivatedbypracticaldata.Forexample onthedatasetMNIST[15] eachclasscorrespondstoadigitandcanhaveseveralcomponentscorrespondingtodifferentwritingstylesofthedigit andanimageinitisasmallperturbationofoneofthecomponents.Ontheotherhand imagesthatbelongtothesamecomponentareclosertoeachotherthantoanimageofanotherdigit.Analysisinthissettingcanthenhelpunderstandhowthestructureofthepracticaldataaffectstheoptimizationandgeneralization.Inthissetting weprovethatwhenthenetworkissufﬁcientlyoverparameterized SGDprovablylearnsanetworkclosetotherandominitializationandwithasmallgeneralizationerror.Thisresultshowsthatintheoverparameterizedsettingandwhenthedataiswellstructured thoughinprinciple32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.thenetworkcanoverﬁt SGDwithrandominitializationintroducesastronginductivebiasandleadstogoodgeneralization.Ourresultalsoshowsthattheoverparameterizationrequirementandthelearningtimedependsontheparametersinherenttothestructureofthedatabutnotontheambientdimensionofthedata.Moreimportantly theanalysistoobtaintheresultalsoprovidessomeinterestingtheoreticalinsightsforvariousaspectsoflearningneuralnetworks.Itrevealsthatthesuccessoflearningcruciallyreliesonoverparameterizationandrandominitialization.ThesetwocombinedtogetherleadtoatightcouplingaroundtheinitializationbetweentheSGDandanotherlearningprocessthathasabenignoptimizationlandscape.Thiscoupling togetherwiththestructureofthedata allowsSGDtoﬁndasolutionthathasalowgeneralizationerror whilestillremainsintheaforementionedneighborhoodoftheinitialization.Ourworkmakesasteptowradsexplaininghowoverparameterizationandrandominitializationhelpoptimization andhowtheinductivebiasandgoodgeneralizationarisefromtheSGDdynamicsonstructureddata.Someothermoretechnicalimplicationsofouranalysiswillbediscussedinlatersections suchastheexistenceofagoodsolutionclosetotheinitialization andthelow-ranknessoftheweightslearned.ComplementaryempiricalstudiesonsyntheticdataandonthebenchmarkdatasetMNISTprovidepositivesupportfortheanalysisandinsights.2RelatedWorkGeneralizationofneuralnetworks.Empiricalstudiesshowinterestingphenomenaaboutthegeneralizationofneuralnetworks:practicalneuralnetworkshavethecapacitytoﬁtrandomlabelsofthetrainingdata yettheystillhavegoodgeneralizationwhentrainedonpracticaldata[24 31 2].Thesenetworksareoverparameterizedinthattheyhavemoreparametersthanstatisticallynecessary andtheirgoodgeneralizationcannotbeexplainedbynaïvelyapplyingtraditionaltheory.Severallinesofworkhaveproposedcertainlowcomplexitymeasuresofthelearnednetworkandderivedgeneralizationboundstobetterexplainthephenomena.[3 23 21]provedspectrally-normalizedmargin-basedgeneralizationbounds [9 23]derivedboundsfromaPAC-Bayesapproach and[1 33 4]derivedboundsfromthecompressionpointofview.They ingeneral donotaddresswhythelowcomplexityarises.Thispapertakesasteptowardsthisdirection thoughontwo-layernetworksandasimpliﬁedmodelofthedata.Overparameterizationandimplicitregularization.Thetrainingobjectivesofoverparameterizednetworksinprinciplehavemany(approximate)globaloptimaandsomegeneralizebetterthantheothers[14 8 2] whileempiricalobservationsimplythattheoptimizationprocessinpracticeprefersthosewithbettergeneralization.Itisthenaninterestingquestionhowthisimplicitregularizationorinductivebiasarisesfromtheoptimizationandthestructureofthedata.RecentstudiesareonSGDfordifferenttasks suchaslogisticregression[27]andmatrixfactorization[11 19 16].Morerelatedtoourworkis[7] whichstudiestheproblemoflearningatwo-layeroverparameterizednetworkonlinearlyseparabledataandshowsthatSGDconvergestoaglobaloptimumwithgoodgeneralization.Ourworkstudiestheproblemondatawithawellclustered(andpotentiallynotlinearlyseparable)structurethatwebelieveisclosertopracticalscenariosandthuscanadvancethislineofresearch.Theoreticalanalysisoflearningneuralnetworks.Therealsoexistsalargebodyofworkthatanalyzestheoptimizationlandscapeoflearningneuralnetworks[13 26 30 10 25 29 6 32 17 5].TheyingeneralneedtoassumeunrealisticassumptionsaboutthedatasuchasGaussian-ness and/orhavestrongassumptionsaboutthenetworksuchasusingonlylinearactivation.Theyalsodonotstudytheimplicitregularizationbytheoptimizationalgorithms.3ProblemSetupInthiswork atwo-layerneuralnetworkwithReLUactivationfork-classesclassiﬁcationisgivenbyf=(f1 f2 ··· fk)suchthatforeachi∈[k]:fi(x)=mXr=1ai rReLU(hwr xi)where{wr∈Rd}aretheweightsforthemneuronsinthehiddenlayer {ai r∈R}aretheweightsofthetoplayer andReLU(z)=max{0 z}.2Assumptionsaboutthedata.ThedataisgeneratedfromadistributionDasfollows.Therearek×lunknowndistributions{Di j}i∈[k] j∈[l]overRdandprobabilitiespi j≥0suchthatPi jpi j=1.Eachdatapoint(x y)isi.i.d.generatedby:(1)Samplez∈[k]×[l]suchthatPr[z=(i j)]=pi j;(2)Setlabely=z[0] andsamplexfromDz.AssumewesampleNpoints{(xi yi)}Ni=1.LetusdeﬁnethesupportofadistributionDwithdensitypoverRdassupp(D)={x:p(x)>0} thedistancebetweentwosetsS1 S2⊆Rdasdist(S1 S2)=minx∈S1 y∈S2{kx−yk2} andthediameterofasetS1⊆Rdasdiam(S1)=maxx y∈S1{kx−yk2}.Thenwearereadytomaketheassumptionsaboutthedata.(A1)(Separability)Thereexistsδ>0suchthatforeveryi16=i2∈[k]andeveryj1 j2∈[l] dist(supp(Di1 j1) supp(Di2 j2))≥δ.Moreover foreveryi∈[k] j∈[l] 1diam(supp(Di j))≤λδ forλ≤1/(8l).(A2)(Normalization)Anyxfromthedistributionhaskxk2=1.Afewremarksareworthy.Insteadofhavingonedistributionforoneclass weallowanarbitraryl≥1distributionsineachclass whichwebelieveisabetterﬁttotherealdata.Forexample inMNIST aclasscanbethenumber1 andlcanbethedifferentstylesofwriting1(1or|or/).Assumption(A2)isforsimplicity while(A1)isourkeyassumption.Withl≥1distributionsinsideeachclass ourassumptionallowsdatathatisnotlinearlyseparable e.g. XORtypedatainR2wheretherearetwoclasses oneconsistingoftwoballsofdiameter1/10withcenters(0 0)and(2 2)andtheotherconsistingoftwoofthesamediameterwithcenters(0 2)and(2 0).SeeFigure3inAppendixCforanillustration.Moreover essentiallytheonlyassumptionwehavehereisλ=O(1/l).Whenl=1 λ=O(1) whichistheminimalrequirementontheorderofλforthedistributiontobeefﬁcientlylearnable.Ourworkallowslargerl sothatthedatacanbemorecomplicatedinsideeachclass.Inthiscase werequiretheseparationtoalsobehigher.Whenweincreaseltoreﬁnethedistributionsinsideeachclass weshouldexpectthediametersofeachdistributionbecomesmalleraswell.Aslongastherateofdiameterdecreasingineachdistributionisgreaterthanthetotalnumberofdistributions thenourassumptionwillhold.Assumptionsaboutthelearningprocess.Wewillonlylearntheweightwrtosimplifytheanalysis.SincetheReLUactivationispositivehomogeneous theeffectofoverparameterizationcanstillbestudied andasimilarapproachhasbeenadoptedinpreviouswork[7].Sothenetworkisalsowrittenasy=f(x w)=(f1(x w) ··· fk(x w))forw=(w1 ··· wr).Weassumethelearningisfromarandominitialization:(A3)(Randominitialization)w(0)r∼N(0 σ2I) ai r∼N(0 1) withσ=1m1/2.Thelearningprocessminimizesthecrossentropylossoverthesoftmax deﬁnedas:L(w)=−1NNXs=1logoys(xs w) whereoy(x w)=efy(x w)Pki=1efi(x w).LetL(w xs ys)=−logoys(xs w)denotethecrossentropylossforaparticularpoint(xs ys).WeconsideraminibatchSGDofbatchsizeB numberofiterationsT=N/Bandlearningrateηasthefollowingprocess:RandomlydividethetotaltrainingexamplesintoTbatches eachofsizeB.Lettheindicesoftheexamplesinthet-thbatchbeBt.Ateachiteration theupdateis2w(t+1)r=w(t)r−η1BXs∈Bt∂L(w(t) xs ys)∂w(t)r ∀r∈[m] where∂L(w xs ys)∂wr=Xi6=ysai roi(xs w)−Xi6=ysays roi(xs w)1hwr xsi≥0xs.(1)1Theassumption1/(8l)canbemadeto1/[(1+α)l]foranyα>0bypayingalargepolynomialin1/αinthesamplecomplexity.Wewillnotproveitinthispaperbecausewewouldliketohighlightthekeyfactors.2Strictlyspeaking L(w xs ys)doesnothavegradienteverywhereduetothenon-smoothnessofReLU.Onecanview∂L(w xs ys)∂wrasaconvenientnotationfortherighthandsideof(1).34MainResultFornotationsimplicity foratargeterrorε(tobespeciﬁedlater) withhighprobability(orw.h.p.)meanswithprobability1−1/poly(1/δ k l m 1/ε)forasufﬁcientlylargepolynomialpoly and˜Ohidesfactorsofpoly(log1/δ logk logl logm log1/ε).Theorem4.1.Supposetheassumptions(A1)(A2)(A3)aresatisﬁed.Thenforeveryε>0 thereisM=poly(k l 1/δ 1/ε)suchthatforeverym≥M afterdoingaminibatchSGDwithbatchsizeB=poly(k l 1/δ 1/ε logm)andlearningrateη=1m·poly(k l 1/δ 1/ε logm)forT=poly(k l 1/δ 1/ε logm)iterations withhighprobability:Pr(x y)∼Dh∀j∈[k] j6=y fy(x w(T))>fj(x w(T))i≥1−ε.Ourtheoremimpliesifthedatasatisﬁesourassumptions andweparametrizethenetworkproperly thenweonlyneedpolynomialink l 1/δmanysamplestoachieveagoodpredictionerror.ThiserrorismeasureddirectlyonthetruedistributionD notmerelyontheinputdatausedtotrainthisnetwork.Ourresultisalsodimensionfree:Thereisnodependencyontheunderlyingdimensiondofthedata thecomplexityisfullycapturedbyk l 1/δ.Moreover nomatterhowmuchthenetworkisoverparameterized itwillonlyincreasethetotaliterationsbyfactorsoflogm.Sowecanoverparameterizebyansub-exponentialamountwithoutsigniﬁcantlyincreasingthecomplexity.Furthermore wecanalwaystreateachinputexampleasanindividualdistribution thusλisalwayszero.Inthiscase ifweusebatchsizeBforTiterations wewouldhavel=N=BT.Thenourtheoremindicatethataslongasm=poly(N 1/δ0) whereδ0istheminimaldistancebetweeneachexamples wecanactuallyﬁtarbitrarylabelsoftheinputdata.However sincethetotaliterationonlydependsonlogm whenm=poly(N 1/δ0)buttheinputdataisactuallystructured(withsmallk landlargeδ) thenSGDcanactuallyachieveasmallgeneralizationerror evenwhenthenetworkhasenoughcapacitytoﬁtarbitrarylabelsofthetrainingexamples(andcanalsobedonebySGD).Thus weprovethatSGDhasastronginductivebiasonstructureddata:Insteadofﬁndingabadglobaloptimathatcanﬁtarbitrarylabels itactuallyﬁndsthosewithgoodgeneralizationguarantees.Thisgivesmorethoroughexplanationtotheempiricalobservationsin[24 31].5IntuitionandProofSketchforASimpliﬁedCaseTotrainaneuralnetworkwithReLUactivations therearetwoquestionsneedtobeaddressed:1WhycanSGDoptimizethetrainingloss?Orevenﬁndingacriticalpoint?Sincetheunder-lyingnetworkishighlynon-smooth existingtheoremsdonotgiveanyﬁniteconvergencerateofSGDfortrainingneuralnetworkwithReLUsactivations.2Whycanthetrainednetworkgeneralize?Evenwhenthecapacityislargeenoughtoﬁtrandomlabelsoftheinputdata?ThisisknownastheinductivebiasofSGD.Thisworktakesasteptowardsansweringthesetwoquestions.Weshowthatwhenthenetworkisoverparameterized itbecomesmore“pseudosmooth” whichmakesiteasirforSGDtominimizethetrainingloss andfurthermore itwillnothurtthegeneralizationerror.Ourproofisbasedonthefollowingimportantobservation:Themoreweoverparameterizethenetwork thelesslikelytheactivationpatternforoneneuronandonedatapointwillchangeinaﬁxednumberofiterations.Thisobservationallowsustocouplethegradientofthetrueneuralnetworkwitha“pseudogradient”wheretheactivationpatternforeachdatapointandeachneuronisﬁxed.Thatis whencomputingthe“pseudogradient” forﬁxedr i whetherther-thhiddennodeisactivatedonthei-thdatapointxiwillalwaysbethesamefordifferentt.(Butforﬁxedt fordifferentrori thesigncanbedifferent.)Weareabletoprovethatunlessthegeneralizationerrorissmall the“pseudogradient”willalwaysbelarge.Moreover weshowthatthenetworkisactuallysmooththusSGDcanminimizetheloss.Wethenshowthatwhenthenumbermofhiddenneuronsincreases withaproperlydecreasinglearningrate thetotalnumberofiterationsittakestominimizethelossisroughlynotchanged.However thetotalnumberofiterationsthatwecancouplethetruegradientwiththepseudoone4increases.Thus thereisapolynomiallylargemsothatwecancouplethesetwogradientsuntilthenetworkreachesasmallgeneralizationerror.5.1ASimpliﬁedCase:NoVarianceHereweillustratetheproofsketchforasimpliﬁedcaseandAppendixAprovidestheproof.TheproofforthegeneralcaseisprovidedinAppendixB.Inthesimpliﬁedcase wefurtherassume:(S)(Novariance)EachDa bisasingledatapoint(xa b a) andalsowearedoingfullbatchgradientdescentasoppositetotheminibatchSGD.ThenwereloadthelossnotationasL(w)=Pa∈[k] b∈[l]pa bL(w xa b a) andthegradientis∂L(w)∂wr=Xa∈[k] b∈[l]pa bXi6=aai roi(xa b w)−Xi6=aaa roi(xa b w)1hwr xa bi≥0xa b.Followingtheintuitionabove wedeﬁnethepseudogradientas˜∂L(w)∂wr=Xa∈[k] b∈[l]pa bXi6=aai roi(xa b w)−Xi6=aaa roi(xa b w)1hw(0)r xa bi≥0xa b whereituses1hw(0)r xa bi≥0insteadof1hwr xa bi≥0asinthetruegradient.Thatis theactivationpatternissettobethatintheinitialization.Intuitively thepseudogradientissimilartothegradientforapseudonetworkg(butnotexactlythesame) deﬁnedasgi(x w):=Pmr=1ai rhwr xi1Dw(0)r xE≥0.Couplingthegradientsisthensimilartocouplingthenetworksfandg.Forsimplicity letva a b:=Pi6=aoi(xa b w)=Pi6=aefi(xa b w)Pki=1efi(xa b w)andwhens6=a vs a b:=−os(xa b w)=−efs(xa b w)Pki=1efi(xa b w).Roughly ifva a bissmall thenfa(xa b w)isrelativelylargercomparedtotheotherfi(xa b w) sotheclassiﬁcationerrorissmall.Weprovethefollowingtwomainlemmas.Theﬁrstsaysthatateachiteration thetotalnumberofhiddenunitswhosegradientcanbecoupledwiththepseudooneisquitelarge.Lemma5.1(Coupling).W.h.p.overtherandominitialization foreveryτ>0 foreveryt=˜O(cid:16)τη(cid:17) wehavethatforatleast1−eτklσfractionofr∈[m]:∂L(w(t))∂wr=˜∂L(w(t))∂wr.Thesecondlemmasaysthatthepseudogradientislargeunlesstheerrorissmall.Lemma5.2.Form=˜Ω(cid:16)k3l2δ(cid:17) forevery{pa bvi a b}i a∈[k] b∈[l]∈[−v v](thatdependsonw(0)r ai r etc.)withmax{pa bvi a b}i a∈[k] b∈[l]=v thereexistsatleastΩ(δkl)fractionofr∈[m]suchthat(cid:13)(cid:13)(cid:13)˜∂L(w)∂wr(cid:13)(cid:13)(cid:13)2=˜Ω(cid:0)vδkl(cid:1).Wenowillustratehowtousethesetwolemmastoshowtheconvergenceforasmallenoughlearningrateη.Forsimplicity letusassumethatkl/δ=O(1)andε=o(1).Thus byLemma5.2weknowthatunlessv≤ε thereareΩ(1)fractionofrsuchthat(cid:13)(cid:13)(cid:13)˜∂L(w)/∂wr(cid:13)(cid:13)(cid:13)2=Ω(ε).Moreover byLemma5.1weknowthatwecanpickτ=Θ(σε)soeτ/σ=Θ(ε) whichimpliesthatthereareΩ(1)fractionofrsuchthatk∂L(w)/∂wrk2=Ω(ε)aswell.Forsmallenoughlearningrateη doingonestepofgradientdescentwillthusdecreaseL(w)byΩ(ηmε2) soitconvergesint=O(cid:0)1/ηmε2(cid:1)iterations.Intheend wejustneedtomakesurethat1/ηmε2≤O(τ/η)=Θ(σε/η)sowecanalwaysapplythecouplingLemma5.1.Byσ=˜O(1/m−1/2)weknowthatthisistrueaslongasm≥poly(1/ε).Asmallvcanbeshowntoleadtoasmallgeneralizationerror.6DiscussionofInsightsfromtheAnalysisOuranalysis thoughforlearningtwo-layernetworksonwellstructureddata alsoshedssomelightuponlearningneuralnetworksinmoregeneralsettings.5Generalization.Severallinesofrecentworkexplainthegeneralizationphenomenonofoverparam-eterizednetworksbylowcomplexityofthelearnednetworks fromthepointviewsofspectrally-normalizedmargins[3 23 21] compression[1 33 4] andPAC-Bayes[9 23].OuranalysishaspartiallyexplainedhowSGD(withproperrandominitialization)onstructureddataleadstothelowcomplexityfromthecompressionandPCA-Bayespointviews.Wehaveshownthatinaneighborhoodoftherandominitialization w.h.p.thegradientsaresimilartothoseofanotherbenignlearningprocess andthusSGDcanreducetheerrorandreachagoodsolutionwhilestillintheneighborhood.Theclosenesstotheinitializationthenmeanstheweights(ormorepreciselythedifferencebetweenthelearnedweightsandtheinitialization)canbeeasilycompressed.Infact empiricalobservationshavebeenmadeandconnectedtogeneralizationin[22 1].Furthermore [1]explicitlypointoutsuchacompressionusingahelperstring(correspondingtotheinitializationinoursetting).[1]alsopointoutthatthecompressionviewcanberegardedasamoreexplicitformofthePAC-Bayesview andthusourintuitionalsoappliestothelatter.Theexistenceofasolutionofasmallgeneralizationerrorneartheinitializationisitselfnotobvious.Intuitively onstructureddata theupdatesarestructuredsignalsspreadoutacrosstheweightsofthehiddenneurons.Thenforprediction therandominitializedpartintheweightshasstrongcancellation whilethestructuredsignalpartintheweightscollectivelyaffectstheoutput.Therefore thelattercanbemuchsmallerthantheformerwhilethenetworkcanstillgiveaccuratepredictions.Inotherwords therecanbeasolutionnotfarfromtheinitializationwithhighprobability.Someinsightisprovidedonthelowrankoftheweights.Moreprecisely whenthedataarewellclusteredaroundafewpatterns theaccumulatedupdates(differencebetweenthelearnedweightsandtheinitialization)shouldbeapproximatelylowrank whichcanbeseenfromcheckingtheSGDupdates.However whenthedifferenceissmallcomparedtotheinitialization thespectrumoftheﬁnalweightmatrixisdominatedbythatoftheinitializationandthuswilltendtoclosertothatofarandommatrix.Again suchobservations/intuitionshavebeenmadeintheliteratureandconnectedtocompressionandgeneralization(e.g. [1]).Implicitregularizationv.s.structureofthedata.Existingworkhasanalyzedtheimplicitregular-izationofSGDonlogisticregression[27] matrixfactorization[11 19 16] andlearningtwo-layernetworksonlinearlyseparabledata[7].Oursettingandalsotheanalysistechniquesarenovelcomparedtotheexistingwork.Onemotivationtostudyonstructureddataistounderstandtheroleofstructureddataplayintheimplicitregularization i.e. theobservationthatthesolutionlearnedonlessstructuredorevenrandomdataisfurtherawayfromtheinitialization.Indeed ouranalysisshowsthatwhenthenetworksizeisﬁxed(andsufﬁcientlyoverparameterized) learningoverpoorlystructureddata(largerkand‘)needsmoreiterationsandthusthesolutioncandeviatemorefromtheinitializationandhashighercomplexity.Anextremeandespeciallyinterestingcaseiswhenthenetworkisoverparameterizedsothatinprincipleitcanﬁtthetrainingdatabyviewingeachpointasacomponentwhileactuallytheycomefromstructureddistributionswithsmallnumberofcomponents.Inthiscase wecanshowthatitstilllearnsanetworkwithasmallgeneralizationerror;seethemoretechnicaldiscussioninSection4.Wealsonotethatouranalysisisundertheassumptionthatthenetworkissufﬁcientlyoverparam-eterized i.e. misasufﬁcientlylargepolynomialofk ‘andotherrelatedparametersmeasuringthestructureofthedata.Therecouldbethecasethatmissmallerthanthispolynomialbutismorethansufﬁcienttoﬁtthedata i.e. thenetworkisstilloverparameterized.Thoughinthiscasetheanalysisstillprovidesusefulinsight itdoesnotfullyapply;seeourexperimentswithrelativelysmallm.Ontheotherhand theempiricalobservations[24 31]suggestthatpracticalnetworksarehighlyoverparameterized soourintuitionmaystillbehelpfulthere.Effectofrandominitialization.Ouranalysisalsoshowshowproperrandominitializationshelpstheoptimizationandconsequentlygeneralization.Essentially thisguaranteesthatw.h.p.forweightsclosetotheinitialization manyhiddenReLUunitswillhavethesameactivationpatterns(i.e. activatedornot)asfortheinitializations whichmeansthegradientsintheneighborhoodlooklikethosewhenthehiddenunitshaveﬁxedactivationpatterns.ThisallowsSGDmakesprogresswhenthelossislarge andeventuallylearnsagoodsolution.Wealsonotethatitisessentialtocarefullysetthescaleoftheinitialization whichisaextensivelystudiedtopic[20 28].Ourinitializationhasascalerelatedtothenumberofhiddenunits whichisparticularlyusefulwhenthenetworksizeisvarying andthuscanbeofinterestinsuchpracticalsettings.6050100150200250300350400Number of steps0.00.20.40.60.81.0Test AccuracyTest Accuracy v.s. number of stepsNumber of hidden nodes50010002000400080001600032000(a)Testaccuracy050100150200250300350400Number of steps0.000.010.020.030.040.050.060.07Activation pattern difference ratioActivation difference v.s. number of stepsNumber of hidden nodes50010002000400080001600032000(b)Coupling050100150200250300350400Number of steps0.0000.0050.0100.0150.0200.0250.0300.0350.040Relative distanceRelative distance v.s. number of stepsNumber of hidden nodes50010002000400080001600032000(c)Distancefromtheinitialization020406080100Singular value index10 610 510 410 310 210 1100Singular valueSingular values of weight matrix and accumulated updatesSpectrum forWeight matrixAccumulated updates(d)Rankofaccumulatedupdates(y-axisinlog-scale)Figure1:Resultsonthesyntheticdata.7ExperimentsThissectionaimsatverifyingsomekeyimplications:(1)theactivationpatternsofthehiddenunitscouplewiththoseatinitialization;(2)Thedistancefromthelearnedsolutionfromtheinitializationisrelativelysmallcomparedtothesizeofinitialization;(3)Theaccumulatedupdates(i.e. thedifferencebetweenthelearnedweightmatrixandtheinitialization)haveapproximatelylowrank.TheseareindeedsupportedbytheresultsonthesyntheticandtheMNISTdata.AdditionalexperimentsarepresentedinAppendixD.Setup.Thesyntheticdataareof1000dimensionandconsistofk=10classes eachhaving‘=2components.Eachcomponentisofequalprobability1/(kl) andisaGaussianwithcovarianceσ2/dIanditsmeanisi.i.d.sampledfromaGaussiandistributionN(0 σ20/d) whereσ=1andσ0=5.1000trainingdatapointsand1000testdatapointsaresampled.ThenetworkstructureandthelearningprocessfollowthoseinSection3;thenumberofhiddenunitsmvariesintheexperiments andtheweightsareinitializedwithN(0 1/√m).Onthesyntheticdata theSGDisrunforT=400stepswithbatchsizeB=16andlearningrateη=10/m.OnMNIST theSGDisrunforT=2×104stepswithbatchsizeB=64andlearningrateη=4×103/m.Besidesthetestaccuracy wereportthreequantitiescorrespondingtothethreeobserva-tions/implicationstobeveriﬁed.First forcoupling wecomputethefractionofhiddenunitswhoseactivationpatternchangedcomparedtothetimeatinitialization.Here theactivationpatternisdeﬁnedas1iftheinputtotheReLUispositiveand0otherwise.Second fordistance wecomputetherelativeratiokw(t)−w(0)kF/kw(0)kF wherew(t)istheweightmatrixattimet.Finally fortherankoftheaccumulatedupdates weplotthesingularvaluesofw(T)−w(0)whereTistheﬁnalstep.Allexperimentsarerepeated5times andthemeanandstandarddeviationarereported.7025005000750010000125001500017500Number of steps0.20.40.60.8Test AccuracyTest Accuracy v.s. number of stepsNumber of hidden nodes10002000400080001600032000(a)Testaccuracy025005000750010000125001500017500Number of steps0.00.10.20.30.4Activation pattern difference ratioActivation difference v.s. number of stepsNumber of hidden nodes10002000400080001600032000(b)Coupling025005000750010000125001500017500Number of steps0.000.250.500.751.001.251.501.752.00Relative distanceRelative distance v.s. number of stepsNumber of hidden nodes10002000400080001600032000(c)Distancefromtheinitialization01020304050607080Singular value index10−1510−1310−1110−910−710−510−310−1101Singular valueSingular values of the eight matrix and accumulated updatesSpectrum forWeight matrixAccumulated updates(d)Rankofaccumulatedupdates(y-axisinlog-scale)Figure2:ResultsontheMNISTdata.Results.Figure1showstheresultsonthesyntheticdata.Thetestaccuracyquicklyconvergesto100% whichisevenmoresigniﬁcantwithlargernumberofhiddenunits showingthattheoverparameterizationhelpstheoptimizationandgeneralization.Recallthatouranalysisshowsthatforalearningratelinearlydecreasingwiththenumberofhiddennodesm thenumberofiterationstogettheaccuracytoachieveadesiredaccuracyshouldberoughlythesame whichisalsoveriﬁedhere.Theactivationpatterndifferenceratioislessthan0.1 indicatingastrongcoupling.Therelativedistanceislessthan0.1 sotheﬁnalsolutionisindeedclosetotheinitialization.Finally thetop20singularvaluesoftheaccumulatedupdatesaremuchlargerthantherestwhilethespectrumoftheweightmatrixdonothavesuchstructure whichisalsoconsistentwithouranalysis.Figure2showstheresultsonMNIST.Theobservationingeneralissimilartothoseonthesyntheticdata(thoughlesssigniﬁcant) andalsotheobservedtrendbecomemoreevidentwithmoreoverpa-rameterization.Someadditionalresults(e.g. varyingthevarianceofthesyntheticdata)areprovidedintheappendixthatalsosupportourtheory.8ConclusionThisworkstudiedtheproblemoflearningatwo-layeroverparameterizedReLUneuralnetworkviastochasticgradientdescent(SGD)fromrandominitialization ondatawithstructureinspiredbypracticaldatasets.WhileourworkmakesasteptowardstheoreticalunderstandingofSGDfortrainingneuralnetwors itisfarfrombeingconclusive.Inparticular therealdatacouldbeseparablewithrespecttodifferentmetricthan‘2 orevenanon-convexdistancegivenbysomemanifold.Weviewthisanimportantopendirection.8AcknowledgementsWewouldliketothanktheanonymousreviewersofNIPS’18andJasonLeeforhelpfulcomments.ThisworkwassupportedinpartbyFA9550-18-1-0166 NSFgrantsCCF-1527371 DMS-1317308 SimonsInvestigatorAward SimonsCollaborationGrant andONR-N00014-16-1-2329.YingyuLiangwouldalsoliketoacknowledgethatsupportforthisresearchwasprovidedbytheOfﬁceoftheViceChancellorforResearchandGraduateEducationattheUniversityofWisconsinMadisonwithfundingfromtheWisconsinAlumniResearchFoundation.References[1]SanjeevArora RongGe BehnamNeyshabur andYiZhang.Strongergeneralizationboundsfordeepnetsviaacompressionapproach.arXivpreprintarXiv:1802.05296 2018.[2]DevanshArpit StanisławJastrz˛ebski NicolasBallas DavidKrueger EmmanuelBengio MaxinderSKanwal TeganMaharaj AsjaFischer AaronCourville YoshuaBengio etal.Acloserlookatmemorizationindeepnetworks.arXivpreprintarXiv:1706.05394 2017.[3]PeterLBartlett DylanJFoster andMatusJTelgarsky.Spectrally-normalizedmarginboundsforneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages6241–6250 2017.[4]CenkBaykal LucasLiebenwein IgorGilitschenski DanFeldman andDanielaRus.Data-dependentcoresetsforcompressingneuralnetworkswithapplicationstogeneralizationbounds.arXivpreprintarXiv:1804.05345 2018.[5]DigvijayBoobandGuanghuiLan.Theoreticalpropertiesoftheglobaloptimizeroftwolayerneuralnetwork.arXivpreprintarXiv:1710.11241 2017.[6]AlonBrutzkusandAmirGloberson.Globallyoptimalgradientdescentforaconvnetwithgaussianinputs.arXivpreprintarXiv:1702.07966 2017.[7]AlonBrutzkus AmirGloberson EranMalach andShaiShalev-Shwartz.Sgdlearnsover-parameterizednetworksthatprovablygeneralizeonlinearlyseparabledata.arXivpreprintarXiv:1710.10174 2017.[8]LaurentDinh RazvanPascanu SamyBengio andYoshuaBengio.Sharpminimacangeneralizefordeepnets.arXivpreprintarXiv:1703.04933 2017.[9]GintareKarolinaDziugaiteandDanielMRoy.Computingnonvacuousgeneralizationboundsfordeep(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata.arXivpreprintarXiv:1703.11008 2017.[10]RongGe JasonDLee andTengyuMa.Learningone-hidden-layerneuralnetworkswithlandscapedesign.arXivpreprintarXiv:1711.00501 2017.[11]SuriyaGunasekar BlakeEWoodworth SrinadhBhojanapalli BehnamNeyshabur andNatiSrebro.Implicitregularizationinmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems pages6152–6160 2017.[12]MoritzHardtandTengyuMa.Identitymattersindeeplearning.arXivpreprintarXiv:1611.04231 2016.[13]KenjiKawaguchi.Deeplearningwithoutpoorlocalminima.InAdvancesinNeuralInformationProcessingSystems pages586–594 2016.[14]NitishShirishKeskar DheevatsaMudigere JorgeNocedal MikhailSmelyanskiy andPingTakPeterTang.Onlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima.arXivpreprintarXiv:1609.04836 2016.[15]YannLeCun LéonBottou YoshuaBengio andPatrickHaffner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE 86(11):2278–2324 1998.9[16]YuanzhiLi TengyuMa andHongyangZhang.Algorithmicregularizationinover-parameterizedmatrixrecovery.arXivpreprintarXiv:1712.09203 2017.[17]YuanzhiLiandYangYuan.Convergenceanalysisoftwo-layerneuralnetworkswithreluactivation.InAdvancesinNeuralInformationProcessingSystems pages597–607 2017.[18]RoiLivni ShaiShalev-Shwartz andOhadShamir.Onthecomputationalefﬁciencyoftrainingneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages855–863 2014.[19]CongMa KaizhengWang YuejieChi andYuxinChen.Implicitregularizationinnonconvexstatisticalestimation:Gradientdescentconvergeslinearlyforphaseretrieval matrixcompletionandblinddeconvolution.arXivpreprintarXiv:1711.10467 2017.[20]JamesMartens.Deeplearningviahessian-freeoptimization.InICML volume27 pages735–742 2010.[21]CisseMoustapha BojanowskiPiotr GraveEdouard DauphinYann andUsunierNicolas.Parse-valnetworks:Improvingrobustnesstoadversarialexamples.arXivpreprintarXiv:1704.08847 2017.[22]VaishnavhNagarajanandZicoKolter.Generalizationindeepnetworks:Theroleofdistancefrominitialization.NIPSworkshoponDeepLearning:BridgingTheoryandPractice 2017.[23]BehnamNeyshabur SrinadhBhojanapalli DavidMcAllester andNathanSrebro.Apac-bayesianapproachtospectrally-normalizedmarginboundsforneuralnetworks.arXivpreprintarXiv:1707.09564 2017.[24]BehnamNeyshabur RyotaTomioka andNathanSrebro.Insearchoftherealinductivebias:Ontheroleofimplicitregularizationindeeplearning.arXivpreprintarXiv:1412.6614 2014.[25]MahdiSoltanolkotabi AdelJavanmard andJasonDLee.Theoreticalinsightsintotheoptimiza-tionlandscapeofover-parameterizedshallowneuralnetworks.arXivpreprintarXiv:1707.04926 2017.[26]DanielSoudryandYairCarmon.Nobadlocalminima:Dataindependenttrainingerrorguaranteesformultilayerneuralnetworks.arXivpreprintarXiv:1605.08361 2016.[27]DanielSoudry EladHoffer andNathanSrebro.Theimplicitbiasofgradientdescentonseparabledata.arXivpreprintarXiv:1710.10345 2017.[28]IlyaSutskever JamesMartens GeorgeDahl andGeoffreyHinton.Ontheimportanceofinitializationandmomentumindeeplearning.InInternationalconferenceonmachinelearning pages1139–1147 2013.[29]YuandongTian.Ananalyticalformulaofpopulationgradientfortwo-layeredrelunetworkanditsapplicationsinconvergenceandcriticalpointanalysis.arXivpreprintarXiv:1703.00560 2017.[30]BoXie YingyuLiang andLeSong.Diversityleadstogeneralizationinneuralnetworks.arXivpreprintArxiv:1611.03131 2016.[31]ChiyuanZhang SamyBengio MoritzHardt BenjaminRecht andOriolVinyals.Understandingdeeplearningrequiresrethinkinggeneralization.arXivpreprintarXiv:1611.03530 2016.[32]KaiZhong ZhaoSong PrateekJain PeterLBartlett andInderjitSDhillon.Recoveryguaranteesforone-hidden-layerneuralnetworks.arXivpreprintarXiv:1706.03175 2017.[33]WendaZhou VictorVeitch MorganeAustern RyanPAdams andPeterOrbanz.Com-pressibilityandgeneralizationinlarge-scaledeeplearning.arXivpreprintarXiv:1804.05862 2018.10,Yuanzhi Li
Yingyu Liang