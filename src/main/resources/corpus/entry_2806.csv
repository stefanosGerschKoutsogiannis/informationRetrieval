2019,Large Memory Layers with Product Keys,This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture  by up to a billion parameters with a negligible computational overhead.
Its design and access pattern is based on product keys  which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words  and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular  we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers  while being twice faster at inference time. We release our code for reproducibility purposes.,Large Memory Layers with Product Keys

Guillaume Lample∗†  Alexandre Sablayrolles∗  Marc’Aurelio Ranzato∗ 

Ludovic Denoyer∗†  Herv´e J´egou∗

{glample asablayrolles ranzato denoyer rvj}@fb.com

Abstract

This paper introduces a structured memory which can be easily integrated into a
neural network. The memory is very large by design and signiﬁcantly increases
the capacity of the architecture  by up to a billion parameters with a negligi-
ble computational overhead. Its design and access pattern is based on product
keys  which enable fast and exact nearest neighbor search. The ability to increase
the number of parameters while keeping the same computational budget lets the
overall system strike a better trade-off between prediction accuracy and compu-
tation efﬁciency both at training and test time. This memory layer allows us to
tackle very large scale language modeling tasks. In our experiments we consider
a dataset with up to 30 billion words  and we plug our memory layer in a state-
of-the-art transformer-based architecture. In particular  we found that a memory
augmented model with only 12 layers outperforms a baseline transformer model
with 24 layers  while being twice faster at inference time. We release our code for
reproducibility purposes.3

1

Introduction

Neural networks are commonly employed to address many complex tasks such as machine trans-
lation [43]  image classiﬁcation [27] or speech recognition [16]. As more and more data becomes
available for training  these networks are increasingly larger [19]. For instance  recent models both
in vision [29] and in natural language processing [20  36  28] have more than a billion parame-
ters. The higher-capacity enables better modeling of data like natural text or images  and it also
improves generalization [41  33]. Unfortunately  increasing capacity has led to a dramatic increase
of computational complexity  both at training and inference time [20].
There is a growing interest in developing architectures with reasonable computational complexity.
Recently  there has been some efforts to develop high capacity architectures that operate on a limited
computational budget [40  18]. This is well illustrated by the “On-device Visual Intelligence Chal-
lenge” [5]  which speciﬁcally focuses on the complexity/accuracy trade-off for image classiﬁcation.
Some researchers have attempted to increase the capacity of a network without increasing its com-
putational complexity. Most notably  Rae et al. [37] incorporate fast nearest neighbor search within
a neural network architecture to leverage large key-value layers with sparse reads and writes. Their
approach relies on an external indexing structure [32]  which is approximate and needs to be re-
learned regularly while training the neural network to avoid a catastrophic drift.
In this work  we propose a key-value memory layer that can scale to very large sizes while keeping
exact search on the key space. This layer dramatically increases the capacity of the overall system
for a negligible computational overhead. Unlike existing models based on key-value memories (see

∗Facebook AI Research
†Sorbonne Universit´es  UPMC Univ Paris 06  UMR 7606  LIP6
3https://github.com/facebookresearch/XLM

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Overview of a key-value memory layer: The input x is processed through a query
network that produces a query vector q  which is compared to all the keys. The output is the sparse
weighted sum over the memories associated with the selected keys. For a large number of keys |K| 
the key selection procedure becomes too expensive in practice. Our product key method is exact and
makes this search process very fast.

Figure 1)  we deﬁne keys as the concatenation of two sub-keys  in the spirit of product quantiza-
tion [21]. As shown in more details in Figure 2  this structure implicitly deﬁnes a very large set of
keys  each being associated with a value memory slot. The set of value vectors introduces the bulk
of the parameters  as it scales quadratically with the number of sub-keys. Despite the large num-
ber of memory slots  ﬁnding the exact closest keys to the input is very efﬁcient  typically requiring

O((cid:112)|K|) vector comparisons  where |K| is the total number of memory slots. All the memory pa-

rameters are trainable  yet only a handful of memory slots are updated for each input at training time.
Sparsity of key selection and parameter updates make both training and inference very efﬁcient.
Our layer allows us to tackle problems where current architectures underﬁt given the vast amount of
available data  or when they are too slow to work in practice. We thus focus on the language mod-
eling task  integrating our memory within the popular transformer architecture [44]. This choice
is motivated by the success of BERT [11] and GPT-2 [36]  which demonstrated that increasing the
capacity of large models directly translates to large improvements in language modeling  which in
turn translates to better performance in both language understanding tasks [11  46] and text genera-
tion [36]. Overall  our paper makes the following contributions:

◦ We introduce a new layer that provides a large capacity to a neural network for only a slight

computational overhead both at train and test time.

◦ Our fast indexing strategy offers exact nearest neighbor search by construction  and avoids

the pitfall of relying on an indexing structure that needs to be re-learned during training.

◦ We demonstrate our method within a large state-of-the-art transformer  composed of 24
layers of dimension 1600. Our method with 1 memory and 12 layers outperforms a 24-
layer transformer while being twice faster at inference time. We show that adding more
memory layers to transformers of various complexities provides systematic and signiﬁcant
improvements on our target task.

2 Related work

Different approaches have been proposed to increase the capacity of neural networks without in-
creasing too much the computational complexity. For instance  conditional computation models
aim at routing inputs into very large neural networks such that only a subset of connections and/or
layers are used to process each input. Different methods have been developed like large mixture of
experts [40]  gating techniques [3  12  6] or even reinforcement learning-based approaches [10].
Another line of research is the development of memory augmented neural networks. For instance 
memory-based neural layers [47  42] are an efﬁcient way to represent variable length inputs for
complex problems such as question answering [48]. Such memories can also operate in feature
space and have various reading and writing mechanisms [23  17]. Unfortunately  these approaches
scale linearly with the size of the memory which is prohibitive for very large memories. Neural cache
models [15] suffer from the same scaling issues  which are circumvented by adopting approximate
lookup techniques at test time [14].

2

querynetwork21||21||keysvalueskey selection2Discretization techniques have been intensively studied for compressing network weights [8  38]
and/or activations [7  38] or to accelerate inference. For instance  Gerald et al. [13] propose to
map an input to a low-dimensional binary code  each code being associated with one category  thus
reducing the complexity of inference by avoiding the use of a ﬁnal large linear layer. Another
model is proposed in [45]  where the authors develop a fast locality-sensitive hashing technique
to approximate the dot product between large matrices and vectors in neural networks. However 
exploiting binary codes or approximate techniques at training time raises several challenges in terms
of optimization  because approximate indexes are not accurate in high-dimensional spaces. In our
paper  we borrow some ideas from product quantization (PQ) [21]. This is an approximate search
technique that maps database vectors into compact codes. However  our goal is different: we do not
build an approximate index  but rather we exploit the idea to represent a large set of key vectors by
a drastically smaller number of vectors  that we update by regular back-propagation. As discussed
later  the selection of the closest keys is exact and inherits from the fast neighbor search of PQ.
Our model is also related to sparsity models which have been mainly studied in the unsupervised
learning setting [34  24]. For instance  the k-sparse autoencoder [30] only keeps the k largest values
in the latent representation of an auto-encoder  similar to our memory layer but without the product
In winner take all autoencoders [31]  sparsity is induced by using mini-batch
keys component.
statistics  while in the sparse access memory [37] reports some speed-up by both thresholding the
memory to a sparse subset  and by using efﬁcient data structures for content-based read operations.
Unfortunately  the fast access to memories rely on an approximate external indexing structure [32]
that has to be re-learned periodically. Our work solves this issue by fully incorporating the key
selection mechanism as a network component.
The transformer network [44] is the current workhorse of Natural Language Processing (NLP): it
is employed ubiquitously across a large variety of tasks. Transformers are built by stacking blocks
composed of self-attention layers followed by fully connected layers (dubbed FFN)  as shown in
Figure 3. The components of the memory layer bear similarities to the query  key and value networks
used in self-attention layers with two notable differences: the keys and values do not correspond to
input tokens but are free embedding vectors  and the number of values (memory size) is very large.

3 Learnable product key memories
We consider the design of a function m : Rd → Rn  that will act as a layer in a neural network. The
purpose of m is to offer a large capacity within a neural network.

3.1 Memory design

High-level structure. The overall structure of our memory is illustrated by Figures 1 and 2. The
memory is composed of three components: a query network  a key selection module containing two
sets of sub-keys  and a value lookup table. It ﬁrst computes a query that is compared to the set of
product keys. For each product key  it computes a score and selects the k product keys with the
highest scores. The scores are then used to produce an output m(x) via a weighted sum over the
values associated with the selected keys. All the parameters of the memory are trainable  yet only
k memory slots are updated for each input. The sparse selection and parameter update make both
training and inference very efﬁcient.
Query generation: pre-processing network. The function q : x (cid:55)→ q(x) ∈ Rdq  referred to as
the query network  maps the d-dimensional input to a latent space of dimensionality dq. Typically 
q is a linear mapping or a multi-layer perceptron that reduces the dimensionality from d to dq =
512. As keys are randomly initialized  they occupy the space relatively uniformly. Adding a batch
normalization layer on the top of the query network helps increasing key coverage during training.
This insight is conﬁrmed by our ablation experiments in Section 4.5.
Standard key assignment and weighting. Let q(x) be a query and Tk denote the top-k operator4.
Given a set of keys K = {k1  . . .   k|K|} composed of |K| dq-dimensional vectors  and an input x 
4If the permutation (i1  . . .   in) sorts numbers (t1  . . .   tn) as ti1 ≥ ti2 ≥ ··· ≥ tin  the top-k indices are

Tk(t1  . . .   tn) = {i1  . . .   ik}

3

Figure 2: Illustration of the product keys. We deﬁne two discrete subsets of keys (sub-key set 1
and sub-key set 2). They induce a much larger set of keys  which are never made explicit (product
keys). Given a query  we split it into two sub-queries (q1 and q2). Selecting the k closest keys (k = 2
in the ﬁgure) in each subset implicitly selects k × k keys. The k keys maximizing the inner product
with the query are guaranteed to belong to this subset  on which the search can be done efﬁciently.

we select the top k keys maximizing the inner product with the query q(x):

(cid:1)

I = Tk

(cid:0)q(x)T ki
w = Softmax(cid:0)(q(x)T ki)i∈I(cid:1)
(cid:88)

m(x) =

i∈I wivi

# Get k nearest neighbors
# Normalize top-k scores
# Aggregate selected values

(1)
(2)
(3)
Here I denotes the indices of the k most similar keys (where the similarity measure is the inner
product)  and w is the vector that represents the normalized scores associated with the selected keys.
All these operations can be implemented using auto-differentiation mechanisms  making our layer
pluggable at any location in a neural network.
Operations (2)  (3) only depend on the top-k indices and are therefore computationally efﬁcient.
In contrast  the exhaustive comparison of Equation (1) is not efﬁcient for large memories since it
involves computing |K| inner products. To circumvent this issue  we resort to a structured set of
keys  that we refer to as product keys.

The product key set
operator  of two vector codebooks C and C(cid:48):

is deﬁned as the outer product  with respect to the vector concatenation

K = {(c  c(cid:48)) | c ∈ C  c(cid:48) ∈ C(cid:48)}

The total number of keys induced by this Cartesian product construction is |K| = |C|×|C(cid:48)|. The sets
C and C(cid:48) both comprise a set of sub-keys of dimension dq/2. We exploit this structure to compute
the closest keys I ∈ (1  ...  K) efﬁciently. First  we split the query q(x) into two sub-queries q1 and
q2. We then compute the k sub-keys in C (resp. C(cid:48)) closest to the sub-query q1 (resp. q2):

(cid:0)(q1(x)T ci)i∈{1...|C|}(cid:1)  

IC = Tk

(cid:0)(q2(x)T c(cid:48)

j)j∈{1...|C(cid:48)|}(cid:1)

IC(cid:48) = Tk

We are guaranteed that the k most similar keys in K are of the form {(ci  c(cid:48)
example of product keys with the key selection process is shown in Figure 2.

(4)
j) | i ∈ IC  j ∈ IC(cid:48)}. An

3.2 Complexity
Searching for the top-k most similar keys when the keys have a ﬂat representation requires |K|
comparisons of vectors of size dq  i.e. O(|K| × dq) operations.
For product keys  we consider the setup where |C| = |C(cid:48)|  i.e.
the conﬁguration that maximizes
only need to compare the two sub-queries with |C| and |C(cid:48)| sub-keys of size dq/2  which amounts to

|C|×|C(cid:48)| for a ﬁxed number of sub-keys |C| +|C(cid:48)|. Since |K| = |C|×|C(cid:48)|  we have |C| =(cid:112)|K|. We
O(|C| × dq/2 + |C(cid:48)| × dq/2) = O(|C| × dq) = O((cid:112)|K| × dq) operations.

Then  we need to search for the top-k keys in {(ci  c(cid:48)
C}  which is a set composed
of k2 keys of dimension dq. This can be done in O(k2 × dq) operations (in practice  this could be

j) | i ∈ IC  j ∈ I(cid:48)

4

c1c3c2c'1c'3c'2sub-key set 1sub-key set 2product keysc3c3c3c2c2c2c1c1c1c'1c'3c'2c'1c'3c'2c'1c'3c'2c2c1c3c'3c'1c'2q1q2q1q2querysub-key retrievalc3c3c2c2c'1c'3c'1c'3 candidate keys2key selectionc3c2c'1c'1 selected keysFigure 3: Left: A typical transformer block is composed by a self-attention layer followed by an
FFN layer (a two layer network). Right:
In our system  we replace the FFN layer with a product
key memory layer  which is analogous to a sparse FFN layer with a very large hidden state. In
practice  we only replace the FFN layer in N layers  where typically N ∈ {0  1  2}.
done in O(k log k) scalar operations with a priority list [1]  but this choice is less compliant with
GPU architectures). As a result  the overall complexity is:

O(cid:16)

((cid:112)|K| + k2) × dq

(cid:17)

For small values of k  and a memory of size |K| = 10242  retrieving the nearest product keys
requires about 103 less operations than an exhaustive search. As shown later in our ablation study 
product keys also lead to a better performance compared to a set composed of ﬂat keys.

3.3 Multi-head memory attention

We make the model more expressive with a multi-head mechanism  where each head independently
computes a query used to select k keys from the memory. The memory simply sums the output

mi(x) of each head i: m(x) =(cid:80)H

i=1 mi(x) where H is the number of heads.

Each head has its own query network and its own set of sub-keys  but all heads share the same
values. This is similar to the multi-head attention used in transformers  except that we do not split
the query into H heads  but instead create H queries. As the query networks are independent from
each other and randomly initialized  they often map the same input to very different values of the
memory. In practice  for the same input we observe very little overlap between the keys selected
by two different heads. This method let us increase key usage and generally improves performance.
The impact of the multi-head attention mechanism is discussed in Section 4.5.

4 Experiments

We report results on large-scale experiments for transformer models equipped with a memory  fol-
lowed by an ablation study that shows the impact of different memory components on the model
performance and memory usage. We propose to replace the FFN block of some transformer layers
by a memory  as presented in Figure 3. In that setting  the memory is integrated with a residual con-
nection in the network  and the input x to the memory layer becomes x ← x + PKM(x) instead of
x ← x + FFN(x). In practice  we could also keep the FFN layer and simply interleave the memory
between some transformer layers.

4.1 Dataset

We evaluate the impact of our memory in a large scale language modeling task  where traditional
models are known to underﬁt. The largest publicly available language modeling dataset is the One
Billion Word corpus [4]. As noted in prior work [2  9  36]  obtaining a good performance on this
dataset requires tedious regularization as it is now too small for standard architectures. In our experi-
ments  we encountered the same issues  and observed that even a small model was enough to overﬁt:
on this dataset  for a 16 layers model with a dimensionality of 1024  we obtain a test perplexity of
25.3 when the validation perplexity starts to increase. The train perplexity is then equal to 14.8 and
keeps improving while the validation perplexity deteriorates.
We therefore evaluate the beneﬁt of our approach on a corpus that is 30 times larger and extracted
from the public Common Crawl. The training set is composed of 28 billion words (140 GB of data)
extracted from about 40 million English news articles indexed by Common Crawl corpora. The
validation and test sets are both composed of 5000 news articles removed from the training set.
Unlike in the One Billion Word corpus  we did not shufﬂe sentences  allowing the model to learn
long range dependencies. On this dataset  we did not observe any overﬁtting  and increasing the

5

Self-attentionFeed-forwardlayer (FFN)++Self-attentionMemory layer(PKM)++model capacity systematically led to a better performance on the validation set. We tokenized the
data using the tokenizer provided by the Moses toolkit [26]. To reduce the vocabulary size  we use
fastBPE5 to apply Byte Pair Encoding (BPE) [39]  with 60k BPE splits.

4.2 Evaluation metrics

We measure the performance of our models by reporting the perplexity on the test set. For models
with memories  we report two different metrics to evaluate the usage:

i =(cid:80)

• The memory usage that represents the fraction of accessed values: #{zi (cid:54)= 0}

• The KL divergence between z and the uniform distribution: log(|K|) +(cid:80) zi log(zi)

where z = z(cid:48)/(cid:107)z(cid:48)(cid:107)1  and z(cid:48) ∈ R|K| is deﬁned as z(cid:48)
x w(x)i where w(x) represents the weights
of the keys accessed in the memory when the network is fed with an input x from the test set (i.e. 
the w(x) are sparse with at most H × k non-zero elements).
At test time  we expect the model to access as many keys as possible  i.e.
to have a usage near
100%; a lower usage means that part of the capacity is not exploited at all. The KL divergence
reﬂects imbalance in the access patterns to the memory: if the model attends the same key for every
query (while giving a tiny weight to the remaining keys)  it would give a perfect usage but a very
high KL  showing that the same performance could be achieved with just one value.

4.3 Training details

We use a transformer architecture with 16 attention heads and learned positional embeddings. We
consider models with 12  16 or 24 layers  with either 1024 or 1600 dimensions. We train our models
with the Adam optimizer [25]  with a learning rate of 2.5 × 10−4  with β1 = 0.9  β2 = 0.98 
following the learning rate schedule of Vaswani et al. [44]. In the memory  the keys and the query
network are learned with the same optimizer and learning rate as the rest of the network. Since the
memory values are learned with sparse updates  we found it beneﬁcial to learn them with a higher
Adam learning rate of 10−3. We implement our models with PyTorch [35]  and train them on 32
Volta GPUs. We use ﬂoat16 operations to speed up training and to reduce the GPU memory usage
of our models. To retrieve key indices efﬁciently  we perform the search over sub-keys with a fast
nearest neighbors implementation by Johnson et al. [22].
For a transformer model with L layers and N memories  we interspersed the memories at regular
intervals. For instance  for L = 16 and N = 2  we replace the FFN of layers 6 and 12. This way  the
network can leverage information at different levels of the architecture. The impact of the memory
position within the network is studied in Section 4.5. In our main experiments  we use H = 4
memory heads  we select k = 32 keys per head  and use |K| = 5122 memory slots.

4.4 Results

Dimension
N memories

12 layers
16 layers
24 layers

1024

1

2

3

0

1600

1

15.6
14.9
14.6

14.8
14.1

-

14.5

-
-

15.0
14.4
14.0

13.7
13.2

-

0

17.7
16.7
16.0

Table 1: Test perplexity for mod-
els with and without memory. PKM
models with 12 layers outperforms 24-
layer models of same dimensionality.
Bold refers to models optimizing per-
formance for a given dimension.

Table 1 and Figure 4 show the perplexity of different models on the test set of the CC-News corpus.
We observe that increasing either the dimensionality or the number of layers leads to signiﬁcant per-
plexity improvements in all the models. However  adding a memory to the model is more beneﬁcial
than increasing the number of layers; for instance  a model with a single memory and 12 layers out-
performs a memoryless model with the same hidden dimension and 24 layers  both when the number
of hidden units is 1024 and 1600. Adding 2 or 3 memory layers further improves performance.
Figure 4 also shows speed as measured in words per second  for different model conﬁgurations.
In particular  when the internal hidden states have 1024 dimensions  a model with 12 layers and a

5https://github.com/glample/fastBPE

6

Figure 4: Trade-off between speed and perplexity on the test set. Labels on the graph represent
the number of layers. Adding memory layers signiﬁcantly improves the performance and has a
negligible impact on the inference speed. Models with 12 layers and a Product Key Memory (PKM)
outperform 24-layer models of the same dimension  while being almost twice faster at inference. In
particular  a 12-layer model of dimension 1024 with a memory outperforms a model of 24 layers of
the same dimension (same conﬁguration as BERT large).

memory obtains a better perplexity than a model with 24 layers (same conﬁguration as BERT large) 
and it is almost twice faster. When adding memory to large models that have internal dimensionality
equal to 1600  inference time barely increases.

4.5 Ablation Study

In this section we study the impact of the different components on the memory layer  and measure
how they affect the model performance and the memory usage. For all experiments  we consider a
transformer network with 6 layers and 8 heads. Unless speciﬁed otherwise  we consider a memory
of 5122 = 262k slots  with 4 memory heads  k = 32 selected keys  and we insert it at layer 5.
Memory size. We train transformer models with memories of size |K| = |C| × |C(cid:48)|  with |C(cid:48)| =
|C| and |C| ∈ {128  256  384  512  768  1024}. Table 2 shows that test perplexity decreases as the
memory becomes larger. A model with a memory size of 16k obtains a perplexity of 22.8. Increasing
the size to 1M decreases the perplexity down to 18.0 while leaving the inference time unchanged.
The dominant factor for inference time is the number of accessed memory values  which is governed
by the number of memory heads and the parameter k  but not the memory size.

Query Batch Normalization. Table 2 and Figure 5 present results with and without batch nor-
malization in the query network. We observe that for small memories the usage is always close to
100%  but for a memory of size 1M  the batch normalization layer improves usage from 25.8% to
80.3%  with a consequent perplexity decrease from 19.8 down to 18.0. For comparison  a model
without memory obtains a perplexity of 23.0  which is on par with a memory of size 16k.
Finally  we observe a correlation between the number of used keys and the model performance. In
particular  a model with a memory of size 1M that does not use batch normalization uses about
25.8% of the memory values (i.e. roughly 250k values)  and obtains a perplexity of 19.8  which is
on par with the model using a memory of size 262k that uses batch normalization  and that has a
nearly optimal memory usage of 100%.

Memory position.
In this experiment we insert the memory at different levels in the transformer 
to see where it is the most beneﬁcial. In Table 3 we observe that the model beneﬁts the most from
the memory when it replaces the FFN of the layers 4 or 5 in the transformer. Putting memory at layer
1 (after the input token embeddings) gives the worst performance. When the memory is inserted in
layer 6  it is located right before the softmax output  the model has only one linear layer to process

7

4k6k8k10k12k14kInference speed (words per second)1314151617Perplexity121624121620121624 (BERT large)1216241216dim 1600  0 PKMdim 1600  1 PKMdim 1024  0 PKMdim 1024  1 PKMdim 1024  2 PKMsTable 2: Perplexity and memory usage for different memory sizes  with and without Batch-
Norm. Adding a batch normalization layer in the query network encourages the model to use more
keys. This is not necessary for small memories of size 16k and 65k where the usage is already close
to 100% without batch normalization  but for memories of size 147k of more  batch normalization
improves the memory usage signiﬁcantly  along with the perplexity.
Memory size
BatchNorm
Perplexity
Usage (%)
KL

Yes
21.9
100.0
0.58

Yes
18.0
80.3
0.95

No
20.5
64.4
1.20

Yes
20.7
99.6
0.65

Yes
19.8
97.9
0.68

65k

147k

262k

590k

1M

No
20.0
38.0
1.70

16k

No
22.8
100
0.56

Yes
23.0
100
0.56

Yes
18.7
90.3
0.83

No
19.8
25.8
2.06

No
21.7
99.0
0.69

No
20.9
83.8
0.94

Figure 5: Memory usage and perplexity with and without query batch normalization. Adding batch normal-
ization increases both performance and the fraction of used memory slots.

Figure 6: Memory usage and perplexity for different number of heads  and number of k-NN. Increasing the
number of heads or k-NN increases both performance and the fraction of used memory slots.

the information read from the memory. The best position to insert the memory is at an intermediate
layer. We surmise that effective use of the memory requires operating in a more abstract feature
space than the input and that it is important to have some layers on the top of the memory to further
process and aggregate information from every location.

Number of heads / k-NN. Figure 6 shows that increasing the number of heads or the number
of k-NN improves both the perplexity of the model  and the memory usage. We also note that
models with identical h × k (h being the number of heads and k the number of nearest neighbors)
have a similar memory usage  i.e. models with (h  k) ∈ {(1  64)  (2  32)  (4  16)  (8  8)} all have a
memory usage around 70%  and a perplexity around 20.5. Adding more heads overall improves the
performance  but also increases the computation time. Overall  we found that using 4 heads and 32
k-NN strikes a good trade-off between speed and performance.

8

16k65k147k262k590k1MMemory size30405060708090100Memory usage (%)Without Query BatchNormWith Query BatchNorm16k65k147k262k590k1MMemory size181920212223PerplexityWithout Query BatchNormWith Query BatchNorm8163264Number of k-NN020406080100Memory usage (%)1 head2 heads4 heads8 heads8163264Number of k-NN19.520.020.521.021.522.022.523.0Perplexity1 head2 heads4 heads8 headsTable 3: Perplexity and memory usage for different memory positions in a transformer with 6
layers. Adding a memory in positions 4 or 5 maximizes the performance (layer 1 is the worst).

Position
Perplexity
Usage (%)
KL

1

21.5
100.0
2.23

2

20.7
100.0
0.95

3

20.4
98.3
0.74

4

20.1
97.1
0.71

5
19.8
97.9
0.68

6

20.3
96.9
1.08

Table 4: Perplexity  memory usage and inference speed with product keys and regular keys.
Models with product keys have a much better usage than models that represent keys by a ﬂat matrix 
and obtain a better perplexity. They also have signiﬁcantly less parameters and are dramatically
faster to run. The speed is measured at inference  in thousands of words per second (w/s). For
models with more than 262k memory slots  we only report the inference time. We observe that with
product keys  the memory size do not impact the inference time.

Memory size
Product Keys
Perplexity
Usage (%)
KL
Speed (w/s)

16k

Yes
No
23.0
23.2
100
19.6
2.04
0.56
35.0k 35.8k

65k

Yes
No
21.9
22.6
13.6 100.0
2.48
0.58
28.5k 36.7k

147k

Yes
No
20.7
22.1
99.6
10.1
2.77
0.65
13.9k 36.4k

262k

No Yes
-
19.8
97.9
-
-
0.68
7.7k 36.3k

590k

No Yes
-
18.7
90.3
-
0.83
-
4.7k 36.2k

1M

No Yes
-
18.0
80.3
-
0.95
-
1.2k 35.7k

Product keys vs. ﬂat keys. Product keys presented in Figure 2 enable ﬁnding the nearest neigh-
bors in a matrix of size (|C|2  dk) with the same time/compute complexity of a search over two
2 ). As a result  product keys contain |C| times less parameters than keys rep-
matrices of size (|C|  dk
resented by a full matrix. Table 4 and Figure 7 compare product keys to the default regular ﬂat keys.
In the second case  searching the nearest keys boils down to a liner index search at each iteration 
which is computationally very expensive. As a result  we only report results for memories of size
16k  65k  147k  as experiments with a ﬂat index on larger memories takes an unreasonable amount
of time to converge. We can see that models with product keys are not only faster but they have also
a much better memory usage  and consequently obtain a better perplexity.

Figure 7: Speed over memory size. Speed
(in thousands of words per second) for different
memory sizes. For regular ﬂat keys  increasing
the number of keys signiﬁcantly slows down the
model  while with product keys  increasing the
memory size barely impacts the inference speed.

5 Conclusion

This paper introduces a memory layer that allows to drastically improve the capacity of a neural
network with a negligible computational overhead. The efﬁciency of our layer relies on two key
ingredients:
the factorization of keys as a product set  and the sparse read/write accesses to the
memory values. Our layer is integrated into an existing neural network architecture. We show
experimentally that it provides important gains on large-scale language modeling  reaching with 12
layers the performance of a 24-layer BERT-large model with half the running time.

9

16k65k147k262k590k1MMemory size05101520253035Words per second (x1000)Regular keysProduct keysReferences
[1] Artem Babenko and Victor Lempitsky. The inverted multi-index.

Pattern Analysis and Machine Intelligence  2014.

IEEE Transactions on

[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language model-

ing. In International Conference on Representation Learning  2019.

[3] Yoshua Bengio  Nicholas L´eonard  and Aaron C. Courville. Estimating or propagating gradi-

ents through stochastic neurons for conditional computation. CoRR  abs/1308.3432  2013.

[4] Ciprian Chelba  Tomas Mikolov  Mike Schuster  Qi Ge  Thorsten Brants  Phillipp Koehn  and
Tony Robinson. One billion word benchmark for measuring progress in statistical language
modeling. Conference of the International Speech Communication Association  2014.

[5] Bo Chen and Jeffrey M. Gilbert. The on-device visual intelligence challenge. https://

ai.googleblog.com/2018/04/introducing-cvpr-2018-on-device-visual.html 
2019. Accessed: 2019-05-20.

[6] Kyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation

ratio for conditional computation in deep learning. CoRR  abs/1406.7362  2014.

[7] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with

weights and activations constrained to +1 or -1. CoRR  2016.

[8] Matthieu Courbariaux  Yoshua Bengio  and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. Advances in Neural Information
Processing Systems  2015.

[9] Zihang Dai  Zhilin Yang  Yiming Yang  William W Cohen  Jaime Carbonell  Quoc V Le 
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. In Conference of the Association for Computational Linguistics  2019.

[10] Ludovic Denoyer and Patrick Gallinari.

abs/1410.0510  2014.

Deep sequential neural network.

CoRR 

[11] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training
In Conference of the North

of deep bidirectional transformers for language understanding.
American Chapter of the Association for Computational Linguistic  2018.

[12] D. Eigen  I. Sutskever  and M. Ranzato. Learning factored representations in a deep mixture

of experts. In Workshop at the International Conference on Learning Representations  2014.

[13] Thomas Gerald  Nicolas Baskiotis  and Ludovic Denoyer. Binary stochastic representations for
large multi-class classiﬁcation. In International Conference on Neural Information Processing 
2017.

[14] Edouard Grave  Moustapha M Cisse  and Armand Joulin. Unbounded cache model for on-
line language modeling with open vocabulary. In Advances in Neural Information Processing
Systems  2017.

[15] Edouard Grave  Armand Joulin  and Nicolas Usunier. Improving neural language models with

a continuous cache. In International Conference on Representation Learning  2017.

[16] Alex Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep
In International Conference on Acoustics  Speech  and Signal

recurrent neural networks.
Processing  2013.

[17] Alex Graves  Greg Wayne  and Ivo Danihelka. Neural turing machines. CoRR  abs/1410.5401 

2014.

[18] Sam Gross  Marc’Aurelio Ranzato  and Arthur Szlam. Hard mixtures of experts for large scale
weakly supervised vision. In Conference on Computer Vision and Pattern Recognition  2017.
[19] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In Conference on Computer Vision and Pattern Recognition  2016.

[20] Yanping Huang  Yonglong Cheng  Dehao Chen  HyoukJoong Lee  Jiquan Ngiam  Quoc V. Le 
and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline paral-
lelism. CoRR  abs/1811.06965  2018.

[21] Herv´e J´egou  Matthijs Douze  and Cordelia Schmid. Product Quantization for Nearest Neigh-

bor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence  2011.

10

[22] Jeff Johnson  Matthijs Douze  and Herv´e J´egou. Billion-scale similarity search with gpus.

IEEE Transactions on Big Data  2017.

[23] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented re-

current nets. In Advances in Neural Information Processing Systems  2015.

[24] Koray Kavukcuoglu  Marc’Aurelio Ranzato  and Yann LeCun. Fast inference in sparse coding
algorithms with applications to object recognition. CoRR  abs/1010.3467  2010. URL http:
//arxiv.org/abs/1010.3467.

[25] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Representation Learning  2015.

[26] Philipp Koehn  Hieu Hoang  Alexandra Birch  Chris Callison-Burch  Marcello Federico 
Nicola Bertoldi  Brooke Cowan  Wade Shen  Christine Moran  Richard Zens  et al. Moses:
Open source toolkit for statistical machine translation. In Conference of the Association for
Computational Linguistics  2007.

[27] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems  2012.
In

[28] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining.

Advances in Neural Information Processing Systems  2019.

[29] Dhruv Mahajan  Ross B. Girshick  Vignesh Ramanathan  Kaiming He  Manohar Paluri  Yixuan
Li  Ashwin Bharambe  and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In European Conference on Computer Vision  2018.

[30] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. In International Conference on

Representation Learning  2014.

[31] Alireza Makhzani and Brendan J Frey. Winner-take-all autoencoders. In Advances in Neural

Information Processing Systems  2015.

[32] Marius Muja and David G. Lowe. Scalable nearest neighbor algorithms for high dimensional

data. IEEE Transactions on Pattern Analysis and Machine Intelligence  2014.

[33] Behnam Neyshabur  Zhiyuan Li  Srinadh Bhojanapalli  Yann LeCun  and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. In
International Conference on Representation Learning  2019.

[34] Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set  a

strategy employed by v1? Vision Research  1997.

[35] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. In Neurips Autodiff Workshop  2017.

[36] Alec Radford  Jeff Wu  Rewon Child  David Luan  Dario Amodei  and Ilya Sutskever. Lan-

guage models are unsupervised multitask learners  2019.

[37] Jack Rae  Jonathan J Hunt  Ivo Danihelka  Timothy Harley  Andrew W Senior  Gregory Wayne 
Alex Graves  and Timothy Lillicrap. Scaling memory-augmented neural networks with sparse
reads and writes. In Advances in Neural Information Processing Systems  2016.

[38] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Ima-
genet classiﬁcation using binary convolutional neural networks. In European Conference on
Computer Vision  2016.

[39] Rico Sennrich  Barry Haddow  and Alexandra Birch. Neural machine translation of rare words

with subword units. In Conference of the Association for Computational Linguistics  2015.

[40] Noam Shazeer  Azalia Mirhoseini  Krzysztof Maziarz  Andy Davis  Quoc V. Le  Geoffrey E.
Hinton  and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-
experts layer. In International Conference on Representation Learning  2017.

[41] Stefano Spigler  Mario Geiger  St´ephane d’Ascoli  Levent Sagun  Giulio Biroli  and Matthieu
Wyart. A jamming transition from under- to over-parametrization affects loss landscape and
generalization. CoRR  abs/1810.09665  2018.

[42] Sainbayar Sukhbaatar  arthur szlam  Jason Weston  and Rob Fergus. End-to-end memory net-

works. In Advances in Neural Information Processing Systems  2015.

11

[43] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural

networks. In Advances in Neural Information Processing Systems  2014.

[44] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez 
In Advances in Neural

Łukasz Kaiser  and Illia Polosukhin. Attention is all you need.
Information Processing Systems  2017.

[45] Sudheendra Vijayanarasimhan  Jonathon Shlens  Rajat Monga  and Jay Yagnik. Deep net-
In Workshop at the International Conference on Learning

works with large output spaces.
Representations  2015.

[46] Alex Wang  Amapreet Singh  Julian Michael  Felix Hill  Omer Levy  and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Representation Learning  2018.

[47] Jason Weston  Sumit Chopra  and Antoine Bordes. Memory networks.

Conference on Representation Learning  2015.

In International

[48] Jason Weston  Antoine Bordes  Sumit Chopra  and Tomas Mikolov. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks. In International Conference on Representation
Learning  2016.

12

,Guillaume Lample
Alexandre Sablayrolles
Marc'Aurelio Ranzato
Ludovic Denoyer
Herve Jegou