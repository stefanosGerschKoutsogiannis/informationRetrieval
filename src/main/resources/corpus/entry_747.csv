2011,Bayesian Partitioning of Large-Scale Distance Data,A Bayesian approach to partitioning distance matrices is presented. It is inspired by the 'Translation-Invariant Wishart-Dirichlet' process (TIWD) in (Vogt et al.  2010) and shares a number of advantageous properties like the fully probabilistic nature of the inference model  automatic selection of the number of clusters and applicability in semi-supervised settings. In addition  our method (which we call 'fastTIWD') overcomes the main shortcoming of the original TIWD  namely its high computational costs. The fastTIWD reduces the workload in each iteration of a Gibbs sampler from O(n^3) in the TIWD to O(n^2). Our experiments show that this cost reduction does not compromise the quality of the inferred partitions. With this new method it is now possible to 'mine' large relational datasets with a probabilistic model  thereby automatically detecting new and potentially interesting clusters.,Bayesian Partitioning of Large-Scale Distance Data

David Adametz

Volker Roth

Department of Computer Science & Mathematics

University of Basel
Basel  Switzerland

{david.adametz volker.roth}@unibas.ch

Abstract

A Bayesian approach to partitioning distance matrices is presented. It is inspired
by the Translation-invariant Wishart-Dirichlet process (TIWD) in [1] and shares
a number of advantageous properties like the fully probabilistic nature of the in-
ference model  automatic selection of the number of clusters and applicability in
semi-supervised settings. In addition  our method (which we call fastTIWD) over-
comes the main shortcoming of the original TIWD  namely its high computational
costs. The fastTIWD reduces the workload in each iteration of a Gibbs sampler
from O(n3) in the TIWD to O(n2). Our experiments show that the cost reduction
does not compromise the quality of the inferred partitions. With this new method
it is now possible to ‘mine’ large relational datasets with a probabilistic model 
thereby automatically detecting new and potentially interesting clusters.

1 Introduction

In cluster analysis we are concerned with identifying subsets of n objects that share some similarity
and therefore potentially belong to the same sub-population. Many practical applications leave us
without direct access to vectorial representations and instead only supply pairwise distance measures
collected in a matrix D. This poses a serious challenge  because great parts of geometric information
are hereby lost that could otherwise help to discover hidden structures. One approach to deal with
this is to encode geometric invariances in the probabilistic model  as proposed in [1]. The most
important properties that distinguish this Translation-invariant Wishart-Dirichlet Process (TIWD)
from other approaches working on pairwise data are its fully probabilistic model  automatic selection
of the number of clusters  and its applicability in semi-supervised settings in which not all classes
are known in advance. Its main drawback  however  is the high computational cost of order O(n3)
per sweep of a Gibbs sampler  limiting its applicability to relatively small data sets.
In this work we present an alternative method which shares all the positive properties of the TIWD
while reducing the computational workload to O(n2) per Gibbs sweep. In analogy to [1] we call this
new approach fastTIWD. The main idea is to solve the problem of missing geometric information by
a normalisation procedure  which chooses one particular geometric embedding of the distance data
and allows us to use a simple probabilistic model for inferring the unknown underlying partition.
The construction we use is guaranteed to give the optimal such geometric embedding if the true
partition was known. Of course  this is only a hypothetical precondition  but we show that even rough
prior estimates of the true partition signiﬁcantly outperform ‘naive’ embedding strategies. Using a
simple hierarchical clustering model to produce such prior estimates leads to clusterings being at
least of the same quality as those obtained by the original TIWD. The algorithmic contribution
here is an efﬁcient algorithm for performing this normalisation procedure in O(n2) time  which
makes the whole pipeline from distance matrix to inferred partition an O(n2) process (assuming
a constant number of Gibbs sweeps). Detailed complexity analysis shows not only a worst-case
complexity reduction from O(n3) to O(n2)  but also a drastic speed improvement. We demonstrate

1

this performance gain for a dataset containing ≈ 350 clusters  which now can be analysed in 6 hours
instead of ≈ 50 days with the original TIWD.
It should be noted that both the TIWD and our fastTIWD model expect (squared) Euclidean dis-
tances on input. While this might be seen as a severe limitation  we argue that (i) a ‘zoo’ of Mercer
kernels has been published in the last decade  e.g. kernels on graphs  sequences  probability distri-
butions etc. All these kernels allow the construction of squared Euclidean distances; (ii) efﬁcient
preprocessing methods like randomised versions of kernel PCA have been proposed  which can be
used to transform an initial matrix into one of squared Euclidean type; (iii) one might even use an
arbitrary distance matrix hoping that the resulting model mismatch can be tolerated.
In the next section we introduce a probabilistic model for partitioning inner product matrices  which
is generalised in section 3 to distance matrices using a preprocessing step that breaks the geomet-
ric symmetry inherent in distance representations. Experiments in section 4 demonstrate the high
quality of clusterings found by our method and its superior computational efﬁciency over the TIWD.

d

2 exp(cid:2)− d
2 tr(ΨS)(cid:3) .

2 exp(cid:2)− d

2 A Wishart Model for Partitioning Inner Product Matrices
Suppose there is a matrix X ∈ Rn×d representing n objects in Rd that belong to one of k sub-
populations. For identifying the underlying cluster structure  we formulate a generative model by
assuming the columns xi ∈ Rn  i = 1 . . . d are i.i.d. according to a normal distribution with zero
mean and covariance Σn×n  i.e. xi ∼ N (0n  Σ)  or in matrix notation: X ∼ N (0n×d  Σ ⊗ I).
d XX t ∈ Rn×n is central Wishart distributed  S ∼ Wd(Σ). For convenience we deﬁne
Then  S = 1
the generalised central Wishart distribution which also allows rank-deﬁcient S and/or Σ as

p(S|Ψ  d) ∝ det(S)

1

2 (d−n−1) det(Ψ)

(1)
where det(•) is the product of non-zero eigenvalues and Ψ denotes the (generalised) inverse of Σ.
The likelihood as a function in Ψ is

2 tr(ΨS)(cid:3)  

L(Ψ) = det(Ψ) d

(2)
Consider now the case where we observe S without direct access to X. Then  an orthogonal trans-
formation X ← OX cannot be retrieved anymore  but it is reasonable to assume such rotations are
irrelevant for ﬁnding the partition. Following the Bayesian inference principle  we complement the
likelihood with a prior over Ψ. Since by assumption there is an underlying joint normal distribu-
tion  a zero entry in Ψ encodes conditional independence between two objects  which means that
block diagonal Ψ matrices deﬁne a suitable partitioning model in which the joint normal is decom-
posed into independent cluster-wise normals. Note that the inverse of a block diagonal matrix is
also block diagonal  so we can formulate the prior in terms of Σ  which is easier to parametrise.
For this purpose we adapt the method in [2] using a Multinomial-Dirichlet process model [3  4  5]
to deﬁne a ﬂexible prior distribution over block matrices without specifying the exact number of
blocks. We only brieﬂy sketch this construction and refer the reader to [1  2] for further details. Let
Bn be the set of partitions of the index set [n]. A partition B ∈ Bn can be represented in matrix
form as B(i  j) = 1 if y(i) = y(j) and B(i  j) = 0 otherwise  with y being a function that maps
[n] to some label set L. Alternatively  B may be represented as a set of disjoint non-empty subsets
called ‘blocks’ b. A partition process is a series of distributions Pn on the set Bn in which Pn is the
marginal of Pn+1. Using a multinomial model for the labels and a Dirichlet prior with rate parame-
ter ξ on the mixing proportions  we may integrate out the latter and derive a Dirichlet-Multinomial
prior over labels. Finally  after using a ‘label forgetting’ transformation  the prior over B is:

Γ(ξ)(cid:81)

p(B|ξ  k) =

k!

(k − kB)!

b∈B Γ(nb + ξ/k)

[Γ(ξ/k)]kB Γ(n + ξ)

.

(3)

In this setting  k is the number of blocks in the population (k can be inﬁnite  which leads to the
Ewens Process [6]  a.k.a. Chinese Restaurant Process)  nb is the number of objects in block b and
kB ≤ k is the total number of blocks in B. The prior is exchangeable meaning rows and columns
can be (jointly) permuted arbitrarily and therefore partition matrices can always be brought to block
diagonal form. To specify the variances of the normal distributions  the models in [1  2] use two
global parameters  α  β  for the within- and between-class scatter. This model can be easily extended
to include block-wise scatter parameters  but for the sake of simplicity we will stay with the simple
parametrisation here. The ﬁnal block diagonal covariance matrix used in (2) has the form

Σ = Ψ−1 = α(In + θB)  with θ := β/α.

(4)

2

b∈B

1
α

tr(ΨS) =(cid:80)

Inference by way of Gibbs sampling. Multiplying the Wishart likelihood (2)  the prior over par-
titions (3) and suitable priors over α  θ gives the joint posterior. Inference for B  α and θ can then be
carried out via a Gibbs sampler. Each Gibbs sweep can be efﬁciently implemented since both trace
and determinant in (2) can be computed analytically  see [1]:

where Sbb denotes the block submatrix corresponding to the bth diagonal block in B  and ¯Sbb =
bSbb1b. 1b is the indicator function mapping block b to a {0  1}n vector  whose elements are 1 if
1t
a sample is contained in b  or 0 otherwise. For the determinant one derives

(cid:3) = 1

(cid:2)tr(S) −(cid:80)

(cid:2)tr(Sbb) − θ
det(Ψ) = α−n(cid:81)
(cid:3). Using the prior α ∼ Inv-Gamma(r0 · d/2  s0 · d/2)  the posterior

b∈B(1 + θnb)−1.

(cid:2)tr(S)−(cid:80)

(6)
The conditional likelihood for α is Inv-Gamma(r  s) with shape parameter r = n· d/2− 1 and scale
s = d
2
is of the same functional form  and we can integrate out α analytically:

(cid:3) 

b∈B

b∈B

θ

1+nbθ

¯Sbb

¯Sbb

θ

1+nbθ

¯Sbb

1+nbθ

α

(5)

Pn(B|•) ∝ Pn(B|ξ  k) det(Ψ)d/2

(cid:0)tr(ΨS)(α=1) + s0
b∈B(1 + θnb)−1 and tr(ΨS)(α=1) = tr(S) −(cid:80)

(cid:2) d

(α=1)

2

where det(Ψ)(α=1) =(cid:81)

¯Sbb. Note that
the (usually unknown) degree of freedom d has the formal role of an annealing parameter  and it can
indeed be used to ‘cool’ the Markov chain by increasing d  if desired  until a partition is ‘frozen’.

b∈B

1+nbθ

θ

(cid:1)(cid:3)−(n+r0)d/2

 

(7)

Complexity analysis.
In one sweep of the Gibbs sampler  we have to iteratively compute the
membership probability of one object indexed by i to the kB currently existing blocks in partition B
(plus one new block)  given the assignments for the n− 1 remaining ones denoted by the superscript
(−i) [7  8]. In every step of this inner loop over kB existing blocks we have to evaluate the Wishart
likelihood  i.e. trace (5) and determinant (6). Given trace tr(−i)  we update ¯Sbb for kB blocks
b ∈ B which in total needs O(n) operations. Given det(−i)  the computation of all kB updated
determinants induces costs of O(kB). In total  there are n objects  so a full sweep requires O(n2 +
nkB) operations  which is equal to O(n2) since the maximum number of blocks is n  i.e. kB ≤ n.
Following [1]  we update θ on a discretised grid of values which adds O(kB) to the workload 
thus not changing the overall complexity of O(n2). Compared to the original TIWD  the worst
case complexity in the Dirichlet process model with an inﬁnite number of blocks in the population 
k = ∞  is reduced from O(n3) to O(n2) .

3 The fastTIWD Model for Partitioning Distance Matrices
Consider now the case where S is not accessible  but only squared pairwise distances D ∈ Rn×n:
(8)

D(i  j) = S(i  i) + S(j  j) − 2 S(i  j).

Observing one speciﬁc D does not imply a unique corresponding S  since there is a surjective
mapping from a set of S-matrices to D  S(D) (cid:55)→ D. Hereby  not only do we lose information
about orthogonal transformations of X  but also information about the origin of the coordinate
system. If S∗ is one (any) matrix that fulﬁlls (8) for a speciﬁc D  the set S(D) is formally deﬁned
as S = {S|S = S∗ + 1vt + v1t  S (cid:23) 0  v ∈ Rn} [9]. The Wishart distribution  however  is
not invariant against the choice of S ∈ S. In fact  if S∗ ∼ W(Σ)  the distribution of a general
S ∈ S is non-central Wishart  which can be easily seen as follows: S is exactly the set of inner
product matrices that can be constructed by varying c ∈ Rd in a modiﬁed matrix normal model
X ∼ N (M  Σ ⊗ Id) with mean matrix M = 1nct. Note that now the d columns in X are still
independent  but no longer identically distributed. Note further that ‘shifts’ ci do not affect pairwise
d XX t is
distances between rows in X. The modiﬁed matrix normal distribution implies that S = 1
non-central Wishart  S ∼ W(Σ  Θ)  with non-centrality matrix Θ := Σ−1M M t. The practical use 
however  is limited by its complicated form and the fundamental problem of estimating Θ based
on only one single observation S. It is thus desirable to work with a simpler probabilistic model.
In principle  there are two possibilities: either the likelihood is reformulated as being constant over
all S ∈ S (the approach taken in [1]  called the translation-invariant Wishart distribution)  or one
tries to ﬁnd a ‘good’ candidate matrix S(cid:48)
∗ that is ‘close’ to the underlying S∗ and uses the much

3

simpler central Wishart model. Both approaches have their pros and cons: encoding the translation
invariance directly in the likelihood is methodologically elegant and seems to work well in a couple
of experiments (cf. [1])  but it induces high computational cost. The alternative route of searching
for a good candidate S(cid:48)
∗ close to S∗ is complicated  because S∗ is unknown and it is not immediately
clear what ‘close’ means. The positive aspect of this approach is the heavily reduced computational
cost due to the formal simplicity of the central Wishart model. It is important to discuss the ‘naive’
way of ﬁnding a good candidate S(cid:48)
∗ by subtracting the empirical column means in X  thus removing
the shifts ci. This normalisation procedure can be implemented solely based on S  leading to the
well-known centering procedure in kernel PCA  [10]:

Sc = QI S Qt

I   with projection QI = I − (1/n)11t.

(9)
Contrary to the PCA setting  however  this column normalisation induced by QI does not work well
here  because the elements of a column vector in X are not independent. Rather  they are coupled
via the Σ component in the covariance tensor Σ ⊗ Id. Hereby  we not only remove the shifts ci  but
also alter the distribution: the non-centrality matrix does not vanish in general and as a result  Sc is
no longer central Wishart distributed.
In the following we present a solution to the problem of ﬁnding a candidate matrix S(cid:48)
∗ that recasts
inference based on the translation-invariant Wishart distribution as a method to reconstruct the op-
timal S∗. Our proposal is guided by a particular analogy between trees and partition matrices and
aims at exploiting a tree-structure to guarantee low computational costs. The construction has the
same functional form as (9)  but uses a different projection matrix Q.

d

L((cid:101)Ψ) ∝ det((cid:101)Ψ)

The translation-invariant Wishart distribution. Let S∗ induce pairwise distances D. Assuming
that S∗ ∼ Wd(Σ)  the distribution of an arbitrary member S ∈ S(D) can be derived analytically as
a generalised central Wishart distribution with a rank-deﬁcient covariance  see [2]. Its likelihood in

the rank-deﬁcient inverse covariance matrix(cid:101)Ψ is
2 exp(cid:2) − d
with(cid:101)Ψ = Ψ− (1tΨ1)−1Ψ11tΨ. Note that although S∗ appears in the ﬁrst term in (10)  the density

2 tr((cid:101)ΨS∗)(cid:3) = det((cid:101)Ψ)

is constant on all S ∈ S(D)  meaning it can be replaced by any other member of S(D). Note further
that S also contains rank-deﬁcient matrices (like  e.g. the column normalised Sc). By multiplying
(10) with the product of nonzero eigenvalues of such a matrix raised to the power of (d − n − 1)/2 
a valid generalised central Wishart distribution is obtained (see (1))  which is normalised on the
manifold of positive semi-deﬁnite matrices of rank r = n − 1 with r distinct positive eigenvalues

[11  12  13]. Unfortunately  (10) has a simple form only in(cid:101)Ψ  but not in the original Ψ  which ﬁnally

4 tr((cid:101)ΨD)(cid:3) 

2 exp(cid:2) d

(10)

d

leads to the O(n3) complexity of the TIWD model.

Selecting an optimal candidate S∗.

Introducing the projection matrix
Q = I − 1

1tΨ1 11tΨ 

(11)

one can rewrite (cid:101)Ψ in (10) as ΨQ or  equivalently  as QtΨQ  see [2] for details. Assume now

S ∼ Wd(Σ) induces distances D and consider the transformed S∗ = QSQt. Note that this trans-
formation does not change the distances  i.e. S ∈ S(D) ⇔ S∗ ∈ S(D)  and that QSQt has rank
r = n − 1 (because Q is a projection with kernel 1). Plugging our speciﬁc S∗ = QSQt into (10) 
extending the likelihood to a generalised central Wishart (1) with rank-deﬁcient inverse covariance

(cid:101)Ψ  exploiting the identity QQ = Q and using the the cyclic property of the trace  we arrive at
transformed matrix S∗ = QSQt  parametrised by the full-rank matrix Ψ if det((cid:101)Ψ) is substi-

(12)
By treating Q as a ﬁxed matrix  this expression can also be seen as a central Wishart in the

p(QSQt|(cid:101)Ψ  d) ∝ det(QSQt)

2 tr(ΨQSQt)(cid:3).

2 (d−n−1) det((cid:101)Ψ)

2 exp(cid:2) − d

tuted by the appropriate normalisation term det(Ψ). From this viewpoint  inference using the
translation-invariant Wishart distribution can be interpreted as ﬁnding a (rank-deﬁcient) represen-
tative S∗ = QSQt ∈ S(D) which follows a generalised central Wishart distribution with full-rank
inverse covariance matrix Ψ. For inferring Ψ  the rank deﬁciency of S∗ is not relevant  since only
the likelihood is needed. Thus S∗ can be seen as an optimal candidate inner-product matrix in the
set S(D) for a central Wishart model parametrised by Ψ.

d

1

4

Approximating S∗ with trees. The above selection of S∗ ∈ S(D) cannot be directly used in a
constructive way  since Q in (11) depends on unknown Ψ. If  on the other hand  we had some initial
estimate of Ψ  we could ﬁnd a reasonable transformation Q(cid:48) and hereby a reasonable candidate
S(cid:48)
∗. Note that even if the estimate of Ψ is far away from the true inverse covariance  the pairwise
distances are at least guaranteed not to change under Q(cid:48)S(Q(cid:48))t.
One particular estimate would be to assume that every object forms a singleton cluster  which means
that our estimate of Ψ is an identity matrix. After substitution into (11) it is easily seen that this as-
sumption results in the column-normalisation projection QI deﬁned in (9). However  if we assume
that there is some non-trivial cluster structure in the data  this would be a very poor approximation.
The main difﬁculty in ﬁnding a better estimate is to not specify the number of blocks. Our con-
struction is guided by an analogy between binary trees and weighted sums of cut matrices  which
are binary complements of partition matrices with two blocks. We use a binary tree with n leaves
representing n objects. It encodes a path distance matrix Dtree between those n objects  and for an
optimal tree Dtree = D. Such an optimal tree exists only if D is additive  and the task of ﬁnding an
approximation is a well-studied problem. We will not discuss the various tree reconstruction algo-
rithms  but only mention that there exist algorithms for reconstructing the closest ultrametric tree (in
the (cid:96)∞ norm) in O(n2) time  [14].

Figure 1: From left to right: Unknown samples X  pairwise distances collected in D  closest tree
structure and an exemplary building block.
A tree metric induced by Dtree is composed of elementary cut (pseudo-)metrics. Any such metric
lies in the metric space L1 and is also a member of (L2)2  which is the metric part of the space of
squared Euclidean distance matrices D. Thus  there exists a positive (semi-)deﬁnite Stree such that
(Dtree)ij = (Stree)ii + (Stree)jj − 2(Stree)ij. In fact  any matrix Stree has a canonical decomposition
into a weighted sum of 2-block partition matrices  which is constructed by cutting all edges (2n − 2
for a rooted tree) and observing the resulting classiﬁcation of leaf nodes. Suppose  we keep track of
such an assignment with indicator 1j induced by a single cut j  then the inner product matrix is

j=1 λj(1j1t

(13)
where λj is the weight of edge j to be cut and ¯1j (cid:55)→ {0  1}n is the complementary assignment  i.e.
1j ﬂipped. Each term (1j1t
j) is a 2-block partition matrix. We demonstrate the construction
of Stree in Fig. 2 for a small dataset of n = 25 objects sampled from S ∼ Wd(Σ) with d = 25 and
Σ = α(In + θB) as deﬁned in (4) with α = 2 and θ = 1. B contains 3 blocks and is depicted in the
ﬁrst panel. The remaining panels show the single-linkage clustering tree  all 2n − 2 = 48 weighted
2-block partition matrices  and the ﬁnal Stree (= sum of all individual 2-block matrices  rescaled to
full gray-value range). Note that single-linkage fails to identify the clusters in the three branches
closest to root  but still the structure of B is clearly visible in Stree.

j + ¯1j ¯1t

Stree =(cid:80)2n−2

j + ¯1j ¯1t

j) 

Figure 2: Inner product matrix of a tree. Left to right: Partition matrix B for n = 25 objects in 3
clusters  single-linkage tree  all weighted 2-block partition matrices  ﬁnal Stree.

The idea is now to have Stree as an estimate of Σ  and use its inverse Ψtree to construct Qtree in (11) 
which  however  naively would involve an O(n3) Cholesky decomposition of Stree.

5

Theorem 1. The n × n matrix S∗ = QtreeSQt
For the proof we need the following lemma:
Lemma 1. The product of Stree ∈ Rn×n and a vector y ∈ Rn can be computed in O(n) time.
Proof. (of lemma 1) Restating (13) and deﬁning m := 2n − 2  we have

tree can be computed in O(n2) time.

j

yl

l=1

j=1

j=1

λj

λj ¯1j +

j + ¯1j ¯1t

(cid:88)m
(cid:88)n
(cid:88)n
(cid:88)n
l=1 yl (cid:80)m

(cid:1)y =
(cid:0)1j1t
(cid:88)m
(cid:88)m
(cid:88)
(cid:88)
λj −(cid:88)
(cid:0)(cid:88)m
j=1 λj and(cid:80)m

j /∈Ri

λj +

j=1

l=1

l=1

yl

yl

(cid:88)m

j=1
λj1j

j=1

(cid:1)

¯1jl yl

1jlyl.

l=1

l=1

λj

(cid:0)1j
(cid:88)n
(cid:88)
(cid:0)λj
(cid:1) + 2

λj

l=1

l=1

λj ¯1j

1jl yl + ¯1j

(cid:88)n
(cid:88)n
1jlyl −(cid:88)m
(cid:88)n
(cid:1) −(cid:88)
(cid:88)
(cid:0)λj
λjyj −(cid:88)m

(cid:88)

j /∈Ri

j=1

yl

l∈Rj

j∈Ri

λjyj.

j=1

(cid:1)

j∈Ri

j∈Ri

Streey =

=

(Streey)i =

Note that(cid:80)n

=

In the next step  let us focus speciﬁcally on the ith element of the resulting vector. Furthermore 
assume Ri is the set of all nodes on the branch starting from node i and leading to the tree’s root:

(14)

yl

l∈Rj

(15)

j=1 λjyj are constants and computed in O(n) time. For each
element i  we are now left to ﬁnd Ri in order to determine the remaining two terms. This can be
done directly on the tree structure in two separate traversals:

1. Bottom up: Starting from the leaf nodes  store the sum of both childrens’ y values in their
parent node j (see Fig. 1  rightmost)  then ascend. Do the same for λj and compute λjyj.
2. Top down: Starting from the root node  recursively descend into the child nodes j and sum

up λj and λjyj until reaching the leafs. This implicitly determines Ri.

It is important to stress that the above two tree traversals fully describe the complete algorithm.
Proof. (of theorem 1) First  note that only the matrix-vector product a := Ψtree1 is needed in

tree =(cid:0)I − 1

QtreeSQt

1tΨtree1 11t Ψtree

(cid:1)S(cid:0)I − Ψtree

1tΨtree1 11t(cid:1)

1

= S − (1/1ta) 1atS − (1/1ta) S a1t + (1/1ta)2 1atS a1t.

(16)

One way of computing a = Ψtree1 is to employ conjugate gradients (CG) and iteratively minimise
||Streea− 1||2. Theoretically  CG is guaranteed to ﬁnd the true a in O(n) iterations  each evaluating
one matrix-vector product (Streey)  y ∈ Rn. Due to lemma 1  a can be computed in O(n2) time
tree (only matrix-vector products  so O(n2) complexity
and is used in (16) to compute S∗ = QtreeSQt
is maintained).

4 Experiments

Synthetic examples: normal clusters.
In a ﬁrst experiment we investigate the performance of
our method on artiﬁcial datasets generated in accordance with underlying model assumptions. A
partition matrix B of size n = 200 containing k = 3 blocks is sampled from which we construct
ΣB = α(I + θB). Then  X is drawn from N (M = 40 · 1n1t
d  Σ = ΣB ⊗ Id) with d = 300
d XX t and D. The covariance parameters are set to α = 2 and θ = 15/d 
to generate S = 1
which deﬁnes a rather difﬁcult clustering problem with a hardly visible structure in D as can be
seen in the left part of Fig. 3. We compared the method to three different hierarchical clustering
strategies (single-linkage  complete-linkage  Ward’s method)  to the standard central Wishart model
using two different normalisations of S (‘WD C’: column normalisation using Sc = QI SQt
I and
‘WD R’: additional row normalisation after embedding Sc using kernel PCA) and to the original
TIWD model. The experiment was repeated 200 times and the quality of the inferred clusters was
measured by the adjusted Rand index w.r.t. the true labels. For the hierarchical methods we report
two different performance values: splitting the tree such that the ‘true’ number k = 3 of clusters is
obtained and computing the best value among all possible splits into [2  n] clusters (‘*.best’ in the
boxplot). The reader should notice that both values are in favour of the hierarchical algorithms  since
neither the true k nor the true labels are used for inferring the clusters in the Wishart-type methods.
From the right part of Fig. 3 we conclude that (i) both ‘naive’ normalisation strategies WD C and
WD R are clearly outperformed by TIWD and fastTIWD (‘fTIWD’ in the boxplot). Signiﬁcance
of pairwise performance differences is measured with a nonparametric Kruskal-Wallis test with a

6

Bonferroni-corrected post-test of Dunn’s type  see the rightmost panel; (ii) the hierarchical methods
have severe problems with high dimensionality and low class separation  and optimising the tree
cutting does not help much. Even Ward’s method (being perfectly suited for spherical clusters) has
problems; (iii) there is no signiﬁcant difference between TIWD and fastTIWD.

Figure 3: Normal distributed toy data. Left half: Partition matrix (top)  distance matrix (bottom)
and 2D-PCA embedding of a dataset drawn from the generative model. Right half: Agreement with
‘true’ labels measured by the adjusted Rand index (left) and outcome of a Kruskal-Wallis/Dunn test
(right). Black squares mean two methods are different at a ‘family’ p-value ≤ 0.05.

Synthetic examples: log-normal clusters.
In a second toy example we explicitly violate underly-
ing model assumptions. For this purpose we sample again 3 clusters in d = 300 dimensions  but now
use a log-normal distribution that tends to produce a high number of ‘atypical’ samples. Note that
such a distribution should not induce severe problems for hierarchical methods when optimising the
Rand index over all possible tree cuttings  since the ‘atypical’ samples are likely to form singleton
clusters while the main structure is still visible in other branches of the tree. This should be partic-
ularly true for Ward’s method  since we still have spherically shaped clusters. As for the fastTIWD
model  we want to test if the prior over partitions is ﬂexible enough to introduce additional singleton
clusters: In the experiment  it performed at least as well as Ward’s method  and clearly outperformed
single- and complete-linkage. We also compared it to the afﬁnity-propagation method (AP)  which 
however  has severe problems on this dataset  even when optimising the input preference parameter
that affects the number of clusters in the partition.

Figure 4: Log-normal distributed toy data. Left: Agreement with ‘true’ labels measured by the
adjusted Rand index. Right: Outcome of a Kruskal-Wallis/Dunn test  analogous to Fig. 3.

Semi-supervised clustering of protein sequences. As large-scale application we present a semi-
supervised clustering example which is an upscaled version of an experiment with protein sequences
presented in [1]. While traditional semi-supervised classiﬁers assume at least one labelled object
per class  our model is ﬂexible enough to allow additional new clusters that have no counterpart
in the subset of labelled objects. We apply this idea on two different databases  one being high
quality due to manual annotation with a stringent review process (SwissProt) while the other contains
automatically annotated proteins and is not reviewed (TrEMBL). The annotations in SwissProt are
used as supervision information resulting in a set of class labels  whereas the proteins in TrEMBL
are treated as unlabelled objects  potentially forming new clusters. In contrast to a relatively small
set of globin sequences in [1]  we extract a total number of 12 290 (manually or automatically)
annotated proteins to have some role in oxygen transport or binding. This set contains a richer class
including  for instance  hemocyanins  hemerythrins  chlorocruorins and erythrocruorins.
The proteins are represented as a matrix of pairwise alignment scores. A subset of 1731 annotated se-
quences is from SwissProt  resulting in 356 protein classes. Among the 10 559 TrEMBL sequences

7

we could identify 23 new clusters which are dissimilar to any SwissProt proteins  see Fig. 5. Most of
the newly identiﬁed clusters contain sequences sharing some rare and speciﬁc properties. In accor-
dance with the results in [1]  we ﬁnd a large new cluster containing ﬂavohemoglobins from speciﬁc
species of funghi and bacteria that share a certain domain architecture composed of a globin domain
fused with ferredoxin reductase-like FAD- and NAD-binding modules. An additional example is a
cluster of proteins with chemotaxis methyl-accepting receptor domain from a very special class of
magnetic bacteria to orient themselves according to earth’s magnetic ﬁeld. The domain architecture
of these proteins involving 6 domains is unique among all sequences in our dataset. Another cluster
contains iron-sulfur cluster repair di-iron proteins that build on a polymetallic system  the di-iron
center  constituted by two iron ions bridged by two sulﬁde ions. Such di-iron centers occur only in
this new cluster.

Figure 5: Partition of all 12 290 proteins into 379 clusters: 356 predeﬁned by sequences from
SwissProt and 23 new formed by sequences from TrEMBL (red box).
In order to gain the above results  5000 Gibbs sweeps were conducted in a total runtime of ≈ 6
hours. Although section 2 highlighted the worst-case complexity of the original TIWD  it is also
important to experimentally compare both models in a real world scenario: we ran 100 sweeps
with each fastTIWD and TIWD and hereby observed an average improvement of factor 192  which
would lead to an estimated runtime of 1152 hours (≈ 50 days) for the latter model. On a side note 
automatic cluster identiﬁcation is a nice example for beneﬁts of large-scale data mining: clearly  one
could theoretically also identify special sequences by digging into various protein domain databases 
but without precise prior knowledge  this would hardly be feasible for ≈ 12 000 proteins.

5 Conclusion

We have presented a new model for partitioning pairwise distance data  which is motivated by the
great success of the TIWD model  shares all its positive properties  and additionally reduces the
computational workload from O(n3) to O(n2) per sweep of the Gibbs sampler. Compared to vecto-
rial representations  pairwise distances do not convey information about translations and rotations of
the underlying coordinate system. While in the TIWD model this lack of information is handled by
making the likelihood invariant against such geometric transformations  here we break this symme-
try by choosing one particular inner-product representation S∗ and thus  one particular coordinate
system. The advantage is being able to use a standard (i.e. central) Wishart distribution for which
we present an efﬁcient Gibbs sampling algorithm.
We show that our construction principle for selecting S∗ among all inner product matrices corre-
sponding to an observed distance matrix D and ﬁnds an optimal candidate if the true covariance was
known. Although it is a pure theoretical guarantee  it is successfully exploited by a simple hierar-
chical cluster method to produce an initial covariance estimate—all without specifying the number
of clusters  which is one of the model’s key properties. On the algorithmic side  we prove that S∗
can be computed in O(n2) time using tree traversals. Assuming the number of Gibbs sweeps nec-
essary is independent of n (which  of course  depends on the problem)  we now have a probabilistic
algorithm for partitioning distance matrices running in O(n2) time. Experiments on simulated data
show that the quality of partitions found is at least comparable to that of the original TIWD. It is
now possible for the ﬁrst time to use the Wishart-Dirichlet process model for large matrices. Our ex-
periment containing ≈ 12 000 proteins shows that fastTIWD can be successfully used to mine large
relational datasets and leads to automatic identiﬁcation of protein clusters sharing rare structural
properties. Assuming that in most clustering problems it is acceptable to obtain a solution within
some hours  any further size increase of the input matrix will become more and more a problem of
memory capacity rather than computation time.

Acknowledgments

This work has been partially supported by the FP7 EU project SIMBAD.

8

References
[1] J. Vogt  S. Prabhakaran  T. Fuchs  and V. Roth. The Translation-invariant Wishart-Dirichlet
Process for Clustering Distance Data. In Proceedings of the 27th International Conference on
Machine Learning  2010.

[2] P. McCullagh and J. Yang. How Many Clusters? Bayesian Analysis  3:101–120  2008.
[3] Y. W. Teh. Dirichlet Processes. In Encyclopedia of Machine Learning. Springer  2010.
[4] J. Sethuraman. A Constructive Deﬁnition of Dirichlet Priors. Statistica Sinica  4:639–650 

1994.

[5] B. A. Frigyik  A. Kapila  and M. R. Gupta.

Introduction to the Dirichlet Distribution and
Related Processes. Technical report  Departement of Electrical Engineering  University of
Washington  2010.

[6] W. Ewens. The Sampling Theory of Selectively Neutral Alleles. Theoretical Population Biol-

ogy  3:87–112  1972.

[7] D. Blei and M. Jordan. Variational Inference for Dirichlet Process Mixtures. Bayesian Analy-

sis  1:121–144  2005.

[8] R. Neal. Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Journal of

Computational and Graphical Statistics  9(2):249–265  2000.

[9] P. McCullagh. Marginal Likelihood for Distance Matrices. Statistica Sinica  19:631–649 

2009.

[10] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear Component Analysis as a Kernel Eigen-

value Problem. Neural Computation  10(5):1299–1319  July 1998.

[11] J.A. Diaz-Garcia  J.R. Gutierrez  and K.V. Mardia. Wishart and Pseudo-Wishart Distributions

and Some Applications to Shape Theory. Journal of Multivariate Analysis  63:73–87  1997.

[12] H. Uhlig. On Singular Wishart and Singular Multivariate Beta Distributions. Annals of Statis-

tics  22:395–405  1994.

[13] M. Srivastava. Singular Wishart and Multivariate Beta Distributions. Annals of Statistics 

31(2):1537–1560  2003.

[14] M. Farach  S. Kannan  and T. Warnow. A Robust Model for Finding Optimal Evolutionary
Trees. In Proceedings of the 25th Annual ACM Symposium on Theory of Computing  pages
137–145  1993.

9

,Shipra Agrawal
Randy Jia