2011,On fast approximate submodular minimization,We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact  for our application  minimum norm can have a running time of about O(n^7 ) (O(n^5 ) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions  the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds  and then repeatedly optimized. We show theoretical properties  and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.,On fast approximate submodular minimization

Stefanie Jegelka†  Hui Lin∗  Jeff Bilmes∗

jegelka@tuebingen.mgp.de {hlin bilmes}@ee.washington.edu

† Max Planck Institute for Intelligent Systems  Tuebingen  Germany

∗ University of Washington  Dept. of EE  Seattle  U.S.A.

Abstract

We are motivated by an application to extract a representative subset of machine
learning training data and by the poor empirical performance we observe of the
popular minimum norm algorithm. In fact  for our application  minimum norm can
have a running time of about O(n7) (O(n5) oracle calls). We therefore propose
a fast approximate method to minimize arbitrary submodular functions. For a
large sub-class of submodular functions  the algorithm is exact. Other submodular
functions are iteratively approximated by tight submodular upper bounds  and then
repeatedly optimized. We show theoretical properties  and empirical results suggest
signiﬁcant speedups over minimum norm while retaining higher accuracies.

1

Introduction

Submodularity has been and continues to be an important property in many ﬁelds. A set function
f : 2V → R deﬁned on subsets of a ﬁnite ground set V is submodular if it satisﬁes the inequality
f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ) for all S  T ⊆ V. Submodular functions include entropy 
graph cuts (deﬁned as a function of graph nodes)  potentials in many Markov Random Fields
[3]  clustering objectives [23] covering functions (e.g.  sensor placement objectives)  and many
more. One might consider submodular functions as being on the boundary between “efﬁciently” 
i.e.  polynomial-time  and “not efﬁciently” optimizable set functions. Submodularity is gaining
importance in machine learning too  but many machine learning data sets are so large that mere
“polynomial-time” efﬁciency is not enough. Indeed  the submodular function minimization (SFM)
algorithms with proven polynomial running time are practical only for very small data sets. An
alternative  often considered to be faster in practice  is the minimum-norm point algorithm [7]. Its
worst-case running time however is still an open question.
Contrary to current wisdom  we demonstrate that for certain functions relevant in practice (see
Section 1.1)  the minimum-norm algorithm has an impractical empirical running time of about
O(n7)  requiring about O(n5) oracle function calls. To our knowledge  and interesting from an
optimization perspective  this is worse than any results reported in the literature  where times of
O(n3.3) were obtained with simpler graph cut functions [22].
Since we found the minimum-norm algorithm to be either slow (when accurate)  or inaccurate (when
fast)  in this work we take a different approach. We view the SFM problem as an instance of a larger
class of problems that includes NP-hard instances. This class admits approximation algorithms  and
we apply those instead of an exact method. Contrary to the possibly poor performance of “exact”
methods  our approximate method is fast  is exact for a large class of submodular functions  and
approximates all other functions with bounded deviation.
Our approach combines two ingredients: 1) the representation of functions by graphs; and 2) a recent
generalization of graph cuts that combines edge-costs non-linearly. Representing functions as graph
cuts is a popular basis for optimization  but cuts cannot efﬁciently represent all submodular functions.
Contrary to previous constructions  including 2) leads to exact representations for any submodular

1

function. To optimize an arbitrary submodular function f represented in our formalism  we construct
a graph-representable tractable submodular upper bound ˆf that is tight at a given set T ⊆ V  i.e. 
ˆf (T ) = f (T )  and ˆf (S) ≥ f (S) for all S ⊆ V. We repeat this “submodular majorization” step and
optimize  in at most a linear number of iterations. The resulting algorithm efﬁciently computes good
approximate solutions for our motivating application and other difﬁcult functions as well.

1.1 Motivating application and the failure of the minimum-norm point algorithm

Our motivating problem is how to empirically evaluate new or expensive algorithms on large data sets
without spending an inordinate amount of time doing so [20  21]. If a new idea ends up performing
poorly  knowing this sooner will avoid futile work. Often the complexity of a training iteration is
linear in the number of samples n but polynomial in the number c of classes or types. For example 
for object recognition  it typically takes O(ck) time to segment an image into regions that each
correspond to one of c objects  using an MRF with non-submodular k-interaction potential functions.
In speech recognition  moreover  a k-gram language model with size-c vocabulary has a complexity
of O(ck)  where c is in the hundreds of thousands and k can be as large as six.
To reduce complexity one can reduce k  but this can be unsatisfactory since the novelty of the
algorithm might entail this very cost. An alternative is to extract and use a subset of the training data 
one with small c. We would want any such subset to possess the richness and intricacy of the original
data while simultaneously ensuring that c is bounded.
This problem can be solved via SFM using the following Bipartite neighborhoods class of submodular
functions: Deﬁne a bipartite graph H = (V U E  w) with left/right nodes V/U  and a modular weight
u∈U w(u). Let the neighborhood of a
set S ⊆ V be N (S) = {u ∈ U : ∃ edge (i  u) ∈ E with i ∈ S}. Then f : 2V → R+  deﬁned as
u∈N (S) w(u)  is non-decreasing submodular. This function class encompasses e.g. set
i∈S Ui| for sets Ui covered by element i. We say f is the submodular

function w : U → R+. A function is modular if w(U ) =(cid:80)
f (S) =(cid:80)
covers of the form f (S) = |(cid:83)
function m : 2V → R+  m(S) = (cid:80)

function induced by modular function w and graph H.
Let U be the set of types in a set of training samples V. More-
over  let w measure the cost of a type u ∈ U (this corresponds
e.g. to the “undesirability” of type u). Deﬁne also a modular
i∈S m(i) as the beneﬁt
of training samples (e.g.  in vision  m(i) is the number of dif-
ferent objects in an image i ∈ V  and in speech  this is the
length of utterance i). Then the above optimization problem
can be solved by ﬁnding argminS⊆V w(N (S)) − λm(S) =
argminS⊆V w(N (S))+λm(V\S) where λ is a tradeoff coefﬁ-
cient. As shown below  this can be easily represented and solved
efﬁciently via graph cuts. In some cases  however  we prefer to
pick certain subclasses of U together. We partition U = U1∪U2
resulting optimization problem is minS⊆V(cid:80)
into blocks  and make it beneﬁcial to pick items from the same block. Beneﬁt restricted to blocks can
arise from non-negative non-decreasing submodular functions g : 2U → R+ restricted to blocks. The
i g(Ui ∩N (S)) + λm(V \ S); the sum over i expresses
(cid:112)
the obvious generalization to a partition into more than just two blocks. Unfortunately  this class of
submodular functions is no longer representable by a bipartite graph  and general SFM must be used.
w(N (S))  the empirical running time of the minimum
With such a function  f (S) = m(S) + 100
norm point algorithm (MN) scales as O(n7)  with O(n5) oracle calls (Figure 1). This rules out large
data sets for our application  but is interesting with regard to the unknown complexity of MN.

Figure 1: Running time of MN

1.2 Background on Algorithms for submodular function minimization (SFM)

The ﬁrst polynomial algorithm for SFM was by Gr¨otschel et al. [13]  with further milestones being the
ﬁrst combinatorial algorithms [15  27] ([22] contains a survey). The currently fastest strongly polyno-
mial combinatorial algorithm has a running time of O(n5T +n6) [24] (where T is function evaluation
time)  far from practical. Thus  the minimum-norm algorithm [7] is often the method of choice.

2

99.51010.546810121416Ground set size (power of 2)CPU time (seconds  power of 2)  min−normO(n7)Luckily  many sub-families of submodular functions permit specialized  faster algorithms. Graph
cut functions fall into this category [1]. They have found numerous applications in computer vision
[2  12]  begging the question as to which functions can be represented and minimized using graph
cuts [9  6  31]. ˘Zivn´y et al. [32] show that cut representations are indeed limited: even when allowing
exponentially many additional variables  not all submodular functions can be expressed as graph cuts.
Moreover  to maintain efﬁciency  we do not wish to add too many auxiliary variables  i.e.  graph
nodes. Other speciﬁc cases of relatively efﬁcient SFM include graphic matroids [25] and symmetric
submodular functions  minimizable in cubic time [26].

A further class of benign functions are those of the form f (S) = ψ((cid:80)

Krause [29] decompose it into a sum of truncated functions of the form f (A) = min{(cid:80)

i∈S w(i)) + m(S) for
nonnegative weights w : V → R+  and certain concave functions ψ : R → R. Fujishige and Iwata
[8] minimize such a function via a parametric max-ﬂow  and we build on their results in Section 4.
However  restrictions apply to the effective number of breakpoints of ψ. Stobbe and Krause [29]
generalize this class to arbitrary concave functions and exploit Nesterov’s accelerated gradient descent.
Whereas Fujishige and Iwata [8] decompose ψ as a minimum of modular functions  Stobbe and
i∈A w(cid:48)(i)  γ}
— this class of functions  however  is also limited. Truncations are expressible by graph cuts  as we
show in Figure 3(b). Thus  if truncations could express any submodular function  then so could
graph cuts  contradicting the results in [32]. This was proven independently in [30]. Moreover  the
formulation itself of some representable functions in terms of concave functions can be challenging.
In this paper  by contrast  we propose a model that is exact for graph-representable functions  and
yields an approximation for all other functions.

2 Representing submodular functions by generalized graph cuts
We begin with the representation of a set function f : 2V → R by a graph
cut  and then extend this to submodular edge weights. Formally  f is
graph-representable if there exists a graph G = (V ∪ U ∪ {s  t} E) with
terminal nodes s  t  one node for each element i in V  a set U of auxiliary nodes
(U can be empty)  and edge weights w : E → R+ such that  for any S ⊆ V:

(cid:88)

e∈δs(S∪U )

f (S) = min

U⊆U w(δ(s ∪ S ∪ U )) = min
U⊆U

w(e).

(1)

Figure 2: max

As an illustrative example  Figure 2 represents the function f (S) = maxi∈S w(i) +(cid:80)

δ(S) is the set of edges leaving S  and δs(S) = δ({s} ∪ S). Recall that any minimal (s  t)-cut
partitions the graph nodes into the set Ts ⊆ V∪U reachable from s and the set Tt = (V∪U)\Ts discon-
nected from s. That means  f (S) equals the weight of the minimum (s  t)-cut that assigns S to Ts and
V \ S to Tt  and the auxiliary nodes to achieve the minimum. The nodes in U act as auxiliary variables.
j∈V\S m(j)
for two elements V = {1  2} and w(2) > w(1)  using one auxiliary node u. For any query set S  u
might be joined with S (u ∈ Ts) or not (u ∈ Tt). If S = {1}  then w(δs({1  u})) = m(2) + w(2) 
and w(δs({1})) = m(2) + w(1) = f (S) < w(δs({1  u})). If S = {1  2}  then w(δs({1  2  u})) =
w(2) < w(δs({1  2})) = w(1) + w(2)  and indeed f (S) = w(2). The graph representation (1) leads
to the equivalence between minimum cuts and the minimizers of f:
Lemma 1. Let S∗ be a minimizer of f  and let U∗ ∈ argminU⊆U w(δs(S∗∪U )). Then the boundary
δs(S∗ ∪ U∗) ⊆ E is a minimum cut in G.
The lemma (proven in [18]) is good news since minimum cuts can be computed efﬁciently. To derive
s ⊆ V ∪ U that
S∗ from a minimum cut  recall that any minimum cut is the boundary of some set T ∗
is still reachable from s after cutting. Then S∗ = T ∗
t . A large
sub-family of submodular functions can be expressed exactly in the form (1)  but possibly with an
exponentially large U. For efﬁciency  the size of U should remain small. To express any submodular
function with few auxiliary nodes  in this paper we extend Equation (1) as is seen below.
Unless the submodular function f is already a graph cut function (and directly representable)  we ﬁrst
decompose f into a modular function and a nondecreasing submodular function  and then build up
the graph part by part. This accounts for any graph-representable component of f. To approximate
the remaining component of the function that is not exactly representable  we use submodular costs
on graph edges (in contrast with graph nodes)  a construction that has been introduced recently in

s and (V \ S∗) ⊆ T ∗

s ∩ V  so S∗ ⊆ T ∗

3

ts12um(1)m(2)w(1)w(2)w(2)(a) maximum

(b) truncation

(c) partition matroid

(d) bipartite

(e) bipartite & truncation (f) basic submodular con-

struction

Figure 3: Example graph constructions. Dashed blue edges can have submodular weights; aux-
iliary nodes are white and ground set nodes are shaded. The bipartite graph can have arbitrary
representations between U and t  3(e) is one example. (All ﬁgures are best viewed in color.)

computer vision [16]. We ﬁrst introduce a relevant decomposition result by Cunningham [4]. A
polymatroid rank function is totally normalized if f (V \ i) = f (V) for all i ∈ V. The marginal costs
are deﬁned as ρf (i|S) = f (S ∪ {i}) − f (S) for all i ∈ V \ S.
Theorem 1 ([4  Thm. 18]). Any submodular function f can be decomposed as f (S) = m(S) + g(S)
into a modular function m and a totally normalized polymatroid rank function g. The components

i∈A ρf (i|V \ i) and g(S) = f (S) − m(S) for all S ⊆ V.

are deﬁned as m(S) =(cid:80)
weights: since m(V) is constant  minimizing m(S) =(cid:80)

We may assume that m(i) < 0 for all i ∈ V. If m(i) ≥ 0 for any i ∈ V  then diminishing marginal
costs  a property of submodular functions  imply that we can discard element i immediately [5  18].
To express such negative costs in a graph cut  we point out an equivalent formulation with positive
i∈S m(i) is equivalent to minimizing the
shifted function m(S) − m(V) = −m(V \ S). Thus  we instead minimize the sum of positive
weights on the complement of the solution. We implement this shifted function in the graph by adding
an edge (s  i) with nonnegative weight −m(i) for each i ∈ V. Every element j ∈ Tt (i.e.  j /∈ S) that
is not selected must be separated from s  and the edge (s  j) contributes −m(j) to the total cut cost.
Having constructed the modular part of the function f by edges (s  i) for all i ∈ V  we address
its submodular part g. If g is a sum of functions  we can add a subgraph for each function. We
begin with some example functions that are explicitly graph-representable with polynomially many
auxiliary nodes U. The illustrations in Figure 3 include the modular part m as well.
Maximum. The function g(S) = maxi∈S w(i) for nonnegative weights w is an extension of
Figure 2. Without loss of generality  we assume the elements to be ordered by weight  so that
w(1) ≤ w(2) ≤ . . . w(n). We introduce n−1 auxiliary nodes uj  and connect them to form an imbal-
anced tree with leaves V  as illustrated in Figure 3(a). The minimum way to disconnect a set S from
t is to cut the single edge (uj−1  uj) with weight w(j) of the largest element j = argmaxi∈S w(i).
Truncations. Truncated functions f (S) = min{w(S)  γ} for w  γ ≥ 0 can be modeled by one
extra variable  as shown in Figure 3(b). If w(S) > γ  then the minimization in (1) puts u in
Ts and cuts the γ-edge. This construction has been successfully used in computer vision [19].
Truncations can model piecewise linear concave functions of w(S) [19  29]  and also represent
negative terms in a pseudo-boolean polynomial [18]. Furthermore  these functions include rank
functions g(S) = min{|S|  k} of uniform matroids  and rank functions of partition matroids. If V
is partitioned into groups G ⊂ V  then the rank of the associated partition matroid counts the number
of groups that S intersects: f (S) = |{G|G ∩ S (cid:54)= ∅}| (Fig. 3(c)).
(cid:80)
Bipartite neighborhoods. We already encountered bipartite submodular functions f (S) =
u∈N (S) w(u) in Section 1.1. The bipartite graph that deﬁnes N (S) is part of the representa-

4

st-m(1)-m(n)w(1)w(2)w(n)w(2)w(n)w(3)Vst-m(1)-m(n)w(1)w(n)γVst-m(1)-m(n)VU11111st-m(1)-m(n)V∞∞∞∞st-m(1)-m(n)V∞∞∞∞st-m(1)-m(n)Vtion shown in Figure 3(d)  and its edges get inﬁnite weight. As a result  if S ∈ Ts  then all neighbors
N (S) of S must also be in Ts  and the edges (u  t) for all u ∈ N (S) are cut. Each u ∈ U has such
an edge (u  t)  and the weight of that edge is the weight w(u) of u.
Additional examples are given in [18].
Of course  all the above constructions can also be applied to subsets Q ⊂ V of nodes. In fact  the
decomposition and constructions above permit us to address arbitrary sums and restrictions of such
graph-representable functions. These example families of functions already cover a wide variety of
functions needed in applications. Minimizing a graph-represented function is equivalent to ﬁnding
the minimum (s  t)-cut  and all edge weights in the above are nonnegative. Thus we can use any
efﬁcient min-cut or max-ﬂow algorithm for any of the above functions.

2.1 Submodular edge weights

Next we address the generic case of a submodular function that is not (efﬁciently) graph-representable
or whose functional form is unknown. We can still decompose this function into a modular part m
and a polymatroid g. Then we construct a simple graph as shown in Figure 3(f). The representation
of m is the same as above  but the cost of the edges (i  t) will be charged differently. Instead of
a sum of weights  we deﬁne the cost of a set of these edges to be a non-additive function on sets
of edges  a polymatroid rank function. Each edge (i  t) is associated with exactly one ground set
element i ∈ V  and selecting i (i ∈ Ts) is equivalent to cutting the edge (i  t). Thus  the cost of edge
(i  t) will model the cost g(i) of its element i ∈ V. Let Et be the set of such edges (i  t)  and denote 
for any subset C ⊆ Et the set of ground set elements adjacent to C by V (C) = {i ∈ V|(i  t) ∈ C}.
Equivalently  C is the boundary of V (C) in Et: δs(V (C)) ∩ Et = C. We deﬁne the cost of C to be
the cost of its adjacent ground set elements  hg(C) (cid:44) g(V (C)); this implies hg(δs(S ∩ Et)) = g(S).
The equivalent of Equation (1) becomes

f (S) = min

U⊆U w(δs(S ∪ U ) \ Et) + hg(δs(S ∪ U ) ∩ Et) = −m(V \ S) + g(S) 

(2)
with U = ∅ in Figure 3(f). This generalization from the standard sum of edge weights to a nondecreas-
ing submodular function permits us to express many more functions  in fact any submodular function
[5]. Such expressiveness comes at a price  however: in general  ﬁnding a minimum (s  t)-cut with
such submodular edge weights is NP-hard  and even hard to approximate [17]. The graphs here that
represent submodular functions correspond to benign examples that are not NP-hard. Nevertheless 
we will use an approximation algorithm that applies to all such non-additive cuts. We describe the
algorithm in Section 3. For the moment  we assume that we can handle submodular costs on edges.
The simple construction in Figure 3(f) itself corresponds to a general submodular function mini-
mization. It becomes powerful when combined with parts of f that are explicitly representable. If g
decomposes into a sum of graph-representable functions and a (nondecreasing submodular) remainder
gr  then we construct a subgraph for each graph-representable function  and combine these subgraphs
with the submodular-edge construction for gr. All the subgraphs share the same ground set nodes V.
In addition  we are in no way restricted to separating graph-representable and general submodular
functions. The cost function in our application is a submodular function induced by a bipartite graph
H = (V U E). Let  as before  N (S) be the neighborhood of S ⊆ V in U. Given a nondecreasing
submodular function gU : 2U → R+ on U  the graph H deﬁnes a function g(S) = gU (N (S)). If
gU is nondecreasing submodular  then so is g [28  §44.6 g]. For any such function  we represent H
explicitly in G  and then add submodular-cost edges from U to t with hg(δs(N (S))) = gU (N (S)) 
as shown in Figure 3(d). If gU is itself exactly representable  then we add the appropriate subgraph
instead (Figure 3(e)).

3 Optimization

To minimize a function f  we ﬁnd a minimum (s  t)-cut in its representation graph. Algorithm 1
applies to any submodular-weight cut; this algorithm is exact if the edge costs are modular (a sum of
weights). In each iteration  we approximate f by a function ˆf that is efﬁciently graph-representable 
and minimize ˆf instead. In this section  we switch from costs f  ˆf of node sets S  T to equivalent
costs w  h of edge sets A  B  C and back.

5

Algorithm 1: Minimizing graph-based approximations.
create the representation graph G = (V ∪ U ∪ {s  t} E) and set S0 = T0 = ∅;
for i = 1  2  . . . do

compute edge weights νi−1 = νδs(Ti−1) (Equation 4);
ﬁnd the (maximal) minimum (s  t)-cut Ti = argminT ⊆ (V∪U ) νi−1(δsT );
if f (Ti) = f (Ti−1) then
return Si = Ti ∩ V;
end

end

The approximation ˆf arises from the cut representation constructed in Section 2: we replace the
exact edge costs by approximate modular edge weights ν in G. Recall that the representation G has
two types of edges: those whose weights w are counted as the usual sum  and those charged via a
submodular function hg derived from g. We denote the latter set by Et  and the former by Em. For
any e ∈ Em  we use the exact cost ν(e) = w(e). The submodular cost hg of the remaining edges is
upper bounded by referring to a ﬁxed set B ⊆ E that we specify later. For any A ⊆ Et  we deﬁne

ˆhB(A) (cid:44) hg(B) +

ρh(e|Et \ e) ≥ hg(A).

(3)

(cid:88)

ρh(e|B ∩ Et) − (cid:88)

e∈A\B

e∈B\A

νB(e) = ρh(e|B ∩ Et)

This inequality holds thanks to diminishing marginal costs  and the approximation is tight at B 
ˆhB(B) = hg(B). Up to a constant shift  this function is equivalent [16] to the edge weights:
if e ∈ B ∩ Et.

(4)
Plugging νB into Equation (2) yields an approximation ˆf of f. In the algorithm  B is always the
boundary B = δs(T ) of a set T ⊆ (V ∪ U). Then G with weights νB represents
ˆf (S) = min

νB(e) = ρh(e|Et \ e)

if e ∈ Et \ B;

and

U⊆U νB(δs(S ∪ U ) ∩ Em) + νB(δs(S ∪ U ) ∩ Et)
U⊆U w(δs(S ∪ U ) ∩ Em) +

(cid:88)

= min

(u t)∈δs(S∪U )∩B

ρg(u|V ∪ U \ u) +

ρg(u|T ).

(u t)∈δs(S∪U )\B

(cid:88)

Here  we used the deﬁnition hg(C) (cid:44) g(V (C)). Importantly  the edge weights νB are always
nonnegative  because  by Theorem 1  g is guaranteed to be nondecreasing. Hence  we can efﬁciently
minimize ˆf as a standard minimum cut. If in Algorithm 1 there is more than one set T deﬁning a
minimum cut  then we pick the largest (i.e.  maximal) such set. Lemma 2 states properties of the Ti.
Lemma 2. Assume G is any of the graphs in Figure 3  and let T ∗ ⊆ V∪U be the maximal set deﬁning
a minimum-cost cut δs(T ∗) in G  so that S∗ = T ∗ ∩ V is a minimizer of the function represented by
G. Then  in any iteration i of Algorithm 1  it holds that Ti−1 ⊆ Ti ⊆ T ∗. In particular  S ⊆ S∗ for
the returned solution S.

Lemma 2 has three important implications. First  the algorithm never picks any element outside the
maximal optimal solution. Second  because the Ti are growing  there are at most |T ∗| ≤ |V ∪ U|
iterations  and the algorithm is strongly polynomial. Finally  the chain property permits more efﬁcient
implementations. The proof of Lemma 2 relies on the deﬁnition of ν and submodularity [18].
Moreover  the weights ν lead to a bound the worst-case approximation factor [18].

3.1

Improvement via summarizations

cost ρh(A|δsTi) is much smaller than the estimated sum of weights  νi(A) = (cid:80)

The approximation ˆf is loosest if the sum of edge weights νi(A) signiﬁcantly overestimates the true
joint cost hg(A) of sets of edges A ⊆ δsT ∗ \ δsTi still to be cut. This happens if the joint marginal
e∈A ρh(e|δsTi).
Luckily  many of the functions that show this behavior strongly resemble truncations. Thus  to tighten
the approximation  we summarize the joint cost of groups of edges by a construction similar to
Figure 3(b). Then the algorithm can take larger steps and pick groups of elements.
We partition Et into disjoint groups Gk of edges (u  t). For each group  we introduce an auxiliary
node tk and re-connect all edges (u  t) ∈ Gk to end in tk instead of t. Their cost remains the

6

same. An extra edge ek connects tk to t  and carries the joint weight νi(ek) of all edges in Gk;
a tighter approximation. The weight νi(ek) is also adapted in each iteration.
Initially  we set
ν0(ek) = hg(Gk) = g(V (Gk)). Subsequent approximations νi refer to cuts δsTi  and such a cut can
contain either single edges from Gk  or the group edge ek. We set the next reference set Bi to be a
copy of δsTi in which each group edge ek was replaced by all its group members Gk. The joint group
νi(e).

weight νi(ek) for any k is then νi(ek) = ρh(Gk \ Bi|Bi) +(cid:80)

e∈Gk

e∈Gk∩Bi ρh(e|Et \ e) ≤(cid:80)
ρh(e|B)− (cid:88)

ρh(e|Et\ e) ≤ ˆh(A) 

e∈(Gk∩A)\B Gk(cid:54)⊆A

e∈B\A

Formally  these weights represent the upper bound
(cid:48)
ˆh
B(A) = hg(B) +

ρh(Gk\ B|B) +

(cid:88)

(cid:88)

Gk⊆A

where we replace Gk by ek whenever Gk ⊆ A. In our experiments  this summarization helps improve
the results while simultaneously reducing running time.

pick the best range. For this construction  g must have the form g(U ) = ψ((cid:80)
˜w(U ) =(cid:80)

4 Parametric constructions for special cases
For certain functions of the form f (S) = m(S) + g(N (S))  the graph representation in Figure 3(d)
admits a speciﬁc algorithm. We use approximations that are exact on limited ranges  and eventually
u∈U ˜w(u)) for
weights ˜w ≥ 0 and one piecewise linear  concave function ψ with a small (polynomial) number
(cid:96) of breakpoints. Alternatively  ψ can be any concave function if the weights ˜w are such that
u∈U ˜w(u) can take at most polynomially many distinct values xk; e.g.  if ˜w(u) = 1 for
all u  then effectively (cid:96) = |U| + 1 by using the xk as breakpoints and interpolating. In all these cases 
ψ is equivalent to the minimum of at most (cid:96) linear (modular) functions.
We build on the approach in [8]  but  whereas their functions are deﬁned on V  g here is deﬁned on U.
Contrary to their functions and owing to our decomposition  the ψ here is nondecreasing. We deﬁne (cid:96)
linear functions  one for each breakpoint xk (and use x0 = 0):

ψk(t) = (ψ(xk) − ψ(xk−1))(t − xk) + ψ(xk) = αkt + βk.

(5)

The ψk are deﬁned such that ψ(t) = mink ψk(t). Therefore  we approximate f by a series ˆfk(S) =
−m(V \ S) + ψk( ˜w(N (S)))  and ﬁnd the exact minimizer Sk for each k. To compute Sk via a
minimum cut in G (Fig. 3(d))  we deﬁne edge weights νk(e) = w(e) for edges e /∈ Et as in Section 3 
and νk(u  t) = αk ˜w(u) for e ∈ Et. Then Tk = Sk ∪ N (Sk) deﬁnes a minimum cut δsTk in G. We
compute ˆfk(Sk) = νk(δsTk) + βk + m(V); the optimal solution is the Sk with minimum cost ˆfk(Sk).
This method is exact. To solve for all k within one max-ﬂow  we use a parametric max-ﬂow method
[10  14]. Parametric max-ﬂow usually works with both edges from s and to t. Here  νk ≥ 0 because
ψ is nondecreasing  and thus we only need t-edges which already exist in the bipartite graph G.
This method is limited to few breakpoints. For more general concave ψ and arbitrary ˜w ≥ 0  we
can approximate ψ by a piecewise linear function. Still  the parametric approach does not directly
i gi(U ∩ Wi) for sets Wi ⊆ U. In contrast 
Algorithm 1 (with the summarization) can handle all of these cases. We point out that without
indirection via the bipartite graph  i.e.  f (S) = m(S) + ψ(w(S)) for a ψ with few breakpoints  we
can minimize f very simply: The solution for ψk includes all j ∈ V with αk ≤ −m(j)/w(j). The
advantage of the graph cut is that it easily combines with other objectives.

generalize to more than one nonlinearity  e.g.  g(U ) =(cid:80)

5 Experiments

In the experiments  we test whether the graph-based methods improve over the minimum-norm point
algorithm in the difﬁcult cases of Section 1.1. We compare the following methods:
MN: a re-implementation of the minimum norm point algorithm in C++ that is about four times
faster than the C code in [7] (see [18])  ensuring that our results are not due to a slow implementation;
MC: a minimum cut with static edge weights ν(e) = hg(e);
GI: the graph-based iterative Algorithm 1  implemented in C++ with the max-ﬂow code of [3]  (i) by
generated by sorting the edges in Et by their weights hg(e)  and then forming groups Gk of edges
adjacent in the order such that for each e ∈ Gk  hg(e) ≤ 1.1hg(Gk) (GIs);

itself; (ii) with summarization via(cid:112)|Et| random groups (GIr); (iii) with summarization via groups

7

Figure 4: (a) Running time  (b) relative and (c) absolute error with varying λ for a data set as described
in Section 1.1  |V| = 54915  |U| = 6871  and f (S) = −m(S) + λ
show absolute errors. (d) Running times with respect to |V|  f (S) = −m(S) + λ

w(N (S)).

(cid:112)|N (S)|. Where f (S∗) = 0  we

(a)

(b)

(c)

(d)

(cid:112)

(cid:112)

GP: the parametric method from Section 4  using |Et| equispaced breakpoints; based on C code
from RIOT1.
We also implemented the SLG method from [29] in C++ (public code is not available)  but found
it to be impractical on the problems here  as gradient computation of our function requires ﬁnding
gradients of |U| truncation functions  which is quite expensive [18]. Thus  we did not include it in the
tests on the large graphs. We use bipartite graphs of the form described in Section 1.1  with a cost
function f (S) = m(S) + λg(N (S)). The function g uses a square root  g(U ) =
w(U ). More
results  also on other functions  can be found in [18].
Solution quality with solution size. Running time and results depend on the size of S∗. Thus  we
vary λ from 50 (S∗ ≈ V) to 9600 (S∗ = ∅) on a speech recognition data set [11]. The bipartite
graph represents a corpus subset extraction problem (Section 1.1) and has |V| = 54915  |U| = 6871
nodes  and uniform weights w(u) = 1 for all u ∈ U. The results look similar with non-uniform
weights  but for uniform weights the parametric method from Section 4 always ﬁnds the optimal
solution and thus allows us to report errors. Figure 4 shows the running times and the relative error
err(S) = |f (S) − f (S∗)|/|f (S∗)| (note that f (S∗) ≤ 0). If f (S∗) = 0  we report absolute errors.
Because of the large graph  we used the minimum-norm algorithm with accuracy 10−5. Still  it
takes up to 100 times longer than the other methods. It works well if S∗ is large  but as λ grows 
its accuracy becomes poor. In particular when f (S∗) = f (∅) = 0  it returns large sets with large
positive cost. In contrast  the deviation of the approximate edge weights νi from the true cost is
bounded [18]. All algorithms except MN return an optimal solution for λ ≥ 2000. Updating the
weights ν clearly improves the performance of Algorithm 1  as does the summarization (GIr/GIs
perform identically here). With the latter  the solutions are very often optimal  and almost always
very good.
Scaling: To test how the methods scale with the size |V|  we sample small graphs from the big
graph  and report average running times across 20 graphs for each size. As the graphs have non-
w(U ) by
a piecewise linear function with |U| breakpoints. All algorithms ﬁnd the same (optimal) solution.
Figure 4(d) shows that the minimum-norm algorithm with high accuracy is much slower than the
other methods. Empirically  MN scales as up to O(n5) (note that Figure 1 is a speciﬁc worst-case
graph)  the parametric version approximately O(n2)  and the variants of GI up to O(n1.5).
Acknowledgments: This material is based upon work supported in part by the National Science
Foundation under grant IIS-0535100  by an Intel research award  a Microsoft research award  and a
Google research award.

uniform weights  we use GP as an approximation method and estimate the nonlinearity(cid:112)

References
[1] R. K. Ahuja  T. L. Magnanti  and J. B. Orlin. Network Flows. Prentice Hall  1993.
[2] Y. Boykov and M.-P. Jolly. Interactive graph cuts for optimal boundary and region segmentation of objects

in n-d images. In ICCV  2001.

1http://riot.ieor.berkeley.edu/riot/Applications/Pseudoflow/parametric.html

8

2000400060008000101102103λlog time (s)  5001000150020000246rel errorλ  MN 1e−5MCGIGIrGIsGP40006000800002000400060008000abs errorλ  10310−2100102104time (s)log n  MN 1e−10MCGIGIrGIsGPtime (s)log n[3] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. IEEE TPAMI  26(9):1124–1137  2004.

[4] W. H. Cunningham. Decomposition of submodular functions. Combinatorica  3(1):53–68  1983.
[5] W. H. Cunningham. Testing membership in matroid polyhedra. J Combinatorial Theory B  36:161–188 

1984.

[6] D. Freedman and P. Drineas. Energy minimization via graph cuts: Settling what is possible. In CVPR 

2005.

[7] S. Fujishige and S. Isotani. A submodular function minimization algorithm based on the minimum-norm

base. Paciﬁc Journal of Optimization  7:3–17  2011.

[8] S. Fujishige and S. Iwata. Minimizing a submodular function arising from a concave function. Discrete

Applied Mathematics  92  1999.

[9] S. Fujishige and S. B. Patkar. Realization of set functions as cut functions of graphs and hypergraphs.

Discrete Mathematics  226:199–210  2001.

[10] G. Gallo  M.D. Grigoriadis  and R.E. Tarjan. A fast parametric maximum ﬂow algorithm and applications.

SIAM J Computing  18(1)  1989.

[11] J.J. Godfrey  E.C. Holliman  and J. McDaniel. Switchboard: Telephone speech corpus for research and

development. In Proc. ICASSP  volume 1  pages 517–520  1992.

[12] D. M. Greig  B. T. Porteous  and A. H. Seheult. Exact maximum a posteriori estimation for binary images.

Journal of the Royal Statistical Society  51(2)  1989.

[13] M. Gr¨otschel  L. Lov´asz  and A. Schrijver. The ellipsoid algorithm and its consequences in combinatorial

optimization. Combinatorica  1:499–513  1981.

[14] D. Hochbaum. The pseudoﬂow algorithm: a new algorithm for the maximum ﬂow problem. Operations

Research  58(4)  2008.

[15] S. Iwata  L. Fleischer  and S. Fujishige. A combinatorial strongly polynomial algorithm for minimizing

submodular functions. J. ACM  48:761–777  2001.

[16] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In

CVPR  2011.

[17] S. Jegelka and J. Bilmes. Approximation bounds for inference using cooperative cuts. In ICML  2011.
[18] S. Jegelka  H. Lin  and J. Bilmes. Fast approximate submodular minimization: Extended version  2011.
[19] P. Kohli  L. Ladick´y  and P. Torr. Robust higher order potentials for enforcing label consistency. Int. J.

Computer Vision  82  2009.

[20] H. Lin and J. Bilmes. An application of the submodular principal partition to training data subset selection.

In NIPS workshop on Discrete Optimization in Machine Learning  2010.

[21] H. Lin and J. Bilmes. Optimal selection of limited vocabulary speech corpora. In Proc. Interspeech  2011.
[22] S. T. McCormick. Submodular function minimization. In K. Aardal  G. Nemhauser  and R. Weismantel 
editors  Handbook on Discrete Optimization  pages 321–391. Elsevier  2006. updated version 3a (2008).

[23] M. Narasimhan  N. Jojic  and J. Bilmes. Q-clustering. In NIPS  2005.
[24] J. B. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Mathemat-

ical Programming  118(2):237–251  2009.

[25] M. Preissmann and A. Seb˝o. Research Trends in Combinatorial Optimization  chapter Graphic Submodular

Function Minimization: A Graphic Approach and Applications  pages 365–385. Springer  2009.

[26] M. Queyranne. Minimizing symmetric submodular functions. Mathematical Programming  82:3–12  1998.
[27] A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. J.

Combin. Theory Ser. B  80:346–355  2000.

[28] A. Schrijver. Combinatorial Optimization. Springer  2004.
[29] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In NIPS  2010.
[30] J. Vondr´ak. personal communication  2011.
[31] S. ˘Zivn´y and P.G. Jeavons. Classes of submodular constraints expressible by graph cuts. Constraints  15:

430–452  2010. ISSN 1383-7133.

[32] S. ˘Zivn´y  D. A. Cohen  and P. G. Jeavons. The expressive power of binary submodular functions. Discrete

Applied Mathematics  157(15):3347–3358  2009.

9

,Charlie Tang
Russ Salakhutdinov
Xiaolong Wang
Liliang Zhang
Liang Lin
Zhujin Liang
Chicheng Zhang
Jimin Song
Kamalika Chaudhuri
Kevin Chen
Boris Hanin
David Rolnick