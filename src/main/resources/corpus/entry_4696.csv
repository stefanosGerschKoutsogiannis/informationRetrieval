2018,Representation Learning of Compositional Data,We consider the problem of learning a low dimensional representation for compositional data. Compositional data consists of a collection of nonnegative data that sum to a constant value. Since the parts of the collection are statistically dependent  many standard tools cannot be directly applied. Instead  compositional data must be first transformed before analysis. Focusing on principal component analysis (PCA)  we propose an approach that allows low dimensional representation learning directly from the original data. Our approach combines the benefits of the log-ratio transformation from compositional data analysis and exponential family PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem  that relates the perspective transform of a Bregman divergence to the Bregman divergence of a perspective transform and a remainder conformal divergence. Our proposed approach includes a convenient surrogate (upper bound) loss of the exponential family PCA which has an easy to optimize form. We also derive the corresponding form for nonlinear autoencoders. Experiments on simulated data and microbiome data show the promise of our method.,Representation Learning of Compositional Data

Marta Avalos-Fernandez† Richard Nock‡§(cid:92) Cheng Soon Ong‡§ Julien Rouar† Ke Sun‡ ∗

†Université de Bordeaux  ‡Data61 

§the Australian National University and (cid:92)the University of Sydney

first.last@{u-bordeaux.fr data61.csiro.au}

Abstract

We consider the problem of learning a low dimensional representation for compo-
sitional data. Compositional data consists of a collection of nonnegative data that
sum to a constant value. Since the parts of the collection are statistically dependent 
many standard tools cannot be directly applied. Instead  compositional data must
be ﬁrst transformed before analysis. Focusing on principal component analysis
(PCA)  we propose an approach that allows low dimensional representation learn-
ing directly from the original data. Our approach combines the beneﬁts of the
log-ratio transformation from compositional data analysis and exponential family
PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem 
that relates the perspective transform of a Bregman divergence to the Bregman
divergence of a perspective transform and a remainder conformal divergence. Our
proposed approach includes a convenient surrogate (upper bound) loss of the ex-
ponential family PCA which has an easy to optimize form. We also derive the
corresponding form for nonlinear autoencoders. Experiments on simulated data
and microbiome data show the promise of our method.

1

Introduction

Compositional data analysis (CoDA) is a subﬁeld of statistics introduced more than three decades ago
[3  2  1  29]. Compositional data consist of a collection of nonnegative measurements that sum to a
constant value  typically  proportions that sum to 1. Because knowing the sum  one component can be
determined from the sum of the remainder  the parts that make up the composition are mathematically
and statistically dependent. This distinct structure complicates analysis and does not allow standard
statistical analyses. Ignoring the underlying nature of the data studied might give rise to misleading
conclusions.
Among others  [1] and [13] provided a framework to perform CoDA by mapping data from the
constrained simplex space to the Euclidian space using nonlinear log-ratio transforms. In this paper 
we focus on Principal Components Analysis (PCA)  one of the main tools for exploratory analysis of
compositional data. Just like in standard Euclidean data  it is particularly useful when the ﬁrst few
principal components explain enough variability to be considered as representative. Unfortunately 
any operation of centering or scaling destroys the compositional nature of the data  which complicates
a direct application of PCA.
Our motivation for studying CoDA comes from the recent explosion of microbiome studies [14  15].
Indeed  spectacular advances in 16S rRNA gene sequencing of the bacterial component of the human
microbial community (microbiota) have enabled researchers to investigate human health and disease 
leading to new insights into the role of these microbial communities. The microbiota sequencing data
are measured as read counts interpreted as a species’ abundance in a microbial community. To make
the microbial abundance comparable across samples  data are normalized to the relative abundances

∗Authors in alphabetical order

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: 2D Visualization of the low dimensional representation A on the arms dataset.

of all bacteria observed. On the other hand  because high-throughput experiments produce large
amounts of data  multivariate analysis is indispensable [27  21]. There is a stress to understand the
soundness of models [5].
In this paper  we propose to learn a low dimensional representation of CoDA from the original data.
To account for the nonlinearity due to the compositional nature of the data  we start from exponential
family PCA [12] that we augment with the compositional constraint and then simplify the loss to
be optimized via a generalization of a recent result [25] on Bregman divergences  which may be
of independent interest. We also propose a nonlinear autoencoder (AE) version to learn the low
dimensional representation.
Let us examine a toy example to illustrate our approach. We generate the arms dataset in S19 by
evenly interpolating the simplex center and each of the 20 vertices with 100 points  therefore yielding
a matrix X20×2000. Figure 1 shows the 2D representation A2×2000 computed by ﬁve methods: the
standard PCA; clr-PCA computes the standard PCA after performing clr; CoDA-PCA and CoDA-AE
are our proposed methods; t-SNE [32] is a popular nonlinear dimensionality reduction method and
is applied on X directly. In the PCA plot  the black segments indicate that the PCA reconstruction
is outside of the simplex. PCA cannot be directly adapted to CoDA because the projection on
the principal components may go beyond the convex hull of the vertices.
It is clear that only
CoDA-PCA and CoDA-AE uncover the true structure  where all the arms are clearly presented  and
their connections are faithfully presented.

2 Compositional Data Analysis

Sd = (cid:8)x ∈ (cid:60)d : ∀j  xj > 0; (cid:80)d

j=1 xj = κ(cid:9)  where κ > 0 is a constant  classically 1. Here
We brieﬂy review some deﬁnitions of CoDA. Compositional data are proportions: X is a compositional
dataset if and only if X ∈ (cid:60)d×m such that ∀i ∈ [1  ...  m] the vector column xi of X is in the simplex
the superscript d does not denote the dimensionality as dim(Sd) = d − 1. For a dataset X(cid:48) which
contains counts of strictly positive values  we reduce it to a compositional dataset by dividing out the
totals  that is we compute the CoDA set X such that: xi = x(cid:48)
is the vector of proportions
for individual i.
Using Bregman divergences makes explicit a dual afﬁne [6] coordinate space which is in fact the log
coordinates of Aitchison [1]. It is in this space that we have afﬁne constraints  which are therefore
non-linear in the "primal"  ambient space. To manage this nonlinear structure  it has been proposed [4]
to ﬁrst apply a log-ratio transformation to transpose the data into real Euclidean space. For instance 
the additive log-ratio transformation (alr) applies log-ratio between a component and a reference
component; the centered log-ratio transformation (clr) scales each subject vector by its geometric
mean; and the isometric log-ratio transformation (ilr) is associated with an orthogonal coordinate
system in the simplex. Afterwards  standard PCA is performed.
By deﬁnition  the clr transformation is

1(cid:80)d
j=1 x(cid:48)

ji

i

where g(x) = ((cid:81)d

cKL(x) = log

= Cclr log(x) = log(x) − log(x)1d 

(1)

j=1 xj)1/d the geometric mean of x  x = 1

1d is the 1× d vector of all ones  and Cclr = Id − 1

d 1d1

j=1 xj is the arithmetic mean of x 
(cid:124)
d. The purpose of the log-ratio transformation

d

(cid:18) x

(cid:19)

g(x)

(cid:80)d

2

−0.45400.4583#comp1−0.43200.4471#comp212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322430130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433540140240340440540640740840941041141241341441541641741841942042142242342442542642750150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753860160260360460560660760860961061161261361461561661761861962062162262362470170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275380180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695710011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158115911601161116211631164116512011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223130113021303130413051306130713081309131013111312131313141315131613171318131913201321132213231324132513261327132813291330133113321333133413351336133713381339134013411401140214031404140514061407140814091410141114121413141414151416141714181419142014211422142314241425142614271428142914301431143214331434143514361437143814391440144114421443144415011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153316011602160316041605160616071608160916101611161216131614161516161617161816191620162116221623162416251626162716281629163016311632163316341635163616371638163916401641164216431644164516461647164816491650165116521653165416551656165716581659166016611662166316641665166616671668166916701671167216731674167516761677167816791680168116821683168416851686168716881689169016911692169316941695169616971698169917001701170217031704170517061707170817091710171117121713171417151716171717181719172017211722172317241725172617271728172917301731173217331801180218031804180518061807180818091810181118121813181418151816181718181819182018211822182318241825182618271828182918301831183218331834183518361837183818391840184118421843184418451846184718481849185018511852185318541855185618571858185918601861186218631864186518661867186818691870187118721873187418751876187718781879188018811882188319011902190319041905190619071908190919101911191219131914191519161917191819191920192119221923PCA−4.7533.976#comp1−4.936.08#comp212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862186318641865186618671868186918701871187218731874187518761877187818791880188118821883188418851886188718881889189018911892189318941895189618971898189919001901190219031904190519061907190819091910191119121913191419151916191719181919192019211922192319241925192619271928192919301931193219331934193519361937193819391940194119421943194419451946194719481949195019511952195319541955195619571958195919601961196219631964196519661967196819691970197119721973197419751976197719781979198019811982198319841985198619871988198919901991199219931994199519961997199819992000clr-PCA−26.227.3#comp1−27.1925.59#comp212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862186318641865186618671868186918701871187218731874187518761877187818791880188118821883188418851886188718881889189018911892189318941895189618971898189919001901190219031904190519061907190819091910191119121913191419151916191719181919192019211922192319241925192619271928192919301931193219331934193519361937193819391940194119421943194419451946194719481949195019511952195319541955195619571958195919601961196219631964196519661967196819691970197119721973197419751976197719781979198019811982198319841985198619871988198919901991199219931994199519961997199819992000CoDA-PCA−83.167.4#comp1−79.175.6#comp212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862186318641865186618671868186918701871187218731874187518761877187818791880188118821883188418851886188718881889189018911892189318941895189618971898189919001901190219031904190519061907190819091910191119121913191419151916191719181919192019211922192319241925192619271928192919301931193219331934193519361937193819391940194119421943194419451946194719481949195019511952195319541955195619571958195919601961196219631964196519661967196819691970197119721973197419751976197719781979198019811982198319841985198619871988198919901991199219931994199519961997199819992000CoDA-AE−66.362.5#comp1−64.163.0#comp212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862186318641865186618671868186918701871187218731874187518761877187818791880188118821883188418851886188718881889189018911892189318941895189618971898189919001901190219031904190519061907190819091910191119121913191419151916191719181919192019211922192319241925192619271928192919301931193219331934193519361937193819391940194119421943194419451946194719481949195019511952195319541955195619571958195919601961196219631964196519661967196819691970197119721973197419751976197719781979198019811982198319841985198619871988198919901991199219931994199519961997199819992000t-SNEKL (x(cid:48)) = exp(x(cid:48))(cid:81)d

(centered or not) is to go back to (cid:60)d from Sd without losing information. Notice that log(xj) 
log(x) ∈ (−∞  0) so the compositional data is embedded in (cid:60)d under the clr transformation. The
reverse operation: x = c−1
j) embeds (cid:60)d into Sd. See the table
below for a comparison of clr  alr and ilr  presented as different transformations C log(x). They are
equivalent up to linear transformations. Without loss of generality we focus on the clr.
(cid:9)

j=1 exp( 1

1−d x(cid:48)

clr

(cid:8)RCclr : RR

ilr

= Id−1

alr

(cid:124)

(d−1)×d = [Id−1 −1d−1]
Calr

Cilr
(d−1)×d ∈

d×d = Id − 1
Cclr

d 1d1

(cid:124)
d

However interpreting the resulting coordinates is still challenging [13  24]: alr transformation is no
distance-preserving; clr leads to degenerate distributions and singular covariance matrices; ilr avoids
the precedent drawbacks  but still  results from complicated nonlinear transformations are difﬁcult to
interpret. Currently  there seems to be no consensus about the best practices ([16] versus [31]) and  in
all cases  log-transforming is not a remedy for all the difﬁculties arisen by CoDA [20].

3 Exponential Family Principal Component Analysis

Another way to apply dimension reduction is to perform a generalized PCA on crude count data.
Based on the same ideas as the generalised linear model  [12] described a generalized PCA model for
distributions from the exponential family. We ﬁrst recall the standard PCA setting.

3.1 Principal Component Analysis

For simplicity suppose that the data matrix X is already centered that can be easily achieved by
appending to A matrix a row of ones.
(Traditional PCA) We have a dataset X ∈ (cid:60)d×m that we approximate as X ∼ V
the following loss wrt the constraints A ∈ (cid:60)(cid:96)×m   V ∈ (cid:60)(cid:96)×d   VV

A by minimizing

= I(cid:96):

(cid:124)

(cid:124)

(cid:96)PCA(X; A  V)

(cid:124)

.

= (cid:107)X − V

A(cid:107)2
F .

(2)

V deﬁning a rank-(cid:96)
Hence  observations are column-wise. V : (cid:60)d → (cid:60)(cid:96) is surjective with V
projection  assuming in general (cid:96) < d. A is the representation of data points. The goodness of ﬁt
of the representation is measured by the squared Frobenious norm. We summarise the different
transformations and loss functions in Table 1.
Observe that instead of ﬁnding a linear representation A and its corresponding linear loadings V 
we can consider nonlinear functions for encoding and decoding the latent representation. When the
nonlinear encoder and decoder are implemented as feed-forward neural networks  we arrive at the
autoencoder setting.

(cid:124)

3.2 Bregman Divergence and ϕ-PCA

As mentioned in the introduction  compositional data do not live in Euclidean space. Count data are
naturally linked to the Poisson distribution  and therefore we should consider an exponential family
model for count data. From the Bayesian viewpoint  the PCA goal is to minimize a distance (L2 for
usual PCA) which is equivalent to a Bregman divergence minimization (or a Likelihood function
maximization).
Deﬁnition 1 (Bregman Divergence) Let ϕ : (cid:60)d → (cid:60) convex differentiable. The Bregman diver-
gence Dϕ with generator ϕ is
(3)

(cid:124)
)

).

)

.

= ϕ(x) − ϕ(x(cid:48)

) − (x − x(cid:48)

Dϕ(x(cid:107) x(cid:48)

∇ϕ(x(cid:48)

A Bregman divergence is just a truncation of the Taylor expansion of a function. It can therefore be
deﬁned for any differentiable function  not just the convex ones. If ϕ is not convex  we call Dϕ a
Bregman distortion  which is a signed dissimilarity. We denote by ϕ(cid:63)(x)
y − ϕ(y)} the
convex conjugate of the generator ϕ [9].

= supy{x

(cid:124)

.

3

Table 1: Summary of methods in this paper

Method
PCA
ϕ-PCA
clr-PCA
gauged-ϕ-PCA
CoDA PCA
S-CODA-PCA
CoDA AE

Original

X
X

cKL(X)
∇ϕ(ˇX)
cKL(X)
ˇxi
X

Reconstruction

A)

∇

(cid:124)
A
V
(cid:124)
∗ϕ(V
(cid:124)
A
V
(cid:124)
A
V
(cid:124)
A
V
∇ ˇKL(exp(V

(cid:124)
gθ ◦ hΦ(X)

ai))

F

Distortion
(cid:107) · − · (cid:107)2
Dϕ(·(cid:107)·)
(cid:107) · − · (cid:107)2
Dϕ(cid:63) (·(cid:107)·)
Dexp(·(cid:107)·)
inner product
Dexp(·(cid:107)·)

F

Notes
classical PCA(2)
exponential family PCA (4)
CoDA with clr (5)
General Bregman PCA (8)
(11) is a special case of (8)
upper bound (17)
neural networks gθ and hΦ

PCA has been generalized to the exponential families in a way that makes ﬁtting occur in the natural
parameter space [12  19] (and references therein). The optimization problem is non-convex. The
algorithmic strategy proposed by [12] is to use an alternating sequence of convex minimizations
under constraints. Alternatively  [19] proposed maximizing the deviance (as a generalized notion
of variance) and [10] proposed maximizing the likelihood function via a variational algorithm and
gradient descent.
We denote exponential family PCA as ϕ-PCA  where ϕ is the cumulant of the exponential family 
which is strictly convex differentiable with convex conjugate ϕ(cid:63)  and uniquely determines the
exponential family under mild conditions [7]. Note that for ϕ-PCA  X is not neccessarily in a vector
space (e.g. X (cid:54)⊆ (cid:60)d×m).
(ϕ-PCA) We have a dataset Xd×m that we approximate as X ∼ ∇ϕ(cid:63)(V
V ∈ (cid:60)(cid:96)×d  VV

= I(cid:96)  through minimizing the Bregman loss

A) with A ∈ (cid:60)(cid:96)×m 

(cid:124)

(cid:124)

(cid:96)ϕ-PCA(X; A  V)

.
=

(cid:88)

i

Dϕ(xi (cid:107)∇ϕ(cid:63)(V

ai)) = Dϕ(X(cid:107)∇ϕ(cid:63)(V

(cid:124)

(cid:124)

A)).

(4)

(cid:124)

Vectors are column-vectors: xi  ai are respectively column observation i in the ambient and principal
spaces  respectively. This formulation has a major advantage that linear algebra may be used to ﬁt
A  V while X may not lie in a vector space  see for example [12  19] and references therein. We
remark that because of the dual symmetry of Bregman divergences  we have Dϕ(X(cid:107)∇ϕ(cid:63)(V
A)) =
A(cid:107)∇ϕ(X)) [8]. Notice there exists a little "hole" in the ϕ-PCA deﬁnition  as X is not
Dϕ(cid:63) (V
necessarily easy to center when it is not in a vector space.
ϕ-PCA includes standard PCA as a special case as when ϕ(x) = 1
F and the corresponding
Bregman divergence becomes Dϕ(x(cid:107) x(cid:48)) = 1
4 Exponential family PCA on Compositional Data

2(cid:107)x − x(cid:48)

2(cid:107)x(cid:107)2

F .
(cid:107)2

(cid:124)

CoDA has found a workaround for the centering problem  centered log-ratio coordinates. From [3 
Def. 4.6  Chap. 8] the associated loss is the standard PCA loss on clr transformed data:

(cid:96)clr-PCA(X; A  V)

.
=

1
2(cid:107)cKL(X) − V

(cid:124)

A(cid:107)2

F = Dϕ(cKL(X)(cid:107)∇ϕ(cid:63)(V

(cid:124)

A)) 

(5)

where cKL(X) is the centered log-ratio transform deﬁned in Equation (1) and ϕ(x) = 1
F. Recall
from the previous section that we could deal with crude count data by using exponential family PCA.
However if we wish to perform PCA on the crude count data  while maintaining the clr transform  we
need an additional normalization term  which requires us to obtain a gauged version of the Bregman
divergence.

2(cid:107)x(cid:107)2

4.1 Scaled Bregman Theorem with Remainder

In this section we generalize the Scaled Bregman Theorem from [25  Theorem 1] to allow for a
remainder term. We use it in this paper to deal with the perspective transform required for CoDA 
but it may be of independent interest. Recall that ϕ is the generator of the Bregman distortion
(Deﬁnition 1). We additionally deﬁne a perspective (or gauge) function g to deal with the fact that we

4

are considering data on the simplex. Whenever ϕ and g are differentiable  the following is immediate
from [25  Theorem 1].
Theorem 2 (Scaled Bregman Theorem with Remainder) For any ϕ : X → (cid:60) and g : X → (cid:60)∗
((cid:60)∗ = (cid:60) \ {0}) that are both differentiable  denoting

(cid:18) x

(cid:19)

g(x)

 

ˇx .
=

x

g(x)

and

ˇϕ(x)

.

= g(x) · ϕ

the following holds true:

g(x) · Dϕ ( ˇx(cid:107) ˇy) = D ˇϕ (x(cid:107) y) + Rϕ g(x(cid:107) y)  

∀x  y ∈ X  

.

where Rϕ g(x(cid:107) y)
We can abstract Theorem 2 by saying that for any ϕ  g differentiable  we have

= ϕ(cid:63) (∇ϕ( ˇy)) · Dg(x(cid:107) y) is called the remainder.

(6)

(7)

perspective-Bregman(ϕ  g) = Bregman(perspective(ϕ)) + conformal-Bregman(g  ϕ) 

where “perspective(ϕ)” is ˇϕ in (6)  and conformal divergences are deﬁned and analyzed in [26].
General classes of perspective transforms of convex functions are introduced in [22  23]. The notion
of perspective transform of a Bregman divergence was introduced in [25]. In [25  Theorem 1] 
conditions are assumed that make Rϕ g(x(cid:107) y) = 0  resulting in the scaled Bregman theorem. Notice
that Dϕ is a Bregman distortion but not necessarily a Bregman divergence if ϕ is not convex. For
reasons explained in [25]  we call g a gauge. In the following we assume that ϕ is separable  so that
we can use both notations ∇ϕ and ϕ(cid:48) to denote the gradient and derivatives involving ϕ.
is homogeneous of degree one  Dϕ ( ˇx(cid:107) ˇy) and
By Theorem 2 
g(x) = (cid:81)d
g(x) [D ˇϕ (x(cid:107) y) + Rϕ g(x(cid:107) y)] are both invariant to re-scaling of x and y and can therefore be
used to deal with compositional data. A general formulation of g satisfying this condition can be
j=1 wj = 1. In this paper  we focus on the special
j=1 x
case ∀j  wj = 1
d so that Dϕ ( ˇx(cid:107) ˇy) can be expressed in terms of the widely used clr transformation.
Setting w to be a one-hot vector (1  0 ···   0) can express Dϕ ( ˇx(cid:107) ˇy) with the alr. This latter case
will be omitted here.

  where ∀j  wj ≥ 0 and(cid:80)d

long as g(x)

wj
j

as

1

4.2 Exponential Family CoDA

We are now in a position to derive the exponential family version of the loss in (5). Let ˇX denote the
matrix of the column vectors ˇxi. It turns out that in the same way as (2) is an approximation of (4) 
the loss in (5) is an approximation of the gauged loss:

(cid:96)gauged-ϕ-PCA(X; A  V)

.
= Dϕ(cid:63) (V

(cid:124)

A(cid:107)∇ϕ(ˇX)) = Dϕ(ˇX(cid:107)∇ϕ(cid:63)(V

(cid:124)

A)).

(8)

Note that the above expression is in terms of the normalised matrix ˇX. To unpack it in terms of
.
the original data X  we apply Theorem 2. In the CoDA case  ϕ(cid:63)(z)
= exp z  the convex dual of
ϕ(z)

= z log z − z. Indeed  after remarking that ∇ϕ(ˇX) = cKL(X)  it follows

.

(cid:124)

A)1 − trace(cid:0)ˇX

A(cid:107) cKL(X)) = DKL
(cid:124)
(cid:124)
V

(cid:124)

(cid:0)ˇX(cid:107) exp(V
A)(cid:1)
A(cid:1) + constant.

(cid:124)

(cid:124)

(cid:96)gauged-KL-PCA(X; A  V) = Dexp(V

(9)
In other words  the CoDA PCA is in fact ﬁtting natural parameters from centered log-ratios being
natural coordinates as well. From (9) we observe that both of them live in the same space. Therefore
V

A is centered in the same way as cKL(X)  and so

exp(V

= 1

(cid:124)

(cid:124)
V1d ∈ ker(A

(cid:124)
) ⇔ A

V1 = 0m .

(10)

Remark that a centering assumption is also explicit in [3  Chapter 8  Eq. 8.1].
Hence  we can deﬁne the CoDA PCA problem as follows.

5

(CoDA PCA) We have a dataset X ∈ (Sd)m that we approximate as cKL(X) ∼ V
(cid:124)
the following loss wrt the constraints A ∈ (cid:60)(cid:96)×m   V ∈ (cid:60)(cid:96)×d   VV
= I(cid:96)   A

(cid:124)

(cid:96)CoDA-PCA(X; A  V) = Dexp(V

(cid:124)

A(cid:107) cKL(X)).

A by minimizing

(cid:124)
V1 = 0:

(11)

Regarding cKL(x)  ˇx and x as different coordinate systems of Sd  we use the Fisher information
metric (FIM) [6]  whose formulation is well studied on the x coordinates  to deﬁne the corresponding
pullback metric G under the cKL(x) and ˇx coordinates  meaning that these metrics correspond to the
same underlying geometry of Sd. We have the following proposition (proof omitted; see [30] for
similar derivations).
Proposition 3 The FIM that uniquely deﬁnes the geometry of c ∈
Gij(c) = δij

(cid:8)cKL(x) : x ∈ Sd(cid:9) is given by

((cid:80)d
i=1 exp(ci))2 ; the FIM under the coordinates ˇx is given by Gij( ˇx) =

(cid:80)d
i=1 exp(ci) − exp(ci+cj )
((cid:80)d
(cid:80)d
i=1 ˇxi)2   where δij = 1 if i = j otherwise δij = 0.
i=1 ˇxi −
(cid:124)G( ˇx)d ˇx

(cid:124) of a tiny shift d ˇx. It is not
Intuitively  the metric G measures the local distance d ˇx
everywhere identity as in a Euclidean space. Therefore the distance should not be measured by the
Frobenious norm as in (5). In contrast  our loss (cid:96)CoDA-PCA(X; A  V) is based on the KL divergence
which locally agrees with the FIM [6].

δij

ˇxi

exp(ci)

1

1

4.3 Relating CoDA PCA to ϕ-PCA

(cid:80)d

We now deﬁne and analyze a generalized perspective transform of the generator of KL divergence:
let ˇKL(x)

j=1 ϕ(xj/g(x)) where ϕ(z)

j xj)1/d.

.

.

= z log(z) − z and g(x)

= g(x) ·

Lemma 4 (Properties of ˇKL) ˇKL satisﬁes the following properties:

= ((cid:81)

.

(1) ˇKL is convex;

(2) the general term of the Hessian H of ˇKL is

Hij

.
= Hij( ˇKL(x)) =

1
dxj ·

(cid:26)

.
= 1 + xa/xb. Furthermore 

(cid:124)

z

Hz =

1
2d

(cid:88)

ij

(xi + xj) ·
(cid:124)

where uab

Hence  z

(cid:124)

(cid:80)
(cid:18) zi

−uji
k(cid:54)=j ukj

xi −

zj
xj

(cid:19)2

if
j (cid:54)= i
otherwise

 

 ∀z ∈ (cid:60)d.

(12)

(13)

Hz ≥ 0 ∀x ∈ (cid:60)d

++ ∀z ∈ (cid:60)d and z

Hz = 0 only when z ∝ x;

(3) function ˇKL ◦ exp is 1-homogeneous on span({1})⊥.

(Proof in SM  Section D) A consequence of Theorem 2 is the following Corollary.

(cid:124)
Corollary 5 For any A  V such that A

V1 = 0  we have
1

(cid:88)

(cid:96)CoDA-PCA(X; A  V) ≤ ˜(cid:96)

.
=

g(xi) · D ˇKL(xi(cid:107) exp(V

i

(cid:124)

ai)).

(14)

Hence  the CoDA PCA loss is upperbounded by a weighted generalized ϕ-PCA loss. Furthermore 
(15)

(cid:124)
ai)) = ˇKL(xi) − x
i ∇ ˇKL(exp(V

D ˇKL(xi(cid:107) exp(V

ai))

(cid:124)

(cid:124)

Proof Since g is concave (Example 1 in Supplement)  Dg(x(cid:107)y) = −D−g(x(cid:107)y) ≤ 0  and (14)
follows from Theorem 7 and the fact that ri ≥ 0 ∀i  which shows (14). (15) is a consequence of the
analytical construct of Bregman divergences (Deﬁnition 1) and point (3) in Lemma 4 and the fact that
(cid:124)
ai ∈ span({1})⊥ by assumption.
V

6

Remark In [3  Chapter 8]  CoDA PCA is presented as a (centered) regular PCA over data that been
subject to two transforms via the centered log-ratio coordinates. What Corollary 5 shows is that
we can solve the problem via a surrogate formulation using non transformed data but minimizing
a loss which is that of a ϕ-PCA transformed twice: ﬁrst taking a perspective transform of the KL
generator ( ˇKL) and then having a weighted Bregman divergence minimization (g−1(.)). We remark
that weights can also be folded in the arguments as we have:

˜(cid:96) =

KL( ˇxi) − ˇx

(cid:124)
i ∇ ˇKL(exp(V

(cid:124)

ai)) .

(16)

(cid:88)

i

Furthermore  the leftmost argument in (16) plays no role in its minimization  and therefore we get the
Surrogate CoDA PCA (S-CODA-PCA) by replacing (11) with a simple inner product:

(cid:88)

i

ˇx

(cid:124)
i ∇ ˇKL(exp(V

(cid:124)

ai)) .

(17)

(cid:96)S-CODA-PCA(X; A  V)

.

= −

5

Implementations

Both the CODA-PCA in (11) and the S-CODA-PCA in (17) can be equivalently written as the
following unconstrained problems

(CODA-PCA)

(S-CODA-PCA)

argmin

B U

argmin

B U

Y(cid:1)(cid:3)  

(cid:124)

(cid:2)1

(cid:124)

d exp(Y)1m − trace(cid:0)ˇX
(cid:124)(cid:18)
(cid:20)

trace

ˇX

exp(−Y) ◦

(cid:124)
1d1
d
d

exp(Y) − Y

 

(cid:19)(cid:21)

(18)

(19)

(cid:124)

d (I(cid:93)

(cid:124)

(cid:124)
1 = B

(cid:124)
UC

(cid:124)

(cid:124)
B(cid:63) = (V(cid:63))

(cid:124)
d or C = Id − I(cid:93)
d 1d1

where “◦” means element-wise product  exp(·) is element-wise exponential  and Y = CU
B with
C ∈ (cid:60)d×d  U ∈ (cid:60)(cid:96)×d  B ∈ (cid:60)(cid:96)×m. C is a constant centering matrix satisfying rank(C) = d − 1 
(cid:124)
1 = 0. Any C
1 = 0  so that Y’s columns are automatically centered and Y
C
satisfying this condition corresponds to a valid re-parametrization of the feasible space  for example
d circularly raises the diagonal entries of Id by 1 row). U’s rows
C = Id − 1
form a nonorthogonal basis of (cid:60)d. B’s columns are the sample coordinates in such a basis. After
(cid:124)
optimization  we take the QR decomposition C(U(cid:63))
T(cid:63)  where V(cid:63)’s rows are orthonormal.
Therefore C(U(cid:63))
T(cid:63)B(cid:63) and A(cid:63) = T(cid:63)B(cid:63) is the corresponding coordinates. An optimal
solution of the original constrained PCA problem is given by (V(cid:63)  A(cid:63)).
Although the losses in (18) and (19) are non-convex  they are both bi-convex. Fixing U  the loss
is a strictly convex function of B that is decomposed into a sum of per-sample convex functions
of bi; ﬁxing B  it is a strictly convex function of U. These convex functions have the general
ξ. Its gradient and Hessian are both in simple closed form:
(cid:124)
i . One can apply an off-the-shelf
convex optimizer  which in the simplest case can be the Newton method  to alternately minimize B
and U until convergence. Our implementation simply uses L-BFGS [9] based on the gradient of the
loss. In summary  we have the following result.

i ξ+βi)αi +ζ; (cid:53)2f =(cid:80)

form f (ξ) =(cid:80)
(cid:53)f =(cid:80)

i exp(α

(cid:124)
i ξ+2βi)αiα

(cid:124)
i ξ + βi) + ζ

(cid:124)
= (V(cid:63))

i exp(α
(cid:124)

i exp(2α

(cid:124)

Proposition 6 The CODA-PCA and the S-CODA-PCA are both equivalent to an unconstrained
bi-convex optimization problem.

As an alternative implementation  we assume a parametric mapping bi = gΘ(xi) that is the (cid:96)-
dimensional output of a feed-forward neural network with input cKL(xi) and xi (or ˇxi) and connection
weights Θ. Then we minimize the cost function in (18) with respect to U and Θ. If gΘ is ﬂexible
enough  then the minimization recovers the CODA-PCA projection. This approach could be favored
as  it learns an out-of-sample mapping gΘ(·) with a compact parametric structure that does not
scale with the sample size m; and  it can be adapted to an online learning scenario. However  it
requires tuning of the neural network architecture and the optimizer. In our experiments  the encoding
map is modeled by a feed-forward neural network with two hidden layers of ELU [11] units  each of
size 100. To distinguish between the two implementations  the method to directly optimize U and B

7

Figure 2: Testing errors (y-axis) against the number of principal components (x-axis) based on three
different distance measures (from left to right) on the Atlas data (ﬁrst row) and the diet swap data
(second row). The numbers along the clr-PCA curves show the percentage of improvement (green)
or disimprovement (red)  comparing CoDA-PCA against clr-PCA.

without assuming the neural network mapping is called non-parametric CODA-PCA  and the latter
parametric version is simply called CODA-PCA.

Θ

(cid:124)

−→ bi

bi. In the general case  we apply a non-linear decoder bi

U−→ yi  where the decoder is
The above implementation resembles an auto-encoder structure: xi
−→ yi in
simply a linear mapping yi = CU
the form yi = ChΦ(bi)  where hΦ(·) is a neural network with parameters Φ and d output dimensions.
At the same time  we add a small random noise to the encoder input so as to avoid overﬁtting.
In this way we obtain a denoising CODA-AUTOENCODER. In contrast to the CODA-PCA  the
CODA-AUTOENCODER can only be trained by gradient-based optimizers.
In practice  the input matrix X may contain zeros that lie on the boundary of Sd. In this case
cKL(x) and ˇx are undeﬁned. A simple way to tackle the zero entries is to replace them with a small
j:xj >0(xj)1/ρ  where

positive number  > 0. Alternatively  one can redeﬁne the gauge as g(x) =(cid:81)

Φ

ρ = |{j : xj > 0}| so that g(x) is always positive and ˇx is well deﬁned on Sd ∪ ∂Sd.
6 Experiments

We compare the following methods: clr-PCA means PCA applied on the centered log-ratio coordi-
nates; CoDA-PCA is the proposed CODA-PCA in (11); SCoDA-PCA is the proposed S-CODA-PCA in
(17); clr-AE is an autoencoder with L2 loss applied on the clr transformation; CoDA-AE is the
proposed CODA-AUTOENCODER in subsection 5. Both clr-AE and CoDA-AE use exactly the same
structure with one hidden layer of 100 ELU [11] units in their decoders.
The baselines are assessed based on an array of measures including (L2-clr) the L2-distance (cid:107)cKL(x)−
cKL(x(cid:48))(cid:107)F between the input data x ∈ Sd and the reconstruction x(cid:48)
∈ Sd in the clr space; (JSD)
); (TV) the total variation distance
the Jensen-Shannon divergence 1

) + 1

2 KL(x(cid:48) : x+x(cid:48)

2 KL(x : x+x(cid:48)

2

2

8

clr-PCACoDA-PCASCoDA-PCAclr-AECoDA-AE110#PCs7.510.012.515.017.520.0-103.4-106.6-102.3-106.2-103.4-97.9-98.9-97.3-96.0-102.7L2-clr(test)110#PCs0.040.060.080.1024.619.126.320.023.333.339.742.641.939.3JSD(test)110#PCs0.150.200.250.300.3514.016.821.016.517.023.029.030.630.326.8TV(test)110#PCs3456-9.6-14.0-35.1-24.9-16.2-17.0-19.7-21.4-27.0-28.3L2-clr(test)110#PCs0.020.040.060.080.1029.920.418.014.630.129.530.732.427.231.0JSD(test)110#PCs0.100.150.200.250.300.3524.517.116.719.128.426.528.131.922.423.8TV(test)(cid:80)d
i=1 |xi − x(cid:48)

i|. These measurements are all invariant to scaling or permutation of x and x(cid:48). See
1
2
the supplementary material for more baselines and performance indicators.
We consider the following datasets available in the microbiome R package [18]  each of which is
randomly split into a training set (90%) and a testing set (10%). The HITChip Atlas dataset [17]
contains 130 genus-level taxonomic groups that cover the majority of the known bacterial diversity
of the human intestine. The data come from 1006 western adults from 15 western countries (Europe
and the United States). Sample sets were analysed with three different DNA extraction methods. The
two-week diet swap study between western (USA) and traditional (rural Africa) diets was reported in
[28]. In this study  a two-week food exchange was performed in subjects from the same populations 
where African Americans were fed a high-ﬁbre  low-fat African-style diet and rural Africans a
high-fat  low-ﬁbre western-style diet. The group diet was indicated by HE (home environment days) 
DI (dietary intervention days) and ED (initial and ﬁnal endoscopy days). Each subject served as
his/her own control  given the known wide individual variation in colonic microbiota composition.
Fig. 2 shows the typical testing results. We observe that on most performance indicators CoDA-PCA
and CoDA-AE show a much smaller testing error as compared to clr-PCA and clr-AE  respectively.
The only exception is on L2-clr  where clr-PCA and clr-AE appear to be favored against our CoDA
variants. This is because L2-clr is exactly the cost function of those two methods. We found that
CoDA-AE is more robust against overﬁtting as compared to clr-AE. The performance of SCoDA-PCA
is close to CoDA-PCA on most of the indicators and is better than CoDA-PCA on L2-clr.
The source codes to reproduce our experimental results are available online2.

7 Conclusion

We propose an approach for learning a low dimensional representation directly on raw count data 
which is compositional in nature. Our proposed algorithm generalizes PCA in two ways  ﬁrst by going
to the exponential family via the Bregman divergence  and second by converting the normalization of
data to a change in the Bregman divergence. The key theorem used for transforming the Bregman
divergence generalizes a recent result  and may be of independent interest.

Acknowledgements

The authors gratefully thank Perrine Soret  Frank Nielsen  Xinhua Zhang  and the anonymous NIPS
reviewers  for their helpful and constructive feedback. This work was done while MAF was visiting
Data61  CSIRO in Canberra  Australia.

References
[1] J. Aitchison. The statistical analysis of compositional data (with discussion). Journal of the

Royal Statistical Society B  44(2):139–177  1982.

[2] J. Aitchison. Principal component analysis of compositional data. Biometrika  70(1):57–65 

1983.

[3] J. Aitchison. The Statistical Analysis of Compositional Data. Chapman and Hall  New York 

1986.

[4] J. Aitchison. Principles of compositional data analysis. Multivariate Analysis and its Applica-

tions  24:73–81  1994.

[5] J. Aitchison and J.-J. Egozcue. Compositional data analysis: Where are we and where should

we be heading? Mathematical Geology Journal  37:829–850  2005.

[6] S.-I. Amari. Information Geometry and Its Applications. Springer-Verlag  Berlin  2016.

[7] O. Barndorff-Nielsen. Information and Exponential Families in Statistical Theory. Wiley

Publishers  1978.

2https://bitbucket.org/RichardNock/coda

9

[8] J.-D. Boissonnat  F. Nielsen  and R. Nock. Bregman Voronoi diagrams. Discrete Comput.

Geom.  44(2):281–307  2010.

[9] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

[10] J. Chiquet  M. Mariadassous  and Stéphane Robin. Variational inference for probabilistic

Poisson PCA. Annals of Applied Statistics (to appear)  2018.

[11] D.-A. Clevert  T. Unterthiner  and S. Hochreiter. Fast and accurate deep network learning by

exponential linear units (ELUs). In 4th ICLR  2016.

[12] M. Collins  S. DasGupta  and R. Schapire. A generalization of principal components analysis to
the exponential family. In T. G. Dietterich  S. Becker  and Z. Ghahramani  editors  NIPS*15 
2002.

[13] J.-J. Egozcue  V. Pawlowsky-Glahn  G. Mateu-Figueras  and C. Barceló-Vidal.

Isometric
logratio transformations for compositional data analysis. Mathematical Geology Journal 
35:279–300  2003.

[14] G. Reid G. B. Gloor. Compositional analysis: a valid approach to analyze microbiome high-

throughput sequencing data. Can J Microbiol.  12:1–12  2016.

[15] G. B. Gloor  J. R. Wu  V. Pawlowsky-Glahn  and J. J. Egozcue. It’s all relative: analyzing

microbiome data as compositions. Ann Epidemiol.  26:322–9  2016.

[16] M. Greenacre. Compositional Data Analysis in Practice. Chapman and Hall  New York  2018.

[17] L. Lahti  J. Salojärvi  A. Salonen  M. Scheffer  and W. M. de Vos. Tipping elements in the

human intestinal ecosystem. Nat. Commun.  5:4344  2014.

[18] L. Lahti  S. Sudarshan  T. Blake  and J. Salojarvi. Microbiome R package. version 1.1.10012 

2017.

[19] A. J. Landgraf. Generalized Principal Component Analysis: Dimensionality Reduction through

the Projection of Natural Parameters. PhD thesis  Ohio State University  2015.

[20] D. Lovell  W. Müller  J. Taylor  A. Zwart  and C. Helliwell. Caution! compositions! can

constraints on omics data lead analyses astray? Technical Report EP10994  CSIRO  2010.

[21] A.F. Andersson L.W. Hugerth. Analysing microbial community composition through amplicon

sequencing: From sampling to hypothesis testing. Frontiers in Microbiology  8:1561  2017.

[22] P. Maréchal. On a functional operation generating convex functions  part 1: duality. J. of

Optimization Theory and Applications  126:175–189  2005.

[23] P. Maréchal. On a functional operation generating convex functions  part 2: algebraic properties.

J. of Optimization Theory and Applications  126:375–366  2005.

[24] J. A. Martín-Fernández  V. Pawlowsky-Glahn  J. J. Egozcue  and R. Tolosona-Delgado. The
statistical analysis of compositional data (with discussion). Math Geosci  50:273–298  2018.

[25] R. Nock  A.-K. Menon  and C.-S. Ong. A scaled Bregman theorem with applications. In

NIPS*29  pages 19–27  2016.

[26] R. Nock  F. Nielsen  and S.-I. Amari. On conformal divergences and their population minimizers.

IEEE Trans. IT  62:1–12  2016.

[27] V. Shankar O. Paliy. Application of multivariate statistical techniques in microbial ecology.

Molecular ecology  25:1032–57  2016.

[28] S. J. D. O’Keefe  J. V. Li  L. Lahti  J. Ou  F. Carbonero  K. Mohammed  J. M. Posma  J. Kinross 
E. Wahl  E. Ruder  K. Vipperla  V. Naidoo  L. Mtshali  S. Tims  P. G. B. Puylaert  J. DeLany 
A. Krasinskas  A. C. Beneﬁel  H. O. Kaseb  K. Newton  J. K. Nicholson  W. M. de Vos  H. R.
Gaskins  and E. G. Zoetendal. Fat  ﬁber and cancer risk in African Americans and rural Africans.
Nat. Commun.  6:6342  2015.

10

[29] V. Pawlowsky-Glahn and A. Buccianti. Compositional Data Analysis  theory and applications.

Wiley  2011.

[30] Ke Sun and Stéphane Marchand-Maillet. An information geometry of statistical manifold

learning. In 31st ICML  pages 1–9  2014.

[31] R. Tolosona-Delgado V. Pawlowsky-Glahn  J. J. Egozcue. Lecture notes on compositional data

analysis. Technical report  Girona University  2007.

[32] L. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research  9(Nov):2579–2605  2008.

11

,Marta Avalos
Richard Nock
Cheng Soon Ong
Julien Rouar
Ke Sun