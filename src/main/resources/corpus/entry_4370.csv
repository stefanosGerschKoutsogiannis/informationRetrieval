2010,Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression,Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs  on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g.  the execution time of a computer program) and features  and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g.  features and their non-linear combinations that dominate the execution time)  enabling a better understanding of the program’s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition  we compare SPORE algorithms to state-of-the-art sparse regression algorithms  and show that SPORE methods  motivated by real applications  outperform the other methods in terms of both interpretability and prediction accuracy.,Predicting Execution Time of Computer Programs

Using Sparse Polynomial Regression

Ling Huang

Intel Labs Berkeley
ling.huang@intel.com

Jinzhu Jia
UC Berkeley

jzjia@stat.berkeley.edu

Byung-Gon Chun
Intel Labs Berkeley

byung-gon.chun@intel.com

Petros Maniatis

Intel Labs Berkeley

petros.maniatis@intel.com

Bin Yu

UC Berkeley

binyu@stat.berkeley.edu

Mayur Naik

Intel Labs Berkeley

mayur.naik@intel.com

Abstract

Predicting the execution time of computer programs is an important but challeng-
ing problem in the community of computer systems. Existing methods require ex-
perts to perform detailed analysis of program code in order to construct predictors
or select important features. We recently developed a new system to automatically
extract a large number of features from program execution on sample inputs  on
which prediction models can be constructed without expert knowledge. In this
paper we study the construction of predictive models for this problem. We pro-
pose the SPORE (Sparse POlynomial REgression) methodology to build accurate
prediction models of program performance using feature data collected from pro-
gram execution on sample inputs. Our two SPORE algorithms are able to build
relationships between responses (e.g.  the execution time of a computer program)
and features  and select a few from hundreds of the retrieved features to con-
struct an explicitly sparse and non-linear model to predict the response variable.
The compact and explicitly polynomial form of the estimated model could reveal
important insights into the computer program (e.g.  features and their non-linear
combinations that dominate the execution time)  enabling a better understanding
of the program’s behavior. Our evaluation on three widely used computer pro-
grams shows that SPORE methods can give accurate prediction with relative error
less than 7% by using a moderate number of training data samples. In addition  we
compare SPORE algorithms to state-of-the-art sparse regression algorithms  and
show that SPORE methods  motivated by real applications  outperform the other
methods in terms of both interpretability and prediction accuracy.

1 Introduction

Computing systems today are ubiquitous  and range from the very small (e.g.  iPods  cellphones 
laptops) to the very large (servers  data centers  computational grids). At the heart of such systems
are management components that decide how to schedule the execution of different programs over
time (e.g.  to ensure high system utilization or efﬁcient energy use [11  15])  how to allocate to each
program resources such as memory  storage and networking (e.g.  to ensure a long battery life or fair
resource allocation)  and how to weather anomalies (e.g.  ﬂash crowds or attacks [6  17  24]).

These management components typically must make guesses about how a program will perform
under given hypothetical inputs  so as to decide how best to plan for the future. For example 
consider a simple scenario in a data center with two computers  fast computer A and slow computer
B  and a program waiting to run on a large ﬁle f stored in computer B. A scheduler is often faced

1

with the decision of whether to run the program at B  potentially taking longer to execute  but
avoiding any transmission costs for the ﬁle; or moving the ﬁle from B to A but potentially executing
the program at A much faster. If the scheduler can predict accurately how long the program would
take to execute on input f at computer A or B  he/she can make an optimal decision  returning
results faster  possibly minimizing energy use  etc.

Despite all these opportunities and demands  uses of prediction have been at best unsophisticated
in modern computer systems. Existing approaches either create analytical models for the programs
based on simplistic assumptions [12]  or treat the program as a black box and create a mapping func-
tion between certain properties of input data (e.g.  ﬁle size) and output response [13]. The success
of such methods is highly dependent on human experts who are able to select important predictors
before a statistical modeling step can take place. Unfortunately  in practice experts may be hard to
come by  because programs can get complex quickly beyond the capabilities of a single expert  or
because they may be short-lived (e.g.  applications from the iPhone app store) and unworthy of the
attention of a highly paid expert. Even when an expert is available  program performance is often
dependent not on externally visible features such as command-line parameters and input ﬁles  but
on the internal semantics of the program (e.g.  what lines of code are executed).

To address this problem (lack of expert and inherent semantics)  we recently developed a new sys-
tem [7] to automatically extract a large number of features from the intermediate execution steps of
a program (e.g.  internal variables  loops  and branches) on sample inputs; then prediction models
can be built from those features without the need for a human expert.

In this paper  we propose two Sparse POlynomial REgression (SPORE) algorithms that use the
automatically extracted features to predict a computer program’s performance. They are variants of
each other in the way they build the nonlinear terms into the model – SPORE-LASSO ﬁrst selects
a small number of features and then entertains a full nonlinear polynomial expansion of order less
than a given degree; while SPORE-FoBa chooses adaptively a subset of the full expanded terms
and hence allows possibly a higher order of polynomials. Our algorithms are in fact new general
methods motivated by the computer performance prediction problem. They can learn a relationship
between a response (e.g.  the execution time of a computer program given an input) and the generated
features  and select a few from hundreds of features to construct an explicit polynomial form to
predict the response. The compact and explicit polynomial form reveals important insights in the
program semantics (e.g.  the internal program loop that affects program execution time the most).
Our approach is general  ﬂexible and automated  and can adapt the prediction models to speciﬁc
programs  computer platforms  and even inputs.

We evaluate our algorithms experimentally on three popular computer programs from web search
and image processing. We show that our SPORE algorithms can achieve accurate predictions with
relative error less than 7% by using a small amount of training data for our application  and that our
algorithms outperform existing state-of-the-art sparse regression algorithms in the literature in terms
of interpretability and accuracy.

Related Work. In prior attempts to predict program execution time  Gupta et al. [13] use a variant of
decision trees to predict execution time ranges for database queries. Ganapathi et al. [11] use KCCA
to predict time and resource consumption for database queries using statistics on query texts and
execution plans. To measure the empirical computational complexity of a program  Trendprof [12]
constructs linear or power-law models that predict program execution counts. The drawbacks of such
approaches include their need for expert knowledge about the program to identify good features  or
their requirement for simple input-size to execution time correlations.

Seshia and Rakhlin [22  23] propose a game-theoretic estimator of quantitative program properties 
such as worst-case execution time  for embedded systems. These properties depend heavily on the
target hardware environment in which the program is executed. Modeling the environment manually
is tedious and error-prone. As a result  they formulate the problem as a game between their algorithm
(player) and the program’s environment (adversary)  where the player seeks to accurately predict the
property of interest while the adversary sets environment states and parameters.

Since expert resource is limited and costly  it is desirable to automatically extract features from pro-
gram codes. Then machine learning techniques can be used to select the most important features
to build a model. In statistical machine learning  feature selection methods under linear regres-
sion models such as LASSO have been widely studied in the past decade. Feature selection with

2

non-linear models has been studied much less  but has recently been attracting attention. The most
notable are the SpAM work with theoretical and simulation results [20] and additive and general-
ized forward regression [18]. Empirical studies with data of these non-linear sparse methods are
very few [21]. The drawback of applying the SpAM method in our execution time prediction prob-
lem is that SpAM outputs an additive model and cannot use the interaction information between
features. But it is well-known that features of computer programs interact to determine the execu-
tion time [12]. One non-parametric modiﬁcation of SpAM to replace the additive model has been
proposed [18]. However  the resulting non-parametric models are not easy to interpret and hence are
not desirable for our execution time prediction problem. Instead  we propose the SPORE method-
ology and propose efﬁcient algorithms to train a SPORE model. Our work provides a promising
example of interpretable non-linear sparse regression models in solving real data problems.

2 Overview of Our System

Our focus in this paper is on algorithms for feature selection and model building. However we ﬁrst
review the problem within which we apply these techniques to provide context [7]. Our goal is to
predict how a given program will perform (e.g.  its execution time) on a particular input (e.g.  input
ﬁles and command-line parameters). The system consists of four steps.

First  the feature instrumentation step analyzes the source code and automatically instruments it
to extract values of program features such as loop counts (how many times a particular loop has
executed)  branch counts (how many times each branch of a conditional has executed)  and variable
values (the k ﬁrst values assigned to a numerical variable  for some small k such as 5).
Second  the proﬁling step executes the instrumented program with sample input data to collect values
for all created program features and the program’s execution times. The time impact of the data
collection is minimal.

Third  the slicing step analyzes each automatically identiﬁed feature to determine the smallest subset
of the actual program that can compute the value of that feature  i.e.  the feature slice. This is the
cost of obtaining the value of the feature; if the whole program must execute to compute the value 
then the feature is expensive and not useful  since we can just measure execution time and we have
no need for prediction  whereas if only a little of the program must execute  the feature is cheap and
therefore possibly valuable in a predictive model.

Finally  the modeling step uses the feature values collected during proﬁling along with the feature
costs computed during slicing to build a predictive model on a small subset of generated features.
To obtain a model consisting of low-cost features  we iterate over the modeling and slicing steps 
evaluating the cost of selected features and rejecting expensive ones  until only low-cost features are
selected to construct the prediction model. At runtime  given a new input  the selected features are
computed using the corresponding slices  and the model is used to predict execution time from the
feature values.

The above description is minimal by necessity due to space constraints  and omits details on the
rationale  such as why we chose the kinds of features we chose or how program slicing works.
Though important  those details have no bearing in the results shown in this paper.

At present our system targets a ﬁxed  overprovisioned computation environment without CPU job
contention or network bandwidth ﬂuctuations. We therefore assume that execution times observed
during training will be consistent with system behavior on-line. Our approach can adapt to modest
change in execution environment by retraining on different environments. In our future research  we
plan to incorporate candidate features of both hardware (e.g.  conﬁgurations of CPU  memory  etc)
and software environment (e.g.  OS  cache policy  etc) for predictive model construction.

3 Sparse Polynomial Regression Model

Our basic premise for predictive program analysis is that a small but relevant set of features may ex-
plain the execution time well. In other words  we seek a compact model—an explicit form function
of a small number of features—that accurately estimates the execution time of the program.

3

To make the problem tractable  we constrain our models to the multivariate polynomial family  for at
least three reasons. First  a “good program” is usually expected to have polynomial execution time in
some (combination of) features. Second  a polynomial model up to certain degree can approximate
well many nonlinear models (due to Taylor expansion). Finally  a compact polynomial model can
provide an easy-to-understand explanation of what determines the execution time of a program 
providing program developers with intuitive feedback and a solid basis for analysis.

For each computer program  our feature instrumentation procedure outputs a data set with n samples
i=1  where yi ∈ R denotes the ith observation of execution time  and xi denotes
as tuples of {yi  xi}n
the ith observation of the vector of p features. We now review some obvious alternative methods to
modeling the relationship between Y = [yi] and X = [xi]  point out their drawbacks  and then we
proceed to our SPORE methodology.

3.1 Sparse Regression and Alternatives

Least square regression is widely used for ﬁnding the best-ﬁtting f (x  β) to a given set of responses
yi by minimizing the sum of the squares of the residuals [14]. Regression with subset selection
ﬁnds for each k ∈ {1  2  . . .   m} the feature subset of size k that gives the smallest residual sum of
squares. However  it is a combinatorial optimization and is known to be NP-hard [14]. In recent
years a number of efﬁcient alternatives based on model regularization have been proposed. Among
them  LASSO [25] ﬁnds the selected features with coefﬁcients ˆβ given a tuning parameter λ as
follows:

ˆβ = arg min
β

1
2

kY − Xβk2

2 + λX

j

|βj|.

(1)

LASSO effectively enforces many βj’s to be 0  and selects a small subset of features (indexed by
non-zero βj’s) to build the model  which is usually sparse and has better prediction accuracy than
models created by ordinary least square regression [14] when p is large. Parameter λ controls the
complexity of the model: as λ grows larger  fewer features are selected.

Being a convex optimization problem is an important advantage of the LASSO method since several
fast algorithms exist to solve the problem efﬁciently even with large-scale data sets [9  10  16  19].
Furthermore  LASSO has convenient theoretical and empirical properties. Under suitable assump-
tions  it can recover the true underlying model [8  25]. Unfortunately  when predictors are highly
correlated  LASSO usually cannot select the true underlying model. The adaptive-LASSO [29]
deﬁned below in Equation (2) can overcome this problem

ˆβ = arg min
β

1
2

kY − Xβk2

2 + λX

j

|

βj
wj

| 

(2)

where wj can be any consistent estimate of β. Here we choose wj to be a ridge estimate of β:

where I is the identity matrix.

wj = (X T X + 0.001I)−1X T Y 

Technically LASSO can be easily extended to create nonlinear models (e.g.  using polynomial basis
functions up to degree d of all p features). However  this approach gives us (cid:0)p+d
d (cid:1) terms  which is
very large when p is large (on the order of thousands) even for small d  making regression computa-
tionally expensive. We give two alternatives to ﬁt the sparse polynomial regression model next.

3.2 SPORE Methodology and Two Algorithms

Our methodology captures non-linear effects of features—as well as non-linear interactions among
features—by using polynomial basis functions over those features (we use terms to denote the poly-
nomial basis functions subsequently). We expand the feature set x = {x1 x2 . . . xk}  k ≤ p to
all the terms in the expansion of the degree-d polynomial (1 + x1 + . . . + xk)d  and use the terms
to construct a multivariate polynomial function f (x  β) for the regression. We deﬁne expan(X  d)
as the mapping from the original data matrix X to a new matrix with the polynomial expansion
terms up to degree d as the columns. For example  using a degree-2 polynomial with feature set

4

x = {x1 x2}  we expand out (1 + x1 + x2)2 to get terms 1  x1  x2  x2
basis functions to construct the following function for regression:

1  x1x2  x2

2  and use them as

expan ([x1  x2]  2) = [1  [x1]  [x2]  [x2

1]  [x1x2]  [x2

2]] 

f (x  β) = β0 + β1x1 + β2x2 + β3x2

1 + β4x1x2 + β5x2
2.

Complete expansion on all p features is not necessary  because many of them have little contri-
bution to the execution time. Motivated by this execution time application  we propose a general
methodology called SPORE which is a sparse polynomial regression technique. Next  we develop
two algorithms to ﬁt our SPORE methodology.

3.2.1 SPORE-LASSO: A Two-Step Method

For a sparse polynomial model with only a few features  if we can preselect a small number of
features  applying the LASSO on the polynomial expansion of those preselected features will still
be efﬁcient  because we do not have too many polynomial terms. Here is the idea:

Step 1: Use the linear LASSO algorithm to select a small number of features and ﬁlter out (often
many) features that hardly have contributions to the execution time.

Step 2: Use the adaptive-LASSO method on the expanded polynomial terms of the selected features
(from Step 1) to construct the sparse polynomial model.

Adaptive-LASSO is used in Step 2 because of the collinearity of the expanded polynomial features.
Step 2 can be computed efﬁciently if we only choose a small number of features in Step 1. We
present the resulting SPORE-LASSO algorithm in Algorithm 1 below.

Algorithm 1 SPORE-LASSO
Input: response Y   feature data X  maximum degree d  λ1  λ2
Output: Feature index S  term index St   weights ˆβ for d-degree polynomial basis.
1: ˆα = arg minα
2: S = {j : ˆαj 6= 0}
3: Xnew = expan(X(S)  d)
4: w = (X T
5: ˆβ = arg minβ
6: St = {j : ˆβj 6= 0}

new Xnew + 0.001I)−1X T

2 kY − Xnew βk2

2 + λ2 Pj | βj

wj

2 kY − Xαk2

2 + λ1kαk1

1

1

new Y

|

X(S) in Step 3 of Algorithm 1 is a sub-matrix of X containing only columns from X indexed by
S. For a new observation with feature vector X = [x1  x2  . . .   xp]  we ﬁrst get the selected feature
vector X(S)  then obtain the polynomial terms Xnew = expan(X(S)  d)  and ﬁnally we compute
the prediction: ˆY = Xnew × ˆβ. Note that the prediction depends on the choice of λ1  λ2 and
maximum degree d. In this paper  we ﬁx d = 3. λ1 and λ2 are chosen by minimizing the Akaike
Information Criterion (AIC) on the LASSO solution paths. The AIC is deﬁned as n log(kY − ˆY k2
2)+
2s  where ˆY is the ﬁtted Y and s is the number of polynomial terms selected in the model. To be
precise  for the linear LASSO step (Step 1 of Algorithm 1)  a whole solution path with a number of
λ1 can be obtained using the algorithm in [10]. On the solution path  for each ﬁxed λ1  we compute
a solution path with varied λ2 for Step 5 of Algorithm 1 to select the polynomial terms. For each
λ2  we calculate the AIC  and choose the (λ1  λ2) with the smallest AIC.
One may wonder whether Step 1 incorrectly discards features required for building a good model
in Step 2. We next show theoretically this is not the case. Let S be a subset of {1  2  . . .   p} and
its complement Sc = {1  2  . . .   p} \ S. Write the feature matrix X as X = [X(S)  X(Sc)]. Let
response Y = f (X(S)) + ǫ  where f (·) is any function and ǫ is additive noise. Let n be the number
of observations and s the size of S. We assume that X is deterministic  p and s are ﬁxed  and ǫ′is are
i.i.d. and follow the Gaussian distribution with mean 0 and variance σ2. Our results also hold for
zero mean sub-Gaussian noise with parameter σ2. More general results regarding general scaling of
n  p and s can also be obtained.

Under the following conditions  we show that Step 1 of SPORE-LASSO  the linear LASSO  selects
the relevant features even if the response Y depends on predictors X(S) nonlinearly:

5

1. The columns (Xj  j = 1  . . .   p) of X are standardized: 1

n X T

j Xj = 1  for all j;

2. Λmin( 1

n X(S)T X(S)) ≥ c with a constant c > 0;

3. min |(X(S)T X(S))−1X(S)T f (X(S))| > α with a constant α > 0;

4. X T

Sc [I−XS (X T

S XS )−1X T

S ]f (XS )

n

< ηαc
2√s+1

  for some 0 < η < 1;

5. kX T

ScXS(X T

S XS)−1k

≤ 1 − η;

∞

where Λmin(·) denotes the minimum eigenvalue of a matrix  kAk
and the inequalities are deﬁned element-wise.
Theorem 3.1. Under the conditions above  with probability → 1 as n → ∞  there exists
some λ  such that ˆβ = ( ˆβS  ˆβSc) is the unique solution of the LASSO (Equation (1))  where
ˆβj 6= 0  for all j ∈ S and ˆβSc = 0.

is deﬁned as maxi hPj |Aij |i

∞

Remark. The ﬁrst two conditions are trivial: Condition 1 can be obtained by rescaling while Con-
dition 2 assumes that the design matrix composed of the true predictors in the model is not singular.
Condition 3 is a reasonable condition which means that the linear projection of the expected re-
sponse to the space spanned by true predictors is not degenerated. Condition 4 is a little bit tricky;
it says that the irrelevant predictors (XSc) are not very correlated with the “residuals” of E(Y ) after
its projection onto XS. Condition 5 is always needed when considering LASSO’s model selection
consistency [26  28]. The proof of the theorem is included in the supplementary material.

3.2.2 Adaptive Forward-Backward: SPORE-FoBa

Using all of the polynomial expansions of a feature subset is not ﬂexible. In this section  we propose
the SPORE-FoBa algorithm  a more ﬂexible algorithm using adaptive forward-backward searching
over the polynomially expanded data: during search step k with an active set T (k)  we examine one
new feature Xj  and consider a small candidate set which consists of the candidate feature Xj  its
higher order terms  and the (non-linear) interactions between previously selected features (indexed
by S) and candidate feature Xj with total degree up to d  i.e.  terms with form

j Πl∈S X dl
X d1

l

  with d1 > 0  dl ≥ 0  and d1 + X dl ≤ d.

(3)

Algorithm 2 below is a short description of the SPORE-FoBa  which uses linear FoBa [27] at step
5and 6. The main idea of SPORE-FoBa is that a term from the candidate set is added into the model
if and only if adding this term makes the residual sum of squares (RSS) decrease a lot. We scan all
of the terms in the candidate set and choose the one which makes the RSS drop most. If the drop in
the RSS is greater than a pre-speciﬁed value ǫ  we add that term to the active set  which contains the
currently selected terms by the SPORE-FoBa algorithm. When considering deleting one term from
the active set  we choose the one that makes the sum of residuals increase the least. If this increment
is small enough  we delete that term from our current active set.

Algorithm 2 SPORE-FoBa
Input: response Y   feature columns X1  . . .   Xp  the maximum degree d
Output: polynomial terms and the weights
1: Let T = ∅
2: while true do
3:
4:
5:
6:
7:
8:

if no terms can be added or deleted then

for j = 1  . . .   p do

break

Let C be the candidate set that contains non-linear and interaction terms from Equation (3)
Use Linear FoBa to select terms from C to form the new active set T .
Use Linear FoBa to delete terms from T to form a new active set T .

6

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

0.2

0.15

0.1

0.05

 

0
0

0.1

 

SPORE−LASSO
SPORE−FoBa

 

SPORE−LASSO
SPORE−FoBa

0.2

0.15

0.1

0.05

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

 

SPORE−LASSO
SPORE−FoBa

0.2

0.15

0.1

0.05

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

0.2

0.3

0.4

Percentage of Training data

0.5

0.6

 

0
0

0.1

0.2

0.3

0.4

Percentage of Training data

0.5

0.6

 

0
0

0.1

0.2

0.3

0.4

Percentage of Training data

0.5

0.6

(a) Lucene

(b) Find Maxima

(c) Segmentation

Figure 1: Prediction errors of our algorithms across the three data sets varying training-set fractions.

4 Evaluation Results

We now experimentally demonstrate that our algorithms are practical  give highly accurate predic-
tors for real problems with small training-set sizes  compare favorably in accuracy to other state-of-
the-art sparse-regression algorithms  and produce interpretable  intuitive models.

To evaluate our algorithms  we use as case studies three programs: the Lucene Search Engine [4] 
and two image processing algorithms  one for ﬁnding maxima and one for segmenting an image
(both of which are implemented within the ImageJ image processing framework [3]). We chose
all three programs according to two criteria. First and most importantly  we sought programs with
high variability in the predicted measure (execution time)  especially in the face of otherwise similar
inputs (e.g.  image ﬁles of roughly the same size for image processing). Second  we sought programs
that implement reasonably complex functionality  for which an inexperienced observer would not
be able to trivially identify the important features.

Our collected datasets are as follows. For Lucene  we used a variety of text input queries from
two corpora: the works of Shakespeare and the King James Bible. We collected a data set with
n = 3840 samples  each of which consists of an execution time and a total of p = 126 automatically
generated features. The time values are in range of (0.88  1.13) with standard deviation 0.19. For
the Find Maxima program within the ImageJ framework  we collected n = 3045 samples (from an
equal number of distinct  diverse images obtained from three vision corpora [1  2  5])  and a total of
p = 182 features. The execution time values are in range of (0.09  2.99) with standard deviation
0.24. Finally  from the Segmentation program within the same ImageJ framework on the same image
set  we collected again n = 3045 samples  and a total of p = 816 features for each. The time values
are in range of (0.21  58.05) with standard deviation 3.05. In all the experiments  we ﬁx degree
d = 3 for polynomial expansion  and normalized each column of feature data into range [0  1].

yi

nt P | ˆyi−yi

Prediction Error. We ﬁrst show that our algorithms predict accurately  even when training on a
small number of samples  in both absolute and relative terms. The accuracy measure we use is the
relative prediction error deﬁned as 1
|  where nt is the size of the test data set  and ˆyi’s
and yi’s are the predicted and actual responses of test data  respectively.
We randomly split every data set into a training set and a test set for a given training-set fraction 
train the algorithms and measure their prediction error on the test data. For each training fraction 
we repeat the “splitting  training and testing” procedure 10 times and show the mean and standard
deviation of prediction error in Figure 1. We see that our algorithms have high prediction accuracy 
even when training on only 10% or less of the data (roughly 300 - 400 samples). Speciﬁcally 
both of our algorithms can achieve less than 7% prediction error on both Lucene and Find Maxima
datasets; on the segmentation dataset  SPORE-FoBa achieves less than 8% prediction error  and
SPORE-LASSO achieves around 10% prediction error on average.

Comparisons to State-of-the-Art. We compare our algorithms to several existing sparse regression
methods by examining their prediction errors at different sparsity levels (the number of features used
in the model)  and show our algorithms can clearly outperform LASSO  FoBa and recently proposed
non-parametric greedy methods [18] (Figure 2). As a non-parametric greedy algorithm  we use Ad-
ditive Forward Regression (AFR)  because it is faster and often achieves better prediction accuracy
than Generalized Forward Regression (GFR) algorithms. We use the Glmnet Matlab implementa-

7

tion of LASSO and to obtain the LASSO solution path [10]. Since FoBa and SPORE-FoBa naturally
produce a path by adding or deleting features (or terms)  we record the prediction error at each step.
When two steps have the same sparsity level  we report the smallest prediction error. To generate
the solution path for SPORE-LASSO  we ﬁrst use Glmnet to generate a solution path for linear
LASSO; then at each sparsity level k  we perform full polynomial expansion with d = 3 on the
selected k features  obtain a solution path on the expanded data  and choose the model with the
smallest prediction error among all models computed from all active feature sets of size k. From the
ﬁgure  we see that our SPORE algorithms have comparable performance  and both of them clearly
achieve better prediction accuracy than LASSO  FoBa  and AFR. None of the existing methods can
build models within 10% of relative prediction error. We believe this is because execution time of a
computer program often depends on non-linear combinations of different features  which is usually
not well-handled by either linear methods or the additive non-parametric methods. Instead  both of
our algorithms can select 2-3 high-quality features and build models with non-linear combinations
of them to predict execution time with high accuracy.

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

0.5

0.4

0.3

0.2

0.1

 

0
1

 

LASSO
FoBa
AFR
SPORE−LASSO
SPORE−FoBa

2

3

4

Sparsity

5

6

7

(a) Lucene

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

0.5

0.4

0.3

0.2

0.1

 

0
1

 

LASSO
FoBa
AFR
SPORE−LASSO
SPORE−FoBa

2

3

4

Sparsity

5

6

7

(b) Find Maxima

r
o
r
r

 

E
n
o

i

i
t
c
d
e
r
P

0.5

0.4

0.3

0.2

0.1

 

0
1

 

LASSO
FoBa
AFR
SPORE−LASSO
SPORE−FoBa

2

3

4

Sparsity

5

6

7

(c) Segmentation

Figure 2: Performance of the algorithms: relative prediction error versus sparsity level.

Model Interpretability. To gain better understanding  we investigate the details of the model con-
structed by SPORE-FoBa for Find Maxima. Our conclusions are similar for the other case studies 
but we omit them due to space. We see that with different training set fractions and with different
sparsity conﬁgurations  SPORE-FoBa can always select two high-quality features from hundreds of
automatically generated ones. By consulting with experts of the Find Maxima program  we ﬁnd that
the two selected features correspond to the width (w) and height (h) of the region of interest in the
image  which may in practice differ from the actual image width and height. Those are indeed the
most important factors for determining the execution time of the particular algorithm used. For a
10% training set fraction and ǫ = 0.01  SPORE-FoBa obtained

f (w  h) = 0.1 + 0.22w + 0.23h + 1.93wh + 0.24wh2

which uses non-linear feature terms(e.g.  wh  wh2) to predict the execution time accurately (around
5.5% prediction error). Especially when Find Maxima is used as a component of a more complex
image processing pipeline  this model would not be the most obvious choice even an expert would
pick. On the contrary  as observed in our experiments  neither the linear nor the additive sparse
methods handle well such nonlinear terms  and result in inferior prediction performance. A more
detailed comparison across different methods is the subject of our on-going work.

5 Conclusion

In this paper  we proposed the SPORE (Sparse POlynomial REgression) methodology to build the
relationship between execution time of computer programs and features of the programs. We in-
troduced two algorithms to learn a SPORE model  and showed that both algorithms can predict
execution time with more than 93% accuracy for the applications we tested. For the three test cases 
these results present a signiﬁcant improvement (a 40% or more reduction in prediction error) over
other sparse modeling techniques in the literature when applied to this problem. Hence our work
provides one convincing example of using sparse non-linear regression techniques to solve real
problems. Moreover  the SPORE methodology is a general methodology that can be used to model
computer program performance metrics other than execution time and solve problems from other
areas of science and engineering.

8

References
[1] Caltech 101 Object Categories.

Caltech101/Caltech101.html.

http://www.vision.caltech.edu/Image_Datasets/

[2] Event Dataset. http://vision.stanford.edu/lijiali/event_dataset/.
[3] ImageJ. http://rsbweb.nih.gov/ij/.
[4] Mahout. lucene.apache.org/mahout.
[5] Visual Object Classes Challenge 2008. http://pascallin.ecs.soton.ac.uk/challenges/

VOC/voc2008/.

[6] S. Chen  K. Joshi  M. A. Hiltunen  W. H. Sanders  and R. D. Schlichting. Link gradients: Predicting the

impact of network latency on multitier applications. In INFOCOM  2009.

[7] B.-G. Chun  L. Huang  S. Lee  P. Maniatis  and M. Naik. Mantis: Predicting system performance through

program analysis and modeling. Technical Report  2010. arXiv:1010.0019v1 [cs.PF].

[8] D. Donoho. For most large underdetermined systems of equations  the minimal 1-norm solution is the

sparsest solution. Communications on Pure and Applied Mathematics  59:797829  2006.

[9] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics 

32(2):407–499  2002.

[10] J. Friedman  T. Hastie  and R. Tibshirani. Regularization paths for generalized linear models via coordi-

nate descent. Journal of Statistical Software  33(1)  2010.

[11] A. Ganapathi  H. Kuno  U. Dayal  J. L. Wiener  A. Fox  M. Jordan  and D. Patterson. Predicting multiple

metrics for queries: Better decisions enabled by machine learning. In ICDE  2009.

[12] S. Goldsmith  A. Aiken  and D. Wilkerson. Measuring empirical computational complexity.

2007.

In FSE 

[13] C. Gupta  A. Mehta  and U. Dayal. PQR: Predicting query execution times for autonomous workload

management. In ICAC  2008.

[14] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer  2009.
[15] M. Isard  V. Prabhakaran  J. Currey  U. Wieder  K. Talwar  and A. Goldberg. Quincy: fair scheduling for

distributed computing clusters. In Proceedings of SOSP’09  2009.

[16] S.-J. Kim  K. Koh  M. Lustig  S. Boyd  and D. Gorinevsky. An interior-point method for large-scale
l1-regularized least squares. IEEE Journal on Selected Topics in Signal Processing  1(4):606–617  2007.
[17] Z. Li  M. Zhang  Z. Zhu  Y. Chen  A. Greenberg  and Y.-M. Wang. WebProphet: Automating performance

prediction for web services. In NSDI  2010.

[18] H. Liu and X. Chen. Nonparametric greedy algorithm for the sparse learning problems. In NIPS 22  2009.
[19] M. Osborne  B. Presnell  and B. Turlach. On the lasso and its dual. Journal of Computational and

Graphical Statistics  9(2):319–337  2000.

[20] P. Ravikumar  J. Lafferty  H. Liu  and L. Wasserman. Sparse additive models. Journal of the Royal

Statistical Society: Series B(Statistical Methodology)  71(5):1009–1030  2009.

[21] P. Ravikumar  V. Vu  B. Yu  T. Naselaris  K. Kay  J. Gallant  and C. Berkeley. Nonparametric sparse hier-
archical models describe v1 fmri responses to natural images. Advances in Neural Information Processing
Systems (NIPS)  21  2008.

[22] S. A. Seshia and A. Rakhlin. Game-theoretic timing analysis. In Proceedings of the IEEE/ACM Interna-

tional Conference on Computer-Aided Design (ICCAD)  pages 575–582. IEEE Press  Nov. 2008.

[23] S. A. Seshia and A. Rakhlin. Quantitative analysis of systems using game-theoretic learning. ACM

Transactions on Embedded Computing Systems (TECS)  2010. To appear.

[24] M. Tariq  A. Zeitoun  V. Valancius  N. Feamster  and M. Ammar. Answering what-if deployment and

conﬁguration questions with wise. In ACM SIGCOMM  2008.

[25] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B.  1996.
[26] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained

quadratic programming (Lasso). IEEE Trans. Information Theory  55:2183–2202  2009.

[27] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances

in Neural Information Processing Systems  22  2008.

[28] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research 

7:2563  2006.

[29] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association 

101(476):1418–1429  2006.

9

,Robert Lindsey
Michael Mozer
William Huggins
Harold Pashler
Julien Audiffren
Liva Ralaivola