2019,Statistical Model Aggregation via Parameter Matching,We consider the problem of aggregating models learned from sequestered  possibly heterogeneous datasets. Exploiting tools from Bayesian nonparametrics  we develop a general meta-modeling framework that learns shared global latent structures by identifying correspondences among local model parameterizations. Our proposed framework is model-independent and is applicable to a wide range of model types. After verifying our approach on simulated data  we demonstrate its utility in aggregating Gaussian topic models  hierarchical Dirichlet process based hidden Markov models  and sparse Gaussian processes with applications spanning text summarization  motion capture analysis  and temperature forecasting.,Statistical Model Aggregation via

Parameter Matching

Mikhail Yurochkin1 2

mikhail.yurochkin@ibm.com

Mayank Agarwal1 2

mayank.agarwal@ibm.com

Soumya Ghosh1 2 3
ghoshso@us.ibm.com

Kristjan Greenewald1 2

kristjan.h.greenewald@ibm.com

Trong Nghia Hoang1 2

nghiaht@ibm.com

IBM Research 1 MIT-IBM Watson AI Lab 2 Center for Computational Health3.

Abstract

We consider the problem of aggregating models learned from sequestered  pos-
sibly heterogeneous datasets. Exploiting tools from Bayesian nonparametrics 
we develop a general meta-modeling framework that learns shared global latent
structures by identifying correspondences among local model parameterizations.
Our proposed framework is model-independent and is applicable to a wide range
of model types. After verifying our approach on simulated data  we demonstrate its
utility in aggregating Gaussian topic models  hierarchical Dirichlet process based
hidden Markov models  and sparse Gaussian processes with applications spanning
text summarization  motion capture analysis  and temperature forecasting.1

1

Introduction

One is often interested in learning from groups of heterogeneous data produced by related  but
unique  generative processes. For instance  consider the problem of discovering shared topics from a
collection of documents  or extracting common patterns from physiological signals of a cohort of
patients. Learning such shared representations can be relevant to many heterogeneous  federated  and
transfer learning tasks. Hierarchical Bayesian models [3  12  29] are widely used for performing such
analyses  as they are able to both naturally model heterogeneity in data and share statistical strength
across heterogeneous groups.
However  when the data is large and scattered across disparate silos  as is increasingly the case in
many real-world applications  use of standard hierarchical Bayesian machinery becomes fraught with
difﬁculties. In addition to costs associated with moving large volumes of data  the computational
cost of full Bayesian inference may be prohibitive. Moreover  pooling sequestered data may also
be undesirable owing to concerns such as privacy [11]. While distributed variants [18] have been
developed  they require frequent communication with a central server and hence are restricted to
situations where sufﬁcient communication bandwidth is available. Yet others [26] have proposed
federated learning algorithms to deal with such scenarios. However  these algorithms tend to be
bespoke and can require signiﬁcant modiﬁcations based on the models being federated.
Motivated by these challenges  in this paper we develop Bayesian nonparametric meta-models that
are able to coherently combine models trained on independent partitions of data (model fusion).
Relying on tools from Bayesian nonparametrics (BNP)  our meta model treats the parameters of
the locally trained models as noisy realizations of latent global parameters  of which there can be

1Code: https://github.com/IBM/SPAHM

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

inﬁnitely many. The generative process is formally characterized through a Beta-Bernoulli process
(BBP) [31]. Model fusion  rather than being an ad-hoc procedure  then reduces to posterior inference
over the meta-model. Governed by the BBP posterior  the meta-model allows local parameters to
either match existing global parameters or create new ones. This ability to grow or shrink the number
of parameters is crucial for combining local models of varying complexity – for instance  hidden
Markov models with differing numbers of states.
Our construction provides several key advantages over alternatives in terms of scalability and
ﬂexibility. First  scaling to large data through parallelization is trivially easy in our framework. One
would simply train the local models in parallel and fuse them. Armed with a Hungarian algorithm-
based efﬁcient MAP inference procedure for the BBP model  we ﬁnd that our train-in-parallel and fuse
scheme affords signiﬁcant speedups. Since our model fusion procedure is independent of the learning
and inference algorithms that may have been used to train individual models  we can seamlessly
combine models trained using disparate algorithms. Furthermore  since we only require access to
trained local models and not the original data  our framework is also applicable in cases where only
pre-trained models are available but not the actual data  a setting that is difﬁcult for existing federated
or distributed learning algorithms.
Finally  we note that our development is largely agnostic to the form of the local models and is reusable
across a wide variety of domains. In fact  up to the choice of an appropriate base measure to describe
the local parameters  the exact same algorithm can be used for fusion across qualitatively different
settings. We illustrate this ﬂexibility by demonstrating proﬁciency at combining a diverse class of
models  which include sparse Gaussian processes  mixture models  topic models and hierarchical
Dirichlet process based hidden Markov models.

2 Background and Related Work

Here  we brieﬂy review the building blocks of our approach and highlight the differences of our
approach from existing work.

Indian Buffet Process and the Beta Bernoulli Process The Indian buffet process (IBP) speciﬁes a
distribution over sparse binary matrices with inﬁnitely many columns [17]. It is commonly described
through the following culinary metaphor. Imagine J customers arrive sequentially at a buffet and
choose dishes to sample. The ﬁrst customer to arrive samples Poisson(γ0) dishes. The j-th subsequent
customer then tries each of the dishes selected by previous customers with probability proportional to
the dish’s popularity  and then additionally samples Poisson(γ0/j) new dishes that have not yet been
sampled by any customer. Thibaux and Jordan [31] showed that the de Finetti mixing distribution
underlying the IBP is a Beta Bernoulli Process (BBP). Let Q be a random measure drawn from
a Beta process  Q | α  γ0  H ∼ BP(α  γ0H)  with mass parameter γ0  base measure H over Ω
such that H(Ω) = 1 and concentration parameter α. It can be shown Q is a discrete measure
i qiδθi formed by an inﬁnitely countable set of (weight  atom) pairs (qi  θi) ∈ [0  1] × Ω. The
weights {qi}∞
j=1 νj
and the atoms θi are drawn i.i.d. from H. Subsets of atoms in Q are then selected via a Bernoulli
process. That is  each subset Tj with j = 1  . . .   J is characterized by a Bernoulli process with
base measure Q  Tj | Q ∼ BeP(Q). Consequently  subset Tj is also a discrete measure formed by
i bjiδθi  where bji | qi ∼ Bernoulli(qi)∀i is a binary random
variable indicating whether atom θi belongs to subset Tj. The collection of such subsets is then said
to be distributed by a Beta-Bernoulli process. Marginalizing over the Beta Process distributed Q we
recover the predictive distribution  TJ | T1  . . .  TJ−1 ∼ BeP
  where
j=1 bji (dependency on J is suppressed for notational simplicity) which can be shown to be
equivalent to the IBP. Our work is related to recent advances [33] in efﬁcient BBP MAP inference.

Q =(cid:80)
pairs (bji  θi) ∈ {0  1} × Ω  Tj :=(cid:80)
mi =(cid:80)J−1

i=1 are distributed by a stick-breaking process [30]  ν1 ∼ Beta(γ0  1)  νi =(cid:81)i

(cid:16) αγ0
J+α−1 H +(cid:80)

(cid:17)

mi

J+α−1 δθi

i

Distributed  Decentralized  and Federated Learning Similarly to us  federated and distributed
learning approaches also attempt to learn from sequestered data. These approaches roughly fall
into two groups  those [9  15  21  22  23] that decompose a global  centralized learning objective
into localized ones that can be optimized separately using local data  and those that iterate between
training local models on private data sources and distilling them into a global model [8  6  18  26].
The former group carefully exploits properties of the local models being combined. It is unclear how

2

methods developed for a particular class of local models (for example  Gaussian processes) can be
adapted to a different class of models (say  hidden Markov models). More recently  [32] also exploited
a BBP construction for federated learning  but were restricted to only neural networks. Alternatively 
[20] follows a different development that requires local models of different classes to be distilled
into the same class of surrogate models before aggregating them  which  however  accumulates local
distillation error (especially when the number of local models is large). Members of the latter group
require frequent communication with a central server  are poorly suited to bandwidth limited cases 
and are not applicable when the pretrained models cannot share their associated data. Others [5] have
proposed decentralized approximate Bayesian algorithms. However  unlike us  they assume that each
of the local models have the same number of parameters  which is unsuitable for federating models
with different complexities.

3 Bayesian Nonparametric Meta Model

We propose a Bayesian nonparametric meta model based on the Beta-Bernoulli process [31]. In
seeking a “meta model”  our goal will be to describe a model that generates collections of parameters
that describe the local models. This meta model can then be used to infer the parameters of a global
model from a set of local models learned independently on private datasets.
Our key assumption is that there is an unknown shared set of parameters of unknown size across
datasets  which we call global parameters  and we are able to learn subsets of noisy realizations
of these parameters from each of the datasets  which we call local parameters. The noise in local
parameters is motivated by estimation error due to ﬁnite sample size and by variations in the
distributions of each of the datasets. Additionally  local parameters are allowed to be permutation
(cid:80)
invariant  which is the case in a variety of widely used models (e.g.  any mixture or an HMM).
We start with Beta process prior on the collection of global parameters  G ∼ BP(α  γ0H) then G =
i piδθi  θi ∼ H  where H is a base measure  θi are the global parameters  and pi are the stick
breaking weights. To devise a meta-model applicable to broad range of existing models  we do not
assume any speciﬁc base measure and instead proceed with general exponential family base measure 

pθ(θ | τ  n0) = H(τ  n0) exp(τ T θ − n0A(θ)).

(1)
Local models do not necessarily have to use all global parameters  e.g. a Hidden Markov Model for a
given time series may only contain a subset of latent dynamic behaviors observed across collection of
time series. We use a Bernoulli process to allow J local models to select a subset of global parameters 

Then Qj =(cid:80)

Qj | G ∼ BeP(G) for j = 1  . . .   J.

(2)
i bjiδθi  where bji | pi ∼ Bern(pi) is a random measure representing the subset of
global parameters characterizing model j. We denote the corresponding subset of indices of the
global parameters induced by Qj as Cj = {i : bji = 1}. The noisy  permutation invariant local
parameters estimated from dataset j are modeled as 

vjl | θc(j l) ∼ F (· | θc(j l)) for l = 1  . . .   Lj 

(3)
where Lj = card(Cj) and c(j  l) : {1  . . .   LJ} → Cj is an unknown mapping of indices of local
parameters to indices of global parameters corresponding to dataset j. Parameters of different models
of potential interest may have different domain spaces and domain-speciﬁc structure. To preserve
generality of our meta-modeling approach we again consider a general exponential family density for
the local parameters 

where T (·) is the sufﬁcient statistic function.

pv(v | θ) = h(v) exp(θT T (v) − A(θ)).

(4)

Interpreting the model. We emphasize that our construction describes a meta model  in particular
it describes a generative process for the parameters of the local models rather than the data itself.
These parameters are “observed” either when pre-trained local models are made available or when the
local models are learned independently and potentially in parallel across datasets. The meta model
then infers shared latent structure among the datasets. The Beta process concentration parameter α
controls the degree of sharing across local models while the mass parameter γ0 controls the number
of global parameters. The interpretation of the exponential family parameters  τ and n0  depends on

3

the choice of the particular exponential family. We provide a concrete example with the Gaussian
distribution in Section 4.1.
Several prior works [10  19  4] explore the meta modeling perspective. The key difference with
our approach is that we consider broader model class allowing for inherent permutation invariant
structure of the parameter space  e.g. mixture models  topic models  hidden Markov models and
sparse Gaussian processes. The aforementioned approaches are only applicable to models with
natural parameter ordering  e.g. linear regression  which is a simpler special case of our construction.
Permutation invariance leads to inferential challenges associated with ﬁnding correspondences across
sets of local parameters and learning the size of the global model  which we address in the next
section.

4 Efﬁcient Meta Model Inference

Taking the optimization perspective  our goal is to maximize the posterior probability of the global
parameters given the local ones. Before discussing the objective function we re-parametrize (4) to
side-step the index mappings c(· ·) as follows:

(cid:32)
· |(cid:88)

(cid:33)

s.t. (cid:88)

Bj

ilθi

vjl | B  θ ∼ F

(cid:88)

Bj

il = 1  bji =

il ∈ {0  1} 
Bj

(5)

i

i

l

il}i j l are the assignment variables such that Bj

where B = {Bj
il = 1 denotes that vjl is matched to
θi  i.e. vjl is the local parameter realization of the global parameter θi; Bj
il = 0 implies the opposite.
The objective function is then P(θ  B | v  Θ)  where Θ = {τ  n0} are the hyperparameters and
indexing is suppressed for simplicity. In the context of our meta model this problem has been
studied when distributions in (1) and (4) are Gaussian [32] or von Mises-Fisher [33]  which are both
special cases of our meta model. However  this objective requires Θ to be chosen a priori leading to
potentially sub-optimal solutions or to be selected via expensive cross-validation.
We show that it is possible to simplify the optimization problem via integrating out θ and jointly learn
hyperparameters and matching variables B  all while maintaining the generality of our meta model.
Deﬁne Zi = {(j  l) | Bj
il = 1} to be the index set of the local parameters assigned to the ith global
parameter  then the objective functions we consider is 
L(B  Θ) = P(B | v) ∝ P(B)

pv(v | B  θ)pθ(θ) dθ = P(B)

pv(vz | θi)pθ(θi) dθi

(cid:90) (cid:89)

(cid:89)

(cid:90)
(cid:90) (cid:89)
H(τ  n0)(cid:81)

z∈Zi

(cid:89)
(cid:89)

i

H(τ  n0)

H(τ +(cid:80)

= P(B)

T (vz))T )θi − (card(Zi) + n0)A(θ)

(6)

 exp

(τ +

(cid:88)

z∈Zi

h(vz)

i

z∈Zi

 dθi

= P(B)

Holding Θ ﬁxed  and then taking the logarithm and noting that(cid:80)

T (vz)  card(Zi) + n0)

z∈Zi

 

i

z∈Zi

h(vz)

B  we wish to maximize 

LΘ(B) = log P(B) −(cid:88)

log H

i

τ +

(cid:88)

j l

(cid:80)
(cid:88)

j l

  

Bj

ilT (vjl) 

Bj

il + n0

(7)

j l Bj

il log h(vjl) is constant in

i

where we have used LΘ(B) to denote the objective when Θ is held constant. Despite the large
number of discrete variables  we show that LΘ(B) admits a reformulation that permits efﬁcient
inference by iteratively solving small sized linear sum assignment problems (e.g.  the Hungarian
algorithm [25]).
We consider iterative optimization where we optimize the assignments Bj0 for some group j0 given
j l Bj
il denote
number of local parameters assigned to the global parameter i  m
il be the same
outside of group j0 and let L\j0 denote the number of unique global parameters corresponding to the

that the assignments for all other groups  denoted B\j0  are held ﬁxed. Let mi =(cid:80)

i =(cid:80)

j(cid:54)=j0 l Bj

\j0

4

L\j0

\j0 )

+Lj0(cid:88)

τ +

local parameters outside of j0. The corresponding objective functions are given by
LB\j0  Θ(Bj0 ) = log P(Bj0 | B
log H

(cid:88)
Proposition 1 (Subtraction trick). When(cid:80)
(cid:80)
i f ((cid:80)

l Bilxl + C) for B is equivalent to optimizing(cid:80)

To arrive at a form of a linear sum assignment problem we deﬁne a subtraction trick:

il T (vj0l) +

(cid:88)

(cid:88)

l Bil ∈ {0  1} and Bil ∈ {0  1} for ∀ i  l  optimizing
i l Bil(f (xl + C)− f (C)) for any function

 .

(8)

\j0
i + n0

ilT (vjl) 

il + m

j(cid:54)=j0 l

Bj0

Bj

Bj

−

i=1

l

l

f  {xl} and C independent of B.

Proof. This result simply follows by observing that both objectives are equal for any values of B
satisfying the constraint.

Applying the subtraction trick to (8) (conditions on B are satisﬁed per (5))  we arrive at a linear sum

assignment formulation LB\j0  Θ(Bj0 ) = −(cid:80)
i l Bj0
H(cid:16)
τ +T (vj0l)+(cid:80)
H(cid:16)
τ +(cid:80)
α+J−1−m
(α+J−1)(i−L\j0 ) − log

log

− log

H(τ n0)

il =

C j0

log

\j0

\j0

αγ0

m
i

i

il C j0

il   where the cost
(cid:17)
\j0
i +n0

ilT (vjl) 1+m

j(cid:54)=j0 l Bj

(cid:17)

j(cid:54)=j0  l Bj
H(τ +T (vj0l) 1+n0)

ilT (vjl) m

 

\j0
i +n0

 

i ≤ L\j0
L\j0 < i ≤ L\j0 + Lj.

(9)

Terms on the left are due to log P(Bj0 | B\j0 ). Details are provided in the supplement. Our algorithm
consists of alternating the Hungarian algorithm with the above cost and hyperparameter optimization
using log P(B | v) from (6)  ignoring P(B) as it is a constant with respect to hyperparameters.
Speciﬁcally  the hyperparameter optimization step is

L(cid:88)

log H(τ  n0) − log H

τ +

(cid:88)

(cid:88)

  

Bj

ilT (vjl) 

Bj

il + n0

(10)

ˆτ   ˆn0 = arg max

τ n0

i=1

j l

j l

where B is held ﬁxed. After obtaining estimates for B and the hyperparameters Θ  it only re-
i=1|B  v  Θ). Given the
mains to compute global parameters estimates {θi}L
assignments  expressions for hyperparameter and global parameter estimates can be obtained using
gradient-based optimization. In Section 4.1 we give a concrete example where derivations may be
done in closed form. Our method  Statistical Parameter Aggregation via Heterogeneous Matching
(SPAHM  pronounced “spam”)  is summarized as Algorithm 1.

i=1 = arg max
θ1 ... θL

P({θi}L

Algorithm 1 Statistical Parameter Aggregation via Heterogeneous Matching (SPAHM)
input Observed local vjl  iterations number M  initial hyperparameter guesses ˆτ   ˆn0.
1: while not converged do
for M iterations do
2:
3:
4:
5:
6:
7:
8: end while
output Matching assignments B  global atom estimates θi.

j ∼ Unif({1  . . .   J}).
Form matching cost matrix C j using eq. (9).
Use Hungarian algorithm to optimize assignments Bj  holding all other assignments ﬁxed.

end for
Given B  optimize (10) to update hyperparameters ˆτ   ˆn0.

4.1 Meta Models with Gaussian Base Measure

We present an example of how a statistical modeler may apply SPAHM in practice. The only choice
modeler has to make is the prior over parameters of their local models  i.e. (4). In many practical
scenarios (as we will demonstrate in the experiments section) model parameters are real-valued and
the Gaussian distribution is a reasonable choice for the prior on the parameters. The Gaussian case is

5

further of interest as it introduces additional parameters. For simplicity we consider the 1-dimensional
Gaussian  which is also straightforward to generalize to multi-dimensional isotropic case.
The modeler starts by writing the density

Here the subscript σ indicates dependence on the additional parameter  i.e. variance. Next 

pv(v | θ  σ2) =

hσ(v) =

1√
2πσ2

1√
2πσ2

(cid:18)

exp

exp

− v2
2σ2

(cid:18)(cid:90)

(cid:18)

Hσ(τ  n0) =

exp

τ θ − n0θ2
2σ2

to ensure pθ(θ|τ  n0) integrates to unity. Hence 
log Hσ(τ  n0) = − τ 2σ2
2n0

(cid:19)

2σ2

(cid:18)
− (v − θ)2
(cid:19)
(cid:19)

(cid:19)−1

dθ

=

  Tσ(v) =

and noticing that

v

σ2   Aσ(v) =
(cid:19)(cid:115)
(cid:18) τ 2σ2

exp

2n0

θ2
2σ2 .

2πσ2

n0

−1

log n0 − log σ2 − log 2π

2

.

+

(cid:80)

These are all we need to customize (9) and (10) to the Gaussian case  which then allows the modeler
to use Algorithm 1 to compute the shared parameters across the datasets. Note that in this case
j l log hσ(vjl) should be added to eq. (10) if it is desired to learn the additional parameter σ2. We
recognize that not every exponential family allows for closed form evaluation of the prior normalizing
constant Hσ(τ  n0)  however it remains possible to use SPAHM by employing Monte Carlo techniques
for estimating entries of the cost (9) and auto-differentiation [2] to optimize hyperparameters.
0 we recover (1)
Continuing our Gaussian example  we note that setting τ = µ0/σ2
as a density of a Gaussian random variable with mean µ0 and variance σ2
0  as expected. The further
beneﬁt of the Gaussian choice is the closed-form solution to the hyperparameters estimation problem.
0 ∀i (i.e.  global parameters are sufﬁciently distant
Under the mild assumption that σ2
L(cid:88)
from each other in comparison to the noise in the local parameters) we obtain

0 + σ2/mi ≈ σ2

0 and n0 = σ2/σ2

ilvjl)2

1

(cid:88)

jl − ((cid:80)

Bj

ilv2

i=1

j l

i=1

1
L

ilvjl

ˆσ2
0 =

− ˆµ0

j l Bj
mi

− L(cid:88)
where N =(cid:80)
by setting corresponding derivatives of eq. (10) +(cid:80)
(cid:80)

ˆσ2
mi

i=1

j Lj is the total number of observed local parameters. The result may be veriﬁed
j l log hσ(vjl) to 0 and solving the system of
equations. Derivations are long but straightforward. Given assignments B  our example reduces to a
hierarchical Gaussian model – see Section 5.4 of Gelman et al. [16] for analogous hyperparameter
derivations. Finally we obtain

 

(cid:88)
(cid:32)(cid:80)

1
mi

j l

L(cid:88)
L(cid:88)

i=1

ˆσ2 =

(cid:33)2

ˆµ0 =

1
L

  

j l Bj
mi

N − L

Bj

ilvjl 

For completeness we provide cost expression corresponding to eq. (9): C j0

2 log

\j0

m
i

α+J−1−m

+ log

\j0

i

m

1+m

µ0σ2 + σ2
0

j l Bj

ilvjl

.

θi =

(cid:18)

σ2 + miσ2
0

σ2 +(cid:80)

vj0 l

+

1+m

0 +vj0l/σ2)2σ2
1+σ2/σ2
0

il

j(cid:54)=j0 l Bj
\j0
i + σ2
σ2
0

− µ2

+

µ0
σ2
0

\j0
i + σ2
σ2
0
\j0
i + σ2
σ2
0
0 +σ2 + (µ0/σ2
σ2

(cid:19)2

vjl
σ2

σ2

(cid:18)

il =
µ0
−
σ2
0

+(cid:80)

il

j(cid:54)=j0 l Bj
\j0
i + σ2
m
σ2
0

(cid:19)2

vjl
σ2

σ2

 

αγ0

σ2

(α+J−1)(i−L\j0 ) + log

2 log
where ﬁrst case is for i ≤ L\j0 and second is for L\j0 < i ≤ L\j0 + Lj.
4.2 Convergence Analysis
Lemma 1 (Algorithmic convergence). Algorithm 1 creates a sequence of iterates for which log P(B |
v) converges as the number of iterations n → ∞. See Section B of the supplement for a proof sketch.

(11)

0
σ2
0

 

6

Hyperparameter Consistency. While the exponential family hyperparameter objective function
(10) is too general to be tractable  the consistency of the hyperparameter estimates can be analyzed
for speciﬁc choices of distributional families. Following the specialization to Gaussian distributions
in Section 4.1  the following result establishes that the closed-form hyperparameter estimates are
consistent in the case of Gaussian priors  subject to the assignments B being correct.
Theorem 1. Assume that the binary assignment variables B are known or estimated correctly. The
estimator for ˆµ0 for the hyperparameter µ0 in the Gaussian case is then consistent as the number of
global atoms L → ∞. Furthermore  the estimators ˆσ2
0 and σ2
j l Bj
il) >
1) → ∞  where I(·) is the indicator function. See Section C of the supplement for a detailed proof.

are consistent as the total number of global atoms with multiple assignments(cid:80)L

0 and ˆσ2 for the hyperparameters σ2

i=1 I(((cid:80)

5 Experiments

Simulated Data. We begin with a correctness veriﬁcation of our
inference procedure via a simulated experiment. We randomly sam-
ple L = 50 global centroids θi ∈ R50 from a Gaussian distribution
θi ∼ N (µ0  σ2
0I). We then simulate j = 1  . . .   J heterogeneous
datasets by picking a random subset of global centroids and adding
white noise with variance σ2 to obtain the “true” local centroids 
{vjl}Lj
l=1 (following generative process in Section 3 with Gaussian
densities). Then each dataset is sampled from a Gaussian mixture
model with the corresponding set of centroids. We want to esti-
mate global centroids and parameters µ0  σ2
0  σ2. We consider two
basic baselines: k-means clustering of all datasets pooled into one
(k-means pooled) and k-means clustering of local centroid estimates
(this can be seen as another form of parameter aggregation - i.e.
k-means “matching”). Both  unlike SPAHM  enjoy access to true L.
To obtain local centroid estimates for SPAHM and k-means “match-
ing”  we run (another) k-means on each of the simulated datasets.
Additionally to quantify how local estimation error may effect our
approach  we compare to SPAHM using true data generating lo-
cal centroids. To measure the quality of different approaches we
evaluate Hausdorff distance between the estimates and true data
generating global centroids. This experiment is presented in Figures
1 and 2. White noise variance σ implies degree of heterogeneity
across J = 20 datasets and as it grows the estimation problem becomes harder  however SPAHM
degrades more gracefully than baselines. Fixing σ2 = 1 and varying number of datasets J may make
the problem harder as there is more overlap among the datasets  however SPAHM is able to maintain
low estimation error. We empirically verify hyperparameter estimation quality in the supplement.

Figure 1: Increasing noise σ

Figure 2: Increasing J

Gaussian Topic Models. We present a practical scenario where problem similar to our simulated
experiment arises — learning Gaussian topic models [7] where local topic models are learned from
the Gutenberg dataset comprising 40 books. We then build the global topic model using SPAHM.
We use basic k-means with k = 25 to cluster word embeddings of words present in a book to obtain
local topics and then apply SPAHM resulting in 155 topics. We compare to the Gibbs sampler of
Das et al. [7] in terms of the UCI coherence score [27]  −2.1 for SPAHM and −4.6 for [7]  where
higher is better. Besides  [7] took 16 hours to run 100 MCMC iterations while SPAHM + k-means
takes only 40 seconds  over 1400 times faster. We present topic interpretation in Fig. 3 and defer
additional details to the Supplement.

Gaussian Processes. We next demonstrate the effectiveness of our approach on the task of temper-
ature prediction using Sparse Gaussian Processes (SGPs) [1]. For this task  we utilize the GSOD data
available from the National Oceanic and Atmospheric Administration2 containing the daily global
surface weather summary from over 9000 stations across the world. We limit the geography of our
data to the United States alone and also ﬁlter the observations to the year 2015 and after. We further

2https://data.noaa.gov/dataset/dataset/global-surface-summary-of-the-day-gsod

7

0246810σ020406080Hausdorff DistanceSPAHM (true local params)SPAHM (estimated local params)K-Means (Pooled)K-Means "matching"20406080100J010203040506070Housdorff DistanceSPAHM (true local params)SPAHM (estimated local params)K-Means (Pooled)K-Means "matching"Figure 3: Topic related to war found by SPAHM and Gaussian LDA. The ﬁve boxes pointing to the
Matched topic represent local topics that SPAHM fused into the global one. The headers of these ﬁve
boxes state the book names along with their Gutenberg IDs.

Table 1: Temperature prediction using sparse Gaussian Processes
EXPERIMENT SETUP

RMSE (ACROSS USA)

GROUP SGPS WITH 50 LOCAL PARAMETERS
SPAHM WITH 289 ± 9.8 GLOBAL PARAMETERS
GROUP SGPS WITH 300 LOCAL PARAMETERS

RMSE (SELF)
5.509 ± 0.0135
5.860 ± 0.0390
5.267 ± 0.0084

14.565 ± 0.0528
8.917 ± 0.1988
15.848 ± 0.0303

select the following 7 features to create the ﬁnal dataset - date (day  month  year)  latitude  longitude 
and elevation of the weather stations  and the previous day’s temperature. We consider states as
datasets of wheather stations observations.
We proceed by training SGPs on each of the 50 states data and evaluate it on the test set consisting of
a random subset drawn from all states. Such locally trained SGPs do not generalize well beyond their
own region  however we can apply SPAHM to match local inducing points along with their response
values and pass it back to each of the states. Using inducing points found by SPAHM  local GPs gain
ability to generalize across the continent while maintaining comparable ﬁt on its own test data (i.e.
test data sampled only from a corresponding state). We summarize the results across 10 experiment
repetitions in Table 1. In addition  we note that Bauer et al. [1] previously showed that increasing
number of inducing inputs tends to improve performance. To ensure this is not the reason for strong
performance of SPAHM we also compare to local SGPs trained with 300 inducing points each.

Hidden Markov Models. Next  we consider the problem of discovering common structure
in collections of related MoCAP sequences collected from the CMU MoCap database (http:
//mocap.cs.cmu.edu). We used a curated subset [14] of the data from two different subjects
each providing three sequences. This subset comes with human annotated labels which allow quan-
titative comparisons. We performed our experiments on this annotated subset. For each subject 
we trained an independent ‘sticky’ HDP-HMM [13] with Normal Wishart likelihoods. We used
memoized variational inference [24] with random restarts and merge moves to alleviate local optima
issues (see supplement for details about parameter settings and data pre-processing). The trained
models discovered nine states for the ﬁrst subject and thirteen for the second. We then used SPAHM
to match local HDP-HMM states across subjects and recovered fourteen global states. The matching
was done on the basis of the posterior means of the local states.
The matched states are visualized in Figure 4 (right) and additional visualizations are available in
the supplement. We ﬁnd that SPAHM correctly recognizes similar activities across subjects. It
also creates singleton states when there are no good matches. For instance  “up-downs”  an activity
characterized by distinct motion patterns is only performed by the second subject. We correctly do
not match it with any of the activities performed by the ﬁrst subject. The ﬁgure also illustrates a
limitation of our procedure wherein poor local states can lead to erroneous matches. Global states ﬁve
and six are a combination of “toe-touches” and “twists”. State ﬁve combines exaggerated motions to
the right while state six is a combination of states with motions to the left. Although the toe-touch

8

enemywarforcesﬁghtingalliedarmedmilitaryinvasionenemies10409: The Crisis of the Naval Warcommandcorpsforcearmymilitarycommandedalliedpersonnelnaval30047: Aircraft and Submarinesmilitaryforceforcesarmycommandpersonneloperationsarmedallied793: Aeroplanes & Dirigibles of Wararmedsoldiersattackedforcesarmyﬁghtingﬁrecapturedtroops22523: History of the American...enemyalliedforcescapturedattackingarmedforcecommandersarmy26879: Night Bombing with the ...armytakenforceenteredbroughttookarmedcarriedmilitarycapturedalliedattackedforcescamebringingGaussian LDA Topic 16armymilitaryforcesarmedalliedcommandcommandersciviliancapturedﬁghtingattackedtakenenemycarriedtroopsMatched Topic 34Figure 4: BBP discovers coherent global structure from MoCap Sequences. We analyze three
MoCAP sequences each from two subjects performing a series of exercises. Some exercises are shared
between subjects while others are not. Two HDP-HMMs were ﬁt to explain the sequences belonging
to each subject independently. Left: We show the fraction of Full HDP-HMM wall clock time taken
by various competing algorithms. The area of each square is proportional to 1 - normalized hamming
distance and adjusted rand index in the top and bottom plots. The actual values are listed above each
square. Larger squares indicate closer matches to ground truth. At less than half the compute SPAHM
produces similar segmentations to the full HDP-HMM while improving signiﬁcantly on the local
models and k-means based matching. Right: Typical motions associated with the matched states from
the two models are visualized in the red and blue boxes. Skeletons are visualized from contiguous
segments of at least 0.5 seconds of data as segmented by the MAP trajectories.

activities exhibit similar motions  the local HDP-HMM splits them into different local states. Our
matching procedure only allows local states to be matched across subjects and not within. As a result 
they get matched to oversegmented “twists” with similar motions.
We also quantiﬁed the quality of the matched solutions using normalized hamming distance [14] and
adjusted rand index [28]. We compare against local HDP-HMMs as well as two strong competitors  an
identical sticky HDP-HMM model but trained on all six sequences jointly  and an alternate matching
scheme based on k-means clustering that clusters together states discovered by the individual HDP-
HMMs. For k-means  we set k to the ground truth number of activities  twelve. The results are shown
in Figure 4 (left). Quantitatively results show that SPAHM does nearly as well as the full HDP-HMM
at less than half the amount of compute time. Note that SPAHM may be applied to larger amount of
sequences  while full HDP-HMM is limited to small data sizes. We also outperform the k-means
scheme despite cheating in its favor by providing it with the true number of labels.

6 Conclusion

This work presents a statistical model aggregation framework for combining heterogeneous local
models of varying complexity trained on federated  private data sources. Our proposed framework is
largely model-agnostic requiring only an appropriately chosen base measure  and our construction
assumes only that the base measure belongs to the exponential family. As a result  our work can
be applied to a wide range of practical domains with minimum adaptation. A possible interesting
direction for future work will be to consider situations where local parameters are learned across
datasets with a time stamp in addition to the grouping structure.

9

References
[1] Bauer  M.  van der Wilk  M.  and Rasmussen  C. E. (2016). Understanding probabilistic sparse
gaussian process approximations. In Advances in neural information processing systems  pages
1533–1541.

[2] Baydin  A. G.  Pearlmutter  B. A.  Radul  A. A.  and Siskind  J. M. (2018). Automatic differenti-

ation in machine learning: A survey. Journal of Machine Learning Research  pages 1–43.

[3] Blei  D. M.  Ng  A. Y.  and Jordan  M. I. (2003). Latent Dirichlet Allocation. Journal of Machine

Learning Research  3  993–1022.

[4] Blomstedt  P.  Mesquita  D.  Lintusaari  J.  Sivula  T.  Corander  J.  and Kaski  S. (2019). Meta-

analysis of bayesian analyses. arXiv preprint arXiv:1904.04484.

[5] Campbell  T. and How  J. P. (2014). Approximate decentralized bayesian inference. In Uncertainty

in Artiﬁcial Intelligence (UAI).

[6] Campbell  T.  Straub  J.  Fisher III  J. W.  and How  J. P. (2015). Streaming  distributed variational
inference for bayesian nonparametrics. In Advances in Neural Information Processing Systems 
pages 280–288.

[7] Das  R.  Zaheer  M.  and Dyer  C. (2015). Gaussian lda for topic models with word embeddings.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) 
volume 1  pages 795–804.

[8] Dean  J.  Corrado  G.  Monga  R.  Chen  K.  Devin  M.  Mao  M.  Senior  A.  Tucker  P.  Yang  K. 
Le  Q. V.  et al. (2012). Large scale distributed deep networks. In Advances in neural information
processing systems  pages 1223–1231.

[9] Deisenroth  M. P. and Ng  J. W. (2015). Distributed Gaussian processes. In Proc. ICML  pages

1481–1490.

[10] Dutta  R.  Blomstedt  P.  and Kaski  S. (2016). Bayesian inference in hierarchical models by

combining independent posteriors. arXiv preprint arXiv:1603.09272.

[11] EU (2016). Regulation (EU) 2016/679 of the European Parliament and of the Council of 27
April 2016 on the protection of natural persons with regard to the processing of personal data and
on the free movement of such data  and repealing Directive 95/46/EC (General Data Protection
Regulation). Ofﬁcial Journal of the European Union  L119  1–88.

[12] Fox  E.  Jordan  M. I.  Sudderth  E. B.  and Willsky  A. S. (2009). Sharing features among
dynamical systems with Beta processes. In Advances in Neural Information Processing Systems 
pages 549–557.

[13] Fox  E. B.  Sudderth  E. B.  Jordan  M. I.  and Willsky  A. S. (2008). An hdp-hmm for systems
with state persistence. In Proceedings of the 25th international conference on Machine learning 
pages 312–319. ACM.

[14] Fox  E. B.  Hughes  M. C.  Sudderth  E. B.  and Jordan  M. I. (2014). Joint modeling of multiple
time series via the beta process with application to motion capture segmentation. The Annals of
Applied Statistics  pages 1281–1313.

[15] Gal  Y.  van der Wilk  M.  and Rasmussen  C. (2014). Distributed variational inference in sparse

Gaussian process regression and latent variable models. In Proc. NIPS.

[16] Gelman  A.  Stern  H. S.  Carlin  J. B.  Dunson  D. B.  Vehtari  A.  and Rubin  D. B. (2013).

Bayesian data analysis. Chapman and Hall/CRC.

[17] Ghahramani  Z. and Grifﬁths  T. L. (2005). Inﬁnite latent feature models and the Indian buffet

process. In Advances in Neural Information Processing Systems  pages 475–482.

[18] Hasenclever  L.  Webb  S.  Lienart  T.  Vollmer  S.  Lakshminarayanan  B.  Blundell  C.  and
Teh  Y. W. (2017). Distributed bayesian learning with stochastic natural gradient expectation
propagation and the posterior server. The Journal of Machine Learning Research  18(1)  3744–
3780.

[19] Heikkilä  M.  Lagerspetz  E.  Kaski  S.  Shimizu  K.  Tarkoma  S.  and Honkela  A. (2017).
Differentially private bayesian learning on distributed data. In Advances in neural information
processing systems  pages 3226–3235.

10

[20] Hoang  Q. M.  Hoang  T. N.  Low  K. H.  and Kingsford  C. (2019a). Collective model fusion

for multiple black-box experts. In Proc. ICML  pages 2742–2750.

[21] Hoang  T. N.  Hoang  Q. M.  and Low  K. H. (2016). A distributed variational inference
framework for unifying parallel sparse Gaussian process regression models. In Proc. ICML  pages
382–391.

[22] Hoang  T. N.  Hoang  Q. M.  Ruofei  O.  and Low  K. H. (2018). Decentralized high-dimensional

bayesian optimization with factor graphs. In Proc. AAAI.

[23] Hoang  T. N.  Hoang  Q. M.  Low  K. H.  and How  J. P. (2019b). Collective online learning of

gaussian processes in massive multi-agent systems. In Proc. AAAI.

[24] Hughes  M. C.  Stephenson  W. T.  and Sudderth  E. (2015). Scalable adaptation of state com-
plexity for nonparametric hidden markov models. In Advances in Neural Information Processing
Systems  pages 1198–1206.

[25] Kuhn  H. W. (1955). The Hungarian method for the assignment problem. Naval Research

Logistics (NRL)  2(1-2)  83–97.

[26] McMahan  B.  Moore  E.  Ramage  D.  Hampson  S.  and y Arcas  B. A. (2017). Communication-
efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics 
pages 1273–1282.

[27] Newman  D.  Lau  J. H.  Grieser  K.  and Baldwin  T. (2010). Automatic evaluation of topic
In Human Language Technologies: The 2010 Annual Conference of the North
coherence.
American Chapter of the Association for Computational Linguistics  pages 100–108. Association
for Computational Linguistics.

[28] Rand  W. M. (1971). Objective criteria for the evaluation of clustering methods. Journal of the

American Statistical association  66(336)  846–850.

[29] Teh  Y. W.  Jordan  M. I.  Beal  M. J.  and Blei  D. M. (2005). Sharing clusters among related
groups: Hierarchical dirichlet processes. In Advances in neural information processing systems 
pages 1385–1392.

[30] Teh  Y. W.  Grür  D.  and Ghahramani  Z. (2007). Stick-breaking construction for the Indian

buffet process. In Artiﬁcial Intelligence and Statistics  pages 556–563.

[31] Thibaux  R. and Jordan  M. I. (2007). Hierarchical Beta processes and the Indian buffet process.

In Artiﬁcial Intelligence and Statistics  pages 564–571.

[32] Yurochkin  M.  Agarwal  M.  Ghosh  S.  Greenewald  K.  Hoang  N.  and Khazaeni  Y. (2019a).
Bayesian nonparametric federated learning of neural networks. In International Conference on
Machine Learning  pages 7252–7261.

[33] Yurochkin  M.  Fan  Z.  Guha  A.  Koutris  P.  and Nguyen  X. (2019b). Scalable inference of
topic evolution via models for latent geometric structures. In Advances in Neural Information
Processing Systems  pages 5949–5959.

11

,Mikhail Yurochkin
Mayank Agarwal
Soumya Ghosh
Kristjan Greenewald
Nghia Hoang