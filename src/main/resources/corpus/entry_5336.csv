2017,Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues,In this work we derive a variant of the classic Glivenko-Cantelli Theorem  which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF.  We apply our bound in the context of revenue learning  which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues  assuming a bound on the k'th moment of the valuations  for any (possibly fractional) k > 1.  For uniform convergence in the limit  we give a complete characterization and a zero-one law: if the first moment of the valuations is finite  then uniform convergence almost surely occurs; conversely  if the first moment is infinite  then uniform convergence almost never occurs.,Submultiplicative Glivenko-Cantelli and

Uniform Convergence of Revenues

Noga Alon

Tel Aviv University  Israel
and Microsoft Research

nogaa@tau.ac.il

Moshe Babaioff
Microsoft Research

moshe@microsoft.com

Yannai A. Gonczarowski

The Hebrew University of Jerusalem  Israel

and Microsoft Research
yannai@gonch.name

Yishay Mansour

Tel Aviv University  Israel
and Google Research  Israel

mansour@tau.ac.il

Shay Moran

Institute for Advanced Study  Princeton

Amir Yehudayoff

Technion — IIT  Israel

shaymoran1@gmail.com

amir.yehudayoff@gmail.com

Abstract

In this work we derive a variant of the classic Glivenko-Cantelli Theorem  which
asserts uniform convergence of the empirical Cumulative Distribution Function
(CDF) to the CDF of the underlying distribution. Our variant allows for tighter
convergence bounds for extreme values of the CDF.
We apply our bound in the context of revenue learning  which is a well-studied
problem in economics and algorithmic game theory. We derive sample-complexity
bounds on the uniform convergence rate of the empirical revenues to the true
revenues  assuming a bound on the kth moment of the valuations  for any (possibly
fractional) k > 1.
For uniform convergence in the limit  we give a complete characterization and a
zero-one law: if the ﬁrst moment of the valuations is ﬁnite  then uniform conver-
gence almost surely occurs; conversely  if the ﬁrst moment is inﬁnite  then uniform
convergence almost never occurs.

1

Introduction

A basic task in machine learning is to learn an unknown distribution µ  given access to samples
from it. A natural and widely studied criterion for learning a distribution is approximating its
Cumulative Distribution Function (CDF). The seminal Glivenko-Cantelli Theorem [13  6] addresses
this question when the distribution µ is over the real numbers. It determines the behavior of the
empirical distribution function as the number of samples grows: let X1  X2  . . . be a sequence of i.i.d.
random variables drawn from a distribution µ on R with Cumulative Distribution Function (CDF) F  
and let x1  x2  . . . be their realizations. The empirical distribution µn is

n(cid:88)

i=1

µn (cid:44) 1
n

δxi 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1

n ·(cid:12)(cid:12){1 ≤ i ≤ n : xi ≤ t}(cid:12)(cid:12). The Glivenko-Cantelli Theorem formalizes the statement that µn

where δxi is the constant distribution supported on xi. Let Fn denote the CDF of µn  i.e.  Fn(t) (cid:44)
converges to µ as n grows  by establishing that Fn(t) converges to F (t)  uniformly over all t ∈ R:
Theorem 1.1 (Glivenko-Cantelli Theorem  [13  6]). Almost surely 

(cid:12)(cid:12)Fn(t) − F (t)(cid:12)(cid:12) = 0.

n→∞ sup
lim

t

Some twenty years after Glivenko [13] and Cantelli [6] discovered this theorem  Dvoretzky  Kiefer 
and Wolfowitz (DKW) [12] strengthened this result by giving an almost1 tight quantitative bound on
the convergence rate. In 1990  Massart [17] proved a tight inequality  conﬁrming a conjecture due to
Birnbaum and McCarty [3]:
Theorem 1.2 ([17]). Pr

(cid:105) ≤ 2 exp(−2n2) for all  > 0  n ∈ N.

(cid:104)

supt

(cid:12)(cid:12)Fn(t) − F (t)(cid:12)(cid:12) > 
∀t :(cid:12)(cid:12)F (t) − Fn(t)(cid:12)(cid:12) ≤  · F (t).

The above theorems show that  with high probability  F and Fn are close up to some additive error.
We would have liked to prove a stronger  multiplicative bound on the error:

However  for some distributions  the above event has probability 0  no matter how large n is. For
example  assume that µ satisﬁes F (t) > 0 for all t. Since the empirical measure µn has ﬁnite support 
there is t with Fn(t) = 0; for such a value of t  such a multiplicative approximation fails to hold.
So  the above multiplicative requirement is too strong to hold in general. A natural compromise is to
consider a submultiplicative bound:

∀t :(cid:12)(cid:12)F (t) − Fn(t)(cid:12)(cid:12) ≤  · F (t)α 

where 0 ≤ α < 1. When α = 0  this is the additive bound studied in the context of the Glivenko-
Cantelli Theorem. When α = 1  this is the unattainable multiplicative bound. Our ﬁrst main result
shows that the case of α < 1 is attainable:
Theorem 1.3 (Submultiplicative Glivenko-Cantelli Theorem). Let  > 0  δ > 0 and 0 ≤ α < 1.
There exists n0(  δ  α) such that for all n > n0  with probability 1 − δ:

∀t :(cid:12)(cid:12)F (t) − Fn(t)(cid:12)(cid:12) ≤  · F (t)α.

the submultiplicative bound from Theorem 1.3 does not even extend to the VC dimension 1 class

observe that for every sample x1  . . .   xn  it holds that pn
therefore  as long as α > 0  it is never the case that

It is worth pointing out a central difference between Theorem 1.3 and other generalizations of
the Glivenko-Cantelli Theorem: for example  the seminal work of Vapnik and Chervonenkis [24]
shows that for every class of events F of VC dimension d  there is n0 = n0(  δ  d) such that for

every n ≥ n0  with probability 1 − δ it holds that ∀A ∈ F : (cid:12)(cid:12)p(A) − pn(A)(cid:12)(cid:12) ≤ . This yields
Glivenko-Cantelli by plugging F =(cid:8)(−∞  t] : t ∈ R(cid:9)  which has VC dimension 1. In contrast 
F =(cid:8){t} : t ∈ R(cid:9). Indeed  pick any distribution p over R such that p(cid:0){t}(cid:1) = 0 for every t  and
(cid:0){xi}(cid:1) ≥ 1/n  however p(cid:0){xi}(cid:1) = 0  and
(cid:12)(cid:12)(cid:12)p(cid:0){xi}(cid:1) − pn
  
(cid:32)
(cid:17)
(cid:16) ln(1/δ)

Our second main result gives an explicit upper bound on n0(  δ  α):
Theorem 1.4 (Submultiplicative Glivenko-Cantelli Bound). Let   δ ≤ 1/4  and α < 1. Then

(cid:0){xi}(cid:1)(cid:12)(cid:12)(cid:12) ≤ p(cid:0){xi}(cid:1)α.
(cid:19)(cid:33) 4α
(cid:18)

Note that for ﬁxed   δ  when α → 0 the above bound approaches the familiar O
bound
by DKW [12] and Massart [17] for α = 0. On the other hand  when α → 1 the above bound tends

 ln(cid:0)6/δ(cid:1)
(cid:16) δ
(cid:1)(cid:17)− 4α
(cid:16) δ
6 · ln(cid:0) 1+α

22

2α

3

n0(  δ  α) ≤ max

12 · D + 4
δ(1 − α)

where D = ln(6/δ)
22

(cid:17)− 4α

1−α

1−α .

 

(D + 1)

10 · ln

1−α

2

1The inequality due to [12] has a larger constant C in front of the exponent on the right hand side.

2

to ∞  reﬂecting the fact that the multiplicative variant of Glivenko-Cantelli (α = 1) does not hold.
Theorems 1.3 and 1.4 are proven in the supplementary material.
Note that the dependency of the above bound on the conﬁdence parameter δ is polynomial. This
contrasts with standard uniform convergence rates  which  due to applications of concentration bounds
such as Chernoff/Hoeffding  achieve logarithmic dependencies on δ. These concentration bounds are
not applicable in our setting when the CDF values are very small  and we use Markov’s inequality
instead. The following example shows that a polynomial dependency on δ is indeed necessary and is
not due to a limitation of our proof.
Example 1.5. For large n  consider n independent samples x1  . . .   xn from the uniform distribution
over [0  1]  and set α = 1/2 and  = 1. The probability of the event

∃i : xi ≤ 1/n3

is roughly 1/n2: indeed  the complementary event has probability (1−1/n3)n ≈ exp(−1/n2) ≈ 1−

1/n2. When this happens  we have: Fn(1/n3) ≥ 1/n >> 1/n3+1/n3/2 = F (1/n3)+(cid:2)F (1/n3)(cid:3)1/2.

Note that this happens with probability inverse polynomial in n (roughly 1/n2) and not inverse
exponential.

An application to revenue learning. We demonstrate an application of our Submultiplicative
Glivenko-Cantelli Theorem in the context of a widely studied problem in economics and algorithmic
game theory: the problem of revenue learning. In the setting of this problem  a seller has to decide
which price to post for a good she wishes to sell. Assume that each consumer draws her private
valuation for the good from an unknown distribution µ. We envision that a consumer with valuation v
will buy the good at any price p ≤ v  but not at any higher price. This implies that the expected
revenue at price p is simply r(p) (cid:44) p · q(p)  where q(p) (cid:44) PrV ∼µ[V ≥ p].
In the language of machine learning  this problem can be phrased as follows: the examples domain
Z (cid:44) R+ is the set of all valuations v. The hypothesis space H (cid:44) R+ is the set of all prices p. The
revenue (which is a gain  rather than loss) of a price p on a valuation v is the function p · 1{p≤v}.
The well-known revenue maximization problem is to ﬁnd a price p∗ that maximizes the expected
revenue  given a sample of valuations drawn i.i.d. from µ. In this paper  we consider the more
demanding revenue estimation problem: the problem of well-approximating r(p)  simultaneously for
all prices p  from a given sample of valuations. (This clearly also implies a good estimation of the
maximum revenue and of a price that yields it.) More speciﬁcally  we address the following question:
when do the empirical revenues  rn(p) (cid:44) p · qn(p)  where qn(p) (cid:44) PrV ∼µn[V ≥ p] = 1
show that for some n0  for n ≥ n0 we have with probability 1 − δ that

n ·(cid:12)(cid:12){1 ≤
i ≤ n : xi ≥ t}(cid:12)(cid:12)  uniformly converge to the true revenues r(p)? More speciﬁcally  we would like to

(cid:12)(cid:12)r(p) − rn(p)(cid:12)(cid:12) ≤ .

The revenue estimation problem is a basic instance of the more general problem of uniform conver-
gence of empirical estimates. The main challenge in this instance is that the prices are unbounded
(and so are the private valuations that are drawn from the distribution µ).
Unfortunately  there is no (upper) bound on n0 that is only a function of  and δ. Moreover  even if
we add the expectation of valuations  i.e.  E[V ] where V is distributed according to µ  still there is no
bound on n0 that is a function of only those three parameters (see Section 2.3 for an example). In
contrast  when we consider higher moments of the distribution µ  we are able to derive bounds on the
value of n0. These bounds are based on our Submultiplicative Glivenko-Cantelli Bound. Speciﬁcally 
assume that EV ∼µ[V 1+θ] ≤ C for some θ > 0 and C ≥ 1. Then  we show that for any   δ ∈ (0  1) 
we have

(cid:104)∃v : (cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) > 

Pr

This essentially reduces uniform convergence bounds to our Submultiplicative Glivenko-Cantelli
variant. It then follows that there exists n0(C  θ    δ) such that for any n ≥ n0  with probability at
least 1 − δ 

(cid:21)

.


1

1+θ

C

q(v)

1

1+θ

(cid:20)
(cid:105) ≤ Pr
∃v : (cid:12)(cid:12)q(v) − qn(v)(cid:12)(cid:12) >
(cid:12)(cid:12)rn(v) − r(v)(cid:12)(cid:12) ≤ .

∀v :

3

(cid:16) ln(1/δ)

(cid:17)

We remark that when θ is large  our bound yields n0 ≈ O
sample complexity bounds obtainable via DKW [12] and Massart [17].
When θ → 0  our bound diverges to inﬁnity  reﬂecting the fact (discussed above) that there is no
bound on n0 that depends only on   δ  and E[V ]. Nevertheless  we ﬁnd that E[V ] qualitatively
determines whether uniform convergence occurs in the limit. Namely  we show that

  which recovers the standard

2

• If Eµ[V ] < ∞  then almost surely limn→∞ supv
• Conversely  if Eµ[V ] = ∞  then almost never limn→∞ supv

(cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) = 0 

(cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) = 0.

1.1 Related work

Generalizations of Glivenko-Cantelli. Various generalizations of the Glivenko-Cantelli Theorem
were established. These include uniform convergence bounds for more general classes of functions as
well as more general loss functions (for example  [24  23  16  2]). The results that concern unbounded
loss functions are most relevant to this work (for example  [9  8  23]). We next brieﬂy discuss the
relevant results from Cortes et al. [8] in the context of this paper; more speciﬁcally  in the context of
Theorem 1.3. To ease presentation  set α in this theorem to be 1/2. Theorem 1.3 analyzes the event
where the empirical quantile is bounded by2

whereas  [8] analyzes the event where it is bounded it by:

qn(p) ≤ q(p) + (cid:112)q(p) 
qn(p) ≥ q(p) − (cid:112)q(p).

qn(p) ≤ ˜O(cid:0)q(p) +(cid:112)q(p)/n + 1/n(cid:1) 
qn(p) ≥ ˜Ω(cid:0)q(p) −(cid:112)qn(p)/n − 1/n(cid:1)

Thus  the main difference is the additive 1/n term in the bound from [8]. In the context of uniform
convergence of revenues  it is crucial to use the upper bound on the empirical quantile as we do 
as it guarantees that large prices will not overﬁt  which is the main challenge in proving uniform
convergence in this context. In particular  the upper bound from [8] does not provide any guarantee
on the revenues of prices p >> n  as for such prices p · 1/n >> 1.
It is also worth pointing out that our lower bound on the empirical quantile implies that with high
probability the quantile of the maximum sampled point is at least 1/n2 (or more generally  at least
1/n1/α when α (cid:54)= 1/2)  while the bound from [8] does not imply any non-trivial lower bound.
Another  more qualitative difference is that unlike the bounds in [8] that apply for general VC classes 
our bound is tailored for the class of thresholds (corresponding to CDF/quantiles)  and does not
extend even to other classes of VC dimension 1 (see the discussion after Theorem 1.3).

Uniform convergence of revenues. The problem of revenue maximization is a central problem in
economics and Algorithmic Game Theory (AGT). The seminal work of Myerson [20] shows that
given a valuation distribution for a single good  the revenue-maximizing selling mechanism for this
good is a posted-price mechanism. In the recent years  there has been a growing interest in the case
where the valuation distribution is unknown  but the seller observes samples drawn from it. Most
papers in this direction assume that the distribution meets some tail condition that is considered
“natural” within the algorithmic game theory community  such as boundedness [18  21  19  1  14  10]3 
such as a condition known as Myerson-regularity [11  15  7  10]  or such as a condition known as
monotone hazard rate [15].4 These papers then go on to derive computation- or sample-complexity
2For consistency with the canonical statement of the Glivenko-Cantelli theorem  we stated our submultiplica-
tive variants of this theorem with regard to the CDFs Fn and F . However  these results also hold when replacing
these CDFs with the respective quantiles (tail CDFs) qn and q. See Section 2.2 for details.

3The analysis of [1] assumes a bound on the realized revenue (from any possible valuation proﬁle) of any
mechanism/auction in the class that they consider. For the class of posted-price mechanisms  this is equivalent
to assuming a bound on the support of the valuation distribution. Indeed  for any valuation v  pricing at v
gives realized revenue v (from the valuation v)  and so unbounded valuations (together with the ability to post
unbounded prices) imply unbounded realized revenues.

4Both Myerson-regularity and monotone hazard rate are conditions on the second derivative of the revenue
as a function of the quantile of the underlying distribution. In particular  they impose restrictions on the tail of
the distribution.

4

√

bounds on learning an optimal price (or an optimal selling mechanism from a given class) for a
distribution that meets the assumed condition.
A recurring theme in statistical learning theory is that learnability guarantees are derived via a 
sometimes implicit  uniform convergence bound. However  this has not been the case in the context
of revenue learning. Indeed  while some papers that studied bounded distributions [18  21  19  1]
did use uniform convergence bounds as part of their analysis  other papers  in particular those
that considered unbounded distributions  had to bypass the usage of uniform convergence by more
specialized arguments. This is due to the fact that many unbounded distributions do not satisfy
any uniform convergence bound. As a concrete example  the (unbounded  Myerson-regular) equal
revenue distribution5 has an inﬁnite expectation and therefore  by our Theorem 2.3  satisﬁes no
uniform convergence  even in the limit. Thus  it turns out that the works that studied the popular
class of Myerson-regular distributions [11  15  7  10] indeed could not have hoped to establish
learnability via a uniform convergence argument. For instance  the way [11  7] establish learnability
for Myerson-regular distributions is by considering the guarded ERM algorithm (an algorithm that
chooses an empirical revenue maximizing price that is smaller than  say  the
nth largest sampled
price)  and proving a uniform convergence bound  not for all prices  but only for prices that are  say 
smaller than the
nth largest sampled price  and then arguing that larger prices are likely to have a
small empirical revenue  compared to the guarded empirical revenue maximizer. This means that the
guarded ERM will output a good price  but it does not (and cannot) imply uniform convergence for
all prices.
We complement the extensive literature surveyed above in a few ways. The ﬁrst is generalizing
the revenue maximization problem to a revenue estimation problem  where the goal is to uniformly
estimate the revenue of all possible prices  when no bound on the possible valuations is given (or
even exists). The problem of revenue estimation arises naturally when the seller has additional
considerations when pricing her good  such as regulations that limit the price choice  bad publicity
if the price is too high (or  conversely  damage to prestige if the price is too low)  or willingness to
suffer some revenue loss for better market penetration (which may translate to more revenue in the
future). In such a case  the seller may wish to estimate the revenue loss due to posting a discounted
(or inﬂated) price.
The second  and most important  contribution to the above literature is that we consider arbitrary
distributions rather than very speciﬁc and limited classes of distributions (e.g.  bounded  Myerson-
regular  monotone hazard rate  etc.). Third  we derive ﬁnite sample bounds in the case that the
expected valuation is bounded for some moment larger than 1. We further derive a zero-one law for
uniform convergence in the limit that depends on the ﬁniteness of the ﬁrst moment. Technically  our
bounds are based on an additive error rather than multiplicative ones  which are popular in the AGT
community.

√

1.2 Paper organization

The rest of the paper is organized as follows. Section 2 contains the application of our Submulti-
plicative Glivenko-Cantelli to revenue estimation  and Section 3 contains a discussion and possible
directions of future work. The proof of the Submultiplicative Glivenko-Cantelli variant  and some
extensions of it  appear in the supplementary material.

2 Uniform Convergence of Empirical Revenues

In this section we demonstrate an application of our Submultiplicative Glivenko-Cantelli variant by
establishing uniform convergence bounds for a family of unbounded random variables in the context
of revenue estimation.

2.1 Model

Consider a good g that we wish to post a price for. Let V be a random variable that models the
valuation of a random consumer for g. Technically  it is assumed that V is a nonnegative random
variable  and we denote by µ its induced distribution over R+. A consumer who values g at a

5This is a distribution that satisﬁes the special property that all prices have the same expected revenue.

5

valuation v is willing to buy the good at any price p ≤ v  but not at any higher price. This implies
that the realized revenue to the seller from a (posted) price p is the random variable p · 1{p≤V }. The
quantile of a value v ∈ R+ is

This models the fraction of the consumers in the population that are willing to purchase the good if
priced at v. The expected revenue from a (posted) price p ∈ R+ is

r(p) = r(p; µ) (cid:44) E

µ

q(v) = q(v; µ) (cid:44) µ(cid:0){x : x ≥ v}(cid:1).
(cid:2)p · 1{p≤V }(cid:3) = p · q(p).
n ·(cid:12)(cid:12){1 ≤ i ≤ n : vi ≥ v}(cid:12)(cid:12).
(cid:2)p · 1{p≤V }(cid:3) = p · qn(p).
(cid:12)(cid:12)rn(p) − r(p)(cid:12)(cid:12).

n (cid:44) sup

µn

p

qn(v) = q(v; µn) (cid:44) 1
The empirical revenue from a price p ∈ R+ is
rn(p) = r(p; µn) (cid:44) E

The revenue estimation error for a given sample of size n is

Let V1  V2  . . . be a sequence of i.i.d. valuations drawn from µ  and let v1  v2  . . . be their realizations.
The empirical quantile of a value v ∈ R+ is

It is worth highlighting the difference between revenue estimation and revenue maximization. Let p∗
be a price that maximizes the revenue  i.e.  p∗ ∈ arg supp r(p). The maximum revenue is r∗ = r(p∗).
The goal in many works in revenue maximization is to ﬁnd a price ˆp such that r∗ − r(ˆp) ≤   or
alternatively  to bound r∗
Given a revenue-estimation error n  one can clearly maximize the revenue within an additive error
of 2n by simply posting a price p∗
n). This
follows since
n) ≥ rn(p∗

n ∈ arg maxp rn(p)  thereby attaining revenue r∗
n) − n ≥ rn(p∗) − n ≥ r(p∗) − 2n = r∗ − 2n.

r∗
n = r(p∗

n = r(p∗

/r( ˆp).

Therefore  good revenue estimation implies good revenue maximization.
We note that the converse does not hold. Namely  there are distributions for which revenue maximiza-
tion is trivial but revenue estimation is impossible. One such case is the equal revenue distribution 
where all values in the support of µ have the same expected revenue. For such distributions  the
problem of revenue maximization becomes trivial  since any posted price is optimal. However  as
follows from Theorem 2.3  since the expected revenue of such distributions is inﬁnite  almost never
do the empirical revenues uniformly converge to the true revenues.

2.2 Quantitative bounds on the uniform convergence rate

Recall that we are interested in deriving sample bounds that would guarantee uniform convergence
for the revenue estimation problem. We will show that given an upper bound on the kth moment of
V for some k > 1  we can derive a ﬁnite sample bound. To this end we utilize our Submultiplicative
Glivenko-Cantelli Bound (Theorem 1.4).
We also consider the case of k = 1  namely that E[V ] is bounded  and show that in this case there is
still uniform convergence in the limit  but that there cannot be any guarantees on the convergence
rate. Interestingly  it turns out that E[V ] < ∞ is not only sufﬁcient but also necessary so that in the
limit  the empirical revenues uniformly converge to the true revenues (see Section 2.3).
We begin by showing that bounds on the kth moment for k > 1 yield explicit bounds on the
convergence rate. It is convenient to parametrize by setting k = 1 + θ  where θ > 0.
Theorem 2.1. Let EV ∼µ[V 1+θ] ≤ C for some θ > 0 and C ≥ 1  and let   δ ∈ (0  1). Set6

ln(1/δ)
2 C
For any n ≥ n0  with probability at least 1 − δ 

n0 = ˜O

2

(cid:32)

∀v :

1

δ ln(cid:0)1 + θ/2(cid:1)(cid:19)4/θ(cid:33)
(cid:18) 6 · C
(cid:12)(cid:12)rn(v) − r(v)(cid:12)(cid:12) ≤ .

1+θ

1+θ

.

(1)

6The ˜O conceals low order terms.

6

(cid:16) ln(1/δ)

(cid:17)

sample complexity bound
Note that when θ is large  this bound approaches the standard O
of the additive Glivenko-Cantelli. For example  if all moments are uniformly bounded  then the
convergence is roughly as fast as in standard uniform convergence settings (e.g.  VC-dimension based
bounds).
The proof of Theorem 2.1 follows from Theorem 1.4 and the next proposition  which reduces bounds
on the uniform convergence rate of the empirical revenues to our Submultiplicative Glivenko-Cantelli.
Proposition 2.2. Let EV ∼µ[V 1+θ] ≤ C for some θ > 0 and C ≥ 1  and let   δ ∈ (0  1). Then 

2

(cid:104)∃v : (cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) > 

(cid:105) ≤ Pr

Pr

(cid:20)
∃v : (cid:12)(cid:12)q(v) − qn(v)(cid:12)(cid:12) >

(cid:21)

.



1

1+θ

C

q(v)

1

1+θ

Theorem 1.4 to the measure µ(cid:48) deﬁned by µ(cid:48)(A) (cid:44) µ(cid:0){−a | a ∈ A}(cid:1) yields the required result with

Thus  to prove Theorem 2.1  we ﬁrst note that Theorem 1.4 (as well as Theorem 1.3) also holds
when Fn and F are respectively replaced in the deﬁnition of n0 with qn and q (indeed  applying
regard to the measure µ). We then plug  ← 
1+θ into this variant of Theorem 1.4 to
yield a bound on the right-hand side of the inequality in Proposition 2.2  whose application concludes
the proof.

and α ← 1

1+θ

C

1

Proof of Proposition 2.2. By Markov’s inequality:

Now 

(cid:104)∃v : (cid:12)(cid:12)r(v)−rn(v)(cid:12)(cid:12) > 
(cid:105)

Pr

q(v) = Pr[V ≥ v] = Pr[V 1+θ ≥ v1+θ] ≤ C

v1+θ .

(cid:104)∃v : (cid:12)(cid:12)v · q(v)−v · qn(v)(cid:12)(cid:12) > 
(cid:105)
(cid:104)∃v : (cid:12)(cid:12)v · q(v)−v · qn(v)(cid:12)(cid:12) >
(cid:104)∃v : (cid:12)(cid:12)v · q(v)−v · qn(v)(cid:12)(cid:12) >
(cid:104)∃v : (cid:12)(cid:12)q(v)−qn(v)(cid:12)(cid:12) >


1

C

= Pr

= Pr

≤ Pr

= Pr

q(v)

1

1+θ

.

C

1+θ

(2)

(cid:105)

1

1+θ

(v1+θ · q(v))

1

1+θ

(v1+θ·q(v))

1

1+θ


1

1+θ

(v1+θ·q(v))

(cid:105)



(cid:105)

where the inequality follows from Equation (2).

2.3 A qualitative characterization of uniform convergence

The sample complexity bounds in Theorem 2.1 are meaningful as long as θ > 0  but deteriorate
drastically as θ → 0. Indeed  as the following example shows  there is no bound on the uniform
convergence sample complexity that depends only on the ﬁrst moment of V   i.e.  its expectation.
Consider a distribution ηp so that with probability p we have V = 1/p and otherwise V = 0. Clearly 
E[V ] = 1. However  we need to sample mp = O(1/p) valuations to see a single nonzero value.
Therefore  there is no bound on the sample size mp as a function of the expectation  which is simply 1.
We can now consider the higher moments of ηp. Consider the kth moment  for k = 1 + θ and
θ > 0  so k > 1. For this moment  we have Ap θ = E[V 1+θ] = pθ/(1+θ)  which implies that

mp = O(cid:0)1/(Ap θ)(1+θ)/θ(cid:1). This does allow us to bound mp as a function of θ and E[V 1+θ]  but for

small θ we have a huge exponent of approximately 1/θ.
While the above examples show that there cannot be a bound on the sample size as a function of the
expectation of the value  it turns out that there is a very tight connection between the ﬁrst moment
and uniform convergence:
Theorem 2.3. The following dichotomy holds for a distribution µ on R+:

1. If Eµ[V ] < ∞  then almost surely limn→∞ supv
2. If Eµ[V ] = ∞  then almost never limn→∞ supv

(cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) = 0.
(cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) = 0.

7

That is  the empirical revenues uniformly converge to the true revenues if and only if Eµ[V ] < ∞.
We use the following basic fact in the Proof of Theorem 2.3:
Lemma 2.4. Let X be a nonnegative random variable. Then

∞(cid:88)

Pr[X ≥ n] ≤ E[X] ≤

Pr[X ≥ n].

∞(cid:88)

Proof. Note that:

n=1

n=0

1{X≥n} = (cid:98)X(cid:99) ≤ X ≤ (cid:98)X(cid:99) + 1 =

∞(cid:88)

n=1

∞(cid:88)

n=0

1{X≥n}.

The lemma follows by taking expectations.
Proof of Theorem 2.3. We start by proving item 2. Let µ be a distribution such that Eµ
supv v · q(v) = ∞ then for every realization v1  . . .   vn there is some v ≥ max{v1  . . .   vn} such
that v· q(v) ≥ 1  but v· qn(v) = 0. So  we may assume supv v· q(v) < ∞. Without loss of generality
we may assume that supv v· q(v) = 1/2 by rescaling the distribution if needed. Consider the sequence
of events E1  E2  . . . where En denotes the event that Vn ≥ n. Since Eµ
n=1 Pr[En] = ∞. Thus  since these events are independent  the second Borel-Cantelli

(cid:2)V(cid:3) = ∞. If
(cid:2)V(cid:3) = ∞  Lemma 2.4

implies that(cid:80)∞

Lemma [4  5] implies that almost surely  inﬁnitely many of them occur and so inﬁnitely often

Vn · qn(Vn) ≥ 1 ≥ Vn · q(Vn) + 1
2 .

Therefore  the probability that v · qn(v) uniformly converge to v · q(v) is 0.
Item 1 follows from the following monotone domination theorem:
Theorem 2.5. Let F be a family of nonnegative monotone functions  and let F be an upper envelope7
for F. If Eµ[F ] < ∞  then almost surely:

Indeed  item 1 follows by plugging F =(cid:8)v · 1x≥v : v ∈ R+(cid:9)  which is uniformly bounded by the

identity function F (x) = x. Now  by assumption Eµ[F ] < ∞  and therefore  almost surely

n→∞ sup
lim
f∈F

µn

(cid:12)(cid:12)r(v) − rn(v)(cid:12)(cid:12) = lim

n→∞ sup
f∈F

[f ] − E

µn

[f ](cid:12)(cid:12) = 0.

n→∞ sup
lim
v∈R+

(cid:12)(cid:12)E

µ

[f ] − E

[f ](cid:12)(cid:12) = 0.
(cid:12)(cid:12)E

µ

Theorem 2.5 follows by known results in the theory of empirical processes (for example  with some
work it can be proved using Theorem 2.4.3 from Vaart and Wellner [22]). For completeness  we give
a short and basic proof in the supplementary material.

3 Discussion

Our main result is a submultiplicative variant of the Glivenko-Cantelli Theorem  which allows for
tighter convergence bounds for extreme values of the CDF. We show that for the revenue learning
setting our submultiplicative bound can be used to derive uniform convergence sample complexity
bounds  assuming a ﬁnite bound on the kth moment of the valuations  for any (possibly fractional)
k > 1. For uniform convergence in the limit  we give a complete characterization  where uniform
convergence almost surely occurs if and only if the ﬁrst moment is ﬁnite.
It would be interesting to ﬁnd other applications of our submultiplicative bound in other settings. A
potentially interesting direction is to consider unbounded loss functions (e.g.  the squared-loss  or
log-loss). Many works circumvent the unboundedness in such cases by ensuring (implicitly) that
the losses are bounded  e.g.  through restricting the inputs and the hypotheses. Our bound offers a
different perspective of addressing this issue. In this paper we consider revenue learning  and replace
the boundedness assumption by assuming bounds on higher moments. An interesting challenge is to

7F is an upper envelope for F if F (v) ≥ f (v) for every v ∈ V and f ∈ F.

8

prove uniform convergence bounds for other practically interesting settings. One such setting might
be estimating the effect of outliers (which correspond to the extreme values of the loss).
In the context of revenue estimation  this work only considers the most naïve estimator  namely of
estimating the revenues by the empirical revenues. One can envision other estimators  for example
ones which regularize the extreme tail of the sample. Such estimators may have a potential of
better guarantees or better convergence bounds. In the context of uniform convergence of selling
mechanism revenues  this work only considers the basic class of posted-price mechanisms. While
for one good and one valuation distribution  it is always possible to maximize revenue via a selling
mechanism of this class  this is not the case in more complex auction environments. While in many
more-complex environments  the revenue-maximizing mechanism/auction is still not understood well
enough  for environments where it is understood [7  10  14] (as well as for simple auction classes that
do not necessarily contain a revenue-maximizing auction [19  1]) it would also be interesting to study
relaxations of the restrictive tail or boundedness assumptions currently common in the literature.

Acknowledgments

The research of Noga Alon is supported in part by an ISF grant and by a GIF grant. Yannai
Gonczarowski is supported by the Adams Fellowship Program of the Israel Academy of Sciences
and Humanities; his work is supported by ISF grant 1435/14 administered by the Israeli Academy
of Sciences and by Israel-USA Bi-national Science Foundation (BSF) grant number 2014389; this
project has received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement No 740282). The research of
Yishay Mansour was supported in part by The Israeli Centers of Research Excellence (I-CORE)
program (Center No. 4/11)  by a grant from the Israel Science Foundation  and by a grant from
United States-Israel Binational Science Foundation (BSF); the research was done while author was
co-afﬁliated with Microsoft Research. The research of Shay Moran is supported by the National
Science Foundations and the Simons Foundations; part of the research was done while author was
co-afﬁliated with Microsoft Research. The research of Amir Yehudayoff is supported by ISF grant
1162/15.

References
[1] Maria-Florina Balcan  Tuomas Sandholm  and Ellen Vitercik. Sample complexity of automated mechanism
design. In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS)  pages
2083–2091  2016.

[2] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2002.

[3] Z. W. Birnbaum and R. C. McCarty. A distribution-free upper conﬁdence bound for Pr{Y < X}  based

on independent samples of X and Y . The Annals of Mathematical Statistics  29(2):558–562  1958.

[4] Émile Borel. Les probabilités dénombrables et leurs applications arithmétiques. Rendiconti del Circolo

Matematico di Palermo (1884-1940)  27(1):247–271  1909.

[5] Francesco Paolo Cantelli. Sulla probabilitá come limite della frequenza. Atti Accad. Naz. Lincei  26(1):39–

45  1917.

[6] Francesco Paolo Cantelli. Sulla determinazione empirica delle leggi di probabilita. Giornalle dell’Istituto

Italiano degli Attuari  4:421–424  1933.

[7] Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of

the 46th Annual ACM Symposium on Theory of Computing (STOC)  pages 243–252  2014.

[8] Corinna Cortes  Spencer Greenberg  and Mehryar Mohri. Relative deviation learning bounds and general-

ization with unbounded loss functions. CoRR  abs/1310.5796  2013.

[9] Corinna Cortes  Yishay Mansour  and Mehryar Mohri. Learning bounds for importance weighting. In
Proceedings of the 24th Conference on Neural Information Processing Systems (NIPS)  pages 442–450 
2010.

[10] Nikhil R. Devanur  Zhiyi Huang  and Christos-Alexandros Psomas. The sample complexity of auctions
with side information. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing
(STOC)  pages 426–439  2016.

9

[11] Peerapong Dhangwatnotai  Tim Roughgarden  and Qiqi Yan. Revenue maximization with a single sample.

Games and Economic Behavior  91:318–333  2015.

[12] Aryeh Dvoretzky  Jack Kiefer  and Jacob Wolfowitz. Asymptotic minimax character of the sample
distribution function and of the classical multinomial estimator. The Annals of Mathematical Statistics 
27(3):642–669  1956.

[13] VL Glivenko. Sulla determinazione empirica delle leggi di probabilita. Giornalle dell’Istituto Italiano

degli Attuari  4:92–99  1933.

[14] Yannai A. Gonczarowski and Noam Nisan. Efﬁcient empirical revenue maximization in single-parameter
auction environments. In Proceedings of the 49th Annual ACM Symposium on Theory of Computing
(STOC)  pages 856–868  2017.

[15] Zhiyi Huang  Yishay Mansour  and Tim Roughgarden. Making the most of your samples. In Proceedings

of the 16th ACM Conference on Economics and Computation (EC)  pages 45–60  2015.

[16] Vladimir Koltchinskii and Dmitriy Panchenko. Rademacher Processes and Bounding the Risk of Function

Learning  pages 443–457. Birkhäuser Boston  Boston  MA  2000.

[17] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The Annals of Probability 

18(3):1269–1283  1990.

[18] Jamie Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly optimal auctions. In
Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS)  pages 136–144 
2015.

[19] Jamie Morgenstern and Tim Roughgarden. Learning simple auctions. In Proceedings of the 29th Annual

Conference on Learning Theory (COLT)  pages 1298–1318  2016.

[20] Roger Myerson. Optimal auction design. Mathematics of Operations Research  6(1):58–73  1981.

[21] Tim Roughgarden and Okke Schrijvers. Ironing in the dark. In Proceedings of the 17th ACM Conference

on Economics and Computation (EC)  pages 1–18  2016.

[22] A. W. van der Vaart and Jon August Wellner. Weak convergence and empirical processes : with applications

to statistics. Springer series in statistics. Springer  New York  1996. Réimpr. avec corrections 2000.

[23] Vladimir Vapnik. Statistical Learning Theory. Wiley  1998.

[24] V.N. Vapnik and A.Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to

their probabilities. Theory Probab. Appl.  16:264–280  1971.

10

,Karin Knudson
Jonathan Pillow
Noga Alon
Moshe Babaioff
Yannai A. Gonczarowski
Yishay Mansour
Shay Moran
Amir Yehudayoff