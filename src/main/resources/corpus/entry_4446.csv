2017,Process-constrained batch Bayesian optimisation,Abstract Prevailing batch Bayesian optimisation methods allow all control variables to be freely altered at each iteration. Real-world experiments  however  often have physical limitations making it time-consuming to alter all settings for each recommendation in a batch. This gives rise to a unique problem in BO: in a recommended batch  a set of variables that are expensive to experimentally change need to be fixed  while the remaining control variables can be varied. We formulate this as a process-constrained batch Bayesian optimisation problem. We propose two algorithms  pc-BO(basic) and pc-BO(nested). pc-BO(basic) is simpler but lacks convergence guarantee. In contrast pc-BO(nested) is slightly more complex  but admits convergence analysis. We show that the regret of pc-BO(nested) is sublinear. We demonstrate the performance of both pc-BO(basic) and pc-BO(nested) by optimising benchmark test functions  tuning hyper-parameters of the SVM classifier  optimising the heat-treatment process for an Al-Sc alloy to achieve target hardness  and optimising the short polymer fibre production process.,Process-constrained batch Bayesian Optimisation

Pratibha Vellanki1  Santu Rana1  Sunil Gupta1  David Rubin2

Alessandra Sutti2  Thomas Dorin2  Murray Height2 Paul Sandars3  Svetha Venkatesh1

1Centre for Pattern Recognition and Data Analytics

Deakin University  Geelong  Australia

[pratibha.vellanki  santu.rana  sunil.gupta  svetha.venkatesh@deakin.edu.au]

2Institute for Frontier Materials  GTP Research

Deakin University  Geelong  Australia

[d.rubindecelisleal  alessandra.sutti  thomas.dorin  murray.height@deakin.edu.au]

3Materials Science and Engineering  Michigan Technological University  USA

[sanders@mtu.edu]

Abstract

Prevailing batch Bayesian optimisation methods allow all control variables to be
freely altered at each iteration. Real-world experiments  however  often have phys-
ical limitations making it time-consuming to alter all settings for each recommend-
ation in a batch. This gives rise to a unique problem in BO: in a recommended
batch  a set of variables that are expensive to experimentally change need to be
Ô¨Åxed  while the remaining control variables can be varied. We formulate this
as a process-constrained batch Bayesian optimisation problem. We propose two
algorithms  pc-BO(basic) and pc-BO(nested). pc-BO(basic) is simpler but lacks
convergence guarantee. In contrast pc-BO(nested) is slightly more complex  but
admits convergence analysis. We show that the regret of pc-BO(nested) is sublin-
ear. We demonstrate the performance of both pc-BO(basic) and pc-BO(nested) by
optimising benchmark test functions  tuning hyper-parameters of the SVM clas-
siÔ¨Åer  optimising the heat-treatment process for an Al-Sc alloy to achieve target
hardness  and optimising the short polymer Ô¨Åbre production process.

1

Introduction

Experimental optimisation is used to design almost all products and processes  scientiÔ¨Åc and indus-
trial  around us. Experimental optimisation involves optimising input control variables in order to
achieve a target output. Design of experiments (DOE) [16] is the conventional laboratory and indus-
trial standard methodology used to efÔ¨Åciently plan experiments. The method is rigid - not adaptive
based on the completed experiments so far. This is where Bayesian optimisation offers an effective
alternative.
Bayesian optimisation [13  17] is a powerful probabilistic framework for efÔ¨Åcient  global optim-
isation of expensive  black box functions. The Ô¨Åeld is undergoing a recent resurgence  spurred by
new theory and problems and is impacting computer science broadly - tuning complex algorithms
[3  22  18  21]  combinatorial optimisation [24  12]  reinforcement learning [4]. Usually  a prior be-
lief in the form of Gaussian process is maintained over the possible set of objective functions and the
posterior is the reÔ¨Åned belief after updating the model with experimental data. The updated model
is used to seek the most promising location of function extrema by using a variety of criteria  e.g.
expected improvement (EI)  and upper conÔ¨Ådence bound (UCB). The maximiser of such a criteria
function is then recommended for the function evaluation. Iteratively the model is updated and re-
commendations are made till the target outcome is achieved. When concurrent function evaluations
are possible  Bayesian optimisation returns multiple suggestions  and this is termed as the batch

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(a) Heat treatment for Al-Sc - temperat-
ure time proÔ¨Åle

(b) Experimental setup for short polymer Ô¨Åbre production.

Figure 1: Examples of real-world applications requiring process constraints.

setting. Bayesian optimisation with batch setting has been investigated by [10  5  6  9  1] wherein
different strategies are used to recommend multiple settings at each iteration. In all these methods 
all the control variables are free to be altered at each iteration. However  in some situations needing
to change all the variables for a single batch may not be efÔ¨Åcient and this leads to the motivation of
our process-constrained Bayesian optimisation.
This work has been directly inÔ¨Çuenced from the way experiments are conducted in many real-world
scenarios with a typical limitation on resources. For example  in our work with metallurgists  we
were given a task to Ô¨Ånd the optimal heat-treatment schedule of an alloy which maximises the
strength. Heat-treatment involves taking the alloy through a series of exposures to different temper-
atures for a variable amount of durations as shown in Figure 1a. Typically  a heat treatment schedule
can last for multiple days  so doing one experiment at a time is not efÔ¨Åcient. Fortunately  a furnace
is big enough to hold multiple samples at the same time. If we have to perform multiple experiments
in one batch yet using only one furnace  then we must design our Bayesian optimisation recom-
mendations in such a way that the temperatures across a batch remain the same  whilst still allowing
the durations to vary. Samples would be put in the same oven  but would be taken out after dif-
ferent elapsed time for each step of the heat treatment. Similar examples abound in other domains
of process and product design. For short polymer Ô¨Åbre production a polymer is injected axially
within another Ô¨Çow of a solvent in a particular geometric manifold [20]. A representation of the
experimental setup marked with the parameters involved is shown in Figure 1b. When optimising
for the yield it is generally easy to change the Ô¨Çow parameters (pump speed setting) than changing
the device geometry (opening up the enclosure and modifying the physical conÔ¨Åguration). Hence in
this case as well  it is beneÔ¨Åcial to recommend a batch of suggested experiments at a Ô¨Åxed geometry
but allowing Ô¨Çow parameters to vary. Many such examples where the batch recommendations are
constrained by the processes involved have been encountered by the authors in realising the potential
of Bayesian optimisation for real-world applications.
To construct a more familiar application we use the hyper-parameter tuning problem for Support
Vector Machines (SVM). When we use parallel tuning using batch Bayesian optimisation  it may be
useful if all the parallel training runs Ô¨Ånished at the same time. This would require Ô¨Åxing the cost
parameter  while allowing the the other hyper-parameters to vary. Whist this may or may not be a
real concern depending on the use cases  we use it here as a case study.
We formulate this unique problem as process-constrained batch Bayesian optimisation. The recom-
mendation schedule needs to constrain a set of variables corresponding to control variables that are
experimentally expensive (time  cost  difÔ¨Åculty) to change (constrained set) and varies all the re-
maining control variables (unconstrained set). Our approach involves incorporating constraints on
stipulated control parameters and allowing the others to change in an unconstrained manner. The
mathematical formulation of our optimisation problem is as follows.

‚àó
x

= argmaxx‚ààX f (x)

and we want a batch Bayesian optimisation sequence

{{xt 0  xt 1  ...  xt K‚àí1}}T

t=1 such that ‚àÄt and xt k = [xuc
(cid:48)‚àÄk  k

(cid:48) ‚àà [0  ...  K ‚àí 1]

t kxc

t k] 

t k = xc
xc
t k

Where xc

t k is the kth constrained variable in tth batch and similarly xuc

t k is the kth unconstrained

variable in the tth batch. T is the total number of iterations and K is the batch-size.

2

Temperature (T)t1t2Time (t)t3t4T1T2T3T4Coagulant flow (ùë£ùë£ùëêùëê)Polymerflow (ùë£ùë£ùëùùëù)Constrictionangle (ùõºùõº)Channelwidth(‚Ñé)Deviceposition(ùëëùëë)ShortNano-fibersWe propose two approaches to the solve this problem: basic process-constrained Bayesian optimisa-
tion (pc-BO(basic)) and nested process-constrained batch Bayesian optimisation (pc-BO(nested)).
pc-BO(basic) is an intuitive modiÔ¨Åcation motivated by the work of [5] and pc-BO(nested) is based
on a nested Bayesian optimisation method we will describe in section 3. We formulate the al-
gorithms pc-BO(basic) and pc-BO(nested)  and for pc-BO(nested) we present the theoretic analysis
to show that the average regret vanishes superlinearly with iterations. We demonstrate the perform-
ance of pc-BO(basic) and pc-BO(nested) on both benchmark test functions and real world problems
that involve hyper-parameter tuning for SVM classiÔ¨Åcation for two datasets: breast cancer and bio-
degradable waste  the industrial problem of heat treatment process for an Aluminium-Scandium
(Al-Sc) alloy  and another industrial problem of short polymer Ô¨Åbre production process.

2 Related background

2.1 Bayesian optimisation

Bayesian optimisation is a sequential method of global optimisation of an expensive and unknown
black-box function f whose domain is X   to Ô¨Ånd its maxima x‚àó = argmax
f (x) (or minima). It is
x‚ààX
especially powerful when the function is expensive to evaluate and it does not have a closed-form
expression  but it is possible to generate noisy observations from experiments.
The Gaussian process (GP) is commonly used as a Ô¨Çexible way to place a prior over the unknown
function [14]. It is are completely described by the mean function m(x) and the covariance function
k(x  x(cid:48)) and they imply our belief and uncertainties about the objective function. Noisy observations
from the experiments are sequentially appended into the model  that in turn updates our belief about
the objective function.
The acquisition function is a surrogate utility function that takes a known tractable closed form and
allows us to choose the next query point. It is maximised in the place of the unknown objective
function and constructed such that it balances between exploring regions of high value (mean) and
exploiting regions of high uncertainties (variances) across the objective function.
Gaussian process based Upper ConÔ¨Ådence Bound (GP-UCB) proposed by [19] is one of the ac-
quisition functions which is shown to achieve sublinear growth in cumulative regret. It is deÔ¨Åne at
tthiteration as

GP‚àíU CB(x) = ¬µt‚àí1(x) +(cid:112)Œ≤tœÉt‚àí1(x)

Œ±t

(1)

where  v = 1 and Œ≤t = 2log(td/2+2œÄ2/3Œ¥) is the conÔ¨Ådence parameter  wherein t denotes the
iteration number  d represents the dimensionality of the data and Œ¥ ‚àà (0  1). We are motivated by
GP-UCB based methods. Although our approach can be intuitively extended to other acquisition
function  we do not explore this in the current work.

2.2 Batch Bayesian optimisation methods

The GP exhibits an interesting characteristic that its predictive variance is dependent on only the
input attributes while updating its mean requires knowledge about the outcome of the experiment.
This leads us to a direction of strategies for multiple recommendations. There are several batch
Bayesian optimisation algorithms for an unconstrained case. GP-BUCB by [6] recommends mul-
tiple batch points using the UCB strategy and the aforementioned characteristic. To Ô¨Åll up a batch  it
updates the variances with the available attribute information and appends the outcomes temporarily
by substituting them with most recently computed posterior mean. A similar strategy is used in
the GP-UCB-PE by [5] that optimises the unknown function by incorporating some batch elements
where uncertainty is high. GP-UCB-PE computes the Ô¨Årst batch element by using the UCB strategy
and recommends the rest of the points by relying on only the predictive variance  and not the mean.
It has been shown that for these GP-UCB based algorithms the regret can be bounded tighter than
the single recommendation methods. To the best of our knowledge these existing batch Bayesian
optimisation techniques do not address the process-constrained problem presented in this work. The
algorithms proposed in this paper are inspired by the previous approaches but address it in context
of a process-constrained setting.

3

2.3 Constrained-batch vs. constrained-space optimisation

We refer to the parameters that are not allowed to change (eg.
temperatures for heat treatment 
or device geometry for Ô¨Åbre production) as constrained set and the other parameters (heat treatment
durations or Ô¨Çow parameters) as unconstrained set. We emphasise that our usage of constraint differs
from the problem settings presented in literature  for example in [2  11  7  8]  where the parameters
values are constrained or the function evaluations are constrained by inequalities. In the problem
setting that we present  all the parameters exist in unconstrained space; for each individual batch 
the constrained variables should have the same value.

3 Proposed method
We recall the maximisation problem from Section 1 as x‚àó = argmaxx‚ààX f (x).
X uc ‚à™ X c  where X c is the constrained subspace and X uc is the unconstrained subspace.

In our case X =

Algorithm 1 pc-BO(basic): Basic process-constrained pure exploration batch Bayesian optimisation
algorithm.
while (t < M axIter)

Algorithm 2 pc-BO(nested): Nested process-constrained batch Bayesian optimisation algorithm.
while (t < M axIter)

for k = 1  ..  K ‚àí 1

t 0xc
t 0

xt 0 =(cid:2)xuc
D = D ‚à™(cid:8)(cid:2)xuc

end

(cid:3) = argmaxx‚ààX Œ±GP‚àíU CB (xt 0 | D)
t 0 (cid:8)xuc
(cid:3)   f(cid:0)(cid:2)xuc

t k | D  xc
(cid:3)(cid:1)(cid:9)K‚àí1
xuc

(cid:16)

t kxc
t 1

t k = argmax xuc‚ààX uc œÉ
xuc

t kxc
t 1

k=0

t k(cid:48)(cid:9)k(cid:48)<k(cid:17)

t = argmaxxc‚ààX c Œ±GP‚àíU CB
xc
(xc
t 0 = argmaxxuc‚ààX uc Œ±GP‚àíU CB
xuc
for k = 1  ...  K-1

uc

c

end

t k = argmaxxuc‚ààX uc œÉuc
xuc

DO = DO ‚à™(cid:8)xc
DI = DI ‚à™(cid:8)(cid:2)xuc

t   f(cid:0)(cid:2)(xuc
(cid:3)   f(cid:0)(cid:2)xuc

t )+ xc
t

t kxc
t

t kxc
t

t | DO)
(xuc
t

(cid:16)
(cid:3)(cid:1)(cid:9)
(cid:3)(cid:1)(cid:9)K‚àí1

xuc
t

k=0

| DI   xc

| DI   xc
t )

t k(cid:48)(cid:9)k(cid:48)<k(cid:17)
t  (cid:8)xuc

end

end

A na√Øve approach to solving the process is to employ any standard batch Bayesian optimisation
algorithm where the Ô¨Årst member is generated and then subsequent members are Ô¨Ålled up by setting
the constraint variables to that of the Ô¨Årst member. We describe this approach as the basic process-
constrained pure exploration batch Bayesian optimisation (pc-BO(basic)) algorithm as detailed in
algorithm 1  where Œ±GP‚àíU CB(x | D) is the acquisition function as deÔ¨Åned in Equation 1. We note
that pc-BO(basic) is an improvisation over the work of [5]. During each iteration  the Ô¨Årst batch
element is recommended using the UCB strategy. The remaining batch elements  as in GP-UCB-
PE  are generated by updating the posterior variance of the GP  after the constrained set attributes
are Ô¨Åxed to those of the Ô¨Årst batch element.
We provide an alternate formulation via a nested optimisation problem called nested process-
constrained batch Bayesian optimisation (pc-BO(nested)) with two stages. For each batch  in the
outer stage optimisation is performed to Ô¨Ånd the optimal values of the constrained variables and
in the inner stage optimisation is performed to Ô¨Ånd optimal values of the unconstrained variables.
(x | D) is the acquisition function for the
The algorithm is detailed in algorithm 2  where Œ±GP‚àíU CB
(x | D) is the acquisition function for the inner stage as deÔ¨Åned in Equa-
outer stage  and Œ±GP‚àíU CB
t ‚àà(cid:110)
t ])  is the unconstrained batch parameter that yields the
tion 1  and (xuc
argmax

uc
t )+ =

(cid:111)K‚àí1

f ([xuc

t xc

xuc

c

xuc
t k

best target goal for the given constrained parameter xc. We are able to analyse the convergence of

k=0

4

pc-BO(nested). It can be expected that in some cases the performance of the pc-BO(basic) and pc-
BO(nested) are close. The pc-BO(basic) method maybe considered simpler  but it lacks guaranteed
convergence.

3.1 Convergence analysis for pc-BO(nested)

We now present the analysis of the convergence of pc-BO(nested) as described in Algorithm 2. The
outer stage optimisation problem for xc and observation Do is expressed as follows.

where 

‚àó

(xc)
g(xc) (cid:44)
(cid:39)

= argmaxxc‚ààX c g(xc) 
f ([xucxc])

max

xuc‚ààX uc

where  X uc (cid:44) {{xt 0  xt 1  ...  xt K‚àí1}}T

such that  xc

t k = xc 

f ([xucxc]) = f ([(xuc)+xc]) 

max

xuc‚ààXuc

DO (cid:44) (cid:110)

xc
t   f

(cid:16)(cid:104)(cid:0)xuc

t k

(cid:1)+ xc

t 

(cid:105)(cid:17)(cid:111)T

t=1

t=1

And the inner stage optimisation problem for xuc and observation DI is expressed as follows.

where 

= argmaxxuc‚ààX uc h (xuc)  

‚àó
(xuc)
h(xuc) (cid:44) f ([xucxc])
t kxc
t

DI (cid:44) (cid:110)(cid:8)(cid:2)xuc

(cid:3)   f(cid:0)(cid:2)xuc

t kxc
t

(cid:3)(cid:1)(cid:9)K‚àí1

(cid:111)T

k=0

t=1

This is solved using a Bayesian optimisation routine. Here (xuc)+ is the unconstrained batch para-
meter that yields the best target goal for the given constrained parameter xc. Unfortunately as g(xc)
is not easily measurable  we use f ([(xuc)+xc]) as an approximation to it. To address this we use
a provable batch Bayesian optimisation such as GP-UCB-PE [5] in the inner stage. The loops are
performed together where in each iteration t  the outer loop Ô¨Årst recommends a single recommend-
ation of xc
k=1. Combining them we get process-
constrained set of recommendations. We show that together these two Bayesian optimisation loops
converge to the optimal solution.
Let us denote (xuc

t and then the inner loop suggests a batch (cid:8)xuc

t ]). Following that we can write g(xc) as 

t )+ = argmax

(cid:9)K

f ([xucxc

t k

g(xc) = f(cid:0)(cid:2)(xuc
(cid:16)(cid:104)

‚àó

xc
t 

t )

xuc‚àà{xuc

k }K

k=1

(cid:16)(cid:104)

(cid:3)(cid:1) = f
(cid:105)(cid:17)

= f

(xuc

t )+ xc

t 

+ ruc
t

(xuc

t )+ xc

t 

(cid:105)(cid:17)

+ f(cid:0)(cid:2)(xuc

‚àó

xc
t 

t )

(cid:16)(cid:104)

(cid:3)(cid:1) ‚àí f

(xuc

t )+ xc

t 

(cid:105)(cid:17)

is the regret of the inner loop.

where ruc
The observational model is given as

t

(cid:16)(cid:104)

Lemma 1. For regret of the inner loop (cid:80)T

yc = g(xc) +  = f

(xuc

t )+ xc

t=1

t + 

+ ruc

(cid:1)2 ‚â§ Œ≤uc

1 C uc

1 Œ≥uc

T + œÄ2

6

(cid:105)(cid:17)
(cid:0)rK

t 

t

(2)

(3)

where  ‚àº N (0  œÉ2)

Proof. As we use GP-UCB-PE for unconstrained parameter optimisation  we can say that the regret
t . Now  even though
t = min rk
rK
t
every batch recommendation for xc will always be run for one iteration only  the œÉ0
t (xt) is computed
from the updated GP. Hence the sum of (œÉ0
t )2 can be upper bounded by Œ≥T . Thus 

‚àÄk = 0  ...  K ‚àí 1 (Lemma 1  [5]). Hence  rK

‚àö
Œ≤1œÉ0

t ‚â§ r0

t ‚â§ 2

T(cid:88)

(cid:16)

t=1

(cid:17)2 ‚â§ Œ≤uc

rK
t

1 C uc

1 Œ≥uc

T +

œÄ2
6

(4)

A‚ààX c |A|=T

Œ≤1 = 2log(1d/2+2œÄ2/3Œ¥)
max

‚àí2);
I(yA : fA) assuming y = f +   where  ‚àº N (0  œÉ2/2) is the maximum

Here 
Œ≥T =
information gain after T rounds. (Please see supplementary material 5 for derivation)
Lemma 2. For the variance of ruc

t has the order of œÉ2

C1 = 8/log(1 + œÉ

conÔ¨Ådence

parameter;

rt ‚àº O(C uc

t + C uc
2 )

1 Œ≤uc

1 Œ≥uc

the

is

5

Proof. We use PE algorithm [5] to compute K-recommendation  hence the variance of the regret
t can be bounded above by
ruc

‚â§ E((ruc

t )2) ‚â§ E

œÉ2
ruc
t

(ruc

t(cid:48) )2

= E

min
k<K

(ruc

t(cid:48)k)2

(cid:33)

(cid:32)

t(cid:88)

t(cid:48)=0

1
t

(cid:33)

The second inequality holds since on an average the gap ruc
t )+xc]) decreases with
iteration t  ‚àÄxc ‚àà X c. From equation 3  equation 4 and using the Lemma 4 and 5 of [5] we can write

t = g(xc)‚àí f ([(xuc

(cid:32)

t(cid:88)

t(cid:48)=0

1
t

(cid:33)

min
k<K

(ruc

t(cid:48)k)2

‚àº O(

1
t

C uc

1 Œ≤uc

1 Œ≥uc

t + C uc
2 )

(5)

2 ‚àà R. Œ≥t is the maximum information gain over t samples. This concludes the

(cid:32)

E

t(cid:88)

t(cid:48)=0

1
t

for some C uc 
proof.

1 C uc

The following lemma guarantees an existence of a Ô¨Ånite T0 after which the noise variance coming
from the inner optimisation loop becomes smaller than the noise in the observation model.
Lemma 3. ‚àÉT0 < ‚àû for which œÉ2

‚â§ œÉ2.

ruc
T0

Proof. In Lemma 1 C uc 
1 C uc
quantity of the form M1 √ó 1
the lemma is proved.

2 and Œ≤uc
t C uc
1 Œ≥uc

1 Œ≤uc

1 are Ô¨Åxed constant and Œ≥uc
t + C uc

tK is sublinear in t. Therefore  any
2 also decreases sublinearly with t for ‚àÄM1 ‚àà R. Hence

Let us denote the instantaneous regret for the outer Bayesian optimisation loop as rc
g(xc

t )  we can write the average regret after T iterations as 

t = g((xc)‚àó) ‚àí

¬ØRT =

(cid:88)
(2(cid:112)Œ≤c

T(cid:88)
(cid:114)

t=0

Œ≤c
T

1
T

t ‚â§ 1
rc
T

(cid:80)(œÉc

‚â§ 2

t ))2

t‚àí1(xc
T

+

1
T

t ) +

t œÉc

t‚àí1(xc

(cid:88) 1

t2

1
t2 )

(cid:33)

T(cid:88)

t=1

1
t2

+

1
T

(6)

(7)

using the Lemma 5.8 of [19] and Cauchy-Schwartz inequality.
¬ØRT ‚Üí 0
Lemma 4. For the outer Bayesian optimisation lim
T‚Üí‚àû

Proof. From the equation 6

T(cid:88)
T(cid:88)

t=1

1
t2

(œÉc

t‚àí1(xc

t ))2

(cid:80)T
(cid:32) T0(cid:88)

Œ≤c
T

(cid:115)
(cid:118)(cid:117)(cid:117)(cid:116) Œ≤c
(cid:114)

T
T

Œ≤c
T
T

¬ØRT ‚â§ 2

= 2

‚â§ 2

t‚àí1(xc

t ))2

t=1(œÉc
T

+

1
T

((œÉc

t‚àí1(xc

t ))2 +

t=1

(AT0 + BT ) +

t=T0+1

1
t2

1
T

T(cid:88)
T(cid:88)

t=1

We then show that AT0 is upper bounded by a constant irrespective of T as long as T ‚â• T0 and BT is
sublinear with T . Œ≤c
6 . Hence the right hand side vanishes as
T ‚Üí ‚àû. The details of the proof is presented in the supplementary material.

T is sublinear in T and lim
T‚Üí‚àû

t2 = œÄ2

t=1

1

However  in reality using regret as the upper bound on ruc
is not necessary  as a tighter upper bound
may exist when we know the maximum value of the function1 and we can safely alter the upper
bound as 

t

t )+xc])  2(cid:112)Œ≤1œÉuc

t‚àí1(xuc

0 ))

(8)

t ‚â§ min(f max ‚àí f ([(xuc
ruc
The above results holds since Lemma 2 still holds.

1e.g. for hyper-parameter tuning we know that maximum value of accuracy is 1.

6

Figure 2: Synthetic test function optimisation using pc-BO(nested)  pc-BO(basic) and s-BO. The
zoomed area on the respective scale is shown for Branin and Goldstein-Price.

4 Experiments

We conducted a set of experiments using both synthetic data and real data to demonstrate the per-
formance of pc-BO(basic) and pc-BO(nested). To the best of our knowledge  there are no other
methods that can selectively constrain parameters in each batch during Bayesian optimisation. Fur-
ther  we also show the results for the test function optimisation using sequential BO (s-BO) using
GP-UCB.
The code is implemented in MATLAB and all the experiments are run on an Intel CPU E5-2640 v3
@2.60GHz machine. We use the squared exponential distance kernel. To show the performance 
we plot the results as the best outcome so far against the number of iterations performed. The
uncertainty bars in the Ô¨Ågures pertain to 10 runs of BO algorithms with different initialisations for a
batch of 3 recommendations. The errors bars show the standard error and the graph shows the mean
best outcome until the respective iteration.

4.1 Benchmark test function optimisation

In this section  we use benchmark test functions and demonstrate the performance of pc-BO(basic)
and pc-BO(nested). We apply the test functions by constraining the second parameter and Ô¨Ånding
the best conÔ¨Åguration across the Ô¨Årst parameter (unconstrained). The Branin  Ackley  Goldstein-
Price and the Egg-holder functions were optimised using pc-BO(basic) and pc-BO(nested)  and the
results are shown in Figure 2. From the results  we note that the pc-BO(nested) is marginally better
or similar in performance when compared with pc-BO(basic). It also shows that batch Bayesian
optimisation is more efÔ¨Åcient in terms of number of iterations than a purely sequential approach for
the problem at hand.

4.2 Hyper-parameter tuning for SVM

Support vector machines with RBF kernel require hyper-parameter tuning for Cost (C) and Gamma
(Œ≥). Out of these parameters  the cost is a critical parameter that trades off error for generalisation.
Consider tuning SVM‚Äôs in parallel. The cost parameter strongly affects the time required for training
SVM. It would be inconvenient if one training process took much longer than the other. Thus
constraining the cost parameter for a single batch maybe a good idea. We use our algorithms to tune

7

0102030 4050607080number of iterations 0.850.90.951best value so farBranin (normalised)pc-BO(nested)pc-BO(basic)s-BO0102030 4050607080number of iterations 0.10.20.30.40.50.60.70.80.91best value so farAckley (normalised)pc-BO(nested)pc-BO(basic)s-BO0102030 4050607080number of iterations 0.930.940.950.960.970.980.991best value so farGoldstein-Price (normalised)pc-BO(nested)pc-BO(basic)s-BO0102030 4050607080number of iterations 0.550.60.650.70.750.80.850.90.951best value so farEgg-holder (normalised)pc-BO(nested)pc-BO(basic)s-BOboth the hyper-parameters C and Œ≥  at each batch only varying Œ≥  but not C. This is demonstrated on
the classiÔ¨Åcation using SVM problem using two datasets downloaded from UCI machine learning
repository: Breast cancer dataset (BCW) and Bio-degradation dataset (QSAR).
BCW has 683 instances with 9 attributes each of the data  where the instances are labelled as be-
nign or malign tumour as per the diagnosis. The QSAR dataset categorises 1055 chemicals with
42 attributes as ready or not ready biodegradable waste. The results are plotted as best accuracy
obtained across number of iterations. We observe from the results in Figure 3  that pc-BO(nested)
again performs marginally better than pc-BO(basic) for the BCW dataset. For the QSAR dataset 
pc-BO(nested) higher accuracy with lesser iterations than what pc-BO(basic) requires.

4.3 Heat treatment for an Al-Sc alloy

Alloy casting involves heat treatment process - exposing the cast to different temperatures for select
times  that ensures target hardness of the alloy. This process is repeated in steps. The underlying
physics of heat-treatment of an alloy is based on nucleation and growth. During the nucleation pro-
cess  ‚Äúnew phases‚Äù or precipitates are formed when clusters of atom self organise. This is a difÔ¨Åcult
stochastic process that happens at lower temperatures. These precipitates then diffuse together to
achieve the requisite target alloy characteristics in the growth step. KWN [15  23] is the industrial
standard precipitation model for the kinetics of nucleation and growth steps. As a preliminary study
we use this simulator to demonstrate the strength of our algorithm.
As explained in the introduction  it is cost efÔ¨Åcient to test heat treatment in the real world by varying
the time and keeping the temperature constrained in each batch. This will allow us to test multiple
samples at one go in a single oven. We use the same constrains for our simulator driven study. We
consider a two stage heat treatment process. The input to Ô¨Årst stage is the alloy composition  the
temperature and time. The nucleation output of this stage is input to the the second stage along
with the temperature and time for the second stage. The Ô¨Ånal output is hardness of the material
(strength in kPa). To optimise this two stage heat treatment process our inputs are [T1  T2  t1  t2] 
where [T1  T2] represent temperatures in Celsius  [t1  t2] represent the time in minutes for each stage.
Figure 4 shows the results of the heat-treatment process optimisation.

4.4 Short polymer Ô¨Åbre polymer production

Short polymer Ô¨Åbre production is a set of experiments we conducted in collaboration with material
scientists at Deakin University. For production of short polymer Ô¨Åbres  a polymer rich Ô¨Çuid is
injected coaxially into the Ô¨Çow of another solvent in a particular geometric manifold. The parameters
included in this experiment are device position in mm  constriction angle in degrees  channel width
in mm  polymer Ô¨Çow in ml/hr  and coagulant speed in cm/s. The Ô¨Ånal output  the combined utility is
the distance of the length and diameter of the polymer from target polymer. The goal is to optimise
the input parameters to obtain a polymer Ô¨Åbre of a desired length and diameter. As explained in the
introduction  it is efÔ¨Åcient to test multiple combinations of polymer Ô¨Çow and coagulant speed for a
Ô¨Åxed geometric setup than in a single batch.

Figure 3: Hyper-parameter tuning for SVM based classiÔ¨Åcation on Breast Cancer Data (BCW) and
bio-degradable waste data (QSAR) using pc-BO(nested) and pc-BO(basic)

8

0102060708030 40 50 number of iterations 0.850.90.951accuracySVM with BCWpc-BO(nested)pc-BO(basic)0102060708030 40 50 number of iterations 0.70.750.80.850.9SVM with QSARpc-BO(nested)pc-BO(basic)accuracyFigure 4: Results for heat-treatment and short polymer Ô¨Åbre production processes. (a) Experimental
result for Al-Sc heat treatment proÔ¨Åle for a two stage heat-treatment process using pc-BO(nested)
and pc-BO(basic). (b) Optimisation for short polymer Ô¨Åbre production with position  constriction
angle and channel width constrained for each batch. Polymer Ô¨Çow and coagulant speed are uncon-
strained. The optimisation is shown for pc-BO(nested) and pc-BO(basic) algorithms.

The parameters in this experiments are discrete  where every parameter takes 3 discrete values 
except the constriction angle which takes 2 discrete values. Coagulant speed and polymer Ô¨Çow are
unconstrained parameters and channel width  constriction angle and position are the constrained
parameters. We conducted the experiment in batches of 3. The Figure 4 shows the optimisation
results for this experiment over 53 iterations.

5 Conclusion

We have identiÔ¨Åed a new problem in batch Bayesian optimisation  motivated from physical limita-
tions in real world experiments while conducting batch experiments. It is not feasible and resource-
friendly to change all available settings in scientiÔ¨Åc and industrial experiments for a batch. We
propose process-constrained batch Bayesian optimisation for such applications  where it is prefer-
able to Ô¨Åx the values of some variables in a batch. We propose two approaches to solve the problem
of process-constrained batches pc-BO(basic) and pc-BO(nested). We present analytical proof for
convergence of pc-BO(nested). Synthetic functions  and real world experiments: hyper-parameter
tuning for SVM  alloy heat treatment process  and short polymer Ô¨Åber production process were op-
timised using the proposed algorithms. We found that pc-BO(nested) in each of these scenarios is
either more efÔ¨Åcient or equally well performing compared with pc-BO(basic).

Acknowledgements

This research was partially funded by the Australian Government through the Australian Research
Council (ARC) and the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning.
Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).

References
[1] J. Azimi  A. Fern  and X. Z. Fern. Batch bayesian optimization via simulation matching. In Advances in

Neural Information Processing Systems  pages 109‚Äì117  2010.

[2] J. Azimi  X. Fern  and A. Fern. Budgeted optimization with constrained experiments. Journal of ArtiÔ¨Åcial

Intelligence Research  56:119‚Äì152  2016.

[3] J. Bergstra  R. Bardenet  Y. Bengio  and B. K√©gl. Algorithms for hyper-parameter optimization.

Advances in Neural Information Processing Systems  pages 2546‚Äì2554  2011.

In

[4] E. Brochu  V. M. Cora  and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions 
with application to active user modeling and hierarchical reinforcement learning. arXiv:1012.2599  (UBC
TR-2009-023 and arXiv:1012.2599)  2010.

[5] E. Contal  D. Buffoni  A. Robicquet  and N. Vayatis. Parallel gaussian process optimization with up-
In Joint European Conference on Machine Learning and

per conÔ¨Ådence bound and pure exploration.
Knowledge Discovery in Databases  pages 225‚Äì240. Springer  2013.

9

05102025300number of iterations iterations7590105120Hardness of the alloypc-BO(nested)pc-BO(basic)Al-Sc alloy heat treatment15 0510152025303540number of iterations00.20.40.60.81best combined utility of polymershort polymer fibre productionpc-BO(nested)pc-BO(basic)[6] T. Desautels  A. Krause  and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in gaussian

process bandit optimization. Journal of Machine Learning Research  15(1):3873‚Äì3923  2014.

[7] J. R. Gardner  M. J. Kusner  Z. E. Xu  K. Q. Weinberger  and J. P. Cunningham. Bayesian optimization

with inequality constraints. In International Conference on Machine Learning  pages 937‚Äì945  2014.

[8] M. A. Gelbart  J. Snoek  and R. P. Adams. Bayesian optimization with unknown constraints. In Uncer-

tainty in ArtiÔ¨Åcial Intelligence  pages 250‚Äì259  2014.

[9] D. Ginsbourger  R. Le Riche  and L. Carraro. A multi-points criterion for deterministic parallel global

optimization based on gaussian processes. Technical report  2008.

[10] J. Gonz√°lez  Z. Dai  P. Hennig  and N. D. Lawrence. Batch bayesian optimization via local penalization.

In ArtiÔ¨Åcial Intelligence and Statistics  pages 648‚Äì657  2015.

[11] J. M. Hern√°ndez-Lobato  M. A. Gelbart  R. P. Adams  M. W. Hoffman  and Z. Ghahramani. A general
framework for constrained bayesian optimization using information-based search. Journal of Machine
Learning Research  17(160):1‚Äì53  2016.

[12] F. Hutter  H. H. Hoos  and K. Leyton-Brown. Sequential model-based optimization for general algorithm

conÔ¨Åguration. In Learning and Intelligent Optimization  pages 507‚Äì523  2011.

[13] D. R. Jones  M. Schonlau  and W. J. Welch. EfÔ¨Åcient global optimization of expensive black-box func-

tions. Journal of Global optimization  13(4):455‚Äì492  1998.

[14] C. E. Rasmussen. Gaussian processes for machine learning. 2006.

[15] J. Robson  M. Jones  and P. Prangnell. Extension of the n-model to predict competing homogeneous and

heterogeneous precipitation in al-sc alloys. Acta Materialia  51(5):1453‚Äì1468  2003.

[16] J. Sacks  W. J. Welch  T. J. Mitchell  and H. P. Wynn. Design and analysis of computer experiments.

Statistical science  pages 409‚Äì423  1989.

[17] B. Shahriari  K. Swersky  Z. Wang  R. P. Adams  and N. de Freitas. Taking the human out of the loop: A

review of bayesian optimization. Proceedings of the IEEE  104(1):148‚Äì175  2016.

[18] J. Snoek  H. Larochelle  and R. P. Adams. Practical bayesian optimization of machine learning algorithms.

In Advances in Neural Information Processing Systems  pages 2960‚Äì2968  2012.

[19] N. Srinivas  A. Krause  S. Kakade  and M. W. Seeger. Gaussian process optimization in the bandit setting:
In Proceedings of the 27th International Conference on Machine

No regret and experimental design.
Learning (ICML-10)  June 21-24  2010  Haifa  Israel  pages 1015‚Äì1022  2010.

[20] A. Sutti  T. Lin  and X. Wang. Shear-enhanced solution precipitation: a simple process to produce short

polymeric nanoÔ¨Åbers. Journal of nanoscience and nanotechnology  11(10):8947‚Äì8952  2011.

[21] K. Swersky  J. Snoek  and R. P. Adams. Multi-task bayesian optimization. In Advances in Neural Inform-

ation Processing Systems  pages 2004‚Äì2012  2013.

[22] C. Thornton  F. Hutter  H. H. Hoos  and K. Leyton-Brown. Auto-weka: combined selection and hyper-
parameter optimization of classiÔ¨Åcation algorithms. In International Conference on Knowledge Discovery
and Data Mining  pages 847‚Äì855  2013.

[23] R. Wagner  R. Kampmann  and P. W. Voorhees. Homogeneous Second-Phase Precipitation. Wiley Online

Library  1991.

[24] Z. Wang  M. Zoghi  F. Hutter  D. Matheson  and N. de Freitas. Bayesian optimization in high dimensions
via random embeddings. In International Joint Conference on ArtiÔ¨Åcial Intelligence  pages 1778‚Äì1784 
2013.

10

,Pratibha Vellanki
Santu Rana
Sunil Gupta
Alessandra Sutti
Thomas Dorin
Murray Height
Svetha Venkatesh