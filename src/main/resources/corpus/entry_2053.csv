2015,Fast and Guaranteed Tensor Decomposition via Sketching,Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper  we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches  but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs  without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity  uniformity of elements  etc. We apply the method for topic mod- eling and obtain competitive results.,FastandGuaranteedTensorDecompositionviaSketchingYiningWang Hsiao-YuTung AlexSmolaMachineLearningDepartmentCarnegieMellonUniversity Pittsburgh PA15213{yiningwa htung}@cs.cmu.edualex@smola.orgAnimaAnandkumarDepartmentofEECSUniversityofCaliforniaIrvineIrvine CA92697a.anandkumar@uci.eduAbstractTensorCANDECOMP/PARAFAC(CP)decompositionhaswideapplicationsinstatisticallearningoflatentvariablemodelsandindatamining.Inthispaper weproposefastandrandomizedtensorCPdecompositionalgorithmsbasedonsketching.Webuildontheideaofcountsketches butintroducemanynovelideaswhichareuniquetotensors.Wedevelopnovelmethodsforrandomizedcom-putationoftensorcontractionsviaFFTs withoutexplicitlyformingthetensors.Suchtensorcontractionsareencounteredindecompositionmethodssuchasten-sorpoweriterationsandalternatingleastsquares.Wealsodesignnovelcollidinghashesforsymmetrictensorstofurthersavetimeincomputingthesketches.Wethencombinethesesketchingideaswithexistingwhiteningandtensorpoweriter-ativetechniquestoobtainthefastestalgorithmonbothsparseanddensetensors.Thequalityofapproximationunderourmethoddoesnotdependonpropertiessuchassparsity uniformityofelements etc.Weapplythemethodfortopicmod-elingandobtaincompetitiveresults.Keywords:TensorCPdecomposition countsketch randomizedmethods spec-tralmethods topicmodeling1IntroductionInmanydata-richdomainssuchascomputervision neuroscienceandsocialnetworksconsistingofmulti-modalandmulti-relationaldata tensorshaveemergedasapowerfulparadigmforhan-dlingthedatadeluge.Animportantoperationwithtensordataisitsdecomposition wheretheinputtensorisdecomposedintoasuccinctform.OneofthepopulardecompositionmethodsistheCANDECOMP/PARAFAC(CP)decomposition alsoknownascanonicalpolyadicdecomposition[12 5] wheretheinputtensorisdecomposedintoasuccinctsumofrank-1components.TheCPdecompositionhasfoundnumerousapplicationsindatamining[4 18 20] computationalneuro-science[10 21] andrecently instatisticallearningforlatentvariablemodels[1 30 28 6].Forlatentvariablemodeling thesemethodsyieldconsistentestimatesundermildconditionssuchasnon-degeneracyandrequireonlypolynomialsampleandcomputationalcomplexity[1 30 28 6].Giventheimportanceoftensormethodsforlarge-scalemachinelearning therehasbeenanin-creasinginterestinscalinguptensordecompositionalgorithmstohandlegiganticreal-worlddatatensors[27 24 8 16 14 2 29].However thepreviousworksfallshortinmanyways asdescribedsubsequently.Inthispaper wedesignandanalyzeefﬁcientrandomizedtensormethodsusingideasfromsketching[23].Theideaistomaintainalow-dimensionalsketchofaninputtensorandthenperformimplicittensordecompositionusingexistingmethodssuchastensorpowerupdates alter-natingleastsquaresoronlinetensorupdates.Weobtainthefastestdecompositionmethodsforbothsparseanddensetensors.Ourframeworkcaneasilyhandlemodernmachinelearningapplicationswithbillionsoftraininginstances andatthesametime comeswithattractivetheoreticalguarantees.1Ourmaincontributionsareasfollows:Efﬁcienttensorsketchconstruction:Weproposeefﬁcientconstructionoftensorsketcheswhentheinputtensorisavailableinfactoredformssuchasinthecaseofempiricalmomenttensors wherethefactorcomponentscorrespondtorank-1tensorsoverindividualdatasamples.WeconstructthetensorsketchviaefﬁcientFFToperationsonthecomponentvectors.Sketchingeachrank-1componenttakesO(n+blogb)operationswherenisthetensordimensionandbisthesketchlength.ThisismuchfasterthantheO(np)complexityforbruteforcecomputationsofapth-ordertensor.SinceempiricalmomenttensorsareavailableinthefactoredformwithNcomponents whereNisthenumberofsamples ittakesO((n+blogb)N)operationstocomputethesketch.Implicittensorcontractioncomputations:Almostalltensormanipulationscanbeexpressedintermsoftensorcontractions whichinvolvesmultilinearcombinationsofdifferenttensorﬁbres[19].Forexample tensordecompositionmethodssuchastensorpoweriterations alternatingleastsquares(ALS) whiteningandonlinetensormethodsallinvolvetensorcontractions.Weproposeahighlyefﬁcientmethodtodirectlycomputethetensorcontractionswithoutformingtheinputtensorex-plicitly.Inparticular giventhesketchofatensor eachtensorcontractioncanbecomputedinO(n+blogb)operations regardlessoforderofthesourceanddestinationtensors.Thissigniﬁ-cantlyacceleratesthebrute-forceimplementationthatrequiresO(np)complexityforpth-orderten-sorcontraction.Inaddition inmanyapplications theinputtensorisnotdirectlyavailableandneedstobecomputedfromsamples suchasthecaseofempiricalmomenttensorsforspectrallearningoflatentvariablemodels.Insuchcases ourmethodresultsinhugesavingsbycombiningimplicittensorcontractioncomputationwithefﬁcienttensorsketchconstruction.Novelcollidinghashesforsymmetrictensors:Whentheinputtensorissymmetric whichisthecaseforempiricalmomenttensorsthatariseinspectrallearningapplications weproposeanovelcollidinghashdesignbyreplacingtheBooleanringwiththecomplexringCtohandlemultiplicities.Asaresult itmakesthesketchbuildingprocessmuchfasterandavoidsrepetitiveFFToperations.Thoughthecomputationalcomplexityremainsthesame theproposedcollidinghashdesignresultsinsigniﬁcantspeed-upinpracticebyreducingtheactualnumberofcomputations.Theoreticalandempiricalguarantees:Weshowthatthequalityofthetensorsketchdoesnotdependonsparseness uniformentrydistribution oranyotherpropertiesoftheinputtensor.Ontheotherhand previousworksassumespeciﬁcsettingssuchassparsetensors[24 8 16] ortensorshavingentrieswithsimilarmagnitude[27].Suchassumptionsareunrealistic andinpractice wemayhavebothdenseandspikytensors forexample unorderedwordtrigramsinnaturallanguageprocessing.Weprovethatourproposedrandomizedmethodfortensordecompositiondoesnotleadtoanysigniﬁcantdegradationofaccuracy.Experimentsonsyntheticandreal-worlddatasetsshowhighlycompetitiveresults.Wedemonstratea10xto100xspeed-upoverexactmethodsfordecomposingdense high-dimensionaltensors.Fortopicmodeling weshowasigniﬁcantreductionincomputationaltimeoverexistingspectralLDAimplementationswithsmallperformanceloss.Inaddition ourproposedalgorithmoutperformscollapsedGibbssamplingwhenrunningtimeisconstrained.WealsoshowthatifaGibbssamplerisinitializedwithouroutputtopics itconvergeswithinseveraliterationsandoutperformsarandomlyinitializedGibbssamplerrunformuchmoreiterations.Sinceourproposedmethodisefﬁcientandavoidslocaloptima itcanbeusedtoacceleratetheslowburn-inphaseinGibbssampling.RelatedWorks:Therehavebeenmanyworksondeployingefﬁcienttensordecompositionmeth-ods[27 24 8 16 14 2 29].Mostoftheseworksexcept[27 2]implementthealternatingleastsquares(ALS)algorithm[12 5].However thisisextremelyexpensivesincetheALSmethodisrunintheinputspace whichrequiresO(n3)operationstoexecuteoneleastsquaressteponann-dimensional(dense)tensor.Thus theyareonlysuitedforextremelysparsetensors.AnalternativemethodistoﬁrstreducethedimensionoftheinputtensorthroughproceduressuchaswhiteningtoO(k)dimension wherekisthetensorrank andthencarryoutALSinthedimension-reducedspaceonk×k×ktensor[13].Thisresultsinsigniﬁcantreductionofcomputationalcomplexitywhentherankissmall(k(cid:28)n).Nonetheless inpractice suchcomplexityisstillprohibitivelyhighaskcouldbeseveralthousandsinmanysettings.Tomakemattersevenworse whenthetensorcorrespondstoempiricalmomentscomputedfromsamples suchasinspectrallearningoflatentvariablemodels itisactuallymuchslowertoconstructthereduceddimension2Table1:Summaryofnotations.SeealsoAppendixF.VariablesOperatorMeaningVariablesOperatorMeaninga b∈Cna◦b∈CnElement-wiseproducta∈Cna⊗3∈Cn×n×na⊗a⊗aa b∈Cna∗b∈CnConvolutionA B∈Cn×mA(cid:12)B∈Cn2×mKhatri-Raoproducta b∈Cna⊗b∈Cn×nTensorproductT∈Cn×n×nT(1)∈Cn×n2Modeexpansionk×k×ktensorfromtrainingdatathantodecomposeit sincethenumberoftrainingsamplesistypicallyverylarge.Anotheralternativeistocarryoutonlinetensordecomposition asopposedtobatchoperationsintheaboveworks.Suchmethodsareextremelyfast[14] butcansufferfromhighvariance.Thesketchingideasdevelopedinthispaperwillimproveourabilitytohandlelargersizesofmini-batchesandthereforeresultinreducedvarianceinonlinetensormethods.Anotheralternativemethodistoconsiderarandomizedsamplingoftheinputtensorineachiterationoftensordecomposition[27 2].However suchmethodscanbeexpensiveduetoI/Ocallsandaresensitivetothesamplingdistribution.Inparticular [27]employsuniformsampling whichisincapableofhandlingtensorswithspikyelements.Thoughnon-uniformsamplingisadoptedin[2] itrequiresanadditionalpassoverthetrainingdatatocomputethesamplingdistribution.Incontrast oursketchbasedmethodtakesonlyonepassofthedata.2PreliminariesTensor tensorproductandtensordecompositionA3rdordertensor1Tofdimensionnhasn3entries.EachentrycanberepresentedasTijkfori j k∈{1 ··· n}.Forann×n×ntensorTandavectoru∈Rn wedeﬁnetwoformsoftensorproducts(contractions)asfollows:T(u u u)=nXi j k=1Ti j kuiujuk;T(I u u)=nXj k=1T1 j kujuk ··· nXj k=1Tn j kujuk.NotethatT(u u u)∈RandT(I u u)∈Rn.FortwocomplextensorsA Bofthesameorderanddimension itsinnerproductisdeﬁnedashA Bi:=PlAlBl wherelrangesoveralltuplesthatindexthetensors.TheFrobeniusnormofatensorissimplykAkF=phA Ai.Therank-kCPdecompositionofa3rd-ordern-dimensionaltensorT∈Rn×n×nin-volvesscalars{λi}ki=1andn-dimensionalvectors{ai bi ci}ki=1suchthattheresidualkT−Pki=1λiai⊗bi⊗cik2Fisminimized.HereR=a⊗b⊗cisa3rdordertensordeﬁnedasRijk=aibjck.AdditionalnotationsaredeﬁnedinTable1andAppendixF.RobusttensorpowermethodThemethodwasproposedin[1]andwasshowntoprovablysuc-ceediftheinputtensorisanoisyperturbationofthesumofkrank-1tensorswhosebasevectorsareorthogonal.FixaninputtensorT∈Rn×n×n ThebasicideaistorandomlygenerateLinitialvectorsandperformTpowerupdatesteps:ˆu=T(I u u)/kT(I u u)k2.ThevectorthatresultsinthelargesteigenvalueT(u u u)isthenkeptandsubsequenteigenvectorscanbeobtainedviadeﬂation.Ifimplementednaively thealgorithmtakesO(kn3LT)timetorun2 requiringO(n3)storage.Inaddition incertaincaseswhenasecond-ordermomentmatrixisavailable thetensorpowermethodcanbecarriedoutonak×k×kwhitenedtensor[1] thusimprovingthetimecom-plexitybyavoidingdependenceontheambientdimensionn.Apartfromthetensorpowermethod otheralgorithmssuchasAlternatingLeastSquares(ALS [12 5])andStochasticGradientDescent(SGD [14])havealsobeenappliedtotensorCPdecomposition.TensorsketchTensorsketchwasproposedin[23]asageneralizationofcountsketch[7].ForatensorTofdimensionn1×···×np randomhashfunctionsh1 ··· hp:[n]→[b]withPrhj[hj(i)=t]=1/bforeveryi∈[n] j∈[p] t∈[b]andbinaryRademachervariablesξ1 ··· ξp:[n]→{±1} thesketchsT:[b]→RoftensorTisdeﬁnedassT(t)=XH(i1 ··· ip)=tξ1(i1)···ξp(ip)Ti1 ··· ip (1)1Thoughwemainlyfocuson3rdordertensorsinthiswork extensiontohigherordertensorsiseasy.2LisusuallysettobealinearfunctionofkandTislogarithmicinn;seeTheorem5.1in[1].3whereH(i1 ··· ip)=(h1(i1)+···+hp(ip))modb.ThecorrespondingrecoveryruleisbTi1 ··· ip=ξ1(i1)···ξp(ip)sT(H(i1 ··· ip)).Foraccuraterecovery Hneedstobe2-wisein-dependent whichisachievedbyindependentlyselectingh1 ··· hpfroma2-wiseindependenthashfamily[26].Finally theestimationcanbemademorerobustbythestandardapproachoftakingBindependentsketchesofthesametensorandthenreportthemedianoftheBestimates[7].3FasttensordecompositionviasketchingInthissectionweﬁrstintroduceanefﬁcientprocedureforcomputingsketchesoffactoredorempir-icalmomenttensors whichappearinawidevarietyofapplicationssuchasparameterestimationoflatentvariablemodels.Wethenshowhowtoruntensorpowermethoddirectlyonthesketchwithreducedcomputationalcomplexity.Inaddition whenaninputtensorissymmetric(i.e. Tijkthesameforallpermutationsofi j k)weproposeanovel“collidinghash”design whichspeedsupthesketchbuildingprocess.Duetospacelimitsweonlyconsidertherobusttensorpowermethodinthemaintext.MethodsandexperimentsforsketchingbasedALSarepresentedinAppendixC.Toavoidconfusions weemphasizethatnisusedtodenotethedimensionofthetensortobedecom-posed whichisnotnecessarilythesameasthedimensionoftheoriginaldatatensor.Indeed oncewhiteningisappliedncouldbeassmallastheintrinsicdimensionkoftheoriginaldatatensor.3.1EfﬁcientsketchingofempiricalmomenttensorsSketchinga3rd-orderdensen-dimensionaltensorviaEq.(1)takesO(n3)operations whichingeneralcannotbeimprovedbecausetheinputsizeisΩ(n3).However inpracticedatatensorsareusuallystructured.Onenotableexampleisempiricalmomenttensors whicharisesnaturallyinparameterestimationproblemsoflatentvariablemodels.Morespeciﬁcally anempiricalmomenttensorcanbeexpressedasT=ˆE[x⊗3]=1NPNi=1x⊗3i whereNisthetotalnumberoftrainingdatapointsandxiistheithdatapoint.Inthissectionweshowthatcomputingsketchesofsuchtensorscanbemadesigniﬁcantlymoreefﬁcientthanthebrute-forceimplementationsviaEq.(1).Themainideaistosketchlow-rankcomponentsofTefﬁcientlyviaFFT atrickinspiredbypreviouseffortsonsketchingbasedmatrixmultiplicationandkernellearning[22 23].WeconsiderthemoregeneralizedcasewhenaninputtensorTcanbewrittenasaweightedsumofknownrank-1components:T=PNi=1aiui⊗vi⊗wi whereaiarescalarsandui vi wiareknownn-dimensionalvectors.Thekeyobservationisthatthesketchofeachrank-1componentTi=ui⊗vi⊗wicanbeefﬁcientlycomputedbyFFT.Inparticular sTicanbecomputedassTi=s1 ui∗s2 vi∗s3 wi=F−1(F(s1 ui)◦F(s2 vi)◦F(s3 wi)) (2)where∗denotesconvolutionand◦standsforelement-wisevectorproduct.s1 u(t)=Ph1(i)=tξ1(i)uiisthecountsketchofuands2 v s3 waredeﬁnedsimilarly.FandF−1de-notetheFastFourierTransform(FFT)anditsinverseoperator.ByapplyingFFT wereducetheconvolutioncomputationintoelement-wiseproductevaluationintheFourierspace.Therefore sTcanbecomputedusingO(n+blogb)operations wheretheO(blogb)termarisesfromFFTevalua-tions.Finally becausethesketchingoperatorislinear(i.e. s(PiaiTi)=Piais(Ti)) sTcanbecomputedinO(N(n+blogb)) whichismuchcheaperthanbrute-forcethattakesO(Nn3)time.3.2FastrobusttensorpowermethodWearenowreadytopresentthefastrobusttensorpowermethod themainalgorithmofthispaper.Thecomputationalbottleneckoftheoriginalrobusttensorpowermethodisthecomputationoftwotensorproducts:T(I u u)andT(u u u).AnaiveimplementationrequiresO(n3)operations.Inthissection weshowhowtospeedupcomputationoftheseproducts.WeshowthatgiventhesketchofaninputtensorT onecanapproximatelycomputebothT(I u u)andT(u u u)inO(blogb+n)steps wherebisthehashlength.Beforegoingintodetails weexplainthekeyideabehindourfasttensorproductcomputation.ForanytwotensorsA B itsinnerproducthA Bicanbeapproximatedby4hA Bi≈hsA sBi.(3)3<(·)denotestherealpartofacomplexnumber.med(·)denotesthemedian.4AllapproximationswillbetheoreticallyjustiﬁedinSection4andAppendixE.2.4Algorithm1Fastrobusttensorpowermethod1:Input:noisysymmetrictensor¯T=T+E∈Rn×n×n;targetrankk;numberofinitializationsL numberofiterationsT hashlengthb numberofindependentsketchesB.2:Initialization:h(m)j ξ(m)jforj∈{1 2 3}andm∈[B];computesketchess(m)¯T∈Cb.3:forτ=1toLdo4:Drawu(τ)0uniformlyatrandomfromunitsphere.5:fort=1toTdo6:Foreachm∈[B] j∈{2 3}computethesketchofu(τ)t−1usingh(m)j ξ(m)jviaEq.(1).7:Computev(m)≈¯T(I u(τ)t−1 u(τ)t−1)asfollows:ﬁrstevaluate¯s(m)=F−1(F(s(m)¯T)◦F(s(m)2 u)◦F(s(m)3 u)).Set[v(m)]ias[v(m)]i←ξ1(i)[¯s(m)]h1(i)foreveryi∈[n].8:Set¯vi←med(<(v(1)i) ··· <(v(B)i))3.Update:u(τ)t=¯v/k¯vk.9:SelectionComputeλ(m)τ≈¯T(u(τ)T u(τ)T u(τ)T)usings(m)¯Tforτ∈[L]andm∈[B].Evaluateλτ=med(λ(1)τ ··· λ(B)τ)andτ∗=argmaxτλτ.Setˆλ=λτ∗andˆu=u(τ∗)T.10:DeﬂationForeachm∈[B]computesketch˜s(m)∆Tfortherank-1tensor∆T=ˆλˆu⊗3.11:Output:theeigenvalue/eigenvectorpair(ˆλ ˆu)andsketchesofthedeﬂatedtensor¯T−∆T.Table2:Computationalcomplexityofsketchedandplaintensorpowermethod.nisthetensordimension;kistheintrinsictensorrank;bisthesketchlength.Per-sketchtimecomplexityisshown.PLAINSKETCHPLAIN+WHITENINGSKETCH+WHITENINGpreprocessing:generaltensors-O(n3)O(kn3)O(n3)preprocessing:factoredtensorsO(Nn3)O(N(n+blogb))O(N(nk+k3))O(N(nk+blogb))withNcomponentspertensorcontractiontimeO(n3)O(n+blogb)O(k3)O(k+blogb)Eq.(3)immediatelyresultsinafastapproximationprocedureofT(u u u)becauseT(u u u)=hT XiwhereX=u⊗u⊗uisarankonetensor whosesketchcanbebuiltinO(n+blogb)timebyEq.(2).Consequently theproductcanbeapproximatelycomputedusingO(n+blogb)operationsifthetensorsketchofTisavailable.FortensorproductoftheformT(I u u).TheithcoordinateintheresultcanbeexpressedashT YiiwhereYi=ei⊗u⊗u;ei=(0 ··· 0 1 0 ··· 0)istheithindicatorvector.WecanthenapplyEq.(3)toapproximatelycomputehT Yiiefﬁciently.However thismethodisnotcompletelysatisfactorybecauseitrequiressketchingnrank-1tensors(Y1throughYn) whichresultsinO(n)FFTevaluationsbyEq.(2).BelowwepresentapropositionthatallowsustouseonlyO(1)FFTstoapproximateT(I u u).Proposition1.hsT s1 ei∗s2 u∗s3 ui=hF−1(F(sT)◦F(s2 u)◦F(s3 u)) s1 eii.Proposition1isprovedinAppendixE.1.Themainideaisto“shift”alltermsnotdependingonitotheleftsideoftheinnerproductandeliminatetheinverseFFToperationontherightsidesothatseicontainsonlyonenonzeroentry.Asaresult wecancomputeF−1(F(sT)◦F(s2 u)◦F(s3 u))onceandreadoffeachentryofT(I u u)inconstanttime.Inaddition thetechniquecanbefurtherextendedtosymmetrictensorsketches withdetailsdeferredtoAppendixBduetospacelimits.Whenoperatingonann-dimensionaltensor ThealgorithmrequiresO(kLT(n+Bblogb))runningtime(excludingthetimeforbuilding˜s¯T)andO(Bb)memory whichsigniﬁcantlyimprovestheO(kn3LT)timeandO(n3)spacecomplexityoverthebruteforcetensorpowermethod.HereL Tarealgorithmparametersforrobusttensorpowermethod.PreviousanalysisshowsthatT=O(logk)andL=poly(k) wherepoly(·)issomeloworderpolynomialfunction.[1]Finally Table2summarizescomputationalcomplexityofsketchedandplaintensorpowermethod.3.3CollidinghashandsymmetrictensorsketchForsymmetricinputtensors itispossibletodesignanewstyleoftensorsketchthatcanbebuiltmoreefﬁciently.Theideaistodesignhashfunctionsthatdeliberatelycollidesymmetricentries i.e. (i j k) (j i k) etc.Consequently weonlyneedtoconsiderentriesTijkwithi≤j≤kwhenbuildingtensorsketches.AnintuitiveideaistousethesamehashfunctionandRademacherrandomvariableforeachorder thatis h1(i)=h2(i)=h3(i)=:h(i)andξ1(i)=ξ2(i)=ξ3(i)=:ξ(i).5Inthisway allpermutationsof(i j k)willcollidewitheachother.However suchadesignhasanissuewithrepeatedentriesbecauseξ(i)canonlytake±1values.Consider(i i k)and(j j k)asanexample:ξ(i)2ξ(k)=ξ(j)2ξ(k)withprobability1evenifi6=j.Ontheotherhand weneedE[ξ(a)ξ(b)]=0foranypairofdistinct3-tuplesaandb.Toaddresstheabove-mentionedissue weextendtheRademacherrandomvariablestothecomplexdomainandconsiderallrootsofzm=1 thatis Ω={ωj}m−1j=0whereωj=ei2πjm.Supposeσ(i)isaRademacherrandomvariablewithPr[σ(i)=ωi]=1/m.Byelementaryalgebra E[σ(i)p]=0whenevermisrelativeprimetopormcanbedividedbyp.Therefore bysettingm=4weavoidcollisionsofrepeatedentriesina3rdordertensor.Morespeciﬁcally ThesymmetrictensorsketchofasymmetrictensorT∈Rn×n×ncanbedeﬁnedas˜sT(t):=X˜H(i j k)=tTi j kσ(i)σ(j)σ(k) (4)where˜H(i j k)=(h(i)+h(j)+h(k))modb.Torecoveranentry weusebTi j k=1/κ·σ(i)·σ(j)·σ(k)·˜sT(H(i j k)) (5)whereκ=1ifi=j=k;κ=3ifi=jorj=kori=k;κ=6otherwise.Forhigherordertensors thecoefﬁcientscanbecomputedviatheYoungtableauxwhichcharacterizessymmetriesunderthepermutationgroup.Comparedtoasymmetrictensorsketches thehashfunctionhneedstosatisfystrongerindependenceconditionsbecauseweareusingthesamehashfunctionforeachorder.Inourcase hneedstobe6-wiseindependenttomake˜H2-wiseindependent.Thefactisduetothefollowingproposition whichisprovedinAppendixE.1.Proposition2.Fixpandq.Forh:[n]→[b]deﬁnesymmetricmapping˜H:[n]p→[b]as˜H(i1 ··· ip)=h(i1)+···+h(ip).Ifhis(pq)-wiseindependentthenHisq-wiseindependent.Thesymmetrictensorsketchdescribedabovecansigniﬁcantlyspeedupsketchbuildingprocesses.ForageneraltensorwithMnonzeroentries tobuild˜sToneonlyneedstoconsiderroughlyM/6entries(thoseTijk6=0withi≤j≤k).Forarank-1tensoru⊗3 onlyoneFFTisneededtobuildF(˜s);incontrast tocomputeEq.(2)oneneedsatleast3FFTevaluations.Finally inAppendixBwegivedetailsonhowtoseamlesslycombinesymmetrichashingandtech-niquesinprevioussectionstoefﬁcientlyconstructanddecomposeatensor.4ErroranalysisInthissectionweprovidetheoreticalanalysisonapproximationerrorofbothtensorsketchandthefastsketchedrobusttensorpowermethod.Wemainlyfocusonsymmetrictensorsketches whileextensiontoasymmetricsettingsistrivial.Duetospacelimits allproofsareplacedintheappendix.4.1TensorsketchconcentrationboundsTheorem1boundstheapproximationerrorofsymmetrictensorsketcheswhencomputingT(u u u)andT(I u u).ItsproofisdeferredtoAppendixE.2.Theorem1.FixasymmetricrealtensorT∈Rn×n×nandarealvectoru∈Rnwithkuk2=1.Supposeε1 T(u)∈Randε2 T(u)∈RnareestimationerrorsofT(u u u)andT(I u u)usingBindependentsymmetrictensorsketches;thatis ε1 T(u)=bT(u u u)−T(u u u)andε2 T(u)=bT(I u u)−T(I u u).IfB=Ω(log(1/δ))thenwithprobability≥1−δthefollowingerrorboundshold:(cid:12)(cid:12)ε1 T(u)(cid:12)(cid:12)=O(kTkF/√b);(cid:12)(cid:12)[ε2 T(u)]i(cid:12)(cid:12)=O(kTkF/√b) ∀i∈{1 ··· n}.(6)Inaddition foranyﬁxedw∈Rn kwk2=1withprobability≥1−δwehavehw ε2 T(u)i2=O(kTk2F/b).(7)4.2AnalysisofthefasttensorpowermethodWepresentatheoremanalyzingrobusttensorpowermethodwithtensorsketchapproximations.AmoredetailedtheoremstatementalongwithitsproofcanbefoundinAppendixE.3.Theorem2.Suppose¯T=T+E∈Rn×n×nwhereT=Pki=1λiv⊗3iwithanorthonor-malbasis{vi}ki=1 λ1>···>λk>0andkEk=.Let{(ˆλi ˆvi)}ki=1betheeigen-6Table3:Squaredresidualnormontop10recoveredeigenvectorsof1000dtensorsandrunningtime(excludingI/Oandsketchbuildingtime)forplain(exact)andsketchedrobusttensorpowermethods.Twovectorsareconsideredmismatch(wrong)ifkv−ˆvk22>0.1.AextendedversionisshownasTable5inAppendixA.ResidualnormNo.ofwrongvectorsRunningtime(min.)log2(b):121314151612131415161213141516σ=.01B=20.40.19.10.09.0886300.851.63.57.416.6B=30.26.10.09.08.07752001.32.45.311.324.6B=40.17.10.08.08.07740001.83.37.315.233.0Exact.070293.5Table4:Negativelog-likelihoodandrunningtime(min)onthelargeWikipediadatasetfor200and300topics.klike.timelog2bitersklike.timelog2biters200Spectral7.493412-3007.395613-Gibbs6.85561-306.38818-30Hybrid6.771441256.313521310value/eigenvectorpairsobtainedbyAlgorithm1.Suppose=O(1/(λ1n)) T=Ω(log(n/δ)+log(1/)maxiλi/(λi−λi−1))andLgrowslinearlywithk.Assumetherandomnessofthetensorsketchisindependentamongtensorproductevaluations.IfB=Ω(log(n/δ))andbsatisﬁesb=Ω(cid:18)max(cid:26)−2kTk2F∆(λ)2 δ−4n2kTk2Fr(λ)2λ21(cid:27)(cid:19)(8)where∆(λ)=mini(λi−λi−1)andr(λ)=maxi j>i(λi/λj) thenwithprobability≥1−δthereexistsapermutationπover[k]suchthatkvπ(i)−ˆvik2≤ |λπ(i)−ˆλi|≤λi/2 ∀i∈{1 ··· k}(9)andkT−Pki=1ˆλiˆv⊗3ik≤cforsomeconstantc.Theorem1showsthatthesketchlengthbcanbesetaso(n3)toprovablyapproximatelydecomposea3rd-ordertensorwithdimensionn.Theorem1togetherwithtimecomplexitycomparisoninTable2showsthatthesketchingbasedfasttensordecompositionalgorithmhasbettercomputationalcom-plexityoverbrute-forceimplementation.Onepotentialdrawbackofouranalysisistheassumptionthatsketchesareindependentlybuiltforeachtensorproduct(contraction)evaluation.Thisisanar-tifactofouranalysisandweconjecturethatitcanberemovedbyincorporatingrecentdevelopmentofdifferentiallyprivateadaptivequeryframework[9].5ExperimentsWedemonstratetheeffectivenessandefﬁciencyofourproposedsketchbasedtensorpowermethodonbothsynthetictensorsandreal-worldtopicmodelingproblems.ExperimentalresultsinvolvingthefastALSmethodarepresentedinAppendixC.3.AllmethodsareimplementedinC++andtestedonasinglemachinewith8IntelX5550@2.67GhzCPUsand32GBmemory.Forsynthetictensordecompositionweuseonlyasinglethread;forfastspectralLDA8to16threadsareused.5.1SynthetictensorsInTable5wecompareourproposedalgorithmswithexactdecompositionmethodsonsynthetictensors.Letn=1000bethedimensionoftheinputtensor.Weﬁrstgeneratearandomorthonormalbasis{vi}ni=1andthensettheinputtensorTasT=normalize(Pni=1λiv⊗3i)+E wheretheeigenvaluesλisatisfyλi=1/i.ThenormalizationstepmakeskTk2F=1beforeimposingnoise.TheGaussiannoisematrixEissymmetricwithEijk∼N(0 σ/n1.5)fori≤j≤kandnoise-to-signallevelσ.Duetotimeconstraints weonlycomparetherecoveryerrorandrunningtimeonthetop10recoveredeigenvectorsofthefull-rankinputtensorT.BothLandTaresetto30.Table3showsthatourproposedalgorithmsachievereasonableapproximationerrorwithinafewminutes whichismuchfasterthenexactmethods.Acompleteversion(Table5)isdeferredtoAppendixA.5.2TopicmodelingWeimplementafastspectralinferencealgorithmforLatentDirichletAllocation(LDA[3])bycom-biningtensorsketchingwithexistingwhiteningtechniquefordimensionalityreduction.Implemen-79101112131415167.888.28.4Log hash lengthNegative Log−likelihood k=50k=100k=200Exact  k=50Exact  k=100Exact  k=200Gibbs sampling  100 iterations  145 minsFigure1:Left:negativelog-likelihoodforfastandexacttensorpowermethodonWikipediadataset.Right:negativelog-likelihoodforcollapsedGibbssampling fastLDAandGibbssamplingusingFastLDAasinitial-ization.tationdetailsareprovidedinAppendixD.WecompareourproposedfastspectralLDAalgorithmwithbaselinespectralmethodsandcollapsedGibbssampling(usingGibbsLDA++[25]implemen-tation)ontworeal-worlddatasets:WikipediaandEnron.DatasetdetailsarepresentedinAOnlythemostfrequentVwordsarekeptandthevocabularysizeVissetto10000.FortherobusttensorpowermethodtheparametersaresettoL=50andT=30.ForALSweiterateuntilconvergence oramaximumnumberof1000iterationsisreached.α0issetto1.0andBissetto30.ObtainedtopicmodelsΦ∈RV×Kareevaluatedonaheld-outdatasetconsistingof1000documentsrandomlypickedoutfromtrainingdatasets.Foreachtestingdocumentd weﬁtatopicmixingvectorˆπd∈RKbysolvingthefollowingoptimizationproblem:ˆπd=argminkπk1=1 π≥0kwd−Φπk2 wherewdistheempiricalworddistributionofdocumentd.Theper-documentlog-likelihoodisthendeﬁnedasLd=1ndPndi=1lnp(wdi) wherep(wdi)=PKk=1ˆπkΦwdi k.Finally theaverageLdoveralltestingdocumentsisreported.Figure1leftshowstheheld-outnegativelog-likelihoodforfastspectralLDAunderdifferenthashlengthsb.Wecanseethatasbincreases theperformanceapproachestheexacttensorpowermethodbecausesketchingapproximationbecomesmoreaccurate.Ontheotherhand Table6showsthatfastspectralLDArunsmuchfasterthanexacttensordecompositionmethodswhileachievingcom-parableperformanceonbothdatasets.Figure1rightcomparestheconvergenceofcollapsedGibbssamplingwithdifferentnumberofiterationsandfastspectralLDAwithdifferenthashlengthsonWikipediadataset.ForcollapsedGibbssampling wesetα=50/Kandβ=0.1following[11].Asshownintheﬁgure fastspectralLDAachievescomparableheld-outlikelihoodwhilerunningfasterthancollapsedGibbssampling.WefurthertakethedictionaryΦoutputbyfastspectralLDAanduseitasinitializationsforcollapsedGibbssampling(thewordtopicassignmentszareobtainedby5-iterationGibbssampling withthedictionaryΦﬁxed).TheresultingGibbssamplerconvergesmuchfaster:withonly3iterationsitalreadyperformsmuchbetterthanarandomlyinitializedGibbssamplerrunfor100iterations whichtakes10xmorerunningtime.WealsoreportperformanceoffastspectralLDAandcollapsedGibbssamplingonalargerdatasetinTable4.Thedatasetwasbuiltbycrawling1 085 768randomWikipediapagesandaheld-outevaluationsetwasbuiltbyrandomlypickingout1000documentsfromthedataset.Numberoftopicskissetto200or300 andaftergettingtopicdictionaryΦfromfastspectralLDAweuse2-iterationGibbssamplingtoobtainwordtopicassignmentsz.Table4showsthatthehybridmethod(i.e. collapsedGibbssamplinginitializedbyspectralLDA)achievesthebestlikelihoodperformanceinamuchshortertime comparedtoarandomlyinitializedGibbssampler.6ConclusionInthisworkweproposedasketchingbasedapproachtoefﬁcientlycomputetensorCPdecom-positionwithprovableguarantees.Weapplyourproposedalgorithmonlearninglatenttopicsofunlabeleddocumentcollectionsandachievesigniﬁcantspeed-upcomparedtovanillaspectralandcollapsedGibbssamplingmethods.Someinterestingfuturedirectionsincludefurtherimprovingthesamplecomplexityanalysisandapplyingtheframeworktoabroaderclassofgraphicalmodels.Acknowledgement:AnimaAnandkumarissupportedinpartbytheMicrosoftFacultyFellowshipandtheSloanFoundation.AlexSmolaissupportedinpartbyaGoogleFacultyResearchGrant.8References[1]A.Anandkumar R.Ge D.Hsu S.Kakade andM.Telgarsky.Tensordecompositionsforlearninglatentvariablemodels.JournalofMachineLearningResearch 15:2773–2832 2014.[2]S.BhojanapalliandS.Sanghavi.Anewsamplingtechniquefortensors.arXiv:1502.05023 2015.[3]D.M.Blei A.Y.Ng andM.I.Jordan.Latentdirichletallocation.JournalofmachineLearningresearch 3:993–1022 2003.[4]A.Carlson J.Betteridge B.Kisiel B.Settles E.R.HruschkaJr andT.M.Mitchell.Towardanarchi-tecturefornever-endinglanguagelearning.InAAAI 2010.[5]J.D.CarrollandJ.-J.Chang.Analysisofindividualdifferencesinmultidimensionalscalingviaann-waygeneralizationof“eckart-youngdecomposition.Psychometrika 35(3):283–319 1970.[6]A.ChagantyandP.Liang.Estimatinglatent-variablegraphicalmodelsusingmomentsandlikelihoods.InICML 2014.[7]M.Charikar K.Chen andM.Farach-Colton.Findingfrequentitemsindatastreams.TheoreticalCom-puterScience 312(1):3–15 2004.[8]J.H.ChoiandS.Vishwanathan.DFacTo:Distributedfactorizationoftensors.InNIPS 2014.[9]C.Dwork V.Feldman M.Hardt T.Pitassi O.Reingold andA.Roth.Preservingstatisticalvalidityinadaptivedataanalysis.InSTOC 2015.[10]A.S.FieldandD.Graupe.Topographiccomponent(parallelfactor)analysisofmultichannelevokedpotentials:practicalissuesintrilinearspatiotemporaldecomposition.BrainTopography 3(4):407–423 1991.[11]T.L.GrifﬁthsandM.Steyvers.Findingscientiﬁctopics.ProceedingsoftheNationalAcademyofSciences 101(suppl1):5228–5235 2004.[12]R.A.Harshman.FoundationsofthePARAFACprocedure:Modelsandconditionsforanexplanatorymulti-modalfactoranalysis.UCLAWorkingPapersinPhonetics 16:1–84 1970.[13]F.Huang S.Matusevych A.Anandkumar N.Karampatziakis andP.Mineiro.Distributedlatentdirichletallocationviatensorfactorization.InNIPSOptimizationWorkshop 2014.[14]F.Huang U.N.Niranjan M.U.Hakeem andA.Anandkumar.Fastdetectionofoverlappingcommunitiesviaonlinetensormethods.arXiv:1309.0787 2013.[15]A.Jain.Fundamentalsofdigitalimageprocessing 1989.[16]U.Kang E.Papalexakis A.Harpale andC.Faloutsos.Gigatensor:Scalingtensoranalysisupby100times-algorithmsanddiscoveries.InKDD 2012.[17]B.KlimtandY.Yang.Introducingtheenroncorpus.InCEAS 2004.[18]T.KoldaandB.Bader.Thetophitsmodelforhigher-orderweblinkanalysis.InWorkshoponlinkanalysis counterterrorismandsecurity 2006.[19]T.KoldaandB.Bader.Tensordecompositionsandapplications.SIAMReview 51(3):455–500 2009.[20]T.G.KoldaandJ.Sun.Scalabletensordecompositionsformulti-aspectdatamining.InICDM 2008.[21]M.Mørup L.K.Hansen C.S.Herrmann J.Parnas andS.M.Arnfred.Parallelfactoranalysisasanexploratorytoolforwavelettransformedevent-relatedeeg.NeuroImage 29(3):938–947 2006.[22]R.Pagh.Compressedmatrixmultiplication.InITCS 2012.[23]N.PhamandR.Pagh.Fastandscalablepolynomialkernelsviaexplicitfeaturemaps.InKDD 2013.[24]A.-H.Phan P.Tichavsky andA.Cichocki.FastalternatingLSalgorithmsforhighorderCANDE-COMP/PARAFACtensorfactorizations.IEEETransactionsonSignalProcessing 61(19):4834–4846 2013.[25]X.-H.PhanandC.-T.Nguyen.GibbsLDA++:AC/C++implementationoflatentdirichletallocation(lda) 2007.[26]M.Ptras¸cuandM.Thorup.Thepowerofsimpletabulationhashing.JournaloftheACM 59(3):14 2012.[27]C.Tsourakakis.MACH:Fastrandomizedtensordecompositions.InSDM 2010.[28]H.-Y.TungandA.Smola.Spectralmethodsforindianbuffetprocessinference.InNIPS 2014.[29]C.Wang X.Liu Y.Song andJ.Han.Scalablemoment-basedinferenceforlatentdirichletallocation.InECML/PKDD 2014.[30]Y.WangandJ.Zhu.Spectralmethodsforsupervisedtopicmodels.InNIPS 2014.9,Kohei Hayashi
Ryohei Fujimaki
Yining Wang
Hsiao-Yu Tung
Alexander Smola
Anima Anandkumar