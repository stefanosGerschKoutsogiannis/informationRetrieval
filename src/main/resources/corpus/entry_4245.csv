2019,Partitioning Structure Learning for Segmented Linear Regression Trees,This paper proposes a partitioning structure learning method for segmented linear regression trees (SLRT)  which assigns linear predictors over the terminal nodes. The recursive partitioning process is driven by an adaptive split selection algorithm that maximizes  at each node  a criterion function based on a conditional Kendall’s τ statistic that measures the rank dependence between the regressors and the fit- ted linear residuals. Theoretical analysis shows that the split selection algorithm permits consistent identification and estimation of the unknown segments. A suffi- ciently large tree is induced by applying the split selection algorithm recursively. Then the minimal cost-complexity tree pruning procedure is applied to attain the right-sized tree  that ensures (i) the nested structure of pruned subtrees and (ii) consistent estimation to the number of segments. Implanting the SLRT as the built-in base predictor  we obtain the ensemble predictors by random forests (RF) and the proposed weighted random forests (WRF). The practical performance of the SLRT and its ensemble versions are evaluated via numerical simulations and empirical studies. The latter shows their advantageous predictive performance over a set of state-of-the-art tree-based models on well-studied public datasets.,Partitioning Structure Learning for Segmented

Linear Regression Trees

Xiangyu Zheng
Peking University

zhengxiangyu@pku.edu.cn

Abstract

Song Xi Chen

Peking University

csx@gsm.pku.edu.cn

This paper proposes a partitioning structure learning method for segmented linear
regression trees (SLRT)  which assigns linear predictors over the terminal nodes.
The recursive partitioning process is driven by an adaptive split selection algorithm
that maximizes  at each node  a criterion function based on a conditional Kendall’s
τ statistic that measures the rank dependence between the regressors and the ﬁt-
ted linear residuals. Theoretical analysis shows that the split selection algorithm
permits consistent identiﬁcation and estimation of the unknown segments. A sufﬁ-
ciently large tree is induced by applying the split selection algorithm recursively.
Then the minimal cost-complexity tree pruning procedure is applied to attain the
right-sized tree  that ensures (i) the nested structure of pruned subtrees and (ii)
consistent estimation to the number of segments. Implanting the SLRT as a built-in
base predictor  we obtain the ensemble predictors by random forests (RF) and the
proposed weighted random forests (WRF). The practical performance of the SLRT
and its ensemble versions are evaluated via numerical simulations and empirical
studies. The latter shows their advantageous predictive performance over a set of
state-of-the-art tree-based models on well-studied public datasets.

1

Introduction

Data partitioning is a fundamental pre-processing method that explores the partitioning structure of
the feature space such that the subspaces are more compliant to a simple model [1]. We consider the
segmented linear regression (SLR) models  which prescribes linear predictors over the partitions.
Partitioning structure learning is the core of SLR  that selects the split variables and levels as well as
determines the number of segments.
SLR has been studied in statistics and econometrics [2  3  4  5]  but the existing methods tend to
assume the split variable is known and univariate  with segments estimated by a costly simultaneous
optimization. We propose a tree-based approach for SLR called segmented linear regression trees
(SLRT)  that does not require the pre-speciﬁed information about the split variables. SLRT is
completely data-driven and facilitates more efﬁcient computation via recursive partitioning  which is
fundamentally based on a split selection algorithm and a tree pruning algorithm.
Split Selection Algorithm At each internal node of the tree  the optimal split variable and level pair
is selected to partition the feature space into two halves. Let ˆe be the ﬁtted residuals by the ordinary
least square regression. Any non-linearity in the underlying regression function is reﬂected in the
dependence between ˆe and the regressors. Based on the conditional Kendall’s τ rank correlation [6] 
we propose the following criterion function at a candidate split variable index j and a split level a 
k=1 {|ˆτ (Xk  ˆe|Xj ≤ a)| + |ˆτ (Xk  ˆe|Xj > a)|}  where ˆτ is the sample version of the
Kendall’s τ  X is a p-dimensional regressors vector with Xk being its k-th component. The optimal
split is selected by maximizing C(j  a) over the candidate split variables {Xj} and levels {a} in the

C(j  a) =(cid:80)p

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

observed sample of Xj. Theoretical analysis shows that it leads to the consistent identiﬁcation and
estimation of the most prevailing split variable and level that attains the maximum of C(j  a).
Tree Pruning Algorithm We deﬁne an adaptive cost-complexity measure that combines the accuracy
of the linear regression ﬁt at each node with a penalty for a large tree size. The optimally pruned tree
is selected from a nested sequence of pruned subtrees by minimizing the cost-complexity measure.
Theoretical analysis shows that the pruning method leads to consistent estimation of the underlying
number of segments  which promotes a parsimonious partitioning structure.
Leaf Modeling and Ensemble Methods For predictors within segments  we employ the LASSO
procedure [7] to select the most inﬂuential variables and estimate the linear parameters. Furthermore 
by implanting SLRT as the base predictor in the random forests (RF) formulation  we obtain the
ensemble predictor that improves the model stability and predictive accuracy. A weighted version of
the RF (WRF) is also proposed  which shows an improved performance over the RF by reducing the
importance of those under-performing trees in weighting.
As a novel tree-based learning method for segmented linear models  SLRT possesses attractive
theoretical properties of the consistent identiﬁcation and estimation of the partitioning structures 
which are conﬁrmed favorably in numerical simulations. Applied on nine benchmark datasets  SLRT
had advantageous predictive performance over several state-of-the-art tree-based methods  with
further improvement offered by the RF and WRF with SLRT as the base predictor.
The source code of the algorithm is available in the supplementary material.

1.1 Related Work

The proposed segmented linear regression tree is a tree-based approach to segmented linear regression
(SLR) models  where the partitions of the feature space is axis-aligned. The existing methods of SLR
tend to assume a known split variable  such that the partitioning structure learning is reduced to the
change-points detection with respect to a given variable. For instance  [2  3] considered the case
where both the univariate partitioning variable and the number of segments are pre-speciﬁed. [4  5]
estimated the number of change-points by minimizing the Bayesian information criteria (BIC). [8]
selected the change-points via the sum of squared residuals in conjunction with the permutation test 
which also assumed a known split variable. Our approach does not require pre-speciﬁed information
of the segments  and learns the partitioning structure via a tree induction process.
SLR also belongs to the class of region-speciﬁc linear models. [1] proposed a partition-wise linear
model  where axis-aligned partitions are pre-speciﬁed and an 0-1 activeness function was assigned to
each region. With each region-speciﬁc linear model being estimated ﬁrst  the activeness functions are
optimized through a global convex loss function. [9] proposed a local supervised learning through
space partitioning for classiﬁcation which allows arbitrary partitions and considered linear classiﬁers 
while [10] employed a Bayesian updating process to partition the feature space to rectangular
bounding boxes and assigned a constant estimation over each partition like CART.
Our approach is closely related to the regression tree (regression part of CART  [11])  a well-known
region-speciﬁc approach that used a constant-valued predictor within each terminal node. There
have been tree-based algorithms which assigns linear predictors in terminal nodes  which tend to
be heuristic without theoretical analysis. One group of the methods [12  13  14] adopted splitting
algorithms similar to that of CART  which tend to ignore the correspondence between the evaluation
criteria for splits and the models in terminal nodes. Another group [15  16  17  18  19] employed
heuristic criteria designed to make the subsets more compliant for linear models in one step  without
considering the properties of the estimated boundaries. Our split selection algorithm is closely related
to GUIDE [17  18] as both utilize the estimated residuals ˆe at a node level. However  GUIDE used
the signs of ˆe that would be less informative than using ˆe via the Kenall’s τ. Another difference is
that GUIDE considered the marginal association between signs of ˆe and the regressors instead of
conditioning on a split variable and level  which can lead to the mis-identiﬁcation of the split variable.

2 Segmented Linear Regression Models

This section presents the framework of SLRT  and provides the motivation and the theoretical
properties to the computational algorithms in Section 3.

2

2.1 Framework

Consider the relationship between a univariate response Y and a multivariate explanatory covariate
X = (X1 ···   Xp)T . Assume that the mean regression function m(X) = E(Y |X) is partition-wise
linear over L0 unknown partitions {Dl}L0

L0(cid:88)

l=1 in the domain of X so that
(αl + X(cid:48)βl) 1(X ∈ Dl) + ε 

Y =

(1)

l=1

where (αl  βl) are regression coefﬁcients over domain Dl and ε is the random error satisfying
In this paper  we consider the case of axis-aligned partitions {Dl}  which are
E(ε|X) = 0.
determined by a collection of split levels {Xjq = aq}Q0
q=1. The model may be extended to more
general shape of {Dl} by undergoing pre-transformations  which will be a topic for a future study.
The determination of {Dl} is equivalent to selecting the split variable and level pairs  namely
q=1  where Q0 is determined by L0 and the geometry of {Dl}. The task of partitioning
S = {(jq  aq)}Q0
structure learning is to identify the underlying split variables and estimate the split levels consistently.
We adopt the computationally efﬁcient regression tree approach by applying the split variable and
level selection algorithm recursively  ending with terminal nodes for the desired partitions.

2.2 Statistical Analysis of Criterion Functions

To select the optimal split variable and level at a node  we ﬁt the least square regression over the node
and select the optimal split by studying the rank correlation between the estimated residuals and the
regressors given a candidate split variable and a split level. This is computationally more efﬁcient
than the commonly used cost minimization procedure [11  17  20]  which would require repeated
least square ﬁtting for each candidate split variable and level.
For the ease of presentation  we consider the one-time split selection over the root node t0 which
contains data Dt0 of n independent observations {X(i)  Y (i)}n
i=1 generated from Model (1). We are
to partition Dt0 into two subsets to make the data on each subset more compliant to a linear model.
To attain this  let ˆY = ˆαt0 + X(cid:48) ˆβt0 be the ﬁtted ordinary least square (OLS) regression over Dt0  and
ˆe = Y − ˆY be the estimated residuals. If the underlying regression function m(X) is nonlinear  the
non-linearity will be reﬂected in the residuals ˆe and their dependence with the potential split variables.
Indeed  if m(X) is piecewise linear  the estimated residuals ˆe is also piecewise linear in X since
I(X ∈ Dl) + ε. A regressor Xk and ˆe tend to be accordant
(discordant) for positive (negative) coefﬁcient within each partition. To capture the dependence  we
employ the Kendall’s τ coefﬁcient [6] to deﬁne the following criterion function:
{|ˆτ (Xk  ˆe|Xj ≤ a)| + |ˆτ (Xk  ˆe|Xj > a)|}  

(αl − ˆαt0) + X(cid:48)(βl − ˆβt0)

ˆe =(cid:80)L

p(cid:88)

(cid:16)

(cid:17)

(2)

l=1

C(j  a) =
for 1 ≤ j ≤ p  a ∈ {Xj(i)}n

k=1
i=1 and

(cid:80)

i<i(cid:48)

ˆτ(cid:0)Xk  ˆe(cid:12)(cid:12)Xj ≤ a(cid:1) =

is the Kendall’s τ statistic  where ItL(j  a) = {i(cid:12)(cid:12)Xj(i) ≤ a  1 ≤ i ≤ n} is the index set for the left
The ˆτ (Xk  ˆe(cid:12)(cid:12)Xj > a)  ItR (j  a) and NtR (j  a) are deﬁned analogously.

partition split by variable Xj at level a and the sample size of ItL(j  a) is NtL(j  a) = |ItL(j  a)|.

sgn ((Xk(i) − ˆe(i))(Xk(i(cid:48)) − ˆe(i(cid:48))))
NtL(j  a)(NtL (j  a) − 1)/2

i i(cid:48)∈ItL (j a)

Based on C(j  a)  we propose the split selection Algorithm 1 in Section 3.1  which is essentially
motivated by the following Lemma 2.1.
Lemma 2.1 Suppose the regressors are uncorrelated conditional on each partition of {Dl}L0
l=1. As-
sume the following technical conditions: (1) E(ε|X) = median(ε|X) = median(Xj −E(Xj)|Xk) =
0 for k (cid:54)= j; (2) for (jk  ak)∈S  0 < P(X (s)
j(cid:48) ≤ a(cid:48)) < 1 when (j(cid:48)  a(cid:48)) (cid:54)= (jk  ak). Let ¯C(j  a)
be the probability limit of C(j  a). Then  for any (j(cid:48)  a(cid:48)) /∈ S  ¯C(j(cid:48)  a(cid:48)) < ¯C(jq  aq) for any (jq  aq) ∈
S  with S being the genuine set of split variable and level pairs.

≤ ak|X (s)

(3)

jk

3

The proof of Lemma 2.1 includes two phases. We ﬁrstly investigate the simple case where L0 = 2 
and then generalize the conclusion to L0 ≥ 2 using the law of iterated expectation. Please refer to
the supplements for the details and a further discussion about the technical conditions. Intuitively
speaking  maximizing C(j  a) is to maximize the sum of rank correlations between the estimated
residuals ˆe and each element of the regressors X over each of the selected subsets ({Xj ≤ a} and
{Xj > a})  such that the rank correlation with X contained in ˆe could be further distilled by regressing
ˆe on the regressors conditional on each subset  which leads to a segmented linear regression.

Deﬁne the distance d((ˆj  ˆa) S) = minq{|(ˆj  ˆa) − (jq  aq)|(cid:12)(cid:12)(jq  aq) ∈ S}. Then  Lemma 2.1 leads to
(cid:17) → 0 as n → ∞ under

the following theorem that validates the consistency property of the selected split.
Theorem 2.1 Let (ˆj  ˆa) = argmaxC(j  a). Then  P
the assumptions of Lemma 2.1. Specially  when ¯C(j  a) has a unique maximum (j∗  a∗)  we have
(j∗  a∗) ∈ S and (ˆj  ˆa) P→ (j∗  a∗) as n → ∞.
When the regressors are not conditional uncorrelated as required by Lemma 2.1  we conduct a
linear transformation when calculating the conditional Kendall’s τ coefﬁcients with ˆe. Speciﬁcally 
let X = (X(1) ···   X(n))(cid:48) be the data matrix for n observations of X. Given a split variable and
level Xj = a  deﬁne XL = X diag{1(Xj(1) ≤ a) ···   1(Xj(n) ≤ a)} and XR = X diag{1(Xj(1) >
a) ···   1(Xj(n) > a)}. Then  there exists a non-singular matrix P (j a) such that Z(cid:48)Z is diagonal
for Z = XP (j a)  XLP (j a) and XRP (j a)  which is facilitated by the simultaneous diagonalization
of positive deﬁnite matrices (see supplements for detailed calculation procedures of P (j a) that are
based on the spectral decomposition). Let Z (¯j ¯a) = XP (¯j ¯a) be the transformed regressors with
Z (¯j ¯a)

being the k-th element. Deﬁne the modiﬁed criterion function with index (¯j  ¯a) 

d((ˆj  ˆa) S) > ε

(cid:16)

k

C(¯j ¯a)(j  a) =

Z (¯j ¯a)

k

  ˆe|Xj ≤ a

Z (¯j ¯a)

k

  ˆe|Xj > a

 

(4)

(cid:16)

(cid:110)(cid:12)(cid:12)(cid:12)ˆτ

p(cid:88)

k=1

(cid:16)

(cid:17)(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)ˆτ

(cid:17)(cid:12)(cid:12)(cid:12)(cid:111)

where Z (¯j ¯a) replaces X in (2) and ˆe is still the residuals of OLS regression at the node t0 without
transformation. The following Lemma 2.2 shows that C(¯j ¯a) possesses properties similar to that of C
introduced in Lemma 2.1  while Lemma 2.2 does not require X to be conditional uncorrelated. This
motivates the Algorithm 2 in Section 3.1 and leads to the convergence result in Theorem 2.2.
Lemma 2.2 Let ¯C(¯j ¯a)(j  a) be the probability limit of C(¯j ¯a)(j  a)  Then  under the same technical
¯C(¯j ¯a)(j  a) = (¯j  ¯a) when (¯j  ¯a) ∈ S 
conditions (1) and (2) as required in Lemma 2.1  argmax(j a)
with S = {(jq  aq)}Q0
Theorem 2.2 Let (ˆj  ˆa)(¯j  ¯a) = argmax(j a) C(¯j ¯a)(j  a). Under the technical conditions in Lemma
2.2  if (¯j  ¯a) ∈ S  then (ˆj  ˆa)(¯j  ¯a)

q=1 being the genuine set of split variable and level pairs.

P−→ (¯j  ¯a) as n → ∞.

Theorem 2.2 implies that the convergence of
S. This motivates the distance minimization procedure in Line 7 of Algorithm 2 .

(¯j ¯a)

to (¯j  ¯a) is a necessary condition for (¯j  ¯a) ∈

(cid:16)ˆj  ˆa
(cid:17)

3 Partitioning Structure Learning

We ﬁrst use the recursive partitioning procedures to generate the initial partitions. Then  we employ
the cost-complexity tree pruning procedure to obtain a parsimonious partitions structure.

3.1

Initial Partitions by Recursive Partitioning

The recursive partitioning needs a split selection algorithm at the node level  and a stopping rule for
the termination of the partitioning process  the latter is based on two tuning parameters: Nmin that
controls the sample size in any leaf node and Depmax that limits the depth of the tree.
The split selection is the core of the recursive partitioning. In the following Algorithm 1 of recursive
partitioning  the split is selected by maximizing C(j  a). This is directly motivated by Lemma 2.1 

4

that shows the maximum of ¯C(j  a) is within the genuine set of split variable and level pairs S.
Besides  according to Theorem 2.1  the selected split level Xˆj = ˆa determined by the maximum of
the criterion function C(j  a) is consistent to one of the underlying genuine partitioning boundary
provided the regressors are uncorrelated conditional on each partition.

i=1.

Algorithm 1 Recursive Partitioning for Conditional Uncorrelated Regressors
Input: Training data Dt0 = {(Xi  Yi)}n
Output: Data partitions D = {Di}L
i=1.
1: Initialize: No pre-speciﬁed partitions  D = ∅; the depth of the root node Dep(t0) = 0.
2: if |Dt0| < 2Nmin or Dep(t0) > Depmax then
3:
4: else
5:
6:

{(j  a)(cid:12)(cid:12)j = 1 ···   p  a∈{Xj(i)|(X(i)  Y (i)) ∈ Dt0}  Nmin ≤ |{i|Xj(i) > a}| < |Dt0| − Nmin}.

return D = D ∪ {Dt0}
Fit a least square linear regression of Y on X over Dt0 and get the estimated residuals ˆe.
Calculate the criterion function C(j  a) for (j  a) in the set of candidate split pairs Ct0 =
(ˆj  ˆa) = argmax(j a) C(j  a)
Let tL and tR be the left and right child-nodes of t0  Dep(tL) =Dep(tR) =Dep(t0) + 1  with
DtL =
t0 ⇐ tL and execute step 2 − 11.
t0 ⇐ tR and execute step 2 − 11.

(cid:111) ∩ Dt0 and DtR =

(cid:12)(cid:12)(cid:12)Xˆj(i) ≤ ˆa

(cid:12)(cid:12)(cid:12)Xˆj(i) > ˆa

(cid:111) ∩ Dt0 .

(X(i)  Y (i))

(X(i)  Y (i))

(cid:110)

7:
8:

9:
10:
11: end if

(cid:110)

When taking the correlation between regressors into consideration  we apply Algorithm 2 to select
the optimal split over the original untransformed variables  which retains easy interpretability of the
partitions but requires higher computation cost as is analyzed in the following. Since Algorithm 1 has
outlined the recursive partitioning process  Algorithm 2 will concentrate on the split selection at the
node level  which corresponds to Line 5−7 of Algorithm 1.
Enlightened by Theorem 2.2  we select the optimal split by minimizing the distance between (ˆj  ˆa)(¯j ¯a)
and (¯j  ¯a) in Algorithm 2  where the standardized distance d(ˆa(¯j ¯a)  ¯a) =|ˆa(¯j ¯a) − ¯a|/ˆσ(X¯j) is used
for ¯j = ˆj(¯j ¯a)  with ˆσ(X¯j) being the sample standard deviation of X¯j.

i=1  |Dt0| > 2Nmin.

calculate the criterion function C(¯j ¯a)(j  a) for each (j  a) ∈ Ct0;
(ˆj  ˆa)(¯j ¯a) = argmax(j a) C¯j ¯a(j  a)  the ‘local’ optimal split under (¯j  ¯a) 

Algorithm 2 Split Selection for Correlated Regressors
Input: Training data Dt0 = {(Xi  Yi)}n
Output: The optimal split variable and level pair (ˆj  ˆa); or no splits and t0 is a terminal node.
1: Fit a least square linear regression of Y on X over Dt0 and get the estimated residuals ˆe.
2: for each (¯j  ¯a) ∈ Ct0 do
3:
4:
5: end for
6: if {(ˆj  ˆa)(¯j ¯a)|¯j = ˆj(¯j ¯a)} (cid:54)= ∅ then
7:
8: else
9:
10: end if

return the optimal split (ˆj  ˆa) = argmin(¯j ¯a){d(ˆa(¯j ¯a)  ¯a)(cid:12)(cid:12)(¯j  ¯a) ∈ Ct0  ¯j = ˆj(¯j ¯a)}.

return no suitable splits  t0 is a terminal node.

As for the computation complexity of Algorithm 2  suppose there are Mt candidate splits in a node t 
then it involves M 2
t times of calculations of the criterion functions. Since the calculation of Kendall’s
τ in C(¯j ¯a)(· ·) is of complexity O(Nt log(Nt))  with Nt being the sample size of node t. Hence
the complexity of Algorithm 2 is M 2
t O(Nt log(Nt))  which is costly compared to Algorithm 1 that
is only MtO(Nt log(Nt)). Therefore  we may adopt a stopping strategy that terminates the split
process when the min d(ˆa(¯j ¯a)  ¯a) in Line 7 in Algorithm 2 is larger than a given threshold.
Applying either Algorithm 1 or Algorithm 2 recursively for split selections leads to an initial tree
Tmax  which determines the initial partitions. We outline the pruning of Tmax in the following.

5

3.2 Minimal Cost-complexity Tree Pruning

We adopt the minimal cost-complexity pruning procedure in CART [11]  but with a newly deﬁned
Deﬁne the accuracy measure at a node t in a tree T as I(t) =(cid:80)
cost-complexity measure Iα(T ) for the regression tree T with linear regression models on leaves.
(X(i) Y (i))∈t(Y (i) − ˆmT (Xi))2 
I(T ) =(cid:80)
t∈(cid:101)T I(t)  where (cid:101)T denotes the set of leaf nodes in T and n is the sample size of the training
where ˆmT (·) is the segmented liner regression function determined by T . The accuracy of T is
data. The model complexity of T is measured by the number of leaf nodes |(cid:101)T|.
measure Iα(T ) = I(T )/n + α|(cid:101)T|  where α is a positive penalizing parameter. The optimally pruned
tree T (α) is deﬁned as the smallest subtree of Tmax that minimizes Iα(T )  same as the Deﬁnition 3.6
in [11]. Proposition 3.1 veriﬁes the existence and the uniqueness of T (α) and the nested structure of
{T (α)  α > 0} as α varies  which is essential for an efﬁcient programming. The proof is by induction
where the key is an inequality satisﬁed by Iα(T ). Please refer to the supplementary for details.

Taking both the accuracy measure and model complexity into consideration  the cost-complexity

Proposition 3.1 Let Tmax be the initial tree  then

(i) given an α  there exists one optimally pruned subtree T (α) of Tmax;

(ii) if α2 > α1  then T (α2) is a subtree of or equal to T (α1).

To obtain the optimally pruned tree  the optimal complexity parameter α∗should be selected. Although
α runs through a continuum of values  there are ﬁnite number of subtrees T (α)  say K subtrees of
Tmax. Then by Proposition 3.1  there exists an increasing sequence of {αk|k = 1 ···   K} such
(cid:12)(cid:12)t ∈ T and t /∈ (cid:101)T} for T = Tmax when k = 1 and T = T (αk−1) when k > 1.
that T (αk+1) ⊂ T (αk)  and for α ∈ [αk  αk+1)  T (α) = T (αk). In fact  {αk}K
k=1 can be exactly
calculated from Tmax. Speciﬁcally  let Tt be the subbranch of a tree T with node t being its root  then
|(cid:101)Tt|−1
αk = mint{ I(t)−I(Tt)
Therefore  the optimization of α∗ is reduced to selecting an optimal k∗ from {1 ···   K}.
let
αkαk+1 for 1≤ k≤ (K−1)and ¯αK = αK . The optimal complexity parameter α∗ is selected
¯αk =
by the sum of squared residuals. Then  the optimally pruned subtree is T (α∗). Let(cid:98)L be the number
from {¯αk}K
k=1 by the ten-fold cross-validation to optimize the average predictive accuracy measured
appropriate α∗  it can be proved that(cid:98)L converges to the genuine number of segments L0 in probability.
of terminal nodes in T (α∗)  under certain general conditions for the distribution of ε and given

√

4 Leaf Modeling and Ensemble Methods

4.1 LASSO Linear Regression on Leaf Nodes

Let { ˆDl}(cid:98)L

l=1 be the partitions structure determined by T (α∗). Conﬁned on each partition  the regression
coefﬁcients (αl  βl) can be estimated by the ordinary least square. However  as X is the overall
regressors  not each of them necessarily owns a non-zero coefﬁcient over ˆDl  and the signiﬁcant
variables set within each ˆDl may vary. Thus  we consider the variables selection within each leaf
node. Besides  as the partitioning process decreases the sample size for estimation in each node  we
would like to determine a smaller set of variables that exhibit the strongest effects on each ˆDl.
To this purpose  the LASSO method [7] is employed for the variables selection  where the regression
coefﬁcients (αl  βl) is estimated by

(cid:88)

p(cid:88)

j=1

(cid:17)

lasso

l = ¯Y ˆDl
ˆα

; ˆβ

lasso
l = argmin

βl

{i|X (s)(i)∈ ˆDl}

{(Y (i) − ¯Y − X (r)(i)(cid:48)βl)2 + λl

|βl(j)|} 

where ¯Y ˆDl

is the sample average over ˆDl and λl is the shrinkage parameter selected by cross-validation.

(cid:16)

(cid:98)L(cid:80)

l=1

l +X (r)(cid:48) ˆβ
ˆαlasso

lasso
l

1(X (s) ∈ ˆDl).

Then  the ﬁnal prediction function is ˆm

lasso
T (α∗ )

(X) =

6

4.2 Weighted Random Forests

We can implant the Kendall’s τ based partitioning learning algorithm in random forests (RF  [21]) to
create the ensemble predictor. Here we propose the weighted random forests (WRF)  that considers
the accuracy of each tree and puts the ﬁnal predictor as a weighted average to improve the predictions.
Suppose {Tb}B
b=1 are the B regression trees induced from the bootstrap training sets. The RF takes
the simple average over the predictions of all regression trees  that is  ˆmRF(X) = 1
(X).
Note given the training set and X  { ˆmT1 (X) ···   ˆmTB (X)} are independent random variables. The
B
variance of ˆmTb (X) reﬂects the predictive accuracy of regression tree Tb  that can be estimated by
I(tTb (X)) for tTb (X) is the leaf node in Tb containing X. According to Proposition 4.1  taking the
variances of single predictors into account will improve the accuracy of the ensemble predictor.
Proposition 4.1 Let {Zb}B
P having a common mean µ and Var(Zb) = σ2

b=1 be independently distributed random variables from a population P ∈
b . Let A be the class of all unbiased linear estimations

for µ. Then  the optimal estimation that minimizes E(A − µ)2(A ∈ A) is(cid:80)B

(cid:80)B

b=1 ˆmTb

b(cid:80)B

1/σ2
j=1 1/σ2
j

Zb.

b=1

Motivated by Proposition 4.1  we propose the weighted random forests (WRF)  which shows improved
predictive accuracy over the RF on the benchmark datasets in Section 5.2.

ˆmWRF(X) =

1/I(tTb (X))
j=1 1/I(tTj (X))

ˆmTb

(X).

(5)

B(cid:88)

(cid:80)B

b=1

5 Experimental Results

5.1 Simulation Study

In this part  we would illustrate SLRT with two examples. One is segmented linear regression function
to investigate the performance of partitions structure learning  the other is a general continuous
regression function considered in [22]  to illustrate how our method works under a general setting.
First  we consider a regression function m(X) that is segmented linear with 12 segments determined
by 4 binary splits at X1 = 10  X2 = 10  X2 = 15  X4 ∈ {a  b} or {c}:

m(X) = 3X1I{X2 > 15} − 3X1I{X2 ≤ 15} − 3X2I{X2 > 10} − 5X2I{X2 ≤ 10}
+X3I{X1 > 10} − X3I{X1 ≤ 10} + X3I{X4 ∈ {‘a’  ‘b’}} − 3X3I{X4 ∈ {‘c’}}.

(6)
Training data of size 1500 was generated from Y = m(X) + ε with ε ∼ N (0  1) and independent
regressors X1∼ U (0  20)  X2∼ U (0  25)  X3∼ U (0  10) and X4 took values in {‘a’  ‘b’  ‘c’} with
equal probabilities (see Supplementary for the case of dependent regressors). Figure 1 shows that the
estimated partitions in the terminal nodes of T (α∗) are quite close to the space partitions in m(X).

Figure 1: The optimally pruned tree T (α∗) of α∗ = 0.0957  with |(cid:101)T (α∗)| = 12 and I(T (α∗)) = 1.13.

Furthermore  100 repetitions of the simulation from (6) were made. Figure 2 provides the histograms
of the estimated split levels on X1  X2 and X4  collected from the 100 optimally pruned trees.

7

All Datax2<15.02x2>15.02x4 in { a  b }x4 in { c }x4 in { a  b }x4 in { c }x2<9.99x2>9.99x2<9.93x2>9.93x1<9.97x1>9.97x1<10.0x1>10.0x1<9.87x1>9.87x1<9.84x1>9.84x1<9.68x1>9.68x1<9.91x1>9.91Figure 2: The histogram of split levels in 100 pruned trees

The selected splits concentrated around the genuine split levels with high probability. Speciﬁcally 
95% of the splits on X1 were within (9  11) (the true split at X1 = 10)  96% of the splits on X2 were
within (9  11)∪ (14  16) (the true splits at X2 = 10 and 15)  and nearly 98% of the splits on X4 were
in the form of {{a  b} {c}}  This strongly supported the consistency results of the split selection
procedure and validated the pruning procedures could effectively remove the redundant splits.

1   e−50X 2

1 +X 2

2 1.25e−5(X 2

The second example is a regression function that
does not conform the segmented linear form 
m(X) = max{e−10X 2
2 )} 
which was also considered in [22]. Figure 3 demon-
strates the surface of m(X) within the domain of
[−1  1]2. We generated the training data of 1000
i.i.d∼ U (−1  1)
records Y = m(X) + ε  for X1 X2
and ε ∼ N (0  0.01). With the same stopping pa-
rameter of Nmin = 10  Depmax = 10  we applied
SLRT and CART respectively  obtaining the ap-
proximated surface in Figure 4 and 5.

Figure 3: The true surface deﬁned by m(X)

Figure 4: The approximated surface by SLRT

Figure 5: The approximated surface by CART

Then we calculated the root mean squared prediction errors (RMSPE) on an independent testing
sample of size 500. The RMSPE of SLRT is 0.047  and that of CART is higher at 0.073. Under
this situation  SLRT obtains a locally linear approximation with a large tree structure  which tend to
outperforms CART since it is locally constant. In practice  since the model complexity (the tree size)
is adaptive to the nature of data  it depends on data whether the estimated regression function is a
interpretable segmented linear approximation or a locally linear approximation.

5.2 Comparisons on Benchmark Datasets

The predictive performance is tested on 9 benchmark datasets from the StatLib library [23] and
the UCI Machine Learning Repository [24]  where the sample sizes range from 74(Pyrimidine) to
39644(News Popularity) and 5 datasets include categorical variables. Detailed information about the
covariates and sample sizes are reported in the supplementary materials.
The proposed SLRT with the least square estimation (SLRTLS) and the LASSO (SLRTLASSO) were
compared with three tree-based methods: CART [11]  GUIDE [17  25] and MARS [26]  with the

8

0.00.20.40.60.8481216X1Frequency0.00.10.20.30.40.55101520X20.00.30.60.9{a  b} {c}{a} {b}{a}  {b  c}X4same Nmin and Depmax for all the methods. Ensemble predictors RFSLRT and WRFSLRT are random
forests (RF) and the newly proposed weighted random forests (WRF) equipped with SLRT as the base
predictor. The conventional RF based on CART (RFCART) as well as WRF with CART (WRFCART)
were also implemented to serve as benchmarks. To make the results of RF and WRF comparable 
their predictions were based on the same ensembles of 50 trees and only different in the way of
aggregation. To evaluate the predictive performance  we divided each dataset into 10 subsets and
implemented each method for 10 times using each subset as the testing set and the rest as the training
set  where all methods shared the same training and testing sets. Table 1 summarizes the average
RMSPEs from the 10-fold cross validation  where the integers in parentheses indicate the ranks
within the single and the ensemble predictors  respectively.

Table 1: RMSPE (rank) of 10-fold cross-validation on 9 data sets: data name(sample size). The best
performance is marked in bold italic  within each group of single predictors and ensemble predictors.

Datasets

Single Predictors

Ensemble Predictors

SLRTLSSLRTLASSO GUIDE CART MARS RFSLRT WRFSLRT RFCART WRFCART
0.174(2) 0.170(1) 0.187(4) 0.262(5) 0.179(3) 0.162(2) 0.158(1) 0.218(4) 0.200(3)
Boston Housing(506)
ComputerHardware(209) 47.89(2) 47.40(1) 48.06(3) 62.60(5) 54.17(4) 38.49(2) 36.77(1) 64.91(4) 39.08(3)
2.831(2) 2.791(1) 3.545(4) 3.680(5) 2.942(3) 2.633(2) 2.614(1) 3.273(3) 3.240(4)
Auto-mpg(392)
0.154(2) 0.140(1) 0.231(5) 0.192(4) 0.184(3) 0.143(2) 0.142(1) 0.165(4) 0.162(3)
Auto-mobile(159)
0.139(2) 0.138(1) 0.140(3) 0.257(5) 0.198(4) 0.117(2) 0.115(1) 0.249(4) 0.247(3)
Kinematics(8192)
2.162(3) 2.143(1) 2.151(2) 2.497(5) 2.161(3) 2.116(2) 2.113(1) 2.458(4) 2.456(3)
Abalone(4176)
9.374(3) 9.327(2) 9.300(1) 10.534(5) 9.660(4) 8.691(2) 8.679(1) 10.326(4) 10.317(3)
Parkinson(5875)
0.088(2) 0.093(3) 0.096(5) 0.094(4) 0.074(1) 0.078(4) 0.076(3) 0.073(2) 0.049(1)
Pyrimidine(74)
NewsPopularity(39644) 0.877(4) 0.872(2) 0.873(3) 0.903(5) 0.865(1) 0.868(1) 0.868(1) 0.901(3) 0.901(3)

Among the ﬁve single tree predictors  SLRTLASSO attained the best prediction in six dataset  MARS in
two  and SLRTLS and GUIDE in one  respectively. This demonstrates the advantages of the proposed
SLRT. Directly comparing SLRT with GUIDE  SLRTLS had better prediction in 7 out of the 9 datasets
and SLRTLASSO in 8 out of 9 datasets. SLRT also compared favorably to MARS  having better
performance in 6 out of the 9 datasets. CART appeared to be the worst predictor in seven datasets 
while GUIDE ranked the last on the other two datasets. The better performance of SLRTLASSO over
SLRTLS shows the beneﬁts of conducting the variables selection on the leaf nodes.
The ensemble predictors RFSLRT and WRFSLRT showed better performance than the conventional
RF with CART in 8 out of 9 datasets. Meanwhile  the ensemble predictors tended to outperform
the single predictors  which suggests the effects of the bagging operation. The proposed WRF also
showed improved predictions over the RF  which beneﬁt from the weighting procedure that reduces
the importance of those under-performing trees.

6 Conclusion

We propose a tree based approach called segmented linear regression trees (SLRT)  which is based
on two consecutive algorithms for partitioning structure learning: one for the split selection at each
internal node based on a cumulative Kendall’s τ statistic; the other for the parsimonious partitioning
structure by tree pruning through an adaptive cost-complexity measure. Theoretical analysis shows
that the split selection algorithm leads to the consistent identiﬁcation and estimation of both the
genuine split variables and the split levels  and the pruning procedure ensures the consistent estimation
of the genuine number of segments. We implant the SLRT as the base predictor in RF and WRF
to create two breeds of ensemble predictors. The proposed procedures are evaluated by numerical
simulations and case studies  which shows advantageous predictive accuracy over other tree-based
methods  and in creating more powerful breeds of ensemble predictors.

Acknowledgments

This research is funded by China’s National Key Research Special Program Grant 2016YFC0207701 
National Key Basic Research Program Grant 2015CB856000 and National Natural Science Founda-
tion of China grant 71532001.

9

References
[1] H. Oiwa and R. Fujimaki  “Partition-wise linear models ” in NeurIPS  2014.

[2] J. Kim and H. J. Kim  “Asymptotic results in segmented multiple regression ” Journal of Multivariate

Analysis  vol. 99  no. 9  pp. 2016–2038  2008.

[3] P. Perron and Z. Qu  “Estimating restricted structural change models ” Journal of Econometrics  vol. 134 

pp. 373–399  2006.

[4] J. Liu  S. Wu  and J. V. Zidek  “On segmented multivariate regression ” Statistica Sinica  vol. 7  pp.

497–525  1997.

[5] J. Gonzaloa and J.-Y. Pitarakisb  “Estimation and model selection based inference in single and multiple

threshold models ” Journal of Econometrics  vol. 110  no. 2  pp. 319–352  2002.

[6] M. G. Kendall  “A new measure of rank correlation ” Biometrika  vol. 30  no. 1/2  pp. 81–93  1938.

[7] H. Trevor  T. Robert  and F. JH  The Elements of Statistical Learning: data mining  inference  and

prediction. New York: Springer series in statistics  2009.

[8] H.-J. Kim  B. Yu  and E. J. Feuer  “Selecting the number of change-points in segmented line regression ”

Statistica Sinica  vol. 19  no. 2  p. 597  2009.

[9] J. Wang and V. Saligrama  “Local supervised learning through space partitioning ” in NeurIPS  2012.

[10] X. Fan  B. Li  and S. SIsson  “Rectangular bounding process ” in NeurIPS  2018.

[11] L. Breiman  J. H. Friedman  R. A. Olshen  and C. J. Stone  Classiﬁcation And Regression Trees. New

York: Wadsworth International Group  1984.

[12] J. R. Quinlan  “Combining instance-based and model-based learning ” in ICML  1993.

[13] L. Torgo  “Partial linear trees ” in ICML  2000.

[14] Y. Wang and I. H. Witten  “Inducing model trees for continuous classes ” in ECML  1997.

[15] W. P. Alexander and S. D. Grimshaw  “Treed regression ” Journal of Computational and Graphical

Statistics  vol. 5  no. 2  pp. 156–175  1996.

[16] K.-C. Li  H.-H. Lue  and C.-H. Chen  “Interactive tree-structured regression via principal hessian directions ”

Journal of the American Statistical Association  vol. 95  pp. 547–560  2000.

[17] W.-Y. Loh  “Regression trees with unbiased variable selection and interaction detection ” Statistica Sinica 

vol. 12  pp. 361–386  2002.

[18] W.-Y. Loh and W. Zheng  “Regression trees for longitudinal and multi-response data ” The Annals of

Applied Statistics  vol. 7  no. 1  pp. 495–522  2013.

[19] D. Malerba  F. Esposito  M. Ceci  and A. Appice  “Top-down induction of model trees with regression
and splitting nodes ” IEEE Transactions on Pattern Analysis and Machine Intelligence  vol. 26  no. 5  pp.
612–625  2004.

[20] A. Karaliˇc  “Linear regression in regression tree leaves ” in ECAI  1992.

[21] L. Breiman  “Random forests ” Machine learning  vol. 45  no. 1  pp. 5–32  2001.

[22] D. Potts and C. Sammut  “Incremental learning of linear model trees ” Machine Learning  vol. 61  no. 1-3 

pp. 5–48  2005.

[23] “Statlib library ” http://lib.stat.cmu.edu/datasets/.

[24] D. Dheeru and E. Karra Taniskidou  “UCI machine learning repository ” http://archive.ics.uci.edu/ml  2017.

[25] W.-Y. Loh  “Guide classiﬁcation and regression trees and forests version 29.7 ” https://www.stat.wisc.edu/

~loh/guide.html  2018.

[26] J. H. Friedman  “Multivariate adaptive regression splines ” The Annals of Statistics  vol. 19  no. 1  pp. 1–67 

1991.

10

,James Martens
Arkadev Chattopadhya
Toni Pitassi
Richard Zemel
Vitaly Kuznetsov
Mehryar Mohri
Umar Syed
Zhe Li
Boqing Gong
Tianbao Yang
Xiangyu Zheng
Song Xi Chen