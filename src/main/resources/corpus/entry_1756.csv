2017,Efficient Second-Order Online Kernel Learning with Adaptive Embedding,Online kernel learning (OKL) is a flexible framework to approach prediction problems  since the large approximation space provided by reproducing kernel Hilbert spaces can contain an accurate function for the problem. Nonetheless  optimizing over this space is computationally expensive. Not only first order methods accumulate $\O(\sqrt{T})$ more loss than the optimal function  but the curse of kernelization results in a $\O(t)$ per step complexity. Second-order methods get closer to the optimum much faster  suffering only $\O(\log(T))$ regret  but second-order updates are even more expensive  with a $\O(t^2)$ per-step cost. Existing approximate OKL methods try to reduce this complexity either by limiting the Support Vectors (SV) introduced in the predictor  or by avoiding the kernelization process altogether using embedding. Nonetheless  as long as the size of the approximation space or the number of SV does not grow over time  an adversary can always exploit the approximation process. In this paper  we propose PROS-N-KONS  a method that combines Nystrom sketching to project the input point in a small  accurate embedded space  and performs efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate  and we show that the per-step cost only grows with the effective dimension of the problem and not with $T$. Moreover  the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably.,Efﬁcient Second-Order Online Kernel
Learning with Adaptive Embedding

Daniele Calandriello

Alessandro Lazaric

Michal Valko

SequeL team  INRIA Lille - Nord Europe  France

{daniele.calandriello  alessandro.lazaric  michal.valko}@inria.fr

Abstract

Online kernel learning (OKL) is a ﬂexible framework for prediction problems 
since the large approximation space provided by reproducing kernel Hilbert spaces
often contains an accurate function for the problem. Nonetheless  optimizing over
√
this space is computationally expensive. Not only ﬁrst order methods accumulate
O(
T ) more loss than the optimal function  but the curse of kernelization results
in a O(t) per-step complexity. Second-order methods get closer to the optimum
much faster  suffering only O(log T ) regret  but second-order updates are even
more expensive with their O(t2) per-step cost. Existing approximate OKL methods
reduce this complexity either by limiting the support vectors (SV) used by the
predictor  or by avoiding the kernelization process altogether using embedding.
Nonetheless  as long as the size of the approximation space or the number of
SV does not grow over time  an adversarial environment can always exploit the
approximation process. In this paper  we propose PROS-N-KONS  a method that
combines Nyström sketching to project the input point to a small and accurate
embedded space; and to perform efﬁcient second-order updates in this space. The
embedded space is continuously updated to guarantee that the embedding remains
accurate. We show that the per-step cost only grows with the effective dimension
of the problem and not with T . Moreover  the second-order updated allows us to
achieve the logarithmic regret. We empirically compare our algorithm on recent
large-scales benchmarks and show it performs favorably.

Introduction

1
Online learning (OL) represents a family of efﬁcient and scalable learning algorithms for building a
predictive model incrementally from a sequence of T data points. A popular online learning approach
[26] is to learn a linear predictor using gradient descent (GD) in the input space Rd. Since we can
explicitly store and update the d weights of the linear predictor  the total runtime of this algorithm is
O(T d)  allowing it to scale to large problems. Unfortunately  it is sometimes the case that no good
predictor can be constructed starting from only the linear combination of the input features. For this
reason  online kernel learning (OKL) [10] ﬁrst maps the points into a high-dimensional reproducing
kernel Hilbert space (RKHS) using a non-linear feature map ϕ  and then runs GD on the projected
points  which is often referred to as functional GD (FGD) [10]. With the kernel approach  each
gradient step does not update a ﬁxed set of weights  but instead introduces the feature-mapped point
in the predictor as a support vector (SV). The resulting kernel-based predictor is ﬂexible and data
adaptive  but the number of parameters  and therefore the per-step space and time cost  now scales
with O(t)  the number of SVs included after t steps of GD. This curse of kernelization results in an
O(T 2) total runtime  and prevents standard OKL methods from scaling to large problems.
Given an RKHS H containing functions with very small prediction loss  the objective of an OL
algorithm is to approach over time the performance of the best predictor in H and thus minimize the
regret  that is the difference in cumulative loss between the OL algorithm and the best predictor in

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

feature map ϕ with an approximate (cid:101)ϕ constructed using a Nyström dictionary approximation. For

√
hindsight. First-order GD achieve a O(
T ) regret for any arbitrary sequence of convex losses [10].
However  if we know that the losses are strongly convex  setting a more aggressive step-size in
ﬁrst-order GD achieves a smaller O(log T ) regret [25]. Unfortunately  most common losses  such as
the squared loss  are not strongly convex when evaluated for a single point xt. Nonetheless  they
posses a certain directional curvature [8] that can be exploited by second-order GD methods  such as
kernelized online Newton step (KONS) [2] and kernel-recursive least squares (KRLS) [24]  to achieve
the O(log T ) regret without strong convexity along all directions. The drawback of second-order
methods is that they have to store and invert the t × t covariance matrix between all SV included
in the predictor. This requires O(t2) space and time per-step  dwarﬁng the O(t) cost of ﬁrst-order
methods and resulting in an even more infeasible O(T 3) runtime.
Contributions In this paper  we introduce PROS-N-KONS  a new OKL method that (1) achieves
logarithmic regret for losses with directional curvature using second-order updates  and (2) avoids
the curse of kernelization  taking only a ﬁxed per-step time and space cost. To achieve this  we start
from KONS  a low-regret exact second-order OKL method proposed in [2]  but replace the exact
a dictionary of size j  this non-linearly embeds the points in Rj  where we can efﬁciently perform
exact second-order updates in constant O(j2) per-step time  and achieve the desired O(log T ) regret.
Combined with an online dictionary learning (KORS [2]) and an adaptive restart strategy  we show
that we never get stuck performing GD in an embedded space that is too distant from the true H 
but at the same time the size of the embedding j never grows larger than the effective dimension
of the problem. While previous methods [13  11] used ﬁxed embeddings  we adaptively construct
a small dictionary that scales only with the effective dimension of the data. We then construct an
accurate approximation of the covariance matrix  to avoid the variance due to dictionary changes
using carefully designed projections.
Related work Although ﬁrst-order OKL methods cannot achieve logarithmic regret  many approxi-
mation methods have been proposed to make them scale to large datasets. Approximate methods
usually take one of two approaches  either performing approximate gradient updates in the true RKHS
(budgeted perceptron [4]  projectron [15]  forgetron [6]) preventing SV from entering the predictor 
or exact gradient updates in an approximate RKHS (Nyström [13]  random feature expansion [11]) 
where the points are embedded in a ﬁnite-dimensional space and the curse of kernelization does not
apply. Overall  the goal is to never exceed a budget of SVs in order to maintain a ﬁxed per-step
update cost. Among budgeted methods  weight degradation [17] can be done in many different ways 
such as removal [6] or more expensive projection [15] and merging. Nonetheless  as long as the
size of the budget is ﬁxed  the adversary can exploit this to increase the regret of the algorithm  and
oblivious inclusion strategies such as uniform sampling [9] fail. Another approach is to replace the

exact feature-map ϕ with an approximate feature map (cid:101)ϕ which allows to explicitly represent the

mapped points  and run linear OL on this embedding [13  21]. When the embedding is oblivious to
data  the method is known as random-feature expansion  while a common data-dependent embedding
mapping is known as Nyström method [19]. Again  if the embedding is ﬁxed or with a limit in size 
the adversary can exploit it. In addition  analyzing a change in embedding during the gradient descent
is an open problem  since the underlying RKHS changes with it.
The only approximate second-order method known to achieve logarithmic regret is SKETCHED-
KONS. Both SKETCHED-KONS and PROS-N-KONS are based on the exact second-order OL
method ONS [8] or its kernelized version KONS [2]. However  SKETCHED-KONS only applies
budgeting techniques to the Hessian of the second-order updates and not to the predictor itself 
resulting in a O(t) per-step evaluation time cost. Moreover  the Hessian sketching is performed
only through SV removal  resulting in high instability. In this paper  we solve these two issues with
PROS-N-KONS by directly approximating KONS using Nyström functional approximation. This
results in updates that are closer to SV projection than removal  and that budget both the representation
of the Hessian and the predictor.

2 Background
Notation We borrow the notation from [14] and [2]. We use upper-case bold letters A for matrices 
lower-case bold letters a for vectors  lower-case letters a for scalars. We denote by [A]ij and [a]i the
(i  j) element of a matrix and i-th element of a vector respectively. We denote by IT ∈ RT×T the
identity matrix of dimension T and by Diag(a) ∈ RT×T the diagonal matrix with the vector a ∈ RT
on the diagonal. We use eT i ∈ RT to denote the indicator vector of dimension T for element i.

2

When the dimension of I and ei is clear from the context  we omit the T   and we also indicate the
identity operator by I. We use A (cid:23) B to indicate that A− B is a positive semi-deﬁnite (PSD) matrix.
Finally  the set of integers between 1 and T is denoted by [T ] := {1  . . .   T}.
Kernels Given an input space X and a kernel function K(· ·) : X × X → R  we denote the
reproducing kernel Hilbert space (RKHS) induced by K by H   and with ϕ(·) : X → H the
associated feature map. Using the feature map  the kernel function can be represented as K(x  x(cid:48)) =
(cid:104)ϕ(x)  ϕ(x(cid:48))(cid:105)H  but with a slight abuse of notation we use the simpliﬁed notation K(x  x(cid:48)) =
ϕ(x)Tϕ(x(cid:48)) in the following. Any function f ∈ H can be represented as a (potentially inﬁnite) set of
weights w such that fw(x) = ϕ(x)Tw. Given a set of t points  Dt = {xs}t
s=1 we denote the feature
matrix with φs as its s-th column by Φt ∈ R∞×t.
Online kernel learning (OKL) We consider online kernel learning  where an adversary chooses
an arbitrary sequence of points {xt}T
t=1. The learning
protocol is the following. At each round t ∈ [T ] (1) the adversary reveals the new point xt  (2)
the learner chooses a function fwt and predicts fwt (xt) = ϕ(xt)Twt  (3) the adversary reveals the
loss (cid:96)t  and (4) the learner suffers (cid:96)t(ϕ(xt)Twt) and observes the associated gradient gt. We are
interested in bounding the cumulative regret between the learner and a ﬁxed function w deﬁned as
t=1 (cid:96)(φtwt) − (cid:96)(φtw). Since H is potentially a very large space  we need to restrict
the class of comparators w. As in [14]  we consider all functions that guarantee bounded predictions 
i.e.  S = {w : ∀t ∈ [T ] |φT
Assumption 1 (Scalar Lipschitz). The loss functions (cid:96)t satisfy |(cid:96)(cid:48)
Assumption 2 (Curvature). There exists σt ≥ σ > 0 such that for all u  w ∈ S and for all t ∈ [T ] 

t w| ≤ C}. We make the following assumptions on the losses.
t(z)| whenever |z| ≤ C.

t=1 and convex differentiable losses {(cid:96)t}T

RT (w) =(cid:80)T

(cid:96)t(φT

t w) := lt(w) ≥ lt(u) + ∇lt(u)T(w − u) +

(∇lt(u)T(w − u))2 .

σt
2

This assumption is weaker than strong convexity as it only requires the losses to be strongly convex
in the direction of the gradient. It is satisﬁed by squared loss  squared hinge loss  and in general  all
exp-concave losses [8]. Under this weaker requirement  second-order learning methods [8  2]  obtain
the O(log T ) regret at the cost of a higher computational complexity w.r.t. ﬁrst-order methods.
Nyström approximation A common approach to alleviate the computational cost is to replace
I = {xi}j

the high-dimensional feature map ϕ with a ﬁnite-dimensional approximate feature map (cid:101)ϕ. Let
ϕ(xi) as columns. We deﬁne the embedding (cid:101)ϕ(x) := Σ−1UTΦTIϕ(x) ∈ Rj  where ΦI = VΣUT

i=1 be a dictionary of j points from the dataset and ΦI be the associated feature matrix with

is the singular value decomposition of the feature matrix. While in general ΦI is inﬁnite dimensional
and cannot be directly decomposed  we exploit the fact that UΣVTVΣUT = ΦTIΦI = KI =
UΛUT and that KI is a (ﬁnite-dimensional) PSD matrix. Therefore it is sufﬁcient to compute
the eigenvectors U and eigenvalues Λ of KI and take the square root Λ1/2 = Σ. Note that with
this deﬁnition we are effectively replacing the kernel K and H with an approximate KI and HI 

such that KI(x  x(cid:48)) = (cid:101)ϕ(x)T(cid:101)ϕ(x(cid:48)) = ϕ(x)TΦIUΣ−1Σ−1UTΦTIϕ(x(cid:48)) = ϕ(x)TPIϕ(x(cid:48)) where
PI = ΦI(ΦTIΦI)−1ΦTI is the projection matrix on the column span of ΦI. Since (cid:101)ϕ returns vectors
down to the size of the dictionary j. The accuracy of (cid:101)ϕ is directly related to the accuracy of the
in Rj  this transformation effectively reduces the computation complexity of kernel operations from t
(cid:101)ϕ(xs)T(cid:101)ϕ(xs(cid:48)) is close to ϕ(xs)TPtϕ(xs(cid:48)) = ϕ(xs)Tϕ(xs(cid:48)).
t  so that for all s  s(cid:48) ∈ [t] 
Ridge leverage scores All that is left is to ﬁnd an efﬁcient algorithm to choose a good dictionary I
to minimize the error PI − Pt. Among dictionary-selection methods  we focus on those that
sample points proportionally to their ridge leverage scores (RLSs) [1] because they provide strong
reconstruction guarantees. We now deﬁne RLS and associated effective dimension.
Deﬁnition 1. Given a kernel function K  a set of points Dt = {xs}t
γ-ridge leverage score (RLS) of point i is deﬁned as

projection PI in approximating the projection Pt = Φt(ΦT

s=1 and a parameter γ > 0  the

t Φt)−1ΦT

τt i = et iKt(Kt + γIt)−1et i = φT

i (ΦtΦT

t + γI)−1φi 

and the effective dimension of Dt as their sum for the each example of Dt 

t(cid:88)

τt i = Tr(cid:0)Kt(Kt + γIt)−1(cid:1) .

dt
eff(γ) =

i=1

3

(1)

(2)

eff(γ) =(cid:80)t

The RLS of a point measures how orthogonal φi is w.r.t. to the other points in Φt  and therefore
how important it is to include it in I to obtain an accurate projection PI. The effective dimension
captures the capacity of the RKHS H over the support vectors in Dt. Let {λi}i be the eigenvalues of
s=1 λi/(λi + γ)  the effective dimension can be seen as the soft rank of Kt
Kt  since dt
where only eigenvalues above γ are counted.
To estimate the RLS and construct an accurate I  we leverage KORS [2] (see Alg. 1 in App. A) that
extends the online row sampling of Cohen et al. [5] to kernels. Starting from an empty dictionary 
at each round  KORS receives a new point xt  temporarily adds it to the current dictionary It and

estimates its associated RLS(cid:101)τt. Then it draws a Bernoulli r.v. proportionally to(cid:101)τt. If the outcome
is one  the point is deemed relevant and added to the dictionary  otherwise it is discarded and never
added. Note that since points get only evaluated once  and never dropped  the size of the dictionary
grows over time and the RKHS HIt is included in the RKHS HIt+1  guaranteeing stability in the
RKHS evolution  unlike alternative methods (e.g.  [3]) that construct smaller but often changing
dictionaries. We restate the quality of the learned dictionaries and the complexity of the algorithm
that we use as a building block.
Proposition 1 ([2  Thm. 2]). Given parameters 0 < ε ≤ 1  0 < γ  0 < δ < 1  if β ≥ 3 log(T /δ)/ε2
then the dictionary learned by KORS is such that w.p. 1 − δ 

(1) for all rounds t ∈ [T ]  we have 0 (cid:22) ΦT
(2) the maximum size of the dictionary J is bounded by 1+ε

t (Pt − PIt)Φt (cid:22) + ε
1−ε 3βdT

1−ε γI  and
eff(γ) log(2T /δ).

The algorithm runs in O(dT

eff(α)2 log4(T )) space and (cid:101)O(dT

eff(α)3) time per iteration.

3 The PROS-N-KONS algorithm
We ﬁrst use a toy OKL example from [2] to illustrate the main challenges for FGD in getting both
computational efﬁciency and optimal regret guarantees. We then propose a different approach which
will naturally lead to the deﬁnition of PROS-N-KONS.
Consider the case of binary classiﬁcation with the square loss  where the point presented by the
adversary in the sequence is always the same point xexp  but each round with an opposite {1 −1}
label. Note that the difﬁculty in this problem arises from the adversarial nature of the labels and it is
not due to the dataset itself. The cumulative loss of the comparator w becomes (ϕ(xexp)Tw − 1)2 +
(ϕ(xexp)Tw + 1)2 + . . . for T steps. Our goal is to achieve O(log T ) regret w.r.t. the best solution
in hindsight  which is easily done by always predicting 0. Intuitively an algorithm will do well when
√
the gradient-step magnitude shrinks as 1/t. Note that these losses are not strongly convex  thus exact
ﬁrst-order FGD only achieves O(
T ) regret and does not guarantee our goal. Exact second-order
methods (e.g.  KONS) achieve the O(log T ) regret  but also store T copies of the SV  and have T 4
runtime. If we try to improve the runtime using approximate updates and a ﬁxed budget of SV  we
lose the O(log T ) regime  since skipping the insertion of a SV also slows down the reduction in the
step-size  both for ﬁrst-order and second-order methods. If instead we try to compensate the scarcity
of SV additions due to the budget with larger updates to the step-size  the adversary can exploit such
an unstable algorithm  as is shown in [2] where in order to avoid an unstable solution forces the
algorithm to introduce SV with a constant probability. Finally  note that this example can be easily
generalized for any algorithm that stores a ﬁxed budget of SV  replacing a single xexp with a set of
repeating vectors that exceed the budget. This also defeats oblivious embedding techniques such as
random feature expansion with a ﬁxed amount of random features or a ﬁxed dictionary  and simple
strategies that update the SV dictionary by insertion and removal.
If we relax the ﬁxed-budget requirement  selection algorithms such as KORS can ﬁnd an appropriate
budget size for the SV dictionary. Indeed  this single sample problem is intrinsically simple: its
eff(α) (cid:39) 1 is small  and its induced RKHS H = ϕ(xexp) is a singleton.
effective dimension dT
Therefore  following an adaptive embedding approach  we can reduce it to a one-dimensional
we can see this approach as constructing an approximate feature map (cid:101)ϕ that after one step will
parametric problem and solve it efﬁciently in this space using exact ONS updates. Alternatively 
replacing K with (cid:101)K. Building on this intuition  we propose PROS-N-KONS  a new second-order
exactly coincide with the exact feature map ϕ  but allows us to run exact KONS updates efﬁciently
FGD method that continuously searches for the best embedding space HIt and  at the same time 
exploits the small embedding space HIt to efﬁciently perform exact second-order updates.

4

5:
6:
7:
8:
9:
10:
11:

jΦT

j UT

We start from an empty dictionary I0 and a null predictor w0 = 0. At each round  PROS-N-KONS
(Algorithm 1) receives a new point xt and invokes KORS to decide whether it should be included in
the current dictionary or not. Let tj with j ∈ [J] be the random step when KORS introduces xtj in
the dictionary. We analyze PROS-N-KONS as an epoch-based algorithm using these milestones tj.
Note that the length hj = tj+1 − tj and total number of epochs J is random  and is decided in a
data-adaptive way by KORS based on the difﬁculty of the problem. During epoch j  we have a
(cid:101)ϕ(x) : X → Rj = Σ−1
ﬁxed dictionary Ij that induces a feature matrix ΦIj containing samples φi ∈ Ij  an embedding
Φj  with its associated approximate kernel function (cid:101)K and induced RKHS Hj. At each round
jϕ(x) based on the singular values Σj and singular vectors Uj of
tj < t < tj+1  we perform an exact KONS update using the approximate map (cid:101)ϕ. This can be
computed explicitly since (cid:101)φt is in Rj and can be easily stored in memory. The update rules are
t−1(cid:101)φt 
(cid:101)At = (cid:101)At−1 +
(cid:101)A−1
t−1(cid:101)gt−1  (cid:101)ωt = ΠAt−1
1: Initialize j = 0  (cid:101)w0 = 0 (cid:101)g0 = 0 (cid:101)P0 = 0  (cid:101)A0 = αI 

(υt) =(cid:101)υt − h((cid:101)φT
t(cid:101)υt)
(cid:101)φT
t−1(cid:101)φt
t(cid:101)A−1

t   (cid:101)υt = (cid:101)ωt−1 − (cid:101)A−1

2(cid:101)gt(cid:101)gT

Input: Feasible parameter C  step-sizes ηt  regularizer α
2: Start a KORS instance with an empty dictionary I0.
3: for t = {1  . . .   T} do { Dictionary changed  reset.}
4:

Receive xt  feed it to KORS.

where the oblique projection ΠAt−1
is
computed using the closed-form solution
from [14]. When t = tj and a new epoch
begins  we perform a reset step before
taking the ﬁrst gradient step in the new
embedded space. We update the feature-

map (cid:101)ϕ  but we reset (cid:101)Atj and(cid:101)ωtj to zero.

Receive zt (point added to dictionary or not)

if zt−1 = 1 then

St

St

σt

−1

j ΦT

j UT
j

end if

12:
13:

−1
j UT

j φt ∈ Rj.

else {Execute a gradient-descent step.}

where h(z) = sign(z) max{|z| − C  0}

14:
15:
16:
17:
18: end for

j = j + 1
Build Kj from Ij and decompose it in UjΣjΣT

While this may seem a poor choice  as
information learned over time is lost  it
leaves intact the dictionary. As long as
(a) the dictionary  and therefore the em-
bedded space where we perform our GD 
keeps improving and (b) we do not need-
lessly reset too often  we can count on
the fast second-order updates to quickly
catch up to the best function in the current
Hj. The motivating reason to reset the de-
scent procedure when we switch subspace
is to guarantee that our starting point in
the descent cannot be inﬂuenced by the ad-
versary  and therefore allow us to bound
the regret for the overall process (Sect. 4).
Computational complexity PROS-N-
KONS’s computation complexity is dom-
inversion required to compute the projection and the gradient update and by the query
to KORS  that internally also inverts a j × j matrix. Therefore  a naïve implementation requires

Set (cid:101)At−1 = αI ∈ Rj×j.
(cid:101)ωt = 0 ∈ Rj
Compute map φt and approximate map (cid:101)φt =
Compute(cid:101)υt = (cid:101)ωt−1 − (cid:101)A
t−1(cid:101)gt−1.
t−1(cid:101)φt
(cid:101)A
Compute(cid:101)ωt = (cid:101)υt − h((cid:101)φT
t(cid:101)υt)
(cid:101)φT
t−1(cid:101)φt
t (cid:101)A
Predict(cid:101)yt = (cid:101)φT
t(cid:101)ωt.
t((cid:101)yt)(cid:101)φt.
Observe(cid:101)gt = ∇(cid:101)ωt (cid:96)t((cid:101)φT
t(cid:101)ωt) = (cid:96)(cid:48)
Update (cid:101)At = (cid:101)At−1 + σt
2(cid:101)gt(cid:101)gT

inated by (cid:101)A−1
O(j3) per-step time and has a space O(j2) space complexity necessary to store (cid:101)At. Notice that
that similarly  the (cid:101)At matrix is constructed using rank-one updates  a careful implementation reduces
using the bound on J provided by Prop. 1 and neglecting logarithmic terms reduces to (cid:101)O(T dT

taking advantage of the fact that KORS only adds SV to the dictionary and never removes them  and
the per-step cost to O(j2). Overall  the total runtime of PROS-N-KONS is then O(T J 2)  which
eff(γ)2).
Compared to other exact second-order FGD methods  such as KONS or RKLS  PROS-N-KONS
dramatically improves the time and space complexity from polynomial to linear. Unlike other
approximate second-order methods  PROS-N-KONS does not add a new SV at each step. This way
it removes T 2 from the O(T 2 + T dT
eff(γ)3) time complexity of SKETCHED-KONS [2]. Moreover 
when mint τt t is small  SKETCHED-KONS needs to compensate by adding a constant probability of
adding a SV to the dictionary  resulting in a larger runtime complexity  while PROS-N-KONS has
no dependency on the value of the RLS. Even compared to ﬁrst-order methods  which incur a larger
regret  PROS-N-KONS performs favorably  improving on the O(T 2) runtime of exact ﬁrst-order
FGD. Compared to other approximate methods  the variant using rank-one updates matches the
O(J 2) per-step cost of the more accurate ﬁrst-order methods such as the budgeted perceptron [4] 
projectron [15]  Nyström GD [13]  while improving on their regret. PROS-N-KONS also closely
matches faster but less accurate O(J) methods such as the forgetron [6] and budgeted GD [23].

Figure 1: PROS-N-KONS

−1

−1

t .

t

Σ

5

4 Regret guarantees
In this section  we study the regret performance of PROS-N-KONS.
constant L  let σ = mint σt. If ηt ≥ σ for all t  α ≤ √
Theorem 1 (proof in App. B ). For any sequence of losses (cid:96)t satisfying Asm. 2 with Lipschitz
T   γ ≤ α  and predictions are bounded by C 
log(cid:0)2σL2T /α(cid:1)(cid:19)
then the regret of PROS-N-KONS over T steps is bounded w.p. 1 − δ as

(cid:18) T γε

(cid:16) α

α(cid:107)w(cid:107)2 +

+ 2JC 

(cid:18)

(cid:17)

+ 1

+

(3)

L2
α

4(1 − ε)

4
σ

dT
eff

σL2

RT (w) ≤ J
where J ≤ 3βdT

(cid:19)
(cid:19)

(cid:18)

eff (γ) log(2T ) is the number of epochs. If γ = α/T the previous bound reduces to
RT (w) = O

eff (α/T ) log(T ) + dT

eff (α) log2(T )

α(cid:107)w(cid:107)2dT

eff (α/T ) dT

.

(4)

eff(γ)dT

the dictionary returned by KORS up to step T   which w.h.p. is (cid:101)O(dT
as (cid:101)O(dT

Remark (bound) The bound in Eq. 3 is composed of three terms. At each epoch of PROS-N-KONS 
an instance of KONS is run on the embedded feature space Hj obtained by using the dictionary Ij
constructed up to the previous epoch. As a result  we directly use the bound on the regret of KONS
(Thm. 1 in [2]) for each of the J epochs  thus leading to the ﬁrst term in the regret. Since a new epoch
is started whenever a new SV is added to the dictionary  the number of epochs J is at most the size of
eff(γ))  making the ﬁrst term scale
eff(α)) overall. Nonetheless  the comparator used in the per-epoch regret of KONS is
constrained to the RKHS Hj induced by the embedding used in epoch j. The second term accounts
for the difference in performance between the best solutions in the RKHS in epoch j and in the
original RKHS H. While this error is directly controlled by KORS through the RLS regularization γ
and the parameter ε (hence the factor γε/(1 − ε) from Property (1) in Prop. 1)  its impact on the
regret is ampliﬁed by the length of each epoch  thus leading to an overall linear term that needs to be
regularized. Finally  the last term summarizes the regret suffered every time a new epoch is started

and the default prediction(cid:98)y = 0 is returned. Since the values yt and(cid:98)yt are constrained in S  this

results in a regret of 2JC.
Remark (regret comparison) Tuning the RLS regularization as γ = α/T leads to the bound in
Eq. 4. While the bound displays an explicit logarithmic dependency on T   this comes at the cost
of increasing the effective dimension  which now depends on the regularization α/T . While in
general this could possibly compromise the overall regret  if the sequence of points φ1  . . .   φT
induces a kernel matrix with a rapidly decaying spectrum  the resulting regret is still competitive.
For instance  if the eigenvalues of KT decrease as λt = at−q with constants a > 0 and q > 1  then
√
eff (α/T ) ≤ aqT 1/q/(q − 1). This shows that for any q > 2 we obtain a regret1 o(
T log2 T )
dT
showing that KONS still improves over ﬁrst-order methods. Furthermore  if the kernel has a low
rank or the eigenvalues decrease exponentially  the ﬁnal regret is poly-logarithmic  thus preserving
the full advantage of the second-order approach. Notice that this scenario is always veriﬁed when
H = Rd  and is also veriﬁed when the adversary draws samples from a stationary distribution
and  e.g.  the Gaussian kernel [22] (see also [16  18]). This result is particularly remarkable when

compared to SKETCHED-KONS  whose regret scales as O(cid:0)α(cid:107)w(cid:107)2 + dT

eff (α) (log T )/η(cid:1)  where η

is the fraction of samples which is forced into the dictionary (when η = 1  we recover the bound
for KONS). Even when the effective dimension is small (e.g.  exponentially decaying eigenvalues) 
SKETCHED-KONS requires setting η to T −p for a constant p > 0 to get a subquadratic space
complexity  at the cost of increasing the regret to O(T p log T ). On the other hand  PROS-N-KONS
achieves a poly-logarithmic regret with linear space complexity up to poly-log factors (i.e.  T dT
eff(γ)2) 
thus greatly improving both the learning and computational performance w.r.t. SKETCHED-KONS.
Finally  notice that while γ = α/T is the best choice agnostic to the kernel  better bounds can
be obtained optimizing Eq. 3 for γ depending on dT
eff(γ). For instance  let γ = α/T s  then the
optimal value of s for q-polynomially decaying spectrum is s = q/(1 + q)  leading to a regret bound

(cid:101)O(T q/(1+q))  which is always o(

√

T ) for any q > 1.

Remark (comparison in the Euclidean case) In the special case H = Rd  we can make a compari-
son with existing approximate methods for OL. In particular  the closest algorithm is SKETCHED-
ONS by Luo et al. [14]. Unlike PROS-N-KONS  and similarly to SKETCHED-KONS  they take the

1Here we ignore the term dT

eff(α) which is a constant w.r.t. T for any constant α.

6

i=k+1 σ2

by k log T + k(cid:80)T

i   where the sum(cid:80)T

approach of directly approximating At in the exact H = Rd using frequent directions [7] to construct
a k-rank approximation of At for a ﬁxed k. The resulting algorithm achieves a regret that is bounded
i is equal to the sum of all the smallest d − k
eigenvalues of the ﬁnal (exact) matrix AT . This quantity can vary from 0  when the data lies in a
subspace of rank r ≤ k  to T d−k
d when the sample lie orthogonally and in equal number along all d
directions available in Rd. Computationally  the algorithm requires O(T dk) time and O(dk) space.
Conversely  PROS-N-KONS automatically adapt its time and space complexity to the effective
dimension of the algorithm dT
eff(α/T ) which is smaller than the rank for any α. As a consequence 

it requires only (cid:101)O(T r2) time and (cid:101)O(r2) space  achieving a O(r2 log T ) regret independently from

i=k+1 σ2

the spectrum of the covariance matrix. Computationally  all of these complexities are smaller than
the ones of SKETCHED-ONS in the regime r < k  which is the only one where SKETCHED-ONS
can guarantee a sublinear regret  and where the regrets of the two algorithms are close. Overall 
while SKETCHED-ONS implicitly relies on the r < k assumption  but continues to operate in a d
dimensional space and suffers large regret if r > k  PROS-N-KONS will adaptively convert the d
dimensional problem into a simpler one with the appropriate rank  fully reaping the computational
and regret beneﬁts.
The bound in Thm. 1 can be reﬁned in the speciﬁc case of squared loss as follows.

Theorem 2. For any sequence of squared losses (cid:96)t = (yt −(cid:98)yt)2  L = 4C and σ = 1/(8C 2)  if ηt ≥ σ
for all t  α ≤ √
(cid:18) 4
RT (w)≤ J(cid:88)
where ε(cid:48) = α(cid:0)α − γε

(cid:19)
j = minw∈H(cid:0)(cid:80)tj+1−1

T and γ ≤ α  the regret of PROS-N-KONS over T steps is bounded w.p. 1 − δ as

(cid:16)
(cid:16) α
(cid:1)−1 − 1 and L∗

(cid:18)
L(cid:0)C +
(cid:1)2
(cid:0)φT

(cid:19)
(cid:1) is the best

(cid:1)+ε(cid:48)α(cid:107)w(cid:107)2

+ α(cid:107)w(cid:107)2

t w − yt

+ε(cid:48)L∗

Tr(Kj)

L2
α

log

2σ

(cid:17)

dj
eff

σ

+J

j

(cid:17)

σL2

L
α

 

2

(5)

j=1

2

1−ε

regularized cumulative loss in H within epoch j.
Let L∗
have that dj

T be the best regularized cumulative loss over all T steps  then L∗
eff and thus regret in Eq. 5 can be (loosely) bounded as

eff ≤ dT

t=tj

T . Furthermore  we

j ≤ L∗

(cid:17)(cid:19)

.

(cid:18)

(cid:16)

RT (w) = O

J

eff(α) log(T ) + +ε(cid:48)L∗
dT

j + ε(cid:48)α(cid:107)w(cid:107)2

2

T scales as O(log T ) for a given regularization α (e.g.  in the realizable case L∗

The major difference w.r.t. the general bound in Eq. 3 is that we directly relate the regret of PROS-N-
KONS to the performance of the best predictor in H in hindsight  which replaces the linear term
γT /α. As a result  we can set γ = α (for which ε(cid:48) = ε/(1 − 2ε)) and avoid increasing the effective
dimension of the problem. Furthermore  since L∗
T is the regularized loss of the optimal batch solution 
we expect it to be small whenever the H is well designed for the prediction task at hand. For instance 
if L∗
T is actually just
α(cid:107)w(cid:107))  then the regret of PROS-N-KONS is directly comparable with KONS up to a multiplicative
factor depending on the number of epochs J and with a much smaller time and space complexity that
adapt to the effective dimension of the problem (see Prop. 1).
5 Experiments
We empirically validate PROS-N-KONS on several regression and binary classiﬁcation problems 
showing that it is competitive with state-of-the-art methods. We focused on verifying 1) the advantage
of second-order vs. ﬁrst-order updates  2) the effectiveness of data-adaptive embedding w.r.t. the
oblivious one  and 3) the effective dimension in real datasets. Note that our guarantees hold for more
challenging (possibly adversarial) settings than what we test empirically.
Algorithms Beside PROS-N-KONS  we introduce two heuristic variants. CON-KONS follows
the same update rules as PROS-N-KONS during the descent steps  but at reset steps it does not

reset the solution and instead computes (cid:101)wt−1 = Φj−1Uj−1Σ−1
(cid:101)ωt = Σ−1

j−1(cid:101)ωt−1 starting from(cid:101)ωt−1 and sets
j(cid:101)wt−1. A similar update rule is used to map (cid:101)At−1 into the new embedded space

without resetting it. B-KONS is a budgeted version of PROS-N-KONS that stops updating the
dictionary at a maximum budget Jmax and then it continues learning on the last space for the rest of
the run. Finally  we also include the best BATCH solution in the ﬁnal space HJ returned by KORS as
a best-in-hindsight comparator. We also compare to two state-of-the-art embedding-based ﬁrst-order

j UT

jΦT

7

Algorithm

FOGD
NOGD
PROS-N-KONS
CON-KONS
B-KONS
BATCH

Algorithm

FOGD
NOGD
PROS-N-KONS
CON-KONS
B-KONS
BATCH

Algorithm

FOGD
NOGD
DUAL-SGD
PROS-N-KONS
CON-KONS
B-KONS
BATCH

parkinson n = 5  875  d = 20
time
—
—
5.16
5.21
5.35
—

#SV
avg. squared loss
0.04909 ± 0.00020
30
0.04896 ± 0.00068
30
0.05798 ± 0.00136
18
0.05696 ± 0.00129
18
0.05795 ± 0.00172
18
0.04535 ± 0.00002 —

cpusmall n = 8  192  d = 12

#SV
avg. squared loss
0.02577 ± 0.00050
30
0.02559 ± 0.00024
30
0.02494 ± 0.00141
20
0.02269 ± 0.00164
20
0.02496 ± 0.00177
20
0.01090 ± 0.00082 —

time
—
—
7.28
7.40
7.37

cadata n = 20  640  d = 8

casp n = 45  730  d = 9

#SV
avg. squared loss
0.04097 ± 0.00015
30
0.03983 ± 0.00018
30
0.03095 ± 0.00110
20
0.02850 ± 0.00174
19
0.03095 ± 0.00118
19
0.02202 ± 0.00002 —

time
—
—

18.59
18.45
18.65

—

#SV
avg. squared loss
0.08021 ± 0.00031
30
0.07844 ± 0.00008
30
0.06773 ± 0.00105
21
0.06832 ± 0.00315
20
0.06775 ± 0.00067
21
0.06100 ± 0.00003 —

time
—
—

40.73
40.91
41.13

—

slice n = 53  500  d = 385

year n = 463  715  d = 90

—

avg. squared loss
0.00726 ± 0.00019
0.02636 ± 0.00460

#SV
30
30
—
did not complete —
did not complete —
0.00913 ± 0.00045
100
0.00212 ± 0.00001 —

time
—
—
—
—
—
60
—

#SV
avg. squared loss
0.01427 ± 0.00004
30
0.01427 ± 0.00004
30
0.01440 ± 0.00000
100
0.01450 ± 0.00014
149
0.01444 ± 0.00017
147
0.01302 ± 0.00006
100
0.01147 ± 0.00001 —

time
—
—
—

884.82
889.42
505.36

—

Table 1: Regression datasets

methods from [13]. NOGD selects the ﬁrst J points and uses them to construct an embedding and
then perform exact GD in the embedded space. FOGD uses random feature expansion to construct
an embedding  and then runs ﬁrst-order GD in the embedded space. While oblivious embedding
methods are cheaper than data-adaptive Nyström  they are usually less accurate. Finally  DUAL-SGD
also performs a random feature expansion embedding  but in the dual space. Given the number
#SV of SVs stored in the predictor  and the input dimension d of the dataset’s samples  the time
complexity of all ﬁrst-order methods is O(T d#SV )  while that of PROS-N-KONS and variants is
O(T (d + #SV )#SV ). When #SV ∼ d (as in our case) the two complexities coincide. The space
complexities are also close  with PROS-N-KONS O(#SV 2) not much larger than the ﬁrst order
methods’ O(#SV ). We do not run SKETCHED-KONS because the T 2 runtime is prohibitive.
Experimental setup We replicate the experimental setting in [13] with 9 datasets for regression
and 3 datasets for binary classiﬁcation. We use the same preprocessing as Lu et al. [13]: each
feature of the points xt is rescaled to ﬁt in [0  1]  for regression the target variable yt is rescaled in
[0  1]  while in binary classiﬁcation the labels are {−1  1}. We also do not tune the Gaussian kernel
bandwidth  but take the value σ = 8 used by [13]. For all datasets  we set β = 1 and ε = 0.5 for all
PROS-N-KONS variants and Jmax = 100 for B-KONS. For each algorithm and dataset  we report
average and standard deviation of the losses. The scores for the competitor baselines are reported as
provided in the original papers [13  12]. We only report scores for NOGD  FOGD  and DUAL-SGD 
since they have been shown to outperform other baselines such as budgeted perceptron [4]  projectron
[15]  forgetron [6]  and budgeted GD [23]. For PROS-N-KONS variant we also report the runtime in
seconds  but do not compare with the runtimes reported by [13  12]  as that would imply comparing
different implementations. Note that since the complexities O(T d#SV ) and O(T (d + #SV )#SV )
are close  we do not expect large differences. All experiments are run on a single machine with 2
Xeon E5-2630 CPUs for a total of 10 cores  and are averaged over 15 runs.
Effective dimension and runtime We use size of the dictionary returned by KORS as a proxy for
the effective dimension of the datasets. As expected  larger datasets and datasets with a larger input
eff(γ) increases (sublinearly) when we
dimension have a larger effective dimension. Furthermore  dT
reduce γ from 1 to 0.01 in the ijcnn1 dataset. More importantly  dT
eff(γ) remains empirically small

8

Algorithm

FOGD
NOGD
DUAL-SGD
PROS-N-KONS
CON-KONS
B-KONS
BATCH

Algorithm

FOGD
NOGD
DUAL-SGD
PROS-N-KONS
CON-KONS
B-KONS
BATCH

α = 1  γ = 1

ijcnn1 n = 141  691  d = 22
time
accuracy
9.06 ± 0.05
—
9.55 ± 0.01
—
8.35 ± 0.20
—
9.70 ± 0.01
9.64 ± 0.01
9.70 ± 0.01
8.33 ± 0.03

#SV
400
100
100
100
101
98
—

211.91
215.71
206.53

—

α = 0.01  γ = 0.01

cod-rna n = 271  617  d = 8
#SV
time
accuracy
10.30 ± 0.10
400
—
13.80 ± 2.10
100
—
4.83 ± 0.21
100
—
13.95 ± 1.19
38
18.99 ± 9.47
38
13.99 ± 1.16
38
3.781 ± 0.01 —

270.81
271.85
274.94

—

ijcnn1 n = 141  691  d = 22
#SV
accuracy
time
9.06 ± 0.05
400
—
9.55 ± 0.01
100
—
8.35 ± 0.20
100
—
10.73 ± 0.12
436
6.23 ± 0.18
432
4.85 ± 0.08
100
5.61 ± 0.01 —
Table 2: Binary classiﬁcation datasets

cod-rna n = 271  617  d = 8
#SV
accuracy
time
10.30 ± 0.10
400
—
13.80 ± 2.10
100
—
4.83 ± 0.21
100
—
4.91 ± 0.04
111
5.81 ± 1.96
111
4.57 ± 0.05
100
3.61 ± 0.01 —

1003.82
987.33
147.22

459.28
458.90
333.57

—

—

even for datasets with hundreds of thousands samples  such as year  ijcnn1 and cod-rna. On the
other hand  in the slice dataset  the effective dimension is too large for PROS-N-KONS to complete
and we only provide results for B-KONS. Overall  the proposed algorithm can process hundreds of
thousands of points in a matter of minutes and shows that it can practically scale to large datasets.
Regression All algorithms are trained and evaluated using the squared loss. Notice that whenever the
budget Jmax is not exceeded  B-KONS and PROS-N-KONS are the same algorithm and obtain the
same result. On regression datasets (Tab. 1) we set α = 1 and γ = 1  which satisﬁes the requirements
of Thm. 2. Note that we did not tune α and γ for optimal performance  as that would require
multiple runs  and violate the online setting. On smaller datasets such as parkinson and cpusmall 
where frequent restarts greatly interfere with the gradient descent  and even a small non-adaptive
embedding can capture the geometry of the data  PROS-N-KONS is outperformed by simpler
ﬁrst-order methods. As soon as T reaches the order of tens of thousands (cadata  casp)  second-order
updates and data adaptivity becomes relevant and PROS-N-KONS outperform its competitors  both
in the number of SVs and in the average loss. In this intermediate regime  CON-KONS outperforms
PROS-N-KONS and B-KONS since it is less affected by restarts. Finally  when the number of
samples raises to hundreds of thousands  the intrinsic effective dimension of the dataset starts playing
a larger role. On slice  where the effective dimension is too large to run  B-KONS still outperforms
NOGD with a comparable budget of SVs  showing the advantage of second-order updates.
Binary classiﬁcation All algorithms are trained using the hinge loss and they are evaluated using
the average online error rate. Results are reported in Tab. 2. While for regression  an arbitrary value
of γ = α = 1 is sufﬁcient to obtain good results  it fails for binary classiﬁcation. Decreasing the
two parameters to 0.01 resulted in a 3-fold increase in the number of SVs included and runtime  but
almost a 2-fold decrease in error rate  placing PROS-N-KONS and B-KONS on par or ahead of
competitors without the need of any further parameter tuning.

6 Conclusions

We presented PROS-N-KONS a novel algorithm for sketched second-order OKL that achieves
O(dT
eff log T ) regret for losses with directional curvature. Our sketching is data-adaptive and  when the
effective dimension of the dataset is constant  it achieves a constant per-step cost  unlike SKETCHED-
KONS [2]  which was previously proposed for the same setting. We empirically showed that
PROS-N-KONS is practical  performing on par or better than state-of-the-art methods on standard
benchmarks using small dictionaries on realistic data.

9

Acknowledgements The research presented was supported by French Ministry of Higher Education and
Research  Nord-Pas-de-Calais Regional Council  Inria and Univertät Potsdam associated-team north-european
project Allocate  and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and
BoB (n.ANR-16-CE23-0003).

References
[1] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical

guarantees. In Neural Information Processing Systems  2015.

[2] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Second-order kernel online convex
optimization with adaptive sketching. In International Conference on Machine Learning  2017.
[3] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Distributed sequential sampling

for kernel matrix approximation. In AISTATS  2017.

[4] Giovanni Cavallanti  Nicolo Cesa-Bianchi  and Claudio Gentile. Tracking the best hyperplane

with a simple budget perceptron. Machine Learning  69(2-3):143–167  2007.

[5] Michael B Cohen  Cameron Musco  and Jakub Pachocki. Online row sampling. International
Workshop on Approximation  Randomization  and Combinatorial Optimization APPROX  2016.
[6] Ofer Dekel  Shai Shalev-Shwartz  and Yoram Singer. The forgetron: A kernel-based perceptron

on a budget. SIAM Journal on Computing  37(5):1342–1372  2008.

[7] Mina Ghashami  Edo Liberty  Jeff M Phillips  and David P Woodruff. Frequent directions:
Simple and deterministic matrix sketching. SIAM Journal on Computing  45(5):1762–1792 
2016.

[8] Elad Hazan  Adam Kalai  Satyen Kale  and Amit Agarwal. Logarithmic regret algorithms for

online convex optimization. In Conference on Learning Theory. Springer  2006.

[9] Wenwu He and James T. Kwok. Simple randomized algorithms for online learning with kernels.

Neural Networks  60:17–24  2014.

[10] J. Kivinen  A.J. Smola  and R.C. Williamson. Online learning with kernels. IEEE Transactions

on Signal Processing  52(8)  2004.

[11] Quoc Le  Tamás Sarlós  and Alex J Smola. Fastfood - Approximating kernel expansions in

loglinear time. In International Conference on Machine Learning  2013.

[12] Trung Le  Tu Nguyen  Vu Nguyen  and Dinh Phung. Dual Space Gradient Descent for Online

Learning. In Neural Information Processing Systems  2016.

[13] Jing Lu  Steven C.H. Hoi  Jialei Wang  Peilin Zhao  and Zhi-Yong Liu. Large scale online

kernel learning. Journal of Machine Learning Research  17(47):1–43  2016.

[14] Haipeng Luo  Alekh Agarwal  Nicolo Cesa-Bianchi  and John Langford. Efﬁcient second-order

online learning via sketching. In Neural Information Processing Systems  2016.

[15] Francesco Orabona  Joseph Keshet  and Barbara Caputo. The projectron: a bounded kernel-

based perceptron. In International conference on Machine learning  2008.

[16] Yi Sun  Jürgen Schmidhuber  and Faustino J. Gomez. On the size of the online kernel sparsiﬁca-

tion dictionary. In International Conference on Machine Learning  2012.

[17] Zhuang Wang  Koby Crammer  and Slobodan Vucetic. Breaking the curse of kernelization:
Budgeted stochastic gradient descent for large-scale svm training. Journal of Machine Learning
Research  13(Oct):3103–3131  2012.

[18] Andrew J. Wathen and Shengxin Zhu. On spectral distribution of kernel matrices related to

radial basis functions. Numerical Algorithms  70(4):709–726  2015.

[19] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel

machines. In Neural Information Processing Systems  2001.

[20] Yi Xu  Haiqin Yang  Lijun Zhang  and Tianbao Yang. Efﬁcient non-oblivious randomized
reduction for risk minimization with improved excess risk guarantee. In AAAI Conference on
Artiﬁcial Intelligence  2017.

[21] Tianbao Yang  Yu-Feng Li  Mehrdad Mahdavi  Rong Jin  and Zhi-Hua Zhou. Nyström method
vs random fourier features: A theoretical and empirical comparison. In Neural Information
Processing Systems  2012.

10

[22] Y. Yang  M. Pilanci  and M. J. Wainwright. Randomized sketches for kernels: Fast and optimal

non-parametric regression. Annals of Statistics  2017.

[23] Peilin Zhao  Jialei Wang  Pengcheng Wu  Rong Jin  and Steven C H Hoi. Fast bounded
online gradient descent algorithms for scalable kernel-based online learning. In International
Conference on Machine Learning  2012.

[24] Fedor Zhdanov and Yuri Kalnishkan. An identity for kernel ridge regression. In Algorithmic

Learning Theory. 2010.

[25] Changbo Zhu and Huan Xu. Online gradient descent in function space. arXiv:1512.02394 

2015.

[26] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In International Conference on Machine Learning  2003.

11

,Daniele Calandriello
Alessandro Lazaric
Michal Valko
Minjia Zhang
Wenhan Wang
Xiaodong Liu
Jianfeng Gao
Yuxiong He