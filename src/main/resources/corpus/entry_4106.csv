2014,Optimal Neural Codes for Control and Estimation,Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory  one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed  biological systems often gain from the extra flexibility of optimizing the sensors themselves. However  this sensory adaptation is geared towards control rather than perception  as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception  even for simple control setups. This implies  consistently with recent experimental results  that when studying sensory adaptation  it is essential to account for the task being performed.,Optimal Neural Codes for Control and Estimation

Alex Susemihl1  Manfred Opper
Methods of Artiﬁcial Intelligence

Technische Universit¨at Berlin
1 Current afﬁliation: Google

Ron Meir

Department of Electrical Engineering

Technion - Haifa

Abstract

Agents acting in the natural world aim at selecting appropriate actions based on
noisy and partial sensory observations. Many behaviors leading to decision mak-
ing and action selection in a closed loop setting are naturally phrased within a
control theoretic framework. Within the framework of optimal Control Theory 
one is usually given a cost function which is minimized by selecting a control
law based on the observations. While in standard control settings the sensors are
assumed ﬁxed  biological systems often gain from the extra ﬂexibility of optimiz-
ing the sensors themselves. However  this sensory adaptation is geared towards
control rather than perception  as is often assumed. In this work we show that sen-
sory adaptation for control differs from sensory adaptation for perception  even for
simple control setups. This implies  consistently with recent experimental results 
that when studying sensory adaptation  it is essential to account for the task being
performed.

1

Introduction

Biological systems face the difﬁcult task of devising effective control strategies based on partial in-
formation communicated between sensors and actuators across multiple distributed networks. While
the theory of Optimal Control (OC) has become widely used as a framework for studying motor con-
trol  the standard framework of OC neglects many essential attributes of biological control [1  2  3].
The classic formulation of closed loop OC considers a dynamical system (plant) observed through
sensors which transmit their output to a controller  which in turn selects a control law that drives
actuators to steer the plant. This standard view  however  ignores the fact that sensors  controllers
and actuators are often distributed across multiple sub-systems  and disregards the communication
channels between these sub-systems. While the importance of jointly considering control and com-
munication within a uniﬁed framework was already clear to the pioneers of the ﬁeld of Cybernetics
(e.g.  Wiener and Ashby)  it is only in recent years that increasing effort is being devoted to the
formulation of a rigorous systems-theoretic framework for control and communication (e.g.  [4]).
Since the ultimate objective of an agent is to select appropriate actions  it is clear that sensation and
communication must subserve effective control  and should be gauged by their contribution to action
selection. In fact  given the communication constraints that plague biological systems (and many
current distributed systems  e.g.  cellular networks  sensor arrays  power grids  etc.)  a major concern
of a control design is the optimization of sensory information gathering and communication (consis-
tently with theories of active perception). For example  recent theoretical work demonstrated a sharp
communication bandwidth threshold below which control (or even stabilization) cannot be achieved
(for a summary of such results see [4]). Moreover  when informational constraints exists within a
control setting  even simple (linear and Gaussian) problems become nonlinear and intractable  as
exempliﬁed in the famous Witsenhausen counter-example [5].
The inter-dependence between sensation  communication and control is often overlooked both in
control theory and in computational neuroscience  where one assumes that the overall solution to
the control problem consists of ﬁrst estimating the state of the controlled system (without reference

1

to the control task)  followed by constructing a controller based on the estimated state. This idea 
referred to as the separation principle in Control Theory  while optimal in certain restricted settings
(e.g.  Linear Quadratic Gaussian (LQG) control) is  in general  sub-optimal [6]. Unfortunately  it is
in general very difﬁcult to provide optimal solutions in cases where separation fails. A special case
of the separation principle  referred to as Certainty Equivalence (CE)  occurs when the controller
treats the estimated state as the true state  and forms a controller assuming full state information. It
is generally overlooked  however  that although the optimal control policy does not depend directly
on the observation model at hand  the expected future costs do depend on the speciﬁcs of that model
[7]. In this sense  even when CE holds  costs still arise from uncertain estimates of the state and one
can optimise the sensory observation model to minimise these costs  leading to sensory adaptation.
At ﬁrst glance  it might seem that the observation model that will minimise the expected future cost
will be the observation model that minimises the estimation error. We will show  however  that this
is not generally the case.
A great deal of the work in computational neuroscience has dealt independently with the problem
of sensory adaptation and control  while  as stated above  these two issues are part and parcel of
the same problem. In fact  it is becoming increasingly clear that biological sensory adaptation is
task-dependent [8  9]. For example  [9] demonstrates that task-dependent sensory adaptation takes
place in purely motor tasks  explaining after-effect phenomena seen in experiments. In [10]  the
authors show that speciﬁc changes occur in sensory regions  implying sensory plasticity in motor
learning. In this work we consider a simple setting for control based on spike time sensory coding 
and study the optimal coding of sensory information required in order to perform a well-deﬁned
motor task. We show that even if CE holds  the optimal encoder strategy  minimising the control cost 
differs from the optimal encoder required for state estimation. This result demonstrates  consistently
with experiments  that neural encoding must be tailored to the task at hand. In other words  when
analyzing sensory neural data  one must pay careful care to the task being performed. Interestingly 
work within the distributed control community dealing with optimal assignment and selection of
sensors  leads to similar conclusions and to speciﬁc schemes for sensory adaptation.
The interplay between information theory and optimal control is a central pillar of modern control
theory  and we believe it must be accounted for in the computational neuroscience community.
Though statistical estimation theory has become central in neural coding issues  often through the
Cram´er-Rao bound  there have been few studies bridging the gap between partially observed control
and neural coding. We hope to narrow this gap by presenting a simple example where control
and estimation yield different conclusions. The remainder of the paper is organised as follows:
In section 1.1 we introduce the notation and concepts; In section 2 we derive expressions for the
cost-to-go of a linear-quadratic control system observed through spikes from a dense populations of
neurons; in section 3 we present the results and compare optimal codes for control and estimation
with point-process ﬁltering  Kalman ﬁltering and LQG control; in section 4 we discuss the results
and their implications.

1.1 Optimal Codes for Estimation and Control

We will deal throughout this paper with a dynamic system with state Xt  observed through noisy
sensory observations Zt  whose conditional distribution can be parametrised by a set of parameters
ϕ  e.g.  the widths and locations of the tuning curves of a population of neurons or the noise prop-
erties of the observation process. The conditional distribution is then given by Pϕ(Zt|Xt = x).
Zt could stand for a diffusion process dependent on Xt (denoted Yt) or a set of doubly-stochastic
Poisson processes dependent on Xt (denoted N m
t ). In that sense  the optimal Bayesian encoder for
an estimation problem  based on the Mean Squared Error (MSE) criterion  can be written as

(cid:20)

(cid:20)(cid:16)

ϕ∗
e = argmin

ϕ

Ez

EXt

Xt − ˆXt(Zt)

(cid:21)(cid:21)

(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12)Zt = z

 

where ˆXt(Zt) = E [Xt|Zt] is the posterior mean  computable  in the linear Gaussian case  by the
Kalman ﬁlter. We will throughout this paper consider the MMSE in the equilibrium  that is  the
error in estimating Xt from long sequences of observations Z[0 t]. Similarly  considering a control
problem with a cost given by

C(X 0  U 0) =

c(Xs  Us  s)ds + cT (XT ) 

(cid:90) T

0

2

where X t = {Xs|s ∈ [t  T ]}  U t = {Us|s ∈ [t  T ]}  and so forth. We can deﬁne

ϕ∗
c = argmin

ϕ

Ez min
U t

[EX t [C(X 0  U 0)|Zt = z]] .

The certainty equivalence principle states that given a control policy γ∗ : X → U which minimises
the cost C 

γ∗ = argmin

γ

C(X 0  γ(X 0)) 

the optimal control policy for the partially observed problem given by noisy observations Z0 of X 0
is given by
Note that we have used the notation γ(X 0) = {γ(Xs)  s ∈ [0  T ]}.

γCE(Zt) = γ∗ (E [X 0|Zt]) .

2 Stochastic Optimal Control

In stochastic optimal control we seek to minimize the expected future cost incurred by a system
with respect to a control variable applied to that system. We will consider linear stochastic systems
governed by the SDE

dXt = (AXt + BUt) dt + D1/2dWt 

(cid:90) T

t

(cid:0)X(cid:62)

(cid:1) ds + X(cid:62)

T QT XT .

(1a)

(1b)

with a cost function C(X t  U t  t) =

s QXs + U(cid:62)

s RUs

From Bellman’s optimality principle or variational analysis [11]  it is well known that the optimal
t = −R−1B(cid:62)StXt  where St is the solution of the Riccati equation − ˙St =
control is given by U∗
Q + ASt + StA(cid:62) − StB(cid:62)R−1BSt  with boundary condition ST = QT . The expected future cost
at time t and state x under the optimal control is then given by

J(x  t) = min
U t

E [C(X t  U t  t)|Xt = x] =

x(cid:62)Stx +

1
2

Tr (DSs) ds.

(cid:90) T

t

t = −R−1B(cid:62)StE [Xt|Yt] [7].

This is usually called the optimal cost-to-go. However  the system’s state is not always directly
accessible and we are often left with noisy observations of it. For a class of systems e.g. LQG
control  CE holds and the optimal control policy for the indirectly observed control problem is
simply the optimal control policy for the original control problem applied to the Bayesian estimate of
the system’s state. In that sense  if the CE were to hold for the system above observed through noisy
observations Yt of the state at time t  the optimal control would be given simply by the observation-
dependent control U∗
Though CE  when applicable  gives us a simple way to determine the optimal control  when con-
sidering neural systems we are often interested in ﬁnding the optimal encoder  or the optimal ob-
servation model for a given system. That is equivalent to ﬁnding the optimal tuning function for a
given neuron model. Since CE neatly separates the estimation and control steps  it would be tempt-
ing to assume the optimal codes obtained for an estimation problem would also be optimal for an
associated control problem. We will show here that this is not the case.
As an illustration  let us consider the case of LQG with incomplete state information. One could 
for example  take the observations to be a secondary process Yt  which itself is a solution to

the optimal cost-to-go would then be given by [11]

E(cid:2)C(X t  U t  t)(cid:12)(cid:12)Y[0 t] = y(cid:3)

J(y  t) = min
U t

dYt = F Xtdt + G1/2dVt 

=ν(cid:62)

t Stνt + Tr (KtSt) +

Tr (DSs) ds +

t

(cid:90) T

Tr(cid:0)SsBR−1B(cid:62)SsKs

(cid:1) ds 

where we have deﬁned Y[0 t] = {Ys  s ∈ [0  t]}  νt = E[Xt|Y[0 t]] and Kt = cov[Xt|Y[0 t]]. We
give a demonstration of these results in the SI  but for a thorough review see [11]. Note that through

(2)

(3)

(cid:90) T

t

3

the last term in equation (3) the cost-to-go now depends on the parameters of the Yt process. More
precisely  the variance of the distribution of Xs given Yt  for s > t obeys the ODE

˙Kt = AKt + KtA(cid:62) + D − KtF (cid:62)G−1F Kt.

(4)
One could then choose the matrices F and G in such a way as to minimise the contribution of
the rightmost term in equation (3). Note that in the LQG case this is not particularly interesting 
as the conclusion is simply that we should strive to make Kt as small as possible  by making the
term F (cid:62)G−1F as large as possible. This translates to choosing an observation process with very
strong steering from the unobserved process (large F ) and a very small noise (small G). One case
that provides some more interesting situations is if we consider a two-dimensional system  where
we are restricted to a noise covariance with constant determinant. That means the hypervolume
spanned by the eigenvectors of the covariance matrix is constant. We will compare this case with
the Poisson-coded case below.

2.1 LQG Control with Dense Gauss-Poisson Codes

Let us now consider the case of the system given by equation (1a)  but instead of observing the
system directly we observe a set of doubly-stochastic Poisson processes {N m
t } with rates given by
(5)

(cid:104)− (x − θm)

P † (x − θm) /2

λm(x) = φ exp

(cid:105)

(cid:62)

.

ˆλ =(cid:80)

is a counting process which counts how many spikes the neuron m
To clarify  the process N m
t
t will give the
has ﬁred up to time t. In that sense  the differential of the counting process dN m
spike train process  a sum of Dirac delta functions placed at the times of spikes ﬁred by neuron
m. Here P † denotes the pseudo-inverse of P   which is used to allow for tuning functions that
do not depend on certain coordinates of the stimulus x. Furthermore  we will assume that the
tuning centre θm are such that the probability of observing a spike of any neuron at a given time
m λm(x) is independent of the speciﬁc value of the world state x. This can be a consequence
of either a dense packing of the tuning centres θm along a given dimension of x  or of an absolute
insensitivity to that aspect of x through a null element in the diagonal of P †. This is often called
the dense coding hypothesis [12]. It can be readily be shown that the ﬁltering distribution is given
by P (Xt|{N[0 t)}) = N (µt  Σt)  where the mean and covariance are solutions to the stochastic
differential equations (see [13])

(cid:1)−1
(6b)
[0 t]}]. Note that we have also
where we have deﬁned µt = E[Xt|{N m
deﬁned N m
t .
m N m
Using Lemma 7.1 from [11] provides a simple connection between the cost function and the solution
of the associated Ricatti equation for a stochastic process. We have

(cid:1)−1
(cid:0)I + P †Σt
dΣt =(cid:0)AΣt + ΣtA(cid:62) + D(cid:1) dt − ΣtP †Σt
(cid:0)I + P †Σt
s up to time t  and Nt =(cid:80)
(cid:3) ds

[0 t]}] and Σt = cov[Xt|{N m
(cid:90) T
(cid:90) T

s |s ∈ [0  t]}  the history of the process N m

P † (θm − µt) dN m
t  

C(X t  U t  t) = X(cid:62)

dµt = (Aµt + BUt) dt +

[0 t] = {N m

T QT XT +

(cid:88)

s RUs

dNt 

(6a)

Σt

m

t

=X(cid:62)

t StXt +

(Us + R−1B(cid:62)SsXs)(cid:62)R(Us + R−1B(cid:62)SsXs)ds

(cid:2)X(cid:62)
s QXs + U(cid:62)
(cid:90) T

(cid:90) T

t

(cid:90) T
(cid:34)(cid:90) T

t

t

+

dW (cid:62)

Tr(DSs)ds +

s D(cid:62)/2SsXsds +

X(cid:62)
s SsD1/2dWs.
We can average over P (X t  N t|{N[0 t)}) to obtain the expected future cost. That gives us
µ(cid:62)
t Stµt+Tr(ΣtSt)+E
We can evaluate the average over P (X t {N m
(cid:20)(cid:90) T
Gaussian densities P (Xs|{N m
[0 s]}) and then over P ({N[0 s]}|{N[0 t)}). The average gives
(cid:62)

(cid:90) T
[0 t)}) in two steps  by ﬁrst averaging over the
(cid:21)
(cid:104)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12){N[0 t)}
(cid:35)
(cid:105)

(Us + R−1B(cid:62)SsXs)(cid:62)R(Us + R−1B(cid:62)SsXs)ds

t }|{N m

+

(cid:62)

(cid:62)

(cid:62)

t

t

t

Ssµs)

R(Us + R

Ssµs) + Tr

SsBR

−1B

SsΣs({N[0 s]})

ds

E

(Us + R

−1B

−1B

(cid:12)(cid:12)(cid:12)(cid:12){N[0 t)}

 

Tr(DSs)ds

t

4

where µs and Σs are the mean and variance associated with the distribution P (Xs|{N[0 s)}). Note
that choosing Us = −R−1B(cid:62)Ssµs will minimise the expression above  consistently with CE. The
optimal cost-to-go is therefore given by

J({N[0 t)}  t) =µ(cid:62)

t Stµt + Tr(ΣtSt)

(cid:90) T

+

Tr (DSs) ds +

t

t

(cid:90) T

Tr(cid:0)SsBR−1B(cid:62)SsE(cid:2)Σs({N[0 s]})|{N[0 t)}(cid:3)(cid:1) ds

(7)

Note that the only term in the cost-to-go function that depends on the parameters of the encoders is
the rightmost term and it depends on it only through the average over future paths of the ﬁltering
variance Σs. The average of the future covariance matrix is precisely the MMSE for the ﬁltering
problem conditioned on the belief state at time t [13]. We can therefore analyse the quality of an
encoder for a control task by looking at the values of the term on the right for different encoding
parameters. Furthermore  since the dynamics of Σt given by equation (6b) is Markovian  we can

write the average E(cid:2)Σs|{N[0 t)}(cid:3) as E [Σs|Σt]. We will deﬁne then the function f (Σ  t) which

gives us the uncertainty-related expected future cost for the control problem as

(cid:90) T

Tr(cid:0)SsBR−1B(cid:62)SsE [Σs|Σt = Σ](cid:1) ds.

(8)

f (Σ  t) =

2.2 Mutual Information

t

Many results in information theory are formulated in terms of the mutual information of the com-
munication channel Pϕ(Y |X). For example  the maximum cost reduction achievable with R bits of
information about an unobserved variable X has been shown to be a function of the rate-distortion
function with the cost as the distortion function [14]. More recently there has also been a lot of
interest in the so-called I-MMSE relations  which provide connections between the mutual infor-
mation of a channel and the minimal mean squared error of the Bayes estimator derived from the
same channel [15  16]. The mutual information for the cases we are considering is not particularly
complex  as all distributions are Gaussians. Let us denote by Σ0
t the covariance of of the unobserved
process Xt conditioned on some initial Gaussian distribution P0 = N (µ0  Σ0) at time 0. We can
then consider the Mutual Information between the stimulus at time t  Xt  and the observations up to
time t  Y[0 t] or N[0 t]. For the LQG/Kalman case we have simply

I(Xt; Y[0 t]|P0) =

dx dyP (x  y) [log P (x|y) − log P (x)] = log |Σ0

t| − log |Σt| 

(cid:90)

where Σt is a solution of equation (4). For the Dense Gauss-Poisson code  we can also denoting the
solution to the stochastic differential equation (6b) for the given value N[0 t] by Σt(N[0 t]) we have.
I(Xt; Nt|P0) =

dx dn P (x  n) [log P (x|n) − log P (x)] = log |Σ0

(cid:2)log |Σt(N[0 t])|(cid:3) .

t| − EN[0 t]

(cid:90)

3 Optimal Neural Codes for Estimation and Control

What could be the reasons for an optimal code for an estimation problem to be sub-optimal for a
control problem? We present examples that show two possible reasons for different optimal coding
strategies in estimation and control. First  one should note that control problems are often deﬁned
over a ﬁnite time horizon. One set of classical experiments involves reaching for a target under
time constraints [3].
If we take the maximal ﬁring rate of the neurons (φ) to be constant while
varying the width of the tuning functions  this will lead the number of observed spikes to be inversely
proportional to the precision of those spikes  forcing a trade-off between the number of observations
and their quality. This trade-off can be tilted to either side in the case of control depending on the
information available at the start of the problem. If we are given complete information on the system
state at the initial time 0  the encoder needs fewer spikes to reliably estimate the system’s state
throughout the duration of the control experiment  and the optimal encoder will be tilted towards
a lower number of spikes with higher precision. Conversely  if at the beginning of the experiment
we have very little information about the system’s state  reﬂected in a very broad distribution  the
encoder will be forced towards lower precision spikes with higher frequency. These results are
discussed in section 3.1.

5

Secondly  one should note that the optimal encoder for estimation does not take into account the
differential weighting of different dimensions of the system’s state. When considering a multidi-
mensional estimation problem  the optimal encoder will generally allocate all its resources equally
between the dimensions of the system’s state. In the framework presented we can think of the dimen-
sions as the singular vectors of the tuning matrix P and the resources allocated to it are the singular
values. In this sense  we will consider a set of coding strategies deﬁned by matrices P of constant
determinant in section 3.2. This constrains the overall ﬁring rate of the population of neurons to be
constant  and we can then consider how the population will best allocate its observations between
these dimensions. Clearly  if we have an anisotropic control problem  which places a higher impor-
tance in controlling one dimension  the optimal encoder for the control problem will be expected to
allocate more resources to that dimension. This is indeed shown to be the case for the Poisson codes
considered  as well as for a simple LQG problem when we constrain the noise covariance to have
the same structure.
We do not mean our analysis to be exhaustive as to the factors leading to different optimal codes
in estimation and control settings  as the general problem is intractable  and indeed  is not even
separable. We intend this to be a proof of concept showing two cases in which the analogy between
control and estimation breaks down.

3.1 The Trade-off Between Precision and Frequency of Observations

√

In this section we consider populations of neurons with tuning functions as given by equation (5)
with tuning centers θm distributed along a one- dimensional line.
In the case of the Ornstein-
Uhlenbeck process these will be simply one-dimensional values θm whereas in the case of the
stochastic oscillator  we will consider tuning centres of the form θm = (ηm  0)(cid:62)  ﬁlling only the
ﬁrst dimension of the stimulus space. Note that in both cases the (dense) population ﬁring rate
2πpφ/|∆θ|  where ∆θ is the separation between neigh-

ˆλ = (cid:80)

m λm(x) will be given by ˆλ =

bouring tuning centres θm.
The Ornstein-Uhlenbeck (OU) process controlled by a process Ut is given by the SDE dXt =
(bUt − γXt)dt + D1/2dWt. Equation (7) can then be solved by simulating the dynamics of Σs.
This has been considered extensively in [13] and we refer to the results therein. Speciﬁcally  it has
been found that the dynamics of the average can be approximated in a mean-ﬁeld approach yielding
surprisingly good results. The evolution of the average posterior variance is given by the average
of equation (6b)  which involves nonlinear averages over the covariances. These are intractable 
but a simple mean-ﬁeld approach yields the approximate equation for the evolution of the average
(cid:104)Σs(cid:105) = E [Σs|Σ0]
d(cid:104)Σs(cid:105)
ds

A(cid:62) + D − ˆλ(cid:104)Σs(cid:105) P † (cid:104)Σs(cid:105)(cid:0)I + P † (cid:104)Σs(cid:105)(cid:1)−1

= A(cid:104)Σs(cid:105) + (cid:104)Σs(cid:105)(cid:62)

.

The alternative is to simulate the stochastic dynamics of Σt for a large number of samples and
compute numerical averages. These results can be directly employed to evaluate the optimal cost-
to-go in the control problem f (Σ  t).
Alternatively  we can look at a system with more complex dynamics  and we take as an example the
stochastic damped harmonic oscillator given by the system of equations

dVt =(cid:0)bUt − γVt − ω2Xt

(cid:1) dt + η1/2dWt.

˙Xt = Vt 

(9)
Furthermore  we assume that the tuning functions only depend on the position of the oscillator 
therefore not giving us any information about the velocity. The controller in turn seeks to keep the
oscillator close to the origin while steering only the velocity. This can be achieved by the choice of
matrices A = (0  1;−ω2 −γ)  B = (0  0; 0  b)  D = (0  0; 0  η2)  R = (0  0; 0  r)  Q = (q  0; 0  0)
and P = (p2  0; 0  0).
In ﬁgure 1 we provide the uncertainty-dependent costs for LQG control  for the Poisson observed
control  as well as the MMSE for the Poisson ﬁltering problem and for a Kalman-Bucy ﬁlter with the
same noise covariance matrix P . More precisely  we are considering a Kalman-Bucy ﬁlter for the
same dynamic system  but observed through a difusion process as in equation (2) with F = I and
G = P . This illustrates nicely the difference between Kalman ﬁltering and the Gauss-Poisson ﬁl-
tering considered here. The Kalman ﬁlter MSE has a simple  monotonically increasing dependence

6

Figure 1: The trade-off between the precision and the frequency of spikes is illustrated for the OU process
(left) and the stochastic oscillator (right). In both ﬁgures  the initial condition has a very uncertain estimate of
the system’s state  biasing the optimal tuning width towards higher values. This forces the encoder to amass the
maximum number of observations within the duration of the control experiment. Parameters for left ﬁgure were:
T = 2  γ = 1.0  η = 0.6  b = 0.2  φ = 0.1  ∆θ = 0.05  Q = 0.1  QT = 0.001  R = 0.1. Parameters for
right ﬁgure were T = 5  γ = 0.4  ω = 0.8  η = 0.4  r = 0.4  q = 0.4  QT = 0  φ = 0.5  ∆θ = 0.1.

on the noise covariance  and one should simply strive to design sensors with the highest possible
precision (p = 0) to minimise the MMSE and control costs. The Poisson case leads to optimal
performance at a non-zero value of p. Importantly the optimal values of p for estimation and control
differ. Furthermore  in view of section 2.2  we also plotted the mutual information between the pro-
cess Xt and the observation process Nt  to illustrate that information-based arguments would lead
to the same optimal encoder as MMSE-based arguments.

3.2 Allocating Observation Resources in Anisotropic Control Problems

A second factor that could lead to different optimal encoders in estimation and control is the struc-
ture of the cost function C. Speciﬁcally  if the cost functions depends more strongly on a certain
coordinate of the system’s state  uncertainty in that particular coordinate will have a higher impact
on expected future costs than uncertainty in other coordinates. We will here consider two simple
linear control systems observed by a population of neurons restricted to a certain ﬁring rate. This
can be thought of as a metabolic constraint  since the regeneration of membrane potential necessary
for action potential generation is one of the most signiﬁcant metabolic expenditures for neurons
[17]. This will lead to a trade-off  where an increase in precision in one coordinate will result in a
decrease in precision in the other coordinate.
We consider a population of neurons whose tuning functions cover a two-dimensional space. Taking
a two-dimensional isotropic OU system with state Xt = (X1 t  X2 t)(cid:62) where both dimensions are
2 )(cid:62) densely covering
uncoupled  we can consider a population with tuning centres θm = (ηm
the stimulus space. To consider a smoother class of stochastic systems we will also consider a
two-dimensional stochastic oscillator with state Xt = (X1 t  V1 t  X2 t  V2 t)(cid:62)  where again  both
2   0)(cid:62)  covering
dimensions are uncoupled  and the tuning centres of the form θm = (ηm
densely the position space  but not the velocity space.
Since we are interested in the case of limited resources  we will restrict ourselves to popula-
tions with a tuning matrix P yielding a constant population ﬁring rate. We can parametrise
these simply as POU (ζ) = p2 Diag(tan(ζ)  cotan(ζ))  for the OU case and POsc(ζ) =
p2 Diag(tan(ζ)  0  cotan(ζ)  0) for the stochastic oscillator  where ζ ∈ (0  π/2). Note that this
will yield the ﬁring rate ˆλ = 2πpφ/(∆θ)2  independent of the speciﬁcs of the matrix P .
We can then compare the performance of all observers with the same ﬁring rate in both control and
estimation tasks. As mentioned  we are interested in control problems where the cost functions are
anisotropic  that is  one dimension of the system’s state vector contributes more heavily to the cost
function. To study this case we consider cost functions of the type c(Xt  Ut) = Q1X 2
2 t +

1 t + Q2X 2

1   ηm

1   0  ηm

7

0.020.040.060.080.100.120.140.160.18MMSE012345p0.0000.0010.0020.0030.0040.0050.0060.007f(0 0)0.050.100.150.200.250.300.350.40MMSE0.00.20.40.60.81.01.2p0.951.001.051.101.151.201.251.301.35f(0 t0)Figure 2: The differential allocation of resources in control and estimation for the OU process (left) and the
stochastic oscillator (right). Even though the estimation MMSE leads to a symmetric optimal encoder both in
the Poisson and in the Kalman ﬁltering problem  the optimal encoders for the control problem are asymmetric 
allocating more resources to the ﬁrst coordinate of the stimulus.

1 t +R2U 2

2 t. This again  can be readily cast into the formalism introduced above  with a suitable
R1U 2
choice of matrices Q and R for both the OU process as for the stochastic oscillator. We will also
consider the case where the ﬁrst dimension of Xt contributes more strongly to the state costs (i.e. 
Q1 > Q2).
The ﬁltering error can be obtained from the formalism developed in [13] in the case of Poisson
observations and directly from the Kalman-Bucy equations in the case of Kalman ﬁltering [18]. For
LQG control  one can simply solve the control problem for the system mentioned using the standard
methods (see e.g. [11]). The Poisson-coded version of the control problem can be solved using
either direct simulation of the dynamics of Σs or by a mean-ﬁeld approach which has been shown
to yield excellent results for the system at hand. These results are summarised in ﬁgure 2  with
similar notation to that in ﬁgure 1. Note the extreme example of the stochastic oscillator  where the
optimal encoder is concentrating all the resources in one dimension  essentially ignoring the second
dimension.

4 Conclusion and Discussion

We have here shown that the optimal encoding strategies for a partially observed control problem is
not the same as the optimal encoding strategy for the associated state estimation problem. Note that
this is a natural consequence of considering noise covariances with a constant determinant in the
case of Kalman ﬁltering and LQG control  but it is by no means trivial in the case of Poisson-coded
processes. For a class of stochastic processes for which the certainty equivalence principle holds we
have provided an exact expression for the optimal cost-to-go and have shown that minimising this
cost provides us with an encoder that in fact minimises the incurred cost in the control problem.
Optimality arguments are central to many parts of computational neuroscience  but it seems that
partial observability and the importance of combining adaptive state estimation and control have
rarely been considered in this literature  although supported by recent experiments. We believe the
present work  while treating only a small subset of the formalisms used in neuroscience  provides a
ﬁrst insight into the differences between estimation and control. Much emphasis has been placed on
tracing the parallels between the two (see [19  20]  for example)  but one must not forget to take into
account the differences as well.

5 Acknowledgements

The research of A.S. was supported by the DFG research training group Sensory Computation in
Neural Systems GRK1589-1. The research of R.M. was partially funded by the Technion V.P.R.
fund and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).
M.O. would like to thank for the support by EU grant FP7-ICT-270327 (ComPLACS).

8

0.60.650.70.750.80.850.9MMSE0.00.20.40.60.81.01.21.41.60.2550.260.2650.270.2750.280.2850.290.2950.3f(0 t0)Poisson MMSEKalman MMSEMean Field fStochastic fLQG f0.20.40.60.81.01.21.4MMSE0.00.20.40.60.81.01.21.41.60.060.070.080.090.100.110.120.130.140.15f(0 t0)PoissonMMSEKalmanMMSEMeanFieldfStochasticfLQGfReferences
[1] Jun Izawa and Reza Shadmehr. On-line processing of uncertain information in visuomotor control. The
Journal of neuroscience : the ofﬁcial journal of the Society for Neuroscience  28(44):11360–8  October
2008.

[2] Emanuel Todorov and Michael I Jordan. Optimal feedback control as a theory of motor coordination.

Nature neuroscience  5(11):1226–35  November 2002.

[3] Peter W Battaglia and Paul R Schrater. Humans trade off viewing time and movement duration to improve
visuomotor accuracy in a fast reaching task. The Journal of neuroscience : the ofﬁcial journal of the
Society for Neuroscience  27(26):6984–94  June 2007.

[4] Boris Rostislavovich Andrievsky  Aleksei Seraﬁmovich Matveev  and Aleksandr L’vovich Fradkov. Con-
trol and estimation under information constraints: Toward a uniﬁed theory of control  computation and
communications. Automation and Remote Control  71(4):572–633  2010.

[5] Hans S Witsenhausen. A counterexample in stochastic optimum control. SIAM Journal on Control 

6(1):131–147  1968.

[6] Edison Tse and Yaakov Bar-Shalom. An actively adaptive control for linear systems with random param-

eters via the dual control approach. Automatic Control  IEEE Transactions on  18(2):109–117  1973.

[7] Yaakov Bar-Shalom and Edison Tse. Dual Effect  Certainty Equivalence  and Separation in Stochastic

Control. IEEE Transactions on Automatic Control  (5)  1974.

[8] D. Huber  D. A. Gutnisky  S. Peron  D. H. O’Connor  J. S. Wiegert  L. Tian  T. G. Oertner  L. L. Looger 
and K. Svoboda. Multiple dynamic representations in the motor cortex during sensorimotor learning.
Nature  484(7395):473–478  Apr 2012. n2123 (unprinted).

[9] AA Mattar  Mohammad Darainy  David J Ostry  et al. Motor learning and its sensory effects: time course
of perceptual change and its presence with gradual introduction of load. J Neurophysiol  109(3):782–791 
2013.

[10] S. Vahdat  M. Darainy  T.E. Milner  and D.J. Ostry. Functionally speciﬁc changes in resting-state senso-

rimotor networks after motor learning. J Neurosci  31(47):16907–16915  2011.

[11] Karl J. ˚Astr¨om. Introdution to Stochastic Control Theory. Courier Dover Publications  Mineola  NY  1st

edition  2006.

[12] Steve Yaeli and Ron Meir. Error-based analysis of optimal tuning functions explains phenomena observed

in sensory neurons. Frontiers in computational neuroscience  4(October):16  2010.

[13] Alex Susemihl  Ron Meir  and Manfred Opper. Dynamic state estimation based on Poisson spike trains
- towards a theory of optimal encoding. Journal of Statistical Mechanics: Theory and Experiment 
2013(03):P03009  March 2013.

[14] Fumio Kanaya and Kenji Nakagawa. On the practical implication of mutual information for statistical

decisionmaking. IEEE transactions on information theory  37(4):1151–1156  1991.

[15] N Merhav. Optimum estimation via gradients of partition functions and information measures: a
statistical-mechanical perspective. Information Theory  IEEE Transactions on  57(6):3887–3898  2011.
[16] Dongning Guo  Shlomo Shamai  and Sergio Verd´u. Mutual information and minimum mean-square error

in gaussian channels. Information Theory  IEEE Transactions on  51(4):1261–1282  2005.

[17] David Attwell and Simon B Laughlin. An energy budget for signaling in the grey matter of the brain.

Journal of Cerebral Blood Flow & Metabolism  21(10):1133–1145  2001.

[18] R. S. Bucy. Nonlinear ﬁltering theory. Automatic Control  IEEE Transactions  10(2):198  1965.
[19] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Journal of basic

Engineering  82(1):35–45  1960.

[20] Emanuel Todorov. General duality between optimal control and estimation. 2008 47th IEEE Conference

on Decision and Control  (5):4286–4292  2008.

9

,Alex Susemihl
Ron Meir
Manfred Opper
Maximilian Alber
Pieter-Jan Kindermans
Kristof Schütt
Klaus-Robert Müller
Fei Sha
Ting-I Hsieh
Yi-Chen Lo
Hwann-Tzong Chen
Tyng-Luh Liu