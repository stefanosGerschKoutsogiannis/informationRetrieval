2014,Tight Continuous Relaxation of the Balanced k-Cut Problem,Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters  corresponding to a balanced k-cut of the graph  are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the hard sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method beats all existing approaches for ratio cut and other balanced k-cut criteria.,Tight Continuous Relaxation of the Balanced k-Cut

Problem

Syama Sundar Rangapuram  Pramod Kaushik Mudrakarta and Matthias Hein

Department of Mathematics and Computer Science

Saarland University  Saarbr¨ucken

Abstract

Spectral Clustering as a relaxation of the normalized/ratio cut has become one of
the standard graph-based clustering methods. Existing methods for the compu-
tation of multiple clusters  corresponding to a balanced k-cut of the graph  are
either based on greedy techniques or heuristics which have weak connection to
the original motivation of minimizing the normalized cut. In this paper we pro-
pose a new tight continuous relaxation for any balanced k-cut problem and show
that a related recently proposed relaxation is in most cases loose leading to poor
performance in practice. For the optimization of our tight continuous relaxation
we propose a new algorithm for the difﬁcult sum-of-ratios minimization problem
which achieves monotonic descent. Extensive comparisons show that our method
outperforms all existing approaches for ratio cut and other balanced k-cut criteria.

1

Introduction

Graph-based techniques for clustering have become very popular in machine learning as they al-
low for an easy integration of pairwise relationships in data. The problem of ﬁnding k clusters in
a graph can be formulated as a balanced k-cut problem [1  2  3  4]  where ratio and normalized
cut are famous instances of balanced graph cut criteria employed for clustering  community detec-
tion and image segmentation. The balanced k-cut problem is known to be NP-hard [4] and thus in
practice relaxations [4  5] or greedy approaches [6] are used for ﬁnding the optimal multi-cut. The
most famous approach is spectral clustering [7]  which corresponds to the spectral relaxation of the
ratio/normalized cut and uses k-means in the embedding of the vertices found by the ﬁrst k eigen-
vectors of the graph Laplacian in order to obtain the clustering. However  the spectral relaxation has
been shown to be loose for k = 2 [8] and for k > 2 no guarantees are known of the quality of the
obtained k-cut with respect to the optimal one. Moreover  in practice even greedy approaches [6]
frequently outperform spectral clustering.
This paper is motivated by another line of recent work [9  10  11  12] where it has been shown that
an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of
balancing functions. Moreover  efﬁcient algorithms for its optimization have been proposed which
produce much better cuts than the standard spectral relaxation. However  the multi-cut problem has
still to be solved via the greedy recursive splitting technique.
Inspired by the recent approach in [13]  in this paper we tackle directly the general balanced k-cut
problem based on a new tight continuous relaxation. We show that the relaxation for the asymmetric
ratio Cheeger cut proposed recently by [13] is loose when the data does not contain k well-separated
clusters and thus leads to poor performance in practice. Similar to [13] we can also integrate label
information leading to a transductive clustering formulation. Moreover  we propose an efﬁcient
algorithm for the minimization of our continuous relaxation for which we can prove monotonic
descent. This is in contrast to the algorithm proposed in [13] for which no such guarantee holds.
In extensive experiments we show that our method outperforms all existing methods in terms of the

1

achieved balanced k-cuts. Moreover  our clustering error is competitive with respect to several other
clustering techniques based on balanced k-cuts and recently proposed approaches based on non-
negative matrix factorization. Also we observe that already with small amount of label information
the clustering error improves signiﬁcantly.

2 Balanced Graph Cuts

Graphs are used in machine learning typically as similarity graphs  that is the weight of an edge
between two instances encodes their similarity. Given such a similarity graph of the instances  the
clustering problem into k sets can be transformed into a graph partitioning problem  where the goal
is to construct a partition of the graph into k sets such that the cut  that is the sum of weights of the
edge from each set to all other sets  is small and all sets in the partition are roughly of equal size.
Before we introduce balanced graph cuts  we brieﬂy ﬁx the setting and notation. Let G(V  W )
denote an undirected  weighted graph with vertex set V with n = |V | vertices and weight matrix
+ with W = W T . There is an edge between two vertices i  j ∈ V if wij > 0. The
W ∈ Rn×n
i∈A j∈B wij and we write 1A for the
indicator vector of set A ⊂ V . A collection of k sets (C1  . . .   Ck) is a partition of V if ∪k
i=1Ci = V  
Ci ∩ Cj = ∅ if i (cid:54)= j and |Ci| ≥ 1  i = 1  . . .   k. We denote the set of all k-partitions of V by Pk.

cut between two sets A  B ⊂ V is deﬁned as cut(A  B) =(cid:80)
Furthermore  we denote by ∆k the simplex {x : x ∈ Rk  x ≥ 0  (cid:80)k

i=1 xi = 1}.

Finally  a set function ˆS : 2V → R is called submodular if for all A  B ⊂ V   ˆS(A∪B)+ ˆS(A∩B) ≤
ˆS(A) + ˆS(B). Furthermore  we need the concept of the Lovasz extension of a set function.
Deﬁnition 1 Let ˆS : 2V → R be a set function with ˆS(∅) = 0. Let f ∈ RV be ordered in increasing
order f1 ≤ f2 ≤ . . . ≤ fn and deﬁne Ci = {j ∈ V | fj > fi} where C0 = V . Then S : RV → R
  is called the Lovasz extension of ˆS. Note that

given by  S(f ) = (cid:80)n

(cid:17)
(cid:16) ˆS(Ci−1) − ˆS(Ci)

S(1A) = ˆS(A) for all A ⊂ V .
The Lovasz extension of a set function is convex if and only if the set function is submodular [14].
The cut function cut(C  C)  where C = V \C  is submodular and its Lovasz extension is given by
TV(f ) = 1
2

(cid:80)n
i j=1 wij|fi − fj|.

i=1 fi

2.1 Balanced k-cuts

The balanced k-cut problem is deﬁned as

k(cid:88)

i=1

min

(C1 ... Ck)∈Pk

cut(Ci  Ci)

ˆS(Ci)

=: BCut(C1  . . .   Ck)

(1)

where ˆS : 2V → R+ is a balancing function with the goal that all sets Ci are of the same “size”.
In this paper  we assume that ˆS(∅) = 0 and for any C (cid:40) V  C (cid:54)= ∅  ˆS(C) ≥ m  for some m > 0.
In the literature one ﬁnds mainly the following submodular balancing functions (in brackets is the
name of the overall balanced graph cut criterion BCut(C1  . . .   Ck)) 

ˆS(C) = |C| 
ˆS(C) = min{|C| |C|} 
ˆS(C) = min{(k − 1)|C|  C}

(Ratio Cut) 
(Ratio Cheeger Cut) 
(Asymmetric Ratio Cheeger Cut).

(2)

The Ratio Cut is well studied in the literature e.g. [3  7  6] and corresponds to a balancing function
without bias towards a particular size of the sets  whereas the Asymmetric Ratio Cheeger Cut recently
proposed in [13] has a bias towards sets of size |V |
k ( ˆS(C) attains its maximum at this point) which
makes perfect sense if one expects clusters which have roughly equal size. An intermediate version
between the two is the Ratio Cheeger Cut which has a symmetric balancing function and strongly
penalizes overly large clusters. For the ease of presentation we restrict ourselves to these balancing
functions. However  we can also handle the corresponding weighted cases e.g.  ˆS(C) = vol(C) =

(cid:80)
i∈C di  where di =(cid:80)n

j=1 wij  leading to the normalized cut[4].

2

3 Tight Continuous Relaxation for the Balanced k-Cut Problem

In this section we discuss our proposed relaxation for the balanced k-cut problem (1). It turns out
that a crucial question towards a tight multi-cut relaxation is the choice of the constraints so that
the continuous problem also yields a partition (together with a suitable rounding scheme). The
motivation for our relaxation is taken from the recent work of [9  10  11]  where exact relaxations
are shown for the case k = 2. Basically  they replace the ratio of set functions with the ratio of
the corresponding Lovasz extensions. We use the same idea for the objective of our continuous
relaxation of the k-cut problem (1) which is given as

k(cid:88)

min

F =(F1 ... Fk) 

TV(Fl)
S(Fl)
F∈Rn×k
subject to : F(i) ∈ ∆k 

l=1

+

i = 1  . . .   n 

max{F(i)} = 1  ∀i ∈ I 
S(Fl) ≥ m 

l = 1  . . .   k 

(3)

(simplex constraints)
(membership constraints)
(size constraints)

where S is the Lovasz extension of the set function ˆS and m = minC(cid:40)V  C(cid:54)=∅ ˆS(C). We have
m = 1  for Ratio Cut and Ratio Cheeger Cut whereas m = k − 1 for Asymmetric Ratio Cheeger
Cut. Note that TV is the Lovasz extension of the cut functional cut(C  C). In order to simplify
notation we denote for a matrix F ∈ Rn×k by Fl the l-th column of F and by F(i) the i-th row
of F . Note that the rows of F correspond to the vertices of the graph and the j-th column of F
corresponds to the set Cj of the desired partition. The set I ⊂ V in the membership constraints is
chosen adaptively by our method during the sequential optimization described in Section 4.
An obvious question is how to get from the continuous solution F ∗ of (3) to a partition
(C1  . . .   Ck) ∈ Pk which is typically called rounding. Given F ∗ we construct the sets  by assigning
each vertex i to the column where the i-th row attains its maximum. Formally 

Ci = {j ∈ V | i = arg max

Fjs} 

s=1 ... k

i = 1  . . .   k 

(Rounding)

(4)

where ties are broken randomly. If there exists a row such that the rounding is not unique  we say
that the solution is weakly degenerated. If furthermore the resulting set (C1  . . .   Ck) do not form a
partition  that is one of the sets is empty  then we say that the solution is strongly degenerated.
First  we connect our relaxation to the previous work of [11] for the case k = 2. Indeed for sym-
metric balancing function such as the Ratio Cheeger Cut  our continuous relaxation (3) is exact even
without membership and size constraints.

Theorem 1 Let ˆS be a non-negative symmetric balancing function  ˆS(C) = ˆS(C)  and denote by
p∗ the optimal value of (3) without membership and size constraints for k = 2. Then it holds

2(cid:88)

i=1

p∗ = min

(C1 C2)∈P2

cut(Ci  Ci)

ˆS(Ci)

.

Furthermore there exists a solution F ∗ of (3) such that F ∗ = [1C∗   1C∗ ]  where (C∗  C∗) is the
optimal balanced 2-cut partition.

Note that rounding trivially yields a solution in the setting of the previous theorem.
A second result shows that indeed our proposed optimization problem (3) is a relaxation of the
balanced k-cut problem (1). Furthermore  the relaxation is exact if I = V .

Proposition 1 The continuous problem (3) is a relaxation of the k-cut problem (1). The relaxation
is exact  i.e.  both problems are equivalent  if I = V .

The row-wise simplex and membership constraints enforce that each vertex in I belongs to exactly
one component. Note that these constraints alone (even if I = V ) can still not guarantee that F
corresponds to a k-way partition since an entire column of F can be zero. This is avoided by the
column-wise size constraints that enforce that each component has at least one vertex.

3

If I = V it is immediate from the proof that problem (3) is no longer a continuous problem as the
feasible set are only indicator matrices of partitions. In this case rounding yields trivially a partition.
On the other hand  if I = ∅ (i.e.  no membership constraints)  and k > 2 it is not guaranteed
that rounding of the solution of the continuous problem yields a partition. Indeed  we will see in
the following that for symmetric balancing functions one can  under these conditions  show that
the solution is always strongly degenerated and rounding does not yield a partition (see Theorem
2). Thus we observe that the index set I controls the degree to which the partition constraint is
enforced. The idea behind our suggested relaxation is that it is well known in image processing that
minimizing the total variation yields piecewise constant solutions (in fact this follows from seeing
the total variation as Lovasz extension of the cut). Thus if |I| is sufﬁciently large  the vertices where
the values are ﬁxed to 0 or 1 propagate this to their neighboring vertices and ﬁnally to the whole
graph. We discuss the choice of I in more detail in Section 4.

Simplex constraints alone are not sufﬁcient to yield a partition: Our approach has been inspired
by [13] who proposed the following continuous relaxation for the Asymmetric Ratio Cheeger Cut

k(cid:88)

(cid:13)(cid:13)Fl − quantk−1(Fl)(cid:13)(cid:13)1

TV(Fl)

min

F =(F1 ... Fk) 

+

l=1

i = 1  . . .   n 

F∈Rn×k
subject to : F(i) ∈ ∆k 

(simplex constraints)

where S(f ) =(cid:13)(cid:13)f − quantk−1(f )(cid:13)(cid:13)1 is the Lovasz extension of ˆS(C) = min{(k − 1)|C|  C} and

quantk−1(f ) is the k− 1-quantile of f ∈ Rn. Note that in their approach no membership constraints
and size constraints are present.
We now show that the usage of simplex constraints in the optimization problem (3) is not sufﬁcient
to guarantee that the solution F ∗ can be rounded to a partition for any symmetric balancing function
in (1). For asymmetric balancing functions as employed for the Asymmetric Ratio Cheeger Cut by
[13] in their relaxation (5) we can prove such a strong result only in the case where the graph is
disconnected. However  note that if the number of components of the graph is less than the number
of desired clusters k  the multi-cut problem is still non-trivial.

Theorem 2 Let ˆS(C) be any non-negative symmetric balancing function. Then the continuous
relaxation

(5)

(6)

k(cid:88)

min

F =(F1 ... Fk) 

TV(Fl)
S(Fl)
F∈Rn×k
subject to : F(i) ∈ ∆k 

l=1

+

of the balanced k-cut problem (1) is void in the sense that the optimal solution F ∗ of the continu-
ous problem can be constructed from the optimal solution of the 2-cut problem and F ∗ cannot be
rounded into a k-way partition  see (4). If the graph is disconnected  then the same holds also for
any non-negative asymmetric balancing function.

i = 1  . . .   n 

(simplex constraints)

The proof of Theorem 2 shows additionally that for any balancing function if the graph is discon-
nected  the solution of the continuous relaxation (6) is always zero  while clearly the solution of the
balanced k-cut problem need not be zero. This shows that the relaxation can be arbitrarily bad in
this case. In fact the relaxation for the asymmetric case can even fail if the graph is not disconnected
but there exists a cut of the graph which is very small as the following corollary indicates.
Corollary 1 Let ˆS be an asymmetric balancing function and C∗ = arg min
that φ∗ := (k− 1) cut(C∗ C∗)

a feasible F with F1 = 1C∗ and Fl = αl1C∗   l = 2  . . .   k such that(cid:80)k
which has objective(cid:80)k

S(Fi) = φ∗ and which cannot be rounded to a k-way partition.

. Then there exists
ˆS(Ci)
l=2 αl = 1  αl > 0 for (6)

< min(C1 ... Ck)∈Pk

C⊂V
cut(Ci Ci)

+ cut(C∗ C∗)

and suppose

(cid:80)k

ˆS(C∗)

ˆS(C∗)

cut(C C)

TV(Fi)

ˆS(C)

i=1

i=1

Theorem 2 shows that the membership and size constraints which we have introduced in our relax-
ation (3) are essential to obtain a partition for symmetric balancing functions. For the asymmetric

4

(a)

(b)

(c)

(d)

(e)

Figure 1: Toy example illustrating that the relaxation of [13] converges to a degenerate solution
when applied to a graph with dominating 2-cut. (a) 10NN-graph generated from three Gaussians in
10 dimensions (b) continuous solution of (5) from [13] for k = 3  (c) rounding of the continuous
solution of [13] does not yield a 3-partition (d) continuous solution found by our method together
with the vertices i ∈ I (black) where the membership constraint is enforced. Our continuous solution
corresponds already to a partition.
(e) clustering found by rounding of our continuous solution
(trivial as we have converged to a partition). In (b)-(e)  we color data point i according to F(i) ∈ R3.

balancing function failure of the relaxation (6) and thus also of the relaxation (5) of [13] is only guar-
anteed for disconnected graphs. However  Corollary 1 indicates that degenerated solutions should
also be a problem when the graph is still connected but there exists a dominating cut. We illustrate
this with a toy example in Figure 1 where the algorithm of [13] for solving (5) fails as it converges
exactly to the solution predicted by Corollary 1 and thus only produces a 2-partition instead of the
desired 3-partition. The algorithm for our relaxation enforcing membership constraints converges to
a continuous solution which is in fact a partition matrix so that no rounding is necessary.

4 Monotonic Descent Method for Minimization of a Sum of Ratios

Apart from the new relaxation another key contribution of this paper is the derivation of an algorithm
which yields a sequence of feasible points for the difﬁcult non-convex problem (3) and reduces
monotonically the corresponding objective. We would like to note that the algorithm proposed by
[13] for (5) does not yield monotonic descent. In fact it is unclear what the derived guarantee for
the algorithm in [13] implies for the generated sequence. Moreover  our algorithm works for any
non-negative submodular balancing function.
The key insight in order to derive a monotonic descent method for solving the sum-of-ratio mini-
mization problem (3) is to eliminate the ratio by introducing a new set of variables β = (β1  . . .   βk).

(7)

k(cid:88)

βl

min
F =(F1 ... Fk) 
F∈Rn×k
+   β∈Rk
subject to : TV(Fl) ≤ βlS(Fl) 

l=1

+

l = 1  . . .   k 
i = 1  . . .   n 
∀i ∈ I 
l = 1  . . .   k.

F(i) ∈ ∆k 
max{F(i)} = 1 
S(Fl) ≥ m 

(descent constraints)
(simplex constraints)
(membership constraints)
(size constraints)
Note that for the optimal solution (F ∗  β∗) of this problem it holds TV(F ∗
l )  l =
1  . . .   k (otherwise one can decrease β∗
l and hence the objective) and thus equivalence holds. This
is still a non-convex problem as the descent  membership and size constraints are non-convex. Our
algorithm proceeds now in a sequential manner. At each iterate we do a convex inner approximation
of the constraint set  that is the convex approximation is a subset of the non-convex constraint set 
based on the current iterate (F t  βt). Then we optimize the resulting convex optimization problem
and repeat the process. In this way we get a sequence of feasible points for the original problem (7)
for which we will prove monotonic descent in the sum-of-ratios.
Convex approximation: As ˆS is submodular  S is convex. Let st
sub-differential of S at the current iterate F t
l . We have by Prop. 3.2 in [14]  (st
ˆS(Cli)  where ji is the index of the ith smallest component of F t
l )i}. Moreover  using the deﬁnition of subgradient  we have S(Fl) ≥ S(F t
(F t
(cid:104)st
l  Fl(cid:105).

l ∈ ∂S(F t
l and Cli = {j ∈ V | (F t
l  Fl − F t

l ) be an element of the
l)ji = ˆS(Cli−1) −
l )j >
l ) + (cid:104)st
l (cid:105) =

l ) = β∗

l S(F ∗

5

 0 1 00 0 11 0 0 1 0 00 0 1 0 1 00 0 11 0 0 0 1 00 0 11 0 0(cid:10)st
(cid:10)st

(cid:11) − δ+
(cid:11) − δ+

For the descent constraints  let λt
the amount of change in each ratio. We further decompose δl as δl = δ+
Let M = maxf∈[0 1]n S(f ) = maxC⊂V ˆS(C)  then for S(Fl) ≥ m 

l ) and introduce new variables δl = βl − λt
l   δ+

l = TV(F t
l )

l − δ−

S(F t

l that capture
l ≥ 0.

l ≥ 0  δ−

l

l

l S(Fl)

l S(Fl) + δ−
l m + δ−
l M

TV(Fl) − βlS(Fl) ≤ TV(Fl) − λt
≤ TV(Fl) − λt

l  Fl
l  Fl
Finally  note that because of the simplex constraints  the membership constraints can be rewritten
as max{F(i)} ≥ 1. Let i ∈ I and deﬁne ji := arg maxj F t
ij (ties are broken randomly). Then the
membership constraints can be relaxed as follows: 0 ≥ 1 − max{F(i)} ≥ 1 − Fiji =⇒ Fiji ≥ 1.
As Fij ≤ 1 we get Fiji = 1. Thus the convex approximation of the membership constraints
ﬁxes the assignment of the i-th point to a cluster and thus can be interpreted as “label constraint”.
However  unlike the transductive setting  the labels for the vertices in I are automatically chosen by
our method. The actual choice of the set I will be discussed in Section 4.1. We use the notation
L = {(i  ji) | i ∈ I} for the label set generated from I (note that L is ﬁxed once I is ﬁxed).
Descent algorithm: Our descent algorithm for minimizing (7) solves at each iteration t the follow-
ing convex optimization problem (8).

k(cid:88)

l − δ−
δ+

min
F∈Rn×k
+  
+  δ−∈Rk

l=1

δ+∈Rk
subject to : TV(Fl) ≤ λt

+

l

l

(cid:10)st
(cid:11) ≥ m 

F(i) ∈ ∆k 
Fiji = 1 

(cid:10)st

l  F t
l

l  Fl

(cid:11) + δ+

l m − δ−

l M 

l = 1  . . . k 

i = 1  . . .   n 
∀(i  ji) ∈ L 
l = 1  . . .   k.

(8)

(descent constraints)
(simplex constraints)
(label constraints)
(size constraints)
∈ ∂S(F t+1

l = TV(F t+1
S(F t+1

following Theorem 3 shows the monotonic descent property of our algorithm.

As its solution F t+1 is feasible for (3) we update λt+1
)  l =
1  . . .   k and repeat the process until the sequence terminates  that is no further descent is possible as
l is smaller than a predeﬁned . The

the following theorem states  or the relative descent in(cid:80)k
Theorem 3 The sequence {F t} produced by the above algorithm satisﬁes (cid:80)k
(cid:80)k

for all t ≥ 0 or the algorithm terminates.

TV(F t+1
S(F t+1
)

and st+1

l=1 λt

)

<

l=1

)

)

l

l

l

l

l

l

TV(F t
l )
S(F t
l )

l=1

The inner problem (8) is convex  but contains the non-smooth term TV in the constraints. We
eliminate the non-smoothness by introducing additional variables and derive an equivalent linear
programming (LP) formulation. We solve this LP via the PDHG algorithm [15  16]. The LP and the
exact iterates can be found in the supplementary material.

4.1 Choice of membership constraints I

cut(cid:0)C∗

The overall algorithm scheme for solving the problem (1) is given in the supplementary material. For
the membership constraints we start initially with I 0 = ∅ and sequentially solve the inner problem
(8). From its solution F t+1 we construct a P (cid:48)
k = (C1  . . .   Ck) via rounding  see (4). We repeat this
process until we either do not improve the resulting balanced k-cut or P (cid:48)
k is not a partition. In this
case we update I t+1 and double the number of membership constraints. Let (C∗
k ) be the
current best partition. For each l ∈ {1  . . .   k} and i ∈ C∗

(cid:88)
j(cid:54)=l  j(cid:54)=s
and deﬁne Ol = {(π1  . . .   π|C∗
|}. The top-ranked vertices in
Ol correspond to the ones which lead to the largest minimal increase in BCut when moved from
C∗
to another component and thus are most likely to belong to their current component. Thus it

s ∪ {i}  C∗
s ∪ {i})
ˆS(C∗
≥ . . . ≥ b∗

 cut(cid:0)C∗

 (9)

l \{i}  C∗
l \{i})
ˆS(C∗

l ∪ {i}(cid:1)

+ min
s(cid:54)=l
l |)| b∗

s\{i}(cid:1)

l we compute

1   . . .   C∗

cut(Cj  Cj)

b∗
li =

≥ b∗

ˆS(Cj)

lπ|C∗

lπ1

lπ2

+

l

l

6

is natural to ﬁx the top-ranked vertices for each component ﬁrst. Note that the rankings Ol  l =
1  . . .   k are updated when a better partition is found. Thus the membership constraints correspond
always to the vertices which lead to largest minimal increase in BCut when moved to another
component. In Figure 1 one can observe that the ﬁxed labeled points are lying close to the centers
of the found clusters. The number of membership constraints depends on the graph. The better
separated the clusters are  the less membership constraints need to be enforced in order to avoid
degenerate solutions. Finally  we stop the algorithm if we see no more improvement in the cut or
the continuous objective and the continuous solution corresponds to a partition.

5 Experiments

We evaluate our method against a diverse selection of state-of-the-art clustering methods like spec-
tral clustering (Spec) [7]  BSpec [11]  Graclus1 [6]  NMF based approaches PNMF [18]  NSC [19] 
ONMF [20]  LSD [21]  NMFR [22] and MTV [13] which optimizes (5). We used the publicly
available code [22  13] with default settings. We run our method using 5 random initializations  7
initializations based on the spectral clustering solution similar to [13] (who use 30 such initializa-
tions). In addition to the datasets provided in [13]  we also selected a variety of datasets from the
UCI repository shown below. For all the datasets not in [13]  symmetric k-NN graphs are built with

(cid:1)  where σx k is the k-NN distance of point x. We chose the

Gaussian weights exp(cid:0)− s(cid:107)x−y(cid:107)2

parameters s and k in a method independent way by testing for each dataset several graphs using all
the methods over different choices of k ∈ {3  5  7  10  15  20  40  60  80  100} and s ∈ {0.1  1  4}.
The best choice in terms of the clustering error across all the methods and datasets  is s = 1  k = 15.

min{σ2

x k σ2

y k}

Iris

150
3

# vertices
# classes

wine

vertebral

ecoli

4moons

webkb4

optdigits

USPS

pendigits

20news MNIST

178
3

310
3

336
6

4000

4

4196

4

5620
10

9298
10

10992

10

19928

20

70000

10

Quantitative results: In our ﬁrst experiment we evaluate our method in terms of solving the bal-
anced k-cut problem for various balancing functions  data sets and graph parameters. The following
table reports the fraction of times a method achieves the best as well as strictly best balanced k-cut
over all constructed graphs and datasets (in total 30 graphs per dataset). For reference  we also report
the obtained cuts for other clustering methods although they do not directly minimize this criterion
in italic; methods that directly optimize the criterion are shown in normal font. Our algorithm can
handle all balancing functions and signiﬁcantly outperforms all other methods across all criteria.
For ratio and normalized cut cases we achieve better results than [7  11  6] which directly optimize
this criterion. This shows that the greedy recursive bi-partitioning affects badly the performance of
[11]  which  otherwise  was shown to obtain the best cuts on several benchmark datasets [23]. This
further shows the need for methods that directly minimize the multi-cut. It is striking that the com-
peting method of [13]  which directly minimizes the asymmetric ratio cut  is beaten signiﬁcantly by
Graclus as well as our method. As this clear trend is less visible in the qualitative experiments  we
suspect that extreme graph parameters lead to fast convergence to a degenerate solution.

RCC-asym

RCC-sym

NCC-asym

NCC-sym

Rcut

Ncut

Best (%)

Strictly Best (%)

Best (%)

Strictly Best (%)

Best (%)

Strictly Best (%)

Best (%)

Strictly Best (%)

Best (%)

Strictly Best (%)

Best (%)

Strictly Best (%)

2.01
0.00

8.72
0.00

37.58
4.70

7.38
0.00

6.71
0.00

38.26
4.70

2.01
0.00

0.00
0.00

23.49
1.34

25.50
10.74

Ours MTV BSpec Spec Graclus PNMF NSC ONMF LSD NMFR
80.54
44.97
94.63
61.74
93.29
56.38
98.66
59.06
85.91
58.39
95.97
61.07

10.07
2.01

20.13
2.68

32.89
8.72

2.01
1.34

1.34
0.00

4.03
0.00

4.03
0.00

4.70
0.00

0.67
0.00

0.67
0.00

1.34
0.67

0.67
0.00

2.01
0.67

0.67
0.00

10.07
0.00

4.70
0.00

3.36
0.00

1.34
0.00

3.36
0.00

38.26
2.01

5.37
0.00

4.03
0.00

5.37
0.00

4.03
0.00

0.67
0.00

13.42
2.01

1.34
0.00

0.67
0.00

1.34
0.00

1.34
0.00

0.67
0.00

0.00
0.00

0.67
0.00

10.07
0.00

19.46
0.67

20.13
0.00

20.81
0.00

7.38
0.00

10.07
0.00

20.13
0.00

9.40
0.00

9.40
0.00

40.27
1.34

37.58
4.03

Qualitative results: In the following table  we report the clustering errors and the balanced k-cuts
obtained by all methods using the graphs built with k = 15  s = 1 for all datasets. As the main goal

1Since [6]  a multi-level algorithm directly minimizing Rcut/Ncut  is shown to be superior to METIS [17]  we do not compare with [17].

7

is to compare to [13] we choose their balancing function (RCC-asym). Again  our method always
achieved the best cuts across all datasets. In three cases  the best cut also corresponds to the best
clustering performance. In case of vertebral  20news  and webkb4 the best cuts actually result in
high errors. However  we see in our next experiment that integrating ground-truth label information
helps in these cases to improve the clustering performance signiﬁcantly.

BSpec

Spec

PNMF

NSC

ONMF

LSD

NMFR

Graclus

MTV

Ours

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Err(%)
BCut

Iris

23.33
1.495
22.00
1.783

22.67
1.508

23.33
1.518

23.33
1.518

23.33
1.518
22.00
1.627

23.33
1.534

22.67
1.508

23.33
1.495

wine

37.64
6.417

20.22
5.820

27.53
4.916

17.98
5.140

28.09
4.881

17.98
5.399

11.24
4.318

8.43
4.293

18.54
5.556
6.74
4.168

vertebral

50.00
1.890

48.71
1.950

50.00
2.250

50.00
2.046

50.65
2.371

39.03
2.557

38.06
2.713

49.68
1.890
34.52
2.433

50.00
1.890

ecoli

19.35
2.550
14.88
2.759

16.37
2.652
14.88
2.754

16.07
2.633

18.45
2.523

22.92
2.556

16.37
2.414

22.02
2.500

16.96
2.399

4moons webkb4

optdigits USPS

pendigits 20news MNIST

36.33
0.634

31.45
0.917

35.23
0.737

32.05
0.933

35.35
0.725

35.68
0.782

36.33
0.840
0.45
0.589

7.72
0.774
0.45
0.589

60.46
1.056

60.32
1.520

60.94
3.520

59.49
3.566

60.94
3.621

47.93
2.082

40.73
1.467
39.97
1.581

48.40
2.346

60.46
1.056

11.30
0.386

7.81
0.442

10.37
0.548

8.24
0.482

10.37
0.548

8.42
0.483

2.08
0.369
1.67
0.350

4.11
0.374

1.71
0.350

20.09
0.822

21.05
0.873

24.07
1.180

20.53
0.850

24.14
1.183

22.68
0.918

22.17
0.992

19.75
0.815
15.13
0.940

19.72
0.802

17.59
0.081

16.75
0.141

17.93
0.415

19.81
0.101

22.82
0.548

13.90
0.188

13.13
0.240
10.93
0.092

20.55
0.193

19.95
0.079

84.21
0.966

79.10
1.170

66.00
2.924

78.86
2.233

69.02
3.058

67.81
2.056
39.97
1.241

60.69
1.431

72.18
3.291

79.51
0.895

11.82
0.471

22.83
0.707

12.80
0.934

21.27
0.688

27.27
1.575

24.49
0.959
fail
-

2.43
0.440

3.77
0.458
2.37
0.439

Transductive Setting: As in [13]  we randomly sample either one label or a ﬁxed percentage of
labels per class from the ground truth. We report clustering errors and the cuts (RCC-asym) for both
methods for different choices of labels. For label experiments their initialization strategy seems to
work better as the cuts improve compared to the unlabeled case. However  observe that in some
cases their method seems to fail completely (Iris and 4moons for one label per class).

Labels

1

1%

5%

10%

MTV

Ours

MTV

Ours

MTV

Ours

MTV

Ours

Err(%)
BCut
Err(%)
BCut

Err(%)
BCut
Err(%)
BCut

Err(%)
BCut
Err(%)
BCut

Err(%)
BCut
Err(%)
BCut

Iris

33.33
3.855
22.67
1.571

33.33
3.855
22.67
1.571
17.33
1.685
17.33
1.685

18.67
1.954
14.67
1.960

wine

9.55
4.288
8.99
4.234

10.67
4.277
6.18
4.220

7.87
4.330
6.74
4.224

7.30
4.332
6.74
4.194

vertebral
42.26
2.244
50.32
2.265
39.03
2.300
41.29
2.288

40.65
2.701
37.10
2.724

39.03
3.187
33.87
3.134

ecoli
13.99
2.430
15.48
2.432

14.29
2.429
13.99
2.419

14.58
2.462
13.99
2.461

13.39
2.776
13.10
2.778

4moons webkb4 optdigits USPS pendigits 20news MNIST

35.75
0.723
0.57
0.610
0.45
0.589
0.45
0.589
0.45
0.589
0.45
0.589
0.38
0.592
0.38
0.592

51.98
1.596
45.11
1.471

48.38
1.584
41.63
1.462

40.09
1.763
38.04
1.719
40.63
2.057
41.97
1.972

1.69
0.352
1.69
0.352
1.67
0.354
1.67
0.354
1.51
0.369
1.53
0.369
1.41
0.377
1.41
0.377

12.91
0.846
12.98
0.812

5.21
0.789
5.13
0.789
4.85
0.812
4.85
0.811
4.19
0.833
4.25
0.833

14.49
0.127
10.98
0.113
7.75
0.129
7.75
0.128

1.79
0.188
1.76
0.188
1.24
0.197
1.24
0.197

50.96
1.286
68.53
1.057

40.18
1.208
37.42
1.157

31.89
1.254
30.07
1.210

27.80
1.346
26.55
1.314

2.45
0.439
2.36
0.439

2.41
0.443
2.33
0.442
2.18
0.455
2.18
0.455

2.03
0.465
2.02
0.465

6 Conclusion

We presented a framework for directly minimizing the balanced k-cut problem based on a new con-
tinuous relaxation. Apart from ratio/normalized cut  our method can also handle new application-
speciﬁc balancing functions. Moreover  in contrast to a recursive splitting approach [24]  our method
enables the direct integration of prior information available in form of must/cannot-link constraints 
which is an interesting topic for future research. Finally  the monotonic descent algorithm proposed
for the difﬁcult sum-of-ratios problem is another key contribution that is of independent interest.
Acknowledgements. The authors would like to acknowledge support by the DFG excellence cluster
MMCI and the ERC starting grant NOLEPRO.

8

References
[1] W. E. Donath and A. J. Hoffman. Lower bounds for the partitioning of graphs. IBM J. Res. Develop. 

17:420–425  1973.

[2] A. Pothen  H. D. Simon  and K.-P. Liou. Partitioning sparse matrices with eigenvectors of graphs. SIAM

J. Matrix Anal. Appl.  11(3):430–452  1990.

[3] L. Hagen and A. B. Kahng. Fast spectral methods for ratio cut partitioning and clustering. In ICCAD 

pages 10–13  1991.

[4] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 

22:888–905  2000.

[5] A. Ng  M. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm.

849–856  2001.

In NIPS  pages

[6] I. Dhillon  Y. Guan  and B. Kulis. Weighted graph cuts without eigenvectors: A multilevel approach.

IEEE Trans. Pattern Anal. Mach. Intell.  pages 1944–1957  2007.

[7] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing  17:395–416  2007.
[8] S. Guattery and G. Miller. On the quality of spectral separators. SIAM J. Matrix Anal. Appl.  19:701–719 

1998.

[9] A. Szlam and X. Bresson. Total variation and Cheeger cuts. In ICML  pages 1039–1046  2010.
[10] M. Hein and T. B¨uhler. An inverse power method for nonlinear eigenproblems with applications in 1-

spectral clustering and sparse PCA. In NIPS  pages 847–855  2010.

[11] M. Hein and S. Setzer. Beyond spectral clustering - tight relaxations of balanced graph cuts. In NIPS 

pages 2366–2374  2011.

[12] X. Bresson  T. Laurent  D. Uminsky  and J. H. von Brecht. Convergence and energy landscape for Cheeger

cut clustering. In NIPS  pages 1394–1402  2012.

[13] X. Bresson  T. Laurent  D. Uminsky  and J. H. von Brecht. Multiclass total variation clustering. In NIPS 

pages 1421–1429  2013.

[14] F. Bach. Learning with submodular functions: A convex optimization perspective. Foundations and

Trends in Machine Learning  6(2-3):145–373  2013.

[15] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with applications to

imaging. J. of Math. Imaging and Vision  40:120–145  2011.

[16] T. Pock and A. Chambolle. Diagonal preconditioning for ﬁrst order primal-dual algorithms in convex

optimization. In ICCV  pages 1762–1769  2011.

[17] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs.

SIAM J. Sci. Comput.  20(1):359–392  1998.

[18] Z. Yang and E. Oja. Linear and nonlinear projective nonnegative matrix factorization. IEEE Transactions

on Neural Networks  21(5):734–749  2010.

[19] C. Ding  T. Li  and M. I. Jordan. Nonnegative matrix factorization for combinatorial optimization: Spec-

tral clustering  graph matching  and clique ﬁnding. In ICDM  pages 183–192  2008.

[20] C. Ding  T. Li  W. Peng  and H. Park. Orthogonal nonnegative matrix tri-factorizations for clustering. In

KDD  pages 126–135  2006.

[21] R. Arora  M. R. Gupta  A. Kapila  and M. Fazel. Clustering by left-stochastic matrix factorization. In

ICML  pages 761–768  2011.

[22] Z. Yang  T. Hao  O. Dikmen  X. Chen  and E. Oja. Clustering by nonnegative matrix factorization using

graph random walk. In NIPS  pages 1088–1096  2012.

[23] A. J. Soper  C. Walshaw  and M. Cross. A combined evolutionary search and multilevel optimisation

approach to graph-partitioning. J. of Global Optimization  29(2):225–241  2004.

[24] S. S. Rangapuram and M. Hein. Constrained 1-spectral clustering. In AISTATS  pages 1143–1151  2012.

9

,Syama Sundar Rangapuram
Pramod Kaushik Mudrakarta
Matthias Hein
Walid Krichene
Alexandre Bayen
Peter Bartlett
Sheng Chen
Arindam Banerjee