2011,Message-Passing for Approximate MAP Inference with Latent Variables,We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes)  marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this.  The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework.  We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets  against several baselines  demonstrate the efficacy of our proposed algorithm.,Message-Passing for Approximate MAP Inference

with Latent Variables

Jiarong Jiang

Dept. of Computer Science
University of Maryland  CP

Piyush Rai

School of Computing

University of Utah

jiarong@umiacs.umd.edu

piyush@cs.utah.edu

Hal Daum´e III

Dept. of Computer Science
University of Maryland  CP
hal@umiacs.umd.edu

Abstract

We consider a general inference setting for discrete probabilistic graphical models
where we seek maximum a posteriori (MAP) estimates for a subset of the random
variables (max nodes)  marginalizing over the rest (sum nodes). We present a hy-
brid message-passing algorithm to accomplish this. The hybrid algorithm passes
a mix of sum and max messages depending on the type of source node (sum or
max). We derive our algorithm by showing that it falls out as the solution of a par-
ticular relaxation of a variational framework. We further show that the Expectation
Maximization algorithm can be seen as an approximation to our algorithm. Ex-
perimental results on synthetic and real-world datasets  against several baselines 
demonstrate the efﬁcacy of our proposed algorithm.

1

Introduction

Probabilistic graphical models provide a compact and principled representation for capturing com-
plex statistical dependencies among a set of random variables. In this paper  we consider the general
maximum a posteriori (MAP) problem in which we want to maximize over a subset of the variables
(max nodes  denoted X)  marginalizing the rest (sum nodes  denoted Z). This problem is termed
as the Marginal-MAP problem. A typical example is the minimum Bayes risk (MBR) problem [1]
where the goal is to ﬁnd an assignment ˆx which optimizes a loss ℓ(ˆx  x) with regard to some usually
unknown truth x. Since x is latent  we need to marginalize it before optimizing with respect to ˆx.
Although the speciﬁc problems of estimating marginals and estimating MAP individually have been
studied extensively [2  3  4]  similar developments for the more general problem of simultaneous
marginal and MAP estimation are lacking. More recently  [5] proposed a method based optimizing
a variational objective on speciﬁc graph structures  and is a simultaneous development as the method
we propose in this paper (please refer to the supplementary material for further details and other
related work).

This problem is fundamentally difﬁcult. As mentioned in [6  7]  even for a tree-structured model 
we cannot solve the Marginal-MAP problem exactly in poly-time unless P = N P . Moreover  it
has been shown [8] that even if a joint distribution p(x  z) belongs to the exponential family  the

corresponding marginal distribution p(x) = Pz p(x  z) is in general not exponential family (with a

very short list of exceptions  such as Gaussian random ﬁelds). This means that we cannot directly
apply algorithms for MAP inference to our task. Motivated by this problem  we propose a hybrid
message passing algorithm which is both intuitive and justiﬁed according to variational principles.
Our hybrid message passing algorithm uses a mix of sum and max messages with the message type
depending on the source node type.

Experimental results on chain and grid structured synthetic data sets and another real-world dataset
show that our hybrid message-passing algorithm works favorably compared to standard sum-
product  standard max-product  or the Expectation-Maximization algorithm which iteratively pro-
vides MAP and marginal estimates. Our estimates can be further improved by a few steps of local

1

search [6]. Therefore  using the solution found by our hybrid algorithm to initialize some local
search algorithms largely improves the performance on both accuracy and convergence speed  com-
pared to the greedy stochastic search method described in [6]. We also give an example in Sec. 5
of how our algorithm can also be used to solve other practical problem which can be cast under the
Marginal-MAP framework. In particular  the Minimum Bayes Risk [9] problem for decomposable
loss-functions can be readily solved under this framework.

2 Problem Setting

In our setting  the nodes in a graphical model with discrete random variables are divided into two
sets: max and sum nodes. We denote a graph G = (V  E)  V = X ∪ Z where X is the set of nodes
for which we want to compute the MAP assignment (max nodes)  and Z is the set of nodes for which
we need the marginals (sum nodes). Let x = {x1  . . .   xm} (xs ∈ Xs)  z = {z1  . . .   zn} (zs ∈ Zs)
be the random variables associated with the nodes in X and Z respectively. The exponential family
distribution p over these random variables is deﬁned as follows:

pθ(x  z) = exp [hθ  φ(x  z)i − A(θ)]

where φ(x  z) is the sufﬁcient statistics of the enumeration of all node assignments  and θ is the vec-

tor of canonical or exponential parameters. A(θ) = logPx z exp[hθ  φ(x  z)i] is the log-partition

function. In this paper  we consider only pairwise node interactions and use standard overcomplete
representation of the sufﬁcient statistics [10] (deﬁned by indicator function I later).

The general MAP problem can be formalized as the following maximization problem:

with corresponding marginal probabilities of the z nodes  given x∗.

x∗ = arg max

x Xz

pθ(x  z)

p(zs|x∗) = XZ\{zs}

p(z|x∗)  s = 1  . . .   n

(1)

(2)

Before proceeding  we introduce some notations for clarity of exposition: Subscripts s  u  t  etc.
denote nodes in the graphical model. zs  xs are sum and max random variables respectively  asso-
ciated with some node s. vs can be either a sum (zs) or a max random (xs) variable  associated with
some node s. N (s) is the set of neighbors of node s. Xs  Zs  Vs are the state spaces from which xs 
zs  vs take values.

2.1 Message Passing Algorithms
The sum-product and max-product algorithms are standard message-passing algorithms for inferring
marginal and MAP estimates respectively in probabilistic graphical models. Their idea is to store
a belief state associated with each node  and iteratively passing messages between adjacent nodes 
which are used to update the belief states. It is known [11] that these algorithms are guaranteed to
converge to the exact solution on trees or polytrees. On loopy graphs  they are no longer guaranteed
to converge  but they can still provide good estimates when converged [12].
In the standard sum product algorithm  the message Mts passed from node s to one of its neighbors
t is as follows:




t)


Mts(vs) ← κ Xv′

t∈Vt

exp[θst(vs  v′

t) + θt(v′

t)] Yu∈N (t)\s

Mut(v′

(3)

where κ is a normalization constant. When the messages converge  i.e. {Mts  Mst} does not change
for every pair of nodes s and t  the belief (psuedomarginal distribution) for the node s is given by

µs(vs) = κ exp{θs(vs)}Qt∈N (s) Mts(vs). The outgoing messages for max product algorithm have

the same form but with a maximization instead of a summation in Eq. (3). After convergence  the
MAP assignment for each node is the assignment with the highest max-marginal probability.

On loopy graphs  the tree-weighted sum and max product [13  14] can help ﬁnd the upper bound
of the marginal or MAP problem. They decompose the loopy graph into several spanning trees and
reweight the messages by the edge appearance probability.

2

2.2 Local Search Algorithm
Eq (1) can be viewed as doing a variable elimination for z nodes ﬁrst  followed by a maximization
over x. Its maximization step may be performed using heuristic search techniques [7  6]. Eq (2) can
be computed by running standard sum-product over z  given the MAP x∗ assignments. In [6]  the
assignment for the MAP nodes are found by greedily searching the best neighboring assignments
which only differs on one node. However  the hybrid algorithm we propose allows simultaneously
approximating both Eq (1) and Eq (2).

3 HYBRID MESSAGE PASSING

In our setting  we wish to compute MAP estimates for one set of nodes and marginals for the rest.
One possible approach is to run standard sum/max product algorithms over the graph  and ﬁnd the
most-likely assignment for each max node according to the maximum of sum or max marginals1.
These na¨ıve approaches have their own shortcomings; for example  although using standard max-
product may perform reasonably when there are many max nodes  it inevitably ignores the effect of
sum nodes which should ideally be summed over. This is analogous to the difference between EM
for Gaussian mixture models and K-means. (See Sec. 6)

3.1 ALGORITHM
We now present a hybrid message-passing algorithm which passes sum-style or max-style messages
based on the type of nodes from which the message originates.
In the hybrid message-passing
algorithm  a sum node sends sum messages to its neighbors and a max node sends max messages.
The type of message passed depends on the type of source node  not the destination node.

More speciﬁcally  the outgoing messages from a source node are as follows:

• Message from sum node t to any neighbor s:

Mts(vs) ← κ1 Xz ′

t∈Zt

exp[θst(vs  z′

t) + θt(z′

t)] Yu∈N (t)\s

Mut(z′

• Message from max node t to any neighbor s:

Mts(vs) ← κ2 max
t∈Xt

x′

exp[θst(vs  x′

t) + θt(x′

t)] Yu∈N (t)\s

Mut(x′

and κ1 κ2 are normalization constants. Algo 1 shows the procedure to do hybrid message-passing.

Algorithm 1 Hybrid Message-Passing Algorithm
Inputs: Graph G = (V  E)  V = X ∪ Z  potentials θs  s ∈ V and θst  (s  t) ∈ E.

1. Initialize the messages to some arbitrary value.
2. For each node s ∈ V in G  do the following until messages converge (or maximum number

of iterations reached)

• If s ∈ X  update messages by Eq.(5).
• If s ∈ Z  update messages by Eq.(4).
3. Compute the local belief for each node s.

µs(ys) = κ exp{θs(vs)}Qt∈N (s) Mts(vs)
4. For all xs ∈ X  return arg maxxs∈Xs µs(xs)
5. For all zs ∈ Z  return µs(zs).

When there is only a single type of node in the graph  the hybrid algorithm reduces to the standard
max or sum-product algorithm. Otherwise  it passes different messages simultaneously and gives an
approximation to the MAP assignment on max nodes as well as the marginals on sum nodes. On
the loopy graphs  we can also apply this scheme to pass hybrid tree-reweighted messages between
nodes to obtain marginal and MAP estimates. (See Appendix C of the supplementary material)

1Running the standard sum-product algorithm and choosing the maximum likelihood assignment for the

max nodes is also called maximum marginal decoding [15  16].

3






t)

t)


(4)

(5)

(7)

(8)

(9)

3.2 VARIATIONAL DERIVATION
In this section  we show that the Marginal-MAP problem can be framed under a variational frame-
work  and the hybrid message passing algorithm turns out to be a solution of it. (a detailed derivation
is in Appendix A of the supplementary material). To see this  we construct a new graph G¯x with xs’
assignments ﬁxed to be ¯x ∈ X = X1 × · · · × Xm  so the log-partition function A(θ¯x) of the graph
G¯x is

p(¯x  z) + log A(θ) = log p(¯x) + const

(6)

A(θ¯x) = logXz

As the constant only depends on the log-partition function of the original graph and does not vary
with different assignments of MAP nodes  A(θ¯x) exactly estimates the log-likelihood of assignment
¯x. Therefore argmax¯x∈X log p(¯x) = argmax¯x∈X A(θ¯x). Moreover  A(θ¯x) can be approximated
by the following [10]:

where M (G¯x) is the following marginal polytope of graph Gx:

A(θ¯x) ≈ sup

hθ  µi + HBethe(µ)

µ∈M (G¯x)

M (G¯x) = 


µ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

µs(zs)  µst(vs  vt): marginals with ¯x ﬁxed to its assignment

µs(xs) = (cid:26) 1

0

if xs = ¯xs
else




Recall  vs stands for xs or zs. HBethe(µ) is the Bethe energy of the graph:

HBethe(µ) =Xs

Hs(µs) − X(s t)∈E

Ist(µst)  Hs(µs) = − Xvs∈Vs

µs(vs) log µs(vs)

Ist(µst) =

µst(vs  vt) log

X(vs vt)∈Vs×Vt

µst(vs  vt)

µs(vs)µt(vt)

For readability  we use µsum  µmax to subsume the node and pairwise marginals for sum/max nodes
and µsum→max  µmax→sum are the pairwise marginals for edges between different types of nodes. The
direction here is used to be consistent with the distinction of the constraints as well as the messages.

Solving the Marginal-MAP problem is therefore equivalent to solving the following optimization
problem:

max
¯x∈X

sup

hθ  µi + HBethe(µ) ≈ sup

sup

hθ  µi + HBethe(µ)

(10)

µother∈M (G¯x)

µmax∈M¯x

µother∈M (G¯x)

µother contains all other node/pairwise marginals except µmax. The Bethe entropy terms can be
written as (H is the entropy and I is mutual information)

HBethe(µ) = Hµmax + Hµsum − Iµmax→µmax − Iµsum→µsum − Iµmax→µsum − Iµsum→µmax

If we force to satisfy the second condition in (8)  the entropy of max nodes Hµmax = Hs(µs) = 0 
∀s ∈ X and the mutual information between max nodes Iµmax→µmax = Ist(xs  xt) = 0  ∀s  t ∈ X.
For mutual information between different types of nodes  we can either force xs to have integral so-
lutions  or relax xs to have non-integral solution  or relax xs on one direction2. In practice  we relax
the mutual information on the message from sum nodes to max nodes  so the mutual information
µs(xs)µt(zt) =
µs(x∗)µt(zt) = 0  ∀s ∈ X  t ∈ Z  where x∗ is the assigned state of x at node
s. Finally  we only require sum nodes to satisfy normalization and marginalization conditions  the
entropy for sum nodes  mutual information between sum nodes  and from sum node to max node
can be nonzero.
The above process relaxes the polytope M (G¯x) to be M¯x × Lz(G¯x)  where
µs(zs) = 1  µs(xs) = 1 iff xs = ¯xs 

on the other direction Iµmax→µsum = Ist(xs  zt) = P(xs zt)∈Xs×Zt µst(xs  zt) log µst(xs zt)
Pzt∈Zt µst(x∗  zt) log µst(x∗ zt)

Lz(G¯x) =


µ ≥ 0(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pzs

Pzt
Pzs

µst(vs  zt) = µs(vs) 
µst(zs  vt) = µt(vt) 

µst(xs  zt) = µt(zt) iff xs = ¯xs 

µst(xs  xt) = 1 iff xs = ¯xs  xt = ¯xt.




2This results in four different relaxations for different combinations of message types and the hybrid algo-

rithm performed empirically the best.

4

This analysis results in the following optimization problem.

sup

sup

µmax∈M ¯x

µothers∈M (G ¯x)

hθ  µi + H(µsum) − I(µsum→sum) − I(µsum→max)

Further relaxing µ¯xs to have non-integral solutions  deﬁne

Finally we get

L(G) =


µ ≥ 0(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

µs(vs) = 1 

µst(vs  vt) = µs(vs) 

µst(vs  vt) = µt(vt). 
Pvs


Pvt
Pvs

sup

hµ  θi + H(µsum) − I(µsum→sum) − I(µsum→max)

µ∈L(G)

(11)

So M¯x × Mz(G¯x) ⊆ M¯x × Lz(G¯x) ⊆ L(G). Unfortunately  M¯x × Mz(G¯x) is not guaranteed to be
convex and we can only obtain an approximate solution to the problem deﬁned in Eq (11). Taking
the Lagrangian formulation  for an x node  the partial derivative of the Lagrangian with respect to
µs(xs)  s ∈ X keeps the same form as in max product derivation[10]  and the situations are identical
for µs(zs)  s ∈ Z and pairwise psuedo-marginals  so the hybrid message-passing algorithm provides
a solution to Eq (11) (see Appendix A of the supplementary material for a detailed derivation).

4 Expectation Maximization

Another plausible approach to solve the Marginal MAP problem is by the Expectation Maximiza-
tion(EM) algorithm [17]  typically used for maximum likelihood parameter estimation in latent vari-
able models. In our setting  the variables Z correspond to the latent variables. We now show one
way of approaching this problem by applying the sum-product and max-product algorithms in the E
and M step respectively. To see this  let us ﬁrst deﬁne3:

F (˜p  x) = E ˜p[log p(x  z)] + H(˜p(z))

(12)

where H(˜p) = −E ˜p[log ˜p(z)].
Then EM can be interpreted as a joint maximization of the function F [18]: At iteration t  for
the E-step  ˜p(t) is set to be the ˜p that maximizes F (˜p  x(t−1)) and for the M-step  x(t) is the x
that maximizes F (˜p(t)  x). Given F   the following two properties4 show that jointly maximizing

function F is equivalent to maximizing the objective function p(x) = Pz p(x  z).

1. With the value of x ﬁxed in function F   the unique solution to maximizing F (˜p  x) is given

by ˜p(z) = p(z|x).

2. If ˜p(z) = p(z|x)  then F (˜p  x) = log p(x) = logPz p(x  z).

4.1 Expectation Maximization via Message Passing
Now we can derive the EM algorithm for solving the Marginal-MAP problem by jointly maximizing
function F . In the E-step  we need to estimate ˜p(z) = p(z|x) given x. This can be done by ﬁxing x
values at their MAP assignments and running the sum-product algorithm over the resulting graph:
pθ(z | ¯x) log pθ(x  z)  where ¯x is the assignment given by the pre-
z∼pθ(z | ¯x) log pθ(x | z)  as the log pθ(z) term in the
z∼pθ(z | ¯x) log pθ(x | z) = maxxPz p(z | ¯x)hθ  φ(x  z)i 

The M-step works by maximizing E
vious M-step. This is equivalent to maximizing E
maximization is independent of x. maxx E
which in the overcomplete representation [10] can be approximated by

Xs∈X i

θs;i + Xt∈Z j




µt;jθst;ij


Is;i(xs) + X(s t)∈E s t∈X X(i j)

θst;ij Ist;ij(xs  xt) + C

where C subsumes the terms irrelevant to the maximization over x  µt is the psuedo-marginal of
node t given ¯x5. Then  the M-step amounts to running the max product algorithm with potentials on
x nodes modiﬁed according to Eq. (13). Summarizing  the EM algorithm for solving marginal-MAP
estimation can be interpreted as follows:

• E-step: Fix xs to be the MAP assignment value from iteration (k − 1) and run sum product

to get beliefs on sum nodes zs  say µt  t ∈ Z.

3By directly applying Jensen’s inequality to the objective function maxx logPz p(x  z)

4The proofs are straightforward following Lemma 1 and 2 in [18] page 4-5. More details are in Appendix

B of the supplementary material

5A detailed derivation is in Appendix B.4 of the supplementary material

5

• M-step: Build a new graph ˜G = ( ˜V   ˜E) only containing the max nodes. ˜V =X and ˜E =
{(s  t)|∀(s  t) ∈ E  s  t ∈ X}. For each max node s in the graph  set its potential as

˜θs;i = θs;i + Pj θst;ijµt;j  where t ∈ Z and (s  t) ∈ E. ˜θst;ij = θst;ij ∀(s  t) ∈ ˜E. Run

max product over this new graph and update the MAP assignment.

4.2 Relationship with the Hybrid Algorithm
Apart from the fact that the hybrid algorithm passes different messages simultaneously and EM
does it iteratively  to see the connection with the hybrid algorithm  let us ﬁrst consider the message
passed in the E-step at iteration k. xs are ﬁxed at the last assignment which maximizes the message
at iteration k − 1  denoted as x∗ here. The M (k−1)

are the messages computed at iteration k − 1.

ut

M (k)

ts (zs) = κ1{exp[θst(zs  x∗

t ) + θt(x∗

t )] Yu∈N (t)\s

M (k−1)

ut

(x∗

t )}

(13)

Now assume there exists an iterative algorithm which  at each iteration  computes the messages
used in both steps of the message-passing variant of the EM algorithm  denoted ˜Mts. Eq (13) then
becomes

˜M (k)

ts (zs) == κ1 max

x′

{exp[θst(zs  x′

t) + θt(x′

t)] Yu∈N (t)\s

˜M (k−1)

ut

(x′

t)}

So the max nodes (x’s) should pass the max messages to its neighbors (z’s)  which is what the hybrid
message-passing algorithm does.

In the M-step for EM (as discussed in Sec. 4)  all the sum nodes t are removed from the graph

and the parameters of the adjacent max nodes are modiﬁed as: θs;i = θs;i + Pj θst;ijµt;j. µt is

computed by the sum product at the E-step of iteration k  and these sum messages are used (in form
of the marginals µt) in the subsequent M-step (with the sum nodes removed). However  a max node
may prefer different assignments according to different neighboring nodes. With such uncertainties 
especially during the ﬁrst a few iterations  it is very likely that making hard decisions will directly
lead to the bad local optima. In comparison  the hybrid message passing algorithm passes mixed
messages instead of making deterministic assignments in each iteration.

5 MBR Decoding

Most work on ﬁnding “best” solutions in graphical models focuses on the MAP estimation problem:
ﬁnd the x that maximizes pθ(x). In many practical applications  one wishes to ﬁnd an x that min-
imizes some risk  parameterized by a given loss function. This is the minimum Bayes risk (MBR)
setting  which has proven useful in a number of domains  such as speech recognition [9]  natural
language parsing [19  20]  and machine translation [1]. We are given a loss function ℓ(x  ˆx) which
measures the loss of ˆx assuming x is the truth. We assume losses are non-negative. Given this loss
function  the minimum Bayes risk solution is the minimizer of Eq (14):

MBRθ = arg min

ˆx

Ex∼p[ℓ(x  ˆx)] = arg min

ˆx Xx

p(x)ℓ(x  ˆx)

(14)

We now assume that ℓ decomposes over the structure of x. In particular  suppose that: ℓ(x  ˆx) =

Pc∈C ℓ(xc  ˆxc)  where C is some set of cliques in x  and xc denotes the variables associated with

that clique. For example  for Hamming loss  the cliques are simply the set of pairs of vertices of the
form (xi  ˆxi)  and the loss simply counts the number of disagreements. Such decompositionality is
widely assumed in structured prediction algorithms [21  22].
Assume lc(x  x′) ≤ L ∀c  x  x′. Therefore l(x  x′) ≤ |C|L. We can then expand Eq (14) into the
following:

MBRθ = arg min

p(x)ℓ(x  ˆx) = argmax

p(x)(|C|L − l(x  x′))

This resulting expression has exactly the same form as the MAP-with-marginal problem  where
x is the variable being marginalized and ˆx being the variable being maximized. Fig. 1 shows a
simple example of transforming a MAP lattice problem into an MBR problem under Hamming loss.
Therefore  we can apply our hybrid algorithm to solve the MBR problem.

6

ˆx Xx
ˆx Xx

= arg max

x′ Xx

exp"hθ  xi + logXc

[L − ℓ(xc  ˆxc)] − A(θ)#

Average KL−divergence on sum nodes

 

Max Product
Sum Product
Hybrid Message Passing

0.05

0.045

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

e
c
n
e
g
r
e
v
d
−
L
K

i

0

 

10

20

30

40
60
% of sum nodes

50

70

80

90

Figure 1: The Augmented Model For Solving
The MBR Problem Under Hamming Loss over
a 6-node simple lattice

Figure 2: Comparison of Various Algorithms For
Marginals on 10-Node Chain Graph

6 EXPERIMENTS

We perform the experiments on synthetic datasets as well as a real-world protein side-chain pre-
diction dataset [23]  and compare our hybrid message-passing algorithm (both its standard belief
propagation and the tree-reweighted belief propagation (TRBP) versions) against a number of base-
lines such as the standard sum/max product based MAP estimates  EM  TRBP  and the greedy local
search algorithm proposed in [6].

6.1 Synthetic Data
For synthetic data  we ﬁrst take a 10-node chain graph with varying splits of sum vs max nodes 
and random potentials. Each node can take one of the two states (0/1). The node and the edge
potentials are drawn from UNIFORM(0 1) and we randomly pick nodes in the graph to be sum
or max nodes. For this small graph  the true assignment is computable by explicitly maximizing

p(x) = Pz p(x  z) = 1

Z Pz Qs∈V ψs(vs)Q(s t)∈E ψst(vs  vt)  where Z is some normalization

constant and ψs(vs) = exp θs(vs).
First  we compare the various algorithms on the MAP assignments. Assume that the aforementioned
n) and some algorithm gives the approximate as-
maximization gives assignment x∗ = (x∗
signment x = (x1  . . .   xn). The metrics we use here are the 0/1 loss and the Hamming loss.

1  . . .   x∗

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
r
o
r
r

E

0/1 Loss on a 10−node chain graph

max
sum
hybrid
EM
max+local search
sum+local search
hybrid+local search

 

0.25

Hamming loss on a 10−node chain graph

 

max
sum
hybrid
EM
max+local search
sum+local search
hybrid+local search

e

t

a
r
 
r
o
r
r

E

0.2

0.15

0.1

0.05

 

0
0.1

0.2

0.3

0.4

0.5

0.6

% of sum nodes

0.7

0.8

0.9

 

0
0.1

0.2

0.3

0.4

0.5

0.6

% of sum nodes

0.7

0.8

0.9

Figure 3: Comparison of Various Algorithms For MAP Estimates on 10-Node Chain Graph: 0-1 Loss (Left) 
Hamming Loss (Right)

Fig. 3 shows the loss on the assignment of the max nodes. In the ﬁgure  as the number of sum nodes
goes up  the accuracy of the standard sum-product based estimation (sum) gets better  whereas the
accuracy of standard max-product based estimation (max) worsens. However  our hybrid message-
passing algorithm (hybrid)  on an average  results in the lowest loss compared to the other baselines 
with running times similar to the sum/max product algorithms.

We also compare a stochastic greedy search approach described in [6] initialized by the results of
sum/max/hybrid algorithm (sum/max/hybrid+local search). As shown in [6]  local search with sum
product initialization empirically performs better than with max product  so later on  we only com-
pare the results with local search using sum product initialization (LS). Best of the three initialization
methods  by starting at the hybrid algorithm results  the search algorithm in very few steps can ﬁnd

7

log−likelihood of the assignment normalized by hybrid algorithm

1.005

1

0.995

0.99

0.985

d
o
o
h

i
l

i

e
k
L
e
v
i
t

 

l

a
e
R

 

d
o
o
h

i
l

i

e
k
L
e
v
i
t

 

l

a
e
R

max
sum
hybrid
LS
hybrid+LS

1.001

1

0.999

0.998

0.997

0.996

0.995

0.994

0.993

0.992

log−likelihood of the assignments normalized by hybrid algo

 

TR−max
TR−sum
TR−hybrid
LS
TR−hybrid+LS

0.7

0.8

0.9

0.98

 

10% 20% 30% 40% 50% 60% 70% 80% 90%

% of sum node

0.991

 

0.1

0.2

0.3

0.4

0.5

0.6

% of sum nodes

Figure 4: Approximate Log-Partition Function Scores on a 50-Node Tree (Left) and an 8*10 Grid (Right)
Graph Normalized By the Result of Hybrid Algorithm

the local optimum  which often happened to be the global optimum as well. In particular  it only
takes 1 or 2 steps of search in the 10-node chain case and 1 to 3 steps in the 50-node tree case.

Next  we experiment with the marginals estimation. Fig 2 shows the mean of KL-divergence on the
marginals for the three message-passing algorithms (each averaged over 100 random experiments)
compared to the true marginals of p(z|x). Greedy search of [6] is not included since it only provides
MAP  not marginals. The x-axis shows the percentage of sum nodes in the graph. Just like in the
MAP case  our hybrid method consistently produces the smallest KL-divergence compared to others.

When the computation of the truth is intractable  the loglikelihood of the assignment can be approx-
imated by the log-partition function with Bethe approximation according to Sec. 3.2. Note that this
is exact on trees. Here  we use a 50-node tree with binary node states and 8 × 10 grid with various
states 1 ≤ |Ys| ≤ 20. On the grid graph  we apply tree-reweighted sum or max product [14  13] 
and our hybrid version based on TRBP. For the edge appearance probability in TRBP  we apply a
common approach that use a greedy algorithm to ﬁnding the spanning trees with as many uncovered
edges as possible until all the edges in the graph are covered at least once. Even if the message-
passing algorithms are not guaranteed to converge on loopy graphs  we can still compare the best
result they provide after a certain number of iterations

Fig. 4 presents the results. In the tree case  as expected  using hybrid message-passing algorithms’s
result to initialize the local search algorithm performs the best. On the grid graph  the local search
algorithm initialized by the sum product results works well when there are few max nodes  but as
the search space grows exponentially with the number of max nodes  so it takes hundreds of steps to
ﬁnd the optimum. On the other hand  because the hybrid TRBP starts in a good area  it consistently
achieves the highest likelihood among all four algorithms with fewer extra steps.
6.2 Real-world Data
We then experiment with the protein side-chain pre-
diction dataset [23  24] which consists a set of pro-
tein structures for which we need to ﬁnd lowest en-
ergy assignment for rotamer residues. There are two
sets of residues: core residues and surface residues.
The core residues are the residues which are con-
nected to more than 19 other residues  and the sur-
face ones are the others. Since the MAP results are
usually lower on the surface residues than the core
residues nodes [24]  we choose the surface residues
to be max nodes and the core nodes to be the sum
nodes. The ground truth is given by the maximum
likelihood assignment of the residues  so we do not
expect to have a better results on the core nodes  but
we hope that any improvement on the accuracy of the surface nodes can make up the loss on the
core nodes and thus give a better performance overall. As shown in Table 6.2  the improvements of
the hybrid methods on the surface nodes are more than the loss the the core nodes  thus improving
the overall performance.

Table 1: Accuracy on the 1st  the 1st & 2rd
Angles
χ1
sum product
max product
hybrid
TRBP
hybrid TRBP
χ1 ∧ χ2
sum product
max product
hybrid
TRBP
hybrid TRBP

CORE
0.8325
0.8336
0.8336
0.8364
0.8359
CORE
0.7005
0.7078
0.7033
0.7174
0.7186

ALL
0.7900
0.7900
0.7910
0.7942
0.7950
ALL
0.6482
0.6512
0.6485
0.6592
0.6597

0.7564
0.7555
0.7573
0.7608
0.7626

0.6069
0.6064
0.6051
0.6112
0.6140

SURFACE

SURFACE

8

References
[1] Shankar Kumar  William Byrne  and Speech Processing. Minimum bayes-risk decoding for statistical

machine translation. In HLT-NAACL  2004.

[2] David Sontag and Tommi Jaakkola. New outer bounds on the marginal polytope.

Neural Information Processing Systems  2007.

In In Advances in

[3] Amir Globerson and Tommi Jaakkola. Fixing max-product: Convergent message passing algorithms for

map lp-relaxations. In NIPS  2007.

[4] Pradeep Ravikumar  Alekh Agarwal  and Martin J. Wainwright. Message-passing for graph-structured

linear programs: proximal projections  convergence and rounding schemes. In ICML  2008.

[5] Qiang Liu and Alexander Ihler. Variational algorithms for marginal map. In UAI  2011.
[6] James D. Park. MAP Complexity Results and Approximation Methods. In UAI  2002.
[7] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press 

2009.

[8] Shaul K. Bar-Lev  Daoud Bshouty  Peter Enis  Gerard Letac  I-Li Lu  and Donald Richards. The diagnonal
multivariate natural exponential families and their classiﬁcation. In Journal of Theoretical Probability 
pages 883–929  1994.

[9] Vaibhava Goel and William J. Byrne. Minimum Bayes-risk automatic speech recognition. Computer

Speech and Language  14(2)  2000.

[10] M. J. Wainwright and M. I. Jordan. Graphical Models  Exponential Families  and Variational Inference.

Foundations and Trends in Machine Learning  2008.

[11] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan

Kaufmann Publishers Inc.  San Francisco  CA  USA  1988.

[12] Jonathan S. Yedidia  William T. Freeman  and Yair Weiss. Generalized belief propagation. In NIPS  2000.
[13] Martin J. Wainwright  Tommi S. Jaakkola  and Alan S. Willsky. Exact map estimates by tree agreement.

In NIPS  2002.

[14] Martin J. Wainwright  Tommi S. Jaakkola  and Alan S. Willsky. Tree-reweighted belief propagation

algorithms and approximate ml estimation by pseudo-moment matching. In AISTATS  2003.

[15] Mark Johnson. Why doesnt em ﬁnd good hmm pos-taggers. In EMNLP  pages 296–305  2007.
[16] Pradeep Ravikumar  Martin J. Wainwright  and Alekh Agarwal. Message-passing for graph-structured

linear programs: Proximal methods and rounding schemes  2008.

[17] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM

algorithm. Journal of The Royal Statistica Society  1977.

[18] Radford M. Neal and Geoffrey E. Hinton. A View of the EM Algorithm that Justiﬁes Incremental  Sparse 

and Other Variants. In Learning in graphical models  pages 355–368  1999.

[19] Slav Petrov and Dan Klein. Discriminative log-linear grammars with latent variables. In NIPS  2008.
[20] Ivan Titov and James Henderson. A latent variable model for generative dependency parsing. In IWPT 

2007.

[21] Ben Taskar  Vassil Chatalbashev  Daphne Koller  and Carlos Guestrin. Learning structured prediction

models: a large margin approach. 2004.

[22] Ioannis Tsochantaridis  Google Inc  Thorsten Joachims  Thomas Hofmann  Yasemin Altun  and Yoram
Singer. Large margin methods for structured and interdependent output variables. Journal of Machine
Learning Research  6:1453–1484  2005.

[23] Chen Yanover  Talya Meltzer  and Yair Weiss. Linear programming relaxations and belief propagation –

an empirical study. Journal of Machine Learning Research  7:1887–1907  2006.

[24] Chen Yanover  Ora Schueler-furman  and Yair Weiss. Minimizing and learning energy functions for

side-chain prediction. In RECOMB2007  2007.

9

,Ichiro Takeuchi
Tatsuya Hongo
Masashi Sugiyama
Shinichi Nakajima