2011,Variational Gaussian Process Dynamical Systems,High dimensional time series are endemic in applications of machine learning  such as robotics (sensor data)  computational biology (gene expression data)  vision   (video sequences) and graphics (motion capture data). Practical nonlinear  probabilistic approaches to this data are required. In this paper we introduce  the variational Gaussian process dynamical system. Our work builds on recent  variational approximations for Gaussian process latent variable models to allow  for nonlinear dimensionality reduction simultaneously with learning a dynamical  prior in the latent space. The approach also allows for the appropriate dimensionality   of the latent space to be automatically determined. We demonstrate the  model on a human motion capture data set and a series of high resolution video  sequences.,Variational Gaussian Process Dynamical Systems

Andreas C. Damianou∗

Department of Computer Science

University of Shefﬁeld  UK

andreas.damianou@sheffield.ac.uk

Michalis K. Titsias

School of Computer Science
University of Manchester  UK

mtitsias@gmail.com

Neil D. Lawrence∗

Department of Computer Science

University of Shefﬁeld  UK

N.Lawrence@dcs.shef.ac.uk

Abstract

High dimensional time series are endemic in applications of machine learning such as robotics
(sensor data)  computational biology (gene expression data)  vision (video sequences) and
graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are
required. In this paper we introduce the variational Gaussian process dynamical system. Our
work builds on recent variational approximations for Gaussian process latent variable models
to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical
prior in the latent space. The approach also allows for the appropriate dimensionality of the
latent space to be automatically determined. We demonstrate the model on a human motion
capture data set and a series of high resolution video sequences.

1

Introduction

Nonlinear probabilistic modeling of high dimensional time series data is a key challenge for the machine learn-
ing community. A standard approach is to simultaneously apply a nonlinear dimensionality reduction to the
data whilst governing the latent space with a nonlinear temporal prior. The key difﬁculty for such approaches is
that analytic marginalization of the latent space is typically intractable. Markov chain Monte Carlo approaches
can also be problematic as latent trajectories are strongly correlated making efﬁcient sampling a challenge. One
promising approach to these time series has been to extend the Gaussian process latent variable model [1  2]
with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent
points [3  4  5]. Ko and Fox [6] further extend these models for fully Bayesian ﬁltering in a robotics setting. We
refer to this class of dynamical models based on the GP-LVM as Gaussian process dynamical systems (GPDS).
However  the use of a MAP approximation for training these models presents key problems. Firstly  since the
latent variables are not marginalised  the parameters of the dynamical prior cannot be optimized without the
risk of overﬁtting. Further  the dimensionality of the latent space cannot be determined by the model: adding
further dimensions always increases the likelihood of the data. In this paper we build on recent developments
in variational approximations for Gaussian processes [7  8] to introduce a variational Gaussian process dynami-
cal system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous
lower bound on the marginal likelihood. As well as providing a principled approach to handling uncertainty in
the latent space  this allows both the parameters of the latent dynamical process and the dimensionality of the
latent space to be determined. The approximation enables the application of our model to time series containing
millions of dimensions and thousands of time points. We illustrate this by modeling human motion capture data
and high dimensional video sequences.

∗Also at the Shefﬁeld Institute for Translational Neuroscience  University of Shefﬁeld  UK.

1

2 The Model
Assume a multivariate times series dataset {yn  tn}N
n=1  where yn ∈ RD is a data vector observed at time
tn ∈ R+. We are especially interested in cases where each yn is a high dimensional vector and  therefore 
we assume that there exists a low dimensional manifold that governs the generation of the data. Speciﬁcally  a
temporal latent function x(t) ∈ RQ (with Q (cid:28) D)  governs an intermediate hidden layer when generating the
data  and the dth feature from the data vector yn is then produced from xn = x(tn) according to

nd ∼ N (0  β−1) 

ynd = fd(xn) + nd  

(1)
where fd(x) is a latent mapping from the low dimensional space to dth dimension of the observation space
and β is the inverse variance of the white Gaussian noise. We do not want to make strong assumptions about
the functional form of the latent functions (x  f ).1 Instead we would like to infer them in a fully Bayesian
non-parametric fashion using Gaussian processes [9]. Therefore  we assume that x is a multivariate Gaussian
process indexed by time t and f is a different multivariate Gaussian process indexed by x  and we write

xq(t) ∼ GP(0  kx(ti  tj))  q = 1  . . .   Q 
fd(x) ∼ GP(0  kf (xi  xj))  d = 1  . . .   D.

(2)
(3)
Here  the individual components of the latent function x are taken to be independent sample paths drawn from
a Gaussian process with covariance function kx(ti  tj). Similarly  the components of f are independent draws
from a Gaussian process with covariance function kf (xi  xj). These covariance functions  parametrized by
parameters θx and θf respectively  play very distinct roles in the model. More precisely  kx determines the
properties of each temporal latent function xq(t). For instance  the use of an Ornstein-Uhlbeck covariance
function yields a Gauss-Markov process for xq(t)  while the squared-exponential covariance function gives rise
to very smooth and non-Markovian processes. In our experiments  we will focus on the squared exponential
covariance function (RBF)  the Mat´ern 3/2 which is only once differentiable  and a periodic covariance function
[9  10] which can be used when data exhibit strong periodicity. These covariance functions take the form:

kx(rbf) (ti   tj ) = σ2

rbfe

− (ti−tj )2
(2l2
t )

  kx(mat) (ti  tj) = σ2
mat

1 +

−√

e

3|ti−tj|
lt

 

(cid:32)

(cid:33)

√

3|ti − tj|

lt

− 1

T (ti−tj ))
sin2( 2π
lt

kx(per) (ti   tj ) = σ2

(4)
The covariance function kf determines the properties of the latent mapping f that maps each low dimensional
variable xn to the observed vector yn. We wish this mapping to be a non-linear but smooth  and thus a suitable
choice is the squared exponential covariance function

pere

.

2

 

kf (xi  xj) = σ2

(5)
which assumes a different scale wq for each latent dimension. This  as in the variational Bayesian formulation
of the GP-LVM [8]  enables an automatic relevance determination procedure (ARD)  i.e. it allows Bayesian
training to “switch off” unnecessary dimensions by driving the values of the corresponding scales to zero.
The matrix Y ∈ RN×D will collectively denote all observed data so that its nth row corresponds to the data
point yn. Similarly  the matrix F ∈ RN×D will denote the mapping latent variables  i.e. fnd = fd(xn)  asso-
ciated with observations Y from (1). Analogously  X ∈ RN×Q will store all low dimensional latent variables
xnq = xq(tn). Further  we will refer to columns of these matrices by the vectors yd  fd  xq ∈ RN . Given the
latent variables we assume independence over the data features  and given time we assume independence over
latent dimensions to give

(cid:80)Q
q=1 wq(xi q−xj  q )2

arde− 1

2

D(cid:89)

d=1

Q(cid:89)

q=1

p(Y  F  X|t) = p(Y |F )p(F|X)p(X|t) =

p(yd|fd)p(fd|X )

p(xq|t) 

(6)

where t ∈ RN and p(yd|fd) is a Gaussian likelihood function term deﬁned from (1). Further  p(fd|X ) is a
marginal GP prior such that

p(fd|X ) = N (fd|0  KNN ) 

(7)

1To simplify our notation  we often write x instead of x(t) and f instead of f (x). Later we also use a similar convention

for the covariance functions by often writing them as kf and kx.

2

where KN N = kf (X  X) is the covariance matrix deﬁned by the covariance function kf and similarly p(xq|t)
is the marginal GP prior associated with the temporal function xq(t) 
p(xq|t) = N (xq|0  Kt )  

(8)
where Kt = kx(t  t) is the covariance matrix obtained by evaluating the covariance function kx on the observed
times t.
Bayesian inference using the above model poses a huge computational challenge as  for instance  marginaliza-
tion of the variables X  that appear non-linearly inside the covariance matrix KN N   is troublesome. Practical
approaches that have been considered until now (e.g. [5  3]) marginalise out only F and seek a MAP solution
for X. In the next section we describe how efﬁcient variational approximations can be applied to marginalize
X by extending the framework of [8].

2.1 Variational Bayesian training
The key difﬁculty with the Bayesian approach is propagating the prior density p(X|t) through the nonlinear
mapping. This mapping gives the expressive power to the model  but simultaneously renders the associated
marginal likelihood 

p(Y |t) =

p(Y |F )p(F|X)p(X|t)dXdF 

(9)

intractable. We now invoke the variational Bayesian methodology to approximate the integral. Following a
standard procedure [11]  we introduce a variational distribution q(Θ) and compute the Jensen’s lower bound Fv
on the logarithm of (9) 

Fv(q  θ) =

q(Θ) log

p(Y |F )p(F|X)p(X|t)

q(Θ)

dXdF 

(10)

where θ denotes the model’s parameters. However  the above form of the lower bound is problematic because
X (in the GP term p(F|X)) appears non-linearly inside the covariance matrix KN N making the integration
over X difﬁcult. As shown in [8]  this intractability is removed by applying the “data augmentation” principle.
More precisely  we augment the joint probability model in (6) by including M extra samples of the GP latent
mapping f  known as inducing points  so that um ∈ RD is such a sample. The inducing points are evaluated at
a set of pseudo-inputs ˜X ∈ RM×Q. The augmented joint probability density takes the form

(cid:90)

(cid:90)

p(Y  F  U  X  ˜X|t) =

p(yd|fd)p(fd|ud  X )p(ud| ˜X)p(X|t) 

(11)

D(cid:89)

d=1

where p(ud| ˜X) is a zero-mean Gaussian with a covariance matrix KMM constructed using the same function
as for the GP prior (7). By dropping ˜X from our expressions  we write the augmented GP prior analytically
(see [9]) as

p(fd|ud  X) = N(cid:0)fd|KNM K−1

(12)
A key result in [8] is that a tractable lower bound (computed analogously to (10)) can be obtained through the
variational density

MM KM N

MM ud  KN N − KNM K−1

(cid:1) .

q(Θ) = q(F  U  X) = q(F|U  X)q(U )q(X) =

p(fd|ud  X)q(ud)q(X) 

(13)

D(cid:89)

d=1

q=1 N (xq|µq  Sq) and q(ud) is an arbitrary variational distribution. Titsias and Lawrence [8]
assume full independence for q(X) and the variational covariances are diagonal matrices. Here  in contrast  the
posterior over the latent variables will have strong correlations  so Sq is taken to be a N × N full covariance
matrix. Optimization of the variational lower bound provides an approximation to the true posterior p(X|Y )
by q(X). In the augmented probability model  the “difﬁcult” term p(F|X) appearing in (10) is now replaced
with (12) and  eventually  it cancels out with the ﬁrst factor of the variational distribution (13) so that F can be
marginalised out analytically. Given the above and after breaking the logarithm in (10)  we obtain the ﬁnal form
of the lower bound (see supplementary material for more details)

where q(X) =(cid:81)Q

Fv(q  θ) = ˆFv − KL(q(X) (cid:107) p(X|t)) 

(14)

3

with ˆFv = (cid:82) q(X) log p(Y |F )p(F|X) dXdF . Both terms in (14) are now tractable. Note that the ﬁrst of

the above terms involves the data while the second one only involves the prior. All the information regarding
data point correlations is captured in the KL term and the connection with the observations comes through the
variational distribution. Therefore  the ﬁrst term in (14) has the same analytical solution as the one derived in
[8]. Equation (14) can be maximized by using gradient-based methods2. However  when not factorizing q(X)
across data points yields O(N 2) variational parameters to optimize. This issue is addressed in the next section.

2.2 Reparametrization and Optimization
The optimization involves the model parameters θ = (β  θf   θx)  the variational parameters {µq  Sq}Q
q(X) and the inducing points3 ˜X.
Optimization of the variational parameters appears challenging  due to their large number and the correlations

between them. However  by reparametrizing our O(cid:0)N 2(cid:1) variational parameters according to the framework

q=1 from

described in [12] we can obtain a set of O(N ) less correlated variational parameters. Speciﬁcally  we ﬁrst take
the derivatives of the variational bound (14) w.r.t. Sq and µq and set them to zero  to ﬁnd the stationary points 

Sq =(cid:0)K −1

(cid:1)−1

(15)
where Λq = −2 ϑ ˆFv (q θ)
is a N−dimensional vector.
The above stationary conditions tell us that  since Sq depends on a diagonal matrix Λq  we can reparametrize it
using only the N−dimensional diagonal of that matrix  denoted by λq. Then  we can optimise the 2(Q × N )
parameters (λq  ¯µq) and obtain the original parameters using (15).

is a N × N diagonal  positive matrix and ¯µq = ϑ ˆFv

and µq = Kt ¯µq 

t + Λq

ϑµq

ϑSq

2.3 Learning from Multiple Sequences

Our objective is to model multivariate time series. A given data set may consist of a group of independent ob-
served sequences  each with a different length (e.g. in human motion capture data several walks from a subject).

Let  for example  the dataset be a group of S independent sequences(cid:0)Y (1)  ...  Y (S)(cid:1). We would like our model
i.e. p(cid:0)X (1)  X (2)  ...  X (S)(cid:1) =(cid:81)S

to capture the underlying commonality of these data. We handle this by allowing a different temporal latent
function for each of the independent sequences  so that X (s) is the set of latent variables corresponding to the
sequence s. These sets are a priori assumed to be independent since they correspond to separate sequences 
s=1 p(X (s))  where we dropped the conditioning on time for simplicity. This
factorisation leads to a block-diagonal structure for the time covariance matrix Kt  where each block corre-
sponds to one sequence. In this setting  each block of observations Y (s) is generated from its corresponding
X (s) according to Y (s) = F (s) +   where the latent function which governs this mapping is shared across all
sequences and  is Gaussian noise.

3 Predictions

Our algorithm models the temporal evolution of a dynamical system. It should be capable of generating com-
pletely new sequences or reconstructing missing observations from partially observed data. For generating
a novel sequence given training data the model requires a time vector t∗ as input and computes a density
p(Y∗|Y  t  t∗). For reconstruction of partially observed data the time-stamp information is additionally ac-
companied by a partially observed sequence Y p∗ ∈ RN∗×Dp from the whole Y∗ = (Y p∗   Y m∗ )  where p and
m are set indices indicating the present (i.e. observed) and missing dimensions of Y∗ respectively  so that
p∪ m = {1  . . .   D}. We reconstruct the missing dimensions by computing the Bayesian predictive distribution
p(Y m∗ |Y p∗   Y  t∗  t). The predictive densities can also be used as estimators for tasks like generative Bayesian
classiﬁcation. Whilst time-stamp information is always provided  in the next section we drop its dependence to
avoid notational clutter.

2See supplementary material for more detailed derivation of (14) and for the equations for the gradients.
3We will use the term “variational parameters” to refer only to the parameters of q(X) although the inducing points are

also variational parameters.

4

3.1 Predictions Given Only the Test Time Points
To approximate the predictive density  we will need to introduce the underlying latent function values F∗ ∈
RN∗×D (the noisy-free version of Y∗) and the latent variables X∗ ∈ RN∗×Q. We write the predictive density as
(16)

p(Y∗|F∗)p(F∗|X∗  Y )p(X∗|Y )dF∗dX∗.

p(Y∗  F∗  X∗|Y )dF∗dX∗ =

p(Y∗|Y ) =

(cid:90)

(cid:90)

The term p(F∗|X∗  Y ) is approximated by the variational distribution
p(f∗ d|ud  X∗)q(ud)dud =

q(F∗|X∗) =

(17)
where q(f∗ d|X∗) is a Gaussian that can be computed analytically  since in our variational framework the optimal
setting for q(ud) is also found to be a Gaussian (see suppl. material for complete forms). As for the term
p(X∗|Y ) in eq. (16)  it is approximated by a Gaussian variational distribution q(X∗) 

d∈D

d∈D

q(f∗ d|X∗) 

(cid:90) (cid:89)

q(X∗) =

q(x∗ q) =

p(x∗ q|xq)q(xq)dxq =

(cid:104)p(x∗ q|xq)(cid:105)q(xq)  

(18)

Q(cid:89)

q=1

(cid:90)

Q(cid:89)

q=1

(cid:89)

Q(cid:89)

q=1

where p(x∗ q|xq) is a Gaussian found from the conditional GP prior (see [9]) and q(X) is also Gaussian. We
can  thus  work out analytically the mean and variance for (18)  which turn out to be:

µx∗ q = K∗N ¯µq

(19)
(20)
where K∗N = kx(t∗  t)  K∗N = K(cid:62)
∗N and K∗∗ = kx(t∗  t∗). Notice that these equations have exactly the
same form as found in standard GP regression problems. Once we have analytic forms for the posteriors in (16) 
the predictive density is approximated as

var(x∗ q) = K∗∗ − K∗N (Kt + Λ−1

q )−1KN∗

p(Y∗|Y ) =

p(Y∗|F∗)q(F∗|X∗)q(X∗)dF∗dX∗ =

p(Y∗|F∗)(cid:104)q(F∗|X∗)(cid:105)q(X∗) dF∗ 

(21)

(cid:90)

(cid:90)

E(F∗) = B(cid:62)Ψ∗

Cov(F∗) = B(cid:62)(cid:0)Ψ∗

1

which is a non-Gaussian integral that cannot be computed analytically. However  following the same argument
as in [9  13]  we can calculate analytically its mean and covariance:

1)(cid:62)(cid:1) B + Ψ∗

(cid:104)(cid:16)
−1(cid:17)
MM − (KMM + βΨ2)
K−1
0 = (cid:104)kf (X∗  X∗)(cid:105)  Ψ∗
1 = (cid:104)KM∗(cid:105) and Ψ∗

0I − Tr

2 − Ψ∗
−1 Ψ(cid:62)

1(Ψ∗
1 Y   Ψ∗

(23)
2 = (cid:104)KM∗K∗M(cid:105). All
where B = β (KMM + βΨ2)
expectations are taken w.r.t. q(X∗) and can be calculated analytically  while KM∗ denotes the cross-covariance
matrix between the training inducing inputs ˜X and X∗. The Ψ quantities are calculated analytically (see suppl.
material). Finally  since Y∗ is just a noisy version of F∗  the mean and covariance of (21) is just computed as:
E(Y∗) = E(F∗) and Cov(Y∗) = Cov(F∗) + β−1IN∗.

I 

2

(22)

Ψ∗

(cid:105)

3.2 Predictions Given the Test Time Points and Partially Observed Outputs
The expression for the predictive density p(Y m∗ |Y p∗   Y ) is similar to (16) 

(cid:90)

p(Y m∗ |Y p∗   Y ) =

p(Y m∗ |F m∗ )p(F m∗ |X∗  Y p∗   Y )p(X∗|Y p∗   Y )dF m∗ dX∗ 

(24)

and is analytically intractable. To obtain an approximation  we ﬁrstly need to apply variational inference and
approximate p(X∗|Y p∗   Y ) with a Gaussian distribution. This requires the optimisation of a new variational
lower bound that accounts for the contribution of the partially observed data Y p∗ . This lower bound approximates
the true marginal likelihood p(Y p∗   Y ) and has exactly analogous form with the lower bound computed only on
the training data Y . Moreover  the variational optimisation requires the deﬁnition of the variational distribution
q(X∗  X) which needs to be optimised and is fully correlated across X and X∗. After the optimisation  the
approximation to the true posterior p(X∗|Y p∗   Y ) is given from the marginal q(X∗). A much faster but less
accurate method would be to decouple the test from the training latent variables by imposing the factorisation
q(X∗  X) = q(X)q(X∗). This is not used  however  in our current implementation.

5

4 Handling Very High Dimensional Datasets

Our variational framework avoids the typical cubic complexity of Gaussian processes allowing relatively large
training sets (thousands of time points  N). Further  the model scales only linearly with the number of dimen-
sions D. Speciﬁcally  the number of dimensions only matters when performing calculations involving the data
matrix Y . In the ﬁnal form of the lower bound (and consequently in all of the derived quantities  such as gra-
dients) this matrix only appears in the form Y Y (cid:62) which can be precomputed. This means that  when N (cid:28) D 
we can calculate Y Y (cid:62) only once and then substitute Y with the SVD (or Cholesky decomposition) of Y Y (cid:62). In
this way  we can work with an N × N instead of an N × D matrix. Practically speaking  this allows us to work
with data sets involving millions of features. In our experiments we model directly the pixels of HD quality
video  exploiting this trick.

5 Experiments

We consider two different types of high dimensional time series  a human motion capture data set consisting
of different walks and high resolution video sequences. The experiments are intended to explore the various
properties of the model and to evaluate its performance in different tasks (prediction  reconstruction  generation
of data). Matlab source code for repeating the following experiments and links to the video ﬁles are available
on-line from http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/vargplvm/.

5.1 Human Motion Capture Data

We followed [14  15] in considering motion capture data of walks and runs taken from subject 35 in the CMU
motion capture database. We treated each motion as an independent sequence. The data set was constructed and
preprocessed as described in [15]. This results in 2 613 separate 59-dimensional frames split into 31 training
sequences with an average length of 84 frames each.
The model is jointly trained  as explained in section 2.3  on both walks and runs  i.e. the algorithm learns a
common latent space for these motions. At test time we investigate the ability of the model to reconstruct test
data from a previously unseen sequence given partial information for the test targets. This is tested once by
providing only the dimensions which correspond to the body of the subject and once by providing those that
correspond to the legs. We compare with results in [15]  which used MAP approximations for the dynamical
models  and against nearest neighbour. We can also indirectly compare with the binary latent variable model
(BLV) of [14] which used a slightly different data preprocessing. We assess the performance using the cumu-
lative error per joint in the scaled space deﬁned in [14] and by the root mean square error in the angle space
suggested by [15]. Our model was initialized with nine latent dimensions. We performed two runs  once using
the Mat´ern covariance function for the dynamical prior and once using the RBF. From table 1 we see that the
variational Gaussian process dynamical system considerably outperforms the other approaches. The appropriate
latent space dimensionality for the data was automatically inferred by our models. The model which employed
an RBF covariance to govern the dynamics retained four dimensions  whereas the model that used the Mat´ern
kept only three. The other latent dimensions were completely switched off by the ARD parameters. The best
performance for the legs and the body reconstruction was achieved by the VGPDS model that used the Mat´ern
and the RBF covariance function respectively.

5.2 Modeling Raw High Dimensional Video Sequences

For our second set of experiments we considered video sequences. Such sequences are typically preprocessed
before modeling to extract informative features and reduce the dimensionality of the problem. Here we work
directly with the raw pixel values to demonstrate the ability of the VGPDS to model data with a vast number of
features. This also allows us to directly sample video from the learned model.
Firstly  we used the model to reconstruct partially observed frames from test video sequences4. For the ﬁrst
video discussed here we gave as partial information approximately 50% of the pixels while for the other two
we gave approximately 40% of the pixels on each frame. The mean squared error per pixel was measured to

4‘Missa’ dataset: cipr.rpi.edu. ‘Ocean’: cogﬁlms.com. ‘Dog’: ﬁtfurlife.com. See details in supplementary. The logo

appearing in the ‘dog’ images in the experiments that follow  has been added with post-processing.

6

Table 1: Errors obtained for the motion capture dataset considering nearest neighbour in the angle space (NN) and in the
scaled space(NN sc.)  GPLVM  BLV and VGPDS. CL / CB are the leg and body datasets as preprocessed in [14]  L and B
the corresponding datasets from [15]. SC corresponds to the error in the scaled space  as in Taylor et al. while RA is the
error in the angle space. The best error per column is in bold.

Data

Error Type

BLV
NN sc.

VGPDS (RBF)

VGPDS (Mat´ern 3/2)

GPLVM (Q = 3)
GPLVM (Q = 4)
GPLVM (Q = 5)

NN sc.

NN

CL
SC
11.7
22.2

-
-
-
-
-
-
-

CB
SC
8.8
20.5

-
-
-
-
-
-
-

L
SC
-
-

11.4
9.7
13.4
13.5
14.0
8.19
6.99

L
RA
-
-

3.40
3.38
4.25
4.44
4.11
3.57
2.88

B
SC
-
-

16.9
20.7
23.4
20.8
30.9
10.73
14.22

B
RA
-
-

2.49
2.72
2.78
2.62
3.20
1.90
2.23

compare with the k−nearest neighbour (NN) method  for k ∈ (1  ..  5) (we only present the error achieved
for the best choice of k in each case). The datasets considered are the following: ﬁrstly  the ‘Missa’ dataset 
a standard benchmark used in image processing. This is a 103 680-dimensional video  showing a woman
talking for 150 frames. The data is challenging as there are translations in the pixel space. We also considered
an HD video of dimensionality 9 × 105 that shows an artiﬁcially created scene of ocean waves as well as
a 230  400−dimensional video showing a dog running for 60 frames. The later is approximately periodic in
nature  containing several paces from the dog. For the ﬁrst two videos we used the Mat´ern and RBF covariance
functions respectively to model the dynamics and interpolated to reconstruct blocks of frames chosen from the
whole sequence. For the ‘dog’ dataset we constructed a compound kernel kx = kx(rbf) + kx(periodic)  where
the RBF term is employed to capture any divergence from the approximately periodic pattern. We then used
our model to reconstruct the last 7 frames extrapolating beyond the original video. As can be seen in table
2  our method outperformed NN in all cases. The results are also demonstrated visually in ﬁgure 1 and the
reconstructed videos are available in the supplementary material.

Table 2: The mean squared error per pixel for VGPDS and NN for the three datasets (measured only in the missing inputs).
The number of latent dimensions selected by our model is in parenthesis.

VGPDS

NN

Missa
2.52 (Q = 12)
2.63

Ocean
9.36 (Q = 9)
9.53

Dog
4.01 (Q = 6)
4.15

As can be seen in ﬁgure 1  VGPDS predicts pixels which are smoothly connected with the observed part of the
image  whereas the NN method cannot ﬁt the predicted pixels in the overall context.
As a second task  we used our generative model to create new samples and generate a new video sequence. This
is most effective for the ‘dog’ video as the training examples were approximately periodic in nature. The model
was trained on 60 frames (time-stamps [t1  t60]) and we generated new frames which correspond to the next 40
time points in the future. The only input given for this generation of future frames was the time-stamp vector 
[t61  t100]. The results show a smooth transition from training to test and amongst the test video frames. The
resulting video of the dog continuing to run is sharp and high quality. This experiment demonstrates the ability
of the model to reconstruct massively high dimensional images without blurring. Frames from the result are
shown in ﬁgure 2. The full video is available in the supplementary material.

6 Discussion and Future Work

We have introduced a fully Bayesian approach for modeling dynamical systems through probabilistic nonlinear
dimensionality reduction. Marginalizing the latent space and reconstructing data using Gaussian processes

7

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 1:
(a) and (c) demonstrate the reconstruction achieved by VGPDS and NN respectively for the most challenging
frame (b) of the ‘missa’ video  i.e. when translation occurs. (d) shows another example of the reconstruction achieved by
VGPDS given the partially observed image. (e) (VGPDS) and (f) (NN) depict the reconstruction achieved for a frame of
the ‘ocean’ dataset. Finally  we demonstrate the ability of the model to automatically select the latent dimensionality by
showing the initial lengthscales (ﬁg: (g)) of the ARD covariance function and the values obtained after training (ﬁg: (h)) on
the ‘dog’ data set.

(a)

(b)

(c)

Figure 2: The last frame of the training video (a) is smoothly followed by the ﬁrst frame (b) of the generated video. A
subsequent generated frame can be seen in (c).

results in a very generic model for capturing complex  non-linear correlations even in very high dimensional
data  without having to perform any data preprocessing or exhaustive search for deﬁning the model’s structure
and parameters.
Our method’s effectiveness has been demonstrated in two tasks; ﬁrstly  in modeling human motion capture data
and  secondly  in reconstructing and generating raw  very high dimensional video sequences. A promising future
direction to follow would be to enhance our formulation with domain-speciﬁc knowledge encoded  for example 
in more sophisticated covariance functions or in the way that data are being preprocessed. Thus  we can obtain
application-oriented methods to be used for tasks in areas such as robotics  computer vision and ﬁnance.

Acknowledgments

Research was partially supported by the University of Shefﬁeld Moody endowment fund and the Greek State
Scholarships Foundation (IKY). We also thank Colin Litster and “Fit Fur Life” for allowing us to use their video
ﬁles as datasets. Finally  we thank the reviewers for their insightful comments.

8

References

[1] N. D. Lawrence  “Probabilistic non-linear principal component analysis with Gaussian process latent vari-

able models ” Journal of Machine Learning Research  vol. 6  pp. 1783–1816  2005.

[2] N. D. Lawrence  “Gaussian process latent variable models for visualisation of high dimensional data ” in

Advances in Neural Information Processing Systems  pp. 329–336  MIT Press  2004.

[3] J. M. Wang  D. J. Fleet  and A. Hertzmann  “Gaussian process dynamical models ” in NIPS  pp. 1441–

1448  MIT Press  2006.

[4] J. M. Wang  D. J. Fleet  and A. Hertzmann  “Gaussian process dynamical models for human motion ”

IEEE Transactions on Pattern Analysis and Machine Intelligence  vol. 30  pp. 283–298  Feb. 2008.

[5] N. D. Lawrence  “Hierarchical Gaussian process latent variable models ” in Proceedings of the Interna-

tional Conference in Machine Learning  pp. 481–488  Omnipress  2007.

[6] J. Ko and D. Fox  “GP-BayesFilters: Bayesian ﬁltering using Gaussian process prediction and observation

models ” Auton. Robots  vol. 27  pp. 75–90  July 2009.

[7] M. K. Titsias  “Variational learning of inducing variables in sparse Gaussian processes ” in Proceedings of
the Twelfth International Conference on Artiﬁcial Intelligence and Statistics  vol. 5  pp. 567–574  JMLR
W&CP  2009.

[8] M. K. Titsias and N. D. Lawrence  “Bayesian Gaussian process latent variable model ” in Proceedings
of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics  pp. 844–851  JMLR
W&CP 9  2010.

[9] C. E. Rasmussen and C. Williams  Gaussian Processes for Machine Learning. MIT Press  2006.
[10] D. J. C. MacKay  “Introduction to Gaussian processes ” in Neural Networks and Machine Learning (C. M.

Bishop  ed.)  NATO ASI Series  pp. 133–166  Kluwer Academic Press  1998.

[11] C. M. Bishop  Pattern Recognition and Machine Learning (Information Science and Statistics). Springer 

1st ed. 2006. corr. 2nd printing ed.  Oct. 2007.

[12] M. Opper and C. Archambeau  “The variational Gaussian approximation revisited ” Neural Computation 

vol. 21  no. 3  pp. 786–792  2009.

[13] A. Girard  C. E. Rasmussen  J. Qui˜nonero-Candela  and R. Murray-Smith  “Gaussian process priors with
uncertain inputs - application to multiple-step ahead time series forecasting ” in Neural Information Pro-
cessing Systems  2003.

[14] G. W. Taylor  G. E. Hinton  and S. Roweis  “Modeling human motion using binary latent variables ” in

Advances in Neural Information Processing Systems  vol. 19  MIT Press  2007.

[15] N. D. Lawrence  “Learning for larger datasets with the Gaussian process latent variable model ” in Pro-
ceedings of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics  pp. 243–250 
Omnipress  2007.

9

,Changyou Chen
Nan Ding
Lawrence Carin
Panagiotis Toulis
David Parkes