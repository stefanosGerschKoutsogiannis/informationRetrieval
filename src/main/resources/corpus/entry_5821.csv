2010,Using body-anchored priors for identifying actions in single images,This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition  because action instances may be similar in terms of body pose  and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor  forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are ‘anchored’ to the detected body parts  and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations  thereby significantly simplifying the learning process and increasing recognition performance.,Usingbody-anchoredpriorsforidentifyingactionsin

singleimages

LeonidKarlinsky

ShimonUllman

DepartmentofComputerScience
WeizmannInstituteofScience

MichaelDinerstein
Rehovot76100 Israel

{leonid.karlinsky  michael.dinerstein  shimon.ullman} @weizmann.ac.il

Abstract

Thispaperpresentsanapproachtothevisualrecognitionofhumanactionsusing
only single images as input. The task is easy for humans but difficult for current
approaches to object recognition  because instances of different actions may be
similarintermsofbodypose andoftenrequiredetailedexaminationofrelations
between participating objects and body parts in order to be recognized. The pro-
posed approach applies a two-stage interpretation procedure to each training and
test image. The first stage produces accurate detection of the relevant body parts
of the actor  forming a prior for the local evidence needed to be considered for
identifyingtheaction. Thesecondstageextractsfeaturesthatareanchoredtothe
detected body parts  and uses these features and their feature-to-part relations in
order to recognize the action. The body anchored priors we propose apply to a
largerangeofhumanactions. Thesepriorsallowfocusingontherelevantregions
andrelations therebysignificantlysimplifyingthelearningprocessandincreasing
recognitionperformance.

Introduction

1
This paper deals with the problem of recognizing transitive actions in single images. A transitive
action is often described by a transitive verb and involves a number of components  or thematic
roles [1]  including an actor  a tool  and in some cases a recipient of the action. Simple examples
aredrinkingfromaglass talkingonthephone eatingfromaplatewithaspoon orbrushingteeth.
Transitive actions are characterized visually by the posture of the actor  the tool she/he is holding 
the type of grasping  and the presence of the action recipient. In many cases  such actions can be
readily identified by human observers from only a single image (see figure 1a). We will consider
below the problem of static action recognition (SAR for short) from a single image  without using
motioninformationthatisexploitedbyapproachesdealingwithdynamicactionrecognitioninvideo
sequences  such as [2]. The problem is of interest first  because in a short observation interval  the
use of motion information for identifying an action (e.g.
talking on the phone) may be limited.
Second  as a natural human capacity  it is of interest for both cognitive and brain studies. Several
studies [3  4  5  6] have shown evidence for the presence of SAR related mechanisms in both the
ventral and dorsal areas of the visual cortex  and computational modeling of SAR may shed new
light on these mechanisms. Unlike the more common task of detecting individual objects such as
faces and cars  SAR depends on detecting object configurations. Different actions may involve
the same type of objects (eg. person  phone) but appearing in different configurations (answering 
dialing)  sometimes differing in subtle details  making their identification difficult compared with
individualobjectrecognition.
Only a few approaches to date have dealt with the SAR problem. [7] studied the recognition of
sports actions using the pose of the actor.
[8] used scene interpretation in terms of objects and

1

(a)

(b)

drinking
drinking
drinking
drinking no cup
drinking no cup
drinking no cup
eating with spoon
eating with spoon
eating with spoon
phone talking
phone talking
phone talking
phone with bottle
phone with bottle
phone with bottle
scratching
scratching
scratching
singing with mike
singing with mike
singing with mike
smoking
smoking
smoking
teeth brushing
teeth brushing
teeth brushing
toasting
toasting
toasting
waving
waving
waving
wearing glasses
wearing glasses
wearing glasses

Figure 1: (a) Examples of similar transitive actions identifiable by humans from single images
(brushing teeth  talking on a cell phone and wearing glasses). (b) Illustration of a single run of the
proposedtwo-stageapproach. Inthefirststagethepartsaredetectedintheface→hand→elbowor-
der. Inthesecondstageweapplybothactionlearningandactionrecognitionusingtheconfiguration
ofthedetectedpartsandthefeaturesanchoredtothehandregion; thebargraphontherightshows
relativelog-posteriorestimatesforthedifferentactions.
their relative configuration to distinguish between different sporting events such as badminton and
sailing. [9] recognized static intransitive actions such as walking and jumping based on a human
bodyposerepresentedbyavariantoftheHOGdescriptor. [10]discriminatedbetweenplayingand
not playing musical instruments using a star-like model. The most detailed static schemes to date
[11  12] recognized static transitive sports actions  such as the tennis forehand and the volleyball
smash. [11]usedafullbodymask bagoffeaturesfordescribingscenecontext andthedetectionof
the objects relevant for the action  such as bats  balls  etc.  while [12] learned joint models of body
poseandobjectsspecifictoeachaction. [11]usedGrabCut[13]toextractthebodymask andboth
[11]and[12]performedfullysupervisedtrainingfortheaprioriknownrelevantobjectsandscenes.
Inthispaperweconsiderthetaskofdifferentiatingbetweensimilartypesoftransitiveactions such
as smoking a cigarette  drinking from a cup  eating from a cup with a spoon  talking on the phone 
etc.  given only a single image as input. The similarity between the body poses in such actions
creates a difficulty for approaches that rely on pose analysis [7  9  11]. The relevant differences
between similar actions in terms of the actor body configuration can be at a fine level of detail.
Therefore onecannotrelyonafixedpre-determinednumberofconfigurationtypes[7 9 11];rather 
oneneedstobeabletomakeasfinediscriminationsasrequiredbythetask. Objectsparticipatingin
different actions may be very small  occupying only a few pixels in a low resolution image (brush 
phone Fig. 1a). Inaddition theseobjectsmaybeunknownapriori suchasinthenaturalcasewhen
the learning is weakly supervised  i.e. we know only the action label of the training images  while
theparticipatingobjectsarenotannotatedandcannotbeindependentlylearnedasin[8 11]. Finally 
the background scene  used by [8  11] to recognize sports actions and events  is uninformative for
manytransitiveactionsofinterest andcannotbedirectlyutilized.
Since SAR is a version of an object recognition problem  a natural question to ask is whether it
can be solved by directly applying state-of-the-art techniques of object recognition. As shown in
the results section 3  the problem is significantly more difficult for current methods compared with
morestandardobjectrecognitionapplications. Theproposedmethodidentifiesmoreaccuratelythe
featuresandgeometricrelationshipsthatcontributetocorrectrecognitioninthisdomain leadingto
better recognition. It is further shown that integrating standard object recognition approaches into
theproposedframeworksignificantlyimprovestheirresultsintheSARdomain.
The main contribution of this paper is an approach  employing the so-called body anchored strat-
egyexplainedbelow forrecognizinganddistinguishingbetweensimilartransitiveactionsinsingle
images. Inboththelearningandthetestsettings theapproachappliesatwo-stageinterpretationto
each (training or test) image. The first stage produces accurate detection and localization of body
parts  and the second then extracts and uses features from locations anchored to body parts. In the
implementationofthefirststage thefaceisdetectedfirst anditsdetectionisextendedtoaccurately
localize the elbow and the hand of the actor. In the second stage  the relative part locations and the
hand region are analyzed for action related learning and recognition. During training  this allows
theautomaticdiscoveryandconstructionofimplicitnon-parametricmodelsfordifferentimportant
aspects of the actions  such as accurate relative part locations  relevant objects  types of grasping 
and part-object configurations. During testing  this allows the approach to focus on image regions 

2

(a)

O
FH

Oo
 
fhn
HE

F

m

 1

 

nk

A

o
hen
mB
m

f

mn

(b)

Figure2: (a)Examplesofthecomputedbinarymasks(cyan)forsearchingforelbowlocationgiven
the detected hand and face marked by a red-green star and magenta rectangle respectively. The
yellow square marks the detected elbow; (b) Graphical representation (in plate notation) of the
proposedprobabilisticmodelforactionrecognition(seesection2.2fordetails).
features and relations that contain all of the relevant information for recognizing the action. As a
result weeliminatetheneedtohaveapriorimodelsfortheobjectsrelevantfortheactionthatwere
usedin[11 12]. Focusinginabody-anchoredmannerontherelevantinformationnotonlyincreases
efficiency butalsoconsiderablyenhancesrecognitionresults. Theapproachisillustratedinfig. 1b.
The rest of the paper is organized as follows. Section 2 describes the proposed approach and its
implementation details. Section 3 describes the experimental validation. Summary and discussion
areprovidedinsection4.
2 Method
Asoutlinedabove theapproachproceedsintwomainstages. Thefirststageisbodyinterpretation 
whichisbyitselfasequentialprocess. First  thepersonisdetectedbydetectingher/hisface. Next 
the face detection is extended to detect the hands and elbows of the person. This is achieved in a
non-parametric manner by following chains of features connecting the face to the part of interest
(hand elbow) byanextensionof[14]. Inthesecondstage featuresgatheredfromthehandregion
and the relative locations of the hand  face and elbow  are used to model and recognize the static
actionofinterest. Thefirststageoftheprocess dealingwiththeface handandelbowdetection is
describedinsection2.1. Thestaticactionmodelingandrecognitionisdescribedinsection2.2and
additionalimplementationdetailsareprovidedinsection2.3.
2.1 Bodypartsdetection
Body parts detection in static images is a challenging problem  which has recently been addressed
by several studies [14  15  16  17  18]. The most difficult parts to detect are the most flexible parts
of the body - the lower arms and the hands. This is due to large pose and appearance variability
and the small size typical to these parts. In our approach  we have adopted an extension of the
non-parametric method for the detection of parts of deformable objects recently proposed by [14].
This method can operate in two modes. The first mode is used for the independent detection of
sufficiently large and rigid objects and object parts  such as the face. The second mode allows
propagating from some of the parts  which are independently detected  to additional parts  which
are more difficult to detect independently  such as hands and elbows. The method extends the so-
called star model by allowing features to vote for the detection target either directly  or indirectly 
via features participating in feature-chains going towards the target. In the independent detection
mode  these feature chains may start anywhere in the image  whereas in the propagation mode
these chains must originate from already detected parts. The method employs a non-parametric
generative probabilistic model  which can efficiently learn to detect any part from a collection of
training sequences with marked target (e.g.  hand) and source (e.g.  face) parts (or only the target
parts in the independent detection mode). The details of this model are described in [14]. In our
approach thefaceisdetectedintheindependentdetectionmodeof[14] andthehandandtheelbow
aredetectedbychains-propagationfromthefacedetection(treatedasthesourcepart). Themethod
istrainedusingacollectionofshortvideosequences eachhavingtheface thehandandtheelbow
markedbythreepoints. Thecodeforthemethodof[14]wasextendedtoallowrestricteddetection
ofdependentparts suchashandandelbow. Insomecases theelbowismoredifficulttodetectthan
the hand  as it has less structure. For each (training or test) image In  we therefore constrain the
elbow detection by a binary mask of possible elbow locations gathered from training images with

3

n − xf

n   1

H = of h

n   ohe

E = ohe

n ≡ xe

n − xh

n andtheelbowby xe

n andthepatchfeatures {F m = f m

the sufficiently similar hand-face offset (within 0.25 face width) to the one detected on In. Figure
2a shows some examples of the detected faces  hands and elbows together with the elbow masks
derivedfromthedetectedface-handoffset.
2.2 Modelingandrecognitionofstaticactions
Givenanimage In (trainingortest) wefirstintroducethefollowingnotation(lowerindexrefersto
the image  upper indices to parts). Denote the instance of the action contained in In by an (known
fortrainingandunknownfortestimages). Denotethedetectedlocationsofthefaceby xf
n thehand
by xh
n. Alsodenotethewidthofthedetectedfaceby sn. Throughoutthepaper 
wewillexpressallsizeanddistanceparametersin sn units inordertoeliminatethedependenceon
thescaleofthepersonperformingtheaction. Formanytransitiveactionsmostofthediscriminating
information about the action resides in regions around specific body parts [19]. Here we focus on
hand regions for hand-related actions  but for other actions their respective parts and part regions
canbelearnedandused. Werepresenttheinformationfromthehandregionbyasetofrectangular
patchfeaturesextractedfromthisregion. Allfeaturesaretakenfromacircularregionwitharadius
0.75 ∙ sn aroundthehandlocation xh
n. Fromthisregionweextract sn × sn pixelrectangularpatch
featurescenteredatallCannyedgepointssub-sampledwitha 0.2 ∙ sn pixelgrid. Denotethesetof
patch features extracted from image In bynf m
n is
n(cid:1)io  where SIF T m
n =hSIF T m
sn(cid:0)xm
n − xf
the SIFT descriptor [20] of the m-th feature  xm
n is its image location  1
n(cid:1) is the offset
sn(cid:0)xm
(in sn units) between the feature and the face  and square brackets denote a row vector. The index
m enumerates the features in arbitrary order for each image. Denote by kn the number of patch
featuresextractedfromimage In.
The probabilistic generative model explaining all the gathered data is defined as follows. The ob-
served variables of the model are: the face-hand offset OF
n  the hand-elbow
offset OH
n }. Theunobservedvariablesofthe
modelaretheactionlabelvariable A  andthesetofbinaryvariables {Bm}  oneforeachextracted
patch feature. The meaning of Bm = 1 is that the m-th patch feature was generated by the action
A  while the meaning of Bm = 0 is that the m-th patch feature was generated independently of
A. Throughout the paper we will use a shorthand form of variable assignments  e.g.  P(cid:0)of h
n (cid:1)
instead of P(cid:0)OF
n (cid:1). We define the joint distribution of the model that generates
thedataforimage In as:
P (Bm) ∙ P(cid:0)f m
n   Bm(cid:1)(1)
P(cid:0)A {Bm}   of h
Here P (A)isaprioractiondistribution whichwetaketobeuniform and:
n (cid:1)
(2)
n (cid:1)
The P (Bm) = α  is the prior probability for the m-th feature to be generated from the action 
and we assume it maintains the following relation: P (Bm = 1) = α (cid:28) (1 − α) = P (Bm = 0)
reflectingthefactthatmostpatchfeaturesarenotrelatedtotheaction. Figure2bshowsthegraphical
representationoftheproposedmodel.
As shown in the Appendix A  in order to find the action label assignment to A that maximizes the
posterioroftheproposedprobabilisticgenerativemodel itissufficienttocompute:
n (cid:1)
n (cid:17)

(3)
As can be seen from eq. 3  and as shown in the Appendix A  the inference is independent of the
exact value of α (as long as α (cid:28) (1 − α)). In section 2.3 we explain how to empirically estimate
theprobabilities P(cid:0)f m

knYm=1
n }(cid:1) = P (A) ∙ P(cid:0)of h
n (cid:1) ∙
n (cid:12)(cid:12)A  of h
n   Bm(cid:1) =(cid:26) P(cid:0)f m
n (cid:12)(cid:12)of h
P(cid:0)f m

n (cid:12)(cid:12)A  of h
P(cid:0)f m

P(cid:0)f m
P(cid:16)f m

n   ohe
n   ohe

n   A  of h
n   of h

knXm=1
n (cid:1)thatarenecessarytocompute3.

n (cid:1)and P(cid:0)f m

n }(cid:1) = arg max

log P(cid:0)A(cid:12)(cid:12)of h

n (cid:12)(cid:12)A  of h

n ≡ xh

n − xf

H = of h

n   OH

E = ohe

if Bm = 1
otherwise

n   ohe

n  {f m

n   A  of h

n   ohe

n   of h

n   ohe

n   ohe
n   ohe

arg max

A

n   ohe

n  {f m

A

n   ohe

n   ohe

n   ohe

4

1: drinking
1: drinking

2: drinking no cup
2: drinking no cup

1
11

3: eating with spoon
3: eating with spoon

5: phone talking with bottle
5: phone talking with bottle

9: teeth brushing
9: teeth brushing

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

5
55

9
99

6: scratching head
6: scratching head

10: toasting
10: toasting

2
22

6
66

10
1010

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

7: singing with mike
7: singing with mike

11: waving
11: waving

4: phone talking
4: phone talking

8: smoking
8: smoking

12: wearing glasses
12: wearing glasses

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
12
12

3
33

7
77

11
1111

4
44

8
88

12
1212

n   A = a  of h

n   ohe

Figure 3: Examples of similar static transitive action recognition on our 12-actions / 10-people
(‘12/10’) dataset. On all examples  the detected face  hand and elbow are shown by cyan circle 
red-green star and yellow square  respectively. At the right hand side of each image  the bar graph
shows the estimated log-posterior of the action variable A. Each example shows a zoomed-in ROI
oftheaction. Additionalexamplesareprovidedinsupplementarymaterial.
2.3 Modelprobabilities
ThemodelprobabilitiesareestimatedfromthetrainingdatausingKernelDensityEstimation(KDE)
[21]. Assumewearegivenasetofsamples {Y1  . . .   YR}fromsomedistributionofinterest. Givena
newsample Y fromthesamedistribution asymmetricGaussianKDEestimate P (Y )fortheproba-
bilityof Y canbeapproximatedas: P (Y ) ≈ 1
R ∙PYr∈N N (Y ) exp(cid:16)−0.5 ∙ kY − Yrk2.σ2(cid:17)where
N N (Y ) is the set of nearest neighbors of Y within the given set of samples. When the number of
samples Rislarge brute-forcesearchforthe N N (Y )setbecomesinfeasible. Therefore weuseAp-
proximateNearestNeighbor(ANN)search(usingtheimplementationof[22])tocomputetheKDE.
To compute P(cid:0)f m
n(cid:1)i
n =hSIF T m
sn(cid:0)xm
n − xh
in test image In  we search for the nearest neighbors of the row vectorhf m
n i in a
ohe
t in training images It  s.t. at = aousinganANN
setofrowvectors: nhf r
query. Recallthat sn wasdefinedasthewidthofthedetectedfaceinimage In andhence 1
sn isthe
scalefactorthatweusefortheoffsetsinthequery. Thequeryreturnsasetof K nearestneighbors 
and the Gaussian KDE with σ = 0.2  is applied to this set to compute the estimated probability
n (cid:1). In our experiments we found that it is sufficient to use K = 25. The
P(cid:0)f m
P(cid:0)f m
3 Results
Totestourapproach wehaveappliedittotwostatictransitiveactionrecognitiondatasets. Thefirst
dataset  denoted ‘12/10’ dataset  was created by us and contained 12 similar transitive actions per-
formedby10differentpeople appearingagainstdifferentnaturalbackgrounds. Theseconddataset
was compiled by [11] for dynamic transitive action recognition. It contains 9 different people per-
forming 6 general transitive actions. Although originally designed and used by Gupta et al in [11]
fordynamicactionrecognition wetransformeditintoastaticactionrecognitiondatasetbyassign-
ingactionlabelstoframesactuallycontainingtheactionsandtreatingeachsuchframeasaseparate
static instance of the action. Since successive frames are not independent  the experiments con-
ducted on both datasets were all performed in a person-leave-one-out manner  meaning that during
the training we completely excluded all the frames of the tested person. Section 3.1 provides more
details on the relevant parts (face  hand  and elbow) detection in our experiments complementing
section 2.1. Sections 3.2 and 3.3 describe the ‘12/10’ and the Gupta et al datasets in more detail

n (cid:1) for the m-th patch feature f m
t i(cid:12)(cid:12)(cid:12) all f r
n (cid:1)iscomputedas: P(cid:0)f m

n (cid:1) =Pa P(cid:0)f m

n   A = a  of h
n   of h

n   ohe

n   1
n   1
of h
sn

n   A = a  of h

n   ohe

n (cid:1).

n   of h

n   ohe

n   ohe

n   1
sn

t   1
st

of h
t

  1
st

ohe

5

(a)
(a)

 
 

(b)
(b)

1
1
1
11

4
4

5
5

6
6

7
7

8
8

2
2
2
22

4
4
44
4

12
12
12
1212

2
2

3
3

1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5
6
6
6
6
7
7
7
7
8
8
8
8
9
9
9
9
10
10
10
10
11
11
11
11
12
12
12
12

1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5
6
6
6
6
7
7
7
7
8
8
8
8
9
9
9
9
10
10
10
10
11
11
11
11
12
12
12
12

 
 

1
1

1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5
6
6
6
6
7
7
7
7
8
8
8
8
9
9
9
9
10
10
10
10
11
11
11
11
12
12
12
12

1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5
6
6
6
6
7
7
7
7
8
8
8
8
9
9
9
9
10
10
10
10
11
11
11
11
12
12
12
12

9 10 11 12 13
9 10 11 12 13

1: drinking
1: drinking
2: drinking no cup
2: drinking no cup
3: eating with spoon
3: eating with spoon
4: phone
4: phone
5: phone with bottle
5: phone with bottle
6: scratching
6: scratching
7: singing with mike
7: singing with mike
8: smoking
8: smoking
9: teeth brushing
9: teeth brushing
10: toasting
10: toasting
11: waving
11: waving
12: wearing glasses
12: wearing glasses
13: no action
13: no action

0.7
0.7
0.6
0.6
0.5
0.5
0.4
0.4
0.3
0.3
0.2
0.2
0.1
0.1
Figure 4: (a) Average static action confusion matrix obtained by leave-one-out cross validation of
the proposed method on the ‘12/10’ dataset; (b) Some interesting failures (red boxes)  on the right
of each failure there is a successfully recognized instance of an action with which the method has
confused. Themeaningofthebar-graphisasinfigure3. Additionalfailureexamplesareprovided
inthesupplementarymaterial.
together with the respective static action recognition experiments performed on them. All exper-
iments were performed on grayscale versions of the images. Figures 3 and 6a illustrate the two
testeddatasetsalsoshowingexamplesofsuccessfullyrecognizedstatictransitiveactions andfigure
4bshowssomeinterestingfailures.
3.1 Partdetectiondetails
Ourapproachisbasedonpriorpartdetectionanditsperformanceisboundedfromabovebythepart
detectionperformance. Thedetectionratesofstate-of-the-artmethodsforlocalizingbodypartsina
generalsettingarecurrentlyasignificantlimitingfactor. Forexample [14]thatweusehere obtains
an average of 66% correct hand detection (comparing favorably to other state-of-the-art methods)
in the general setting experiments  when both the person and the background are unseen during
part detector training. However  as shown in [14]  average 85% part detection performance can be
achievedinmorerestrictedsettings. Onesuchsetting(denotedself-trained)iswhenanindependent
short part detection training period of several seconds is allowed for each test person  as for e.g.
in the human-computer interaction applications. Another setting (denoted environment-trained) is
whentheenvironmentinwhichpeopleperformtheactionisfixed e.g. inapplicationswherewecan
trainpartdetectorsonsomepeople andthenapplythemtonewunseenpeople butappearinginthe
sameenvironment. Asdemonstratedinthemethodscomparisonexperimentinsection3.2 itappears
thatpartdetectionisanessentialcomponentofsolvingSAR.Currentperformanceinautomaticbody
parts detection is well below human performance  but the area is now a focus of active research
which is likely to reduce this current performance gap. In our experiments we adopted the more
constrained (but still useful) part detection settings described above  the self-trained for the 12-10
dataset (having each person in different environment) and the environment-trained for the Gupta et
al. dataset(havingallthepeopleinthesamegeneralenvironment).
In the 12-10 dataset experiments  the part detection models for the face  hand and elbow described
in section 2.1  were trained using 10 additional short movies  one for each person  in which the
actors randomly moved their hands. On these 10 movies  face  hand and elbow locations were
manuallymarked. Thelearnedmodelswerethenappliedtodetecttheirrespectivepartsonthe120
movie sequences of our dataset. The hand detection performance was 94.1% (manually evaluated
on a subset of frames). Qualitative examples are provided in the supplementary material. The part
detection for the Gupta et al. dataset was performed in a person-leave-one-out manner. For each
person the parts (face  hand) were detected using models trained on other people. The mean hand
detection performance was 88% (manually evaluated). Since most people in the dataset wear very
dark clothing  in many cases the elbow is invisible and therefore it was not used in this experiment
(itisstraightforwardtoremoveitfromthemodelbyassigningafixedvaluetothehand-elbowoffset
inbothtrainingandtest).
3.2 The‘12/10’datasetexperiments
The ‘12/10’ dataset consists of 120 videos of 10 people performing 12 similar transitive actions 
namelydrinking talkingonthephone scratching toasting waving brushingteeth smoking wear-
ing glasses  eating with a spoon  singing to a microphone  and also drinking without a cup and

6

1

0.5

0.5

drinking

scratching

(a)
1eating with spoon
1drinking without cup
1
0.5
0.5
0.5
0
0
0
0
1
1
0
1singing with mike
1phone talking with bottle
1
0.5
0.5
0.5
0
0
0
0
1 teeth brushing
1
1
0.5
0.5
0.5
0
0
0

0.5
toasting

0.5
waving

0.5

0.5

1

0

0

0

0

0.5

0

0

0.5

0.5

1

1

1

1

1

0

0.5
smoking

1 phone talking
0.5
0
1
0.5
0
0
1 wearing glasses
0.5
0

0.5

0.5

0

1

1

1

(b)
Full person 
bounding box (no 
anchoring)
24.6 ±12.5%
8.8 ±1%
16.6 ±6.6%

Hand 
anchored
region
58.5 ±9.1%
37.7 ±2.7%
45.7 ±9.1%

Method / 
experiment
SAR method 
(section 2.2)

[2 ]

BoWSVM

Figure5: (a)ROCbasedcomparisonwiththestate-of-the-artmethodofobjectdetection[23]applied
torecognizestaticactions. Foreachaction thebluelineistheaverageROCof[23] andthemagenta
line is the average ROC of the proposed method. (b) Comparing state-of-the-art object recognition
methodsontheSARtaskwithandwithout‘bodyanchoring’.
makingaphonecallwithabottle. Allpeopleexceptonewerefilmedagainstdifferentbackgrounds.
All backgrounds were natural indoor / outdoor scenes containing both clutter and people passing
by. The drinking and toasting actions were performed with 2-4 different tools  and phone talking
was performed with mobile and regular phones. Overall  the dataset contains 44 522 frames. Not
all frames contain actions of interest (e.g. in drinking there are frames where the person reaches to
/ puts down a cup). The ground-truth action labels were manually assigned to the relevant frames.
Eachoftheresulting23 277relevantactionframeswasconsideredaseparateinstanceofanaction.
Theremainingframeswerelabeled‘no-action’. Theaveragerecognitionaccuracywas 59.3±8.6for
the13actions(includingno-action)and 58.5 ± 9.1%forthe12mainactions(excludingno-action).
Figure4ashowstheconfusionmatrixfor13actionsaveragedoverthe10testpeople.
Asmentionedintheintroduction oneoftheimportantquestionswewishtoansweristheneedfor
the detection of the fine details of the person  such as the accurate hand  and elbow locations  in
order to recognize the action. To test this issue  we have compared the results of three approaches:
deformable parts model [23]  Bag-of-Words (BoW) SVM ([24])  and our approach described in
section 2.2  in two settings. In the first setting the methods were trained to distinguish between the
actions based on a bounding box of the entire person (i.e. without focusing on the fine details such
as provided by the hand and elbow detection). In the second  body anchored setting  the methods
were applied to the hand anchored regions (small regions around the detected hand as described in
section2.2). Themethodof[23]isoneofthestate-of-the-artobjectrecognitionschemes achieving
top scores on recent PASCAL-VOC competitions [25]  and BoW SVM is a popular method in
the literature also obtaining state-of-the art results for some datasets. Figure 5b shows the results
obtainedbythethreemethodsinthetwosettings. Figure5aprovidesROC-basedcomparisonofthe
resultsofourfullapproachwiththeonesobtainedby[23]. Theobtainedresultsstronglysuggestthat
bodyanchoringisapowerfulpriorforthetaskofdistinguishingbetweensimilartransitiveactions.
3.3 Guptaetaldatasetexperiments
Thisdatasetwascompiledby[11]. Itconsistsof46moviesequencesof9differentpeopleperform-
ing 6 distinct transitive actions: drinking  spraying  answering the phone  making a call  pouring
from a pitcher and lighting a flashlight. In each movie  we manually assigned action labels to all
the frames actually containing the action  labeling the remainder of the frames ‘no-action’. Since
the distinction between ‘making a call’ and ‘answering phone’ was in the presence or absence of
the ‘dialing’ action in the respective video  we re-labeled the frames of these actions into ‘phone
talking’ and ‘dialing’. The action recognition performance was measured using the person-leave-
one-out cross-validation  in the same manner as for our dataset. The average accuracy over the 7
staticactions(includingno-action)was 82 ± 11.5% andwas 86 ± 14.4%excludingno-action. The
average 7-action confusion matrix is shown in figure 6b. The presented results are for the static
action recognition  and hence are not directly comparable with the results obtained on this dataset
for the dynamic action recognition by [11]  who obtained 93.34% recognition (out of the 46 video

7

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

 
 

1
1

(a)
(a)

(b)
(b)

 
 

5
5
55

6
6
66

00.10.20.30.40.50.60.70.80.9
00.10.20.30.40.50.60.70.80.9

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

1
1
1
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6

2
2
22

3
3
33

1
1
11

4
4
44

4
4

5
5

6
6

7
7

2
2

3
3

1: dialing
1: dialing
2: drinking
2: drinking
3: flashlight
3: flashlight
4: phone talking
4: phone talking
5: pouring
5: pouring
6: spraying
6: spraying
7: no action
7: no action
Figure6: (a)somesuccessfullyidentifiedactionexamplesfromthedatasetof[11]; (b)meanstatic
actionconfusionmatrixforleave-one-outcrossvalidationexperimentsontheGuptaetal. dataset.
sequencesandnotframes)usingboththetemporalinformation(parttracks etc.) andapriorimodels
fortheparticipatingobjects(cup pitcher flashlight spraybottleandphone).
4 Discussion
We have presented a method for recognizing transitive actions from single images. This task is
performed naturally and efficiently by humans  but performance by current recognition methods is
severely limited. The proposed method can successfully handle both similar transitive actions (the
‘12/10’ dataset)  and general transitive actions (the Gupta et al dataset). The method uses priors
thatfocusonbodypartanchoredfeaturesandrelations. Ithasbeenshownthatmostcommonverbs
are associated with specific body parts [19]; the actions considered here were all hand-related in
this sense. The detection of hands and elbows therefore provided useful priors in terms of regions
and properties likely to contribute to the SAR task in this setting. The proposed approach can be
generalizedtodealwithotheractionsbydetectingallthebodypartsassociatedwithcommonverbs 
automaticallydetectingtherelevantpartsforeachspecificactionduringtraining andfinallyapply-
ing the body anchored SAR model described in section 2.2. The comparisons show that without
using the body anchored priors there is a highly significant drop in SAR performance even when
employing state-of-the-art methods for object recognition. The main reasons for this drop are the
fine details and local nature of the relevant evidence for distinguishing between actions  the huge
numberofpossiblelocations anddetailedfeaturesthatneedtobesearchedifbody-anchoredpriors
arenotused. Directionsforfuturestudiesthereforeincludeamorecompleteandaccuratebodyparts
detectionandtheiruseinprovidingusefulpriorsforstaticactionrecognitionandinterpretation.
A Log-posteriorderivation
Here we derive the equivalent form of log-posterior (eq. 3) of the proposed probabilistic action
recognition model defined in eq. 1. In 4  the symbol ∼ means equivalent in terms of maximizing
overthevaluesoftheactionvariable A.
log P(cid:0)A(cid:12)(cid:12)of h
n }(cid:1) ∼ logP{Bm}
P(cid:0)A {Bm}   of h
n  {f m
loghP (A) ∙ P(cid:0)of h
m=1hP1
n (cid:12)(cid:12)A  of h
Bm=0 P (Bm) ∙ P(cid:0)f m
n (cid:1) ∙Qkn
n   ohe
n   Bm(cid:1)i =
m=1 loghP1
n (cid:12)(cid:12)A  of h
Bm=0 P (Bm) ∙ P(cid:0)f m
Pkn
n (cid:12)(cid:12)A  of h
n (cid:12)(cid:12)of h
m=1 log(cid:2)α ∙ P(cid:0)f m
n (cid:1)(cid:3) =
n (cid:1) + (1 − α) ∙ P(cid:0)f m
Pkn
n )(cid:21) +Pkn
m=1 log(cid:20)1 +
n (cid:12)(cid:12)of h
m=1 log(cid:2)(1 − α) ∙ P(cid:0)f m
Pkn
n )(cid:21) ≈(∗)Pkn
m=1 log(cid:20)1 +
n ) ∼Pkn
Pkn
Pkn
(4)
In eq. 4  γ = α/ (1 − α)  the termPkn
n (cid:1)(cid:3) is independent of the
n (cid:12)(cid:12)of h
m=1 log(cid:2)(1 − α) ∙ P(cid:0)f m
action(constantforagivenimage In)andthuscanbedropped and (∗)followsfrom log (1 + ε) ≈ ε
for ε (cid:28) 1andfrom γ beinglargeduetoourassumptionthat α (cid:28) (1 − α).

n }(cid:1) =
n   Bm(cid:1)ii ∼
n (cid:1)(cid:3) ∼

n |A of h
P (f m
n |of h
γ∙P (f m
n |A of h
P (f m
n |of h
γ∙P (f m
n  ohe
n )
n )
n  ohe

P (f m
γ∙P (f m

n |A of h
n |of h

n  ohe
n  ohe
n  ohe
n  ohe

n  ohe
n  ohe

n )
n ) ∼

P (f m
P (f m

n |A of h
n |of h

P (f m
P (f m

n  A of h
n  of h

n   ohe

n  {f m

n )

n  ohe
n  ohe

n   ohe

n   ohe

n   ohe

n   ohe

n )

m=1

m=1

n   ohe

n )

m=1

n   ohe

n   ohe

8

movies. In: CVPR.(2008)1–8
AnnalsofNeurology(2007)

References
[1] Jackendoff R.: Semanticinterpretationingenerativegrammar. TheMITPress(1972)
[2] Laptev  I.  Marszalek  M.  Schmid  C.  Rozenfeld  B.: Learning realistic human actions from
[3] Iacoboni M. Mazziotta J.C.: Mirrorneuronsystem: basicfindingsandclinicalapplications.
[4] Kim J. Biederman I.: Wheredoobjectsbecomescenes? JournalofVision(2009)
[5] Helbig H. Graf M. Kiefer M.: Theroleofactionrepresentationsinvisualobjectrecognition.
ExperimentalBrainResearch(2006)
[6] Sakata  H.  Taira  M.  Kusunoki  M.  Murata  A.  Tanaka  Y.  Tsutsui  K.: Neural coding of
3dfeaturesofobjectsforhandactionintheparietalcortexofthemonkey. PhilosTransRSoc
LondBBiolSci.(1998)
[7] Wang  Y.  Jiang  H.  Drew  M.S.  nian Li  Z.  Mori  G.: Unsupervised discovery of action
classes. In: CVPR.(2006) 5
[8] Li L. Fei-Fei L.: What whereandwho? classifyingeventsbysceneandobjectrecognition.
In: ICCV.(2007)1–8
[9] Thurau  C.  Hlavac  V.: Pose primitive based human action recognition in videos or still
images. In: CVPR.(2008)1–8
[10] Yao  B.  Fei-Fei  L.: Grouplet: A structured image representation for recognizing human and
objectinteractions. CVPR(2010)
[11] Gupta  A.  Kembhavi  A.  Davis  L.: Observing human-object interactions: Using spatial and
functionalcompatibilityforrecognition. PAMI(2009)
[12] Yao  B.  Fei-Fei  L.: Modeling mutual context of object and human pose in human-object
interactionactivities. CVPR(2010)
[13] Blake A. Rother C. Brown M. Perez P. Torr P.: Interactiveimagesegmentationusingan
adaptivegmmrfmodel. ECCV(2004)
[14] Karlinsky L. Dinerstein M. Harari D. Ullman S.: Thechainsmodelfordetectingpartsby
theircontext. CVPR(2010)
[15] Ferrari  V.  Marin  M.  Zisserman  A.: Progressive search space reduction for human pose
estimation. CVPR(2008)
[16] Andriluka  M.  Roth  S.  Schiele  B.: Pictorial structures revisited: People detection and
articulatedposeestimation. CVPR(2009)
[17] Felzenszwalb P. Huttenlocher D.: Pictorialstructuresforobjectrecognition. IJCV61(2005)
55–79
[18] Ramanan  D.  Forsyth  D.A.  Barnard  K.: Building models of animals from video. PAMI
(2006)
[19] Maouene  J.  Hidaka  S.  Smith  L.B.: Bodypartsandearly-learnedverbs. CognitiveScience
(2008)
[20] Lowe D.: Distinctiveimagefeaturesfromscale-invariantkeypoints. IJCV(2004)
[21] Duda R. Hart P.: Patternclassificationandsceneanalysis. Wiley(1973)
[22] Mount  D.  Arya  S.: Ann: A library for approximate nearest neighbor searching. CGC 2nd
AnnualWorkshoponComp.Geometry(1997)
[23] Felzenszwalb  P.  McAllester  D.  Ramanan  D.: A discriminatively trained  multiscale  de-
formablepartmodel. CVPR(2008)1–8
[24] Zhang J. Marszalek M. Lazebnik S. Schmid C.: Localfeaturesandkernelsforclassifica-
tionoftextureandobjectcategories: Acomprehensivestudy. IJCV(2007)
[25] Everingham  M.  Van Gool  L.  Williams  C.  Winn  J.  Zisserman  A.: The pascal visual ob-
ject classes challenge 2007 results. http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007
(2007)

9

,Christopher Meek
Marina Meila