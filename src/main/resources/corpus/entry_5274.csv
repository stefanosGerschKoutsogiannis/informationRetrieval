2017,Straggler Mitigation in Distributed Optimization Through Data Encoding,Slow running or straggler tasks can significantly reduce computation speed in distributed computation. Recently  coding-theory-inspired approaches have been applied to mitigate the effect of straggling  through embedding redundancy in certain linear computational steps of the optimization algorithm  thus completing the computation without waiting for the stragglers. In this paper  we propose an alternate approach where we embed the redundancy directly in the data itself  and allow the computation to proceed completely oblivious to encoding. We propose several encoding schemes  and demonstrate that popular batch algorithms  such as gradient descent and L-BFGS  applied in a coding-oblivious manner  deterministically achieve sample path linear convergence to an approximate solution of the original problem  using an arbitrarily varying subset of the nodes at each iteration. Moreover  this approximation can be controlled by the amount of redundancy and the number of nodes used in each iteration. We provide experimental results demonstrating the advantage of the approach over uncoded and data replication strategies.,Straggler Mitigation in Distributed Optimization

Through Data Encoding

Can Karakus

UCLA

Los Angeles  CA

karakus@ucla.edu

Suhas Diggavi

UCLA

Los Angeles  CA

suhasdiggavi@ucla.edu

Yifan Sun

Technicolor Research

Los Altos  CA

Yifan.Sun@technicolor.com

Wotao Yin

UCLA

Los Angeles  CA

wotaoyin@math.ucla.edu

Abstract

Slow running or straggler tasks can signiﬁcantly reduce computation speed in
distributed computation. Recently  coding-theory-inspired approaches have been
applied to mitigate the effect of straggling  through embedding redundancy in
certain linear computational steps of the optimization algorithm  thus completing
the computation without waiting for the stragglers. In this paper  we propose an
alternate approach where we embed the redundancy directly in the data itself  and
allow the computation to proceed completely oblivious to encoding. We propose
several encoding schemes  and demonstrate that popular batch algorithms  such as
gradient descent and L-BFGS  applied in a coding-oblivious manner  determinis-
tically achieve sample path linear convergence to an approximate solution of the
original problem  using an arbitrarily varying subset of the nodes at each iteration.
Moreover  this approximation can be controlled by the amount of redundancy
and the number of nodes used in each iteration. We provide experimental results
demonstrating the advantage of the approach over uncoded and data replication
strategies.

1

Introduction

Solving large-scale optimization problems has become feasible through distributed implementations.
However  the efﬁciency can be signiﬁcantly hampered by slow processing nodes  network delays or
node failures. In this paper we develop an optimization framework based on encoding the dataset 
which mitigates the effect of straggler nodes in the distributed computing system. Our approach
can be readily adapted to the existing distributed computing infrastructure and software frameworks 
since the node computations are oblivious to the data encoding.
In this paper  we focus on problems of the form
1
2n

(1)
where X ∈ Rn×p  y ∈ Rn×1 represent the data matrix and vector respectively. The function f (w) is
mapped onto a distributed computing setup depicted in Figure 1  consisting of one central server and
m worker nodes  which collectively store the row-partitioned matrix X and vector y. We focus on
batch  synchronous optimization methods  where the delayed or failed nodes can signiﬁcantly slow
down the overall computation. Note that asynchronous methods are inherently robust to delays caused

(cid:107)Xw − y(cid:107)2 

f (w) :=

min
w∈Rp

min
w∈Rp

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

approach consists of adding redundancy by encoding the data X and y into (cid:101)X = SX and(cid:101)y = Sy 

by stragglers  although their convergence rates can be worse than their synchronous counterparts. Our
respectively  where S ∈ R(βn)×n is an encoding matrix with redundancy factor β ≥ 1  and solving
the effective problem

(cid:107)(cid:101)Xw −(cid:101)y(cid:107)2

(2)

w∈Rp (cid:101)f (w) := min

w∈Rp

min

1

2βn

(cid:107)S (Xw − y)(cid:107)2 = min
w∈Rp

1

2βn

instead. In doing so  we proceed with the computation in each iteration without waiting for the
stragglers  with the idea that the inserted redundancy will compensate for the lost data. The goal is to
design the matrix S such that  when the nodes obliviously solve the problem (2) without waiting for
the slowest (m − k) nodes (where k is a design parameter) the achieved solution approximates the
original solution w∗ = arg minw f (w) sufﬁciently closely. Since in large-scale machine learning and
data analysis tasks one is typically not interested in the exact optimum  but rather a “sufﬁciently" good
solution that achieves a good generalization error  such an approximation could be acceptable in many
scenarios. Note also that the use of such a technique does not preclude the use of other  non-coding
straggler-mitigation strategies (see [24] and references therein)  which can still be implemented on
top of the redundancy embedded in the system  to potentially further improve performance.
Focusing on gradient descent and L-BFGS algorithms  we show that under a spectral condition on
S  one can achieve an approximation of the solution of (1)  by solving (2)  without waiting for the
stragglers. We show that with sufﬁcient redundancy embedded  and with updates from a sufﬁciently
large  yet strict subset of the nodes in each iteration  it is possible to deterministically achieve linear
convergence to a neighborhood of the solution  as opposed to convergence in expectation (see Fig.
4). Further  one can adjust the approximation guarantee by increasing the redundancy and number
of node updates waited for in each iteration. Another potential advantage of this strategy is privacy 
since the nodes do not have access to raw data itself  but can still perform the optimization task over
the jumbled data to achieve an approximate solution.
Although in this paper we focus on quadratic objectives and two speciﬁc algorithms  in principle
our approach can be generalized to more general  potentially non-smooth objectives and constrained
optimization problems  as we discuss in Section 4 ( adding a regularization term is also a simple
generalization).
Our main contributions are as follows. (i) We demonstrate that gradient descent (with constant
step size) and L-BFGS (with line search) applied in a coding-oblivious manner on the encoded
problem  achieves (universal) sample path linear convergence to an approximate solution of the
original problem  using only a fraction of the nodes at each iteration. (ii) We present three classes of
coding matrices; namely  equiangular tight frames (ETF)  fast transforms  and random matrices  and
discuss their properties. (iii) We provide experimental results demonstrating the advantage of the
approach over uncoded (S = I) and data replication strategies  for ridge regression using synthetic
data on an AWS cluster  as well as matrix factorization for the Movielens 1-M recommendation task.

Related work. Use of data replication to aid with the straggler problem has been proposed and
studied in [22  1]  and references therein. Additionally  use of coding in distributed computing
has been explored in [13  7]. However  these works exclusively focused on using coding at the
computation level  i.e.  certain linear computational steps are performed in a coded manner  and
explicit encoding/decoding operations are performed at each step. Speciﬁcally  [13] used MDS-coded
distributed matrix multiplication and [7] focused on breaking up large dot products into shorter
dot products  and perform redundant copies of the short dot products to provide resilience against
stragglers. [21] considers a gradient descent method on an architecture where each data sample is
replicated across nodes  and designs a code such that the exact gradient can be recovered as long as
fewer than a certain number of nodes fail. However  in order to recover the exact gradient under any
potential set of stragglers  the required redundancy factor is on the order of the number of straggling
nodes  which could mean a large amount of overhead for a large-scale system. In contrast  we show
that one can converge to an approximate solution with a redundancy factor independent of network
size or problem dimensions (e.g.  2 as in Section 5).
Our technique is also closely related to randomized linear algebra and sketching techniques [14  6  17] 
used for dimensionality reduction of large convex optimization problems. The main difference
between this literature and the proposed coding technique is that the former focuses on reducing the
problem dimensions to lighten the computational load  whereas coding increases the dimensionality

2

X =(cid:2)X(cid:62)

Figure 1: Left: Uncoded distributed optimization with partitioning  where X and y are partitioned as
. Right: Encoded distributed optimization 

. . . X(cid:62)

1 y(cid:62)

2 . . . y(cid:62)

1 X(cid:62)

where node i stores (SiX  Siy)  instead of (Xi  yi). The uncoded case corresponds to S = I.

m

m

2

(cid:3)(cid:62)

and y =(cid:2)y(cid:62)

(cid:3)(cid:62)

of the problem to provide robustness. As a result of the increased dimensions  coding can provide a
much closer approximation to the original solution compared to sketching techniques.
A longer version of this paper is available on [12].

2 Encoded Optimization Framework

(cid:17)

(cid:16)(cid:101)Xi (cid:101)yi

= (SiX  Siy) and S =(cid:2)S(cid:62)

Figure 1 shows a typical data-distributed computational model in large-scale optimization (left)  as
well as our proposed encoded model (right). Our computing network consists of m machines  where
machine i stores
. The optimization process
is oblivious to the encoding  i.e.  once the data is stored at the nodes  the optimization algorithm
proceeds exactly as if the nodes contained uncoded  raw data (X  y). In each iteration t  the central
server broadcasts the current estimate wt  and each worker machine computes and sends to the server

the gradient terms corresponding to its own partition gi(wt) := (cid:101)X(cid:62)

(cid:3)(cid:62)
i ((cid:101)Xiwt −(cid:101)yi).

. . . S(cid:62)

1 S(cid:62)

m

2

Note that this framework of distributed optimization is typically communication-bound  where
communication over a few slow links constitute a signiﬁcant portion of the overall computation time.
We consider a strategy where at each iteration t  the server only uses the gradient updates from the
ﬁrst k nodes to respond in that iteration  thereby preventing such slow links and straggler nodes from
stalling the overall computation:
1

1

(cid:88)

(cid:101)X(cid:62)
A ((cid:101)XAwt −(cid:101)yA) 

(cid:101)gt =

2βηn

i∈At

gi(wt) =

βηn

. (Similarly  SA = [Si]i∈At

and (cid:101)XA = [SiX]i∈At

where At ⊆ [m]  |At| = k are the indices of the ﬁrst k nodes to respond at iteration t  η := k
m
.) Given the gradient approximation  the central
server then computes a descent direction dt through the history of gradients and parameter estimates.
For the remaining nodes i (cid:54)∈ At  the server can either send an interrupt signal  or simply drop their
updates upon arrival  depending on the implementation.
Next  the central server chooses a step size αt  which can be chosen as constant  decaying  or through
again assume the central server only hears from the fastest k nodes  denoted by Dt ⊆ [m]  where
Dt (cid:54)= At in general  to compute

exact line search 1 by having the workers compute (cid:101)Xdt that is needed to compute the step size. We

αt = −ν

t (cid:101)gt
t (cid:101)X(cid:62)
D(cid:101)XDdt

d(cid:62)

 

d(cid:62)

where (cid:101)XD = [SiX]i∈Dt
like L-BFGS. Additionally  although the algorithm only works with the encoded function (cid:101)f  our goal

Our goal is to especially focus on the case k < m  and design an encoding matrix S such that  for
any sequence of sets {At}  {Dt}  f (wt) universally converges to a neighborhood of f (w∗). Note
that in general  this scheme with k < m is not guaranteed to converge for traditionally batch methods

  and 0 < ν < 1 is a back-off factor of choice.

is to provide a convergence guarantee in terms of the original function f.

(3)

1Note that exact line search is not more expensive than backtracking line search for a quadratic loss  since it

only requires a single matrix-vector multiplication.

3

(cid:107)X1w−y1(cid:107)2(cid:107)X2w−y2(cid:107)2(cid:107)Xmw−ym(cid:107)2N1N2NmM(cid:107)S1(Xw−y)(cid:107)2(cid:107)S2(Xw−y)(cid:107)2(cid:107)Sm(Xw−y)(cid:107)2N1N2NmM3 Algorithms and Convergence Analysis
Let the smallest and largest eigenvalues of X(cid:62)X be denoted by µ > 0 and M > 0  respectively.
Let η with 1

(cid:8)S(β)(cid:9) where β is the aspect ratio (redundancy factor)  such that for any  > 0  and any A ⊆ [m]

β < η ≤ 1 be given. In order to prove convergence we will consider a family of matrices

with |A| = ηm 

(1 − )I (cid:22) S(cid:62)

A SA (cid:22) (1 + )I 

(4)
for sufﬁciently large β ≥ 1  where SA = [Si]i∈A is the submatrix associated with subset A (we drop
dependence on β for brevity). Note that this is similar to the restricted isometry property (RIP) used
in compressed sensing [4]  except that (4) is only required for submatrices of the form SA. Although
this condition is needed to prove worst-case convergence results  in practice the proposed encoding
scheme can work well even when it is not exactly satisﬁed  as long as the bulk of the eigenvalues of
A SA lie within a small interval [1 −   1 + ]. We will discuss several speciﬁc constructions and
S(cid:62)
their relation to property (4) in Section 4.

Gradient descent. We consider gradient descent with constant step size  i.e. 

wt+1 = wt + αdt = wt − α(cid:101)gt.

The following theorem characterizes the convergence of the encoded problem under this algorithm.
Theorem 1. Let ft = f (wt)  where wt is computed using gradient descent with updates from a set
of (fastest) workers At  with constant step size αt ≡ α = 2ζ
M (1+) for some 0 < ζ ≤ 1  for all t. If S
satisﬁes (4) with  > 0  then for all sequences of {At} with cardinality |At| = k 

ft ≤ (κγ1)t f0 +

f (w∗)  

t = 1  2  . . .  

(cid:16)

κ2(κ − γ1)
1 − κγ1

(cid:17)

where κ = 1+

1−   and γ1 =

1 − 4µζ(1−ζ)

M (1+)

  and f0 = f (w0) is the initial objective value.

The proof is provided in Appendix B of [12]  which relies on the fact that the solution to the effective
“instantaneous" problem corresponding to the subset At lies in the set {w : f (w) ≤ κ2f (w∗)}  and
therefore each gradient descent step attracts the estimate towards a point in this set  which must
eventually converge to this set. Note that in order to guarantee linear convergence  we need κγ1 < 1 
which can be ensured by property (4).
Theorem 1 shows that gradient descent over the encoded problem  based on updates from only k < m
nodes  results in deterministically linear convergence to a neighborhood of the true solution w∗ 
for sufﬁciently large k  as opposed to convergence in expectation. Note that by property (4)  by
controlling the redundancy factor β and the number of nodes k waited for in each iteration  one can
control the approximation guarantee. For k = m and S designed properly (see Section 4)  then κ = 1
and the optimum value of the original function f (w∗) is reached.

Limited-memory-BFGS. Although L-BFGS is originally a batch method  requiring updates from
all nodes  its stochastic variants have also been proposed recently [15  3]. The key modiﬁcation to
ensure convergence is that the Hessian estimate must be computed via gradient components that are
common in two consecutive iterations  i.e.  from the nodes in At ∩ At−1. We adapt this technique to
our scenario. For t > 0  deﬁne ut := wt − wt−1  and

rt :=

m

2βn|At ∩ At−1|

i∈At∩At−1

(gi(wt) − gi(wt−1)) .

−Bt(cid:101)gt  where Bt is the inverse Hessian estimate for iteration t  which is computed by
Then once the gradient terms {gt}i∈At

are collected  the descent direction is computed by dt =

(cid:88)

B((cid:96)+1)

t

= V (cid:62)

j(cid:96)

t Vj(cid:96) + ρj(cid:96)uj(cid:96)u(cid:62)
B((cid:96))

j(cid:96)

  ρk =

  Vk = I − ρkrku(cid:62)

k

1
r(cid:62)
k uk

4

t = r(cid:62)
t rt
r(cid:62)
t ut

with j(cid:96) = t −(cid:101)σ + (cid:96)  B(0)

I  and Bt := B((cid:101)σ)
memory length. Once the descent direction dt is computed  the step size is determined through exact
line search  using (3)  with back-off factor ν = 1−
For our convergence result for L-BFGS  we need another assumption on the matrix S  in addition to
(4). Deﬁning ˘St = [Si]i∈At∩At−1

t with(cid:101)σ := min{t  σ}  where σ is the L-BFGS

for t > 0  we assume that for some δ > 0 

1+   where  is as in (4).

δI (cid:22) ˘S(cid:62)

˘St

t

2 + 1

(5)
for all t > 0. Note that this requires that one should wait for sufﬁciently many nodes to ﬁnish so
that the overlap set At ∩ At−1 has more than a fraction 1
β of all nodes  and thus the matrix ˘St can
be full rank. This is satisﬁed if η ≥ 1
2β in the worst-case  and under the assumption that node
delays are i.i.d.  it is satisﬁed in expectation if η ≥ 1√
β . However  this condition is only required for a
worst-case analysis  and the algorithm may perform well in practice even when this condition is not
satisﬁed. The following lemma shows the stability of the Hessian estimate.
Lemma 1. If (5) is satisﬁed  then there exist constants c1  c2 > 0 such that for all t  the inverse
Hessian estimate Bt satisﬁes c1I (cid:22) Bt (cid:22) c2I.
The proof  provided in Appendix A of [12]  is based on the well-known trace-determinant method.
Using Lemma 1  we can show the following result.
Theorem 2. Let ft = f (wt)  where wt is computed using L-BFGS as described above  with gradient
updates from machines At  and line search updates from machines Dt. If S satisﬁes (4) and (5)  for
all sequences of {At} {Dt} with |At| = |Dt| = k 

(cid:16)

where κ = 1+

1−   and γ2 =

ft ≤ (κγ2)t f0 +

(cid:17)

1 − 4µc1c2
M (c1+c2)2

κ2(κ − γ2)
1 − κγ2

f (w∗)  

  and f0 = f (w0) is the initial objective value.

The proof is provided in Appendix B of [12]. Similar to Theorem 1  the proof is based on the
observation that the solution of the effective problem at time t lies in a bounded set around the true
solution w∗. As in gradient descent  coding enables linear convergence deterministically  unlike the
stochastic and multi-batch variants of L-BFGS [15  3].

Generalizations. Although we focus on quadratic cost functions and two speciﬁc algorithms  our
approach can potentially be generalized for objectives of the form (cid:107)Xw − y(cid:107)2 + h(w) for a simple
convex function h  e.g.  LASSO; or constrained optimization minw∈C (cid:107)Xw − y(cid:107)2 (see [11]); as
well as other ﬁrst-order algorithms used for such problems  e.g.  FISTA [2]. In the next section
we demonstrate that the codes we consider have desirable properties that readily extend to such
scenarios.

4 Code Design

We consider three classes of coding matrices: tight frames  fast transforms  and random matrices.
Tight frames. A unit-norm frame for Rn is a set of vectors F = {φi}nβ
i=1 with (cid:107)φi(cid:107) = 1  where
β ≥ 1  such that there exist constants ξ1 ≥ ξ2 > 0 such that  for any u ∈ Rn 

ξ1(cid:107)u(cid:107)2 ≤ nβ(cid:88)

|(cid:104)u  φi(cid:105)|2 ≤ ξ2(cid:107)u(cid:107)2.

i=1

The frame is tight if the above satisﬁed with ξ1 = ξ2. In this case  it can be shown that the constants
are equal to the redundancy factor of the frame  i.e.  ξ1 = ξ2 = β. If we form S ∈ R(βn)×n by rows
that are a tight frame  then we have S(cid:62)S = βI  which ensures (cid:107)Xw − y(cid:107)2 = 1
β(cid:107)SXw − Sy(cid:107)2.

Then for any solution (cid:101)w∗ to the encoded problem (with k = m) 

∇(cid:101)f ((cid:101)w∗) = X(cid:62)S(cid:62)S(X(cid:101)w∗ − y) = β(X(cid:101)w∗ − y)(cid:62)X = β∇f ((cid:101)w∗).

5

Figure 2: Sample spectrum of S(cid:62)
A SA for var-
ious constructions with high redundancy  and
relatively small k (normalized).

Figure 3: Sample spectrum of S(cid:62)
A SA for var-
ious constructions with low redundancy  and
large k (normalized).

Therefore  the solution to the encoded problem satisﬁes the optimality condition for the original
problem as well:

and if f is also strongly convex  then (cid:101)w∗ = w∗ is the unique solution. Note that since the computation

∇(cid:101)f ((cid:101)w∗) = 0  ⇔ ∇f ((cid:101)w∗) = 0 

is coding-oblivious  this is not true in general for an arbitrary full rank matrix  and this is  in addition
to property (4)  a desired property of the encoding matrix. In fact  this equivalency extends beyond
smooth unconstrained optimization  in that

(cid:68)∇(cid:101)f ((cid:101)w∗)  w − (cid:101)w∗(cid:69) ≥ 0  ∀w ∈ C ⇔ (cid:104)∇f ((cid:101)w∗)  w − (cid:101)w∗(cid:105) ≥ 0  ∀w ∈ C

for any convex constraint set C  as well as

−∇(cid:101)f ((cid:101)w∗) ∈ ∂h((cid:101)w∗)  ⇔ −∇f ((cid:101)w∗) ∈ ∂h((cid:101)w∗) 

for any non-smooth convex objective term h(x)  where ∂h is the subdifferential of h. This means
that tight frames can be promising encoding matrix candidates for non-smooth and constrained
optimization too. In [11]  it was shown that when {At} is static  equiangular tight frames allow for a
close approximation of the solution for constrained problems.
A tight frame is equiangular if |(cid:104)φi  φj(cid:105)| is constant across all pairs (i  j) with i (cid:54)= j.
Proposition 1 (Welch bound [23]). Let F = {φi}nβ
Moreover  equality is satisﬁed if and only if F is an equiangular tight frame.

i=1 be a tight frame. Then ω(F ) ≥ (cid:113) β−1

2nβ−1 .

Therefore  an ETF minimizes the correlation between its individual elements  making each submatrix
S(cid:62)
A SA as close to orthogonal as possible  which is promising in light of property (4). We speciﬁcally
evaluate Paley [16  10] and Hadamard ETFs [20] (not to be confused with Hadamard matrix  which is
discussed next) in our experiments. We also discuss Steiner ETFs [8] in Appendix D of [12]  which
enable efﬁcient implementation.

Fast transforms. Another computationally efﬁcient method for encoding is to use fast transforms:
Fast Fourier Transform (FFT)  if S is chosen as a subsampled DFT matrix  and the Fast Walsh-
Hadamard Transform (FWHT)  if S is chosen as a subsampled real Hadamard matrix. In particular 
one can insert rows of zeroes at random locations into the data pair (X  y)  and then take the FFT
or FWHT of each column of the augmented matrix. This is equivalent to a randomized Fourier or
Hadamard ensemble  which is known to satisfy the RIP with high probability [5].

Random matrices. A natural choice of encoding is using i.i.d. random matrices. Although such
random matrices do not have the computational advantages of fast transforms or the optimality-
preservation property of tight frames  their eigenvalue behavior can be characterized analytically. In
particular  using the existing results on the eigenvalue scaling of large i.i.d. Gaussian matrices [9  19]
and union bound  it can be shown that

→ 0 

→ 0 

(6)

(7)

(cid:32)
(cid:32)

P

P

max
A:|A|=k

λmax

min

A:|A|=k

λmin

(cid:19)
(cid:19)

(cid:18)
(cid:18)

>

<

1 +

1 −

(cid:114) 1
(cid:114) 1

βη

(cid:19)2(cid:33)
(cid:19)2(cid:33)

βη

(cid:18) 1
(cid:18) 1

βηn

βηn

S(cid:62)
A SA

S(cid:62)
A SA

6

Figure 4: Left: Sample evolution of uncoded  replication  and Hadamard (FWHT)-coded cases  for
k = 12  m = 32. Right: Runtimes of the schemes for different values of η  for the same number of
iterations for each scheme. Note that this essentially captures the delay proﬁle of the network  and
does not reﬂect the relative convergence rates of different methods.

(cid:16) 1√

(cid:17)

as n → ∞  where σi denotes the ith singular value. Hence  for sufﬁciently large redundancy and
problem dimension  i.i.d. random matrices are good candidates for encoding as well. However  for
ﬁnite β  even if k = m  in general for this encoding scheme the optimum of the original problem is
not recovered exactly.

βη

Property (4) and redundancy requirements. Using the analytical bounds (6)–(7) on i.i.d. Gaus-
sian matrices  one can see that such matrices satisfy (4) with  = O
  independent of problem
dimensions or number of nodes m. Although we do not have tight eigenvalue bounds for subsampled
ETFs  numerical evidence (Figure 2) suggests that they may satisfy (4) with smaller  than random
matrices  and thus we believe that the required redundancy in practice is even smaller for ETFs.
Note that our theoretical results focus on the extreme eigenvalues due to a worst-case analysis; in
practice  most of the energy of the gradient will be on the eigen-space associated with the bulk of the
eigenvalues  which the following proposition suggests can be mostly 1 (also see Figure 3)  which
means even if (4) is not satisﬁed  the gradient (and the solution) can be approximated closely for a
modest redundancy  such as β = 2. The following result is a consequence of the Cauchy interlacing
theorem  and the deﬁnition of tight frames.
Proposition 2. If the rows of S are chosen to form an ETF with redundancy β  then for η ≥ 1 − 1
β  
β S(cid:62)

A SA has n(1 − βη) eigenvalues equal to 1.

1

5 Numerical Results

1

2βn

+ λ

(cid:13)(cid:13)(cid:13)(cid:101)Xw −(cid:101)y
(cid:13)(cid:13)(cid:13)2

Ridge regression with synthetic data on AWS EC2 cluster. We generate the elements of matrix
X i.i.d. ∼ N (0  1)  the elements of y i.i.d. ∼ N (0  p)  for dimensions (n  p) = (4096  6000)  and
2(cid:107)w(cid:107)2  for regularization parameter λ = 0.05. We
solve the problem minw
evaluate column-subsampled Hadamard matrix with redundancy β = 2 (encoded using FWHT
for fast encoding)  data replication with β = 2  and uncoded schemes. We implement distributed
L-BFGS as described in Section 3 on an Amazon EC2 cluster using the mpi4py Python package 
over m = 32 m1.small worker node instances  and a single c3.8xlarge central server instance.
We assume the central server encodes and sends the data variables to the worker nodes (see Appendix
D of [12] for a discussion of how to implement this more efﬁciently).
Figure 4 shows the result of our experiments  which are aggregated over 20 trials. As baselines 
we consider the uncoded scheme  as well as a replication scheme  where each uncoded partition is
replicated β = 2 times across nodes  and the server uses the faster copy in each iteration. It can be
seen from the right ﬁgure that one can speed up computation by reducing η from 1 to  for instance 
0.375  resulting in more than 40% reduction in the runtime. Note that in this case  uncoded L-BFGS
fails to converge  whereas the Hadamard-coded case stably converges. We also observe that the data
replication scheme converges on average  but in the worst case  the convergence is much less smooth 
since the performance may deteriorate if both copies of a partition are delayed.

7

Figure 5: Test RMSE for m = 8 (left) and m = 24 (right) nodes 
where the server waits for k = m/8 (top) and k = m/2 (bottom)
responses. “Perfect" refers to the case where k = m.

Figure 6: Total runtime with m =
8 and m = 24 nodes for different
values of k  under ﬁxed 100 itera-
tions for each scheme.

(cid:88)

min

xi yj  ui vj

i j: observed

(cid:88)

(cid:88)

 .

Matrix factorization on Movielens 1-M dataset. We next apply matrix factorization on the
MovieLens-1M dataset [18] for the movie recommendation task. We are given R  a sparse ma-
trix of movie ratings 1–5  of dimension #users × #movies  where Rij is speciﬁed if user i has
rated movie j. We withhold randomly 20% of these ratings to form an 80/20 train/test split. The
goal is to recover user vectors xi ∈ Rp and movie vectors yi ∈ Rp (where p is the embedding
dimension) such that Rij ≈ xT
i yj + ui + vj + µ  where ui  vj  and µ are user  movie  and global
biases  respectively. The optimization problem is given by

(Rij − ui − vj − xT

i yj − µ)2 + λ

(cid:107)xi(cid:107)2

2 + (cid:107)u(cid:107)2

2 +

(cid:107)yj(cid:107)2

2 + (cid:107)v(cid:107)2

2

i

j

(8)
We choose µ = 3  p = 15  and λ = 10  which achieves a test RMSE 0.861  close to the current best
test RMSE on this dataset using matrix factorization2.
Problem (8) is often solved using alternating minimization  minimizing ﬁrst over all (xi  ui)  and then
all (yj  vj)  in repetition. Each such step further decomposes by row and column  made smaller by the
sparsity of R. To solve for (xi  ui)  we ﬁrst extract Ii = {j | rij is observed}  and solve the resulting
sequence of regularized least squares problems in the variables wi = [x(cid:62)
i   ui](cid:62) distributedly using
coded L-BFGS; and repeat for w = [y(cid:62)
j   vj](cid:62)  for all j. As in the ﬁrst experiment  distributed coded
L-BFGS is solved by having the master node encoding the data locally  and distributing the encoded
data to the worker nodes (Appendix D of [12] discusses how to implement this step more efﬁciently).
The overhead associated with this initial step is included in the overall runtime in Figure 6.
The Movielens experiment is run on a single 32-core machine with 256 GB RAM. In order to simulate
network latency  an artiﬁcial delay of ∆ ∼ exp(10 ms) is imposed each time the worker completes a
task. Small problem instances (n < 500) are solved locally at the central server  using the built-in
function numpy.linalg.solve. Additionally  parallelization is only done for the ridge regression
instances  in order to isolate speedup gains in the L-BFGS distribution. To reduce overhead  we create
a bank of encoding matrices {Sn} for Paley ETF and Hadamard ETF  for n = 100  200  . . .   3500 
and then given a problem instance  subsample the columns of the appropriate matrix Sn to match
the dimensions. Overall  we observe that encoding overhead is amortized by the speed-up of the
distributed optimization.
Figure 5 gives the ﬁnal performance of our distributed L-BFGS for various encoding schemes  for
each of the 5 epochs  which shows that coded schemes are most robust for small k. A full table of
results is given in Appendix C of [12].

2http://www.mymedialite.net/examples/datasets.html

8

Acknowledgments

This work was supported in part by NSF grants 1314937 and 1423271.

References
[1] G. Ananthanarayanan  A. Ghodsi  S. Shenker  and I. Stoica. Effective straggler mitigation: Attack of the

clones. In NSDI  volume 13  pages 185–198  2013.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[3] A. S. Berahas  J. Nocedal  and M. Takác. A multi-batch l-bfgs method for machine learning. In Advances

in Neural Information Processing Systems  pages 1055–1063  2016.

[4] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory  51

(12):4203–4215  2005.

[5] E. J. Candes and T. Tao. Near-optimal signal recovery from random projections: Universal encoding

strategies? IEEE Transactions on Information Theory  52(12):5406–5425  2006.

[6] P. Drineas  M. W. Mahoney  S. Muthukrishnan  and T. Sarlós. Faster least squares approximation.

Numerische mathematik  117(2):219–249  2011.

[7] S. Dutta  V. Cadambe  and P. Grover. Short-dot: Computing large linear transforms distributedly using
coded short dot products. In Advances In Neural Information Processing Systems  pages 2092–2100  2016.
[8] M. Fickus  D. G. Mixon  and J. C. Tremain. Steiner equiangular tight frames. Linear Algebra and Its

Applications  436(5):1014–1027  2012.

[9] S. Geman. A limit theorem for the norm of random matrices. The Annals of Probability  pages 252–261 

1980.

[10] J. Goethals and J. J. Seidel. Orthogonal matrices with zero diagonal. Canad. J. Math  1967.
[11] C. Karakus  Y. Sun  and S. Diggavi. Encoded distributed optimization. In 2017 IEEE International

Symposium on Information Theory (ISIT)  pages 2890–2894. IEEE  2017.

[12] C. Karakus  Y. Sun  S. Diggavi  and W. Yin. Straggler mitigation in distributed optimization through data

encoding. Arxiv.org  https://arxiv.org/abs/1711.04969  2017.

[13] K. Lee  M. Lam  R. Pedarsani  D. Papailiopoulos  and K. Ramchandran. Speeding up distributed machine
In Information Theory (ISIT)  2016 IEEE International Symposium on  pages

learning using codes.
1143–1147. IEEE  2016.

[14] M. W. Mahoney et al. Randomized algorithms for matrices and data. Foundations and Trends R(cid:13) in

Machine Learning  3(2):123–224  2011.

[15] A. Mokhtari and A. Ribeiro. Global convergence of online limited memory BFGS. Journal of Machine

Learning Research  16:3151–3181  2015.

[16] R. E. Paley. On orthogonal matrices. Studies in Applied Mathematics  12(1-4):311–320  1933.
[17] M. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees. IEEE

Transactions on Information Theory  61(9):5096–5115  2015.

[18] J. Riedl and J. Konstan. Movielens dataset  1998.
[19] J. W. Silverstein. The smallest eigenvalue of a large dimensional wishart matrix. The Annals of Probability 

pages 1364–1368  1985.

[20] F. Szöll˝osi. Complex hadamard matrices and equiangular tight frames. Linear Algebra and its Applications 

438(4):1962–1967  2013.

[21] R. Tandon  Q. Lei  A. G. Dimakis  and N. Karampatziakis. Gradient coding. ML Systems Workshop

(MLSyS)  NIPS  2016.

[22] D. Wang  G. Joshi  and G. Wornell. Using straggler replication to reduce latency in large-scale parallel

computing. ACM SIGMETRICS Performance Evaluation Review  43(3):7–11  2015.

[23] L. Welch. Lower bounds on the maximum cross correlation of signals (corresp.). IEEE Transactions on

Information theory  20(3):397–399  1974.

[24] N. J. Yadwadkar  B. Hariharan  J. Gonzalez  and R. H. Katz. Multi-task learning for straggler avoiding

predictive job scheduling. Journal of Machine Learning Research  17(4):1–37  2016.

9

,Can Karakus
Suhas Diggavi
Wotao Yin
Yue Wang
Ziyu Jiang
Xiaohan Chen
Pengfei Xu
Yang Zhao
Yingyan Lin
Zhangyang Wang