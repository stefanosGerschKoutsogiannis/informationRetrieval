2017,Affine-Invariant Online Optimization and the Low-rank Experts Problem,We present a new affine-invariant optimization algorithm called Online Lazy Newton. The regret of Online Lazy Newton is independent of conditioning: the algorithm's performance depends on the best possible preconditioning of the problem in retrospect and on its \emph{intrinsic} dimensionality. As an application  we show how Online Lazy Newton can be used to achieve an optimal regret of order $\sqrt{rT}$ for the low-rank experts problem  improving by a $\sqrt{r}$ factor over the previously best known bound and resolving an open problem posed by Hazan et al (2016).,Aﬃne-Invariant Online Optimization
and the Low-rank Experts Problem

Tomer Koren
Google Brain

1600 Amphitheatre Pkwy
Mountain View  CA 94043

Roi Livni

Princeton University

35 Olden St.

Princeton  NJ 08540

tkoren@google.com

rlivni@cs.princeton.edu

Abstract

We present a new aﬃne-invariant optimization algorithm called Online Lazy Newton.
The regret of Online Lazy Newton is independent of conditioning: the algorithm’s
performance depends on the best possible preconditioning of the problem in
√
retrospect and on its intrinsic dimensionality. As an application  we show how
√
Online Lazy Newton can be used to achieve an optimal regret of order
rT for the
low-rank experts problem  improving by a
r factor over the previously best known
bound and resolving an open problem posed by Hazan et al. [15].

1

Introduction

In the online convex optimization setting  a learner is faced with a stream of T convex functions
over a bounded convex domain (cid:88) ⊆ (cid:82)d. At each round t the learner gets to observe a single convex
function ft and has to choose a point xt ∈ (cid:88). The aim of the learner is to minimize the cumulative
T-round regret  deﬁned as

T(cid:88)

t=1

T(cid:88)

ft(xt) − min
x∈(cid:88)

ft(x).

t=1

√
In this very general setting  Online Gradient Descent [25] achieves an optimal regret rate of Θ(GD
T) 
where G is a bound on the Lipschitz constants of the ft and D is a bound on the diameter of the
domain  both with respect to the Euclidean norm. For simplicity  let us restrict the exposition to linear
losses ft(xt) = gT
t xt  in which case G bounds the maximal Euclidean norm (cid:107)gt(cid:107); it is well known that
the general convex case can be easily reduced to this case.
One often useful way to obtain faster convergence in optimization is to employ preconditioning 
namely to apply a linear transformation P to the gradients before using them to make update steps.
√
In an online optimization setting  if we have had access to the best preconditioner in hindsight we
could have achieved a regret rate of the form Θ(inf P GPDP
T)  where DP is the diameter of the set
P · (cid:88)and GP is a bound on the norm of the conditioned gradients (cid:107)P−1gt(cid:107). We shall thus refer to the
quantity GPDP as the conditioning of the problem when a preconditioner P is used.
In many cases  however  it is more natural to directly assume a bound |gT
t xt| ≤ C on the magnitude of
the losses rather than assuming the bounds D and G. In this case  the condition number need not
be bounded and typical guarantees of gradient-based methods such as online gradient descent do
not directly apply. In principle  it is possible to ﬁnd a preconditioner P such that GPDP = O(C
d) 
and if one further assumes that the intrinsic dimensionality of the problem (i.e.  the rank of the
loss vectors g1  . . .   gT ) is r (cid:28) d  the conditioning of the optimization problem can be improved
√
to O(C
r). However  this approach requires one to have access to the transformation P  which is
typically data-dependent and known only in retrospect.

√

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√
In this paper we address the following natural question: can one achieve a regret rate of O(C
rT)
without the explicit prior knowledge of a good preconditioner P? We answer to this question in the
aﬃrmative and present a new algorithm that achieves this rate  called Online Lazy Newton. Our
algorithm is a variant of the Online Newton Step algorithm due to Hazan et al. [14]  that employs a lazy
projection step. While the Online Newton Step algorithm was designed to exploit curvature in the loss
functions (speciﬁcally  a property called exp-concavity)  our adaptation is aimed at general—possibly
√
even linear—online convex optimization and exploits latent low-dimensional structure. It turns out
that this adaptation of the algorithm is able to achieve O(C
rT) regret up to a small logarithmic factor 
without any prior knowledge of the optimal preconditioner. A crucial property of our algorithm is its
aﬃne-invariance: Online Lazy Newton is invariant to any aﬃne transformation of the gradients gt  in
the sense that running the algorithm on gradients g(cid:48)
= P−1gt and applying the inverse transformation
P on the produced decisions results with the same decisions that would have been obtained by applying
the algorithm directly to the original vectors gt.
As our main application  we establish a new regret rate for the low rank experts problem  introduced
by Hazan et al. [15]. The low rank experts setting is a variant of the classical prediction with expert
advice problem  where one further assumes that the experts are linearly dependent and their losses
span a low dimensional space of rank r. The challenge in this setting is to achieve a regret rate which
[15] proved a lower bound of Ω(√
is independent of number of experts  and only depends on their rank r. In this setting  Hazan et al.
rT) on the regret  but fell short of providing a matching upper
√
a O((cid:112)rT log T) regret bound  which is optimal up to a(cid:112)log T factor and improves upon the prior
bound and only gave an algorithm achieving a suboptimal O(r
T) regret bound. Applying the Online
Lazy Newton algorithm to this problem  we are able to improve upon the latter bound and achieve

t

bound unless T is exponential in the rank r.

1.1 Related work

Adaptive regularization is an important topic in online optimization that has received considerable
attention in recent years. The AdaGrad algorithm presented in [9] (as well as the closely related
algorithm was analyzed in [22]) dynamically adapts to the geometry of the data. In a sense  AdaGrad
learns the best preconditioner from a trace-bounded family of Mahalanobis norms. (See Section 2.2
for a detailed discussion and comparison of guarantees). The MegaGrad algorithm of van Erven and
Koolen [23] uses a similar dynamic regularization technique in order to obliviously adapt to possible
curvature in the loss functions. Lower bounds for preconditioning when the domain is unbounded
has been presented in [7]. These lower bounds are inapplicable  however  once losses are bounded
(as assumed in this paper). More generally  going beyond worst case analysis and exploiting latent
structure in the data is a very active line of research within online learning. Work in this direction
include adaptation to stochastic i.i.d data (e.g.  [11  12  20  8])  as well as the exploration of various
structural assumptions that can be leveraged for better guarantees [4  12  13  5  19].
Our Online Lazy Newton algorithm is a part of a wide family of algorithms named Follow the
Regularized Leader (FTRL). FTRL methods choose at each iteration the minimizer of past observed
losses with an additional regularization term [16  12  21]. Our algorithm is closely related to the
Follow The Approximate Leader (FTAL) algorithm presented in [14]. The FTAL algorithm is designed
to achieve logarithmic regret rate for exp-concave problems  exploiting the curvature information
of such functions. In contrast  our algorithm is aimed for optimizing general convex functions
with possibly no curvature; while FTAL performs FTL over the second-order approximation of the
functions  Online Lazy Newton instead utilizes a ﬁrst-order approximation with an additional rank-one
quadratic regularizer. Another algorithm closely related to ours is the Second-order Perceptron
algorithm of Cesa-Bianchi et al. [3] (which in turn is closely related to the Vovk-Azoury-Warmuth
forecaster [24  1])  which is a variant of the classical Perceptron algorithm adapted to the case where
the data is “skewed”  or ill-conditioned in the sense used above. Similarly to our algorithm  the
Second-order Perceptron employs adaptive whitening of the data to address its skewness. Finally 
the SON algorithm  proposed in [18] is an enhanced version of Online Newton Step which utilizes
sketching to improve over previous second order online learning algorithms. Similar to our paper 
they propose a version that is completely invariant to linear transformations. Their regret bound (for
our setting) depends linearly on the dimension of the ambient space  and quadratic in the rank of the
loss matrix. In contrast  our regret bounds do not depend on the dimension of the ambient space and
are linear in the rank of the loss matrix – two properties that are necessary in order to achieve optimal
regret bound for the low rank expert problem.

2

rate of(cid:101)O(r

This work is highly inspired and motivated by the problem of low rank experts to which we give an
√
optimal algorithm. The problem was ﬁrst introduced in [15] where the authors established a regret
T)  where r is the rank of the experts’ losses  which was the ﬁrst regret bound in this
setting that did not depend on the total number of experts. The problem has been further studied and
investigated by Cohen and Mannor [6]  Barman et al. [2] and Foster et al. [10]. Here we establish the
ﬁrst tight upper bound (up to logarithmic factor) that is independent of the total number of experts N.

2 Setup and Main Results

We begin by recalling the standard framework of Online Convex Optimization. At each round
t = 1  . . .   T a learner chooses a decision xt from a bounded convex subset (cid:88) ⊆ (cid:82)d in d-dimensional
space. An adversary then chooses a convex cost function ft  and the learner suﬀers a loss of ft(xt).
We measure the performance of the learner in terms of the regret  which is deﬁned as the diﬀerence
between accumulated loss incurred by the learner and the loss of the best decision in hindsight.
Namely  the T-round regret of the learner is given by

T(cid:88)

t=1

T(cid:88)

t=1

RegretT

=

ft(xt) − min
x∈(cid:88)

ft(x).

One typically assumes that the diameter of the set (cid:88) is bounded  and that the convex functions
f1  . . .   fT are all Lipschitz continuous  both with respect to certain norms on (cid:82)d (typically  the norms
are taken as dual to each other). However  a main point of this paper is to refrain from making explicit
assumptions on the geometry of the optimization problem  and design algorithms that are  in a sense 
oblivious to it.
Notation. Given a positive deﬁnite matrix A (cid:31) 0 we will denote by (cid:107) · (cid:107)A the norm induced by A 
namely  (cid:107)x(cid:107)A =
= sup(cid:107)x(cid:107) A≤1 |xTg| and can be
shown to be equal to (cid:107)g(cid:107)∗
= (cid:107)g(cid:107)A−1. Finally  for a non–invertible matrix A  we denote by A† its
Moore–Penrose psuedo inverse.

xT Ax. The dual norm to (cid:107) · (cid:107)A is deﬁned by (cid:107)g(cid:107)∗

√

A

A

2.1 Main Results
Our main results are aﬃne invariant regret bounds for the Online Lazy Newton algorithm  which we
present below in Section 3. We begin with a bound for linear losses that controls the regret in terms
of the intrinsic dimensionality of the problem and a bound on the losses.
Theorem 1. Consider the online convex optimization setting with linear losses ft(x) = gT
assume that |gT
the regret is bounded as

t x  and
t x| ≤ C for all t and x ∈ (cid:88). If Algorithm 1 is run with η < 1/C  then for every H (cid:31) 0

(cid:16)

1 +

(DHGHT)2

r

(cid:17)

t x(cid:63)|2(cid:17)

|gT

T(cid:88)

t=1

+ 3η

1 +

 

(1)

where r = rank((cid:80)T

η

RegretT ≤ 4r
log
t ) ≤ d and
DH = max
x y∈(cid:88)

t=1gtgT

(cid:107)x − y(cid:107)H  

GH = max
1≤t≤T

(cid:107)gt(cid:107)∗
H .

(cid:16)

(cid:16)

t xt|. Then  for every H (cid:31) 0 the regret is bounded as

By a standard reduction  the analogue statement for convex losses holds  as long as we assume that
the dot-products between gradients and decision vectors are bounded.
Corollary 2. Let f1  . . .   fT be an arbitrary sequence of convex functions over (cid:88). Suppose Algorithm 1
is run with 1/η > maxt maxx∈(cid:88)|∇T
RegretT ≤ 4r
log
t ) ≤ d and
t=1∇t∇T
DH = max
x y∈(cid:88)

where r = rank((cid:80)T

t x(cid:63)|2(cid:17)

GH = max
1≤t≤T

(DHGHT)2

(cid:107)x − y(cid:107)H  

(cid:107)∇t(cid:107)∗
H .

T(cid:88)

+ 3η

1 +

|∇T

1 +

(cid:16)

(cid:17)

(2)

t=1

η

 

r

3

In particular  we can use the theorem to show that as long as we bound |∇ f(xt)Txt| by a constant—a
signiﬁcantly weaker requirement than assuming bounds on the diameter of (cid:88)and on the norms of the
gradients—one can ﬁnd a norm (cid:107) · (cid:107)H for which the quantities DH and GH are properly bounded.
We stress again that  importantly  Algorithm 1 need not know the matrix H in order to achieve the
corresponding bound.
Theorem 3. Assume that

Let r = rank((cid:80)T
algorithm is then at most O(cid:0)C(cid:112)rT log T(cid:1).

t ) ≤ d  and run Algorithm 1 with η = Θ(cid:0)(cid:112)r log(T)/T(cid:1). The regret of the

|∇T
t x| ≤ C.

t=1∇t∇T

max
1≤t≤T

max
x∈(cid:88)

2.2 Discussion

It is worth comparing our result to previously studied adaptive regularization algorithms techniques.
Perhaps the most popular gradient method that employs adaptive regularization is the AdaGrad
algorithm introduced in [9]. The AdaGrad algorithm enjoys the regret bound depicted in Eq. (3). It is
competitive with any ﬁxed regularization matrix S (cid:31) 0 such that Tr(S) ≤ d:

(cid:32)√
(cid:18)√

= (cid:101)O

(cid:113)(cid:88)T
(cid:113)(cid:88)T

d inf
S(cid:31)0 
Tr(S)≤d

r inf
S(cid:31)0

t=1 (cid:107)x(cid:63)(cid:107)2

2 (cid:107)∇t(cid:107)2
S∗

t=1 (cid:107)x(cid:63)(cid:107)2

S

(cid:107)∇t(cid:107)2
S∗

(cid:33)

 

.

(cid:19)

RegretT(AdaGrad) = O

RegretT(OLN)

(3)

(4)

t x(cid:63)(cid:107) ≤ (cid:107)∇t(cid:107)∗

On the other hand  for every matrix S (cid:31) 0 by the generalized Cauchy-Schwartz inequality we have
(cid:107)∇T
S(cid:107)x(cid:63)(cid:107)S. Plugging this into Eq. (2) and a proper tuning of η gives a bound which is
competitive with any ﬁxed regularization matrix S (cid:31) 0  depicted in Eq. (4).
Our bound improves on AdaGrad’s regret bound in two ways. First  the bound in Eq. (4) scales with
the intrinsic dimension of the problem: when the true underlying dimensionality of the problem is
smaller than the dimension of the ambient space  Online Lazy Newton enjoys a superior regret bound.
Furthermore  as demonstrated in [15]  the dependence of AdaGrad’s regret on the ambient dimension
is not an artifact of the analysis  and there are cases where the actual regret grows polynomially with
d rather than with the true rank r (cid:28) d.
The second case where the Online Lazy Newton bound can be superior over AdaGrad’s is when there
exists a conditioning matrix that improves not only the norm of the gradients with respect to the
Euclidean norm  but also that the norm of x(cid:63) is smaller with respect to the optimal norm induced by S.
2  and in particular when (cid:107)x(cid:63)(cid:107)S < (cid:107)x(cid:63)(cid:107)2 

More generally  whenever(cid:80)T

S (cid:107)x(cid:63)(cid:107)2
Eq. (4) will produce a tighter bound than the one in Eq. (3).

t x(cid:63))2 <(cid:80)T

t=1 (cid:107)∇t(cid:107)2

t=1(∇T

3 The Online Lazy Newton Algorithm

We next present the main focus of this paper: the aﬃne-invariant algorithm Online Lazy Newton (OLN) 
given in Algorithm 1. The algorithm maintains two vectors  xt and yt. The vector yt is updated at
each iteration using the gradient of ft at xt  via yt = yt−1 − ∇t where ∇t = ∇ ft(xt). The vector yt is
not guaranteed to be at (cid:88)  hence the actual prediction of OLN is determined via a projection onto the
set (cid:88)  resulting with the vector xt+1 ∈ (cid:88). However  similarly to ONS  the algorithm ﬁrst transforms
s  and
projections are taken with respect to At. In this context  we use the notation

yt via the (pseudo-)inverse of the matrix At given by the sum of the outer products(cid:80)t

s=1∇s∇T

(cid:88)(y) = arg min
Π A
x∈(cid:88)

(x − y)T A(x − y).

to denote the projection onto a set (cid:88) with respect to the (semi-)norm (cid:107) · (cid:107)A induced by a positive
semideﬁnite matrix A (cid:23) 0.

4

Algorithm 1 OLN: Online Lazy Newton

Parameters: initial point x1 ∈ (cid:88)  step size η > 0
Initialize y0 = 0 and A0 = 0
for t = 1  2 . . . T do

Play xt  incur cost ft(xt)  observe gradient ∇t = ∇ ft(xt)
Rank 1 update At = At−1 + η∇t∇T
Online Newton step and projection:

t

yt = yt−1 − ∇t
(A†
t yt)
xt+1 = Π At

(cid:88)

end for
return

The main motivation behind working with the At as preconditioners is that—as demonstrated in
our analysis—the algorithm becomes invariant to linear transformations of the gradient vectors ∇t.
Indeed  if P is some linear transformation  one can observe that if we run the algorithm on P∇t instead
of ∇t  this will transform the solution at step t from xt to P−1xt. In turn  the cumulative regret is
invariant to such transformations. As seen in Theorem 1  this invariance indeed leads to an algorithm
with an improved regret bound when the input representation of the data is poorly conditioned.
While the algorithm is very similar to ONS  it is diﬀerent in several important aspects. First  unlike
ONS  our lazy version maintains at each step a vector yt which is updated without any projections.
Projection is then applied only when we need to calculate xt+1. In that sense  it can be thought as a
gradient descent method with lazy projections (analogous to dual-averaging methods) while ONS is
similar to gradient descent methods with a greedy projection step (reminiscent of mirror-descent type
algorithms). The eﬀect of this  is a decoupling between past and future conditioning and projections:
and if the transformation matrix At changes between rounds  the lazy approach allows us to condition
and project the problem at each iteration independently.
Second  ONS uses an initialization of A0 =  Id (while OLN uses A0 = 0) and  as a result  it is not
invariant to aﬃne transformations. While this diﬀerence might seem negligible as  is typically chosen
to be very small  recall that the matrices At are used as preconditioners and their small eigenvalues
can be very meaningful  especially when the problem at hand is poorly conditioned.

4 Application: Low Rank Experts

In this section we consider the Low-rank Experts problem and show how the Online Lazy Newton
algorithm can be used to obtain a nearly optimal regret in that setting. In the Low-rank Experts
problem  which is a variant of the classic problem of prediction with expert advice  a learner has to
choose at each round t = 1  . . .   T between following the advice of one of N experts. On round t 
the learner chooses a distribution over the experts in the form of a probability vector xt ∈ ∆n (here
∆n denotes the n-dimensional simplex); thereafter  an adversary chooses a cost vector gt ∈ [−1  1]N
t gt ∈ [−1  1]. In contrast with the standard
assigning losses to experts  and the player suﬀers a loss of xT
experts setting  here we assume that in hindsight the expert share a common low rank structure and
we have that rank(g1  . . .   gT) ≤ r  for some r < N.
It is known that in the stochastic setting (i.e.  gt are drawn i.i.d. from some ﬁxed distribution) a
follow-the-leader algorithm will enjoy a regret bound of min{√
Set η =(cid:112)r log(T)/T  and run Algorithm 1 with (cid:88) = ∆n and ft(x) = gT
This bound matches the Ω(√
O(r

asked whether one can achieve the same regret bound in the online setting. Here we answer this
question on the aﬃrmative.
Theorem 4 (Low Rank Experts). Consider the low rank expert setting  where rank(g1  . . .   gT) ≤ r.
t x. Then  the obtained regret
satisﬁes

rT) lower bound of [15] up to a log T factor  and improves upon their
√
T) upper bound  so long as T is not exponential in r. Put diﬀerently  if one aims at ensuring an

rT (cid:112)T log N}. In [15] the authors

= O((cid:112)rT log T).

RegretT

5

average regret of at most   the OLN algorithm would need O((r/2) log(1/)) iterations as opposed
to the O(r2/2) iterations required by the algorithm of [15]. We also remark that  since the Hedge

algorithm can be used to obtain regret rate of O((cid:112)T log N)  we can obtain an algorithm with regret
bound of the form O(cid:0) min(cid:8)(cid:112)rT log T (cid:112)T log N(cid:9)(cid:1) by treating Hedge and OLN as meta-experts and

applying Hedge over them.

5 Analysis

For the proofs of our main theorems we will rely on the following two technical lemmas.
Lemma 5 ([17]  Lemma 5). Let Φ1  Φ2 : (cid:88)(cid:55)→ (cid:82) be two convex functions deﬁned over a closed and
convex domain (cid:88) ⊆ (cid:82)d  and let x1 ∈ arg minx∈(cid:88) Φ1(x) and x2 ∈ arg minx∈(cid:88) Φ2(x). Assume that Φ2
is σ-strongly-convex with respect to a norm (cid:107) · (cid:107). Then  for φ = Φ2 − Φ1 we have

Furthermore  if φ is convex then

(cid:107)x2 − x1(cid:107) ≤ 1

σ

(cid:107)∇φ(x1)(cid:107)∗

.

(cid:0)(cid:107)∇φ(x1)(cid:107)∗(cid:1)2

.

0 ≤ φ(x1) − φ(x2) ≤ 1

σ

Lemma 6. Let g1  . . .   gT ∈ (cid:82)d be a sequence of vectors  and deﬁne Gt = H +(cid:80)t
T(cid:88)
where r is the rank of the matrix(cid:80)t

The following lemma is a slight strengthening of a result given in [14].
where H is a positive deﬁnite matrix such that (cid:107)gt(cid:107)∗
H ≤ γ for all t. Then
t gt ≤ r log
t G−1
gT
t=1
.
s=1 gsgT
Proof. Following [14]  we ﬁrst prove that
t gt ≤ log

= log det(cid:0)H−1/2GT H−1/2(cid:1).

T(cid:88)

t G−1
gT

γ2T
r

1 +

(cid:17)

(cid:16)

 

s

det GT
det H

t=1

To this end  let G0 = H  so that we have Gt = Gt−1 + gtgT
determinant lemma  which states that det(A − uuT) = (1 − uT A−1u) det(A)  gives

s=1 gsgT

s

for all t 

(5)
t for all t ≥ 1. The well-known matrix
= 1 − det(Gt−1)
det Gt

.

t=1

t=1

log

= log

det Gt

t G−1
gT

det GT
det H  

det Gt
det Gt−1

Using the inequality 1 − x ≤ log(1/x) and summing over t = 1  . . .   T  we obtain

which yields Eq. (5).

t gt = 1 − det(Gt − gtgT
t )
t G−1
gT
t gt ≤ T(cid:88)
T(cid:88)
Next  observe that H−1/2GT H−1/2 = I +(cid:80)T
(cid:33)
T(cid:88)
T(cid:88)
(cid:1) =
Tr(cid:0)gT
s H−1/2 = H−1/2((cid:80)T
Also  the rank of the matrix(cid:80)T
(cid:16) 1
r(cid:88)

log det(cid:0)H−1/2GT H−1/2(cid:1) =

s=1 H−1/2gsgT
s H−1gs

log λi ≤ r log

H−1/2gsgT

H)2 ≤ γ2T .
s)H−1/2 is at most r. Hence 
all the eigenvalues of the matrix H−1/2GT H−1/2 are equal to 1  except for r of them whose sum is at
most r + γ2T. Denote the latter by λ1  . . .   λr; using the concavity of log(·) and Jensen’s inequality 
we conclude that

s H−1/2
s=1 H−1/2gsgT

s=1
s=1 gsgT

(cid:17) ≤ r log

(cid:32) T(cid:88)

s H−1/2 and

r(cid:88)

((cid:107)gs(cid:107)∗

(cid:17)

(cid:16)

Tr

s=1

=

s=1

i=1

λi

r

i=1

1 +

γ2T
r

 

(cid:3)

which together with Eq. (5) gives the lemma.

6

We can now prove our main results. We begin by proving Theorem 1.

Proof of Theorem 1. For all t  let

˜ft(x) = gT

t x +

(gT
t x)2

η
2

and set

(cid:101)Ft(x) =
(cid:101)Ft(x) =

s=1

t(cid:88)
(cid:0)x − A†

1
2

Observe that xt+1  which is the choice of Algorithm 1 at iteration t + 1  is the minimizer of(cid:101)Ft; indeed 

xT Atx.

t x +

since yt is in the column span of At  we can write up to a constant:

˜fs(x) = −yT

1
2

(cid:1)T At

(cid:0)x − A†

t yt

(cid:1) + const.

t yt

In other words  Algorithm 1 is equivalent to a follow-the-leader algorithm on the functions ˜ft.
Next  ﬁx some positive deﬁnite matrix H (cid:31) 0 and let DH = maxx y∈(cid:88) (cid:107)x − y(cid:107)H and GH =
max1≤t≤T (cid:107)gt(cid:107)∗

H. Next we have

(cid:101)Ft(x) + η

H

=

2 (cid:107)x − xt+1(cid:107)2
1
2 (cid:107)x − xt+1(cid:107)2
xT Atx − yT
t x + η
2
1
t x − 2xT
(cid:107)x(cid:107)2
H − yT
(cid:107)x(cid:107)2
η
2
2
t x − 2xT
(cid:107)x(cid:107)2
− yT
t+1x + (cid:107)xt+1(cid:107)2
η
H 
2

Gt

+

At

H

=

=

=

t+1x + (cid:107)xt+1(cid:107)2

H

where Gt =(cid:80)t
Gt = H +(cid:80) gtgT
Φ2(x) =(cid:101)Ft(x) + η

˜ft(xt) − ˜ft(xt+1) +

s=1 gtgT

In turn  we have that the function is η-strongly convex with respect to the norm (cid:107) · (cid:107)Gt   where

t   and is minimized at x = xt+1. Then by Lemma 5 with Φ1(x) = (cid:101)Ft−1(x) and

+ H.

t

2 (cid:107)x − xt+1(cid:107)2

H  thus φ(x) = ˜ft(x) + η

2 (cid:107)x − xt+1(cid:107)2

H  we have

)2

Gt

η
2

(cid:107)xt − xt+1(cid:107)2
t xt + ηH(xt − xt+1)(cid:107)∗

H

((cid:107)gt + ηgtgT
(1 + ηgT
((cid:107)gt(cid:107)∗
((cid:107)gt(cid:107)∗
((cid:107)gt(cid:107)∗

Gt

Gt

Gt

t xt)2((cid:107)gt(cid:107)∗
)2 + 2η((cid:107)H(xt − xt+1)(cid:107)∗
)2 + 2η((cid:107)H(xt − xt+1)(cid:107)∗
)2 + 2η(cid:107)xt − xt+1(cid:107)2
H .

Gt

Gt

)2
H)2

≤ 1
η
≤ 2
η
≤ 8
η
≤ 8
η
8
η

=

)2 + 2η((cid:107)H(xt − xt+1)(cid:107)∗

Gt

Overall  we obtain

By the FTL-BTL Lemma (e.g.  [16])  we have that(cid:80)T

˜ft(xt) − ˜ft(xt+1) ≤ 8

t G−1
gT

η

t gt +

t=1

Hence  we obtain that:

˜ft(xt) − ˜ft(x(cid:63)) ≤ 8

η

t G−1
gT

t gt +

t=1

T(cid:88)
T(cid:88)

t=1

t=1

T(cid:88)
T(cid:88)

t=1

7

)2

∵ (cid:107)v + u(cid:107)2 ≤ 2(cid:107)v(cid:107)2 + 2(cid:107)u(cid:107)2
∵ 1
η

≥ max
x∈(cid:88)

|gT
t x|

∵ H ≺ Gt ⇒ H−1 (cid:31) G−1

t

t=1

3η
2

(cid:107)xt − xt+1(cid:107)2
H .

T(cid:88)
˜ft(xt) − ˜ft(x(cid:63)) ≤(cid:80)T
T(cid:88)

(cid:107)xt − xt+1(cid:107)2
H .

3η
2

t=1

˜ft(xt) − ˜ft(xt+1).

t=1

T(cid:88)

Plugging in ft(x) = gT

t x)2 and rearranging  we obtain

T(cid:88)

gT

t

2(gT

t x + η

(cid:0)xt − x(cid:63)(cid:1) ≤ 8

T(cid:88)
T(cid:88)

t=1

T(cid:88)
T(cid:88)

3η
2

t G−1
gT

t gt +

(cid:107)xt − xt+1(cid:107)2

H

+

η
2

(gT
t x(cid:63))2

t=1

t=1

t=1

η
2

(cid:17)

t=1
(gT
t x(cid:63))2 +

t G−1
t gt +
gT
G2
HT
r

η
≤ 8
η
≤ 8r
η
Finally  note that we have obtained the last inequality for every matrix H (cid:31) 0. By rescaling a matrix H
and re-parametrizing H → H/(√
GH → √
T and
(cid:3)
Proof of Theorem 3. To simplify notations  let us assume that |∇T
t x∗| ≤ 1. We get from Corollary 2
that for every η:

(cid:16)
T DH) we obtain a matrix whose diameter is DH → 1/√

T DHGH. Plugging these into the last inequality yield the result.

T(cid:88)

3η
2 T D2
(gT
t x(cid:63))2 

t=1
3η
2 T D2

1 +

log

η
2

t=1

+

+

H

H

For each GH and DH we can set η =

RegretT ≤ 2r

(cid:17)

D2G2T2

log

1 +

(cid:16)
(cid:113)(2r/T) log(1 + G2
(cid:115)
(cid:16)

η

r

rT log

1 +

RegretT ≤

H D2

+ 3η(1 + T).
H/r) and obtain the regret bound
D2
HG2
r

HT

(cid:17)

.

Hence  we only need to show that there exists a matrix H (cid:31) 0 such that D2
= O(r). Indeed  set
S = span(∇1  . . .   ∇T)  and denote (cid:88)S to be the projection of (cid:88) onto S (i.e.  (cid:88)S = P(cid:88) where P is the
projection over S). Deﬁne

HG2

H

(cid:66) = {∇ ∈ S : |∇Tx| ≤ 1  ∀ x ∈ (cid:88)S}.

t=1 ⊆ (cid:66). Further  (cid:66) is a symmetric convex set  hence by an
Note that by deﬁnition we have that {∇t}T
ellipsoid approximation we obtain a positive semideﬁnite matrix B (cid:23) 0  with positive eigenvalues
restricted to S  such that

(cid:66) ⊆ {∇ ∈ S : ∇TB∇ ≤ 1} ⊆ r (cid:66).
r (cid:88)S ⊆ 1
∗ ⊆ {v ∈ S : vTB†v ≤ 1}.

r (cid:66)

By duality we have that
Thus if PS is the projection over S we have for every x ∈ (cid:88) that xTPS B†PSx ≤ r. On the other hand
t B∇t ≤ 1. We can now choose H = B† +  Id where  is arbitrary small and
for every ∇t we have ∇T
have

1

∇T
t H−1∇t = ∇T

t (B† +  Id)−1∇t ≤ 2

and

Acknowledgements

xTHx = xTPT

S B†PSx + (cid:107)x(cid:107)2 ≤ 2r.

(cid:3)

The authors would like to thank Elad Hazan for helpful discussions. RL is supported by the Eric and
Wendy Schmidt Fund for Strategic Innovations.

References
[1] K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the

exponential family of distributions. Machine Learning  43(3):211–246  2001.

[2] S. Barman  A. Gopalan  and A. Saha. Online learning for structured loss spaces. arXiv preprint

arXiv:1706.04125  2017.

8

[3] N. Cesa-Bianchi  A. Conconi  and C. Gentile. A second-order perceptron algorithm. SIAM

Journal on Computing  34(3):640–668  2005.

[4] N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with

expert advice. Machine Learning  66(2-3):321–352  2007.

[5] C.-K. Chiang  T. Yang  C.-J. Lee  M. Mahdavi  C.-J. Lu  R. Jin  and S. Zhu. Online optimization

with gradual variations. In COLT  pages 6–1  2012.

[6] A. Cohen and S. Mannor. Online learning with many experts. arXiv preprint arXiv:1702.07870 

2017.

[7] A. Cutkosky and K. Boahen. Online learning without prior information. arXiv preprint

arXiv:1703.02629  2017.

[8] S. De Rooij  T. Van Erven  P. D. Grünwald  and W. M. Koolen. Follow the leader if you can 

hedge if you must. The Journal of Machine Learning Research  15(1):1281–1316  2014.

[9] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. The Journal of Machine Learning Research  12:2121–2159  2011.

[10] D. J. Foster  A. Rakhlin  and K. Sridharan. Zigzag: A new approach to adaptive online learning.

arXiv preprint arXiv:1704.04010  2017.

[11] E. Hazan and S. Kale. On stochastic and worst-case models for investing. In Advances in Neural

Information Processing Systems  pages 709–717  2009.

[12] E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in

costs. Machine learning  80(2-3):165–188  2010.

[13] E. Hazan and S. Kale. Better algorithms for benign bandits. The Journal of Machine Learning

Research  12:1287–1311  2011.

[14] E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex
optimization. In International Conference on Computational Learning Theory  pages 499–513.
Springer  2006.

[15] E. Hazan  T. Koren  R. Livni  and Y. Mansour. Online learning with low rank experts. In 29th

Annual Conference on Learning Theory  pages 1096–1114  2016.

[16] A. Kalai and S. Vempala. Eﬃcient algorithms for online decision problems. Journal of Computer

and System Sciences  71(3):291–307  2005.

[17] T. Koren and K. Levy. Fast rates for exp-concave empirical risk minimization. In Advances in

Neural Information Processing Systems  pages 1477–1485  2015.

[18] H. Luo  A. Agarwal  N. Cesa-Bianchi  and J. Langford. Eﬃcient second order online learning
by sketching. In Advances in Neural Information Processing Systems  pages 902–910  2016.
[19] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Conference on

Learning Theory  pages 993–1019  2013.

[20] A. Rakhlin  O. Shamir  and K. Sridharan. Localization and adaptation in online learning. In
Proceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics 
pages 516–526  2013.

[21] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends®

in Machine Learning  4(2):107–194  2012.

[22] M. Streeter and H. B. McMahan. Less regret via online conditioning. Technical report  2010.
[23] T. van Erven and W. M. Koolen. Metagrad: Multiple learning rates in online learning. In

Advances in Neural Information Processing Systems  pages 3666–3674  2016.

[24] V. Vovk. Competitive on-line statistics. International Statistical Review  69(2):213–248  2001.
[25] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. 2003.

9

,Tomer Koren
Roi Livni
Shoubo Hu
Zhitang Chen
Vahid Partovi Nia
Laiwan CHAN
Yanhui Geng