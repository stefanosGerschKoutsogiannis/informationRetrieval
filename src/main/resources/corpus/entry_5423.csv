2017,Testing and Learning on Distributions with Symmetric Noise Invariance,Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD)  the resulting distance between distributions  are useful tools for fully nonparametric two-sample testing and learning on distributions. However  it is rarely that all possible differences between samples are of interest -- discovered differences can be due to different types of measurement noise  data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise  aimed at testing whether the assumed true underlying processes differ. Moreover  we construct invariant features of distributions  leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise.,Testing and Learning on Distributions with

Symmetric Noise Invariance

Ho Chung Leon Law
Department of Statistics
University Of Oxford

hlaw@stats.ox.ac.uk

Christopher Yau

Centre for Computational Biology

University of Birmingham
c.yau@bham.ac.uk

Dino Sejdinovic

Department of Statistics
University Of Oxford

dino.sejdinovic@stats.ox.ac.uk

Abstract

Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD) 
the resulting distance between distributions  are useful tools for fully nonparametric
two-sample testing and learning on distributions. However  it is rare that all possible
differences between samples are of interest – discovered differences can be due to
different types of measurement noise  data collection artefacts or other irrelevant
sources of variability. We propose distances between distributions which encode
invariance to additive symmetric noise  aimed at testing whether the assumed
true underlying processes differ. Moreover  we construct invariant features of
distributions  leading to learning algorithms robust to the impairment of the input
distributions with symmetric additive noise.

1

Introduction

j=1 and {X2j}N2

There are many sources of variability in data  and not all of them are pertinent to the questions that
a data analyst may be interested in. Consider  for example  a nonparametric two-sample testing
problem  which has recently been attracting signiﬁcant research interest  especially in the context
of kernel embeddings of distributions [2  5  7]. We observe samples {X1j}N1
j=1 from
two data generating processes P1 and P2  respectively  and would like to test the null hypothesis that
P1 = P2 without making any parametric assumptions on these distributions. With a large sample-size 
the minutiae of the two data generating processes are uncovered (e.g. slightly different calibration
of the data collecting equipment  different numerical precision)  and we ultimately reject the null
hypothesis  even if the sources of variation across the two samples may be irrelevant for the analysis.
Similarly  we may be interested in learning on distributions [14  23  24]  where the appropriate
level of granularity in the data is distributional. For example  each label yi in supervised learning
is associated to a whole bag of observations Bi = {Xij}Ni
j=1 – assumed to come from a probability
distribution Pi  or we may be interested in clustering such bags of observations. Again  nonparametric
distances used in such contexts to facilitate a learning algorithm on distributions  such as Maximum
Mean Discrepancy (MMD) [5]  can be sensitive to irrelevant sources of variation and may lead to
suboptimal or even misleading results  in which case building predictors which are invariant to noise
is of interest.
While it may be tempting to revert back to a parametric setup and work with simple  easy to interpret
models  we argue that a different approach is possible: we stay within a nonparametric framework 
exploit the irregular and complicated nature of real life distributions and encode invariances to sources

of variation assumed to be irrelevant. In this contribution  we focus on invariances to symmetric
additive noise on each of the data generating distributions. Namely  assume that the i-th sample
{Xij}Ni
j=1 we observe does not follow the distribution Pi of interest but instead its convolution Pi (cid:63)Ei
with some unknown noise distributions Ei assumed to be symmetric about 0 (we also require that it
has a positive characteristic function). We would like to assess the differences between Pi and Pi(cid:48)
while allowing Ei and Ei(cid:48) to differ in an arbitrary way. We investigate two approaches to this problem:
(1) measuring the degree of asymmetry of the paired differences {Xij − Xi(cid:48)j}  and (2) comparing
the phase functions of the corresponding samples. While the ﬁrst approach is simpler and presents
a sensible solution for the two-sample testing problem  we demonstrate that phase functions give a
much better gauge on the relative comparisons between bags of observations  as required for learning
on distributions.
The paper is outlined as follows. In section 2  we provide an overview of the background. In section 3 
we provide details of the construction and implementation of phase features. In section 4  we discuss
the approach based on asymmetry in paired differences for two sample testing with invariances.
Section 5 provides experiments on synthetic and real data  before concluding in section 6.

2 Background and Setup

(cid:2)exp(iω(cid:62)E)(cid:3) > 0  ∀ω ∈ Rd. This means

We will say that a random vector E on Rd is a symmetric positive deﬁnite (SPD) component if its
characteristic function is positive  i.e. ϕE(ω) = EX∼E
that E is (1) symmetric about zero  i.e. E and −E have the same distribution and (2) if it has a
density  this density must be a positive deﬁnite function [20]. Note that many distributions used to
model additive noise  including the spherical zero-mean Gaussian distribution  as well as multivariate
Laplace  Cauchy or Student’s t (but not uniform)  are all SPD components.
Following the terminology similar to that of [3]  we will say that a random vector X on Rd is
decomposable if its characteristic function can be written as ϕX = ϕX0 ϕE  with ϕE > 0. Thus 
if X can be written in the form X = X0 + E  where X0 and E are independent and E is an
SPD noise component  then X is decomposable. We will say that X is indecomposable if it is
not decomposable. In this paper  we will assume that mostly the indecomposable components of
distributions are of interest and will construct tools to directly measure differences between these
indecomposable components  encoding invariance to other sources of variability. The class of Borel
Probability measures on Rd will be denoted M1
+(Rd)  while the class of indecomposable probability
measures will be denoted by I(Rd) ⊆ M1
+(Rd).

2.1 Kernel Embeddings  Fourier Features and learning on distributions
For any positive deﬁnite function k : X × X (cid:55)→ R  there exists a unique reproducing kernel Hilbert
space (RKHS) Hk of real-valued functions on X . Function k(·  x) is an element of Hk and represents
(cid:82)
evaluation at x  i.e. (cid:104)f  k(·  x)(cid:105)H = f (x)  ∀f ∈ Hk  ∀x ∈ X . The kernel mean embedding
(cf. [15] for a recent review) of a probability measure P is deﬁned by µP = EX∼P [k(·  X)] =
X k(·  x)dP (x). The Maximum Mean Discrepancy (MMD) between probability measures P and Q
is then given by (cid:107)µP − µQ(cid:107)Hk. For shift-invariant kernels on Rd  using Bochner’s characterisation of
positive deﬁniteness [26  6.2]  the squared MMD can be written as a weighted L2-distance between
characteristic functions [22  Corollary 4]

(cid:90)

Rd

(cid:107)µP − µQ(cid:107)2Hk

=

|ϕP (ω) − ϕQ (ω)|2 dΛ (ω)  

(1)

where Λ is the non-negative spectral measure (inverse Fourier transform) of kernel k as a function of
x − y  while ϕP (ω) and ϕQ(ω) are the characteristic functions of probability measures P and Q.
Bochner’s theorem is also used to construct random Fourier features (RFF) [19] for fast approxi-
mations to kernel methods in order to approximate a pre-speciﬁed shift-invariant kernel by a ﬁnite
dimensional explicit feature map. If we can draw samples from its spectral measure Λ  we can

2

j y)(cid:3) = (cid:104)φ(x)  φ(y)(cid:105)R2m

approximate k by1

ˆk(x  y) =

m(cid:88)

j=1

1
m

(cid:2) cos(ωT

j x) cos(ωT

j y) + sin(ωT

j x) sin(ωT

(cid:113) 1

m

(cid:2)cos(cid:0)ω(cid:62)

1 x(cid:1)   sin(cid:0)ω(cid:62)

1 x(cid:1) . . .   cos(cid:0)ω(cid:62)

mx(cid:1)   sin(cid:0)ω(cid:62)

mx(cid:1)(cid:3) . Thus  the

where ω1  . . .   ωm ∼ Λ and φ(x) :=
explicit computation of the kernel matrix is not needed and the computational complexity is
reduced. This also allows computation with the approximate  ﬁnite-dimensional embeddings
˜µP = Φ(P ) = EX∼P φ(X) ∈ R2m  which can be understood as the evaluations (real and complex
part stacked together) of the characteristic function ϕP at frequencies ω1  . . .   ωm. We will refer to
the approximate embeddings Φ(P ) as Fourier features of distribution P .
Kernel embeddings can be used for supervised learning on distributions. Assume we have a training
set {Bi  yi}n
j=1 is a bag of samples taking values in X   and yi is
i=1  where input Bi = {xij}Ni
(cid:80)Ni
a response. Given a kernel k : X × X → R  we ﬁrst map each Bi to the empirical embedding
j=1 k(·  xij) ∈ Hk and then can apply any positive deﬁnite kernel on Hk as the kernel
µ ˆPi
  µ ˆPi(cid:48)(cid:105)Hk  in order to perform classiﬁcation [14]
on bag inputs  e.g. linear kernel ˜K(Bi  B(cid:48)
or regression [24]. Approximate kernel embeddings have also been applied in this context [23].

i) = (cid:104)µ ˆPi

= 1
Ni

3 Phase Discrepancy and Phase Features

While MMD and kernel embeddings are related to characteristic functions  and indeed the same
connection forms a basis for fast approximations to kernel methods using random Fourier features
[19]  the relevant notion in our context is the phase function of a probability measure  recently used
for nonparametric deconvolution by [3]. In this section  we overview this formalism. Based on
the empirical phase functions  we will then derive and investigate hypothesis testing and learning
framework using phase features of distributions.
In nonparametric deconvolution [3]  the goal is to estimate the density function f0 of a univariate r.v.
iid∼ X = X0 + E  where E denotes
X0  but in general we only have noisy data samples X1  . . .   Xn
an independent noise term. Even though the distribution of E is unknown  making the assumption
that E is an SPD noise component  and that X0 is indecomposable  i.e. X0 itself does not contain
any SPD noise components  [3] show that it is possible to obtain consistent estimates of f0.
They distinguish between the symmetric noise and the underlying indecomposable component by
matching phase functions  deﬁned as

ρX (ω) =

ϕX (ω)
|ϕX (ω)|

where ϕX (ω) denotes the characteristic function of X. Observe that |ρX (ω)| = 1  and thus we
are effectively removing the amplitude information from the characteristic function. For a SPD
noise component E  the phase function is ρE(ω) ≡ 1. But then since ϕX = ϕX0ϕE  we have that
ρX0 = ρX = ϕX /|ϕX|  i.e. the phase function is invariant to additive SPD noise components. This
motivates us to construct explicit feature maps of distributions with the same property and similarly
to the motivation of [3]  we argue that real-world distributions of interest often exhibit certain amount
of irregularity and it is exactly this irregularity which is exploited in our methodology.
In analogy to the MMD  we ﬁrst deﬁne the phase discrepancy (PhD) as a weighted L2-distances
between the phase functions:

PhD(X  Y ) =

|ρX (ω) − ρY (ω)|2 dΛ (ω)

(2)

(cid:90)

Rd

for some non-negative measure Λ (w.l.o.g. a probability measure). Now suppose we write X =
X0 + U  Y = Y0 + V   where U and V are SPD noise components. This then implies ρX = ρX0
and ρY = ρY0 Λ-everywhere  so that PhD(X  Y ) = PhD(X0  Y0). It is clear then that the PhD is

(cid:2)exp(cid:0)iω(cid:62)

1 x(cid:1)   . . .   exp(cid:0)iω(cid:62)

mx(cid:1)(cid:3) can also be used  but we follow the con-

1a complex feature map φ(x) =

(cid:113) 1

m

vention of real-valued Fourier features  since kernels of interest are typically real-valued.

3

where ξω (x) =(cid:2)cos(cid:0)ω(cid:62)x(cid:1)   sin(cid:0)ω(cid:62)x(cid:1)(cid:3)(cid:62)

Proposition 2.

not affected by additive SPD noise components  so it captures desired invariance. However  the
PhD for Λ supported everywhere is in fact not a proper metric on the indecomposable probability
measures I(Rd)  as one can ﬁnd indecomposable random variables X and Y s.t. ρX = ρY and thus
PhD(X  Y ) = 0. An example is given in Appendix A.
While such cases appear contrived  we hence restrict attention to a subset of indecomposable
probability measures P(Rd) ⊂ I(Rd)  which are uniquely determined by phase functions  i.e.
∀P  Q ∈ P(Rd) : ρP = ρQ ⇒ P = Q.
We now have the two following propositions (proofs are given in Appendix B).
Proposition 1.

dΛ(ω)

and (cid:107) · (cid:107) denotes the standard L2 norm.

(cid:17)

(cid:107)Eξω(Y )(cid:107)

(cid:107)Eξω(X)(cid:107)

(cid:17)(cid:62)(cid:16) Eξω(Y )
PhD(X  Y ) = 2 − 2(cid:82)(cid:16) Eξω(X)
(cid:17)
(cid:17)(cid:62)(cid:16) Eξω(Y )
K (PX   PY ) =(cid:82)(cid:16) Eξω(X)
(cid:20) Eξω1 (X)
(cid:113) 1

dΛ(ω)

(cid:107)Eξω(Y )(cid:107)

(cid:107)Eξω(X)(cid:107)
is a positive deﬁnite kernel on probability measures.
i=1 ∼
Now  we can construct an approximate explicit feature map for kernel K. Taking a sample {ωi}m
Λ  we deﬁne Ψ : PX (cid:55)→ R2m given by Ψ(PX ) =
. We will refer
to Ψ(·) as the phase features. Note that these are very similar to Fourier features  but the cos  sin-pair
corresponding to each frequency is normalised to have unit L2 norm. In other words  Ψ(·) can be
thought of as evaluations of the phase function at the selected frequencies. By construction  phase
features are invariant to additive SPD noise components. For an empirical measure  we simply have
the following:

(cid:107)Eξω1 (X)(cid:107)   . . .  

Eξωm (X)
(cid:107)Eξωm (X)(cid:107)

(cid:21)

m

(cid:21)

(cid:13)(cid:13)(cid:13)Ψ( ˆPX )

(3)

(cid:13)(cid:13)(cid:13) = 1  we

Ψ( ˆPX ) =

(cid:107)ˆEξω1 (X)(cid:107)   . . .  

ˆEξωm (X)
(cid:107)ˆEξωm (X)(cid:107)

m

(cid:20) ˆEξω1 (X)
(cid:113) 1
(cid:13)(cid:13)(cid:13)Ψ( ˆPX ) − Ψ( ˆPY )
(cid:13)(cid:13)(cid:13)2

where we have replaced the expectations by their empirical estimates. Because

can construct (cid:100)PhD( ˆPX   ˆPY ) =

= 2 − 2Ψ( ˆPX )(cid:62)Ψ( ˆPY ) 

(4)
which is a Monte Carlo estimator of PhD( ˆPX   ˆPY ). In summary  Ψ( ˆP ) ∈ R2m is an explicit feature
vector of the empirical distribution which encodes invariance to additive SPD noise components
present in P 2  as demonstrated in Figure F.1 in the Appendix. It can now be directly applied to (1)
two-sample testing up to SPD components  where the distance between the phase features  i.e. an
estimate (4) of the PhD  can be used as a test statistic  with details given in section 5.1 and (2) learning
on distributions  where we use phase features as the explicit feature map for a bag of samples.
Although we have assumed an indecomposable underlying distribution so far  this assumption is
not strict. For distribution regression  if the indecomposable assumption is invalid  given that the
underlying distribution is irregular  it may still be useful to encode invariance as long as the beneﬁt
of removing the SPD components irrelevant for learning outweighs the signal in the SPD part of
the distribution  i.e. there is a trade off between SPD noise and SPD signal. In practice  the phase
features we propose can be used to encode such invariance where appropriate or in conjunction with
other features which do not encode invariance.
In order to construct the approximate mean embeddings for learning  we ﬁrst compute an
explicit feature map by taking averages of the Fourier features  as given by Φ( ˆPX ) =
. For phase features  we need to compute an additional normal-
i=1  we can draw

isation term over each frequency as in (3). To obtain the set of frequencies {wi}m

(cid:104)ˆEξω1(X)  . . .   ˆEξωm(X)

(cid:113) 1

(cid:105)

m

2Note that  unlike the population expression Ψ(P )  the empirical estimator Ψ( ˆP ) will in general have a
distribution affected by the noise components and is thus only approximately invariant  but we observe that it
captures invariance very well as long as the signal-to-noise regime remains relatively high (Section 5.1).

4

samples from a probability measure Λ corresponding to an inverse Fourier transform of a shift-
invariant kernel  e.g. Gaussian Kernel. However  given a supervised signal  we can also optimise a set
of frequencies {wi}m
i=1 that will give us a useful representation and good discriminative performance.
In other words  we no longer focus on a speciﬁc shift-invariant kernel k  but are learning discrim-
inative Fourier/phase features. To do this  we can construct a neural network (NN) with special
activation functions  pooling layers as shown in Algorithm D.1 and Figure D.1 in the Appendix.

4 Asymmetry in Paired Differences

We now consider a separate approach to nonparametric two-sample test  where we wish to test the
null hypothesis that H0 : P d=Q vs. the general alternative  but we only have iid samples arising from
X ∼ P (cid:63) E1 and Y ∼ Q (cid:63) E2. i.e.

X = X0 + U Y = Y0 + V

d=Y0.

i=1 and {Yi}n

where X0 ∼ P   Y0 ∼ Q lie in the space of P(Rd) of indecomposable distributions uniquely
determined by phase functions and U and V are SPD noise components. With this setting (proof in
Appendix B):
Proposition 3. Under the null hypothesis H0  X − Y is SPD ⇐⇒ X0
This motivates us to simply perform a two-sample test on X − Y and Y − X since its rejection would
d=Y0  as it tests for symmetry. However  note that this is a test for symmetry
imply rejection of X0
only and that for consistency against all alternatives  positivity of characteristic function would need
to be checked separately. Now  given two i.i.d. samples {Xi}n
i=1 with n even  we split
the two samples into two halves and compute Wi = Xi − Yi on one half and Zi = Yi − Xi on the
other half  and perform a nonparametric two sample test on W and Z (which are  by construction 
independent of each other). The advantage of this regime is that we can use any two-sample test –
in particular in this paper  we will focus on the linear time mean embedding (ME) test [7]  which
was found to have performance similar to or better than the original MMD two-sample test [5]  and
explicitly formulates a criterion which maximises the test power. We will refer to the resulting test on
paired differences as the Symmetric Mean Embedding (SME).
Although we have assumed here that X0  Y0 lie in the space P(Rd) of indecomposable distributions 
in practice  the SME test would not reject if the underlying distributions of interest differ only in the
symmetric components (or in the SPD components for the PhD test). We argue this to be unlikely due
to real life distributions being complex in nature with interesting differences often having a degree of
asymmetry. In practice  we recommend the use of the ME and SME or PhD test together to provide
an exploratory tool to understand the underlying differences  as demonstrated in the Higgs Data
experiment in section 5.1.
It is tempting to also consider learning on distributions with invariances using this formalism. However
note that the MMD on paired differences is not invariant to the additive SPD noise components under
the alternative  i.e. in general MMD(X − Y  Y − X) (cid:54)= MMD(X0 − Y0  Y0 − X0). This means that
the paired differences approach to learning is sensitive to the actual type and scale of the additive
SPD noise components  hence not suitable for learning. The mathematical details and empirical
experiments to show this are presented in Appendix C and F.1.

5 Experimental Results

5.1 Two-Sample Tests with Invariances

In this section  we demonstrate the performance of the SME test and the PhD test on both artiﬁcial
and real-world data for testing the hypothesis H0 : X0
i=1 from X0 + U
and {Yi}N
i=1 from Y0 + V   where U and V are arbitrary SPD noise components (we assume the same
number of samples for simplicity). SME test follows the setup in [7] but applied to {Xi − Yi}N/2
i=1 and
{Yi − Xi}N
is unclear what the exact form of the null distribution is  so we use a permutation test  by recomputing
this statistic on the samples which are ﬁrst merged and then randomly split in the original proportions.

i=N/2+1. For the PhD test  we use as the test statistic the estimate (cid:100)PhD( ˆPX   ˆPY ) of (2). It

d=Y0 based on samples {Xi}N

5

ϕnull =

ϕX0ϕU +

ϕX0 ϕV = ϕX0(

ϕU +

ϕV )

1
2

1
2

1
2

1
2

Figure 1: Type I error and Power under various additional symmetric noise in the synthetic χ2 dataset.
Dashed line is the 99% Wald interval here. Left: Type I error  n11 denotes the noise to signal ratio
for the ﬁrst set of samples and n12 for the second set. Right: Power  n1 denotes the noise to signal
ratio for the X set of samples and n2 denotes the noise to signal ratio for the Y set of samples.
While we are combining samples with different distributions  the permutation test is still justiﬁed
d=Y0  the resulting characteristic function ϕnull of the mixture
since  under the null hypothesis X0
can be written as

and since the mixture of the SPD noise terms is also SPD  we have that ρnull = ρX0 = ρY0. For our
experiments  we denote by N the sample size  d the dimension of the samples  and we take α = 0.05
to be the signiﬁcance level. In the SME test  we take the number of test locations J to be 10  and
use 20% of the samples to optimise the test locations. All experimental results are averaged over
1000 runs  where each run repeats the simulation or randomly samples without replacement from the
dataset.

5.1.1 Synthetic example: Noisy χ2

1 and n2 = 4σ2
2.

i=1 and {Yn2 i}N
1I) and similarly Yn2 ∼ Y0 + V   where V ∼ N (0  σ2

We start by demonstrating our tests with invariances on a simulated dataset where X0 and Y0 are
random vectors with d = 5  each dimension is the same in distribution and follows χ2(4)/4 and
χ2(8)/8 respectively  i.e. chi-squared random variables  with different degrees of freedom  rescaled to
have the same mean 1 (but have different variances  1/2 and 1/4 respectively). An illustration of the
true and empirical phase and characteristic function with noise for these two distributions can be found
i=1 such that Xn1 ∼ X0 + U  where
in Appendix F.2. We construct samples {Xn1 i}N
U ∼ N (0  σ2
2I)  ni denotes the noise-to-signal
ratio given by the ratio of variances in each dimension  i.e. n1 = 2σ2
We ﬁrst verify that Type I error is indeed controlled at our design level of α = 0.05 up to various
d=Y0  both constructed
additive SPD noise components. This is shown in Figure 1 (left)  where X0
using χ2(4)/4  with the noiseless case found in Figure F.6 in the Appendix. It is noted here that the
ME test rejects the null hypothesis for even a small difference in noise levels  hence it is unable to
let us target the underlying distributions we are concerned with. This is unlike the SME test which
controls the Type I error even for large differences in noise levels. The PhD test  on the other hand 
while correctly controlling Type I at small noise levels  was found to have inﬂated Type I error rates
for large noise  with more results and explanation provided in Figure F.6 in the Appendix. Namely 
the test relies on the invariance to SPD of the population expression of PhD  but the estimator of the
null distribution of the corresponding test statistic will in general be affected by the differing noise
levels.
Next  we investigate the power  shown in Figure 1 (right). For a fair comparison  we have included
the PhD test power only for small noise levels  in which the Type I error is controlled at the design
level. In these cases  the PhD test has better power than the SME test. This is not surprising  as for the
SME we have to halve the sample size in order to construct a valid test. However  recall that the PhD
test has an inﬂated Type I error for large noises  which means that its results should be considered
with caution in practice. ME test rejects at all levels at all sample sizes as it picks up all possible

6

10002000300040005000600070008000Sample Size0.00.20.40.60.81.0Rejection RatioME n11=0.01 n12=0.05PhD n11=0.01 n12=0.05SME n11=0.01 n12=0.05ME n11=0.25 n12=0.5PhD n11=0.25 n12=0.5SME n11=0.25 n12=0.5010002000300040005000600070008000Sample Size0.00.20.40.60.81.0PowerME All levelsPhD n1=0.0 n2=0.0SME n1=0.0 n2=0.0PhD n1=0.01 n2=0.05SME n1=0.01 n2=0.05SME n1=0.1 n2=0.1SME n1=0.25 n2=0.25Figure 2: Rejection ratio vs. sample size for
extremely low level features for Higgs dataset.
Dashed line is the 99% Wald interval for 1000
repetitions for α = 0.05. Note PhD is not used
here  due to its expensive computational cost.

Figure 3: RMSE on the Aerosol test set  cor-
rupted by various levels of noise averaged
over 100 runs  with the 5th and the 95th per-
centile. The noiseless case is shown with one
run. RMSE from mean is 0.206.

differences. SME and PhD are by construction more conservative tests whose rejection provides a
much stronger statement: two samples differ even when all arbitrary additive SPD components have
been stripped off.

5.1.2 Higgs Dataset

The UCI Higgs dataset [1  11] is a dataset with 11 million observations  where the problem is to
distinguish between the signal process where Higgs bosons are found  versus the background process
that do not produce Higgs bosons. In particular  we will consider a two-sample test with the ME
and SME test on the high level features derived by physicists  as well as a two-sample test on four
extremely low level features (azimuthal angular momentum φ measured by four particle jets in the
detector). The high level features here (in R7) have been shown to have good discriminative properties
in [1]. Thus  we expect them to have different distributions across two processes. Denoting by X the
high level features of the process without Higgs Boson  and Y as the corresponding distribution for
the processes where Higgs bosons are produced  we test the null hypothesis that the indecomposable
parts of X and Y agree. The results can be found in Table F.1 in the Appendix  which shows that the
high level features differ even up to additive SPD components  with a high power for the SME and
ME test even at small sample sizes (rejection rate of 0.94 at N = 500). Now we perform the same
experiment  but with the low level features ∈ R4  commented in [1] to carry very little discriminating
information  using the setup from [2].
The results for the ME and SME test can be found in Figure 2. Here we observe that while ME
test clearly rejects and ﬁnds the difference between the two distributions  there is no evidence that
the indecomposable parts of the joint distributions of the angular momentum actually differ. In
fact  the test rejection rate remains around the chosen design level of α = 0.05 for all sample sizes.
This highlights the signiﬁcance in using the SME test  suggesting that the nature of the difference
between the two processes can potentially be explained by some additive symmetric noise components
which may be irrelevant for discrimination  providing an insight into the dataset. Furthermore  this
also highlights the argument that given two samples from complex data collection and generation
processes  a nonparametric two sample test like ME will likely reject given sufﬁcient sample sizes 
even if the discovered difference may not be of interest. With the SME test however  we can ask a
much more subtle question about the differences between the assumed true underlying processes.
Figures showing that the Type I error is controlled at the design level of α = 0.05 for both low and
high level features can be found in Figure F.7 in the Appendix.

5.2 Learning with Phase Features

5.2.1 Aerosol Dataset

To demonstrate the phase features invariance to SPD noise component  we use the Aerosol MISR1
dataset also studied by [24] and [25] and consider a situation with covariate shift [18] on distribution
inputs: the testing data is impaired by additive SPD components different to that in the training data.

7

Table 1: Mean Square Error (MSE) on dark
matter dataset for 500 runs with 5th and 95th
percentile.

Algorithm
Mean
PLRR
GLRR
LGRR
PGRR
GGRR

MSE
0.16

0.021 (0.018  0.024)
0.033 (0.030  0.037)

0.032 (0.028  0.036)
0.021 (0.017  0.024)
0.018 (0.015  0.019)

Figure 4: MSE with various levels of noise
added on test set  with 5th and 95th percentile.

Here  we have an aerosol optical depth (AOD) multi-instance learning problem with 800 bags  where
each bag contains 100 randomly selected multispectral (potentially cloudy) pixels within 20km radius
around an AOD sensor. The label yi for each bag is given by the AOD sensor measurements and each
sample xi is 16-dimensional. This can be understood as a distribution regression problem where each
bag is treated as a set of samples from some distribution.
We use 640 bags for training and 160 bags for testing. Here in the bags for testing only  we add
varying levels of Gaussian noise  ∼ N (0  Z) to each bag  where Z is a diagonal matrix with
diagonal components zi ∼ U [0  σvi] with vi being the empirical variance in dimension i across all
samples  accounting for different scales across dimensions. For comparisons  we consider linear
ridge regression on embeddings with respect to a Gaussian kernel  approximated with RFF (GLRR)
as described in section 2.1 (i.e. a linear kernel is applied on approximate embeddings)  linear ridge
regression on phase features (PLRR) (i.e. normalisation step is applied to obtain (3))  and also the
phase and Fourier neural networks (NN)  described in Appendix D  tuning all hyperparameters with
3-fold cross validation. With the same model  we now measure Root Mean Square Error (RMSE)
100 times with various noise-corrupted test sets and results are shown in ﬁgure 3. It is also noted that
a second level non-linear kernel ˜K does not improve performance signiﬁcantly on this problem [24].
We see that GLRR and PLRR are competitive (see Appendix Table F.2) in the noiseless case  and
these clearly outperform both the Fourier NN and Phase NN (likely due to the small size of the
dataset). For increasing noise  the performance of GLRR degrades signiﬁcantly  and while the
performance of PLRR degrades also  the model is much more robust under additional SPD noise.
In comparison  the Phase NN implementation is almost insensitive to covariate shift in the test sets 
unlike the performance of PLRR  highlighting the importance of learning discriminative frequencies
w in a very low signal-to-noise setting.
It is noted that the Fourier NN performs similarly to that of the Phase NN on this example. Interest-
ingly  discriminative frequencies learnt on the training data correspond to Fourier features that are
nearly normalised (i.e. they are close to unit norm - see Figure F.8 in the Appendix). This means
that the Fourier NN has learned to be approximately invariant based on training data  indicating that
the original Aerosol data potentially has irrelevant SPD noise components. This is reinforced by the
nature of the dataset (each bag contains 100 randomly selected potentially cloudy pixels  known to
be noisy [25]) and no loss of performance from going from GLRR to PLRR. The results highlights
that phase features are stable under additive SPD noise.

5.2.2 Dark Matter Dataset

We now study the use of phase features on the dark matter dataset  composing of a catalog of galaxy
clusters. In this setting  we would like to predict the total mass of galaxy clusters  using the dispersion
of velocities in the direction along our line of sight. In particular  we will use the ‘ML1’ dataset 
as obtained from the authors of [16  17]  who constructed a catalog of massive halos from the
MultiDark mdpl simulation [9]. The dataset contains 5028 bags  with each sample consisting of
its sub-object velocity and its mass label in R. By viewing each galaxy cluster at multiple lines of
sights  we obtain 15 000 bags  using the same experimental setup as in [10]. For experiments  we use
approximately 9000 bags for training  and 3000 bags each for validation and testing  keeping those
of multiple lines of sight in the same set. As before  we use GLRR and PLRR and we also include

8

in comparisons methods with a second level Gaussian kernel (with RFF) applied to phase features
(PGRR) and to approximate embeddings (GGRR). For a baseline  we also include a ﬁrst level linear
kernel (equivalent to representing each bag with its mean)  before applying a second level gaussian
kernel (LGRR). We use the same set of randomly sampled frequencies across the methods  tuning for
the scale of the frequencies and for regularisation parameters.
Table 1 shows the results of the methods across 10 different data splits  with 50 sets of randomised
frequencies for each data split. We see that PLRR is signiﬁcantly better than GLRR. This suggests
that under this model structure  by removing SPD components from each bag  we can target the
underlying signal and obtain superior performance  highlighting the applicability of phase features.
Considering a second level gaussian kernel  we see that the GGRR has a slight advantage over PGRR 
with PGRR performing similar to PLRR. This suggests that the SPD components of the distribution
of sub-object velocity may be useful for predicting the mass of a galaxy cluster if an additional
nonlinearity is applied to embeddings – whereas the beneﬁts of removing them outweigh the signal
present in them without this additional nonlinearity. To show that indeed the phase features are robust
to SPD components  we perform the same covariate shift experiment as in the aerosol dataset  with
results given in Figure 4. Note that LGRR is robust to noise  as each bag is represented by its mean.

6 Conclusion

No dataset is immune from measurement noise and often this noise differs across different data
generation and collection processes. When measuring distances between distributions  can we
disentangle the differences in noise from the differences in the signal? We considered two different
ways to encode invariances to additive symmetric noise in those distances  each with different
strengths: a nonparametric measure of asymmetry in paired sample differences and a weighted
distance between the empirical phase functions. The former was used to construct a hypothesis test on
whether the difference between the two generating processes can be explained away by the difference
in postulated noise  whereas the latter allowed us to introduce a ﬂexible framework for invariant
feature construction and learning algorithms on distribution inputs which are robust to measurement
noise and target underlying signal distributions.

Acknowledgements

We thank Dougal Sutherland for suggesting the use of of the dark matter dataset  Michelle Ntampaka
for providing the catalog  as well as Ricardo Silva  Hyunjik Kim and Kaspar Martens for useful
discussions. This work was supported by the EPSRC and MRC through the OxWaSP CDT
programme (EP/L016710/1). C.Y. and H.C.L.L. also acknowledge the support of the MRC Grant No.
MR/L001411/1.

The CosmoSim database used in this paper is a service by the Leibniz-Institute for Astro-
physics Potsdam (AIP). The MultiDark database was developed in cooperation with the Spanish
MultiDark Consolider Project CSD2009-00064. The authors gratefully acknowledge the Gauss
Centre for Supercomputing e.V. (www.gauss-centre.eu) and the Partnership for Advanced
Supercomputing in Europe (PRACE  www.prace-ri.eu) for funding the MultiDark simulation project
by providing computing time on the GCS Supercomputer SuperMUC at Leibniz Supercomputing
Centre (LRZ  www.lrz.de).

9

References
[1] Pierre Baldi  Peter Sadowski  and Daniel Whiteson. Searching for exotic particles in high-energy

physics with deep learning. Nature communications  5  2014.

[2] Kacper P Chwialkowski  Aaditya Ramdas  Dino Sejdinovic  and Arthur Gretton. Fast two-
sample testing with analytic representations of probability measures. In Advances in Neural
Information Processing Systems  pages 1981–1989  2015.

[3] Aurore Delaigle and Peter Hall. Methodology for non-parametric deconvolution when the
error distribution is unknown. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  78(1):231–252  2016.

[4] Paul Fearnhead and Dennis Prangle. Constructing summary statistics for approximate bayesian
computation: semi-automatic approximate bayesian computation. Journal of the Royal Statisti-
cal Society: Series B (Statistical Methodology)  74(3):419–474  2012.

[5] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

[6] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning (ICML) 
pages 448–456  2015.

[7] Wittawat Jitkrittum  Zoltán Szabó  Kacper P Chwialkowski  and Arthur Gretton. Interpretable
distribution features with maximum testing power. In Advances in Neural Information Process-
ing Systems 29  pages 181–189. 2016.

[8] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[9] Anatoly Klypin  Gustavo Yepes  Stefan Gottlober  Francisco Prada  and Steffen Hess. Mul-
tiDark simulations: the story of dark matter halo concentrations and density proﬁles. 2014.
arXiv:1411.4001.

[10] Ho Chung Leon Law  Dougal J. Sutherland  Dino Sejdinovic  and Seth Flaxman. Bayesian

approaches to distribution regression. arXiv preprint arXiv:1705.04293  2017.

[11] M. Lichman. UCI machine learning repository  2013.

[12] Yu V Linnik and IV Ostrovskii. Decomposition of random variables and vectors. 1977.

[13] J. Mitrovic  D. Sejdinovic  and Y.W. Teh. DR-ABC: Approximate Bayesian Computation
with Kernel-Based Distribution Regression. In International Conference on Machine Learning
(ICML)  pages 1482–1491  2016.

[14] Krikamol Muandet  Kenji Fukumizu  Francesco Dinuzzo  and Bernhard Schölkopf. Learning
from distributions via support measure machines. In Advances in Neural Information Processing
Systems 25  pages 10–18. 2012.

[15] Krikamol Muandet  Kenji Fukumizu  Bharath Sriperumbudur  and Bernhard Schölkopf. Kernel
mean embedding of distributions: A review and beyonds. arXiv preprint arXiv:1605.09522 
2016.

[16] Michelle Ntampaka  Hy Trac  Dougal J. Sutherland  Nicholas Battaglia  Barnabás Póczos  and
Jeff Schneider. A machine learning approach for dynamical mass measurements of galaxy
clusters. The Astrophysical Journal  803(2):50  2015. arXiv:1410.0686.

[17] Michelle Ntampaka  Hy Trac  Dougal J. Sutherland  S. Fromenteau  B. Poczos  and Jeff
Schneider. Dynamical mass measurements of contaminated galaxy clusters using machine
learning. The Astrophysical Journal  831(2):135  2016. arXiv:1509.05409.

[18] Joaquin Quinonero-Candela  Masashi Sugiyama  Anton Schwaighofer  and Neil D. Lawrence.

Dataset Shift in Machine Learning. The MIT Press  2009.

10

[19] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in Neural Information Processing Systems  pages 1177–1184  2007.

[20] H-J Rossberg. Positive deﬁnite probability densities and probability distributions. Journal of

Mathematical Sciences  76(1):2181–2197  1995.

[21] Le Song  Kenji Fukumizu  and Arthur Gretton. Kernel embeddings of conditional distributions:
A uniﬁed kernel framework for nonparametric inference in graphical models. IEEE Signal
Processing Magazine  30(4):98–111  2013.

[22] Bharath K. Sriperumbudur  Arthur Gretton  Kenji Fukumizu  Bernhard Schölkopf  and Gert R.G.
Lanckriet. Hilbert space embeddings and metrics on probability measures. J. Mach. Learn.
Res.  11:1517–1561  August 2010.

[23] Dougal J. Sutherland  Junier B. Oliva  Barnabás Póczos  and Jeff G. Schneider. Linear-time
learning on distributions with approximate kernel embeddings. In Proc. AAAI Conference on
Artiﬁcial Intelligence  pages 2073–2079  2016.

[24] Zoltán Szabó  Arthur Gretton  Barnabás Póczos  and Bharath K. Sriperumbudur. Two-stage
In Proc. International Conference on Artiﬁcial

sampled learning theory on distributions.
Intelligence and Statistics  AISTATS 2015  2015.

[25] Z. Wang  L. Lan  and S. Vucetic. Mixture model for multiple instance regression and applications
in remote sensing. IEEE Transactions on Geoscience and Remote Sensing  50(6):2226–2237 
June 2012.

[26] H. Wendland. Scattered Data Approximation. Cambridge University Press  Cambridge  UK 

2004.

11

,José Miguel Hernández-Lobato
James Lloyd
Daniel Hernández-Lobato
Cong Xie
Ling Yan
Wu-Jun Li
Zhihua Zhang
Ho Chung Law
Christopher Yau
Dino Sejdinovic
Lisa Zhang
Gregory Rosenblatt
Ethan Fetaya
Renjie Liao
William Byrd
Matthew Might
Raquel Urtasun
Richard Zemel