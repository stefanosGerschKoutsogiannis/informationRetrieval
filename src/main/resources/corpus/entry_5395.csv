2011,TD_gamma: Re-evaluating Complex Backups in Temporal Difference Learning,We show that the lambda-return target used in the TD(lambda) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. We introduce the gamma-return estimator  an alternative target based on a more accurate model of variance  which defines the TD_gamma family of complex-backup temporal difference learning algorithms. We derive TD_gamma  the gamma-return equivalent of the original TD(lambda) algorithm  which eliminates the lambda parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm  TD_gamma(C)  with a capacity parameter C. TD_gamma(C) requires C times more time and memory than TD(lambda) and is incremental and online. We show that TD_gamma outperforms TD(lambda) for any setting of lambda on 4 out of 5 benchmark domains  and that TD_gamma(C) performs as well as or better than TD_gamma for intermediate settings of C.,TDγ: Re-evaluating Complex Backups in Temporal

Difference Learning

George Konidaris∗∗†

MIT CSAIL†

Cambridge MA 02139
gdk@csail.mit.edu

Scott Niekum∗‡

Philip S. Thomas∗‡

University of Massachusetts Amherst‡

Amherst MA 01003

{sniekum pthomas}@cs.umass.edu

Abstract

We show that the λ-return target used in the TD(λ) family of algorithms is the
maximum likelihood estimator for a speciﬁc model of how the variance of an n-
step return estimate increases with n. We introduce the γ-return estimator  an
alternative target based on a more accurate model of variance  which deﬁnes the
TDγ family of complex-backup temporal difference learning algorithms. We de-
rive TDγ  the γ-return equivalent of the original TD(λ) algorithm  which elimi-
nates the λ parameter but can only perform updates at the end of an episode and
requires time and space proportional to the episode length. We then derive a sec-
ond algorithm  TDγ(C)  with a capacity parameter C. TDγ(C) requires C times
more time and memory than TD(λ) and is incremental and online. We show that
TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains 
and that TDγ(C) performs as well as or better than TDγ for intermediate settings
of C.

1

Introduction

Most reinforcement learning [1] algorithms are value-function based—learning is performed by es-
timating the expected return (discounted sum of rewards) obtained by following the current policy
from each state  and then updating the policy based on the resulting so-called value function. Efﬁ-
cient value function learning algorithms are crucial to this process and have been the focus of a great
deal of reinforcement learning research.
The most successful and widely-used family of value function algorithms is the TD(λ) family [2].
The TD(λ) family forms an estimate of return  called the λ-return  that blends both low variance 
bootstrapped and biased temporal-difference estimates of return with high variance  unbiased Monte
Carlo estimates of return using a parameter λ. While several different algorithms exist within the
TD(λ) family—the original incremental and online algorithm [2]  replacing traces [3]  least-squares
algorithms [4]  algorithms for learning state-action value functions [1  5]  and algorithms for adapt-
ing λ [6]  among others—the λ-return formulation has remained unchanged since its introduction in
1988 [2]. Our goal is to understand the modeling assumptions implicit in the λ-return formulation
and improve them.
We show that the λ-return estimator is only a maximum-likelihood estimator of return given a spe-
ciﬁc model of how the variance of an n-step return estimate increases with n. We propose a more
accurate model of that variance increase and use it to obtain an expression for a new return estimator 
the γ-return. This results in the TDγ family of algorithms  of which we derive TDγ  the γ-return
version of the original TD(λ) algorithm. TDγ is only suitable for the batch setting where updates
occur at the end of the episode and requires time and space proportional to the length of the episode 

∗All three authors are primary authors on this occasion.

1

but it eliminates the λ parameter. We then derive a second algorithm  TDγ(C)  which requires C
times more time and memory than TD(λ) and can be used in an incremental and online setting. We
show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains  and that
TDγ(C) performs as well as or better than TDγ for intermediate settings of C.

2 Complex Backups

Estimates of return lie at the heart of value-function based reinforcement learning algorithms: a
value function V π (which we denote here as V   assuming a ﬁxed policy) estimates return from each
state  and the learning process aims to reduce the error between estimated and observed returns.
Temporal difference (TD) algorithms use a return estimate obtained by taking a single transition in
the MDP and then estimating the remaining return using the value function itself:

RTD
st

= rt + γV (st+1) 

(1)

where RTD
is the return estimate from state st and rt is the reward for going from st to st+1 via
st
action at. Monte Carlo algorithms (for episodic tasks) do not use intermediate estimates but instead
use the full return sample directly:

RMC
st

=

γirt+i 

(2)

for an episode L transitions in length after time t. These two types of return estimates can be
considered instances of the more general notion of an n-step return sample  for n ≥ 1:

R(n)
st

= rt + γrt+1 + γ2rt+2 + . . . + γn−1rt+n−1 + γnV (st+n).

(3)

i=0

Here  n transitions are observed from the MDP and the remaining portion of return is estimated using
the value function. The important observation here is that all n-step return estimates can be used
simultaneously for learning. The TD(λ) family of algorithms accomplishes this using a parameter
λ ∈ [0  1] to average n-step return estimates  according to the following equation:

L−1(cid:88)

∞(cid:88)

n=0

= (1 − λ)

Rλ
st

λnR(n+1)

st

.

(4)

Note that for any episodic MDP we always obtain a ﬁnite episode length. The usual formulation of
an episodic MDP uses absorbing terminal states—states where only zero-reward self-transitions are
available. In such cases the n-step returns past the end of the episode are all equal  and therefore
TD(λ) allocates the weights corresponding to all of those return estimates to the ﬁnal transition.
st  known as the λ-return  is an estimator that blends one-step temporal difference estimates (which
Rλ
are biased  but low variance) at λ = 0 and Monte Carlo estimates (which are unbiased  but high
variance) at λ = 1. This forms the target for the entire family of TD(λ) algorithms  whose members
differ largely in their use of the resulting estimates to update the value function.
What makes this a good way to average the n-step returns? Why choose this method over any
other? Viewing this as a statistical estimation problem where each n-step return is a sample of the
true return  under what conditions and for what model is equation (4) a good estimator for return?
The most salient feature of the n-step returns is that their variances increase with n. Therefore  con-
sider the following model: each n-step return estimate R(n)
is sampled from a Gaussian distribution
st
centered on the true return  Rst 1 with variance k(n) that is some increasing function of n. If we
assume the n-step returns are independent 2 then the likelihood function for return estimate ˆRst is:

L( ˆRst|R(1)

st

  ...  R(n)
st

; k) =

N (R(n)

st

| ˆRst  k(n)).

(5)

1We should note that this assumption is not quite true: only the Monte Carlo return is unbiased.
2Again  this assumption is not true. However  it allows us to obtain a simple  closed-form estimator.

n=1

2

L(cid:89)

Maximizing the log of this equation obtains the maximum likelihood estimator for ˆRst:

(cid:80)L
(cid:80)L
n=1 k(n)−1R(n)

n=1 k(n)−1

st

ˆRst =

.

(6)

Thus  we obtain a weighted sum: each n-step return is weighted by the inverse of its variance and
the entire sum is normalized so that the weights sum to 1. From here we can see that if we let L go
to inﬁnity and set k(n) = λ−n  0 ≤ λ ≤ 1  then we obtain the λ-return estimator in equation (4) 

since(cid:80)∞

n=0 λn = 1/(1 − λ).

Thus  λ-return (as used in the TD(λ) family of algorithms) is the maximum-likelihood estimator of
return under the following assumptions:

1. The n-step returns from a given state are independent.
2. The n-step returns from a given state are normally distributed with a mean of the true return.
3. The variances of the n-step returns from each state increase according to a geometric pro-

gression in n  with common ratio λ−1.

All of these assumptions could be improved  but the third is the most interesting. In this view  the
variance of an n-step sample return increases geometrically with n and λ parametrizes the shape of
this geometric increase.

3 γ-Return and the TDγ Family of Algorithms

Consider the variance of an n-step sample return  n > 1:

(cid:104)

(cid:105)

V ar

R(n)
st

(cid:104)
(cid:104)

(cid:105)

st

R(n−1)
R(n−1)

st

(cid:104)

=V ar

=V ar

+ 2Cov

R(n−1)

st

(cid:104)

− γn−1V (st+n−1) + γn−1rt+n−1 + γnV (st+n)
+ γ2(n−1)V ar

V (st+n−1) − (rt+n−1 + γV (st+n))
 −γn−1V (st+n−1) + γn−1rt+n−1 + γnV (st+n)

(cid:105)
(cid:105)

.

Examining the last term  we see that:

(cid:104)
(cid:104)
(cid:104)
(cid:104)

 −γn−1V (st+n−1) + γn−1rt+n−1 + γnV (st+n)

(cid:105)

Cov

=Cov

=Cov

=Cov

st

st

R(n−1)
R(n−1)
R(n−1)
R(n−1)

st

st

st

− R(n−1)

(cid:105) − Cov
(cid:105) − V ar

(cid:104)
(cid:104)

  R(n)
st

  R(n)
st

  R(n)
st

(cid:105)

st

R(n−1)
R(n−1)

st

  R(n−1)

st

(cid:105)

.

(cid:105)

(cid:105)

(7)

(8)

(9)

(10)

(11)

(12)

st

Since R(n−1)
Cov[R(n−1)
st and R(n−1)
Thus  equation (12) is approximately zero. Hence  equation (8) becomes:

st are highly correlated—being two successive return samples—we assume that
are perfectly correlated).

] (equality holds when R(n)

st

st

and R(n)
st ] ≈ V ar[R(n−1)
  R(n)
(cid:105)

(cid:105) ≈ V ar

R(n−1)

(cid:104)

st

st

(cid:104)

V ar

R(n)
st

+ γ2(n−1)V ar

V (st+n−1) − (rt+n−1 + γV (st+n))

.

(13)

(cid:105)

Notice that the ﬁnal term on the right hand side of equation (13) is the discounted variance of the
temporal difference error n-steps after the current state. We assume that this variance is roughly the
same for all states; let that value be κ. Since κ also approximates the variance of the 1-step return
(i.e.  k(1) = κ)  we obtain the following simple model of the variance of an n-step sample of return:

k(n) =

γ2(i−1)κ.

(14)

(cid:104)

n(cid:88)

i=1

3

Substituting equation (14) into the general return estimator in equation (6)  we obtain:

L(cid:88)

n=1

=

w(n  L)R(n)
st

 

(15)

(16)

Rγ
st

where

i=1 γ2(i−1))−1R(n)

st

i=1 γ2(i−1))−1

n=1((cid:80)n
= κ−1(cid:80)L
κ−1(cid:80)L
n=1((cid:80)n
((cid:80)n
n=1((cid:80)n
(cid:80)L

w(n  L) =

i=1 γ2(i−1))−1

i=1 γ2(i−1))−1

is the weight associated with the nth-step return in a trajectory of length L after time t. This estimator
has the virtue of being parameter-free since the κ values cancel. Therefore  we need not estimate κ—
under the assumption of independent  Gaussian n-step returns with variances increasing according
to equation (13)  equation (15) is the maximum likelihood estimator for any value of κ. We call this
estimator the γ-return since it weights the n-step returns according to the discount factor.
Figure 1 compares the weightings obtained using λ-return and γ-return for a few example trajectory
lengths. There are two important qualitative differences. First  the λ-return weightings spike at the
end of the trajectory  whereas the γ-return weightings do not. This occurs because even though any
sample trajectory has ﬁnite length  the λ-return as deﬁned in equation (4) is actually an inﬁnite sum;
the remainder of the weight mass is allocated to the Monte Carlo return. This allows the normalizing
factor in equation (4) to be a constant  rather than having it depend on the length of the trajectory  as
it does in equation (15) for the γ-return. This signiﬁcantly complicates the problem of obtaining an
incremental algorithm using γ-return  as we shall see in later sections.

Figure 1: Example weights for trajectories of various lengths for λ-return (left) and γ-return (right).

Second  while the λ-return weightings tend to zero over time  the γ-return weightings tend toward
a constant. This means that very long trajectories will be effectively “cut-off” after some point and
have effectively no contribution to the λ-return  whereas after a certain length in the γ-return all
n-step returns have roughly equal weighting. This also complicates the problem of obtaining an
incremental algorithm using γ-return: even if we were to assume inﬁnitely many n-step returns past
the end of the episode  the normalizing constant would not become ﬁnite.
Nevertheless  we can use the γ-return estimator to obtain an entire family of TDγ learning algo-
rithms; for any TD(λ) algorithm we can derive an equivalent TDγ algorithm.
In the following
section  we derive TDγ  the γ-return equivalent of the original TD(λ) algorithm.

4 TDγ
Given a set of m trajectories T = {τ1  τ2  . . .   τm}  where lτ = |τ| denotes the number of
t+1) tuples in the trajectory τ. Using approximator ˆVθ with parameters θ to approximate
(sτ
t   rτ

t   sτ

4

05101520253000.050.10.150.20.25Return Estimate LengthWeight  Lambda=0.75  L=10Lambda=0.85  L=20Lambda=0.8  L=3005101520253000.050.10.150.20.250.30.35Return Estimate LengthWeight  L=10  Gamma=0.95L=20  Gamma=0.95L=30  Gamma=0.95L=30  Gamma=0.8(17)

(18)

(19)

(20)

(21)

(22)

(23)

(cid:105)∇θ ˆVθ(sτ

t ) 

t ).

(cid:105)∇θ ˆVθ(sτ
(cid:105)∇θ ˆVθ(sτ

t ).

(cid:104)

(cid:33)

(cid:104)

(cid:104)

V   the objective function for regression is:

(cid:88)
(cid:88)

τ∈T

lτ−1(cid:88)
lτ−1(cid:88)

t=0

(cid:16)
(cid:32)lτ−t(cid:88)

Rγ
sτ
t

τ∈T

t=0

n=1

(cid:17)2

− ˆVθ(sτ
t )

w(n  lτ − t)R(n)

sτ
t

− ˆVθ(sτ
t )

(cid:33)2

.

E(θ) =

=

1
2

1
2

Because(cid:80)lτ−t

n=1 w(n  lτ − t) = 1  we can write

(cid:88)
(cid:88)

τ∈T

lτ−1(cid:88)
lτ−1(cid:88)

t=0

(cid:32)lτ−t(cid:88)
(cid:32)lτ−t(cid:88)

n=1

τ∈T

t=0

n=1

E(θ) =

=

1
2

1
2

− lτ−t(cid:88)

n=1

(cid:104)

(cid:105)(cid:33)2

.

w(n  lτ − t)

R(n)
sτ
t

− ˆVθ(sτ
t )

w(n  lτ − t)R(n)

sτ
t

w(n  lτ − t) ˆVθ(sτ
t )

(cid:33)2

Our goal is to minimize E(θ). One approach is to descend the gradient ∇θE(θ)  assuming that the
R(n)
sτ
t

t ) and not a function of θ  as in the derivation of TD(λ) [7]:

are noisy samples of V (sτ

∆θ = −α∇θE(θ) = −α

−w(n  lτ − t)

R(n)
sτ
t

− ˆVθ(sτ
t )

where α is a learning rate. We can substitute in n = u− t (where u is the current time step  st is the
state we want to update the value estimate of  and n is the length of the n-step return that ends at the
current time step) to get:

(cid:88)

lτ−1(cid:88)

lτ−t(cid:88)

τ∈T

t=0

n=1

∆θ = −α

−w(u − t  lτ − t)

R(u−t)

sτ
t

− ˆVθ(sτ
t )

Swapping the sums allows us to derive the backward version of TDγ:

∆θ = −α

−w(u − t  lτ − t)

R(u−t)

sτ
t

− ˆVθ(sτ
t )

(cid:88)

lτ−1(cid:88)

lτ(cid:88)

τ∈T

t=0

u=t+1

(cid:88)

lτ(cid:88)

u−1(cid:88)

τ∈T

u=1

t=0

Expanding and rearranging the terms gives us an algorithm for TDγ when using linear function
approximation with weights θ:

∆θ = −α

= −α

= −α

τ∈T

(cid:88)
(cid:88)
(cid:88)

τ∈T

u=1

lτ(cid:88)
lτ(cid:88)
lτ(cid:88)

u=1

t=0

u−1(cid:88)
u−1(cid:88)
u−1(cid:88)

t=0

τ∈T

u=1

t=0

(cid:32)u−1(cid:88)

i=t

−

(cid:34)
(cid:34)

w(u − t  lτ − t)

w(u − t  lτ − t)

γi−trsτ

i

− γu−t(θ · φsτ

) + (θ · φsτ

u

t

(cid:32)u−1(cid:88)

(cid:33)(cid:35)

θ · (φsτ

t

− γu−tφsτ

u

) −

γi−trsτ

i

φsτ

t

w(u − t  lτ − t) [θ · a − b] φsτ

t

 

i=t

(cid:35)

)

φsτ

t

(24)

(25)

(26)

t is the feature vector at state sτ

. This leads to
where φsτ
TDγ for episodic tasks (given in Algorithm 1)  which eliminates the eligibility trace parameter λ. For
episode length lτ and feature vector size F   the algorithm can be implemented with time complexity
of O(lτ F ) per step and space complexity O(lτ F ). Unfortunately  implementing this backward TDγ
incrementally is problematic: lτ is not known until the end of the trajectory is reached  and without

u  and b =

t   a = φsτ

i=t

t

i

− γu−tφsτ

u−1(cid:80)

γi−trsτ

5

Algorithm 1 TDγ
Given: A discount factor  γ
1: θ ← →
0
2: for each trajectory τ ∈ T do
3:
4:
5:
6:
7:
8:

Store φ0 in memory
for u = 1 to lτ do
Store φu and ru−1 in memory
δ ← →
0
for t = 0 to u − 1 do
b ← u−1(cid:80)
a ← φt − γu−tφu

γi−tri

9:

i=t

δ ← δ + w(u − t  lτ − t)[θ · a − b]φt

end for
Discard all φ and r from memory

end for
θ ← θ − αδ

10:
11:
12:
13:
14:
15: end for

it  the normalizing constant of the weights used in the updates cannot be computed. Thus  Algorithm
1 can only be used for batch updates where each episode’s trajectory data is stored until an update
is performed at the end of an episode; this is often undesirable  and in continuing tasks  impossible.
TD(λ) is able to achieve O(F ) time and space for two reasons. First  the weight normalization is
a constant and does not depend on the length of the episode. Second  the operation that must be
performed on each trace is the same—a multiplication by λ. Thus  TD(λ) need only store the sum
of the return estimates from each state  rather than having to store each individually.
One approach to deriving an incremental algorithm is to use only the ﬁrst C n-step returns from
each state  similar to truncated temporal differences [8]. This eliminates the ﬁrst barrier: weight
normalization no longer relies on the episode length  except for the ﬁnal C − 1 states  which can
be corrected for after the episode ends. This approach has time complexity O(CF ) and space
complexity O(CF )—and is therefore C times more expensive than TD(λ)—and replaces λ with
the more intuitive parameter C rather than eliminating it  but it affords the incremental TDγ(C)
algorithm given in Algorithm 2. Note that setting C = 1 obtains TD(0)  and C ≥ lτ obtains TDγ.

5 Empirical Comparisons

Figure 2 shows empirical comparisons of TD(λ) (for various values of λ)  TDγ and TDγ(C) for 5
common benchmarks. The ﬁrst is a 10 × 10 discrete gridworld with the goal in the bottom right 
deterministic movement in the four cardinal directions  a terminal reward of +10 and −0.5 for all
other transitions  and γ = 1.0. For the gridworld  the agent selected one of the optimal actions with
probability 0.4  and each of the other actions with probability 0.2. The second domain is the 5-state
discrete chain domain [1] with random transitions to the left and right  and γ = 0.95. The third
domain is the pendulum swing-up task [7] with a reward of 1.0 for entering a terminal state (the
pendulum is nearly vertical) and zero elsewhere  and γ = 0.95. The optimal action was selected
with probability 0.5  with a random action selected otherwise. The fourth domain is mountain car
[1] with γ = 0.99  and using actions from a hand-coded policy with probability 0.75  and random
actions otherwise. The ﬁfth and ﬁnal domain is the acrobot [1] with a terminal reward of 10 and
−0.1 elsewhere. A random policy was used with γ = 0.95. In all cases the start state was selected
uniformly from the set of nonterminal states. A 5th order Fourier basis [9] was used as the function
approximator for the 3 continuous domains. We used 10  5  10  3  and 10 trajectories  respectively.
TDγ outperforms TD(λ) for all settings of λ in 4 out of the 5 domains. In the chain domain TDγ
performs better than most settings of λ but slightly worse than the optimal setting. An interesting and
somewhat unexpected result is that TDγ(C) with a relatively low setting of C does at least as well
as—or in some cases better than—TDγ. This could occur because the n-step returns become very

6

Store φ0 in memory
for u = 1 to lτ do

Algorithm 2 TDγ(C)
Given: A discount factor  γ
A cut-off length  C
1: θ ← →
0
2: for each trajectory τ ∈ T do
3:
4:
5:
6:
7:
8:
9:
10:
11:

If u > C  discard φu−C−1  θu−C−1  and ru−C−1 from memory
θu−1 ← θ
Store φu  θu−1  and ru−1 in memory
δ ← →
0
m = max(0  u − C)
for t = m to u − 1 do
b ← u−1(cid:80)
a ← φt − γu−tφu

γi−tri

12:

13:
14:
15:
16:

i=t

δ ← δ + w(u − t  C)[θ · a − b]φt

end for
θ ← θ − αδ

end for

17: m = min(lτ   C)
18:
19:
20:
21:
22:

θ ← θlτ−m
for ˆu = lτ − m to lτ do
δ ← →
0
for t = m to ˆu − 1 do
b ← ˆu−1(cid:80)
a ← φt − γ ˆu−tφˆu

γi−tri

23:

i=t

δ ← δ + w(ˆu − t  m − t)[θ · a − b]φt

end for
θ ← θ − αδ
Discard φˆu  θ ˆu−1  and r ˆu−1 from memory

24:
25:
26:
27:
28:
29: end for

end for

similar for large n due to either γ discounting diminishing the difference  or to the additional one-
step rewards accounting for a very small fraction of the total return. These near-identical estimates
will accumulate a large fraction of the weighting (see Figure 1) and come to dominate the γ-return
estimate. This suggests that once the returns start to become almost identical they should not be
considered independent samples and should instead be discarded.

6 Discussion and Future Work

An immediate goal of future work is ﬁnding an automatic way to set C. We may be able to calculate
bounds on the diminishing differences between n-step returns due to γ  or empirically track the
point at which those differences begin to diminish. Another avenue for future research is deriving a
version of TDγ or TDγ(C) that provably converges for off-policy data with function approximation 
most likely using recent insights on gradient-descent based TD algorithms [10]. Thereafter  we aim
to develop an algorithm based on γ-return for control rather than just prediction  for example Sarsaγ.
We have shown that the widely used λ-return formulation is the maximum-likelihood estimator of
return given three assumptions (see section 2). The results presented here have shown that reevaluat-
ing just one of these assumptions results in more accurate value function approximation algorithms.
We expect that re-evaluating all three will prove a fruitful avenue for future research.

7

Gridworld

Chain

Pendulum

Mountain Car

Acrobot

Figure 2: Mean squared error (MSE) over sample trajectories from ﬁve benchmark domains for
TD(λ) with various settings of λ  TDγ  and TDγ(C)  for various settings of C. Error bars are
standard error over 100 samples. Each result is the minimum MSE (weighted by state visitation
frequency) between each algorithm’s approximation and the correct value function (obtained using
a very large number of Monte Carlo samples)  found by searching over α at increments of 0.0001.

Acknowledgments

We would like to thank David Silver  Hamid Maei  Gustavo Goretkin  Sridhar Mahadevan and Andy
Barto for useful discussions. George Konidaris was supported in part by the AFOSR under grant
AOARD-104135 and the Singapore Ministry of Education under a grant to the Singapore-MIT In-
ternational Design Center. Scott Niekum was supported by the AFOSR under grant FA9550-08-1-
0418.

8

300340E220260300=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10000080.090.1E0.050.060.070.08=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10000.60.8E0.20.40.6=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(10009001000E600700800=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(100056905710E563056505670=005.15.225.335.445.555.665.775.885.99599=1γ2)5)0)0)0)0)0)0)0)MSEλ=λ=0.0λ=0.λ=0.1λ=0.λ=0.2λ=0.λ=0.3λ=0.λ=0.4λ=0.λ=0.5λ=0.λ=0.6λ=0.λ=0.7λ=0.λ=0.8λ=0.λ=0.9λ=0.9λ=γ(2γ(5γ(10γ(20γ(50γ(100γ(200γ(500γ(1000References
[1] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press  Cambridge 

MA  1998.

[2] R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning 

3(1):9–44  1988.

[3] S. Singh and R.S. Sutton. Reinforcement learning with replacing eligibility traces. Machine

Learning  22:123–158  1996.

[4] J.A. Boyan. Least squares temporal difference learning. In Proceedings of the 16th Interna-

tional Conference on Machine Learning  pages 49–56  1999.

[5] H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artiﬁcial
General Intelligence  2010.

[6] C. Downey and S. Sanner. Temporal difference Bayesian model averaging: A Bayesian per-
spective on adapting lambda. In Proceedings of the 27th International Conference on Machine
Learning  pages 311–318  2010.

[7] K. Doya. Reinforcement learning in continuous time and space. Neural Computation 

12(1):219–245  2000.

[8] P. Cichosz. Truncating temporal differences: On the efﬁcient implementation of TD(λ) for

reinforcement learning. Journal of Artiﬁcial Intelligence Research  2:287–318  1995.

[9] G.D. Konidaris  S. Osentoski  and P.S. Thomas. Value function approximation in reinforcement
learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial
Intelligence  pages 380–385  2011.

[10] R.S. Sutton  H.R. Maei  D. Precup  S. Bhatnagar  D. Silver  Cs. Szepesvari  and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approxi-
mation. In Proceedings of the 26th International Conference on Machine Learning  2009.

9

,Roger Grosse
Chris Maddison
Russ Salakhutdinov
Zhongwen Xu
Joseph Modayil
Hado van Hasselt
Andre Barreto
David Silver
Tom Schaul