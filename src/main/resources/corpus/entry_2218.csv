2007,Gaussian Process Models for Link Analysis and Transfer Learning,In this paper we develop a Gaussian process (GP) framework to model a collection of reciprocal random variables defined on the \emph{edges} of a network. We show how to construct GP priors  i.e. ~covariance functions  on the edges of directed  undirected  and bipartite graphs. The model suggests an intimate connection between \emph{link prediction} and \emph{transfer learning}  which were traditionally considered two separate research topics. Though a straightforward GP inference has a very high complexity  we develop an efficient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity.,Gaussian Process Models for

Link Analysis and Transfer Learning

Kai Yu

NEC Laboratories America

Cupertino  CA 95014

Wei Chu

Columbia University  CCLS

New York  NY 10115

Abstract

This paper aims to model relational data on edges of networks. We describe appro-
priate Gaussian Processes (GPs) for directed  undirected  and bipartite networks.
The inter-dependencies of edges can be effectively modeled by adapting the GP
hyper-parameters. The framework suggests an intimate connection between link
prediction and transfer learning  which were traditionally two separate research
topics. We develop an efﬁcient learning algorithm that can handle a large number
of observations. The experimental results on several real-world data sets verify
superior learning capacity.

1 Introduction

In many scenarios the data of interest consist of relational observations on the edges of networks.
Typically  a given ﬁnite collection of such relational data can be represented as an M × N matrix
Y = {yi j}  which is often partially observed because many elements are missing. Sometimes
accompanying Y are attributes of nodes or edges. As an important nature of networks  {yi j} are
highly inter-dependent even conditioned on known node or edge attributes. The phenomenon is
extremely common in real-world data  for example 

• Bipartite Graphs. The data represent relations between two different sets of objects or
measurements under a pair of heterogeneous conditions. One notable example is transfer
learning  also known as multi-task learning  which jointly learns multiple related but dif-
ferent predictive functions based on the M × N observed labels Y  namely  the results of
N functions acting on a set of M data examples. Collaborative ﬁltering is an important
application of transfer learning that learns many users’ interests on a large set of items.
• Undirected and Directed Graphs. The data are measurements of existences  strengths  and
types of links between a set of nodes in a graph  where a given collection of observations
are an M ×M (in this case N = M) matrix Y  which can be symmetric or asymmetric  de-
pending on whether the links are undirected or directed. Examples include protein-protein
interactions  social networks  citation networks  and hyperlinks on the WEB. Link predic-
tion aims to recover those missing measurements in Y  for example  predicting unknown
protein-protein interactions based on known interactions.

The goal of this paper is to design a Gaussian process (GP) [13] framework to model the depen-
dence structure of networks  and to contribute an efﬁcient algorithm to learn and predict large-scale
relational data. We explicitly construct a series of parametric models indexed by their dimension-
ality  and show that in the limit we obtain nonparametric GP priors consistent with the dependence
of edge-wise measurements. Since the kernel matrix is on a quadratic number of edges and the
computation cost is even cubic of the kernel size  we develop an efﬁcient algorithm to reduce the
computational complexity. We also demonstrate that transfer learning has an intimate connection to
link prediction. Our method generalizes several recent transfer learning algorithms by additionally
learning a task-speciﬁc kernel that directly expresses the dependence between tasks.

1

The application of GPs to learning on networks or graphs has been fairly recent. Most of the work
in this direction has focused on GPs over nodes of graphs and targeted at the classiﬁcation of nodes
[20  6  10]. In this paper  we regard the edges as the ﬁrst-class citizen and develop a general GP
framework for modeling the dependence of edge-wise observations on bipartite  undirected and
directed graphs. This work extends [19]  which built GPs for only bipartite graphs and proposed
an algorithm scaling cubically to the number of nodes. In contrast  the work here is more general
and the algorithm scales linearly to the number of edges. Our study promises a careful treatment to
model the nature of edge-wise observations and offers a promising tool for link prediction.

2 Gaussian Processes for Network Data

2.1 Modeling Bipartite Graphs

We ﬁrst review the edge-wise GP for bipartite graphs [19]  where each observation is a measurement
on a pair of objects of different types  or under a pair of heterogenous conditions. Formally  let U
and V be two index sets  then yi j denotes a measurement on edge (i  j) with i ∈ U and j ∈ V.
In the context of transfer learning  the pair involves a data instance i and a task j  and yi j denotes
the label of data i within task j. The probabilistic model assumes that yi j are noisy outcomes of a
real-valued function f : U ×V → R  which follows a Gaussian process GP(b  K)  characterized by
mean function b and covariance (kernel) function between edges

K ((i  j)  (i(cid:48)  j(cid:48))) = Σ(i  i(cid:48))Ω(j  j(cid:48))

(1)
where Σ and Ω are kernel functions on U and V  respectively. As a result  the realizations of f on a
ﬁnite set i = 1  . . .   M and j = 1  . . .   N form a matrix F  following a matrix-variate normal dis-
tribution NM×N (B  Σ  Ω)  or equivalently a normal distribution N (b  K) with mean b = vec(B)
and covariance K = Ω⊗Σ  where ⊗ means Kronecker product. The dependence structure of edges
is decomposed into the dependence of nodes. Since a kernel is a notion of similarity  the model ex-
presses a prior belief – if node i is similar to node i(cid:48) and node j is similar node j(cid:48)  then so are f(i  j)
and f(i(cid:48)  j(cid:48)).
It is essential to learn the kernels Σ and Ω based on the partially observed Y  in order to capture the
dependence structure of the network. For transfer learning  this means to learn the kernel Σ between
data instances and the kernel Ω between tasks. Having Σ and Ω is it then possible to predict those
missing yi j based on known observations by using GP inference.

Theorem 2.1 ([19]). Let f(i  j) = D−1/2(cid:80)D

iid∼ GP(0  Σ) and
iid∼ GP(0  Ω)  then f ∼ GP(b  K) in the limit D → ∞  and the covariance between pairs is

k=1 gk(i)hk(j) + b(i  j)  where gk

hk
K ((i  j)  (i(cid:48)  j(cid:48))) = Σ(i  i(cid:48))Ω(j  j(cid:48)).

Theorem (2.1) offers an alternative view to understand the model. The edge-wise function f can be
decomposed into a product of two sets of intermediate node-wise functions  {gk}∞
k=1 and {hk}∞
k=1 
which are i.i.d. samples from two GP priors GP(0  Σ) and GP(0  Ω). The theorem suggests that the
GP model for bipartite relational data is a generalization of a Bayesian low-rank matrix factorization
F = HG(cid:62) + B  under the prior H ∼ NM×D(0  Σ  I) and G ∼ NN×D(0  Ω  I). When D is ﬁnite 
the elements of F are not Gaussian random variables.

2.2 Modeling Directed and Undirected Graphs
In this section we model observations on pairs of nodes of the same set U. This case includes both
directed and undirected graphs. It turns out that the directed graph is relatively easy to handle while
deriving a GP prior for undirected graphs is slightly non-trivial. For the case of directed graphs  we
let the function f : U × U → R follow GP(b  K)  where the covariance function between edges is
(2)
and C : U × U → R is a kernel function between nodes. Since a random function f drawn from the
GP is generally asymmetric (even if b is symmetric)  namely f(i  j) (cid:54)= f(j  i)  the direction of edges
can be modeled. The covariance function Eq. (2) can be derived from Theorem (2.1) by setting
that {gk} and {hk} are two independent sets of functions i.i.d. sampled from the same GP prior

K ((i  j)  (i(cid:48)  j(cid:48))) = C(i  i(cid:48))C(j  j(cid:48))

2

GP(0  C)  modeling the situation that each node’s behavior as a sender is different but statistically
related to it’s behavior as a receiver. This is a reasonable modeling assumption. For example  if two
papers cite a common set of papers  their are also likely to be cited by a common set of other papers.

For the case of undirected graphs  we need to design a GP that ensures any sampled function to
be symmetric. Following the construction of GP in Theorem (2.1)  it seems that f is symmetric if
gk ≡ hk for k = 1  . . .   D. However a calculation reveals that f is not bounded in the limit D → ∞.
Theorem (2.2) shows that the problem can be solved by subtracting a growing quantity D1/2C(i  j)
as D → ∞  and suggests the covariance function

K ((i  j)  (i(cid:48)  j(cid:48))) = C(i  i(cid:48))C(j  j(cid:48)) + C(i  j(cid:48))C(j  i(cid:48)).

(3)
With such covariance function   f is ensured to be symmetric because the covariance between f(i  j)
and f(j  i) equals the variance of either.

Theorem 2.2. Let f(i  j) = D−1/2(cid:80)D

k=1 tk(i)tk(j)+b(i  j)−D1/2C(i  j)  where tk

iid∼ GP(0  C) 
then f ∼ GP(b  K) in the limit D → ∞  and the covariance between pairs is K ((i  j)  (i(cid:48)  j(cid:48))) =
C(i  i(cid:48))C(j  j(cid:48)) + C(i  j(cid:48))C(j  i(cid:48)). If b(i  j) = b(j  i)  then f(i  j) = f(j  i).
Proof. Without loss of generality  let b(i  j) ≡ 0. Based on the central limit theorem  for every (i  j) 
(cid:80)D
f(i  j) converges to a zero-mean Gaussian random variable as D → ∞  because {tk(i)tk(j)}D
k=1
is a collection of random variables independently following the same distribution  and has the mean
k=1{E[tk(i)tk(j)tk(i(cid:48))tk(j(cid:48))] −
C(i  j). The covariance function is Cov(f(i  j)  f(i(cid:48)  j(cid:48))) = 1
C(i  j)E[tk(i(cid:48))tk(j(cid:48))] − C(i(cid:48)  j(cid:48))E[tk(i)tk(j)] + C(i  j)C(i(cid:48)  j(cid:48))} = C(i  i(cid:48))C(j  j(cid:48)) +
C(i  j(cid:48))C(j  i(cid:48)) + C(i  j)C(i(cid:48)  j(cid:48)) − C(i  j)C(i(cid:48)  j(cid:48)) = C(i  i(cid:48))C(j  j(cid:48)) + C(i  j(cid:48))C(j  i(cid:48)).

D

Interestingly  Theorem (2.2) recovers Theorem (2.1) and is thus more general. To see the connection 
let hk ∼ GP(0  Σ) and gk ∼ GP(0  Ω) be concatenated to form a function tk  then we have
tk ∼ GP(0  C) and the covariance is

Σ(i  j) 

Ω(i  j) 
0 

C(i  j) =

if i  j ∈ U 
if i  j ∈ V 
if i  j are in different sets.

D(cid:88)

(4)

(5)

(6)

For i  i(cid:48) ∈ U and j  j(cid:48) ∈ V  applying Theorem (2.2) leads to

D(cid:88)

f(i  j) = D−1/2
K ((i  j)  (i(cid:48)  j(cid:48))) = C(i  i(cid:48))C(j  j(cid:48)) + C(i  j(cid:48))C(j  i(cid:48)) = Σ(i  i(cid:48))Ω(j  j(cid:48)).

tk(i)tk(j) + b(i  j) − D1/2C(i  j) = D−1/2

k=1

k=1

hk(i)gk(j) + b(i  j) 

Theorems (2.1) and (2.2) suggest a general GP framework to model directed or undirected relation-
ships connecting heterogeneous types of nodes. Basically  we learn node-wise covariance functions 
like Σ  Ω  and C  such that edge-wise covariances composed by Eq. (1)  (2)  or (3) can explain the
happening of observations yi j on edges. The proposed framework can be extended to cope with
more complex network data  for example  networks containing both undirected links and directed
links. We will brieﬂy discuss some extensions in Sec. 6.

3 An Efﬁcient Learning Algorithm

We consider the regression case under a Gaussian noise model  and later brieﬂy discuss extensions
to the classiﬁcation case. Let y = [yi j](i j)∈O be the observational vector of length |O|  f be the
corresponding quantities of the latent function f  and K be the |O|×|O| matrix of K between edges
having observations  computed by Eq. (1)-(3). Then observations on edges are generated by

(7)
iid∼ N (0  β−1)  and the mean has a parametric form bi j = µi + νj. In the
where f ∼ N (0  K)  i j
directed/undirected graph case we let µi = νi for any i ∈ U. f can be analytically marginalized out 
the marginal distribution of observations is then

yi j = f(i  j) + bi j + i j

p(y|θ) = N (y; b  K + β−1I) 

(8)

3

L(θ) =

|O|
2

where θ = {β  b  K}. The parameters can be estimated by minimizing the penalized negative log-
likelihood L(θ) = − ln p(y|θ) + (cid:96)(θ) under a suitable regularization (cid:96)(θ). The objective function
has the form:

ln|C| +

1
2

1
2

tr

log 2π +

(9)
where C = K + β−1I  m = y − b and b = [bi j]  (i  j) ∈ O. (cid:96)(θ) will be conﬁgured in Sec. 3.1.
Gradient-based optimization packages can be applied to ﬁnd a local optimum of θ. However the
computation can be prohibitively high when the size |O| of measured edges is very big  because the
memory cost is O(|O|2)  and the computational cost is O(|O|3). In our experiments |O| is about
tens of thousands or even millions. A slightly improved algorithm was introduced in [19]  with
a complexity O(M 3 + N 3) cubic to the size of nodes. The algorithm employed a non-Gaussian
approximation based on Theorem (2.1) and is applicable to only bipartite graphs.

+ (cid:96)(θ) 

(cid:163)
C−1mm(cid:62)(cid:164)

We reduce the memory and computational cost by exploring the special structure of K as discussed
in Sec. 2 and assume K to be composed by node-wise linear kernels Σ(i  i(cid:48)) = (cid:104)xi  xi(cid:48)(cid:105)  Ω(i  i(cid:48)) =
(cid:104)zj  zj(cid:48)(cid:105)  and C(i  j) = (cid:104)xi  xj(cid:105)  with x ∈ RL1 and z ∈ RL2. The edge-wise covariance is then

• Bipartite Graphs: K ((i  j)  (i(cid:48)  j(cid:48))) = (cid:104)xi ⊗ zj  xi(cid:48) ⊗ zj(cid:48)(cid:105).
• Directed Graphs: K ((i  j)  (i(cid:48)  j(cid:48))) = (cid:104)xi ⊗ xj  xi(cid:48) ⊗ xj(cid:48)(cid:105).
• Undirected Graphs: K ((i  j)  (i(cid:48)  j(cid:48))) = (cid:104)xi ⊗ xj  xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xi ⊗ xj  xj(cid:48) ⊗ xi(cid:48)(cid:105)

(cid:163)

(cid:164)

We turn the problem of optimizing K into the problem of optimizing X = [x1  . . .   xM ](cid:62) and
Z = [z1  . . .   zN ](cid:62).
It is important to note that in all the cases the kernel matrix has the form
K = UU(cid:62)  where U is an |O| × L matrix  L (cid:191) |O|  therefore applying the Woodbury identity
C−1 = β[I−U(U(cid:62)U+β−1I)−1U(cid:62)] can dramatically reduce the computational cost. For example 
in the bipartite graph case and the directed graph case  respectively there are

U(cid:62) =

xi ⊗ zj

and U(cid:62) =

xi ⊗ xj

(i j)∈O 

(10)
where the rows of U are indexed by (i  j) ∈ O. For the undirected graph case  we ﬁrst rewrite the
kernel function
K ((i  j)  (i(cid:48)  j(cid:48))) = (cid:104)xi ⊗ xj  xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xi ⊗ xj  xj(cid:48) ⊗ xi(cid:48)(cid:105)
=

(cid:104)xi ⊗ xj  xi(cid:48) ⊗ xj(cid:48)(cid:105) + (cid:104)xj ⊗ xi  xj(cid:48) ⊗ xi(cid:48)(cid:105) + (cid:104)xi ⊗ xj  xj(cid:48) ⊗ xi(cid:48)(cid:105) + (cid:104)xj ⊗ xi  xi(cid:48) ⊗ xj(cid:48)(cid:105)
(xi ⊗ xj + xj ⊗ xi)  (xi(cid:48) ⊗ xj(cid:48) + xj(cid:48) ⊗ xi(cid:48))

(cid:104)
(cid:104)(cid:173)

(i j)∈O 

(cid:174)(cid:105)

(11)

(cid:105)

=

 

(cid:164)

1
2
1
2

(cid:163)

(cid:105)

(cid:104)

and then obtain a simple form for the undirected graph case
xi ⊗ xj + xj ⊗ xi

U(cid:62) =

(12)
The overall computational cost is at O(L3 + |O|L2). Empirically we found that the algorithm is
efﬁcient to handle L = 500 when |O| is about millions. The gradients with respect to U can be
found in [12]. Further calculation of gradients with respect to X and Z can be easily derived. Here
we omit the details for saving the space. Finally  in order to predict the missing measurements  we
only need to estimate a simple linear model f(i  j) = w(cid:62)ui j + bi j.

(i j)∈O

1√
2

3.1

Incorporating Additional Attributes and Learning from Discrete Observations

There are different ways to incorporate node or edge attributes into our model. A common practice
is to let the kernel K  Σ  or Ω be some parametric function of attributes. One such choice is the RBF
function. However  node or edge attributes are typically local information while the network itself
is rather a global dependence structure  thus the network data often has a large part of patterns that
are independent of those known predictors. In the following  via the example of placing a Bayesian
prior on Σ : U×U → R  we describe a ﬂexible solution to incorporate additional knowledge. Let Σ0
Z exp(−τ E(Σ))
be the covariance that we wish Σ to be apriori close to. We apply the prior p(Σ) = 1
and use its negative log-likelihood as a regularization for Σ:
log |Σ + γ−1I| + tr

(Σ + γ−1I)−1Σ0

(cid:162)(cid:105)

(13)

(cid:104)

(cid:161)

(cid:96)(Σ) = τ E(Σ) = τ
2

4

where τ is a hyperparameter predetermined on validation data  and γ−1 is a small number to be
optimized. The energy function E(Σ) is related to the KL divergence DKL(GP(0  Σ0)||GP(0  Σ +
γ−1δ))  where δ(· ·) is the dirac kernel. If we let Σ0 be the linear kernel of attributes  normalized
by the dimensionality  then E(Σ) can be derived from a likelihood of Σ as if each dimension of the
attributes is a random sample from GP(0  Σ + γ−1δ). If the attributes are nonlinear predictors we
can conveniently set Σ0 by a nonlinear kernel. We set Σ0 = I if the corresponding attributes are
absent. (cid:96)(Ω)  (cid:96)(C) and (cid:96)(K) can be set in the same way.
The observations can be discrete variables rather than real values. In this case  an appropriate like-
lihood function can be devised accordingly. For example  the probit function could be employed
as the likelihood function for binary labels  which relates f(i  j) to the target yi j ∈ {−1  +1}  by
a cumulative normal Φ (yi j(f(i  j) + bi j)). To preserve computationally tractability  a family of
inference techniques  e.g. Laplace approximation  can be applied to ﬁnding a Gaussian distribution
that approximates the true likelihood. Then  the marginal likelihood (8) can be written as an explicit
expression and the gradient can be derived analytically as well.

4 Discussions on Related Work

Transfer Learning: As we have suggested before  the link prediction for bipartite graphs has a tight
connection to transfer learning. To make it clear  let fj(·) = f(·  j)  then the edge-wise function
f : U × V → R consists of N node-wise functions fj : U → R for j = 1  . . .   N. If we ﬁx
Ω(j  j(cid:48)) ≡ δ(j  j(cid:48))  namely a Dirac delta function  then fj are assumed to be i.i.d. GP functions
from GP(0  Σ)  where each function corresponds to one learning task. This is the hierarchical
Baysian model that assumes multiple tasks sharing the same GP prior [18]. In particular  the negative
logarithm of p

is

(cid:161){yi j} {fj}|Σ
(cid:179)
(cid:180)

(cid:162)
N(cid:88)

L

{fj}  Σ

=

(cid:88)

(cid:161)

(cid:162)

 + N

2

l

yi j  fj(i)

+

1
2

f jΣ−1f j

j=1

i∈Oj

log |Σ| 

(14)

where l(yi j  fj(i)) = − log p(yi j|fj(i)). The form is close to the recent convex multi-task learning
in a regularization framework [3]  if the log-determinant term is replaced by a trace regularization
term λtr(Σ). It was proven in [3] that if l(· ·) is convex with fj  then the minimization of (14)
is convex with jointly {fj} and Σ. The GP approach differs from the regularization approach in
two aspects: (1) fj are treated as random variables which are marginalized out  thus we only need to
estimate Σ; (2) The regularization for Σ is a non-convex log-determinant term. Interestingly  because
log |Σ| ≤ tr(Σ)−M  the trace norm is the convex envelope for the log-determinant  and thus the two
minimization problems are somehow doing similar things. However  the framework introduced in
this paper goes beyond the two methods by introducing an informative kernel Ω between tasks. From
a probabilistic modeling point of view  the independence of {fj} conditioned on Σ is a restrictive
assumption and even incorrect when some task-speciﬁc attributes are given (which means that {fj}
are not exchangeable anymore). The task-speciﬁc kernel for transfer learning has been recently
introduced in [4]  which however increased the computational complexity by a factor of N 2. One
contribution of this paper on transfer learning is an algorithm that can efﬁciently solve the learning
problem with both data kernel Σ and task kernel Ω.
Gaussian Process Latent-Variable Model (GPLVM): Our learning algorithm is also a generaliza-
tion of GPLVM. If we enforce Ω(j  j(cid:48)) = δ(j  j(cid:48)) in the model of bipartite graphs  then the evidence
Eq. (9) is equivalent to the form of GPLVM 

1
2

tr

ln|(Σ + β−1I)| +

(Σ + β−1I)−1YY(cid:62)

L(Σ  β) = M N
2

log 2π + N
2

(15)
where Y is a fully observed M × N matrix  the mean B = 0  and there is no further regularization
on Σ. GPLVM assumes that columns of Y are conditionally independent given Σ. In this paper we
consider a situation with complex dependence of edges in network graphs.
Other Related Work: Getoor et al. [7] introduced link uncertainty in the framework of probabilistic
relational models. Latent-class relational models [17  11  1] have been popular  aiming to ﬁnd
the block structure of links. Link prediction was casted as structured-output prediction in [15  2].
Statistical models based on matrix factorization was studied by [8]. Our work is similar to [8] in the

 

(cid:105)

(cid:104)

5

Figure 1: The left-hand side: the subset of the UMist Faces data that contains 10 people at 10 different views.
The blank blocks indicate the ten knocked-off images as test cases; The right-hand side: the ten knocked-off
images (the ﬁrst row) along with predictive images. The second row is of our results  the third row is of the
MMMF results  and the fourth row is of the bilinear results.

sense that relations are modeled by multiplications of node-wise factors. Very recently  Hoff showed
in [9] that the multiplicative model generalizes the latent-class models [11  1] and can encode the
transitivity of relations.

5 Numerical Experiments

We set the dimensionality of the model via validation on 10% of training data. In cases that the
additional attributes on nodes or edges are either unavailable or very weak  we compare our method
with max-margin matrix factorization (MMMF) [14] using a square loss  which is similar to singular
value decomposition (SVD) but can handle missing measurements.

5.1 A Demonstration on Face Reconstruction
A subset of the UMist Faces images of size 112 × 92 was selected to illustrate our algorithm  which
consists of 10 people at 10 different views. We manually knocked 10 images off as test cases  as
presented in Figure 1  and treated each image as a vector that leads to a 103040 × 10 matrix with
103040 missing values  where each column corresponds a view of faces. GP was trained by setting
L1 = L2 = 4 on this matrix to learn from the appearance relationships between person identity
and pose. The images recovered by GP for the test cases are presented as the second row of Figure
1-right (RMSE=0.2881). The results of MMMF are presented as the third row (RMSE=0.4351). We
also employed the bilinear models introduced by [16]  which however does not handle missing data
of a matrix  and put the results at the bottom row for comparison. Quantitatively and perceptually
our model offers a better generalization to unseen views of known persons.

5.2 Collaborative Filtering

Collaborative ﬁltering is a typical case of bipartite graphs  where ratings are measurements on edges
of user-item pairs. We carried out a serial of experiments on the whole EachMovie data  which
includes 61265 users’ 2811718 distinct numeric ratings on 1623 movies. We randomly selected
80% of each user’s ratings for training and used the remaining 20% as test cases. The random
selection was carried out 20 times independently.
For comparison purpose  we also evaluated the predictive performance of four other approaches:
1) Movie Mean: the empirical mean of ratings per movie was used as the predictive value of all
users’ rating on the movie; 2) User Mean:
the empirical mean of ratings per user was used as
the predictive value of the users’ rating on all movies; 3) Pearson Score: the Pearson correlation
coefﬁcient corresponds to a dot product between normalized rating vectors. We computed the Gram
matrices of the Pearson score with mean imputation for movies and users respectively  and took
principal components as their individual attributes. We tried 20 or 50 principal components as
attributes in this experiment and carried out least square regression on observed entries. 4) MMMF.
The optimal rank was decided by validation.

6

Table 1: Test results on the EachMovie data. The number in bracket indicates the rank we applied. The
results are averaged over 20 trials  along with the standard deviation. To evaluate accuracy  we utilize root
mean squared error (RMSE)  mean absolute error (MAE)  and normalized mean squared error  i.e.  the RMSE
normalized by the standard deviation of observations.

MAE

RMSE

NMSE

METHODS
MOVIE MEAN 1.3866±0.0013 1.1026±0.0010 0.7844±0.0012
USER MEAN 1.4251±0.0011 1.1405±0.0009 0.8285±0.0008
PEARSON(20) 1.3097±0.0012 1.0325±0.0013 0.6999±0.0011
PEARSON(50) 1.3034±0.0018 1.0277±0.0015 0.6931±0.0019
1.2245±0.0503 0.9392±0.0246 0.6127±0.0516
MMMF(3)
MMMF(15) 1.1696±0.0283 0.8918±0.0146 0.5585±0.0286
1.1557±0.0010 0.8781±0.0009 0.5449±0.0011
GP(3)

Table 2: Test results on the Cora data. The classiﬁcation accuracy rate is averaged over 5 trials  each with 4
folds for training and one fold for test.

DS

PL

HA

METHODS
CONTENT 53.70±0.50 67.50±1.70 68.30±1.60 56.40±0.70
48.90±1.70 65.80±1.40 60.70±1.10 58.20±0.70
LINK
PCA(50) 61.61±1.42 69.36±1.36 70.06±0.90 60.26±1.16
62.10±0.84 75.40±0.80 78.30±0.78 63.25±0.60
GP(50)

ML

The results of these approaches are reported in Table 1. The per-movie average yields much better
results than the per-user average  which is consistent with the ﬁndings previously reported by [5].
The improvement is noticeable by using more components of the Pearson score  but not signiﬁcant.
The generalization performance of our algorithm is better than that of others. T-test showed a signif-
icant difference with p-value 0.0387 of GP over MMMF (with 15 dimensions) in terms of RMSE.
It is well worth highlighting another attractiveness of our algorithm – the compact representation of
factors. On the EachMovie data  there are only three factors that well represent thousands of items
individually. We also trained MMMF with 3 factors as well. Although the three-factor solution GP
found is also accessible to other models  MMMF failed to achieve comparable performance on this
case (i.e.  see results of MMMF(3)). In each trial  the number of training samples is around 2.25
million. Our program took about 865 seconds to accomplish 500 L-BFGS updates on all 251572
parameters using an AMD Opteron 2.6GHz processor.

5.3 Text Categorization based on Contents and Links

We used a part of Cora corpus including 751 papers on data structure (DS)  400 papers on hardware
and architecture (HA)  1617 on machine learning (ML) and 1575 on programming language (PL).
We treated the citation network as a directed graph and modeled the link existence as binary labels.
Our model applied the probit likelihood and learned a node-wise covariance function C  L = 50 ×
50  which composes an edge-wise covariance K by Eq. (2). We set the prior covariance C0 by the
linear kernel computed by bag-of-word content attributes. Thus the learned linear features encode
both link and content information  which were then used for document classiﬁcation. We compare
several other methods that provide linear features for one-against-all categorization using SVM: 1)
CONTENT: bag-of-words features; 2) LINK: each paper’s citation list; 3) PCA: 50 components
by PCA on the concatenation of bag-of-word features and citation list for each paper. We chose
the dimensionality 50 for both GP and PCA  because their performances both saturated when the
dimensionality exceeds 50. We reported results based on 5-fold cross validation in Table 2. GP
clearly outperformed other methods in 3 out of 4 categories. The main reason we believe is that our
approach models the in-bound and out-bound behaviors simultaneously for each paper .

6 Conclusion and Extensions

In this paper we proposed GPs for modeling data living on links of networks. We described solu-
tions to handle directed and undirected links  as well as links connecting heterogenous nodes. This
work paves a way for future extensions for learning more complex relational data. For example  we
can model a network containing both directed and undirected links. Let (i  j) be directed and (i(cid:48)  j(cid:48))
√
be undirected. Based on the feature representations  Eq.(10)-right for directed links and Eq.(12)
for undirected links  the covaraince is K((i  j)  (i(cid:48)  j(cid:48))) = 1/
2[C(i  i(cid:48))C(j  j(cid:48)) + C(i  j(cid:48))C(j  i(cid:48))] 

7

which indicates that dependence between a directed link and an undirected link is penalized com-
pared to dependence between two undirected links. Moreover  GPs can be employed to model
multiple networks involving multiple different types of nodes. For each type  we use one node-wise
covariance. Letting covariance between two different types of nodes be zero  we obtain a huge
block-diagonal node-wise covariance matrix  where each block corresponds to one type of nodes.
This big covariance matrix will induce the edge-wise covariance for links connecting nodes of the
same or different types. In the near future it is promising to apply the model to various link prediction
or network completion problems.

References

[1] E. M. Airoldi  D. M. Blei  S. E. Fienberg  and E. P. Xing  Mixed membership stochastic block
models for relational data with application to protein-protein interactions. Biometrics Society
Annual Meeting  2006.

[2] S. Andrews and T. Jebara  Structured Network Learning. NIPS Workshop on Learning to

Compare Examples  2006.

[3] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learn-

ing  2007.

[4] E. V. Bonilla  F. V. Agakov  and C. K. I. Williams. Kernel multi-task learning using task-

speciﬁc features. International Conferences on Artiﬁcial Intelligence and Statistics  2007.

[5] J. Canny. Collaborative ﬁltering with privacy via factor analysis. International ACM SIGIR

Conference   2002.

[6] W. Chu  V. Sindhwani  Z. Ghahramani  and S. S. Keerthi. Relational learning with gaussian

processes. Neural Informaiton Processing Systems 19  2007.

[7] L. Getoor  E. Segal  B. Taskar  and D. Koller. Probabilistic models of text and link structure

for hypertext classiﬁcation. ICJAI Workshop  2001.

[8] P. Hoff. Multiplicative latent factor models for description and prediction of social networks.

to appear in Computational and Mathematical Organization Theory  2007.

[9] P. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data.

appear in Neural Informaiton Processing Systems 20  2007.

to

[10] A. Kapoor  Y. Qi  H. Ahn  and R. W. Picard. Hyperparameter and kernel learning for graph

based semi-supervised classiﬁcation. Neural Informaiton Processing Systems 18  2006.

[11] C. Kemp  J. B. Tenenbaum  T. L. Grifﬁths  T. Yamada  and N. Ueda. Learning systems of

concepts with an inﬁnite relational model. AAAI Conference on Artiﬁcial Intelligence  2006.

[12] N. Lawrence. Gaussian process latent variable models. Journal of Machine Learning Research 

2005.

[13] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT

Press  2006.

[14] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative

prediction. International Conference on Machine Learning  2005.

[15] B. Taskar  M. F. Wong  P. Abbeel  and D. Koller. Link prediction in relational data. Neural

Informaiton Processing Systems 16  2004.

[16] J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural

Computation  2000.

[17] Z. Xu  V. Tresp  K. Yu  and H.-P. Kriegel. Inﬁnite hidden relational models. International

Conference on Uncertainty in Artiﬁcial Intelligence  2006.

[18] K. Yu  V. Tresp  and A. Schwaighofer. Learning Gaussian processes from multiple tasks.

International Conference on Machine Learning  2005.

[19] K. Yu  W. Chu  S. Yu  V. Tresp  and Z. Xu. Stochastic relational models for discriminative link

prediction. Neural Informaiton Processing Systems 19  2007.

[20] X. Zhu  J. Lafferty  and Z. Ghahramani. Semi-supervised learning: From gaussian ﬁelds to

gaussian processes. Technical Report CMU-CS-03-175  Carnegie Mellon University  2003.

8

,Xiangyu Xu
Li Siyao
Wenxiu Sun
Qian Yin
Ming-Hsuan Yang