2015,Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets,Inference is typically intractable in high-treewidth undirected graphical models  making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set  most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set  namely a set of “fast-mixing parameters” where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC  such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics  (not just graphical models) when parameters are constrained to a fast-mixing set  gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized  to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon  disregarding logarithmic factors. When ridge-regularized  strong convexity allows a solution epsilon-accurate in parameter distance with an effort quadratic in 1/epsilon. Both of these provide of a fully-polynomial time randomized approximation scheme.,Maximum Likelihood Learning With Arbitrary

Treewidth via Fast-Mixing Parameter Sets

Justin Domke

NICTA  Australian National University
justin.domke@nicta.com.au

Abstract

Inference is typically intractable in high-treewidth undirected graphical models 
making maximum likelihood learning a challenge. One way to overcome this is to
restrict parameters to a tractable set  most typically the set of tree-structured pa-
rameters. This paper explores an alternative notion of a tractable set  namely a set
of “fast-mixing parameters” where Markov chain Monte Carlo (MCMC) inference
can be guaranteed to quickly converge to the stationary distribution. While it is
common in practice to approximate the likelihood gradient using samples obtained
from MCMC  such procedures lack theoretical guarantees. This paper proves that
for any exponential family with bounded sufﬁcient statistics  (not just graphical
models) when parameters are constrained to a fast-mixing set  gradient descent
with gradients approximated by sampling will approximate the maximum likeli-
hood solution inside the set with high-probability. When unregularized  to ﬁnd a
solution ϵ-accurate in log-likelihood requires a total amount of effort cubic in 1/ϵ 
disregarding logarithmic factors. When ridge-regularized  strong convexity allows
a solution ϵ-accurate in parameter distance with effort quadratic in 1/ϵ. Both of
these provide of a fully-polynomial time randomized approximation scheme.

1 Introduction

In undirected graphical models  maximum likelihood learning is intractable in general. For exam-
ple  Jerrum and Sinclair [1993] show that evaluation of the partition function (which can easily be
computed from the likelihood) for an Ising model is #P-complete  and that even the existence of a
fully-polynomial time randomized approximation scheme (FPRAS) for the partition function would
imply that RP = NP.

If the model is well-speciﬁed (meaning that the target distribution falls in the assumed family) then
there exist several methods that can efﬁciently recover correct parameters  among them the pseu-
dolikelihood [3]  score matching [16  22]  composite likelihoods [20  30]  Mizrahi et al.’s [2014]
method based on parallel learning in local clusters of nodes and Abbeel et al.’s [2006] method based
on matching local probabilities. While often useful  these methods have some drawbacks. First 
these methods typically have inferior sample complexity to the likelihood. Second  these all assume
a well-speciﬁed model. If the target distribution is not in the assumed class  the maximum-likelihood
solution will converge to the M-projection (minimum of the KL-divergence)  but these estimators
do not have similar guarantees. Third  even when these methods succeed  they typically yield a
distribution in which inference is still intractable  and so it may be infeasible to actually make use
of the learned distribution.

Given these issues  a natural approach is to restrict the graphical model parameters to a tractable set
Θ  in which learning and inference can be performed efﬁciently. The gradient of the likelihood is
determined by the marginal distributions  whose difﬁculty is typically determined by the treewidth of
the graph. Thus  probably the most natural tractable family is the set of tree-structured distributions 

1

where Θ = {θ : ∃tree T ∀(i  j) ̸∈ T  θij = 0}. The Chow-Liu algorithm [1968] provides an
efﬁcient method for ﬁnding the maximum likelihood parameter vector θ in this set  by computing
the mutual information of all empirical pairwise marginals  and ﬁnding the maximum spanning tree.
Similarly  Heinemann and Globerson [2014] give a method to efﬁciently learn high-girth models
where correlation decay limits the error of approximate inference  though this will not converge to
the M-projection when the model is mis-speciﬁed.

This paper considers a fundamentally different notion of tractability  namely a guarantee that Markov
chain Monte Carlo (MCMC) sampling will quickly converge to the stationary distribution. Our
fundamental result is that if Θ is such a set  and one can project onto Θ  then there exists a FPRAS
for the maximum likelihood solution inside Θ. While inspired by graphical models  this result works
entirely in the exponential family framework  and applies generally to any exponential family with
bounded sufﬁcient statistics.

The existence of a FPRAS is established by analyzing a common existing strategy for maximum
likelihood learning of exponential families  namely gradient descent where MCMC is used to gen-
erate samples and approximate the gradient. It is natural to conjecture that  if the Markov chain is
fast mixing  is run long enough  and enough gradient descent iterations are used  this will converge
to nearly the optimum of the likelihood inside Θ  with high probability. This paper shows that this is
indeed the case. A separate analysis is used for the ridge-regularized case (using strong convexity)
and the unregularized case (which is merely convex).

2 Setup

Though notation is introduced when ﬁrst used  the most important symbols are given here for more
reference.

• θ - parameter vector to be learned
• Mθ - Markov chain operator corresponding to θ
• θk - estimated parameter vector at k-th gradient descent iteration
• qk = Mv

θk−1r - approximate distribution sampled from at iteration k. (v iterations of the

Markov chain corresponding to θk−1 from arbitrary starting distribution r.)

• Θ - constraint set for θ
• f - negative log-likelihood on training data
• L - Lipschitz constant for the gradient of f .
• θ∗ = arg minθ∈Θ f (θ) - minimizer of likelihood inside of Θ
• K - total number of gradient descent steps
• M - total number of samples drawn via MCMC
• N - length of vector x.
• v - number of Markov chain transitions applied for each sample
• C  α - parameters determining the mixing rate of the Markov chain. (Equation 3)
• Ra - sufﬁcient statistics norm bound.
• ϵf - desired optimization accuracy for f
• ϵθ - desired optimization accuracy for θ
• δ - permitted probability of failure to achieve a given approximation accuracy

This paper is concerned with an exponential family of the form
pθ(x) = exp(θ · t(x) − A(θ)) 

where t(x) is a vector of sufﬁcient statistics  and the log-partition function A(θ) ensures normal-
ization. An undirected model can be seen as an exponential family where t consists of indicator
functions for each possible conﬁguration of each clique [32]. While such graphical models motivate
this work  the results are most naturally stated in terms of an exponential family and apply more
generally.

2

• Initialize θ0 = 0.
• For k = 1  2  ...  K

– Draw samples. For i = 1  ...  M   sample

xk−1
i ∼ qk−1 := Mv

θk−1r.

– Estimate the gradient as

f′(θk−1) + ek ←

1
M

M

– Update the parameter vector as

i

t(xk−1

) − ¯t + λθ.

!i=1
(f′(θk−1) + ek))# .

1
L

θk ← ΠΘ"θk−1 −

f (θ)

θ ∗

θ 0

Θ

k=1 θk.

K $K

• Output θK or 1
Figure 1: Left: Algorithm 1  approximate gradient descent with gradients approximated via
MCMC  analyzed in this paper. Right: A cartoon of the desired performance  stochastically ﬁnding
a solution near θ∗  the minimum of the regularized negative log-likelihood f (θ) in the set Θ.

We are interested in performing maximum-likelihood learning  i.e. minimizing  for a dataset
z1  ...  zD 

log pθ(zi) +

λ
2∥θ∥2

2 = A(θ) − θ · ¯t +

λ
2∥θ∥2
2 

(1)

where we deﬁne ¯t = 1

i=1 t(zi). It is easy to see that the gradient of f takes the form

1
D

D

!i=1

f (θ) = −
D $D

f′(θ) = Epθ [t(X)] − ¯t + λθ.

If one would like to optimize f using a gradient-based method  computing the expectation of t(X)
with respect to pθ can present a computational challenge. With discrete graphical models  the ex-
pected value of t is determined by the marginal distributions of each factor in the graph. Typ-
ically  the computational difﬁculty of computing these marginal distributions is determined by the
treewidth of the graph– if the graph is a tree  (or close to a tree) the marginals can be computed by the
junction-tree algorithm [18]. One option  with high treewidth  is to approximate the marginals with
a variational method. This can be seen as exactly optimizing a “surrogate likelihood” approximation
of Eq. 1 [31].

i=1 from a distribution close to pθ  and then approximate Epθ [t(X)] by (1/M )$M

Another common approach is to use Markov chain Monte Carlo (MCMC) to compute a sample
{xi}M
i=1 t(xi).
This strategy is widely used  varying in the model type  the sampling algorithm  how samples are
initialized  the details of optimization  and so on [10  25  27  24  7  33  11  2  29  5]. Recently 
Steinhardt and Liang [28] proposed learning in terms of the stationary distribution obtained from a
chain with a nonzero restart probability  which is fast-mixing by design.

While popular  such strategies generally lack theoretical guarantees. If one were able to exactly
sample from pθ  this could be understood simply as stochastic gradient descent. But  with MCMC 
one can only sample from a distribution approximating pθ  meaning the gradient estimate is not
only noisy  but also biased. In general  one can ask how should the step size  number of iterations 
number of samples  and number of Markov chain transitions be set to achieve a convergence level.

The gradient descent strategy analyzed in this paper  in which one updates a parameter vector θk
using approximate gradients is outlined and shown as a cartoon in Figure 1. Here  and in the rest
of the paper  we use pk as a shorthand for pθk   and we let ek denote the difference between the
estimated gradient and the true gradient f′(θk−1). The projection operator is deﬁned by ΠΘ[φ] =
arg minθ∈Θ ||θ − φ||2.
We assume that the parameter set θ is constrained to a set Θ such that MCMC is guaranteed to mix
at a certain rate (Section 3.1). With convexity  this assumption can bound the mean and variance

3

of the errors at each iteration  leading to a bound on the sum of errors. With strong convexity  the
error of the gradient at each iteration is bounded with high probability. Then  using results due to
[26] for projected gradient descent with errors in the gradient  we show a schedule the number of
iterations K  the number of samples M   and the number of Markov transitions v such that with high
probability 

f! 1

K

θk# − f (θ∗) ≤ ϵf or ∥θK − θ∗∥2 ≤ ϵθ 

K

"k=1

for the convex or strongly convex cases  respectively  where θ∗ ∈ arg minθ∈Θ f (θ). The total num-
ber of Markov transitions applied through the entire algorithm  KM v grows as (1/ϵf )3 log(1/ϵf )
for the convex case  (1/ϵ2
θ) for the strongly convex case  and polynomially in all other
parameters of the problem.

θ) log(1/ϵ2

3 Background

3.1 Mixing times and Fast-Mixing Parameter Sets

2$x |p(x) − q(x)|.

This Section discusses some background on mixing times for MCMC. Typically  mixing times are
deﬁned in terms of the total-variation distance ∥p− q∥T V = maxA |p(A)− q(A)|  where the max-
imum ranges over the sample space. For discrete distributions  this can be shown to be equivalent to
∥p − q∥T V = 1
We assume that a sampling algorithm is known  a single iteration of which can be thought of an
operator Mθ that transforms some starting distribution into another. The stationary distribution is
pθ  i.e. limv→∞ Mv
θq = pθ for all q. Informally  a Markov chain will be fast mixing if the total
variation distance between the starting distribution and the stationary distribution decays rapidly in
the length of the chain. This paper assumes that a convex set Θ and constants C and α are known
such that for all θ ∈ Θ and all distributions q 

∥Mv

θq − pθ∥T V ≤ Cαv.

(2)

This means that the distance between an arbitrary starting distribution q and the stationary distri-
bution pθ decays geometrically in terms of the number of Markov iterations v. This assumption is
justiﬁed by the Convergence Theorem [19  Theorem 4.9]  which states that if M is irreducible and
aperiodic with stationary distribution p  then there exists constants α ∈ (0  1) and C > 0 such that
(3)

d(v) := sup

q ∥Mvq − p∥T V ≤ Cαv.

Many results on mixing times in the literature  however  are stated in a less direct form. Given a
constant ϵ  the mixing time is deﬁned by τ(ϵ) = min{v : d(v) ≤ ϵ}. It often happens that bounds
on mixing times are stated as something like τ(ϵ) ≤ %a + b ln 1
ϵ& for some constants a and b. It
follows from this that ∥Mvq − p∥T V ≤ Cαv with C = exp(a/b) and α = exp(−1/b).
A simple example of a fast-mixing exponential family is the Ising model  deﬁned for x ∈
{−1  +1}N as

θixi − A(θ)⎞
⎠ .
A simple result for this model is that  if the maximum degree of any node is ∆ and |θij| ≤ β for
all (i  j)  then for univariate Gibbs sampling with random updates  τ(ϵ) ≤ ⌈ N log(N/ϵ)
1−∆ tanh(β)⌉ [19]. The
algorithm discussed in this paper needs the ability to project some parameter vector φ onto Θ to ﬁnd
arg minθ∈Θ ||θ−φ||2. Projecting a set of arbitrary parameters onto this set of fast-mixing parameters
is trivial– simply set θij = β for θij > β and θij ← −β for θij < −β.
For more dense graphs  it is known [12  9] that  for a matrix norm ∥·∥ that is the spectral norm ∥·∥2 
or induced 1 or inﬁnity norms 

p(x|θ) = exp⎛

θijxixj +"i

⎝ "(i j)∈Pairs

1 − ∥R(θ)∥ 
τ(ϵ) ≤+ N log(N/ϵ)

4

(4)

where Rij(θ) = |θij|. Domke and Liu [2013] show how to perform this projection for the Ising
model when ∥ · ∥ is the spectral norm ∥ · ∥2 with a convex optimization utilizing the singular value
decomposition in each iteration.

Loosely speaking  the above result shows that univariate Gibbs sampling on the Ising model is fast-
mixing  as long as the interaction strengths are not too strong. Conversely  Jerrum and Sinclair
[1993] exhibited an alternative Markov chain for the Ising model that is rapidly mixing for arbitrary
interaction strengths  provided the model is ferromagnetic  i.e.
that all interaction strengths are
positive with θij ≥ 0 and that the ﬁeld is unidirectional. This Markov chain is based on sampling
in different “subgraphs world” state-space. Nevertheless  it can be used to estimate derivatives of
the Ising model log-partition function with respect to parameters  which allows estimation of the
gradient of the log-likelihood. Huber [2012] provided a simulation reduction to obtain an Ising
model sample from a subgraphs world sample.

More generally  Liu and Domke [2014] consider a pairwise Markov random ﬁeld  deﬁned as

p(x|θ) = exp⎛
⎝#i j

θij(xi  xj) +#i

θi(xi) − A(θ)⎞
⎠  

1

2|θij(a  b)−θij(a  c)|  then again Equation 4 holds.

and show that  if one deﬁnes Rij(θ) = maxa b c
An algorithm for projecting onto the set Θ = {θ : ∥R(θ)∥ ≤ c} exists.
There are many other mixing-time bounds for different algorithms  and different types of models
[19]. The most common algorithms are univariate Gibbs sampling (often called Glauber dynamics
in the mixing time literature) and Swendsen-Wang sampling. The Ising model and Potts models are
the most common distributions studied  either with a grid or fully-connected graph structure. Often 
the motivation for studying these systems is to understand physical systems  or to mathematically
characterize phase-transitions in mixing time that occur as interactions strengths vary. As such 
many existing bounds assume uniform interaction strengths. For all these reasons  these bounds
typically require some adaptation for a learning setting.

4 Main Results

4.1 Lipschitz Gradient

For lack of space  detailed proofs are postponed to the appendix. However  informal proof sketches
are provided to give some intuition for results that have longer proofs. Our ﬁrst main result is that
the regularized log-likelihood has a Lipschitz gradient.
Theorem 1. The regularized log-likelihood gradient is L-Lipschitz with L = 4R2

2 + λ  i.e.

∥f′(θ) − f′(φ)∥2 ≤ (4R2

2 + λ)∥θ − φ∥2.

Proof sketch. It is easy  by the triangle inequality  that ∥f′(θ)−f′(φ)∥2 ≤ ∥ dA
Next  using the assumption that ∥t(x)∥2 ≤ R2  one can bound that ∥ dA
Finally  some effort can bound that ∥pθ − pφ∥T V ≤ 2R2∥θ − φ∥2.
4.2 Convex convergence

dθ − dA

dθ − dA

dφ∥2 +λ∥θ−φ∥2.
dφ ∥2 ≤ 2R2∥pθ−pφ∥T V .

Now  our ﬁrst major result is a guarantee on the convergence that is true both in the regularized case
where λ > 0 and the unregularized case where λ = 0.
Theorem 2. With probability at least 1 − δ  at long as M ≥ 3K/ log( 1
K
√M

δ )  Algorithm 1 will satisfy

θk’ − f (θ∗) ≤

KL ( L∥θ0 − θ∗∥2

+ KCαv)2

+ log

f& 1

K

8R2
2

1
δ

+

4R2

.

K

#k=1

Proof sketch. First  note that f is convex  since the Hessian of f is the covariance of t(X) when
λ = 0 and λ > 0 only adds a quadratic. Now  deﬁne the quantity dk = 1
m) −

m=1 t(X k

M *M

5

Eqk [t(X)] to be the difference between the estimated expected value of t(X) under qk and the
true value. An elementary argument can bound the expected value of ∥dk∥  while the Efron-Stein
inequality can bounds its variance. Using both of these bounds in Bernstein’s inequality can then
show that  with probability 1 − δ  !K
δ ). Finally  we can observe
k=1 ∥ek∥ ≤!K
that!K
k=1 ∥dk∥ +!K
[t(X)]∥2. By the assumption on mixing
speed  the last term is bounded by 2KR2Cαv. And so  with probability 1 − δ  !K
k=1 ∥ek∥ ≤
2R2(K/√M + log 1
δ ) + 2KR2Cαv. Finally  a result due to Schmidt et al. [26] on the convergence

k=1 ∥dk∥ ≤ 2R2(K/√M + log 1
k=1 ∥Eqk[t(X)]− Epθk

of gradient descent with errors in estimated gradients gives the result.

Intuitively  this result has the right character. If M grows on the order of K 2 and v grows on the
order of log K/(− log α)  then all terms inside the quadratic will be held constant  and so if we set
K of the order 1/ϵ  the sub-optimality will on the order of ϵ with a total computational effort roughly
on the order of (1/ϵ)3 log(1/ϵ). The following results pursue this more carefully. Firstly  one can
observe that a minimum amount of work must be performed.
Theorem 3. For a  b  c  α > 0  if K  M  v > 0 are set so that 1

K (a + b K√M

+ Kcαv)2 ≤ ϵ  then

KM v ≥

a4b2
ϵ3

log ac
ϵ
(− log α)

.

Since it must be true that a/√K + b"K/M + √Kcαv ≤ √ϵ  each of these three terms must also

be at most √ϵ  giving lower-bounds on K  M   and v. Multiplying these gives the result.

Next  an explicit schedule for K  M   and v is possible  in terms of a convex set of parameters
β1  β2  β3. Comparing this to the lower-bound above shows that this is not too far from optimal.
Theorem 4. Suppose that a  b  c  α > 0. If β1 + β2 + β3 = 1  β1  β2  β3 > 0  then setting
K = a2
1 ϵ   M = ( ab
+
β 2
Kcαv)2 ≤ ϵ with a total work of

β1β3ϵ /(− log α) is sufﬁcient to guarantee that 1

β1β2ϵ )2  v = log ac

K (a + b K√M

KM v =

1
β4
1 β2
2

a4b2
ϵ3

β1β3ϵ

log ac
(− log α)

.

Simply verify that the ϵ bound holds  and multiply the terms together.
For example  setting β1 = 0.66  β2 = 0.33 and β3 = 0.01 gives that KM v ≈ 48.4 a4b2
.
Finally  we can give an explicit schedule for K  M   and v  and bound the total amount of work that
needs to be performed.

log ac
ϵ +5.03
(− log α)

ϵ3

Theorem 5. If D ≥ max#∥θ0 − θ∗∥2  4R2

that f ( 1

k=1 θk) − f (θ∗) ≤ ϵf with probability 1 − δ and
log

2D4

K !K

L log 1

KM v ≤

32LR2
2 ϵ3
β4
1 β2

f (1 − α)

4DR2C
β1β3ϵf

.

δ$  then for all ϵ there is a setting of K  M  v such

[Proof sketch] This follows from setting K  M   and v as in Theorem 4 with a = L∥θ0 −
θ∗∥2/(4R2) + log 1

δ   b = 1  c = C  and ϵ = ϵf L/(8R2
2).

4.3 Strongly Convex Convergence

This section gives the main result for convergence that is true only in the regularized case where
λ > 0. Again  the main difﬁculty in this proof is showing that the sum of the errors of estimated
gradients at each iteration is small. This is done by using a concentration inequality to show that the
error of each estimated gradient is small  and then applying a union bound to show that the sum is
small. The main result is as follows.
Theorem 6. When the regularization constant obeys λ > 0  with probability at least 1−δ Algorithm
1 will satisfy

∥θK − θ∗∥2 ≤ (1 −

λ
L

)K∥θ0 − θ∗∥2 +

L

λ %& R2

2M %1 +&2 log

K

δ ’ + 2R2Cαv’ .

6

i=1 t(xk

i ) − Eqk [t(X)]∥2 + ∥Eqk [t(X)] − Epθk

Proof sketch. When λ = 0  f is convex (as in Theorem 2) and so is strongly convex when
λ > 0. The basic proof technique here is to decompose the error in a particular step as ∥ek+1∥2 ≤
M !M
∥ 1
[t(X)]∥2. A multidimensional variant of Ho-
δ )/√M  
effding’s inequality can bound the ﬁrst term  with probability 1 − δ′ by R2(1 +"2 log 1
while our assumption on mixing speed can bound the second term by 2R2Cαv. Applying this to
all iterations using δ′ = δ/K gives that all errors are simultaneously bounded as before. This can
then be used in another result due to Schmidt et al. [26] on the convergence of gradient descent with
errors in estimated gradients in the strongly convex case.

A similar proof strategy could be used for the convex case where  rather than directly bounding the
sum of the norm of errors of all steps using the Efron-Stein inequality and Bernstein’s bound  one
could simply bound the error of each step using a multidimensional Hoeffding-type inequality  and
then apply this with probability δ/K to each step. This yields a slightly weaker result than that
shown in Theorem 2. The reason for applying a uniform bound on the errors in gradients here is
that Schmidt et al.’s bound [26] on the convergence of proximal gradient descent on strongly convex
functions depends not just on the sum of the norms of gradient errors  but a non-uniform weighted
variant of these.
Again  we consider how to set parameters to guarantee that θK is not too far from θ∗ with a minimum
amount of work. Firstly  we show a lower-bound.

Theorem 7. Suppose a  b  c > 0. Then for any K  M  v such that γKa+ b√M#log(K/δ)+cαv ≤ ϵ.

it must be the case that

log a

ϵ log c
ϵ

KM v ≥

b2
ϵ2

(− log γ)(− log α)
[Proof sketch] This is established by noticing that γKa 
than ϵ  giving lower bounds on K  M   and v.

ϵ

log$ log a
δ(− log γ)% .
b√M"log K

δ   and cαv must each be less

ϵ2β 2

b2
ϵ2β2
2

KM V ≤

Next  we can give an explicit schedule that is not too far off from this lower-bound.
Theorem 8. Suppose that a  b  c  α > 0. If β1 + β2 + β3 = 1  βi > 0  then setting K =
β1ϵ )/(− log γ)  M = b2
log( a
to guarantee that γKa + b√M

2 &1 +#2 log(K/δ)’2
(1 +#2 log(K/δ)) + cαv ≤ ϵ with a total work of at most
β1ϵ’ log& c
β3ϵ’
log& a
(− log γ)(− log α) ⎛

β3ϵ’ /(− log α) is sufﬁcient
δ(− log γ)⎞
⎠

and v = log& c
⎝1 +*2 log

For example  if you choose β2 = 1/√2 and β1 = β3 = (1 − 1/√2)/2 ≈ 0.1464  then this varies
from the lower-bound in Theorem 7 by a factor of two  and a multiplicative factor of 1/β3 ≈ 6.84
inside the logarithmic terms.
Corollary 9. If we choose K ≥ L
  and v ≥
1−α log (2LR2C/(β3ϵλ))  then ∥θK − θ∗∥2 ≤ ϵθ with probability at least 1 − δ  and the total

λ log&∥θ0−θ∥2

β1ϵ ’   M ≥ L2R2
β1ϵθ %-1 +*2 log$ L

2 λ2 &1 +#2 log(K/δ)’2
β1ϵθ %%.
log$∥θ0 − θ∥2

log$∥θ0 − θ∥2

amount of work is bounded by

L3R2
2 λ3(1 − α)

KM v ≤

log( a

β1ϵ )

2ϵ2

θβ2

2

.

2

.

2ϵ2β 2

λδ

1

5 Discussion

An important detail in the previous results is that the convex analysis gives convergence in terms of
the regularized log-likelihood  while the strongly-convex analysis gives convergence in terms of the
parameter distance. If we drop logarithmic factors  the amount of work necessary for ϵf - optimality
in the log-likelihood using the convex algorithm is of the order 1/ϵ3
f   while the amount of work
necessary for ϵθ - optimality using the strongly convex analysis is of the order 1/ϵ2
θ. Though these
quantities are not directly comparable  the standard bounds on sub-optimality for λ-strongly convex
functions with L-Lipschitz gradients are that λϵ2
θ/2. Thus  roughly speaking  when
regularized for the strongly-convex analysis shows that ϵf optimality in the log-likelihood can be
achieved with an amount of work only linear in 1/ϵf .

θ/2 ≤ ϵf ≤ Lϵ2

7

100

*)
f(θk) − f(θ

|| θk − θ

* ||2

100

10−1

0.4

0.2

0

−0.2

10−5
0

20
40
iterations k

10−2
0

60

20
40
iterations k

−0.4
100

60

101

iterations k

102

Figure 2: Ising Model Example. Left: The difference of the current test log-likelihood from the
optimal log-likelihood on 5 random runs. Center: The distance of the current estimated parameters
from the optimal parameters on 5 random runs. Right: The current estimated parameters on one run 
as compared to the optimal parameters (far right).

6 Example

While this paper claims no signiﬁcant practical contribution  it is useful to visualize an example.

Take an Ising model p(x) ∝ exp(!(i j)∈Pairs θijxixj) for xi ∈ {−1  1} on a 4 × 4 grid with 5
random vectors as training data. The sufﬁcient statistics are t(x) = {xixj|(i  j) ∈ Pairs}  and with
24 pairs  ∥t(x)∥2 ≤ R2 = √24. For a fast-mixing set  constrain |θij| ≤ .2 for all pairs. Since
the maximum degree is 4  τ(ϵ) ≤ ⌈ N log(N/ϵ)
1−4 tanh(.2)⌉ . Fix λ = 1  ϵθ = 2 and δ = 0.1. Though the
theory above suggests the Lipschitz constant L = 4R2
2 + λ = 97  a lower value of L = 10 is used 
which converged faster in practice (with exact or approximate gradients). Now  one can derive that

∥θ0 − θ∗∥2 ≤ D ="24 × (2 × .2)2  C = log(16) and α = exp(−(1 − 4 tanh .2)/16). Applying

Corollary 9 with β1 = .01  β2 = .9 and β3 = .1 gives K = 46  M = 1533 and v = 561. Fig. 2
shows the results. In practice  the algorithm ﬁnds a solution tighter than the speciﬁed ϵθ  indicating
a degree of conservatism in the theoretical bound.

7 Conclusions
This section discusses some weaknesses of the above analysis  and possible directions for future
work. Analyzing complexity in terms of the total sampling effort ignores the complexity of projec-
tion itself. Since projection only needs to be done K times  this time will often be very small in
comparison to sampling time. (This is certainly true in the above example.) However  this might not
be the case if the projection algorithm scales super-linearly in the size of the model.

Another issue to consider is how the samples are initialized. As far as the proof of correctness
goes  the initial distribution r is arbitrary. In the above example  a simple uniform distribution was
used. However  one might use the empirical distribution of the training data  which is equivalent to
contrastive divergence [5]. It is reasonable to think that this will tend to reduce the mixing time when
the pθ is close to the model generating the data. However  the number of Markov chain transitions
v prescribed above is larger than typically used with contrastive divergence  and Algorithm 1 does
not reduce the step size over time. While it is common to regularize to encourage fast mixing
with contrastive divergence [14  Section 10]  this is typically done with simple heuristic penalties.
Further  contrastive divergence is often used with hidden variables. Still  this provides a bound for
how closely a variant of contrastive divergence could approximate the maximum likelihood solution.

The above analysis does not encompass the common strategy for maximum likelihood learning
where one maintains a “pool” of samples between iterations  and initializes one Markov chain at
each iteration from each element of the pool. The idea is that if the samples at the previous iteration
were close to pk−1 and pk−1 is close to pk  then this provides an initialization close to the current
solution. However  the proof technique used here is based on the assumption that the samples xk
i at
each iteration are independent  and so cannot be applied to this strategy.

Acknowledgements
Thanks to Ivona Bezáková  Aaron Defazio  Nishant Mehta  Aditya Menon  Cheng Soon Ong and
Christfried Webers. NICTA is funded by the Australian Government through the Dept. of Commu-
nications and the Australian Research Council through the ICT Centre of Excellence Program.

8

References

[1] Abbeel  P.  Koller  D.  and Ng  A. Learning factor graphs in polynomial time and sample complexity.

Journal of Machine Learning Research  7:1743–1788  2006.

[2] Asuncion  A.  Liu  Q.  Ihler  A.  and Smyth  P. Learning with blocks composite likelihood and contrastive

divergence. In AISTATS  2010.

[3] Besag  J. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society. Series D (The

Statistician)  24(3):179–195  1975.

[4] Boucheron  S.  Lugosi  G.  and Massart  P. Concentration Inequalities: A Nonasymptotic Theory of

Independence. Oxford University Press  2013.

[5] Carreira-Peripiñán  M. A. and Hinton  G. On contrastive divergence learning. In AISTATS  2005.
[6] Chow  C. I. and Liu  C. N. Approximating discrete probability distributions with dependence trees. IEEE

Transactions on Information Theory  14:462–467  1968.

[7] Descombes  X.  Robin Morris  J. Z.  and Berthod  M. Estimation of markov Random ﬁeld prior parame-
ters using Markov chain Monte Carlo maximum likelihood. IEEE Transactions on Image Processing  8
(7):954–963  1996.

[8] Domke  J. and Liu  X. Projecting Ising model parameters for fast mixing. In NIPS  2013.
[9] Dyer  M. E.  Goldberg  L. A.  and Jerrum  M. Matrix norms and rapid mixing for spin systems. Ann.

Appl. Probab.  19:71–107  2009.

[10] Geyer  C. Markov chain Monte Carlo maximum likelihood. In Symposium on the Interface  1991.
[11] Gu  M. G. and Zhu  H.-T. Maximum likelihood estimation for spatial models by Markov chain Monte
Carlo stochastic approximation. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy)  63(2):339–355  2001.

[12] Hayes  T. A simple condition implying rapid mixing of single-site dynamics on spin systems. In FOCS 

2006.

[13] Heinemann  U. and Globerson  A. Inferning with high girth graphical models. In ICML  2014.
[14] Hinton  G. A practical guide to training restricted boltzmann machines. Technical report  University of

Toronto  2010.

[15] Huber  M. Simulation reductions for the ising model. Journal of Statistical Theory and Practice  5(3):

413–424  2012.

[16] Hyvärinen  A. Estimation of non-normalized statistical models by score matching. Journal of Machine

Learning Research  6:695–709  2005.

[17] Jerrum  M. and Sinclair  A. Polynomial-time approximation algorithms for the ising model. SIAM Journal

on Computing  22:1087–1116  1993.

[18] Koller  D. and Friedman  N. Probabilistic Graphical Models: Principles and Techniques. MIT Press 

2009.

[19] Levin  D. A.  Peres  Y.  and Wilmer  E. L. Markov chains and mixing times. American Mathematical

Society  2006.

[20] Lindsay  B. Composite likelihood methods. Contemporary Mathematics  80(1):221–239  1988.
[21] Liu  X. and Domke  J. Projecting Markov random ﬁeld parameters for fast mixing. In NIPS  2014.
[22] Marlin  B. and de Freitas  N. Asymptotic efﬁciency of deterministic estimators for discrete energy-based

models: Ratio matching and pseudolikelihood. In UAI  2011.

[23] Mizrahi  Y.  Denil  M.  and de Freitas  N. Linear and parallel learning of markov random ﬁelds. In ICML 

2014.

[24] Papandreou  G. and Yuille  A. L. Perturb-and-map random ﬁelds: Using discrete optimization to learn

and sample from energy models. In ICCV  2011.

[25] Salakhutdinov  R. Learning in Markov random ﬁelds using tempered transitions. In NIPS  2009.
[26] Schmidt  M.  Roux  N. L.  and Bach  F. Convergence rates of inexact proximal-gradient methods for

convex optimization. In NIPS  2011.

[27] Schmidt  U.  Gao  Q.  and Roth  S. A generative perspective on MRFs in low-level vision. In CVPR 

2010.

[28] Steinhardt  J. and Liang  P. Learning fast-mixing models for structured prediction. In ICML  2015.
[29] Tieleman  T. Training restricted Boltzmann machines using approximations to the likelihood gradient. In

ICML  2008.

[30] Varin  C.  Reid  N.  and Firth  D. An overview of composite likelihood methods. Statistica Sinica  21:

5–24  2011.

[31] Wainwright  M. Estimating the "wrong" graphical model: Beneﬁts in the computation-limited setting.

Journal of Machine Learning Research  7:1829–1859  2006.

[32] Wainwright  M. and Jordan  M. Graphical models  exponential families  and variational inference. Found.

Trends Mach. Learn.  1(1-2):1–305  2008.

[33] Zhu  S. C.  Wu  Y.  and Mumford  D. Filters  random ﬁelds and maximum entropy (FRAME): Towards a

uniﬁed theory for texture modeling. International Journal of Computer Vision  27(2):107–126  1998.

9

,Justin Domke
Haipeng Luo
Alekh Agarwal
Nicolò Cesa-Bianchi
John Langford