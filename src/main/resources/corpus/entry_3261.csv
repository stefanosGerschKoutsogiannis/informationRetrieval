2019,Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond,Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper  we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions  particularly for f-divergences as well as a generalization to capture mutual information loss. First  we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance  which may be of independent interest. Next  we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss  due to its several important applications such as model compression.,Locality-Sensitive Hashing for f-Divergences
and Kre˘ın Kernels: Mutual Information Loss

and Beyond

Lin Chen1 2 Hossein Esfandiari2 Thomas Fu2 Vahab S. Mirrokni2

lin.chen@yale.edu  {esfandiari thomasfu mirrokni}@google.com

1Yale University 2Google Research

Abstract

Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications
in machine learning and data science. A popular and eﬀective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper  we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions 
particularly for f-divergences as well as a generalization to capture mutual
information loss. First  we provide a general framework to design LHS
schemes for f-divergence distance functions and develop LSH schemes for
the generalized Jensen-Shannon divergence and triangular discrimination in
this framework. We show a two-sided approximation result for approximation
of the generalized Jensen-Shannon divergence by the Hellinger distance 
which may be of independent interest. Next  we show a general method of
reducing the problem of designing an LSH scheme for a Kre˘ın kernel (which
can be expressed as the diﬀerence of two positive deﬁnite kernels) to the
problem of maximum inner product search. We exemplify this method by
applying it to the mutual information loss  due to its several important
applications such as model compression.

Introduction

1
A central problem in machine learning and data mining is to ﬁnd top-k similar items to each
item in a dataset. Such problems  referred to as approximate nearest neighbor problems  are
especially challenging in high dimensional spaces and are an important part of a wide range
of data mining tasks such as ﬁnding near-duplicate pages in a corpus of images or web pages 
or clustering items in a high-dimensional metric space. A popular technique for solving these
problems is the locality-sensitive hashing (LSH) technique [19]. In this method  items in a
high-dimensional metric space are ﬁrst mapped into buckets (via a hashing scheme) with
the property that closer items have a higher chance of being assigned to the same bucket.
LSH-based nearest neighbor methods limit their scope of search to the items that fall into
the same bucket in which the target item resides 1.
Locality sensitive hashing was ﬁrst introduced and studied by [19]. They provide a family of
basic locality-sensitive hash functions for the Hamming distance in a d-dimensional space
1We note that LSH is a popular data-independent technique for nearest neighbor search. Another
category of nearest neighbor search algorithms  referred to as data-dependent techniques  are
learning-to-hash methods [37] which learn a hash function that maps each item to a compact code.
However  this line of work is out of the scope of this paper.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

and for the L1 distance in a d-dimensional Euclidean space. They also show that such a
family of hash functions provides a randomized (1 + )-approximation algorithm for the
nearest neighbor search problem with sublinear space and sublinear query time. Following
[19]  several families of locality-sensitive hash functions have been designed and implemented
for diﬀerent metrics  each serving a certain application. We summarize further results in
this area in Section 1.1.
In several applications  data points can be represented as probability distributions. One
example is the space of users’ browsed web pages  read articles or watched videos. In order
to represent such data  one can represent each user by a distribution of documents they
read  and the documents by topics included in those documents. Other examples are time
series distributions  content of documents  or images that can be represented as histograms.
Particularly  analysis of similarities in time series distributions or documents can be used
in the context of attacks  and spam detection. Analysis of user similarities can be used in
recommendation systems and online advertisement.
In fact  many of the aforementioned applications deal with huge datasets and require very
time eﬃcient algorithms to ﬁnd similar data points. These applications motivated us to study
LSH functions for distributions  especially for distance measures with information-theoretic
justiﬁcations. In fact  in addition to k-nearest neighbor  LSH functions can be used to
implement very fast distributed algorithms for traditional clusterings such as k-means [7].
Recently  Mao et al. [26] noticed the importance and lack of LSH functions for the distance
of distributions  especially for information-theoretic measures. They attempted to design an
LSH to capture the famous Jensen-Shannon (JS) divergence. However  instead of directly
providing locality-sensitive hash functions for Jensen-Shannon divergence  they take two
steps to turn this distance function into a new distance function that is easier to hash. They
ﬁrst looked at a less common divergence measure S2JSD which is the square root of two
times the JS divergence. Then they deﬁned a related distance function S2JSDapprox
new   which
was obtained by only keeping the linear terms in the Taylor expansion of the logarithm in
the expression of S2JSD and designed a locality-sensitive hash function for the new measure
new . This is an interesting work; however  unfortunately it does not provide any
S2JSDapprox
bound on the actual JS divergence using the LSH that they designed for S2JSDapprox
new . Our
results resolve this issue by providing LSH schemes with provable guarantees for information-
theoretic distance measures including the JS divergence and its generalizations.
Mu and Yan [27] proposed an LSH algorithm for non-metric data by embedding them into a
reproducing kernel Kre˘ın space. However  their method is indeed data-dependent. Given a
ﬁnite set of data points M  they compute the distance matrix D whose (i  j)-entry is the
distance between i and j  where both i and j are data points in M. Data is embedded into
a reproducing kernel Kre˘ın space by performing singular value decomposition on a transform
of the distance matrix D. The embedding changes if we are given another dataset.
Our Contributions. In this paper  we ﬁrst study LSH schemes for f-divergences2 between
two probability distributions. We ﬁrst in Proposition 1 provide a simple reduction tool for
designing LSH schemes for the family of f-divergence distance functions. This proposition
is not hard to prove but might be of independent interest. Next we use this tool and
provide LSH schemes for two examples of f-divergence distance functions  Jensen-Shannon
divergence and triangular discrimination. Interestingly our result holds for a generalized
version of Jensen-Shannon divergence. We apply this tool to design and analyze an LSH
scheme for the generalized Jensen-Shannon (GJS) divergence through approximation by the
squared Hellinger distance. We use a similar technique to provide an LSH for triangular
discrimination. Our approximation is provably lower bounded by a factor 0.69 for the Jensen-
Shannon divergence and is lower bounded by a factor 0.5 for triangular discrimination. The
approximation result of the generalized Jensen-Shannon divergence by the squared Hellinger
requires a more involved analysis and the lower and upper bounds depend on the weight
parameter. This approximation result may be of independent interest for other machine
learning tasks such as approximate information-theoretic clustering [12]. Our technique may
be useful for designing LSH schemes for other f-divergences.

2The formal deﬁnition of f-divergence is presented in Section 2.2.

2

Next  we propose a general approach to designing an LSH for Kre˘ın kernels. A Kre˘ın
kernel is a kernel function that can be expressed as the diﬀerence of two positive deﬁnite
kernels. Our approach is built upon a reduction to the problem of maximum inner product
search (MIPS) [33  28  41]. In contrast to our LSH schemes for f-divergence functions via
approximation  our approach for Kre˘ın kernels involves no approximation and is theoretically
lossless. Contrary to [27]  this approach is data-independent. We exemplify our approach
by designing an LSH function speciﬁcally for mutual information loss. Mutual information
loss is of our particular interest due to its several important applications such as model
compression [6  17]  and compression in discrete memoryless channels [20  30  42].

1.1 Other Related Work
Datar et al. [16] designed an LSH for Lp distances using p-stable distributions. Broder
[10] designed MinHash for the Jaccard similarity. LSH for other distances and similarity
measures were proposed later  for example  angle similarity [11]  spherical LSH on a unit
hypersphere [34]  rank similarity [40]  and non-metric LSH [27]. Li et al. [24] demonstrated
that uniform quantization outperforms the standard method in [16] with a random oﬀset.
Gorisse et al. [18] proposed an LSH family for χ2 distance by relating it to the L2 distance
via an algebraic transform. Interested readers are referred to a more comprehensive survey of
existing LSH methods [38]. Another related problem is the construction of feature maps of
positive deﬁnite kernels. A feature map maps a data point into a usually higher-dimensional
space such that the inner product in that space agrees with the kernel in the original space.
Explicit feature maps for additive kernels are introduced in [35]. Bregman divergences are
another broad class of distances that arise naturally in practical applications. The nearest
neighbor search problem for Bregman divergences were studied in [3  2  1].

2 Preliminaries
2.1 Locality-Sensitive Hashing
Let M be the universal set of items (the database)  endowed with a distance function D.
Ideally  we would like to have a family of hash functions such that for any two items p and
q in M that are close to each other  their hash values collide with a higher probability 
and if they reside far apart  their hash values collide with a lower probability. A family of
hash functions with the above property is said to be locality-sensitive. A hash value is also
known as a bucket in other literature. Using this metaphor  hash functions are imagined
as sorters that place items into buckets. If hash functions are locality-sensitive  it suﬃces
to search the bucket into which an item falls if one wants to know its nearest neighbors.
The (r1  r2  p1  p2)-sensitive LSH family formulates the intuition of locality sensitivity and is
formally deﬁned in Deﬁnition 1.
Deﬁnition 1 ([19]). Let H = {h : M → U} be a family of hash functions  where U is the
set of possible hash values. Assume that there is a distribution h ∼ H over the family of
functions. This family H is called (r1  r2  p1  p2)-sensitive (r1 < r2 and p1 > p2) for D  if for
∀p  q ∈ M the following statements hold: (1) if D(p  q) ≤ r1  then Prh∼H[h(p) = h(q)] ≥ p1;
(2) if D(p  q) > r2  then Prh∼H[h(p) = h(q)] ≤ p2.
We would like to note that the gap between the high probability p1 and p2 can be ampliﬁed
by constructing a compound hash function that concatenates multiple functions from an LSH
family. For example  one can construct g : M → U K such that g(p) (cid:44) (h1(p)  . . .   hK(p)) for
∀p ∈ M  where h1  . . .   hK are chosen from the LSH family H. This conjunctive construction
reduces the amount of items in one bucket. To improve the recall  an additional disjunction
is introduced. To be precise  if g1  . . .   gL are L such compound hash functions  we search all
of the buckets g1(p)  . . .   gL(p) in order to ﬁnd the nearest neighbors of p.

2.2 f-Divergence
Let P and Q be two probability measures associated with a common sample space Ω. We
write P (cid:28) Q if P is absolutely continuous with respect to Q  which requires that for every
subset A of Ω  Q(A) = 0 imply P(A) = 0.

3

Let f : (0 ∞) → R be a convex function that satisﬁes f(1) = 0. If P (cid:28) Q  the f-divergence
from P to Q [14] is deﬁned by

Df(P k Q) =

dQ 

(1)

Z

f

Ω

(cid:18) dP

(cid:19)

dQ

√

√

Ω(

R

dQ dP [13]. If hel(t) = 1
2(
dP − √
dQ)2 [15]. If δ(t) = (t−1)2

R
the triangular discrimination between P and Q is given by ∆(P k Q) =P

dQ is the Radon-Nikodym derivative of P
provided that the right-hand side exists  where dP
with respect to Q. In general  an f-divergence is not symmetric: Df(P k Q) 6= Df(Q k P).
If fKL(t) = t ln t + (1 − t)  the fKL-divergence yields the KL divergence DKL(P k Q) =
t − 1)2  the hel-divergence is the squared Hellinger distance
Ω ln dP
H2(P  Q) = 1
t+1   the δ-divergence is the triangular
2
discrimination (also known as Vincze-Le Cam distance) [22  36]. If the sample space is ﬁnite 
(P (i)−Q(i))2
.
P (i)+Q(i)
The Jensen-Shannon (JS) divergence is a symmetrized version of the KL divergence. If
P (cid:28) Q  Q (cid:28) P and M = (P + Q)/2  the JS divergence is deﬁned by
2 DKL(Q k M) .

2 DKL(P k M) + 1

DJS(P k Q) = 1

(2)

i∈Ω

2.3 Mutual Information Loss and Generalized Jensen-Shannon Divergence
The mutual information loss arises naturally in many machine learning tasks  such as
information-theoretic clustering [17] and categorical feature compression [6].
Suppose that two random variables X and C obeys a joint distribution p(X  C). This joint
distribution can model a dataset where X denotes the feature value of a data point and C
denotes its label [6]. Let X and C denote the support of X and C (i.e.  the universal set of
all possible feature values and labels)  respectively. Consider clustering two feature values
into a new combined value. This operation can be represented by the following map

(cid:26)t 

z 

t ∈ X \ {x  y}  
t = x  y  

πx y : X → X \ {x  y} ∪ {z}

such that πx y(t) =

where x and y are the two feature values to be clustered and z /∈ X is the new combined feature
value. To make the dataset after applying the map πx y preserve as much information of the
original dataset as possible  one has to select two feature values x and y such that the mutual
information loss incurred by the clustering operation mil(x  y) = I(X; C) − I(πx y(X); C) is
minimized  where I(·;·) is the mutual information between two random variables [13]. Note
that the mutual information loss (MIL) divergence mil : X × X → R is symmetric in both
arguments and always non-negative due to the data processing inequality [13].
Next  we motivate the generalized Jensen-Shannon divergence. If we let P and Q be the
conditional distribution of C given X = x and X = y  respectively  such that P(c) = p(C =
c|X = x) and Q(c) = p(C = c|X = y)  the mutual information loss can be re-written as

λDKL(P k Mλ) + (1 − λ)DKL(Q k Mλ)  

(3)
p(x)+p(y) and the distribution Mλ = λP + (1− λ)Q. Note that (3) is a generalized
where λ = p(x)
version of (2). Therefore  we deﬁne the generalized Jensen-Shannon (GJS) divergence between
GJS(P k Q) = λDKL(P k Mλ)+(1−λ)DKL(Q k Mλ)  where λ ∈ [0  1]
P and Q [25  5  17] by Dλ
and Mλ = λP +(1−λ)Q. We immediately have D
GJS(P k Q) = DJS(P k Q)  which indicates
1/2
that the JS divergence is indeed a special case of the GJS divergence when λ = 1/2. The GJS
GJS(P k Q) = H(Mλ)− λH(P)−(1− λ)H(Q) 
divergence has another equivalent deﬁnition Dλ
where H(·) denotes the Shannon entropy [13]. In contrast to the MIL divergence  the GJS
GJS(· k ·) is not symmetric in general as the weight λ ∈ [0  1] is ﬁxed and not necessarily
Dλ
equal to 1/2. We will show in Lemma 1 that the GJS divergence is an f-divergence.

2.4 Positive Deﬁnite Kernel and Kre˘ın Kernel
We ﬁrst review the deﬁnition of a positive deﬁnite kernel.

4

real numbers a1  . . .   an ∈ R  and x  . . .   xn ∈ X   it holds thatPn

Pn
Deﬁnition 2 (Positive deﬁnite kernel [32]). Let X be a non-empty set. A symmetric 
real-valued map k : X × X → R is a positive deﬁnite kernel on X if for all positive integer n 
j=1 aiajk(xi  xj) ≥ 0.
A kernel is said to be a Kre˘ın kernel if it can be represented as the diﬀerence of two positive
deﬁnite kernels. The formal deﬁnition is presented below.
Deﬁnition 3 (Kre˘ın kernel [29]). Let X be a non-empty set. A symmetric  real-valued map
k : X × X → R is a Kre˘ın kernel on X if there exists two positive deﬁnite kernels k1 and k2
on X such that k(x  y) = k1(x  y) − k2(x  y) holds for all x  y ∈ X .

i=1

3 LSH Schemes for f-Divergences

Q(i)) [31].

We build LSH schemes for f-divergences based on approximation via another f-divergence if
the latter admits an LSH family. If Df and Dg are two divergences associated with convex
functions f and g as deﬁned by (1)  the approximation ratio of Df(P k Q) to Dg(P k Q)
is determined by the ratio of the functions f and g  as well as the ratio of P to Q (to be
precise  inf i∈Ω P (i)
Proposition 1 (Proof in Appendix A.4). Let β0 ∈ (0  1)  L  U > 0 and let f and g be two
convex functions (0 ∞) → R that obey f(1) = 0  g(1) = 0  and f(t)  g(t) > 0 for every t 6= 1.
Let P be a set of probability measures on a ﬁnite sample space Ω such that for every i ∈ Ω
and P  Q ∈ P  0 < β0 ≤ P (i)
0 )  it holds
that 0 < L ≤ f(β)
g(β) ≤ U < ∞. If H forms an (r1  r2  p1  p2)-sensitive family for g-divergence
on P  then it is also an (Lr1  U r2  p1  p2)-sensitive family for f-divergence on P.
Proposition 1 provides a general strategy of constructing LSH families for f-divergences.
The performance of such LSH families depends on the tightness of the approximation. In
Sections 3.1 and 3.2  as instances of the general strategy  we derive concrete results for the
generalized Jensen-Shannon divergence and triangular discrimination  respectively.

0 . Assume that for every β ∈ (β0  1) ∪ (1  β−1

Q(i) ≤ β−1

3.1 Generalized Jensen-Shannon Divergence
First  Lemma 1 shows that the GJS divergence is indeed an instance of f-divergence.
Lemma 1 (Proof in Appendix A.3). Deﬁne mλ(t) = λt ln t − (λt + 1 − λ) ln(λt + 1 − λ).
For any λ ∈ [0  1]  mλ(t) is convex on (0 ∞) and mλ(1) = 0. Furthermore  mλ-divergence
yields the GJS divergence with parameter λ.
We choose to approximate it via the squared Hellinger distance  which plays a central role in
the construction of the hash family with desired properties.
The approximation guarantee is established in Theorem 1. We show that the ratio of
GJS(P k Q) to H2(P  Q) is upper bounded by the function U(λ) and lower bounded by
Dλ
the function L(λ). Furthermore  Theorem 1 shows that U(λ) ≤ 1  which implies that the
squared Hellinger distance is an upper bound of the GJS divergence.
Theorem 1 (Proof in Appendix A.2). We assume that the sample space Ω is ﬁnite. Let P
and Q be two diﬀerent distributions on Ω. For every t > 0 and λ ∈ (0  1)  we have

L(λ)H2(P  Q) ≤ Dλ

GJS(P k Q) ≤ U(λ)H2(P  Q) ≤ H2(P  Q) 

where L(λ) = 2 min{η(λ)  η(1 − λ)}  η(λ) = −λ ln λ and U(λ) = 2λ(1−λ)
We show Theorem 1 by showing a two-sided approximation result regarding mλ and hel. This
result might be of independent interest for other machine learning tasks  say  approximate
information-theoretic clustering [12].
Lemma 2 (Proof in Appendix A.1). Deﬁne κλ(t) = mλ(t)
we have κλ(t) = κ1−λ(1/t) and κλ(t) ∈ [L(λ)  U(λ)].

hel(t) . For every t > 0 and λ ∈ (0  1) 

1−2λ ln 1−λ
λ .

5

We illustrate the upper and lower bound functions U(λ) and L(λ) in Appendix B. Recall that
if λ = 1/2  the generalized Jensen-Shannon divergence reduces to the usual Jensen-Shannon
divergence. Theorem 1 yields the approximation guarantee 0.69 < ln 2 ≤ DJS(PkQ)
If the common sample space Ω with which the two distributions P and Q are associated is
ﬁnite  one can identify P and Q with the |Ω|-dimensional vectors [P(i)]i∈Ω and [Q(i)]i∈Ω 
P (cid:44) [pP(i)]i∈Ω and
2k√
P − √
respectively. In this case  H2(P  Q) = 1
2  which is exactly half of the squared
√
L2 distance between the two vectors
the squared Hellinger distance can be endowed with the L2-LSH family [16] applied to the
square root of the vector. In light of this  the locality-sensitive hash function that we propose
&a · √
’
for the generalized Jensen-Shannon divergence is

Q (cid:44) [pQ(i)]i∈Ω. Therefore 

H2(P Q) ≤ 1.

Qk2

√

ha b(P) =

P + b
r

 

(4)

1

P − √

u f2(t/u)(1 − t/r)dt.

where a ∼ N (0  I) is a |Ω|-dimensional standard normal random vector  · denotes the inner
product  b is uniformly at random on [0  r]  and r is a positive real number.
Theorem 2 (Proof in Appendix A.5). Let c = k√
Qk2 and f2 be the probability
density function of the absolute value of the standard normal distribution. The hash functions
{ha b} deﬁned in (4) form a (R  c2 U(λ)
R r
L(λ) R  p1  p2)-sensitive family for the generalized Jensen-
Shannon divergence with parameter λ  where R > 0  p1 = p(1)  p2 = p(c)  and p(u) =
0
3.2 Triangular Discrimination
Recall that triangular discrimination is the δ-divergence  where δ(t) = (t−1)2
t+1 . As shown
in the proof of Theorem 3 (Appendix A.6)  the function δ can be approximated by the
function hel(t) that deﬁnes the squared Hellinger distance 1 ≤ δ(t)
hel(t) ≤ 2. The squared
Hellinger distance can be sketched via L2-LSH after taking the square root  as exempliﬁed in
Section 3.1. By Proposition 1  the LSH family for the square Hellinger distance also forms
an LSH family for the triangular discrimination. Theorem 3 shows that the LSH family
deﬁned in (4) form a (R  2c2R  p1  p2)-sensitive family for triangular discrimination.
Theorem 3 (Proof in Appendix A.6). Let c = k√
Qk2 and f2 be the probability density
function of the absolute value of the standard normal distribution. The hash functions {ha b}
R > 0  p1 = p(1)  p2 = p(c)  and p(u) =R r
deﬁned in (4) form a (R  2c2R  p1  p2)-sensitive family for triangular discrimination  where

u f2(t/u)(1 − t/r)dt.

P −√

1

0

4 Kre˘ın-LSH for Mutual Information Loss
In this section  we ﬁrst show that the mutual information loss is a Kre˘ın kernel. Then we
propose Kre˘ın-LSH  an asymmetric LSH method [33] for mutual information loss. We would
like to remark that this method can be easily extended to other Kre˘ın kernels  provided that
the associated positive deﬁnite kernels allow an explicit feature map.

4.1 Mutual Information Loss is a Kre˘ın Kernel
Recall that in Section 2.3 we assume a joint distribution p(X  C) whose support is X ×C. Let
x  y ∈ X be represented by x = [p(c  x) : c ∈ C] ∈ [0  1]|C| and y = [p(c  y) : c ∈ C] ∈ [0  1]|C| 
respectively. We consider the mutual information loss of merging x and y  which is given by
I(X; C) − I(πx y(X); C).
Theorem 4 (Proof in Appendix A.8). The mutual information loss mil(x  y) is a
Kre˘ın kernel on [0  1]|C|.
In other words  there exist two positive deﬁnite kernels K1
and K2 on [0  1]|C| such that mil(x  y) = K1(x  y) − K2(x  y). To be explicit  we set
c∈C k(p(c  x)  p(c  y))  where

c∈C p(c  y)) and K2(x  y) = P

K1(x  y) = k(P

c∈C p(c  x) P

k(a  b) = a ln a

a+b + b ln b

a+b.

6

To prove Theorem 4 and construct explicit feature maps for K1 and K2  we need the following
lemma.
Lemma 3 (Proof in Appendix A.7). The kernel k is a positive deﬁnite kernel on [0  1].
Moreover  it is endowed with the following explicit feature map x 7→ Φw(x) such that
and Φw(x)∗ denotes the

k(x  y) =R
The map Φ(x) : w 7→ Φw(x) is called the feature map of x. The integralR

R Φw(x)∗Φw(y)dw  where Φw(x) (cid:44) e−iw ln(x)q

complex conjugate of Φw(x).

R Φw(x)∗Φw(y)dw

2 sech(πw)

is also denoted by a Hermitian inner product hΦ(x)  Φ(y)i.

x

1+4w2

4.2 Kre˘ın-LSH for Mutual Information Loss
Now we are ready to present an asymmetric LSH scheme [33] for mutual information loss.
This method can be easily extended to other Kre˘ın kernels  provided that the associated
positive deﬁnite kernels admit an explicit feature map. In fact  we reduce the problem of
designing the LSH for a Kre˘ın kernel to the problem of designing the LSH for maximum
inner product search (MIPS) [33  28  41]. We call this general reduction Kre˘ın-LSH.

4.2.1 Reduction to Maximum Inner Product Search
Our reduction is based on the following observation. Suppose that K is a Kre˘ın kernel on
X such that K = K1 − K2 where K1 and K2 are positive deﬁnite kernels on X . Assume
that K1 and K2 admit feature maps Φ1 and Φ2 such that K1(x  y) = hΨ1(x)  Ψ1(y)i and
K2(x  y) = hΨ2(x)  Ψ2(y)i. Then the Kre˘ın kernel K can also represented as an inner product
(5)
where ⊕ denotes the direct sum. If we deﬁne a pair of transforms T1(x) (cid:44) Φ1(x) ⊕ Φ2(x)
and T2(x) (cid:44) Φ1(x) ⊕ −Φ2(x)  then we have K(x  y) = hT1(x)  T2(y)i. We call this pair of
transforms left and right Kre˘ın transforms.

K(x  y) = hΦ1(x) ⊕ Φ2(x)  Φ1(y) ⊕ −Φ2(y)i  

Algorithm 1 Kre˘ın-LSH
Input: Discretization parameters J ∈ N and ∆ > 0.
Output: The left and right Kre˘ın transform η1 and η2.
1: wj ← (j − 1/2)∆ for j = 1  . . .   J
2: Construct the atomic transform

"

s

Z j∆

(j−1)∆

τ(x  w  j) (cid:44)

cos(w ln(x))

2x

ρ(w0)dw0  sin(w ln(x))

s

2x

Z j∆

(j−1)∆

#

.

ρ(w0)dw0

3: Construct the left and right basic transform

τ(p(x)  wj  j) ⊕ JM
τ(p(x)  wj  j) ⊕ JM

j=1

j=1

M
M

c∈C

c∈C

j=1

η1(x) (cid:44) JM
η2(x) (cid:44) JM
q

j=1

τ(p(c  x)  wj  j)  

−τ(p(c  x)  wj  j) .

q

4: Construct the left and right Kre˘ın transform

T1(x  M) (cid:44) [η1 

M − kη1(x)k2

5: Sample a ∼ N (0  I) and construct the hash function h(x; M) (cid:44) sign(a>T(x  M))  where

where M is a constant such that M ≥ kη1(x)k2
T is either the left or right transform.

2  0]  T2(y  M) (cid:44) [η2  0 

M − kη2(x)k2
2] .
2 (note that kη1(x)k2= kη2(x)k2).

We exemplify this technique by applying it to the MIL divergence. For ease of exposition 
we deﬁne ρ(w) (cid:44) 2 sech(πw)
. The proposed approach Kre˘ın-LSH is presented in Algorithm 1.
1+4w2

7

and discretize the integral k(x  y) =R

To make the intuition of (5) applicable in a practical implementation  we have to truncate
R Φw(x)∗Φw(y)dw. First we analyze the truncation.
The analysis is similar to Lemma 10 of [4].
Lemma 4 (Truncation error bound  proof in Appendix A.9). If t > 0 and x  y ∈ [0  1]  the
truncation error can be bounded as follows

(cid:12)(cid:12)(cid:12)k(x  y) −R t

(cid:12)(cid:12)(cid:12) ≤ 4e−t.

−t Φw(x)∗Φw(y)dw

To discretize the ﬁnite integralR t
(cid:12)(cid:12)(cid:12)R ∆J
−∆J Φw(x)∗Φw(y)dw −DLJ
hcos(w ln(x))

−t Φw(x)∗Φw(y)dw  we divide the inteval into 2J sub-intervals
of length ∆. The following lemma bounds the discretization error.
Lemma 5 (Discretization error bound  proof in Appendix A.10). If J is a positive in-
teger  ∆ > 0  and wj = (j − 1/2)∆  the discretization error is bounded as follows

j=1 τ(y  wj  j)E(cid:12)(cid:12)(cid:12) ≤ 2∆  where τ(x  w  j) =
j=1 τ(x  wj  j) LJ
q
(j−1)∆ ρ(w0)dw0i ∈ R2.
2xR j∆

(j−1)∆ ρ(w0)dw0  sin(w ln(x))

2xR j∆

q







.

ln 8(1+|C|)

By Lemmas 4 and 5  to guarantee that the total approximation error (including both
truncation and discretization errors) is at most   it suﬃces to set ∆ =
4(1+|C|) and
J ≥ 4(1+|C|)
4.2.2 LSH for Maximum Inner Product Search
The second stage of our proposed method is to apply LSH to the MIPS problem. As
an example  in Line 5  we use the Simple-LSH introduced by [28]. Let us have a quick
review of Simple-LSH. Assume that M ⊆ Rd is a ﬁnite set of vectors and that for all
x ∈ M  there is a universal bound on the squared 2-norm  i.e.  kxk2
2≤ M. Neyshabur and
Srebro [28] assume that M = 1 without loss of generality. We allow M to be any positive
real number. For two vectors x  y ∈ M  Simple-LSH performs the following transform
2]. Note that the norm of L1 and L2 is
M and that therefore their cosine similarity equals their inner product. In fact  Simple-LSH
is a reduction from MIPS to LSH for the cosine similarity. Then a random-projection-based
LSH for the cosine similarity [11  38]

2  0]  L2(y) (cid:44) [y  0 pM − kyk2

L1(x) (cid:44) [x pM − kxk2

h(x) (cid:44) sign(x>Li(x)) 

a ∼ N (0  I)  i = 1  2

can be used for MIPS and thereby LSH for the MIL divergence via our reduction.

additional term pM − kxk2

Discussion We have some important remarks for practical implementation of Kre˘ın-LSH.
Although [28] provides a theoretical guarantee for LSH for MIPS  as noted in [41]  the
2 may dominate in the 2-norm and signiﬁcantly degrade the
performance of LSH. To circumvent this issue  we recommend a method that partitions the
dataset according to the 2-norm  e.g.  the norm-ranging method [41].

5 Experiment Results

(a) λ = 1/2
(c) λ = 1/10
Figure 1: The empirical performance of Hellinger approximation

(b) λ = 1/3

Approximation Guarantee. In the ﬁrst part  we verify the theoretical bounds derived
in Theorem 1 on real data. We used the latent Dirichlet allocation to extract the topic

8

GJSGJS GJS(a) Fashion MNIST

(b) MNIST

(c) CIFAR-10

Figure 2: Precision vs. speed-up factor for diﬀerent λ’s.

distributions of Reuters-21578  Distribution 1.0. The number of topics is set to 10. We
sampled 100 documents uniformly at random and computed the GJS divergence and Hellinger
distance between each pair of topic distributions. Each dot in Fig. 1 represents the topic
distribution of a document. The horizontal axis denotes the Hellinger distance while
the vertical axis denotes the GJS divergence. We chose diﬀerent parameter values (λ =
1/2  1/3  1/10) for the GJS divergence. From the three subﬁgures  we observe that both the
upper and lower bounds are tight for the data.
Nearest Neighbor Search. In the second part  we apply the proposed LSH scheme for the
GJS divergence to the nearest neighbor search problem in Fashion MNIST [39]  MNIST [23] 
and CIFAR-10 [21]. Each image in the datasets is ﬂattened into a vector and L1-normalized 
thereby summing to 1. As described in Section 2.1  a concatenation of hash functions is
used. We denote the number of concatenated hash functions by K and the number of
compound hash functions by L. In the ﬁrst set of experiments  we set K = 3 and vary
L from 20 to 40. We measure the execution time of LSH-based k-nearest neighbor search
and the exact (brute-force) algorithm  where k is set to 20. Both algorithms were run on a
2.2 GHz Intel Core i7 processor. The speed-up factor is the ratio of the execution time of
the exact algorithm to that of the LSH-based method. The quality of the result returned
by the LSH-based method is quantiﬁed by its precision  which is the fraction of correct
nearest neighbors among the retrieved items. We would like to remark that the precision
and recall are equal in our case since both algorithms return k items. We also vary the
parameter of the GJS divergence and choose λ from {1/2  1/3  1/10}. The result is illustrated
in Figs. 2a to 2c. We observe a trade-oﬀ between the quality of the output (precision) and
computational eﬃciency (speed-up factor). The performance appears to be robust to the
parameter of the GJS divergence. In the second set of experiments  we ﬁx the parameter of
the GJS divergence to 1/2; i.e.  the JS divergence is used. The number of concatenated hash
functions K ranges from 3 to 5 or 4 to 6. The result is presented in Appendix C. In addition
to the aforementioned quality-eﬃciency trade-oﬀ  we observe that a larger K results in a
more eﬃcient algorithm given the same target precision.

6 Conclusion
In this paper  we propose a general strategy of designing an LSH family for f-divergences.
We exemplify this strategy by developing LSH schemes for the generalized Jensen-Shannon
divergence and triangular discrimination in this framework. They are endowed with an LSH
family via the Hellinger approximation. In particular  we show a two-sided approximation
for the generalized Jensen-Shannon divergence by the Hellinger distance. This may be of
independent interest. Next  we propose a general approach to designing an LSH scheme
for Kre˘ın kernels via a reduction to the problem of maximum inner product search. In
contrast to our strategy for f-divergences  this approach involves no approximation and is
theoretically lossless. We exemplify this approach by applying to mutual information loss.

Acknowledgments
LC was supported by the Google PhD Fellowship.

9

References
[1] Ahmed Abdelkader  Sunil Arya  Guilherme D da Fonseca  and David M Mount.
“Approximate nearest neighbor searching with non-Euclidean and weighted distances”.
In: SODA. SIAM. 2019  pp. 355–372.

[2] Amirali Abdullah and Suresh Venkatasubramanian. “A directed isoperimetric inequality
with application to bregman near neighbor lower bounds”. In: STOC. ACM. 2015 
pp. 509–518.

[3] Amirali Abdullah  John Moeller  and Suresh Venkatasubramanian. “Approximate
Bregman near neighbors in sublinear time: Beyond the triangle inequality”. In: SoCG.
ACM. 2012  pp. 31–40.

[4] Amirali Abdullah  Ravi Kumar  Andrew McGregor  Sergei Vassilvitskii  and Suresh
Venkatasubramanian. “Sketching  Embedding and Dimensionality Reduction in Infor-
mation Theoretic Spaces”. In: AISTATS. 2016  pp. 948–956.

[5] Syed Mumtaz Ali and Samuel D Silvey. “A general class of coeﬃcients of divergence of
one distribution from another”. In: Journal of the Royal Statistical Society. Series B
(Methodological) (1966)  pp. 131–142.

[6] MohammadHossein Bateni  Lin Chen  Hossein Esfandiari  Thomas Fu  Vahab S Mir-
rokni  and Afshin Rostamizadeh. “Categorical Feature Compression via Submodular
Optimization”. In: ICML (2019).

[7] Aditya Bhaskara and Maheshakya Wijewardena. “Distributed Clustering via LSH

Based Data Partitioning”. In: ICML. 2018  pp. 569–578.

[8] Christopher M Bishop. Pattern recognition and machine learning. springer  2006.
[9] S. Bochner  M. Tenenbaum  and H. Pollard. Lectures on Fourier Integrals. Annals of

mathematics studies. Princeton University Press  1959.

[10] Andrei Z Broder. “On the resemblance and containment of documents”. In: SE-

[11] Moses S Charikar. “Similarity estimation techniques from rounding algorithms”. In:

QUENCES. IEEE. 1997  pp. 21–29.

STOC. ACM. 2002  pp. 380–388.

[12] Kamalika Chaudhuri and Andrew McGregor. “Finding Metric Structure in Information

Theoretic Clustering.” In: COLT. Vol. 8. 2008  p. 10.

[14]

[13] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley &

Sons  2012.
Imre Csiszár. “Eine informationstheoretische ungleichung und ihre anwendung auf
beweis der ergodizitaet von markoﬀschen ketten”. In: Publ. Math. Inst. Hungar. Acad.
8 (1963)  pp. 95–108.

[17]

[15] Constantinos Daskalakis and Qinxuan Pan. “Square Hellinger Subadditivity for
Bayesian Networks and its Applications to Identity Testing”. In: COLT. 2017  pp. 697–
703.

[16] Mayur Datar  Nicole Immorlica  Piotr Indyk  and Vahab S Mirrokni. “Locality-sensitive
hashing scheme based on p-stable distributions”. In: SoCG. ACM. 2004  pp. 253–262.
Inderjit S Dhillon  Subramanyam Mallela  and Rahul Kumar. “A divisive information-
theoretic feature clustering algorithm for text classiﬁcation”. In: JMLR 3.Mar (2003) 
pp. 1265–1287.

[18] David Gorisse  Matthieu Cord  and Frederic Precioso. “Locality-sensitive hashing for
chi2 distance”. In: IEEE transactions on pattern analysis and machine intelligence
34.2 (2012)  pp. 402–409.

[19] Piotr Indyk and Rajeev Motwani. “Approximate nearest neighbors: towards removing

the curse of dimensionality”. In: STOC. ACM. 1998  pp. 604–613.

[20] Assaf Kartowsky and Ido Tal. “Greedy-Merge Degrading has Optimal Power-Law”. In:

IEEE Transactions on Information Theory 65.2 (2018)  pp. 917–934.

[21] Alex Krizhevsky and Geoﬀrey Hinton. Learning multiple layers of features from tiny

[22] Lucien Le Cam. Asymptotic methods in statistical decision theory. Springer Science &

images. Tech. rep. Citeseer  2009.

Business Media  2012.

10

[23] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haﬀner. “Gradient-based
learning applied to document recognition”. In: Proceedings of the IEEE 86.11 (1998) 
pp. 2278–2324.

[24] Ping Li  Michael Mitzenmacher  and Anshumali Shrivastava. “Coding for random

projections”. In: ICML. 2014  pp. 676–684.

[25] Jianhua Lin. “Divergence measures based on the Shannon entropy”. In: IEEE Trans-

actions on Information theory 37.1 (1991)  pp. 145–151.

[26] Xianling Mao  Bo-Si Feng  Yi-Jing Hao  Liqiang Nie  Heyan Huang  and Guihua Wen.
“S2JSD-LSH: A Locality-Sensitive Hashing Schema for Probability Distributions.” In:
AAAI. 2017  pp. 3244–3251.

[27] Yadong Mu and Shuicheng Yan. “Non-Metric Locality-Sensitive Hashing.” In: AAAI.

2010  pp. 539–544.

[28] Behnam Neyshabur and Nathan Srebro. “On Symmetric and Asymmetric LSHs for

Inner Product Search”. In: ICML. 2015  pp. 1926–1934.

[29] Cheng Soon Ong  Xavier Mary  Stéphane Canu  and Alexander J Smola. “Learning

with non-positive kernels”. In: ICML. ACM. 2004  p. 81.

[31]

[30] Yuta Sakai and Ken-ichi Iwata. “Suboptimal quantizer design for outputs of discrete
memoryless channels with a ﬁnite-input alphabet”. In: ISIT. IEEE. 2014  pp. 120–124.
Igal Sason and Sergio Verdú. “f-divergence Inequalities”. In: IEEE Transactions on
Information Theory 62.11 (2016)  pp. 5973–6006.

[32] Bernhard Schölkopf. “The kernel trick for distances”. In: NeurIPS. 2001  pp. 301–307.
[33] Anshumali Shrivastava and Ping Li. “Asymmetric LSH (ALSH) for sublinear time

maximum inner product search (MIPS)”. In: NeurIPS. 2014  pp. 2321–2329.

[34] Kengo Terasawa and Yuzuru Tanaka. “Spherical lsh for approximate nearest neighbor
search on unit hypersphere”. In: Workshop on Algorithms and Data Structures. Springer.
2007  pp. 27–38.

[35] Andrea Vedaldi and Andrew Zisserman. “Eﬃcient additive kernels via explicit feature
maps”. In: IEEE transactions on pattern analysis and machine intelligence 34.3 (2012) 
pp. 480–492.
István Vincze. “On the concept and measure of information contained in an observation”.
In: Contributions to Probability. Elsevier  1981  pp. 207–214.

[37] Jingdong Wang  Ting Zhang  Nicu Sebe  Heng Tao Shen  et al. “A survey on learning
to hash”. In: IEEE transactions on pattern analysis and machine intelligence 40.4
(2018)  pp. 769–790.

[38] Jingdong Wang  Heng Tao Shen  Jingkuan Song  and Jianqiu Ji. “Hashing for similarity

[36]

search: A survey”. In: arXiv preprint arXiv:1408.2927 (2014).

[39] Han Xiao  Kashif Rasul  and Roland Vollgraf. “Fashion-mnist: a novel image dataset
for benchmarking machine learning algorithms”. In: arXiv preprint arXiv:1708.07747
(2017).

[40] Jay Yagnik  Dennis Strelow  David A Ross  and Ruei-sung Lin. “The power of compar-

ative reasoning”. In: ICCV. IEEE. 2011  pp. 2431–2438.

[41] Xiao Yan  Jinfeng Li  Xinyan Dai  Hongzhi Chen  and James Cheng. “Norm-Ranging

LSH for Maximum Inner Product Search”. In: NeurIPS. 2018  pp. 2952–2961.

[42] Jiuyang Alan Zhang and Brian M Kurkoski. “Low-complexity quantization of discrete

memoryless channels”. In: ISITA. IEEE. 2016  pp. 448–452.

11

,Lin Chen
Hossein Esfandiari
Gang Fu
Vahab Mirrokni