2019,Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion,In this paper  we propose our Correlation For Completion Network (CFCNet)  an end-to-end deep learning model that uses the correlation between two data sources to perform sparse depth completion. CFCNet learns to capture  to the largest extent  the semantically correlated features between RGB and depth information. Through pairs of image pixels and the visible measurements in a sparse depth map  CFCNet facilitates feature-level mutual transformation of different data sources. Such a transformation enables CFCNet to predict features and reconstruct data of missing depth measurements according to their corresponding  transformed RGB features. We extend canonical correlation analysis to a 2D domain and formulate it as one of our training objectives (i.e. 2d deep canonical correlation  or “2D^2CCA loss"). Extensive experiments  validate the ability and flexibility of our CFCNet compared to the state-of-the-art methods on both indoor and outdoor scenes with different real-life sparse patterns. Codes are available at: https://github.com/choyingw/CFCNet.,Deep RGB-D Canonical Correlation Analysis For

Sparse Depth Completion

University of Southern California

University of Southern California

Cho-Ying Wu∗

Los Angeles  California
choyingw@usc.edu

Yiqi Zhong∗

Los Angeles  California
yiqizhon@usc.edu

Suya You

US Army Research Laboratory

Playa Vista  California

suya.you.civ@mail.mil

Ulrich Neumann

University of Southern California

Los Angeles  California
uneumann@usc.edu

Abstract

In this paper  we propose our Correlation For Completion Network (CFCNet)  an
end-to-end deep learning model that uses the correlation between two data sources
to perform sparse depth completion. CFCNet learns to capture  to the largest extent 
the semantically correlated features between RGB and depth information. Through
pairs of image pixels and the visible measurements in a sparse depth map  CFCNet
facilitates feature-level mutual transformation of different data sources. Such a
transformation enables CFCNet to predict features and reconstruct data of missing
depth measurements according to their corresponding  transformed RGB features.
We extend canonical correlation analysis to a 2D domain and formulate it as one
of our training objectives (i.e. 2d deep canonical correlation  or “2D2CCA loss").
Extensive experiments validate the ability and ﬂexibility of our CFCNet compared
to the state-of-the-art methods on both indoor and outdoor scenes with different real-
life sparse patterns. Codes are available at: https://github.com/choyingw/CFCNet.

1

Introduction

Depth measurements are widely used in computer vision applications [1  2  3]. However  most of
the existing techniques for depth capture produce depth maps with incomplete data. For example 
structured-light cameras cannot capture depth measurements where surfaces are too shiny; Visual
Simultaneous Localization And Mappings (VSLAMs) are not able to recover depth of non-textured
objects; LiDARs produce semi-dense depth map due to the limited scanlines and scanning frequency.
Recently  researchers have introduced the sparse depth completion task  aiming to ﬁll missing depth
measurements using deep learning based methods [4  5  6  7  8  9  10]. Those studies produce
dense depth maps by fusing features of sparse depth measurements and corresponding RGB images.
However  they usually treat feature extraction of these two types of information as independent
processes  which in reality turns the task they work on into "multi-modality depth prediction" rather
than "depth completion." While the multi-modality depth prediction may produce dense outputs 
they fail to fully utilize observable data. The depth completion task is unique in that part of its
output is already observable in the input. Revealing the relationship between data pairs (i.e. between
observable depth measurements and the corresponding image pixels) may help complete depth maps
by emphasizing the information from image domain at the locations where the depth values are
non-observable.

∗Both authors contributed equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Sample results of CFCNet on different sparse patterns. We show in the order of RGB images 
sparse depth maps with different sparse patterns  and dense depth maps completed by CFCNet. For
the stereo pattern and the ORB pattern  we show the depth groundtruth in the last column.

To accomplish the depth completion task from a novel perspective  we propose an end-to-end deep
learning based framework  Correlation For Completion Network (CFCNet). We view a completed
dense depth map as composed of two parts. One is the sparse depth which is observable and used as
the input  another is non-observable and recovered by the task. Also  the corresponding full RGB
image of the depth map can be decomposed into two parts  one is called the sparse RGB  which
holds the corresponding RGB values at the observable locations in the sparse depth. The other part
is complementary RGB  which is the subtraction of the sparse RGB from the full RGB images.
See Figure 2 for examples. During the training phase  CFCNet learns the relationship between
sparse depth and sparse RGB and uses the learned knowledge to recover non-observable depth from
complementary RGB.
To learn the relationship between two modalities  we propose a 2D deep canonical correlation analysis
(2D2CCA). In the proposed method  our 2D2CCA tries to learn non-linear projections where the
projected features from RGB and depth domain are maximally correlated. Using 2D2CCA as an
objective function  we could capture the semantically correlated features from the RGB and depth
domain. In this fashion  we utilize the relationship of observable depth and its corresponding non-
observable locations of the RGB input. We then use the joint information learned from the input data
pairs to output a dense depth map. The pipeline of our CFCNet is shown in Figure 2. Details of our
method are described in Section 3. The main contributions of CFCNet can be summarized as follows.
• Constructing a framework for the sparse depth completion task which leverages the rela-
tionship between sparse depth and its corresponding RGB image  using the complementary
RGB information to complement the missing sparse depth information.
• Proposing the 2D2CCA which forces feature encoders to extract the most similar semantics
from multiple modalities. Our CFCNet is the ﬁrst to apply the two-dimensional approach in
CCA with deep learning studies. It overcomes the small sample size problem in other CCA
based deep learning frameworks on modern computer vision tasks.
• Achieving state-of-the-art of the depth completion on several datasets with a variety of

sparse patterns that serve real-world settings.

2 Related Work

Sparse Depth Completion is a task that targets at dense depth completion from sparse depth
measurements and a corresponding RGB image. The nature of sparse depth measurements varies
across scenarios and sensors. Sparse depth generated by the stereo method contains more information
on object contours and less information on non-textured areas [11]. LiDAR sensors produce structured
sparsity due to the scanning behavior [12]. Feature based SLAM systems (such as ORB SLAM [13])
only capture depth information at the positions of corresponding feature points. Besides these most
popular three patterns  some other patterns have also been studied. For instance  [14] uses a line
pattern to simulate partial observations from laser systems; [15] culls the depth data of shiny surfaces
area out of the dense depth map to mimic commodity depth cameras’ output. [8] uses uniform grid
patterns. The latter appears a simpliﬁed and artiﬁcial pattern. Real-life situations require a more
practical tool.
As for input sparsity  [4] stacks sparse depth maps and corresponding RGB images together to build a
four-channel (RGB-D) input before fed into a ResNet based depth estimation network. This treatment
produces better results than monocular depth estimation with only RGB images. Other studies

2

involve a two-branch encoder-decoder based framework which is similar to those used in RGB-D
segmentation tasks [9  10  16  17]. Their approaches do not apply special treatments to the sparse
depth branch. They work well on the dataset where sparsity is not extremely severe  e.g. KITTI depth
completion benchmark [6]. In most of the two-branch frameworks  features from different sources
are extracted independently and fused through direct concatenations or additions  or using features
from RGB branch to provide an extra guidance to reﬁne depth prediction results.
Canonical Correlation Analysis is a standard statistical technique for learning the shared subspace
across several original data spaces. For two modalities  from the shared subspace  each representation
is the most predictive to the other representation and the most predictable by the other [18  19]. To
overcome the constraints of traditional CCA where the projections must be linear  deep canonical
correlation analysis (DCCA) [20  21] has been proposed. DCCA uses deep neural network to learn
more complex non-linear projections between multiple modalities. CCA  DCCA  and other variants
have been widely used on multi-modal representation learning problems [22  23  24  25  26  27  28].
The one-dimensional CCA method suffers from the singularity problem of covariance matrices in the
case of high-dimensional space with small sample size (SSS). Existing works have extended CCA to
a two-dimensional way to avoid the SSS problem. [29  30  31] use a similar approach to building
full-rank covariance matrices inspired by 2DPCA [32] and 2DLDA [33] on the face recognition task.
However  those studies do not approximate complex non-linear projections as [20  21] attempt. Our
CFCNet is the ﬁrst to integrate two-dimensional CCA into deep learning frameworks to overcome
the intrinsic problem of applying DCCA to modern computer vision tasks  detailed in Section 3.2.

3 Our Approach

Our goal is to leverage the relationship of the sparse depth and their corresponding pixels in RGB
images in order to optimize the performance of the depth completion task. We try to complement
the missing depth components using cues from RGB domain. Since CCA could learn the shared
subspace with its predictive characteristics  we estimate the missing depth component using features
from RGB domain through CCA. However  traditional CCA has SSS problem in modern computer
vision task  detailed in Section 3.2. We further propose the 2D2CCA to capture similar semantics
from both RGB/depth encoders. After encoders learning the semantically similar features  we use a
transformer network to transform features from RGB to depth domain. This design not only enables
the reconstruction of missing depth features from complementary RGB information but also ensures
semantics similarity and the same numerical range of the two data sources. Based on this structure 
the decoder in CFCNet is capable of using the reconstructed depth features along with the observable
depth features to recover the dense depth map.

3.1 Network Architecture

Proposed CFCNet structure is in Figure 2. CFCNet takes in sparse depth map  sparse RGB  and
complementary RGB. We use our Sparsity-aware Attentional Convolutions (SAConv  as shown in
Figure 3) in VGG16-like encoders. SAConv is inspired by local attention mask [34]. Harley et al. [34]
introduces the segmentation-aware mask to let convolution operators "focus" on the signals consistent
with the segmentation mask. In order to propagate information from reliable sources  we use sparsity
masks to make convolution operations attend on the signals from reliable locations. Difference of
our SAConv and the local attention mask is that SAConv does not apply mask normalization. We
avoid mask normalization because it affect the stability of our later 2D2CCA calculations due to the
numerically small extracted features it produces after several times normalization. Also  similar to
[6]  we use maxpooling operation on masks after every SAConv to keep track of the visibility. If
there is at least one nonzero value visible to a convolutional kernel  the maxpooling would evaluate
the value at the position to 1.
Most multi-modal deep learning approaches simply concatenate or elementwisely add bottleneck
features. However  when the extracted semantics and range of feature value differs among elements 
direct concatenation and addition on multi-modal data source would not always yield better perfor-
mance than single-modal data source  as seen in [35  17]. To avoid this problem. We use encoders to
extract higher-level semantics from two branches. We propose 2D2CCA  detailed in 3.2  to ensure
the extracted features from two branches are maximally correlated. The intuition is that we want to
capture the same semantics from the RGB and depth domains. Next  we use a transformer network

3

Figure 2: Our network architecture. Here ⊕ is for concatenation operation. The input 0 - 1 sparse mask
represents the sparse pattern of depth measurements. The complementary mask is complementary to
the sparse mask. We separate a full RGB image into a sparse RGB and a complementary RGB by the
mask and feed them with masks into networks.

Figure 3: Our SAConv. The (cid:12) is for
Hadamard product. The ⊗ is for convo-
lution. The + is for elementwise addi-
tion. The kernel size is 3×3 and stride is
1 for both convolution and maxpooling.

Figure 4: Samples of sparsiﬁed depth maps for ex-
periments with different sparsiﬁers. (a) The source
dense depth map from NYUv2. (b) With uniform
sparsiﬁer (500 points). (c) With stereo sparsiﬁer
(500 points). (d) With ORB sparsiﬁer.

to transform extracted features from RGB domain to depth domain  making extracted features from
different sources share the same numerical range. During the training phase  we use features of sparse
depth and corresponding sparse RGB image to calculate the 2D2CCA loss and transformer loss.
We use a symmetric decoder structure to decode the embedded features. For the input  we concatenate
the sparse depth features with the reconstructed missing depth features. The reconstructed missing
depth features are extracted from complementary RGB image through the RGB encoder and the
transformer. To ensure single-stage training  we adopt weight-sharing strategies as shown in Figure 2.

Figure 5: The visualizations of FI and FD using an example from NYUv2 dataset. Visuals are heat
maps of the extracted features. Brighter color means a larger feature value. The values within a
single map were normalized to [0  1]. (a) The feed-forward heat maps at the ﬁrst iteration. (b) The
feed-forward heat maps after being trained with 10000 iterations. The ﬁgure shows the heat maps of
the ﬁrst 6 channels. The numbers under the heat maps represent the channel numbers. The example
demonstrates that the 2D2CCA is able to capture similar semantics from different sources.

4

3.2

2D Deep Canonical Correlation Analysis (2D2CCA)

Existing CCA based techniques introduced in Section 2 have limitations in modern computer vision
tasks. Since modern computer vision studies usually use very deep networks to extract information
from images of relatively large resolution  the batch size is limited by GPU-memory use. Meanwhile 
the latent feature representations in networks are high-dimensional  since the batch size is limited 
using DCCA with one-dimensional vector representation would lead to SSS problem. Therefore  We
propose a novel 2D deep canonical correlation analysis(2D2CCA) to overcome the limitations.
We denote the completed depth map as D with its corresponding RGB image as I. Sparse depth
map in the input and the corresponding sparse RGB image are denoted as sD and sI. RGB/Depth
encoders are denoted as fI and fD where the parameters of the encoders are denoted as θI and θD
respectively. As described in Section 3.1  fI and fD use the SAConv to propagate information from
reliable points to extract features from sparse inputs. We generate 3D feature grids embedding
pair ( FsD ∈ Rm×n×C  FsI ∈ Rm×n×C) for each sparse depth map/image pair (sD sI) by deﬁning
FsD = fD (sD;θD) and FsI = fI (sI;θI). Inside each feature grid pair  there are C feature map pairs
sI)  ∀i (cid:54)= j  we analyze the channelwise canonical
sD Fi
sI
result in getting features with similar semantic meanings for each modality  as shown in Figure 6 
which guides fI to embed more valuable information related to depth completion.
Using 1-dimensional feature representation would lead to SSS problem in modern deep learning
based computer vision task. We introduce the 2-dimensional approach similar to [32] to generate
full-rank covariance matrix ˆΣsD sI ∈ Rm×n  which is calculated as

sI ∈ Rm×n(cid:1)  ∀i < C  and C = 512 in our network. Rather than analyzing the global
(cid:1). This channelwise correlation analysis will

sD ∈ Rm×n Fi

(cid:0)Fi
correlation between the same channel number(cid:0)Fi

correlation between any possible pairs of (Fi

sD Fj

(cid:2)Fi
sD − E [FsD](cid:3)(cid:2)Fi

sI − E [FsI](cid:3)T

 

(1)

1
C

C−1
∑

i=0

ˆΣsD sI =
C ∑C−1

ˆΣsD =

1
C

C−1
∑

i=0

in which we deﬁne E [F] = 1
ˆΣsI) with the regularization constant r1 and identity matrix I as

i=0 Fi. Besides  we generate covariance matrices ˆΣsD (and respective

The correlation between FsD and FsI is calculated as
− 1
sD )( ˆΣsD sI)( ˆΣ
2

corr(FsD FsI) =

sD − E [FsD](cid:3)(cid:2)Fi
(cid:2)Fi
(cid:13)(cid:13)(cid:13)(cid:13)( ˆΣ

sD − E [FsD](cid:3)T
(cid:13)(cid:13)(cid:13)(cid:13)tr

− 1
2
sI )

.

+ r1I.

(2)

(3)

The higher value of corr(FsD FsI) represents the higher correlation between two feature blocks. Since
corr(FsD FsI) is an non-negative scalar  we use −corr(FsD FsI) as the optimization objective to guide
training of two feature encoders. To compute the gradient of corr(FsD FsI) with respect to θD and θI 
we can compute its gradient with respect to FsD and FsI and then do the back propagation. The detail
− 1
is showed following. Regarding to the gradient computation  we deﬁne M = ( ˆΣ
2
sI )
and decompose M as M = USVT using SVD decomposition. Then we deﬁne

− 1
sD )( ˆΣsD sI)( ˆΣ
2

∂corr(FsD FsI)

∂ FsI

=

1
C (2∇sDsDFsD + ∇sDsIFsI)  

(4)

− 1
sD UV T ˆΣ
where ∇sDsI = ˆΣ
2
calculations as ∂corr(FsD FsI )

− 1
sRGB and ∇sDsD = − 1
2
in Equation (4).

2

∂ FsI

− 1
sD UDU T ˆΣ
2

ˆΣ

− 1
sD . ∂corr(FsD FsI )
2

∂ FsD

follows the similar

3.3 Loss Function
We denote our channelwise 2D2CCA loss as L2D2CCA = −corr(FsD FsI). We denote the transformed
component from sparse RGB to depth domain as ˆFsD. The transformer loss describes the numerical
similarity between RGB and depth domain. We use L2 norm to measure the numerical similarity. Our
transformer loss is Ltrans = (cid:107)FsD − ˆFsD(cid:107)2
2.

5

We also build another encoder and another transformer network which share weights with the encoder
and transformer network for the spare RGB. The input of the encoder is a complementary RGB image.
We use features extracted from complementary RGB image to predict features of non-observable
depth using transformer network. For the complementary RGB image  we denote the extracted feature
and transformed component as FcI and ˆFcD. Later  we concatenate FsD and ˆFcD  both of which are
512-channel. We got an 1024-channel bottleneck feature on depth domain. We pass this bottleneck
feature into the decoder described in Section 3.1. The output from the decoder is a completed dense
depth map ˆD. To compare the inconsistency between the groundtruth Dgt and the completed depth
map  we use pixelwise L2 norm. Thus our reconstruction loss is Lrecon = (cid:107)Dgt − ˆD(cid:107)2
2.
Also  since bottleneck features have limited expressiveness  if the sparsity of inputs is severe  e.g. only
0.1% sampled points of the whole resolution  the completed depth maps usually have griding effects.
To resolve the griding effects  we introduce the smoothness term as in [36] into our loss function.
Lsmooth = (cid:107)∇2 ˆD(cid:107)1  where ∇2 denotes the second-order gradients. Our ﬁnal total loss function with
weights becomes

Ltotal = L2D2CCA + wtLtrans + wrLrecon + wsLsmooth.

(5)

4 Experiments

4.1 Dataset and Experiment Details

Implementation details. We use PyTorch to implement the network. Our encoders are similar to
VGG16  without the fully-connected layers. We use ReLU on extracted features after every SAConv
operation. Downsampling is applied to both the features and masks in encoders. The transformer
network is a 2-layer network  size 3× 3  stride 1  and 512-dimension  with our SAConv. The decoder
is also a VGG16-like network using deconvolution to upsample. We use SGD optimizer. We conclude
all the hyperparameter tuning in the supplemental material.
Datasets. We have done extensive experiments on outdoor scene datasets such as KITTI odometry
dataset [12] and Cityscape depth dataset[37]  and on indoor scene datasets such as NYUv2 [38] and
SLAM RGBD datasets as ICL_NUM [39] and TUM [40].

• KITTI dataset. The KITTI dataset contains both RGB and LiDAR measurements  total 22
sequences for autonomous driving use. We use the ofﬁcial split  where 46K images are for
training and 46K for testing. We adopt the same settings described in [4  41] which drops
the upper part of the images and resizes the images to 912×228.

• Cityscape dataset. The Cityscape dataset contains RGB and depth maps calculated from
stereo matching of outdoor scenes. We use the ofﬁcial training/validation dataset split. The
training set contains 23K images from 41 sequences and the testing set contains 3 sequences.
We center crop the images to the size of 900×335 to avoid the upper sky and lower car logo.
• NYUv2 dataset. The NYUv2 dataset contains 464 sequences of indoor RGB and depth
data using Kinect. We use the ofﬁcial dataset split and follow [4] to sample 50K images as
training data. The testing data contains 654 images.

• SLAM RGBD dataset. We use the sequences of ICL-NUIM[42] and TUM RGBD SLAM
datasets from stereo camera. [40]. The former is synthetic  and the latter was acquired with
Kinect. We use the same testing sequences as described in [1].

Sparsiﬁers. A sparsiﬁer describes the strategy of sampling the dense/semi-dense depth maps in the
dataset to make them become the sparse depth input for the training and evaluation purposes. We
deﬁne three sparsiﬁers to simulate different sparse patterns existing in the real-world applications.
Uniform sparsiﬁer uniformly samples the dense depth map  simulating the scanning effect caused by
LiDAR which is nearly uniform. Stereo sparsiﬁer only samples the depth measurements on the edge
or textured objects in the scene to simulate the sparse patterns generated by stereo matching or direct
VSLAM. ORB sparsiﬁer only maintains the depth measurements according to the location of ORB
features in the corresponding RGB images. ORB sparsiﬁer simulates the output sparse depth map
from feature based VSLAM. We set a sample number for uniform and stereo sparsiﬁers to control
the sparsity. Since the ORB feature number varies in different images  we do not predeﬁne a sample
number but take all the depth at the ORB feature positions.

6

Error metrics. We use the error metrics the same as in most previous works. (1) RMSE: root mean
square error (2) MAE: mean absolute error (3) δi: percentage of predicted pixels where the relative
error is within 1.25i. Most of related works adopt i = 1 2 3. RMSE and MAE both measure error in
meters in our experiments.
Ablation studies. To examine the effectiveness of multi-modal approach  we evaluate the network
performance using four types of inputs  i.e. (1) dense RGB images; (2) sparse depth; (3) dense RGB
image + sparse depth; (4) complementary RGB image + sparse depth. The evaluation results are
demonstrated in Table 1. We could observe that the networks with single-modal input perform worse
than those with multi-modal input  which validates our multi-modal design. Besides  we observe that

Table 1: Ablation study of using different data sources on NYUv2 dataset. Dense RGB means we
feed in full RGB images. sD means sparse depth generated by 100-point stereo sparsiﬁer . cRGB
means complementary RGB image.

Input data
Dense RGB
sD(100 pts)

Dense RGB+sD(100 pts)

cRGB+sD(100 pts)

MAE
0.576
0.524
0.479
0.473

RMSE
0.740
0.700
0.638
0.631

δ1
63.5
68.1
73.0
72.4

δ2
89.0
90.2
92.4
92.6

δ3
97.0
97.0
97.7
98.1

using dense RGB with sparse depth has similar but worse performance than using complementary
RGB with sparse depth. The sparse depth inputs are precise. However  if we extract RGB domain
features for the locations where we already have precise depth information  it would cause ambiguity
thus the performance is worse than using complementary RGB information. We also conduct ablation
studies for different loss combinations in our supplementary material on KITTI and NYUv2 dataset.
Furthermore  we conduct the ablation study with different sparsity on NYUv2 dataset. The stereo
sparsiﬁer is used to sample from dense depth maps to generate sparse depth data for training and
testing. We show how different sparsity can affect the predicted depth map quality. The results are in
Table 2.

Table 2: Ablation study of different sample numbers on NYUv2 using stereo sparsiﬁer.

Sample#

50
100
200
500
1000
2000
5000
10000

MAE
0.547
0.426
0.385
0.342
0.290
0.242
0.222
0.151

RMSE
0.715
0.580
0.531
0.476
0.419
0.352
0.323
0.231

δ1
65.5
77.5
80.9
83.0
87.0
91.3
93.3
96.6

δ2
90.1
94.1
95.1
96.1
97.0
98.2
98.9
99.5

δ3
97.4
98.4
98.7
99.0
99.2
99.6
99.8
99.9

4.2 Outdoor scene - KITTI odometry and Cityscapes

For KITTI and Cityscapes these two outdoor datasets  we use the uniform sparsiﬁer. For the KITTI
dataset  we sample 500 points as sparse depth the same as some previous works. We compare
with some state-of-the-art works  [4  43  41  44]. We follow the evaluation settings in these works 
randomly choose 3000 images to calculate the numerical results. The results are in Table 3. Next  we
conduct experiments using both KITTI and Cityscape datasets. Some monocular depth prediction
works use Cityscape dataset for training and KITTI dataset for testing. We choose this setting and
use 100 uniformly sampled sparse depth as inputs. The results are shown in Table 4.

4.3

Indoor scene - NYUv2 and SLAM RGBD datasets

For NYUv2 indoor scene dataset  we use the stereo sparsiﬁer to sample points. We compare to the
state-of-the-art [4] with different sparsity using their publicly released code. The results are shown in
Table 5.

7

Table 3: 500 points sparse depth completion on KITTI dataset.

Ma et al.[4]
SPN [43]
CSPN[41]

CSPN+UNet[41]

PnP[44]

CFCNet w/o smoothness
CFCNet w/ smoothness

MAE

-
-
-
-

1.024
1.233
1.197

RMSE
3.378
3.243
3.029
2.977
2.975
2.967
2.964

δ1
93.5
94.3
95.5
95.7
94.9
94.1
94.0

δ2
97.6
97.8
98.0
98.0
98.0
98.1
98.0

δ3
98.9
99.1
99.0
99.1
99.0
99.3
99.3

Table 4: Depth evaluation results: cap 50m means only taking the depth that smaller than 50m into
consideration when doing the evaluation. CS->K means we train the network on the Cityscape dataset
and we do the evaluation on the KITTI dataset. Comparing methods all train train/test with 100 pts.

Methods

Zhou et al. [36]
Godard et al. [45]
Aleotti et al. [46]
CFCNet(50 pts)
CFCNet(100 pts)

Zhou et al. [36](cap 50m)
CFCNet(50 pts  cap 50m)
CFCNet(100 pts  cap 50m)
CFCNet(50 pts  cap 50m)
CFCNet(100 pts  cap 50m)
CFCNet(100 pts  cap 50m)

Input
RGB
RGB
RGB

RGB+sD
RGB+sD

RGB

RGB+sD
RGB+sD
RGB+sD
RGB+sD
RGB+sD

Dataset
CS→K
CS→K
CS→K
CS →K
CS→K
CS→K
CS →K
CS →K
CS→CS
CS→CS
K→K

RMSE
7.580
14.445
14.051
7.841
5.827
6.148
6.334
4.524
9.019
6.887
3.157

δ 1
57.7
5.3
6.3
78.3
82.6
59.0
79.2
83.7
82.8
88.9
91.0

δ 2
84.0
32.6
39.4
92.7
94.7
85.2
93.2
95.2
94.1
96.1
97.1

δ 3
93.7
86.2
87.6
97.0
97.9
94.5
97.3
98.1
97.2
98.1
98.9

Table 5: Comparisons on NYUv2 dataset using stereo sparsiﬁer.

Sample#

100
100
200
200
500
500

Methods

CFCNet

CFCNet

[4]

[4]

[4]

CFCNet

MAE
0.473
0.426
0.451
0.385
0.384
0.342

RMSE
0.629
0.580
0.603
0.531
0.529
0.476

δ1
71.5
77.5
73.0
80.9
79.2
83.0

δ2
92.4
94.1
93.5
95.1
94.9
96.1

δ3
98.0
98.4
98.4
98.7
98.6
99.0

Next  we conduct experiments on SLAM RGBD dataset. We follow the setting in the state-of-the-art 
CNN-SLAM [1]  and do the cross-dataset evaluation. We train the model on NYUv2 using ORB
sparsiﬁer and evaluate on the SLAM RGBD dataset. We use the metric in CNN-SLAM  calculating
the percentage of accurate estimations. Accurate estimations mean the error is within ±10% of the
groundtruth. The results are in Table 6.

Table 6: Comparison in terms of percentage of correctly estimated depth on two SLAM RGBD
datasets  ICL-NUIM and TUM. (TUM/seq1 name: fr3/long ofﬁce household  TUM/seq2 name:
fr3/nostructure texture near withloop  TUM/seq3 name: fr3/structure texture far)

Sequence#
ICL/ofﬁce0
ICL/ofﬁce1
ICL/ofﬁce2
ICL/living0
ICL/living1
ICL/living2
TUM/seq1
TUM/seq2
TUM/seq3
Average

CFCNet
41.97
43.86
63.64
51.76
64.34
59.07
54.70
66.30
74.61
57.81

CNN-SLAM[1]

19.41
29.15
37.22
12.84
13.03
26.56
12.47
24.07
27.39
22.46

Laina[47]

17.19
20.83
30.63
15.00
11.44
33.01
12.98
15.41
9.45
18.44

Remode[48]

4.47
3.13
16.70
4.47
2.42
8.68
9.54
12.65
6.73
7.64

8

Figure 6: Visual results on KITTI dataset. (a) The RGB image (b) 500 points sparse depth as inputs.
(c) Our Completed depth maps. (d) Results from [4].
5 Conclusion

In this paper  we directly analyze the relationship between the sparse depth information and their
corresponding pixels in RGB images. To better fuse information  we propose 2D2CCA to ensure the
most similar semantics are captured from two branches and use the complementary RGB information
to complement the missing depth. Extensive experiments using total four outdoor/indoor scene
datasets show our results achieve state-of-the-art.

References
[1] Keisuke Tateno  Federico Tombari  Iro Laina  and Nassir Navab  “Cnn-slam: Real-time dense monocular
slam with learned depth prediction ” in IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2017  pp. 6243–6252.

[2] Weiyue Wang and Ulrich Neumann  “Depth-aware cnn for rgb-d segmentation ” in Proceedings of the

European Conference on Computer Vision (ECCV)  2018  pp. 135–150.

[3] Weiyue Wang  Qiangeng Xu  Duygu Ceylan  Radomir Mech  and Ulrich Neumann  “Disn: Deep implicit
surface network for high-quality single-view 3d reconstruction ” in Advances in Neural Information
Processing Systems (NeurIPS)  2019.

[4] Fangchang Ma and Sertac Karaman  “Sparse-to-dense: Depth prediction from sparse depth samples and a

single image ” in IEEE International Conference on Robotics and Automation (ICRA)  2018.

[5] Shreyas S. Shivakumar  Ty Nguyen  Steven W. Chen  and Camillo J. Taylor  “Dfusenet: Deep fusion of rgb
and sparse depth information for image guided dense depth completion ” arXiv preprint arXiv:1902.00761 
2019.

[6] Jonas Uhrig  Nick Schneider  Lukas Schneider  Uwe Franke  Thomas Brox  and Andreas Geiger  “Sparsity

invariant cnns ” in IEEE International Conference on 3D Vision (3DV)  2017  pp. 11–20.

[7] Jiaxiong Qiu  Zhaopeng Cui  Yinda Zhang  Xingdi Zhang  Shuaicheng Liu  Bing Zeng  and Marc Pollefeys 
“Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and
single color image ” arXiv preprint arXiv:1812.00488  2018.

[8] Zhao Chen  Vijay Badrinarayanan  Gilad Drozdov  and Andrew Rabinovich  “Estimating depth from rgb

and sparse sensing ” in European Conference on Computer Vision (ECCV)  2018  pp. 167–182.

[9] Yanchao Yang  Alex Wong  and Stefano Soatto  “Dense depth posterior (ddp) from single image and

sparse range ” arXiv preprint arXiv:1901.10034  2019.

[10] Yilun Zhang  Ty Nguyen  Ian D Miller  Steven Chen  Camillo J Taylor  Vijay Kumar  et al.  “Dﬁnenet:
Ego-motion estimation and depth reﬁnement from sparse  noisy depth input with rgb guidance ” arXiv
preprint arXiv:1903.06397  2019.

9

[11] David A Forsyth and Jean Ponce  “Computer vision: A modern approach ” 2003.

[12] Andreas Geiger  Philip Lenz  and Raquel Urtasun  “Are we ready for autonomous driving? the kitti vision
benchmark suite ” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2012  pp.
3354–3361.

[13] Raul Mur-Artal  Jose Maria Martinez Montiel  and Juan D Tardos  “Orb-slam: a versatile and accurate

monocular slam system ” IEEE Transactions on robotics  vol. 31  no. 5  pp. 1147–1163  2015.

[14] Yiyi Liao  Lichao Huang  Yue Wang  Sarath Kodagoda  Yinan Yu  and Yong Liu  “Parse geometry from a
line: Monocular depth estimation with partial laser observation ” in IEEE International Conference on
Robotics and Automation (ICRA)  2017  pp. 5059–5066.

[15] Yinda Zhang and Thomas Funkhouser  “Deep depth completion of a single rgb-d image ” in Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018  pp. 175–185.

[16] Saif Imran  Yunfei Long  Xiaoming Liu  and Daniel Morris  “Depth coefﬁcients for depth completion ”

arXiv preprint arXiv:1903.05421  2019.

[17] Maximilian Jaritz  Raoul De Charette  Emilie Wirbel  Xavier Perrotton  and Fawzi Nashashibi  “Sparse
and dense data with cnns: Depth completion and semantic segmentation ” IEEE International Conference
on 3D Vision (3DV)  2018.

[18] Harold Hotelling  “Relations between two sets of variates ” in Breakthroughs in statistics  pp. 162–190.

Springer  1992.

[19] TW Anderson  “An introduction to multivariate statistical analysis. ” 1984.

[20] Galen Andrew  Raman Arora  Jeff Bilmes  and Karen Livescu  “Deep canonical correlation analysis ” in

International Conference on Machine Learning (ICML)  2013  pp. 1247–1255.

[21] Weiran Wang  Raman Arora  Karen Livescu  and Jeff Bilmes  “On deep multi-view representation learning ”

in International Conference on Machine Learning (ICML)  2015  pp. 1083–1092.

[22] Changxing Ding and Dacheng Tao  “A comprehensive survey on pose-invariant face recognition ” ACM

Transactions on intelligent systems and technology  vol. 7  no. 3  pp. 37  2016.

[23] Xinghao Yang  Weifeng Liu  Dapeng Tao  and Jun Cheng  “Canonical correlation analysis networks for

two-view image recognition ” Information Sciences  vol. 385  pp. 338–352  2017.

[24] Weiran Wang  Raman Arora  Karen Livescu  and Jeff A Bilmes  “Unsupervised learning of acoustic
features via deep canonical correlation analysis ” in IEEE International Conference on Acoustics  Speech
and Signal Processing (ICASSP)  2015  pp. 4590–4594.

[25] Meina Kan  Shiguang Shan  and Xilin Chen  “Multi-view deep network for cross-view classiﬁcation ” in

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016  pp. 4847–4855.

[26] Fei Yan and Krystian Mikolajczyk  “Deep correlation for matching images and text ” in IEEE Conference

on Computer Vision and Pattern Recognition (CVPR)  2015  pp. 3441–3450.

[27] Youssef Mroueh  Etienne Marcheret  and Vaibhava Goel  “Deep multimodal learning for audio-visual
speech recognition ” in IEEE International Conference on Acoustics  Speech and Signal Processing
(ICASSP)  2015  pp. 2130–2134.

[28] Marie Katsurai and Shin’ichi Satoh  “Image sentiment analysis using latent correlations among visual 
textual  and sentiment views ” in IEEE International Conference on Acoustics  Speech and Signal
Processing (ICASSP)  2016  pp. 2837–2841.

[29] G Kukharev and E Kamenskaya  “Application of two-dimensional canonical correlation analysis for face
image processing and recognition ” Pattern Recognition and Image Analysis  vol. 20  no. 2  pp. 210–219 
2010.

[30] Cai-rong Zou  Ning Sun  Zhen-hai Ji  and Li Zhao  “2dcca: A novel method for small sample size face

recognition ” in IEEE Workshop on Applications of Computer Vision (WACV)  2007  pp. 43–43.

[31] Sun Ho Lee and Seungjin Choi  “Two-dimensional canonical correlation analysis ” IEEE Signal Processing

Letters  vol. 14  no. 10  pp. 735–738  2007.

10

[32] Jian Yang  David D Zhang  Alejandro F Frangi  and Jing-yu Yang  “Two-dimensional pca: a new approach
to appearance-based face representation and recognition ” IEEE Transactions on Pattern Analysis and
Machine Intelligence  2004.

[33] Ming Li and Baozong Yuan  “A novel statistical linear discriminant analysis for image matrix: two-
dimensional ﬁsherfaces ” in IEEE International Conference on Signal Processing  2004  vol. 2  pp.
1419–1422.

[34] Iasonas Kokkinos Adam W Harley  Konstantinos G. Derpanis  “Segmentation-aware convolutional
networks using local attention masks ” in IEEE International Conference on Computer Vision (ICCV) 
2017.

[35] Jiquan Ngiam  Aditya Khosla  Mingyu Kim  Juhan Nam  Honglak Lee  and Andrew Y Ng  “Multimodal

deep learning ” in International Conference on Machine Learning  2011  pp. 689–696.

[36] Tinghui Zhou  Matthew Brown  Noah Snavely  and David G Lowe  “Unsupervised learning of depth and
ego-motion from video ” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2017 
pp. 1851–1858.

[37] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo Benen-
son  Uwe Franke  Stefan Roth  and Bernt Schiele  “The cityscapes dataset for semantic urban scene
understanding ” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016  pp.
3213–3223.

[38] Nathan Silberman  Derek Hoiem  Pushmeet Kohli  and Rob Fergus  “Indoor segmentation and support
inference from rgbd images ” in European Conference on Computer Vision (ECCV)  2012  pp. 746–760.

[39] Ankur Handa  Thomas Whelan  John McDonald  and Andrew J Davison  “A benchmark for rgb-d visual
odometry  3d reconstruction and slam ” in IEEE International Conference on Robotics and automation
(ICRA)  2014  pp. 1524–1531.

[40] Jürgen Sturm  Nikolas Engelhard  Felix Endres  Wolfram Burgard  and Daniel Cremers  “A benchmark for
the evaluation of rgb-d slam systems ” in IEEE/RSJ International Conference on Intelligent Robots and
Systems  2012  pp. 573–580.

[41] Xinjing Cheng  Peng Wang  and Ruigang Yang  “Learning depth with convolutional spatial propagation

network ” European Conference on Computer Vision (ECCV)  2018.

[42] A. Handa  T. Whelan  J.B. McDonald  and A.J. Davison  “A benchmark for RGB-D visual odometry  3D
reconstruction and SLAM ” in IEEE International Conference on Robotics and Automation (ICRA)  2014.

[43] Sifei Liu  Shalini De Mello  Jinwei Gu  Guangyu Zhong  Ming-Hsuan Yang  and Jan Kautz  “Learning
afﬁnity via spatial propagation networks ” in Advances in Neural Information Processing Systems (NIPS) 
2017  pp. 1520–1530.

[44] Tsun-Hsuan Wang  Fu-En Wang  Juan-Ting Lin  Yi-Hsuan Tsai  Wei-Chen Chiu  and Min Sun  “Plug-
and-play: Improve depth estimation via sparse data propagation ” in IEEE International Conference on
Robotics and Automation (ICRA)  2019.

[45] Clément Godard  Oisin Mac Aodha  and Gabriel J Brostow  “Unsupervised monocular depth estimation
with left-right consistency ” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
2017  pp. 270–279.

[46] Filippo Aleotti  Fabio Tosi  Matteo Poggi  and Stefano Mattoccia  “Generative adversarial networks for
unsupervised monocular depth prediction ” in European Conference on Computer Vision (ECCV)  2018.

[47] Iro Laina  Christian Rupprecht  Vasileios Belagiannis  Federico Tombari  and Nassir Navab  “Deeper depth
prediction with fully convolutional residual networks ” in IEEE International Conference on 3D Vision
(3DV)  2016  pp. 239–248.

[48] Matia Pizzoli  Christian Forster  and Davide Scaramuzza  “Remode: Probabilistic  monocular dense
reconstruction in real time ” in IEEE International Conference on Robotics and Automation (ICRA)  2014 
pp. 2609–2616.

11

,Yiqi Zhong
Cho-Ying Wu
Suya You
Ulrich Neumann