2019,Quadratic Video Interpolation,Video interpolation is an important problem in computer vision  which helps overcome the temporal limitation of camera sensors. Existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation  which cannot well approximate the complex motion in the real world. To address these issues  we propose a quadratic video interpolation method which exploits the acceleration information in videos. This method allows prediction with curvilinear trajectory and variable velocity  and generates more accurate interpolation results. For high-quality frame synthesis  we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame. In addition  we present techniques for flow refinement. Extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets.,Quadratic Video Interpolation

Xiangyu Xu‚àó‚Ä†

Carnegie Mellon University
xuxiangyu2014@gmail.com

Li Siyao‚àó

SenseTime Research

Wenxiu Sun

SenseTime Research

lisiyao1@sensetime.com

sunwenxiu@sensetime.com

Qian Yin

Beijing Normal University

yinqian@bnu.edu.cn

Ming-Hsuan Yang

University of California  Merced Google

mhyang@ucmerced.edu

Abstract

Video interpolation is an important problem in computer vision  which helps
overcome the temporal limitation of camera sensors. Existing video interpolation
methods usually assume uniform motion between consecutive frames and use linear
models for interpolation  which cannot well approximate the complex motion in
the real world. To address these issues  we propose a quadratic video interpolation
method which exploits the acceleration information in videos. This method allows
prediction with curvilinear trajectory and variable velocity  and generates more
accurate interpolation results. For high-quality frame synthesis  we develop a Ô¨Çow
reversal layer to estimate Ô¨Çow Ô¨Åelds starting from the unknown target frame to the
source frame. In addition  we present techniques for Ô¨Çow reÔ¨Ånement. Extensive
experiments demonstrate that our approach performs favorably against the existing
linear models on a wide variety of video datasets.

1

Introduction

Video interpolation aims to synthesize intermediate frames between the original input images  which
can temporally upsample low-frame rate videos to higher-frame rates. It is a fundamental problem in
computer vision as it helps overcome the temporal limitations of camera sensors and can be used in
numerous applications  such as motion deblurring [5  35]  video editing [25  38]  virtual reality [1] 
and medical imaging [11].
Most state-of-the-art video interpolation methods [2  3  9  14  17] explicitly or implicitly assume
uniform motion between consecutive frames  where the objects move along a straight line at a
constant speed. As such  these approaches usually adopt linear models for synthesizing intermediate
frames. However  the motion in real scenarios can be complex and non-uniform  and the uniform
assumption may not always hold in the input videos  which often leads to inaccurate interpolation
results. Moreover  the existing models are mainly developed based on two consecutive frames for
interpolation  and the higher-order motion information of the video (e.g.  acceleration) has not been
well exploited. An effective frame interpolation algorithm should use additional input frames and
estimate the higher-order information for more accurate motion prediction.
To this end  we propose a quadratic video interpolation method to exploit additional input frames
to overcome the limitations of linear models. SpeciÔ¨Åcally  we develop a data-driven model which
integrates convolutional neural networks (CNNs) [13  24] and quadratic models [15] for accurate
motion estimation and image synthesis. The proposed algorithm is acceleration-aware  and thus

‚àóEqual contributions.
‚Ä†Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Exploiting the quadratic model for acceleration-aware video interpolation. The leftmost
subÔ¨Ågure shows four consecutive frames from a video  describing the projectile motion of a football.
The other three subÔ¨Ågures show the interpolated results between frame 0 and 1 by different algorithms.
Note that we overlap these results for better visualizing the interpolation trajectories. Since the linear
model [31] assumes uniform motion between the two frames  it does not approximate the movement
in real world well. In contrast  our quadratic approach can exploit the acceleration information from
the four neighboring frames and generate more accurate in-between video frames.

allows predictions with curvilinear trajectory and variable velocity. Although the ideas of our method
are intuitive and sensible  this task is challenging as we need to estimate the Ô¨Çow Ô¨Åeld from the
unknown target frame to the source frame (i.e.  backward Ô¨Çow) for image synthesis  which cannot
be easily obtained with existing approaches. To address this issue  we propose a Ô¨Çow reversal layer
to effectively convert forward Ô¨Çow to backward Ô¨Çow. In addition  we introduce new techniques for
Ô¨Åltering the estimated Ô¨Çow maps. As shown in Figure 1  the proposed quadratic model can better
approximate pixel motion in real world and thus obtain more accurate interpolation results.
The contributions of this work can be summarized as follows. First  we propose a quadratic inter-
polation algorithm for synthesizing accurate intermediate video frames. Our method exploits the
acceleration information of the video  which can better model the nonlinear movements in the real
world. Second  we develop a Ô¨Çow reversal layer to estimate the Ô¨Çow Ô¨Åeld from the target frame to
the source frame  thereby facilitating high-quality frame synthesis. In addition  we present novel
techniques for reÔ¨Åning Ô¨Çow Ô¨Åelds in the proposed method. We demonstrate that our method performs
favorably against the state-of-the-art video interpolation methods on different video datasets. While
we focus on quadratic functions in this work  the proposed framework for exploiting the acceleration
information is general  and can be further extended to higher-order models.

2 Related Work

Most state-of-the-art approaches [2  3  4  9  14  17  19] for video interpolation explicitly or implicitly
assume uniform motion between consecutive frames. As a typical example  Baker et al. [2] use
optical Ô¨Çow and forward warping to linearly move pixels to the intermediate frames. Liu et al.
[14] develop a CNN model to directly learn the uniform motion for interpolating the middle frame.
Similarly  Jiang et al. [9] explicitly assume uniform motion with Ô¨Çow estimation networks  which
enables a multi-frame interpolation model.
On the other hand  Meyer et al. [17] develop a phase-based method to combine the phase information
across different levels of a multi-scale pyramid  where the phase is modeled as a linear function of
time with the implicit uniform motion assumption. Since the above linear approaches do not exploit
higher-order information in videos  the interpolation results are less accurate.
Kernel-based algorithms [20  21  33] have also been proposed for frame interpolation. While these
methods are not constrained by the uniform motion model  existing schemes do not handle nonlinear
motion in complex scenarios well as only the visual information of two consecutive frames is used
for interpolation.
Closely related to our work is the method by McAllister and Roulier [15] which uses quadratic
splines for data interpolation to preserve the convexity of the input. However  this method can only
be applied to low-dimensional data  while we solve the problem of video interpolation which is in
much higher dimensions.

2

frame -1frame 0frame 1frame 2ground truthlinear modelquadratic modelFigure 2: Overview of the quadratic video interpolation algorithm. We Ô¨Årst use the off-the-shelf
model to estimate Ô¨Çow Ô¨Åelds for the input frames. Then we introduce quadratic Ô¨Çow prediction and
Ô¨Çow reversal layers to estimate ft‚Üí0 and ft‚Üí1. We describe the estimation process of ft‚Üí0 in details
in this paper  and ft‚Üí1 can be computed similarly. Finally  we synthesize the in-between frame by
warping and fusing the input frames with ft‚Üí0 and ft‚Üí1.

3 Proposed Algorithm
To synthesize an intermediate frame ÀÜIt where t ‚àà (0  1)  existing algorithms [9  14  21] usually
assume uniform motion between the two consecutive frames I0  I1  and adopt linear models for
interpolation. However  this assumption cannot approximate the complex motion in real world well
and often leads to inaccurately interpolated results. To solve this problem  we propose a quadratic
interpolation method for predicting more accurate intermediate frames. The proposed method is
acceleration-aware  and thus can better approximate real-world scene motion.
An overview of our quadratic interpolation algorithm is shown in Figure 2  where we synthesize the
frame ÀÜIt by fusing pixels warped from I0 and I1. We use I‚àí1  I0  and I1 to warp pixels from I0 and
describe this part in details in the following sections  and the warping from the other side (i.e.  I1)
can be performed similarly by using I0  I1  and I2. SpeciÔ¨Åcally  we Ô¨Årst compute optical Ô¨Çow f0‚Üí1
and f0‚Üí‚àí1 with the state-of-the-art Ô¨Çow estimation network PWC-Net [31]. We then predict the
intermediate Ô¨Çow map f0‚Üít using f0‚Üí1 and f0‚Üí‚àí1 in Section 3.1. In Section 3.2  we propose a
new method to estimate the backward Ô¨Çow ft‚Üí0 by reversing the forward Ô¨Çow f0‚Üít. Finally  we
synthesize the interpolated results with the backward Ô¨Çow in Section 3.3.

3.1 Quadratic Ô¨Çow prediction

To interpolate frame ÀÜIt  we Ô¨Årst consider the motion model of a pixel from I0:

(cid:90) t

(cid:20)

(cid:90) Œ∫

(cid:21)

f0‚Üít =

v0 +

aœÑ dœÑ

dŒ∫ 

(1)

0

0

where f0‚Üít denotes the displacement of the pixel from frame 0 to t  v0 is the velocity at frame 0 
and aœÑ represents the acceleration at frame œÑ.
Existing models [9  14  21] usually explicitly or implicitly assume uniform motion and set aœÑ = 0
between consecutive frames  where (1) can be rewritten as a linear function of t:

f0‚Üít = tf0‚Üí1.

(2)

However  the objects in real scenarios do not always travel in a straight line at a constant velocity .
Thus  these linear approaches cannot effectively model the complex non-uniform motion and often
lead to inaccurate interpolation results.
In contrast  we take higher-order information into consideration and assume a constant aœÑ for
œÑ ‚àà [‚àí1  1]. Correspondingly  the Ô¨Çow from frame 0 to t can be derived as:

f0‚Üít = (f0‚Üí1 + f0‚Üí‚àí1)/2 √ó t2 + (f0‚Üí1 ‚àí f0‚Üí‚àí1)/2 √ó t 
which is equivalent to temporally interpolating pixels with a quadratic function.

(3)

3

1012   IIII‚àíFlow estimationQuadratic flow predictionQuadratic flow predictionFlow reversalFlow reversalSynthesistIÔÄ§01f‚Üí‚àí01f‚Üí10f‚Üí12f‚Üí0tf‚Üí1tf‚Üí0tf‚Üí1tf‚ÜíFigure 3: Effectiveness of the Ô¨Çow reversal layer and the adaptive Ô¨Çow Ô¨Åltering. The car is moving
along the arrow direction in the frame sequence. (a) is the ft‚Üí0 estimated with the naive strategy
from [9]. (b) is the backward Ô¨Çow generated by our Ô¨Çow reversal layer. (c) and (d) represent the
results of the deep CNNs in [9] and our adaptive Ô¨Çow Ô¨Åltering  respectively.

This formulation relaxes the constraint of constant velocity and rectilinear movement of linear models 
and thus allows accelerated and curvilinear motion prediction between frames. In addition  existing
methods with linear models only use the two closest frames I0  I1  whereas our algorithm naturally
exploits visual information from more neighboring frames.

3.2 Flow reversal layer

While we obtain forward Ô¨Çow f0‚Üít from quadratic Ô¨Çow prediction  it cannot be easily used for
synthesizing images. Instead  we need backward Ô¨Çow ft‚Üí0 for high-quality frame synthesis [9  14 
16  31]. To estimate the backward Ô¨Çow  Jiang et al. [9] introduce a simple method which linearly
combines f0‚Üí1 and f1‚Üí0 to approximate ft‚Üí0. However  this approach does not perform well
around motion boundaries as shown in Figure 3(a). More importantly  this approach cannot be applied
in our quadratic method to exploit the acceleration information.
In this work  we propose a Ô¨Çow reversal layer for better prediction of ft‚Üí0. We Ô¨Årst project the Ô¨Çow
map f0‚Üít to frame t  where a pixel x on I0 corresponds to x + f0‚Üít(x) on It. Next  we compute
the Ô¨Çow of a pixel u on It by reversing and averaging the projected Ô¨Çow values that fall into the
neighborhood N (u) of pixel u. Mathematically  this process can be written as:

(cid:80)
(cid:80)
x+f0‚Üít(x)‚ààN (u) w((cid:107)x + f0‚Üít(x) ‚àí u(cid:107)2)(‚àíf0‚Üít(x))
x+f0‚Üít(x)‚ààN (u) w((cid:107)x + f0‚Üít(x) ‚àí u(cid:107)2)

ft‚Üí0(u) =

 

(4)

where w(d) = e‚àíd2/œÉ2 is the Gaussian weight for each Ô¨Çow. The proposed Ô¨Çow reversal layer is
conceptually similar to the surface splatting [39] in computer graphics where the optical Ô¨Çow in our
work is replaced by camera projection. During training  while the reversal layer itself does not have
learnable parameters  it is differentiable and allows the gradients to be backpropagated to the Ô¨Çow
estimation module in Figure 2  and thus enables end-to-end training of the whole system.
Note that the proposed reversal approach can lead to holes in the estimated Ô¨Çow map ft‚Üí0  which is
mostly due to the objects visible in It but occluded in I0. And the missing objects are Ô¨Ålled with the
pixels warped from I1 which is on the other side of the interpolation model.

3.3 Frame synthesis

In this section  we Ô¨Årst reÔ¨Åne the reversed Ô¨Çow Ô¨Åeld with adaptive Ô¨Åltering. Then we use the reÔ¨Åned
Ô¨Çow to generate the interpolated results with backward warping and frame fusion.

Adaptive Ô¨Çow Ô¨Åltering. While our approach is effective in reversing Ô¨Çow maps  the generated
backward Ô¨Çow ft‚Üí0 may still have some ringing artifacts around edges as shown in Figure 3(b) 
which are mainly due to outliers in the original estimations of the PWC-Net. A straightforward
way to reduce these artifacts [6  9  31] is to train deep CNNs with residual connections to reÔ¨Åne the
initial Ô¨Çow maps. However  this strategy does not work well in our practice as shown in Figure 3(c).
This is because the artifacts from the Ô¨Çow reversal layer are mostly thin streaks with spike values
(Figure 3(b)). Such outliers cannot be easily removed since the weighted averaging of convolution
can be affected by the spiky outliers.
Inspired by the median Ô¨Ålter [7] which samples only one pixel from a neighborhood and avoids the
issues of weighted averaging  we propose a Ô¨Çow Ô¨Åltering network to adaptively sample the Ô¨Çow

4

ùêº"(a)(b)(c)(d)moving directionmap for removing outliers. While the classical median Ô¨Ålter involves indifferentiable operation and
cannot be easily trained in our end-to-end model  the proposed method learns to sample one pixel in
a neighborhood with neural networks and can more effectively reduce the artifacts of the Ô¨Çow map.
SpeciÔ¨Åcally  we formulate the adaptive Ô¨Åltering process as follows:

f(cid:48)
t‚Üí0(u) = ft‚Üí0(u + Œ¥(u)) + r(u) 

(5)
where f(cid:48)
t‚Üí0 denotes the Ô¨Åltered backward Ô¨Çow  and Œ¥(u) is the learned sampling offset of pixel
u. We constrain Œ¥(u) ‚àà [‚àík  k] by using k √ó tanh(¬∑) as the activation function of Œ¥ such that the
proposed Ô¨Çow Ô¨Ålter has a local receptive Ô¨Åeld of 2k + 1. Since the Ô¨Çow map is sparse and smooth in
most regions  we do not directly rectify the artifacts with CNNs as the schemes in [6  9  31]. Instead 
we rely on the Ô¨Çow values around outliers by sampling in a neighborhood  where Œ¥ is trained to
Ô¨Ånd the suitable sampling locations. The residual map r is learned for further improvement. Our
Ô¨Åltering method enables spatially-variant and nonlinear reÔ¨Ånement of ft‚Üí0  which could be seen as a
learnable median Ô¨Ålter in spirit. As show in Figure 3(d)  the proposed algorithm can effectively reduce
the artifacts in the reversed Ô¨Çow maps. More implementation details are presented in Section 4.2.
Warping and fusing source frames. While we obtain f(cid:48)
we can also estimate f(cid:48)
video frames as:

t‚Üí0 with the input frames I‚àí1  I0  and I1 
t‚Üí1 in a similar way with I0  I1  and I2. Finally  we synthesize the intermediate
(1 ‚àí t)m(u)I0(u + f(cid:48)

t‚Üí0(u)) + t(1 ‚àí m(u))I1(u + f(cid:48)

t‚Üí1(u))

(1 ‚àí t)m(u) + t(1 ‚àí m(u))

ÀÜIt(u) =

(6)

(7)

where Ii(u + f(cid:48)
t‚Üíi(u)) denotes the pixel warped from frame i to t with bilinear function [8]. m
is a mask learned with a CNN to fuse the warped frames. Similar to [9]  we also use the temporal
distance 1 ‚àí t and t for the source frames I0 and I1  such that we can give higher conÔ¨Ådence to
temporally-closer pixels. Note that we do not directly use the pixels in I‚àí1 and I2 for image synthesis 
as almost all the contents in the intermediate frame can be found in I0 and I1. Instead  I‚àí1 and I2 are
exploited for acceleration-aware motion estimation.
Since all the above steps of our method are differentiable  we can train the proposed interpolation
model in an end-to-end manner. The loss function for training our network is a combination of the (cid:96)1
loss and perceptual loss [10  37]:

(cid:107) ÀÜIt ‚àí It(cid:107)1 + Œª(cid:107)œÜ( ÀÜIt) ‚àí œÜ(It)(cid:107)2 

where It is the ground truth  and œÜ is the conv4_3 feature extractor of the VGG16 model [28].

4 Experiments

In this section  we Ô¨Årst provide implementation details of the proposed model  including training data 
network structure  and hyper-parameters. We then present evaluation results of our algorithm with
comparisons to the state-of-the-art methods on video datasets. The source code  data  and the trained
models are available at: https://sites.google.com/view/xiangyuxu/qvi_nips19.

4.1 Training data

To train the proposed interpolation model  we collect high-quality videos from the Internet  where
each frame is of 1080√ó1920 pixels at the frame rate of 960 fps. From the collected videos  we
select the clips with both camera shake and dynamic object motion  which are beneÔ¨Åcial for more
effective network training. The Ô¨Ånal training dataset consists of 173 video clips of different scenes
and 36926 frames in total. In addition  the 960 fps video clips are randomly downsampled to 240 fps
and 480 fps for data augmentation. During the training process  we extract non-overlapped frame
groups from these video clips  where each has 4 input frames I‚àí1  I0  I1  I2  and 7 target frames It 
t = 0.125  0.25  . . .   0.875. We resize the frames into 360√ó640 and randomly crop 352√ó352 patches
for training. Image Ô¨Çipping and sequence reversal are also performed to fully utilize the video data.

4.2

Implementation details

We learn the adaptive Ô¨Çow Ô¨Åltering with a 23-layer U-Net [26  36] which is an encoder-decoder
network. The encoder is composed of 12 convolution layers with 5 average pooling layers for

5

Table 1: Quantitative evaluations on the GOPRO and Adobe240 datasets. ‚ÄúOurs w/o qua.‚Äù represents
our model without using the quadratic Ô¨Çow prediction.

GOPRO

Adobe240

whole

center

whole

center

Method

PSNR SSIM

IE

PSNR SSIM

IE

PSNR SSIM

IE

PSNR SSIM

IE

Phase
DVF
SepConv
SuperSloMo
Ours w/o qua.
Ours

23.95
21.94
29.52
29.00
29.57
31.27

0.700
0.776
0.922
0.918
0.923
0.948

17.89
21.30
9.26
9.51
9.02
7.23

22.05
20.55
27.69
27.33
27.86
29.62

0.620
0.720
0.895
0.892
0.898
0.929

22.08
25.14
11.38
11.50
10.93
8.73

25.60
28.23
32.19
31.30
31.64
32.95

0.735
0.896
0.954
0.949
0.952
0.966

16.93
11.76
7.71
8.18
7.93
6.84

23.65
26.90
30.87
30.17
30.48
32.09

0.647
0.871
0.940
0.935
0.939
0.959

20.65
13.30
8.91
9.22
8.96
7.47

t‚Üí1 with (5). Then we warp I0 and I1 with Ô¨Çow f(cid:48)

1  f0‚Üí1  f1‚Üí0  ft‚Üí0  and ft‚Üí1  where I t
t‚Üí0 and f(cid:48)

dowsampling  and the decoder has 11 convolution layers with 5 bilinear layers for upsampling.
We add skip connections with pixel-wise summation between the same-resolution layers in the
encoder and decoder to jointly use low-level and high-level features. The input of our network is
a concatenation of I0  I1  I t
0  I t
i (u) = Ii(u + ft‚Üíi(u))
denotes the pixel warped with ft‚Üíi. The U-Net produces the output Œ¥ and r which are used to
estimate the Ô¨Åltered Ô¨Çow map f(cid:48)
t‚Üí0 and
f(cid:48)
t‚Üí1  and feed the warped images to a 3-layer CNN to estimate the fusion mask m which is Ô¨Ånally
used for frame interpolation with (6).
We Ô¨Årst train the proposed network with the Ô¨Çow estimation module Ô¨Åxed for 200 epochs  and then
Ô¨Ånetune the whole system for another 40 epochs. Similar to [34]  we use the Adam optimizer [12] for
training. We initialize the learning rate as 10‚àí4 and further decrease it by a factor of 0.1 at the end of
the 100th and 150th epochs. The trade-off parameter Œª of the loss function (7) is set to be 0.005. k in
the activation function of Œ¥ is set to be 10. In the Ô¨Çow reversal layer  we set the Gaussian standard
deviation œÉ = 1.
For evaluation  we report PSNR  Structural Similarity Index (SSIM) [32]  and the interpolation
error (IE) [2] between the predictions and ground truth intermediate frames  where IE is deÔ¨Åned as
root-mean-squared (RMS) difference between the reference and interpolated image.

4.3 Comparison with the state-of-the-arts

We evaluate our model with the state-of-the-art video interpolation approaches  including the
phase-based method (Phase) [17]  separable adaptive convolution (SepConv) [21]  deep voxel Ô¨Çow
(DVF) [14]  and SuperSloMo [9]. We use the original implementations for Phase  SepConv  DVF and
the implementation from [22] for SuperSlomo. We retrain DVF and SuperSlomo with our data. We
were not able to retrain SepConv as the training code is not publicly available  and directly use the
original models [21] in our experiments instead. Note that the proposed quadratic video interpolation
can be used for synthesizing arbitrary intermediate frames  which is evaluated on the high frame
rate video datasets such as GOPRO [18] and Adobe240 [30]. We also conduct experiments on the
UCF101 [29] and DAVIS [23] datasets for performance evaluation of single-frame interpolation.

Multi-frame interpolation on the GOPRO [18] dataset. This dataset is composed of 33 high-
quality videos with a frame rate of 720 fps and image resolution of 720√ó1280. These videos are
recorded with hand-held cameras  which often contain non-linear camera motion. In addition  this
dataset has dynamic object motion from both indoor and outdoor scenes  which are challenging for
existing interpolation algorithms.
We extract 4275 non-overlapped frame sequences with a length of 25 from the GOPRO videos. To
evaluate the proposed quadratic model  we use the 1st  9th  17th  and 25th frames of each sequence
as our inputs  which respectively correspond to I‚àí1  I0  I1  I2 in the proposed model. As discussed
in Section 1  the baseline methods only exploit the 9th and 17th frames for video interpolation. We
synthesize 7 frames between the 9th and 17th frames  and thus all the corresponding ground truth
frames are available for evaluation.
As shown in Table 1  we separately evaluate the scores of the center frame (i.e.  the 4th frame 
denoted as center) and the average of all the 7 interpolated frames (denoted as whole). The quadratic
interpolation model consistently performs favorably against all the other linear methods. Noticeably 

6

frame 0 & frame 1

(a) GT

(b) SepConv

(c) SuperSloMo

(d) Ours w/o qua.

(e) Ours

Figure 4: Qualitative results on the GOPRO dataset. The Ô¨Årst row of each example shows the overlap
of the interpolated center frame and the ground truth. A clearer overlapped image indicates more
accurate interpolation result. The second row of each example shows the interpolation trajectory of
all the 7 interpolated frames by feature point tracking.

the PSNRs of our results on either the center frame or the average of the whole frames improve over
the second best method by more than 1 dB.
To understand the effectiveness of the proposed quadratic interpolation algorithm  we visualize
the trajectories of the interpolated results in Figure 4 and compare with the baseline methods both
qualitatively and quantitatively. SpeciÔ¨Åcally  for each test sequence from the GOPRO dataset  we use
the classic feature tracking algorithm [27] to select 10000 feature points in the 9th frame  and track
them through the 7 synthesized in-between frames. For better performance evaluation  we exclude
the points that disappear or move out of the image boundaries during tracking.
We show two typical examples in Figure 4 and visualize the interpolation trajectory by connecting
the tracking points (i.e.  the red lines). In the the Ô¨Årst example  the object moves along a quite sharp
curve  mostly due to a sudden violent change of the camera‚Äôs moving direction. All the existing
methods fail on this example as the linear models assume uniform motion and cannot predict the
motion change well. In contrast  our quadratic model enables higher-order video interpolation and
exploits the acceleration information from the neighboring frames. As shown in Figure 4(e)  the
proposed method approximates the curvilinear motion well against the ground truth.
In addition  we overlap the predicted center frame with its ground truth to evaluate the interpolation
accuracy of different methods (Ô¨Årst row of each example of Figure 4). For linear models  the
overlapped frames are severely blurred  which demonstrates the large shift between ground truth and
the linearly interpolated results. In contrast  the generated frames by our approach align with the
ground truth well  which indicates better interpolation results with smaller errors.
Different from the Ô¨Årst example which contains severe non-linear movements  we present a video
with motion trajectory closer to straight lines in the second example. As shown in Figure 4  although
the motion in this video is closer to the uniform assumption of linear models  existing approaches
still do not generate accurate interpolation results. This demonstrates the importance of the proposed
quadratic algorithm  since there are few scenes strictly satisfying uniform motion  and minor per-
turbations to this strict motion assumption can lead to obvious shifts in the synthesized images. As
shown in the second example of Figure 4(e)  the proposed quadratic method estimates the moving
trajectory well against the ground truth and thus generates more accurate interpolation results.

7

Table 2: ASFP on the GOPRO dataset.

Table 3: Evaluations on the UCF101 and DAVIS datasets.

Method

whole center

SepConv
SuperSloMo
Ours w/o qua.
Ours

1.79
2.04
1.33
0.97

2.17
2.38
1.69
1.22

UCF101

DAVIS

Method

PSNR SSIM IE

PSNR SSIM IE

Phase
DVF
SepConv
SuperSloMo
Ours w/o qua.
Ours

29.84
29.88
31.97
32.04
32.02
32.54

0.900
0.916
0.943
0.945
0.945
0.948

7.97
7.66
5.89
5.99
5.99
5.79

21.54
22.24
26.21
25.76
26.83
27.73

0.556
0.742
0.857
0.850
0.874
0.894

26.76
23.66
15.84
15.93
13.69
12.32

frame 1

frame 2

GT

SepConv
Figure 5: Visual results from the DAVIS dataset.

Phase

DVF

SuperSloMo

Ours

In addition  if we do not consider the acceleration in the proposed method (i.e.  using (2) to replace
(3))  the interpolation performance of our model decreases drastically to that of linear models (‚ÄúOurs
w/o qua.‚Äù in Table 1 and Figure 4)  which shows the importance of the higher-order information.
To quantitatively measure the shifts between the synthesized frames and ground truth  we deÔ¨Åne a
new error metric for video interpolation denoted as average shift of feature points (ASFP):

N(cid:88)

i=1

ASF P (It  ÀÜIt) =

1
N

(cid:107)p(It  i) ‚àí p( ÀÜIt  i)(cid:107)2 

(8)

where p(It  i) denotes the position of the ith feature point on It  and N is the number of feature
points. We respectively compute the average ASFP of the center frame and the whole 7 interpolated
frames on the GOPRO dataset. Table 2 shows the proposed quadratic algorithm performs favorably
against the state-of-the-art methods while signiÔ¨Åcantly reducing the average shift.

Evaluations on the Adobe240 [30] dataset. This dataset consists of 133 videos with a frame rate
of 240 fps and image resolution of 720√ó1280 pixels. The frames are resized to 360√ó480 during
testing. We extract 8702 non-overlapped frame sequences from the videos in the Adobe240 dataset 
and each sequence contain 25 consecutive frames similar with the settings of the GOPRO dataset.
We also synthesize 7 in-between frames for 8 times temporally upsampling. As shown in Table 1 
the proposed quadratic algorithm performs favorably against the state-of-the-art linear interpolation
methods.

Single-frame interpolation on the UCF101 [29] and DAVIS [23] datasets.
In addition to the
multi-frame interpolation evaluated on high frame rate videos  we test the proposed quadratic model
on single-frame interpolation using videos with 30 fps  i.e.  UCF101 [29] and DAVIS [23] datasets.
Liu et al. [14] previously extract 100 triplets from the videos of the UCF101 dataset as test data 
which cannot be used to evaluate our algorithms since we need four consecutive frames as inputs.
Thus  we re-generate the test data by Ô¨Årst temporally downsampling the original videos to 15 fps and
then randomly extracting 4 adjacent frames (i.e.  I‚àí1  I0  I1  I2) from these videos. The sequences
with static scenes are removed for more accurate evaluations. We collect 100 quintuples (4 input
frames I‚àí1  I0  I1  I2 and 1 target frame I0.5) where each frame is resized to 225√ó225 pixels as [14].
For the DAVIS dataset  we evaluate our method on the whole 90 video clips which are divided into
2847 quintuples using the original image resolution.
We interpolate the center frame for these two dataset  which is equivalent to converting a 15 fps video
to a 30 fps one. As shown in Table 3  the quadratic interpolation approach performs slightly better
than the baseline models on the UCF101 dataset as the videos are of relatively low quality with low
image resolution and slow motion. For the DAVIS dataset which contains complex motion from
both camera shake and dynamic scenes  our method signiÔ¨Åcantly outperforms other approaches in
terms of all evaluation metrics. We show one example from the DAVIS dataset in Figure 5 for visual
comparisons.

8

(a) ft‚Üí0

(b) f(cid:48)

t‚Üí0

(c) Ours w/o ada.

(d) Ours

Figure 6: Adaptive Ô¨Çow Ô¨Åltering reduces artifacts in (a) and generates higher-quality image (d).

Overall  the quadratic approach achieves state-of-the-art performance on a wide variety of video
datasets for both single-frame and multi-frame interpolations. More importantly  experimental results
demonstrate that it is important and effective to exploit the acceleration information for accurate
video frame interpolation.

4.4 Ablation study

Method

PSNR SSIM IE

Table 4: Ablation study on the DAVIS dataset.

We analyze the contribution of each component in
our model on the DAVIS video dataset [23] in Ta-
ble 4. In particular  we study the impact of quadratic
interpolation by replacing the quadratic Ô¨Çow predic-
tion (3) with the linear function (2) (w/o qua.). We
further study the effectiveness of the adaptive Ô¨Çow
Ô¨Åltering by directly learning residuals for Ô¨Çow reÔ¨Åne-
ment similar with [6  9  31] (w/o ada.). In addition  we compare the Ô¨Çow reversal layer with the linear
combination strategy in [9] which approximates ft‚Üí0 by simply fusing f0‚Üí1 and f1‚Üí0 (w/o rev.).
As shown in Table 4  removing each of the three components degrades performance in all metrics.
Particularly  the quadratic Ô¨Çow prediction plays a crucial role  which veriÔ¨Åes our approach to exploit
the acceleration information from additional neighboring frames. Note that while the quantitative
improvement from the adaptive Ô¨Çow Ô¨Åltering is small  this component is effective in generating
high-quality interpolation results by reducing artifacts of the Ô¨Çow Ô¨Åelds as shown in Figure 6.

Ours w/o rev.
Ours w/o qua.
Ours w/o ada.
Ours full model

0.873
0.874
0.892
0.894

26.71
26.83
27.60
27.73

13.84
13.69
12.41
12.32

5 Conclusion

In this paper we propose a quadratic video interpolation algorithm which can synthesize high-quality
intermediate frames. This method exploits the acceleration information from neighboring frames of
a video for non-linear video frame interpolation  and facilitates end-to-end training. The proposed
method is able to model complex motion in real world more accurately and generate more favorable
results than existing linear models on different video datasets. While we focus on quadratic function in
this work  the proposed formulation is general and can be extended to even higher-order interpolation
methods  e.g.  the cubic model. We also expect this framework to be applied to other related tasks 
such as multi-frame optical Ô¨Çow and novel view synthesis.

References
[1] R. Anderson  D. Gallup  J. T. Barron  J. Kontkanen  N. Snavely  C. Hern√°ndez  S. Agarwal  and S. M. Seitz.

Jump: virtual reality video. ACM Transactions on Graphics (TOG)  35:198  2016. 1

[2] S. Baker  D. Scharstein  J. Lewis  S. Roth  M. J. Black  and R. Szeliski. A database and evaluation

methodology for optical Ô¨Çow. IJCV  92:1‚Äì31  2011. 1  2  6

[3] W. Bao  W.-S. Lai  X. Zhang  Z. Gao  and M.-H. Yang. Memc-net: Motion estimation and motion
compensation driven neural network for video interpolation and enhancement. arXiv:1810.08768  2018. 1 
2

[4] J. L. Barron  D. J. Fleet  and S. S. Beauchemin. Performance of optical Ô¨Çow techniques. IJCV  12:43‚Äì77 

[5] T. Brooks and J. T. Barron. Learning to synthesize motion blur. In CVPR  2019. 1
[6] Y. Gan  X. Xu  W. Sun  and L. Lin. Monocular depth estimation with afÔ¨Ånity  vertical pooling  and label

1994. 2

2015. 5

enhancement. In ECCV  2018. 4  5  9

[7] R. C. Gonzalez and R. E. Woods. Digital image processing. Prentice hall New Jersey  2002. 4
[8] M. Jaderberg  K. Simonyan  A. Zisserman  and K. Kavukcuoglu. Spatial transformer networks. In NIPS 

9

[9] H. Jiang  D. Sun  V. Jampani  M. Yang  E. G. Learned-Miller  and J. Kautz. Super slomo: High quality

estimation of multiple intermediate frames for video interpolation. In CVPR  2018. 1  2  3  4  5  6  9

[10] J. Johnson  A. Alahi  and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In

ECCV  2016. 5

[11] A. Karargyris and N. Bourbakis. Three-dimensional reconstruction of the digestive wall in capsule
endoscopy videos using elastic video interpolation. IEEE Transactions on Medical Imaging  30:957‚Äì971 
2010. 1

[12] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR  2014. 6
[13] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86:2278‚Äì2324  1998. 1

[14] Z. Liu  R. Yeh  X. Tang  Y. Liu  and A. Agarwala. Video frame synthesis using deep voxel Ô¨Çow. In ICCV 

[15] D. F. McAllister and J. A. Roulier. Interpolation by convex quadratic splines. Mathematics of Computation 

[16] S. Meister  J. Hur  and S. Roth. UnÔ¨Çow: Unsupervised learning of optical Ô¨Çow with a bidirectional census

[17] S. Meyer  O. Wang  H. Zimmer  M. Grosse  and A. Sorkine-Hornung. Phase-based frame interpolation for

2017. 1  2  3  4  6  8

32:1154‚Äì1162  1978. 1  2

loss. In AAAI  2018. 4

video. In CVPR  2015. 1  2  6

deblurring. In CVPR  2017. 6

[18] S. Nah  T. H. Kim  and K. M. Lee. Deep multi-scale convolutional neural network for dynamic scene

[19] S. Niklaus and F. Liu. Context-aware synthesis for video frame interpolation. In CVPR  2018. 2
[20] S. Niklaus  L. Mai  and F. Liu. Video frame interpolation via adaptive convolution. In CVPR  2017. 2
[21] S. Niklaus  L. Mai  and F. Liu. Video frame interpolation via adaptive separable convolution. In ICCV 

[22] A. Paliwal. Pytorch implementation of super slomo. https://github.com/avinashpaliwal/Super-SloMo  2018.

2017. 2  3  6

6

[23] F. Perazzi  J. Pont-Tuset  B. McWilliams  L. Van Gool  M. Gross  and A. Sorkine-Hornung. A benchmark

dataset and evaluation methodology for video object segmentation. In CVPR  2016. 6  8  9

[24] W. Ren  S. Liu  L. Ma  Q. Xu  X. Xu  X. Cao  J. Du  and M.-H. Yang. Low-light image enhancement via a

deep hybrid network. TIP  28:4364‚Äì4375  2019. 1

[25] W. Ren  J. Zhang  X. Xu  L. Ma  X. Cao  G. Meng  and W. Liu. Deep video dehazing with semantic

segmentation. TIP  28:1895‚Äì1908  2018. 1

[26] O. Ronneberger  P. Fischer  and T. Brox. U-net: Convolutional networks for biomedical image segmentation.

[27] J. Shi and C. Tomasi. Good features to track. In CVPR  1994. 7
[28] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

In MICCAI  2015. 5

ICLR  2015. 5

[29] K. Soomro  A. R. Zamir  and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the

[30] S. Su  M. Delbracio  J. Wang  G. Sapiro  W. Heidrich  and O. Wang. Deep video deblurring for hand-held

[31] D. Sun  X. Yang  M.-Y. Liu  and J. Kautz. Pwc-net: Cnns for optical Ô¨Çow using pyramid  warping  and

wild. arXiv:1212.0402  2012. 6  8

cameras. In CVPR  2017. 6  8

cost volume. In CVPR  2018. 2  3  4  5  9

[32] Z. Wang  A. C. Bovik  H. R. Sheikh  and E. P. Simoncelli. Image quality assessment: from error visibility

to structural similarity. TIP  13:600‚Äì612  2004. 6

[33] X. Xu  M. Li  and W. Sun. Learning deformable kernels for image and video denoising. arXiv:1904.06903 

[34] X. Xu  Y. Ma  and W. Sun. Towards real scene super-resolution with raw images. In CVPR  2019. 6
[35] X. Xu  J. Pan  Y.-J. Zhang  and M.-H. Yang. Motion blur kernel estimation via deep learning. TIP 

2019. 2

27:194‚Äì205  2017. 1

[36] X. Xu  D. Sun  S. Liu  W. Ren  Y.-J. Zhang  M.-H. Yang  and J. Sun. Rendering portraitures from monocular

camera and beyond. In ECCV  2018. 5

images. In ICCV  2017. 5

[37] X. Xu  D. Sun  J. Pan  Y. Zhang  H. PÔ¨Åster  and M.-H. Yang. Learning to super-resolve blurry face and text

[38] C. L. Zitnick  S. B. Kang  M. Uyttendaele  S. Winder  and R. Szeliski. High-quality video view interpolation

using a layered representation. ACM transactions on graphics (TOG)  23:600‚Äì608  2004. 1

[39] M. Zwicker  H. PÔ¨Åster  J. Van Baar  and M. Gross. Surface splatting. In Proceedings of the 28th annual

conference on Computer graphics and interactive techniques  2001. 4

10

,Xiangyu Xu
Li Siyao
Wenxiu Sun
Qian Yin
Ming-Hsuan Yang