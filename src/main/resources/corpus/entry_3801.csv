2011,Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels,We consider regularized risk minimization in a large dictionary of Reproducing  kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation.  This setting  commonly referred to as Sparse Multiple Kernel Learning  (MKL)  may be viewed as the non-parametric extension of group sparsity in linear  models. While the two dominant algorithmic strands of sparse learning  namely  convex relaxations using l1 norm (e.g.  Lasso) and greedy methods (e.g.  OMP)   have both been rigorously extended for group sparsity  the sparse MKL literature  has so farmainly adopted the former withmild empirical success. In this paper  we  close this gap by proposing a Group-OMP based framework for sparse multiple  kernel learning. Unlike l1-MKL  our approach decouples the sparsity regularizer  (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms)  which leads to better empirical performance as well as a simpler optimization  procedure that only requires a black-box single-kernel solver. The algorithmic  development and empirical studies are complemented by theoretical analyses in  terms of Rademacher generalization bounds and sparse recovery conditions analogous  to those for OMP [27] and Group-OMP [16].,Non-parametric Group Orthogonal Matching Pursuit

for Sparse Learning with Multiple Kernels

Vikas Sindhwani and Aur´elie C. Lozano

IBM T.J. Watson Research Center

Yorktown Heights  NY 10598

{vsindhw aclozano}@us.ibm.com

Abstract

We consider regularized risk minimization in a large dictionary of Reproducing
kernel Hilbert Spaces (RKHSs) over which the target function has a sparse repre-
sentation. This setting  commonly referred to as Sparse Multiple Kernel Learning
(MKL)  may be viewed as the non-parametric extension of group sparsity in linear
models. While the two dominant algorithmic strands of sparse learning  namely
convex relaxations using l1 norm (e.g.  Lasso) and greedy methods (e.g.  OMP) 
have both been rigorously extended for group sparsity  the sparse MKL literature
has so far mainly adopted the former with mild empirical success. In this paper  we
close this gap by proposing a Group-OMP based framework for sparse MKL. Un-
like l1-MKL  our approach decouples the sparsity regularizer (via a direct l0 con-
straint) from the smoothness regularizer (via RKHS norms)  which leads to better
empirical performance and a simpler optimization procedure that only requires a
black-box single-kernel solver. The algorithmic development and empirical stud-
ies are complemented by theoretical analyses in terms of Rademacher generaliza-
tion bounds and sparse recovery conditions analogous to those for OMP [27] and
Group-OMP [16].

1

Introduction

Kernel methods are widely used to address a variety of learning problems including classiﬁcation  re-
gression  structured prediction  data fusion  clustering and dimensionality reduction [22  23]. How-
ever  choosing an appropriate kernel and tuning the corresponding hyper-parameters can be highly
challenging  especially when little is known about the task at hand. In addition  many modern prob-
lems involve multiple heterogeneous data sources (e.g. gene functional classiﬁcation  prediction of
protein-protein interactions) each necessitating the use of a different kernel. This strongly suggests
avoiding the risks and limitations of single kernel selection by considering ﬂexible combinations of
multiple kernels. Furthermore  it is appealing to impose sparsity to discard noisy data sources. As
several papers have provided evidence in favor of using multiple kernels (e.g. [19  14  7])  the mul-
tiple kernel learning problem (MKL) has generated a large body of recent work [13  5  24  33]  and
become the focal point of the intersection between non-parametric function estimation and sparse
learning methods traditionally explored in linear settings.

Given a convex loss function  the MKL problem is usually formulated as the minimization of em-
pirical risk together with a mixed norm regularizer  e.g.  the square of the sum of individual RKHS
norms  or variants thereof  that have a close relationship to the Group Lasso criterion [30  2]. Equiv-
alently  this formulation may be viewed as simultaneous optimization of both the non-negative con-
vex combination of kernels  as well as prediction functions induced by this combined kernel. In
constraining the combination of kernels  the l1 penalty is of particular interest as it encourages spar-
sity in the supporting kernels  which is highly desirable when the number of kernels considered is
large. The MKL literature has rapidly evolved along two directions: one concerns scalability of op-

1

timization algorithms beyond the early pioneering proposals based on Semi-deﬁnite programming
or Second-order Cone programming [13  5] to simpler and more efﬁcient alternating optimization
schemes [20  29  24]; while the other concerns the use of lp norms [10  29] to construct complex
non-sparse kernel combinations with the goal of outperforming 1-norm MKL which  as reported in
several papers  has demonstrated mild success in practical applications.

The class of Orthogonal Matching Pursuit techniques has recently received considerable attention  as
a competitive alternative to Lasso. The basic OMP algorithm originates from the signal-processing
community and is similar to forward greedy feature selection  except that it performs re-estimation
of the model parameters in each iteration  which has been shown to contribute to improved accuracy.
For linear models  some strong theoretical performance guarantees and empirical support have been
provided for OMP [31] and its extension for variable group selection  Group-OMP [16]. In particular
it was shown in [25  9] that OMP and Lasso exhibit competitive theoretical performance guarantees.
It is therefore desirable to investigate the use of Matching Pursuit techniques in the MKL framework
and whether one may be able to improve upon existing MKL methods.

Our contributions in this paper are as follows. We propose a non-parametric kernel-based extension
to Group-OMP [16]. In terms of the feature space (as opposed to function space) perspective of
kernel methods  this allows Group-OMP to handle groups that can potentially contain inﬁnite fea-
tures. By adding regularization in Group-OMP  we allow it to handle settings where the sample size
might be smaller than the number of features in any group. Rather than imposing a mixed l1/RKHS-
norm regularizer as in group-Lasso based MKL  a group-OMP based approach allows us to consider
the exact sparse kernel selection problem via l0 regularization instead. Note that in contrast to the
group-lasso penalty  the l0 penalty by itself has no effect on the smoothness of each individual com-
ponent. This allows for a clear decoupling between the role of the smoothness regularizer (namely 
an RKHS regularizer) and the sparsity regularizer (via the l0 penalty). Our greedy algorithms allow
for simple and ﬂexible optimization schemes that only require a black-box solver for standard learn-
ing algorithms. In this paper  we focus on multiple kernel learning with Regularized least squares
(RLS). We provide a bound on the Rademacher complexity of the hypothesis sets considered by
our formulation. We derive conditions analogous to OMP [27] and Group-OMP [16] to guarantee
the “correctness” of kernel selection. We close this paper with empirical studies on simulated and
real-world datasets that conﬁrm the value of our methods.

2 Learning Over an RKHS Dictionary

In this section  we setup some notation and give a brief background before introducing our main
objective function and describing our algorithm in the next section. Let H1 . . .HN be a collection
of Reproducing Kernel Hilbert Spaces with associated Kernel functions k1 . . . kN deﬁned on the
input space X ⊂ Rd. Let H denote the sum space of functions 

N

H = H1 ⊕ H2 . . . ⊕ HN = {f : X 7→ R|f (x) =

Let us equip this space with the following lp norms 

kfklp(H) = inf





N

Xj=1

kfjkp

Hj


1
p

: f (x) =

N

Xj=1

fj(x)  x ∈ X   fj ∈ Hj  j = 1 . . . N}

Xj=1
fj(x)  x ∈ X   fj ∈ Hj  j = 1 . . . N


(1)

It is now natural to consider a regularized risk minimization problem over such a RKHS dictionary 
given a collection of training examples {xi  yi}l

i=1 

arg min

f ∈H

1
l

l

Xi=1

V (yi  f (xi)) + λkfk2

lp(H)

(2)

where V (· ·) is a convex loss function such as squared loss in the Regularized Least Squares (RLS)
algorithm or the hinge loss in the SVM method. If this problem again has elements of an RKHS
structure  then  via the Representer Theorem  it can again be reduced to a ﬁnite dimensional problem
and efﬁciently solved.

2

Let q = p

2−p and let us deﬁne the q-convex hull of the set of kernel functions to be the following 

coq(k1 . . . kN ) =


kγ : X × X 7→ R | kγ (x  z) =

γjkj(x  z) 

N

Xj=1

N

Xj=1

γq

j = 1  γj ≥ 0


where γ ∈ RN . It is easy to see that the non-negative combination of kernels  kγ  is itself a valid
kernel with an associated RKHS Hkγ . With this deﬁnition  [17] show the following 

kfklp(H) = inf

γ nkfkHkγ

  kγ ∈ coq(k1 . . . kN )o

(3)

This relationship connects Tikhonov regularization with lp norms over H to regularization over
RKHSs parameterized by the kernel functions kγ. This leads to a large family of “multiple kernel
learning” algorithms (whose variants are also sometimes referred to as lq-MKL) where the basic
idea is to solve an equivalent problem 

arg min

f ∈Hkγ  γ∈△q

1
l

l

Xi=1

V (yi  f (xi)) + λkfk2

Hkγ

(4)

j=1γj ≥ 0}. For a ﬁxed γ  the optimization over f ∈ Hkγ is
where △q = {γ ∈ RN : kγkq = 1 ∀n
recognizable as an RKHS problem for which a standard black box solver may be used. The weights
γ may then optimized in an alternating minimization scheme  although several other optimization
procedures are also be used (see e.g. 
[4]). The case where p = 1 is of particular interest in
the setting when the size of the RKHS dictionary is large but the unknown target function can be
approximated in a much smaller number of RKHSs. This leads to a large family of sparse multiple
kernel learning algorithms that have a strong connection to the Group Lasso [2  20  29].

3 Multiple Kernel Learning with Group Orthogonal Matching Pursuit

Let us recall the l0 pseudo-norm  which is the cardinality of the sparsest representation of f in the

dictionary  kfkl0(H) = min{|J| : f = Pj∈J fj}. We now pose the following exact sparse kernel

selection problem 

arg min

f ∈H

1
l

l

Xi=1

V (yi  f (xi)) + λkfk2

l2(H)

subject to kfkl0(H) ≤ s

(5)

It is important to note the following: when using a dictionary of universal kernels  e.g.  Gaussian
l2(H) is critical (i.e. 
kernels with different bandwidths  the presence of the regularization term kfk2
λ > 0) since otherwise the labeled data can be perfectly ﬁt by any single kernel. In other words  the
kernel selection problem is ill-posed. While conceptually simple  our formulation is quite different
l2(H) penalty) is
from those proposed earlier since the role of a smoothness regularizer (via the kfk2
decoupled from the role of a sparsity regularizer (via the constraint on kfkl0(H) ≤ s). Moreover  the
latter is imposed directly as opposed through a p = 1 penalty making the spirit of our approach closer
to Group Orthogonal Matching Pursuit (Group-OMP [16]) where groups are formed by very high-
dimensional (inﬁnite for Gaussian kernels) feature spaces associated with the kernels. It has been
observed in recent work [10  29] on l1-MKL that sparsity alone does not lead it to improvements in
real-world empirical tasks and hence several methods have been proposed to explore lq-norm MKL
with q > 1 in Eqn. 4  making MKL depart away from sparsity in kernel combinations. By contrast 
we note that as q → ∞  p → 2. Our approach gives a direct knob both on smoothness (via λ)
and sparsity (via s) with a solution path along these dimensions that differs from that offered by
Group-Lasso based lq-MKL as q is varied. By combining l0 pseudo-norm with RKHS norms  our
method is conceptually reminiscent of the elastic net [32] (also see [26  12  21]). If kernels arise
from different subsets of input variables  our approach is also related to sparse additive models [18].

Our algorithm  MKL-GOMP  is outlined below for regularized least squares. Extensions for other
loss functions  e.g.  hinge loss for SVMs  can also be similarly derived. In the description of the algo-
rithm  our notation is as follows: For any function f belonging to an RKHS Fk with kernel function
l Pl
k(· ·)  we denote the regularized objective function as  Rλ(f  y) = 1
i=1(yi−f (xi))2 +λkfkFk

3

where k · kF denotes the RKHS norm. Recall that the minimizer f ⋆ = arg minf ∈F Rλ(f  y) is
given by solving the linear system  α = (K + λlI)−1y where K is the gram matrix of the ker-
nel on the labeled data  and by setting f ⋆(x) = Pl
i=1 αik(x  xi). Moreover  the objective value
achieved by the minimizer is: Rλ(f ⋆  y) = λyT (K + λlI)−1y. Note that MKL-GOMP should
not be confused with Kernel Matching Pursuit [28] whose goal is different: it is designed to spar-
sify α in a single-kernel setting. The MKL-GOMP procedure iteratively expands the hypothesis
space  HG(1) ⊆ HG(2) . . . ⊆ HG(i)  by greedily selecting kernels from a given dictionary  where
G(i) ⊂ {1 . . . N} is a subset of indices and HG = Sj∈G Hj. Note that each HG is an RKHS with
kernelPj∈G kj (see Section 6 in [1]). The selection criteria is the best improvement  I(f (i) Hj) 
given by a new hypothesis space Hj in reducing the norm of the current residual r(i) = y − f (i)
where f (i) = [f (i)(x1) . . . f (i)(xl)]T   by ﬁnding the best regularized (smooth) approximation. Note
that since ming∈Hj Rλ(g  r) ≤ Rλ(0  r) = krk2  the value of the improvement function 

I(f (i) Hj) = kr(i)k2

2 − min

g∈Hj Rλ(g  r(i))

is always non-negative. Once a kernel is selected  the function is re-estimated by learning in HG(i).
Note that since HG is an RKHS whose kernel function is the sum Pj∈G kj  we can use a simple
RLS linear system solver for reﬁtting. Unlike group-Lasso based MKL  we do not need an iterative
kernel reweighting step which essentially arises as a mechanism to transform the less convenient
group sparsity norms into reweighted squared RKHS norms. MKL-GOMP converges when the best
improvement is no better than ǫ.

Kernel Dictionary {kj(· ·)}N

j=1  Precision ǫ > 0

◮ Input: Data matrix X = [x1 . . . xl]T   Label vector y ∈ Rl 

◮ Output: Selected Kernels G(i) and a function f (i) ∈ HG(i)
◮ Initialization: G(0) = ∅  f (0) = 0  set residual r(0) = y
◮ for i = 0  1  2  ...

1. Kernel Selection: For all j /∈ G(i)  set:

2 − ming∈Hj Rλ(g  r(i))
I(f (i) Hj) = kr(i)k2
= r(i)T (cid:0)I − λ(Kj + λlI)−1(cid:1) r(i)
Pick j(i) = arg maxj /∈G(i) I(f (i) Hj)
2. Convergence Check: if(cid:0)I(f (i) Hj(i) ) ≤ ǫ(cid:1) break
3. Reﬁtting: Set G(i+1) = G(i)S{j(i)}. Set f (i+1)(x) =Pl
where k =Pj∈G(i+1) kj and α =(cid:16)Pj∈G(i+1) Kj + λlI(cid:17)−1
4. Update Residual: r(i+1) = y − f (i+1) where f (i+1) = [f (i+1)(x1) . . . f (i+1)(xl)]T .
end

j=1 αjk(x  xj)

y

Remarks: Note that our algorithm can be applied to multivariate problems with group structure
among outputs similar to Multivariate Group-OMP [15]. In particular  in our experiments on mul-
ticlass datasets  we treat all outputs as a single group and evaluate each kernel for selection based
on how well the total residual is reduced across all outputs simultaneously. Kernel matrices are nor-
malized to unit trace or to have uniform variance of data points in their associated feature spaces  as
in [10  33]. In practice  we can also monitor error on a validation set to decide the optimal degree
of sparsity. For efﬁciency  we can precompute the matrices Qj = (I − λ(Kj + λlI)−1)
2 so that
2 can be very quickly evaluated at selection time  and/or reduce the search
I(f (i) Hj) = kQj rk2
space by considering a random subsample of the dictionary.

1

4 Theoretical Analysis

Our analysis is composed of two parts.
In the ﬁrst part  we establish generalization bounds for
the hypothesis spaces considered by our formulation  based on the notion of Rademacher complex-

4

ity. The second component of our theoretical analysis consists of deriving conditions under which
MKL-GOMP can recover good solutions. While the ﬁrst part can be seen as characterizing the
“statistical convergence” of our method  the second part characterizes its “numerical convergence”
as an optimization method  and is required to complement the ﬁrst part. This is because matching
pursuit methods can be deemed to solve an exact sparse problem approximately  while regularized
methods (e.g. l1 norm MKL) solve an approximate problem exactly. We therefore need to show that
MKL-GOMP recovers a solution that is close to an optimum solution of the exact sparse problem.

4.1 Rademacher Bounds

Theorem 1. Consider the hypothesis space of sufﬁciently sparse and smooth functions1 

Hτ s =nf ∈ H : kfk2

l2(H) ≤ τ kfkl0(H) ≤ so

Let δ ∈ (0  1) and κ = supx∈X  j=1...N kj(x  x). Let ρ be any probability distribution on (x  y) ∈
X × R satisfying |y| ≤ M almost surely  and let {xi  yi}l
i=1 be randomly sampled according to
i=1 (yi − f (xi))2 to be the empirical risk minimizer and f ⋆ =
ρ. Deﬁne  ˆf = arg minf ∈Hτ s
arg minf ∈Hτ s R(f ) to be the true risk minimizer in Hτ s where R(f ) = E(x y)∼ρ (y − f (x))2
denotes the true risk. Then  with probability atleast 1 − δ over random draws of samples of size l 

l Pl

1

R( ˆf ) ≤ R(f ⋆) + 8Lr sκτ

l

where ky − fk∞ ≤ L = (M + √sκτ ).

+ 4L2s log( 3

2l

δ )

(6)

l

The proof is given in supplementary material  but can also be reasoned as follows. In the standard
single-RKHS case  the Rademacher complexity can be upper bounded by a quantity that is propor-
tional to the square root of the trace of the Gram matrix  which is further upper bounded by √lκ.
In our case  any collection of s-sparse functions from a dictionary of N RKHSs reduces to a single
RKHS whose kernel is the sum of s base kernels  and hence the corresponding trace can be bounded
by √lsκ for all possible subsets of size s. Once it is established that the empirical Rademacher
complexity of Hλ s is upper bounded byp sκτ
  the generalization bound follows from well-known
results [6] tailored to regularized least squares regression with bounded target variable.
For l1-norm MKL  in the context of margin-based loss functions  Cortes et. al.  2010 [8] bound
the Rademacher complexity asq ce⌈log(N )⌉κτ
where ⌈·⌉ is the ceiling function that rounds to next
22 . Using VC-based lower-bound arguments  they point
out that the plog(N ) dependence on N is essentially optimal. By contrast  our greedy approach

with sequential regularized risk minimization imposes direct control over degree of sparsity as well
as smoothness  and hence the Rademacher complexity in our case is independent of N. If s =
O(logN )  the bounds are similar. A critical difference between l1-norm MKL and sparse greedy
approximations  however  is that the former is convex and hence the empirical risk can be minimized
exactly in the hypothesis space whose complexity is bounded by Rademacher analysis. This is not
true in our case  and therefore  to complement Rademacher analysis  we need conditions under
which good solutions can be recovered.

integer  e is the exponential and c = 23

l

4.2 Exact Recovery Conditions in Noiseless Settings

We now assume that the regression function fρ(x) = R ydρ(y|x) is sparse  i.e.  fρ ∈ HGgood for
some subset Ggood of s “good” kernels and that it is sufﬁciently smooth in the sense that for some
λ > 0  given sufﬁcient samples  the empirical minimizer ˆf = arg minf ∈HGgood Rλ(f  y) gives near
optimal generalization as per Theorem 1. In this section our main concern is to characterize Group-
OMP like conditions under which MKL-GOMP will be able to learn ˆf by recovering the support
Ggood exactly.

1Note that Tikhonov regularization using a penalty term λk · k2  and Ivanov Regularization which uses a

ball constraint k · k2 ≤ τ return identical solutions for some one-to-one correspondence between λ and τ .

5

Let us denote r(i) = ˆf − f (i) as the residual function at step i of the algorithm.
Initially 
r(0) = ˆf ∈ HGgood. Our argument is inductive:
if at any step i  r(i) ∈ HGgood and we can
always guarantee that maxj∈Ggood I(f (i) Hj) > maxj /∈Ggood I(f (i) Hj)  i.e.  a good kernel of-
fers better greedy improvement  then it is clear that the algorithm correctly expands the hypothesis
space and never makes a mistake. Without loss of generality  let us rearrange the dictionary so that
Ggood = {1 . . . s}. For any function f ∈ HGgood  we now wish to derive the following upper bound 
(7)

k(I(f Hs+1) . . . I(f HN ))k∞
k(I(f H1) . . . I(f Hs))k∞ ≤ µH(Ggood)2

Clearly  a sufﬁcient condition for exact recovery is µH(Ggood) < 1.
We need some notation to state our main result. Let s = |Ggood|  i.e.  the number of good kernels.
For any matrix A ∈ Rls×l(N −s)  let kAk(2 1) denote the matrix norm induced by the following
vector norms: for any vector u = [u1 . . . us] ∈ Rls deﬁne kuk(2 1) = Ps
i=1 kuik2; and similarly 
for any vector v = [v1 . . . vN −s] ∈ Rl(N −s) deﬁne kvk(2 1) = PN −s
i=1 kvik2. Then  kAk(2 1) =
supv∈Rl(N −s)
Theorem 2. Given the kernel dictionary {kj(· ·)}N
the labeled data  MKL-GOMP correctly recovers the good kernels  i.e.  G(s) = Ggood  if

j=1 with associated gram matrices {Kj}N

. We can now state the following:

kAvk(2 1)
kvk(2 1)

i=1 over

µH(Ggood) = kCλ H(Ggood)k(2 1) < 1

where Cλ H(Ggood) ∈ Rls×l(N −s) is a coherence matrix whose (i  j)th block of size l × l  i ∈
Ggood  j /∈ Ggood  is given by 

Cλ H(Ggood)i j = KGgood Qi

 Xk∈Ggood

−1

Qk


QkK2

Ggood

Qj KGgood

(8)

1

2   j = 1 . . . N.

Kj  Qj = (I − λ(Kj + λlI)−1)

where KGgood =Pj∈Ggood
The proof is given in supplementary material. This result is analogous to sparse recovery conditions
for OMP and l1 methods and their (linear) group counterparts. In the noiseless setting  Tropp [27]
gives an exact recovery condition of the form kX†
goodXbadk1 < 1  where Xgood and Xbad refer
to the restriction of the data matrix to good and bad features  and k · k1 refers to the l1 induced
matrix norm. Intriguingly  the same paper shows that this condition is also sufﬁcient for the Basis
Pursuit l1 minimization problem. For Group-OMP [16]  the condition generalizes to involve a group
sensitive matrix norm on the same matrix objects. Likewise  Bach [2] generalizes the Lasso variable
selection consistency conditions to apply to Group Lasso and then further to non-parametric l1-
MKL. The above result is similar in spirit. A stronger sufﬁcient condition can be derived by requiring
kQj KGgoodk2 to be sufﬁciently small for all j /∈ Ggood. Intuitively  this means that smooth functions
in HGgood cannot be well approximated by using smooth functions induced by the “bad” kernels  so
that MKL-GOMP is never led to making a mistake.

5 Empirical Studies

We report empirical results on a collection of simulated datasets and 3 classiﬁcation problems from
computational cell biology.
In all experiments  as in [10  33]  candidate kernels are normalized
multiplicatively to have uniform variance of data points in their associated feature spaces.

5.1 Adaptability to Data Sparsity - Simulated Setting

We adapt the experimental setting proposed by [10] where the sparsity of the target function is ex-
plicitly controlled  and the optimal subset of kernels is varied from requiring the entire dictionary to
requiring a single kernel. Our goal is to study the solution paths offered by MKL-GOMP in compar-
ison to lq-norm MKL. For consistency  we use squared loss in all experiments2. We implemented

2l

q-MKL with SVM hinge loss behaves similarly.

6

r
o
r
r
e
 
t
s
e
t

0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

Figure 1: Simulated Setting: Adaptability to Data Sparsity

 

1−norm MKL
4/3−norm MKL
2−norm MKL
4−norm MKL
∞−norm MKL (=RLS)
MKL−GOMP
Bayes Error

 
y
t
i
s
r
a
p
S

80

60

40

20

% of Kernels Selected

0

v(θ) = fraction of noise kernels [in %]

44

66

82

92

98

140
120
100
80
60
40
20

 
s
s
e
n
h
t
o
o
m
S

Value of λ

0

v(θ) = fraction of noise kernels [in %]

44

66

82

92

98

 
0

v(θ) = fraction of noise kernels [in %]

44

66

82

92

98

lq-norm MKL for regularized least squares (RLS) using an alternating minimization scheme adapted
from [17  29]. Different binary classiﬁcation datasets3 with 50 labeled examples are randomly gen-
erated by sampling the two classes from 50-dimensional isotropic Gaussian distributions with equal
covariance matrices (identity) and equal but opposite  means µ1 = 1.75 θ
kθk and µ2 = −µ1 where θ
is a binary vector encoding the true underlying sparsity. The fraction of zero components in θ is a
measure for the feature sparsity of the learning problem. For each dataset  a linear kernel (normal-
ized as in [10]) is generated from each feature and the resulting dictionary is input to MKL-GOMP
and lq-norm MKL. For each level of sparsity  a training of size 50  validation and test sets of size
10000 are generated 10 times and average classiﬁcation errors are reported. For each run  the vali-
dation error is monitored as kernel selection progresses in MKL-GOMP and the number of kernels
with smallest validation error are chosen. The regularization parameters for both MKL-GOMP and
lq norm MKL are similarly chosen using the validation set. Figure 5.1 shows test error rates as a
function of sparsity of the target function: from non-sparse (all kernels needed) to extremely sparse
(only 1 kernel needed). We recover the observations also made in [10]: l1-norm MKL excels in
extremely sparse settings where a single kernel carries the whole discriminative information of the
learning problem. However  in the other scenarios it mostly performs worse than the other q > 1
variants  despite the fact that the vector θ remains sparse in all but the uniform scenario. As q is
increased  the error rate in these settings improves but deteriorates in sparse settings. As reported
in [11]  the elastic net MKL approach of [26] performs similar to l1-MKL in the hinge loss case.
As can be seen in the Figure  the error curve of MKL-GOMP tends to be below the lower envelope
of the error rates given by lq-MKL solutions. To adapt to the sparsity of the problem  lq methods
clearly need to tune q requiring several fresh invocations of the appropriate lq-MKL solver. On the
other hand  in MKL-GOMP the hypothesis space grows as function of the iteration number and the
solution trajectory naturally expands sequentially in the direction of decreasing sparsity. The right
plot in Figure 5.1 shows the number of kernels selected by MKL-GOMP and the optimal value of
λ  suggesting that MKL-GOMP adapts to the sparsity and smoothness of the learning problem.

5.2 Protein Subcellular Localization

The multiclass generalization of l1-MKL proposed in [33] (MCMKL) is state of the art methodology
in predicting protein subcellular localization  an important cell biology problem that concerns the
estimation of where a protein resides in a cell so that  for example  the identiﬁcation of drug targets
can be aided. We use three multiclass datasets: PSORT+  PSORT- and PLANT provided by the au-
thors of [33] at http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc
together with a dictionary of 69 kernels derived with biological insight: 2 kernels on phylogenetic
trees  3 kernels based on similarity to known proteins (BLAST E-values)  and 64 kernels based
on amino-acid sequence patterns. The statistics of the three datasets are as follows: PSORT+ has
541 proteins labeled with 4 location classes  PSORT- has 1444 proteins in 5 classes and PLANT is

3Provided by the authors of [10] at mldata.org/repository/data/viewslug/mkl-toy/

7

)
r
e
t
t
e
b
 
s
i
 
r
e
h
g
i
h
(
 
e
c
n
a
m
r
o
f
r
e
P

100

95

90

85

80

75

 

mklgomp mcmkl

sum

 

single 

other

psort+

psort−

plant

Figure 2: Protein Subcellular Localization Results

a 4-class problem with 940 proteins. For each dataset  results are averaged over 10 splits of the
dataset into training and test sets. We used exactly the same experimental protocol  data splits and
evaluation methodology as given in [33]: the hyper-parameters of MKL-GOMP (sparsity and the
regularization parameter λ) were tuned based on 3-fold cross-validation; results on PSORT+  PSORT-
are F1-scores averaged over the classes while those on PLANT are Mathew’s correlation coefﬁcient4.
Figure 2 compare MKL-GOMP against MCMKL  baselines such as using the sum of all the kernels
and using the best single kernel  and results from other prediction systems proposed in the literature.
As can be seen  MKL-GOMP slightly outperforms MCMKL on PSORT+ an PSORT- datasets and
is slightly worse on PLANT where RLS with the sum of all the kernels also performs very well.
On the two PSORT datasets  [33] report selecting 25 kernels using MCMKL. On the other hand  on
average  MKL-GOMP selects 14 kernels on PSORT+  15 on PSORT- and 24 kernels on PLANT. Note
that MKL-GOMP is applied in multivariate mode: the kernels are selected based on their utility to
reduce the total residual error across all target classes.

6 Conclusion

By proposing a Group-OMP based framework for sparse multiple kernel learning  analyzing theoret-
ically the performance of the resulting methods in relation to the dominant convex relaxation-based
approach  and demonstrating the value of our framework through extensive experimental studies 
we believe greedy methods arise as a natural alternative for tackling MKL problems. Relevant
directions for future research include extending our theoretical analysis to the stochastic setting 
investigating complex multivariate structures and groupings over outputs  e.g.  by generalizing the
multivariate version of Group-OMP [15]  and extending our algorithm to incorporate interesting
structured kernel dictionaries [3].

Acknowledgments

We thank Rick Lawrence  David S. Rosenberg and Ha Quang Minh for helpful conversations and
support for this work.

References

[1] N. Aronszajn. Theory of reproducing kernel hilbert spaces. Transactions of the American Mathematical

Society  68(3):337–404  1950.

[2] F. Bach. Consistency of group lasso and multiple kernel learning. JMLR  9:1179–1225  2008.
[3] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. In Technical

report  HAL 00413473  2009.

[4] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization with sparsity-inducing penalties.

Technical report  HAL 00413473  2010.

In

4see http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/protsubloc-wabi08-supp.pdf

8

[5] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the smo

algorithm. In ICML  2004.

[6] P. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.

JMLR  3:463–482  2002.

[7] A. Ben-Hur and W. S. Noble. Kernel methods for predicting protein–protein interactions. Bioinformatics 

21  January 2005.

[8] C. Cortes  M. Mohri  and Afshin Rostamizadeh. Generalization bounds for learning kernels. In ICML 

2010.

[9] A. K. Fletcher and S. Rangan. Orthogonal matching pursuit from noisy measurements: A new analysis.

In NIPS  2009.

[10] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien. l

2011.

p-norm multiple kernel learning. JMLR  12:953–997 

[11] M. Kloft  U. Ruckert  and P. Bartlett. A unifying view of multiple kernel learning. In European Conference

on Machine Learning (ECML)  2010.

[12] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. The Annals of Statistics  38(6):3660–

3695  2010.

[13] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. J. Mach. Learn. Res.  5:27–72  December 2004.

[14] G. R. G. Lanckriet  T. De Bie  N. Cristianini  M. I. Jordan  and W. S. Noble. A statistical framework for

genomic data fusion. Bioinformatics  20  November 2004.

[15] A. C. Lozano and V. Sindhwani. Block variable selection in multivariate regression and high-dimensional

causal inference. In NIPS  2010.

[16] A. C. Lozano  G. Swirszcz  and N. Abe. Group orthogonal matching pursuit for variable selection and

prediction. In NIPS  2009.

[17] C. Michelli and M. Pontil. Learning the kernel function via regularization. JMLR  6:1099–1125  2005.
[18] H. Liu P. Ravikumar  J. Lafferty and L. Wasserman. Sparse additive models. Journal of the Royal

Statistical Society: Series B (Statistical Methodology) (JRSSB)  71 (5):1009–1030  2009.

[19] P. Pavlidis  J. Cai  J. Weston  and W.S. Noble. Learning gene functional classiﬁcations from multiple data

types. Journal of Computational Biology  9:401–411  2002.

[20] A. Rakotomamonjy  F.Bach  S. Cano  and Y. Grandvalet. SimpleMKL. Journal of Machine Learning

Research  9:2491–2521  2008.

[21] G. Raskutti  M. Wainwrigt  and B. Yu. Minimax-optimal rates for sparse additive models over kernel

classes via convex programming. In Technical Report 795  Statistics Department  UC Berkeley.  2010.

[22] Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines  Regular-

ization  Optimization  and Beyond. MIT Press  2001.

[23] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press 

2004.

[24] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. J. Mach.

Learn. Res.  7  December 2006.

[25] Zhang T. Sparse recovery with orthogonal matching pursuit under rip. Computing Research Repository 

2010.

[26] R. Tomioka and T. Suzuki. Sparsity-accuracy tradeoff in mkl. In NIPS Workshop: Understanding Multiple

Kernel Learning Methods. Technical report  arXiv:1001.2615v1  2010.

[27] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform. Theory  

50(10):2231–2242  2004.

[28] P. Vincent and Y. Bengio. Kernel matching pursuit. Machine Learning  48:165–188  2002.
[29] Z. Xu  R. Jin  H. Yang  I. King  and M.R. Lyu. Simple and efﬁcient multiple kernel learning by group

lasso. In ICML  2010.

[30] Ming Yuan  Ali Ekici  Zhaosong Lu  and Renato Monteiro. Dimension reduction and coefﬁcient estima-
tion in multivariate linear regression. Journal Of The Royal Statistical Society Series B  69(3):329–346 
2007.

[31] Tong Zhang. On the consistency of feature selection using greedy least squares regression. J. Mach.

Learn. Res.  10  June 2009.

[32] H. Zhou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society  67(2):301–320  2005.

[33] A. Zien and Cheng S. Ong. Multiclass multiple kernel learning. ICML  2007.

9

,Thomas Hofmann
Aurelien Lucchi
Simon Lacoste-Julien
Brian McWilliams