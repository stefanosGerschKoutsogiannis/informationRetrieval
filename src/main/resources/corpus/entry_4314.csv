2017,Policy Gradient With Value Function Approximation For Collective Multiagent Planning,Decentralized (PO)MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity  recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing CDec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this  we show how a particular decomposition of the approximate action-value function over agents leads to effective updates  and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.,Policy Gradient With Value Function Approximation

For Collective Multiagent Planning

Duc Thien Nguyen Akshat Kumar Hoong Chuin Lau

{dtnguyen.2014 akshatkumar hclau}@smu.edu.sg

School of Information Systems

Singapore Management University
80 Stamford Road  Singapore 178902

Abstract

Decentralized (PO)MDPs provide an expressive framework for sequential deci-
sion making in a multiagent system. Given their computational complexity  re-
cent research has focused on tractable yet practical subclasses of Dec-POMDPs.
We address such a subclass called CDec-POMDP where the collective behavior
of a population of agents affects the joint-reward and environment dynamics. Our
main contribution is an actor-critic (AC) reinforcement learning method for opti-
mizing CDec-POMDP policies. Vanilla AC has slow convergence for larger prob-
lems. To address this  we show how a particular decomposition of the approximate
action-value function over agents leads to effective updates  and also derive a new
way to train the critic based on local reward signals. Comparisons on a synthetic
benchmark and a real world taxi ﬂeet optimization problem show that our new AC
approach provides better quality solutions than previous best approaches.

1

Introduction

Decentralized partially observable MDPs (Dec-POMDPs) have emerged in recent years as a promis-
ing framework for multiagent collaborative sequential decision making (Bernstein et al.  2002).
Dec-POMDPs model settings where agents act based on different partial observations about the
environment and each other to maximize a global objective. Applications of Dec-POMDPs include
coordinating planetary rovers (Becker et al.  2004b)  multi-robot coordination (Amato et al.  2015)
and throughput optimization in wireless network (Winstein and Balakrishnan  2013; Pajarinen et al. 
2014). However  solving Dec-POMDPs is computationally challenging  being NEXP-Hard even for
2-agent problems (Bernstein et al.  2002).
To increase scalability and application to practical problems  past research has explored restricted
interactions among agents such as state transition and observation independence (Nair et al.  2005;
Kumar et al.  2011  2015)  event driven interactions (Becker et al.  2004a) and weak coupling among
agents (Witwicki and Durfee  2010). Recently  a number of works have focused on settings where
agent identities do not affect interactions among agents. Instead  environment dynamics are pri-
marily driven by the collective inﬂuence of agents (Varakantham et al.  2014; Sonu et al.  2015;
Robbel et al.  2016; Nguyen et al.  2017)  similar to well known congestion games (Meyers and
Schulz  2012). Several problems in urban transportation such as taxi supply-demand matching can
be modeled using such collective planning models (Varakantham et al.  2012; Nguyen et al.  2017).
In this work  we focus on the collective Dec-POMDP framework (CDec-POMDP) that formalizes
such a collective multiagent sequential decision making problem under uncertainty (Nguyen et al. 
2017). Nguyen et al. present a sampling based approach to optimize policies in the CDec-POMDP
model. A key drawback of this previous approach is that policies are represented in a tabular form
which scales poorly with the size of observation space of agents. Motivated by the recent suc-

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: T-step DBN for a CDec-POMDP

cess of reinforcement learning (RL) approaches (Mnih et al.  2015; Schulman et al.  2015; Mnih
et al.  2016; Foerster et al.  2016; Leibo et al.  2017)  our main contribution is a actor-critic (AC)
reinforcement learning method (Konda and Tsitsiklis  2003) for optimizing CDec-POMDP policies.
Policies are represented using function approxi-
mator such as a neural network  thereby avoiding
the scalability issues of a tabular policy. We derive
the policy gradient and develop a factored action-
value approximator based on collective agent in-
teractions in CDec-POMDPs. Vanilla AC is slow
to converge on large problems due to known issues
of learning with global reward in large multiagent
systems (Bagnell and Ng  2005). To address this 
we also develop a new way to train the critic  our
action-value approximator  that effectively utilizes
local value function of agents.
We test our approach on a synthetic multirobot grid navigation domain from (Nguyen et al.  2017) 
and a real world supply-demand taxi matching problem in a large Asian city with up to 8000 taxis (or
agents) showing the scalability of our approach to large multiagent systems. Empirically  our new
factored actor-critic approach works better than previous best approaches providing much higher
solution quality. The factored AC algorithm empirically converges much faster than the vanilla AC
validating the effectiveness of our new training approach for the critic.
Related work: Our work is based on the framework of policy gradient with approximate value
function similar to Sutton et al. (1999). However  as we empirically show  directly applying the
original policy gradient from Sutton et al. (1999) into the multi-agent setting and speciﬁcally for
the CDec-POMDP model results in a high variance solution.
In this work  we show a suitable
form of compatible value function approximation for CDec-POMDPs that results in an efﬁcient and
low variance policy gradient update. Reinforcement learning for decentralized policies has been
studied earlier in Peshkin et al. (2000)  Aberdeen (2006). Guestrin et al. (2002) also proposed using
REINFORCE to train a softmax policy of a factored value function from the coordination graph.
However in such previous works  policy gradient is estimated from the global empirical returns
instead of a decomposed critic. We show in section 4 that having a decomposed critic along with an
individual value function based training of this critic is important for sample-efﬁcient learning. Our
empirical results show that our proposed critic training has faster convergence than training with
global empirical returns.

2 Collective Decentralized POMDP Model
We ﬁrst describe the CDec-POMDP model introduced in (Nguyen et al.  2017). A T -step Dynamic
Bayesian Network (DBN) for this model is shown using the plate notation in ﬁgure 1. It consists of
the following:
• A ﬁnite planning horizon H.
• The number of agents M. An agent m can be in one of the states in the state space S. The joint
• A set of action A for each agent m. We denote an individual action as j ∈ A.
• Let (s1:H   a1:H )m = (sm

H ) denote the complete state-action trajectory of an
agent m. We denote the state and action of agent m at time t using random variables sm
t   am
t .
Different indicator functions It(·) are deﬁned in table 1. We deﬁne the following count given the
trajectory of each agent m ∈ M:

m=1S. We denote a single state as i ∈ S.

state space is ×M

1   am

1   sm

2 . . .   sm

H   am

M(cid:88)

nt(i  j  i

(cid:48)) =

Im
t (i  j  i

(cid:48)) ∀i  i

(cid:48)∈S  j∈A

m=1

As noted in table 1  count nt(i  j  i(cid:48)) denotes the number of agents in state i taking action j
at time step t and transitioning to next state i(cid:48); other counts  nt(i) and nt(i  j)  are deﬁned
analogously. Using these counts  we can deﬁne the count tables nst and nstat for the time step
t as shown in table 1.

2

m=1:Mom1om1sm1sm2am2am1smTamTnsTns2ns1rmTom2t = i

t (i)∈{0  1}
Im
t (i  j)∈{0  1}
Im
t (i  j  i(cid:48))∈{0  1}
Im
nt(i)∈ [0; M ]
nt(i  j)∈ [0; M ]
nt(i  j  i(cid:48))∈ [0; M ] Number of agents at state i taking action j at time t and transitioning to state i(cid:48) at time t + 1
nst
nstat
nstatst+1
Table 1: Summary of notations given the state-action trajectories  (s1:H   a1:H )m ∀m  for all the agents

if agent m is at state i at time t or sm
if agent m takes action j in state i at time t or (sm
if agent m takes action j in state i at time t and transitions to state i(cid:48) or (sm
Number of agents at state i at time t
Number of agents at state i taking action j at time t
Count table (nt(i) ∀i∈ S)
Count table (nt(i  j) ∀i∈ S  j∈ A)
Count table (nt(i  j  i(cid:48)) ∀i  i(cid:48)∈ S  j∈ A)

t ) = (i  j)

t   am

t   am

t   sm

t+1) = (i  j  i(cid:48))

• We assume a general partially observable setting wherein agents can have different observations
In
based on the collective inﬂuence of other agents. An agent observes its local state sm
t .
addition  it also observes om
t and the count table nst. E.g. 
an agent m in state i at time t can observe the count of other agents also in state i (=nt(i)) or
other agents in some neighborhood of the state i (={nt(j) ∀j ∈ Nb(i)}).

t at time t based on its local state sm

(cid:1). The transition function is the same

• The transition function is φt

(cid:0)sm

for all the agents. Notice that it is affected by nst  which depends on the collective behavior of
the agent population.

t+1 = i(cid:48)|sm

t = i  am

t = j  nst

H ).
1   . . .   πm

• Each agent m has a non-stationary policy πm
m to take action j given its observation (i  om
planning horizon of an agent m to be πm = (πm

t (j|i  om
t (i  nst)) denoting the probability of agent
t (i  nst)) at time t. We denote the policy over the

the counts nst.

t = rt(i  j  nst) dependent on its local state and action  and

• An agent m receives the reward rm
• Initial state distribution  bo = (P (i)∀i ∈ S)  is the same for all agents.
We present here the simplest version where all the agents are of the same type having similar state
transition  observation and reward models. The model can handle multiple agent types where agents
have different dynamics based on their type. We can also incorporate an external state that is unaf-
fected by agents’ actions (such as taxi demand in transportation domain). Our results are extendible
to address such settings also.
Models such as CDec-POMDPs are useful in settings where agent population is large  and agent
identity does not affect the reward or the transition function. A motivating application of this model
is for the taxi-ﬂeet optimization where the problem is to compute policies for taxis such that the total
proﬁt of the ﬂeet is maximized (Varakantham et al.  2012; Nguyen et al.  2017). The decision making
for a taxi is as follows. At time t  each taxi observes its current city zone z (different zones constitute
the state-space S)  and also the count of other taxis in the current zone and its neighboring zones
as well as an estimate of the current local demand. This constitutes the count-based observation
o(·) for the taxi. Based on this observation  the taxi must decide whether to stay in the current
zone z to look for passengers or move to another zone. These decision choices depend on several
factors such as the ratio of demand and the count of other taxis in the current zone. Similarly  the
environment is stochastic with variable taxi demand at different times. Such historical demand data
is often available using GPS traces of the taxi ﬂeet (Varakantham et al.  2012).
Count-Based statistic for planning: A key property in the CDec-POMDP model is that the model
dynamics depend on the collective interaction among agents rather than agent identities. In settings
such as taxi ﬂeet optimization  the agent population size can be quite large (≈ 8000 for our real
world experiments). Given such a large population  it is not possible to compute unique policy for
each agent. Therefore  similar to previous work (Varakantham et al.  2012; Nguyen et al.  2017) 
our goal is to compute a homogenous policy π for all the agents. As the policy π is dependent on
counts  it represents an expressive class of policies.
For a ﬁxed population M  let {(s1:T   a1:T )m ∀m} denote the state-action trajectories of different
agents sampled from the DBN in ﬁgure 1. Let n1:T ={(nst  nstat  nstatst+1 ) ∀t = 1 : T} be the
combined vector of the resulting count tables for each time step t. Nguyen et al. show that counts n
are the sufﬁcient statistic for planning. That is  the joint-value function of a policy π over horizon

3

H can be computed by the expectation over counts as (Nguyen et al.  2017):

V (π) =

E[rm

T ] =

P (n; π)

nT (i  j)rT

T =1
Set Ω1:H is the set of all allowed consistent count tables as:

T =1

m=1

(cid:88)

n∈Ω1:H

(cid:1)(cid:21)

(cid:0)i  j  nT

(1)

nT (i) = M ∀T ;

nT (i  j) = nT (i) ∀j∀T ;

nT (i  j  i

(cid:48)) = nT (i  j) ∀i ∈ S  j ∈ A ∀T

(cid:88)

i∈S j∈A

(cid:20) H(cid:88)
(cid:88)

i(cid:48)∈S

M(cid:88)

H(cid:88)
(cid:88)

j∈A

(cid:88)

i∈S

P (n; π) is the distribution over counts (detailed expression in appendix). A key beneﬁt of this result
is that we can evaluate the policy π by sampling counts n directly from P (n) without sampling in-
dividual agent trajectories (s1:H   a1:H )m for different agents  resulting in signiﬁcant computational
savings. Our goal is to compute the optimal policy π that maximizes V (π). We assume a RL setting
with centralized learning and decentralized execution. We assume a simulator is available that can
provide count samples from P (n; π).

3 Policy Gradient for CDec-POMDPs

Previous work proposed an expectation-maximization (EM) (Dempster et al.  1977) based sampling
approach to optimize the policy π (Nguyen et al.  2017). The policy is represented as a piecewise
linear tabular policy over the space of counts n where each linear piece speciﬁes a distribution over
next actions. However  this tabular representation is limited in its expressive power as the number of
pieces is ﬁxed apriori  and the range of each piece has to be deﬁned manually which can adversely
affect performance. Furthermore  exponentially many pieces are required when the observation o is
multidimensional (i.e.  an agent observes counts from some local neighborhood of its location). To
address such issues  our goal is to optimize policies in a functional form such as a neural network.
We ﬁrst extend the policy gradient theorem of (Sutton et al.  1999) to CDec-POMDPs. Let θ denote
the vector of policy parameters. We next show how to compute ∇θV (π). Let st  at denote the
joint-state and joint-actions of all the agents at time t. The value function of a given policy π in an
expanded form is given as:

where P π(st  at|bo) = (cid:80)

Vt(π) =

P π(st  at|bo  π)Qπ

t (st  at)

(2)

s1:t−1 a1:t−1 P π(s1:t  a1:t|bo) is the distribution of the joint state-action

st  at under the policy π. The value function Qπ

Qπ

t (st  at) = rt(st  at) +

t (st  at) is computed as:
P π(st+1  at+1|st  at)Qπ

t+1(st+1  at+1)

(cid:88)

st at

(cid:88)

st+1 at+1

(cid:20)

We next state the policy gradient theorem for CDec-POMDPs:
Theorem 1. For any CDec-POMDP  the policy gradient is given as:

∇θV1(π) =

Est at|bo π

Qπ

t (st  at)

nt(i  j)∇θ log πt

(cid:88)

i∈S j∈A

H(cid:88)

t=1

(3)

(4)

(cid:0)j|i  o(i  nst)(cid:1)(cid:21)

The proofs of this theorem and other subsequent results are provided in the appendix.
Notice that computing the policy gradient using the above result is not practical for multiple reasons.
The space of join-state action (st  at) is combinatorial. Given that the agent population size can be
large  sampling each agent’s trajectory is not computationally tractable. To remedy this  we later
show how to compute the gradient by directly sampling counts n∼ P (n; π) similar to policy evalua-
tion in (1). Similarly  one can estimate the action-value function Qπ
t (st  at) using empirical returns
as an approximation. This would be the analogue of the standard REINFORCE algorithm (Williams 
1992) for CDec-POMDPs. It is well known that REINFORCE may learn slowly than other methods
that use a learned action-value function (Sutton et al.  1999). Therefore  we next present a function
approximator for Qπ
t   and show the computation of policy gradient by directly sampling counts n.

4

3.1 Policy Gradient with Action-Value Approximation

One can approximate the action-value function Qπ
the following special form of the approximate value function fw:

t (st  at) in several different ways. We consider

t (st  at) ≈ fw(st  at) =
Qπ

f m
w

t   o(sm

t   nst)  am
t

(5)

M(cid:88)

m=1

(cid:0)sm

(cid:1)

w is deﬁned for each agent m and takes as input the agent’s local state  action and
where each f m
the observation. Notice that different components f m
w are correlated as they depend on the com-
mon count table nst. Such a decomposable form is useful as it leads to efﬁcient policy gradient
computation. Furthermore  an important class of approximate value function having this form for
CDec-POMDPs is the compatible value function (Sutton et al.  1999) which results in an unbiased
policy gradient (details in appendix).
Proposition 1. Compatible value function for CDec-POMDPs can be factorized as:

fw(st  at) =

f m
w (sm

t   o(sm

t   nst)  am)

(cid:88)

m

We can directly replace Qπ(·) in policy gradient (4) by the approximate action-value function fw.
Empirically  we found that variance using this estimator was high. We exploit the structure of fw
and show further factorization of the policy gradient next which works much better empirically.
Theorem 2. For any value function having the decomposition as:

fw(st  at) =

t   o(sm

t   nst)  am
t

(cid:0)sm

f m
w

(cid:88)
∇θ log π(cid:0)am

m

t   nst)(cid:1)f m

w

t |sm

t   o(sm

(cid:1) 
(cid:0)sm

(6)

(7)

(cid:1)(cid:105)

t   o(sm

t   nst)  am
t

the policy gradient can be computed as

H(cid:88)

(cid:104)(cid:88)

∇θV1(π) =

Est at

t=1

m

The above result shows that if the approximate value function is factored  then the resulting policy
gradient also becomes factored. The above result also applies to agents with multiple types as we
w is different for each agent. In the simpler case when all the agents are of
assumed the function f m
same type  then we have the same function fw for each agent  and also deduce the following:

fw(st  at) =

(cid:88)

(cid:104)(cid:88)

∇θV1(π) =

Est at

nt(i  j)fw

(cid:0)i  j  o(i  nst)(cid:1)

(cid:88)
nt(i  j)∇θ log π(cid:0)j|i  o(i  nst)(cid:1)fw(i  j  o(i  nst))

i j

(cid:105)

(8)

(9)

Using the above result  we simplify the policy gradient as:

t

i j

3.2 Count-based Policy Gradient Computation

Notice that in (9)  the expectation is still w.r.t. joint-states and actions (st  at) which is not efﬁcient
in large population sizes. To address this issue  we exploit the insight that the approximate value
function in (8) and the inner expression in (9) depends only on the counts generated by the joint-state
and action (st  at).

Theorem 3. For any value function having the form: fw(st  at) =(cid:80)

i j nt(i  j)fw

the policy gradient can be computed as:

(cid:20) H(cid:88)

(cid:88)

t=1

i∈S j∈A

En1:H∈Ω1:H

nt(i  j)∇θ log π(cid:0)j|i  o(i  nt)(cid:1)fw(i  j  o(i  nt))

(cid:0)i  j  o(i  nst)(cid:1) 
(cid:21)

(10)

The above result shows that the policy gradient can be computed by sampling count table vectors
n1:H from the underlying distribution P (·) analogous to computing the value function of the policy
in (1)  which is tractable even for large population sizes.

5

4 Training Action-Value Function

In our approach  after count samples n1:H are generated to compute the policy gradient  we also
need to adjust the parameters w of our critic fw. Notice that as per (8)  the action value function
fw(st  at) depends only on the counts generated by the joint-state and action (st  at). Training fw
can be done by taking a gradient step to minimize the following loss function:

K(cid:88)

H(cid:88)

(cid:16)

ξ=1

t=1

min

w

(cid:17)2

fw(nξ

t ) − Rξ

t

(11)

where nξ
function and Rξ

1:H is a count sample generated from the distribution P (n; π); fw(nξ
t is the total empirical return for time step t computed using (1):

t ) is the action value

fw(nξ

t ) =

nξ
t (i  j)fw(i  j  o(i  nξ

t )); Rξ

t =

nξ
T (i  j)rT (i  j  nξ
T )

(12)

H(cid:88)

(cid:88)

T =t

i∈S j∈A

(cid:88)

i j

t (i  j) = E[(cid:80)H

However  we found that the loss in (11) did not work well for training the critic fw for larger prob-
lems. Several count samples were required to reliably train fw which adversely affects scalability
for large problems with many agents. It is already known in multiagent RL that algorithms that
solely rely on the global reward signal (e.g. Rξ
t in our case) may require several more samples than
approaches that take advantage of local reward signals (Bagnell and Ng  2005). Motivated by this
observation  we next develop a local reward signal based strategy to train the critic fw.
Individual Value Function: Let nξ
1:H  let
1:H ] denote the total expected reward obtained by an agent
V ξ
that is in state i and takes action j at time t. This individual value function can be computed using
dynamic programming as shown in (Nguyen et al.  2017). Based on this value function  we next
show an alternative reparameterization of the global empirical reward Rξ
Lemma 1. The empirical return Rξ
parameterized as: Rξ
i∈S j∈A nξ

t for the time step t given the count sample nξ
t (i  j)V ξ

1:H be a count sample. Given the count sample nξ

1:H can be re-

t in (12):

m = j  nξ

t (i  j).

t = i  at

t(cid:48) |sm

t(cid:48)=t rm

Individual Value Function Based Loss: Given lemma 1  we next derive an upper bound on the on
the true loss (11) which effectively utilizes individual value functions:

t =(cid:80)
(cid:17)2

(cid:88)

(cid:88)

(cid:16)

ξ

t

fw(nξ) − Rξ

t

nξ
t (i  j)fw(i  j  o(i  nξ

nξ
t (i  j)V ξ

t (i  j)

(cid:16)(cid:88)
(cid:88)
(cid:88)
(cid:18)(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

i j

i j

ξ

ξ

t

t

=

=

(cid:16)

(cid:16)

t )) −(cid:88)

i j

(cid:17)2

(13)

(14)

(cid:17)(cid:19)2

(cid:17)2

nξ
t (i  j)

fw(i  j  o(i  nξ

t )) − V ξ

t (i  j)

≤ M

nt(i  j)

fw(i  j  o(i  nξ

t )) − V ξ

t (i  j)

ξ

t i j

where the last relation is derived by Cauchy-Schwarz inequality. We train the critic using the modi-
ﬁed loss function in (14). Empirically  we observed that for larger problems  this new loss function
in (14) resulted in much faster convergence than the original loss function in (13). Intuitively  this is
because the new loss (14) tries to adjust each critic component fw(i  j  o(i  nξ
t )) closer to its counter-
part empirical return V ξ
t (i  j). However  in the original loss function (13)  the focus is on minimizing
the global loss  rather than adjusting each individual critic factor fw(·) towards the corresponding
empirical return.
Algorithm 1 shows the outline of our AC approach for CDec-POMDPs. Lines 7 and 8 show two
different options to train the critic. Line 7 represents critic update based on local value functions 
also referred to as factored critic update (fC). Line 8 shows update based on global reward or global
critic update (C). Line 10 shows the policy gradient computed using theorem 2 (fA). Line 11 shows
how the gradient is computed by directly using fw from eq. (5) in eq. 4.

6

Algorithm 1: Actor-Critic RL for CDec-POMDPs
1 Initialize network parameter θ for actor π and and w for critic fw
2 α ← actor learning rate
3 β ← critic learning rate
4 repeat
5
6

1:H ∼ P (n1:H ; π) ∀ξ = 1 to K

Sample count vectors nξ
Update critic as:
fC : w = w − β 1
C : w = w − β 1
Update actor as:
fA : θ = θ + α 1

(cid:104)(cid:80)
(cid:104)(cid:80)
K ∇w
K ∇w
(cid:80)
(cid:80)
(cid:80)
(cid:80)

A : θ = θ + α 1

K ∇θ
K ∇θ

ξ

ξ

ξ

ξ

t i j nξ

(cid:80)
(cid:16)(cid:80)
(cid:80)
(cid:104)(cid:80)
(cid:104)(cid:80)

t (i  j)
i j nξ

t )) − V ξ

fw(i  j  o(i  nξ

(cid:16)
t (i  j) log π(cid:0)j|i  o(i  nξ
t (i  j) log π(cid:0)j|i  o(i  nξ

t (i  j)fw(i  j  o(i  nξ

(cid:17)2(cid:105)
t )) −(cid:80)
t )(cid:1)fw(i  j  o(nξ
t )(cid:1)(cid:105)(cid:104)(cid:80)

t (i  j)

i j nξ

i j nξ
i j nξ

t

t

t

7

8

9

10

11
12 until convergence
13 return θ  w

(cid:17)2(cid:105)

i j nξ

t (i  j)V ξ

t (i  j)

(cid:105)

t   i))

t (i  j)fw(i  j  o(nξ

t   i))

(cid:105)

5 Experiments

t |sm

t |sm

t and the single count observation nt(sm

This section compares the performance of our AC approach with two other approaches for solv-
ing CDec-POMDPs—Soft-Max based ﬂow update (SMFU) (Varakantham et al.  2012)  and the
Expectation-Maximization (EM) approach (Nguyen et al.  2017). SMFU can only optimize policies
t )  as it approximates the effect
where an agent’s action only depends on its local state  π(am
of counts n by computing the single most likely count vector during the planning phase. The EM
t  ·) is a piecewise
approach can optimize count-based piecewise linear policies where πt(am
function over the space of all possible count observations ot.
Algorithm 1 shows two ways of updating the critic (in lines 7  8) and two ways of updating the actor
(in lines 10  11) leading to 4 possible settings for our actor-critic approach—fAfC  AC  AfC  fAC.
We also investigate the properties of these different actor-critic approaches. The neural network
structure and other experimental settings are provided in the appendix.
For fair comparisons with previous approaches  we use three different models for counts-based
t and not on counts. In
observation ot. In ‘o0’ setting  policies depend only on agent’s local state sm
t ). That
‘o1’ setting  policies depend on the local state sm
t . In ‘oN’ setting 
is  the agent can only observe the count of other agents in its current state sm
t and also the count of other agents from a local neighborhood
the agent observes its local state sm
t . The ‘oN’ observation model provides the most information to an
(deﬁned later) of the state sm
agent. However  it is also much more difﬁcult to optimize as policies have more parameters. The
SMFU only works with ‘o0’ setting; EM and our actor-critic approach work for all the settings.
Taxi Supply-Demand Matching: We test our approach on this real-world domain described in
section 2  and introduced in (Varakantham et al.  2012). In this problem  the goal is to compute taxi
policies for optimizing the total revenue of the ﬂeet. The data contains GPS traces of taxi movement
in a large Asian city over 1 year. We use the observed demand information extracted from this
dataset. On an average  there are around 8000 taxis per day (data is not exhaustive over all taxi
operators). The city is divided into 81 zones and the plan horizon is 48 half hour intervals over 24
hours. For details about the environment dynamics  we refer to (Varakantham et al.  2012).
Figure 2(a) shows the quality comparisons among different approaches with different observation
models (‘o0’  ‘o1’ and ‘oN’). We test with total number of taxis as 4000 and 8000 to see if taxi pop-
ulation size affects the relative performance of different approaches. The y-axis shows the average
per day proﬁt for the entire ﬂeet. For the ‘o0’ case  all approaches (fAfC-‘o0’  SMFU  EM-‘o0’)
give similar quality with fAfC-‘o0’ and EM-‘o0’ performing slightly better than SMFU for the 8000
taxis. For the ‘o1’ case  there is sharp improvement in quality by fAfC-‘o1’ over fAfC-‘o0’ con-
ﬁrming that taking count based observation into account results in better policies. Our approach
fAfC-‘o1’ is also signiﬁcantly better than the policies optimized by EM-‘o1’ for both 4000 and
8000 taxi setting.

7

(a) Solution quality with varying taxi population

(b) Solution quality in grid navigation problem

Figure 2: Solution quality comparisons on the taxi problem and the grid navigation

(a) AC convergence with ‘o0’

(b) AC convergence with ‘o1’

(c) AC convergence with ‘oN’

Figure 3: Convergence of different actor-critic variants on the taxi problem with 8000 taxis

To further test the scalability and the ability to optimize complex policies by our approach in the ‘oN’
setting  we deﬁne the neighborhood of each state (which is a zone in the city) to be the set of its
geographically connected zones based on the zonal decomposition shown in (Nguyen et al.  2017).
On an average  there are about 8 neighboring zones for a given zone  resulting in 9 count based
observations available to the agent for taking decisions. Each agent observes both the taxi count
and the demand information from such neighboring zones. In ﬁgure 2(a)  fAfC-‘oN’ result clearly
shows that taking multiple observations into account signiﬁcantly increases solution quality—fAfC-
‘oN’ provides an increase of 64% in quality over fAfC-‘o0’ and 20% over fAfC-‘o1’ for the 8000
taxi case. For EM-‘oN’  we used a bare minimum of 2 pieces per observation dimension (resulting
in 29 pieces per time step). We observed that EM was unable to converge within 30K iterations and
provided even worse quality than EM-‘o1’ at the end. These results show that despite the larger
search space  our fAfC approach can effectively optimize complex policies whereas the tabular
policy based EM approach was ineffective for this case.
Figures 3(a-c) show the quality Vs. iterations for different variations of our actor critic approach—
fAfC  AC  AfC  fAC—for the ‘o0’  ‘o1’ and the ‘oN’ observation model. These ﬁgures clearly
show that using factored actor and the factored critic update in fAfC is the most reliable strategy
over all the other variations and for all the observation models. Variations such as AC and fAC
were not able to converge at all despite having exactly the same parameters as fAfC. These results
validate different strategies that we have developed in our work to make vanilla AC converge faster
for large problems.
Robot navigation in a congested environment: We also tested on a synthetic benchmark intro-
duced in (Nguyen et al.  2017). The goal is for a population of robots (= 20) to move from a set
of initial locations to a goal state in a 5x5 grid. If there is congestion on an edge  then each agent
attempting to cross the edge has higher chance of action failure. Similarly  agents also receive a
negative reward if there is edge congestion. On successfully reaching the goal state  agents receive
a positive reward and transition back to one of the initial state. We set the horizon to 100 steps.
Figure 2(b) shows the solution quality comparisons among different approaches. In the ‘oN’ obser-
vation model  the agent observes its 4 immediate neighbor node’s count information. In this prob-
lem  SMFU performed worst  fAfC and EM both performed much better. As expected fAfC-‘oN’

8

40008000Taxi Population05000001000000150000020000002500000QualityfAfC-o0fAfC-o1fAfC-oNSMFUEM-o0EM-o1Grid Navigation−50050100150QualityfAfC-o0fAfC-o1fAfC-oNSMFUEM-o0EM-o1EM-oN05000100001500020000Iteration750000500000250000025000050000075000010000001250000QualityfAfCACfACAfC05000100001500020000Iteration50000005000001000000Quality05000100001500020000Iteration500000050000010000001500000Quality0500010000150002000025000Iteration500000100000015000002000000Qualityprovides the best solution quality over all the other approaches. In this domain  EM is competi-
tive with fAfC as for this relatively smaller problem with 25 agents  the space of counts is much
smaller than in the taxi domain. Therefore  EM’s piecewise policy is able to provide a ﬁne grained
approximation over the count range.

6 Summary

We addressed the problem of collective multiagent planning where the collective behavior of a pop-
ulation of agents affects the model dynamics. We developed a new actor-critic method for solving
such collective planning problems within the CDec-POMDP framework. We derived several new
results for CDec-POMDPs such as the policy gradient derivation  and the structure of the compatible
value function. To overcome the slow convergence of the vanilla actor-critic method we developed
multiple techniques based on value function factorization and training the critic using individual
value function of agents. Using such techniques  our approach provided signiﬁcantly better quality
than previous approaches  and proved scalable and effective for optimizing policies in a real world
taxi supply-demand problem and a synthetic grid navigation problem.

7 Acknowledgments

This research project is supported by National Research Foundation Singapore under its Corp Lab
@ University scheme and Fujitsu Limited. First author is also supported by A(cid:63)STAR graduate
scholarship.

9

References
Aberdeen  D. (2006). Policy-gradient methods for planning. In Advances in Neural Information

Processing Systems  pages 9–16.

Amato  C.  Konidaris  G.  Cruz  G.  Maynor  C. A.  How  J. P.  and Kaelbling  L. P. (2015). Planning
for decentralized control of multiple robots under uncertainty. In IEEE International Conference
on Robotics and Automation  ICRA  pages 1241–1248.

Bagnell  J. A. and Ng  A. Y. (2005). On local rewards and scaling distributed reinforcement learning.

In International Conference on Neural Information Processing Systems  pages 91–98.

Becker  R.  Zilberstein  S.  and Lesser  V. (2004a). Decentralized Markov decision processes with
In Proceedings of the 3rd International Conference on Autonomous

event-driven interactions.
Agents and Multiagent Systems  pages 302–309.

Becker  R.  Zilberstein  S.  Lesser  V.  and Goldman  C. V. (2004b). Solving transition independent
decentralized Markov decision processes. Journal of Artiﬁcial Intelligence Research  22:423–
455.

Bernstein  D. S.  Givan  R.  Immerman  N.  and Zilberstein  S. (2002). The complexity of decentral-

ized control of Markov decision processes. Mathematics of Operations Research  27:819–840.

Dempster  A. P.  Laird  N. M.  and Rubin  D. B. (1977). Maximum likelihood from incomplete data

via the EM algorithm. Journal of the Royal Statistical society  Series B  39(1):1–38.

Foerster  J. N.  Assael  Y. M.  de Freitas  N.  and Whiteson  S. (2016). Learning to communicate
In Advances in Neural Information Processing

with deep multi-agent reinforcement learning.
Systems  pages 2137–2145.

Guestrin  C.  Lagoudakis  M.  and Parr  R. (2002). Coordinated reinforcement learning. In ICML 

volume 2  pages 227–234.

Konda  V. R. and Tsitsiklis  J. N. (2003). On actor-critic algorithms. SIAM Journal on Control and

Optimization  42(4):1143–1166.

Kumar  A.  Zilberstein  S.  and Toussaint  M. (2011). Scalable multiagent planning using probabilis-
tic inference. In Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial
Intelligence  pages 2140–2146  Barcelona  Spain.

Kumar  A.  Zilberstein  S.  and Toussaint  M. (2015). Probabilistic inference techniques for scalable

multiagent decision making. Journal of Artiﬁcial Intelligence Research  53(1):223–270.

Leibo  J. Z.  Zambaldi  V. F.  Lanctot  M.  Marecki  J.  and Graepel  T. (2017). Multi-agent rein-
forcement learning in sequential social dilemmas. In International Conference on Autonomous
Agents and Multiagent Systems.

Meyers  C. A. and Schulz  A. S. (2012). The complexity of congestion games. Networks  59:252–

260.

Mnih  V.  Badia  A. P.  Mirza  M.  Graves  A.  Lillicrap  T.  Harley  T.  Silver  D.  and Kavukcuoglu 
K. (2016). Asynchronous methods for deep reinforcement learning. In International Conference
on Machine Learning  pages 1928–1937.

Mnih  V.  Kavukcuoglu  K.  Silver  D.  Rusu  A. A.  Veness  J.  Bellemare  M. G.  Graves  A. 
Riedmiller  M. A.  Fidjeland  A.  Ostrovski  G.  Petersen  S.  Beattie  C.  Sadik  A.  Antonoglou 
I.  King  H.  Kumaran  D.  Wierstra  D.  Legg  S.  and Hassabis  D. (2015). Human-level control
through deep reinforcement learning. Nature  518(7540):529–533.

Nair  R.  Varakantham  P.  Tambe  M.  and Yokoo  M. (2005). Networked distributed POMDPs: A
synthesis of distributed constraint optimization and POMDPs. In AAAI Conference on Artiﬁcial
Intelligence  pages 133–139.

Nguyen  D. T.  Kumar  A.  and Lau  H. C. (2017). Collective multiagent sequential decision making

under uncertainty. In AAAI Conference on Artiﬁcial Intelligence  pages 3036–3043.

10

Pajarinen  J.  Hottinen  A.  and Peltonen  J. (2014). Optimizing spatial and temporal reuse in wire-
less networks by decentralized partially observable Markov decision processes. IEEE Trans. on
Mobile Computing  13(4):866–879.

Peshkin  L.  Kim  K.-E.  Meuleau  N.  and Kaelbling  L. P. (2000). Learning to cooperate via policy
search. In Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence  pages
489–496. Morgan Kaufmann Publishers Inc.

Robbel  P.  Oliehoek  F. A.  and Kochenderfer  M. J. (2016). Exploiting anonymity in approxi-
mate linear programming: Scaling to large multiagent MDPs. In AAAI Conference on Artiﬁcial
Intelligence  pages 2537–2543.

Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M.  and Moritz  P. (2015). Trust region policy opti-

mization. In International Conference on Machine Learning  pages 1889–1897.

Sonu  E.  Chen  Y.  and Doshi  P. (2015).

Individual planning in agent populations: Exploiting
anonymity and frame-action hypergraphs. In International Conference on Automated Planning
and Scheduling  pages 202–210.

Sutton  R. S.  McAllester  D.  Singh  S.  and Mansour  Y. (1999). Policy gradient methods for
reinforcement learning with function approximation. In International Conference on Neural In-
formation Processing Systems  pages 1057–1063.

Varakantham  P.  Adulyasak  Y.  and Jaillet  P. (2014). Decentralized stochastic planning with

anonymity in interactions. In AAAI Conference on Artiﬁcial Intelligence  pages 2505–2511.

Varakantham  P. R.  Cheng  S.-F.  Gordon  G.  and Ahmed  A. (2012). Decision support for agent
populations in uncertain and congested environments. In AAAI Conference on Artiﬁcial Intelli-
gence  pages 1471–1477.

Williams  R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine Learning  8(3):229–256.

Winstein  K. and Balakrishnan  H. (2013). Tcp ex machina: Computer-generated congestion control.

In Proceedings of the ACM SIGCOMM 2013 Conference  SIGCOMM ’13  pages 123–134.

Witwicki  S. J. and Durfee  E. H. (2010). Inﬂuence-based policy abstraction for weakly-coupled
Dec-POMDPs. In International Conference on Automated Planning and Scheduling  pages 185–
192.

11

,Duc Thien Nguyen
Akshat Kumar
Hoong Chuin Lau