2018,Information-theoretic Limits for Community Detection in Network Models,We analyze the information-theoretic limits for the recovery of node labels in several network models. This includes the Stochastic Block Model  the Exponential Random Graph Model  the Latent Space Model  the Directed Preferential Attachment Model  and the Directed Small-world Model. For the Stochastic Block Model  the non-recoverability condition depends on the probabilities of having edges inside a community  and between different communities. For the Latent Space Model  the non-recoverability condition depends on the dimension of the latent space  and how far and spread are the communities in the latent space. For the Directed Preferential Attachment Model and the Directed Small-world Model  the non-recoverability condition depends on the ratio between homophily and neighborhood size. We also consider dynamic versions of the Stochastic Block Model and the Latent Space Model.,Information-theoretic Limits for Community

Detection in Network Models

Department of Computer Science

Department of Computer Science

Jean Honorio

Purdue University

West Lafayette  IN 47907
jhonorio@purdue.edu

Chuyang Ke

Purdue University

West Lafayette  IN 47907

cke@purdue.edu

Abstract

We analyze the information-theoretic limits for the recovery of node labels in
several network models. This includes the Stochastic Block Model  the Expo-
nential Random Graph Model  the Latent Space Model  the Directed Preferential
Attachment Model  and the Directed Small-world Model. For the Stochastic Block
Model  the non-recoverability condition depends on the probabilities of having
edges inside a community  and between different communities. For the Latent
Space Model  the non-recoverability condition depends on the dimension of the
latent space  and how far and spread are the communities in the latent space. For
the Directed Preferential Attachment Model and the Directed Small-world Model 
the non-recoverability condition depends on the ratio between homophily and
neighborhood size. We also consider dynamic versions of the Stochastic Block
Model and the Latent Space Model.

1

Introduction

Network models have already become a powerful tool for researchers in various ﬁelds. With the rapid
expansion of online social media including Twitter  Facebook  LinkedIn and Instagram  researchers
now have access to more real-life network data and network models are great tools to analyze the vast
amount of interactions [16  2  1  21]. Recent years have seen the applications of network models in
machine learning [5  33  23]  bioinformatics [9  15  11]  as well as in social and behavioral researches
[26  14].
Among these literatures one of the central problems related to network models is community detection.
In a typical network model  nodes represent individuals in a social network  and edges represent
interpersonal interactions. The goal of community detection is to recover the label associated with
each node (i.e.  the community where each node belongs to). The exact recovery of 100% of the labels
has always been an important research topic in machine learning  for instance  see [2  10  20  27].
One particular issue researchers care about in the recovery of network models is the relation between
the number of nodes  and the proximity between the likelihood of connecting within the same
community and across different communities. For instance  consider the Stochastic Block Model
(SBM)  in which p is the probability for connecting two nodes in the same community  and q is the
probability for connecting two nodes in different communities. Clearly if p equals q  it is impossible
to identify the communities  or equivalently  to recover the labels for all nodes. Intuitively  as the
difference between p and q increases  labels are easier to be recovered.
In this paper  we analyze the information-theoretic limits for community detection. Our main contri-
bution is the comprehensive study of several network models used in the literature. To accomplish
that task  we carefully construct restricted ensembles. The key idea of using restricted ensembles is
that for any learning problem  if a subclass of models is difﬁcult to be learnt  then the original class

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Comparison of network models (S - static; UD - undirected dynamic; DD - directed dynamic)

Type Model

S

SBM

Our Result
(p−q)2
q(1−q) ≤ 2 log 2

n − 4 log 2

n2

S
S
UD
UD
DD
DD

(4σ2 + 1)−1−p/2(cid:107)µ(cid:107)2

2(cosh β − 1) ≤ 2 log 2

n − 4 log 2
ERGM
2 ≤ log 2
LSM
q(1−q) ≤ (n−2) log 2
(p−q)2
DSBM
n2−n
2 ≤ (n−2) log 2
(4σ2 + 1)−1−p/2(cid:107)µ(cid:107)2
DLSM
4n2−4n
(s + 1)/8m ≤ 2(n−2)/(n2−n)/n2
DPAM
DSWM (s + 1)2/(mp(1 − p)) ≤ 22(n−2)/n2

2n − log 2

n2

n2

/n

Previous Result
(p−q)2
p+q ≤ 2
n[25]
(p−q)2
q(1−q) ≤ O( 1
n )[10]
Novel
Novel
Novel
Novel
Novel
Novel

Thm. No.

Thm. 1

Cor. 1
Thm. 2
Thm. 3
Thm. 4
Thm. 5
Thm. 6

of models will be at least as difﬁcult to be learnt. The use of restricted ensembles is customary for
information-theoretic lower bounds [28  31].
We provide a series of novel results in this paper. While the information-theoretic limits of the
Stochastic Block Model have been heavily studied (in slightly different ways)  none of the other
models considered in this paper have been studied before. Thus  we provide new information-theoretic
results for the Exponential Random Graph Model (ERGM)  the Latent Space Model (LSM)  the
Directed Preferential Attachment Model (DPAM)  and the Directed Small-world Model (DSWM).
We also provide new results for dynamic versions of the Stochastic Block Model (DSBM) and the
Latent Space Model (DLSM).
Table 1 summarizes our results.

2 Static Network Models

In this section we analyze the information-theoretic limits for two static network models:
the
Stochastic Block Model (SBM) and the Latent Space Model (LSM). Furthermore  we include a
particular case of the Exponential Random Graph Model (ERGM) as a corollary of our results for the
SBM. We call these static models  because in these models edges are independent of each other.

2.1 Stochastic Block Model

Among different network models the Stochastic Block Model (SBM) has received particular attention.
Variations of the Stochastic Block Model include  for example  symmetric SBMs [3]  binary SBMs
[27  13]  labelled SBMs [36  20  34  18]  and overlapping SBMs [4]. For regular SBMs [25] and
[10] showed that under certain conditions recovering the communities in a SBM is fundamentally
impossible. Our analysis for the Stochastic Block Model follows the method used in [10] but we
analyze a different regime. In [10]  two clusters are required to have the equal size (Planted Bisection
Model)  while in our SBM setup  nature picks the label of each node uniformly at random. Thus in
our model only the expectation of the sizes of the two communities are equal.
We now deﬁne the Stochastic Block Model  which has two parameters p and q.
Deﬁnition 1 (Stochastic Block Model). Let 0 < q < p < 1. A Stochastic Block Model with
parameters (p  q) is an undirected graph of n nodes with the adjacency matrix A  where each
Aij ∈ {0  1}. Each node is in one of the two classes {+1 −1}. The distribution of true labels
i is assigned to +1 with probability 0.5  and −1 with
Y ∗ = (y∗
probability 0.5.
The adjacency matrix A is distributed as follows: if y∗
otherwise Aij is Bernoulli with parameter q.
The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the
observation of A. We are interested in the information-theoretic lower bounds. Thus  we deﬁne the
Markov chain Y ∗ → A → ˆY . Using Fano’s inequality  we obtain the following results.

j then Aij is Bernoulli with parameter p;

n) is uniform  i.e.  each label y∗

1  . . .   y∗

i = y∗

2

Theorem 1. In a Stochastic Block Model with parameters (p  q) with 0 < q < p < 1  if

(p − q)2
q(1 − q)

≤ 2 log 2

n

− 4 log 2
n2

 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .
Notice that our result for the Stochastic Block Model is similar to the one in [10]. This means that
the method of generating labels does not affect the information-theoretic bound.

2.2 Exponential Random Graph Model

form: P(A) = exp(φ(A))/(cid:80)

Exponential Random Graph Models (ERGMs) are a family of distributions on graphs of the following
A(cid:48) exp(φ(A(cid:48)))  where φ : {0  1}n×n → R is some potential function
over graphs. Selecting different potential functions enables ERGMs to model various structures in
network graphs  for instance  the potential function can be a sum of functions over edges  triplets 
cliques  among other choices [16].
In this section we analyze a special case of the Exponential Random Graph Model as a corollary of
our results for the Stochastic Block Model  in which the potential function is deﬁned as a sum of
φij(yi  yj)  where φij(yi  yj) = βyiyj and β > 0
i j βAijyiyj. This leads to the

functions over edges. That is  φ(A) =(cid:80)
is a parameter. Simplifying the expression above  we have φ(A) =(cid:80)

i j|Aij=1

following deﬁnition.
Deﬁnition 2 (Exponential Random Graph Model). Let β > 0. An Exponential Random Graph
Model with parameter β is an undirected graph of n nodes with the adjacency matrix A  where
each Aij ∈ {0  1}. Each node is in one of the two classes {+1 −1}. The distribution of true labels
i is assigned to +1 with probability 0.5  and −1 with
Y ∗ = (y∗
probability 0.5.

The adjacency matrix A is distributed as follows: P(A|Y ) = exp(β(cid:80)
Z(β) =(cid:80)

A(cid:48)∈{0 1}n×n exp(β(cid:80)

n) is uniform  i.e.  each label y∗

i<j Aijyiyj)/Z(β)  where

i<j A(cid:48)

ijyiyj).

1  . . .   y∗

The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the
observation of A. Theorem 1 leads to the following result.

Corollary 1. In an Exponential Random Graph Model with parameter β > 0  if

2(cosh β − 1) ≤ 2 log 2

n

− 4 log 2
n2

 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

2.3 Latent Space Model

The Latent Space Model (LSM) was ﬁrst proposed by [19]. The core assumption of the model is that
each node has a low-dimensional latent vector associated with it. The latent vectors of nodes in the
same community follow a similar pattern. The connectivity of two nodes in the Latent Space Model is
determined by the distance between their corresponding latent vectors. Previous works on the Latent
Space Model [30] analyzed asymptotic sample complexity  but did not focus on information-theoretic
limits for exact recovery.
We now deﬁne the Latent Space Model  which has three parameters σ > 0  d ∈ Z+ and µ ∈ Rd 
µ (cid:54)= 0.
Deﬁnition 3 (Latent Space Model). Let d ∈ Z+  µ ∈ Rd and µ (cid:54)= 0  σ > 0. A Latent Space Model
with parameters (d  µ  σ) is an undirected graph of n nodes with the adjacency matrix A  where
each Aij ∈ {0  1}. Each node is in one of the two classes {+1 −1}. The distribution of true labels
i is assigned to +1 with probability 0.5  and −1 with
Y ∗ = (y∗
probability 0.5.

n) is uniform  i.e.  each label y∗

1  . . .   y∗

3

For every node i  nature generates a latent d-dimensional vector zi ∈ Rd according to the Gaussian
distribution Nd(yiµ  σ2I).
The adjacency matrix A is distributed as follows: Aij is Bernoulli with parameter exp(−(cid:107)zi − zj(cid:107)2
2).

The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the observa-
tion of A. Notice that we do not have access to Z. we are interested in the information-theoretic
lower bounds. Thus  we deﬁne the Markov chain Y ∗ → A → ˆY . Fano’s inequality and a proper
conversion of the above model lead to the following theorem.

Theorem 2. In a Latent Space Model with parameters (d  µ  σ)  if

(4σ2 + 1)−1−d/2(cid:107)µ(cid:107)2

2 ≤ log 2
2n

− log 2
n2  

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

3 Dynamic Network Models

In this section we analyze the information-theoretic limits for two dynamic network models: the
Dynamic Stochastic Block Model (DSBM) and the Dynamic Latent Space Model (DLSM). We call
these dynamic models  because we assume there exists some ordering for edges  and the distribution
of each edge not only depends on its endpoints  but also depends on previously generated edges.
We start by giving the deﬁnition of predecessor sets. Notice that the following deﬁnition of predecessor
sets employs a lexicographic order  and the motivation is to use it as a subclass to provide a bound
for general dynamic models. Fano’s inequality is usually used for a restricted ensemble  i.e.  a
subclass of the original class of interest. If a subclass (e.g.  dynamic SBM or LSM with a particular
predecessor set τ) is difﬁcult to be learnt  then the original class (SBMs or LSMs with general
dynamic interactions) will be at least as difﬁcult to be learnt. The use of restricted ensembles is
customary for information-theoretic lower bounds [28  31].
Deﬁnition 4. For every pair i and j with i < j  we denote its predecessor set using τi j  where

τij ⊆ {(k  l)|(k < l) ∧ (k < i ∨ (k = i ∧ l < j))}

and

Aτij = {Akl|(k  l) ∈ τij}.

In a dynamic model  the probability distribution of each edge Aij not only depends on the labels of
nodes i and j (i.e.  y∗
Next  we prove the following lemma using the deﬁnition above.
Lemma 1. Assume now the probability distribution of A given labeling Y is P(A|Y ) =

j )  but also on the previously generated edges Aτij .

i and y∗

P(Aij|Aτij   yi  yj). Then for any labeling Y and Y (cid:48)  we have

i<j

KL(PA|Y (cid:107)PA|Y (cid:48)) ≤

KL(PAij|Aτij  yi yj(cid:107)PAij|Aτij  y(cid:48)
i y(cid:48)

j

).

max
i j

(cid:18)n

(cid:19)

2

(cid:81)

(cid:81)

Similarly 

if

the probability distribution of A given labeling Y

is P(A|Y )

=

i<j

(cid:19)
P(Aij|Aτij   y1  . . .   yj)  we have

(cid:18)n

KL(PA|Y (cid:107)PA|Y (cid:48)) ≤

2

KL(PAij|Aτij  y1 ... yj(cid:107)PAij|Aτij  y(cid:48)

1 ... y(cid:48)

j

).

max
i j

4

3.1 Dynamic Stochastic Block Model

1  . . .   y∗
i is assigned to +1 with probability 0.5  and −1 with probability 0.5.

The Dynamic Stochastic Block Model (DSBM) shares a similar setting with the Stochastic Block
Model  except that we take the predecessor sets into consideration.
Deﬁnition 5 (Dynamic Stochastic Block Model). Let 0 < q < p < 1. Let F = {fk}(n
2)
k=0 be a set of
functions  where fk : {0  1}k → (0  1]. A Dynamic Stochastic Block Model with parameters (p  q  F )
is an undirected graph of n nodes with the adjacency matrix A  where each Aij ∈ {0  1}. Each node
is in one of the two classes {+1 −1}. The distribution of true labels Y ∗ = (y∗
n) is uniform 
i.e.  each label y∗
The adjacency matrix A is distributed as follows: if y∗
pf|τij|(Aτij ); otherwise Aij is Bernoulli with parameter qf|τij|(Aτij ).
The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the
observation of A. We are interested in the information-theoretic lower bounds. Thus  we deﬁne
the Markov chain Y ∗ → A → ˆY . Using Fano’s inequality and Lemma 1  we obtain the following
results.

j then Aij is Bernoulli with parameter

i = y∗

Theorem 3. In a Dynamic Stochastic Block Model with parameters (p  q) with 0 < q < p < 1  if

(p − q)2
q(1 − q)

≤ n − 2
n2 − n

log 2 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

3.2 Dynamic Latent Space Model

1  . . .   y∗

n) is uniform  i.e.  each label y∗

The Dynamic Latent Space Model (DLSM) shares a similar setting with the Latent Space Model 
except that we take the predecessor sets into consideration.
Deﬁnition 6 (Dynamic Latent Space Model). Let d ∈ Z+  µ ∈ Rd and µ (cid:54)= 0  σ > 0. Let
F = {fk}(n
k=0 be a set of functions  where fk : {0  1}k → (0  1]. A Latent Space Model with
2)
parameters (d  µ  σ  F ) is an undirected graph of n nodes with the adjacency matrix A  where each
Aij ∈ {0  1}. Each node is in one of the two classes {+1 −1}. The distribution of true labels
i is assigned to +1 with probability 0.5  and −1 with
Y ∗ = (y∗
probability 0.5.
For every node i  nature generates a latent d-dimensional vector zi ∈ Rd according to the Gaussian
distribution Nd(yiµ  σ2I).
The adjacency matrix A is distributed as follows: Aij is Bernoulli with parameter f|τij|(Aτij ) ·
exp(−(cid:107)zi − zj(cid:107)2
2).
The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the observa-
tion of A. Notice that we do not have access to Z. We are interested in the information-theoretic
lower bounds. Thus  we deﬁne the Markov chain Y ∗ → A → ˆY . Using Fano’s inequality and
Lemma 1  our analysis leads to the following theorem.

Theorem 4. In a Dynamic Latent Space Model with parameters (d  µ  σ {fk})  if

(4σ2 + 1)−1−d/2(cid:107)µ(cid:107)2

2 ≤ n − 2
4(n2 − n)

log 2 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

5

4 Directed Network Models

In this section we analyze the information-theoretic limits for two directed network models: the
Directed Preferential Attachment Model (DPAM) and the Directed Small-world Model (DSWM). In
contrast to previous sections  here we consider directed graphs.
Note that in social networks such as Twitter  the graph is directed. That is  each user follows other
users. Users that are followed by many others (i.e.  nodes with high out-degree) are more likely to be
followed by new users. This is the case of popular singers  for instance. Additionally  a new user will
follow people with similar preferences. This is referred in the literature as homophily. In our case  a
node with positive label will more likely follow nodes with positive label  and vice versa.
The two models deﬁned in this section will require an expected number of in-neighbors m  for each
node. In order to guarantee this in a setting in which nodes decide to connect to at most k > m nodes
independently  one should guarantee that the probability of choosing each of the k nodes is less than
or equal to 1/m.

The above motivates an algorithm that takes a vector in the k-simplex (i.e.  w ∈ Rk and(cid:80)k
and produces another vector in the k-simplex (i.e.  ˜w ∈ Rk (cid:80)k

i=1 wi = 1)
i=1 ˜wi = 1 and for all i  ˜wi ≤ 1/m).

Consider the following optimization problem:

minimize

˜w

1
2

( ˜wi − wi)2

subject to 0 ≤ ˜wi ≤ 1
m

for all i

k(cid:88)

i=1

k(cid:88)

i=1

˜wi = 1.

which is solved by the following algorithm:
Algorithm 1: k-simplex
input

:vector w ∈ Rk where(cid:80)k
output :vector ˜w ∈ Rk where(cid:80)k

expected number of in-neighbors m ≤ k

i=1 wi = 1 
i=1 ˜wi = 1 and ˜wi ≤ 1/m for all i

˜wi ← wi;

1 for i ∈ {1  . . .   k} do
2
3 end
4 for i ∈ {1  . . .   k} such that ˜wi > 1

m do

S ← ˜wi − 1
m;
˜wi ← 1
m;
Distribute S evenly across all j ∈ {1  . . .   k} such that ˜wj < 1
m;

5
6
7
8 end
One important property that we will use in our proofs is that mini ˜wi ≥ mini wi  as well as
maxi ˜wi ≤ maxi wi.

4.1 Directed Preferential Attachment Model

Here we consider a Directed Preferential Attachment Model (DPAM) based on the classic Preferential
Attachment Model [7]. While in the classic model every mode has exactly m neighbors  in our model
the expected number of in-neighbors is m.
Deﬁnition 7 (Directed Preferential Attachment Model). Let m be a positive integer with 0 < m (cid:28) n.
Let s > 0 be the homophily parameter. A Directed Preferential Attachment Model with parameters
(m  s) is a directed graph of n nodes with the adjacency matrix A  where each Aij ∈ {0  1}. Each
node is in one of the two classes {+1 −1}. The distribution of true labels Y ∗ = (y∗
n) is
uniform  i.e.  each label y∗
Nodes 1 through m are not connected to each other  and they all have an in-degree of 0. For
node i from m + 1 to n  nature ﬁrst generates the weight wji for each node j < i  where wji ∝

1  . . .   y∗
i is assigned to +1 with probability 0.5  and −1 with probability 0.5.

6

((cid:80)i−1

j ]s + 1)  and(cid:80)i−1

k=1 Ajk + 1)(1[y∗

i = y∗

j=1 wji = 1. Then every node j < i connects to node
j ) = m ˜wji  where ( ˜w1i... ˜wi−1 i) is

i with the following probability: P(Aji = 1 | Aτij   y∗
computed from (w1i...wi−1 i) as in Algorithm 1.
The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the
observation of A. We are interested in the information-theoretic lower bounds. Thus  we deﬁne the
Markov chain Y ∗ → A → ˆY . Using Fano’s inequality  we obtain the following results.

1  . . .   y∗

Theorem 5. In a Directed Preferential Attachment Model with parameters (m  s)  if

s + 1
8m

≤ 2(n−2)/(n2−n)

n2

 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

4.2 Directed Small-world Model

1  . . .   y∗

n) is uniform  i.e.  each label y∗

Here we consider a Directed Small-world Model (DSWM) based on the classic small-world phe-
nomenon [32]. While in the classic model every mode has exactly m neighbors  in our model the
expected number of in-neighbors is m.
Deﬁnition 8 (Directed Small-world Model). Let m be a positive integer with 0 < m (cid:28) n. Let s > 0
be the homophily parameter. Let p be the mixture parameter with 0 < p < 1. A Directed Small-world
Model with parameters (m  s  p) is a directed graph of n nodes with the adjacency matrix A  where
each Aij ∈ {0  1}. Each node is in one of the two classes {+1 −1}. The distribution of true labels
i is assigned to +1 with probability 0.5  and −1 with
Y ∗ = (y∗
probability 0.5.
Nodes 1 through m are not connected to each other  and they all have an in-degree of 0. For node i
from m + 1 to n  nature ﬁrst generates the weight wji for each node j < i  where wji ∝ (1[y∗
i =
wji = 1 − p. Then every node j < i connects to node
y∗
i with the following probability: P(Aji = 1 | Aτij   y∗
j ) = m ˜wji  where ( ˜w1i... ˜wi−1 i) is
computed from (w1i...wi−1 i) as in Algorithm 1.
The goal is to recover labels ˆY = (ˆy1  . . .   ˆyn) that are equal to the true labels Y ∗  given the
observation of A. We are interested in the information-theoretic lower bounds. Thus  we deﬁne the
Markov chain Y ∗ → A → ˆY . Using Fano’s inequality  we obtain the following results.

j=i−m wji = p (cid:80)i−m−1

j ]s + 1)  and(cid:80)i−1

1  . . .   y∗

j=1

Theorem 6. In a Directed Small-world Model with parameters (m  s  p)  if

(s + 1)2
mp(1 − p)

≤ 22(n−2)/n2

n

 

then we have that for any algorithm that a learner could use for picking ˆY   the probability of error
P( ˆY (cid:54)= Y ∗) is greater than or equal to 1
2 .

5 Concluding Remarks

In the past decade a lot of effort has been made in the Stochastic Block Model (SBM) community to
ﬁnd polynomial time algorithms for the exact recovery. For example  [2  10] provided analyses to
various parameter regimes in symmetric SBMs  and showed that some easy regimes could be solved
in polynomial time using semideﬁnite programming relaxation; [3] also provides quasi-linear time
algorithms for SBMs; [17] and [6] discovered the existence of phase transition in the exact recovery
of symmetric SBMs. All of the aforementioned literature has mathematical guarantees of statistical
and computational efﬁciency. There exists algorithms without formal guarantees  for example 
[16] introduced some MCMC-based methods. Other heuristic algorithms include Kernighan-Lin’s
algorithm  METIS  Local Spectral Partitioning  etc. (See e.g.  [22] for reference.)

7

We want to highlight that community detection for undirected models could be viewed as a special
case of the Markov random ﬁeld (MRF) inference problem. In the MRF model  if the pairwise
potentials are submodular  the problem could be solved exactly in polynomial time via graph cuts in
the case of two communities [8].
Regarding our contributions  we highlight that the entries in the adjacency matrix A are not inde-
pendent in several models considered in our paper  including the Dynamic Stochastic Block Model 
the Dynamic Latent Space Model  the Directed Preferential Attachment Model and the Directed
Small-world Model. Also  in the Latent Space Model and the Dynamic Latent Space Model  we
have additional latent variables. Furthermore  in the Directed Preferential Attachment Model and
the Directed Small-world Model  an entry in A also depends on several entries in Y ∗ to account for
homophily.
Our research could be extended in several ways. First  our models only involve two symmetric clusters.
For the Latent Space Model and dynamic models  it might be interesting to analyze the case with
multiple clusters. Some more complicated models involving Markovian assumptions  for example 
the Dynamic Social Network in Latent Space model [29]  can also be analyzed. We acknowledge
that the information-theoretic lower bounds we provide in this paper may not be necessarily tight. It
would be interesting to analyze phase transitions and information-computational gaps for the new
models.

References
[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments.

arXiv preprint arXiv:1703.10146  2017.

[2] Emmanuel Abbe  Afonso S Bandeira  and Georgina Hall. Exact recovery in the stochastic block

model. IEEE Transactions on Information Theory  62(1):471–487  2016.

[3] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
Fundamental limits and efﬁcient algorithms for recovery. In Foundations of Computer Science
(FOCS)  2015 IEEE 56th Annual Symposium on  pages 670–688. IEEE  2015.

[4] Edoardo M Airoldi  David M Blei  Stephen E Fienberg  and Eric P Xing. Mixed membership

stochastic blockmodels. Journal of Machine Learning Research  9(Sep):1981–2014  2008.

[5] Brian Ball  Brian Karrer  and Mark EJ Newman. Efﬁcient and principled method for detecting

communities in networks. Physical Review E  84(3):036103  2011.

[6] Afonso S Bandeira. Random laplacian matrices and convex relaxations. Foundations of

Computational Mathematics  18(2):345–379  2018.

[7] Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. science 

286(5439):509–512  1999.

[8] Yuri Boykov and Olga Veksler. Graph cuts in vision and graphics: Theories and applications.

In Handbook of mathematical models in computer vision  pages 79–96. Springer  2006.

[9] Irineo Cabreros  Emmanuel Abbe  and Aristotelis Tsirigos. Detecting community structures in
Hi-C genomic data. In Information Science and Systems (CISS)  2016 Annual Conference on 
pages 584–589. IEEE  2016.

[10] Yudong Chen and Jiaming Xu. Statistical-computational phase transitions in planted models:
The high-dimensional setting. In International Conference on Machine Learning  pages 244–
252  2014.

[11] Melissa S Cline  Michael Smoot  Ethan Cerami  Allan Kuchinsky  Nerius Landys  Chris
Workman  Rowan Christmas  Iliana Avila-Campilo  Michael Creech  Benjamin Gross  et al.
Integration of biological networks and gene expression data using Cytoscape. Nature protocols 
2(10):2366  2007.

[12] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons 

2012.

8

[13] Yash Deshpande  Emmanuel Abbe  and Andrea Montanari. Asymptotic mutual information
for the binary stochastic block model. In Information Theory (ISIT)  2016 IEEE International
Symposium on  pages 185–189. IEEE  2016.

[14] Santo Fortunato. Community detection in graphs. Physics reports  486(3-5):75–174  2010.

[15] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks.

Proceedings of the national academy of sciences  99(12):7821–7826  2002.

[16] Anna Goldenberg  Alice X Zheng  Stephen E Fienberg  Edoardo M Airoldi  et al. A survey
of statistical network models. Foundations and Trends R(cid:13) in Machine Learning  2(2):129–233 
2010.

[17] Bruce Hajek  Yihong Wu  and Jiaming Xu. Achieving exact cluster recovery threshold via
semideﬁnite programming. IEEE Transactions on Information Theory  62(5):2788–2797  2016.

[18] Simon Heimlicher  Marc Lelarge  and Laurent Massoulié. Community detection in the labelled
stochastic block model. NIPS Workshop on Algorithmic and Statistical Approaches for Large
Social Networks  2012.

[19] Peter D Hoff  Adrian E Raftery  and Mark S Handcock. Latent space approaches to social
network analysis. Journal of the american Statistical association  97(460):1090–1098  2002.

[20] Varun Jog and Po-Ling Loh. Information-theoretic bounds for exact recovery in weighted
stochastic block models using the Renyi divergence. IEEE Allerton Conference on Communica-
tion  Control  and Computing  2015.

[21] Bomin Kim  Kevin Lee  Lingzhou Xue  and Xiaoyue Niu. A review of dynamic network models

with latent variables. arXiv preprint arXiv:1711.10421  2017.

[22] Jure Leskovec  Kevin J Lang  and Michael Mahoney. Empirical comparison of algorithms for
network community detection. In Proceedings of the 19th international conference on World
wide web  pages 631–640. ACM  2010.

[23] Greg Linden  Brent Smith  and Jeremy York. Amazon. com recommendations: Item-to-item

collaborative ﬁltering. IEEE Internet computing  7(1):76–80  2003.

[24] Arakaparampil M Mathai and Serge B Provost. Quadratic forms in random variables: theory

and applications. Dekker  1992.

[25] Elchanan Mossel  Joe Neeman  and Allan Sly. Stochastic block models and reconstruction.

arXiv preprint arXiv:1202.1499  2012.

[26] Mark EJ Newman  Duncan J Watts  and Steven H Strogatz. Random graph models of social

networks. Proceedings of the National Academy of Sciences  99(suppl 1):2566–2572  2002.

[27] Hussein Saad  Ahmed Abotabl  and Aria Nosratinia. Exact recovery in the binary stochastic
block model with binary side information. IEEE Allerton Conference on Communication 
Control  and Computing  2017.

[28] Narayana P Santhanam and Martin J Wainwright. Information-theoretic limits of selecting
IEEE Transactions on Information Theory 

binary graphical models in high dimensions.
58(7):4117–4134  2012.

[29] Purnamrita Sarkar and Andrew W Moore. Dynamic social network analysis using latent space

models. In Advances in Neural Information Processing Systems  pages 1145–1152  2006.

[30] Minh Tang  Daniel L Sussman  Carey E Priebe  et al. Universally consistent vertex classiﬁcation

for latent positions graphs. The Annals of Statistics  41(3):1406–1430  2013.

[31] Wei Wang  Martin J Wainwright  and Kannan Ramchandran. Information-theoretic bounds on
model selection for gaussian markov random ﬁelds. In Information Theory Proceedings (ISIT) 
2010 IEEE International Symposium on  pages 1373–1377. IEEE  2010.

9

[32] Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. nature 

393(6684):440  1998.

[33] Rui Wu  Jiaming Xu  Rayadurgam Srikant  Laurent Massoulié  Marc Lelarge  and Bruce Hajek.
Clustering and inference from pairwise comparisons. In ACM SIGMETRICS Performance
Evaluation Review  volume 43  pages 449–450. ACM  2015.

[34] Jiaming Xu  Laurent Massoulié  and Marc Lelarge. Edge label inference in generalized
In Conference on

stochastic block models: from spectral theory to impossibility results.
Learning Theory  pages 903–920  2014.

[35] Bin Yu. Assouad  Fano  and Le Cam. Festschrift for Lucien Le Cam  423:435  1997.

[36] Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic

block model. In Advances in Neural Information Processing Systems  pages 965–973  2016.

10

,Chuyang Ke
Jean Honorio