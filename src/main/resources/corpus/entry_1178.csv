2019,Convergent Policy Optimization for Safe Reinforcement Learning,We study the safe reinforcement learning problem with nonlinear function approximation  where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem  we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators.  We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore  to extend our theoretical results  we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.,Convergent Policy Optimization for Safe

Reinforcement Learning

Ming Yu ⇤

Zhuoran Yang †

Mladen Kolar ‡

Zhaoran Wang §

Abstract

We study the safe reinforcement learning problem with nonlinear function approx-
imation  where policy optimization is formulated as a constrained optimization
problem with both the objective and the constraint being nonconvex functions.
For such a problem  we construct a sequence of surrogate convex constrained
optimization problems by replacing the nonconvex functions locally with convex
quadratic functions obtained from policy gradient estimators. We prove that the
solutions to these surrogate problems converge to a stationary point of the original
nonconvex problem. Furthermore  to extend our theoretical results  we apply our
algorithm to examples of optimal control and multi-agent reinforcement learning
with safety constraints.

1

Introduction

Reinforcement learning [58] has achieved tremendous success in video games [41  44  56  36  66]
and board games  such as chess and Go [53  55  54]  in part due to powerful simulators [8  62]. In
contrast  due to physical limitations  real-world applications of reinforcement learning methods often
need to take into consideration the safety of the agent [5  26]. For instance  in expensive robotic
and autonomous driving platforms  it is pivotal to avoid damages and collisions [25  9]. In medical
applications  we need to consider the switching cost [7].
A popular model of safe reinforcement learning is the constrained Markov decision process (CMDP) 
which generalizes the Markov decision process by allowing for inclusion of constraints that model
the concept of safety [3]. In a CMDP  the cost is associated with each state and action experienced
by the agent  and safety is ensured only if the expected cumulative cost is below a certain threshold.
Intuitively  if the agent takes an unsafe action at some state  it will receive a huge cost that punishes
risky attempts. Moreover  by considering the cumulative cost  the notion of safety is deﬁned for the
whole trajectory enabling us to examine the long-term safety of the agent  instead of focusing on
individual state-action pairs. For a CMDP  the goal is to take sequential decisions to achieve the
expected cumulative reward under the safety constraint.
Solving a CMDP can be written as a linear program [3]  with the number of variables being the
same as the size of the state and action spaces. Therefore  such an approach is only feasible for the
tabular setting  where we can enumerate all the state-action pairs. For large-scale reinforcement
learning problems  where function approximation is applied  both the objective and constraint of the
CMDP are nonconvex functions of the policy parameter. One common method for solving CMDP
is to formulate an unconstrained saddle-point optimization problem via Lagrangian multipliers and
solve it using policy optimization algorithms [18  60]. Such an approach suffers the following two
drawbacks:

⇤The University of Chicago Booth School of Business  Chicago  IL. Email: ming93@uchicago.edu.
†Department of Operations Research and Financial Engineering  Princeton University  Princeton  NJ.
‡The University of Chicago Booth School of Business  Chicago  IL.
§Department of Industrial Engineering and Management Sciences  Northwestern University  Evanston  IL.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

First  for each ﬁxed Lagrangian multiplier  the inner minimization problem itself can be viewed
as solving a new reinforcement learning problem. From the computational point of view  solving
the saddle-point optimization problem requires solving a sequence of MDPs with different reward
functions. For a large scale problem  even solving a single MDP requires huge computational
resources  making such an approach computationally infeasible.
Second  from a theoretical perspective  the performance of the saddle-point approach hinges on
solving the inner problem optimally. Existing theory only provides convergence to a stationary point
where the gradient with respect to the policy parameter is zero [28  37]. Moreover  the objective  as a
bivariate function of the Lagrangian multiplier and the policy parameter  is not convex-concave and 
therefore  ﬁrst-order iterative algorithms can be unstable [27].
In contrast  we tackle the nonconvex constrained optimization problem of the CMDP directly. We
propose a novel policy optimization algorithm  inspired by [38]. Speciﬁcally  in each iteration  we
replace both the objective and constraint by quadratic surrogate functions and update the policy
parameter by solving the new constrained optimization problem. The two surrogate functions can
be viewed as ﬁrst-order Taylor-expansions of the expected reward and cost functions where the
gradients are estimated using policy gradient methods [59]. Additionally  they can be viewed as
convex relaxations of the original nonconvex reward and cost functions. In §4 we show that  as the
algorithm proceeds  we obtain a sequence of convex relaxations that gradually converge to a smooth
function. More importantly  the sequence of policy parameters converges almost surely to a stationary
point of the nonconvex constrained optimization problem.

Related work. Our work is pertinent to the line of research on CMDP [3]. For CMDPs with
large state and action spaces  [19] proposed an iterative algorithm based on a novel construction of
Lyapunov functions. However  their theory only holds for the tabular setting. Using Lagrangian
multipliers  [46  18  1  60] proposed policy gradient [59]  actor-critic [33]  or trust region policy
optimization [51] methods for CMDP or constrained risk-sensitive reinforcement learning [26]. These
algorithms either do not have convergence guarantees or are shown to converge to saddle-points of the
Lagrangian using two-time-scale stochastic approximations [10]. However  due to the projection on
the Lagrangian multiplier  the saddle-point achieved by these approaches might not be the stationary
point of the original CMDP problem. In addition  [65] proposed a cross-entropy-based stochastic
optimization algorithm  and proved the asymptotic behavior using ordinary differential equations.
In contrast  our algorithm and the theoretical analysis focus on the discrete time CMDP. Outside
of the CMDP setting  [31  35] studied safe reinforcement learning with demonstration data  [61]
studied the safe exploration problem with different safety constraints  and [4] studied multi-task safe
reinforcement learning.

Our contribution. Our contribution is three-fold. First  for the CMDP policy optimization problem
where both the objective and constraint function are nonconvex  we propose to optimize a sequence
of convex relaxation problems using convex quadratic functions. Solving these surrogate problems
yields a sequence of policy parameters that converge almost surely to a stationary point of the original
policy optimization problem. Second  to reduce the variance in the gradient estimator that is used to
construct the surrogate functions  we propose an online actor-critic algorithm. Finally  as concrete
applications  our algorithms are also applied to optimal control (in §5) and parallel and multi-agent
reinforcement learning problems with safety constraints (in supplementary material).

2 Background
A Markov decision process is denoted by (S A  P   r  µ )  where S is the state space  A is the action
space  P is the transition probability distribution   2 (0  1) is the discount factor  r : S⇥A! R is
the reward function  and µ 2P (S) is the distribution of the initial state s0 2S   where we denote
P(X ) as the set of probability distributions over X for any X . A policy is a mapping ⇡ : S!P (A)
that speciﬁes the action that an agent will take when it is at state s.
Policy gradient method. Let {⇡✓ : S!P (A)} be a parametrized policy class  where ✓ 2 ⇥
is the parameter deﬁned on a compact set ⇥. This parameterization transfers the original inﬁnite
dimensional policy class to a ﬁnite dimensional vector space and enables gradient based methods
to be used to maximize (1). For example  the most popular Gaussian policy can be written as
⇡(·|s  ✓) = Nµ(s  ✓)  (s  ✓)  where the state dependent mean µ(s  ✓) and standard deviation

2

(s  ✓) can be further parameterized as µ(s  ✓) = ✓>µ · x(s) and (s  ✓) = exp✓> · x(s) with x(s)

being a state feature vector. The goal of an agent is to maximize the expected cumulative reward

where s0 ⇠ µ  and for all t  0  we have st+1 ⇠ P (·| st  at) and at ⇠ ⇡(·| st). Given a policy ⇡(✓) 
we deﬁne the state- and action-value functions of ⇡✓  respectively  as

R(✓) = E⇡Xt0

t · r(st  at) 
tr(st  at) s0 = s  Q✓(s  a) = E⇡✓Xt0
✓k+1 = ✓k + ⌘ ·br✓R(✓k) 

V ✓(s) = E⇡✓Xt0

The policy gradient method updates the parameter ✓ through gradient ascent

tr(st  at) s0 = s  a0 = a. (2)
where br✓R(✓k) is a stochastic estimate of the gradient r✓R(✓k) at k-th iteration. Policy gradient
method  as well as its variants (e.g. policy gradient with baseline [58]  neural policy gradient
[64  39  16]) is widely used in reinforcement learning. The gradient r✓R(✓) can be estimated
according to the policy gradient theorem [59] 

(1)

r✓R(✓) = Ehr✓ log ⇡✓(s  a) · Q✓(s  a)i.

Actor-critic method. To further reduce the variance of the policy gradient method  we could
estimate both the policy parameter and value function simultaneously. This kind of method is called
actor-critic algorithm [33]  which is widely used in reinforcement learning. Speciﬁcally  in the value
function evaluation (critic) step we estimate the action-value function Q✓(s  a) using  for example  the
temporal difference method TD(0) [20]. The policy parameter update (actor) step is implemented as
before by the Monte-Carlo method according to the policy gradient theorem (3) with the action-value
Q✓(s  a) replaced by the estimated value in the policy evaluation step.

Constrained MDP.
In this work  we consider an MDP problem with an additional constraint
on the model parameter ✓. Speciﬁcally  when taking action at some state we incur some cost
value. The constraint is such that the expected cumulative cost cannot exceed some pre-deﬁned
constant. A constrained Markov decision process (CMDP) is denoted by (S A  P   r  d  µ )  where
d : S⇥A! R is the cost function and the other parameters are as before. The goal of an the agent
in CMDP is to solve the following constrained problem

(3)

(4)

✓2⇥

minimize

J(✓) = E⇡✓Xt0
subject to D(✓) = E⇡✓Xt0

t · r(st  at) 
t · d(st  at)  D0 

where D0 is a ﬁxed constant. We consider only one constraint D(✓)  D0  noting that it is
straightforward to generalize to multiple constraints. Throughout this paper  we assume that both the

reward and cost value functions are bounded:r(st  at)  rmax andd(st  at)  dmax. Also  the

parameter space ⇥ is assumed to be compact.

3 Algorithm

In this section  we develop an algorithm to solve the optimization problem (4). Note that both the
objective function and the constraint in (4) are nonconvex and involve expectation without closed-
form expression. As a constrained problem  a straightforward approach to solve (4) is to deﬁne the
following Lagrangian function

and solve the dual problem

L(✓  ) = J(✓) +  ·⇥D(✓)  D0⇤ 

inf
0

sup

L(✓  ).

✓

3

However  this problem is a nonconvex minimax problem and  therefore  is hard to solve and establish
theoretical guarantees for solutions [2]. Another approach to solve (4) is to replace J(✓) and D(✓)
by surrogate functions with nice properties. For example  one can iteratively construct local quadratic
approximations that are strongly convex [52]  or are an upper bound for the original function [57].
However  an immediate problem of this naive approach is that  even if the original problem (4) is
feasible  the convex relaxation problem need not be. Also  these methods only deal with deterministic
and/or convex constraints.
In this work  we propose an iterative algorithm that approximately solves (4) by constructing a
sequence of convex relaxations  inspired by [38]. Our method is able to handle the possible infeasible
situation due to the convex relaxation as mentioned above  and handle stochastic and nonconvex
constraint. Since we do not have access to J(✓) or D(✓)  we ﬁrst deﬁne the sample negative
cumulative reward and cost functions as

J⇤(✓) = Xt0

t · r(st  at)

and

D⇤(✓) =Xt0

t · d(st  at).

Given ✓  J⇤(✓) and D⇤(✓) are the sample negative cumulative reward and cost value of a realization
(i.e.  a trajectory) following policy ⇡✓. Note that both J⇤(✓) and D⇤(✓) are stochastic due to the
randomness in the policy  state transition distribution  etc. With some abuse of notation  we use
J⇤(✓) and D⇤(✓) to denote both a function of ✓ and a value obtained by the realization of a trajectory.

Clearly we have J(✓) = E⇥J⇤(✓)⇤ and D(✓) = E⇥D⇤(✓)⇤.
We start from some (possibly infeasible) ✓0. Let ✓k denote the estimate of the policy parameter in
the k-th iteration. As mentioned above  we do not have access to the expected cumulative reward
J(✓). Instead we sample a trajectory following the current policy ⇡✓k and obtain a realization of the
negative cumulative reward value and the gradient of it as J⇤(✓k) and r✓J⇤(✓k)  respectively. The
cumulative reward value is obtained by Monte-Carlo estimation  and the gradient is also obtained by
Monte-Carlo estimation according to the policy gradient theorem in (3). We provide more details on
the realization step later in this section. Similarly  we use the same procedure for the cost function
and obtain realizations D⇤(✓k) and r✓D⇤(✓k).
We approximate J(✓) and D(✓) at ✓k by the quadratic surrogate functions

(5)
(6)

(7)

(8)

where ⌧> 0 is any ﬁxed constant. In each iteration  we solve the optimization problem

eJ(✓  ✓k ⌧ ) = J⇤(✓k) + hr✓J⇤(✓k) ✓  ✓ki + ⌧k✓  ✓kk2
eD(✓  ✓k ⌧ ) = D⇤(✓k) + hr✓D⇤(✓k) ✓  ✓ki + ⌧k✓  ✓kk2

2 
2 

✓k = argmin

J

(k)

(✓)

subject to

(k)

D

(✓)  D0 

✓

where we deﬁne

(k)

J
(k)

(✓) = (1  ⇢k) · J
(✓) = (1  ⇢k) · D

(0)

D
(0)

(k1)

(k1)

(✓) + ⇢k · eJ(✓  ✓k ⌧ ) 
(✓) + ⇢k · eD(✓  ✓k ⌧ ) 

(✓) = D

with the initial value J
(✓) = 0. Here ⇢k is the weight parameter to be speciﬁed later.
According to the deﬁnition (5) and (6)  problem (7) is a convex quadratically constrained quadratic
program (QCQP). Therefore  it can be efﬁciently solved by  for example  the interior point method.
However  as mentioned before  even if the original problem (4) is feasible  the convex relaxation
problem (7) could be infeasible. In this case  we instead solve the following feasibility problem

✓k = argmin

✓ ↵

↵

subject to

(k)

D

(✓)  D0 + ↵.

(9)

(k)

In particular  we relax the infeasible constraint and ﬁnd ✓k as the solution that gives the minimum
relaxation. Due to the speciﬁc form in (6)  D
(✓) is decomposable into quadratic forms of each
component of ✓  with no terms involving ✓i · ✓j. Therefore  the solution to problem (9) can be written
in a closed form. Given ✓k from either (7) or (9)  we update ✓k by
(10)
✓k+1 = (1  ⌘k) · ✓k + ⌘k · ✓k 
where ⌘k is the learning rate to be speciﬁed later. Note that although we consider only one constraint
in the algorithm  both the algorithm and the theoretical result in Section 4 can be directly generalized
to multiple constraints setting. The whole procedure is summarized in Algorithm 1.

4

Obtain a sample J⇤(✓k) and D⇤(✓k) by Monte-Carlo sampling.
Obtain a sample r✓J⇤(✓k) and r✓D⇤(✓k) by policy gradient theorem.
if problem (7) is feasible then

Algorithm 1 Successive convex relaxation algorithm for constrained MDP
1: Input: Initial value ✓0  ⌧  {⇢k} {⌘k}.
2: for k = 1  2  3  . . . do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for

end if
Update ✓k+1 by (10).

Obtain ✓k by solving (7).

Obtain ✓k by solving (9).

else

r✓J(✓) = E⇡✓hr✓ log ⇡✓(s  a) · Q✓(s  a)i.

Obtaining realizations J⇤(✓k) and r✓J⇤(✓k). We detail how to obtain realizations J⇤(✓k) and
r✓J⇤(✓k) corresponding to the lines 3 and 4 in Algorithm 1. The realizations of D⇤(✓k) and
r✓D⇤(✓k) can be obtained similarly.
First  we discuss ﬁnite horizon setting  where we can sample the full trajectory according to the
policy ⇡✓. In particular  for any ✓k  we use the policy ⇡✓k to sample a trajectory and obtain J⇤(✓k)
by Monte-Carlo method. The gradient r✓J(✓) can be estimated by the policy gradient theorem [59] 
(11)
Again we can sample a trajectory and obtain the policy gradient realization r✓J⇤(✓k) by Monte-Carlo
method.
In inﬁnite horizon setting  we cannot sample the inﬁnite length trajectory. In this case  we utilize the
truncation method introduced in [48]  which truncates the trajectory at some stage T and scales the
undiscounted cumulative reward to obtain an unbiased estimation. Intuitively  if the discount factor
 is close to 0  then the future reward would be discounted heavily and  therefore  we can obtain
an accurate estimate with a relatively small number of stages. On the other hand  if  is close to 1 
then the future reward is more important compared to the small  case and we have to sample a long
trajectory. Taking this intuition into consideration  we deﬁne T to be a geometric random variable
with parameter 1  : Pr(T = t) = (1  )t. Then  we simulate the trajectory until stage T and
use the estimator Jtruncate(✓) = (1  ) ·PT
t=0 r(st  at)  which is an unbiased estimator of the
expected negative cumulative reward J(✓)  as proved in proposition 5 in [43]. We can apply the same
truncation procedure to estimate the policy gradient r✓J(✓).
Variance reduction. Using the naive sampling method described above  we may suffer from high
variance problem. To reduce the variance  we can modify the above procedure in the following ways.
First  instead of sampling only one trajectory in each iteration  a more practical and stable way is to
sample several trajectories and take average to obtain the realizations. As another approach  we can
subtract a baseline function from the action-value function Q✓(s  a) in the policy gradient estimation
step (11) to reduce the variance without changing the expectation. A popular choice of the baseline
function is the state-value function V ✓(s) as deﬁned in (2). In this way  we can replace Q✓(s  a) in
(11) by the advantage function A✓(s  a) deﬁned as

A✓(s  a) = Q✓(s  a)  V ✓(s).

This modiﬁcation corresponds to the standard REINFORCE with Baseline algorithm [58] and can
signiﬁcantly reduce the variance of policy gradient.
Actor-critic method. Finally  we can use an actor-critic update to improve the performance further.
In this case  since we need unbiased estimators for both the gradient and the reward value in (5) and
(6) in online fashion  we modify our original problem (4) to average reward setting as

minimize

✓2⇥

J(✓) = lim
T!1

subject to D(✓) = lim
T!1

E⇡✓
E⇡✓ 1

T

1
T

r(st  at) 
TXt=0
d(st  at)  D0.
TXt=0

5

Take action a  observe reward r  cost d  and new state s0.
Critic step:

Algorithm 2 Actor-Critic update for constrained MDP
1: for k = 1  2  3  . . . do
2:
3:
4:
5:
6:
7:
8:
9:
10:

w w + w · JrwV J
v v + v · DrvV J
Calculate TD error:
J = r  J + V J
D = d  D + V D
Solve ✓k by (7) or (9) with

w (s)  J J + w ·r  J.
v (s)  D D + v ·d  D.
w (s).
w (s0)  V J
v (s).
v (s0)  V D

Actor step:

s s0.
11:
12: end for

J⇤(✓k)  r✓J⇤(✓k) in (5) replaced by J and J ·r ✓ log ⇡✓(s  a);
D⇤(✓k)  r✓D⇤(✓k) in (6) replaced by D and D ·r ✓ log ⇡✓(s  a).

✓ (s) and V D

Let V J
✓ (s) denote the value and cost functions corresponding to (2). We use possibly
nonlinear approximation with parameter w for the value function: V J
w (s) and v for the cost function:
v (s). In the critic step  we update w and v by TD(0) with step size w and v; in the actor step  we
V D
solve our proposed convex relaxation problem to update ✓. The actor-critic procedure is summarized
in Algorithm 2. Here J and D are estimators of J(✓k) and D(✓k). Both of J and D  and the TD
error J  D can be initialized as 0.
The usage of the actor-critic method helps reduce variance by using a value function instead of
Monte-Carlo sampling. Speciﬁcally  in Algorithm 1 we need to obtain a sample trajectory and
calculate J⇤(✓) and r✓J⇤(✓) by Monte-Carlo sampling. This step has a high variance since we need
to sample a potentially long trajectory and sum up a lot of random rewards. In contrast  in Algorithm
2  this step is replaced by a value function V J

w (s)  which reduces the variance.

4 Theoretical Result

In this section  we show almost sure convergence of the iterates obtained by our algorithm to a
stationary point. We start by stating some mild assumptions on the original problem (4) and the
choice of some parameters in Algorithm 1.

Assumption 1 The choice of {⌘k} and {⇢k} satisfy limk!1Pk ⌘k = 1  limk!1Pk ⇢k = 1
and limk!1Pk ⌘2
k < 1. Furthermore  we have limk!1 ⌘k/⇢k = 0 and ⌘k is decreasing.
Assumption 2 For any realization  J⇤(✓) and D⇤(✓) are continuously differentiable as functions of
✓. Moreover  J⇤(✓)  D⇤(✓)  and their derivatives are uniformly Lipschitz continuous.

k + ⇢2

Assumption 1 allows us to specify the learning rates. A practical choice would be ⌘k = kc1 and
⇢k = kc2 with 0.5 < c2 < c1 < 1. This assumption is standard for gradient-based algorithms.
Assumption 2 is also standard and is known to hold for a number of models. It ensures that the
reward and cost functions are sufﬁciently regular. In fact  it can be relaxed such that each realization
is Lipschitz (not uniformly)  and the event that we keep generating realizations with monotonically
increasing Lipschitz constant is an event with probability 0. See condition iv) in [67] and the
discussion thereafter. Also  see [45] for sufﬁcient conditions such that both the expected cumulative
reward function and the gradient of it are Lipschitz.
The following Assumption 3 is useful only when we initialize with an infeasible point. We ﬁrst state
it here and we will discuss this assumption after the statement of the main theorem.

Assumption 3 Suppose (✓S ↵ S) is a stationary point of the optimization problem

minimize

✓ ↵

↵

subject to

D(✓)  D0 + ↵.

(12)

We have that ✓S is a feasible point of the original problem (4)  i.e. D(✓S)  D0.

6

and

We are now ready to state the main theorem.
Theorem 4 Suppose the Assumptions 1 and 2 are satisﬁed with small enough initial step size ⌘0.
Suppose also that  either ✓0 is a feasible point  or Assumption 3 is satisﬁed. If there is a subsequence

(kj )

J

lim
j!1

(kj )

D

lim
j!1

(✓) = bJ(✓)

(✓) = bD(✓).

{✓kj} of {✓k} that converges to somee✓  then there exist uniformly continuous functions bJ(✓) and
bD(✓) satisfying
Furthermore  suppose there exists ✓ such that bD(✓) < D0 (i.e. the Slater’s condition holds)  thene✓ is

a stationary point of the original problem (4) almost surely.

The proof of Theorem 4 is provided in the supplementary material.
Note that Assumption 3 is not necessary if we start from a feasible point  or we reach a feasible point
in the iterates  which could be viewed as an initializer. Assumption 3 makes sure that the iterates
in Algorithm 1 keep making progress without getting stuck at any infeasible stationary point. A
similar condition is assumed in [38] for an infeasible initializer. If it turns out that ✓0 is infeasible
and Assumption 3 is violated  then the convergent point may be an infeasible stationary point of (12).
In practice  if we can ﬁnd a feasible point of the original problem  then we proceed with that point.
Alternatively  we could generate multiple initializers and obtain iterates for all of them. As long as
there is a feasible point in one of the iterates  we can view this feasible point as the initializer and
Theorem 4 follows without Assumption 3. In our later experiments  for every single replicate  we
could reach a feasible point  and therefore Assumption 3 is not necessary.
Our algorithm does not guarantee safe exploration during the training phase. Ensuring safety during
learning is a more challenging problem. Sometimes even ﬁnding a feasible point is not straightforward 
otherwise Assumption 3 is not necessary.
Our proposed algorithm is inspired by [38]. Compared to [38] which deals with an optimization
problem  solving the safe reinforcement learning problem is more challenging. We need to verify
that the Lipschitz condition is satisﬁed  and also the policy gradient has to be estimated (instead of
directly evaluated as in a standard optimization problem). The usage of the Actor-Critic algorithm
reduces the variance of the sampling  which is unique to Reinforcement learning.

5 Application to Constrained Linear-Quadratic Regulator

We apply our algorithm to the linear-quadratic regulator (LQR)  which is one of the most fundamental
problems in control theory. In the LQR setting  the state dynamic equation is linear  the cost function
is quadratic  and the optimal control theory tells us that the optimal control for LQR is a linear
function of the state [23  6]. LQR can be viewed as an MDP problem and it has attracted a lot of
attention in the reinforcement learning literature [12  13  21  47].
We consider the inﬁnite-horizon  discrete-time LQR problem. Denote xt as the state variable and ut
as the control variable. The state transition and the control sequence are given by

xt+1 = Axt + But + vt 

ut = F xt + wt 

(13)

where vt and wt represent possible Gaussian white noise  and the initial state is given by x0. The goal
is to ﬁnd the control parameter matrix F such that the expected total cost is minimized. The usual
cost function of LQR corresponds to the negative reward in our setting and we impose an additional
quadratic constraint on the system. The overall optimization problem is given by

minimize J(F ) = EXt0
subject to D(F ) = EXt0

x>t Q1xt + u>t R1ut 
x>t Q2xt + u>t R2ut  D0 

7

where Q1  Q2  R1  and R2 are positive deﬁnite matrices. Note that even thought the matrices are
positive deﬁnite  both the objective function J and the constraint D are nonconvex with respect to

the parameter F . Furthermore  with the additional constraint  the optimal control sequence may no
longer be linear in the state xt. Nevertheless  in this work  we still consider linear control given by
(13) and the goal is to ﬁnd the best linear control for this constrained LQR problem. We assume that
the choice of A  B are such that the optimal cost is ﬁnite.

Random initial state. We ﬁrst consider the setting where the initial state x0 ⇠D follows a random
distribution D  while both the state transition and the control sequence (13) are deterministic (i.e.
vt = wt = 0). In this random initial state setting  [24] showed that without the constraint  the policy
gradient method converges efﬁciently to the global optima in polynomial time. In the constrained
case  we can explicitly write down the objective and constraint function  since the only randomness
comes from the initial state. Therefore  we have the state dynamic xt+1 = (A  BF )xt and the
objective function has the following expression ([24]  Lemma 1)
(14)

where PF is the solution to the following equation

J(F ) = Ex0⇠D⇥x>0 PF x0⇤ 

The gradient is given by

PF = Q1 + F >R1F + (A  BF )>PF (A  BF ).
1Xt=0

rF J(F ) = 2⇣R1 + B>PF BF  B>PF A⌘ ·Ex0⇠D

(15)

(16)

xtx>t.

Let SF =P1t=0 xtx>t and observe that

SF = x0x>0 + (A  BF )SF (A  BF )>.

(17)
We start from some F0 and apply our Algorithm 1 to solve the constrained LQR problem. In iteration
k  with the current estimator denoted by Fk  we ﬁrst obtain an estimator of PFk by starting from
Q1 and iteratively applying the recursion PFk Q1 + F >k R1Fk + (A  BFk)>PFk (A  BFk)
until convergence. Next  we sample an x⇤0 from the distribution D and follow a similar recursion
given by (17) to obtain an estimate of SFk. Plugging the sample x⇤0 and the estimates of PFk and
SFk into (14) and (16)  we obtain the sample reward value J⇤(Fk) and rF J⇤(Fk)  respectively.
(F ). We apply the same procedure to
With these two values  we follow (5) and (8) and obtain J
the cost function D(F ) with Q1  R1 replaced by Q2  R2 to obtain D
(F ). Finally we solve the
optimization problem (7) (or (9) if (7) is infeasible) and obtain Fk+1 by (10).

(k)

(k)

Random state transition and control. We then consider the setting where both vt and wt are
independent standard Gaussian white noise.
In this case  the state dynamic can be written as
xt+1 = (A  BF )xt + ✏t where ✏t ⇠N (0  I + BB>). Let PF be deﬁned as in (15) and SF be the
solution to the following Lyapunov equation

SF = I + BB> + (A  BF )SF (A  BF )>.
The objective function has the following expression ([68]  Proposition 3.1)
J(F ) = Ex⇠N (0 SF )hx>(Q1 + F >R1F )xi + tr(R1) 

and the gradient is given by

(18)

(19)

rF J(F ) = 2⇣R1 + B>PF BF  B>PF A⌘ · Ex⇠N (0 SF )hxx>i.

Although in this setting it is straightforward to calculate the expectation in a closed form  we keep the
current expectation form to be in line with our algorithm. Moreover  when the error distribution is
more complicated or unknown  we can no longer calculate the closed form expression and have to
sample in each iteration. With the formulas given by (18) and (19)  we again apply our Algorithm 1.
We sample x ⇠N (0  SF ) in each iteration and solve the optimization problem (7) or (9). The whole
procedure is similar to the random initial state case described above.

Other applications. Our algorithm can also be applied to constrained parallel MDP and constrained
multi-agent MDP problem. Due to the space limit  we relegate them to supplementary material.

8

33

32.5

32

31.5

31

l

e
u
a
v
 
t

i

n
a
r
t
s
n
o
c

30.5

0

46.5

46

45.5

45

44.5

44

l

e
u
a
v
 

e
v
i
t
c
e
b
o

j

43.5

0

500

1000

1500

iteration

500

1000

1500

iteration

(a) Constraint value D(✓k) in each iteration.

(b) Objective value J(✓k) in each iteration.

Figure 1: An experiment on constrained LQR problem. The iterate starts from an infeasible point and
then becomes feasible and eventually converges.

min value

Our method
Lagrangian

30.689 ± 0.114
30.693 ± 0.113
Table 1: Comparison of our method with Lagrangian method

approx. min value
30.694 ± 0.114
30.699 ± 0.113

# iterations
2001 ± 1172
7492 ± 1780

approx. # iterations

604.3 ± 722.4
5464 ± 2116

6 Experiment

We verify the effectiveness of the proposed algorithm through experiments. We focus on the LQR
setting with a random initial state as discussed in Section 5. In this experiment we set x 2 R15 and
u 2 R8. The initial state distribution is uniform on the unit cube: x0 ⇠D = Uniform[1  1]15.
Each element of A and B is sampled independently from the standard normal distribution and scaled
such that the eigenvalues of A are within the range (1  1). We initialize F0 as an all-zero matrix 
and the choice of the constraint function and the value D0 are such that (1) the constrained problem is
feasible; (2) the solution of the unconstrained problem does not satisfy the constraint  i.e.  the problem
is not trivial; (3) the initial value F0 is not feasible. The learning rates are set as ⌘k = 2
3 k3/4 and
3 k2/3. The conservative choice of step size is to avoid the situation where an eigenvalue of
⇢k = 2
A  BF runs out of the range (1  1)  and so the system is stable. 5
Figure 1(a) and 1(b) show the constraint and objective value in each iteration  respectively. The red
horizontal line in Figure 1(a) is for D0  while the horizontal line in Figure 1(b) is for the unconstrained
minimum objective value. We can see from Figure 1(a) that we start from an infeasible point  and the
problem becomes feasible after about 100 iterations. The objective value is in general decreasing
after becoming feasible  but never lower than the unconstrained minimum  as shown in Figure 1(b).

Comparison with the Lagrangian method. We compare our proposed method with the usual
Lagrangian method. For the Lagrangian method  we follow the algorithm proposed in [18] for safe
reinforcement learning  which iteratively applies gradient descent on the parameter F and gradient
ascent on the Lagrangian multiplier  for the Lagrangian function until convergence.
Table 1 reports the comparison results with mean and standard deviation based on 50 replicates. In
the second and third columns  we compare the minimum objective value and the number of iterations
to achieve it. We also consider an approximate version  where we are satisﬁed with the result if the
objective value exceeds less than 0.02% of the minimum value. The fourth and ﬁfth columns show
the comparison results for this approximate version. We can see that both methods achieve similar
minimum objective values  but ours requires less number of policy updates  for both minimum and
approximate minimum version.

5The code is available at https://github.com/ming93/Safe_reinforcement_learning

9

References
[1] Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel. Constrained policy optimization.

In International Conference on Machine Learning  pages 22–31  2017.

[2] Leonard Adolphs. Non convex-concave saddle point optimization. Master’s thesis  ETH Zurich 

2018.

[3] Eitan Altman. Constrained Markov decision processes  volume 7. CRC Press  1999.
[4] Haitham Bou Ammar  Rasul Tutunov  and Eric Eaton. Safe policy search for lifelong reinforce-
ment learning with sublinear regret. In International Conference on Machine Learning  pages
2361–2369  2015.

[5] Dario Amodei  Chris Olah  Jacob Steinhardt  Paul Christiano  John Schulman  and Dan Man´e.

Concrete problems in ai safety. arXiv preprint arXiv:1606.06565  2016.

[6] Brian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier

Corporation  2007.

[7] Yu Bai  Tengyang Xie  Nan Jiang  and Yu-Xiang Wang. Provably efﬁcient q-learning with low

switching cost. arXiv preprint arXiv:1905.12849  2019.

[8] Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence
Research  47:253–279  2013.

[9] Felix Berkenkamp  Matteo Turchetta  Angela Schoellig  and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in Neural Information Processing
Systems  pages 908–918  2017.

[10] Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters 

29(5):291–294  1997.

[11] Craig Boutilier. Planning  learning and coordination in multiagent decision processes. In
Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge  pages
195–210. Morgan Kaufmann Publishers Inc.  1996.

[12] Steven J Bradtke. Reinforcement learning applied to linear quadratic regulation. In Advances in

neural information processing systems  pages 295–302  1993.

[13] Steven J Bradtke  B Erik Ydstie  and Andrew G Barto. Adaptive linear quadratic control
using policy iteration. In Proceedings of the American control conference  volume 3  pages
3475–3475. Citeseer  1994.

[14] Leo Breiman. Random forests. Machine learning  45(1):5–32  2001.
[15] Lucian Busoniu  Robert Babuska  and Bart De Schutter. A comprehensive survey of multia-
gent reinforcement learning. IEEE Transactions on Systems  Man  And Cybernetics-Part C:
Applications and Reviews  38 (2)  2008  2008.

[16] Qi Cai  Zhuoran Yang  Jason D Lee  and Zhaoran Wang. Neural temporal-difference learning

converges to global optima. arXiv preprint arXiv:1905.10027  2019.

[17] Tianyi Chen  Kaiqing Zhang  Georgios B Giannakis  and Tamer Bas¸ar. Communication-efﬁcient

distributed reinforcement learning. arXiv preprint arXiv:1812.03239  2018.

[18] Yinlam Chow  Mohammad Ghavamzadeh  Lucas Janson  and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. Journal of Machine Learning Research 
18(167):1–167  2017.

[19] Yinlam Chow  Oﬁr Nachum  Edgar Duenez-Guzman  and Mohammad Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708 
2018.

[20] Christoph Dann  Gerhard Neumann  and Jan Peters. Policy evaluation with temporal differences:
A survey and comparison. The Journal of Machine Learning Research  15(1):809–883  2014.
[21] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the sample

complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688  2017.

[22] Nelson Dunford and Jacob T Schwartz. Linear operators part I: general theory  volume 7.

Interscience publishers New York  1958.

10

[23] Lawrence C Evans. An introduction to mathematical optimal control theory. Lecture Notes 

University of California  Department of Mathematics  Berkeley  2005.

[24] Maryam Fazel  Rong Ge  Sham Kakade  and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning  pages 1466–1475  2018.

[25] Jaime F Fisac  Anayo K Akametalu  Melanie N Zeilinger  Shahab Kaynama  Jeremy Gillula 
and Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control  2018.

[26] Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research  16(1):1437–1480  2015.

[27] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[28] Ivo Grondman  Lucian Busoniu  Gabriel AD Lopes  and Robert Babuska. A survey of actor-
critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on
Systems  Man  and Cybernetics  Part C (Applications and Reviews)  42(6):1291–1307  2012.
[29] Jingyu He  Saar Yalov  and P Richard Hahn. XBART: Accelerated Bayesian additive regression
trees. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics  pages
1130–1138  2019.

[30] Jingyu He  Saar Yalov  Jared Murray  and P Richard Hahn. Stochastic tree ensembles for

regularized supervised learning. Technical report  2019.

[31] Jessie Huang  Fa Wu  Doina Precup  and Yang Cai. Learning safe policies with expert guidance.

arXiv preprint arXiv:1805.08313  2018.

[32] John L Kelley. General topology. Courier Dover Publications  2017.
[33] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information

processing systems  pages 1008–1014  2000.

[34] R Matthew Kretchmar. Parallel reinforcement learning. In The 6th World Conference on

Systemics  Cybernetics  and Informatics. Citeseer  2002.

[35] Jonathan Lacotte  Yinlam Chow  Mohammad Ghavamzadeh  and Marco Pavone. Risk-sensitive

generative adversarial imitation learning. arXiv preprint arXiv:1808.04468  2018.

[36] Dennis Lee  Haoran Tang  Jeffrey O Zhang  Huazhe Xu  Trevor Darrell  and Pieter Abbeel.
Modular architecture for starcraft ii with deep reinforcement learning. In Fourteenth Artiﬁcial
Intelligence and Interactive Digital Entertainment Conference  2018.

[37] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274  2017.
[38] An Liu  Vincent Lau  and Borna Kananian. Stochastic successive convex approximation for

non-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266  2018.

[39] Boyi Liu  Qi Cai  Zhuoran Yang  and Zhaoran Wang. Neural proximal/trust region policy

optimization attains globally optimal policy. arXiv preprint arXiv:1906.10306  2019.

[40] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lilli-
crap  Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on machine learning  pages 1928–1937 
2016.

[41] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[42] Arun Nair  Praveen Srinivasan  Sam Blackwell  Cagdas Alcicek  Rory Fearon  Alessandro
De Maria  Vedavyas Panneershelvam  Mustafa Suleyman  Charles Beattie  Stig Petersen  et al.
Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296 
2015.

[43] Santiago Paternain. Stochastic Control Foundations of Autonomous Behavior. PhD thesis 

University of Pennsylvania  2018.

11

[44] Peng Peng  Ying Wen  Yaodong Yang  Quan Yuan  Zhenkun Tang  Haitao Long  and Jun Wang.
Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning
to play starcraft combat games. arXiv preprint arXiv:1703.10069  2017.

[45] Matteo Pirotta  Marcello Restelli  and Luca Bascetta. Policy gradient in lipschitz markov

decision processes. Machine Learning  100(2-3):255–283  2015.

[46] LA Prashanth and Mohammad Ghavamzadeh. Variance-constrained actor-critic algorithms for

discounted and average reward mdps. Machine Learning  105(3):367–417  2016.

[47] Benjamin Recht. A tour of reinforcement learning: The view from continuous control. Annual

Review of Control  Robotics  and Autonomous Systems  2018.

[48] Chang-han Rhee and Peter W Glynn. Unbiased estimation with square root convergence for sde

models. Operations Research  63(5):1026–1043  2015.

[49] Andrzej Ruszczy´nski. Feasible direction methods for stochastic programming problems. Math-

ematical Programming  19(1):220–229  1980.

[50] Joris Scharpff  Diederik M Roijers  Frans A Oliehoek  Matthijs TJ Spaan  and Mathijs Michiel
de Weerdt. Solving transition-independent multi-agent mdps with sparse interactions. In AAAI 
pages 3174–3180  2016.

[51] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897 
2015.

[52] Gesualdo Scutari  Francisco Facchinei  Peiran Song  Daniel P Palomar  and Jong-Shi Pang.
Decomposition by partial linearization: Parallel optimization of multi-agent systems. IEEE
Transactions on Signal Processing  62(3):641–656  2013.

[53] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al.
Mastering the game of go with deep neural networks and tree search. nature  529(7587):484 
2016.

[54] David Silver  Thomas Hubert  Julian Schrittwieser  Ioannis Antonoglou  Matthew Lai  Arthur
Guez  Marc Lanctot  Laurent Sifre  Dharshan Kumaran  Thore Graepel  et al. A general
reinforcement learning algorithm that masters chess  shogi  and go through self-play. Science 
362(6419):1140–1144  2018.

[55] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  et al. Mastering the game of
go without human knowledge. Nature  550(7676):354  2017.

[56] Peng Sun  Xinghai Sun  Lei Han  Jiechao Xiong  Qing Wang  Bo Li  Yang Zheng  Ji Liu 
Yongsheng Liu  Han Liu  et al. Tstarbots: Defeating the cheating level builtin ai in starcraft ii in
the full game. arXiv preprint arXiv:1809.07193  2018.

[57] Ying Sun  Prabhu Babu  and Daniel P Palomar. Majorization-minimization algorithms in signal
processing  communications  and machine learning. IEEE Transactions on Signal Processing 
65(3):794–816  2016.

[58] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[59] Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour. Policy gradient
In Advances in neural

methods for reinforcement learning with function approximation.
information processing systems  pages 1057–1063  2000.

[60] Chen Tessler  Daniel J Mankowitz  and Shie Mannor. Reward constrained policy optimization.

arXiv preprint arXiv:1805.11074  2018.

[61] Matteo Turchetta  Felix Berkenkamp  and Andreas Krause. Safe exploration in ﬁnite markov
decision processes with gaussian processes. In Advances in Neural Information Processing
Systems  pages 4312–4320  2016.

[62] Oriol Vinyals  Timo Ewalds  Sergey Bartunov  Petko Georgiev  Alexander Sasha Vezhnevets 
Michelle Yeo  Alireza Makhzani  Heinrich K¨uttler  John Agapiou  Julian Schrittwieser  et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 
2017.

12

[63] Hoi-To Wai  Zhuoran Yang  Zhaoran Wang  and Mingyi Hong. Multi-agent reinforcement
learning via double averaging primal-dual optimization. arXiv preprint arXiv:1806.00877 
2018.

[64] Lingxiao Wang  Qi Cai  Zhuoran Yang  and Zhaoran Wang. Neural policy gradient methods:

Global optimality and rates of convergence. arXiv preprint arXiv:1909.01150  2019.

[65] Min Wen and Ufuk Topcu. Constrained cross-entropy method for safe reinforcement learning.

In Advances in Neural Information Processing Systems  pages 7461–7471  2018.

[66] Sijia Xu  Hongyu Kuang  Zhi Zhuang  Renjie Hu  Yang Liu  and Huyang Sun. Macro action
selection with deep reinforcement learning in starcraft. arXiv preprint arXiv:1812.00336  2018.
[67] Yang Yang  Gesualdo Scutari  Daniel P Palomar  and Marius Pesavento. A parallel decomposi-
tion method for nonconvex stochastic multi-agent optimization problems. IEEE Transactions
on Signal Processing  64(11):2949–2964  2016.

[68] Zhuoran Yang  Yongxin Chen  Mingyi Hong  and Zhaoran Wang. On the global conver-
gence of actor-critic: A case for linear quadratic regulator with ergodic cost. arXiv preprint
arXiv:1907.06246  2019.

[69] Kaiqing Zhang  Zhuoran Yang  Han Liu  Tong Zhang  and Tamer Bas¸ar. Finite-sample analyses
for fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783 
2018.

13

,Yu Liu
Kris De Brabanter
Ming Yu
Zhuoran Yang
Mladen Kolar
Zhaoran Wang