2019,Can you trust your model's uncertainty?  Evaluating predictive uncertainty under dataset shift,Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks  but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings  which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity.  In such settings  well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted.  Many probabilistic deep learning methods  including Bayesian-and non-Bayesian methods  have been proposed in the literature for quantifying predictive uncertainty  but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration.  We find that traditional post-hoc calibration does indeed fall short  as do several other previous methods.  However  some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.,Can You Trust Your Model’s Uncertainty? Evaluating

Predictive Uncertainty Under Dataset Shift

Yaniv Ovadia⇤
Google Research

Emily Fertig⇤†
Google Research

Jie Ren†

Google Research

yovadia@google.com

emilyaf@google.com

jjren@google.com

Zachary Nado
Google Research

D Sculley

Google Research

Sebastian Nowozin
Google Research

znado@google.com

dsculley@google.com

nowozin@google.com

Joshua V. Dillon
Google Research

Balaji Lakshminarayanan‡

DeepMind

Jasper Snoek‡
Google Research

jvdillon@google.com

balajiln@google.com

jsnoek@google.com

Abstract

Modern machine learning methods including deep learning have achieved great
success in predictive accuracy for supervised learning tasks  but may still fall short
in giving useful estimates of their predictive uncertainty. Quantifying uncertainty
is especially critical in real-world settings  which often involve input distributions
that are shifted from the training distribution due to a variety of factors including
sample bias and non-stationarity. In such settings  well calibrated uncertainty
estimates convey information about when a model’s output should (or should not)
be trusted. Many probabilistic deep learning methods  including Bayesian-and non-
Bayesian methods  have been proposed in the literature for quantifying predictive
uncertainty  but to our knowledge there has not previously been a rigorous large-
scale empirical comparison of these methods under dataset shift. We present a large-
scale benchmark of existing state-of-the-art methods on classiﬁcation problems
and investigate the effect of dataset shift on accuracy and calibration. We ﬁnd that
traditional post-hoc calibration does indeed fall short  as do several other previous
methods. However  some methods that marginalize over models give surprisingly
strong results across a broad spectrum of tasks.

Introduction

1
Recent successes across a variety of domains have led to the widespread deployment of deep
neural networks (DNNs) in practice. Consequently  the predictive distributions of these models are
increasingly being used to make decisions in important applications ranging from machine-learning
aided medical diagnoses from imaging (Esteva et al.  2017) to self-driving cars (Bojarski et al.  2016).
Such high-stakes applications require not only point predictions but also accurate quantiﬁcation
of predictive uncertainty  i.e. meaningful conﬁdence values in addition to class predictions. With
sufﬁcient independent labeled samples from a target data distribution  one can estimate how well

⇤Equal contribution
†AI Resident
‡Corresponding authors

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

a model’s conﬁdence aligns with its accuracy and adjust the predictions accordingly. However  in
practice  once a model is deployed the distribution over observed data may shift and eventually be
very different from the original training data distribution. Consider  e.g.  online services for which the
data distribution may change with the time of day  seasonality or popular trends. Indeed  robustness
under conditions of distributional shift and out-of-distribution (OOD) inputs is necessary for the
safe deployment of machine learning (Amodei et al.  2016). For such settings  calibrated predictive
uncertainty is important because it enables accurate assessment of risk  allows practitioners to know
how accuracy may degrade  and allows a system to abstain from decisions due to low conﬁdence.
A variety of methods have been developed for quantifying predictive uncertainty in DNNs. Probabilis-
tic neural networks such as mixture density networks (MacKay & Gibbs  1999) capture the inherent
ambiguity in outputs for a given input  also referred to as aleatoric uncertainty (Kendall & Gal  2017).
Bayesian neural networks learn a posterior distribution over parameters that quantiﬁes parameter
uncertainty  a type of epistemic uncertainty that can be reduced through the collection of additional
data. Popular approximate Bayesian approaches include Laplace approximation (MacKay  1992) 
variational inference (Graves  2011; Blundell et al.  2015)  dropout-based variational inference (Gal
& Ghahramani  2016; Kingma et al.  2015)  expectation propagation Hern´andez-Lobato & Adams
(2015) and stochastic gradient MCMC (Welling & Teh  2011). Non-Bayesian methods include
training multiple probabilistic neural networks with bootstrap or ensembling (Osband et al.  2016;
Lakshminarayanan et al.  2017). Another popular non-Bayesian approach involves re-calibration of
probabilities on a held-out validation set through temperature scaling (Platt  1999)  which was shown
by Guo et al. (2017) to lead to well-calibrated predictions on the i.i.d. test set.
Using Distributional Shift to Evaluate Predictive Uncertainty While previous work has evaluated
the quality of predictive uncertainty on OOD inputs (Lakshminarayanan et al.  2017)  there has not
to our knowledge been a comprehensive evaluation of uncertainty estimates from different methods
under dataset shift. Indeed  we suggest that effective evaluation of predictive uncertainty is most
meaningful under conditions of distributional shift. One reason for this is that post-hoc calibration
gives good results in independent and identically distributed (i.i.d.) regimes  but can fail under even a
mild shift in the input data. And in real world applications  as described above  distributional shift is
widely prevalent. Understanding questions of risk  uncertainty  and trust in a model’s output becomes
increasingly critical as shift from the original training data grows larger.
Contributions In the spirit of calls for more rigorous understanding of existing methods (Lipton
& Steinhardt  2018; Sculley et al.  2018; Rahimi & Recht  2017)  this paper provides a benchmark
for evaluating uncertainty that focuses not only on the i.i.d. setting but also uncertainty under
distributional shift. We present a large-scale evaluation of popular approaches in probabilistic deep
learning  focusing on methods that operate well in large-scale settings  and evaluate them on a diverse
range of classiﬁcation benchmarks across image  text  and categorical modalities. We use these
experiments to evaluate the following questions:
• How trustworthy are the uncertainty estimates of different methods under dataset shift?
• Does calibration in the i.i.d. setting translate to calibration under dataset shift?
• How do uncertainty and accuracy of different methods co-vary under dataset shift? Are there

methods that consistently do well in this regime?

In addition to answering the questions above  our code is made available open-source along with our
model predictions such that researchers can easily evaluate their approaches on these benchmarks 4.

2 Background
Notation and Problem Setup Let x 2 Rd represent a set of d-dimensional features and y 2
{1  . . .   k} denote corresponding labels (targets) for k-class classiﬁcation. We assume that a training
dataset D consists of N i.i.d.samples D = {(xn  yn)}N
Let p⇤(x  y) denote the true distribution (unknown  observed only through the samples D)  also
referred to as the data generating process. We focus on classiﬁcation problems  in which the true
distribution is assumed to be a discrete distribution over k classes  and the observed y 2{ 1  . . .   k}

n=1.

4https://github.com/google-research/google-research/tree/master/uq benchmark 2019

2

is a sample from the conditional distribution p⇤(y|x). We use a neural network to model p✓(y|x) and
estimate the parameters ✓ using the training dataset. At test time  we evaluate the model predictions
against a test set  sampled from the same distribution as the training dataset. However  here we also
evaluate the model against OOD inputs sampled from q(x  y) 6= p⇤(x  y). In particular  we consider
two kinds of shifts:
• shifted versions of the test inputs where the ground truth label belongs to one of the k classes. We
use shifts such as corruptions and perturbations proposed by Hendrycks & Dietterich (2019)  and
ideally would like the model predictions to become more uncertain with increased shift  assuming
shift degrades accuracy. This is also referred to as covariate shift (Sugiyama et al.  2017).

• a completely different OOD dataset  where the ground truth label is not one of the k classes. Here
we check if the model exhibits higher predictive uncertainty for those new instances and to this
end report diagnostics that rely only on predictions and not ground truth labels.

High-level overview of existing methods A large variety of methods have been developed to either
provide higher quality uncertainty estimates or perform OOD detection to inform model conﬁdence.
These can roughly be divided into:
1. Methods which deal with p(y|x) only  we discuss these in more detail in Section 3.
2. Methods which model the joint distribution p(y  x)  e.g. deep hybrid models (Kingma et al.  2014;

Alemi et al.  2018; Nalisnick et al.  2019; Behrmann et al.  2018).

3. Methods with an OOD-detection component in addition to p(y|x) (Bishop  1994; Lee et al.  2018;

Liang et al.  2018)  and related work on selective classiﬁcation (Geifman & El-Yaniv  2017).

We refer to Shafaei et al. (2018) for a recent summary of these methods. Due to the differences in
modeling assumptions  a fair comparison between these different classes of methods is challenging;
for instance  some OOD detection methods rely on knowledge of a known OOD set  or train using a
none-of-the-above class  and it may not always be meaningful to compare predictions from these
methods with those obtained from a Bayesian DNN. We focus on methods described by (1) above  as
this allows us to focus on methods which make the same modeling assumptions about data and differ
only in how they quantify predictive uncertainty.

3 Methods and Metrics
We select a subset of methods from the probabilistic deep learning literature for their prevalence 
scalability and practical applicability5. These include (see also references within):
• (Vanilla) Maximum softmax probability (Hendrycks & Gimpel  2017)
• (Temp Scaling) Post-hoc calibration by temperature scaling using a validation set (Guo et al.  2017)
• (Dropout) Monte-Carlo Dropout (Gal & Ghahramani  2016; Srivastava et al.  2015) with rate p
• (Ensembles) Ensembles of M networks trained independently on the entire dataset using random
• (SVI) Stochastic Variational Bayesian Inference for deep learning (Blundell et al.  2015; Graves 
2011; Louizos & Welling  2017  2016; Wen et al.  2018). We refer to Appendix A.6 for details of
our SVI implementation.

initialization (Lakshminarayanan et al.  2017) (we set M = 10 in experiments below)

• (LL) Approx. Bayesian inference for the parameters of the last layer only (Riquelme et al.  2018)

– (LL SVI) Mean ﬁeld stochastic variational inference on the last layer only
– (LL Dropout) Dropout only on the activations before the last layer

In addition to metrics (we use arrows to indicate which direction is better) that do not depend on
predictive uncertainty  such as classiﬁcation accuracy "  the following metrics are commonly used:
5The methods used scale well for training and prediction (see in Appendix A.9.). We also explored methods
such as scalable extensions of Gaussian Processes (Hensman et al.  2015)  but they were challenging to train on
the 37M example Criteo dataset or the 1000 classes of ImageNet.

3

(p(y|xn  ✓)  (y  yn))2 = |Y|1⇣1  2p(yn|xn  ✓) +Xy2Y

Negative Log-Likelihood (NLL) # Commonly used to evaluate the quality of model uncertainty on
some held out set. Drawbacks: Although a proper scoring rule (Gneiting & Raftery  2007)  it can
over-emphasize tail probabilities (Quinonero-Candela et al.  2006).
Brier Score # (Brier  1950) Proper scoring rule for measuring the accuracy of predicted probabilities.
It is computed as the squared error of a predicted probability vector  p(y|xn  ✓)  and the one-hot
encoded true response  yn. That is 
p(y|xn  ✓)2⌘. (1)
BS = |Y|1Xy2Y
The Brier score has a convenient interpretation as BS = uncertainty  resolution + reliability 
where uncertainty is the marginal uncertainty over labels  resolution measures the deviation of
individual predictions against the marginal  and reliability measures calibration as the average
violation of long-term true label frequencies. We refer to DeGroot & Fienberg (1983) for the
decomposition of Brier score into calibration and reﬁnement for classiﬁcation and to (Br¨ocker  2009)
for the general decomposition for any proper scoring rule. Drawbacks: Brier score is insensitive to
predicted probabilities associated with in/frequent events.
Both the Brier score and the negative log-likelihood are proper scoring rules and therefore the
optimum score corresponds to a perfect prediction. In addition to these two metrics  we also evaluate
two metrics—expected calibration error and entropy. Neither of these is a proper scoring rule  and
thus there exist trivial solutions which yield optimal scores; for example  returning the marginal
probability p(y) for every instance will yield perfectly calibrated but uninformative predictions. Each
proper scoring rule induces a calibration measure (Br¨ocker  2009). However  ECE is not the result of
such decomposition and has no corresponding proper scoring rule; we instead include ECE because
it is popularly used and intuitive. Each proper scoring rule is also associated with a corresponding
entropy function and Shannon entropy is that for log probability (Gneiting & Raftery  2007).
Expected Calibration Error (ECE) # Measures the correspondence between predicted probabilities
and empirical accuracy (Naeini et al.  2015). It is computed as the average gap between within
bucket accuracy and within bucket predicted probability for S buckets Bs = {n 2 1 . . . N :
p(yn|xn  ✓) 2 (⇢s ⇢ s+1]}. That is  ECE =PS
s=1 |Bs|N | acc(Bs)  conf(Bs)|  where acc(Bs) =
[yn = ˆyn]  conf(Bs) = |Bs|1Pn2Bs
|Bs|1Pn2Bs
p(ˆyn|xn  ✓)  and ˆyn = arg maxy p(y|xn  ✓)
is the n-th prediction. When bins {⇢s : s 2 1 . . . S} are quantiles of the held-out predicted
probabilities  |Bs|⇡| Bk| and the estimation error is approximately constant. Drawbacks: Due to
binning  ECE does not monotonically increase as predictions approach ground truth. If |Bs|6 = |Bk| 
the estimation error varies across bins.
There is no ground truth label for fully OOD inputs. Thus we report histograms of conﬁdence
and predictive entropy on known and OOD inputs and accuracy versus conﬁdence plots (Laksh-
minarayanan et al.  2017): Given the prediction p(y = k|xn  ✓)  we deﬁne the predicted label as
ˆyn = arg maxy p(y|xn  ✓)  and the conﬁdence as p(y = ˆy|x  ✓) = maxk p(y = k|xn  ✓). We ﬁlter
out test examples corresponding to a particular conﬁdence threshold ⌧ 2 [0  1] and compute the
accuracy on this set.

4 Experiments and Results
We evaluate the behavior of the predictive uncertainty of deep learning models on a variety of datasets
across three different modalities: images  text and categorical (online ad) data. For each we follow
standard training  validation and testing protocols  but we additionally evaluate results on increasingly
shifted data and an OOD dataset. We detail the models and implementations used in Appendix A.
Hyperparameters were tuned for all methods using Bayesian optimization (Golovin et al.  2017)
(except on ImageNet) as detailed in Appendix A.8.

4.1 An illustrative example - MNIST
We ﬁrst illustrate the problem setup and experiments using the MNIST dataset. We used the
LeNet (LeCun et al.  1998) architecture  and  as with all our experiments  we follow standard training 
validation  testing and hyperparameter tuning protocols. However  we also compute predictions on
increasingly shifted data (in this case increasingly rotated or horizontally translated images) and study

4

(a) Rotated MNIST

(b) Translated MNIST

(c) Conﬁdence vs Acc Rotated 60

(d) Count vs Conﬁdence Rotated 60

(e) Entropy on OOD

(f) Conﬁdence on OOD

Figure 1: Results on MNIST: 1(a) and 1(b) show accuracy and Brier score as the data is increasingly
shifted. Shaded regions represent standard error over 10 runs. To understand the discrepancy between
accuracy and Brier score  we explore the predictive distributions of each method by looking at the
conﬁdence of the predictions in 1(c) and 1(d). We also explore the entropy and conﬁdence of each
method on entirely OOD data in 1(e) and 1(f). SVI has lower accuracy on the validation and test
splits  but it is signiﬁcantly more robust to dataset shift as evidenced by a lower Brier score  lower
overall conﬁdence 1(d) and higher predictive entropy under shift (1(c)) and OOD data (1(e) 1(f)).

the behavior of the predictive distributions of the models. In addition  we predict on a completely
OOD dataset  Not-MNIST (Bulatov  2011)  and observe the entropy of the model’s predictions. We
summarize some of our ﬁndings in Figure 1 and discuss below.
What we would like to see: Naturally  we expect the accuracy of a model to degrade as it predicts
on increasingly shifted data  and ideally this reduction in accuracy would coincide with increased
forecaster entropy. A model that was well-calibrated on the training and validation distributions would
ideally remain so on shifted data. If calibration (ECE or Brier reliability) remained as consistent
as possible  practitioners and downstream tasks could take into account that a model is becoming
increasingly uncertain. On the completely OOD data  one would expect the predictive distributions to
be of high entropy. Essentially  we would like the predictions to indicate that a model “knows what it
does not know” due to the inputs straying away from the training data distribution.
What we observe: We see in Figures 1(a) and 1(b) that accuracy certainly degrades as a function of
shift for all methods tested  and they are difﬁcult to disambiguate on that metric. However  the Brier
score paints a clearer picture and we see a signiﬁcant difference between methods  i.e. prediction
quality degrades more signiﬁcantly for some methods than others. An important observation is that
while calibrating on the validation set leads to well-calibrated predictions on the test set  it does
not guarantee calibration on shifted data. In fact  nearly all other methods (except vanilla) perform
better than the state-of-the-art post-hoc calibration (Temperature scaling) in terms of Brier score
under shift. While SVI achieves the worst accuracy on the test set  it actually outperforms all other
methods by a much larger margin when exposed to signiﬁcant shift. In Figures 1(c) and 1(d) we look
at the distribution of conﬁdences for each method to understand the discrepancy between metrics. We
see in Figure 1(d) that SVI has the lowest conﬁdence in general but in Figure 1(c) we observe that
SVI gives the highest accuracy at high conﬁdence (or conversely is much less frequently conﬁdently
wrong)  which can be important for high-stakes applications. Most methods demonstrate very low
entropy (Figure 1(e)) and give high conﬁdence predictions (Figure 1(f)) on data that is entirely OOD 
i.e. they are conﬁdently wrong about completely OOD data.

5

0.10.20.30.40.50.60.70.80.91.0Accuracy9alid7eVt15◦30◦45◦60◦75◦90◦105◦120◦135◦150◦165◦180◦IntenVity oI 6hiIt0.00.20.40.60.81.01.21.41.6Brier0.00.20.40.60.81.0AccuracyValidTeVt2Sx4Sx6Sx8Sx10Sx12Sx14SxIntenVity oI 6hiIt0.00.20.40.60.81.01.21.41.6Brier0.00.20.40.60.81.0τ0.20.30.40.50.60.70.8AccurDcy on exDmSleV p(y|x)≥τVDnLllD7emS 6cDlLngEnVemEleLL-DroSout6VILL-6VIDroSout0.00.20.40.60.81.0τ02000400060008000100001umEer oI exDmSleV p(y|x)≥τVDnLllDTemS 6cDlLngEnVemEleLL-DroSout6VILL-6VIDroSout0.00.51.01.52.02.5(ntroSy (1DtV)050001000015000200002500030000# oI (xDmSleVVDnLllDSVILL-DroSoutLL-SVI(nVemEleTemS ScDlLngDroSout0.00.20.40.60.81.0τ02000400060008000100001umEer oI exDmSleV p(y|x)≥τVDnLllDTemS 6cDlLngEnVemEleLL-DroSout6VILL-6VIDroSoutFigure 2: Calibration under distributional shift: a detailed comparison of accuracy and ECE under
all types of corruptions on (a) CIFAR-10 and (b) ImageNet. For each method we show the mean on
the test set and summarize the results on each intensity of shift with a box plot. Each box shows the
quartiles summarizing the results across all (16) types of shift while the error bars indicate the min
and max across different shift types. Figures showing additional metrics are provided in Figures S4
(CIFAR-10) and S5 (ImageNet). Tables for numerical comparisons are provided in Appendix G.

Image Models: CIFAR-10 and ImageNet

4.2
We now study the predictive distributions of residual networks (He et al.  2016) trained on two
benchmark image datasets  CIFAR-10 (Krizhevsky  2009) and ImageNet (Deng et al.  2009)  under
distributional shift. We use 20-layer and 50-layer ResNets for CIFAR-10 and ImageNet respectively.
For shifted data we use 80 different distortions (16 different types with 5 levels of intensity each  see
Appendix B for illustrations) introduced by Hendrycks & Dietterich (2019). To evaluate predictions
of CIFAR-10 models on entirely OOD data  we use the SVHN dataset (Netzer et al.  2011).
Figure 2 summarizes the accuracy and ECE for CIFAR-10 (top) and ImageNet (bottom) across all 80
combinations of corruptions and intensities from (Hendrycks & Dietterich  2019). Figure 3 inspects
the predictive distributions of the models on CIFAR-10 (top) and ImageNet (bottom) for shifted
(Gaussian blur) and OOD data. Classiﬁers on both datasets show poorer accuracy and calibration
with increasing shift. Comparing accuracy for different methods  we see that ensembles achieve

6

Test12345Shift intensity0.20.30.40.50.60.70.80.91.0AccuracyMethodVanillaTemp ScalingEnsembleDropoutLL DropoutSVILL SVITest12345Shift intensity0.00.10.20.30.40.50.60.70.8AccuracyMethodVanillaTemp ScalingEnsembleDropoutLL DropoutLL SVITest12345Shift intensity0.000.050.100.150.200.250.300.35ECEMethodVanillaTemp ScalingEnsembleDropoutLL DropoutLL SVITest12345Shift intensity0.00.10.20.30.40.50.60.7ECEMethodVanillaTemp ScalingEnsembleDropoutLL DropoutSVILL SVI(a) CIFAR-10(b) ImageNet(a) CIFAR: Conﬁdence vs Accuracy (b) CIFAR: Count vs Conﬁdence

(c) CIFAR: Entropy on OOD

(d) ImageNet: Conﬁdence vs Acc (e) ImageNet: Count vs Conﬁdence

(f) CIFAR: Conﬁdence on OOD

Figure 3: Results on CIFAR-10 and ImageNet. Left column: 3(a) and 3(d) show accuracy as a
function of conﬁdence. Middle column: 3(b) and 3(e) show the number of examples greater than
given conﬁdence values for Gaussian blur of intensity 3. Right column: 3(c) and 3(f) show histogram
of entropy and conﬁdences from CIFAR-trained models on a completely different dataset (SVHN).

highest accuracy under distributional shift. Comparing the ECE for different methods  we observe
that while the methods achieve comparable low values of ECE for small values of shift  ensembles
outperform the other methods for larger values of shift. To test whether this result is due simply to
the larger aggregate capacity of the ensemble  we trained models with double the number of ﬁlters
for the Vanilla and Dropout methods. The higher-capacity models showed no better accuracy or
calibration for medium- to high-shift than the corresponding lower-capacity models (see Appendix C).
In Figures S8 and S9 we also explore the effect of the number of samples used in dropout  SVI and
last layer methods and size of the ensemble  on CIFAR-10. We found that while increasing ensemble
size up to 50 did help  most of the gains of ensembling could be achieved with only 5 models.
Interestingly  while temperature scaling achieves low ECE for low values of shift  the ECE increases
signiﬁcantly as the shift increases  which indicates that calibration on the i.i.d. validation dataset
does not guarantee calibration under distributional shift. (Note that for ImageNet  we found similar
trends considering just the top-5 predicted classes  See Figure S5.) Furthermore  the results show that
while temperature scaling helps signiﬁcantly over the vanilla method  ensembles and dropout tend to
be better. In Figure 3  we see that ensembles and dropout are more accurate at higher conﬁdence.
However  in 3(c) we see that temperature scaling gives the highest entropy on OOD data. Ensembles
consistently have high accuracy but also high entropy on OOD data. We refer to Appendix C for
additional results; Figures S4 and S5 report additional metrics on CIFAR-10 and ImageNet  such as
Brier score (and its component terms)  as well as top-5 error for increasing values of shift.
Overall  ensembles consistently perform best across metrics and dropout consistently performed
better than temperature scaling and last layer methods. While the relative ordering of methods is
consistent on both CIFAR-10 and ImageNet (ensembles perform best)  the ordering is quite different
from that on MNIST where SVI performs best. Interestingly  LL-SVI and LL-Dropout perform worse
than the vanilla method on shifted datasets as well as SVHN. We also evaluate a variational Gaussian
process as a last layer method in Appendix E but it did not outperform LL-SVI and LL-Dropout.

4.3 Text Models
Following Hendrycks & Gimpel (2017)  we train an LSTM (Hochreiter & Schmidhuber  1997) on
the 20newsgroups dataset (Lang  1995) and assess the model’s robustness under distributional shift

7

0.00.20.40.60.81.0τ0.650.700.750.800.850.900.951.00AccurDcy on exDmSleV p(y|x)≥τ9DnLllD7emS 6cDlLngEnVemEleDroSoutLL-DroSout69ILL-69I0.00.20.40.60.81.0τ100020003000400050006000700080009000100001umEer oI exDmSleV p(y|x)≥τ9DnLllD7emS 6cDlLngEnVemEleDroSoutLL-DroSout69ILL-69I0.00.51.01.52.02.5(ntroSy (1DtV)0100020003000400050006000700080009000# oI (xDmSleV9DnLllDDroSout69ILL-DroSoutLL-69I(nVemEle7emS 6cDlLng0.00.10.20.30.40.50.60.70.80.9τ0.30.40.50.60.70.80.91.0AccurDcy on exDmSleV p(y|x)≥τ9DnLllDLL 69IDroSoutLL DroSoutEnVemEle7emS 6cDlLng0.00.10.20.30.40.50.60.70.80.9τ010000200003000040000500001umEer oI exDmSleV p(y|x)≥τ9DnLllDLL 69IDroSoutLL DroSoutEnVemEle7emS 6cDlLng0.00.20.40.60.81.0τ02000400060008000100001umEer oI exDmSleV p(y|x)≥τVDnLllDTemS 6cDlLngEnVemEleDroSoutLL-DroSout6VILL-6VI(a) Conﬁdence vs Acc.

(b) Conﬁdence vs Count (c) Conﬁdence vs Accuracy (d) Conﬁdence vs Count

Figure 4: Top row: Histograms of the entropy of the predictive distributions for in-distribution (solid
lines)  shifted (dotted lines)  and completely different OOD (dashed lines) text examples. Bottom
row: Conﬁdence score vs accuracy and count respectively when evaluated for in-distribution and
in-distribution shift text examples (a b)  and in-distribution and OOD text examples (c d).

and OOD text. We use the even-numbered classes (10 classes out of 20) as in-distribution and the 10
odd-numbered classes as shifted data. We provide additional details in Appendix A.4.
We look at conﬁdence vs accuracy when the test data consists of a mix of in-distribution and either
shifted or completely OOD data  in this case the One Billion Word Benchmark (LM1B) (Chelba
et al.  2013). Figure 4 (bottom row) shows the results. Ensembles signiﬁcantly outperform all other
methods  and achieve better trade-off between accuracy versus conﬁdence. Surprisingly  LL-Dropout
and LL-SVI perform worse than the vanilla method  giving higher conﬁdence incorrect predictions 
especially when tested on fully OOD data.
Figure 4 reports histograms of predictive entropy on in-distribution data and compares them to those
for the shifted and OOD datasets. This reﬂects how amenable each method is to abstaining from
prediction by applying a threshold on the entropy. As expected  most methods achieve the highest
predictive entropy on the completely OOD dataset  followed by the shifted dataset and then the
in-distribution test dataset. Only ensembles have consistently higher entropy on the shifted data 
which explains why they perform best on the conﬁdence vs accuracy curves in the second row of
Figure 4. Compared with the vanilla model  Dropout and LL-SVI have more a distinct separation
between in-distribution and shifted or OOD data. While Dropout and LL-Dropout perform similarly
on in-distribution  LL-Dropout exhibits less uncertainty than Dropout on shifted and OOD data.
Temperature scaling does not appear to increase uncertainty signiﬁcantly on the shifted data.

4.4 Ad-Click Model with Categorical Features
Finally  we evaluate the performance of different methods on the Criteo Display Advertising Chal-
lenge6 dataset  a binary classiﬁcation task consisting of 37M examples with 13 numerical and 26
categorical features per example. We introduce shift by reassigning each categorical feature to a
random new token with some ﬁxed probability that controls the intensity of shift. This coarsely
simulates a type of shift observed in non-stationary categorical features as category tokens appear
and disappear over time  for example due to hash collisions. The model consists of a 3-hidden-layer
multi-layer-perceptron (MLP) with hashed and embedded categorical features and achieves a negative
log-likelihood of approximately 0.5 (contest winners achieved 0.44). Due to class imbalance (⇠ 25%
of examples are positive)  we report AUC instead of classiﬁcation accuracy.
Results from these experiments are depicted in Figure 5. (Figure S7 in Appendix C shows additional
results including ECE and Brier score decomposition.) We observe that ensembles are superior
in terms of both AUC and Brier score for most of the values of shift  with the performance gap
between ensembles and other methods generally increasing as the shift increases. Both Dropout
model variants yielded improved AUC on shifted data  and Dropout surpassed ensembles in Brier

6https://www.kaggle.com/c/criteo-display-ad-challenge

8

0.00.51.01.52.02.5EntroSy0.00.51.01.52.02.53.03.5DenVityVDniOODIn-diVt.Skewed22D−0.50.00.51.01.52.02.5EntroSy0.00.51.01.52.02.53.03.5LL-SVI0.00.51.01.52.02.5Entropy0.00.51.01.52.02.53.03.5Dropout0.00.51.01.52.02.5Entropy0.00.51.01.52.02.53.03.5LL-Dropout0.00.51.01.52.02.5Entropy0.00.51.01.52.02.53.03.5EnsemEle0.00.51.01.52.02.5EntroSy0.00.51.01.52.02.53.03.5TemS Scaling0.00.10.20.30.40.50.60.70.80.9τ405060708090100AccurDcy on exDmSleV p(y|x)≥τ9DnLllDLL-69IDroSoutLL-DroSoutEnVemEle7emS 6cDlLng0.00.10.20.30.40.50.60.70.80.9τ0200040006000800010000120001umEer oI exDmSleV p(y|x)≥τ9DnLllDLL-69IDroSoutLL-DroSoutEnVemEle7emS 6cDlLng0.00.10.20.30.40.50.60.70.80.9τ405060708090100AccurDcy on exDmSleV p(y|x)≥τ9DnLllDLL-69IDroSoutLL-DroSoutEnVemEle7emS 6cDlLng0.00.10.20.30.40.50.60.70.80.9τ0200040006000800010000120001umEer oI exDmSleV p(y|x)≥τ9DnLllDLL-69IDroSoutLL-DroSoutEnVemEle7emS 6cDlLngFigure 5: Results on Criteo: The ﬁrst two plots show degrading AUCs and Brier scores with increasing
shift while the latter two depict the distribution of prediction conﬁdences and their corresponding
accuracies at 75% randomization of categorical features. SVI is excluded as it performed too poorly.

score at shift-randomization values above 60%. SVI proved challenging to train  and the resulting
model uniformly performed poorly; LL-SVI fared better but generally did not improve upon the
vanilla model. Strikingly  temperature scaling has a worse Brier score than Vanilla indicating that
post-hoc calibration on the validation set actually harms calibration under dataset shift.

5 Takeaways and Recommendations
We presented a large-scale evaluation of different methods for quantifying predictive uncertainty
under dataset shift  across different data modalities and architectures. Our take-home messages are
the following:

• Along with accuracy  the quality of uncertainty consistently degrades with increasing dataset shift

regardless of method.

• Better calibration and accuracy on the i.i.d. test dataset does not usually translate to better

calibration under dataset shift (shifted versions as well as completely different OOD data).

• Post-hoc calibration (on i.i.d validation) with temperature scaling leads to well-calibrated uncer-
tainty on the i.i.d. test set and small values of shift  but is signiﬁcantly outperformed by methods
that take epistemic uncertainty into account as the shift increases.

• Last layer Dropout exhibits less uncertainty on shifted and OOD datasets than Dropout.
• SVI is very promising on MNIST/CIFAR but it is difﬁcult to get to work on larger datasets such

as ImageNet and other architectures such as LSTMs.

• The relative ordering of methods is mostly consistent (except for MNIST) across our experiments.
The relative ordering of methods on MNIST is not reﬂective of their ordering on other datasets.
• Deep ensembles seem to perform the best across most metrics and be more robust to dataset shift.
We found that relatively small ensemble size (e.g. M = 5) may be sufﬁcient (Appendix D).
• We also compared the set of methods on a real-world challenging genomics problem from Ren
et al. (2019). Our observations were consistent with the other experiments in the paper. Deep
ensembles performed best  but there remains signiﬁcant room for improvement  as with the other
experiments in the paper. See Section F for details.

We hope that this benchmark is useful to the community and inspires more research on uncertainty
under dataset shift  which seems challenging for existing methods. While we focused only on the
quality of predictive uncertainty  applications may also need to consider computational and memory
costs of the methods; Table S1 in Appendix A.9 discusses these costs  and the best performing
methods tend to be more expensive. Reducing the computational and memory costs  while retaining
the same performance under dataset shift  would also be a key research challenge.
Acknowledgements
We thank Alexander D’Amour  Jakub ´Swia¸tkowski and our reviewers for helpful feedback that
improved the manuscript.

9

7Uain9alid7eVt5%15%25%35%45%55%65%75%85%95%0.550.600.650.700.750.80A8C7rDLn9DlLd7eVt5%15%25%35%45%55%65%75%85%95%0.320.340.360.380.400.420.440.46%rLer 6core9DnLllDDroSoutLL-DroSoutLL-69I7emS 6cDlLngEnVemEle0.40.50.60.70.80.91.0τ010000200003000040000500001umEer oI exDmSleV p(y|x)≥τ9DnLllDDroSoutLL-DroSoutLL-69I7emS 6cDlLngEnVemEle0.40.50.60.70.80.91.0τ0.700.750.800.850.900.951.00AccurDcy on exDmSleV p(y|x)≥τ9DnLllDDroSoutLL-DroSoutLL-69I7emS 6cDlLngEnVemEleReferences
Alemi  A. A.  Fischer  I.  and Dillon  J. V. Uncertainty in the variational information bottleneck.

arXiv preprint arXiv:1807.00906  2018.

Amodei  D.  Olah  C.  Steinhardt  J.  Christiano  P.  Schulman  J.  and Man´e  D. Concrete problems

in AI safety. arXiv preprint arXiv:1606.06565  2016.

Behrmann  J.  Duvenaud  D.  and Jacobsen  J.-H. Invertible residual networks. arXiv preprint

arXiv:1811.00995  2018.

Bishop  C. M. Novelty Detection and Neural Network Validation. IEE Proceedings-Vision  Image

and Signal processing  141(4):217–222  1994.

Blundell  C.  Cornebise  J.  Kavukcuoglu  K.  and Wierstra  D. Weight uncertainty in neural networks.

In ICML  2015.

Bojarski  M.  Testa  D. D.  Dworakowski  D.  Firner  B.  Flepp  B.  Goyal  P.  Jackel  L. D.  Monfort 
M.  Muller  U.  Zhang  J.  Zhang  X.  Zhao  J.  and Zieba  K. End to end learning for self-driving
cars. arXiv preprint arXiv:1604.07316  2016.

Brier  G. W. Veriﬁcation of forecasts expressed in terms of probability. Monthly weather review 

1950.

Br¨ocker  J. Reliability  sufﬁciency  and the decomposition of proper scores. Quarterly Journal of the

Royal Meteorological Society  135(643):1512–1519  2009.

Bulatov  Y. NotMNIST dataset  2011. URL http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html.

Chelba  C.  Mikolov  T.  Schuster  M.  Ge  Q.  Brants  T.  Koehn  P.  and Robinson  T. One
billion word benchmark for measuring progress in statistical language modeling. arXiv preprint
arXiv:1312.3005  2013.

DeGroot  M. H. and Fienberg  S. E. The comparison and evaluation of forecasters. The statistician 

1983.

Deng  J.  Dong  W.  Socher  R.  Li  L.-J.  Li  K.  and Fei-Fei  L. ImageNet: A Large-Scale Hierarchical

Image Database. In Computer Vision and Pattern Recognition  2009.

Esteva  A.  Kuprel  B.  Novoa  R. A.  Ko  J.  Swetter  S. M.  Blau  H. M.  and Thrun  S. Dermatologist-

level classiﬁcation of skin cancer with deep neural networks. Nature  542  1 2017.

Gal  Y. and Ghahramani  Z. Dropout as a Bayesian approximation: Representing model uncertainty

in deep learning. In ICML  2016.

Geifman  Y. and El-Yaniv  R. Selective classiﬁcation for deep neural networks. In NeurIPS  2017.

Gneiting  T. and Raftery  A. E. Strictly proper scoring rules  prediction  and estimation. Journal of

the American Statistical Association  102(477):359–378  2007.

Golovin  D.  Solnik  B.  Moitra  S.  Kochanski  G.  Karro  J.  and Sculley  D. Google vizier: A service
for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining  pp. 1487–1495. ACM  2017.

Graves  A. Practical variational inference for neural networks. In NeurIPS  2011.

Guo  C.  Pleiss  G.  Sun  Y.  and Weinberger  K. Q. On calibration of modern neural networks. In

International Conference on Machine Learning  2017.

He  K.  Zhang  X.  Ren  S.  and Sun  J. Deep residual learning for image recognition. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  pp. 770–778  2016.

Hendrycks  D. and Dietterich  T. Benchmarking neural network robustness to common corruptions

and perturbations. In ICLR  2019.

10

Hendrycks  D. and Gimpel  K. A Baseline for Detecting Misclassiﬁed and Out-of-Distribution

Examples in Neural Networks. In ICLR  2017.

Hensman  J.  Matthews  A.  and Ghahramani  Z. Scalable variational gaussian process classiﬁcation.

In International Conference on Artiﬁcial Intelligence and Statistics. JMLR  2015.

Hern´andez-Lobato  J. M. and Adams  R. Probabilistic Backpropagation for Scalable Learning of

Bayesian Neural Networks. In ICML  2015.

Hochreiter  S. and Schmidhuber  J. Long short-term memory. Neural Comput.  9(8):1735–1780 

November 1997.

Kendall  A. and Gal  Y. What uncertainties do we need in Bayesian deep learning for computer

vision? In NeurIPS  2017.

Kingma  D. and Ba  J. Adam: A Method for Stochastic Optimization. In ICLR  2014.
Kingma  D. P.  Mohamed  S.  Rezende  D. J.  and Welling  M. Semi-supervised learning with deep

generative models. In NeurIPS  2014.

Kingma  D. P.  Salimans  T.  and Welling  M. Variational dropout and the local reparameterization

trick. In NeurIPS  2015.

Klambauer  G.  Unterthiner  T.  Mayr  A.  and Hochreiter  S. Self-normalizing neural networks. In

NeurIPS  2017.

Krizhevsky  A. Learning multiple layers of features from tiny images. 2009.
Lakshminarayanan  B.  Pritzel  A.  and Blundell  C. Simple and Scalable Predictive Uncertainty

Estimation Using Deep Ensembles. In NeurIPS  2017.

Lang  K. Newsweeder: Learning to ﬁlter netnews. In Machine Learning. 1995.
LeCun  Y.  Bottou  L.  Bengio  Y.  and Haffner  P. Gradient-based learning applied to document

recognition. In Proceedings of the IEEE  November 1998.

Lee  K.  Lee  K.  Lee  H.  and Shin  J. A simple uniﬁed framework for detecting out-of-distribution

samples and adversarial attacks. In NeurIPS  2018.

Liang  S.  Li  Y.  and Srikant  R. Enhancing the Reliability of Out-of-Distribution Image Detection in

Neural Networks. ICLR  2018.

Lipton  Z. C. and Steinhardt  J. Troubling trends in machine learning scholarship. arXiv preprint

arXiv:1807.03341  2018.

Louizos  C. and Welling  M. Structured and efﬁcient variational deep learning with matrix Gaussian

posteriors. arXiv preprint arXiv:1603.04733  2016.

Louizos  C. and Welling  M. Multiplicative Normalizing Flows for Variational Bayesian Neural

Networks. In ICML  2017.

MacKay  D. J. Bayesian methods for adaptive models. PhD thesis  California Institute of Technology 

1992.

MacKay  D. J. and Gibbs  M. N. Density Networks. Statistics and Neural Networks: Advances at the

Interface  1999.

Naeini  M. P.  Cooper  G. F.  and Hauskrecht  M. Obtaining Well Calibrated Probabilities Using

Bayesian Binning. In AAAI  pp. 2901–2907  2015.

Nalisnick  E.  Matsukawa  A.  Teh  Y. W.  Gorur  D.  and Lakshminarayanan  B. Hybrid models with

deep and invertible features. arXiv preprint arXiv:1902.02767  2019.

Netzer  Y.  Wang  T.  Coates  A.  Bissacco  A.  Wu  B.  and Ng  A. Y. Reading Digits in Natural
In NeurIPS Workshop on Deep Learning and

Images with Unsupervised Feature Learning.
Unsupervised Feature Learning  2011.

11

Osband  I.  Blundell  C.  Pritzel  A.  and Van Roy  B. Deep exploration via bootstrapped DQN. In

NeurIPS  2016.

Platt  J. C. Probabilistic outputs for support vector machines and comparisons to regularized likelihood

methods. In Advances in Large Margin Classiﬁers  pp. 61–74. MIT Press  1999.

Quinonero-Candela  J.  Rasmussen  C. E.  Sinz  F.  Bousquet  O.  and Sch¨olkopf  B. Evaluating

predictive uncertainty challenge. In Machine Learning Challenges. Springer  2006.

Rahimi  A. and Recht  B. An addendum to alchemy  2017.
Ren  J.  Liu  P. J.  Fertig  E.  Snoek  J.  Poplin  R.  DePristo  M. A.  Dillon  J. V.  and Lakshmi-
narayanan  B. Likelihood ratios for out-of-distribution detection. arXiv preprint arXiv:1906.02845 
2019.

Riquelme  C.  Tucker  G.  and Snoek  J. Deep Bayesian Bandits Showdown: An Empirical Compari-

son of Bayesian Deep Networks for Thompson Sampling. In ICLR  2018.

Sculley  D.  Snoek  J.  Wiltschko  A.  and Rahimi  A. Winner’s curse? On pace  progress  and

empirical rigor. 2018.

Shafaei  A.  Schmidt  M.  and Little  J. J. Does Your Model Know the Digit 6 Is Not a Cat? A Less

Biased Evaluation of “Outlier” Detectors. ArXiv e-Print arXiv:1809.04729  2018.

Srivastava  R. K.  Greff  K.  and Schmidhuber  J. Training Very Deep Networks. In NeurIPS  2015.
Sugiyama  M.  Lawrence  N. D.  Schwaighofer  A.  et al. Dataset shift in machine learning. The MIT

Press  2017.

Welling  M. and Teh  Y. W. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In

ICML  2011.

Wen  Y.  Vicol  P.  Ba  J.  Tran  D.  and Grosse  R. Flipout: Efﬁcient pseudo-independent weight

perturbations on mini-batches. arXiv preprint arXiv:1803.04386  2018.

Wu  A.  Nowozin  S.  Meeds  E.  Turner  R. E.  Hernandez-Lobato  J. M.  and Gaunt  A. L. Determin-

istic Variational Inference for Robust Bayesian Neural Networks. In ICLR  2019.

12

,Yaniv Ovadia
Emily Fertig
Jie Ren
Zachary Nado
D. Sculley
Sebastian Nowozin
Joshua Dillon
Balaji Lakshminarayanan
Jasper Snoek