2019,Acceleration via Symplectic Discretization of High-Resolution Differential Equations,We study first-order optimization algorithms obtained by discretizing ordinary differential equations (ODEs) corresponding to Nesterov’s accelerated gradient methods (NAGs) and Polyak’s heavy-ball method. We consider three discretization schemes: symplectic Euler (S)  explicit Euler (E) and implicit Euler (I) schemes. We show that the optimization algorithm generated by applying the symplectic scheme to a high-resolution ODE proposed by Shi et al. [2018] achieves the accelerated rate for minimizing both strongly convex function and convex function. On the other hand  the resulting algorithm either fails to achieve acceleration or is impractical when the scheme is implicit  the ODE is low-resolution  or the scheme is explicit.,Acceleration via Symplectic Discretization of

High-Resolution Differential Equations

Bin Shi

University of California  Berkeley

binshi@berkeley.edu

Simon S. Du

Institute for Advanced Study

ssdu@ias.edu

Weijie J. Su

University of Pennsylvania

suw@wharton.upenn.edu

Michael I. Jordan

University of California  Berkeley
jordan@cs.berkeley.edu

Abstract

We study ﬁrst-order optimization algorithms obtained by discretizing ordinary
differential equations (ODEs) corresponding to Nesterov’s accelerated gradient
methods (NAGs) and Polyak’s heavy-ball method. We consider three discretization
schemes: symplectic Euler (S)  explicit Euler (E) and implicit Euler (I) schemes.
We show that the optimization algorithm generated by applying the symplectic
scheme to a high-resolution ODE proposed by Shi et al. [2018] achieves the accel-
erated rate for minimizing both strongly convex functions and convex functions.
On the other hand  the resulting algorithm either fails to achieve acceleration or is
impractical when the scheme is implicit  the ODE is low-resolution  or the scheme
is explicit.

1

Introduction

In this paper  we consider unconstrained minimization problems:

f (x) 

(1.1)

min
x2Rn

where f is a smooth convex function. The touchstone method in this setting is gradient descent (GD):
(1.2)
where x0 is a given initial point and s > 0 is the step size. Whether there exist methods that improve
on GD while remaining within the framework of ﬁrst-order optimization is a subtle and important
question.
Modern attempts to address this question date to Polyak [1964  1987]  who incorporated a momentum
term into the gradient step  yielding a method that is referred to as the heavy-ball method:

xk+1 = xk  srf (xk) 

yk+1 = xk  srf (xk) 

xk+1 = yk+1  ↵(xk  xk1) 

(1.3)
where ↵> 0 is a momentum coefﬁcient. While the heavy-ball method provably attains a faster rate
of local convergence than GD near a minimum of f  it generally does not provide a guarantee of
acceleration globally [Polyak  1964].
The next major development in ﬁrst-order methods is due to Nesterov  who introduced ﬁrst-order
gradient methods that have a faster global convergence rate than GD [Nesterov  1983  2013]. For a
µ-strongly convex objective f with L-Lipschitz gradients  Nesterov’s accelerated gradient method
(NAG-SC) involves the following pair of update equations:

yk+1 = xk  srf (xk) 

xk+1 = yk+1 +

(yk+1  yk) .

(1.4)

1  pµs
1 + pµs

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

If one sets s = 1/L  then NAG-SC enjoys a O⇣(1 pµ/L)k⌘ convergence rate  improving on
the O⇣(1  µ/L)k⌘ convergence rate of GD. Nesterov also developed an accelerated algorithm

(NAG-C) targeting smooth convex functions that are not strongly convex:

yk+1 = xk  srf (xk) 

xk+1 = yk+1 +

k

k + 3

(yk+1  yk).

(1.5)

This algorithm has a O(L/k2) convergence rate  which is faster than GD’s O(L/k) rate.
While yielding optimal and effective algorithms  the design principle of Nesterov’s accelerated
gradient algorithms (NAG) is not transparent. Convergence proofs for NAG often use the estimate
sequence technique  which is inductive in nature and relies on series of algebraic tricks [Bubeck 
2015]. In recent years progress has been made in the understanding of acceleration by moving to a
continuous-time formulation. In particular  Su et al. [2016] showed that as s ! 0  NAG-C converges
to an ordinary differential equation (ODE) (Equation (2.2)); moreover  for this ODE  Su et al. [2016]
derived a (continuous-time) convergence rate using a Lyapunov function  and further transformed
this Lyapunov function to a discrete version and thereby provided a new proof of the fact that
NAG-C enjoys a O(L/k2) rate.
Further progress in this vein has involved taking a variational point of view that derives ODEs from
an underlying Lagrangian rather than from a limiting argument [Wibisono et al.  2016]. While this
approach captures many of the variations of Nesterov acceleration presented in the literature  it does
not distinguish between the heavy-ball dynamics and the NAG dynamics  and thus fails to distinguish
between local and global acceleration. More recently  Shi et al. [2018] have returned to limiting
arguments with a more sophisticated methodology. They have derived high-resolution ODEs for the
heavy-ball method (Equation (2.4))  NAG-SC (Equation (2.5)) and NAG-C (Equation (2.6)). Notably 
the high-resolution ODEs for the heavy-ball dynamics and the accelerated dynamics are different.
Shi et al. [2018] also presented Lyapunov functions for these ODEs as well as the corresponding
algorithms  and showed that these Lyapunov functions can be used to derive the accelerated rates
of NAG-SC and NAG-C. A number of other papers have also contributed to the understanding of
acceleration by working in a continuous-time formulation [Krichene and Bartlett  2017  Krichene
et al.  2015  Diakonikolas and Orecchia  2017  Ghadimi and Lan  2016  Diakonikolas and Orecchia 
2017].
This emerging literature has thus provided a new level of understanding of design principles for
accelerated optimization. The design involves an interplay between continuous-time and discrete-time
dynamics. ODEs are obtained either variationally or via a limiting scheme  and various properties of
the ODEs are studied  including their convergence rate  topological aspects of their ﬂow and their
behavior under perturbation. Lyapunov functions play a key role in such analyses  and also allow
aspects of the continuous-time analysis to be transferred to discrete time [see  e.g.  Wilson et al. 
2016].
And yet the literature has not yet provided a full exploration of the transition from continuous-time
ODEs to discrete-time algorithms. Indeed  this transition is a non-trivial one  as evidenced by the
decades of research on numerical methods for the discretization of ODEs  including most notably the
sophisticated arsenal of techniques referred to as “geometric numerical integration” that are used for
ODEs obtained from underlying variational principles [Hairer et al.  2006]. Recent work has begun
to explore these issues; examples include the use of symplectic integrators by Betancourt et al. [2018]
and the use of Runge-Kutta integration by Zhang et al. [2018]. However  these methods do not
always yield proofs that accelerated rates are retained in discrete time  and when they do they involve
implicit discretization  which is generally not practical except in the setting of quadratic objectives.
Thus we wish to address the following fundamental question:

Can we systematically and provably obtain new accelerated methods via the numerical discretization

of ordinary differential equations?

Our approach to this question is a dynamical systems framework based on Lyapunov theory. Our
main results are as follows:
1. In Section 3.1  we consider three simple numerical discretization schemes—symplectic Euler
(S)  explicit Euler (E) and implicit Euler (I) schemes—to discretize the high-resolution ODE of

2

Nesterov’s accelerated method for strongly convex functions. We show that the optimization

method generated by symplectic discretization achieves a O((1  O(1)pµ/L)k) rate  thereby

attaining acceleration. In sharp contrast  the implicit scheme is not practical for implementation 
and the explicit scheme  while being simple  fails to achieve acceleration.

2. In Section 3.2  we apply these discretization schemes to the ODE for modeling the heavy-ball
method  which can be viewed as a low-resolution ODE that lacks a gradient-correction term [Shi
et al.  2018]. In contrast to the previous two cases of high-resolution ODEs  the symplectic scheme
does not achieve acceleration for this low-resolution ODE. More broadly  in Appendix D we
present more examples of low-resolution ODEs where symplectic discretization does not lead to
acceleration.

3. Next  we apply the three simple Euler schemes to the high-resolution ODE of Nesterov’s acceler-
ated method for convex functions. Again  our Lyapunov analysis sheds light on the superiority of
the symplectic scheme over the other two schemes. This is the subject of Section 4.

Taken together  the three ﬁndings have the implication that high-resolution ODEs and symplectic
schemes are critical to achieving acceleration using numerical discretization. More precisely  in
addition to allowing relatively simple implementations  symplectic schemes allow for a large step size
without a loss of stability  in a manner akin to (but better than) implicit schemes. In stark contrast 
in the setting of low-resolution ODEs  only the implicit schemes remain stable with a large step
size  due to the lack of gradient correction. Moreover  the choice of Lyapunov function is equally
essential to obtaining sharp convergence rates. This important fact is highlighted in Theorem A.6 in
the Appendix  where we analyze GD by considering it as a discretization method for gradient ﬂow
(the ODE counterpart of GD). Using the discrete version of the Lyapunov function proposed in Su
et al. [2016] instead of the classical one  we show that GD in fact minimizes the squared gradient
norm (choosing the best iterate so far) at a rate of O(L2/k2). Although this rate of convergence in
the problem of squared gradient norm minimization is known in the literature [Nesterov  2012]  the
Lyapunov function argument provides a systematic approach to obtaining this rate in this problem and
others. In particular  this example demonstrates the usefulness and ﬂexibility of Lyapunov functions
as a mathematical tool for optimization problems.

2 Preliminaries

In this section  we introduce necessary notation  and review ODEs derived in previous work and three
classical numerical discretization schemes.
We mostly follow the notation of Nesterov [2013]  with slight modiﬁcations tailored to the present
L(Rn) if
paper. Let F 1
f (y)  f (x) + hrf (x)  y  xi for all x  y 2 Rn and its gradient is L-Lipschitz continuous in the
sense that

L(Rn) be the class of L-smooth convex functions deﬁned on Rn; that is  f 2F 1

krf (x)  rf (y)k  Lkx  yk  

µ L(Rn) if f 2F p

µ L(Rn) denote the subclass of F p

L(Rn) and f (y)  f (x)+hrf (x)  y  xi+ µ

where k·k denotes the standard Euclidean norm and L > 0 is the Lipschitz constant. The function class
L(Rn) such that each f has a Lipschitz-continuous Hessian. For p = 1  2 
L(Rn) is the subclass of F 1
F 2
let S p
L(Rn) such that each member f is µ-strongly convex for some
2 ky  xk2
0 < µ  L. That is  f 2S p
for all x  y 2 Rn. Let x? denote a minimizer of f (x).
2.1 Approximating ODEs
In this section we list all of the ODEs that we will discretize in this paper. We refer readers to recent
papers by Su et al. [2016]  Wibisono et al. [2016] and Shi et al. [2018] for the rigorous derivations of
these ODEs. We begin with the simplest. Taking the step size s ! 0 in Equation (1.2)  we obtain the
following ODE (gradient ﬂow):
(2.1)

˙X = rf (X) 

with any initial X(0) = x0 2 Rn.
Next  by taking s ! 0 in Equation (1.5)  Su et al. [2016] derived the low-resolution ODE of NAG-C:
(2.2)

¨X +

3
t

˙X + rf (X) = 0 

3

¨X + 2pµ ˙X + rf (X) = 0

with X(0) = x0 and ˙X(0) = 0. For strongly convex functions  by taking s ! 0  one can derive the
following low-resolution ODE (see  for example  Wibisono et al. [2016])
(2.3)
that models both the heavy-ball method and NAG-SC. This ODE has the same initial conditions
as (2.2).
Recently  Shi et al. [2018] proposed high-resolution ODEs for modeling acceleration methods. The
key ingredient in these ODEs is that the O(ps) terms are preserved in the ODEs. As a result  the
heavy-ball method and NAG-SC have different models as ODEs.
(a) If f 2S 1

µ L(Rn)  the high-resolution ODE of the heavy-ball method (1.3) is

¨X + 2pµ ˙X + (1 + pµs)rf (X) = 0 

with X(0) = x0 and ˙X(0) =  2psrf (x0)
low-resolution counterpart (2.3) due to the absence of r2f (X) ˙X.

µ L(Rn)  the high-resolution ODE of NAG-SC (1.4) is

1+pµs

(b) If f 2S 2

. This ODE has essentially the same properties as its

(2.4)

(2.5)

(2.6)

¨X + 2pµ ˙X + psr2f (X) ˙X + (1 + pµs)rf (X) = 0 

with X(0) = x0 and ˙X(0) =  2psrf (x0)

1+pµs

.

(c) If f 2F 2

L(Rn)  the high-resolution ODE of NAG-C (1.5) is
3ps

2t ◆rf (X) = 0
for t  3ps/2  with X(3ps/2) = x0 and ˙X(3ps/2) = psrf (x0).

˙X + psr2f (X) ˙X +✓1 +

¨X +

3
t

rule:

2.2 Discretization schemes
To discretize ODEs (2.1)-(2.6)  we replace ˙X by xk+1  xk  ˙V by vk+1  vk and replace other terms
with approximations. Different discretization schemes correspond to different approximations.
• The most straightforward scheme is the explicit scheme  which uses the following approximation

xk+1  xk = psvk 
xk+1  xk = psvk+1 

• Another discretization scheme is the implicit scheme  which uses the following approximation

psr2f (xk)vk ⇡ rf (xk+1)  rf (xk).
psr2f (xk+1)vk+1 ⇡ rf (xk+1)  rf (xk).
Note that compared with the explicit scheme  the implicit scheme is not practical because the
update of xk+1 requires knowing vk+1 while the update of vk+1 requires knowing xk+1.

rule:

• The last discretization scheme considered in this paper is the symplectic scheme  which uses the

following approximation rule.

psr2f (xk+1)vk ⇡ rf (xk+1)  rf (xk).
Note this scheme is practical because the update of xk+1 only requires knowing vk.

xk+1  xk = psvk 

We remark that for low-resolution ODEs  there is no r2f (x) term  whereas for high-resolution ODEs 
we have this term and we use the difference of gradients to approximate this term. This additional
approximation term is critical to acceleration.

3 High-Resolution ODEs for Strongly Convex Functions

This section considers numerical discretization of the high-resolution ODEs of NAG-SC and the
heavy-ball method using the symplectic Euler  explicit Euler and implicit Euler scheme. In particular 
we compare rates of convergence towards the objective minimum of the three simple Euler schemes
and the two methods (NAG-SC and the heavy-ball method) in Section 3.1 and Section 3.2  respectively.
For both cases  the associated symplectic scheme is shown to exhibit surprisingly similarity to the
corresponding classical method.

4

3.1 NAG-SC
The high-resolution ODE (2.5) of NAG-SC can be equivalently written in the phase space as

˙X = V 

˙V = 2pµV  psr2f (X)V  (1 + pµs)rf (X) 
. For any f 2S 2
of Shi et al. [2018] shows that the solution X = X(t) of the ODE (2.5) satisﬁes

with the initial conditions X(0) = x0 and V (0) =  2psrf (x0)

1+pµs

(3.1)

µ L(Rn)  Theorem 1

f (X)  f (x?) 

2kx0  x?k2

s

pµt
4  

e

for any step size 0 < s  1/L. In particular  setting the step size to s = 1/L  we get

In the phase space representation  NAG-SC is formulated as

f (X)  f (x?)  2Lkx0  x?k2 e

pµt
4 .

xk+1  xk = psvk
vk+1  vk = 

2pµs
1  pµs

8><>:

with the initial condition v0 =  2psrf (x0)

1+pµs

of the ODE by recognizing

vk+1  ps(rf (xk+1)  rf (xk)) 

1 + pµs
1  pµs · psrf (xk+1) 

(3.2)
for any x0. This method maintains the accelerated rate

;

f (xk)  f (x?) 

5Lkx0  x?k2
(1 +pµ/L/12)k
(see Theorem 3 in Shi et al. [2018]) and the identiﬁcation t ⇡ kps.
Viewing NAG-SC as a numerical discretization of (2.5)  one might wonder if any of the three
simple Euler schemes—symplectic Euler scheme  explicit Euler scheme  and implicit Euler scheme—
maintain the accelerated rate in discretizing the high-resolution ODE. For clarity  the update rules of

1+pµs

the three schemes are given as follows  each with the initial points x0 and v0 =  2psrf (x0)
Euler scheme of (3.1): (S)  (E) and (I) respectively
(S) ( xk+1  xk = psvk
(E) ( xk+1  xk = psvk
(I) ( xk+1  xk = psvk+1

vk+1  vk = 2pµsvk+1  ps (rf (xk+1)  rf (xk))  ps(1 + pµs)rf (xk+1).
vk+1  vk = 2pµsvk  ps (rf (xk+1)  rf (xk))  ps(1 + pµs)rf (xk).
vk+1  vk = 2pµsvk+1  ps (rf (xk+1)  rf (xk))  ps(1 + pµs)rf (xk+1).

.

1

Among the three Euler schemes  the symplectic scheme is the closest to NAG-SC (3.2). More
precisely  NAG-SC differs from the symplectic scheme only in an additional factor of
1pµs in
the second line of (3.2). When the step size s is small  NAG-SC is  roughly speaking  a symplectic
method if we make use of
1pµs ⇡ 1. In relating to the literature  the connection between accelerated
methods and the symplectic schemes has been explored in Betancourt et al. [2018]  which mainly
considers the leapfrog integrator  a second-order symplectic integrator. In contrast  the symplectic
Euler scheme studied in this paper is a ﬁrst-order symplectic integrator.
Interestingly  the close resemblance between the two algorithms is found not only in their formulations 
but also in their convergence rates  which are both accelerated as shown by Theorem B.1 and
Theorem 3.1.
Note that the discrete Lyapunov function used in the proof of the symplectic Euler scheme of (3.1) is

1

E(k) =

1
4 kvkk2 +

1

42pµ(xk+1  x?) + vk + psrf (xk)2

5

+ (1 + pµs) (f (xk)  f (x?)) 

(1 + pµs)2
1 + 2pµs ·

s
2 krf (xk)k2 .

(3.3)

The proof of Theorem B.1 is deferred to Appendix B.1. The following result is a useful consequence
of this theorem.
Theorem 3.1 (Discretization of NAG-SC ODE). For any f 2S 1
hold:

µ L(Rn)  the following conclusions

(a) Taking step size s = 4/(9L)  the symplectic Euler scheme of (3.1) satisﬁes

(b) Taking step size s = µ/(100L2)  the explicit Euler scheme of (3.1) satisﬁes

f (xk)  f (x?) 

5Lkx0  x?k2
Lk .
1 + 1
9p µ
80L⌘k
f (xk)  f (x?)  3Lkx0  x?k2⇣1 
13kx0  x?k2
Lk .
41 + 1
4p µ

f (xk)  f (x?) 

µ

.

(3.4)

(3.5)

(3.6)

(c) Taking step size s = 1/L  the implicit Euler scheme of (3.1) satisﬁes

In addition  Theorem 3.1 shows that the implicit scheme also achieves acceleration. However  unlike
NAG-SC  the symplectic scheme  and the explicit scheme  the implicit scheme is generally not easy
to use in practice because it requires solving a nonlinear ﬁxed-point equation when the objective is
not quadratic. On the other hand  the explicit scheme can only take a smaller step size O(µ/L2) 
which prevents this scheme from achieving acceleration.

3.2 The heavy-ball method
We turn to the heavy-ball method ODE (2.4)  whose phase space representation reads

˙X = V 

˙V = 2pµV  (1 + pµs)rf (X) 

shows that the solution X = X(t) to this ODE satisﬁes

with the initial conditions X(0) = x0 and V (0) =  2psrf (x0)
7kx0  x?k2

1+pµs

e

f (X(t))  f (x?) 

2s

pµt
4  

(3.7)

. Theorem 2 in Shi et al. [2018]

for f 2S 1

µ L(Rn) and any step size 0 < s  1/L. In particular  taking s = 1/L gives

f (X(t))  f (x?) 

7Lkx0  x?k2

pµt
4 .

e

2

Returning to the discrete regime  Polyak’s heavy-ball method uses the following update rule:

xk+1  xk = psvk
vk+1  vk = 

8><>:

vk+1 

2pµs
1  pµs

which attains a non-accelerated rate (see Theorem 4 of Shi et al. [2018]):

1 + pµs
1  pµs · psrf (xk+1) 
5Lkx0  x?k2
1 + µ
16Lk
scheme starts with any arbitrary x0 and v0 =  2psrf (x0)

f (xk)  f (x?) 

The three simple Euler schemes for numerically solving the ODE (2.4) are given as follows. Every
. As in the case of NAG-SC  the symplectic

1+pµs

(3.8)

.

scheme is the closest to the heavy-ball method.

6

Euler scheme of (3.7): (S)  (E) and (I) respectively

(S)

(E)

(I)

( xk+1  xk = psvk 
vk+1  vk = 2pµsvk+1  ps(1 + pµs)rf (xk+1).
( xk+1  xk = psvk
vk+1  vk = 2pµsvk  ps(1 + pµs)rf (xk).
( xk+1  xk = psvk+1
vk+1  vk = 2pµsvk+1  ps(1 + pµs)rf (xk+1).

The theorem below characterizes the convergence rates of the three schemes. This theorem is extended
to general step sizes by Theorem B.2 in Appendix B.2.
Theorem 3.2 (Discretization of heavy-ball ODE). For any f 2S 1
hold:

µ L(Rn)  the following conclusions

(a) Taking step size s = µ/(16L2)  the symplectic Euler scheme of (3.7) satisﬁes

(b) Taking step size s = µ/(36L2)  the explicit Euler scheme of (3.7) satisﬁes

(c) Taking step size s = 1/L  the implicit Euler scheme of (3.7) satisﬁes

.

f (xk)  f (x?) 

3Lkx0  x?k2
1 + µ
16Lk
f (xk)  f (x?)  3Lkx0  x?k2⇣1 
15Lkx0  x?k2
Lk .
41 + 1
4p µ

f (xk)  f (x?) 

µ

48L⌘k

.

(3.10)

(3.9)

(3.11)

Taken together  (3.8) and Theorem 3.2 imply that neither the heavy-ball method nor the symplectic
scheme attains an accelerated rate. In contrast  the implicit scheme achieves acceleration as in the
NAG-SC case  but it is impractical except for quadratic objectives.

4 High-Resolution ODEs for Convex Functions

In this section  we turn to numerical discretization of the high-resolution ODE (2.6) related to NAG-C.
All proofs are deferred to Appendix C. This ODE in the phase space representation reads [Shi et al. 
2018] as follows:

˙X = V 

˙V = 

(4.1)
with X(3ps/2) = x0 and V (3ps/2) = psrf (x0). Theorem 5 of Shi et al. [2018] shows that
Let f 2F 1
L(Rn). For any step size 0 < s  1/L  the solution X = X(t) of the high-resolution
ODE (2.6) satisﬁes

3

t · V  psr2f (X)V ✓1 +

3ps

2t ◆rf (X) 

(4 + 3sL)kx0  x?k2

f (X)  f (x?) 
t0utkrf (X(u))k2 

inf

t (2t + ps)
(12 + 9sL)kx0  x?k2

2ps (t3  t3

0)

8>>><>>>:

 

(4.2)

for any t > t0 = 1.5ps. A caveat here is that it is unclear how to use a Lyapunov function to
prove convergence of the (simple) explicit  symplectic or implicit Euler scheme by direct numerical
discretization of the ODE (2.2). See Appendix C.2 for more discussion on this point. Therefore  we
slightly modify the ODE to the following one:

˙X = V 

˙V = 

3

t · V  psr2f (X)V ✓1 +

3ps

t ◆rf (X).

(4.3)

7

The only difference is in the third term on the right-hand side of the second equation  where we replace
⇣1 + 3ps
t ⌘rf (X). Now  we apply the three schemes on this (modiﬁed)
2t ⌘rf (X) by⇣1 + 3ps
ODE in the phase space  including the original NAG-C  which all start with x0 and v0 = psrf (x0).
Euler scheme of (4.3): (S)  (E) and (I) respectively
xk+1  xk = psvk
(S) 8<:
vk+1  ps (rf (xk+1)  rf (xk))  ps✓ k + 4
k + 1◆rf (xk+1).
vk+1  vk = 
xk+1  xk = psvk
(E) 8<:
vk  ps (rf (xk+1)  rf (xk))  ps✓ k + 3
k ◆rf (xk).
vk+1  vk = 
xk+1  xk = psvk+1
(I) 8<:
k + 1◆rf (xk+1).
vk+1  ps (rf (xk+1)  rf (xk))  ps✓ k + 4
vk+1  vk = 
Theorem 4.1. Let f 2F 1
(a) For any step size 0 < s  1/(3L)  the symplectic Euler scheme of (4.3) (original NAG-C) satisﬁes

L (Rn). The following statements are true:

k + 1

3
k

3

3

k + 1

f (xk)  f (x?) 

119kx0  x?k2

s(k + 1)2

  min

0ik krf (xi)k2 

8568kx0  x?k2

s2(k + 1)3

;

(4.4)

(b) Taking any step size 0 < s  1/L  the implicit Euler scheme of (4.3) satisﬁes

f (xk)  f (x?) 

(3sL + 2)kx0  x?k2

s(k + 2)(k + 3)

  min

0ik krf (xi)k2 

(3sL + 2)kx0  x?k2

s2(k + 1)3

.
(4.5)

Note that Theorem 4.1 (a) is the same as Theorem 6 of Shi et al. [2018]. The explicit Euler scheme
does not guarantee convergence; see the analysis in Appendix C.1.

5 Discussion

In this paper  we have analyzed the convergence rates of three numerical discretization schemes—the
symplectic Euler scheme  explicit Euler scheme  and implicit Euler scheme—applied to ODEs that are
used for modeling Nesterov’s accelerated methods and Polyak’s heavy-ball method. The symplectic
scheme is shown to achieve accelerated rates for the high-resolution ODEs of NAG-SC and (slightly
modiﬁed) NAG-C [Shi et al.  2018]  whereas no acceleration rates are observed when the same
scheme is used to discretize the low-resolution counterparts [Su et al.  2016]. For comparison  the
explicit scheme only allows for a small step size in discretizing these ODEs in order to ensure stability 
thereby failing to achieve acceleration. Although the implicit scheme is proved to yield accelerated
methods no matter whether high-resolution or low-resolution ODEs are discretized  this scheme is
generally not practical except for a limited number of cases (for example  quadratic objectives).
We conclude this paper by presenting several directions for future work. This work suggests that
both symplectic schemes and high-resolution ODEs are crucial for numerical discretization to
achieve acceleration. It would be of interest to formalize and prove this assertion. For example 
does any higher-order symplectic scheme maintain acceleration for the high-resolution ODEs of
NAGs? What is the fundamental mechanism of the gradient correction in high-resolution ODE in
stabilizing symplectic discretization? Moreover  since the discretizations are applied to the modiﬁed
high-resolution ODE of NAG-C  it is tempting to perform a comparison study between the two
high-resolution ODEs in terms of discretization properties. Finally  recognizing Nesterov’s method
(NAG-SC) is very similar to  but still different from  the corresponding symplectic scheme  one can
design new algorithms as interpolations of the two methods; it would be interesting to investigate the
convergence properties of these new algorithms.

8

References
Michael Betancourt  Michael I Jordan  and Ashia C Wilson. On symplectic optimization. arXiv

preprint arXiv:1802.03653  2018.

Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in

Machine Learning  8(3-4):231–357  2015.

Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A uniﬁed theory

of ﬁrst-order methods. arXiv preprint arXiv:1712.02485  2017.

Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and

stochastic programming. Mathematical Programming  156(1-2):59–99  2016.

Ernst Hairer  Christian Lubich  and Gerhard Wanner. Geometric numerical integration: structure-
preserving algorithms for ordinary differential equations  volume 31. Springer Science & Business
Media  2006.

Walid Krichene and Peter L Bartlett. Acceleration and averaging in stochastic descent dynamics. In

Advances in Neural Information Processing Systems  pages 6796–6806  2017.

Walid Krichene  Alexandre Bayen  and Peter L Bartlett. Accelerated mirror descent in continuous
and discrete time. In Advances in Neural Information Processing Systems  pages 2845–2853  2015.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2).

Soviet Mathematics Doklady  27(2):372–376  1983.

Yurii Nesterov. How to make the gradients small. Optima  88:10–11  2012.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87. Springer

Science & Business Media  2013.

Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

Boris T Polyak. Introduction to optimization. Optimization Software  Inc  New York  1987.
Bin Shi  Simon S Du  Michael I Jordan  and Weijie J Su. Understanding the acceleration phenomenon

via high-resolution differential equations. arXiv preprint arXiv:1810.08907  2018.

Weijie Su  Stephen Boyd  and Emmanuel J Candès. A differential equation for modeling Nesterov’s
accelerated gradient method: theory and insights. Journal of Machine Learning Research  17(153):
1–43  2016.

Andre Wibisono  Ashia C Wilson  and Michael I Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences  113(47):E7351–E7358 
2016.

Ashia C Wilson  Benjamin Recht  and Michael I Jordan. A Lyapunov analysis of momentum methods

in optimization. arXiv preprint arXiv:1611.02635  2016.

Jingzhao Zhang  Aryan Mokhtari  Suvrit Sra  and Ali Jadbabaie. Direct Runge–Kutta discretization

achieves acceleration. arXiv preprint arXiv:1805.00521  2018.

9

,Bin Shi
Simon Du
Weijie Su
Michael Jordan