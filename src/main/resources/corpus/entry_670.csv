2015,BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions,We propose a simple method to learn linear causal cyclic models in the presence of latent variables. The method relies on equilibrium data of the model recorded under a specific kind of interventions (``shift interventions''). The location and strength of these interventions do not have to be known and can be estimated from the data. Our method  called BACKSHIFT  only uses second moments of the data and performs simple joint matrix diagonalization  applied to differences between covariance matrices. We give a sufficient and necessary condition for identifiability of the system  which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings  one of which can be pure observational data. We demonstrate the performance on some simulated data and applications in flow cytometry and financial time series.,BACKSHIFT: Learning causal cyclic graphs from

unknown shift interventions

Dominik Rothenh¨ausler⇤

Seminar f¨ur Statistik

ETH Z¨urich  Switzerland

rothenhaeusler@stat.math.ethz.ch

Christina Heinze⇤
Seminar f¨ur Statistik

ETH Z¨urich  Switzerland

heinze@stat.math.ethz.ch

Jonas Peters

jonas.peters@tuebingen.mpg.de

Max Planck Institute for Intelligent Systems

T¨ubingen  Germany

Nicolai Meinshausen
Seminar f¨ur Statistik

ETH Z¨urich  Switzerland

meinshausen@stat.math.ethz.ch

Abstract

We propose a simple method to learn linear causal cyclic models in the presence
of latent variables. The method relies on equilibrium data of the model recorded
under a speciﬁc kind of interventions (“shift interventions”). The location and
strength of these interventions do not have to be known and can be estimated from
the data. Our method  called BACKSHIFT  only uses second moments of the data
and performs simple joint matrix diagonalization  applied to differences between
covariance matrices. We give a sufﬁcient and necessary condition for identiﬁ-
ability of the system  which is fulﬁlled almost surely under some quite general
assumptions if and only if there are at least three distinct experimental settings 
one of which can be pure observational data. We demonstrate the performance on
some simulated data and applications in ﬂow cytometry and ﬁnancial time series.

Introduction

1
Discovering causal effects is a fundamentally important yet very challenging task in various disci-
plines  from public health research and sociological studies  economics to many applications in the
life sciences. There has been much progress on learning acyclic graphs in the context of structural
equation models [1]  including methods that learn from observational data alone under a faithful-
ness assumption [2  3  4  5]  exploiting non-Gaussianity of the data [6  7] or non-linearities [8].
Feedbacks are prevalent in most applications  and we are interested in the setting of [9]  where we
observe the equilibrium data of a model that is characterized by a set of linear relations

x = Bx + e 

(1)
where x 2 Rp is a random vector and B 2 Rp⇥p is the connectivity matrix with zeros on the diag-
onal (no self-loops). Allowing for self-loops would lead to an identiﬁability problem  independent
of the method. See Section B in the Appendix for more details on this setting. The graph corre-
sponding to B has p nodes and an edge from node j to node i if and only if Bi j 6= 0. The error
terms e are p-dimensional random variables with mean 0 and positive semi-deﬁnite covariance ma-
trix ⌃e = E(eeT ). We do not assume that ⌃e is a diagonal matrix which allows the existence of
latent variables.
The solutions to (1) can be thought of as the deterministic equilibrium solutions (conditional on the
noise term) of a dynamic model governed by ﬁrst-order difference equations with matrix B in the

⇤Authors contributed equally.

1

sense of [10]. For well-deﬁned equilibrium solutions of (1)  we need that IB is invertible. Usually
we also want (1) to converge to an equilibrium when iterating as x(new) Bx(old) + e or in other
words limm!1 Bm ⌘ 0. This condition is equivalent to the spectral radius of B being strictly
smaller than one [11]. We will make an assumption on cyclic graphs that restricts the strength of the
feedback. Speciﬁcally  let a cycle of length ⌘ be given by (m1  . . .   m⌘+1 = m1) 2{ 1  . . .   p}1+⌘
and mk 6= m` for 1  k <`  ⌘. We deﬁne the cycle-product CP(B) of a matrix B to be the
maximum over cycles of all lengths 1 <⌘  p of the path-products

(2)

CP(B) :=

max

(m1 ... m⌘ m⌘+1) cycle

1<⌘p

Y1k⌘Bmk+1 mk .

The cycle-product CP(B) is clearly zero for acyclic graphs. We will assume the cycle-product to be
strictly smaller than one for identiﬁability results  see Assumption (A) below. The most interesting
graphs are those for which CP(B) < 1 and for which the spectral radius of B is strictly smaller than
one. Note that these two conditions are identical as long as the cycles in the graph do not intersect 
i.e.  there is no node that is part of two cycles (for example if there is at most one cycle in the graph).
If cycles do intersect  we can have models for which either (i) CP(B) < 1 but the spectral radius
is larger than one or (ii) CP(B) > 1 but the spectral radius is strictly smaller than one. Models in
situation (ii) are not stable in the sense that the iterations will not converge under interventions. We
can for example block all but one cycle. If this one single unblocked cycle has a cycle-product larger
than 1 (and there is such a cycle in the graph if CP(B) > 1)  then the solutions of the iteration are
not stable1. Models in situation (i) are not stable either  even in the absence of interventions. We
can still in theory obtain the now instable equilibrium solutions to (1) as (I  B)1e and the theory
below applies to these instable equilibrium solutions. However  such instable equilibrium solutions
are arguably of little practical interest. In summary: all interesting feedback models that are stable
under interventions satisfy both CP(B) < 1 and have a spectral radius strictly smaller than one. We
will just assume CP(B) < 1 for the following results.
It is impossible to learn the structure B of this model from observational data alone without making
further assumptions. The LINGAM approach has been extended in [11] to cyclic models  exploiting
a possible non-Gaussianity of the data. Using both experimental and interventional data  [12  9]
could show identiﬁability of the connectivity matrix B under a learning mechanism that relies on
data under so-called “surgical” or “perfect” interventions. In their framework  a variable becomes
independent of all its parents if it is being intervened on and all incoming contributions are thus ef-
fectively removed under the intervention (also called do-interventions in the classical sense of [13]).
The learning mechanism makes then use of the knowledge where these “surgical” interventions oc-
curred. [14] also allow for “changing” the incoming arrows for variables that are intervened on;
but again  [14] requires the location of the interventions while we do not assume such knowledge.
[15] consider a target variable and allow for arbitrary interventions on all other nodes. They neither
permit hidden variables nor cycles.
Here  we are interested in a setting where we have either no or just very limited knowledge about
the exact location and strength of the interventions  as is often the case for data observed under
different environments (see the example on ﬁnancial time series further below) or for biological data
[16  17]. These interventions have been called “fat-hand” or “uncertain” interventions [18]. While
[18] assume acyclicity and model the structure explicitly in a Bayesian setting  we assume that the
data in environment j are equilibrium observations of the model

xj = Bxj + cj + ej 

(3)
where the random intervention shift cj has a mean and covariance ⌃c j. The location of these
interventions (or simply the intervened variables) are those components of cj that are not zero
with probability one. Given these locations  the interventions simply shift the variables by a value
determined by cj; they are therefore not “surgical” but can be seen as a special case of what is
called an “imperfect”  “parametric” [19] or “dependent” intervention [20] or “mechanism change”
[21]. The matrix B and the error distribution of ej are assumed to be identical in all environments.
In contrast to the covariance matrix for the noise term ej  we do assume that ⌃c j is a diagonal
1The blocking of all but one cycle can be achieved by do-interventions on appropriate variables under the
following condition: for every pair of cycles in the graph  the variables in one cycle cannot be a subset of the
variables in the other cycle. Otherwise the blocking could be achieved by deletion of appropriate edges.

2

matrix  which is equivalent to demanding that interventions at different variables are uncorrelated.
This is a key assumption necessary to identify the model using experimental data. Furthermore  we
will discuss in Section 4.2 how a violation of the model assumption (3) can be detected and used to
estimate the location of the interventions.
In Section 2 we show how to leverage observations under different environments with different inter-
ventional distributions to learn the structure of the connectivity matrix B in model (3). The method
rests on a simple joint matrix diagonalization. We will prove necessary and sufﬁcient conditions for
identiﬁability in Section 3. Numerical results for simulated data and applications in ﬂow cytometry
and ﬁnancial data are shown in Section 4.

2 Method
2.1 Grouping of data
Let J be the set of experimental conditions under which we observe equilibrium data from
model (3). These different experimental conditions can arise in two ways: (a) a controlled ex-
periment was conducted where the external input or the external imperfect interventions have been
deliberately changed from one member of J to the next. An example are the ﬂow cytometry data
[22] discussed in Section 4.2. (b) The data are recorded over time. It is assumed that the exter-
nal input is changing over time but not in an explicitly controlled way. The data are grouped into
consecutive blocks j 2J of observations  see Section 4.3 for an example.
2.2 Notation
Assume we have nj observations in each setting j 2J . Let Xj be the (nj ⇥ p)-matrix of obser-
vations from model (3). For general random variables aj 2 Rp   the population covariance matrix
in setting j 2J is called ⌃a j = Cov(aj)  where the covariance is under the setting j 2J .
Furthermore  the covariance on all settings except setting j 2J is deﬁned as an average over all
environments except for the j-th environment  (|J |1)⌃c j :=Pj02J \{j} ⌃c j0. The population
T ). Let the (p ⇥ p)-dimensional ˆ⌃a j be the empirical
Gram matrix is deﬁned as Ga j = E(ajaj
covariance matrix of the observations Aj 2 Rnj⇥p of variable aj in setting j 2J . More precisely 
let ˜Aj be the column-wise mean-centered version of Aj. Then ˆ⌃a j := (nj  1)1 ˜AT
˜Aj. The
empirical Gram matrix is denoted by ˆGa j := n1

j

j AT

j Aj.

2.3 Assumptions
The main assumptions have been stated already but we give a summary below.
(A) The data are observations of the equilibrium observations of model (3). The matrix I  B is
invertible and the solutions to (3) are thus well deﬁned. The cycle-product (2) CP(B) is strictly
smaller than one. The diagonal entries of B are zero.

(B) The distribution of the noise ej (which includes the inﬂuence of latent variables) and the con-
nectivity matrix B are identical across all settings j 2J . In each setting j 2J   the interven-
tion shift cj and the noise ej are uncorrelated.
(C) Interventions at different variables in the same setting are uncorrelated  that is ⌃c j is an (un-

known) diagonal matrix for all j 2J .

We will discuss a stricter version of (C) in Section D in the Appendix that allows the use of Gram
matrices instead of covariance matrices. The conditions above imply that the environments are
characterized by different interventions strength  as measured by the variance of the shift c in each
setting. We aim to reconstruct both the connectivity matrix B from observations in different en-
vironments and also aim to reconstruct the a-priori unknown intervention strength and location in
each environment. Additionally  we will show examples where we can detect violations of the model
assumptions and use these to reconstruct the location of interventions.

2.4 Population method
The main idea is very simple. Looking at the model (3)  we can rewrite

(I  B)xj = cj + ej.

3

(4)

The population covariance of the transformed observations are then for all settings j 2J given by
(5)
The last term ⌃e is constant across all settings j 2J (but not necessarily diagonal as we allow
hidden variables). Any change of the matrix on the left-hand side thus stems from a shift in the
covariance matrix ⌃c j of the interventions. Let us deﬁne the difference between the covariance
of c and x in setting j as

(I  B)⌃x j(I  B)T = ⌃c j + ⌃e.

⌃c j := ⌃c j  ⌃c j 
Assumption (B) together with (5) implies that

and ⌃x j := ⌃x j  ⌃x j.

(6)

(I  B)⌃x j(I  B)T = ⌃c j

8j 2J .

(7)
Using assumption (C)  the random intervention shifts at different variables are uncorrelated and the
right-hand side in (7) is thus a diagonal matrix for all j 2J . Let D⇢ Rp⇥p be the set of all
invertible matrices. We also deﬁne a more restricted space Dcp which only includes those members
of D that have entries all equal to one on the diagonal and have a cycle-product less than one 
(8)

D :=nD 2 Rp⇥p : D invertibleo
Dcp :=nD 2 Rp⇥p : D 2D and diag(D) ⌘ 1 and CP(I  D) < 1o.
Under Assumption (A)  I  B 2D cp. Motivated by (7)  we now consider the minimizer
D = argminD02DcpXj2J
L(D0⌃x jD0T )  where L(A) :=Xk6=l

is the loss L for any matrix A and deﬁned as the sum of the squared off-diagonal elements. In
Section 3  we present necessary and sufﬁcient conditions on the interventions under which D =
I  B is the unique minimizer of (10). In this case  exact joint diagonalization is possible so that
L(D⌃x jDT ) = 0 for all environments j 2J . We discuss an alternative that replaces covariance
with Gram matrices throughout in Section D in the Appendix. We now give a ﬁnite-sample version.

(9)

(10)

A2
k l

2.5 Finite-sample estimate of the connectivity matrix
In practice  we estimate B by minimizing
the empirical counterpart of (10) in two
steps. First  the solution of the optimiza-
tion is only constrained to matrices in D.
Subsequently  we enforce the constraint on
the solution to be a member of Dcp. The
BACKSHIFT algorithm is presented in Al-
gorithm 1 and we describe the important
steps in more detail below.

Algorithm 1 BACKSHIFT
Input: Xj 8j 2J
1: Compute d⌃x j 8j 2J
2: ˜D = FFDIAG(d⌃x j)
3: ˆD = PermuteAndScale( ˜D)
4: ˆB = I  ˆD
Output: ˆB

Steps 1 & 2. First  we minimize the following empirical  less constrained variant of (10)

where the population differences between covariance matrices are replaced with their empirical
counterparts and the only constraint on the solution is that it is invertible  i.e. ˜D 2D . For the
optimization we use the joint approximate matrix diagonalization algorithm FFDIAG [23].

Step 3. The constraint on the cycle product and the diagonal elements of D is enforced by (a)
permuting and (b) scaling the rows of ˜D. Part (b) simply scales the rows so that the diagonal
elements of the resulting matrix ˆD are all equal to one. The more challenging ﬁrst step (a) consists
of ﬁnding a permutation such that under this permutation the scaled matrix from part (b) will have
a cycle product as small as possible (as follows from Theorem 3  at most one permutation can lead
to a cycle product less than one). This optimization problem seems computationally challenging at
ﬁrst  but we show that it can be solved by a variant of the linear assignment problem (LAP) (see e.g.
[24])  as proven in Theorem 3 in the Appendix. As a last step  we check whether the cycle product
of ˆD is less than one  in which case we have found the solution. Otherwise  no solution satisfying
the model assumptions exists and we return a warning that the model assumptions are not met. See
Appendix B for more details.

4

˜D := argminD02DXj2J

L(D0(d⌃x j)D0T ) 

(11)

Computational cost. The computational complexity of BACKSHIFT is O(|J |·n·p2) as computing
the covariance matrices costs O(|J |·n·p2)  FFDIAG has a computational cost of O(|J |·p2) and both
the linear assignment problem and computing the cycle product can be solved in O(p3) time. For
instance  this complexity is achieved when using the Hungarian algorithm for the linear assignment
problem (see e.g. [24]) and the cycle product can be computed with a simple dynamic programming
approach.

2.6 Estimating the intervention variances
One additional beneﬁt of BACKSHIFT is that the location and strength of the interventions can be
estimated from the data. The empirical  plug-in version of Eq. (7) is given by

(I  ˆB)d⌃x j(I  ˆB)T = d⌃c j = b⌃c j b⌃c j

So the element (d⌃c j)kk is an estimate for the difference between the variance of the interven-

tion at variable k in environment j  namely (⌃c j)kk  and the average in all other environments 
(⌃c j)kk. From these differences we can compute the intervention variance for all environments
up to an offset. By convention  we set the minimal intervention variance across all environments
equal to zero. Alternatively  one can let observational data  if available  serve as a baseline against
which the intervention variances are measured.

8j 2J .

(12)

Identiﬁability

3
Let for simplicity of notation 

⌘j k := (⌃c j)kk

be the variance of the random intervention shifts cj at node k in environment j 2J as per the
deﬁnition of ⌃c j in (6). We then have the following identiﬁability result (the proof is provided
in Appendix A).
Theorem 1. Under assumptions (A)  (B) and (C)  the solution to (10) is unique if and only if for all
k  l 2{ 1  . . .   p} there exist j  j0 2J such that

⌘j k⌘j0 l 6= ⌘j l⌘j0 k .

(13)

If none of the intervention variances ⌘j k vanishes  the uniqueness condition is equivalent to de-
manding that the ratio between the intervention variances for two variables k  l must not stay iden-
tical across all environments  that is there exist j  j0 2J such that

⌘j k
⌘j l

⌘j0 k
⌘j0 l

6=

 

(14)

which requires that the ratio of the variance of the intervention shifts at two nodes k  l is not identical
across all settings. This leads to the following corollary.
Corollary 2.

(i) The identiﬁability condition (13) cannot be satisﬁed if |J | = 2 since then ⌘j k =

⌘j0 k for all k and j 6= j0. We need at least three different environments for identiﬁability.

(ii) The identiﬁability condition (13) is satisﬁed for all |J |  3 almost surely if the variances of the
intervention cj are chosen independently (over all variables and environments j 2J ) from a
distribution that is absolutely continuous with respect to Lebesgue measure.

Condition (ii) can be relaxed but shows that we can already achieve full identiﬁability with a very
generic setting for three (or more) different environments.

4 Numerical results
In this section  we present empirical results for both synthetic and real data sets. In addition to
estimating the connectivity matrix B  we demonstrate various ways to estimate properties of the
interventions. Besides computing the point estimate for BACKSHIFT  we use stability selection [25]
to assess the stability of retrieved edges. We attach R-code with which all simulations and analyses
can be reproduced2.

2An R-package called “backShift” is available from CRAN.

5

6

5
−0.73

0.54
7
2.1

0.46
2

9
0.52

10
1

4

3

0.76
0.72
0.67
−0.65

0.34
−0.69

8

1

(a)

E3

I3

3

X3

3

W

1

2

X1

X2

(b)

E1

1

I1

E2

2

I2

1.00

0.75

0.50

I

I

N
O
S
C
E
R
P

0.25

0.00

0.00

0.25

0.75

0.50

RECALL
(c)

Method
● BACKSHIFT
LING

0
0.5
1

Interv. strength
●
●
●
Sample size

1000
10000

●
●
Hidden vars.
● FALSE
TRUE

●●

●

1.00

Figure 1: Simulated data. (a) True network. (b) Scheme for data generation. (c) Performance metrics for the
settings considered in Section 4.1. For BACKSHIFT  precision and recall values for Settings 1 and 2 coincide.

Setting 1
n = 1000
no hidden vars.
mI = 1

Setting 2
n = 10000
no hidden vars.
mI = 1

T
F
I

H
S
K
C
A
B

6

5

7

4

8

3

1

2

9

10

6

5

7

4

8

3

1

2

9

10

6

Setting 3
n = 10000
hidden vars.
mI = 1

5

7

4

8

3

1

2

9

10

6

5

7

4

8

3

1

SHD = 0  |t| = 0.25

SHD = 0  |t| = 0.25

SHD = 2  |t| = 0.25

SHD = 12

G
N

I

L

6

5

7

4

8

3

1

2

9

10

6

5

7

4

8

3

1

2

9

10

6

5

7

4

8

3

1

2

9

10

6

5

7

4

8

3

1

Setting 4
n = 10000
no hidden vars.
mI = 0

Setting 5
n = 10000
no hidden vars.
mI = 0.5
3

4

2

9

2

9

10

6

5

7

2

9

10

8

1

SHD = 5  |t| = 0.25

5

7

4

8

3

1

2

9

10

10

6

SHD = 17  |t| = 0.91 SHD = 14  |t| = 0.68 SHD = 16  |t| = 0.98 SHD = 8  |t| = 0.25 SHD = 7  |t| = 0.29

Figure 2: Point estimates of BACKSHIFT and LING for synthetic data. We threshold the point estimate of
BACKSHIFT at t = ±0.25 to exclude those entries which are close to zero. We then threshold the estimate of
LING so that the two estimates have the same number of edges. In Setting 4  we threshold LING at t = ±0.25
as BACKSHIFT returns the empty graph. In Setting 3  it is not possible to achieve the same number of edges as
all remaining coefﬁcients in the point estimate of LING are equal to one in absolute value. The transparency
of the edges illustrates the relative magnitude of the estimated coefﬁcients. We report the structural Hamming
distance (SHD) for each graph. Precision and recall values are shown in Figure 1(c).
4.1 Synthetic data

kI j

1  . . .   I j

k  where j

We compare the point estimate of BACKSHIFT against LING [11]  a generalization of LINGAM to
the cyclic case for purely observational data. We consider the cyclic graph shown in Figure 1(a) and
generate data under different scenarios. The data generating mechanism is sketched in Figure 1(b).
Speciﬁcally  we generate ten distinct environments with non-Gaussian noise. In each environment 
the random intervention variable is generated as (cj)k = j
p are drawn i.i.d.
from Exp(mI) and I j
p are independent standard normal random variables. The intervention
shift thus acts on all observed random variables. The parameter mI regulates the strength of the
intervention. If hidden variables exist  the noise term (ej)k of variable k in environment j is equal to
kW j  where the weights 1  . . .   p are sampled once from a N (0  1)-distribution and the random
variable W j has a Laplace(0  1) distribution. If no hidden variables are present  then (ej)k  k =
1  . . .   p is sampled i.i.d. Laplace(0  1). In this set of experiments  we consider ﬁve different settings
(described below) in which the sample size n  the intervention strength mI as well as the existence
of hidden variables varies.
We allow for hidden variables in only one out of ﬁve settings as LING assumes causal sufﬁciency
and can thus in theory not cope with hidden variables. If no hidden variables are present  the pooled
data can be interpreted as coming from a model whose error variables follow a mixture distribution.
But if one of the error variables comes from the second mixture component  for example  the other

1  . . .   j

6

Erk

Akt

PIP3

PKA

PIP2

PLCg

p38

PKC

(a)

Mek

Raf

JNK

Erk

Akt

PIP3

PKA

PIP2

PLCg

p38

PKC

(b)

Mek

Raf

JNK

Erk

Akt

PIP3

PKA

Mek

Raf

JNK

Erk

Akt

PIP3

PKA

PIP2

PLCg

p38

PKC

(c)

Mek

Raf

JNK

PIP2

PLCg

p38

PKC

(d)

Figure 3: Flow cytometry data. (a) Union of the consensus network (according to [22])  the reconstruction by
[22] and the best acyclic reconstruction by [26]. The edge thickness and intensity reﬂect in how many of these
three sources that particular edge is present. (b) One of the cyclic reconstructions by [26]. The edge thickness
and intensity reﬂect the probability of selecting that particular edge in the stability selection procedure. For
more details see [26]. (c) BACKSHIFT point estimate  thresholded at ±0.35. The edge intensity reﬂects the
relative magnitude of the coefﬁcients and the coloring is a comparison to the union of the graphs shown in
panels (a) and (b). Blue edges were also found in [26] and [22]  purple edges are reversed and green edges were
not previously found in (a) or (b). (d) BACKSHIFT stability selection result with parameters E(V ) = 2 and
⇡thr = 0.75. The edge thickness illustrates how often an edge was selected in the stability selection procedure.

error variables come from the second mixture component  too. In this sense  the data points are not
independent anymore. This poses a challenge for LING which assumes an i.i.d. sample. We also
cover a case (for mI = 0) in which all assumptions of LING are satisﬁed (Scenario 4).
Figure 2 shows the estimated connectivity matrices for ﬁve different settings and Figure 1(c) shows
the obtained precision and recall values. In Setting 1  n = 1000  mI = 1 and there are no hidden
variables. In Setting 2  n is increased to 10000 while the other parameters do not change. We observe
that BACKSHIFT retrieves the correct adjacency matrix in both cases while LING’s estimate is not
very accurate. It improves slightly when increasing the sample size. In Setting 3  we do include
hidden variables which violates the causal sufﬁciency assumption required for LING. Indeed  the
estimate is worse than in Setting 2 but somewhat better than in Setting 1. BACKSHIFT retrieves
two false positives in this case. Setting 4 is not feasible for BACKSHIFT as the distribution of the
variables is identical in all environments (since mI = 0). In Step 2 of the algorithm  FFDIAG does
not converge and therefore the empty graph is returned. So the recall value is zero while precision
is not deﬁned. For LING all assumptions are satisﬁed and the estimate is more accurate than in
the Settings 1–3. Lastly  Setting 5 shows that when increasing the intervention strength to 0.5 
BACKSHIFT returns a few false positives. Its performance is then similar to LING which returns its
most accurate estimate in this scenario. The stability selection results for BACKSHIFT are provided
in Figure 5 in Appendix E.
In short  these results suggest that the BACKSHIFT point estimates are close to the true graph if the
interventions are sufﬁciently strong. Hidden variables make the estimation problem more difﬁcult
but the true graph is recovered if the strength of the intervention is increased (when increasing mI
to 1.5 in Setting 3  BACKSHIFT obtains a SHD of zero). In contrast  LING is unable to cope with
hidden variables but also has worse accuracy in the absence of hidden variables under these shift
interventions.

4.2 Flow cytometry data
The data published in [22] is an instance of a data set where the external interventions differ be-
tween the environments in J and might act on several compounds simultaneously [18]. There are
nine different experimental conditions with each containing roughly 800 observations which corre-
spond to measurements of the concentration of biochemical agents in single cells. The ﬁrst setting
corresponds to purely observational data.
In addition to the original work by [22]  the data set has been described and analyzed in [18] and
[26]. We compare against the results of [26]  [22] and the “well-established consensus”  according
to [22]  shown in Figures 3(a) and 3(b). Figure 3(c) shows the (thresholded) BACKSHIFT point
estimate. Most of the retrieved edges were also found in at least one of the previous studies. Five
edges are reversed in our estimate and three edges were not discovered previously. Figure 3(d) shows
the corresponding stability selection result with the expected number of falsely selected variables

7

)

I

E
C
R
P
G
O
L

(

0

.

9

5

.

8

0

.

8

5

.

7

0

.

7

5

.

6

DAX

NASDAQ

S&P 500

S
N
R
U
T
E
R
−
G
O
L

DAX

S&P 500

NASDAQ

 

I

E
C
N
A
R
A
V
N
O
T
N
E
V
R
E
T
N

I

I
 
.

T
S
E

DAX

S&P 500

NASDAQ

I

 

E
C
N
A
R
A
V
N
O
T
N
E
V
R
E
T
N

I

I
 
.

T
S
E

DAX

S&P 500

NASDAQ

2001

2003

2005

2007

2009

2011

2001

2003

2005

2007

2009

2011

2001

2003

2005

2007

2009

2011

2001

2003

2005

2007

2009

2011

TIME

TIME

TIME

TIME

(c) BACKSHIFT

(b) Daily log-returns

(a) Prices (logarithmic)
Figure 4: Financial time series with three stock indices: NASDAQ (blue; technology index)  S&P 500 (green;
American equities) and DAX (red; German equities). (a) Prices of the three indices between May 2000 and end
of 2011 on a logarithmic scale. (b) The scaled log-returns (daily change in log-price) of the three instruments
are shown. Three periods of increased volatility are visible starting with the dot-com bust on the left to the
ﬁnancial crisis in 2008 and the August 2011 downturn. (c) The scaled estimated intervention variance with
the estimated BACKSHIFT network. The three down-turns are clearly separated as originating in technology 
American and European equities. (d) In contrast  the analogous LING estimated intervention variances have a
peak in American equities intervention variance during the European debt crisis in 2011.

(d) LING

E(V ) = 2. This estimate is sparser in comparison to the other ones as it bounds the number of false
discoveries. Notably  the feedback loops between PIP2 $ PLCg and PKC $ JNK were also found
in [26].
It is also noteworthy that we can check the model assumptions of shift interventions  which is impor-
tant for these data as they can be thought of as changing the mechanism or activity of a biochemical
agent rather than regulate the biomarker directly [26]. If the shift interventions are not appropri-
ate  we are in general not able to diagonalize the differences in the covariance matrices. Large
off-diagonal elements in the estimate of the r.h.s in (7) indicate a mechanism change that is not just
explained by a shift intervention as in (1). In four of the seven interventions environments with
known intervention targets the largest mechanism violation happens directly at the presumed inter-
vention target  see Appendix C for details. It is worth noting again that the presumed intervention
target had not been used in reconstructing the network and mechanism violations.

4.3 Financial time series

Finally  we present an application in ﬁnancial time series where the environment is clearly changing
over time. We consider daily data from three stock indices NASDAQ  S&P 500 and DAX for a
period between 2000-2012 and group the data into 74 overlapping blocks of 61 consecutive days
each. We take log-returns  as shown in panel (b) of Figure 4 and estimate the connectivity matrix 
which is fully connected in this case and perhaps of not so much interest in itself. It allows us 
however  to estimate the intervention strength at each of the indices according to (12)  shown in
panel (c). The intervention variances separate very well the origins of the three major down-turns of
the markets on the period. Technology is correctly estimated by BACKSHIFT to be at the epicenter
of the dot-com crash in 2001 (NASDAQ as proxy)  American equities during the ﬁnancial crisis in
2008 (proxy is S&P 500) and European instruments (DAX as best proxy) during the August 2011
downturn.

5 Conclusion
We have shown that cyclic causal networks can be estimated if we obtain covariance matrices of
the variables under unknown shift interventions in different environments. BACKSHIFT leverages
solutions to the linear assignment problem and joint matrix diagonalization and the part of the com-
putational cost that depends on the number of variables is at worst cubic. We have shown sufﬁcient
and necessary conditions under which the network is fully identiﬁable  which require observations
from at least three different environments. The strength and location of interventions can also be
reconstructed.

References
[1] K.A. Bollen. Structural Equations with Latent Variables. John Wiley & Sons  New York  USA  1989.

8

[2] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. MIT Press  Cambridge 

USA  2nd edition  2000.

[3] D.M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning

Research  3:507–554  2002.

[4] M.H. Maathuis  M. Kalisch  and P. B¨uhlmann. Estimating high-dimensional intervention effects from

observational data. Annals of Statistics  37:3133–3164  2009.

[5] A. Hauser and P. B¨uhlmann. Characterization and greedy learning of interventional Markov equivalence

classes of directed acyclic graphs. Journal of Machine Learning Research  13:2409–2464  2012.

[6] P.O. Hoyer  D. Janzing  J.M. Mooij  J. Peters  and B. Sch¨olkopf. Nonlinear causal discovery with additive
noise models. In Advances in Neural Information Processing Systems 21 (NIPS)  pages 689–696  2009.
[7] S. Shimizu  T. Inazumi  Y. Sogawa  A. Hyv¨arinen  Y. Kawahara  T. Washio  P.O. Hoyer  and K. Bollen.
DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model. Journal
of Machine Learning Research  12:1225–1248  2011.

[8] J.M. Mooij  D. Janzing  T. Heskes  and B. Sch¨olkopf. On causal discovery with cyclic additive noise

models. In Advances in Neural Information Processing Systems 24 (NIPS)  pages 639–647  2011.

[9] A. Hyttinen  F. Eberhardt  and P. O. Hoyer. Learning linear cyclic causal models with latent variables.

Journal of Machine Learning Research  13:3387–3439  2012.

[10] S.L. Lauritzen and T.S. Richardson. Chain graph models and their causal interpretations. Journal of the

Royal Statistical Society  Series B  64:321–348  2002.

[11] G. Lacerda  P. Spirtes  J. Ramsey  and P.O. Hoyer. Discovering cyclic causal models by independent
In Proceedings of the 24th Conference on Uncertainty in Artiﬁcial Intelligence

components analysis.
(UAI)  pages 366–374  2008.

[12] R. Scheines  F. Eberhardt  and P.O. Hoyer. Combining experiments to discover linear cyclic models with
latent variables. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
185–192  2010.

[13] J. Pearl. Causality: Models  Reasoning  and Inference. Cambridge University Press  New York  USA 

2nd edition  2009.

[14] F. Eberhardt  P. O. Hoyer  and R. Scheines. Combining experiments to discover linear cyclic models with
latent variables. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages
185–192  2010.

[15] J. Peters  P. B¨uhlmann  and N. Meinshausen. Causal inference using invariant prediction: identiﬁcation

and conﬁdence intervals. Journal of the Royal Statistical Society  Series B  to appear.  2015.

[16] A.L. Jackson  S.R. Bartz  J. Schelter  S.V. Kobayashi  J. Burchard  M. Mao  B. Li  G. Cavet  and P.S.
Linsley. Expression proﬁling reveals off-target gene regulation by RNAi. Nature Biotechnology  21:635–
637  2003.

[17] M.M. Kulkarni  M. Booker  S.J. Silver  A. Friedman  P. Hong  N. Perrimon  and B. Mathey-Prevot. Ev-
idence of off-target effects associated with long dsrnas in drosophila melanogaster cell-based assays.
Nature methods  3:833–838  2006.

[18] D. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 107–114  2007.

[19] F. Eberhardt and R. Scheines. Interventions and causal inference. Philosophy of Science  74:981–995 

2007.

[20] K. Korb  L. Hope  A. Nicholson  and K. Axnick. Varieties of causal intervention. In Proceedings of the

Paciﬁc Rim Conference on AI  pages 322–331  2004.

[21] J. Tian and J. Pearl. Causal discovery from changes.

In Proceedings of the 17th Conference Annual

Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 512–522  2001.

[22] K. Sachs  O. Perez  D. Pe’er  D. Lauffenburger  and G. Nolan. Causal protein-signaling networks derived

from multiparameter single-cell data. Science  308:523–529  2005.

[23] A. Ziehe  P. Laskov  G. Nolte  and K.-R. M¨uller. A fast algorithm for joint diagonalization with non-
orthogonal transformations and its application to blind source separation. Journal of Machine Learning
Research  5:801–818  2004.

[24] R.E. Burkard. Quadratic assignment problems. In P. M. Pardalos  D.-Z. Du  and R. L. Graham  editors 

Handbook of Combinatorial Optimization  pages 2741–2814. Springer New York  2nd edition  2013.

[25] N. Meinshausen and P. B¨uhlmann. Stability selection. Journal of the Royal Statistical Society  Series B 

72:417–473  2010.

[26] J.M. Mooij and T. Heskes. Cyclic causal discovery from continuous equilibrium data. In Proceedings of

the 29th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 431–439  2013.

9

,Dominik Rothenhäusler
Christina Heinze
Jonas Peters
Nicolai Meinshausen
Mengdi Wang
Ji Liu
Ethan Fang
Shikun Liu
Andrew Davison
Edward Johns