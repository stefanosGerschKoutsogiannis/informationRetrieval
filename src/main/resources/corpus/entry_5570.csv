2019,Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels,We revisit the classic randomized sketch of a tensor product of $q$ vectors $x_i\in\mathbb{R}^n$. The $i$-th coordinate $(Sx)_i$ of the sketch is equal to
$\prod_{j = 1}^q \langle u^{i  j}  x^j \rangle / \sqrt{m}$  where $u^{i j}$ are independent random sign vectors. Kar and Karnick (JMLR  2012) show that
if the sketching dimension $m = \Omega(\epsilon^{-2} C_{\Omega}^2 \log (1/\delta))$  where $C_{\Omega}$ is a certain property of the point set $\Omega$ one wants to sketch  then with probability $1-\delta$  $\|Sx\|_2 = (1\pm \epsilon)\|x\|_2$ for all $x\in\Omega$. However  in their analysis $C_{\Omega}^2$ can be as large as $\Theta(n^{2q})$  even for a set $\Omega$ of $O(1)$ vectors $x$.

We give a new analysis of this sketch  providing nearly optimal bounds.
Namely  we show an upper bound of
$m = \Theta \left (\epsilon^{-2} \log(n/\delta) + \epsilon^{-1} \log^q(n/\delta) \right ) $
which by composing with CountSketch  can be improved to
$\Theta(\epsilon^{-2}\log(1/(\delta \epsilon)) + \epsilon^{-1} \log^q (1/(\delta \epsilon))$. For the important case of $q = 2$ and $\delta = 1/\poly(n)$  this shows that $m = \Theta(\epsilon^{-2} \log(n) + \epsilon^{-1} \log^2(n))$ 
demonstrating that the $\epsilon^{-2}$ and $\log^2(n)$ terms do not multiply each other. We also show a nearly matching lower bound of
$m = \Omega(\eps^{-2} \log(1/(\delta)) + \eps^{-1} \log^q(1/(\delta)))$.
In a number of applications  one has $|\Omega| = \poly(n)$ and in this case our bounds are optimal up to a constant factor. This is the first high probability sketch for tensor products that has optimal sketch size and can be implemented in $m \cdot \sum_{i=1}^q \textrm{nnz}(x_i)$ time  where $\textrm{nnz}(x_i)$ is the
number of non-zero entries of $x_i$.

Lastly  we empirically compare our sketch to other sketches for tensor products  and give a novel application to compressing neural networks.,Tight Dimensionality Reduction for Sketching Low

Degree Polynomial Kernels

Michela Meister∗
Cornell University
Ithaca  NY 14850

Tamas Sarlos
Google Research

Mountain View  CA 94043

meister.michela@gmail.com

stamas@google.com

David P. Woodruff†

Department of Computer Science

Carnegie Mellon University

Pittsburgh  PA 15213

dwoodruf@cs.cmu.edu

Abstract

√
j=1(cid:104)ui j  xj(cid:105)/

The i-th coordinate (Sx)i of the sketch is equal to(cid:81)q

we show an upper bound of m = Θ(cid:0)−2 log(n/δ) + −1 logq(n/δ)(cid:1)   which

We revisit the classic randomized sketch of a tensor product of q vectors xi ∈ Rn.
m  where
ui j are independent random sign vectors. Kar and Karnick (JMLR  2012) show
that if the sketching dimension m = Ω(−2C 2
Ω log(1/δ))  where CΩ is a certain
property of the point set Ω one wants to sketch  then with probability 1 − δ 
(cid:107)Sx(cid:107)2 = (1 ± )(cid:107)x(cid:107)2 for all x ∈ Ω. However  in their analysis C 2
Ω can be as large
as Θ(n2q)  even for a set Ω of O(1) vectors x.
We give a new analysis of this sketch  providing nearly optimal bounds. Namely 
by composing with CountSketch  can be improved to Θ(−2 log(1/(δ)) +
−1 logq(1/(δ)). For the important case of q = 2 and δ = 1/poly(n)  this
shows that m = Θ(−2 log(n) + −1 log2(n))  demonstrating that the −2 and
log2(n) terms do not multiply each other. We also show a nearly matching lower
bound of m = Ω(ε−2 log(1/(δ)) + ε−1 logq(1/(δ))). In a number of applications 
one has |Ω| = poly(n) and in this case our bounds are optimal up to a constant
factor. This is the ﬁrst high probability sketch for tensor products that has optimal
i=1 nnz(xi) time  where nnz(xi) is

sketch size and can be implemented in m ·(cid:80)q

the number of non-zero entries of xi.
Lastly  we empirically compare our sketch to other sketches for tensor products 
and give a novel application to compressing neural networks.

1

Introduction

Dimensionality reduction  or sketching  is a way of embedding high-dimensional data into a low-
dimensional space  while approximately preserving distances between data points. The embedded
data is often easier to store and manipulate  and typically results in much faster algorithms. Therefore 
it is often beneﬁcial to sketch a dataset ﬁrst and then run machine learning algorithms on the sketched
data. This technique has been applied to numerical linear algebra problems [37]  classiﬁcation [9  10] 

∗Work done at Google Research.
†Work done at Google Research  and while visiting the Simons Institute for the Theory of Computing.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

data stream algorithms [33]  nearest neighbor search [22]  sparse recovery [12  20]  and numerous
other problems.
While effective  in many modern machine learning problems the points one would like to embed
are often only speciﬁed implicitly. Kernel machines  such as support vector machines  are one
example  for which one ﬁrst non-linearly transforms the input points before running an algorithm.
Such machines are much more powerful than their linear counterparts  as they can approximate
any function or decision boundary arbitrary well with enough training data. In kernel applications
there is a feature map φ : Rn → Rn(cid:48)
which maps inputs in Rn to a typically much higher n(cid:48)-
dimensional space  with the important property that for x  y ∈ Rn  one can typically quickly compute
(cid:104)φ(x)  φ(y)(cid:105) given only (cid:104)x  y(cid:105). As many applications only depend on the geometry of the input
points  or equivalently inner product information  this allows one to work in the potentially much
higher and richer n(cid:48)-dimensional space while running in time proportional to that of the smaller
n-dimensional space. Here often one would like to sketch the n(cid:48)-dimensional points φ(x)  without
explicitly computing φ(x) and then applying the sketch  as this would be too slow.
A speciﬁc example is the polynomial kernel of degree q  for which n(cid:48) = nq and φ(x)i1 i2 ... iq =
xi1 · xi2 ··· xiq. The polynomial kernel is also often used for approximating more general functions
via Taylor expansion [17  30]. Note that the polynomial kernel φ(x) can be written as a special type
of tensor product  φ(x) = x ⊗ x ⊗ ··· ⊗ x  where φ(x) is the tensor product of x with itself q times.
In this work we explore the more general problem of sketching a tensor product of arbitrary vectors
x1  . . .   xq ∈ Rn with the goal of embedding polynomial kernels. We will focus on the typical case
when q is an absolute constant independent of n. In this problem we would like to quickly compute
S · x  where x = x1 ⊗ x2 ⊗ ··· ⊗ xq  where S is a sketching matrix with a small number m of rows 
which corresponds to the embedding dimension.
The most naïve solution would be to explicitly compute x and then apply an off-the-shelf Johnson
Lindenstrauss transform S [25  18  28  16]  which using the best known bounds gives an embedding
dimension of m = Θ(−2 log(1/δ))  which is optimal [24  27  31]. However  the running time
(cid:80)q
is prohibitive  since it is at least the number nnz(x) of non-zeros of x  which can be as large as
nq. A much more practical alternative is TENSORSKETCH [34  35] which gives a running time of
i=1 nnz(xi)  which is optimal  but the embedding dimension is a prohibitive Θ(−2/δ). Note that
for high probability applications  where one may want to set δ = 1/poly(n)  this gives an embedding
dimension as large as poly(n)  which since x has length nq = poly(n)  may defeat the purpose of
dimensionality reduction.
Thus  we are at a crossroads; on the one hand we have a sketch with the optimal embedding dimension
with a prohibitive running time  and on the other hand we have a sketch with the optimal running
time but with a prohibitive embedding dimension. A natural question is if there is another sketch
which achieves both a small embedding dimension and enjoys a fast running time.

1.1 Our Contributions

1.1.1 Near-Optimal Analysis of Tensorized Random Projection Sketch

m ·(cid:81)q

Our ﬁrst contribution shows that a previously analyzed sketch by Kar and Karnick for tensor products
[30]  referred here to as a Tensorized Random Projection  has exponentially better embedding
dimension than previously known. Given vectors x1  . . .   xq ∈ Rn in this sketch one computes the
sketch S · x of the tensor product x = x1 ⊗ x2 ⊗ ··· ⊗ xq where the i-th coordinate (Sx)i of the
j=1(cid:104)ui j  xj(cid:105). Here the ui j ∈ {−1  1}n are independent random sign
sketch is equal to 1√
vectors  and q is typically a constant. The previous analysis of this sketch in [35] describes the sketch
as having large variance and requires a sketching dimension that grows as n2q  as detailed in the
supplementary  in Appendix D.
We give a much improved analysis of this sketch in 2.1  showing that for any x  y ∈ Rnq and
] ≤ δ. Notably our dimension bound grows as logq(n) rather than n2q  providing an exponential
improvement over previous analyses of this sketch. Another interesting aspect of our bound is
that the second term only depends linearly on −1  rather than quadratically. This can represent
a substantial savings for small   e.g.  if  = .001. Thus  for example  if  ≤ 1/ logq−1(n)  our

δ < 1/nq  there is an m = Θ(cid:0)−2 log(n/δ) + −1 logq(n/δ)(cid:1) for which Pr[|(cid:104)Sx  Sy(cid:105) − (cid:104)x  y(cid:105)| >

2

with CountSketch) in time O((cid:80)q

sketch size is Θ(−2 log(n)) which is optimal for any possibly adaptive and possibly non-linear
sketch  in light of lower bounds for arbitrary Johnson-Lindenstrauss transforms [31]. Thus  at least
for this natural setting of parameters  this sketch does not incur very large variance  contrary to the
beliefs stated above. Moreover  q = 2 is one of the most common settings for the polynomial kernel
in natural language processing [2]  since larger degrees tend to overﬁt. In this case  our bound is
m = Θ(−2 log(n) + −1 log2(n))  and the separation of the −2 and log2(n) terms in our sketching
dimension is especially signiﬁcant.
We next show in 2.2 that a simple composition of the Tensorized Random Projection with a CountS-
ketch [14] slightly improves the embedding dimension to m = Θ(−2 log(1/(δ))+−1 logq(1/(δ))
and works for all δ < 1. Moreover  we can compute the entire sketch (including the composition
i=1 m · nnz(xi)). This makes our sketch a "best of both worlds"
in comparison to the Johnson-Lindenstrauss transform and TensorSketch: Tensorized Random
Projection runs much faster than the Johnson-Lindenstrauss transform and it enjoys a smaller
embedding dimension than TensorSketch. Additionally  we are able to show a nearly matching
m = Ω(ε−2 log(1/δ) + ε−1 logq(1/δ)) lower bound for this sketch  by exhibiting an input x for
which (cid:107)Sx(cid:107)2 /∈ (1 ± )(cid:107)x(cid:107)2 with probability more than δ.
It is also worthwhile to contrast our results with earlier work in the data streaming community
[23  11] that analyzed the variance only for q = 2 and general q respectively  and then achieved high
probability bounds by taking the median of multiple independent copies of S. The non-linear median
operation makes the former constructs unsuitable for machine learning applications. In contrast 
we show high probability bounds for the linear embedding S directly. Recent work [4]  which
was a merger of [5  29]  provide different sketches with different trade-offs. Their main focus is a
sketching dimension with a (polynomial) dependence on q  making it more suitable for approximating
high-degree polynomial kernels. Our focus is instead on improving the analysis of an existing sketch 
which is most useful for small values of q.
From a technical standpoint  our work builds off the recent proof of the Johnson-Lindenstrauss trans-
form in [16]. We write the sketch S as σT Aσ  where in our setting σ corresponds to the concatenation
of u1 1  u2 1  . . .   um 1  while A is a random matrix which depends on all of u1 j  u2 j  . . .   um j for
j = 2  3  . . .   q. Following the proof in [16]  we then apply the Hanson-Wright inequality to upper
bound the w-th moment E[|σT Aσ − E[σT Aσ]|w]  for integers w  in terms of the Frobenius norm
(cid:107)A(cid:107)F and operator norm (cid:107)A(cid:107)2 of the matrix A. The main twist here is that in the tensor setting 
when we try to apply this inequality  the matrix A is a random variable itself. Bounding (cid:107)A(cid:107)2 can
be accomplished by essentially viewing A as a (q − 1)-th order tensor  ﬂattening it q − 1 times 
and applying Khintchine’s inequality each time. The more complicated part of the argument is in
bounding (cid:107)A(cid:107)F   which again involves an inductive argument to obtain tail bounds on the Frobenius
norm of each of the blocks of A  which itself is a block-diagonal matrix with m blocks. The tail
bounds are not as strong as sub-Gaussian or even sub-exponential random variables  which makes
standard analyses based on moment generating functions inapplicable. We instead give a “level-set”
argument by giving a novel adaptation of analyses of Tao  originally needed for showing concentration
of p-norms for 0 < p < 1  to our tensor setting (see  e.g.  Proposition 6 in [36]).

1.1.2 Approximating Polynomial Kernels

Replicating experiments from [35]  we approximate polynomial kernels using Tensorized Random
Projection  TensorSketch  and Random Maclaurin [30] features. In Section 4.1 we demonstrate that
TensorSketch always fails for certain sparse inputs  while Tensorized Random Projection succeeds
with high probability. We show in 4.2 that Tensorized Random Projection has similar accuracy to
TensorSketch  and both vastly outperform Random Maclaurin features.

1.1.3 Compressing Neural Networks

We also experiment with using Tensorized Random Projection to compress the layers of a neural
network. In [8]  Arora et al. propose a method for compressing the layers of a neural network via
random projections and prove generalization bounds for such networks. To compress an individual
layer  they choose a basis set of random Rademacher matrices and project the layer’s weight matrix
onto this random basis set. We refer to this method here as Random Projection. The simplest  order
q = 2  Tensorized Random Projection can be viewed as a more efﬁcient  rank-1 version of Random
Projection: instead of using a basis set of fully-random Rademacher matrices  the basis set is made

3

up of random rank-1 Rademacher matrices. We show in 4.3 that Tensorized Random Projection
has similar test accuracy as Random Projection when compressing the top layer of a small neural
network.

1.2 Preliminaries

For a survey of using sketching for algorithms in randomized numerical linear algebra  we refer the
reader to [37]. We give a brief background here on several concepts related to our work.
√
There are many variants of the Johnson-Lindenstrauss Lemma  though for us the most useful is that for
an m × n matrix S of independent entries drawn from {−1/
m}  if m = Ω(−2 log(1/δ)) 
then for any ﬁxed vector x ∈ Rn  we have:
[(cid:107)Sx(cid:107)2

√
m  1/

2 = (1 ± )(cid:107)x(cid:107)2

2] ≥ 1 − δ.

Pr
S

This lemma is also known to hold for any matrix S with independent sub-Gaussian entries.
The matrix S is dense  and the CountSketch transform is instead much sparser.
Deﬁnition 1.1 (CountSketch). A CountSketch transform is deﬁned to be Π = ΦD ∈ Rm×n. Here 
D is an n × n random diagonal matrix with each diagonal entry independently chosen to be +1 or
−1 with equal probability  and Φ ∈ {0  1}m×n is an m × n binary matrix with Φh(i) i = 1 and all
remaining entries 0  where h : [n] → [m] is a random map such that for each i ∈ [n]  h(i) = j with
probability 1/m for each j ∈ [m]. For a matrix A ∈ Rn×d  ΠA can be computed in O(nnz(A)) time 
where nnz(A) denotes the number of non-zero entries of A.

We now deﬁne a tensor product and various sketches for tensors.
Deﬁnition 1.2 (⊗ product for vectors). Given q vectors u1 ∈ Rn1  u2 ∈ Rn2  ···   uq ∈ Rnq  we use
u1 ⊗ u2 ⊗ ··· ⊗ uq to denote an n1 × n2 × ··· × nq tensor such that  for each (j1  j2 ···   jq) ∈
[n1] × [n2] × ··· × [nq] 

(u1 ⊗ u2 ⊗ ··· ⊗ uq)j1 j2 ···  jq = (u1)j1(u2)j2 ··· (uq)jq  

where (ui)ji denotes the ji-th entry of vector ui.
We now formally deﬁne TensorSketch:
Deﬁnition 1.3 (TensorSketch [34]). Given q vectors v1  v2 ···   vq where for each i ∈ [q]  vi ∈ Rni 
let m be the target dimension. The TensorSketch transform is speciﬁed using q 3-wise independent
hash functions  h1 ···   hq  where for each i ∈ [q]  hi : [ni] → [m]  as well as q 4-wise independent
sign functions s1 ···   sq  where for each i ∈ [q]  si : [ni] → {−1  +1}.
TensorSketch applied to v1 ···   vq is then CountSketch applied to φ(v1 ···   vq) with hash function

i=1 ni] → [m] and sign functions S : [(cid:81)q

i=1 ni] → {−1  +1} deﬁned as follows:

H : [(cid:81)q

H(i1 ···   iq) = h1(i1) + h2(s2) + ··· + hq(iq)

(mod m) 

and

m log m)) time.

S(i1 ···   iq) = s1(i1) · s2(i2) · ··· · sq(iq).

Using the Fast Fourier Transform  TensorSketch(v1 ···   vq) can be computed in O((cid:80)q
xi ∈ Rn. The i-th coordinate (Sx)i of the sketch is equal to(cid:81)q

The main sketch we study is the classic randomized sketch of a tensor product of q vectors
m  where ui j
are independent random sign vectors. Kar and Karnick show [30] that if the sketching dimension
m = Ω(−2C 2
Ω log(1/δ))  where CΩ is a certain property of the point set Ω one wants to sketch  then
with probability 1 − δ  (cid:107)Sx(cid:107)2 = (1 ± )(cid:107)x(cid:107)2 for all x ∈ Ω. However  in their analysis C 2
Ω can be as
large as Θ(n2q)  even for a set Ω of O(1) vectors x.

√
j=1(cid:104)ui j  xj(cid:105)/

i=1(nnz(vi) +

2 Main Theorem and its Proof

Our main theorem combining sketches S and T described in Sections 2.1 and 2.2 is the following.
We provide its proof in Section 2.3.

4

Theorem 2.1. There is an oblivious sketch S · T : Rnq → Rm for m = Θ(−2 log(1/(δ)) +
−1 logq(1/(δ))  such that for any ﬁxed vector x ∈ Rnq and constant q  Pr[(cid:107)ST x(cid:107)2
2 = (1 ±
2] ≥ 1 − δ  where 0 <   δ < 1. Further  if x has the form x = x1 ⊗ x2 ⊗ ··· ⊗ xq for vectors
)(cid:107)x(cid:107)2

xi ∈ Rn for i = 1  2  . . .   q  then the time to compute ST x is O((cid:80)q

i=1 nnz(xi)m).

2.1

Initial Bound on Our Sketch Size

We are ready to present Tensorized Random Projection sketch S and the outermost layer of its analysis.
We defer statements and proofs of some key technical lemmas to Appendix A in the supplementary.
Note that both the sketching dimension m and the failure probability δ depend on n  which we later
eliminate with the help of Section B.
Theorem 2.2. Deﬁne oblivious sketch S : Rnq → Rm for m = Θ(−2 log(n/δ)+−1 logq(n/δ)) as
follows. Choose m· q independent uniformly random vectors ui j ∈ {+1 −1}n  where i = 1  . . .   m
m)u(cid:96) 1 ⊗ u(cid:96) 2 ⊗ ··· ⊗ u(cid:96) q  that is  the
and j = 1  . . .   q. Let the (cid:96) = 1  . . .   m-th row of S be (1/
√
. Then for any ﬁxed vector x ∈ Rnq
(i1  i2  . . .   iq)-th entry of the (cid:96)-th row of S is (1/
and failure probability δ < 1/nq it holds that Pr[(cid:107)Sx(cid:107)2

√
j=1 u(cid:96) j
ij
2 = (1 ± )(cid:107)x(cid:107)2

m)(cid:81)q

2] ≥ 1 − δ.

Proof. It sufﬁces to show for any unit vector x ∈ Rnq  that
2 − 1| > ] ≤ δ.

Pr[|(cid:107)Sx(cid:107)2

√
We deﬁne Si ∈ Rm×nq−1 to have (cid:96)-th row equal to (1/

u(cid:96) q  and deﬁne x = (x1  . . .   xn)  with each xi ∈ Rnq−1  so that Sx =(cid:80)n

m)u(cid:96) 1

i

(1)
· v(cid:96)  where v(cid:96) = u(cid:96) 2 ⊗ u(cid:96) 3 ⊗···⊗

i=1 Sixi. Then 

2 = (cid:107) n(cid:88)
Lemma 2.3 below proves that(cid:80)n

(cid:107)Sx(cid:107)2

i=1

n(cid:88)

i=1

(cid:88)

i(cid:54)=i(cid:48)

Sixi(cid:107)2

2 =

(cid:107)Sixi(cid:107)2

2 + 2

(cid:104)Sixi  Si(cid:48)

xi(cid:48)(cid:105).

2 holds with probability at least 1−δ/10.
2 = (1±/3)(cid:107)x(cid:107)2
We prove Lemma 2.3 and in effect Theorem 2.2 by induction on q and applying Theorem 2.2 for
q(cid:48) = q − 1. To complete the proof  we need to show that that

(cid:104)Sixi  Si(cid:48)

xi(cid:48)(cid:105) ≤ ε/3

(2)

i=1 (cid:107)Sixi(cid:107)2
(cid:88)

i(cid:54)=i(cid:48)

i (cid:104)v(cid:96)  xi(cid:105).
(cid:88)
m(cid:88)

(cid:96)=1

i(cid:54)=i(cid:48)

Z :=

1
m

(cid:80)
i(cid:54)=i(cid:48)(cid:80)m

least 1 − 9δ/10.
with probability at
(cid:96)th coordinate
√
m)u(cid:96) 1
is
of Sixi 
showing
(1/
i(cid:48) (cid:104)v(cid:96)  xi(cid:105)(cid:104)v(cid:96)  xi(cid:48)(cid:105) ≤ /3. Rearranging the order of summation  we
i u(cid:96) 1
(cid:96)=1 u(cid:96) 1
1
m
need to upper bound

Note
showing

equivalent

(Sixi)(cid:96) 

that

the

(2)

So

to

is

i u(cid:96) 1
u(cid:96) 1

i(cid:48) (cid:104)v(cid:96)  xi(cid:105)(cid:104)v(cid:96)  xi(cid:48)(cid:105) := uT Au 

Let E be the event that(cid:80)n

where u ∈ Rnm×1 and A ∈ Rnm×nm is a block-diagonal matrix with m blocks  each of size n × n.
2 = (1 ± /3). By Lemma 2.3  we have that Pr[E] ≥ 1 − δ/10.

i=1 (cid:107)Sixi(cid:107)2

Furthermore  let F be the event that (cid:107)A(cid:107)2 = O( log(q−1)(qnqm/δ)

) and

m

√
(cid:107)A(cid:107)F = O(1/

m + log1/2(1/δ) log(2q−3)/2(m/δ) log log(m/δ)/m)

bounds hold for the operator and Frobenius norm of A. By a union bound over Lemmas A.4 and A.7 
we have that Pr[F] ≥ 1 − δ/10. Lemma A.3 uses the Hanson-Wright Theorem to bound Z in terms
of (cid:107)A(cid:107)2 and (cid:107)A(cid:107)F and proves that Pr[Z ≥ ε/3|F] ≤ δ/2.

5

Putting this all together  we achieve our initial bound on (cid:107)Sx(cid:107)2
and v(cid:96)  we have 

2: Taking the probability over all u(cid:96)

Pr[|(cid:107)Sx(cid:107)2

2 − 1| > ] ≤ Pr[¬E] + Pr[|(cid:107)Sx(cid:107)2
Pr[Z ≥ /3]

≤ δ/10 + Pr[Z ≥ /3 | E]
≤ δ/10 +

Pr[E]

2 − 1| >  | E]

Pr[Z ≥ /3]
1 − δ/10

≤ δ/10 +
≤ δ/10 + (1 + δ/5) Pr[Z ≥ /3]
≤ δ/10 + (1 + δ/5)(Pr[Z ≥ /3 | F] + Pr[¬F])
≤ δ/10 + (1 + δ/5)(δ/2 + δ/10) = 3δ2/25 + 7δ/10
≤ 3δ/25 + 7δ/10 ≤ δ.

From the δ ≤ 1 assumption it follows that δ2 ≤ δ  which implies the second to last inequality and
concludes the proof.
Lemma 2.3. For all q ≥ 2  any set of ﬁxed vectors x1  . . .   xn ∈ Rnq−1  sketching dimension
m = Θ(−2 log(n/δ) + −1 logq−1(n/δ))  δ < 1/nq−1  and matrices Si ∈ Rm×nq−1 deﬁned in the

proof of Theorem 2.2  we have that Pr[(cid:80)n

2 = (1 ± /3)(cid:107)x(cid:107)2

2] ≥ 1 − δ/10.

i=1 (cid:107)Sixi(cid:107)2

√
Proof. Deﬁne matrix S0 ∈ Rm×nq−1 such that its (cid:96)-th row is v(cid:96)/
m from the proof of Theorem 2.2.
Additionally deﬁne m × m diagonal matrices Di such that Di
. Note that Si = DiS0 and
therefore (cid:107)Sixi(cid:107)2 = (cid:107)DiS0xi(cid:107)2 = (cid:107)S0xi(cid:107)2 holds since Di is ±1 diagonal matrix. To prove the
lemma  it is sufﬁcient to show that

(cid:96) (cid:96) := u(cid:96) 1

i

holds  since then we have that(cid:80)n

∀i ∈ [1  n] : Pr[(cid:107)S0xi(cid:107)2

i=1 (cid:107)Sixi(cid:107)2

2 = (cid:80)n

2 = (1 ± /3)(cid:107)xi(cid:107)2

i=1 (cid:107)S0xi(cid:107)2

2] ≥ 1 − δ/(10n)

2 = (1 ± /3)(cid:80)n

2 with probability at least 1 − δ by a union bound.

/3)(cid:107)x(cid:107)2
We prove inequality (3) by induction on q. In the base q = 2 case  entries of v(cid:96) = u(cid:96) 2 vectors are
i.i.d. ±1 random variables. Equivalently the entries of S0 are i.i.d. ±1 random variables. Applying
the Johnson-Lindenstrauss lemma [31] to S0 and each xi with δ(cid:48) = δ/(10n) proves the base case.
Now assume that Theorem 2.2 holds for q(cid:48) = q − 1. Observe that the structure of S0 for q(cid:48) = q − 1 is
exactly like that of S for q. Setting δ(cid:48) = δ/(10n) in Theorem 2.2 we have that inequality (3) holds for

sketching dimension m(cid:48) = Θ(cid:0)−2 log(n/δ(cid:48)) + −1 logq−1(n/δ(cid:48))(cid:1). Since log(n/δ(cid:48)) = log(n2/δ) =
Θ(log(n/δ)) we can simplify m(cid:48) to Θ(cid:0)−2 log(n/δ) + −1 logq−1(n/δ)(cid:1) as claimed.

(3)
2 = (1 ±

i (cid:107)xi(cid:107)2

2.2 Optimizing Our Sketch Size

We deﬁne the sketch T   which is a tensor product of CountSketch matrices. We compose our sketch
S from Section 2.1 with T in order to remove the dependence on n. See Section B for the proof.
Theorem 2.4. Let T be a tensor product of q CountSketch matrices T = T 1 ⊗ ··· ⊗ T q  where
each T i maps Rn → Rt for t = Θ(q3/(2δ)). Then for any unit vector x ∈ Rnq  we have
2 − 1| > ] ≤ δ. Furthermore  if x is of the form x1 ⊗ x2 ⊗ ··· ⊗ xq  for xi ∈ Rn for
Pr[|(cid:107)T x(cid:107)2
i = 1  2  . . .   q  then T x = T 1x1 ⊗ ··· ⊗ T qxq  where nnz(T ixi) ≤ nnz(xi) and where the time to
compute T ixi is O(nnz(xi)) for i = 1  2  . . .   q.

2.3 Proof of Theorem 2.1

Finally we prove our main claim by composing sketches S and T from Sections 2.1 and 2.2.
Proof. Our overall sketch is S · T   where S is the sketching matrix of Section 2.1  with sketch-
ing dimension m = Θ(−2 log(t/δ) + −1 logq(t/δ))  and T is the sketching matrix of Sec-
tion 2.2  with sketching dimension t = Θ(q3/(2δ)). To satisfy the conditions of Theorem

6

2.2  set δS = 0.5/tq. S is applied with approximation error /2 and failure probability δS
and T is applied with /2 and δ/2 respectively. Note that δS ≤ δ/2 and for q constant 
log(t/δS) = Θ(log(tq+1)) = Θ(log(t)) = Θ(log(1/(δ))) holds. Thus  the sketching dimension m
of ST is now Θ(−2 log(1/(δ)) + −1 logq(1/(δ))  and has no dependence on n. By Theorems
2.2  2.4  and a union bound  we have that for any unit vector x ∈ Rnq  Pr[|(cid:107)S · T x(cid:107)2
2 − 1| > ] ≤ δ.
In Theorem 2.4 above we show that  if x is a vector of the form x1 ⊗ x2 ⊗ ··· ⊗ xq  for xi ∈ Rn
for i = 1  2  . . .   q  then T x = T 1x1 ⊗ ··· ⊗ T qxq where each T ixi can be computed in O(nnz(xi))

time and where nnz(T ixi) ≤ nnz(xi). Thus  we can apply S to T x in O((cid:80)q

i=1 nnz(xi)m) time.

3 Lower Bound on Our Sketch Size
We next show that our sketching dimension of m = Θ(−2 log(1/(δ)) + −1 logq(1/(δ)) is nearly
tight for our particular sketch S · T . We will assume that q is constant. Note that S · T is an
oblivious sketch  and consequently by lower bounds for any oblivious sketch [24  27  31]  one has
that m = Ω(−2 log(1/δ)). More interestingly  we show a lower bound of m = Ω(−1 logq(1/δ))
summarized in the following theorem; see Section C for the proof.
Theorem 3.1. For any constant integer q  there is an input x ∈ Rnq for which if the number
m of rows of S satisﬁes m = o(ε−2 log(1/δ) + ε−1 logq(1/δ))  then with probability at least δ 
(cid:107)ST x(cid:107)2
Recall that the upper bound on our sketch size  for constant q  is m = O(ε−2 log(1/(δ)) +
ε−1 logq(1/(δ)))  and thus our analysis is nearly tight whenever log(1/(δ)) = Θ(log(1/δ)).
This holds  for example  whenever δ <   which is a typical setting since δ = 1/poly(n) for high
probability applications.

2 > (1 + )(cid:107)x(cid:107)2
2.

4 Experiments

We evaluate Tensorized Random Projections in three different applications. In Section 4.1 we
show that Tensorized Random Projections always succeed with high probability while TensorSketch
always fails on extremely sparse inputs. Then in Section 4.2 we observe that TensorSketch and
Tensorized Random Projections approximate non-linear SVMs with polynomial kernels equally
well. Finally in Section 4.3 we demonstrate that Random Projections and Tensorized Random
Projections are equally effective in reducing the number of parameters in a neural network while
Tensorized Random Projections are faster to compute. To the best of our knowledge this comprises
the ﬁrst experimental evaluation of [8]’s compression technique in terms of accuracy. The code for
the experiments is available at https://github.com/google-research/google-research/
tree/master/poly_kernel_sketch.

4.1 Success Probability of TensorSketch vs Tensorized Random Projection

In this section we demonstrate that TensorSketch cannot approximate the polynomial kernel κ(x  y) =
(cid:104)x  y(cid:105)q accurately for all pairs x  y ∈ V simultaneously if the vectors in the set V are not smooth 
i.e.  if (cid:107)x(cid:107)∞/(cid:107)x(cid:107)2 = Ω(1) holds for all x in V . TensorSketch fails even if the sketching dimension
m is much larger than |V |. On the contrary  Tensorized Random Projection works well.
Let a set S of data points be a standard basis in d dimensions. If k ≥ 2 coordinates of different vectors
collide in the same TensorSketch hash bucket then their common bucket is either zero or non-zero. If
it is 0  then (cid:104)ei  ei(cid:105)q is incorrectly estimated as 0 instead of 1. If the common bucket’s value is not
0  then the estimate of (cid:104)ei  ej(cid:105)q is non-zero  where i and j are any pair of two colliding coordinates.
Thus if there is a collision  then TensorSketch cannot estimate all dot products exactly. Moreover
the estimate cannot be close to the true kernel value either since if the dot product is incorrect  then

it is off by at least 1. Now if n ≥(cid:112)2m ln(1/(1 − p) then by the Birthday paradox [1] we have at

least one collision with probability p. If the number of vectors (and dimension) n is greater than the
sketching dimension m  which is the interesting case for sketching  then there is always a collision
by the pigeonhole principle. We remark that [26] provides a more detailed analysis of this sketching

7

dimension vs input vector smoothness tradeoff for CountSketch  which is a key building block of
TensorSketch.
We illustrate the above phenomena in Figure 1(a) as follows. We ﬁx the sketch size m = 100 and
vary the input dimension (= number of vectors) n along the x-axis. We measure the largest absolute
error in approximating κ(ei  ej) = (cid:104)x  y(cid:105)2 = δij among the ﬁrst n standard basis vectors and repeat
the experiment with 100 randomly drawn TensorSketch and Tensorized Random Projection instances.
The y-axis shows the average of the maximum error in approximating the true kernel  where error
bars correspond to one standard deviation. It is clear that TensorSketch’s error quickly becomes the
largest possible  1  as the number n of vectors passes the critical threshold
100  while Tensorized
Random Projection’s max error is much smaller  more concentrated  and grows at a much slower rate
in the same setting.

√

(a) Max error vs input dimension (n)

(b) Max error vs sketch size (m)

Figure 1: Maximum Error

Next  in Figure 1(b) we ﬁx the input dimension (= number of vectors) to n = 100 and vary the sketch
size m along the x-axis instead. The y-axis remains unchanged. We again observe that TensorSketch’s
max error decreases very slowly and it is still about 40% of the largest error possible (1) on average
at sketching size m = n2 = 104 (cid:28) d. Tensorized Random Projection’s max error is almost an order
of magnitude smaller at the same sketch size.

4.2 Comparison of Sketching Methods for SVMs with Polynomial Kernel

We replicate experiments from [35] to compare Tensorized Random Projections with TensorSketch
(TS) and Random Maclaurin (RM) sketch. We approximate the polynomial kernel (cid:104)x  y(cid:105)2 for the
Adult [19] and MNIST [32] datasets  by applying one of the above three sketches to the dataset. We
then train a linear SVM on the sketched dataset using LIBLINEAR [21]  and report the training
accuracy. This accuracy is the median accuracy of 5 trials. Our baseline is the training accuracy of a
non-linear SVM trained with the exact kernel by LIBSVM [13]. We experiment with between 100
and 500 random features.
Both Figures 2(a) and 2(b) show that Tensorized Random Projection has similar accuracy to TensorS-
ketch  and both have far better accuracy than Random Maclaurin. Recall that Random Maclaurin
approximates the kernel function κ with its Maclaurin series. For each sketch coordinate it randomly
picks degree t with probability 2−t and computes degree-t Tensorized Random Projection. This
is rather inefﬁcient for the polynomial kernel  which has exactly one non-zero coefﬁcient in its
Maclaurin expansion. Random Maclaurin’s generality is not required for the polynomial kernel and
we can obtain more accurate results for general kernels by sampling degree t proportional to its
Maclaurin coefﬁcient.

4.3 Compressing Neural Networks

We begin with a standard 2-layer fully connected neural network trained on MNIST [32] with a
baseline test accuracy of around 0.97. The ﬁrst layer has dimension (784x512) and the top layer has

8

(a) Adult dataset

(b) MNIST dataset

Figure 2: Accuracy vs Number of Random Features

dimension (512x10). Further speciﬁcs of the model can be found in the TensorFlow tutorials [3].
We sketch the weight matrix in the top layer using either Tensorized Random Projection or Random
Projection. We then reinsert this sketched matrix into the original model and evaluate its accuracy on
the MNIST test set. We compare both the test accuracy and the time needed to compute the sketch
for both methods.
In Figure 3(a) we see that both Tensorized Random Projection and Random Projection reach similar
test accuracy for the same number of parameters. Figure 3(b) in illustrates that Tensorized Random
Projection runs somewhat faster than ordinary Random Projection.

(a) Test Accuracy vs Sketch Size

(b) Time vs Sketch Size

Figure 3: Sketching the Last Layer of MNIST Neural Network

5 Conclusion

whether its m ·(cid:80)q

We presented a new analysis of Tensorized Random Projection  providing nearly optimal bounds and
demonstrated its versatility in multiple applications. An interesting question left for future work is
i=1 nnz(xi) running time could be further improved for dense x. We conjecture
that the iid random u(cid:96)
i Rademacher vectors might be replaced with fast pseudo-random rotations 
perhaps a product of one or more randomized Hadamard matrices similar to ideas in [7]  which could
possibly lead to an O(m log n) running time.

9

References
[1] https://en.wikipedia.org/wiki/Birthday_problem#Cast_as_a_collision_

problem.

[2] https://en.wikipedia.org/wiki/Polynomial_kernel  practical use section.

[3] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S. Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow 
Andrew Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser 
Manjunath Kudlur  Josh Levenberg  Dan Mané  Rajat Monga  Sherry Moore  Derek Murray 
Chris Olah  Mike Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal Talwar  Paul
Tucker  Vincent Vanhoucke  Vijay Vasudevan  Fernanda Viégas  Oriol Vinyals  Pete Warden 
Martin Wattenberg  Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-scale
machine learning on heterogeneous systems  2015. Software available from tensorﬂow.org.

[4] Thomas D Ahle  Michael Kapralov  Jakob BT Knudsen  Rasmus Pagh  Ameya Velingker 
David P Woodruff  and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels.
In SODA  2020.

[5] Thomas D. Ahle and Jakob Bæk Tejs Knudsen. Almost optimal tensor sketch. CoRR 

abs/1909.01821  2019.

[6] Noga Alon  Yossi Matias  and Mario Szegedy. The space complexity of approximating the

frequency moments. J. Comput. Syst. Sci.  58(1):137–147  1999.

[7] Alexandr Andoni  Piotr Indyk  Thijs Laarhoven  Ilya Razenshteyn  and Ludwig Schmidt.
Practical and optimal lsh for angular distance. In Advances in Neural Information Processing
Systems  pages 1225–1233  2015.

[8] Sanjeev Arora  Rong Ge  Behnam Neyshabur  and Yi Zhang. Stronger generalization bounds
for deep nets via a compression approach. In Proceedings of the 35th International Conference
on Machine Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018 
pages 254–263  2018.

[9] Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and
random projection. In 40th Annual Symposium on Foundations of Computer Science  FOCS
’99  17-18 October  1999  New York  NY  USA  pages 616–623  1999.

[10] Avrim Blum. Random projection  margins  kernels  and feature-selection. In Subspace  Latent
Structure and Feature Selection  Statistical and Optimization  Perspectives Workshop  SLSFS
2005  Bohinj  Slovenia  February 23-25  2005  Revised Selected Papers  pages 52–68  2005.

[11] Vladimir Braverman  Kai-Min Chung  Zhenming Liu  Michael Mitzenmacher  and Rafail
Ostrovsky. Ams without 4-wise independence on product domains. In 27th International
Symposium on Theoretical Aspects of Computer Science (STACS 2010)  2010.

[12] Emmanuel J. Candès  Justin K. Romberg  and Terence Tao. Robust uncertainty principles: exact
signal reconstruction from highly incomplete frequency information. IEEE Trans. Information
Theory  52(2):489–509  2006.

[13] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology  2:27:1–27:27  2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.

[14] Moses Charikar  Kevin Chen  and Martin Farach-Colton. Finding frequent items in data streams.
In International Colloquium on Automata  Languages  and Programming  pages 693–703.
Springer  2002.

[15] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference  STOC’13  Palo Alto  CA 
USA  June 1-4  2013  pages 81–90  2013.

10

[16] Michael B. Cohen  T. S. Jayram  and Jelani Nelson. Simple analyses of the sparse Johnson-
Lindenstrauss transform. In 1st Symposium on Simplicity in Algorithms  SOSA 2018  January
7-10  2018  New Orleans  LA  USA  pages 15:1–15:9  2018.

[17] Andrew Cotter  Joseph Keshet  and Nathan Srebro. Explicit approximations of the Gaussian

kernel. CoRR  2011.

[18] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and

Lindenstrauss. Random Struct. Algorithms  22(1):60–65  2003.

[19] Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine learning repository  2017.

[20] David L. Donoho. Compressed sensing. IEEE Trans. Information Theory  52(4):1289–1306 

2006.

[21] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. LIB-
LINEAR: A library for large linear classiﬁcation. Journal of Machine Learning Research 
9:1871–1874  2008.

[22] Sariel Har-Peled  Piotr Indyk  and Rajeev Motwani. Approximate nearest neighbor: Towards

removing the curse of dimensionality. Theory of Computing  8(1):321–350  2012.

[23] Piotr Indyk and Andrew McGregor. Declaring independence via the sketching of sketches. In
Proceedings of the Nineteenth Annual ACM-SIAM symposium on Discrete algorithms (SODA) 
pages 737–745. Society for Industrial and Applied Mathematics  2008.

[24] T. S. Jayram and David P. Woodruff. Optimal bounds for Johnson-Lindenstrauss transforms and

streaming problems with subconstant error. ACM Trans. Algorithms  9(3):26:1–26:17  2013.

[25] William B Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert

space. Contemporary Mathematics  1984.

[26] Lior Kamma  Casper B Freksen  and Kasper Green Larsen. Fully understanding the hashing

trick. In Advances in Neural Information Processing Systems  pages 5394–5404  2018.

[27] Daniel M. Kane  Raghu Meka  and Jelani Nelson. Almost optimal explicit Johnson-
Lindenstrauss families. In Approximation  Randomization  and Combinatorial Optimization.
Algorithms and Techniques - 14th International Workshop  APPROX 2011  and 15th Interna-
tional Workshop  RANDOM 2011  Princeton  NJ  USA  August 17-19  2011. Proceedings  pages
628–639  2011.

[28] Daniel M. Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. J. ACM 

61(1):4:1–4:23  2014.

[29] Michael Kapralov  Rasmus Pagh  Ameya Velingker  David P. Woodruff  and Amir Zandieh.

Oblivious sketching of high-degree polynomial kernels. CoRR  abs/1909.01410  2019.

[30] Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels.

In
Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics 
AISTATS 2012  La Palma  Canary Islands  Spain  April 21-23  2012  pages 583–591  2012.
Later: Journal of Machine Learning Research (JMLR) : WCP  22:583-591.

[31] Kasper Green Larsen and Jelani Nelson. Optimality of the Johnson-Lindenstrauss lemma. In
58th IEEE Annual Symposium on Foundations of Computer Science  FOCS 2017  Berkeley  CA 
USA  October 15-17  2017  pages 633–638  2017.

[32] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.

[33] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in

Theoretical Computer Science  1(2)  2005.

[34] Rasmus Pagh. Compressed matrix multiplication. In Innovations in Theoretical Computer

Science 2012  Cambridge  MA  USA  January 8-10  2012  pages 442–451  2012.

11

[35] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In
The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
KDD 2013  Chicago  IL  USA  August 11-14  2013  pages 239–247  2013.

[36] Terence Tao. Math 254a: Notes 1: Concentration of measure  https://terrytao.

wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/  2010.

[37] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in

Theoretical Computer Science  10(1-2):1–157  2014.

12

,Brandon Amos
Ivan Jimenez
Jacob Sacks
Byron Boots
J. Zico Kolter
Michela Meister
Tamas Sarlos
David Woodruff