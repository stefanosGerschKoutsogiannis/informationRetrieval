2015,Reflection  Refraction  and Hamiltonian Monte Carlo,Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However  it has difficulty simulating Hamiltonian dynamics with non-smooth functions  leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies  where intersections of the trajectory with discontinuities are detected  and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples  this method improves on traditional HMC.,Reﬂection  Refraction  and Hamiltonian Monte Carlo

Hadi Mohasel Afshar

Research School of Computer Science

Australian National University

Canberra  ACT 0200

hadi.afshar@anu.edu.au

Justin.Domke@nicta.com.au

Justin Domke

National ICT Australia (NICTA) &

Australian National University

Canberra  ACT 0200

Abstract

Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from con-
tinuous densities. However  it has difﬁculty simulating Hamiltonian dynamics
with non-smooth functions  leading to poor performance. This paper is motivated
by the behavior of Hamiltonian dynamics in physical systems like optics. We in-
troduce a modiﬁcation of the Leapfrog discretization of Hamiltonian dynamics on
piecewise continuous energies  where intersections of the trajectory with disconti-
nuities are detected  and the momentum is reﬂected or refracted to compensate for
the change in energy. We prove that this method preserves the correct stationary
distribution when boundaries are afﬁne. Experiments show that by reducing the
number of rejected samples  this method improves on traditional HMC.

1

Introduction

Markov chain Monte Carlo sampling is among the most general methods for probabilistic inference.
When the probability distribution is smooth  Hamiltonian Monte Carlo (HMC) (originally called
hybrid Monte Carlo [4]) uses the gradient to simulate Hamiltonian dynamics and reduce random
walk behavior. This often leads to a rapid exploration of the distribution [7  2]. HMC has recently
become popular in Bayesian statistical inference [13]  and is often the algorithm of choice.
Some problems display piecewise smoothness  where the density is differentiable except at certain
boundaries. Probabilistic models may intrinsically have ﬁnite support  being constrained to some
region. In Bayesian inference  it might be convenient to state a piecewise prior. More complex and
highly piecewise distributions emerge in applications where the distributions are derived from other
distributions (e.g. the distribution of the product of two continuous random variables [5]) as well as
applications such as preference learning [1]  or probabilistic programming [8].
While HMC is motivated by smooth distributions  the inclusion of an acceptance probability means
HMC does asymptotically sample correctly from piecewise distributions1. However  since leapfrog
numerical integration of Hamiltonian dynamics (see [9]) relies on the assumption that the corre-
sponding potential energy is smooth  such cases lead to high rejection probabilities  and poor per-
formance. Hence  traditional HMC is rarely used for piecewise distributions.
In physical systems that follow Hamiltonian dynamics [6]  a discontinuity in the energy can result
in two possible behaviors. If the energy decreases across a discontinuity  or the momentum is large
enough to overcome an increase  the system will cross the boundary with an instantaneous change
in momentum  known as refraction in the context of optics [3]. If the change in energy is too large to
be overcome by the momentum  the system will reﬂect off the boundary  again with an instantaneous
change in momentum.

1Technically  here we assume the total measure of the non-differentiable points is zero so that  with proba-

bility one  none is ever encountered

1

(a)

(b)

(c)

Figure 1: Example trajectories of baseline and reﬂective HMC. (a) Contours of the target distribution in
two dimensions  as deﬁned in Eq. 18. (b) Trajectories of the rejected (red crosses) and accepted (blue dots)
proposals using baseline HMC. (c) The same with RHMC. Both use leapfrog parameters L = 25 and  = 0.1
In RHMC  the trajectory reﬂects or refracts on the boundaries of the internal and external polytope boundaries
and thus has far fewer rejected samples than HMC  leading to faster mixing in practice. (More examples in
supplementary material.)

Recently  Pakman and Paninski [11  10] proposed methods for HMC-based sampling from piecewise
Gaussian distributions by exactly solving the Hamiltonian equations  and accounting for what we
refer to as refraction and reﬂection above. However  since Hamiltonian equations of motion can
rarely be solved exactly  the applications of this method are restricted to distributions whose log-
density is piecewise quadratic.
In this paper  we generalize this work to arbitrary piecewise continuous distributions  where each
region is a polytope  i.e. is determined by a set of afﬁne boundaries. We introduce a modiﬁcation to
the leapfrog numerical simulation of Hamiltonian dynamics  called Reﬂective Hamiltonian Monte
Carlo (RHMC)  by incorporating reﬂection and refraction via detecting the ﬁrst intersection of a
linear trajectory with a boundary. We prove that our method has the correct stationary distribution 
where the main technical difﬁculty is proving volume preservation of our dynamics to establish de-
tailed balance. Numerical experiments conﬁrm that our method is more efﬁcient than baseline HMC 
due to having fewer rejected proposal trajectories  particularly in high dimensions. As mentioned 
the main advantage of this method over [11] and [10] is that it can be applied to arbitrary piece-
wise densities  without the need for a closed-form solution to the Hamiltonian dynamics  greatly
increasing the scope of applicability.

2 Exact Hamiltonian Dynamics

Consider a distribution P (q) ∝ exp(−U (q)) over Rn  where U is the potential energy.
HMC [9] is based on considering a joint distribution on momentum and position space
P (q  p) ∝ exp(−H(q  p))  where H(q  p) = U (q) + K(p)  and K is a quadratic  meaning that
P (p) ∝ exp(−K(p)) is a normal distribution. If one could exactly simulate the dynamics  HMC
would proceed by (1) iteratively sampling p ∼ P (p)  (2) simulating the Hamiltonian dynamics

dqi
dt

=

∂H
∂pi

= pi

(1)

dpi
dt

= − ∂H
∂qi

= − ∂U
∂qi

(2)

for some period of time   and (3) reversing the ﬁnal value p. (Only needed for the proof of cor-
rectness  since this will be immediately discarded at the start of the next iteration in practice.)
Since steps (1) and (2-3) both leave the distribution P (p  q) invariant  so does a Markov chain that
alternates between the two steps. Hence  the dynamics have P (p  q) as a stationary distribution. Of
course  the above differential equations are not well-deﬁned when U has discontinuities  and are
typically difﬁcult to solve in closed-form.

2

0.20.20.20.20.250.250.250.250.250.750.750.750.750.80.80.80.850.850.90.90.95q1q2−6−4−20246−6−4−20246−6−4−20246−6−4−20246q1q2−6−4−20246−6−4−20246q1q2refractionreflection3 Reﬂection and Refraction with Exact Hamiltonian Dynamics

⊥ :=(cid:112)(cid:107)p⊥(cid:107)2 − 2∆U · p⊥(cid:107)p⊥(cid:107). That is  the discontinuity is passed  but the momentum is changed

Take a potential function U (q) which is differentiable in all points except at some boundaries of
partitions. Suppose that  when simulating the Hamiltonian dynamics  (q  p) evolves over time as in
the above equations whenever these equations are differentiable. However  when the state reaches a
boundary  decompose the momentum vector p into a component p⊥ perpendicular to the boundary
and a component p(cid:107) parallel to the boundary. Let ∆U be the (signed) difference in potential energy
on the two sides of the discontinuity. If (cid:107)p⊥(cid:107)2 > 2∆U then p⊥ is instantaneously replaced by
p(cid:48)
in the direction perpendicular to the boundary (refraction). (If ∆U is positive  the momentum will
decrease  and if it is negative  the momentum will increase.) On the other hand  if (cid:107)p⊥(cid:107)2 ≤ 2∆U 
then p⊥ is instantaneously replaced by −p⊥. That is  if the particle’s momentum is insufﬁcient
to climb the potential boundary  it bounces back by reversing the momentum component which is
perpendicular to the boundary.
Pakman and Paninski [11  10] present an algorithm to exactly solve these dynamics for quadratic
U. However  for non-quadratic U  the Hamiltonian dynamics rarely have a closed-form solution 
and one must resort to numerical integration  the most successful method for which is known as the
leapfrog dynamics.

4 Reﬂection and Refraction with Leapfrog Dynamics

Informally  HMC with leapfrog dynamics iterates three steps. (1) Sample p ∼ P (p). (2) Perform
leapfrog simulation  by discretizing the Hamiltonian equations into L steps using some small step-
size . Here  one interleaves a position step q ← q + p between two half momentum steps p ←
p − ∇U (q)/2. (3) Reverse the sign of p. If (q  p) is the starting point of the leapfrog dynamics 
and (q(cid:48)  p(cid:48)) is the ﬁnal point  accept the move with probability min(1  exp(H(p  q) − H(p(cid:48)  q(cid:48))))
See Algorithm 1.
It can be shown that this baseline HMC method has detailed balance with respect to P (p)  even if
U (q) is discontinuous. However  discontinuities mean that large changes in the Hamiltonian may
occur  meaning many steps can be rejected. We propose a modiﬁcation of the dynamics  namely 
reﬂective Hamiltonian Monte Carlo (RHMC)  which is also shown in Algorithm 1.
The only modiﬁcation is applied to the position steps: In RHMC  the ﬁrst intersection of the trajec-
tory with the boundaries of the polytope that contains q must be detected [11  10]. The position step
is only taken up to this boundary  and reﬂection/refraction occurs  depending on the momentum and
change of energy at the boundary. This process continues until the entire amount of time  has been
simulated. Note that if there is no boundary in the trajectory to time   this is equivalent to baseline
HMC. Also note that several boundaries might be visited in one position step.
As with baseline HMC  there are two alternating steps  namely drawing a new momentum variable
p from P (p) ∝ exp(−K(p)) and proposing a move (p  q) → (p(cid:48)  q(cid:48)) and accepting or rejecting it
with a probability determined by a Metropolis-Hastings ratio. We can show that both of these steps
leave the joint distribution P invariant  and hence a Markov chain that also alternates between these
steps will also leave P invariant.
As it is easy to see  drawing p from P (p) will leave P (q  p) invariant  we concentrate on the second
step i.e. where a move is proposed according to the piecewise leapfrog dynamics shown in Alg. 1.
Firstly  it is clear that these dynamics are time-reversible  meaning that if the simulation takes state
(q  p) to (q(cid:48)  p(cid:48)) it will also take state (q(cid:48)  p(cid:48)) to (q  p). Secondly  we will show that these dynamics
are volume preserving. Formally  if D denotes the leapfrog dynamics  we will show that the absolute
value of the determinant of the Jacobian of D is one. These two properties together show that the
probability density of proposing a move from (q  p) to (q(cid:48)  p(cid:48)) is the same of that proposing a move
from (q(cid:48)  p(cid:48)) to (q  p). Thus  if the move (q  p) → (q(cid:48)  p(cid:48)) is accepted according to the standard
Metropolis-Hastings ratio  R ((q  p) → (q(cid:48)  p(cid:48))) = min(1  exp(H(q  p) − H(q(cid:48)  p(cid:48)))  then detailed
balance will be satisﬁed. To see this  let Q denote the proposal distribution  Then  the usual proof of
correctness for Metropolis-Hastings applies  namely that

3

P (q  p)Q ((q  p) → (q(cid:48)  p(cid:48))) R ((q  p) → (q(cid:48)  p(cid:48)))
P (q(cid:48)  p(cid:48))Q((q(cid:48)  p(cid:48)) → (q  p))R((q(cid:48)  p(cid:48)) → (q  p))

P (q  p) min(1  exp(H(q  p) − H(q(cid:48)  p(cid:48)))
P (q(cid:48)  p(cid:48)) min(1  exp(H(q(cid:48)  p(cid:48)) − H(q  p))

=

= 1.

(3)

(The ﬁnal equality is easy to establish  consider-
ing the cases where H(q  p) ≥ H(q(cid:48)  p(cid:48)) and
H(q(cid:48)  p(cid:48)) ≤ H(q  p) separately.) This means that
detailed balance holds  and so P is a stationary dis-
tribution.
The major difference in the analysis of RHMC  rela-
tive to traditional HMC is that showing conservation
of volume is more difﬁcult. With standard HMC
and leapfrog steps  volume conservation is easy to
show by observing that each part of a leapfrog step
is a shear transformation. This is not the case with
RHMC  and so we must resort to a full analysis of
the determinant of the Jacobian  as explored in the
following section.

q(cid:48)

p(cid:48)

p

x

q

q1 = c

Figure 2: Transformation T :(cid:104)q p(cid:105)→(cid:104)q(cid:48) p(cid:48)(cid:105)
described by Lemma 1 (Refraction).

Algorithm 1: BASELINE & REFLECTIVE HMC ALGORITHMS

input : q0  current sample; U  potential function  L  # leapfrog steps;   leapfrog step size
output: next sample
begin

q ← q0;
p ∼ N (0  1)
H0 ← (cid:107)p(cid:107)2/2 + U (q)
for l = 1 to L do
p ← p − ∇U (q)/2
if BASELINEHMC then

else

q ← q + p
t0 ← 0

# Half-step evolution of momentum
# Full-step evolution of position:

while(cid:0)(cid:104)x  tx  ∆U  φ(cid:105) ← FIRSTDISCONTINUITY(q  p   − t0  U )(cid:1) (cid:54)= ∅ do

# i.e. if REFLECTIVEHMC:

p ← −p
H ← (cid:107)p(cid:107)2/2 + U (q); ∆H ← H − H0
if s ∼ U(0  1) < e−∆H return q else return q0

# Half-step evolution of momentum
# Not required in practice; for reversibility proof

end
note

: FIRSTDISCONTINUITY(·) returns x  the position of the ﬁrst intersection of a boundary plain
with line segment [q  q + ( − t0)p]; tx  the time it is visited; ∆U  the change in energy at the
discontinuity  and φ  the visited partition boundary. If no such point exists  ∅ is returned.

4

1
2
3
4
5

6
7
8
9
10
11
12
13
14

15
16
17
18
19
20
21
22
23
24
25

q ← x
t0 ← t0 + tx
(cid:104)p⊥  p(cid:107)(cid:105) = DECOMPOSE(p  φ)
if (cid:107)p⊥(cid:107)2 > 2∆U then

p⊥ ←(cid:112)(cid:107)p⊥(cid:107)2 − 2∆U · p⊥(cid:107)p⊥(cid:107)

else

p⊥ ← −p⊥
p ← p⊥ + p(cid:107)
q ← q + ( − t0)p
p ← p − ∇U (q)/2

# Perpendicular/ parallel to boundary plane φ

# Refraction

# Reﬂection

5 Volume Conservation

5.1 Refraction

In our ﬁrst result  we assume without loss of generality  that there is a boundary located at the
hyperplane q1 = c. This Lemma shows that  in the refractive case  volume is conserved. The setting
is visualized in Figure 2.
Lemma 1. Let T : (cid:104)q  p(cid:105) → (cid:104)q(cid:48)  p(cid:48)(cid:105) be a transformation in Rn that takes a unit mass located at
q := (q1  . . .   qn) and moves it with constant momentum p := (p1  . . .   pn) till it reaches a plane
q1 = c (at some point x := (c  x2  . . .   xn) where c is a constant). Subsequently the momentum is
changed to p(cid:48) =
1 > 2∆U (x)).
The move is carried on for the total time period τ till it ends in q(cid:48). For all n ∈ N  T satisﬁes the
volume preservation property.
Proof. Since for i > 1  the momentum is not affected by the collision  q(cid:48)
Thus 

(where ∆U (·) is a function of x s.t. p2

1 − 2∆U (x)  p2  . . .   pn

(cid:16)(cid:112)p2

i = pi.

(cid:17)

∂q(cid:48)
i
∂qj

∂q(cid:48)
i
∂pj

∂p(cid:48)
i
∂qj

=

=

=

i = qi + τ · pi and p(cid:48)
∂p(cid:48)
i
∂pj

= 0.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1

1

2

∂q(cid:48)
1
∂p1
∂p(cid:48)
1
∂p1
∂q(cid:48)
2
∂p1

∂q(cid:48)
1
∂qk
∂p(cid:48)
1
∂qk
∂q(cid:48)
2
∂qk

Therefore  if we explicitly write out the Jacobian determinant |J| of the transformation T   it is
∂q(cid:48)
1
∂pk
∂p(cid:48)
1
∂pk
∂q(cid:48)
2
∂pk
...
∂p(cid:48)
k−1
∂pk
∂q(cid:48)
k
∂pk
1

∂q(cid:48)
∂pk−1
∂p(cid:48)
∂pk−1
∂q(cid:48)
∂pk−1
...

···
···
···
...
···
···
···

∂p(cid:48)
k−1
∂qk
1
0

···
···
···
...
···
···
···

∂q(cid:48)
1
∂q1
∂p(cid:48)
1
∂q1
∂q(cid:48)
2
∂q1
...
∂p(cid:48)
k−1
∂q1
∂q(cid:48)
k
∂q1
∂p(cid:48)
k
∂q1

∂q(cid:48)
1
∂pk
∂p(cid:48)
1
∂pk
∂q(cid:48)
2
∂pk
...
∂p(cid:48)
k−1
∂pk
∂q(cid:48)
k
∂pk
∂p(cid:48)
k
∂pk

∂q(cid:48)
∂pk−1
∂p(cid:48)
∂pk−1
∂q(cid:48)
∂pk−1
...
∂p(cid:48)
k−1
∂pk−1
∂q(cid:48)
∂pk−1
∂p(cid:48)
∂pk−1

∂p(cid:48)
k−1
∂qk
∂q(cid:48)
k
∂qk
∂p(cid:48)
k
∂qk

∂p(cid:48)
k−1
∂p1
∂q(cid:48)
k
∂p1
∂p(cid:48)
k
∂p1

∂q(cid:48)
1
∂qk
∂p(cid:48)
1
∂qk
∂q(cid:48)
2
∂qk

∂q(cid:48)
1
∂p1
∂p(cid:48)
1
∂p1
0

∂q(cid:48)
1
∂q1
∂p(cid:48)
1
∂q1
0

0
0

0
0

=

0

0
0

...

0

1

k

1

1

2

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∀j ∈ {2  . . .   n}s.t. j (cid:54)= i 

(4)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂q(cid:48)

1
∂q1
∂p(cid:48)
1
∂q1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

∂q(cid:48)
1
∂p1
∂p(cid:48)
1
∂p1

Now  using standard properties of the determinant  we have that |J| =

We will now explicitly calculate these four derivatives. Due to the signiﬁcance of the result  we carry
out the computations in detail. Nonetheless  as this is a largely mechanical process  for brevity  we
do not comment on the derivation.
Let t1 be the time to reach x and t2 be the period between reaching x and the last point q(cid:48). Then:

def
=

c − q1
(5)
t1
p1
1 · t2 (8)
q(cid:48)
1 = c + p(cid:48)

p(cid:48)

1

def
=

(cid:113)

x = q + t1p

(6)

def

= τ − t1 = τ +

t2

q1 − c
p1

1 − 2∆U (x) (9)
p2

∂t2
∂q1

by (7)
=

1
p1

· ∂t2
∂q1

(8 & 10)

= t2 · ∂p(cid:48)

1
∂q1

∂q(cid:48)
1
∂q1
∂q(cid:48)
1
∂p1

=

1

1

∂q(cid:48)
∂p(cid:48)
∂q(cid:48)
∂p(cid:48)

1

1

· ∂p(cid:48)

1
∂q1

· ∂p(cid:48)

1
∂p1

+

+

∂q(cid:48)
1
∂t2
∂q(cid:48)
1
∂t2

=

2(cid:112)p2

∂p(cid:48)
1
∂p1

(9)
=

1

1 − 2∆U (x)

(7 & 8)

· ∂t2
∂p1

= t2 · ∂p(cid:48)
1 − 2∆U (x)(cid:1)
· ∂(cid:0)p2

1
∂p1

=

∂p1

1 · 1
+ p(cid:48)
p1
1 · c − q1

p2
1

+ p(cid:48)

p1 − ∂∆U (x)/∂p1

p(cid:48)

1

5

(7)

(10)

(11)

(12)

(13)

(cid:16)

∂p(cid:48)
1
∂q1

=

q + c−q1

p1

∂p1

∂

(5  6)
=

∂x
∂p1

2(cid:112)p2
(cid:17)

p

=

1 − 2∆U (x)(cid:1)
· ∂(cid:0)p2

∂q1

· −∂∆U (x)

∂q1

=

1
p(cid:48)

1

(14)

1

1 − 2∆U (x)

∂q
∂p1

+ (c − q1) · ∂(p/p1)
∂p1

= (c − q1)

−1
p2
1

· (0  p2  p3  . . .   pn) (15)

∂x
∂q1

(5  6)
=

∂q
∂q1

+

· ∂(c − q1)

∂q1

p
p1

=

q
q1

− p
p1

= (1  0  . . .   0) − (1 

p2
p1

  . . .  

pn
p1

) =

−1
p1

(0  p2  . . .   pn)

(16)

Substituting the appropriate terms into |J| = | ∂q(cid:48)

|J| (4)
=

=

∂q(cid:48)
1
∂q1
p(cid:48)
1
p1

1
∂p1

· ∂p(cid:48)
− ∂q(cid:48)
(cid:18) ∂p(cid:48)
1
∂p1
q1 − c
(cid:18) ∂∆U (x)
p1

1
∂p1

+

= 1 − 1
p1

t2

=

(11 & 12)

1
∂q1

(cid:18)
· ∂p(cid:48)
(cid:19) (13 & 14)
· ∂p(cid:48)
1
∂q1
· ∂x
(cid:18) q1 − c
∂x
∂p1
· ∂∆U (x)

=
q1 − c
p1

+

∂x

p2
1

|  we get that

∂p(cid:48)
1
∂p1

− ∂p(cid:48)

1
∂q1

∂q(cid:48)
1
∂p1

1
∂q1

+

(cid:19)

p(cid:48)
1
p1

(cid:18)
· ∂p(cid:48)
∂p(cid:48)
−
1
1
∂p1
∂p1
− q1 − c
p1 − ∂∆U (x)
(cid:19)
∂p1
· ∂x
∂q1

∂p(cid:48)
(cid:18)
1
∂q1
1
p1
· ∂∆U (x)

∂x

p1

t2

(cid:19)
(cid:19)

1

c − q1
+ p(cid:48)
p2
1
· ∂∆U (x)

∂q1

· (0  p2  p3  . . .   pn) +

q1 − c
p1

· −1

p1

(0  p2  . . .   pn)

= 1.

· ∂p(cid:48)

1
∂q1

(cid:19)

(15 & 16)

= 1 − 1
p1

5.2 Reﬂection

Now  we turn to the reﬂective case  and again show that volume is conserved. Again  we assume
without loss of generality that there is a boundary located at the hyperplane q1 = c.
Lemma 2. Let T : (cid:104)q  p(cid:105) → (cid:104)q(cid:48)  p(cid:48)(cid:105) be a transformation in Rn that takes a unit mass located at
q := (q1  . . .   qn) and moves it with the constant momentum p := (p1  . . .   pn) till it reaches a plane
q1 = c (at some point x := (c  x2  . . .   xn) where c is a constant). Subsequently the mass is bounced
back (reﬂected) with momentum p(cid:48) = (−p1  p2  . . .   pn) The move is carried on for a total time
period τ till it ends in q(cid:48). For all n ∈ N  T satisﬁes the volume preservation property.

Proof. Similar to Lemma 1  for i > 1  q(cid:48)
s.t. j (cid:54)= i 

= ∂q(cid:48)

= ∂p(cid:48)

= ∂p(cid:48)

i = qi +τ·pi and p(cid:48)
= 0. Consequently  by equation (4)  and since p(cid:48)

i = pi. Therefore  for any j ∈ {2  . . .   n}

1 = −p1 

i
∂pj

i
∂qj

∂q(cid:48)
i
∂qj

i
∂pj

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂q(cid:48)

1
∂q1
∂p(cid:48)
1
∂q1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂q(cid:48)

1
∂q1
0

∂q(cid:48)
1
∂p1
∂p(cid:48)
1
∂p1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

∂q(cid:48)
1
−1
∂p1

|J| =

−∂q(cid:48)
1
∂q1

(17)

As before  let t1 be the time to reach x and t2 be the period between reaching x and the last point
1 · t2 is equal to 2c − τ p1 − q1.
q(cid:48). That is  t1
Hence  |J| = 1.

= τ − t1. It follows that q(cid:48)

= c + p(cid:48)
def

= c−q1

and t2

def

def

p1

1

5.3 Reﬂective Leapfrog Dynamics

Theorem 1. In RHMC (Algorithm 1) for sampling from a continuous and piecewise distribution P
which has afﬁne partitioning boundaries  leapfrog simulation preserves volume in (q  p) space.
Proof. We split the algorithm into several atomic transformations Ti. Each transformation is either
(a) a momentum step  (b) a full position step with no reﬂection/refraction or (c) a full or partial
position step where exactly one reﬂection or refraction occurs.

6

To prove that the total algorithm preserves volume  it is sufﬁcient to show that the volume is pre-
served under each Ti (i.e. |JTi(q  p)| = 1) since:

|JT1oT2o···oTm| = |JT1| · |JT2|···|JTm|

Transformations of kind (a) and (b) are shear mappings and therefore they preserve the volume [9].
Now consider a (full or partial) position step where a single refraction occurs. If the reﬂective plane
is in form q1 = c  by lemma 1  the volume preservation property holds. Otherwise  as long as
the reﬂective plane is afﬁne  via a rotation of basis vectors  the problem is reduced to the former
case. Since volume is conserved under rotation  in this case the volume is also conserved. With
similar reasoning  by lemma 2  reﬂection on a afﬁne reﬂective boundary preserves volume. Thus 
since all component transformations of RHMC leapfrog simulation preserve volume  the proof is
complete.

Along with the fact that the leapfrog dynamics are time-reversible  this shows that the algorithm
satisﬁes detailed balance  and so has the correct stationary distribution.

6 Experiment

Compared to baseline HMC  we expect that RHMC will simulate Hamiltonian dynamics more ac-
curately and therefore leads to fewer rejected samples. On the other hand  this comes at the expense
of slower leapfrog position steps since intersections  reﬂections and refractions must be computed.
To test the trade off  we compare the RHMC to baseline HMC [9] and tuned Metroplis-Hastings
(MH) with a simple isotropic Normal proposal distribution. MH is automatically tuned after [12] by
testing 100 equidistant proposal variances in interval (0  1] and accepting a variance for which the
acceptance rate is closest to 0.24. The baseline HMC and RHMC number of steps L and step size 
are chosen to be 100 and 0.1 respectively. (Many other choices are in the Appendix.) While HMC
performance is highly standard to these parameters [7] RHMC is consistently faster.
The comparison takes place on a heavy tail piecewise model with (non-normalized) negative log
probability



(cid:112)q(cid:62)A q
1 +(cid:112)q(cid:62)A q

+∞ 

U (q) =

if (cid:107)q(cid:107)∞ ≤ 3
if 3 < (cid:107)q(cid:107)∞ ≤ 6
otherwise

(18)

(19)

where A is a positive deﬁnite matrix. We carry out the experiment on three choices of (position
space) dimensionalites  n = 2  10 and 50.
Due to the symmetry of the model  the ground truth expected value of q is known to be 0. Therefore 
the absolute error of the expected value (estimated by a chain q(1)  . . .   q(k) of MCMC samples) in
each dimension d = 1  . . .   n is the absolute value of the mean of d-th element of the sample vectors.
The worst mean absolute error (WMAE) over all dimensions is taken as the error measurement of
the chain.

(cid:16)

q(1)  . . .   q(k)(cid:17)

WMAE

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k(cid:88)

s=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

1
k

max

d=1 ...n

s

qd

For each algorithm  20 Markov chains are run and the mean WMAE and 99% conﬁdence intervals
(as error bars) versus the number of iterations (i.e. Markov chain sizes) are time (milliseconds) are
depicted in ﬁgure 2. All algorithms are implemented in java and run on a single thread of a 3.40GHz
CPU.
For each of the 20 repetitions  some random starting point is chosen uniformly and used for all three
of the algorithms. We use a diagonal matrix for A where  for each repetition  each entry on the main
diagonal is either exp(−5) or exp(5) with equal probabilities.
As the results show  even in low dimensions  the extra cost of the position step is more or less
compensated by its higher effective sample size but as the dimensionality increases  the RHMC
signiﬁcantly outperforms both baseline HMC and tuned MH.

7

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: Error (worst mean absolute error per dimension) versus (a-c) iterations and (e-f) time (ms).
Tuned.HM is Metropolis Hastings with a tuned isotropic Gaussian proposal distribution. (Many more examples
in supplementary material.)

7 Conclusion
We have presented a modiﬁcation of the leapfrog dynamics for Hamiltonian Monte Carlo for piece-
wise smooth energy functions with afﬁne boundaries (i.e. each region is a polytope)  inspired by
physical systems. Though traditional Hamiltonian Monte Carlo can in principle be used on such
functions  the fact that the Hamiltonian will often be dramatically changed by the dynamics can re-
sult in a very low acceptance ratio  particularly in high dimensions. By better preserving the Hamil-
tonian  reﬂective Hamiltonian Monte Carlo (RHMC) accepts more moves and thus has a higher
effective sample size  leading to much more efﬁcient probabilistic inference. To use this method 
one must be able to detect the ﬁrst intersection of a position trajectory with polytope boundaries.

Acknowledgements

NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program.

8

1001011021031040123456Iteration (dim=2  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMC1011021030123456Time (dim=2  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMC1001011021031040123456Iteration (dim=10  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMC1011021030123456Time (dim=10  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMC1001011021031040123456Iteration (dim=50  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMC1011021031040123456Time (dim=50  L=100  ε =0.1)Error Baseline.HMCTuned.MHReflective.HMCReferences
[1] Hadi Mohasel Afshar  Scott Sanner  and Ehsan Abbasnejad. Linear-time gibbs sampling in
In Association for the Advancement of Artiﬁcial Intelligence 

piecewise graphical models.
pages 665–673  2015.

[2] Steve Brooks  Andrew Gelman  Galin Jones  and Xiao-Li Meng. Handbook of Markov Chain

Monte Carlo. CRC press  2011.

[3] Hans Adolph Buchdahl. An introduction to Hamiltonian optics. Courier Corporation  1993.
[4] Simon Duane  Anthony D Kennedy  Brian J Pendleton  and Duncan Roweth. Hybrid monte

carlo. Physics letters B  195(2):216–222  1987.

[5] Andrew G Glen  Lawrence M Leemis  and John H Drew. Computing the distribution of
the product of two continuous random variables. Computational statistics & data analysis 
44(3):451–464  2004.

[6] Donald T Greenwood. Principles of dynamics. Prentice-Hall Englewood Cliffs  NJ  1988.
[7] Matthew D Homan and Andrew Gelman. The no-u-turn sampler: Adaptively setting path
lengths in hamiltonian monte carlo. The Journal of Machine Learning Research  15(1):1593–
1623  2014.

[8] David Lunn  David Spiegelhalter  Andrew Thomas  and Nicky Best. The bugs project: evolu-

tion  critique and future directions. Statistics in medicine  28(25):3049–3067  2009.

[9] Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo  2  2011.

[10] Ari Pakman and Liam Paninski. Auxiliary-variable exact hamiltonian monte carlo samplers
for binary distributions. In Advances in Neural Information Processing Systems  pages 2490–
2498  2013.

[11] Ari Pakman and Liam Paninski. Exact hamiltonian monte carlo for truncated multivariate

gaussians. Journal of Computational and Graphical Statistics  23(2):518–542  2014.

[12] Gareth O Roberts  Andrew Gelman  Walter R Gilks  et al. Weak convergence and optimal
scaling of random walk metropolis algorithms. The annals of applied probability  7(1):110–
120  1997.

[13] Stan Development Team. Stan Modeling Language Users Guide and Reference Manual  Ver-

sion 2.5.0  2014.

9

,Hadi Mohasel Afshar
Justin Domke