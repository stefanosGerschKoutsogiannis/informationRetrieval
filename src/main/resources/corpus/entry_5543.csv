2018,Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds,In recent years  unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However  its theoretical understanding is still immature  which prevents us from fully utilizing the power of neural networks. In this work  we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure  unfolded ISTA can attain a linear convergence  which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore  we propose to incorporate thresholding in the network to perform support selection  which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations  including sparse vector recovery and a compressive sensing experiment on real image data  corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available: https://github.com/xchen-tamu/linear-lista-cpss.,Theoretical Linear Convergence of Unfolded ISTA

and its Practical Weights and Thresholds

Xiaohan Chen∗

Department of Computer Science and Engineering

Texas A&M University

College Station  TX 77843  USA

chernxh@tamu.edu

Zhangyang Wang

Department of Computer Science and Engineering

Texas A&M University

College Station  TX 77843  USA

atlaswang@tamu.edu

Jialin Liu∗

Department of Mathematics

University of California  Los Angeles

Los Angeles  CA 90095  USA
liujl11@math.ucla.edu

Wotao Yin

Department of Mathematics

University of California  Los Angeles

Los Angeles  CA 90095  USA
wotaoyin@math.ucla.edu

Abstract

In recent years  unfolding iterative algorithms as neural networks has become an
empirical success in solving sparse recovery problems. However  its theoretical
understanding is still immature  which prevents us from fully utilizing the power
of neural networks. In this work  we study unfolded ISTA (Iterative Shrinkage
Thresholding Algorithm) for sparse signal recovery. We introduce a weight struc-
ture that is necessary for asymptotic convergence to the true sparse signal. With this
structure  unfolded ISTA can attain a linear convergence  which is better than the
sublinear convergence of ISTA/FISTA in general cases. Furthermore  we propose
to incorporate thresholding in the network to perform support selection  which
is easy to implement and able to boost the convergence rate both theoretically
and empirically. Extensive simulations  including sparse vector recovery and a
compressive sensing experiment on real image data  corroborate our theoretical
results and demonstrate their practical usefulness. We have made our codes publicly
available.2.

Introduction

1
This paper aims to recover a sparse vector x∗ from its noisy linear measurements:

b = Ax∗ + ε 

(1)
where b ∈ Rm  x ∈ Rn  A ∈ Rm×n  ε ∈ Rm is additive Gaussian white noise  and we have m (cid:28) n.
(1) is an ill-posed  highly under-determined system. However  it becomes easier to solve if x∗ is
assumed to be sparse  i.e. the cardinality of support of x∗  S = {i|x∗
i (cid:54)= 0}  is small compared to n.
A popular approach is to model the problem as the LASSO formulation (λ is a scalar):

(2)
and solve it using iterative algorithms such as the iterative shrinkage thresholding algorithm (ISTA)
[1]:

minimize

x

(cid:107)b − Ax(cid:107)2

1
2

2 + λ(cid:107)x(cid:107)1
(cid:17)

(cid:16)

xk+1 = ηλ/L

 
∗These authors contributed equally and are listed alphabetically.
2https://github.com/xchen-tamu/linear-lista-cpss

xk +

AT (b − Axk)

1
L

k = 0  1  2  . . .

(3)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

where ηθ is the soft-thresholding function3 and L is usually taken as the largest eigenvalue of AT A.
In general  ISTA converges sublinearly for any given and ﬁxed dictionary A and sparse code x∗ [2]
In [3]  inspired by ISTA  the authors proposed a learning-based model named Learned ISTA (LISTA).
They view ISTA as a recurrent neural network (RNN) that is illustrated in Figure 1(a)  where
L λ. LISTA  illustrated in Figure 1(b)  unrolls the RNN and
W1 = 1
truncates it into K iterations:

L AT   W2 = I − 1

L AT A  θ = 1

xk+1 = ηθk (W k

1 b + W k

2 xk) 

k = 0  1 ···   K − 1 

(4)

leading to a K-layer feed-forward neural network with side connections.
Different from ISTA where no parameter is learnable (except the hyper parameter λ to be tuned) 
LISTA is treated as a specially structured neural network and trained using stochastic gradient descent
(SGD)  over a given training dataset {(x∗
i=1 sampled from some distribution P(x  b). All the
parameters Θ = {(W k

k=0 are subject to learning. The training is modeled as:

2   θk)}K−1

i   bi)}N

1   W k

Θ

minimize

(5)
Many empirical results  e.g.  [3–7]  show that a trained K-layer LISTA (with K usually set to 10 ∼ 20)
or its variants can generalize more than well to unseen samples (x(cid:48)  b(cid:48)) from the same P(x  b) and
recover x(cid:48) from b(cid:48) to the same accuracy within one or two order-of-magnitude fewer iterations than
the original ISTA. Moreover  the accuracies of the outputs {xk} of the layers k = 1  ..  K gradually
improve.

2

(cid:13)(cid:13)(cid:13)xK(cid:16)

Θ  b  x0(cid:17) − x∗(cid:13)(cid:13)(cid:13)2

.

Ex∗ b

(a) RNN structure of ISTA.

(b) Unfolded learned ISTA Network.

Figure 1: Diagrams of ISTA and LISTA.

1.1 Related Works
Many recent works [8  9  4  10  11] followed the idea of [3] to construct feed-forward networks by
unfolding and truncating iterative algorithms  as fast trainable regressors to approximate the solutions
of sparse coding models. On the other hand  progress has been slow towards understanding the
efﬁcient approximation from a theoretical perspective. The most relevant works are discussed below.
[12] attempted to explain the mechanism of LISTA by re-factorizing the Gram matrix of dictionary 
which tries to nearly diagonalize the Gram matrix with a basis that produces a small perturbation of
the (cid:96)1 ball. They re-parameterized LISTA into a new factorized architecture that achieved similar
acceleration gain to LISTA. Using an “indirect” proof  [12] was able to show that LISTA can converge
faster than ISTA  but still sublinearly. Lately  [13] tried to relate LISTA to a projected gradient descent
descent (PGD) relying on inaccurate projections  where a trade-off between approximation error and
convergence speed was made possible.
[14] investigated the convergence property of a sibling architecture to LISTA  proposed in [4]  which
was obtained by instead unfolding/truncating the iterative hard thresholding (IHT) algorithm rather
than ISTA. The authors argued that they can use data to train a transformation of dictionary that
can improve its restricted isometry property (RIP) constant  when the original dictionary is highly
correlated  causing IHT to fail easily. They moreover showed it beneﬁcial to allow the weights to
decouple across layers. However  the analysis in [14] cannot be straightforwardly extended to ISTA
although IHT is linearly convergent [15] under rather strong assumptions.
In [16]  a similar learning-based model inspired by another iterative algorithm solve LASSO  approx-
imated message passing (AMP)  was studied. The idea was advanced in [17] to substituting the AMP

3Soft-thresholding function is deﬁned in a component-wise way: ηθ(x) = sign(x) max(0 |x| − θ)

2

proximal operator (soft-thresholding) with a learnable Gaussian denoiser. The resulting model  called
Learned Denoising AMP (L-DAMP)  has theoretical guarantees under the asymptotic assumption
named “state evolution.” While the assumption is common in analyzing AMP algorithms  the tool is
not directly applicable to ISTA. Moreover  [16] shows L-DAMP is MMSE optimal  but there is no
result on its convergence rate. Besides  we also note the empirical effort in [18] that introduces an
Onsager correction to LISTA to make it resemble AMP.

1.2 Motivations and Contributions
We attempt to answer the following questions  which are not fully addressed in the literature yet:

exploiting certain dependencies among its parameters {(W k
network and improve the recovery results?

• Rather than training LISTA as a conventional “black-box” network  can we beneﬁt from
k=0 to simplify the
• Obtained with sufﬁciently many training samples from the target distribution P(x  b)  LISTA
works very well. So  we wonder if there is a theoretical guarantee to ensure that LISTA (4)
converges 4 faster and/or produces a better solution than ISTA (3) when its parameters are
ideal? If the answer is afﬁrmative  can we quantize the amount of acceleration?
• Can some of the acceleration techniques such as support detection that were developed for

2   θk)}K−1

1   W k

LASSO also be used to improve LISTA?

Our Contributions: this paper aims to introduce more theoretical insights for LISTA and to further
unleash its power. To our best knowledge  this is the ﬁrst attempt to establish a theoretical convergence
rate (upper bound) of LISTA directly. We also observe that the weight structure and the thresholds
can speedup the convergence of LISTA:

• We give a result on asymptotic coupling between the weight matrices W k

2 . This
result leads us to eliminating one of them  thus reducing the number of trainable parameters.
This elimination still retains the theoretical and experimental performance of LISTA.
• ISTA is generally sublinearly convergent before its iterates settle on a support. We prove
that  however  there exists a sequence of parameters that makes LISTA converge linearly
since its ﬁrst iteration. Our numerical experiments support this theoretical result.
• Furthermore  we introduce a thresholding scheme for support selection  which is extremely
simple to implement and signiﬁcantly boosts the practical convergence. The linear conver-
gence results are extended to support detection with an improved rate.

1 and W k

Detailed discussions of the above three points will follow after Theorems 1  2 and 3  respectively.
Our proofs do not rely on any indirect resemblance  e.g.  to AMP [18] or PGD [13]. The theories
are supported by extensive simulation experiments  and substantial performance improvements are
observed when applying the weight coupling and support selection schemes. We also evaluated
LISTA equipped with those proposed techniques in an image compressive sensing task  obtaining
superior performance over several of the state-of-the-arts.

2 Algorithm Description
We ﬁrst establish the necessary condition for LISTA convergence  which implies a partial weight
coupling structure for training LISTA. We then describe the support-selection technique.

2.1 Necessary Condition for LISTA Convergence and Partial Weight Coupling
Assumption 1 (Basic assumptions). The signal x∗ and the observation noise ε are sampled from the
following set:

(x∗  ε) ∈ X (B  s  σ) (cid:44)(cid:110)

(cid:12)(cid:12)(cid:12)|x∗

(cid:111)

.

(x∗  ε)

i | ≤ B ∀i  (cid:107)x∗(cid:107)0 ≤ s (cid:107)ε(cid:107)1 ≤ σ

(6)

In other words  x∗ is bounded and s-sparse5 (s ≥ 2)  and ε is bounded.
Theorem 1 (Necessary Condition). Given {W k
(1) and {xk}∞

k=0 and x0 = 0  let b be observed by
k=1 be generated layer-wise by LISTA (4). If the following holds uniformly for any
4 The convergence of ISTA/FISTA measures how fast the k-th iterate proceeds; the convergence of LISTA

2   θk}∞

1   W k

measures how fast the output of the k-th layer proceeds as k increases.

5A signal is s-sparse if it has no more than s non-zero entries.

3

(x∗  ε) ∈ X (B  s  0) (no observation noise):

xk(cid:16){W τ

τ =0  b  x0(cid:17) → x∗ 

1   W τ

2   θτ}k−1

as k → ∞

and {W k

2 }∞

k=1 are bounded

then {W k

1   W k

2   θk}∞

(cid:107)W k
k=0 must satisfy

2 (cid:107)2 ≤ BW  

∀k = 0  1  2 ···  

2 − (I − W k
W k
θk → 0 

1 A) → 0 
as k → ∞.

as k → ∞

(7)
(8)

1   W k

Proofs of the results throughout this paper can be found in the supplementary. The conclusion (7)
2 }∞
demonstrates that the weights {W k
k=0 in LISTA asymptotically satisﬁes the following partial
weight coupling structure:
2 = I − W k
W k
(cid:17)

We adopt the above partial weight coupling for all layers  letting W k = (W k
simplifying LISTA (4) to:

(9)
1 )T ∈ (cid:60)m×n  thus

(cid:16)

1 A.

xk + (W k)(cid:62)(b − Axk)

 

k = 0  1 ···   K − 1 

(10)

xk+1 = ηθk

k=0 remain as free parameters to train. Empirical results in Fig. 3 illustrate that the

where {W k  θk}K−1
structure (9)  though having fewer parameters  improves the performance of LISTA.
The coupled structure (9) for soft-thresholding based algorithms was empirically studied in [16]. The
similar structure was also theoretically studied in Proposition 1 of [14] for IHT algorithms using the
ﬁxed-point theory  but they let all layers share the same weights  i.e. W k
2.2 LISTA with Support Selection
We introduce a special thresholding scheme to LISTA  called support selection  which is inspired by
“kicking” [19] in linearized Bregman iteration. This technique shows advantages on recoverability
and convergence. Its impact on improving LISTA convergence rate and reducing recovery errors
will be analyzed in Section 3. With support selection  at each LISTA layer before applying soft
thresholding  we will select a certain percentage of entries with largest magnitudes  and trust them
as “true support” and won’t pass them through thresholding. Those entries that do not go through
thresholding will be directly fed into next layer  together with other thresholded entires.
Assume we select pk% of entries as the trusted support at layer k. LISTA with support selection can
be generally formulated as

1 = W1 ∀k.

2 = W2  W k

xk+1 = ηss

pk
θk

W k

1 b + W k

k = 0  1 ···   K − 1 

2 xk(cid:17)

 

where ηss is the thresholding operator with support selection  formally deﬁned as:

(ηss

pk
θk (v))i =

vi
vi − θk
0
vi + θk
vi

: vi > θk 
: vi > θk 
: −θk ≤ vi ≤ θk
: vi < −θk 
: vi < −θk 

i ∈ Spk
i /∈ Spk
i /∈ Spk
i ∈ Spk

(v) 
(v) 

(v) 
(v) 

where Spk

(v) includes the elements with the largest pk% magnitudes in vector v:

(cid:110)
i1  i2 ···   ipk

(cid:12)(cid:12)(cid:12)|vi1| ≥ |vi2| ≥ ···|vipk|··· ≥ |vin|(cid:111)

.

Spk

(v) =

(cid:16)



(11)

(12)

To clarify  in (11)  pk is a hyperparameter to be manually tuned  and θk is a parameter to train. We use
an empirical formula to select pk for layer k: pk = min(p · k  pmax)  where p is a positive constant
and pmax is an upper bound of the percentage of the support cardinality. Here p and pmax are both
hyperparameters to be manually tuned.
If we adopt the partial weight coupling in (9)  then (11) is modiﬁed as

k = 0  1 ···   K − 1.

(13)

(cid:16)

xk+1 = ηss

pk
θk

(cid:17)

 

xk + (W k)T (b − Axk)

4

and the initial point x0. Strictly speaking  xk should be written as xk(cid:16){W τ   θτ}k−1

Algorithm abbreviations For simplicity  hereinafter we will use the abbreviation “CP” for the
partial weight coupling in (9)  and “SS” for the support selection technique. LISTA-CP denotes
the LISTA model with weights coupling (10). LISTA-SS denotes the LISTA model with support
selection (11). Similarly  LISTA-CPSS stands for a model using both techniques (13)  which has the
best performance. Unless otherwise speciﬁed  LISTA refers to the baseline LISTA (4).
3 Convergence Analysis
In this section  we formally establish the impacts of (10) and (13) on LISTA’s convergence. The
output of the kth layer xk depends on the parameters {W τ   θτ}k−1
τ =0  the observed measurement b
. By the
observation model b = Ax∗ + ε  since A is given and x0 can be taken as 0  xk therefore depends
on {(W τ   θτ )}k
. For simplicity  we instead
just write xk(x∗  ε).
Theorem 2 (Convergence of LISTA-CP). Given {W k  θk}∞
k=1 be generated
by (10). If Assumption 1 holds and s is sufﬁciently small  then there exists a sequence of parameters
{W k  θk} such that  for all (x∗  ε) ∈ X (B  s  σ)  we have the error bound:

τ =0  x∗ and ε. So  we can write xk(cid:16){W τ   θτ}k−1

k=0 and x0 = 0  let {xk}∞

τ =0  b  x0(cid:17)

τ =0  x∗  ε

(cid:17)

(cid:107)xk(x∗  ε) − x∗(cid:107)2 ≤ sB exp(−ck) + Cσ 

∀k = 1  2 ···  

(14)

where c > 0  C > 0 are constants that depend only on A and s. Recall s (sparsity of the signals) and
σ (noise-level) are deﬁned in (6).

If σ = 0 (noiseless case)  (14) reduces to

(cid:107)xk(x∗  0) − x∗(cid:107)2 ≤ sB exp(−ck).

(15)

The recovery error converges to 0 at a linear rate as the number of layers goes to inﬁnity. Combined
with Theorem 1  we see that the partial weight coupling structure (10) is both necessary and sufﬁcient
to guarantee convergence in the noiseless case. Fig. 3 validates (14) and (15) directly.
Discussion: The bound (15) also explains why LISTA (or its variants) can converge faster than ISTA
and fast ISTA (FISTA) [2]. With a proper λ (see (2))  ISTA converges at an O(1/k) rate and FISTA
converges at an O(1/k2) rate [2]. With a large enough λ  ISTA achieves a linear rate [20  21]. With
¯x(λ) being the solution of LASSO (noiseless case)  these results can be summarized as: before the
iterates xk settle on a support6 

xk → ¯x(λ) sublinearly  (cid:107)¯x(λ) − x∗(cid:107) = O(λ) 
xk → ¯x(λ) linearly  (cid:107)¯x(λ) − x∗(cid:107) = O(λ) 

λ > 0
λ large enough.

Based on the choice of λ in LASSO  the above observation reﬂects an inherent trade-off between
convergence rate and approximation accuracy in solving the problem (1)  see a similar conclusion in
[13]: a larger λ leads to faster convergence but a less accurate solution  and vice versa.
However  if λ is not constant throughout all iterations/layers  but instead chosen adaptively for each
step  more promising trade-off can arise7. LISTA and LISTA-CP  with the thresholds {θk}K−1
k=0 free to
train  actually adopt this idea because {θk}K−1
k=0 corresponds to a path of LASSO parameters {λk}K−1
k=0 .
With extra free trainable parameters  {W k}K−1
k=0 (LISTA-CP) or {W k
k=0 (LISTA)  learning
based algorithms are able to converge to an accurate solution at a fast convergence rate. Theorem 2
demonstrates the existence of such sequence {W k  θk}k in LISTA-CP (10). The experiment results
in Fig. 4 show that such {W k  θk}k can be obtained by training.
Assumption 2. Signal x∗ and observation noise ε are sampled from the following set:
i | ≤ B ∀i  (cid:107)x∗(cid:107)1 ≥ B (cid:107)x∗(cid:107)0 ≤ s (cid:107)ε(cid:107)1 ≤ σ

(x∗  ε) ∈ ¯X (B  B  s  σ) (cid:44)(cid:110)

2 }K−1

(x∗  ε)

1   W k

(cid:111)

.

(16)

(cid:12)(cid:12)(cid:12)|x∗

6After xk settles on a support  i.e. as k large enough such that support(xk) is ﬁxed  even with small λ 

ISTA reduces to a linear iteration  which has a linear convergence rate [22].

7This point was studied in [23  24] with classical compressive sensing settings  while our learning settings

can learn a good path of parameters without a complicated thresholding rule or any manual tuning.

5

Theorem 3 (Convergence of LISTA-CPSS). Given {W k  θk}∞
k=1 be
generated by (13). With the same assumption and parameters as in Theorem 2  the approximation
error can be bounded for all (x∗  ε) ∈ X (B  s  σ):

k=0 and x0 = 0  let {xk}∞

(cid:107)xk(x∗  ε) − x∗(cid:107)2 ≤ sB exp

+ Cssσ 

∀k = 1  2 ···  

(17)

ct
ss

(cid:16) − k−1(cid:88)

t=0

(cid:16) − k−1(cid:88)

(cid:17)

(cid:17)

ss ≥ c for all k and Css ≤ C.

where ck
If Assumption 2 holds  s is small enough  and B ≥ 2Cσ (SNR is not too small)  then there exists
another sequence of parameters { ˜W k  ˜θk} that yields the following improved error bound: for all
(x∗  ε) ∈ ¯X (B  B  s  σ) 

(cid:107)xk(x∗  ε) − x∗(cid:107)2 ≤ sB exp

+ ˜Cssσ 

∀k = 1  2 ···  

(18)

˜ct
ss

where ˜ck

ss ≥ c for all k  ˜ck

ss > c for large enough k  and ˜Css < C.

t=0

The bound in (17) ensures that  with the same assumptions and parameters  LISTA-CPSS is at least no
worse than LISTA-CP. The bound in (18) shows that  under stronger assumptions  LISTA-CPSS can
be strictly better than LISTA-CP in both folds: ˜ck
ss > c is the better convergence rate of LISTA-CPSS;
˜Css < C means that the LISTA-CPSS can achieve smaller approximation error than the minimum
error that LISTA can achieve.
4 Numerical Results
For all the models reported in this section  including the baseline LISTA and LAMP models   we
adopt a stage-wise training strategy with learning rate decaying to stabilize the training and to get
better performance  which is discussed in the supplementary.

4.1 Simulation Experiments
Experiments Setting. We choose m = 250  n = 500. We sample the entries of A i.i.d. from the
standard Gaussian distribution  Aij ∼ N (0  1/m) and then normalize its columns to have the unit
(cid:96)2 norm. We ﬁx a matrix A in each setting where different networks are compared. To generate
sparse vectors x∗  we decide each of its entry to be non-zero following the Bernoulli distribution with
pb = 0.1. The values of the non-zero entries are sampled from the standard Gaussian distribution. A
test set of 1000 samples generated in the above manner is ﬁxed for all tests in our simulations.
All the networks have K = 16 layers. In LISTA models with support selection  we add p% of entries
into support and maximally select pmax% in each layer. We manually tune the value of p and pmax
for the best ﬁnal performance. With pb = 0.1 and K = 16  we choose p = 1.2 for all models in
simulation experiments and pmax = 12 for LISTA-SS but pmax = 13 for LISTA-CPSS. The recovery
performance is evaluated by NMSE (in dB):

NMSE(ˆx  x∗) = 10 log10

(cid:18)E(cid:107)ˆx − x∗(cid:107)2

(cid:19)

E(cid:107)x∗(cid:107)2

 

2 → I − W k

1 A  and θk → 0  as k → ∞. Theorem 1 is directly validated.

where x∗ is the ground truth and ˆx is the estimate obtained by the recovery algorithms (ISTA  FISTA 
LISTA  etc.).
1 A)(cid:107)2 and θk  obtained
Validation of Theorem 1. In Fig 2  we report two values  (cid:107)W k
by the baseline LISTA model (4) trained under the noiseless setting. The plot clearly demonstrates
that W k
Validation of Theorem 2. We report the test-set NMSE of LISTA-CP (10) in Fig. 3. Although (10)
ﬁxes the structure between W k
2   the ﬁnal performance remains the same with the baseline
LISTA (4)  and outperforms AMP  in both noiseless and noisy cases. Moreover  the output of interior
layers in LISTA-CP are even better than the baseline LISTA. In the noiseless case  NMSE converges
exponentially to 0; in the noisy case  NMSE converges to a stationary level related with the noise-level.
This supports Theorem 2: there indeed exist a sequence of parameters {(W k  θk)}K−1
k=0 leading to
linear convergence for LISTA-CP  and they can be obtained by data-driven learning.

2 − (I − W k

1 and W k

6

(a) Weight W k

2 → I − W k

1 A as k → ∞.

(b) The threshold θk → 0.

Figure 2: Validation of Theorem 1.

(a) SNR = ∞

(b) SNR = 30

Figure 3: Validation of Theorem 2.

Validation of Discussion after
Theorem 2. In Fig 4  We com-
pare LISTA-CP and ISTA with
different λs (see the LASSO
problem (2)) as well as an adap-
tive threshold rule similar to one
in [23]  which is described in the
supplementary. As we have dis-
cussed after Theorem 2  LASSO
has an inherent tradeoff based
on the choice of λ. A smaller
λ leads to a more accurate solu-
tion but slower convergence. The
Figure 4: Validating Discussion after Theorem 2 (SNR = ∞).
adaptive thresholding rule ﬁxes
this issue:
it uses large λk for
small k  and gradually reduces it as k increases to improve the accuracy [23]. Except for adaptive
thresholds {θk}k (θk corresponds to λk in LASSO)  LISTA-CP has adaptive weights {W k}k  which
further greatly accelerate the convergence. Note that we only ran ISTA and FISTA for 16 iterations 
just enough and fair to compare them with the learned models. The number of iterations is so small
that the difference between ISTA and FISTA is not quite observable.
Validation of Theorem 3. We compare the recovery NMSEs of LISTA-CP (10) and LISTA-CPSS
(13) in Fig. 5. The result of the noiseless case (Fig. 5(a)) shows that the recovery error of LISTA-SS
converges to 0 at a faster rate than that of LISTA-CP. The difference is signiﬁcant with the number of
layers k ≥ 10  which supports our theoretical result: “˜ck
ss > c as k large enough” in Theorem 3. The
result of the noisy case (Fig. 5(b)) shows that LISTA-CPSS has better recovery error than LISTA-CP.
This point supports ˜Css < C in Theorem 3. Notably  LISTA-CPSS also outperforms LAMP [16] 
when k > 10 in the noiseless case  and even earlier as SNR becomes lower.
Performance with Ill-Conditioned Matrix. We train LISTA  LAMP  LISTA-CPSS with ill-
conditioned matrices A of condition numbers κ = 5  30  50. As is shown in Fig. 6  as κ increases 
the performance of LISTA remains stable while LAMP becomes worse  and eventually inferior to
LISTA when κ = 50. Although our LISTA-CPSS also suffers from ill-conditioning  its performance
always stays much better than LISTA and LAMP.

7

1234567891011121314151600.511.522.531234567891011121314151600.20.40.60.81012345678910111213141516-50-40-30-20-100ISTAFISTAAMPLISTALISTA-CP012345678910111213141516-50-40-30-20-100ISTAFISTAAMPLISTALISTA-CP0100200300400500600700800-40-30-20-100NMSE (dB)ISTA ( = 0.1)ISTA ( = 0.05)ISTA ( = 0.025)LISTA-CPISTA (adaptivek)(a) Noiseless Case

(b) Noisy Case: SNR=40dB

(c) Noisy Case: SNR=30dB

(d) Noisy Case: SNR=20dB

Figure 5: Validation of Theorem 3.

(a) κ = 5

(b) κ = 30

(c) κ = 50

Figure 6: Performance in ill-conditioned situations (SNR = ∞).

4.2 Natural Image Compressive Sensing
Experiments Setting. We perform a compressive sensing (CS) experiment on natural images
(patches). We divide the BSD500 [25] set into a training set of 400 images  a validation set of
50 images  and a test set of 50 images. For training  we extract 10 000 patches f ∈ R16×16 at
random positions of each image  with all means removed. We then learn a dictionary D ∈ R256×512
from them  using a block proximal gradient method [26]. For each testing image  we divide it into
non-overlapping 16 × 16 patches. A Gaussian sensing matrices Φ ∈ Rm×256 is created in the same
manner as in Sec. 4.1  where m
Since f is typically not exactly sparse under the dictionary D  Assumptions 1 and 2 no longer strictly
hold. The primary goal of this experiment is thus to show that our proposed techniques remain robust
and practically useful in non-ideal conditions  rather than beating all CS state-of-the-arts.
Network Extension. In the real data case  we have no ground-truth sparse code available as the
regression target for the loss function (5). In order to bypass pre-computing sparse codes f over D
on the training set  we are inspired by [11]: ﬁrst using layer-wise pre-training with a reconstruction
loss w.r.t. dictionary D plus an l1 loss  shown in (19)  where k is the layer index and Θk denotes all
parameters in the k-th and previous layers; then appending another learnable fully-connected layer
(initialized by D) to LISTA-CPSS and perform an end-to-end training with the cost function (20).

256 is the CS ratio.

N(cid:88)
N(cid:88)

i=1

Lk(Θk) =

L(Θ  WD) =

(cid:107)fi − D · xk

i (Θk)(cid:107)2

2 + λ(cid:107)xk

i (Θk)(cid:107)1

(cid:107)fi − WD · xK

i (Θ)(cid:107)2

2 + λ(cid:107)xK

i (Θ)(cid:107)1

(19)

(20)

i=1

8

012345678910111213141516-70-60-50-40-30-20-100ISTAFISTAAMPLISTALAMPLISTA-CPLISTA-SSLISTA-CPSS012345678910111213141516-50-40-30-20-100ISTAFISTAAMPLISTALAMPLISTA-CPLISTA-SSLISTA-CPSS012345678910111213141516-40-30-20-100ISTAFISTAAMPLISTALAMPLISTA-CPLISTA-SSLISTA-CPSS012345678910111213141516-30-20-100ISTAFISTAAMPLISTALAMPLISTA-CPLISTA-SSLISTA-CPSS012345678910111213141516-70-60-50-40-30-20-100LISTALAMPLISTA-CPSS012345678910111213141516-70-60-50-40-30-20-100LISTALAMPLISTA-CPSS012345678910111213141516-70-60-50-40-30-20-100LISTALAMPLISTA-CPSSTable 1: The Average PSRN (dB) for Set 11 test images with CS ratio ranging from 0.2 to 0.6

Algorithm
TVAL3

Recon-Net

LIHT
LISTA

LISTA-CPSS

20% 30% 40% 50% 60%
33.16
25.37
32.44
27.18
25.83
34.00
35.99
28.17
28.25
36.39

31.51
31.39
31.73
34.26
34.60

29.76
30.49
29.93
32.75
32.87

28.39
29.11
27.83
30.43
30.54

Results. The results are reported in Table 1. We build CS models at the sample rates of
20%  30%  40%  50%  60% and test on the standard Set 11 images as in [27]. We compare our
results with three baselines: the classical iterative CS solver  TVAL3 [28]; the “black-box” deep
learning CS solver  Recon-Net [27];a l0-based network unfolded from IHT algorithm [15]  noted as
LIHT; and the baseline LISTA network  in terms of PSNR (dB)8. We build 16-layer LIHT  LISTA
and LISTA-CPSS networks and set λ = 0.2. For LISTA-CPSS  we set p% = 0.4% more entries
into the support in each layer for support selection. We also select support w.r.t. a percentage of the
largest magnitudes within the whole batch rather than within a single sample as we do in theorems
and simulated experiments  which we emprically ﬁnd is beneﬁcial to the recovery performance. Table
1 conﬁrms LISTA-CPSS as the best performer among all. The advantage of LISTA-CPSS and LISTA
over Recon-Net also endorses the incorporation of the unrolled sparse solver structure into deep
networks.
5 Conclusions
In this paper  we have introduced a partial weight coupling structure to LISTA  which reduces the
number of trainable parameters but does not hurt the performance. With this structure  unfolded ISTA
can attain a linear convergence rate. We have further proposed support selection  which improves
the convergence rate both theoretically and empirically. Our theories are endorsed by extensive
simulations and a real-data experiment. We believe that the methodology in this paper can be
extended to analyzing and enhancing other unfolded iterative algorithms.

Acknowledgments

The work by X. Chen and Z. Wang is supported in part by NSF RI-1755701. The work by J. Liu and
W. Yin is supported in part by NSF DMS-1720237 and ONR N0001417121. We would also like to
thank all anonymous reviewers for their tremendously useful comments to help improve our work.

References
[1] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approximations.

Journal of Fourier analysis and Applications  14(5-6):629–654  2008.

[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM journal on imaging sciences  2(1):183–202  2009.

[3] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings
of the 27th International Conference on International Conference on Machine Learning  pages
399–406. Omnipress  2010.

[4] Zhangyang Wang  Qing Ling  and Thomas Huang. Learning deep l0 encoders.

Conference on Artiﬁcial Intelligence  pages 2194–2200  2016.

In AAAI

[5] Zhangyang Wang  Ding Liu  Shiyu Chang  Qing Ling  Yingzhen Yang  and Thomas S Huang.
D3: Deep dual-domain based fast restoration of jpeg-compressed images. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition  pages 2764–2772  2016.

[6] Zhangyang Wang  Shiyu Chang  Jiayu Zhou  Meng Wang  and Thomas S Huang. Learning a
task-speciﬁc deep architecture for clustering. In Proceedings of the 2016 SIAM International
Conference on Data Mining  pages 369–377. SIAM  2016.

8We applied TVAL3  LISTA and LISTA-CPSS on 16 × 16 patches to be fair. For Recon-Net  we used their
default setting working on 33 × 33 patches  which was veriﬁed to perform better than using smaller patches.

9

[7] Zhangyang Wang  Yingzhen Yang  Shiyu Chang  Qing Ling  and Thomas S Huang. Learning a

deep (cid:96)∞ encoder for hashing. pages 2174–2180  2016.

[8] Pablo Sprechmann  Alexander M Bronstein  and Guillermo Sapiro. Learning efﬁcient sparse
and low rank models. IEEE transactions on pattern analysis and machine intelligence  2015.

[9] Zhaowen Wang  Jianchao Yang  Haichao Zhang  Zhangyang Wang  Yingzhen Yang  Ding Liu 
and Thomas S Huang. Sparse Coding and its Applications in Computer Vision. World Scientiﬁc.

[10] Jian Zhang and Bernard Ghanem. ISTA-Net: Interpretable optimization-inspired deep network

for image compressive sensing. In IEEE CVPR  2018.

[11] Joey Tianyi Zhou  Kai Di  Jiawei Du  Xi Peng  Hao Yang  Sinno Jialin Pan  Ivor W Tsang 
Yong Liu  Zheng Qin  and Rick Siow Mong Goh. SC2Net: Sparse LSTMs for sparse coding. In
AAAI Conference on Artiﬁcial Intelligence  2018.

[12] Thomas Moreau and Joan Bruna. Understanding trainable sparse coding with matrix factoriza-

tion. In ICLR  2017.

[13] Raja Giryes  Yonina C Eldar  Alex Bronstein  and Guillermo Sapiro. Tradeoffs between
convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions on
Signal Processing  2018.

[14] Bo Xin  Yizhou Wang  Wen Gao  David Wipf  and Baoyuan Wang. Maximal sparsity with deep

networks? In Advances in Neural Information Processing Systems  pages 4340–4348  2016.

[15] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing.

Applied and computational harmonic analysis  27(3):265–274  2009.

[16] Mark Borgerding  Philip Schniter  and Sundeep Rangan. AMP-inspired deep networks for

sparse linear inverse problems. IEEE Transactions on Signal Processing  2017.

[17] Christopher A Metzler  Ali Mousavi  and Richard G Baraniuk. Learned D-AMP: Principled neu-
ral network based compressive image recovery. In Advances in Neural Information Processing
Systems  pages 1770–1781  2017.

[18] Mark Borgerding and Philip Schniter. Onsager-corrected deep learning for sparse linear inverse
problems. In 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP).

[19] Stanley Osher  Yu Mao  Bin Dong  and Wotao Yin. Fast linearized bregman iteration for
compressive sensing and sparse denoising. Communications in Mathematical Sciences  2010.

[20] Kristian Bredies and Dirk A Lorenz. Linear convergence of iterative soft-thresholding. Journal

of Fourier Analysis and Applications  14(5-6):813–837  2008.

[21] Lufang Zhang  Yaohua Hu  Chong Li  and Jen-Chih Yao. A new linear convergence result for

the iterative soft thresholding algorithm. Optimization  66(7):1177–1189  2017.

[22] Shaozhe Tao  Daniel Boley  and Shuzhong Zhang. Local linear convergence of ista and ﬁsta on

the lasso problem. SIAM Journal on Optimization  26(1):313–336  2016.

[23] Elaine T. Hale  Wotao Yin  and Yin Zhang. Fixed-point continuation for (cid:96)1-minimization:

methodology and convergence. SIAM Journal on Optimization  19(3):1107–1130  2008.

[24] Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares

problem. SIAM Journal on Optimization  23(2):1062–1091  2013.

[25] David Martin  Charless Fowlkes  Doron Tal  and Jitendra Malik. A database of human seg-
mented natural images and its application to evaluating segmentation algorithms and measuring
In Proceedings of the International Conference on Computer Vision 
ecological statistics.
volume 2  pages 416–423  2001.

[26] Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex
optimization with applications to nonnegative tensor factorization and completion. SIAM
Journal on imaging sciences  6(3):1758–1789  2013.

10

[27] Kuldeep Kulkarni  Suhas Lohit  Pavan Turaga  Ronan Kerviche  and Amit Ashok. Recon-
Net: Non-iterative reconstruction of images from compressively sensed measurements. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2016.

[28] Chengbo Li  Wotao Yin  Hong Jiang  and Yin Zhang. An efﬁcient augmented lagrangian method
with applications to total variation minimization. Computational Optimization and Applications 
56(3):507–530  2013.

[29] Dimitris Bertsimas and John N Tsitsiklis. Introduction to linear optimization. Athena Scientiﬁc

Belmont  MA  1997.

11

,Xiaohan Chen
Jialin Liu
Zhangyang Wang
Wotao Yin