2016,SDP Relaxation with Randomized Rounding for Energy Disaggregation,We develop a scalable  computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance based on the total energy-consumption signal of a household. The current state of the art models the problem as inference in factorial HMMs  and finds an approximate solution to the resulting quadratic integer program via quadratic programming. Here we take a more principled approach  better suited to integer programming problems  and find an approximate optimum by combining convex semidefinite relaxations with randomized rounding  as well as with a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results demonstrate the superiority of our methods both in synthetic and real-world datasets.,SDP Relaxation with Randomized Rounding for

Energy Disaggregation

Kiarash Shaloudegi

Imperial College London

k.shaloudegi16@imperial.ac.uk

András György

Imperial College London

a.gyorgy@imperial.ac.uk

Csaba Szepesvári
University of Alberta

szepesva@ualberta.ca

Wilsun Xu

University of Alberta
wxu@ualberta.ca

Abstract

We develop a scalable  computationally efﬁcient method for the task of energy
disaggregation for home appliance monitoring. In this problem the goal is to
estimate the energy consumption of each appliance over time based on the total
energy-consumption signal of a household. The current state of the art is to model
the problem as inference in factorial HMMs  and use quadratic programming to
ﬁnd an approximate solution to the resulting quadratic integer program. Here we
take a more principled approach  better suited to integer programming problems 
and ﬁnd an approximate optimum by combining convex semideﬁnite relaxations
randomized rounding  as well as a scalable ADMM method that exploits the special
structure of the resulting semideﬁnite program. Simulation results both in synthetic
and real-world datasets demonstrate the superiority of our method.

1

Introduction

Energy efﬁciency is becoming one of the most important issues in our society. Identifying the
energy consumption of individual electrical appliances in homes can raise awareness of power
consumption and lead to signiﬁcant saving in utility bills. Detailed feedback about the power
consumption of individual appliances helps energy consumers to identify potential areas for energy
savings  and increases their willingness to invest in more efﬁcient products. Notifying home owners
of accidentally running stoves  ovens  etc.  may not only result in savings but also improves safety.
Energy disaggregation or non-intrusive load monitoring (NILM) uses data from utility smart meters
to separate individual load consumptions (i.e.  a load signal) from the total measured power (i.e.  the
mixture of the signals) in households.
The bulk of the research in NILM has mostly concentrated on applying different data mining and
pattern recognition methods to track the footprint of each appliance in total power measurements.
Several techniques  such as artiﬁcial neural networks (ANN) [Prudenzi  2002  Chang et al.  2012 
Liang et al.  2010]  deep neural networks [Kelly and Knottenbelt  2015]  k-nearest neighbor (k-NN)
[Figueiredo et al.  2012  Weiss et al.  2012]  sparse coding [Kolter et al.  2010]  or ad-hoc heuristic
methods [Dong et al.  2012] have been employed. Recent works  rather than turning electrical events
into features fed into classiﬁers  consider the temporal structure of the data[Zia et al.  2011  Kolter
and Jaakkola  2012  Kim et al.  2011  Zhong et al.  2014  Egarter et al.  2015  Guo et al.  2015] 
resulting in state-of-the-art performance [Kolter and Jaakkola  2012]. These works usually model the
individual appliances by independent hidden Markov models (HMMs)  which leads to a factorial
HMM (FHMM) model describing the total consumption.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

FHMMs  introduced by Ghahramani and Jordan [1997]  are powerful tools for modeling times series
generated from multiple independent sources  and are great for modeling speech with multiple people
simultaneously talking [Rennie et al.  2009]  or energy monitoring which we consider here [Kim et al. 
2011]. Doing exact inference in FHMMs is NP hard; therefore  computationally efﬁcient approximate
methods have been the subject of study. Classic approaches include sampling methods  such as
MCMC or particle ﬁltering [Koller and Friedman  2009] and variational Bayes methods [Wainwright
and Jordan  2007  Ghahramani and Jordan  1997]. In practice  both methods are nontrivial to make
work and we are not aware of any works that would have demonstrated good results in our application
domain with the type of FHMMs we need to work and at practical scales.
In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by
FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of
the output of the underlying HMMs (perhaps with some noise)  and (ii) the number of transitions
are small in comparison to the signal length. FHMMs with the ﬁrst property are called additive. In
this paper we derive an efﬁcient  convex relaxation based method for FHMMs of the above type 
which signiﬁcantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting
relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular 
we replace the quadratic programming relaxation of Kolter and Jaakkola  2012 with a relaxation
to an semi-deﬁnite program (SDP)  which  based on the literature of relaxations is expected to be
tighter and thus better. While SDPs are convex and could in theory be solved using interior-point
(IP) methods in polynomial time [Malick et al.  2009]  IP scales poorly with the size of the problem
and is thus unsuitable to our large scale problem which may involve as many a million variables. To
address this problem  capitalizing on the structure of our relaxation coming from our FHMM model 
we develop a novel variant of ADMM [Boyd et al.  2011] that uses Moreau-Yosida regularization
and combine it with a version of randomized rounding that is inspired by the the recent work of
Park and Boyd [2015]. Experiments on synthetic and real data conﬁrm that our method signiﬁcantly
outperforms other algorithms from the literature  and we expect that it may ﬁnd its applications in
other FHMM inference problems  too.

1.1 Notation
Throughout the paper  we use the following notation: R denotes the set of real numbers  Sn
+ denotes
the set of n ⇥ n positive semideﬁnite matrices  I{E} denotes the indicator function of an event E
(that is  it is 1 if the event is true and zero otherwise)  1 denotes a vector of appropriate dimension
whose entries are all 1. For an integer K  [K] denotes the set {1  2  . . .   K}. N (µ  ⌃) denotes the
Gaussian distribution with mean µ and covariance matrix ⌃. For a matrix A  trace(A) denotes its
trace and diag(A) denotes the vector formed by the diagonal entries of A.

2 System Model

Following Kolter and Jaakkola [2012]  the energy usage of the household is modeled using an additive
factorial HMM [Ghahramani and Jordan  1997]. Suppose there are M appliances in a household.
Each of them is modeled via an HMM: let Pi 2 RKi⇥Ki denote the transition-probability matrix of
appliance i 2 [M ]  and assume that for each state s 2 [Ki]  the energy consumption of the appliance
is constant µi s (µi denotes the corresponding Ki-dimensional column vector (µi 1  . . .   µi Ki)>).
Denoting by xt i 2{ 0  1}Ki the indicator vector of the state st i of appliance i at time t (i.e. 
xt i s = I{st i=s})  the total power consumption at time t isPi2[M ] µ>i xt i  which we assume is
observed with some additive zero mean Gaussian noise of variance 2: yt ⇠N (Pi2[M ] µ>i xt i  2).1

Given this model  the maximum likelihood estimate of the appliance state vector sequence can be
obtained by minimizing the log-posterior function

arg min
xt i

subject to

i=1 x>t iµi)2
22

(yt PM
TXt=1
xt i 2{ 0  1}Ki  1>xt i = 1  i 2 [M ] and t 2 [T ] 

T1Xt=1

x>t i(log Pi)xt+1 i



MXi=1

(1)

1Alternatively  we can assume that the power consumption yt iof each appliance is normally distributed with

mean µ>i xt i and variance 2

i   where 2 =Pi2[M ] 2

i   and yt =Pi2[M ] yt i.

2

where log Pi denotes a matrix obtained from Pi by taking the logarithm of each entry.
In our particular application  in addition to the signal’s temporal structure  large changes in total power
(in comparison to signal noise) contain valuable information that can be used to further improve the
inference results (in fact  solely this information was used for energy disaggregation  e.g.  by Dong
et al.  2012  2013  Figueiredo et al.  2012). This observation was used by Kolter and Jaakkola [2012]
to amend the posterior with a term that tries to match the large signal changes to the possible changes
in the power level when only the state of a single appliance changes.
Formally  let yt = yt+1  yt  µ(i)
m k = µi k  µi m  and deﬁne the matrices Et i 2 RKi⇥Ki
by (Et i)m k = ( yt  µ(i)
diff)  for some constant diff > 0. Intuitively  (Et i)m k is
m k)2/(22
the negative log-likelihood (up to a constant) of observing a change yt in the power level when
appliance i transitions from state m to state k under some zero-mean Gaussian noise with variance
diff. Making the heuristic approximation that the observation noise and this noise are independent
2
(which clearly does not hold under the previous model)  Kolter and Jaakkola [2012] added the term

i=1 x>t iEt ixt+1 i) to the objective of (1)  arriving at

(PT1

t=1 PM

arg min
xt i

(yt PM

TXt=1

f (x1  . . .   xT ) :=

i=1 x>t iµi)2
22

T1Xt=1
MXi=1
subject to xt i 2{ 0  1}Ki  1>xt i = 1  i 2 [M ] and t 2 [T ] .
In the rest of the paper we derive an efﬁcient approximate solution to (2)  and demonstrate that it is
superior to the approximate solution derived by Kolter and Jaakkola [2012] with respect to several
measures quantifying the accuracy of load disaggregation solutions.

x>t i(Et i + log Pi)xt+1 i

(2)



3 SDP Relaxation and Randomized Rounding

There are two major challenges to solve the optimization problem (2) exactly: (i) the optimization is
over binary vectors xt i; and (ii) the objective function f  even when considering its extension to a
convex domain  is in general non-convex (due to the second term). As a remedy we will relax (2) to
make it an integer quadratic programming problem  then apply an SDP relaxation and randomized
rounding to solve approximately the relaxed problem. We start with reviewing the latter methods.

3.1 Approximate Solutions for Integer Quadratic Programming

In this section we consider approximate solutions to the integer quadratic programming problem

minimize
subject to

f (x) = x>Dx + 2d>x
x 2{ 0  1}n 

(3)

+ is positive semideﬁnite  and d 2 Rn. While an exact solution of (3) can be found
where D 2 Sn
by enumerating all possible combination of binary values within a properly chosen box or ellipsoid 
the running time of such exact methods is nearly exponential in the number n of binary variables 
making these methods unﬁt for large scale problems.
One way to avoid exponential running times is to replace (3) with a convex problem with the hope that
the solutions of the convex problems can serve as a good starting point to ﬁnd high-quality solutions
to (3). The standard approach to this is to linearize (3) by introducing a new variable X 2 Sn
+
tied to x trough X = xx>  so that x>Dx = trace(DX)  and then relax the nonconvex constraints
X = xx>  x 2{ 0  1}n to X ⌫ xx>  diag(X) = x  x 2 [0  1]n. This leads to the relaxed SDP
problem

minimize

subject to

trace(D>X) + 2d>x

1 x>
x X ⌫ 0 

diag(X) = x 

x 2 [0  1]n

(4)

3

By introducing ˆX =1 x>

x X this can be written in the compact SDP form

minimize
subject to

trace( ˆD> ˆX)
ˆX ⌫ 0  A ˆX = b .

where ˆD =0 d>

d D 2 Sn+1

+   b 2 Rm and A : Sn

+ ! Rm is an appropriate linear operator. This
general SDP optimization problem can be solved with arbitrary precision in polynomial time using
interior-point methods [Malick et al.  2009  Wen et al.  2010]. As discussed before  this approach
becomes impractical in terms of both the running time and the required memory if either the number
of variables or the optimization constraints are large [Wen et al.  2010]. We will return to the issue of
building scaleable solvers for NILM in Section 5.
Note that introducing the new variable X  the problem is projected into a higher dimensional space 
which is computationally more challenging than just simply relaxing the integrality constraint in (3) 
but leads to a tighter approximation of the optimum (c.f.  Park and Boyd  2015; see also Lovász and
Schrijver  1991  Burer and Vandenbussche  2006).
To obtain a feasible point of (3) from the solution of (5)  we still need to change the solution x to
a binary vector. This can be done via randomized rounding [Park and Boyd  2015  Goemans and
Williamson  1995]: Instead of letting x 2 [0  1]n  the integrality constraint x 2{ 0  1}n in (3) can be
replaced by the inequalities xi(xi  1)  0 for all i 2 [n]. Although these constraints are nonconvex 
they admit an interesting probabilistic interpretation: the optimization problem

minimize
subject to

is equivalent to

Ew⇠N (µ ⌃)[w>Dw + 2d>w]
Ew⇠N (µ ⌃)[wi(wi  1)]  0 

i 2 [n]  µ 2 Rn  ⌃ ⌫ 0

(5)

(6)

minimize
subject to

trace((⌃ + µµ>)D) + 2d>µ
⌃i i + µ2

i  µi  0 

i 2 [n] 

which is in the form of (4) with X =⌃+ µµ> and x = µ (above  Ex⇠P [f (x)] stands for
R f (x)dP (x)). This leads to the rounding procedure: starting from a solution (x⇤  X⇤) of (4) 
we randomly draw several samples w(j) from N (x⇤  X⇤  x⇤x⇤>)  round w(j)
to 0 or 1 to obtain
x(j)  and keep the x(j) with the smallest objective value. In a series of experiments  Park and Boyd
[2015] found this procedure to be better than just naively rounding the coordinates of x⇤.

i

4 An Efﬁcient Algorithm for Inference in FHMMs

To arrive at our method we apply the results of the previous subsection to (2). To do so  as mentioned
at the beginning of the section  we need to change the problem to a convex one  since the elements of
the second term in the objective of (2)  x>t i(Et i + log Pi)xt+1 i are not convex. To address this
issue  we relax the problem by introducing new variables Zt i = xt ix>t+1 i and replace the constraint
Zt i = xt ix>t+1 i with two new ones:

Zt i1 = xt i

and Z>t i1 = xt+1 i.

To simplify the presentation  we will assume that Ki = K for all i 2 [M ]. Then problem (2) becomes

arg min
xt i

subject to

TXt=1⇢ 1

22yt  x>t µ2

 p>t zt

xt 2{ 0  1}M K 
ˆzt 2{ 0  1}M KK 
1>xt i = 1 
Zt i1> = xt i 

t 2 [T ] 
t 2 [T  1] 

t 2 [T ] and i 2 [M ] 
Z>t i1> = xt+1 i  

4

(7)

t 2 [T  1] and i 2 [M ] 

Algorithm 1 ADMM-RR: Randomized rounding algorithm for suboptimal solution to (2)

t

t

:= X⇤t for t = 1  . . .   T

Given: number of iterations: itermax  length of input data: T
Solve the optimization problem (8): Run Algorithm 2 to get X⇤t and z⇤t
:= z⇤t and X best
Set xbest
for t = 2  . . .   T  1 do
t1 >  xbest
Set x := [xbest
Set X := block(X best
arguments
Set f best := 1
Form the covariance matrix ⌃:= X  xxT and ﬁnd its Cholesky factorization LL> =⌃ .
for k = 1  2  . . .   itermax do

t >  xbest
t1   X best

t+1 >]>

t

  X best

t+1 ) where block(· ·) constructs block diagonal matrix from input

Random sampling: zk := x + Lw  where w ⇠N (0  I)
Round zk to the nearest integer point xk that satisﬁes the constraints of (7)
If f best > ft(xk) then update xbest
respectively

and X best

t

t

from the corresponding entries of xk and xkxk> 

end for

end for

where x>t = [x>t 1  . . .   x>t M ]  µ> = [µ>1   . . .   µ>M ]  z>t = [vec(Zt 1)>  . . .   vec(Zt M )>] and
p>t = [vec(Et 1 + log P1)  . . .   vec(log PT )]  with vec(A) denoting the column vector obtained
by concatenating the columns of A for a matrix A. Expanding the ﬁrst term of (7) and following the
relaxation method of Section 3.1  we get the following SDP problem:2

arg min
Xt zt
subject to

TXt=1

trace(D>t Xt) + d>t zt

(8)

+

+

ytµ

BXt + Czt + EXt+1 = g 
Xt  zt  0 .

AXt = b 
Xt ⌫ 0 
! Rm0 and C2 RM KK⇥m0 are all appropriate linear
! Rm  B E : SM K+1
Here A : SM K+1
operators  and the integers m and m0 are determined by the number of equality constraints  while
µµ>  and dt = pt. Notice that (8) is a simple  though huge-dimensional SDP
22 0
ytµ>
Dt = 1
problem in the form of (5) where ˆD has a special block structure.
Next we apply the randomized rounding method from Section 3.1 to provide an approximate solution
to our original problem (2). Starting from an optimal solution (z⇤  X⇤) of (8)   and utilizing that
we have an SDP problem for each time step t  we obtain Algorithm 1 that performs the rounding
sequentially for t = 1  2  . . .   T . However we run the randomized method for three consecutive time
steps  since Xt appears at both time steps t  1 and t + 1 in addition to time t (cf.  equation 9).
Following Park and Boyd [2015]  in the experiments we introduce a simple greedy search within
Algorithm 1: after ﬁnding the initial point xk  we greedily try to objective the target value by change
the status of a single appliance at a single time instant. The search stops when no such improvement
is possible  and we use the resulting point as the estimate.

5 ADMM Solver for Large-Scale  Sparse Block-Structured SDP Problems

Given the relaxation and randomized rounding presented in the previous subsection all that remains
is to ﬁnd X⇤t   z⇤t to initialize Algorithm 1. Although interior point methods can solve SDP problems
efﬁciently  even for problems with sparse constraints as (4)  the running time to obtain an ✏ optimal
solution is of the order of n3.5 log(1/✏) [Nesterov  2004  Section 4.3.3]  which becomes prohibitive
in our case since the number of variables scales linearly with the time horizon T .
As an alternative solution  ﬁrst-order methods can be used for large scale problems [Wen et al.  2010].
Since our problem (8) is an SDP problem where the objective function is separable  ADMM is a
promising candidate to ﬁnd a near-optimal solution. To apply ADMM  we use the Moreau-Yosida
quadratic regularization [Malick et al.  2009]  which is well suited for the primal formulation we

2The only modiﬁcation is that we need to keep the equality constraints in (7) that are missing from (3).

5

Algorithm 2 ADMM for sparse SDPs of the form (8)

Given: length of input data: T   number of iterations: itermax.
Set the initial values to zero. W 0
Set µ = 0.001 {Default step-size value}
for k = 0  1  . . .   itermax do

t   S0 = 0  0

t = 0  ⌫0

t   P 0

for t = 1  2  . . .   T do

t = 0  and r0

t   h0

t = 0

Update P k

t   W k

t   k  Sk

t   rk

t   hk

t   and ⌫k

t   respectively  according to (11) (Appendix A).

end for

end for

consider. When implementing ADMM over the variables (Xt  zt)t  the sparse structure of our
constraints allows to consider the SDP problems for each time step t sequentially:

arg min
Xt zt
subject to

trace(D>t Xt) + d>t zt
AXt = b 
BXt + Czt + EXt+1 = g 
BXt1 + Czt1 + EXt = g 
Xt ⌫ 0 

Xt  zt  0 .

The regularized Lagrangian function for (9) is3

(9)

(10)

Lµ =trace(D>X) + d>z +

1
2µkX  Sk2
+ ⌫>(g B X C z E X+) + ⌫>
 trace(W >X)  trace(P >X)  h>z 

F +

1
2µkz  rk2

2 + >(b A X)

 (g B X C z E X)

where   ⌫  W  0  P ⌫ 0  and h  0 are dual variables  and µ > 0 is a constant. By taking the
derivatives of Lµ and computing the optimal values of X and z  one can derive the standard ADMM
updates  which  due to space constraints  are given in Appendix A. The ﬁnal algorithm  which updates
the variables for each t sequentially  is given by Algorithm 2.
Algorithms 1 and 2 together give an efﬁcient algorithm for ﬁnding an approximate solution to (2) and
thus also to the inference problem of additive FHMMs.

6 Learning the Model

The previous section provided an algorithm to solve the inference part of our energy disaggregation
problem. However  to be able to run the inference method  we need to set up the model. To learn
the HMMs describing each appliance  we use the method of Kontorovich et al. [2013] to learn the
transition matrix  and the spectral learning method of Anandkumar et al. [2012] (following Mattfeld 
2014) to determine the emission parameters.
However  when it comes to the speciﬁc application of NILM  the problem of unknown  time-varying
bias also needs to be addressed  which appears due to the presence of unknown/unmodeled appliances
in the measured signal. A simple idea  which is also followed by Kolter and Jaakkola [2012]  is to
use a “generic model” whose contribution to the objective function is downweighted. Surprisingly 
incorporating this idea in the FHMM inference creates some unexpected challenges.4
Therefore  in this work we come up with a practical  heuristic solution tailored to NILM. First we
identify all electric events deﬁned by a large change yt in the power usage (using some ad-hoc
threshold). Then we discard all events that are similar to any possible level change µ(i)
m k. The
remaining large jumps are regarded as coming from a generic HMM model describing the unregistered
appliances: they are clustered into K  1 clusters  and an HMM model is built where each cluster is
regarded as power usage coming from a single state of the unregistered appliances. We also allow an
“off state” with power usage 0.

3We drop the subscript t and replace t + 1 and t  1 with + and  signs  respectively.
4For example  the incorporation of this generic model breaks the derivation of the algorithm of Kolter and

Jaakkola [2012]. See Appendix B for a discussion of this.

6

7 Experimental Results

We evaluate the performance of our algorithm in two setups:5 we use a synthetic dataset to test
the inference method in a controlled environment  while we used the REDD dataset of Kolter and
Johnson [2011] to see how the method performs on non-simulated  “real” data. The performance of
our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and
Jordan [1997]  the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall
refer to the last two algorithms as KJ and ZGS  respectively.

7.1 Experimental Results: Synthetic Data

The synthetic dataset was generated randomly (the exact procedure is described in Appendix C).
To evaluate the performance  we use normalized disaggregation error as suggested by Kolter and
Jaakkola [2012] and also adopted by Zhong et al. [2014]. This measures the reconstruction error for
each individual appliance. Given the true output yt i and the estimated output ˆyt i (i.e. ˆyt i = µ>i ˆxt i) 
the error measure is deﬁned as

NDE =qPt i(yt i  ˆyt i)2/Pt i (yt i)2 .

Figures 1 and 2 show the performance of the algorithms as the number HMMs (M) (resp.  number of
states  K) is varied. Each plot is a report for T = 1000 steps averaged over 100 random models and
realizations  showing the mean and standard deviation of NDE. Our method  shown under the label
ADMM-RR  runs ADMM for 2500 iterations  runs the local search at the end of each 250 iterations 
and chooses the result that has the maximum likelihood. ADMM is the algorithm which applies naive
rounding. It can be observed that the variational inference method is signiﬁcantly outperformed by
all other methods  while our algorithm consistently obtained better results than its competitors  KJ
coming second and ZGS third.

Number of states: 3; Data length T=1000; Number of samples: 100
5

Number of states: 3; Data length T=1000; Number of samples: 100
1

ADMM-RR
KJ method
ADMM
ZGS method

ADMM-RR
KJ method
ADMM
Variational Approx.
ZGS method

r
o
r
r
e
d
e
z

 

i
l

a
m
r
o
N

0.8

0.6

0.4

0.2

0

-0.2

2

3

4

5

6

7

8

9

2

3

4

5

6

7

8

9

Figure 1: Disaggregation error varying the number of HMMs.

r
o
r
r
e
d
e
z

 

i
l

a
m
r
o
N

4

3

2

1

0

-1

3

r
o
r
r
e
 
d
e
z

i
l

a
m
r
o
N

2.5

2

1.5

1

0.5

0

-0.5

Number of appliances: 5; Data length T=1000; Number of samples: 100

ADMM-RR
KJ method
ADMM
Variational Approx.
ZGS method

Number of appliances: 5; Data length T=1000; Number of samples: 100
0.8

ADMM-RR
KJ method
ADMM
ZGS method

r
o
r
r
e

 

d
e
z

i
l

a
m
r
o
N

0.6

0.4

0.2

0

-0.2

2

3

4

5

6

2

3

4

5

6

Figure 2: Disaggregation error varying the number of states.

7.2 Experimental Results: Real Data

In this section  we also compared the 3 best methods on the real dataset REDD [Kolter and Johnson 
2011]. We use the ﬁrst half of the data for training and the second half for testing. Each HMM (i.e. 

5Our code is available online at https://github.com/kiarashshaloudegi/FHMM_inference.

7

Appliance
1 Oven-3
2 Fridge
3 Microwave
4 Bath. GFI-12
5 Kitch. Out.-15
6 Wash./Dry.-20-A
7 Unregistered-A
8 Oven-4
9 Dishwasher-6
10 Wash./Dryer-10
11 Kitch. Out.-16
12 Wash./Dry.-20-B
13 Unregistered-B
Average

KJ method

ZGS method
5.35/15.04%
46.89/87.10%

ADMM-RR
61.70/78.30% 27.62/72.32%
90.22/97.63% 41.20/97.46%
12.40/74.74%
50.88/60.25% 12.87/51.46%
69.23/98.85% 16.66/79.47%
70.41/98.19%
98.23/93.80%
85.35/25.91%
94.27/87.80%
25.41/76.37%
13.60/78.59%
54.53/90.91%
25.20/98.72%
21.92/63.58% 18.63/25.79%
8.87/100%
17.88/79.04%
72.13/77.10%
98.19/28.31%
97.78/91.73%
96.92/73.97%
60.97/78.56% 38.68/75.02%

13.40/96.32% 4.55/45.07%
6.16/42.67%
5.69/26.72%
15.91/35.51%
57.43/99.31%
9.52/12.05%
29.42/31.01%
7.79/3.01%
0.00/0.00%
27.44/71.25%
33.63/99.98%
17.97/36.22%

Table 1: Comparing the disaggregation performance of three different algorithms: precision/recall.
Bold numbers represent statistically better performance on both measures.

appliance) is trained separately using the associated circuit level data  and the HMM corresponding
to unregistered appliances is trained using the main panel data. In this set of experiments we monitor
appliances consuming more than 100 watts. ADMM-RR is run for 1000 iterations  and the local
search is run at the end of each 250 iterations  and the result with the largest likelihood is chosen.
To be able to use the ZGS method on this data  we need to have some prior information about the
usage of each appliance; the authors suggestion is to us national energy surveys  but in the lack of
this information (also about the number of residents  type of houses  etc.) we used the training data to
extract this prior knowledge  which is expected to help this method.
Detailed results about the precision and recall of estimating which appliances are ‘on’ at any given
time are given in Table 1. In Appendix D we also report the error of the total power usage assigned
to different appliances (Table 2)  as well as the amount of assigned power to each appliance as
a percentage of total power (Figure 3). As a summary  we can see that our method consistently
outperformed the others  achieving an average precision and recall of 60.97% and 78.56%  with about
50% better precision than KJ with essentially the same recall (38.68/75.02%)  while signiﬁcantly
improving upon ZGS (17.97/36.22%). Considering the error in assigning the power consumption to
different appliances  our method achieved about 30  35% smaller error (ADMM-RR: 2.87%  KJ:
4.44%  ZGS: 3.94%) than its competitors.
In our real-data experiments  there are about 1 million decision variables: M = 7 or 6 appliances
(for phase A and B power  respectively) with K = 4 states each and for about T = 30  000 time
steps for one day  1 sample every 6 seconds. KJ and ZGS solve quadratic programs  increasing their
memory usage (14GB vs 6GB in our case). On the other hand  our implementation of their method 
using the commercial solver MOSEK inside the Matlab-based YALMIP [Löfberg  2004]  runs in 5
minutes  while our algorithm  which is purely Matlab-based takes 5 hours to ﬁnish. We expect that an
optimized C++ version of our method could achieve a signiﬁcant speed-up compared to our current
implementation.

8 Conclusion

FHMMs are widely used in energy disaggregation. However  the resulting model has a huge
(factored) state space  making standard inference FHMM algorithms infeasible even for only a
handful of appliances. In this paper we developed a scalable approximate inference algorithm  based
on a semideﬁnite relaxation combined with randomized rounding  which signiﬁcantly outperformed
the state of the art in our experiments. A crucial component of our solution is a scalable ADMM
method that utilizes the special block-diagonal-like structure of the SDP relaxation and provides a
good initialization for randomized rounding. We expect that our method may prove useful in solving
other FHMM inference problems  as well as in large scale integer quadratic programming.

Acknowledgements

This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity
Centre for Machine Learning and by NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian 
whom provided much useful technical advise  while all authors are grateful for Zico Kolter for sharing his code.

8

References
A. Anandkumar  D. Hsu  and S. M. Kakade. A Method of Moments for Mixture Models and Hidden Markov

Models. In COLT  volume 23  pages 33.1–33.34  2012.

S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed Optimization and Statistical Learning via

the Alternating Direction Method of Multipliers. FTML  3(1):1–122  2011.

S. Burer and D. Vandenbussche. Solving Lift-and-Project Relaxations of Binary Integer Programs. SIAM Journal

on Optimization  16(3):726–750  2006.

H.-H. Chang  K.-L. Chen  Y.-P. Tsai  and W.-J. Lee. A New Measurement Method for Power Signatures of
Nonintrusive Demand Monitoring and Load Identiﬁcation. IEEE T. on Industry Applications  48:764–771 
2012.

M. Dong  P. C. M. Meira  W. Xu  and W. Freitas. An Event Window Based Load Monitoring Technique for

Smart Meters. IEEE Transactions on Smart Grid  3(2):787–796  June 2012.

M. Dong  Meira  W. Xu  and C. Y. Chung. Non-Intrusive Signature Extraction for Major Residential Loads.

IEEE Transactions on Smart Grid  4(3):1421–1430  Sept. 2013.

D. Egarter  V. P. Bhuvana  and W. Elmenreich. PALDi: Online Load Disaggregation via Particle Filtering. IEEE

Transactions on Instrumentation and Measurement  64(2):467–477  2015.

M. Figueiredo  A. de Almeida  and B. Ribeiro. Home Electrical Signal Disaggregation for Non-intrusive Load

Monitoring (NILM) Systems. Neurocomputing  96:66–73  Nov. 2012.

Z. Ghahramani and M. Jordan. Factorial Hidden Markov Models. Machine learning  29(2):245–273  1997.
M. X. Goemans and D. P. Williamson. Improved Approximation Algorithms for Maximum Cut and Satisﬁability

Problems Using Semideﬁnite Programming. J. of the ACM  42(6):1115–1145  1995.

Z. Guo  Z. J. Wang  and A. Kashani. Home Appliance Load Modeling From Aggregated Smart Meter Data.

IEEE Transactions on Power Systems  30(1):254–262  Jan. 2015.

J. Kelly and W. Knottenbelt. Neural NILM: Deep Neural Networks Applied to Energy Disaggregation. In

BuildSys  pages 55–64  2015.

H. Kim  M. Marwah  M. F. Arlitt  G. Lyon  and J. Han. Unsupervised Disaggregation of Low Frequency Power

Measurements. In ICDM  volume 11  pages 747–758  2011.

D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. Adaptive computation

and machine learning. MIT Press  Cambridge  MA  2009.

J. Z. Kolter and T. Jaakkola. Approximate Inference in Additive Factorial HMMs with Application to Energy

Disaggregation. In AISTATS  pages 1472–1482  2012.

J. Z. Kolter and M. J. Johnson. REDD: A Public Data Set for Energy Disaggregation Research. In Workshop on

Data Mining Applications in Sustainability (SIGKDD)  pages 59–62  2011.

J. Z. Kolter  S. Batra  and A. Y. Ng. Energy Disaggregation via Discriminative Sparse Coding. In Advances in

Neural Information Processing Systems  pages 1153–1161  2010.

A. Kontorovich  B. Nadler  and R. Weiss. On Learning Parametric-Output HMMs. In ICML  pages 702–710 

J. Liang  S. K. K. Ng  G. Kendall  and J. W. M. Cheng. Load Signature Study -Part I: Basic Concept  Structure 

and Methodology. IEEE Transactions on Power Delivery  25(2):551–560  Apr. 2010.

J. Löfberg. YALMIP : A Toolbox for Modeling and Optimization in MATLAB. In CACSD  2004.
L. Lovász and A. Schrijver. Cones of Matrices and Set-functions and 0-1 Optimization. SIAM Journal on

Optimization  1(2):166–190  1991.

J. Malick  J. Povh  F. Rendl  and A. Wiegele. Regularization Methods for Semideﬁnite Programming. SIAM

Journal on Optimization  20(1):336–356  Jan. 2009. ISSN 1052-6234  1095-7189.

C. Mattfeld. Implementing spectral methods for hidden Markov models with real-valued emissions. arXiv

2013.

Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  2004.
J. Park and S. Boyd. A Semideﬁnite Programming Method for Integer Convex Quadratic Minimization. arXiv

preprint arXiv:1404.7472  2014.

preprint arXiv:1504.07672  2015.

A. Prudenzi. A neuron nets based procedure for identifying domestic appliances pattern-of-use from energy

recordings at meter panel. In PESW  volume 2  pages 941–946  2002.

S. J. Rennie  J. R. Hershey  and P. Olsen. Single-channel speech separation and recognition using loopy belief

propagation. In ICASSP  pages 3845–3848  2009.

M. J. Wainwright and M. I. Jordan. Graphical Models  Exponential Families  and Variational Inference. FTML 

M. Weiss  A. Helfenstein  F. Mattern  and T. Staake. Leveraging smart meter data to recognize home appliances.

1(1–2):1–305  2007.

In PerCom  pages 190–197  2012.

Z. Wen  D. Goldfarb  and W. Yin. Alternating direction augmented Lagrangian methods for semideﬁnite

programming. Mathematical Programming Computation  2(3-4):203–230  Dec. 2010.

M. Zhong  N. Goddard  and C. Sutton. Signal Aggregate Constraints in Additive Factorial HMMs  with

Application to Energy Disaggregation. In NIPS  pages 3590–3598  2014.

T. Zia  D. Bruckner  and A. Zaidi. A hidden Markov model based procedure for identifying household electric

loads. In IECON  pages 3218–3223  2011.

9

,Kiarash Shaloudegi
András György
Csaba Szepesvari
Wilsun Xu
Abhishek Gupta
Russell Mendonca
YuXuan Liu
Pieter Abbeel
Sergey Levine