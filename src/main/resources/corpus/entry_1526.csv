2018,Disconnected Manifold Learning for Generative Adversarial Networks,Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold  and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work  we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold  and investigate how sample quality  mode dropping and local convergence are affected by this. Next  we show how using a collection of generators can address this problem  providing new insights into the success of such multi-generator GANs. Finally  we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs  its consequences in practice  and the effectiveness of our proposed modifications in alleviating these issues.,Disconnected Manifold Learning for Generative

Adversarial Networks

Mahyar Khayatkhoei

Department of Computer Science

Rutgers University

m.khayatkhoei@cs.rutgers.edu

Ahmed Elgammal

Department of Computer Science

Rutgers University

elgammal@cs.rutgers.edu

Maneesh Singh
Verisk Analytics

maneesh.singh@verisk.com

Abstract

Natural images may lie on a union of disjoint manifolds rather than one globally
connected manifold  and this can cause several difﬁculties for the training of
common Generative Adversarial Networks (GANs). In this work  we ﬁrst show
that single generator GANs are unable to correctly model a distribution supported
on a disconnected manifold  and investigate how sample quality  mode dropping
and local convergence are affected by this. Next  we show how using a collection of
generators can address this problem  providing new insights into the success of such
multi-generator GANs. Finally  we explain the serious issues caused by considering
a ﬁxed prior over the collection of generators and propose a novel approach for
learning the prior and inferring the necessary number of generators without any
supervision. Our proposed modiﬁcations can be applied on top of any other GAN
model to enable learning of distributions supported on disconnected manifolds. We
conduct several experiments to illustrate the aforementioned shortcoming of GANs 
its consequences in practice  and the effectiveness of our proposed modiﬁcations in
alleviating these issues.

1

Introduction

Consider two natural images  picture of a bird and picture of a cat for example  can we continuously
transform the bird into the cat without ever generating a picture that is not neither bird nor cat? In
other words  is there a continuous transformation between the two that never leaves the manifold of
"real looking" images? It is often the case that real world data falls on a union of several disjoint
manifolds and such a transformation does not exist  i.e. the real data distribution is supported on a
disconnected manifold  and an effective generative model needs to be able to learn such manifolds.
Generative Adversarial Networks (GANs) [10]  model the problem of ﬁnding the unknown distribu-
tion of real data as a two player game where one player  called the discriminator  tries to perfectly
separate real data from the data generated by a second player  called the generator  while the second
player tries to generate data that can perfectly fool the ﬁrst player. Under certain conditions  Good-
fellow et al. [10] proved that this process will result in a generator that generates data from the real
data distribution  hence ﬁnding the unknown distribution implicitly. However  later works uncovered
several shortcomings of the original formulation  mostly due to violation of one or several of its
assumptions in practice [1  2  20  24]. Most notably  the proof only works for when optimizing in
the function space of generator and discriminator (and not in the parameter space) [10]  the Jensen
Shannon Divergence is maxed out when the generated and real data distributions have disjoint support

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

resulting in vanishing or unstable gradient [1]  and ﬁnally the mode dropping problem where the
generator fails to correctly capture all the modes of the data distribution  for which to the best of our
knowledge there is no deﬁnitive reason yet.
One major assumption for the convergence of GANs is that the generator and discriminator both
have unlimited capacity [10  2  24  14]  and modeling them with neural networks is then justiﬁed
through the Universal Approximation Theorem. However  we should note that this theorem is only
valid for continuous functions. Moreover  neural networks are far from universal approximators in
practice. In fact  we often explicitly restrict neural networks through various regularizers to stabilize
training and enhance generalization. Therefore  when generator and discriminator are modeled by
stable regularized neural networks  they may no longer enjoy a good convergence as promised by the
theory.
In this work  we focus on learning distributions with disconnected support  and show how limitations
of neural networks in modeling discontinuous functions can cause difﬁculties in learning such
distributions with GANs. We study why these difﬁculties arise  what consequences they have in
practice  and how one can address these difﬁculties by using a collection of generators  providing
new insights into the recent success of multi-generator models. However  while all such models
consider the number of generators and the prior over them as ﬁxed hyperparameters [3  14  9]  we
propose a novel prior learning approach and show its necessity in effectively learning a distribution
with disconnected support. We would like to stress that we are not trying to achieve state of the art
performance in our experiments in the present work  rather we try to illustrate an important limitation
of common GAN models and the effectiveness of our proposed modiﬁcations. We summarize the
contributions of this work below:

insights into the success of multi generator GAN models in practice (Section 3).

• We identify a shortcoming of GANs in modeling distributions with disconnected support 
and investigate its consequences  namely mode dropping  worse sample quality  and worse
local convergence (Section 2).
• We illustrate how using a collection of generators can solve this shortcoming  providing new
• We show that choosing the number of generators and the probability of selecting them are
important factors in correctly learning a distribution with disconnected support  and propose
a novel prior learning approach to address these factors. (Section 3.1)
• Our proposed model can effectively learn distributions with disconnected supports and infer
the number of necessary disjoint components through prior learning. Instead of one large
neural network as the generator  it uses several smaller neural networks  making it more
suitable for parallel learning and less prone to bad weight initialization. Moreover  it can be
easily integrated with any GAN model to enjoy their beneﬁts as well (Section 5).

2 Difﬁculties of Learning Disconnected Manifolds

A GAN as proposed by Goodfellow et al. [10]  and most of its successors (e.g. [2  11]) learn a
continuous G : Z → X   which receives samples from some prior p(z) as input and generates real
data as output. The prior p(z) is often a standard multivariate normal distribution N (0  I) or a
bounded uniform distribution U(−1  1). This means that p(z) is supported on a globally connected
subspace of Z. Since a continuous function always keeps the connectedness of space intact [15] 
the probability distribution induced by G is also supported on a globally connected space. Thus G 
a continuous function by design  can not correctly model a union of disjoint manifolds in X . We
highlight this fact in Figure 1 using an illustrative example where the support of real data is {+2 −2}.
We will look at some consequences of this shortcoming in the next part of this section. For the
remainder of this paper  we assume the real data is supported on a manifold Sr which is a union of
disjoint globally connected manifolds each denoted by Mi; we refer to each Mi as a submanifold
(note that we are overloading the topological deﬁnition of submanifolds in favor of brevity):

nr(cid:91)

Sr =

Mi

∀i (cid:54)= j : Mi ∩ Mj = ∅

i=1

Sample Quality. Since GAN’s generator tries to cover all submanifolds of real data with a single
globally connected manifold  it will inevitably generate off real-manifold samples. Note that to avoid

2

(a) Suboptimal Continuous G

(b) Optimal G∗

Figure 1: Illustrative example of continuous generator G(z) : Z → X with prior z ∼ U(−1  1) 
2 (δ(x − 2) + δ(x + 2))  a distribution supported on
trying to capture real data coming from p(x) = 1
union of two disjoint manifolds. (a) shows an example of what a stable neural network is capable of
learning for G (a continuous and smooth function)  (b) shows an optimal generator G∗(z). Note that
since z is uniformly sampled  G(z) is necessarily generating off manifold samples (in [−2  2]) due to
its continuity.

off manifold regions  one should push the generator to learn a higher frequency function  the learning
of which is explicitly avoided by stable training procedures and means of regularization. Therefore
the GAN model in a stable training  in addition to real looking samples  will also generate low quality
off real-manifold samples. See Figure 2 for an example of this problem.
Mode Dropping. In this work  we use the term mode dropping to refer to the situation where one
or several submanifolds of real data are not completely covered by the support of the generated
distribution. Note that mode collapse is a special case of this deﬁnition where all but a small part of
a single submanifold are dropped. When the generator can only learn a distribution with globally
connected support  it has to learn a cover of the real data submanifolds  in other words  the generator
can not reduce the probability density of the off real-manifold space beyond a certain value. However 
the generator can try to minimize the volume of the off real-manifold space to minimize the probability
of generating samples there. For example  see how in Figure 2b the learned globally connected
manifold has minimum off real-manifold volume  for example it does not learn a cover that crosses
the center (the same manifold is learned in 5 different runs). So  in learning the cover  there is a trade
off between covering all real data submanifolds  and minimizing the volume of the off real-manifold
space in the cover. This trade off means that the generator may sacriﬁce certain submanifolds  entirely
or partially  in favor of learning a cover with less off real-manifold volume  hence mode dropping.
Local Convergence. Nagarajan and Kolter [21] recently proved that the training of GANs is
locally convergent when generated and real data distributions are equal near the equilibrium point 
and Mescheder et al. [19] showed the necessity of this condition on a prototypical example. Therefore
when the generator can not learn the correct support of the real data distribution  as is in our discussion 
the resulting equilibrium may not be locally convergent. In practice  this means the generator’s support
keeps oscillating near the data manifold.

3 Disconnected Manifold Learning
There are two ways to achieve disconnectedness in X : making Z disconnected  or making G :
Z → X discontinuous. The former needs considerations for how to make Z disconnected  for
example adding discrete dimensions [6]  or using a mixture of Gaussians [12]. The latter solution
can be achieved by introducing a collections of independent neural networks as G. In this work  we
investigate the latter solution since it is more suitable for parallel optimization and can be more robust
to bad initialization.
We ﬁrst introduce a set of generators Gc : Z → X instead of a single one  independently constructed
on a uniform prior in the shared latent space Z. Each generator can therefore potentially learn a
separate connected manifold. However  we need to encourage these generators to each focus on a
different submanifold of the real data  otherwise they may all learn a cover of the submanifolds and

3

+2-2+1-1𝒵𝒳+2-2+1-1𝒵𝒳(a) Real Data

(b) WGAN-GP

(c) DMWGAN

(d) DMWGAN-PL

Figure 2: Comparing Wasserstein GAN (WGAN) and its Disconnected Manifold version with
and without prior learning (DMWGAN-PL  DMWGAN) on disjoint line segments dataset when
ng = 10. Different colors indicate samples from different generators. Notice how WGAN-GP fails to
capture the disconnected manifold of real data  learning a globally connected cover instead  and thus
generating off real-manifold samples. DMWGAN also fails due to incorrect number of generators.
In contrast  DMWGAN-PL is able to infer the necessary number of disjoint components without
any supervision and learn the correct disconnected manifold of real data. Each ﬁgure shows 10K
samples from the respective model. We train each model 5 times  the results shown are consistent
across different runs.

(a) WGAN-GP

(b) DMWGAN

(c) DMWGAN-PL

Figure 3: Comparing WGAN-GP  DMWGAN and DMWGAN-PL convergence on unbalanced
disjoint line segments dataset when ng = 10. The real data is the same line segments as in Figure 2 
except the top right line segment has higher probability. Different colors indicate samples from
different generators. Notice how DMWGAN-PL (c) has vanished the contribution of redundant
generators wihtout any supervision. Each ﬁgure shows 10K samples from the respective model. We
train each model 5 times  the results shown are consistent across different runs.

experience the same issues of a single generator GAN. Intuitively  we want the samples generated
by each generator to be perfectly unique to that generator  in other words  each sample should be
a perfect indicator of which generator it came from. Naturally  we can achieve this by maximizing
the mutual information I(c; x)  where c is generator id and x is generated sample. As suggested
by Chen et al. [6]  we can implement this by maximizing a lower bound on mutual information
between generator ids and generated images:

I(c; x) = H(c) − H(c|x)

(cid:2)Ec(cid:48)∼p(c(cid:48)|x) [ln p(c(cid:48)|x)](cid:3)

= H(c) + Ec∼p(c) x∼pg(x|c)
= H(c) + Ex∼pg(x) [KL(p(c|x)||q(c|x))] + Ec∼p(c) x∼pg(x|c) c(cid:48)∼p(c(cid:48)|x) [ln q(c(cid:48)|x)]
≥ H(c) + Ec∼p(c) x∼pg(x|c) c(cid:48)∼p(c(cid:48)|x) [ln q(c(cid:48)|x)]
= H(c) + Ec∼p(c) x∼pg(x|c) [ln q(c|x)]

where q(c|x) is the distribution approximating p(c|x)  pg(x|c) is induced by each generator Gc 
KL is the Kullback Leibler divergence  and the last equality is a consequence of Lemma 5.1 in [6].
Therefore  by modeling q(c|x) with a neural network Q(x; γ)  the encoder network  maximizing
I(c; x) boils down to minimizing a cross entropy loss:

Lc = −Ec∼p(c) x∼pg(x|c) [ln q(c|x)]

(1)

4

Utilizing the Wasserstein GAN [2] objectives  discriminator (critic) and generator maximize the
following  where D(x; w) : X → R is the critic function:

Vd = Ex∼pr(x) [D(x; w)] − Ec∼p(c) x∼pg(x|c) [D(x; w)]
Vg = Ec∼p(c) x∼pg(x|c) [D(x; w)] − λLc

(2)
(3)
We call this model Disconnected Manifold Learning WGAN (DMWGAN) in our experiments. We
can similarly apply our modiﬁcations to the original GAN [10] to construct DMGAN. We add the
single sided version of penalty gradient regularizer [11] to the discriminator/critic objectives of both
models and all baselines. See Appendix A for details of our algorithm and the DMGAN objectives.
See Appendix F for more details and experiments on the importance of the mutual information term.
The original convergence theorems of Goodfellow et al. [10] and Arjovsky et al. [2] holds for the
proposed DM versions respectively  because all our modiﬁcations concern the internal structure of the
generator  and can be absorbed into the unlimited capacity assumption. More concretely  all generators
together can be viewed as a uniﬁed generator where p(c)pg(x|c) becomes the generator probability 
and Lc can be considered as a constraint on the generator function space incorporated using a
Lagrange multiplier. While most multi-generator models consider p(c) as a uniform distribution over
generators  this naive choice of prior can cause certain difﬁculties in learning a disconnected support.
We will discuss this point  and also introduce and motivate the metrics we use for evaluations  in the
next two subsections.

3.1 Learning the Generator’s Prior

In practice  we can not assume that the true number of submanifolds in real data is known a priori.
So let us consider two cases regarding the number of generators ng  compared to the true number of
submanifolds in data nr  under a ﬁxed uniform prior p(c). If ng < nr then some generators have to
cover several submanifolds of the real data  thus partially experiencing the same issues discussed in
Section 2. If ng > nr  then some generators have to share one real submanifold  and since we are
forcing the generators to maintain disjoint supports  this results in partially covered real submanifolds 
causing mode dropping. See Figures 2c and 3b for examples of this issue. Note that an effective
solution to the latter problem reduces the former problem into a trade off: the more the generators 
the better the cover. We can address the latter problem by learning the prior p(c) such that it vanishes
the contribution of redundant generators. Even when ng = nr  what if the distribution of data over
submanifolds are not uniform? Since we are forcing each generator to learn a different submanifold 
a uniform prior over the generators would result in a suboptimal distribution. This issue further shows
the necessity of learning the prior over generators.
We are interested in ﬁnding the best prior p(c) over generators. Notice that q(c|x) is implicitly
learning the probability of x ∈ X belonging to each generator Gc  hence q(c|x) is approximating
the true posterior p(c|x). We can take an EM approach to learning the prior: the expected value of
q(c|x) over the real data distribution gives us an approximation of p(c) (E step)  which we can use to
train the DMGAN model (M step). Instead of using empirical average to learn p(c) directly  we learn
it with a model r(c; ζ)  which is a softmax function over parameters {ζi}ng
i=1 corresponding to each
generator. This enables us to control the learning of p(c)  the advantage of which we will discuss
shortly. We train r(c) by minimizing the cross entropy as follows:
H(p(c)  r(c)) = −Ec∼p(c) [log r(c)] = −Ex∼pr(x) c∼p(c|x) [log r(c)] = Ex∼pr(x) [H(p(c|x)  r(c))]
Where H(p(c|x)  r(c)) is the cross entropy between model distribution r(c) and true posterior p(c|x)
which we approximate by q(c|x). However  learning the prior from the start  when the generators
are still mostly random  may prevent most generators from learning by vanishing their probability
too early. To avoid this problem  we add an entropy regularizer and decay its weight λ(cid:48)(cid:48) with time to
gradually shift the prior r(c) away from uniform distribution. Thus the ﬁnal loss for training r(c)
becomes:

Lprior = Ex∼pr(x) [H(q(c|x)  r(c))] − αtλ(cid:48)(cid:48)H(r(c))

(4)
Where H(r(c)) is the entropy of model distribution  α is the decay rate  and t is training timestep.
The model is not very sensitive to λ(cid:48)(cid:48) and α  any combination that insures a smooth transition away
from uniform distribution is valid. We call this augmented model Disconnected Manifold Learning
GAN with Prior Learning (DMGAN-PL) in our experiments. See Figures 2 and 3 for examples
showing the advantage of learning the prior.

5

3.2 Choice of Metrics

We require metrics that can assess inter-mode variation  intra-mode variation and sample quality. The
common metric  Inception Score [23]  has several drawbacks [4  18]  most notably it is indifferent to
intra-class variations and favors generators that achieve close to uniform distribution over classes of
data. Instead  we consider more direct metrics together with FID score [13] for natural images.
For inter mode variation  we use the Jensen Shannon Divergence (JSD) between the class distribution
of a pre-trained classiﬁer over real data and generator’s data. This can directly tell us how well the
distribution over classes are captured. JSD is favorable to KL due to being bounded and symmetric.
For intra mode variation  we deﬁne mean square geodesic distance (MSD): the average squared
geodesic distance between pairs of samples classiﬁed into each class. To compute the geodesic
distance  Euclidean distance is used in a small neighborhood of each sample to construct the Isomap
graph [26] over which a shortest path distance is calculated. This shortest path distance is an
approximation to the geodesic distance on the true image manifold [25]. Note that average square
distance  for Euclidean distance  is equal to twice the trace of the Covariance matrix  i.e. sum of the
eigenvalues of covariance matrix  and therefore can be an indicator of the variance within each class:

(cid:2)xT x(cid:3) − 2Ex [x]T Ex [x] = 2T r(Cov(x))

(cid:2)||x − y||2(cid:3) = 2Ex

Ex y

In our experiments  we choose the smallest k for which the constructed k nearest neighbors graph
(Isomap) is connected in order to have a better approximation of the geodesic distance (k = 18).
Another concept we would like to evaluate is sample quality. Given a pretrained classiﬁer with small
test error  samples that are classiﬁed with high conﬁdence can be reasonably considered good quality
samples. We plot the ratio of samples classiﬁed with conﬁdence greater than a threshold  versus the
conﬁdence threshold  as a measure of sample quality: the more off real-manifold samples  the lower
the resulting curve. Note that the results from this plot are exclusively indicative of sample quality
and should be considered in conjunction with the aforementioned metrics.
What if the generative model memorizes the dataset that it is trained on? Such a model would
score perfectly on all our metrics  while providing no generalization at all. First  note that a single
generator GAN model can not memorize the dataset because it can not learn a distribution supported
on N disjoint components as discussed in Section 2. Second  while our modiﬁcations introduces
disconnnectedness to GANs  the number of generators we use in our proposed modiﬁcations are in
the order of data submanifolds which is several orders of magnitude less than common dataset sizes.
Note that if we were to assign one unique point of the Z space to each dataset sample  then a neural
network could learn to memorize the dataset by mapping each selected z ∈ Z to its corresponding
real sample (we have introduced N disjoint component in Z space in this case)  however this is not
how GANs are modeled. Therefore  the memorization issue is not of concern for common GANs
and our proposed models (note that this argument is addressing the very narrow case of dataset
memorization  not over-ﬁtting in general).

4 Related Works

(cid:2)||z − Fθ(Gγ(z))||2

2

(cid:3). The main advantage of these models is to prevent

Several recent works have directly targeted the mode collapse problem by introducing a network
F : X → Z that is trained to map back the data into the latent space prior p(z). It can therefore
provide a learning signal if the generated data has collapsed. ALI [8] and BiGAN [7] consider pairs
of data and corresponding latent variable (x  z)  and construct their discriminator to distinguish such
pairs of real and generated data. VEEGAN [24] uses the same discriminator  but also adds an explicit
reconstruction loss Ez∼p(z)
loss of information by the generator (mapping several z ∈ Z to a single x ∈ X ). However  in case of
distributions with disconnected support  these models do not provide much advantage over common
GANs and suffer from the same issues we discussed in Section 2 due to having a single generator.
Another set of recent works have proposed using multiple generators in GANs in order to improve
their convergence. MIX+GAN [3] proposes using a collection of generators based on the well-known
advantage of learning a mixed strategy versus a pure strategy in game theory. MGAN [14] similarly
uses a collection of k generators in order to model a mixture distribution  and train them together
with a k-class classiﬁer to encourage them to each capture a different component of the real mixture
distribution. MAD-GAN [9]  also uses k generators  together with a k + 1-class discriminator which
is trained to correctly classify samples from each generator and true data (hence a k + 1 classiﬁer) 

6

JSD MNIST ×10−2
Model
0.13 std 0.05
WGAN-GP
0.17 std 0.08
MIX+GAN
DMWGAN
0.23 std 0.06
DMWGAN-PL 0.06 std 0.02

JSD Face-Bed ×10−4
0.23 std 0.15
0.83 std 0.57
0.46 std 0.25
0.10 std 0.05

FID Face-Bed
8.30 std 0.27
8.02 std 0.14
7.96 std 0.08
7.67 std 0.16

Table 1: Inter-class variation measured by Jensen Shannon Divergence (JSD) with true class distri-
bution for MNIST and Face-Bedroom dataset  and FID score for Face-Bedroom (smaller is better).
We run each model 5 times with random initialization  and report average values with one standard
deviation interval

in order to increase the diversity of generated images. While these models provide reasons for why
multiple generators can model mixture distributions and achieve more diversity  they do not address
why single generator GANs fail to do so. In this work  we explain why it is the disconnectedness of
the support that single generator GANs are unable to learn  not the fact that real data comes from a
mixture distribution. Moreover  all of these works use a ﬁxed number of generators and do not have
any prior learning  which can cause serious problems in learning of distributions with disconnected
support as we discussed in Section 3.1 (see Figures 2c and 3b for examples of this issue).
Finally  several works have targeted the problem of learning the correct manifold of data. MDGAN [5] 
uses a two step approach to closely capture the manifold of real data. They ﬁrst approximate the
data manifold by learning a transformation from encoded real images into real looking images 
and then train a single generator GAN to generate images similar to the transformed encoded
images of previous step. However  MDGAN can not model distributions with disconnected supports.
InfoGAN [6] introduces auxiliary dimensions to the latent space Z  and maximizes the mutual
information between these extra dimensions and generated images in order to learn disentangled
representations in the latent space. DeLiGAN [12] uses a ﬁxed mixture of Gaussians as its latent
prior  and does not have any mechanisms to encourage diversity. While InfoGAN and DeLiGAN can
generate disconnected manifolds  they both assume a ﬁxed number of discreet components equal to
the number of underlying classes and have no prior learning over these components  thus suffering
from the issues discussed in Section 3.1. Also  neither of these works discusses the incapability of
single generator GANs to learn disconnected manifolds and its consequences.

5 Experiments

In this section we present several experiments to investigate the issues and proposed solutions men-
tioned in Sections 2 and 3 respectively. The same network architecture is used for the discriminator
and generator networks of all models under comparison  except we use 1
4 number of ﬁlters in each
layer of multi-generator models compared to the single generator models  to control the effect of
complexity. In all experiments  we train each model for a total of 200 epochs with a ﬁve to one update
ratio between discriminator and generator. Q  the encoder network  is built on top of discriminator’s
last hidden layer  and is trained simultaneously with generators. Each data batch is constructed
by ﬁrst selecting 32 generators according to the prior r(c; ζ)  and then sampling each one using
z ∼ U(−1  1). See Appendix B for details of our networks and the hyperparameters.
Disjoint line segments. This dataset is constructed by sampling data with uniform distribution over
four disjoint line segments to achieve a distribution supported on a union of disjoint low-dimensional
manifolds. See Figure 2 for the results of experiments on this dataset. In Figure 3  an unbalanced
version of this dataset is used  where 0.7 probability is placed on the top right line segment  and the
other segments have 0.1 probability each. The generator and discriminator are both MLPs with two
hidden layers  and 10 generators are used for multi-generator models. We choose WGAN-GP as the
state of the art GAN model in these experiments (we observed similar or worse convergence with
other ﬂavors of single generator GANs). MGAN achieves similar results to DMWGAN.
MNIST dataset. MNIST [16] is particularly suitable since samples with different class labels can
be reasonably interpreted as lying on disjoint manifolds (with minor exceptions like certain 4s and
9s). The generator and discriminator are DCGAN like networks [22] with three convolution layers.
Figure 4 shows the mean squared geodesic distance (MSD) and Table 1 reports the corresponding

7

(a) Intra-class variation MNIST

(b) Sample quality MNIST

(c) Sample quality Face-Bed

Figure 4: (a) Shows intra-class variation in MNIST. Bars show the mean square distance (MSD)
within each class of the dataset. On average  DMGAN-PL outperforms WGAN-GP in capturing
intra class variation  as measured by MSD  with larger signiﬁcance on certain classes. (b) Shows the
sample quality in MNIST experiment. (c) Shows sample quality in Face-Bed experiment. Notice
how DMWGAN-PL outperforms other models due to fewer off real-manifold samples. We run each
model 5 times with random initialization  and report average values with one standard deviation
intervals in both ﬁgures. 10K samples are used for metric evaluations.

(a) WGAN-GP

(b) DMWGAN

(c) DMWGAN-PL

Figure 5: Samples randomly generated by GAN models trained on Face-Bed dataset. Notice how
WGAN-GP generates combined face-bedroom images (red boxes) in addition to faces and bedrooms 
due to learning a connected cover of the real data support. DMWGAN does not generate such
samples  however it generates completely off manifold samples (red boxes) due to having redundant
generators and a ﬁxed prior. DMWGAN-PL is able to correctly learn the disconnected support of
real data. The samples and trained models are not cherry picked.

divergences in order to compare their inter mode variation. 20 generators are used for multi-generator
models. See Appendix C for experiments using modiﬁed GAN objective. Results demonstrate the
advantage of adding our proposed modiﬁcation on both GAN and WGAN. See Appendix D for
qualitative results.
Face-Bed dataset. We combine 20K face images from CelebA dataset [17] and 20K bedroom images
from LSUN Bedrooms dataset [27] to construct a natural image dataset supported on a disconnected
manifold. We center crop and resize images to 64 × 64. 5 generators are used for multi-generator

8

0123456789Modes01000200030004000500060007000MSDAverage Distance over ModesRealDMWGAN-PLWGAN-GP0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0Sample RatioSample QualityRealDMWGAN-PLWGAN-GP0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0Sample RatioSample QualityRealDMWGAN-PLDMWGANMIXGANWGAN-GP(a)

(c)

(b)

(d)

Figure 6: DMWGAN-PL prior learning during training on MNIST with 20 generators (a b) and
on Face-Bed with 5 generators (c  d). (a  c) show samples from top generators with prior greater
than 0.05 and 0.2 respectively. (b  d) show the probability of selecting each generator r(c; ζ) during
training  each color denotes a different generator. The color identifying each generator in (b) and
the border color of each image in (a) are corresponding  similarly for (d) and (c). Notice how prior
learning has correctly learned probability of selecting each generators and dropped out redundant
generators without any supervision.

models. Figures 4c  5 and Table 1 show the results of this experiment. See Appendix E for more
qualitative results.

6 Conclusion and Future Works

In this work we showed why the single generator GANs can not correctly learn distributions supported
on disconnected manifolds  what consequences this shortcoming has in practice  and how multi-
generator GANs can effectively address these issues. Moreover  we showed the importance of learning
a prior over the generators rather than using a ﬁxed prior in multi-generator models. However  it is
important to highlight that throughout this work we assumed the disconnectedness of the real data
support. Verifying this assumption in major datasets  and studying the topological properties of these
datasets in general  are interesting future works. Extending the prior learning to other methods  such
as learning a prior over shape of Z space  and also investigating the effects of adding diversity to
discriminator as well as the generators  also remain as exciting future paths for research.

Acknowledgement

This work was supported by Verisk Analytics and NSF-USA award number 1409683.

9

0100000200000300000400000500000Iterations0.000.020.040.060.080.100.12ProbabilityRL Policy0100000200000300000400000500000Iterations0.00.10.20.30.40.5ProbabilityRL PolicyReferences
[1] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks.

arXiv preprint arXiv:1701.04862  2017.

[2] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial networks. In

International Conference on Machine Learning  pages 214–223  2017.

[3] Sanjeev Arora  Rong Ge  Yingyu Liang  Tengyu Ma  and Yi Zhang. Generalization and equilibrium in

generative adversarial nets (gans). arXiv preprint arXiv:1703.00573  2017.

[4] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973  2018.

[5] Tong Che  Yanran Li  Athul Paul Jacob  Yoshua Bengio  and Wenjie Li. Mode regularized generative

adversarial networks. arXiv preprint arXiv:1612.02136  2016.

[6] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel.

Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems  pages 2172–2180  2016.

[7] Jeff Donahue  Philipp Krähenbühl  and Trevor Darrell. Adversarial feature learning. arXiv preprint

arXiv:1605.09782  2016.

[8] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Arjovsky  and

Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704  2016.

[9] Arnab Ghosh  Viveka Kulharia  Vinay Namboodiri  Philip HS Torr  and Puneet K Dokania. Multi-agent

diverse generative adversarial networks. arXiv preprint arXiv:1704.02906  2017.

[10] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[11] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville. Improved
training of wasserstein gans. In Advances in Neural Information Processing Systems  pages 5769–5779 
2017.

[12] Swaminathan Gurumurthy  Ravi Kiran Sarvadevabhatla  and V Babu Radhakrishnan. Deligan: Generative
adversarial networks for diverse and limited data. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  volume 1  2017.

[13] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems  pages 6629–6640  2017.

[14] Quan Hoang  Tu Dinh Nguyen  Trung Le  and Dinh Phung. MGAN: Training generative adversarial
nets with multiple generators. In International Conference on Learning Representations  2018. URL
https://openreview.net/forum?id=rkmu5b0a-.

[15] John L Kelley. General topology. Courier Dover Publications  2017.

[16] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/  1998.

[17] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the wild. In

Proceedings of International Conference on Computer Vision (ICCV)  2015.

[18] Mario Lucic  Karol Kurach  Marcin Michalski  Sylvain Gelly  and Olivier Bousquet. Are gans created

equal? a large-scale study. arXiv preprint arXiv:1711.10337  2017.

[19] Lars Mescheder  Andreas Geiger  and Sebastian Nowozin. Which training methods for gans do actually

converge? arXiv preprint arXiv:1801.04406  2018.

[20] Luke Metz  Ben Poole  David Pfau  and Jascha Sohl-Dickstein. Unrolled generative adversarial networks.

arXiv preprint arXiv:1611.02163  2016.

[21] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances

in Neural Information Processing Systems  pages 5591–5600  2017.

[22] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

10

[23] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems  pages 2234–2242 
2016.

[24] Akash Srivastava  Lazar Valkoz  Chris Russell  Michael U Gutmann  and Charles Sutton. Veegan: Reducing
mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing
Systems  pages 3310–3320  2017.

[25] Joshua B Tenenbaum  Vin De Silva  and John C Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  2000.

[26] Ming-Hsuan Yang. Extended isomap for pattern classiﬁcation. In AAAI/IAAI  pages 224–229  2002.

[27] Fisher Yu  Yinda Zhang  Shuran Song  Ari Seff  and Jianxiong Xiao. Lsun: Construction of a large-scale

image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365  2015.

11

,Mahyar Khayatkhoei
Maneesh Singh
Ahmed Elgammal