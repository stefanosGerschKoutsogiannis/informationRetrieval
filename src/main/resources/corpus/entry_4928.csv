2019,Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression,This paper develops a prediction-based prescriptive model for optimal decision
making that (i) predicts the outcome under each action using a robust
nonlinear model  and (ii) adopts a randomized prescriptive policy determined
by the predicted outcomes. The predictive model combines a new regularized
regression technique  which was developed using Distributionally Robust
Optimization (DRO) with an ambiguity set constructed from the Wasserstein
metric  with the K-Nearest Neighbors (K-NN) regression  which helps to
capture the nonlinearity embedded in the data. We show theoretical results
that guarantee the out-of-sample performance of the predictive model  and
prove the optimality of the randomized policy in terms of the expected true
future outcome. We demonstrate the proposed methodology on a hypertension
dataset  showing that our prescribed treatment leads to a larger reduction in
the systolic blood pressure compared to a series of alternatives. A clinically
meaningful threshold level used to activate the randomized policy is also
derived under a sub-Gaussian assumption on the predicted outcome.,Selecting Optimal Decisions via Distributionally

Robust Nearest-Neighbor Regression

Division of Systems Engineering

Department of Electrical and Computer Engineering

Ruidi Chen

Boston University
Boston  MA 02215
rchen15@bu.edu

Ioannis Ch. Paschalidis ∗

Division of Systems Engineering

and Department of Biomedical Engineering

Boston University
Boston  MA 02215
yannisp@bu.edu

Abstract

This paper develops a prediction-based prescriptive model for optimal decision
making that (i) predicts the outcome under each action using a robust nonlinear
model  and (ii) adopts a randomized prescriptive policy determined by the predicted
outcomes. The predictive model combines a new regularized regression technique 
which was developed using Distributionally Robust Optimization (DRO) with
an ambiguity set constructed from the Wasserstein metric  with the K-Nearest
Neighbors (K-NN) regression  which helps to capture the nonlinearity embedded in
the data. We show theoretical results that guarantee the out-of-sample performance
of the predictive model  and prove the optimality of the randomized policy in terms
of the expected true future outcome. We demonstrate the proposed methodology
on a hypertension dataset  showing that our prescribed treatment leads to a larger
reduction in the systolic blood pressure compared to a series of alternatives. A
clinically meaningful threshold level used to activate the randomized policy is also
derived under a sub-Gaussian assumption on the predicted outcome.

Introduction

1
Suppose we are given a discrete set of available actions [M ] (cid:44) {1  . . .   M}  and our goal is to choose
m ∈ [M ] such that the future outcome y is optimized. We are interested in ﬁnding the optimal
decision with the aid of auxiliary data x ∈ Rp that are concurrently observed  and correlated with
the uncertain outcome y. A main challenge with learning from observational data lies in the lack of
counterfactual information. One solution is to estimate/predict the effects of counterfactual policies
by learning an action-dependent predictive model that groups the training samples based on their
actions  and ﬁts a model in each group between the outcome y and the feature x. The predictions
from this composite model can be used to determine the optimal action to take. The performance of
the prescribed decision hinges on the quality of the predictive model. We have observed that (i) there
are often “outliers” in the data  especially in the medical applications motivating this work  caused
by recording errors  missing values  and factors not captured in the data  and (ii) the underlying
relationship we try to learn is usually nonlinear and its parametric form is not known a priori. To deal
with these issues  a nonparametric robust learning procedure is in need.
Motivated by the observation that individuals with similar features x would have similar outcomes y
if they were to take the same action  we propose a predictive model that makes predictions based
on the outcomes of similar individuals/neighbors in each group of the training set. It is a nonlinear
and nonparametric estimator which constructs locally linear (constant) curves based on the similarity
between individuals. To ﬁnd reasonable neighbors  we need to accurately identify the set of features
that are predictive of the outcome. We propose a regularized regression procedure for this task in

∗http://sites.bu.edu/paschalidis

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

consideration of the outliers that could potentially bias the estimation. Our prescriptive methodology
is established on the basis of a regression informed K-Nearest Neighbors (K-NN) model [2] that
evaluates the importance of features through regularized regression  and estimates the outcome by
averaging over the neighbors identiﬁed by a regression coefﬁcients-weighted distance metric.
The regularized regression has its root in Distributionally Robust Linear Regression (DRLR) with a
Wasserstein metric-based uncertainty set [13]. The K-NN model builds locally linear (and globally
nonlinear) predictions using information from neighbors  accounting for the non-linearity that is
not captured by DRLR. Furthermore  it is easy to estimate and is efﬁcient to solve. Our framework
uses both parametric (DRLR) and nonparametric (K-NN) predictive models  producing robust
predictions immunized against outliers and capturing the underlying non-linearity in the data. It
is more information-efﬁcient and more interpretable than the vanilla K-NN. We then develop a
randomized prescriptive policy that chooses each action m  whose predicted outcome is ˆym(x)  with
j=1 e−ξ ˆyj (x)  for some pre-speciﬁed positive constant ξ. As we will see 

probability e−ξ ˆym(x)/(cid:80)M

this randomized strategy leads to a nearly optimal future outcome by an appropriate choice of ξ.
In recent years there has been an emerging interest in combining ideas from machine learning
with operations research to develop a framework that uses data to prescribe optimal decisions
[4  16  10  22]. Current research has focused on applying machine learning methodologies to predict
the counterfactuals  based on which optimal decisions can be made. Local learning methods such
as K-NN [2]  LOESS (LOcally Estimated Scatterplot Smoothing) [15]  CART (Classiﬁcation And
Regression Trees) [12]  and Random Forests [11] have been studied [4  7  8  17  9]. Extensions to
continuous and multi-dimensional decision spaces with observational data were considered in [5].
To prevent overﬁtting  [6] proposed two robust prescriptive methods based on Nadaraya-Watson
and nearest-neighbors learning. Deviating from such a predict-optimize paradigm  [3] presented
a new bandit algorithm based on the LASSO to learn a model of decision rewards conditional on
individual-speciﬁc covariates.
Our method constructs a locally linear estimator of the future outcome through learning a robust
metric in the feature space. Different from the classical metric learning works (e.g.  [20])  we solve a
downstream decision making problem by utilizing the information ﬁltered by the learned metric. [20]
focuses on the computational aspect of solving the metric regression problem. By contrast  we focus
on developing a novel method for the optimal decision making problem rather than improving the
algorithmic efﬁciency. Moreover  [20] studies only the regression problem  whereas we considered a
richer framework of combining regression with a randomized prescriptive policy.
Our problem is closely related to contextual bandits [14  1  23  25] where an agent learns a sequence
of decisions conditional on the contexts  with the aim of maximizing its cumulative reward. It has
recently found applications in learning personalized treatment for chronic diseases from mobile
health data [24  26  27]. However  in this work  we learn the interaction between the context and
rewards in each action group across similar individuals  not over the history of the same individual as
in contextual bandits. Contextual bandits are most suitable for learning sequential strategies through
repeated interactions with the environment  which requires a substantial amount of historical data
for exploring the reward function and exploiting the promising actions. By contrast  our method
does not require the availability of historical data  but instead learns the payoff function from similar
individuals. This can be viewed as a different type of exploration  i.e.  when little information can
be acquired for the past states of an individual  investigating the behavior of similar subjects may
be beneﬁcial. This is essential for learning from Electronic Health Records (EHRs) available in the
hospital  which do not include frequent patient data. For instance  we may observe a very sparse
treatment history for some patients  and the lag between patient visits is usually large.
Our method is similar to K-NN regression with an Ordinary Least Squares (OLS)-weighted metric
used in [7] to learn the optimal treatment for type-2 diabetic patients. The key differences lie in
that: (i) we adopt a robustiﬁed regression procedure that is immunized against outliers and is thus
more stable and reliable; (ii) we propose a randomized prescriptive policy that adds robustness to
the methodology  whereas [7] deterministically prescribed the treatment with the best predicted
outcome; (iii) we establish theoretical guarantees on the quality of the predictions and the prescribed
actions  and (iv) the prescriptive rule in [7] was activated when the improvement of the recommended
treatment over the standard of care exceeded a certain threshold whereas our method looks into the
improvement over the previous regimen. This distinction makes our algorithm applicable in the
scenario where the standard of care is unknown or ambiguous. Further  we derive a closed-form

2

expression for the threshold level  which greatly improves the computational efﬁciency compared to
[7] where a threshold was selected by cross-validation.
The remainder of the paper is organized as follows. In Sec. 2  we introduce the DRLR+K-NN model
and present the performance guarantees on its predictive power. Sec. 3 develops the randomized
prescriptive policy and proves its optimality in terms of the expected true outcome. Experimental
results using real medical (EHR) data are presented in Sec. 4. We conclude the paper in Sec. 5.

2 DRLR Informed K-Nearest Neighbors
Given a feature vector x ∈ Rp  and a set of M available actions [M ]  our goal is to predict the future
outcome ym(x) under each possible action m ∈ [M ]. Assume the following relationship between
the features and the outcome:

ym = x(cid:48)

mβ

∗
m + hm(xm) + m 

where prime denotes transpose  (xm  ym) represents the feature-outcome pair of an individual taking
m is the coefﬁcient that captures the linear trend; hm(·) is a Lipschitz continuous
∗
action m; β
nonlinear function (whose form is unknown) describing the nonlinear ﬂuctuation in ym  and m is
the noise term with zero mean and standard deviation ηm that expresses the intrinsic randomness of
ym and is assumed to be independent of xm.
Suppose for each m ∈ [M ]  we observe Nm training samples (xmi  ymi)  i = 1  . . .   Nm  that
∗
take action m. To estimate β
m  we adopt the robust formulation that was developed in [13]. A
robust model could lead to an improved out-of-sample performance  and accommodate the potential
∗
nonlinearity that is not explicitly revealed by the linear coefﬁcients β
m  thus resulting in a more
accurate assessment of the features. The DRLR model developed in [13] minimizes the worst-case
absolute loss within a distributional ambiguity set deﬁned using the Wasserstein metric [18  19] that
contains all possible perturbations on the distribution of the data. The robustness is achieved through
hedging against this family of distributions. The learning problem is formulated as:

EQm(cid:2)|ym − x(cid:48)

mβm|(cid:3) 

inf
βm

sup

Qm∈Ωm

(1)

where Qm is the probability distribution of (xm  ym)  belonging to some set Ωm deﬁned as:

Ωm (cid:44) {Qm ∈ M(Zm) : W1(Qm  ˆPNm) ≤ rm} 

where Zm is the set of all possible values for (xm  ym); M(Zm) is the space of probability distri-
butions supported on Zm; ˆPNm is the uniform empirical distribution on the Nm observed samples
(xmi  ymi)  i = 1  . . .   Nm; rm is a pre-speciﬁed parameter indicating the amount of ambiguity
allowed; and W1(Qm  ˆPNm) is the order-1 Wasserstein distance between Qm and ˆPNm deﬁned as:

W1(Qm  ˆPNm ) = sup
f∈L

Zm

f (zm) Qm(dzm) −

f (zm) ˆPNm(dzm)

 

where zm = (xm  ym)  and L is the space of all Lipschitz continuous functions satisfying |f (zm1) −
f (zm2)| ≤ (cid:107)zm1 − zm2(cid:107)2  ∀zm1  zm2 ∈ Zm.
With Nm independently and identically distributed samples (xmi  ymi)  i = 1  . . .   Nm  [13] has
shown that problem (1) can be reformulated as:

(cid:90)

Zm

(cid:41)

(cid:40)(cid:90)

Nm(cid:88)

i=1

1
Nm

inf
βm

|ymi − x(cid:48)

miβm| + rm(cid:107)(−βm  1)(cid:107)2.

(2)

∗
Solving Eq. (2) gives us a robust estimator of the linear regression coefﬁcient β
m  which we denote
by ˆβm. The elements of ˆβm measure the relative signiﬁcance of the predictors in determining the
outcome ym. We feed the estimator into the nonlinear non-parametric K-NN regression model  by
considering the following ˆβm-weighted metric:

(cid:113)

(cid:107)x − xmi(cid:107) ˆWm

=

(x − xmi)(cid:48) ˆWm(x − xmi) 

(3)

3

where ˆWm is a diagonal matrix with elements ( ˆβm1)2  . . .   ( ˆβmp)2  with ˆβmi the i-th element of ˆβm.
For a new test sample x  within each group m  we ﬁnd its Km nearest neighbors using the weighted
distance function (3). The predicted future outcome for x under action m  denoted by ˆym(x)  is
computed as the average response among the Km nearest neighbors  i.e. 

Km(cid:88)

i=1

ˆym(x) =

1
Km

ym(i) 

(4)

where ym(i) is the response of the i-th closest sample to x within group m. Eq. (4) computes a K-NN
estimate of the future outcome by using the linear regression coefﬁcients weighted distance function 
which can be viewed as a locally smoothed estimator in the neighborhood of x. The intuition for
using the ˆβm-weighted metric is to amplify the weight of features that are most predictive of ym
and downweight the unimportant ones. As a result  the selected samples are close to x in the most
relevant features  and their corresponding response values should serve as a good approximation.
Notice that Eq. (4) treats all neighbors equally by using the same weight. An alternative is to take
a distance-weighted average of the responses of neighbors; we have numerically tried this strategy
on our medical datasets in Section 4  but we ﬁnd that its effect is not signiﬁcantly different from
the strategy where a uniform average of the responses is taken. We also want to point out that the
following theoretical analysis can be easily adapted to the weighted average response prediction.
We next show that Eq. (4) provides a good prediction in the sense of Mean Squared Error (MSE).
The bias-variance decomposition implies the following:

(cid:1) (cid:44) E(cid:104)(cid:0)ˆym(x) − ym(x)(cid:1)2(cid:12)(cid:12)(cid:12)x  xmi  i = 1  . . .   Nm
(cid:105)
MSE(cid:0)ˆym(x)(cid:12)(cid:12)x  xmi  i = 1  . . .   Nm
(cid:20)(cid:16) 1
(cid:1)(cid:17)2(cid:12)(cid:12)(cid:12)x  xmi  ∀i
(cid:1) −(cid:0)x(cid:48)β
m + hm(xm(i))(cid:1)(cid:17)2
(cid:16)
Km(cid:88)
(cid:0)x(cid:48)
m + hm(x) − hm(xm(i))(cid:1)(cid:17)2
(cid:16) 1
(cid:0)(x − xm(i))(cid:48)β

m + hm(x) − 1
∗
Km(cid:88)
Km

∗
m + hm(xm(i)) + m(i)

(cid:21)

(5)

∗
m + hm(x) + m

Km(cid:88)
(cid:0)x(cid:48)

η2
m
Km

=

=

∗

m(i)β

+ η2

m 

Km

i=1

i=1

∗

+

+ η2
m

m(i)β

= E

x(cid:48)β

Km

i=1

+

η2
m
Km

where ym(x) is the true future outcome on x if action m is prescribed; and xm(i)  m(i) are the
feature vector and the noise term corresponding to the i-th closest sample to x within group m 
respectively. The third equality comes from the fact that the error term is independent of the features.
For each m ∈ [M ]  we aim to provide a probabilistic bound for 5 w.r.t. the measure of the Nm
training samples. By examining the ﬁrst term of the last line of (5)  we see that for MSE to be small 
m − ˆβm(cid:107)2 is small; (ii) (cid:107)x − xm(i)(cid:107) ˆWm
the following three conditions need to be satisﬁed: (i) (cid:107)β
∗
is
small for i = 1  . . .   Km; and (iii) hm(x) − hm(xm(i)) is small for i = 1  . . .   Km. Below we state
the assumptions that are needed to establish the result.
Assumption A (cid:107)(xm  ym)(cid:107)2 ≤ Rm a.s..
Assumption B (cid:107)(−βm  1)(cid:107)2 ≤ ¯Bm.
Assumption C For some set A(β
m  1)(cid:107)2} ∩ Sp+1 and
m) = cone{v| (cid:107)(−β
∗
∗
some positive scalar αm  where Sp+1 is the unit sphere in the (p + 1)-dimensional Euclidean space 

m  1) + v(cid:107)2 ≤ (cid:107)(−β
∗

v(cid:48)ZmZ(cid:48)

mv ≥ αm 

inf

v∈A(β∗

m)

where Zm = [zm1 ··· zmNm ] is the matrix with columns zm1  . . .   zmNm  with zmi = (xmi  ymi).
Assumption D (xm  ym) is a centered sub-Gaussian random vector  i.e.  it has zero mean and
satisﬁes the following condition:

|||(xm  ym)|||ψ2

= sup
u∈Sp+1

|||(xm  ym)(cid:48)u|||ψ2

≤ µm.

4

Assumption E The covariance matrix of (xm  ym) has bounded positive eigenvalues. Set Γm =
E[(xm  ym)(xm  ym)(cid:48)]; then 

0 < λm0 (cid:44) λmin(Γm) ≤ λmax(Γm) (cid:44) λm1 < ∞.

To see the validity of the above assumptions  notice that with standardized data  Assumptions A and B
are easily satisﬁed. Assumptions C  D  and E bound the variance of (x  y) in terms of the eigenvalues
of its covariance matrix and its sub-Gaussian norm. (If the variance in the data is prohibitively high 
the samples would contain little information to learn from.) Due to limited space  we defer the
intermediate results that bound (cid:107)β
to the supplementary. But those
results will be used as the foundation to derive the bound on the MSE of ˆym(x).

Theorem 2.1 Suppose we are given Nm i.i.d. copies of (xm  ym)  denoted by (xmi  ymi)  i =
1  . . .   Nm  where xm has independent  centered coordinates  and

m − ˆβm(cid:107)2 and (cid:107)x − xm(i)(cid:107) ˆWm
∗
(cid:17)

(cid:16)

cov(xm) = diag

σ2
m1  . . .   σ2

mp

.

Given a ﬁxed predictor x = (x1  . . .   xp)  and some scalar ¯wm  assuming

(cid:80)p

1. hm(·) is Lipschitz continuous with a Lipschitz constant Lm on the metric spaces (Xm (cid:107)·(cid:107)2)

and (Ym | · |)  where Xm Ym are the domain and codomain of hm(·)  respectively.
m > ¯B2

2. ¯w2
j=1(σ2
3. |(xmij − xj)2 − (σ2
4. The coordinates of any feasible solution to (2) have absolute values greater than or equal to

j )  where ¯Bm is speciﬁed in Assumption B.
j )| ≤ Tm  ∀i  j  where xmij is the j-th component of xmi.

mj + x2

mj + x2

m

some positive number bm (dense estimators).

Under Assumptions A  B  C  D  E  when Nm ≥ nm  with probability at least δm − I1−pm0 (Nm −
Km + 1  Km) w.r.t. the measure of samples 

bm

+

√

p ¯wm + Lm ¯wm
¯Bm

(ˆym(x)−ym(x))2(cid:12)(cid:12)(cid:12)x  xmi  i = 1  . . .   Nm
E(cid:104)
(cid:105) ≤
(cid:17)2
and for any a ≥(cid:16) ¯wmτm
(cid:12)(cid:12)(cid:12)x  xmi  i = 1  . . .   Nm
P(cid:16)(cid:0)ˆym(x)−ym(x)(cid:1)2 ≥ a
(cid:16)
(cid:16) Tm
(cid:17)

pm0 = 1 − exp

− σ2
m
T 2
m

(cid:18)

(cid:16)

with

g

var

(xmij − xj)2

 

(cid:118)(cid:117)(cid:117)(cid:116) p(cid:88)

σm =

+ η2
m
Km

√

+

bm

m 
+ η2

(cid:18) ¯wmτm
(cid:16) ¯wmτm
m −(cid:80)

(cid:17) ≤

bm

σ2
m

where I1−pm0(· ·) is the regularized incomplete beta function  and
j(σ2

m/ ¯B2
¯w2

mj + x2
j )

(cid:19)2

a

(cid:17)

(cid:17)(cid:19)

 

g(u) = (1 + u) log(1 + u) − u.

p ¯wm +

Lm ¯wm

¯Bm

+

η2
m
Km

+η2

m  (6)

(cid:17)2

√

+

p ¯wm + Lm ¯wm
¯Bm

+ η2
m
Km

+ η2
m

 
(7)

j=1

The notations nm  δm  and τm come from a simpliﬁed version of Theorem 3.11 in [13]  which states
that when the sample size Nm ≥ nm  with probability at least δm 

(cid:107)β

m − ˆβm(cid:107)2 ≤ τm.
∗

The parameters nm  δm  τm are related to the sub-Gaussian norm of (xm  ym)  the eigenvalues of
∗
the covariance matrix of (xm  ym)  and the geometric structure of the true regression coefﬁcient β
m.
Remark 2.1 The expectation in (6) and the probability in (7) are w.r.t. the measure of the noise m.
Thm. 2.1 essentially says that for any given x  with a high probability (w.r.t. the measure of samples) 
the prediction is close to the true future outcome. The prediction bias is determined by the accuracy

5

mj  j = 1  . . .   p  which are assumed to be at least b2

of the linear coefﬁcient estimate  the similarity between the individual in query and its K nearest
neighbors  the dimensionality of data  and the smoothness of the regression hypothesis.
Remark 2.2 The dependence on bm in the upper bound provided by (6) is due to the fact that ˆWm
has diagonal elements ˆβ2
m. If we multiply ˆWm
by a very large number  the neighbor selection criterion is not affected  since the relative signiﬁcance
of the predictors stays unchanged  but the bm appearing in (6) would be replaced by a very large
number  diminishing the effect of the ﬁrst term in the parenthesis  at the price of increasing ¯Bm and
¯wm  which in turn has an effect on the number of neighbors that are needed. It might be interesting to
explore this implicit trade-off and ﬁnd the optimal ˆWm to achieve the smallest MSE. For simplicity 
we just use ˆWm = diag( ˆβ2
Remark 2.3 We offered similar insights to [20] for the generalization bounds. Theorem 5.1 in [20]
provided a risk bound that depends on the empirical risk (reﬂected in τm and ¯wm of our bound)  the
dimensionality of data (p)  and the smoothness of the regression hypothesis (Lm).

mp) in this work.

m1  . . .   ˆβ2

3 Prescriptive Policy Development

We now proceed to develop the prescriptive policy with the aim of minimizing the future outcome. A
natural idea is to take the action that yields the minimum predicted outcome. To allow for ﬂexibility in
exploring alternatives that have a comparable performance  and also to correct for potential prediction
errors that might mislead the ranking of actions  we propose a randomized policy that prescribes
each action with a probability inversely proportional to its exponentiated predicted outcome. It can
be viewed as an ofﬂine Hedge algorithm [21] that increases the robustness of our method through
exploration. Speciﬁcally  given an individual with a feature vector x  and her predicted future outcome
under each action m  denoted by ˆym(x)  we consider a randomized policy that chooses action m
j=1 e−ξ ˆyj (x)  with ξ being some pre-speciﬁed positive constant. We

with probability e−ξ ˆym(x)/(cid:80)M
prescribes action m with probability e−ξ ˆym(x)/(cid:80)M

would like to explore properties of this policy in terms of its expected true outcome.
Theorem 3.1 Given any ﬁxed predictor x ∈ Rp  denote its predicted and true future outcome under
action m by ˆym(x) and ym(x)  respectively. Assume that we adopt a randomized strategy that
j=1 e−ξ ˆyj (x)  for some ξ ≥ 0. Assume ˆym(x) and
ym(x) are non-negative  ∀m. For any k ∈ [M ]  the expected true outcome of this policy satisﬁes:

M(cid:88)

m=1

(cid:18)
(cid:80)
e−ξ ˆym(x)
j e−ξ ˆyj (x) ym(x) ≤ yk(x) +
(cid:18) 1
M(cid:88)

+ ξ

M

m=1

ˆy2
m(x) +

ˆyk(x) − 1
M

ˆym(x)

(cid:19)

M(cid:88)
M(cid:88)
(cid:80)

m=1

m=1

(cid:19)

e−ξ ˆym(x)
j e−ξ ˆyj (x) y2

m(x)

(8)

+

log M

ξ

.

Theorem 3.1 says that the expected true outcome of the randomized policy is no worse than the true
outcome of any action k plus two components  one accounting for the gap between the predicted
outcome under k and the average predicted outcome  and the other depending on the parameter ξ.
Thinking about choosing k = arg minm ym(x)  if ˆyk(x) is below the average predicted outcome
(which should be true if we have an accurate prediction)  it follows from (8) that the randomized
policy leads to a nearly optimal future outcome by an appropriate choice of ξ.
In medical applications  when determining the future prescription for a patient  we usually have
access to some auxiliary information such as the current prescription she is receiving  and her current
measurements. In consideration of the health care costs and treatment transients  it is not desired to
switch patients’ treatments too frequently. We thus set a threshold level for the expected improvement
in the outcome  below which the randomized strategy will be rendered inactive and the current
−ξ ˆyj (x) ˆyk(x) ≤ xco − T (x)  mf(x) = m w.p.
j=1 e−ξ ˆyj (x); otherwise mf(x) = mc(x)  where mf(x) and mc(x) are the future and
current prescriptions for patient x  respectively; m is the prescribed action under the randomized
policy; xco represents the current observed outcome (e.g.  current blood pressure)  which is assumed
to be one of the components of x; and T (x) is some threshold level which will be determined later.
This prescriptive rule basically says that the randomized strategy will be activated only if the expected
improvement relative to the current observed outcome is signiﬁcant.

therapy will be continued. Speciﬁcally  if(cid:80)
e−ξ ˆym(x)/(cid:80)M

e−ξ ˆyk (x)
j e

(cid:80)

k

6

Theorem 3.2 Assume that the distribution of the predicted outcome ˆym(x) conditional on x  is
2Cm(x)  for any m ∈ [M ] and any x. Given a small
sub-Gaussian  and its ψ2-norm is equal to
0 < ¯ < 1  in order to satisfy

√

P

(cid:32)(cid:88)
(cid:16)
where µˆym(x) = E[ˆym(x)|x].

it sufﬁces to set a threshold

T (x) = max

k

(cid:33)

≤ ¯ 

(cid:80)

ˆyk(x) > xco − T (x)

e−ξ ˆyk(x)
j e−ξ ˆyj (x)
(cid:16)
xco − µˆym (x) −(cid:112)−2C 2

0  min
m

(cid:17)(cid:17)

m(x) log(¯/M )

 

Theorem 3.2 ﬁnds the largest threshold T (x) such that the probability of the expected improvement
being less than T (x) is small. The parameters µˆym(x) and Cm(x) for m ∈ [M ] can be estimated by
simulation through random sampling from a subset of the training examples.

Algorithm 1 Estimating the conditional mean and standard deviation of the predicted outcome.

Input: a feature vector x; am: the number of subsamples used to compute ˆβm  am < Nm; dm:
the number of repetitions.
for i = 1  . . .   dm do

Randomly pick am samples from group m  and use them to estimate a robust regression

coefﬁcient ˆβmi through solving 2.

The future outcome for x under action m is predicted as ˆymi(x) = x(cid:48) ˆβmi.

end for
Output: Estimate the conditional mean of ˆym(x) as:

and the conditional standard deviation as:

dm(cid:88)

i=1

1
dm

µˆym(x) =

ˆymi (x) 

(cid:118)(cid:117)(cid:117)(cid:116) 1

dm − 1

dm(cid:88)

(cid:16)

(cid:17)2
ˆymi(x) − µˆym (x)

.

i=1

Cm(x) =

A Special Case. As ξ → ∞  the randomized policy will assign probability 1 to the action with the
lowest predicted outcome  which is equivalent to the following deterministic policy:

mf(x) =

arg min
m
mc(x) 

ˆym(x) ≤ xco − T (x) 

ˆym(x)  if min
m

otherwise.

A slight modiﬁcation to the threshold level T (x) is given below:

(cid:16)

xco − µˆym(x) −(cid:112)−2C 2

(cid:17)(cid:17)

m(x) log ¯

.

T (x) = max

0  min
m

(cid:40)

(cid:16)

4 Numerical Results on a Hypertension Dataset

In this section  we will apply our method to develop optimal prescriptions for patients with hyperten-
sion. The data used for the study come from a large academic hospital system handling more than 1
million patient visits per year and consist of Electronic Health Records (EHR) containing the patients’
medical history in the period 1999–2014. Our goal is to ﬁnd the treatment that minimizes the future
systolic blood pressure based on the medical histories.

7

4.1 Dataset Description

According to certain cohort selection criteria (see the supplementary)  we have identiﬁed 49 401
patients who have been diagnosed with hypertension. Each patient may have multiple entries in
her/his medical record. To capture the period when the patient was experiencing the effect of the drug
regimen  we deﬁne the line of therapy as a time period (between 200 and 500 days) during which the
combination of drugs prescribed to the patient does not change.

Patient Visits. During each line of therapy  we split the treatment history of each patient into
several patient visits  to reﬂect changes in the features and outcomes. The patient visits are considered
to be occurring every 70 days. The measurements and lab tests are averaged over the 10 days prior to
the visit. We deﬁne the current prescription of each visit as the combination of drugs that were given
during the 10 days immediately preceding the visit  and the standard of care as the drug regimen that
is prescribed by the doctors at the time of the visit. The future outcome of the visit is computed as the
average systolic blood pressure in mmHg 70 to 180 days after it. Patient visits that contain missing
values for the outcome are dropped. We have obtained 26 128 valid visits  each with 63 features.

Features. The features for building the predictive model include: (i) demographics: sex  age and
race; (ii) measurements: systolic blood pressure and diastolic blood pressure  Body Mass Index (BMI)
and pulse; (iii) lab tests: blood chemistry tests and hematology tests; and (iv) diagnosis history.

Prescriptions. We consider six types of prescriptions for hypertension  each corresponding to a
different medication that could be prescribed: ACE inhibitor  Angiotensin Receptor Blockers (ARB) 
calcium channel blockers  thiazide and thiazide-like diuretics  α-blockers and β-blockers. The patient
visits are grouped based on their standard of care.

4.2 Model Development and Results

We will compare our algorithm with several alternatives that replace our DRLR informed K-NN with
a different predictive model such as LASSO  CART  and OLS informed K-NN [7]. Both deterministic
and randomized prescriptive policies are considered using predictions from these models.

Parameter tuning. Within each prescription group  we randomly split the patient visits into three
sets: a training set (80%)  a validation set (10%)  and a test set (10%). To reﬂect the dependency
of the number of neighbors on the number of training samples  we perform a linear regression
between these two quantities  which we use to determine the number of neighbors needed in different
settings. To tune the exponent ξ for the randomized strategy  we need to evaluate the effects of
counterfactual treatments. We assess the predictive power of a series of robust predictive models
(see the supplementary) in terms of their R2 and out-of-sample estimation errors  and select the
DRLR+K-NN model (imputation model) that excels in all metrics  to impute the outcome for an
unobservable treatment m  using the validation set.
When comparing the predictive performance of the models (Table 1  Supplementary)  we ﬁt a common
regression model to all patients without dividing them into groups  with the prescription being used
as a predictor. This leads to a signiﬁcant reduction in the unexplained variance of y  and thus the
advantages of DRLR+K-NN are not signiﬁcant. However  when we do groupwise regression where
prescription is not used as a predictor  the unexplained noise increases  robustness becomes more
critical  and thus the advantages of our method become more prominent (see the following Table 1).

Model training. We solve the predictive models on the whole training set with the best tuned
parameters  the output of which is used to develop the optimal prescriptions for the test set patients.
The parameter ¯ in the threshold T (x) is set to 0.1. We compute the average improvement (reduction)
in outcomes for patients in the test set  which is deﬁned to be the difference between the (expected)
future outcome under the recommended therapy and the current observed outcome. If the recommen-
dation does not match the standard of care  its future outcome is estimated through the imputation
model that was discussed earlier  where Km should be selected to ﬁt the size of the test set.

Reﬁnement of the policy.
In consideration of the sensitivity of K-NN to the number of neighbors 
we propose a reﬁnement of our model  where a patient-speciﬁc number of neighbors Km is used 

8

and the neighbors that are relatively far away from the patient in query are discarded. This can be
considered as taking a weighted average of the responses of neighbors to make the K-NN prediction.
the distance between the patient in query and her i-th closest neighbor in
Speciﬁcally  denote by dm
i
2 ≤ . . . dm
group m; we know dm
. Given some
threshold ˜T   the number of neighbors K(cid:48)

. Deﬁne j∗
m will be determined as follows.

j −(cid:80)j−1

m = arg maxj

1 ≤ dm

dm
j−1
i

(cid:16)

(cid:17)

dm

Km

i=1

j∗

−(cid:80)j∗
(cid:80)j∗

m−1

i=1

dm
j∗
i
m−1
dm
j∗
m−1

i

m

m − 1  if dm
j∗
Km 

m−1
otherwise.

i=1

K(cid:48)

m =

> ˜T  

Results and discussion. The reductions in outcomes (future minus current) for various models
are shown in Table 1. The columns indicate the prescriptive policies (deterministic or randomized);
the rows represent the predictive models whose outcomes ˆym(x) serve as inputs to the prescriptive
algorithm. We compare two strategies that use different rules for selecting the number of neighbors 
with a validated threshold ˜T = 1 for the patient-speciﬁc strategy. We test the performance of all
algorithms over ﬁve repetitions  each with a different training set. We also list the reductions in
outcomes resulting from the standard of care  and the current prescription which continues the
current drug regimen.
Several observations are in order: (i) all models outperform the current prescription and the standard
of care; (ii) the DRLR+K-NN model leads to the largest reduction in outcomes with a relatively
stable performance; (iii) using a patient-speciﬁc K(cid:48)
m in general leads to a more signiﬁcant reduction
in outcomes  and (iv) the randomized policy achieves a similar (slightly better) performance than the
deterministic one. Overall  the best DRLR+K-NN model leads to a 69% reduction in future systolic
blood pressure compared to the 2nd best model. We expect the randomized strategy to win when
the effects of several treatments do not differ much  in which case the deterministic algorithm might
produce misleading results. The randomized policy could potentially improve the out-of-sample
performance  as it gives the ﬂexibility of exploring options that are suboptimal on the training set  but
might be optimal on the test set.
It may be argued that the observed improvement is due to the evaluation model we choose; speciﬁcally 
using DRLR+K-NN to assess the performance of all candidates might cause bias that favors our
method. To mitigate this bias  we also used a mixture of OLS+K-NN and DRLR+K-NN (with equal
weights) as the imputation model  given that they achieve the best predictive performance. Under this
scheme  our model still outperforms all others.

Table 1: The reduction in future systolic blood pressured (mmHg); mean (standard deviation).

m

Training with a patient-speciﬁc K(cid:48)
Deterministic
-4.34 (0.28)
-4.46 (0.46)
-4.30 (0.35)
-7.42 (0.46)

Randomized
-4.33 (0.28)
-4.49 (0.50)
-4.30 (0.32)
-7.58 (0.51)

-2.56 (0.14)
-2.37 (0.11)

Training with a uniform Km
Deterministic Randomized
-4.22 (0.19)
-4.22 (0.20)
-4.48 (0.55)
-4.51 (0.49)
-4.29 (0.31)
-4.27 (0.32)
-6.58 (0.70)
-6.78 (0.73)

-2.50 (0.16)
-2.37 (0.11)

LASSO
CART

OLS+K-NN
DRLR+K-NN

Current prescription

Standard of care

5 Conclusions

We developed a prediction-based prescriptive method that determines the probability of taking each
action based on the predictions from a DRLR informed K-NN model. Theoretical guarantees on the
out-of-sample performance of the predictive model and the optimality of the prescriptive algorithm
were established. We also derived a closed-form expression for the threshold level that is used to
activate the randomized policy. The proposed approach was applied to actual hypertension patient data
obtained from a major academic hospital system  providing numerical evidence for the superiority of
our algorithm in terms of the improvement in outcomes.

9

Acknowledgments

The research was partially supported by the NSF under grants IIS-1914792  DMS-1664644  and
CNS-1645681  by the ONR under grant N00014-19-1-2571  by the NIH under grant 1R01GM135930 
by the Boston University Data Science Initiative  and by the BU Center for Information and Systems
Engineering.

References
[1] Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihong Li  and Robert Schapire.
Taming the monster: A fast and simple algorithm for contextual bandits. In International
Conference on Machine Learning  pages 1638–1646  2014.

[2] Naomi S Altman. An introduction to kernel and nearest-neighbor nonparametric regression.

The American Statistician  46(3):175–185  1992.

[3] Hamsa Bastani and Mohsen Bayati. Online decision-making with high-dimensional covariates.

2015.

[4] Dimitris Bertsimas and Nathan Kallus. From predictive to prescriptive analytics. arXiv preprint

arXiv:1402.5481  2014.

[5] Dimitris Bertsimas and Christopher McCord. Optimization over continuous and multi-

dimensional decisions with observational data. arXiv preprint arXiv:1807.04183  2018.

[6] Dimitris Bertsimas and Bart Van Parys. Bootstrap robust prescriptive analytics. arXiv preprint

arXiv:1711.09974  2017.

[7] Dimitris Bertsimas  Nathan Kallus  Alexander M Weinstein  and Ying Daisy Zhuo. Personalized
diabetes management using electronic medical records. Diabetes care  page dc160826  2016.

[8] Dimitris Bertsimas  Jack Dunn  and Nishanth Mundru. Optimal prescriptive trees. INFORMS

Journal on Optimization  2018.

[9] Max Biggs and Rim Hariss. Optimizing objective functions determined from random forests.

2018.

[10] Fernanda Bravo and Yaron Shaposhnik. Discovering optimal policies: A machine learning

approach to model analysis. 2017.

[11] Leo Breiman. Random forests. Machine learning  45(1):5–32  2001.

[12] Leo Breiman. Classiﬁcation and regression trees. Routledge  2017.

[13] Ruidi Chen and Ioannis Ch Paschalidis. A robust learning approach for regression models based
on distributionally robust optimization. Journal of Machine Learning Research  19(13)  2018.

[14] Wei Chu  Lihong Li  Lev Reyzin  and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence
and Statistics  pages 208–214  2011.

[15] William S Cleveland and Susan J Devlin. Locally weighted regression: an approach to regression
analysis by local ﬁtting. Journal of the American statistical association  83(403):596–610 
1988.

[16] Dick den Hertog and Krzysztof Postek. Bridging the gap between predictive and pre-
scriptive analytics-new optimization methodology needed. Technical report  Technical re-
port  Tilburg University  Netherlands  2016. Available at: http://www. optimization-online.
org/DB HTML/2016/12/5779. html  2016.

[17] Jack Dunn. Optimal Trees for Prediction and Prescription. PhD thesis  PhD thesis  Mas-

sachusetts Institute of Technology  2018. http://jack. dunn. nz/papers/Thesis. pdf  2018.

10

[18] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the Wasserstein metric: Performance guarantees and tractable reformulations. Mathemati-
cal Programming  pages 1–52  2017.

[19] Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with Wasserstein

distance. arXiv preprint arXiv:1604.02199  2016.

[20] Lee-Ad Gottlieb  Aryeh Kontorovich  and Robert Krauthgamer. Efﬁcient regression in metric
spaces via approximate Lipschitz extension. IEEE Transactions on Information Theory  63(8):
4838–4849  2017.

[21] Elad Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Opti-

mization  2(3-4):157–325  2016.

[22] Suvrajeet Sen and Yunxiao Deng. Learning enabled optimization: Towards a fusion of statistical
learning and stochastic programming. INFORMS Journal on Optimization (submitted)  2018.

[23] Aleksandrs Slivkins. Contextual bandits with similarity information. The Journal of Machine

Learning Research  15(1):2533–2568  2014.

[24] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile

health. In Mobile Health  pages 495–517. Springer  2017.

[25] Huasen Wu  R Srikant  Xin Liu  and Chong Jiang. Algorithms with logarithmic or sublinear
regret for constrained contextual bandits. In Advances in Neural Information Processing Systems 
pages 433–441  2015.

[26] Isaac Xia. The Price of Personalization: An Application of Contextual Bandits to Mobile Health.

PhD thesis  2018.

[27] Feiyun Zhu  Jun Guo  Ruoyu Li  and Junzhou Huang. Robust actor-critic contextual bandit
for mobile health (mhealth) interventions. In Proceedings of the 2018 ACM International
Conference on Bioinformatics  Computational Biology  and Health Informatics  pages 492–501.
ACM  2018.

11

,Ruidi Chen
Ioannis Paschalidis