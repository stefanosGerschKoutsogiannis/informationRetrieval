2018,Learning and Inference in Hilbert Space with Quantum Graphical Models,Quantum Graphical Models (QGMs) generalize classical graphical models by adopting the formalism for reasoning about uncertainty from quantum mechanics. Unlike classical graphical models  QGMs represent uncertainty with density matrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize Bayesian inference in Hilbert spaces. We investigate the link between QGMs and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel regression  respectively. We show that these operations can be kernelized  and use these insights to propose a Hilbert Space Embedding of Hidden Quantum Markov Models (HSE-HQMM) to model dynamics. We present experimental results showing that HSE-HQMMs are competitive with state-of-the-art models like LSTMs and PSRNNs on several datasets  while also providing a nonparametric method for maintaining a probability distribution over continuous-valued features.,Learning and Inference in Hilbert Space with

Quantum Graphical Models

Siddarth Srinivasan
College of Computing

Georgia Tech

Atlanta  GA 30332

sidsrini@gatech.edu

Carlton Downey

Department of Machine Learning

Carnegie Mellon University

Pittsburgh  PA 15213

cmdowney@cs.cmu.edu

Byron Boots

College of Computing

Georgia Tech

Atlanta  GA 30332

bboots@cc.gatech.edu

Abstract

Quantum Graphical Models (QGMs) generalize classical graphical models by
adopting the formalism for reasoning about uncertainty from quantum mechanics.
Unlike classical graphical models  QGMs represent uncertainty with density matri-
ces in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize
Bayesian inference in Hilbert spaces. We investigate the link between QGMs
and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent
to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel
regression  respectively. We show that these operations can be kernelized  and
use these insights to propose a Hilbert Space Embedding of Hidden Quantum
Markov Models (HSE-HQMM) to model dynamics. We present experimental
results showing that HSE-HQMMs are competitive with state-of-the-art models
like LSTMs and PSRNNs on several datasets  while also providing a nonparametric
method for maintaining a probability distribution over continuous-valued features.

Introduction and Related Work

1
Various formulations of Quantum Graphical Models (QGMs) have been proposed by researchers in
physics and machine learning [Srinivasan et al.  2018  Yeang  2010  Leifer and Poulin  2008] as a
way of generalizing probabilistic inference on graphical models by adopting quantum mechanics’
formalism for reasoning about uncertainty. While Srinivasan et al. [2018] focused on modeling
dynamical systems with Hidden Quantum Markov Models (HQMMs) [Monras et al.  2010]  they
also describe the basic operations on general quantum graphical models  which generalize Bayesian
reasoning within a framework consistent with quantum mechanical principles. Inference using Hilbert
space embeddings (HSE) is also a generalization of Bayesian reasoning  where data is mapped to a
Hilbert space in which kernel sum  chain  and Bayes rules can be used [Smola et al.  2007  Song et al. 
2009  2013]. These methods can model dynamical systems such as HSE-HMMs [Song et al.  2010] 
HSE-PSRs [Boots et al.  2012]  and PSRNNs [Downey et al.  2017]. [Schuld and Killoran  2018]
present related but orthogonal work connecting kernels  Hilbert spaces  and quantum computing.
Since quantum states live in complex Hilbert spaces  and both QGMs and HSEs generalize Bayesian
reasoning  it is natural to ask: what is the relationship between quantum graphical models and Hilbert
space embeddings? This is precisely the question we tackle in this paper. Overall  we present four
contributions: (1) we show that the sum rule for QGMs is identical to the kernel sum rule for HSEs 
while the Bayesian update in QGMs is equivalent to performing Nadaraya-Watson kernel regression 
(2) we show how to kernelize these operations and argue that with the right choice of features  we are
mapping our data to quantum systems and modeling dynamics as quantum state evolution  (3) we use
these insights to propose a HSE-HQMM to model dynamics by mapping data to quantum systems
and performing inference in Hilbert space  and  ﬁnally  (4) we present a learning algorithm and
experimental results showing that HSE-HQMMs are competitive with other state-of-the-art methods
for modeling sequences  while also providing a nonparametric method for estimating the distribution
of continuous-valued features.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

2 Quantum Graphical Models
2.1 Classical vs Quantum Probability
In classical discrete graphical models  an observer’s uncertainty about a random variable X can
be represented by a vector ~xwhose entries give the probability of X being in various states. In
quantum mechanics  we write the ‘pure’ quantum state of a particle A as | iA  a complex-valued
column-vector in some orthonormal basis that lives in a Hilbert space  whose entries are ‘probability
amplitudes’ of system states. The squared norm of these probability amplitudes gives the probability
of the corresponding system state  so the sum of squared norms of the entries must be 1. To describe
‘mixed states’  where we have a probabilistic mixture of quantum states  (e.g. a mixture of N quantum
systems  each with probability pi) we use a Hermitian ‘density matrix’  deﬁned as follows:

NXi

ˆ⇢ =

pi| iih i|

(1)

The diagonal entries of a density matrix give the probabilities of being in each system state  and
off-diagonal elements represent quantum coherences  which have no classical interpretation. Conse-
quently  the normalization condition is tr(ˆ⇢) = 1. Uncertainty about an n-state system is represented
by an n ⇥ n density matrix. The density matrix is the quantum analogue of the classical belief ~x.
2.2 Operations on Quantum Graphical Models
Here  we further develop the operations on QGMs introduced by Srinivasan et al. [2018]  working
with the notion that the density matrix is the quantum analogue of a classical belief state.
Joint Distributions The joint distribution of an n-state variable A and m-state variable B can be
written as an nm ⇥ nm ‘joint density matrix’ ˆ⇢AB. When A and B are independent  ˆ⇢AB = ˆ⇢A ⌦ ˆ⇢B.
As a valid density matrix  the diagonal elements represent probabilities corresponding to the states in
the Cartesian product of the basis states of the composite variables (so tr (ˆ⇢AB) = 1).
Marginalization Given a joint density matrix  we can recover the marginal ‘reduced density matrix’
for a subsystem of interest with the ‘partial trace’ operation. This operation is the quantum analogue
of classical marginalization. For example  the partial trace for a two-variable joint system ˆ⇢AB where
we trace over the second particle to obtain the state of the ﬁrst particle is:

ˆ⇢A = trB (ˆ⇢AB) =Xj

Bhj|ˆ⇢AB|jiB

(2)

Finally  we discuss the quantum analogues of the sum rule and Bayes rule. Consider a prior ~⇡ = P (X)
and a likelihood P (Y |X) represented by the column stochastic matrix A. We can then ask two
questions: what are P (Y ) and P (X|y)?
Sum Rule The classical answer to the ﬁrst question involves multiplying the likelihood with the
prior and marginalizing out X  like so:

P (Y ) =Xx

P (Y |x)P (x) = A~⇡

(3)

Srinivasan et al. [2018] show how we can construct a quantum circuit to perform the classical sum
rule (illustrated in Figure 1a  see appendix for note on interpreting quantum circuits). First  recall
that all operations on quantum states must be represented by unitary matrices in order to preserve
the 2-norm of the state. ˆ⇢env is an environment ‘particle’ always prepared in the same state that will
eventually encode ˆ⇢Y : it is initially a matrix with zeros everywhere except ˆ⇢1 1 = 1. Then  if the prior
~⇡ is encoded in a density matrix ˆ⇢X  and the likelihood table A is encoded in a higher-dimensional
unitary matrix  we can replicate the classical sum rule. Letting the prior ˆ⇢X be any density matrix and
ˆU1 be any unitary matrix generalizes the circuit to perform the ‘quantum sum rule’. This circuit can
be written as the following operation (the unitary matrix produces the joint distribution  the partial
trace carries out the marginalization):

ˆ⇢Y = trX⇣ ˆU1 (ˆ⇢X ⌦ ˆ⇢env) ˆU†1⌘

(4)

Bayes Rule Classically  we perform Bayesian update as follows (where diag(A(: y)) selects the
row of matrix A corresponding to observation y and stacks it along a diagonal):

P (X|y) =

P (y|X)P (X)

Px(y|x)P (x)

2

=

diag(A(y :))~⇡
1T diag(A(y :))~⇡

(5)

ˆ⇢X

ˆ⇢env

ˆU1

ˆ⇢Y

ˆ⇢X

ˆ⇢env

ˆ⇢X|y

ˆU2

ˆ⇢env

ˆ⇢y

ˆU ⇡
3

ˆ⇢X|y

(a) Quantum circuit to
compute P (Y )

(c) Alternate circuit to
compute P (X|y)
Figure 1: Quantum-circuit analogues of conditioning in graphical models

(b) Quantum circuit to
compute P (X|y)

The quantum circuit for Bayesian update presented by Srinivasan et al. [2018] is shown in Figure 1b.
It involves encoding the prior in ˆ⇢X as before  and encoding the likelihood table A in a unitary matrix
ˆU2. Applying the unitary matrix ˆU2 prepares the joint state ˆ⇢XY   and we apply a von Neumann
projection operator (denoted ˆPy) corresponding to the observation y (the D-shaped symbol in the
circuit)  to obtain the conditioned state ˆ⇢X|y in the ﬁrst particle. The projection operator selects the
entries from the joint distribution ˆ⇢XY that correspond to the actual observation y  and zeroes out
the other entries  analogous to using an indicator vector to index into a joint probability table. This
operation can be written (denominator renormalizes to recover a valid density matrix) as:

ˆ⇢X|y =

trenv⇣Py ˆU2 (ˆ⇢X ⌦ ˆ⇢env) ˆU†2 P †y⌘
tr⇣trenv⇣Py ˆU2 (ˆ⇢X ⌦ ˆ⇢env) ˆU†2 P †y⌘⌘

(6)

However  there is an alternate quantum circuit that could implement Bayesian conditioning. Consider
re-writing the classical Bayesian update as a linear update as follows:

1

P (X|y) = (A · diag(~⇡))T (diag (A~⇡))1 ~ey

(7)
where (A · diag(~⇡))T yields the joint probability table P (X  Y )  (diag (A~⇡))1 is a diagonal matrix
with the inverse probabilities
P (Y =y) on the diagonal  serving to renormalize the columns of the
joint probability table P (X  Y ). Thus  (A · diag(~⇡))T (diag (A~⇡))1 produces a column-stochastic
matrix  and ~ey is just an indicator vector that selects the column corresponding to the observation y.
Then  just as the circuit in Figure 1a is the quantum generalization for Equation 3  we can use the
quantum circuit shown in 1c for this alternate Bayesian update. Here  ˆ⇢y encodes the indicator vector
corresponding to the observation y  and ˆU ⇡
3 is a unitary matrix constructed using the prior ⇡ on X.
Letting ˆU ⇡
3 to be any unitary matrix constructed from some prior on X would give an alternative
quantum Bayesian update.
These are two different ways of generalizing classical Bayesian rule within quantum graphical models.
So which circuit should we use? One major disadvantage of the second approach is that we must
construct different unitary matrices ˆU ⇡
3 for different priors on X. The ﬁrst approach also explicitly
involves measurement  which is nicely analogous to classical observation. As we will see in the next
section  the two circuits are different ways of performing inference in Hilbert space  with the ﬁrst
approach being equivalent to Nadaraya-Watson kernel regression and the second approach being
equivalent to kernel Bayes rule for Hilbert space embeddings.

3 Translating to the language of Hilbert Space Embeddings
In the previous section  we generalized graphical models to quantum graphical models using the
quantum view of probability. And since quantum states live in complex Hilbert spaces  inference in
QGMs happens in Hilbert space. Here  we re-write the operations on QGMs in the language of Hilbert
space embeddings  which should be more familiar to the statistical machine learning community.
3.1 Hilbert Space Embeddings
Previous work [Smola et al.  2007] has shown that we can embed probability distributions over a data
domain X in a reproducing kernel Hilbert space (RKHS) F – a Hilbert space of functions  with some
kernel k. The feature map (x) = k(x ·) maps data points to the RKHS  and the kernel function
satisﬁes k(x  x0) = h(x)  (x0)iF. Additionally  the dot product in the Hilbert space satisﬁes the
reproducing property:

hf (·)  k(x ·)iF = f (x) 

and hk(x ·)  k(x0 ·)iF = k(x  x0)

(8)

3

3.1.1 Mean Embeddings
The following equations describe how a distribution of a random variable X is embedded as a
mean map [Smola et al.  2007]  and how to empirically estimate the mean map from data points
{x1  . . .   xn} drawn i.i.d from P (X)  respectively:

µX := EX [(X)]

ˆµX =

(xi)

(9)

1
n

nXi=1

Quantum Mean Maps We still take the expectation of the features of the data  except we require
that the feature maps (·) produce valid density matrices representing pure states (i.e.  rank 1).
Consequently  quantum mean maps have the nice property of having probabilities along the diagonal.
Note that these feature maps can be complex and inﬁnite  and in the latter case  they map to density
operators. For notational consistency  we require the feature maps to produce rank-1 vectorized
density matrices (by vertically concatenating the columns of the matrix)  and treat the quantum mean
map as a vectorized density matrix ~µX = ~⇢X.
3.1.2 Cross-Covariance Operators
Cross-covariance operators can be used to embed joint distributions; for example  the joint distribution
of random variables X and Y can be represented as a cross-covariance operator (see Song et al.
[2013] for more details):

CXY := EXY [(X) ⌦ (Y )]

(10)

Quantum Cross-Covariance Operators The quantum embedding of a joint distribution P (X  Y )
is a square mn ⇥ mn density matrix ˆ⇢XY for constituent m ⇥ m embedding of a sample from P (X)
and n ⇥ n embedding of a sample from P (Y ). To obtain a quantum cross-covariance matrix CXY  
we simply reshape ˆ⇢XY to an m2 ⇥ n2 matrix  which is also consistent with estimating it from data
as the expectation of outer product of feature maps (·) that produce vectorized density matrices.
3.2 Quantum Sum Rule as Kernel Sum Rule
We now re-write the quantum sum rule for quantum graphical models from Equation 4  in the
language of Hilbert space embeddings. Srinivasan et al. [2018] showed that Equation 4 can be written

~µY =Xi ⇣⇣Vi ˆU W⌘⇤

as ˆ⇢Y =Pi Vi ˆU W ˆ⇢XW † ˆU†V †i   where matrices W and V tensor with an environment particle and
partial trace respectively. Observe that a quadratic matrix operation can be simpliﬁed to a linear
operation  i.e.  ˆU ˆ⇢ ˆU† = reshape(( ˆU⇤ ⌦ ˆU )~⇢) where ~⇢ is the vectorized density matrix ˆ⇢. Then:
⌦⇣Vi ˆU W⌘! ~µX = A~µX

⌦⇣Vi ˆU W⌘⌘ ~µX = Xi ⇣Vi ˆU W⌘⇤

where A = (Pi(Vi ˆU W )⇤ ⌦ (Vi ˆU W )). We have re-written the complicated transition update as a
simple linear operation  though A should have constraints to ensure the operation is valid according to
quantum mechanics. Consider estimating A from data by solving a least squares problem: suppose we
have data (⌥X  Y ) where  2 Rd1⇥n  ⌥ 2 Rd2⇥n are matrices of n vectorized d1  d2-dimensional
density matrices and n is the number of data points. Solving for A gives us A = Y ⌥†X(⌥X⌥†X)1.
nPn
i (~µYi ⌦ ~µ†Xi
But Y ⌥†X = n · CY X where CY X = 1
XX  allowing us to
re-write Equation 11 as:
~µY = CY XC1
XX ~µX

(12)
But this is exactly the kernel sum rule from Song et al. [2013]  with the conditional embedding
operator CY |X = CY XC1
XX. Thus  when the feature maps that produce valid (vectorized) rank-1
density matrices  the quantum sum rule is identical to the kernel sum rule. One thing to note is that
solving for A using least-squares needn’t preserve the quantum-imposed constraints; so either the
learning algorithm must force these constraints  or we project ~µY back to a valid density matrix.

). Then  A = CY XC1

(11)

Finite Sample Kernel Estimate We can straightforwardly adopt the kernelized version of the
conditional embedding operator from HSEs [Song et al.  2013] ( is a regularization parameter):

CY |X =( Kxx + I)1⌥†

(13)
where = ( (y1)  . . .   (yn))  ⌥= ( (x1)  . . .   (xn))  and K =⌥ †⌥  and these feature maps
produce vectorized rank-1 density matrices. The data points in Hilbert space can be written as
~µY = ↵Y and ~µX =⌥ ↵X where ↵ 2 Rn are weights for the training data points  and the kernel
quantum sum rule is simply:
(14)

~µY = CY |X ~µX ) ↵Y =( Kxx + I)1⌥†⌥↵X ) ↵Y = (Kxx + I)1Kxx↵X

4

3.3 Quantum Bayes Rule as Nadaraya-Watson Kernel Regression
Here  we re-write the Bayesian update for QGMs from Equation 6 in the language of HSEs. First  we
modify the quantum circuit in 1b to allow for measurement of a rank-1 density matrix ˆ⇢y in any basis
(see Appendix for details) to obtain the circuit shown in Figure 2  described by the equation:

ˆ⇢X|y / trenv⇣(I ⌦ ˆu)P (I ⌦ ˆu†) ˆU (ˆ⇢X ⌦ ˆ⇢env) ˆU†(I ⌦ ˆu†)†P †(I ⌦ ˆu)†⌘

where ˆu changes the basis of the environment variable to one in which the rank-1 density matrix
encoding the observation ˆ⇢Y is diagonalized to ⇤ – a matrix with all zeros except ⇤1 1 = 1. The
projection operator will be P = (I ⌦ ⇤)  which means the terms (I ⌦ ˆu)P (I ⌦ ˆu†) = (I ⌦ ˆu)(I ⌦
⇤)(I ⌦ ˆu†) = (I ⌦ u⇤u†) = (I ⌦ ˆ⇢y)  allowing us to rewrite Equation 15 as:
ˆ⇢X|y / trenv⇣(I ⌦ ˆ⇢y) ˆU (ˆ⇢X ⌦ ˆ⇢env) ˆU†(I ⌦ ˆ⇢y)†⌘
ˆ⇢XY = ˆU (ˆ⇢X ⌦ ˆ⇢env) ˆU† = ˆU W ˆ⇢XW † ˆU†
trenv(I ⌦ ˆ⇢y)ˆ⇢XY (I ⌦ ˆ⇢y)†
ˆ⇢X|y =
tr (trenv ((I ⌦ ˆ⇢y)ˆ⇢XY (I ⌦ ˆ⇢y)†))

Let us break this equation into two steps:

(16)

(17)

(15)

Now  we re-write the ﬁrst expression in the language of HSEs. The quadratic matrix operation
can be re-written as a linear operation by vectorizing the density matrix as we did in Section
3.2: ~µXY = (( ˆU W )⇤ ⌦ ( ˆU W ))~µX. But for ~µX 2 Rn2⇥1  W 2 Rns⇥n  and ˆU 2 Cns⇥ns
this operation gives ~µXY 2 Rn2s2⇥1  which we can reshape into an n2 ⇥ s2 matrix C⇡X
XY (the
superscript simply indicates the matrix was composed from a prior on X). We can then directly
XY = B ⇥3 ~µX  where B is (( ˆU W )⇤ ⌦ ( ˆU W )) reshaped into a three-mode tensor and ⇥3
write C⇡X
represents a tensor contraction along the third mode. But  just as we solved A = CY XC1
XX in Section
3.2 we can estimate B = C(XY )XC1
XX = CXY |X as a matrix and reshape into a 3-mode tensor 
allowing us to re-write the ﬁrst step in Equation 17 as:
(18)
Now  to simplify the second step  observe that the numerator can be rewritten to get ~µX|y /
XY ˆ⇢T
y ⌦ ˆ⇢y~t  where ~t is a vector of 1s and 0s that carries out the partial trace operation. But  for
C⇡X
a rank 1 density matrix ˆ⇢y  this actually simpliﬁes further:
One way to renormalize ~µX|y is to computeCXY |X ⇥3 ~µX ~⇢y and reshape it back into a density

matrix and divide by its trace. Alternatively  we can rewrite this operation using a vectorized identity
matrix ~I that carries out the full trace in the denominator to renormalize as:

XY ~⇢y =CXY |X ⇥3 ~µX ~⇢y

XX ~µX = CXY |X ⇥3 ~µX

XY = C(XY )XC1
C⇡X

~µX|y /C ⇡X

(19)

~µX|y = CXY |X ⇥3 ~µX ~⇢y
~ITCXY |X ⇥3 ~µX ~⇢y

(20)

Finite Sample Kernel Estimate We kernelize these operations as follows (where (y) = ~⇢y):

~µX|y =

⌥ · diag (↵X) · T (y)
~IT ⌥ · diag (↵X) · T (y)

= Pi ⌥i (↵X)i k(yi  y)
Pj (↵X)j k(yj  y)

Y )i = (↵X )ik(yi y)

where (↵(X)
~I carries out the trace operation. As it happens  this method of estimating the conditional embedding

Pj (↵X )j k(yj  y)  and ~IT ⌥= 1T since ⌥ contains vectorized density matrices  and

=⌥ ↵(X)

Y

(21)

ˆ⇢X

ˆ⇢env

ˆU

ˆu†

ˆ⇢X|y

ˆu

Figure 2: Quantum circuit to compute posterior P (X|y)

5

~µX|y is equivalent to performing Nadaraya-Watson kernel regression [Nadaraya  1964  Watson 
1964] from the joint embedding to the kernel embedding. Note that this result only holds for the
kernels satisfying Equation 4.22 in Wasserman [2006]; importantly  the kernel function must only
output positive numbers. One way to enforce this is by using a squared kernel; this is equivalent to
a 2nd-order polynomial expansion of the features or computing the outer product of features. Our
choice of feature map produces density matrices (as the outer product of features)  so their inner
product in Hilbert space is equivalent to computing the squared kernel  and this constraint is satisﬁed.
3.4 Quantum Bayes Rule as Kernel Bayes Rule
As we discussed at the end of Section 2.2  Figure 1c is an alternate way of generalizing Bayes rule for
QGMs. But following the same approach of rewriting the quantum circuit in the language of Hilbert
Space embeddings as in Section 3.2  we get exactly Kernel Bayes Rule [Song et al.  2013]:

~µX|y = C⇡

XY (C⇡

Y Y )1(y)

(22)

What we have shown thus far As promised  we see that the two different but valid ways of
generalizing Bayes rule for QGMs affects whether we condition according to Kernel Bayes Rule or
Nadaraya-Watson kernel regression. However  we stress that conditioning according to Nadaraya-
Watson is computationally much easier; the kernel Bayes rule given by Song et al. [2013] using Gram
matrices is written:

ˆµX|y =⌥ DKyy((DKyy)2 + I)1DK:y

(23)
where D = diag((Kxx + I)1Kxx↵X). Observe that this update requires squaring and inverting
the Gram matrix Kyy – an expensive operation. By contrast  performing Bayesian update using
Nadaraya-Watson as per Equation 21 is straightforward. This is one of the key insights of this paper;
showing that Nadaraya-Watson kernel regression is an alternate  valid  but simpler way of generalizing
Bayes rule to Hilbert space embeddings. We note that interpreting operations on QGMs as inference
in Hilbert space is a special case; if the feature maps don’t produce density matrices  we can still
perform inference in Hilbert space using the quantum/kernel sum rule  and Nadaraya-Watson/kernel
Bayes rule  but lose the probabilistic interpretation of a quantum graphical model.
4 HSE-HQMMs
We now consider mapping data to vectorized density matrices and modeling the dynamics in Hilbert
space using a speciﬁc quantum graphical model – hidden quantum Markov models (HQMMs). The
quantum circuit for HQMMs is shown in Figure 3 [Srinivasan et al.  2018].

ˆU1

ˆ⇢t1
ˆ⇢Xt

ˆ⇢Yt

ˆ⇢t

ˆU2

ˆu†

ˆu

Figure 3: Quantum Circuit for HSE-HQMM

We use the outer product of random Fourier features (RFF) [Rahimi and Recht  2008] (which produce
a valid density matrix) to map data to a Hilbert space. ˆU1 encodes transition dynamics  ˆU2 encodes
observation dynamics  and ˆ⇢t is the density matrix after a transition update and conditioning on some
observation. The transition and observation equations describing this circuit (with ˆUI = I ⌦ ˆu†) are:
ˆ⇢0t = trˆ⇢t1⇣ ˆU1 (ˆ⇢t1 ⌦ ˆ⇢Xt) ˆU†1⌘ and
ˆU†I⌘ (24)

ˆ⇢t / trYt⇣ ˆUIPy ˆU†I

As we saw in the previous section  we can rewrite these in the language of Hilbert Space embeddings:

ˆU2 (ˆ⇢0t ⌦ ˆ⇢Yt) ˆU†2

ˆUIP †y

And the kernelized version of these operations (where ⌥= ( (x1)  . . .   (xn)) is (see appendix):

~µ0xt = (Cxtxt1C1

xt1xt1)~µxt1

and

↵0xt = (Kxt1xt1 + I)1Kxt1xt↵xt1

6

~µt =

(Cxtyt|xt ⇥3 ~µ0xt)(yt)
~IT (Cxtyt|xt ⇥3 ~µ0xt)(yt)
and ↵xt = Pi ⌥i↵0xti k(yi  y)
Pj↵0xtj k(yj  y)

(25)

(26)

It is also possible to combine the operations setting Cxtyt|xt1 = Cxtyt|xtCxtxt1C1
our single update in Hilbert space:

xt1xt1 to write

~µxt =

(Cxtyt|xt1 ⇥3 ~µxt1)(yt)
~IT (Cxtyt|xt1 ⇥3 ~µxt1)(yt)

(27)

Making Predictions As discussed in Srinivasan et al. [2018]  conditioning on some discrete-
valued observation y in the quantum model produces an unnormalized density matrix whose trace
is the probability of observing y. However  in the case of continuous-valued observations  we can
go further and treat this trace as the unnormalized density of the observation yt  i.e.  fY (yt) /
~IT (Cxtyt|xt1 ⇥3 ~µ0t1)(yt) – the equivalent operation in the language of quantum circuits is the
trace of the unnormalized ˆ⇢t shown in Figure 3. A beneﬁt of building this model using the quantum
formalism is that we can immediately see that this trace is bounded and lies in [0  1]. It is also
straightforward to see that a tighter bound for the unnormalized densities is given by the largest
and smallest eigenvalues of the reduced density matrix ˆ⇢Yt = trXt(ˆ⇢XtYt) where ˆ⇢XtYt is the joint
density matrix after the application of ˆU2.
To make a prediction  we sample from the convex hull of our training set  compute densities as
described  and take the expectation to make a point prediction. This formalism is potentially powerful
as it lets us maintain a whole distribution over the outputs (e.g. Figure 5)  instead of just a point
estimate for the next prediction as with LSTMs. A deeper investigation of the density estimation
properties of our model would be an interesting direction for future work.

Learning HSE-HQMMs We estimate model parameters using 2-stage regression (2SR) [Hefny
et al.  2015]  and reﬁne them using back-propagation through time (BPTT). With this approach  the
learned parameters are not guaranteed to satisfy the quantum constraints  and we handle this by
projecting the state back to a valid density matrix at each time step. Details are given in Algorithm 1

Algorithm 1 Learning Algorithm using Two-Stage Regression for HSE-HQMMs
Input: Data as y1:T = y1  ...  yT
Output: Cross-covariance matrix Cxtyt|xt1  can be reshaped into 3-mode tensor for prediction
1: Compute features of the past (h)  future (f)  shifted future (s) from data (with window k):

ht = h(ytk:t1)

ft = f (yt:t+k)

st = f (yt+1:t+k+1)

2: Project data and features of past  future  shifted future into RKHS using random Fourier features

of desired kernel (feature map (·)) to generate quantum systems:
|fit f (ft)

|hit h(ht)

|yit y(yt)

3: Construct density matrices in the RKHS and vectorize them:

|sit f (st)

~⇢(y)
t = vec (|yithy|t)

~⇢(s)
t = vec (|siths|t)
4: Compose matrices whose columns are the vectorized density matrices  for each available time-

~⇢(h)
t = vec (|hithh|t)

~⇢(f )
t = vec (|fithf|t)

step (accounting for window size k)  denoted y  ⌥h  ⌥f   and ⌥s respectively.

5: Obtain extended future via tensor product ~⇢(s y)
6: Perform Stage 1 regression

and collect into matrix s y.

t

7: Use operators from stage 1 regression to obtain denoised predictive quantum states:

t ~⇢(s)

t ⌦ ~⇢(y)
Cf|h ⌥f ⌥†h⇣⌥h⌥†h + ⌘1
Cs y|h s y⌥†h⇣⌥h⌥†h + ⌘1
˜⌥f|h C f|h⌥h
˜s y|h C s y|h⌥h
Cxtyt|xt1 ˜s y|h ˜⌥†f|h⇣ ˜⌥f|h ˜⌥†f|h + ⌘1

8: Perform Stage 2 regression to obtain model parameters

7

5 Comparison with Previous Work
5.1 HQMMs
Srinivasan et al. [2018] present a maximum-likelihood learning algorithm to estimate the parameters
of a HQMM from data. However  it is very limited in its scope; the algorithm is slow and doesn’t
scale for large datasets. In this paper  we leverage connections to HSEs  kernel methods  and RFFs to
achieve a more practical and scalable learning algorithm for these models. However  one difference to
note is that the algorithm presented by Srinivasan et al. [2018] guaranteed that the learned parameters
would produce valid quantum operators  whereas our algorithm only approximately produces valid
quantum operators; we will need to project the updated state back to the nearest quantum state to
ensure that we are tracking a valid quantum system.

5.2 PSRNNs
Predictive State Recurrent Neural Networks (PSRNNs) [Downey et al.  2017] are a recent state-of-
the-art model developed by embedding a Predictive State Representation (PSR) into an RKHS. The
PSRNN update equation is:

~!t =

W ⇥3 ~!t1 ⇥2 (yt)
||W ⇥3 ~!t1 ⇥2 (yt)||F

(28)

where W is a three mode tensor corresponding to the cross-covariance between observations and
the state at time t conditioned on the state at time t  1  and ! is a factorization of a p.s.d state
matrix µt = !!T (so renormalizing ! by Frobenius norm is equivalent to renormalizing µ by its
trace). There is a clear connection between PSRNNs and the HSE-HQMMs; this matrix µt is what
we vectorize to use as our state ~µt in HSE-HQMMs  and both HSE-HQMMs and PSRNNs are
parameterized (in the primal space using RFFs) in terms of a three-mode tensor (W for PSRNNs
and (Cxtyt|xt1 for HSE-HQMMs). We also note that while PSRNNs modiﬁed kernel Bayes rule
(from Equation 22) heuristically  we have shown that this modiﬁcation can be interpreted as a
generalization of Bayes rule for QGMs or Nadaraya-Watson kernel regression. One key difference
between these approaches is that we directly use states in Hilbert space to estimate the probability
density of observations; in other words HSE-HQMMs are a generative model. By contrast PSRNNs
are a discriminative model which rely on an additional ad-hoc mapping from states to observations.
6 Experiments
We use the following datasets in our experiments1:

• Penn Tree Bank (PTB) Marcus et al. [1993]. We train a character-prediction model with a
train/test split of 120780/124774 characters due to hardware limitations.
• Swimmer Simulated swimmer robot from OpenAI gym2. We collect 25 trajectories from a
robot that is trained to swim forward (via the cross entropy with a linear policy) with a 20/5
train/test split. There are 5 features at each time step: the angle of the robots nose  together
with the 2D angles for each of it’s joints.
• Mocap Human Motion Capture Dataset. We collect 48 skeletal tracks from three human
subjects with a 40/8 train/test split. There are 22 total features at each time step: the 3D
positions of the skeletal parts (e.g.  upper back  thorax  clavicle).

We compare the performance of three models: HSE-HQMMs  PSRNNs  and LSTMs. We initialize
PSRNNs and HSE-HQMMs using Two-Stage Regression (2SR) [Downey et al.  2017] and LSTMs
using Xavier Initialization and reﬁne all three models using Back Propagation Through Time (BPTT).
We optimize and evaluate all models on Swimmer and Mocap with respect to the Mean Squared
Error (MSE) using 10 step predictions as is conventional in the robotics community. This means that
to evaluate the model we perform recursive ﬁltering on the test set to produce states  then use these
states to make predictions about observations 10 steps in the future. We optimize all models on PTB
with respect to Perplexity (Cross Entropy) using 1 step predictions  as is conventional in the NLP
community. As we can see in Figure 4  HSE-HQMMs outperform both PSRNNs and LSTMs on the
swimmer dataset  and achieve comparable performance to the best alternative on Mocap and PTB.
Hyperparameters and other experimental details can be found in Appendix E.

1Code will be made available at https://github.com/cmdowney/hsehqmm
2https://gym.openai.com/

8

Figure 4: Performance of HSE-HQMM  PSRNN  and LSTM on Mocap  Swimmer  and PTB

Visualizing Probability Densities As mentioned previously  HSE-HQMMs can maintain a prob-
ability density function over future observations  and we visualize this for a model trained on the
Mocap dataset in Figure 5. We take the 22 dimensional joint density and marginalize it to produce
three marginal distributions  each over a single feature. We plot the resulting marginal distributions
over time using a heatmap  and superimpose the ground-truth and model predictions. We observe
that BPTT (second row) improves the marginal distribution. Another interesting observation  from
the the last ⇠30 timesteps of the marginal distribution in the top-left image  is that our model is
able to produce a bi-modal distribution with probability mass at both yi = 1.5 and yi = 0.5 
without making any parametric assumptions. This kind of information is difﬁcult to obtain using a
discriminative model such as a LSTM or PSRNN. Additional heatmaps can be found in Appendix D.

Figure 5: Heatmap Visualizing the Probability Densities generated by our HSE-HQMM model.
Red indicates high probability  blue indicates low probability  x-axis corresponds to time  y-axis
corresponds to the feature value. Each column corresponds to the predicted marginal distribution of a
single feature changing with time. The ﬁrst row is the probability distribution after 2SR initialization 
the second row is the probability distribution after the model in row 1 has been reﬁned via BPTT.
7 Conclusion and Future Work
We explored the connections between QGMs and HSEs  and showed that the sum rule and Bayes rule
in QGMs is equivalent to kernel sum rule and a special case of Nadaraya-Watson kernel regression.
We proposed HSE-HQMMs to model dynamics  and showed experimentally that these models are
competitive with LSTMs and PSRNNs on making point predictions  while also being a nonparametric
method for maintaining a probability distribution over continuous-valued features. Looking forward 
we note that our experiments only consider real kernels/features  so we are not utilizing the full
complex Hilbert space; it would be interesting to investigate whether incorporating complex numbers
improves our model. Additionally  by estimating parameters using least-squares  the parameters
only approximately adhere to quantum constraints. The ﬁnal model also bears strong resemblance to
PSRNNs [Downey et al.  2017]. It would be interesting to investigate both what happens if we are
stricter about enforcing quantum constraints  and if we give the model greater freedom to drift from
the quantum constraints. Finally  the density estimation properties of the model are also an avenue
for future exploration.

9

References
B. Boots  A. Gretton  and G. J. Gordon. Hilbert space embeddings of PSRs. NIPS Workshop on Spectral

Algorithms for Latent Variable Models  2012.

B. Boots  G. J. Gordon  and A. Gretton. Hilbert space embeddings of predictive state representations. CoRR 

abs/1309.6819  2013. URL http://arxiv.org/abs/1309.6819.

C. Downey  A. Hefny  B. Li  B. Boots  and G. J. Gordon. Predictive state recurrent neural networks.

Proceedings of Advances in Neural Information Processing Systems (NIPS)  2017.

In

A. Hefny  C. Downey  and G. J. Gordon. Supervised learning for dynamical system learning. In Advances in

neural information processing systems  pages 1963–1971  2015.

M. Leifer and D. Poulin. Quantum graphical models and belief propagation. Ann. Phys.  323:1899  2008.

M. P. Marcus  M. A. Marcinkiewicz  and B. Santorini. Building a large annotated corpus of english: The penn

treebank. Computational linguistics  19(2):313–330  1993.

A. Monras  A. Beige  and K. Wiesner. Hidden quantum Markov models and non-adaptive read-out of many-body

states. arXiv preprint arXiv:1002.2337  2010.

E. A. Nadaraya. On estimating regression. Theory of Probability & Its Applications  9(1):141–142  1964.

M. A. Nielsen and I. Chuang. Quantum computation and quantum information  2002.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in neural information

processing systems  pages 1177–1184  2008.

M. Schuld and N. Killoran. Quantum machine learning in feature Hilbert spaces.

arXiv:1803.07128  2018.

arXiv preprint

A. Smola  A. Gretton  L. Song  and B. Schölkopf. A Hilbert space embedding for distributions. In International

Conference on Algorithmic Learning Theory  pages 13–31. Springer  2007.

L. Song  J. Huang  A. Smola  and K. Fukumizu. Hilbert space embeddings of conditional distributions with
applications to dynamical systems. In Proceedings of the 26th Annual International Conference on Machine
Learning  pages 961–968. ACM  2009.

L. Song  B. Boots  S. M. Siddiqi  G. J. Gordon  and A. J. Smola. Hilbert space embeddings of hidden Markov

models. In Proceedings of the 27th International Conference on Machine Learning (ICML)  2010.

L. Song  K. Fukumizu  and A. Gretton. Kernel embeddings of conditional distributions: A uniﬁed kernel
framework for nonparametric inference in graphical models. IEEE Signal Processing Magazine  30(4):
98–111  2013.

S. Srinivasan  G. J. Gordon  and B. Boots. Learning hidden quantum Markov models. In Proceedings of the 21st

International Conference on Artiﬁcial Intelligence and Statistics  2018.

L. Wasserman. All of Nonparametric Statistics (Springer Texts in Statistics). Springer-Verlag  Berlin  Heidelberg 

2006. ISBN 0387251456.

G. S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics  Series A  pages 359–372 

1964.

C.-H. Yeang. A probabilistic graphical model of quantum systems. In Machine Learning and Applications

(ICMLA)  2010 Ninth International Conference on  pages 155–162. IEEE  2010.

10

,Siddarth Srinivasan
Carlton Downey
Byron Boots