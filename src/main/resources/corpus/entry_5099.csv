2014,Sparse Bayesian structure learning with “dependent relevance determination” priors,In many problem settings  parameter vectors are not merely sparse  but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as “region sparsity”. Classical sparse regression methods  such as the lasso and automatic relevance determination (ARD)  model parameters as independent a priori  and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth  region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework  where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients  which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g.  group lasso and smooth RVM) for both simulated and real datasets from brain imaging.,Sparse Bayesian structure learning with dependent

relevance determination prior

Anqi Wu1

Mijung Park2

Oluwasanmi Koyejo3

Jonathan W. Pillow4

1 4 Princeton Neuroscience Institute  Princeton University 

{anqiw  pillow}@princeton.edu

2 The Gatsby Unit  University College London  mijung@gatsby.ucl.ac.uk

3 Department of Psychology  Stanford University  sanmi@stanford.edu

Abstract

In many problem settings  parameter vectors are not merely sparse  but depen-
dent in such a way that non-zero coefﬁcients tend to cluster together. We re-
fer to this form of dependency as “region sparsity”. Classical sparse regression
methods  such as the lasso and automatic relevance determination (ARD)  model
parameters as independent a priori  and therefore do not exploit such dependen-
cies. Here we introduce a hierarchical model for smooth  region-sparse weight
vectors and tensors in a linear regression setting. Our approach represents a hi-
erarchical extension of the relevance determination framework  where we add a
transformed Gaussian process to model the dependencies between the prior vari-
ances of regression weights. We combine this with a structured model of the prior
variances of Fourier coefﬁcients  which eliminates unnecessary high frequencies.
The resulting prior encourages weights to be region-sparse in two different bases
simultaneously. We develop efﬁcient approximate inference methods and show
substantial improvements over comparable methods (e.g.  group lasso and smooth
RVM) for both simulated and real datasets from brain imaging.

1

Introduction

Recent work in statistics has focused on high-dimensional inference problems where the number of
parameters p equals or exceeds the number of samples n. Although ill-posed in general  such prob-
lems are made tractable when the parameters have special structure  such as sparsity in a particular
basis. A large literature has provided theoretical guarantees about the solutions to sparse regression
problems and introduced a suite of practical methods for solving them efﬁciently [1–7].
The Bayesian interpretation of standard “shrinkage” based methods for sparse regression problems
involves maximum a postieriori (MAP) inference under a sparse  independent prior on the regres-
sion coefﬁcients [8–15]. Under such priors  the posterior has high concentration near the axes  so
the posterior maximum is at zero for many weights unless it is pulled strongly away by the likeli-
hood. However  these independent priors neglect a statistical feature of many real-world regression
problems  which is that non-zero weights tend to arise in clusters  and are therefore not independent
a priori. In many settings  regression weights have an explicit topographic relationship  as when
they index regressors in time or space (e.g.  time series regression  or spatio-temporal neural recep-
tive ﬁeld regression). In such settings  nearby weights exhibit dependencies that are not captured by
independent priors  which results in sub-optimal performance.
Recent literature has explored a variety of techniques for improving sparse inference methods by
incorporating different types of prior dependencies  which we will review here brieﬂy. The smooth
relevance vector machine (s-RVM) extends the RVM to incorporate a smoothness prior deﬁned

1

in a kernel space  so that weights are smooth as well as sparse in a particular basis [16]. The
group lasso captures the tendency for groups of coefﬁcients to remain in or drop out of a model
in a coordinated manner by using an l1 penalty on the l2 norms pre-deﬁned groups of coefﬁcients
[17]. A method described in [18] uses a multivariate Laplace distribution to impose spatio-temporal
coupling between prior variances of regression coefﬁcients  which imposes group sparsity while
leaving coefﬁcients marginally uncorrelated. The literature includes many related methods [19–24] 
although most require a priori knowledge of the dependency structure  which may be unavailable in
many applications of interest.
Here we introduce a novel  ﬂexible method for capturing dependencies in sparse regression prob-
lems  which we call dependent relevance determination (DRD). Our approach uses a Gaussian
process to model dependencies between latent variables governing the prior variance of regres-
sion weights. (See [25]  which independently proposed a similar idea.) We simultaneously impose
smoothness by using a structured model of the prior variance of the weights’ Fourier coefﬁcients.
The resulting model captures sparse  local structure in two different bases simultaneously  yielding
estimates that are sparse as well as smooth. Our method extends previous work on automatic local-
ity determination (ALD) [26] and Bayesian structure learning (BSL) [27]  both of which described
hierarchical models for capturing sparsity  locality  and smoothness. Unlike these methods  DRD
can tractably recover region-sparse estimates with multiple regions of non-zero coefﬁcients  without
pre-deﬁnining number of regions. We argue that DRD can substantially improve structure recovery
and predictive performance in real-world applications.
This paper is organized as follows: Sec. 2 describes the basic sparse regression problem; Sec. 3 in-
troduces the DRD model; Sec. 4 and Sec. 5 describe the approximate methods we use for inference;
In Sec. 6  we show applications to simulated data and neuroimaging data.

2 Problem setup

2.1 Observation model
We consdier a scalar response yi ∈ R linked to an input vector xi ∈ Rp via the linear model:

yi = xi

(cid:62)w + i 

i = 1  2 ···   n 

(1)
with observation noise i ∼ N (0  σ2). The regression (linear weight) vector w ∈ Rp is the quantity
of interest. We denote the design matrix by X ∈ Rn×p  where each row of X is the ith input vector
xi

(cid:62)  and the observation vector by y = [y1 ···   yn](cid:62) ∈ Rn. The likelihood can be written:

for

y|X  w  σ2 ∼ N (y|Xw  σ2I).

2.2 Prior on regression vector

We impose the zero-mean multivariate normal prior on w:
w|θ ∼ N (0  C(θ))

(2)

(3)

where the prior covariance matrix C(θ) is a function of hyperparameters θ. One can specify C(θ)
based on prior knowledge on the regression vector  e.g. sparsity [28–30]  smoothness [16  31]  or
both [26]. Ridge regression assumes C(θ) = θ−1I where θ is a scalar for precision. Automatic rel-
evance determination (ARD) uses a diagonal prior covariance matrix with a distinct hyperparameter
θi for each element of the diagonal  thus Cii = θ−1
. Automatic smoothness determination (ASD)
assumes a non-diagonal prior covariance  given by a Gaussian kernel  Cij = exp(−ρ − ∆ij/2δ2)
where ∆ij is the squared distance between the ﬁlter coefﬁcients wi and wj in pixel space and
θ = {ρ  δ2}. Automatic locality determination (ALD) parametrizes the local region with a Gaus-
sian form  so that prior variance of each ﬁlter coefﬁcient is determined by its Mahalanobis distance
(in coordinate space) from some mean location ν under a symmetric positive semi-deﬁnite matrix
Ψ. The diagonal prior covariance matrix is given by Cii = exp(− 1
2 (χi − ν)(cid:62)Ψ−1(χi − ν)))  where
χi is the space-time location (i.e.  ﬁlter coordinates) of the ith ﬁlter coefﬁcient wi and θ = {ν  Ψ}.

i

2

3 Dependent relevance determination (DRD) priors

We formulate the prior covariances to capture the region dependent sparsity in the regression vector
in the following.

Sparsity inducing covariance

We ﬁrst parameterise the prior covariance to capture region sparsity in w

Cs = diag[exp(u)] 

(4)

where the parameters are u ∈ Rp. We impose the Gaussian process (GP) hyperprior on u

u ∼ N (b1  K).

(5)
The GP hyperprior is controlled by the mean parameter b ∈ R and the squared exponential kernel
parameters  overall scale ρ ∈ R and the length scale l ∈ R. We denote the hyperparameters by
θs = {b  ρ  l}. We refer to the prior distribution associated with the covariance Cs as dependent
relevance determination (DRD) prior.
Note that this hyperprior induces dependencies between the ARD precisions  that is  prior variance
changes slowly between neighboring coefﬁcients. If the ith coefﬁcient of u has large prior variance 
then probably the i + 1 and i − 1 coefﬁcients are large as well.

Smoothness inducing covariance

We then formulate the smoothness inducing covariance in frequency domain. Smoothness is cap-
tured by a low pass ﬁlter with only lower frequencies passing through. Therefore  we deﬁne a zero-
mean Gaussian prior over the Fourier-transformed weights w using a diagonal covariance matrix
Cf with diagonal

Cf ii = exp(− χ2

i

2δ2 ) 

(6)

where χi is the ith location of the regression weights w in frequency domain and δ2 is the Gaussian
covariance. We denote the hyperparameters by θf = δ2. This formulation imposes neighboring
weights to have similar levels of Fourier power.
Similar to the automatic determination in frequency coordinates (ALDf) [26]  this way of formulat-
ing the covariance requires taking discrete Fourier transform of input vectors to construct the prior in
the frequency domain. This is a signiﬁcant consumption in computation and memory requirements
especially when p is large. To avoid the huge expense  we abandon the single frequency version Cf
but combine it with Cs to form Csf with both sparsity and smoothness induced as the following.

Smoothness and region sparsity inducing covariance

Finally  to capture both region sparsity and smoothness in w  we combine Cs and Cf in the following
way

Csf = C

1
2

s B(cid:62)Cf BC

1
2

s  

(7)

where B is the Fourier transformation matrix which could be huge when p is large. Implementation
exploits the speed of the FFT to apply B implicitly. This formulation implies that the sparse regions
that are captured by Cs are pruned out and the variance of the remaining entries in weights are
correlated by Cf . We refer to the prior distribution associated with the covariance Csf as smooth
dependent relevance determination (sDRD) prior.
Unlike Cs  the covariance Csf is no longer diagonal. To reduce computational complexity and
storage requirements  we only store those values that correspond to non-zero portions in the diagonal
of Cs and Cf from the full Csf .

3

Figure 1: Generative model for locally smooth and glob-
ally sparse Bayesian structure learning. The ith response
yi is linked to an input vector xi and a weight vector w
in each i. The weight vector w is governed by u and θf .
The hyper-prior p(u|θs) imposes correlated sparsity in w
and the hyperparameters θf imposes smoothness in w.

4 Posterior inference for w
First  we denote the overall hyperparameter set to be θ = {σ2  θs  θf} = {σ2  b  ρ  l  δ2}. We
compute the maximum likelihood estimate for θ (denoted by ˆθ) and compute the conditional MAP
estimate for the weights w given ˆθ (closed form)  which is the empirical Bayes procedure equipped
with a hyper-prior. Our goal is to infer w. The posterior distribution over w is obtained by

p(w|X  y) =

p(w  u  θ|X  y)dudθ 

(8)

(cid:90) (cid:90)

which is analytically intractable. Instead  we approximate the marginal posterior distribution with
the conditional distribution given the MAP estimate of u  denoted by µu  and the maximum likeli-
hood estimation of σ2  θs  θf denoted by ˆσ2  ˆθs  ˆθf  

p(w|X  y) ≈ p(w|X  y  µu  ˆσ2  ˆθs  ˆθf ).

(9)

The approximate posterior over w is multivariate normal with the mean and covariance given by

p(w|X  y  µu  ˆσ2  ˆθs  ˆθf ) = N (µw  Λw) 

Λw = (

µw = 1

X(cid:62)X + C−1

1
ˆσ2
ˆσ2 ΛwX T y.

µu  ˆθs  ˆθf

)−1 

(10)

(11)

(12)

Inference for hyperparameters

5
The MAP inference of w derived in the previous section depends on the values of ˆθ = { ˆσ2  ˆθs  ˆθf}.
To estimate ˆθ  we maximize the marginal likelihood of the evidence.

ˆθ = arg max

log p(y|X  θ)

θ

where

p(y|X  θ) =

(cid:90) (cid:90)

p(y|X  w  σ2)p(w|u  θf )p(u|θs)dwdu.

(13)

(14)

Unfortunately  computing the double integrals is intractable. In the following  we consider the the
approximation method based on Laplace approximation to compute the integral approximately.

Laplace approximation to posterior over u

To approximate the marginal likelihood  we can rewrite Bayes’ rule to express the marginal likeli-
hood as the likelihood times prior divided by the posterior 

p(y|X  θ) =

p(y|X  u)p(u|θ)

(15)
Laplace’s method allows us to approximate p(u|y  X  θ)  the posterior over the latent u given
the data {X  y} and hyper-parameters θ  using a Gaussian centered at the mode of the distri-
bution and inverse covariance given by the Hessian of the negative log-likelihood. Let µu =
arg maxu p(u|y  X  θ) and Λu = −(
∂u∂u(cid:62) log p(u|y  X  θ))−1 denote the mean and covariance

p(u|y  X  θ)

∂2

 

4

Figure 2: Comparison of estimators for 1D simulated example. First column: True ﬁlter
weight  maximum likelihood (linear regression) estimate  empirical Bayesian ridge regression (L2-
penalized) estimate; Second column: ARD estimate with different and independent prior covari-
ance hyperparameters  lasso regression with L1-regularization and group lasso with group size of 5;
Third column: ALD methods in space-time domain  frequency domain and combination of both  re-
spectively; Fourth column: DRD method in space-time domain only and its smooth version sDRD
imposing both sparsity (space-time) and smoothness (frequency)  and smooth RVM initialized with
elastic net estimate.

of this Gaussian  respectively. Although the right-hand-side can be evaluated at any value of u  a
common approach is to use the mode u = µu  since this is where the Laplace approximation is
most accurate. This leads to the following expression for the log marginal likelihood:
2 log |2πΛu|.

(16)
Then by optimizing log p(y|X  θ) with regard to θ  we can get ˆθ given a ﬁxed µu  de-
noted as ˆθµu. Following an iterative optimization procedure  we obtain an updating rule µt
u =
arg maxu p(u|y  X  ˆθµt−1
) at tth iteration. The algorithm will stop when u and θ converge. More
information and details about formulation and derivation are described in the appendix.

log p(y|X  θ) ≈ log p(y|X  µu) + log p(µu|θ) − 1

u

6 Experiment and Results

6.1 One Dimensional Simulated Data

Beginning with simulated data  we compare performances of various regression estimators. One
dimensional data is generated from a generative model with d = 200 dimensions. Firstly to generate
a Gaussian process  a covariance kernel matrix K is built with squared exponential kernel with the
spatial locations of regression weights as inputs. Then a scalar b is set as the mean function to
determine the scale of prior covariance. Given the Gaussian process  we generate a multivariate
vector u  and then take its exponential to obtain the diagonal of prior covariance Cs in space-time
domain. To induce smoothness  eq. 7 is introduced to get covariance Csf . Then a weight vector w
is sampled from a Gaussian distribution with zero mean and Csf . Finally  we obtain the response
y given stimulus x with w plus Gaussian noise . In our case   should be large enough to ensure
that data and response won’t impose strong likelihood over prior knowledge. Thus the introduced
prior would largely dominate the estimate. Three local regions are constructed  which are positive 
negative and a half-positive-half-negative with sufﬁcient zeros between separate bumps clearly apart.
As shown in Figure 2  the left top subﬁgure shows the underlying weight vector w.
Traditional methods like maximum likelihood  without any prior  are signiﬁcantly overwhelmed by
large noise of the data. Weak priors such as ridge  ARD  lasso could ﬁt the true weight better with

5

Figure 3: Estimated ﬁlter weights
and prior covariances. Upper row
shows the true ﬁlter (dotted black)
and estimated ones (red); Bottom
row shows the underlying prior co-
variance matrix.

different levels of sparsity imposed  but are still not sparse enough and not smooth at all. Group
lasso enforces a stronger sparsity than lasso by assuming block sparsity  thus making the result
smoother locally. ALD based methods have better performance  compared with traditional ones  in
identifying one big bump explicitly. ALDs is restricted by the assumption of one modal Gaussian 
therefore is able to ﬁnd one dominating local region. ALDf focuses localities in frequency domain
thus make the estimate smoother but no spatial local regions are discovered. ALDsf combines
the effects in both ALDs and ALDf  and thus possesses smoothness but only one region is found.
Smooth Relevance Vector Machine (sRVM) can smooth the curve by incorporating a ﬂexible noise-
dependent smoothness prior into the RVM  but is not able to draw information from data likelihood
magniﬁcently. Our DRD can impose distinct local sparsity via Gaussian process prior and sDRD can
induce smoothness via bounding the frequencies. For all baseline models  we do model selection
via cross-validation varying through a wide range of parameter space  thus we can guarantee the
fairness for comparisons.
To further illustrate the beneﬁts and principles of DRD  we demonstrate the estimated covariance
via ARD  ALDsf and sDRD in Figure 3. It can be stated that ARD could detect multiple localities
since its priors are purely independent scalars which could be easily inﬂuenced by data with strong
likelihood  but the consideration is the loss of dependency and smoothness. ALDsf can only detect
one locality due to its deterministic Gaussian form when likelihood is not sufﬁciently strong  but
with Fourier components over the prior  it exhibits smoothness. sDRD could capture multiple local
sparse regions as well as impose smoothness. The underlying Gaussian process allows multiple
non-zero regions in prior covariance with the result of multiple local sparsities for weight tensor.
Smoothness is introduced by a Gaussian type of function controlling the frequency bandwidth and
direction.
In addition  we examine the convergence properties of various estimators as a function of the amount
of collected data and give the average relative errors of each method in Figure 4. Responses are
simulated from the same ﬁlter as above with large Gaussian white noise which weakens the data
likelihood and thus guarantees a signiﬁcant effect of prior over likelihood. The results show that
sDRD estimate achieves the smallest MSE (mean squared error)  regardless of the number of training
samples. The MSE  mentioned here and in the following paragraphs  refers to the error compared
with the underlying w. The test error  which will be mentioned in later context  refers to the error
compared with true y. The left plot in Figure 4 shows that other methods require at least 1-2 times
more data than sDRD to achieve the same error rate. The right ﬁgure shows the ratio of the MSE for
each estimate to the MSE for sDRD estimate  showing that the next best method (ALDsf) exhibits
an error nearly two times of sDRD.

6.2 Two Dimensional Simulated Data

To better illustrate the performance of DRD and lay the groundwork for real data experiment  we
present a 2-dimensional synthetic experiment. The data is generated to match characteristics of
real fMRI data  as will be outlined in the next section. With a similar generation procedure as in 1-
dimensional experiment  a 2-dimensional w is generated with analogical properties as the regression
weights in fMRI data. The analogy is based on reasonable speculation and accumulated acknowl-
edge from repeated trials and experiment. Two comparative studies are conducted to investigate the
inﬂuences of sample size on the recovery accuracy of w and predictive ability  both with dimension
= 1600 (the same as fMRI). To demonstrate structural sparsity recovery  we only compare our DRD
method with ARD  lasso  elastic net (elnet)  group lasso (glasso).

6

Figure 4: Convergence of error rates on simulated data with varying training size (Left) and the
relative error (MSE ratio) for sDRD (Right)

Figure 5: Test error for each method when n = 215 (Left) and n = 800 (Right) for 2D simulated
data.

The sample size n varies in {215  800}. The results are shown in Fig. 5 and Fig. 6. When n = 215 
only DRD is able to reveal an approximative estimation of true w with a small level of noise as well
as giving the lowest predictive error. Group lasso performs slightly better than ARD  lasso and elnet 
and presents only a weakly distinct block wise estimation compared with lasso and elnet. Lasso
and elnet both show similar performances and give a stronger sparsity than ARD  which indicates
that ARD fails to impose strong sparsity in this synthetic case due to its complete independencies
among dimensions when data is less sufﬁcient and noisy. When n = 800  DRD still holds the
best prediction. Group lasso fails to keep the record since block-wise penalty can capture group
information but miss the subtlety when ﬁner details matter. ARD progresses to the second place
because when data likelihood is strong enough  posterior of w won’t be greatly inﬂuenced by the
noise but follow the likelihood and the prior. Additionally  since ARD’s prior is more ﬂexible and
independent than lasso and elnet  the posterior would approximate the underlying w better and ﬁner.

6.3

fMRI Data

We analyzed functional MRI data from the Human Connectome Project 1 collected from 215 healthy
adult participants on a relational reasoning task. We used contrast images for the comparison of re-
lational reasoning and matching tasks. Data were processed using the HCP minimal preprocessing
pipelines [32]  down-sampled to 63× 76× 63 voxels using the ﬂirt applyXfm tool [33]  then tailored
further down to 40 × 76 × 40 by deleting zero-signal regions outside the brain. We analyzed 215
samples  each of which is an average from Z-slice 37 to 39 slices of 3D structure based on recom-
mendations by domain experts. As the dependent variable in the regression  we selected the number
of correct responses on the Penn Matrix Text  which is a measure of ﬂuid intelligence that should be
related to relational reasoning performance.
In each run  we randomly split the fMRI data into ﬁve sets for ﬁve-fold cross-validation  and took
an average of test errors across ﬁve folds for each run. Hyperparameters were chosen by a ﬁve-fold
cross-validation within the training set  and the optimal hyper parameter set was used for computing
test performance. Fig. 7 shows the regions of positive (red) and negative (blue) support for the
regression weights we obtained using different sparse regression methods. The rightmost panel
quantiﬁes performance using mean squared error on held out test data. Both predictive performance
and estimated pattern are similar to n = 215 result in 2D synthetic experiment. ARD returns a quite
noisy estimation due to the complete independencies and weak likelihood. The elastic net estimate
improves slightly over lasso but is signiﬁcantly better than ARD  which indicates that lasso type
of regularizations impose stronger sparsity than ARD in this case. Group lasso is slightly better

1http://www.humanconnectomeproject.org/.

7

Figure 6: Surface plot of estimated w from each method using 2-dimensional simulated data when
n = 215.

Figure 7: Positive (red) and negative (blue) supports of the estimated weights from each method
using real fMRI data and the corresponding test errors.

because of its block-wise regularization  but more noisy blocks pop up inﬂuencing the predictive
ability. DRD reveals strong sparsity as well as clustered local regions. It also possesses the smallest
test error indicating the best predictive ability. Given that local group information most likely gather
around a few pixels in fMRI data  smoothness would be less valuable to be induced. This is the
reason that sDRD doesn’t show a distinct outperforming result over DRD  as a result of which we
omit smoothness imposing comparative experiment for fMRI data. In addition  we also test the
StructOMP [24] method for both 2D simulated data and fMRI data  but it doesn’t show satisfactory
estimation and predictive ability in the 2D data with our data’s intrinsic properties. Therefore we
chose to not show it for comparison in this study.

7 Conclusion

We proposed DRD  a hierarchal model for smooth and region-sparse weight tensors  which uses a
Gaussian process to model spatial dependencies in prior variances  an extension of the relevance
determination framework. To impose smoothness  we also employed a structured model of the
prior variances of Fourier coefﬁcients  which allows for pruning of high frequencies. Due to the
intractability of marginal likelihood integration  we developed an efﬁcient approximate inference
method based on Laplace approximation  and showed substantial improvements over comparable
methods on both simulated and fMRI real datasets. Our method yielded more interpretable weights
and indeed discovered multiple sparse regions that were not detected by other methods. We have
shown that DRD can gracefully incorporate structured dependencies to recover smooth  region-
sparse weights without any speciﬁcation of groups or regions  and believe it will be useful for other
kinds of high-dimensional datasets from biology and neuroscience.

Acknowledgments

This work was supported by the McKnight Foundation (JP)  NSF CAREER Award IIS-1150186
(JP)  NIMH grant MH099611 (JP) and the Gatsby Charitable Foundation (MP).

8

References
[1] R. Tibshirani. Journal of the Royal Statistical Society. Series B  pages 267–288  1996.
[2] H. Lee  A. Battle  R. Raina  and A. Ng. In NIPS  pages 801–808  2006.
[3] H. Zou and T. Hastie. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 

67(2):301–320  2005.

[4] B. Efron  T. Hastie  I. Johnstone  and et al. Tibshirani  R. Least angle regression. The Annals of statistics 

32(2):407–499  2004.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] G. Yuan  K. Chang  C. Hsieh  and C. Lin. JMLR  11:3183–3234  2010.
[7] F. Bach  R. Jenatton  J. Mairal  and et al. Obozinski  G. Convex optimization with sparsity-inducing

norms. Optimization for Machine Learning  pages 19–53  2011.

[8] R. Neal. Bayesian learning for neural networks. PhD thesis  University of Toronto  1995.
[9] M. Tipping. Sparse bayesian learning and the relevance vector machine. JMLR  1:211–244  2001.
[10] D. MacKay. Bayesian non-linear modeling for the prediction competition.

In Maximum Entropy and

Bayesian Methods  pages 221–234. Springer  1996.

[11] T. Mitchell and J. Beauchamp. Bayesian variable selection in linear regression. JASA  83(404):1023–

1032  1988.

[12] E. George and R. McCulloch. Variable selection via gibbs sampling. JASA  88(423):881–889  1993.
[13] C. Carvalho  N. Polson  and J. Scott. Handling sparsity via the horseshoe. In International Conference

on Artiﬁcial Intelligence and Statistics  pages 73–80  2009.

[14] C. Hans. Bayesian lasso regression. Biometrika  96(4):835–845  2009.
[15] B. Anirban  P. Debdeep  P. Natesh  and David D. Bayesian shrinkage. December 2012.
[16] A. Schmolck. Smooth Relevance Vector Machines. PhD thesis  University of Exeter  2008.
[17] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.

[18] M. Van Gerven  B. Cseke  F. De Lange  and T. Heskes. Efﬁcient bayesian multivariate fmri analysis using

a sparsifying spatio-temporal prior. NeuroImage  50(1):150–161  2010.

[19] J. Friedman  T. Hastie  and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv

preprint arXiv:1001.0736  2010.

[20] L. Jacob  G. Obozinski  and J. Vert. Group lasso with overlap and graph lasso. In Proceedings of the 26th

Annual International Conference on Machine Learning  pages 433–440. ACM  2009.

[21] H. Liu  L. Wasserman  and J. Lafferty. Nonparametric regression and classiﬁcation with joint sparsity

constraints. In NIPS  pages 969–976  2009.

[22] R. Jenatton  J. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms. JMLR 

12:2777–2824  2011.

[23] S. Kim and E. Xing. Statistical estimation of correlated genome associations to a quantitative trait net-

work. PLoS genetics  5(8):e1000587  2009.

[24] J. Huang  T. Zhang  and D. Metaxas. Learning with structured sparsity. JMLR  12:3371–3412  2011.
[25] B. Engelhardt and R. Adams. Bayesian structured sparsity from gaussian ﬁelds.

arXiv preprint

arXiv:1407.2235  2014.

[26] M. Park and J. Pillow. Receptive ﬁeld inference with localized priors. PLoS computational biology 

7(10):e1002219  2011.

[27] M. Park  O. Koyejo  J. Ghosh  R. Poldrack  and J. Pillow. In Proceedings of the Sixteenth International

Conference on Artiﬁcial Intelligence and Statistics  pages 489–497  2013.

[28] M. Tipping. Sparse Bayesian learning and the relevance vector machine. JMLR  1:211–244  2001.
[29] A. Tipping and A. Faul. Analysis of sparse bayesian learning. NIPS  14:383–389  2002.
[30] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In NIPS  2007.
[31] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions.

NIPS  pages 317–324  2003.

[32] M. Glasser  S. Sotiropoulos  A. Wilson  T. Coalson  B. Fischl  J. Andersson  J. Xu  S. Jbabdi  M. Webster 

and et al. Polimeni  J. NeuroImage  2013.

[33] N.M. Alpert  D. Berdichevsky  Z. Levin  E.D. Morris  and A.J. Fischman. Improved methods for image

registration. NeuroImage  3(1):10 – 18  1996.

9

,Evan Archer
Il Memming Park
Jonathan Pillow
Anqi Wu
Mijung Park
Oluwasanmi Koyejo