2016,Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators,We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density  including entropies and divergences. Rather than plugging a consistent density estimate (which requires k → ∞ as the sample size n → ∞) into the functional of interest  the estimators we consider fix k and perform a bias correction. This can be more efficient computationally  and  as we show  statistically  leading to faster convergence rates. Our framework unifies several previous estimators  for most of which ours are the first finite sample guarantees.,Finite-Sample Analysis of Fixed-k Nearest Neighbor

Density Functional Estimators

Shashank Singh

Statistics & Machine Learning Departments

Carnegie Mellon University
sss1@andrew.cmu.edu

Barnabás Póczos

Machine Learning Departments

Carnegie Mellon University
bapoczos@cs.cmu.edu

Abstract

We provide ﬁnite-sample analysis of a general framework for using k-nearest neigh-
bor statistics to estimate functionals of a nonparametric continuous probability
density  including entropies and divergences. Rather than plugging a consistent
density estimate (which requires k → ∞ as the sample size n → ∞) into the
functional of interest  the estimators we consider ﬁx k and perform a bias cor-
rection. This is more efﬁcient computationally  and  as we show in certain cases 
statistically  leading to faster convergence rates. Our framework uniﬁes several
previous estimators  for most of which ours are the ﬁrst ﬁnite sample guarantees.

Introduction

1
Estimating entropies and divergences of probability distributions in a consistent manner is of im-
portance in a number of problems in machine learning. Entropy estimators have applications in
goodness-of-ﬁt testing [13]  parameter estimation in semi-parametric models [51]  studying fractal
random walks [3]  and texture classiﬁcation [14  15]. Divergence estimators have been used to
generalize machine learning algorithms for regression  classiﬁcation  and clustering from inputs in
RD to sets and distributions [40  33].
Divergences also include mutual informations as a special case; mutual information estimators have
applications in feature selection [35]  clustering [2]  causality detection [16]  optimal experimental
design [26  38]  fMRI data analysis [7]  prediction of protein structures [1]  and boosting and facial
expression recognition [41]. Both entropy estimators and mutual information estimators have been
used for independent component and subspace analysis [23  47  37  17]  as well as for image
registration [14  15]. Further applications can be found in [25].
This paper considers the more general problem of estimating functionals of the form

F (P ) := E
X∼P

[f (p(X))]  

(1)

using n IID samples from P   where P is an unknown probability measure with smooth density
function p and f is a known smooth function. We are interested in analyzing a class of nonparametric
estimators based on k-nearest neighbor (k-NN) distance statistics. Rather than plugging a consistent
estimator of p into (1)  which requires k → ∞ as n → ∞  these estimators derive a bias correction
for the plug-in estimator with ﬁxed k; hence  we refer to this type of estimator as a ﬁxed-k estimator.
Compared to plug-in estimators  ﬁxed-k estimators are faster to compute. As we show  ﬁxed-k
estimators can also exhibit superior rates of convergence.
As shown in Table 1  several authors have derived bias corrections necessary for ﬁxed-k estimators of
entropies and divergences  including  most famously  the Shannon entropy estimator of [20]. 1 The
estimators in Table 1 estimators are known to be weakly consistent  2 but  except for Shannon entropy 
1MATLAB code for these estimators is in the ITE toolbox https://bitbucket.org/szzoli/ite/ [48].
2Several of these proofs contain errors regarding the use of integral convergence theorems when their

conditions do not hold  as described in [39].

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

E [log p(X)]

Functional Name
Shannon Entropy
Rényi-α Entropy
KL Divergence

E(cid:2)pα−1(X)(cid:3)
E(cid:104)
(cid:105)
(cid:20)(cid:16) p(X)
(cid:17)α−1(cid:21)
expectations are over X ∼ P . Γ(t) =(cid:82) ∞

α-Divergence

log p(X)
q(X)

q(X)

E

Functional Form Bias Correction

Additive constant: ψ(n) − ψ(k) + log(k/n)
Multiplicative constant:
None∗

Γ(k+1−α)

Γ(k)

Ref.
[20][13]
[25  24]
[50]

Multiplicative constant:

Γ2(k)

Γ(k−α+1)Γ(k+α−1)

[39]

Table 1: Functionals with known bias-corrected k-NN estimators  their bias corrections  and references. All
dx log (Γ(x)) is
the digamma function. α ∈ R\{1} is a free parameter. ∗For KL divergence  bias corrections for p and q cancel.

0 xt−1e−x dx is the gamma function  and ψ(x) = d

no ﬁnite sample bounds are known. The main goal of this paper is to provide ﬁnite-sample analysis
of these estimators  via uniﬁed analysis of the estimator after bias correction. Speciﬁcally  we show
conditions under which  for β-Hölder continuous (β ∈ (0  2]) densities on D dimensional space  the

bias of ﬁxed-k estimators decays as O(cid:0)n−β/D(cid:1) and the variance decays as O(cid:0)n−1(cid:1)  giving a mean
squared error of O(cid:0)n−2β/D + n−1(cid:1). Hence  the estimators converge at the parametric O(n−1) rate

when β ≥ D/2  and at the slower rate O(n−2β/D) otherwise. A modiﬁcation of the estimators would
be necessary to leverage additional smoothness for β > 2  but we do not pursue this here. Along the
way  we prove a ﬁnite-sample version of the useful fact [25] that (normalized) k-NN distances have
an Erlang asymptotic distribution  which may be of independent interest.
We present our results for distributions P supported on the unit cube in RD because this signiﬁcantly
simpliﬁes the statements of our results  but  as we discuss in the supplement  our results generalize
fairly naturally  for example to distributions supported on smooth compact manifolds. In this context 
it is worth noting that our results scale with the intrinsic dimension of the manifold. As we discuss
later  we believe deriving ﬁnite sample rates for distributions with unbounded support may require a
truncated modiﬁcation of the estimators we study (as in [49])  but we do not pursue this here.

2 Problem statement and notation
Let X := [0  1]D denote the unit cube in RD  and let µ denote the Lebesgue measure. Suppose P is an
unknown µ-absolutely continuous Borel probability measure supported on X   and let p : X → [0 ∞)
denote the density of P . Consider a (known) differentiable function f : (0 ∞) → R. Given n
samples X1  ...  Xn drawn IID from P   we are interested in estimating the functional

F (P ) := E
X∼P

[f (p(X))] .

Somewhat more generally (as in divergence estimation)  we may have a function f : (0 ∞)2 → R
of two variables and a second unknown probability measure Q  with density q and n IID samples
Y1  ...  Yn. Then  we are interested in estimating
F (P  Q) := E
X∼P

[f (p(X)  q(X))] .

Fix r ∈ [1 ∞] and a positive integer k. We will work with distances induced by the r-norm

(cid:107)x(cid:107)r :=

xr
i

and deﬁne

cD r :=

(2Γ(1 + 1/r))D

Γ(1 + D/r)

= µ(B(0  1)) 

where B(x  ε) := {y ∈ RD : (cid:107)x − y(cid:107)r < ε} denotes the open radius-ε ball centered at x. Our
estimators use k-nearest neighbor (k-NN) distances:
Deﬁnition 1. (k-NN distance): Given n IID samples X1  ...  Xn from P   for x ∈ RD  we deﬁne the
k-NN distance εk(x) by εk(x) = (cid:107)x− Xi(cid:107)r  where Xi is the kth-nearest element (in (cid:107)·(cid:107)r) of the set
{X1  ...  Xn} to x. For divergence estimation  given n samples Y1  ...  Yn from Q  then we similarly
deﬁne δk(x) by δk(x) = (cid:107)x − Yi(cid:107)r  where Yi is the kth-nearest element of {Y1  ...  Yn} to x.
µ-absolute continuity of P precludes the existence of atoms (i.e.  ∀x ∈ RD  P ({x}) = µ({x}) = 0).
Hence  each εk(x) > 0 a.s. We will require this to study quantities such as log εk(x) and 1/εk(x).

(cid:33)1/r

(cid:32) D(cid:88)

i=1

2

3 Estimator
3.1 k-NN density estimation and plug-in functional estimators
The k-NN density estimator

ˆpk(x) =

k/n

µ(B(x  εk(x))

=

k/n
cDεD
k (x)

is well-studied nonparametric density estimator [28]  motivated by noting that  for small ε > 0 

p(x) ≈ P (B(x  ε))
µ(B(x  ε))

 

and that  P (B(x  εk(x))) ≈ k/n. One can show that  for x ∈ RD at which p is continuous  if
k → ∞ and k/n → 0 as n → ∞  then ˆpk(x) → p(x) in probability ([28]  Theorem 3.1). Thus  a
natural approach for estimating F (P ) is the plug-in estimator

1
n

n(cid:88)
β+D  1}(cid:17)

i=1

ˆFP I :=

f (ˆpk(Xi)) .

(2)

(cid:16)

n

− min{ 2β

Since ˆpk → p in probability pointwise as k  n → ∞ and f is smooth  one can show ˆFP I is consistent 
and in fact derive ﬁnite sample convergence rates (depending on how k → ∞). For example  [44]
show a convergence rate of O
for β-Hölder continuous densities (after sample
splitting and boundary correction) by setting k (cid:16) n
Unfortunately  while necessary to ensure V [ˆpk(x)] → 0  the requirement k → ∞ is computationally
burdensome. Furthermore  increasing k can increase the bias of ˆpk due to over-smoothing (see (5)
below)  suggesting that this may be sub-optimal for estimating F (P ). Indeed  similar work based on
kernel density estimation [42] suggests that  for plug-in functional estimators  under-smoothing may
be preferable  since the empirical mean results in additional smoothing.

β+d .

β

3.2 Fixed-k functional estimators
An alternative approach is to ﬁx k as n → ∞. Since ˆFP I is itself an empirical mean  unlike V [ˆpk(x)] 

(cid:105) → 0 as n → ∞. The more critical complication of ﬁxing k is bias. Since f is typically

V(cid:104) ˆFP I

non-linear  the non-vanishing variance of ˆpk translates into asymptotic bias. A solution adopted by
several papers is to derive a bias correction function B (depending only on known factors) such that

(cid:20)

(cid:18)

(cid:18)

B

f

(cid:19)(cid:19)(cid:21)

(cid:20)

(cid:18) P (B(x  εk(x)))

(cid:19)(cid:21)

µ(B(x  εk(x))

E

X1 ... Xn

k/n

µ(B(x  εk(x))

=

E

X1 ... Xn

f

For continuous p  the quantity

pεk(x)(x) :=

P (B(x  εk(x)))
µ(B(x  εk(x))

.

(3)

(4)

is a consistent estimate of p(x) with k ﬁxed  but it is not computable  since P is unknown. The bias
correction B gives us an asymptotically unbiased estimator

ˆFB(P ) :=

1
n

B (f (ˆpk(Xi))) =

1
n

k/n

µ(B(Xi  εk(Xi))

(cid:18)

(cid:18)

B

f

n(cid:88)

i=1

(cid:19)(cid:19)

.

n(cid:88)

i=1

that uses k/n in place of P (B(x  εk(x))). This estimate extends naturally to divergences:

ˆFB(P  Q) :=

1
n

B (f (ˆpk(Xi)  ˆqk(Xi))) .

n(cid:88)

i=1

As an example  if f = log (as in Shannon entropy)  then it can be shown that  for any continuous p 

E [log P (B(x  εk(x)))] = ψ(k) − ψ(n).

3

Hence  for Bn k := ψ(k) − ψ(n) + log(n) − log(k) 

(cid:20)

(cid:18)

(cid:19)(cid:21)

E

X1 ... Xn

f

k/n

µ(B(x  εk(x))

+ Bn k =

E

X1 ... Xn

(cid:20)

f

(cid:18) P (B(x  εk(x)))

(cid:19)(cid:21)

µ(B(x  εk(x))

.

giving the estimator of [20]. Other examples of functionals for which the bias correction is known
are given in Table 1.
In general  deriving an appropriate bias correction can be quite a difﬁcult problem speciﬁc to the
functional of interest  and it is not our goal presently to study this problem; rather  we are interested in
bounding the error of ˆFB(P )  assuming the bias correction is known. Hence  our results apply to all
of the estimators in Table 1  as well as any estimators of this form that may be derived in the future.

4 Related work
4.1 Estimating information theoretic functionals
Recently  there has been much work on analyzing estimators for entropy  mutual information 
divergences  and other functionals of densities. Besides bias-corrected ﬁxed-k estimators  most of
this work has taken one of three approaches. One series of papers [27  42  43] studied a boundary-
corrected plug-in approach based on under-smoothed kernel density estimation. This approach has
strong ﬁnite sample guarantees  but requires prior knowledge of the support of the density  and can
have a slow rate of convergence. A second approach [18  22] uses von Mises expansion to partially
correct the bias of optimally smoothed density estimates. This is statistically more efﬁcient  but can
require computationally demanding numerical integration over the support of the density. A ﬁnal line
of work [30  31  44  46] studied plug-in estimators based on consistent  boundary corrected k-NN
density estimates (i.e.  with k → ∞ as n → ∞). [32] study a divergence estimator based on convex
risk minimization  but this relies of the context of an RKHS  making results are difﬁcult to compare.
Rates of Convergence: For densities over RD satisfying a Hölder smoothness condition parametrized
by β ∈ (0 ∞)  the minimax mean squared error rate for estimating functionals of the form
. [22] recently derived iden-

(cid:82) f (p(x)) dx has been known since [6] to be O

4β+D  1}(cid:17)

− min{ 8β

(cid:16)

n

tical minimax rates for divergence estimation.

n

Most of the above estimators have been shown to converge at the rate O
. Only
the von Mises approach [22] is known to achieve the minimax rate for general β and D  but due to
its computational demand (O(2Dn3))  3 the authors suggest using other statistically less efﬁcient
estimators for moderate sample size. Here  we show that  for β ∈ (0  2]  bias-corrected ﬁxed-k
estimators converge at the relatively fast rate O
. For β > 2  modiﬁcations are
needed for the estimator to leverage the additional smoothness of the density. Notably  this rate is
adaptive; that is  it does not require selecting a smoothing parameter depending on the unknown β;
our results (Theorem 5) imply the above rate is achieved for any ﬁxed choice of k. On the other hand 
since no empirical error metric is available for cross-validation  parameter selection is an obstacle for
competing estimators.

D  1}(cid:17)
n− min{ 2β

(cid:16)

(cid:16)

β+D  1}(cid:17)

− min{ 2β

4.2 Prior analysis of ﬁxed-k estimators
As of writing this paper  the only ﬁnite-sample results for ˆFB(P ) were those of [5] for the Kozachenko-
Leonenko (KL) 4 Shannon entropy estimator. [20] Theorem 7.1 of [5] shows that  if the density p has
compact support  then the variance of the KL estimator decays as O(n−1). They also claim (Theorem
7.2) to bound the bias of the KL estimator by O(n−β)  under the assumptions that p is β-Hölder
continuous (β ∈ (0  1])  bounded away from 0  and supported on the interval [0  1]. However  in
their proof  [5] neglect to bound the additional bias incurred near the boundaries of [0  1]  where the
density cannot simultaneously be bounded away from 0 and continuous. In fact  because the KL
estimator does not attempt to correct for boundary bias  it is not clear that the bias should decay as
O(n−β) under these conditions; we require additional conditions at the boundary of X .

3Fixed-k estimators can be computed in O(cid:0)Dn2(cid:1) time  or O(cid:0)2Dn log n(cid:1) using k-d trees for small D.

4Not to be confused with Kullback-Leibler (KL) divergence  for which we also analyze an estimator.

4

√

√

√

n-consistency. Their estimator
[49] studied a closely related entropy estimator for which they prove
n  replacing εk(x) with
is identical to the KL estimator  except that it truncates k-NN distances at
n}. This sort of truncation may be necessary for certain ﬁxed-k estimators to satisfy
min{εk(x) 
ﬁnite-sample bounds for densities of unbounded support  though consistency can be shown regardless.
Finally  two very recent papers [12  4] have analyzed the KL estimator. In this case  [12] generalize
the results of [5] to D > 1  and [4] weaken the regularity and boundary assumptions required by our
bias bound  while deriving the same rate of convergence. Moreover  they show that  if k increases
with n at the rate k (cid:16) log5 n  the KL estimator is asymptotically efﬁcient (i.e.  asymptotically normal 
with optimal asymptotic variance). As explained in Section 8  together with our results this elucidates
the role of k in the KL estimator: ﬁxing k optimizes the convergence rate of the estimator  but
increasing k slowly can further improve error by constant factors.

5 Discussion of assumptions
The lack of ﬁnite-sample results for ﬁxed-k estimators is due to several technical challenges. Here 
we discuss some of these challenges  motivating the assumptions we make to overcome them.
First  these estimators are sensitive to regions of low probability (i.e.  p(x) small)  for two reasons:
1. Many functions f of interest (e.g.  f = log or f (z) = zα  α < 0) have singularities at 0.
2. The k-NN estimate ˆpk(x) of p(x) is highly biased when p(x) is small. For example  for p

β-Hölder continuous (β ∈ (0  2])  one has ([29]  Theorem 2)

(cid:18) k

(cid:19)β/D

np(x)

Bias(ˆpk(x)) (cid:16)

.

(5)

For these reasons  it is common in analysis of k-NN estimators to assume the following [5  39]:

(A1) p is bounded away from zero on its support. That is  p∗ := inf x∈X p(x) > 0.

Second  unlike many functional estimators (see e.g.  [34  45  42])  the ﬁxed-k estimators we consider
do not attempt correct for boundary bias (i.e.  bias incurred due to discontinuity of p on the boundary
∂X of X ). 5 The boundary bias of the density estimate ˆpk(x) does vanish at x in the interior X ◦
of X as n → ∞  but additional assumptions are needed to obtain ﬁnite-sample rates. Either of the
following assumptions would sufﬁce:

(A2) p is continuous not only on X ◦ but also on ∂X (i.e.  p(x) → 0 as dist(x  ∂X ) → 0).
(A3) p is supported on all of RD. That is  the support of p has no boundary. This is the approach
of [49]  but we reiterate that  to handle an unbounded domain  they require truncating εk(x).

Unfortunately  both assumptions (A2) and (A3) are inconsistent with (A1). Our approach is to assume
(A2) and replace assumption (A1) with a much milder assumption that p is locally lower bounded on
its support in the following sense:

(A4) There exist ρ > 0 and a function p∗ : X → (0 ∞) such that  for all x ∈ X   r ∈ (0  ρ] 

p∗(x) ≤ P (B(x r))
µ(B(x r)) .

We show in Lemma 2 that assumption (A4) is in fact very mild; in a metric measure space of positive
dimension D  as long as p is continuous on X   such a p∗ exists for any desired ρ > 0. For simplicity 
we will use ρ =
As hinted by (5) and the fact that F (P ) is an expectation  our bounds will contain terms of the form

D = diam(X ).

√

(cid:34)

(cid:35)

(cid:90)

=

E
X∼P

1

(p∗(X))β/D

p(x)

X

(p∗(x))β/D

dµ(x)

(with an additional f(cid:48)(p∗(x)) factor if f has a singularity at zero). Hence  our key assumption is that
these quantities are ﬁnite. This depends primarily on how quickly p approaches zero near ∂X . For
many functionals  Lemma 6 gives a simple sufﬁcient condition.

5This complication was omitted in the bias bound (Theorem 7.2) of [5] for entropy estimation.

5

6 Preliminary lemmas
Here  we present some lemmas  both as a means of summarizing our proof techniques and also
because they may be of independent interest for proving ﬁnite-sample bounds for other k-NN methods.
Due to space constraints  all proofs are given in the appendix. Our ﬁrst lemma states that  if p is
continuous  then it is locally lower bounded as described in the previous section.
Lemma 2. (Existence of Local Bounds) If p is continuous on X and strictly positive on the interior
X ◦ of X   then  for ρ :=
D = diam(X )  there exists a continuous function p∗ : X ◦ → (0 ∞) and
a constant p∗ ∈ (0 ∞) such that

√

0 < p∗(x) ≤ P (B(x  r))
µ(B(x  r))

≤ p∗ < ∞ 

∀x ∈ X   r ∈ (0  ρ].

We now use these local lower and upper bounds to prove that k-NN distances concentrate around a
term of order (k/(np(x)))1/D. Related lemmas  also based on multiplicative Chernoff bounds  are
used by [21  9] and [8  19] to prove ﬁnite-sample bounds on k-NN methods for cluster tree pruning
and classiﬁcation  respectively. For cluster tree pruning  the relevant inequalities bound the error of
the k-NN density estimate  and  for classiﬁcation  they lower bound the probability of nearby samples
of the same class. Unlike in cluster tree pruning  we are not using a consistent density estimate  and 
unlike in classiﬁcation  our estimator is a function of k-NN distances themselves (rather than their
ordering). Thus  our statement is somewhat different  bounding the k-NN distances themselves:
Lemma 3. (Concentration of k-NN Distances) Suppose p is continuous on X and strictly positive
on X ◦. Let p∗ and p∗ be as in Lemma 2. Then  for any x ∈ X ◦ 

(cid:16) k
(cid:20)
(cid:16) k

p∗(x)n

0 

p∗n

(cid:17)1/D
(cid:17)1/D(cid:19)

 

1. if r >

2. if r ∈

then

P [εk(x) > r] ≤ e−p∗(x)rDn

 

then

P [εk(x) < r] ≤ e−p∗(x)rDn

(cid:18)

(cid:19)k
(cid:19)kp∗(x)/p∗

.

.

e

p∗(x)rDn

(cid:18) ep∗rDn

k

k

It is worth noting an asymmetry in the above bounds: counter-intuitively  the lower bound depends on
p∗. This asymmetry is related to the large bias of k-NN density estimators when p is small (as in (5)).
The next lemma uses Lemma 3 to bound expectations of monotone functions of the ratio ˆpk/p∗. As
suggested by the form of integrals (6) and (7)  this is essentially a ﬁnite-sample statement of the fact
that (appropriately normalized) k-NN distances have Erlang asymptotic distributions; this asymptotic
statement is key to consistency proofs of [25] and [39] for α-entropy and divergence estimators.
Lemma 4. Let p be continuous on X and strictly positive on X ◦. Deﬁne p∗ and p∗ as in Lemma 2.
Suppose f : (0 ∞) → R is continuously differentiable and f(cid:48) > 0. Then  we have the upper bound 6

(cid:19)(cid:21)

E

(cid:20)
(cid:18) p∗(x)

f+

(cid:18) p∗(x)
(cid:19)(cid:21)

ˆpk(x)

ˆpk(x)

sup
x∈X ◦

(cid:20)

E

f−

≤ f+(1) + e

e−yyk
Γ(k + 1)

f+

√

k

k

(cid:90) ∞
(cid:90) κ(x)
D(cid:19)
(cid:17) 1

0

(cid:115)
(cid:18)(cid:16)

≤ f−(1) + e

k

κ(x)

e−yyκ(x)
Γ(κ(x) + 1)

f−

(cid:17)
(cid:16) y
(cid:16) y

k

k

dy 

(cid:17)

dy

(6)

(7)

and  for all x ∈ X ◦  for κ(x) := kp∗(x)/p∗  the lower bound

Note that plugging the function z (cid:55)→ f
into Lemma 4 gives bounds on
E [f (εk(x))]. As one might guess from Lemma 3 and the assumption that f is smooth  this bound is
D . For example  for any α > 0  a simple calculation from (6) gives

roughly of the order (cid:16)(cid:16) k

cD rnp∗(x)

kz

(cid:17)(cid:18)

(cid:19) α

D

1 +

α
D

k

cD rnp∗(x)

.

(8)

(cid:17) 1
k (x)] ≤(cid:16)

E [εα

np(x)

(8) is used for our bias bound  and more direct applications of Lemma 4 are used in variance bound.
6f+(x) = max{0  f (x)} and f−(x) = − min{0  f (x)} denote the positive and negative parts of f. Recall

that E [f (X)] = E [f+(X)] − E [f−(X)].

6

7 Main results
Here  we present our main results on the bias and variance of ˆFB(P ). Again  due to space constraints 
all proofs are given in the appendix. We begin with bounding the bias:
Theorem 5. (Bias Bound) Suppose that  for some β ∈ (0  2]  p is β-Hölder continuous with constant
L > 0 on X   and p is strictly positive on X ◦. Let p∗ and p∗ be as in Lemma 2. Let f : (0 ∞) → R
be differentiable  and deﬁne Mf p : X → [0 ∞) by

Assume

(cid:34)

Mf p(X)
(p∗(X))

β
D

Cf := E
X∼p

(cid:35)

Mf p(x) :=

sup

z∈[p∗(x) p∗]

< ∞.

Then 

f (z)

(cid:12)(cid:12)(cid:12)(cid:12) d
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)E ˆFB(P ) − F (P )

dz

(cid:18) k

(cid:19) β

D

n

.

(cid:12)(cid:12)(cid:12) ≤ Cf L
(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12) ∂

∂w

The statement for divergences is similar  assuming that q is also β-Hölder continuous with constant L
and strictly positive on X ◦. Speciﬁcally  we get the same bound if we replace Mf o with

Mf p(x) :=

f (w  z)

sup

(cid:35)

(cid:35)

β
D

β
D

(cid:34)

< ∞.

X (p(x))

X (p∗(x))

+ E
X∼p

Cf := E
X∼p

Mf q(X)
(q∗(X))

−β/D dµ(x) < ∞.

and deﬁne Mf q similarly (i.e.  with ∂

As an example of the applicability of Theorem 5  consider estimating the Shannon entropy. Then 

(w z)∈[p∗(x) p∗]×[q∗(x) q∗]
∂z ) and we assume that
Mf p(X)
(p∗(X))

(cid:34)
f (z) = log(x)  and so we need Cf =(cid:82)
form(cid:82)
for all x ∈ X with ε(x) := dist(x  ∂X ) < ρ∂  p(x) ≥ c∂εb∂ (x). Then (cid:82)

The assumption Cf < ∞ is not immediately transparent. For the functionals in Table 1  Cf has the
−c dx  for some c > 0  and hence Cf < ∞ intuitively means p(x) cannot approach
zero too quickly as dist(x  ∂X ) → 0. The following lemma gives a formal sufﬁcient condition:
Lemma 6. (Boundary Condition) Let c > 0. Suppose there exist b∂ ∈ (0  1

c )  c∂  ρ∂ > 0 such that 
−c dµ(x) < ∞.
In the supplement  we give examples showing that this condition is fairly general  satisﬁed by
densities proportional to xb∂ near ∂X (i.e.  those with at least b∂ nonzero one-sided derivatives on
the boundary).
We now bound the variance. The main obstacle here is that the ﬁxed-k estimator is an empirical
mean of dependent terms (functions of k-NN distances). We generalize the approach used by [5] to
bound the variance of the KL estimator of Shannon entropy. The key insight is the geometric fact
that  in (RD (cid:107) · (cid:107)p)  there exists a constant Nk D (independent of n) such that any sample Xi can be
amongst the k-nearest neighbors of at most Nk D other samples. Hence  at most Nk D + 1 of the
terms in (2) can change when a single Xi is added  suggesting a variance bound via the Efron-Stein
inequality [10]  which bounds the variance of a function of random variables in terms of its expected
change when its arguments are resampled. [11] originally used this approach to prove a general Law
of Large Numbers (LLN) for nearest-neighbors statistics. Unfortunately  this LLN relies on bounded
kurtosis assumptions that are difﬁcult to justify for the log or negative power statistics we study.
Theorem 7. (Variance Bound) Suppose B ◦ f is continuously differentiable and strictly monotone.
Assume Cf p := EX∼P

(cid:2)B2(f (p∗(X)))(cid:3) < ∞  and Cf :=(cid:82) ∞

X (p∗(x))

0 e−yykf (y) < ∞. Then  for

we have V(cid:104) ˆFB(P )

(cid:105) ≤ CV

.

n

CV := 2 (1 + Nk D) (3 + 4k) (Cf p + Cf )  

As an example  if f = log (as in Shannon entropy)  then  since B is an additive constant  we simply
X p(x) log2(p∗(x)) < ∞. In general  Nk D is of the order k2cD  for some c > 0. Our

require(cid:82)
bound is likely quite loose in k; in practice  V(cid:104) ˆFB(P )
(cid:105)

typically decreases somewhat with k.

7

8 Conclusions and discussion
In this paper  we gave ﬁnite-sample bias and variance error bounds for a class of ﬁxed-k estimators
of functionals of probability density functions  including the entropy and divergence estimators in
Table 1. The bias and variance bounds in turn imply a bound on the mean squared error (MSE) of the
bias-corrected estimator via the usual decomposition into squared bias and variance:
Corollary 8. (MSE Bound) Under the conditions of Theorems 5 and 7 

(cid:20)(cid:16) ˆHk(X) − H(X)
(cid:17)2(cid:21)

E

(cid:18) k

(cid:19)2β/D

≤ C 2

f L2

n

+

CV
n

.

(9)

Choosing k: Contrary to the name  ﬁxing k is not required for “ﬁxed-k” estimators. [36] empirically
studied the effect of changing k with n and found that ﬁxing k = 1 gave best results for estimating
F (P ). However  there has been no theoretical justiﬁcation for ﬁxing k. Assuming tightness of our
bias bound in k  we provide this in a worst-case sense: since our bias bound is nondecreasing in k and
our variance bound is no larger than the minimax MSE rate for these estimation problems  reducing
variance (i.e.  increasing k) does not improve the (worst-case) convergence rate. On the other hand 
[4] recently showed that slowly increasing k can improves the asymptotic variance of the estimator 
with the rate k (cid:16) log5 n leading to asymptotic efﬁciency. In view of these results  we suggest that
increasing k can improve error by constant factors  but cannot improve the convergence rate.
Finally  we note that [36] found increasing k quickly (e.g.  k = n/2) was best for certain hypothesis
tests based on these estimators. Intuitively  this is because  in testing problems  bias is less problematic
than variance (e.g.  an asymptotically biased estimator can still lead to a consistent test).
Acknowledgments
This material is based upon work supported by a National Science Foundation Graduate Research
Fellowship to the ﬁrst author under Grant No. DGE-1252522.
References
[1] C. Adami. Information theory in molecular biology. Physics of Life Reviews  1:3–22  2004.
[2] M. Aghagolzadeh  H. Soltanian-Zadeh  B. Araabi  and A. Aghagolzadeh. A hierarchical clustering based
on mutual information maximization. In Proc. of IEEE International Conf. on Image Processing  2007.

[3] P. A. Alemany and D. H. Zanette. Fractal random walks from a variational formalism for Tsallis entropies.

Phys. Rev. E  49(2):R956–R958  Feb 1994. doi: 10.1103/PhysRevE.49.R956.

[4] Thomas B Berrett  Richard J Samworth  and Ming Yuan. Efﬁcient multivariate entropy estimation via

k-nearest neighbour distances. arXiv preprint arXiv:1606.00304  2016.

[5] Gérard Biau and Luc Devroye. Entropy estimation. In Lectures on the Nearest Neighbor Method  pages

75–91. Springer  2015.

[6] L. Birge and P. Massart. Estimation of integral functions of a density. Annals of Statistics  23:11–29  1995.
[7] B. Chai  D. B. Walther  D. M. Beck  and L. Fei-Fei. Exploring functional connectivity of the human brain

using multivariate information analysis. In NIPS  2009.

[8] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classiﬁcation. In

Advances in Neural Information Processing Systems  pages 3437–3445  2014.

[9] Kamalika Chaudhuri  Sanjoy Dasgupta  Samory Kpotufe  and Ulrike von Luxburg. Consistent procedures

for cluster tree estimation and pruning. IEEE Trans. on Information Theory  60(12):7900–7912  2014.

[10] Bradley Efron and Charles Stein. The jackknife estimate of variance. Ann. of Stat.  pages 586–596  1981.
[11] D. Evans. A law of large numbers for nearest neighbor statistics. In Proceedings of the Royal Society 

volume 464  pages 3175–3192  2008.

[12] Weihao Gao  Sewoong Oh  and Pramod Viswanath. Demystifying ﬁxed k-nearest neighbor information

estimators. arXiv preprint arXiv:1604.03006  2016.

[13] M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. L. Novi Inverardi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. J. Nonparametric Stat.  17:277–297  2005.
[14] A. O. Hero  B. Ma  O. Michel  and J. Gorman. Alpha-divergence for classiﬁcation  indexing and retrieval 

2002. Communications and Signal Processing Laboratory Technical Report CSPL-328.

[15] A. O. Hero  B. Ma  O. J. J. Michel  and J. Gorman. Applications of entropic spanning graphs. IEEE Signal

Processing Magazine  19(5):85–95  2002.

[16] K. Hlaváckova-Schindler  M. Paluˆsb  M. Vejmelkab  and J. Bhattacharya. Causality detection based on

information-theoretic approaches in time series analysis. Physics Reports  441:1–46  2007.

[17] M. M. Van Hulle. Constrained subspace ICA based on mutual information optimization directly. Neural

Computation  20:964–973  2008.

[18] Kirthevasan Kandasamy  Akshay Krishnamurthy  Barnabas Poczos  Larry Wasserman  et al. Nonparametric
von Mises estimators for entropies  divergences and mutual informations. In NIPS  pages 397–405  2015.

8

[19] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classiﬁer. In Proceedings of the Eighteenth

International Conference on Artiﬁcial Intelligence and Statistics  pages 480–488  2015.

[20] L. F. Kozachenko and N. N. Leonenko. A statistical estimate for the entropy of a random vector. Problems

of Information Transmission  23:9–16  1987.

[21] Samory Kpotufe and Ulrike V Luxburg. Pruning nearest neighbor cluster trees. In Proceedings of the 28th

International Conference on Machine Learning (ICML-11)  pages 225–232  2011.

[22] A. Krishnamurthy  K. Kandasamy  B. Poczos  and L. Wasserman. Nonparametric estimation of renyi

divergence and friends. In International Conference on Machine Learning (ICML)  2014.

[23] E. G. Learned-Miller and J. W. Fisher. ICA using spacings estimates of entropy. J. Machine Learning

Research  4:1271–1295  2003.

[24] N. Leonenko and L. Pronzato. Correction of ‘a class of Rényi information estimators for mulitidimensional

densities’ Ann. Statist.  36(2008) 2153-2182  2010.

[25] N. Leonenko  L. Pronzato  and V. Savani. A class of Rényi information estimators for multidimensional

densities. Annals of Statistics  36(5):2153–2182  2008.

[26] J. Lewi  R. Butera  and L. Paninski. Real-time adaptive information-theoretic optimization of neurophysi-

ology experiments. In Advances in Neural Information Processing Systems  volume 19  2007.

[27] H. Liu  J. Lafferty  and L. Wasserman. Exponential concentration inequality for mutual information

estimation. In Neural Information Processing Systems (NIPS)  2012.

[28] D. O. Loftsgaarden and C. P. Quesenberry. A nonparametric estimate of a multivariate density function.

Ann. Math. Statist  36:1049–1051  1965.

[29] YP Mack and M Rosenblatt. Multivariate k-nearest neighbor density estimates. J. Multivar. Analysis  1979.
[30] Kevin Moon and Alfred Hero. Multivariate f-divergence estimation with conﬁdence. In Advances in

Neural Information Processing Systems  pages 2420–2428  2014.

[31] Kevin R Moon and Alfred O Hero. Ensemble estimation of multivariate f-divergence. In Information

Theory (ISIT)  2014 IEEE International Symposium on  pages 356–360. IEEE  2014.

[32] X. Nguyen  M.J. Wainwright  and M.I. Jordan. Estimating divergence functionals and the likelihood ratio

by convex risk minimization. IEEE Transactions on Information Theory  To appear.  2010.

[33] J. Oliva  B. Poczos  and J. Schneider. Distribution to distribution regression. In International Conference

on Machine Learning (ICML)  2013.

[34] D. Pál  B. Póczos  and Cs. Szepesvári. Estimation of Rényi entropy and mutual information based on
generalized nearest-neighbor graphs. In Proceedings of the Neural Information Processing Systems  2010.
[35] H. Peng and C. Dind. Feature selection based on mutual information: Criteria of max-dependency 
max-relevance  and min-redundancy. IEEE Trans On Pattern Analysis and Machine Intelligence  27  2005.
[36] F. Pérez-Cruz. Estimation of information theoretic measures for continuous random variables. In Advances

in Neural Information Processing Systems 21  2008.

[37] B. Póczos and A. L˝orincz. Independent subspace analysis using geodesic spanning trees. In ICML  2005.
[38] B. Póczos and A. L˝orincz. Identiﬁcation of recurrent neural networks by Bayesian interrogation techniques.

J. Machine Learning Research  10:515–554  2009.

[39] B. Poczos and J. Schneider. On the estimation of alpha-divergences. In International Conference on AI and
Statistics (AISTATS)  volume 15 of JMLR Workshop and Conference Proceedings  pages 609–617  2011.
[40] B. Poczos  L. Xiong  D. Sutherland  and J. Schneider. Nonparametric kernel estimators for image

classiﬁcation. In 25th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2012.

[41] C. Shan  S. Gong  and P. W. Mcowan. Conditional mutual information based boosting for facial expression

recognition. In British Machine Vision Conference (BMVC)  2005.

[42] S. Singh and B. Poczos. Exponential concentration of a density functional estimator. In Neural Information

Processing Systems (NIPS)  2014.

[43] S. Singh and B. Poczos. Generalized exponential concentration inequality for Rényi divergence estimation.

In International Conference on Machine Learning (ICML)  2014.

[44] Kumar Sricharan  Raviv Raich  and Alfred O Hero. k-nearest neighbor estimation of entropies with

conﬁdence. In IEEE International Symposium on Information Theory  pages 1205–1209. IEEE  2011.

[45] Kumar Sricharan  Raviv Raich  and Alfred O Hero III. Estimation of nonlinear functionals of densities

with conﬁdence. Information Theory  IEEE Transactions on  58(7):4135–4159  2012.

[46] Kumar Sricharan  Dennis Wei  and Alfred O Hero. Ensemble estimators for multivariate entropy estimation.

IEEE Transactions on Information Theory  59(7):4374–4388  2013.

[47] Z. Szabó  B. Póczos  and A. L˝orincz. Undercomplete blind subspace deconvolution. J. Machine Learning

Research  8:1063–1095  2007.

[48] Zoltán Szabó. Information theoretical estimators toolbox. Journal of Machine Learning Research  15:

283–287  2014. (https://bitbucket.org/szzoli/ite/).

[49] A. B. Tsybakov and E. C. van der Meulen. Root-n consistent estimators of entropy for densities with

unbounded support. Scandinavian J. Statistics  23:75–83  1996.

[50] Q. Wang  S.R. Kulkarni  and S. Verdú. Divergence estimation for multidimensional densities via k-nearest-

neighbor distances. IEEE Transactions on Information Theory  55(5)  2009.

[51] E. Wolsztynski  E. Thierry  and L. Pronzato. Minimum-entropy estimation in semi-parametric models.

Signal Process.  85(5):937–949  2005. ISSN 0165-1684.

9

,Shashank Singh
Barnabas Poczos