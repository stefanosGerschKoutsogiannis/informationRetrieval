2019,Hierarchical Optimal Transport for Multimodal Distribution Alignment,In many machine learning applications  it is necessary to meaningfully aggregate  through alignment  different but related datasets. Optimal transport (OT)-based approaches pose alignment as a divergence minimization problem: the aim is to transform a source dataset to match a target dataset using the Wasserstein distance as a divergence measure. We introduce a hierarchical formulation of OT which leverages clustered structure in data to improve alignment in noisy  ambiguous  or multimodal settings. To solve this numerically  we propose a distributed ADMM algorithm that also exploits the Sinkhorn distance  thus it has an efficient computational complexity that scales quadratically with the size of the largest cluster. When the transformation between two datasets is unitary  we provide performance guarantees that describe when and how well aligned cluster correspondences can be recovered with our formulation  as well as provide worst-case dataset geometry for such a strategy. We apply this method to synthetic datasets that model data as mixtures of low-rank Gaussians and study the impact that different geometric properties of the data have on alignment. Next  we applied our approach to a neural decoding application where the goal is to predict movement directions and instantaneous velocities from populations of neurons in the macaque primary motor cortex. Our results demonstrate that when clustered structure exists in datasets  and is consistent across trials or time points  a hierarchical alignment strategy that leverages such structure can provide significant improvements in cross-domain alignment.,Hierarchical Optimal Transport for
Multimodal Distribution Alignment

John Lee†⇤  Max Dabagia†  Eva L. Dyer†‡§  Christopher J. Rozell†§

†School of Electrical and Computer Engineering 
‡Coulter Department of Biomedical Engineering

Georgia Institute of Technology  Atlanta  GA  30332 USA

{john.lee  maxdabagia  evadyer  crozell}@gatech.edu

Abstract

In many machine learning applications  it is necessary to meaningfully aggregate 
through alignment  different but related datasets. Optimal transport (OT)-based
approaches pose alignment as a divergence minimization problem: the aim is to
transform a source dataset to match a target dataset using the Wasserstein distance
as a divergence measure under alignment constraints. We introduce a hierarchical
formulation of OT which leverages clustered structure in data to improve alignment
in noisy  ambiguous  or multimodal settings. To solve this numerically  we propose
a distributed ADMM algorithm that exploits the Sinkhorn distance  thus it has an
efﬁcient computational complexity that scales quadratically with the size of the
largest cluster. When the transformation between two datasets is unitary  we provide
performance guarantees that describe when and how well cluster correspondences
can be recovered with our formulation  and then describe the worst-case dataset
geometry for such a strategy. We apply this method to synthetic datasets that
model data as mixtures of low-rank Gaussians and study the impact that different
geometric properties of the data have on alignment. Next  we applied our approach
to a neural decoding application where the goal is to predict movement directions
and instantaneous velocities from populations of neurons in the macaque primary
motor cortex. Our results demonstrate that when clustered structure exists in
datasets  and is consistent across trials or time points  a hierarchical alignment
strategy that leverages such structure can provide signiﬁcant improvements in
cross-domain alignment.

Introduction

1
In many machine learning applications  it is necessary to meaningfully aggregate  through alignment 
different but related datasets (e.g.  data across time points or under different conditions or contexts).
Alignment is an important problem at the heart of transfer learning [1  2]  point set registration [3  4  5] 
and shape analysis [6  7  8]  but is generally NP hard. In recent years  distribution alignment methods
that use optimal transport (OT) to quantify similarity between two distributions have increased in
popularity due to their attractive mathematical properties and impressive performance in a variety of
tasks [9  10]. However  using OT to solve unsupervised distribution alignment problems that must
simultaneously match two datasets’ distributions (using OT) while also learning a transformation
between their latent spaces  is extremely challenging  especially when the data has complicated
multi-modal structure. Leveraging additional structure in the problem is thus necessary to regularize
OT and constrain the solution space.
Here  we leverage the fact that heterogeneous datasets often admit clustered or multi-subspace struc-
ture to improve OT-based distribution alignment. Our solution to this problem is to simultaneously

⇤JL is currently with DSO National Laboratories of Singapore.
§Equal contributing senior authors.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

estimate the cluster alignment across two datasets using their local geometry  while also solving a
global alignment problem to meld these local estimates. While it is advantageous to regularize the OT
problem with known cluster pairings [10  11]  we are instead concerned with the substantially harder
unsupervised setting where such information is missing. We introduce a hierarchical formulation of
OT for clustered and multi-subspace datasets called Hierarchical Wasserstein Alignment (HiWA)3.
We empirically show that when data are well approximated with Gaussian mixture models (GMMs)
or lie on a union of subspaces  we may leverage existing clustering pipelines (e.g.  sparse subspace
clustering [12] [13]) to improve alignment. When the transformation between datasets is unitary 
we provide analyses that reveal key geometric and sampling insights  as well as perturbation and
failure mode analyses. To solve the problem numerically  we propose an efﬁcient distributed ADMM
algorithm that also exploits the Sinkhorn distance  thus beneﬁting from efﬁcient computational
complexity that scales quadratically with the size of the largest cluster.
To test and benchmark our approach  we applied it to synthetic data generated from mixtures of
low-rank Gaussians and studied the impact of different geometric properties of the data on alignment
to conﬁrm the predictions of our theoretical analysis. Next  we applied our approach to a neural
decoding application where the goal is to predict movement directions from populations of neurons
in the macaque primary motor cortex. Our results demonstrate that when clustered structure exists
in neural datasets and is consistent across trials or time points  a hierarchical alignment strategy
that leverages such structure can provide signiﬁcant improvements in unsupervised decoding from
ambiguous (symmetric) movement patterns. This suggests OT can be applied to a wider range of
neural datasets  and shows that a hierarchical strategy avoids local minima encountered by a global
alignment strategy that ignores clustered structure.
2 Background and related work
Transfer learning and distribution alignment. A fundamental goal in transfer learning is to
aggregate related datasets by learning a mapping between them. We wish to learn a transformation
T 2 T   where T refers to some class of transformations that aligns distributions under a notion of
probability divergence D(·|·) between a target distribution µ and a reference (source) distribution ⌫:
(1)

min

T2T D(T (µ)|⌫).

Various probability divergences have been proposed in the literature  such as Euclidean least-squares
(when data ordering is known) [14  15  16]  Kullback-Leibler (KL) [17]  maximum mean discrepancy
(MMD) [18  19  20  21]  and the Wasserstein distance [10]  where trade-offs are often statistical
(e.g.  consistency  sample complexity) versus computational. Alignment problems are ill-posed since
the space of T is large  so a priori structure is often necessary to constrain T based on geometric
assumptions. Compact manifolds like the Grassmann or Stiefel [22  23] are primary choices when
little information is present  as they preserve isometry. Non-isometric transformations  though richer 
demand much more structure (e.g.  manifold or graph structure) [24  25  26  27  10].
Low-rank and union of subspaces models. Principal components analysis (PCA)  one of the most
popular methods in data science  assumes a low-rank model where the top-k principal components of
a dataset provide the optimal rank-k approximation under an Euclidean loss. This has been extended
to robust (sparse errors) settings [12]  and multi- (union of) subspaces settings where data can be
partitioned into disjoint subsets where each subset of data is locally low-rank [28]. Transfer learning
methods based on subspace alignment [29  30  31] work well with zero-mean unimodal datasets  but
struggle on more complicated modalities (e.g.  Gaussian mixtures or union of subspaces) due to a
mixing of covariances. Related to our work  [32] performs multi-subspace alignment by greedily
assigning correspondences between subspaces using chordal distances; this however discards valuable
information about a distribution’s shape.
Optimal transport. Optimal transport (OT) [33] is a natural type of divergence for registration
problems because it accounts for the underlying geometry of the space. In Euclidean settings  OT
gives rise to a metric known as the Wasserstein distance W(µ  ⌫) which measures the minimum effort
required to “displace” points across measures µ and ⌫ (understood here as empirical point clouds).
Therefore  OT relieves the need for kernel estimation to create an overlapping support of the measures

3MATLAB code can be found at https://github.com/siplab-gt/hiwa-matlab. Neural datasets and Python code

are provided at http://nerdslab.github.io/neuralign

2

µ  ⌫. Despite this attractive property  it has both a poor numerical complexity of O(n3 log n) (where
n is the sample size) and a dimension-dependent sample complexity of O(n1/d)  where the data
dimension is d [34  35]. Recently  an entropically regularized version of OT known as the Sinkhorn
distance [36] has emerged as a compelling divergence measure; it not only inherits OT’s geometric
properties but also has superior computational and sample complexities of O(n2) and O(n1/2)4 
respectively. It has also become a versatile building block in domain adaptation [10  38]. Prior art
[10] has largely exploited the OT’s push-forward as the alignment map since this map minimizes
the OT cost between the source and target distributions while allowing a priori structure to be easily
incorporated (e.g.  to preserve label/graphical integrity). Such an approach  however  is fundamentally
expensive when d ⌧ n since the primary optimization variable is a large transport coupling (i.e. 
Rn⇥n)  while in reality the alignment mapping is merely Rd 7! Rd. Moreover  it assumes that the
source and target distributions are close in terms of their squared Euclidean distance (i.e.  an identity
transformation)  but this does not generally hold between arbitrary latent spaces.
Hierarchical OT and related work. The idea of learning an afﬁne or unitary transformation to align
datasets with an OT-based divergence has previously been studied in [39  40  41]  a problem known
as OT Procrustes. However  these methods don’t use problem-speciﬁc or clustered structure in data.
Hierarchical OT is a recent generalization of OT [42  43  44] that is an effective and efﬁcient way of
injecting structure into OT but it has never been used to jointly solve alignment problems – our work
represents a ﬁrst attempt at doing so. Thus  a key contribution of this paper is putting both of these
two ingredients together to develop a scalable strategy that leverages multimodal structure in data
solve the OT Procrustes problem.
3 Hierarchical Wasserstein alignment
Preliminaries and notation. Consider clustered datasets {Xi 2 RD⇥nx i}S
i=1 and {Yj 2
j=1 whose clusters are denoted with the indices i  j and whose columns are treated
RD⇥ny j}S
as RD embedding coordinates. The number of samples in the i-th (j-th) cluster of dataset X
(dataset Y ) is given by nx i (ny j). We express the empirical measures of clusters Xi and Yj as
l=1 Yj (l)  respectively  where x refers to a point mass
µi := 1
located at coordinate x 2 RD. The squared 2-Wasserstein distance between µi and ⌫j is deﬁned as

ny jPny j

nx iPnx i

k=1 Xi(k) and ⌫j := 1

2 (µi  ⌫j) :=

Q2U(nx i ny j )

Q(k  l)kXi(k)  Yj(l)k2

nx iXk=1
where Q is a doubly stochastic matrix that encodes point-wise correspondences (i.e.  the (k  l)-th
entry describes the ﬂow of mass between Xi(k) and Yj (l))  Xi(k) is the k-th column of matrix
Xi  and the constraint U(m  n) := {Q 2 Rm⇥n
: Q n = m/m  Q> m = n/n} refers to the
uniform transport polytope (with m a length m vector containing ones). We will use k·k to denote
the operator norm  X† to denote the pseudo-inverse of X  and Id to denote the d ⇥ d identity matrix.
Overview. Although unsupervised alignment is challenging due to the presence of local minima  the
imposition of additional structure will help to prune them away. Our key insight is that hierarchical
structure decomposes a complicated optimization surface into simpler ones that are less prone to
local minima. We formulate a hierarchical Wasserstein approach to align datasets with known (or
j=1 but whose correspondences are unknown. The task therefore is
estimated) clusters {µi}S
to jointly learn the alignment T and the cluster-correspondences:

i=1 {⌫j}S

ny jXl=1

+

W 2

min

2

min

P2BS  T2T

SXi=1

SXj=1

PijW 2

2 (T (µi)  ⌫j) 

(2)

where the matrix P encodes the strength of correspondences between clusters  with a large Pij value
indicating a correspondence between clusters i  j  and a small value indicating a lack thereof. We
note that BS := U(S  S) is a special type of transport polytope known as the S-th Birkhoff polytope.
Interestingly  this becomes a nested (or block) OT formulation  where correspondences are resolved
at two levels: the outer level resolves cluster-correspondences (via P ) while the inner level resolves
point-wise correspondences between cluster points (via the Wasserstein distance).
Alignment over the Stiefel manifold. Assuming clusters lie on subspaces and principal angles
between subspaces are “well preserved” across X and Y (we make this precise in Theorem 4.2)  an

4Dependent on a regularization parameter [37].

3

isometric transformation sufﬁces. Hence  we solve (2) with T VD D  the Stiefel manifold which
is deﬁned as Vk d := {R 2 Rk⇥d : R>R = Id}. Explicitly  we can re-formulate equation (2) as:
(3)

PijCij(R  Qij)

min

P  R {Qij}Xi j

where Cij(R  Qij) :=

s.t. P 2 BS  R 2 VD D  Qij 2 U(nx i  ny j) 
DXk l

Qij(k  l)kRXi(k)  Yj(l)k2

1

2

measures pairwise cluster divergences using the squared 2-Wasserstein distance under a Stiefel
transformation R acting on the ith cluster.
Finally  we include entropic regularization over transportation couplings P and all Qij’s to modify
the Wasserstein distances to Sinkhorn distances  so as to take advantage of its superior computational
and sample complexities. Omitting constraints for brevity  our ﬁnal problem is given as

(4)

(5)

min

P  R {Qij}Xi j ⇣PijCij(R  Qij) + H2(Qij)⌘ + H1(P ) 

µ

min

µ
D

where 1  2 > 0 are the entropic regularization parameters and the negative entropy function is

deﬁned as H(P ) := Pi j Pij log Pij. Parameters 1  2 control the correspondence entropy 
therefore (5) approximates (3) when 1  2 > 0  but reverts to the original problem (3) as 1  2 ! 0.
Distributed ADMM approach. Problem (5) is non-convex due to multilinearity in the objective
and its Stiefel manifold domain. Although alternating directions method of multipliers (ADMM) is a
convergent convex solver framework [45  46]  it is being applied in increasingly many non-convex
settings [47]. Since (5) readily admits a splitting structure that separates the individual Cij blocks 
we develop a distributed ADMM approach. We proceed to split (5) as follows:

⇤ij  Rij  Ri +

noting that the set constraints are omitted for brevity. The augmented Lagrangian is given by

P  eR {Rij  Qij}Xi j ⇣PijCij(Rij  Qij) + H2(Qij)⌘ + H1(P )
Lµ =Xi j ⇣PijCij(Rij  Qij) + h
2DkRij  eRk2

s.t. Rij = eR  8i  j 
F + H2(Qij)⌘ + H1(P ) 
where µ > 0 is the ADMM parameter and {⇤ij} are Lagrange multipliers. Full details of the
update steps are included in the Supplementary Material. The algorithm may be summarized in
two steps (Alg. 1): (i) a distributed step that asks all cluster pairs to individually ﬁnd their optimal
transformations Rij in parallel  and (ii) a consensus step that aggregates all the locally estimated
transformations according to a weighting that is proportional to correspondence strengths Pij.
Parameters. Entropic parameters 1  2 relax the one-to-one cluster correspondence assumption 
balancing a trade off between alignment precision (small ) and sample complexity (large ).
Numerically  negative entropy adds strong convexity to the program  reducing sensitivity towards
perturbations at the cost of a slower convergence rate. The ADMM parameter µ controls the ‘strength’
of the consensus  or from an algorithmic viewpoint  the gradient step size.
Distributed consensus. Update steps for Qij  Rij  Lij can be performed in parallel over all cluster
pairs (S2 in total)  making it amenable for a distributed implementation. The runtime complexity of
this algorithm is presented in the supplementary Materials.
Robustness against initial conditions. We intentionally build robustness against initial conditions
by ordering updates for Rij and Qij before P such that when µ is sufﬁciently small  the ADMM
sequence is inﬂuenced more by the data than by initial conditions.
4 Theoretical guarantees for cluster-based alignment
While the previous section explains how to align clustered datasets  in this section  we aim to answer
the question of when and how well they can be aligned. We provide necessary conditions for cluster-
based alignability as well as alignment perturbation bounds according to equation (3)’s formulation.
To simplify our analysis  we make the following assumptions: (i) each of the clusters contain the
same number of datapoints n  (ii) the ground truth cluster correspondences are P ? = IS/S (i.e. 

4

i=1 {Yj}S

j=1)

. Initialization

end while

for all i  j in parallel do

Qij nx i >ny j /nx iny j
while not converged do

R random VD D  P S >S /S2  ⇤ij 0  8i  j
while not converged do

Rij STIEFELALIGNMENT(2PijYjQ>ijX>i + µ(R  ⇤ij))
D kRijXi(k)  Yj(l)k2
Qij SINKHORN(2/Pij  C(k  l) 1
2)
end for
P SINKHORN(1  C(i  j) Cij(Rij  Qij))
R STIEFELALIGNMENT(Pi j Rij + ⇤ij)
⇤ij ⇤ij + Rij  R 

Algorithm 1 Hierarchical Wasserstein Alignment (HiWA) Algorithm
1: procedure HIERARCHICALWASSERSTEINALIGNMENT(1  2  µ {Xi}S
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
end while
14:
15: end procedure
1: procedure SINKHORN(  C 2 Rm⇥n)
2:
3:
4:
5:
6:
7:
8: end procedure

K exp(C/) 
while not converged do
m ↵ Kv
n ↵ K>u

u m
v n
end while
P diag(u)K diag(v)

Notation:
↵: elementwise division
exp(·): elementwise exponential
diag(·): diagonal matrix of argument

v n

n

8i  j

1: procedure STIEFELALIGNMENT(A)
(U   ⌃  V ) SVD(A)
2:
R U V >
3:
4: end procedure

diagonal containing 1/S). However  this analysis can be extended to the case where the number of
points is unequal without loss of generality. Detailed proofs are given in the Supp. Material.
The following result is a criterion that  if met  ensures the existence of a global minimizer of the
cluster-correspondence P ?. This criterion requires that matched clusters must be closer in Wasserstein
distance than mismatched clusters  according to a threshold determined by Wasserstein’s sample
complexity (i.e.  an asymptotic rate dependent on the clusters’ sample sizes and intrinsic dimensions).
Since these sample complexity results are based on the Wasserstein distance  we expect a less stringent
criterion when using the Sinkhorn distance in (5) (due to superior sample complexity [37]).
Theorem 4.1 (Correspondence disambiguity criterion). Let all clusters be strictly low-rank where the
ij :=
minR2VD D Qij2Bn Cij(R  Qij). Problem (3) yields the solution P ? = IS/S with probability at
least 1   if  8i  j : i 6= j  the following criterion is satisﬁed:

dimension of the i-th cluster in the x-th dataset is dx i. Let dx i  dy j > 4 8i  j 2JSK. Deﬁne bC?

jj > Bx i() + By i() + Bx j() + By j()

bC?
ij + bC?

ji  bC?

where Bz k() := cz kn 2

ii  bC?
dz k +plog(1/)/2n 

cz k = 1458⇣2 +

1

3dz k/22  1⌘.

Proof sketch. The proof contains two parts. In the ﬁrst part  we consider perturbation conditions of
the cost matrix C in a (non-variational) optimal transport program over the Birkhoff polytope. To
be unperturbed from P ? = IS/S  we require that Cij + Cji  Cii  Cjj > 0 8i  j : i 6= j. In the
second part  we extend this condition to the the ﬁnite-sample regime by utilizing recently developed
concentration bounds [35] for the p-Wasserstein distance  which essentially raises the disambiguity
lower bound due to ﬁnite-sample uncertainty. (Supp. Material  Section 2)

Now  even if we know the global correspondence P ?  we still do not have the full picture about
the alignment’s quality. For example  all matching clusters may have very similar covariances  but
principal angles between the clusters are “distorted” across the datasets. Our next theorem gives us
an upper bound on the alignment error (for unitary transformations)  and makes precise the notion of
global structure distortion.

5

Theorem 4.2 (Cluster-based alignment perturbation bounds). Consider data matrices {Xi  Yi 2
RD⇥n}c

i=1 with known point-wise correspondence matrices {Qii 2 Bn}c

i=1. Deﬁne matrices

X := [X1Q11  X2Q22  . . .   XcQcc] 

Y := [Y1  Y2  . . .   Yc].

and "kX†k  1p2

Set "2 :=Y >Y  X>XF . If the criterion stated in theorem 4.1 is satisﬁed  X is full row rank 

(kXkkX†k)1/2  then
P2Bc R2VD DXi j
min
i=1 tr(Xi(I/n  QiiQ>ii )X>i + (1/n  1)YiY >i ) is a data-dependent constant.

PijCij(R)  (kXkkX†k + 2)2kX†k2"4 + D 

Proof sketch. We utilize a recent perturbation result on the Procrustes problem (on a Frobenius
norm objective) by Arias-Castro et al. [48] and adapt it to our squared 2-Wasserstein objective.
(Supp. Material  Section 3)

where D =Pc

Note that " plays a major role in the alignment error bound and quantiﬁes the notion of global structure
distortion  which allows us to understand on how phenomena like covariate shift or misclustering
impacts alignment. To shed some light in this regard  we consider a simple analysis on a cluster-
pair’s error contribution to "  denoted as "ij. Consider the decomposition of the (i  j)-th block of
the Gramians related to clusters i and j  where their respective singular value decompositions are
XiQii = Ai⌃x iV > and Yj = Bj⌃y jV >. Deﬁning the blockwise error between clusters i  j as

"ij :=Y >i Yj  Q>ijX>i XjQjjF =⌃y iB>i Bj⌃y j  ⌃x iA>i Aj⌃x jF  

two components stand out: (i) angular shift  which is characterized by differences in principal angles
between B>i Bj and A>i Aj  and (ii) spectral shift  which is characterized by differences in spectra.
Finally  we show that the subspace conﬁguration of a dataset’s clusters can also affect alignment.
Pretend for a moment that external alignment information were present to aid in the disambiguation
between two clusters. The following lemma tells us when such information is useless (Proof in
Supp. Material  Section 4).
Lemma 4.3 (Uninformative alignment). Consider clusters Xi  Yj 2 RD⇥n and known point-wise
correspondences Qij 2 U(n  n). Denote the left and right singular vectors of YjQ>ijX>i associated
with the non-zero singular values as ˜U   ˜V 2 RD⇥r with r  D. Deﬁne the set of orthogonal
transformations that are constrained to agree with known angular directions as
: R>R = I  RV 0 = U0} 

T (U0  V 0) := {R 2 RD⇥D

+

where U0  V 0 2 VD r with r  D. Given U0  V 0 2 RD⇥r0 with r0  D  we have

min

R2T (U0 V 0)

Cij(R)  min
R2VD D

Cij(R) 

(6)

with equality holding when h ˜U   U0i = h ˜V   V 0i.
Direct consequences of this lemma are the following: When a dataset has equally-spaced subspaces 
it has a maximally uninformative geometric conﬁguration since angular information from other
clusters (i.e.  U0  V 0) can never increase the inter-cluster distance Cij (i.e.  equality in (6) always
holds); it is hence a worst-case scenario for alignment. This also explains why alignment in very
high-dimensional space is harder: All subspaces may be orthogonal to each other  and hence offer no
“geometric” advantage.
5 Numerical experiments
5.1 Synthetic low-rank Gaussian mixture dataset
In this section  we validate our method as well as demonstrate its limiting characteristics under
symmetric-subspace and ﬁnite-sample regimes. To generate our synthetic data  we repeat the
following procedure for each of the S clusters. We ﬁrst randomly generate Gaussian distribution
parameters µi 2 Rd  ⌃i 2 Rd : ⌃i ⌫ 0 (positive semi-deﬁnite)  then randomly sample n data-points
from these parameters  and ﬁnally project them into a random subspace Vi 2 RD⇥d in a D > d
dimensional embedding. In these experiments  we assume that the clusters are known  but the

6

F /kR?Xk2

Figure 1: Synthetic experiments. HiWA was tested in two subspace conﬁgurations (a b): randomly-spaced
(average-case  solid) versus equally-spaced (worst-case  dashed) for S = 5  d = 2  D = 6  n = {25  100} 
where S is the number of clusters  d the dimension of each cluster  D is the embedding dimension  and n is the
sample size. As we expect  performance in terms of the (a) alignment and (b) correspondence error is better
in the average (vs. worst) case. In (c d)  we report (c) alignment and (d) correspondence errors as d and n
varies  and report the error’s 25th/50th/75th percentiles. In (e f)  we show ablation results (50 trials  no random
restarts permitted) for semi-supervised HiWA (known clusters)  completely unsupervised HiWA-SSC (unknown
clusters)  non-structured Wasserstein alignment (WA)  subspace alignment methods (SA [29]  CORAL [31]) 
and iterative closest point (ICP) [49] for n = 50  d = 2  and (e) S = 5 D = 6  and (f) S = 2 D = 2.
cluster-correspondence across datasets is unknown. We measure performance with respect to two
metrics: (i) alignment error  deﬁned as the relative difference between the recovered versus true
F   and (ii) correpondence error  deﬁned as the

rotation acting on the data kbRX  R?Xk2
sum of absolute differences between the recovered and the true correspondencesPij |bP  P ?|ij.
To understand how global geometry impacts alignment  we applied HiWA in two different settings
(Figure 1a-b): (i) a worst-case setting where subspaces are equally spaced with a subspace similarity
of kV >i Vjk = 1 8i 6= j  and (ii) the random setting where subspaces are randomly selected from
the Grassmann manifold. We observe that equally-spaced subspaces have signiﬁcantly inferior
performance when compared to randomly-spaced subspaces  providing some evidence that equally
spaced subspaces are indeed the worst-case scenario in alignment  as suggested by Lemma 4.3.
Next  we studied the effect of dimensions d and sample size n on the accuracy of alignment (Figure 1
(c-d)). We tested HiWA across various dataset conditions by varying parameters d = {2  3  4  5} and
n = {12  25  50  100  200} while approximately maintaining the average subspace correlations (i.e. 
EkV >i Vjk) by ﬁxing the cluster size S = 5 and tuning D to control the subspace spacing. In both
cases  sample complexities are better than the theoretical rate of O(n1/d)  which is likely due to the
Sinkhorn distance’s superior sample complexity. In Figure 1e-f  we conduct an ablation study and
evaluate our algorithm against benchmark methods in transfer learning and point set registration in
two settings: a simple one in low-d (e) and a harder one in higher-d (f). Speciﬁcally  we compare
HiWA when clusters are known (but pairwise correspondences are unknown)  HiWA with clustering
via sparse subspace clustering [12] (HiWA-SSC) to represent completely unsupervised alignment 
a Wasserstein alignment variant with no cluster-structure (WA) which is akin to OT Procrustes
[50  39  40  41]  subspace alignment [29]  correlation alignment [31]  and iterative closest point (ICP)
[49]. HiWA exhibits strongest performance  with HiWA-SSC trailing closely behind (since clusters
are independently resolved)  followed by WA  then other algorithms. Subspace alignment methods
have remarkably poor performance in higher dimensions due to their inability to resolve subspace
sign ambiguities  while ICP demonstrates its notorious dependence on good initial conditions. These
results indicates HiWA’s strong robustness against initial conditions and good scaling properties.
5.2 Neural population decoding example
Decoding intent (e.g.  where you want to move your arm) or evoked responses (e.g.  what you are
looking at or listening to) directly from neural activity is a widely studied problem in neuroscience 
and the ﬁrst step in the design of a brain machine interface (BMI). A critical challenge with BMIs
is that neural decoders need to be recalibrated (or re-trained) due to drift in neural responses or
electrophysiology measurements/readouts [51]. A recent method for semi-supervised brain decoding
ﬁnds a transformation between projected neural responses and movements by solving a KL-divergence

7

00.511.5200.20.40.60.8110-210-110000.20.40.60.810.511.500.20.40.60.8112255010020010-210-110012255010020000.511.522.5300.511.5200.20.40.60.81abcdefFigure 2: Results on neural decoding dataset: How distribution alignment is used to translate neural activity
into movement – low-dimensional embeddings of neural data are aligned with target movement patterns (a). In
(b)  we compare the performance (cluster correspondence) of HiWA  WA  and DAD as the number of points in
the source dataset decreases. Next  we compared the performance of HiWA with known and estimated clusters
(via GMM). Movement patterns in which cluster separability is high and the geometry is preserved across
datasets  can be aligned in both cases (green stars). Patterns where separability is low but geometry is useful can
be aligned when the cluster arrangements are known are denoted with yellow stars.
minimization problem [52]. Using this approach  one could build robust decoders that work across
days and shifts in neural responses through alignment.
We test the utility of hierarchical alignment for neural decoding on datasets collected from the arm
region of primary motor cortex of a non-human primate (NHP) during a center out reaching task [52].
After spike sorting and binning the data  we applied factor analysis to reduce the data dimensionality
to 3D (source distribution) and applied HiWA to align the neural data to a 3D movement distribution
(target distribution) (Figure 2). We compared its performance to (procrustes) Wasserstein alignment
(WA) without hierarchical structure  and a baseline brute force search method called distribution
alignment decoding (DAD) [52]. We examined the prediction accuracy of the target reach direction
for the motor decoding task (i.e.  the cluster classiﬁcation accuracy).
Next  we examined the impact of the sampling density (Figure 2b) on alignment performance. Our
results demonstrate that HiWA continues to produce consistent cluster correspondences (> 70%
accuracy)  even as the number of samples per cluster drops to 8. In comparison  DAD is competitive
at larger sample sizes but its performance rapidly drops off as sampling density decreases because it
requires estimating a distribution from samples. WA suffers from the presence of many local minima
and fails to ﬁnd the correct cluster correspondences. Our results suggest that HiWA consistently
provides stable solutions  outperforming competitor methods for this application.
Finally  to study the impact of local and global geometry on whether an unlabeled source and target
can be aligned  we applied HiWA to permutations of eight subsets of reach directions (movement
patterns). When just two reach directions are considered (Figure 2c  Columns 1-4)  global geometry
becomes useless in determining the correct rotation. In this case  we observe that HiWA is only
capable of consistent alignment when cluster asymmetries are sufﬁciently extreme in both the source
and target. When three reach directions are considered (Figure 2c  Columns 5-8)  the global geometry
can be used  yet there still exist symmetrical cases where recovering the correct rotation is difﬁcult
without adequate local asymmetries or some supervised (labeled) data to match clusters. These results
suggest that hierarchical structure can be critical in resolving ambiguities in alignment of globally
symmetric movement distributions.
6 Conclusion
This paper introduces a new method for hierarchical alignment with Wasserstein distances  provided
an efﬁcient numerical solution with analytical guarantees. We tested our method and compared
its performance against other methods on a synthetic mixture model dataset and on a real neural
decoding dataset. Future directions include extensions to non-rigid transformations  and applications
to higher dimensional neural datasets that do not rely on external measured behavioral covariates.

8

Acknowledgments
JL was supported by DSO National Laboratories of Singapore  ED and MD were supported by
NSF grant IIS-1755871  and CR was supported by NSF grant CCF-1409422 and CAREER award
CCF-1350954.

References
[1] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and

Data Engineering  22(10):1345–1359  2009.

[2] Karl Weiss  Taghi M Khoshgoftaar  and DingDing Wang. A survey of transfer learning. Journal of Big

data  3(1):9  2016.

[3] Haili Chui and Anand Rangarajan. A new point matching algorithm for non-rigid registration. Computer

Vision and Image Understanding  89(2-3):114–141  2003.

[4] Andriy Myronenko and Xubo Song. Point set registration: Coherent point drift. IEEE Transactions on

Pattern Analysis and Machine Intelligence  32(12):2262–2275  2010.

[5] Gary KL Tam  Zhi-Quan Cheng  Yu-Kun Lai  Frank C Langbein  Yonghuai Liu  David Marshall  Ralph R
Martin  Xian-Fang Sun  and Paul L Rosin. Registration of 3d point clouds and meshes: a survey from rigid
to nonrigid. IEEE Transactions on Visualization and Computer Graphics  19(7):1199–1217  2013.

[6] Alexander M Bronstein  Michael M Bronstein  and Ron Kimmel. Generalized multidimensional scaling:
a framework for isometry-invariant partial surface matching. Proceedings of the National Academy of
Sciences  103(5):1168–1172  2006.

[7] Alexander M Bronstein  Michael M Bronstein  Leonidas J Guibas  and Maks Ovsjanikov. Shape google:
Geometric words and expressions for invariant shape retrieval. ACM Transactions on Graphics (TOG) 
30(1):1  2011.

[8] Maks Ovsjanikov  Mirela Ben-Chen  Justin Solomon  Adrian Butscher  and Leonidas Guibas. Functional
maps: a ﬂexible representation of maps between shapes. ACM Transactions on Graphics  31(4):30  2012.

[9] Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine

Learning  11(5-6):355–607  2019.

[10] Nicolas Courty  Rémi Flamary  Devis Tuia  and Alain Rakotomamonjy. Optimal transport for domain

adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence  39(9):1853–1865  2017.

[11] Debasmit Das and CS George Lee. Unsupervised domain adaptation using regularized hyper-graph
matching. In 2018 25th IEEE International Conference on Image Processing (ICIP)  pages 3758–3762.
IEEE  2018.

[12] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm  theory  and applications. IEEE

Transactions on Pattern Analysis and Machine Intelligence  35(11):2765–2781  2013.

[13] Eva L Dyer  Aswin C Sankaranarayanan  and Richard G Baraniuk. Greedy feature selection for subspace

clustering. The Journal of Machine Learning Research  14(1):2487–2517  2013.

[14] Xiaoxiao Shi  Qi Liu  Wei Fan  S Yu Philip  and Ruixin Zhu. Transfer learning on heterogenous feature
spaces via spectral transformation. In Data Mining (ICDM)  2010 IEEE 10th International Conference on 
pages 1049–1054. IEEE  2010.

[15] Sumit Shekhar  Vishal M Patel  Hien V Nguyen  and Rama Chellappa. Generalized domain-adaptive
dictionaries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
361–368  2013.

[16] Yahong Han  Fei Wu  Dacheng Tao  Jian Shao  Yueting Zhuang  and Jianmin Jiang. Sparse unsupervised
dimensionality reduction for multiple view data. IEEE Transactions on Circuits and Systems for Video
Technology  22(10):1485  2012.

[17] Masashi Sugiyama  Shinichi Nakajima  Hisashi Kashima  Paul V Buenau  and Motoaki Kawanabe. Direct
importance estimation with model selection and its application to covariate shift adaptation. In Advances
in Neural Information Processing Systems  pages 1433–1440  2008.

[18] Sinno Jialin Pan  Ivor W Tsang  James T Kwok  and Qiang Yang. Domain adaptation via transfer

component analysis. IEEE Transactions on Neural Networks  22(2):199–210  2011.

9

[19] Mahsa Baktashmotlagh  Mehrtash T Harandi  Brian C Lovell  and Mathieu Salzmann. Unsupervised
domain adaptation by domain invariant projection. In Proceedings of the IEEE International Conference
on Computer Vision  pages 769–776  2013.

[20] Mingsheng Long  Jianmin Wang  Guiguang Ding  Jiaguang Sun  and Philip S Yu. Transfer joint matching
for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 1410–1417  2014.

[21] Mingming Gong  Kun Zhang  Tongliang Liu  Dacheng Tao  Clark Glymour  and Bernhard Schölkopf.
Domain adaptation with conditional transferable components. In Proceedings of the 33rd International
Conference on Machine Learning  pages 2839–2848  2016.

[22] Raghuraman Gopalan  Ruonan Li  and Rama Chellappa. Domain adaptation for object recognition: An
unsupervised approach. In IEEE International Conference on Computer Vision  pages 999–1006. IEEE 
2011.

[23] Boqing Gong  Yuan Shi  Fei Sha  and Kristen Grauman. Geodesic ﬂow kernel for unsupervised domain
adaptation. In IEEE Conference on Computer Vision and Pattern Recognition  pages 2066–2073. IEEE 
2012.

[24] Chang Wang and Sridhar Mahadevan. Manifold alignment using procrustes analysis. In Proceedings of

the 25th International Conference on Machine learning  pages 1120–1127. ACM  2008.

[25] Chang Wang and Sridhar Mahadevan. A general framework for manifold alignment. In 2009 AAAI Fall

Symposium Series  2009.

[26] Sira Ferradans  Nicolas Papadakis  Gabriel Peyré  and Jean-François Aujol. Regularized discrete optimal

transport. SIAM Journal on Imaging Sciences  7(3):1853–1882  2014.

[27] Zhen Cui  Hong Chang  Shiguang Shan  and Xilin Chen. Generalized unsupervised manifold alignment.

In Advances in Neural Information Processing Systems  pages 2429–2437  2014.

[28] Yonina C Eldar and Moshe Mishali. Robust recovery of signals from a structured union of subspaces.

IEEE Transactions on Information Theory  55(11):5302–5316  2009.

[29] Basura Fernando  Amaury Habrard  Marc Sebban  and Tinne Tuytelaars. Unsupervised visual domain
adaptation using subspace alignment. In Proceedings of the IEEE International Conference on Computer
Vision  pages 2960–2967  2013.

[30] Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain adaptation. In

BMVC  volume 4  pages 24–1  2015.

[31] Baochen Sun  Jiashi Feng  and Kate Saenko. Return of frustratingly easy domain adaptation. In Thirtieth

AAAI Conference on Artiﬁcial Intelligence  2016.

[32] Kowshik Thopalli  Rushil Anirudh  Jayaraman J Thiagarajan  and Pavan Turaga. Multiple subspace

alignment improves domain adaptation. arXiv preprint arXiv:1811.04491  2018.

[33] Leonid Vitalevich Kantorovich. On a problem of monge. Journal of Mathematical Sciences  133(4):1383–

1383  2006.

[34] Richard M Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical

Statistics  40(1):40–50  1969.

[35] Jonathan Weed and Francis Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical

measures in wasserstein distance. arXiv preprint arXiv:1707.00087  2017.

[36] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems  pages 2292–2300  2013.

[37] Aude Genevay  Lénaic Chizat  Francis Bach  Marco Cuturi  and Gabriel Peyré. Sample complexity of

sinkhorn divergences. arXiv preprint arXiv:1810.02733  2018.

[38] Nicolas Courty  Rémi Flamary  Amaury Habrard  and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In Advances in Neural Information Processing Systems  pages
3730–3739  2017.

[39] Meng Zhang  Yang Liu  Huanbo Luan  and Maosong Sun. Earth mover’s distance minimization for
unsupervised bilingual lexicon induction. In Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing  pages 1934–1945  2017.

10

[40] David Alvarez-Melis  Stefanie Jegelka  and Tommi S Jaakkola. Towards optimal transport with global

invariances. arXiv preprint arXiv:1806.09277  2018.

[41] Edouard Grave  Armand Joulin  and Quentin Berthet. Unsupervised alignment of embeddings with

wasserstein procrustes. arXiv preprint arXiv:1805.11222  2018.

[42] Mikhail Yurochkin  Sebastian Claici  Edward Chien  Farzaneh Mirzazadeh  and Justin Solomon. Hierarchi-

cal optimal transport for document representation. arXiv preprint arXiv:1906.10827  2019.

[43] Bernhard Schmitzer and Christoph Schnörr. A hierarchical approach to optimal transport. In International
Conference on Scale Space and Variational Methods in Computer Vision  pages 452–464. Springer  2013.

[44] David Alvarez-Melis  Tommi S Jaakkola  and Stefanie Jegelka. Structured optimal transport. arXiv preprint

arXiv:1712.06199  2017.

[45] Jonathan Eckstein and Dimitri P Bertsekas. On the douglas—rachford splitting method and the proximal
point algorithm for maximal monotone operators. Mathematical Programming  55(1-3):293–318  1992.

[46] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  Jonathan Eckstein  et al. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning  3(1):1–122  2011.

[47] Yu Wang  Wotao Yin  and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth optimiza-

tion. Journal of Scientiﬁc Computing  78(1):29–63  2019.

[48] Ery Arias-Castro  Adel Javanmard  and Bruno Pelletier. Perturbation bounds for procrustes  classical
scaling  and trilateration  with applications to manifold learning. arXiv preprint arXiv:1810.09569  2018.

[49] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes. In Sensor Fusion IV: Control
Paradigms and Data Structures  volume 1611  pages 586–607. International Society for Optics and
Photonics  1992.

[50] Anand Rangarajan  Haili Chui  and Fred L Bookstein. The softassign procrustes matching algorithm. In
Biennial International Conference on Information Processing in Medical Imaging  pages 29–42. Springer 
1997.

[51] Chethan Pandarinath  K Cora Ames  Abigail A Russo  Ali Farshchian  Lee E Miller  Eva L Dyer  and
Jonathan C Kao. Latent factors and dynamics in motor cortex and their application to brain–machine
interfaces. Journal of Neuroscience  38(44):9390–9401  2018.

[52] Eva L Dyer  Mohammad Gheshlaghi Azar  Matthew G Perich  Hugo L Fernandes  Stephanie Naufel 
Lee E Miller  and Konrad P Körding. A cryptography-based approach for movement decoding. Nature
Biomedical Engineering  1(12):967  2017.

11

,John Lee
Max Dabagia
Eva Dyer
Christopher Rozell