2017,A KL-LUCB algorithm for Large-Scale Crowdsourcing,This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB  offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions  which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.,A KL-LUCB Bandit Algorithm for

Large-Scale Crowdsourcing

Ervin Tánczos∗ and Robert Nowak†

University of Wisconsin-Madison

tanczos@wisc.edu 

rdnowak@wisc.edu

Bob Mankoff

Former Cartoon Editor of the New Yorker

bmankoff@hearst.com

Abstract

This paper focuses on best-arm identiﬁcation in multi-armed bandits with bounded
rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB 
offering the best qualities of the two algorithms in one method. This is achieved by
proving a novel anytime conﬁdence bound for the mean of bounded distributions 
which is the analogue of the LIL-type bounds recently developed for sub-Gaussian
distributions. We corroborate our theoretical results with numerical experiments
based on the New Yorker Cartoon Caption Contest.

1 Multi-Armed Bandits for Large-Scale Crowdsourcing

This paper develops a new multi-armed bandit (MAB) for large-scale crowdsourcing  in the style
of the KL-UCB [4  9  3]. Our work is strongly motivated by crowdsourcing contests  like the New
Yorker Cartoon Caption contest [10]3. The new approach targets the “best-arm identiﬁcation problem”
[1] in the ﬁxed conﬁdence setting and addresses two key limitations of existing theory and algorithms:

(i) State of the art algorithms for best arm identiﬁcation are based on sub-Gaussian conﬁdence bounds

[5] and fail to exploit the fact that rewards are usually bounded in crowdsourcing applications.

(ii) Existing KL-UCB algorithms for best-arm identiﬁcation do exploit bounded rewards [8]   but have
suboptimal performance guarantees in the ﬁxed conﬁdence setting  both in terms of dependence
on problem-dependent hardness parameters (Chernoff information) and on the number of arms 
which can be large in crowdsourcing applications.

The new algorithm we propose and analyze is called lil-KLUCB  since it is inspired by the lil-UCB
algorithm [5] and the KL-LUCB algorithm [8]. The lil-UCB algorithm is based on sub-Gaussian
bounds and has a sample complexity for best-arm identiﬁcation that scales as

(cid:88)

i≥2

∆−2

i

log(δ−1 log ∆−2

i

)  

where δ ∈ (0  1) is the desired conﬁdence and ∆i = µ1 − µi is the gap between the means of the
best arm (denoted as arm 1) and arm i. If the rewards are in [0  1]  then the KL-LUCB algorithm has
†This work was partially supported by the NSF grant IIS-1447449 and the AFSOR grant FA9550-13-1-0138.
3For more details on the New Yorker Cartoon Caption Contest  see the Supplementary Materials.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

a sample complexity scaling essentially like4(cid:88)

(D∗

i )−1 log(nδ−1(D∗

i )−1)  

i≥2
i := D∗(µ1  µi) is the Chernoff-information between a Ber(µ1)
where n is the number of arms and D∗
and a Ber(µi) random variable5. Ignoring the logarithmic factor  this bound is optimal for the case
of Bernoulli rewards [7  11]. Comparing these two bounds  we observe that KL-LUCB may offer
beneﬁts since D∗
i /2  but lil-UCB has better logarithmic
i and no explicit dependence on the number of arms n. Our new algorithm
dependence on the ∆2
lil-KLUCB offers the best of both worlds  providing a sample complexity that scales essentially like

i = D∗(µ1  µi) ≥ (µ1 − µi)2/2 = ∆2

(cid:88)

i≥2

(D∗

i )−1 log(δ−1 log(D∗

i )−1) .

The key to this result is a novel anytime conﬁdence bound for sums of bounded random variables 
which requires a signiﬁcant departure from previous analyses of KL-based conﬁdence bounds.
The practical beneﬁt of lil-KLUCB is illustrated in terms of the New Yorker Caption Contest problem
[10]. The goal of that crowdsourcing task is to identify the funniest cartoon caption from a batch
of n ≈ 5000 captions submitted to the contest each week. The crowd provides “3-star” ratings for
the captions  which can be mapped to {0  1/2  1}  for example. Unfortunately  many of the captions
are not funny  getting average ratings close to 0 (and consequently very small variances). This
fact  however  is ideal for KL-based conﬁdence intervals  which are signiﬁcantly tighter than those
based on sub-Gaussianity and the worst-case variance of 1/4. Compared to existing methods  the
lil-KLUCB algorithm better addresses the two key features in this sort of application: (1) a very large
number of arms  and (2) bounded reward distributions which  in many cases  have very low variance.
In certain instances  this can have a profound effect on sample complexity (e.g.  O(n2) complexity
for algorithms using sub-Gaussian bounds vs. O(n log n) for lil-KLUCB  as shown in Table 1).
The paper is organized as follows. Section 2 deﬁnes the best-arm identiﬁcation problem  gives the
lil-KLUCB algorithm and states the main results. We also brieﬂy review related literature  and
compare the performance of lil-KLUCB to that of previous algorithms. Section 3 provides the main
technical contribution of the paper  a novel anytime conﬁdence bound for sums of bounded random
variables. Section 4 analyzes the performance of the lil-KLUCB algorithm. Section 5 provides
experimental support for the lil-KLUCB algorithm using data from the New Yorker Caption Contest.

2 Problem Statement and Main Results
Consider a MAB problem with n arms. We use the shorthand notation [n] := {1  . . .   n}. For every
i ∈ [n] let {Xi j}j∈N denote the reward sequence of arm i  and suppose that P(Xi j ∈ [0  1]) = 1 for
all i ∈ [n]  j ∈ N. Furthermore  assume that all rewards are independent  and that Xi j ∼ Pi for all
j ∈ N. Let the mean reward of arm i be denoted by µi and assume w.l.o.g. that µ1 > µ2 ≥ ··· ≥ µn.
We focus on the best-arm identiﬁcation problem in the ﬁxed-conﬁdence setting. At every time t ∈ N
we are allowed to select an arm to sample (based on past rewards) and observe the next element in
its reward sequence. Based on the observed rewards  we wish to ﬁnd the arm with the highest mean
reward. In the ﬁxed conﬁdence setting  we prescribe a probability of error δ ∈ (0  1) and our goal is
to construct an algorithm that ﬁnds the best arm with probability at least 1− δ. Among 1− δ accurate
algorithms  one naturally favors those that require fewer samples. Hence proving upper bounds on
the sample complexity of a candidate algorithm is of prime importance.
The lil-KLUCB algorithm that we propose is a fusion of lil-UCB [5] and KL-LUCB [8]  and its
operation is essentially a special instance of LUCB++ [11]. At each time step t  let Ti(t) denote the

total number of samples drawn from arm i so far  and let(cid:98)µi Ti(t) denote corresponding empirical

mean. The algorithm is based on lower and upper conﬁdence bounds of the following general form:

4A more precise characterization of the sample complexity is given in Section 2.
5The Chernoff-information between random variables Ber(x) and Ber(y) (0 < x < y < 1) is D∗(x  y) =
1−x and z∗ is the unique z ∈ (x  y) such that

x + (1 − z) log 1−z

D(z∗  x) = D(z∗  y)  where D(z  x) = z log z
D(z  x) = D(z  y).

2

for each i ∈ [n] and any δ ∈ (0  1)

(cid:26)
(cid:27)
m <(cid:98)µi Ti(t) : D(cid:0)(cid:98)µi Ti(t)  m(cid:1) ≤ c log (κ log2(2Ti(t))/δ)
(cid:26)
(cid:27)
m >(cid:98)µi Ti(t) : D(cid:0)(cid:98)µi Ti(t)  m(cid:1) ≤ c log (κ log2(2Ti(t))/δ)

Ti(t)

Ti(t)

Li(t  δ) = inf

Ui(t  δ) = sup

where c and κ are small constants (deﬁned in the next section). These bounds are designed so that
with probability at least 1 − δ  Li(Ti(t)  δ) ≤ µi ≤ Ui(Ti(t)  δ) holds for all t ∈ N. For any t ∈ N
let TOP(t) be the index of the arm with the highest empirical mean  breaking ties at random. With
this notation  we state the lil-KLUCB algorithm and our main theoretical result.

lil-KLUCB

1. Initialize by sampling every arm once.
2. While LTOP(t)(TTOP(t)(t)  δ/(n − 1)) ≤ max
i(cid:54)=TOP(t)

Ui(Ti(t)  δ) do:

• Sample the following two arms:

– TOP(t)  and
– arg max
i(cid:54)=TOP(t)

Ui(Ti(t)  δ)

and update means and conﬁdence bounds.

3. Output TOP(t)

Theorem 1. For every i ≥ 2 let(cid:101)µi ∈ (µi  µ1)  and(cid:101)µ = maxi≥2(cid:101)µi. With probability at least 1 − 2δ 

lil-KLUCB returns the arm with the largest mean and the total number of samples it collects is upper
bounded by

c0 log(cid:0)(n − 1)δ−1 log D∗(µ1 (cid:101)µ)−1(cid:1)

D∗(µ1 (cid:101)µ)

(cid:88)

i≥2

+

c0 log(cid:0)δ−1 log D∗(µi (cid:101)µi)−1(cid:1)

D∗(µi (cid:101)µi)

 

(cid:101)µ2 ... (cid:101)µn

inf

where c0 is some universal constant  D∗(x  y) is the Chernoff-information.
Remark 1. Note that the LUCB++ algorithm of [11] is general enough to handle identiﬁcation of
the top k arms (not just the best-arm). All arguments presented in this paper also go through when
considering the top-k problem for k > 1. However  to keep the arguments clear and concise  we
chose to focus on the best-arm problem only.

2.1 Comparison with previous work

We now compare the sample complexity of lil-KLUCB to that of the two most closely related
algorithms  KL-LUCB [8] and lil-UCB [5]. For a detailed review of the history of MAB problems
and the use of KL-conﬁdence intervals for bounded rewards  we refer the reader to [3  9  4].
For the KL-LUCB algorithm  Theorem 3 of [8] guarantees a high-probability sample complexity
upper bound scaling as

(D∗(µi  c))−1 log(cid:0)nδ−1(D∗(µi  c))−1(cid:1) .

(cid:88)

i≥1

inf

c∈(µ1 µ2)

Our result improves this in two ways. On one hand  we eliminate the unnecessary logarithmic
dependence on the number of arms n in every term. Note that the log n factor still appears in
Theorem 1 in the term corresponding to the number of samples on the best arm. It is shown in [11]
that this factor is indeed unavoidable. The other improvement lil-KLUCB offers over KL-LUCB
is improved logarithmic dependence on the Chernoff-information terms. This is due to the tighter
conﬁdence intervals derived in Section 3.
Comparing Theorem 1 to the sample complexity of lil-UCB  we see that the two are of the same form 
the exception being that the Chernoff-information terms take the place of the squared mean-gaps

3

(which arise due to the use of sub-Gaussian (SG) bounds). To give a sense of the improvement this
can provide  we compare the sums6

(cid:88)

i≥2

SKL =

1

D∗(µi  µ1)

and SSG =

1
∆2
i

.

(cid:88)

i≥2

Let µ  µ(cid:48) ∈ (0  1)  µ < µ(cid:48) and ∆ = |µ − µ(cid:48)|. Note that the Chernoff-information between Ber(µ)
and Ber(µ(cid:48)) can be expressed as

D∗(µ  µ(cid:48)) = max
x∈[µ µ(cid:48)]

min{D(x  µ)  D(x  µ(cid:48))} = D(x∗  µ) = D(x∗  µ(cid:48)) = D(x∗ µ)+D(x∗ µ(cid:48))

2

 

for some unique x∗ ∈ [µ  µ(cid:48)]. It follows that

D∗(µ  µ(cid:48)) ≥ min
x∈[µ µ(cid:48)]

D(x  µ) + D(x  µ(cid:48))

2

(cid:112)µ(µ + ∆) +(cid:112)(1 − µ)(1 − µ − ∆)

1

.

= log

Using this with every term in SKL gives us an upper bound on that sum. If the means are all bounded
well away from 0 and 1  then SKL may not differ that much from SSG. There are some situations
however  when the two expressions behave radically differently. As an example  consider a situation
when µ1 = 1. In this case we get

SKL ≤ (cid:88)

2
1−∆i

1

log

≤ 2

i≥2

(cid:88)

i≥2

1
∆i

(cid:28)(cid:88)

i≥2

1
∆2
i

= SSG .

Table 1 illustrates the difference between the scaling of the sums SKL and SSG when the gaps have
the parametric form ∆i = (i/n)α.

Table 1: SKL versus SSG for mean gaps ∆i = ( i

α

SKL
SSG

∈ (0  1/2)

n
n

1/2

n

n log n

∈ (1/2  1)

n )α  i = 1  . . .   n
∈ (1 ∞)

1

n
n2α

n log n

n2

nα
n2α

We see that KL-type conﬁdence bounds can sometimes provide a signiﬁcant advantage in terms of
the sample complexity. Intuitively  the gains will be greatest when many of the means are close to 0
or 1 (and hence have low variance). We will illustrate in Section 5 that such gains often also manifest
in practical applications like the New Yorker Caption Contest problem.

3 Anytime Conﬁdence Intervals for Sums of Bounded Random Variables

i )−1 log log(D∗

i )−1) samples from a suboptimal arm i  where D∗

The main step in our analysis is proving a sharp anytime conﬁdence bound for the mean of
bounded random variables. These will be used to show  in Section 4  that lil-KLUCB draws at
most O((D∗
:= D∗(µ1  µi) is
the Chernoff-information between a Ber(µ1) and a Ber(µi) random variable and arm 1 is the arm
with the largest mean. The iterated log factor is a necessary consequence of the law-of-the-iterated
logarithm [5]  and in it is in this sense that we call the bound sharp. Prior work on MAB algorithms
based on KL-type conﬁdence bounds [4  9  3] did not focus on deriving tight anytime conﬁdence
bounds.
Consider a sequence of iid random variables Y1  Y2  . . . that are bounded in [0  1] and have mean µ.

j∈[t] Yj be the empirical mean of the observations up to time t ∈ N.

Let(cid:98)µt = 1

t

Theorem 2. Let µ ∈ [0  1] and δ ∈ (0  1) be arbitrary. Fix any l ≥ 0 and set N = 2l  and deﬁne

(cid:80)

i

(cid:88)

t∈[N ]

(cid:88)

k≥l

 N

N +1

.

κ(N ) = δ1/(N +1)

1{l(cid:54)=0} log2(2t)− N +1

N + N

(k + 1)− N +1

N

6Consulting the proof of Theorem 1 it is clear that the number of samples on the sub-optimal arms of
lil-KLUCB scales essentially as SKL w.h.p. (ignoring doubly logarithmic terms)  and a similar argument can be
made about lil-UCB. This justiﬁes considering these sums in order to compare lil-KLUCB and lil-UCB.

4

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(i) Deﬁne the sequence zt ∈ (0  1 − µ]  t ∈ N such that

if a solution exists  and zt = 1 − µ otherwise. Then P (∃t ∈ N : (cid:98)µt − µ > zt) ≤ δ.

N +1 zt  µ

µ + N

D

=

t

 

log (κ(N ) log2(2t)/δ)

(1)

(ii) Deﬁne the sequence zt > 0  t ∈ N such that

if a solution exists  and zt = µ otherwise. Then P (∃t ∈ N : (cid:98)µt − µ < −zt) ≤ δ.

N +1 zt  µ

D

=

t

 

log (κ(N ) log2(2t)/δ)

µ − N

The result above can be used to construct anytime conﬁdence bounds for the mean as follows. Consider
part (i) of Theorem 2 and ﬁx µ. The result gives a sequence zt that upper bounds the deviations of
the empirical mean. It is deﬁned through an equation of the form D(µ + N zt/(N + 1)  µ) = ft.
Note that the arguments of the function on the left must be in the interval [0  1]  in particular
N zt/(N + 1) < 1 − µ  and the maximum of D(µ + x  µ) for x > 0 is D(1  µ) = log µ−1. Hence 
equation 1 does not have a solution if ft is too large (that is  if t is small). In these cases we set
zt = 1 − µ. However  since ft is decreasing  equation 1 does have a solution when t ≥ T (for some
T depending on µ)  and this solution is unique (since D(µ + x  µ) is strictly increasing).

With high probability(cid:98)µt − µ ≤ zt for all t ∈ N by Theorem 2. Furthermore  the function D(µ + x  µ)

is increasing in x ≥ 0. By combining these facts we get that with probability at least 1 − δ

k≥l

5

On the other hand

D

by deﬁnition. Chaining these two inequalities leads to the lower conﬁdence bound

(cid:16)

D

µ + N

N +1 zt  µ

.

N +1   µ

(cid:16) N(cid:98)µt+µ

(cid:17) ≥ D
(cid:17)
(cid:17) ≤ log (κ(N ) log2(2t)/δ)
(cid:16) N(cid:98)µt+m
(cid:16) N(cid:98)µt+m

(cid:17) ≤ log (κ(N ) log2(2t)/δ)
(cid:27)
(cid:17) ≤ log (κ(N ) log2(2t)/δ)

N +1   m

t

 

(cid:27)

(cid:16)
(cid:26)

(cid:26)

µ + N

N +1 zt  µ

m <(cid:98)µt : D

m >(cid:98)µt : D

(2)

(3)

L(t  δ) = inf

which holds for all times t with probability at least 1 − δ. Considering the left deviations of(cid:98)µt − µ

t

we can get an upper conﬁdence bound in a similar manner:

.

t

N +1   m

U (t  δ) = sup

N +1   m

L(cid:48)(t  δ) = inf

That is  for all times t  with probability at least 1 − 2δ we have L(t  δ) ≤ (cid:98)µt ≤ U (t  δ).
N +1 ≈ (cid:98)µt  and thus
Note that the constant log κ(N ) ≈ 2 log2(N )  so the choice of N plays a relatively mild role in
the bounds. However  we note here that if N is sufﬁciently large  then N(cid:98)µt+m
(cid:17) ≈ D ((cid:98)µt  m)  in which case the bounds above are easily compared to those in prior
(cid:16) N(cid:98)µt+m
(cid:27)
(cid:27)

D
works [4  9  3]. We make this connection more precise and show that the conﬁdence intervals deﬁned
as

m <(cid:98)µt : D ((cid:98)µt  m) ≤ c(N ) log (κ(N ) log2(2t)/δ)
m >(cid:98)µt : D ((cid:98)µt  m) ≤ c(N ) log (κ(N ) log2(2t)/δ)

Theorem 1 in the Supplementary Material  where the correctness of L(cid:48)(t  δ) and U(cid:48)(t  δ) is shown.

satisfy L(cid:48)(t  δ) ≤ (cid:98)µt ≤ U(cid:48)(t  δ) for all t  with probability 1 − 2δ. The constant c(N ) is deﬁned in
we only prove part (i). Note that {(cid:98)µt − µ > zt} ⇐⇒ {St > tzt}  where St =(cid:80)
P(cid:0)∃t ∈ [2k  2k+1] : St > tzt

Proof of Theorem 2. The proofs of parts (i) and (ii) are completely analogous  hence in what follows
j∈[t](Yj − µ)

P (∃t ∈ N : St > tzt) ≤ P (∃t ∈ [N ] : St > tzt) +

denotes the centered sum up to time t. We start with a simple union bound

U(cid:48)(t  δ) = inf

(cid:88)

(cid:26)
(cid:26)

(cid:1) .

  and

(4)

t

t

 

First  we bound each summand in the second term individually. In an effort to save space  we deﬁne
the event Ak = {∃t ∈ [2k  2k+1] : St > tzt}. Let tj k = (1 + j
N )2k. In what follows we use the
notation tj ≡ tj k. We have

P (Ak) ≤ (cid:88)

P (∃t ∈ [tj−1  tj] : St > tzt) ≤ (cid:88)

P(cid:0)∃t ∈ [tj−1  tj] : St > tj−1ztj−1

(cid:1)  

j∈[N ]

j∈[N ]

where the last step is true if tzt is non-decreasing in t. This technical claim is formally shown in
Lemma 1 in the Supplementary Material. However  to give a short heuristic  it is easy to see that
tzt has an increasing lower bound. Noting that D(µ + x  µ) is convex in x (the second derivative is
positive)  and that D(µ  µ) = 0  we have D(1  µ)x ≥ D(µ + x  µ). Hence zt (cid:38) t−1 log log t.
Using a Chernoff-type bound together with Doob’s inequality  we can continue as

P (Ak) ≤ inf

(cid:88)

j∈[N ]

exp

exp

=

λ>0

j∈[N ]

≤ (cid:88)
(cid:88)
P (Ak) ≤ (cid:88)
(cid:88)

j∈[N ]

=

j∈[N ]

j∈[N ]

(cid:1)(cid:1)

λ>0

eλStj

− sup

P(cid:0)∃t ∈ [tj−1  tj] : exp (λSt) > exp(cid:0)λtj−1ztj−1
(cid:17)(cid:17)(cid:19)
(cid:18)
eλ(Y1−µ)(cid:17)(cid:17)(cid:19)
(cid:18)
eλ(ξ−µ)(cid:17)(cid:17)(cid:19)

λtj−1ztj−1 − log E(cid:16)
(cid:16)
N +j ztj−1 − log E(cid:16)
(cid:16)
(cid:18)
λαjztj−1 − log E(cid:16)
(cid:16)
exp(cid:0)−tjD(cid:0)µ + αjztj−1   µ(cid:1)(cid:1)  

−tj sup
λ≥0

−tj sup
λ≥0

λ N +j−1

exp

.

(5)

(6)

Using E(eλY1) ≤ E(eλξ) where ξ ∼ Ber(µ) (see Lemma 9 of [4])  and the notation αj = N +j−1

 

N +j

since the rate function of a Bernoulli random variable can be explicitly computed  namely we have
supλ>0(λx − log E(eλξ)) = D(µ + x  µ) (see [2]).
Again  we use the convexity of D(µ + x  µ). For any α ∈ (0  1) we have αD(µ + x  µ) ≥
D(µ + αx  µ)  since D(µ  µ) = 0. Using this with α =

N

αj (N +1) and x = αjztj−1  we get that
µ + N

N +1 ztj−1   µ

.

(cid:17)
(cid:17)(cid:17)

Plugging in the deﬁnition of tj and the sequence zt  and noting that δ < 1  we arrive at the bound

Regarding the ﬁrst term in (4)  again using the Bernoulli rate function bound we have

(cid:16)
(cid:16)

N

αj (N +1) D(cid:0)µ + αjztj−1   µ(cid:1) ≥ D
P (Ak) ≤ (cid:88)
(cid:18)

(cid:16)−tj

N +1
N αjD

j∈[N ]

exp

This implies

(cid:18)
P (Ak) ≤ (cid:88)
P (∃t ∈ [N ] : (cid:98)µt − µ > zt) ≤ (cid:88)

− N + 1

j∈[N ]

exp

log

N

Using the convexity of D(µ + x  µ) as before  we can continue as

P (∃t ∈ [N ] : (cid:98)µt − µ > zt) ≤ (cid:88)
≤ (cid:88)

µ + N

N +1 ztj−1  µ

.

(7)

(cid:19)(cid:19)

(cid:16)

(cid:17) N +1

N

.

κ(N )(k+1)

exp (−tD(µ + zt  µ)) .

δ

N

)/δ

t∈[N ]

≤ N

κ(N ) log2(2k+1 N + j − 1
P ((cid:98)µt − µ > zt) ≤ (cid:88)
(cid:16)−t N +1
(cid:16)
(cid:17)(cid:17)
N log (κ(N ) log2(2t)/δ)(cid:1)
exp(cid:0)− N +1
(cid:88)

N +1 zt  µ

µ + N

N D

t∈[N ]

t∈[N ]

t∈[N ]

exp

N +1

N κ(N )− N +1

N

log2(2t)− N +1
N .

≤ δ

t∈[N ]

6

Plugging the two bounds back into (4) we conclude that

P (∃t : (cid:98)µt − µ > zt) ≤ δ

N +1

N κ(N )− N +1

N

1{l(cid:54)=0} log2(2j)− N +1

N +

(cid:88)

k≥l

(k + 1)− N +1

N

 ≤ δ  

(cid:88)

j∈[N ]

by the deﬁnition of κ(N ).

4 Analysis of lil-KLUCB

Recall that the lil-KLUCB algorithm uses conﬁdence bounds of the form Ui(t  δ) = sup{m >(cid:98)µt :
D((cid:98)µt  m) ≤ ft(δ)} with some decreasing sequence ft(δ). In this section we make this dependence

explicit  and use the notations Ui(ft(δ)) and Li(ft(δ)) for upper and lower conﬁdence bounds. For
any  > 0 and i ∈ [n]  deﬁne the events Ωi() = {∀t ∈ N : µi ∈ [Li(ft())  Ui(ft())]}.
The correctness of the algorithm follows from the correctness of the individual conﬁdence intervals 
as is usually the case with LUCB algorithms. This is shown formally in Proposition 1 provided in the
Supplementary Materials. The main focus in this section is to show a high probability upper bound
on the sample complexity. This can be done by combining arguments frequently used for analyzing
LUCB algorithms and those used in the analysis of the lil-UCB [5]. The proof is very similar in spirit
to that of the LUCB++ algorithm [11]. Due to spatial restrictions  we only provide a proof sketch
here  while the detailed proof is provided in the Supplementary Materials.

Proof sketch of Theorem 1. Observe that at each time step two things can happen (apart from stop-
ping): (1) Arm 1 is not sampled (two sub-optimal arms are sampled); (2) Arm 1 is sampled together
with some other (suboptimal) arm. Our aim is to upper bound the number of times any given arm is
sampled for either of the reasons above. We do so by conditioning on the event

   for a certain choice of {δi} deﬁned below.

(cid:92)

i≥2

Ω(cid:48) = Ω1(δ) ∩

Ωi(δi)

For instance  if arm 1 is not sampled at a given time t  we know that TOP(t) (cid:54)= 1  which means
there must be an arm i ≥ 2 such that Ui(Ti(t)  δ) ≥ U1(T1(t)  δ). However  on the event Ω1(δ)  the
UCB of arm 1 is accurate  implying that Ui(Ti(t)  δ) ≥ µ1. This implies that Ti(t) can not be too

big  since on Ωi(δi) (cid:98)µi t is “close" to µi  and also Ui(Ti(t)  δ) is not much larger then(cid:98)µi. All this is

made formal in Lemma 2  yielding the following upper bound on number of times arm i is sampled
for reason (1):

τi(δ · δi) = min{t ∈ N : ft(δ · δi) < D∗(µi  µ1)} .

Similar arguments can be made about the number of samples of any suboptimal arm i for reason (2) 
and also the number of samples on arm 1. This results in the sample complexity upper bound

K1 log(cid:0)(n − 1)δ−1 log D∗(µ1 (cid:101)µ)−1(cid:1)
according to Theorem 1 in the Supplementary Material. Substituting γ = exp(−D∗(µi (cid:101)µi)z) we get

on the event Ω(cid:48)  where K1 is a universal constant. Finally  we deﬁne the quantities δi = sup{ >
0 : Ui(ft()) ≥ µi ∀t ∈ N}. Note that we have P(δi < γ) = P(∃t ∈ N : Ui(ft(γ)) ≥ µi) ≤ γ

K1 log(cid:0)δ−1 log D∗(µi (cid:101)µi)−1(cid:1) + log δ−1

D∗(µi (cid:101)µi)

D∗(µ1 (cid:101)µ)

(cid:88)

i≥2

+

 

i

P(cid:16) log δ
D∗(µi (cid:101)µi) ≥ z

−1
i

(cid:17) ≤ exp(−D∗(µi (cid:101)µi)z) .

Hence {δi}i≥2 are independent sub-exponential variables  which allows us to control their contribu-
tion to the sum above using standard techniques.

5 Real-World Crowdsourcing

We now compare the performance of lil-KLUCB to that of other algorithms in the literature. We do
this using both synthetic data and real data from the New Yorker Cartoon Caption contest [10]7. To

7These data can be found at https://github.com/nextml/caption-contest-data

7

keep comparisons fair  we run the same UCB algorithm for all the competing conﬁdence bounds.
We set N = 8 and δ = 0.01 in our experiments. The conﬁdence bounds are [KL]: the KL-bound
derived based on Theorem 2  [SG1]: a matching sub-Gaussian bound derived using the proof of
Theorem 2  using sub-Gaussian tails instead of the KL rate-function (the exact derivations are in the
Supplementary Material)  and [SG2]: the sharper sub-Gaussian bound provided by Theorem 8 of [7].
We compare these methods by computing the empirical probability that the best-arm is among the top
5 empirically best arms  as a function of the total number of samples. We do so using using synthetic
data in Figure 5   where the Bernoulli rewards simulate cases from Table 1  and using real human
response data from two representative New Yorker caption contests in Figure 5.

Figure 1: Probability of the best-arm in the top 5 empirically best arms  as a function of the number of samples 
based on 250 repetitions. µi = 1 − ((i − 1)/n)α  with α = 1 in the left panel  and α = 1/2 in the right panel.
The mean-proﬁle is shown above each plot. [KL] Blue; [SG1] Red; [SG2] Black.

As seen in Table 1  the KL conﬁdence bounds have the potential to greatly outperform the sub-
Gaussian ones. To illustrate this indeed translates into superior performance  we simulate two cases 
with means µi = 1 − ((i − 1)/n)α  with α = 1/2 and α = 1  and n = 1000. As expected  the
KL-based method requires signiﬁcantly fewer samples (about 20 % for α = 1 and 30 % for α = 1/2)
to ﬁnd the best arm. Furthermore  the arms with means below the median are sampled about 15 and
25 % of the time respectively – key in crowdsourcing applications  since having participants answer
fewer irrelevant (and potentially annoying) questions improves both efﬁciency and user experience.

Figure 2: Probability of the best-arm in the top 5 empirically best arms vs. number of samples  based on 250
bootstrapped repetitions. Data from New Yorker contest 558 (µ1 = 0.536) on left  and contest 512 (µ1 = 0.8)
on right. Mean-proﬁle above each plot. [KL] Blue; [SG1] Red; [SG2] Black.

8

0.00.20.40.60.81.01.2P(best arm in top 5)  alpha=1Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCB0.000.751.502.253.003.754.50llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.01.2P(best arm in top 5)  alpha=1/2Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCB0.000.150.300.450.600.750.90llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.01.2P(best arm in top 5)  Contest 558Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCB02468101214161820llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.01.2P(best arm in top 5)  Contest 512Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCB0.000.751.502.253.003.754.50llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllTo see how these methods fair on real data  we also run these algorithms on bootstrapped human
response data from the real New Yorker Caption Contest. The mean reward of the best arm in these
contests is usually between 0.5 and 0.85  hence we choose one contest from each end of this spectrum.
At the lower end of the spectrum  the three methods fair comparably. This is expected because the
sub-Gaussian bounds are relatively good for means about 0.5. However  in cases where the top mean
is signiﬁcantly larger than 0.5 we see a marked improvement in the KL-based algorithm.

Extension to numerical experiments

Since a large number of algorithms have been proposed in the literature for best arm identiﬁcation 
we include another algorithm in the numerical experiments for comparison.
Previously we compared lil-KLUCB to lil-UCB as a comparison for two reasons. First  this compari-
son illustrates best the gains of using the novel anytime conﬁdence bounds as opposed to those using
sub-Gaussian tails. Second  since lil-UCB is the state of the art algorithm  any other algorithm will
likely perform worse.
The authors of [6] compare a number of different best arm identiﬁcation methods  and conclude
that two of them seem to stand out: lil-UCB and Thompson sampling. Therefore  we now include
Thomspon sampling [Th] in our numerical experiments for the New Yorker data.
We implemented the method as prescribed in [6]. As can bee seen in Figure 5  Thompson sampling
seems to perform somewhat worse than the previous methods in these two instances.

Figure 3: Probability of the best-arm in the top 5 empirically best arms vs. number of samples  based on 250
bootstrapped repetitions. Data from New Yorker contest 558 (µ1 = 0.536) on left  and contest 512 (µ1 = 0.8)
on right. Mean-proﬁle above each plot. [KL] Blue; [SG1] Red; [SG2] Black; [Th] Purple.

9

0.00.20.40.60.81.01.2P(best arm in top 5)  Contest 512Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCBThompson02468101214161820llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.01.2P(best arm in top 5)  Contest 512Number of samples (10 thousands)(Empirical) probability − 250 trialsKaufmann lil−UCBKL−UCBSG lil−UCBThompson0.000.751.502.253.003.754.50llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllReferences
[1] Jean-Yves Audibert and Sébastien Bubeck. Best arm identiﬁcation in multi-armed bandits. In COLT-23th

Conference on Learning Theory-2010  pages 13–p  2010.

[2] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration inequalities: A nonasymptotic

theory of independence. Oxford university press  2013.

[3] Olivier Cappé  Aurélien Garivier  Odalric-Ambrym Maillard  Rémi Munos  Gilles Stoltz  et al. Kullback–
leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of Statistics  41(3):1516–
1541  2013.

[4] Aurélien Garivier and Olivier Cappé. The kl-ucb algorithm for bounded stochastic bandits and beyond. In

COLT  pages 359–376  2011.

[5] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil-ucb: An optimal exploration

algorithm for multi-armed bandits. In Conference on Learning Theory  pages 423–439  2014.

[6] Kevin G Jamieson  Lalit Jain  Chris Fernandez  Nicholas J Glattard  and Rob Nowak. Next: A system for
real-world development  evaluation  and application of active learning. In Advances in Neural Information
Processing Systems  pages 2656–2664  2015.

[7] Emilie Kaufmann  Olivier Cappé  and Aurélien Garivier. On the complexity of best arm identiﬁcation in

multi-armed bandit models. The Journal of Machine Learning Research  2016.

[8] Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selection. In

COLT  pages 228–251  2013.

[9] Odalric-Ambrym Maillard  Rémi Munos  Gilles Stoltz  et al. A ﬁnite-time analysis of multi-armed bandits

problems with kullback-leibler divergences. In COLT  pages 497–514  2011.

[10] B. Fox Rubin. How new yorker cartoons could teach computers to be funny. CNET News  2016.

https://www.cnet.com/news/how-new-yorker-cartoons-could-teach-computers-to-be-funny/.

[11] Max Simchowitz  Kevin Jamieson  and Benjamin Recht. The simulator: Understanding adaptive sampling

in the moderate-conﬁdence regime. arXiv preprint arXiv:1702.05186  2017.

10

,Ervin Tanczos
Robert Nowak
Bob Mankoff