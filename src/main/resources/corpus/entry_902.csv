2018,CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces,In this paper  we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end  we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes.  We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace  and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class.  With low dimensionality of capsule subspace as well as an iterative method to estimate the matrix inverse  only a small negligible computing overhead is incurred to train the network. Experiment results on image datasets show the presented network can greatly improve the performance of state-of-the-art Resnet backbones by $10-20\%$ with almost the same computing cost.,CapProNet: Deep Feature Learning via Orthogonal

Projections onto Capsule Subspaces

Liheng Zhang†  Marzieh Edraki†  and Guo-Jun Qi†‡∗
†Laboratory for MAchine Perception and LEarning 

University of Central Florida
http://maple.cs.ucf.edu
‡Huawei Cloud  Seattle  USA

Abstract

In this paper  we formalize the idea behind capsule nets of using a capsule vector
rather than a neuron activation to predict the label of samples. To this end  we
propose to learn a group of capsule subspaces onto which an input feature vector is
projected. Then the lengths of resultant capsules are used to score the probability
of belonging to different classes. We train such a Capsule Projection Network
(CapProNet) by learning an orthogonal projection matrix for each capsule sub-
space  and show that each capsule subspace is updated until it contains input feature
vectors corresponding to the associated class. We will also show that the capsule
projection can be viewed as normalizing the multiple columns of the weight matrix
simultaneously to form an orthogonal basis  which makes it more effective in
incorporating novel components of input features to update capsule representations.
In other words  the capsule projection can be viewed as a multi-dimensional weight
normalization in capsule subspaces  where the conventional weight normalization
is simply a special case of the capsule projection onto 1D lines. Only a small
negligible computing overhead is incurred to train the network in low-dimensional
capsule subspaces or through an alternative hyper-power iteration to estimate the
normalization matrix. Experiment results on image datasets show the presented
model can greatly improve the performance of the state-of-the-art ResNet back-
bones by 10 − 20% and that of the Densenet by 5 − 7% respectively at the same
level of computing and memory expenses. The CapProNet establishes the com-
petitive state-of-the-art performance for the family of capsule nets by signiﬁcantly
reducing test errors on the benchmark datasets.

1

Introduction

Since the idea of capsule net [15  9] was proposed  many efforts [8  17  14  1] have been made to
seek better capsule architectures as the next generation of deep network structures. Among them are
the dynamic routing [15] that can dynamically connect the neurons between two consecutive layers
based on their output capsule vectors. While these efforts have greatly revolutionized the idea of
building a new generation of deep networks  there are still a large room to improve the state of the art
for capsule nets.
In this paper  we do not intend to introduce some brand new architectures for capsule nets. Instead 
we focus on formalizing the principled idea of using the overall length of a capsule rather than

∗Corresponding author: G.-J. Qi  email: guojunq@gmail.com and guojun.qi@huawei.com.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

a single neuron activation to model the presence of an entity [15  9]. Unlike the existing idea in
literature [15  9]  we formulate this idea by learning a group of capsule subspaces to represent a set
of entity classes. Once capsule subspaces are learned  we can obtain set of capsules by performing an
orthogonal projection of feature vectors onto these capsule subspaces.
Then  one can adopt the principle of separating the presence of an entity and its instantiation
parameters into capsule length and orientation  respectively.
In particular  we use the lengths
of capsules to score the presence of entity classes corresponding to different subspaces  while
their orientations are used to instantiate the parameters of entity properties such as poses  scales 
deformations and textures. In this way  one can use the capsule length to achieve the intra-class
invariance in detecting the presence of an entity against appearance variations  as well as model the
equivalence of the instantiation parameters of entities by encoding them into capsule orientations
[15].
Formally  each capsule subspace is spanned by a basis from the columns of a weight matrix in the
neural network. A capsule projection is performed by projecting input feature vectors fed from a
backbone network onto the capsule subspace. Speciﬁcally  an input feature vector is orthogonally
decomposed into the capsule component as the projection onto a capsule subspace and the complement
component perpendicular to the subspace. By analyzing the gradient through the capsule projection 
one can show that a capsule subspace is iteratively updated along the complement component that
contains the novel characteristics of the input feature vector. The training process will continue until
all presented feature vectors of an associated class are well contained by the corresponding capsule
subspace  or simply the back-propagated error accounting for misclassiﬁcation caused by capsule
lengths vanishes.
We call the proposed deep network with the capsule projections the CapProNet for brevity. The
CapProNet is friendly to any existing network architecture – it is built upon the embedded features
generated by some neural networks and outputs the projected capsule vectors in the subspaces
according to different classes. This makes it amenable to be used together with existing network
architectures. We will conduct experiments on image datasets to demonstrate the CapProNet can
greatly improve the state-of-the-art results by sophisticated networks with only small negligible
computing overhead.

1.1 Our Findings

Brieﬂy  we summarize our main ﬁndings from experiments upfront about the proposed CapProNet.
• The proposed CapProNet signiﬁcantly advances the capsule net performance [15] by re-
ducing its test error from 10.3% and 4.3% on CIFAR10 and SVHN to 3.64% and 1.54%
respectively upon the chosen backbones.
• The proposed CapProNet can also greatly reduce the error rate of various backbone networks
by adding capsule projection layers into these networks. For example  The error rate can
be reduced by more than 10 − 20% based on Resnet backbone  and by more than 5 − 6%
based on densenet backbone  with only < 1% and 0.04% computing and memory overhead
in training the model compared with the backbones.
• The orthogonal projection onto capsule subspaces plays a critical role in delivering com-
petitive performance. On the contrary  simply grouping neurons into capsules could not
obviously improve the performance. This shows the capsule projection plays an indispens-
able role in the CapProNet delivering competitive results.
• Our insight into the gradient of capsule projection in Section 2.3 explains the advantage of
updating capsule subspaces to continuously contain novel components of training examples
until they are correctly classiﬁed. We also ﬁnd that the capsule projection can be viewed as a
high-dimensional extension of weight normalization in Section 2.4  where the conventional
weight normalization is merely a simple case of the capsule projection onto 1D lines.

The source code is available at https://github.com/maple-research-lab.
The remainder of this paper is organized as follows. We present the idea of the Capsule Projection
Net (CapProNet) in Section 2  and discuss the implementation details in Section 3. The review of
related work follows in Section 4  and the experiment results are demonstrated in Section 5. Finally 
we conclude the paper and discuss the future work in Section 6.

2

2 The Capsule Projection Nets

In this section  we begin by shortly revisiting the idea of conventional neural networks in classiﬁcation
tasks. Then we formally present the orthogonal projection of input feature vectors onto multiple
capsule subspaces where capsule lengths are separated from their orientations to score the presence
of entities belonging to different classes. Finally  we analyze the gradient of the resultant capsule
projection by showing how capsule subspaces are updated iteratively to adopt novel characteristics of
input feature vectors through back-propagation.

2.1 Revisit: Conventional Neural Networks
Consider a feature vector x ∈ Rd generated by a deep network to represent an input entity. Given its
ground truth label y ∈ {1  2 ···   L}  the output layer of the deep network aims to learn a group of
weight vectors {w1  w2 ···   wL} such that

wT

y x > wT

l x  for all  l (cid:54)= y.

(1)

This hard constraint is usually relaxed to a differentiable softmax objective  and the backpropagation
algorithm is performed to train {w1  w2 ···   wL} and the backbone network generating the input
feature vector x.

2.2 Capsule Projection onto Subspaces

Unlike simply grouping neurons to form capsules for classiﬁcation  we propose to learn a group
of capsule subspaces {S1 S2 ···  SL}  each associated with one of L classes. Suppose we have
a feature vector x ∈ Rd generated by a backbone network from an input sample. Then  to learn
a proper feature representation  we project x onto these capsule subspaces  yielding L capsules
{v1  v2 ···   vL} as projections. Then  we will use the lengths of these capsules to score the
probability of the input sample belonging to different classes by assigning it to the one according to
the longest capsule.
Formally  for each capsule subspace Sl of dimension c  we learn a weight matrix Wl ∈ Rd×c
the columns of which form the basis of the subspace  i.e.  Sl = span(Wl) is spanned by the
column vectors. Then the orthogonal projection vl of a vector x onto Sl is found by solving
vl = arg minv∈span(Wl) (cid:107)x − v(cid:107)2. This orthogonal projection problem has the following closed-
form solution

vl = Plx  and Pl = WlW+
l

l x

l Plx =

vT
l vl =

xT WlΣlWT

(2)
l Wl)−1 can be seen as a normalization matrix applied to the transformed feature
where Σl = (WT
vector WT
l x as a way to normalize the Wl-transformation based on the capsule projection. As we
will discuss in the next subsection  this normalization plays a critical role in updating Wl along the
orthogonal direction of the subspace so that novel components pertaining to the properties of input
entities can be gradually updated to the subspace.
In practice  since c (cid:28) d  the c columns of Wl are usually independent in a high-dimensional d-D
space. Otherwise  one can always add a small I to WT
l Wl to avoid the numeric singularity when
taking the matrix inverse. Later on  we will discuss a fast iterative algorithm to compute the matrix
inverse with a hyper-power sequence that can be seamlessly integrated with the back-propagation
iterations.

2A projection matrix P for a subspace S is a symmetric idempotent matrix (i.e.  PT = P and P2 = P)

such that its range space is S.

3

where Pl is called projection matrix 2 for capsule subspace Sl  and W+
pseudoinverse [4].
l Wl)−1WT
When the columns of Wl are independent  W+
only need the capsule length (cid:107)vl(cid:107)2 to predict the class of an entity  we have

l becomes (WT

l

is the Moore-Penrose

l . In this case  since we

(cid:113)

(cid:107)vl(cid:107)2 =

(cid:113)

xT PT

(cid:113)

2.3

Insight into Gradients

In this section  we take a look at the gradient used to update Wl in each iteration  which can give us
some insight into how the CapProNet works in learning the capsule subspaces.
Suppose we minimize a loss function (cid:96) to train the capsule projection and the network. For simplicity 
we only consider a single sample x and its capsule vl. Then by the chain rule and the differential of
inverse matrix [13]  we have the following gradient of (cid:96) wrt Wl

∂(cid:96)
∂Wl

=

∂(cid:96)

∂(cid:107)vl(cid:107)2

∂(cid:107)vl(cid:107)
∂Wl

=

∂(cid:96)

∂(cid:107)vl(cid:107)2

(I − Pl)xxT W+T

l

(cid:107)vl(cid:107)2

(3)

l

l

∂(cid:96)

∂Wl

denotes the transpose of W+
is the back-propagated error accounting for misclassiﬁcation caused by (cid:107)vl(cid:107)2.

where the operator (I − Pl) can be viewed as the projection onto the orthogonal complement of the
capsule subspace spanned by the columns of Wl  W+T
l   and the factor
∂(cid:107)vl(cid:107)2
Denote by x⊥ (cid:44) (I − Pl)x the projection of x onto the orthogonal component perpendicular to the
current capsule subspace Sl. Then  the above gradient ∂(cid:96)
only contains the columns parallel to x⊥
(up to coefﬁcients in the vector xT W+T
). This shows that the basis of the current capsule subspace
Sl in the columns of Wl is updated along this orthogonal component of the input x to the subspace.
One can regard x⊥ as representing the novel component of x not yet contained in the current Sl  it
shows capsule subspaces are updated to contain the novel component of each input feature vector
until all training feature vectors are well contained in these subspaces  or the back-propagated errors
vanish that account for misclassiﬁcation caused by (cid:107)vl(cid:107)2.
Figure 1 illustrates an example of 2-D capsule
subspace S spanned by two basis vectors w1
and w2. An input feature vector x is decom-
posed into the capsule projection v onto S and
an orthogonal complement x⊥ perpendicular to
the subspace. In one training iteration  two basis
1 and w(cid:48)
vectors w1 and w2 are updated to w(cid:48)
2
along the orthogonal direction x⊥  where x⊥ is
viewed as containing novel characteristics of an
entity not yet contained by S.

2.4 A Perspective of Multiple-Dimensional
Weight Normalization

Figure 1: This ﬁgure illustrates a 2-D capsule sub-
space S spanned by two basis vectors w1 and w2.
An input feature vector x is decomposed into the
capsule projection v onto S and an orthogonal
complement x⊥ perpendicular to the subspace. In
one training iteration  two basis vectors w1 and
w2 are updated to w(cid:48)
2 along the orthogonal
direction x⊥  where x⊥ is viewed as containing
novel characteristics of an entity not yet contained
by S.

As discussed in the last subsection and Figure 2 
we can explain the orthogonal components rep-
resent the novel information in input data  and
the orthogonal decomposition thus enables us
to update capsule subspaces by more effectively
incorporating novel characteristics/components
than the classic capsule nets.
One can also view the capsule projection as nor-
malizing the column basis of weight matrix Wl
simultaneously in a high-dimensional capsule
space. If the capsule dimension c is set to 1  it is
not hard to see that Eq. (2) can be rewritten by
l x|
setting vl to |WT
(cid:107)Wl(cid:107) . It produces the conventional weight normalization of the vector Wl ∈ Rd  as a
special 1D case of the capsule projection. As the capsule dimension c grows  Wl can be normalized
l x  which keeps (cid:107)vl(cid:107) unchanged in Eq. (2). This enables us to extend
by replacing vl with Σ1/2
the conventional weight normalization to high dimensional capsule subspaces.

1 and w(cid:48)

l WT

4

3

Implementation Details

We will discuss some implementation details in this section  including 1) the computing cost to
perform capsule projection and a fast iterative method by using hyper-power sequences without
restart; 2) the objective to train the capsule projection.

3.1 Computing Normalization Matrix

Taking a matrix inverse to get the normalization matrix Σl would be expensive with an increasing
dimension c. But after the model is trained  it is ﬁxed in the inference with only one-time computing.
Fortunately  the dimension c of a capsule subspace is usually much smaller than the feature dimension
d that is usually hundreds and even thousands. For example  c is typically no larger than 8 in
experiments. Thus  taking a matrix inverse to compute these normalization matrices only incurs
a small negligible computing overhead compared with the training of many other layers in a deep
network.
Alternatively  one can take advantage of an iterative algorithm to compute the normalization matrix.
We consider the following hyper-power sequence

Σl ← 2Σl − ΣlWT

l WlΣl

which has proven to converge to (WT W)−1 with a proper initial point [2  3]. In stochastic gradient
method  since only a small change is made to update Wl in each training iteration  thus it is often
sufﬁcient to use this recursion to make an one-step update on the normalization matrix from the last
l Wl)−1 at the very ﬁrst iteration to
iteration. The normalization matrix Σl can be initialized to (WT
give an ideal start. This could further save computing cost in training the network.
In experiments  a very small computing overhead was incurred in the capsule projection. For example 
training the ResNet110 on CIFAR10/100 costed about 0.16 seconds per iteration on a batch of 128
images. In comparison  training the CapProNet with a ResNet110 backbone in an end-to-end fashion
only costed an additional < 0.001 seconds per iteration  that is less than 1% computing overhead
for the CapProNet compared with its backbone. For the inference  we did not ﬁnd any noticeable
computing overhead for the CapProNet compared with its backbone network.

3.2 Training Capsule Projections
Given a group of capsule vectors {v1  v2 ···   vL} corresponding to a feature vector x and its ground
truth label y  we train the model by requiring

(cid:107)vy(cid:107)2 > (cid:107)vl(cid:107)2  for all  l (cid:54)= y.

In other words  we require (cid:107)vy(cid:107)2 should be larger than all the length of the other capsules. As
a consequence  we can minimize the following negative logarithmic softmax function (cid:96)(x  y) =
(cid:80)L
exp((cid:107)vy(cid:107)2)
− log
l=1 exp((cid:107)vl(cid:107)2) to train the capsule subspaces and the network generating x through back-
propagation in an end-to-end fashion. Once the model is trained  we will classify a test sample into
the class with the longest capsule.

4 Related Work

The presented CapProNets are inspired by the CapsuleNets by adopting the idea of using a capsule
vector rather than a neural activation output to predict the presence of an entity and its properties
[15  9]. In particular  the overall length of a capsule vector is used to represent the existence of the
entity and its direction instantiates the properties of the entity. We formalize this idea in this paper by
explicitly learning a group of capsule subspaces and project embedded features onto these subspaces.
The advantage of these capsule subspaces is their directions can represent characteristics of an entity 
which contains much richer information  such as its positions  orientations  scales and textures 
than a single activation output. By performing an orthogonal projection of an input feature vector
onto a capsule subspace  one can ﬁnd the best direction revealing these properties. Otherwise  the
entity is thought of being absent as the projection vanishes when the input feature vector is nearly
perpendicular to the capsule subspace.

5

5 Experiments

We conduct experiments on benchmark datasets to evaluate the proposed CapProNet compared with
the other deep network models.

5.1 Datasets

We use both CIFAR and SVHN datasets in experiments to evaluate the performance.
CIFAR The CIFAR dataset contains 50  000 and 10  000 images of 32 × 32 pixels for the training
and test sets respectively. A standard data augmentation is adopted with horizonal ﬂipping and
shifting. The images are labeled with 10 and 100 categories  namely CIFAR10 and CIFAR100
datasets. A separate validation set of 5  000 images are split from the training set to choose the model
hyperparameters  and the ﬁnal test errors are reported with the chosen hyperparameters by training
the model on all 50  000 training images.
SVHN The Street View House Number (SVHN) dataset has 73  257 and 26  032 images of colored
digits in the training and test sets  with an additional 531  131 training images available. Following
the widely used evaluation protocol in literature [5  11  12  16]  all the training examples are used
without data augmentation  while a separate validation set of 6  000 images is split from the training
set. The model with the smallest validation error is selected and the error rate is reported.
ImageNet The ImageNet data-set consists of 1.2 million training and 50k validation images. We
apply mean image subtraction as the only pre-processing step on images and use random cropping 
scaling and horizontal ﬂipping for data augmentation [6]. The ﬁnal resolution of both train and
validation sets is 224 × 224  and 20k images are chosen randomly from training set for tuning hyper
parameters.

5.2 Backbone Networks

We test various networks such as ResNet [6]  ResNet (pre-activation) [7]  WideResNet [18] and
Densenet [10] as the backbones in experiments. The last output layer of a backbone network is
replaced by the capsule projection  where the feature vector from the second last layer of the backbone
is projected onto multiple capsule subspaces.
The CapProNet is trained from the scratch in an end-to-end fashion on the given training set. For the
sake of fair comparison  the strategies used to train the respective backbones [6  7  18]  such as the
learning rate schedule  parameter initialization  and the stochastic optimization solver  are adopted to
train the CapProNet. We will denote the CapProNet with a backbone X by CapProNet+X below.

5.3 Results

We perform experiments with various networks as backbones for comparison with the proposed
CapProNet. In particular  we consider three variants of ResNets – the classic one reported in [11] with
110 layers  the ResNet with pre-activation [7] with 164 layers  and two paradigms of WideResNets
[18] with 16 and 28 layers  as well as densenet-BC [10] with 100 layers. Compared with ResNet
and ResNet with pre-activation  WideResNet has fewer but wider layers that reaches smaller error
rates as shown in Table 1. We test the CapProNet+X with these different backbone networks to
evaluate if it can consistently improve these state-of-the-art backbones. It is clear from Table 1
that the CapProNet+X outperforms the corresponding backbone networks by a remarkable margin.
For example  the CapProNet+ResNet reduces the error rate by 19%  17.5% and 10% on CIFAR10 
CIFAR100 and SVHN  while CapProNet+Densenet reduces the error rate by 5.8%  4.8% and 6.8%
respectively. Finally  we note that the CapProNet signiﬁcantly advances the capsule net performance
[15] by reducing its test error from 10.3% and 4.3% on CIFAR10 and SVHN to 3.64% and 1.54%
respectively based on the chosen backbones.
We also evaluate the CapProNet with Resnet50 and Resnet101 backbones for single crop Top-1/Top-5
results on ImageNet validation set. To ensure fair comparison  we retrain the backbone networks
based on the ofﬁcal Resnet model3  where both original Resnet[6] and CapProNet are trained with
the same training strategies on four GPUs. The results are reported in Table 2  where CapProNet+X

3https://github.com/tensorﬂow/models/tree/master/ofﬁcial/resnet

6

Table 1: Error rates on CIFAR10  CIFAR100  and SVHN. The best results are highlighted in bold
for the methods with the same network architectures. Not all results on different combinations of
network backbones or datasets have been reported in literature  and missing results are remarked “-"
in the table.

Method

ResNet [6]

ResNet (reported by [11])
CapProNet+ResNet (c=2)
CapProNet+ResNet (c=4)
CapProNet+ResNet (c=8)
ResNet (pre-activation) [7]

CapProNet+ResNet (pre-activation c=4)
CapProNet+ResNet (pre-activation c=8)

WideResNet [18]

with Dropout

CapProNet+WideResNet (c=4)

with Dropout

CapProNet+WideResNet (c=8)

with Dropout

Densenet-BC k=12 [10]

CapProNet Densenet-BC k=12 (c=4)
CapProNet Densenet-BC k=12 (c=8)

Depth
110
110
110
110
110
164
164
164
16
28
16
16
28
16
16
28
16
100
100
100

-

6.61
6.41
5.24
5.27
5.19
5.46
4.88
4.89
4.81
4.17

27.22
22.65
22.45
21.93
24.33
21.37
20.91
22.07
20.50

Params CIFAR10 CIFAR100
1.7M
1.7M
1.7M
1.7M
1.7M
1.7M
1.7M
1.7M
11.0M
36.5M
2.7M
11.0M
36.5M
2.7M
11.0M
36.5M
2.7M
0.8M
0.8M
0.8M

22.27
21.22
21.19

4.51
4.35
4.25

20.12
19.83

21.33
19.98

4.04
3.85

4.20
3.64

-

-

-

-

-

-

SVHN

-

2.01
1.79
1.82
1.79

-
-
-
-
-

-
-

-
-

1.64

1.58

1.54
1.76
1.64
1.64

Table 2: The CapProNet results with Resnet50 and Resnet101 backbones for Single crop top-1/top-5
error rate on ImageNet validation set with image resolution of 224 × 224  as well as the comparison
with original baseline results.
reported result[6]

CapProNet (c=2) CapProNet (c=4) CapProNet (c=8)

23.282 / 6.8
22.192 / 6.178

23.265 / 6.648

21.89 / 6

23.203 / 6.78
21.9 / 6.01

Method
Resnet50
Resnet101

24.8 / 7.8
23.6 / 7.1

our rerun
24.09/7.13
22.81 /6.67

successfully outperforms the original backbones on both Top-1 and Top-5 error rates. It is worth
noting the gains are only obtained with the last layer of backbones replaced by the capsule project
layer. We believe the error rate can be further reduced by replacing the intermediate convolutional
layers with the capsule projections  and we leave it to our future research.
We also note that the CapProNet+X consistently outperforms the backbone counterparts with varying
dimensions c of capsule subspaces. In particular  with the WideResNet backbones  in most cases  the
error rates are reduced with an increasing capsule dimension c on all datasets  where the smallest
error rates often occur at c = 8. In contrast  while CapProNet+X still clearly outperforms both
ResNet and ResNet (pre-activation) backbones  the error rates are roughly at the same level. This is
probably because both ResNet backbones have a much smaller input dimension d = 64 of feature
vectors into the capsule projection than that of WideResNet backbone where d = 128 and d = 160
with 16 and 28 layers  respectively. This turns out to suggest that a larger input dimension can enable
to use capsule subspaces of higher dimensions to encode patterns of variations along more directions
in a higher dimensional input feature space.
To further assess the effect of capsule projection  we compare with the method that simply groups
the output neurons into capsules without performing orthogonal projection onto capsule subspaces.
We still use the lengths of these resultant “capsules" of grouped neurons to classify input images
and the model is trained in an end-to-end fashion accordingly. Unfortunately  this approach  namely
GroupNeuron+ResNet in Table 3  does not show signiﬁcant improvement over the backbone network.
For example  the smallest error rate by GroupNeuron+ResNet is 6.26 at c = 2  a small improvement

7

Figure 2: These ﬁgures plot the 2-D capsule subspaces and projected capsules corresponding to
ten classes on CIFAR10 dataset. In each ﬁgure  red capsules represent samples from the class
corresponding to the subspace  while green capsules belong to a different class. It shows red samples
have larger capsule length (relative to the origin) than those of green samples. This validates the
capsule length as the classiﬁcation criterion in the proposed model. Note that some ﬁgures have
different scales in two axes for a better illustration.

over the error rate of 6.41 reached by ResNet110. This demonstrates the capsule projection makes an
indispensable contribution to improving model performances.
When training on CIFAR10/100 and SVHN  one iteration typically costs ∼ 0.16 seconds for Resnet-
110  with an additional less than 0.01 second to train the corresponding CapProNet. That is less
than 1% computing overhead. The memory overhead for the model parameters is even smaller. For
example  the CapProNet+ResNet only has an additional 640 − 6400 parameters at c = 2 compared
with 1.7M parameters in the backbone ResNet. We do not notice any large computing or memory
overheads with the ResNet (pre-activation) or WideResNet  either. This shows the advantage of
CapProNet+X as its error rate reduction is not achieved by consuming much more computing and
memory resources.

5.4 Visualization of Projections onto Capsule Subspaces

2 WT

l Wl)− 1

Table 3: Comparison between GroupNeuron and
CapProNet with the ResNet110 backbone on CI-
FAR10 dataset. The best results are highlighted in
bold for c = 2  4  8 capsules. It shows the need of
capsule projection to obtain better results.

To give an intuitive insight into the learned cap-
sule subspaces  we plot the projection of input
feature vectors onto capsule subspaces. Instead
of directly using Plx to project feature vectors
onto capsule subspaces in the original input s-
pace Rd  we use (WT
l x to project
an input feature vector x onto Rc  since this
projection preserves the capsule length (cid:107)vl(cid:107)2
deﬁned in (2).
Figure 2 illustrates the 2-D capsule subspaces
learned on CIFAR10 when c = 2 and d = 64 in
CapProNet+ResNet110  where each subspace
corresponds to one of ten classes. Red points
represent the capsules projected from the class of input samples corresponding to the subspace while
green points correspond to one of the other classes. The ﬁgure shows that red capsules have larger
length than green ones  which suggests the capsule length is a valid metric to classify samples into
their corresponding classes. Meanwhile  the orientation of a capsule reﬂects various instantiations of
a sample in these subspaces. These ﬁgures visualize the separation of the lengths of capsules from
their orientations in classiﬁcation tasks.

c GroupNeuron CapProNet
2
4
8

5.24
5.27
5.19

6.26
6.29
6.42

8

6 Conclusions and Future Work

In this paper  we present a novel capsule project network by learning a group of capsule subspaces for
different classes. Speciﬁcally  the parameters of an orthogonal projection is learned for each class and
the lengths of projected capsules are used to predict the entity class for a given input feature vector.
The training continues until the capsule subspaces contain input feature vectors of corresponding
classes or the back-propagated error vanishes. Experiment results on real image datasets show that
the proposed CapProNet+X could greatly improve the performance of backbone network without
incurring large computing and memory overheads. While we only test the capsule projection as the
output layer in this paper  we will attempt to insert it into intermediate layers of backbone networks as
well  and hope this could give rise to a new generation of capsule networks with more discriminative
architectures in future.

Acknowledgements

L. Zhang and M. Edraki made equal contributions to implementing the idea: L. Zhang conducted
experiments on CIFAR10 and SVHN datasets  and visualized projections in capsule subspaces on
CIFAR10. M. Edraki performed experiments on CIFAR100. G.-J. Qi initialized and formulated the
idea  and prepared the paper.

References
[1] Mohammad Taha Bahadori. Spectral capsule networks. 6th International Conference on Learning

Representations  2018.

[2] Adi Ben-Israel. An iterative method for computing the generalized inverse of an arbitrary matrix. Mathe-

matics of Computation  19(91):452–455  1965.

[3] Adi Ben-Israel and Dan Cohen. On iterative computation of generalized inverses and associated projections.

SIAM Journal on Numerical Analysis  3(3):410–419  1966.

[4] Adi Ben-Israel and Thomas NE Greville. Generalized inverses: theory and applications  volume 15.

Springer Science & Business Media  2003.

[5] Ian J Goodfellow  David Warde-Farley  Mehdi Mirza  Aaron Courville  and Yoshua Bengio. Maxout

networks. arXiv preprint arXiv:1302.4389  2013.

[6] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

[7] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual networks.

In European Conference on Computer Vision  pages 630–645. Springer  2016.

[8] Geoffrey Hinton  Nicholas Frosst  and Sara Sabour. Matrix capsules with em routing. 2018.

[9] Geoffrey E Hinton  Alex Krizhevsky  and Sida D Wang. Transforming auto-encoders. In International

Conference on Artiﬁcial Neural Networks  pages 44–51. Springer  2011.

[10] Gao Huang  Zhuang Liu  Laurens Van Der Maaten  and Kilian Q Weinberger. Densely connected

convolutional networks. In CVPR  volume 1  page 3  2017.

[11] Gao Huang  Yu Sun  Zhuang Liu  Daniel Sedra  and Kilian Q Weinberger. Deep networks with stochastic

depth. In European Conference on Computer Vision  pages 646–661. Springer  2016.

[12] Chen-Yu Lee  Saining Xie  Patrick Gallagher  Zhengyou Zhang  and Zhuowen Tu. Deeply-supervised nets.

In Artiﬁcial Intelligence and Statistics  pages 562–570  2015.

[13] Kaare Brandt Petersen et al. The matrix cookbook.

[14] David Rawlinson  Abdelrahman Ahmed  and Gideon Kowadlo. Sparse unsupervised capsules generalize

better. arXiv preprint arXiv:1804.06094  2018.

[15] Sara Sabour  Nicholas Frosst  and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in

Neural Information Processing Systems  pages 3859–3869  2017.

9

[16] Pierre Sermanet  Soumith Chintala  and Yann LeCun. Convolutional neural networks applied to house
numbers digit classiﬁcation. In Pattern Recognition (ICPR)  2012 21st International Conference on  pages
3288–3291. IEEE  2012.

[17] Dilin Wang and Qiang Liu. An optimization view on dynamic routing between capsules. 6th International

Conference on Learning Representations  2018.

[18] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 

2016.

10

,Liheng Zhang
Marzieh Edraki
Guo-Jun Qi
Micah Carroll
Rohin Shah
Mark Ho
Tom Griffiths
Sanjit Seshia
Pieter Abbeel
Anca Dragan