2019,Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration,The alternating direction method of multipliers (ADMM) is one of the most widely used first-order optimisation methods in the literature owing to its simplicity  flexibility and efficiency. Over the years  numerous efforts are made to improve the performance of the method  such as the inertial technique. By studying the geometric properties of ADMM  we discuss the limitations of current inertial accelerated ADMM and then present and analyze an adaptive acceleration scheme for the method. Numerical experiments on problems arising from image processing  statistics and machine learning demonstrate the advantages of the proposed acceleration approach.,Trajectory of Alternating Direction Method of

Multipliers and Adaptive Acceleration

Clarice Poon∗

University of Bath  Bath UK

cmhsp20@bath.ac.uk

Jingwei Liang∗

University of Cambridge  Cambridge UK

jl993@cam.ac.uk

Abstract

The alternating direction method of multipliers (ADMM) is one of the most widely
used ﬁrst-order methods in the literature owing to its simplicity  ﬂexibility and
efﬁciency. Over the years  numerous efforts are made to improve the performance
of ADMM  such as the inertial technique. By studying the geometric properties
of ADMM  we discuss the limitations of current inertial accelerated ADMM  then
present and analyze an adaptive acceleration scheme for the method. Numerical
experiments on problems arising from image processing  statistics and machine
learning demonstrate the advantages of the proposed acceleration approach.

Introduction

1
Consider the following constrained and composite optimisation problem

min

x∈Rn y∈Rm

R(x) + J(y)

such that Ax + By = b 

(PADMM)

where the following basic assumptions are imposed

(A.1) R ∈ Γ0(Rn) and J ∈ Γ0(Rm) are proper convex and lower semi-continuous functions.
(A.2) A : Rn → Rp and B : Rm → Rp are injective linear operators.
(A.3) ri(dom(R) ∩ dom(J)) (cid:54)= ∅  and the set of minimizers is non-empty.

Over the past years  problem (PADMM) has attracted a great deal of interests as it covers many
problems arising from data science  machine learning  statistics  inverse problems and imaging  etc.;
See Section 5 for examples. In the literature  different methods are proposed to handle the problem 
among them the alternating direction method of multipliers (ADMM) is the most prevailing one.
Earlier works of ADMM include [17  16  15  11]  and recently it has gained increasing popularity  in
part due to [6]. To derive ADMM  ﬁrst consider the augmented Lagrangian associated to (PADMM)
L(x  y; ψ) def= R(x) + J(y) + (cid:104)ψ  Ax + By − b(cid:105) + γ
2||Ax + By − b||2  where γ > 0 and ψ ∈ Rp is
the Lagrangian multiplier. To ﬁnd a saddle-point of L(x  y; ψ)  ADMM applies the iteration

xk = argminx∈Rn R(x) + γ
yk = argminy∈Rm J(y) + γ
ψk = ψk−1 + γ(Axk + Byk − b).

2||Ax + Byk−1 − b + 1
2||Axk + By − b + 1

γ ψk−1||2 
γ ψk−1||2 

(1)

(2)

By deﬁning zk

def= ψk−1 + γAxk  we can rewrite ADMM iteration (1) into the following form

xk = argminx∈Rn R(x) + γ
2
zk = ψk−1 + γAxk 
yk = argminy∈Rm J(y) + γ
2
ψk = zk + γ(Byk − b).

||Ax − 1

γ (zk−1 − 2ψk−1)||2 

||By + 1

γ (zk − γb)||2 

For the rest of the paper  we will consider the above four-point formulation.

∗Equal contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Contributions The contribution of our paper is threefold. First  for the sequence {zk}k∈N of (2) 
we show that it has two different types of trajectory:
• When both R  J are non-smooth functions  under the assumption that they are partly smooth (see
Deﬁnition 2.1)  we show that the eventual trajectory of {zk}k∈N is approximately a spiral which
can be characterized precisely if R  J are moreover locally polyhedral around the solution.
• When at least one of R  J is smooth  we show that depends on the choice of γ  the eventual
trajectory of {zk}k∈N can be either straight line or spiral.
Second  based on trajectory of {zk}k∈N  we discuss the limitations of the current combination of
ADMM and inertial acceleration technique. In Section 3  we distinguish the situations where inertial
acceleration will work and when it fails. More precisely: inertial technique will work if the trajectory
of {zk}k∈N is or close to a straight line  and will fail if the trajectory is a spiral.
Our core contribution is an adaptive acceleration for ADMM  which is inspired by the trajectory of
ADMM and dubbed “A3DMM”. The limitation of inertial technique  particularly its failure  implies
that the right acceleration scheme should be able to follow the trajectory of the iterates. In Section 4 
we propose an adaptive linear prediction scheme to accelerate ADMM which is able to following
the trajectory of the method. Our proposed A3DMM belongs to the realm of extrapolation method 
and provides an alternative geometrical interpretation for polynomial extrapolation methods such as
Minimal Polynomial Extrapolation (MPE) [9] and Reduced Rank Extrapolation (RRE) [12  21].
Related works Over the past decades  owing to the tremendous success of inertial acceleration [22 
5]  the inertial technique has been widely adapted to accelerate other ﬁrst-order methods. In terms of
ADMM  related work can be found in [23  18  14]  either from proximal point algorithm perspective or
continuous dynamical system. However  to ensure that inertial acceleration works  strong assumptions
are imposed on R  J in (PADMM)  such as smooth differentiability or strong convexity. When it
comes to general non-smooth problems  these works may fail to provide acceleration. Recently in
[13]  an O(1/k2) convergence rate is established for ADMM using Nesterov acceleration  however
the result holds only for the continuous dynamical system while the discrete-time optimization
scheme remains unavailable.
For more generic acceleration techniques  there are extensive works in numerical analysis on the topic
of convergence acceleration for sequences. The goal of convergence acceleration is  given an arbitrary
sequence {zk}k∈N ⊂ Rn with limit z(cid:63)  ﬁnding a transformation Ek : {zk−j}q
j=1 → ¯zk ∈ Rn such
that ¯zk converges faster to z(cid:63). In general  the process by which {zk}k∈N is generated is unknown 
q is chosen to be a small integer  and ¯zk is referred to as the extrapolation of zk. Some of the best
known examples include Richardson’s extrapolation [24]  the ∆2-process of Aitken [1] and Shank’s
algorithm [26]. We refer to [7  8  27] and references therein for a detailed historical perspective on
the development of these techniques. Much of the works on the extrapolation of vector sequences was
initiated by Wynn [29] who generalized the work of Shank to vector sequences. In the supplementary
material  the formulation of some of these methods are provided. In particular  minimal polynomial
extrapolation (MPE) [9] and Reduced Rank Extrapolation (RRE) [12  21] (which is also a variant of
Anderson acceleration developed independently in [3])  which are particularly relevant to this present
work (see Section 4.2 for brief discussion).
More recently  there has been a series of work on a regularised version of RRE stemming from
[25]. We remark however the regularisation parameter in these works rely on a grid search based on
objective function  their applicability to the general ADMM setting is unclear.
Notations Denote Rn a n-dimensional Euclidean space equipped with scalar product (cid:104)·  ·(cid:105) and norm
|| · ||. Id denotes the identity operator on Rn. Γ0(Rn) denotes the class of proper convex and lower-
semicontinuous functions on Rn. For a nonempty convex set S ⊂ Rn  denote ri(S) its relative interior 
par(S) the smallest subspace parallel to S and PS the projection operator onto S. The sub-differential

of a function R ∈ Γ0(Rn) is deﬁned by ∂R(x) def=(cid:8)g ∈ Rn|R(x(cid:48)) ≥ R(x) +(cid:104)g  x(cid:48)− x(cid:105) ∀x(cid:48) ∈ Rn(cid:9).

The spectral radius of a matrix M is denoted by ρ(M ).
2 Trajectory of ADMM
In this section  we discuss the trajectory of the sequence {zk}k∈N generated by ADMM based on the
concept “partial smoothness” which was ﬁrst introduced in [19].
2.1 Partial smoothness
Let M ⊂ Rn be a C 2-smooth submanifold  denote TM(x) the tangent space of M at a point x ∈ M.

2

Deﬁnition 2.1 (Partly smooth function [19]). A function R ∈ Γ0(Rn) is partly smooth at ¯x relative
to a set M¯x if ∂R(¯x) (cid:54)= ∅ and M¯x is a C 2 manifold around ¯x  and moreover

Smoothness R restricted to M¯x is C 2 around ¯x.
Sharpness The tangent space TM¯x (¯x) = par(∂R(¯x))⊥.
Continuity The set-valued mapping ∂R is continuous at x relative to M¯x.

The class of partly smooth functions at ¯x relative to M¯x is denoted as PSF¯x(M¯x). Popular examples
of partly smooth functions can be found in [20  Chapter 5]. Loosely speaking  a partly smooth
function behaves smoothly as we move along M¯x  and sharply if we move transversal to it.
2.2 Trajectory of ADMM
The iteration of ADMM is non-linear in general owing to the non-smoothness and non-linearity of
R and J. However  if they are partly smooth  the local C 2-smoothness allows us to linearize the
ADMM iteration  and hence enables us to study the trajectory of sequence generated by the method.
We denote (x(cid:63)  y(cid:63)  ψ(cid:63)) a saddle-point of L(x  y; ψ) and let z(cid:63) = ψ(cid:63) + γAx(cid:63).
def= zk − zk−1 and
To discuss the trajectory of ADMM  we rely on sequence {zk}k∈N. Deﬁne vk
(cid:104)vk  vk−1(cid:105)
||vk||||vk−1|| ) the angle between vk  vk−1. We use {θk}k∈N to characterize the trajectory
θk
of {zk}k∈N. Given (x(cid:63)  y(cid:63)  ψ(cid:63))  the ﬁrst-order optimality condition entails −AT ψ(cid:63) ∈ ∂R(x(cid:63)) and
−BT ψ(cid:63) ∈ ∂J(y(cid:63))  below we impose

def= arccos(

− AT ψ(cid:63) ∈ ri(cid:0)∂R(x(cid:63))(cid:1)

and − BT ψ(cid:63) ∈ ri(cid:0)∂J(y(cid:63))(cid:1).

x(cid:63)  MJ

def= A ◦ P

  BJ

T R
x(cid:63)

x(cid:63) )  J ∈ PSFy(cid:63) (MJ

y(cid:63) at x(cid:63)  y(cid:63)  respectively. Let AR

y(cid:63) ) are partly smooth. Denote T R
def= B ◦ P

(ND)
Both R  J are non-smooth Let MR
y(cid:63) be two smooth manifolds around x(cid:63)  y(cid:63) respectively 
and suppose R ∈ PSFx(cid:63) (MR
y(cid:63) the tangent
x(cid:63)   T J
spaces of MR
y(cid:63) and TAR   TBJ
be the range of AR  BJ respectively. Denote (αj)j=1 ... the Principal angles (see Section D.2 in the
supplementary for deﬁnition) between TAR   TBJ   and let αF   α(cid:48) be the smallest and 2nd smallest of
αj which are larger than 0.
Theorem 2.2. For problem (PADMM) and ADMM iteration (1)  assume that conditions (A.1)-(A.3)
are true  then (xk  yk  ψk) converges to a saddle point (x(cid:63)  y(cid:63)  ψ(cid:63)) of L(x  y; ψ). Suppose that
R ∈ PSFx(cid:63) (MR
(i) There exists a matrix M such that vk = M vk−1 + o(||vk−1||) holds for all k large enough.
(ii) If moreover  R  J are locally polyhedral around x(cid:63)  y(cid:63)  then vk = M vk−1 with M being
normal and having eigenvalues of the form cos(αj)e±iαj   and cos(θk) = cos(αF ) + O(η2k)
with η = cos(α(cid:48))/ cos(αF ).

y(cid:63) ) and condition (ND) holds  then

x(cid:63) )  J ∈ PSFy(cid:63) (MJ

x(cid:63)  MJ

T J

Remark 2.3. The result indicates that  when both R  J are locally polyhedral  the trajectory of
{zk}k∈N is a spiral. For the case R  J being general partly smooth function  though we cannot prove 
numerical evidence shows that the trajectory of {zk}k∈N could be either straight line or also a spiral.
R or/and J is smooth Now we consider the case that at least one function out of R  J is smooth.
For simplicity  consider that R is smooth and J remains non-smooth.
Proposition 2.4. For problem (PADMM) and ADMM iteration (1)  assume that conditions (A.1)-
(A.3) are true  then (xk  yk  ψk) converges to a saddle point (x(cid:63)  y(cid:63)  ψ(cid:63)) of L(x  y; ψ). Suppose R
is locally C 2 around x(cid:63)  J ∈ PSFy(cid:63) (MJ
y(cid:63) ) is partly smooth and condition (ND) holds for J  then
Theorem 2.2(i) holds for all k large enough. If moreover  A is full rank square matrix  then all the
eigenvalues of M are real for γ > ||(AT A)− 1
Remark 2.5. The spectrum of M is real  numerical evidence shows that the eventual trajectory of
{zk}k∈N is a straight line  which is different from the case where both functions are non-smooth. If
o(||vk−1||) is vanishing fast enough  we can also prove that θk → 0.
It should be emphasized that the trajectory is determined by the property of the leading eigenvalue of
M. Therefore  for γ ≤ ||(AT A)− 1
2||  though M will have complex eigenvalues 
the leading one is not necessarily to be complex. As a result  the trajectory of {zk}k∈N could be
either spiral (complex leading eigenvalue) or straight line (real leading eigenvalue).
In Figure 1 (a) and (c)  we present two examples of the trajectory of ADMM. Subﬁgure (a) shows a
spiral trajectory in R2 which is obtained from solving a polyhedral problem  while subﬁgure (c) is an
eventual straight line trajectory in R3.

2∇2R(x(cid:63))(AT A)− 1
2||.

2∇2R(x(cid:63))(AT A)− 1

3

(a) Spiral

(b) γ =

||K||2
10

(c) Eventual straight line

(d) γ = ||K||2 + 0.1

Figure 1: Trajectory of sequence {zk}k∈N and effects of inertial on ADMM. (a) Spiral trajectory of
ADMM; (b) failure of inertial ADMM on spiral trajectory; (c) Eventual straight line trajectory; (d)
success of inertial ADMM on straight line trajectory.

3 The failure of inertial acceleration
One simple approach for combining inertial technique with ADMM is described below

xk = argminx∈Rn R(x) + γ
zk = ψk−1 + γAxk 
¯zk = zk + ak(zk − zk−1) 
yk = argminy∈Rm J(y) + γ
ψk = ¯zk + γ(Byk − b) 

2||Ax − 1

γ (¯zk−1 − 2ψk−1)||2 

2||By + 1

γ (¯zk − γb)||2 

(3)

which considers only the momentum of {zk}k∈N without any stronger assumptions on R  J. The
above scheme can reformulated as an instance of inertial Proximal Point Algorithm  guaranteed to be
convergent for ak < 1
3 [2]; We refer to [23] or [20  Chapter 4.3] for more details. To our knowledge 
there is no acceleration guarantee for (3).
Remark 3.1. Besides (3)  other combinations of inertial technique and ADMM are also proposed 
see for instance [23  18]. To ensure acceleration guarantees  stronger assumptions  such as Lipschitz
smoothness and strong convexity  are needed.
We use LASSO problem to demonstrate the combination of the above inertial technique and ADMM 
especially when it failures. The formulation of LASSO in the form of (PADMM) reads

µ||x||1 + 1

||Ky − f||2

such that x − y = 0 

2

min
x y∈Rn

(4)
where K ∈ Rm×n  m < n is a random Gaussian matrix. Since 1
2||Ky − f||2 is quadratic  owing to
Proposition 2.4  the eventual trajectory of {zk}k∈N is a straight line if γ > ||K||2  and a spiral for
some γ ≤ ||K||2. Therefore  we consider two different choices of γ which are γ = ||K||2/10 and
γ = ||K||2 + 0.1  and for each γ  four different choices of ak are considered

ak ≡ 0.3 

ak ≡ 0.7

and ak = k−1
k+3 .

The 3rd choice of ak corresponds to FISTA [10]. For the numerical example  we let K ∈ R640×2048
and µ = 1  f is the measurement of an 128-sparse signal. The results are shown in Figure 1 (b) & (d) 
• γ = ||K||2/10: The inertial scheme works only for ak ≡ 0.3  which is due to that fact that the
trajectory of {zk}k∈N is a spiral for γ = ||K||2/10. As a result  the direction zk − zk−1 is not
pointing towards z(cid:63)  hence unable to provide satisfactory acceleration.
• γ = ||K||2 + 0.1: All choices of ak work since {zk}k∈N eventually forms a straight line. Among
these four choices of ak  ak ≡ 0.7 is the fastest  while ak = k−1

k+3 eventually is the slowest.
It should be noted that  though ADMM is faster under γ = ||K||2/10 than γ = ||K||2 + 0.1  our main
focus here is to show how the trajectory of {zk}k∈N affects the outcome of inertial acceleration.
||K||2
10   imply that the trajectory of {zk}k∈N is crucial
The above comparisons  particularly for γ =
for the acceleration outcome of the inertial scheme (3). Since the trajectory of {zk}k∈N depends on
the properties of R  J and choice of γ  this implies that the right scheme that can achieve uniform
acceleration despite R  J and γ should be able to adapt itself to the trajectory of the method. More
discussions on the failure of inertial can be found in Section A of the supplementary material.

4

5010015020010-810-41002004006008001000120014001600180010-810-41004 A3DMM: adaptive acceleration for ADMM
The previous section shows the trajectory of {zk}k∈N eventually settles onto a regular path i.e. either
straight line or spiral. In this section  we exploit this regularity to design adaptive acceleration for
ADMM  which is called “A3DMM”; See Algorithm 1.
The update of ¯zk in (3) can be viewed as a special case of the following extrapolation

¯zk = E(zk  zk−1 ···   zk−q−1) 
j=0  deﬁne vj
def= [vk−1 ···   vk−q] ∈ Rn×q  and let ck

(5)
for the choice of q = 0. The idea is: given {zk−j}q+1
def= zj − zj−1 and predict the future
iterates by considering how the past directions vk−1  . . .   vk−q approximate the latest direction vk.
def= argminc∈Rq||Vk−1c − vk||2 =
In particular  deﬁne Vk−1
def= zk + Vkc ≈ zk+1. By
iterating this s times  we obtain ¯zk s ≈ zk+s.
More precisely  given c ∈ Rq  deﬁne the mapping H by H(c) =
Ck = H(ck)  note that Vk = Vk−1Ck. Deﬁne ¯Vk 0
VkC s

(cid:104) c1:q−1
j=1 cjvk−j − vk||2. The idea is then that Vkck ≈ vk+1 and so  ¯zk 1
(cid:0)(cid:80)s

Idq−1
01 q−1
def= Vk and for s ≥ 1  deﬁne ¯Vk s
k is the power of Ck. Let (C)(: 1) be the ﬁrst column of matrix C  then

i=1( ¯Vk i)(: 1) = zk +(cid:80)s

(cid:105) ∈ Rq×q. Let
(cid:1)

def= ¯Vk s−1Ck

¯zk s = zk +(cid:80)s

i=1 C i
k
which is the desired trajectory following extrapolation. Now deﬁne the extrapolation

k)(: 1) = zk + Vk

i=1 Vk(C i

||(cid:80)q

k where C s

def=

(6)

(: 1) 

cq

Es q(zk ···   zk−q−1) def= Vk

(cid:0)(cid:80)s

(cid:1)

i=1 C i
k

(: 1)

parameterized by s  q  we obtain the following trajectory following adaptive acceleration for ADMM.

Algorithm 1: A3DMM - Adaptive Acceleration for ADMM
Initial: Let s ≥ 1  q ≥ 1 be integers. Let ¯z0 = z0 ∈ Rp and V0 = 0 ∈ Rp×(q+1).
Repeat:
• For k ≥ 1:

||By + 1

γ (¯zk−1 − γb)||2 

yk = argminy∈Rm J(y) + γ
2
ψk = ¯zk−1 + γ(Byk − b) 
xk = argminx∈Rn R(x) + γ
2
zk = ψk + γAxk 
vk = zk − zk−1

||Ax − 1

γ (¯zk−1 − 2ψk)||2 
and Vk = [vk  Vk−1(:  1 : q − 1)].

• If mod(k  q+2) = 0: compute ck and Ck  if ρ(Ck) < 1: ¯zk = zk + akEs q(zk ···   zk−q−1).
Until: ||vk|| ≤ tol.

Remark 4.1.

pseudoinverse of Vk−1. And the value of q usually is taken very small  e.g. q ≤ 10.

• The extra computational cost of A3DMM is very small  which is about nq2 for computing the
• The reason we change the order of updates in Algorithm 1 is that the update of yk requires only
¯zk  doing so we only need to extrapolate zk which requires the minimal computational overhead.
Moreover  the extrapolation can also be applied to xk  yk  ψk under proper adaptation.
• A3DMM carries out (q + 2) standard ADMM iterations to set up the extrapolation step Es q. As
Es q contains the sum of the powers of Ck  it is guaranteed to be convergent when ρ(Ck) < 1.
Therefore  we only apply Es q when the spectral radius ρ(Ck) < 1 is true. In this case  there is a
closed form expression for Es q when s = +∞; See Eq. (7).
• The purpose of adding ak in front of Es q(zk ···   zk−q−1) is so that we can control the value

of ak to ensure the convergence of the algorithm; See below the discussion.

4.1 Convergence of A3DMM
To discuss the convergence of A3DMM  we shall treat the algorithm as a perturbation of the original
ADMM. If the perturbation error is absolutely summable  then we obtain the convergence of A3DMM.
More precisely  let εk ∈ Rn whose value takes

0 : mod(k  q + 2) (cid:54)= 0 or mod(k  q + 2) = 0 & ρ(Ck) ≥ 1 

(cid:26)

εk =

akEs q(zk ···   zk−q−1) : mod(k  q + 2) = 0 & ρ(Ck) < 1.

5

Suppose the ﬁxed-point formulation of ADMM can be written as zk = F(zk−1) for some F (see
Section B.2 of the appendix for details). Then Algorithm 1 can be written as zk = F(zk−1 +
εk−1)  and we can obtain the following convergence for Algorithm 1 which is based on the classic
convergence result of inexact Krasnosel’ski˘ı-Mann ﬁxed-point iteration [4  Proposition 5.34].
Proposition 4.2. For problem (PADMM) and Algorithm 1  suppose that the conditions (A.1)-(A.3)
k ||εk|| < +∞  zk → z(cid:63) ∈ ﬁx(F) def= {z ∈ Rp : z = F(z)} and (xk  yk  ψk)
converges to (x(cid:63)  y(cid:63)  ψ(cid:63)) which is a saddle point of L(x  y; ψ).
k ||εk|| < +∞ in general cannot be guaran-
teed. However  it can be enforced by a simple online updating rule. Let a ∈ [0  1] and b  δ > 0  then

are true. If moreover (cid:80)
On-line updating rule The summability condition(cid:80)
ak can be determined by ak = min(cid:8)a  b/(k1+δ||zk − zk−1||)(cid:9).

Inexact A3DMM Observe that in A3DMM  when A  B are non-trivial  in general there are no
closed form solutions for xk and yk. Take xk for example  suppose it is computed approximately 
then in zk there will be another approximation error ε(cid:48)

k  and consequently

zk = F(zk−1 + εk−1 + γε(cid:48)

k−1).

k ||ε(cid:48)

k−1|| < +∞  Proposition 4.2 remains true for the above perturbation form.

If there holds(cid:80)

(7)

(: 1) =

= zk−1 + Vk

1−(cid:80)s

1
i=1 ck i

(: 1)

(cid:1) 

k = Ck

i=0 C i

(cid:80)+∞

(cid:80)+∞
(cid:0)(Id − Ck)−1 − Id(cid:1)
(cid:0)(Id − Ck)−1(cid:1)

j=0) is an approximation to zk+s. In this section  we make precise this statement.

4.2 Acceleration guarantee for A3DMM
We have so far alluded to the idea that the extrapolated point ¯zk s deﬁned in (6) (which depends only
on {zk−j}q
Relationship to MPE and RRE We ﬁrst show that ¯zk ∞ is (almost) equivalent to MPE. Recall that
i=0 C i.
Now for the summation of the power of Ck in (6)  when s = +∞  we have

given a square matrix C  if its Neumann series is convergent  then there holds (Id−C)−1 =(cid:80)+∞

k = Ck(Id − Ck)−1 = (Id − Ck)−1 − Id.

¯zk ∞ def= zk + Vk

(: 1) = zk − vk + Vk

i=1 C i
Back to (6)  then we get

(cid:0)(Id − Ck)−1(cid:1)
(cid:0)zk −(cid:80)q−1
which turns out to be MPE  with the slight difference of taking the weighted sum of {zj}k
as opposed to the weighted sum of {zj}k−1
the coefﬁcients c is computed in the following way: b ∈ argmina∈Rq+1 (cid:80)
¯zk ∞ =(cid:80)q−1
b0 (cid:54)= 0 and deﬁne cj

j=k−q+1
j=k−q (See appendix for more details of MPE). Note that if
j=0 ajvk−j|| and
= b0  and
j=0 bjzk−j is precisely the RRE update (again with the slight difference of summing over
iterates shifted by one iteration).
Acceleration guarantee for A3DMM Let {zk}k∈N be a sequence in Rn and let vk
def= zk − zk−1.
Assume that vk = M vk−1 for some M ∈ Rn×n. Denote λ(M ) the spectrum of M. The following
proposition provides control on the extrapolation error for ¯zk s from (6).
Proposition 4.3. Deﬁne the coefﬁcient ﬁtting error by k
(i) For s ∈ N  we have

def= −bj/b0 for j = 1  . . .   q. Then (1 −(cid:80)q

j aj =1||(cid:80)q
b0+(cid:80)q

def= minc∈Rq ||Vk−1c − vk||.

i=1 ci)−1 =

j=1 ck jzk−j

(cid:96)=1 ||M (cid:96)|||(cid:80)s−(cid:96)
def=(cid:80)s

||¯zk s − z(cid:63)|| ≤ ||zk+s − z(cid:63)|| + Bsk 
i=0(C i

k)(1 1)|. If ρ(M ) < 1 and ρ(Ck) < 1  then(cid:80)

Bs is uniformly bounded in s. For s = +∞  B∞ def= |1 −(cid:80)

i ck i|−1(cid:80)∞

(cid:96)=1 ||M||(cid:96)

where Bs

(8)
i ck i (cid:54)= 1 and

(ii) Suppose that M is diagonalizable. Let (λj)j denote its distinct eigenvalues ordered such that

|λj| ≥ |λj+1| and |λ1| = ρ(M ) < 1. Suppose that |λq| > |λq+1|.

• Asymptotic bound (ﬁxed q and as k → +∞): k = O(|λq+1|k).
• Non-asymptotic bound (ﬁxed q and k): Suppose λ(M ) is real-valued and contained in

b0
j=1 bj

[α  β] with −1 < α < β < 1. Then  let K def= 2||z0 − z(cid:63)||||(Id − M ) 1

2|| and η = 1−α
1−β

1 −(cid:80)

k

i ck i

≤ Kβk−q(cid:0)√

η−1√

η+1

(cid:1)q

.

(9)

Remark 4.4.

6

• From Theorem 2.2(ii)  when R and J are both polyhedral  we have a perfect local linearisation
with the corresponding linearisation matrix being normal and hence  the conditions of Proposition
4.3 holds for all k large enough. The ﬁrst bound (i) shows that the extrapolated point ¯zk s moves
along the true trajectory as s increases  up to the ﬁtting error k. Although ¯zk ∞ is essentially an
MPE update which is known to satisfy error bound (9) (see [28])  this proposition offers a further
interpretation of these extrapolation methods in terms of following the “sequence trajectory” 
and combined with our local analysis of ADMM  provides justiﬁcation of these methods for the
acceleration of non-smooth optimisation problems.
• Proposition 4.3 (ii) shows that extrapolation improves the convergence rate from O(|λ1|k) to
O(|λq+1|k)  and the nonasymptotic bound shows that the improvement of extrapolation is
optimal in the sense of Nesterov [22]. Recalling the form of the eigenvalues of M from Theorem
2.2  in the case of two nonsmooth polyhedral terms  we must have |λ2j−1| = |λ2j| > |λ2j+1|
for all j ≥ 1. Hence  no acceleartion can be guaranteed or observed when q = 1  while the
choice of q = 2 provides guaranteed acceleration.

Extension of A3DMM to variants of ADMM is provided in Section B of the supplementary material.
5 Numerical experiments
Below we present numerical experiments on afﬁne constrained minimisation (e.g. Basis Pursuit)
and LASSO problems to demonstrate the performance of A3DMM. Extra comparisons can be found
in the supplementary material Section C. In the numerical comparison below  we mainly compare
with the original ADMM and its inertial version (3) with ﬁxed ak ≡ 0.3. For the proposed A3DMM 
two settings are considered: (q  s) = (4  100) and (q  s) = (4  +∞). MATLAB source codes for
reproducing the results can be found at: https://github.com/jliang993/A3DMM.

(a) (cid:96)1-norm

(b) (cid:96)1 2-norm

(c) Nuclear norm

(d) (cid:96)1-norm

(e) (cid:96)1 2-norm

(f) Nuclear norm

Figure 2: Performance comparisons and {θk}k∈N of ADMM for afﬁne constrained problem.

Afﬁne constrained minimisation Consider the following constrained problem  given ◦
x

min
x∈Rn

R(x)

such that Kx = K

◦
x.

Denote the set Ω def= {x ∈ Rn : Kx = K

(10)
x} and ιΩ its indicator function. Then (10) can be written as
(11)
which is special case of (PADMM) with A = Id  B = −Id and b = 0. Here K is generated from the
standard Gaussian ensemble  and the following three choices of R are considered:

such that x − y = 0 

R(x) + ιΩ(y)

min
x y∈Rn

(cid:96)1-norm (m  n) = (640  2048)  ◦
(cid:96)1 2-norm (m  n) = (640  2048)  ◦

x is 128-sparse;
x has 32 non-zero blocks of size 4;

Nuclear norm (m  n) = (1448  64 × 64)  ◦
The property of {θk}k∈N is shown in Figure 2 (a)-(c). Note that the indicator function ιΩ(y) in (11)
is polyhedral since Ω is an afﬁne subspace 

x has rank of 4.

◦

7

1002003004005006000.80.850.90.951501001502002500.80.850.90.951501001502002503003504004500.80.850.90.9515010015020025030035010-810-41005010015020010-810-410010020030040050010-810-4100• As (cid:96)1-norm is polyhedral  we have in Figure 2(a) that θk is converging to a constant which
• Since (cid:96)1 2-norm and nuclear norm are no longer polyhedral functions  we have that θk eventually

complies with Theorem 2.2(ii).
oscillates in a range  meaning that the trajectory of {zk}k∈N is an elliptical spiral.

Comparisons of the four schemes are shown below in Figure 2 (d)-(f):

• Since both functions in (11) are non-smooth  the eventual trajectory of {zk}k∈N for ADMM is
• A3DMM is faster than both ADMM and inertial ADMM. For the two different settings of

spiral. Inertial ADMM fails to provide acceleration locally.

A3DMM  their performances are very close.

LASSO We consider again the LASSO problem (4) with three datasets from LIBSVM2. The
numerical experiments are provided below in Figure 3.
It can be observed that the proposed A3DMM is signiﬁcantly faster than the other schemes  especially
for s = +∞. Between ADMM and inertial ADMM  the inertial technique can provided consistent
acceleration for all three examples since θk → 0; See ﬁrst row of Figure 3. For Figure 3 (a)  the
oscillation after k = 2000 is due to machine error.

(a) covtype: 1 − cos(θk)

(b) ijcnn1: 1 − cos(θk)

(c) phishing: 1 − cos(θk)

(d) covtype: ||xk − x(cid:63)||

(e) ijcnn1: ||xk − x(cid:63)||

(f) phishing: ||xk − x(cid:63)||

Figure 3: Performance comparisons for LASSO problem.

6 Conclusions
In this article  by analyzing the trajectory of the ﬁxed point sequences associated to ADMM and
extrapolating along the trajectory  we provide an alternative derivation of these methods. Furthermore 
our local linear analysis allows for the application of previous results on extrapolation methods  and
hence provides guaranteed (local) acceleration.
Acknowledgments
We would like to thank Arieh Iserles for pointing out the connection between trajectory following
adaptive acceleration and vector extrapolation. We also like to thank the reviewers whose comments
helped to improve the paper. JL was partly supported by Leverhulme trust and Newton trust.
References
[1] A. C. Aitken. Xxv.–on Bernoulli’s numerical solution of algebraic equations. Proceedings of the Royal

Society of Edinburgh  46:289–305  1927.

[2] F. Alvarez and H. Attouch. An inertial proximal method for maximal monotone operators via discretization

of a nonlinear oscillator with damping. Set-Valued Analysis  9(1-2):3–11  2001.

[3] D. G. Anderson. Iterative procedures for nonlinear integral equations. J. ACM  12(4):547–560  October

1965.

[4] H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer  2011.

2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

8

1000200030004000500060007000800090001000010-1010-810-610-410-21002004006008001000120010-1410-1210-1010-810-610-410-2100500100015002000250010-1410-1210-1010-810-610-410-210020004000600080001000010-810-41002004006008001000120010-810-4100500100015002000250010-810-4100[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] S. Boyd  N. Parikh  E. Chu  B. Peleato  J. Eckstein  et al. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine learning  3(1):1–122 
2011.

[7] C. Brezinski. Convergence acceleration during the 20th century. Numerical Analysis: Historical Develop-

ments in the 20th Century  page 113  2001.

[8] C. Brezinski and M. R. Zaglia. Extrapolation methods: theory and practice  volume 2. Elsevier  2013.
[9] S. Cabay and L. W. Jackson. A polynomial extrapolation method for ﬁnding limits and antilimits of vector

sequences. SIAM Journal on Numerical Analysis  13(5):734–752  1976.

[10] A. Chambolle and C. Dossal. On the convergence of the iterates of the “fast iterative shrinkage/thresholding

algorithm”. Journal of Optimization Theory and Applications  166(3):968–982  2015.

[11] J. Eckstein and D. P. Bertsekas. On the douglas–rachford splitting method and the proximal point algorithm

for maximal monotone operators. Mathematical Programming  55(1-3):293–318  1992.

[12] R. P. Eddy. Extrapolating to the limit of a vector sequence. In Information linkage between applied

mathematics and industry  pages 387–396. Elsevier  1979.

[13] G. Franca  D. P. Robinson  and R. Vidal. A dynamical systems perspective on nonsmooth constrained

optimization. arXiv preprint arXiv:1808.04048  2018.

[14] Guilherme Franca  Daniel Robinson  and Rene Vidal. ADMM and accelerated ADMM as continuous
dynamical systems. In Jennifer Dy and Andreas Krause  editors  Proceedings of the 35th International
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning Research  pages
1559–1567  Stockholmsmassan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[15] D. Gabay. Chapter ix applications of the method of multipliers to variational inequalities. Studies in

mathematics and its applications  15:299–331  1983.

[16] D. Gabay and B. Mercier. A dual algorithm for the solution of non linear variational problems via ﬁnite

element approximation. Institut de recherche d’informatique et d’automatique  1975.

[17] R. Glowinski and A. Marroco. Sur l’approximation  par éléments ﬁnis d’ordre un  et la résolution  par
pénalisation-dualité d’une classe de problèmes de dirichlet non linéaires. ESAIM: Mathematical Modelling
and Numerical Analysis-Modélisation Mathématique et Analyse Numérique  9(R2):41–76  1975.

[18] M. Kadkhodaie  K. Christakopoulou  M. Sanjabi  and A. Banerjee. Accelerated alternating direction method
of multipliers. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery
and data mining  pages 497–506. ACM  2015.

[19] A. S. Lewis. Active sets  nonsmoothness  and sensitivity. SIAM Journal on Optimization  13(3):702–725 

2003.

[20] J. Liang. Convergence rates of ﬁrst-order operator splitting methods. PhD thesis  Normandie Université;

GREYC CNRS UMR 6072  2016.

[21] M. Mešina. Convergence acceleration for the iterative solution of the equations x= ax+ f. Computer

Methods in Applied Mechanics and Engineering  10(2):165–173  1977.

[22] Y. Nesterov. A method for solving the convex programming problem with convergence rate O(1/k2).

Dokl. Akad. Nauk SSSR  269(3):543–547  1983.

[23] I. Pejcic and C. N. Jones. Accelerated admm based on accelerated douglas-rachford splitting. In 2016

European Control Conference (ECC)  pages 1952–1957. Ieee  2016.

[24] L. F. Richardson and J. A. Gaunt. Viii. the deferred approach to the limit. Philosophical Transactions
of the Royal Society of London. Series A  containing papers of a mathematical or physical character 
226(636-646):299–361  1927.

[25] D. Scieur  A. d’Aspremont  and F. Bach. Regularized nonlinear acceleration. In Advances In Neural

Information Processing Systems  pages 712–720  2016.

[26] D. Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal of Mathe-

matics and Physics  34(1-4):1–42  1955.

[27] A. Sidi. Practical extrapolation methods: Theory and applications  volume 10. Cambridge University

Press  2003.

[28] A. Sidi. Vector extrapolation methods with applications  volume 17. SIAM  2017.
[29] P. Wynn. Acceleration techniques for iterated vector and matrix problems. Mathematics of Computation 

16(79):301–322  1962.

9

,Clarice Poon
Jingwei Liang