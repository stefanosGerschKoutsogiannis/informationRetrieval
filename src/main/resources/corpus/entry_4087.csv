2018,Online convex optimization for cumulative constraints,We propose the algorithms for online convex
  optimization which lead to cumulative squared constraint violations
  of the form
  $\sum\limits_{t=1}^T\big([g(x_t)]_+\big)^2=O(T^{1-\beta})$  where
  $\beta\in(0 1)$.  Previous literature has
  focused on long-term constraints of the form
  $\sum\limits_{t=1}^Tg(x_t)$. There  strictly feasible solutions
  can cancel out the effects of violated constraints.
  In contrast  the new form heavily penalizes large constraint
  violations and cancellation effects cannot occur. 
  Furthermore  useful bounds on the single step constraint violation
  $[g(x_t)]_+$ are derived.
  For convex objectives  our regret bounds generalize
  existing bounds  and for strongly convex objectives we give improved
  regret bounds.
  In numerical experiments  we show that our algorithm closely follows
  the constraint boundary leading to low cumulative violation.,Online convex optimization for cumulative constraints

Department of Electrical and Computer Engineering

Jianjun Yuan

University of Minnesota
Minneapolis  MN  55455
yuanx270@umn.edu

Department of Electrical and Computer Engineering

Andrew Lamperski

University of Minnesota
Minneapolis  MN  55455
alampers@umn.edu

Abstract

T(cid:80)

(cid:0)[g(xt)]+

(cid:1)2

t=1

T(cid:80)

We propose the algorithms for online convex optimization which lead to cumulative
= O(T 1−β)  where
squared constraint violations of the form
β ∈ (0  1) . Previous literature has focused on long-term constraints of the form
g(xt). There  strictly feasible solutions can cancel out the effects of violated
t=1
constraints. In contrast  the new form heavily penalizes large constraint violations
and cancellation effects cannot occur. Furthermore  useful bounds on the single
step constraint violation [g(xt)]+ are derived. For convex objectives  our regret
bounds generalize existing bounds  and for strongly convex objectives we give
improved regret bounds. In numerical experiments  we show that our algorithm
closely follows the constraint boundary leading to low cumulative violation.

1

Introduction

Online optimization is a popular framework for machine learning  with applications such as dictionary
learning [14]  auctions [1]  classiﬁcation  and regression [3]. It has also been inﬂuential in the
development of algorithms in deep learning such as convolutional neural networks [11]  deep Q-
networks [15]  and reinforcement learning [8  20].
The general formulation for online convex optimization (OCO) is as follows: At each time t  we
choose a vector xt in convex set S = {x : g(x) ≤ 0}. Then we receive a loss function ft : S → R
drawn from a family of convex functions and we obtain the loss ft(xt). In this general setting  there
is no constraint on how the sequence of loss functions ft is generated. See [21] for more details.
The goal is to generate a sequence of xt ∈ S for t = 1  2  ..  T to minimize the cumulative regret
which is deﬁned by:

RegretT (x∗) =

(1)

T(cid:88)

ft(xt) − T(cid:88)

t=1

t=1

ft(x∗)

T(cid:80)

where x∗ is the optimal solution to the following problem: min
x∈S
solution to Problem (1) is called Hannan consistent if RegretT (x∗) is sublinear in T .

t=1

ft(x). According to [2]  the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

For online convex optimization with constraints  a projection operator is typically applied to the
updated variables in order to make them feasible at each time step [21  6  7]. However  when the
constraints are complex  the computational burden of the projection may be too high for online
computation. To circumvent this dilemma  [13] proposed an algorithm which approximates the
true desired projection with a simpler closed-form projection. The algorithm gives a cumulative
T )  but the constraint g(xt) ≤ 0 may not
regret RegretT (x∗) which is upper bounded by O(
g(xt) ≤
be satisﬁed in every time step. Instead  the long-term constraint violation satisﬁes
O(T 3/4)  which is useful when we only require the constraint violation to be non-positive on average:

T(cid:80)

√

t=1

T(cid:80)

t=1

limT→∞

g(xt)/T ≤ 0.

T(cid:80)

T(cid:80)

t=1

√
T ) regret and a bound of O(

More recently  [10] proposed an adaptive stepsize version of this algorithm which can make
RegretT (x∗) ≤ O(T max{β 1−β}) and
g(xt) ≤ O(T 1−β/2). Here β ∈ (0  1) is a user-
√
determined trade-off parameter. In related work  [19] provides another algorithm which achieves
O(
In this paper  we propose two algorithms for the following two different cases:
Convex Case: The ﬁrst algorithm is for the convex case  which also has the user-determined trade-
off as in [10]  while the constraint violation is more strict. Speciﬁcally  we have RegretT (x∗) ≤
O(T max{β 1−β}) and
Note the square term heavily penalizes large constraint violations and constraint violations from one
step cannot be canceled out by strictly feasible steps. Additionally  we give a bound on the cumulative

(cid:1)2 ≤ O(T 1−β) where [g(xt)]+ = max{0  g(xt)} and β ∈ (0  1).

T ) on the long-term constraint violation.

(cid:0)[g(xt)]+

t=1

constraint violation

[g(xt)]+ ≤ O(T 1−β/2)  which generalizes the bounds from [13  10].

T(cid:80)
T(cid:80)

t=1

T(cid:80)

√
g(xt) ≤ O(

In the case of β = 0.5  which we call "balanced"  both RegretT (x∗) and
√
T ). More importantly  our algorithm guarantees that at each time step 
same upper bound of O(
T 1/6 )  which does not follow from
the clipped constraint term [g(xt)]+ is upper bounded by O( 1
the results of [13  10]. However  our results currently cannot generalize those of [19]  which has

([g(xt)]+)2 have the

t=1

T ). As discussed below  it is unclear how to extend the work of [19] to the clipped

T(cid:80)

[g(xt)]+ ≤ O((cid:112)log(T )T ). The improved bounds match

t=1
constraints  [g(xt)]+.
Strongly Convex Case: Our second algorithm for strongly convex function ft(x) gives us
the improved upper bounds compared with the previous work in [10]. Speciﬁcally  we have
RegretT (x∗) ≤ O(log(T ))  and
the regret order of standard OCO from [9]  while maintaining a constraint violation of reasonable
order.
We show numerical experiments on three problems. A toy example is used to compare trajectories of
our algorithm with those of [10  13]  and we see that our algorithm tightly follows the constraints.
The algorithms are also compared on a doubly-stochastic matrix approximation problem [10] and an
economic dispatch problem from power systems. In these  our algorithms lead to reasonable objective
regret and low cumulative constraint violation.

t=1

2 Problem Formulation

The basic projected gradient algorithm for Problem (1) was deﬁned in [21]. At each step  t  the
algorithm takes a gradient step with respect to ft and then projects onto the feasible set. With some
assumptions on S and ft  this algorithm achieves a regret of O(
Although the algorithm is simple  it needs to solve a constrained optimization problem at every
time step  which might be too time-consuming for online implementation when the constraints are

T ).

√

2

(2)

(4)

(5)

complex. Speciﬁcally  in [21]  at each iteration t  the update rule is:

xt+1 = ΠS(xt − η∇ft(xt)) = arg min
y∈S

(cid:107)y − (xt − η∇ft(xt))(cid:107)2
(cid:107) is the (cid:96)2 norm.

min

T(cid:80)

where ΠS is the projection operation to the set S and (cid:107)
In order to lower the computational complexity and accelerate the online processing speed  the work
of [13] avoids the convex optimization by projecting the variable to a ﬁxed ball S ⊆ B  which always
has a closed-form solution. That paper gives an online solution for the following problem:
gi(xt) ≤ 0  i = 1  2  ...  m

(3)
where S = {x : gi(x) ≤ 0  i = 1  2  ...  m} ⊆ B. It is assumed that there exist constants R > 0 and
r < 1 such that rK ⊆ S ⊆ RK with K being the unit (cid:96)2 ball centered at the origin and B = RK.
Compared to Problem (1)  which requires that xt ∈ S for all t  (3) implies that only the sum of
constraints is required. This sum of constraints is known as the long-term constraint.
To solve this new problem  [13] considers the following augmented Lagrangian function at each
iteration t:

ft(xt) − min
x∈S

x1 ... xT ∈B

T(cid:80)

T(cid:80)

ft(x)

s.t.

t=1

t=1

t=1

Lt(x  λ) = ft(x) +

λigi(x) − ση
2

λ2
i

m(cid:88)

(cid:110)

i=1

(cid:111)

The update rule is as follows:

xt+1 = ΠB(xt − η∇xLt(xt  λt))  λt+1 = Π[0 +∞)m(λt + η∇λLt(xt  λt))

where η and σ are the pre-determined stepsize and some constant  respectively.
More recently  an adaptive version was developed in [10]  which has a user-deﬁned trade-off param-
eter. The algorithm proposed by [10] utilizes two different stepsize sequences to update x and λ 
respectively  instead of using a single stepsize η.
In both algorithms of [13] and [10]  the bound for the violation of the long-term constraint is that ∀i 
gi(xt) ≤ O(T γ) for some γ ∈ (0  1). However  as argued in the last section  this bound does not
enforce that the violation of the constraint xt ∈ S gets small. A situation can arise in which strictly
satisﬁed constraints at one time step can cancel out violations of the constraints at other time steps.
This problem can be rectiﬁed by considering clipped constraint  [gi(xt)]+  in place of gi(xt).

T(cid:80)

t=1

T(cid:80)

(cid:0)[gi(xt)]+

(cid:1)2  which  as discussed in the

For convex problems  our goal is to bound the term
previous section  is more useful for enforcing small constraint violations  and also recovers the

t=1

existing bounds for both
the improvement on the upper bounds compared to the results in [10].
In sum  in this paper  we want to solve the following problem for the general convex condition:

gi(xt). For strongly convex problems  we also show

[gi(xt)]+ and

t=1

t=1

ft(xt) − min
x∈S

t=1

min

ft(x)

x1 x2 ... xT ∈B

(6)
where γ ∈ (0  1). The new constraint from (6) is called the square-clipped long-term constraint
(since it is a square-clipped version of the long-term constraint) or square-cumulative constraint
(since it encodes the square-cumulative violation of the constraints).
To solve Problem (6)  we change the augmented Lagrangian function Lt as follows:

s.t.

t=1

t=1

T(cid:80)

(cid:0)[gi(xt)]+

(cid:1)2 ≤ O(T γ) ∀i

T(cid:80)

T(cid:80)

T(cid:80)

T(cid:80)

Lt(x  λ) = ft(x) +

λi[gi(x)]+ − θt
2

λ2
i

(7)

m(cid:88)

(cid:110)

i=1

(cid:111)

In this paper  we will use the following assumptions as in [13]: 1. The convex set S is non-empty 
closed  bounded  and can be described by m convex functions as S = {x : gi(x) ≤ 0  i = 1  2  ...  m}.

3

Algorithm 1 Generalized Online Convex Optimization with Long-term Constraint
1: Input: constraints gi(x) ≤ 0  i = 1  2  ...  m  stepsize η  time horizon T  and constant σ > 0.
2: Initialization: x1 is in the center of the B .
3: for t = 1 to T do
4:
5:
6:

Input the prediction result xt.
Obtain the convex loss function ft(x) and the loss value ft(xt).
Calculate a subgradient ∂xLt(xt  λt)  where:
∂xLt(xt  λt) = ∂xft(xt) +

λi
t∂x([gi(xt)]+)  ∂x([gi(xt)]+) =

gi(xt) ≤ 0

(cid:26)0 

m(cid:80)

∇xgi(xt)  otherwise

7:

Update xt and λt as below:

i=1

xt+1 = ΠB(xt − η∂xLt(xt  λt))  λt+1 = [g(xt+1)]+

ση

8: end for

2. Both the loss functions ft(x)  ∀t and constraint functions gi(x)  ∀i are Lipschitz continuous in the
set B. That is  (cid:107)ft(x) − ft(y)(cid:107) ≤ Lf (cid:107)x − y(cid:107)  (cid:107)gi(x) − gi(y)(cid:107) ≤ Lg (cid:107)x − y(cid:107)  ∀x  y ∈ B and ∀t  i.
G = max{Lf   Lg}  and

x y∈B ft(x) − ft(y) ≤ 2Lf R  D = max

max

i=1 2 ... m

x∈B gi(x) ≤ LgR

max

F = max

t=1 2 ... T

3 Algorithm

3.1 Convex Case:

The main algorithm for this paper is shown in Algorithm 1. For simplicity  we abuse the subgradient
notation  denoting a single element of the subgradient by ∂xLt(xt  λt). Comparing our algorithm
with Eq.(5)  we can see that the gradient projection step for xt+1 is similar  while the update rule for
λt+1 is different. Instead of a projected gradient step  we explicitly maximize Lt+1(xt+1  λ) over λ.
This explicit projection-free update for λt+1 is possible because the constraint clipping guarantees
that the maximizer is non-negative. Furthermore  this constraint-violation-dependent update helps to
enforce small cumulative and individual constraint violations. Speciﬁc bounds on constraint violation
are given in Theorem 1 and Lemma 1 below.
Based on the update rule in Algorithm 1  the following theorem gives the upper bounds for both the

(cid:16)

T(cid:80)

(cid:17)2

[gi(xt)]+

in Problem 6.

regret on the loss and the squared-cumulative constraint violation 
For space purposes  all proofs are contained in the supplementary material.
Theorem 1. Set σ = (m+1)G2
α ∈ (0  1) and x∗ being the optimal solution for min
x∈S

2(1−α)   η =

T(cid:80)

(m+1)RT

√

t=1

G

1

(cid:17) ≤ O(

√

(cid:16)

T(cid:80)

t=1

ft(x)  we have
√

(cid:17)2 ≤ O(

ft(xt) − ft(x∗)

T ) 

[gi(xt)]+

t=1

t=1

(cid:16)

T(cid:80)

. If we follow the update rule in Algorithm 1 with

T ) ∀i ∈ {1  2  ...  m}

√
From Theorem 1  we can see that by setting appropriate stepsize  η  and constant  σ  we can obtain
the upper bound for the regret of the loss function being less than or equal to O(
T )  which is also
shown in [13] [10]. The main difference of the Theorem 1 is that previous results of [13] [10] all

obtain the upper bound for the long-term constraint

gi(xt)  while here the upper bound for the

T(cid:80)

t=1

constraint violation of the form
is achieved. Also note that the stepsize depends on
T   which may not be available. In this case  we can use the ’doubling trick’ described in the book [2]
to transfer our T -dependent algorithm into T -free one with a worsening factor of

√
2/(

2 − 1).

[gi(xt)]+

√

t=1

(cid:16)

T(cid:80)

(cid:17)2

4

T(cid:80)

The proposed algorithm and the resulting bound are useful for two reasons: 1. The square-cumulative

constraint implies a bound on the cumulative constraint violation 
[gi(xt)]+  while enforcing
larger penalties for large violations. 2. The proposed algorithm can also upper bound the constraint
violation for each single step [gi(xt)]+  which is not bounded in the previous literature.
The next results show how to bound constraint violations at each step.
Lemma 1. If there is only one differentiable constraint function g(x) with Lipschitz continuous
gradient parameter L  and we run the Algorithm 1 with the parameters in Theorem 1 and large
enough T   we have

t=1

[g(xt)]+ ≤ O( 1

T 1/6 )  ∀t ∈ {1  2  ...  T} 

if

[g(x1)]+ ≤ O( 1

T 1/6 ).

Lemma 1 only considers single constraint case. For case of multiple differentiable constraints  we
have the following:
Proposition 1. For multiple differentiable constraint functions gi(x)  i ∈ {1  2  ...  m} with Lipschitz
continuous gradient parameters Li  if we use ¯g(x) = log
as the constraint function
in Algorithm 1  then for large enough T   we have
T 1/6 )  ∀i  t 

[gi(xt)]+ ≤ O( 1

[¯g(x1)]+ ≤ O( 1

(cid:16) m(cid:80)

exp gi(x)

T 1/6 ).

(cid:17)

i=1

if

Clearly  both Lemma 1 and Proposition 1 only deal with differentiable functions. For a non-
differentiable function g(x)  we can ﬁrst use a differentiable function ¯g(x) to approximate the
g(x) with ¯g(x) ≥ g(x)  and then apply the previous Lemma 1 and Proposition 1 to upper bound each
individual gi(xt). Many non-smooth convex functions can be approximated in this way as shown in
[16].

3.2 Strongly Convex Case:

T(cid:80)

For ft(x) to be strongly convex  the Algorithm 1 is still valid. But in order to have lower upper
bounds for both objective regret and the clipped long-term constraint
[gi(xt)]+ compared with
Proposition 3 in next section  we need to use time-varying stepsize as the one used in [9]. Thus  we
modify the update rule of xt  λt to have time-varying stepsize as below:

t=1

xt+1 = ΠB(xt − ηt∂xLt(xt  λt))  λt+1 = [g(xt+1)]+

θt+1

.

(8)

If we replace the update rule in Algorithm 1 with Eq.(8)  we can obtain the following theorem:
Theorem 2. Assume ft(x) has strongly convexity parameter H1. If we set ηt = H1
1)G2  follow the new update rule in Eq.(8)  and x∗ being the optimal solution for min
x∈S
∀i ∈ {1  2  ...  m}  we have
ft(xt) − ft(x∗)

T(cid:80)
[gi(xt)]+ ≤ O((cid:112)log(T )T ).

(cid:17) ≤ O(log(T )) 

gi(xt) ≤ T(cid:80)

t+1   θt = ηt(m +
ft(x)  for

T(cid:80)

T(cid:80)

(cid:16)

t=1

t=1

t=1

t=1

The paper [10] also has a discussion of strongly convex functions  but only provides a bound similar
to the convex one. Theorem 2 shows the improved bounds for both objective regret and the constraint
violation. On one hand the objective regret is consistent with the standard OCO result in [9]  and on
the other the constraint violation is further reduced compared with the result in [10].

4 Relation with Previous Results

In this section  we extend Theorem 1 to enable direct comparison with the results from [13] [10]. In
particular  it is shown how Algorithm 1 recovers the existing regret bounds  while the use of the new
augmented Lagrangian (7) in the previous algorithms also provides regret bounds for the clipped
constraint case.

5

The ﬁrst result puts a bound on the clipped long-term constraint  rather than the sum-of-squares that
appears in Theorem 1. This will allow more direct comparisons with the existing results.

Proposition 2. If σ = (m+1)G2
result of Algorithm 1 satisﬁes

2(1−α)   η = O( 1√

T

)  α ∈ (0  1)  and x∗ = argmin
x∈S

ft(x)  then the

T(cid:80)

t=1

(cid:16)

T(cid:80)

t=1

ft(xt) − ft(x∗)

(cid:17) ≤ O(

√

T(cid:80)

t=1

T ) 

gi(xt) ≤ T(cid:80)

t=1

[gi(xt)]+ ≤ O(T 3/4) ∀i ∈ {1  2  ...  m}

This result shows that our algorithm generalizes the regret and long-term constraint bounds of [13].
The next result shows that by changing our constant stepsize accordingly  with the Algorithm 1  we
can achieve the user-deﬁned trade-off from [10]. Furthermore  we also include the squared version
and clipped constraint violations.

T β )  α ∈ (0  1)  β ∈ (0  1)  and x∗ = argmin
x∈S

ft(x) 

T(cid:80)

t=1

T(cid:80)

([gi(xt)]+)2 ≤ O(T 1−β) ∀i ∈ {1  2  ...  m}

Proposition 3. If σ = (m+1)G2
then the result of Algorithm 1 satisﬁes

2(1−α)   η = O( 1

(cid:17) ≤ O(T max{β 1−β}) 

T(cid:80)
T(cid:80)

t=1

(cid:16)
gi(xt) ≤ T(cid:80)

ft(xt) − ft(x∗)

[gi(xt)]+ ≤ O(T 1−β/2) 

t=1

t=1

t=1

Proposition 3 provides a systematic way to balance the regret of the objective and the constraint
violation. Next  we will show that previous algorithms can use our proposed augmented Lagrangian
function to have their own clipped long-term constraint bound.
Proposition 4. If we run Algorithm 1 in [13] with the augmented Lagrangian formula deﬁned in
Eq.(7)  the result satisﬁes

(cid:16)

T(cid:80)

(cid:17) ≤ O(

√

T ) 

T(cid:80)

gi(xt) ≤ T(cid:80)

ft(xt) − ft(x∗)

[gi(xt)]+ ≤ O(T 3/4) ∀i ∈ {1  2  ...  m}.

t=1

t=1

t=1

For the update rule proposed in [10]  we need to change the Lt(x  λ) to the following one:

Lt(x  λ) = ft(x) + λ[g(x)]+ − θt
2

λ2

(9)

where g(x) = maxi∈{1 ... m} gi(x).
Proposition 5. If we use the update rule and the parameter choices in [10] with the augmented
Lagrangian in Eq.(9)  then ∀i ∈ {1  ...  m}  we have

(cid:16)

T(cid:80)

(cid:17) ≤ O(T max{β 1−β}) 

T(cid:80)

gi(xt) ≤ T(cid:80)

ft(xt) − ft(x∗)

[gi(xt)]+ ≤ O(T 1−β/2).

t=1

t=1

t=1

Propositions 4 and 5 show that clipped long-term constraints can be bounded by combining the
algorithms of [13  10] with our augmented Lagrangian. Although these results are similar in part to
our Propositions 2 and 3  they do not imply the results in Theorems 1 and 2 as well as the new single
step constraint violation bound in Lemma 1  which are our key contributions. Based on Propositions
4 and 5  it is natural to ask whether we could apply our new augmented Lagrangian formula (7) to the
recent work in [19] . Unfortunately  we have not found a way to do so.

(cid:16)

(cid:17)2

(cid:16)

(cid:17)2

[gi(xt)]+

is also convex  we could deﬁne ˜gi(xt) =

Furthermore  since
and apply
the previous algorithms [13] [10] and [19]. This will result in the upper bounds of O(T 3/4) [13] and
O(T 1−β/2) [10]  which are worse than our upper bounds of O(T 1/2) (Theorem 1) and O(T 1−β) (
Proposition 3). Note that the algorithm in [19] cannot be applied since the clipped constraints do not
satisfy the required Slater condition.

[gi(xt)]+

6

Figure 1: Toy Example Results: Trajectories generated by different algorithms. Note how trajectories
generated by Clipped-OGD follow the desired constraints tightly. In contrast  OGD oscillates around
the true constraints  and A-OGD closely follows the boundary of the outer ball.

(a)

(b)

(c)

Figure 2: Doubly-Stochastic Matrices. Fig.2(a): Clipped Long-term Constraint Violation. Fig.2(b):
Long-term Constraint Violation. Fig.2(c): Cumulative Regret of the Loss function

(a)

(b)

(c)

Figure 3: Economic Dispatch. Fig.3(a): Power Demand Trajectory. Fig.3(b): Constraint Violation
for each time step. All of the previous algorithms incurred substantial constraint violations. The
ﬁgure on the right shows the violations of our algorithm  which are signiﬁcantly smaller. Fig.3(c):
Running Average of the Objective Loss

7

−1.00−0.75−0.50−0.250.000.250.500.751.00x1−1.00−0.75−0.50−0.250.000.250.500.751.00x2Clipped−OGD(β=0.5)A−OGD(β=0.5)OGD−1.00−0.75−0.50−0.250.000.250.500.751.00x1−1.00−0.75−0.50−0.250.000.250.500.751.00x2Clipped−OGD(β=2/3)A−OGD(β=2/3)OGD02500500075001000012500150001750020000Different Iterations T050100150200250Clipped Constraint RegretClipped−OGD(β=0.5)Clipped−OGD(β=2/3)A−OGD(β=0.5)A−OGD(β=2/3)OGDOur-Strong02500500075001000012500150001750020000Different Iterations T050100150200250Constraint RegretClipped−OGD(β=0.5)Clipped−OGD(β=2/3)A−OGD(β=0.5)A−OGD(β=2/3)OGDOur-Strong02500500075001000012500150001750020000Different Iterations T50100150200250Objective RegretClipped−OGD(β=0.5)Clipped−OGD(β=2/3)A−OGD(β=0.5)A−OGD(β=2/3)OGDOur-Strong050010001500200025003000Time Slots(each 5 min)10203040506070Demand050010001500200025003000Time Slots(each 5 min)050100150200Constrain Violation %Clipped−OGD(β=0.5)Clipped−OGD(β=2/3)A−OGD(β=0.5)A−OGD(β=2/3)OGD050010001500200025003000Time Slots(each 5 min)0246810Constrain Violation %Clipped−OGD(β=0.5)Clipped−OGD(β=2/3)050010001500200025003000Time Slots(each 5 min)255075100125150175200Objective CostRunning Average Objective CostClipped−OGD(β=0.5)Clipped−OGD(β=2/3)A−OGD(β=0.5)A−OGD(β=2/3)OGDBest fixed strategy in hindsight5 Experiments

In this section  we test the performance of the algorithms including OGD [13]  A-OGD [10]  Clipped-
OGD (this paper)  and our proposed algorithm strongly convex case (Our-strong). Throughout
the experiments  our algorithm has the following ﬁxed parameters: α = 0.5  σ = (m+1)G2
2(1−α)   η =
. In order to better show the result of the constraint violation trajectories  we aggregate

√

T β G
all the constraints as a single one by using g(xt) = maxi∈{1 ... m} gi(xt) as done in [13].

1
R(m+1)

5.1 A Toy Experiment

For illustration purposes  we solve the following 2-D toy experiment with x = [x1  x2]T :

T(cid:80)

t=1

min

cT
t x 

s.t.|x1| + |x2| − 1 ≤ 0.

(10)

where the constraint is the (cid:96)1-norm constraint. The vector ct is generated from a uniform random
vector over [0  1.2] × [0  1] which is rescaled to have norm 1. This leads to slightly average cost on
the on the ﬁrst coordinate. The ofﬂine solutions for different T are obtained by CVXPY [5].
All algorithms are run up to T = 20000 and are averaged over 10 random sequences of {ct}T
t=1.
Since the main goal here is to compare the variables’ trajectories generated by different algorithms 
the results for different T are in the supplementary material for space purposes. Fig.1 shows these
trajectories for one realization with T = 8000. The blue star is the optimal point’s position.
From Fig.1 we can see that the trajectories generated by Clipped-OGD follows the boundary very
tightly until reaching the optimal point. This can be explained by the Lemma 1 which shows that
the constraint violation for single step is also upper bounded. For the OGD  the trajectory oscillates
widely around the boundary of the true constraint. For the A-OGD  its trajectory in Fig.1 violates the
constraint most of the time  and this violation actually contributes to the lower objective regret shown
in the supplementary material.

5.2 Doubly-Stochastic Matrices

T(cid:80)

1

We also test the algorithms for approximation by doubly-stochastic matrices  as in [10]:

F

t=1

min

2 (cid:107)Yt − X(cid:107)2

s.t. X1 = 1  X T 1 = 1  Xij ≥ 0.

(11)
where X ∈ Rd×d is the matrix variable  1 is the vector whose elements are all 1  and matrix Yt is the
permutation matrix which is randomly generated.
After changing the equality constraints into inequality ones (e.g. X1 = 1 into X1 ≥ 1 and X1 ≤ 1) 
we run the algorithms with different T up to T = 20000 for 10 different random sequences of
{Yt}T
t=1. Since the objective function ft(x) is strongly convex with parameter H1 = 1  we also
include our designed strongly convex algorithm as another comparison. The ofﬂine optimal solutions
are obtained by CVXPY [5].
The mean results for both constraint violation and objective regret are shown in Fig.2. From the
result we can see that  for our designed strongly convex algorithm Our-Strong  its result is around the
best ones in not only the clipped constraint violation  but the objective regret. For our most-balanced
convex case algorithm Clipped-OGD with β = 0.5  although its clipped constraint violation is
relatively bigger than A-OGD  it also becomes quite ﬂat quickly  which means the algorithm quickly
converges to a feasible solution.

5.3 Economic Dispatch in Power Systems

This example is adapted from [12] and [18]  which considers the problem of power dispatch. That
is  at each time step t  we try to minimize the power generation cost ci(xt i) for each generator i
while maintaining the power balance
xt i = dt  where dt is the power demand at time t. Also 
each power generator produces an emission level Ei(xt i). To bound the emissions  we impose the

n(cid:80)

i=1

8

Ei(xt i) ≤ Emax. In addition to requiring this constraint to be satisﬁed on average  we

constraint
also require bounded constraint violations at each timestep. The problem is formally stated as:

i=1

n(cid:80)

xt i − dt)2(cid:17)

n(cid:80)

min

ci(xt i) + ξ(

 

s.t.

t=1

i=1

i=1

i=1

Ei(t  i) ≤ Emax  0 ≤ xt i ≤ xi max.
(12)

n(cid:80)
(cid:16) n(cid:80)

T(cid:80)

t i + bixt i  and Ei = dix2

where the second constraint is from the fact that each generator has the power generation limit.
In this example  we use three generators. We deﬁne the cost and emission functions according to
[18] and [12] as ci(xt i) = 0.5aix2
t i + eixt i  respectively. The parameters
are: a1 = 0.2  a2 = 0.12  a3 = 0.14  b1 = 1.5  b2 = 1  b3 = 0.6  d1 = 0.26  d2 = 0.38  d3 = 0.37 
Emax = 100  ξ = 0.5  and x1 max = 20  x2 max = 15  x3 max = 18. The demand dt is adapted
from real-world 5-minute interval demand data between 04/24/2018 and 05/03/2018 1  which is
shown in Fig.3(a). The ofﬂine optimal solution or best ﬁxed strategy in hindsight is obtained by
an implementation of SAGA [4]. The constraint violation for each time step is shown in Fig.3(b) 
and the running average objective cost is shown in Fig.3(c). From these results we can see that our
algorithm has very small constraint violation for each time step  which is desired by the requirement.
Furthermore  our objective costs are very close to the best ﬁxed strategy.

6 Conclusion

In this paper  we propose two algorithms for OCO with both convex and strongly convex objective
functions. By applying different update strategies that utilize a modiﬁed augmented Lagrangian
function  they can solve OCO with a squared/clipped long-term constraints requirement. The
algorithm for general convex case provides the useful bounds for both the long-term constraint
violation and the constraint violation at each timestep. Furthermore  the bounds for the strongly
convex case is an improvement compared with the previous efforts in the literature. Experiments show
that our algorithms can follow the constraint boundary tightly and have relatively smaller clipped
long-term constraint violation with reasonably low objective regret. It would be useful if future work
could explore the noisy versions of the constraints and obtain the similar upper bounds.

Acknowledgments

Thanks to Tianyi Chen for valuable discussions about algorithm’s properties.

References
[1] Avrim Blum  Vijay Kumar  Atri Rudra  and Felix Wu. Online learning in online auctions.

Theoretical Computer Science  324(2-3):137–146  2004.

[2] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge university

press  2006.

[3] Koby Crammer  Ofer Dekel  Joseph Keshet  Shai Shalev-Shwartz  and Yoram Singer. Online
passive-aggressive algorithms. Journal of Machine Learning Research  7(Mar):551–585  2006.

[4] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  pages 1646–1654  2014.

[5] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research  17(83):1–5  2016.

[6] John Duchi  Shai Shalev-Shwartz  Yoram Singer  and Tushar Chandra. Efﬁcient projections
onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international
conference on Machine learning  pages 272–279. ACM  2008.

1https://www.iso-ne.com/isoexpress/web/reports/load-and-demand

9

[7] John C Duchi  Shai Shalev-Shwartz  Yoram Singer  and Ambuj Tewari. Composite objective

mirror descent. In COLT  pages 14–26  2010.

[8] Maryam Fazel  Rong Ge  Sham M Kakade  and Mehran Mesbahi. Global convergence of policy

gradient methods for linearized control problems. arXiv preprint arXiv:1801.05039  2018.

[9] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  69(2):169–192  2007.

[10] Rodolphe Jenatton  Jim Huang  and Cédric Archambeau. Adaptive algorithms for online convex
optimization with long-term constraints. In International Conference on Machine Learning 
pages 402–411  2016.

[11] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[12] Yingying Li  Guannan Qu  and Na Li. Online optimization with predictions and switching costs:

Fast algorithms and the fundamental limit. arXiv preprint arXiv:1801.07780  2018.

[13] Mehrdad Mahdavi  Rong Jin  and Tianbao Yang. Trading regret for efﬁciency: online convex
optimization with long term constraints. Journal of Machine Learning Research  13(Sep):2503–
2528  2012.

[14] Julien Mairal  Francis Bach  Jean Ponce  and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th annual international conference on machine learning 
pages 689–696. ACM  2009.

[15] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529–533  2015.

[16] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming 

103(1):127–152  2005.

[17] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87.

Springer Science & Business Media  2013.

[18] K Senthil and K Manikandan. Economic thermal power dispatch with emission constraint
and valve point effect loading using improved tabu search algorithm. International Journal of
Computer Applications  2010.

[19] Hao Yu  Michael Neely  and Xiaohan Wei. Online convex optimization with stochastic con-

straints. In Advances in Neural Information Processing Systems  pages 1427–1437  2017.

[20] Jianjun Yuan and Andrew Lamperski. Online control basis selection by a regularized actor critic

algorithm. In American Control Conference (ACC)  2017  pages 4448–4453. IEEE  2017.

[21] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.
In Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages
928–936  2003.

10

,Siddharth N
Brooks Paige
Jan-Willem van de Meent
Alban Desmaison
Noah Goodman
Pushmeet Kohli
Frank Wood
Philip Torr
Jianjun Yuan
Andrew Lamperski