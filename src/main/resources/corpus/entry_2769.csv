2019,Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration,In this paper we consider the nonparametric least square regression in a Reproducing Kernel Hilbert Space (RKHS). We propose a new randomized algorithm that has optimal generalization error bounds with respect to the square loss  closing a long-standing gap between upper and lower bounds. Moreover  we show that our algorithm has faster finite-time and asymptotic rates on problems where the Bayes risk with respect to the square loss is small. We state our results using standard tools from the theory of least square regression in RKHSs  namely  the decay of the eigenvalues of the associated integral operator and the complexity of the optimal predictor measured through the integral operator.,Kernel Truncated Randomized Ridge Regression:

Optimal Rates and Low Noise Acceleration

Kwang-Sung Jun

The University of Arizona˚
kjun@cs.arizona.edu

Ashok Cutkosky
Google Research

ashok@cutkosky.com

Francesco Orabona
Boston University

francesco@orabona.com

Abstract

In this paper  we consider the nonparametric least square regression in a Reproduc-
ing Kernel Hilbert Space (RKHS). We propose a new randomized algorithm that
has optimal generalization error bounds with respect to the square loss  closing a
long-standing gap between upper and lower bounds. Moreover  we show that our
algorithm has faster ﬁnite-time and asymptotic rates on problems where the Bayes
risk with respect to the square loss is small. We state our results using standard
tools from the theory of least square regression in RKHSs  namely  the decay of the
eigenvalues of the associated integral operator and the complexity of the optimal
predictor measured through the integral operator.

Introduction

1
Given a training set S “ txt  ytun
t“1 of n samples drawn identically and independently distributed
from a ﬁxed but unknown distribution ρ on X ˆ Y  the goal of nonparametric least square regression
is to ﬁnd a function ˆf whose risk

ż

´
ˆfpxq ´ y

¯

2

dρ

Rp ˆfq :“

XˆY

is close to the optimal risk

R‹ :“ inf

f

Rpfq .

We focus on the kernel-based methods  which consider candidate functions from a Reproducing
Kernel Hilbert Space (RKHS) of functions and possibly their composition with elementary functions.
A classic kernel-based algorithm for nonparametric least squares is Kernel Ridge Regression (KRR) 
which constructs the prediction function ˆf as

ˆf “ argmin
fPHK

λ}f}2 ` 1
n

pfpxtq ´ ytq2 

nÿ

t“1

where HK is a RKHS associated with a kernel K and λ is the hyperparameter controlling the amount
of regularization.
It has been proved that  when the amount of regularization is chosen optimally and under similar
assumptions  KRR converges to the Bayes risk at the best known rate among kernel-based algo-
rithms [Lin et al.  2018]. Despite this result  kernel-based learning is still not a solved problem: these
rates match the known lower bounds in Fischer and Steinwart [2017] only in some regimes  unless
additional assumptions are used [Steinwart et al.  2009]. Indeed  it was not even known if the lower
bound was optimal in all the regimes [Pillaud-Vivien et al.  2018].

˚This work was done while the author was at Boston University.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

recent empirical

results.

regularization seems

results have also challenged the theoretical

In
Moreover 
particular  KRR without
to perform very well on real-world
datasets [Zhang et al.  2017  Belkin et al.  2018]  at least in the classiﬁcation setting  and even
outperform KRRs with any nonzero regularization in a popular computer vision dataset [Liang and
Rakhlin  2018  Figure 1]. This challenges the theoretical ﬁndings because our current understanding
of kernel-based learning tells us that a non-zero regularization is needed in all cases for learning
in inﬁnite dimensional RKHSs. Given the current gap in upper and lower bounds  it is unclear if
this mismatch between theory and practice is due to (i) suboptimal analyses that lead to suboptimal
choices of the amount of regularization or (ii) not taking into account crucial data-dependent
quantities (e.g.  capturing “easiness” of the problem) that allow fast rates and minimal regularization.
In this work  we address all these questions. We propose a new kernel-based learning algorithm
named Kernel Truncated Randomized Ridge Regression (KTR3). We show that the performance of
KTR3 is minimax optimal  matching known lower bounds. This closes the gap between upper and
lower bounds  without the need for additional assumptions. Moreover  we show that the generalization
guarantee of KTR3 accelerates when the Bayes risk is zero or close to zero. As far as we know  the
phenomenon is new in this literature. Finally  we identify a regime of easy problems in which the
best amount of regularization is exactly zero.
Another important contribution lies in our proof methods  which vastly differ from the usual one in
this ﬁeld. In particular  we use methods from the online learning literature that make the proof very
simple and rely only on population quantities rather than empirical ones. We believe the community
of nonparametric kernel-regression will greatly beneﬁt from the addition of these new tools.
The rest of the paper is organized as follows: In the next section  we formally introduce the setting
and our assumptions. In Section 3 we introduce our KTR3 algorithm and its theoretical guarantee 
and in Section 4 the precise comparison with similar results. In Section 5  we empirically evaluate
our ﬁndings. Finally  Section 6 discusses open problems and future directions of research.

2 Setting and Notation: Source Condition and Eigenvalue Decay

In this section  we formally introduce our learning setting and our characterization of the complexity
of each regression problem. This characterization is standard in the literature on regression in RKHS 
see  e.g.  Steinwart and Christmann [2008]  Steinwart et al. [2009]  Dieuleveut and Bach [2016]  Lin
et al. [2018].
Let X Ă Rd a compact set and HK a separable RKHS associated to a Mercer kernel K : Xˆ X Ñ R
implementing the inner product x¨ ¨y and induced norm } ¨ }. The inner product is deﬁned so that it
satisﬁes the reproducing property  xKpx ¨q  fp¨qy “ fpxq. Denote by Kt P Rtˆt the Gram matrix
such that Ki j “ Kpxi  xjq where xi  xj belong to St Ď S that contains the ﬁrst2 t elements of the
training set S.
Our ﬁrst assumption is related to the boundedness of the kernel and labels.
Assumption 1 (Boundedness). We assume K to be bounded  that is  supxPX Kpx  xq “ R2 ă 8.
To avoid superﬂuous notations and without loss of generality  we further assume R “ 1. We also
assume the labels to be bounded: Y “ r´Y  Y s where Y ă 8.
Denote by ρX the marginal probability measure on X and let L2
ρX be the space of square integrable
functions with respect to ρX. We will assume that the support of ρX is X  whose norm is denoted by
}g}ρ :“
X g2pxqdρX. It is well known that the function minimizing the risk over all functions
ρX is fρpxq :“
Y ydρpy|xq  which has the Bayes risk with respect to the square loss  R‹ “
in L2
Rpfρq “ inf fPL2
If we use a Universal Kernel (e.g.  the Gaussian kernel) [Steinwart  2001] and X is compact  we have
that inf fPHK Rpfq “ R‹ [Steinwart and Christmann  2008  Corollary 5.29]. This suggests that using
a universal kernel is somehow enough to reach the Bayes risk. However  while fρ P L2
ρX  this actually

does not imply that fρ P HK but only that fρ PĚHK  which is the closure of HK. Thus  the question

ş
ρX Rpfq.

bş

2Note that the ordering of the elements in S is immaterial  but our algorithm will depend on it. So we can

just consider S ordered according to an initial random shufﬂing.

2

Algorithm 1 KTR3: Kernel Truncated Randomized Ridge Regression

Input: A training set S “ tpxi  yiqun
Randomly permute the training set S
for t “ 0  1  . . .   n ´ 1 do
λ}f}2 ` 1
Set ft “ argminfPHK
(take the minimum norm solution when there is no unique solution)
end for
Return T Y ˝ fk  where k is uniformly at random between 0 and n ´ 1

i“1  a regularization parameter λ ě 0
ř

i“1pfpxiq ´ yiq2

n

t

ş

of whether it is possible to achieve the Bayes risk is relevant even for Universal kernels. We address
this by the standard parametrization called source condition that smoothly characterizes whether fρ
belongs or not to HK. To introduce the formalism  let LK : L2
ρX be the integral operator
deﬁned by pLKfqpxq “
X Kpx  x1qfpx1qdρXpx1q. There exists an orthonormal basis tΦ1  Φ2 ¨¨¨u
ρX consisting of eigenfunctions of LK with corresponding non-negative eigenvalues tλ1  λ2 ¨¨¨u
of L2
and the set tλiu is ﬁnite or λk Ñ 0 when k Ñ 8 [Cucker and Zhou  2007  Theorem 4.7]. Since
K is a Mercer kernel  LK is compact and positive. Moreover  given that we assumed the kernel to
be bounded  LK is trace class  hence compact [Steinwart and Christmann  2008]. Therefore  the
fractional power operator Lβ

K is well-deﬁned for any β ě 0. We indicate its range space by
KpL2
Lβ

*
i ă 8
a2

"
f “

ρX Ñ L2

ρXq :“

λβ
i aiΦi

8ÿ

8ÿ

i“1

:

i“1

.

This space has a key role in our analysis. In particular  we will use the following assumption.
Assumption 2 (Source Condition). We assume that fρ P Lβ
KpL2
2   which is
Kpgq .

ρXq for 0 ă β ď 1

ρX : fρ “ Lβ

Dg P L2

KpL2

ρXq “ L2

ρX. On the other hand  we have that L1{2

K pL2
ρX  and }f} “ }L
2s allow us to consider spaces in between L2

Note that the assumption above is always satisﬁed for β “ 0 because  by deﬁnition of the orthonormal
ρXq “ HK  that is every function
basis  L0
´1{2
f P HK can be written as L1{2
K f}ρ [Cucker and Zhou  2007 
K g for some g P L2
Corollary 4.13]. Hence  the values of β in r0  1
ρX and
HK  including the extremes. Thus  a bigger β means a simpler function fρ.
Another assumption needed to characterize the learning process is on the complexity of the RKHS
itself  rather than on the complexity of the optimal function. This is typically done assuming that the
eigenvalue of the integral operator satisﬁes a certain rate of decay. We will use equivalent condition 
assuming that the trace of some fractional power of the integral operator is bounded.
Assumption 3 (Eigenvalue Decay). Assume that there exists b P r0  1s such that TrrLb
Note that the sum of the eigenvalues of LK is at most supxPX Kpx  xq  which we assumed to be
bounded in Assumption 1. This implies that the assumption above is always satisﬁed with b “ 1.
Hence  a smaller b corresponds to an RKHS with a smaller complexity.

Ks ă 8.

3 Kernel Truncated Randomized Ridge Regression

We now describe our algorithm called Kernel Truncated Randomized Ridge Regression (KTR3).
The pseudo-code is in Algorithm 1. The algorithm consists of two stages. In the ﬁrst stage  we
generate n candidate functions solving KRR with increasing sizes of the training set and a ﬁxed
regularization weight λ. In the second stage  we select the prediction function as the truncation of
one of the candidate functions uniformly at random. Note that this is equivalent to extracting a subset
of the training set of size i  where i is uniformly at random between 0 and n ´ 1 and training a KRR
on the subset with parameter λ. The truncation function is deﬁned as follows

The deﬁnition of the truncation function implies that pT Y pˆyq ´ yq2 ď pˆy ´ yq2 @ˆy P R  y P Y.

T Y pzq :“ minpY |z|q ¨ signpzq .

3

We now present our two main theorems on the excess risk of KTR3 where Theorem 1 is on λ ą 0 and
Theorem 2 is on λ “ 0 for an “easy” problem regime. The proof of Theorem 2 is in the Appendix.
Theorem 1. Let X Ă Rd be a compact domain and K a Mercer kernel such that Assumptions 1 2 
and 3 are veriﬁed. Deﬁne by fS λ the function returned by the KTR3 algorithm on a training set S
with regularization parameter λ ą 0. Then
ﬀ
ErRpfS λqs ´ Rpfρq
ď λ2β}L´β

4Y 2 TrrLb
Ks

` Rpfρq

λ2β´1}L´β

K fρ}2

ln1´b

K fρ}2

ρ ` min

ˆ

ˆ

˙

˙

«

ρ

1 ` 1
λ

 

1
b

 

min

λbn

n

 

λn

where the expectation is with respect to S and the randomization of the algorithm.
Theorem 2. Let λ “ 0 and assume the same conditions as in Theorem 1 except for λ. Assume
β “ 1{2 and Rpfρq “ 0. Assume that the distribution ρ satisﬁes that Kn is invertible with
probability 1. Then  ErRpfS 0qs ´ Rpfρq “ Opn´1q.
Remark. Our algorithm can be changed to randomize at the prediction time for each test data point
rather than the training time while enjoying the same risk bound. Furthermore  our algorithm can
sample from tp1´ αqnu to n´ 1 for some α P p0  1s instead of from t0  . . .   n´ 1u and obtain a rate
α factor worse than the bounds above; our choice of presentation of Algorithm 1 is for simplicity.
From the above theorem  with appropriate settings of the regularization parameter λ it is possible to
obtain the following convergence rates.
Corollary 1. Under the assumptions of Theorem 1  there exists a setting of λ ě 0 such that:
¯¯

(i) When b ‰ 0 

´

1

ErRpfS λqs ´ Rpfρq ď O

min

(ii) In the case b “ 0 and β “ 1
2  3

ErRpfS λqs ´ Rpfρq ď O

`

´
pn{Rpfρqq´ 2β
2β`1 ` n´2β  n´ 2β
2β`b
˘˘
`
Ks
1 ` n{ TrrL0

n´1 TrrL0

Ks log

.

.

The proof and the tuning of λ can be found in the Appendix. Before moving to the proof of Theorem 1
in the next section  there are some interesting points to stress.

• In the case of Rpfρq ‰ 0  our rate n´ 2β

2β`b matches the worst-case lower bound [Fischer
and Steinwart  2017] without additional assumptions for the ﬁrst time in the literature  to
our knowledge. Speciﬁcally  our bound is a strict improvement in the regime 2β ` b ă 1
upon the best-known bound Opn´2βq of KRR [Lin et al.  2018] and stochastic gradient
descent [Dieuleveut and Bach  2016]. In this regime  our rate goes to Opn´1q as b goes to 0.
• If Rpfρq “ 0  we have convergence of the risk to 0 at a faster rate of n´
mint2β`b 1u . It is
important to stress that this holds also in the case that fρ R HK  i.e.  β ă 1
2. As far as we
know  this result is new and we are not aware of lower bounds under the same assumptions.
• When Rpfρq “ 0  the optimal λ that minimizes the generalization upper bound in Theorem 1
goes to zero when β goes to 1{2 and becomes exactly 0 when β is exactly 1{2.

2β

3.1 Proof of Theorem 1

Our proof technique is vastly different from the existing ones for analyzing KRR and stochastic
gradient descent methods. It is also extremely short and simple compared to the proofs of similar
results. Our technique is based on the well-known possibility to solve batch problems through a
reduction to online learning ones. In turn  we use a recent result on the performance of online kernel
ridge regression  Theorem 3 by Zhdanov and Kalnishkan [2013]. This result is the key to obtain
the improved rates in the regime 2β ` b ă 1. In particular  it allows us to analyze the effect of the
3When b “ 0 the space is ﬁnite dimensional  hence β can only have value 0 or 1{2 and there is no convergence

to the Bayes risk when β “ 0.

4

eigenvalues using only the expectation of the Gram matrix Kn and nothing else. Instead  previous
proofs [e.g.  Lin and Cevher  2018] involved the study of the convergence of empirical covariance
operator to the population one  which seems to deteriorate when the regularization parameter becomes
too small  which is precisely needed in the regime 2β ` b ă 1.
Theorem 3. [Zhdanov and Kalnishkan  2013  Theorem 1] Take a kernel K on a domain X and a
nÿ
parameter λ ą 0. Then  with the notation of Algorithm 1  we have

nÿ

pft´1pxtq ´ ytq2

1 ` dt

λn

1
n

t“1

“ min
fPHK

λ}f}2 ` 1
n

t“1

pfpxtq ´ ytq2  

:“ Kpxt  xtq ´ kt´1pxtqJpKt´1 ` λnIq´1kt´1pxtq ě 0  kt´1pxtq
where dt
rKpxt  x1q  . . .   Kpxt  xt´1qsJ  and Kt´1 is the Gram matrix of the samples x1  . . .   xt´1.
We use the following well-known result to upper bound the approximation error  which is the gap
between the value of the regularized population risk minimization problem and the Bayes risk.
Theorem 4. [Cucker and Zhou  2007  Proposition 8.5.ii] Let X Ă Rd be a compact domain and K
a Mercer kernel such that Assumption 2 holds. Then  for any 0 ă β ď 1{2  we have

:“

min
fPHK

λ}f}2 ` Rpfq ´ Rpfρq ď λ2β}L´β
K fρ}2
ρ .
˙
˙
We also need the following technical lemmas. The proof of the next lemma is in the Appendix.
Lemma 1. Under Assumptions 1 and 3  and with λ ą 0  we have
1 ` 1
λ

Ks
TrrLb
λb

n Kn|

ln1´b

ˆ

ˆ



„

1
b

 

.

|λIn ` 1
|λIn|
„
Furthermore  if b “ 0  then

ES

ln

˙

ES

ln

n Kn|

|λIn ` 1
|λIn|

ď ln

1
TrrL0

Ksλ

TrrL0

Ks .

ď min


ˆ
1 `

Note that the logarithmic term is unavoidable when b “ 0 because in the ﬁnite dimensional case we
pay ´ lnpλq due to the online learning setting. The last lemma is a classic result in online learning [e.g.
Cesa-Bianchi et al.  2005].
Lemma 2. With the notation in Theorem 3  we have that

dt

dt ` λn

ď ln

n Kn|

|λIn ` 1
|λIn|

.

`

˘

`

. Hence 
. Also  using Zhdanov and Kalnishkan [2013  Lemma 3] we have

x`1  we have that

λn

dt

dt

λn

n
t“1

ś

1 ` dt

dt`λn ď log

ř
ś
dt`λn ď ln
Proof. From the elementary inequality lnp1` xq ě x
n
t“1
t“1pλn ` dtq “ |λnIn ` Kn|. Putting all together  we have the stated bound.
n
We are now ready to prove Theorem 1.
Proof of Theorem 1. Deﬁne fλ “ argminfPHK
ization true risk minimization problem.
First  we use the so-called online-to-batch conversion [Cesa-Bianchi et al.  2004] to have
ES krRpT Y ˝ fkqs “ ES

n´1ÿ

˘ﬀ

“ ES

`

R

λ}f}2 ` Rpfq  which is the solution of the regular-

1 ` dt

nÿ
˘

t“1

«
«
«

1
n

1
n

1
n

t“0

n´1ÿ
n´1ÿ
nÿ
`

t“0

t“1

“ ES

“ ES

T Y ˝ ft
`

ESt

T Y pftpxqq ´ y

T Y pft´1pxtqq ´ yt

5

˘ﬀ
`
T Y ˝ ft
n´1ÿ
`

«
EStR

t“0
“ ES

1
n

t“0

1
n

«
ﬀ
˘2
ﬀ
˘2

.

ﬀ

˘2

ESt

T Y pftpxt`1qq ´ yt`1

«

Denote by d1

ES

1
n

nÿ

t“1

λn  (cid:96)1

`
t “ dt
T Y pft´1pxtqq ´ yt

˘2

ﬀ

nÿ
t “ pTY pft´1pxtqq ´ ytq2  and (cid:96)t “ pft´1pxtq ´ ytq2. We have that

ﬀ

«

ﬀ

ﬀ

(cid:96)1
1 ` d1

t

t

«
«

1
n

1
n

nÿ
nÿ

t“1

t“1

nÿ

1
n

ˆ

t“1
ln1´b

(cid:96)1

t

ﬀ
“ ES
(cid:96)1
td1
1 ` d1
ﬀ

t

t

t

t

d1
ˆ
1 ` d1
«

1 ` 1
λ

“ ES

ď ES
«

ď 4Y 2ES

ď 4Y 2
n
nÿ

min

ﬀ

«

1
n
` ES

nÿ
«

t“1
1
n

t

td1
(cid:96)1
nÿ
1 ` d1

t

t“1

„

˙
˙
ď 4Y 2
n
nÿ

1
b

 

ES
ln
TrrLb
Ks
ﬀ
λb

.

ﬀ
` ES

1
n

t“1

(cid:96)t
1 ` d1

t

.



|λI ` 1
n Kn|
|λI|

We now focus on the ﬁrst sum in the last inequality and we upper bound it in two different ways.
First  using Lemma 2 and Lemma 1  we have

«

ES

1
n

ﬀ

nÿ

t“1

ﬀ

t

t

(cid:96)1
td1
1 ` d1
«

«

nÿ

Also  we can upper bound the same term as

(cid:96)td1
1 ` d1

t

(cid:96)1
td1
1 ` d1

t

t

t

1
n

1
n

t“1

t“1

ES

ﬀ

nÿ

ď ES

ď ES
nÿ
“ ES
t“1
“ λ}fλ}2 ` Rpfλq “ min
fPHK
ď λ2β}L´β
ρ ` Rpfρq .
Putting all together  we have the stated bound.

λ}f}2 ` 1
n

K fρ}2

(cid:96)t
1 ` d1

minHK

ES

t“1

1
n

t

«
Now  using Theorems 3 and 4 with the fact that dt ď 1  we bound the term ES

«

ﬀ

nÿ

t“1

1
n

«
ř

ﬀ

.

(cid:96)t
ı
1 ` d1

t

ﬀ

t

as

(cid:96)t
1`d1
pfλpxtq ´ ytq2

maxt d1

t

t

n

t“1

”

ES

ď 1
λn

(cid:96)t
1 ` d1
«
pfpxtq ´ ytq2
λ}fλ}2 ` 1
n
λ}f}2 ` Rpfq ´ Rpfρq ` Rpfρq

ď ES

1
n

n
t“1

nÿ

t“1

4 Detailed Comparison with Previous Results

The sheer volume of research on regression  see  e.g.  Lin and Cevher [2018  Table 1]  precludes
a complete survey of the results. In this section  we focus on the closely related ones that involve
inﬁnite dimensional spaces.
First  it is useful to compare our convergence rate to the one we would get from known guarantees for
KRR. We can compare it to the stability bound in Shalev-Shwartz and Ben-David [2014] for KRR:

ES

R

f KRR
S λ

ď

`

“
`

“

˘‰

ˆ

˘‰

˙

ES

nÿ

`

t“1

«
˙

1
n

1 ` 192
ˆ
λn

ﬀ

˘2

S λ pxtq ´ yt
f KRR

.

It is easy to see4 that this bound implies the following convergence rate
K fρ}2

´ Rpfρq ď

λ2β}L´β

f KRR
S λ

ES

R

1 ` 192
λn

ρ ` 192Rpfρq

.

λn

This convergence rate matches only half of our bound. In particular  it does not contain the term that
depends in the capacity of the RKHS through b. Also  the theorem in Shalev-Shwartz and Ben-David
m. This essentially prevents the setting of λ “ 0 and the possibility to
[2014] holds only for λ ě 4
achieve the rate of n´1 in the case that β “ 1
2 and Rpfρq “ 0.
˙
˘‰
Another similar bound is the Leave-One-Out analysis in Zhang [2003]  which gives
pfpxtq ´ ytq2 .

nÿ

ES

`

“

R

ď

2

f KRR
S λ

ˆ
1 ` 2
λn

λ}f}2 ` 1
n

min
fPHK

t“1

4For completeness  the proof is in Theorem 5 in the Appendix.

6

“

`

˘‰

ˆ

As for the stability bound  using Theorem 4  this bound implies the following bound for λ ą 0:

ES

R

f KRR
S λ

´ Rpfρq ď

1 ` 2
λn

2

λ2β}L´β

K fρ}2

ρ `

4
λn

` 4
λ2n2

Rpfρq .

ˆ

˙

˙

#

Hence  this bound suffers from the same problems of the stability bound; it is suboptimal with respect
to the capacity of the space and the presence of the square always makes the λ that minimizes the
risk bound bounded away from zero.
The best known results for nonparametric least square under Assumptions 1–3 are obtained by
KRR [Lin et al.  2018] and by stochastic least square [Dieuleveut and Bach  2016]  with the rate

ES rRpfS λqs ´ Rpfρq ď

O

O

¯

´
`

˘

n´ 2β
2β`b
n´2β

  if 2β ` b ě 1 

  otherwise.

2β`bq in all regimes for truncated KRR.

This kind of rates are suboptimal in the regime 2β` b ă 1. In contrast  our result achieves the optimal
rate in all regimes. Also  these rates do not depend in any way on the risk of the optimal function fρ.
Hence  they never support the choice of a regularization parameter being zero. Pillaud-Vivien et al.
[2018] call the regime 2β ` b ă 1 the “hard” problems and prove that SGD with multiple passes
achieves the optimal rate for a subset of the hard problems However  their result makes an additional
assumption on the inﬁnity norm of the functions in HK. Under the same assumption  Steinwart et al.
[2009] present a convergence rate of Opn´ 2β
The only result we are aware of that shows an acceleration in the low noise case is Orabona [2014].
Using a SGD-like procedure that does not require to set parameters  he proves a rate of Opn´ 2β
2β`1q
that accelerates to Opn´ 2β
Turning to KRR used for classiﬁcation  in the extreme case of the Tsybakov’s noise condition
(also called Massart low noise condition [Massart and Nédélec  2006]) Yao et al. [2007] proved an
exponential rate of convergence. However  this is speciﬁc to the classiﬁcation case only and it does
not apply to the regression setting. Under stronger assumptions  i.e. data separable with margin  the
same effect was already proved in Zhang [2001]. It is also interesting to note that these results require
a non-zero implicit or explicit regularization.
More recently  Hastie et al. [2019] showed5 an asymptotic result (as nÑ8) that the best regularization
parameter λ of ridge regression is 0 when there is no label noise (i.e.  Rpfρq “ 0) and β “ 1
2. Their
result aligns well with ours  but we are not limited to asymptotic regimes nor ﬁnite dimensional
spaces. On the other hand  our guarantee is an upper bound on the risk rather than an equality.

β`1q when Rpfρq “ 0  for smooth and Lipschitz losses.

5 Empirical Validation

In this section  we empirically validate some of our theoretical ﬁndings. Inspired by Pillaud-Vivien
et al. [2018]  we consider a spline kernel of order q ě 2 where q is even [Wahba  1990  Eq. (2.1.7)].
Speciﬁcally  we deﬁne

Λqpx  x1q “ 1 ` 2

cosp2πkps ´ tqq

p2πkqq

.

8ÿ

k“1

and use the kernel Kpx  x1q “ Λ1{bpx  x1q for some b P r0  1s. We consider the uniform distribution
ρX on X “ r0  1s and deﬁne the target function to be f‹pxq “ Λ β
px  0q for x P X. We deﬁne the
observed response of x to be f‹pxq ` B where B is a uniform random variable r´  s. One can
show that this problem satisﬁes Assumptions 1–3 [Pillaud-Vivien et al.  2018].
For each n in ﬁne-grained grid points in r102  103s and λ in another ﬁne-grained set of numbers  we
draw n training points  compute fn by Algorithm 1  and estimate its excess risk by a test set. Finally 
for each n we choose the λ that minimizes the average excess risk. We repeat the same 5 times. First 
we set b “ 1
16  and  “ 0.1. Figure 1(a) plots the excess risk of the best λ’s vs n  which
approximately achieves the predicted rate n´ 7
8 .

8 and β “ 7

b ` 1

2

5To see this  set σ2 “ 0 in Hastie et al. [2019  Theorem 6].

7

(a)

(b)

Figure 1: Expected excess risk of KTR3 vs the number of training points on a synthetic dataset with
a spline kernel. (a) and (b) show two different difﬁculties of the task  as parametrized by β and b.

To verify our improved rate in the regime 2β ` b ă 1  we also consider the case of β “ 1
4  b “ 1
6 
and  “ 0.1. Figure 1(b) plots the excess risk of the best λ’s vs n  which approximately achieves the
predicted rate n´ 3

4 rather than the slow rate n´ 1

2 of prior art.6

6 Discussion and Open Problems

We have presented a new algorithm for kernel-based nonparametric least squares that achieves optimal
generalization rates with respect to the source condition and complexity of the RKHS. Moreover 
faster rates are possible when the Bayes risk is zero  even when the optimal predictor is not in HK.
One natural open problem is to prove similar guarantees for KRR. We conjecture that the randomiza-
tion used in our analysis is not strictly necessary; it only greatly simpliﬁes the proof. One may try to
prove that the generalization error of KRR is nonincreasing with n in which case the randomization
only harms the generalization and thus implies that KRR enjoys the same error bound as KTR3. Such
a claim is  unfortunately  not true  shown by Viering et al. [2019  Example III] where the error rate of
KRR can increase with n.
It would also be interesting to prove lower bounds for the Rpfρq “ 0 case  to understand if the ob-
tained rates are optimal or not. Furthermore  alleviating the boundedness assumption (Assumption 1)
would be interesting  possibly with some mild moment conditions that appear in Hsu et al. [2012] 
Audibert and Catoni [2011] and Hsu and Sabato [2016].
One consequence of our work is that it shows a gap between the best-known bounds for SGD and
ERM-based algorithms. Indeed  before this work  the rates of SGD and ERM-based algorithms (e.g. 
KRR) under Assumptions 1–3 were the same. It would be interesting to understand if some variants
of SGD can achieve the optimal rates or if there is indeed a clear separation between the rates.
The limitation of this work is mainly with regards to the parametrization of the problem via the source
condition and the complexity of the RKHS. Speciﬁcally  our rates are only valid for β ď 1{2 (see
Assumption 2)  due to use of Theorem 4. However  this is unlikely to be a limitation of the analysis
but rather a consequence of the use of a regularizer and the consequent “saturation” phenomenon 
see discussion in Yao et al. [2007]. Another limitation of our framework is that it is well-known
that the guarantee on the approximation error in Theorem 4 is non-trivial for a Gaussian kernel
with ﬁxed bandwidth only if fρ P C8 [Smale and Zhou  2003]. While this is a strong condition
from a mathematical point of view  it is unclear how strong it is for real-world problems  where the
bandwidth of the Gaussian kernel is often tuned.

6We remark that the considered kernel satisﬁes an extra assumption (e.g.  Pillaud-Vivien et al. [2018 
Assumption (A3)]) that in fact allows KRR to achieve the same optimal rate as ours. We are not aware of simple
problems where that condition is not satisﬁed. However  our theory clearly does not make such an assumption
yet achieves the optimal rate.

8

4.555.566.57log of number of training points-6-5-4-3-2log of expected excess riskBest fit line  rate n-0.854.555.566.57log of number of training points-6-5-4-3-2log of expected excess riskBest fit line  rate n-0.72Finally  we believe the assumptions considered too strong in the theory community can be reconsid-
ered with modern machine learning tasks. Indeed  most results in the community have ignored the
case of Rpfρq “ 0  perhaps due to the fact that it was considered too strong as a condition. However 
most of the visual perception tasks on which modern machine learning has been successful seem to
satisfy this assumption; for example  humans have zero or very close to zero error in recognizing cats
versus dogs from a photograph. In this view  a more ambitious open problem is to ﬁnd the correct
characterization of “easiness” for real-world problems  rather than using mathematically appealing
ones.

Acknowledgements

The authors thank Junhong Lin  Lorenzo Rosasco  and Alessandro Rudi for the comments and
discussions on this work. This material is based upon work supported by the National Science
Foundation under grant no. 1908111 “Collaborative Research: TRIPODS Institute for Optimization
and Learning”.

References
J.-Y. Audibert and O. Catoni. Robust linear least squares regression. The Annals of Statistics  39(5):

2766–2794  2011.

M. Belkin  S. Ma  and S. Mandal. To understand deep learning we need to understand kernel learning.
In J. Dy and A. Krause  editors  International Conference on Machine Learning  volume 80
of Proceedings of Machine Learning Research  pages 541–549  Stockholmsmässan  Stockholm
Sweden  2018. PMLR.

N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning
algorithms. IEEE Trans. Inf. Theory  50(9):2050–2057  2004. URL https://homes.di.unimi.
it/~cesabian/Pubblicazioni/J20.pdf.

N. Cesa-Bianchi  A. Conconi  and C. Gentile. A second-order Perceptron algorithm. SIAM Journal

on Computing  34(3):640–668  2005.

F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge

University Press  New York  NY  USA  2007.

A. Dieuleveut and F. Bach. Nonparametric stochastic approximation with large step-sizes. The

Annals of Statistics  44(4):1363–1399  2016.

S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithm.

arXiv preprint arXiv:1702.07254  2017.

T. Hastie  A. Montanari  S. Rosset  and R. J. Tibshirani. Surprises in high-dimensional ridgeless least

squares interpolation. arXiv preprint arXiv:1903.08560v2  2019.

D. Hsu and S. Sabato. Loss minimization and parameter estimation with heavy tails. The Journal of

Machine Learning Research  17(1):543–582  2016.

D. Hsu  S. M. Kakade  and T. Zhang. Random design analysis of ridge regression. In Proc. of the

25th Conference on Learning Theory  pages 9–1  2012.

T. Liang and A. Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize. arXiv

preprint arXiv:1808.00387  2018.

J. Lin and V. Cevher. Optimal convergence for distributed learning with stochastic gradient methods

and spectral algorithms. arXiv preprint arXiv:1801.07226  2018.

J. Lin  A. Rudi  L. Rosasco  and V. Cevher. Optimal rates for spectral algorithms with least-squares

regression over Hilbert spaces. Applied and Computational Harmonic Analysis  2018.

P. Massart and É. Nédélec. Risk bounds for statistical learning. The Annals of Statistics  34(5):

2326–2366  2006.

9

F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic

learning. In Advances in Neural Information Processing Systems 27  2014.

L. Pillaud-Vivien  A. Rudi  and F. Bach. Statistical optimality of stochastic gradient descent on hard
learning problems through multiple passes. In Advances in Neural Information Processing Systems
31  pages 8114–8124. Curran Associates  Inc.  2018.

L. Rosasco  M. Belkin  and E. De Vito. On learning with integral operators. J. Mach. Learn. Res.  11:

905–934  March 2010.

S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press  New York  NY  USA  2014.

S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Analysis and

Applications  1(01):17–41  2003.

I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of

machine learning research  2(Nov):67–93  2001.

I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.

I. Steinwart  D. R. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In

Proc. of the 22nd Conference on Learning Theory  2009.

T. Viering  A. Mey  and M. Loog. Open problem: Monotonicity of learning.

Conference on Learning Theory (COLT)  pages 3198–3201. PMLR  2019.

In Proc. of the

G. Wahba. Spline models for observational data  volume 59. Siam  1990.

Y. Yao  L. Rosasco  and A. Caponnetto. On early stopping in gradient descent learning. Constr.

Approx.  26:289–315  2007.

C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires
rethinking generalization. In 5th International Conference on Learning Representations  ICLR
2017  Toulon  France  April 24-26  2017  Conference Track Proceedings  2017.

T. Zhang. Convergence of large margin separable linear classiﬁcation.

Information Processing Systems  pages 357–363  2001.

In Advances in Neural

T. Zhang. Leave-one-out bounds for kernel methods. Neural Comput.  15(6):1397–1437  June 2003.

F. Zhdanov and Y. Kalnishkan. An identity for kernel ridge regression. Theor. Comput. Sci.  473:

157–178  February 2013.

10

,Kwang-Sung Jun
Ashok Cutkosky
Francesco Orabona