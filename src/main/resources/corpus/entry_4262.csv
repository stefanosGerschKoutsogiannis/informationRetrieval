2009,Periodic Step Size Adaptation for Single Pass On-line Learning,It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e.  epoch) through the training examples. However  2SGD requires computing the inverse of the Hessian matrix of the loss function  which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA)  which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.,Periodic Step-Size Adaptation for

Single-Pass On-line Learning

Chun-Nan Hsu1 2 ∗  Yu-Ming Chang1  Han-Shen Huang1 and Yuh-Jye Lee3

1Institute of Information Science  Academia Sinica  Taipei 115  Taiwan
2USC/Information Sciences Institute  Marina del Rey  CA 90292  USA

3Department of Computer Science and Information Engineering 

National Taiwan University of Science and Technology  Taipei 106  Taiwan

∗chunnan@isi.edu

Abstract

It has been established that the second-order stochastic gradient descent (2SGD)
method can potentially achieve generalization performance as well as empirical
optimum in a single pass (i.e.  epoch) through the training examples. However 
2SGD requires computing the inverse of the Hessian matrix of the loss function 
which is prohibitively expensive. This paper presents Periodic Step-size Adapta-
tion (PSA)  which approximates the Jacobian matrix of the mapping function and
explores a linear relation between the Jacobian and Hessian to approximate the
Hessian periodically and achieve near-optimal results in experiments on a wide
variety of models and tasks.

1

Introduction

On-line learning has been studied for decades. Early works concentrate on minimizing the required
number of model corrections made by the algorithm through a single pass of training examples.
More recently  on-line learning is considered as a solution of large scale learning mainly because
of its fast convergence property. New on-line learning algorithms for large scale learning  such as
SMD [1] and EG [2]  are designed to learn incrementally to achieve fast convergence. They usually
still require several passes (or epochs) through the training examples to converge at a satisfying
model. However  the real bottleneck of large scale learning is I/O time. Reading a large data set
from disk to memory usually takes much longer than CPU time spent in learning. Therefore  the
study of on-line learning should focus more on single-pass performance. That is  after processing
all available training examples once  the learned model should generalize as well as possible so
that used training example can really be removed from memory to minimize disk I/O time.
In
natural learning  single-pass learning is also interesting because it allows for continual learning from
unlimited training examples under the constraint of limited storage  resembling a nature learner.

Previously  many authors  including [3] and [4]  have established that given a sufﬁciently large set
of training examples  2SGD can potentially achieve generalization performance as well as empirical
optimum in a single pass through the training examples. However  2SGD requires computing the
inverse of the Hessian matrix of the loss function  which is prohibitively expensive. Many attempts
to approximate the Hessian have been made. For example  one may consider to modify L-BFGS [5]
for online settings. L-BFGS relies on line search. But in online settings  we only have the surface of
the loss function given one training example  as opposed to all in batch settings. The search direction
obtained by line search on such a surface rarely leads to empirical optimum. A review of similar
attempts can be found in Bottou’s tutorial [6]  where he suggested that none is actually sufﬁcient to
achieve theoretical single-pass performance in practice. This paper presents a new 2SGD method 
called Periodic Step-size Adaptation (PSA). PSA approximates the Jacobian matrix of the mapping
function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian

1

periodically. The per-iteration time-complexity of PSA is linear to the number of nonzero dimen-
sions of the data. We analyze the accuracy of the approximation and derive the asymptotic rate of
convergence for PSA. Experimental results show that for a wide variety of models and tasks  PSA is
always very close to empirical optimum in a single-pass. Experimental results also show that PSA
can run much faster compared to state-of-the-art algorithms.

2 Aitken’s Acceleration

Let w ∈ ℝ be a -dimensional weight vector of a model. A machine learning problem can be
formulated as a ﬁxed-point iteration that solves the equation w = ℳ(w)  where ℳ is a mapping
ℳ : ℝ → ℝ  until w∗ = ℳ(w∗). Assume that the mapping ℳ is differentiable. Then we
can apply Aitken’s acceleration  which attempts to extrapolate to the local optimum in one step  to
accelerate the convergence of the mapping:

w∗ = w() + (I − J)−1(ℳ(w()) − w()) 

(1)
where J := ℳ′(w∗) is the Jacobian of the mapping ℳ at w∗. When  := eig(J) ∈ (−1  1)  the
mapping ℳ is guaranteed to converge. That is  when  → ∞  w() → w∗.
It is usually difﬁcult to compute J for even a simple machine learning model. To alleviate this issue 
we can approximate J with the estimates of its -th eigenvalue  by

()



:= ℳ(w()) − w()
 − w(−1)
w()





  ∀ 

and extrapolate at each dimension  by:
= w()

w(+1)



 + (1 − ()

 )−1(ℳ(w()) − w()
 ) .

(2)

(3)

In practice  Aitken’s acceleration alternates a step for preparing () and a step for the extrapolation.
That is  when  is an even number  ℳ is used to obtain w(+1). Otherwise  the extrapolation (3) is
used. A beneﬁt of the above approximation is that the cost for performing an extrapolation is () 
linear in terms of the dimension.

3 Periodic Step-Size Adaptation

When ℳ is a gradient descent update rule  that is  ℳ(w) ← w − g(w; D)  where  is a scalar
step size  D is the entire set of training examples  and g(w; D) is the gradient of a loss function to
be minimized  Aitken’s acceleration is equivalent to Newton’s method  because

(I − J)−1 =

1


J = ℳ′(w) = I − H(w; D) 

(4)

H(w; D)−1  and ℳ(w) − w = w − g(w; D) − w = −g(w; D) 

where H(w; D) = ′(w; D)  the Hessian matrix of the loss function  and the extrapolation given in
(1) becomes

w = w + (I − J)−1(ℳ(w) − w) = w −

1


H−1g = w − H−1g.

In this case  Aitken’s acceleration enjoys the same local quadratic convergence as Newton’s method.
This can also be extended to a SGD update rule: w(+1) ← w() −  ∙ g(w(); B())  where the
mini-batch B ⊆ D  ∣B∣ ≪ ∣D∣  is a randomly selected small subset of D. A genuine on-line learner
+ and “∙” denotes
usually has ∣B∣ = 1. We consider a positive vector-valued step-size  ∈ ℝ
component-wise (Hadamard) product of two vectors. Again  by exploiting (4)  since

eig(I − diag()H) = eig(ℳ′) = eig(J) ≈  

where  is an estimated eigenvalue of J as given in (2)  when H is a symmetric matrix  its eigenvalue
is given by

eig(J) = 1 − eig(H) ⇒ eig(H) =

2

1 − eig(J)

.



Therefore  we can update the step size component-wise by

eig(H−1) =



1 − eig(J) ≈



1 −  ⇒ (+1)



∝



()
1 − ()



.

(5)

Since the mapping ℳ in SGD involves the gradient g(w(); B()) of a randomly selected training
example B()  ℳ is itself a random variable. It is unlikely that we can obtain a reliable eigenvalue
estimation at each single iteration. To increase stationary of the mapping  we take advantage of the
law of large numbers and aggregate consecutive SGD mappings into a new mapping

ℳ = ℳ(ℳ(. . . ℳ(w) . . .))
}

{z

|



 

which reduces the variance of gradient estimation by 1
   compared to the plain SGD mapping ℳ.
The approximation is valid because w(+)   = 0  . . .    − 1 are approximately ﬁxed when  is
sufﬁciently small [7].
We can proceed to estimate the eigenvalues of ℳ from w()  w(+) and w(+2) by applying (2)
for each component :

¯
 =

w(+2)

w(+)





− w(+)
− w()



.

(6)

We note that our aggregate mapping ℳ is different from a mapping that takes  mini-batches as the
input in a single iteration. Their difference is similar to that between batch and stochastic gradient
descent. Aggregate mappings have  chances to adjust its search direction  while mappings that use
 mini-batches together only have one.
With the estimated eigenvalues  we can present the complete update rule to adjust the step size
vector . To ensure that the estimated values of eig(J) ∈ (−1  1) and to ensure numerical stability 
 ∣. Let u denote the constrained ¯.
we introduce a positive constant  < 1 as the upper bound of ∣¯
Its components are given by
(7)

 := sgn(¯

 ∣  )  ∀.
Then we can update the step size every 2 iterations based on u by:

 ) min(∣¯

where v is a discount factor with components deﬁned by

(+2+1) = v ∙ (+2) 

 :=

 + 

 +  + 

  ∀.

(8)

(9)

1− >  ≈ 1 +  to ensure
The discount factor is derived from (5) and the fact that when  < 1 
numerical stability  with  and  controlling the range. Let  be the maximum value and  be the
minimum value of . We can obtain  and  by solving  ≤  ≤  for all . Since − ≤  ≤  
we have  =  when  =  and  =  when  = −. Solving these equations yields:

1

 =

 + 
 − 

 and  =

2(1 − )
 − 

.

(10)

For example  if we want to set  = 0.9999 and  = 0.99  then  and  will be 201 and 0.0202 
respectively. Setting 0 <  <  ≤ 1 ensures that the step size is decreasing and approaches zero so
that SGD can be guaranteed to converge [7].

Algorithm 1 shows the PSA algorithm.
In a nutshell  PSA applies SGD with a ﬁxed step size
and periodically updates the step size by approximating Jacobian of the aggregated mapping. The
 ) because the cost of eigenvalue estimation given in (6) is 2 and it
complexity per iteration is ( 
is required for every 2 iterations. That is  PSA updates  after learning from 2 ⋅ B examples.

3

−  and  ← 2(1−)
− 

⊳ Equation (10)

Choose a small batch B() uniformly at random from the set of training examples D
update (+1) ← () −  ∙ g((); B())
if ( + 1) mod 2 = 0 then

Algorithm 1 The PSA Algorithm
1: Given:      < 1 and 
2: Initialize (0) and (0);  ← 0;  ← +
3: repeat
4:
5:
6:





7:


(+)


−(+)
−()

 ← (+2)

update ¯
For all   update  ← sgn(¯
For all   update  ← +
update (+1) ← v ∙ ()
(+1) ← ()

8:
9:
10:
11:
12:
13:
14:
15: until Convergence

end if
 ←  + 1

else

++

 ) min(∣¯

 ∣  )

4 Analysis of PSA

⊳ SGD update
⊳ Update 
⊳ Equation (6)

⊳ Equation (7)
⊳ Equation (9)
⊳ Equation (8)

We analyze the accuracy of ()
J = QΛQ−1 and u be column vectors of Q and v



as an eigenvalue estimate as follows. Let eigen decomposition

 be row vectors of Q−1. Then we have

J =



ࢣ=1

 u v

  

where  is the -th eigenvalue of J. By applying Taylor’s expansion to ℳ  we have

w() − w∗ ≈ J(w(0) − w∗)

w(−1) − w∗ ≈ J−1(w(0) − w∗)

⇒ Δ() = w() − w(−1) ≈ JJ−1(J − I)(w(0) − w∗)

⇒ Δ(+1) = w(+1) − w() ≈

Now let



ࢣ=1



 u v

 J−1(J − I)(w(0) − w∗)

 := e

 u v

 J−1(J − I)(w(0) − w∗) 

where e is the -th column of I. Let Δ be the -th element of Δ and  be the largest eigenvalue
of J such that  ∕= 0. Then
Δ(+1)
Δ()

(/ )/
(/ )/

=1 +1
=1 

 


 ≡

=

.





= ࢣ
ࢣ

 +ࢣ∕=
1 +ࢣ∕=

Therefore  we can conclude that

componentwise rate of convergence.

∙  →  as  → ∞ because ∀  if  ∕= 0 then / ≤ 1.  ≡  is the -th
∙  =  if J is a diagonal matrix. In this case  our approximation is exact. This happens
when there are high percentages of missing data for a Bayesian network model trained
by EM [8] and when features are uncorrelated for training a conditional random ﬁeld
model [9].

∙  is the average of eigenvalues weighted by 

 =   we have  ≈ .

. Since  is usually the largest when

4

When we have the least possible step size (+1) = () for all  mod 2 = 0 in PSA  the
expectation of w() obtained by PSA can be shown to be:

(w()) = w∗ +



Ü=1 − (0)⌊ 
= w∗ + S()(w(0) − w∗).

 ⌋H(w∗; D) (w(0) − w∗)

The rate of convergence is governed by the largest eigenvalue of S(). We now derive a bound of
this eigenvalue.

Theorem 1 Let ℎ be the least eigenvalue of H(w∗; D). The asymptotic rate of convergence of PSA
is bounded by

1 −   .
eig(S()) ≤ exp−(0)ℎ

Proof We can show that

eig(S()) =



Ü=11 − (0)⌊ 

 ⌋ℎ

≤ exp−

ࢣ=1
because for any 0 ≤  < 1  1 −  ≤ −  



(0)ℎ⌊ 

 ⌋ = exp−(0)ℎ

⌊ 

 ⌋



ࢣ=1



0 ≤

Ü=1
 ⌋ ≈⎛
ࢣ=0
⎝

 ⌋

⌊ 

Now  since

we have



ࢣ=1

⌊ 

(1 − ) ≤

− = − ࢣ

=1  .



Ü=1

⎞
⎠ = 

⌊ 

 ⌋

ࢣ=0

 −→



1 − 

when  → ∞ 

eig(S()) ≤ exp−(0)ℎ



ࢣ=1

⌊ 

 ⌋ → exp−(0)ℎ

1 −   when  → ∞.

□

Though this analysis suggests that for rapid convergence to ∗  we should assign  ≈ 1 with a
large  and (0)  it is based on a worst-case scenario and thus insufﬁcient as a practical guideline
for parameter assignment. In practice  we ﬁx (    ) = (0.9999  0.99  0.9) and tune  as follows.
When the training set size ∣D∣ ≫ 2000  set  in the order of 0.5∣D∣/1000 is usually sufﬁcient.
In fact  when 
This setting implies that the step size will be adjusted per ∣D∣/1000 examples.
is in the same order  PSA performs similarly. Consider the following three settings: (    ) =
(10  0.9999  0.99)  (100  0.999  0.9) or (1  0.99999  0.999). They all yield nearly identical single-
pass F-scores for the BaseNP task (see Section 5). The ﬁrst setting was used in this paper. To see
why this is the case  consider the decreasing factor  (see (8) and (9))  which will be conﬁned within
the interval (  ). Assume that  is selected at ransom uniformly  then the mean of  = 0.995
when (  ) = (0.9999  0.99) and  will be decreased by a factor of 0.995 on average in each PSA
update. When  = 10  PSA will update  per 20 examples. After learning from 200 examples  PSA
will decrease  10 times by a combined factor of 0.9511. Similarly  we can obtain that the factors
for the other two settings are 0.95 and 0.9512  respectively  nearly identical.

5

5 Experimental Results

Table 1 shows the tasks chosen for our comparison. The tasks for CRF have been used in compe-
titions and the performance was measured by F-score. Weight for CRF reported here is Number
of features provided by CRF++. Target provides the empirical optimal performance achieved
by batch learners. If PSA accurately approximates 2SGD  then its single-pass performance should
be very close to Target. The target F-score for BioNLP/NLPBA is not >85% as reported in [1]
because it was due to a bug that included true labels as a feature 1.

Table 1: Tasks for the experiments.

Task

Model Training

Test

Tag/Class Weight

Target

Base NP
Chunking
BioNLP/NLPBA
BioCreative 2
LS FD
LS OCR
MNIST [14]

8936
CRF
8936
CRF
18546
CRF
CRF
15000
LSVM 2734900
LSVM 1750000
CNN
60000

2012
2012
3856
5000
2734900
1750000
10000

3
23
11
3
2
2
10

1015662
7448606
5977675
10242972
900
1156
134066

94.0% [10]
93.6% [11]
70.0% [12]
86.5% [13]
3.26%
23.94%
0.99%

5.1 Conditional Random Field

We compared PSA with plain SGD and SMD [1] to evaluate PSA’s performance for training
conditional random ﬁelds (CRF). We implemented PSA by replacing the L-BFGS optimizer in
CRF++ [11]. For SMD  we used the implementation available in the public domain 2. Our SGD
implementation for CRF is from Bottou 3. All the above implementations are revisions of CRF++.
Finally  we ran the original CRF++ with default settings to obtain the performance results of L-
BFGS. We simply used the original parameter settings for SGD and SMD as given in the literature.
For PSA  we used  = 0.9  (  ) = (0.9999  0.99)   = 10  and (0)
 = 0.1  ∀. The batch size
is one for all tasks. These parameters were determined by using a small subset from CoNLL 2000
baseNP and we simply used them for all tasks. All of the experiments reported here for CRF were
ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM.

Table 2 compares SGD variants in terms of the execution time and F-scores achieved after processing
the training examples for a single pass. Since the loss function in CRF training is convex  the
convergence results of L-BFGS can be considered as the empirical minimum. The results show that
single-pass F-scores achieved by PSA are about as good as the empirical minima  suggesting that
PSA has effectively approximated Hessian in CRF training.

Fig. 1 shows the learning curves in terms of the CPU time. Though as expected  plain SGD is the
fastest  it is remarkable that PSA is faster than SMD for all tasks. SMD is supposed to have an edge
here because the mini-batch size for SMD was set to 6 or 8  as speciﬁed in [1]  while PSA used one
for all tasks. But PSA is still faster than SMD partly because PSA can take advantage of the sparsity
trick as plain SGD [15].

5.2 Linear SVM

We also evaluated PSA’s single-pass performance for training linear SVM. It is straightforward to
apply PSA as a primal optimizer for linear SVM. We used two very large data sets: FD (face detec-
tion) and OCR (see Table 1)  from the Pascal large-scale learning challenge in 2008 and compared
the performance of PSA with the state-of-the-art linear SVM solvers: Liblinear 1.33 [16]  the winner
of the challenge  and SvmSgd  from Bottou’s SGD web site. They have been shown to outperform
many well-known linear SVM solvers  such as SVM-perf [17] and Pegasos [15].

1Thanks to Shing-Kit Chan of the Chinese University of Hong Kong for pointing that out.
2Available under LGPL from the following URL: http://sml.nicta.com.au/code/crfsmd/.
3http://leon.bottou.org/projects/sgd.

6

Method (pass)

Base NP

time

F-score

Chunking
time

F-score

BioNLP/NLPBA
F-score

time

BioCreative 2

time

F-score

SGD (1)
SMD (1)
PSA (1)
L-BFGS (batch)

1.15
41.50
16.30
221.17

92.42
91.81
93.31
93.91

13.04
350.00
160.00
8694.40

92.26
91.89
93.16
93.78

12.23
522.00
206.00
20130.00

66.37
66.53
69.41
70.30

3.18
497.71
191.61
1601.50

34.33
69.04
80.79
86.82

Table 2: CPU time in seconds and F-scores achieved after a single pass of CRF training.

94.5

94

93.5

93

92.5

92

91.5

91

90.5

e
r
o
c
s
−
F

90
 
0

50

80

70

60

50

40

30

e
r
o
c
s
−
F

BaseNP

 

PSA

SMD

SGD

L−BFGS

100

Time(sec)
NLPBA04

150

200

 

PSA

SMD

SGD

L−BFGS

e
r
o
c
s
−
F

94.5

94

93.5

93

92.5

92

91.5

91

90.5

90
 
0

90

80

70

60

50

40

e
r
o
c
s
−
F

Chunking

 

PSA

SMD

SGD

L−BFGS

200

400

600

Time(sec)

800

1000

1200

BioCreative 2 GM Task

 

PSA

SMD

SGD

L−BFGS

20
 
0

100

200

400

300
500
Time(sec)

600

700

800

30
 
0

100

200
300
Time(sec)

400

500

Figure 1: Comparison of CPU time; Horizontal lines indicate target F-scores.

We selected L2-regularized logistic regression as the loss function for PSA and Liblinear because
it is twice differentiable. The weight  of the margin error term was set to one. We kept SvmSgd
intact. The experiment was run on an Open-SUSE Linux machine with Intel Xeon E7320 CPU
(2.13GHz) and 64GB RAM. Table 3 shows the results. Again  PSA achieves the best single-pass
accuracy for both tasks. Its test accuracies are very close to that of converged Liblinear. PSA takes
much less time than the other two solvers. PSA (1) is faster than SvmSgd (1) for SVM because
SvmSgd uses the sparsity trick [15]  which speeds up training for sparse data  but otherwise may
slow down. Both data sets we used turn out to be dense  i.e.  with no zero features. We implemented
PSA with the sparsity trick for CRF only but not for SVM and CNN.

Method (pass)

Liblinear converge
Liblinear (1)
SvmSgd (20)
SvmSgd (10)
SvmSgd (1)
PSA (1)

LS FD

accuracy

LS OCR

accuracy

time

time

96.74
91.43
93.78
93.77
93.60
95.10

4648.49
290.58
1135.67
567.68
56.78
30.65

76.06
74.33
-
73.71
73.76
75.68

4454.42
398.00
-
473.35
46.96
25.33

Table 3: Test accuracy rates and elapsed CPU time in seconds by various linear SVM solvers.

7

The parameter settings for PSA are basically the same as those for CRF but with a large period
 = 1250 for FD and 500 for OCR. For FD  the worst accuracy by PSA is 94.66% with  between
250 to 2000. For OCR  the worst is 75.20% with  between 100 to 1000  suggesting that PSA is not
very sensitive to parameter settings.

5.3 Convolutional Neural Network

Approximating Hessian is particularly challenging when the loss function is non-convex. We tested
PSA in such a setting by applying PSA to train a large convolutional neural network for the original
10-class MNIST task (see Table 1). We tried to duplicate the implementation of LeNet described in
[18] in C++. Our implementation  referred to as “LeNet-S”  is a simpliﬁed variant of LeNet-5. The
differences include that the sub-sampling layers in LeNet-S picks only the upper-left value from a
2 × 2 area and abandons the other three. LeNet-S used more maps (50 vs. 16) in the third layer and
less nodes (120 vs. 100) in the ﬁfth layer  due to the difference in the previous sub-sampling layer.
Finally  we did not implement the Gaussian connections in the last layer. We trained LeNet-S by
plain SGD and PSA. The initial  for SGD was 0.7 and decreased by 3 percent per pass. For PSA 
we used  = 0.9  (  ) = (0.99999  0.999)   = 10  (0)
 = 0.5  ∀  and the mini-batch size is
one for all tasks. We also adapted a trick given in [19] which advises that step sizes in the lower
layers should be larger than in the higher layer. Following their trick  the initial step sizes for the
ﬁrst and the third layers were 5 and √2.5 times as large as those for the other layers  respectively.
The experiments were ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM.

Table 4 shows the results. To obtain the empirical optimal error rate of our LeNet-S model  we ran
plain SGD with sufﬁcient passes and obtained 0.99% error rate at convergence  slightly higher than
LeNet-5’s 0.95% [18]. Single-pass performance of PSA with the layer trick is within one percentage
point to the target. Starting from an initial weight closer to the optimum helped improving PSA’s
performance further. We ran SGD 100 passes with randomly selected 10K training examples then
re-started training with PSA using the rest 50K training examples for a single pass. Though PSA did
achieve a better error rate  this is infeasible because it took 4492 seconds to run SGD 100 passes.
Finally  though not directly comparable  we also report the performance of TONGA given in [20] as
a reference. TONGA is a 2SGD method based on natural gradient.

Method (pass)

time

error Method (pass)

time

error

SGD (1)
SGD (140)
TONGA (n/a)

266.77
37336.20
500.00

2.36
0.99
2.00

PSA w/o layer trick (1)
PSA w/ layer trick (1)
PSA re-start (1)

311.95
311.00
253.72

2.31
1.97
1.90

Table 4: CPU time in seconds and percentage test error rates for various neural network trainers.

6 Conclusions

It has been shown that given a sufﬁciently large training set  a single pass of 2SGD generalizes as
well as the empirical optimum. Our results show that PSA provides a practical solution to accom-
plish near optimal performance of 2SGD as predicted theoretically for a variety of large scale models
and tasks with a reasonably low cost per iteration compared to competing 2SGD methods. The ben-
eﬁt of 2SGD with PSA over plain SGD becomes clearer when the scale of the tasks are increasingly
large. For non-convex neural network tasks  since the curvature of the error surface is so complex 
it is still very challenging for an eigenvalue approximation method like PSA. A complete version of
this paper will appear as [21]. Source codes of PSA are available at http://aiia.iis.sinica.edu.tw.

References

[1] S.V.N. Vishwanathan  Nicol N. Schraudolph  Mark W. Schmidt  and Kevin P. Murphy. Accel-
erated training of conditional random ﬁelds with stochastic gradient methods. In Proceedings
of the 23rd International Conference on Machine Learning (ICML’06)  Pittsburgh  PA  USA 
June 2006.

8

[2] Michael Collins  Amir Globerson  Terry Koo  Xavier Carreras  and Peter L. Bartlett. Expo-
nentiated gradient algorithms for conditional random ﬁelds and max-margin markov networks.
Journal of Machine Learning Research  9:1775–1822  August 2008.

[3] Noboru Murata and Shun-Ichi Amari. Statistical analysis of learning dynamics. Signal Pro-

cessing  74(1):3–28  April 1999.

[4] L´eon Bottou and Yann LeCun. On-line learning for very large data sets. Applied Stochastic

Models in Business and Industry  21(2):137–151  2005.

[5] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer  1999.
[6] L´eon Bottou. The tradeoffs of large-scale learning. Tutorial  the 21st Annual Conference
on Neural Information Processing Systems (NIPS 2007)  Vancouver  BC  Canada  December
2007. http://leon.bottou.org/talks/largescale.

[7] Albert Benveniste  Michel Metivier  and Pierre Priouret. Adaptive Algorithms and Stochastic

Approximations. Springer-Verlag  1990.

[8] Chun-Nan Hsu  Han-Shen Huang  and Bo-Hou Yang. Global and componentwise extrapola-
tion for accelerating data mining from large incomplete data sets with the EM algorithm. In
Proceedings of the Sixth IEEE International Conference on Data Mining (ICDM’06)  pages
265–274  Hong Kong  China  December 2006.

[9] Han-Shen Huang  Bo-Hou Yang  Yu-Ming Chang  and Chun-Nan Hsu. Global and componen-
twise extrapolations for accelerating training of Bayesian networks and conditional random
ﬁelds. Data Mining and Knowledge Discovery  19(1):58–91  2009.

[10] Fei Sha and Fernando Pereira. Shallow parsing with conditional random ﬁelds. In Proceedings
of Human Language Technology  the North American Chapter of the Association for Compu-
tational Linguistics (NAACL’03)  pages 213–220  2003.

[11] Taku Kudo. CRF++: Yet another CRF toolkit  2006. Available under LGPL from the following

URL: http://crfpp.sourceforge.net/.

[12] Burr Settles. Biomedical named entity recognition using conditional random ﬁelds and novel
In Proceedings of the Joint Workshop on Natural Language Processing in

feature sets.
Biomedicine and its Applications (JNLPBA-2004)  pages 104–107  2004.

[13] Cheng-Ju Kuo  Yu-Ming Chang  Han-Shen Huang  Kuan-Ting Lin  Bo-Hou Yang  Yu-Shi
Lin  Chun-Nan Hsu  and I-Fang Chung. Rich feature set  uniﬁcation of bidirectional parsing
and dictionary ﬁltering for high f-score gene mention tagging. In Proceedings of the Second
BioCreative Challenge Evaluation Workshop  pages 105–107  2007.

[14] Yann LeCun and Corinna Cortes.

The MNIST database of handwritten digits  1998.

http://yann.lecun.com/exdb/mnist/.

[15] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal Estimated sub-
GrAdient SOlver for SVM. In ICML’07: Proceedings of the 24th international conference on
Machine learning  pages 807–814  New York  NY  USA  2007. ACM Press.

[16] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines  2001.

Software available at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[17] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’06) 
pages 217–226  New York  NY  USA  2006. ACM.

[18] Yann LeCun  L´eon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[19] Yann LeCun  Leon Bottou  Genevieve B. Orr  and Klaus-Robert Muller. Efﬁcient backprop.

In G. Orr and Muller K.  editors  Neural Networks: Tricks of the trade. Springer  1998.

[20] Nicolas LeRoux  Pierre-Antoine Manzagol  and Yoshua Bengio. Topmoumoute online natural
gradient algorithm. In Advances in Neural Information Processing Systems  20 (NIPS 2007) 
Cambridge  MA  USA  2008. MIT Press.

[21] Chun-Nan Hsu  Yu-Ming Chang  Han-Shen Huang  and Yuh-Jye Lee. Periodic step-size adap-
tation in second-order gradient descent for single-pass on-line structured learning. To appear in
Mchine Learning  Special Issue on Structured Prediction. DOI: 10.1007/s10994-009-5142-6 
2009.

9

,Sergei Vassilvitskii