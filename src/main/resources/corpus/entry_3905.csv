2016,On Multiplicative Integration with Recurrent Neural Networks,We introduce a general simple structural design called “Multiplicative Integration” (MI) to improve recurrent neural networks (RNNs). MI changes the way of how the information flow gets integrated in the computational building block of an RNN  while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models  including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.,On Multiplicative Integration with

Recurrent Neural Networks

Yuhuai Wu1 ∗  Saizheng Zhang2 ∗  Ying Zhang2  Yoshua Bengio2 4 and Ruslan Salakhutdinov3 4

1University of Toronto  2MILA  Université de Montréal  3Carnegie Mellon University  4CIFAR
ywu@cs.toronto.edu 2{firstname.lastname}@umontreal.ca rsalakhu@cs.cmu.edu

Abstract

We introduce a general and simple structural design called “Multiplicative Integra-
tion” (MI) to improve recurrent neural networks (RNNs). MI changes the way in
which information from difference sources ﬂows and is integrated in the compu-
tational building block of an RNN  while introducing almost no extra parameters.
The new structure can be easily embedded into many popular RNN models  includ-
ing LSTMs and GRUs. We empirically analyze its learning behaviour and conduct
evaluations on several tasks using different RNN models. Our experimental results
demonstrate that Multiplicative Integration can provide a substantial performance
boost over many of the existing RNN models.

1

Introduction

Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs)
[1  2  3]. Most of these designs are derived from popular structures including vanilla RNNs  Long
Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5]. Despite of their
varying characteristics  most of them share a common computational building block  described by the
following equation:
(1)
where x ∈ Rn and z ∈ Rm are state vectors coming from different information sources  W ∈ Rd×n
and U ∈ Rd×m are state-to-state transition matrices  and b is a bias vector. This computational
building block serves as a combinator for integrating information ﬂow from the x and z by a sum
operation “+”  followed by a nonlinearity φ. We refer to it as the additive building block. Additive
building blocks are widely implemented in various state computations in RNNs (e.g. hidden state
computations for vanilla-RNNs  gate/cell computations of LSTMs and GRUs.
In this work  we propose an alternative design for constructing the computational building block by
changing the procedure of information integration. Speciﬁcally  instead of utilizing sum operation
“+"  we propose to use the Hadamard product “(cid:12)” to fuse Wx and Uz:

φ(Wx + Uz + b) 

φ(Wx (cid:12) Uz + b)

(2)
The result of this modiﬁcation changes the RNN from ﬁrst order to second order [6]  while introducing
no extra parameters. We call this kind of information integration design a form of Multiplicative
Integration. The effect of multiplication naturally results in a gating type structure  in which Wx
and Uz are the gates of each other. More speciﬁcally  one can think of the state-to-state computation
Uz (where for example z represents the previous state) as dynamically rescaled by Wx (where
for example x represents the input). Such rescaling does not exist in the additive building block  in
which Uz is independent of x. This relatively simple modiﬁcation brings about advantages over the
additive building block as it alters RNN’s gradient properties  which we discuss in detail in the next
section  as well as verify through extensive experiments.

∗Equal contribution.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In the following sections  we ﬁrst introduce a general formulation of Multiplicative Integration. We
then compare it to the additive building block on several sequence learning tasks  including character
level language modelling  speech recognition  large scale sentence representation learning using a
Skip-Thought model  and teaching a machine to read and comprehend for a question answering
task. The experimental results (together with several existing state-of-the-art models) show that
various RNN structures (including vanilla RNNs  LSTMs  and GRUs) equipped with Multiplicative
Integration provide better generalization and easier optimization. Its main advantages include: (1) it
enjoys better gradient properties due to the gating effect. Most of the hidden units are non-saturated;
(2) the general formulation of Multiplicative Integration naturally includes the regular additive
building block as a special case  and introduces almost no extra parameters compared to the additive
building block; and (3) it is a drop-in replacement for the additive building block in most of the
popular RNN models  including LSTMs and GRUs. It can also be combined with other RNN training
techniques such as Recurrent Batch Normalization [7]. We further discuss its relationship to existing
models  including Hidden Markov Models (HMMs) [8]  second order RNNs [6] and Multiplicative
RNNs [9].

2 Structure Description and Analysis
2.1 General Formulation of Multiplicative Integration

The key idea behind Multiplicative Integration is to integrate different information ﬂows Wx and Uz 
by the Hadamard product “(cid:12)”. A more general formulation of Multiplicative Integration includes
two more bias vectors β1 and β2 added to Wx and Uz:

φ((Wx + β1) (cid:12) (Uz + β2) + b)

(3)
where β1  β2 ∈ Rd are bias vectors. Notice that such formulation contains the ﬁrst order terms as
in a additive building block  i.e.  β1 (cid:12) Uht−1 + β2 (cid:12) Wxt. In order to make the Multiplicative
Integration more ﬂexible  we introduce another bias vector α ∈ Rd to gate2 the term Wx (cid:12) Uz 
obtaining the following formulation:

φ(α (cid:12) Wx (cid:12) Uz + β1 (cid:12) Uz + β2 (cid:12) Wx + b) 

(4)
Note that the number of parameters of the Multiplicative Integration is about the same as that of the
additive building block  since the number of new parameters (α  β1 and β2) are negligible compared
to total number of parameters. Also  Multiplicative Integration can be easily extended to LSTMs
and GRUs3  that adopt vanilla building blocks for computing gates and output states  where one can
directly replace them with the Multiplicative Integration. More generally  in any kind of structure
where k information ﬂows (k ≥ 2) are involved (e.g. residual networks [10])  one can implement
pairwise Multiplicative Integration for integrating all k information sources.

2.2 Gradient Properties
The Multiplicative Integration has different gradient properties compared to the additive building
block. For clarity of presentation  we ﬁrst look at vanilla-RNN and RNN with Multiplicative
Integration embedded  referred to as MI-RNN. That is  ht = φ(Wxt + Uht−1 + b) versus
ht = φ(Wxt (cid:12) Uht−1 + b). In a vanilla-RNN  the gradient
can be computed as follows:

∂ht

∂ht−n

UT diag(φ(cid:48)

k) 

(5)

t(cid:89)

∂ht
∂ht−n

=

k=t−n+1

where φ(cid:48)
k = φ(cid:48)(Wxk + Uhk−1 + b). The equation above shows that the gradient ﬂow through time
heavily depends on the hidden-to-hidden matrix U  but W and xk appear to play a limited role: they
only come in the derivative of φ(cid:48) mixed with Uhk−1. On the other hand  the gradient
of a
MI-RNN is4:

∂ht−n

∂ht

UT diag(Wxk)diag(φ(cid:48)

k) 

(6)

t(cid:89)

∂ht
∂ht−n

=

k=t−n+1

2If α = 0  the Multiplicative Integration will degenerate to the vanilla additive building block.
3See exact formulations in the Appendix.
4Here we adopt the simplest formulation of Multiplicative Integration for illustration. In the more general

case (Eq. 4)  diag(Wxk) in Eq. 6 will become diag(α (cid:12) Wxk + β1).

2

k = φ(cid:48)(Wxk (cid:12) Uhk−1 + b). By looking at the gradient  we see that the matrix W and
where φ(cid:48)
the current input xk is directly involved in the gradient computation by gating the matrix U  hence
more capable of altering the updates of the learning system. As we show in our experiments  with
Wxk directly gating the gradient  the vanishing/exploding problem is alleviated: Wxk dynamically
reconciles U  making the gradient propagation easier compared to the regular RNNs. For LSTMs
and GRUs with Multiplicative Integration  the gradient propagation properties are more complicated.
But in principle  the beneﬁts of the gating effect also persists in these models.
3 Experiments
In all of our experiments  we use the general form of Multiplicative Integration (Eq. 4) for any hidden
state/gate computations  unless otherwise speciﬁed.
3.1 Exploratory Experiments
To further understand the functionality of Multiplicative Integration  we take a simple RNN for
illustration  and perform several exploratory experiments on the character level language modeling
task using Penn-Treebank dataset [11]  following the data partition in [12]. The length of the
training sequence is 50. All models have a single hidden layer of size 2048  and we use Adam
optimization algorithm [13] with learning rate 1e−4. Weights are initialized to samples drawn from
uniform[−0.02  0.02]. Performance is evaluated by the bits-per-character (BPC) metric  which is
log2 of perplexity.
3.1.1 Gradient Properties
To analyze the gradient ﬂow of the model  we divide the gradient in Eq. 6 into two parts: 1. the
gated matrix products: UT diag(Wxk)  and 2. the derivative of the nonlinearity φ(cid:48)  We separately
analyze the properties of each term compared to the additive building block. We ﬁrst focus on the
gating effect brought by diag(Wxk). In order to separate out the effect of nonlinearity  we chose φ
to be the identity map  hence both vanilla-RNN and MI-RNN reduce to linear models  referred to as
lin-RNN and lin-MI-RNN.
For each model we monitor the log-L2-norm of the gradient log ||∂C/∂ht||2 (averaged over the
training set) after every training epoch  where ht is the hidden state at time step t  and C is the
negative log-likelihood of the single character prediction at the ﬁnal time step (t = 50). Figure. 1
shows the evolution of the gradient norms for small t  i.e.  0  5  10  as they better reﬂect the gradient
propagation behaviour. Observe that the norms of lin-MI-RNN (orange) increase rapidly and soon
exceed the corresponding norms of lin-RNN by a large margin. The norms of lin-RNN stay close to
zero (≈ 10−4) and their changes over time are almost negligible. This observation implies that with
the help of diag(Wxk) term  the gradient vanishing of lin-MI-RNN can be alleviated compared to
lin-RNN. The ﬁnal test BPC (bits-per-character) of lin-MI-RNN is 1.48  which is comparable to a
vanilla-RNN with stabilizing regularizer [14]  while lin-RNN performs rather poorly  achieving a test
BPC of over 2.
Next we look into the nonlinearity φ. We chose φ = tanh for both vanilla-RNN and MI-RNN.
Figure 1 (c) and (d) shows a comparison of histograms of hidden activations over all time steps on
the validation set after training. Interestingly  in (c) for vanilla-RNN  most activations are saturated
with values around ±1  whereas in (d) for MI-RNN  most activations are non-saturated with values
around 0. This has a direct consequence in gradient propagation: non-saturated activations imply
k) ≈ 1 for φ = tanh  which can help gradients propagate  whereas saturated activations
that diag(φ(cid:48)
imply that diag(φ(cid:48)
3.1.2 Scaling Problem
When adding two numbers at different order of magnitude  the smaller one might be negligible for the
sum. However  when multiplying two numbers  the value of the product depends on both regardless
of the scales. This principle also applies when comparing Multiplicative Integration to the additive
building blocks. In this experiment  we test whether Multiplicative Integration is more robust to the
scales of weight values. Following the same models as in Section 3.1.1  we ﬁrst calculated the norms
of Wxk and Uhk−1 for both vanilla-RNN and MI-RNN for different k after training. We found that
in both structures  Wxk is a lot smaller than Uhk−1 in magnitude. This might be due to the fact that
xk is a one-hot vector  making the number of updates for (columns of) W be smaller than U. As a
result  in vanilla-RNN  the pre-activation term Wxk + Uhk−1 is largely controlled by the value of
Uhk−1  while Wxk becomes rather small. In MI-RNN  on the other hand  the pre-activation term
Wxk (cid:12) Uhk−1 still depends on the values of both Wxk and Uhk−1  due to multiplication.

k) ≈ 0  resulting in gradients vanishing.

3

Figure 1: (a) Curves of log-L2-norm of gradients for lin-RNN (blue) and lin-MI-RNN (orange). Time gradually
changes from {1  5  10}. (b) Validation BPC curves for vanilla-RNN  MI-RNN-simple using Eq. 2  and MI-
RNN-general using Eq. 4. (c) Histogram of vanilla-RNN’s hidden activations over the validation set  most
activations are saturated. (d) Histogram of MI-RNN’s hidden activations over the validation set  most activations
are not saturated.

We next tried different initialization of W and U to test their sensitivities to the scaling. For each
model  we ﬁx the initialization of U to uniform[−0.02  0.02] and initialize W to uniform[−rW  rW]
where rW varies in {0.02  0.1  0.3  0.6}. Table 1  top left panel  shows results. As we increase
the scale of W  performance of the vanilla-RNN improves  suggesting that the model is able to
better utilize the input information. On the other hand  MI-RNN is much more robust to different
initializations  where the scaling has almost no effect on the ﬁnal performance.

3.1.3 On different choices of the formulation
In our third experiment  we evaluated the performance of different computational building blocks 
which are Eq. 1 (vanilla-RNN)  Eq. 2 (MI-RNN-simple) and Eq. 4 (MI-RNN-general)5. From the
validation curves in Figure 1 (b)  we see that both MI-RNN  simple and MI-RNN-general yield much
better performance compared to vanilla-RNN  and MI-RNN-general has a faster convergence speed
compared to MI-RNN-simple. We also compared our results to the previously published models
in Table 1  bottom left panel  where MI-RNN-general achieves a test BPC of 1.39  which is to our
knowledge the best result for RNNs on this task without complex gating/cell mechanisms.

3.2 Character Level Language Modeling
In addition to the Penn-Treebank dataset  we also perform character level language modeling on two
larger datasets: text86 and Hutter Challenge Wikipedia7. Both of them contain 100M characters from
Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet
size of 205. For both datasets  we follow the training protocols in [12] and [1] respectively. We use
Adam for optimization with the starting learning rate grid-searched in {0.002  0.001  0.0005}. If the
validation BPC (bits-per-character) does not decrease for 2 epochs  we half the learning rate.
We implemented Multiplicative Integration on both vanilla-RNN and LSTM  referred to as MI-
RNN and MI-LSTM. The results for the text8 dataset are shown in Table 1  bottom middle panel.
All ﬁve models  including some of the previously published models  have the same number of

5We perform hyper-parameter search for the initialization of {α  β1  β2  b} in MI-RNN-general.
6http://mattmahoney.net/dc/textdata
7http://prize.hutter1.net/

4

510152025number of epochs−7−6−5−4−3−2−1log||dC / dh_t||_2(a)lin-RNN  t=0lin-RNN  t=5lin-RNN  t=10lin-MI-RNN  t=0lin-MI-RNN  t=5lin-MI-RNN  t=100510152025number of epochs1.51.82.12.42.73.0validation BPC(b)vanilla-RNNMI-RNN-simpleMI-RNN-general−1.0−0.50.00.51.0activation values of h_t0.00.10.20.30.40.5normalized fequency(d)−1.0−0.50.00.51.0activation values of h_t0.000.020.040.060.080.100.12normalized fequency(d)rW = 0.02 0.1 0.3 0.6
RNN

std
1.69 1.65 1.57 1.54 0.06
MI-RNN 1.39 1.40 1.40 1.41 0.008

WSJ Corpus
CER WER
DRNN+CTCbeamsearch [15]
10.0 14.1
Encoder-Decoder [16]
9.3
6.4
LSTM+CTCbeamsearch [17]
8.7
9.2
7.3
Eesen [18]
-
LSTM+CTC+WFST (ours)
6.5
8.7
MI-LSTM+CTC+WFST (ours) 6.0
8.2

Penn-Treebank
RNN [12]
HF-MRNN [12]
RNN+stabalization [14]
MI-RNN (ours)
linear MI-RNN (ours)

BPC
1.42
1.41
1.48
1.39
1.48

text8
RNN+smoothReLu [19]
HF-MRNN [12]
MI-RNN (ours)
LSTM (ours)
MI-LSTM(ours)

BPC
1.55
1.54
1.52
1.51
1.44

HutterWikipedia
stacked-LSTM [20]
GF-LSTM [1]
grid-LSTM [2]
MI-LSTM (ours)

BPC
1.67
1.58
1.47
1.44

Table 1: Top: test BPCs and the standard deviation of models with different scales of weight initializations. Top
right: test CERs and WERs on WSJ corpus. Bottom left: test BPCs on character level Penn-Treebank dataset.
Bottom middle: test BPCs on character level text8 dataset. Bottom right: test BPCs on character level Hutter
Prize Wikipedia dataset.
parameters (≈4M). For RNNs without complex gating/cell mechanisms (the ﬁrst three results)  our
MI-RNN (with {α  β1  β2  b} initialized as {2  0.5  0.5  0}) performs the best  our MI-LSTM (with
{α  β1  β2  b} initialized as {1  0.5  0.5  0}) outperforms all other models by a large margin8.
On Hutter Challenge Wikipedia dataset  we compare our MI-LSTM (single layer with 2048 unit 
≈17M  with {α  β1  β2  b} initialized as {1  1  1  0}) to the previous stacked LSTM (7 layers 
≈27M) [20]  GF-LSTM (5 layers  ≈20M) [1]  and grid-LSTM (6 layers  ≈17M) [2]. Table 1  bottom
right panel  shows results. Despite the simple structure compared to the sophisticated connection
designs in GF-LSTM and grid-LSTM  our MI-LSTM outperforms all other models and achieves the
new state-of-the-art on this task.

3.3 Speech Recognition

We next evaluate our models on Wall Street Journal (WSJ) corpus (available as LDC corpus
LDC93S6B and LDC94S13B)  where we use the full 81 hour set “si284” for training  set “dev93” for
validation and set “eval92” for test. We follow the same data preparation process and model setting
as in [18]  and we use 59 characters as the targets for the acoustic modelling. Decoding is done with
the CTC [21] based weighted ﬁnite-state transducers (WFSTs) [22] as proposed by [18].
Our model (referred to as MI-LSTM+CTC+WFST) consists of 4 bidirectional MI-LSTM lay-
ers  each with 320 units for each direction. CTC is performed on top to resolve the alignment
issue in speech transcription. For comparison  we also train a baseline model (referred to as
LSTM+CTC+WFST) with the same size but using vanilla LSTM. Adam with learning rate 0.0001
is used for optimization and Gaussian weight noise with zero mean and 0.05 standard deviation
is injected for regularization. We evaluate our models on the character error rate (CER) without
language model and the word error rate (WER) with extended trigram language model.
Table 1  top right panel  shows that MI-LSTM+CTC+WFST achieves quite good results on both CER
and WER compared to recent works  and it has a clear improvement over the baseline model. Note
that we did not conduct a careful hyper-parameter search on this task  hence one could potentially
obtain better results with better decoding schemes and regularization techniques.

3.4 Learning Skip-Thought Vectors

Next  we evaluate our Multiplicative Integration on the Skip-Thought model of [23]. Skip-Thought is
an encoder-decoder model that attempts to learn generic  distributed sentence representations. The
model produces sentence representation that are robust and perform well in practice  as it achieves
excellent results across many different NLP tasks. The model was trained on the BookCorpus dataset
that consists of 11 038 books with 74 004 228 sentences. Not surprisingly  a single pass through

8[7] reports better results but they use much larger models (≈16M) which is not directly comparable.

5

Semantic-Relatedness
uni-skip [23]
bi-skip [23]
combine-skip [23]
uni-skip (ours)
MI-uni-skip (ours)

r

ρ MSE
0.8477 0.7780 0.2872
0.8405 0.7696 0.2995
0.8584 0.7916 0.2687
0.8436 0.7735 0.2946
0.8588 0.7952 0.2679

Paraphrase detection Acc F1
73.0 81.9
uni-skip [23]
71.2 81.2
bi-skip [23]
73.0 82.0
combine-skip [23]
74.0 81.9
uni-skip (ours)
74.0 82.1
MI-uni-skip (ours)

MR CR SUBJ MPQA
Classiﬁcation
75.5 79.3 92.1
86.9
uni-skip [23]
83.3
bi-skip [23]
73.9 77.9 92.5
combine-skip [23] 76.5 80.1 93.6
87.1
87.0
uni-skip (ours)
75.9 80.1 93.0
MI-uni-skip (ours) 77.9 82.3 93.3
88.1

Attentive Reader
LSTM [7]
BN-LSTM [7]
BN-everywhere [7]
LSTM (ours)
MI-LSTM (ours)
MI-LSTM+BN (ours)
MI-LSTM+BN-everywhere (ours)

Val. Err.
0.5033
0.4951
0.5000
0.5053
0.4721
0.4685
0.4644

Table 2: Top left: skip-thought+MI on Semantic-Relatedness task. Top Right: skip-thought+MI on Paraphrase
Detection task. Bottom left: skip-thought+MI on four different classiﬁcation tasks. Bottom right: Multiplicative
Integration (with batch normalization) on Teaching Machines to Read and Comprehend task.

the training data can take up to a week on a high-end GPU (as reported in [23]). Such training
speed largely limits one to perform careful hyper-parameter search. However  with Multiplicative
Integration  not only the training time is shortened by a factor of two  but the ﬁnal performance is
also signiﬁcantly improved.
We exactly follow the authors’ Theano implementation of the skip-thought model9: Encoder and
decoder are single-layer GRUs with hidden-layer size of 2400; all recurrent matrices adopt orthogonal
initialization while non-recurrent weights are initialized from uniform distribution. Adam is used
for optimization. We implemented Multiplicative Integration only for the encoder GRU (embedding
MI into decoder did not provide any substantial gains). We refer our model as MI-uni-skip  with
{α  β1  β2  b} initialized as {1  1  1  0}. We also train a baseline model with the same size  referred
to as uni-skip(ours)  which essentially reproduces the original model of [23].
During the course of training  we evaluated the skip-thought vectors on the semantic relatedness
task  using SICK dataset  every 2500 updates for both MI-uni-skip and the baseline model (each
iteration processes a mini-batch of size 64). The results are shown in Figure 2a. Note that MI-uni-skip
signiﬁcantly outperforms the baseline  not only in terms of speed of convergence  but also in terms
of ﬁnal performance. At around 125k updates  MI-uni-skip already exceeds the best performance
achieved by the baseline  which takes about twice the number of updates.
We also evaluated both models after one week of training  with the best results being reported on six
out of eight tasks reported in [23]: semantic relatedness task on SICK dataset  paraphrase detection
task on Microsoft Research Paraphrase Corpus  and four classiﬁcation benchmarks: movie review
sentiment (MR)  customer product reviews (CR)  subjectivity/objectivity classiﬁcation (SUBJ)  and
opinion polarity (MPQA). We also compared our results with the results reported on three models in
the original skip-thought paper: uni-skip  bi-skip  combine-skip. Uni-skip is the same model as our
baseline  bi-skip is a bidirectional model of the same size  and combine-skip takes the concatenation
of the vectors from uni-skip and bi-skip to form a 4800 dimension vector for task evaluation. Table
2 shows that MI-uni-skip dominates across all the tasks. Not only it achieves higher performance
than the baseline model  but in many cases  it also outperforms the combine-skip model  which has
twice the number of dimensions. Clearly  Multiplicative Integration provides a faster and better way
to train a large-scale Skip-Thought model.

3.5 Teaching Machines to Read and Comprehend

In our last experiment  we show that the use of Multiplicative Integration can be combined with
other techniques for training RNNs  and the advantages of using MI still persist. Recently  [7]
introduced Recurrent Batch-Normalization. They evaluated their proposed technique on a uni-

9https://github.com/ryankiros/skip-thoughts

6

Figure 2: (a) MSE curves of uni-skip (ours) and MI-uni-skip (ours) on semantic relatedness task on SICK
dataset. MI-uni-skip signiﬁcantly outperforms baseline uni-skip. (b) Validation error curves on attentive reader
models. There is a clear margin between models with and without MI.

directional Attentive Reader Model [24] for the question answering task using the CNN corpus10. To
test our approach  we evaluated the following four models: 1. A vanilla LSTM attentive reader model
with a single hidden layer size 240 (same as [7]) as our baseline  referred to as LSTM (ours)  2. A
multiplicative integration LSTM with a single hidden size 240  referred to as MI-LSTM  3. MI-
LSTM with Batch-Norm  referred to as MI-LSTM+BN  4. MI-LSTM with Batch-Norm everywhere
(as detailed in [7])  referred to as MI-LSTM+BN-everywhere. We compared our models to results
reported in [7] (referred to as LSTM  BN-LSTM and BN-LSTM everywhere) 11.
For all MI models  {α  β1  β2  b} were initialized to {1  1  1  0}. We follow the experimental
protocol of [7]12 and use exactly the same settings as theirs  except we remove the gradient clipping
for MI-LSTMs. Figure. 2b shows validation curves of the baseline (LSTM)  MI-LSTM  BN-LSTM 
and MI-LSTM+BN  and the ﬁnal validation errors of all models are reported in Table 2  bottom right
panel. Clearly  using Multiplicative Integration results in improved model performance regardless
of whether Batch-Norm is used. However  the combination of MI and Batch-Norm provides the
best performance and the fastest speed of convergence. This shows the general applicability of
Multiplication Integration when combining it with other optimization techniques.

4 Relationship to Previous Models
4.1 Relationship to Hidden Markov Models

One can show that under certain constraints  MI-RNN is effectively implementing the forward
algorithm of the Hidden Markov Model(HMM). A direct mapping can be constructed as follows (see
[25] for a similar derivation). Let U ∈ Rm×m be the state transition probability matrix with Uij =
Pr[ht+1 = i|ht = j]  W ∈ Rm×n be the observation probability matrix with Wij = Pr[xt =
i|ht = j]. When xt is a one-hot vector (e.g.  in many of the language modelling tasks)  multiplying
it by W is effectively choosing a column of the observation matrix. Namely  if the jth entry of xt
is one  then Wxt = Pr[xt|ht = j]. Let h0 be the initial state distribution with h0 = Pr[h0] and
{ht}t≥1 be the alpha values in the forward algorithm of HMM  i.e.  ht = Pr[x1  ...  xt  ht]. Then
Uht = Pr[x1  ...  xt  ht+1]. Thus ht+1 = Wxt+1 (cid:12) Uht = Pr[xt+1|ht+1]· Pr[x1  ...  xt  ht+1] =
Pr[x1  ...  xt+1  ht+1]. To exactly implement the forward algorithm using Multiplicative Integration 
the matrices W and U have to be probability matrices  and xt needs to be a one-hot vector. The
function φ needs to be linear  and we drop all the bias terms. Therefore  RNN with Multiplicative
Integration can be seen as a nonlinear extension of HMMs. The extra freedom in parameter values
and nonlinearity makes the model more ﬂexible compared to HMMs.

4.2 Relations to Second Order RNNs and Multiplicative RNNs

MI-RNN is related to the second order RNN [6] and the multiplicative RNN (MRNN) [9]. We ﬁrst
describe the similarities with these two models:
The second order RNN involves a second order term st in a vanilla-RNN  where the ith element
t T (i)ht−1  where T (i) ∈ Rn×m(1 ≤ i ≤ m) is
st i is computed by the bilinear form: st i = xT

10Note that [7] used a truncated version of the original dataset in order to save computation.
11Learning curves and the ﬁnal result number are obtained by emails correspondence with authors of [7].
12https://github.com/cooijmanstim/recurrent-batch-normalization.git.

7

050100150200250number of iterations (2.5k)0.260.280.300.320.340.36MSE(a)uni-skip (ours)MI-uni-skip (ours)0200400600800number of iterations (1k)0.450.500.550.600.650.70validation error(b)LSTM [7]BN-LSTM [7]MI-LSTM (ours)MI-LSTM+BN (ours)approximates T by a tensor decomposition(cid:80) x(i)

the ith slice of a tensor T ∈ Rm×n×m. Multiplicative Integration also involve a second order term
st = α (cid:12) Wxt (cid:12) Uht−1  but in our case st i = αi(wi · xt)(ui · ht−1) = xT
t (αwi ⊗ ui)ht−1 
where wi and ui are ith row in W and U  and αi is the ith element of α. Note that the outer product
αiwi ⊗ ui is a rank-1 matrix. The Multiplicative RNN is also a second order RNN  but which
t T (i) = Pdiag(Vxt)Q. For MI-RNN  we can
also think of the second order term as a tensor decomposition: α(cid:12) Wxt (cid:12) Uht−1 = U(xt)ht−1 =
[diag(α)diag(Wxt)U]ht−1.
There are however several differences that make MI a favourable model: (1) Simpler Parametrization:
MI uses a rank-1 approximation compared to the second order RNNs  and a diagonal approximation
compared to Multiplicative RNN. Moreover  MI-RNN shares parameters across the ﬁrst and second
order terms  whereas the other two models do not. As a result  the number of parameters are largely
reduced  which makes our model more practical for large scale problems  while avoiding overﬁtting.
(2) Easier Optimization: In tensor decomposition methods  the products of three different (low-rank)
matrices generally makes it hard to optimize [9]. However  the optimization problem becomes
easier in MI  as discussed in section 2 and 3. (3) General structural design vs. vanilla-RNN design:
Multiplicative Integration can be easily embedded in many other RNN structures  e.g. LSTMs and
GRUs  whereas the second order RNN and MRNN present a very speciﬁc design for modifying
vanilla-RNNs.
Moreover  we also compared MI-RNN’s performance to the previous HF-MRNN’s results (Multi-
plicative RNN trained by Hessian-free method) in Table 1  bottom left and bottom middle panels  on
Penn-Treebank and text8 datasets. One can see that MI-RNN outperforms HF-MRNN on both tasks.

4.3 General Multiplicative Integration

Multiplicative Integration can be viewed as a general way of combining information ﬂows from
two different sources. In particular  [26] proposed the ladder network that achieves promising
results on semi-supervised learning. In their model  they combine the lateral connections and the
backward connections via the “combinator” function by a Hadamard product. The performance would
severely degrade without this product as empirically shown by [27]. [28] explored neural embedding
approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions 
and compared a variety of embedding models on the link prediction task. Surprisingly  the best
results among all bilinear functions is the simple weighted Hadamard product. They further carefully
compare the multiplicative and additive interactions and show that the multiplicative interaction
dominates the additive one.
5 Conclusion

In this paper we proposed to use Multiplicative Integration (MI)  a simple Hadamard product to
combine information ﬂow in recurrent neural networks. MI can be easily integrated into many popular
RNN models  including LSTMs and GRUs  while introducing almost no extra parameters. Indeed 
the implementation of MI requires almost no extra work beyond implementing RNN models. We also
show that MI achieves state-of-the-art performance on four different tasks or 11 datasets of varying
sizes and scales. We believe that the Multiplicative Integration can become a default building block
for training various types of RNN models.

Acknowledgments

The authors acknowledge the following agencies for funding and support: NSERC  Canada Research
Chairs  CIFAR  Calcul Quebec  Compute Canada  Disney research and ONR Grant N000141310721.
The authors thank the developers of Theano [29] and Keras [30]  and also thank Jimmy Ba for many
thought-provoking discussions.

References
[1] Junyoung Chung  Caglar Gulcehre  Kyunghyun Cho  and Yoshua Bengio. Gated feedback recurrent neural

networks. arXiv preprint arXiv:1502.02367  2015.

[2] Nal Kalchbrenner  Ivo Danihelka  and Alex Graves. Grid long short-term memory. arXiv preprint

arXiv:1507.01526  2015.

8

[3] Saizheng Zhang  Yuhuai Wu  Tong Che  Zhouhan Lin  Roland Memisevic  Ruslan Salakhutdinov 
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. arXiv preprint
arXiv:1602.08210  2016.

[4] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[5] Kyunghyun Cho  Bart Van Merriënboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares  Holger
Schwenk  and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078  2014.

[6] Mark W Goudreau  C Lee Giles  Srimat T Chakradhar  and D Chen. First-order versus second-order

single-layer recurrent neural networks. Neural Networks  IEEE Transactions on  5(3):511–513  1994.

[7] Tim Cooijmans  Nicolas Ballas  César Laurent  and Aaron Courville. Recurrent batch normalization.

http://arxiv.org/pdf/1603.09025v4.pdf  2016.

[8] LE Baum and JA Eagon. An inequality with application to statistical estimation for probabilistic functions
of markov processes and to a model for ecology. Bulletin of the American Mathematical Society  73:360–
363  1967.

[9] Ilya Sutskever  James Martens  and Geoffrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  pages 1017–1024 
2011.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385  2015.

[11] Mitchell P Marcus  Mary Ann Marcinkiewicz  and Beatrice Santorini. Building a large annotated corpus

of english: The penn treebank. Computational linguistics  19(2):313–330  1993.

[12] Tomáš Mikolov  Ilya Sutskever  Anoop Deoras  Hai-Son Le  and Stefan Kombrink. Subword language

modeling with neural networks. preprint  (http://www.ﬁt.vutbr.cz/imikolov/rnnlm/char.pdf)  2012.

[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

arXiv:1511.08400  2015.

[14] David Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv preprint

[15] Awni Y Hannun  Andrew L Maas  Daniel Jurafsky  and Andrew Y Ng. First-pass large vocabulary
continuous speech recognition using bi-directional recurrent dnns. arXiv preprint arXiv:1408.2873  2014.
[16] Dzmitry Bahdanau  Jan Chorowski  Dmitriy Serdyuk  Philemon Brakel  and Yoshua Bengio. End-to-end

attention-based large vocabulary speech recognition. arXiv preprint arXiv:1508.04395  2015.

[17] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks.
In Proceedings of the 31st International Conference on Machine Learning (ICML-14)  pages 1764–1772 
2014.

[18] Yajie Miao  Mohammad Gowayyed  and Florian Metze. Eesen: End-to-end speech recognition using deep

rnn models and wfst-based decoding. arXiv preprint arXiv:1507.08240  2015.

[19] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language models:

when are they needed? arXiv preprint arXiv:1301.5650  2013.

[20] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850  2013.
[21] Alex Graves  Santiago Fernández  Faustino Gomez  and Jürgen Schmidhuber. Connectionist temporal
classiﬁcation: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the
23rd international conference on Machine learning  pages 369–376. ACM  2006.

[22] Mehryar Mohri  Fernando Pereira  and Michael Riley. Weighted ﬁnite-state transducers in speech recogni-

tion. Computer Speech & Language  16(1):69–88  2002.

[23] Ryan Kiros  Yukun Zhu  Ruslan R Salakhutdinov  Richard Zemel  Raquel Urtasun  Antonio Torralba 
and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems  pages
3276–3284  2015.

[24] Karl Moritz Hermann  Tomas Kocisky  Edward Grefenstette  Lasse Espeholt  Will Kay  Mustafa Suleyman 
and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information
Processing Systems  pages 1684–1692  2015.

[25] T. Wessels and C. W. Omlin. Reﬁning hidden markov models with recurrent neural networks. In Neural
Networks  2000. IJCNN 2000  Proceedings of the IEEE-INNS-ENNS International Joint Conference on 
volume 2  pages 271–276 vol.2  2000.

[26] Antti Rasmus  Harri Valpola  Mikko Honkala  Mathias Berglund  and Tapani Raiko. Semi-supervised

learning with ladder network. arXiv preprint arXiv:1507.02672  2015.

[27] Mohammad Pezeshki  Linxi Fan  Philemon Brakel  Aaron Courville  and Yoshua Bengio. Deconstructing

the ladder network architecture. arXiv preprint arXiv:1511.06430  2015.

[28] Bishan Yang  Wen-tau Yih  Xiaodong He  Jianfeng Gao  and Li Deng. Embedding entities and relations

for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575  2014.

[29] Rami Al-Rfou  Guillaume Alain  Amjad Almahairi  and et al. Theano: A python framework for fast

computation of mathematical expressions  2016.

[30] François Chollet. Keras. GitHub repository: https://github.com/fchollet/keras  2015.

9

,Yuhuai Wu
Saizheng Zhang
Ying Zhang
Yoshua Bengio
Russ Salakhutdinov
Yung-Kyun Noh
Masashi Sugiyama
Kee-Eung Kim
Frank Park
Daniel Lee
Hongzi Mao
Parimarjan Negi
Akshay Narayan
Hanrui Wang
Jiacheng Yang
Haonan Wang
Ryan Marcus
ravichandra addanki
Mehrdad Khani Shirkoohi
Songtao He
Vikram Nathan
Frank Cangialosi
Shaileshh Venkatakrishnan
Wei-Hung Weng
Song Han
Tim Kraska
Dr.Mohammad Alizadeh