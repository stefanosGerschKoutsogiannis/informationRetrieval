2013,On Algorithms for Sparse Multi-factor NMF,Nonnegative matrix factorization (NMF) is a popular data analysis method  the objective of which is to decompose a matrix with all nonnegative components into the product of two other nonnegative matrices.  In this work  we describe a new simple and efficient algorithm for multi-factor nonnegative matrix factorization problem ({mfNMF})  which generalizes the original NMF problem to more than two factors.  Furthermore  we extend the mfNMF algorithm to incorporate a regularizer based on Dirichlet distribution over normalized columns to encourage sparsity in the obtained factors.  Our sparse NMF algorithm affords a closed form and an intuitive interpretation  and is more efficient in comparison with previous works that use fix point iterations. We demonstrate the effectiveness and efficiency of our algorithms on both synthetic and real data sets.,On Algorithms for Sparse Multi-factor NMF

Xin Wang
Siwei Lyu
Computer Science Department
University at Albany  SUNY

Albany  NY 12222

{slyu xwang26}@albany.edu

Abstract

Nonnegative matrix factorization (NMF) is a popular data analysis method  the
objective of which is to approximate a matrix with all nonnegative components
into the product of two nonnegative matrices. In this work  we describe a new
simple and efﬁcient algorithm for multi-factor nonnegative matrix factorization
(mfNMF) problem that generalizes the original NMF problem to more than two
factors. Furthermore  we extend the mfNMF algorithm to incorporate a regularizer
based on the Dirichlet distribution to encourage the sparsity of the components of
the obtained factors. Our sparse mfNMF algorithm affords a closed form and an
intuitive interpretation  and is more efﬁcient in comparison with previous works
that use ﬁx point iterations. We demonstrate the effectiveness and efﬁciency of
our algorithms on both synthetic and real data sets.

1

Introduction

(cid:81)K

The goal of nonnegative matrix factorization (NMF) is to approximate a nonnegative matrix V
with the product of two nonnegative matrices  as V ≈ W1W2. Since the seminal work of [1] that
introduces simple and efﬁcient multiplicative update algorithms for solving the NMF problem  it has
become a popular data analysis tool for applications where nonnegativity constraints are natural [2].
In this work  we address the multi-factor NMF (mfNMF) problem  where a nonnegative matrix V is
approximated with the product of K ≥ 2 nonnegative matrices  V ≈
k=1 Wk. It has been argued
that using more factors in NMF can improve the algorithm’s stability  especially for ill-conditioned
and badly scaled data [2].
Introducing multiple factors into the NMF formulation can also ﬁnd
practical applications  for instance  extracting hierarchies of topics representing different levels of
abstract concepts in document analysis or image representations [2].
Many practical applications also require the obtained nonnegative factors to be sparse  i.e.  having
many zero components. Most early works focuses on the matrix (cid:96)1 norm [6]  but it has been pointed
out that (cid:96)1 norm becomes completely toothless for factors that have constant (cid:96)1 norms  as in the case
of stochastic matrices [7  8]. Regularizers based on the entropic prior [7] or the Dirichlet prior [8]
have been shown to be more effective but do not afford closed-form solutions.
The main contribution of this work is therefore two-fold. First  we describe a new algorithm for
the mfNMF problem. Our solution to mfNMF seeks optimal factors that minimize the total dif-
k=1 Wk  and it is based on the solution of a special matrix optimization
problem that we term as the stochastic matrix sandwich (SMS) problem. We show that the SMS
problem affords a simple and efﬁcient algorithm consisting of only multiplicative update and nor-
malization (Lemma 2). The second contribution of this work is a new algorithm to incorporate the
Dirichlet sparsity regularizer in mfNMF. Our formulation of the sparse mfNMF problem leads to
a new closed-form solution  and the resulting algorithm naturally embeds sparsity control into the
mfNMF algorithm without iteration (Lemma 3). We further show that the update steps of our sparse

ference between V and(cid:81)K

1

mfNMF algorithm afford a simple and intuitive interpretation. We demonstrate the effectiveness and
efﬁciency of our sparse mfNMF algorithm on both synthetic and real data.

2 Related Works

Most exiting works generalizing the original NMF problem to more than two factors are based on
the multi-layer NMF formulation  in which we solve a sequence of two factor NMF problems  as
V ≈ W1H1  H1 ≈ W2H2 ···   and HK−1 ≈ WK−1WK [3–5]. Though simple and efﬁcient 
such greedy algorithms are not associated with a consistent objective function involving all factors
simultaneously. Because of this  the obtained factors may be suboptimal measured by the difference
between the target matrix V and their product. On the other hand  there exist much fewer works
directly solving the mfNMF problem  one example is a multiplicative algorithm based on the general
Bregmann divergence [9]. In this work  we focus on the generalized Kulback-Leibler divergence (a
special case of the Bregmann divergence)  and use its decomposing property to simplify the mfNMF
objective and remove the undesirable multitude of equivalent solutions in the general formulation.
These changes lead to a more efﬁcient algorithm that usually converges to a better local minimum
of the objective function in comparison of the work in [9] (see Section 6).
As a common means to encouraging sparsity in machine learning  the (cid:96)1 norm has been incorpo-
rated into the NMF objective function [6] as a sparsity regularizer. However  the (cid:96)1 norm may be
less effective for nonnegative matrices  for which it reduces to the sum of all elements and can be
decreased trivially by scaling down all factors without affecting the number of zero components.
Furthermore  the (cid:96)1 norm becomes completely toothless in cases when the nonnegative factors are
constrained to have constant column or row sums (as in the case of stochastic matrices).
An alternative solution is to use the Shannon entropy of each column of the matrix factor as sparsity
regularizer [7]  since a vector with unit (cid:96)1 norm and low entropy implies that only a few of its com-
ponents are signiﬁcant. However  the entropic prior based regularizer does not afford a closed form
solution  and an iterative ﬁxed point algorithm is described based on the Lamber’s W-function in [7].
Another regularizer is based on the symmetric Dirichlet distribution with concentration parameter
α < 1  as such a model allocates more probability weights to sparse vectors on a probability sim-
plex [8  10  11]. However  using the Dirichlet regularizer has a practical problem  as it can become
unbounded when there is a zero element in the factor. Simply ignoring such cases as in [11] can
lead to unstable algorithm (see Section 5.2). Two approaches have been described to solve this prob-
lem  one is based on the constrained concave-convex procedure (CCCP) [10]. The other uses the
psuedo-Dirichlet regularizer [8]  which is a bounded perturbation of the original Dirichlet model. A
drawback common to these methods is that they require extra iterations for the ﬁx point algorithm.
Furthermore  the effects of the updating steps on the sparsity of the resulting factors are obscured by
the iterative steps. In contrast  our sparse mfNMF algorithm uses the original Dirichlet model and
does not require extra ﬁx point iteration. More interestingly  the update steps of our sparse mfNMF
algorithm afford a simple and intuitive interpretation.

3 Basic Deﬁnitions

We denote 1m as the all-one column vector of dimension m and Im as the m-dimensional identity
matrix  and use V ≥ 0 or v ≥ 0 to indicate that all elements of a matrix V or a vector v are
nonnegative. Throughout the paper  we assume a matrix does not have all-zero columns or rows. An
m × n nonnegative matrix V is stochastic if V T 1m = 1n  i.e.  each column has a total sum of one.
Also  for stochastic matrices W1 and W2  their product W1W2 is also stochastic. Furthermore  an
m × n nonnegative matrix V can be uniquely represented as V = SD  with an n × n nonnegative
diagonal matrix D = diag(V T 1m) and an m × n stochastic matrix S = V D−1.
For nonnegative matrices V and W   their generalized Kulback-Leibler (KL) divergence [1] is de-
ﬁned as

(cid:19)

.

(1)

m(cid:88)

n(cid:88)

(cid:18)

d(V  W ) =

i=1

j=1

Vij log

Vij
Wij − Vij + Wij

2

We have d(V  W ) ≥ 0 and the equality holds if and only if V = W 1. We emphasize the following
decomposition of the generalized KL divergence: representing V and W as products of stochastic
matrices and diagonal matrices  V = S(V )D(V ) and W = S(W )D(W )  we can decompose d(V  W )
into two terms involving only stochastic matrices or diagonal matrices  as
S(V )
ij
S(W )
ij

D(V )  D(W )(cid:17)
(cid:16)

V  S(W )D(V )(cid:17)

D(V )  D(W )(cid:17)

m(cid:88)

n(cid:88)

d(V  W )=d

Vij log

(cid:16)

(cid:16)

(cid:35)

i=1

j=1

(cid:34)

+ d

+ d

=

.
(2)

Due to space limit  the proof of Eq.(2) is deferred to the supplementary materials.

4 Multi-Factor NMF

lk−1 × lk for k = 1 ···   K  with l0 = m and lK = n that minimize d(V (cid:81)K

In this work  we study the multi-factor NMF problem based on the generalized KL divergence.
Speciﬁcally  given an m × n nonnegative matrix V   we seek K ≥ 2 matrices Wk of dimensions
k=1 Wk)  s.t.  Wk ≥ 0.
This simple formulation has a drawback as it is invariant to relative scalings between the factors:
for any γ > 0  we have d(V  W1 ··· Wi ··· Wj ··· WK) = d(V  W1 ··· (γWi)··· ( 1
γ Wj)··· WK).
In other words  there exist inﬁnite number of equivalent solutions  which gives rise to an intrinsic
ill-posed nature of the mfNMF problem.
To alleviate this problem  we constrain the ﬁrst K − 1 factors  W1 ···   WK−1  to be stochastic
matrices  and differentiate the notationns with X1 ···   XK−1. Using the property of nonnegative
matrices  we represent the last nonnegative factor WK as the product of a stochastic matrix XK
k=1 Wk
k=1 Xk and a diagonal matrix D(W ). Similarly 
we also decompose the target nonnegative matrix V as the product of a stochastic matrix S(V )
and a nonnegative diagonal matrix D(V ). It is not difﬁcult to see that any solution from this more
constrained formulation leads to a solution to the original problem and vice versa.
Applying the decomposition in Eq.(2)  the mfNMF optimization problem can be re-expressed as

and a nonnegative diagonal matrix D(W ). As such  we represent the nonnegative matrix(cid:81)K
as the product of a stochastic matrix S(W ) = (cid:81)K

min

X1 ···  XK  D(W )

s.t. X T

d

V 

(cid:17)

k=1 Xk

(cid:16)(cid:81)K

D(V )(cid:17)

+ d(cid:0)D(V )  D(W )(cid:1)

(cid:16)
k 1lk−1 = 1lk   Xk ≥ 0  k = 1 ···   K  D(W ) ≥ 0.
(cid:16)

D(V )  D(W )(cid:17)

d

  s.t. D(W ) ≥ 0.

min
D(W )

As such  the original problem is solved with two sub-problems  each for a different set of variables.
The ﬁrst sub-problem solves for the diagonal matrix D(W )  as:

Per the property of the generalized KL divergence  its solution is trivially given by D(W ) = D(V ).
The second sub-problem optimizes the K stochastic factors  X1 ···   XK  which  after dropping
irrelevant constants and rearranging terms  becomes

m(cid:88)

n(cid:88)

(cid:32) K(cid:89)

(cid:33)

max

X1 ···  XK

Vij log

Xk

i=1

j=1

k=1

ij

  s.t. X T

k 1lk−1 = 1lk   Xk ≥ 0  k = 1 ···   K.

(3)

Note that Eq.(3) is essentially the maximization of the similarity between the stochastic part of V  
SV with the stochastic matrix formed as the product of the K stochastic matrices X1 ···   XK 
weighted by DV .

4.1 Stochastic Matrix Sandwich Problem

Before describing the algorithm solving (3)  we ﬁrst derive the solution to another related problem
that we term as the stochastic matrix sandwich (SMS) problem  from which we can construct a
solution to (3). Speciﬁcally  in an SMS problem one minimizes the following objective function
with regards to an m(cid:48)

× n(cid:48) stochastic matrix X  as

1In computing the generalized KL divergence  we deﬁne 0 log 0 = 0 and 0

0 = 0.

3

m(cid:88)

n(cid:88)

X

i=1

j=1

max

Cij log (AXB)ij  s.t. X T 1m(cid:48) = 1n(cid:48)  X ≥ 0 
where A and B are two known stochastic matrices of dimensions m × m(cid:48) and n(cid:48)
× n  respectively 
and C is an m × n nonnegative matrix.
We note that (4) is a convex optimization problem [12]  which can be solved with general numerical
procedures such as interior-point methods. However  we present here a new simple solution based
on multiplicative updates and normalization  which completely obviates control parameters such as
the step sizes. We ﬁrst show that there exists an “auxiliary function” to log (AXB)ij.
Lemma 1. Let us deﬁne

(4)

(cid:48)(cid:88)

m

(cid:48)(cid:88)

n

k=1

l=1

(cid:16)

(cid:17)

Aik ˜XklBlj

A ˜XB

ij

(cid:16)

(cid:18) Xkl

˜Xkl

log

(cid:19)

(cid:17)

A ˜XB

 

ij

Fij(X; ˜X) =

then we have Fij(X; ˜X) ≤ log (AXB)ij and Fij(X; X) = log (AXB)ij.
Proof of Lemma 1 can be found in the supplementary materials.
Based Lemma 1 we can develop an EM style iterative algorithm to optimize (4)  in which  starting
with an initial values X = X0  we iteratively solve the following optimization problem 

m(cid:88)

i=1

n(cid:88)
CijFij(X; Xt) and t ← t + 1.
(cid:88)

(cid:88)

j=1

CijFij(Xt+1; Xt) ≤

Cij log (AXt+1B)ij 

(cid:88)

Xt+1 ←

argmax

X:X T 1m(cid:48) =1n(cid:48)  X≥0

Using the relations given in Lemma 1  we have:
CijFij(Xt; Xt) ≤

Cij log (AXtB)ij =

(cid:88)

(5)

(6)

i j
which shows that each iteration of (5) leads to feasible X and does not decrease the objective func-
tion of (4). Rearranging terms and expressing results using matrix operations  we can simplify the

i j

i j

i j

objective function of (5) as(cid:88)

where

(cid:104)
CijFij(X; ˜X) =

i j

m

(cid:48)(cid:88)
AT(cid:16)

k=1

n

(cid:48)(cid:88)
(cid:16)

l=1

Mkl log Xkl + const 

(cid:17)(cid:17)

BT(cid:105)

A ˜XB

C (cid:11)

(7)
where ⊗ and (cid:11) correspond to element-wise matrix multiplication and division  respectively. A
detailed derivation of (6) and (7) is given in the supplemental materials. The following result shows
that the resulting optimization has a closed-form solution.
Lemma 2. The global optimal solution to the following optimization problem 
Mkl log Xkl  s.t. X T 1m(cid:48) = 1n(cid:48)  X ≥ 0 

M = ˜X ⊗
(cid:48)(cid:88)
(cid:48)(cid:88)

max

(8)

m

X

n

 

is given by

k=1

l=1

Mkl(cid:80)

.

k Mkl

Xkl =

Proof of Lemma 2 can be found in the supplementary materials.
Next  we can construct a coordinate-wise optimization solution to the mfNMF problem (3) that
iteratively optimizes each Xk while ﬁxing the others  based on the solution to the SMS problem
given in Lemma 2. In particular  it is easy to see that for C = V  

• solving for X1 with ﬁxed X2 ···   XK is an SMS problem with A = Im  X = X1 and
• solving for Xk  k = 2 ···   K − 1  with ﬁxed X1 ···   Xk−1  Xk+1 ···   XK is an SMS

B =(cid:81)K
with A =(cid:81)k−1
• and solving for XK with ﬁxed X1 ···   XK−1 is an SMS problem with A =(cid:81)K−1

k(cid:48)=1 Xk(cid:48)  X = Xk  and B =(cid:81)K

k(cid:48)=k+1 Xk(cid:48);

k=1 Xk 

k=2 Xk;

X = XK and B = In.

In practice  we do not need to run each SMS optimization to converge  and the algorithm can be
implemented with a few ﬁxed steps updating each factor in order.
It should be pointed out that even though SMS is a convex optimization problem guaranteed with
a global optimal solution  this is not the case for the general mfNMF problem (3)  the objective
function of which is non-convex (it is an example of the multi-convex function [13]).

4

5 Sparse Multi-Factor NMF

(cid:88)

i j

(cid:88)

i j

(cid:32) K(cid:89)

(cid:33)

K(cid:88)

k=1

Next  we describe incorporating sparsity regularization in the mfNMF formulation. We assume that
the sparsity requirement is applied to each individual factor in the mfNMF objective function (3)  as

max

X1 ···  XK

Vij log

Xk

+

k=1

ij

(cid:96)(Xk)  s.t. X T

k 1lk−1 = 1lk   Xk ≥ 0 

(9)

where (cid:96)(X) is the sparsity regularizer that is larger for stochastic matrix X with more zero entries.
As the overall mfNMF can be solved by optimizing each individual factor in an SMS problem  we
focus here on the case where the sparsity regularizer of each factor is introduced in (4)  to solve

max

X

Cij log (AXB)ij + (cid:96)(X)  s.t. X T 1m(cid:48) = 1n(cid:48)  X ≥ 0.

(10)

(cid:107)X(cid:107)1 =(cid:80)

5.1 Dirichlet Sparsity Regularizer
As we have mentioned  the typical choice of (cid:96)(·) as the matrix (cid:96)1 norm is problematic in (10)  as
ij Xij = n(cid:48) is a constant. On the other hand  if we treat each column of X as a point on
(cid:81)d
a probability simplex  as their elements are nonnegative and sum to one  then we can induce a sparse
regularizer from the Dirichlet distribution. Speciﬁcally  a Dirchilet distribution of d-dimensional
k=1 vα−1
vectors v : v ≥ 0  1T v = 1 is deﬁned as Dir(v; α) = Γ(dα)
  where Γ(·) is the standard
Gamma function2. The parameter α ∈ [0  1] is the parameter that controls the sparsity of samples –
smaller α corresponds to higher likelihood of sparse v in Dir(v; α).
Incorporating a Dirichlet regularizer with parameter αl to each column of X and dropping irrelevant
constant terms  (10) reduces to3

Γ(α)d

k

m(cid:88)

n(cid:88)

(cid:48)(cid:88)

m

(cid:48)(cid:88)

n

(cid:48)(cid:88)

m

(cid:48)(cid:88)

n

k=1

l=1

max

X

Cij log (AXB)ij +

i=1

j=1

k=1

l=1

(αl − 1) log Xkl  s.t. X T 1m(cid:48) = 1n(cid:48)  X ≥ 0.

(11)

As in the case of mfNMF  we introduce the auxiliary function of log(AXB)ij to form an upper-
bound of (11) and use an EM-style algorithm to optimize (11). Using the result given in Eqs.(6) and
(7)  the optimization problem can be further simpliﬁed as:

max

X

(Mkl + αl − 1) log Xkl  s.t. X T 1m(cid:48) = 1n(cid:48)  X ≥ 0.

(12)

5.2 Solution to SMS with Dirichlet Sparse Regularizer
However  a direct optimization of (12) is problematic when αl < 1: if there exists Mkl < 1− αl  the
objective function of (12) becomes non-convex and unbounded – the term (Mkl + αl − 1) log Xkl
approaches ∞ as Xkl → 0. This problem is addressed in [8] by modifying the deﬁnition of the
Dirichlet regularizer in (11) to (αl − 1) log(Xkl + )  where  > 0 is a predeﬁned parameter. This
avoids the problem of taking logarithm of zero  but it leads to a less efﬁcient algorithm based on an
iterative ﬁx point procedure. In addition  the ﬁx point algorithm is difﬁcult to interpret as its effect
on the sparsity of the obtained factors is obscured by the iterative steps.
On the other hand  notice that if we tighten the nonnegativity constraint to Xkl ≥   the objective
function of (12) will always be ﬁnite. Therefore  we can simply modify the optimization of (12) the
objective function to become inﬁnity  as:

max

X

(Mkl + αl − 1) log Xkl  s.t. X T 1m(cid:48) = 1n(cid:48)  Xkl ≥  ∀k  l.

(13)

The following result shows that with a sufﬁciently small   the constrained optimization problem in
(13) has a unique global optimal solution that affords a closed-form and intuitive interpretation.

2For simplicity  we only discuss the symmetric Dirichlet model  but the method can be easily extended to

the non-symmetric Dirichlet model with different α value for different dimension.

3Alternatively  this special case of NMF can be formulated as C = AXB + E  where E contains inde-
pendent Poisson samples [14]  and (11) can be viewed as a (log) maximum a posteriori estimation of column
vectors of X with a Poisson likelihood and symmetric Dirichlet prior.

5

(cid:48)(cid:88)

m

(cid:48)(cid:88)

n

k=1

l=1

case 1

case 2

case 3

Figure 1: Sparsiﬁcation effects on the updated vectors before (left) and after (right) applying the algorithm
(cid:16)
given in Lemma 3  with each column illustrating one of the three cases.
Lemma 3. Without loss of generality  we assume Mkl (cid:54)= 1 − αl ∀k  l4. If we choose a constant
  and for each column l deﬁne Nl = {k|Mkl < 1 − αl} as the set of
 ∈
elements with Mkl + αl − 1 < 0  then the following is the global optimal solution to (13):

0  minkl{|Mkl+αl−1|}
m(cid:48) maxkl{|Mkl+αl−1|}

(cid:17)

• case 1. |Nl| = 0  i.e.  all constant coefﬁcients of (13) are positive 

(cid:80)

ˆXkl =

Mkl + αl − 1
k(cid:48) [Mk(cid:48)l + αl − 1]

 

• case 2. 0 < |Nl| < m(cid:48)  i.e.  the constant coefﬁcients of (13) have mixed signs 
(cid:80)
(1 − |Nl|) [Mkl + αl − 1]
k(cid:48) {[Mk(cid:48)l + αl − 1] · δ [k(cid:48) (cid:54)∈ Nl]} · δ [k (cid:54)∈ Nl]  
where δ(c) is the Kronecker function that takes 1 if c is true and 0 otherwise.
• case 3. |Nl| = m(cid:48)  i.e.  all constant coefﬁcients of (13) are negative 

ˆXkl =  · δ [k ∈ Nl] +
(cid:34)

(cid:35)

(cid:34)

ˆXkl = (1− (m

− 1))· δ

k = argmax

k(cid:48)∈{1 ···  m(cid:48)}

Mk(cid:48)l

+ · δ

k (cid:54)= argmax
k(cid:48)∈{1 ···  m(cid:48)}

Mk(cid:48)l

.

(16)

(cid:48)

(14)

(15)

(cid:35)

Proof of Lemma 3 can be found in the supplementary materials. Note that the algorithm provided in
Lemma 3 is still valid when  = 0  but the theoretical result of it attaining the global optimum of a
ﬁnite optimization problem only holds for  satisfying the condition in Lemma 3.
We can provide an intuitive interpretation to Lemma 3  which is schematically illustrated in Fig.1
for a toy example. For the ﬁrst case (ﬁrst column of Fig.1) when all constant coefﬁcients of (13) are
positive  it simply reduces to ﬁrst decrease every Mkl by 1−αl and then renormalize each column to
sum to one  Eq.(14). This operation of reducing the same amount from all elements in one column
of M has the effect of making “the rich get richer and the poor get poorer” (known as Dalton’s 3rd
law)  which increases the imbalance of the elements and improves the chances of small elements
to be reduced to zero in the subsequent steps [15]5. In the second case (second column of Fig.1) 
when the coefﬁcients of (13) have mixed signs  the effect of the updating step in (15) is two-fold.
For Mkl < 1 − αl (ﬁrst term in Eq.(15))  they are all reduced to   which is the de facto zero. In
other words  components below the threshold 1 − αl are eliminated to zero. On the other hands 
terms with Mkl > 1− αl (second term in Eq.(15)) are redistribute with the operation of reduction of
1 − αl followed by renormalization. In the last case when all coefﬁcients of (13) are negative (third
column of Fig.1)  only the element corresponding to Mkl that is closest to the threshold 1 − αk  or
equivalently  the largest of all Mkl  will survive with a non-zero value that is essentially 1 (ﬁrst term
in Eq.(16))  while the rest of the elements all become extinct (second term in Eq.(16))  analogous to
a scenario of “survival of the ﬁttest”. Note that it is the last two cases actually generating zero entries
in the factors  but the ﬁrst case makes more entries suitable for being set to zero. The thresholding
and renormalization steps resemble algorithms in sparse coding [16].

6 Experimental Evaluations

We perform experimental evaluations of the sparse multi-factor NMF algorithm using synthetic and
real data sets. In the ﬁrst set of experiments  we study empirically the convergence of the multi-
plicative algorithm for the SMS problem (Lemma 2). Speciﬁcally  with several different choices of

in X to zero. So we can technically ignore such elements for each column index l.

4It is easy to show that the optimal solution in this case is Xkl = 0  i.e.  setting the corresponding component
5Some early works (e.g.  [11]) obtain simpler solution by setting negative Mk(cid:48)l + αl − 1 to zero followed

by normalization. Our result shows that such a solution is not optimal.

6

MklkkˆXkl1↵lMklkkˆXkl1↵lMklkkˆXkl1↵lFigure 2: Convergences of the SMS objective function with multiplicative update algorithm (mult solid curve)
and the projected gradient ascent method (pgd dashed curve) for different problem sizes.

(m  n  m(cid:48)  n(cid:48))  we randomly generate stochastic matrices A (m × m(cid:48)) and B (n(cid:48)
× n)  and non-
negative matrix C (m × n). We then apply the SMS algorithm to solve for the optimal X. We
compare our algorithm with a projected gradient ascent optimization of the SMS problem  which
updates X using the gradient of the SMS objective function and chooses a step size to satisfy the
nonnegative and normalization constraints. We do not consider methods that use the Hessian ma-
trix of the objective function  as constructing a general Hessian matrix in this case have prohibitive
memory requirement even for a medium sized problem. Shown in Fig.2 are several runs of the two
algorithms starting at the same initial values  as the the objective function of SMS vs. the number of
updates of X. Because of the convex nature of the SMS problem  both algorithms converge to the
same optimal value regardless of the initial values. On the other hand  the multiplicative updates for
SMS usually achieve two order speed up in terms of the number of iterations and typically about 10
times faster in running time when compared to the gradient based algorithm.
In the second set of experiments  we evaluate the performance of the coordinate-update mfNMF
algorithm based on the multiplicative updating algorithm of the SMS problem (Section 4.1). Specif-
ically  we consider the mfNMF problem that approximates a randomly generated target nonnegative
matrix V of dimension m×n with the product of three stochastic factors  W1 (m×m(cid:48))  W2 (m(cid:48)
×n(cid:48)) 
and W3 (n(cid:48)
× n). The performance of the algorithm is evaluated by the logarithm of the generalized
KL divergence for between V and W1W2W3  of which lower numerical values suggest better per-
formances. As a comparison  we also implemented a multi-layer NMF algorithm [5]  which solves
two NMF problems in sequence  as: V ≈ W1 ˜V and ˜V ≈ W2W3  and the multiplicative update
algorithm of mfNMF of [9]  both of which are based on the generalized KL divergence. To make
the comparison fair  we start all three algorithms with the same initial values.

m  n  m(cid:48)  n(cid:48)

multi-layer NMF [5]
multi-factor NMF [9]

multi-factor NMF (this work)

50 40 30 10

1.733
1.431
1.325

200 100 60 30

1000 400 200 50

2.595
2.478
2.340

70.526
66.614
62.086

5000 2000 100 20

183.617
174.291
161.338

Table 1: Comparison of the multi-layer NMF method and two mfNMF methods for a three factor with different
problem sizes. The values correspond to the logarithm of generalized KL divergence  log d(V  W1W2W3).
Lower numerical values (in bold font) indicate better performances.
The results of several runs of these algorithms for different problem sizes are summarized in Table
1  which show that in general  mfNMF algorithms lead to better solutions corresponding to lower
generalized KL divergences between the target matrix and the product of the three estimated factors.
This is likely due to the fact that these algorithms optimize the generalized KL divergence directly 
while multi-layer NMF is a greedy algorithm with sub-optimal solutions. On the other hand  our
mfNMF algorithm consistently outperforms the method of [9] by a signiﬁcant margin  with on
average 40% less iterations. We think the improved performance and running efﬁciency are due
to our formulation of the mfNMF problem based on stochastic matrices  which reduces the solution
space and encourage convergence to a better local minimum of the objective function.
We apply the sparse mfNMF algorithm to data converted from grayscale images from the MNIST
Handwritten Digits data set [17] that are vectorized to column vectors and normalized to have total
sum of one. All vectorized and normalized images are collected to form the target stochastic ma-
trix V   which are decomposed into the product of three factors W1W2W3. We also incorporate the
Dirichlet sparsity regularizers with different conﬁgurations. For simplicity  we use the same param-
eter for all column vectors in one factor. The threshold is set as  = 10−8/n where n is the total
number of images. Shown in Fig.3 are the decomposition results corresponding to 500 vectorized
20 × 20 images of handwritten digit 3  that are decomposed into three factors of size 400 × 196 

7

100101102103104105m=20 n=20m’=10 n’=10itr #obj. fun pgdmult100101102103104105m=90 n=50m’=20 n’=5itr #obj. fun pgdmult100101102103104105m=200 n=100m’=50 n’=25itr #obj. fun pgdmult100101102103104105m=1000 n=200m’=35 n’=15itr #obj. fun pgdmultW1

W2

W1W2

W3

W1W2W3

Figure 3: Sparse mfNMF algorithm on the handwritten digit images. The three rows correspond to three cases
as: α1 = 1  α2 = 1  α3 = 1  α1 = 1  α2 = 1  α3 = 0.99  α1 = 1  α2 = 0.99  α3 = 0.99  respectively. See
texts for more details.
196 × 100  and 100 × 500. The columns of the factors are reshaped to shown as images  where the
brightness of each pixel in the ﬁgure is proportional to the nonnegative values in the corresponding
factors. Due to space limit  we only show the ﬁrst 25 columns in each factor. All three factorization
results can reconstruct the target matrix (last column)  but they put different constraints on the ob-
tained factors. The factors are also visually meaningful: factor W1 contains low level components
of the images that when combined with factor W2 forms more complex structures. The ﬁrst row
corresponds to running the mfNMF without sparsity regularizer. The two rows below correspond
to the cases when the Dirichlet sparsity regularizer is applied to the third factor and to the second
and third factor simultaneously. Compare with the corresponding results in the non-sparse case 
the obtained factors contain more zeros. As a comparison  we also implement mfNMF algorithm
using a pseudo-Dirichlet sparse regularizer [8]. With similar decomposition results  our algorithm
is typically 3 − 5 times faster as it does not require the extra iterations of a ﬁx point algorithm.
7 Conclusion

We describe in this work a simple and efﬁcient algorithm for the sparse multi-factor nonnegative
matrix factorization (mfNMF) problem  involving only multiplicative update and normalization. Our
solution to incorporate Dirichlet sparse regularizer leads to a closed form solution and the resulting
algorithm is more efﬁcient than previous works based on ﬁx point iterations. The effectiveness and
efﬁciency of our algorithms are demonstrated on both synthetic and real data sets.
There are several directions we would like to further explore. First  we are studying if similar
multiplicative update algorithm also exists for mfNMF with more general similarity norms such
as Csizar’s divergence [18]  Itakura-Saito divergence  [19]  α-β divergence [20] or the Bregmann
divergence [9]. We will also study incorporating other constraints (e.g.  value ranges) over the
factors into the mfNMF algorithm. Last  we would like to further study applications of mfNMF
in problems such as co-clustering or hierarchical document topic analysis  exploiting its ability to
recover hierarchical decomposition of nonnegative matrices.

Acknowledgement

This work is supported by the National Science Foundation under Grant Nos. IIS-0953373  IIS-
1208463 and CCF-1319800.

8

References
[1] Daniel D. Lee and H. Sebastian Seung. Algorithms for nonnegative matrix factorization. In Advances in

Neural Information Processing Systems (NIPS 13)  2001. 1  2

[2] A. Cichocki  R. Zdunek  A.H. Phan  and S. Amari. Nonnegative Matrix and Tensor Factorizations:

Applications to Exploratory Multi-way Data Analysis and Blind Source Separation. Wiley  2009. 1

[3] Seungjin Choi Jong-Hoon Ahn and Jong-Hoon Oh. A multiplicative up-propagation algorithm. In ICML 

2004. 2

[4] Nicolas Gillis and Fran cois Glineur. A multilevel approach for nonnegative matrix factorization. Journal

of Computational and Applied Mathematics  236 (7):1708–1723  2012. 2

[5] A Cichocki and R Zdunek. Multilayer nonnegative matrix factorisation. Electronics Letters  42(16):947–

948  2006. 2  7

[6] Patrik O. Hoyer and Peter Dayan. Non-negative matrix factorization with sparseness constraints. Journal

of Machine Learning Research  5:1457–1469  2004. 1  2

[7] Bhiksha Raj Madhusudana Shashanka and Paris Smaragdis. Sparse overcomplete latent variable decom-

position of counts data. In NIPS  2007. 1  2

[8] Martin Larsson and Johan Ugander. A concave regularization technique for sparse mixture models. In

NIPS  2011. 1  2  5  8

[9] Suvrit Sra and Inderjit S Dhillon. Nonnegative matrix approximation: Algorithms and applications.

Computer Science Department  University of Texas at Austin  2006. 2  7  8

[10] Jussi Kujala. Sparse topic modeling with concave-convex procedure: EMish algorithm for latent dirichlet

allocation. In Technical Report  2004. 2

[11] Jagannadan Varadarajan  R´emi Emonet  and Jean-Marc Odobez. A sequential topic model for mining
recurrent activities from long term video logs. International Journal of Computer Vision  103(1):100–
126  2013. 2  6

[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2005. 4
[13] P. Gahinet  P. Apkarian  and M. Chilali. Afﬁne parameter-dependent Lyapunov functions and real para-

metric uncertainty. IEEE Transactions on Automatic Control  41(3):436–442  1996. 4

[14] Wray Buntine and Aleks Jakulin. Discrete component analysis. In Subspace  Latent Structure and Feature

Selection Techniques. Springer-Verlag  2006. 5

[15] N. Hurley and Scott Rickard. Comparing measures of sparsity. Information Theory  IEEE Transactions

on  55(10):4723–4741  2009. 6

[16] Misha Denil and Nando de Freitas. Recklessly approximate sparse coding. CoRR  abs/1208.0959  2012.

6

[17] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998. 7

[18] Andrzej Cichocki  Rafal Zdunek  and Shun-ichi Amari. Csiszar’s divergences for non-negative matrix fac-
torization: Family of new algorithms. In Independent Component Analysis and Blind Signal Separation 
pages 32–39. Springer  2006. 8

[19] C´edric F´evotte  Nancy Bertin  and Jean-Louis Durrieu. Nonnegative matrix factorization with the itakura-

saito divergence: With application to music analysis. Neural Computation  21(3):793–830  2009. 8

[20] Andrzej Cichocki  Rafal Zdunek  Seungjin Choi  Robert Plemmons  and Shun-Ichi Amari. Non-negative
tensor factorization using alpha and beta divergences. In Acoustics  Speech and Signal Processing  2007.
ICASSP 2007. IEEE International Conference on  volume 3  pages III–1393. IEEE  2007. 8

[21] V. Chv´atal. Linear Programming. W. H. Freeman and Company  New York  1983.

9

,Siwei Lyu
Xin Wang
Chris Piech
Jonathan Bassen
Jonathan Huang
Surya Ganguli
Mehran Sahami
Leonidas Guibas
Jascha Sohl-Dickstein