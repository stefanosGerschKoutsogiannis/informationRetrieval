2009,Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness,This paper uses information-theoretic techniques to determine minimax rates for estimating nonparametric sparse additive regression models under high-dimensional scaling.  We assume an additive decomposition of the form $f^*(X_1  \ldots  X_p) = \sum_{j \in S} h_j(X_j)$  where each component function $h_j$ lies in some Hilbert Space $\Hilb$ and $S \subset \{1  \ldots  \pdim \}$ is an unknown subset with cardinality $\s = |S$.  Given $\numobs$ i.i.d. observations of $f^*(X)$ corrupted with white Gaussian noise where the covariate vectors $(X_1  X_2  X_3 ... X_{\pdim})$ are drawn with i.i.d. components from some distribution $\mP$  we determine tight lower bounds on the minimax rate for estimating the regression function with respect to squared $\LTP$ error. The main result shows that the minimax rates are $\max{\big(\frac{\s \log \pdim / \s}{n}  \LowerRateSq \big)}$.  The first term reflects the difficulty of performing \emph{subset selection} and is independent of the Hilbert space $\Hilb$; the second term $\LowerRateSq$ is an \emph{\s-dimensional estimation} term  depending only on the low dimension $\s$ but not the ambient dimension $\pdim$  that captures the difficulty of estimating a sum of $\s$ univariate functions in the Hilbert space $\Hilb$.  As a special case  if $\Hilb$ corresponds to the $\m$-th order Sobolev space $\SobM$ of functions that are $m$-times differentiable  the $\s$-dimensional estimation term takes the form $\LowerRateSq \asymp \s \; n^{-2\m/(2\m+1)}$. The minimax rates are compared with rates achieved by an $\ell_1$-penalty based approach  it can be shown that a certain $\ell_1$-based approach achieves the minimax optimal rate.,Lower bounds on minimax rates for nonparametric

regression with additive sparsity and smoothness

Garvesh Raskutti1  Martin J. Wainwright1 2  Bin Yu1 2

1UC Berkeley Department of Statistics

2UC Berkeley Department of Electrical Engineering and Computer Science

Abstract

We study minimax rates for estimating high-dimensional nonparametric regression mod-
els with sparse additive structure and smoothness constraints. More precisely  our goal
is to estimate a function f∗ : Rp → R that has an additive decomposition of the form
f∗(X1  . . .   Xp) = Pj∈S h∗j (Xj)  where each component function h∗j lies in some class
H of “smooth” functions  and S ⊂ {1  . . .   p} is an unknown subset with cardinality s = |S|.
Given n i.i.d. observations of f∗(X) corrupted with additive white Gaussian noise where the
covariate vectors (X1  X2  X3  ...  Xp) are drawn with i.i.d. components from some distribu-
tion P  we determine lower bounds on the minimax rate for estimating the regression function
with respect to squared-L2(P) error. Our main result is a lower bound on the minimax rate

that scales as max(cid:0) s log(p/s)
n(H)(cid:1). The ﬁrst term reﬂects the sample size required for
performing subset selection  and is independent of the function class H. The second term
n(H) is an s-dimensional estimation term corresponding to the sample size required for
s ǫ2
estimating a sum of s univariate functions  each chosen from the function class H. It depends
linearly on the sparsity index s but is independent of the global dimension p. As a special case 
if H corresponds to functions that are m-times differentiable (an mth-order Sobolev space) 
n(H) ≍ s n−2m/(2m+1). Either of
then the s-dimensional estimation term takes the form sǫ2
the two terms may be dominant in different regimes  depending on the relation between the
sparsity and smoothness of the additive decomposition.

  s ǫ2

n

1 Introduction

Many problems in modern science and engineering involve high-dimensional data  by which we mean that the
ambient dimension p in which the data lies is of the same order or larger than the sample size n. A simple
example is parametric linear regression under high-dimensional scaling  in which the goal is to estimate a
regression vector β∗ ∈ Rp based on n samples.
In the absence of additional structure  it is impossible to
obtain consistent estimators unless the ratio p/n converges to zero which precludes the regime p ≫ n. In
many applications  it is natural to impose sparsity conditions  such as requiring that β∗ have at most s non-zero
parameters for some s ≪ p. The method of ℓ1-regularized least squares  also known as the Lasso algorithm [14] 
has been shown to have a number of attractive theoretical properties for such high-dimensional sparse models
(e.g.  [1  19  10]).

Of course  the assumption of a parametric linear model may be too restrictive for some applications. Accord-
ingly  a natural extension is the non-parametric regression model y = f∗(x1  . . .   xp)+w  where w ∼ N (0  σ2)
is additive observation noise. Unfortunately  this general non-parametric model is known to suffer severely from
the “curse of dimensionality”  in that for most natural function classes  the sample size n required to achieve
a given estimation accuracy grows exponentially in the dimension. This challenge motivates the use of addi-
tive non-parametric models (see the book [6] and references therein)  in which the function f∗ is decomposed
j=1 h∗j (xj) of univariate functions h∗j . A natural sub-class of these

additively as a sum f∗(x1  x2  ...  xp) = Pp

1

models are the sparse additive models  studied by Ravikumar et. al [12]  in which

f∗(x1  x2  ...  xp) =Xj∈S

h∗j (xj) 

(1)

where S ⊂ {1  2  . . .   p} is some unknown subset of cardinality |S| = s.
A line of past work has proposed and analyzed computationally efﬁcient algorithms for estimating regression
functions of this form. Just as ℓ1-based relaxations such as the Lasso have desirable properties for sparse
parametric models  similar ℓ1-based approaches have proven to be successful. Ravikumar et al. [12] propose a
back-ﬁtting algorithm to recover the component functions hj and prove consistency in both subset recovery and
consistency in empirical L2(Pn) norm. Meier et al. [9] propose a method that involves a sparsity-smoothness
penalty term  and also demonstrate consistency in L2(P) norm. In the special case that H is a reproducing
kernel Hilbert space (RKHS)  Koltchinskii and Yuan [7] analyze a least-squares estimator based on imposing
an ℓ1 − ℓH
-penalty. The analysis in these paper demonstrates that under certain conditions on the covariates 
such regularized procedures can yield estimators that are consistent in the L2(P)-norm even when n ≪ p.
Of complementary interest to the rates achievable by practical methods are the fundamental limits of the esti-
mating sparse additive models  meaning lower bounds that apply to any algorithm. Although such lower bounds
are well-known under classical scaling (where p remains ﬁxed independent of n)  to the best of our knowledge 
lower bounds for minimax rates on sparse additive models have not been determined. In this paper  our main

result is to establish a lower bound on the minimax rate in L2(P) norm that scales as max(cid:0) s log(p/s)

n(H)(cid:1).
The ﬁrst term s log(p/s)
is a subset selection term  independent of the univariate function space H in which
n(H) in an
the additive components lie  that reﬂects the difﬁculty of ﬁnding the subset S. The second term sǫ2
s-dimensional estimation term  which depends on the low dimension s but not the ambient dimension p  and
reﬂects the difﬁculty of estimating the sum of s univariate functions  each drawn from function class H. Either
the subset selection or s-dimensional estimation term dominates  depending on the relative sizes of n  p  and
s as well as H. Importantly  our analysis applies both in the low-dimensional setting (n ≫ p) and the high-
dimensional setting (p ≫ n) provided that n  p and s are going to ∞. Our analysis is based on information-
theoretic techniques centered around the use of metric entropy  mutual information and Fano’s inequality in
order to obtain lower bounds. Such techniques are standard in the analysis of non-parametric procedures under
classical scaling [5  2  17]  and have also been used more recently to develop lower bounds for high-dimensional
inference problems [16  11].

  sǫ2

n

n

The remainder of the paper is organized as follows. In the next section  the results are stated including appropri-
ate preliminary concepts  notation and assumptions. In Section 3  we state the main results  and provide some
comparisons to the rates achieved by existing algorithms. In Section 4  we provide an overview of the proof.
We discuss and summarize the main consequences in Section 5.

2 Background and problem formulation

In this paper  we consider a non-parametric regression model with random design  meaning that we make n
observations of the form

y(i) = f∗(X (i)) + w(i) 

for i = 1  2  . . .   n.
(2)
Here the random vectors X (i) ∈ Rp are the covariates  and have elements X (i)
drawn i.i.d. from some un-
derlying distribution P. We assume that the noise variables w(i) ∼ N (0  σ2) are drawn independently  and
independent of all X (i)’s. Given a base class H of univariate functions with norm k · kH
  consider the class of
functions f : Rp → R that have an additive decomposition:
pXj=1

F : =(cid:8)f : Rp → R | f (x1  x2  ...  xp) =

khjkH ≤ 1 ∀j = 1  . . .   p(cid:9).

hj(xj) 

and

Given some integer s ∈ {1  . . .   p}  we deﬁne the function class F0(s)  which is a union of(cid:0)p
subspaces of F  given by

s(cid:1) s-dimensional

j

F0(s) : =(cid:8)f ∈ F |

pXj=1

I(hj 6= 0) ≤ s(cid:9).

(3)

The minimax rate of estimation over F0(s) is deﬁned by the quantity min bf maxf ∗∈F0(s) Ekbf−f∗k2
the expectation is taken over the noise w  and randomness in the sampling  and bf ranges over all (measurable)

L2(P)  where

2

functions of the observations {(y(i)  X (i))}n
minimax rate.

i=1. The goal of this paper is to determine lower bounds on this

2.1 Inner products and norms

Given a univariate function hj ∈ H  we deﬁne the usual L2(P) inner product
hj(x)h′j(x) dP(x).

hhj  h′jiL2(P) : =ZR

(With a slight abuse of notation  we use P to refer to the measure over Rp as well as the induced marginal
measure in each direction deﬁned over R). Without loss of generality (re-centering the functions as needed)  we
may assume

E[hj(X)] =ZR

hj(x) dP(x) = 0 

for all hj ∈ H. As a consequence  we have E[f (X1  . . .   Xp)] = 0 for all functions f ∈ F0(s). Given our
assumption that the covariate vector X = (X1  . . .   Xp) has independent components  the L2(P) inner product
on F has the additive decomposition hf  f′iL2(P) = Pp
j=1 hhj  h′jiL2(P). (Note that if independence were not
assumed the L2(P) inner product over F would involve cross-terms.)
2.2 Kullback-Leibler divergence

Since we are using information theoretic techniques  we will be using the Kullback-Leibler (KL) divergence as a

measure of “distance” between distributions. For a given pair of functions f and ef  consider the n-dimensional
vectors f (X) = (cid:0)f (X (1))  f (X (2))  . . .   f (X (n))(cid:1)T and ef (X) = (cid:0)ef (X (1))  ef (X (2))  . . .   ef (X (n))(cid:1)T . Since
Y |f (X) ∼ N (f (X)  σ2In×n) and Y |ef (X) ∼ N (ef (X)  σ2In×n) 

1

D(Y |f (X)k Y |ef (X)) =

2σ2kf (X) − ef (X)k2

2.

We also use the notation D(f k ef ) to mean the average K-L divergence between the distributions of Y induced
by the functions f and ef respectively. Therefore we have the relation

(4)

(5)

D(f k ef ) = EX(cid:2)D(Y |f (X)k Y |ef (X))(cid:3)

n

=

L2(P).

2σ2kf − efk2

This relation between average K-L divergence and squared L2(P) distance plays an important role in our proof.

2.3 Metric entropy for function classes

In this section  we deﬁne the notion of metric entropy  which provides a way in which to measure the relative
sizes of different function classes with respect to some metric ρ. More speciﬁcally  central to our results is the
metric entropy of F0(s) with respect to the L2(P) norm.
Deﬁnition 1 (Covering and packing numbers). Consider a metric space consisting of a set S and a metric
ρ : S × S → R+.

(a) An ǫ-covering of S in the metric ρ is a collection {f 1  . . .   f N} ⊂ S such that for all f ∈ S  there
exists some i ∈ {1  . . .   N} with ρ(f  f i) ≤ ǫ. The ǫ-covering number Nρ(ǫ) is the cardinality of the
smallest ǫ-covering.

(b) An ǫ-packing of S in the metric ρ is a collection {f 1  . . .   f M} ⊂ S such that ρ(f i  f j) ≥ ǫ for all

i 6= j. The ǫ-packing number Mρ(ǫ) is the cardinality of the largest ǫ-packing.

The covering and packing entropy (denoted by log Nρ(ǫ) and log Mρ(ǫ) respectively) are simply the logarithms
of the covering and packing numbers  respectively.
It can be shown that for any convex set  the quantities
log Nρ(ǫ) and log Mρ(ǫ) are of the same order (within constant factors independent of ǫ).

3

In this paper  we are interested in packing (and covering) subsets of the function class F0(s) in the L2(P) metric 
and so drop the subscript ρ from here onwards. En route to characterizing the metric entropy of F0(s)  we need
to understand the metric entropy of the unit balls of our univariate function class H—namely  the sets

The metric entropy (both covering and packing entropy) for many classes of functions are known. We provide
some concrete examples here:

B

H(1) : = {h ∈ H | khkH ≤ 1}.

[0  1] → [0  1]

H(1) scales as log M (ǫ;H) ∼ log(1/ǫ).

(i) Consider the class H = {hβ : R → R | hβ(x) = βx} of all univariate linear functions with the norm
khβkH = |β|. Then it is known [15] that the metric entropy of B
(ii) Consider the class H = {h :
|h(x) − h(y)| ≤ |x − y|} of all 1-Lipschitz func-
tions on [0  1] with the norm khkH = supx∈[0 1] |h(x)|. In this case  it is known [15] that the metric entropy
scales as log MH(ǫ;H) ∼ 1/ǫ. Compared to the previous example of linear models  note that the metric
entropy grows much faster as ǫ → 0  indicating that the class of Lipschitz functions is much richer.
(iii) Consider the class of Sobolev spaces W m for m ≥ 1  consisting of all functions that have m derivatives 
and the mth derivative is bounded in L2(P) norm. In this case  it is known that log M (ǫ;H) ∼ ǫ− 1
m (e.g.  [3]).
Clearly  increasing the smoothness constraint m leads to smaller classes. Such Sobolev spaces are a particular
class of functions whose packing/covering entropy grows at a rate polynomial in 1
ǫ .

|

In our analysis  we require that the metric entropy of B
Assumption 1. Using log M (ǫ;H) to denote the packing entropy of the unit ball B
assume that there exists some α ∈ (0  1) such that

H(1) satisfy the following technical condition:

H(1) in the L2(P)-norm 

lim
ǫ→0

log M (αǫ;H)
log M (ǫ;H)

> 1.

The condition is required to ensure that log M (cǫ)/ log M (ǫ) can be made arbitrarily small or large uniformly
over small ǫ by changing c  so that a bound due to Yang and Barron [17] can be applied. It is satisﬁed for most
non-parametric classes  including (for instance) the Lipschitz and Sobolev classes deﬁned in Examples (ii) and
(iii) above. It may fail to hold for certain parametric classes  such as the set of linear functions considered
in Example (i); however  we can use an alternative technique to derive bounds for the parametric case (see
Corollary 2).

3 Main result and some consequences

In this section  we state our main result and then develop some of its consequences. We begin with a theorem
that covers the function class F0(s) in which the univariate function classes H have metric entropy that satisﬁes
Assumption 1. We state a corollary for the special cases of univariate classes H with metric entropy growing
polynomial in (1/ǫ)  and also a corollary for the special case of sparse linear regression.
Consider the observation model (2) where the covariate vectors have i.i.d. elements Xj ∼ P  and the regression
function f∗ ∈ F0(s). Suppose that the univariate function class H that underlies F0(s) satisﬁes Assumption 1.
Under these conditions  we have the following result:
Theorem 1. Given n i.i.d. samples from the sparse additive model (2)  the minimax risk in squared-L2(P)
norm is lower bounded as

min
bf

max

f ∗∈F0(s)

Ekbf − f∗k2

L2(P) ≥ max(cid:20) σ2s log(p/s)

32n

n(H)(cid:21) 

ǫ2

 

s
16

(6)

where  for a ﬁxed constant c  the quantity ǫn(H) = ǫn > 0 is largest positive number satisfying the inequality
(7)

nǫ2
n

2σ2 ≤ log M(cid:0)c ǫn(cid:1).

For the case where H has an entropy that is growing to ∞ at a polynomial rate as ǫ → 0—say log M (ǫ;H) =
Θ(ǫ−1/m) for some m > 1

2   we can compute the rate for the s-dimensional estimation term explicitly.

4

Corollary 1. For the sparse additive model (2) with univariate function space H such that such that
log M (ǫ;H) = Θ(ǫ−1/m)  we have

min
bf

max

f ∗∈F0(s)

for some C > 0.

3.1 Some consequences

Ekbf − f∗k2

L2(P) ≥ max(cid:20) σ2s log(p/s)

32n

  C s(cid:0) σ2

2m+1(cid:21) 
n (cid:1) 2m

(8)

In this section  we discuss some consequences of our results.

Effect of smoothness: Focusing on Corollary 1  for spaces with m bounded derivatives (i.e.  functions in the
Sobolev space W m)  the minimax rate is n− 2m
2m+1 (for details  see e.g. Stone [13]). Clearly  faster rates are
obtained for larger smoothness indices m  and as m → ∞  the rate approaches the parametric rate of n−1.
Since we are estimating over an s-dimensional space (under the assumption of independence)  we are effectively
estimating s univariate functions  each lying within the function space H. Therefore the uni-dimensional rate is
multiplied by s.
Smoothness versus sparsity: It is worth noting that depending on the relative scalings of s  n and p and the metric
entropy of H  it is possible for either the subset selection term or s-dimensional estimation term to dominate
the lower bound. In general  if log(p/s)
n(H))  the s-dimensional estimation term dominates  and vice
versa (at the boundary  either term determines the minimax rate). In the case of a univariate function class H
with polynomial entropy as in Corollary 1  it can be seen that for n = o((log(p/s))2m+1)  the s-dimensional
estimation term dominates while for n = Ω((log(p/s))2m+1)  the subset selection term dominates.
Rates for linear models: Using an alternative proof technique (not the one used in this paper)  it is possible [11]
to derive the exact minimax rate for estimation in the sparse linear regression model  in which we observe

= o(ǫ2

n

y(i) =Xj∈S

βjX (i)

j + w(i) 

for i = 1  2  ...  n.

(9)

Note that this is a special case of the general model (2) in which H corresponds to the class of univariate linear
functions (see Example (i)).
Corollary 2. For sparse linear regression model (9)  the the minimax rate scales as max(cid:0) s log(p/s)
In this case  we see clearly the subset selection term dominates for p → ∞  meaning the subset selection
problem is always “harder” (in a statistical sense) than the s-dimensional estimation problem. As shown
by Bickel et al. [1]  the rate achieved by ℓ1-regularized methods is s log p
under suitable conditions on the
covariates X.

n(cid:1).

  s

n

n

Upper bounds: To show that the lower bounds are tight  upper bounds that are matching need to be derived.
Upper bounds (matching up to constant factors) can be derived via a classical information-theoretic approach
(e.g.  [5  2])  which involves constructing an estimator based on a covering set and bounding the covering
entropy of F0(s). While this estimation approach does not lead to an implementable algorithm  it is a simple
theoretical device to demonstrate that lower bounds are tight. We turn our focus on implementable algorithms
in the next point.

Comparison to existing bounds: We now provide a brief comparison of the minimax lower bounds with upper
bounds on rates achieved by existing implementable algorithms provided by past work [12  7  9]. Ravikumar
et al. [12] propose a back-ﬁtting algorithm to minimize the least-squares objective with a sparsity constraint on
the the function f. The rates derived in Koltchinskii and Yuan [7] do not match the lower bounds derived in
Theorem 1. Further  it is difﬁcult to directly compare the rates in Ravikumar et al. [12] and Meier et al. [9] with
our minimax lower bounds since their analysis does not explicitly track the sparsity index s. We are currently
in the process of conducting a thorough comparison with the above-mentioned ℓ1-based methods.

4 Proof outline

In this section  we provide an outline of the proof of Theorem 1; due to space constraints  we defer some of
the technical details to the full-length version. The proof is based on a combination of information-theoretic

5

techniques and the concepts of packing and covering entropy  as deﬁned previously in Section 2.3. First  we
provide a high-level overview of the proof. The basic idea is to carefully choose two subsets T1 and T2 of the
function class F0(s) and lower bound the minimax rates over these two subsets. In Section 4.1  application of
the generalized Fano method—a technique based on Fano’s inequality—to the set T1 deﬁned in equation (10)
yields a lower bound on the subset selection term. In Section 4.2  we apply an alternative method for obtaining
lower bounds over a second set T2 deﬁned in equation (11) that captures the difﬁculty of estimating the sum
of s univariate functions.. The second technique also exploits Fano’s inequality but uses a more reﬁned upper
bound on the mutual information developed by Yang and Barron [17].
Before procedding  we ﬁrst note that for any T ⊂ F0(s)  we have
max
f ∗∈T

L2(P) ≥ min
bf

min
bf

L2(P).

max

Ekbf − f∗k2

Ekbf − f∗k2
Moreover  for any subsets T1  T2 ⊂ F0(s)  we have
L2(P) ≥ max(cid:0) min

f ∗∈F0(s)

f ∗∈F0(s)

Ekbf − f∗k2

min
bf

max

bf

max
f ∗∈T1

Ekbf − f∗k2

L2(P)  min
bf

max
f ∗∈T2

Ekbf − f∗k2

L2(P)(cid:1) 

since the bound holds for each of the two terms. We apply this lower bound using the subsets T1 and T2 deﬁned
in equations (10) and (11).

4.1 Bounding the complexity of subset selection

For part of the proof  we use the generalized Fano’s method [4]  which we state below without proof. Given
some parameter space  we let d be a metric on it.
Lemma 1. (Generalized Fano Method) For a given integer r ≥ 2  consider a collection Mr = {P1  . . .   Pr}
of r probability distributions such that

and the pairwise KL divergence satisﬁes

d(θ(Pi)  θ(Pj)) ≥ αr

for all i 6= j 

Then the minimax risk over the family is lower bounded as
αr

D(Pi k Pj) ≤ βr

for all i  j = 1  . . .   r.

max

j

Ejd(θ(Pj) bθ) ≥

2 (cid:18)1 −

βr + log 2

log r (cid:19).

T1 : =(cid:26)f : f (X1  X2  ...  Xp) =

pXj=1

(10)

The proof of Lemma 1 involves applying Fano’s inequality over the discrete set of parameters θ ∈ Θ indexed
by the set of distributions Mr. Now we construct the set T1 which creates the set of probability distributions
Mr.
Let g be an arbitrary function in H such that kgkL2(P) = σ

. The set T1 is deﬁned as

4q log (p/s)
cjg(Xj)  cj ∈ {−1  0  1} | kck0 = s(cid:27).

n

T1 may be viewed as a hypercube of F0(s) and will lead to the lower bound for the ’subset selection’ term. This
hypercube construction is often used to prove lower bounds (see Yu [18]). Next  we require a further reduction
of the set T1 to a set A (deﬁned in Lemma 2) to ensure that elements of A are well-separated in L2(P) norm.
The construction of A is as follows:
Lemma 2. There exists a subset A ⊂ T1 such that:
(i) log |A| ≥ 1
(ii) kf − f′k2
∀ f  f′ ∈ A  and
(iii) D(f k f′) ≤ 1
The proof involves using a combinatoric argument to construct the set A. For an argument on how the set is
constructed  see K¨uhn [8]. For s log p
s ≥ 8 log 2  applying the Generalized Fano Method (Lemma 1) together
with Lemma 2 yields the bound

2 s log(p/s) 
L2(P) ≥ σ2s log(p/s)

8 s log(p/s) ∀f  f′ ∈ A.

16n

min
bf

max

f ∗∈F0(s)

Ekbf − f∗k2

L2(P) ≥ min
bf

max
f ∗∈A

This completes the proof for the subset selection term ( s log(p/s)

Ekbf − f∗k2

L2(P) ≥
) in Theorem 1.

n

σ2s log(p/s)

32n

.

6

4.2 Bounding the complexity of s-dimensional estimation

Next we derive a bound for the s-dimensional estimation term by determining a lower bound over T2. Let S be
an arbitrary subset of s integers in {1  2  ..  p}  and deﬁne the set FS as
T2 : = FS : =(cid:8)f ∈ F : f (X) =Xj∈S

hj(Xj)(cid:9).

(11)

Clearly FS ⊂ F0(s) meaning that
max

min
bf

f ∗∈F0(s)

Ekbf − f∗k2

L2(P) ≥ min
bf

max
f ∗∈FS

Ekbf − f∗k2

L2(P).

We use a technique used in Yang and Barron [17] to lower bound the minimax rate over FS. The idea is to
construct a maximal δn-packing set for FS and a minimal ǫn-covering set for FS  and then to apply Fano’s
inequality to a carefully chosen mixture distribution involving the covering and packing sets (see the full-length
version for details). Following these steps yields the following result:
Lemma 3.

min
bf

max
f ∗∈FS

Ekbf − f∗k2

L2(P) ≥

δ2
n

4 (cid:18)1 −

log N (ǫn;FS) + nǫ2

n/2σ2 + log 2

log M (δn;FS)

(cid:19).

Now we have a bound with expressions involving the covering and packing entropies of the s-dimensional space
FS. The following Lemma allows bounds on log M (ǫ;FS) and log N (ǫ;FS) in terms of the unidimensional
packing and covering entropies respectively:
Lemma 4. Let H be function space with a packing entropy log M (ǫ;H) that satisﬁes Assumption 1. Then we
have the bounds

log M (ǫ;FS) ≥ s log M (ǫ/√s;H) 

and

log N (ǫ;FS) ≤ s log N (ǫ/√s;H).

The proof involves constructing ǫ√s - packing set and covering sets in each of the s dimensions and displaying
that these are ǫ-packing and coverings sets in FS (respectively). Combining Lemmas 3 and 4 leads to the
inequality

min
bf

max
f ∗∈FS

Ekbf − f∗k2

L2(P) ≥

δ2
n

4 (cid:18)1 −

s log N (ǫn/√s;H) + nǫ2

s log M (δn/√s;H)

n/2σ2 + log 2

Now we choose ǫn and δn to meet the following constraints:

n
2σ2 ǫ2
ǫn√s

ǫn√s
n ≤ s log N (
δn√s
;H).

;H) ≤ log M (

;H) 

and

4 log N (

(cid:19).

(12)

(13a)

(13b)

Combining Assumption 1 with the well-known relations log M (2ǫ;H) ≤ log N (2ǫ;H) ≤ log M (ǫ;H)  we
conclude that in order to satisfy inequalities (13a) and (13b)  it is sufﬁcient to choose ǫn = cδn for a constant c 
2σ2 . Furthermore  if we deﬁne δn/√s = eδn  then this inequality can
and then require that s log M ( cδn√s ;H) ≥ nδ2
be re-expressed as log M (ceδn) ≥ nfδn
n ≥ log 2  using inequalities (13a) and (13b) together with

equation (12) yields the desired rate

2σ2 . For n

2σ2 ǫ2

n

2

min
bf

max
f ∗∈FS

Ekbf − f∗k2

L2(P) ≥

seδn

16

thereby completing the proof.

5 Discussion

2

 

In this paper  we have derived lower bounds for the minimax risk in squared L2(P) error for estimating sparse
additive models based on the sum of univariate functions from a function class H. The rates show that the
estimation problem effectively decomposes into a subset selection problem and an s-dimensional estimation

7

problem  and the “harder” of the two problems (in a statistical sense) determines the rate of convergence.
More concretely  we demonstrated that the subset selection term scales as s log(p/s)
  depending linearly on
the number of components s and only logarithmically in the ambient dimension p. This subset selection term is
independent of the univariate function space H. On the other hand  the s-dimensional estimation term depends
on the “richness” of the univariate function class  measured by its metric entropy; it scales linearly with s and is
independent of p. Ongoing work suggests that our lower bounds are tight in many cases  meaning that the rates
derived in Theorem 1 are minimax optimal for many function classes.

n

There are a number of ways in which the work can be extended. One implicit and strong assumption in our
analysis was that the covariates Xj  j = 1  2  ...  p are independent. It would be interesting to investigate the case
when the random variables are endowed with some correlation structure. One would expect the rates to change 
particularly if many of the variables are collinear. It would also be interesting to develop a more complete
understanding of whether computationally efﬁcient algorithms [7  12  9] based on regularization achieve the
lower bounds on the minimax rate derived in this paper.

References

[1] P. Bickel  Y. Ritov  and A. Tsybakov. Simultaneous analysis of the Lasso and Dantzig selector. Annals of

Statistics  2009. To appear.

[2] L. Birg´e. Approximation dans les espaces metriques et theorie de l’estimation. Z. Wahrsch. verw. Gebiete 

65:181–327  1983.

[3] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the classes W α
p .

Math. USSR-Sbornik  2(3):295–317  1967.

[4] T. S. Han and S. Verdu. Generalizing the Fano inequality. IEEE Transactions on Information Theory 

40:1247–1251  1994.

[5] R. Z. Has’minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform

metric. Theory Prob. Appl.  23:794–798  1978.

[6] T. Hastie and R. Tibshirani. Generalized Additive Models. Chapman and Hall Ltd  Boca Raton  1999.
[7] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In Proceedings of

COLT  2008.

[8] T. K¨uhn. A lower estimate for entropy numbers. Journal of Approximation Theory  110:120–124  2001.
[9] L. Meier  S. van de Geer  and P. Buhlmann. High-dimensional additive modeling. Annals of Statistics  To

appear.

[10] N. Meinshausen and B.Yu. Lasso-type recovery of sparse representations for high-dimensional data. An-

nals of Statistics  37(1):246–270  2009.

[11] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax rates of estimation for high-dimensional linear regres-

sion over ℓq-balls. Technical Report arXiv:0910.2042  UC Berkeley  Department of Statistics  2009.

[12] P. Ravikumar  H. Liu  J. Lafferty  and L. Wasserman. Sparse additive models. Journal of the Royal

Statistical Society  To appear.

[13] C. J. Stone. Optimal global rates of convergence for nonparametric regression. Annals of Statistics 

10:1040–1053  1982.

[14] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[15] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press  2000.
[16] M. J. Wainwright. Information-theoretic bounds for sparsity recovery in the high-dimensional and noisy
setting. IEEE Trans. Info. Theory  December 2009. Presented at International Symposium on Information
Theory  June 2007.

[17] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of

Statistics  27(5):1564–1599  1999.

[18] B. Yu. Assouad  Fano and Le Cam. Research Papers in Probability and Statistics: Festschrift in Honor of

Lucien Le Cam  pages 423–435  1996.

[19] C. H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensional linear regres-

sion. Annals of Statistics  36:1567–1594  2006.

8

,Trapit Bansal
Chiranjib Bhattacharyya
Ravindran Kannan
Chuan-Yung Tsai
Andrew Saxe
Andrew Saxe
David Cox