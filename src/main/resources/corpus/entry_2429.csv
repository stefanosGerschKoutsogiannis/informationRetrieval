2013,Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms,All the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task. In this paper  different from existing methods  we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks. Specifically  we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one  a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it. For regression problems  we extend the kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.,Heterogeneous-Neighborhood-based Multi-Task

Local Learning Algorithms

Department of Computer Science  Hong Kong Baptist University

yuzhang@comp.hkbu.edu.hk

Yu Zhang

Abstract

All the existing multi-task local learning methods are deﬁned on homogeneous
neighborhood which consists of all data points from only one task. In this paper 
different from existing methods  we propose local learning methods for multi-
task classiﬁcation and regression problems based on heterogeneous neighborhood
which is deﬁned on data points from all tasks. Speciﬁcally  we extend the k-
nearest-neighbor classiﬁer by formulating the decision function for each data point
as a weighted voting among the neighbors from all tasks where the weights are
task-speciﬁc. By deﬁning a regularizer to enforce the task-speciﬁc weight matrix
to approach a symmetric one  a regularized objective function is proposed and
an efﬁcient coordinate descent method is developed to solve it. For regression
problems  we extend the kernel regression to multi-task setting in a similar way
to the classiﬁcation case. Experiments on some toy data and real-world datasets
demonstrate the effectiveness of our proposed methods.

1

Introduction

For single-task learning  besides global learning methods there are local learning methods [7]  e.g. 
k-nearest-neighbor (KNN) classiﬁer and kernel regression. Different from the global learning meth-
ods  the local learning methods make use of locality structure in different regions of the feature space
and are complementary to the global learning algorithms. In many applications  the single-task lo-
cal learning methods have shown comparable performance with the global counterparts. Moreover 
besides classiﬁcation and regression problems  the local learning methods are also applied to some
other learning problems  e.g.  clustering [18] and dimensionality reduction [19]. When the number
of labeled data is not very large  the performance of the local learning methods is limited due to s-
parse local density [14]. In this case  we can leverage the useful information from other related tasks
to help improve the performance which matches the philosophy of multi-task learning [8  4  16].
Multi-task learning utilizes supervised information from some related tasks to improve the perfor-
mance of one task at hand and during the past decades many advanced methods have been proposed
for multi-task learning  e.g.  [17  3  9  1  2  6  12  20  14  13]. Among those methods  [17  14] are
two representative multi-task local learning methods. Even though both methods in [17  14] use
KNN as the base learner for each task  Thrun and O’Sullivan [17] focus on learning cluster structure
among different tasks while Parameswaran and Weinberger [14] learn different distance metrics for
different tasks. The KNN classiﬁers use in both two methods are deﬁned on the homogeneous neigh-
borhood which is the set of nearest data points from the same task the query point belongs to. In
some situation  it is better to use a heterogeneous neighborhood which is deﬁned as the set of nearest
data points from all tasks. For example  suppose we have two similar tasks marked with two colors
as shown in Figure 1. For a test data point marked with ‘?’ from one task  we obtain an estima-
tion with low conﬁdence or even a wrong one based on the homogeneous neighborhood. However 
if we can use the data points from both two tasks to deﬁne the neighborhood (i.e.  heterogeneous
neighborhood)  we can obtain a more conﬁdent estimation.

1

In this paper  we propose novel local learning models for
multi-task learning based on the heterogeneous neighbor-
hood. For multi-task classiﬁcation problems  we extend
the KNN classiﬁer by formulating the decision function
on each data point as weighted voting of its neighbors
from all tasks where the weights are task-speciﬁc. Since
multi-task learning usually considers that the contribution
of one task to another one equals that in the reverse direc-
tion  we deﬁne a regularizer to enforce the task-speciﬁc
weight matrix to approach a symmetric matrix and then
based on this regularizer  a regularized objective function
is proposed. We develop an efﬁcient coordinate descent
method to solve it. Moreover  we also propose a local
method for multi-task regression problems. Speciﬁcally 
we extend the kernel regression method to multi-task setting in a similar way to the classiﬁcation
case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our
proposed methods.

Figure 1: Data points with one color
(i.e.  black or red) are from the same
task and those with one type of marker
(i.e.  ‘+’ or ‘-’) are from the same class.
A test data point is represented by ‘?’.

2 A Multi-Task Local Classiﬁer based on Heterogeneous Neighborhood

In this section  we propose a local classiﬁer for multi-task learning by generalizing the KNN algo-
rithm  which is one of the most widely used local classiﬁers for single-task learning.
Suppose we are given m learning tasks {Ti}m
i=1. The training set consists of n triples (xi  yi  ti)
with the ith data point as xi ∈ RD  its label yi ∈ {−1  1} and its task indicator ti ∈ {1  . . .   m}. So
each task is a binary classiﬁcation problem with ni = |{j|tj = i}| data points belonging to the ith
task Ti.
For the ith data point xi  we use Nk(i) to denote the set of the indices of its k nearest neighbors. If
Nk(i) is a homogeneous neighborhood which only contains data points from the task that xi belongs
to make a decision for xi where sgn(·) denotes the
to  we can use d(xi) = sgn
sign function and s(i  j) denotes a similarity function between xi and xj. Here  by deﬁning Nk(i) as
a heterogeneous neighborhood which contains data points from all tasks  we cannot directly utilize
this decision function and instead we introduce a weighted decision function by using task-speciﬁc
weights as

j∈Nk(i) s(i  j)yj

(cid:16)(cid:80)

(cid:17)

 (cid:88)

j∈Nk(i)



d(xi) = sgn

wti tj s(i  j)yj

where wqr represents the contribution of the rth task Tr to the qth one Tq when Tr has some data
points to be neighbors of a data point from Tq. Of course  the contribution from one task to itself
should be positive and also the largest  i.e.  wii ≥ 0 and −wii ≤ wij ≤ wii for j (cid:54)= i. When
wqr(q (cid:54)= r) approaches wqq  it means Tr is very similar to Tq in local regions. At another extreme
where wqr(q (cid:54)= r) approaches −wqq  if we ﬂip the labels of data points in Tr  Tr can have a positive
contribution −wqr to Tq which indicates that Tr is negatively correlated to Tq. Moreover  when
wqr/wqq(q (cid:54)= r) is close to 0 which implies there is no contribution from Tr to Tq  Tr is likely
to be unrelated to Tq. So the utilization of {wqr} can model three task relationships: positive task
correlation  negative task correlation and task unrelatedness as in [6  20].

We use f (xi) to deﬁne the estimation function as f (xi) =(cid:80)

j∈Nk(i) wti tj s(i  j)yj. Then similar to
support vector machine (SVM)  we use hinge loss l(y  y(cid:48)) = max(0  1 − yy(cid:48)) to measure empirical
performance on the training data. Moreover  recall that wqr represents the contribution of Tr to
Tq and wrq is the contribution of Tq to Tr. Since multi-task learning usually considers that the
contribution of Tr to Tq almost equals that of Tq to Tr  we expect wqr to be close to wrq. To encode
this priori information into our model  we can either formulate it as wqr = wrq  a hard constraint 
or a soft regularizer  i.e.  minimizing (wqr − wrq)2 to enforce wqr ≈ wrq  which is more preferred.
Combining all the above considerations  we can construct a objective function for our proposed
method MT-KNN as

l(yi  f (xi)) +

(cid:107)W − WT(cid:107)2

F +

λ1
4

(cid:107)W(cid:107)2

F

λ2
2

s.t. wqq ≥ 0  wqq ≥ wqr ≥ −wqq (q (cid:54)= r)

(1)

2

n(cid:88)

i=1

min
W

where W is a m× m matrix with wqr as its (q  r)th element and (cid:107)·(cid:107)F denotes Frobenius norm of a
matrix. The ﬁrst term in the objective function of problem (1) measures the training loss  the second
one enforces W to be a symmetric matrix which implies wqr ≈ wrq  and the last one penalizes
the complexity of W. The regularization parameters λ1 and λ2 balance the trade-off between these
three terms.

(cid:17)

2.1 Optimization Procedure

j=1 wtij

(cid:80)m
In this section  we discuss how to solve problem (1). We ﬁrst rewrite f (xi) as f (xi) =
m × 1 vector with the jth element as(cid:80)
k (i) denotes the set of the indices of xi’s nearest
neighbors from the jth task in Nk(i)  wti = (wti1  . . .   wtim) is the tith row of W  and ˆxi is a
k (i) s(i  l)yl. Then we can reformulate problem (1) as
s.t. wqq ≥ 0  wqq ≥ wqr ≥ −wqq(q (cid:54)= r).

l∈N j
(cid:107)W − WT(cid:107)2

= wti ˆxi where N j

k (i) s(i  l)yl

(cid:16)(cid:80)
(cid:88)

l(yj  wiˆxj) +

m(cid:88)

(cid:107)W(cid:107)2

l∈N j

F +

F

λ1
4

λ2
2

min
W

i=1

j∈Ti

(2)
To solve problem (2)  we use a coordinate descent method  which is also named as an alternating
optimization method in some literatures.
By adopting the hinge loss in problem (2)  the optimization problem for wik (k (cid:54)= i) is formulated
as

(cid:88)

min
wik

λ
2

ik − βikwik +
w2

max(0  aj

ikwik + bj

ik)

s.t. cik ≤ wik ≤ eik

(3)

j∈Ti

< cik}  C2 = {j|aj

ik = −yj ˆxjk  bj

ik > 0 − bj
ik
aj
ik
ik < 0 − bj
ik
aj
ik

(cid:80)
ik = 1 −
where λ = λ1 + λ2  βik = λ1wki  ˆxjk is the kth element of ˆxj  aj
t(cid:54)=k wit ˆxjt  cik = −wii  and eik = wii. If the objective function of problem (3) only has
yj
the ﬁrst two terms  this problem will become a univariate quadratic programming (QP) problem
with a linear inequality constraint  leading to an analytical solution. Moreover  similar to SVM we
can introduce some slack variables for the third term in the objective function of problem (3) and
then that problem will become a QP problem with ni + 1 variables and 2ni + 1 linear constraints.
We can use off-the-shelf softwares to solve this problem in polynomial time. However  the whole
optimization procedure may not be very efﬁcient since we need to solve problem (3) and call QP
solvers for multiple times. Here we utilize the piecewise linear structure of the last term in the
objective function of problem (3) and propose a more efﬁcient solution.
We assume all aj are non-zero and otherwise we can discard them without affecting the solution
since the corresponding losses are constants. We deﬁne six index sets as
C1 = {j|aj

ik > 0 − bj
ik
aj
ik
ik < 0 − bj
C4 = {j|aj
ik
aj
ik
It is easy to show that when j ∈ C1∪C6 where the operator ∪ denotes the union of sets  aj
ikw+bj
ik >
0 holds for w ∈ [cik  eik]  corresponding to the set of data points with non-zero loss. Oppositely
when j ∈ C3 ∪ C4  the values of the corresponding losses become zero since aj
ik ≤ 0 holds
for w ∈ [cik  eik]. The variation lies in the data points with indices j ∈ C2 ∪ C5. We sort sequence
ik|j ∈ C2} and record it in a vector u of length du with u1 ≤ . . . ≤ udu. Moreover  we also
{−bj
keep a index mapping qu with its rth element qu
ik. Similarly 
for sequence {−bj
ik|j ∈ C5}  we deﬁne a sorted vector v of length dv and the corresponding
index mapping qv. By using the merge-sort algorithm  we merge u and v into a sorted vector s and
then we add cik and eik into s as the minimum and maximum elements if they are not contained in
s. Obviously  in range [sl  sl+1] where sl is the lth element of s and ds is the length of s  problem
(3) becomes a univariate QP problem which has an analytical solution. So we can compute local
minimums in successive regions [sl  sl+1] (l = 1  . . .   ds − 1) and get the global minimum over
region [cik  eik] by comparing all local optima. The key operation is to compute the coefﬁcients
of quadratic function over each region [sl  sl+1] and we devise an algorithm in Table 1 which only
needs to scan s once  leading to an efﬁcient solution for problem (3).

ik > 0  cik ≤ − bj
ik
aj
ik
ik < 0  cik ≤ − bj
ik
aj
ik

r = j if ur = −bj

≤ eik}  C3 = {j|aj

≤ eik}  C6 = {j|aj

< cik}  C5 = {j|aj

r deﬁned as qu

ikw + bj

> eik}.

> eik}

ik/aj

ik/aj

ik/aj

3

The ﬁrst step of the algorithm in Table 1 needs O(ni)
time complexity to construct the six sets C1 to C6. In step
2  we need to sort two sequences to obtain u and v in
O(du ln du + dv ln dv) time and merge two sequences to
get s in O(du + dv). Then it costs O(ni) to calculate
coefﬁcients c0 and c1 by scanning C1  C2 and C6 in step
4 and 5. Then from step 6 to step 13  we need to scan
vector s once which costs O(du + dv) time. The overall
complexity of the algorithm in Table 1 is O(du ln du +
dv ln dv + ni) which is at most O(ni ln ni) due to du +
dv ≤ ni.
For wii  the optimization problem is formulated as
s.t. wii ≥ ci 

(cid:88)

max(0  aj

i wii + bj
i )

ii +

w2

(4)

min
wii

λ2
2

j∈Ti

(cid:80)

Table 1: Algorithm for problem (3)
01: Construct four sets C1  C2  C3  C4  C5 and C6;
02: Construct u  qu  v  qv and s;
03: Insert cik and eik into s if needed;
bj
ik;
ik − βik;
aj

04: c0 :=(cid:80)
05: c1 :=(cid:80)

j∈C1∪C2∪C6
j∈C1∪C2∪C6

06: w := sds ;
07: o := c0 + c1w + λw2/2;

for l = ds − 1 to 1
c0 := c0 − b

if sl+1 = ur for some r

08:

end if
if sl+1 = vr for some r

qu
r

ik ; c1 := c1 − a

qu
ik ;
r

end if

c0 := c0 + b

qv
09:
ik ; c1 := c1 + a
r
10: w0 := min(sl+1  max(sl  − c1
11:

o0 := c0 + c1w0 + λw2
if o0 < o

0/2;

λ ));

qv
ik ;
r

w := w0; o := o0;

12:

13:

end if
l := l − 1;

i = −yj ˆxji  bj

i = 1 − yj

end for

ii +(cid:80)

where aj
t(cid:54)=i wit ˆxjt  ci =
max(0  maxj(cid:54)=i(|wij|))  and |·| denotes the absolute val-
ue of a scalar. The main difference between problem (3)
and (4) is that there exist a box constraint for wik in problem (3) but in problem (4) wii is only
lower-bounded. We deﬁne ei as ei = maxj{− bj
} for all aj
i (cid:54)= 0. For wii ∈ [ei  +∞)  the objective
i
aj
i > 0}
i ) where S = {j|aj
i
j∈S (aj
function of problem (4) can be reformulated as λ2
2 w2
and the minimum value in [ei  +∞) will take at w(1)
}. Then we can use the
ii = max{ei −
algorithm in Table 1 to ﬁnd the minimizor w(2)
in the interval [ci  ei] for problem (4). Finally we
ii
ii } by comparing the corresponding
can choose the optimal solution to problem (4) from {w(1)
values of the objective function.
Since the complexity to solve both problem (3) and (4) is O(ni ln ni)  the complexity of one update
i=1 ni ln ni). Usually the coordinate descent algorithm converges
very fast in a small number of iterations and hence the whole algorithm to solve problem (2) or (1)
is very efﬁcient.
We can use other loss functions for problem (2) instead of hinge loss  e.g.  square loss l(s  t) =
(s − t)2 as in the least square SVM [10]. It is easy to show that problem (3) has an analytical
and the solution to problem (4) can be
solution as wik = min

for the whole matrix W is O(m(cid:80)m

(cid:80)
i wii + bj
j∈S aj
λ2

ii   w(2)

(cid:18)

(cid:19)

(cid:19)

max

i

βik−2(cid:80)
λ+2(cid:80)

  eik

aj
ikbj
ik
(aj
ik)2
. Then the computational complexity of the whole

computed as wii = max
algorithm to solve problem (2) by adopting square loss is O(mn).

ci 

j∈Ti
j∈Ti

(cid:19)

j∈Ti
j∈Ti
aj
i bj
i
(aj
i )2

(cid:18)
−2(cid:80)
λ2+2(cid:80)

cik 

(cid:18)

3 A Multi-Task Local Regressor based on Heterogeneous Neighborhood

In this section  we consider the situation that each task is a regression problem with each label
yi ∈ R.
Similar to the classiﬁcation case in the previous section  one candidate for multi-task local regressor
is a generalization of kernel regression  a counterpart of KNN classiﬁer for regression problems  and
the estimation function can be formulated as

(cid:80)
(cid:80)

f (xi) =

j∈Nk(i) wti tj s(i  j)yj
j∈Nk(i) wti tj s(i  j)

(5)

where wqr also represents the contribution of Tr to Tq. Since the denominator of f (xi) is a linear
combination of elements in each row of W with data-dependent combination coefﬁcients  if we
utilize a similar formulation to problem (1) with square loss  we need to solve a complex and non-
convex fractional programming problem. For computational consideration  we resort to another way
to construct the multi-task local regressor.

4

(cid:80)m

(cid:16)(cid:80)

(cid:17)

that

Recall

j=1 wtij

k (i) s(i  l)yl

the estimation function for the classiﬁcation case is formulated as f (xi) =
. We can see that the expression in the brackets on the right-hand
l∈N j
side can be viewed as a prediction for xi based on its neighbors in the jth task. Inspired by this
observation  we can construct a prediction ˆyi
j for xi based on its neighbors from the jth task by
utilizing any regressor  e.g.  kernel regression and support vector regression. Here due to the local
nature of our proposed method  we choose the kernel regression method  which is a local regression
s(i l)yl
s(i l) . When j equals
method  as a good candidate and hence ˆyi
ti which means we use neighbored data points from the task that xi belongs to  we can use this
prediction in conﬁdence. However  if j (cid:54)= ti  we cannot totally trust the prediction and need to add
some weight wti j as a conﬁdence. Then by using the square loss  we formulate an optimization
problem to get the estimation function f (xi) based on {ˆyi
wti j(y − ˆyi

j is formulated as ˆyi

(cid:80)m
j} as
(cid:80)m

f (xi) = arg min

l∈N j
k
l∈N j
k

m(cid:88)

(cid:80)
(cid:80)

j)2 =

j =

(6)

(i)

(i)

.

j

j=1 wti j ˆyi
j=1 wti j

y

j=1

summation of W to be 1  i.e.  (cid:80)m

Compared with the regression function of the direct extension of kernel regression to multi-task
learning in Eq. (5)  the denominator of our proposed regressor in Eq. (6) only includes the row
summation of W  making the optimization problem easier to solve as we will see later. Since the
scale of wij does not matter the value of the estimation function in Eq. (6)  we constrain the row
j=1 wij = 1 for i = 1  . . .   m. Moreover  the estimation ˆyi
(cid:80)
ti
by using data from the same task as xi is more trustful than the estimations based on other tasks 
which suggests wii should be the largest among elements in the ith row. Then this constraint implies
that wii ≥ 1
m > 0. To capture the negative task correlations  wij (i (cid:54)= j) is only
required to be a real scalar and wij ≥ −wii. Combining the above consideration  we formulate an
m(cid:88)
(cid:88)
optimization problem as

k wik = 1

m

(cid:107)W(cid:107)2

F s.t. W1 = 1  wii ≥ wij ≥ −wii 

(wiˆyj − yj)2 +

(cid:107)W − WT(cid:107)2

F +

(7)

λ1
4

λ2
2

min
W

i=1

j∈Ti

where 1 denotes a vector of all ones with appropriate size and ˆyj = (ˆyj
section  we discuss how to optimize problem (7).

1  . . .   ˆyj

m)T . In the following

3.1 Optimization Procedure

Due to the linear equality constraints in problem (7)  we cannot apply a coordinate descent method
to update variables one by one in a similar way to problem (2). However  similar to the SMO
algorithm [15] for SVM  we can update two variables in one row of W at one time to keep the linear
equality constraints valid.
We update each row one by one and the optimization problem with respect to wi is formulated as

m(cid:88)
where A = 2(cid:80)
by setting the (i  i)th element to be 0  b = −2(cid:80)

j + λ1Ii

i + wibT

wiAwT

ˆyj ˆyT

min
wi

j∈Ti

s.t.

1
2

j=1

yj ˆyT
by setting its ith element to 0. We deﬁne the Lagrangian as

j∈Ti

wij = 1  −wii ≤ wij ≤ wii ∀j (cid:54)= i 
m + λ2Im  Im is an m × m identity matrix  Ii
m(cid:88)

(wii − wij)βj −(cid:88)

wij − 1) −(cid:88)

j − λ1cT

J =

1
2

wiAwT

i + wibT − α(

j=1

j(cid:54)=i

m is a copy of Im
i   and ci is the ith column of W

(wii + wij)γj.

j(cid:54)=i

(8)

(9)

(10)

(11)
(12)

The Karush-Kuhn-Tucker (KKT) optimality condition is formulated as
= wiaj + bj − α + βj − γj = 0  for j (cid:54)= i

= wiai + bi − α −(cid:88)

∂J
∂wij
∂J
∂wii
βj ≥ 0  (wii − wij)βj = 0 ∀j (cid:54)= i
γj ≥ 0  (wii + wij)γj = 0 ∀j (cid:54)= i 

k(cid:54)=i

(βk + γk) = 0

5

α. Moreover  Eq. (10) implies that wiai + bi = α +(cid:80)

where aj is the jth column of A and bj is the jth element of b. It is easy to show that βjγj = 0
for all j (cid:54)= i. When wij satisﬁes wij = wii  according to Eq. (12) we have γj = 0 and further
wiaj + bj = α − βj ≤ α according to Eq. (9). When wij = −wii  based on Eq. (11) we can
get βj = 0 and then wiaj + bj = α + γj ≥ α. For wij between those two extremes (i.e. 
−wii < wij < wii)  γj = βj = 0 according to Eqs. (11) and (12)  which implies that wiaj + bj =
k(cid:54)=i(βk + γk) ≥ α. We deﬁne sets as
S1 = {j|wij = wii  j (cid:54)= i}  S2 = {j| − wii < wij < wii}  S3 = {j|wij = −wii}  and S4 = {i}.
Then a feasible wi is a stationary point of problem (8) if and only if maxj∈S1∪S2{wiaj + bj} ≤
mink∈S2∪S3∪S4{wiak + bk}. If there exist a pair of indices (j  k)  where j ∈ S1 ∪ S2 and k ∈
S2 ∪ S3 ∪ S4  satisfying wiaj + bj > wiak + bk  {j  k} is called a violating pair. If the current
estimation wi is not an optimal solution  there should exist some violating pairs. Our SMO algorithm
updates a violating pair at one step by choosing the most violating pair {j  k} with j and k deﬁned
as j = arg maxl∈S1∪S2{wial + bl} and k = arg minl∈S2∪S3∪S4{wial + bl}. We deﬁne the update
rule for wij and wik as ˜wij = wij + t and ˜wik = wik − t. By noting that j cannot be i  t should
satisfy the following constraints to make the updated solution feasible:

when k = i  t − wik ≤ wij + t ≤ wik − t  t − wik ≤ wil ≤ wik − t ∀l (cid:54)= j&l (cid:54)= k
when k (cid:54)= i  −wii ≤ wij + t ≤ wii  −wii ≤ wik − t ≤ wii.

When k = i  there will be a constraint on t as t ≤ e ≡ min(cid:0) wik−wij

  minl(cid:54)=j&l(cid:54)=k(wik − |wil|)(cid:1)

and otherwise t will satisfy c ≤ t ≤ e where c = max(wik − wii −wij − wii) and e = min(wii −
wij  wii + wik). Then the optimization problem for t can be uniﬁed as

2

t2 + (wiaj + bj − wiai − bi)t

s.t. c ≤ t ≤ e 

(cid:16)

min

t

(cid:16)

where for the case that k = i  c is set to be −∞. This problem has an analytical solution as
. We update each row of W one by one until convergence.
t = min

c  wiai+bi−wiaj−bj

e  max

ajj +aii−2aji

ajj + aii − 2aji

2

(cid:17)(cid:17)

4 Experiments

In this section  we test the empirical performance of our proposed methods in some toy data and
real-world problems.

4.1 Toy Problems

and

(cid:21)

(cid:21)

(cid:20) 0.1014

(cid:20) 0.1025

We ﬁrst use one UCI dataset  i.e.  diabetes data  to analyze the learned W matrix. The diabetes data
consist of 768 data points from two classes. We randomly select p percent of data points to form
the training set of two learning tasks respectively. The regularization parameters λ1 and λ2 are ﬁxed
as 1 and the number of nearest neighbors is set to 5. When p = 20 and p = 40  the means of the
estimated W over 10 trials are
. This result shows
wij (j (cid:54)= i) is very close to wii for i = 1  2. This observation implies our method can ﬁnd that these
two tasks are positive correlated which matches our expectation since those two tasks are from the
same distribution.
For the second experiment  we randomly select p percent of data points to form the training set
of two learning tasks respectively but differently we ﬂip the labels of one task so that those two
tasks should be negatively correlated. The matrices W’s learned for p = 20 and p = 40 are
. We can see that wij (j (cid:54)= i) is very close

(cid:20) 0.1019 −0.0999

(cid:20) 0.1019 −0.1017

0.1004
0.1010

0.1011
0.1056

0.0980

0.1010

(cid:20) 0.1575

to −wii for i = 1  2  which is what we expect.
As the third problem  we construct two learning tasks as in the ﬁrst one but ﬂip 50% percent of the
(cid:21)
class labels in each class of those two tasks. Here those two tasks can be viewed as unrelated tasks
since the label assignment is random. The estimated matrices W’s for p = 20 and p = 40 are
  where wij (i (cid:54)= j) is much smaller than wii. From
the structure of the estimations  we can see that those two tasks are more likely to be unrelated 
matching our expectation. In summary  our method can learn the positive correlations  negative
correlations and task unrelatedness for those toy problems.

(cid:20) 0.1015 −0.0003

0.0144
0.1281

0.0398

0.0081

0.1077

−0.1007

−0.0997

0.1012

0.1038

(cid:21)

and

(cid:21)

(cid:21)

and

6

4.2 Experiments on Classiﬁcation Problems

2

2σ2

Letter

0.0775±0.0053
0.0511±0.0053
0.0505±0.0038
0.0466±0.0023
0.0494±0.0028

USPS

0.0445±0.0131
0.0141±0.0038
0.0140±0.0025
0.0114±0.0013
0.0124±0.0014

Table 2: Comparison of classiﬁcation errors of different
methods on the two classiﬁcation problems in the form of
mean±std.
KNN
mtLMNN
MTFL
MT-KNN(hinge)
MT-KNN(square)

Two multi-task classiﬁcation prob-
lems are used in our experiments.
The ﬁrst problem we investigate is
a handwritten letter classiﬁcation ap-
plication consisting of seven tasks
each of which is to distinguish t-
wo letters. The corresponding letter-
s for each task to classify are: c/e 
g/y  m/n  a/g  a/o  f/t and h/n. Each
class in each task has about 1000 data
points which have 128 features corre-
sponding to the pixel values of hand-
written letter images. The second one is the USPS digit classiﬁcation problem and it consists of nine
binary classiﬁcation tasks each of which is to classify two digits. Each task contains about 1000 data
points with 255 features for each class.
Here the similarity function we use is a heat
kernel s(i  j) = exp{−(cid:107)xi−xj(cid:107)2
} where σ
is set
to the mean pairwise Euclidean dis-
tance among training data. We use 5-fold
cross validation to determine the optimal λ1
and λ2 whose candidate values are chosen
from n × {0.01  0.1  0.5  1  5  10  100} and the
optimal number of nearest neighbors from
{5  10  15  20}. The classiﬁcation error is used
as the performance measure. We compare our
method  which is denoted as MT-KNN  with
the KNN classiﬁer which is a single-task learn-
ing method  the multi-task large margin nearest
neighbor (mtLMNN) method [14]1 which is a
multi-task local learning method based on the
homogeneous neighborhood  and the multi-task
feature learning (MTFL) method [2] which is a
global method for multi-task learning. By uti-
lizing hinge and square losses  we also consider two variants of our MT-KNN method. To mimic
the real-world situation where the training data are usually limited  we randomly select 20% of the
whole data as training data and the rest to form the test set. The random selection is repeated for 10
times and we record the results in Table 2. From the results  we can see that our method MT-KNN
is better than KNN  mtLMNN and MTFL methods  which demonstrates that the introduction of the
heterogeneous neighborhood is helpful to improve the performance. For different loss functions
utilized by our method  MT-KNN with hinge loss is better than that with square loss due to the
robustness of the hinge loss against the square loss.
For those two problems  we also compare our proposed coordinate descent method described in
Table 1 with some off-the-shelf solvers such as the CVX solver [11] with respect to the running
time. The platform to run the experiments is a desktop with Intel i7 CPU 2.7GHz and 8GB RAM
and we use Matlab 2009b for implementation and experiments. We record the average running time
over 100 trials in Figure 2 and from the results we can see that on the classiﬁcation problems above 
our proposed coordinate descent method is much faster than the CVX solver which demonstrates
the efﬁciency of our proposed method.

Figure 2: Comparison on average running time
over 100 trials between our proposed coordinate
descent methods and the CVX solver on classiﬁ-
cation and regression problems.

4.3 Experiments on Regression Problems

Here we study a multi-task regression problem to learn the inverse dynamics of a seven degree-of-
freedom SARCOS anthropomorphic robot arm.2 The objective is to predict seven joint torques based

1http://www.cse.wustl.edu/˜kilian/code/files/mtLMNN.zip
2http://www.gaussianprocess.org/gpml/data/

7

LetterUSPSRobot00.10.20.30.40.50.60.70.8Dataset Running Time (in second) Our MethodCVX Solveron 21 input features  corresponding to seven joint positions  seven joint velocities and seven joint
accelerations. So each task corresponds to the prediction of one torque and can be formulated as a
regression problem. Each task has 2000 data points. The similarity function used here is also the heat
kernel and 5-fold cross validation is used to determine the hyperparameters  i.e.  λ1  λ2 and k. The
performance measure used is normalized mean squared error (nMSE)  which is mean squared error
on the test data divided by the variance of the ground truth. We compare our method denoted by MT-
KR with single-task kernel regression (KR)  the multi-task feature learning (MTFL) under different
conﬁgurations on the size of the training set. Compared with KR and MTFL methods  our method
achieves better performance over different sizes of the training sets. Moreover  for our proposed
coordinate descent method introduced in section 3.1  we compare it with CVX solver and record
the results in the last two columns of Figure 2. We ﬁnd the running time of our proposed method is
much smaller than that of the CVX solver which demonstrates that the proposed coordinate descent
method can speed up the computation of our MT-KR method.

Figure 3: Comparison of different methods on the robot arm application when varying the size of
the training set.

4.4 Sensitivity Analysis

Here we test the sensitivity of the performance
with respect to the number of nearest neighbors.
By changing the number of nearest neighbors
from 5 to 40 at an interval of 5  we record the
mean of the performance of our method over 10
trials in Figure 4. From the results  we can see
our method is not very sensitive to the number
of nearest neighbors  which makes the setting
of k not very difﬁcult.

5 Conclusion

Figure 4: Sensitivity analysis of the performance
of our method with respect to the number of near-
est neighbors at different data sets.

In this paper  we develop local learning meth-
ods for multi-task classiﬁcation and regression
problems. Based on an assumption that all task
pairs contributes to each other almost equally 
we propose regularized objective functions and develop efﬁcient coordinate descent methods to
solve them. Up to here  each task in our studies is a binary classiﬁcation problem. In some applica-
tions  there may be more than two classes in each task. So we are interested in an extension of our
method to multi-task multi-class problems. Currently the task-speciﬁc weights are shared by all data
points from one task. One interesting research direction is to investigate a localized variant where
different data points have different task-speciﬁc weights based on their locality structure.

Acknowledgment

Yu Zhang is supported by HKBU ‘Start Up Grant for New Academics’.

8

0.10.20.300.020.040.060.08The size of training setnMSE KRMTFLMT−KR5101520253035400.010.020.030.040.050.06Number of NeighborsError LetterUSPSRobotReferences
[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-

beled data. Journal of Machine Learning Research  6:1817–1853  2005.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. In B. Sch¨olkopf  J. C. Platt  and
T. Hoffman  editors  Advances in Neural Information Processing Systems 19  pages 41–48  Vancouver 
British Columbia  Canada  2006.

[3] B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal of Machine

Learning Research  4:83–99  2003.

[4] J. Baxter. A Bayesian/information theoretic model of learning to learn via multiple task sampling. Ma-

chine Learning  28(1):7–39  1997.

[5] J. C. Bezdek and R. J. Hathaway. Convergence of alternating optimization. Neural  Parallel & Scientiﬁc

Computations  11(4):351–368  2003.

[6] E. Bonilla  K. M. A. Chai  and C. Williams. Multi-task Gaussian process prediction.

In J.C. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 153–160  Vancouver  British Columbia  Canada  2007.

[7] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation  4(6):888–900  1992.
[8] R. Caruana. Multitask learning. Machine Learning  28(1):41–75  1997.
[9] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceedings of the Tenth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 109–117  Seattle  Washing-
ton  USA  2004.

[10] T. V. Gestel  J. A. K. Suykens  B. Baesens  S. Viaene  J. Vanthienen  G. Dedene  B. De Moor  and J. Van-
dewalle. Benchmarking least squares support vector machine classiﬁers. Machine Learning  54(1):5–32 
2004.

[11] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming  2011.
[12] L. Jacob  F. Bach  and J.-P. Vert. Clustered multi-task learning: a convex formulation.

In D. Koller 
D. Schuurmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information Processing Systems
21  pages 745–752  Vancouver  British Columbia  Canada  2008.

[13] A. Kumar and H. Daum´e III. Learning task grouping and overlap in multi-task learning. In Proceedings

of the 29 th International Conference on Machine Learning  Edinburgh  Scotland  UK  2012.

[14] S. Parameswaran and K. Weinberger. Large margin multi-task metric learning. In J. Lafferty  C. K. I.
Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta  editors  Advances in Neural Information Process-
ing Systems 23  pages 1867–1875  2010.

[15] J. C. Platt.

Fast training of support vector machines using sequential minimal optimization.

In
B. Sch¨olkopf  C. J. C. Burges  and A. J. Smola  editors  Advances in Kernel Methods: Support Vector
Learning. MIT Press  1998.

[16] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In D. S. Touretzky  M. Mozer  and
M. E. Hasselmo  editors  Advances in Neural Information Processing Systems 8  pages 640–646  Denver 
CO  1995.

[17] S. Thrun and J. O’Sullivan. Discovering structure in multiple learning tasks: The TC algorithm.

In
Proceedings of the Thirteenth International Conference on Machine Learning  pages 489–497  Bari  Italy 
1996.

[18] M. Wu and B. Sch¨olkopf. A local learning approach for clustering. In B. Sch¨olkopf  J. C. Platt  and
T. Hoffman  editors  Advances in Neural Information Processing Systems 19  pages 1529–1536  Vancou-
ver  British Columbia  Canada  2006.

[19] M. Wu  K. Yu  S. Yu  and B. Sch¨olkopf. Local learning projections. In Proceedings of the Twenty-Fourth

International Conference on Machine Learning  pages 1039–1046  Corvallis  Oregon  USA  2007.

[20] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning.
In Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence  pages 733–742  Catalina
Island  California  2010.

9

,Yu Zhang
Nesime Tatbul
Tae Jun Lee
Stan Zdonik
Mejbah Alam
Justin Gottschlich