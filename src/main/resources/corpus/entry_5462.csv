2011,The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers,We derive here new generalization bounds  based on Rademacher Complexity theory  for model selection and error estimation   of linear (kernel) classifiers  which exploit  the availability of unlabeled samples.   In particular  two results are obtained: the first one shows that  using the unlabeled samples  the  confidence term of the conventional bound can be reduced by a factor of three; the second one  shows that the unlabeled samples can be used to obtain much tighter bounds  by building localized versions  of the hypothesis class containing the optimal classifier.,The Impact of Unlabeled Patterns in Rademacher

Complexity Theory for Kernel Classiﬁers

Davide Anguita  Alessandro Ghio  Luca Oneto  Sandro Ridella

Department of Biophysical and Electronic Engineering

University of Genova

Via Opera Pia 11A  I-16145 Genova  Italy

{Davide.Anguita Alessandro.Ghio} @unige.it

{Luca.Oneto Sandro.Ridella} @unige.it

Abstract

We derive here new generalization bounds  based on Rademacher Complexity the-
ory  for model selection and error estimation of linear (kernel) classiﬁers  which
exploit the availability of unlabeled samples.
In particular  two results are ob-
tained: the ﬁrst one shows that  using the unlabeled samples  the conﬁdence term
of the conventional bound can be reduced by a factor of three; the second one
shows that the unlabeled samples can be used to obtain much tighter bounds  by
building localized versions of the hypothesis class containing the optimal classi-
ﬁer.

1

Introduction

Understanding the factors that inﬂuence the performance of a statistical procedure is a key step for
ﬁnding a way to improve it. One of the most explored procedures in the machine learning approach
to pattern classiﬁcation aims at solving the well–known model selection and error estimation prob-
lem  which targets the estimation of the generalization error and the choice of the optimal predictor
from a set of possible classiﬁers. For reaching this target  several approaches have been proposed
[1  2  3  4]  which provide an upper bound on the generalization ability of the classiﬁer  which can
be used for model selection purposes as well. Typically  all these bounds consists of three terms:
the ﬁrst one is the empirical error of the classiﬁer (i.e. the error performed on the training data) 
the second term is a bias that takes into account the complexity of the class of functions  which the
classiﬁer belongs to  and the third one is a conﬁdence term  which depends on the cardinality of the
training set. These approaches are quite interesting because they investigate the ﬁnite sample behav-
ior of a classiﬁer  instead of the asymptotic one  even though their practical applicability has been
questioned for a long time1. One of the most recent methods for obtaining these bounds is to exploit
the Rademacher Complexity  which is a powerful statistical tool that has been deeply investigated
during the last years [5  6  7]. This approach has shown to be of practical use  by outperforming more
traditional methods [8  9] for model selection in the small–sample regime [10  5  6]  i.e. when the
dimensionality of the samples is comparable  or even larger  than the cardinality of the training set.
We show in this work how its performance can be further improved by exploiting some extra knowl-
edge on the problem. In fact  real–world classiﬁcation problems often are composed of datasets
with labeled and unlabeled data [11  12]: for this reason an interesting challenge is ﬁnding a way to
exploit the unlabeled data for obtaining tighter bounds and  therefore  better error estimations.

In this paper  we present two methods for exploiting the unlabeled data in the Rademacher Com-
plexity theory [2]. First  we show how the unlabeled data can have a role in reducing the conﬁdence

1See  for example  the NIPS 2004 Workshop (Ab)Use of Bounds or the 2002 Neurocolt Workshop on Bounds

less than 0.5

1

term  by obtaining a new bound that takes into account both labeled and unlabeled data. Then  we
propose a method  based on [7]  which exploits the unlabeled data for selecting a better hypothesis
space  which the classiﬁer belongs to  resulting in a much sharper and accurate bound.

2 Theoretical framework and results

1  Y l

nl   Y l

1 ) ···   (X l

We consider the following prediction problem: based on a random observation of X ∈ X ⊆ Rd
one has to estimate Y ∈ Y ⊆ {−1  1} by choosing a suitable prediction rule f : X → [−1  1].
The generalization error L(f ) = E{X  Y}ℓ(f (X)  Y ) associated to the prediction rule is deﬁned
through a bounded loss function ℓ(f (X)  Y ) : [−1  1] × Y → [0  1]. We observe a set of labeled
nu )(cid:9).
nl )(cid:9) and a set of unlabeled ones Dnu :(cid:8)(X u
samples Dnl :(cid:8)(X l
The data consist of a sequence of independent  identically distributed (i.i.d.) samples with the same
distribution P (X  Y) for Dnl and Dnu. The goal is to obtain a bound on L(f ) that takes into
account both the labeled and unlabeled data. As we do not know the distribution that have generated
the data  we do not know L(f ) but only its empirical estimation Lnl (f ) = 1/nlPnl
i ).
i )  Y l
In the typical context of Structural Risk Minimization (SRM) [13] we deﬁne an inﬁnite sequence of
hypothesis spaces of increasing complexity {Fi  i = 1  2 ··· }  then we choose a suitable function
space Fi and  consequently  a model f∗ ∈ Fi that ﬁts the data. As we do not know the true data
distribution  we can only say that:
(1)

1 ) ···   (X u

i=1 ℓ(f (X l

{L(f ) − Lnl (f )}f∈Fi ≤ sup

f∈Fi {L(f ) − Lnl (f )}

or  equivalently:

L(f ) ≤ Lnl (f ) + sup

f∈Fi {L(f ) − Lnl (f )}  

∀f ∈ Fi

(2)

In this framework  the SRM procedure brings us to the following choice of the function space and
the corresponding optimal classiﬁer:

f∗ F∗ :

arg

Fi∈{F1 F2 ··· }" min

f∈Fi

min

Lnl (f )f∈Fi + sup

f∈Fi {L(f ) − Lnl (f )}#

(3)

Since the generalization bias (supf∈Fi {L(f ) − Lnl (f )}) is a random variable  it is possible to
statistically analyze it and obtain a bound that holds with high probability [5].

From this point  we will consider two types of prediction rule with the associated loss function:
1 − yfH (x)

fH (x) =sign(wT φ(x) + b) 

ℓH (fH (x)  y) =

(4)

if wT φ(x) + b > 0
if wT φ(x) + b ≤ 0

fS(x) =(cid:26)min(1  wT φ(x) + b)
max(−1  wT φ(x) + b)
where φ(·) : Rd → RD with D >> d  w ∈ RD and b ∈ R. The function φ(·) is introduced to
allow for a later introduction of kernels  even though  for simplicity  we will focus only on the linear
case. Note that both the hard loss ℓH (fH (x)  y) and the soft loss (or ramp loss) [14] ℓS(fS(x)  y)
are bounded ([0  1]) and symmetric (ℓ(f (x)  y) = 1 − ℓ(f (x) −y)). Then  we recall the deﬁnition
of Rademacher Complexity (R) for a class of functions F:

1 − yfS(x)

ℓS(fS(x)  y) =

(5)

2

 

2

ˆRnl (F) = Eσ sup
f∈F

2
nl

σiℓ(f (xi)  yi) = Eσ sup
f∈F

1
nl

nl

Xi=1

nl

Xi=1

σif (xi)

(6)

where σ1  . . .   σnl are nl independent Rademacher random variables  i.e. independent random vari-
ables for which P(σi = +1) = P(σi = −1) = 1/2  and the last equality holds if we use one
of the losses deﬁned before. Note that ˆR is a computable realization of the expected Rademacher
ˆR(F). The most renowed result in Rademacher Complexity theory
Complexity R(F) = E(X  Y)
states that [2]:

L(f )f∈F ≤ Lnl (f )f∈F + ˆRnl (F) + 3s log(cid:0) 2
δ(cid:1)

2nl

(7)

which holds with probability (1 − δ) and allows to solve the problem of Eq. (3).

2

2.1 Exploiting unlabeled samples for reducing the conﬁdence term

Assuming that the amount of unlabeled data is larger than the number of labeled samples  we split
them in blocks of similar size by deﬁning the quantity m = ⌊nu/nl⌋ + 1  so that we can consider a
composed of mnl pattern. Then  we can upper bound the expected generaliza-
ghost sample D′mnl
tion bias in the following way 2:

E{X  Y} sup

1
m

E{X ′ Y′}
f∈F
f∈F {L(f ) − Lnl (f )} = E{X  Y} sup

Xi=1
f∈F
Xi=1

Xk=(i−1)·nl+1

= E{X  Y}E{X ′ Y′}Eσ

≤ E{X  Y}E{X ′ Y′}

≤ E{X  Y}Eσ

Xi=1

1
m

1
m

1
m

2
nl

sup

i·nl

m

m

m

sup

f∈F


m

nl

i·nl

i·nl

sup

1
nl

1
nl

1
nl

ℓ′k
 −

ℓi
Xi=1
Xk=(i−1)·nl+1

Xk=(i−1)·nl+1(cid:16)ℓ′k − ℓ|k|nl(cid:17)

σ|k|nlhℓ′k − ℓ|k|nli

ˆRi
nl (F)

Xi=1
f∈F

Xk=(i−1)·nl+1
ℓk
 = E{X  Y}

Xi=1

σ|k|nl

1
m

1
nl

i·nl

m

where |k|nl = (k− 1) mod (nl) + 1. The last quantity (that we call Expected Extended Rademacher
ˆRnu (F)) and the expected generalization bias are both deterministic quantities
Complexity E{X  Y}
and we know only one realization of them  dependent on the sample. Then  we can use the McDi-
armid’s inequality [15] to obtain:

P"sup
f∈F {L(f ) − Lnl (f )} ≥ E{X  Y} sup

f∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F) + ǫ# ≤
P"sup
f∈F {L(f ) − Lnl (f )} + aǫ# +
ˆRnu (F) ≥ ˆRnu (F) + (1 − a)ǫi ≤

PhE{X  Y}

(mnl)

e−2nla2ǫ2

+ e−

2

(1−a)2ǫ2

with a ∈ [0  1]. By choosing a =

√m
2+√m   we can write:

P"sup
f∈F {L(f ) − Lnl (f )} ≥

1
m

m

Xi=1

nl (F) + ǫ# ≤ 2e−
ˆRi

2mnl ǫ2
(2+√m)2

and obtain an explicit bound which holds with probability (1 − δ):

L(f )f∈F ≤ Lnl (f )f∈F +

1
m

m

Xi=1

ˆRi
nl (F) +

2 + √m

√m s log(cid:0) 2
δ(cid:1)

2nl

(8)

(9)

(10)

(11)

(12)

(13)

where ˆRi
nl (F) is the Rademacher Complexity of the class F computed on the i-th block of
unlabeled data. Note that for m = 1 the training set does not contain any unlabeled data
and the bound given by Eq.
(3) is recovered  while for large m the conﬁdence term is re-
duced by a factor of 3. At a ﬁrst sight  it would seem impossible to compute the term ˆRi
nl
In
without knowing the labels of the data  but it is easy to show that this is not the case.
= +1o and K−i =
fact 

i = nk ∈ {k = (i − 1) · nl + 1  . . .   i · nl} : σ|k|nl

let us deﬁne K+

2we deﬁne ℓ(f (xi)  yi) ≡ ℓi to simplify the notation

3

(a) Coventional function classes

(b) Localized function classes

Figure 1: The effect of selecting a better center for the hypothesis classes.

m

1
m

2

Eσ sup
f∈F

ˆRnu (F) = 1 +

Xi=1
Xi=1

= −1o  then we have:
nk ∈ {k = (i − 1) · nl + 1  . . .   i · nl} : σ|k|nl
nl 
ℓ(fk  yk) − Xk∈K−i
 Xk∈K+
f∈F
nl Xk∈K+
ℓ(fk −yk) −
−
f∈F
Xi=1
Xk=(i−1)·nl+1
−
f∈F
Xi=1
= 1 −


Eσ sup

Eσ sup

Eσ inf

= 1 +

= 1 +

i

2
nl

m

1
m

m

1
m

m

1
m

i

2

i

2

1
ℓ(fk  yk) − Xk∈K+

ℓ(fk  yk)
nl Xk∈K−i

yk)
ℓ(fk −σ|k|nl

)


ℓ(fk  σ|k|nl

i·nl

2
nl

i·nl

Xk=(i−1)·nl+1

which corresponds to solving a classiﬁcation problem using all the available data with random labels.
The expectation can be easily computed with some Monte Carlo trials.

2.2 Exploiting the unlabeled data for tightening the bound

Another way of exploiting the unlabeled data is to use them for selecting a more suitable sequence of
hypothesis spaces. For this purpose we could use some of the unlabeled samples or  even better  the
nc = nu − ⌊nu/nl⌋ nl samples left from the procedure of the previous section. The idea is inspired
by the work of [3] and [7]  which propose to inﬂate the hypothesis classes by centering them around
a ‘good’ classiﬁer. Usually  in fact  we have no a-priori information on what can be considered a
good choice of the class center  so a natural choice is the origin [13]  as in Figure 1(a). However 
if it happens that the center is ‘close’ to the optimal classiﬁer  the search for a suitable class will
stop very soon and the resulting Rademacher Complexity will be consequently reduced (see Figure
1(b)). We propose here a method for ﬁnding two possible ‘good’ centers for the hypothesis classes.
Let us consider nc unlabeled samples and run a clustering algorithm on them  by setting the number
of clusters to 2  and obtaining two clusters C1 and C2. We build two distinct labeled datasets by
assigning the labels +1 and −1 to C1 and C2  respectively  and then vice-versa. Finally  we build
two classiﬁers fC1(x) and fC2(x) = −fC1(x) by learning the two datasets3. The two classiﬁers 
which have been found using only unlabeled samples  can then be used as centers for searching
a better hypothesis class.
It is worthwhile noting that any supervised learning algorithm can be
used [16]  because the centers are only a hint for a better centered hypothesis space: their actual
classiﬁcation performance is not of paramount importance. The underlying principle that inspired

3Note that we could build only one classiﬁer by assigning the most probable labels to the nc samples 
according to the nl labeled ones but  rigorously speaking  this is not allowed by the SRM principle  because
it would lead to use the same data for both choosing the space of functions and computing the Rademacher
Complexity.

4

this procedure relies on the reasonable hypothesis that P(X ) is correlated with P(X  Y): in fact  in
an unlucky scenario  where the two classes are heavily overlapped  the method would obviously fail.

Choosing a good center for the SRM procedure can greatly reduce the second term of the bound
given by Eq. (13) [7] (the bias or complexity term). Note  however  that the conﬁdence term is not
affected  so we propose here an improved bound  which makes this term depending on ˆRi
nl (F) as
well. We use a recent concentration result for Self Bounding Functions [17]  instead of the looser
McDiarmid’s inequality. The detailed proof is omitted due to space constraints and we give here
only the sketch (it is a more general version of the proof in [18] for Rademacher Complexities):

f∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F) + ǫ# ≤ e−2nla2ǫ2
P"sup

+ e−

(mnl)(1−a)2 ǫ2
ˆRnu (F )
2E{X  Y}

(14)

with a ∈ [0  1]. Choosing a =

√m
√m+2qE{X  Y}

1

m Pm

i=1

  we obtain:

ˆRi

nl

(F )

f∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F) + ǫ# ≤ 2e
P"sup

−

2mnl ǫ2

(√m+2√E{X  Y}

ˆRnu (F ))2

so that the following explicit bound holds with probability (1 − δ):

L(f )f∈F ≤ Lnl (f )f∈F + ˆRnu (F) +

2qE{X  Y}

ˆRnu (F) + √m
√m

s log(cid:0) 2
δ(cid:1)

2nl

(15)

(16)

ˆRnu (F) = 1 and we obtain again Eq. (13). Unfortunately  the
Note that  in the worst case  E{X  Y}
Expected Extended Rademacher Complexity cannot be computed  but we can upper bound it with
its empirical version (see  for example  [19]  pages 420–422  for a justiﬁcaton of this step) as in
Eq.(10) to obtain:

f∈F {L(f ) − Lnl (f )} ≥ ˆRnu (F) + ǫ# ≤ e−2nla2ǫ2
P"sup

+ e−

(mnl)(1−a)2 ǫ2

2( ˆRnu (F )+(1−a)ǫ)

(17)

with a ∈ [0  1]. Differently from Eq. (15) the previous expression cannot be put in explicit form  but
it can be simply computed numerically by writing it as:

L(f )f∈F ≤ Lnl (f )f∈F +

1
m

m

Xi=1

ˆRi
nl (F) + ǫb

u

(18)

u can be obtained by upper bounding with δ the last term of Eq. (17) and solving the

The value ǫb
inequality respect to a and ǫ  so that the bound holds with probability (1 − δ).
We can show the improvements obtained through these new results  by plotting the values of the
conﬁdence terms and comparing them with the conventional one [2]. Figure 2 shows the value of
u  as a function of the number of
ǫl in Eq. (7) against ǫu  the corresponding term in Eq. (13)  and ǫb
samples.

3 Performing the Structural Risk Minimization procedure

Computing the values of the bounds described in the previous sections is a straightforward process 
at least in theory. The empirical error Lnl (f ) is found by learning a classiﬁer with the original
labeled dataset  while the (Extended) Rademacher Complexity ˆRi
nl(F) is computed by learning the
dataset composed of both labeled and unlabeled samples with random labels.
In order apply in practice the results of the previous section and to better control the hypothesis
space  we formulate the learning phase of the classiﬁer as the following optimization problem  based

5

m ∈ [1 10]

m = 1  R ∈ [0 1]

1

0.9

0.8

0.7

0.6

ε

0.5

0.4

0.3

0.2

0.1

0

 

 

ε
l
ε
u

m = 1

m = 2

m = 10

40

60

80

100

120
n

140

160

180

200

1

0.9

0.8

0.7

0.6

ε

0.5

0.4

0.3

0.2

0.1

0

 

 

ε
u
εb
u

R = 1

R = 0.9

R = 0

40

60

80

100

120
n

140

160

180

200

(a) ǫl VS ǫu

(b) ǫnl VS ǫb

u with m = 1

Figure 2: Comparison of the new conﬁdence terms with the conventional one.

on the Ivanov version of the Support Vector Machine (I-SVM) [13]:

min
w b ξ

n

ηi

Xi=1
kw − ˆwk2 ≤ ρ2
yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1 − ξi
ξi ≥ 0 

ηi = min (2  ξi)

(19)

(20)

where the size of the hypothesis space  centered in ˆw  is controlled by the hyperparameter ρ and
the last constraint is introduced for bounding the SVM loss function  which would be otherwise
unbounded and would prevent the application of the theory developed so far. Note that  in practice 
two sub-problems must be solved: the ﬁrst one with ˆw = + ˆwC1 and the second one with ˆw =
− ˆwC1  then the solution corresponding to the smaller value of the objective function is selected.
Unfortunately  solving a classiﬁcation problem with a bounded loss function is computationally in-
tractable  because the problem is no longer convex and even state-of-the-art solvers like  for example 
CPLEX [20] fail to found an exact solution  when the training set size exceeds few tens of samples.
Therefore  we propose here to ﬁnd an approximate solution through well–known algorithms like 
for example  the Peeling [6] or the Convex–Concave Constrained Programming (CCCP) technique
[14  21  22]. Furthermore  we derive a dual formulation of problem (19) that allows us exploiting
the well known Sequential Minimal Optimization (SMO) algorithm for SVM learning [23].

Problem (19) can be rewritten in the equivalent Tikhonov formulation:

min
w b ξ

n

ηi

Xi=1

1
2kw − ˆwk2 + C
yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1 − ξi
ξi ≥ 0 

ηi = min (2  ξi)

which gives the same solution of the Ivanov formulation for some value of C [13]. The method
for ﬁnding the value of C  corresponding to a given value of ρ  is reported in [10]  where it is also
shown that C cannot be used directly to control the hypothesis space. Then  it is possible to apply
the CCCP technique  which is synthesized in Algorithm 1  by splitting the objective function in its
convex and concave parts:

min
w b ξ

Jconvex(θ)

Jconcave(θ)

n

n

ξi

}|

z
{
}|
z
1
Xi=1
2kw − ˆwk2 + C
−C
yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1 − ξi
ςi = max(0  ξi − 2)
ξi ≥ 0 

Xi=1

{

ςi

6

(21)

where θ = [w|b] is introduced to simplify the notation. Obviously  the algorithm does not guarantee
to ﬁnd the optimal solution  but it converges to a (usually good) solution in a ﬁnite number of steps
[14]. To apply the algorithm we must compute the derivative of the concave part of the objective
function:

(cid:18) dJconcave(θ)

dθ

Then  the learning problem becomes:

min
w b ξ

where

n

βiyi(cid:0)wT φ(xi) + b(cid:1)

n

dθ

Xi=1

d (−Cςi)

(cid:12)(cid:12)(cid:12)(cid:12)θt(cid:19) θ = n
Xi=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)θt! θ =
1
Xi=1
∆iyi(cid:0)wT φ(xi) + b(cid:1)
2kw − ˆwk2 + C
yi(cid:0)wT φ(xi) + b(cid:1) ≥ 1 − ξi 
ξi ≥ 0
∆i =(cid:26) C

if yif t(xt) < −1
otherwise

Xi=1

ξi +

0

n

Finally  it is possible to obtain the dual formulation (derivation is omitted due to lack of space):

βiβjyiyjK(xi  xj) +

min

β

n

n

1
2

Xj=1

Xi=1
− ∆i ≤ βi ≤ C − ∆i 

βiyi = 0

n

Xi=1

n

Xi=1

nC1

Xj=1




ˆαjyi ˆyjK(ˆxj  xi) − 1
 βi

where we have used the kernel trick [24] K(· ·) = φ(·)T φ(·).
4 A case study

(22)

(23)

(24)

(25)

We consider the MNIST dataset [25]  which consists of 62000 images  representing the numbers
from 0 to 9: in particular  we consider the 13074 patterns containing 0’s and 1’s  allowing us to deal
with a binary classiﬁcation problem. We simulate the small–sample regime by randomly sampling
a training set with low cardinality (nl < 500)  while the remaining 13074 − nl images are used as
a test set or as an unlabeled dataset  by simply discarding the labels. In order to build statistically
relevant results  this procedure is repeated 30 times.
In Table 1 we compare the conventional bound with our proposal. In the ﬁrst column the number
of labeled patterns (nl) is reported  while the second column shows the number of unlabeled ones
(nu). The optimal classiﬁer f∗ is selected by varying ρ in the range [10−6  1]  and selecting the
function corresponding to the minimum of the generalization error estimate provided by each bound.
Then  for each case  the selected f∗ is tested on the remaining 13074 − (nl + nu) samples and the
classiﬁcation results are reported in column three and four  respectively. The results show that the
f∗ selected by exploiting the unlabeled patterns behaves better than the other and  furthermore  the
estimated L(f )  reported in column ﬁve and six  shows that the bound is tighter  as expected by
theory.

The most interesting result  however  derives from the use of the new bound of Eq. (18)  as reported
in Table 2  where the unlabeled data is exploited for selecting a more suitable center of the hypoth-
esis space. The results are reported analogously to Table 1. Note that  for each experiment  30%

Algorithm 1 CCCP procedure

Initialize θ0
repeat

θt+1 = arg minθ Jconvex(θ) +(cid:16) dJconcave(θ)

until θt+1 = θt

dθ

(cid:12)(cid:12)(cid:12)θt(cid:17) θ

7

Table 1: Model selection and error estimation  exploiting unlabeled data for tightening the bound.

nl
10
20
40
60
80
100
120
150
170
200
250
300
400

nu
20
40
80
120
160
200
240
300
340
400
500
600
800

Test error of f∗

Eq. (7)

Eq. (13)

13.20 ± 0.86
8.93 ± 1.20
6.26 ± 0.16
5.95 ± 0.12
5.61 ± 0.07
5.36 ± 0.21
4.98 ± 0.40
4.41 ± 0.53
3.59 ± 0.57
2.75 ± 0.47
2.07 ± 0.03
2.02 ± 0.04
1.93 ± 0.02

12.40 ± 0.82
8.93 ± 1.29
6.02 ± 0.17
5.88 ± 0.13
5.30 ± 0.07
5.51 ± 0.22
5.36 ± 0.40
4.08 ± 0.51
3.40 ± 0.64
2.67 ± 0.48
2.05 ± 0.03
1.94 ± 0.04
1.79 ± 0.02

Estimated L(f )

Eq. (7)

Eq. (13)

194.00 ± 0.97
142.00 ± 1.06
103.00 ± 0.59
85.50 ± 0.48
73.70 ± 0.40
66.10 ± 0.37
61.30 ± 0.33
55.10 ± 0.28
52.40 ± 0.26
48.10 ± 0.19
42.70 ± 0.22
39.20 ± 0.17
34.90 ± 0.19

157.70 ± 0.97
116.33 ± 1.06
84.85 ± 0.59
70.68 ± 0.48
60.86 ± 0.40
54.62 ± 0.37
50.82 ± 0.33
45.73 ± 0.28
43.60 ± 0.26
39.98 ± 0.19
35.44 ± 0.22
32.57 ± 0.17
29.16 ± 0.19

Table 2: Model selection and error estimation  exploiting unlabeled data for selecting a more suitable
hypothesis center.

nl
7
14
28
42
56
70
84
105
119
140
175
210
280

nu
3
6
12
18
24
30
36
45
51
60
75
90
120

Test error of f∗

Eq. (7)

Eq. (18)

Estimated L(f )

Eq. (7)

Eq. (18)

13.20 ± 0.86
8.93 ± 1.20
6.26 ± 0.16
5.95 ± 0.12
5.61 ± 0.07
5.36 ± 0.21
4.98 ± 0.40
4.41 ± 0.53
3.59 ± 0.57
2.75 ± 0.47
2.07 ± 0.03
2.02 ± 0.04
1.93 ± 0.02

8.98 ± 1.12
5.10 ± 0.67
3.05 ± 0.23
2.36 ± 0.23
1.96 ± 0.14
1.63 ± 0.11
1.44 ± 0.11
1.27 ± 0.09
1.20 ± 0.08
1.08 ± 0.09
0.92 ± 0.05
0.81 ± 0.07
0.70 ± 0.06

219.15 ± 0.97
159.79 ± 1.06
115.58 ± 0.59
95.77 ± 0.48
82.59 ± 0.40
74.05 ± 0.37
68.56 ± 0.33
61.59 ± 0.28
58.50 ± 0.26
53.72 ± 0.19
47.73 ± 0.22
43.79 ± 0.17
38.88 ± 0.19

104.01 ± 1.62
86.70 ± 0.01
51.35 ± 0.00
38.37 ± 0.00
31.39 ± 0.00
26.83 ± 0.00
23.77 ± 0.00
20.36 ± 0.00
18.77 ± 0.00
16.82 ± 0.00
14.52 ± 0.00
12.91 ± 0.00
10.86 ± 0.00

of the data (nu) are used for selecting the hypothesis center and the remaining ones (nl) are used
for training the classiﬁer. The proposed method consistently selects a better classiﬁer  which reg-
isters a threefold classiﬁcation error reduction on the test set  especially for training sets of smaller
cardinality. The estimation of L(f ) is largely reduced as well.
We have to consider that this very clear performance increase is also favoured by the characteristics
of the MNIST dataset  which consists of well–separated classes: this particular data distribution im-
plies that only few samples sufﬁce for identifying a good hypothesis center. Many more experiments
with different datasets and varying the ratio between labeled and unlabeled samples are needed  and
are currently underway  for establishing the general validity of our proposal but  in any case  these
results appear to be very promising.

5 Conclusion

In this paper we have studied two methods which exploit unlabeled samples to tighten the
Rademacher Complexity bounds on the generalization error of linear (kernel) classiﬁers. The ﬁrst
method improves a very well–known result  while the second one aims at changing the entire ap-
proach by selecting more suitable hypothesis spaces  not only acting on the bound itself. The recent
literature on the theory of bounds attempts to obtain tighter bounds through more reﬁned concentra-
tion inequalities (e.g. improving Mc Diarmid’s inequality)  but we believe that the idea of reducing
the size of the hypothesis space is a more appealing ﬁeld of research because it opens the road to
possible signiﬁcant improvements.

References

[1] V.N. Vapnik and A.Y. Chervonenkis. On the uniform convergence of relative frequencies of

events to their probabilities. Theory of Probability and its Applications  16:264  1971.

8

[2] P.L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. The Journal of Machine Learning Research  3:463–482  2003.

[3] P.L. Bartlett  O. Bousquet  and S. Mendelson. Local rademacher complexities. The Annals of

Statistics  33(4):1497–1537  2005.

[4] O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning

Research  2:499–526  2002.

[5] P.L. Bartlett  S. Boucheron  and G. Lugosi. Model selection and error estimation. Machine

Learning  48(1):85–113  2002.

[6] D. Anguita  A. Ghio  and S. Ridella. Maximal discrepancy for support vector machines. Neu-

rocomputing  74(9):1436–1443  2011.

[7] D. Anguita  A. Ghio  L. Oneto  and S. Ridella. Selecting the Hypothesis Space for Improv-
ing the Generalization Ability of Support Vector Machines. In The 2011 International Joint
Conference on Neural Networks (IJCNN)  San Jose  California. IEEE  2011.

[8] S. Arlot and A. Celisse. A survey of cross-validation procedures for model selection. Statistics

Surveys  4:40–79  2010.

[9] B. Efron and R. Tibshirani. An introduction to the bootstrap. Chapman & Hall/CRC  1993.
[10] D. Anguita  A. Ghio  L. Oneto  and S. Ridella. In-sample Model Selection for Support Vector
Machines. In The 2011 International Joint Conference on Neural Networks (IJCNN)  San Jose 
California. IEEE  2011.

[11] K.P. Bennett and A. Demiriz. Semi-supervised support vector machines. In Advances in neural
information processing systems 11: proceedings of the 1998 conference  page 368. The MIT
Press  1999.

[12] O. Chapelle  B. Scholkopf  and A. Zien. Semi-supervised learning. The MIT Press  page 528 

2010.

[13] V.N. Vapnik. The nature of statistical learning theory. Springer Verlag  2000.
[14] R. Collobert  F. Sinz  J. Weston  and L. Bottou. Trading convexity for scalability.

In Pro-
ceedings of the 23rd international conference on Machine learning  pages 201–208. ACM 
2006.

[15] C. McDiarmid. On the method of bounded differences. Surveys in combinatorics  141(1):148–

188  1989.

[16] S. Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR Upper Saddle

River  NJ  USA  1994.

[17] S. Boucheron  G. Lugosi  and P. Massart. On concentration of self-bounding functions. Elec-

tronic Journal of Probability  14:1884–1899  2009.

[18] S. Boucheron  G. Lugosi  and P. Massart. Concentration inequalities using the entropy method.

The Annals of Probability  31(3):1583–1614  2003.

[19] G. Casella and R.L. Berger. Statistical inference. 2001.
[20] I. CPLEX. 11.0 users manual. ILOG SA  2008.
[21] J. Wang  X. Shen  and W. Pan. On efﬁcient large margin semisupervised learning: Method and

theory. Journal of Machine Learning Research  10:719–742  2009.

[22] J. Wang and X. Shen. Large margin semi–supervised learning. Journal of Machine Learning

Research  8:1867–1891  2007.

[23] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector ma-

chines. Advances in Kernel MethodsSupport Vector Learning  208:1–21  1998.

[24] J. Shawe-Taylor and N. Cristianini. Margin distribution and soft margin. Advances in Large

Margin Classiﬁers  pages 349–358  2000.

[25] H. Larochelle  D. Erhan  A. Courville  J. Bergstra  and Y. Bengio. An empirical evaluation of
deep architectures on problems with many factors of variation. In 24th ICML  pages 473–480 
2007.

9

,Shiau Hong Lim
Huan Xu
Shie Mannor
Ian Osband
Benjamin Van Roy
Weifeng Chen
Zhao Fu
Dawei Yang
Jia Deng