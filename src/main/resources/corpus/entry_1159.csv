2014,Controlling privacy in recommender systems,Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper  we explore a two-tiered notion of privacy where there is a small set of ``public'' users who are willing to share their preferences openly  and a large set of ``private'' users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover  we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.,Controlling privacy in recommender systems

Yu Xin

CSAIL  MIT

yuxin@mit.edu

Tommi Jaakkola

CSAIL  MIT

tommi@csail.mit.edu

Abstract

Recommender systems involve an inherent trade-off between accuracy of recom-
mendations and the extent to which users are willing to release information about
their preferences. In this paper  we explore a two-tiered notion of privacy where
there is a small set of “public” users who are willing to share their preferences
openly  and a large set of “private” users who require privacy guarantees. We
show theoretically and demonstrate empirically that a moderate number of public
users with no access to private user information already sufﬁces for reasonable
accuracy. Moreover  we introduce a new privacy concept for gleaning relational
information from private users while maintaining a ﬁrst order deniability. We
demonstrate gains from controlled access to private user preferences.

1

Introduction

Recommender systems exploit fragmented information available from each user. In a realistic sys-
tem there’s also considerable “churn”  i.e.  users/items entering or leaving the system. The core
problem of transferring the collective experience of many users to an individual user can be under-
stood in terms of matrix completion ([13  14]). Given a sparsely populated matrix of preferences 
where rows and columns of the matrix correspond to users and items  respectively  the goal is to
predict values for the missing entries.
Matrix completion problems can be solved as convex regularization problems  using trace norm
as a convex surrogate to rank. A number of algorithms are available for solving large-scale trace-
norm regularization problems. Such algorithms typically operate by iteratively building the matrix
from rank-1 components (e.g.  [7  17]). Under reasonable assumptions (e.g.  boundedness  noise 
restricted strong convexity)  the resulting empirical estimators have been shown to converge to the
underlying matrix with high probability ([12  8  2]). Consistency guarantees have mostly involved
matrices of ﬁxed dimension  i.e.  generalization to new users is not considered. In this paper  we
reformulate the regularization problem in a manner that depends only on the item (as opposed to
user) features  and characterize the error for out-of-sample users.
The completion accuracy depends directly on the amount of information that each user is will-
ing to share with the system ([1]).
It may be possible in some cases to side-step this statistical
trade-off by building Peer-to-Peer networks with homomorphic encryption that is computationally
challenging([3  11]). We aim to address the statistical question directly.
The statistical trade-off between accuracy and privacy further depends on the notion of privacy we
adopt. A commonly used privacy concept is Differential Privacy (DP) ([6])  ﬁrst introduced to
protect information leaked from database queries. In a recommender context  users may agree to a
trusted party to hold and aggregate their data  and perform computations on their behalf. Privacy
guarantees are then sought for any results published beyond the trusted party (including back to
the users). In this setting  differential privacy can be achieved through obfuscation (adding noise)
without a signiﬁcant loss of accuracy ([10]).

1

In contrast to [10]  we view the system as an untrusted entity  and assume that users wish to guard
their own data. We depart from differential privacy and separate computations that can be done
locally (privately) by individual users and computations that must be performed by the system (e.g. 
aggregation). For example  in terms of low rank matrices  only the item features have to be solved by
the system. The corresponding user features can be obtained locally by the users and subsequently
used for ranking.
From this perspective  we divide the set of users into two pools  the set of public users who openly
share their preferences  and the larger set of private users who require explicit privacy guarantees.
We show theoretically and demonstrate empirically that a moderate number of public users sufﬁce
for accurate estimation of item features. The remaining private users can make use of these item
features without any release of information. Moreover  we propose a new 2nd order privacy concept
which uses limited (2nd order) information from the private users as well  and illustrate how recom-
mendations can be further improved while maintaining marginal deniability of private information.

2 Problem formulation and summary of results

Recommender setup without privacy Consider a recommendation problem with n users and
m items. The underlying complete rating matrix to be recovered is ˚X ∈ Rn×m. If only a few
latent factors affect user preferences  ˚X can be assumed to have low rank. As such  it is also
recoverable from a small number of observed entries. We assume that entries are observed with
noise. Speciﬁcally 

Yi j = ˚Xi j + i j  (i  j) ∈ Ω

(1)

where Ω denotes the set of observed entries. Noise is assumed to be i.i.d and follows a zero-
mean sub-Gaussian distribution with parameter (cid:107)(cid:107)ψ2 = σ. Following [16]  we refer to the noise
distribution as Sub(σ2).
To bias our estimated rating matrix X to have low rank  we use convex relaxation of rank in the form
i σi(X).

of trace norm. The trace-norm is the sum of singular values of the matrix or (cid:107)X(cid:107)∗ = (cid:80)

The basic estimation problem  without any privacy considerations  is then given by

min

X∈Rm×n

1
N

(Yi j − Xi j)2 +

λ√
mn

(cid:107)X(cid:107)∗

(2)

(cid:88)

(i j)∈Ω

√

mn ensures that the regularization does not grow with either dimension.

where λ is a regularization parameter and N = |Ω| is the total number of observed ratings. The
factor
√
The above formulation requires the server to explicitly obtain predictions for each user  i.e.  solve
for X. We can instead write X = U V T and Σ = (1/
mn)V V T   and solve for Σ only. If the server
then communicates the resulting low rank Σ (or just V ) to each user  the users can reconstruct the
relevant part of U locally  and reproduce X as it pertains to them. To this end  let φi = {j : (i  j) ∈
Ω} be the set of observed entries for user i  and let Yi φi be a column vector of user i’s ratings. Then
we can show that Eq.(2) is equivalent to solving

n(cid:88)

i=1

min
Σ∈S+

Y T
i φi

(λ(cid:48)I + Σφi φi)Yi φi +

√

nm(cid:107)Σ(cid:107)∗

√
where S+ is the set of positive semi-deﬁnite m × m matrices and λ(cid:48) = λN/
we can predict ratings for unobserved items (index set φc

i for user i) by

ˆXi φc

i

= Σφc

i  φi (λ(cid:48)I + Σφi φi )−1Yi φi

(3)

nm. By solving ˆΣ 

(4)

Note that we have yet to address any privacy concerns. The solution to Eq.(3) still requires access
to full ratings Yi φi for each user.

Recommender setup with privacy Our privacy setup assumes an untrusted server. Any user
interested in guarding their data must therefore keep and process their data locally  releasing in-
formation to the server only in a controlled manner. We will initially divide users into two broad

2

categories  public and private. Public users are willing to share all their data with the server while
private users are unwilling to share any. This strict division is removed later when we permit private
users to release  in a controlled manner  limited information pertaining to their ratings (2nd order
information) so as to improve recommendations.
Any data made available to the server enables the server to model the collective experience of users 
for example  to solve Eq.(3). We will initially consider the setting where Eq.(3) is solved on the
basis of public users only. We use an EM type algorithm for training. In the E-step  the current Σ
is sent to public users to complete their rating vectors and send back to the server. In the M-step 
Σ is then updated based on these full rating vectors. The resulting ˆΣ (or ˆV ) can be subsequently
shared with the private users  enabling the private users (their devices) to locally rank candidate
items without any release of private information. The estimation of ˆΣ is then improved by asking
private users to share 2nd order relational information about their ratings without any release of
marginal selections/ratings.
Note that we do not consider privacy beyond ratings. In other words  we omit any subsequent release
of information due to users exploring items recommended to them.

Summary of contributions We outline here our major contributions towards characterizing the
role of public users and the additional controlled release of information from private users.

(cid:112) ˚X T ˚X/

√

1) We show that ˚Σ =
nm can be estimated in a consistent  accurate manner on the basis
of public users alone. In particular  we express the error (cid:107) ˆΣ− ˚Σ(cid:107)F as a function of the total number
of observations. Moreover  if the underlying public user ratings can be thought of as i.i.d. samples 
we also bound (cid:107)˚Σ − Σ∗(cid:107)F in terms of the number of public users. Here Σ∗ is the true limiting
estimate. See section 3.1 for details.
2) We show how the accuracy of predicted ratings ˆXi φc
for private users relates to the accuracy of
estimating ˆΣ (primarily from public users). Since the ratings for user i may not be related to the
subspace that ˆΣ lies in  we can only characterize the accuracy when sufﬁcient overlap exists. We
quantify this overlap  and show how (cid:107) ˆXi φc
(cid:107) depends on this overlap  accuracy of ˆΣ  and
the observation noise. See section 3.2 for details.
3) Having established the accuracy of predictions based on public users alone  we go one step further
and introduce a new privacy mechanism and algorithms for gleaning additional relational (2nd order)
information from private users. This 2nd order information is readily used by the server to estimate
ˆΣ. The privacy concept constructively maintains ﬁrst order (marginal) deniability for private users.
We demonstrate empirically the gains from the additional 2nd order information. See section 4.

− ˚Xi φc

i

i

i

3 Analysis

3.1 Statistical Consistency of ˆΣ

mn

(cid:112) ˆX T ˆX we can ﬁrst analyze errors in ˆX and then relate them to ˆΣ. To this end 

Let ˆX be a solution to Eq.(2). We can write ˆX = ˆU ˆV T   where ˆU T ˆU = ˆIm with 0/1 diagonal.
Since ˆΣ = 1√
we follow the restricted strong convexity (RSC) analysis[12]. However  their result depends on
the inverse of the minimum number of ratings of all users and items. In practice (see below)  the
number of ratings decays exponentially across sorted users  making such a result loose. We provide
a modiﬁed analysis that depends only on the total number of observations N.
Throughout the analysis  we assume that each row vector ˚Xi · belongs to a ﬁxed r dimensional
subspace. We also assume that both noiseless and noisy entries are bounded  i.e. |Yi j| | ˚Xi j| ≤
(i j)∈Ω(Yi j − Xi j)2 . The
α ∀(i  j). For brevity  we use (cid:107)Y − X(cid:107)2
restricted strong convexity property (RSC) assumes that there exists a constant κ > 0 such that

Ω to denote the empirical loss(cid:80)

κ
mn

(cid:107) ˆX − ˚X(cid:107)2

F ≤ 1
N

(cid:107) ˆX − ˚X(cid:107)2

Ω

(5)

3

for ˆX − ˚X in a certain subset. RSC provides the step from approximating observations to ap-
proximating the full underlying matrix.
It is satisﬁed with high probability provided that N =
(m + n) log(m + n)).
Assume the SVD of ˚X = ˚P S ˚QT   and let row(X) and col(X) denote the row and column spaces of
X. We deﬁne the following two sets 

A(P  Q)

:= {X  row(X) ⊆ ˚P   col(X) ⊆ ˚Q}
:= {X  row(X) ⊆ ˚P ⊥  col(X) ⊆ ˚Q⊥}

B(P  Q)

(6)
Let πA(X) and πB(X) be the projection of X onto sets A and B  respectively  and πA = I − πA 
πB = I − πB. Let ∆ = ˆX − ˚X be the difference between the estimated and the underlying rating
matrices. Our ﬁrst lemma demonstrates that ∆ lies primarily in a restricted subspace and the second
one guarantees that the noise remains bounded.
Lemma 3.1. Assume i j for (i  j) ∈ Ω are i.i.d. sub-gaussian with σ = (cid:107)i j(cid:107)ψ1. Then with
probability 1 − e
log2 N. Here h > 0 is an absolute
constant associated with the sub-gaussian noise.

N 4ch   (cid:107)πB(∆)(cid:107)∗ ≤ (cid:107)πB(∆)(cid:107)∗ + 2c2σ2√
N = b log N(cid:112) n
(cid:112) mn
  then c2σ2√

If λ = λ0cσ log2 N√
N where we leave the de-
pendence on n explicit. Let D(b  n  N ) denote the set of difference matrices that satisfy lemma 3.1
above. By combining the lemma and the RSC property  we obtain the following theorem.
Theorem 3.2. Assume RSC for the set D(b  n  N ) with parameter κ > 0 where b = cσ
λ = λ0cσ log N√
  then we have
where h  c > 0 are constants.

. Let
with probability at least 1− e

mn(cid:107)∆(cid:107)F ≤ 2cσ( 1√
1√

√
κ ) log N√

mn log N
N λ

= cσ log N

κ +

N 4ch

√

λ0

m

mn

N λ

2r

N

N

λ0

N

The bound in the theorem consists of two terms  pertaining to the noise and the regularization. In
contrast to [12]  the terms only relate to the total number of observations N.
We now turn our focus on the accuracy of ˆΣ. First  we map the accuracy of ˆX to that of ˆΣ using a
perturbation bound for polar decomposition (see [9]).
Lemma 3.3. If

mn(cid:107) ˆX − ˚X(cid:107)F ≤ δ  then (cid:107) ˆΣ − ˚Σ(cid:107)F ≤ √

1√

2δ

This completes our analysis in terms of recovering ˚Σ for a ﬁxed size underlying matrix ˚X. As a
ﬁnal step  we turn to the question of how the estimation error changes as the number of users or n
grows. Let ˚Xi be the underlying rating vector for user i and deﬁne Θn = 1
˚Xi. Then
mn
2 . We bound the distance between ˚Σ and Σ∗.
˚Σ = (Θn) 1
Theorem 3.4. Assume ˚Xi are i.i.d samples from a distribution with support only in a subspace
of dimension r and bounded norm (cid:107) ˚Xi(cid:107) ≤ α
m. Let β1 and βr be the smallest and largest
eigenvalues of Σ∗. Then  for large enough n  with probability at least 1 − r
n2  

2 . If Θ∗ is the limit of Θn  then Σ∗ = (Θ∗) 1

˚X T
i

√

i=1

(cid:80)n

(cid:115)

√
(cid:107)˚Σ − Σ∗(cid:107)F ≤ 2

rα

βr log n

β1n

+ o(

log n

n

)

(7)

Combining the two theorems and using triangle inequality  we obtain a high probability bound on
(cid:107) ˆΣ − Σ∗(cid:107)F . Assuming the number of ratings for each user is larger than ξm  then N > ξnm and
n) with η being a constant that depends on ξ. For large
the bound grows in the rate of η(log n/
enough ξ  the required n to achieve a certain error bound is small. Therefore a few public users with
large number of ratings could be enough to obtain a good estimate of Σ∗.

√

3.2 Prediction accuracy

We are ﬁnally ready to characterize the error in the predicted ratings ˆXi φc
for all users as deﬁned in
Eq.(4). For brevity  we use δ to represent the bound on (cid:107) ˆΣ− Σ∗(cid:107) obtained on the basis of our results
above. We also use xφ and xφc as shorthands for Xi φi and Xi φc
with the idea that xφ typically
refers to a new private user.

i

i

4

The key issue for us here is that the partial rating vector xφ may be of limited use. For example 
if the number of observed ratings is less than rank r  then we would be unable to identify a rating
vector in the r dimensional subspace even without noise. We seek to control this in our analysis by
assuming that the observations have enough signal to be useful. Let SVD of Σ∗ be Q∗S∗(Q∗)T  
and β1 be its minimum eigenvalue. We constrain the index set of observations φ such that it belongs
to the set

(cid:26)

(cid:27)
F  ∀x ∈ row((Q∗)T )

D(γ) =

φ ⊆ {1  . . .   m}  s.t.(cid:107)x(cid:107)2

F ≤ γ

m

|φ|(cid:107)xφ(cid:107)2

The parameter γ depends on how the low dimensional sub-space is aligned with the coordinate axes.
We are only interested in characterizing prediction errors for observations that are in D(γ). This is
quite different from the RSC property. Our main result is then
Theorem 3.5. Suppose (cid:107)Σ − Σ∗(cid:107)F ≤ δ (cid:28) β1  φ ∈ D(γ). For any ˚x ∈ row((Q∗)T )  our
observation xφ = ˚xφ + φ where φ ∼ Sub(σ2) is the noise vector. The predicted ratings over
the remaining entries are given by ˆxφc = Σφc φ(λ(cid:48)I + Σφ φ)−1xφ. Then  with probability at least
1 − exp(−c2 min(c4

1 (cid:112)|φ|c2

1)) 

(cid:114)

√
(cid:107)xφc − ˚xφc(cid:107)F ≤ 2

λ(cid:48) + δ(

γ

m
|φ| + 1)(

(cid:107)˚x(cid:107)F√

β1

2c1σ|φ| 1
4√
λ(cid:48)

)

+

where c1  c2 > 0 are constants.
√
All the proofs are provided in the supplementary material. The term proportional to (cid:107)˚x(cid:107)F /
due to the estimation error of Σ∗  while the term proportional to 2c1σ|φ| 1
4 /
noise in the observed ratings.

β1 is
λ(cid:48) comes from the

√

4 Controlled privacy for private users

Our theoretical results already demonstrate that a relatively small number of public users with many
ratings sufﬁces for a reasonable performance guarantee for both public and private users. Empirical
results (next section) support this claim. However  since public users enjoy no privacy guarantees 
we would like to limit the required number of such users by requesting private users to contribute in
a limited manner while maintaining speciﬁc notions of privacy.
Deﬁnition 4.1. : Privacy preserving mechanism. Let M : Rm×1 → Rm×1 be a random mecha-
nism that takes a rating vector r as input and outputs M(r) of the same dimension with jth element
M(r)j. We say that M(r) is element-wise privacy preserving if Pr(M(r)j = z) = p(z) for
j = 1  ...  m  and some ﬁxed distribution p.
For example  a privacy preserving mechanism M(r) is element-wise private if its coordinates fol-
low the same marginal distribution such as uniform. Note that such a mechanism can still release
information about how different ratings interact (co-vary) which is necessary for estimation.
Discrete values. Assume that each element in r and M(r) belongs to a discrete set S with |S| = K.
A natural privacy constraint is to insist that the marginal distribution of M(r)j is uniform  i.e. 
Pr(M(r)j = z) = 1/K  for z ∈ S. A trivial mechanism that satisﬁes the privacy constraint is to
select each value uniformly at random from S. In this case  the returned rating vector contributes
nothing to the server model. Our goal is to design a mechanism that preserves useful 2nd order
information.
We assume that a small number of public user proﬁles are available  from which we can learn
an initial model parameterized by (µ  V )  where µ is the item mean vector and V is a low rank
component of Σ. The server provides each private user the pair (µ  V ) and asks  once  for a response
M(r). Here r is the user’s full rating vector  completed (privately) with the help of the server model
(µ  V ).
The mechanism M(r) is assumed to be element-wise privacy preserving  thus releasing nothing
about a single element in isolation. What information should it carry? If each user i provided their
full rating vector ri  the server could estimate Σ according to
2 . Thus 

i=1(ri−µ)(ri−µ)T ) 1

nm ((cid:80)n

1√

5

if M(r) preserves the second order statistics to the extent possible  the server could still obtain an
accurate estimate of Σ.
Consider a particular user and their completed rating vector r. Let P(x) = Pr(M(r) = x). We
select distribution P(x) by solving the following optimization problem geared towards preserving
interactions between the ratings under the uniform marginal constraint.

min

P
s.t.

Ex∼P(cid:107)(x − µ)(x − µ)T − (r − µ)(r − µ)T(cid:107)2
P(xi = s) = 1/K  ∀i  ∀s ∈ S.

F

i   x2

i   ...  xK

i and xq

i } forms a permutation of S.

(8)
where K = |S|. The exact solution is difﬁcult to obtain because the number of distinct assignments
of x is K m. Instead  we consider an approximate solution. Let x1  ...  xK ∈ Rm×1 be K different
vectors such that  for each i  {x1
If we choose x with
Pr(x = xj) = 1/K  then the marginal distribution of each element is uniform therefore maintaining
element-wise privacy.
It remains to optimize the set x1  ...  xK to capture interactions between
ratings.
We use a greedy coordinate descent algorithm to optimize x1  ...  xK. For each coordinate i  we
randomly select a pair xp and xq  and switch xp
i if the objective function in (8) is reduced.
The process is repeated a few times before we move on to the next coordinate. The algorithm can
be implemented efﬁciently because each operation deals only with a single coordinate.
Finally  according to the mechanism  the private user selects one of xj  j = 1  . . .   K  uniformly
at random and sends the discrete vector back to the server. Since the resulting rating vectors from
private users are noisy  the server decreases their weight relative to the information from public users
as part of the overall M-step for estimating Σ.
Continuous values. Assuming the rating values are continuous and unbounded  we require instead
that the returned rating vectors follow the marginal distributions with a given mean and variance.
Speciﬁcally  E[M(r)i] = 0 and Var[M(r)i] = v where v is a constant that remains to be deter-
mined. Note that  again  any speciﬁc element of the returned vector will not  in isolation  carry any
information speciﬁc to the element.
As before  we search for the distribution P so as to minimize the L2 error of the second order
statistics under marginal constraints. For simplicity  denote r(cid:48) = r− µ where r is the true completed
rating vector  and ui = M(r)i. The objective is given by

min
P v
s.t.

Eu∼P(cid:107)uuT − r(cid:48)r(cid:48)T(cid:107)2
E[ui] = 0  Var[ui] = v  ∀i.

F

i=1 |r(cid:48)

i) and h = ((cid:80)m

(9)
Note that the formulation does not directly constrain that P has identical marginals  only that the
means and variances agree. However  the optimal solution does  as shown next.
i|)/m. The minimizing distribution of (9) is
Theorem 4.2. Let zi = sign(r(cid:48)
given by Pr(u = zh) = Pr(u = −zh) = 1/2.
We leave the proof in the supplementary material. A few remarks are in order. The mechanism in this
case is a two component mixture distribution  placing a probability mass on sign(r(cid:48))h (vectorized)
and −sign(r(cid:48))h with equal probability. As a result  the server  knowing the algorithm that private
users follow  can reconstruct sign(r(cid:48)) up to an overall randomly chosen sign. Note also that the
value of h is computed from user’s private rating vector and therefore releases some additional
information about r(cid:48) = r − µ albeit weakly. To remove this information altogether  we could use
the same h for all users and estimate it based on public users.
The privacy constraints will clearly have a negative impact on the prediction accuracy in comparison
to having direct access to all the ratings. However  the goal is to improve accuracy beyond the public
users alone by obtaining limited information from private users. While improvements are possible 
the limited information surfaces in several ways. First  since private users provide no ﬁrst order
information  the estimation of mean rating values cannot be improved beyond public users. Second 
the sampling method we use to enforce element-wise privacy adds noise to the aggregate second
order information from which V is constructed. Finally  the server can run the M-step with respect to
the private users’ information only once  whereas the original EM algorithm could entertain different
completions for user ratings iteratively. Nevertheless  as illustrated in the next section  the algorithm
can still achieve a good accuracy  improving with each additional private user.

6

5 Experiments

We perform experiments on the Movielens 10M dataset which contains 10 million ratings from
69878 users on 10677 movies. The test set contains 10 ratings for each user. We begin by demon-
strating that indeed a few public users sufﬁce for making accurate recommendations. Figure 1 left
shows the test performance of both weighted (see [12]) and unweighted (uniform) trace norm regu-
larization as we add users. Here users with most ratings are added ﬁrst.

Figure 1: Left: Test RMSE as a function of the percentage of public users; Right: Performance
changes with different rating numbers.

With only 1% of public users added  the test RMSE of unweighted trace norm regularization is
0.876 which is already close to the optimal prediction error. Note that the loss of weighted trace
norm regularization actually starts to go up when the number of users increases. The reason is that
the weighted trace norm penalizes less for users with few ratings. As a result  the resulting low
dimensional subspace used for prediction is inﬂuenced more by users with few ratings.
The statistical convergence bound in section 3.1 involves both terms that decrease as a function of
the number of ratings N and the number of public users n. The two factors are usually coupled. It
is interesting to see how they impact performance individually. Given a number of total ratings  we
compare two different methods of selecting public users. In the ﬁrst method  users with most ratings
are selected ﬁrst  whereas the second method selects users uniformly at random. As a result  if we
equalize the total number of ratings from each method  the second method selects a lot more users.
Figure 1 Right shows that the second method achieves better performance. An interpretation  based
on the theory  is that the right side of error bound (7) decreases as the number of users increases.
We also show how performance improves based on controlled access to private user preferences.
First  we take the top 100 users with the most ratings as the public users  and learn the initial
prediction model from their ratings. To highlight possible performance gains  private users with
more ratings are selected ﬁrst. The results remain close if we select private users uniformly.
The rating values are from 0.5 to 5 with totally 10 different discrete values. Following the privacy
mechanism for discrete values  each private user generates ten different candidate vectors and returns
one of them uniformly at random. In the M-step  the weight for each private user is set to 1/2
compared to 1 for public users. During training  after processing w = 20 private users  we update
parameters (µ  V )  re-complete the rating vectors of public users  making predictions for next batch
of private users more accurate. The privacy mechanism for continuous values is also tested under
the same setup. We denote the two privacy mechanism as PMD and PMC  respectively.
We compare ﬁve different scenarios. First  we use a standard DP method that adds Laplace noise to
the completed rating vector. Let the DP parameter be   because the maximum difference between
rating values is 4.5  the noise follows Lap(0  4.5/). As before  we give a smaller weight to the
noisy rating vectors and this is determined by cross validation. Second  [5] proposed a notion of
“local privacy” in which differential privacy is guaranteed for each user separately. An optimal
strategy for d-dimensional multinomial distribution in this case reduces effective samples from n to
n2/d where  is the DP parameter. In our case the dimension corresponds to the number of items

7

00.20.40.60.810.860.870.880.890.90.910.920.930.940.950.96Percentage of UsersTest RMSE Uniform(cid:9)Weighted2004006008001000120014001600180020000.80.911.11.21.31.41.5Number of ratings (k)Test RMSE Most ratingsRandomFigure 2: Test RMSE as a function of private user numbers. PMC: the privacy mechanism for
continuous values; PMD: the privacy mechanism for discrete values; Lap eps=1: DP with Laplace
noise   = 1; Lap eps=5: same as before except  = 5; SSLP eps=5: sampling strategy described
in [4] with DP parameter  = 5; Exact 2nd order: with exact second order statistics from private
users (not a valid privacy mechanism); Full EM: EM without any privacy protection.

making estimation challenging under DP constraints. We also compare to this method and denote it
as SSLP (sampling strategy for local privacy).
In addition  to understand how our approximation to second order statistics affects the performance 
we also compare to the case that r(cid:48)a is given to the server directly where a = {−1  1} with proba-
bility 1/2. In this way  the server can obtain the exact second order statistics using r(cid:48)r(cid:48)T . Note that
this is not a valid privacy preserving mechanism. Finally  we compare to the case that the algorithm
can access private user rating vectors multiple times and update the parameters iteratively. Again 
this is not a valid mechanism but illustrates how much could be gained.
Figure 2 shows the performance as a function of the number of private users. The standard Laplace
noise method performs reasonably well when  = 5  however the corresponding privacy guarantee
is very weak. SSLP improves the accuracy mildly.
In contrast  with the privacy mechanism we deﬁned in section 4 the test RMSE decreases signiﬁ-
cantly as more private users are added. If we use the exact second order information without the
sampling method  the ﬁnal test RMSE would be reduced by 0.07 compared to PMD. Lastly  full
EM without privacy protection reduces the test RMSE by another 0.08. These performance gaps are
expected because there is an inherent trade-off between accuracy and privacy.

6 Conclusion

Our contributions in this paper are three-fold. First  we provide explicit guarantees for estimating
item features in matrix completion problems. Second  we show how the resulting estimates  if shared
with new users  can be used to predict their ratings depending on the degree of overlap between
their private ratings and the relevant item subspace. The empirical results demonstrate that only a
small number of public users with large number of ratings sufﬁces for a good performance. Third 
we introduce a new privacy mechanism for releasing 2nd order information needed for estimating
item features while maintaining 1st order deniability. The experiments show that this mechanism
indeed performs well in comparison to other mechanisms. We believe that allowing different levels
of privacy is an exciting research topic. An extension of our work would be applying the privacy
mechanism to the learning of graphical models in which 2nd or higher order information plays an
important role.

7 Acknowledgement

The work was partially supported by Google Research Award and funding from Qualcomm Inc.

8

0501001502002503003504000.870.8750.880.8850.890.8950.90.9050.910.9150.92Number of ’’private’’ usersTest RMSE PMCPMDLap eps=1Lap eps=5SSLP eps=5Exact 2nd orderFull EMReferences
[1] M´ario S Alvim  Miguel E Andr´es  Konstantinos Chatzikokolakis  Pierpaolo Degano  and
Catuscia Palamidessi. Differential privacy: on the trade-off between utility and information
leakage. In Formal Aspects of Security and Trust  pages 39–54. Springer  2012.

[2] E. Candes and Y. Plan. Matrix completion with noise. In Proceedings of the IEEE  2010.
[3] J. Canny. Collaborative ﬁltering with privacy via factor analysis. In SIGIR  2002.
[4] John Duchi  Martin J Wainwright  and Michael Jordan. Local privacy and minimax bounds:
Sharp rates for probability estimation. In Advances in Neural Information Processing Systems 
pages 1529–1537  2013.

[5] John C Duchi  Michael I Jordan  and Martin J Wainwright. Privacy aware learning. In NIPS 

pages 1439–1447  2012.

[6] C. Dwork. Differential privacy: A survey of results. In Theory and Applications of Models of

Computation  2008.

[7] M. Jaggi and M. Sulovsk. A simple algorithm for nuclear norm regularized problems.

ICML  2010.

In

[8] R. Keshavan  A. Montanari  and Sewoong Oh. Matrix completion from noisy entries. JMLR 

2010.

[9] R. Mathias. Perturbation bounds for the polar decomposition. BIT Numerical Mathematics 

1997.

[10] F. McSherry and I. Mironov. Differentially private recommender systems: Building privacy

into the netﬂix prize contenders. In SIGKDD  2009.

[11] B. N. Miller  J. A. Konstan  and J. Riedl. Pocketlens: Toward a personal recommender system.

ACM Trans. Inf. Syst.  2004.

[12] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix comple-

tion: optimal bounds with noise. JMLR  2012.

[13] R. Salakhutdinov and N. Srebro. Collaborative ﬁltering in a non-uniform world: Learning with

the weighted trace norm. In NIPS  2010.

[14] N. Srebro  J. Rennie  and T. Jaakkola. Maximum margin matrix factorization. In NIPS  2004.
[15] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math. 

2012.

[16] R. Vershynin.

arXiv:1011.3027.

Introduction to the non-asymptotic analysis of

random matrices.

[17] Y. Xin and T. Jaakkola. Primal-dual methods for sparse constrained matrix completion. In

AISTATS  2012.

9

,Yu Xin
Tommi Jaakkola
Kevin Winner
Daniel Sheldon
George Papamakarios
Theo Pavlakou
Iain Murray