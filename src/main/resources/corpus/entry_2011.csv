2015,Fast Distributed k-Center Clustering with Outliers on Massive Data,Clustering large data is a fundamental problem with a vast number of applications.  Due to the increasing size of data  practitioners interested in clustering have turned to distributed computation methods.  In this work  we consider the widely used k-center clustering problem and its variant used to handle noisy data  k-center with outliers. In the noise-free setting we demonstrate how a previously-proposed distributed method is actually an O(1)-approximation algorithm  which accurately explains its strong empirical performance. Additionally  in the noisy setting  we develop a novel distributed algorithm that is also an O(1)-approximation. These algorithms are highly parallel and lend themselves to virtually any distributed computing framework. We compare both empirically against the best known noisy sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions. The algorithms are all one can hope for in distributed settings: they are fast  memory efficient and they match their sequential counterparts.,Fast Distributed k-Center Clustering with Outliers on

Massive Data

Gustavo Malkomes  Matt J. Kusner  Wenlin Chen

Department of Computer Science and Engineering

Washington University in St. Louis

St. Louis  MO 63130

{luizgustavo mkusner wenlinchen}@wustl.edu
Benjamin Moseley

Kilian Q. Weinberger

Department of Computer Science

Department of Computer Science and Engineering

Cornell University
Ithaca  NY 14850

kqw4@cornell.edu

Washington University in St. Louis

St. Louis  MO 63130

bmoseley@wustl.edu

Abstract

Clustering large data is a fundamental problem with a vast number of applications.
Due to the increasing size of data  practitioners interested in clustering have turned
to distributed computation methods. In this work  we consider the widely used k-
center clustering problem and its variant used to handle noisy data  k-center with
outliers. In the noise-free setting we demonstrate how a previously-proposed dis-
tributed method is actually an O(1)-approximation algorithm  which accurately
explains its strong empirical performance. Additionally  in the noisy setting  we
develop a novel distributed algorithm that is also an O(1)-approximation. These
algorithms are highly parallel and lend themselves to virtually any distributed
computing framework. We compare each empirically against the best known se-
quential clustering methods and show that both distributed algorithms are con-
sistently close to their sequential versions. The algorithms are all one can hope
for in distributed settings: they are fast  memory efﬁcient and they match their
sequential counterparts.

1

Introduction

Clustering is a fundamental machine learning problem with widespread applications. Example ap-
plications include grouping documents or webpages by their similarity for search engines [30] or
grouping web users by their demographics for targeted advertising [2]. In a clustering problem one
is given as input a set U of n data points  characterized by a set of features  and is asked to cluster
(partition) points so that points in a cluster are similar by some measure. Clustering is a well un-
derstood task on modestly sized data sets; however  today practitioners seek to cluster datasets of
massive size. Once data becomes too voluminous  sequential algorithms become ineffective due to
their running time and insufﬁcient memory to store the data. Practitioners have turned to distributed
methods  in particular MapReduce [13]  to efﬁciently process massive data sets.
One of the most fundamental clustering problems is the k-center problem. Here  it is assumed
that for any two input points a pair-wise distance can be computed that reﬂects their dissimilarity
(typically these arise from a metric space). The objective is to choose a subset of k points (called
centers) that give rise to a clustering of the input set into k clusters. Each input point is assigned to
the cluster deﬁned by its closest center (out of the k center points). The k-center objective selects
these centers to minimize the farthest distance of any point to its cluster center.

1

The k-center problem has been studied for over three decades and is a fundamental task used for
exemplar based clustering [22]. It is known to be NP-Hard and  further  no algorithm can achieve
a (2− )-approximation for any  > 0 unless P=NP [16  20]. In the sequential setting  there are
algorithms which match this bound achieving a 2-approximation [16  20].
The k-center problem is popular for clustering datasets which are not subject to noise since the
objective is sensitive to error in the data because the worst case (maximum) distance of a point to
the centers is used for the objective. In the case where data can be noisy [1  18  19]  previous work
has considered the k-centers with outliers problem [10]. In this problem  the objective is the same 
but additionally one may discard a set of z points from the input. These z points are the outliers and
are ignored in the objective. Here  the best known algorithm is a 3-approximation [10].
Once datasets become large  the known algorithms for these two problems become ineffective. Due
to this  previous work on clustering has resorted to alternative algorithmics. There have been several
works on streaming algorithms [3  17  24  26]. Others have focused on distributed computing [6 
7  14  25]. The work in the distributed setting has focused on algorithms which are implementable
in MapReduce  but are also inherently parallel and work in virtually any distributed computing
framework. The work of [14] was the ﬁrst to consider k-center clustering in the distributed setting.
Their work gave an O(1)-round O(1)-approximate MapReduce algorithm. Their algorithm is a
sampling based MapReduce algorithm which can be used for a variety of clustering objectives.
Unfortunately  as the authors point out in their paper  the algorithm does not always perform well
empirically for the k-center objective since the objective function is very sensitive to missing data
points and the sampling can cause large errors in the solution.
The work of Kumar et al. [23] gave a (1− 1
e )-approximation algorithm for submodular function
maximization subject to a cardinality constraint in the MapReduce setting  however  their algorithm
requires a non-constant number of MapReduce rounds. Whereas  Mirzasoleiman et al. [25] (re-
cently  extended in [8]) gave a two MapReduce rounds algorithm but their approximation ratio is not
constant. It is known that an exact algorithm for submodular maximization subject to a cardinality
constraint gives an exact algorithm for the k-center problem. Unfortunately  both problems are NP-
Hard and the reduction is not approximation preserving. Therefore  their theoretical results do not
imply a nontrivial approximation for the k-center problem.
For these problems  the following questions loom: What can be achieved for k-center clustering
with or without outliers in the large-scale distributed setting? What underlying algorithmic ideas are
needed for the k-center with outliers problem to be solved in the distributed setting? The k-center
with outliers problem has not been studied in the distributed setting. Given the complexity of the
sequential algorithm  it is not clear what such an algorithm would look like.
Contributions. In this work  we consider the k-center and k-center with outliers problems in the
distributed computing setting. Although the algorithms are highly parallel and work in virtually
any distributed computing framework  they are particularly well suited for the MapReduce [13]
as they require only small amounts of inter-machine communication and very little memory on
each machine. We therefore state our results for the MapReduce framework [13]. We will assume
throughout the paper that our algorithm is given some number of machines  m  to process the data.
We ﬁrst begin by considering a natural interpretation of the algorithm of Mirzasoleiman et al. [25]
on submodular optimization for the k-center problem. The algorithm we introduce runs in two
MapReduce rounds and achieves a small constant approximation.
Theorem 1.1. There is a two round MapReduce algorithm which achieves a 4-approximation for
the k-center problem which communicates O(km) amount of data assuming the data is already
partitioned across the machines. The algorithm uses O(max{n/m  mk}) memory on each machine.
Next we consider the k-center with outliers problem. This problem is far more challenging and pre-
vious distributed techniques do not lend themselves to this problem. Here we combine the algorithm
developed for the problem without outliers with the sequential algorithm for k-center with outliers.
We show a two round MapReduce algorithm that achieves an O(1)-approximation.
Theorem 1.2. There is a two round MapReduce algorithm which achieves a 13-approximation for
the k-center with outliers problem which communicates O(km log n) amount of data assuming the
data is already partitioned across the machines. The algorithm uses O(max{n/m  m(k+z) log n})
memory on each machine.

2

Finally  we perform experiments with both algorithms on real world datasets. For k-center we
observe that the quality of the solutions is effectively the same as that of the sequential algorithm for
all values of k—the best one could hope for. For the k-center problem with outliers our algorithm
matches the sequential algorithm as the values of k and z vary and it signiﬁcantly outperforms the
algorithm which does not explicitly consider outliers. Somewhat surprisingly our algorithm achieves
an order of magnitude speed-up over the sequential algorithm even if it is run sequentially.

2 Preliminaries

Map-Reduce. We will consider algorithms in the distributed setting where our algorithms are given
m machines. We deﬁne our algorithms in a general distributed manner  but they particularly suited
to the MapReduce model [21]. This model has become widely used both in theory and in applied
machine learning [4  5  9  12  15  21  25  27  31]. In the MapReduce setting  algorithms run in
rounds. In each round the machines are allowed to run a sequential computation without machine
communication. Between rounds  data is distributed amongst the machines in preparation for new
computation. The goal is to design an algorithm which runs in a small number of rounds since
the main running time bottleneck is distributing the data amongst the machine between each round.
Generally it is assumed that each of the machines uses sublinear memory [21]. The motivation here
is that since MapReduce is used to process large data sets  the memory on the machines should be
much smaller than the input size to the problem. It is additionally assumed that there is enough
memory to store the entire dataset across all machines. Our algorithms fall into this category and
the memory required on each machine scales inversely with m.
k-center (with outliers) problem. In the problems considered  there is a universe U of n points.
Between each pair of points u  v ∈ U there is a distance d(u  v) specifying their dissimilarity. The
points are assumed to lie in a metric space which implies that for all u  v  u(cid:48) ∈ U we have that 1.
d(u  u) = 0  2. d(u  v) = d(v  u)  and 3. d(u  v)≤ d(u  u(cid:48))+d(u(cid:48)  v) (triangle inequality). For a set
X of points  we let dX (u) := minv∈X{d(u  v)} denote the minimum distance of a point u ∈ U to
any point in X. In the k-center problem  the goal is to choose a set of centers X of k points such
that maxv∈U dX (v) is minimized (i.e.  dX (v) is the distance between v and its cluster center and we
would like to minimize the largest distance  across all points). In the k-center with outliers problem 
the goal is to choose a set X of k points and a set Z of z points such that maxv∈U\Z dX (v) is
minimized. Note that in this problem the algorithm simply needs to choose the set X as the optimal
set of Z points is well deﬁned: It is the set of points in U farthest from the centers X.
Sequential algorithms The most widely used k-center (with-
out outliers) algorithm is the following simple greedy proce-
dure  summarized in pseudo-code in Algorithm 1. The algo-
rithm sets X = ∅ and then iteratively adds points from U to
X until |X| = k. At each step  the algorithm greedily se-
lects the farthest point in U from X  and then adds this point
to the updated set X. This algorithm is natural and efﬁcient
and is known to give a 2-approximation for the k-center prob-
lem [20]. However  it is also inherently sequential and does
not lend itself to the distributed setting (except for very small
k). A na¨ıve MapReduce implementation can be obtained by ﬁnding the element v∈ U to maximize
dX (v) in a distributed fashion (line 4 in Algorithm 1). This  however  requires k rounds of Map-
Reduce that must distribute the entire dataset each round. Therefore it is unsuitably inefﬁcient for
many real world problems. The sequential algorithm for k-center with outliers is more complicated
due to the increased difﬁculty of the problem (for reference see: [10]). This algorithm is even more
fundamentally sequential than Algorithm 1.
3 k-Center in MapReduce

1: X = ∅
2: Add any point u ∈ U to X
3: while |X| < k do
u = argmaxv∈U dX (v)
4:
5: X = X ∪ {u}
6: end while

Algorithm 1 Sequential k-center
GREEDY(U  k)

In this section we consider the k-center problem where no outliers are allowed. As mentioned
before  a similar variant of this problem has been previously studied in Mirzasoleiman et al. [25]
in the distributed setting. The work of Mirzasoleiman et al. considers submodular maximization
m}-approximation where m is the number of machines. Their algorithm
and showed a min{ 1
was shown to perform extremely well in practice (in a slightly modiﬁed clustering setup). The

k   1

3

where machine i receives Ui.

2: Machine i assigns Ci = GREEDY(Ui  k)
3: All sets Ci are assigned to machine 1
4: Machine 1 sets X = GREEDY(∪m
5: Output X

i=1Ci  k)

Algorithm 2 Distributed k-center
GREEDY-MR(U  k)
1: Partition U into m equal sized sets U1  . . .   Um

k-center problem can be mapped to submodular maximization  but the standard reduction is not
approximation preserving and their result does not imply a non-trivial approximation for k-center.
In this section  we give a natural interpretation of their algorithm without submodular maximization.
Algorithm 2 summarizes a distributed approach for solving the k-center problem. First the data
points of U are partitioned across all m machines. Then each machine i runs the GREEDY algorithm
on the partition they are given to compute a set Ci of k points. These points are assigned to a
single machine  which runs GREEDY again to compute the ﬁnal solution. The algorithm runs in two
MapReduce rounds and the only information communicated is Ci for each i if the data is already
assigned to machines. Thus  we have the following proposition.
Proposition 3.1. The algorithm GREEDY-MR runs in two MapReduce rounds and communicates
O(km) amount of data assuming the data is originally partitioned across the machines. The algo-
rithm uses O(max{n/m  mk}) memory on each machine.
We aim to bound the approximation ratio of
GREEDY-MR. Let OPT denote the optimal
solution value for the k-center problem. The
previous proposition and following lemma
give Theorem 1.1.
Lemma 3.2. The algorithm GREEDY-MR is
a 4-approximation algorithm.
Proof. We ﬁrst show for any i that dCi(u) ≤
2OPT for any u ∈ Ui. Indeed  say that this is
not the case for sake of contradiction for some i. Then for some u ∈ Ui  dCi(u) > 2OPT which
implies u is distance greater than 2OPT from all points in Ci. By deﬁnition of GREEDY for any pair
of points v  v(cid:48) ∈ Ci it must be the case that d(v  v(cid:48)) ≥ dCi(u) > 2OPT (otherwise u would have
been included in Ci). Thus  in the set {u} ∪ Ci there are k + 1 points all of distance greater than
2OPT from each other. However  then two of these points v  v(cid:48) ∈ ({u} ∪ Ci) must be assigned to
the same center v∗ in the optimal solution. Using the triangle inequality and the deﬁnition of OPT it
must be the case that d(v  v(cid:48)) ≤ d(v∗  v) + d(v∗  v(cid:48)) ≤ 2OPT  a contradiction. Thus  for all points
u ∈ Ui  it must be that dCi(u) ≤ 2OPT.
Let X denote the output solution by GREEDY-MR. We can show a similar result for points in ∪m
i=1Ci
i=1Ci. Indeed  say that
when compared to X. That is  we show that dX (u) ≤ 2OPT for any u ∈ ∪m
i=1Ci  dX (u) > 2OPT which
this is not the case for sake of contradiction. Then for some u ∈ ∪m
implies u is distance greater than 2OPT from all points in X. By deﬁnition of GREEDY for any pair
of points v  v(cid:48) ∈∪m
i=1Ci it must be that d(v  v(cid:48)) ≥ dX (u) > 2OPT. Thus  in the set {u} ∪ X there
are k+1 points all of distance greater than 2OPT from each other. However  then two of these points
v  v(cid:48) ∈ ({u}∪ X) must be assigned to the same center v∗ in the optimal solution. Using the triangle
inequality and the deﬁnition of OPT it must be the case that d(v  v(cid:48)) ≤ d(v∗  v)+d(v∗  v(cid:48)) ≤ 2OPT 
a contradiction. Thus  for all points u ∈ ∪m
Now we put these together to get a 4-approximation. Consider any point u ∈ U. If u is in Ci for any
i  it must be the case that dX (u) ≤ 2OPT by the above argument. Otherwise  u is not in Ci for any i.
Let Uj be the partition which u belongs to. We know that u is within distance 2OPT to some point
v ∈ Cj and further we know that v is within distance 2OPT from X from the above arguments.
Thus  using the triangle inequality  dX (u) ≤ d(u  v) + dX (v) ≤ 2OPT + 2OPT ≤ 4OPT.
4 k-center with Outliers
In this section  we consider the k-center with outliers problem and give the ﬁrst MapReduce algo-
rithm for the problem. The problem is more challenging than the version without outliers because
one has to also determine which points to discard  which can drastically change which centers should
be chosen. Intuitively  the right algorithmic strategy is to choose centers such that there are many
points around them. Given that they are surrounded by many points  this is a strong indicator that
these points are not outliers. This idea was formalized in the algorithm of Charikar et al. [10]  a
well-known and inﬂuential algorithm for this problem in the single machine setting.
Algorithm 3 summarizes the approach of Charikar et al. [10]. It takes as input the set of points
U  the desired number centers k and a parameter G. The parameter G is a ‘guess’ of the optimal
solution’s value. The algorithm’s performance is best when G = OPT where OPT denotes the

i=1Ci  it must be that dX (u) ≤ 2OPT.

4

∀u ∈ U(cid:48) let Bu ={v : v ∈ U(cid:48)  du v ≤ G}
Let v(cid:48) = argmaxu∈U(cid:48)|Bu|
Set X = X ∪ {v(cid:48)}
Compute B(cid:48)v(cid:48) ={v : v ∈ U(cid:48)  dv(cid:48) v ≤ 3G}
U(cid:48) = U(cid:48) \ B(cid:48)v(cid:48)

Algorithm 3 Sequential k-center with outliers [10]
OUTLIERS(U  k  G)
1: U(cid:48) = U  X = ∅
2: while |X| < k do
3:
4:
5:
6:
7:
8: end while

optimal k-center objective after discarding z points. The number of outliers to be discarded  z  is
not a parameter of the algorithm and is communicated implicitly through G. The value of G can be
determined by doing a binary search on possible values of G—between the minimum and maximum
distances of any two points.
For each point u ∈ U  the set Bu contains
points within distance G of u. The algo-
rithm adds the point v(cid:48) to the solution set
which covers the largest number of points
with Bv(cid:48). The idea here is to add points which
have many points nearby (and thus large Bv(cid:48)).
Then the algorithm removes all points from
the universe which are within distance 3G
from v(cid:48) and continues until k points are cho-
sen to be in the set X. Recall that in the out-
liers problem  choosing the centers is a well
deﬁned solution and the outliers are simply the farthest z points from the centers. Further  it can
be shown that when G = OP T   after selecting the k centers  there are at most z outliers remaining
in U(cid:48). It is known that this algorithm gives a 3-approximation [10]—however it is not efﬁcient on
large or even medium sized datasets due to the computation of the sets Bu within each iteration. For
instance  it can take ≈ 100 hours on a data set with 45  000 points.
We now give a distributed approach (Algorithm 4) for clustering with outliers. This algorithm is
naturally parallel  yet it is signiﬁcantly faster even if run sequentially on a single machine. It uses a
sub-procedure (Algorithm 5) which is a generalization of OUTLIERS.
The algorithm ﬁrst partitions the points
across the m machines (e.g.  set Ui goes
to machine i). Each machine i runs the
GREEDY algorithm on Ui  but selects k+z
points rather than k. This results in a set
Ci. For each c ∈ Ci  we assign a weight
wc that is the number of points in Ui that
have c as their closest point in Ci (i.e.  if
Ci deﬁnes an intermediate clustering of Ui
then wc is the number of points in the c-
cluster). The algorithm then runs a vari-
ation of OUTLIERS called CLUSTER  de-
scribed in Algorithm 5  on only the points
in ∪m
i=1Ci. The main differences are that
CLUSTER represents each point c by the number of points wc closest to it  and that it uses 5G and
11G for the radii in Bu and B(cid:48)u.
The total machine-wise communica-
tion required for OUTLIERS-MR is
that needed to send each of the sets
to Machine 1 along with their
Ci
weights. Each weight can have size at
most n  so it only requires O(log n)
space to encode the weight. This
gives the following proposition.
Proposition 4.1. OUTLIERS-MR
runs in two MapReduce rounds and
communicates O((k + z)m log n)
amount of data assuming the data
is originally partitioned across the
machines. The algorithm uses O(max{n/m  m(k + z) log n}) memory on each machine.
Our goal is to show that OUTLIERS-MR is an O(1)-approximation algorithm (Theorem 1.2). We
ﬁrst present intermediate lemmas and give proof sketches  leaving intermediate proofs to the sup-
plementary material. We overload notation and let OPT denote a ﬁxed optimal solution as well as

Algorithm 5 Clustering subroutine
CLUSTER(U  k  G)
1: U(cid:48) = U  X = ∅
2: while |X| < k do
3:
4:
5:
6:
7:
8: end while
9: Output X

2: Machines i sets Ci = GREEDY(Ui  k + z)
3: For each point c ∈ Ci  machine i set wc = |{v :
4: All sets Ci are assigned to machine 1 with the
5: Machine 1 sets X = CLUSTER(∪m
6: Output X

v ∈ Ui  d(v  c) = dCi(v)}| + 1
weights of the points in Ci

∀u ∈ U compute Bu = {v : v ∈ U(cid:48)  du v ≤ 5G}
Let v(cid:48) = argmaxu∈U
Set X = X ∪ {v(cid:48)}
Compute B(cid:48)v(cid:48) ={v : v ∈ U(cid:48)  dv(cid:48) v ≤ 11G}
U(cid:48) = U(cid:48) \ B(cid:48)v(cid:48)

u(cid:48)∈Bu wu(cid:48)

Algorithm 4 Distributed k-center with outliers
OUTLIERS-MR(U  k  z  G  α  β)

1: Partition U into m equal sized sets U1  . . .   Um

where machine i receives Ui.

i=1Ci  k  G)

(cid:80)

5

the optimal objective to the problem. We will assume throughout the proof that G = OPT  as we can
perform a binary search to ﬁnd ˆG = OPT(1 + ) for arbitrarily small  > 0 when running CLUSTER
on a single machine. We ﬁrst claim that any point in Ui is not too far from any point in Ci.
Lemma 4.2. For every point u ∈ Ui it is the case that dCi(u) ≤ 2OPT for all 1 ≤ i ≤ m.
Given the above lemma  let O1  . . .   Ok denote the clusters in the optimal solution. A cluster in OPT
is deﬁned as a subset of the points in U  not including outliers identiﬁed by OPT  that are closest to
some ﬁxed center chosen by OPT. The high level idea of our proof is similar to that used in [10].
Our goal is to show that when our algorithm choses each center  the set of points discarded from U(cid:48)
in CLUSTER can be mapped to some cluster in the optimal solution. At the end of CLUSTER there
should be at most z points in U(cid:48)  which are the outliers in the optimal solution. Knowing that we
only discard points from U(cid:48) close to centers we choose  this will imply the approximation bound.
For every point u ∈ U  which must fall into some Ui  we let c(u) denote the closest point in Ci to u
(i.e.  c(u) is the closest intermediate cluster center found by GREEDY to u). Consider the output of
CLUSTER  X = {x1  x2  . . .   xk}  ordered by how elements were added to X. We will say that an
optimal cluster Oi is marked at CLUSTER iteration j if there is a point u ∈ Oi such that c(u) /∈ U(cid:48)
just before xj is added to X. Essentially if a cluster is marked  we can make no guarantee about
covering it within some radius of xj (which will then be discarded). Figure 1 shows examples where
Oi is (and is not) marked. We begin by noting that when xj is added to X that the weight of the
points removed from U(cid:48) is at least as large as the maximum number of points in an unmarked cluster
in the optimal solution.

Lemma 4.3. When xj is added  then(cid:80)

wu(cid:48) ≥|Oi| for any unmarked cluster Oi.

u(cid:48)∈Bxj

Given this result  the following lemma considers a point v that is in some
cluster Oi. If c(v) is within the ball Bxj for xj added to X  then intuitively 
this means that we cover all of the points in Oi with B(cid:48)xj . Another way to say
this is that after we remove the ball B(cid:48)xj   no points in Oi contribute weight to
any point in U(cid:48).
Lemma 4.4. Consider that xj is to be added to X. Say that c(v) ∈ Bxj for
some point v ∈ Oi for some i. Then  for every point u ∈ Oi either c(u) ∈ B(cid:48)xj
or c(u) has already been removed from U(cid:48).

See the supplementary material for the proof. The ﬁnal lemma below states
that the weight of the points in ∪xi:1≤i≤kB(cid:48)xi is at least as large as the number
of points in ∪ 1≤i≤kOi. Further  we know that | ∪ 1≤i≤k Oi| = n − z since
OPT has z outliers. Viewing the points in B(cid:48)xi as being assigned to xi in the
algorithm’s solution then this shows that the number of points covered is at
least as large as the number of points that the optimal solution covers. Hence 
there cannot be more than z points uncovered by our algorithm.

Lemma 4.5. (cid:80)k

i=1

(cid:80)

u∈B(cid:48)

xi

wu ≥ n − z

Finally  we are ready to complete the proof of Theorem 1.2.

Figure 1: Examples
in which Oi
is/is
not marked.

Proof of [Theorem 1.2] Lemma 4.5 implies that the sum of the weights of the
points which are in ∪xi:1≤i≤kB(cid:48)xi are at least n − z. We know that every point u contributes to the
weight of some point c(u) which is in Ci for 1 ≤ i ≤ m and by Lemma 4.2 d(u  c(u)) ≤ 2OPT.
We map every point u ∈ U to xi  such that c(u) ∈ B(cid:48)xi. By deﬁnition of B(cid:48)xi and Lemma 4.2 it is
the case d(u  xi) ≤ 13OPT by the triangle inequality. Thus  we have mapped n − z points to some
point in X within distance 13OPT. Hence  our algorithm discards at most n− z points and achieves
a 13-approximation. With Proposition 4.1 we have shown Theorem 1.2.

2

5 Experiments

We evaluate the real-world performance of the above clustering algorithms on seven clustering
datasets  described in Table 1. We compare all methods using the k-center with outliers objective  in
which z outliers may be discarded. We begin with a brief description of the clustering methods we

6

markedOiunmarkedOiOiuc(u)U09vc(v)c(v0)v0deleted from U0Oic(u)U0vc(v)c(v0)v0u8Table 1: The clustering datasets (and their descriptions) used for evaluation.
name
Parkinsons [28]
Census1
Skin1

description
patients with early-stage Parkinson’s disease
census household information
RGB-pixel samples from face images

Yahoo [11] web-search ranking dataset (features are GBRT outputs [29])
Covertype1
Power1
Higgs1

a forest cover dataset with cartographic features
household electric power readings
particle detector measurements (the seven ‘high-level’ features)

n

5  875
45  222
245  057
473  134
522  911
2  049  280
11  000  000

dim.
22
12
3

500
13
7
7

compare. We then show how the distributed algorithms compare with their sequential counterparts
on datasets small enough to run the sequential methods  for a variety of settings. Finally  in the
large-scale setting  we compare all distributed methods for different settings of k.
Methods. We implemented the sequential GREEDY and OUTLIERS and distributed GREEDY-MR
[25] and OUTLIERS-MR. We also implemented two baseline methods: RANDOM|RANDOM: m
machines randomly select k +z points  then a single machine randomly selects k points out of the
previously selected m(k+z) points; RANDOM|OUTLIERS: m machines randomly select k+z points 
then OUTLIERS (Algorithm 4) is run over the m(k+z) points previously selected; All methods were
implemented in MATLABTM and conducted on an 8-core Intel Xeon 2 GHz machine.

Figure 2: The performance of sequential and distributed methods. We plot the objective value of
four small datasets for varying k  z  and m.
Sequential vs. Distributed. Our ﬁrst set of experiments evaluate how close the proposed distributed
methods are to their sequential counterparts. To this end  we vary all parameters: number of centers
k  number of outliers z  and the number of machines m. We consider datasets for which computing
the sequential methods is practical: Parkinsons  Census and two random subsamples (10  000 inputs
each) of Covertype and Power. We show the results in Figure 2. Each column contains the results for
a single dataset and each row for a single varying parameter (k  z  or m)  along with standard errors
over 5 runs. When a parameter is not varied we ﬁx k = 50  z = 256  and m = 10. As expected  the
objective value for all methods generally decreases as k increases (as the distance of any point to its
cluster center must shrink with more clusters). RANDOM|RANDOM and RANDOM|OUTLIERS usu-
ally perform worse than GREEDY-MR for small k (save 10k Covertype) and RANDOM|OUTLIERS

1https://archive.ics.uci.edu/ml/datasets/

7

510152000.511.522.53x 105k=50  z=256m5678900.511.522.533.544.55x 105k=50  m=10log2(z)2040608010000.511.522.53x 105m=10  z=256k510152015202530354045k=50  z=256m5678915202530354045k=50  m=10log2(z)2040608010015202530354045m=10  z=256k51015200510152025303540k=50  z=256m567890510152025303540k=50  m=10log2(z)204060801000510152025303540m=10  z=256k510152000.40.81.21.62k=50  z=256m5678900.40.81.21.62k=50  m=10log2(z)2040608010000.40.81.21.62m=10  z=256kobjective valueParkinsonnumber of clusters: knumber of machines: m10k Covertype10k PowerCensus555number of outliers: log(z)2Random | RandomRandom | OutliersOutliersGreedyGreedy-MROutliers-MRFigure 3: The objective value of ﬁve large-scale datasets  for varying k

sometimes matches it for large k. For all values of k tested  OUTLIERS-MR outperforms all other
distributed methods. Furthermore  it matches or slightly outperforms (which we attribute to ran-
domness) the sequential OUTLIERS method in all settings. As z increases the two random methods
improve  beyond GREEDY-MR in some cases. Similar to the ﬁrst plot  OUTLIERS-MR outperforms
all other distributed methods while matching the sequential clustering method. For very small set-
tings of m (i.e.  m = 2  6)  OUTLIERS-MR and GREEDY-MR perform slightly worse than sequen-
tial OUTLIERS and GREEDY. However  for practical settings of m (i.e.  m ≥ 10)  OUTLIERS-MR
matches OUTLIERS and GREEDY-MR matches GREEDY. In terms of speed  on the largest of these
datasets (Census) OUTLIERS-MR run sequentially is more than 677× faster than OUTLIERS  see
Table 2. This large speedup is due to the fact that we cannot store the full distance matrix for Census 
thus all distances need to be computed on demand.
Large-scale. Our second set of experiments
focus on the performance of the distributed
methods on ﬁve large-scale datasets  shown
in Figure 3. We vary k between 0 and
100  and ﬁx m = 10 and z = 256. Note
that for certain datasets  clustering while tak-
ing into account outliers produces a notice-
able reduction in objective value. On Ya-
hoo  the GREEDY-MR method is even outper-
formed by RANDOM|OUTLIERS that considers
outliers. Similar to the small dataset results
OUTLIERS-MR outperforms nearly all distributed methods (save for small k on Covertype). Even
on datasets where there appear to be few outliers OUTLIERS-MR has excellent performance. Fi-
nally  OUTLIERS-MR is extremely fast: clustering on Higgs took less than 15 minutes.

Table 2: The speedup of the distributed algo-
rithms  run sequentially  over their sequential
counterparts on the small datasets.

10k Power
Parkinson
Census

6.2
9.4
4.4
677.7

3.6
4.8
4.9
12.4

dataset

k-center

outliers

10k Covertype

6 Conclusion

In this work we described algorithms for the k-center and k-center with outliers problems in the dis-
tributed setting. For both problems we studied two round MapReduce algorithms which achieve an
O(1)-approximation and demonstrated that they perform almost identically to their sequential coun-
terparts on real data. Further  a number of our experiments validate that using k-center clustering
on noisy data degrades the quality of the solution. We hope these techniques lead to the discovery
of fast and efﬁcient distributed algorithms for other clustering problems. In particular  what can be
shown for the k-median or k-means with outliers problems are exciting open questions.
Acknowledgments GM was supported by CAPES/BR; MJK and KQW were supported by the NSF
grants IIA-1355406  IIS-1149882  EFRI-1137211; and BM was supported by the Google and Yahoo
Research Awards.

References
[1] P. K. Agarwal and J. M. Phillips. An efﬁcient algorithm for 2d euclidean 2-center with outliers. In ESA 

pages 64–75  2008.

[2] C. C. Aggarwal  J. L. Wolf  and P. S Yu. Method for targeted advertising on the web based on accumulated
self-learning data  clustering users and semantic node graph techniques  March 30 2004. US Patent
6 714 975.

[3] N. Ailon  R. Jaiswal  and C. Monteleoni. Streaming k-means approximation.

2009.

In NIPS  pages 10–18 

8

20406080100020406080100m=10  z=256k2040608010011.21.41.61.822.22.4m=10  z=256k20406080100050100150200250m=10  z=256kCovertypePowerobjective valueSkin204060801000.150.20.250.3m=10  z=256kYahoo2040608010005101520m=10  z=256kHiggsnumber of clusters: kRandom | RandomRandom | OutliersGreedy-MROutliers-MR[4] A. Andoni  A. Nikolov  K. Onak  and G. Yaroslavtsev. Parallel algorithms for geometric graph problems.

In STOC  pages 574–583  2014.

[5] B. Bahmani  R. Kumar  and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. PVLDB  5

(5):454–465  2012.

[6] Bahman Bahmani  Benjamin Moseley  Andrea Vattani  Ravi Kumar  and Sergei Vassilvitskii. Scalable

k-means++. PVLDB  5(7):622–633  2012.

[7] M. Balcan  S. Ehrlich  and Y. Liang. Distributed k-means and k-median clustering on general communi-

cation topologies. In NIPS  pages 1995–2003  2013.

[8] Rafael Barbosa  Alina Ene  Huy Nguyen  and Justin Ward. The power of randomization: Distributed

submodular maximization on massive datasets. In ICML  pages 1236–1244  2015.

[9] A. Z. Broder  L. G. Pueyo  V. Josifovski  S. Vassilvitskii  and S. Venkatesan. Scalable k-means by ranked

retrieval. In WSDM  pages 233–242  2014.

[10] M. Charikar  S. Khuller  D. M. Mount  and G. Narasimhan. Algorithms for facility location problems

with outliers. In SODA  pages 642–651  2001.

[11] M. Chen  K. Q. Weinberger  O. Chapelle  D. Kedem  and Z. Xu. Classiﬁer cascade for minimizing feature

evaluation cost. In AIStats  pages 218–226  2012.

[12] F. Chierichetti  R. Kumar  and A. Tomkins. Max-cover in map-reduce. In WWW  pages 231–240  2010.
[13] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing on large clusters. In OSDI  pages

137–150  2004.

[14] A. Ene  S. Im  and B. Moseley. Fast clustering using MapReduce. In KDD  pages 681–689  2011.
[15] J. Feldman  S. Muthukrishnan  A. Sidiropoulos  C. Stein  and Z. Svitkina. On distributing symmetric

streaming computations. In SODA  pages 710–719  2008.

[16] T. F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science 

38(0):293 – 306  1985. ISSN 0304-3975.

[17] S. Guha  A. Meyerson  N. Mishra  R. Motwani  and L. O’Callaghan. Clustering data streams: Theory and

practice. IEEE Trans. Knowl. Data Eng.  15(3):515–528  2003.

[18] Sudipto Guha  Rajeev Rastogi  and Kyuseok Shim. Techniques for clustering massive data sets.

In
Clustering and Information Retrieval  volume 11 of Network Theory and Applications  pages 35–82.
Springer US  2004. ISBN 978-1-4613-7949-2.

[19] M. Hassani  E. M¨uller  and T. Seidl. EDISKCO: energy efﬁcient distributed in-sensor-network k-center

clustering with outliers. In SensorKDD-Workshop  pages 39–48  2009.

[20] D. S. Hochbaum and D. B. Shmoys. A best possible heuristic for the k-center problem. Mathematics of

Operations Research  10(2):180–184  1985.

[21] H. J. Karloff  S. Suri  and S. Vassilvitskii. A model of computation for MapReduce. In SODA  pages

938–948  2010.

[22] L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: An Introduction to Cluster Analysis. Wiley-

Interscience  9th edition  March 1990. ISBN 0471878766.

[23] Ravi Kumar  Benjamin Moseley  Sergei Vassilvitskii  and Andrea Vattani. Fast greedy algorithms in

mapreduce and streaming. In SPAA  pages 1–10  2013.

[24] R. M. McCutchen and S. Khuller. Streaming algorithms for k-center clustering with outliers and with

anonymity. In APPROX-RANDOM  pages 165–178  2008.

[25] B. Mirzasoleiman  A. Karbasi  R. Sarkar  and A. Krause. Distributed submodular maximization: Identi-

fying representative elements in massive data. In NIPS  pages 2049–2057  2013.

[26] M. Shindler  A. Wong  and A. W. Meyerson. Fast and accurate k-means for large datasets. In NIPS  pages

2375–2383  2011.

[27] S. Suri and S. Vassilvitskii. Counting triangles and the curse of the last reducer. In WWW  pages 607–614 

2011.

[28] A. Tsanas  M. A Little  P. E McSharry  and L. O Ramig. Enhanced classical dysphonia measures and
In ICASSP  pages 594–597.

sparse regression for telemonitoring of parkinson’s disease progression.
IEEE  2010.

[29] S. Tyree  K.Q. Weinberger  K. Agrawal  and J. Paykin. Parallel boosted regression trees for web search

ranking. In WWW  pages 387–396. ACM  2011.

[30] O. Zamir  O. Etzioni  O. Madani  and R. M Karp. Fast and intuitive clustering of web documents. In

KDD  volume 97  pages 287–290  1997.

[31] Z. Zhao  G. Wang  A.R. Butt  M. Khan  V.S.A. Kumar  and M.V. Marathe. Sahad: Subgraph analysis in

massive networks using hadoop. In IPDPS  pages 390–401  May 2012.

9

,Gustavo Malkomes
Matt Kusner
Wenlin Chen
Kilian Weinberger
Benjamin Moseley
Han-Jia Ye
De-Chuan Zhan
Xue-Min Si
Yuan Jiang
Zhi-Hua Zhou
Ehsan Elhamifar
M. Clara De Paolis Kaluza