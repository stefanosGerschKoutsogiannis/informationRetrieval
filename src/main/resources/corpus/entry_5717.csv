2018,Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units,This paper presents a general framework for norm-based capacity control for $L_{p q}$ weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an $L_{p q}$ normalization where $q\le p^*$ and $1/p+1/p^{*}=1$  we discuss properties of a width-independent capacity control  which only depends on the depth by a square root term. We further analyze the approximation properties of $L_{p q}$ weight normalized deep neural networks. In particular  for an $L_{1 \infty}$ weight normalized network  the approximation error can be controlled by the $L_1$ norm of the output layer  and the corresponding generalization error only depends on the architecture by the square root of the depth.,Understanding Weight Normalized Deep Neural

Networks with Rectiﬁed Linear Units

Yixi Xu

Department of Statistics

Purdue University

West Lafayette  IN 47907

xu573@purdue.edu

Xiao Wang

Department of Statistics

Purdue University

West Lafayette  IN 47907
wangxiao@purdue.edu

Abstract

This paper presents a general framework for norm-based capacity control for Lp q
weight normalized deep neural networks. We establish the upper bound on the
Rademacher complexities of this family. With an Lp q normalization where q ≤ p∗
and 1/p+1/p∗ = 1  we discuss properties of a width-independent capacity control 
which only depends on the depth by a square root term. We further analyze the
approximation properties of Lp q weight normalized deep neural networks. In
particular  for an L1 ∞ weight normalized network  the approximation error can be
controlled by the L1 norm of the output layer  and the corresponding generalization
error only depends on the architecture by the square root of the depth.

1

Introduction

During the past decade  deep neural networks (DNNs) have demonstrated an amazing performance in
solving many complex artiﬁcial intelligence tasks such as object recognition and identiﬁcation  text
understanding and translation  question answering  and more [11]. The capacity of unregularized fully
connected DNNs  as a function of the network size and depth  is fairly well understood [1  4  23]. By
bounding the L2 norm of the incoming weights of each unit  [22] is able to accelerate the convergence
of stochastic gradient descent optimization across applications in supervised image recognition 
generative modeling  and deep reinforcement learning. However  theoretical investigations on such
networks are less explored in the literature  and a few exceptions are [4  5  10  18  19  25]. There is a
central question waiting for an answer: Can we bound the capacity of fully connected DNNs with
bias neurons by weight normalization alone  which has the least dependence on the architecture?
In this paper  we focus on networks with rectiﬁed linear units (ReLU) and study a more general
weight normalized deep neural network (WN-DNN)  which includes all layer-wise Lp q weight
normalizations. In addition  these networks have a bias neuron per hidden layer  while prior stud-
ies [4  5  10  18  19  25] either exclude the bias neuron  or only include the bias neuron in the
input layer  which differs from the practical application. We establish the upper bound on the
Rademacher complexities of this family and study the theoretical properties of WN-DNNs in terms
of the approximation error.
We ﬁrst examine how the Lp q WN-DNN architecture inﬂuences their generalization properties.
Speciﬁcally  for Lp q normalization where q ≤ p∗ and 1/p + 1/p∗ = 1  we obtain a complexity
bound that is independent of width and only has a square root dependence on the depth. To the
best of our knowledge  this is the ﬁrst theoretical result for the fully connected DNNs including a
bias neuron for each hidden layer in terms of generalization. We will demonstrate later that it is
nontrivial to extend the existing results to the DNNs with bias neurons. Even excluding the bias
neurons  existing generalization bounds for DNNs depend on either width or depth logarithmically
[5]  polynomially[10  18]  or even exponentially [19  25]. Even for [5]  the logarithmic dependency

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

is not always guaranteed  as the margin bound is

log(max d)/

√

n

O

(cid:13)(cid:13)(cid:13)WT

i − MT

i

(cid:13)(cid:13)(cid:13)2/3

2 1

/(cid:107)Wi(cid:107)2/3
σ

(cid:33)3/2  

(cid:107)Wi(cid:107)σ

i=1

k(cid:89)
(cid:13)(cid:13)(cid:13)WT

(cid:32) k(cid:88)
(cid:13)(cid:13)(cid:13)2 1

i=1

where (cid:107)·(cid:107)σ is the spectral norm  and Mi is a collection of predetermined reference matrix. The
bound will worsen  when the Wi moves farther from Mi. For example  if

for some constant A0  then the above bound will rely on the network size by O(cid:0)log(max d)k3/2(cid:1).

/(cid:107)Wi(cid:107)σ ≥ A0

i − MT

i

We also examine the approximation error of WN-DNNs. It is shown that the L1 ∞ WN-DNN is
able to approximate any Lipschitz continuous function arbitrarily well by increasing the norm of
its output layer and growing its size. Early work on neural network approximation theory includes
the universal approximation theorem [8  13  20]  indicating that a fully connected network with a
single hidden layer can approximate any continuous functions. More recent work expands the result
of shallow networks to deep networks with an increased interest in the expressive power of deep
networks especially for some families of "hard" functions [2  9  16  21  26  27]. For instance  [26]
shows that for any positive integer l  there exist neural networks with Θ(l3) layers and Θ(1) nodes per
layer  which can not be approximated by networks with Θ(l) layers unless they possess Ω(2l) nodes.
These results on the other hand request for an artiﬁcial neural network of which the generalization
bounds grow slowly with depth and even avoid explicit dependence on depth.
The contributions of this paper are summarized as follows.

1. We extend the L2 ∞ weight normalization [22] to the more general Lp q WN-DNNs and
2. We include a bias node not only in the input layer but also in every hidden layer. As

relate these classes to those represented by unregularized DNNs.

discussed in Claim 1  it is nontrivial to extend prior research to study this case.
3. We study the Rademacher complexities of WN-DNNs. Especially  with any Lp q normaliza-
tion satisfying that q ≤ p∗  we have a capacity control that is independent of the width and
depends on the depth by O(

k).

√

4. We analyze the approximation property of Lp q WN-DNNs and further show the theoretical

advantage of L1 ∞ WN-DNNs.

The paper is organized as follows. In Section 2  we deﬁne the Lp q WN-DNNs and analyze the
corresponding function class. Section 3 gives the Rademacher complexities. In Section 4  we provide
the error bounds for the approximation error of Lipschitz continuous functions.

2 Preliminaries

In this section  we deﬁne the WN-DNNs  of which the weights and biases for all layers are scaled
by some norm up to a normalization constant c. Furthermore  we demonstrate how it surpasses
unregularized DNNs theoretically.
A neural network on Rd0 → Rdk+1 with k hidden layers is deﬁned by a set of k + 1 afﬁne trans-
formations T1 : Rd0 → Rd1  T2 : Rd1 → Rd2  ···   Tk+1 : Rdk → Rdk+1 and the ReLU activation
σ(u) = (u)+ = uI{u > 0}. The afﬁne transformations are parameterized by Ti(u) = W T
i u + Bi 
where W i ∈ Rdi−1×di  Bi ∈ Rdi for i = 1 ···   k + 1. The function represented by this neural
network is

f (x) = Tk+1 ◦ σ ◦ Tk ◦ ··· ◦ σ ◦ T1 ◦ x

Before introducing Lp q WN-DNNs  we build an augmented layer for each hidden layer by appending
the bias neuron 1 to the original layer  then combine the weight matrix and the bias vector as a new
matrix.
Deﬁne f∗0 (x) = (1  xT )T . Then the ﬁrst hidden layer

f1(x) = T1 ◦ x (cid:44) ˜V

T
1 f∗0 (x) 

2

where ˜V1 = (B1  W T

1 )T ∈ R(d0+1)×d1. Deﬁne the augmented ﬁrst hidden layer as

f∗1 (x) = (1  (f1(x))T )T ∈ Rd1+1.

Then f∗1 (x) (cid:44) VT
Rd0+1. Sequentially for i = 2 ···   k  deﬁne the ith hidden layer as

1 f∗0 (x)  where V1 = (e10  ˜V1) ∈ R(d0+1)×(d1+1) and e10 = (1  0 ···   0)T ∈

fi(x) = Ti ◦ σ ◦ fi−1(x) (cid:44) (cid:104) ˜Vi  σ ◦ f∗i−1(x)(cid:105) 

where ˜Vi = (Bi  W T
The augmented ith hidden layer is

i )T ∈ R(di−1+1)×di. Note that σ(1) = 1  thus (1  σ ◦ fi−1(x)) = σ ◦ f∗i−1(x).

(1)

(2)

(3)

(4)

f∗i (x) = (1  (fi(x))T )T ∈ Rdi+1 

and f∗i (x) (cid:44) (cid:104)Vi  σ ◦ f∗i−1(x)(cid:105)  where
and e1i = (1  0 ···   0)T ∈ Rdi−1+1. The output layer is

Vi = (e1i  ˜Vi) ∈ R(di−1+1)×(di+1) 

f (x) = Tk+1 ◦ σ ◦ f∗k (x) (cid:44) (cid:104) ˜Vk+1  σ ◦ f∗k (x)(cid:105) 
k+1)T ∈ R(dk+1)×dk+1.

where ˜Vk+1 = (Bk+1  W T
The Lp  q Norm. The Lp  q norm of a s1 × s2 matrix A is deﬁned as

 s2(cid:88)

(cid:32) s1(cid:88)

j=1

i=1

(cid:33)q/p1/q

 

|aij|p

(cid:107)A(cid:107)p q =

(cid:18) s1(cid:80)

(cid:19)1/p

(cid:48)

(cid:13)(cid:13)(cid:13)(cid:13) T1

(cid:13)(cid:13)(cid:13)(cid:13)

where 1 ≤ p < ∞ and 1 ≤ q ≤ ∞. When q = ∞  (cid:107)A(cid:107)p ∞
p = q = 2  the Lp q is the Frobenius norm.
We motivate our introduction of WN-DNNs with a negative result when directly applying existing
studies on fully connected DNNs with bias neurons.
A Motivating Example. As shown in Figure 1a  deﬁne f = T2 ◦ σ ◦ T1 : R → R  where
T1(x) = (−x + 1 −x − 1) (cid:44) ˜V
2 (1  u1  u2)T . Consider
f

1 (1  x)T and T2(u) = 1 − u1 − u2 (cid:44) ˜V

100 T1  as shown in Figure 1b . Then

= 100T2 ◦ σ ◦ 1

|aij|p

. When

= supj

i=1

T

T

(cid:48)

(cid:48)

(x) = 100 − σ(−x + 1) − σ(−x − 1) = 99 + f (x)

f

Note that the product of the norms of all layers for f

remains the same as that for f:

 

∗

∗

(cid:13)(cid:13)(cid:13)

∗ (cid:107)T1(cid:107)
∗

(cid:13)(cid:13)(cid:13) ˜Vi

(cid:107)100T2(cid:107)
∗

= (cid:107)T2(cid:107)
∗
100
∗
where the norm of the afﬁne transformation (cid:107)Ti(cid:107)
is deﬁned as the norm of its corresponding linear
∗
transformation matrix
for i = 1  2. Using a similar trick  we could replace the 100 in this
example with any positive number. This on the other hand suggests an unbounded output even when
the product of the norms of all layers is small.
Furthermore  a negative result will be presented in terms of Rademacher complexity in the following
claim.
Claim 1. Deﬁne N k d
γ∗≤γ as a function class that contains all functions representable by some neural
network of depth k+1 and widths d: f = Tk+1◦σ◦Tk◦···◦σ◦T1◦x  where d = (m1  d1 ···   dk  1) 
i (1  uT )T   for i = 1 ···   k + 1  such that
(cid:107)·(cid:107)
∗
≤ γ.
Then for a ﬁxed n and any sample S = {x1 ···   xn} ⊆ Rm1 

is an arbitrary norm  and Ti(u) : Rdi−1 → Rdi = ˜V

γ∗ =

T

(cid:13)(cid:13)(cid:13) ˜Vi
(cid:13)(cid:13)(cid:13)
k+1(cid:89)
(cid:98)RS(N k d
γ∗≤γ) = ∞.

i=1

∗

3

(a) Visualization of f.

(b) Visualization of f

(cid:48)

.

Figure 1: The motivating example.

i=1

k+1(cid:81)

(cid:107)Wi(cid:107)
∗

Claim 1 shows the failure of current norm-based constraints on fully connected neural networks with
the bias neuron in each hidden layer. Prior studies [4  5  10  18  19  25] included the bias neuron only
in the input layer and considered layered networks parameterized by a sequence of weight matrices
only  that is Bi = 0 for all i = 1 ···   k + 1. While ﬁxing the architecture of neural networks  these
works imply that
is sufﬁcient to control the Rademacher complexity of the function class
represented by these DNNs  where (cid:107)·(cid:107)
is the spectral norm in [5  18]  the L1 ∞ norm in [4  25]  the
L1 ∞/L2 2 norm in [10]  and the Lp q norm in [19] for any p ∈ [1 ∞)  q ∈ [1 ∞]. However  this
∗
kind of control fails once the bias neuron is added to each hidden layer  demonstrating the necessity
to use WN-DNNs instead.
The Lp q WN-DNNs. An Lp q WN-DNN by a normalization constant c ≥ 1 with k hidden layers
is deﬁned by a set of k + 1 afﬁne transformations T1 : Rd0 → Rd1  T2 : Rd1 → Rd2 ···   Tk+1 :
Rdk → Rdk+1 and the ReLU activation  where Ti(u) = ˜V
i (1  uT )T   ˜Vi ∈ R(di−1+1)×di and
(cid:107)Ti(cid:107)p q
Deﬁne N k d
with the normalization constant c satisfying:

  for i = 1 ···   k + 1. In addition  (cid:107)Ti(cid:107)p q ≡ c for i = 1 ···   k.

p q c co as the collection of all functions that could be represented by an Lp q WN-DNN

(cid:44)(cid:13)(cid:13)(cid:13) ˜Vi

(cid:13)(cid:13)(cid:13)p q

T

(a) The number of neurons in the ith hidden layer is di for i = 1  2 ···   k. The dimension of

input is d0  and output dk+1;

(b) It has k hidden layers;
(c) (cid:107)Ti(cid:107)p q ≡ c for i = 1 ···   k;
(d) (cid:107)Tk+1(cid:107)p q ≤ co.

The following theorem provides some useful observations regarding N k d
Theorem 1. Let c  co  c1  c2  c1
(d0  d1 ···   dk+1) ∈ Nk+2
Nk2+2

o > 0  p ∈ [1 ∞)  q ∈ [1 ∞]  k  k1  k2 ∈ N  d =
k2+1) ∈
0  d1

.
(a) A function f : Rd0 → Rdk+1 = Tk+1 ◦ σ ◦ Tk ◦ ··· ◦ σ ◦ T1 ◦ x  where Ti(u) =
  as long as (cid:107)Ti(cid:107)p q ≤ c for i = 1 ···   k

i u + Bi : Rdi−1 → Rdi. Then f ∈ N k d

k1+1) ∈ Nk1+2

+   d1 = (d1

  and d2 = (d2

1 ···   d2

1 ···   d1

p q c co.

0  d2

o  c2

p q c co

+

+

W T
and (cid:107)Tk+1(cid:107)p q ≤ co.
⊆ N k d

p q c1 co

(b) N k d

cog ∈ N k d

p q c co

p q c2 co
.

if c1 ≤ c2. N k d

p q c c1
o

⊆ N k d

p q c c2
o

if c1

o ≤ c2

o. If g ∈ N k d

p q c 1  then

(c) N k d

p1 q c co

⊆ N k d

p2 q c co

if 1 ≤ p1 ≤ p2 < ∞. N k d

p q1 c co

⊆ N k d

p q2 c co

if 1 ≤ q1 ≤ q2 ≤ ∞.

4

1x11−11−1y1−1−11x10.01−0.010.01−0.01y100−100−100N k d
p ∞ c co
when dk+1 = 1  ˜co = co.

⊆ N k d

p q ˜c ˜co

  where ˜c = c max

1

q {d1  d2 ···   dk} and ˜co = d

1
q

k+1co. Especially 

(d) N k1 d1

⊆ N k2 d2
for i > k1  and d2

p q c co

p q c co

k2+1 = d1

k1+1 = 1.

if c ≥ 1  k1 ≤ k2  d2

0 = d1

0  d2

i ≥ d1

i for i = 1 ···   k1  d2

i ≥ d1

k1+1

In particular  Part (a) connects normalized neural networks to unregularized DNNs. Part (b) shows
the increased expressive power of neural networks by increasing the normalization constant or the
output layer norm constraint. Part (c) discusses the inﬂuence of the choice of Lp q normalization on
its representation capacity. Part (d) describes the gain in representation power by either widening or
deepening the neural networks.

3 Estimating the Rademacher Complexities of N k d

p q c co

In this section  we bound the Rademacher complexities of N k d
p q c co  where d0 = m1 and dk+1 = 1.
Without loss of generality  assume the input space X = [−1  1]m1 in the following sections. Further
deﬁne p∗ by 1/p + 1/p∗ = 1.
Proposition 1. Fix q ≥ 1  k ≥ 0  c  co > 0  di ∈ N+ for i = 1 ···   k  then for any set S =
{x1 ···   xn} ⊆ X   we have

(cid:98)RS(N k d

1 q c co) ≤ co√
(cid:112)

n

(cid:16)
k(cid:88)

i=0

(cid:112)

(cid:112)

min

2 max(1  ck)

k + 2 + log(m1 + 1) 

(cid:33)

(cid:112)

k log 16

ci + ck(

2 log(2m1) +

k log 16)

.

Proof sketch. As σ(1) = 1  we could treat the bias neuron in the ith hidden layer as a hidden neuron
computed from the (i − 1)th hidden layer by
σ(eT

1if∗i−1(x)) = 1 

where e1i = (1  0 ···   0)T ∈ Rdi−1+1  and f∗i−1 is the augmented (i − 1)th hidden layer as deﬁned
in Equation (2). Therefore  the new afﬁne transformation could be parameterized by Vi deﬁned in
Equation (3)  such that (cid:107)Vi(cid:107)1 ∞
= max(1  c). Then the result is the minimum of the bound of [10 
Theorem 2] on DNNs without bias neurons and that of Proposition 2 when p = 1.
Proposition 2. Fix p  q ≥ 1  k ≥ 0  c  co > 0  di ∈ N+ for i = 1 ···   k  then for any set
S = {x1 ···   xn} ⊆ X   we have

k(cid:89)

(cid:32)k+1(cid:88)
(cid:104)

i=1

(k + 1) log 16

n

q ]+

p∗ − 1
[ 1
i

d

1

p∗
1

m

ck−i+1

(cid:16)

l=i

(cid:112)
p∗ − 1 

(cid:112)

min

(

[ 1
p∗ − 1
d
l

q ]+

+

2 log(2m1)

(cid:17)

(cid:112)

+

(k + 1) log 16

(cid:105)

 

(cid:33)

+

ck−i+1

[ 1
p∗ − 1
d
l

q ]+

(cid:32)k+1(cid:88)
(cid:16)(cid:112)

i=1

1

p∗
m
1

k(cid:89)

l=i

2 log(2m1) +

(k + 1) log 16

(5)

(6)

(cid:17)

.

(cid:33)

(cid:112)

(a) for p ∈ (1  2] 

(cid:98)RS(N k d

p q c co) ≤ co

cock√
n

(cid:114)
k(cid:89)

i=1

(b) for p ∈ 1 ∪ (2 ∞) 

(cid:98)RS(N k d

p q c co) ≤ co

(cid:114)

1√
n

cock

(k + 1) log 16

k(cid:89)

n

p∗ − 1
[ 1
d
i

q ]+

i=1

5

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)p∗

 

iσ ◦ fj(xi)

Zj =

sup
f∈N k d

p q c co

(cid:32)

E exp(tZj) ≤ 4j exp

t2ns2
j

2

+ tcj

[ 1
p∗ − 1
d
i

q ]+

where

sj =

j(cid:88)

i=2

j(cid:89)

l=i

cj−i+1

q ]+

[ 1
p∗ − 1
l

d

∗

+ (m1/p

1 + 1)cj

j(cid:89)

i=1

(cid:33)

Ap

m1 S

 

q ]+

[ 1
p∗ − 1
l

 

d

j(cid:89)

l=1

(cid:33) .

if (xi)

(cid:32) n(cid:88)

i=1

Proof sketch. The proof consists of two steps. In the ﬁrst step  following the notations in Section 2 
we deﬁne a series of random variables

where {1 ···   n} are n i.i.d Rademacher random variables  and fj is the jth hidden layer of the
neural network f. We prove by induction that for any t ∈ R 

and Ap
m1 S is some constant only depends on the sample. In addition  we relies on Hölder’s inequality
with an optimal parameter to separate the bias neuron. Step 2 is motivated by the idea of [10]. By
Jensen’s inequality

n(cid:98)RS(N k d

sup
f∈N k d
Finally we get the desired result by choosing the optimal λ.

p q c co

p q c co) ≤ 1
λ

log E exp

λ

When d = d1  the upper bound of Rademacher complexity depends on the width by O(dk[ 1
q ]+) 
which is similar to the case without bias neurons [19]. Furthermore  the dependence on widths
disappears as long as q ∈ [1  p∗].
In order to investigate the tightness of the bound given in
Proposition 2  we consider the binary classiﬁcation as a speciﬁc case  indicating that when 1
q < 1 
the dependence on width is unavoidable.
Proposition 3. [19  Theorem 3] For any p  q ≥ 1  d = d1 and any k ≥ 2  n {−1  +1} points could
be shattered with unit margin by N k d

p + 1

p∗ − 1

  with

p q c co

ckco ≤ (log2 n)

1

p n( 1

p + 1

q )d−(k−2)[ 1

p∗ − 1

q ]+.

k

k2

Issues on Bias Neurons. Lp q norm-constrained fully connected DNNs with no bias neuron were
investigated in prior studies [4  10  19  25]. First of all  the generalization bounds given by [4  19  25]
have explicit exponential dependence on the depth  thus it is not meaningful to compare these results
with ours. Secondly  [10] provides the up-to-date Rademacher complexity bounds of both L1 ∞ and
L2 2 norm-constrained fully connected DNNs without bias neurons. However  it is not straightforward
to extend their results to fully connected DNNs with a bias neuron in each hidden layer. For example 
consider the L2 2 WN-DNNs with c = 1. If we simply treat each bias neuron as a hidden neuron  as
√
in the proof for Proposition 1  the complexity bounds [10] grows exponentially with respect to the
depth by O(
2 )  while our Proposition 2 gives a much tighter bound O(k
and L2 2 WN-DNNs.
Comparison with [10] on the Rademacher compexity bounds of L1 ∞
[10] is the most recent work on the Rademacher complexities of the L1 ∞ and L2 2 norm-constrained
fully connected DNNs without bias neurons. Consider a speciﬁc case when log(m1) is small and
co = 1 to shed light on the possible inﬂuence of the bias neurons on the generalization properties.
As summarized in Table 1  these comparisons suggest that the inclusion of a bias neuron in each
hidden layer might lead to extra dependence of generalization bounds on the depth especially when c
kck → 0  as k → ∞. For
is small. Note that  when c < 1 
√k√n ) without bias
L2 2 WN-DNNs  when c = 1  the bounds are O( k
) if including bias neurons
neurons. For L2 2 WN-DNNs  when c > 1  the bounds are O(
√kck
and O(
√n ) if excluding bias neurons. Another interesting observation is that the complexity
bounds remain the same no matter whether bias neurons are included or not  when c > 1 for L1 ∞
WNN-DNNs.

k(1 − ck+1)/(1 − c) → ∞  while

2√n ) if with bias neurons and O(

√k(ck+1−1)

√

√

2 ).

√n

3

3

6

With Bias Neurons Without Bias Neurons [10]

c < 1

O(

c = 1  L1 ∞
c = 1  L2 2
c > 1  L1 ∞
c > 1  L2 2

O(

√k(1−ck+1)
(1−c)√n )
√k√n )
O(
O( k3/2
√n )
√kck
√n )
O(
√k(ck+1−1)
√n

)

√kck
√n )
√k√n )
√k√n )
√kck
√n )
√kck
√n )

O(

O(

O(

O(

O(

Table 1: Rademacher complexity bounds for L1 ∞/L2 2 WN-DNNs with/without bias neurons.

4 Approximation Properties

r(cid:88)

In this section  we analyze the approximation properties of Lp q WN-DNNs and show the theoretical
advantage of L1 ∞ WN-DNN. We ﬁrst introduce a technical lemma  demonstrating that any wide
one-hidden-layer neural network could be exactly represented by a deep but narrow normalized
neural network. In addition  Lemma 1 indicates that N 1 (m1 r 1)
for
any r > 1  k ∈ N   and co > 0  where [x] is the smallest integer which is greater than or equal to x 
1 ∞ · co
and 1k = (1 ···   1) ∈ Rk.
Lemma 1. Assume that a function

⊆ N k (m1 ([r/k]+2m1+3)1k 1)

p ∞ 1 2co

r(cid:80)

i=1

satisﬁes that

g(x) : Rm1 → R =

|ci| ≤ co and(cid:13)(cid:13)(bi  wT

i )(cid:13)(cid:13)1 = 1. Then for any integer k ∈ [1  r] 

i=1

ciσ(wT

i x + bi)

where widk = [r/k] + 2m1 + 3  dk

0 = m1  dk

i = widk for i = 1 ···   k  and dk

k+1 = 1.

g ∈ N k dk

p q wid1/q

k  2co

 

Proof sketch. Note that the shallow neural network g could be decomposed as

i   c−i > 0 and r1 + r2 = r. We consider a simpliﬁed case when g(x) =

i

i=1

c+

(cid:1)  

i )T x + b+

(cid:1) − r2(cid:88)

c−i σ(cid:0)(w−i )T x + b−i

r1(cid:88)
i σ(cid:0)(w+
(cid:1) to illustrate the main idea of our proof. Without loss of generality  as-
i )(cid:13)(cid:13)1 = 1. First create a set
C = {σ(cid:0)(w+

(cid:1)   i = 1 ···   r1}.

i )T x + b+

i=1

i

where c+

i σ(cid:0)(w+
r1(cid:80)
sume that(cid:13)(cid:13)(bi  2wT

c+

i=1

i )T x + b+

i

In order to build a k + 1-layer WN-DNN to represent g  we partition C into k equally sized subsets:
C1 ···  Ck. The key idea is to get all elements of Cj in the jth hidden layer for j = 1 ···   k  while
keeping both σ◦x  and σ◦−x. In addition  the normalized cumulative sum Sj of ∪i≤jCi is computed
in the j + 1th hidden layer. More speciﬁcally 

jr1/k(cid:80)

i σ(cid:0)(w+
jr1/k(cid:80)

c+

c+
i

i=1

Sj =

i=1

(cid:1)

.

i )T x + b+

i

Note that

(w+

i )T x + b+

i = (w+

i )T σ ◦ x − (w+

i )T σ ◦ (−x) + b+
i  

7

and

(j−1)r1/k(cid:80)
jr1/k(cid:80)

i=1

jr1/k(cid:88)

σ(cid:0)(w+

(cid:1) .

i

i=1

i=1

c+
i

c+
i

c+
i

Sj =

i )T x + b+

i=(j−1)r1/k+1

σ(Sj−1) +

jr1/k(cid:80)
c+
i
Thus the L1 ∞ norm of the corresponding transformation still ≤ 1.
Based on Lemma 1  we establish that a WN-DNN is able to approximate any Lipschitz-continuous
function arbitrarily well by loosing the constraint for the norm of the output layer and either widening
or deepening the neural network at the same time. Especially  for Lp ∞ WN-DNNs  the approximation
error could be purely controlled by the norm of the output layer  while the Lp ∞ norm of each hidden
layer is ﬁxed to be 1.
≤ L  and |f (x) − f (y)| ≤
Theorem 2. f
q ∈ [1 ∞]  and any integer k ∈
L(cid:107)x − y(cid:107)
∞
]  if co greater than a constant depending
[1  Cr(m1)(log co
only on m1  there exists a function h ∈ N k dk

L )−2(m1+1)/(m1+4)(cid:0) co

: X → R 
Then for any p ∈ [1 ∞) 

satisfying that (cid:107)f(cid:107)

  where

∞

L

.

(cid:1)2(m1+3)/(m1+4)
(cid:16) co

p q wid1/q

k  2co

)− 2(m1+1)

m1+4

co
L

(cid:17) 2(m1+3)

m1+4

L

] + 2m1 + 3 

widk = [k−1Cr(m1)(log
dk = (m1  widk ···   widk  1)  such that

|f (x) − h(x)| ≤ C(m1)L(

)− 2

m1+1 log

co
L

co
L

 

sup

(cid:107)x(cid:107)∞≤1

where Cr(m1) and C(m1) denotes some constant that depends only on m1.

Theorem 2 shows that the approximation bounds could be controlled by co given a sufﬁciently deep
or wide Lp q WN-DNN. Assume that the loss function is 1-Lipschitz continuous  then the dependence
of the corresponding generalization bound on the architecture for each N k dk
deﬁned above
are summarized as follows:
(a) p = 1  q = ∞: O
(b) p = 1  q < ∞: O
(c) p > 1  q ∈ (p∗ ∞]: O
(d) p > 1  q ∈ [1  p∗]: O

(cid:17)
(cid:16)√
(cid:18)√
(cid:16)√
(cid:16)√

p∗ ]k(cid:17)
q ]k(cid:17)

kco[(1 + widk)

k
q
kcowid
k

kco[(1 + widk)

p q wid1/q

(cid:19)

k  2co

kco

;

;

;

.

1

1

5 Concluding Remarks

We present a general framework for capacity control on WN-DNNs. In particular  we provide a
satisfying answer for the central question: we obtain the generalization bounds for L1 ∞ WN-DNNs
that grows with depth by a square root term while getting the approximation error controlled. It will
be interesting to extend this work to mullticlass classiﬁcation. However  if handling via Radermacher
complexity analysis  the generalization bound will depend on the square root of the number of classes
[28]. Besides the extension to convolutional neural networks  we are also working on the design of
effective algorithms for L1 ∞ WN-DNNs.
Acknowledgments

We thank the anonymous reviewers for their careful reading of our manuscript and their insightful
comments that have greatly improved the paper.

8

References
[1] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations.

cambridge university press  2009.

[2] Raman Arora  Amitabh Basu  Poorya Mianjy  and Anirbit Mukherjee. Understanding deep
neural networks with rectiﬁed linear units. In International Conference on Learning Represen-
tations  2018.

[3] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of

Machine Learning Research  18(19):1–53  2017.

[4] Peter L Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size
of the weights is more important than the size of the network. IEEE transactions on Information
Theory  44(2):525–536  1998.

[5] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems  pages 6241–6250 
2017.

[6] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3:463–482  2002.

[7] Stéphane Boucheron  Gábor Lugosi  and Olivier Bousquet. Concentration inequalities. In

Summer School on Machine Learning  pages 208–240  2003.

[8] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of

control  signals and systems  2(4):303–314  1989.

[9] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In

Conference on Learning Theory  pages 907–940  2016.

[10] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity of

neural networks. In Proceedings of the 31st Conference On Learning Theory  2018.

[11] Ian Goodfellow  Yoshua Bengio  Aaron Courville  and Yoshua Bengio. Deep learning  volume 1.

MIT press Cambridge  2016.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[13] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks 

4(2):251–257  1991.

[14] Sham M Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds  margin bounds  and regularization. In Advances in Neural Information Processing
Systems  pages 793–800  2009.

[15] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and

processes. Springer Science & Business Media  2013.

[16] Shiyu Liang and R Srikant. Why deep neural networks for function approximation?

International Conference on Learning Representations  2017.

In

[17] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning.

MIT press  2012.

[18] Behnam Neyshabur  Srinadh Bhojanapalli  and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on
Learning Representations  2018.

[19] Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. Norm-based capacity control in neural

networks. In Conference on Learning Theory  pages 1376–1401  2015.

9

[20] Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica 

8:143–195  1999.

[21] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with
neural networks. In Proceedings of the 34th International Conference on Machine Learning 
volume 70 of Proceedings of Machine Learning Research  pages 2979–2987  International
Convention Centre  Sydney  Australia  2017. PMLR.

[22] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems  pages 901–909  2016.

[23] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[24] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning algorithms.

Machine Learning  69(2-3):115–142  2007.

[25] Shizhao Sun  Wei Chen  Liwei Wang  Xiaoguang Liu  and Tie-Yan Liu. On the depth of deep

neural networks: A theoretical view. In AAAI  pages 2066–2072  2016.

[26] Matus Telgarsky. Beneﬁts of depth in neural networks. In 29th Annual Conference on Learning
Theory  volume 49 of Proceedings of Machine Learning Research  pages 1517–1539  Columbia
University  New York  New York  USA  2016. PMLR.

[27] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks 

94:103–114  2017.

[28] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

10

,Maren Mahsereci
Philipp Hennig
Zequn Jie
Xiaodan Liang
Jiashi Feng
Xiaojie Jin
Wen Lu
Shuicheng Yan
El Mahdi El Mhamdi
Hadrien Hendrikx
Alexandre Maurer
Yixi Xu
Xiao Wang
Dinghuai Zhang
Tianyuan Zhang
Yiping Lu
Zhanxing Zhu
Bin Dong