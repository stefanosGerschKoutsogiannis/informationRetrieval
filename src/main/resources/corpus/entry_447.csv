2018,Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution,We study adversarial perturbations when the instances are uniformly distributed over {0 1}^n. We study both "inherent" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes.

As the current literature contains multiple  definitions of adversarial risk and robustness  we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial risk and robustness under different definitions by attacking the hypotheses using instances  drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus  this study advocates for the use of the error-region definition  even though other definitions  in other contexts with context-dependent assumptions  may coincide with the error-region definition.

Using the error-region definition of adversarial perturbations  we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0 1}^n. Using the isoperimetric inequality for the Boolean hypercube  we show that for initial error 0.01  there always exists an adversarial perturbation that changes O(√n) bits of the instances to increase the risk to 0.5  making classifier's decisions meaningless. Furthermore  by also using the central limit theorem we show that when n→∞  at most c√n bits of perturbations  for a universal constant c<1.17  suffice for increasing the risk to 0.5  and the same c√n bits of perturbations on average suffice to increase the risk to 1  hence bounding the robustness by c√n.,Adversarial Risk and Robustness: General Deﬁnitions

and Implications for the Uniform Distribution

Dimitrios I. Diochnos∗
University of Virginia

diochnos@virginia.edu

Saeed Mahloujifar∗
University of Virginia
saeed@virginia.edu

Mohammad Mahmoody†

University of Virginia

mohammad@virginia.edu

Abstract

We study adversarial perturbations when the instances are uniformly distributed

over {0  1}n. We study both “inherent” bounds that apply to any problem and any

classiﬁer for such a problem as well as bounds that apply to speciﬁc problems and
speciﬁc hypothesis classes.
As the current literature contains multiple deﬁnitions of adversarial risk and ro-
bustness  we start by giving a taxonomy for these deﬁnitions based on their direct
goals; we identify one of them as the one guaranteeing misclassiﬁcation by push-
ing the instances to the error region. We then study some classic algorithms for
learning monotone conjunctions and compare their adversarial robustness under
different deﬁnitions by attacking the hypotheses using instances drawn from the
uniform distribution. We observe that sometimes these deﬁnitions lead to signiﬁ-
cantly different bounds. Thus  this study advocates for the use of the error-region
deﬁnition  even though other deﬁnitions  in other contexts with context-dependent
assumptions  may coincide with the error-region deﬁnition.
Using the error-region deﬁnition of adversarial perturbations  we then study inher-
ent bounds on risk and robustness of any classiﬁer for any classiﬁcation problem

whose instances are uniformly distributed over {0  1}n. Using the isoperimetric
inequality for the Boolean hypercube  we show that for initial error 0.01  there
always exists an adversarial perturbation that changes O(√n) bits of the instances
to increase the risk to 0.5  making classiﬁer’s decisions meaningless. Furthermore 
by also using the central limit theorem we show that when n → ∞  at most c·√n
bits of perturbations  for a universal constant c < 1.17  sufﬁce for increasing the
risk to 0.5  and the same c√n bits of perturbations on average sufﬁce to increase
the risk to 1  hence bounding the robustness by c · √n.

1

Introduction

In recent years  modern machine learning tools (e.g.  neural networks) have pushed to new heights
the classiﬁcation results on traditional datasets that are used as testbeds for various machine learning
methods.1 As a result  the properties of these methods have been put into further scrutiny.
In
particular  studying the robustness of the trained models in various adversarial contexts has gained
special attention  leading to the active area of adversarial machine learning.

Within adversarial machine learning  one particular direction of research that has gained attention
in recent years deals with the study of the so-called adversarial perturbations of the test instances.
This line of work was particularly popularized  in part  by the work of Szegedy et al. [32] within

∗Authors have contributed equally.
†Supported by NSF CAREER CCF-1350939 and University of Virginia SEAS Research Innovation Award.
1For example  http://rodrigob.github.io/are_we_there_yet/build/ has a summary of state-of-the-art results.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

the context of deep learning classiﬁers  but the same problem can be asked for general classiﬁers as
well. Brieﬂy  when one is given a particular instance x for classiﬁcation  an adversarial perturbation
x′ for that instance is a new instance with minimal changes in the features of x so that the resulting
perturbed instance x′ is misclassiﬁed by the classiﬁer h. The perturbed instance x′ is commonly
referred to as an adversarial example (for the classiﬁer h). Adversarial machine learning has its
roots at least as back as in [19  24  17]. However  the work of [32] revealed pairs of images that
differed slightly so that a human eye could not identify any real differences between the two  and
yet  contrary to what one would naturally expect  machine learning classiﬁers would predict different
labels for the classiﬁcations of such pairs of instances. It is perhaps this striking resemblance to the
human eye of the pairs of images that were provided in [32] that really gave this new push for
intense investigations within the context of adversarial perturbations. Thus  a very intense line of
work started  aiming to understand and explain the properties of machine learning classiﬁers on such
adversarial perturbations; e.g.  [15  23  2  8  20]. These attacks are also referred to as evasion attacks
[25  4  15  8  36]. There is also work that aims at making the classiﬁers more robust under such
attacks [27  36]  yet newer attacks of Carlini and Wagner [7] broke many proposed defenses.

Our general goal. In this work  we study barriers against robust classiﬁcation of adversarial exam-
ples. We are particularly interested in foundational bounds that potentially apply to broad class of
problems and distributions. One can study this question from the perspectives of both risk and ro-
bustness. In the case of risk  the adversary’s goal is to increase the error probability of the classiﬁer
(e.g.  to reach risk 0.5) by small perturbations of the instances  and in the case of robustness  we are
interested in the average amount of perturbations needed for making the classiﬁer always fail.

Studying the uniform distribution. We particularly study adversarial risk and robustness for learn-
ing problems where the input distribution is Un which is uniform over the hypercube {0  1}n. We
tance between the original and perturbed instances x  x′ ∈ {0  1}n is the number of locations that

measure the cost of perturbations using the natural metric of Hamming distance. Namely  the dis-

they are different. This class of distributions already include many learning problems of interest.
So  by studying adversarial risk and robustness for such a natural distribution  we can immediately
obtain results for a broad class of problems. We believe it is crucial to understand adversarial risk
and robustness for natural distributions (e.g.  Un uniform over the hypercube) and metrics (e.g.  the
Hamming distance) to develop a theory of adversarial risk and robustness that can ultimately shed
light on the power and limitations of robust classiﬁcation for practical data sets. Furthermore  natu-
ral distributions like Un model a broad class of learning problems directly; e.g.  see [5  28  18  30].
The hope is that understanding the limitations of robust learning for these basic natural distributions
will ultimately shed light on challenges related to addressing broader problems of interest.

Related work. The work of Gilmer et al. [14] studied the above problem for the special case
of input distributions that are uniform over unit spheres in dimension n. They showed that for
any classiﬁcation problem with such input distribution  so long as there is an initial constant error
probability µ  the robustness under the ℓ2 norm is at most O(√n). Fawzi et al. [11] studied the above
question for Gaussian distributions in dimension n and showed that when the input distribution
has ℓ2 norm ≈ 1  then by ≈ √n perturbations in ℓ2 norm  we can make the classiﬁer change

its prediction (but doing this does not guarantee that the perturbed instance x′ will be misclassiﬁed).
Schmidt et al. [29] proved limits on robustness of classifying uniform instances by speciﬁc classiﬁers
and using a deﬁnition based on “corrupted inputs” (see Section 2)  while we are mainly interested
in bounds that apply to any classiﬁers and guarantee misclassiﬁcation of the adversarial inputs.

Discussion. Our results  like all other current provable bounds in the literature for adversarial risk
and robustness only apply to speciﬁc distributions that do not cover the case of image distributions.
These results  however  are ﬁrst steps  and indicate similar phenomena (e.g.  relation to isoperimetric
inequalities). Thus  as pursued in [14]  these works motivate a deeper study of such inequalities for
real data sets. Finally  as discussed in [11]  such theoretical attacks could potentially imply direct
attacks on real data  assuming the existence of smooth generative models for latent vectors with
theoretically nice distributions (such as Gaussian or uniform over the hypercube) into natural data.

1.1 Our Contribution and Results

As mentioned above  our main goal is to understand inherent barriers against robust classiﬁcation
of adversarial examples  and our focus is on the uniform distribution Un of instances. In order to
achieve that goal  we both do a deﬁnitions study and prove technical limitation results.

2

General deﬁnitions and a taxonomy. As the current literature contains multiple deﬁnitions of
adversarial risk and robustness  we start by giving a taxonomy for these deﬁnitions based on their
direct goals. More speciﬁcally  suppose x is an original instance that the adversary perturbs into a
“close” instance x′. Suppose h(x)  h(x′) are the predictions of the hypothesis h(·) and c(x)  c(x′)
are the true labels of x  x′ deﬁned by the concept function c(·). To call x′ a successful “adversarial
example”  a natural deﬁnition would compare the predicted label h(x′) with some other “anticipated
answer”. However  what h(x′) is exactly compared to is where various deﬁnitions of adversarial
examples diverge. We observe in Section 2 that the three possible deﬁnitions (based on comparing
h(x′) with either of h(x)  c(x) or c(x′)) lead to three different ways of deﬁning adversarial risk and
robustness. We then identify one of them (that compares h(x) with c(x′)) as the one guaranteeing
misclassiﬁcation by pushing the instances to the error region. We also discuss natural conditions
under which these deﬁnitions coincide. However  these conditions do not hold in general.

A comparative study through monotone conjunctions. We next ask: how close/far are these
deﬁnitions in settings where  e.g.  the instances are drawn from the uniform distribution? To answer
this question  we make a comparative study of adversarial risk and robustness for a particular case

of learning monotone conjunctions under the uniform distribution Un (over {0  1}n). A monotone
conjunction f is a function of the form f = (xi1 ∧ ··· ∧ xik ). This class of functions is perhaps one
of the most natural and basic learning problems that are studied in computational learning theory as
it encapsulates  in the most basic form  the class of functions that determine which features should
be included as relevant for a prediction mechanism. For example  Valiant in [35] used this class of
functions under Un to exemplify the framework of evolvability. We attack monotone conjunctions
under Un in order to contrast different behavior of deﬁnitions of adversarial risk and robustness.

In Section 3  we show that previous deﬁnitions of robustness that are not based on the error region 
lead to bounds that do not equate the bounds provided by the error-region approach. We do so by
ﬁrst deriving theorems that characterize the adversarial risk and robustness of a given hypothesis
and a concept function under the uniform distribution. Subsequently  by performing experiments we
show that  on average  hypotheses computed by two popular algorithms (FIND-S [22] and SWAP-
PING ALGORITHM [35]) also exhibit the behavior that is predicted by the theorems. Estimating
the (expected value of) the adversarial risk and robustness of hypotheses produced by other classic
algorithms under speciﬁc distributions  or for other concept classes  is an interesting future work.

Inherent bounds for any classiﬁcation task under the uniform distribution. Finally  after es-
tablishing further motivation to use the error-region deﬁnition as the default deﬁnition for studying
adversarial examples in general settings  we turn into studying inherent obstacles against robust
classiﬁcation when the instances are drawn from the uniform distribution. We prove that for any
learning problem P with input distribution Un (i.e.  uniform over the hypercube) and for any clas-
siﬁer h for P with a constant error µ  the robustness of h to adversarial perturbations (in Hamming
distance) is at most O(√n). We also show that by the same amount of O(√n) perturbations in the
worst case  one can increase the risk to 0.99. Table 1 lists some numerical examples.

Table 1: Each row focuses on the number of tampered bits to achieve its stated goal. The second
column shows results using direct calculations for speciﬁc dimensions. The third column shows that
these results are indeed achieved in the limit  and the last column shows bounds proved for all n.

Adversarial goals

From initial risk 0.01 to 0.99
From initial risk 0.01 to 0.50
Robustness for initial risk 0.01

n = 103  104  105

≈ 2.34√n
≈ 1.17√n
≈ 1.17√n

Types of bounds
n 7→ ∞
< 2.34√n < 3.04√n
< 1.17√n < 1.52√n
< 1.17√n < 1.53√n

all n

To prove results above  we apply the isoperimetric inequality of [26  16] to the error region of the
classiﬁer h and the ground truth c. In particular  it was shown in [16  26] that the subsets of the
hypercube with minimum “expansion” (under Hamming distance) are Hamming balls. This fact
enables us to prove our bounds on the risk. We then prove the bounds on robustness by proving a
general connection between risk and robustness that might obe of independent interest. Using the
central limit theorem  we sharpen our bounds for robustness and obtain bounds that closely match
the bounds that we also obtain by direct calculations (based on the isoperimetric inequalities and
picking Hamming balls as error region) for speciﬁc values of dimension n = 103  104  105.

3

Full version. All proofs could be found in the full version of the paper2  which also includes results
related to the adversarial risk of monotone conjunctions  complementing the picture of Section 3.

2 General Deﬁnitions of Risk and Robustness for Adversarial Perturbations

Notation. We use calligraphic letters (e.g.  X ) for sets and capital non-calligraphic letters (e.g. 
D) for distributions. By x ← D we denote sampling x from D. In a classiﬁcation problem P =
(X  Y D C H)  the set X is the set of possible instances  Y is the set of possible labels  D is
a set of distributions over X   C is a class of concept functions  and H is a class of hypotheses 
where any f ∈ C ∪ H is a mapping from X to Y. An example is a labeled instance. We did not
state the loss function explicitly  as we work with classiﬁcation problems  however all main three
deﬁnitions of this section directly extend to arbitrary loss functions. For x ∈ X   c ∈ C  D ∈ D  the
risk or error of a hypothesis h ∈ H is the expected (0-1) loss of (h  c) with respect to D  namely
Risk(h  c  D) = Prx←D[h(x) 6= c(x)]. We are usually interested in learning problems with a ﬁxed
distribution D = {D}  as we are particularly interested in robustness of learning under the uniform
distribution Un over {0  1}n. Note that since we deal with negative results  ﬁxing the distribution
only makes our results stronger. As a result  whenever D = {D}  we omit D from the risk notation
and simply write Risk(h  c). We usually work with problems P = (X  Y  D C H  d) that include
a metric d over the instances. For a set S ⊆ X we let d(x S) = inf{d(x  y) | y ∈ S}  and
by Ballr(x) = {x′ | d(x  x′) ≤ r} we denote the ball of radius r centered at x under the metric
d. By HD we denote Hamming distance for pairs of instances from {0  1}n. Finally  we use the
term adversarial instance to refer to an adversarially perturbed instance x′ of an originally sampled
instance x when the label of the adversarial example is either not known or not considered.

Below we present our formal deﬁnitions of adversarial risk and robustness. In all of these deﬁnitions
we will deal with attackers who perturb the initial test instance x into a close adversarial instance
x′. We will measure how much an adversary can increase the risk by perturbing a given input x into
a close adversarial example x′. When to exactly call x′ a successful adversarial example is where
these deﬁnitions differ. First we formalize the main deﬁnition that we use in this work based on
adversary’s ability to push instances to the error region.
Deﬁnition 2.1 (Error-region risk and robustness). Let P = (X  Y  D C H  d) be a classiﬁcation
problem (with metric d deﬁned over instances X ).

• Risk. For any r ∈ R+  h ∈ H  c ∈ C  the error-region risk under r-perturbation is

r

RiskER

(h  c) = Pr
x←D

[∃x′ ∈ Ballr(x)  h(x′) 6= c(x′)] .
(h  c) = Risk(h  c) becomes the standard notion of risk.

For r = 0  RiskER
• Robustness. For any h ∈ H  x ∈ X   c ∈ C  the error-region robustness is the expected
distance of a sampled instance to the error region  formally deﬁned as follows

r

RobER(h  c) = E
x←D

[inf{r : ∃x′ ∈ Ballr(x)  h(x′) 6= c(x′)}] .

Deﬁnition 2.1 requires the adversarial instance x′ to be
misclassiﬁed  namely  h(x′) 6= c(x′). So  x′ clearly be-
longs to the error region of the hypothesis h compared to
the ground truth c. This deﬁnition is implicit in the work
of [14]. In what follows  we compare our main deﬁnition
above with previously proposed deﬁnitions of adversar-
ial risk and robustness found in the literature and discuss
when they are (and when they are not) equivalent to Def-
inition 2.1. Figure 1 summarizes the differences between
the three main deﬁnitions that have appeared in the liter-
ature  where we distinguish cases by comparing the clas-
siﬁer’s prediction h(x′) at the new point x′ with either of
h(x)  c(x)  or c(x′)  leading to three different deﬁnitions.

2See https://arxiv.org/abs/1810.12272.

h(x)

c(x)

e
g
n
a
h
C
n
o
i
t
c
i
d
e
r
P

C orruptedInstance

h(x′)

Error Region

c(x′)

Figure 1: The three main deﬁnitions
based on what h(x′) is compared with.

4

Deﬁnitions based on hypothesis’s prediction change (PC risk and robustness). Many works 
including the works of [32  11] use a deﬁnition of robustness that compares classiﬁer’s prediction

h(x′) with the prediction h(x) on the original instance x. Namely  they require h(x′) 6= h(x)
rather than h(x′) 6= c(x′) in order to consider x′ an adversarial instance. Here we refer to this
deﬁnition (that does not depend on the ground truth c) as prediction-change (PC) risk and robustness
(h) and RobPC(h)). We note that this deﬁnition captures the error-region risk
(denoted as RiskPC
and robustness if we assume the initial correctness (i.e.  h(x) = c(x)) of classiﬁer’s prediction on
all x ← X and “truth proximity”  i.e.  that c(x) = c(x′) holds for all x′ that are “close” to x. Both
of these assumptions are valid in some natural scenarios. For example  when input instances consist
of images that look similar to humans (if used as the ground truth c(·)) and if h is also correct on the
original (non-adversarial) test examples  then the two deﬁnitions (based on error region or prediction
change) coincide. But  these assumptions do not hold in in general.

r

Deﬁnitions based on the notion of corrupted instance (CI risk and robustness). The works
of [21  12  13  1] study the robustness of learning models in the presence of corrupted inputs. A
more recent framework was developed in [20  29] for modeling risk and robustness that is inspired
by robust optimization [3] (with an underlying metric space) and model adversaries that corrupt the
the original instance in (exponentially more) ways. When studying adversarial perturbations using
corrupted instances  we deﬁne adversarial risk by requiring the adversarial instance x′ to satisfy
h(x′) 6= c(x). The term “corrupted instance” is particularly helpful as it emphasizes on the fact that
the goal (of the classiﬁer) is to ﬁnd the true label of the original (uncorrupted) instance x  while we
are only given a corrupted version x′. Hence  we refer to this deﬁnition as the corrupted instance
(CI) risk and robustness and denote them by RiskCI
r (h  c) and RobCI(h  c). The advantage of this
deﬁnition compared to the prediction-change based deﬁnitions is that here  we no longer need to
assume the initial correctness assumption. Namely  only if the “truth proximity” assumption holds 

then we have c(x) = c(x′) which together with the condition h(x′) 6= c(x) we can conclude that x′
is indeed misclassiﬁed. However  if small perturbations can change the ground truth  c(x′) can be
different from c(x)  in which case  it is no long clear whether x′ is misclassiﬁed or not.

Stronger deﬁnitions of risk and robustness with more restrictions on adversarial instance.

The corrupted-input deﬁnition requires an adversarial instance x′ to satisfy h(x′) 6= c(x)  and the
error-region deﬁnition requires h(x′) 6= c(x′). What if we require both of these conditions to call
x′ a true adversarial instance? This is indeed the deﬁnition used in the work of Suggala et al. [31] 
though more formally in their work  they subtract the original risk (without adversarial perturbation)
from the adversarial risk. This deﬁnition is certainly a stronger guarantee for the adversarial instance.
As this deﬁnition is a hybrid of the error-region and corrupted-instance deﬁnitions  we do not make
a direct study of this deﬁnition and only focus on the other three deﬁnitions described above.

How about when the classiﬁer h is 100% correct? We emphasize that when h happens to be the
same function as c  (the error region) Deﬁnition 2.1 implies h has zero adversarial risk and inﬁnite
adversarial robustness RobER(h  c) = ∞. This is expected  as there is no way an adversary can
perturb any input x into a misclassiﬁed x′. However  both of the deﬁnitions of risk and robustness
based on prediction change [32] and corrupted instance [21  20] could compute large risk and small
robustness for such h. In fact  in a recent work [33] it is shown that for deﬁnitions based on corrupted
input  correctness might be provably at odds with robustness in some cases. Therefore  even though
all these deﬁnitions could perhaps be used to approximate the risk and robustness when we do not
have access to the ground truth c′ on the new point x′  in this work we separate the deﬁnition of risk
and robustness from how to compute/approximate them  so we will use Deﬁnition 2.1 by default.

3 A Comparative Study through Monotone Conjunctions

In this section  we compare the risk and robustness under the three deﬁnitions of Section 2 through
a study of monotone conjunctions under the uniform distribution. Namely  we consider adversarial

perturbations of truth assignments that are drawn from the uniform distribution Un over {0  1}n

when the concept class contains monotone conjunctions. As we will see  these deﬁnitions diverge in
this natural case. Below we ﬁx the setup under which all the subsequent results are obtained.
Problem Setup 1. Let Cn be the concept class of all monotone conjunctions formed by at least one
and at most n Boolean variables. The target concept (ground truth) c that needs to be learned is

5

c =

xi ∧

^i=1

^k=1

xi ∧

^i=1

^ℓ=1

drawn from Cn. Let the hypothesis class be H = Cn and let h ∈ H be the hypothesis obtained by a
learning algorithm after processing the training data. With |h| and |c| we denote the size of h and c
respectively; that is  number of variables that h and c contain.3 Now let 

m

u

m

w

yk

and

h =

zℓ .

(1)

We will call the variables that appear both in h and c as mutual  the variables that appear in c but
not in h as undiscovered  and the variables that appear in h but not in c as wrong (or redundant).
Therefore in (1) we have m mutual variables  u undiscovered and w wrong. We denote the error
region of a hypothesis h and the target concept c with E (h  c).
That is  E (h  c) = {x ∈ {0  1}n | h(x) 6= c(x)}. The probability mass of the error region between
h and c  denoted by µ  under the uniform distribution Un over {0  1}n is then 
[x ∈ E (h  c)] = µ = (2w + 2u − 2) · 2−m−u−w .

x←Un

Pr

(2)

In this problem setup we are interested in computing the adversarial risk and robustness that attack-

ers can achieve when instances are drawn from the uniform distribution Un over {0  1}n.
Remark 3.1. Note that µ is a variable that depends on the particular h and c.

Using the Problem Setup 1  in what follows we compute the adversarial robustness that an arbitrary
hypothesis has against an arbitrary target using the error region (ER) deﬁnition that we advocate
in contexts where the perturbed input is supposed to be misclassiﬁed and do the same calculations
for adversarial risk and robustness that are based on the deﬁnitions of prediction change (PC) and
corrupted instance (CI). The important message is that the adversarial robustness of a hypothesis
based on the ER deﬁnition is Θ (min{|h|  |c|})  whereas the adversarial robustness based on PC and
CI is Θ (|h|). In the full version of the paper we also give theorems (that have similar ﬂavor) for
calculating the adversarial risk based on the three main deﬁnitions (ER  PC  CI).
Theorem 3.2. Consider the Problem Setup 1. Then  if h = c we have RobER(h  c) = ∞  while if
h 6= c we have min{|h|  |c|}/16 ≤ RobER(h  c) ≤ 1 + min{|h|  |c|}.
Theorem 3.3. Consider the Problem Setup 1. Then  RobPC(h) = |h| /2 + 2−|h|.
Theorem 3.4. Consider the Problem Setup 1. Then  |h| /4 < RobCI(h  c) < |h| + 1/2.

3.1 Experiments for the Expected Values of Adversarial Robustness

In this part  we complement the theorems that we presented earlier with experiments. This way we
are able to examine how some popular algorithms behave under attack  and we explore the extent to
which the generated solutions of such algorithms exhibit differences in their (adversarial) robustness
on average against various target functions drawn from the class of monotone conjunctions.

The ﬁrst algorithm is the standard Occam algorithm that starts from the full conjunction and elimi-
nates variables from the hypothesis that contradict the positive examples received; this algorithm is
known as FIND-S in [22] but has appeared without a name earlier by Valiant in [34] and its roots are
at least as old as in [6]. The second algorithm is the SWAPPING ALGORITHM from the framework
of evolvability [35]. This algorithm searches for an ε-optimal solution among monotone conjunc-
tions that have at most ⌈lg(3/(2ε))⌉ variables in their representation using a local search method
where hypotheses in the neighborhood are obtained by swapping in and out some variable(s) from
the current hypothesis; we follow the analysis that was used in [10] and is a special case of [9].

In each experiment  we ﬁrst learn hypotheses by using the algorithms under Un against different
target sizes. For both algorithms  during the learning process  we use ε = 0.01 and δ = 0.05 for
the learning parameters. We then examine the robustness of the generated hypotheses by drawing
examples again from the uniform distribution Un as this is the main theme of this paper. In particular 
we test against the 30 target sizes from the set {1  2  . . .   24  25  30  50  75  99  100}. For each such
target size  we plot the average value  over 500 runs  of the robustness of the learned hypothesis that

3 For example  h1 = x1 ∧ x5 ∧ x8 is a monotone conjunction of three variables in a space where we have

n ≥ 8 variables and |h1| = 3.

6

we obtain. In each run  we repeat the learning process using a random target of the particular size as
well as a fresh training sample and subsequently estimate the robustness of the learned hypothesis
by drawing another 10  000 examples from Un that we violate (depending on the deﬁnition). The
dimension of the instances is n = 100.

Figure 2 presents the values of the three robustness measures for the case of FIND-S. In the full
version of the paper we provide more details on the algorithms and more information regarding our
experiments. The message is that the adversarial robustness that is based on the deﬁnitions of pre-
diction change and corrupted instance is more or less the same  whereas the adversarial robustness
based on the error region deﬁnition may obtain wildly different values compared to the other two.

s
s
e
n

t
s
u
b
o
r

 50

 45

 40

 35

 30

 25

 20

 15

 10

 5

 0

1 5 10 15 20 25 30

prediction change
corrupted instance
error region
 65 70 75 80 85

 35 40 45 50 55 60

 90 95 100

target size |c|

Figure 2: Experimental comparison of the different robustness measures. The values for PC and CI
almost coincide and they can hardly be distinguished. The value for ER robustness is completely
different compared to the other two. Note that ER robustness is ∞ when the target size |c| is in
{1  . . .   8}∪{100} and for this reason only the points between 9 and 99 are plotted. When |c| ≥ 20 
almost always the learned hypothesis is the initialized full conjunction. The reason is that positive
examples are very rare and our training set contains none. As a result no variable is eliminated
from the initialized hypothesis h (full conjunction). Hence  when |c| ≥ 20 we see that PC and CI
robustness is about max{|h|  |c|}/2 = |h|/2  whereas ER is roughly min{|h|  |c|}/2 = |c|/2.

4

Inherent Bounds on Risk and Robustness for the Uniform Distribution

In this section  we state our main theorems about error region adversarial risk and robustness of arbi-
trary learning problems whose instances are distributed uniformly over the n-dimension hypercube
{0  1}n. The proofs of the theorems below are available in the full version of the paper.

We ﬁrst deﬁne a useful notation for the size of the (partial) Hamming balls.
Deﬁnition 4.1. For every n ∈ N we deﬁne the (partial) “Hamming Ball Size” function
BSizen : [n] × [0  1) → [0  1) as follows

k(cid:19)! .
i(cid:19) + λ ·(cid:18)n
Note that this function is a bijection and we use BSize−1(·) to denote its inverse. When n is clear
from the context  we will simply use BSize(· ·) and BSize−1(·) instead.

BSizen(k  λ) = 2−n · k−1
Xi=0(cid:18)n

The following theorem  gives a general lower bound for the adversarial risk of any classiﬁcation

problem for uniform distribution Un over the hypercube {0  1}n  depending on the original error.
Theorem 4.2. Suppose P = ({0  1}n Y  Un C H  HD) is a classiﬁcation problem. For any h ∈
H  c ∈ C and r ∈ N  let µ = Risk(h  c) > 0 be the original risk and (k  λ) = BSize−1 (µ) be a
function of the original risk. Then  the error-region adversarial risk under r-perturbation is at least

RiskER

r

(h  c) ≥ BSize(k + r  λ).

7

The following corollary determines an asymptotic lower bound for risk based on Theorem 4.2.

Corollary 4.3 (Error-region risk for all n). Suppose P = ({0  1}n Y  Un C H  HD) is a classiﬁca-
tion problem. For any hypothesis h  c with risk µ ∈ (0  1
2 ] in predicting a concept function c  we can
increase the risk of (h  c) from µ ∈ (0  1

2   1] by changing at most

2 ] to µ′ ∈ [ 1

bits in the input instances. Namely  by using the above r  we have RiskER
increase the error to 1

r =r−n · ln µ

+r−n · ln(1 − µ′)
2 we only need to change at most r′ =q −n·ln(µ)

bits.

2

2

2

Example. Corollary 4.3 implies that for classiﬁcation tasks over Un  by changing at most 3.04√n
number of bits in each example we can increase the error of an hypothesis from 1% to 99%. Further-
more  for increasing the error just to 0.5 we need half of the number of bits  which is 1.52√n.
Also  the corollary bellow  gives a lower bound on the limit of adversarial risk when n 7→ ∞. This
lower bound matches the bound we have in our computational experiments.
Corollary 4.4 (Error-region risk for large n). Let µ ∈ (0  1] and µ′ ∈ (µ  1] and P =
({0  1}n Y  Un C H  HD) be a classiﬁcation problem. Then for any h ∈ H  c ∈ C such that
Risk(h  c) ≥ µ we have Riskr(h  c) ≥ µ′ for

r

(h  c) ≥ µ′. Also  to

r ≈ √n ·

Φ−1(µ′) − Φ−1(µ)

2

when n 7→ ∞

where Φ is the CDF of the standard normal distribution.

Example. Corollary 4.4 implies that for classiﬁcation tasks over Un  when n is large enough  we
can increase the error from 1% to 99% by changing at most 2.34√n bits  and we can we can increase
the error from 1% to 50% by changing at most 1.17√n bits in test instances.
The following theorem shows how to upper bound the adversarial robustness using the original risk.

Theorem 4.5. Suppose P = ({0  1}n Y  Un C H  HD) is a classiﬁcation problem. For any h ∈ H
and c ∈ C  if µ = Risk(h  c) and (k  λ) = BSize−1(µ) depends on the original risk  then the

error-region robustness is at most

RobER(h  c) ≤

n−k+1

Xr=0

(1 − BSize(k + r  λ)) .

Following  using Theorem 4.5  we give an asymptotic lower bound for robustness .

Corollary 4.6. Suppose P = ({0  1}n Y  Un C H  HD) is a classiﬁcation problem. For any
hypothesis h with risk µ ∈ (0  1
2 ]  we can make h to give always wrong answers by changing
r =p−n · ln µ/2 + µ ·pn/2 number of bits on average. Namely  we have

RobER(h  c) ≤r−n · ln µ

2

+ µ ·r n

2

.

And the following Corollary gives a lower bound on the robustness in limit.

Corollary 4.7. For any µ ∈ (0  1]  classiﬁcation problem P = ({0  1}n Y  Un C H  HD)  and any
h ∈ H  c ∈ C such that Risk(h  c) ≥ µ  we have

RobER(h  c) ≤

2

Φ−1(µ)

· √n + µ ·r π · n

8

when n 7→ ∞ 

where Φ is the CDF of the standard normall distribution.
Example. By changing 1.53√n number of bits on average we can increase the error of an hypoth-
esis from 1% to 100%. Also  if n 7→ ∞  by changing only 1.17√n number of bits on average we

can increase the error from 1% to 100%.

8

References

[1] Idan Attias  Aryeh Kontorovich  and Yishay Mansour.
robust learning. arXiv preprint arXiv:1810.02180  2018.

Improved generalization bounds for

[2] Osbert Bastani  Yani Ioannou  Leonidas Lampropoulos  Dimitrios Vytiniotis  Aditya V. Nori 
and Antonio Criminisi. Measuring Neural Net Robustness with Constraints. In NIPS  pages
2613–2621  2016.

[3] Aharon Ben-Tal  Laurent El Ghaoui  and Arkadi S. Nemirovski. Robust Optimization. Prince-

ton Series in Applied Mathematics. Princeton University Press  October 2009.

[4] Battista Biggio  Giorgio Fumera  and Fabio Roli. Security evaluation of pattern classiﬁers

under attack. IEEE transactions on knowledge and data engineering  26(4):984–996  2014.

[5] Avrim Blum  Merrick L. Furst  Jeffrey C. Jackson  Michael J. Kearns  Yishay Mansour  and
Steven Rudich. Weakly learning DNF and characterizing statistical query learning using
Fourier analysis. In STOC  pages 253–262  1994.

[6] Jerome S. Bruner  Jacqueline J. Goodnow  and George A. Austin. A study of thinking. John

Wiley & Sons  New York  NY  USA  1957.

[7] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence
and Security  pages 3–14. ACM  2017.

[8] Nicholas Carlini and David A. Wagner. Towards Evaluating the Robustness of Neural Net-
works. In 2017 IEEE Symposium on Security and Privacy  SP 2017  San Jose  CA  USA  May
22-26  2017  pages 39–57  2017.

[9] Dimitrios I. Diochnos. On the Evolution of Monotone Conjunctions: Drilling for Best Approx-

imations. In ALT  pages 98–112  2016.

[10] Dimitrios I. Diochnos and György Turán. On Evolvability: The Swapping Algorithm  Product

Distributions  and Covariance. In SAGA  pages 74–88  2009.

[11] Alhussein Fawzi  Hamza Fawzi  and Omar Fawzi. Adversarial vulnerability for any classiﬁer.

arXiv preprint arXiv:1802.08686  2018.

[12] Uriel Feige  Yishay Mansour  and Robert Schapire. Learning and inference in the presence of

corrupted inputs. In Conference on Learning Theory  pages 637–657  2015.

[13] Uriel Feige  Yishay Mansour  and Robert E Schapire. Robust inference for multiclass classiﬁ-

cation. In Algorithmic Learning Theory  pages 368–386  2018.

[14] Justin Gilmer  Luke Metz  Fartash Faghri  Samuel S Schoenholz  Maithra Raghu  Martin Wat-

tenberg  and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774  2018.

[15] Ian Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and Harnessing Adver-

sarial Examples. In ICLR  2015.

[16] Lawrence H Harper. Optimal numberings and isoperimetric problems on graphs. Journal of

Combinatorial Theory  1(3):385–393  1966.

[17] Ling Huang  Anthony D. Joseph  Blaine Nelson  Benjamin I. P. Rubinstein  and J. D. Tygar.
In Proceedings of the 4th ACM Workshop on Security and

Adversarial Machine Learning.
Artiﬁcial Intelligence  AISec 2011  Chicago  IL  USA  October 21  2011  pages 43–58  2011.

[18] Jeffrey C. Jackson and Rocco A. Servedio. On Learning Random DNF Formulas Under the

Uniform Distribution. Theory of Computing  2(8):147–172  2006.

[19] Daniel Lowd and Christopher Meek. Adversarial learning. In KDD  pages 641–647  2005.

9

[20] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian
arXiv preprint
to appear in International Conference on Learning Representations

Vladu. Towards deep learning models resistant to adversarial attacks.
arXiv:1706.06083;
(ICLR)  2018.

[21] Yishay Mansour  Aviad Rubinstein  and Moshe Tennenholtz. Robust probabilistic inference. In
Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms  pages
449–460. Society for Industrial and Applied Mathematics  2015.

[22] Thomas M. Mitchell. Machine Learning. McGraw-Hill  Inc.  New York  NY  USA  1 edition 

1997.

[23] Seyed-Mohsen Moosavi-Dezfooli  Alhussein Fawzi  and Pascal Frossard. DeepFool: A Simple

and Accurate Method to Fool Deep Neural Networks. In CVPR  pages 2574–2582  2016.

[24] Blaine Nelson  Benjamin I. P. Rubinstein  Ling Huang  Anthony D. Joseph  and J. D. Tygar.

Classiﬁer Evasion: Models and Open Problems. In PSDM  pages 92–98  2010.

[25] Blaine Nelson  Benjamin IP Rubinstein  Ling Huang  Anthony D Joseph  Steven J Lee  Satish
Rao  and JD Tygar. Query strategies for evading convex-inducing classiﬁers. Journal of Ma-
chine Learning Research  13(May):1293–1332  2012.

[26] R. G. Nigmatullin. Some metric relations in the unit cube (in russian). Diskretny Analiz 9 

Novosibirsk  pages 47–58  1967.

[27] Nicolas Papernot  Patrick D. McDaniel  Xi Wu  Somesh Jha  and Ananthram Swami. Dis-
tillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. In IEEE
Symposium on Security and Privacy  SP 2016  San Jose  CA  USA  May 22-26  2016  pages
582–597  2016.

[28] Yoshifumi Sakai and Akira Maruoka. Learning Monotone Log-Term DNF Formulas under the

Uniform Distribution. Theory of Computing Systems  33(1):17–33  2000.

[29] Ludwig Schmidt  Shibani Santurkar  Dimitris Tsipras  Kunal Talwar  and Aleksander Madry.
Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285 
2018.

[30] Linda Sellie. Exact learning of random DNF over the uniform distribution. In STOC  pages

45–54  2009.

[31] Arun Sai Suggala  Adarsh Prasad  Vaishnavh Nagarajan  and Pradeep Ravikumar. On Adver-

sarial Risk and Training. arXiv preprint arXiv:1806.02924  2018.

[32] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Good-

fellow  and Rob Fergus. Intriguing properties of neural networks. In ICLR  2014.

[33] Dimitris Tsipras  Shibani Santurkar  Logan Engstrom  Alexander Turner  and Aleksander
Madry. Robustness May Be at Odds with Accuracy. arXiv preprint arXiv:1805.12152  2018.

[34] Leslie G. Valiant. A Theory of the Learnable. Communications of the ACM  27(11):1134–1142 

1984.

[35] Leslie G. Valiant. Evolvability. Journal of the ACM  56(1):3:1–3:21  2009.

[36] Weilin Xu  David Evans  and Yanjun Qi. Feature Squeezing: Detecting Adversarial Exam-
ples in Deep Neural Networks. arXiv preprint arXiv:1704.01155. To appear in Network and
Distributed System Security Symposium (NDSS)  2018.

10

,Dimitrios Diochnos
Saeed Mahloujifar
Mohammad Mahmoody