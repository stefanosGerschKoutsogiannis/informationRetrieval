2017,Fast Black-box Variational Inference through Stochastic Trust-Region Optimization,We introduce TrustVI  a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration  TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information  but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI  demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI  demonstrating that our convergence theory can matter in practice.,Fast Black-box Variational Inference

through Stochastic Trust-Region Optimization

Jeffrey Regier

jregier@cs.berkeley.edu

Michael I. Jordan

jordan@cs.berkeley.edu

Jon McAuliffe

jon@stat.berkeley.edu

Abstract

We introduce TrustVI  a fast second-order algorithm for black-box variational
inference based on trust-region optimization and the “reparameterization trick.” At
each iteration  TrustVI proposes and assesses a step based on minibatches of draws
from the variational distribution. The algorithm provably converges to a stationary
point. We implemented TrustVI in the Stan framework and compared it to two
alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-
free Stochastic Gradient Variational Inference (HFSGVI). The former is based
on stochastic ﬁrst-order optimization. The latter uses second-order information 
but lacks convergence guarantees. TrustVI typically converged at least one order
of magnitude faster than ADVI  demonstrating the value of stochastic second-order
information. TrustVI often found substantially better variational distributions than
HFSGVI  demonstrating that our convergence theory can matter in practice.

Introduction

1
The “reparameterization trick” [1  2  3] has led to a resurgence of interest in variational inference (VI) 
making it applicable to essentially any differentiable model. This new approach  however  requires
stochastic optimization rather than fast deterministic optimization algorithms like closed-form
coordinate ascent. Some fast stochastic optimization algorithms exist  but variational objectives have
properties that make them unsuitable: they are typically nonconvex  and the relevant expectations
cannot usually be replaced by ﬁnite sums. Thus  to date  practitioners have used SGD and its variants
almost exclusively. Automatic Differentiation Variational Inference (ADVI) [4] has been especially
successful at making variational inference based on ﬁrst-order stochastic optimization accessible.
Stochastic ﬁrst-order optimization  however  is slow in theory (sublinear convergence) and in practice
(thousands of iterations)  negating a key beneﬁt of VI.
This article presents TrustVI  a fast algorithm for variational inference based on second-order
trust-region optimization and the reparameterization trick. TrustVI routinely converges in tens
of iterations for models that take thousands of ADVI iterations. TrustVI’s iterations can be more
expensive  but on a large collection of Bayesian models  TrustVI typically reduced total computation
by an order of magnitude. Usually TrustVI and ADVI ﬁnd the same objective value  but when they
differ  TrustVI is typically better.
TrustVI adapts to the stochasticity of the optimization problem  raising the sampling rate for assessing
proposed steps based on a Hoeffding bound. It provably converges to a stationary point. TrustVI
generalizes the Newton trust-region method [5]  which converges quadratically and has performed
well at optimizing analytic variational objectives even at an extreme scale [6]. With large enough
minibatches  TrustVI iterations are nearly as productive as those of a deterministic trust region
method. Fortunately  large minibatches make effective use of single-instruction multiple-data (SIMD)
parallelism on modern CPUs and GPUs.
TrustVI uses either explicitly formed approximations of Hessians or approximate Hessian-vector
products. Explicitly formed Hessians can be fast for low-dimensional problems or problems with
sparse Hessians  particularly when expensive computations (e.g.  exponentiation) already need to be

performed to evaluate a gradient. But Hessian-vector products are often more convenient. They can
be computed efﬁciently through forward-mode automatic differentiation  reusing the implementation
for computing gradients [7  8]. This is the approach we take in our experiments.
Fan et al. [9] also note the limitations of ﬁrst-order stochastic optimization for variational inference:
the learning rate is difﬁcult to set  and convergence is especially slow for models with substantial
curvature. Their approach is to apply Newton’s method or L-BFGS to problems that are both
stochastic and nonconvex. All stationary points—minima  maxima  and saddle points—act as
attractors for Newton steps  however  so while Newton’s method may converge quickly  it may
also converge poorly. Trust region methods  on the other hand  are not only unharmed by negative
curvature  they exploit it: descent directions that become even steeper are among the most productive.
In section 5  we empirically compare TrustVI to Hessian-free Stochastic Gradient Variation Inference
(HFSGVI) to assess the practical importance of our convergence theory.
TrustVI builds on work from the derivative-free optimization community [10  11  12]. The STORM
framework [12] is general enough to apply to a derivative-free setting  as well as settings where
higher-order stochastic information is available. STORM  however  requires that a quadratic model of
the objective function can always be constructed such that  with non-trivial probability  the quadratic
model’s absolute error is uniformly bounded throughout the trust region. That requirement can
be satisﬁed for the kind of low-dimensional problems one can optimize without derivatives  where
the objective may be sampled throughout the trust region at a reasonable density  but not for most
variational objective functions.

2 Background
Variational inference chooses an approximation to the posterior distribution from a class of candidate
distributions through numerical optimization [13]. The candidate approximating distributions q!
are parameterized by a real-valued vector !. The variational objective function L  also known as
the evidence lower bound (ELBO)  is an expectation with respect to latent variables z that follow
an approximating distribution q!:

L(!)   Eq! {log p(x  z)  log q!(z)} .
(1)
Here x  the data  is ﬁxed.
If this expectation has an analytic form  L may be maximized by
deterministic optimization methods  such as coordinate ascent and Newton’s method. Realistic
Bayesian models  however  not selected primarily for computational convenience  seldom yield
variational objective functions with analytic forms.
Stochastic optimization offers an alternative. For many common classes of approximating
distributions  there exists a base distribution p0 and a function g! such that  for e ⇠ p0 and z ⇠ q! 
g!(e) d= z. In words: the random variable z whose distribution depends on !  is a deterministic
function of a random variable e whose distribution does not depend on !. This alternative expression
of the variational distribution is known as the “reparameterization trick” [1  2  3  14]. At each
iteration of an optimization procedure  ! is updated based on an unbiased Monte Carlo approximation
to the objective function:

ˆL(!; e1  . . .   eN )   1

N

NXi=1

{log p(x  g!(ei))  log q!(g!(ei))}

(2)

for e1  . . .   eN sampled from the base distribution.
3 TrustVI
TrustVI performs stochastic optimization of the ELBO L to ﬁnd a distribution q! that approximates
the posterior. For TrustVI to converge  the ELBO only needs to satisfy Condition 1. (Subsequent
conditions apply to the algorithm speciﬁcation  not the optimization problem.)
Condition 1. L : RD ! R is a twice-differentiable function of ! that is bounded above. Its gradient
has Lipschitz constant L.

Condition 1 is compatible with all models whose conditional distributions are in the exponential
family. The ELBO for a model with categorical random variables  for example  is twice differentiable
in its parameters when using a mean-ﬁeld categorical variational distribution.

2

Algorithm 1 TrustVI
Require: Initial iterate !0 2 RD; initial trust region radius 0 2 (0  max]; and settings for the

parameters listed in Table 1.
for k = 0  1  2  . . . do

Draw stochastic gradient gk satisfying Condition 2.
Select symmetric matrix Hk satisfying Condition 3.
Solve for sk   arg max g|
2 s|Hks : ksk  k.
k s + 1
Compute m0k   g|
2 s|
kHksk.
k sk + 1
Select Nk satisfying Inequality 11 and Inequality 13.
Draw `0k1  . . .  ` 0kNk satisfying Condition 4.
Compute `0k   1
if `0k  ⌘m0k  2
!k+1 !k + sk
k+1 min(k  max)
!k+1 !k
k+1 k/

NkPNk

else

i=1 `0ki.

k then

end if
end for

Table 1: User-selected parameters for TrustVI

brief description
model ﬁtness threshold
trust region expansion factor
trust region radius constraint
tradeoff between trust region radius and objective value
tradeoff between both sampling rates
accuracy of “good” stochastic gradients’ norms
accuracy of “good” stochastic gradients’ directions
probability of “good” stochastic gradients
probability of accepting a “good” step
maximum norm of the quadratic models’ Hessians
maximum trust region radius for enforcing some conditions

name
⌘


↵
⌫1
⌫2
⌫3
⇣0
⇣1
H

max maximum trust region radius

allowable range
(0  1/2]
(1 1)
(0 1)
(/(1  2) 1)
(0  1  ⌘)
(0  1)
(0  1  ⌘  ⌫1)
(1/2  1)
(1/(2⇣0)  1)
[0 1)
(0 1]
(0 1)

The domain of L is taken to be all of RD. If instead the domain is a proper subset of a real coordinate
space  the ELBO can often be reparameterized so that its domain is RD [4].
TrustVI iterations follow the form of common deterministic trust region methods: 1) construct a
quadratic model of the objective function restricted to the current trust region; 2) ﬁnd an approximate
optimizer of the model function: the proposed step; 3) assess whether the proposed step leads to
an improvement in the objective; and 4) update the iterate and the trust region radius based on the
assessment. After introducing notation in Section 3.1  we describe proposing a step in Section 3.2
and assessing a proposed step in Section 3.3. TrustVI is summarized in Algorithm 1.

3.1 Notation

TrustVI’s iteration number is denoted by k. During iteration k  until variables are updated at its
end  !k is the iterate  k is the trust region radius  and L(!k) is the objective-function value. As
shorthand  let Lk   L(!k).
During iteration k  a quadratic model mk is formed based on a stochastic gradient gk of L(!k)  as
well as a local Hessian approximation Hk. The maximizer of this model on the trust region  sk  we
call the proposed step. The maximum  denoted m0k   mk(sk)  we refer to as the model improvement.
We use the “prime” symbol to denote changes relating to a proposed step sk that is not necessarily

3

accepted; e.g.  L0k = L(!k + sk)L k. We use the  symbol to denote change across iterations; e.g. 
Lk = Lk+1 L k. If a proposed step is accepted  then  for example  Lk = L0k and k = 0k.
Each iteration k has two sources of randomness: mk and `0k  an unbiased estimate of L0k that
determines whether to accept proposed step sk. `0k is based on an iid random sample of size Nk
(Section 3.3).
For the random sequence m1 ` 01  m2 ` 02  . . .  it is often useful to condition on the earlier variables
when reasoning about the next. Let Mk refer to the -algebra generated by m1  . . .   mk1 and
`01  . . .  ` 0k1. When we condition on Mk   we hold constant all the outcomes that precede iteration
k. Let M+
k refer to the -algebra generated by m1  . . .   mk and `01  . . .  ` 0k1. When we condition
on M+
k   we hold constant all the outcomes that precede drawing the sample that determines whether
to accept the kth proposed step.
Table 1 lists the user-selected parameters that govern the behavior of the algorithm. TrustVI
converges to a stationary point for any selection of parameters in the allowable range (column 3). As
shorthand  we refer to a particular trust region radius  derived from the user-selected parameters  as

k   min  r ⌘m0k



 

⌫2L + ⌫2⌘H + 8H! .

⌫2⌫3krLkk

(3)

(5)

3.2 Proposing a step

At each iteration  TrustVI proposes the step sk that maximizes the local quadratic approximation

1
2

k s +

s|Hks : ksk  k

mk(s) = Lk + g|
to the function L restricted to the trust region.
We set gk to the gradient of ˆL at !k  where ˆL is evaluated using a freshly drawn sample e1  . . .   eN.
From Equation 2 we see that gk is a stochastic gradient constructed from a minibatch of size N. We
must choose N large enough to satisfy the following condition:
Condition 2. If k  k   then  with probability ⇣0  given Mk  

(4)

g|
krLk  (⌫1 + ⌫3)krLkkkgkk + ⌘kgkk2

and

kgkk  ⌫2krLkk.

(6)

Condition 2 is the only restriction on the stochastic gradients: they have to point in roughly the right
direction most of the time  and they have to be of roughly the right magnitude when they do. By
constructing the stochastic gradients from large enough minibatches of draws from the variational
distribution  this condition can always be met.
In practice  we cannot observe rL  and we do not explicitly set ⌫1  ⌫2  and ⌫3. Fortunately  Con-
dition 2 holds as long as our stochastic gradients remain large in relation to their variance. Because
we base each stochastic gradient on at least one sizable minibatch  we always have many iid samples
to inform us about the population of stochastic gradients. We use a jackknife estimator [15] to con-
servatively bound the standard deviation of the norm of the stochastic gradient. If the norm of a given
stochastic gradient is small relative to its standard deviation  we double the next iteration’s sampling
rate. If it is large relative to its standard deviation  we halve it. Otherwise  we leave it unchanged.
The gradient observations may include randomness from sources other than sampling the variational
distribution too. In the “doubly stochastic” setting [3]  for example  the data is also subsampled.
This setting is fully compatible with our algorithm  though the size of the subsample may need to
vary across iterations. To simplify our presentation  we henceforth only consider stochasticity from
sampling the variational distribution.
Condition 3 is the only restriction on the quadratic models’ Hessians.
Condition 3. There exists ﬁnite H satisfying  for the spectral norm 

for all iterations k with k  k .

kHkk  H a. s.

4

(7)

For concreteness we bound the spectral norm of Hk  but a bound on any Lp norm sufﬁces. The
algorithm speciﬁcation does not involve H  but the convergence proof requires that H be ﬁnite.
This condition sufﬁces to ensure that  when the trust region is small enough  the model’s Hessian
cannot interfere with ﬁnding a descent direction. With such mild conditions  we are free to use
nearly arbitrary Hessians. Hessians may be formed like the stochastic gradients  by sampling from
the variational distribution. The number of samples can be varied. The quadratic model’s Hessian
could even be set to the identity matrix if we prefer not to compute second-order information.
Low-dimensional models  and models with block diagonal Hessians  may be optimized explicitly
by inverting Hk + ↵kI  where ↵k is either zero for interior solutions  or just large enough that
(Hk + ↵kI)1gk is on the boundary of the trust region [5]. Matrix inversion has cubic runtime
though  and even explicitly storing Hk is prohibitive for many variational objectives.
In our experiments  we instead maximize the model without explicitly storing the Hessian  through
Hessian-vector multiplication  assembling Krylov subspaces through both conjugate gradient
iterations and Lanczos iterations [16  17]. We reuse our Hessian approximation for two consecutive
iterations if the iterate does not change (i.e.  the proposed steps are rejected). A new stochastic
gradient gk is still drawn for each of these iterations.

3.3 Assessing the proposed step

Deterministic trust region methods only accept steps that improve the objective by enough. In a
stochastic setting  we must ensure that accepting “bad” steps is improbable while accepting “good”
steps is likely.
To assess steps  TrustVI draws new samples from the variational distribution—we may not
reuse the samples that gk and Hk are based on. The new samples are used to estimate both
L(!k) and L(!k + sk). Using the same sample to estimate both quantities is analogous to a
matched-pairs experiment; it greatly reduces the variance of the improvement estimator. Formally 
for i = 1  . . .   NK  let eki follow the base distribution and set

Let

`0ki   ˆL(!k + sk; eki)  ˆL(!k; eki).

(8)

(9)

`0k   1
Nk

`0ki.

NkXi=1

Then  `0k is an unbiased estimate of L0k—the quantity a deterministic trust region method would use
to assess the proposed step.

3.3.1 Choosing the sample size

To pick the sample size NK  we need additional control on the distribution of the `0ki. The next
condition gives us that.
Condition 4. For each k  there exists ﬁnite k such that the `0ki are k-subgaussian.
Unlike the quantities we have introduced earlier  such as L and H  the k need to be known to carry
out the algorithm. Because `0k1 ` 0k2  . . . are iid  2
k may be estimated—after the sample is drawn—by
i=1(`0ki  `0k). We discuss below  in the context of
the population variance formula  i.e. 
setting Nk  how to make use of a “retrospective” estimate of k in practice.
Two user-selected constants control what steps are accepted: ⌘ 2 (0  1/2) and > 0. The step
is accepted iff 1) the observed improvement `0k exceeds the fraction ⌘ of the model improvement
m0k  and 2) the model improvement is at least a small fraction /⌘ of the trust region radius squared.
Formally  steps are accepted iff

Nk1PNk

1

`0k  ⌘m0k  2
k.

If ⌘m0k < 2
Otherwise  we pick the smallest Nk such that

k  the step is rejected regardless of `0k: we set Nk = 0.

Nk 

22
k

(⌘m0k + y)2 log✓ ⌧22

k + y
⌧12

k ◆   8y > max✓

(10)

(11)

⌘m0k
2

k◆
 ⌧22

5

where

⌧2   ↵(2  2).

⌧1   ↵(1  2)   and

(12)
Finding the smallest such Nk is a one-dimensional optimization problem. We solve it via bisection.
Inequality 11 ensures that we sample enough to reject most steps that do not improve the objective
sufﬁciently. If we knew exactly how a proposed step changed the objective  we could express in
closed form how many samples would be needed to detect bad steps with sufﬁciently high probability.
Since we do not know that  Inequality 11 is for all such change-values in a range. Nonetheless  Nk
is rarely large in practice: the second factor lower bounding Nk is logarithmic in y; in the ﬁrst factor
the denominator is bounded away from zero.
Finally  if k  k   we also ensure Nk is large enough that

Nk  22

k log(1  ⇣1)
⌫2
1krLkk22

.

(13)

k

Selecting Nk this large ensures that we sample enough to detect most steps that improve the value of
the objective sufﬁciently when the trust region is small. This bound is not high in practice. Because
of how the `0ki are collected (a “matched-pairs experiment”)  as k becomes small  k becomes small
too  at roughly the same rate.
In practice  at the end of each iteration  we estimate whether Nk was large enough to meet the condi-
tions. If not  we set Nk+1 = 2Nk. If Nk exceeds the size of the gradient’s minibatch  and it is more
than twice as large as necessary to meet the conditions  we set Nk+1 = Nk/2. These Nk function
evaluations require little computation compared to computing gradients and Hessian-vector products.

4 Convergence to a stationary point

To show that TrustVI converges to a stationary point  we reason about the stochastic process (k)1k=1 
where

k   Lk  ↵2
k.

(14)

In words  k is the objective function penalized by the weighted squared trust region radius.
Because TrustVI is stochastic  neither Lk nor k necessarily increase at every iteration. But  k
increases in expectation at each iteration (Lemma 1). That alone  however  does not sufﬁce to show
TrustVI reaches a stationary point; k must increase in expectation by enough at each iteration.
Lemma 1 and Lemma 2 in combination show just that. The latter states that the trust region radius
cannot remain small unless the gradient is small too  while the former states that the expected
increase is a constant fraction of the squared trust region radius. Perhaps surprisingly  Lemma 1
does not depend on the quality of the quadratic model: Rejecting a proposed step always leads to
sufﬁcient increase in k. Accepting a bad step  though possible  rapidly becomes less likely as the
proposed step gets worse. No matter how bad a proposed step is  k increases in expectation.
Theorem 1 uses the lemmas to show convergence by contradiction. The structure of its proof 
excluding the proofs of the lemmas  resembles the proof from [5] that a deterministic trust region
method converges. The lemmas’ proofs  on the other hand  more closely resemble the style of
reasoning in the stochastic optimization literature [12].
Theorem 1. For Algorithm 1 

lim

k!1krLkk = 0 a. s.

(15)

Proof. By Condition 1  L is bounded above. The trust region radius k is positive almost surely by
construction. Therefore  k is bounded above almost surely by the constant supL. Let the constant
c   supL 0. Then 

1Xk=1

E[k | Mk ]  c a. s.

(16)

6

k ]  and hence E[k | Mk ]  is almost surely nonnegative. Therefore 
By Lemma 1  E[k | M+
E[k | Mk ] ! 0 almost surely. By an additional application of Lemma 1  2
k ! 0 almost surely
too.
Suppose there exists K0 and ✏> 0 such that krLkk  ✏ for all k > K1. Fix K  K0 such that
k meets the conditions of Lemma 2 for all k  K. By Lemma 2  (log k)1K is a submartingale.
A submartingale almost surely does not go to 1  so k almost surely does not go to 0. The
contradiction implies that krLkk <✏ inﬁnitely often.
Because our choice of ✏ was arbitrary 

Because 2
Lemma 1.

lim inf

k!1 krLkk = 0 a. s.

k ! 0 almost surely  this limit point is unique.
k⇤  2
k ] = (1  ⇡)[↵(1  2)2
k] + ⌧12

E⇥k | M+

E[k | M+

k a. s.

(17)

(18)

(19)
(20)

(21)
(22)

(23)

(24)

(25)

(26)

Proof. Let ⇡ denote the probability that the proposed step is accepted. Then 

k] + ⇡[L0k  ↵(2  1)]2
k + 2
k.

k

= ⇡[L0k  ⌧22
By the lower bound on ↵  ⌧1  0. If ⌘m0k < 2
holds. Also  if L0k  ⌧22
k.
⌘m0k  2
The probability ⇡ of accepting this step is a tail bound on the sum of iid subgaussian random
variables. By Condition 4  Hoeffding’s inequality applies. Then  Inequality 11 lets us cancel some
of the remaining iteration-speciﬁc variables:

k  the step is rejected regardless of `k  so the lemma
k and

k  then lemma holds for any ⇡ 2 [0  1]. So  consider just L0k <⌧ 22

⇡ = P(`0k  ⌘m0k | M+
k )
= P(`0k L 0k  ⌘m0k L 0k | M+
k )

k!
(`0ki L 0k)  (⌘m0k L 0k)Nk M+

(⌘m0k L 0k)2Nk

22
k



= P NKXi=1
 exp⇢



⌧22

⌧12
k
k L 0k

.

The lemma follows from substituting Inequality 25 into Equation 20.
Lemma 2. For each iteration k  on the event k  k   we have
P(`0k  ⌘m0k | Mk )  ⇣0⇣1 >
The proof appears in Appendix A of the supplementary material.

1
2

.

5 Experiments
Our experiments compare TrustVI to both Automatic Differentiation Variational Inference (ADVI) [4]
and Hessian-free Stochastic Gradient Variational Inference (HFSGVI) [9]. We use the authors’
Stan [21] implementation of ADVI  and implement the other two algorithms in Stan as well.
Our study set comprises 183 statistical models and datasets from [22]  an online repository of
open-source Stan models and datasets. For our trials  the variational distribution is always mean-ﬁeld
multivariate Gaussian. The dimensions of ELBO domains range from 2 to 2012.

7

(a) A variance components model (“Dyes”) from [18].
18-dimensional domain.

(b) A bivariate normal hierarchical model (“Birats”)
from [19]. 132-dimensional domain.

(c) A multi-level
from [20]. 100-dimensional domain.

linear model (“Electric Chr”)

(d) A multi-level linear model (“Radon Redundant
Chr”) from [20]. 176-dimensional domain.

Figure 1: Each panel shows optimization paths for ﬁve runs of ADVI  TrustVI  and HFSGVI  for
a particular dataset and statistical model. Both axes are log scale.

In addition to the ﬁnal objective value for each method  we compare the runtime each method
requires to produce iterates whose ELBO values are consistently above a threshold. As the threshold 
for each pair of methods we compare  we take the ELBO value reached by the worse performing
method  and subtract one nat from it.
We measure runtime in “oracle calls” rather than wall clock time so that the units are independent
of the implementation. Stochastic gradients  stochastic Hessian-vector products  and estimates
of change in ELBO value are assigned one  two  and one oracle calls  respectively  to reﬂect the
number of ﬂoating point operations required to compute them. Each stochastic gradient is based
on a minibatch of 256 samples of the variational distribution. The number of variational samples
for stochastic Hessian-vector products and for estimates of change (85 and 128  respectively) are
selected to match the degree of parallelism for stochastic gradient computations.
To make our comparison robust to outliers  for each method and each model  we optimize ﬁve times 
but ignore all runs except the one that attains the median ﬁnal objective value.

5.1 Comparison to ADVI

ADVI has two phases that contribute to runtime: During the ﬁrst phase  a learning rate is selected
based on progress made by SGD during trials of 50 (by default) “adaptation” SGD iterations  for
as many as six learning rates. During the second phase  the variational objective is optimized with
the learning rate that made the most progress during the trials. If the number of adaptation iterations
is small relative to the number of iterations needed to optimize the variational objective  then the
learning rate selected may be too large: what appears most productive at ﬁrst may be overly “greedy”
for a longer run. Conversely  a large number of adaptation iteration may leave little computational
budget for the actual optimization. We experimented with both more and fewer adaptation iterations

8

100101102103104runtime (oracle calls)1031041051061071081091010ELBOADVITrustVIHFSGVI--------100101102103104runtime (oracle calls)103104105106107108ELBOADVITrustVIHFSGVI------100101102103104runtime (oracle calls)102.90102.95103.00103.05103.10103.15103.20103.25ELBOADVITrustVIHFSGVI--------100101102103104runtime (oracle calls)103.2103.4103.6103.8104.0104.2ELBOADVITrustVIHFSGVI------than the default but did not ﬁnd a setting that was uniformly better than the default. Therefore  we
report on the default number of adaption iterations for our experiments.
Case studies. Figure 1 and Appendix B show the optimization paths for several models  chosen to
demonstrate typical performance. Often ADVI does not ﬁnish its adaptation phase before TrustVI
converges. Once the adaptation phase ends  ADVI generally increased the objective value function
more gradually than TrustVI did  despite having expended iterations to tune its learning rate.
Quality of optimal points. For 126 of the 183 models (69%)  on sets of ﬁve runs  the median optimal
values found by ADVI and TrustVI did not differ substantively. For 51 models (28%)  TrustVI found
better optimal values than ADVI. For 6 models (3%)  ADVI found better optimal values than TrustVI.
Runtime. We excluded model-threshold pairs from the runtime comparison that did not require at
least ﬁve iterations to solve; they were too easy to be representative of problems where the choice
of optimization algorithm matters. For 136 of 137 models (99%) remaining in our study set  TrustVI
was faster than ADVI. For 69 models (50%)  TrustVI was at least 12x faster than ADVI. For 34
models (25%)  TrustVI was at least 36x faster than ADVI.

5.2 Comparison to HFSGVI

HFSGVI applies Newton’s method—an algorithm that converges for convex and deterministic
objective functions—to an objective function that is neither. But do convergence guarantees matter
in practice?
Often HFSGVI takes steps so large that numerical overﬂow occurs during the next iteration: the
gradient “explodes” during the next iteration if we take a bad enough step. With TrustVI  we reject
obviously bad steps (e.g.  those causing numerical overﬂow) and try again with a smaller trust region.
We tried several heuristics to workaround this problem with HFSGVI  including shrinking the
norm of the very large steps that would otherwise cause numerical overﬂow. But “large” is relative 
depending on the problem  the parameter  and the current iterate; severely restricting step size would
unfairly limit HFSGVI’s rate of convergence. Ultimately  we excluded 23 of the 183 models from
further analysis because HFSGVI consistently generated numerical overﬂow errors for them  leaving
160 models in our study set.
Case studies. Figure 1 and Appendix B show that even when HFSGVI does not step so far as to
cause numerical overﬂow  it nonetheless often makes the objective value worse before it gets better.
HFSGVI  however  sometimes makes faster progress during the early iterations  while TrustVI is
rejecting steps as it searches for an appropriate trust region radius.
Quality of optimal points. For 107 of the 160 models (59%)  on sets of ﬁve runs  the median optimal
value found by TrustVI and HFSGVI did not differ substantively. For 51 models (28%)  TrustVI
found a better optimal values than HFSGVI. For 1 model (0.5%)  HFSGVI found a better optimal
value than TrustVI.
Runtime. We excluded 45 model-threshold pairs from the runtime comparison that did not require
at least ﬁve iterations to solve  as in Section 5.1. For the remainder of the study set  TrustVI was
faster than HFSGVI for 61 models  whereas HFSGVI was faster than TrustVI for 54 models. As
a reminder  HFSGVI failed to converge on another 23 models that we excluded from the study set.

6 Conclusions

For variational inference  it is no longer necessary to pick between slow stochastic ﬁrst-order
optimization (e.g.  ADVI) and fast-but-restrictive deterministic second-order optimization. The
algorithm we propose  TrustVI  leverages stochastic second-order information  typically ﬁnding
a solution at least one order of magnitude faster than ADVI. While HFSGVI also uses stochastic
second-order information  it lacks convergence guarantees. For more than one-third of our
experiments  HFSGVI terminated at substantially worse ELBO values than TrustVI  demonstrating
that convergence theory matters in practice.

9

References
[1] Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on

Learning Representations  2014.

[2] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In International Conference on Machine Learning  2014.
[3] Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate

inference. In International Conference on Machine Learning  2014.

[4] Alp Kucukelbir  Dustin Tran  Rajesh Ranganath  Andrew Gelman  and David M. Blei. Automatic

Differentiation Variational Inference. Journal of Machine Learning Research  18(14):1–45  2017.

[5] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer  2nd edition  2006.
[6] Jeffrey Regier et al. Learning an astronomical catalog of the visible universe through scalable Bayesian

inference. arXiv preprint arXiv:1611.03404  2016.

[7] Jeffrey Fike and Juan Alonso. Automatic differentiation through the use of hyper-dual numbers for second

derivatives. In Recent Advances in Algorithmic Differentiation  pages 163–173. Springer  2012.

[8] Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation  6(1):147–160  1994.
[9] Kai Fan  Ziteng Wang  Jeffrey Beck  James Kwok  and Katherine Heller. Fast second-order stochastic
backpropagation for variational inference. In Advances in Neural Information Processing Systems  2015.
[10] Sara Shashaani  Susan Hunter  and Raghu Pasupathy. ASTRO-DF: Adaptive sampling trust-region op-
timization algorithms  heuristics  and numerical experience. In IEEE Winter Simulation Conference  2016.
[11] Geng Deng and Michael C Ferris. Variable-number sample-path optimization. Mathematical Programming 

117(1):81–109  2009.

[12] Ruobing Chen  Matt Menickelly  and Katya Scheinberg. Stochastic optimization using a trust-region

method and random models. Mathematical Programming  pages 1–41  2017.

[13] David M Blei  Alp Kucukelbir  and Jon D McAuliffe. Variational inference: A review for statisticians.

Journal of the American Statistical Association  2017.

[14] James Spall. Introduction to stochastic search and optimization: Estimation  simulation  and control.

John Wiley & Sons  2005.

[15] Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics  pages

586–596  1981.

[16] Nicholas Gould  Stefano Lucidi  Massimo Roma  and Philippe Toint. Solving the trust-region subproblem

using the Lanczos method. SIAM Journal on Optimization  9(2):504–525  1999.

[17] Felix Lenders  Christian Kirches  and Andreas Potschka. trlib: A vector-free implementation of the GLTR

method for iterative solution of the trust region problem. arXiv preprint arXiv:1611.04718  2016.

[18] OpenBugs developers. Dyes: Variance components model. http://www.openbugs.net/Examples/

Dyes.html  2017. [Online; accessed Oct 8  2017].

[19] OpenBugs developers. Rats: A normal hierarchical model. http://www.openbugs.net/Examples/

Rats.html  2017. [Online; accessed Oct 8  2017].

[20] Andrew Gelman and Jennifer Hill. Data analysis using regression and multilevel/hierarchical models.

Cambridge University Press  2006.

[21] Bob Carpenter et al. Stan: A probabilistic programming language. Journal of Statistical Software  20  2016.
[22] Stan developers. https://github.com/stan-dev/example-models  2017. [Online; accessed Jan

3  2017; commit 6fbbf36f9d14ed69c7e6da2691a3dbe1e3d55dea].

[23] OpenBugs developers. Alligators: Multinomial-logistic regression. http://www.openbugs.net/

Examples/Aligators.html  2017. [Online; accessed Oct 4  2017].

[24] OpenBugs developers. Seeds: Random effect logistic regression. http://www.openbugs.net/

Examples/Seeds.html  2017. [Online; accessed Oct 4  2017].

[25] David Lunn  Chris Jackson  Nicky Best  Andrew Thomas  and David Spiegelhalter. The BUGS book:

A practical introduction to Bayesian analysis. CRC press  2012.

10

,Daniel Yamins
Ha Hong
Charles Cadieu
James DiCarlo
Jeffrey Regier
Michael Jordan
Jon McAuliffe