2018,Actor-Critic Policy Optimization in Partially Observable Multiagent Environments,Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper  we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases  leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games)  using RL-style function approximation. We evaluate on commonly used benchmark Poker domains  showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero-sum games  without any domain-specific state space reductions.,Actor-Critic Policy Optimization in Partially

Observable Multiagent Environments

Sriram Srinivasan∗ 1

srsrinivasan@

Marc Lanctot∗ 1

lanctot@

Vinicius Zambaldi1

vzambaldi@

Julien Pérolat1

perolat@

Karl Tuyls1
karltuyls@

Rémi Munos1

munos@

Michael Bowling1

bowlingm@

...@google.com. 1DeepMind. ∗These authors contributed equally.

Abstract

Optimization of parameterized policies for reinforcement learning (RL) is an impor-
tant and challenging problem in artiﬁcial intelligence. Among the most common
approaches are algorithms based on gradient ascent of a score function representing
discounted return. In this paper  we examine the role of these policy gradient and
actor-critic algorithms in partially-observable multiagent environments. We show
several candidate policy update rules and relate them to a foundation of regret
minimization and multiagent learning techniques for the one-shot and tabular cases 
leading to previously unknown convergence guarantees. We apply our method to
model-free multiagent reinforcement learning in adversarial sequential decision
problems (zero-sum imperfect information games)  using RL-style function ap-
proximation. We evaluate on commonly used benchmark Poker domains  showing
performance against ﬁxed policies and empirical convergence to approximate Nash
equilibria in self-play with rates similar to or better than a baseline model-free
algorithm for zero-sum games  without any domain-speciﬁc state space reductions.

1

Introduction

There has been much success in learning parameterized policies for sequential decision-making
problems. One paradigm driving progress is deep reinforcement learning (Deep RL)  which uses
deep learning [52] to train function approximators that represent policies  reward estimates  or both 
to learn directly from experience and rewards [85]. These techniques have learned to play Atari
games beyond human-level [60]  Go  chess  and shogi from scratch [82  81]  complex behaviors in
3D environments [59  97  37]  robotics [27  73]  character animation [67]  among others.
When multiple agents learn simultaneously  policy optimization becomes more complex. First  each
agent’s environment is non-stationary and naive approaches can be non-Markovian [58]  violating the
requirements of many traditional RL algorithms. Second  the optimization problem is not as clearly
deﬁned as maximizing one’s own expected reward  because each agent’s policy affects the others’
optimization problems. Consequently  game-theoretic formalisms are often used as the basis for
representing interactions and decision-making in multiagent systems [17  79  64].
Computer poker is a common multiagent benchmark domain. The presence of partial observability
poses a challenge for traditional RL techniques that exploit the Markov property. Nonetheless  there
has been steady progress in poker AI. Near-optimal solutions for heads-up limit Texas Hold’em were
found with tabular methods using state aggregation  powered by policy iteration algorithms based
on regret minimization [102  87  12]. These approaches were founded on a basis of counterfactual

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

regret minimization (CFR)  which is the root of recent advances in no-limit  such as Libratus [16]
and DeepStack [61]. However  (i) both required Poker-speciﬁc domain knowledge  and (ii) neither
were model-free  and hence are unable to learn directly from experience  without look-ahead search
using a perfect model of the environment.
In this paper  we study the problem of multiagent reinforcement learning in adversarial games with
partial observability  with a focus on the model-free case where agents (a) do not have a perfect
description of their environment (and hence cannot do a priori planning)  (b) learn purely from
their own experience without explicitly modeling the environment or other players. We show that
actor-critics reduce to a form of regret minimization and propose several policy update rules inspired
by this connection. We then analyze the convergence properties and present experimental results.

2 Background and Related Work

We brieﬂy describe the necessary background. While we draw on game-theoretic formalisms  we
align our terminology with RL. We include clariﬁcations in Appendix A1. For details  see [79  85].

of rewards is a return Gt =(cid:80)∞

2.1 Reinforcement Learning and Policy Gradient Algorithms
An agent acts by taking actions a ∈ A in states s ∈ S from their policy π : s → ∆(A)  where ∆(X)
is the set of probability distributions over X  which results in changing the state of the environment
st+1 ∼ T (st  at); the agent then receives an observation o(st  at  st+1) ∈ Ω and reward Rt.2 A sum
t(cid:48)=t Rt(cid:48)  and aim to ﬁnd π∗ that maximizes expected return Eπ[G0].3
Value-based solution methods achieve this by computing estimates of vπ(s) = Eπ[Gt | St = s]  or
qπ(s  a) = Eπ[Gt | St = s  At = a]  using temporal difference learning to bootstrap from other esti-
mates  and produce a series of -greedy policies π(s  a) = /|A| + (1− )I(a = argmaxa(cid:48) qπ(s  a(cid:48))).
In contrast  policy gradient methods deﬁne a score function J(πθ) of some parameterized (and
differentiable) policy πθ with parameters θ  and use gradient ascent directly on J(πθ) to update θ.
There have been several recent successful applications of policy gradient algorithms in complex
domains such as self-play learning in AlphaGo [80]  Atari and 3D maze navigation [59]  continuous
control problems [76  54  21]  robotics [27]  and autonomous driving [78]. At the core of several
recent state-of-the-art Deep RL algorithms [37  22] is the advantage actor-critic (A2C) algorithm
deﬁned in [59]. In addition to learning a policy (actor)  A2C learns a parameterized critic: an estimate
of vπ(s)  which it then uses both to estimate the remaining return after k steps  and as a control
variate (i.e. baseline) that reduces the variance of the return estimates.

2.2 Game Theory  Regret Minimization  and Multiagent Reinforcement Learning
In multiagent RL (MARL)  n = |N| = |{1  2 ···   n}| agents interact within the same environment.
At each step  each agent i takes an action  and the joint action a leads to a new state st+1 ∼ T (st  at);
each player i receives their own separate observation oi(st  a  st+1) and reward rt i. Each agent
maximizes their own return Gt i  or their expected return which depends on the joint policy π.
Much work in classical MARL focuses on Markov games where the environment is fully observable
and agents take actions simultaneously  which in some cases admit Bellman operators [55  103  70  69].
When the environment is partially observable  policies generally map to values and actions from
agents’ observation histories; even when the problem is cooperative  learning is hard [65].

We focus our attention to the setting of zero-sum games  where (cid:80)

In this case 
polynomial algorithms exist for ﬁnding optimal policies in ﬁnite tasks for the two-player case. The
guarantees that Nash equilibrium provides are less clear for the (n > 2)-player case  and ﬁnding one
is hard [20]. Despite this  regret minimization approaches are known to ﬁlter out dominated actions 
and have empirically found good (e.g. competition-winning) strategies in this setting [74  26  48].

i∈N rt i = 0.

1Appendices are included in the technical report version of the paper; see [84].
2Note that in fully-observable settings  o(st  at  st+1) = st+1. In partially observable environments [39  65] 

an observation function O : S × A → ∆(Ω) is used to sample o(st  at  st+1) ∼ O(st  at).

3 We assume ﬁnite episodic tasks of bounded length and leave out the discount factor γ to simplify the

notation  without loss of generality. We use γ(= 0.99)-discounted returns in our experiments.

2

Partially observable environments require a few key deﬁnitions in order to deﬁne the notion of state.
A history h ∈ H is a sequence of actions from all players including the environment taken from
the start of an episode. The environment (also called “nature”) is treated as a player with a ﬁxed
policy and there is a deterministic mapping from any h to the actual state of the environment. Deﬁne
an information state  st = {h ∈ H | player i’s sequence of observations  oi t(cid:48)<t(st(cid:48)  at(cid:48)  st(cid:48)+1) 
is consistent with h}4. So  st includes histories leading to st that are indistinguishable to player i;
e.g. in Poker  the h ∈ st differ only in the private cards dealt to opponents. A joint policy π is a Nash
i π−i[G0 i]−Eπ[G0 i] =
equilibrium if the incentive to deviate to a best response δi(π) = maxπ(cid:48)
0 for each player i ∈ N   where π−i is the set of i(cid:48)s opponents’ policies. Otherwise  -equilibria are
approximate  with  = maxi δi(π). Regret minimization algorithms produce iterates whose average
i δi(π). Nash equilibrium is

Eπ(cid:48)

i

¯π reduces an upper bound on   measured using NASHCONV(π) =(cid:80)
ηπ(ht) =(cid:81)

minimax-optimal in two-player zero-sum games: using one minimizes worst-case losses.
There are well-known links between learning  game theory and regret minimization [9]. One method 
counterfactual regret (CFR) minimization [102]  has led to signiﬁcant progress in Poker AI. Let
t(cid:48)<t π(st(cid:48)  at(cid:48))  where ht(cid:48) (cid:64) ht is a preﬁx  ht(cid:48) ∈ st(cid:48)  ht ∈ st  be the reach probability
of h under π from all policies’ action choices. This can be split into player i’s contribution and their
opponents’ (including nature’s) contribution  ηπ(h) = ηπ
i (h)ηπ−i(h). Suppose player i is to play at
s: under perfect recall  player i remembers the sequence of their own states reached  which is the
same for all h ∈ s  since they differ only in private information seen by opponent(s); as a result
∀h  h(cid:48) ∈ s  ηπ
i (s). For some history h and action a  we call h a preﬁx history
h (cid:64) ha  where ha is the history h followed by action a; they may also be smaller  so h (cid:64) ha (cid:64)
hab ⇒ h (cid:64) hab. Let Z = {z ∈ H | z is terminal} and Z(s  a) = {(h  z) ∈ H × Z | h ∈ s  ha (cid:118)
z}. CFR deﬁnes counterfactual values vc
i (z)ui(z)  and
i (π  st  at)  where ui(z) is the return to player i along z  and accumulates
vc
a π(st  a)vc
i (π  st  a(cid:48)) − vc
regrets REGi(π  st  a(cid:48)) = vc
i (π  st)  producing new policies from cumulative regret
using e.g. regret-matching [28] or exponentially-weighted experts [6  15].
CFR is a policy iteration algorithm that computes the expected values by visiting every possible
trajectory  described in detail in Appendix B. Monte Carlo CFR (MCCFR) samples trajectories using
an exploratory behavior policy  computing unbiased estimates ˆvc
importance sampling [49]. Therefore  MCCFR is an off-policy Monte Carlo method. In one MCCFR
variant  model-free outcome sampling (MFOS)  the behavior policy at opponent states is deﬁned as
π−i enabling online regret minimization (player i can update their policy independent of π−i and T ).
There are two main problems with (MC)CFR methods: (i) signiﬁcant variance is introduced by
sampling (off-policy) since quantities are divided by reach probabilities  (ii) there is no generalization
across states except through expert abstractions and/or forward simulation with a perfect model. We
show that actor-critics address both problems and that they are a form of on-policy MCCFR.

i (π  st) and (cid:100)REGi(π  st) corrected by

i (π  st  at) = (cid:80)

(h z)∈Z(st at) ηπ−i(h)ηπ

i (h) = ηπ

i (h(cid:48)) := ηπ

i (π  st) =(cid:80)

2.3 Most Closely Related Work

There is a rich history of policy gradient approaches in MARL. Early uses of gradient ascent showed
that cyclical learning dynamics could arise  even in zero-sum matrix games [83]. This was partly
addressed by methods that used variable learning rates [13  11]  policy prediction [99]  and weighted
updates [1]. The main limitation with these classical works was scalability: there was no direct way
to use function approximation  and empirical analyses focused almost exclusively on one-shot games.
Recent work on policy gradient approaches to MARL addresses scalability by using newer algorithms
such as A3C or TRPO [76]. However  they focus signiﬁcantly less (if at all) on convergence
guarantees. Naive approaches such as independent reinforcement learning fail to ﬁnd optimal
stochastic policies [55  32] and can overﬁt the training data [50]. Much progress has been achieved
for cooperative MARL: learning to communicate [51]  Starcraft unit micromanagement [24]  taxi
ﬂeet optimization [63]  and autonomous driving [78]. There has also been progress for mixed
cooperative/competitive environments: using a centralized critic [57]  learning to negotiate [18] 
anticipating/learning opponent responses in social dilemmas [23  53]  and control in realistic physical
environments [3  7]. The most common methodology has been to train centrally (for decentralized
execution)  either having direct access to the other players’ policy parameters or modeling them. As a
result  assumptions are made about the other agents’ policies  utilities  or learning mechanisms.

4In deﬁning st  we drop the reference to acting player i in turn-based games without loss of generality.

3

There are also methods that attempt to model the opponents [36  30  4]. Our methods do no such
modeling  and can be classiﬁed in the “forget” category of the taxonomy proposed in [33]: that is 
due to its on-policy nature  actors and critics adapt to and learn mainly from new/current experience.
We focus on the model-free (and online) setting: other agents’ policies are inaccessible; training
is not separated from execution. Actor-critics were recently studied in this setting for multiagent
games [68]  whereas we focus on partially-observable environments; only tabular methods are known
to converge. Fictitious Self-Play computes approximate best responses via RL [31  32]  and can
also be model-free. Regression CFR (RCFR) uses regression to estimate cumulative regrets from
CFR [93]. RCFR is closely related to Advantage Regret Minimization (ARM) [38]. ARM [38] shows
regret estimation methods handle partial observability better than standard RL  but was not evaluated
in multiagent environments. In contrast  we focus primarily on the multiagent setting.

3 Multiagent Actor-Critics: Advantages and Regrets

TCREGi(K  s  a) = ((cid:80)

CFR deﬁnes policy update rules

from thresholded cumulative counterfactual

regret:
k∈{1 ···  K} REGi(πk  s  a))+  where k is the number of iterations and
(x)+ = max(0  x). In CFR  regret matching updates a policy to be proportional to TCREGi(K  s  a).
On the other hand  REINFORCE [95] samples trajectories and computes gradients for each state st 
updating θ toward ∇θ log(st  at; θ)Gt. A baseline is often subtracted from the return: Gt − vπ(st) 
and policy gradients then become actor-critics  training π and vπ separately. The log appears due to the
fact that action at is sampled from the policy  the value is divided by π(st  at) to ensure the estimate
is properly estimating the true expectation [85  Section 13.3]  and ∇θπ(st  at; θ)/π(st  at  θ) =
∇θ log π(st  at; θ). One could instead train qπ-based critics from states and actions. This leads to a
q-based Policy Gradient (QPG) (also known as Mean Actor-Critic [5]):

∇QPG

θ

(s) =

[∇θπ(s  a; θ)]

π(s  b; θ)q(s  b  w)

 

(1)

(cid:32)
q(s  a; w) −(cid:88)

b

(cid:33)

(cid:33)+

(cid:88)

a

(cid:88)

(cid:32)
q(s  a; w) −(cid:88)

an advantage actor-critic algorithm differing from A2C in the (state-action) representation of the crit-
ics [56  96] and summing over actions similar to the all-action algorithms [86  71  19  5]. Interpreting
b π(s  b)qπ(s  b) as a regret  we can instead minimize a loss deﬁned by an
k(aπk (s  a))+  moving

aπ(s  a) = qπ(s  a) −(cid:80)
upper bound on the thresholded cumulative regret:(cid:80)
(cid:32)
q(s  a; w) −(cid:88)

the policy toward a no-regret region. We call this Regret Policy Gradient (RPG):

k(aπk (s  a))+ ≥ ((cid:80)

(s) = −(cid:88)

π(s  b; θ)q(s  b; w)

.

(2)

(cid:33)+

∇RPG

∇θ

θ

a

b

The minus sign on the front represents a switch from gradient ascent on the score to descent on the
loss. Another way to implement an adaptation of the regret-matching rule is by weighting the policy
gradient by the thresholded regret  which we call Regret Matching Policy Gradient (RMPG):

∇RMPG

θ

(s) =

[∇θπ(s  a; θ)]

π(s  b; θ)q(s  b  w)

.

(3)

a

b

In each case  the critic q(st  at; w) is trained in the standard way  using (cid:96)2 regression loss from
sampled returns. The pseudo-code is given in Algorithm 2 in Appendix C. In Appendix F  we show
that the QPG gradient is proportional to the RPG gradient at s: ∇RPG

(s) ∝ ∇QPG

(s).

θ

θ

3.1 Analysis of Learning Dynamics on Normal-Form Games

The ﬁrst question is whether any of these variants can converge to an equilibrium  even in the
simplest case. So  we now show phase portraits of the learning dynamics on Matching Pennies: a
two-action version of Rock  Paper  Scissors. These analyses are common in multiagent learning as
they allow visual depiction of the policy changes and how different factors affect the (convergence)
behavior [83  92  13  91  11  94  1  99  98  8  89]. Convergence is difﬁcult in Matching Pennies as
the only Nash equilibrium π∗ = (( 1
2 )) requires learning stochastic policies. We give more
detail and results on different games that cause cyclic learning behavior in Appendix D.

2 )  ( 1

2   1

2   1

4

(a) Replicator Dynamics

(c) Average RPG Dynamics
Figure 1: Learning Dynamics in Matching Pennies: (a) and (b) show the vector ﬁeld for ∂π/∂t
including example particle traces  where each point is each player’s probability of their ﬁrst action;

(c) shows example traces of policies following a discrete approximation to(cid:82) t

(b) RPG Dynamics

0 ∂π/∂t.

In Figure 1  we see the similarity of the regret dynamics to replicator dynamics [88  75]. We also
show the average policy dynamics and observe convergence to equilibrium in each game we tried 
which is a known to be guaranteed in two-player zero-sum games using CFR  ﬁctitious play [14]  and
continuous replicator dynamics [35]. However  computing the average policy is complex [31  102]
and potentially worse with function approximation  requiring storing past data in large buffers [32].

3.2 Partially Observable Sequential Games

How do the values vc
i (π  st  at) and qπ i(st  at) differ? The authors of [38] posit that they are
approximately equal when st rarely occurs more than once in a trajectory. First  note that st cannot
be reached more than once in a trajectory from our deﬁnition of st  because the observation histories
(of the player to play at st) would be different in each occurrence (i.e. due to perfect recall). So  the
two values are indeed equal in deterministic  single-agent environments. In general  counterfactual
values are conditioned on player i playing to reach st  whereas q-function estimates are conditioned
on having reached st. So  qπ i(st  at) = Eρ∼π[Gt i | St = st  At = at]

Pr(h | st)ηπ(ha  z)ui(z)
Pr(st | h) Pr(h)

ηπ(ha  z)ui(z)

h z∈Z(st at)

Pr(st)

where ηπ(ha  z) =

ηπ(z)

ηπ(h)π(s  a)

by Bayes’ rule

since h ∈ st  h is unique to st

h z∈Z(st at)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

h z∈Z(st at)

h z∈Z(st at)

h z∈Z(st at)

h z∈Z(st at)

h z∈Z(st at)

=

=

=

=

=

=

=

ηπ(ha  z)ui(z)

ηπ(ha  z)ui(z)

ηπ(ha  z)ui(z)

Pr(h)
Pr(st)

(cid:80)
(cid:80)
i (s)(cid:80)
(cid:80)

ηπ

h(cid:48)∈st

ηπ(h)

ηπ(h(cid:48))

h(cid:48)∈st
ηπ
i (h)ηπ−i(h)
h(cid:48)∈st
ηπ
i (s)ηπ−i(h)

i (h(cid:48))ηπ−i(h(cid:48))
ηπ

h(cid:48)∈st

ηπ−i(h(cid:48))

ηπ(ha  z)ui(z)

due to def. of st and perfect recall

ηπ−i(h)

ηπ−i(h(cid:48))

ηπ(ha  z)ui(z) =

vc
i (π  st  at).

ηπ−i(h)

1(cid:80)
i (π  st)/(cid:80)

h∈st

5

izing constant B−i(π  st) =(cid:80)

The derivation is similar to show that vπ i(st) = vc
ηπ−i(h). Hence  counterfactual
values and standard value functions are generally not equal  but are scaled by the Bayes normal-
ηπ−i(h). If there is a low probability of reaching st due to the

h∈st

environment or due to opponents’ policies  these values will differ signiﬁcantly.

h∈st

This leads to a new interpretation of actor-critic algorithms in the multiagent partially observable
setting: the advantage values qπ i(st  at) − vπ i(st  at) are immediate counterfactual regrets scaled
by 1/B−i(π  st). This then determines requirements for convergence guarantees in the tabular case.
Note that the standard policy gradient theorem holds: gradients can be estimated from samples. This
follows from the derivation of the policy gradient in the tabular case (see Appendix E). When TD
bootstrapping is not used  the Markov property is not required; having multiple agents and/or partial
observability does not change this. For a proof using REINFORCE (Gt only)  see [78  Theorem 1].
The proof trivially follows using Gt i − vπ i since vπ i is trained separately and does not depend on ρ.
s µ(s)(cid:80)
(cid:80)
actor-critic equivalent is ∇θJ AC(πθ) ∝ (cid:80)
Policy gradient algorithms perform gradient ascent on J P G(πθ) = vπθ (s0)  using ∇θJ P G(πθ) ∝
a ∇θπθ(s  a)qπ(s  a)  where µ is on-policy distribution under π [85  Section 13.2]. The
b π(s  b)qπ(s  b)).
Note that the baseline is unnecessary when summing over the actions and ∇θJ AC(πθ) =
∇θJ P G(πθ) [5]. However  our analysis relies on a projected gradient descent algorithm that does
not assume simplex constraints on the policy: in that case  in general ∇θJ AC(πθ) (cid:54)= ∇θJ P G(πθ).
Deﬁnition 1. Deﬁne policy gradient policy iteration (PGPI) as a process that iteratively runs
θ ← θ + α∇θJ P G(πθ)  and actor-critic policy iteration (ACPI) similarly using ∇θJ AC(πθ).

a ∇θπθ(s  a)(qπ(s  a) −(cid:80)

s µ(s)(cid:80)

In two-player zero-sum games  PGPI/ACPI are gradient ascent-descent problems  because each
player is trying to ascend their own score function  and when using tabular policies a solution
exists due to the minimax theorem [79]. Deﬁne player i’s external regret over K steps as RK
i =
maxπ(cid:48)

  where Πi is the set of deterministic policies.

[G0 i] − Eπk [G0 i]

(cid:16)(cid:80)K

Eπ(cid:48)

(cid:17)

i∈Πi

k=1

i

(cid:48)(cid:107)2  where ∆(S A) = {θ | ∀s ∈ S (cid:80)

Theorem 1. In two-player zero-sum games  when using tabular policies and an (cid:96)2 projection
P (θ) = argminθ(cid:48)∈∆(S A) (cid:107)θ − θ
b∈A θs b = 1} is the space
i (s)B−i(π  st) at s on iteration
of tabular simplices  if player i uses learning rates of αs k = k− 1
2 ηπk
s · ← P ({θk
J P G(πθk )}a) 
k  and θk
  where Si is the set of player i’s states  ∆r
i (s). The same holds for projected ACPI (see appendix).

has regret RK
is the reward range  and ηmin

s a > 0 for all k and s  then projected PGPI  θk+1

K − 1
i = mins k ηk

2 )|A|(∆r)2(cid:17)

|Si|(cid:16)√

i ≤ 1

s a + αs k

K + (

∂
∂θk

√

ηmin

s a

i

The proof is given in Appendix E. In the case of sampled trajectories  as long as every state is
reached with positive probability  Monte Carlo estimators of qπ i will be consistent. Therefore  we
use exploratory policies and decay exploration over time. With a ﬁnite number of samples  the
probability that an estimator ˆqπ i(s  a) differs by some quantity away from its mean is determined by
Hoeffding’s inequality and the reach probabilities. We suspect these errors could be accumulated to
derive probabilistic regret bounds similar to the off-policy Monte Carlo case [46].
What happens in the sampling case with a ﬁxed per-state learning rate αs? If player i collects a
batch of data from many sampled episodes and applies them all at once  then the effective learning
rates (expected update rate relative to the other states) is scaled by the probability of reaching s:
i (s)B−i(π  s)  which matches the value in the condition of Theorem 1. This suggests using a
ηπ
globally decaying learning rate to simulate the remaining k− 1
2 .
The analysis so far has concentrated on establishing guarantees for the optimization problem that
underlies standard formulation of policy gradient and actor-critic algorithms. A better guarantee can
be achieved by using stronger policy improvement (proof and details are found in Appendix E):
Theorem 2. Deﬁne a state-local J P G(πθ  s) = vπθ  i(s)  composite gradient { ∂
J P G(πθ  s)}s a 
strong policy gradient policy iteration (SPGPI)  and strong actor-critic policy iteration (SACPI) as
J P G(πθ  s). Then  in two-player
in Deﬁnition 1 except replacing the gradient components with
zero-sum games  when using tabular policies and projection P (θ) as deﬁned in Theorem 1 with learn-
J P G(πθ  s)}a) 
ing rates αk = k− 1
  where Si is the set of player i’s states and ∆r
has regret RK
is the reward range. This also holds for projected SACPI (see appendix).

2 )|A|(∆r)2(cid:17)

i ≤ |Si|(cid:16)√

2 on iteration k  projected SPGPI  θk+1

s · ← P ({θk

K − 1

s a + αk

K + (

∂
∂θk

√

∂θs a

∂θs a

s a

∂

6

4 Empirical Evaluation

We now assess the behavior of the actor-critic algorithms in practice. While the analyses in the
previous section established guarantees for the tabular case  ultimately we want to assess scalability
and generalization potential for larger settings. Our implementation parameterizes critics and policies
using neural networks with two fully-connected layers of 128 units each  and rectiﬁed linear unit
activation functions  followed by a linear layer to output a single value q or softmax layer to output π.
We chose these architectures to remain consistent with previous evaluations [32  50].

4.1 Domains: Kuhn and Leduc Poker

We evaluate the actor-critic algorithms on two n-player games: Kuhn poker  and Leduc poker.
Kuhn poker is a toy game where each player starts with 2 chips  antes 1 chip to play  and receives
one card face down from a deck of size n + 1 (one card remains hidden). Players proceed by betting
(raise/call) by adding their remaining chip to the pot  or passing (check/fold) until all players are
either in (contributed as all other players to the pot) or out (folded  passed after a raise). The player
with the highest-ranked card that has not folded wins the pot.
In Leduc poker  players have a limitless number of chips  and the deck has size 2(n + 1)  divided
into two suits of identically-ranked cards. There are two rounds of betting  and after the ﬁrst round a
single public card is revealed from the deck. Each player antes 1 chip to play  and the bets are limited
to two per round  and number of chips limited to 2 in the ﬁrst round  and 4 in the second round.
The rewards to each player is the number of chips they had after the game minus before the game. To
remain consistent with other baselines  we use the form of Leduc described in [50] which does not
restrict the action space  adding reward penalties if/when illegal moves are chosen.

4.2 Baseline: Neural Fictitious Self-Play

We compare to one main baseline. Neural Fictitious Self-Play (NFSP) is an implementation of
ﬁctitious play  where approximate best responses are used in place of full best response [32]. Two
transition buffers of are used: DRL and DM L; the former to train a DQN agent towards a best response
πi to ¯π−i  data in the latter is replaced using reservoir sampling  and trains ¯πi by classiﬁcation.

4.3 Main Performance Results

Here we show the empirical convergence to approximate Nash equlibria for each algorithm in self-
play  and performance against ﬁxed bots. The standard metric to use for this is NASHCONV(π)
deﬁned in Section 2.2  which reports the accuracy of the approximation to a Nash equilibrium.
Training Setup. In the domains we tested  we observed that the variance in returns was high and
hence we performed multiple policy evaluation updates (q-update for ∇QPG   ∇RPG  and ∇RMPG  and
v-update for A2C) followed by policy improvement (policy gradient update). These updates were
done using separate SGD optimizers with their respective learning rates of ﬁxed 0.001 for policy
evaluation  and annealed from a starting learning rate to 0 over 20M steps for policy improvement.
(See Appendix G for exact values). Further  the policy improvement step is applied after Nq policy
evaluation updates. We treat Nq and batch size as a hyper parameters and sweep over a few reasonable
values. In order to handle different scales of rewards in the multiple domains  we used the streaming
Z-normalization on the rewards  inspired by its use in Proximal Policy Optimization (PPO) [77]. In
addition  the agent’s policy is controlled by a(n inverse) temperature added as part of the softmax
operator. The temperature is annealed from 1 to 0 over 1M steps to ensure adequate state space
coverage. An additional entropy cost hyper-parameter is added as is standard practice with Deep RL
policy gradient methods such as A3C [59  77]. For NFSP  we used the same values presented in [50].
Convergence to Equilibrium. See Figure 2 for convergence results. Please note that we plot the
NASHCONV for the average policy in the case of NFSP  and the current policy in the case of the policy
gradient algorithms. We see that in 2-player Leduc  the actor-critic variants we tried are similar in
performance; NFSP has faster short-term convergence but long-term the actor critics are comparable.
Each converges signiﬁcantly faster than A2C. However RMPG seems to plateau.

7

NASHCONV in 2-player Kuhn

NASHCONV in 3-player Kuhn

NASHCONV in 2-player Leduc

NASHCONV in 3-player Leduc

2-player Leduc vs. CFR500

3-player Leduc vs CFR500

Figure 2: Empirical convergence rates for NASHCONV(π) and performance versus CFR agents.

Performance Against Fixed Bots. We also measure the expected reward against ﬁxed bots  averaged
over player seats. These bots  CFR500  correspond to the average policy after 500 iterations of CFR.
QPG and RPG do well here  scoring higher than A2C and even beating NFSP in the long-term.

5 Conclusion

In this paper  we discuss several update rules for actor-critic algorithms in multiagent reinforcement
learning. One key property of this class of algorithms is that they are model-free  leading to a
purely online algorithm  independent of the opponents and environment. We show a connection
between these algorithms and (counterfactual) regret minimization  leading to previously unknown
convergence properties underlying model-free MARL in zero-sum games with imperfect information.
Our experiments show that these actor-critic algorithms converge to approximate Nash equilibria in
commonly-used benchmark Poker domains with rates similar to or better than baseline model-free
algorithms for zero-sum games. However  they may be easier to implement  and do not require storing
a large memory of transitions. Furthermore  the current policy of some variants do signiﬁcantly better
than the baselines (including the average policy of NFSP) when evaluated against ﬁxed bots. Of the
actor-critic variants  RPG and QPG seem to outperform RMPG in our experiments.
As future work  we would like to formally develop the (probabilistic) guarantees of the sample-based
on-policy Monte Carlo CFR algorithms and/or extend to continuing tasks as in MDPs [41]. We are
also curious about what role the connections between actor-critic methods and CFR could play in
deriving convergence guarantees in model-free MARL for cooperative and/or potential games.
Acknowledgments. We would like to thank Martin Schmid  Audr¯unas Gruslys  Neil Burch  Noam
Brown  Kevin Waugh  Rich Sutton  and Thore Graepel for their helpful feedback and support.

8

012345678Episodes1e60.00.20.40.60.81.0NashConvNFSPA2CRPGQPGRM0.00.20.40.60.81.0Episodes1e70.00.51.01.52.02.5NashConvNFSPA2CRPGQPGRM0.00.20.40.60.81.01.21.41.6Episodes1e7012345NashConvNFSPA2CRPGQPGRM012345Episodes1e602468101214NashConvNFSPA2CRPGQPGRM0.20.40.60.81.01.21.4Episodes1e676543210Mean rewardA2CRPGQPGNFSP0.51.01.52.02.53.03.54.0Episodes1e69876543210Mean rewardA2CQPGRPGNFSPReferences

[1] Sherief Abdallah and Victor Lesser. A multiagent reinforcement learning algorithm with

non-linear dynamics. JAIR  33(1):521–549  2008.

[2] Abbas Abdolmaleki  Jost Tobias Springenberg  Yuval Tassa  Nicolas Heess Remi Munos  and
Martin Riedmiller. Maximum a posteriori policy optimisation. CoRR  abs/1806.06920  2018.

[3] Maruan Al-Shedivat  Trapit Bansal  Yuri Burda  Ilya Sutskever  Igor Mordatch  and Pieter
Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environ-
ments. In Proceedings of the Sixth International Conference on Learning Representations 
2018.

[4] Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A compre-

hensive survey and open problems. Artiﬁcial Intelligence  258:66–95  2018.

[5] Cameron Allen  Melrose Roderick Kavosh Asadi  Abdel rahman Mohamed  George Konidaris 

and Michael Littman. Mean actor critic. CoRR  abs/1709.00503  2017.

[6] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on
Foundations of Computer Science  pages 322–331  1995.

[7] Trapit Bansal  Jakub Pachocki  Szymon Sidor  Ilya Sutskever  and Igor Mordatch. Emergent
complexity via multi-agent competition. In Proceedings of the Sixth International Conference
on Learning Representations  2018.

[8] Daan Bloembergen  Karl Tuyls  Daniel Hennes  and Michael Kaisers. Evolutionary dynamics

of multi-agent learning: A survey. J. Artif. Intell. Res. (JAIR)  53:659–697  2015.

[9] A. Blum and Y. Mansour. Learning  regret minimization  and equilibria. In Algorithmic Game

Theory  chapter 4. Cambridge University Press  2007.

[10] Branislav Bošanský  Viliam Lisý  Marc Lanctot  Jiˇrí ˇCermák  and Mark H.M. Winands.
Algorithms for computing strategies in two-player simultaneous move games. Artiﬁcial
Intelligence  237:1—-40  2016.

[11] Michael Bowling. Convergence and no-regret in multiagent learning. In Advances in Neural

Information Processing Systems 17 (NIPS)  pages 209–216  2005.

[12] Michael Bowling  Neil Burch  Michael Johanson  and Oskari Tammelin. Heads-up Limit

Hold’em Poker is solved. Science  347(6218):145–149  January 2015.

[13] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate.

Artiﬁcial Intelligence  136:215–250  2002.

[14] G. W. Brown. Iterative solutions of games by ﬁctitious play. In T.C. Koopmans  editor  Activity

Analysis of Production and Allocation  pages 374–376. John Wiley & Sons  Inc.  1951.

[15] Noam Brown  Christian Kroer  and Tuomas Sandholm. Dynamic thresholding and pruning for
regret minimization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI) 
2017.

[16] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus

beats top professionals. Science  360(6385)  December 2017.

[17] L. Busoniu  R. Babuska  and B. De Schutter. A comprehensive survey of multiagent reinforce-
ment learning. IEEE Transaction on Systems  Man  and Cybernetics  Part C: Applications and
Reviews  38(2):156–172  2008.

[18] Kris Cao  Angeliki Lazaridou  Marc Lanctot  Joel Z Leibo  Karl Tuyls  and Stephen Clark.
Emergent communication through negotiation. In Proceedings of the Sixth International
Conference on Learning Representations (ICLR)  2018.

9

[19] Kamil Ciosek and Shimon Whiteson. Expected policy gradients.

In Proceedings of the

Thirty-Second AAAI conference on Artiﬁcial Intelligence (AAAI-18)  2018.

[20] Constantinos Daskalakis  Paul W. Goldberg  and Christos H. Papadimitriou. The complexity
of computing a nash equilibrium. In Proceedings of the Thirty-eighth Annual ACM Symposium
on Theory of Computing  STOC ’06  pages 71–78  New York  NY  USA  2006. ACM.

[21] Yan Duan  Xi Chen  Rein Houthooft  John Schulman  and Pieter Abbeel. Benchmarking deep

reinforcement learning for continuous control. CoRR  abs/1604.06778  2016.

[22] Lasse Espeholt  Hubert Soyer  Rémi Munos  Karen Simonyan  Volodymyr Mnih  Tom Ward 
Yotam Doron  Vlad Firoiu  Tim Harley  Iain Dunning  Shane Legg  and Koray Kavukcuoglu.
IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures.
CoRR  abs/1802.01561  2018.

[23] Jakob N. Foerster  Richard Y. Chen  Maruan Al-Shedivat  Shimon Whiteson  Pieter Abbeel 
In Proceedings of the

and Igor Mordatch. Learning with opponent-learning awareness.
International Conference on Autonomous Agents and Multiagent Systems (AAMAS)  2017.

[24] Jakob N. Foerster  Gregory Farquhar  Triantafyllos Afouras  Nantas Nardelli  and Shimon
Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence  2017.

[25] N. Gatti  F. Panozzo  and M. Restelli. Efﬁcient evolutionary dynamics with extensive-form
games. In Proceedings of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence 
pages 335–341  2013.

[26] Richard Gibson. Regret minimization in non-zero-sum games with applications to building

champion multiplayer computer poker agents. CoRR  abs/1305.0034  2013.

[27] Shixiang Gu  Ethan Holly  Timothy P. Lillicrap  and Sergey Levine. Deep reinforcement

learning for robotic manipulation. CoRR  abs/1610.00633  2016.

[28] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.

Econometrica  68(5):1127–1150  2000.

[29] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-

tion  2(3–4):157–325  2015.

[30] He He  Jordan L. Boyd-Graber  Kevin Kwok  and Hal Daumé III. Opponent modeling in deep
reinforcement learning. In Proceedings of The 33rd International Conference on Machine
Learning (ICML 2016)  2016.

[31] Johannes Heinrich  Marc Lanctot  and David Silver. Fictitious self-play in extensive-form
games. In Proceedings of the 32nd International Conference on Machine Learning (ICML
2015)  2015.

[32] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-

information games. CoRR  abs/1603.01121  2016.

[33] Pablo Hernandez-Leal  Michael Kaisers  Tim Baarslag  and Enrique Munoz de Cote. A survey
of learning in multiagent environments: Dealing with non-stationarity. CoRR  abs/1707.09183 
2017.

[34] Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge

University Press  1998.

[35] Josef Hofbauer  Sylvain Sorin  and Yannick Viossat. Time average replicator and best-reply

dynamics. Mathematics of Operations Research  34(2):263–269  2009.

[36] Zhang-Wei Hong  Shih-Yang Su  Tzu-Yun Shann  Yi-Hsiang Chang  and Chun-Yi Lee. A

deep policy inference q-network for multi-agent systems. CoRR  abs/1712.07893  2017.

10

[37] Max Jaderberg  V. Mnih  W. M. Czarnecki  T. Schaul  J. Z. Leibo  D. Silver  and
K Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In Proceed-
ings of the International Conference on Representation Learning  2017.

[38] Peter H. Jin  Sergey Levine  and Kurt Keutzer. Regret minimization for partially observable

deep reinforcement learning. CoRR  abs/1710.11424  2017.

[39] Leslie Pack Kaelbling  Michael L. Littman  and Anthony R. Cassandra. Planning and acting in

partially observable stochastic domains. Artiﬁcial Intelligence  101:99–134  1998.

[40] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Proceedings of the Nineteenth International Conference on Machine Learning  ICML ’02 
pages 267–274  San Francisco  CA  USA  2002. Morgan Kaufmann Publishers Inc.

[41] Ian A. Kash and Katja Hoffman. Combining no-regret and Q-learning. In European Workshop

on Reinforcement Learning (EWRL) 14  2018.

[42] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR 

abs/1412.6980  2014.

[43] Vojtech Kovarík and Viliam Lisý. Analysis of hannan consistent selection for monte carlo tree

search in simultaneous move games. CoRR  abs/1509.00149  2015.

[44] H. W. Kuhn. Extensive games and the problem of information. Contributions to the Theory of

Games  2:193–216  1953.

[45] Shapley L. Some topics in two-person games.

University Press.  1964.

In Advances in Game Theory. Princeton

[46] M. Lanctot  K. Waugh  M. Bowling  and M. Zinkevich. Sampling for regret minimization in
extensive games. In Advances in Neural Information Processing Systems (NIPS 2009)  pages
1078–1086  2009.

[47] Marc Lanctot. Monte Carlo Sampling and Regret Minimization for Equilibrium Computation
and Decision-Making in Large Extensive Form Games. PhD thesis  Department of Computing
Science  University of Alberta  Edmonton  Alberta  Canada  June 2013.

[48] Marc Lanctot. Further developments of extensive-form replicator dynamics using the sequence-
form representation. In Proceedings of the Thirteenth International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS)  pages 1257–1264  2014.

[49] Marc Lanctot  Kevin Waugh  Martin Zinkevich  and Michael Bowling. Monte Carlo sampling
for regret minimization in extensive games. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I.
Williams  and A. Culotta  editors  Advances in Neural Information Processing Systems 22 
pages 1078–1086  2009.

[50] Marc Lanctot  Vinicius Zambaldi  Audrunas Gruslys  Angeliki Lazaridou  Karl Tuyls  Julien
Perolat  David Silver  and Thore Graepel. A uniﬁed game-theoretic approach to multiagent
reinforcement learning. In Advances in Neural Information Processing Systems  2017.

[51] Angeliki Lazaridou  Alexander Peysakhovich  and Marco Baroni. Multi-agent cooperation
and the emergence of (natural) language. In Proceedings of the International Conference on
Learning Representations (ICLR)  April 2017.

[52] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521:436–444 

2015.

[53] Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilem-

mas using deep reinforcement learning. CoRR  abs/1707.01068  2017.

[54] Timothy P. Lillicrap  Jonathan J. Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval
Tassa  David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning.
CoRR  abs/1509.02971  2015.

11

[55] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
In Proceedings of the Eleventh International Conference on Machine Learning  pages 157–163.
Morgan Kaufmann  1994.

[56] Hao Liu  Yihao Feng  Yi Mao  Dengyong Zhou  Jian Peng  and Qiang Liu. Action-dependent
control variates for policy optimization via stein identity. In Proceedings of the International
Conference on Learning Representations (ICLR)  2018.

[57] Ryan Lowe  YI WU  Aviv Tamar  Jean Harb  OpenAI Pieter Abbeel  and Igor Mordatch. Multi-
agent actor-critic for mixed cooperative-competitive environments. In I. Guyon  U. V. Luxburg 
S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in
Neural Information Processing Systems 30  pages 6379–6390. Curran Associates  Inc.  2017.

[58] L. Matignon  G. J. Laurent  and N. Le Fort-Piat.

Independent reinforcement learners in
cooperative Markov games: a survey regarding coordination problems. The Knowledge
Engineering Review  27(01):1–31  2012.

[59] Volodymyr Mnih  Adrià Puigdomènech Badia  Mehdi Mirza  Alex Graves  Timothy P. Lilli-
crap  Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In Proceedings of the 33rd International Conference on Machine
Learning (ICML)  pages 1928–1937  2016.

[60] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G.
Bellemare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig
Petersen  Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran 
Daan Wierstra  Shane Legg  and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature  518:529–533  2015.

[61] Matej Moravˇcík  Martin Schmid  Neil Burch  Viliam Lisý  Dustin Morrill  Nolan Bard  Trevor
Davis  Kevin Waugh  Michael Johanson  and Michael Bowling. Deepstack: Expert-level
artiﬁcial intelligence in heads-up no-limit poker. Science  358(6362)  October 2017.

[62] Todd W. Neller and Marc Lanctot. An introduction to counterfactual regret minimization. In
Proceedings of Model AI Assignments  The Fourth Symposium on Educational Advances in
Artiﬁcial Intelligence (EAAI-2013)  2013. http://modelai.gettysburg.edu/2013/cfr/
index.html.

[63] Duc Thien Nguyen  Akshat Kumar  and Hoong Chuin Lau. Policy gradient with value
function approximation for collective multiagent planning.
In I. Guyon  U. V. Luxburg 
S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in
Neural Information Processing Systems 30  pages 4319–4329. Curran Associates  Inc.  2017.

[64] A. Nowé  P. Vrancx  and Y-M. De Hauwere. Game theory and multi-agent reinforcement

learning. In Reinforcement Learning: State-of-the-Art  chapter 14  pages 441–470. 2012.

[65] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.

Springer  2016.

[66] Fabio Panozzo  Nicola Gatti  and Marcello Restelli. Evolutionary dynamics of q-learning
over the sequence form. In Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial
Intelligence  pages 2034–2040  2014.

[67] Xue Bin Peng  Pieter Abbeel  Sergey Levine  and Michiel van de Panne. Deepmimic: Example-
guided deep reinforcement learning of physics-based character skills. CoRR  abs/1804.02717 
2018.

[68] Julien Perolat  Bilal Piot  and Olivier Pietquin. Actor-critic ﬁctitious play in simultaneous
move multistage games. In Amos Storkey and Fernando Perez-Cruz  editors  Proceedings of
the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics  volume 84
of Proceedings of Machine Learning Research  pages 919–928  Playa Blanca  Lanzarote 
Canary Islands  09–11 Apr 2018. PMLR.

12

[69] Julien Pérolat  Bilal Piot  Bruno Scherrer  and Olivier Pietquin. On the use of non-stationary
strategies for solving two-player zero-sum markov games. In The 19th International Confer-
ence on Artiﬁcial Intelligence and Statistics (AISTATS 2016)  2016.

[70] Julien Pérolat  Bruno Scherrer  Bilal Piot  and Olivier Pietquin. Approximate dynamic
programming for two-player zero-sum markov games. In Proceedings of the International
Conference on Machine Learning (ICML)  2015.

[71] Jan Peters. Policy gradient methods for control applications. Technical Report TR-CLMC-

2007-1  University of Southern California  2002.

[72] Yu Qian  Fang Debin  Zhang Xiaoling  Jin Chen  and Ren Qiyu. Stochastic evolution dynamic
of the rock–scissors–paper game based on a quasi birth and death process. Scientiﬁc Reports 
6(1):28585  2016.

[73] Deirdre Quillen  Eric Jang  Oﬁr Nachum  Chelsea Finn  Julian Ibarz  and Sergey Levine. Deep
reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation
of off-policy methods. CoRR  abs/1802.10264  2018.

[74] N. A. Risk and D. Szafron. Using counterfactual regret minimization to create competitive
multiplayer poker agents. In Proceedings of the International Conference on Autonomus
Agents and Multiagent Systems (AAMAS)  pages 159–166  2010.

[75] William H. Sandholm. Population Games and Evolutionary Dynamics. The MIT Press  2010.

[76] John Schulman  Sergey Levine  Philipp Moritz  Michael I. Jordan  and Pieter Abbeel. Trust

region policy optimization. CoRR  abs/1502.05477  2015.

[77] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[78] Shai Shalev-Shwartz  Shaked Shammah  and Amnon Shashua. Safe  multi-agent  reinforce-

ment learning for autonomous driving. CoRR  abs/1610.03295  2016.

[79] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic  Game-Theoretic  and

Logical Foundations. Cambridge University Press  2009.

[80] David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den
Driessche  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot 
Sander Dieleman  Dominik Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy
Lillicrap  Madeleine Leach  Koray Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mas-
tering the game of Go with deep neural networks and tree search. Nature  529:484—-489 
2016.

[81] David Silver  Thomas Hubert  Julian Schrittwieser  Ioannis Antonoglou  Matthew Lai  Arthur
Guez  Marc Lanctot  Laurent Sifre  Dharshan Kumaran  Thore Graepel  Timothy P. Lillicrap 
Karen Simonyan  and Demis Hassabis. Mastering chess and shogi by self-play with a general
reinforcement learning algorithm. CoRR  abs/1712.01815  2017.

[82] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  Yutian Chen  Timothy
Lillicrap  Fan Hui  Laurent Sifre  George van den Driessche  Thore Graepel  and Demis
Hassabis. Mastering the game of go without human knowledge. Nature  530:354–359  2017.

[83] Satinder P. Singh  Michael J. Kearns  and Yishay Mansour. Nash convergence of gradient
dynamics in general-sum games. In Proceedings of the 16th Conference on Uncertainty in
Artiﬁcial Intelligence  UAI ’00  pages 541–548  San Francisco  CA  USA  2000. Morgan
Kaufmann Publishers Inc.

[84] Sriram Srinivasan  Marc Lanctot  Vinicius Zambaldi  Julien Pérolat  Karl Tuyls  Rémi Munos 
and Michael Bowling. Actor-critic policy optimization in partially observable multiagent
environments. CoRR  abs/1810.09026  2018.

13

[85] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press  2nd edition 

2018.

[86] Richard S. Sutton  Satinder Singh  and David McAllester. Comparing policy-gradient algo-

rithms  2001. Unpublished.

[87] Oskari Tammelin  Neil Burch  Michael Johanson  and Michael Bowling. Solving heads-up
limit Texas Hold’em. In Proceedings of the 24th International Joint Conference on Artiﬁcial
Intelligence  2015.

[88] Taylor and Jonker. Evolutionarily stable strategies and game dynamics. Mathematical

Biosciences  40:145–156  1978.

[89] Karl Tuyls  Julien Perolat  Marc Lanctot  Joel Z Leibo  and Thore Graepel. A Generalised

Method for Empirical Game Theoretic Analysis . In AAMAS  2018.

[90] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical

Software  11(1):37–57.

[91] W. E. Walsh  D. C. Parkes  and R. Das. Choosing samples to compute heuristic-strategy Nash
equilibrium. In Proceedings of the Fifth Workshop on Agent-Mediated Electronic Commerce 
2003.

[92] William E Walsh  Rajarshi Das  Gerald Tesauro  and Jeffrey O Kephart. Analyzing Complex

Strategic Interactions in Multi-Agent Systems. In AAAI  2002.

[93] Kevin Waugh  Dustin Morrill  J. Andrew Bagnell  and Michael Bowling. Solving games with
functional regret estimation. In Proceedongs of the AAAI Conference on Artiﬁcial Intelligence 
2015.

[94] Michael P. Wellman. Methods for empirical game-theoretic analysis. In Proceedings  The
Twenty-First National Conference on Artiﬁcial Intelligence and the Eighteenth Innovative
Applications of Artiﬁcial Intelligence Conference  pages 1552–1556  2006.

[95] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning  8(3):229–256  1992.

[96] Cathy Wu  Aravind Rajeswaran  Yan Duan  Vikash Kumar  Alexandre M. Bayen  Sham
Kakade  Igor Mordatch  and Pieter Abbeel. Variance reduction for policy gradient with action-
dependent factorized baselines. In Proceedings of the International Conference on Learning
Representations (ICLR)  2018.

[97] Yuxin Wu and Yuandong Tian. Training agent for ﬁrst-person shooter game with actor-
critic curriculum learning. In Proceedings of the International Conference on Representation
Learning  2017.

[98] Michael Wunder  Michael Littman  and Monica Babes. Classes of multiagent q-learning
dynamics with -greedy exploration. In Proceedings of the 27th International Conference on
International Conference on Machine Learning  ICML’10  pages 1167–1174  2010.

[99] Chongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In Proceedings

of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence  pages 927–934  2010.

[100] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of Twentieth International Conference on Machine Learning (ICML-2003)  2003.
[101] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

Technical Report CMU-CS-03-110  Carnegie Mellon University  2003.

[102] M. Zinkevich  M. Johanson  M. Bowling  and C. Piccione. Regret minimization in games with
incomplete information. In Advances in Neural Information Processing Systems 20 (NIPS
2007)  2008.

[103] Martin Zinkevich  Amy Greenwald  and Michael L. Littman. Cyclic equilibria in markov
games. In Proceedings of the 18th International Conference on Neural Information Processing
Systems  NIPS’05  pages 1641–1648  Cambridge  MA  USA  2005. MIT Press.

14

,Sriram Srinivasan
Marc Lanctot
Vinicius Zambaldi
Julien Perolat
Karl Tuyls
Remi Munos
Michael Bowling