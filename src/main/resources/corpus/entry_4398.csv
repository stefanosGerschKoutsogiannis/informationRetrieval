2019,Principal Component Projection and Regression in Nearly Linear Time through Asymmetric SVRG,Given a n-by-d data matrix A  principal component projection (PCP) and principal component regression (PCR)  i.e. projection and regression restricted to the top-eigenspace of A  are fundamental problems in machine learning  optimization  and numerical analysis. In this paper we provide the first algorithms that solve these problems in nearly linear time for fixed eigenvalue distribution and large n. This improves upon previous methods which had superlinear running times when either the number of top eigenvalues or gap between the eigenspaces were large. 

We achieve our results by applying rational polynomial approximations to reduce the problem to solving asymmetric linear systems which we solve by a variant of SVRG. We corroborate these  findings with preliminary empirical experiments.,Principal Component Projection and Regression in
Nearly Linear Time through Asymmetric SVRG

Yujia Jin

Stanford Universty

yujiajin@stanford.edu

Aaron Sidford

Stanford Universty

sidford@stanford.edu

Abstract

Given a data matrix A∈Rn×d  principal component projection (PCP) and princi-
pal component regression (PCR)  i.e. projection and regression restricted to the
top-eigenspace of A  are fundamental problems in machine learning  optimization 
and numerical analysis. In this paper we provide the ﬁrst algorithms that solve
these problems in nearly linear time for ﬁxed eigenvalue distribution and large n.
This improves upon previous methods which have superlinear running times when
both the number of top eigenvalues and inverse gap between eigenspaces is large.
We achieve our results by applying rational polynomial approximations to reduce
PCP and PCR to solving asymmetric linear systems which we solve by a variant of
SVRG. We corroborate these ﬁndings with preliminary empirical experiments.

1 Introduction

PCA is one of the most fundamental algorithmic tools for analyzing large data sets. Given a data
matrix A∈Rn×d and a parameter k the classic principal component analysis (PCA) problem asks to
compute the top k eigenvectors of A(cid:62)A. This is a core computational task in machine learning and
often used for feature selection [1–3]  data visualization [4  5]  and model compression [6].
However  as k becomes large  the running time of PCA can degrade. Even just writing down the output
takes Ω(kd) time and the performance of many methods degrade with k. This high-computational
cost for exploring large-cardinality top-eigenspaces has motivated researchers to consider prominent
tasks solved by PCA  for example principal component projection (PCP) which asks to project a
vector onto the top-k eigenspace  and principal component regression (PCR) which asks to solve
regression restricted to this top-k eigenspace (see Section 1.2).
Recent work [7  8] showed that the dependence on k in solving PCP and PCR can be overcome
by instead depending on eigengap γ  deﬁned as the ratio between the smallest eigenvalue in the
space projected onto and the largest eigenvalue of the space projected orthogonal to. These works
replace the typical poly(k)nnz(A) dependence in runtime with a poly(1/γ)nnz(A) (at the cost of
lower order terms)  by reducing these problems to solving poly(1/γ) ridge-regression problems.
Unfortunately  for large-scale problems  as data-set sizes grow so too can k and 1/γ  yielding large
super-linear running times for all previously known methods (see Section 1.4). Consequently  this
leaves the following fundamental open problem:

Can we obtain nearly linear running times for solving PCP and PCR to high precision  i.e.
with running time ˜O(nnz(A)) plus an additive term depending only on the eigenvalue distribution?

The main contribution of the paper is an afﬁrmative answer to this question. We design randomized
algorithms that solve PCP and PCR with high probability in nearly linear time. Leveraging rational
polynomial approximations we reduce these problems to solving asymmetric linear systems  which

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

we solve by a technique we call asymmetric SVRG. Further  we provide experiments demonstrating
the efﬁcacy of this method.

1.1 Approach

To obtain our results  critically we depart from the previous frameworks in Frostig et al. [7]  Allen-
Zhu and Li [8] for solving PCP and PCR. These papers use polynomial approximations to the sign
function to reduce PCP and PCR to solving ridge regression. Their runtime is limited by the necessary
Ω(1/γ) degree for polynomial approximation of the sign function shown by Eremenko and Yuditskii
[9]. Consequently  to obtain nearly linear runtime  a new insight is required.
In our paper  we instead consider rational approximations to the sign function and show that these
efﬁciently reduce PCP and PCR to solving a particular class of squared linear systems. The closed
form expression for the best rational approximation to sign function was given by Zolotarev [10] and
has recently been proposed for matrix sign approximation [11]. The degree of such rational functions
is logarithmic in 1/γ  leading to much fewer linear systems to solve. While the squared systems
[(A(cid:62)A−cI)2 +µ2I]x = b µ > 0 induced by this rational approximation are computationally more

expensive to solve  as compared with simple ridge regression problems(cid:2)A(cid:62)A+µI(cid:3)x = b µ > 0 

interestingly  we show that these systems can still be solved in nearly linear time for sufﬁciently large
matrices. As a by-product of this analysis  we also obtain an efﬁcient algorithm for leveraging linear
system solvers to apply the square-root of a positive semideﬁnite (PSD) matrix to a vector  where we
call a matrix M positive semideﬁnite and denote M(cid:23) 0 when ∀x x(cid:62)Mx≥ 0.
We believe the solver we develop for these squared systems is of independent interest. Our solver is a
variant of the stochastic variance-reduced gradient descent algorithm (SVRG) [12] modiﬁed to solve
asymmetric linear systems. Our iterative method can be viewed as an instance of the variance-reduced
algorithm for monotone operators discussed in Section 6 of Palaniappan and Bach [13]  with a more
careful analysis of the error. We also combine this method with approximate proximal point [14] or
Catalyst [15] to obtain accelerated variants.
The conventional wisdom when solving asymmetric systems Mx = b that are not positive semideﬁnite
(PSD)  i.e. M(cid:15) 0  is to instead solve its PSD counterpart M(cid:62)Mx = M(cid:62)b. However  this operation
can greatly impair the performance of stochastic methods  e.g. SVRG [12]  SAG [16]  etc. (See
Section 3.) The solver developed in this paper constitutes one of few known cases where transforming
it into asymmetric system solving enables better algorithm design and thus provides large savings
(see Corollary 1.) Ultimately  we believe this work on SVRG-based methods outside of convex
optimization as well as our improved PCP and PCR algorithms may ﬁnd further impact.

1.2 The Problems

Here we formally deﬁne the PCP (Deﬁnition 1)  PCR (Deﬁnition 2)  Squared Ridge Regression
(Deﬁnition 3)  and Square-root Computation (Deﬁnition 4) problems we consider throughout this
paper. Throughout  we let A∈Rn×d (n≥ d) denote a data matrix where each row ai∈Rn is viewed
as a datapoint. Our algorithms typically manipulate the positive semideﬁnite (PSD) matrix A(cid:62)A.
We denote the eigenvalues of A(cid:62)A as λ1 ≥ λ2 ≥ ··· ≥ λd ≥ 0 and corresponding eigenvectors as
ν1 ν2 ··· νd∈Rd  i.e. A(cid:62)A = VΛV(cid:62) with V def= (ν1 ··· νd)(cid:62) and Λ def= diag(λ1 ··· λd).
Given eigenvalue threshold λ∈ (0 λ1) we deﬁne Pλ
def= (ν1 ··· νk)(ν1 ··· νk)(cid:62) as a projection matrix
projecting any vector onto the top-k eigenvectors of A(cid:62)A  i.e. span{ν1 ν2 ··· νk}  where λk is the
minimum eigenvalue of A(cid:62)A no smaller than λ  i.e. λk ≥ λ > λk+1. Without speciﬁcation (cid:107)·(cid:107) is the
standard (cid:96)2-norm of vector or matrix.

Given γ ∈ (0 1)  the goal of a PCP algorithm is to project any given vector v =(cid:80)

i∈[d] αiνi in a
desired way: mapping νi of A(cid:62)A with eigenvalue λi in [λ(1+γ) ∞) to itself  eigenvector νi with
eigenvalue λi in [0 λ(1−γ)] to 0  and any eigenvector νi with eigenvalue λi in between the gap to
anywhere between 0 and νi. Formally  we deﬁne the PCP as follows.
Deﬁnition 1 (Principal Component Projection). The principal component projection (PCP) of v∈Rd
λ = Pλv. Given threshold λ and eigengap γ  an algorithm APCP(v  δ) is an
at threshold λ is v∗

2

-approximate PCP algorithm if with probability 1−δ  its output satisﬁes following:

•(cid:107)P(1+γ)λ(APCP(v)−v)(cid:107)≤ (cid:107)v(cid:107); and
•(cid:107)(P(1+γ)−P(1−γ)λ)(APCP(v)−v)(cid:107)≤(cid:107)(P(1+γ)−P(1−γ)λ)v(cid:107)+(cid:107)v(cid:107)

(cid:107)(I−P(1−γ)λ)APCP(v)(cid:107)≤ (cid:107)v(cid:107)

(1)

The goal of a PCR problem is to solve regression restricted to the particular eigenspace we are
projecting onto in PCP. The resulting solution should have no correlation with eigenvectors νi
corresponding to λi ≤ λ(1− γ)  while being accurate for νi corresponding to eigenvalues above
λi ≥ λ(1 + γ). Also  it shouldn’t have too large correlation with νi corresponding to eigenvalues
between (λ(1−γ) λ(1+γ)). Formally  we deﬁne the PCR problem as follows.
Deﬁnition 2 (Principal Component Regression). The principal component regression (PCR) of an
arbitrary vector b ∈ Rn at threshold λ is x∗
λ = minx∈Rd (cid:107)APλx − b(cid:107). Given threshold λ and
eigengap γ  an algorithm APCR(b  δ) is an -approximate PCR algorithm if with probability 1−δ 
its output satisﬁes following:

(cid:107)(I−P(1−γ)λ)APCR(b )(cid:107)≤ (cid:107)b(cid:107) and (cid:107)AAPCR(b )−b(cid:107)≤(cid:107)Ax∗

(1+γ)λ−b(cid:107)+(cid:107)b(cid:107) .

(2)

We reduce PCP and PCR to solving squared linear systems. The solvers we develop for this squared
regression problem deﬁned below we believe are of independent interest.
Deﬁnition 3 (Squared Ridge Regression Solver). Given c∈ [0 λ1]  v∈Rd  we consider a squared
ridge regression problem where exact solution is x∗ = ((A(cid:62)A − cI)2 + µ2I)−1v. We call an
algorithm RidgeSquare(A c µ2 v  δ) an -approximate squared ridge regression solver if with
probability 1−δ it returns a solution ˜x satisfying (cid:107)˜x−x∗(cid:107)≤ (cid:107)v(cid:107).

Using a similar idea of rational polynomial approximation  we also examine the problem of M1/2v
for arbitrarily given PSD matrix M to solving PSD linear systems approximately.
Deﬁnition 4 (Square-root Computation). Given a PSD matrix M∈ Rn×n such that µI(cid:22) M(cid:22) λI
and v ∈ Rn  an algorithm SquareRoot(M v  δ) is an -approximate square-root solver if with
probability 1−δ it returns a solution x satisfying (cid:107)x−M1/2v(cid:107)≤ (cid:107)M1/2v(cid:107).
1.3 Our Results

F/(cid:107)A(cid:107)2

2 =(cid:107)A(cid:107)2

Here we present the main results of our paper  all proved in Appendix D. For data matrix
A ∈ Rn×d  our running times are presented in terms of the following quantities: Input spar-
sity nnz(A) def= number of nonzero entries in A; Frobenius norm (cid:107)A(cid:107)2
def= Tr(A(cid:62)A); stable rank
sr(A) def= (cid:107)A(cid:107)2
F/λ1; condition number of top-eigenspace: κ def= λ1/λ. When present-
ing running times we use ˜O to hide polylogarithmic factors in the input parameters λ1 γ v b  error
rates   and success probability δ.
For A∈ Rn×d (n≥ d)  v∈ Rd  b∈ Rn  without loss of generality we assume λ1 ∈ [1/2 1]1. Given
threshold λ∈ (0 λ1) and eigengap γ∈ (0 2/3]  the main results of this paper are the following new
running times for solving these problems.
Theorem 1 (Principal Component Projection). For any ∈ (0 1)  there is an -approximate PCP
algorithm (see Deﬁnition 1) ISPCP(A v λ γ  δ) speciﬁed in Algorithm 5 with runtime

F

Theorem 2 (Principal Component Regression). For any ∈ (0 1)  there is an -approximate PCR
algorithm (see Deﬁnition 2) ISPCR(A b λ γ  δ) speciﬁed in Algorithm 6 with runtime

(cid:16)
(cid:16)

˜O

˜O

nnz(A)+(cid:112)nnz(A)·d·sr(A)κ/γ
nnz(A)+(cid:112)nnz(A)·d·sr(A)κ/γ

(cid:17)
(cid:17)

.

.

We achieve these results by introducing a technique we call asymmetric SVRG to solve squared
systems [(A(cid:62)A−cI)2 +µ2I]x = v with c∈ [0 λ1]. The resulting algorithm is closely related to the
SVRG algorithm for monotone operators in Palaniappan and Bach [13]  but involves a more ﬁne-
grained error analysis. This analysis coupled with approximate proximal point [14] or Catalyst [15]
yields the following result (see Section 3 for more details).

1This can be achieved by getting a constant approximating overestimate ˜λ1 of A(cid:62)A’s top eigenvector λ1

through power method in ˜O(nnz(A)) time  and consider A← A/

(cid:112)˜λ1 λ← λ/˜λ1 b← b/

(cid:112)˜λ1 instead.

3

Theorem 3 (Squared Solver). For any ∈ (0 1)  there is an -approximate squared ridge regression
solver (see Deﬁnition 3) using AsySVRG(M ˆv z0 (cid:107)v(cid:107) δ) that runs in time

(cid:16)

˜O

nnz(A)+(cid:112)nnz(A)d·sr(A)λ1/µ

(cid:17)

.

When the eigenvalues of A(cid:62)A−cI are bounded away from 0  such a solver can be utilized to solve
non-PSD linear systems in form (A(cid:62)A− cI)x = v through preconditioning and considering the
corresponding problem (A(cid:62)A−cI)2x = (A(cid:62)A−cI)v (see Corollary 1).
Corollary 1. Given c ∈ [0  λ1]  and a non-PSD system (A(cid:62)A − cI)x = v and an initial
point x0  for arbitrary c satisfying (A(cid:62)A − cI)2 (cid:23) µ2I  µ > 0  there is an algorithm returns

with probability 1 − δ a solution (cid:101)x such that (cid:107)(cid:101)x − (A(cid:62)A − cI)−1v(cid:107) ≤ (cid:107)v(cid:107)  within runtime
˜O(cid:0)nnz(A)+(cid:112)nnz(A)d·sr(A)λ1/µ(cid:1).

Another byproduct of the rational approximation used in the paper is a nearly-linear runtime for
computing an -approximate square-root of PSD matrix M(cid:23) 0 applied to an arbitrary vector.
Theorem 4 (Square-root Computation). For any  ∈ (0  1)  given µI (cid:22) M (cid:22) λI  there is an -
approximate square-root solver (see Deﬁnition 4) SquareRoot(M v  δ) that runs in time

˜O(nnz(M)+T )

where T is the runtime for solving (M+κI)x = v for arbitrary v∈Rn and κ∈ [ ˜Ω(µ)  ˜O(λ)].
1.4 Comparison to Previous Work

PCP and PCR: The starting point for our paper is the work of [7]  which provided the ﬁrst nearly
linear time algorithm for the problem with constant eigengap by reducing the problem to ﬁnding
the best polynomial approximation to sign function and solving a sequence of regression problems.
It was improved by [8] and then [17]. These algorithms were ﬁrst to achieve input sparsity for
eigenspaces of any non-trivial size  but with super-linear running times whenever the eigenvalue-gap
is super-constant. Departing from their polynomial approximation  we use rational function as
approximant and reduce to different subproblems to get new algorithms with better running time
guarantee in some regime. See Table 1 for a comparison between those results and ours.

Table 1: Comparison with previous PCP/PCR runtimes. (Notations same as in Section 1.3.)

Algorithm
FMMS16 [7]

AL17 [8]  MMS18 [17]

Theorems 1 and 2

(cid:16)

˜O

Runtime

(cid:16) 1
(cid:17)
(cid:16) 1
(cid:17)
γ2 (nnz(A)+d·sr(A)κ)
nnz(A)+(cid:112)nnz(A)·d·sr(A)κ/γ
γ (nnz(A)+d·sr(A)κ)

˜O
˜O

(cid:17)

Asymmetric SVRG and Iterative Methods for Solving Linear Systems: Variance reduction or
varianced reduced iterative methods (e.g. SVRG [12] is a powerful tool for improving convergence
of stochastic methods. There has been work that used SVRG to develop primal-dual algorithms
for solving saddle-point problems and extended it to monotone operators [13]. Our asymmetric
SVRG solver can be viewed as an instance of their algorithm. We obtain improved running time
analysis by performing a more ﬁne-grained analysis exploiting problem structure. Especially  we
provide Section 1.4 to comparing the effectiveness of our asymmetric SVRG solver with some classic
optimization methods for solving non-PSD system (A(cid:62)A− cI)x = v satisfying (A(cid:62)A− cI)2 (cid:23)
µ2I µ > 0 (full discussion in Section 3 and Appendix C.4).

Table 2: Comparison for runtimes of solving non-PSD system (A(cid:62)A−cI)x = v.

Algorithm

AGD applied to squared counterpart
SVRG applied to squared counterpart

Asymmetric SVRG (Corollary 1)

Runtime

˜O(nnz(A)λ1/µ)

˜O(nnz(A)+nnz(A)3/4d1/4sr(A)1/2λ1/µ)

˜O(cid:0)nnz(A)+(cid:112)nnz(A)d·sr(A)λ1/µ(cid:1)

4

Fast Matrix Multiplication: One can also use fast-matrix multiplication (FMM) to possibly speed
up all runtimes for PCA  PCR  and PCP  mainly by computing A(cid:62)A in O(ndω) time and SVD
of this matrix in an additional O(dω) time [18] where ω < 2.379 [19] is the matrix multiplication
constant. Given the well-known practicality concerns of methods using fast matrix multiplication  we
focus much of our comparison on methods that do not use FMM.

1.5 Paper Organization

The remainder of the paper is organized as follows. In Section 2  we reduce the PCP problem2 to
matrix sign approximation and study the property of Zolotarev rational function used in approximation.
In Section 3  we develop the asymmetric and squared linear system solvers using variance reduction
and show the theoretical guarantee to prove Theorem 3  and correspondingly Corollary 1. In Section 4 
we conduct experiments and compare with previous methods to show efﬁcacy of proposed algorithms.
We conclude the paper in Section 5.

2 PCP through Matrix Sign Approximation

Here we provide our reductions from PCP to sign function approximation. We consider the rational
approximation r(x) found by Zolotarev [10] and study its properties for efﬁcient (Theorem 5) and
stable (Lemma 5) algorithm design to reduce the problem to solving squared ridge regressions.
Throughout the section  we denote sign function as sgn(x) : R → R  where sgn(x) = 1 whenever
x > 0  sgn(x) =−1 whenever x < 0  and sgn(0) = 0. Pk
def= {akxk +··· + a1x + a0|ak (cid:54)= 0} denote
def= {rm n|rm n = pm/qn pm ∈Pm  qn ∈Pn} denote class of
class of degree-k polynomials. Rm n
(m n)-degree (or referred to as max{m n}-degree) rational functions.
For the PCP problem (see Deﬁnition 1)  we need an efﬁcient algorithm that can approximately apply
Pλ to any given vector v∈ Rd. Consider the shifted matrix A(cid:62)A−λI so that its eigenvalues are
shifted to [−1 1] with λ mapping to 0. Previous work has shown [7  8] solving PCP can be reduced
to ﬁnding f (x) that approximates sign function sgn(x) on [−1 1]  formally through the following
reduction.
Lemma 1 (Reduction: from PCP to Matrix Sign Approximation). Given a function f (x) that
2-approximates sgn(x):

then(cid:101)v = 1

2

|f (x)−sgn(x)|≤ 2 ∀|x|∈ [λγ 1] and

(cid:0)f (A(cid:62)A−λI)+I(cid:1)v is an -approximate PCP solution satisfying (1).

|f (x)|≤ 1 ∀x∈ [−1 1] 

However  instead of approximating sgn(x) with polynomials as in previous work [7  8]  where the
optimal degree for achieving condition |f (x)−sgn(x)|≤ 2 ∀|x|∈ [γ 1] is proved to be ˜O(1/γ) in [9] 
we use Zolotarev rational function for approximation. This brings down the degree to ˜O(log(1/λγ)) 
leading to the nearly input sparsity runtime improvement in the paper.
k (x) = x·p(x2)/q(x2)∈R2k+1 2k
Formally  Zolotarev rationals are deﬁned as the optimal solution rγ
for the optimization problem:

max
p q∈Pk

min
γ≤x≤1

x

p(x2)
q(x2)

s.t.

x

p(x2)
q(x2)

≤ 1 ∀x∈ [0 1]

Zolotarev [10] showed this optimization problem (up to scaling) is equivalent to solving

min

r∈R2k+1 2k

max
|x|∈[γ 1]

|sgn(x)−r(x)| .

(3)

(4)

(5)

Further Zolotarev [11] showed that the analytic formula of rγ

(cid:89)

i∈[k]

rγ
k (x) = Cx

x2 +c2i
x2 +c2i−1

with ci

k is given by
2k+1 ;γ(cid:48))
2k+1 ;γ(cid:48))

def= γ2 sn2( iK(cid:48)
cn2( iK(cid:48)
k (γ) =−(1−rγ

 i∈ [2k].

and C is the rescaling parameter to make sure 1−rγ
k (1)). Note all coefﬁcients are
dependent of degree k and range γ. The explicit formulas for ci K(cid:48) γ(cid:48) are shown in Appendix B.1.

2We refer reader to Appendix D.2 for the known reduction from PCR to PCP.

5

(a) γ=0.1

(b) γ=0.05

(c) γ=0.01

Figure 1: same degree = 21  different γ

This rational polynomial approximates sgn(x) on range |x|∈ [γ 1] with error decaying exponentially
with degree  as formally characterized by the following theorem.
Theorem 5 (Rational Approximation Error). For any given ∈ (0 1)  when k≥ Ω(log(1/)log(1/γ)) 
it holds that max|x|∈[γ 1]|sgn(x)−rγ
As a quick illustration  Fig. 1 shows a comparison between the approximation errors of Zolotarev
rational function  polynomial used in [7] and chebyshev polynomial used in [8] with same degree.
k with k≥ Ω(log(1/)log(1/λγ)) as the desired f in Lemma 1  it sufﬁces to compute
Treating rλγ

k (x)|≤ 2.

k ((A(cid:62)A−λI))v = C(A(cid:62)A−λI)
rλγ

(A(cid:62)A−λI)2 +c2iI
(A(cid:62)A−λI)2 +c2i−1I

v.

k(cid:89)

i=1

To compute this formula approximately  we need to solve squared linear systems of the form
((A(cid:62)A−λI)2 +c2i−1I)x = v  the hardness of which is determined by the size of c2i−1(> 0). The
larger c2i−1 is  the more positive-deﬁnite (PD) the system becomes  and the faster we can solve it.
k we need to use has coefﬁcients ci = ˜Ω(1/λ2γ2) when
The following lemma shows that  the rλγ
k = Θ(log(1/)log(1/λγ)).
k   coefﬁcients ci are nondecreasing in i  ∀i∈ [2k]. Also  ∃ some
Lemma 2 (Bounding ci). For rλγ
constant 0 < β2 β3 <∞  such that c1≥ β2
Given a squared ridge regression solver RidgeSquare(A λ c2i−1 v  δ) (See Section 3)  we can
get an -approximate PCP algorithm ISPCP(A v λ γ  δ) shown in Algorithm 5 and its theoretical
guarantee in Theorem 1. Using the known reduction [7  8] from PCP to PCR solver  this also gives
results in Theorem 2. We refer readers to Appendix D for parameter choice and corresponding proofs.

k2   c2k ≤ β3k2.

λ2γ2

Algorithm 1: ISPCP(A v λ γ  δ)
Input: A data matrix  v projecting vector  λ threshold  γ eigengap   accuracy  δ probability.
Parameter: degree k (Theorem 5)  coefﬁcients {ci}2k
1 for i← 1 to k do
2
3

Output: A vector(cid:101)v that solves PCP -approximately.
(cid:101)v← (A(cid:62)A−λI)2(cid:101)v+c2i(cid:101)v
(cid:101)v← RidgeSquare(A λ c2i−1 (cid:101)v 1 δ/k)
4 (cid:101)v← C(A(cid:62)A−λI)(cid:101)v
2 (v+(cid:101)v)
5 (cid:101)v← 1

i=1 C (Eqn. (5))  accuracy 1 (Appendix D)

3 SVRG for Solving Asymmetric / Squared Systems

In this section  we reduce solving squared systems into solving asymmetric systems (Lemma 3) and
develop SVRG-type solvers (Algorithm 2) for them. We study its theoretical guarantees in both
general (Theorems 6 and 7) and our speciﬁc case (Theorem 8). We defer all proofs to Appendix C.
In Section 2  we get low-degree rational function approximation at the cost of more complicated sub-
problems to solve. Indeed  instead of solving ridge-regression-type subproblems (A(cid:62)A+λI)x = v

6

-1-0.500.51-101polynomialchebyshevrational-1-0.500.51-101polynomialchebyshevrational-1-0.500.51-101polynomialchebyshevrationalas in previous work [7  8]  we need to solve squared systems in the following form:
[(A(cid:62)A−cI)2 +µ2I]x = v  with A∈Rn×d v∈Rd µ > 0 c∈ [0 λ1].

(6)
When the squared system is ill-conditioned (i.e. when λ1/µ(cid:29) 0)  previous state-of-the-art methods
can have fairly large running times. As shown in Section 1.4 and proved in Appendix C.4  Accel-

erated Gradient Descent [20] applied to solving system(cid:0)(A(cid:62)A−cI)2 +µ2I(cid:1)x = v gives a runtime

˜O(nnz(A)λ1/µ)  which is not nearly linear in nnz(A). Applying the standard SVRG [12] technique
to the same system leads to a runtime ˜O(nnz(A) + d· sr2(A)λ4
1/µ4 comes
from the high variance in sampling aia(cid:62)
Thus rather than working with the squared system directly  we propose to consider (equivalently) a
larger dimensional space where we develop estimators with lower variance at the cost of asymmetry 
formally in the reduction below.
Lemma 3 (Reducing Squared Systems to Asymmetric Systems). Deﬁne z∗ as the solution to the
following asymmetric linear system:

j from (A(cid:62)A)2 independently.

1/µ4)  where sr2(A)λ4

i aja(cid:62)

I

− 1
µ (A(cid:62)A−cI)

(cid:19)

(cid:18) 0

(cid:19)

1

µ (A(cid:62)A−cI)

If we are given a solver that returns with probability 1−δ a solution(cid:101)z satisfying (cid:107)(cid:101)z−z∗(cid:107)2≤  within

runtime T ( δ)  then we can use it to get an -approximate squared ridge regression solver (see
Deﬁnition 3) with runtime T ((cid:107)v(cid:107) δ) .
3.1 SVRG for General Asymmetric Linear System Solving

v/µ2

z =

I

.

(7)

(cid:18)

The general goal for this section is to solve the general asymmetric system with PSD symmetric part 
formally deﬁned as:

solve Mz = ˆv with ˆv∈Ra  M∈Ra×a  M =

Mi  (cid:107)Mi(cid:107)≤ Li 

(M(cid:62) +M).(cid:23) µI

(8)

1
2

(cid:88)

i∈[n]

For simplicity  we denote Tmv(Mi) as the cost of the matrix-vector product of Mix for any x and
T = maxi∈[n]Tmv(Mi). All results in this subsection can be viewed as a variant of Palaniappan and
Bach [13] and can be recovered by their slightly different algorithm which used proximal methods.
Using the idea of variance-reduced sampling [12]: At step t  we sample it ∈ [n with probability

pit = Lit/((cid:80)

i∈[n]Li) independently and conduct update

(cid:0)Mitzt−Mitz0 +pit(Mz0− ˆv)(cid:1).

zt+1 := zt− η
pit

(9)

Algorithm 2: AsySVRG(M ˆv z0  δ)
Input: M∈Ra×a  ˆv∈Ra  z0∈Ra  desired accuracy  δ probability parameter.
Output: z(Q+1)

i∈[n]Li)2/µ2(cid:101)  pi = Li/((cid:80)

i∈[n]Li)2 T =(cid:100)((cid:80)

1 Set η = µ/4((cid:80)

∈Ra.

0

i∈[n]Li) i∈ [n] unless speciﬁed

2 for q = 1 to Q = Θ(log(1/δ)) do
3
4

for t← 1 to T do

Sample it∼ [n] according to {pi}n
(cid:80)T
t−1−η/pit(Mitz(q)
t ← z(q)
z(q)
z(q+1)
t=1z(q)
= 1
0
T

5

6

t

i=1

t−1−Mitz(q)

0 +pit(Mz(q)

0 − ˆv))

The full pseudocode is shown in Algorithm 2. It has the following theoretical guarantee.
Theorem 6 (General Asymmetric SVRG Solver). For asymmetric system Mz = ˆv in (8)  there is a
solver AsySVRG(M ˆv z0  δ) as speciﬁed in Algorithm 2 that returns with high probability ≥ 1−δ a

vector(cid:101)z such that (cid:107)(cid:101)z−M−1 ˆv(cid:107)≤   within runtime ˜O(nnz(M)+T ((cid:80)
Using approximate proximal point [14] or Catalyst [15]  when nnz(M)≤T ((cid:80)

i∈[n]Li)2/µ2).

i∈[n]Li)2/µ2  we can

further improve this running time to the following:

7

Theorem 7 (Accelerated Asymmetric SVRG Solver). Under (8)  when nnz(M)≤T ((cid:80)
solution(cid:101)z satisfying (cid:107)(cid:101)z−M−1 ˆv(cid:107)≤   within runtime ˜O(cid:0)(cid:112)nnz(M)T ((cid:80)
i∈[n]Li)/µ(cid:1).

i∈[n]Li)2/µ2 
the algorithm can be further accelerated to return with high probability ≥ 1− δ an approximate

3.2 Asymmetric Linear System Solving for Squared System Solver

From Lemma 3  the asymmetric linear system we actually need to solve is Mz = ˆv  where

(cid:19)
µ (A(cid:62)A−cI)
− 1

I

and ˆv =(cid:0)0 v/µ2(cid:1)(cid:62)

.

(10)

M =

I

1

µ (A(cid:62)A−cI)

(cid:18)



Through a more ﬁne-grained analysis shown in Appendix C.2  AsySVRG(M ˆv z0  δ) with particular
choices of Mi {pi}i∈[n]  η  T can have a better runtime guarantee and be accelerated using similar
idea as in the general case. This is stated formally in the following theorem.
F  η = µ2/2λ1(cid:107)A(cid:107)2
Theorem 8 (Particular Asymmetric SVRG Solver). Set pi = (cid:107)ai(cid:107)2/(cid:107)A(cid:107)2
F 
T =(cid:100)2(cid:107)A(cid:107)2

Fλ1/µ2(cid:101) and

Mi :=

(cid:107)ai(cid:107)2
(cid:107)A(cid:107)2
i −c

F

1

µ (aia(cid:62)

I
(cid:107)ai(cid:107)2
(cid:107)A(cid:107)2

F

)I

(cid:107)ai(cid:107)2
i −c
− 1
µ (aia(cid:62)
(cid:107)A(cid:107)2
(cid:107)ai(cid:107)2
I
(cid:107)A(cid:107)2

F

I)

F

Then AsySVRG(M  ˆv  z0    δ) as speciﬁed in Algorithm 2 returns with probability ≥ 1 − δ an -

approximate solution(cid:101)z satisfying (cid:107)(cid:101)z− M−1 ˆv(cid:107) ≤  within runtime ˜O(cid:0)nnz(A) + d· sr(A)λ2
An accelerated variant of it improves the runtime to ˜O(cid:0)λ1
RidgeSquare(A  λ  c2i−1  v    δ)  with worst-case runtime (cid:101)O(cid:0)nnz(A) + d · sr(A) κ2

1/µ2(cid:1).
(cid:112)nnz(A)d·sr(A)/µ(cid:1) when nnz(A) ≤
(cid:1). (See Ap-

Picking c = λ  µ2 = c2i−1 = ˜Ω(1/λ2γ2) (see Lemma 5) in (10)  we know under minimal
trasformations AsySVRG(M  ˆv  z0  (cid:107)v(cid:107)  δ) is equivalent to an -approximate squared solver

d·sr(A)λ2

1/µ2.

γ2

pendix C.3 and Algorithm 4 for details.)

  ∀i∈ [n].

4 Numerical Experiments

We evaluate our proposed algorithms following the settings in Frostig et al. [7]  Allen-Zhu and Li
[8]. As the runtimes in Theorems 1 and 2 show improvement compared with the ones in previous
work [7  8] when nnz(A)/γ(cid:29) d2κ2/γ2  we pick the data matrix A such that κ = Θ(1) and n(cid:29) d
γ to
corroborate the theoretical results.
Since experiments in several papers [7  8] have studied the reduction from PCR to PCP (see
Lemma 13)  here we only show results regarding solving PCP problems. In all ﬁgures below 
the y-axis denotes the relative error measured in (cid:107)APCP(v)−Pλv(cid:107)/(cid:107)Pλv(cid:107) and x-axis denotes the
total number of vector-vector products to achieve corresponding accuracy.

√

Datasets. Similar to that in previous work [7  8]  we set λ = 0.5 n = 2000 d = 50 and form a
matrix A = UΛ1/2V∈R2000×50. Here  U and V are random orthonormal matrices  and Σ contains
λi. Referring to [0 λ(1−γ)]∪[λ(1+γ) 1] as the away-from-
randomly chosen singular values σi =
λ region  and λ(1−γ)·[0.9 1]∪λ(1+γ)·[1 1.1] as the close-to-λ region  we generate λi differently
to simulate the following three different cases:
i. Eigengap-Uniform Case: generate all λi uniformly in the away-from-λ region.
ii. Eigengap-Skewed Case: generate half the λi uniformly in the away-from-λ and half uniformly in
the close-to-λ regions.
iii. No-Eigengap-Skewed Case: uniformly generate half in [0 1]  and half in the close-to-λ region.

Algorithms. We implemented the following algorithms and compared them in the above settings:
1. polynomial: the PC−Proj algorithm in Frostig et al. [7].
2. chebyshev: the QuickPCP algorithm in Allen-Zhu and Li [8].
3. lanczos: the algorithm using Lanczos method discussed in Section 8.1 of Musco et al. [17].
4. rational: the ISPCP algorithm (see Algorithm 5) proposed in our paper.
5. rlanczos: the algorithm using rational Lanczos method [21] combined with ISPCP. (See Ap-
pendix E.1 for a more detailed discussion.)

8

(a) Eigengap-Uniform Case

(b) Eigengap-Skewed Case

(c) No-Eigengap-Skewed Case

Figure 2: Synthetic Data: n = 2000  d = 50  λ = 0.5  γ = 0.05.

(a) n=5000

(b) n=10000

(c) n=20000

Figure 3: Synthetic Data: Changing n  d = 50  λ = 0.5  γ = 0.05. No-Eigengap-Skewed Case.

6. slanczos: the algorithm using Lanczos method [17] with changed search space from form f ( x−λ
x+λ )
into f (

(x−λ)(x+λ)
(x−λ)2+γ(x+λ)2 ) for approximation (f polynomial  x← A(cid:62)A).

We remark that 1-3 are algorithms in previous work; 4 is an exact implementation of ISPCP proposed
in the paper; 5  6 are variants of ISPCP combined with Lanczos method  both using the squared
system solver. Algorithms 5  6 are explained in greater detail in Appendix E.
There are several observations from the experiments:
• For different eigenvalue distributions  (4-6) in general outperform all existing methods (1-3) in
most accuracy regime in terms of number of vector products as shown in Fig. 2.
• In no-eigengap case  all methods get affected in precision. This is due to the projection error of
eigenvalues very close to eigengap  which simple don’t exist in Eigengap cases. Nevertheless  (6) is
still the most accurate one with least computation cost  as shown in Fig. 2.
• When n gets larger  (4 5) tends to enjoy similar performance  outperforming all other methods
including (6)  as shown in Fig. 3. This aligns with theory that runtime of (4 5) is dominated by
nnz(A) while runtime of (6) is dominated by nnz(A)/
γ (see Theorem 12 for theoretical analysis
of slanczos)  demonstrating the power of nearly-linear runtime of ISPCP proposed.

√

5 Conclusion

In this paper we provided a new linear algebraic primitive  asymmetric SVRG for solving squared
ridge regression problems  and showed that it lead to nearly-linear-time algorithms for PCP and PCR.
Beyond the direct running time improvements  this work shows that running time improvements
can be achieved for fundamental linear-algebraic problems by leveraging stronger subroutines than
standard ridge regression. The improvements we obtain for PCP  demonstrated theoretically and
empirically  we hope are just the ﬁrst instance of a more general approach for improving the running
time for solving large-scale machine learning problems.

Acknowledgements This research was partially supported by NSF CAREER Award CCF-1844855
and Stanford Graduate Fellowship. We would also like to thank the anonymous reviewers who helped
improve the completeness and readability of this paper by providing many helpful comments.

9

100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczos100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczos100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczos100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczos100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczos100105# vector product / n10-410-310-210-1100relative errorpolynomialchebyshevlanzcosrationalrlanczosslanczosReferences

[1] Arnaz Malhi and Robert X Gao. Pca-based feature selection scheme for machine defect classiﬁcation.

IEEE Transactions on Instrumentation and Measurement  53(6):1517–1525  2004.

[2] Fengxi Song  Zhongwei Guo  and Dayong Mei. Feature selection using principal component analysis. In
2010 international conference on system science  engineering design and manufacturing informatization 
volume 1  pages 27–30. IEEE  2010.

[3] Cláudia Pascoal  M Rosario De Oliveira  Rui Valadas  Peter Filzmoser  Paulo Salvador  and António
Pacheco. Robust feature selection and robust pca for internet trafﬁc anomaly detection. In 2012 Proceedings
IEEE INFOCOM  pages 1755–1763. IEEE  2012.

[4] Tomasz Niedoba. Multi-parameter data visualization by means of principal component analysis (pca) in
qualitative evaluation of various coal types. physicochemical problems of Mineral processing  50  2014.

[5] Tauno Metsalu and Jaak Vilo. Clustvis: a web tool for visualizing clustering of multivariate data using

principal component analysis and heatmap. Nucleic acids research  43(W1):W566–W570  2015.

[6] Yihang Yin  Fengzheng Liu  Xiang Zhou  and Quanzhong Li. An efﬁcient data compression model
based on spatial clustering and principal component analysis in wireless sensor networks. Sensors  15(8):
19443–19465  2015.

[7] Roy Frostig  Cameron Musco  Christopher Musco  and Aaron Sidford. Principal component projection

without principal component analysis. In ICML  pages 2349–2357  2016.

[8] Zeyuan Allen-Zhu and Yuanzhi Li. Faster principal component regression and stable matrix chebyshev
approximation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 107–115. JMLR. org  2017.

[9] Alexandre Eremenko and Peter Yuditskii. Uniform approximation of sgn x by polynomials and entire

functions. Journal d’Analyse Mathématique  101(1):313–324  2007.

[10] EI Zolotarev. Application of elliptic functions to questions of functions deviating least and most from zero.

Zap. Imp. Akad. Nauk. St. Petersburg  30(5):1–59  1877.

[11] Yuji Nakatsukasa and Roland W Freund. Computing fundamental matrix decompositions accurately via the
matrix sign function in two iterations: The power of zolotarev’s functions. SIAM Review  58(3):461–493 
2016.

[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In Advances in neural information processing systems  pages 315–323  2013.

[13] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point

problems. In Advances in Neural Information Processing Systems  pages 1416–1424  2016.

[14] Roy Frostig  Rong Ge  Sham Kakade  and Aaron Sidford. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine
Learning  pages 2540–2548  2015.

[15] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization. In

Advances in Neural Information Processing Systems  pages 3384–3392  2015.

[16] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112  2017.

[17] Cameron Musco  Christopher Musco  and Aaron Sidford. Stability of the lanczos method for matrix
function approximation. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete
Algorithms  pages 1605–1624. Society for Industrial and Applied Mathematics  2018.

[18] Victor Y. Pan and Zhao Q. Chen. The complexity of the matrix eigenproblem. In Proceedings of the
Thirty-ﬁrst Annual ACM Symposium on Theory of Computing  STOC ’99  pages 507–516  New York  NY 
USA  1999. ACM. ISBN 1-58113-067-8. doi: 10.1145/301250.301389. URL http://doi.acm.org/
10.1145/301250.301389.

[19] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In Proceedings of
the 44th Symposium on Theory of Computing Conference  STOC 2012  New York  NY  USA  May 19 - 22 
2012  pages 887–898  2012.

10

[20] Y. E. NESTEROV. A method for solving the convex programming problem with convergence rate O(1/k2).

Dokl. Akad. Nauk SSSR  269:543–547  1983.

[21] Kyle Gallivan  G Grimme  and Paul Van Dooren. A rational lanczos algorithm for model reduction.

Numerical Algorithms  12(1):33–63  1996.

[22] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regular-

ized loss minimization. In International Conference on Machine Learning  pages 64–72  2014.

[23] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The Journal of

Machine Learning Research  18(1):8194–8244  2017.

[24] Yurii Nesterov and Sebastian U Stich. Efﬁciency of the accelerated coordinate descent method on structured

optimization problems. SIAM Journal on Optimization  27(1):110–123  2017.

[25] Naman Agarwal  Sham Kakade  Rahul Kidambi  Yin Tat Lee  Praneeth Netrapalli  and Aaron Sidford.
Leverage score sampling for faster accelerated regression and erm. arXiv preprint arXiv:1711.08426  2017.

[26] Andrei Aleksandrovich Gonchar. Zolotarev problems connected with rational functions. Matematicheskii

Sbornik  120(4):640–654  1969.

[27] Frank Olver  Daniel Lozier  Ronald Boisvert  and Charles Clark. Nist handbook of mathematical functions.

01 2010.

[28] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas  graphs  and

mathematical tables  volume 55. Courier Corporation  1965.

[29] Sébastien Bubeck  Yin Tat Lee  and Mohit Singh. A geometric alternative to nesterov’s accelerated gradient

descent. arXiv preprint arXiv:1506.08187  2015.

[30] Yurii Nesterov. Lectures on convex optimization  volume 137. Springer  2018.

11

,Travis Monk
Cristina Savin
Jörg Lücke
Yujia Jin
Aaron Sidford