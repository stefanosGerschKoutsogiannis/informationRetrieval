2016,Density Estimation via Discrepancy Based Adaptive Sequential Partition,Given $iid$ observations from an unknown continuous distribution defined on some domain $\Omega$  we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of $\Omega$.  The key ingredient of the algorithm is to use discrepancy  a concept originates from Quasi Monte Carlo analysis  to control the partition process. The resulting algorithm is simple  efficient  and has  provable convergence rate. We demonstrate empirically its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means.,Density Estimation via Discrepancy Based

Adaptive Sequential Partition

Dangna Li

ICME 

Stanford University
Stanford  CA 94305

dangna@stanford.edu

Kun Yang

Google

Mountain View  CA 94043
kunyang@stanford.edu

Wing Hung Wong

Department of Statistics

Stanford University
Stanford  CA 94305

whwong@stanford.edu

Abstract

Given iid observations from an unknown absolute continuous distribution deﬁned
on some domain Ω  we propose a nonparametric method to learn a piecewise
constant function to approximate the underlying probability density function. Our
density estimate is a piecewise constant function deﬁned on a binary partition of
Ω. The key ingredient of the algorithm is to use discrepancy  a concept originates
from Quasi Monte Carlo analysis  to control the partition process. The resulting
algorithm is simple  efﬁcient  and has a provable convergence rate. We empirically
demonstrate its efﬁciency as a density estimation method. We also show how it can
be utilized to ﬁnd good initializations for k-means.

1

Introduction

Density estimation is one of the fundamental problems in statistics. Once an explicit estimate of the
density function is constructed  various kinds of statistical inference tasks follow naturally. Given iid
observations  our goal in this paper is to construct an estimate of their common density function via a
nonparametric domain partition approach.
As pointed out in [1]  for density estimation  the bias due to the limited approximation power of a
parametric family will become dominant in the over all error as the sample size grows. Hence it is
necessary to adopt a nonparametric approach to handle this bias. The kernel density estimation [2]
is a popular nonparametric density estimation method. Although in theory it can achieve optimal
convergence rate when the kernel and the bandwidth are appropriately chosen  its result can be
sensitive to the choice of bandwidth  especially in high dimension. In practice  kernel density
estimation is typically not applicable to problems of dimension higher than 6.
Another widely used nonparametric density estimation method in low dimension is the histogram. But
similarly with kernel density estimation  it can not be scaled easily to higher dimensions. Motivated
by the usefulness of histogram and the need for a method to handle higher dimensional cases  we
propose a novel nonparametric density estimation method which learns a piecewise constant density
function deﬁned on a binary partition of domain Ω.
A key ingredient for any partition based method is the decision for stopping. Based on the observation
that for any piecewise constant density  the distribution conditioned on each sub-region is uniform 
we propose to use star discrepancy  which originates from analysis of Quasi-Monte Carlo methods 
to formally measure the degree of uniformity. We will see in section 4 that this allows our density
estimator to have near optimal convergence rate.
In summary  we highlight our contribution as follows:

• To the best of our knowledge  our method is the ﬁrst density estimation method that utilizes

Quasi-Monte Carlo technique in density estimation.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

• We provide an error analysis on binary partition based density estimation method. We
establish an O(n− 1
2 ) error bound for the density estimator. The result is optimal in the sense
that essentially all Monte Carlo methods have the same convergence rate. Our simulation
results support the tightness of this bound.
• One of the advantage of our method over existing ones is its efﬁciency. We demonstrate in
section 5 that our method has comparable accuracy with other methods in terms of Hellinger
distance while achieving an approximately 102-fold speed up.
• Our method is a general data exploration tool and is readily applicable to many important
learning tasks. Speciﬁcally  we demonstrate in section 5.3 how it can be used to ﬁnd good
initializations for k-means.

2 Related work
Existing domain partition based density estimators can be divided into two categories: the ﬁrst
category belongs to the Bayesian nonparametric framework. Optional Pólya Tree (OPT) [3] is a
class of nonparametric conjugate priors on the set of piecewise constant density functions deﬁned on
some partition of Ω. Bayesian Sequential Partitioning (BSP) [1] is introduced as a computationally
more attractive alternative to OPT. Inferences for both methods are performed by sampling from the
posterior distribution of density functions. Our improvement over these two methods is two-fold.
First  we no longer restrict the binary partition to be always at the middle. By introducing a new
statistic called the “gap”  we allow the partitions to be adaptive to the data. Second  our method
does not stem from a Bayesian origin and proceeds in a top down  greedy fashion. This makes our
method computationally much more attractive than OPT and BSP  whose inference can be quite
computationally intensive.
The second category is tree based density estimators [4] [5]. As an example  Density Estimation
Trees [5] is generalization of classiﬁcation trees and regression trees for the task of density estimation.
Its tree based origin has led to a loss minimization perspective: the learning of the tree is done by
minimizing the integrated squared error. However  the true loss function can only be approximated by
a surrogate and the optimization problem is difﬁcult to solve. The objective of our method is much
simpler and leads to an intuitive and efﬁcient algorithm.

in Rd. We use the short hand notation [a  b] =(cid:81)d

3 Main algorithm
3.1 Notations and deﬁnitions
In this paper we consider the problem of estimating a joint density function f from a given set of
observations. Without loss of generality  we assume the data domain Ω = [0  1]d  a hyper-rectangle
j=1[aj  bj] to denote a hyper-rectangle in Rd  where
a = (a1 ···   ad)  b = (b1 ···   bd) ∈ [0  1]d. Each (aj  bj) pair speciﬁes the lower and upper bound
of the hyper-rectangle along dimension j.
We restrict our attention to the class of piecewise constant functions after balancing the trade-off
between simplicity and representational power: Ideally  we would like the function class to have
concise representation while at the same time allowing for efﬁcient evaluation. On the other hand 
we would like to be able to approximate any continuous density function arbitrarily well (at least as
the sample size goes to inﬁnity). This trade-off has led us to choose the set of piecewise constant
functions supported on binary partitions: First  we only need 2d + 1 ﬂoating point numbers to
uniquely deﬁne a sub-rectangle (2d for its location and 1 for its density value). Second  it is well
known that the set of positive  integrable  piesewise constant functions is dense in Lp for p ∈ [1 ∞).
The binary partition we consider can be deﬁned in the following recursive way: starting with
P0 = Ω. Suppose we have a binary partition Pt = {Ω(1) ···   Ω(t)} at level t  where ∪t
i=1Ω(i) = Ω 
Ω(i) ∩ Ω(j) = ∅  i (cid:54)= j  a level t + 1 partition Pt+1 is obtained by dividing one sub-rectangle Ω(i) in
Pt along one of its coordinates  parallel to one of the dimension. See Figure 1 for an illustration.

3.2 Adaptive partition and discrepancy control
The above recursive build up has two key steps. The ﬁrst is to decide whether to further split a sub-
rectangle. One helpful intuition is that for piecewise constant densities  the distribution conditioned
on each sub-rectangle is uniform. Therefore the partition should stop when the points inside a sub-
rectangle are approximatly uniformly scattered. In other words  we stop the partition when further

2

Figure 1: Left: a sequence of binary partition and the corresponding tree representation; if we encode
partitioning information (e.g.  the location where the split occurs) in the nodes  there is a one to one
mapping between the tree representations and the partitions. Right: the gaps with m = 3  we split
the rectangle at location D  which corresponds to the largest gap (Assuming it does not satisfy (2) 
see the text for more details)

.

partitioning does not reveal much additional information about the underlying density landscape. We
propose to use star discrepancy  which is a concept originates from the analysis of Quasi-Monte Carlo
methods  to formally measure the degree of uniformity of points in a sub-rectangle. Star discrepancy
is deﬁned as:
Deﬁnition 1. Given n points Xn = {x1  ...  xn} in [0  1]d. The star discrepancy D∗(Xn) is deﬁned
as:

∗

D

(Xn) = sup

a∈[0 1]d

(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

1{xi ∈ [0  a)} − d(cid:89)

i=1

j=1

(cid:12)(cid:12)(cid:12)

aj

(1)

The supremum is taken over all d-dimensional sub-rectangles [0  a). Given star discrepancy D∗(Xn) 
we have the following error bound for Monte Carlo integration (See [6] for a proof):
Theorem 2. (Koksma-Hlawka inequality) Let Xn = {x1  x2  ...  xn} be a set of points in [0  1]d with
discrepancy D∗(Xn); Let f be a function on [0  1]d of bounded variation V(f ). Then 

(cid:12)(cid:12)(cid:12)(cid:90)

f (x)dx − 1
n

[0 1]d

f (xi)

n(cid:88)

i=1

(cid:12)(cid:12)(cid:12) ≤ V(f )D

∗

(Xn)

to ﬁnd a good location to split for [a  b] =(cid:81)d
|(1/n)(cid:80)n

where V(f ) is the total variation in the sense of Hardy and Krause (See [7] for its precise deﬁnition).
The above theorem implies if the star discrepancy D∗(Xn) is under control  the empirical distribution
will be a good approximation to the true distribution. Therefore  we may decide to keep partitioning
a sub-rectangle until its discrepancy is lower than some threshold. We shall see in section 4 that this
provably guarantees our density estimate is a good approximation to the true density function.
Another important ingredient of all partition based methods is the choice of splitting point. In order
j=1[aj  bj]  we divide jth dimension into m equal-sized
bins: [aj  aj + (bj − aj)/m]  ...  [aj + (bj − aj)(m − 2)/m  aj + (bj − aj)(m − 1)/m] and keep
track of the gaps at aj + (bj − aj)/m  ...  aj + (bj − aj)(m − 1)/m  where the gap gjk is deﬁned as
i=1 1(xij < aj + (bj − aj)k/m) − k/m| for k = 1  ...  (m − 1)  there are total (m − 1)d
gaps recorded (Figure 1). Here m is a hyper-parameter chosen by the user. [a  b] is split into two
sub-rectangles along the dimension and location corresponding to maximum gap (Figure 1). The
pseudocode for the complete algorithm is given in Algorithm 1. We refer to this algorithm as DSP in
the sequel. One distinct feature of DSP is it only requires the user to specify two parameters: m  θ 
where m is the number of bins along each dimension; θ is the parameter for discrepancy control (See
theorem 2 for more details). In some applications  the user may prefer putting an upper bound on
the number of total partitions. In that case  there is typically no need to specify θ. Choices for these
parameters are discussed in Section 5.
The resulting density estimates ˆp is a piecewise constant function deﬁned on a binary partition
i=1 d(ri)1{x ∈ ri} where 1 is the indicator function; L is the total number of
sub-rectangles in the ﬁnal partition; {ri  d(ri)}L
i=1 are the sub-rectangle and density pairs. We
demonstrate in section 5 how ˆp(x) can be leveraged to ﬁnd good initializations for k-means. In the
following section  we establish a convergence result of our density estimator.

of Ω: ˆp(x) = (cid:80)L

3

●●●●●●●●●●●●●●●●●●●●●●●A:1/60B:1/60C:2/60D:7/60●●●●●●●●●●●●●●●●●●●●●●●●Algorithm 1 Density Estimation via Discrepancy Based Sequential Partition (DSP)
Input: XN   m  θ
Output: A piecewise constant function Pr(·) deﬁned on a binary partition R
Let Pr(r) denote the probability mass of region r ⊂ Ω; let XN (r) denote the points in XN that lie
within r  where r ⊂ Ω. ni denotes the size of set X (i).
1: procedure DSP(XN   m  θ)
2:
3:
4:
5:
6:

B = {[0  1]d}  Pr([0  1]d) = 1
while true do
R(cid:48) = ∅
for each ri = [a(i)  b(i)] in R do

Calculate gaps {gjk}j=1 ... d k=1 ... m−1
Scale X(ri) = {xil}ni
if X(ri) (cid:54)= ∅ and D∗( ˜X (i)) > θ

l=1 to ˜X (i) = {˜xil = (
N /ni then

√

xil  1−a

(i)
1

b

(i)
1

  ... 

xil  d−a

(i)
d

b

(i)
d

)}ni

l=1

(cid:46) Condition (2) in Theorem 4

)|

Split ri into ri1 = [a(i1)  b(i1)] and ri2 = [a(i2)  b(i2)] along the max gap (Figure 1).
|P (ri1
Pr(ri1 ) = Pr(ri)
ni
R(cid:48) = R(cid:48) ∪ {ri1   ri2}
else R(cid:48) = R(cid:48) ∪ {ri}
if R(cid:48) (cid:54)= R then R = R(cid:48)
else return R  Pr(·)

  Pr(ri2 ) = Pr(ri) − Pr(ri1 )

7:

8:
9:
10:
11:
12:
13:
14:

4 Theoretical results
Before we establish our main theorem  we need the following lemma:1
n = inf{x1 ... xn}∈[0 1]d D∗(x1  ...  xn)  then we have
Lemma 3. Let D∗

(cid:114)
n ≤ c
∗
D
for all n  d ∈ R+  where c is some positive constant.

d
n

We now state our main theorem:
Theorem 4. Let f be a function deﬁned on Ω = [0  1]d with bounded variation. Let XN =
{x1  ...  xN ∈ Ω} and {[a(i)  b(i)]  i = 1 ···   L} be a level L binary partition of Ω. Further denote
by X (i) = {xj = (xj1  ...  xjd)  xj ∈ [a(i)  b(i)] and } ∩ XN   i.e. the part of XN in sub-rectangle i.
ni = |X (i)|. Suppose in each sub-rectangle [a(i)  b(i)]  X (i) satisﬁes

(cid:113) N

nid

(2)

θ

c for some positive

(3)

∗

D

( ˜X (i)) ≤ α(i)D

∗
ni

where ˜X (i) = {˜xj = ( xj1−a(i)
constant θ  D∗

  ...  xjd−a(i)
is deﬁned as in lemma 3. Then

b(i)
d

b(i)
1

d

1

ni

(cid:12)(cid:12)(cid:12)(cid:90)

f (x)ˆp(x)dx − 1
N

[0 1]d

)  xj ∈ X (i)}   α(i) =

N(cid:88)

i=1

(cid:12)(cid:12)(cid:12) ≤ θ√

N

V(f )

f (xi)

where ˆp(x) is a piecewise constant density estimator given by

L(cid:88)

i=1

ˆp(x) =

di1{x ∈ [a(i)  b(i)]}

with di = ((cid:81)d

j=1(b(i)

j − a(i)

j ))−1ni/N  i.e.  the empirical density.

In the above theorem  α(i) controls the relative uniformity of the points and is adaptive to X (i). It
imposes more restrictive constraints on regions containing larget proportion of the sample (ni/N).
Although our density estimate is not the only estimator which satisﬁes (3)  (for example  both the
empirical distribution in the asymptotic limit and kernel density estimator with sufﬁciently small
bandwidth meet the criterion)  one advantage of our density estimator is that it provides a very concise

1The proof for Lemma 3 can be found in [8]. Theorem 4 and Corollary 5 are proved in the supplementary

material.

4

summary of the data while at the same time capturing the landscape of the underlying distribution. In
addition  the piecewise constant function does not suffer from having too many “local bumps”  which
is a common problem for kernel density estimator. Moreover  under certain regularity conditions
(e.g. bounded second moments)  the convergence rate of Monte Carlo methods for 1
i=1 f (xi) to
N
[0 1]d f (x)p(x)dx is of order O(N− 1
2 ). Our density estimate is optimal in the sense that it achieves
the same rate of convergence. Given theorem 4  we have the following convergence result:
Corollary 5. Let ˆp(x) be the estimated density function as in theorem 4. For any hyper-rectangle

(cid:82)
A = [a  b] ⊂ [0  1]d  let ˆP (A) =(cid:82)

A ˆp(x)dx and P (A) =(cid:82)

A p(x)dx  then

(cid:80)N

| ˆP (A) − P (A)| → 0

sup

A⊂[0 1]d

at the order O(n− 1
2 ).
Remark 4.1. It is worth pointing out that the total variation distance between two probability
measures ˆP and P is deﬁned as δ( ˆP   P ) = supA∈B | ˆP (A) − P (A)|  where B is the Borel σ-algebra
of [0  1]d. In contrast  Corollary 5 restricts A to be hyper-rectangles.

5 Experimental results
5.1
Implementation details
In some applications  we ﬁnd it helpful to ﬁrst estimate the marginal densities for each component
variables x.j (j = 1  ...  d)  then make a copula transformation z.j= ˆFj(x.j)  where ˆFj is the estimated
cdf of x.j. After such a transformation  we can take the domain to be [0  1]d. Also we ﬁnd this can
save the number of partition needed by DSP. Unless otherwise stated  we use copula transform in our
experiments whenever the dimension exceeds 3.
We make the following observations to improve the efﬁciency of DSP: 1) First observe that
maxj=1 ... d D∗({xij}n
i=1). Let x(i)j be the ith smallest element in {xij}n
i=1 
then D∗({xij}n
2n | [9]  which has complexity O(n log n). Hence
i=1) = 1
maxj=1 ... d D∗({xij}n
L/n ﬁrst before calculating
D∗({xi}n
√
i=1) is bounded above by 1; 3)
i=1) is bounded below by cd log(d−1)/2 n−1 with some
θ
N /n ≤  
constant cd depending on d [10]; thus we can keep splitting without checking (2) when θ
where  is a small positive constant (say 0.001) speciﬁed by the user. This strategy has proved to be
effective in decreasing the runtime signiﬁcantly at the cost of introducing a few more sub-rectangles.
Another approximation works well in practice is by replacing star discrepancy with computationally
2 ; in fact 
several statistics to test uniformity hypothesis based on D(2) are proposed in [11]; however  the
theoretical guarantee in Theorem 4 no longer holds. By Warnock’s formula [9] 

i=1) ≤ D∗({xi}n
2n + maxi |x(i)j − 2i−1
√
i=1) can be used to compare against θ
N /n is large when n is small  but D∗({xi}n

attractive L2 star discrepancy  i.e.  D(2)(Xn) = ((cid:82)

N /n is tiny when n is large and D∗({xi}n

i=1 ai|2da) 1

[0 1]d | 1

i=1); 2) θ

√

√

n

(cid:80)n
i=1 1xi∈[0 a) −(cid:81)d
d(cid:89)
n(cid:88)

n(cid:88)

d(cid:89)

[D(2)(Xn)]2 =

1

3d − 21−d

n

(1 − x2

ij) +

1
n2

min{1 − xij  1 − xlj}

i=1

j=1

j=1

i l=1

R in Algorithm 1  the total complexity is at most(cid:80)L

D(2) can be computed in O(n logd−1 n) by K. Frank and S. Heinrich’s algorithm [9]. At each scan of
i=1 O(ni logd−1 N ) ≤

i=1 O(ni logd−1 ni) ≤(cid:80)L

O(N logd−1 N ).
There are no closed form formulas for calculating D∗(Xn) and D∗
n except for low dimensions. If
we replace α(i) in (2) and apply Lemma 3  what we are actually trying to do is to control D∗( ˜X (i))
N /ni. There are many existing work on ways to approximate D∗(Xn). In particular  a new
by θ
randomized algorithm based on threshold accepting is developed in [12]. Comprehensive numerical
tests indicate that it improves upon other algorithms  especially in when 20 ≤ d ≤ 50. We used
this algorithm in our experiments. The interested readers are referred to the original paper for more
details.

√

5

5.2 DSP as a density estimate
1) To demonstrate the method and visualize the results  we apply it on several 2-dimensional data sets
simulated from 3 distributions with different geometry:

µ2 = (.50  .75)T   Σ1 = Σ2 = [0.04  0.01; 0.01  0.01];

1. Gaussian: x ∼ N (µ  Σ)1{x ∈ [0  1]2}  with µ = (.5  .5)T   Σ = [0.08  0.02; 0.02  0.02]
2. Mixture of Gaussians: x ∼ 1
3. Mixture of Betas: x ∼ 1

(cid:80)2
i=1 N (µi  Σi)1{x ∈ [0  1]2} with µ1 = (.50  .25)T   and

3 (beta(2  5)beta(5  2) + beta(4  2)beta(2  4) + beta(1  3)beta(3  1));
where N (µ  Σ) denotes multivariate Gaussian distribution and beta(α  β) denotes beta distribution.
We simulated 105 points for each distribution. See the ﬁrst row of Figure 2 for visualizations of the
estimated densities. The ﬁgure shows DSP accurately estimates the true density landscape in these
three toy examples.

2

Figure 2: First row: estimated densities for 3 simulated 2D datasets. The modes are marked with
stars. The corresponding contours of true densities are embedded for comparison. Second row:
simulation of 2  5 and 10 dimensional cases (from left to right) with reference functions f1  f2  f3.
x-axis: sample size n. y-axis: error between the true integral and the estimated integral. The vertical
bars are standard error bars obtained from 10 replications. See section 5.2 2) for more details.

(cid:80)d

i=1

1
2

j=1 x

i=1

1
2

j=1 x

i=1

(cid:80)d

j=1 beta(xj  5  15)

1
2
distribution.

ij  f2(x) = (cid:80)n

[0 1]d fk(x)ˆp(x)dx| is bounded by |(cid:82)

dimension d = 2  5 and 10 respectively: f1(x) = (cid:80)n
f3(x) = ((cid:80)n
(cid:16)(cid:81)d
j=1 beta(xj  15  5) +(cid:81)d
The error |(cid:82)
j=1 fk(xj)| + |(cid:82)
(cid:80)n

(cid:80)d
[0 1]d fk(x)p(x)dx − (cid:82)

2) To evaluate the theoretical bound (3)  we choose the following three 3 reference functions with
j=1 xij 
ij)2. We generate n ∈ {102  103  104  105  106} samples from p(x) =
  where beta(·  α  β) is the density function of beta

(cid:17)
(cid:80)n
[0 1]d fk(x)p(x)dx −
j=1 fk(xj)| where ˆp(x) is the estimated density;
For almost all Monte Carlo methods  the ﬁrst term is of order O(n− 1
2 ). The second term is controlled
by (3). Thus in total the error is of order O(n− 1
2 ). We have plot the error against the sample size
on log-log scale for each dimension in the second row of Figure 2. The linear trends in the plots
corroborate the bound in (3).
3) To show the efﬁciency and scalability of DSP  we compare it with KDE  OPT and BSP in terms
i=1 πiN (µi  Σi))1{x ∈
[0  1]d} with d = {2  3 ···   6} and N = {103  104  105} respectively. The estimation error measured
in terms of Hellinger Distance is summarized in Table 1. We set m = 10  θ = 0.01 in our experiments.
We found the resulting Hellinger distance to be quite robust as m ranges from 3 to 20 (equally

of estimation error and running time. We simulate samples from x ∼ ((cid:80)4

[0 1]d fk(x)ˆp(x)dx − 1

1
n

n

6

10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 10210310410510610-410-310-210-1 00.20.40.60.8100.20.40.60.81 00.20.40.60.8100.20.40.60.8100.20.40.60.8100.20.40.60.81f1f2f3f1f2f3f1f2f3*****spaced). The supplementary material includes the exact details about the parameters of the simulating
distributions  estimation of Hellinger distance and other implementation details for the algorithms.
The table shows DSP achieves comparable accuracy with the best of the other three methods. As
mentioned at the beginning of this paper  one major advantage of DSP’s is its speed. Table 2 shows
our method achieves a signiﬁcant speed up over all other three algorithms.

Table 1: Error in Hellinger Distance between the true density and KDE  OPT  BSP  our method
for each (d  n) pair. The numbers in parentheses are standard errors from 20 replicas. The best of the
four method is highlighted in bold. Note that the simulations  being based on mixtures of Gaussians 
is unfavorable for methods based on domain partitions.

Hellinger Distance (n = 103)

Hellinger Distance (n = 104)

Hellinger Distance (n = 105)

d KDE
2

0.2331
(0.0421)
0.2893
(0.0227)
0.3913
(0.0325)
0.4522
(0.0317)
0.5511
(0.0318)

3

4

5

6

OPT
0.2147
(0.0172)
0.3279
(0.0128)
0.3839
(0.0136)
0.4748
(0.009)
0.5508
(0.0307)

BSP
0.2533
(0.0163)
0.2983
(0.0133)
0.3872
(0.0117)
0.4435
(0.0167)
0.5515
(0.0354)

DSP
0.2634
(0.0207)
0.3072
(0.0265)
0.3895
(0.0191)
0.4307
(0.0302)
0.5527
(0.0381)

KDE
0.1104
(0.0102)
0.2003
(0.0199)
0.2466
(0.0113)
0.3599
(0.0199)
0.4833
(0.0255)

OPT
0.0957
(0.0036)
0.1722
(0.0028)
0.2726
(0.0031)
0.3562
(0.0025)
0.4015
(0.0023)

BSP
0.1222
(0.0043)
0.1717
(0.0083)
0.2882
(0.0047)
0.3987
(0.0022)
0.4093
(0.0046)

DSP
0.0803
(0.0013)
0.1721
(0.0073)
0.2955
(0.0065)
0.3563
(0.0031)
0.3911
(0.0037)

KDE
0.0305
(0.0021)
0.1466
(0.0047)
0.1900
(0.0057)
0.2817
(0.0088)
0.3697
(0.0122)

OPT
0.0376
(0.0021)
0.1117
(0.0008)
0.1880
(0.0006)
0.2822
(0.0005)
0.3409
(0.0005)

BSP
0.0345
(0.0025)
0.1323
(0.0009)
0.2100
(0.0006)
0.2916
(0.0003)
0.3693
(0.0004)

DSP
0.0312
(0.0027)
0.1020
(0.004)
0.1827
(0.0059)
0.2910
(0.0002)
0.3701
(0.0002)

Table 2: Average CPU time in seconds of KDE  OPT  BSP and our method for each (d  n) pair.
The numbers in parentheses are standard errors from 20 replicas. The speed-up is the fold speed-up
computed as the ratio between the minimum run time of the other three methods and the run time of
DSP. All methods are implemented in C++. See the supplementary material for more details.

d
2

3

4

5

6

KDE
2.445
(0.191)
2.655
(0.085)
3.540
(0.116)
4.107
(0.110)
4.986
(0.214)

Running time (n = 103)
DSP
OPT
0.020
9.484
(0.002)
(0.029)
0.019
25.073
(0.002)
(0.056)
32.112
0.019
(0.002)
(0.072)
0.020
37.599
(0.002)
(0.088)
0.020
41.565
(0.147)
(0.001)

BSP
0.833
(0.006)
1.054
(0.010)
1.314
(0.014)
1.713
(0.019)
2.749
(0.024)

speed-up

41

55

69

85

137

KDE
21.903
(1.905)
26.964
(1.089)
37.141
(2.244)
45.580
(2.124)
53.291
(2.767)

Running time (n = 104)
DSP
OPT
0.033
31.561
(0.002)
(0.079)
0.044
36.683
(0.001)
(0.076)
39.219
0.049
(0.002)
(0.221)
0.078
44.520
(0.002)
(0.587)
0.127
43.032
(0.413)
(0.004)

BSP
1.445
(0.014)
2.819
0.036)
5.861
(0.076)
12.220
(0.154)
21.696
(0.213)

speed-up

43

64

119

157

170

KDE
230.179
(130.572)
278.075
(10.576)
347.501
(14.676)
412.828
(16.252)
519.298
(29.276)

Running time (n = 105)
DSP
OPT
0.242
44.561
(0.015)
(0.639)
0.378
56.329
(0.011)
(0.911)
67.366
0.485
(0.018)
(3.018)
0.706
77.776
(0.051)
(2.215)
0.896
81.023
(3.703)
(0.071)

BSP
7.750
(0.178)
21.104
(0.576)
53.620
(2.917)
115.869
(6.872)
218.999
(6.046)

speed-up

33

55

108

110

90

5.3 DSP-kmeans
In addition to being a competitive density estimator  we demonstrate in this section how DSP can be
used to get good initializations for k-means. The resulting algorithm is referred to as DSP-kmeans.
Recall that given a ﬁxed number of clusters K  the goal of k-means is to minimize the following
objective function:

JK

∆=

(cid:107)xi − mk(cid:107)2

2

(4)

K(cid:88)

(cid:88)

k=1

i∈Ck

where Ck denote the set of points in cluster k; {mk}K
k=1 denote the cluster means. The original
k-means algorithms proceeds by alternating between assigning points to centers and recomputing
the means. As a result  the ﬁnal clustering is usually only a local optima and can be sensitive to the
initializations. Finding a good initialization has attracted a lot of attention over the past decade and
now there is a descent number existing methods  each with their own perspectives. Below we review
a few representative types.
One type of methods look for good initial centers sequentially. The idea is once the ﬁrst center is
picked  the second should be far away from the one that is already chosen. A similar argument applies
to the rest of the centers. [13] [14] fall under this category. Several studies [15] [16] borrow ideas
from hierarchical agglomerative clustering (HAC) to look for good initializations. In our experiments
we used the algorithm described in [15]. One essential ingredient of this type of algorithms is the inter
cluster distance  which could be problem dependent. Last but not least  there is a class of methods
that attempt to utilize the relationship between PCA and k-means. [17] proposes a PCA-guided
search for initial centers. [18] combines the relationship between PCA and k-means to look for
good initialization. The general idea is to recursively splitting a cluster according the ﬁrst principal
component. We refer to this algorithm as PCA-REC.

7

(cid:80)

DSP-kmeans is different from previous methods in that it tackles the initialization problem from
a density estimation point of view. The idea behind DSP-kmeans is that cluster centers should be
close to the modes of underlying probability density function. If a density estimator can accurately
locate the modes of the underlying true density function  it should also be able to ﬁnd good cluster
centers. Due to its concise representation  DSP can be used for ﬁnding initializations for k-means
in the following way: Suppose we are trying to cluster a dataset Y with K clusters. We ﬁrst apply
DSP on Y to ﬁnd a partition with K non-empty sub-rectangles  i.e. sub-rectangles that have at least
one point from Y . The output of DSP will be K sub-rectangles. Denote the set of indices for the
points in sub-rectangle j by Sj  j = 1  . . .   K  let Ij = 1|Sj|
Yi  i.e. Ij is the sample average
of points fall into sub-rectangle j. We then use {I1 ···   IK} to initialize k-means. We also explored
the following two-phase procedure: ﬁrst over partition the space to build a more accurate density
estimate. Points in different sub-rectangles are considered to be in different clusters. Then we merge
the sub-rectangles hierarchically based on some measure of between cluster distance. We have found
this to be helpful when the number of clusters K is relatively small. For completeness  we have
included the details of this two-phase DSP-kmeans in the supplementary material.
We test DSP-kmeans on 4 real world datasets of various number of data points and dimensions. Two
of them are taken from the UCI machine learning repository [19]; the stem cell data set is taken from
the FlowCAP challenges [20]; the mouse bone marrow data set is a recently published single-cell
dataset measured using mass cytometry [21]. We use random initialization as the base case and
compare it with DSP-kmeans  k-means++  PCA-REC and HAC. The numbers in Table 3 are the
improvements in k-means objective function of a method over random initialization. The result
shows when the number of clusters is relatively large DSP-kmeans achieves lower objective value
in these four datasets. Although in theory almost all density estimator could be used to ﬁnd good

i∈Sj

Table 3: Comparison of different initialization methods. The number for method j is relative
to random initialization: JK j−JK 0
  where JK j is the k-means objective value of method j at
convergence. Here we use 0 as index for random initialization. Negative number means the method
perform worse than random initialization.
Improvement over random init.

Improvement over random init.

JK 0

k-means++

PCA-REC HAC DSP-kmeans Mouse bone marrow

k-means++

PCA-REC HAC DSP-kmeans

k-means++

PCA-REC HAC DSP-kmeans US census

k-means++

PCA-REC HAC DSP-kmeans

Road network
n
d

4.3e+04
3

Stem cell
n
d

9.9e+03
6

k
4
10
20
40
60
k
4
10
20
40
60

0.0
0.0
0.43
11.7
19.78

3.45
3.82
9.96
9.95
6.12

-0.02
-0.12
-0.46
-2.52
-3.45

-2.1
-4.2
-3.59
-6.39
-7.29

0.01
0.25
1.68
2.27
18.69

3.67
3.79
9.91
10.11
8.19

0.0
0.08
2.04
13.62
20.91

3.96
3.6
9.39
12.49
13.7

n
d

n
d

8.7e+04
39

2.4e+06
68

k
4
10
20
40
60
k
4
10
20
40
60

1.51
0.45
0.63
1.99
2.48

47.44
40.52
32.63
32.66
21.7

0.03
0.24
-1.2
-3.56
-5.25

-2.33
-1.9
-1.97
-5.15
-1.19

1.25
0.77
0.68
2.06
2.57

46.72
41.48
29.49
33.41
16.28

0.4
0.83
0.79
2.55
2.65

40.44
39.52
32.55
34.61
21.68

initializations. Based on the comparison of Hellinger distance in Table 1  we would expect them to
have similar performances. However  for OPT and BSP  their runtime would be a major bottleneck for
their applicability The situation for KDE is slightly more complicated: not only it is computationally
quite intensive  its output can not be represented as concisely as partition based methods. Here we
see that the efﬁciency of DSP makes it possible to utilize it for other machine learning tasks.
6 Conclusion
In this paper we propose a novel density estimation method based on ideas from Quasi-Monte Carlo
analysis. We prove it achieves a O(n− 1
2 ) error rate. By comparing it with other density estimation
methods  we show DSP has comparable performance in terms of Hellinger distance while achieving
a signiﬁcant speed-up. We also show how DSP can be used to ﬁnd good initializations for k-means.
Due to space limitation  we were unable to include other interesting applications including mode
seeking  data visualization via level set tree and data compression [22].

Acknowledgements. This work was supported by NIH-R01GM109836  NSF-DMS1330132 and
NSF-DMS1407557. The second author’s work was done when the author was a graduate student at
Stanford University.

8

References

[1] Luo Lu  Hui Jiang  and Wing H Wong. Multivariate density estimation by bayesian sequential partitioning.

Journal of the American Statistical Association  108(504):1402–1410  2013.

[2] Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical

statistics  33(3):1065–1076  1962.

[3] Wing H Wong and Li Ma. Optional pólya tree and bayesian inference. The Annals of Statistics  38(3):1433–

1459  2010.

[4] Han Liu  Min Xu  Haijie Gu  Anupam Gupta  John Lafferty  and Larry Wasserman. Forest density

estimation. The Journal of Machine Learning Research  12:907–951  2011.

[5] Parikshit Ram and Alexander G Gray. Density estimation trees. In Proceedings of the 17th ACM SIGKDD

international conference on Knowledge discovery and data mining  pages 627–635. ACM  2011.

[6] Lauwerens Kuipers and Harald Niederreiter. Uniform distribution of sequences. Courier Dover Publications 

2012.

[7] Art B Owen. Multidimensional variation for quasi-monte carlo. In International Conference on Statistics

in honour of Professor Kai-Tai Fang’s 65th birthday  pages 49–74  2005.

[8] Stefan Heinrich  Erich Novak  Grzegorz W Wasilkowski  and Henryk Wozniakowski. The inverse of the
star-discrepancy depends linearly on the dimension. ACTA ARITHMETICA-WARSZAWA-  96(3):279–302 
2000.

[9] Carola Doerr  Michael Gnewuch  and Magnus Wahlstróm. Calculation of discrepancy measures and

applications. Preprint  2013.

[10] Michael Gnewuch. Entropy  randomization  derandomization  and discrepancy. In Monte Carlo and

quasi-Monte Carlo methods 2010  pages 43–78. Springer  2012.

[11] Jia-Juan Liang  Kai-Tai Fang  Fred Hickernell  and Runze Li. Testing multivariate uniformity and its

applications. Mathematics of Computation  70(233):337–355  2001.

[12] Michael Gnewuch  Magnus Wahlstróm  and Carola Winzen. A new randomized algorithm to approximate
the star discrepancy based on threshold accepting. SIAM Journal on Numerical Analysis  50(2):781–807 
2012.

[13] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings
of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  pages 1027–1035. Society for
Industrial and Applied Mathematics  2007.

[14] Ioannis Katsavounidis  C-C Jay Kuo  and Zhen Zhang. A new initialization technique for generalized lloyd

iteration. Signal Processing Letters  IEEE  1(10):144–146  1994.

[15] Chris Fraley. Algorithms for model-based gaussian hierarchical clustering. SIAM Journal on Scientiﬁc

Computing  20(1):270–281  1998.

[16] Stephen J Redmond and Conor Heneghan. A method for initialising the k-means clustering algorithm

using kd-trees. Pattern recognition letters  28(8):965–973  2007.

[17] Qin Xu  Chris Ding  Jinpei Liu  and Bin Luo. Pca-guided search for k-means. Pattern Recognition Letters 

54:50–55  2015.

[18] Ting Su and Jennifer G Dy. In search of deterministic methods for initializing k-means and gaussian

mixture clustering. Intelligent Data Analysis  11(4):319–338  2007.

[19] Manohar Kaul  Bin Yang  and Christian S Jensen. Building accurate 3d spatial networks to enable next
generation intelligent transportation systems. In Mobile Data Management (MDM)  2013 IEEE 14th
International Conference on  volume 1  pages 137–146. IEEE  2013.

[20] Nima Aghaeepour  Greg Finak  Holger Hoos  Tim R Mosmann  Ryan Brinkman  Raphael Gottardo 
Richard H Scheuermann  FlowCAP Consortium  DREAM Consortium  et al. Critical assessment of
automated ﬂow cytometry data analysis techniques. Nature methods  10(3):228–238  2013.

[21] Matthew H Spitzer  Pier Federico Gherardini  Gabriela K Fragiadakis  Nupur Bhattacharya  Robert T Yuan 
Andrew N Hotson  Rachel Finck  Yaron Carmi  Eli R Zunder  Wendy J Fantl  et al. An interactive reference
framework for modeling a dynamic immune system. Science  349(6244):1259425  2015.

[22] Robert M Gray and Richard A Olshen. Vector quantization and density estimation. In Compression and

Complexity of Sequences 1997. Proceedings  pages 172–193. IEEE  1997.

9

,Jie Wang
Jiayu Zhou
Jun Liu
Peter Wonka
Jieping Ye
Dangna Li
Kun Yang
Wing Hung Wong
Miguel Carreira-Perpinan
Pooya Tavallali