2019,Learning Fairness in Multi-Agent Systems,Fairness is essential for human society  contributing to stability and productivity. Similarly  fairness is also the key for many multi-agent systems. Taking fairness into multi-agent learning could help multi-agent systems become both efficient and stable. However  learning efficiency and fairness simultaneously is a complex  multi-objective  joint-policy optimization. To tackle these difficulties  we propose FEN  a novel hierarchical reinforcement learning model. We first decompose fairness for each agent and propose fair-efficient reward that each agent learns its own policy to optimize. To avoid multi-objective conflict  we design a hierarchy consisting of a controller and several sub-policies  where the controller maximizes the fair-efficient reward by switching among the sub-policies that provides diverse behaviors to interact with the environment. FEN can be trained in a fully decentralized way  making it easy to be deployed in real-world applications. Empirically  we show that FEN easily learns both fairness and efficiency and significantly outperforms baselines in a variety of multi-agent scenarios.,Learning Fairness in Multi-Agent Systems

Jiechuan Jiang
Peking University

jiechuan.jiang@pku.edu.cn

Zongqing Lu∗
Peking University

zongqing.lu@pku.edu.cn

Abstract

Fairness is essential for human society  contributing to stability and productivity.
Similarly  fairness is also the key for many multi-agent systems. Taking fairness
into multi-agent learning could help multi-agent systems become both efﬁcient
and stable. However  learning efﬁciency and fairness simultaneously is a complex 
multi-objective  joint-policy optimization. To tackle these difﬁculties  we propose
FEN  a novel hierarchical reinforcement learning model. We ﬁrst decompose
fairness for each agent and propose fair-efﬁcient reward that each agent learns its
own policy to optimize. To avoid multi-objective conﬂict  we design a hierarchy
consisting of a controller and several sub-policies  where the controller maximizes
the fair-efﬁcient reward by switching among the sub-policies that provides diverse
behaviors to interact with the environment. FEN can be trained in a fully decentral-
ized way  making it easy to be deployed in real-world applications. Empirically 
we show that FEN easily learns both fairness and efﬁciency and signiﬁcantly
outperforms baselines in a variety of multi-agent scenarios.

1

Introduction

Fairness is essential for human society  contributing to stability and productivity. Similarly  fairness
is also the key for many multi-agent systems  e.g.  routing [1]  trafﬁc light control [2]  and cloud
computing [3]. More speciﬁcally  in routing  link bandwidth needs to be fairly allocated to packets to
achieve load balance; in trafﬁc light control  resources provided by infrastructure needs to be fairly
shared by vehicles; in cloud computing  resource allocation of virtual machines has to be fair to
optimize proﬁt.
Many game-theoretic methods [4  5  6] have been proposed for fair division in multi-agent systems 
which mainly focus on proportional fairness and envy-freeness. Most of them are in static settings 
while some [7  8  9] consider the dynamic environment. Recently  multi-agent reinforcement learning
(RL) has been successfully applied to multi-agent sequential decision-making  such as [10  11  12 
13  14  15  16]. However  most of them try to maximize the reward of each individual agent or the
shared reward among agents  without taking fairness into account. Only a few methods consider
fairness  but are handcrafted for different applications [17  18  19]  which all require domain-speciﬁc
knowledge and cannot be generalized. Some methods [20  21  22] aim to encourage cooperation in
social dilemmas but cannot guarantee fairness.
Taking fairness into multi-agent learning could help multi-agent systems become both efﬁcient and
stable. However  learning efﬁciency (system performance) and fairness simultaneously is a complex 
multi-objective  joint-policy optimization. To tackle these difﬁculties  we propose a novel hierarchical
RL model  FEN  to enable agents to easily and effectively learn both efﬁciency and fairness. First 
we decompose fairness for each agent and propose fair-efﬁcient reward that each agent learns its own
policy to optimize it. We prove that agents achieve Pareto efﬁciency and fairness is guaranteed in
inﬁnite-horizon sequential decision-making if all agents maximize their own fair-efﬁcient reward

∗Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

and learn the optimal policies. However  the conﬂicting nature between fairness and efﬁciency
makes it hard for a single policy to learn effectively. To overcome this  we then design a hierarchy 
which consists a controller and several sub-policies. The controller maximizes the fair-efﬁcient
reward by switching among the sub-policies which directly interact with the environment. One of the
sub-policies is designated to maximize the environmental reward  and other sub-policies are guided
by information-theoretic reward to explore diverse possible behaviors for fairness. Additionally 
average consensus  which is included in the fair-efﬁcient reward  coordinates the policies of agents in
fully decentralized multi-agent learning. By saying fully decentralized  we emphasize that there is no
centralized controller  agents exchange information locally  and they learn and execute based on only
local information.
We evaluate FEN in three classic scenarios  i.e.  job scheduling  the Mathew effect  and manufacturing
plant. It is empirically demonstrated that FEN obtains both fairness and efﬁciency and signiﬁcantly
outperforms existing methods. By ablation studies  we conﬁrm the hierarchy indeed helps agents to
learn more easily. Beneﬁted from distributed average consensus  FEN can learn and execute in a fully
decentralized way  making it easy to be deployed in real-world applications.

2 Related Work

Fairness. There are many existing works on fair division in multi-agent systems. Most of them
focus on static settings [4  5  6]  where the information of entire resources and agents are known and
ﬁxed  while some of them work on dynamic settings [7  8  9]  where resource availability and agents
are changing. For multi-agent sequential decision-making  a regularized maximin fairness policy is
proposed [8] to maximize the worst performance of agents while considering the overall performance 
and the policy is computed by linear programming or game-theoretic approach. However  none of
these works are learning approach. Some multi-agent RL methods [17  18  19] have been proposed
and handcrafted for resource allocation in speciﬁc applications  such as resource allocation on multi-
core systems[17]  sharing network resources among UAVs[18]  and balancing various resources in
complex logistics networks[19]. However  all these methods require domain-speciﬁc knowledge
and thus cannot be generalized. Some methods [20  21  22] are proposed to improve cooperation
in social dilemmas. In [20]  the reward is shaped for two-player Stag Hunt games  which could
be agents’ average reward in its multi-player version. In [20]  one agent’s reward is set to be the
weighted average reward of two agents in two-player Stag Hunt games to induce prosociality. By
extending the inequity aversion model  a shaped reward is designed in [21] to model agent’s envy
and guilt. A reward network is proposed in [22] to generate intrinsic reward for each agent  which
is evolved based on the group’s collective reward. Although cooperation in social dilemmas helps
improve agents’ sum reward  it does not necessarily mean the fairness is guaranteed.
The Matthew effect  summarized as the rich get richer and the poor get poorer  can be witnessed in
many aspects of human society [23]  as well as in multi-agent systems  such as preferential attachment
in networking [24  25] and mining process in blockchain systems [26]. The Matthew effect causes
inequality in society and also performance bottleneck in multi-agent systems. Learning fairness could
avoid the Matthew effect and help systems become stable and efﬁcient.
Multi-Agent RL. Many multi-agent RL models have been recently proposed  such as [10  11  12  13 
14  15  16]  but all of them only consider efﬁciency. CommNet [11] and ATOC [13] use continuous
communication for multi-agent cooperation. Opponent modeling [27  28] learns to reason about
other agents’ behaviors or minds for better cooperation or competition. MADDPG [10] is designed
for mixed cooperative-competitive environments. In these models  each agent only focuses on
optimizing its own local reward. Thus  more capable agents will obtain more rewards and fairness
is not considered. VDN [14]  QMIX [15]  and COMA [16] are designed for the scenario where all
agents jointly maximize a shared reward. The shared reward is not directly related to fairness. Even
if the shared reward is deﬁned as the sum of local rewards of all agents  we can easily see that higher
reward sum does not mean fairer.
Hierarchical RL. To solve more complex tasks with sparse rewards or long time horizons and to
speed up the learning process  hierarchical RL trains multiple levels of policies. The higher level
policies give goals or options to the lower level policies and only the lowest level applies actions to
the environment. So  the higher levels are able to plan over a longer time horizon or a more complex
task. Learning a decomposition of complex tasks into sub-goals are considered in [29  30  31]  while

2

learning options are considered in [32  33  34]. However  none of these hierarchical RL models can
be directly applied to learning both fairness and efﬁciency in multi-agent systems.

3 Methods

We propose Fair-Efﬁcient Network  FEN  to enable agents to learn both efﬁciency and fairness
in multi-agent systems. Unlike existing work  we decompose fairness for each agent and propose
fair-efﬁcient reward  and each agent learns its own policy to optimize it. However  optimizing the
two conﬂicting objectives is hard for a single learning policy. To this end  we propose a hierarchy
speciﬁcally designed for easing this learning difﬁculty. The hierarchy consists a controller and several
sub-policies  where the controller learns to select sub-policies and each sub-policy learns to interact
with the environment in a different way. Average consensus  which is included in the fair-efﬁcient
reward  coordinates agents’ policies and enables agents to learn in a fully decentralized way.

3.1 Fair-Efﬁcient Reward

n−1

i=1

¯u2

(ui−¯u)2

(cid:80)n

(cid:113) 1

In the multi-agent system we consider  there are n agents and limited resources in the environment.
The resources are non-excludable and rivalrous (common resources)  e.g.  CPU  memory  and network
bandwidth. At each timestep  the environmental reward r an agent obtains is only related to its
occupied resources at that timestep. We deﬁne the utility of agent i at timestep t as ui
j 
j=0 ri
which is the average reward over elapsed timesteps. We use the coefﬁcient of variation (CV) of agents’
utilities
to measure fairness [35]  where ¯u is average utility of all agents. A
system is said to be fairer if and only if the CV is smaller.
In multi-agent sequential decision-making  it is difﬁcult for an individual agent to optimize the CV
since it is not just related to the agent’s own policy  but the joint policies of all agents. However  as
the resources are limited  the upper bound of ¯u can be easily reached by self-interested agents. Thus 
¯u is hardly affected by an individual agent and the contribution of agent i to the variance could be
approximated as (ui − ¯u)2/¯u2. We decompose the fairness objective for each agent and propose the
fair-efﬁcient reward

(cid:80)t

t = 1
t

The fair-efﬁcient reward allows each agent to respond to the behaviors of other agents  which can be
summarized by ¯u. Therefore  ¯u can actually coordinate agents’ policies in decentralized multi-agent
learning.
Proposition 1. The optimal fair-efﬁcient policy set π∗ is Pareto efﬁcient in inﬁnite-horizon sequential
decision-making.
Proof. We prove by contradiction. We ﬁrst prove the resources must be fully occupied. Since
the decision-making is inﬁnite-horizon  the resources could be allocated in any proportion in the
time domain. Assume the resources are not fully used  there must exist another π(cid:48)  under which
each agent could occupy the remaining resources according to the ratio of ui/n¯u. Then  we have
|ui(cid:48)
i > Fi  which contradicts the
pre-condition that π∗ is optimal. We then prove π∗ is Pareto efﬁcient. Assume Pareto efﬁciency is
not achieved  there must exist ∀i  ui(cid:48) (cid:62) ui ∧∃i  ui(cid:48)
i=1 ui  which contradicts
the pre-condition that the resources are fully occupied.
Proposition 2. The optimal fair-efﬁcient policy set π∗ achieves equal allocation when the resources
are fully occupied.
Proof. We prove by contradiction. Assume the allocation is not equal when the resources are fully
occupied  ∃i  ui (cid:54)= ¯u. There must exist another π(cid:48)  under which agents that have ui > ¯u can give up

/¯u(cid:48) − 1| = |ui/¯u − 1|  but ¯u(cid:48)/c > ¯u/c. Thus  for each agent i  F (cid:48)
i=1 ui(cid:48)

> ui  so(cid:80)n

>(cid:80)n

3

¯ut/c

ˆri
t =

 +(cid:12)(cid:12)ui

t/¯ut − 1(cid:12)(cid:12)  
of the system  encouraging the agent to improve efﬁciency;(cid:12)(cid:12)ui
learns its own policy to maximize the objective Fi = E(cid:2)(cid:80)∞

where c is a constant that normalizes the numerator and is set to the maximum environmental reward
the agent obtains at a timestep. In the fair-efﬁcient reward  ¯ut/c can be seen as the resource utilization

deviation from the average  and the agent will be punished no matter it is above or below the average 
which leads to low variance;  is a small positive number to avoid zero denominator. Each agent i

t/¯ut − 1(cid:12)(cid:12) measures the agent’s utility
(cid:3)  where γ is the discount factor.

t=0 γtˆri
t

resources to make ui = ¯u. Then  for the remaining resources and other agents  this is an isomorphic
subproblem. According to Proposition 1  the resources will be fully occupied by other agents. After
that  we have F (cid:48)
i > Fi  which contradicts the
pre-condition that π∗ is optimal.

i > Fi. This process can be repeated until ∀i  F (cid:48)

3.2 Hierarchy

A learning policy could feel ambiguous while considering both fairness and efﬁciency since they
might conﬂict in some states. For example  if different behaviors of other agents cause the change of
¯u  an agent may need to perform different action at a same state to maximize its fair-efﬁcient reward.
However  this is hard for a single learned policy.
To overcome this difﬁculty  we design a hierarchy that
consists of a controller and several sub-policies param-
eterized respectively by θ and φ   illustrated in Figure 1.
The controller selects one of sub-policies by the sampled
index zt ∼ πθ(·|ot) based on the partial observation ot.
The controller receives the fair-efﬁcient reward ˆr and
acts at a lower temporal resolution than the sub-policies.
Every T timesteps  the controller chooses a sub-policy
and in the next T timesteps  the chosen sub-policy out-
puts actions to interact with the environment.
To obtain efﬁciency  we designate one of the sub-policies
parameterized by φ1 to maximize the reward r given by
the environment. For other sub-policies  we exploit an
information-theoretic objective to guide the sub-policies
to explore diverse possible behaviors for fairness.
From the perspective of the controller  these sub-policies
should be able to be distinguished from each other and
thus the controller can have more choices. Obviously  we
can not quantify the difference of sub-policies directly.
However  the experienced observations under a sub-policy could indirectly reﬂect the policy. The
more differences between sub-policies  the less the uncertainty of z is  given observation o under the
policy. That is to say  the mutual information I(Z; O) should be maximized by the sub-policy and
we take it as one term of the objective. On the other hand  to explore diverse possible behaviors  the
sub-policy should act as randomly as possible  so we also maximize the entropy between the action
and observation H(A|O). In summary  the objective of the sub-policy is to maximize

Figure 1: FEN architecture.

J(φ) = I(Z; O) + H(A|O)

= H(Z) − H(Z|O) + H(A|O)
≈ −H(Z|O) + H(A|O)
= Ez∼πθ o∼πφ[log p(z|o)] + Ea∼πφ[−p(a|o) log p(a|o)].

As H(Z) is only related to θ  H(Z) can be seen a constant and be neglected. The controller just
outputs the probability pθ(z|o)  and thus the ﬁrst term of the objective can be interpreted as that each
sub-policy tries to maximize the expected probability that it would be selected by the controller. To
maximize it  we can give sub-policies a reward ˜r = log pθ(z|o) at each timestep and use RL to train
the sub-policies. The second term can be treated as an entropy regularization  which is differentiable
and could be optimized by backpropagation.
The hierarchy reduces the difﬁculty of learning both efﬁciency and fairness. The controller focuses
on the fair-efﬁcient reward and learns to decide when to optimize efﬁciency or fairness by selecting
the sub-policy  without directly interacting with the environment. Sub-policy φ1 learns to optimize
the environmental reward  i.e.  efﬁciency. Other sub-policies learn diverse behaviors to meet the
controller’s demand of fairness. The fair-efﬁcient reward changes slowly since it is slightly affected
by immediate environmental reward the sub-policy obtains. Thus  the controller can plan over a long
time horizon to optimize both efﬁciency and fairness  while the sub-policies only optimize their own
objectives within the given time interval T .

4

controller....sub-policies(Gossip)3.3 Decentralized Training

The centralized policy has an advantage in coordinating all agents’ behaviors. However  the central-
ized policy is hard to train  facing the curse of dimensionality as the number of agents increases. FEN
is a decentralized policy in both training and execution. Although each agent only focuses on its own
fair-efﬁcient reward  they are coordinated by the average consensus on utility.
In the decentralized training  each agent need to perceive the average utility ¯u to know its current
utility deviation from the average. When the number of agents is small  it is easy to collect the utility
from each agent and compute the average. When the number of agents is large  it may be costly to
collect the utility from each agent in real-world applications. To deal with this  we adopt a gossip
algorithm for distributed average consensus [36]. Each agent i maintains the average utility ¯ui and
iteratively updates it by

¯ui(t + 1) = ¯ui(t) +

wij × (¯uj(t) − ¯ui(t)) 

(cid:88)

j∈Ni

where ¯ui(0) = ui  Ni is the set of neighboring agents in agent i’s observation  and the weight
wij = 1/(max{di  dj} + 1)  where the degree di = |Ni|. The gossip algorithm is distributed and
requires only limited communication between neighbors to estimate the average.
The training of FEN is detailed in Algo-
rithm 1. The controller and sub-policies
are trained both using PPO [37]. The con-
troller selects one sub-policy every T to in-
teract with the environment. The selected
sub-policy is updated based on the trajec-
tory during T . The controller is updated
based on the trajectory of every sub-policy
selection and its obtained fair-efﬁcient re-
ward during each episode.

Algorithm 1 FEN training
1: Initialize ui  ¯ui  the controller θ and sub-policies φ
2: for episode = 1  . . .  M do
3:
4:
5:

The chosen sub-policy φz acts to the environment
and gets the reward
if t%T = 0 then

The controller chooses one sub-policy φz
for t = 1  . . .   max-episode-length do

log pθ(z|ot)

if z = 1 

(cid:26)

else

rt

4 Experiments

Update φz using PPO
Update ¯ui (with gossip algorithm)
Calculate ˆri =
The controller reselects one sub-policy

+|ui/¯ui−1|

¯ui/c

6:
7:
8:
9:
10:
11:
12:
13:
14: end for

For the experiments  we design three sce-
narios as abstractions of job scheduling 
the Matthew effect  and manufacturing
plant  which are illustrated in Figure 2.
We demonstrate that by each agent decen-
tralizedly optimizing the fair-efﬁcient reward the multi-agent system could obtain a great balance
between efﬁciency and fairness  and that the hierarchy indeed helps to learn both fairness and ef-
ﬁciency more easily. In the experiments  we compare FEN against several baselines which have
different optimization objectives and are summarized as follows.

end if
end for
Update θ using PPO

• Independent agents are fully self-interested and each agent maximizes its expected sum
• Inequity Aversion agents receive a shaped reward ri − α
N−1

of discounted environmental rewards ψi = E(cid:2)(cid:80)∞

(cid:3).
(cid:80) max(ri − rj  0) to model the envy and guilt [21].

(cid:80) max(rj − ri  0) −

t=0 γtri
t

β

N−1

Figure 2: Illustration of experimental scenarios: job scheduling (left)  the Matthew effect (mid) 
manufacturing plant (right).

5

AgentResourcePac-manGhostAgentGems(cid:80) ψi/n [20].

• Avg agents take the average reward of agents as a shared reward and maximize avgψ =

• Min agents consider the worst performance of agents and maximize minψ.
• Min+αAvg agents consider both the worst performance and system performance and maxi-

mize the regularized maximin fairness [8]  minψ + αavgψ.

Note that in the last three baselines  all agents share the same optimization objective. To ensure the
comparison is fair  the basic hyperparameters are all the same for FEN and the baselines  which
are summarized in Appendix. The details about the experimental setting of each scenario are also
available in Appendix. Moreover  the code of FEN is at https://github.com/PKU-AI-Edge/FEN.

4.1

Job Scheduling

In this scenario of job scheduling  we investigate whether agents can learn to fairly and efﬁciently
share a resource. In a 5 × 5 grid world  there are 4 agents and 1 resource  illustrated in Figure 2 (left).
The resource’s location is randomly initialized in different episodes  but ﬁxed during an episode.
Each agent has a local observation that contains a square view with 3 × 3 grids centered at the agent
itself. At each timestep  each agent can move to one of four neighboring grids or stay at current grid.
If the agent occupies the resource (move to or stay at the resource’s location)  it receives a reward of
1  which could be seen as the job is scheduled  otherwise the reward is 0. Two agents cannot stay at a
same grid  making sure the resource can only be occupied by one agent at a timestep. We trained all
the methods for ﬁve runs with different random seeds. All experimental results are presented with
standard deviation (also in other two scenarios). Moreover  as all agents are homogeneous in the task 
we let agents share weights for all the methods.
Table 1 shows the performance of FEN and the baselines in terms of resource utilization (sum of
utility)  coefﬁcient of variation (CV) of utility (fairness)  min utility  and max utility. Independent
has the highest resource utilization  but also the worst CV. Self-interested Independent agents
would not give up the resource for fairness  which is also witnessed by that min utility is 0 and max
utility is 0.88  close to resource utilization. FEN has the lowest CV and the highest min utility. As
only one agent can use the resource at a time  fairly sharing the resource among agents inevitably
incurs the reduction of resource utilization. However  FEN can obtain much better fairness at a
subtle cost of resource utilization  and its resource utilization is slightly less than Independent.
Maximizing avgψ causes high CV since the average is not directly related to fairness. Its resource
utilization is also lower  because avgψ is determined by all the agents  making it hard for individual
agents to optimize by decentralized training. For the same reason  minψ is hard to be maximized by
individual agents. The regularized maximin fairness reward minψ + αavgψ is designed to obtain
a balance between fairness and resource utilization. However  due to the limitations of these two
objective terms  Min+αAvg is much worse than FEN. The CV of Inequity Aversion is better than
Independent but still worse than FEN  and the resource utilization is much lower  showing modeling
envy and guilt is not effective in fairness problems. Moreover  the hyperparameters α and β might
greatly affect the performance.

Table 1: Job scheduling
resource utilization

CV

Independent

Inequity Aversion

Min
Avg

FEN

Min+αAvg

FEN w/o Hierarchy
Min
Avg

centralized policy

Min+αAvg

w/ Hierarchy

Min
Avg

Min+αAvg

96% ±11%
72% ±9%
47% ±8%
84% ±7%
63% ±5%
90% ±5%
57% ±13%
12% ±4%
61% ±5%
19% ±5%
62% ±9%
84% ±6%
71% ±8%

6

0

1.57 ±0.26
0.04 ±0.01
0.69 ±0.17
0.07 ±0.02
0.30 ±0.07
0.05 ±0.03
0.75 ±0.13
0.39 ±0.03
0.09 ±0.03
0.17 ±0.05 0.18±0.03
0.22 ±0.06
0.10 ±0.03
0.82 ±0.11
1.46 ±0.14
0.57 ±0.05
0.31 ±0.11
0.61 ±0.14
0.28 ±0.09

min utility max utility
0.88 ±0.17
0.35 ±0.12
0.16 ±0.05
0.46 ±0.17
0.24 ±0.06
0.28 ±0.07
0.18 ±0.11
0.06 ±0.03
0.53 ±0.06
0.09 ±0.03
0.21 ±0.05
0.41 ±0.07
0.26 ±0.06

0.02 ±0.01
0.09 ±0.02
0.08 ±0.03
0.11 ±0.04

0
0

Figure 3: Learning curves of FEN and FEN
w/o Hierarchy in job scheduling.

Figure 4: Probability of selecting different
sub-policies in terms of (ui − ¯u)/¯u.

Since minψ  avgψ  and minψ + αavgψ do not depend on individual agents  but all agents  we adopt
a centralized policy  which takes all observations and outputs actions for all agents  to optimize each
objective. As shown in Table 1  the centralized policies for Min  Avg  and Min+αAvg are even worse
than their decentralized versions. Although the centralized policy could coordinate agents’ behaviors 
it is hard to train because of the curse of dimensionality. We also tried the centralized policy in the
Matthew effect and manufacturing plant  but it did not work and thus is omitted.
Does the hierarchy indeed help the learning of FEN? To verify the effect of the hierarchy  we trained a
single policy to maximize the fair-efﬁcient reward directly without the hierarchy. Figure 3 illustrates
the learning curves of FEN and FEN w/o Hierarchy  where we can see that FEN converges to a
much higher mean fair-efﬁcient reward than FEN w/o Hierarchy. As shown in Table 1  although
FEN w/o Hierarchy is fairer than other baselines  the resource utilization is mediocre. This is
because it is hard for a single policy to learn efﬁciency from the fair-efﬁcient reward. However  in
FEN  one of the sub-policies explicitly optimizes the environmental reward to improve the efﬁciency 
other sub-policies learn diverse fairness behaviors  and the controller optimizes fair-efﬁcient reward
by long time horizon planing. The hierarchy successfully decomposes the complex objective and
reduce the learning difﬁculty.
To further verify the effectiveness of the hierarchy  we use the hierarchy with other baselines. The
controller maximizes each own objective and the sub-policies are the same as FEN. Table 1 shows
their performance has a certain degree of improvement  especially the resource utilizations of Min
and Min+αAvg raise greatly and the CV of Min+αAvg reduces signiﬁcantly. That demonstrates the
hierarchy we proposed could reduce learning difﬁculty in many general cases with both global and
local objectives. However  these baselines with the hierarchy are still worse than FEN in both resource
utilization and CV  verifying the effectiveness of the fair-efﬁcient reward.
In order to analyze the behavior of the controller  in Figure 4 we visualize the probability of selecting
sub-policy φ1 and other sub-policies in terms of the utility deviation from average  (ui − ¯u)/¯u. It
shows when the agent’s utility is below average  the controller is more likely to select φ1 to occupy
the resources  and when the agent’s utility is above average  the controller tends to select other
sub-policies to improve fairness. The controller learns the sensible strategy based on the fair-efﬁcient
reward.

4.2 The Matthew Effect

In this scenario of the Matthew effect  we investigate whether agents can learn to mitigate/avoid the
Matthew effect. In the scenario  there are 10 pac-men (agents) initialized with different positions 
sizes  and speeds and also 3 stationary ghosts initialized at random locations  illustrated in Figure 2
(mid). Each pac-man can observe the nearest three other pac-men and the nearest one ghost. It could
move to one of four directions or stay at current position. Once the distance between the pac-men
and a ghost is less than the agent’s size  the ghost is consumed and the agent gets a reward 1. Then  a
new ghost will be generated at a random location. When the agent gets a reward  its size and speed
will increase correspondingly until the upper bounds are reached. In this scenario  the pac-man who
consumes more ghosts becomes larger and faster  making consume ghosts easier. So  there exists
inherent inequality in the setting. We trained all the models for ﬁve runs with different random seeds.
As pac-men are homogeneous  we let pac-men share weights for all the methods.

7

Figure 5: The Matthew effect

Figure 5 shows the performance of FEN and the baselines in terms of social welfare (total ghosts
consumed by all the pac-men)  CV  and min and max income (consumed ghosts) among pac-men 
episodes to converge. The detailed results are available in Appendix. Random pac-men take random
actions and their CV shows the inherent unfairness of this scenario. Min pac-men cannot learn
reasonable policies  because minψ is always closed to 0. Min+αAvg is only a little fairer than Avg
since the effect of minψ is very weak. Independent causes the Matthew effect as indicated by the
min (close to 0) and max (more than 200) income  where pac-man with initial larger size becomes
larger and larger and ghosts are mostly consumed by these larger pac-men. Inequity Aversion is
slightly fairer than Independent but lower social welfare.
Although Independent has the largest pac-man which consumes ghosts faster than others  this does
not necessarily mean they together consume ghosts fast. FEN is not only fairer than the baselines
but also has the highest social welfare  even higher than Independent. FEN pac-men have similar
sizes and consume more ghosts than the baselines. This demonstrates FEN is capable of tackling the
Matthew effect and helps social welfare increase. FEN w/o Hierarchy focuses more on fairness 
neglecting the efﬁciency as in the scenario of job scheduling. Moreover  learning without hierarchy is
much slower than FEN in this scenario  as illustrated in Figure 6. FEN w/o Hierarchy takes about
6000 episodes  while FEN takes only about 300 episodes  conﬁrming that the hierarchy indeed speeds
up the training.
Does distributed average consensus affect the perfor-
mance of FEN? Instead of using the centrally computed
average utility  we employ the gossip algorithm to es-
timate the average utility  where each agent only ex-
changes information with the agents in its observation.
As shown in Figure 5  FEN w/ Gossip performs equiv-
alently to FEN with only slight variation on each perfor-
mance metric. The learning curve of FEN w/ Gossip
is also similar to FEN  as illustrated in Figure 6. These
conﬁrm that FEN can be trained in a fully decentralized
way.
Do sub-policies really learn something useful? To an-
swer this question  after the training of FEN  we keep the
learned weights θ and φ1 and replace other sub-policies
with a random sub-policy. Once the controller chooses
other sub-policies instead of φ1  the agent will perform
random actions. In this FEN w/ random sub-policy  the min income become lower than FEN and
CV becomes higher  because the random sub-policy cannot provide fairness behavior the controller
requests. To investigate the difference of learned sub-policies and random sub-policy  we ﬁx the three
ghosts as a triangle at the center of the ﬁeld and visualize the distribution of an agent’s positions
under each sub-policy  as illustrated in Figure 7. It is clear that the learned sub-policies keep away
from the three ghosts for fairness and their distributions are distinct  concentrated at different corners 
verifying the effect of the information-theoretic reward.

Figure 6: Learning curves of FEN  FEN w/
Gossip  and FEN w/o Hierarchy in the
Matthew effect.

4.3 Manufacturing Plant

In this scenario of manufacturing plant  we investigate whether agents with different needs can learn
to share different types of resources and increase the production in a manufacturing plant. In a 8 × 8

8

Figure 7: Visualization of learned sub-policies (left three) and random sub-policy (right) in the
Matthew effect.

grid world  there are 5 agents and 8 gems  as illustrated in Figure 2 (right). The gems belong to
three types (y  g  b). Each agent has a local observation that contains a square view with 5 × 5 grids
centered at the agent itself  and could move to one of four neighboring grids or stay. When an agent
moves to the grid of one gem  the agent collects the gem and gets a reward of 0.01  and then a new
gem with random type and random location is generated. The total number of gems is limited  and
when all the gems are collected by the agents  the game ends. Each agent has a unique requirement of
numbers for the three types of gems to manufacture a unique part of the product and receive a reward
1. Each product is assembled by the ﬁve unique parts manufactured by the ﬁve agents  respectively.
So  the number of manufactured products is determined by the least part production among the agents.
Due to the heterogeneity  we let each agent learn its own weights for FEN and the baselines.
Table 2 shows the performance of FEN and the baselines in terms of resource utilization (the ratio
of the number of gems consumed to manufacture the products over the total number of gems)  CV 
number of products (minimum number of manufactured parts among agents)  and max number of
manufactured parts among agents. In this scenario  agents need to learn to collect the right gems and
then to balance the parts manufactured by each agent (i.e.  manufacturing similar large number of
parts)  because the unused collected gems and redundant parts will be wasted. FEN manufactures
the most products  more than two times than the baselines. The more products are assembled  the
higher the resource utilization is. Thus  FEN also has the highest resource utilization. Moreover  FEN
is also the fairest one. Although FEN w/o Hierarchy agents are fairer than other baselines  they all
manufacture less parts and hence eventually less products. Avg agents assemble the least products 
though one agent manufactures the largest number of parts  resulting in serious waste.

Table 2: Manufacturing plant

resource utilization

CV

no. products max parts

28% ±5%
27% ±6%
29% ±6%
13% ±3%
34% ±6%
82% ±5%
22% ±3%

0.38 ±0.08
0.27 ±0.06
0.26 ±0.01
0.63 ±0.07
0.28 ±0.01
0.10 ±0.03
0.18 ±0.07

19 ±3
19 ±4
20 ±4
9 ±2
23 ±4
48 ±3
15 ±1

58 ±8
42 ±7
41 ±7
71 ±9
45 ±7
63 ±3
24 ±4

Independent

Inequity Aversion

FEN w/o Hierarchy

Min
Avg

FEN

Min+αAvg

5 Conclusion

We have proposed FEN  a novel hierarchical reinforcement learning model to learn both fairness and
efﬁciency  driven by fair-efﬁcient reward  in multi-agent systems. FEN consists of one controller and
several sub-policies  where the controller learns to optimize the fair-efﬁcient reward  one sub-policy
learns to optimize the environmental reward  and other sub-policies learn to provide diverse fairness
behaviors guided by the derived information-theoretic reward. FEN can learn and execute in a fully
decentralized way  coordinated by average consensus. It is empirically demonstrated that FEN easily
learns both fairness and efﬁciency and signiﬁcantly outperforms baselines in a variety of multi-agent
scenarios including job scheduling  the Matthew effect  and manufacturing plant.

Acknowledgments

This work was supported in part by Huawei Noah’s Ark Lab  Peng Cheng Lab  and NSF China under
grant 61872009.

9

References
[1] Jiechuan Jiang  Chen Dun  and Zongqing Lu. Graph convolutional reinforcement learning for multi-agent

cooperation. arXiv preprint arXiv:1810.09202  2018.

[2] Bram Bakker  Shimon Whiteson  Leon Kester  and Frans CA Groen. Trafﬁc light control by multiagent
reinforcement learning systems. In Interactive Collaborative Information Systems  pages 475–510. 2010.

[3] Dorian Minarolli and Bernd Freisleben. Virtual machine resource allocation in cloud computing via

multi-agent fuzzy control. In International Conference on Cloud and Green Computing  2013.

[4] Steven de Jong  K Tuyls  Katja Verbeeck  and Nico Roos. Considerations for fairness in multi-agent

systems. In ALAMAS  pages 104–110. 2007.

[5] Ariel D Procaccia. Thou shalt covet thy neighbor’s cake. In International Joint Conference on Artiﬁcial

Intelligence (IJCAI)  2009.

[6] Yiling Chen  John K Lai  David C Parkes  and Ariel D Procaccia. Truth  justice  and cake cutting. Games

and Economic Behavior  77(1):284–297  2013.

[7] Ian Kash  Ariel D Procaccia  and Nisarg Shah. No agent left behind: Dynamic fair division of multiple

resources. Journal of Artiﬁcial Intelligence Research  51:579–603  2014.

[8] Chongjie Zhang and Julie A Shah. Fairness in multi-agent sequential decision-making. In Advances in

Neural Information Processing Systems (NeurIPS)  2014.

[9] Aurélie Beynier  Nicolas Maudet  and Anastasia Damamme. Fairness in multiagent resource allocation
with dynamic and partial observations. In International Joint Conference on Autonomous Agents and
Multiagent Systems (AAMAS)  2018.

[10] Ryan Lowe  Yi Wu  Aviv Tamar  Jean Harb  OpenAI Pieter Abbeel  and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing
Systems (NeurIPS)  2017.

[11] Sainbayar Sukhbaatar  Rob Fergus  et al. Learning multiagent communication with backpropagation. In

Advances in Neural Information Processing Systems (NeurIPS)  2016.

[12] Yaodong Yang  Rui Luo  Minne Li  Ming Zhou  Weinan Zhang  and Jun Wang. Mean ﬁeld multi-agent

reinforcement learning. In International Conference on Machine Learning (ICML)  2018.

[13] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.

Advances in Neural Information Processing Systems (NeurIPS)  2018.

[14] Peter Sunehag  Guy Lever  Audrunas Gruslys  Wojciech Marian Czarnecki  Vinicius Zambaldi  Max
Jaderberg  Marc Lanctot  Nicolas Sonnerat  Joel Z Leibo  Karl Tuyls  et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296  2017.

[15] Tabish Rashid  Mikayel Samvelyan  Christian Schroeder de Witt  Gregory Farquhar  Jakob Foerster  and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. arXiv preprint arXiv:1803.11485  2018.

[16] Jakob Foerster  Gregory Farquhar  Triantafyllos Afouras  Nantas Nardelli  and Shimon Whiteson. Counter-

factual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2018.

[17] Rahul Jain  Preeti Ranjan Panda  and Sreenivas Subramoney. Cooperative multi-agent reinforcement
learning-based co-optimization of cores  caches  and on-chip network. ACM Transactions on Architecture
and Code Optimization  14(4):32  2017.

[18] Jingjing Cui  Yuanwei Liu  and Arumugam Nallanathan. Multi-agent reinforcement learning based resource

allocation for uav networks. arXiv preprint arXiv:1810.10408  2018.

[19] Xihan Li  Jia Zhang  Jiang Bian  Yunhai Tong  and Tie-Yan Liu. A cooperative multi-agent reinforcement
learning framework for resource balancing in complex logistics network. arXiv preprint arXiv:1903.00714 
2019.

[20] Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve generalized stag hunts better
than selﬁsh ones. In International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) 
2018.

10

[21] Edward Hughes  Joel Z Leibo  Matthew Phillips  Karl Tuyls  Edgar Dueñez-Guzman  Antonio García
Castañeda  Iain Dunning  Tina Zhu  Kevin McKee  Raphael Koster  et al. Inequity aversion improves
cooperation in intertemporal social dilemmas. In Advances in Neural Information Processing Systems
(NeurIPS)  2018.

[22] Jane X Wang  Edward Hughes  Chrisantha Fernando  Wojciech M Czarnecki  Edgar A Duéñez-Guzmán 
and Joel Z Leibo. Evolving intrinsic motivations for altruistic behavior. In International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS)  2019.

[23] Thijs Bol  Mathijs de Vaan  and Arnout van de Rijt. The matthew effect in science funding. Proceedings of

the National Academy of Sciences  115(19):4887–4890  2018.

[24] Matjaž Perc.

The matthew effect in empirical data.

11(98):20140378  2014.

Journal of The Royal Society Interface 

[25] Yichuan Jiang and Zhichuan Huang. The rich get richer: Preferential attachment in the task allocation of
cooperative networked multiagent systems with resource caching. IEEE Transactions on Systems  Man 
and Cybernetics-Part A: Systems and Humans  42(5):1040–1052  2012.

[26] Yulong Zeng and Song Zuo. The matthew effect in computation contests: High difﬁculty may lead to 51%

dominance. arXiv preprint arXiv:1902.09089  2019.

[27] Zhang-Wei Hong  Shih-Yang Su  Tzu-Yun Shann  Yi-Hsiang Chang  and Chun-Yi Lee. A deep policy
inference q-network for multi-agent systems. In International Conference on Autonomous Agents and
MultiAgent Systems (AAMAS)  2018.

[28] Neil C Rabinowitz  Frank Perbet  H Francis Song  Chiyuan Zhang  SM Eslami  and Matthew Botvinick.

Machine theory of mind. arXiv preprint arXiv:1802.07740  2018.

[29] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in Neural Information

Processing Systems (NeurIPS)  1993.

[30] Alexander Sasha Vezhnevets  Simon Osindero  Tom Schaul  Nicolas Heess  Max Jaderberg  David Silver 
In International

and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.
Conference on Machine Learning (ICML)  2017.

[31] Oﬁr Nachum  Shixiang Shane Gu  Honglak Lee  and Sergey Levine. Data-efﬁcient hierarchical reinforce-

ment learning. In Advances in Neural Information Processing Systems (NeurIPS)  2018.

[32] Richard S Sutton  Doina Precup  and Satinder Singh. Between mdps and semi-mdps: A framework for

temporal abstraction in reinforcement learning. Artiﬁcial intelligence  112(1-2):181–211  1999.

[33] Pierre-Luc Bacon  Jean Harb  and Doina Precup. The option-critic architecture. In AAAI Conference on

Artiﬁcial Intelligence (AAAI)  2017.

[34] Kevin Frans  Jonathan Ho  Xi Chen  Pieter Abbeel  and John Schulman. Meta learning shared hierarchies.

arXiv preprint arXiv:1710.09767  2017.

[35] Rajendra K Jain  Dah-Ming W Chiu  and William R Hawe. A quantitative measure of fairness and

discrimination. Technical report  1984.

[36] Lin Xiao  Stephen Boyd  and Seung-Jean Kim. Distributed average consensus with least-mean-square

deviation. Journal of parallel and distributed computing  67(1):33–46  2007.

[37] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

11

Appendix

Hyperparameters

In the experiments  we use PPO for every RL agent. The PPO structure keeps same for the controller and
sub-policies of FEN  and also for the baselines. The value network and policy network are MLPs with two
256-unit hidden layers and ReLU activation. The learning rates for the value network and policy network of
PPO are 10−3 and 3 × 10−4  respectively. We trained all networks using Adam optimizer. The discounted
factor is γ = 0.98. For Min+αAvg agents  α = 0.01. For Inequity Aversion agents  α = 5 and β = 0.05 
the setting used in [21]. For FEN  the number of the sub-policies is 4   in the fair-efﬁcient reward is set to 0.1.
T is 25  50  and 50 in job scheduling  the Matthew effect  and manufacturing plant  respectively.

Experimental Settings

In the scenario of job scheduling  we trained all the models for ﬁve runs with different random seeds  each
episode contains 1000 timesteps.
In the scenario of the Matthew effect  the size and speed of pac-man are randomly initiated between (0.01  0.04)
and (0.018  0.042)  respectively. Each time after a pac-men consumes a ghost  its size will increase by 0.005
and the speed will increase by 0.004. The max size is 0.15 and the max speed is 0.13. We trained all the models
for ﬁve runs with different random seeds  where each episode contains 1000 timesteps.
In the scenario of manufacturing plant  the total number of gems is 700. The unique requirements of numbers
of three types of gems (Ly  Lg  Lb) are (2  1  0)  (1  0  1)  (0  1  1)  (1  1  0)  (0  1  2)  respectively  for the ﬁve
agents. We trained all the models for ﬁve runs with different random seeds  where each episode ends after all the
gems are collected by agents.
All the experiments were conducted on Dell XPS desktops with i7-8700k 6-Core processors and 1080TI GPUs.

Experimental Results

The details of the experimental results in the Matthew effect are shown in Table 3.

Table 3: The Matthew effect

social welfare

CV

min income max income episodes

Random

Independent

Inequity Aversion

Min
Avg

FEN

Min+αAvg

FEN w/ Gossip

FEN w/o Hierarchy

FEN w/ Random Sub-policy

22 ±10
202 ±33
152 ±18

7±4

126±18
103±16
94±3
95±4
26±1
99±6

-

100
1000
6000
1000
2000
300
300
6000

-

84 ±30
791 ±62
702 ±90
18 ±8
527±113
441±75
830±22
841±55
251±12
834±47

0.93 ±0.25
0.86 ±0.11
0.80 ±0.16
2.04 ±0.66
0.86 ±0.21
0.85 ±0.18
0.06 ±0.01
0.07 ±0.01
0.06 ±0.04
0.08 ±0.02

1
1
2
0
2
1±1
79±2
76±4
23±2
66±7

12

,Jiechuan Jiang
Zongqing Lu