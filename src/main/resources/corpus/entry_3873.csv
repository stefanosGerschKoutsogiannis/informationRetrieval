2013,On Decomposing the Proximal Map,The proximal map is the key step in gradient-type algorithms  which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures  this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory.,On Decomposing the Proximal Map

Department of Computing Science  University of Alberta  Edmonton AB T6G 2E8  Canada

yaoliang@cs.ualberta.ca

Yaoliang Yu

Abstract

The proximal map is the key step in gradient-type algorithms  which have be-
come prevalent in large-scale high-dimensional problems. For simple functions
this proximal map is available in closed-form while for more complicated func-
tions it can become highly nontrivial. Motivated by the need of combining regu-
larizers to simultaneously induce different types of structures  this paper initiates
a systematic investigation of when the proximal map of a sum of functions de-
composes into the composition of the proximal maps of the individual summands.
We not only unify a few known results scattered in the literature but also discover
several new decompositions obtained almost effortlessly from our theory.

Introduction

1
Regularization has become an indispensable part of modern machine learning algorithms. For ex-
ample  the (cid:96)2-regularizer for kernel methods [1] and the (cid:96)1-regularizer for sparse methods [2] have
led to immense successes in various ﬁelds. As real data become more and more complex  different
types of regularizers  usually nonsmooth functions  have been designed. In many applications  it
is thus desirable to combine regularizers  usually taking their sum  to promote different structures
simultaneously.
Since many interesting regularizers are nonsmooth  they are harder to optimize numerically  es-
pecially in large-scale high-dimensional settings. Thanks to recent advances [3–5]  gradient-type
algorithms have been generalized to take nonsmooth regularizers explicitly into account. And due
to their cheap per-iteration cost (usually linear-time)  these algorithms have become prevalent in
many ﬁelds recently. The key step of such gradient-type algorithms is to compute the proximal map
(of the nonsmooth regularizer)  which is available in closed-form for some speciﬁc regularizers.
However  the proximal map becomes highly nontrivial when we start to combine regularizers.
The main goal of this paper is to systematically investigate when the proximal map of a sum of
functions decomposes into the composition of the proximal maps of the individual functions  which
we simply term prox-decomposition. Our motivation comes from a few known decomposition
results scattered in the literature [6–8]  all in the form of our interest. The study of such prox-
decompositions is not only of mathematical interest  but also the backbone of popular gradient-type
algorithms [3–5]. More importantly  a precise understanding of this decomposition will shed light
on how we should combine regularizers  taking computational efforts explicitly into account.
After setting the context in Section 2  we motivate the decomposition rule with some justiﬁca-
tions  as well as some cautionary results. Based on a sufﬁcient condition presented in Section 3.1 
we study how “invariance” of the subdifferential of one function would lead to nontrivial prox-
decompositions. Speciﬁcally  we prove in Section 3.3 that when the subdifferential of one function
is scaling invariant  then the prox-decomposition always holds if and only if another function is
radial—which is  quite unexpectedly  exactly the same condition proven recently for the validity of
the representer theorem in the context of kernel methods [9  10]. The generalization to cone invari-
ance is considered in Section 3.4  and enables us to recover most known prox-decompositions  as
well as some new ones falling out quite naturally.

1

Our notations are mostly standard. We use ιC(x) for the indicator function that takes 0 if x ∈ C
and ∞ otherwise  and 1C(x) for the indicator that takes 1 if x ∈ C and 0 otherwise. The symbol
Id stands for the identity map and the extended real line R ∪ {∞} is denoted as ¯R. Throughout the
paper we denote ∂f (x) as the subdifferential of the function f at point x.

2 Preliminary
Let our domain be some (real) Hilbert space (H (cid:104)· ·(cid:105))  with the induced Hilbertian norm (cid:107) · (cid:107). If
needed  we will assume some ﬁxed orthonormal basis {ei}i∈I is chosen for H  so that for x ∈ H
we are able to refer to its “coordinates” xi = (cid:104)x  ei(cid:105).
For any closed convex proper function f : H → ¯R  we deﬁne its Moreau envelop as [11]

∀y ∈ H  Mf (y) = min
x∈H

1

2(cid:107)x − y(cid:107)2 + f (x) 

(1)

and the related proximal map

Pf (y) = argmin

(2)
Due to the strong convexity of (cid:107) · (cid:107)2 and the closedness and convexity of f  Pf (y) always exists
and is unique. Note that Mf : H → R while Pf : H → H. When f = ιC is the indicator of some
closed convex set C  the proximal map reduces to the usual projection. Perhaps the most interesting
property of Mf   known as Moreau’s identity  is the following decomposition [11]

x∈H

1

2(cid:107)x − y(cid:107)2 + f (x).

(3)
where f∗(z) = supx (cid:104)x  z(cid:105)− f (x) is the Fenchel conjugate of f. It can be shown that Mf is Frechét
differentiable  hence taking derivative w.r.t. y in both sides of (3) yields

Mf (y) + Mf∗ (y) = 1

2(cid:107)y(cid:107)2 

Pf (y) + Pf∗ (y) = y.

(4)

3 Main Results
Our main goal is to investigate and understand the equality (we always assume f + g (cid:54)≡ ∞)

?= Pf ◦ Pg

?= Pg ◦ Pf  

Pf +g

2f +P−1

(5)
where f  g ∈ Γ0  the set of all closed convex proper functions on H  and f ◦ g denotes the mapping
composition. We present ﬁrst some cautionary results.
2g )−1◦2Id.
Note that Pf = (Id+∂f )−1  hence under minor technical assumptions Pf +g = (P−1
However  computationally this formula is of little use. On the other hand  it is possible to develop
forward-backward splitting procedures1 to numerically compute Pf +g  using only Pf and Pg as
subroutines [12]. Our focus is on the exact closed-form formula (5).
Interestingly  under some
“shrinkage” assumption  the prox-decomposition (5)  even if not necessarily hold  can still be used
in subgradient algorithms [13].
Our ﬁrst result is encouraging:
Proposition 1. If H = R  then for any f  g ∈ Γ0  there exists h ∈ Γ0 such that Ph = Pf ◦ Pg.
In fact  Moreau [11  Corollary 10.c] proved that P : H → H is a proximal map iff it
Proof:
is nonexpansive and it is the subdifferential of some convex function in Γ0. Although the latter
condition in general is not easy to verify  it reduces to monotonic increasing when H = R (note that
P must be continuous). Since both Pf and Pg are increasing and nonexpansive  it follows easily that
so is Pf ◦ Pg  hence the existence of h ∈ Γ0 so that Ph = Pf ◦ Pg.
In a general Hilbert space H  we again easily conclude that the composition Pf ◦ Pg is always a
nonexpansion  which means that it is “close” to be a proximal map. This justiﬁes the composition
Pf ◦ Pg as a candidate for the decomposition of Pf +g. However  we note that Proposition 1 indeed
can fail already in R2:

1In some sense  this procedure is to compute Pf +g ≈ limt→∞(Pf ◦ Pg)t  modulo some intermediate steps.

Essentially  our goal is to establish the one-step convergence of that iterative procedure.

2

Example 1. Let H = R2. Let f = ι{x1=x2} and g = ι{x2=0}. Clearly both f and g are in Γ0. The
proximal maps in this case are simply projections: Pf (x) = ( x1+x2
) and Pg(x) = (x1  0).
Therefore Pf (Pg(x)) = ( x1
2 ). We easily verify that the inequality

  x1+x2

2   x1

2

2

(cid:107)Pf (Pg(x)) − Pf (Pg(y))(cid:107)2 ≤ (cid:104)Pf (Pg(x)) − Pf (Pg(y))  x − y(cid:105)

is not always true  contradiction if Pf ◦ Pg was a proximal map [11  Eq. (5.3)].
Even worse  when Proposition 1 does hold  in general we can not expect the decomposition (5) to
be true without additional assumptions.
Example 2. Let H = R and q(x) = 1
Pq ◦ Pq = 1
Nevertheless  as we will see  the equality in (5) does hold in many scenarios  and an interesting
theory can be suitably developed.

3 Id = Pq+q. We will give an explanation for this failure of composition shortly.

It is easily seen that Pλq(x) = 1

1+λ x. Therefore

4 Id (cid:54)= 1

2 x2.

3.1 A Sufﬁcient Condition
We start with a sufﬁcient condition that yields (5). This result  although easy to obtain  will play a
key role in our subsequent development.
Using the ﬁrst order optimality condition and the deﬁnition of the proximal map (2)  we have

Pf +g(y) − y + ∂(f + g)(Pf +g(y)) (cid:51) 0
Pg(y) − y + ∂g(Pg(y)) (cid:51) 0
Pf (Pg(y)) − Pg(y) + ∂f (Pf (Pg(y))) (cid:51) 0.

(6)
(7)
(8)

(9)

Adding the last two equations we obtain

Pf (Pg(y)) − y + ∂g(Pg(y)) + ∂f (Pf (Pg(y))) (cid:51) 0.

Comparing (6) and (9) gives us
Theorem 1. A sufﬁcient condition for Pf +g = Pf ◦ Pg is

∀ x ∈ H  ∂g(Pf (x)) ⊇ ∂g(x).

(10)
Proof: Let x = Pg(y). Then by (9) and the subdifferential rule ∂(f + g) ⊇ ∂f + ∂g we verify that
Pf (Pg(y)) satisﬁes (6)  hence follows Pf +g = Pf ◦ Pg since the proximal map is single-valued.
We note that a special form of our sufﬁcient condition has appeared in the proof of [8  Theorem 1] 
whose main result also follows immediately from our Theorem 4 below. Let us ﬁx f  and deﬁne

Kf = {g ∈ Γ0 : f + g (cid:54)≡ ∞  (f  g) satisfy (10)}.

Immediately we have
Proposition 2. For any f ∈ Γ0  Kf is a cone. Moreover  if g1 ∈ Kf   g2 ∈ Kf   f + g1 + g2 (cid:54)≡ ∞
and ∂(g1 + g2) = ∂g1 + ∂g2  then g1 + g2 ∈ Kf too.
The condition ∂(g1 +g2) = ∂g1 +∂g2 in Proposition 2 is purely technical; it is satisﬁed when  say g1
is continuous at a single  arbitrary point in dom g1 ∩ dom g2. For comparison purpose  we note that
it is not clear how Pf +g+h = Pf ◦ Pg+h would follow from Pf +g = Pf ◦ Pg and Pf +h = Pf ◦ Ph.
This is the main motivation to consider the sufﬁcient condition (10). In particular
Deﬁnition 1. We call f ∈ Γ0 self-prox-decomposable (s.p.d.) if f ∈ Kαf for all α > 0.
For any s.p.d. f  since Kf is a cone  βf ∈ Kαf for all α  β ≥ 0. Consequently  P(α+β)f =
Pβf ◦ Pαf = Pαf ◦ Pβf .
is to require f ∈ Kf   from which we conclude that
Remark 1. A weaker deﬁnition for s.p.d.
βf ∈ Kf for all β ≥ 0  in particular P(m+n)f = Pnf ◦ Pmf = Pmf ◦ Pnf for all natural numbers
m and n. The two deﬁnitions coincide for positive homogeneous functions. We have not been able
to construct a function that satisﬁes this weaker deﬁnition but not the stronger one in Deﬁnition 1.
Example 3. We easily verify that all afﬁne functions (cid:96) = (cid:104)·  a(cid:105) + b are s.p.d.  in fact  they are the
only differentiable functions that are s.p.d.  which explains why Example 2 must fail. Another trivial
class of s.p.d. functions are projectors to closed convex sets. Also  univariate gauges2 are s.p.d.  due
to Theorem 4 below. Some multivariate s.p.d. functions are given in Remark 5 below.

2A gauge is a positively homogeneous convex function that vanishes at the origin.

3

The next example shows that (10) is not necessary.
Example 4. Fix z ∈ H  f = ι{z}  and g ∈ Γ0 with full domain. Clearly for any x ∈ H  Pf +g(x) =
z = Pf [Pg(x)]. However  since x is arbitrary  ∂g(Pf (x)) = ∂g(z) (cid:54)⊇ ∂g(x) if g is not linear.
On the other hand  if f  g are differentiable  then we actually have equality in (10)  which is clearly
necessary in this case. Since convex functions are almost everywhere differentiable (in the interior
of their domain)  we expect the sufﬁcient condition (10) to be necessary “almost everywhere” too.
Thus we see that the key for the decomposition (5) to hold is to let the proximal map of f and the
subdifferential of g “interact well” in the sense of (10). Interestingly  both are fully equivalent to the
function itself.
Proposition 3 ([11  §8]). Let f  g ∈ Γ0. f = g + c for some c ∈ R ⇐⇒ ∂f ⊆ ∂g ⇐⇒ Pf = Pg.
Proof: The ﬁrst implication is clear. The second follows from the optimality condition Pf =
(Id + ∂f )−1. Lastly  Pf = Pg implies that Mf∗ = Mg∗ − c for some c ∈ R (by integration).
Conjugating we get f = g + c for some c ∈ R.
Therefore some properties of the proximal map will transfer to some properties of the function f
itself  and vice versa. The next result is easy to obtain  and appeared essentially in [14].
Proposition 4. Let f ∈ Γ0 and x ∈ H be arbitrary  then
i). Pf is odd iff f is even;
ii). Pf (U x) = U Pf (x) for all unitary U iff f (U x) = f (x) for all unitary U;
iii). Pf (Qx) = QPf (x) for all permutation Q (under some ﬁxed basis) iff f is permutation invari-

ant  that is f (Qx) = f (x) for all permutation Q.

In the following  we will put some invariance assumptions on the subdifferential of g and accordingly
ﬁnd the right family of f whose proximal map “respects” that invariance. This way we will meet
(10) by construction therefore effortlessly have the decomposition (5).

3.2 No Invariance
To begin with  consider ﬁrst the trivial case where no invariance on the subdifferential of g is as-
sumed. This is equivalent as requiring (10) to hold for all g ∈ Γ0. Not surprisingly  we end up with
a trivial choice of f.
Theorem 2. Fix f ∈ Γ0. Pf +g = Pf ◦ Pg for all g ∈ Γ0 if and only if

• dim(H) ≥ 2; f ≡ c or f = ι{w} + c for some c ∈ R and w ∈ H;
• dim(H) = 1 and f = ιC + c for some closed and convex set C and c ∈ R.

Proof: ⇐: Straightforward calculations  see [15] for details.
⇒: We ﬁrst prove that f is constant on its domain even when g is restricted to indicators. Indeed 
let x ∈ dom f and take g = ι{x}. Then x = Pf +g(x) = Pf [Pg(x)] = Pf (x)  meaning that
x ∈ argmin f. Since x ∈ dom f is arbitrary  f is constant on its domain. The case dim(H) = 1 is
complete. We consider the other case where dim(H) ≥ 2 and dom f contains at least two points.
If dom f (cid:54)= H  there exists z (cid:54)∈ dom f such that Pf (z) = y for some y ∈ dom f  and closed
convex set C ∩ dom f (cid:54)= ∅ with y (cid:54)∈ C (cid:51) z. Let g = ιC we obtain Pf +g(z) ∈ C ∩ dom f while
Pf (Pg(z)) = Pf (z) = y (cid:54)∈ C  contradiction.
Observe that the decomposition (5) is not symmetric in f and g  also reﬂected in the next result:
Theorem 3. Fix g ∈ Γ0. Pf +g = Pf ◦ Pg for all f ∈ Γ0 iff g is a continuous afﬁne function.
Proof: ⇒: If g = (cid:104)·  a(cid:105) + c  then Pg(x) = x − a. Easy calculation reveals that Pf +g(x) =
Pf (x − a) = Pf [Pg(x)].
⇐: The converse is true even when f is restricted to continuous linear functions. Indeed  let a ∈ H
be arbitrary and consider f = (cid:104)·  a(cid:105). Then Pf +g(x) = Pg(x − a) = Pf (Pg(x)) = Pg(x) − a.
Letting a = x yields Pg(x) = x + Pg(0) = P(cid:104)· −Pg(0)(cid:105)(x). Therefore by Proposition 3 we know g
is equal to a continuous afﬁne function.

4

Naturally  the next step is to put invariance assumptions on the subdifferential of g  effectively
restricting the function class of g. As a trade off  the function class of f  that satisﬁes (10)  becomes
larger so that nontrivial results will arise.

3.3 Scaling Invariance
The ﬁrst invariance property we consider is scaling-invariance. What kind of convex functions have
their subdifferential invariant to (positive) scaling? Assuming 0 ∈ dom g and by simple integration

(cid:90) t

(cid:90) t

g(tx) − g(0) =

g(cid:48)(sx)ds =

(cid:104)∂g(sx)  x(cid:105) ds = t · [g(x) − g(0)] 

0

0

where the last equality follows from the scaling invariance of the subdifferential of g. Therefore  up
to some additive constant  g is positive homogeneous (p.h.). On the other hand  if g ∈ Γ0 is p.h.
(automatically 0 ∈ dom g)  then from deﬁnition we verify that ∂g is scaling-invariant. Therefore 
under the scaling-invariance assumption  the right function class for g is the set of all p.h. functions
in Γ0  up to some additive constant. Consequently  the right function class for f is to have the
proximal map Pf (x) = λ · x for some λ ∈ [0  1] that may depend on x as well3. The next theorem
completely characterizes such functions.
Theorem 4. Let f ∈ Γ0. Consider the statements
i). f = h((cid:107) · (cid:107)) for some increasing function h : R+ → ¯R;
ii). x ⊥ y =⇒ f (x + y) ≥ f (y);
iii). Pf (u) = λ · u for some λ ∈ [0  1] (that may itself depend on u);
iv). 0 ∈ dom f and Pf +κ = Pf ◦ Pκ for all p.h. (up to some additive constant) function κ ∈ Γ0.
Then we have i) =⇒ ii) ⇐⇒ iii) ⇐⇒ iv). Moreover  when dim(H) ≥ 2  ii) =⇒ i) as well  in
which case Pf (u) = Ph((cid:107)u(cid:107))/(cid:107)u(cid:107) · u (where we interpret 0/0 = 0).
Remark 2. When dim(H) = 1  ii) is equivalent as requiring f to attain its minimum at 0  in which
case the implication ii) =⇒ iv)  under the redundant condition that f is differentiable  was proved
by Combettes and Pesquet [14  Proposition 3.6]. The implication ii) =⇒ iii) also generalizes [14 
Corollary 2.5]  where only the case dim(H) = 1 and f differentiable is considered. Note that there
exists non-even f that satisﬁes Theorem 4 when dim(H) = 1. Such is impossible for dim(H) ≥ 2 
in which case any f that satisﬁes Theorem 4 must also enjoy all properties listed in Proposition 4.
Proof: i) =⇒ ii): x ⊥ y =⇒ (cid:107)x + y(cid:107) ≥ (cid:107)y(cid:107).
ii) =⇒ iii): Indeed  by deﬁnition

Mf (u) = min

x

= min

λ

1

2(cid:107)x − u(cid:107)2 + f (x) = minu⊥ λ
2(cid:107)λu − u(cid:107)2 + f (λu) = minλ∈[0 1]

1

1

2(cid:107)u⊥ + λu − u(cid:107)2 + f (u⊥ + λu)

1

2 (λ − 1)2(cid:107)u(cid:107)2 + f (λu) 

where the third equality is due to ii)  and the nonnegative constraint in the last equality can be seen
as follows: For any λ < 0  by increasing it to 0 we can only decrease both terms; similar argument
for λ > 1. Therefore there exists λ ∈ [0  1] such that λu minimizes the Moreau envelop Mf hence
we have Pf (u) = λu due to uniqueness.
iii) =⇒ iv): Note ﬁrst that iii) implies 0 ∈ ∂f (0)  therefore 0 ∈ dom f. Since the subdifferential
of κ is scaling-invariant  iii) implies the sufﬁcient condition (10) hence iv).
iv) =⇒ iii): Fix y and construct the gauge function

(cid:26) 0 

∞ 

κ(z) =

if z = λ · y for some λ ≥ 0
otherwise

.

Then Pκ(y) = y  hence Pf (Pκ(y)) = Pf (y) = Pf +κ(y) by iv). On the other hand 

Mf +κ(y) = min

2 + f (x) + κ(x) = minλ≥0
3Note that λ ≤ 1 is necessary since any proximal map is nonexpansive.

x

1

2(cid:107)x − y(cid:107)2

1

2(cid:107)λy − y(cid:107)2

2 + f (λy).

(11)

5

Take y = 0 we obtain Pf +κ(0) = 0. Thus Pf (0) = 0  i.e. 0 ∈ ∂f (0)  from which we deduce that
Pf (y) = Pf +κ(y) = λy for some λ ∈ [0  1]  since f (λy) in (11) is increasing on [1 ∞[.
iii) =⇒ ii): First note that iii) implies that Pf (0) = 0 hence 0 ∈ ∂f (0)  in particular  0 ∈ dom f.
If dim(H) = 1 we are done  so we assume dim(H) ≥ 2 in the rest of the proof. In this case  it is
known  cf. [9  Theorem 1] or [10  Theorem 3]  that ii) ⇐⇒ i) (even without assuming f convex).
All we left is to prove iii) =⇒ ii) or equivalently i)  for the case dim(H) ≥ 2.
We ﬁrst prove the case when dom f = H. By iii)  Pf (x) = λx for some λ ∈ [0  1] (which may
depend on x as well). Using the ﬁrst order optimality condition for the proximal map we have
0 ∈ λx− x + ∂f (λx)  that is ( 1
λ − 1)y ∈ ∂f (y) for each y ∈ ran(Pf ) = H due to our assumption
dom f = H. Now for any x ⊥ y  by the deﬁnition of the subdifferential 

f (x + y) ≥ f (y) + (cid:104)x  ∂f (y)(cid:105) = f (y) +(cid:10)x  ( 1

λ − 1)y(cid:11) = f (y).

2 Pf (x) + 1

4 x = ( 1

2 λ + 1

g = A(f  q) = [( 1

2 (f∗ + q)∗ + 1

For the case when dom f ⊂ H  we consider the proximal average [16]
4 q)∗ − q]∗ 

(12)
2(cid:107) · (cid:107)2. Importantly  since q is deﬁned on the whole space  the proximal average g has
where q = 1
4 )x. Therefore
full domain too [16  Corollary 4.7]. Moreover  Pg(x) = 1
by our previous argument  g satisﬁes ii) hence also i). It is easy to check that i) is preserved under
taking the Fenchel conjugation (note that the convexity of f implies that of h). Since we have shown
that g satisﬁes i)  it follows from (12) that f satisﬁes i) hence also ii).
As mentioned  when dim(H) ≥ 2  the implication ii) =⇒ i) was shown in [9  Theorem 1]. The
formula Pf (u) = Ph((cid:107)u(cid:107))/(cid:107)u(cid:107) · u for f = h((cid:107) · (cid:107)) follows from straightforward calculation.
We now discuss some applications of Theorem 4. When dim(H) ≥ 2  iii) in Theorem 4 automati-
cally implies that the scalar constant λ depends on x only through its norm. This fact  although not
entirely obvious  does have a clear geometric picture:
Corollary 1. Let dim(H) ≥ 2  C ⊆ H be a closed convex set that contains the origin. Then the
projection onto C is simply a shrinkage towards the origin iff C is a ball (of the norm (cid:107) · (cid:107)).
Proof: Let f = ιC and apply Theorem 4.

2(cid:107) · (cid:107)2. In many applications  in addition to the regularizer κ
Example 5. As usual  denote q = 1
(usually a gauge)  one adds the (cid:96)2
2 regularizer λq either for stability or grouping effect or strong
convexity. This incurs no computational cost in the sense of computing the proximal map: We easily
compute that Pλq = 1
λ+1 Pκ  whence it is also
clear that adding an extra (cid:96)2 regularizer tends to double “shrink” the solution. In particular  let
H = Rd and take κ = (cid:107) · (cid:107)1 (the sum of absolute values) we recover the proximal map for the
elastic-net regularizer [17].
Example 6. The Berhu regularizer

λ+1 Id. By Theorem 4  for any gauge κ  Pκ+λq = 1

h(x) = |x|1|x|<γ + x2+γ2

2γ 1|x|≥γ = |x| + (|x|−γ)2

2γ

1|x|≥γ 

being the reverse of Huber’s function  is proposed in [18] as a bridge between the lasso ((cid:96)1 regular-
2 regularization). Let f (x) = h(x) − |x|. Clearly  f satisﬁes ii) of
ization) and ridge regression ((cid:96)2
Theorem 4 (but not differentiable)  hence

Ph = Pf ◦ P|·| 

whereas simple calculation veriﬁes that

Pf (x) = sign(x) · min{|x|  γ

1+γ (|x| + 1)} 

and of course P|·|(x) = sign(x) · max{|x| − 1  0}. Note that this regularizer is not s.p.d.
Corollary 2. Let dim(H) ≥ 2  then the p.h. function f ∈ Γ0 satisﬁes any item of Theorem 4 iff it is
a positive multiple of the norm (cid:107) · (cid:107).
Proof:
multiple of the norm.
Therefore (positive multiples of) the Hilbertian norm is the only p.h. convex function f that satisﬁes
Pf +κ = Pf ◦ Pκ for all gauge κ. In particular  this means that the norm (cid:107) · (cid:107) is s.p.d. Moreover  we
easily recover the following result that is perhaps not so obvious at ﬁrst glance:

[10  Theorem 4] showed that under positive homogeneity  i) implies that f is a positive

6

i=1 (cid:107)·(cid:107)gi

Corollary 3 (Jenatton et al. [7]). Fix the orthonormal basis {ei}i∈I of H. Let G ⊆ 2I be a collection
of tree-structured groups  that is  either g ⊆ g(cid:48) or g(cid:48) ⊆ g or g ∩ g(cid:48) = ∅ for all g  g(cid:48) ∈ G. Then

P(cid:80)m
Proof: Let f = (cid:107) · (cid:107)g1 and κ = (cid:80)m

where we arrange the groups so that gi ⊂ gj =⇒ i > j  and the notation (cid:107) · (cid:107)gi denotes the
Hilbertian norm that is restricted to the coordinates indexed by the group gi.
i=2 (cid:107) · (cid:107)gi. Clearly they are both p.h. (and convex). By the
tree-structured assumption we can partition κ = κ1 + κ2  where gi ⊂ g1 for all gi appearing in κ1
while gj ∩ g1 = ∅ for all gj appearing in κ2. Restricting to the subspace spanned by the variables in
g1 we can treat f as the Hilbertian norm. Apply Theorem 4 we obtain Pf +κ1 = Pf ◦ Pκ1. On the
other hand  due to the non-overlapping property  nothing will be affected by adding κ2  thus

= P(cid:107)·(cid:107)g1

◦ ··· ◦ P(cid:107)·(cid:107)gm

 

P(cid:80)m

i=1 (cid:107)·(cid:107)gi

= P(cid:107)·(cid:107)g1

i=2 (cid:107)·(cid:107)gi

.

◦ P(cid:80)m

We can clearly iterate the argument to unravel the proximal map as claimed.
For notational clarity  we have chosen not to incorporate weights in the sum of group seminorms:
Such can be absorbed into the seminorm and the corollary clearly remains intact. Our proof also
reveals the fundamental reason why Corollary 3 is true: The (cid:96)2 norm admits the decomposition (5)
for any gauge g! This fact  to the best of our knowledge  has not been recognized previously.

3.4 Cone Invariance
In the previous subsection  we restricted the subdifferential of g to be constant along each ray. We
now generalize this to cones. Speciﬁcally  consider the gauge function

(13)
where J is a ﬁnite index set and each aj ∈ H. Such polyhedral gauge functions have become
extremely important due to the work of Chandrasekaran et al. [19]. Deﬁne the polyhedral cones

κ(x) = max
j∈J

(cid:104)aj  x(cid:105)  

(14)
Assume Kj (cid:54)= ∅ for each j (otherwise delete j from J). Since ∂κ(x) = {aj|j ∈ J  x ∈ Kj}  the
sufﬁcient condition (10) becomes

Kj = {x ∈ H : (cid:104)aj  x(cid:105) = κ(x)}.

∀j ∈ J  Pf (Kj) ⊆ Kj.

(15)
In other words  each cone Kj is “ﬁxed” under the proximal map of f. Although it would be very
interesting to completely characterize f under (15)  we show that in its current form  (15) already
implies many known results  with some new generalizations falling out naturally.
Corollary 4. Denote E a collection of pairs (m  n)  and deﬁne the total variational norm (cid:107)x(cid:107)tv =

(cid:80){m n}∈E wm n|xm − xn|  where wm n ≥ 0. Then for any permutation invariant function4 f 

Pf +(cid:107)·(cid:107)tv = Pf ◦ P(cid:107)·(cid:107)tv .

Pick an arbitrary pair (m  n) ∈ E and let κ = |xm − xn|.

Proof:
Clearly
J = {1  2}  K1 = {xm ≥ xn} and K2 = {xm ≤ xn}. Since f is permutation invariant 
its proximal map Pf (x) maintains the order of x  hence we establish (15). Finally apply Proposi-
tion 2 and Theorem 1.
Remark 3. The special case where E = {(1  2)  (2  3)  . . .} is a chain  wm n ≡ 1 and f is the (cid:96)1
norm  appeared ﬁrst in [6] and is generally known as the fused lasso. The case where f is the (cid:96)p
norm appeared in [20].
We call the permutation invariant function f symmetric if ∀x  f (|x|) = f (x)  where | · | denotes
the componentwise absolute value. The proof for the next corollary is almost the same as that of
Corollary 4  except that we also use the fact sign([Pf (x)]m) = sign(xm) for symmetric functions.
for any symmetric function f  Pf +(cid:107)·(cid:107)oct = Pf ◦ P(cid:107)·(cid:107)oct.

Corollary 5. As in Corollary 4  deﬁne the norm (cid:107)x(cid:107)oct =(cid:80){m n}∈E wm n max{|xm| |xn|}. Then

4All we need is the weaker condition: For all {m  n} ∈ E  xm ≥ xn =⇒ [Pf (x)]m ≥ [Pf (x)]n.

7

Remark 4. This norm (cid:107) · (cid:107)oct is proposed in [21] for feature grouping. Surprisingly  Corollary 5
appears to be new. The proximal map P(cid:107)·(cid:107)oct is derived in [22]  which turns out to be another

decomposition result. Indeed  for i ≥ 2  deﬁne κi(x) =(cid:80)

(cid:88)

i≥2

(cid:107) · (cid:107)oct =

j≤i−1 max{|xi| |xj|}. Thus
κi.

Importantly  we observe that κi is symmetric on the ﬁrst i − 1 coordinates. We claim that

P(cid:107)·(cid:107)oct = Pκ|I| ◦ . . . ◦ Pκ2 .

The proof is by recursion: Write (cid:107) · (cid:107)oct = f + g  where f = κ|I|. Note that the subdifferential of
g depends only on the ordering and sign of the ﬁrst |I| − 1 coordinates while the proximal map of
f preserves the ordering and sign of the ﬁrst |I| − 1 coordinates (due to symmetry). If we pre-sort
x  the individual proximal maps Pκi (x) become easy to compute sequentially and we recover the
algorithm in [22] with some bookkeeping.
Corollary 6. As in Corollary 3  let G ⊆ 2I be a collection of tree-structured groups  then

where we arrange the groups so that gi ⊂ gj =⇒ i > j  and (cid:107)x(cid:107)gi k =(cid:80)k

i=1 (cid:107)·(cid:107)gi k = P(cid:107)·(cid:107)g1 k ◦ ··· ◦ P(cid:107)·(cid:107)gm k  

j=1 |xgi|[j] is the sum

P(cid:80)m

of the k (absolute-value) largest elements in the group gi  i.e.  Ky-Fan’s k-norm.
Proof: Similar as in the proof of Corollary 3  we need only prove that
P(cid:107)·(cid:107)g1  k+(cid:107)·(cid:107)g2 k = P(cid:107)·(cid:107)g1 k ◦ P(cid:107)·(cid:107)g2 k  

where w.l.o.g. we assume g1 contains all variables while g2 ⊂ g1. Therefore (cid:107) · (cid:107)g1 k can be treated
as symmetric and the rest follows the proof of Corollary 5.
Note that the case k ∈ {1 |I|} was proved in [7] and Corollary 6 can be seen as an interpolation.
Interestingly  there is another interpolated result whose proof should be apparent now.
Corollary 7. Corollary 6 remains true if we replace Ky-Fan’s k-norm with
max{|xi1|  . . .  |xik|}.

(cid:107)x(cid:107)oct k =

(cid:88)

(16)

1≤i1<i2<...<ik≤|I|

Therefore we can employ the norm (cid:107)x(cid:107)oct 2 for feature grouping in a hierarchical manner. Clearly
we can also combine Corollary 6 and Corollary 7.
Corollary 8. For any symmetric f  Pf +(cid:107)·(cid:107)oct k = Pf ◦ P(cid:107)·(cid:107)oct k. Similarly for Ky-Fan’s k-norm.
Remark 5. The above corollary implies that Ky-Fan’s k-norm and the norm (cid:107) · (cid:107)oct k deﬁned in
(16) are both s.p.d. (see Deﬁnition 1). The special case for the (cid:96)p norm where p ∈ {1  2 ∞} was
proved in [23  Proposition 11]  with a substantially more complicated argument. As pointed out in
[23]  s.p.d. regularizers allow us to perform lazy updates in gradient-type algorithms.

We remark that we have not exhausted the possibility to have the decomposition (5). It is our hope
to stimulate further work in understanding the prox-decomposition (5).
Added after acceptance: We have managed to extend the results in this subsection to the Lovász
extension of submodular set functions. Details will be given elsewhere.

4 Conclusion
The main goal of this paper is to understand when the proximal map of the sum of functions de-
composes into the composition of the proximal maps of the individual functions. Using a simple
sufﬁcient condition we are able to completely characterize the decomposition when certain scaling
invariance is exhibited. The generalization to cone invariance is also considered and we recover
many known decomposition results  with some new ones obtained almost effortlessly. In the future
we plan to generalize some of the results here to nonconvex functions.

Acknowledgement

The author thanks Bob Williamson and Xinhua Zhang from NICTA—Canberra for their hospitality
during the author’s visit when part of this work was performed; Warren Hare  Yves Lucet  and
Heinz Bauschke from UBC—Okanagan for some discussions around Theorem 4; and the reviewers
for their valuable comments.

8

References
[1] Bernhard Scholköpf and Alexander J. Smola. Learning with Kernels: Support Vector Ma-

chines  Regularization  Optimization  and Beyond. MIT Press  2001.

[2] Peter Bühlmann and Sara van de Geer. Statistics for High-Dimensional Data. Springer  2011.
[3] Patrick L. Combettes and Valérie R. Wajs. Signal recovery by proximal forward-backward

splitting. Multiscale Modeling and Simulation  4(4):1168–1200  2005.

[4] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[5] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Pro-

gramming  Series B  140:125–161  2013.

[6] Jerome Friedman  Trevor Hastie  Holger Höﬂing  and Robert Tibshirani. Pathwise coordinate

optimization. The Annals of Applied Statistics  1(2):302–332  2007.

[7] Rodolphe Jenatton  Julien Mairal  Guillaume Obozinski  and Francis Bach. Proximal methods
for hierarchical sparse coding. Journal of Machine Learning Research  12:2297–2334  2011.
[8] Jiayu Zhou  Jun Liu  Vaibhav A. Narayan  and Jieping Ye. Modeling disease progression via

fused sparse group lasso. In Conference on Knowledge Discovery and Data Mining  2012.

[9] Francesco Dinuzzo and Bernhard Schölkopf. The representer theorem for Hilbert spaces: a

necessary and sufﬁcient condition. In NIPS  2012.

[10] Yao-Liang Yu  Hao Cheng  Dale Schuurmans  and Csaba Szepesvári. Characterizing the rep-

resenter theorem. In ICML  2013.

[11] Jean J. Moreau. Proximité et dualtité dans un espace Hilbertien. Bulletin de la Société Mathé-

matique de France  93:273–299  1965.

[12] Patrick L. Combettes  Ðinh D˜ung  and B`˘ang Công V˜u. Proximity for sums of composite

functions. Journal of Mathematical Analysis and Applications  380(2):680–688  2011.

[13] André F. T. Martins  Noah A. Smith  Eric P. Xing  Pedro M. Q. Aguiar  and Mário A. T.
Figueiredo. Online learning of structured predictors with multiple kernels. In Conference on
Artiﬁcial Intelligence and Statistics  2011.

[14] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal thresholding algorithm for min-

imization over orthonormal bases. SIAM Journal on Optimization  18(4):1351–1376  2007.

[15] Yaoliang Yu. Fast Gradient Algorithms for Stuctured Sparsity. PhD thesis  University of

Alberta  2013.

[16] Heinz H. Bauschke  Rafal Goebel  Yves Lucet  and Xianfu Wang. The proximal average:

Basic theory. SIAM Journal on Optimization  19(2):766–785  2008.

[17] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal

of the Royal Statistical Society B  67:301–320  2005.

[18] Art B. Owen. A robust hybrid of lasso and ridge regression. In Prediction and Discovery 

pages 59–72. AMS  2007.

[19] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[20] Xinhua Zhang  Yaoliang Yu  and Dale Schuurmans. Polar operators for structured sparse

estimation. In NIPS  2013.

[21] Howard Bondell and Brian Reich. Simultaneous regression shrinkage  variable selection  and

supervised clustering of predictors with oscar. Biometrics  64(1):115–123  2008.

[22] Leon Wenliang Zhong and James T. Kwok. Efﬁcient sparse modeling with automatic feature

grouping. In ICML  2011.

[23] John Duchi and Yoram Singer. Efﬁcient online and batch learning using forward backward

splitting. Journal of Machine Learning Research  10:2899–2934  2009.

9

,Yao-Liang Yu
Chris Oates
Steven Niederer
Angela Lee
François-Xavier Briol
Mark Girolami
Vladimir Kniaz
Vladimir Knyaz
Fabio Remondino