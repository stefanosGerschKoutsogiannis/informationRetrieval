2014,Multivariate Regression with Calibration,We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods  CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally  we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity $O(1/\epsilon)$  where $\epsilon$ is a pre-specified numerical accuracy. Theoretically  we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.,Multivariate Regression with Calibration⇤

Department of Operations Research and Financial Engineering

Han Liu

Princeton University

Lie Wang

Department of Mathematics

Massachusetts Institute of Technology

Tuo Zhao†

Department of Computer Science

Johns Hopkins University

Abstract

We propose a new method named calibrated multivariate regression (CMR) for
ﬁtting high dimensional multivariate regression models. Compared to existing
methods  CMR calibrates the regularization for each regression task with respect
to its noise level so that it is simultaneously tuning insensitive and achieves an
improved ﬁnite-sample performance. Computationally  we develop an efﬁcient
smoothed proximal gradient algorithm which has a worst-case iteration complex-
ity O(1/✏)  where ✏ is a pre-speciﬁed numerical accuracy. Theoretically  we prove
that CMR achieves the optimal rate of convergence in parameter estimation. We
illustrate the usefulness of CMR by thorough numerical simulations and show
that CMR consistently outperforms other high dimensional multivariate regres-
sion methods. We also apply CMR on a brain activity prediction problem and ﬁnd
that CMR is as competitive as the handcrafted model created by human experts.

1

Introduction

Given a design matrix X 2 Rn⇥d and a response matrix Y 2 Rn⇥m  we consider a multivariate
linear model Y = XB0 + Z  where B0 2 Rd⇥m is an unknown regression coefﬁcient matrix and
Z 2 Rn⇥m is a noise matrix [1]. For a matrix A = [Ajk] 2 Rd⇥m  we denote Aj⇤ = (Aj1  ... 
Ajm) 2 Rm and A⇤k = (A1k  ...  Adk)T 2 Rd to be its jth row and kth column respectively. We
assume that all Zi⇤’s are independently sampled from an m-dimensional Gaussian distribution with
mean 0 and covariance matrix ⌃ 2 Rm⇥m.
We can represent the multivariate linear model as an ensemble of univariate linear regression models:
⇤k +Z⇤k  k = 1  ...  m. Then we get a multi-task learning problem [3  2  26]. Multi-task
Y⇤k = XB0
learning exploits shared common structure across tasks to obtain improved estimation performance.
In the past decade  signiﬁcant progress has been made towards designing a variety of modeling
assumptions for multivariate regression.
A popular assumption is that all the regression tasks share a common sparsity pattern  i.e.  many
’s are zero vectors. Such a joint sparsity assumption is a natural extension of that for univariate
B0
j⇤
linear regressions. Similar to the L1-regularization used in Lasso [23]  we can adopt group regular-
ization to obtain a good estimator of B0 [25  24  19  13]. Besides the aforementioned approaches 
there are other methods that aim to exploit the covariance structure of the noise matrix Z [7  22]. For
⇤The authors are listed in alphabetical order. This work is partially supported by the grants NSF
IIS1408910  NSF IIS1332109  NSF Grant DMS-1005539  NIH R01MH102339  NIH R01GM083084  and
NIH R01HG06841.

†Tuo Zhao is also afﬁliated with Department of Operations Research and Financial Engineering at Princeton

University.

1

1

pm||bB  B0||F  C · max r s log d

nm

+r sm12/p

n

!  

1  2

2  . . .   2

instance  [22] assume that all Zi⇤’s follow a multivariate Gaussian distribution with a sparse inverse
covariance matrix ⌦ = ⌃1. They propose an iterative algorithm to estimate sparse B0 and ⌦ by
maximizing the penalized Gaussian log-likelihood. Such an iterative procedure is effective in many
applications  but the theoretical analysis is difﬁcult due to its nonconvex formulation.
In this paper  we assume an uncorrelated structure for the noise matrix Z 
diag(2
problem with a convex program as follows
1
pn||Y  XB||2

i.e.  ⌃ =
m). Under this setting  we can efﬁciently solve the resulting estimation

F + ||B||1 p 

m1  2

jk is the Frobenius norm of a ma-
jk and
j=1 max1km |Bjk|. Computationally  the optimization problem in (1.1) can be
efﬁciently solved by some ﬁrst order algorithms [11  12  4].
The problem with the uncorrelated noise structure is amenable to statistical analysis. Under suit-
able conditions on the noise and design matrices  let max = maxk k  if we choose  =

where > 0 is a tuning parameter  and ||A||F = qPj k A2
trix A. Popular choices of p include p = 2 and p = 1:
||B||1 1 = Pd
2c · maxplog d + m11/p  for some c > 1  then the estimator bB in (1.1) achieves the opti-

mal rates of convergence1 [13]  i.e.  there exists some universal constant C such that with high
probability  we have

||B||1 2 = Pd

j=1qPm

k=1 B2

bB = argmin

(1.1)

B

where s is the number of rows with non-zero entries in B0. However  the estimator in (1.1) has two
drawbacks: (1) All the tasks are regularized by the same tuning parameter   even though different
tasks may have different k’s. Thus more estimation bias is introduced to the tasks with smaller k’s
to compensate the tasks with larger k’s. In another word  these tasks are not calibrated. (2) The
tuning parameter selection involves the unknown parameter max. This requires tuning the regular-
ization parameter over a wide range of potential values to get a good ﬁnite-sample performance.
To overcome the above two drawbacks   we formulate a new convex program named calibrated
multivariate regression (CMR). The CMR estimator is deﬁned to be the solution of the following
convex program:

B

(1.2)

||Y  XB||2 1 + ||B||1 p 

where ||A||2 1 = PkqPj A2

bB = argmin
jk is the nonsmooth L2 1 norm of a matrix A = [Ajk] 2 Rd⇥m.
This is a multivariate extension of the square-root Lasso [5]. Similar to the square-root Lasso  the
tuning parameter selection of CMR does not involve max. Moreover  the L2 1 loss function can
be viewed as a special example of the weighted least square loss  which calibrates each regression
task (See more details in §2). Thus CMR adapts to different k’s and achieves better ﬁnite-sample
performance than the ordinary multivariate regression estimator (OMR) deﬁned in (1.1).
Since both the loss and penalty functions in (1.2) are nonsmooth  CMR is computationally more
challenging than OMR. To efﬁciently solve CMR  we propose a smoothed proximal gradient (SPG)
algorithm with an iteration complexity O(1/✏)  where ✏ is the pre-speciﬁed accuracy of the objec-
tive value [18  4]. Theoretically  we provide sufﬁcient conditions under which CMR achieves the
optimal rates of convergence in parameter estimation. Numerical experiments on both synthetic and
real data show that CMR universally outperforms existing multivariate regression methods. For a
brain activity prediction task  prediction based on the features selected by CMR signiﬁcantly out-
performs that based on the features selected by OMR  and is even competitive with that based on the
handcrafted features selected by human experts.
Notations: Given a vector v = (v1  . . .   vd)T 2 Rd  for 1  p  1  we deﬁne the Lp-vector
if 1  p < 1 and ||v||p = max1jd |vj| if p = 1.

norm of v as ||v||p = ⇣Pd

j=1 |vj|p⌘1/p

1The rate of convergence is optimal when p = 2  i.e.  the regularization function is ||B||1 p

2

j=1Pm

Given two matrices A = [Ajk] and C = [Cjk] 2 Rd⇥m  we deﬁne the inner product of A
and C as hA  Ci = Pd
k=1 AjkCjk = tr(AT C)  where tr(A) is the trace of a matrix A.
We use A⇤k = (A1k  ...  Adk)T and Aj⇤ = (Aj1  ...  Ajm) to denote the kth column and jth
row of A. Let S be some subspace of Rd⇥m  we use AS to denote the projection of A onto S:
F. Moreover  we deﬁne the Frobenius and spectral norms of A as
AS = argminC2S ||C  A||2
||A||F = phA  Ai and ||A||2 = 1(A)  1(A) is the largest singular value of A. In addition 
we deﬁne the matrix block norms as ||A||2 1 =Pm
k=1 ||A⇤k||2  ||A||2 1 = max1km ||A⇤k||2 
||A||1 p =Pd
j=1 ||Aj⇤||p  and ||A||1 q = max1jd ||Aj⇤||q  where 1  p  1 and 1  q  1.
It is easy to verify that ||A||2 1 is the dual norm of ||A||2 1. Let 1/1 = 0  then if 1/p + 1/q = 1 
||A||1 q and ||A||1 p are also dual norms of each other.
2 Method

We solve the multivariate regression problem by the following convex program 

bB = argmin

B

||Y  XB||2 1 + ||B||1 p.

(2.1)

The only difference between (2.1) and (1.1) is that we replace the L2-loss function by the nonsmooth
L2 1-loss function. The L2 1-loss function can be viewed as a special example of the weighted square
loss function. More speciﬁcally  we consider the following optimization problem 

1

kpn||Y⇤k  XB⇤k||2

2 + ||B||1 p 

(2.2)

bB = argmin

B

mXk=1

ek =

1

kpn is a weight assigned to calibrate the kth regression task. Without prior knowledge on

where
k’s  we use the following replacement of k’s 

1
pn||Y⇤k  XB⇤k||2  k = 1  ...  m.

(2.3)

By plugging (2.3) into the objective function in (2.2)  we get (2.1). In another word  CMR calibrates
different tasks by solving a penalized weighted least square program with weights deﬁned in (2.3).
The optimization problem in (2.1) can be solved by the alternating direction method of multipliers
(ADMM) with a global convergence guarantee [20]. However  ADMM does not take full advantage
of the problem structure in (2.1). For example  even though the L2 1 norm is nonsmooth  it is
nondifferentiable only when a task achieves exact zero residual  which is unlikely in applications.
In this paper  we apply the dual smoothing technique proposed by [18] to obtain a smooth surrogate
function so that we can avoid directly evaluating the subgradient of the L2 1 loss function. Thus we
gain computational efﬁciency like other smooth loss functions.
We consider the Fenchel’s dual representation of the L2 1 loss:

||Y  XB||2 1 = max

||U||2 11hU  Y  XBi.

(2.4)

Let µ > 0 be a smoothing parameter. The smooth approximation of the L2 1 loss can be obtained
by solving the following optimization problem
||Y  XB||µ = max

||U||2 11hU  Y  XBi 
F is the proximity function. Due to the fact that ||U||2

where ||U||2
following uniform bound by combing (2.4) and (2.5) 

µ
2||U||2
F 
F  m||U||2

  we obtain the

(2.5)

2 1

(2.6)
From (2.6)  we see that the approximation error introduced by the smoothing procedure can be
controlled by a suitable µ. Figure 2.1 shows several two-dimensional examples of the L2 norm

mµ
2 || Y  XB||µ || Y  XB||2 1.

||Y  XB||2 1 

smoothed by different µ’s. The optimization problem in (2.5) has a closed form solution bUB with
bUB
⇤k = (Y⇤k  XB⇤k)/ max{||Y⇤k  XB⇤k||2  µ}.
The next lemma shows that ||Y  XB||µ is smooth in B with a simple form of gradient.

3

(a) µ = 0

(b) µ = 0.1

(c) µ = 0.25

(d) µ = 0.5

Figure 2.1: The L2 norm (µ = 0) and its smooth surrogates with µ = 0.1  0.25  0.5. A larger µ
makes the approximation more smooth  but introduces a larger approximation error.

Gµ(B) =

@⇣hbUB  Y  XBi + µ||bUB||2
F/2⌘

Lemma 2.1. For any µ > 0  ||Y  XB||µ is a convex and continuously differentiable function in
B. In addition  Gµ(B)—the gradient of ||Y  XB||µ w.r.t. B—has the form
= XTbUB.
1
µ||XT X(B0  B00)||F 

Moreover  let  = ||X||2
constant /µ  i.e.  for any B0  B00 2 Rd⇥m 
||Gµ(B0)  Gµ(B00)||F = ||hX bUB0  bUB00i||F 
Lemma 2.1 is a direct result of Theorem 1 in [18] and implies that ||Y  XB||µ has good computa-
tional structure. Therefore we apply the smooth proximal gradient algorithm to solve the smoothed
version of the optimization problem as follows 

2  then we have that Gµ(B) is Lipschitz continuous in B with the Lipschitz


µ||B0  B00||F.

(2.7)

@B

eB = argmin

B

||Y  XB||µ + ||B||1 p.

(2.8)

We then adopt the fast proximal gradient algorithm to solve (2.8) [4]. To derive the algorithm 
we ﬁrst deﬁne three sequences of auxiliary variables {A(t)}  {V(t)}  and {H(t)} with A(0) =
H(0) = V(0) = B(0)  a sequence of weights {✓t = 2/(t + 1)}  and a nonincreasing sequence of
step-sizes {⌘t > 0}. For simplicity  we can set ⌘t = µ/. In practice  we use the backtracking
line search to dynamically adjust ⌘t to boost the performance. At the tth iteration  we ﬁrst take
V(t) = (1  ✓t)B(t1) + ✓tA(t1). We then consider a quadratic approximation of ||Y  XH||µ
as

Q⇣H  V(t) ⌘ t⌘ = ||Y  XV(t)||µ + hGµ(V(t))  H  V(t)i +

1
2⌘t||H  V(t)||2
F.

1

2⌘t||H  eH(t)||2

F + ||H||1 p.

(2.9)

Consequently  let eH(t) = V(t)  ⌘tGµ(V(t))  we take
Q⇣H  V(t) ⌘ t⌘ + ||H||1 p = argmin

H(t) = argmin

H

H

When p = 2  (2.9) has a closed form solution H(t)
details about other choices of p in the L1 p norm can be found in [11] and [12]. To ensure that the
objective value is nonincreasing  we choose
argmin

j⇤ = eHj⇤ · maxn1  ⌘t/||eHj⇤||2  0o. More

B(t) =

(2.10)

||Y  XB||µ + ||B||1 p.

B2{H(t)  B(t1)}

At last  we take A(t) = B(t1)+ 1
✓t
where " is the stopping precision.
The numerical rate of convergence of the proposed algorithm with respect to the original optimiza-
tion problem (2.1) is presented in the following theorem.

(H(t)B(t1)). The algorithm stops when ||H(t)V(t)||F  " 

Theorem 2.2. Given a pre-speciﬁed accuracy ✏ and let µ = ✏/m  after t = 2pm||B(0)bB||F/✏
1 = O (1/✏) iterations  we have ||Y  XB(t)||2 1 + ||B(t)||1 p || Y  XbB||2 1 + ||bB||1 p + ✏.

The proof of Theorem 2.2 is provided in Appendix A.1. This result achieves the minimax optimal
rate of convergence over all ﬁrst order algorithms [18].

4

3 Statistical Properties
For notational simplicity  we deﬁne a re-scaled noise matrix W = [Wik] 2 Rn⇥m with Wik =
Zik/k  where EZ2
k. Thus W is a random matrix with all entries having mean 0 and variance
1. We deﬁne G0 to be the gradient of ||Y  XB||2 1 at B = B0. It is easy to see that

ik = 2

G0

⇤k =

XT Z⇤k
||Z⇤k||2

=

XT W⇤kk
||W⇤kk||2

=

XT W⇤k
||W⇤k||2

⇤k works as an important
does not depend on the unknown quantities k for all k = 1  ...  m. G0
pivotal in our analysis. Moreover  our analysis exploits the decomposability of the L1 p norm [17].
More speciﬁcally  we assume that B0 has s rows with all zero entries and deﬁne
j⇤ = 0  
j⇤ 6= 0 .

(3.1)
(3.2)
Note that we have B0 2S and the L1 p norm is decomposable with respect to the pair (S N )  i.e. 
The next lemma shows that when  is suitably chosen  the solution to the optimization problem in
(2.1) lies in a restricted set.

S =C 2 Rd⇥m | Cj⇤ = 0 for all j such that B0
N =C 2 Rd⇥m | Cj⇤ = 0 for all j such that B0

||A||1 p = ||AS||1 p + ||AN||1 p.

Lemma 3.1. Let B0 2S and bB be the optimum to (2.1)  and 1/p + 1/q = 1. We denote the
estimation error as b = bB  B0. If   c||G0||1 q for some c > 1  we have
c  1||S||1 p .

b 2M c :=⇢ 2 Rd⇥m | ||N||1 p 

The proof of Lemma 3.1 is provided in Appendix B.1. To prove the main result  we also need to
assume that the design matrix X satisﬁes the following condition.
Assumption 3.1. Let B0 2S   then there exist positive constants  and c > 1 such that

(3.3)

c + 1

  min

2Mc\{0}

||X||F
pn||||F

.

Assumption 3.1 is the generalization of the restricted eigenvalue conditions for analyzing univariate
sparse linear models [17  15  6]  Many common examples of random design satisfy this assumption
[13  21].
Note that Lemma 3.1 is a deterministic result of the CMR estimator for a ﬁxed . Since G is
essentially a random matrix  we need to show that   cR⇤(G0) holds with high probability to
deliver a concrete rate of convergence for the CMR estimator in the next theorem.
Theorem 3.2. We assume that each column of X is normalized as m1/21/pkX⇤jk2 = pn for all
j = 1  ...  d. Then for some universal constant c0 and large enough n  taking

 =

p1  c0
with probability at least 1  2 exp(2 log d)  2 expnc2
1  c0 r sm12/p
2(c  1)r 1 + c0

0/8 + log m  we have
nm ! .
+r s log d

16cmax

n

1

 

pm||bB  B0||F 

The proof of Theorem 3.2 is provided in Appendix B.2. Note that when we choose p = 2  the
column normalization condition is reduced to kX⇤jk2 = pn. Meanwhile  the corresponding error
bound is further reduced to

2c(m11/p + plog d)

(3.4)

1

pm||bB  B0||F = OP r s

n

nm !  
+r s log d

which achieves the minimax optimal rate of convergence presented in [13]. See Theorem 6.1 in [13]
for more technical details. From Theorem 3.2  we see that CMR achieves the same rates of conver-
gence as the noncalibrated counterpart  but the tuning parameter  in (3.4) does not involve k’s.
Therefore CMR not only calibrates all the regression tasks  but also makes the tuning parameter
selection insensitive to max.

5

4 Numerical Simulations

To compare the ﬁnite-sample performance between the calibrated multivariate regression (CMR)
and ordinary multivariate regression (OMR)  we generate a training dataset of 200 samples. More
speciﬁcally  we use the following data generation scheme: (1) Generate each row of the design
matrix Xi⇤  i = 1  ...  200  independently from a 800-dimensional normal distribution N (0  ⌃)
where ⌃jj = 1 and ⌃j` = 0.5 for all ` 6= j.(2) Let k = 1  . . .   13  set the regression coefﬁcient
matrix B0 2 R800⇥13 as B0
jk = 0 for all j 6= 1  2  4. (3)
Generate the random noise matrix Z = WD  where W 2 R200⇥13 with all entries of W are
independently generated from N (0  1)  and D is either of the following matrices

4k = 1.5  and B0

2k = 2  B0

1k = 3  B0

DI = max · diag⇣20/4  21/4 ···   211/4  212/4⌘ 2 R13⇥13
DH = max · I 2 R13⇥13.

1

F  where

F  and Est. Err. = 1

m||bB  B0||2

F  where X and Y denotes the design

We generate a validation set of 200 samples for the regularization parameter selection and a testing
set of 10 000 samples to evaluate the prediction accuracy.
In numerical experiments  we set max = 1  2  and 4 to illustrate the tuning insensitivity
of CMR. The regularization parameter  of both CMR and OMR is chosen over a grid ⇤ =

240/40  239/40 ···   217/40  218/40   where 0 = plog d + pm. The optimal regular-
ization parameterb is determined by the prediction error asb = argmin2⇤ ||eY eXbB||2
bB denotes the obtained estimate using the regularization parameter   and eX and eY denote the
10000||Y  XbB||F  Adj. Pre. Err. =
10000m||(Y  XbB)D1||2

design and response matrices of the validation set.
Since the noise level k’s are different in regression tasks  we adopt the following three crite-
ria to evaluate the empirical performance: Pre. Err. = 1

and response matrices of the testing set.
All simulations are implemented by MATLAB using a PC with Intel Core i5 3.3GHz CPU and 16GB
memory. CMR is solved by the proposed smoothing proximal gradient algorithm  where we set the
stopping precision " = 104  the smoothing parameter µ = 104. OMR is solved by the monotone
fast proximal gradient algorithm  where we set the stopping precision " = 104. We set p = 2  but
the extension to arbitrary p > 2 is straightforward.
We ﬁrst compare the smoothed proximal gradient (SPG) algorithm with the ADMM algorithm (the
detailed derivation of ADMM can be found in Appendix A.2). We adopt the backtracking line search
to accelerate both algorithms with a shrinkage parameter ↵ = 0.8. We set max = 2 for the adopted
multivariate linear models. We conduct 200 simulations. The results are presented in Table 4.1. The
SPG and ADMM algorithms attain similar objective values  but SPG is up to 4 times faster than
ADMM. Both algorithms also achieve similar estimation errors.
We then compare the statistical performance between CMR and OMR. Tables 4.2 and 4.3 summa-
rize the results averaged over 200 replicates. In addition  we also present the results of the oracle
estimator  which is obtained by solving (2.2)  since we know the true values of k’s. Note that the
oracle estimator is only for comparison purpose  and it is not a practical estimator. Since CMR
calibrates the regularization for each task with respect to k  CMR universally outperforms OMR 
and achieves almost the same performance as the oracle estimator when we adopt the scale matrix
DI to generate the random noise. Meanwhile  when we adopt the scale matrix DH  where all k’s
are the same  CMR and OMR achieve similar performance. This further implies that CMR can be a
safe replacement of OMR for multivariate regressions.
In addition  we also examine the optimal regularization parameters for CMR and OMR over all

In particular  we adopt the Gaussian kernel  and the kernel bandwidth is selected based on the 10-
fold cross validation. Figure 4.1 illustrates the estimated density functions. The horizontal axis

replicates. We visualize the distribution of all 200 selectedb’s using the kernel density estimator.
bplog d+pm⌘. We see that the optimal
corresponds to the rescaled regularization parameter as log⇣

regularization parameters of OMR signiﬁcantly vary with different max. In contrast  the optimal
regularization parameters of CMR are more concentrated. This is inconsistent with our claimed
tuning insensitivity.

6

Table 4.1: Quantitive comparison of the computational performance between SPG and ADMM with
the noise matrices generated using DI. The results are averaged over 200 replicates with standard
errors in parentheses. SPG and ADMM attain similar objective values  but SPG is up to about 4
times faster than ADMM.

SPG

ADMM

 Algorithm Timing (second)
2.8789(0.3141)
8.4731(0.8387)
3.2633(0.3200)
11.976(1.460)
3.7868(0.4551)
18.360(1.9678)

ADMM

ADMM

SPG

SPG

0

20

0.50

Obj. Val.

Num. Ite.

Est. Err.

508.21(3.8498)
508.22(3.7059)
370.53(3.6144)
370.53(3.4231)
297.24(3.6125)
297.25(3.3863)

493.26(52.268)
437.7(37.4532)
565.80(54.919)
600.94(74.629)
652.53(78.140)
1134.0(136.08)

0.1213(0.0286)
0.1215(0.0291)
0.0819(0.0205)
0.0822(0.0233)
0.1399(0.0284)
0.1409(0.0317)

Table 4.2: Quantitive comparison of the statistical performance between CMR and OMR with the
noise matrices generated using DI. The results are averaged over 200 simulations with the standard
errors in parentheses. CMR universally outperforms OMR  and achieves almost the same perfor-
mance as the oracle estimator.

max

1

2

4

Method
Oracle
CMR
OMR
Oracle
CMR
OMR
Oracle
CMR
OMR

Pre. Err.

5.8759(0.0834)
5.8761(0.0673)
5.9012(0.0701)
23.464(0.3237)
23.465(0.2598)
23.580(0.2832)
93.532(0.8843)
93.542(0.9794)
94.094(1.0978)

Adj. Pre.Err
1.0454(0.0149)
1.0459(0.0123)
1.0581(0.0162)
1.0441(0.0148)
1.0446(0.0121)
1.0573(0.0170)
1.0418(0.0962)
1.0421(0.0118)
1.0550(0.0166)

Est. Err.

0.0245(0.0086)
0.0249(0.0071)
0.0290(0.0091)
0.0926(0.0342)
0.0928(0.0279)
0.1115(0.0365)
0.3342(0.1255)
0.3346(0.1063)
0.4125(0.1417)

Table 4.3: Quantitive comparison of the statistical performance between CMR and OMR with the
noise matrices generated using DH. The results are averaged over 200 simulations with the standard
errors in parentheses. CMR and OMR achieve similar performance.

max

1

2

4

1.4

1.2

1

0.8

0.6

0.4

0.2

Method
CMR
OMR
CMR
OMR
CMR
OMR

Pre. Err.

13.565(0.1408)
13.697(0.1554)
54.171(0.5771)
54.221(0.6173)
215.98(2.104)
216.19(2.391)

Adj. Pre.Err
1.0435(0.0108)
1.0486(0.0142)
1.0418(0.0110)
1.0427(0.0118)
1.0384(0.0101)
1.0394(0.0114)

Est. Err.

0.0599(0.0164)
0.0607(0.0128)
0.2252(0.0649)
0.2359(0.0821)
0.80821(0.25078)
0.81957(0.31806)

 

Oracle(1)
Oracle(2)
Oracle(4)
CMR(1)
CMR(2)
CMR(4)
OMR(1)
OMR(2)
OMR(4)

1.4

1.2

1

0.8

0.6

0.4

0.2

 

CMR(1)
CMR(2)
CMR(4)
OMR(1)
OMR(2)
OMR(4)

0
 
−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

(a) The noise matrices are generated using DI

2.5

0
 
−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

2.5

(b) The noise matrices are generated using DH

Figure 4.1: The distributions of the selected regularization parameters using the kernel density esti-
mator. The numbers in the parentheses are max’s. The optimal regularization parameters of OMR
are spreader with different max than those of CMR and the oracle estimator.

7

5 Real Data Experiment

We apply CMR on a brain activity prediction problem which aims to build a parsimonious model to
predict a person’s neural activity when seeing a stimulus word. As is illustrated in Figure 5.1  for
a given stimulus word  we ﬁrst encode it into an intermediate semantic feature vector using some
corpus statistics. We then model the brain’s neural activity pattern using CMR. Creating such a
predictive model not only enables us to explore new analytical tools for the fMRI data  but also
helps us to gain deeper understanding on how human brain represents knowledge [16].
Predict fMRI brain activity patterns in response to text stimulus

89/:4:2% -.&2

!"#$%)/01'2

%0334'&

!"#$%&'()*'

!"#$%0334'&

%50//'.&

%6)*7*4'&

stimulus word

!"#$%50//'.&

"apple"

!"#$%6)*7*4'&

(Mitchell et al.  Science 2008)

predicted 
activities 
for "apple"

+' % -.&

Model

?

intermediate semantic features

mapping learned from fMRI data

Standard solution 
(a) illustration of the data collection procedure
Linear models
(More restrictive)

Nonlinear models
(Less restrictive)

Our solution

.

(b) model for predicting fMRI brain activity pattern 

;5'%'<3'.)/'+=2%0.'%*-+&:*='&%)+%>")=*5'44%'=%0?%8*)'+*'%@AB

Figure 5.1: An illustration of the fMRI brain activity prediction problem [16]. (a) To collect the
data  a human participant sees a sequence of English words and their images. The corresponding
fMRI images are recorded to represent the brain activity patterns; (b) To build a predictive model 
each stimulus word is encoded into intermediate semantic features (e.g. the co-occurrence statistics
of this stimulus word in a large text corpus). These intermediate features can then be used to predict
the brain activity pattern.

Our experiments involves 9 participants  and Table 5.1 summarizes the prediction performance of
different methods on these participants. We see that the prediction based on the features selected by
CMR signiﬁcantly outperforms that based on the features selected by OMR  and is as competitive
as that based on the handcrafted features selected by human experts. But due to the space limit  we
present the details of the real data experiment in the technical report version.

Table 5.1: Prediction accuracies of different methods (higher is better). CMR outperforms OMR for
8 out of 9 participants  and outperforms the handcrafted basis words for 6 out of 9 participants

Method
CMR
OMR

P. 1
0.840
0.803
Handcraft 0.822

P. 2
0.794
0.789
0.776

P. 3
0.861
0.801
0.773

P. 4
0.651
0.602
0.727

P. 5
0.823
0.766
0.782

P. 6
0.722
0.623
0.865

P. 7
0.738
0.726
0.734

P. 8
0.720
0.749
0.685

P. 9
0.780
0.765
0.819

6 Discussions

A related method is the square-root sparse multivariate regression [8]. They solve the convex pro-
gram with the Frobenius loss function and L1 p regularization function
||Y  XB||F + ||B||1 p.

(6.1)

The Frobenius loss function in (6.1) makes the regularization parameter selection independent of
max  but it does not calibrate different regression tasks. Note that we can rewrite (6.1) as
1
pnm||Y  XB||F.

F + ||B||1 p s. t. =

(6.2)

B

bB = argmin
pnm||Y  XB||2

1

(bB b) = argmin

B 

Since  in (6.2) is not speciﬁc to any individual task  it cannot calibrate the regularization. Thus it
is fundamentally different from CMR.

8

References
[1] T.W Anderson. An introduction to multivariate statistical analysis. Wiley New York  1958.
[2] Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks

and unlabeled data. The Journal of Machine Learning Research  6(11):1817–1853  2005.

[3] J Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research  12:149–198 

2000.

[4] A. Beck and M Teboulle. Fast gradient-based algorithms for constrained total variation image denoising

and deblurring problems. IEEE Transactions on Image Processing  18(11):2419–2434  2009.

[5] A. Belloni  V. Chernozhukov  and L Wang. Square-root lasso: pivotal recovery of sparse signals via conic

programming. Biometrika  98(4):791–806  2011.

[6] Peter J Bickel  Yaacov Ritov  and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig

selector. The Annals of Statistics  37(4):1705–1732  2009.

[7] L. Breiman and J.H Friedman. Predicting multivariate responses in multiple linear regression. Journal of

the Royal Statistical Society: Series B  59(1):3–54  2002.

[8] Florentina Bunea  Johannes Lederer  and Yiyuan She. The group square-root lasso: Theoretical properties

and fast algorithms. IEEE Transactions on Information Theory  60:1313 – 1325  2013.

[9] Iain M Johnstone. Chi-square oracle inequalities. Lecture Notes-Monograph Series  pages 399–418 

2001.

[10] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces:

Springer  2011.

isoperimetry and processes.

[11] H. Liu  M. Palatucci  and J Zhang. Blockwise coordinate descent procedures for the multi-task lasso 
with applications to neural semantic basis discovery. In Proceedings of the 26th Annual International
Conference on Machine Learning  pages 649–656. ACM  2009.

[12] J. Liu and J Ye. Efﬁcient `1/`q norm regularization. Technical report  Arizona State University  2010.
[13] K. Lounici  M. Pontil  S. Van De Geer  and A.B Tsybakov. Oracle inequalities and optimal inference

under group sparsity. The Annals of Statistics  39(4):2164–2204  2011.

[14] N. Meinshausen and P. B¨uhlmann. Stability selection. Journal of the Royal Statistical Society: Series B 

72(4):417–473  2010.

[15] Nicolai Meinshausen and Bin Yu. Lasso-type recovery of sparse representations for high-dimensional

data. The Annals of Statistics  37(1):246–270  2009.

[16] T.M. Mitchell  S.V. Shinkareva  A. Carlson  K.M. Chang  V.L. Malave  R.A. Mason  and M.A Just. Pre-
dicting human brain activity associated with the meanings of nouns. Science  320(5880):1191–1195 
2008.

[17] Sahand N. Negahban  Pradeep Ravikumar  Martin J. Wainwright  and Bin Yu. A uniﬁed framework
Statistical Science 

for high-dimensional analysis of m-estimators with decomposable regularizers.
27(4):538–557  2012.

[18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming  103(1):127–

152  2005.

[19] G. Obozinski  M.J. Wainwright  and M.I Jordan. Support union recovery in high-dimensional multivariate

regression. The Annals of Statistics  39(1):1–47  2011.

[20] Hua Ouyang  Niao He  Long Tran  and Alexander Gray. Stochastic alternating direction method of
In Proceedings of the 30th International Conference on Machine Learning  pages 80–88 

multipliers.
2013.

[21] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Restricted eigenvalue properties for correlated

gaussian designs. The Journal of Machine Learning Research  11(8):2241–2259  2010.

[22] A.J. Rothman  E. Levina  and J Zhu. Sparse multivariate regression with covariance estimation. Journal

of Computational and Graphical Statistics  19(4):947–962  2010.

[23] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[24] B.A. Turlach  W.N. Venables  and S.J Wright.

47(3):349–363  2005.

Simultaneous variable selection.

Technometrics 

[25] M. Yuan and Y Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B  68(1):49–67  2005.

[26] Jian Zhang. A probabilistic framework for multi-task learning. PhD thesis  Carnegie Mellon University 

Language Technologies Institute  School of Computer Science  2006.

9

,Han Liu
Lie Wang
Tuo Zhao