2015,Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients,Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning  statistics and data analysis  but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However  to obtain high quality solutions  the number of random features should be the same order of magnitude as the number of data points  making such approach not directly applicable to the regime with millions of data points.We propose a simple  computationally efficient  and memory friendly algorithm based on the ``doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis  such as kernel PCA  CCA and SVD. Despite the \emph{non-convex} nature of these problems  our method enjoys theoretical guarantees that it converges at the rate $\Otil(1/t)$ to the global optimum  even for the top $k$ eigen subspace. Unlike many alternatives  our algorithm does not require explicit orthogonalization  which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.,Scale Up Nonlinear Component Analysis with

Doubly Stochastic Gradients

Bo Xie1  Yingyu Liang2  Le Song1
1Georgia Institute of Technology

bo.xie@gatech.edu  lsong@cc.gatech.edu

2Princeton University

yingyul@cs.princeton.edu

Abstract

Nonlinear component analysis such as kernel Principle Component Analysis
(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in
machine learning  statistics and data analysis  but they cannot scale up to big
datasets. Recent attempts have employed random feature approximations to con-
vert the problem to the primal form for linear computational complexity. How-
ever  to obtain high quality solutions  the number of random features should be
the same order of magnitude as the number of data points  making such approach
not directly applicable to the regime with millions of data points.
We propose a simple  computationally efﬁcient  and memory friendly algorithm
based on the “doubly stochastic gradients” to scale up a range of kernel nonlinear
component analysis  such as kernel PCA  CCA and SVD. Despite the non-convex
nature of these problems  our method enjoys theoretical guarantees that it con-
verges at the rate ˜O(1/t) to the global optimum  even for the top k eigen subspace.
Unlike many alternatives  our algorithm does not require explicit orthogonaliza-
tion  which is infeasible on big datasets. We demonstrate the effectiveness and
scalability of our algorithm on large scale synthetic and real world datasets.

Introduction

1
Scaling up nonlinear component analysis has been challenging due to prohibitive computation and
memory requirements. Recently  methods such as Randomized Component Analysis (RCA) [12]
are able to scale to larger datasets by leveraging random feature approximation. Such methods ap-
proximate the kernel function by using explicit random feature mappings  then perform subsequent
steps in the primal form  resulting in linear computational complexity. Nonetheless  theoretical anal-
ysis [18  12] shows that in order to get high quality results  the number of random features should
grow linearly with the number of data points. Experimentally  one often sees that the statistical
performance of the algorithm improves as one increases the number of random features.
Another approach to scale up the kernel component analysis is to use stochastic gradient descent and
online updates [15  16]. These stochastic methods have also been extended to the kernel case [9  5 
8]. They require much less computation than their batch counterpart  converge in O(1/t) rate  and
are naturally applicable to streaming data setting. Despite that  they share a severe drawback: all
data points used in the updates need to be saved  rendering them impractical for large datasets.
In this paper  we propose to use the “doubly stochastic gradients” for nonlinear component analysis.
This technique is a general framework for scaling up kernel methods [6] for convex problems and
has been successfully applied to many popular kernel machines such as kernel SVM  kernel ridge
regressions  and Gaussian process. It uses two types of stochastic approximation simultaneously:
random data points instead of the whole dataset (as in stochastic update rules)  and random features
instead of the true kernel functions (as in RCA). These two approximations lead to the following

1

beneﬁts: (1) Computation efﬁciency The key computation is the generation of a mini-batch of
random features and the evaluation of them on a mini-batch of data points  which is very efﬁcient.
(2) Memory efﬁciency Instead of storing training data points  we just keep a small program for
regenerating the random features  and sample previously used random features according to pre-
speciﬁed random seeds. This leads to huge savings: the memory requirement up to step t is O(t) 
independent of the dimension of the data. (3) Adaptibility Unlike other approaches that can only
work with a ﬁxed number of random features beforehand  doubly stochastic approach is able to
increase the model complexity by using more features when new data points arrive  and thus enjoys
the advantage of nonparametric methods.
Although on ﬁrst look our method appears similar to the approach in [6]  the two methods are
fundamentally different. In [6]  they address convex problems  whereas our problem is highly non-
convex. The convergence result in [6] crucially relies on the properties of convex functions  which
do not translate to our problem. Instead  our analysis centers around the stochastic update of power
iterations  which uses a different set of proof techniques.
In this paper  we make the following contributions.
General framework We show that the general framework of doubly stochastic updates can be
applied in various kernel component analysis tasks  including KPCA  KSVD  KCCA  etc..
Strong theoretical guarantee We prove that the ﬁnite time convergence rate of doubly stochastic
approach is ˜O(1/t). This is a signiﬁcant result since 1) the global convergence result is w.r.t. a non-
convex problem; 2) the guarantee is for update rules without explicit orthogonalization. Previous
works require explicit orthogonalization  which is impractical for kernel methods on large datasets.
Strong empirical performance Our algorithm can scale to datasets with millions of data points.
Moreover  the algorithm can often ﬁnd much better solutions thanks to the ability to use many more
random features. We demonstrate such beneﬁts on both synthetic and real world datasets.
Since kernel PCA is a typical task  we focus on it in the paper and provide a description of other
tasks in Section 4.3. Although we only state the guarantee for kernel PCA  the analysis naturally
carries over to the other tasks.
2 Related work
Many efforts have been devoted to scale up kernel methods. The random feature approach [17  18]
approximates the kernel function with explicit random feature mappings and solves the problem
in primal form  thus circumventing the quadratic computational complexity. It has been applied
to various kernel methods [11  6  12]  among which most related to our work is RCA [12]. One
drawback of RCA is that their theoretical guarantees are only for kernel matrix approximation: it
does not say anything about how close the solution obtained from RCA is to the true solution. In
contrast  we provide a ﬁnite time convergence rate of how our solution approaches the true solution.
In addition  even though a moderate size of random features can work well for tens of thousands of
data points  datasets with tens of millions of data points require many more random features. Our
online approach allows the number of random features  hence the ﬂexibility of the function class 
to grow with the number of data points. This makes our method suitable for data streaming setting 
which is not possible for previous approaches.
Online algorithms for PCA have a long history. Oja proposed two stochastic update rules for ap-
proximating the ﬁrst eigenvector and provided convergence proof in [15  16]  respectively. These
rules have been extended to the generalized Hebbian update rules [19  20  3] that compute the top
k eigenvectors (the subspace case). Similar ones have also been derived from the perspective of
optimization and stochastic gradient descent [20  2]. They are further generalized to the kernel
case [9  5  8]. However  online kernel PCA needs to store all the training data  which is impracti-
cal for large datasets. Our doubly stochastic method avoids this problem by using random features
and keeping only a small program for regenerating previously used random features according to
pre-speciﬁed seeds. As a result  it can scale up to tens of millions of data points.
For ﬁnite time convergence rate  [3] proved the O(1/t) rate for the top eigenvector in linear PCA
using Oja’s rule. For the same task  [21] proposed a noise reduced PCA with linear convergence rate 
where the rate is in terms of epochs  i.e.  number of passes over the whole dataset. The noisy power
method presented in [7] provided linear convergence for a subspace  although it only converges
linearly to a constant error level. In addition  the updates require explicit orthogonalization  which

2

1 = DSGD-KPCA(P(x)  k)

i=1)

j=1) ∈ Rk.

j hihi  for j = 1  . . .   i − 1.

Sample ωi ∼ P(ω) with seed i.
h = h + φωi (x)αi.

Sample xi ∼ P(x).
Sample ωi ∼ P(ω) with seed i.
hi = Evaluate(xi {αj}i−1
αi = ηiφωi (xi)hi.
αj = αj − ηiα(cid:62)

Algorithm 2: h = Evaluate(x  {αi}t
Require: P(ω)  φω(x).
1: Set h = 0 ∈ Rk.
2: for i = 1  . . .   t do
3:
4:
5: end for

Algorithm 1: {αi}t
Require: P(ω)  φω(x).
1: for i = 1  . . .   t do
2:
3:
4:
5:
6:
7: end for
is impractical for kernel methods. In comparison  our method converges in O(1/t) for a subspace 
without the need for orthogonalization.
3 Preliminaries
(cid:80)n
Kernels and Covariance Operators A kernel k(x  y) : X × X (cid:55)→ R is a function that is
positive-deﬁnite (PD)  i.e.  for all n > 1  c1  . . .   cn ∈ R  and x1  . . .   xn ∈ X   we have
i j=1 cicjk(xi  xj) ≥ 0. A reproducing kernel Hilbert space (RKHS) F on X is a Hilbert
space of functions from X to R. F is an RKHS if and only if there exists a k(x  x(cid:48)) such
that ∀x ∈ X   k(x ·) ∈ F  and ∀f ∈ F (cid:104)f (·)  k(x ·)(cid:105)F = f (x). Given P(x)  k(x  x(cid:48)) with
RKHS F  the covariance operator A : F (cid:55)→ F is a linear self-adjoint operator deﬁned as
Af (·) := Ex[f (x) k(x ·)]  ∀f ∈ F  and furthermore (cid:104)g  Af(cid:105)F = Ex[f (x) g(x)]  ∀g ∈ F.
Let F = (f1(·)  f2(·)  . . .   fk(·)) be a list of k functions in the RKHS  and we deﬁne matrix-like
notation AF (·) := (Af1(·)  . . .   Afk(·))  and F (cid:62)AF is a k × k matrix  whose (i  j)-th element
is (cid:104)fi  Afj(cid:105)F . The outer-product of a function v ∈ F deﬁnes a linear operator vv(cid:62) such that
(vv(cid:62))f (·) := (cid:104)v  f(cid:105)F v(·)  ∀f ∈ F. Let V = (v1(·)  . . .   vk(·)) be a list of k functions  then the
(cid:80)k
weighted sum of a set of linear operators can be denoted using matrix-like notation as V ΣkV (cid:62) :=
i=1 λiviv(cid:62)

i   where Σk is a diagonal matrix with λi on the i-th entry of the diagonal.

j=1 αj

i k(xj ·)  where {αi}k

vi =(cid:80)n
(cid:82)

Eigenfunctions and Kernel PCA A function v is an eigenfunction of A with the corresponding
eigenvalue λ if Av(·) = λv(·). Given a set of eigenfunctions {vi} and associated eigenvalues {λi} 
where (cid:104)vi  vj(cid:105)F = δij  we can write the eigen-decomposion as A = V ΣkV (cid:62) + V⊥Σ⊥V (cid:62)
⊥   where V
is the list of top k eigenfunctions  Σk is a diagonal matrix with the corresponding eigenvalues  V⊥ is
(cid:80)
the list of the rest of the eigenfunctions  and Σ⊥ is a diagonal matrix with the rest of the eigenvalues.
Kernel PCA aims to identify the the top k subspace V . In the ﬁnite data case  the empirical co-
i k(xi ·) ⊗ k(xi ·). According to the representer theorem  we have
variance operator is A = 1
n
i=1 ∈ Rn are weights for the data points. Using Av(·) = λv(·)
and the kernel trick  we have Kαi = λiαi  where K is the n × n Gram matrix.
Random feature approximation The random feature approximation for shift-invariant ker-
nels k(x  y) = k(x − y)  e.g.  Gaussian RBF kernel  relies on the identity k(x − y) =
Rd eiω(cid:62)(x−y) dP(ω) = E [φω(x)φω(y)] since the Fourier transform of a PD function is non-
negative  thus can be considered as a (scaled) probability measure [17]. We can therefore ap-
proximate the kernel function as an empirical average of samples from the distribution. In other
words  k(x  y) ≈ 1
i are i.i.d. samples drawn from P(ω). For
Gaussian RBF kernel  k(x − x(cid:48)) = exp(−(cid:107)x − x(cid:48)(cid:107)2/2σ2)  this yields a Gaussian distribution
P(ω) = exp(−σ2(cid:107)ω(cid:107)2/2). See [17] for more details.
4 Algorithm
In this section  we describe an efﬁcient algorithm based on the “doubly stochastic gradients” to
scale up kernel PCA. KPCA is essentially an eigenvalue problem in a functional space. Traditional
approaches convert it to the dual form  leading to another eigenvalue problem whose size equals
the number of training points  which is not scalable. Other approaches solve it in the primal form
with stochastic functional gradient descent. However  these algorithms need to store all the training
points seen so far. They quickly run into memory issues when working with millions of data points.
We propose to tackle the problem with “doubly stochastic gradients”  in which we make two unbi-
ased stochastic approximations. One stochasticity comes from sampling data points as in stochastic
gradient descent. Another source of stochasticity is from random features to approximate the kernel.

(cid:80)
i φωi(x)φωi(y)  where {(ωi)}B

B

3

One technical difﬁculty in designing doubly stochastic KPCA is an explicit orthogonalization step
required in the update rules  which ensures the top k eigenfunctions are orthogonal. This is infeasible
for kernel methods on a large dataset since it requires solving an increasingly larger KPCA problem
in every iteration. To solve this problem  we formulate the orthogonality constraints into Lagrange
multipliers which leads to an Oja-style update rule. The new update enjoys small per iteration
complexity and converges to the ground-truth subspace.
We present the algorithm by ﬁrst deriving the stochastic functional gradient update without random
feature approximations  then introducing the doubly stochastic updates. For simplicity of presenta-
tion  the following description uses one data point and one random feature at a time  but typically a
mini-batch of data points and random features are used in each iteration.
4.1 Stochastic functional gradient update
Kernel PCA can be formulated as the following non-convex optimization problem

tr(cid:0)G(cid:62)AG(cid:1) s.t. G(cid:62)G = I 

max

where G :=(cid:0)g1  . . .   gk(cid:1) and gi is the i-th function.
where gt =(cid:2)g1

t (xt)(cid:3)(cid:62)

t (xt)  . . .   gk

G

Gradient descent on the Lagrangian leads to Gt+1 = Gt + ηt
approximation for A: Atf (·) = f (xt) k(xt ·)  we have AtGt = k(xt ·)g(cid:62)

t

(cid:0)I − GtG(cid:62)
(cid:1) + ηtk(xt ·)g(cid:62)

t .

Gt+1 = Gt

. Therefore  the update rule is

(cid:0)I − ηtgtg(cid:62)

t

(cid:1) AGt. Using a stochastic

t and G(cid:62)

t AtGt = gtg(cid:62)
t  

(1)

(2)

(3)

(cid:62)

 

(cid:16)

(cid:62)(cid:17)

This rule can also be derived using stochastic gradient and Oja’s rule [15  16].
4.2 Doubly stochastic update
The update rule (2) must store all the data points it has seen so far  which is impractical for large scale
datasets. To address this issue  we use the random feature approximation k(x ·) ≈ φωi(x)φωi(·).
Denote Ht the function we get at iteration t  the update rule becomes

t (xt)(cid:3)(cid:62)

where ht is the evaluation of Ht at the current data point: ht =(cid:2)h1

I − ηththt

Ht+1 = Ht

+ ηtφωt(xt)φωt(·)ht

t AtGt

. The speciﬁc
updates in terms of the coefﬁcients are summarized in Algorithms 1 and 2. Note that in theory new
random features are drawn in each iteration  but in practice one can revisit these random features.
4.3 Extensions
Locating individual eigenfunctions The algorithm only ﬁnds the eigen subspace  but not necessar-
ily individual eigenfunctions. A modiﬁed version  called Generalized Hebbian Algorithm (GHA)

[19] can be used for this purpose: Gt+1 = Gt + ηtAtGt − ηtGt UT(cid:2)G(cid:62)

(cid:3)  where UT [·] is an

t (xt)  . . .   hk

operator that sets the lower triangular parts to zero.
Latent variable models and kernel SVD Recently  spectral methods have been proposed to learn
latent variable models with provable guarantees [1  22]  in which the key computation is SVD. Our
algorithm can be straightforwardly extended to solve kernel SVD  with two simultaneous update
rules. The algorithm is summarized in Algorithm 3. See the supplementary for derivation details.
Kernel CCA and generalized eigenvalue problem Given two variables X and Y   CCA ﬁnds two
projections such that the correlations between the two projected variables are maximized.
It is
equivalent to a generalized eigenvalue problem  which can also be solved in our framework. We
present the updates for coefﬁcients in Algorithm 4  and derivation details in the supplementary.
Kernel sliced inverse regression Kernel sliced inverse regression [10] aims to do sufﬁcient dimen-
sion reduction in which the found low dimension representation preserves the statistical correlation
with the targets. It also reduces to a generalized eigenvalue problem  and has been shown to ﬁnd the
same subspace as KCCA [10].
5 Analysis
In this section  we provide ﬁnite time convergence guarantees for our algorithm. As discussed
in the previous section  explicit orthogonalization is not scalable for the kernel case  therefore we

4

1

Algorithm 3: DSGD-KSVD
Require: P(ω)  φω(x)  k.
Output: {αi  βi}t
1: for i = 1  . . .   t do
2:
3:
4:
5:
6: W = uiv(cid:62)
7:
8:
9:
10:
11: end for

Sample xi ∼ P(x). Sample yi ∼ P(y).
Sample ωi ∼ P(ω) with seed i.
ui = Evaluate(xi {αj}i−1
vi = Evaluate(yi {βj}i−1

j=1) ∈ Rk.
j=1) ∈ Rk.

i + viu(cid:62)
αi = ηiφωi (xi)vi.
βi = ηiφωi (yi)ui.
αj = αj − ηiW αj  for j = 1  . . .   i − 1.
βj = βj − ηiW βj  for j = 1  . . .   i − 1.

i

1

Algorithm 4: DSGD-KCCA
Require: P(ω)  φω(x)  k.
Output: {αi  βi}t
1: for i = 1  . . .   t do
2:
3:
4:
5:
6: W = uiv(cid:62)
7:
8:
9: end for

Sample xi ∼ P(x). Sample yi ∼ P(y).
Sample ωi ∼ P(ω) with seed i.
ui = Evaluate(xi {αj}i−1
vi = Evaluate(yi {βj}i−1
αi = ηiφωi (xi) [vi − W ui].
βi = ηiφωi (yi) [ui − W vi].

j=1) ∈ Rk.
j=1) ∈ Rk.

i + viu(cid:62)

i

need to provide guarantees for the updates without orthogonalization. This challenge is even more
prominent when using random features  since it introduces additional variance.
the top k-dimension subspace. Although the convergence
Furthermore  our guarantees are w.r.t.
without normalization for a top eigenvector has been established before [15  16]  the subspace case
is complicated by the fact that there are k angles between k-dimension subspaces  and we need to
bound the largest angle. To the best of our knowledge  our result is the ﬁrst ﬁnite time convergence
result for a subspace without explicit orthogonalization.
Note that even though it appears our algorithm is similar to [6] on the surface  the underlying analysis
In [6]  the result only applies to convex problems where every local
is fundamentally different.
optimum is a global optimum while the problems we consider are highly non-convex. As a result 
many techniques that [6] builds upon are not applicable.
Conditions and Assumptions We will focus on the case when a good initialization V0 is given:

0 V0 = I  cos2 θ(V  V0) ≥ 1/2.
V (cid:62)

(4)
In other words  we analyze the later stage of the convergence  which is typical in the literature (e.g. 
[21]). The early stage can be analyzed using established techniques (e.g.  [3]). In practice  one can
achieve a good initialization by solving a small RCA problem [12] with  e.g.
thousands  of data
points and random features.
Throughout the paper we suppose |k(x  x(cid:48))| ≤ κ |φω(x)| ≤ φ and regard κ and φ as constants.
Note that this is true for all the kernels and corresponding random features considered. We further
regard the eigengap λk − λk+1 as a constant  which is also true for typical applications and datasets.
5.1 Update without random features
Our guarantee is on the cosine of the principal angle between the computed subspace and the ground
truth eigen subspace (also called potential function): cos2 θ(V  Gt) = minw

.

(cid:107)V (cid:62)Gtw(cid:107)2
(cid:107)Gtw(cid:107)2

Consider the two different update rules  one with explicit orthogonalization and another without

Ft+1 ← orth(Ft + ηtAtFt)
Gt+1 ← Gt + ηt

(cid:0)I − GtG(cid:62)

(cid:1) AtGt

t

where At is the empirical covariance of a mini-batch. Our ﬁnal guarantee for Gt is the following.
Theorem 1 Assume (4) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t  (cid:107)A − Ai(cid:107) <
(λk − λk+1)/8. There exist step sizes ηi = O(1/i) such that

1 − cos2 θ(V  Gt) = O(1/t).

The convergence rate O(1/t) is in the same order as that of computing only the top eigenvector in
linear PCA [3]. The bound requires the mini-batch size is large enough so that the spectral norm of
A is approximated up to the order of the eigengap. This is because the increase of the potential is in
the order of the eigengap. Similar terms appear in the analysis of the noisy power method [7] which 
however  requires orthogonalization and is not suitable for the kernel case. We do not specify the

5

mini-batch size  but by assuming suitable data distributions  it is possible to obtain explicit bounds;
see for example [23  4].

t+1 ≥ c2
c2

t )) − O(η2
t ).

Proof sketch We ﬁrst prove the guarantee for the orthogonalized subspace Ft which is more conve-
nient to analyze  and then show that the updates for Ft and Gt are ﬁrst order equivalent so Gt enjoys
the same guarantee. To do so  we will require lemma 2 and 3 below
Lemma 2 1 − cos2 θ(V  Ft) = O(1/t).
Let c2

t denote cos2 θ(V  Ft)  then a key step in proving the lemma is to show the following recurrence
(5)

t (1 + 2ηt(λk − λk+1 − 2(cid:107)A − At(cid:107))(1 − c2

We will need the mini-batch size large enough so that 2(cid:107)A − At(cid:107) is smaller than the eigen-gap.
Another key element in the proof of the theorem is the ﬁrst order equivalence of the two update
rules. To show this  we introduce F (Gt) ← orth(Gt + ηtAtGt) to denote the subspace by applying
the update rule of Ft on Gt. We show that the potentials of Gt+1 and F (Gt) are close:
Lemma 3 cos2 θ(V  Gt+1) = cos2 θ(V  F (Gt)) ± O(η2
t ).
The lemma means that applying the two update rules to the same input will result in two sub-
spaces with similar potentials. Then by (5)  we have 1 − cos2 θ(V  Gt) = O(1/t) which
leads to our theorem. The proof of Lemma 3 is based on the observation that cos2 θ(V  X) =
λmin(V (cid:62)X(X(cid:62)X)−1X(cid:62)V ). Comparing the Taylor expansions w.r.t. ηt for X = Gt+1 and
X = F (Gt) leads to the lemma.
5.2 Doubly stochastic update
The Ht computed in the doubly stochastic update is no longer in the RKHS so the principal angle is
not well deﬁned. Instead  we will compare the evaluation of functions from Ht and the true principal
subspace V respectively on a point x. Formally  we show that for any function v ∈ V with unit norm
(cid:107)v(cid:107)F = 1  there exists a function h in Ht such that for any x  err := |v(x) − h(x)|2 is small with
high probability.
To do so  we need to introduce a companion update rule: ˜Gt+1 ← ˜Gt + ηtk(xt ·)h(cid:62)
t − ηt ˜Gthth(cid:62)
resulting in function in the RKHS  but the update makes use of function values from ht ∈ Ht which
is outside the RKHS. Let w = ˜G(cid:62)v be the coefﬁcients of v projected onto ˜G  h = Htw  and
z = ˜Gtw. Then the error can be decomposed as

t

|v(x) − h(x)|2 = |v(x) − z(x) + z(x) − h(x)|2 ≤ 2|v(x) − z(x)|2 + 2|z(x) − h(x)|2

By deﬁnition  (cid:107)v − z(cid:107)2F = (cid:107)v(cid:107)2F −(cid:107)z(cid:107)2F ≤ 1−cos2 θ(V  ˜Gt)  so the ﬁrst error term can be bounded
by the guarantee on ˜Gt  which can be obtained by similar arguments in Theorem 1. For the second
term  note that ˜Gt is deﬁned in such a way that the difference between z(x) and h(x) is a martingale 
which can be bounded by careful analysis.
Theorem 4 Assume (4) and suppose the mini-batch sizes satisfy that for any 1 ≤ i ≤ t  (cid:107)A − Ai(cid:107) <
(λk − λk+1)/8 and are of order Ω(ln t
δ ). There exist step sizes ηi = O(1/i)  such that the following
˜Gi) = O(1) for all 1 ≤ i ≤ t  then for any x and any
holds. If Ω(1) = λk( ˜G(cid:62)
function v in the span of V with unit norm (cid:107)v(cid:107)F = 1  we have that with probability at least 1 − δ 

there exists h in the span of Ht satisfying |v(x) − h(x)|2 = O(cid:0) 1

˜Gi) ≤ λ1( ˜G(cid:62)

(cid:1) .

i

i

t ln t

δ

The point-wise error scales as ˜O(1/t) with the step t. Besides the condition that (cid:107)A − Ai(cid:107) is up
to the order of the eigengap  we additionally need that the random features approximate the kernel
function up to constant accuracy on all the data points up to time t  which eventually leads to Ω(ln t
δ )
mini-batch sizes. Finally  we need ˜G(cid:62)
˜Gi to be roughly isotropic  i.e.  ˜Gi is roughly orthonormal.
Intuitively  this should be true for the following reasons: ˜G0 is orthonormal; the update for ˜Gt is
close to that for Gt  which in turn is close to Ft that are orthonormal.

i

6

(cid:124)
(cid:125)
≤ 2κ2 (cid:107)v − z(cid:107)2F

(cid:123)(cid:122)

(I: Lemma 5)

(cid:124)

+ 2|z(x) − h(x)|2

(II: Lemma 6)

(cid:123)(cid:122)

(cid:125)

.

(6)

(a)

(b)

(a)

(b)

Figure 1: (a) Convergence of our algorithm on
the synthetic dataset. It is on par with the ˜O(1/t)
rate denoted by the dashed red line. (b) Recovery
of the top three eigenfunctions. Our algorithm (in
red) matches the ground-truth (dashed blue).

Figure 2: Visualization of the molecular space
dataset by the ﬁrst two principal components.
Bluer dots represent lower PCE values while red-
der dots are for higher PCE values. (a) Kernel
PCA; (b) linear PCA. (Best viewed in color)

Proof sketch In order to bound term I in (6)  we show that

Lemma 5 1 − cos2 θ(V  ˜Gt) = O(cid:0) 1

(cid:1).

t ln t

δ

This is proved by following similar arguments to get the recurrence (5)  except with an additional
error term  which is caused by the fact that the update rule for ˜Gt+1 is using the evaluation ht(xt)
rather than ˜gt(xt). Bounding this additional term thus relies on bounding the difference between
ht(x) − ˜gt(x)  which is also what we need for bounding term II in (6). For this  we show:
Lemma 6 For any x and unit vector w  with probability ≥ 1 − δ over (Dt  ωt)  |˜gt(x)w −

ht(x)w|2 = O(cid:0) 1
t ln(cid:0) t

δ

(cid:1)(cid:1) .

The key to prove this lemma is that our construction of ˜Gt makes sure that the difference between
˜gt(x)w and ht(x)w consists of their difference in each time step. Furthermore  the difference forms
a martingale and thus can be bounded by Azuma’s inequality. See the supplementary for the details.
6 Experiments
Synthetic dataset with analytical solution We ﬁrst verify the convergence rate of DSGD-KPCA
on a synthetic dataset with analytical solution of eigenfunctions [24]. If the data follow a Gaussian
distribution  and we use a Gaussian kernel  then the eigenfunctions are given by the Hermite poly-
nomials. We generated 1 million such 1-D data points  and ran DSGD-KPCA with a total of 262144
random features. In each iteration  we use a data mini-batch of size 512  and a random feature mini-
batch of size 128. After all random features are generated  we revisit and adjust the coefﬁcients of
existing random features. The kernel bandwidth is set as the true bandwidth. The step size is sched-
uled as ηt = θ0/(1 + θ1t)  where θ0 and θ1 are two parameters. We use a small θ1 ≈ 0.01 such
that in early stages the step size is large enough to arrive at a good initial solution. Figure 1a shows
the convergence rate of the proposed algorithm seeking top k = 3 subspace. The potential function
is the squared sine of the principle angle. We can see the algorithm indeed converges at the rate
O(1/t). Figure 1b show the recovered top k = 3 eigenfunctions compared with the ground-truth.
The solution coincides with one eigenfunction  and deviates only slightly from two others.
Kernel PCA visualization on molecular space dataset MolecularSpace dataset contains 2.3 mil-
lion molecular motifs [6]. We are interested in visualizing the dataset with KPCA. The data are
represented by sorted Coulomb matrices of size 75 × 75 [14]. Each molecule also has an attribute
called power conversion efﬁciency (PCE). We use a Gaussian kernel with bandwidth chosen by
the “median trick”. We ran kernel PCA with a total of 16384 random features  with a feature
mini-batch size of 512  and data mini-batch size of 1024. We ran 4000 iterations with step size
ηt = 1/(1 + 0.001 ∗ t). Figure 2 presents visualization by projecting the data onto the top two prin-
ciple components. Compared with linear PCA  KPCA shrinks the distances between the clusters
and brings out the important structures in the dataset. We can also see higher PCE values tend to lie
towards the center of the ring structure.
Nonparametric Latent Variable Model We learn latent variable models with DSGD-KSVD using
one million data points [22]  achieving higher quality solutions compared with two other approaches.
The dataset consists of two latent components  one is a Gaussian distribution and the other a Gamma
distribution with shape parameter α = 1.2. DSGD-KSVD uses a total of 8192 random features  and
uses a feature mini-batch of size 256 and a data mini-batch of size 512. We compare with 1) random

7

10510610−410−310−210−1100Number of data pointsPotential function ConvergenceO(1/t)−2−1012−0.1−0.0500.050.1 estimatedgroundtruth(a)

(b)

(c)

Figure 4: Comparison on
KUKA dataset.

Figure 3: Recovered latent components: (a) DSGD-KSVD  (b) 2048
random features  (c) 2048 Nystrom features.
Fourier features  and 2) random Nystrom features  both of ﬁxed 2048 functions [12]. Figures 3
shows the learned conditional distributions for each component. We can see DSGD-KSVD achieves
almost perfect recovery  while Fourier and Nystrom random feature methods either confuse high
density areas or incorrectly estimate the spread of conditional distributions.
KCCA MNIST8M We compare our algorithm on MNIST8M in the KCCA task  which consists of
8.1 million digits and their transformations. We divide each image into the left and right halves  and
learn their correlations. The evaluation criteria is the total correlations on the top k = 50 canonical
correlation directions measured on a separate test set of size 10000. We compare with 1) random
Fourier and 2) random Nystrom features on both total correlation and running time. Our algorithm
uses a total of 20480 features  with feature mini-batches of size 2048 and data mini-batches of size
1024  and with 3000 iterations. The kernel bandwidth is set using the “median trick” and is the same
for all methods. All algorithms are run 5 times  and the mean is reported. The results are presented
in Table 1. Our algorithm achieves the best test-set correlations in comparable run time with random
Fourier features. This is especially signiﬁcant for random Fourier features  since the run time would
increase by almost four times if double the number of features were used. In addition  Nystrom
features generally achieve better results than Fourier features since they are data dependent. We can
also see that for large datasets  it is important to use more random features for better performance.

Table 1: KCCA results on MNIST 8M (top 50 largest correlations)

# of feat

256
512
1024
2048
4096

Random features
corrs.
minutes
25.2
30.7
35.3
38.8
41.5

3.2
7.0
13.9
54.3
186.7

Nystrom features
corrs.
minutes
30.4
35.3
38.0
41.1
42.7

3.0
5.1
10.1
27.0
71.0

DSGD-KCCA (20480)
corrs.
43.5

minutes
183.2

linear CCA

minutes

1.1

corrs.
27.4

Kernel sliced inverse regression on KUKA dataset We evaluate our algorithm under the setting
of kernel sliced inverse regression [10]  a way to perform sufﬁcient dimension reduction (SDR)
for high dimension regression. After performing SDR  we ﬁt a linear regression model using the
projected input data  and evaluate mean squared error (MSE). The dataset records rhythmic motions
of a KUKA arm at various speeds  representing realistic settings for robots [13]. We use a variant
that contains 2 million data points generated by the SL simulator. The KUKA robot has 7 joints 
and the high dimension regression problem is to predict the torques from positions  velocities and
accelerations of the joints. The input has 21 dimensions while the output is 7 dimensions. Since there
are seven independent joints  we set the reduced dimension to be seven. We randomly select 20% as
test set and out of the remaining training set  we randomly choose 5000 as validation set to select step
sizes. The total number of random features is 10240  with mini-feature batch and mini-data batch
both equal to 1024. We run a total of 2000 iterations using step size ηt = 15/(1+0.001∗t). Figure 4
shows the regression errors for different methods. The error decreases with more random features 
and our algorithm achieves lowest MSE by using 10240 random features. Nystrom features do not
perform as well in this setting probably because the spectrum decreases slowly (there are seven
independent joints) as Nystrom features are known to work well for fast decreasing spectrum.
Acknowledge
The research was supported in part by NSF/NIH BIGDATA 1R01GM108341  ONR N00014-15-
1-2340  NSF IIS-1218749  NSF CAREER IIS-1350983  NSF CCF-0832797  CCF-1117309  CCF-
1302518  DMS-1317308  Simons Investigator Award  and Simons Collaboration Grant.

8

−10−505101500.050.10.150.20.25 Estimated Component2Estimated Component1True Component2True Component1−10−505101500.050.10.150.20.25 Estimated Component2Estimated Component1True Component2True Component1−10−505101500.050.10.150.20.25 Estimated Component2Estimated Component1True Component2True Component15121024204840961024000.020.040.060.08Random feature dimensionMean squared error Random Fourier featuresNystrom featuresDSGDReferences
[1] A. Anandkumar  D. P. Foster  D. Hsu  S. M. Kakade  and Y.-K. Liu. Two svds sufﬁce: Spectral decom-

positions for probabilistic topic modeling and latent dirichlet allocation. CoRR  abs/1204.6703  2012.

[2] R. Arora  A. Cotter  and N. Srebro. Stochastic optimization of pca with capped msg. In Advances in

Neural Information Processing Systems  pages 1815–1823  2013.

[3] A. Balsubramani  S. Dasgupta  and Y. Freund. The fast convergence of incremental pca. In Advances in

Neural Information Processing Systems  pages 3174–3182  2013.

[4] T. T. Cai and H. H. Zhou. Optimal rates of convergence for sparse covariance matrix estimation. The

Annals of Statistics  40(5):2389–2420  2012.

[5] T.-J. Chin and D. Suter. Incremental kernel principal component analysis. IEEE Transactions on Image

Processing  16(6):1662–1674  2007.

[6] B. Dai  B. Xie  N. He  Y. Liang  A. Raj  M.-F. F. Balcan  and L. Song. Scalable kernel methods via doubly

stochastic gradients. In Advances in Neural Information Processing Systems  pages 3041–3049  2014.

[7] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In Advances in

Neural Information Processing Systems  pages 2861–2869  2014.

[8] P. Honeine. Online kernel principal component analysis: A reduced-order model. IEEE Trans. Pattern

Anal. Mach. Intell.  34(9):1814–1826  2012.

[9] K. Kim  M. O. Franz  and B. Sch¨olkopf. Iterative kernel principal component analysis for image modeling.

IEEE Transactions on Pattern Analysis and Machine Intelligence  27(9):1351–1366  2005.

[10] M. Kim and V. Pavlovic. Covariance operator based dimensionality reduction with extension to semi-
supervised settings. In International Conference on Artiﬁcial Intelligence and Statistics  pages 280–287 
2009.

[11] Q. Le  T. Sarlos  and A. J. Smola. Fastfood — computing hilbert space expansions in loglinear time. In

International Conference on Machine Learning  2013.

[12] D. Lopez-Paz  S. Sra  A. Smola  Z. Ghahramani  and B. Sch¨olkopf. Randomized nonlinear component

analysis. In International Conference on Machine Learning (ICML)  2014.

[13] F. Meier  P. Hennig  and S. Schaal. Incremental local gaussian regression. In Z. Ghahramani  M. Welling 
C. Cortes  N. Lawrence  and K. Weinberger  editors  Advances in Neural Information Processing Systems
27  pages 972–980. Curran Associates  Inc.  2014.

[14] G. Montavon  K. Hansen  S. Fazli  M. Rupp  F. Biegler  A. Ziehe  A. Tkatchenko  A. von Lilienfeld 
and K.-R. M¨uller. Learning invariant representations of molecules for atomization energy prediction. In
Neural Information Processing Systems  pages 449–457  2012.

[15] E. Oja. A simpliﬁed neuron model as a principal component analyzer. J. Math. Biology  15:267–273 

1982.

[16] E. Oja. Subspace methods of pattern recognition. John Wiley and Sons  New York  1983.
[17] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. Platt  D. Koller  Y. Singer 
and S. Roweis  editors  Advances in Neural Information Processing Systems 20. MIT Press  Cambridge 
MA  2008.

[18] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with random-

ization in learning. In Neural Information Processing Systems  2009.

[19] T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward network. Neural Net-

works  2:459–473  1989.

[20] N. N. Schraudolph  S. G¨unter  and S. V. N. Vishwanathan. Fast iterative kernel PCA. In B. Sch¨olkopf 
J. Platt  and T. Hofmann  editors  Advances in Neural Information Processing Systems 19  Cambridge
MA  June 2007. MIT Press.

[21] O. Shamir. A stochastic pca algorithm with an exponential convergence rate.

arXiv:1409.2848  2014.

arXiv preprint

[22] L. Song  A. Anamdakumar  B. Dai  and B. Xie. Nonparametric estimation of multi-view latent variable

models. In International Conference on Machine Learning (ICML)  2014.

[23] R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix? Journal of

Theoretical Probability  25(3):655–686  2012.

[24] C. K. I. Williams and M. Seeger. The effect of the input density distribution on kernel-based classiﬁers.
In P. Langley  editor  Proc. Intl. Conf. Machine Learning  pages 1159–1166  San Francisco  California 
2000. Morgan Kaufmann Publishers.

9

,Bo Xie
Yingyu Liang
Le Song
Shantanu Jain
Martha White
Predrag Radivojac
Zeyuan Allen-Zhu