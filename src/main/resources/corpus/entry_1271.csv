2019,DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters,We propose a novel spectral convolutional neural network (CNN) model on graph structured data  namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters  called feedback-looped filters  to provide better localization on vertices  while still attaining fast convergence and linear memory requirements. Theoretically  feedback-looped filters can guarantee convergence w.r.t. a specified error bound  and be applied universally to any graph without knowing its structure. Furthermore  the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.,DFNets: Spectral CNNs for Graphs with

Feedback-Looped Filters

Asiri Wijesinghe

Research School of Computer Science

The Australian National University
asiri.wijesinghe@anu.edu.au

Qing Wang

Research School of Computer Science

The Australian National University

qing.wang@anu.edu.au

Abstract

We propose a novel spectral convolutional neural network (CNN) model on graph
structured data  namely Distributed Feedback-Looped Networks (DFNets). This
model is incorporated with a robust class of spectral graph ﬁlters  called feedback-
looped ﬁlters  to provide better localization on vertices  while still attaining fast
convergence and linear memory requirements. Theoretically  feedback-looped
ﬁlters can guarantee convergence w.r.t. a speciﬁed error bound  and be applied
universally to any graph without knowing its structure. Furthermore  the propaga-
tion rule of this model can diversify features from the preceding layers to produce
strong gradient ﬂows. We have evaluated our model using two benchmark tasks:
semi-supervised document classiﬁcation on citation networks and semi-supervised
entity classiﬁcation on a knowledge graph. The experimental results show that our
model considerably outperforms the state-of-the-art methods in both benchmark
tasks over all datasets.

Introduction

1
Convolutional neural networks (CNNs) [20] are a powerful deep learning approach which has been
widely applied in various ﬁelds  e.g.  object recognition [29]  image classiﬁcation [14]  and semantic
segmentation [22]. Traditionally  CNNs only deal with data that has a regular Euclidean structure 
such as images  videos and text. In recent years  due to the rising trends in network analysis and
prediction  generalizing CNNs to graphs has attracted considerable interest [3  7  11  26]. However 
since graphs are in irregular non-Euclidean domains  this brings up the challenge of how to enhance
CNNs for effectively extracting useful features (e.g. topological structure) from arbitrary graphs.
To address this challenge  a number of studies have been devoted to enhancing CNNs by developing
ﬁlters over graphs. In general  there are two categories of graph ﬁlters: (a) spatial graph ﬁlters  and
(b) spectral graph ﬁlters. Spatial graph ﬁlters are deﬁned as convolutions directly on graphs  which
consider neighbors that are spatially close to a current vertex [1  9  11]. In contrast  spectral graph
ﬁlters are convolutions indirectly deﬁned on graphs  through their spectral representations [3  5  7].
In this paper  we follow the line of previous studies in developing spectral graph ﬁlters and tackle the
problem of designing an effective  yet efﬁcient CNNs with spectral graph ﬁlters.
Previously  Bruna et al. [3] proposed convolution operations on graphs via a spectral decomposition
of the graph Laplacian. To reduce learning complexity in the setting where the graph structure is
not known a priori  Henaff et al. [13] developed a spectral ﬁlter with smooth coefﬁcients. Then 
Defferrard et al. [7] introduced Chebyshev ﬁlters to stabilize convolution operations under coefﬁcient
perturbation and these ﬁlters can be exactly localized in k-hop neighborhood. Later  Kipf et al. [19]
proposed a simple layer-wise propagation model using Chebyshev ﬁlters on 1-hop neighborhood.
Very recently  some works attempted to develop rational polynomial ﬁlters  such as Cayley ﬁlters [21]

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: A simpliﬁed example of illustrating feedback-looped ﬁlters  where v1 is the current vertex
and the similarity of the colours indicates the correlation between vertices  e.g.  v1 and v5 are highly
correlated  but v2 and v6 are less correlated with v1: (a) an input graph  where λi is the original
frequency to vertex vi; (b) the feedforward ﬁltering  which attenuates some low order frequencies 
e.g. λ2  and amplify other frequencies  e.g. λ5 and λ6; (c) the feedback ﬁltering  which reduces the
error in the frequencies generated by (b)  e.g. λ6.

and ARMA1 [2]. From a different perspective  Petar et al. [31] proposed a self-attention based CNN
architecture for graph ﬁlters  which extracts features by considering the importance of neighbors.
One key idea behind existing works on designing spectral graph ﬁlters is to approximate the frequency
responses of graph ﬁlters using a polynomial function (e.g. Chebyshev ﬁlters [7]) or a rational
polynomial function (e.g. Cayley ﬁlters [21] and ARMA1 [2]). Polynomial ﬁlters are sensitive
to changes in the underlying graph structure. They are also very smooth and can hardly model
sharp changes  as illustrated in Figure 1. Rational polynomial ﬁlters are more powerful to model
localization  but they often have to trade off computational efﬁciency  resulting in higher learning and
computational complexities  as well as instability.
Contributions. In this work  we aim to develop a new class of spectral graph ﬁlters that can overcome
the above limitations. We also propose a spectral CNN architecture (i.e. DFNet) to incorporate these
graph ﬁlters. In summary  our contributions are as follows:

• Improved localization. A new class of spectral graph ﬁlters  called feedback-looped ﬁlters 
is proposed to enable better localization  due to its rational polynomial form. Basically 
feedback-looped ﬁlters consist of two parts: feedforward and feedback. The feedforward
ﬁltering is k-localized as polynomial ﬁlters  while the feedback ﬁltering is unique which
reﬁnes k-localized features captured by the feedforward ﬁltering to improve approximation
accuracy. We also propose two techniques: scaled-normalization and cut-off frequency to
avoid the issues of gradient vanishing/exploding and instabilities.
• Efﬁcient computation. For feedback-looped ﬁlters  we avoid the matrix inversion implied
by the denominator through approximating the matrix inversion with a recursion. Thus 
beneﬁted from this approximation  feedback-looped ﬁlters attain linear convergence time
and linear memory requirements w.r.t. the number of edges in a graph.
• Theoretical properties. Feedback-looped ﬁlters enjoy several nice theoretical properties.
Unlike other rational polynomial ﬁlters for graphs  they have theoretically guaranteed
convergence w.r.t. a speciﬁed error bound. On the other hand  they still have the universal
property as other spectral graph ﬁlters [17]  i.e.  can be applied without knowing the
underlying structure of a graph. The optimal coefﬁcients of feedback-looped ﬁlters are
learnable via an optimization condition for any given graph.
• Dense architecture. We propose a layer-wise propagation rule for our spectral CNN model
with feedback-looped ﬁlters  which densely connects layers as in DenseNet [15]. This
design enables our model to diversify features from all preceding layers  leading to a strong
gradient ﬂow. We also introduce a layer-wise regularization term to alleviate the overﬁtting
issue. In doing so  we can prevent the generation of spurious features and thus improve
accuracy of the prediction.

2

λ2(b) Feedforward(c) Feedbackv1v3v2v6v4v5λ5λ60110(a) Inputv1v3v2v6v4v5v1v3v2v6v4v5(q=1)(p=2)01λ2λ5λ6λ2λ5λ6To empirically verify the effectiveness of our work  we have evaluated feedback-looped ﬁlters within
three different CNN architectures over four benchmark datasets to compare against the state-of-the-art
methods. The experimental results show that our models signiﬁcantly outperform the state-of-the-art
methods. We further demonstrate the effectiveness of our model DFNet through the node embeddings
in a 2-D space of vertices from two datasets.

Dii =(cid:80)

2 Spectral Convolution on Graphs
Let G = (V  E  A) be an undirected and weighted graph  where V is a set of vertices  E ⊆ V × V
is a set of edges  and A ∈ Rn×n is an adjacency matrix which encodes the weights of edges. We
let n = |V | and m = |E|. A graph signal is a function x : V → R and can be represented
as a vector x ∈ Rn whose ith component xi is the value of x at the ith vertex in V . The graph
Laplacian is deﬁned as L = I − D−1/2AD−1/2  where D ∈ Rn×n is a diagonal matrix with
i=0 ∈ Rn 
known as the graph Fourier basis  and non-negative eigenvalues {λi}n−1
i=0   known as the graph
frequencies [5]. L is diagonalizable by the eigendecomposition such that L = U ΛU H  where
Λ = diag ([λ0  . . .   λn−1]) ∈ Rn×n and U H is a hermitian transpose of U. We use λmin and λmax
to denote the smallest and largest eigenvalues of L  respectively.
Given a graph signal x  the graph Fourier transform of x is ˆx = U H x ∈ Rn and its inverse is
x = U ˆx [27  30]. The graph Fourier transform enables us to apply graph ﬁlters in the vertex domain.
A graph ﬁlter h can ﬁlter x by altering (amplifying or attenuating) the graph frequencies as

j Aij and I is an identity matrix. L has a set of orthogonal eigenvectors {ui}n−1

h(L)x = h(U ΛU H )x = U h(Λ)U H x = U h(Λ)ˆx.

(1)
Here  h(Λ) = diag([h(λ0)  . . .   h(λn−1)])  which controls how the frequency of each component
in a graph signal x is modiﬁed. However  applying graph ﬁltering as in Eq. 1 requires the eigen-
decomposition of L  which is computationally expensive. To address this issue  several works
[2  7  12  19  21  23] have studied the approximation of h(Λ) by a polynomial or rational polynomial
function.

Chebyshev ﬁlters. Hammond et al. [12] ﬁrst proposed to approximate h(λ) by a polynomial
function with kth-order polynomials and Chebyshev coefﬁcients. Later  Defferrard et al.
[7]
developed Chebyshev ﬁlters for spectral CNNs on graphs. A Chebyshev ﬁlter is deﬁned as

k−1(cid:88)

k−1(cid:88)

hθ(˜λ) =

θjTj(˜λ) 

(2)

where θ ∈ Rk is a vector of learnable Chebyshev coefﬁcients  ˜λ ∈ [−1  1] is rescaled from λ  the
Chebyshev polynomials Tj(λ) = 2λTj−1(λ) − Tj−2(λ) are recursively deﬁned with T0(λ) = 1 and
T1(λ) = λ  and k controls the size of ﬁlters  i.e.  localized in k-hop neighborhood of a vertex [12].
Kipf and Welling [19] simpliﬁed Chebyshev ﬁlters by restricting to 1-hop neighborhood.

j=0

Lanczos ﬁlters. Recently  Liao et al. [23] used the Lanczos algorithm to generate a low-rank matrix
approximation T for the graph Laplacian. They used the afﬁnity matrix S = D−1/2AD−1/2. Since
L = I − S holds  L and S share the same eigenvectors but have different eigenvalues. As a result 
L and S correspond to the same ˆx. To approximate the eigenvectors and eigenvalues of S  they
diagonalize the tri-diagonal matrix T ∈ Rm×m to compute Ritz-vectors V ∈ Rn×m and Ritz-values
R ∈ Rm×m  and thus S ≈ V RV T . Accordingly  a k-hop Lanczos ﬁlter operation is 

hθ(R) =

θjRj 

(3)

j=0

where θ ∈ Rk is a vector of learnable Lanczos ﬁlter coefﬁcients. Thus  spectral convolutional
operation is deﬁned as hθ(S)x ≈ V hθ(R)V T x. Such Lanczos ﬁlter operations can signiﬁcantly
reduce computation overhead when approximating large powers of S  i.e. Sk ≈ V RkV T . Thus  they
can efﬁciently compute the spectral graph convolution with a very large localization range to easily
capture the multi-scale information of the graph.

3

k−1(cid:88)

Cayley ﬁlters. Observing that Chebyshev ﬁlters have difﬁculty in detecting narrow frequency bands
due to ˜λ ∈ [−1  1]  Levie et al. [21] proposed Cayley ﬁlters  based on Cayley polynomials:

θj(sλ − i)j(sλ + i)−j) 

j=1

hθ s(λ) = θ0 + 2Re(

(4)
where θ0 ∈ R is a real coefﬁcient and (θ1  . . .   θk−1) ∈ Ck−1 is a vector of complex coefﬁcients.
Re(x) denotes the real part of a complex number x  and s > 0 is a parameter called spectral
zoom  which controls the degree of “zooming” into eigenvalues in Λ. Both θ and s are learnable
during training. To improve efﬁciency  the Jacobi method is used to approximately compute Cayley
polynomials.
ARMA1 ﬁlters. Bianchi et al. [2] sought to address similar issues as identiﬁed in [21]. However 
different from Cayley ﬁlters  they developed a ﬁrst-order ARMA ﬁlter  which is approximated by a
ﬁrst-order recursion:
(5)
where a and b are the ﬁlter coefﬁcients  ¯x(0) = x  and ˜L = (λmax − λmin)/2I − L. Accordingly 
the frequency response is deﬁned as:

¯x(t+1) = a ˜L¯x(t) + bx 

r

 

h(˜λ) =

˜λ − p

(6)
where ˜λ = (λmax − λmin)/2λ  r = −b/a  and p = 1/a [17]. Multiple ARMA1 ﬁlters can be
applied in parallel to obtain a ARMAk ﬁlter. However  the memory complexity of k parallel ARMA1
ﬁlters is k times higher than ARMA1 graph ﬁlters.
We make some remarks on how these existing spectral ﬁlters are related to each other. (i) As discussed
in [2  21  23]  polynomial ﬁlters (e.g. Chebyshev and Lanczos ﬁlters) can be approximately treated
as a special kind of rational polynomial ﬁlters. (ii) Further  Chebyshev ﬁlters can be regarded as a
special case of Lanczos ﬁlters. (iii) Although both Cayley and ARMAk ﬁlters are rational polynomial
ﬁlters  they differ in how they approximate the matrix inverse implied by the denominator of a rational
function. Cayley ﬁlters use a ﬁxed number of Jacobi iterations  while ARMAk ﬁlters use a ﬁrst-order
recursion plus a parallel bank of k ARMA1. (iv) ARMA1 by Bianchi et al. [2] is similar to GCN by
Kipf et al. [19] because they both consider localization within 1-hop neighborhood.

3 Proposed Method
We introduce a new class of spectral graph ﬁlters  called feedback-looped ﬁlters  and propose a
spectral CNN for graphs with feedback-looped ﬁlters  namely Distributed Feedback-Looped Networks
(DFNets). We also discuss optimization techniques and analyze theoretical properties.

3.1 Feedback-Looped Filters

Feedback-looped ﬁlters belong to a class of Auto Regressive Moving Average (ARMA) ﬁlters [16  17].
Formally  an ARMA ﬁlter is deﬁned as:

(cid:16)

p(cid:88)

ψjLj(cid:17)−1(cid:16) q(cid:88)

φjLj(cid:17)

I +

hψ φ(L)x =

(7)
The parameters p and q refer to the feedback and feedforward degrees  respectively. ψ ∈ Cp and
φ ∈ Cq+1 are two vectors of complex coefﬁcients. Computing the denominator of Eq. 7 however
requires a matrix inversion  which is computationally inefﬁcient for large graphs. To circumvent this
issue  feedback-looped ﬁlters use the following approximation:
ψj ˜Lj ¯x(t−1) +

φj ˜Ljx 

(8)

j=0

j=1

x.

q(cid:88)

¯x(0) = x and ¯x(t) = − p(cid:88)
)I  ˆL = I − ˆD−1/2 ˆA ˆD−1/2  ˆA = A + I  ˆDii =(cid:80)
(cid:80)q
1 +(cid:80)p

j=0 φjλj

h(λi) =

j=1

j=0

.

i

j=1 ψjλj

i

4

where ˜L = ˆL − (
ˆAij and ˆλmax is the
largest eigenvalue of ˆL. Accordingly  the frequency response of feedback-looped ﬁlters is deﬁned as:

ˆλmax

2

j

(9)

2

ˆλmax

To alleviate the issues of gradient vanishing/exploding and numerical instabilities  we further introduce
two techniques in the design of feedback-looped ﬁlters: scaled-normalization and cut-off frequency.
Scaled-normalization technique. To assure the stability of feedback-looped ﬁlters  we apply the
scaled-normalization technique to increasing the stability region  i.e.  using the scaled-normalized
Laplacian ˜L = ˆL − (
)I  rather than just ˆL. This accordingly helps centralize the eigenvalues of
the Laplacian ˆL and reduce its spectral radius bound. The scaled-normalized Laplacian ˜L consists of
graph frequencies within [0  2]  in which eigenvalues are ordered in an increasing order.
Cut-off frequency technique.
To map graph frequencies within [0  2] to a uniform discrete
2 − η)  where η ∈ [0  1] and λmax refers to
distribution  we deﬁne a cut-off frequency λcut = ( λmax
the largest eigenvalue of ˜L. The cut-off frequency is used as a threshold to control the amount of
i=0 are converted to binary values {˜λi}n−1
attenuation on graph frequencies. The eigenvalues {λi}n−1
such that ˜λi = 1 if λi ≥ λcut and ˜λi = 0 otherwise. This trick allows the generation of ideal
high-pass ﬁlters so as to sharpen a signal by amplifying its graph Fourier coefﬁcients. This technique
also solves the issue of narrow frequency bands existing in previous spectral ﬁlters  including both
polynomial and rational polynomial ﬁlters [7  21]. This is because these previous spectral ﬁlters
only accept a small band of frequencies. In contrast  our proposed feedback-looped ﬁlters resolve
this issue using a cut-off frequency technique  i.e.  amplifying frequencies higher than a certain low
cut-off value while attenuating frequencies lower than that cut-off value. Thus  our proposed ﬁlters
can accept a wider range of frequencies and capture better characteristic properties of a graph.

i=0

3.2 Coefﬁcient Optimisation
i=0 → R  we aim to ﬁnd
Given a feedback-looped ﬁlter with a desired frequency response: ˆh : {˜λi}n−1
the optimal coefﬁcients ψ and φ that make the frequency response as close as possible to the desired
frequency response  i.e. to minimize the following error:

(cid:80)q
1 +(cid:80)p
p(cid:88)

j=0 φj

˜λj
i
j=1 ψj

˜λj
i

i − q(cid:88)

´e(˜λi) = ˆh(˜λi) −

(10)

However  the above equation is not linear w.r.t. the coefﬁcients ψ and φ. Thus  we redeﬁne the error
as follows:

e(˜λi) = ˆh(˜λi) + ˆh(˜λi)

ψj

˜λj

φj

˜λj
i .

(11)

Let e = [e(˜λ0)  . . .   e(˜λn−1)]T   ˆh = [ˆh(˜λ0)  . . .   ˆh(˜λn−1)]T   α ∈ Rn×p with αij = ˜λj
i and
β ∈ Rn×(q+1) with βij = ˜λj−1
are two Vandermonde-like matrices. Then  we have e = ˆh +
diag(ˆh)αψ − βφ. Thus  the stable coefﬁcients ψ and φ can be learned by minimizing e as a convex
constrained least-squares optimization problem:

i

j=1

j=0

minimizeψ φ ||ˆh + diag(ˆh)αψ − βφ||2
subject to ||αψ||∞ ≤ γ and γ < 1

(12)

Here  the parameter γ controls the tradeoff between convergence efﬁciency and approximation accu-
racy. A higher value of γ can lead to slower convergence but better accuracy. It is not recommended
to have very low γ values due to potentially unacceptable accuracy. ||αψ||∞ ≤ γ < 1 is the stability
condition  which will be further discussed in detail in Section 3.4.

3.3 Spectral Convolutional Layer

P = −(cid:80)p

We propose a CNN-based architecture  called DFNets  which can stack multiple spectral con-
volutional layers with feedback-looped ﬁlters to extract features of increasing abstraction. Let
j=0 φj ˜Lj. The propagation rule of a spectral convolutional layer is

j=1 ψj ˜Lj and Q =(cid:80)q

deﬁned as:

¯X (t+1) = σ(P ¯X (t)θ(t)

1 + QXθ(t)

2 + µ(θ(t)

1 ; θ(t)

2 ) + b) 

(13)

5

where σ refers to a non-linear activation function such as ReLU. ¯X (0) = X ∈ Rn×f is a graph
signal matrix where f refers to the number of features. ¯X (t) is a matrix of activations in the tth layer.
1 ∈ Rc×h and θ(t)
2 ∈ Rf×h are two trainable weight matrices in the tth layer. To compute ¯X (t+1) 
θ(t)
a vertex needs access to its p-hop neighbors with the output signal of the previous layer ¯X (t)  and its
q-hop neighbors with the input signal from X. To attenuate the overﬁtting issue  we add µ(θ(t)
1 ; θ(t)
2 ) 
namely kernel regularization [6]  and a bias term b. We use the xavier normal initialization method
[10] to initialise the kernel and bias weights  the unit-norm constraint technique [8] to normalise
the kernel and bias weights by restricting parameters of all layers in a small range  and the kernel
regularization technique to penalize the parameters in each layer during the training. In doing so  we
can prevent the generation of spurious features and thus improve the accuracy of prediction 1.
In this model  each layer is directly connected to all subsequent layers in a feed-forward manner  as
in DenseNet [15]. Consequently  the tth layer receives all preceding feature maps F0  . . .   Ft−1 as
input. We concatenate multiple preceding feature maps column-wise into a single tensor to obtain
more diversiﬁed features for boosting the accuracy. This densely connected CNN architecture has
several compelling beneﬁts: (a) reduce the vanishing-gradient issue  (b) increase feature propagation
and reuse  and (c) reﬁne information ﬂow between layers [15].

3.4 Theoretical Analysis

to guarantee the stability  we can derive the stability condition || −(cid:80)p

Feedback-looped ﬁlters have several nice properties  e.g.  guaranteed convergence  linear convergence
time  and universal design. We discuss these properties and analyze computational complexities.
Convergence. Theoretically  a feedback-looped ﬁlter can achieve a desired frequency response only
when t → ∞ [17]. However  due to the property of linear convergence preserved by feedback-looped
ﬁlters  stability can be guaranteed after a number of iterations w.r.t. a speciﬁed small error [16]. More
speciﬁcally  since the pole of rational polynomial ﬁlters should be in the unit circle of the z-plane
j=1 ψjLj|| < 1 by Eq. 7 in
the vertex domain and correspondingly obtain the stability condition ||αψ||∞ ≤ γ ∈ (0  1) in the
frequency domain as stipulated in Eq. 12 [16].
Universal design. The universal design is beneﬁcial when the underlying structure of a graph is
unknown or the topology of a graph changes over time. The corresponding ﬁlter coefﬁcients can
be learned independently of the underlying graph and are universally applicable. When designing
feedback-looped ﬁlters  we deﬁne the desired frequency response function ˆh over graph frequencies
˜λi in a binary format in the uniform discrete distribution as discussed in Section 3.1. Then  we
solve Eq. 12 in the least-squares sense for this ﬁnite set of graph frequencies to ﬁnd optimal ﬁlter
coefﬁcients.

Spectral Graph Filter
Chebyshev ﬁlters [7]
Lanczos ﬁlters [23]
Cayley ﬁlters [21]
ARMA1 ﬁlters [2]
d parallel ARMA1 ﬁlters [2]
Feedback-looped ﬁlters (ours)

Type

Polynomial

Rational
polynomial

Learning
Complexity

O(k)
O(k)

Time

Complexity

O(km)
O(km2)

O((r + 1)k) O((r + 1)km)

O(t)
O(t)

O(tm)
O(tm)

Memory

Complexity

O(m)
O(m2)
O(m)
O(m)
O(dm)
O(m)

Table 1: Learning  time and space complexities of spectral graph ﬁlters.

O(tp + q)

O((tp + q)m)

Complexity. When computing ¯x(t) as in Eq. 8  we need to calculate ˜Lj ¯x(t−1) for j = 1  . . .   p and
˜Ljx for j = 1  . . .   q. Nevertheless  ˜Ljx is computed only once because ˜Ljx = ˜L( ˜Lj−1x). Thus 
we need p multiplications for each t in the ﬁrst term in Eq. 8  and q multiplications for the second
term in Eq. 8. Table 1 summarizes the complexity results of existing spectral graph ﬁlters and ours 
where r refers to the number of Jacobi iterations in [21]. Note that  when t = 1 (i.e.  one spectral
convolutional layer)  feedback-looped ﬁlters have the same learning  time and memory complexities
as Chebyshev ﬁlters  where p + q = k.

1DFNets implementation can be found at: https://github.com/wokas36/DFNets

6

4 Numerical Experiments

We evaluate our models on two benchmark tasks: (1) semi-supervised document classiﬁcation in
citation networks  and (2) semi-supervised entity classiﬁcation in a knowledge graph.

4.1 Experimental Set-Up

Datasets. We use three citation network datasets Cora  Citeseer  and Pubmed [28] for semi-supervised
document classiﬁcation  and one dataset NELL [4] for semi-supervised entity classiﬁcation. NELL is
a bipartite graph extracted from a knowledge graph [4]. Table 2 contains dataset statistics [33].

Type
Dataset
Cora
Citation network
Citeseer Citation network
Pubmed Citation network
NELL
Knowledge graph

#Nodes
2 708
3 327
19 717
65 755
Table 2: Dataset statistics.

#Edges
5 429
4 732
44 338
266 144

#Classes
7
6
3
210

#Features %Labeled Nodes
0.052
0.036
0.003
0.001

1 433
3 703
500
5 414

Baseline methods. We compare against twelve baseline methods  including ﬁve methods using
spatial graph ﬁlters  i.e.  Semi-supervised Embedding (SemiEmb) [32]  Label Propagation (LP) [34] 
skip-gram graph embedding model (DeepWalk) [26]  Iterative Classiﬁcation Algorithm (ICA) [24] 
and semi-supervised learning with graph embedding (Planetoid*) [33]  and seven methods using
spectral graph ﬁlters: Chebyshev [7]  Graph Convolutional Networks (GCN) [19]  Lanczos Networks
(LNet) and Adaptive Lanczos Networks (AdaLNet) [23]  CayleyNet [21]  Graph Attention Networks
(GAT) [31]  and ARMA Convolutional Networks (ARMA1) [2].
We evaluate our feedback-looped ﬁlters using three different spectral CNN models: (i) DFNet: a
densely connected spectral CNN with feedback-looped ﬁlters  (ii) DFNet-ATT: a self-attention based
densely connected spectral CNN with feedback-looped ﬁlters  and (iii) DF-ATT: a self-attention
based spectral CNN model with feedback-looped ﬁlters.

Model
DFNet
DFNet-ATT
DF-ATT

L2 reg.
9e-2
9e-4
9e-3

#Layers

5
4
2

#Units
[8  16  32  64  128]
[8  16  32  64]
[32  64]

Dropout

0.9
0.9

[0.1  0.9]

[p  q]
[5  3]
[5  3]
[5  3]

λcut
0.5
0.5
0.5

Table 3: Hyperparameter settings for citation network datasets.

Hyperparameter settings. We use the same data splitting for each dataset as in Yang et al. [33]. The
hyperparameters of our models are initially selected by applying the orthogonalization technique (a
randomized search strategy). We also use a layerwise regularization (L2 regularization) and bias terms
to attenuate the overﬁtting issue. All models are trained 200 epochs using the Adam optimizer [18]
with a learning rate of 0.002. Table 3 summarizes the hyperparameter settings for citation network
datasets. The same hyperparameters are applied to the NELL dataset except for L2 regularization (i.e. 
9e-2 for DFNet and DFnet-ATT  and 9e-4 for DF-ATT). For γ  we choose the best setting for each
model. For self-attention  we use 8 multi-attention heads and 0.5 attention dropout for DFNet-ATT 
and 6 multi-attention heads and 0.3 attention dropout for DF-ATT. The parameters p = 5  q = 3 and
λcut = 0.5 are applied to all three models over all datasets.

4.2 Comparison with Baseline Methods

Table 4 summarizes the results of classiﬁcation in terms of accuracy. The results of the baseline
methods are taken from the previous works [19  23  31  33]. Our models DFNet and DFNet-ATT
outperform all the baseline methods over four datasets. Particularly  we can see that: (1) Compared
with polynomial ﬁlters  DFNet improves upon GCN (which performs best among the models using
polynomial ﬁlters) by a margin of 3.7%  3.9%  5.3% and 2.3% on the datasets Cora  Citeseer  Pubmed
and NELL  respectively. (2) Compared with rational polynomial ﬁlters  DFNet improves upon
CayleyNet and ARMA1 by 3.3% and 1.8% on the Cora dataset  respectively. For the other datasets 
CayleyNet does not have results available in [21]. (3) DFNet-ATT further improves the results of
DFNet due to the addition of a self-attention layer. (4) Compared with GAT (Chebyshev ﬁlters with

7

self-attention)  DF-ATT also improves the results and achieves 0.4%  0.6% and 3.3% higher accuracy
on the datasets Cora  Citeseer and Pubmed  respectively.
Additionally  we compare DFNet (our feedback-looped ﬁlters + DenseBlock) with GCN + Dense-
Block and GAT + DenseBlock. The results are also presented in Table 4. We can see that our
feedback-looped ﬁlters perform best  no matter whether or not the dense architecture is used.

Model
SemiEmb [32]
LP [34]
DeepWalk [26]
ICA [24]
Planetoid* [33]
Chebyshev [7]
GCN [19]
LNet [23]
AdaLNet [23]
CayleyNet [21]
ARMA1 [2]
GAT [31]
GCN + DenseBlock
GAT + Dense Block
DFNet (ours)
DFNet-ATT (ours)
DF-ATT (ours)

Cora
59.0
68.0
67.2
75.1
64.7
81.2
81.5
79.5
80.4
81.9∗
83.4
83.0

82.7 ± 0.5
83.8 ± 0.3
85.2 ± 0.5
86.0 ± 0.4
83.4 ± 0.5

Citeseer

Pubmed

59.6
45.3
43.2
69.1
75.7
69.8
70.3
66.2
68.7

-

72.5
72.5

71.3 ± 0.3
73.1 ± 0.3
74.2 ± 0.3
74.7 ± 0.4
73.1 ± 0.4

71.1
63.0
65.3
73.9
77.2
74.4
79.0
78.3
78.1

-

78.9
79.0

81.5 ± 0.5
81.8 ± 0.3
84.3 ± 0.4
85.2 ± 0.3
82.3 ± 0.3

NELL
26.7
26.5
58.1
23.1
61.9

66.0

-

-
-
-
-
-

-

66.4 ± 0.3
68.3 ± 0.4
68.8 ± 0.3
67.6 ± 0.3

Table 4: Accuracy (%) averaged over 10 runs (* was obtained using a different data splitting in [21])

4.3 Comparison under Different Polynomial Orders

.

In order to test how the polynomial orders p and q inﬂuence the performance of our model DFNet  we
conduct experiments to evaluate DFNet on three citation network datasets using different polynomial
orders p = [1  3  5  7  9] and q = [1  3  5  7  9]. Figure 2 presents the experimental results. In our
experiments  p = 5 and q = 3 turn out to be the best parameters for DFNet over these datasets. In
other words  this means that feedback-looped ﬁlters are more stable on p = 5 and q = 3 than other
values of p and q. This is because  when p = 5 and q = 3  Eq. 12 can obtain better convergence for
ﬁnding optimal coefﬁcients than in the other cases. Furthermore  we observe that: (1) Setting p to be
too low or too high can both lead to poor performance  as shown in Figure 2.(a)  and (2) when q is
larger than p  the accuracy decreases rapidly as shown in Figure 2.(b). Thus  when choosing p and q 
we require that p > q holds.

Figure 2: Accuracy (%) of DFNet under different polynomial orders p and q.

4.4 Evaluation of Scaled-Normalization and Cut-off Frequency

To understand how effectively the scaled-normalisation and cut-off frequency techniques can help
learn graph representations  we compare our methods that implement these techniques with the
variants of our methods that only implement one of these techniques. The results are presented in
Figure 3. We can see that  the models using these two techniques outperform the models that only use
one of these techniques over all citation network datasets. Particularly  the improvement is signiﬁcant
on the Cora and Citeseer datasets.

8

13579p405060708090Accuracy(a) q=313579q405060708090(b) p=5CoraCiteseerPubmedFigure 3: Accuracy (%) of our models in three cases: (1) using both scaled-normalization and cut-off
frequency  (2) using only cut-off frequency  and (3) using only scaled-normalization.

4.5 Node Embeddings

We analyze the node embeddings by DFNets over two datasets: Cora and Pubmed in a 2-D space.
Figures 4 and 5 display the visualization of the learned 2-D embeddings of GCN  GAT  and DFNet
(ours) on Pubmed and Cora citation networks by applying t-SNE [25] respectively. Colors denote
different classes in these datasets. It reveals the clustering quality of theses models. These ﬁgures
clearly show that our model DFNet has better separated 3 and 7 clusters respectively in the embedding
spaces of Pubmed and Cora datasets. This is because features extracted by DFNet yield better node
representations than GCN and GAT models.

(a) GCN

(b) GAT

(c) DFNet (ours)

Figure 4: The t-SNE visualization of the 2-D node embedding space for the Pubmed dataset.

(a) GCN

(b) GAT

(c) DFNet (ours)

Figure 5: The t-SNE visualization of the 2-D node embedding space for the Cora dataset.

5 Conclusions
In this paper  we have introduced a spectral CNN architecture (DFNets) with feedback-looped ﬁlters
on graphs. To improve approximation accuracy  we have developed two techniques: scaled nor-
malization and cut-off frequency. In addition to these  we have discussed some nice properties of
feedback-looped ﬁlters  such as guaranteed convergence  linear convergence time  and universal de-
sign. Our proposed model outperforms the state-of-the-art approaches signiﬁcantly in two benchmark
tasks. In future  we plan to extend the current work to time-varying graph structures. As discussed in
[17]  feedback-looped graph ﬁlters are practically appealing for time-varying settings  and similar to
static graphs  some nice properties would likely hold for graphs that are a function of time.

9

CoraCiteseerPubmedDatasets020406080100Accuracy(a) DFNetCoraCiteseerPubmedDatasets020406080100Accuracy(b) DFNet-ATTCoraCiteseerPubmedDatasets020406080100Accuracy(c) DF-ATTCase (1)Case (2)Case (3)References
[1] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural

Information Processing Systems (NeurIPS)  pages 1993–2001  2016.

[2] F. M. Bianchi  D. Grattarola  L. Livi  and C. Alippi. Graph neural networks with convolutional

ARMA ﬁlters. arXiv preprint arXiv:1901.01343  2019.

[3] J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral networks and locally connected

networks on graphs. International Conference on Learning Representations (ICLR)  2013.

[4] A. Carlson  J. Betteridge  B. Kisiel  B. Settles  E. R. Hruschka  and T. M. Mitchell. Toward
an architecture for never-ending language learning. In Twenty-Fourth AAAI Conference on
Artiﬁcial Intelligence (AAAI)  2010.

[5] F. R. Chung and F. C. Graham. Spectral graph theory. Number 92. American Mathematical

Soc.  1997.

[6] C. Cortes  M. Mohri  and A. Rostamizadeh. L2 regularization for learning kernels. In Pro-
ceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages
109–116  2009.

[7] M. Defferrard  X. Bresson  and P. Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral ﬁltering. In Advances in neural information processing systems
(NeurIPS)  pages 3844–3852  2016.

[8] S. C. Douglas  S.-i. Amari  and S.-Y. Kung. On gradient adaptation with unit-norm constraints.

IEEE Transactions on Signal processing  48(6):1843–1847  2000.

[9] D. K. Duvenaud  D. Maclaurin  J. Iparraguirre  R. Bombarell  T. Hirzel  A. Aspuru-Guzik 
and R. P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In
Advances in neural information processing systems (NeurIPS)  pages 2224–2232  2015.

[10] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence
and statistics (AIStats)  pages 249–256  2010.

[11] W. Hamilton  Z. Ying  and J. Leskovec. Inductive representation learning on large graphs. In

Advances in Neural Information Processing Systems (NeurIPS)  pages 1024–1034  2017.

[12] D. K. Hammond  P. Vandergheynst  and R. Gribonval. Wavelets on graphs via spectral graph

theory. Applied and Computational Harmonic Analysis  30(2):129–150  2011.

[13] M. Henaff  J. Bruna  and Y. LeCun. Deep convolutional networks on graph-structured data.

arXiv preprint arXiv:1506.05163  2015.

[14] W. Hu  Y. Huang  L. Wei  F. Zhang  and H. Li. Deep convolutional neural networks for

hyperspectral image classiﬁcation. Journal of Sensors  2015  2015.

[15] G. Huang  Z. Liu  L. Van Der Maaten  and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR)  pages 4700–4708  2017.

[16] E. Isuﬁ  A. Loukas  and G. Leus. Autoregressive moving average graph ﬁlters: a stable
distributed implementation. In 2017 IEEE International Conference on Acoustics  Speech and
Signal Processing (ICASSP)  pages 4119–4123  2017.

[17] E. Isuﬁ  A. Loukas  A. Simonetto  and G. Leus. Autoregressive moving average graph ﬁltering.

IEEE Transactions on Signal Processing  65(2):274–288  2017.

[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

Conference on Learning Representations (ICLR)  2015.

In International

[19] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR)  2017.

10

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems (NeurIPS)  pages
1097–1105  2012.

[21] R. Levie  F. Monti  X. Bresson  and M. M. Bronstein. Cayleynets: Graph convolutional neural
IEEE Transactions on Signal Processing 

networks with complex rational spectral ﬁlters.
67(1):97–109  2017.

[22] Y. Li  H. Qi  J. Dai  X. Ji  and Y. Wei. Fully convolutional instance-aware semantic segmentation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 2359–2367  2017.

[23] R. Liao  Z. Zhao  R. Urtasun  and R. S. Zemel. LanczosNet: Multi-scale deep graph con-
volutional networks. In Proceedings of the seventh International Conference on Learning
Representation (ICLR)  2019.

[24] Q. Lu and L. Getoor. Link-based classiﬁcation. In Proceedings of the 20th International

Conference on Machine Learning (ICML)  pages 496–503  2003.

[25] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[26] B. Perozzi  R. Al-Rfou  and S. Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining (SIGKDD)  pages 701–710  2014.

[27] A. Sandryhaila and J. M. Moura. Discrete signal processing on graphs: Graph fourier transform.
In 2013 IEEE International Conference on Acoustics  Speech and Signal Processing (ICASSP) 
pages 6167–6170  2013.

[28] P. Sen  G. Namata  M. Bilgic  L. Getoor  B. Galligher  and T. Eliassi-Rad. Collective classiﬁca-

tion in network data. AI magazine  29(3):93–93  2008.

[29] A. Sharif Razavian  H. Azizpour  J. Sullivan  and S. Carlsson. Cnn features off-the-shelf: an
astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision
and pattern recognition workshops  pages 806–813  2014.

[30] D. Shuman  S. Narang  P. Frossard  A. Ortega  and P. Vandergheynst. The emerging ﬁeld of
signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains. IEEE Signal Processing Magazine  30:83–98  2013.

[31] P. Veliˇckovi´c  G. Cucurull  A. Casanova  A. Romero  P. Lio  and Y. Bengio. Graph attention

networks. International Conference on Learning Representations (ICLR)  2017.

[32] J. Weston  F. Ratle  H. Mobahi  and R. Collobert. Deep learning via semi-supervised embedding.

In Neural Networks: Tricks of the Trade  pages 639–655. Springer  2012.

[33] Z. Yang  W. W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with graph
In Proceedings of The 33rd International Conference on Machine Learning

embeddings.
(ICML)  pages 40–48  2016.

[34] X. Zhu  Z. Ghahramani  and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds and
harmonic functions. In Proceedings of the 20th International conference on Machine learning
(ICML)  pages 912–919  2003.

11

Appendices

In the following  we provide further experiments on comparing our work with the others.

Comparison with different spectral graph ﬁlters. We have conducted an ablation study of our
proposed graph ﬁlters. Speciﬁcally  we compare our feedback-looped ﬁlters  i.e.  the newly proposed
spectral ﬁlters in this paper  against other spectral ﬁlters such as Chebyshev ﬁlters and Cayley
ﬁlters. To conduct this ablation study  we remove the dense connections from our model DFNet.
The experimental results are presented in table 5. It shows that feedback-looped ﬁlters improve
localization upon Chebyshev ﬁlters by a margin of 1.4%  1.7% and 7.3% on the datasets Cora 
Citeseer and Pubmed  respectively. It also improves upon Cayley ﬁlters by a margin of 0.7% on the
Cora dataset.

Model
Chebyshev ﬁlters [7]
Cayley ﬁlters [21]
Feedback-looped ﬁlters (ours)

Cora
81.2
81.9

71.5 ± 0.4
Table 5: Accuracy (%) averaged over 10 runs.

82.6 ± 0.3

Citeseer

Pubmed

69.8

-

74.4

-

81.7 ± 0.6

Comparison with LNet and AdaLNet using different data splittings. We have benchmarked the
performance of our DFNet model against the models LNet and AdaLNet proposed in [23]  as well as
Chebyshev  GCN and GAT  over three citation network datasets Cora  Citeseer and Pubmed. We use
the same data splittings as used in [23]. All the experiments are repeated 10 times. For our model
DFNet  we use the same hyperparameter settings as discussed in Section 4.2.

Training Split
5.2% (standard)
3%
1%
0.5%

Chebyshev
78.0 ± 1.2
62.1 ± 6.7
44.2 ± 5.6
33.9 ± 5.0

GCN

80.5 ± 0.8
74.0 ± 2.8
61.0 ± 7.2
52.9 ± 7.4

GAT

82.6 ± 0.7
56.8 ± 7.9
48.6 ± 8.0
41.4 ± 6.9

LNet

79.5 ± 1.8
76.3 ± 2.3
66.1 ± 8.2
58.1 ± 8.2

AdaLNet
80.4 ± 1.1
77.7 ± 2.4
67.5 ± 8.7
60.8 ± 9.0

DFNet
85.2 ± 0.5
80.5 ± 0.4
69.5 ± 2.3
61.3 ± 4.3

Table 6: Accuracy (%) averaged over 10 runs on the Cora dataset.

Training Split
3.6% (standard)
1%
0.5%
0.3%

GAT

GCN

Chebyshev
70.1 ± 0.8
59.4 ± 5.4
45.3 ± 6.6
39.3 ± 4.9

AdaLNet
68.7 ± 1.0
63.3 ± 1.8
53.8 ± 4.7
46.7 ± 5.6
Table 7: Accuracy (%) averaged over 10 runs on the Citeseer dataset.

66.2 ± 1.9
61.3 ± 3.9
53.2 ± 4.0
44.4 ± 4.5

72.2 ± 0.9
46.5 ± 9.3
38.2 ± 7.1
30.9 ± 6.9

68.1 ± 1.3
58.3 ± 4.0
47.7 ± 4.4
39.2 ± 6.3

LNet

Training Split
0.3% (standard)
0.1%
0.05%
0.03%

GAT

GCN

Chebyshev
69.8 ± 1.1
55.2 ± 6.8
48.2 ± 7.4
45.3 ± 4.5

AdaLNet
78.1 ± 0.4
72.8 ± 4.6
66.0 ± 4.5
61.0 ± 8.7
Table 8: Accuracy (%) averaged over 10 runs on the Pubmed dataset.

78.3 ± 0.3
73.4 ± 5.1
68.8 ± 5.6
60.4 ± 8.6

76.7 ± 0.5
59.6 ± 9.5
50.4 ± 9.7
50.9 ± 8.8

77.8 ± 0.7
73.0 ± 5.5
64.6 ± 7.5
57.9 ± 8.1

LNet

DFNet
74.2 ± 0.3
67.4 ± 2.3
55.1 ± 3.2
48.3 ± 3.5

DFNet
84.3 ± 0.4
75.2 ± 3.6
67.2 ± 7.3
59.3 ± 6.6

Tables 6-8 present the experimental results. Table 6 shows that DFNet performs signiﬁcantly better
than all the other models over the Cora dataset  including LNet and AdaLNet proposed in [23].
Similarly  Table 7 shows that DFNet performs signiﬁcantly better than all the other models over the
Citeseer dataset. For the Pubmed dataset  as shown in Table 8  DFNet performs signiﬁcantly better
than almost all the other models  except for only one case in which DFNet performs slightly worse
than AdaLNet using the splitting 0.03%. These results demonstrate the robustness of our model
DFNet.

12

,David Lopez-Paz
Marc'Aurelio Ranzato
W. O. K. Asiri Suranga Wijesinghe
Qing Wang