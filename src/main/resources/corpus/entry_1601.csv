2014,Divide-and-Conquer Learning by Anchoring a Conical Hull,We reduce a broad class of machine learning problems  usually addressed by EM or sampling  to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ ``anchors'' lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors  we propose a novel divide-and-conquer learning scheme ``DCA'' that distributes the problem to $\mathcal O(k\log k)$ same-type sub-problems on different low-D random hyperplanes  each can be solved by any solver. For the 2D sub-problem  we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull  which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM  HMM  LDA  NMF and subspace clustering  then show its competitive performance and scalability over other methods on rich datasets.,Divide-and-Conquer Learning by Anchoring a

Conical Hull

Tianyi Zhou†  Jeff Bilmes‡  Carlos Guestrin†

†Computer Science & Engineering  ‡Electrical Engineering  University of Washington  Seattle

{tianyizh  bilmes  guestrin}@u.washington.edu

Abstract

We reduce a broad class of fundamental machine learning problems  usually
addressed by EM or sampling  to the problem of ﬁnding the k extreme rays
spanning the conical hull of a1 data point set. These k “anchors” lead to a global
solution and a more interpretable model that can even outperform EM and sampling
on generalization error. To ﬁnd the k anchors  we propose a novel divide-and-
conquer learning scheme “DCA” that distributes the problem to O(k log k) same-
type sub-problems on different low-D random hyperplanes  each can be solved
independently by any existing solver. For the 2D sub-problem  we instead present
a non-iterative solver that only needs to compute an array of cosine values and its
max/min entries. DCA also provides a faster subroutine inside other algorithms
to check whether a point is covered in a conical hull  and thus improves these
algorithms by providing signiﬁcant speedups. We apply our method to GMM 
HMM  LDA  NMF and subspace clustering  then show its competitive performance
and scalability over other methods on large datasets.

1

Introduction

Expectation-maximization (EM) [10]  sampling methods [13]  and matrix factorization [20  25] are
three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP))
estimates of models with latent variables/factors  and thus are used in a wide range of applications
such as clustering  topic modeling  collaborative ﬁltering  structured prediction  feature engineering 
and time series analysis. However  their learning procedures rely on alternating optimization/updates
between parameters and latent variables  a process that suffers from local optima. Hence  their quality
greatly depends on initialization and on using a large number of iterations for proper convergence [24].
The method of moments [22  6  17]  by contrast  solves m equations by relating the ﬁrst m moments
of observation x ∈ Rp to the m model parameters  and thus yields a consistent estimator with a
global solution. In practice  however  sample moments usually suffer from unbearably large variance 
which easily leads to the failure of ﬁnal estimation  especially when m or p is large. Although recent
spectral methods [8  18  15  1] reduces m to 2 or 3 when estimating O(p) (cid:29) m parameters [2] by
relating the eigenspace of lower-order moments to parameters in a matrix form up to column scale 
the variance of sample moments is still sensitive to large p or data noise  which may result in poor
estimation. Moreover  although spectral methods using SVDs or tensor decomposition evidently
simpliﬁes learning  the computation can still be expensive for big data. In addition  recovering a
parameter matrix with uncertain column scale might not be feasible for some applications.
In this paper  we reduce the learning in a rich class of models (e.g.  matrix factorization and latent
variable model) to ﬁnding the extreme rays of a conical hull from a ﬁnite set of real data points.
This is obtained by applying a general separability assumption to either the data matrix in matrix
factorization or the 2nd/3rd order moments in latent variable models. Separability posits that a ground
set of n points  as rows of matrix X  can be represented by X = F XA  where the rows (bases) in
XA are a subset A ⊂ V = [n] of rows in X  which are called “anchors” and are interesting to various

1

Figure 1: Geometry of general minimum conical hull prob-
lem and basic idea of divide-and-conquer anchoring (DCA).

models when |A| = k (cid:28) n. This property was introduced in [11] to establish the uniqueness of
non-negative matrix factorization (NMF) under simplex constraints  and was later [19  14] extended to
non-negative constraints. We generalize it further to the model X = F YA for two (possibly distinct)
ﬁnite sets of points X and Y   and build a new theory for the identiﬁability of A. This generalization
enables us to apply it to more general models (ref. Table 1) besides NMF. More interestingly  it leads
to a learning method with much higher tolerance to the variance of sample moments or data noise  a
unique global solution  and a more interpretable model.
Another primary contribution of this paper is a dis-
tributed learning scheme “divide-and-conquer anchor-
ing” (DCA)  for ﬁnding an anchor set A such that
X = F YA by solving same-type sub-problems on only
O(k log k) randomly drawn low-dimensional (low-D)
hyperplanes. Each sub-problem is of the form of
(XΦ) = F · (Y Φ)A with random projection matrix
Φ  and can easily be handled by most solvers due to
the low dimension. This is based on the observation
that the geometry of the original conical hull is partially
preserved after a random projection. We analyze the
probability of success for each sub-problem to recover
part of A  and then study the number of sub-problems
for recovering the whole A with high probability (w.h.p.). In particular  we propose an very fast
non-iterative solver for sub-problems on the 2D plane  which requires computing an array of cosines
and its max/min values  and thus results in learning algorithms with speedups of tens to hundreds of
times. DCA improves multiple aspects of algorithm design since: 1) its idea of divide-and-conquer
randomization gives rise to distributed learning that can reduce the original problem to multiple
extremely low-D sub-problems that are much easier and faster to solve  and 2) it provides a fast
subroutine  checking if a point is covered by a conical hull  which can be embedded into other solvers.
We apply both the conical hull anchoring model and DCA to ﬁve learning models: Gaussian mixture
models (GMM) [27]  hidden Markov models (HMM) [5]  latent Dirichlet allocation (LDA) [7] 
NMF [20]  and subspace clustering (SC) [12]. The resulting models and algorithms show signiﬁcant
improvement in efﬁciency. On generalization performance  they consistently outperform spectral
methods and matrix factorization  and are comparable to or even better than EM and sampling.
In the following  we will ﬁrst generalize the separability assumption and minimum conical hull
problem risen from NMF in §2  and then show how to reduce more general learning models to a
(general) minimum conical hull problem in §3. §4 presents a divide-and-conquer learning scheme that
can quickly locate the anchors of the conical hull by solving the same problem in multiple extremely
low-D spaces. Comprehensive experiments and comparison can be found in §5.
2 General Separability Assumption and Minimum Conical Hull Problem
The original separability property [11] is deﬁned on the convex hull of a set of data points  namely
that each point can be represented as a convex combination of certain subsets of vertices that deﬁne
the convex hull. Later works on separable NMF [19  14] extend it to the conical hull case  which
replaced convex with conical combinations. Given the deﬁnition of (convex) cone and conical hull 
the separability assumption can be deﬁned both geometrically and algebraically.
Deﬁnition 1 (Cone & conical hull). A (convex) cone is a non-empty convex set that is closed with
respect to conical combinations of its elements. In particular  cone(R) can be deﬁned by its k
generators (or rays) R = {ri}k

(cid:27)

i=1 such that

(cid:26)(cid:88)k

cone(R) =

αiri | ri ∈ R  αi ∈ R+ ∀i

i=1

.

(1)

See [29] for the original separability assumption  the equivalence between separable NMF and the
minimum conical hull problem  which is deﬁned as a submodular set cover problem.
2.1 General Separability Assumption and General Minimum Conical Hull Problem

By generalizing the separability assumption  we obtain a general minimum conical hull problem
that can reduce more general learning models besides NMF  e.g.  latent variable models and matrix
factorization  to ﬁnding a set of “anchors” on the extreme rays of a conical hull.

2

OX = { } Y = { } YA = { } Cone(YA) cone(YÃ Φ) HXΦ = { } YΦ = { } YÃ Φ = { } Deﬁnition 2 (General separability assumption). All the n data points(rows) in X are covered in a
ﬁnitely generated and pointed cone (i.e.  if x ∈ cone(YA) then −x (cid:54)∈ cone(YA)) whose generators
form a subset A ⊆ [m] of data points in Y such that (cid:64)i (cid:54)= j  YAi = a · YAj . Geometrically  it says
(2)

∀i ∈ [n]  Xi ∈ cone (YA)   YA = {yi}i∈A.

+

An equivalent algebraic form is X = F YA  where |A| = k  F (cid:48) ∈ S ⊆ R(n−k)×k
When X = Y and S = R(n−k)×k
  it degenerates to the original separability assumption given
in [29]. We generalize the minimum conical hull problem from [29]. Under the general separability
assumption  it aims to ﬁnd the anchor set A from the points in Y rather than X.
Deﬁnition 3 (General Minimum Conical Hull Problem). Given a ﬁnite set of points X and a set
Y having an index set V = [m] of its rows  the general minimum conical hull problem ﬁnds the
subset of rows in Y that deﬁne a super-cone for all the rows in X. That is  ﬁnd A ∈ 2V that solves:
(3)

|A|  s.t.  cone(YA) ⊇ cone(X).

+

.

min
A⊂V

where cone(YA) is the cone induced by the rows A of Y .

When X = Y   this also degenerates to the original minimum conical hull problem deﬁned in [29].
A critical question is whether/when the solution A is unique. When X = Y and X = F XA  by
following the analysis of the separability assumption in [29] we can prove that A is unique and
identiﬁable given X. However  when X (cid:54)= Y and X = F YA  it is clear that there could be multiple
legal choices of A (e.g.  there could be multiple layers of conical hulls containing a conical hull
covering all points in X). Fortunately  when the rows of Y are rank-one matrices after vectorization
(concatenating all columns to a long vector)  which is the common case in most latent variable models
in §3.2  A can be uniquely determined if the number of rows in X exceeds 2.
Lemma 1 (Identiﬁability). If X = F YA with the additional structure Ys = vec(Os
is a pi × k matrix and Os
2  two (non-identical) rows in X are sufﬁcient to exactly recover the unique A  Oi and Oj.

j ) where Oi
i is its sth column  under the general separability assumption in Deﬁnition

i ⊗Os

See [29] for proof and additional uniqueness conditions when applied to latent variable models.

3 Minimum Conical Hull Problem for General Learning Models
Table 1: Summary of reducing NMF  SC  GMM  HMM and LDA to a conical hull anchoring model X = F YA in §3  and their learning
algorithms achieved by A = DCA(X  Y  k  M) in Algorithm 1 . Minimal conical hull A = MCH(X  Y ) is deﬁned in Deﬁnition 4.
vec(·) denotes the vectorization of a matrix. For GMM and HMM  Xi ∈ Rn×pi is the data matrix for view i (i.e.  a subset of features) and
the ith observation of all triples of sequential observations  respectively. Xt i is the tth row of Xi and associates with point/triple t. ηt is a
vector uniformly drawn from the unit sphere. More details are given in [29].

Model
NMF
SC
GMM
HMM
LDA

Algo
NMF
SC
GMM
HMM
LDA

+

1 Diag(X3ηt)X2]t∈[q]]/n
2 Diag(X1ηt)X3]t∈[q]]/n

X in conical hull problem
data matrix X ∈ Rn×p
data matrix X ∈ Rn×p
1 X2]; vec[XT
[vec[XT
[vec[XT
2 X3]; vec[XT
word-word co-occurrence matrix X ∈ Rp×p
Each sub-problem in DCA
˜A = MCH(XΦ  XΦ)  can be solved by (10)
˜A =anchors of clusters achieved by meanshift( (cid:92)(XΦ)ϕ)
˜A = MCH(XΦ  Y Φ)  can be solved by (10)
˜A = MCH(XΦ  Y Φ)  can be solved by (10)
˜A = MCH(XΦ  XΦ)  can be solved by (10)

+

Y in conical hull problem
Y := X

Y := X
[vec(Xt 1 ⊗ Xt 2)]t∈[n]
[vec(Xt 2 ⊗ Xt 3)]t∈[n]
Y := X

Post-processing after A :=(cid:83)

˜Ai

i

solving F in X = F XA
clustering anchors XA
N/A
solving T in OT = XA 3
col-normalize {F : X = F XA}

k in conical hull problem
# of factors
# of basis from all clusters
# of components/clusters
# of hidden states
# of topics

Interpretation of anchors indexed by A
basis XA are real data points
cluster i is a cone cone(XAi
centers [XA i]i∈[3] from real data
emission matrix O = XA 2
anchor word for topic i (topic prob. Fi)

)

In this section  we discuss how to reduce the learning of general models such as matrix factorization
and latent variable models to the (general) minimum conical hull problem. Five examples are given
in Table 1 to show how this general technique can be applied to speciﬁc models.

3.1 Matrix Factorization
Besides NMF  we consider more general matrix factorization (MF) models that can operate on
negative features and specify a complicated structure of F . The MF X = F W is a deterministic
latent variable model where F and W are deterministic latent factors. By assigning a likelihood
p(Xi j|Fi  (W T )j) and priors p(F ) and p(W )  its optimization model can be derived from maximum

3

ical hull problem that selects the subset A with X = F XA. In this setting  RW (W ) =(cid:80)k

likelihood or MAP estimate. The resulting objective is usually a loss function (cid:96)(·) of X − F W plus
regularization terms for F and W   i.e.  min (cid:96)(X  F W ) + RF (F ) + RW (W ).
Similar to separable NMF  minimizing the objective of general MF can be reduced to a minimum con-
i=1 g(Wi)
where g(w) = 0 if w = Xi for some i and g(w) = ∞ otherwise. This is equivalent to applying a
prior p(Wi) with ﬁnite support set on the rows of X to each row of W . In addition  the regularization
of F can be transformed to geometric constraints between points in X and in XA. Since Fi j is the
conical combination weight of XAj in recovering Xi  a large Fi j intuitively indicates a small angle
between XAj and Xi  and vice verse. For example  the sparse and graph Laplacian prior for rows of
F in subspace clustering can be reduced to “cone clustering” for ﬁnding A. See [29] for an example
of reducing the subspace clustering to general minimum conical hull problem.

3.2 Latent Variable Model

Different from deterministic MF  we build a system of equations from the moments of probabilistic
latent variable models  and then formulate it as a general minimum conical hull problem  rather
than directly solve it. Let the generalization model be h ∼ p(h; α) and x ∼ p(x|h; θ)  where h is a
latent variable  x stands for observation  and {α  θ} are parameters. In a variety of graphical models
such as GMMs and HMMs  we need to model conditional independence between groups of features.
This is also known as the multi-view assumption. W.l.o.g.  we assume that x is composed of three
groups(views) of features {xi}i∈[3] such that ∀i (cid:54)= j  xi ⊥⊥ xj|h. We further assume the dimension
k of h is smaller than pi  the dimension of xi. Since the goal is learning {α  θ}  decomposing the
moments of x rather than the data matrix X can help us get rid of the latent variable h and thus avoid
alternating minimization between {α  θ} and h. When E(xi|h) = hT OT
i (linearity assumption) 
the second and third order moments can be written in the form of matrix operator.

(cid:26) E (xi ⊗ xj) = E[E(xi|h) ⊗ E(xj|h)] = OiE(h ⊗ h)OT

j  
E (xi ⊗ xj · (cid:104)η  xl(cid:105)) = Oi [E(h ⊗ h ⊗ h) ×3 (Olη)] OT
j  

(4)
where A ×n U denotes the n-mode product of a tensor A by a matrix U  ⊗ is the outer product  and
the operator parameter η can be any vector. We will mainly focus on the models in which {α  θ} can
be exactly recovered from conditional mean vectors {Oi}i∈[3] and E(h ⊗ h)1  because they cover
most popular models such as GMMs and HMMs in real applications.
The left hand sides (LHS) of both equations in (4) can be directly estimated from training data  while
j with Oi ∈ Rpi×k and
their right hand sides (RHS) can be written in a uniﬁed matrix form OiDOT
D ∈ Rk×k. By using different η  we can obtain 2 ≤ q ≤ pl + 1 independent equations  which
compose a system of equations for Oi and Oj. Given the LHS  we can obtain the column spaces of
Oi and Oj  which respectively equal to the column and row space of OiDOT
j   a low-rank matrix
when pi > k. In order to further determine Oi and Oj  our discussion falls into two types of D.
When D is a diagonal matrix. This happens when ∀i (cid:54)= j  E(hihj) = 0. A common example is
that h is a label/state indicator such that h = ei for class/state i  e.g.  h in GMM and HMM. In this
case  the two D matrices in the RHS of (4) are

(cid:40) E(h ⊗ h) = Diag(

−−−→E(h2

i )) 

E(h ⊗ h ⊗ h) ×3 (Olη) = Diag(

−−−→E(h3

i ) · Olη) 

(5)

−−−→E(ht

rank-one matrices  i.e. (cid:80)k

i) = [E(ht

where

1)  . . .   E(ht

s=1 σ(s)Os

i ⊗ Os

k)]. So either matrix in the LHS of (4) can be written as a sum of k

j   where Os

i is the sth column of Oi.

i ⊗ Os

The general separability assumption posits that the set of k rank-one basis matrices constructing the
RHS of (4) is a unique subset A ⊆ [n] of the n samples of xi ⊗ xj constructing the left hand sides 
j = [xi ⊗ xj]As = XAs i ⊗ XAs j  the outer product of xi and xj in (As)th data point.
i.e.  Os
1Note our method can also handle more complex models that violate the linearity assumption and need higher
order moments for parameter estimation. By replacing xi in (4) with vec(xi⊗n)  the vectorization of the nth
tensor power of xi  Oi can contain nth order moments for p(xi|h; θ). However  since higher order moments are
either not necessary or difﬁcult to estimate due to high sample complexity  we will not study them in this paper.

4

Therefore  by applying q − 1 different η to (4)  we obtain the system of q equations in the following
form  where Y t is the estimate of the LHS of tth equation from training data.

k(cid:88)

s=1

∀t ∈ [q]  Y (t) =

σt s[xi ⊗ xj]As ⇔ [vec(Y (t))]t∈[q] = σ[vec(Xt i ⊗ Xt j)]t∈A.

(6)
The right equation in (6) is an equivalent matrix representation of the left one. Its LHS is a q × pipj
matrix  and its RHS is the product of a q × k matrix σ and a k × pipj matrix. By letting X ←
[vec(Y (t))]t∈[q]  F ← σ and Y ← [vec(Xt i ⊗ Xt j)]t∈[n]  we can ﬁt (6) to X = F YA in Deﬁnition
2. Therefore  learning {Oi}i∈[3] is reduced to selecting k rank-one matrices from {Xt i ⊗ Xt j}t∈[n]
indexed by A whose conical hull covers the q matrices {Y (t)}t∈[q]. Given the anchor set A  we have
ˆOi = XA i and ˆOj = XA j by assigning real data points indexed by A to the columns of Oi and Oj.
Given Oi and Oj  σ can be estimated by solving (6). In many models  a few rows of σ are sufﬁcient
to recover α. See [29] for a practical acceleration trick based on matrix completion.
When D is a symmetric matrix with nonzero off-diagonal entries. This happens in “admixture”
models  e.g.  h can be a general binary vector h ∈ {0  1}k or a vector on the probability simplex  and
the conditional mean E(xi|h) is a mixture of columns in Oi. The most well known example is LDA 
in which each document is generated by multiple topics.
We apply the general separability assumption by only using the ﬁrst equation in (4)  and treating the
matrix in its LHS as X in X = F XA. When the data are extremely sparse  which is common in
text data  selecting the rows of the denser second order moment as bases is a more reasonable and
effective assumption compared to sparse data points. In this case  the p rows of F contain k unit
vectors {ei}i∈[k]. This leads to a natural assumption of “anchor word” for LDA [3].
See [29] for the example of reducing multi-view mixture model  HMM  and LDA to general minimum
conical hull problem. It is also worth noting that we can show our method  when applied to LDA 
yields equal results but is faster than a Bayesian inference method [3]  see Theorem 4 in [29].
4 Algorithms for Minimum Conical Hull Problem
4.1 Divide-and-Conquer Anchoring (DCA) for General Minimum Conical Hull Problems

The key insights of DCA come from two observations on the geometry of the convex cone. First 
projecting a conical hull to a lower-D hyperplane partially preserves its geometry. This enables us
to distribute the original problem to a few much smaller sub-problems  each handled by a solver
to the minimum conical hull problem. Secondly  there exists a very fast anchoring algorithm for a
sub-problem on 2D plane  which only picks two anchor points based on their angles to an axis without
iterative optimization or greedy pursuit. This results in a signiﬁcantly efﬁcient DCA algorithm that
can be solely used  or embedded as a subroutine  checking if a point is covered in a conical hull.
4.2 Distributing Conical Hull Problem to Sub-problems in Low Dimensions
Due to the convexity of cones  a low-D projection of a conical hull is still a conical hull that covers the
projections of the same points covered in the original conical hull  and generated by the projections
of a subset of anchors on the extreme rays of the original conical hull.
Lemma 2. For an arbitrary point x ∈ cone(YA) ⊂ Rp  where A is the index set of the k anchors
(generators) selected from Y   for any Φ ∈ Rp×d with d ≤ p  we have

∃ ˜A ⊆ A : xΦ ∈ cone(Y ˜AΦ) 

(7)
Since only a subset of A remains as anchors after projection  solving a minimum conical hull problem
on a single low-D hyperplane rarely returns all the anchors in A. However  the whole set A can be
recovered from the anchors detected on multiple low-D hyperplanes. By sampling the projection
matrix Φ from a random ensemble M  it can be proved that w.h.p. solving only s = O(ck log k)
sub-problems are sufﬁcient to ﬁnd all anchors in A. Note c/k is the lower bound of angle α − 2β in
Theorem 1  so large c indicates a less ﬂat conical hull. See [29] for our method’s robustness to the
failure in identifying “ﬂat” anchors.
For the special case of NMF when X = F XA  the above result is proven in [28]. However  the
analysis cannot be trivially extended to the general conical hull problem when X = F YA (see Figure
1). A critical reason is that the converse of Lemma 2 does not hold: the uniqueness of the anchor set ˜A

5

Algorithm 1 DCA(X  Y  k  M)

Input: Two sets of points (rows) X ∈ Rn×p and Y ∈ Rm×p in matrix forms (ref. Table 1 to see X and Y
for different models)  number of latent factors/variables k  random matrix ensemble M;
Output: Anchor set A ⊆ [m] such that ∀i ∈ [n]  Xi ∈ cone(YA);
Divide Step (in parallel):
for i = 1 → s := O(k log k) do

Randomly draw a matrix Φ ∈ Rp×d from M;
Solve sub-problem such as ˜At = MCH(XΦ  Y Φ) by any solver  e.g.  (10);

end for
Conquer Step:

∀i ∈ [m]  compute ˆg(Yi) = (1/s)(cid:80)s

1 ˜At (Yi);

t=1

Return A as index set of the k points with the largest ˆg(Yi).

on low-D hyperplane could be violated  because non-anchors in Y may have non-zero probability to
be projected as low-D anchors. Fortunately  we can achieve a unique ˜A by deﬁning a “minimal conical
hull” on a low-D hyperplane. Then Proposition 1 reveals when w.h.p. such an ˜A is a subset of A.
Deﬁnition 4 (Minimal conical hull). Given two sets of points (rows) X and Y   the conical hull
spanned by anchors (generators) YA is the minimal conical hull covering all points in X iff

∀{i  j  s} ∈(cid:8)i  j  s | i ∈ AC = [m] \ A  j ∈ A  s ∈ [n]  Xs ∈ cone(YA) ∩ cone(Yi∪(A\j))(cid:9) (8)
we have (cid:91)XsYi > (cid:91)XsYj  where(cid:99)xy denotes the angle between two vectors x and y. The solution of

minimal conical hull is denoted by A = MCH(X  Y ).

It is easy to verify that the minimal conical hull is unique  and
the general minimum conical hull problem X = F YA under the
general separability assumption (which leads to the identiﬁability of
A) is a special case of A = MCH(X  Y ). In DCA  on each low-D
hyperplane Hi  the associated sub-problem aims to ﬁnd the anchor
set ˜Ai = MCH(XΦi  Y Φi). The following proposition gives the
probability of ˜Ai ⊆ A in a sub-problem solution.
Proposition 1 (Probability of success in sub-problem). As de-
ﬁned in Figure 2  Ai ∈ A signiﬁes an anchor point in YA  Ci ∈ X
signiﬁes a point in X ∈ Rn×p  Bi ∈ AC signiﬁes a non-anchor
point in Y ∈ Rm×p  the green ellipse marks the intersection hy-
perplane between cone(YA) and the unit sphere Sp−1  the super-
script ·(cid:48) denotes the projection of a point on the intersection hy-
perplane. Deﬁne d-dim (d ≤ p) hyperplanes {Hi}i∈[4] such that
1 ⊥ H4  let α = (cid:92)H1H2 be the angle between hyper-
A(cid:48)
3A(cid:48)
planes H1 and H2  β = (cid:92)H3H4 be the angle between H3 and H4. If H with associated projection
matrix Φ ∈ Rp×d is a d-dim hyperplane uniformly drawn from the Grassmannian manifold Gr(d  p) 
and ˜A = M CH(XΦ  Y Φ) is the solution to the minimal conical hull problem  we have

2 ⊥ H3  B(cid:48)

2 ⊥ H2  B(cid:48)

2 ⊥ H1  A(cid:48)

Figure 2: Proposition 1.

1C(cid:48)

1A(cid:48)

1A(cid:48)

Pr(B1 ∈ ˜A) =

  Pr(A2 ∈ ˜A) =

β
2π

α − β
2π

.

(9)

See [29] for proof  discussion and analysis of robustness to unimportant “ﬂat” anchors and data noise.
Theorem 1 (Probability bound). Following the same notations in Proposition 1  suppose p∗∗ =

min{A1 A2 A3 B1 C1}(α − 2β) ≥ c/k > 0. It holds with probability at least 1 − k exp(cid:0)− cs

DCA successfully identiﬁes all the k anchors in A  where s is the number of sub-problems solved.

(cid:1) that

3k

See [29] for proof. Given Theorem 1  we can immediately achieve the following corollary about the
number of sub-problems that guarantee success of DCA in ﬁnding A.
Corollary 1 (Number of sub-problems). With probability 1 − δ  DCA can correctly recover the
anchor set A by solving Ω( 3k

c log k

δ ) sub-problems.

See [29] for the idea of divide-and-conquer randomization in DCA  and its advantage over Johnson-
Lindenstrauss (JL) Lemma based methods.

6

H1 A’1 A’2 A’3$C’2 C1 B’1 H2 H3 H4 α"β"O$A1 A3$A2 C’1 C2 … … 4.3 Anchoring on 2D Plane

Although DCA can invoke any solver for the sub-problem on any low-D hyperplane  a very fast
solver for the 2D sub-problem always shows high accuracy in locating anchors when embedded into
DCA. Its motivation comes from the geometry of conical hull on a 2D plane  which is a special
case of a d-dim hyperplane H in the sub-problem of DCA. It leads to a non-iterative algorithm for
A = MCH(X  Y ) on the 2D plane. It only requires computing n + m cosine values  ﬁnding the
min/max of the n values  and comparing the remaining m ones with the min/max value.
According to Figure 1  the two anchors Y ˜AΦ on a 2D plane have the min/max (among points in Y Φ )
angle (to either axis) that is larger/smaller than all angles of points in XΦ  respectively. This leads to
the following closed form of ˜A.

˜A = {arg min
i∈[m]

( (cid:92)(YiΦ)ϕ − max
j∈[n]

(cid:92)(XjΦ)ϕ)+  arg min
i∈[m]

(cid:92)(XjΦ)ϕ − (cid:92)(YiΦ)ϕ)+} 

(min
j∈[n]

(10)
where (x)+ = x if x ≥ 0 and ∞ otherwise  and ϕ can be either the vertical or horizontal axis on a
2D plane. By plugging (10) in DCA as the solver for s sub-problems on random 2D planes  we can
obtain an extremely fast learning algorithm.
Note for the special case when X = Y   (10) degenerates to ﬁnding the two points in XΦ with the
(cid:92)(XiΦ)ϕ}.
smallest and largest angles to an axis ϕ  i.e.  ˜A = {arg mini∈[n]
This is used in matrix factorization and the latent variable model with nonzero off-diagonal D.
See [29] for embedding DCA as a fast subroutine into other methods  and detailed off-the-shelf DCA
algorithms of NMF  SC  GMM  HMM and LDA. A brief summary is in Table 1.
5 Experiments

(cid:92)(XiΦ)ϕ  arg maxi∈[n]

See [29] for a complete experimental section with results of DCA for NMF  SC  GMM  HMM  and
LDA  and comparison to other methods on more synthetic and real datasets.

Figure 3: Separable NMF on a randomly generated 300 × 500 matrix  each point on each curve is the result by averaging 10 independent
random trials. SFO-greedy algorithm for submodular set cover problem. LP-test is the backward removal algorithm from [4]. LEFT: Accuracy
of anchor detection (higher is better). Middle: Negative relative (cid:96)2 recovery error of anchors (higher is better). Right: CPU seconds.

Figure 4: Clustering accuracy (higher is better) and CPU seconds vs. Number of clusters for Gaussian mixture model on CMU-PIE (left) and
YALE (right) human face dataset. We randomly split the raw pixel features into 3 groups  each associates to a view in our multi-view model.

Figure 5: Likelihood (higher is better) and CPU seconds vs. Number of states for using an HMM to model the stock price of 2 companies from
01/01/1995-05/18/2014 collected by Yahoo Finance. Since no ground truth label is given  we measure likelihood on training data.
DCA for Non-negative Matrix Factorization on Synthetic Data. The experimental comparison
results are shown in Figure 3. Greedy algorithms SPA [14]  XRAY [19] and SFO achieves the best

7

10−210−110010100.10.20.30.40.50.60.70.80.91noise levelanchor index recovery rate SPAXRAYDCA(s=50)DCA(s=92)DCA(s=133)DCA(s=175)SFOLP−test10−210−1100101−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10noise level−anchor recovery error SPAXRAYDCA(s=50)DCA(s=92)DCA(s=133)DCA(s=175)SFOLP−test10−210−110010110−510−410−310−210−1100101noise levelCPU seconds SPAXRAYDCA(s=50)DCA(s=92)DCA(s=133)DCA(s=175)SFOLP−test30609012015018021024027030000.050.10.150.20.250.30.35Number of Clusters/Mixture ComponentsClustering Accuracycmu−pie DCA GMM(s=171)DCA GMM(s=341)DCA GMM(s=682)DCA GMM(s=1023)k−meansSpectral GMMEM for GMM30609012015018021024027030010−1100101102103Number of Clusters/Mixture ComponentsCPU secondscmu−pie DCA GMM(s=171)DCA GMM(s=341)DCA GMM(s=682)DCA GMM(s=1023)k−meansSpectral GMMEM for GMM193857769511413315217119000.050.10.150.20.250.30.350.40.450.5Number of Clusters/Mixture ComponentsClustering Accuracyyale DCA GMM(s=171)DCA GMM(s=341)DCA GMM(s=682)DCA GMM(s=853)k−meansSpectral GMMEM for GMM193857769511413315217119010−210−1100101102Number of Clusters/Mixture ComponentsCPU secondsyale DCA GMM(s=171)DCA GMM(s=341)DCA GMM(s=682)DCA GMM(s=853)k−meansSpectral GMMEM for GMM34567891028.52929.53030.53131.53232.53333.5Number of StatesloglikelihoodBarclays DCA HMM(s=32)DCA HMM(s=64)DCA HMM(s=96)DCA HMM(s=160)Baum−Welch(EM)Spectral method34567891010−310−210−1100101Number of StatesCPU secondsBarclays DCA HMM(s=32)DCA HMM(s=64)DCA HMM(s=96)DCA HMM(s=160)Baum−Welch(EM)Spectral method3456789102345678Number of StatesloglikelihoodJP−Morgan DCA HMM(s=32)DCA HMM(s=96)DCA HMM(s=160)DCA HMM(s=256)Baum−Welch(EM)Spectral method34567891010−310−210−1100101Number of StatesCPU secondsJP−Morgan DCA HMM(s=32)DCA HMM(s=96)DCA HMM(s=160)DCA HMM(s=256)Baum−Welch(EM)Spectral methodaccuracy and smallest recovery error when noise level is above 0.2  but XRAY and SFO are the
slowest two. SPA is slightly faster but still much slower than DCA. DCA with different number of
sub-problems shows slightly less accuracy than greedy algorithms  but the difference is acceptable.
Considering its signiﬁcant acceleration  DCA offers an advantageous trade-off. LP-test [4] has the
exact solution guarantee  but it is not robust to noise  and too slow. Therefore  DCA provides a much
faster and more practical NMF algorithm with comparable performance to the best ones.
DCA for Gaussian Mixture Model on CMU-PIE and YALE Face Dataset. The experimental
comparison results are shown in Figure 4. DCA consistently outperforms other methods (k-means 
EM  spectral method [1]) on accuracy  and shows speedups in the range 20-2000. By increasing the
number of sub-problems  the accuracy of DCA improves. Note the pixels of face images always
exceed 1000  and thus results in slow computation of pairwise distances required by other clustering
methods. DCA exhibits the fastest speed because the number of sub-problems s = O(k log k) does
not depend on the feature dimension  and thus merely 171 2D random projections are sufﬁcient
for obtaining a promising clustering result. The spectral method performs poorer than DCA due
to the large variance of sample moments. DCA uses the separability assumption in estimating the
eigenspace of the moment  and thus effectively reduces the variance.
Table 2: Motion prediction accuracy (higher is better) of the test set for 6 motion capture sequences from CMU-mocap dataset. The motion
for each frame is manually labeled by the authors of [16]. In the table  s13s29(39/63) means that we split sequence 29 of subject 13 into
sub-sequences  each has 63 frames  in which the ﬁrst 39 ones are for training and the rest are for test. Time is measured in ms.

Sequence
Measure
Baum-Welch (EM)
Spectral Method
DCA-HMM (s=9)
DCA-HMM (s=26)
DCA-HMM (s=52)
DCA-HMM (s=78)

s13s29(39/63)
Time
Acc
383
0.50
80
0.20
0.33
3.3
3.3
0.50
3.4
0.50
0.66
3.4

s13s30(25/51)
Time
Acc
140
0.50
43
0.25
0.92
1
1.00
1
1.1
0.50
0.93
1.1

s13s31(25/50)
Time
Acc
148
0.46
58
0.13
0.19
1.5
0.65
1.6
1.6
0.43
0.41
1.6

s14s06(24/40)
Time
Acc
368
0.34
66
0.29
0.29
4.8
0.60
4.8
4.9
0.48
0.51
4.9

s14s14(29/43)
Time
Acc
529
0.62
134
0.63
0.79
3
3
0.45
0.80
3.2
0.80
6.7

s14s20(29/43)
Time
Acc
345
0.77
70
0.59
0.28
3
0.89
3
3.1
0.78
0.83
3.2

Figure 6: LEFT: Perplexity (smaller is better) on test set and CPU seconds vs. Number of topics for LDA on NIPS1-17 Dataset  we randomly
selected 70% documents for training and the rest 30% is used for test. RIGHT: Mutual Information (higher is better) and CPU seconds vs.
Number of clusters for subspace clustering on COIL-100 Dataset.
DCA for Hidden Markov Model on Stock Price and Motion Capture Data. The experimental
comparison results for stock price modeling and motion segmentation are shown in Figure 5 and Table
2  respectively. In the former one  DCA always achieves slightly lower but comparable likelihood
compared to Baum-Welch (EM) method [5]  while the spectral method [2] performs worse and
unstably. DCA shows a signiﬁcant speed advantage compared to others  and thus is more preferable
in practice. In the latter one  we evaluate the prediction accuracy on the test set  so the regularization
caused by separability assumption leads to the highest accuracy and fastest speed of DCA.
DCA for Latent Dirichlet Allocation on NIPS1-17 Dataset. The experimental comparison results
for topic modeling are shown in Figure 6. Compared to both traditional EM and the Gibbs sam-
pling [23]  DCA not only achieves both the smallest perplexity (highest likelihood) on the test set
and the highest speed  but also the most stable performance when increasing the number of topics. In
addition  the “anchor word” achieved by DCA provides more interpretable topics than other methods.
DCA for Subspace Clustering on COIL-100 Dataset. The experimental comparison results for
subspace clustering are shown in Figure 6. DCA provides a much more practical algorithm that can
achieve comparable mutual information but at a more than 1000 times speedup over the state-of-the-art
SC algorithms such as SCC [9]  SSC [12]  LRR [21]  and RSC [26].
Acknowledgments: We would like to thank MELODI lab members for proof-reading and the
anonymous reviewers for their helpful comments. This work is supported by TerraSwarm research
center administered by the STARnet phase of the Focus Center Research Program (FCRP) sponsored
by MARCO and DARPA  by the National Science Foundation under Grant No. (IIS-1162606)  and
by Google  Microsoft  and Intel research awards  and by the Intel Science and Technology Center for
Pervasive Computing.

8

51322303847556372802000220024002600280030003200340036003800Number of TopicsPerplexity DCA LDA(s=801)DCA LDA(s=2001)DCA LDA(s=3336)DCA LDA(s=5070)EM variationalGibbs samplingSpectral method513223038475563728010−1100101102103104Number of TopicsCPU seconds DCA LDA(s=801)DCA LDA(s=2001)DCA LDA(s=3336)DCA LDA(s=5070)EM variationalGibbs samplingSpectral method204060801001201401601802000.10.20.30.40.50.60.70.8Number of Clusters/Mixture ComponentsMutual Information DCA SC(s=307)DCA SC(s=819)DCA SC(s=1229)DCA SC(s=1843)SSCSCCLRRRSC2040608010012014016018020010−1100101102103104105Number of Clusters/Mixture ComponentsCPU seconds DCA SC(s=307)DCA SC(s=819)DCA SC(s=1229)DCA SC(s=1843)SSCSCCLRRRSCReferences
[1] A. Anandkumar  D. P. Foster  D. Hsu  S. Kakade  and Y. Liu. A spectral algorithm for latent dirichlet

allocation. In NIPS  2012.

[2] A. Anandkumar  D. Hsu  and S. M. Kakade. A method of moments for mixture models and hidden markov

models. In COLT  2012.

[3] S. Arora  R. Ge  Y. Halpern  D. M. Mimno  A. Moitra  D. Sontag  Y. Wu  and M. Zhu. A practical algorithm

for topic modeling with provable guarantees. In ICML  2013.

[4] S. Arora  R. Ge  R. Kannan  and A. Moitra. Computing a nonnegative matrix factorization - provably. In

STOC  2012.

[5] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of ﬁnite state Markov chains.

Annals of Mathematical Statistics  37:1554–1563  1966.

[6] M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS  2010.
[7] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent dirichlet allocation. Journal of Maching Learning Research

(JMLR)  3:993–1022  2003.

[8] J. T. Chang. Full reconstruction of markov models on evolutionary trees: Identiﬁability and consistency.

Mathematical Biosciences  137(1):51–73  1996.

[9] G. Chen and G. Lerman. Spectral curvature clustering (scc). International Journal of Computer Vision

(IJCV)  81(3):317–330  2009.

[10] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the em

algorithm. Journal of the Royal Statistical Society  Series B  39(1):1–38  1977.

[11] D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition

into parts? In NIPS  2003.

[12] E. Elhamifar and R. Vidal. Sparse subspace clustering. In CVPR  2009.
[13] S. Geman and D. Geman. Stochastic relaxation  gibbs distributions  and the bayesian restoration of images.

IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  6(6):721–741  1984.

[14] N. Gillis and S. A. Vavasis. Fast and robust recursive algorithmsfor separable nonnegative matrix fac-
torization. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  36(4):698–714 
2014.

[15] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden markov models. In COLT 

2009.

[16] M. C. Hughes  E. B. Fox  and E. B. Sudderth. Effective split-merge monte carlo methods for nonparametric

models of sequential data. In NIPS  2012.

[17] A. T. Kalai  A. Moitra  and G. Valiant. Efﬁciently learning mixtures of two gaussians. In STOC  2010.
[18] R. Kannan  H. Salmasian  and S. Vempala. The spectral method for general mixture models. In COLT 

2005.

[19] A. Kumar  V. Sindhwani  and P. Kambadur. Fast conical hull algorithms for near-separable nonnegative

matrix factorization. In ICML  2013.

[20] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature 

401:788–791  1999.

[21] G. Liu  Z. Lin  and Y. Yu. Robust subspace segmentation by low-rank representation. In ICML  2010.
[22] K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the

Royal Society of London. A  185:71–110  1894.

[23] I. Porteous  D. Newman  A. Ihler  A. Asuncion  P. Smyth  and M. Welling. Fast collapsed gibbs sampling

for latent dirichlet allocation. In SIGKDD  pages 569–577  2008.

[24] R. A. Redner and H. F. Walker. Mixture Densities  Maximum Likelihood and the Em Algorithm. SIAM

Review  26(2):195–239  1984.

[25] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In NIPS  2008.
[26] M. Soltanolkotabi  E. Elhamifar  and E. J. Cand`es. Robust subspace clustering. arXiv:1301.2603  2013.
[27] D. Titterington  A. Smith  and U. Makov. Statistical Analysis of Finite Mixture Distributions. Wiley  New

York  1985.

[28] T. Zhou  W. Bian  and D. Tao. Divide-and-conquer anchoring for near-separable nonnegative matrix

factorization and completion in high dimensions. In ICDM  2013.

[29] T. Zhou  J. Bilmes  and C. Guestrin. Extended version of “divide-and-conquer learning by anchoring a

conical hull”. In Extended version of a accepted NIPS-2014 paper  2014.

9

,Tianyi Zhou
Jeff Bilmes
Carlos Guestrin
Prateek Jain
Ambuj Tewari