2017,Robust Optimization for Non-Convex Objectives,We consider robust optimization problems  where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to stochastic optimization: given an oracle that returns $\alpha$-approximate solutions for distributions over objectives  we compute a distribution over solutions that is $\alpha$-approximate in the worst case.  We show that derandomizing this solution is NP-hard in general  but can be done for a broad class of statistical learning tasks.  We apply our results to robust neural network training and submodular optimization.  We evaluate our approach experimentally on corrupted character classification and robust influence maximization in networks.,Robust Optimization for Non-Convex Objectives

Robert Chen

Computer Science
Harvard University

Brendan Lucier
Microsoft Research

New England

Yaron Singer

Computer Science
Harvard University

Vasilis Syrgkanis
Microsoft Research

New England

Abstract

We consider robust optimization problems  where the goal is to optimize in the
worst case over a class of objective functions. We develop a reduction from
robust improper optimization to stochastic optimization: given an oracle that
returns ↵-approximate solutions for distributions over objectives  we compute a
distribution over solutions that is ↵-approximate in the worst case. We show that
derandomizing this solution is NP-hard in general  but can be done for a broad
class of statistical learning tasks. We apply our results to robust neural network
training and submodular optimization. We evaluate our approach experimentally on
corrupted character classiﬁcation and robust inﬂuence maximization in networks.

1

Introduction

In many learning tasks we face uncertainty about the loss we aim to optimize. Consider  for example 
a classiﬁcation task such as character recognition  required to perform well under various types of
distortion. In some environments  such as recognizing characters in photos  the classiﬁer must handle
rotation and patterned backgrounds. In a different environment  such as low-resolution images  it
is more likely to encounter noisy pixelation artifacts. Instead of training a separate classiﬁer for
each possible scenario  one seeks to optimize performance in the worst case over different forms of
corruption (or combinations thereof) made available to the trainer as black-boxes.
More generally  our goal is to ﬁnd a minimax solution that optimizes in the worst case over a given
family of functions. Even if each individual function can be optimized effectively  it is not clear such
solutions would perform well in the worst case. In many cases of interest  individual objectives are
non-convex and hence state-of-the-art methods are only approximate. In stochastic optimization 
where one must optimize a distribution over loss functions  approximate stochastic optimization is
often straightforward  since loss functions are commonly closed under convex combination. Can
approximately optimal stochastic solutions yield an approximately optimal robust solution?
In this paper we develop a reduction from robust optimization to stochastic optimization. Given an ↵-
approximate oracle for stochastic optimization we show how to implement an ↵-approximate solution
for robust optimization under a necessary extension  and illustrate its effectiveness in applications.

Main Results. Given an ↵-approximate stochastic oracle for distributions over (potentially non-
convex) loss functions  we show how to solve ↵-approximate robust optimization in a convexiﬁed
solution space. This outcome is “improper” in the sense that it may lie outside the original solution
space  if the space is non-convex. This can be interpreted as computing a distribution over solutions.
We show that the relaxation to improper learning is necessary in general: It is NP-hard to achieve
robust optimization with respect to the original outcome space  even if stochastic optimization can be
solved exactly  and even if there are only polynomially many loss functions. We complement this
by showing that in any statistical learning scenario where loss is convex in the predicted dependent
variable  we can ﬁnd a single (deterministic) solution with matching performance guarantees.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Technical overview. Our approach employs an execution of no-regret dynamics on a zero-sum
game  played between a learner equipped with an ↵-approximate stochastic oracle  and an adversary
who aims to ﬁnd a distribution over loss functions that maximizes the learner’s loss. This game
converges to an approximately robust solution  in which the learner and adversary settle upon an ↵-
approximate minimax solution. This convergence is subject to an additive regret term that converges
at a rate of T 1/2 over T rounds of the learning dynamics.

Applications. We illustrate the power of our reduction through two main examples. We ﬁrst
consider statistical learning via neural networks. Given an arbitrary training method  our reduction
generates a net that optimizes robustly over a given class of loss functions. We evaluate our method
experimentally on a character recognition task  where the loss functions correspond to different
corruption models made available to the learner as black boxes. We verify experimentally that our
approach signiﬁcantly outperforms various baselines  including optimizing for average performance
and optimizing for each loss separately. We also apply our reduction to inﬂuence maximization 
where the goal is to maximize a concave function (the independent cascade model of inﬂuence
[11]) over a non-convex space (subsets of vertices in a network). Previous work has studied robust
inﬂuence maximization directly [9  5  15]  focusing on particular  natural classes of functions (e.g. 
edge weights chosen within a given range) and establishing hardness and approximation results.
In comparison  our method is agnostic to the particular class of functions  and achieves a strong
approximation result by returning a distribution over solutions. We evaluate our method on real and
synthetic datasets  with the goal of robustly optimizing a suite of random inﬂuence instantiations. We
verify experimentally that our approach signiﬁcantly outperforms natural baselines.

Related work. There has recently been a great deal of interest in robust optimization in machine
learning [20  4  17  21  16]. For continuous optimization  the work that is closest to ours is perhaps
that by Shalev-Shwartz and Wexler [20] and Namkoong and Duchi [17] that use robust optimization
to train against convex loss functions. The main difference is that we assume a more general setting
in which the loss functions are non-convex and one is only given access to the stochastic oracle.
Hence  the proof techniques and general results from these papers do not apply to our setting. We
note that our result generalizes these works  as they can be considered as the special case in which
we have a distributional oracle whose approximation is optimal. In particular  [20  Theorem 1]
applies to the realizable statistical learning setting where the oracle has small mistake bound C. Our
applications require a more general framing that hold for any optimization setting with access to
an approximate oracle  and approximation is in the multiplicative sense with respect to the optimal
value. In submodular optimization there has been a great deal of interest in robust optimization as
well [12  13  10  6]. The work closest to ours is that by He and Kempe [10] who consider a slightly
different objective than ours. Kempe and He’s results apply to inﬂuence but do not extend to general
submodular functions. Finally  we note that unlike recent work on non-convex optimization [7  1  8]
our goal in this paper is not to optimize a non-convex function. Rather  we abstract the non-convex
guarantees via the approximate stochastic oracle.

2 Robust Optimization with Approximate Stochastic Oracles

We consider the following model of optimization that is robust to objective uncertainty. There is a
space X over which to optimize  and a ﬁnite set of loss functions1 L = {L1  . . .   Lm} where each
Li 2L is a function from X to [0  1]. Intuitively  our goal is to ﬁnd some x 2X that achieves low
loss in the worst-case over loss functions in L. For x 2X   write g(x) = maxi2[m] Li(x) for the
worst-case loss of x. The minimax optimum ⌧ is given by

⌧ = min
x2X

g(x) = min
x2X

max
i2[m]

Li(x).

(1)

The goal of ↵-approximate robust optimization is to ﬁnd x such that g(x)  ↵⌧.2

1We describe an extension to inﬁnite sets of loss functions in the full version of the paper. Our results also

extend naturally to the goal of maximizing the minimum of a class of reward functions.

2This oracle framework is similar to that used by Ben-Tal et al. [3]  but where the approximation is

multiplicative rather than additive.

2

Algorithm 1 Oracle Efﬁcient Improper Robust Optimization

Input: Objectives L = {L1  . . .   Lm}  Apx stochastic oracle M  parameters T ⌘
for each time step t 2 [T ] do

Set

wt[i] / exp(⌘

t1X⌧ =1
end for
Output: the uniform distribution over {x1  . . .   xT}

Set xt = M (wt)

Li(x⌧ ))

(3)

Given a distribution P over solutions X   write g(P) = maxi2[m] Ex⇠P [Li(x)] for the worst-case
expected loss of a solution drawn from P. A weaker version of robust approximation is improper
robust optimization: ﬁnd a distribution P over X such that g(P)  ↵⌧.
Our results take the form of reductions to an approximate stochastic oracle  which ﬁnds a solution
x 2X that approximately minimizes a given distribution over loss functions.3
Deﬁnition 1 (↵-Approximate Stochastic Oracle). Given a distribution D over L  an ↵-approximate
stochastic oracle M (D) computes x⇤ 2X such that
EL⇠D [L(x⇤)]  ↵ min
(2)
x2X
Improper Robust Optimization with Oracles

EL⇠D [L(x)] .

2.1

We ﬁrst show that  given access to an ↵-approximate stochastic oracle  it is possible to efﬁciently
implement improper ↵-approximate robust optimization  subject to a vanishing additive loss term.

Theorem 1. Given access to an ↵-approximate stochastic oracle  Algorithm 1 with ⌘ =q log(m)
computes a distribution P over solutions  deﬁned as a uniform distribution over a set {x1  . . .   xT} 
so that

2T

Ex⇠P [Li(x)]  ↵⌧ +r 2 log(m)
Moreover  for any ⌘ the distribution P computed by Algorithm 1 satisﬁes:
.

max
i2[m]

log(m)

Ex⇠P [Li(x)]  ↵(1 + ⌘)⌧ +

⌘T

.

T

max
i2[m]

(4)

(5)

Proof. We give the proof of the ﬁrst result and defer the second result to the full version of the paper.
We can interpret Algorithm 1 in the following way. We deﬁne a zero-sum game between a learner
and an adversary. The learner’s action set is equal to X and the adversary’s action set is equal to [m].
The loss of the learner when he picks x 2X and the adversary picks i 2 [m] is deﬁned as Li(x).
The corresponding payoff of the adversary is Li(x).
We will run no-regret dynamics on this zero-sum game  where at every iteration t = 1  . . .   T   the
adversary will pick a distribution over functions and subsequently the learner picks a solution xt.
For simpler notation we will denote with wt the probability density function on [m] associated with
the distribution of the adversary. That is  wt[i] is the probability of picking function Li 2L . The
adversary picks a distribution wt based on some arbitrary no-regret learning algorithm on the m
actions in L. For concreteness consider the case where the adversary picks a distribution based on the
multiplicative weight updates algorithm  i.e. 

wt[i] / exp(r log(m)

2T

Li(x⌧ )) .

t1X⌧ =1

(6)

3All our results easily extend to the case where the oracle computes a solution that is approximately optimal
up to an additive error  rather than a multiplicative one. For simplicity of exposition we present the multiplicative
error case as it is more in line with the literature on approximation algorithms.

3

Subsequently the learner picks a solution xt that is the output of the ↵-approximate stochastic oracle
on the distribution selected by the adversary at time-step t. That is 

xt = M (wt) .

(7)

Write ✏(T ) =q 2 log(m)

that

T

. By the guarantees of the no-regret algorithm for the adversary  we have

1
T

TXt=1

EI⇠wt [LI(xt)]  max
i2[m]

1
T

TXt=1

Li(xt)  ✏(T ).

(8)

Combining the above with the guarantee of the stochastic oracle we have

⌧ = min
x2X

max
i2[m]

Li(x)  min
x2X

1
T

EI⇠wt [LI(x)] 

TXt=1
1
↵ · EI⇠wt [LI(xt)]

1
T

TXt=1

min
x2X

EI⇠wt [LI(x)]

(By oracle guarantee for each t)

1
T

TXt=1
↵ · max

i2[m]

1





Li(xt)  ✏(T )! .

1
T

TXt=1

(By no-regret of adversary)

Thus  if we deﬁne with P to be the uniform distribution over {x1  . . .   xT}  then we have derived
(9)

max
i2[m]

Ex⇠P [Li(x)]  ↵⌧ + ✏(T )

as required.

A corollary of Theorem 1 is that if the solution space X is convex and the objective functions Li 2L
are all convex functions  then we can compute a single solution x⇤ that is approximately minimax
optimal. Of course  in this setting one can calculate and optimize the maximum loss directly in time
proportional to |L|; this result therefore has the most bite when the set of functions is large.
Corollary 2. If the space X is a convex space and each loss function Li 2L is a convex function 
t=1 xt 2X   where {x1  . . .   xT} are the output of Algorithm 1  satisﬁes:
then the point x⇤ = 1
(10)

T PT

Li(x⇤)  ↵⌧ +r 2 log(m)

max
i2[m]

T

Proof. By Theorem 1  we get that if P is the uniform distribution over {x1  . . .   xT} then

Ex⇠P [Li(x)]  ↵⌧ +r 2 log(m)

T

.

max
i2[m]

Since X is convex  the solution x⇤ = Ex⇠P [x] is also part of X . Moreover  since each Li 2L is
convex  we have that Ex⇠P [Li(x)]  Li(Ex⇠P [x]) = Li(x⇤). We therefore conclude

max
i2[m]

Li(x⇤)  max
i2[m]

as required.

Ex⇠P [Li(x)]  ↵⌧ +r 2 log(m)

T

2.2 Robust Statistical Learning
Next we apply our main theorem to statistical learning. Consider regression or classiﬁcation settings
where data points are pairs (z  y)  z 2Z is a vector of features  and y 2Y is the dependent variable.
The solution space X is then a space of hypotheses H  with each h 2H a function from Z to Y. We
also assume that Y is a convex subset of a ﬁnite-dimensional vector space.
We are given a set of loss functions L = {L1  . . .   Lm}  where each Li 2L is a functional
Li : H! [0  1]. Theorem 1 implies that  given an ↵-approximate stochastic optimization oracle 

4

we can compute a distribution over T hypotheses from H that achieves an ↵-approximate minimax
guarantee. If the loss functionals are convex over hypotheses  then we can compute a single ensemble
hypothesis h⇤ (possibly from a larger space of hypotheses  if H is non-convex) that achieves this
guarantee.
Theorem 3. Suppose that L = {L1  . . .   Lm} are convex functionals. Then the ensemble hypoth-
esis h⇤ = 1
t=1 h  where {h1  . . .   hT} are the hypotheses output by Algorithm 1 given an
↵-approximate stochastic oracle  satisﬁes

T PT

max
i2[m]

Li(h⇤)  ↵ min
h2H

max
i2[m]

Li(h) +r 2 log(m)

T

.

(11)

Proof. The proof is similar to the proof of Corollary 2.

We emphasize that the convexity condition in Theorem 3 is over the class of hypotheses  rather than
over features or any natural parameterization of H (such as weights in a neural network). This is a
mild condition that applies to many examples in statistical learning theory. For instance  consider the
case where each loss Li(h) is the expected value of some ex-post loss function `i(h(z)  y) given a
distribution Di over Z ⇥ Y :

Li(h) = E(z y)⇠Di [`i(h(z)  y)] .

(12)
In this case  it is enough for the function `i(· ·) to be convex with respect to its ﬁrst argument (i.e. 
the predicted dependent variable). This is satisﬁed by most loss functions used in machine learning 
such as multinomial logistic loss (cross-entropy loss) `(ˆy  y) = Pc2[k] yc log(ˆyc) from multi-class
classiﬁcation or squared loss `(ˆy  y) = kˆy yk2 as used in regression. For all these settings  Theorem
3 provides a tool for improper robust learning  where the ﬁnal hypothesis h⇤ is an ensemble of T base
hypotheses from H. Again  the underlying optimization problem can be arbitrarily non-convex in the
natural parameters of the hypothesis space; in Section 3.1 we will show how to apply this approach to
robust training of neural networks  where the stochastic oracle is simply a standard network training
method. For neural networks  the fact that we achieve improper learning (as opposed to standard
learning) corresponds to training a neural network with a single extra layer relative to the networks
generated by the oracle.

2.3 Robust Submodular Maximization
In robust submodular maximization we are given a family of reward functions F = {f1  . . .   fm} 
where each fi 2F is a monotone submodular function from a ground set N of n elements to [0  1].
Each function is assumed to be monotone and submodular  i.e.  for any S ✓ T ✓ N  fi(S)  fi(T );
and for any S  T ✓ N  f (S [ T ) + f (S \ T )  f (S) + f (T ). The goal is to select a set S ✓ N
of size k whose worst-case value over i  i.e.  g(S) = mini2[m] fi(S)  is at least a 1/↵ factor of the
minimax optimum ⌧ = maxT :|T|k mini2[m] fi(T ).
This setting is a special case of our general robust optimization setting (phrased in terms of rewards
rather than losses). The solution space X is equal to the set of subsets of size k among all elements in
N and the set F is the set of possible objective functions. The stochastic oracle 1  instantiated in
this setting  asks for the following: given a convex combination of submodular functions F (S) =
Pm
i=1 w[i] · fi(S)  compute a set S⇤ such that F (S⇤)  1
Computing the maximum value set of size k is NP-hard even for a single submodular function. The
following very simple greedy algorithm computes a (1  1/e)-approximate solution [19]: begin with
Scur = ;  and at each iteration add to the current solution Scur the element j 2 N  Scur that has
the largest marginal contribution: f ({j}[ Scur)  f (Scur). Moreover  this approximation ratio is
known to be the best possible in polynomial time [18]. Since a convex combination of monotone
submodular functions is also a monotone submodular function  we immediately get that there exists a
(1  1/e)-approximate stochastic oracle that can be computed in polynomial time. The algorithm is
formally given in Algorithm 2. Combining the above with Theorem 1 we get the following corollary.
Corollary 4. Algorithm 1  with stochastic oracle Mgreedy  computes in time poly(T  n) a distribution
P over sets of size k  deﬁned as a uniform distribution over a set {S1  . . .   ST}  such that

↵ maxS:|S|k F (S).

ES⇠P [fi(S)] ✓1 

1

e◆ (1  ⌘)⌧ 

min
i2[m]

log(m)

⌘T

.

(13)

5

Algorithm 2 Greedy stochastic Oracle for Submodular Maximization Mgreedy

Input: Set of elements N  objectives F = {f1  . . .   fm}  distribution over objectives w
Set Scur = ;
for j = 1 to k do
Let j⇤ = arg maxj2NScurPm
Set Scur = {j⇤}[ Scur

i=1 w[i] (fi({j}[ Scur)  fi(Scur))

end for

Figure 1: Sample MNIST image with each of the corruptions applied to it. Background Corruption
Set & Shrink Corruption Set (top). Pixel Corruption Set & Mixed Corruption Set (bottom).

We show in the full version of the paper that computing a single set S that achieves a (1  1/e)-
approximation to ⌧ is also N P -hard. This is true even if the functions fi are additive. However  by
allowing a randomized solution over sets we can achieve a constant factor approximation to ⌧ in
polynomial time.
Since the functions are monotone  the above result implies a simple way of constructing a single set
S⇤ that is of larger size than k  which deterministically achieves a constant factor approximation to ⌧.
The latter holds by simply taking the union of the sets {S1  . . .   ST} in the support of the distribution
returned by Algorithm 1. We get the following bi-criterion approximation scheme.
Corollary 5. Suppose that we run the reward version of Algorithm 1  with ⌘ = ✏ and for T = log(m)
 
⌧ ✏2
returning {S1  . . .   ST}. Then the set S⇤ = S1 [ . . . [ ST   which is of size at most k log(m)
  satisﬁes

⌧ ✏2

fi(S⇤) ✓1 

min
i2[m]

1

e  2✏◆ ⌧.

(14)

3 Experiments4

3.1 Robust Classiﬁcation with Neural Networks

A classic application of our robust optimization framework is classiﬁcation with neural networks
for corrupted or perturbed datasets. We have a data set Z of pairs (z  y) of an image z 2Z and
label y 2Y that can be corrupted in m different ways which produces data sets Z1  . . .   Zm. The
hypothesis space H is the set of all neural nets of some ﬁxed architecture and for each possible
assignment of weights. We denote each such hypothesis with h(·; ✓) : Z!Y for ✓ 2 Rd  with d
being the number of parameters (weights) of the neural net. If we let Di be the uniform distribution
over each corrupted data set Zi  then we are interested in minimizing the empirical cross-entropy
(aka multinomial logistic) loss in the worst case over these different distributions Di. The latter is a
special case of our robust statistical learning framework from Section 2.2.
Training a neural network is a non-convex optimization problem and we have no guarantees on its
performance. We instead assume that for any given distribution D over pairs (z  y) of images and
labels and for any loss function `(h(z; ✓)  y)  training a neural net with stochastic gradient descent
run on images drawn from D can achieve an ↵ approximation to the optimal expected loss  i.e.
min✓2Rd E(z y)⇠D [`(h(z; ✓)  y)]. Notice that this implies an ↵-approximate stochastic oracle for the
4Code used to implement the algorithms and run the experiments is available at https://github.com/

12degrees/Robust-Classification/.

6

corrupted dataset robust training problem: for any distribution w over the different corruptions [m] 
the stochastic oracle asks to give an ↵-approximation to the minimization problem:

min
✓2Rd

mXi=1

w[i] · E(z y)⇠Di [`(h(z; ✓)  y)]

(15)

function being the weighted combination of loss functionsPm

The latter is simply another expected loss problem with distribution over images being the mixture
distribution deﬁned by ﬁrst drawing a corruption index i from w and then drawing a corrupted
image from distribution Di. Hence  our oracle assumption implies that SGD on this mixture is an
↵-approximation. By linearity of expectation  an alternative way of viewing the stochastic oracle
problem is that we are training a neural net on the original distribution of images  but with loss
i=1 w[i] · `(h(ci(z); ✓)  y)  where
ci(z) is the i-th corrupted version of image z. In our experiments we implemented both of these
interpretations of the stochastic oracle  which we call the Hybrid Method and Composite Method 
respectively  when designing our neural network training scheme (see the full version of the paper
for further details). Finally  because we use the cross-entropy loss  which is convex in the prediction
of the neural net  we can also apply Theorem 3 to get that the ensemble neural net  which takes the
average of the predictions of the neural nets created at each iteration of the robust optimization  will
also achieve good worst-case loss (we refer to this as Ensemble Bottleneck Loss).

Experiment Setup. We use the MNIST handwritten digits data set containing 55000 training
images  5000 validation images  and 10000 test images  each image being a 28 ⇥ 28 pixel grayscale
image. The intensities of these 576 pixels (ranging from 0 to 1) are used as input to a neural network
that has 1024 nodes in its one hidden layer. The output layer uses the softmax function to give a
distribution over digits 0 to 9. The activation function is ReLU and the network is trained using
Gradient Descent with learning parameter 0.5 through 500 iterations of mini-batches of size 100.
In general  the corruptions can be any black-box corruption of the image. In our experiments  we
consider four types of corruption (m = 4). See the full version of the paper for further details about
corruptions.

Baselines. We consider three baselines: (i) Individual Corruption: for each corruption type i 2 [m] 
we construct an oracle that trains a neural network using the training data perturbed by corruption i 
and then returns the trained network weights as ✓t  for every t = 1  . . .   T . This gives m baselines 
one for each corruption type; (ii) Even Split: this baseline alternates between training with different
corruption types between iterations. In particular  call the previous m baseline oracles O1  ...  Om.
Then this new baseline oracle will produce ✓t with Oi+1  where i ⌘ t mod m  for every t = 1  ...  T ;
(iii) Uniform Distribution: This more advanced baseline runs the robust optimization scheme with the
Hybrid Method (see Appendix)  but without the distribution updates. Instead  the distribution over
corruption types is ﬁxed as the discrete uniform [ 1
m ] over all T iterations. This allows us to
check if the multiplicative weight updates in the robust optimization algorithm are providing beneﬁt.

m   ...  1

Results. The Hybrid and Composite Methods produce results far superior to all three baseline
types  with differences both substantial in magnitude and statistically signiﬁcant  as shown in Figure
2. The more sophisticated Composite Method outperforms the Hybrid Method.
Increasing T
improves performance  but with diminishing returns–largely because for sufﬁciently large T   the
distribution over corruption types has moved from the initial uniform distribution to some more
optimal stable distribution (see the full version for details). All these effects are consistent across
the 4 different corruption sets tested. The Ensemble Bottleneck Loss is empirically much smaller
than Individual Bottleneck Loss. For the best performing algorithm  the Composite Method  the
mean Ensemble Bottleneck Loss (mean Individual Bottleneck Loss) with T = 50 was 0.34 (1.31)
for Background Set  0.28 (1.30) for Shrink Set  0.19 (1.25) for Pixel Set  and 0.33 (1.25) for Mixed
Set. Thus combining the T classiﬁers obtained from robust optimization is practical for making
predictions on new data.

3.2 Robust Inﬂuence Maximization

We apply the results of Section 2.3 to the robust inﬂuence maximization problem. Given a directed
graph G = (V  E)  the goal is to pick a seed set S of k nodes that maximize an inﬂuence function

7

Figure 2: Comparison of methods  showing mean of 10 independent runs and a 95% conﬁdence band. The
criterion is Individual Bottleneck Loss: min[m] E✓⇠P [`(h(z; ✓)  y)]  where P is uniform over all solutions ✓i
for that method. Baselines (i) and (ii) are not shown as they produce signiﬁcantly higher loss.

fG(S)  where fG(S) is the expected number of individuals inﬂuenced by opinion of the members of
S. We used fG(S) to be the number of nodes reachable from S (our results extend to other models).
In robust inﬂuence maximization  the goal is to maximize inﬂuence in the worst-case (Bottleneck
Inﬂuence) over m functions {f1  . . .   fm}  corresponding to m graphs {G1  . . .   Gm}  for some ﬁxed
seed set of size k. This is a special case of robust submodular maximization after rescaling to [0  1].

Experiment Setup. Given a base directed graph G(V  E)  we produce m graphs Gi = (V  Ei) by
randomly including each edge e 2 E with some probability p. We consider two base graphs and two
sets of parameters for each: (i) The Wikipedia Vote Graph [14]. In Experiment A  the parameters are
|V | = 7115  |E| = 103689  m = 10  p = 0.01 and k = 10. In Experiment B  change p = 0.015 and
k = 3. (ii) The Complete Directed Graph on |V | = 100 vertices. In Experiment A  the parameters
are m = 50  p = 0.015 and k = 2. In Experiment B  change p = 0.01 and k = 4.

1   . . .   Sg

Baselines. We compared our algorithm (Section 2.3) to three baselines: (i) Uniform over Individual
Greedy Solutions: Apply greedy maximization (Algorithm 2) on each graph separately  to get
solutions {Sg
m}. Return the uniform distribution over these solutions; (ii) Greedy on Uniform
Distribution over Graphs: Return the output of greedy submodular maximization (Algorithm 2)
on the uniform distribution over inﬂuence functions. This can be viewed as maximizing expected
inﬂuence; (iii) Uniform over Greedy Solutions on Multiple Perturbed Distributions: Generate T
distributions {w⇤1  . . .   w⇤T} over the m functions  by randomly perturbing the uniform distribution.
Perturbation magnitudes were chosen s.t. w⇤t has the same expected `1 distance from uniform as the
distribution returned by robust optimization at iteration t.

Results. For both graph experiments  robust optimization outperforms all baselines on Bottleneck
Inﬂuence; the difference is statistically signiﬁcant as well as large in magnitude for all T > 50 (see
Figure 3). Moreover  the individual seed sets generated at each iteration t of robust optimization
themselves achieve empirically good inﬂuence as well; see the full version for further details.

References
[1] Zeyuan Allen Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
Proceedings of the 33nd International Conference on Machine Learning  ICML 2016  New York
City  NY  USA  June 19-24  2016  pages 699–707  2016.

8

Figure 3: Comparison for various T   showing mean Bottleneck Inﬂuence and 95% conﬁdence on 10 runs.

[2] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a

meta-algorithm and applications. Theory of Computing  8(6):121–164  2012.

[3] Aharon Ben-Tal  Elad Hazan  Tomer Koren  and Shie Mannor. Oracle-based robust optimization

via online learning. Operations Research  63(3):628–638  2015.

[4] Sabyasachi Chatterjee  John C. Duchi  John D. Lafferty  and Yuancheng Zhu. Local minimax
complexity of stochastic convex optimization. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016  December
5-10  2016  Barcelona  Spain  pages 3423–3431  2016.

[5] Wei Chen  Tian Lin  Zihan Tan  Mingfei Zhao  and Xuren Zhou. Robust inﬂuence maximization.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  San Francisco  CA  USA  August 13-17  2016  pages 795–804  2016.

[6] Wei Chen  Tian Lin  Zihan Tan  Mingfei Zhao  and Xuren Zhou. Robust inﬂuence maximization.
In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  San Francisco  CA  USA  August 13-17  2016  pages 795–804  2016.

[7] Elad Hazan  Kﬁr Y. Levy  and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015  December 7-12  2015  Montreal  Quebec 
Canada  pages 1594–1602  2015.

[8] Elad Hazan  Kﬁr Yehuda Levy  and Shai Shalev-Shwartz. On graduated optimization for
stochastic non-convex problems. In Proceedings of the 33nd International Conference on
Machine Learning  ICML 2016  New York City  NY  USA  June 19-24  2016  pages 1833–1841 
2016.

[9] Xinran He and David Kempe. Robust inﬂuence maximization. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  San Francisco 
CA  USA  August 13-17  2016  pages 885–894  2016.

[10] Xinran He and David Kempe. Robust inﬂuence maximization. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  San Francisco 
CA  USA  August 13-17  2016  pages 885–894  2016.

[11] David Kempe  Jon Kleinberg  and Éva Tardos. Maximizing the spread of inﬂuence through
a social network. In Proceedings of the Ninth ACM SIGKDD International Conference on

9

Knowledge Discovery and Data Mining  KDD ’03  pages 137–146  New York  NY  USA  2003.
ACM.

[12] Andreas Krause  H. Brendan McMahan  Carlos Guestrin  and Anupam Gupta. Selecting obser-
vations against adversarial objectives. In Advances in Neural Information Processing Systems
20  Proceedings of the Twenty-First Annual Conference on Neural Information Processing
Systems  Vancouver  British Columbia  Canada  December 3-6  2007  pages 777–784  2007.

[13] Andreas Krause  Alex Roper  and Daniel Golovin. Randomized sensing in adversarial environ-
ments. In Proceedings of the 22nd International Joint Conference On Artiﬁcial Intelligence 
Barcelona  Catalonia  Spain  July 16-22  2011  pages 2133–2139  2011.

[14] Jure Leskovec. Wikipedia vote network. Stanford Network Analysis Project.
[15] Meghna Lowalekar  Pradeep Varakantham  and Akshat Kumar. Robust inﬂuence maximization:
(extended abstract). In Proceedings of the 2016 International Conference on Autonomous
Agents & Multiagent Systems  Singapore  May 9-13  2016  pages 1395–1396  2016.

[16] Yishay Mansour  Aviad Rubinstein  and Moshe Tennenholtz. Robust probabilistic inference. In
Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA
2015  San Diego  CA  USA  January 4-6  2015  pages 449–460  2015.

[17] Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally
robust optimization with f-divergences. In Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016  December 5-10  2016 
Barcelona  Spain  pages 2208–2216  2016.

[18] G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a

submodular set function. Mathematics of Operations Research  3(3):177–188  1978.

[19] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions—i. Mathematical Programming  14(1):265–294  1978.

[20] Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why. In
Proceedings of the 33nd International Conference on Machine Learning  ICML 2016  New York
City  NY  USA  June 19-24  2016  pages 793–801  2016.

[21] Jacob Steinhardt and John C. Duchi. Minimax rates for memory-bounded sparse linear regres-
sion. In Proceedings of The 28th Conference on Learning Theory  COLT 2015  Paris  France 
July 3-6  2015  pages 1564–1587  2015.

10

,Gao Huang
Chuan Guo
Matt Kusner
Yu Sun
Fei Sha
Kilian Weinberger
Robert Chen
Brendan Lucier
Yaron Singer
Vasilis Syrgkanis
Zhengyuan Zhou
Panayotis Mertikopoulos
Susan Athey
Peter Glynn