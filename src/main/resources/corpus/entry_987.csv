2017,Collaborative Deep Learning in Fixed Topology Networks,There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server  aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context  this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant  CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST  CIFAR-10 and CIFAR-100.,Collaborative Deep Learning in

Fixed Topology Networks

Zhanhong Jiang1  Aditya Balu1  Chinmay Hegde2  and Soumik Sarkar1

1Department of Mechanical Engineering  Iowa State University 

zhjiang  baditya  soumiks@iastate.edu

2Department of Electrical and Computer Engineering   Iowa State University  chinmay@iastate.edu

Abstract

There is signiﬁcant recent interest to parallelize deep learning algorithms in order
to handle the enormous growth in data and model sizes. While most advances
focus on model parallelization and engaging multiple computing agents via using
a central parameter server  aspect of data parallelization along with decentralized
computation has not been explored sufﬁciently. In this context  this paper presents
a new consensus-based distributed SGD (CDSGD) (and its momentum variant 
CDMSGD) algorithm for collaborative deep learning over ﬁxed topology networks
that enables data parallelization as well as decentralized computation. Such a frame-
work can be extremely useful for learning agents with access to only local/private
data in a communication constrained environment. We analyze the convergence
properties of the proposed algorithm with strongly convex and nonconvex objective
functions with ﬁxed and diminishing step sizes using concepts of Lyapunov func-
tion construction. We demonstrate the efﬁcacy of our algorithms in comparison
with the baseline centralized SGD and the recently proposed federated averaging
algorithm (that also enables data parallelism) based on benchmark datasets such as
MNIST  CIFAR-10 and CIFAR-100.

1

Introduction

In this paper  we address the scalability of optimization algorithms for deep learning in a distributed
setting. Scaling up deep learning [1] is becoming increasingly crucial for large-scale applications
where the sizes of both the available data as well as the models are massive [2]. Among various
algorithmic advances  many recent attempts have been made to parallelize stochastic gradient descent
(SGD) based learning schemes across multiple computing agents. An early approach called Downpour
SGD [3]  developed within Google’s disbelief software framework  primarily focuses on model
parallelization (i.e.  splitting the model across the agents). A different approach known as elastic
averaging SGD (EASGD) [4] attempts to improve perform multiple SGDs in parallel; this method
uses a central parameter server that helps in assimilating parameter updates from the computing
agents. However  none of the above approaches concretely address the issue of data parallelization 
which is an important issue for several learning scenarios: for example  data parallelization enables
privacy-preserving learning in scenarios such as distributed learning with a network of mobile and
Internet-of-Things (IoT) devices. A recent scheme called Federated Averaging SGD [5] attempts
such a data parallelization in the context of deep learning with signiﬁcant success; however  they still
use a central parameter server.
In contrast  deep learning with decentralized computation can be achieved via gossip SGD algo-
rithms [6  7]  where agents communicate probabilistically without the aid of a parameter server.
However  decentralized computation in the sense of gossip SGD is not feasible in many real life
applications. For instance  consider a large (wide-area) sensor network [8  9] or multi-agent robotic

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Method
SGD

f

Str-con

Downpour SGD [3] Nonconvex

EASGD [4]

Gossip SGD [7]

FedAvg [5]

CDSGD [This paper]

Str-con
Str-con Lip.&Bou.
Str-con Lip.&Bou.

Nonconvex

Lip.

Str-con Lip.&Bou.
Str-con Lip.&Bou.
Nonconvex Lip.&Bou.
Nonconvex Lip.&Bou.

Table 1: Comparisons between different optimization approaches

∇f
Lip.
Lip.
Lip.

Con.

Con.&Ada. N/A

Step Size Con.Rate D.P. D.C. C.C.T.
O(γk) No No No
Yes No No
O(γk) No No No
O(γk) No Yes No
O( 1
k )
Yes No No
N/A
O(γk)
O( 1
k )
N/A
N/A

Con.
Con.
Dim.
Con.
Con.
Dim.
Con.
Dim.

Yes Yes Yes

Con.Rate: convergence rate  Str-con: strongly convex. Lip.&Bou.: Lipschitz continuous and
bounded. Con.: constant and Con.&Ada.: constant&adagrad. Dim.: diminishing. γ ∈ (0  1) is a
positive constant.  ∈ (0.5  1] is a positive constant. D.P.: data parallelism. D.C.: decentralized
computation. C.C.T.: constrained communication topology.

network that aims to learn a model of the environment in a collaborative manner [10  11]. For such
cases  it may be infeasible for arbitrary pairs of agents to communicate on-demand; typically  agents
are only able to communicate with their respective neighbors in a communication network in a ﬁxed
(or evolving) topology.
Contribution: This paper introduces a new class of approaches for deep learning that enables both
data parallelization and decentralized computation. Speciﬁcally  we propose consensus-based dis-
tributed SGD (CDSGD) and consensus-based distributed momentum SGD (CDMSGD) algorithms for
collaborative deep learning that  for the ﬁrst time  satisﬁes all three requirements: data parallelization 
decentralized computation  and constrained communication over ﬁxed topology networks. Moreover 
while most existing studies solely rely on empirical evidence from simulations  we present rigorous
convergence analysis for both (strongly) convex and non-convex objective functions  with both ﬁxed
and diminishing step sizes using a Lyapunov function construction approach. Our analysis reveals
several advantages of our method: we match the best existing rates of convergence in the centralized
setting  while simultaneously supporting data parallelism as well as constrained communication
topologies; to our knowledge  this is the ﬁrst approach that achieves all three desirable properties; see
Table 1 for a detailed comparison.
Finally  we validate our algorithms’ performance on benchmark datasets  such as MNIST  CIFAR-10 
and CIFAR-100. Apart from centralized SGD as a baseline  we also compare performance with
that of Federated Averaging SGD as it also enables data parallelization. Empirical evidence (for a
given number of agents and other hyperparametric conditions) suggests that while our method is
slightly slower  we can achieve higher accuracy compared to the best available algorithm (Federated
Averaging (FedAvg)). Empirically  the proposed framework in this work is suitable for situations
without central parameter servers  but also robust to a central parameter server failture situation.
Related work: Apart from the algorithms mentioned above  a few other related works exist  including
a distributed system called Adam for large deep neural network (DNN) models [12] and a distributed
methodology by Strom [13] for DNN training by controlling the rate of weight-update to reduce the
amount of communication. Natural Gradient Stochastic Gradient Descent (NG-SGD) based on model
averaging [14] and staleness-aware async-SGD [15] have also been developed for distributed deep
learning. A method called CentralVR [16] was proposed for reducing the variance and conducting
parallel execution with linear convergence rate. Moreover  a decentralized algorithm based on
gossip protocol called the multi-step dual accelerated (MSDA) [17] was developed for solving
deterministically smooth and strongly convex distributed optimization problems in networks with a
provable optimal linear convergence rate. A new class of decentralized primal-dual methods [18]
was also proposed recently in order to improve inter-node communication efﬁciency for distributed
convex optimization problems. To minimize a ﬁnite sum of nonconvex functions over a network  the
authors in [19] proposed a zeroth-order distributed algorithm (ZENITH) that was globally convergent
with a sublinear rate. From the perspective of distributed optimization  the proposed algorithms
have similarities with the approaches of [20  21]. However  we distinguish our work due to the
collaborative learning aspect with data parallelization and extension to the stochastic setting and
nonconvex objective functions. In [20] the authors only considered convex objective functions in a

2

deterministic setting  while the authors in [21] presented results for non-convex optimization problems
in a deterministic setting. Our proof techniques are different from those in [20  21] with the choice
of Lyapunov function  as well as the notion of stochastic Lyapunov gradient. More importantly  we
provide an extensive and thorough suite of numerical comparisons with both centralized methods and
distributed methods on benchmark datasets.
The rest of the paper is organized as follows. While section 2 formulates the distributed  unconstrained
stochastic optimization problem  section 3 presents the CDSGD algorithm and the Lyapunov stochas-
tic gradient required for analysis presented in section 4. Validation experiments and performance
comparison results are described in section 5. The paper is summarized  concluded in section 6
along with future research directions. Detailed proofs of analytical results  extensions (e.g.  effect of
diminishing step size) and additional experiments are included in the supplementary section 7.
2 Formulation

We consider the standard (unconstrained) empirical risk minimization problem typically used in
machine learning problems (such as deep learning):

n(cid:88)

i=1

min 1
n

f i(x) 

(1)

where x ∈ Rd denotes the parameter of interest and f : Rd → R is a given loss function  and f i
is the function value corresponding to a data point i. In this paper  we are interested in learning
problems where the computational agents exhibit data parallelism  i.e.  they only have access to
their own respective training datasets. However  we assume that the agents can communicate over
a static undirected graph G = (V E)  where V is a vertex set (with nodes corresponding to agents)
and E is an edge set. With N agents  we have V = {1  2  ...  N} and E ⊆ V × V. If (j  l) ∈ E 
then Agent j can communicate with Agent l. The neighborhood of agent j ∈ V is deﬁned as:
N b(j) (cid:44) {l ∈ V : (j  l) ∈ E or j = l}. Throughout this paper we assume that the graph G is
connected. Let Dj  j = 1  . . .   n denote the subset of the training data (comprising nj samples)
j=1 nj = n. With this setup  we have the following

corresponding to the jth agents such that(cid:80)N
(cid:88)

simpliﬁcation of Eq. 1:

N(cid:88)

N(cid:88)

(cid:88)

f i
j (x) 

(2)

min 1
n

f i(x) =

N
n

j=1

i∈Dj

j=1

i∈Dj

where  fj(x) = 1

state the optimization problem in a distributed manner  where f (x) =(cid:80)N

N f (x) is the objective function speciﬁc to Agent j. This formulation enables us to
j=1 fj(x). 1 Furthermore 

the problem (1) can be reformulated as

where x := (x1  x2  . . .   xN )T ∈ RN×d and F(x) can be written as

min N
n

1T F(x) :=

N
n

s.t. xj = xl ∀(j  l) ∈ E 

(cid:20)(cid:88)

i∈D1

(cid:88)

i∈D2

F(x) =

f i
1(x1) 

f i
2(x2)  . . .  

N(cid:88)

(cid:88)

j=1

i∈Dj

f i
j (xj)

(cid:21)T

f i
N (xN )

(cid:88)

i∈DN

(3a)

(3b)

(4)

Note that with d > 1  the parameter set x as well as the gradient ∇F(x) correspond to matrix
variables. However  for simplicity in presenting our analysis  we set d = 1 in this paper  which
corresponds to the case where x and ∇F(x) are vectors.

1Note that in our formulation  we are assuming that every agent has the same local objective function while

in general distributed optimization problems they can be different.

3

We now introduce several key deﬁnitions and assumptions that characterize the objective functions
and the agent interaction matrix.
Deﬁnition 1. A function f : Rd → R is H-strongly convex  if for all x  y ∈ Rd  we have f (y) ≥
f (x) + ∇f (x)T (y − x) + H
Deﬁnition 2. A function f : Rd → R is γ-smooth if for all x  y ∈ Rd  we have f (y) ≤ f (x) +
∇f (x)T (y − x) + γ

2 (cid:107)y − x(cid:107)2.

2(cid:107)y − x(cid:107)2.

As a consequence of Deﬁnition 2  we can conclude that ∇f is Lipschitz continuous  i.e.  (cid:107)∇f (y) −
∇f (x)(cid:107) ≤ γ(cid:107)y − x(cid:107) [22].
Deﬁnition 3. A function c is said to be coercive if it satisﬁes: c(x) → ∞ when(cid:107)x(cid:107) → ∞.
Assumption 1. The objective functions fj : Rd → R are assumed to satisfy the following conditions:
a) Each fj is γj-smooth; b) each fj is proper (not everywhere inﬁnite) and coercive; and c) each fj
is Lj-Lipschitz continuous  i.e.  |fj(y) − fj(x)| < Lj(cid:107)y − x(cid:107) ∀x  y ∈ Rd.

As a consequence of Assumption 1  we can conclude that(cid:80)N
(cid:80)N

j=1 fj(xj) is strongly convex with Hm = minjHj.

j=1 fj(xj) possesses Lipschitz continu-
ous gradient with parameter γm := maxjγj. Similarly  each fj is strongly convex with Hj such that

Regarding the communication network  we use Π to denote the agent interaction matrix  where the
element πjl signiﬁes the link weight between agents j and l.
Assumption 2. a) If (j  l) /∈ E  then πjl = 0; b) ΠT = Π; c) null{I − Π} = span{1}; and d)
I (cid:23) Π (cid:31) −I.

The main outcome of Assumption 2 is that the probability transition matrix is doubly stochastic
and that we have λ1(Π) = 1 > λ2(Π) ≥ ··· ≥ λN (Π) ≥ 0  where λz(Π) denotes the z-th largest
eigenvalue of Π.

3 Proposed Algorithm

3.1 Consensus Distributed SGD

For solving stochastic optimization problems  SGD and its variants have been commonly used to
centralized and distributed problem formulations. Therefore  the following algorithm is proposed
based on SGD and the concept of consensus to solve the problem laid out in Eq. 2 

(cid:88)

l∈N b(j)

xj
k+1 =

πjlxl

k − αgj(xj
k)

(5)

k) = 1

b(cid:48)(cid:80)

where N b(j) indicates the neighborhood of agent j  α is the step size  gj(xj
k) is stochastic gradient
of fj at xj
k  which corresponds to a minibatch of sampled data points at the kth epoch. More
k)  where b(cid:48) is the size of the minibatch D(cid:48) randomly selected
formally  gj(xj
from the data subset Dj. While the pseudo-code of CDSGD is shown below in Algorithm 1 
momentum versions of CDSGD based on Polyak momentum [23] and Nesterov momentum [24] are
also presented in the supplementary section 7. In experiments  Nesterov momentum is used as it has
been shown in the traditional SGD implementations that the Nesterov variant outperforms the Polyak
momentum. Note  that mini-batch implementations of these algorithms are straightforward  hence 

q(cid:48)∈D(cid:48) ∇f q(cid:48)

j (xj

4

are not discussed here in detail  and that the convergence analysis of momentum variants is out of
scope in this paper and will be presented in our future work.

: m  α  N

Algorithm 1: CDSGD
Input
Initialize: xj
Distribute the training dataset to N agents.
for each agent do

0  (j = 1  2  . . .   N)

for k = 0 : m do

k+1 =(cid:80)

Randomly shufﬂe the corresponding data subset Dj (without replacement)
wj
xj
k+1 = wj

l∈N b(j) πjlxl
k
k+1 − αgj(xj
k)

end

end

3.2 Tools for convergence analysis
We now analyze the convergence properties of the iterates {xj
k} generated by Algorithm 1. The
following section summarizes some key intermediate concepts required to establish our main results.
First  we construct an appropriate Lyapunov function that will enable us to establish convergence.
Observe that the update law in Alg. 1 can be expressed as:

xk+1 = Πxk − αg(xk) 

(6)

where

g(xk) = [g1(x1

k)g2(x2

k)...gN (xN

k )]T

Denoting wk = Πxk  the update law can be re-written as xk+1 = wk − αg(xk). Moreover 
xk+1 = xk − xk + wk − αg(xk). Rearranging the last equality yields the following relation:

xk+1 = xk − α(g(xk) + α−1(xk − wk)) = xk − α(g(xk) + α−1(I − Π)xk)

(7)
where the last term in Eq. 7 is the Stochastic Lyapunov Gradient. From Eq. 7  we observe that
the “effective" gradient step is given by g(xk) + α−1(I − Π)xk. Rewriting ∇J i(xk) = g(xk) +
α−1(I − Π)xk  the updates of CDSGD can be expressed as:

xk+1 = xk − α∇J i(xk).

The above expression naturally motivates the following Lyapunov function candidate:

(cid:107)x(cid:107)2

1
2α

N
n

I−Π

V (x  α) :=

1T F(x) +

j=1 fj(xj) has a
γm-Lipschitz continuous gradient  ∇V (x) also is a Lipschitz continuous gradient with parameter:

where (cid:107) · (cid:107)I−Π denotes the norm with respect to the PSD matrix I − Π. Since(cid:80)N
Similarly  as(cid:80)N

ˆγ := γm + α−1λmax(I − Π) = γm + α−1(1 − λN (Π)).

j=1 fj(xj) is Hm-strongly convex  then V (x) is strongly convex with parameter:
ˆH := Hm + (2α)−1λmin(I − Π) = Hm + (2α)−1(1 − λ2(Π)).

Based on Deﬁnition 1  V has a unique minimizer  denoted by x∗ with V ∗ = V (x∗). Correspondingly 
using strong convexity of V   we can obtain the relation:

2 ˆH(V (x) − V ∗) ≤ (cid:107)∇V (x)(cid:107)2 for all x ∈ RN .

(10)
From strong convexity and the Lipschitz continuous property of ∇fj  the constants Hm and γm
further satisfy Hm ≤ γm and hence  ˆH ≤ ˆγ.
Next  we introduce two key lemmas that will help establish our main theoretical guarantees. Due to
space limitations  all proofs are deferred to the supplementary material in Section 7.

5

(8)

(9)

Lemma 1. Under Assumptions 1 and 2  the iterates of CDSGD satisfy ∀k ∈ N:

E[V (xk+1)] − V (xk) ≤ −α∇V (xk)T E[∇J i(xk)] +

α2E[(cid:107)∇J i(xk)(cid:107)2]

ˆγ
2

(11)

At a high level  since E[∇J i(xk)] is the unbiased estimate of ∇V (xk)  using the updates ∇J i(xk)
will lead to sufﬁcient decrease in the Lyapunov function. However  unbiasedness is not enough  and
we also need to control higher order moments of ∇J i(xk) to ensure convergence. Speciﬁcally  we
consider the variance of ∇J i(xk):

V ar[∇J i(xk)] := E[(cid:107)∇J i(xk)(cid:107)2] − (cid:107)E[∇J i(xk)](cid:107)2

(12)
To bound the variance of ∇J i(xk)  we use a standard assumption presented in [25] in the context of
(centralized) deep learning. Such an assumption aims at providing an upper bound for the “gradient
noise" caused by the randomness in the minibatch selection at each iteration.
Assumption 3. a) There exist scalars ζ2 ≥ ζ1 > 0 such that ∇V (xk)T E[∇J i(xk)] ≥
ζ1(cid:107)∇V (xk)(cid:107)2 and (cid:107)E[∇J i(xk)](cid:107) ≤ ζ2(cid:107)∇V (xk)(cid:107) for all k ∈ N; b) There exist scalars Q ≥ 0 and
QV ≥ 0 such that V ar[∇J i(xk)] ≤ Q + QV (cid:107)∇V (xk)(cid:107)2 for all k ∈ N.
Remark 1. While Assumption 3(a) guarantees the sufﬁcient descent of V in the direction of
−∇J i(xk)  Assumption 3(b) states that the variance of ∇J i(xk) is bounded above by the sec-
ond moment of ∇V (xk). The constant Q can be considered to represent the second moment of the
“gradient noise" in ∇J i(xk). Therefore  the second moment of ∇J i(xk) can be bounded above as
E[(cid:107)∇J i(xk)(cid:107)2] ≤ Q + Qm(cid:107)∇V (xk)(cid:107)2  where Qm := QV + ζ 2
Lemma 2. Under Assumptions 1  2  and 3  the iterates of CDSGD satisfy ∀k ∈ N:

2 ≥ ζ 2

1 > 0.

E[V (xk+1)] − V (xk) ≤ −(ζ1 − ˆγ
2

αQm)α(cid:107)∇V (xk)(cid:107)2 +

ˆγ
2

α2Q .

(13)

In Lemma 2  the ﬁrst term is strictly negative if the step size satisﬁes the following necessary
condition:

However  in latter analysis  when such a condition is substituted into the convergence analysis  it may
produce a larger upper bound. For obtaining a tight upper bound  we impose a sufﬁcient condition
for the rest of analysis as follows:

As ˆγ is a function of α  the above inequality can be rewritten as 0 < α ≤ ζ1−(1−λN (Π))Qm
4 Main Results
We now present our main theoretical results establishing the convergence of CDSGD. First  we show
that for most generic loss functions (whether convex or not)  CDSGD achieves consensus across
different agents in the graph  provided the step size (which is ﬁxed across iterations) does not exceed
a natural upper bound.
Proposition 1. (Consensus with ﬁxed step size) Under Assumptions 1 and 2  the iterates of CDSGD
(Algorithm 1) satisfy ∀k ∈ N:

γmQm

.

E[(cid:107)xj

k − sk(cid:107)] ≤

αL

where α satisﬁes 0 < α ≤ ζ1−(1−λN (Π))Qm
(deﬁned properly and discussed in Lemma 4 in the supplementary section 7) and sk = 1
N
represents the average parameter estimate.

(16)
and L is an upper bound of E[(cid:107)g(xk)(cid:107)] ∀k ∈ N
j=1 xj

1 − λ2(Π)

(cid:80)N

γmQm

k

The proof of this proposition can be adapted from [26  Lemma 1].
Next  we show that for strongly convex loss functions  CDSGD converges linearly to a neighborhood
of the global optimum.

6

0 < α ≤ 2ζ1
ˆγQm

0 < α ≤ ζ1
ˆγQm

(14)

(15)

Theorem 1. (Convergence of CDSGD with ﬁxed step size  strongly convex case) Under Assump-
tions 1  2 and 3  the iterates of CDSGD satisfy the following inequality ∀k ∈ N:

E[V (xk) − V ∗] ≤ (1 − α ˆHζ1)k−1(V (x1) − V ∗) +

α2ˆγQ

2

(1 − α ˆHζ1)l

k−1(cid:88)

l=0

= (1 − (αHm + 1 − λ2(Π))ζ1)k−1(V (x1) − V ∗)

(17)

(α2γm + α(1 − λN (Π)))Q

+

2

(1 − (αHm + 1 − λ2(Π))ζ1)l

k−1(cid:88)

l=0

.

when the step size satisﬁes 0 < α ≤ ζ1−(1−λN (Π))Qm

γmQm

= (αγm+1−λN (Π))Q
2(Hm+α−1(1−λ2(Π))ζ1

A detailed proof is presented in the supplementary section 7. We observe from Theorem 1 that
the sequence of Lyapunov function values {V (xk)} converges linearly to a neighborhood of the
optimal value  i.e.  limk→∞ E[V (xk) − V ∗] ≤ αˆγQ
. We also observe that
2 ˆHζ1
the term on the right hand side decreases with the spectral gap of the agent interaction matrix Π 
i.e.  1 − λ2(Π)  which suggests an interesting relation between convergence and topology of the
graph. Moreover  we observe that the upper bound is proportional to the step size parameter α  and
smaller step sizes lead to smaller radii of convergence. (However  choosing a very small step-size
may negatively affect the convergence rate of the algorithm). Finally  if the gradient in this context is
not stochastic (i.e.  the parameter Q = 0)  then linear convergence to the optimal value is achieved 
which matches known rates of convergence with (centralized) gradient descent under strong convexity
and smoothness assumptions.
n 1T F(x∗) = V ∗  the sequence of objective
Remark 2. Since E[ N
n 1T F(x∗)] ≤ E[V (xk)−
function values are themselves upper bounded as follows: E[ N
V ∗]. Therefore  using Theorem 1 we can establish analogous convergence rates in terms of the true
objective function values { N
The above convergence result for CDSGD is limited to the case when the objective functions are
strongly convex. However  most practical deep learning systems (such as convolutional neural
network learning) involve optimizing over highly non-convex objective functions  which are much
harder to analyze. Nevertheless  we show that even under such situations  CDSGD exhibits a (weaker)
notion of convergence.
Theorem 2. (Convergence of CDSGD with ﬁxed step size  nonconvex case) Under Assumptions 1  2 
and 3  the iterates of CDSGD satisfy ∀m ∈ N:
(cid:107)∇V (xk)(cid:107)2] ≤ ˆγmαQ

n 1T F(xk)] ≤ E[V (xk)] and N

n 1T F(xk)} as well.

n 1T F(xk)− N

2(V (x1) − Vinf)

m(cid:88)

E[

+

k=1

ζ1

ζ1α

(γmα + 1 − λN (Π))mQ

2(V (x1) − Vinf)

+

.

=

(18)

.

m

ζ1α

γmQm

ζ1
when the step size satisﬁes 0 < α ≤ ζ1−(1−λN (Π))Qm
the quantity E[(cid:80)m
Remark 3. Theorem 2 states that when in the absence of “gradient noise" (i.e.  when Q = 0) 
k=1 (cid:107)∇V (xk)(cid:107)2] remains ﬁnite. Therefore  necessarily {(cid:107)∇V (xk)(cid:107)} → 0 and
(cid:80)m
the estimates approach a stationary point. On the other hand  if the gradient calculations are
stochastic  then a similar claim cannot be made. However  for this case we have the upper bound
k=1 (cid:107)∇V (xk)(cid:107)2] ≤ (γmα+1−λN (Π))Q
limm→∞ E[ 1
. This tells us that while we cannot guarantee
convergence in terms of sequence of objective function values  we can still assert that the average
of the second moment of gradients is strictly bounded from above even for the case of nonconvex
objective functions.
Moreover  the upper bound cannot be solely controlled via the step-size parameter α (which is
different from what is implied in the strongly convex case by Theorem 1). In general  the upper bound
becomes tighter as λN (Π) increases; however  an increase in λN (Π) may result in a commensurate
increase in λ2(Π)  leading to worse connectivity in the graph and adversely affecting consensus
among agents. Again  our upper bounds are reﬂective of interesting tradeoffs between consensus and
convergence in the gradients  and their dependence on graph topology.

ζ1

7

(a)

(b)

Figure 1: Average training (solid lines) and validation (dash lines) accuracy for (a) comparison of
CDSGD with centralized SGD and (b) CDMSGD with Federated average method

The above results are for ﬁxed step size α  and we can prove complementary results for CDSGD even
for the (more prevalent) case of diminishing step size αk. These are presented in the supplementary
material due to space constraints.
5 Experimental Results
This section presents the experimental results using the benchmark image recognition dataset  CIFAR-
10. We use a deep convolutional nerual network (CNN) model (with 2 convolutional layers with 32
ﬁlters each followed by a max pooling layer  then 2 more convolutional layers with 64 ﬁlters each
followed by another max pooling layer and a dense layer with 512 units  ReLU activation is used in
convolutional layers) to validate the proposed algorithm. We use a fully connected topology with 5
agents and uniform agent interaction matrix except mentioned otherwise. A mini-batch size of 128
and a ﬁxed step size of 0.01 are used in these experiments. The experiments are performed using Keras
and TensorFlow [27  28] and the codes will be made publicly available soon. While we included the
training and validation accuracy plots for the different case studies here  the corresponding training
loss plots  results with other becnmark datasets such as MNIST and CIFAR-100 and decaying as well
as different ﬁxed step sizes are presented in the supplementary section 7.
5.1 Performance comparison with benchmark methods
We begin with comparing the accuracy of CDSGD with that of the centralized SGD algorithm
as shown in Fig. 1(a). While the CDSGD convergence rate is signiﬁcantly slower compared to
SGD as expected  it is observed that CDSGD can eventually achieve high accuracy  comparable
with centralized SGD. However  another interesting observation is that the generalization gap (the
difference between training and validation accuracy as deﬁned in [29]) for the proposed CDSGD
algorithm is signiﬁcantly smaller than that of SGD which is an useful property. We also compare both
CDSGD and CDMSGD with the Federated averaging SGD (FedAvg) algorithm which also performs
data parallelization (see Fig. 1(b)). For the sake of comparison  we use same number of agents and
choose E = 1 and C = 1 as the hyperparameters in the FedAvg algorithm as it is close to a fully
connected topology scenario as considered in the CDSGD and CDMSGD experiments. As CDSGD
is signiﬁcantly slow  we mainly compare the CDMSGD with FedAvg which have similar convergence
rates (CDMSGD being slightly slower). The main observation is that CDMSGD performs better
than FedAvg at the steady state and can achieve centralized SGD level performance. It is important
to note that FedAvg does not perform decentralized computation. Essentially it runs a brute force
parameter averaging on a central parameter server at every epoch (i.e.  consensus at every epoch)
and then broadcasts the updated parameters to the agents. Hence  it tends to be slightly faster than
CDMSGD which uses a truly decentralized computation over a network.
5.2 Effect of network size and topology
In this section  we investigate the effects of network size and topology on the performance of the
proposed algorithms. Figure 2(a) shows the change in training performance as the number of agents
grow from 2 to 8 and to 16. Although with increase in number of agents  the convergence rate slows
down  all networks are able to achieve similar accuracy levels. Finally  we investigate the impact of
network sparsity (as quantiﬁed by the second largest eigenvalue) on the learning performance. The
primary observation is convergence of average accuracy value happens faster for sparser networks

8

010002000300040005000Number of epochs0.00.20.40.60.81.0accscifar10 experimentSGDCDSGD02004006008001000Number of epochs0.00.20.40.60.81.0accscifar10 experimentSGDCDSGDCDMSGDFederated Averaging(a)

(b)

Figure 2: Average training (solid lines) and validation (dash lines) accuracy along with accuracy
variance over agents for CDMSGD algorithm with (a) varying network size and (b) varying network
topology

(higher second largest eigenvalue). This is similar to the trend observed for FedAvg algorithm
while reducing the Client fraction (C) which makes the (stochastic) agent interaction matrix sparser.
However  from the plot of the variance of accuracy values over agents (a smooth version using moving
average ﬁlter)  it can be observed that the level of consensus is more stable for denser networks
compared to that for sparser networks. This is also expected as discussed in Proposition 1. Note 
with the availability of a central parameter server (as in federated averaging)  sparser topology may
be useful for a faster convergence  however  consensus (hence  topology density) is critical for a
collaborative learning paradigm with decentralized computation.

6 Conclusion and Future Work

This paper addresses the collaborative deep learning (and many other machine learning) problem
in a completely distributed manner (i.e.  with data parallelism and decentralized computation) over
networks with ﬁxed topology. We establish a consensus based distributed SGD framework and
proposed associated learning algorithms that can prove to be extremely useful in practice. Using a
Lyapunov function construction approach  we show that the proposed CDSGD algorithm can achieve
linear convergence rate with sufﬁciently small ﬁxed step size and sublinear convergence rate with
diminishing step size (see supplementary section 7 for details) for strongly convex and Lipschitz
differentiable objective functions. Moreover  decaying gradients can be observed for the nonconvex
objective functions using CDSGD. Relevant experimental results using benchmark datasets show that
CDSGD can achieve centralized SGD level accuracy with sufﬁcient training epochs while maintaining
a signiﬁcantly low generalization error. The momentum variant of the proposed algorithm  CDMSGD
can outperform recently proposed FedAvg algorithm which also uses data parallelism but does not
perform a decentralized computation  i.e.  uses a central parameter server. The effects of network
size and topology are also explored experimentally which conforms to the analytical understandings.
While current and future research is focusing on extensive testing and validation of the proposed
framework especially for large networks  a few technical research directions include: (i) collaborative
learning with extreme non-IID data; (ii) collaborative learning over directed time-varying graphs; and
(iii) understanding the dependencies between learning rate and consensus.

Acknowledgments

This paper is based upon research partially supported by the USDA-NIFA under Award no. 2017-
67021-25965  the National Science Foundation under Grant No. CNS-1464279 and No. CCF-
1566281. Any opinions  ﬁndings  and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views of the funding agencies.

9

02004006008001000Number of epochs0.00.20.40.60.81.0accscifar10 experiment2 Agents8 Agents16 Agents02004006008001000Number of e ochs0.00.20.40.60.81.0accs0.000.010.020.030.04Variance among agents for accscifar10 ex erimentFully Connected with λ2(()=0S arse To ology with λ2(()=0.54S arse To ology with λ2(()=0.86References
[1] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[2] Suyog Gupta  Wei Zhang  and Josh Milthorpe. Model accuracy and runtime tradeoff in

distributed deep learning. arXiv preprint arXiv:1509.04210  2015.

[3] Jeffrey Dean  Greg Corrado  Rajat Monga  Kai Chen  Matthieu Devin  Mark Mao  Andrew
Senior  Paul Tucker  Ke Yang  Quoc V Le  et al. Large scale distributed deep networks. In
Advances in neural information processing systems  pages 1223–1231  2012.

[4] Sixin Zhang  Anna E Choromanska  and Yann LeCun. Deep learning with elastic averaging

sgd. In Advances in Neural Information Processing Systems  pages 685–693  2015.

[5] H Brendan McMahan  Eider Moore  Daniel Ramage  Seth Hampson  et al. Communication-
efﬁcient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629 
2016.

[6] Michael Blot  David Picard  Matthieu Cord  and Nicolas Thome. Gossip training for deep

learning. arXiv preprint arXiv:1611.09726  2016.

[7] Peter H Jin  Qiaochu Yuan  Forrest Iandola  and Kurt Keutzer. How to scale distributed deep

learning? arXiv preprint arXiv:1611.04581  2016.

[8] Kushal Mukherjee  Asok Ray  Thomas Wettergren  Shalabh Gupta  and Shashi Phoha. Real-time
adaptation of decision thresholds in sensor networks for detection of moving targets. Automatica 
47(1):185 – 191  2011.

[9] Chao Liu  Yongqiang Gong  Simon Laﬂamme  Brent Phares  and Soumik Sarkar. Bridge damage
detection using spatiotemporal patterns extracted from dense sensor network. Measurement
Science and Technology  28(1):014011  2017.

[10] H.-L. Choi and J. P. How. Continuous trajectory planning of mobile sensors for informative

forecasting. Automatica  46(8):1266–1275  2010.

[11] D. K. Jha  P. Chattopadhyay  S. Sarkar  and A. Ray. Path planning in gps-denied environments
with collective intelligence of distributed sensor networks. International Journal of Control  89 
2016.

[12] Trishul M Chilimbi  Yutaka Suzue  Johnson Apacible  and Karthik Kalyanaraman. Project
adam: Building an efﬁcient and scalable deep learning training system. In OSDI  volume 14 
pages 571–582  2014.

[13] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In

INTERSPEECH  volume 7  page 10  2015.

[14] Hang Su and Haoyu Chen. Experiments on parallel training of deep neural network using model

averaging. arXiv preprint arXiv:1507.01239  2015.

[15] Wei Zhang  Suyog Gupta  Xiangru Lian  and Ji Liu. Staleness-aware async-sgd for distributed

deep learning. arXiv preprint arXiv:1511.05950  2015.

[16] Soham De and Tom Goldstein. Efﬁcient distributed sgd with variance reduction. In Data Mining

(ICDM)  2016 IEEE 16th International Conference on  pages 111–120. IEEE  2016.

[17] Kevin Scaman  Francis Bach  Sébastien Bubeck  Yin Tat Lee  and Laurent Massoulié. Optimal
algorithms for smooth and strongly convex distributed optimization in networks. arXiv preprint
arXiv:1702.08704  2017.

[18] Guanghui Lan  Soomin Lee  and Yi Zhou. Communication-efﬁcient algorithms for decentralized

and stochastic optimization. arXiv preprint arXiv:1701.03961  2017.

[19] Davood Hajinezhad  Mingyi Hong  and Alfredo Garcia. Zenith: A zeroth-order distributed

algorithm for multi-agent nonconvex optimization.

10

[20] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent opti-

mization. IEEE Transactions on Automatic Control  54(1):48–61  2009.

[21] Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. arXiv preprint

arXiv:1608.05766  2016.

[22] Angelia Nedi´c and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on
time-varying directed graphs. IEEE Transactions on Automatic Control 61.12  pages 3936–3947 
2016.

[23] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

[24] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87.

Springer Science & Business Media  2013.

[25] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. arXiv preprint arXiv:1606.04838  2016.

[26] Kun Yuan  Qing Ling  and Wotao Yin. On the convergence of decentralized gradient descent.

arXiv preprint arXiv:1310.7063  2013.

[27] François Chollet. Keras. https://github.com/fchollet/keras  2015.

[28] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 
2016.

[29] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. CoRR  abs/1611.03530  2016.

[30] Angelia Nedi´c and Alex Olshevsky. Distributed optimization over time-varying directed graphs.

IEEE Transactions on Automatic Control  60(3):601–615  2015.

[31] S. Ram  A. Nedic  and V. Veeravalli. A new class of distributed optimization algorithms:
application to regression of distributed data. Optimization Methods and Software  27(1):71– 88 
2012.

11

,Zhanhong Jiang
Aditya Balu
Chinmay Hegde
Soumik Sarkar