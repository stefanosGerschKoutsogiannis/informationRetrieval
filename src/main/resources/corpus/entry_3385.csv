2018,Maximum-Entropy Fine Grained Classification,Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes  and often requires expert annotators to collect data. Utilizing this notion of small visual diversity  we revisit Maximum-Entropy learning in the context of fine-grained classification  and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach  and achieve state-of-the-art performance across a variety of classification tasks in FGVC  that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values  amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.,Maximum Entropy Fine-Grained Classiﬁcation

Abhimanyu Dubey Otkrist Gupta Ramesh Raskar Nikhil Naik

{dubeya  otkrist  raskar  naik}@mit.edu

Massachusetts Institute of Technology

Cambridge  MA  USA

Abstract

Fine-Grained Visual Classiﬁcation (FGVC) is an important computer vision prob-
lem that involves small diversity within the different classes  and often requires
expert annotators to collect data. Utilizing this notion of small visual diversity 
we revisit Maximum-Entropy learning in the context of ﬁne-grained classiﬁcation 
and provide a training routine that maximizes the entropy of the output probability
distribution for training convolutional neural networks on FGVC tasks. We provide
a theoretical as well as empirical justiﬁcation of our approach  and achieve state-
of-the-art performance across a variety of classiﬁcation tasks in FGVC  that can
potentially be extended to any ﬁne-tuning task. Our method is robust to different
hyperparameter values  amount of training data and amount of training label noise
and can hence be a valuable tool in many similar problems.

1

Introduction

For ImageNet [7] classiﬁcation and similar large-scale classiﬁcation tasks that span numerous diverse
classes and millions of images  strongly discriminative learning by minimizing the cross-entropy
from the labels improves performance for convolutional neural networks (CNNs). Fine-grained
visual classiﬁcation problems differ from such large-scale classiﬁcation in two ways: (i) the classes
are visually very similar to each other and are harder to distinguish between (see Figure 1a)  and
(ii) there are fewer training samples and therefore the training dataset might not be representative
of the application scenario. Consider a technique that penalizes strongly discriminative learning 
by preventing a CNN from learning a model that memorizes speciﬁc artifacts present in training
images in order to minimize the cross-entropy loss from the training set. This is helpful in ﬁne-
grained classiﬁcation: for instance  if a certain species of bird is mostly photographed against a
different background compared to other species  memorizing the background will lower generalization
performance while lowering training cross-entropy error  since the CNN will associate the background
to the bird itself.
In this paper  we formalize this intuition and revisit the classical Maximum-Entropy regime  based on
the following underlying idea: the entropy of the probability logit vector produced by the CNN is a
measure of the “peakiness” or “conﬁdence” of the CNN. Learning CNN models that have a higher
value of output entropy will reduce the “conﬁdence” of the classiﬁer  leading in better generalization
abilities when training with limited  ﬁne-grained training data. Our contributions can be listed as
follows: (i) we formalize the notion of “ﬁne-grained” vs “large-scale” image classiﬁcation based on a
measure of diversity of the features  (ii) we derive bounds on the (cid:96)2 regularization of classiﬁer weights
based on this diversity and entropy of the classiﬁer  (iii) we provide uniform convergence bounds on
estimating entropy from samples in terms of feature diversity  (iv) we formulate a ﬁne-tuning objective
function that obtains state-of-the-art performance on ﬁve most-commonly used FGVC datasets across
six widely-used CNN architectures  and (v) we analyze the effect of Maximum-Entropy training
over different hyperparameter values  amount of training data  and amount of training label noise to
demonstrate that our method is consistently robust to all the above.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a)

(b)

Figure 1: (a) Samples from the CUB-200-2011 FGVC (top) and ImageNet (bottom) datasets. (b) Plot of top 2
principal components (obtained from ILSVRC-training set on GoogleNet pool5 features) on ImageNet (red) and
CUB-200-2011 (blue) validation sets. CUB-200-2011 data is concentrated with less diversity  as hypothesized.
2 Related Work

Maximum-Entropy Learning: The principle of Maximum-Entropy  proposed by Jaynes [16] is a
classic idea in Bayesian statistics  and states that the probability distribution best representing the
current state of knowledge is the one with the largest entropy  in context of testable information (such
as accuracy). This idea has been explored in different domains of science  from statistical mechan-
ics [1] and Bayesian inference [12] to unsupervised learning [8] and reinforcement learning [29  27].
Regularization methods that penalize minimum entropy predictions have been explored in the context
of semi-supervised learning [11]  and on deterministic entropy annealing [36] for vector quantization.
In the domain of machine learning  the regularization of the entropy of classiﬁer weights has been
used empirically [4  42] and studied theoretically [37  49].
In most treatments of the Maximum-Entropy principle in classiﬁcation  emphasis has been given to
the entropy of the weights of classiﬁers themselves [37]. In our formulation  we focus instead on the
Maximum-Entropy principle applied to the prediction vectors. This formulation has been explored
experimentally in the work of Pereyra et al.[33] for generic image classiﬁcation. Our work builds on
their analysis by providing a theoretical treatment of ﬁne-grained classiﬁcation problems  and justiﬁes
the application of Maximum-Entropy to target scenarios with limited diversity between classes with
limited training data. Additionally  we obtain large improvements in ﬁne-grained classiﬁcation  which
motivates the usage of the Maximum-Entropy training principle in the ﬁne-tuning setting  opening up
this idea to much broader range of applied computer vision problems. We also note the related idea
of label smoothing regularization [41]  which tries to prevent the largest logit from becoming much
larger than the rest and shows improved generalization in large scale image classiﬁcation problems.
Fine-Grained Classiﬁcation: Fine-Grained Visual Classiﬁcation (FGVC) has been an active area
of interest in the computer vision community. Typical ﬁne-grained problems such as differentiating
between animal and plant species  or types of food. Since background context can act as a distraction
in most cases of FGVC  there has been research in improving the attentional and localization
capabilities of CNN-based algorithms. Bilinear pooling [25] is an instrumental method that combines
pairwise local features to improve spatial invariance. This has been extended by Kernel Pooling [6]
that uses higher-order interactions instead of dot products proposed originally  and Compact Bilinear
Pooling [9] that speeds up the bilinear pooling operation. Another approach to localization is the
prediction of an afﬁne transformation of the original image  as proposed by Spatial Transformer
Networks [15]. Part-based Region CNNs [35] use region-wise attention to improve local features.
Leveraging additional information such as pose and regions have also been explored [3  46]  along
with robust image representations such as CNN ﬁlter banks [5]  VLAD [17] and Fisher vectors [34].
Supplementing training data [21] and model averaging [30] have also had signiﬁcant improvements.
The central theme among current approaches is to increase the diversity of relevant features that
are used in classiﬁcation  either by removing irrelevant information (such as background) by better
localization or pooling  or supplementing features with part and pose information  or more training
data. Our method focuses on the classiﬁcation task after obtaining features (and is hence compatible
with existing approaches)  by selecting the classiﬁer that assumes the minimum information about
the task by principle of Maximum-Entropy. This approach is very useful in context of ﬁne-grained
tasks  especially when ﬁne-tuning from ImageNet CNN models that are already over-parameterized.

2

Fine Grained Classification samples (Stanford Dogs) with typically low visual diversityLarge-Scale classification samples (ImageNet LSVRC12) with very high visual diversity3 Method

In the case of Maximum Entropy ﬁne-tuning  we optimize the following objective:

(cid:98)Ex∼D [DKL (¯y(x)||p(y|x; θ)) − γH[p(y|x; θ)]]

θ∗ = arg min

θ

(1)

Where θ represents the model parameters  and is initialized using a pretrained model such as
ImageNet [7] and γ is a hyperparameter. The entropy can be understood as a measure of the
“peakiness” or “indecisiveness” of the classiﬁer in its prediction for the given input. For instance 
if the classiﬁer is strongly conﬁdent in its belief of a particular class k  then all the mass will be
concentrated at class k  giving us an entropy of 0. Conversely  if a classiﬁer is equally confused
between all C classes  we will obtain a value of log(C) of the entropy  which is the maximum value
it can take. In problems such as ﬁne-grained classiﬁcation  where samples that belong to different
classes can be visually very similar  it is a reasonable idea to prevent the classiﬁer from being too
conﬁdent in its outputs (have low entropy)  since the classes themselves are so similar.

3.1 Preliminaries

Consider the multi-class classiﬁcation problem over C classes. The input domain is given by
X ⊂ RZ  with an accompanying probability metric px(·) deﬁned over X . The training data is given
by N i.i.d. samples D = {x1  ...  xN} drawn from X . Each point x ∈ X has an associated label
¯y(x) = [0  ...  1  ...0] ∈ RC. We learn a CNN such that for each point in X   the CNN induces a
conditional probability distribution over the m classes whose mode matches the label ¯y(x).
A CNN architecture consists of a series of convolutional and subsampling layers that culminate in an
activation Φ(·)  which is fed to an C-way classiﬁer with weights w = {w1  ...  wC} such that:

p(yi|x; w  Φ(·)) =

i Φ(x)(cid:1)
exp(cid:0)w(cid:62)
j=1 exp(cid:0)w(cid:62)
j Φ(x)(cid:1)
(cid:80)C

During training  we learn parameters w and feature extractor Φ(·) (collectively referred to as θ) 
by minimizing the expected KL (Kullback-Liebler)-divergence of the CNN conditional probability
distribution from the true label vector over the training set D:

θ∗ = arg min

(3)
During ﬁne-tuning  we learn a feature map Φ(·) from a large training set (such as ImageNet)  discard
the original classiﬁer w (referred now onwards as wS) and learn new weights w on the smaller
dataset (note that the number of classes  and hence the shape of w  may also change for the new task).
The entropy of conditional probability distribution in Equation 2 is given by:

θ

(cid:98)Ex∼D [DKL (¯y(x)||p(y|x; θ))]

p(yi|x; θ) log(p(yi|x; θ))

(4)

H[p(·|x; θ)] (cid:44) − m(cid:88)

i=1

(cid:90)

(2)

(5)

(6)

To minimize the overall entropy of the classiﬁer over a data distribution x ∼ px(·)  we would be
interested in the expected value of the entropy over the distribution:

Similarly  the empirical average of the conditional entropy over the training set D is:

H[p(·|x; θ)]px(x)dx

Ex∼px [H[p(·|x; θ)]] =
(cid:98)Ex∼D[H[p(·|x; θ)]] =

x∼px

1
N

N(cid:88)

i=1

H[p(·|xi; θ)]

To have high training accuracy  we do not need to learn a model that gives zero cross-entropy loss.
Instead  we only require a classiﬁer to output a conditional probability distribution whose arg max
coincides with the correct class. Next  we show that for problems with low diversity  higher validation
accuracy can be obtained with a higher entropy (and higher training cross-entropy). We now formalize
the notion of diversity in feature vectors over a data distribution.

3

3.2 Diversity and Fine-Grained Visual Classiﬁcation
We assume the pretrained n-dimensional feature map Φ(·) to be a multivariate mixture of m Gaussians 
where m is unknown (and may be very large). Using an overall mean subtraction  we can re-center
the Gaussian distribution to be zero-mean. Φ(x) for x ∼ px is then given by:

αiN (µi  Σi)  where x ∼ px  αi > 0 ∀i and Ex∼px[Φ(x)] = 0 

(7)

for class i. The zero-mean implies that ¯µ =(cid:80)m

where Σis are n-dimensional covariance matrices for each class i  and µi is the mean feature vector
i=1 αiµi = 0. For this distribution  the equivalent

i=1

covariance matrix can be given by:

Φ(x) ∼ m(cid:88)

m(cid:88)

m(cid:88)

m(cid:88)

Var[Φ(x)] =

αiΣi +

αi(µi − ¯µ)(µi − ¯µ)(cid:62) =

αi(Σi + µiµ(cid:62)

i ) (cid:44) Σ∗

(8)

i=1

i=1

i=1

Now  the eigenvalues λ1  ...  λn of the overall covariance matrix Σ∗ characterize the variance of the
distribution across n dimensions. Since Σ∗ is positive-deﬁnite  all eigenvalues are positive (this can
be shown using the fact that each covariance matrix is itself positive-deﬁnite  and diag(µiµ(cid:62)
i )k =
i )2 ≥ 0 ∀i  k). Thus  to describe the variance of the feature distribution we deﬁne Diversity.
(µk
Deﬁnition 1. Let the data distribution be px over space X   and feature extractor be given by Φ(·).
Then  the Diversity ν of the features is deﬁned as:

λi  where {λ1  ...  λn} satisfy det(Σ∗ − λiIn) = 0

i=1

This deﬁnition of diversity is consistent with multivariate analysis  and is a common measure of
x (·) denote the data distribution under a
the total variance of a data distribution [18]. Now  let pL
x (·) denote the data distribution
large-scale image classiﬁcation task such as ImageNet  and let pF
under a ﬁne-grained image classiﬁcation task. We can then characterize ﬁne-grained problems as
data distributions pF

x (·) for any feature extractor Φ(·) that have the property:

x ) (cid:28) ν(Φ  pL
x )

ν(Φ  pF

(9)
On plotting pretrained Φ(·) for both the ImageNet validation set and the validation set of CUB-200-
2011 (a ﬁne-grained dataset)  we see that the CUB-200-2011 features are concentrated with a lower
variance compared to the ImageNet training set (see Figure 1b)  consistent with Equation 9. In the
next section  we describe the connections of Maximum-Entropy with model selection in ﬁne-grained
classiﬁcation.

ν(Φ  px) (cid:44) n(cid:88)

3.3 Maximum-Entropy and Model Selection

(cid:80)
j(cid:107)wj(cid:107)2

By the Tikhonov regularization of a linear classiﬁer [10]  we would want to select w such that
2 is small ((cid:96)2 regularization)  to get higher generalization performance. This technique is
also implemented in neural networks trained using stochastic gradient descent (SGD) by the process of
“weight-decay”. Several recent works around obtaining spectrally-normalized risk bounds for neural
networks have demonstrated that the excess risk scales with the Frobenius norm of the weights [31  2].
Our next result provides some insight into how ﬁne-grained problems can potentially limit model
selection  by analysing the best-case generalization gap (difference between training and expected
risk). We use the following result to lower-bound the norm of the weights (cid:107)w(cid:107)2 =
2 in
terms of the expected entropy and the feature diversity:
Theorem 1. Let the ﬁnal layer weights be denoted by w = {w1  ...  wC}  the data distribution be
px over X   and feature extractor be given by Φ(·). For the expected condtional entropy  the following
holds true:

(cid:113)(cid:80)C

i=1(cid:107)wi(cid:107)2

(cid:107)w(cid:107)2 ≥ log(C) − Ex∼px[H[p(·|x; θ)]]

2(cid:112)ν(Φ  px)

4

A full proof of Theorem 1 is included in the supplement. Let us consider the case when ν(Φ  px) is
large (ImageNet classiﬁcation). In this case  this lower bound is very weak and inconsequential.
However  in the case of small ν(Φ  px) (ﬁne-grained classiﬁcation)  the denominator is small  and
this lower bound can subsequently limit the space of model selection  by only allowing models with
large values of weights  leading to a larger best-case generalization gap (that is  when  Theorem 1
holds with equality). We see that if the numerator is small  the diversity of the features has a smaller
impact on limiting the model selection  and hence  it can be advantageous to maximize prediction
entropy. We note that since this is a lower bound  the proof is primarily expository and we can only
comment on best-case generalization performance.

More intuitively  however  it can be understood that problems that are ﬁne-grained will often require
more information to distinguish between classes  and regularizing the prediction entropy prevents
creating models that memorize a lot of information about the training data  and thus can potentially
beneﬁt generalization. In this sense  using a Maximum-Entropy objective function is similar to an
online calibration of neural network predictions [13]  to account for ﬁne-grained problems. Now 
Theorem 1 involves the expected conditional entropy over the data distribution. However  during
training we only have sample access to the data distribution  which we can use as a surrogate. It
is essential to then ensure that the empirical estimate of the conditional entropy (from N training
samples) is an accurate estimate of the true expected conditional entropy. The next result ensures
that for large N  in a ﬁne-grained classiﬁcation problem  the sample estimate of average conditional
entropy is close to the expected conditional entropy.
Theorem 2. Let the ﬁnal layer weights be denoted by w = {w1  ...  wC}  the data distribution
be px over X   and feature extractor be given by Φ(·). With probability at least 1 − δ > 1
2 and
(cid:107)w(cid:107)∞ = max ((cid:107)w1(cid:107)2  ... (cid:107)wC(cid:107)2)  we have:

(cid:12)(cid:12)(cid:12)(cid:98)ED[H[p(·|x; θ)]] − Ex∼px[H[p(·|x; θ)]]

(cid:12)(cid:12)(cid:12) ≤ (cid:107)w(cid:107)∞

(cid:16)(cid:114) 2

) + (cid:101)O(cid:0)N−0.75(cid:1)(cid:17)

ν(Φ  px) log(

4
δ

N

A full proof of Theorem 2 is included in the supplement. We see that as long as the diversity of
features is small  and N is large  our estimate for entropy will be close to the expected value. Using
this result  we can express Theorem 1 in terms of the empirical mean conditional entropy.
Corollary 1. With probability at least 1 − δ > 1

2   the empirical mean conditional entropy follows:

(cid:107)w(cid:107)2 ≥

(cid:0)2 −(cid:113) 2

log(C) −(cid:98)Ex∼D[H[p(·|x; θ)]]

δ )(cid:1)(cid:112)ν(Φ  px) − (cid:101)O(cid:0)N−0.75(cid:1)

N log( 2

A full proof of Corollary 1 is included in the supplement. We see that we recover the result from
Theorem 1 as N → ∞. Corollary 1 shows that as long as the diversity of features is small  and N is
large  the same conclusions drawn from Theorem 1 apply in the case of the empirical mean entropy
as well. We will now proceed to describing the results obtained from maximum-entropy ﬁne-grained
classiﬁcation.

4 Experiments

We perform all experiments using the PyTorch [32] framework over a cluster of NVIDIA Titan X
GPUs. We now describe our results on benchmark datasets in ﬁne-grained recognition and some
ablation studies.

4.1 Fine-Grained Visual Classiﬁcation

Maximum-Entropy training improves performance across ﬁve standard ﬁne-grained datasets  with
substantial gains in low-performing models. We obtain state-of-the-art results on all ﬁve datasets
(Table 1-(A-E)). Since all these datasets are small  we report numbers averaged over 6 trials.
Classiﬁcation Accuracy: First  we observe that Maximum-Entropy training obtains signiﬁcant
performance gains when ﬁne-tuning from models trained on the ImageNet dataset (e.g.  GoogLeNet

5

(A) CUB-200-2011 [44]

(B) Cars [22]

(C) Aircrafts [28]

Method

Top-1 ∆

Method

Top-1 ∆

Method

Prior Work

Prior Work

Prior Work

STN[15]
Zhang et al. [47]
Lin et al. [24]
Cui et al. [6]

84.10
84.50
85.80
86.20

-
-
-
-

Wang et al. [45]
Liu et al. [26]
Lin et al. [24]
Cui et al. [6]

85.70
86.80
92.00
92.40

-
-
-
-

Simon et al. [38]
Cui et al. [6]
LRBP [20]
Lin et al. [24]

Top-1 ∆

85.50
86.90
87.30
88.50

-
-
-
-

Our Results

GoogLeNet
MaxEnt-GoogLeNet
ResNet-50
MaxEnt-ResNet-50
VGGNet16
MaxEnt-VGGNet16
Bilinear CNN [25]
MaxEnt-BilinearCNN 85.27
DenseNet-161
MaxEnt-DenseNet-161 86.54

68.19 (6.18)
74.37
75.15 (5.22)
80.37
73.28 (3.74)
77.02
84.10 (1.17)
84.21 (2.33)

Our Results

GoogLeNet
MaxEnt-GoogLeNet
ResNet-50
MaxEnt-ResNet-50
VGGNet16
MaxEnt-VGGNet16
Bilinear CNN [25]
MaxEnt-Bilinear CNN 92.81
DenseNet-161
MaxEnt-DenseNet-161 93.01

84.85 (2.17)
87.02
91.52 (2.33)
93.85
80.60 (3.28)
83.88
91.20 (1.61)
91.83 (1.18)

Our Results

GoogLeNet
MaxEnt-GoogLeNet
ResNet-50
MaxEnt-ResNet-50
VGGNet16
MaxEnt-VGGNet16
BilinearCNN [25]
MaxEnt-BilinearCNN 86.12
DenseNet-161
MaxEnt-DenseNet-161 89.76

74.04 (5.12)
79.16
81.19 (2.67)
83.86
74.17 (3.91)
78.08
84.10 (2.02)
86.30 (3.46)

(D) NABirds [43]

(E) Stanford Dogs [19]

Prior Work

Our Results

Top-1 ∆

80.43
80.60

-
-

Prior Work

Our Results

Method

Top-1 ∆

Method

Branson et al. [3]
Van et al. [43]

35.70
75.00

-
-

Zhang et al. [48]
Krause et al. [21]

GoogLeNet
MaxEnt-GoogLeNet
ResNet-50
MaxEnt-ResNet-50
VGGNet16
MaxEnt-VGGNet16
BilinearCNN [25]
MaxEnt-BilinearCNN 82.66
DenseNet-161
MaxEnt-DenseNet-161 83.02

70.66 (2.38)
73.04
63.55 (5.66)
69.21
68.34 (4.28)
72.62
80.90 (1.76)
79.35 (3.67)

GoogLeNet
MaxEnt-GoogLeNet
ResNet-50
MaxEnt-ResNet-50
VGGNet16
MaxEnt-VGGNet16
BilinearCNN [25]
MaxEnt-BilinearCNN 83.18
DenseNet-161
MaxEnt-DenseNet-161 83.63

55.76 (6.25)
62.01
69.92 (3.64)
73.56
61.92 (3.52)
65.44
82.13 (1.05)
81.18 (2.45)

Table 1: Maximum-Entropy training (MaxEnt) obtains state-of-the-art performance on ﬁve widely-
used ﬁne-grained visual classiﬁcation datasets (A-E). Improvement over the baseline model is reported
as (∆). All results averaged over 6 trials.

[40]  Resnet-50 [14]). For example  on the CUB-200-2011 dataset  ﬁne-tuning GoogLeNet by
standard ﬁne-tuning gives an accuracy of 68.19%. Fine-tuning with Maximum-Entropy gives an
accuracy of 74.37%—which is a large improvement  and it is persistent across datasets. Since a lot of
ﬁne-tuning tasks use general base models such as GoogLeNet and ResNet  this result is relevant to
the large number of applications that involve ﬁne-tuning on specialized datasets.
Maximum-Entropy classiﬁcation also improves prediction performance for CNN architectures specif-
ically designed for ﬁne-grained visual classiﬁcation. For instance  it improves the performance of the
Bilinear CNN [25] on all 5 datasets and obtains state-of-the-art results  to the best of our knowledge.
The gains are smaller  since these architectures improve diversity in the features by localization  and
hence maximizing entropy is less crucial in this case. However  it is important to note that most
pooling architectures [25] use a large model as a base-model (such as VGGNet [39]) and have an
expensive pooling operation. Thus they are computationally very expensive  and infeasible for tasks
that have resource constraints in terms of data and computation time.
Increase in Generality of Features: We hypothesize that Maximum-Entropy training will encourage
the classiﬁer to reduce the speciﬁcity of the features. To evaluate this hypothesis  we perform the
eigendecomposition of the covariance matrix on the pool5 layer features of GoogLeNet trained on
CUB-200-2011  and analyze the trend of sorted eigenvalues (Figure 2a). We examine the features
from CNNs with (i) no ﬁne-tuning (“Basic”)  (ii) regular ﬁne-tuning  and (iii) ﬁne-tuning with
Maximum-Entropy.
For a feature matrix with large covariance between the features of different classes  we would
expect the ﬁrst few eigenvalues to be large  and the rest to diminish quickly  since fewer orthogonal
components can summarize the data. Conversely  in a completely uncorrelated feature matrix  we
would see a longer tail in the decreasing magnitudes of eigenvalues. Figure 2a shows that for the
Basic features (with no ﬁne-tuning)  there is a fat tail in both training and test sets due to the presence
of a large number of uncorrelated features. After ﬁne-tuning on the training data  we observe a

6

Method
GoogLeNet
MaxEnt + GoogLeNet
DenseNet-121
MaxEnt + DenseNet-121

CIFAR-10

84.16
84.10
92.19
92.22

∆

(-0.06)

(0.03)

CIFAR-100

70.24
73.50
75.01
76.22

∆

(3.26)

(1.21)

Table 2: Maximum Entropy obtains larger gains on the ﬁner CIFAR-100 dataset as compared to
CIFAR-10. Improvement over the baseline model is reported as (∆).

Method
GoogLeNet
MaxEnt + GoogLeNet
ResNet-50
MaxEnt + ResNet-50

Random-ImageNet

71.85
72.20
82.01
82.29

∆

(0.35)

(0.28)

Dogs-ImageNet

62.28
64.91
73.81
75.66

∆

(2.63)

(1.86)

Table 3: Maximum Entropy obtains larger gains on the a subset of ImageNet containing dog
sub-classes versus a randomly chosen subset of the same size which has higher visual diversity.
Improvement over the baseline model (in cross-validation) is reported as (∆).

reduction in the tail of the curve  implying that some generality in features has been introduced in the
model through the ﬁne-tuning. The test curve follows a similar decrease  justifying the increase in
test accuracy. Finally  for Maximum-Entropy  we observe a substantial decrease in the width of the
tail of eigenvalue magnitudes  suggesting a larger increase in generality of features in both training
and test sets  which conﬁrms our hypothesis.
Effect on Prediction Probabilities: For Maximum-Entropy training  the predicted logit vector is
smoother  leading to a higher cross entropy during both training and validation. We observe that the
average value of the logit probability of the top predicted class decreases signiﬁcantly with Maximum-
Entropy  as predicted by the mathematical formulation (for γ = 1). On CUB-200-2011 dataset for
GoogLeNet architecture  with Maximum-Entropy  the mean probability of the top class is 0.34  as
compared to 0.77 without it. Moreover  the tail of probability values is fatter with Maximum-Entropy 
as depicted in Figure 2b.

(a)

(b)

Figure 2: (a) Maximum-Entropy training encourages the network to reduce the speciﬁcity of the features  which
is reﬂected in the longer tail of eigenvalues for the covariance matrix of pool5 GoogLeNet features for both
training and test sets of CUB-200-2011. We plot the value of log(λi) for the ith eigenvalue λi obtained after
decomposition of test set (dashed) and training set (solid) (for γ = 1). (b) For Maximum-Entropy training  the
predicted logit vector is smoother with a fatter tail (GoogleNet on CUB-200-2011).

4.2 Ablation Studies

CIFAR-10 and CIFAR-100: We evaluate Maximum-Entropy on the CIFAR-10 and CIFAR-100
datasets [23]. CIFAR-100 has the same set of images as CIFAR-10 but with ﬁner category distinction
in the labels  with each “superclass” of 20 containing ﬁve ﬁner divisions  and a 100 categories in
total. Therefore  we expect (and observe) that Maximum-Entropy training provides stronger gains on
CIFAR-100 as compared to CIFAR-10 across models (Table 2).

7

2004006008001000i1510505101520log(λi)Train BasicTrain Fine-TunedTrain Fine-Tuned with EntropyTest BasicTest Fine-TunedTest Fine-Tuned with Entropy5101520top-k index0.00.20.40.60.81.0mean logit valueStandard SGDSGD + Maximum-EntropyMethod

VGG-Net16 MaxEnt
LSR
MaxEnt

ResNet-50
LSR
DenseNet-161 MaxEnt
LSR

CUB-200-2011

77.02
70.03
80.37
78.20
86.54
84.86

Cars
83.88
81.45
93.85
92.04
93.01
91.96

Aircrafts NABirds

78.08
75.06
83.86
81.26
89.76
87.05

72.62
69.28
69.21
64.02
83.02
80.11

Stanford Dogs

65.44
63.06
73.56
70.03
83.63
82.98

Table 4: Maximum-Entropy training obtains much large gains on Fine-grained Visual Classiﬁcation
as compared to Label Smoothing Regularization (LSR) [40].

(a)

(b)

(c)

Figure 3: (a) Classiﬁcation performance is robust to the choice of γ over a large region as shown here for
CUB-200-2011 with models VGGNet-16 and BilinearCNN. (b) Maximum-Entropy is more robust to increasing
amounts of label noise (CUB-200-2011 on GoogleNet with γ = 1). (c) Maximum-Entropy obtains higher
validation performance despite higher training cross-entropy loss.

ImageNet Ablation Experiment: To understand the effect of Maximum-Entropy training on datasets
with more samples compared to the small ﬁne-grained datasets  we create two synthetic datasets: (i)
Random-ImageNet  which is formed by selecting 116K images from a random subset of 117 classes
of ImageNet [7]  and (ii) Dogs-ImageNet  which is formed by selecting all classes from ImageNet
that have dogs as labels  which has the same number of images and classes as Random-ImageNet.
Dogs-ImageNet has less diversity compared to Random-ImageNet  and thus we expect the gains from
Maximum-Entropy to be higher. On a 5-way cross-validation on both dataset  we observe higher
gains on the Dogs-ImageNet dataset for two CNN models (Table 3).
Choice of Hyperparameter γ: An integral component of regularization is the choice of weighing
parameter. We ﬁnd that performance is fairly robust to the choice of γ (Figure 3a). Please see
supplement for experiment-wise details.
Robustness to Label Noise: In this experiment  we gradually introduce label noise by randomly
permuting a fraction of labels for increasing fractions of total data. We follow an identical evaluation
protocol as the previous experiment  and observe that Maximum-Entropy is more robust to label
noise (Figure 3b).
Training Cross-Entropy and Validation Accuracy: We expect Maximum-Entropy training to
provide higher accuracy at the cost of higher training cross-entropy. In Figure 3c  we show that
we achieve a higher validation accuracy when training with Maximum-Entropy despite the training
cross-entropy loss converging to a higher value.
Comparison with Label-Smoothing Regularization: Label-Smoothing Regularization [40] penal-
izes the KL-divergence of the classiﬁer logits from the uniform distribution – and is also a method to
prevent peaky distributions. On comparing performance with Label-Smoothing Regularization  we
found that Maximum-Entropy provides much larger gains on ﬁne-grained recognition (see Table 4).

5 Discussion and Conclusion

Many real-world applications of computer vision models involve extensive ﬁne-tuning on small 
relatively imbalanced datasets with much smaller diversity in the training set compared to the large-
scale models they are ﬁne-tuned from  a notable example of which is ﬁne-grained recognition. In
this domain  Maximum-Entropy training provides an easy-to-implement and simple to understand
training schedule that consistently improves performance. There are several extensions  however  that

8

10-410-310-210-1100101102103104105γ0.00.10.20.30.40.50.60.70.8test accuracyVGGNet-16BilinearCNN0.00.10.20.30.40.50.60.70.80.9percentage of label noise0.00.20.40.60.81.0test accuracySGDSGD + Maximum-Entropy050100150200250300training epoch0246810training cross-entropyγ=0γ=0.1γ=1γ=100.00.20.40.60.81.0validation accuracycan be explored: explicitly enforcing a large diversity in the features through a different regularizer
might be an interesting extension to this study  as well as potential extensions to large-scale problems
by tackling clusters of diverse objects separately. We leave these as a future study with our results as
a starting point.
Acknowledgements: We thank Ryan Farrell  Pei Guo  Xavier Boix  Dhaval Adjodah  Spandan
Madan  and Ishaan Grover for their feedback on the project and Google’s TensorFlow Research
Cloud Program for providing TPU computing resources.

References
[1] Sumiyoshi Abe and Yuko Okamoto. Nonextensive statistical mechanics and its applications  volume 560.

Springer Science & Business Media  2001.

[2] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds for neural

networks. In Advances in Neural Information Processing Systems  pages 6240–6249  2017.

[3] Steve Branson  Grant Van Horn  Serge Belongie  and Pietro Perona. Bird species categorization using pose

normalized deep convolutional nets. arXiv preprint arXiv:1406.2952  2014.

[4] Yihua Chen  Eric K Garcia  Maya R Gupta  Ali Rahimi  and Luca Cazzanti. Similarity-based classiﬁcation:

Concepts and algorithms. Journal of Machine Learning Research  10(Mar):747–776  2009.

[5] Mircea Cimpoi  Subhransu Maji  and Andrea Vedaldi. Deep ﬁlter banks for texture recognition and
segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
3828–3836  2015.

[6] Yin Cui  Feng Zhou  Jiang Wang  Xiao Liu  Yuanqing Lin  and Serge Belongie. Kernel pooling for

convolutional neural networks. IEEE Conference on Computer Vision and Pattern Recognition  2017.

[7] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image

Database. In CVPR09  2009.

[8] Mario A. T. Figueiredo and Anil K. Jain. Unsupervised learning of ﬁnite mixture models. IEEE Transac-

tions on pattern analysis and machine intelligence  24(3):381–396  2002.

[9] Yang Gao  Oscar Beijbom  Ning Zhang  and Trevor Darrell. Compact bilinear pooling. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition  pages 317–326  2016.

[10] Gene H Golub  Per Christian Hansen  and Dianne P O’Leary. Tikhonov regularization and total least

squares. SIAM Journal on Matrix Analysis and Applications  21(1):185–194  1999.

[11] Yves Grandvalet and Yoshua Bengio. Entropy regularization.
[12] Stephen F Gull. Bayesian inductive inference and maximum entropy. In Maximum-entropy and Bayesian

methods in science and engineering  pages 53–74. Springer  1988.

[13] Chuan Guo  Geoff Pleiss  Yu Sun  and Kilian Q Weinberger. On calibration of modern neural networks.

arXiv preprint arXiv:1706.04599  2017.

[14] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[15] Max Jaderberg  Karen Simonyan  Andrew Zisserman  and Koray Kavukcuoglu. Spatial transformer

networks. In Advances in Neural Information Processing Systems  pages 2017–2025  2015.

[16] Edwin T Jaynes. Information theory and statistical mechanics. Physical review  106(4):620  1957.
[17] Herve Jegou  Florent Perronnin  Matthijs Douze  Jorge Sánchez  Patrick Perez  and Cordelia Schmid.
Aggregating local image descriptors into compact codes. IEEE transactions on pattern analysis and
machine intelligence  34(9):1704–1716  2012.

[18] Dag Jonsson. Some limit theorems for the eigenvalues of a sample covariance matrix. Journal of

Multivariate Analysis  12(1):1–38  1982.

[19] Aditya Khosla  Nityananda Jayadevaprakash  Bangpeng Yao  and Fei-Fei Li. Novel dataset for ﬁne-grained

image categorization: Stanford dogs.

[20] Shu Kong and Charless Fowlkes. Low-rank bilinear pooling for ﬁne-grained classiﬁcation. IEEE Confer-

ence on Computer Vision and Pattern Recognition  pages 7025–7034  2017.

[21] Jonathan Krause  Benjamin Sapp  Andrew Howard  Howard Zhou  Alexander Toshev  Tom Duerig  James
Philbin  and Li Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-grained recognition. In
European Conference on Computer Vision  pages 301–320. Springer  2016.

9

[22] Jonathan Krause  Michael Stark  Jia Deng  and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops 
pages 554–561  2013.

[23] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. The cifar-10 dataset  2014.
[24] Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with cnns. arXiv preprint arXiv:1707.06772 

2017.

[25] Tsung-Yu Lin  Aruni RoyChowdhury  and Subhransu Maji. Bilinear cnn models for ﬁne-grained visual
recognition. In Proceedings of the IEEE International Conference on Computer Vision  pages 1449–1457 
2015.

[26] Maolin Liu  Chengyue Yu  Hefei Ling  and Jie Lei. Hierarchical joint cnn-based models for ﬁne-grained
cars recognition. In International Conference on Cloud Computing and Security  pages 337–347. Springer 
2016.

[27] Yuping Luo  Chung-Cheng Chiu  Navdeep Jaitly  and Ilya Sutskever. Learning online alignments with

continuous rewards policy gradient. arXiv preprint arXiv:1608.01281  2016.

[28] Subhransu Maji  Esa Rahtu  Juho Kannala  Matthew Blaschko  and Andrea Vedaldi. Fine-grained visual

classiﬁcation of aircraft. arXiv preprint arXiv:1306.5151  2013.

[29] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap  Tim Harley 
David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In
International Conference on Machine Learning  pages 1928–1937  2016.

[30] Mohammad Moghimi  Mohammad Saberian  Jian Yang  Li-Jia Li  Nuno Vasconcelos  and Serge Belongie.
Boosted convolutional neural networks. In British Machine Vision Conference (BMVC)  York  UK  2016.
[31] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nathan Srebro. A pac-bayesian approach

to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564  2017.

[32] Adam Paskze and Soumith Chintala. Tensors and Dynamic neural networks in Python with strong GPU

acceleration. https://github.com/pytorch. Accessed: [January 1  2017].

[33] Gabriel Pereyra  George Tucker  Jan Chorowski  Łukasz Kaiser  and Geoffrey Hinton. Regularizing neural

networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548  2017.

[34] Florent Perronnin  Jorge Sánchez  and Thomas Mensink. Improving the ﬁsher kernel for large-scale image

classiﬁcation. Computer Vision–ECCV 2010  pages 143–156  2010.

[35] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. In Advances in neural information processing systems  pages 91–99  2015.
[36] Kenneth Rose. Deterministic annealing for clustering  compression  classiﬁcation  regression  and related

optimization problems. Proceedings of the IEEE  86(11):2210–2239  1998.

[37] John Shawe-Taylor and David Hardoon. Pac-bayes analysis of maximum entropy classiﬁcation. In Artiﬁcial

Intelligence and Statistics  pages 480–487  2009.

[38] Marcel Simon  Erik Rodner  Yang Gao  Trevor Darrell  and Joachim Denzler. Generalized orderless

pooling performs implicit salient matching. arXiv preprint arXiv:1705.00487  2017.

[39] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556  2014.

[40] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru
Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 1–9  2015.

[41] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 2818–2826  2016.

[42] Martin Szummer and Tommi Jaakkola. Partially labeled classiﬁcation with markov random walks. In

Advances in neural information processing systems  pages 945–952  2002.

[43] Grant Van Horn  Steve Branson  Ryan Farrell  Scott Haber  Jessie Barry  Panos Ipeirotis  Pietro Perona 
and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The
ﬁne print in ﬁne-grained dataset collection. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 595–604  2015.

[44] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The caltech-ucsd

birds-200-2011 dataset. 2011.

[45] Yaming Wang  Jonghyun Choi  Vlad Morariu  and Larry S. Davis. Mining discriminative triplets of patches
for ﬁne-grained classiﬁcation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  June 2016.

10

[46] Ning Zhang  Ryan Farrell  and Trever Darrell. Pose pooling kernels for sub-category recognition. In
Computer Vision and Pattern Recognition (CVPR)  2012 IEEE Conference on  pages 3665–3672. IEEE 
2012.

[47] Xiaopeng Zhang  Hongkai Xiong  Wengang Zhou  Weiyao Lin  and Qi Tian. Picking deep ﬁlter responses
for ﬁne-grained image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 1134–1142  2016.

[48] Yu Zhang  Xiu-Shen Wei  Jianxin Wu  Jianfei Cai  Jiangbo Lu  Viet-Anh Nguyen  and Minh N Do. Weakly
supervised ﬁne-grained categorization with part-based image representation. IEEE Transactions on Image
Processing  25(4):1713–1725  2016.

[49] Jun Zhu and Eric P Xing. Maximum entropy discrimination markov networks. Journal of Machine

Learning Research  10(Nov):2531–2569  2009.

11

,Rahul Krishnan
Simon Lacoste-Julien
David Sontag
Yu-Chuan Su
Kristen Grauman
Abhimanyu Dubey
Otkrist Gupta
Ramesh Raskar
Nikhil Naik