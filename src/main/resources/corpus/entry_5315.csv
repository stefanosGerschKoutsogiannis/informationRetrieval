2009,Structured output regression for detection with partial truncation,We develop a structured output model for object category detection that explicitly accounts for alignment  multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling  and both training and inference are computationally efficient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert (ECCV 2008) to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing  right facing  front) with the same formulation; and (iv)  most significantly for performance  that truncated and truncated instances can be included in both training and inference with an explicit truncation mask. We demonstrate the method by training and testing on the PASCAL VOC 2007 data set -- training includes the truncated examples  and in testing object instances are detected at multiple scales  alignments  and with significant truncations.,Structured output regression for detection with

partial truncation

Andrea Vedaldi

Andrew Zisserman

Department of Engineering

University of Oxford

{vedaldi az}@robots.ox.ac.uk

Oxford  UK

Abstract

We develop a structured output model for object category detection that explicitly
accounts for alignment  multiple aspects and partial truncation in both training and
inference. The model is formulated as large margin learning with latent variables
and slack rescaling  and both training and inference are computationally efﬁcient.
We make the following contributions: (i) we note that extending the Structured
Output Regression formulation of Blaschko and Lampert [1] to include a bias term
signiﬁcantly improves performance; (ii) that alignment (to account for small rota-
tions and anisotropic scalings) can be included as a latent variable and efﬁciently
determined and implemented; (iii) that the latent variable extends to multiple as-
pects (e.g.
left facing  right facing  front) with the same formulation; and (iv) 
most signiﬁcantly for performance  that truncated and truncated instances can be
included in both training and inference with an explicit truncation mask.
We demonstrate the method by training and testing on the PASCAL VOC 2007
data set – training includes the truncated examples  and in testing object instances
are detected at multiple scales  alignments  and with signiﬁcant truncations.

1 Introduction

There has been a steady increase in the performance of object category detection as measured by the
annual PASCAL VOC challenges [3]. The training data provided for these challenges speciﬁes if an
object is truncated – when the provided axis aligned bounding box does not cover the full extent of
the object. The principal cause of truncation is that the object partially lies outside the image area.
Most participants simple disregard the truncated training instances and learn from the non-truncated
ones. This is a waste of training material  but more seriously many truncated instances are missed
in testing  signiﬁcantly reducing the recall and hence decreasing overall recognition performance.
In this paper we develop a model (Fig. 1) which explicitly accounts for truncation in both train-
ing and testing  and demonstrate that this leads to a signiﬁcant performance boost. The model is
speciﬁed as a joint kernel and learnt using an extension of the structural SVM with latent variables
framework of [13]. We use this approach as it allows us to address a second deﬁciency of the pro-
vided supervision – that the annotation is limited to axis aligned bounding boxes  even though the
objects may be in plane rotated so that the box is a loose bound. The latent variables allow us to
specify a pose transformation for each instances so that we achieve a spatial correspondence be-
tween all instances with the same aspect. We show the beneﬁts of this for both the learnt model and
testing performance.
Our model is complementary to that of Felzenszwalb et al. [4] who propose a latent SVM frame-
work  where the latent variables specify sub-part locations. The parts give their model some toler-
ance to in plane rotation and foreshortening (though an axis aligned rectangle is still used for a ﬁrst

1

(a)

(b)

(c)

(d)

Figure 1: Model overview. Detection examples on the VOC images for
the bicycle class demonstrate that the model can handle severe trunca-
tions (a-b)  multiple objects (c)  multiple aspects (d)  and pose variations
(small in-plane rotations) (e). Truncations caused by the image bound-
ary  (a) & (b)  are a signiﬁcant problem for template based detectors 
since the template can then only partially align with the image. Small
in-plane rotations and anisotropic rescalings of the template are approxi-
mated efﬁciently by rearranging sub-blocks of the HOG template (white
boxes in (e)).

(e)

stage as a “root ﬁlter”) but they do not address the problem of truncation. Like them we base our
implementation on the efﬁcient and successful HOG descriptor of Dalal and Triggs [2].
Previous authors have also considered occlusion (of which truncation is a special case). Williams et
al. [11] used pixel wise binary latent variables to specify the occlusion and an Ising prior for spatial
coherence. Inference involved marginalizing out the latent variables using a mean ﬁeld approxima-
tion. There was no learning of the model from occluded data. For faces with partial occlusion  the
resulting classiﬁer showed an improvement over a classiﬁer which did not model occlusion. Others
have explicitly included occlusion at the model learning stage  such as the Constellation model of
Fergus et al. [5] and the Layout Consistent Random Field model of Winn et al. [12]. There are nu-
merous papers on detecting faces with various degrees of partial occlusion from glasses  or synthetic
truncations [6  7].
Our contribution is to deﬁne an appropriate joint kernel and loss function to be used in the context
of structured output prediction. We then learn a structured regressor  mapping an image to a list
of objects with their pose (or bounding box)  while at the same time handling explicitly truncation
and multiple aspects. Our choice of kernel is inspired by the restriction kernel of [1]; however  our
kernel performs both restriction and alignment to a template  supports multiple templates to handle
different aspects and truncations  and adds a bias term which signiﬁcantly improves performance.
We reﬁne pose beyond translation and scaling with an additional transformation selected from a
ﬁnite set of possible perturbations covering aspect ratio change and small in plane rotations. Instead
of explicitly transforming the image with each element of this set (which would be prohibitively ex-
pensive) we introduce a novel approximation based on decomposing the HOG descriptor into small
blocks and quickly rearranging those. To handle occlusions we selectively switch between an object
descriptor and an occlusion descriptor. To identify which portions of the template are occluded we
use a ﬁeld of binary variables. These could be treated as latent variables; however  since we consider
here only occlusions caused by the image boundaries  we can infer them deterministically from the
position of the object relative to the image boundaries. Fig. 1 illustrates various detection examples
including truncation  multiple instances and aspects  and in-plane rotation.
In training we improve the ground-truth pose parameters  since the bounding boxes and aspect asso-
ciations provided in PASCAL VOC are quite coarse indicators of the object pose. For each instance
we add a latent variable which encodes a pose adjustment. Such variables are then treated as part of
learning using the technique presented in [13]. However  while there the authors use the CCCP algo-
rithm to treat the case of margin rescaling  here we show that a similar algorithm applies to the case
of slack rescaling. The resulting optimization alternates between optimizing the model parameters
given the latent variables (a convex problem solved by a cutting plane algorithm) and optimizing the
latent variable given the model (akin to inference).

2

RIGHTLEFTLEFTLEFTLEFTLEFTLEFTLEFTRIGHTRIGHTThe overall method is computationally efﬁcient both in training and testing  achieves very good
performances  and it is able to learn and recognise truncated objects.

2 Model
Our purpose is to learn a function y = f(x)  x ∈ X   y ∈ Y which  given an image x  returns the
poses y of the objects portrayed in the image. We use the structured prediction learning framework
of [9  13]  which considers along with the input and output variables x and y  an auxiliary latent
variable h ∈ H as well (we use h to specify a reﬁnement to the ground-truth pose parameters). The
function f is then deﬁned as f(x; w) = ˆyx(w) where

(ˆyx(w)  ˆhx(w)) = argmax
(y h)∈Y×H

F (x  y  h; w)  F (x  y  h; w) = (cid:104)w  Ψ(x  y  h)(cid:105) 

(1)

and Ψ(x  y  h) ∈ Rd is a joint feature map. This maximization estimates both y and h from the
data x and corresponds to performing inference. Given training data (x1  y1)  . . .   (xN   yN )  the
parameters w are learned by minimizing the regularized empirical risk

N(cid:88)

i=1

1
2

(cid:107)w(cid:107)2 + C
N

R(w) =

ˆyi(w) = ˆyxi(w) 

∆(yi  ˆyi(w)  ˆhi(w))  where

ˆhi(w) = ˆhxi(w).
(2)
Here ∆(yi  y  h) ≥ 0 is an appropriate loss function that encodes the cost of an incorrect prediction.
In this section we develop the model Ψ(x  y  h)  or equivalently the joint kernel function
K(x  y  h  x(cid:48)  y(cid:48)  h(cid:48)) = (cid:104)Ψ(x  y  h)  Ψ(x(cid:48)  y(cid:48)  h(cid:48))(cid:105)  in a number of stages. We ﬁrst deﬁne the kernel
for the case of a single unoccluded instance in an image including scale and perturbing transforma-
tions  then generalise this to include truncations and aspects; and ﬁnally to multiple instances. An
appropriate loss function ∆(yi  y  h) is subsequently deﬁned which takes into account the pose of
the object speciﬁed by the latent variables.

2.1 Restriction and alignment kernel
Denote by R a rectangular region of the image x  and by x|R the image cropped to that rectangle.
A restriction kernel [1] is the kernel K((x  R)  (x(cid:48)  R(cid:48))) = Kimage(x|R  x(cid:48)|R) where Kimage is an
appropriate kernel between images. The goal is that the joint kernel should be large when the two
regions have similar appearance.
Our kernel is similar  but captures both the idea of restriction and alignment. Let R0 be a reference
rectangle and denote by R(p) = gpR0 the rectangle obtained from R0 by a geometric transformation
gp : R2 → R2. We call p the pose of the rectangle R(p). Let ¯x be an image deﬁned on the reference
rectangle R0 and let H(¯x) ∈ Rd be a descriptor (e.g. SIFT  HOG  GIST [2]) computed from the
image appearance. Then a natural deﬁnition of the restriction and alignment kernel is

p x).

K((x  p)  (x(cid:48)  p(cid:48))) = Kdescr(H(x; p)  H(x(cid:48); p(cid:48)))

(3)
where Kdescr is an appropriate kernel for descriptors  and H(x; p) is the descriptor computed on the
transformed patch x as H(x; p) = H(g−1
Implementation with HOG descriptors. Our choice of the HOG descriptor puts some limits on
the space of poses p that can be efﬁciently explored. To see this  it is necessary to describe how
HOG descriptors are computed.
The computation starts by decomposing the image x into cells of d × d pixels (here d = 8). The
descriptor of a cell is the nine dimensional histogram of the orientation of the image gradient inside
the cell (appropriately weighed and normalized as in [2]). We obtain the HOG descriptor of a
rectangle of w × h cells by stacking the enclosed cell descriptors (this is a 9 × w × h vector). Thus 
given the cell histograms  we can immediately obtain the HOG descriptors H(x  y) for all the cell-
aligned translations (x  y) of the dw × dh pixels rectangle. To span rectangles of different scales
this construction is simply repeated on the rescaled image g−1
s x  where gs(z) = γsz is a rescaling 
γ > 0  and s is a discrete scale parameter.

3

To further reﬁne pose beyond scale and translation  here we consider an additional perturbation gt 
indexed by a pose parameter t  selected in a set of transformations g1  . . .   gT (in the experiments
we use 16 transformations  obtained from all combinations of rotations of ±5 and ±10 degrees and
scaling along x of 95%  90%  80% and 70%). Those could be achieved in the same manner as
scaling by transforming the image g−1
t x for each one  but this would be very expensive (we would
need to recompute the cell descriptors every time). Instead  we approximate such transformations
by rearranging the cells of the template (Fig. 1 and 2). First  we partition the w × h cells of the
template into blocks of m × m cells (e.g. m = 4). Then we transform the center of each block
according to gt and we translate the block to the new center (approximated to units of cells). Notice
that we could pick m = 1 (i.e. move each cell of the template independently)  but we prefer to use
blocks as this accelerates inference (see Sect. 4).
Hence  pose is for us a tuple (x  y  s  t) representing translation  scale  and additional perturbation.
Since HOG descriptors are designed to be compared with a linear kernel  we deﬁne

K((x  p)  (x(cid:48)  p(cid:48))) = (cid:104)Ψ(x  p)  Ψ(x(cid:48)  p(cid:48))(cid:105) 

Ψ(x  p) = H(x; p).

(4)

2.2 Modeling truncations

If part of the object is occluded (either by clutter or by the image boundaries)  some of the descriptor
cells will be either unpredictable or undeﬁned. We explicitly deal with occlusion at the granularity
of the HOG cells by adding a ﬁeld of w × h binary indicator variables v ∈ {0  1}wh. Here vj = 1
means that the j-th cell of the HOG descriptor H(x  p) should be considered to be visible  and
vj = 0 that it is occluded. We thus deﬁne a variant of (4) by considering the feature map

(cid:21)

(cid:20)

Ψ(x  p  v) =

(v ⊗ 19) (cid:12) H(x  p)

((1wh − v) ⊗ 19) (cid:12) H(x  p)

(5)

(6)

where 1d is a d-dimensional vector of all ones  ⊗ denotes the Kroneker product  and (cid:12) the Hadamard
(component wise) product. To understand this expression  recall that H is the stacking of w × h 9-
dimensional histograms  so for instance (v ⊗ 19) (cid:12) H(x  p) preserves the visible cells and nulls the
others. Eq. (5) is effectively deﬁning a template for the object and one for the occlusions.
Notice that v are in general latent variables and should be estimated as such. However here we
note that the most severe and frequent occlusions are caused by the image boundaries (ﬁnite ﬁeld of
view). In this case  which we explore in the experiments  we can write v = v(p) as a function of
the pose p  and remove the explicit dependence on v in Ψ. Moreover the truncated HOG cells are
undeﬁned and can be assigned a nominal common value. So here we work with a simpliﬁed kernel 
in which occlusions are represented by a scalar proportional to the number of truncated cells:

(cid:20)(v(p) ⊗ 19) (cid:12) H(x  p)
(cid:21)

wh − |v(p)|

Ψ(x  p) =

2.3 Modeling aspects

A template model works well as long as pose captures accurately enough the transformations result-
ing from changes in the viewing conditions. In our model  the pose p  combined with the robustness
of the HOG descriptor  can absorb a fair amount of viewpoint induced deformation. It cannot  how-
ever  capture the 3D structure of a physical object. Therefore  extreme changes of viewpoint require
switching between different templates. To this end  we augment pose with an aspect indicator a (so
that pose is the tuple p = (x  y  s  t  a))  which indicates which template to use.
Note that now the concept of pose has been generalized to include a parameter  a  which  differently
from the others  does not specify a geometric transformation. Nevertheless  pose speciﬁes how the
model should be aligned to the image  i.e. by (i) choosing the template that corresponds to the
aspect a  (ii) translating and scaling such template according to (x  y  s)  and (iii) applying to it
the additional perturbation gt. In testing  all such parameters are estimated as part of inference.
In training  they are initialized from the ground truth data annotations (bounding boxes and aspect
labels)  and are then reﬁned by estimating the latent variables (Sect. 2.4).

4

We assign each aspect to a different “slot” of the feature vector Ψ(x  p). Then we null all but the
one of the slots  as indicated by a:

Ψ(x  p) =

(7)

 δa=1Ψ1(x; p)

...

δa=AΨA(x; p)



where Ψa(x; p) is a feature vector in the form of (6). In this way  we compare different templates
for different aspects  as indicated by a.
The model can be extended to capture symmetries of the aspects (resulting from symmetries of the
objects). For instance  a left view of a bicycle can be obtained by mirroring a right view  so that the
same template can be used for both aspects by deﬁning

Ψ(x; p) = δa=leftΨleft(x; p) + δa=right ﬂip Ψright(x; p) 

(8)
where ﬂip is the operator that “ﬂips” the descriptor (this can be deﬁned in general by computing the
descriptor of the mirrored image  but for HOG it reduces to rearranging the descriptor components).
The problem remains of assigning aspects to the training data. In the Pascal VOC data  objects are
labeled with one of ﬁve aspects: front  left  right  back  undeﬁned. However  such assignments may
not be optimal for use in a particular algorithm. Fortunately  our method is able to automatically
reassign aspects as part of the estimation of the hidden variables (Sect. 2.4 and Fig. 2).

2.4 Latent variables

The PASCAL VOC bounding boxes yield only a coarse estimate of the ground truth pose parameters
(e.g. they do not contain any information on the object rotation) and the aspect assignments may
also be suboptimal (see previous section). Therefore  we introduce latent variables h = (δp) that
encode an adjustment to the ground-truth pose parameters y = (p). In practice  the adjustment δp
is a small variation of translation x  y  scale s  and perturbation t  and can switch the aspect a all
together.
We modify the feature maps to account for the adjustment in the obvious way. For instance (6)
becomes

(cid:20)(v(p + δp) ⊗ 19) (cid:12) H(x  p + δp)
(cid:21)

Ψ(x  p  δp) =

wh − |v(p + δp)|

(9)

2.5 Variable number of objects: loss function  bias  training

So far  we have deﬁned the feature map Ψ(x  y) = Ψ(x; p) for the case in which the label y = (p)
contains exactly one object  but an image may contain no or multiple object instances (denoted
respectively y =  and y = (p1  . . .   pn)). We deﬁne the loss function between a ground truth label
yi and the estimated output y as

∆(yi  y) =

1 − overl(B(p)  B(p(cid:48)))
1

if yi = y =  
if yi = (p) and y = (p(cid:48)) 
if yi (cid:54)=  and y =   or yi =  and y (cid:54)=  

(10)

where B is the ground truth bounding box  and B(cid:48) is the prediction (the smallest axis aligned bound-
ing box that contains the warped template gpR0). The overlap score between B and B(cid:48) is given by
overl(B  B(cid:48)) = |B ∩ B(cid:48)|/|B ∪ B(cid:48)|. Note that the ground truth poses are deﬁned so that B(pl)
matches the PASCAL provided bounding boxes [1] (or the manually extended ones for the trun-
cated ones).
The hypothesis y =  (no object) receives score F (x  ; w) = 0 by deﬁning Ψ(x  ) = 0 as in [1].
In this way  the hypothesis y = (p) is preferred to y =  only if F (x  p; w) > F (x  ; w) = 0 
which implicitly sets the detection threshold to zero. However  there is no reason to assume that this
threshold should be appropriate (in Fig. 2 we show that it is not). To learn an arbitrary threshold 
it sufﬁces to append to the feature vector Ψ(x  p) a large constant κbias  so that the score of the
hypothesis y = (p) becomes F (x  (p); w) = (cid:104)w  Ψ(x  p)(cid:105) + κbiaswbias. Note that  since the constant
is large  the weight wbias remains small and has negligible effect on the SVM regularization term.

5

0

to this case by setting F (x  y; w) = (cid:80)L

Finally  an image may contain more than one instance of the object. The model can be extended
l=1 F (x  pl; w) + R(y)  where R(y) encodes a “repul-
sive” force that prevents multiple overlapping detections of the same object. Performing infer-
ence with such a model becomes however combinatorial and in general very difﬁcult. Fortu-
nately  in training the problem can be avoided entirely. If an image contains multiple instances 
the image is added to the training set multiple times  each time “activating” one of the instances 
and “deactivating” the others. Here “deactivating” an instance simply means removing it from
the detector search space. Formally  let p0 be the pose of the active instance and p1  . . .   pN
the poses of the inactive ones. A pose p is removed from the search space if  and only if 
maxi overl(B(p)  B(pi)) ≥ max{overl(B(p)  B(p0))  0.2}.

3 Optimisation

Minimising the regularised risk R(w) as deﬁned by Eq. (2) is difﬁcult as the loss depends on w
through ˆyi(w) and ˆhi(w) (see Eq. (1)). It is however possible to optimise an upper bound (derived
below) given by

1
2

(cid:107)w(cid:107)2 + C
N

max

(y h)∈Y×H

∆(yi  y  h) [1 + (cid:104)w  Ψ(xi  y  h)(cid:105) − (cid:104)w  Ψ(xi  yi  h∗

i (w))(cid:105)] .

(11)

i (w) = argmaxh∈H(cid:104)w  Ψ(xi  yi  h)(cid:105) completes the label (yi  h∗

Here h∗
which only the observed part yi is known from the ground truth).

i (w)) of the sample xi (of

Alternation optimization. Eq. (11) is not a convex energy function due to the dependency of h∗
i (w)
on w. Similarly to [13]  however  it is possible to ﬁnd a local minimum by alternating optimizing w
and estimating h∗
i . To do this  the CCCP algorithm proposed in [13] for the case of margin rescaling 
must be extended to the slack rescaling formulation used here.
Starting from an estimate wt of the solution  deﬁne h∗

(cid:104)w  Ψ(xi  yi  h∗

i (w))(cid:105) = max

it = hi(wt)  so that  for any w 
it)(cid:105)
h(cid:48) (cid:104)w  Ψ(xi  yi  h(cid:48))(cid:105) ≥ (cid:104)w  Ψ(xi  yi  h∗

and the equality holds for w = wt. Hence the energy (11) is bounded by

N(cid:88)

i=1

N(cid:88)

i=1

1
2

(cid:107)w(cid:107)2 + C
N

max

(y h)∈Y×H

∆(yi  y  h) [1 + (cid:104)w  Ψ(xi  y  h)(cid:105) − (cid:104)w  Ψ(xi  yi  h∗

it)(cid:105)]

(12)

and the bound is strict for w = wt. Optimising (12) will therefore result in an improvement of the
energy (11) as well. The latter can be carried out with the cutting-plane technique of [9].

Derivation of the bound (11). The derivation involves a sequence of bounds  starting from
1 + (cid:104)w  Ψ(xi  ˆyi(w)  ˆhi(w))(cid:105) − (cid:104)w  Ψ(xi  yi  h
∆(yi  ˆyi(w)  ˆhi(w)) ≤ ∆(yi  ˆyi(w)  ˆhi(w))
∗

(13)
This bound holds because  by construction  the quantity in the square brackets is not smaller than
one  as can be veriﬁed by substituting the deﬁnitions of ˆyi(w)  ˆhi(w) and h∗
i (w). We further upper
bound the loss by

i (w))(cid:105)i

h

∆(yi  ˆyi(w)  ˆhi(w)) ≤ ∆(yi  y  h) [1 + (cid:104)w  Ψ(xi  y  h)(cid:105) − (cid:104)w  Ψ(xi  yi  h

i (w))(cid:105)]
∗

≤ max
(y h)∈Y×H ∆(yi  y  h) [1 + (cid:104)w  Ψ(xi  y  h)(cid:105) − (cid:104)w  Ψ(xi  yi  h

i (w))(cid:105)]
∗

(14)
Substituting this bound into (2) yields (11). Note that ˆyi(w) and ˆhi(w) are deﬁned as the max-
imiser of (cid:104)w  Ψ(xi  y  h)(cid:105) alone (see Eq. 1)  while the energy maximised in (14) depends on the loss
∆(yi  y  h) as well.

6

˛˛˛y=ˆyi(w) h=ˆhi(w)

(b)

(a)

Figure 2: Effect of different model components. The left panel evaluates the effect of differ-
ent components of the model on the task of learning a detector for the left-right facing PASCAL
VOC 2007 bicycles. In order of increasing AP (see legend): baseline model (see text); bias term
(Sect. 2.5); detecting trunctated instances  training on truncated instances  and counting the trun-
cated cells as a feature (Sect.: 2.2); with searching over small translation  scaling  rotation  skew
(Sect. 2.1). Right panel: (a) Original VOC speciﬁed bounding box and aspect; (b) alignment and as-
pect after pose inference – in addition to translation and scale  our templates are searched over a set
of small perturbations. This is implemented efﬁciently by breaking the template into blocks (dashed
boxes) and rearranging those. Note that blocks can partially overlap to capture foreshortening. The
ground truth pose parameters are approximate because they are obtained from bounding boxes (a).
The algorithm improves their estimate as part of inference of the latent variables h. Notice that not
only translation  scale  and small jitters are re-estimated  but also the aspect subclass can be updated.
In the example  an instance originally labeled as misc (a) is reassigned to the left aspect (b).

4 Experiments

Data. As training data we use the PASCAL VOC annotations. Each object instance is labeled
with a bounding box and a categorical aspect variable (left  right  front  back  undeﬁned). From
the bounding box we estimate translation and scale of the object  and we use aspect to select one
of multiple HOG templates. Symmetric aspects (e.g. left and right) are mapped to the same HOG
template as suggested in Sect. 2.3.
While our model is capable of handling correctly truncations  truncated bounding boxes provide a
poor estimate of the pose of the object pose which prevents using such objects for training. While we
could simply avoid training with truncated boxes (or generate artiﬁcially truncated examples whose
pose would be known)  we prefer exploiting all the available training data. To do this  we manually
augment all truncated PASCAL VOC annotations with an additional “physical” bounding box. The
purpose is to provide a better initial guess for the object pose  which is then reﬁned by optimizing
over the latent variables.
Training and testing speed. Performing inference with the model requires evaluating (cid:104)w  Ψ(x  p)(cid:105)
for all possible poses p. This means matching a HOG template O(W HT A) times  where W ×
H is the dimension of the image in cells  T the number of perturbations (Sect. 2.1)  and A the
number of aspects (Sect. 2.3).1 For a given scale and aspect  matching the template for all locations
reduces to convolution. Moreover  by breaking the template into blocks (Fig. 2) and pre-computing
the convolution with each of those  we can quickly compute perturbations of the template. All in
all  detection requires roughly 30 seconds per image with the full model and four aspects. The
cutting plane algorithm used to minimize (12) requires at each iteration solving problems similar
to inference. This can be easily parallelized  greatly improving training speed. To detect additional
objects at test time we run inference multiple times  but excluding all detections that overlap by
more than 20% with any previously detected object.

1Note that we do not multiply by the number S of scales as at each successive scale W and H are reduced

geometrically.

7

00.20.40.60.8100.10.20.30.40.50.60.70.80.91VOC 2007 left−right bicyclesrecallprecision  baseline 22.9+ bias 33.7+ test w/ trunc. 55.7+ train w/ trunc. 58.6+ empty cells count 60.0+ transformations 63.0MISCLEFTFigure 3: Top row. Examples of detected bicycles. The dashed boxes are bicycles that were detected
with or without truncation support  while the solid ones were detectable only when truncations were
considered explicitly. Bottom row. Some cases of correct detections despite extreme truncation for
the horse class.

Beneﬁt of various model components. Fig. 2 shows how the model improves by the successive
introduction of the various features of the model. The example is carried on the VOC left-right
facing bicycle  but similar effects were observed for other categories. The baseline model uses
only the HOG template without bias  truncations  nor pose reﬁnement (Sect. 2.1). The two most
signiﬁcant improvements are (a) the ability of detecting truncated instances (+22% AP  Fig. 3) and
(b) the addition of the bias (+11% AP). Training with the truncated instances  adding the number
of occluded HOG cells as a feature component  and adding jitters beyond translation and scaling all
yield an improvement of about +2–3% AP.
Full model. The model was trained to detect the class bicycle in the PASCAL VOC 2007 data  using
ﬁve templates  initialized from the PASCAL labeling left  right  front/rear  other. Initially  the pose
reﬁnimenent h is null and the alternation optimization algorithm is iterated ﬁve times to estimate
the model w and reﬁnement h. The detector is then tested on all the test data  enabling multiple
detections per image  and computing average-precision as speciﬁed by [3]. The AP score was 47%.
By comparison  the state of the art for this category [8] achieves 56%. The experiment was repeated
for the class horse  obtaining a score of 40%. By comparison  the state of the art on this category 
our MKL sliding window classiﬁer [10]  achieves 51%. Note that the proposed method uses only
HOG  while the others use a combination of at least two features. However [4]  using only HOG but
a ﬂexible part model  also achieves superior results. Further experiments are needed to evaluate the
combined beneﬁts of truncation/occlusion handling (proposed here)  with multiple features [10] and
ﬂexible parts [4].

Conclusions

We have shown how structured output regression with latent variables provides an integrated and ef-
fective solution to many problems in object detection: truncations  pose variability  multiple objects 
and multiple aspects can all be dealt in a consistent framework.
While we have shown that truncated examples can be used for training  we had to manually extend
the PASCAL VOC annotations for these cases to include rough “physical” bounding boxes (as a hint
for the initial pose parameters). We plan to further extend the approach to infer pose for truncated
examples in a fully automatic fashion (weak supervision).

Acknowledgments. We are grateful for discussions with Matthew Blaschko. Funding was provided
by the EU under ERC grant VisRec no. 228180; the RAEng  Microsoft  and ONR MURI N00014-
07-1-0182.

8

References
[1] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regres-

sion. In Proc. ECCV  2008.

[2] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. CVPR 

2005.

[3] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman.

PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results.
pascal-network.org/challenges/VOC/voc2008/workshop/index.html 
2008.

The
http://www.

[4] P. F. Felzenszwalb  R. B. Grishick  D. McAllister  and D. Ramanan. Object detection with

discriminatively trained part based models. PAMI  2009.

[5] R. Fergus  P. Perona  and A. Zisserman. Object class recognition by unsupervised scale-
invariant learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  volume 2  pages 264–271  June 2003.

[6] K. Hotta. Robust face detection under partial occlusion. In Proceedings of the IEEE Interna-

tional Conference on Image Processing  2004.

[7] Y. Y. Lin  T. L. Liu  and C. S. Fuh. Fast object detection with occlusions. In Proceedings of

the European Conference on Computer Vision  pages 402–413. Springer-Verlag  May 2004.

[8] P. Schnitzspan  M. Fritz  S. Roth  and B. Schiele. Discriminative structure learning of hierar-

chical representations for object detection. In Proc. CVPR  2009.

[9] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for

interdependent and structured output spaces. In Proc. ICML  2004.

[10] A. Vedaldi  V. Gulshan  M. Varma  and A. Zisserman. Multiple kernels for object detection.

In Proc. ICCV  2009.

[11] O. Williams  A. Blake  and R. Cipolla. The variational ising classiﬁer (VIC) algorithm for

coherently contaminated data. In Proc. NIPS  2005.

[12] J. Winn and J. Shotton. The Layout Consistent Random Field for Recognizing and Segmenting

Partially Occluded Objects. In Proc. CVPR  2006.

[13] C.-N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In Proc. ICML 

2009.

9

,Pedro Felzenszwalb
John Oberlin
Lucas Maystre
Matthias Grossglauser
Lei Le
Andrew Patterson
Martha White