2019,An Embedding Framework for Consistent Polyhedral Surrogates,We formalize and study the natural approach of designing convex surrogate loss functions via embeddings for problems such as classification or ranking. In this approach  one embeds each of the finitely many predictions (e.g. classes) as a point in \reals^d  assigns the original loss values to these points  and convexifies the loss in some way to obtain a surrogate.  We prove that this approach is equivalent  in a strong sense  to working with polyhedral (piecewise linear convex) losses.  Moreover  given any polyhedral loss L  we give a construction of a link function through which L is a consistent surrogate for the loss it embeds.  We go on to illustrate the power of this embedding framework with succinct proofs of consistency or inconsistency of various polyhedral surrogates in the literature.,An Embedding Framework for Consistent Polyhedral

Surrogates

Jessie Finocchiaro

jefi8453@colorado.edu

CU Boulder

Rafael Frongillo

raf@colorado.edu

CU Boulder

Bo Waggoner

bwag@colorado.edu

CU Boulder

Abstract

We formalize and study the natural approach of designing convex surrogate loss
functions via embeddings for problems such as classiﬁcation or ranking. In this
approach  one embeds each of the ﬁnitely many predictions (e.g. classes) as a point
in Rd  assigns the original loss values to these points  and convexiﬁes the loss in
some way to obtain a surrogate. We prove that this approach is equivalent  in a
strong sense  to working with polyhedral (piecewise linear convex) losses. More-
over  given any polyhedral loss L  we give a construction of a link function through
which L is a consistent surrogate for the loss it embeds. We go on to illustrate
the power of this embedding framework with succinct proofs of consistency or
inconsistency of various polyhedral surrogates in the literature.

1

Introduction

Convex surrogate losses are a central building block in machine learning for classiﬁcation and
classiﬁcation-like problems. A growing body of work seeks to design and analyze convex surrogates
for given loss functions  and more broadly  understand when such surrogates can and cannot be found.
For example  recent work has developed tools to bound the required number of dimensions of the
surrogate’s hypothesis space [13  24]. Yet in some cases these bounds are far from tight  such as
for abstain loss (classiﬁcation with an abstain option) [4  24  25  33  34]. Furthermore  the kinds of
strategies available for constructing surrogates  and their relative power  are not well-understood.
We augment this literature by studying a particularly natural approach for ﬁnding convex surrogates 
wherein one “embeds” a discrete loss. Speciﬁcally  we say a convex surrogate L embeds a discrete
loss (cid:96) if there is an injective embedding from the discrete reports (predictions) to a vector space
such that (i) the original loss values are recovered  and (ii) a report is (cid:96)-optimal if and only if the
embedded report is L-optimal. If this embedding can be extended to a calibrated link function  which
maps approximately L-optimal reports to (cid:96)-optimal reports  then consistency follows [2]. Common
examples of this general construction include hinge loss as a surrogate for 0-1 loss and the abstain
surrogate mentioned above.
Using tools from property elicitation  we show a tight relationship between such embeddings and
the class of polyhedral (piecewise-linear convex) loss functions. In particular  by focusing on Bayes
risks  we show that every discrete loss is embedded by some polyhedral loss  and every polyhedral
loss function embeds some discrete loss. Moreover  we show that any polyhedral loss gives rise to
a calibrated link function to the loss it embeds  thus giving a very general framework to construct
consistent convex surrogates for arbitrary losses.

Related works. The literature on convex surrogates focuses mainly on smooth surrogate losses [4 
5  7  8  26  30]. Nevertheless  nonsmooth losses  such as the polyhedral losses we consider  have
been proposed and studied for a variety of classiﬁcation-like problems [19  31  32]. A notable
addition to this literature is Ramaswamy et al. [25]  who argue that nonsmooth losses may enable

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

dimension reduction of the prediction space (range of the surrogate hypothesis) relative to smooth
losses  illustrating this conjecture with a surrogate for abstain loss needing only log n dimensions for
n labels  whereas the best known smooth loss needs n − 1. Their surrogate is a natural example of an
embedding (cf. Section 5.1)  and serves as inspiration for our work.
While property elicitation has by now an extensive literature [10  12  15  17  18  22  28  29]  these
works are mostly concerned with point estimation problems. Literature directly connecting property
elicitation to consistency is sparse  with the main reference being Agarwal and Agarwal [2]; note
however that they consider single-valued properties  whereas properties elicited by general convex
losses are necessarily set-valued.

2 Setting

For discrete prediction problems like classiﬁcation  due to hardness of directly optimizing a given
discrete loss  many machine learning algorithms can be thought of as minimizing a surrogate loss
function with better optimization qualities  e.g.  convexity. Of course  to show that this surrogate
loss successfully addresses the original problem  one needs to establish consistency  which depends
crucially on the choice of link function that maps surrogate reports (predictions) to original reports.
After introducing notation  and terminology from property elicitation  we thus give a sufﬁcient
condition for consistency (Def. 4) which depends solely on the conditional distribution over Y.
2.1 Notation and Losses
Let Y be a ﬁnite outcome (label) space  and throughout let n = |Y|. The set of probability distributions
on Y is denoted ∆Y ⊆ RY  represented as vectors of probabilities. We write py for the probability of
outcome y ∈ Y drawn from p ∈ ∆Y. We ﬁrst discuss the conditional setting  with just labels Y and
no features X   and show in § 2.3 how these notions relate to the usual X × Y setting.
We assume that a given discrete prediction problem  such as classiﬁcation  is given in the form of
a discrete loss (cid:96) : R → RY+  which maps a report (prediction) r from a ﬁnite set R to the vector of
loss values (cid:96)(r) = ((cid:96)(r)y)y∈Y for each possible outcome y ∈ Y. We will assume throughout that the
given discrete loss is non-redundant  meaning every report is uniquely optimal (minimizes expected
loss) for some distribution p ∈ ∆Y. Similarly  surrogate losses will be written L : Rd → RY+ 
typically with reports written u ∈ Rd. We write the corresponding expected loss when Y ∼ p as
(cid:104)p  (cid:96)(r)(cid:105) and (cid:104)p  L(u)(cid:105). The Bayes risk of a loss L : Rd → RY+ is the function L : ∆Y → R+ given
by L(p) := inf u∈Rd(cid:104)p  L(u)(cid:105); naturally for discrete losses we write (cid:96) (and the inﬁmum is over R).
For example  0-1 loss is a discrete loss with R = Y = {−1  1} given by (cid:96)0-1(r)y = 1{r (cid:54)= y}  with
Bayes risk (cid:96)0-1(p) = 1 − maxy∈Y py. Two important surrogates for (cid:96)0-1 are hinge loss Lhinge(u)y =
(1 − yu)+  where (x)+ = max(x  0)  and logistic loss L(u)y = log(1 + exp(−yu)) for u ∈ R.
Most of the surrogates L we consider will be polyhedral  meaning piecewise linear and convex;
we therefore brieﬂy recall the relevant deﬁnitions. In Rd  a polyhedral set or polyhedron is the
intersection of a ﬁnite number of closed halfspaces. A polytope is a bounded polyhedral set. A convex
function f : Rd → R is polyhedral if its epigraph is polyhedral  or equivalently  if it can be written as
a pointwise maximum of a ﬁnite set of afﬁne functions [27].
Deﬁnition 1 (Polyhedral loss). A loss L : Rd → RY+ is polyhedral if L(u)y is a polyhedral (convex)
function of u for each y ∈ Y.
For example  hinge loss is polyhedral  whereas logistic loss is not. To motivate our focus on
polyhedral losses  we echo Ramaswamy et al. [25  Section 1.2]  who note that smooth surrogates
often encode much more information than necessary  and in these cases non-smooth surrogates are
the only candidates to achieve a low dimension d above.

2.2 Property Elicitation

To make headway  we will appeal to concepts and results from the property elicitation literature 
which elevates the property  or map from distributions to optimal reports  as a central object to study
in its own right. In our case  this map will often be multivalued  meaning a single distribution could

2

yield multiple optimal reports. (For example  when p = (1/2  1/2)  both r = 1 and r = −1 optimize
0-1 loss.) To this end  we will use double arrow notation to mean a mapping to all nonempty subsets 
⇒ R is shorthand for Γ : ∆Y → 2R \ ∅. See the discussion following Deﬁnition 3
so that γ : ∆Y
for conventions regarding R  Γ  γ  L  (cid:96)  etc.
⇒ R. The level set of Γ for report
Deﬁnition 2 (Property  level set). A property is a function Γ : ∆Y
r is the set Γr := {p : r ∈ Γ(p)}.
Intuitively  Γ(p) is the set of reports which should be optimal for a given distribution p  and Γr is the
set of distributions for which the report r should be optimal. For example  the mode is the property
mode(p) = arg maxy∈Y py  and captures the set of optimal reports for 0-1 loss: for each distribution
over the labels  one should report the most likely label. In this case we say 0-1 loss elicits the mode 
as we formalize below.
Deﬁnition 3 (Elicits). A loss L : R → RY+  elicits a property Γ : ∆Y
r∈R (cid:104)p  L(r)(cid:105) .

∀p ∈ ∆Y   Γ(p) = arg min

⇒ R if

(1)

As Γ is uniquely deﬁned by L  we write prop[L] to refer to the property elicited by a loss L.
For ﬁnite properties (those with |R| < ∞) and discrete losses  we will use lowercase notation γ and
(cid:96)  respectively  with reports r ∈ R; for surrogate properties and losses we use Γ and L  with reports
u ∈ Rd. For general properties and losses  we will also use Γ and L  as above.
2.3 Links and Embeddings

To assess whether a surrogate and link function align with the original loss  we turn to the common
condition of calibration. Roughly  a surrogate and link are calibrated if the best possible expected
loss achieved by linking to an incorrect report is strictly suboptimal.
Deﬁnition 4. Let original loss (cid:96) : R → RY+  proposed surrogate L : Rd → RY+  and link function
ψ : Rd → R be given. We say (L  ψ) is calibrated with respect to (cid:96) if for all p ∈ ∆Y

 

inf

u∈Rd:ψ(u)(cid:54)∈γ(p)(cid:104)p  L(u)(cid:105) > inf

u∈Rd(cid:104)p  L(u)(cid:105) .

(2)

It is well-known that calibration implies consistency  in the following sense (cf. [2]). Given feature
space X   ﬁx a distribution D ∈ ∆(X × Y). Let L∗ be the best possible expected L-loss achieved
by any hypothesis H : X → Rd  and (cid:96)∗ the best expected (cid:96)-loss for any hypothesis h : X → R 
respectively. Then (L  ψ) is consistent if a sequence of surrogate hypotheses H1  H2  . . . whose
L-loss limits to L∗  then the (cid:96)-loss of ψ ◦ H1  ψ ◦ H2  . . . limits to (cid:96)∗. As Deﬁnition 4 does not
involve the feature space X   we will drop it for the remainder of the paper.
Several consistent convex surrogates in the literature can be thought of as “embeddings”  wherein one
maps the discrete reports to a vector space  and ﬁnds a convex loss which agrees with the original
loss. A key condition is that the original reports should be optimal exactly when the corresponding
embedded points are optimal. We formalize this notion as follows.
Deﬁnition 5. A loss L : Rd → RY embeds a loss (cid:96) : R → RY if there exists some injective
embedding ϕ : R → Rd such that (i) for all r ∈ R we have L(ϕ(r)) = (cid:96)(r)  and (ii) for all
p ∈ ∆Y   r ∈ R we have
(3)

r ∈ prop[(cid:96)](p) ⇐⇒ ϕ(r) ∈ prop[L](p) .

Note that it is not clear if embeddings give rise to calibrated links; indeed  apart from mapping the
embedded points back to their original reports via ψ(ϕ(r)) = r  how to map the remaining values is
far from clear. We address the question of when embeddings lead to calibrated links in Section 4.
To illustrate the idea of embedding  let us examine hinge loss in detail as a surrogate for 0-1 loss
for binary classiﬁcation. Recall that we have R = Y = {−1  +1}  with Lhinge(u)y = (1 − uy)+
and (cid:96)0-1(r)y := 1{r (cid:54)= y}  typically with link function ψ(u) = sgn(u). We will see that hinge
loss embeds (2 times) 0-1 loss  via the embedding ϕ(r) = r. For condition (i)  it is straightforward

3

to check that Lhinge(r)y = 2(cid:96)0-1(r)y for all r  y ∈ {−1  1}. For condition (ii)  let us compute the
property each loss elicits  i.e.  the set of optimal reports for each p:



[1 ∞)
1
[−1  1]
−1
(−∞ −1] p1 = 0

p1 = 1
p1 ∈ (1/2  1)
p1 = 1/2
p1 ∈ (0  1/2)

.

1

p1 > 1/2
{−1  1} p1 = 1/2
−1
p1 < 1/2

prop[(cid:96)0-1](p) =

prop[Lhinge](p) =

In particular  we see that −1 ∈ prop[(cid:96)0-1](p) ⇐⇒ p1 ∈ [0  1/2] ⇐⇒ −1 ∈ prop[Lhinge](p) 
and 1 ∈ prop[(cid:96)0-1](p) ⇐⇒ p1 ∈ [1/2  1] ⇐⇒ 1 ∈ prop[Lhinge](p). With both conditions of
Deﬁnition 5 satisﬁed  we conclude that Lhinge embeds 2(cid:96)0-1. In this particular case  it is known
(Lhinge  ψ) is calibrated for ψ(u) = sgn(u); in Section 4 we show that  perhaps surprisingly  all
embeddings lead to calibration with an appropriate link.

3 Embeddings and Polyhedral Losses

(cid:96)(p) = min

In this section  we establish a tight relationship between the technique of embedding and the use of
polyhedral (piecewise-linear convex) surrogate losses. We defer to the following section the question
of when such surrogates are consistent.
To begin  we observe that our embedding condition in Deﬁnition 5 is equivalent to merely matching
Bayes risks. This useful fact will drive many of our results.
Proposition 1. A loss L embeds discrete loss (cid:96) if and only if L = (cid:96).
Proof. Throughout we have L : Rd → RY+  (cid:96) : R → RY+  and deﬁne Γ = prop[L] and γ = prop[(cid:96)].
⇒ U by γ(cid:48) : p (cid:55)→
Suppose L embeds (cid:96) via the embedding ϕ. Letting U := ϕ(R)  deﬁne γ(cid:48) : ∆Y
Γ(p) ∩ U. To see that γ(cid:48)(p) (cid:54)= ∅ for all p ∈ ∆Y  note that by the deﬁnition of γ as the property
elicited by (cid:96) we have some r ∈ γ(p)  and by the embedding condition (3)  ϕ(r) ∈ Γ(p). By [9 
Lemma 3]  we see that L|U (the loss L with reports restricted to U) elicits γ(cid:48) and L = L|U . As
L(ϕ(·)) = (cid:96)(·) by the embedding  we have
r∈R(cid:104)p  (cid:96)(r)(cid:105) = min

r∈R(cid:104)p  L(ϕ(r))(cid:105) = min
for all p ∈ ∆Y. Combining with the above  we now have L = (cid:96).
For the reverse implication  assume that L = (cid:96). In what follows  we implicitly work in the afﬁne hull
of ∆Y  so that interiors are well-deﬁned  and (cid:96) may be differentiable on the (relative) interior of ∆Y.
Since (cid:96) is discrete  −(cid:96) is polyhedral as the pointwise maximum of a ﬁnite set of linear functions. The
projection of its epigraph E(cid:96) onto ∆Y forms a power diagram by [3]  whose cells are full-dimensional
and correspond to the level sets γr of γ = prop[(cid:96)].
For each r ∈ R  let pr be a distribution in the interior of γr  and let ur ∈ Γ(p). Observe that 
by deﬁnition of the Bayes risk and Γ  for all u ∈ Rd the hyperplane v (cid:55)→ (cid:104)v −L(ur)(cid:105) supports
the epigraph EL of −L at the point (p −(cid:104)p  L(u)(cid:105)) if and only if u ∈ Γ(p). Thus  the hyperplane
v (cid:55)→ (cid:104)v −L(ur)(cid:105) supports EL = E(cid:96) at the point (pr −(cid:104)pr  L(ur)(cid:105))  and thus does so at the entire
facet {(p −(cid:104)p  L(ur)(cid:105)) : p ∈ γr}; by the above  ur ∈ Γ(p) for all such distributions as well. We
conclude that ur ∈ Γ(p) ⇐⇒ p ∈ γr ⇐⇒ r ∈ γ(p)  satisfying condition (3) for ϕ : r (cid:55)→ ur. To
see that the loss values match  we merely note that the supporting hyperplanes to the facets of EL
and E(cid:96) are the same  and the loss values are uniquely determined by the supporting hyperplane. (In
particular  if h supports the facet corresponding to γr  we have (cid:96)(r)y = L(ur)y = h(δy)  where δy is
the point distribution on outcome y.)

u∈U(cid:104)p  L(u)(cid:105) = L|U  

From this more succinct embedding condition  we can in turn simplify the condition that a loss
embeds some discrete loss: it does if and only if its Bayes risk is polyhedral. (We say a concave
function is polyhedral if its negation is a polyhedral convex function.) Note that the Bayes risk  a
function from distributions over Y to the reals  may be polyhedral even if the loss itself is not.
Proposition 2. A loss L embeds a discrete loss if and only if L is polyhedral.

4

Proof. If L embeds (cid:96)  Proposition 1 gives us L = (cid:96)  and its proof already argued that (cid:96) is polyhedral.
For the converse  let L be polyhedral; we again examine the proof of Proposition 1. The projection of
L onto ∆Y forms a power diagram by [3] with ﬁnitely many cells C1  . . .   Ck  which we can index by
⇒ R by γr = Cr for r ∈ R  we see that the same
R := {1  . . .   k}. Deﬁning the property γ : ∆Y
construction gives us points ur ∈ Rd such that ur ∈ Γ(p) ⇐⇒ r ∈ γ(p). Deﬁning (cid:96) : R → RY+ by
(cid:96)(r) = L(ur)  the same proof shows that L embeds (cid:96).

Combining Proposition 2 with the observation that polyhedral losses have polyhedral Bayes risks [9 
Lemma 5]  we obtain the ﬁrst direction of our equivalence between polyhedral losses and embedding.
Theorem 1. Every polyhedral loss L embeds a discrete loss.

We now turn to the reverse direction: which discrete losses are embedded by some polyhedral loss?
Perhaps surprisingly  we show that every discrete loss is embeddable  using a construction via convex
conjugate duality which has appeared several times in the literature (e.g. [1  8  11]). Note however
that the number of dimensions d required could be as large as |Y|.
Theorem 2. Every discrete loss (cid:96) is embedded by a polyhedral loss.
Proof. Let n = |Y|  and let C : Rn → R be given by (−(cid:96))∗  the convex conjugate of −(cid:96). From
standard results in convex analysis  C is polyhedral as −(cid:96) is  and C is ﬁnite on all of RY as the
domain of −(cid:96) is bounded [27  Corollary 13.3.1]. Note that −(cid:96) is a closed convex function  as the
inﬁmum of afﬁne functions  and thus (−(cid:96))∗∗ = −(cid:96). Deﬁne L : Rn → RY by L(u) = C(u)1 − u 
where 1 ∈ RY is the all-ones vector. We ﬁrst show that L embeds (cid:96)  and then establish that the range
of L is in fact RY+  as desired.
We compute Bayes risks and apply Proposition 1 to see that L embeds (cid:96). For any p ∈ ∆Y  we have

L(p) = inf

u∈Rn(cid:104)p  C(u)1 − u(cid:105)
C(u) − (cid:104)p  u(cid:105)
= inf
u∈Rn
= − sup
u∈Rn(cid:104)p  u(cid:105) − C(u)
= −C∗(p) = −(−(cid:96)(p))∗∗ = (cid:96)(p) .

It remains to show L(u)y ≥ 0 for all u ∈ Rn  y ∈ Y. Letting δy ∈ ∆Y be the point distribution on
outcome y ∈ Y  we have for all u ∈ Rn  L(u)y ≥ inf u(cid:48)∈Rn L(u(cid:48))y = L(δy) = (cid:96)(δy) ≥ 0  where
the ﬁnal inequality follows from the nonnegativity of (cid:96).

4 Consistency via Calibrated Links

We have now seen the tight relationship between polyhedral losses and embeddings; in particular 
every polyhedral loss embeds some discrete loss. The embedding itself tells us how to link the
embedded points back to the discrete reports (map ϕ(r) to r)  but it is not clear when this link can be
extended to the remaining reports  and whether such a link can lead to consistency. In this section 
we give a construction to generate calibrated links for any polyhedral loss.
The full version [9  Appendix D] contains the full proof; this section provides a sketch along with the
main construction and result. The ﬁrst step is to give a link ψ such that exactly minimizing expected
surrogate loss L  followed by applying ψ  always exactly minimizes expected original loss (cid:96). The
existence of such a link is somewhat subtle  because in general some point u that is far from any
embedding point can minimize expected loss for two very different distributions p  p(cid:48)  making it
unclear whether there exists a choice ψ(u) ∈ R that is (cid:96)-optimal for both distributions. We show that
as we vary p over ∆Y  there are only ﬁnitely many sets of the form U = arg minu∈Rd(cid:104)p  L(u)(cid:105) [9 
Lemma 4]. Associating each U with RU ⊆ R  the set of reports whose embedding points are in
U  we enforce that all points in U link to some report in RU . (As a special case  embedding points
must link to their corresponding reports.) Proving that these choices are well-deﬁned uses a chain
of arguments involving the Bayes risk  ultimately showing that if u lies in multiple such sets U  the
corresponding report sets RU all intersect at some r =: ψ(u).
Intuitively  to ensure calibration  we just need to “thicken” this construction  by mapping all
approximately-optimal points u to optimal reports r. Let U contain all optimal report sets U

5

of the form above. A key step in the following deﬁnition will be to narrow down a “link envelope” Ψ
where Ψ(u) denotes the legal or valid choices for ψ(u).
Deﬁnition 6. Given a polyhedral L that embeds some (cid:96)  an  > 0  and a norm (cid:107) · (cid:107)  the -thickened
link ψ is constructed as follows. First  initialize Ψ : Rd ⇒ R by setting Ψ(u) = R for all u. Then for
each U ∈ U  for all points u such that inf u∗∈U (cid:107)u∗ − u(cid:107) <   update Ψ(u) = Ψ(u) ∩ RU . Finally 
deﬁne ψ(u) ∈ Ψ(u)  breaking ties arbitrarily. If Ψ(u) became empty  then leave ψ(u) undeﬁned.
Theorem 3. Let L be polyhedral  and (cid:96) the discrete loss it embeds from Theorem 1. Then for small
enough  > 0  the -thickened link ψ is well-deﬁned and  furthermore  is a calibrated link from L to (cid:96).

Sketch. Well-deﬁned: For the initial construction above  we argued that if some collection such as
U  U(cid:48)  U(cid:48)(cid:48) overlap at a u  then their report sets RU   RU(cid:48)  RU(cid:48)(cid:48) also overlap  so there is a valid choice
r = ψ(u). Now  we thicken all sets U ∈ U by a small enough ; it can be shown that if the thickened
sets overlap at u  then U  U(cid:48)  U(cid:48)(cid:48) themselves overlap  so again RU   RU(cid:48)  RU(cid:48)(cid:48) overlap and there is a
valid chioce r = ψ(u).
Calibrated: By construction of the thickened link  if u maps to an incorrect report  i.e. ψ(u) (cid:54)∈ γ(p) 
then u must have at least distance  to the optimal set U. We then show that the minimal gradient
of the expected loss along any direction away from U is lower-bounded  giving a constant excess
expected loss at u.

Note that the construction given above in Deﬁnition 6 is not necessarily computationally efﬁcient as
the number of labels n grows. In practice this potential inefﬁciency is not typically a concern  as the
family of losses typically has some closed form expression in terms of n  and thus the construction
can proceed at the symbolic level. We illustrate this formulaic approach in § 5.1.

5 Application to Speciﬁc Surrogates

Our results give a framework to construct consistent surrogates and link functions for any discrete
loss  but they also provide a way to verify the consistency or inconsistency of given surrogates. Below 
we illustrate the power of this framework with speciﬁc examples from the literature  as well as new
examples. In some cases we simplify existing proofs  while in others we give new results  such as a
new calibrated link for abstain loss  and the inconsistency of the recently proposed Lovász hinge.

5.1 Consistency of abstain surrogate and link construction

In classiﬁcation settings with a large number of labels  several authors consider a variant of classiﬁca-
tion  with the addition of a “reject” or abstain option. For example  Ramaswamy et al. [25] study the
loss (cid:96)α : [n] ∪ {⊥} → RY+ deﬁned by (cid:96)α(r)y = 0 if r = y  α if r = ⊥  and 1 otherwise. Here  the
report ⊥ corresponds to “abstaining” if no label is sufﬁciently likely  speciﬁcally  if no y ∈ Y has
py ≥ 1 − α. Ramaswamy et al. [25] provide a polyhedral surrogate for (cid:96)α  which we present here for
α = 1/2. Letting d = (cid:100)log2(n)(cid:101) their surrogate is L1/2 : Rd → RY+ given by

(4)
where B : [n] → {−1  1}d is a arbitrary injection; let us assume n = 2d so that we have a bijection.
Consistency is proven for the following link function 

+  

L1/2(u)y =(cid:0)maxj∈[d] B(y)juj + 1(cid:1)
(cid:26)

ψ(u) =

⊥
B−1(sgn(−u)) otherwise

mini∈[d] |ui| ≤ 1/2

.

(5)

In light of our framework  we can see that L1/2 is an excellent example of an embedding  where
ϕ(y) = B(y) and ϕ(⊥) = 0 ∈ Rd. Moreover  the link function ψ can be recovered from Theorem 3
with norm (cid:107) · (cid:107)∞ and  = 1/2; see Figure 1(L). Hence  our framework would have simpliﬁed the
process of ﬁnding such a link  and the corresponding proof of consistency. To illustrate this point
further  we give an alternate link ψ1 corresponding to (cid:107) · (cid:107)1 and  = 1  shown in Figure 1(R):

(cid:26)

ψ1(u) =

⊥
(cid:107)u(cid:107)1 ≤ 1
B−1(sgn(−u)) otherwise .

6

(6)

Figure 1: Constructing links for the abstain surrogate L1/2 with d = 2. The embedding is shown in bold labeled
by the corresponding reports. (L) The link envelope Ψ resulting from Theorem 3 using (cid:107) · (cid:107)∞ and  = 1/2  and
a possible link ψ which matches eq. (5) from [25]. (M) An illustration of the thickened sets from Deﬁnition 6
for two sets U ∈ U  using (cid:107) · (cid:107)1 and  = 1. (R) The Ψ and ψ from Theorem 3 using (cid:107) · (cid:107)1 and  = 1.

Theorem 3 immediately gives calibration of (L1/2  ψ1) with respect to (cid:96)1/2. Aside from its simplicity 
one possible advantage of ψ1 is that it appears to yield the same constant in generalization bounds as
ψ  yet assigns ⊥ to much less of the surrogate space Rd. It would be interesting to compare the two
links in practice.

5.2

Inconsistency of Lovász hinge

Many structured prediction settings can be thought of as making multiple predictions at once  with
a loss function that jointly measures error based on the relationship between these predictions [14 
16  23]. In the case of k binary predictions  these settings are typically formalized by taking the
predictions and outcomes to be ±1 vectors  so R = Y = {−1  1}k. One then deﬁnes a joint loss
function  which is often merely a function of the set of mispredictions  meaning we may write
(cid:96)g(r)y = g({i ∈ [k] : ri (cid:54)= yi}) for some set function g : 2[k] → R. For example  Hamming loss
is given by g(S) = |S|. In an effort to provide a general convex surrogate for these settings when
g is a submodular function  Yu and Blaschko [32] introduce the Lovász hinge  which leverages the
well-known convex Lovász extension of submodular functions. While the authors provide theoretical
justiﬁcation and experiments  consistency of the Lovász hinge is left open  which we resolve.
Rather than formally deﬁne the Lovász hinge  we defer the complete analysis to the full version of the
paper [9]  and focus here on the k = 2 case. For brevity  we write g∅ := g(∅)  g1 2 := g({1  2})  etc.
Assuming g is normalized and increasing (meaning g1 2 ≥ {g1  g2} ≥ g∅ = 0)  the Lovász hinge
L : Rk → RY+ is given by
Lg(u)y = max

(1 − u1y1)+g1 + (1 − u2y2)+(g1 2 − g1) 

(cid:110)

(7)
(1 − u2y2)+g2 + (1 − u1y1)+(g1 2 − g2)
where (x)+ = max{x  0}. We will explore the range of values of g for which Lg is consistent  where
the link function ψ : R2 → {−1  1}2 is ﬁxed as ψ(u)i = sgn(ui)  with ties broken arbitrarily.
Let us consider the coefﬁcients g∅ = 0  g1 = g2 = g1 2 = 1  for which (cid:96)g is merely 0-1 loss on Y. For
consistency  for any distribution p ∈ ∆Y  we must have that whenever u ∈ arg minu(cid:48)∈R2(cid:104)p  Lg(u(cid:48))(cid:105) 
the outcome ψ(u) must be the most likely  i.e.  in arg maxy∈Y p(y). Simplifying eq. (7)  however 
we have

 

(cid:111)

Lg(u)y = max(cid:8)(1 − u1y1)+  (1 − u2y2)+

(cid:9) = max(cid:8)1 − u1y1  1 − u2y2  0(cid:9)  

(8)
which is exactly the abstain surrogate (4) for d = 2. We immediately conclude that Lg cannot be
consistent with (cid:96)g  as the origin will be the unique optimal report for Lg under distributions with
py < 0.5 for all y  and one can simply take a distribution which disagrees with the way ties are broken
in ψ. For example  if we take sgn(0) = 1  then under p((1  1)) = p((1 −1)) = p((−1  1)) = 0.2
and p((−1 −1)) = 0.4  we have {0} = arg minu∈R2(cid:104)p  Lg(u)(cid:105)  yet we also have ψ(0) = (1  1) /∈
{(−1 −1)} = arg minr∈R(cid:104)p  (cid:96)g(r)(cid:105).

7

u1u2•1•2•3•4•⊥RRRR1 ⊥1 ⊥2 ⊥2 ⊥3 ⊥3 ⊥4 ⊥4 ⊥1 2 ⊥1 4 ⊥2 3 ⊥3 4 ⊥u1u2UU0•1•2•3•4•⊥u1u2•1•2•3•4•⊥Figure 2: Minimizers of (cid:104)p  (cid:96)top-2(cid:105) and (cid:104)p  (cid:96)2(cid:105)  respectively  varying p over ∆3.

In fact  this example is typical: using our embedding framework  and characterizing when 0 ∈ R2
is an embedded point  one can show that Lg is consistent if and only if g1 2 = g1 + g2. Moreover 
in this linear case  which corresponds to g being modular  the Lovász hinge reduces to weighted
Hamming loss  which is trivially consistent from the consistency of hinge loss for 0-1 loss. In the full
version of the paper [9]  we generalize this observation for all k: Lg is consistent if and only if g is
modular. In other words  even for k > 2  the only consistent Lovász hinge is weighted Hamming
loss. These results cast doubt on the effectiveness of the Lovász hinge in practice.

5.3

Inconsistency of top-k losses

In certain classiﬁcation problems when ground truth may be ambiguous  such as object identiﬁcation 
it is common to predict a set of possible labels. As one instance  the top-k classiﬁcation problem
is to predict the set of k most likely labels; formally  we have R := {r ∈ {0  1}n : (cid:107)r(cid:107)0 = k} 
1 < k < n  Y = [n]  and discrete loss (cid:96)top-k(r)y = 1 − ry. Surrogates for this problem commonly
take reports u ∈ Rn  with the link ψ(u) = {u[1]  . . .   u[k]}  where u[i] is the ith largest entry of u.
Lapin et al. [19  20  21] provide the following convex surrogate loss for this problem  which Yang
and Koyejo [31] show to be inconsistent:1

(cid:16)

Lk(u)y :=

(cid:80)k

(cid:17)

1 − uy + 1

k

i=1(u − ey)[i]

 

+

(9)

(cid:0)1 − uy + 1

2 (u[1] + u[2] − min(1  uy))(cid:1)

where ey is 1 in component y and 0 elsewhere. With our framework  we can say more. Speciﬁcally 
while (Lk  ψ) is not consistent for (cid:96)top-k  since Lk is polyhedral  we know from Theorem 1 that it
embeds some discrete loss (cid:96)k  and from Theorem 3 there is a link ψ(cid:48) such that (Lk  ψ(cid:48)) is calibrated
(and consistent) for (cid:96)k. We therefore turn to deriving this discrete loss (cid:96)k.
For concreteness  consider the case with k = 2 over n = 3 outcomes. We can re-write L2(u)y =
. By inspection  we can derive the properties elicited by
(cid:96)top-2 and L2  respectively  which reveals that the set R(cid:48) consisting of all permutations of (1  0  0) 
(1  1  0)  and (2  1  0)  are always represented among the minimizers of L2. Thus  L2 embeds the loss
2(cid:104)r  1 − ey(cid:105) otherwise. Observe that (cid:96)2 is just (cid:96)top-2 with
(cid:96)2(r)y = 0 if ry = 2 or (cid:96)2(r)y = 1 − ry + 1
an extra term punishing weight on elements other than y  and a reward for a weight of 2 on y.
Moreover  we can visually inspect the corresponding properties (Fig. 2) to immediately see why L2
is inconsistent: for distributions where the two least likely labels are roughly equally (un)likely  the
minimizer will put all weight on the most likely label  and thus fail to distinguish the other two. More
generally  L2 cannot be consistent because the property it embeds does not “reﬁne” (subdivide) the
top-k property  so not just ψ  but no link function  could make L2 consistent.

+

1Yang and Koyejo also introduce a consistent surrogate  but it is non-convex.

8

p1p2p3(1 0 1)(1 1 0)(0 1 1)p1p2p3(1 0 1)(2 0 1)(1 0 2)(1 0 0)(0 0 1)(2 1 0)(0 1 2)(1 1 0)(0 1 1)(0 1 0)(0 2 1)(1 2 0)6 Conclusion and Future Directions

This paper formalizes an intuitive way to design convex surrogate losses for classiﬁcation-like
problems—by embedding the reports into Rd. We establish a close relationship between embeddings
and polyhedral surrogates  showing both that every polyhedral loss embeds a discrete loss (Theorem 1)
and that every discrete loss is embedded by some polyhedral loss (Theorem 2). We then construct a
calibrated link function from any polyhedral loss to the discrete loss it embeds  giving consistency for
all such losses (Theorem 3). We conclude with examples of how the embedding framework presented
can be applied to understand existing surrogates in the literature  including those for the abstain loss 
top-k loss  and Lovász hinge. In particular  our link construction recovers the link function proposed
by Ramaswamy et al. [25] for abstain loss  as well as another simpler link based on the L1 norm.
One open question of particular interest involves the dimension of the surrogate prediction space;
given a discrete loss  can we construct a surrogate that embeds it of minimal dimension? If we naïvely
embed the reports into an n-dimensional space  the dimensionality of the problem scales linearly in
the number of possible labels n. As the dimension of the optimization problem is a function of this
embedding dimension d  a promising direction is to leverage tools from elicitation complexity [13  18]
and convex calibration dimension [24] to understand when we can take d << n.

Acknowledgements

We thank Arpit Agarwal and Peter Bartlett for many early discussions  which led to several important
insights. We thank Eric Balkanski for help with a lemma about submodular functions. This material
is based upon work supported by the National Science Foundation under Grant No. 1657598.

9

References
[1] Jacob Abernethy  Yiling Chen  and Jennifer Wortman Vaughan. Efﬁcient market making via
convex optimization  and a connection to online learning. ACM Transactions on Economics
and Computation  1(2):12  2013. URL http://dl.acm.org/citation.cfm?id=2465777.

[2] Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property
elicitation. In JMLR Workshop and Conference Proceedings  volume 40  pages 1–19  2015.
URL http://www.jmlr.org/proceedings/papers/v40/Agarwal15.pdf.

[3] Franz Aurenhammer. Power diagrams: properties  algorithms and applications. SIAM Journal
on Computing  16(1):78–96  1987. URL http://epubs.siam.org/doi/pdf/10.1137/
0216006.

[4] Peter L Bartlett and Marten H Wegkamp. Classiﬁcation with a reject option using a hinge loss.

Journal of Machine Learning Research  9(Aug):1823–1840  2008.

[5] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and
risk bounds. Journal of the American Statistical Association  101(473):138–156  2006. URL
http://amstat.tandfonline.com/doi/abs/10.1198/016214505000000907.

[6] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press  2004.

[7] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-

based vector machines. Journal of machine learning research  2(Dec):265–292  2001.

[8] John Duchi  Khashayar Khosravi  Feng Ruan  et al. Multiclass classiﬁcation  information 

divergence and surrogate risk. The Annals of Statistics  46(6B):3246–3275  2018.

[9] Jessie Finocchiaro  Rafael Frongillo  and Bo Waggoner. An embedding framework for consistent

polyhedral surrogates. In Advances in neural information processing systems  2019.

[10] Tobias Fissler  Johanna F Ziegel  and others. Higher order elicitability and Osband’s principle.

The Annals of Statistics  44(4):1680–1707  2016.

[11] Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis. In

Web and Internet Economics  pages 354–370. Springer  2014.

[12] Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th

Conference on Learning Theory  pages 1–18  2015.

[13] Rafael Frongillo and Ian A. Kash. On Elicitation Complexity. In Advances in Neural Information

Processing Systems 29  2015.

[14] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Proceedings of the

24th annual conference on learning theory  pages 341–358  2011.

[15] T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical

Association  106(494):746–762  2011.

[16] Tamir Hazan  Joseph Keshet  and David A McAllester. Direct loss minimization for structured
prediction. In Advances in Neural Information Processing Systems  pages 1594–1602  2010.

[17] Nicolas S. Lambert. Elicitation and evaluation of statistical forecasts. 2018. URL https:

//web.stanford.edu/~nlambert/papers/elicitability.pdf.

[18] Nicolas S. Lambert  David M. Pennock  and Yoav Shoham. Eliciting properties of probability
distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce  pages
129–138  2008.

[19] Maksim Lapin  Matthias Hein  and Bernt Schiele. Top-k multiclass svm. In Advances in Neural

Information Processing Systems  pages 325–333  2015.

10

[20] Maksim Lapin  Matthias Hein  and Bernt Schiele. Loss functions for top-k error: Analysis and
insights. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 1468–1477  2016.

[21] Maksim Lapin  Matthias Hein  and Bernt Schiele. Analysis and optimization of loss functions
for multiclass  top-k  and multilabel classiﬁcation. IEEE transactions on pattern analysis and
machine intelligence  40(7):1533–1554  2018.

[22] Kent Osband and Stefan Reichelstein. Information-eliciting compensation schemes. Jour-
nal of Public Economics  27(1):107–115  June 1985.
ISSN 0047-2727. doi: 10.1016/
0047-2727(85)90031-3. URL http://www.sciencedirect.com/science/article/pii/
0047272785900313.

[23] Anton Osokin  Francis Bach  and Simon Lacoste-Julien. On structured prediction theory with
calibrated convex surrogate losses. In Advances in Neural Information Processing Systems 
pages 302–313  2017.

[24] Harish G Ramaswamy and Shivani Agarwal. Convex calibration dimension for multiclass loss

matrices. The Journal of Machine Learning Research  17(1):397–441  2016.

[25] Harish G Ramaswamy  Ambuj Tewari  Shivani Agarwal  et al. Consistent algorithms for
multiclass classiﬁcation with an abstain option. Electronic Journal of Statistics  12(1):530–554 
2018.

[26] M.D. Reid and R.C. Williamson. Composite binary losses. The Journal of Machine Learning

Research  9999:2387–2422  2010.

[27] R.T. Rockafellar. Convex analysis  volume 28 of Princeton Mathematics Series. Princeton

University Press  1997.

[28] L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American

Statistical Association  pages 783–801  1971.

[29] Ingo Steinwart  Chloé Pasin  Robert Williamson  and Siyu Zhang. Elicitation and Identiﬁcation
of Properties. In Proceedings of The 27th Conference on Learning Theory  pages 482–526 
2014.

[30] Robert C Williamson  Elodie Vernet  and Mark D Reid. Composite multiclass losses. Journal

of Machine Learning Research  17(223):1–52  2016.

[31] Forest Yang and Sanmi Koyejo. On the consistency of top-k surrogate losses. CoRR 

abs/1901.11141  2019. URL http://arxiv.org/abs/1901.11141.

[32] Jiaqian Yu and Matthew B Blaschko. The lovász hinge: A novel convex surrogate for submodu-

lar losses. IEEE transactions on pattern analysis and machine intelligence  2018.

[33] Ming Yuan and Marten Wegkamp. Classiﬁcation methods with reject option based on convex

risk minimization. Journal of Machine Learning Research  11(Jan):111–130  2010.

[34] Chong Zhang  Wenbo Wang  and Xingye Qiao. On reject and reﬁne options in multicategory
classiﬁcation. Journal of the American Statistical Association  113(522):730–745  2018.
doi: 10.1080/01621459.2017.1282372. URL https://doi.org/10.1080/01621459.2017.
1282372.

11

,Jessica Finocchiaro
Rafael Frongillo
Bo Waggoner