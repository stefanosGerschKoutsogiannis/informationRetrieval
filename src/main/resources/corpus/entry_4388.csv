2018,Predictive Uncertainty Estimation via Prior Networks,Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters  irreducible \emph{data uncertainty} and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently  baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods  however  attempt to model uncertainty due to distributional mismatch either implicitly through \emph{model uncertainty} or as \emph{data uncertainty}. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models \emph{distributional uncertainty}. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets  where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.,Predictive Uncertainty Estimation via Prior Networks

Andrey Malinin

Department of Engineering
University of Cambridge
am969@cam.ac.uk

Mark Gales

Department of Engineering
University of Cambridge
mjfg@eng.cam.ac.uk

Abstract

Estimating how uncertain an AI system is in its predictions is important to improve
the safety of such systems. Uncertainty in predictive can result from uncertainty in
model parameters  irreducible data uncertainty and uncertainty due to distributional
mismatch between the test and training data distributions. Different actions might
be taken depending on the source of the uncertainty so it is important to be able to
distinguish between them. Recently  baseline tasks and metrics have been deﬁned
and several practical methods to estimate uncertainty developed. These methods 
however  attempt to model uncertainty due to distributional mismatch either im-
plicitly through model uncertainty or as data uncertainty. This work proposes a
new framework for modeling predictive uncertainty called Prior Networks (PNs)
which explicitly models distributional uncertainty. PNs do this by parameterizing
a prior distribution over predictive distributions. This work focuses on uncertainty
for classiﬁcation and evaluates PNs on the tasks of identifying out-of-distribution
(OOD) samples and detecting misclassiﬁcation on the MNIST and CIFAR-10
datasets  where they are found to outperform previous methods. Experiments on
synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian
methods PNs are able to distinguish between data and distributional uncertainty.

1

Introduction

Neural Networks (NNs) have become the dominant approach to addressing computer vision (CV) [1 
2  3]  natural language processing (NLP) [4  5  6]  speech recognition (ASR) [7  8] and bio-informatics
(BI) [9  10] tasks. Despite impressive  and ever improving  supervised learning performance  NNs
tend to make over-conﬁdent predictions [11] and until recently have been unable to provide measures
of uncertainty in their predictions. Estimating uncertainty in a model’s predictions is important  as
it enables  for example  the safety of an AI system [12] to be increased by acting on the model’s
prediction in an informed manner. This is crucial to applications where the cost of an error is high 
such as in autonomous vehicle control and medical  ﬁnancial and legal ﬁelds.
Recently notable progress has been made on predictive uncertainty for Deep Learning through
the deﬁnition of baselines  tasks and metrics [13] and the development of practical methods for
estimating uncertainty. One class of approaches stems from Bayesian Neural Networks [14  15  16 
17]. Traditionally  these approaches have been computationally more demanding and conceptually
more complicated than non-Bayesian NNs. Crucially  their performance depends on the form
of approximation made due to computational constraints and the nature of the prior distribution
over parameters. A recent development has been the technique of Monte-Carlo Dropout [18] 
which estimates predictive uncertainty using an ensemble of multiple stochastic forward passes and
computing the mean and spread of the ensemble. This technique has been successfully applied to
tasks in computer vision [19  20]. A number of non-Bayesian ensemble approaches have also been
proposed. One approach based on explicitly training an ensemble of DNNs  called Deep Ensembles
[11]  yields competitive uncertainty estimates to MC dropout. Another class of approaches  developed

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

for both regression [21] and classiﬁcation [22]  involves explicitly training a model in a multi-task
fashion to minimize its Kullback-Leibler (KL) divergence to both a sharp in-domain predictive
posterior and a ﬂat out-of-domain predictive posterior  where the out-of-domain inputs are sampled
either from a synthetic noise distribution or a different dataset during training. These methods
are explicitly trained to detect out-of-distribution inputs and have the advantage of being more
computationally efﬁcient at test time.
The primary issue with these approaches is that they conﬂate different aspects of predictive uncertainty 
which results from three separate factors - model uncertainty  data uncertainty and distributional
uncertainty. Model uncertainty  or epistemic uncertainty [23]  measures the uncertainty in estimating
the model parameters given the training data - this measures how well the model is matched to the
data. Model uncertainty is reducible1 as the size of training data increases. Data uncertainty  or
aleatoric uncertainty [23]  is irreducible uncertainty which arises from the natural complexity of the
data  such as class overlap  label noise  homoscedastic and heteroscedastic noise. Data uncertainty
can be considered a ’known-unknown’ - the model understands (knows) the data and can conﬁdently
state whether a given input is difﬁcult to classify (an unknown). Distributional uncertainty arises due
to mismatch between the training and test distributions (also called dataset shift [24]) - a situation
which often arises for real world problems. Distributional uncertainty is an ’unknown-unknown’ - the
model is unfamiliar with the test data and thus cannot conﬁdently make predictions. The approaches
discussed above either conﬂate distributional uncertainty with data uncertainty or implicitly model
distributional uncertainty through model uncertainty  as in Bayesian approaches. The ability to
separately model the 3 types of predictive uncertainty is important  as different actions can be taken
by the model depending on the source of uncertainty. For example  in active learning tasks detection
of distributional uncertainty would indicate the need to collect training data from this distribution.
This work addresses the explicit prediction of each of the three types of predictive uncertainty by
extending the work done in [21  22] while taking inspiration from Bayesian approaches.
Summary of Contributions. This work describes the limitations of previous methods of obtaining
uncertainty estimates and proposes a new framework for modeling predictive uncertainty  called Prior
Networks (PNs)  which allows distributional uncertainty to be treated as distinct from both data
uncertainty and model uncertainty. This work focuses on the application of PNs to classiﬁcation tasks.
Additionally  this work presents a discussion of a range of uncertainty metrics in the context of each
source of uncertainty. Experiments on synthetic and real data show that unlike previous non-Bayesian
methods PNs are able to distinguish between data uncertainty and distributional uncertainty. Finally 
PNs are evaluated2 on the tasks of identifying out-of-distribution (OOD) samples and detecting
misclassiﬁcation outlined in [13]  where they outperform previous methods on the MNIST and
CIFAR-10 datasets.

2 Current Approaches to Uncertainty Estimation

This section describes current approaches to predictive uncertainty estimation. Consider a distribution
p(x  y) over input features x and labels y. For image classiﬁcation x corresponds to images
and y object labels. In a Bayesian framework the predictive uncertainty of a classiﬁcation model
P(!c|x⇤ D) 3 trained on a ﬁnite dataset D = {xj  yj}N
j=1 ⇠ p(x  y) will result from data (aleatoric)
uncertainty and model (epistemic) uncertainty. A model’s estimates of data uncertainty are described
by the posterior distribution over class labels given a set of model parameters ✓ and model uncertainty
is described by the posterior distribution over the parameters given the data (eq. 1).

P(!c|x⇤ D) = Z P(!c|x⇤  ✓)
}

{z

|

Data

p(✓|D)

| {z }M odel

d✓

(1)

Here  uncertainty in the model parameters induces a distribution over distributions P(!c|x⇤  ✓). The
expected distribution P(!c|x⇤ D) is obtained by marginalizing out the parameters ✓. Unfortunately 
obtaining the true posterior p(✓|D) using Bayes’ rule is intractable  and it is necessary to use either
an explicit or implicit variational approximation q(✓) [25  26  27  28]:
(2)

p(✓|D) ⇡ q(✓)

1Up to identiﬁability limits. In the limit of inﬁnite data p(✓|D) yields equivalent parameterizations.
2Code available at https://github.com/KaosEngineer/DirichletPriorNetworks
3Using the standard shorthand for P(y = !c|x⇤ D).

2

Furthermore  the integral in eq. 1 is also intractable for neural networks and is typically approximated
via sampling (eq. 3)  using approaches like Monte-Carlo dropout [18]  Langevin Dynamics [29] or
explicit ensembling [11]. Thus 

P(!c|x⇤ D) ⇡

1
M

MXi=1

P(!c|x⇤  ✓(i))  ✓(i) ⇠ q(✓)

(3)

Each P(!c|x⇤  ✓(i)) in an ensemble {P(!c|x⇤  ✓(i))}M
i=1 obtained sampled from q(✓) is a categorical
distribution µ 4 over class labels y conditioned on the input x⇤  and can be visualized as a point on a
simplex. For the same x⇤ this ensemble is a collection of points on a simplex (ﬁg. 1a)  which can be
seen as samples of categorical distributions from an implicit conditional distribution over a simplex
(ﬁg. 1b) induced via the posterior over model parameters.

(a) Ensemble

(b) Distribution

Figure 1: Distributions on a Simplex

By selecting an appropriate approximate inference scheme and model prior p(✓) Bayesian approaches
i=1 is
aim to craft an approximate model posterior q(✓) such that the ensemble {P(!c|x⇤  ✓(i))}M
consistent in the region of training data  and becomes increasingly diverse when the input x⇤ is far
from the training data. Thus  these approaches aim to craft an implicit conditional distribution over a
simplex (ﬁg. 1b) with the attributes that it is sharp at the corners of a simplex for inputs similar to
the training data and ﬂat over the simplex for out-of-distribution inputs. Given an ensemble from
such a distribution  the entropy of the expected distribution P(!c|x⇤ D) will indicate uncertainty
in predictions. It is not possible  however  to determine from the entropy whether this uncertainty
is due to a high degree of data uncertainty  or whether the input is far from the region of training
data. It is necessary to use measures of spread of the ensemble  such as Mutual Information  to
assess uncertainty in predictions due to model uncertainty. This allows sources of uncertainty to be
determined.
In practice  however  for deep  distributed black-box models with tens of millions of parameters  such
as DNNs  it is difﬁcult to select an appropriate model prior and approximate inference scheme to
craft a model posterior which induces an implicit distribution with the desired properties. This makes
it hard to guarantee the desired properties of the induced distribution for current state-of-the-art Deep
Learning approaches. Furthermore  creating an ensemble can be computationally expensive.
An alternative  non-Bayesian class of approaches derives measures of uncertainty via the predictive
posteriors of regression [21] and classiﬁcation [13  22  30] DNNs. Here  DNNs are explicitly trained
[22  21] to yield high entropy posterior distributions for out-of-distribution inputs. These approaches
are easy to train and inference is computationally cheap. However  a high entropy posterior over
classes could indicate uncertainty in the prediction due to either an in-distribution input in a region
of class overlap or an out-of-distribution input far from the training data. Thus  it is not possible to
robustly determine the source of uncertainty using these approaches. Further discussion of uncertainty
measures can be found in section 4.

3 Prior Networks

Having described existing approaches  an alternative approach to modeling predictive uncertainty 
called Prior Networks  is proposed in this section. As previously described  Bayesian approaches aim
to construct an implicit conditional distribution over distributions on a simplex (ﬁg 1b) with certain
desirable attributes by appropriate selection of model prior and approximate inference method. In
practice this is a difﬁcult task and an open research problem.

4Where µ is a vector of probabilities:⇥µ1  ···   µK⇤T = ⇥P(y = !1)  ···   P(y = !K)⇤T

3

This work proposes to instead explicitly parameterize a distribution over distributions on a simplex 
p(µ|x⇤  ✓)  using a DNN referred to as a Prior Network and train it to behave like the implicit
distribution in the Bayesian approach. Speciﬁcally  when it is conﬁdent in its prediction a Prior
Network should yield a sharp distribution centered on one of the corners of the simplex (ﬁg. 2a). For
an input in a region with high degrees of noise or class overlap (data uncertainty) a Prior Network
should yield a sharp distribution focused on the center of the simplex  which corresponds to being
conﬁdent in predicting a ﬂat categorical distribution over class labels (known-unknown) (ﬁg. 2b).
Finally  for ’out-of-distribution’ inputs the Prior Network should yield a ﬂat distribution over the
simplex  indicating large uncertainty in the mapping x 7! y (unknown-unknown) (ﬁg. 2c).

(a) Conﬁdent Prediction

(b) High data uncertainty

(c) Out-of-distribution

Figure 2: Desired behaviors of a distribution over distributions

In the Bayesian framework distributional uncertainty  or uncertainty due to mismatch between the
distributions of test and training data  is considered a part of model uncertainty. In this work it will be
considered to be a source of uncertainty separate from data uncertainty or model uncertainty. Prior
Networks will be explicitly constructed to capture data uncertainty and distributional uncertainty. In
Prior Networks data uncertainty is described by the point-estimate categorical distribution µ and
distributional uncertainty is described by the distribution over predictive categoricals p(µ|x⇤  ✓). The
parameters ✓ of the Prior Network must encapsulate knowledge both about the in-domain distribution
and the decision boundary which separates the in-domain region from everything else. Construction
of a Prior Network is discussed in sections 3.1 and 3.2. Before this it is necessary to discuss its
theoretical properties.
Consider modifying eq. 1 by introducing the term p(µ|x⇤  ✓) as follows:
p(✓|D)
| {z }M odel

P(!c|x⇤ D) = Z Z p(!c|µ)
| {z }

In this expression data  distribution and model uncertainty are now each modeled by a separate term
within an interpretable probabilistic framework. The relationship between uncertainties is made
explicit - model uncertainty affects estimates of distributional uncertainty  which in turn affects
the estimates of data uncertainty. This is expected  as a large degree of model uncertainty will
yield a large variation in p(µ|x⇤  ✓)  and large uncertainty in µ will lead to a large uncertainty in
estimates of data uncertainty. Thus  model uncertainty affects estimates of data and distributional
uncertainties  and distributional uncertainty affects estimates of data uncertainty. This forms a
hierarchical model - there are now three layers of uncertainty: the posterior over classes  the per-data
prior distribution and the global posterior distribution over model parameters. Similar constructions
have been previously explored for non-neural Bayesian models  such as Latent Dirichlet Allocation
[31]. However  typically additional levels of uncertainty are added in order to increase the ﬂexibility
of models  and predictions are obtained by marginalizing or sampling. In this work  however  the
additional level of uncertainty is added in order to be able to extract additional measures of uncertainty 
depending on how the model is marginalized. For example  consider marginalizing out µ in eq. 4 
thus re-obtaining eq. 1:

Distributional

p(µ|x⇤  ✓)
|
{z
}

dµd✓

(4)

Data

Z hZ p(!c|µ)p(µ|x⇤  ✓)dµip(✓|D)d✓ = Z P(!c|x⇤  ✓)p(✓|D)d✓

Since the distribution over µ is lost in the marginalization it is unknown how sharp or ﬂat it was
around the point estimate. If the expected categorical P(!c|x⇤  ✓) is "ﬂat" it is now unknown whether
this is due to high data or distributional uncertainty. In this situation  it will be necessary to again
rely on measures which assess the spread of an MC ensemble  like mutual information (section 4)  to
establish the source of uncertainty. Thus  Prior Networks are consistent with previous approaches to

(5)

4

modeling uncertainty  both Bayesian and non-Bayesian - they can be viewed as an ’extra tool in the
uncertainty toolbox’ which is explicitly crafted to capture the effects of distributional mismatch in a
probabilistically interpretable way. Alternatively  consider marginalizing out ✓ in eq. 4 as follows:

Z p(!c|µ)hZ p(µ|x⇤  ✓)p(✓|D)d✓idµ = Z p(!c|µ)p(µ|x⇤ D)dµ

(6)

This yields expected estimates of data and distributional uncertainty given model uncertainty. Eq. 6
can be seen as a modiﬁcation of eq. 1 where the model is redeﬁned as p(!c|µ) and the distribution
over model parameters p(µ|x⇤ D) is now conditional on both the training data D and the test input
x⇤. This explicitly yields the distribution over the simplex which the Bayesian approach implicitly
induces. Further discussion of how measures of uncertainty are derived from the marginalizations of
equation 4 is presented in section 4.
Unfortunately  like eq. 1  the marginalization in eq. 6 is generally intractable  though it can be
approximated via Bayesian MC methods. For simplicity  this work will assume that a point-estimate
(eq. 7) of the parameters will be sufﬁcient given appropriate regularization and training data size.

p(✓|D) = (✓  ˆ✓) =) p(µ|x⇤;D) ⇡ p(µ|x⇤; ˆ✓)

(7)

3.1 Dirichlet Prior Networks
A Prior Network for classiﬁcation parametrizes a distribution over a simplex  such as a Dirichlet
(eq. 8)  Mixture of Dirichlet distributions or the Logistic-Normal distribution. In this work the
Dirichlet distribution is chosen due to its tractable analytic properties. A Dirichlet distribution is a
prior distribution over categorical distribution  which is parameterized by its concentration parameters
↵  where ↵0  the sum of all ↵c  is called the precision of the Dirichlet distribution. Higher values of
↵0 lead to sharper distributions.

µ↵c1
c

 ↵ c > 0 ↵ 0 =

↵c

(8)

A Prior Network which parametrizes a Dirichlet will be referred to as a Dirichlet Prior Network
(DPN). A DPN will generate the concentration parameters ↵ of the Dirichlet distribution.

(↵0)
c=1 (↵c)

KYc=1

Dir(µ|↵) =

QK
p(µ|x⇤; ˆ✓) = Dir(µ|↵)  ↵ = f (x⇤; ˆ✓)
The posterior over class labels will be given by the mean of the Dirichlet:
P(!c|x⇤; ˆ✓) =Z p(!c|µ)p(µ|x⇤; ˆ✓)dµ =

↵c
↵0

KXc=1

If an exponential output function is used for the DPN  where ↵c = ezc  then the expected posterior
probability of a label !c is given by the output of the softmax (eq. 11).

(9)

(10)

(11)

P(!c|x⇤; ˆ✓) =

ezc(x⇤)
k=1 ezk(x⇤)

PK

Thus  standard DNNs for classiﬁcation with a softmax output function can be viewed as predicting the
expected categorical distribution under a Dirichlet prior. The mean  however  is insensitive to arbitrary
scaling of ↵c. Thus the precision ↵0  which controls the sharpness of the Dirichlet  is degenerate
under standard cross-entropy training. It is necessary to change the cost function to explicitly train a
DPN to yield a sharp or ﬂat prior distribution around the expected categorical depending on the input
data.

3.2 Dirichlet Prior Network Training
There are potentially many ways in which a Prior Network can be trained and it is not the focus of
this work to investigate them all. This work considers one approach to training a DPN based on the
work done in [21  22] and here. The DPN is explicitly trained in a multi-task fashion to minimize
the KL divergence (eq. 12) between the model and a sharp Dirichlet distribution focused on the
appropriate class for in-distribution data  and between the model and a ﬂat Dirichlet distribution for

5

out-of-distribution data. A ﬂat Dirichlet is chosen as the uncertain distribution in accordance with the
principle of insufﬁcient reason [32]  as all possible categorical distributions are equiprobable.

L(✓) = Epin(x)[KL[Dir(µ| ˆ↵)||p(µ|x; ✓)]] + Epout(x)[KL[Dir(µ| ˜↵)||p(µ|x; ✓)]]

(12)

In order to train using this loss function the in-distribution targets ˆ↵ and out-of-distribution targets ˜↵
must be deﬁned. It is simple to specify a ﬂat Dirichlet distribution by setting all ˜↵c = 1. However 
directly setting the in-distribution target ˆ↵c is not convenient. Instead the concentration parameters
ˆ↵c are re-parametrized into ˆ↵0  the target precision  and the means ˆµc = ˆ↵c
. ˆ↵0 is a hyper-parameter
ˆ↵0
set during training and the means are simply the 1-hot targets used for classiﬁcation. A further
complication is that learning sparse ’1-hot’ continuous distributions  which are effectively delta
functions  is challenging under the deﬁned KL loss  as the error surface becomes poorly suited for
optimization. There are two solutions - ﬁrst  it is possible to smooth the target means (eq. 13)  which
redistributes a small amount of probability density to the other corners of the Dirichlet. Alternatively 
teacher-student training [33] can be used to specify non-sparse target means ˆµ. The smoothing
approach is used in this work. Additionally  cross-entropy can be used as an auxiliary loss for
in-distribution data.

ˆµc =n 1  (K  1)✏if

✏if



(y = !c) = 1
(y = !c) = 0

(13)

The multi-task training objective (eq. 12) requires samples of ˜x from the out-of-domain distribution
pout(x). However  the true out-of-domain distribution is unknown and samples are unavailable.
One solution is to synthetically generate points on the boundary of the in-domain region using a
generative model [21  22]. An alternative is to use a different  real dataset as a set of samples from
the out-of-domain distribution [22].

4 Uncertainty Measures

The previous section introduced a new framework for modeling uncertainty. This section explores a
range of measures for quantifying uncertainty given a trained DNN  DPN or Bayesian MC ensemble.
The discussion is broken down into 4 classes of measure  depending on how eq. 4 is marginalized.
Details of derivation can be found in Appendix C.
The ﬁrst class derives measures of uncertainty from the expected predictive categorical P(!c|x⇤;D) 
given a full marginalization of eq. 4 which can be approximated either with a point estimate of the
parameters ˆ✓ or a Bayesian MC ensemble. The ﬁrst measure is the probability of the predicted class
(mode)  or max probability (eq. 14)  which is a measure of conﬁdence in the prediction used in
[13  22  30  23  11].

(14)
The second measure is the entropy (eq. 15) of the predictive distribution [23  18  11]. It behaves
similar to max probability  but represents the uncertainty encapsulated in the entire distribution.

P(!c|x⇤;D)

P = max

c

KXc=1

H[P(y|x⇤;D)] = 

P(!c|x⇤;D) ln(P(!c|x⇤;D))

(15)

Max probability and entropy of the expected distribution can be seen as measures of the total
uncertainty in predictions.
The second class of measures considers marginalizing out µ in eq. 4  yielding eq. 1. Mutual
Information (MI) [23] between the categorical label y and the parameters of the model ✓ is a measure
i=1 [18] which assess uncertainty in predictions due to
of the spread of an ensemble {P(!c|x⇤  ✓(i))}M
model uncertainty. Thus  MI implicitly captures elements of distributional uncertainty. MI can be
expressed as the difference of the total uncertainty  captured by the entropy of expected distribution 
and the expected data uncertainty  captured by expected entropy of each member of the ensemble
(eq. 16). This interpretation was given in [34].

M odel U ncertainty

I[y  ✓|x⇤ D]
}
|

{z

= H[Ep(✓|D)[P(y|x⇤  ✓)]]
}
|

T otal U ncertainty

{z

6

 Ep(✓|D)[H[P(y|x⇤  ✓)]]
}
|

Expected Data U ncertainty

{z

(16)

The third class of measures considers marginalizing out ✓ in eq. 4  yielding eq. 6. The ﬁrst measure
in this class is the mutual information between y and µ (eq. 17)  which behaves in exactly the same
way as MI between y and ✓  but the spread is now explicitly due to distributional uncertainty  rather
than model uncertainty.

= H[Ep(µ|x⇤;D)[P(y|µ)]]
|
}

T otal U ncertainty

{z

 Ep(µ|x⇤;D)[H[P(y|µ)]]
|
}

Expected Data U ncertainty

{z

(17)

Distributional U ncertainty

I[y  µ|x⇤;D]
|
}

{z

Another measure of uncertainty is the differential entropy (eq. 18) of the DPN. This measure is
maximized when all categorical distributions are equiprobable  which occurs when the Dirichlet
Distribution is ﬂat - in other words when there is the greatest variety of samples from the Dirichlet
prior. Differential entropy is well suited to measuring distributional uncertainty  as it can be low even
if the expected categorical under the Dirichlet prior has high entropy  and also captures elements of
data uncertainty.

H[p(µ|x⇤;D)] = ZSK1

p(µ|x⇤;D) ln(p(µ|x⇤;D))dµ

(18)

The ﬁnal class of measures uses the full eq. 4 and assesses the spread of p(µ|x⇤; ✓) due to model
uncertainty via the MI between µ and ✓  which can be computed via Bayesian ensemble approaches.

5 Experiments

The previous sections discussed modeling different aspects of predictive uncertainty and presented
several measures of quantifying it. This section compares the proposed and previous methods in two
sets of experiments. The ﬁrst experiment illustrates the advantages of a DPN over other non-Bayesian
methods [22  30] on synthetic data and the second set of experiments evaluate DPNs on MNIST and
CIFAR-10 and compares them to DNNs and ensembles generated via Monte-Carlo Dropout (MCDP)
on the tasks of misclassiﬁcation detection and out-of-distribution data detection. The experimental
setup is described in Appendix A and additional experiments are described in Appendix B.

5.1 Synthetic Experiments

A synthetic experiment was designed to illustrate the limitation of using uncertainty measures derived
from P(!c|x⇤;D) [22  30] to detect out-of-distribution samples. A simple dataset with 3 Gaussian
distributed classes with equidistant means and tied isotropic variance  is created. The classes are
non-overlapping when  = 1 (ﬁg. 3a) and overlap when  = 4 (ﬁg. 3d). The entropy of the true
posterior over class labels is plotted in blue in ﬁgures 3a and 3d  which show that when the classes are
distinct the entropy is high only on the decision boundaries  but when the classes overlap the entropy
is high also within the data region. A small DPN with 1 hidden layer of 50 neurons is trained on this
data. Figures 3b and 3c show that when classes are distinct both the entropy of the DPN’s predictive
posterior and the differential entropy of the DPN have identical behaviour - low in the region of
data and high elsewhere  allowing in-distribution and out-of-distribution regions to be distinguished.
Figures 3e and 3f  however  show that when there is a large degree of class overlap the entropy and
differential entropy have different behavior - entropy is high both in region of class overlap and far
from training data  making difﬁcult to distinguish out-of-distribution samples and in-distribution
samples at a decision boundary. In contrast  the differential entropy is low over the whole region of
training data and high outside  allowing the in-distribution region to be clearly distinguished from the
out-of-distribution region.

5.2 MNIST and CIFAR-10 Experiments

An in-domain misclassiﬁcation detection experiment and an out-of-distribution (OOD) input detection
experiment were run on the MNIST and CIFAR-10 datasets [35  36] to assess the DPN’s ability to
estimate uncertainty. The misclassiﬁcation detection experiment involves detecting whether a given
prediction is incorrect given an uncertainty measure. Misclassiﬁcations are chosen as the positive
class. The misclassiﬁcation detection experiment was run on the MNIST valid+test set and the
CIFAR-10 test set. The out-of-distribution detection experiment involves detecting whether an input

7

(a)  = 1

(b) Entropy  = 1

(c) Diff. Entropy  = 1

(d)  = 4

(e) Entropy  = 4

(f) Diff. Entropy  = 4

Figure 3: Synthetic Experiment

is out-of-distribution given a measure of uncertainty. Out-of-distribution samples are chosen as the
positive class. The OMNIGLOT dataset [37]  scaled down to 28x28 pixels  was used as real ’OOD’
data for MNIST. 15000 samples of OMNIGLOT data were randomly selected to form a balanced
set of positive (OMNIGLOT) and negative (MNIST valid+test) samples. For CIFAR-10 three OOD
datasets were considered - SVHN  LSUN and TinyImagetNet (TIM) [38  39  40]. The two considered
baseline approaches derive uncertainty measures from either the class posterior of a DNN [13] or an
ensemble generated via MC dropout applied to the same DNN [23  18]. All uncertainty measures
described in section 4 are explored for both tasks in order to see which yield best performance. The
performance is assessed by area under the ROC (AUROC) and Precision-Recall (AUPR) curves in
both experiments as in [13].

Table 1: MNIST and CIFAR-10 misclassiﬁcation detection

Data

MNIST

CIFAR10

Model
DNN
MCDP
DPN
DNN
MCDP
DPN

Max.P
98.0
97.2
99.0
92.4
92.5
92.2

-

AUROC
Ent. M.I. D.Ent. Max.P
26.6
98.6
33.0
97.2
43.6
98.9
92.3
48.7
48.4
92.0
52.7
92.1

90.4
92.1

96.9
98.6

90.9

92.9

-
-

-
-

-

AUPR

-

-
-

27.8
30.7

Ent. M.I. D.Ent.
25.0
29.0
39.7
47.1
45.5
51.0

37.6
51.0

45.5

25.5

-
-

-

% Err.
0.4
0.4
0.6
8.0
8.0
8.5

Table 1 shows that the DPN consistently outperforms both a DNN  and a MC dropout ensemble
(MCDP) in misclassiﬁcation detection performance  although there is a negligible drop in accuracy of
the DPN as compared to a DNN or MCDP. Max probability yields the best results  closely followed
by the entropy of the predictive distribution. This is expected  as they are measures of total uncertainty
in predictions  while the other measures capture the either model or distributional uncertainty. The
performance difference is more pronounced on AUPR  which is sensitive to misbalanced classes.
Table 2 shows that a DPN consistently outperforms the baselines in OOD sample detection for both
MNIST and CIFAR-10 datasets. On MNIST  the DPN is able to perfectly classify all samples using
max probability  entropy and differential entropy. On the CIFAR-10 dataset the DPN consistently
outperforms the baselines by a large margin. While high performance against SVHN and LSUN is
expected  as LSUN  SVHN and CIFAR-10 are quite different  high performance against TinyIma-
geNet  which is also a dataset of real objects and therefore closer to CIFAR-10  is more impressive.
Curiously  MC dropout does not always yield better results than a standard DNN  which supports

8

the assertion that it is difﬁcult to achieve the desired behaviour for a Bayesian distribution over
distributions.

Table 2: MNIST and CIFAR-10 out-of-domain detection

Data

ID

OOD

MNIST

OMNI

CIFAR10

SVHN

CIFAR10

LSUN

CIFAR10

TIM

Model
DNN
MCDP
DPN
DNN
MCDP
PN
DNN
MCDP
DPN
DNN
MCDP
DPN

Max.P
98.7
99.2
100.0
90.1
89.6
98.1
89.8
89.1
94.4
87.5
87.6
94.3

-

-

-
-

100.0

99.3
99.5

AUROC
Ent. M.I. D.Ent. Max.P
98.3
98.8
99.0
99.2
100.0
100.0
90.8
84.6
84.1
90.6
97.7
98.2
87.0
91.4
86.5
90.9
94.4
93.3
84.7
88.7
85.1
89.2
94.3
94.0

86.9
94.3

83.7
98.2

89.3
94.4

98.5

94.6

94.6

-
-

-
-

-
-

-

-

AUPR

-

-

-
-

-
-

100.0

99.3
97.5

Ent. M.I. D.Ent.
98.5
99.1
100.0
85.1
84.8
97.8
90.0
89.6
93.4
87.2
87.9
94.0

86.4
93.4

83.2
94.0

73.1
97.8

98.2

93.3

94.2

-
-

-
-

-

-

The experiments above suggest that there is little beneﬁt of using measures such as differential entropy
and mutual information over standard entropy. However  this is because MNIST and CIFAR-10 are
low data uncertainty datasets - all classes are distinct. It is interesting to see whether differential
entropy of the Dirichlet prior will be able to distinguish in-domain and out-of-distribution data better
than entropy when the classes are less distinct. To this end zero mean isotropic Gaussian noise with
a standard deviation  = 3 noise is added to the inputs of the DNN and DPN during both training
and evaluation on the MNIST dataset. Table 3 shows that in the presence of strong noise entropy
and MI fail to successfully discriminate between in-domain and out-of-distribution samples  while
performance using differential entropy barely falls.

Table 3: MNIST vs OMNIGLOT. Out-of-distribution detection AUROC on noisy data.

M.I.

D.Ent.


DNN
MCDP
DPN

Ent.

0.0
98.8
98.8
100.0

3.0
58.4
58.4
51.8

0.0
-

99.3
99.5

3.0
-

79.1
22.3

0.0
-
-

100.0

3.0
-
-

99.8

6 Conclusion

This work describes the limitations of previous work on predictive uncertainty estimations within the
context of sources of uncertainty and proposes to treat out-of-distribution (OOD) inputs as a separate
source of uncertainty  called Distributional Uncertainty. To this end  this work presents a novel
framework  called Prior Networks (PN)  which allows data  distributional and model uncertainty to
be treated separately within a consistent probabilistically interpretable framework. A particular form
of these PNs are applied to classiﬁcation  Dirichlet Prior Networks (DPNs). DPNs are shown to yield
more accurate estimates of distributional uncertainty than MC Dropout and standard DNNs on the task
of OOD detection on the MNIST and CIFAR-10 datasets. The DPNs also outperform other methods
on the task of misclassiﬁcation detection. A range of uncertainty measures is presented and analyzed
in the context of the types of uncertainty which they assess. It was noted that measures of total
uncertainty  such as max probability or entropy of the predictive distribution  yield the best results on
misclassiﬁcation detection. Differential entropy of DPN was best for measure of uncertainty for OOD
detection  especially when classes are less distinct. This was illustrated on both a synthetic experiment
and on a noise-corrupted MNIST task. Uncertainty measures can be analytically calculated at test
time for DPNs  reducing computational cost relative to ensemble approaches. Having investigated
PNs for image classiﬁcation  it is interesting to apply them to other tasks computer vision  NLP 
machine translation  speech recognition and reinforcement learning. Finally  it is necessary to explore
Prior Networks for regression tasks.

9

Acknowledgments
This paper reports on research partly supported by Cambridge Assessment  University of Cambridge.
This work also partly funded by a DTA EPSRC away and a Google Research award. We would also
like to thank members of the CUED Machine Learning group  especially Dr. Richard Turner  for
fruitful discussions.

References
[1] Ross Girshick  “Fast R-CNN ” in Proc. 2015 IEEE International Conference on Computer

Vision (ICCV)  2015  pp. 1440–1448.

[2] Karen Simonyan and Andrew Zisserman  “Very Deep Convolutional Networks for Large-Scale
Image Recognition ” in Proc. International Conference on Learning Representations (ICLR) 
2015.

[3] Ruben Villegas  Jimei Yang  Yuliang Zou  Sungryull Sohn  Xunyu Lin  and Honglak Lee 
“Learning to Generate Long-term Future via Hierarchical Prediction ” in Proc. International
Conference on Machine Learning (ICML)  2017.

[4] Tomas Mikolov et al.  “Linguistic Regularities in Continuous Space Word Representations ” in

Proc. NAACL-HLT  2013.

[5] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean  “Efﬁcient Estimation of Word

Representations in Vector Space ” 2013  arXiv:1301.3781.

[6] Tomas Mikolov  Martin Karaﬁát  Lukás Burget  Jan Cernocký  and Sanjeev Khudanpur  “Re-

current Neural Network Based Language Model ” in Proc. INTERSPEECH  2010.

[7] Geoffrey Hinton  Li Deng  Dong Yu  George Dahl  Abdel rahman Mohamed  Navdeep Jaitly 
Andrew Senior  Vincent Vanhoucke  Patrick Nguyen  Tara Sainath  and Brian Kingsbury  “Deep
neural networks for acoustic modeling in speech recognition ” Signal Processing Magazine 
2012.

[8] Awni Y. Hannun  Carl Case  Jared Casper  Bryan Catanzaro  Greg Diamos  Erich Elsen  Ryan
Prenger  Sanjeev Satheesh  Shubho Sengupta  Adam Coates  and Andrew Y. Ng  “Deep speech:
Scaling up end-to-end speech recognition ” 2014  arXiv:1412.5567.

[9] Rich Caruana  Yin Lou  Johannes Gehrke  Paul Koch  Marc Sturm  and Noemie Elhadad 
“Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission ”
in Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  New York  NY  USA  2015  KDD ’15  pp. 1721–1730  ACM.

[10] Babak Alipanahi  Andrew Delong  Matthew T. Weirauch  and Brendan J. Frey  “Predicting
the sequence speciﬁcities of DNA- and RNA-binding proteins by deep learning ” Nature
Biotechnology  vol. 33  no. 8  pp. 831–838  July 2015.

[11] B. Lakshminarayanan  A. Pritzel  and C. Blundell  “Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles ” in Proc. Conference on Neural Information Processing
Systems (NIPS)  2017.

[12] Dario Amodei  Chris Olah  Jacob Steinhardt  Paul F. Christiano  John Schulman  and Dan Mané 
“Concrete problems in AI safety ” http://arxiv.org/abs/1606.06565  2016  arXiv:
1606.06565.

[13] Dan Hendrycks and Kevin Gimpel  “A Baseline for Detecting Misclassiﬁed and Out-of-
Distribution Examples in Neural Networks ” http://arxiv.org/abs/1610.02136 
2016  arXiv:1610.02136.

[14] David JC MacKay  “A practical bayesian framework for backpropagation networks ” Neural

computation  vol. 4  no. 3  pp. 448–472  1992.

[15] David JC MacKay  Bayesian methods for adaptive models  Ph.D. thesis  California Institute of

Technology  1992.

[16] Geoffrey E. Hinton and Drew van Camp  “Keeping the neural networks simple by minimizing
the description length of the weights ” in Proc. Sixth Annual Conference on Computational
Learning Theory  New York  NY  USA  1993  COLT ’93  pp. 5–13  ACM.

10

[17] Radford M. Neal  Bayesian learning for neural networks  Springer Science & Business Media 

1996.

[18] Yarin Gal and Zoubin Ghahramani  “Dropout as a Bayesian Approximation: Representing
Model Uncertainty in Deep Learning ” in Proc. 33rd International Conference on Machine
Learning (ICML-16)  2016.

[19] A. Kendall  Y. Gal  and R. Cipolla  “Multi-Task Learning Using Uncertainty to Weight Losses
for Scene Geometry and Semantics ” in Proc. Conference on Neural Information Processing
Systems (NIPS)  2017.

[20] A. Kendall and Y. Gal  “What Uncertainties Do We Need in Bayesian Deep Learning for
Computer Vision ” in Proc. Conference on Neural Information Processing Systems (NIPS) 
2017.

[21] A. Malinin  A. Ragni  M.J.F. Gales  and K.M. Knill  “Incorporating Uncertainty into Deep
Learning for Spoken Language Assessment ” in Proc. 55th Annual Meeting of the Association
for Computational Linguistics (ACL)  2017.

[22] Kimin Lee  Honglak Lee  Kibok Lee  and Jinwoo Shin  “Training conﬁdence-calibrated
classiﬁers for detecting out-of-distribution samples ” International Conference on Learning
Representations  2018.

[23] Yarin Gal  Uncertainty in Deep Learning  Ph.D. thesis  University of Cambridge  2016.
[24] Joaquin Quiñonero-Candela  Dataset Shift in Machine Learning  The MIT Press  2009.
[25] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra  “Weight Uncer-
tainty in Neural Networks ” in Proc. International Conference on Machine Learning (ICML) 
2015.

[26] Alex Graves  “Practical variational inference for neural networks ” in Advances in neural

information processing systems  2011  pp. 2348–2356.

[27] Christos Louizos and Max Welling  “Structured and efﬁcient variational deep learning with
matrix gaussian posteriors ” in International Conference on Machine Learning  2016  pp.
1708–1716.

[28] Diederik P Kingma  Tim Salimans  and Max Welling  “Variational dropout and the local
reparameterization trick ” in Advances in Neural Information Processing Systems  2015  pp.
2575–2583.

[29] Max Welling and Yee Whye Teh  “Bayesian Learning via Stochastic Gradient Langevin

Dynamics ” in Proc. International Conference on Machine Learning (ICML)  2011.

[30] Shiyu Liang  Yixuan Li  and R. Srikant  “Enhancing the reliability of out-of-distribution image
detection in neural networks ” in Proc. International Conference on Learning Representations 
2018.

[31] David M. Blei  Andrew Y. Ng  and Michael I. Jordan  “Latent Dirichlet Allocation ” Journal of

Machine Learning Research  vol. 3  pp. 993–1022  Mar. 2003.

[32] Kevin P. Murphy  Machine Learning  The MIT Press  2012.
[33] Geoffrey Hinton  Oriol Vinyals  and Jeff Dean  “Distilling the knowledge in a neural network ”

2015  arXiv:1503.02531.

[34] Stefan Depeweg  José Miguel Hernández-Lobato  Finale Doshi-Velez  and Steffen Udluft  “De-
composition of uncertainty for active learning and reliable reinforcement learning in stochastic
systems ” arXiv preprint arXiv:1710.07283  2017.

[35] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document

recognition ” Proceedings of the IEEE  vol. 86  pp. 2278–2324  1998.

[36] Alex Krizhevsky  “Learning multiple layers of features from tiny images ” 2009.
[37] Brenden M. Lake  Ruslan Salakhutdinov  and Joshua B. Tenenbaum  “Human-level concept
learning through probabilistic program induction ” Science  vol. 350  no. 6266  pp. 1332–1338 
2015.

[38] Ian J. Goodfellow  Yaroslav Bulatov  Julian Ibarz  Sacha Arnoud  and Vinay D. Shet  “Multi-
digit number recognition from street view imagery using deep convolutional neural networks ”
2013  arXiv:1312.6082.

11

[39] Fisher Yu  Yinda Zhang  Shuran Song  Ari Seff  and Jianxiong Xiao  “LSUN: construction of a
large-scale image dataset using deep learning with humans in the loop ” 2015  arXiv:1506.03365.
[40] Stanford CS231N  “Tiny ImageNet ” https://tiny-imagenet.herokuapp.com/ 

2017.

[41] M Buscema  “Metanet: The theory of independent judges ” Substance Use & Misuse  vol. 33 

no. 2  pp. 439–461  1998.

[42] Martín Abadi et al.  “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems ”

2015  Software available from tensorﬂow.org.

[43] Timothy Dozat  “Incorporating Nesterov Momentum into Adam ” in Proc. International

Conference on Learning Representations (ICLR)  2016.

12

,Andrey Malinin
Mark Gales