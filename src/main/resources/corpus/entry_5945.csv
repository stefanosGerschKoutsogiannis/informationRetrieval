2019,Which Algorithmic Choices Matter at Which Batch Sizes?  Insights From a Noisy Quadratic Model,Increasing the batch size is a popular way to speed up neural network training  but beyond some critical batch size  larger batch sizes yield diminishing returns. In this work  we study how the critical batch size changes based on properties of the optimization algorithm  including acceleration and preconditioning  through two different lenses: large scale experiments and analysis using a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning  specifically Adam and K-FAC  result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training  despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers  previous results with accelerated gradient descent  and other results around optimal learning rates and large batch training  making it a useful tool to generate testable predictions about neural network optimization.
We demonstrate empirically that the simple noisy quadratic model (NQM) displays many similarities to neural networks in terms of large-batch training. We prove analytical convergence results for the NQM model that predict such behavior and hence provide possible explanations and a better understanding for many large-batch training phenomena.,Which Algorithmic Choices Matter at Which Batch

Sizes? Insights From a Noisy Quadratic Model

Guodong Zhang1 2 3⇤  Lala Li3  Zachary Nado3  James Martens4 

Sushant Sachdeva1  George E. Dahl3  Christopher J. Shallue3  Roger Grosse1 2
1University of Toronto  2Vector Institute  3Google Research  Brain Team  4DeepMind

Abstract

Increasing the batch size is a popular way to speed up neural network training 
but beyond some critical batch size  larger batch sizes yield diminishing returns.
In this work  we study how the critical batch size changes based on properties of
the optimization algorithm  including acceleration  preconditioning and averaging 
through two different lenses: large scale experiments  and analysis of a simple
noisy quadratic model (NQM). We experimentally demonstrate that optimization
algorithms that employ preconditioning  speciﬁcally Adam and K-FAC  result in
much larger critical batch sizes than stochastic gradient descent with momentum.
We also demonstrate that the NQM captures many of the essential features of
real neural network training  despite being drastically simpler to work with. The
NQM predicts our results with preconditioned optimizers and exponential moving
average  previous results with accelerated gradient descent  and other results around
optimal learning rates and large batch training  making it a useful tool to generate
testable predictions about neural network optimization.

1

Introduction

Increasing the batch size is one of the most appealing ways to accelerate neural network training
on data parallel hardware. Larger batch sizes yield better gradient estimates and  up to a point 
reduce the number of steps required for training  which reduces the training time. The importance of
understanding the beneﬁts of modern parallel hardware has motivated a lot of recent work on training
neural networks with larger batch sizes [Goyal et al.  2017  Osawa et al.  2018  McCandlish et al. 
2018  Shallue et al.  2018]. To date  the most comprehensive empirical study of the effects of batch
size on neural network training is Shallue et al. [2018]  who conﬁrmed that increasing the batch size
initially achieves perfect scaling (i.e. doubling the batch size halves the number of steps needed) up
to a problem-dependent critical batch size  beyond which it yields diminishing returns [Balles et al. 
2017  Goyal et al.  2017  Jastrz˛ebski et al.  2018  McCandlish et al.  2018]. Shallue et al. [2018] also
provided experimental evidence that the critical batch size depends on the optimization algorithm 
the network architecture  and the data set. However  their experiments only covered plain SGD 
SGD with (heavy-ball) momentum  and SGD with Nesterov momentum  leaving open the enticing
possibility that other optimizers might extend perfect scaling to even larger batch sizes.
Empirical scaling curves like those in Shallue et al. [2018] are essential for understanding the effects
of batch size  but generating such curves  even for a single optimizer on a single task  can be very
expensive. On the other hand  existing theoretical analyses that attempt to analytically derive critical
batch sizes (e.g. Ma et al. [2018]  Yin et al. [2018]  Jain et al. [2018]) do not answer our questions
about which optimizers scale the best with batch size. They tend to make strong assumptions  produce
parameter-dependent results that are difﬁcult to apply  or are restricted to plain SGD. It would be

⇤Work done as part of the Google Student Researcher Program. Email: gdzhang@cs.toronto.edu

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

ideal to ﬁnd a middle ground between a purely empirical investigation and theoretical analysis by
building a model of neural network optimization problems that captures the essential behavior we
see in real neural networks  while still being easy to understand. Additionally  we need to study
optimizers beyond momentum SGD since they might provide us an approach to exploit speedups
from the very largest batch sizes. In this work  we make the following contributions:

1. We show that a simple noisy quadratic model (NQM) is remarkably consistent with the batch
size effects observed in real neural networks  while allowing us to run experiments in seconds 
making it a great tool to generate testable predictions about neural network optimization.

2. We show that the NQM successfully predicts that momentum should speed up training relative

to plain SGD at larger batch sizes  but have no beneﬁt at small batch sizes.

3. Through large scale experiments with Adam [Kingma and Ba  2014] and K-FAC [Martens and
Grosse  2015]  we conﬁrm that  as predicted by the NQM  preconditioning extends perfect batch
size scaling to larger batch sizes than are possible with momentum SGD alone. Furthermore 
unlike momentum  preconditioning can help at small batch sizes as well.

4. Lastly  we show that  as predicted by the NQM  exponential moving averages reduce the number
of steps required for a speciﬁc batch size and can achieve the same acceleration with smaller
batch sizes  thereby saving computation.

2 Related Work

In a classic paper  Bottou and Bousquet [2008] studied the asymptotics of stochastic optimization
algorithms and found SGD to be competitive with fancier approaches. They showed that stochastic
optimization involves fundamentally different tradeoffs from full-batch optimization. More recently 
several studies have investigated the relationship between batch size and training time for neural
networks. Chen et al. [2018] studied the effect of network width on the critical batch size  and showed
experimentally that it depends on both the data set and network architecture. Golmant et al. [2018]
studied how various heuristics for adjusting the learning rate as a function of batch size affect the
relationship between batch size and training time. Shallue et al. [2018] conducted a comprehensive
empirical study on the relationship between batch size and training time with different neural network
architectures and data sets using plain SGD  heavy-ball momentum  and Nesterov momentum. Finally 
McCandlish et al. [2018] used the average gradient noise over training to predict the critical batch
size. All of these studies described a basic relationship between batch size and training steps to
a ﬁxed error goal  which is comprised of three regions: perfect scaling initially  then diminishing
returns  and ﬁnally no beneﬁt for all batch sizes greater than the critical batch size.
Other studies have attempted to characterize the critical batch size analytically in stochastic optimiza-
tion. Under varying assumptions  Ma et al. [2018]  Yin et al. [2018]  Jain et al. [2018] all derived
analytical notions of critical batch size  but to our knowledge  all for SGD.
Additionally  previous studies have shown that SGD and momentum SGD are equivalent for small
learning rates (after appropriate rescaling)  both for the continuous limit [Leen and Orr  1994] and
discrete settings Yuan et al. [2016]. However  they do not explain why momentum SGD (including
heavy-ball and Nesterov momentum) sometimes outperforms plain SGD in mini-batch training (as
observed by Kidambi et al. [2018] and Shallue et al. [2018]). Concurrently  Smith et al. [2019]
showed that momentum outperforms plain SGD at large batch sizes.
Finally  there are a few works studying average of the iterates  rather than working with the last iterate.
This is a classical idea in optimization  where it is known to provide improved convergence [Polyak
and Juditsky  1992  Bach and Moulines  2013  Dieuleveut and Bach  2016]. However  most of
them focused on tail averaging  which you have to decide ahead of time the iteration to start
accumulating the running averaging. More commonly (especially in deep learning)  exponential
moving average [Martens  2014] is preferred for its simplicity and ability to handle non-convex
landscape. However  no analysis was done especially when mini-batch is used.

3 Analysis of the Noisy Quadratic Model (NQM)

In this section  we work with a noisy quadratic model (NQM)  a stochastic optimization problem
whose dynamics can be simulated analytically  in order to reason about various phenomena en-

2

countered in training neural networks. In this highly simpliﬁed model  we ﬁrst assume the loss
function being optimized is a convex quadratic  with noisy observations of the gradient. For analytic
tractability  we further assume the noise covariance is codiagonalizable with the Hessian. Because
we are not interested in modeling overﬁtting effects  we focus on the online training setting  where
the observations are drawn i.i.d. in every training iteration. Under these assumptions  we derive an
analytic expression for the risk after any number of steps of SGD with a ﬁxed step size  as well as a
dynamic programming method to compute the risk following a given step size schedule.
Convex quadratics may appear an odd model for a complicated nonconvex optimization landscape.
However  one obtains a convex quadratic objective by linearizing the network’s function around a
given weight vector and taking the second-order Taylor approximation to the loss function (assuming
it is smooth and convex). Indeed  recent theoretical works [Jacot et al.  2018  Du et al.  2019  Zhang
et al.  2019a] show that for wide enough networks  the weights stay close enough to the initialization
for the linearized approximation to remain accurate. Empirically  linearized approximations closely
match a variety of training phenomena for large but realistic networks [Lee et al.  2019].

3.1 Problem Setup
We now introduce the noisy quadratic model [Schaul et al. 
2013  Martens  2014  Wu et al.  2018]  where the true function
being optimized is a convex quadratic. Because we analyze
rotation-invariant and translation-invariant optimizers such as
SGD and heavy-ball momentum  we assume without loss of
generality that the quadratic form is diagonal  and that the
optimum is at the origin. Hence  our exact cost function decom-
poses as a sum of scalar quadratic functions for each coordinate:

L(✓) =

1
2

✓>H✓ =

1
2

hi✓2

i  

dXi=1

dXi=1

`(✓i).

(1)

Figure 1: Cartoon of the evolution of
risk for different coordinates with and
without learning rate decay.

Without loss of generality  we assume h1  h2  ...  hd. We consider a single gradient query
to have the form g(✓) = H✓ + ✏ where E[✏] = 0 and Cov(✏) = C. To reduce the variance
of gradient estimation  we can average over multiple independent queries  which corresponds to
"mini-batch training" in neural network optimization. We denote the averaged gradient as gB(✓) and
the covariance Cov(gB(✓)) = C/B  where B is the number of queries (mini-batch size).
For analytical tractability  we make the nontrivial assumption that H and C are codiagonalizable.
(Since H is diagonal  this implies that C = diag(c1  . . .   cd).) See Section 3.5 for justiﬁcation of this
assumption. Under gradient descent with ﬁxed step size ↵  each dimension evolves independently as
(2)
where ↵ is the learning rate and ✏i is zero-mean unit variance iid noise. By treating ✓i as a random
variable  we immediately obtain the dynamics of its mean and variance.

✓i(t + 1) = (1  ↵hi)✓i(t) + ↵pci/B✏i 

E [✓i(t + 1)] = (1  ↵hi)E [✓i(t)]   V [✓i(t + 1)] = (1  ↵hi)2V [✓i(t)] +

Based on eqn. (3)  the expected risk after t steps in a given dimension i is

↵2ci
B

.

(3)

(4)

E [`(✓i(t))] = (1  ↵hi)2t
}

convergence rate

{z

|

E [`(✓i(0))] +1  (1  ↵hi)2t

↵ci

{z

 

}

2B(2  ↵hi)

steady state risk

|

where we have assumed that ↵hi  2. (Note that this can be seen as a special case of the convergence
result derived for convex quadratics in Martens [2014].)
Remarkably  each dimension converges exponentially to a steady state risk. Unfortunately  there is
a trade-off in the sense that higher learning rates (up to 1/hi) give faster convergence to the steady
state risk  but also produce higher values of the steady-state risk. The steady state risk also decreases
proportionally to increases in batch size; this is important to note because in the following subsections 
we will show that traditional acceleration techniques (e.g.  momentum and preconditioning) help
improve the convergence rate at the expense of increasing the steady state risk. Therefore  the NQM
implies that momentum and preconditioning would beneﬁt more from large-batch training compared
to plain SGD  as shown in later sections.

3

0501001502002503003504006teSs0.00.20.40.60.81.05iskhigh curvature (0.5)lRwer curvature (0.01)decay at steS 100decay at steS 1003.2 Momentum Accelerates Training at Large Batch Sizes
Applied to the same noisy quadratic model as before  the update equations for momentum SGD are:

mi(t + 1) = mi(t) + hi✓i(t) +pci/B✏i 

✓i(t + 1) = ✓i(t)  ↵mi(t + 1).

2

)  (rt

1  rt
2))

1  rt+1

We show in the following theorem (see Appendix C for proof) that momentum SGD performs
similarly to plain SGD in the regime of small batch sizes but helps in the large-batch regime  which
can be viewed as a near-deterministic optimization problem.

time t associated with that dimension satisﬁes the upper bound

Theorem 1. Given a dimension index i  and 0  < 1 with  6= (1  p↵hi)2  the expected risk at
E [`(✓i(t))] ✓ (rt+1
where r1 and r2 (with r1  r2) are the two roots of the quadratic equation x2(1↵hi+)x+ = 0.
As with plain SGD (c.f. eqn. (4))  the loss associated with each dimension can be expressed as the
sum of two terms  where the ﬁrst one decays exponentially and corresponds to the behavior of the
deterministic version of the algorithm  and the second remains constant.
Following the existing treatment of the deterministic version of the algorithm [Chiang  1974  Qian 
1999  Yang et al.  2018  Goh  2017]  we divide our analysis two cases: overdamping and underdamp-

ing. In the case of overdamping  where < (1  p↵hi)2  both roots r1 and r2 are real and therefore
the convergence rate is determined by the larger one (i.e. r1)  which has the value

2B(2 + 2  ↵hi)(1  )

E [`(✓i(0))] +

r1  r2

(1 + )↵ci

◆2

(5)

 

(6)

(7)

r1 =

1  ↵hi +  +p(1  )2  2(1 + )↵hi + ↵2h2

2

i

With a ﬁxed learning rate  the steady state risk will be constant  and the best achievable expected risk
will be lower bounded by it. Thus  to achieve a certain target loss we must either drive the learning
rate down  or the batch size up. Assuming a small batch size and a low target risk  we are forced to
pick a small learning rate  in which case one can show2 that r1 ⇡ 1  ↵h/1. In Figure 2 we plot the
convergence rate as a function of   and we indeed observe that the convergence rate closely matches
1  ↵h/1  assuming a relative small learning rate. We further note that the convergence rate and
steady state risk of eqn. (6) are the same as the ones in plain SGD (eqn. (4))  except that they use an
"effective learning rate" of ↵/1. To help validate these predictions  in Appendix E.3 we provide a
comparison of momentum SGD with plain SGD using the effective learning rate.
In the case of underdamping where > (1  p↵hi)2 
both r1 and r2 will be complex and have norm p. We
note that the optimal  should be equal to or smaller than
(1  p↵hd)2  since otherwise all dimensions are under-

damped  and we can easily improve the convergence rate
and steady state risk by reducing .
Next we observe that the convergence of the total loss
will eventually be dominated by the slowest converging
dimension (which corresponds to the smallest curvature
hd)  and this will be in the overdamping regime as argued
above. By our analysis of the overdamping case  we can
achieve the same convergence rate for this dimension by
simply replacing the learning rate ↵ in the bound for plain
SGD (eqn. (4)) with the effective learning rate ↵/1.
So while momentum gives no long-term training acceleration for very low ﬁxed learning rates (which
we are forced to use when the batch size is small)  we note that it can help in large-batch training.
With > 0  the steady state risk roughly ampliﬁes by a factor of 1/1  and we note that steady state
risk also decreases proportionally to increases in batch size. Therefore  we expect momentum SGD
to exhibit perfect scaling up to larger batch sizes than plain SGD.

Figure 2: Convergence rate and steady state
risk (SSK) as a function of momentum for
a single dimension with ↵h = 0.0005 and
batch size B = 1.

2To see this  note that the term in the square root of eqn. (7) for r1 can be written as (1 (1+)↵hi/1)2 +

i ) term and simplifying gives the claimed expression for r1.

O(↵2h2

i ). Dropping the O(↵2h2

4

10-410-310-210-11−|r1|unGerGDmSLngRverGDmSLng1−(1−αh1−β)RStLmDl 1−β10-310-210-11001−β10-410-310-210-1100101steDGy stDte rLsk (SSK)SSK Rf SGD wLtK 0RmentumSSK Rf SGD usLng effectLve LRRStLmDl 1−β3.3 Preconditioning Further Extends Perfect Scaling to Larger Batch Sizes
Many optimizers  such as Adam and K-FAC  can be viewed as preconditioned gradient descent
methods. In each update  the gradient is rescaled by a PSD matrix P1  called the preconditioner.
(8)
In lieu of trying to construct noisy quadratic analogues of particular optimizers  we analyze precondi-
tioners of the form P = Hp with 0  p  1. Note that P remains ﬁxed throughout training since the
Hessian H is constant in the NQM. We can recover standard SGD by setting p = 0.
Conveniently  for our NQM  the dynamics of preconditioned SGD are equivalent to the SGD dynamics
in an NQM with Hessian ˜H = P1/2HP1/2 and gradient covariance ˜C = P1/2CP1/2.
Hence  the dynamics can be simulated using eqn. (4)  exactly like the non-preconditioned case. We
immediately obtain the following bound on the risk:

✓(t + 1) = ✓(t)  ↵P1 [H✓ + ✏] .

E [L(✓(t))] 

(1  ↵h(1p)

i

)2tE [`(✓i(0))] +

dXi=1

dXi=1

↵cihp

i

2B(2  ↵h1p

i

.

)

(9)

To qualitatively understand the effect of preconditioning  ﬁrst consider the ﬁrst term in eqn. (8).
The convergence of this term resembles that of gradient descent on a deterministic quadratic  which
(with optimal ↵ ⇡ 2/˜h1) converges exponentially at a rate of approximately 2/˜  where ˜ = ˜h1/˜hd
is the condition number of the transformed problem. Since ˜ = 1p  this implies a factor of p
improvement in the rate of convergence. Hence  for near-deterministic objectives where the ﬁrst term
dominates  values of p closer to 1 correspond to better preconditioners  and result in much faster
convergence. Unfortunately  there is no free lunch  as larger values of p will also increase the second
term (steady state risk). Assuming an ill-conditioned loss surface (  1)  the steady state risk of
each dimension becomes

1
2B

i

↵cihp
2  ↵h(1p)

i

ci

2Bh1

⇡

(hi/h1)p

1  (hi/h1)(1p)  

(10)

which is a monotonically increasing function with respect to p. Even without this ampliﬁcation effect 
the steady state risk will eventually become the limiting factor in the minimization of the expected
risk. One way to reduce the steady state risk  apart from using Polyak averaging [Polyak and Juditsky 
1992] or decreasing the learning rate (which will harm the rate of convergence)  is to increase the
batch size. This suggests that the beneﬁts of using stronger preconditioners will be more clearly
observed for larger batch sizes  which is an an effect that we empirically demonstrate in later sections.

3.4 Exponential Moving Average Reduces Steady State Risk
Following the same procedure as previous two sections  we analyze exponential moving averages
(EMA) on our NQM. The update rule of EMA can be written as
✓(t + 1) = ✓(t)  ↵ [H✓ + ✏]  
˜✓(t + 1) =  ˜✓(t) + (1  )✓(t + 1).

The averaged iterate ˜✓ is used at test time. The computational overhead is minimal (storing an
additional copy of the parameters  plus some cheap arithmetic operations). We now show that EMA
outperforms plain SGD by reducing the steady state risk term.
Theorem 2. Given a dimension index i  and 0  < 1  the expected risk at time t associated with
that dimension satisﬁes the upper bound

(11)

Eh`(˜✓i(t))i ✓ (rt+1

↵ci

+

1  rt+1

2

)  (1  ↵hi)(rt

1  rt
2))

r1  r2

2B(2  ↵hi)

(1  )(1 + (1  ↵hi))
(1 + )(1  (1  ↵hi))

 

◆2

E [`(✓i(0))]

(12)

where r1 = 1  ↵hi and r2 = .
By properly choosing an averaging coefﬁcient < 1  ↵hd such that r1 > r2  one can show that
EMA reduces the steady state risk without sacriﬁcing the convergence rate. To see this  we note
that the red part of eqn. (12) is strictly less than 1 given the fact 1  ↵hi < 1 while the other part is
exactly the same as the steady state risk of plain SGD.

5

(b) Fixed LR vs. Schedules

(a) Momentum and Preconditioning
Figure 3: (a) Effects of momentum and preconditioning. Steps required to reach target loss as a function of
batch size under different preconditioning power. Solid lines are momentum SGD while dashed lines are plain
SGD. The black dashed line is the information theoretic lower bound. (b) Effect of learning rate decay. The
solid lines use the optimized piecewise constant scheme  which are shown in (c) for power 0. The dashed curves
in (b) are plain SGD for comparison. We observe that learning rate schedules close most of the gap between the
ﬁxed learning rate performance and the information theoretic lower bound.

(c) Optimized LR Schedules

3.5 Choice of H and C

We’ve found that the qualitative behavior of optimizers in our NQM depends on the choices of H
and C. Therefore  we choose matrices motivated by theoretical and empirical considerations about
i=1 for some integer d  giving
neural net training. First  we set the diagonal entries of H to be { 1
a condition number of d. This closely matches the estimated eigenspectrum of the Hessian of a
convolutional network (see Figure 9 and Appendix E.4)  and is also consistent with recent work
ﬁnding heavy tailed eigenspectra of neural network Hessians [Ubaru et al.  2017  Ghorbani et al. 
2019]. We choose d = 104  which approximately matches the condition number of the K-FAC
Hessian approximation for ResNet8. (Qualitative behaviors were consistent for a wide range of d.)
We also set C = H (a nontrivial assumption). This was motivated by theoretical arguments that 
under the assumption that the implicit conditional distribution over the network’s output is close to
the conditional distribution of targets from the training distribution  the Hessian closely matches the
gradient covariance in neural network training [Martens  2014]. Empirically  this relationship appears
to hold tightly for a convolutional network and moderately well for a transformer (see Appendix E.2).

i}d

3.6

Information Theoretic Lower Bound

Since our NQM assumes the inﬁnite data (online optimization) setting  it’s instructive to compare
the performance of optimizers against an information theoretic lower bound. Speciﬁcally  under the
assumption that H = C  the NQM is equivalent to maximum likelihood estimation of the mean
vector for a multivariate Gaussian distribution with covariance H1. Hence  the risk obtained by any
optimizer can be bounded below by the risk of the maximum likelihood estimator for the Gaussian 
which is d/2N  where d is the dimension and N is the total number of training examples visited. We
indicate this bound with a dashed black line in our plots.

3.7 Noisy Quadratic Experiments

In this section  we simulate noisy quadratic optimization using the closed-form dynamics. Our aim is
to formulate hypotheses for how different optimizers would behave for neural network optimization.
Our main metric is the number of steps required to achieve a target risk. For efﬁciency  rather than
explicitly representing all the eigenvalues of H  we quantize them into 100 bins and count the number
of eigenvalues in each bin. Unless otherwise speciﬁed  we initialize ✓ as N (0  I) and use a target
risk of 0.01. (The results don’t seem to be sensitive to either the initial variance or the target risk;
some results with varying target risk thresholds are shown in Appendix E.5).

3.7.1 Effect of Momentum  Preconditioning and Exponential Moving Average
We ﬁrst experiment with momentum and varying preconditioner powers on our NQM. We treat both
the (ﬁxed) learning rate ↵ and momentum decay parameter  as hyperparameters  which we tune
using a ﬁne-grained grid search.
Consistent with the empirical results of Shallue et al. [2018]  each optimizer shows two distinct
regimes: a small-batch (stochastic) regime with perfect linear scaling  and a large-batch (deterministic)

6

22242628210212214216218220Batch size2-220222426282102122142162182206teSs to thresholdSow 0Sow 0.25Sow 0.5Sow 0.75lower bound22242628210212214216218220Batch size2-220222426282102122142162182206teSs to thresholdSow 0Sow 0.25Sow 0.5Sow 0.75lower bound010203040503Leces2-92-82-72-62-52-42-32-22-12021LearnLng 5ateB6 16B6 32B6 64B6 128B6 256B6 512B6 1024B6 2048B6 4096Remarks
Same as Shallue et al. [2018] except
without dropout regularization.

LR

Constant

Same as Shallue et al. [2018].
Ghost batch norm is used.
Ghost batch norm is used.
Shallow model in Shallue et al. [2018]

Constant
Linear Decay
Linear Decay
Constant

Data Set
MNIST
FMNIST

Size
55 000
55 000

CIFAR10

45 000

Model

Simple CNN

ResNet8 without BN
ResNet32 with BN
VGG11 with BN

LM1B
Table 1: Data sets and models used in our experiments. See Appendix F.2 for full details.

⇠30M Two-layer Transformer

regime insensitive to batch size. We call the phase transition between these regimes the critical batch
size. Consistent with the analysis of Section 3.2 and the observations of Smith et al. [2018]  Shallue
et al. [2018]  Kidambi et al. [2018]  the performance of momentum-based optimizers matches that of
the plain SGD methods in the small-batch regime  but momentum increases the critical batch size
and gives substantial speedups in the large batch regime. Preconditioning also increases the critical
batch size and gives substantial speedups in the large batch regime  but interestingly  also improves
performance by a small constant factor even for very small batches. Combining momentum with
preconditioning extends both of these trends.
We next experiment with EMA and varying preconditioning pow-
ers on our NQM. Following the same procedure as before  we
tune both learning rate ↵ and averaging coefﬁcient  using grid
search. As expected  EMA reduces the number of steps required
especially for plain SGD with preconditioning power 0. Another
interesting observation is that EMA becomes redundant in the
large batch (near-deterministic) regime since the main effect of
EMA is reducing the steady-state risk  which can also be done by
increasing the batch size. This implies that EMA would reduce
the critical batch size and therefore achieve the same amount of
acceleration with less computation.

Figure 4: Effects of exponential
moving average (EMA). Solid lines
are SGD with EMA while dashed
lines are plain SGD.

3.7.2 Optimal Learning Rate and Decay Scheme

In the NQM  we can calculate the optimal constant learning rate given a speciﬁc batch size. Figure 14
shows the optimal learning rate as a function of batch size for a target risk of 0.01. Notably  the
optimal learning rate of plain (preconditioned) SGD (Figure 14a) scales linearly with batch size
before it hits the critical batch size  matching the scheme used in Goyal et al. [2017]. The linear
scaling also holds for the effective learning rate of momentum SGD. In the small batch regime  the
optimal effective learning rate for momentum SGD matches the optimal plain SGD learning rate 
suggesting that the momentum and learning rate are interchangeable in the small batch regime.
While a ﬁxed learning rate often works well for simple problems  good performance on the ImageNet
benchmark [Russakovsky et al.  2015] requires a carefully tuned schedule. Here we explicitly
optimize a piecewise constant learning rate schedule for SGD (with 50 pieces)  in terms of the number
of steps to reach the loss threshold.3 In Figure 3b  we show that optimized learning rate schedules
help signiﬁcantly in the small batch regime  consistent with the analysis in Wu et al. [2018]. We
observe the same linear scaling as with ﬁxed-learning-rate SGD  but with a better constant factor.
In fact  optimized schedules nearly achieve the information theoretic optimum. However  learning
rate schedules do not improve at all over ﬁxed learning rates in the large batch regime. Figure 3c
shows optimized schedules for different batch sizes; interestingly  they maintain a large learning
rate throughout training followed by a roughly exponential decay  consistent with commonly used
neural network training schedules. Additionally  even though the different batch sizes start with the
same learning rate  their ﬁnal learning rates at the end of training scale linearly with batch size (see
Figure 15 in Appendix E.7).

3For a given schedule and number of time steps  we obtain the exact risk using dynamic programming with
eqn. (3). For stability  the learning rates are constrained to be at most 2/h1. For a ﬁxed number of time steps  we
minimize this risk using BFGS. We determine the optimal number of time steps using binary search.

7

22242628210212214216218220Batch size2-220222426282102122142162182206teSs to thresholdSow 0Sow 0.25Sow 0.5Sow 0.75lower bound(a) Simple CNN on MNIST

(b) Simple CNN on Fashion MNIST

(c) ResNet8 on CIFAR10

(d) VGG11 on CIFAR10

(e) ResNet32 on CIFAR10

(f) Transformer on LM1B

Figure 5: Empirical relationship between batch size and steps to result. Key observations: 1) momentum
SGD has no beneﬁt over plain SGD at small batch sizes  but extends the perfect scaling to larger batch sizes; 2)
preconditioning also extends perfect scaling to larger batch sizes  i.e. K-FAC > Adam > momentum SGD. This
is most noticeable in the Transformer model; 3) preconditioning (particularly K-FAC) reduces the number of
steps needed to reach the target even for small batch sizes. All of these agree with the predictions by NQM.
4 Neural Network Experiments

We investigated whether the predictions made by the NQM hold in practice by running experiments
with ﬁve neural network architectures across three image classiﬁcation tasks and one language
modeling task (see Table 1). For each model and task  we compared a range of optimizers: SGD 
momentum SGD  Adam (with and without momentum)  and K-FAC (with and without momentum).
For K-FAC  preconditioning is applied before momentum. See Appendix F for more details.
The primary quantity we measured is the number of steps required to reach a target accuracy (for
image classiﬁcation tasks) or cross entropy (for language modeling). Unless otherwise speciﬁed  we
measured steps to target on the validation set. We chose the target metric values based on an initial
set of experiments with practical computational budgets. For each model  task  optimizer  and batch
size  we independently tuned the learning rate ↵  the parameters governing the learning rate schedule
(where applicable)  and optimizer-speciﬁc metaparameters (see Appendix F.4). We manually chose
the search spaces based on our initial experiments  and we veriﬁed after each experiment that the
optimal metaparameter values were far from the search space boundaries. We used quasi-random
search [Bousquet et al.  2017] to tune the metaparameters with ﬁxed budgets of non-divergent4 trials
(100 for Simple CNN  ResNet8  and Transformer  and 200 for ResNet32 and VGG11). We chose the
trial that reached the target metric value using the fewest number of steps.

4.1 Critical Batch Size Depends on the Optimizer
Figure 5 shows the relationship between batch size and steps to target for each model  task  and
optimizer. In each case  as the batch size grows  there is an initial period of perfect scaling where
doubling the batch size halves the steps to target  but once the batch size exceeds a problem-dependent
critical batch size  there are rapidly diminishing returns  matching the results of [Goyal et al.  2017 
McCandlish et al.  2018  Shallue et al.  2018]. K-FAC has the largest critical batch size in all
cases  highlighting the usefulness of preconditioning. Momentum SGD extends perfect scaling to
larger batch sizes than plain SGD  but for batch sizes smaller than the plain SGD critical batch size 
momentum SGD requires as many steps as plain SGD to reach the target. This is consistent with
both the empirical results of Shallue et al. [2018] and our NQM simulations. By contrast  Adam and
K-FAC can reduce the number of steps needed to reach the target compared to plain SGD even for the
smallest batch sizes  although neither optimizer does so in all cases. Finally  we see some evidence
that the beneﬁt of momentum diminishes with preconditioning (Figures 5a and 5b)  as predicted by
our NQM simulations  although we do not see this in all cases (e.g. Figure 5c and 5f).

4We discarded trials with a divergent training loss  which occurred when the learning rate was too high.

8

242526272829210211212213Batch size2425262728292102112122132146teSs to target7arget Accuracy: 0.992242526272829210211212213Batch size25262728292102112122132146teSs to target7arget Accuracy: 0.920242526272829210211212213Batch size28292102112122132142152162176teSs to target7arget Accuracy: 0.800sgdheavy balladam w/o momentumadamkfac w/o momentumkfac2526272829210211212Batch size2102112122132142152162172186teSs to target7arget Accuracy: 0.91026272829210211212213Batch size2102112122132142152162176teSs to target7arget Accuracy: 0.9302628210212214216218Batch 6ize292112132152172192216teSs to target7arget cross entroSy: 3.904.2 Exponential Moving Average Improves Convergence with Minimal Computation Cost
To verify the predictions of NQM on
exponential moving average (EMA) 
we conducted some experiments on
comparing EMA with plain SGD. We
follow the same protocol of Figure 5
and report the results in Figure 6. As
expected  the results on real neural net-
works closely match our predictions
based on NQM analysis. In particular 
SGD with EMA appears to reach the
same target with fewer steps than plain SGD at small batch sizes  though the beneﬁt of EMA dimin-
ishes with large batch sizes. Besides  we note that EMA leads to smaller critical batch sizes and
achieves the same acceleration with less computation.

Figure 6: Steps to training accuracy versus batch size. Left:
ResNet8 on CIFAR10; Right: Simple CNN on MNIST.

4.3 Optimal Learning Rate
The NQM predicts that the optimal
constant learning rate for plain SGD
(or effective learning rate for momen-
tum SGD) scales linearly with batch
size initially  and then levels off after
a certain batch size. Figure 7 shows
the empirical optimal (effective) learn-
Figure 7: Optimal learning rates for plain SGD and momentum
ing rate as a function of batch size for
SGD. Left: Simple CNN on MNIST; Right: ResNet8 on CIFAR10
simple CNN on MNIST and ResNet8
on CIFAR10. For small batch sizes  the optimal learning rate of plain SGD appears to match the
optimal effective learning rate of momentum SGD. However  after a certain batch size  the optimal
learning rate for plain SGD saturates while the optimal effective learning rate of momentum SGD
keeps increasing. Interestingly  plain SGD and momentum SGD appear to deviate at the same batch
size in the optimal effective learning rate and steps to target plots (Figures 5 and 7).

4.4 Steps to Target on the Training Set
Figure 8 shows the empirical rela-
tionship between batch size and steps
to target  measured on the training
set  for ResNet8 and ResNet32 on CI-
FAR10. For ResNet8  the curves are
almost identical to those using vali-
dation accuracy (Figure 5c)  but for
ResNet32  the gaps between different
optimizers become much smaller than
in Figure 5e and the effects of momen-
tum and preconditioning appear to become less signiﬁcant. Nevertheless  the qualitative differences
between optimizers are consistent with the validation set measurements.

Figure 8: Steps to training accuracy versus batch size on CIFAR10.
Left: ResNet8; Right: ResNet32.

5 Conclusion

In this work  we analyzed the interactions between the batch size and the optimization algorithm
from two perspectives: experiments with real neural networks  and a noisy quadratic model with
parameters chosen based on empirical observations about neural networks. Despite its simplicity  the
noisy quadratic model agrees remarkably well with a variety of neural network training phenomena 
including learning rate scaling  critical batch sizes  and the effects of momentum  preconditioning
and averaging. More importantly  the noisy quadratic model allows us to run experiments in seconds 
while it can take weeks  or even months  to conduct careful large-scale experiments with real neural
networks. Therefore  the noisy quadratic model is a convenient and powerful way to quickly formulate
testable predictions about neural network optimization.

9

242526272829210211212213Batch size2102112122132142152162176teSs to target7arget Accuracy: 0.800242526272829210211Batch size2728292102112126teSs to target7arget Accuracy: 0.990sgdsgd with ema242526272829210211212213Batch sLze2-42-32-22-1202122232425262ptLmaO (effectLve) L5242526272829210211212213Batch sLze2-62-52-42-32-22-120212223242ptLmaO (effectLve) L5sgdheavy baOO242526272829210211212213Batch size28292102112122132142152162176teSs to target7arget Accuracy: 0.830sgdheavy balladam w/o momentumadamkfac w/o momentumkfac26272829210211212213Batch size292102112122132142152166teSs to target7arget Accuracy: 0.990Acknowledgements

RG acknowledges support from the CIFAR Canadian AI Chairs program and the Ontario MRIS Early
Researcher Award.

References
Jimmy Ba  Roger Grosse  and James Martens. Distributed second-order optimization using Kronecker-

factored approximations. In International Conference on Learning Representations  2017.

Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with
convergence rate o (1/n). In Advances in neural information processing systems  pages 773–781 
2013.

Juhan Bae  Guodong Zhang  and Roger Grosse. Eigenvalue corrected noisy natural gradient. In
Workshop of Bayesian Deep Learning  Advances in neural information processing systems  2018.

Lukas Balles  Javier Romero  and Philipp Hennig. Coupling adaptive batch sizes with learning rates.

In Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 2017. AUAI Press  2017.

Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural

information processing systems  pages 161–168  2008.

Olivier Bousquet  Sylvain Gelly  Karol Kurach  Olivier Teytaud  and Damien Vincent. Critical

hyper-parameters: No random  no cry. arXiv preprint arXiv:1706.03200  2017.

Lingjiao Chen  Hongyi Wang  Jinman Zhao  Dimitris Papailiopoulos  and Paraschos Koutris. The
In Advances in Neural

effect of network width on the performance of large-batch training.
Information Processing Systems  pages 9302–9309  2018.

A.C. Chiang. Fundamental Methods of Mathematical Economics. International student edition.

McGraw-Hill  1974. ISBN 9780070107809.

Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes.

THE ANNALS  44(4):1363–1399  2016.

Simon S. Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations 
2019. URL https://openreview.net/forum?id=S1eK3i09YQ.

Thomas George  César Laurent  Xavier Bouthillier  Nicolas Ballas  and Pascal Vincent. Fast
approximate natural gradient descent in a Kronecker-factored eigenbasis. In Advances in Neural
Information Processing Systems  pages 9550–9560  2018.

Behrooz Ghorbani  Shankar Krishnan  and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In Proceedings of the 36th International Conference on Machine
Learning  pages 2232–2241  2019.

Gabriel Goh. Why momentum really works. Distill  2(4):e6  2017.

Noah Golmant  Nikita Vemuri  Zhewei Yao  Vladimir Feinberg  Amir Gholami  Kai Rothauge 
Michael W Mahoney  and Joseph Gonzalez. On the computational inefﬁciency of large batch sizes
for stochastic gradient descent. arXiv preprint arXiv:1811.12941  2018.

I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press  2016. http://www.

deeplearningbook.org.

Priya Goyal  Piotr Dollár  Ross Girshick  Pieter Noordhuis  Lukasz Wesolowski  Aapo Kyrola 
Andrew Tulloch  Yangqing Jia  and Kaiming He. Accurate  large minibatch SGD: Training
Imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

Roger Grosse and James Martens. A kronecker-factored approximate ﬁsher matrix for convolution

layers. In International Conference on Machine Learning  pages 573–582  2016.

10

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 770–778  2016.

Elad Hoffer  Itay Hubara  and Daniel Soudry. Train longer  generalize better: Closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems  pages 1731–1741  2017.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8571–8580  2018.

Prateek Jain  Sham M Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford. Parallelizing
stochastic gradient descent for least squares regression: mini-batching  averaging  and model
misspeciﬁcation. Journal of Machine Learning Research  18(223):1–42  2018.

Stanisław Jastrz˛ebski  Zachary Kenton  Devansh Arpit  Nicolas Ballas  Asja Fischer  Yoshua Bengio 
and Amos Storkey. Three factors inﬂuencing minima in SGD. In International Conference on
Artiﬁcial Neural Networks  2018.

Rahul Kidambi  Praneeth Netrapalli  Prateek Jain  and Sham Kakade. On the insufﬁciency of existing
momentum schemes for stochastic optimization. In 2018 Information Theory and Applications
Workshop (ITA)  pages 1–9. IEEE  2018.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations  2014.

Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720  2019.

Todd K. Leen and Genevieve B. Orr. Optimal stochastic search and adaptive momentum.

In
J. D. Cowan  G. Tesauro  and J. Alspector  editors  Advances in Neural Information Processing
Systems 6  pages 477–484. Morgan-Kaufmann  1994. URL http://papers.nips.cc/paper/
772-optimal-stochastic-search-and-adaptive-momentum.pdf.

Siyuan Ma  Raef Bassily  and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of SGD in modern over-parametrized learning. In International Conference on
Machine Learning  pages 3331–3340  2018.

James Martens. New insights and perspectives on the natural gradient method. arXiv preprint

arXiv:1412.1193  2014.

James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate

curvature. In International Conference on Machine Learning  pages 2408–2417  2015.

Sam McCandlish  Jared Kaplan  Dario Amodei  and OpenAI Dota Team. An empirical model of

large-batch training. arXiv preprint arXiv:1812.06162  2018.

Kazuki Osawa  Yohei Tsuji  Yuichiro Ueno  Akira Naruse  Rio Yokota  and Satoshi Matsuoka.
Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35
epochs. arXiv preprint arXiv:1811.12019  2018.

Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization  30(4):838–855  1992.

Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks  12(1):

145–151  1999.

11

Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. ImageNet large scale visual recognition
challenge. International Journal of Computer Vision  115(3):211–252  2015.

Levent Sagun  Leon Bottou  and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity

and beyond. arXiv preprint arXiv:1611.07476  2016.

Tom Schaul  Sixin Zhang  and Yann LeCun. No more pesky learning rates.

Conference on Machine Learning  pages 343–351  2013.

In International

Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.

Neural computation  14(7):1723–1738  2002.

Christopher J Shallue  Jaehoon Lee  Joe Antognini  Jascha Sohl-Dickstein  Roy Frostig  and George E
Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint
arXiv:1811.03600  2018.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. In International Conference on Learning Representations  2015.

Samuel L. Smith  Pieter-Jan Kindermans  and Quoc V. Le. Don’t decay the learning rate  increase
the batch size. In International Conference on Learning Representations  2018. URL https:
//openreview.net/forum?id=B1Yy1BxCZ.

Samuel L Smith  Erich Elsen  and Soham De. Momentum enables large batch training. In Theoretical

Physics for Deep Learning Workshop  International Conference on Machine Learning.  2019.

Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 2818–2826  2016.

Shashanka Ubaru  Jie Chen  and Yousef Saad. Fast estimation of tr(f(a)) via stochastic lanczos

quadrature. SIAM Journal on Matrix Analysis and Applications  38(4):1075–1099  2017.

Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint

arXiv:1706.05350  2017.

Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
In Advances in neural information

Kaiser  and Illia Polosukhin. Attention is all you need.
processing systems  pages 5998–6008  2017.

Chaoqi Wang  Roger Grosse  Sanja Fidler  and Guodong Zhang. Eigendamage: Structured pruning
in the Kronecker-factored eigenbasis. In Proceedings of the 36th International Conference on
Machine Learning  pages 6566–6575  2019.

Yuhuai Wu  Mengye Ren  Renjie Liao  and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In International Conference on Learning Representations  2018.
URL https://openreview.net/forum?id=H1MczcgR-.

Lin Yang  Raman Arora  Tuo Zhao  et al. The physical systems behind optimization algorithms. In

Advances in Neural Information Processing Systems  pages 4372–4381  2018.

Dong Yin  Ashwin Pananjady  Max Lam  Dimitris Papailiopoulos  Kannan Ramchandran  and Peter
Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International
Conference on Artiﬁcial Intelligence and Statistics  pages 1998–2007  2018.

Kun Yuan  Bicheng Ying  and Ali H. Sayed. On the inﬂuence of momentum acceleration on
online learning. Journal of Machine Learning Research  17(192):1–66  2016. URL http:
//jmlr.org/papers/v17/16-157.html.

Guodong Zhang  James Martens  and Roger Grosse. Fast convergence of natural gradient descent for

overparameterized neural networks. arXiv preprint arXiv:1905.10961  2019a.

Guodong Zhang  Chaoqi Wang  Bowen Xu  and Roger Grosse. Three mechanisms of weight
decay regularization. In International Conference on Learning Representations  2019b. URL
https://openreview.net/forum?id=B1lz-3Rct7.

12

,Guodong Zhang
Lala Li
Zachary Nado
James Martens
Sushant Sachdeva
George Dahl
Chris Shallue
Roger Grosse