2012,Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems,We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system.  Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently  an asymptotic regret bound of $\tilde{O}(\sqrt{T})$ was shown for $T \gg p$ where $p$ is the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that for $p \gg 1$ and $T \gg \polylog(p)$ achieves a regret bound of $\tilde{O}(p \sqrt{T})$. In particular  our algorithm has an average cost of $(1+\eps)$ times the optimum cost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previous work on the dense dynamics where the algorithm needs $\Omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy. We believe our result has prominent applications in the emerging area of computational advertising  in particular targeted online advertising and advertising in social networks.,Efﬁcient Reinforcement Learning for High

Dimensional Linear Quadratic Systems

Morteza Ibrahimi
Stanford University
Stanford  CA 94305

ibrahimi@stanford.edu

Adel Javanmard
Stanford University
Stanford  CA 94305

adelj@stanford.edu

Benjamin Van Roy
Stanford University
Stanford  CA 94305

bvr@stanford.edu

Abstract

We study the problem of adaptive control of a high dimensional linear quadratic
(LQ) system. Previous work established the asymptotic convergence to an optimal
√
controller for various adaptive control schemes. More recently  for the average
cost LQ problem  a regret bound of O(
T ) was shown  apart form logarithmic
factors. However  this bound scales exponentially with p  the dimension of the
state space. In this work we consider the case where the matrices describing the
dynamic of the LQ system are sparse and their dimensions are large. We present
an adaptive control scheme that achieves a regret bound of O(p
T )  apart from
logarithmic factors. In particular  our algorithm has an average cost of (1 + )
times the optimum cost after T = polylog(p)O(1/2). This is in comparison to
previous work on the dense dynamics where the algorithm requires time that scales
exponentially with dimension in order to achieve regret of  times the optimal cost.
We believe that our result has prominent applications in the emerging area of
computational advertising  in particular targeted online advertising and advertising
in social networks.

√

1

Introduction

In this paper we address the problem of adaptive control of a high dimensional linear quadratic (LQ)
system. Formally  the dynamics of a linear quadratic system are given by
x(t + 1) = A0x(t) + B0u(t) + w(t + 1) 

c(t) = x(t)T Qx(t) + u(t)T Ru(t) 

(1)
where u(t) ∈ Rr is the control (action) at time t  x(t) ∈ Rp is the state at time t  c(t) ∈ R is
the cost at time t  and {w(t + 1)}t≥0 is a sequence of random vectors in Rp with i.i.d. standard
Normal entries. The matrices Q ∈ Rp×p and R ∈ Rr×r are positive semi-deﬁnite (PSD) matrices
that determine the cost at each step. The evolution of the system is described through the matrices
A0 ∈ Rp×p and B0 ∈ Rp×r. Finally by high dimensional system we mean the case where p  r (cid:29) 1.
A celebrated fundamental theorem in control theory asserts that the above LQ system can be op-
timally controlled by a simple linear feedback if the pair (A0  B0) is controllable and the pair
(A0  Q1/2) is observable. The optimal controller can be explicitly computed from the matrices
describing the dynamics and the cost. Throughout this paper we assume that controllability and
observability conditions hold.
When the matrix Θ0 ≡ [A0  B0] is unknown  the task is that of adaptive control  where the system
is to be learned and controlled at the same time. Early works on the adaptive control of LQ systems
relied on the certainty equivalence principle [2]. In this scheme at each time t the unknown param-
eter Θ0 is estimated based on the observations collected so far and the optimal controller for the

1

estimated system is applied. Such controllers are shown to converge to an optimal controller in the
case of minimum variance cost  however  in general they may converge to a suboptimal controller
[11]. Subsequently  it has been shown that introducing random exploration by adding noise to the
control signal  e.g.  [14]  solves the problem of converging to suboptimal estimates.
All the aforementioned work have been concerned with the asymptotic convergence of the controller
to an optimal controller. In order to achieve regret bounds  cost-biased parameter estimation [12  8 
1]  in particular the optimism in the face of uncertainty (OFU) principle [13] has been shown to be
effective. In this method a conﬁdence set S is found such that Θ0 ∈ S with high probability. The

system is then controlled using the most optimistic parameter estimates  i.e. (cid:98)Θ ∈ S with the smallest

optimum cost. The asymptotic convergence of the average cost of OFU for the LQR problem was
shown in [6]. This asymptotic result was extended in [1] by providing a bound for the cumulative
regret. Assume x(0) = 0 and for a control policy π deﬁne the average cost

Jπ = limsup
T→∞

1
T

T(cid:88)

t=0

E[ct] .

(2)

(3)

Further  deﬁne the cumulative regret as

R(T ) =

T(cid:88)

(cπ(t) − J∗)  

t=0

√

√
where cπ(t) is the cost of control policy π at time t and J∗ = J(Θ0) is the optimal average cost.
The algorithm proposed in [1] is shown to have cumulative regret ˜O(
T ) where ˜O is hiding the
√
logarithmic factors. While no lower bound was provided for the regret  comparison with the multi-
armed bandit problem where a lower bound of O(
T ) was shown for the general case [9]  suggests
that this scaling with time for the cumulative regret is optimal.
The focus of [1] was on scaling of the regret with time horizon T . However  the regret of the pro-
posed algorithm scales poorly with dimension. More speciﬁcally  the analysis in [1] proves a regret
bound of R(T ) < Cpp+r+2
T . The current paper focuses on (many) applications where the state
and control dimensions are much larger than the time horizon of interest. A powerful reinforcement
learning algorithm for these applications should have regret which depends gracefully on dimension.
In general  there is little to be achieved when T < p as the number of degrees of freedom (pr + p2)
is larger than the number of observations (T p) and any estimator can be arbitrary inaccurate. How-
ever  when there is prior knowledge about the unknown parameters A0  B0  e.g.  when A0  B0 are
sparse  accurate estimation can be feasible. In particular  [3] proved that under suitable conditions
the unknown parameters of a noise driven system (i.e.  no control) whose dynamics are modeled by
linear stochastic differential equations can be estimated accurately with as few as O(log(p)) sam-
ples. However  the result of [3] is not directly applicable here since for a general feedback gain L
even if A0 and B0 are sparse  the closed loop gain A0 − B0L need not be sparse. Furthermore 
system dynamics would be correlated with past observations through the estimated gain matrix L.
Finally  there is no notion of cost in [3] while here we have to obtain bounds on cost and its scaling
with p.
In this work we extend the result of [3] by showing that under suitable conditions  un-
known parameters of sparse high dimensional LQ systems can be accurately estimated with as few
as O(log(p + r)) observations. Equipped with this efﬁcient learning method  we show that sparse
high dimensional LQ systems can be adaptively controlled with regret ˜O(p
To put this result in perspective note that even when x(t) = 0  the expected cost at time t + 1 is
Ω(p) due to the noise. Therefore  the cumulative cost at time T is bounded as Ω(pT ). Comparing
2 )  the cumulative cost of our algorithm
this to our regret bound  we see that for T = polylog(p)O( 1
is bounded by (1 + ) times the optimum cumulative cost. In other words  our algorithm performs
close to optimal after polylog(p) steps. This is in contrast with the result of [1] where the algorithm
needs Ω(p2p) steps in order to achieve regret of  times the optimal cost.
Sparse high dimensional LQ systems appear in many engineering applications. Here we are par-
ticularly motivated by an emerging ﬁeld of applications in marketing and advertising. The use of
dynamical optimal control models in advertising has a history of at least four decades  cf. [17  10]
for a survey. In these models  often a partial differential equation is used to describe how advertising
expenditure over time translates into sales. The basic problem is to ﬁnd the advertising expendi-
ture that maximizes the net proﬁt. The focus of these works is to model the temporal dynamics of

T ).

√

2

the advertising expenditure (the control variable) and the variables of interest (sales  goodwill level 
etc.). There also exists a rich literature studying the spatial interdependence of consumers’ and
ﬁrms’ behavior to devise marketing schemes [7]. In these models space can be generalized beyond
geographies to include notions like demographies and psychometry.
Combination of spatial interdependence and temporal dynamics models for optimal advertising was
also considered [16  15]. A simple temporal dynamics model is extended in [15] by allowing state
and control variables to have spatial dependence and introducing a diffusive component in the con-
trolled PDE which describes the spatial dynamics. The controlled PDE is then showed to be equiv-
alent to an abstract linear control system of the form

dx(t)

dt

= Ax(t) + Bu(t).

(4)

Both [15] and [7] are concerned with the optimal control and the interactions are either dictated
by the model or assumed known. Our work deals with a discrete and noisy version of (4) where
the dynamics is to be estimated but is known to be sparse. In the model considered in [15] the
state variable x lives in an inﬁnite dimensional space. Spatial models in marketing [7] usually
consider state variables which have a large number of dimensions  e.g.  number of zip codes in the
US (∼ 50K). High dimensional state space and control is a recurring theme in these applications.
In particular  with the modern social networks customers are classiﬁed in a highly granular way  po-
tentially with each customer representing his own class. With the number of classes and complexity
of their interactions  its unlikely that we could formulate an effective model a priori for how classes
interact. Further  the nature of these interactions change over time with the changing landscape of
Internet services and information available to customers. This makes it important to efﬁciently learn
from real-time data about the nature of these interactions.
Notation: We bundle the unknown parameters into one variable Θ0 = [A0  B0] ∈ Rp×q where
q = p + r and call it the interaction matrix. For v ∈ Rn  M ∈ Rm×n and p ≥ 1  we denote by (cid:107)v(cid:107)p
the standard p-norm and by (cid:107)M(cid:107)p the corresponding operator norm. For 1 ≤ i ≤ m  Mi represents
the ith row of matrix M. For S ⊆ [m]  J ⊆ [n]  MSJ is the submatrix of M formed by the rows in
S and columns in J. For a set S denote by |S| its cardinality. For an integer n denote by [n] the set
{1  . . .   n}.

2 Algorithm

Our algorithm employs the Optimism in the Face of Uncertainty (OFU) principle in an episodic
fashion. At the beginning of episode i the algorithm constructs a conﬁdence set Ω(i) which is
guaranteed to include the unknown parameter Θ0 with high probability. The algorithm then chooses

(cid:101)Θ(i) ∈ Ω(i) that has the smallest expected cost as the estimated parameter for episode i and applies

the optimal control for the estimated parameter during episode i.
The conﬁdence set is constructed using observations from the last episode only but the length of
episodes are chosen to increase geometrically allowing for more accurate estimates and shrinkage
of the conﬁdence set by a constant factor at each episode. The details of each step and the pseudo
code for the algorithm follows.
Constructing conﬁdence set: Deﬁne τi to be the start of episode i with τ0 = 0. Let L(i) be the
controller that has been chosen for episode i. For t ∈ [τi  τi+1) the system is controlled by u(t) =
−L(i)x(t) and the system dynamics can be written as x(t + 1) = (A0 − B0L(i))x(t) + w(t + 1). At
optimization problem for each row Θu ∈ Rq separately:

the beginning of episode i + 1  ﬁrst an initial estimate(cid:98)Θ is obtained by solving the following convex

∈ argmin L(Θu) + λ(cid:107)Θu(cid:107)1 

u

(cid:98)Θ(i+1)
τi+1−1(cid:88)

t=τi

3

where

L(Θu) =

1

2∆τi+1

{xu(t + 1) − Θu(cid:101)L(i)x(t)}2  ∆τi+1 = τi+1 − τi 

(5)

(6)

(cid:19)

(cid:18) 1

j (cid:107)2)  and
4 · 103 k2(cid:96)2
α(1 − ρ)C 2
4 · 103 k2(cid:96)(Θ0  )2

min

0

Output: Series of estimates(cid:101)Θ(i)  conﬁdence sets Ω(i) and controllers L(i)

ALGORITHM: Reinforcement learning algorithm for LQ systems.
Input: Precision   failure probability 4δ  initial (ρ  Cmin  α) identiﬁable controller L(0)  (cid:96)(Θ0  )
1: Let (cid:96)0 = max(1  maxj∈[r] (cid:107)L(0)

k

log(

) 

n0 =

n1 =

4kq
δ

2 +

(cid:18) 1

(1 − ρ)2
k

(cid:19)
Let ∆τ0 = n0  ∆τi = 4i(1 + i/ log(q/δ))n1 for i ≥ 1  and τi =(cid:80)i
Calculate the estimate(cid:98)Θ(i+1) from (5) and construct the conﬁdence set Ω(i+1).
Apply the control u(t) = −L(i)x(t) until τi+1 − 1 and observe the trace {x(t)}τi≤t<τi+1.
Calculate(cid:101)Θ(i+1) from (9) and set L(i+1) ← L((cid:101)Θ(i+1)).

2: for i = 0  1  2  . . . do
3:
4:
5:

(1 − ρ)C 2

(1 − ρ)2

j=0 ∆τj.

4kq
δ

2 +

log(

).

min

and (cid:101)L(i) = [I −L(i)T]T. The estimator (cid:98)Θu is known as the LASSO estimator. The ﬁrst term

in the cost function is the normalized negative log likelihood which measures the ﬁdelity to the
observations while the second term imposes the sparsity constraint on Θu. λ is the regularization
parameter.
For Θ(1)  Θ(2) ∈ Rp×q deﬁne the distance d(Θ(1)  Θ(2)) as
(cid:107)Θ(1)

d(Θ(1)  Θ(2)) = max
u∈[p]
where Θu is the uth row of the matrix Θ.
It is worth noting that for k-sparse matrices with k
√
constant  this distance does not scale with p or q. In particular  if the absolute value of the elements
of Θ(1) and Θ(2) are bounded by Θmax then d(Θ(1)  Θ(2)) ≤ 2

Having the estimator(cid:98)Θ(i) the algorithm constructs the conﬁdence set for episode i as

u − Θ(2)

u (cid:107)2 

kΘmax.

(7)

Ω(i) = {Θ ∈ Rp×q | d(Θ (cid:98)Θ(i)) ≤ 2−i} 

(8)
where  > 0 is an input parameter to the algorithm. For any ﬁxed δ > 0  by choosing τi judiciously
we ensure that with probability at least 1 − δ  Θ0 ∈ Ω(i)  for all i ≥ 1. (see Theorem 3.2).
Design of the controller: Let J(Θ) be the minimum expected cost if the interaction matrix is
Θ = [A  B] and denote by L(Θ) the optimal controller that achieves the expected cost J(Θ). The

algorithm implements OFU principle by choosing  at the beginning of episode i  an estimate(cid:101)Θ(i) ∈

(9)

The optimal control corresponding to (cid:101)Θ(i) is then applied during episode i 
−L((cid:101)Θ(i))x(t) for t ∈ [τi  τi+1). Recall that for Θ = [A  B]  the optimal controller is given through

i.e.  u(t) =

Θ∈Ω(i)

the following relations

K(Θ) = Q + ATK(Θ)A − ATK(Θ)B(BTK(Θ)B + R)−1BTK(Θ)A  
L(Θ) = (BTK(Θ)B + R)−1BTK(Θ)A .

(Riccati equation)

The pseudo code for the algorithm is summarized in the table.

3 Main Results

In this section we present performance guarantees in terms of cumulative regret and learning ac-
curacy for the presented algorithm. In order to state the theorems  we ﬁrst need to present some
assumptions on the system.

4

Ω(i) such that

(cid:101)Θ(i) ∈ argmin

J(Θ).

Λ − Θ(cid:101)LΛ(cid:101)LTΘT = I.

Given Θ ∈ Rp×q and L ∈ Rr×p  deﬁne(cid:101)L = [I −LT]T ∈ Rq×p and let Λ ∈ Rp×p be a solution to

the following Lyapunov equation

(10)
If the closed loop system (A0 − B0L) is stable then the solution to the above equation exists and the
state vector x(t) has a Normal stationary distribution with covariance Λ.
We proceed by introducing an identiﬁable regulator.

Deﬁnition 3.1. For a k-sparse matrix Θ0 = [A0  B0] ∈ Rp×q and L ∈ Rr×p  deﬁne (cid:101)L =
[I −LT]T ∈ Rq×p and let H = (cid:101)LΛ(cid:101)LT where Λ is the solution of Eq. (10) with Θ = Θ0. De-

ﬁne L to be (ρ  Cmin  α) identiﬁable (with respect to Θ0) if it satisﬁes the following conditions for
all S ⊆ [q]  |S| ≤ k.

(1) (cid:107)A0 − B0L(cid:107)2 ≤ ρ < 1 

(2) λmin(HSS) ≥ Cmin 

(3) (cid:107)HScSH−1

SS(cid:107)∞ ≤ 1 − α.

The ﬁrst condition simply states that if the system is controlled using the regulator L then the closed
loop autonomous system is asymptotically stable. The second and third conditions are similar to
what is referred to in the sparse signal recovery literature as the mutual incoherence or irreprep-
resentable conditions. Various examples and results exist for the matrix families that satisfy these
conditions [18]. Let S be the set of indices of the nonzero entries in a speciﬁc row of Θ0. The
second condition states that the corresponding entries in the extended state variable y = [xT  uT] are
sufﬁciently distinguishable from each other. In other words  if the trajectories corresponding to this
group of state variables are observed  non of them can be well approximated as a linear combination
of the others. The third condition can be thought of as a quantiﬁcation of the ﬁrst vs. higher order
dependencies. Consider entry j in the extended state variable. Then  the dynamic of yj is directly
inﬂuenced by entries yS. However they are also inﬂuenced indirectly by other entries of y. The third
condition roughly states that the indirect inﬂuences are sufﬁciently weaker than the direct inﬂuences.
There exists a vast literature on the applicability of these conditions and scenarios in which they are
known to hold. These conditions are almost necessary for the successful recovery by (cid:96)1 relaxation.
For a discussion on these and other similar conditions imposed for sparse signal recovery we refer
the reader to [19] and [20] and the references therein.
Deﬁne Θmin = mini∈[p] j∈[q] Θ0
efﬁciently from its trajectory observations when it is controlled by an identiﬁable regulator.
Theorem 3.2. Consider the LQ system of Eq. (1) and assume Θ0 = [A0  B0] is k-sparse. Let
u(t) = −Lx(t) where L is a (ρ  Cmin  α) identiﬁable regulator with respect to Θ0 and deﬁne
(cid:96) = max(1  maxj∈[r] (cid:107)Lj(cid:107)2). Let n denote the number of samples of the trajectory that is observed.
For any 0 <  < min(Θmin  (cid:96)
2  

ij|. Our ﬁrst result states that the system can be learned

1−ρ)  there exists λ such that  if

ij(cid:54)=0 |Θ0

3

n ≥ 4 · 103 k2(cid:96)2
α2(1 − ρ)C 2

then the (cid:96)1-regularized least squares solution (cid:98)Θ of Eq. (5) satisﬁes d((cid:98)Θ  Θ0) ≤  with probability
larger than 1 − δ. In particular  this is achieved by taking λ = 6(cid:96)(cid:112)log(4q/δ)/(nα2(1 − ρ)) .

min

(1 − ρ)2

2 +

(11)

k

log(

4kq
δ

)  

(cid:18) 1

(cid:19)

√
Our second result states that equipped with an efﬁcient learning algorithm  the LQ system of Eq. (1)
can be controlled with regret ˜O(p
Deﬁne an -neighborhood of Θ0 as N(Θ0) = {Θ ∈ Rp×q | d(Θ0  Θ) ≤ }. Our assumption asserts
the identiﬁably of L(Θ) for Θ close to Θ0.
Assumption: There exist   C > 0 such that for all Θ ∈ N(Θ0)  L(Θ) is identiﬁable w.r.t. Θ0 and

2 (1/δ)) under suitable assumptions.

T log 3

σL(Θ0  ) = sup

Θ∈N(Θ0)

(cid:107)L(Θ)(cid:107)2 ≤ C 

σK(Θ0  ) = sup

Θ∈N(Θ0)

(cid:107)K(Θ)(cid:107)2 ≤ C.

Also deﬁne

(cid:107)Lj(Θ)(cid:107)2) .
Note that (cid:96)(Θ0  ) ≤ max(C  1)  since maxj∈[r] (cid:107)Lj(Θ)(cid:107)2 ≤ (cid:107)L(Θ)(cid:107)2.

max(1  max
j∈[r]

(cid:96)(Θ0  ) = sup

Θ∈N(Θ0)

5

Theorem 3.3. Consider the LQ system of Eq. (1). For some constants   Cmin and 0 < α  ρ < 1 
assume that an initial (ρ  Cmin  α) identiﬁable regulator L(0) is given. Further  assume that for any
Θ ∈ N(Θ0)  L(Θ) is (ρ  Cmin  α) identiﬁable. Then  with probability at least 1 − δ the cumulative
regret of ALGORITHM (cf. the table) is bounded as
√

R(T ) ≤ ˜O(p

T log 3

2 (1/δ))  

(12)

where ˜O is hiding the logarithmic factors.

4 Analysis

4.1 Proof of Theorem 3.2

To prove theorem 3.2 we ﬁrst state a set of sufﬁcient conditions for the solution of the (cid:96)1-regularized
least squares to be within some distance  as deﬁned by d(· ·)  of the true parameter. Subsequently 
we prove that these conditions hold with high probability.
Deﬁne X = [x(0)  x(1)  . . .   x(n − 1)] ∈ Rp×n and let W = [w(1)  . . .   w(n)] ∈ Rp×n be the
matrix containing the Gaussian noise realization. Further let the Wu denote the uth row of W .
Deﬁne the normalized gradient and Hessian of the likelihood function (6) as

(cid:98)G = −∇L(Θ0

u) =

1
n

(cid:101)LXW T

u  

(cid:98)H = ∇2L(Θ0

u) =

1
n

(cid:101)LXX T(cid:101)LT .

(13)

The following proposition  a proof of which can be found in [20]  provides a set of sufﬁcient condi-
tions for the accuracy of the (cid:96)1-regularized least squares solution.
u with |S| < k  and H be deﬁned per Deﬁnition 3.1.
Proposition 4.1. Let S be the support of Θ0
Assume there exist 0 < α < 1 and Cmin > 0 such that

λmin(HS S) ≥ Cmin  

(cid:107)HSc SH−1

For any 0 <  < Θmin if the following conditions hold

(cid:107)(cid:98)G(cid:107)∞ ≤ λα
(cid:107)(cid:98)HSC S − HSC S(cid:107)∞ ≤ α

3  

Cmin√
k

S S(cid:107)∞ ≤ 1 − α .
(cid:107)(cid:98)GS(cid:107)∞ ≤ Cmin
(cid:107)(cid:98)HSS − HSS(cid:107)∞ ≤ α

− λ 

4k

12

(14)

(15)

(16)

Cmin√
k

 

the (cid:96)1-regularized least square solution (5) satisﬁes d((cid:98)Θu  Θ0

12

 

u) ≤ .

In the sequel  we prove that the conditions in Proposition 4.1 hold with high probability given that the
assumptions of Theorem 3.2 are satisﬁed. A few lemmas are in order proofs of which are deferred
to the Appendix.

The ﬁrst lemma states that (cid:98)G concentrates in inﬁnity norm around its mean of zero.

Lemma 4.2. Assume ρ = (cid:107)A0 − B0L(cid:107)2 < 1 and let (cid:96) = max(1  maxi∈[r] (cid:107)Li(cid:107)2). Then  for any
S ⊆ [q] and 0 <  < (cid:96)

2

P(cid:8)(cid:107)(cid:98)GS(cid:107)∞ > (cid:9) ≤ 2|S| exp

(cid:18)
of the elements of (cid:98)H from their mean H  i.e.  |(cid:98)Hij − Hij|.

(cid:19)

− n(1 − ρ)2

4(cid:96)2

To prove the conditions in Eq. (16) we ﬁrst bound in the following lemma the absolute deviations

.

(17)

Lemma 4.3. Let i  j ∈ [q]  ρ = (cid:107)A0 − B0L(cid:107)2 < 1  and 0 <  < 3
− n(1 − ρ)32

P(|(cid:98)Hij − Hij| > ) ≤ 2 exp

The following corollary of Lemma 4.3 bounds (cid:107)(cid:98)HJS − HJS(cid:107)∞ for J  S ⊆ [q].

24(cid:96)2

1−ρ < n . Then 

(cid:19)

.

(cid:18)

(18)

6

Corollary 4.4. Let J  S ⊆ [q]  ρ = (cid:107)A0 − B0L(cid:107)2 < 1   < 3|S|

(cid:18)

(cid:19)

P((cid:107)(cid:98)HJS − HJS(cid:107)∞ > ) ≤ 2|J||S| exp
P((cid:107)(cid:98)HJS − HJS(cid:107)∞ > ) ≤ |J||S| max

1−ρ . Then 

.

1−ρ   and n > 3
− n(1 − ρ)32
24|S|2(cid:96)2
P(|(cid:98)Hij − Hij| > /|S|).

i∈J j∈S

The proof of Corollary 4.4 is by applying union bound as

(19)

(20)

Proof of Theorem 3.2. We show that the conditions given by Proposition 4.1 hold. The conditions
in Eq. (14) are true by the assumption of identiﬁability of L with respect to Θ0. In order to make the

ﬁrst constraint on (cid:98)G imply the second constraint on (cid:98)G  we assume that λα/3 ≤ Cmin/(4k) − λ 
which is ensured to hold if λ ≤ Cmin/(6k). By Lemma 4.2  P((cid:107)(cid:98)G(cid:107)∞ > λα/3) ≤ δ/2 if

36(cid:96)2

n(1 − ρ)α2 log(

4q
δ

) .

λ2 =
Requiring λ ≤ Cmin/(6k)  we obtain
n ≥

The conditions on (cid:98)H can also be aggregated as (cid:107)(cid:98)H[q] S−H[q] S(cid:107)∞ ≤ αCmin/(12
4.4  P((cid:107)(cid:98)H[q]S − H[q]S(cid:107)∞ > αCmin/(12

2α2C 2

log(

√

) .

√

362 k2(cid:96)2
min(1 − ρ)

4q
δ

(21)

(22)

k) . By Corollary

Merging the conditions in Eq. (22) and (23) we conclude that the conditions in Proposition 4.1 hold
with probability at least 1 − δ if

log(

).

(23)

k)) ≤ δ/2 if
3456 k3(cid:96)2
α2(1 − ρ)3C 2

min

n ≥

(cid:18) 1

4kq
δ

(cid:19)

n ≥ 4 · 103 k2(cid:96)2
α2(1 − ρ)C 2
Which ﬁnishes the proof of Theorem 3.2.

min

2 +

k

(1 − ρ)2

log(

4kq
δ

).

(24)

4.2 Proof of Theorem 3.3

The high-level idea of the proof is similar to the proof of main Theorem in [1]. First  we give a
decomposition for the gap between the cost obtained by the algorithm and the optimal cost. We then
upper bound each term of the decomposition separately.

4.2.1 Cost Decomposition

 

u

(cid:26)

Writing the Bellman optimality equations [5  4] for average cost dynamic programming  we get

is the σ-ﬁeld generated by the variables {(zτ   xτ )}t
cost occurred with initial state x(t) [5  4]. Therefore 

x(t)TQx(t) + uTRu + E(cid:2)z(t + 1)TK((cid:101)Θt)z(t + 1)|Ft

(cid:3)(cid:27)
J((cid:101)Θt) + x(t)TK((cid:101)Θt)x(t) = min
where (cid:101)Θt = [(cid:101)A  (cid:101)B] is the estimate used at time t  z(t + 1) = (cid:101)Atx(t) + (cid:101)Btu + w(t + 1)  and Ft
J((cid:101)Θt) + x(t)TK((cid:101)Θt)x(t) = x(t)TQx(t) + u(t)TRu(t)
(cid:3)
+ E(cid:2)((cid:101)Atx(t) + (cid:101)Btu(t) + w(t + 1))TK((cid:101)Θt)((cid:101)Atx(t) + (cid:101)Btu(t) + w(t + 1))|Ft
= x(t)TQx(t) + u(t)TRu(t) + E(cid:2)((cid:101)Atx(t) + (cid:101)Btu(t))TK((cid:101)Θt)((cid:101)Atx(t) + (cid:101)Btu(t))|Ft
+ E(cid:2)w(t + 1)TK((cid:101)Θt)w(t + 1)|Ft]
= x(t)TQx(t) + u(t)TRu(t) + E(cid:2)x(t + 1)TK((cid:101)Θt)x(t + 1)|Ft
(cid:16)
((cid:101)Atx(t) + (cid:101)Btu(t))TK((cid:101)Θt)((cid:101)Atx(t) + (cid:101)Btu(t))
(cid:17)
− (A0x(t) + B0u(t))TK((cid:101)Θt)(A0x(t) + B0u(t))

τ =0. Notice that the left-hand side is the average

(cid:3)

+

.

(cid:3)

7

Consequently

where

t=0

t=0

T(cid:88)

T(cid:88)

C1 =

(cid:0)x(t)TQx(t) + u(t)TRu(t)(cid:1) =
J((cid:101)Θt) + C1 + C2 + C3 
(cid:18)
(cid:3)(cid:19)
T(cid:88)
x(t)TK((cid:101)Θt)x(t) − E(cid:2)x(t + 1)TK((cid:101)Θt+1)x(t + 1)(cid:12)(cid:12)Ft
C2 = − T(cid:88)
E(cid:2)x(t + 1)T(K((cid:101)Θt) − K((cid:101)Θt+1))x(t + 1)(cid:12)(cid:12)Ft
(cid:3) 
(cid:16)
C3 = − T(cid:88)
((cid:101)Atx(t) + (cid:101)Btu(t))TK((cid:101)Θt)((cid:101)Atx(t) + (cid:101)Btu(t))
(cid:17)
− (A0x(t) + B0u(t))TK((cid:101)Θt)(A0x(t) + B0u(t))

t=0

t=0

t=0

.

 

(25)

(26)

(27)

(28)

4.2.2 Good events

We proceed by deﬁning the following two events in the probability space under which we can bound
the terms C1  C2  C3. We then provide a lower bound on the probability of these events.

E2 = {(cid:107)w(t)(cid:107) ≤ 2(cid:112)p log(T /δ)  for 1 ≤ t ≤ T + 1}.

E1 = {Θ0 ∈ Ω(i)  for i ≥ 1} 

4.2.3 Technical lemmas

The following lemmas establish upper bounds on C1  C2  C3.
Lemma 4.5. Under the event E1 ∩ E2  the following holds with probability at least 1 − δ.

√
128 C
(1 − ρ)2

√

C1 ≤

T p log( T
δ

)

log(

1
δ

) .

(cid:114)

Lemma 4.6. Under the event E1 ∩ E2  the following holds.

Lemma 4.7. Under the event E1 ∩ E2  the following holds with probability at least 1 − δ.
√

(cid:16) C

(cid:17) 5

2

|C3| ≤ 800

1 − ρ

k

1 + k2

(1 − ρ)2

· log( pT
δ

)

log(

4kq
δ

) · p log T

T .

(cid:115)(cid:16)

C2 ≤ 8C

δ

(1 − ρ)2 p log( T
(cid:17) · 1 + C

Cmin

) log T .

(cid:114)

Lemma 4.8. The following holds true.

P(E1) ≥ 1 − δ  P(E2) ≥ 1 − δ.

Therefore  P(E1 ∩ E2) ≥ 1 − 2δ.
We are now in position to prove Theorem 3.3.
Proof (Theorem 3.3). Using cost decomposition (Eq. (25))  under the event E1 ∩ E2  we have

(29)

(30)

(31)

(32)

T(cid:88)

t=0

(x(t)TQx(t) + u(t)TRu(t)) =

T(cid:88)

t=0

J((cid:101)Θt) + C1 + C2 + C3

where the last inequality stems from the choice of(cid:101)Θt by the algorithm (cf. Eq (9)) and the fact that

Θ0 ∈ Ωt  for all t under the event E1. Hence  R(T ) ≤ C1 + C2 + C3 . Now using the bounds on
C1  C2  C3  we get the desired result.

≤ T J(Θ0) + C1 + C2 + C3 

Acknowledgments

The authors thank the anonymous reviewers for their insightful comments. A.J. is supported by a
Caroline and Fabian Pease Stanford Graduate Fellowship.

8

References
[1] Y. Abbasi-Yadkori and C. Szepesv´ari. Regret bounds for the adaptive control of linear quadratic

systems. Proceeding of the 24th Annual Conference on Learning Theory  pages 1–26  2011.

[2] Y. Bar-Shalom and E. Tse. Dual effect  certainty equivalence  and separation in stochastic

control. Automatic Control  IEEE Transactions on  19(5):494–500  1974.

[3] J. Bento  M. Ibrahimi  and A. Montanari. Learning networks of stochastic differential equa-

tions. Advances in Neural Information Processing Systems 23  pages 172–180  2010.

[4] D. Bertsekas. Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall 

1987.

[5] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc  3rd edition 

2007.

[6] S. Bittanti and M. Campi. Adaptive control of linear time invariant systems: the bet on the best

principle. Communications in Information and Systems  6(4):299–320  2006.

[7] E. Bradlow  B. Bronnenberg  G. Russell  N. Arora  D. Bell  S. Duvvuri  F. Hofstede  C. Sis-
meiro  R. Thomadsen  and S. Yang. Spatial models in marketing. Marketing Letters 
16(3):267–278  2005.

[8] M. Campi. Achieving optimality in adaptive control: the bet on the best approach. In Decision
and Control  1997.  Proceedings of the 36th IEEE Conference on  volume 5  pages 4671–4676.
IEEE  1997.

[9] V. Dani  T. Hayes  and S. Kakade. Stochastic linear optimization under bandit feedback. In

Proceedings of the 21st Annual Conference on Learning Theory (COLT)  2008.

[10] G. Feichtinger  R. Hartl  and S. Sethi. Dynamic optimal control models in advertising: recent

developments. Management Science  pages 195–226  1994.

[11] L. Guo and H. Chen. The ˚astrom-wittenmark self-tuning regulator revisited and els-based

adaptive trackers. Automatic Control  IEEE Transactions on  36(7):802–812  1991.

[12] P. Kumar and A. Becker. A new family of optimal adaptive controllers for markov chains.

Automatic Control  IEEE Transactions on  27(1):137–146  1982.

[13] T. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied

mathematics  6(1):4–22  1985.

[14] T. Lai and C. Wei. Least squares estimates in stochastic regression models with applications to
identiﬁcation and control of dynamic systems. The Annals of Statistics  10(1):154–166  1982.
[15] C. Marinelli and S. Savin. Optimal distributed dynamic advertising. Journal of Optimization

Theory and Applications  137(3):569–591  2008.

[16] T. Seidman  S. Sethi  and N. Derzko. Dynamics and optimization of a distributed sales-

advertising model. Journal of Optimization Theory and Applications  52(3):443–462  1987.

[17] S. Sethi. Dynamic optimal control models in advertising: a survey. SIAM review  pages 685–

725  1977.

[18] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.

Information Theory  IEEE Transactions on  52(3):1030–1051  2006.

[19] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using-
Information Theory  IEEE Transactions on 

constrained quadratic programming (lasso).
55(5):2183–2202  2009.

[20] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research  7:2541–2563  2006.

9

,Kai Zhong
Ian En-Hsu Yen
Inderjit Dhillon
Pradeep Ravikumar
Charlie Frogner
Chiyuan Zhang
Hossein Mobahi
Mauricio Araya
Tomaso Poggio