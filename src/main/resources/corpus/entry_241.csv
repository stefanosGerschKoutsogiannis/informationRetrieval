2017,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,There has been a resurgence of interest in multiagent reinforcement learning (MARL)  due partly to the recent success of deep neural networks. The simplest form of MARL is independent reinforcement learning (InRL)  where each agent treats all of its experience as part of its (non stationary) environment. In this paper  we first observe that policies learned using InRL can overfit to the other agents' policies during training  failing to sufficiently generalize during execution. We introduce a new metric  joint-policy correlation  to quantify this effect. We describe a meta-algorithm for general MARL  based on approximate best responses to mixtures of policies generated using deep reinforcement learning  and empirical game theoretic analysis to compute meta-strategies for policy selection. The meta-algorithm generalizes previous algorithms such as InRL  iterated best response  double oracle  and fictitious play. Then  we propose a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally  we demonstrate the generality of the resulting policies in three partially observable settings: gridworld coordination problems  emergent language games  and poker.,A Uniﬁed Game-Theoretic Approach to

Multiagent Reinforcement Learning

Marc Lanctot

DeepMind
lanctot@

Vinicius Zambaldi

DeepMind

vzambaldi@

Audr¯unas Gruslys

DeepMind
audrunas@

Angeliki Lazaridou

DeepMind
angeliki@

Karl Tuyls
DeepMind

karltuyls@

Julien Pérolat

DeepMind
perolat@

David Silver
DeepMind

davidsilver@

Thore Graepel

DeepMind
thore@

...@google.com

Abstract

To achieve general intelligence  agents must learn how to interact with others in
a shared environment: this is the challenge of multiagent reinforcement learning
(MARL). The simplest form is independent reinforcement learning (InRL)  where
each agent treats its experience as part of its (non-stationary) environment. In
this paper  we ﬁrst observe that policies learned using InRL can overﬁt to the
other agents’ policies during training  failing to sufﬁciently generalize during
execution. We introduce a new metric  joint-policy correlation  to quantify this
effect. We describe an algorithm for general MARL  based on approximate best
responses to mixtures of policies generated using deep reinforcement learning  and
empirical game-theoretic analysis to compute meta-strategies for policy selection.
The algorithm generalizes previous ones such as InRL  iterated best response 
double oracle  and ﬁctitious play. Then  we present a scalable implementation
which reduces the memory requirement using decoupled meta-solvers. Finally 
we demonstrate the generality of the resulting policies in two partially observable
settings: gridworld coordination games and poker.

1

Introduction

Deep reinforcement learning combines deep learning [59] with reinforcement learning [94  64] to
compute a policy used to drive decision-making [73  72]. Traditionally  a single agent interacts with
its environment repeatedly  iteratively improving its policy by learning from its observations. Inspired
by recent success in Deep RL  we are now seeing a renewed interest in multiagent reinforcement
learning (MARL) [90  17  99]. In MARL  several agents interact and learn in an environment
simultaneously  either competitively such as in Go [91] and Poker [39  105  74]  cooperatively such
as when learning to communicate [23  93  36]  or some mix of the two [60  95  35].
The simplest form of MARL is independent RL (InRL)  where each learner is oblivious to the other
agents and simply treats all the interaction as part of its (“localized”) environment. Aside from
the problem that these local environments are non-stationary and non-Markovian [57] resulting in
a loss of convergence guarantees for many algorithms  the policies found can overﬁt to the other
agents’ policies and hence not generalize well. There has been relatively little work done in RL
community on overﬁtting to the environment [102  69]  but we argue that this is particularly important
in multiagent settings where one must react dynamically based on the observed behavior of others.
Classical techniques collect or approximate extra information such as the joint values [62  19  29  56] 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

use adaptive learning rates [12]  adjust the frequencies of updates [48  81]  or dynamically respond
to the other agents actions online [63  50]. However  with the notable exceptions of very recent
work [22  80]  they have focused on (repeated) matrix games and/or the fully-observable case.
There have several proposals for treating partial observability in the multiagent setting. When the
model is fully known and the setting is strictly adversarial with two players  there are policy iteration
methods based on regret minimization that scale very well when using domain-speciﬁc abstrac-
tions [27  14  46  47]  which was a major component of the expert no-limit poker AI Libratus [15];
recently these methods were combined with deep learning to create an expert no-limit poker AI
called DeepStack [74]. There is a signiﬁcant amount of work that deals with the case of decentralized
cooperative problems [76  79]  and in the general setting by extending the notion of belief states
and Bayesian updating from POMDPs [28]. These models are quite expressive  and the resulting
algorithms are fairly complex. In practice  researchers often resort to approximate forms  by sampling
or exploiting structure  to ensure good performance due to intractability [41  2  68].
In this paper  we introduce a new metric for quantifying the correlation effects of policies learned by
independent learners  and demonstrate the severity of the overﬁtting problem. These coordination
problems have been well-studied in the fully-observable cooperative case [70]: we observe similar
problems in a partially-observed mixed cooperative/competitive setting and  and we show that the
severity increases as the environment becomes more partially-observed. We propose a new algorithm
based on economic reasoning [82]  which uses (i) deep reinforcement learning to compute best
responses to a distribution over policies  and (ii) empirical game-theoretic analysis to compute new
meta-strategy distributions. As is common in the MARL setting  we assume centralized training for
decentralized execution: policies are represented as separate neural networks and there is no sharing
of gradients nor architectures among agents. The basic form uses a centralized payoff table  which is
removed in the distributed  decentralized form that requires less space.

2 Background and Related Work

In this section  we start with basic building blocks necessary to describe the algorithm. We interleave
this with the most relevant previous work for our setting. Several components of the general idea have
been (re)discovered many times across different research communities  each with slightly different
but similar motivations. One aim here is therefore to unify the algorithms and terminology.
A normal-form game is a tuple (Π  U  n) where n is the number of players  Π = (Π1 ···   Πn)
is the set of policies (or strategies  one for each player i ∈ [[n]]  where [[n]] = {1 ···   n})  and
U : Π → (cid:60)n is a payoff table of utilities for each joint policy played by all players. Extensive-form
games extend these formalisms to the multistep sequential case (e.g. poker).
Players try to maximize their own expected utility. Each player does this by choosing a policy
from Πi  or by sampling from a mixture (distribution) over them σi ∈ ∆(Πi). In this multiagent
setting  the quality of σi depends on other players’ strategies  and so it cannot be found nor assessed
independently. Every ﬁnite extensive-form game has an equivalent normal-form [53]  but since it is
exponentially larger  most algorithms have to be adapted to handle the sequential setting directly.
There are several algorithms for computing strategies. In zero-sum games (where ∀π ∈ Π (cid:126)1 ·
U (π) = 0)  one can use e.g. linear programming  ﬁctitious play [13]  replicator dynamics [97] 
or regret minimization [8]. Some of these techniques have been extended to extensive (sequential)
form [39  25  54  107] with an exponential increase in the size of the state space. However  these
extensions have almost exclusively treated the two-player case  with some notable exceptions [54  26].
Fictitious play also converges in potential games which includes cooperative (identical payoff) games.
The double oracle (DO) algorithm [71] solves a set of (two-player  normal-form) subgames induced
by subsets Πt ⊂ Π at time t. A payoff matrix for the subgame Gt includes only those entries
corresponding to the strategies in Πt. At each time step t  an equilibrium σ∗ t is obtained for Gt  and
∗ t−i ) from the full space Πi  so for all i 
to obtain Gt+1 each player adds a best response πt+1
}. The algorithm is illustrated in Figure 1. Note that ﬁnding an equilibrium in a
Πt+1
i = Πt
zero-sum game takes time polynomial in |Πt|  and is PPAD-complete for general-sum [89].
Clearly  DO is guaranteed to converge to an equilibrium in two-player games. But  in the worst-case 
the entire strategy space may have to be enumerated. For example  this is necessary for Rock-Paper-

i ∪ {πt+1

∈ BR(σ

i

i

2

Figure 1: The Double Oracle Algorithm. Figure taken from [10] with authors’ permission.

3   1

3   1

Scissors  whose only equilibrium has full support ( 1
3 ). However  there is evidence that support
sizes shrink for many games as a function of episode length  how much hidden information is revealed
and/or affects it has on the payoff [65  86  10]. Extensions to the extensive-form games have been
developed [67  9  10] but still large state spaces are problematic due to the curse of dimensionality.
Empirical game-theoretic analysis (EGTA) is the study of meta-strategies obtained through simulation
in complex games [100  101]. An empirical game  much smaller in size than the full game  is
constructed by discovering strategies  and meta-reasoning about the strategies to navigate the strategy
space. This is necessary when it is prohibitively expensive to explicitly enumerate the game’s
strategies. Expected utilities for each joint strategy are estimated and recorded in an empirical
payoff table. The empirical game is analyzed  and the simulation process continues. EGTA has been
employed in trading agent competitions (TAC) and automated bidding auctions.
One study used evolutionary dynamics in the space of known expert meta-strategies in Poker [83].
Recently  reinforcement learning has been used to validate strategies found via EGTA [104]. In this
work  we aim to discover new strategies through learning. However  instead of computing exact best
responses  we compute approximate best responses using reinforcement learning. A few epochs of
this was demonstrated in continuous double auctions using tile coding [87]. This work follows up in
this line  running more epochs  using modern function approximators (deep networks)  a scalable
implementation  and with a focus on ﬁnding policies that can generalize across contexts.
A key development in recent years is deep learning [59]. While most work in deep learning has
focused on supervised learning  impressive results have recently been shown using deep neural
networks for reinforcement learning  e.g. [91  38  73  77]. For instance  Mnih et al. [73] train policies
for playing Atari video games and 3D navigation [72]  given only screenshots. Silver et al. introduced
AlphaGo [91  92]  combining deep RL with Monte Carlo tree search  outperforming human experts.
Computing approximate responses is more computationally feasible  and ﬁctitious play can handle
approximations [42  61]. It is also more biologically plausible given natural constraints of bounded
rationality. In behavioral game theory [103]  the focus is to predict actions taken by humans  and
the responses are intentionally constrained to increase predictive ability. A recent work uses a deep
learning architecture [34]. The work that closely resembles ours is level-k thinking [20] where
level k agents respond to level k − 1 agents  and more closely cognitive hierarchy [18]  in which
responses are to distributions over levels {0  1  . . .   k − 1}. However  our goals and motivations are
very different: we use the setup as a means to produce more general policies rather than to predict
human behavior. Furthermore  we consider the sequential setting rather than normal-form games.
Lastly  there has been several studies from the literature on co-evolutionary algorithms; speciﬁcally 
how learning cycles and overﬁtting to the current populations can be mitigated [78  85  52].

3 Policy-Space Response Oracles

We now present our main conceptual algorithm  policy-space response oracles (PSRO). The algorithm
is a natural generalization of Double Oracle where the meta-game’s choices are policies rather than
actions. It also generalizes Fictitious Self-Play [39  40]. Unlike previous work  any meta-solver
can be plugged in to compute a new meta-strategy. In practice  parameterized policies (function
approximators) are used to generalize across the state space without requiring any domain knowledge.
The process is summarized in Algorithm 1. The meta-game is represented as an empirical game 
starting with a single policy (uniform random) and growing  each epoch  by adding policies (“oracles”)

3

:initial policy sets for all players Π

Algorithm 1: Policy-Space Response Oracles
input
Compute exp. utilities U Π for each joint π ∈ Π
Initialize meta-strategies σi = UNIFORM(Πi)
while epoch e in {1  2 ···} do
for many episodes do
Sample π−i ∼ σ−i
Train oracle π(cid:48)
Πi = Πi ∪ {π(cid:48)
i}

for player i ∈ [[n]] do

i over ρ ∼ (π(cid:48)

i  π−i)

Compute missing entries in U Π from Π
Compute a meta-strategy σ from U Π

Output current solution strategy σi for player i

Algorithm 2: Deep Cognitive Hierarchies
input
while not terminated do

:player number i  level k

CHECKLOADMS({j|j ∈ [[n]]  j (cid:54)= i}  k)
CHECKLOADORACLES(j ∈ [[n]]  k(cid:48) ≤ k)
CHECKSAVEMS(σi k)
CHECKSAVEORACLE(πi k)
Sample π−i ∼ σ−i k
Train oracle πi k over ρ1 ∼ (πi k  π−i)
if iteration number mod Tms = 0 then

Sample πi ∼ σi k
Compute ui(ρ2)  where ρ2 ∼ (πi  π−i)
Update stats for πi and update σi k

Output σi k for player i at level k

that approximate best responses to the meta-strategy of the other players. In (episodic) partially
observable multiagent environments  when the other players are ﬁxed the environment becomes
Markovian and computing a best response reduces to solving a form of MDP [30]. Thus  any
reinforcement learning algorithm can be used. We use deep neural networks due to the recent success
in reinforcement learning. In each episode  one player is set to oracle(learning) mode to train π(cid:48)
i  and
a ﬁxed policy is sampled from the opponents’ meta-strategies (π−i ∼ σ−i). At the end of the epoch 
the new oracles are added to their policy sets Πi  expected utilities for new policy combinations are
computed via simulation and added to the empirical tensor U Π  which takes time exponential in |Π|.
Deﬁne ΠT = ΠT−1 ∪ π(cid:48) as the policy space including the currently learning oracles  and |σi| = |ΠT
i |
for all i ∈ [[n]]. Iterated best response is an instance of PSRO with σ−i = (0  0 ···   1  0). Similarly 
Independent RL and ﬁctitious play are instances of PSRO with σ−i = (0  0 ···   0  1) and σ−i =
(1/K  1/K ···   1/K  0)  respectively  where K = |ΠT−1−i
|. Double Oracle is an instance of PSRO
with n = 2 and σT set to a Nash equilibrium proﬁle of the meta-game (ΠT−1  U ΠT −1
An exciting question is what can happen with (non-ﬁxed) meta-solvers outside this known space?
Fictitious play is agnostic to the policies it is responding to; hence it can only sharpen the meta-
strategy distribution by repeatedly generating the same best responses. On the other hand  responses
to equilibrium strategies computed by Double Oracle will (i) overﬁt to a speciﬁc equilibrium in the
n-player or general-sum case  and (ii) be unable to generalize to parts of the space not reached by any
equilibrium strategy in the zero-sum case. Both of these are undesirable when computing general
policies that should work well in any context. We try to balance these problems of overﬁtting with a
compromise: meta-strategies with full support that force (mix in) γ exploration over policy selection.

).

3.1 Meta-Strategy Solvers

A meta-strategy solver takes as input the empirical game (Π  U Π) and produces a meta-strategy σi
for each player i. We try three different solvers: regret-matching  Hedge  and projected replicator
dynamics. These speciﬁc meta-solvers accumulate values for each policy (“arm”) and an aggregate
value based on all players’ meta-strategies. We refer to ui(σ) as player i’s expected value given
all players’ meta-strategies and the current empirical payoff tensor U Π (computed via multiple
tensor dot products.) Similarly  denote ui(πi k  σ−i) as the expected utility if player i plays their
kth ∈ [[K]] ∪ {0} policy and the other players play with their meta-strategy σ−i. Our strategies use
an exploration parameter γ  leading to a lower bound of
K+1 on the probability of selecting any πi k.
The ﬁrst two meta-solvers (Regret Matching and Hedge) are straight-forward applications of previous
algorithms  so we defer the details to Appendix A.1 Here  we introduce a new solver we call projected
replicator dynamics (PRD). From Appendix A  when using the asymmetric replicator dynamics 
e.g. with two players  where U Π = (A  B)  the change in probabilities for the kth component (i.e. 
the policy πi k) of meta-strategies (σ1  σ2) = (x  y) are:
dyk
dt

= yk[(xT B)k − xT By] 

= xk[(Ay)k − xT Ay] 

dxk
dt

γ

1Appendices are available in the longer technical report version of the paper  see [55].

4

To simulate the replicator dynamics in practice  discretized updates are simulated using a step-size of δ.
We add a projection operator P (·) to these equations that guarantees exploration: x ← P (x + δ dx
dt ) 
y ← P (y + δ dy
{||x(cid:48) − x||}  if any xk < γ/(K + 1) or x
k xk = 1} is the γ-exploratory simplex of size K + 1.
otherwise  and ∆K+1
This enforces exploratory σi(πi k) ≥ γ/(K + 1). The PRD approach can be understood as directing
exploration in comparison to standard replicator dynamics approaches that contain isotropic diffusion
or mutation terms (which assume undirected and unbiased evolution)  for more details see [98].

dt )  where P (x) = argminx(cid:48)∈∆K+1

K+1  (cid:80)

= {x | xk ≥ γ

γ

γ

3.2 Deep Cognitive Hierarchies

Figure 2: Overview of DCH

While the generality of PSRO is clear and appealing  the
RL step can take a long time to converge to a good re-
sponse.
In complex environments  much of the basic
behavior that was learned in one epoch may need to be
relearned when starting again from scratch; also  it may be
desirable to run many epochs to get oracle policies that can
recursively reason through deeper levels of contingencies.
To overcome these problems  we introduce a practical
parallel form of PSRO. Instead of an unbounded number
of epochs  we choose a ﬁxed number of levels in advance.
Then  for an n-player game  we start nK processes in
parallel (level 0 agents are uniform random): each one trains a single oracle policy πi k for player i
and level k and updates its own meta-strategy σi k  saving each to a central disk periodically. Each
process also maintains copies of all the other oracle policies πj k(cid:48)≤k at the current and lower levels  as
well as the meta-strategies at the current level σ−i k  which are periodically refreshed from a central
disk. We circumvent storing U Π explicitly by updating the meta-strategies online. We call this a
Deep Cognitive Hierarchy (DCH)  in reference to Camerer  Ho  & Chong’s model augmented with
deep RL. Example oracle response dynamics shown in Figure 2  and the pseudo-code in Algorithm 2.
Since each process uses slightly out-dated copies of the other process’s policies and meta-strategies 
DCH approximates PSRO. Speciﬁcally  it trades away accuracy of the correspondence to PSRO
for practical efﬁciency and  in particular  scalability. Another beneﬁt of DCH is an asymptotic
reduction in total space complexity. In PSRO  for K policies and n players  the space required to
store the empirical payoff tensor is K n. Each process in DCH stores nK policies of ﬁxed size  and
n meta-strategies (and other tables) of size bounded by k ≤ K. Therefore the total space required
is O(nK · (nK + nK)) = O(n2K 2). This is possible is due to the use of decoupled meta-solvers 
which compute strategies online without requiring a payoff tensor U Π  which we describe now.

3.2.1 Decoupled Meta-Strategy Solvers

In the ﬁeld of online learning  the experts algorithms (“full information” case) receive information
about each arm at every round. In the bandit (“partial information”) case  feedback is only given for
the arm that was pulled. Decoupled meta-solvers are essentially sample-based adversarial bandits [16]
applied to games. Empirical strategies are known to converge to Nash equilibria in certain classes of
games (i.e. zero-sum  potential games) due to the folk theorem [8].
We try three: decoupled regret-matching [33]  Exp3 (decoupled Hedge) [3]  and decoupled PRD. Here
again  we use exploratory strategies with γ of the uniform strategy mixed in  which is also necessary
to ensure that the estimates are unbiased. For decoupled PRD  we maintain running averages for the
overall average value an value of each arm (policy). Unlike in PSRO  in the case of DCH  one sample
is obtained at a time and the meta-strategy is updated periodically from online estimates.

4 Experiments

In all of our experiments  oracles use Reactor [31] for learning  which has achieved state-of-the-art
results in Atari game-playing. Reactor uses Retrace(λ) [75] for off-policy policy evaluation  and
β-Leave-One-Out policy gradient for policy updates  and supports recurrent network training  which
could be important in trying to match online experiences to those observed during training.

5

....................Nplayersπi kK+1levelsσ1 1π1 1randrandrandrandσi kThe action spaces for each player are identical  but the algorithms do not require this. Our implemen-
tation differs slightly from the conceptual descriptions in Section 3; see App. C for details.
First-Person Gridworld Games. Each agent has a local ﬁeld-of-view (making the world partially
observable)  sees 17 spaces in front  10 to either side  and 2 spaces behind. Consequently  observations
are encoded as 21x20x3 RGB tensors with values 0 – 255. Each agent has a choice of turning left or
right  moving forward or backward  stepping left or right  not moving  or casting an endless light
beam in their current direction. In addition  the agent has two composed actions of moving forward
and turning. Actions are executed simultaneously  and order of resolution is randomized. Agents
start on a random spawn point at the beginning of each episode. If an agent is touched (“tagged”) by
another agent’s light beam twice  then the target agent is immediately teleported to a spawn point. In
laser tag  the source agent then receives 1 point of reward for the tag. In another variant  gathering 
there is no tagging but agents can collect apples  for 1 point per apple  which refresh at a ﬁxed rate.
In pathﬁnd  there is no tagging nor apples  and both agents get 1 point reward when both reach their
destinations  ending the episode. In every variant  an episode consists of 1000 steps of simulation.
Other details  such as speciﬁc maps  can be found in Appendix D.
Leduc Poker is a common benchmark in Poker AI  consisting of a six-card deck: two suits with
three cards (Jack  Queen  King) each. Each player antes 1 chip to play  and receives one private card.
There are two rounds of betting  with a maximum of two raises each  whose values are 2 and 4 chips
respectively. After the ﬁrst round of betting  a single public card is revealed. The input is represented
as in [40]  which includes one-hot encodings of the private card  public card  and history of actions.
Note that we use a more difﬁcult version than in previous work; see Appendix D.1 for details.

4.1

Joint Policy Correlation in Independent Reinforcement Learning

To identify the effect of overﬁtting in independent reinforcement learners  we introduce joint policy
correlation (JPC) matrices. To simplify the presentation  we describe here the special case of
symmetric two-player games with non-negative rewards; for a general description  see Appendix B.2.
Values are obtained by running D instances of the same experiment  differing only in the seed used
to initialize the random number generators. Each experiment d ∈ [[D]] (after many training episodes)
2 ). The entries of each D × D matrix shows the mean return over T = 100
produces policies (πd
1 and and player 2 uses
T (Rt
t=1
dj
column policy π
2 . Hence  entries on the diagonals represent returns for policies that learned together
(i.e.  same instance)  while off-diagonals show returns from policies that trained in separate instances.

2)  obtained when player 1 uses row policy πdi

episodes (cid:80)T

1   πd
1 + Rt

1

Figure 3: Example JPC matrices for InRL on Laser Tag small2 map (left) and small4 (right).

From a JPC matrix  we compute an average proportional loss in reward as R− = ( ¯D− ¯O)/ ¯D where
¯D is the mean value of the diagonals and ¯O is the mean value of the off-diagonals. E.g. in Figure 3:
D = 30.44  O = 20.03  R− = 0.342. Even in a simple domain with almost full observability
(small2)  an independently-learned policy could expect to lose 34.2% of its reward when playing with
another independently-learned policy even though it was trained under identical circumstances! This
clearly demonstrates an important problem with independent learners. In the other variants (gathering
and pathﬁnd)  we observe no JPC problem  presumably because coordination is not required and the
policies are independent. Results are summarized in Table 1. We have also noticed similar effects
when using DQN [73] as the oracle training algorithm; see Appendix B.1 for example videos.

6

01234Player #243210Player #125.127.329.65.329.827.331.727.630.626.214.612.930.37.323.829.930.817.811.015.530.730.923.93.79.261218243001234Player #243210Player #10.53.50.63.919.23.62.64.120.54.718.22.320.06.02.612.218.211.30.88.223.02.918.57.10.648121620Environment Map

InRL
¯O

¯D

Laser Tag
Laser Tag
Laser Tag
Gathering
Pathﬁnd

small2
small3
small4
ﬁeld
merge
Table 1: Summary of JPC results in ﬁrst-person gridworld games.

30.44
23.06
20.15
147.34
108.73

20.03
9.06
5.71
146.89
106.32

DCH(Reactor  2  10)
¯D
R−
0.055
0.082
0.150
0.007
< 0

26.63
23.45
15.90
138.74
91.492

28.20
27.00
18.72
139.70
90.15

R−
0.342
0.625
0.717
0.003
0.022

¯O

JPC Reduction

28.7 %
54.3 %
56.7 %

–
–

We see that a (level 10) DCH agent reduces the JPC problem signiﬁcantly. On small2  DCH reduces
the expected loss down to 5.5%  28.7% lower than independent learners. The problem gets larger as
the map size grows and problem becomes more partially observed  up to a severe 71.7% average loss.
The reduction achieved by DCH also grows from 28.7% to 56.7%.
Is the Meta-Strategy Necessary During Execution? The ﬁgures above represent the fully-mixed
strategy σi 10. We also analyze JPC for only the highest-level policy πi 10 in the laser tag levels. The
values are larger here: R− = 0.147  0.27  0.118 for small2-4 respectively  showing the importance of
the meta-strategy. However  these are still signiﬁcant reductions in JPC: 19.5%  36.5%  59.9%.
How Many Levels? On small4  we also compute values for level 5 and level 3: R− = 0.156 and
R− = 0.246  corresponding to reductions in JPC of 56.1% and 44%  respectively. Level 5 reduces
JPC by a similar amount as level 10 (56.1% vs 56.7%)  while level 3 less so (44% vs. 56.1%.)

4.2 Learning to Safely Exploit and Indirectly Model Opponents in Leduc Poker

i maxσ(cid:48)

i∈Σi ui(σ(cid:48)

is commonly used in poker AI: NASHCONV(σ) =(cid:80)n

We now show results for a Leduc poker where strong benchmark algorithms exist  such as counter-
factual regret (CFR) minimization [107  11]. We evaluate our policies using two metrics: the ﬁrst
is performance against ﬁxed players (random  CFR’s average strategy after 500 iterations “cfr500” 
and a puriﬁed version of “cfr500pure” that chooses the action with highest probability.) The second
i  σ−i) − ui(σ)  represent-
ing how much can be gained by deviating to their best response (unilaterally)  a value that can be
interpreted as a distance from a Nash equilibrium (called exploitability in the two-player setting).
NashConv is easy to compute in small enough games [45]; for CFR’s values see Appendix E.1.
Effect of Exploration and Meta-Strategy Overview. We now analyze the effect of the various
meta-strategies and exploration parameters. In Figure 4  we measure the mean area-under-the-curve
(MAUC) of the NashConv values for the last (right-most) 32 values in the NashConv graph  and
exploration rate of γ = 0.4. Figures for the other values of γ are in Appendix E  but we found this
value of γ works best for minimizing NashConv. Also  we found that decoupled replicator dynamics
works best  followed by decoupled regret-matching and Exp3. Also  it seems that the higher the level 
the lower the resulting NashConv value is  with diminishing improvements. For exploitation  we
found that γ = 0.1 was best  but the meta-solvers seemed to have little effect (see Figure 10.)
Comparison to Neural Fictitious Self-Play. We now compare to Neural Fictitious Self-Play
(NFSP) [40]  an implementation of ﬁctitious play in sequential games using reinforcement learning.
Note that NFSP  PSRO  and DCH are all sample-based learning algorithms that use general function
approximation  whereas CFR is a tabular method that requires a full game-tree pass per iteration.
NashConv graphs are shown for {2 3}-player in Figure 5  and performance vs. ﬁxed bots in Figure 6.

(a)

(b)

Figure 4: (a) Effect of DCH parameters on NashConv in 2 player Leduc Poker. Left: decoupled PRD 
Middle: decoupled RM  Right: Exp3  and (b) MAUC of the exploitation graph against cfr500.

7

uprdurmexp3metasolver0.00.51.01.52.02.53.0MAUC (NashConv)level123456789101112131415123456789101112131415level2.01.81.61.41.21.00.80.60.40.2MAUCmin_exploration_weight0.10.250.4(a) 2 players

(b) 3 players

Figure 5: Exploitability for NFSP x DCH x PSRO.

(a) Random bots as ref. set

(b) 2-player CFR500 bots as ref. set (c) 3-player CFR500 bots as ref. set
Figure 6: Evaluation against ﬁxed set of bots. Each data point is an average of the four latest values.

We observe that DCH (and PSRO) converge faster than NFSP at the start of training  possibly due to
a better meta-strategy than the uniform random one used in ﬁctitious play. The convergence curves
eventually plateau: DCH in two-player is most affected  possibly due to the asynchronous nature of
the updates  and NFSP converges to a lower exploitability in later episodes. We believe that this is
due to NFSP’s ability to learn a more accurate mixed average strategy at states far down in the tree 
which is particularly important in poker  whereas DCH and PSRO mix at the top over full policies.
On the other hand  we see that PSRO/DCH are able to achieve higher performance against the
ﬁxed players. Presumably  this is because the policies produced by PSRO/DCH are better able to
recognize ﬂaws in the weaker opponent’s policies  since the oracles are speciﬁcally trained for this 
and dynamically adapt to the exploitative response during the episode. So  NFSP is computing a safe
equilibrium while PSRO/DCH may be trading convergence precision for the ability to adapt to a range
of different play observed during training  in this context computing a robust counter-strategy [44  24].

5 Conclusion and Future Work

In this paper  we quantify a severe problem with independent reinforcement learners  joint pol-
icy correlation (JPC)  that limits the generality of these approaches. We describe a generalized
algorithm for multiagent reinforcement learning that subsumes several previous algorithms. In our
experiments  we show that PSRO/DCH produces general policies that signiﬁcantly reduce JPC in
partially-observable coordination games  and robust counter-strategies that safely exploit opponents
in a common competitive imperfect information game. The generality offered by PSRO/DCH can
be seen as a form of “opponent/teammate regularization”  and has also been observed recently in
practice [66  5]. We emphasize the game-theoretic foundations of these techniques  which we hope
will inspire further investigation into algorithm development for multiagent reinforcement learning.
In future work  we will consider maintaining diversity among oracles via loss penalties based on policy
dissimilarity  general response graph topologies  environments such as emergent language games [58]
and RTS games [96  84]  and other architectures for prediction of behavior  such as opponent
modeling [37] and imagining future states via auxiliary tasks [43]. We would also like to investigate
fast online adaptation [1  21] and the relationship to computational Theory of Mind [106  4]  as well
as generalized (transferable) oracles over similar opponent policies using successor features [6].
Acknowledgments. We would like to thank DeepMind and Google for providing an excellent
research environment that made this work possible. Also  we would like to thank the anonymous
reviewers and several people for helpful comments: Johannes Heinrich  Guy Lever  Remi Munos 
Joel Z. Leibo  Janusz Marecki  Tom Schaul  Noam Brown  Kevin Waugh  Georg Ostrovski  Sriram
Srinivasan  Neil Rabinowitz  and Vicky Holgate.

8

05001000150020002500Episodes (in thousands)012345NashConvNFSPDCHPSRO0100200300400500600700Episodes (in thousands)2468101214NashConvNFSPDCHPSRO2004006008001000120014001600Episodes (in thousands)21012Mean scoreNFSPDCHPSRO50010001500200025003000Episodes (in thousands)5432101Mean scoreNFSPDCHPSRO100200300400500600700800Episodes (in thousands)543210Mean scoreNFSPDCHPSROReferences

[1] Maruan Al-Shedivat  Trapit Bansal  Yuri Burda  Ilya Sutskever  Igor Mordatch  and Pieter Abbeel. Contin-
uous adaptation via meta-learning in nonstationary and competitive environments. CoRR  abs/1710.03641 
2017.

[2] Christopher Amato and Frans A. Oliehoek. Scalable planning and learning for multiagent POMDPs. In

AAAI15  pages 1995–2002  January 2015.

[3] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. Gambling in a rigged casino: The adversarial
multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer
Science  pages 322–331  1995.

[4] C.L. Baker  R.R. Saxe  and J.B. Tenenbaum. Bayesian theory of mind: Modeling joint belief-desire
attribution. In Proceedings of the Thirty-Third Annual Conference of the Cognitive Science Society  pages
2469–2474  2011.

[5] Trapit Bansal  Jakub Pachocki  Szymon Sidor  Ilya Sutskever  and Igor Mordatch. Emergent complexity

via multi-agent competition. CoRR  abs/1710.03748  2017.

[6] André Barreto  Will Dabney  Rémi Munos  Jonathan Hunt  Tom Schaul  David Silver  and Hado van
Hasselt. Transfer in reinforcement learning with successor features and generalised policy improvement.
In Proceedings of the Thirty-First Annual Conference on Neural Information Processing Systems (NIPS
2017)  2017. To appear. Preprint available at http://arxiv.org/abs/1606.05312.

[7] Daan Bloembergen  Karl Tuyls  Daniel Hennes  and Michael Kaisers. Evolutionary dynamics of multi-

agent learning: A survey. J. Artif. Intell. Res. (JAIR)  53:659–697  2015.

[8] A. Blum and Y. Mansour. Learning  regret minimization  and equilibria. In Algorithmic Game Theory 

chapter 4. Cambridge University Press  2007.

[9] Branislav Bosansky  Viliam Lisy  Jiri Cermak  Roman Vitek  and Michal Pechoucek. Using double-oracle
method and serialized alpha-beta search for pruning in simultaneous moves games. In Proceedings of the
Twenty-Third International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2013.

[10] Branislav Bošanský  Viliam Lisý  Marc Lanctot  Jiˇrí ˇCermák  and Mark H.M. Winands. Algorithms for
computing strategies in two-player simultaneous move games. Artiﬁcial Intelligence  237:1–40  2016.

[11] Michael Bowling  Neil Burch  Michael Johanson  and Oskari Tammelin. Heads-up Limit Hold’em Poker

is solved. Science  347(6218):145–149  January 2015.

[12] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artiﬁcial

Intelligence  136:215–250  2002.

[13] G. W. Brown. Iterative solutions of games by ﬁctitious play. In T.C. Koopmans  editor  Activity Analysis

of Production and Allocation  pages 374–376. John Wiley & Sons  Inc.  1951.

[14] Noam Brown  Sam Ganzfried  and Tuomas Sandholm. Hierarchical abstraction  distributed equilibrium
computation  and post-processing  with application to a champion no-limit Texas Hold’em agent. In
Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems  pages
7–15. International Foundation for Autonomous Agents and Multiagent Systems  2015.

[15] Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information games.

CoRR  abs/1705.02955  2017.

[16] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit

problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[17] L. Busoniu  R. Babuska  and B. De Schutter. A comprehensive survey of multiagent reinforcement
IEEE Transaction on Systems  Man  and Cybernetics  Part C: Applications and Reviews 

learning.
38(2):156–172  2008.

[18] Colin F. Camerer  Teck-Hua Ho  and Juin-Kuan Chong. A cognitive hierarchy model of games. The

Quarterly Journal of Economics  2004.

[19] C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. In
Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence (AAAI-98)  pages 746–752 
1998.

9

[20] M. A. Costa-Gomes and V. P. Crawford. Cognition and behavior in two-person guessing games: An

experimental study. The American Economy Review  96(6):1737–1768  2006.

[21] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th International Conference
on Machine Learning  volume 70 of Proceedings of Machine Learning Research  pages 1126–1135 
International Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[22] Jakob Foerster  Nantas Nardelli  Gregory Farquhar  Triantafyllos Afouras  Philip H. S. Torr  Pushmeet
Kohli  and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning (ICML 2017)  2017.

[23] Jakob N. Foerster  Yannis M. Assael  Nando de Freitas  and Shimon Whiteson. Learning to communicate
with deep multi-agent reinforcement learning. In 30th Conference on Neural Information Processing
Systems (NIPS 2016)  2016.

[24] Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics

and Computation (TEAC)  3(2):1–28  2015. Special issue on selected papers from EC-12.

[25] N. Gatti  F. Panozzo  and M. Restelli. Efﬁcient evolutionary dynamics with extensive-form games. In

Proceedings of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence  pages 335–341  2013.

[26] Richard Gibson. Regret minimization in non-zero-sum games with applications to building champion

multiplayer computer poker agents. CoRR  abs/1305.0034  2013.

[27] A. Gilpin. Algorithms for Abstracting and Solving Imperfect Information Games. PhD thesis  Carnegie

Mellon University  2009.

[28] Gmytrasiewicz and Doshi. A framework for sequential planning in multiagent settings. Journal of

Artiﬁcial Intelligence Research  24:49–79  2005.

[29] Amy Greenwald and Keith Hall. Correlated Q-learning. In Proceedings of the Twentieth International

Conference on Machine Learning (ICML 2003)  pages 242–249  2003.

[30] Amy Greenwald  Jiacui Li  and Eric Sodomka. Solving for best responses and equilibria in extensive-form
games with reinforcement learning methods. In Rohit Parikh on Logic  Language and Society  volume 11
of Outstanding Contributions to Logic  pages 185–226. Springer International Publishing  2017.

[31] Audrunas Gruslys  Mohammad Gheshlaghi Azar  Marc G. Bellemare  and Remi Munos. The Reactor: A

sample-efﬁcient actor-critic architecture. CoRR  abs/1704.04651  2017.

[32] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica 

68(5):1127–1150  2000.

[33] Sergiu Hart and Andreu Mas-Colell. A reinforcement procedure leading to correlated equilibrium. In

Economics Essays: A Festschrift for Werner Hildenbrand. Springer Berlin Heidelberg  2001.

[34] Jason S. Hartford  James R. Wright  and Kevin Leyton-Brown. Deep learning for predicting human
strategic behavior. In Proceedings of the 30th Conference on Neural Information Processing Systems
(NIPS 2016)  2016.

[35] Matthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. In

Proceedings of the International Conference on Learning Representations (ICLR)  May 2016.

[36] Matthew John Hausknecht. Cooperation and communication in multiagent deep reinforcement learning.

PhD thesis  University of Texas at Austin  Austin  USA  2016.

[37] He He  Jordan Boyd-Graber  Kevin Kwok    and Hal Daumé III. Opponent modeling in deep reinforcement
learning. In In Proceedings of The 33rd International Conference on Machine Learning (ICML)  pages
1804–1813  2016.

[38] Nicolas Heess  Gregory Wayne  David Silver  Timothy P. Lillicrap  Tom Erez  and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015  December 7-12  2015 
Montreal  Quebec  Canada  pages 2944–2952  2015.

[39] Johannes Heinrich  Marc Lanctot  and David Silver. Fictitious self-play in extensive-form games. In

Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)  2015.

10

[40] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-information

games. CoRR  abs/1603.01121  2016.

[41] Trong Nghia Hoang and Kian Hsiang Low. Interactive POMDP lite: Towards practical planning to predict
and exploit intentions for interacting with self-interested agents. In Proceedings of the Twenty-Third
International Joint Conference on Artiﬁcial Intelligence  IJCAI ’13  pages 2298–2305. AAAI Press  2013.

[42] Josef Hofbauer and William H. Sandholm. On the global convergence of stochastic ﬁctitious play.

Econometrica  70(6):2265–2294  11 2002.

[43] Max Jaderberg  Volodymyr Mnih  Wojciech Marian Czarnecki  Tom Schaul  Joel Z. Leibo  David
Silver  and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR 
abs/1611.05397  2016.

[44] M. Johanson  M. Zinkevich  and M. Bowling. Computing robust counter-strategies. In Advances in
Neural Information Processing Systems 20 (NIPS)  pages 1128–1135  2008. A longer version is available
as a University of Alberta Technical Report  TR07-15.

[45] Michael Johanson  Michael Bowling  Kevin Waugh  and Martin Zinkevich. Accelerating best response
calculation in large extensive games. In Proceedings of the Twenty-Second International Joint Conference
on Artiﬁcial Intelligence (IJCAI)  pages 258–265  2011.

[46] Michael Johanson  Neil Burch  Richard Valenzano  and Michael Bowling. Evaluating state-space
In Proceedings of the Twelfth International Conference on

abstractions in extensive-form games.
Autonomous Agents and Multi-Agent Systems (AAMAS)  2013.

[47] Michael Bradley Johanson. Robust Strategies and Counter-Strategies: From Superhuman to Optimal
Play. PhD thesis  University of Alberta  2016. http://johanson.ca/publications/theses/
2016-johanson-phd-thesis/2016-johanson-phd-thesis.pdf.

[48] Michael Kaisers and Karl Tuyls. Frequency adjusted multi-agent Q-learning.

In 9th International
Conference on Autonomous Agents and Multiagent Systems AAMAS 2010)  Toronto  Canada  May 10-14 
2010  Volume 1-3  pages 309–316  2010.

[49] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[50] M. Kleiman-Weiner  M. K. Ho  J. L. Austerweil  M. L. Littman  and J. B. Tenenbaum. Coordinate to
cooperate or compete: abstract goals and joint intentions in social interaction. In Proceedings of the 38th
Annual Conference of the Cognitive Science Society  2016.

[51] D. Koller  N. Megiddo  and B. von Stengel. Fast algorithms for ﬁnding randomized strategies in game
trees. In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC ’94)  pages 750–759 
1994.

[52] Kostas Kouvaris  Jeff Clune  Loizos Kounios  Markus Brede  and Richard A. Watson. How evolution
learns to generalise: Using the principles of learning theory to understand the evolution of developmental
organisation. PLOS Computational Biology  13(4):1–20  04 2017.

[53] H. W. Kuhn. Extensive games and the problem of information. Contributions to the Theory of Games 

2:193–216  1953.

[54] Marc Lanctot. Further developments of extensive-form replicator dynamics using the sequence-form
representation. In Proceedings of the Thirteenth International Conference on Autonomous Agents and
Multi-Agent Systems (AAMAS)  pages 1257–1264  2014.

[55] Marc Lanctot  Vinicius Zambaldi  Audr¯unas Gruslys  Angeliki Lazaridou  Karl Tuyls  Julien Pérolat 
David Silver  and Thore Graepel. A uniﬁed game-theoretic approach to multiagent reinforcement learning.
CoRR  abs/1711.00832  2017.

[56] M. Lauer and M. Riedmiller. Reinforcement learning for stochastic cooperative multi-agent systems. In

Proceedings of the AAMAS ’04  New York  2004.

[57] Guillaume J. Laurent  Laëtitia Matignon  and Nadine Le Fort-Piat. The world of independent learners
International Journal of Knowledge-Based and Intelligent Engineering Systems 

is not Markovian.
15(1):55–64  March 2011.

[58] Angeliki Lazaridou  Alexander Peysakhovich  and Marco Baroni. Multi-agent cooperation and the emer-
gence of (natural) language. In Proceedings of the International Conference on Learning Representations
(ICLR)  April 2017.

11

[59] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521:436–444  2015.

[60] Joel Z. Leibo  Vinicius Zambaldia  Marc Lanctot  Janusz Marecki  and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. In Proceedings of the Sixteenth International
Conference on Autonomous Agents and Multiagent Systems  2017.

[61] David S. Leslie and Edmund J. Collins. Generalised weakened ﬁctitious play. Games and Economic

Behavior  56(2):285–298  2006.

[62] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning.

In In
Proceedings of the Eleventh International Conference on Machine Learning  pages 157–163. Morgan
Kaufmann  1994.

[63] Michael L. Littman. Friend-or-foe Q-learning in general-sum games. In Proceedings of the Eighteenth
International Conference on Machine Learning  ICML ’01  pages 322–328  San Francisco  CA  USA 
2001. Morgan Kaufmann Publishers Inc.

[64] Michael L. Littman. Reinforcement learning improves behaviour from evaluative feedback. Nature 

521:445–451  2015.

[65] J. Long  N. R. Sturtevant  M. Buro  and T. Furtak. Understanding the success of perfect information Monte
Carlo sampling in game tree search. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence 
pages 134–140  2010.

[66] Ryan Lowe  Yi Wu  Aviv Tamar  Jean Harb  Pieter Abbeel  and Igor Mordatch. Multi-agent actor-critic

for mixed cooperative-competitive environments. CoRR  abs/1706.02275  2017.

[67] N. Burch M. Zinkevich  M. Bowling. A new algorithm for generating equilibria in massive zero-sum

games. In Proceedings of the Twenty-Seventh Conference on Artiﬁcial Intelligence (AAAI-07)  2007.

[68] Janusz Marecki  Tapana Gupta  Pradeep Varakantham  Milind Tambe  and Makoto Yokoo. Not all agents
are equal: Scaling up distributed pomdps for agent networks. In Proceedings of the Seventh International
Joint Conference on Autonomous Agents and Multi-agent Systems  2008.

[69] Vukosi N. Marivate. Improved Empirical Methods in Reinforcement Learning Evaluation. PhD thesis 

Rutgers  New Brunswick  New Jersey  2015.

[70] L. Matignon  G. J. Laurent  and N. Le Fort-Piat. Independent reinforcement learners in cooperative
Markov games: a survey regarding coordination problems. The Knowledge Engineering Review  27(01):1–
31  2012.

[71] H.B. McMahan  G. Gordon  and A. Blum. Planning in the presence of cost functions controlled by an
adversary. In Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003) 
2003.

[72] Volodymyr Mnih  Adrià Puigdomènech Badia  Mehdi Mirza  Alex Graves  Timothy P. Lillicrap  Tim
Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
In Proceedings of the 33rd International Conference on Machine Learning (ICML)  pages 1928–1937 
2016.

[73] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G. Bellemare 
Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Petersen  Charles Beattie 
Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra  Shane Legg  and
Demis Hassabis. Human-level control through deep reinforcement learning. Nature  518:529–533  2015.

[74] Matej Moravˇcík  Martin Schmid  Neil Burch  Viliam Lisý  Dustin Morrill  Nolan Bard  Trevor Davis 
Kevin Waugh  Michael Johanson  and Michael Bowling. Deepstack: Expert-level artiﬁcial intelligence in
heads-up no-limit poker. Science  358(6362)  October 2017.

[75] R. Munos  T. Stepleton  A. Harutyunyan  and M. G. Bellemare. Safe and efﬁcient off-policy reinforcement

learning. In Advances in Neural Information Processing Systems  2016.

[76] Ranjit Nair. Coordinating multiagent teams in uncertain domains using distributed POMDPs. PhD thesis 

University of Southern California  Los Angeles  USA  2004.

[77] Junhyuk Oh  Xiaoxiao Guo  Honglak Lee  Richard L. Lewis  and Satinder P. Singh. Action-conditional
video prediction using deep networks in atari games. In Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015  December 7-12  2015 
Montreal  Quebec  Canada  pages 2863–2871  2015.

12

[78] F.A. Oliehoek  E.D. de Jong  and N. Vlassis. The parallel Nash memory for asymmetric games. In

Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)  2006.

[79] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer-

Briefs in Intelligent Systems. Springer  2016. Authors’ pre-print.

[80] Shayegan Omidshaﬁei  Jason Pazis  Christopher Amato  Jonathan P. How  and John Vian. Deep decen-
tralized multi-task multi-agent reinforcement learning under partial observability. In Proceedings of the
34th International Conference on Machine Learning (ICML 2017)  2017.

[81] Liviu Panait  Karl Tuyls  and Sean Luke. Theoretical advantages of lenient learners: An evolutionary

game theoretic perspective. Journal of Machine Learning Research  9:423–457  2008.

[82] David C. Parkes and Michael P. Wellman. Economic reasoning and artiﬁcial intelligence. Science 

349(6245):267–272  2015.

[83] Marc Ponsen  Karl Tuyls  Michael Kaisers  and Jan Ramon. An evolutionary game theoretic analysis of

poker strategies. Entertainment Computing  2009.

[84] F. Sailer  M. Buro  and M. Lanctot. Adversarial planning through strategy simulation. In IEEE Symposium

on Computational Intelligence and Games (CIG)  pages 37–45  2007.

[85] Spyridon Samothrakis  Simon Lucas  Thomas Philip Runarsson  and David Robles. Coevolving Game-
IEEE Transactions on Evolutionary

Playing Agents: Measuring Performance and Intransitivities.
Computation  April 2013.

[86] Martin Schmid  Matej Moravcik  and Milan Hladik. Bounding the support size in extensive form
games with imperfect information. In Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial
Intelligence  2014.

[87] L. Julian Schvartzman and Michael P. Wellman. Stronger CDA strategies through empirical game-
theoretic analysis and reinforcement learning. In Proceedings of The 8th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS)  pages 249–256  2009.

[88] Wenling Shang  Kihyuk Sohn  Diogo Almeida  and Honglak Lee. Understanding and improving con-
volutional neural networks via concatenated rectiﬁed linear units. In Proceedings of the International
Conference on Machine Learning (ICML)  2016.

[89] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic  Game-Theoretic  and Logical

Foundations. Cambridge University Press  2009.

[90] Yoav Shoham  Rob Powers  and Trond Grenager. If multi-agent learning is the answer  what is the

question? Artif. Intell.  171(7):365–377  2007.

[91] David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den Driessche  Julian
Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander Dieleman  Dominik
Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap  Madeleine Leach  Koray
Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the game of Go with deep neural networks
and tree search. Nature  529:484–489  2016.

[92] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur Guez 
Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  Yutian Chen  Timothy Lillicrap  Fan Hui 
Laurent Sifre  George van den Driessche  Thore Graepel  and Demis Hassabis. Mastering the game of go
without human knowledge. Nature  550:354–359  2017.

[93] S. Sukhbaatar  A. Szlam  and R. Fergus. Learning multiagent communication with backpropagation. In

30th Conference on Neural Information Processing Systems (NIPS 2016)  2016.

[94] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.

[95] Ardi Tampuu  Tambet Matiisen  Dorian Kodelja  Ilya Kuzovkin  Kristjan Korjus  Juhan Aru  Jaan Aru 
and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PLoS ONE 
12(4)  2017.

[96] Anderson Tavares  Hector Azpurua  Amanda Santos  and Luiz Chaimowicz. Rock  paper  starcraft:
Strategy selection in real-time strategy games. In The Twelfth AAAI Conference on Artiﬁcial Intelligence
and Interactive Digital Entertainment (AIIDE-16)  2016.

13

[97] Taylor and Jonker. Evolutionarily stable strategies and game dynamics. Mathematical Biosciences 

40:145–156  1978.

[98] K. Tuyls and R. Westra. Replicator dynamics in discrete and continuous strategy spaces. In Agents 

Simulation and Applications  pages 218–243. Taylor and Francis  2008.

[99] Karl Tuyls and Gerhard Weiss. Multiagent learning: Basics  challenges  and prospects. AI Magazine 

33(3):41–52  2012.

[100] W. E. Walsh  R. Das  G. Tesauro  and J.O. Kephart. Analyzing complex strategic interactions in multi-

agent games. In AAAI-02 Workshop on Game Theoretic and Decision Theoretic Agents  2002.  2002.

[101] Michael P. Wellman. Methods for empirical game-theoretic analysis. In Proceedings of the National

Conference on Artiﬁcial Intelligence (AAAI)  2006.

[102] S. Whiteson  B. Tanner  M. E. Taylor  and P. Stone. Protecting against evaluation overﬁtting in empirical
reinforcement learning. In 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL)  pages 120–127  2011.

[103] James R. Wright and Kevin Leyton-Brown. Beyond equilibrium: Predicting human behavior in normal
form games. In Proceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-10) 
pages 901–907  2010.

[104] Mason Wright. Using reinforcement learning to validate empirical game-theoretic analysis: A continuous

double auction study. CoRR  abs/1604.06710  2016.

[105] Nikolai Yakovenko  Liangliang Cao  Colin Raffel  and James Fan. Poker-CNN: A pattern learning
strategy for making draws and bets in poker games using convolutional networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

[106] Wako Yoshida  Ray J. Dolan  and Karl J. Friston. Game theory of mind. PLOS Computational Biology 

4(12):1–14  12 2008.

[107] M. Zinkevich  M. Johanson  M. Bowling  and C. Piccione. Regret minimization in games with incomplete

information. In Advances in Neural Information Processing Systems 20 (NIPS 2007)  2008.

14

,Marc Lanctot
Vinicius Zambaldi
Audrunas Gruslys
Angeliki Lazaridou
Karl Tuyls
David Silver
Thore Graepel