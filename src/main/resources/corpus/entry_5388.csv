2008,Syntactic Topic Models,We develop \name\ (STM)  a nonparametric Bayesian model of parsed documents. \Shortname\ generates words that are both thematically and syntactically constrained  which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree specific syntactic transitions. Words are assumed generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes  and we report qualitative and quantitative results on both synthetic data and hand-parsed documents.,Syntactic Topic Models

Jordan Boyd-Graber

Department of Computer Science

35 Olden Street

Princeton University
Princeton  NJ 08540

jbg@cs.princeton.edu

blei@cs.princeton.edu

David Blei

Department of Computer Science

35 Olden Street

Princeton University
Princeton  NJ 08540

Abstract

We develop the syntactic topic model (STM)  a nonparametric Bayesian model
of parsed documents. The STM generates words that are both thematically and
syntactically constrained  which combines the semantic insights of topic models
with the syntactic information available from parse trees. Each word of a sentence
is generated by a distribution that combines document-speciﬁc topic weights and
parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an
order that respects the parse tree. We derive an approximate posterior inference
method based on variational methods for hierarchical Dirichlet processes  and we
report qualitative and quantitative results on both synthetic data and hand-parsed
documents.

1 Introduction

Probabilistic topic models provide a suite of algorithms for ﬁnding low dimensional structure in a
corpus of documents. When ﬁt to a corpus  the underlying representation often corresponds to the
“topics” or “themes” that run through it. Topic models have improved information retrieval [1]  word
sense disambiguation [2]  and have additionally been applied to non-text data  such as for computer
vision and collaborative ﬁltering [3  4].
Topic models are widely applied to text despite a willful ignorance of the underlying linguistic
structures that exist in natural language. In a topic model  the words of each document are assumed
to be exchangeable; their probability is invariant to permutation. This simpliﬁcation has proved
useful for deriving efﬁcient inference techniques and quickly analyzing very large corpora [5].
However  exchangeable word models are limited. While useful for classiﬁcation or information
retrieval  where a coarse statistical footprint of the themes of a document is sufﬁcient for success 
exchangeable word models are ill-equipped for problems relying on more ﬁne-grained qualities of
language. For instance  although a topic model can suggest documents relevant to a query  it cannot
ﬁnd particularly relevant phrases for question answering. Similarly  while a topic model might
discover a pattern such as “eat” occurring with “cheesecake ” it lacks the representation to describe
selectional preferences  the process where certain words restrict the choice of the words that follow.
It is in this spirit that we develop the syntactic topic model  a nonparametric Bayesian topic model
that can infer both syntactically and thematically coherent topics. Rather than treating words as the
exchangeable unit within a document  the words of the sentences must conform to the structure of a
parse tree. In the generative process  the words arise from a distribution that has both a document-
speciﬁc thematic component and a parse-tree-speciﬁc syntactic component.
We illustrate this idea with a concrete example. Consider a travel brochure with the sentence “In
.” Both the low-level syntactic context of a word and
the near future  you could ﬁnd yourself in
its document context constrain the possibilities of the word that can appear next. Syntactically  it

1

(a) Overall Graphical Model

(b) Sentence Graphical Model

Figure 1: In the graphical model of the STM  a document is made up of a number of sentences 
represented by a tree of latent topics z which in turn generate words w. These words’ topics are
chosen by the topic of their parent (as encoded by the tree)  the topic weights for a document θ 
and the node’s parent’s successor weights π. (For clarity  not all dependencies of sentence nodes
are shown.) The structure of variables for sentences within the document plate is on the right  as
demonstrated by an automatic parse of the sentence “Some phrases laid in his mind for years.” The
STM assumes that the tree structure and words are given  but the latent topics z are not.

is going to be a noun consistent as the object of the preposition “of.” Thematically  because it is in
a travel brochure  we would expect to see words such as “Acapulco ” “Costa Rica ” or “Australia”
more than “kitchen ” “debt ” or “pocket.” Our model can capture these kinds of regularities and
exploit them in predictive problems.
Previous efforts to capture local syntactic context include semantic space models [6] and similarity
functions derived from dependency parses [7]. These methods successfully determine words that
share similar contexts  but do not account for thematic consistency. They have difﬁculty with pol-
ysemous words such as “ﬂy ” which can be either an insect or a term from baseball. With a sense
of document context  i.e.  a representation of whether a document is about sports or animals  the
meaning of such terms can be distinguished.
Other techniques have attempted to combine local context with document coherence using linear
sequence models [8  9]. While these models are powerful  ordering words sequentially removes
the important connections that are preserved in a syntactic parse. Moreover  these models gener-
ate words either from the syntactic or thematic context. In the syntactic topic model  words are
constrained to be consistent with both.
The remainder of this paper is organized as follows. We describe the syntactic topic model  and
develop an approximate posterior inference technique based on variational methods. We study its
performance both on synthetic data and hand parsed data [10]. We show that the STM captures
relationships missed by other models and achieves lower held-out perplexity.

2 The syntactic topic model

We describe the syntactic topic model (STM)  a document model that combines observed syntactic
structure and latent thematic structure. To motivate this model  we return to the travel brochure
.”. The word that ﬁlls in the blank is
sentence “In the near future  you could ﬁnd yourself in
constrained by its syntactic context and its document context. The syntactic context tells us that it is
an object of a preposition  and the document context tells us that it is a travel-related word.
The STM attempts to capture these joint inﬂuences on words.
It models a document corpus as
exchangeable collections of sentences  each of which is associated with a tree structure such as a

2

ααTβπkτk∞MθdαDσParse trees grouped into M documentsw1:laidw2:phrasesw6:forw5:hisw4:somew5:mindw7:yearsw3:inz1z2z3z4z5z5z6z7parse tree (Figure 1(b)). The words of each sentence are assumed to be generated from a distribution
inﬂuenced both by their observed role in that tree and by the latent topics inherent in the document.
The latent variables that comprise the model are topics  topic transition vectors  topic weights  topic
assignments  and top-level weights. Topics are distributions over a ﬁxed vocabulary (τk in Figure
1). Each is further associated with a topic transition vector (πk)  which weights changes in topics
between parent and child nodes. Topic weights (θd) are per-document vectors indicating the degree
to which each document is “about” each topic. Topic assignments (zn  associated with each internal
node of 1(b)) are per-word indicator variables that refer to the topic from which the corresponding
word is assumed to be drawn. The STM is a nonparametric Bayesian model. The number of topics
is not ﬁxed  and indeed can grow with the observed data.
The STM assumes the following generative process of a document collection.

1. Choose global topic weights β ∼ GEM(α)
2. For each topic index k = {1  . . .}:

3. For each document d = {1  . . . M}:

(a) Choose topic τk ∼ Dir(σ)
(b) Choose topic transition distribution πk ∼ DP(αT   β)
(a) Choose topic weights θd ∼ DP(αD  β)
(b) For each sentence in the document:

i. Choose topic assignment z0 ∝ θdπstart
ii. Choose root word w0 ∼ mult(1  τz0)
iii. For each additional word wn and parent pn  n ∈ {1  . . . dn}

• Choose topic assignment zn ∝ θdπzp(n)
• Choose word wn ∼ mult(1  τzn)

The distinguishing feature of the STM is that the topic assignment is drawn from a distribution that
combines two vectors: the per-document topic weights and the transition probabilities of the topic
assignment from its parent node in the parse tree. By merging these vectors  the STM models both
the local syntactic context and corpus-level semantics of the words in the documents. Because they
depend on their parents  the topic assignments and words are generated by traversing the tree.
A natural alternative model would be to traverse the tree and choose the topic assignment from either
the parental topic transition πzp(n) or document topic weights θd  based on a binary selector variable.
This would be an extension of [8] to parse trees  but it does not enforce words to be syntactically
consistent with their parent nodes and be thematically consistent with a topic of the document. Only
one of the two conditions must be true. Rather  this approach draws on the idea behind the product
of experts [11]  multiplying two vectors and renormalizing to obtain a new distribution. Taking
the point-wise product can be thought of as viewing one distribution through the “lens” of another 
effectively choosing only words whose appearance can be explained by both.
The STM is closely related to the hierarchical Dirichlet process (HDP). The HDP is an extension of
Dirichlet process mixtures to grouped data [12]. Applied to text  the HDP is a probabilistic topic
model that allows each document to exhibit multiple topics. It can be thought of as the “inﬁnite”
topic version of latent Dirichlet allocation (LDA) [13]. The difference between the STM and the
HDP is in how the per-word topic assignment is drawn. In the HDP  this topic assignment is drawn
directly from the topic weights and  thus  the HDP assumes that words within a document are ex-
changeable. In the STM  the words are generated conditioned on their parents in the parse tree. The
exchangeable unit is a sentence.
The STM is also closely related to the inﬁnite tree with independent children [14]. The inﬁnite tree
models syntax by basing the latent syntactic category of children on the syntactic category of the
parent. The STM reduces to the Inﬁnite Tree when θd is ﬁxed to a vector of ones.

3 Approximate posterior inference

The central computational problem in topic modeling is to compute the posterior distribution of the
latent structure conditioned on an observed collection of documents. Speciﬁcally  our goal is to
compute the posterior topics  topic transitions  per-document topic weights  per-word topic assign-

3

ments  and top-level weights conditioned on a set of documents  each of which is a collection of
parse trees.
This posterior distribution is intractable to compute. In typical topic modeling applications  it is
approximated with either variational inference or collapsed Gibbs sampling. Fast Gibbs sampling
relies on the conjugacy between the topic assignment and the prior over the distribution that gen-
erates it. The syntactic topic model does not enjoy such conjugacy because the topic assignment is
drawn from a multiplicative combination of two Dirichlet distributed vectors. We appeal to varia-
tional inference.
In variational inference  the posterior is approximated by positing a simpler family of distributions 
indexed by free variational parameters. The variational parameters are ﬁt to be close in relative
entropy to the true posterior. This is equivalent to maximizing the Jensen’s bound on the marginal
probability of the observed data [15].
We use a fully-factorized variational distribution 

q(β  z  θ  π  τ|β∗  φ  γ  ν) = q(β|β∗)Q

(1)
Following [16]  q(β|β∗) is not a full distribution  but is a degenerate point estimate truncated so that
all weights whose index is greater than K are zero in the variational distribution. The variational
parameters γd and νz index Dirichlet distributions  and φn is a topic multinomial for the nth word.
From this distribution 
the Jensen’s lower bound on the log probability of the corpus is
L(γ  ν  φ; β  θ  π  τ) =

d q(θd|γd)Q

k q(πk|νk)Q

n q(zn|φn).

Eq [log p(β|α) + log p(θ|αD  β) + log p(π|αP   β) + log p(z|θ  π)+

log p(w|z  τ ) + log p(τ|σ)] − Eq [log q(θ) + log q(π) + log q(z)] .

(2)
Expanding Eq [log p(z|θ  π)] is difﬁcult  so we add an additional slack parameter  ωn to approximate
the expression. This derivation and the complete likelihood bound is given in the supplement. We
use coordinate ascent to optimize the variational parameters to be close to the true posterior.

Per-word variational updates The variational update for the topic assignment of the nth word is

n
Ψ(γi) − Ψ(PK
j=1 γj) +PK
+P
PK
−P
PK
c∈c(n)
P
c∈c(n) ω−1

(cid:16)
P

j=1 φc j

c

j

γj νi j

j=1 φp(n) j
Ψ(νi j) − Ψ

(cid:16)
(cid:16)PK

Ψ(νj i) − Ψ

(cid:17)(cid:17)

k=1 νi k

o

(cid:16)PK

k=1 νj k

(cid:17)(cid:17)

φni ∝ exp

k γk

k νi k

+ log τi wn

.

(3)

The inﬂuences on estimating the posterior of a topic assignment are: the document’s topic γ  the
topic of the node’s parent p(n)  the topic of the node’s children c(n)  the expected transitions be-
tween topics ν  and the probability of the word within a topic τi wn.
Most terms in Equation 3 are familiar from variational inference for probabilistic topic models  as
the digamma functions appear in the expectations of multinomial distributions. The second to last
term is new  however  because we cannot assume that the point-wise product of πk and θd will sum
to one. We approximate the normalizer for their produce by introducing ω; its update is

ωn =X

X

i=1

j=1

PK

PK

γiνj i

k=1 γk

k=1 νj k

.

φp(n) j

Variational Dirichlet distributions and topic composition This normalizer term also appears in
the derivative of the likelihood function for γ and ν (the parameters to the variational distributions
on θ and π  respectively)  which cannot be solved in a closed form. We use conjugate gradient
optimization to determine the appropriate updates for these parameters [17].
Top-level weights Finally  we consider the top-level weights. The ﬁrst K − 1 stick-breaking
proportions are drawn from a Beta distribution with parameters (1  α)  but we assume that the ﬁnal
stick-breaking proportion is unity (thus implying β∗ is non-zero only from 1 . . . K). Thus  we only
optimize the ﬁrst K − 1 positions and implicitly take β∗
β∗
i . This constrained
optimization is performed using the barrier method [17].

K = 1 −PK−1

i

4

4 Empirical results

Before considering real-world data  we demonstrate the STM on synthetic natural language data. We
generated synthetic sentences composed of verbs  nouns  prepositions  adjectives  and determiners.
Verbs were only in the head position; prepositions could appear below nouns or verbs; nouns only
appeared below verbs; prepositions or determiners and adjectives could appear below nouns. Each
of the parts of speech except for prepositions and determiners were sub-grouped into themes  and
a document contains a single theme for each part of speech. For example  a document can only
contain nouns from a single “economic ” “academic ” or “livestock” theme.
Using a truncation level of 16  we ﬁt three different nonparametric Bayesian language models to
the synthetic data (Figure 2).1 The inﬁnite tree model is aware of the tree structure but not docu-
ments [14] It is able to separate parts of speech successfully except for adjectives and determiners
(Figure 2(a)). However  it ignored the thematic distinctions that actually divided the terms between
documents. The HDP is aware of document groupings and treats the words exchangeably within
them [12]. It is able to recover the thematic topics  but has missed the connections between the parts
of speech  and has conﬂated multiple parts of speech (Figure 2(b)).
The STM is able to capture the the topical themes and recover parts of speech (with the exception of
prepositions that were placed in the same topic as nouns with a self loop). Moreover  it was able to
identify the same interconnections between latent classes that were apparent from the inﬁnite tree.
Nouns are dominated by verbs and prepositions  and verbs are the root (head) of sentences.

Qualitative description of topics learned from hand-annotated data The same general proper-
ties  but with greater variation  are exhibited in real data. We converted the Penn Treebank [10]  a
corpus of manually curated parse trees  into a dependency parse [18]. The vocabulary was pruned
to terms that appeared in at least ten documents.
Figure 3 shows a subset of topics learned by the STM with truncation level 32. Many of the re-
sulting topics illustrate both syntactic and thematic consistency. A few nonspeciﬁc function topics
emerged (pronoun  possessive pronoun  general verbs  etc.). Many of the noun categories were more
specialized. For instance  Figure 3 shows clusters of nouns relating to media  individuals associated
with companies (“mr ” “president ” “chairman”)  and abstract nouns related to stock prices (“shares ”
“quarter ” “earnings ” “interest”)  all of which feed into a topic that modiﬁes nouns (“his ” “their ”
“other ” “last”). Thematically related topics are separated by both function and theme.
This division between functional and topical uses for the latent classes can also been seen in the
values for the per-document multinomial over topics. A number of topics in Figure 3(b)  such as 17 
15  10  and 3  appear to some degree in nearly every document  while other topics are used more
sparingly to denote specialized content. With α = 0.1  this plot also shows that the nonparametric
Bayesian framework is ignoring many later topics.

Perplexity To study the performance of the STM on new data  we estimated the held out proba-
bility of previously unseen documents with an STM trained on a portion of the Penn Treebank. For
each position in the parse trees  we estimate the probability the observed word. We compute the
perplexity as the exponent of the inverse of the per-word average log probability. The lower the per-
plexity  the better the model has captured the patterns in the data. We also computed perplexity for
individual parts of speech to study the differences in predictive power between content words  such
as nouns and verbs  and function words  such as prepositions and determiners. This illustrates how
different algorithms better capture aspects of context. We expect function words to be dominated by
local context and content words to be determined more by the themes of the document.
This trend is seen not only in the synthetic data (Figure 4(a))  where parsing models better predict
functional categories like prepositions and document only models fail to account for patterns of
verbs and determiners  but also in real data. Figure 4(b) shows that HDP and STM both perform
better than parsing models in capturing the patterns behind nouns  while both the STM and the
inﬁnite tree have lower perplexity for verbs. Like parsing models  our model was better able to

1In Figure 2 and Figure 3  we mark topics which represent a single part of speech and are essentially the lone
representative of that part of speech in the model. This is a subjective determination of the authors  does not
reﬂect any specialization or special treatment of topics by the model  and is done merely for didactic purposes.

5

START
1.00

ponders   
discusses  falls  
queries  runs

0.46

PROFESSOR   

PHD_CANDIDATE   
GRAD_STUDENT   

SHEEP  
PONY 

0.20

0.98

0.28

0.61

on  about   
over  with

that  evil  the  

this  a

PHD_CANDIDATE   
GRAD_STUDENT   with  
over  PROFESSOR  on

PROFESSOR  

PHD_CANDIDATE  evil  

GRAD_STUDENT  

ponders. this

SHEEP  COW  PONY  

over  about  on

PROFESSOR  

GRAD_STUDENT  over  
PHD_CANDIDATE  on

STOCK  

MUTUAL_FUND  
SHARE  with  about

SHARE  STOCK  

MUTUAL_FUND  on  

over

(a) Parse transition only

(b) Document multinomial only

Themes

START

 0.13

 0.26

 0.30

bucks  surges  
climbs  falls  runs

 0.26
ponders  
falls  runs

discusses  queries  

 0.36

 0.22

 0.30
 0.35

0.37

SHEEP  PONY  
COW  over  with

runs  falls  walks  

sits  climbs

 0.30

 0.27

 0.22

 0.36

STOCK SHARE  
MUTUAL_FUND  

on  with

0.38

 0.31

 0.23

 0.33

hates  dreads  
mourns  fears  

despairs

 0.40

 0.24
 0.30
 0.35

PROFESSOR  

PHD_CANDIDATE  
GRAD_STUDENT  

over  on

P
a
r
t
s
 
o
f
 

S
p
e
e
c
h

0.33

 0.33

 0.30

 0.26

evil  that  this  the

stupid  that  the  

insolent

evil  this  the  that

(c) Combination of parse transition and document multinomial

Figure 2: Three models were ﬁt to the synthetic data described in Section 4. Each box illustrates the
top ﬁve words of a topic; boxes that represent homogenous parts of speech have rounded edges and
are shaded. Edges between topics are labeled with estimates of their transition weight π. While the
inﬁnite tree model (a) is able to reconstruct the parts of speech used to generate the data  it lumps
all topics into the same categories. Although the HDP (b) can discover themes of recurring words  it
cannot determine the interactions between topics or separate out ubiquitous words that occur in all
documents. The STM (c) is able to recover the structure.

predict the appearance of prepositions  but also remained competitive with HDP on content words.
On the whole  the STM had lower perplexity than HDP and the inﬁnite tree.

5 Discussion

We have introduced and evaluated the syntactic topic model  a nonparametric Bayesian model of
parsed documents. The STM achieves better perplexity than the inﬁnite tree or the hierarchical
Dirichlet process and uncovers patterns in text that are both syntactically and thematically consistent.
This dual relevance is useful for work in natural language processing. For example  recent work [19 
20] in the domain of word sense disambiguation has attempted to combine syntactic similarity with
topical information in an ad hoc manner to improve the predominant sense algorithm [21]. The
syntactic topic model offers a principled way to learn both simultaneously rather than combining
two heterogenous methods.
The STM is not a full parsing model  but it could be used as a means of integrating document
context into parsing models. This work’s central premise is consistent with the direction of recent
improvements in parsing technology in that it provides a method for reﬁning the parts of speech
present in a corpus. For example  lexicalized parsers [22] create rules speciﬁc to individual terms 
and grammar reﬁnement [23] divides general roles into multiple  specialized ones. The syntactic
topic model offers an alternative method of ﬁnding more speciﬁc rules by grouping words together
that appear in similar documents and could be extended to a full parser.

6

START
 0.95

says  
could  
can  did  
do  may  
does  say

0.11

0.37

 0.57

 0.29

0.11

they  who  
he  there  
one  we  
also  if

 0.28

television  
public  
australia  
cable  host  
franchise  
service

0.34

 0.67

 0.09
0.25

 0.06

his  their  
other  us  
its  last  
one  all

 0.10

 0.42

market  sales  

shares  
quarter  
earnings  
interest  
months  yield

0.22

t

n
e
m
u
c
o
D

 0.06

 0.31

mr  inc  co  
president  
chairman  
vice  analyst 

corp  

 0.08

europe  
eastern  
protection  
corp  poland  
hungary  
chapter  aid

0.52

(a) Sinks and sources

garden  visit  
having  aid  

prime  
despite  
minister  
especially

 0.26

policy  
gorbachev  
mikhail  
leader  soviet  
restructuring  

software

sales
market
junk
fund
bonds

his
her
their
other
one

says
could
can
did
do

1 3 5 7 9

12

15

18

21

24

27

30

Topic

(b) Topic usage

Figure 3: Selected topics (along with strong links) after a run of the syntactic topic model with
a truncation level of 32. As in Figure 2  parts of speech that aren’t subdivided across themes are
indicated. In the Treebank corpus (left)  head words (verbs) are shared  but the nouns split off into
many separate specialized categories before feeding into pronoun sinks. The specialization of topics
is also visible in plots of the variational parameter γ normalized for the ﬁrst 300 documents of the
Treebank (right)  where three topics columns have been identiﬁed. Many topics are used to some
extent in every document  showing that they are performing a functional role  while others are used
more sparingly for semantic content.

HDP
Infinite Tree  independent children
STM

  30

  25

y
t
i
x
e
l
p
r
e
P

  20

  15

  10

  5

  0

ALL

NOUN

VERB

ADJ
(a) Synthetic

DET

PREP

y
t
i
x
e
l
p
r
e
P

  4 500
  4 000
  3 500
  3 000
  2 500
  2 000
  1 500
  1 000
  500
  0

ALL

HDP
Infinite Tree  independent children
STM

PREP

(b) Treebank

NOUN

VERB

Figure 4: After ﬁtting three models on synthetic data  the syntactic topic model has better (lower)
perplexity on all word classes except for adjectives. HDP is better able to capture document- level
patterns of adjectives. The inﬁnite tree captures prepositions best  which have no cross- document
variation. On real data 4(b)  the syntactic topic model was able to combine the strengths of the
inﬁnite tree on functional categories like prepositions with the strengths of the HDP on content
categories like nouns to attain lower overall perplexity.

While traditional topic models reveal groups of words that are used in similar documents  the STM
uncovers groups that are used the same way in similar documents. This decomposition is useful for
tasks that require a more ﬁne- grained representation of language than the bag of words can offer or
for tasks that require a broader context than parsing models.

References
[1] Wei  X.  B. Croft. LDA- based document models for ad- hoc retrieval. In Proceedings of the ACM SIGIR

Conference on Research and Development in Information Retrieval. 2006.

[2] Cai  J. F.  W. S. Lee  Y. W. Teh. NUS- ML:Improving word sense disambiguation using topic features. In

Proceedings of SemEval- 2007. Association for Computational Linguistics  2007.

7

[3] Fei-Fei Li  P. Perona. A Bayesian hierarchical model for learning natural scene categories. In CVPR ’05

- Volume 2  pages 524–531. IEEE Computer Society  Washington  DC  USA  2005.

[4] Marlin  B. Modeling user rating proﬁles for collaborative ﬁltering. In S. Thrun  L. Saul  B. Sch¨olkopf 

eds.  Advances in Neural Information Processing Systems. MIT Press  Cambridge  MA  2004.

[5] Grifﬁths  T.  M. Steyvers. Probabilistic topic models.

In T. Landauer  D. McNamara  S. Dennis 

W. Kintsch  eds.  Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum  2006.

[6] Pad´o  S.  M. Lapata. Dependency-based construction of semantic space models. Computational Linguis-

tics  33(2):161–199  2007.

[7] Lin  D. An information-theoretic deﬁnition of similarity. In Proceedings of International Conference of

Machine Learning  pages 296–304. 1998.

[8] Grifﬁths  T. L.  M. Steyvers  D. M. Blei  et al. Integrating topics and syntax. In L. K. Saul  Y. Weiss 
L. Bottou  eds.  Advances in Neural Information Processing Systems  pages 537–544. MIT Press  Cam-
bridge  MA  2005.

[9] Gruber  A.  M. Rosen-Zvi  Y. Weiss. Hidden topic Markov models. In Proceedings of Artiﬁcial Intelli-

gence and Statistics. San Juan  Puerto Rico  2007.

[10] Marcus  M. P.  B. Santorini  M. A. Marcinkiewicz. Building a large annotated corpus of English: The

Penn treebank. Computational Linguistics  19(2):313–330  1994.

[11] Hinton  G. Products of experts. In Proceedings of the Ninth International Conference on Artiﬁcial Neural

Networks  pages 1–6. IEEE  Edinburgh  Scotland  1999.

[12] Tee  Y. W.  M. I. Jordan  M. J. Beal  et al. Hierarchical dirichlet processes. Journal of the American

Statistical Association  101(476):1566–1581  2006.

[13] Blei  D.  A. Ng  M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research  3:993–

1022  2003.

[14] Finkel  J. R.  T. Grenager  C. D. Manning. The inﬁnite tree. In Proceedings of Association for Computa-
tional Linguistics  pages 272–279. Association for Computational Linguistics  Prague  Czech Republic 
2007.

[15] Jordan  M.  Z. Ghahramani  T. S. Jaakkola  et al. An introduction to variational methods for graphical

models. Machine Learning  37(2):183–233  1999.

[16] Liang  P.  S. Petrov  M. Jordan  et al. The inﬁnite PCFG using hierarchical Dirichlet processes.

Proceedings of Emperical Methods in Natural Language Processing  pages 688–697. 2007.

In

[17] Boyd  S.  L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[18] Johansson  R.  P. Nugues. Extended constituent-to-dependency conversion for English. In (NODALIDA).

2007.

[19] Koeling  R.  D. McCarthy. Sussx: WSD using automatically acquired predominant senses. In Proceedings

of SemEval-2007. Association for Computational Linguistics  2007.

[20] Boyd-Graber  J.  D. Blei. PUTOP: Turning predominant senses into a topic model for WSD. In Proceed-

ings of SemEval-2007. Association for Computational Linguistics  2007.

[21] McCarthy  D.  R. Koeling  J. Weeds  et al. Finding predominant word senses in untagged text. In Pro-
ceedings of Association for Computational Linguistics  pages 280–287. Association for Computational
Linguistics  2004.

[22] Collins  M. Head-driven statistical models for natural language parsing. Computational Linguistics 

29(4):589–637  2003.

[23] Klein  D.  C. Manning. Accurate unlexicalized parsing. In Proceedings of Association for Computational

Linguistics  pages 423–430. Association for Computational Linguistics  2003.

8

,Murat Erdogdu
Yash Deshpande
Andrea Montanari
Zhuwen Li
Qifeng Chen
Vladlen Koltun
Nikolas Ioannou
Celestine Mendler-Dünner
Thomas Parnell