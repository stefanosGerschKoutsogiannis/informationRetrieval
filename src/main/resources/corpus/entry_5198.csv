2010,Static Analysis of Binary Executables Using Structural SVMs,We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general  inference in segmentation models  such as semi-CRFs  can be cubic in the length of the sequence. By taking advantage of the structure of our problem  we derive a linear-time inference algorithm which makes our approach practical  given that even small programs are tens or hundreds of thousands bytes long. Furthermore  we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally  we present experimental results that demonstrate the advantages of our method against a strong baseline.,Static Analysis of Binary Executables Using

Structural SVMs

Nikos Karampatziakis∗

Department of Computer Science

Cornell University
Ithaca  NY 14853

nk@cs.cornell.edu

Abstract

We cast the problem of identifying basic blocks of code in a binary executable as
learning a mapping from a byte sequence to a segmentation of the sequence. In
general  inference in segmentation models  such as semi-CRFs  can be cubic in
the length of the sequence. By taking advantage of the structure of our problem 
we derive a linear-time inference algorithm which makes our approach practical 
given that even small programs are tens or hundreds of thousands bytes long. Fur-
thermore  we introduce two loss functions which are appropriate for our problem
and show how to use structural SVMs to optimize the learned mapping for these
losses. Finally  we present experimental results that demonstrate the advantages
of our method against a strong baseline.

1

Introduction

In this work  we are interested in the problem of extracting the CPU instructions that comprise a
binary executable ﬁle. Solving this problem is an important step towards verifying many simple
properties of a given program. In particular we are motivated by a computer security application  in
which we want to detect whether a previously unseen executable contains malicious code. This is a
task that computer security experts have to solve many times every day because in the last few years
the volume of malicious software has witnessed an exponential increase (estimated at 50000 new
malicious code samples every day). However  the tools that analyze binary executables require a lot
of manual effort in order to produce a correct analysis. This happens because the tools themselves
are based on heuristics and make many assumptions about the way a binary executable is structured.
But why is it hard to ﬁnd the instructions inside a binary executable? After all  when running
a program the CPU always knows which instructions it is executing. The caveat here is that we
want to extract the instructions from the executable without running it. On one hand  running the
executable will in general reveal little information about all possible instructions in the program  and
on the other hand it may be dangerous or even misguiding.1
Another issue that makes this task challenging is that binary executables contain many other things
except the instructions they will execute.2 Furthermore  the executable does not contain any demar-
cations about the locations of instructions in the ﬁle.3 Nevertheless  an executable ﬁle is organized
into sections such as a code section  a section with constants  a section containing global variables
etc. But even inside the code section  there is a lot more than just a stream of instructions. We will

∗http://www.cs.cornell.edu/∼nk
1Many malicious programs try to detect whether they are running under a controlled environment.
2Here  we are focusing on Windows executables for the Intel x86 architecture  though everything carries

over to any other modern operating system and any other architecture with a complex instruction set.

3Executables that contain debugging information are an exception  but most software is released without it

1

refer to all instructions as code and to everything else as data. For example  the compiler may  for
performance reasons  prefer to pad a function with up to 3 data bytes so that the next function starts
at an address that is a multiple of 4. Moreover  data can appear inside functions too. For example 
a “switch” statement in C is usually implemented in assembly using a table of addresses  one for
each “case” statement. This table does not contain any instructions  yet it can be stored together
with the instructions that make up the function in which the “switch” statement appears. Apart from
the compiler  the author of a malicious program can also insert data bytes in the code section of her
program. The ultimate goal of this act is to confuse the heuristic tools via creative uses of data bytes.

1.1 A text analogy

To convey more intuition about the difﬁculties in our task we will use a text analogy. The following
is an excerpt from a message sent to General Burgoyne during the American revolutionary war [1]:

You will have heard  Dr Sir I doubt not long before this can have reached you
that Sir W. Howe is gone from hence. The Rebels imagine that he is gone to
the Eastward. By this time however he has ﬁlled Chesapeak bay with surprize
and terror. Washington marched the greater part of the Rebels to Philadelphia in
order to oppose Sir Wm’s. army.

The sender also sent a mask via a different route that  when placed on top of the message  revealed
only the words that are shown here in bold. Our task can be thought as learning what needs to be
masked so that the hidden message is revealed. In this sense  words play the role of instructions
and letters play the role of bytes. For complex instruction sets like the Intel x86  instructions are
composed of a variable number of bytes  as words are composed of a variable number of letters.
There are also some minor differences. For example  programs have control logic (i.e. execution
can jump from one point to another)  while text is read sequentially. Moreover  programs do not
have spaces while most written languages do (exceptions are Chinese  Japanese  and Thai).
This analogy motivates tackling our problem as predicting a segmentation of the input sequence into
blocks of code and blocks of data. An obvious ﬁrst approach for this task would be to treat it as a
sequence labeling problem and train  for example  a linear chain conditional random ﬁeld (CRF) [2]
to tag each byte in the sequence as being the beginning  inside  or outside of a data block. However
this approach ignores much of the problem’s structure  most importantly that transitions from code to
data can only occur at speciﬁc points. Instead  we will use a more ﬂexible model which  in addition
to sequence labeling features  can express features of whole code blocks. Inference in our model is as
fast as for sequence labeling and we show a connection to weighted interval scheduling. This strikes
a balance between efﬁcient but simple sequence labeling models such as linear chain CRFs  and
expressive but slow4 segmentation models such as semi-CRFs [3] and semi-Markov SVMs [4]. To
learn the parameters of the model  we will use structural SVMs to optimize performance according
to loss functions that are appropriate for our task  such as the sum of incorrect plus missed CPU
instructions induced by the segmentation.
Before explaining our model in detail  we present some background on the workings of widely
used tools for binary code analysis in section 2  which allows us to easily explain our approach
in section 3. We empirically demonstrate the effectiveness of our model in section 4 and discuss
related work and other applications in section 5. Finally  section 6 discusses future work and states
our conclusions.

2 Heuristic tools for analyzing binary executables

Tools for statically analyzing binary executables differ in the details of their workings but they all
share the same high level logic  which is called recursive disassembly.5 The tool starts by obtaining
the address of the ﬁrst instruction from a speciﬁc location inside the executable. It then places this
address on a stack and executes the following steps while the stack is non-empty. It takes the next

4More speciﬁcally  inference needs O(nL2) time where L is an a priori bound on the lengths of the segments
(L = 2800 in our data) and n is the length of the sequence. With additional assumptions on the features  [5]
gives an O(nM ) algorithm where M is the maximum span of any edge in the CRF.

5Two example tools are IdaPro (http://www.hex-rays.com/idapro) and OllyDbg (http://www.ollydbg.de)

2

address from the stack and disassembles (i.e. decompiles to assembly) the sequence starting from
that address. All the disassembled instructions would execute one after the other until we reach
an instruction that changes the ﬂow of execution. These control ﬂow instructions  are conditional
and unconditional jumps  calls  and returns. After the execution of an unconditional jump the next
instruction to be executed is at the address speciﬁed by the jump’s argument. Other control ﬂow
instructions are similar to the unconditional jump. A conditional jump also speciﬁes a condition and
does nothing if the condition is false. A call saves the address of the next instruction and then jumps.
A return jumps to the address saved by a call (and does not need an address as an argument). The
tool places the arguments of control ﬂow instructions it encounters on the stack. If the control ﬂow
instruction is a conditional jump or a call  it continues disassembling  otherwise it takes the next
address  that has not yet been disassembled  from the stack and repeats.
Even though recursive disassembly seems like a robust way of extracting the instructions from a
program  there are many reasons that can make it fail [6]. Most importantly  the arguments of the
control ﬂow instructions do not have to be constants  they can be registers whose values are generally
not available during static analysis. Hence  recursive disassembly can ran out of addresses much
before all the instructions have been extracted. After this point  the tool has to resort to heuristics to
populate its stack. For example  a heuristic might check for positions in the sequence that match a
hand-crafted regular expression. Furthermore  some heuristics have to be applied on multiple passes
over the sequence. According to its documentation  OllyDbg does 12 passes over the sequence.
Recursive disassembly can also fail because of its assumptions. Recall that after encountering a call
instruction  it continues disassembling the next instruction  assuming that the call will eventually
return to execute it. Similarly for a conditional jump it assumes that both branches can potentially
execute. Though these assumptions are reasonable for most programs  malicious programs can
exploit them to confuse the static analysis tools. For example  the author of a malicious program
can write a function that  say  adds 3 to the return address that was saved by the call instruction.
This means that if the call instruction was spanning positions a  . . .   a + (cid:96)− 1 of the sequence  upon
the function’s return the next instruction will be at position a + (cid:96) + 3  not at a + (cid:96). This will give
a completely different decoding of the sequence and is called disassembly desynchronization. To
return to a text analogy  recursive disassembly parses the sequence “driverballetterrace” as [driver 
ballet  terrace] while the actual parsing  obtained by starting three positions down  is [xxx  verbal 
letter  race]  where x denotes junk data.

3 A structured prediction model

In this section we will combine ideas from recursive disassembly and structured prediction to derive
an expressive and efﬁcient model for predicting the instructions inside a binary executable. As
in recursive disassembly  if we are certain that code begins at position i we can unambiguously
disassemble the byte sequence starting from position i until we reach a control ﬂow instruction. But
unlike recursive disassembly  we maintain a trellis graph  a directed graph that succinctly represents
all possibilities. The trellis graph has vertices bi that denote the possibility that a code block starts at
position i. It also has vertices ej and edges (bi  ej) which denote that disassembling from position
i yields a possible code block that spans positions i  . . .   j. Furthermore  vertices di denote the
possibility that the i-th position is part of a data block. Edges (ej  bj+1) and (ej  dj+1) encode that
the next byte after a code block can either be the beginning of another code block  or a data byte
respectively. For data blocks no particular structure is assumed and we just use edges (dj  dj+1) and
(dj  bj+1) to denote that a data byte can be followed either by another data byte or by the beginning
of a code block respectively. Finally  we include vertices s and t and edges (s  b1)  (s  d1)  (dn  t)
and (en  t) to encode that sequences can start and end either with code or data.
An example is shown in Figure 1. The graph encodes all possible valid6 segmentations of the
sequence. In fact  there is a simple bijection P from any valid segmentation y to an s − t path P (y)
in this graph. For example  the sequence in Figure 1 contains three code blocks that span positions
1–7  8–8  and 10–12. This segmentation can be encoded by the path s  b1  e7  b8  e8  d9  b10  e12  t.

6Some subsequences will produce errors while decoding to assembly because some bytes may not corre-
spond to any instructions. These could never be valid code blocks because they would crash the program. Also
the program cannot do something interesting and crash in the same code block; interesting things can only
happen with system calls which  being call instructions  have to be at the end of their code block

3

Figure 1: The top line shows an example byte sequence in hexadecimal. Below this  we show the
actual x86 instructions with position 9 being a data byte. We show both the mnemonic instructions
as well as the bytes they are composed of. Some alternative decodings of the sequence are shown on
the bottom. The decoding that starts from the second position is able to skip over two control ﬂow
instructions. In the middle we show the graph that captures all possible decodings of the sequence.
Disassembling from positions 3  5  and 12 leads to decoding errors.

As usual for predicting structured outputs [2] [7]  we deﬁne the score of a segmentation y for a
sequence x to be the inner product w(cid:62)Ψ(x  y) where w ∈ Rd are the parameters of our model and
Ψ(x  y) ∈ Rd is a vector of features that captures the compatibility of the segmentation y and the
sequence x. Given a sequence x and a vector of parameters w  the inference task is to ﬁnd the
highest scoring segmentation

(1)
where Y is the space of all valid segmentations of x. We will assume that Ψ(x  y) decomposes as

ˆy = argmax

y∈Y

w(cid:62)Ψ(x  y)

Ψ(x  y) := (cid:88)
(cid:88)

(u v)∈P (y)

ˆy = argmax

y∈Y

(u v)∈P (y)

Φ(u  v  x)

w(cid:62)Φ(u  v  x)

where Φ(u  v  x) is a vector of features that can be computed using only the endpoints of edge (u  v)
and the byte sequence. This assumption allows efﬁcient inference because (1) can be rewritten as

which we recognize as computing the heaviest path in the trellis graph with edge weights given by
w(cid:62)Φ(u  v  x). This problem can be solved efﬁciently with dynamic programming by visiting each
vertex in topological order and updating the longest path to each of its neighbors.
The inference task can be viewed as a version of the weighted interval scheduling problem. Disas-
sembling from position i in the sequence yields an interval [i  j] where j is the position where the
ﬁrst encountered control ﬂow instruction ends. In weighted interval scheduling we want to select a
subset of non-overlapping intervals with maximum total weight. Our inference problem is the same
except we also have a cost for switching to the next interval  say the one that starts at position j + 2 
which is captured by the cost of the path ej  dj+1  bj+2. Finally  the dynamic programming algo-
rithm for solving this version is a simple modiﬁcation of the classic weighted interval scheduling
algorithm. Section 5 discusses other setups where this inference problem arises.

3.1 Loss functions

Now we introduce loss functions that measure how close an inferred segmentation ˆy is to the real
one y. First  we argue that Hamming loss  how well the bytes of the blocks in ˆy overlap with with
the bytes of the blocks in y  is not appropriate for this task because  as we recall from the text

4

d1d283c7043bfe7211c390407510d3d4d5d6d7d8d9d10d11d12b1b2b3b4b5b6b7b8b9b10b11b12e7e8e12staddedi4cmpediesijb0x401018retinceaxjnz0x40101cinceaxjnz0x40101cinceaxjnz0x40101cmov[ebx+edi]0xc31172fenopnopadcebxeaxanalogy at the end of section 2  two blocks may be overlapping very well but they may lead to
completely different decodings of the sequence. Hence  we introduce two loss functions which are
more appropriate for our task.
The ﬁrst loss function  which we call block loss  comes from the observation that the beginnings of
the code blocks are necessary and sufﬁcient to describe the segmentation. Therefore  we let y and
ˆy be the sets of positions where the code blocks start in the two segmentations and the block loss
counts how well these two sets overlap using the cardinality of their symmetric difference

∆B(y  ˆy) = |y| + |ˆy| − 2|y ∩ ˆy|

The second loss function  which we call instruction loss  is a little less stringent. In the case where
the inferred ˆy identiﬁes  say  the second instruction in a block as its start  we would like to penalize
this less  since the disassembly is still synchronized  and only missed one instruction. Formally 
we let y and ˆy be the sets of positions where the instructions start in the two segmentations and we
deﬁne the instruction loss ∆I(y  ˆy) to be the cardinality of their symmetric difference.
As an example  consider the segmentation which corresponds to path s  d1  b2  e12  t in Figure 1.
Therefore ˆy = {2} and from the ﬁgure we see that the segmentation in the top line has to pass
through b1  b8  b10 i.e. y = {1  8  10}. Hence its block loss is 4 because it misses b1  b8  b10 and it in-
troduces b2. For the instruction loss  the positions of the real instructions are y = {1  4  6  8  10  11}
while the proposed segmentation has ˆy = {2  9  10  11}. Taking the symmetric difference of these
sets  we see that the instruction loss has value 6.
Finally a variation of these loss functions occurs when we aggregate the losses over a set of se-
quences. If we simply sum the losses for each sequence then the losses in longer executables may
overshadow the losses on shorter ones. To represent each executable equally in the ﬁnal measure we
can normalize our loss functions  for example we can deﬁne the normalized instruction loss to be

∆N I(y  ˆy) =

|y| + |ˆy| − 2|y ∩ ˆy|

|y|

and we similarly deﬁne a normalized block loss ∆N B. If |ˆy| = |y|  ∆N I and ∆N B are scaled
versions of a popular loss function 1 − F1  where F1 is the harmonic mean of precision and recall.

3.2 Training

Given a set of training pairs (xi  yi) i = 1  . . .   n of sequences and segmentations we can learn a
vector of parameters w  that assigns a high score to segmentation yi and a low score to all other
possible segmentations of xi. For this we will use the structural SVM formulation with margin
rescaling [7] that solves the following problem:

min
w ξi

1
2

||w||2 + C
n

ξi

n(cid:88)

i=1

s.t. ∀i ∀¯y ∈ Yi : w(cid:62)Ψ(xi  yi) − w(cid:62)Ψ(xi  ¯y) ≥ ∆(yi  ¯y) − ξi

The constraints of this optimization problem enforce that the difference in score between the correct
segmentation y and any incorrect segmentation ¯y is at least as large as the loss ∆(yi  ¯y). If ˆyi is the
inferred segmentation then the slack variable ξi upper bounds ∆(yi  ˆyi). Hence  the objective is a
tradeoff between a small upper bound of the average training loss and a low-complexity hypothesis
w. The tradeoff is controlled by C which is set using cross-validation. Since the sets of valid
segmentations Yi are exponentially large  we solve the optimization problem with a cutting plane
algorithm [7]. We start with an empty set of constraints and in each iteration we ﬁnd the most
violated constraint for each example. We add these constraints in our optimization problem and
re-optimize. We do this until there are no constraints which are violated by more than a prespeciﬁed
 ) iterations [8]. For a training pair (xi  yi) the
tolerance . This procedure will terminate after O( 1
most violated constraint is:

ˆy = argmax

¯y∈Yi

w(cid:62)Ψ(xi  ¯y) + ∆(yi  ¯y)

(2)

Apart from the addition of ∆(yi  ¯y)  this is the same as the inference problem. For the losses we
introduced  we can solve the above problem with the same inference algorithm in a slightly modiﬁed

5

Bytes Blocks Block length (bytes) Block length (instructions)

Maximum 49152
Average
16712

3502
887

2794
13

1009

4

Table 1: Some statistics about the executable sections of the programs in the dataset

trellis graph. More precisely  for every vertex v we can deﬁne a cost c(v) for visiting it (this can be
absorbed into the costs of v’s incoming edges) and ﬁnd the longest path in this modiﬁed graph. This
is possible because our losses decompose over the vertices of the graph. This is not true for losses
such 1 − F1 for which (2) seems to require time quadratic in the length of the sequence.
For the block loss  the costs are deﬁned as follows. If bi ∈ y then c(di) = 1. This encodes that
using di instead of bi misses the beginning of one block. If bi /∈ y then bi deﬁnes an incorrect code
block which spans bytes i  . . .   j and c(bi) = 1 + |{k|bk ∈ y ∧ i < k ≤ j}|  capturing that we will
introduce one incorrect block and we will skip all the blocks that begin between positions i and j.
All other vertices in the graph have zero cost. In Figure 1 vertices d1  d8 and d10 have a cost of 1 
while b2  b4  b6  b7  b9  and b11 have costs 3  1  1  3  2  and 1 respectively.
For the instruction loss  y is a set of instruction positions. Similarly to the block loss if i ∈ y
then c(di) = 1. If i /∈ y then bi is the beginning of an incorrect block that spans bytes i  . . .   j
and produces instructions in a set of positions ˜yi. Let s be the ﬁrst position in this block that gets
synchronized with the correct decoding i.e. s = min(˜yi ∩ y) with s = j if the intersection is empty.
Then c(bi) = |{k|k ∈ ˜yi ∧ i ≤ k < s}| + |{k|k ∈ y ∧ i < k < s}|. The ﬁrst term captures
the number of incorrect instructions produced by treating bi as the start of a code block  while the
second term captures the number of missed real instructions. All other vertices in the graph have
zero cost. In Figure 1 vertices d1  d4  d6  d8  d10 and d11 have a cost of 1  while b2  b7  and b9 have
costs 5  3  and 1 respectively. For the normalized losses  we simply divide the costs by |y|.

4 Experiments

To evaluate our model we tried two different ways of collecting data  since we could not ﬁnd a
publicly available set of programs together with their segmentations. First  we tried using debugging
information  i.e. compile a program with and without debugging information and use the debug
annotations to identify the code blocks. This approach could not discover all code blocks  especially
when the compiler was automatically inserting code that did not exist in the source  such as the
calls to destructors generated by C++ compilers. Therefore we resorted to treating the output of
OllyDbg  a heuristic tool  as the ground truth. Since the executables we used were 200 common
programs from a typical installation of Windows XP  we believe that the outputs of heuristic tools
should have little noise. For a handful of programs we manually veriﬁed that another heuristic tool 
IdaPro  mostly agreed with OllyDbg. Of course  our model is a general statistical model and given
an expressive feature map  it can learn any ground truth. In this view the experiments suggest the
relative performance of the compared models. The dataset  and an implementation of our model  is
available at http://www.cs.cornell.edu/∼nk/svmwis. Table 1 shows some statistics of the dataset.
We use two kinds of features  byte-level and instruction-level features. For each edge in the graph 
the byte-level features are extracted from an 11 byte window around the source of the edge (so if the
source vertex is at position i  the window spans positions i − 5  . . .   i + 5). The features are which
bytes and byte pairs appear in which position inside the window. An example feature is “does byte c3
appear in position i − 1?”. In x86 architectures  when the previous instruction is a return instruction
this feature ﬁres. Of course  it also ﬁres in other cases and that is why we need instruction-level
features. These are obtained from histograms of instructions that occur in candidate code blocks
(i.e. edges of the form (bi  ej)). We use two kinds of histograms  one where we abstract the values
of the arguments of the instructions but keep their type (register  memory location or constant)  and
one where we completely discard all information about the arguments. An example of the former
type of feature would be “number of times the instruction [add register  register] appears in this
block”. An example of the latter type of feature would be “number of times the instruction [mov]
appears in this block”. In total  we have 2.3 million features. Finally  we normalize the features by
dividing them by the length of the sequence.

6

Greedy
SVMhmm
SVMwis ∆I
SVMwis ∆N I
SVMwis ∆B
SVMwis ∆N B

∆H
1623.6
236.2
98.8
104.3
86.5
85.2

¯L · ∆N H
1916.6
201.3
115.6
103.7
98.2
87.2

∆I

2164.3

—
44.6
45.5
39.6
40.6

¯I · ∆N I
7045.2

—
98.0
79.7
80.2
75.4

∆B
1564.9
45.1
26.1
30.5
21.5
23.4

¯B · ∆N B
4747.2
46.9
41.1
35.5
32.1
29.8

Table 2: Empirical results. ∆H is Hamming loss. Normalized losses (∆N X) are multiplied with the
average number of bytes (¯L)  instructions (¯I)  or blocks ( ¯B) to bring all numbers to a similar scale.

We compare our model SVMwis (standing for weighted interval scheduling  to underscore that it
is not a general segmentation model)  trained to minimize the losses we introduced  with a very
strong baseline  a discriminatively trained HMM (using SVMhmm). This model uses only the byte-
level features since it cannot express the instruction-level features. It tags each byte as being the
beginning  inside or outside of a code block using Viterbi and optimizes Hamming loss. Running a
general segmentation model [4] was impractical since inference depends quadratically on the max-
imum length of the code blocks  which was 2800 in our data. Finally  it would be interesting to
compare with [5]  but we could not ﬁnd their inference algorithm available as a ready to use soft-
ware. For all experiments we use ﬁve fold cross-validation where three folds are used for training
one fold for validation (selecting C) and one fold for testing.
Table 2 shows the results of our comparison for different loss functions (columns): Hamming loss 
instruction loss  block loss  and their normalized counterparts. Results for normalized losses have
been multiplied with the average number of bytes (¯L)  instructions (¯I)  or blocks ( ¯B) to bring all
numbers to a similar scale. To highlight the stregth of our main baseline  SVMhmm  we have
included a very simple baseline which we call greedy. Greedy starts decoding from the begining of
the sequence and after decoding a block (bi  ej) it repeats at position j + 1. It only marks a byte as
data if the decoding fails  in which case it starts decoding from the next byte in the sequence. The
results suggest that just treating our task as a simple sequence labeling problem at the level of bytes
already goes a long way in terms of Hamming loss and block loss. SVMhmm sometimes predicts
as the beginning of a code block a position that leads to a decoding error. Since it is not clear how
to compute the instruction loss in this case  we do not report instruction losses for this model. The
last four rows of the table show the results for our model  trained to minimize the loss indicated on
each line. We observe a further reduction in loss for all of our models. To assess this reduction 
we used paired Wilcoxon signed rank tests between the losses of SVMhmm’s predictions and the
losses of our model’s predictions (200 pairs). For all four models the tests suggest a statistically
signiﬁcant improvement over SVMhmm at the 1% level. For the block loss and its normalized
version ∆N B  we see that the best performance is obtained for the model trained to minimize the
respective loss. However this is not true for the other loss functions. For the Hamming loss  this is
expected since the SVMwis models are more expressive and a small block loss or instruction loss
implies a small Hamming loss  but not vice versa. For the instruction loss  we believe this occurs
because of two reasons. First our data consists of benign programs and for them learning to identify
the code blocks may be enough. Second it may be harder to learn with the instruction loss since its
value depends on how quickly each decoding synchronizes with another (the correct) decoding of
the stream  something that is not modeled in the feature map we are using. The end result is that the
models trained for block loss also attain the smallest losses for all other loss functions.

5 Related work and other applications

There are two lines of research which are relevant to this work: one is structured prediction ap-
proaches for segmenting sequences and the other is research on static analysis techniques for ﬁnding
code and data blocks in executables. Segmentation of sequences can be done via sequence labeling
e.g. [9]. If features of whole segments are needed then more expressive models such as semi-CRFs
[3] or semi-Markov SVMs [4] can be used. The latter work introduced training of segmentation
models for speciﬁc losses. However  if the segments are allowed to be long enough  these models
have polynomial but impractical inference complexity. With additional assumptions on the features

7

[5] gives an efﬁcient  though somewhat complicated  inference algorithm. In our model inference
takes linear time  is simple to implement  and does not depend on the length of the segments.
Previous techniques for identifying code blocks in executables have used no or very little statistical
learning. For example  [10] and [11] use recursive disassembly and pattern heuristics similarly to
currently used tools such as OllyDbg and IdaPro. These heuristics make many assumptions about
the data which are lucidly explained in [6]. In this work  the authors use simple statistical models
based on unigram and bigram instruction models in addition to the pattern heuristics. However 
these approaches make independent decisions for every candidate code block and they have a less
principled way of dealing with equally plausible but overlapping code blocks.
Our work is most similar to [12] which uses a CRF to locate the entry points of functions. They
use features that induce pairwise interactions between all possible positions in the executable which
makes their formulation intractable. They perform approximate inference with a custom iterative
algorithm but this is still slow. Our model can capture all the types of features that were used in
that model except one. This feature encodes whether an address that is called by a function is not
marked as a function and including this in our structure would make exact inference NP-hard. One
way to approximate this feature would be to count how many candidate code blocks have instructions
that jump to or call the current position in the sequence. For their task  compiling with debugging
information was enough to get real labels and they showed that  according to these labels  heuristic
tools are outperformed by their learning approach.
Finally  we conclude this section with a discussion on the broader impact of this work. Our model
is a general structured learning model and can be used in many sequence labeling problems. First 
it can encode all features of a linear chain CRF and can simulate it by specifying a structure where
each block is required to end at the same position where it starts. Furthermore  it can be used for any
application where each position can yield at most one or a small number of arbitrarily long possible
intervals and still have linear time inference  while inference in segmentation models depends on the
length of the segments. Applications of this form can arise in any kind of scheduling problem where
we want to learn a scheduler from example schedules. For example  a news website may decide to
show an ad in their front page together with their news stories. Each advertiser submits an ad along
with the times on which they want the ad to be shown. The news website can train a model like the
one we proposed based on past schedules and the observed total proﬁt for each of those days. The
proﬁt may not be directly observable for each individual ad depending on who serves the ads. When
one or more ads change in the future  the model can still create a good schedule because its decisions
depend on the features of the ads (such as the words in each ad)  the time selected for displaying the
ad as well as the surrounding ads.

6 Conclusions

In this work we proposed a code segmentation model SVMwis that can help security experts in
the static analysis of binary executables. We showed that inference in this model is as fast as for
sequence labeling  even though our model can have features that can be computed from entire blocks
of code. Moreover  our model is trained for the loss functions that are appropriate for the task.
We also compared our model with a very strong baseline  a sequence labeling approach using a
discriminatively trained HMM  and showed that we consistently outperform it.
In the future we would like to use data annotated with real segmentations which might be possible
to extract via a closer look at the compilation and linking process. We also want to look into richer
features such as some approximation of call consistency (since the actual constraints give rise to NP-
hard inference)  so that addresses which are targets of call or jump instructions from a code block
do not lie inside data blocks. Finally  we plan to extend our model to allow for joint segmentation
and classiﬁcation of the executable as malicious or not.

Acknowledgments

I would like to thank Adam Siepel for bringing segmentation models to my attention and Thorsten
Joachims  Dexter Kozen  Ainur Yessenalina  Chun-Nam Yu  and Yisong Yue for helpful discussions.

8

References
[1] F. B. Wrixon Codes  Ciphers  Secrets and Cryptic Communication. page 490  Black Dog &

Leventhal Publishers  2005.

[2] John D. Lafferty  Andrew McCallum  and Fernando C. N. Pereira. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling sequence data. In ICML ’01: Proceedings of
the Eighteenth International Conference on Machine Learning  pages 282–289  San Francisco 
CA  USA  2001. Morgan Kaufmann Publishers Inc.

[3] S. Sarawagi and W.W. Cohen. Semi-markov conditional random ﬁelds for information extrac-

tion. Advances in Neural Information Processing Systems  17:1185–1192  2005.

[4] Q. Shi  Y. Altun  A. Smola  and SVN Vishwanathan. Semi-Markov Models for Sequence

Segmentation. In Proceedings of the 2007 EMNLP-CoNLL.

[5] S. Sarawagi. Efﬁcient inference on sequence segmentation models. In Proceedings of the 23rd

international conference on Machine learning  page 800. ACM  2006.

[6] C. Kruegel  W. Robertson  F. Valeur  and G. Vigna. Static disassembly of obfuscated binaries.
In Proceedings of the 13th conference on USENIX Security Symposium-Volume 13  page 18.
USENIX Association  2004.

[7] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured
and interdependent output variables. Journal of Machine Learning Research  6(2):1453  2006.
[8] T. Joachims  T. Finley  and C-N. Yu. Cutting-Plane Training of Structural SVMs. Machine

Learning  77(1):27  2009.

[9] F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds.

HLT-NAACL  pages 213–220  2003.

In Proceedings of

[10] H. Theiling. Extracting safe and precise control ﬂow from binaries. In Seventh International

Conference on Real-Time Computing Systems and Applications  pages 23–30  2000.

[11] C. Cifuentes and M. Van Emmerik. UQBT: Adaptable binary translation at low cost. Computer 

33(3):60–66  2000.

[12] N. Rosenblum  X. Zhu  B. Miller  and K. Hunt. Learning to analyze binary computer code. In

Conference on Artiﬁcial Intelligence (AAAI 2008)  Chicago  Illinois  2008.

9

,Murat Erdogdu
Lane McIntosh
Niru Maheswaranathan
Aran Nayebi
Surya Ganguli
Stephen Baccus