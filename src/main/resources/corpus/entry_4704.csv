2016,Reshaped Wirtinger Flow for Solving Quadratic System of Equations,We study the problem of recovering a vector $\bx\in \bbR^n$ from its magnitude measurements $y_i=|\langle \ba_i  \bx\rangle|  i=1 ...  m$. Our work is along the line of the Wirtinger flow (WF) approach \citet{candes2015phase}  which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization. In contrast to the smooth loss function used in WF  we adopt a nonsmooth but lower-order loss function  and design a gradient-like algorithm (referred to as reshaped-WF). We show that for random Gaussian measurements  reshaped-WF enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is at the order of $\cO(n)$  where $n$ is the dimension of the unknown $\bx$. This improves the sample complexity of WF  and achieves  the same sample complexity as truncated-WF \citet{chen2015solving} but without truncation at gradient step. Furthermore  reshaped-WF costs less computationally than WF  and runs faster numerically than both WF and truncated-WF. Bypassing higher-order variables in the loss function and truncations in the gradient loop  analysis of reshaped-WF is simplified.,Reshaped Wirtinger Flow for

Solving Quadratic System of Equations

Huishuai Zhang

Department of EECS
Syracuse University
Syracuse  NY 13244
hzhan23@syr.edu

Yingbin Liang

Department of EECS
Syracuse University
Syracuse  NY 13244
yliang06@syr.edu

Abstract

We study the problem of recovering a vector x ∈ Rn from its magnitude measure-
ments yi = |(cid:104)ai  x(cid:105)|  i = 1  ...  m. Our work is along the line of the Wirtinger ﬂow
(WF) approach Candès et al. [2015]  which solves the problem by minimizing a
nonconvex loss function via a gradient algorithm and can be shown to converge
to a global optimal point under good initialization. In contrast to the smooth loss
function used in WF  we adopt a nonsmooth but lower-order loss function  and
design a gradient-like algorithm (referred to as reshaped-WF). We show that for
random Gaussian measurements  reshaped-WF enjoys geometric convergence to
a global optimal point as long as the number m of measurements is at the order
of O(n)  where n is the dimension of the unknown x. This improves the sample
complexity of WF  and achieves the same sample complexity as truncated-WF
Chen and Candes [2015] but without truncation at gradient step. Furthermore 
reshaped-WF costs less computationally than WF  and runs faster numerically than
both WF and truncated-WF. Bypassing higher-order variables in the loss function
and truncations in the gradient loop  analysis of reshaped-WF is simpliﬁed.

1

Introduction

for i = 1 ···   m 

i=1 and the design vectors {ai}m

Recovering a signal via a quadratic system of equations has gained intensive attention recently.
More speciﬁcally  suppose a signal of interest x ∈ Rn/Cn is measured via random design vectors
ai ∈ Rn/Cn with the measurements yi given by
yi = |(cid:104)ai  x(cid:105)|  

(1)
i = |(cid:104)ai  x(cid:105)|2. The goal is to recover
which can also be written equivalently in a quadratic form as y(cid:48)
the signal x based on the measurements y = {yi}m
i=1. Such a
problem arises naturally in the phase retrieval applications  in which the sign/phase of the signal is
to be recovered from only measurements of magnitudes. Various algorithms have been proposed to
solve this problem since 1970s. The error-reduction methods proposed in Gerchberg [1972]  Fienup
[1982] work well empirically but lack theoretical guarantees. More recently  convex relaxation of
the problem has been formulated  for example  via phase lifting Chai et al. [2011]  Candès et al.
[2013]  Gross et al. [2015] and via phase cut Waldspurger et al. [2015]  and the correspondingly
developed algorithms typically come with performance guarantee. The reader can refer to the review
paper Shechtman et al. [2015] to learn more about applications and algorithms of the phase retrieval
problem.
While with good theoretical guarantee  these convex methods often suffer from computational com-
plexity particularly when the signal dimension is large. On the other hand  more efﬁcient nonconvex
approaches have been proposed and shown to recover the true signal as long as initialization is good
enough. Netrapalli et al. [2013] proposed AltMinPhase algorithm  which alternatively updates the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

phase and the signal with each signal update solving a least-squares problem  and showed that Alt-
MinPhase converges linearly and recovers the true signal with O(n log3 n) Gaussian measurements.
More recently  Candès et al. [2015] introduces Wirtinger ﬂow (WF) algorithm  which guarantees
signal recovery via a simple gradient algorithm with only O(n log n) Gaussian measurements and
attains −accuracy within O(mn2 log 1/) ﬂops. More speciﬁcally  WF obtains good initialization
by the spectral method  and then minimizes the following nonconvex loss function

(cid:96)W F (z) :=

1
4m

(|aT

i z|2 − y2

i )2 

(2)

m(cid:88)

i=1

via the gradient descent scheme.
WF was further improved by truncated Wirtinger ﬂow (truncated-WF) algorithm proposed in Chen
and Candes [2015]  which adopts a Poisson loss function of |aT
i z|2  and keeps only well-behaved
measurements based on carefully designed truncation thresholds for calculating the initial seed and
every step of gradient . Such truncation assists to yield linear convergence with certain ﬁxed step size
and reduces both the sample complexity to (O(n)) and the convergence time to (O(mn log 1/)).
i z|2 so that the optimization objective is a
It can be observed that WF uses the quadratic loss of |aT
smooth function of aT
i z and the gradient step becomes simple. But this comes with a cost of a quartic
loss function. In this paper  we adopt the quadratic loss of |aT
i z|. Although the loss function is not
smooth everywhere  it reduces the order of aT
i z to be two  and the general curvature can be more
amenable to convergence of the gradient algorithm. The goal of this paper is to explore potential
advantages of such a nonsmooth lower-order loss function.

1.1 Our Contribution

This paper adopts the following loss function1

(cid:96)(z) :=

1
2m

i=1

m(cid:88)

(cid:0)|aT

i z| − yi

(cid:1)2

.

(3)

Compared to the loss function (2) in WF that adopts |aT
i z|2  the above loss function adopts the
absolute value/magnitude |aT
i z| and hence has lower-order variables. For such a nonconvex and
nonsmooth loss function  we develop a gradient descent-like algorithm  which sets zero for the
“gradient" component corresponding to nonsmooth samples. We refer to such an algorithm together
with truncated initialization using spectral method as reshaped Wirtinger ﬂow (reshaped-WF). We
show that the lower-order loss function has great advantage in both statistical and computational
efﬁciency  although scarifying smoothness. In fact  the curvature of such a loss function behaves
similarly to that of a least-squares loss function in the neighborhood of global optimums (see Section
2.2)  and hence reshaped-WF converges fast. The nonsmoothness does not signiﬁcantly affect the
convergence of the algorithm because only with negligible probability the algorithm encounters
nonsmooth points for some samples  which furthermore are set not to contribute to the gradient
direction by the algorithm. We summarize our main results as follows.

• Statistically  we show that reshaped-WF recovers the true signal with O(n) samples  when
the design vectors consist of independently and identically distributed (i.i.d.) Gaussian
entries  which is optimal in the order sense. Thus  even without truncation in gradient
steps (truncation only in initialization stage)  reshaped WF improves the sample complexity
O(n log n) of WF  and achieves the same sample complexity as truncated-WF. It is thus
more robust to random measurements.
• Computationally  reshaped-WF converges geometrically  requiring O(mn log 1/) ﬂops
to reach −accuracy. Again  without truncation in gradient steps  reshaped-WF improves
computational cost O(mn2 log(1/) of WF and achieves the same computational cost as
truncated-WF. Numerically  reshaped-WF is generally two times faster than truncated-WF
and four to six times faster than WF in terms of the number of iterations and time cost.

Compared to WF and truncated-WF  our technical proof of performance guarantee is much simpler 
because the lower-order loss function allows to bypass higher-order moments of variables and

1The loss function (3) was also used in Fienup [1982] to derive a gradient-like update for the phase retrieval
problem with Fourier magnitude measurements. However  our paper is to characterize global convergence
guarantee for such an algorithm with appropriate initialization  which was not studied in Fienup [1982].

2

truncation in gradient steps. We also anticipate that such analysis is more easily extendable. On the
other hand  the new form of the gradient step due to nonsmoothness of absolute function requires
new developments of bounding techniques.
1.2 Connection to Related Work

Along the line of developing nonconvex algorithms with global performance guarantee for the phase
retrieval problem  Netrapalli et al. [2013] developed alternating minimization algorithm  Candès et al.
[2015]  Chen and Candes [2015]  Zhang et al. [2016]  Cai et al. [2015] developed/studied ﬁrst-order
gradient-like algorithms  and a recent study Sun et al. [2016] characterized geometric structure of the
nonconvex objective and designed a second-order trust-region algorithm. Also notably is Wei [2015] 
which empirically demonstrated fast convergence of a so-called Kaczmarz stochastic algorithm. This
paper is most closely related to Candès et al. [2015]  Chen and Candes [2015]  Zhang et al. [2016] 
but develops a new gradient-like algorithm based on a lower-order nonsmooth (as well as nonconvex)
loss function that yields advantageous statistical/computational efﬁciency.
Various algorithms have been proposed for minimizing a general nonconvex nonsmooth objective 
such as gradient sampling algorithm Burke et al. [2005]  Kiwiel [2007] and majorization-minimization
method Ochs et al. [2015]. These algorithms were often shown to convergence to critical points
which may be local minimizers or saddle points  without explicit characterization of convergence
rate. In contrast  our algorithm is speciﬁcally designed for the phase retrieval problem  and can be
shown to converge linearly to global optimum under appropriate initialization.
The advantage of nonsmooth loss function exhibiting in our study is analogous in spirit to that of
the rectiﬁer activation function (of the form max{0 ·}) in neural networks. It has been shown that
rectiﬁed linear unit (ReLU) enjoys superb advantage in reducing the training time Krizhevsky et al.
[2012] and promoting sparsity Glorot et al. [2011] over its counterparts of sigmoid and hyperbolic
tangent functions  in spite of non-linearity and non-differentiability at zero. Our result in fact also
demonstrates that a nonsmooth but simpler loss function yields improved performance.
1.3 Paper Organization and Notations

The rest of this paper is organized as follows. Section 2 describes reshaped-WF algorithm in detail
and establishes its performance guarantee. In particular  Section 2.2 provides intuition about why
reshaped-WF is fast. Section 3 compares reshaped-WF with other competitive algorithms numerically.
Finally  Section 4 concludes the paper with comments on future directions.
Throughout the paper  boldface lowercase letters such as ai  x  z denote vectors  and boldface capital
letters such as A  Y denote matrices. For two matrices  A (cid:22) B means that B− A is positive deﬁnite.
The indicator function 1A = 1 if the event A is true  and 1A = 0 otherwise. The Euclidean distance
between two vectors up to a global sign difference is deﬁned as dist(z  x) := min{(cid:107)z−x(cid:107) (cid:107)z +x(cid:107)}.
2 Algorithm and Performance Guarantee
In this paper  we wish to recover a signal x ∈ Rn based on m measurements yi given by

(4)
where ai ∈ Rn for i = 1 ···   m are known measurement vectors generated by Gaussian distribution
N (0  I n×n). We focus on the real-valued case in analysis  but the algorithm designed below is
applicable to the complex-valued case and the case with coded diffraction pattern (CDP) as we
demonstrate via numerical results in Section 3.
We design reshaped-WF (see Algorithm 1) for solving the above problem  which contains two stages:
spectral initialization and gradient loop. Suggested values for parameters are αl = 1  αu = 5 and
µ = 0.8. The scaling parameter in λ0 and the conjugate transpose a∗
i allow the algorithm readily
applicable to complex and CDP cases. We next describe the two stages of the algorithm in detail in
Sections 2.1 and 2.2  respectively  and establish the convergence of the algorithm in Section 2.3.
2.1

Initialization via Spectral Method

yi = |(cid:104)ai  x(cid:105)|  

for i = 1 ···   m 

We ﬁrst note that initialization can adopt the spectral initialization method for WF in Candès et al.
[2015] or that for truncated-WF in Chen and Candes [2015]  both of which are based on |a∗
i x|2.
i x| instead  and
Here  we propose an alternative initialization in Algorithm 1 that uses magnitude |a∗
truncates samples with both lower and upper thresholds as in (5). We show that such initialization
achieves smaller sample complexity than WF and the same order-level sample complexity as truncated-
WF  and furthermore  performs better than both WF and truncated-WF numerically.

3

i=1  {ai}m
i=1;

Algorithm 1 Reshaped Wirtinger Flow
Input: y = {yi}m
Parameters: Lower and upper thresholds αl  αu for truncation in initialization  stepsize µ;
Initialization: Let z(0) = λ0˜z  where λ0 =
vector of

(cid:1) and ˜z is the leading eigen-

mn(cid:80)m
i=1 (cid:107)ai(cid:107)1

·(cid:0) 1

(cid:80)m

i=1 yi

m

Y :=
Gradient loop: for t = 0 : T − 1 do

1
m

z(t+1) = z(t) − µ
m

m(cid:88)
m(cid:88)

i=1

i=1

yiaia∗

i 1{αlλ0<yi<αuλ0}.

(cid:18)

i z(t) − yi · a∗
i z(t)
a∗
i z(t)|
|a∗

(cid:19)

ai.

(5)

(6)

Output z(T ).

≈ (cid:112) π

mn(cid:80)m
i=1 (cid:107)ai(cid:107)1

2 . Moreover  yi = |aT

i=1 yi ≈(cid:113) 2
(cid:80)m

Our initialization consists of estimation of both the norm and direction of x. The norm estimation
(cid:113) 2
of x is given by λ0 in Algorithm 1 with mathematical justiﬁcation in Suppl. A. Intuitively  with
i x|
real Gaussian measurements  the scaling coefﬁcient
π(cid:107)x(cid:107)  and thus

are independent sub-Gaussian random variables for i = 1  . . .   m with mean
π(cid:107)x(cid:107). Combining these two facts yields the desired argument.

1
m
The direction of x is approximated by the leading eigen-
vector of Y   because Y approaches E[Y ] by concen-
tration of measure and the leading eigenvector of E[Y ]
takes the form cx for some scalar c ∈ R. We note that
(5) involves truncation of samples from both sides  in
contrast to truncation only by an upper threshold in
Chen and Candes [2015]. This is because yi = |aT
i x|2
in Chen and Candes [2015] so that small |aT
i x| is fur-
ther reduced by the square power to contribute less
i x| can still intro-
in Y   but small values of yi = |aT
duce considerable contributions and hence should be
truncated by the lower threshold.
We next provide the formal statement of the perfor-
mance guarantee for the initialization step that we pro-
pose. The proof adapts that in Chen and Candes [2015]
and is provided in Suppl. A.
Proposition 1. Fix δ > 0. The initialization step in
Algorithm 1 yields z(0) satisfying (cid:107)z(0) − x(cid:107) ≤ δ(cid:107)x(cid:107) with probability at least 1 − exp(−c(cid:48)m2) 
if m > C(δ  )n  where C is a positive number only affected by δ and   and c(cid:48) is some positive
constant.

Figure 1: Comparison of three initialization
methods with m = 6n and 50 iterations
using power method.

Finally  Figure 1 demonstrates that reshaped-WF achieves better initialization accuracy in terms of
the relative error (cid:107)z(0)−x(cid:107)

than WF and truncated-WF with Gaussian measurements.

(cid:107)x(cid:107)

2.2 Gradient Loop and Why Reshaped-WF is Fast

The gradient loop of Algorithm 1 is based on the loss function (3)  which is rewritten below

We deﬁne the update direction as

∇(cid:96)(z) :=

1
m

m(cid:88)
(cid:0)|aT
i z)(cid:1) ai =

(cid:1)2
i z| − yi
(cid:18)
m(cid:88)

i=1

.

1
m

i=1

(cid:96)(z) :=

1
2m

m(cid:88)

(cid:0)aT

i z − yi · sgn(aT

i=1

4

(7)

(cid:19)

ai 

(8)

i z − yi · aT
i z
aT
|aT
i z|

010002000300040005000600070008000n: signal dimension0.60.70.80.911.1Relative error reshaped-WFtruncated-WFWFwhere sgn(·) is the sign function for nonzero arguments. We further set sgn(0) = 0 and 0|0| = 0.
In fact  ∇(cid:96)(z) equals the gradient of the loss function (7) if aT
i z (cid:54)= 0 for all i = 1  ...  m. For
samples with nonsmooth point  i.e.  aT
i z = 0  we adopt Fréchet superdifferential Kruger [2003] for
nonconvex function to set the corresponding gradient component to be zero (as zero is an element in
Fréchet superdifferential). With abuse of terminology  we still refer to ∇(cid:96)(z) in (8) as “gradient” for
simplicity  which rather represents the update direction in the gradient loop of Algorithm 1.
We next provide the intuition about why reshaped WF is fast. Suppose that the spectral method sets
an initial point in the neighborhood of ground truth x. We compare reshaped-WF with the following
problem of solving x from linear equations yi = (cid:104)ai  x(cid:105) with yi and ai for i = 1  . . .   m given. In
particular  we note that this problem has both magnitude and sign observation of the measurements.
Further suppose that the least-squares loss is used and gradient descent is applied to solve this
problem. Then the gradient is given by

Least square gradient: ∇(cid:96)LS(z) =

1
m

(cid:0)aT

m(cid:88)

i=1

i x(cid:1) ai.

i z − aT

We now argue informally that the gradient (8) of reshaped-WF behaves similarly to the least-squares
gradient (9). For each i  the two gradient components are close if |aT
i z) is viewed as
an estimate of aT
i x. The following lemma (see Suppl. B.2 for the proof) shows that if dist(z  x) is
small (guaranteed by initialization)  then aT
Lemma 1. Let ai ∼ N (0  I n×n). For any given x and z satisfying (cid:107)x − z(cid:107) <

i x|.
(cid:107)x(cid:107)  we have

i z has the same sign with aT

P{(aT

i x)(aT

i x)2 = t(cid:107)x(cid:107)2} ≤ erfc

i z) < 0(cid:12)(cid:12)(aT
(cid:82) ∞
u exp(−τ 2)dτ.

π

where h = z − x and erfc(u) := 2√

(9)

(10)

i x| · sgn(aT
i x for large |aT
√
2−1√
(cid:18)√

(cid:19)

2

t(cid:107)x(cid:107)
2(cid:107)h(cid:107)

 

i z so that the
i x may have
i z but contributes less to the gradient. Hence  overall the two gradients (8) and (9)

It is easy to observe in (10) that large aT
i x is likely to have the same sign as aT
corresponding gradient components in (8) and (9) are likely equal  whereas small aT
different sign as aT
should be close to each other with a large probability.
This fact can be further veriﬁed numerically. Figure 2(a) illustrates that reshaped-WF takes almost
the same number of iterations for recovering a signal (with only magnitude information) as the least-
squares gradient descent method for recovering a signal (with both magnitude and sign information).

(a) Convergence behavior

(b) Expected loss of reshaped-WF

(c) Expected loss of WF

Figure 2: Intuition of why reshaped-WF fast. (a) Comparison of convergence behavior between
reshaped-WF and least-squares gradient descent. Initialization and parameters are the same for two
methods: n = 1000  m = 6n  step size µ = 0.8. (b) Expected loss function of reshaped-WF for
x = [1 − 1]T . (c) Expected loss function of WF for x = [1 − 1]T .

Figure 2(b) further illustrates that the expected loss surface of reshaped-WF (see Suppl. B for
expression) behaves similarly to a quadratic surface around the global optimums as compared to the
expected loss surface for WF (see Suppl. B for expression) in Figure 2(c).

5

020406080100Number of iterations10-2010-1510-1010-5100Relative errorLeast-squares gradientRWF-200.511.5-1-222.533.5-1.544.55-1z10-0.5z200.5111.5222020402608011001201.51401601z100.5z20-0.5-1-1-1.5-2-22.3 Geometric Convergence of Reshaped-WF

We characterize the convergence of reshaped-WF in the following theorem.
Theorem 1. Consider the problem of solving any given x ∈ Rn from a system of equations (4) with
Gaussian measurement vectors. There exist some universal constants µ0 > 0 (µ0 can be set as 0.8 in
practice)  0 < ρ  ν < 1 and c0  c1  c2 > 0 such that if m ≥ c0n and µ < µ0  then with probability at
least 1 − c1 exp(−c2m)  Algorithm 1 yields

dist(z(t)  x) ≤ ν(1 − ρ)t(cid:107)x(cid:107) 

∀t ∈ N.

(11)

Outline of the Proof. We outline the proof here with details relegated to Suppl. C. Compared to WF
and truncated-WF  our proof is much simpler due to the lower-order loss function that reshaped-WF
relies on.
The central idea is to show that within the neighborhood of global optimums  reshaped-WF satisﬁes
the Regularity Condition RC(µ  λ  c) Chen and Candes [2015]  i.e. 
(cid:107)h(cid:107)2

(12)
for all z and h = z − x obeying (cid:107)h(cid:107) ≤ c(cid:107)x(cid:107)  where 0 < c < 1 is some constant. Then  as shown in
Chen and Candes [2015]  once the initialization lands into this neighborhood  geometric convergence
can be guaranteed  i.e. 

(cid:104)∇(cid:96)(z)  h(cid:105) ≥ µ
2

(cid:107)∇(cid:96)(z)(cid:107)2 +

λ
2

dist2 (z + µ∇(cid:96)(z)  x) ≤ (1 − µλ)dist2(z  x) 

(13)

for any z with (cid:107)z − x(cid:107) ≤ (cid:107)x(cid:107).
Lemmas 2 and 3 in Suppl.C yield that

(cid:104)∇(cid:96)(z)  h(cid:105) ≥ (1 − 0.26 − 2)(cid:107)h(cid:107)2 = (0.74 − 2)(cid:107)h(cid:107)2.

And Lemma 4 in Suppl.C further yields that

(14)
Therefore  the above two bounds imply that Regularity Condition (12) holds for µ and λ satisfying

(cid:107)∇(cid:96)(z)(cid:107) ≤ (1 + δ) · 2(cid:107)h(cid:107).

0.74 − 2 ≥ µ
2

· 4(1 + δ)2 +

λ
2

.

(15)

We note that (15) implies an upper bound µ ≤ 0.74
2 = 0.37  by taking  and δ to be sufﬁciently small.
This suggests a range to set the step size in Algorithm 1. However  in practice  µ can be set much
larger than such a bound  say 0.8  while still keeping the algorithm convergent. This is because the
coefﬁcients in the proof are set for convenience of proof rather than being tightly chosen.
Theorem 1 indicates that reshaped-WF recovers the true signal with O(n) samples  which is order-
level optimal. Such an algorithm improves the sample complexity O(n log n) of WF. Furthermore 
reshaped-WF does not require truncation of weak samples in the gradient step to achieve the same
sample complexity as truncated-WF. This is mainly because reshaped-WF beneﬁts from the lower-
order loss function given in (7)  the curvature of which behaves similarly to the least-squares loss
function locally as we explain in Section 2.2.
Theorem 1 also suggests that reshaped-WF converges geometrically at a constant step size. To
reach −accuracy  it requires computational cost of O(mn log 1/) ﬂops  which is better than WF
(O(mn2 log(1/)). Furthermore  it does not require truncation in gradient steps to reach the same
computational cost as truncated-WF. Numerically  as we demonstrate in Section 3  reshaped-WF is
two times faster than truncated-WF and four to six times faster than WF in terms of both iteration
count and time cost in various examples.
Although our focus in this paper is on the noise-free model  reshaped-WF can be applied to noisy
models as well. Suppose the measurements are corrupted by bounded noises {ηi}m
i=1 satisfying
(cid:107)η(cid:107)/
m ≤ c(cid:107)x(cid:107). Then by adapting the proof of Theorem 1  it can be shown that the gradient loop
of reshaped-WF is robust such that

√

dist(z(t)  x) (cid:46) (cid:107)η(cid:107)√

(16)
for some ρ ∈ (0  1). The numerical result under the Poisson noise model in Section 3 further
corroborates the stability of reshaped-WF.

m

+ (1 − ρ)t(cid:107)x(cid:107) 

∀t ∈ N 

6

Table 1: Comparison of iteration count and time cost among algorithms (n = 1000  m = 8n)

real case

complex case

Algorithms
iterations
time cost(s)
iterations
time cost(s)

reshaped-WF
72
0.477
272.7
6.956

truncated-WF WF
182
1.232
486.7
12.815

319.2
2.104
915.4
23.306

AltMinPhase
5.8
0.908
156
93.22

3 Numerical Comparison with Other Algorithms

In this section  we demonstrate the numerical efﬁciency of reshaped-WF by comparing its perfor-
mance with other competitive algorithms. Our experiments are run not only for real-valued case but
also for complex-valued and CDP cases. All the experiments are implemented in Matlab 2015b and
carried out on a computer equipped with Intel Core i7 3.4GHz CPU and 12GB RAM.
We ﬁrst compare the sample complexity of reshaped-WF with those of truncated-WF and WF
via the empirical successful recovery rate versus the number of measurements. For reshaped-WF 
we follow Algorithm 1 with suggested parameters. For truncated-WF and WF  we use the codes
provided in the original papers with the suggested parameters. We conduct the experiment for real 
complex and CDP cases respectively. For real and complex cases  we set the signal dimension n
to be 1000  and the ratio m/n take values from 2 to 6 by a step size 0.1. For each m  we run 100
trials and count the number of successful trials. For each trial  we run a ﬁxed number of iterations
T = 1000 for all algorithms. A trial is declared to be successful if z(T )  the output of the algorithm 
satisﬁes dist(z(T )  x)/(cid:107)x(cid:107) ≤ 10−5. For the real case  we generate signal x ∼ N (0  I n×n)  and the
measurement vectors ai ∼ N (0  I n×n) i.i.d. for i = 1  . . .   m. For the complex case  we generate
signal x ∼ N (0  I n×n) + jN (0  I n×n) and measurements ai ∼ 1
2N (0  I n×n)
i.i.d. for i = 1  . . .   m. For the CDP case  we generate signal x ∼ N (0  I n×n) + jN (0  I n×n) that
yields measurements

2N (0  I n×n) + j 1

y(l) = |F D(l)x| 

1 ≤ l ≤ L 

(17)

where F represents the discrete Fourier transform (DFT) matrix  and D(l) is a diagonal matrix
(mask). We set n = 1024 for convenience of FFT and m/n = L = 1  2  . . .   8. All other settings are
the same as those for the real case.

(a) Real case

(b) Complex case

(c) CDP case

Figure 3: Comparison of sample complexity among reshaped-WF  truncated-WF and WF.

Figure 3 plots the fraction of successful trials out of 100 trials for all algorithms  with respect to m. It
can be seen that for although reshaped-WF outperforms only WF (not truncated-WF) for the real
case  it outperforms both WF and truncated-WF for complex and CDP cases. An intuitive explanation
for the real case is that a substantial number of samples with small |aT
i z| can deviate gradient so
that truncation indeed helps to stabilize the algorithm if the number of measurements is not large.
Furthermore  reshaped-WF exhibits shaper transition than truncated-WF and WF.
We next compare the convergence rate of reshaped-WF with those of truncated-WF  WF and AltMin-
Phase. We run all of the algorithms with suggested parameter settings in the original codes. We gener-
ate signal and measurements in the same way as those in the ﬁrst experiment with n = 1000  m = 8n.
All algorithms are seeded with reshaped-WF initialization. In Table 1  we list the number of iterations
and time cost for those algorithms to achieve the relative error of 10−14 averaged over 10 trials.
Clearly  reshaped-WF takes many fewer iterations as well as runing much faster than truncated-WF
and WF. Although reshaped-WF takes more iterations than AltMinPhase  it runs much faster than

7

2n3n4n5n6nm: Number of measurements (n=1000)00.20.40.60.81Empirical success ratereshaped-WFtruncated-WFWF2n3n4n5n6nm: Number of measurements (n=1000)00.20.40.60.81Empirical success ratereshaped-WFtruncated-WFWF1n2n3n4n5n6n7n8nm: Number of measurements (n=1024)00.20.40.60.81Empirical success ratereshaped-WFtruncated-WFWFAltMinPhase due to the fact that each iteration of AltMinPhase needs to solve a least-squares problem
that takes much longer time than a simple gradient update in reshaped-WF.
We also compare the performance of the above algorithms on the recovery of a real image from the
Fourier intensity measurements (2D CDP with the number of masks L = 16). The image (provided in
Suppl.D) is the Milky Way Galaxy with resolution 1920× 1080. Table 2 lists the number of iterations
and time cost of the above four algorithms to achieve the relative error of 10−15. It can be seen that
reshaped-WF outperforms all other three algorithms in computational time cost. In particular  it is
two times faster than truncated-WF and six times faster than WF in terms of both the number of
iterations and computational time cost.

Table 2: Comparison of iterations and time cost among algorithms on Galaxy image (L = 16)

Algorithms
iterations
time cost(s)

reshaped-WF
65
141

truncated-WF WF AltMinPhase
160
567

420
998

110
213

We next demonstrate the robustness of reshaped-WF to noise corruption and compare it with truncated-
WF. We consider the phase retrieval problem in imaging applications  where random Poisson noises
are often used to model the sensor and electronic noise Fogel et al. [2013]. Speciﬁcally  the noisy
measurements of intensity can be expressed as yi =
for i = 1  2  ...m
where α denotes the level of input noise  and Poisson(λ) denotes a random sample generated by the
Poisson distribution with mean λ. It can be observed from Figure 4 that reshaped-WF performs better
than truncated-WF in terms of recovery accuracy under different noise levels.

α · Poisson(cid:0)|aT

i x|2/α(cid:1) 

(cid:113)

Figure 4: Comparison of relative error under Poisson noise between reshaped-WF and truncated WF.

4 Conclusion

In this paper  we proposed reshaped-WF to recover a signal from a quadratic system of equations 
based on a nonconvex and nonsmooth quadratic loss function of absolute values of measurements.
This loss function sacriﬁces the smoothness but enjoys advantages in statistical and computational
efﬁciency. It also has potential to be extended in various scenarios. One interesting direction is to
extend such an algorithm to exploit signal structures (e.g.  nonnegativity  sparsity  etc) to assist the
recovery. The lower-order loss function may offer great simplicity to prove performance guarantee in
such cases. Another interesting topic is to study stochastic version of reshaped-WF. We have observed
in preliminary experiments that the stochastic version of reshaped-WF converges fast numerically.
It will be of great interest to fully understand the theoretic performance of such an algorithm and
explore the reason behind its fast convergence.

Acknowledgments

This work is supported in part by the grants AFOSR FA9550-16-1-0077 and NSF ECCS 16-09916.

8

0102030405060Number of iterations10-410-310-210-1100Relative errorreshaped-WFtruncated-WFnoise level  =1noise level =0.001References
J. V. Burke  A. S. Lewis  and M. L. Overton. A robust gradient sampling algorithm for nonsmooth  nonconvex

optimization. SIAM Journal on Optimization  15(3):751–779  2005.

T. T. Cai  X. Li  and Z. Ma. Optimal rates of convergence for noisy sparse phase retrieval via thresholded

wirtinger ﬂow. arXiv preprint arXiv:1506.03382  2015.

E. J. Candès  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude
measurements via convex programming. Communications on Pure and Applied Mathematics  66(8):1241–
1274  2013.

E. J. Candès  X. Li  and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and algorithms. IEEE

Transactions on Information Theory  61(4):1985–2007  2015.

A. Chai  M. Moscoso  and G. Papanicolaou. Array imaging using intensity-only measurements.

Problems  27(1)  2011.

Inverse

Y. Chen and E. Candes. Solving random quadratic systems of equations is nearly as easy as solving linear

systems. In Advances in Neural Information Processing Systems (NIPS). 2015.

J. D. Donahue. Products and quotients of random variables and their applications. Technical report  DTIC

Document  1964.

J. R. Fienup. Phase retrieval algorithms: a comparison. Applied Optics  21(15):2758–2769  1982.

F. Fogel  I. Waldspurger  and A. d’Aspremont. Phase retrieval for imaging problems. arXiv preprint

arXiv:1304.7735  2013.

R. W. Gerchberg. A practical algorithm for the determination of phase from image and diffraction plane pictures.

Optik  35:237  1972.

X. Glorot  A. Bordes  and Y. Bengio. Deep sparse rectiﬁer neural networks. In International Conference on

Artiﬁcial Intelligence and Statistics (AISTATS)  2011.

D. Gross  F. Krahmer  and R. Kueng. Improved recovery guarantees for phase retrieval from coded diffraction

patterns. Applied and Computational Harmonic Analysis  2015.

K. C. Kiwiel. Convergence of the gradient sampling algorithm for nonsmooth nonconvex optimization. SIAM

Journal on Optimization  18(2):379–388  2007.

A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks.

In Advances in neural information processing systems (NIPS)  2012.

A. Y. Kruger. On fréchet subdifferentials. Journal of Mathematical Sciences  116(3):3325–3358  2003.

P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. Advances in Neural

Information Processing Systems (NIPS)  2013.

P. Ochs  A. Dosovitskiy  T. Brox  and T. Pock. On iteratively reweighted algorithms for nonsmooth nonconvex

optimization in computer vision. SIAM Journal on Imaging Sciences  8(1):331–372  2015.

Y. Shechtman  Y. C. Eldar  O. Cohen  H. N. Chapman  J. Miao  and M. Segev. Phase retrieval with application

to optical imaging: a contemporary overview. IEEE Signal Processing Magazine  32(3):87–109  2015.

J. Sun  Q. Qu  and J. Wright. A geometric analysis of phase retrieval. arXiv preprint arXiv:1602.06664  2016.

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing  Theory

and Applications  pages 210 – 268  2012.

I. Waldspurger  A. d’Aspremont  and S. Mallat. Phase recovery  maxcut and complex semideﬁnite programming.

Mathematical Programming  149(1-2):47–81  2015.

K. Wei. Solving systems of phaseless equations via kaczmarz methods: a proof of concept study. Inverse

Problems  31(12):125008  2015.

H. Zhang  Y. Chi  and Y. Liang. Provable non-convex phase retrieval with outliers: Median truncated wirtinger

ﬂow. arXiv preprint arXiv:1603.03805  2016.

9

,Qiang Cheng
Qiang Liu
Feng Chen
Alexander Ihler
Siqi Sun
Mladen Kolar
Jinbo Xu
Huishuai Zhang
Yingbin Liang