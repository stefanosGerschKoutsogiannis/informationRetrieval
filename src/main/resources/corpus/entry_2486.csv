2018,How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?,When the linear measurements of an instance of low-rank matrix recovery
satisfy a restricted isometry property (RIP) --- i.e. they
are approximately norm-preserving --- the problem is known
to contain no spurious local minima  so exact recovery is guaranteed.
In this paper  we show that moderate RIP is not enough to eliminate
spurious local minima  so existing results can only hold for near-perfect
RIP. In fact  counterexamples are ubiquitous: every $x$ is the spurious
local minimum of a rank-1 instance of matrix recovery that satisfies
RIP. One specific counterexample has RIP constant $\delta=1/2$  but
causes randomly initialized stochastic gradient descent (SGD) to fail
12\% of the time. SGD is frequently able to avoid and escape spurious
local minima  but this empirical result shows that it can occasionally
be defeated by their existence. Hence  while exact recovery guarantees
will likely require a proof of no spurious local minima  arguments
based solely on norm preservation will only be applicable to a narrow
set of nearly-isotropic instances.,How Much Restricted Isometry is Needed In

Nonconvex Matrix Recovery?

Richard Y. Zhang

University of California  Berkeley

Cédric Josz

University of California  Berkeley

ryz@alum.mit.edu

cedric.josz@gmail.com

Somayeh Sojoudi

University of California  Berkeley

sojoudi@berkeley.edu

Javad Lavaei

University of California  Berkeley

lavaei@berkeley.edu

Abstract

When the linear measurements of an instance of low-rank matrix recovery sat-
isfy a restricted isometry property (RIP)—i.e.
they are approximately norm-
preserving—the problem is known to contain no spurious local minima  so exact
recovery is guaranteed. In this paper  we show that moderate RIP is not enough to
eliminate spurious local minima  so existing results can only hold for near-perfect
RIP. In fact  counterexamples are ubiquitous: we prove that every x is the spu-
rious local minimum of a rank-1 instance of matrix recovery that satisﬁes RIP.
One speciﬁc counterexample has RIP constant δ = 1/2  but causes randomly
initialized stochastic gradient descent (SGD) to fail 12% of the time. SGD is fre-
quently able to avoid and escape spurious local minima  but this empirical result
shows that it can occasionally be defeated by their existence. Hence  while exact
recovery guarantees will likely require a proof of no spurious local minima  argu-
ments based solely on norm preservation will only be applicable to a narrow set
of nearly-isotropic instances.

1

Introduction

Recently  several important nonconvex problems in machine learning have been shown to contain no
spurious local minima [19  4  21  8  20  34  30]. These problems are easily solved using local search
algorithms despite their nonconvexity  because every local minimum is also a global minimum  and
every saddle-point has sufﬁciently negative curvature to allow escape. Formally  the usual ﬁrst- and
second-order necessary conditions for local optimality (i.e. zero gradient and a positive semideﬁnite
Hessian) are also sufﬁcient for global optimality; satisfying them to -accuracy will yield a point
within an -neighborhood of a globally optimal solution.
Many of the best-understood nonconvex problems with no spurious local minima are variants of the
low-rank matrix recovery problem. The simplest version (known as matrix sensing) seeks to recover
an n×n positive semideﬁnite matrix Z of low rank r (cid:28) n  given measurement matrices A1  . . .   Am
and noiseless data bi = (cid:104)Ai  Z(cid:105). The usual  nonconvex approach is to solve the following
(cid:104)Am  X(cid:105)]T

(cid:107)A(xxT ) − b(cid:107)2 where A(X) = [(cid:104)A1  X(cid:105)

minimize
x∈Rn×r

···

(1)

to second-order optimality  using a local search algorithm like (stochastic) gradient descent [19  24]
and trust region Newton’s method [16  7]  starting from a random initial point.
Exact recovery of the ground truth Z is guaranteed under the assumption that A satisﬁes the re-
stricted isometry property [14  13  31  11] with a sufﬁciently small constant. The original result is

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(1 − δr)(cid:107)X(cid:107)2

due to Bhojanapalli et al. [4]  though we adapt the statement below from a later result by Ge et al. [20 
Theorem 8]. (Zhu et al. [43] give an equivalent statement for nonsymmetric matrices.)
Deﬁnition 1 (Restricted Isometry Property). The linear map A : Rn×n → Rm is said to satisfy
(r  δr)-RIP with constant 0 ≤ δr < 1 if there exists a ﬁxed scaling γ > 0 such that for all rank-r
matrices X:
F ≤ γ · (cid:107)A(X)(cid:107)2 ≤ (1 + δr)(cid:107)X(cid:107)2
F .
We say that A satisﬁes r-RIP if A satisﬁes (r  δr)-RIP with some δr < 1.
Theorem 2 (No spurious local minima). Let A satisfy (2r  δ2r)-RIP with δ2r < 1/5. Then  (1)
has no spurious local minima: every local minimum x satisﬁes xxT = Z  and every saddle point
has an escape (the Hessian has a negative eigenvalue). Hence  any algorithm that converges to a
second-order critical point is guaranteed to recover Z exactly.
Standard proofs of Theorem 2 use a norm-preserving argument: if A satisﬁes (2r  δ2r)-RIP with a
small constant δ2r  then we can view the least-squares residual A(xxT ) − b as a dimension-reduced
embedding of the displacement vector xxT − Z  as in

(2)

F up to scaling.

(cid:107)A(xxT ) − b(cid:107)2 = (cid:107)A(xxT − Z)(cid:107)2 ≈ (cid:107)xxT − Z(cid:107)2

(3)
The high-dimensional problem of minimizing (cid:107)xxT − Z(cid:107)2
F over x contains no spurious local min-
ima  so its dimension-reduced embedding (1) should satisfy a similar statement. Indeed  this same
argument can be repeated for noisy measurements and nonsymmetric matrices to result in similar
guarantees [4  20].
The norm-preserving argument also extends to “harder” choices of A that do not satisfy RIP over
its entire domain. In the matrix completion problem  the RIP-like condition (cid:107)A(X)(cid:107)2 ≈ (cid:107)X(cid:107)2
F
holds only when X is both low-rank and sufﬁciently dense [12]. Nevertheless  Ge et al. [21] proved
a similar result to Theorem 2 for this problem  by adding a regularizing term to the objective. For a
detailed introduction to the norm-preserving argument and its extension with regularizers  we refer
the interested reader to [21  20].

1.1 How much restricted isometry?

The RIP threshold δ2r < 1/5 in Theorem 2 is highly conservative—it is only applicable to nearly-
isotropic measurements like Gaussian measurements. Let us put this point into perspective by mea-
suring distortion using the condition number1 κ2r ∈ [1 ∞). Deterministic linear maps from real-life
applications usually have condition numbers κ2r between 102 and 104  and these translate to RIP
constants δ2r = (κ2r − 1)/(κ2r + 1) between 0.99 and 0.9999. By contrast  the RIP threshold
δ2r < 1/5 requires an equivalent condition number of κ2r = (1 + δ2r)/(1 − δ2r) < 3/2  which
would be considered near-perfect in linear algebra.
In practice  nonconvex matrix completion works for a much wider class of problems than those
suggested by Theorem 2 [6  5  32  1]. Indeed  assuming only that A satisﬁes 2r-RIP  solving (1)
to global optimality is enough to guarantee exact recovery [31  Theorem 3.2]. In turn  stochastic
algorithms like stochastic gradient descent (SGD) are often able to attain global optimality. This
disconnect between theory and practice motivates the following question.
Can Theorem 2 be substantially improved—is it possible to guarantee the inexistence of spu-
rious local minima with (2r  δ2r)-RIP and any value of δ2r < 1?
At a basic level  the question gauges the generality and usefulness of RIP as a base assumption for
nonconvex recovery. Every family of measure operators A—even correlated and “bad” measure-
ment ensembles—will eventually come to satisfy 2r-RIP as the number of measurements m grows
large. Indeed  given m ≥ n(n + 1)/2 linearly independent measurements  the operator A becomes
invertible  and hence trivially 2r-RIP. In this limit  recovering the ground truth Z from noiseless
measurements is as easy as solving a system of linear equations. Yet  it remains unclear whether
nonconvex recovery is guaranteed to succeed.
At a higher level  the question also gauges the wisdom of exact recovery guarantees through “no
spurious local minima”. It may be sufﬁcient but not necessary; exact recovery may actually hinge

1Given a linear map  the condition number measures the ratio in size between the largest and smallest
images  given a unit-sized input. Within our speciﬁc context  the 2r-restricted condition number is the smallest
κ2r = L/(cid:96) such that (cid:96)(cid:107)X(cid:107)2

F ≤ (cid:107)A(X)(cid:107)2 ≤ L(cid:107)X(cid:107)2

F holds for all rank-2r matrices X.

2

Figure 1: Solving Example 3 using stochastic gradient descent randomly initialized with the standard
Gaussian. (Left) Histogram over 100 000 trials of ﬁnal error (cid:107)xxT − Z(cid:107)F after 103 steps with
learning rate α = 10−3 and momentum β = 0.9. (Right) Two typical stochastic gradient descent
√
trajectories  showing convergence to the spurious local minimum at (0  1/
2)  and to the ground
truth at (1  0).

on SGD’s ability to avoid and escape spurious local minima when they do exist. Indeed  there is
growing empirical evidence that SGD outmaneuvers the “optimization landscape” of nonconvex
functions [6  5  27  32  1]  and achieves some global properties [22  40  39]. It remains unclear
whether the success of SGD for matrix recovery should be attributed to the inexistence of spurious
local minima  or to some global property of SGD.

1.2 Our results

In this paper  we give a strong negative answer to the question above. Consider the counterexample
below  which satisﬁes (2r  δ2r)-RIP with δ2r = 1/2  but nevertheless contains a spurious local
minimum that causes SGD to fail in 12% of trials.
Example 3. Consider the following (2  1/2)-RIP instance of (1) with matrices

(cid:20)1 0
(cid:21)

0 0

(cid:20)√

2
0

√
0
1/

(cid:21)

(cid:20) 0 (cid:112)3/2
(cid:21)
(cid:112)3/2
(cid:20)0

(cid:21)

.

0

(cid:20)0
(cid:21)
0 (cid:112)3/2
(cid:21)
(cid:20)0

0
8

 

0

Z =

  A1 =

  A2 =

0
F ≤ (cid:107)A(X)(cid:107)2 ≤ 3(cid:107)X(cid:107)2
Note that the associated operator A is invertible and satisﬁes (cid:107)X(cid:107)2
X. Nevertheless  the point x = (0  1/

2) satisﬁes second-order optimality 

2
√

  A3 =

F for all

f (x) ≡ (cid:107)A(xxT − Z)(cid:107)2 =

3
2

 

∇f (x) =

 

0

∇2f (x) =

and randomly initialized SGD can indeed become stranded around this point  as shown in Figure 1.
Repeating these trials 100 000 times yields 87 947 successful trials  for a failure rate of 12.1± 0.3%
to three standard deviations.

Accordingly  RIP-based exact recovery guarantees like Theorem 2 cannot be improved beyond
δ2r < 1/2. Otherwise  spurious local minima can exist  and SGD may become trapped. Using
a local search algorithm with a random initialization  “no spurious local minima” is not only sufﬁ-
cient for exact recovery  but also necessary.
In fact  there exists an inﬁnite number of counterexamples like Example 3. In Section 3  we prove
that  in the rank-1 case  almost every choice of x  Z generates an instance of (1) with a strict spurious
local minimum.
Theorem 4 (Informal). Let x  z ∈ Rn be nonzero and not colinear. Then  there exists an instance
of (1) satisfying (n  δn)-RIP with δn < 1 that has Z = zzT as the ground truth and x as a strict
spurious local minimum  i.e. with zero gradient and a positive deﬁnite Hessian. Moreover  δn is

3

00.511.5kxxT−ZkF00.20.40.60.81p-0.200.20.40.60.81x1-0.200.20.40.60.81x2bounded in terms of the length ratio ρ = (cid:107)x(cid:107)/(cid:107)z(cid:107) and the incidence angle φ satisfying xT z =
(cid:107)x(cid:107)(cid:107)z(cid:107) cos φ as

δn ≤ τ +(cid:112)1 − ζ 2

τ + 1

(cid:113)

where ζ =

sin2 φ

(ρ2 − 1)2 + 2ρ2 sin2 φ

 

τ =

2(cid:112)ρ2 + ρ−2

ζ 2

It is therefore impossible to establish “no spurious local minima” guarantees unless the RIP con-
stant δ is small. This is a strong negative result on the generality and usefulness of RIP as a base
assumption  and also on the wider norm-preserving argument described earlier in the introduction.
In Section 4  we provide strong empirical evidence for the following sharp version of Theorem 2.
Conjecture 5. Let A satisfy (2r  δ2r)-RIP with δ2r < 1/2. Then  (1) has no spurious local minima.
Moreover  the ﬁgure of 1/2 is sharp due to the existence of Example 3.

How is the practical performance of SGD affected by spurious local minima? In Section 5  we apply
randomly initialized SGD to instances of (1) engineered to contain spurious local minima. In one
case  SGD recovers the ground truth with a 100% success rate  as if the spurious local minima did not
exist. But in another case  SGD fails in 59 of 1 000 trials  for a positive failure rate of 5.90 ± 2.24%
to three standard deviations. Examining the failure cases  we observe that SGD indeed becomes
trapped around a spurious local minimum  similar to Figure 1 in Example 3.

1.3 Related work

There have been considerable recent interest in understanding the empirical “hardness” of non-
convex optimization  in view of its well-established theoretical difﬁculties. Nonconvex functions
contain saddle points and spurious local minima  and local search algorithms may become trapped
in them. Recent work have generally found the matrix sensing problem to be “easy”  particularly
under an RIP-like incoherence assumption. Our results in this paper counters this intuition  show-
ing—perhaps surprisingly—that the problem is generically “hard” even under RIP.
Comparison to convex recovery. Classical theory for the low-rank matrix recovery problem is
based on convex relaxation: replacing xxT in (1) by a convex term X (cid:23) 0  and augmenting the
objective with a trace penalty λ · tr(X) to induce a low-rank solution [12  31  15  11]. The con-
vex approach enjoys RIP-based exact recovery guarantees [11]  but these are also fundamentally
restricted to small RIP constants [10  38]—in direct analogy with our results for nonconvex recov-
ery. In practice  convex recovery is usually much more expensive than nonconvex recovery  because
it requires optimizing over an n × n matrix variable instead of an n × r vector-like variable. On the
other hand  it is statistically consistent [3]  and guaranteed to succeed with m ≥ 1
2 n(n + 1) noise-
less  linearly independent measurements. By comparison  our results show that nonconvex recovery
can still fail in this regime.
Convergence to spurious local minima. Recent results on “no spurious local minima” are often
established using a norm-preserving argument: the problem at hand is the low-dimension embedding
of a canonical problem known to contain no spurious local minima [19  34  35  4  21  20  30  43].
While the approach is widely applicable in its scope  our results in this paper ﬁnds it to be restrictive
in the problem data. More speciﬁcally  the measurement matrices A1  . . .   Am must come from a
nearly-isotropic ensemble like the Gaussian and the sparse binary.
Special initialization schemes. An alternative way to guarantee exact recovery is to place the initial
point sufﬁciently close to the global optimum [25  26  23  42  41  36]. This approach is more general
because it does not require a global “no spurious local minima” guarantee. On the other hand  good
initializations are highly problem-speciﬁc and difﬁcult to generalize. Our results show that spurious
local minima can exist arbitrarily close to the solution. Hence  exact recovery guarantees must give
proof of local attraction  beyond simply starting close to the ground truth.
Ability of SGD to escape spurious local minima. Practitioners have long known that stochastic
gradient descent (SGD) enjoys properties inherently suitable for the sort of nonconvex optimization
problems that appear in machine learning [27  6]  and that it is well-suited for generalizing unseen
data [22  40  39]. Its speciﬁc behavior is yet not well understood  but it is commonly conjectured
that SGD outperforms classically “better” algorithms like BFGS because it is able to avoid and
escape spurious local minima. Our empirical ﬁndings in Section 5 partially conﬁrms this suspicion 

4

showing that randomly initialized SGD is sometimes able to avoid and escape spurious local minima
as if they did not exist. In other cases  however  SGD can indeed become stuck at a local minimum 
thereby resulting in a positive failure rate.

Notation

We use x to refer to any candidate point  and Z = zzT to refer to a rank-r factorization of the
ground truth Z. For clarity  we use lower-case x  z even when these are n × r matrices.
The sets Rn×n ⊃ Sn are the space of n×n real matrices and real symmetric matrices  and (cid:104)X  Y (cid:105) ≡
tr(X T Y ) and (cid:107)X(cid:107)2
F ≡ (cid:104)X  X(cid:105) are the Frobenius inner product and norm. We write X (cid:23) 0 (resp.
X (cid:31) 0) if X is positive semideﬁnite (resp. positive deﬁnite). Given a matrix M  its spectral norm
is (cid:107)M(cid:107)  and its eigenvalues are λ1(M )  . . .   λn(M ). If M = M T   then λ1(M ) ≥ ··· ≥ λn(M )
and λmax(M ) ≡ λ1(M )  λmin(M ) ≡ λn(M ). If M is invertible  then its condition number is
cond(M ) = (cid:107)M(cid:107)(cid:107)M−1(cid:107); if not  then cond(M ) = ∞.
: Rn×n → Rn2 preserves inner products (cid:104)X  Y (cid:105) =
The vectorization operator vec
vec (X)T vec (Y ) and Euclidean norms (cid:107)X(cid:107)F = (cid:107)vec (X)(cid:107). In each case  the matricization op-
erator mat(·) is the inverse of vec (·).

2 Key idea: Spurious local minima via convex optimization
Given arbitrary x ∈ Rn×r and rank-r positive semideﬁnite matrix Z ∈ Sn  consider the problem
of ﬁnding an instance of (1) with Z as the ground truth and x as a spurious local minimum. While
not entirely obvious  this problem is actually convex  because the ﬁrst- and second-order optimality
conditions associated with (1) are linear matrix inequality (LMI) constraints [9] with respect to the
kernel operator H ≡ ATA. The problem of ﬁnding an instance of (1) that also satisﬁes RIP is indeed
nonconvex. However  we can use the condition number of H as a surrogate for the RIP constant δ
of A: if the former is ﬁnite  then the latter is guaranteed to be less than 1. The resulting optimization
is convex  and can be numerically solved using an interior-point method  like those implemented in
SeDuMi [33]  SDPT3 [37]  and MOSEK [2]  to high accuracy.
We begin by ﬁxing some deﬁnitions. Given a choice of A : Sn → Rm and the ground truth
Z = zzT   we deﬁne the nonconvex objective

f : Rn×r → R

f (x) = (cid:107)A(xxT − zzT )(cid:107)2

such that

(4)
whose value is always nonnegative by construction. If the point x attains f (x) = 0  then we call it
a global minimum; otherwise  we call it a spurious point. Under RIP  x is a global minimum if and
only if xxT = zzT [31  Theorem 3.2]. The point x is said to be a local minimum if f (x) ≤ f (x(cid:48))
holds for all x(cid:48) within a local neighborhood of x. If x is a local minimum  then it must satisfy the
ﬁrst and second-order necessary optimality conditions (with some ﬁxed µ ≥ 0):

∀u ∈ Rn×r 
∀u ∈ Rn×r.

(cid:104)∇f (x)  u(cid:105) = 2(cid:104)A(xxT − zzT ) A(xuT + uxT )(cid:105) = 0

(cid:104)∇2f (x)u  u(cid:105) = 2(cid:104)A(xxT − zzT )  uuT(cid:105) + (cid:107)A(xuT + uxT )(cid:107)2 ≥ µ(cid:107)u(cid:107)2

(5)
(6)
Conversely  if x satisﬁes the second-order sufﬁcient optimality conditions  that is (5)-(6) with µ > 0 
then it is a local minimum. Local search algorithms are only guaranteed to converge to a ﬁrst-order
critical point x satisfying (5)  or a second-order critical point x satisfying (5)-(6) with µ ≥ 0. The
latter class of algorithms include stochastic gradient descent [19]  randomized and noisy gradient
descent [19  28  24  18]  and various trust-region methods [17  29  16  7].
Given arbitrary choices of x  z ∈ Rn×r  we formulate the problem of picking an A satisfying (5) and
(6) as an LMI feasibility. First  we deﬁne A = [vec (A1)  . . .   vec (Am)]T satisfying A· vec (X) =
A(X) for all X as the matrix representation of the operator A. Then  we rewrite (5) and (6) as
2 · L (AT A) = 0 and 2 · M (AT A) (cid:23) µI  where the linear operators L and M are deﬁned

F

such that

L : Sn2 → Rn×r
M : Sn2 → Snr×nr

(7)
(8)
with respect to the error vector e = vec (xxT − zzT ) and the n2 × nr matrix X that implements the
symmetric product operator X · vec (u) = vec (xuT + uxT ). To compute a choice of A satisfying

L (H) ≡ 2 · XT He 
M (H) ≡ 2 · [Ir ⊗ mat(He)T ] + XT HX 

such that

5

L (AT A) = 0 and M (AT A) (cid:23) 0  we solve the following LMI feasibility problem

maximize

H

0

subject to

L (H) = 0  M (H) (cid:23) µI  H (cid:23) 0 

(9)

and factor a feasible H back into AT A  e.g. using Cholesky factorization or an eigendecomposition.
Once a matrix representation A is found  we recover the matrices A1  . . .   Am implementing the
operator A by matricizing each row of A.
Now  the problem of picking A with the smallest condition number may be formulated as the fol-
lowing LMI optimization

(10)
maximize
with solution H(cid:63)  η(cid:63). Then  1/η(cid:63) is the best condition number achievable  and any A recovered

ηI (cid:22) H (cid:22) I  L (H) = 0  M (H) (cid:23) µI  H (cid:23) 0 

subject to

H η

η

1 − 1 − η(cid:63)

1 + η(cid:63)

(cid:107)X(cid:107)2 ≤ 2

1 + η(cid:63)(cid:107)A(X)(cid:107)2

F ≤

1 − η(cid:63)
1 + η(cid:63)

1 +

(cid:107)X(cid:107)2

for all X  that is  with any rank. As such  A is (n  δn)-RIP with δn = (1 − η(cid:63))/(1 + η(cid:63))  and hence
also (p  δp)-RIP with δp ≤ δn for all p ∈ {1  . . .   n}; see e.g. [31  11]. If the optimal value η(cid:63) is
strictly positive  then the recovered A yields an RIP instance of (1) with zzT as the ground truth and
x as a spurious local minimum  as desired.
It is worth emphasizing that a small condition number—a large η(cid:63) in (10)—will always yield a small
RIP constant δn  which then bounds all other RIP constants via δn ≥ δp for all p ∈ {1  . . .   n}.
However  the converse direction is far less useful  as the value of δn = 1 does not preclude δp with
p < n from being small.

from H(cid:63) will satisfy(cid:18)

(cid:19)

(cid:18)

(cid:19)

3 Closed-form solutions

It turns out that the LMI problem (10) in the rank-1 case is sufﬁciently simple that it can be solved
in closed-form. (All proofs are given in the Appendix.) Let x  z ∈ Rn be arbitrary nonzero vectors 
and deﬁne

(cid:19)

(cid:18) xT z

(cid:107)x(cid:107)(cid:107)z(cid:107)

φ ≡ arccos

 

(11)

as their associated length ratio and incidence angle. We begin by examining the prevalence of
spurious critical points.
Theorem 6 (First-order optimality). The best-conditioned H(cid:63) (cid:23) 0 such that L (H(cid:63)) = 0 satisﬁes

(cid:113)

where

ζ ≡

sin φ

(ρ2 − 1)2 + 2ρ2 sin2 φ

.

(12)

ρ ≡ (cid:107)x(cid:107)
(cid:107)z(cid:107)  

1 +(cid:112)1 − ζ 2
1 −(cid:112)1 − ζ 2

cond(H(cid:63)) =

δ =(cid:112)1 − ζ 2 < 1 given in (12).

Hence  if φ (cid:54)= 0  then x is a ﬁrst-order critical point for an instance of (1) satisfying (2  δ)-RIP with

The point x = 0 is always a local maximum for f  and hence a spurious ﬁrst-order critical point.
With a perfect RIP constant δ = 0  Theorem 6 says that x = 0 is also the only spurious ﬁrst-order
critical point. Otherwise  spurious ﬁrst-order critical points may exist elsewhere  even when the
RIP constant δ is arbitrarily close to zero. This result highlights the importance of converging to
second-order optimality  in order to avoid getting stuck at a spurious ﬁrst-order critical point.
Next  we examine the prevalence of spurious local minima.
Theorem 7 (Second-order optimality). There exists H satisfying L (H) = 0  M (H) (cid:23) µI  and
ηI (cid:22) H (cid:22) I where
η ≥ 1
1 + τ

τ ≡ 2(cid:112)ρ2 + ρ−2

1 +(cid:112)1 − ζ 2
1 −(cid:112)1 − ζ 2

and ζ is deﬁned in (12). Hence  if φ (cid:54)= 0 and ρ > 0 is ﬁnite  then x is a strict local minimum for an

instance of (1) satisfying (2  δ)-RIP with δ = (τ +(cid:112)1 − ζ 2)/(1 + τ ) < 1.

(cid:107)z(cid:107)2
1 + τ

(cid:32)

(cid:33)

µ =

ζ 2

·

 

 

6

If φ (cid:54)= 0 and ρ > 0  then x is guaranteed to be a strict local minimum for a problem instance sat-
isfying 2-RIP. Hence  we must conclude that spurious local minima are ubiquitous. The associated
RIP constant δ < 1 is not too much worse than than the ﬁgure quoted in Theorem 6. On the other
hand  spurious local minima must cease to exist once δ < 1/5 according to Theorem 2.

4 Experiment 1: Minimum δ with spurious local minima

What is smallest RIP constant δ2r that still admits an instance of (1) with spurious local minima?
Let us deﬁne the threshold value as the following

δ(cid:63) = min

x Z A{δ : ∇f (x) = 0  ∇2f (x) (cid:23) 0  A satisﬁes (2r  δ)-RIP}.

(13)
Here  we write f (x) = (cid:107)A(xxT − Z)(cid:107)2  and optimize over the spurious local minimum x ∈ Rn×r 
the rank-r ground truth Z (cid:23) 0  and the linear operator A : Rn×n → Rm. Note that δ(cid:63) gives a “no
spurious local minima” guarantee  due to the inexistence of counterexamples.
Proposition 8. Let A satisfy (2r  δ2r)-RIP. If δ2r < δ(cid:63)  then (1) has no spurious local minimum.

Proof. Suppose that (1) contained a spurious local minimum x for ground truth Z. Then  substitut-
ing this choice of x  Z A into (13) would contradict the deﬁnition of δ(cid:63) as the minimum.

Our convex formulation in Section 2 bounds δ(cid:63) from above. Speciﬁcally  our LMI problem (10)
with optimal value η(cid:63) is equivalent to the following variant of (13)

δub(x  Z) = minA {δ : ∇f (x) = 0  ∇2f (x) (cid:23) 0  A satisﬁes (n  δ)-RIP} 

(14)
with optimal value δub(x  Z) = (1− η(cid:63))/(1 + η(cid:63)). Now  (14) gives an upper-bound on (13) because
(n  δ)-RIP is a sufﬁcient condition for (2r  δ)-RIP. Hence  we have δub(x  Z) ≥ δ(cid:63) for every valid
choice of x and Z.
The same convex formulation can be modiﬁed to bound δ(cid:63) from below2. Speciﬁcally  a necessary
condition for A to satisfy (2r  δ2r)-RIP is the following

(1 − δ2r)(cid:107)U Y U T(cid:107)2

(15)
where U is a ﬁxed n × 2r matrix. This is a convex linear matrix inequality; substituting (15) into
(13) in lieu of of (2r  δ)-RIP yields a convex optimization problem

F ≤ (cid:107)A(U Y U T )(cid:107)2 ≤ (1 + δ2r)(cid:107)U Y U T(cid:107)2

∀Y ∈ R2r×2r

F

δlb(x  Z U) = minA {δ : ∇f (x) = 0  ∇2f (x) (cid:23) 0 

(15)} 

(16)

that generates lower-bounds δ(cid:63) ≥ δlb(x  Z  U ).
Our best upper-bound is likely δ(cid:63) ≤ 1/2. The existence of Example 3 gives the upper-bound of
δ(cid:63) ≤ 1/2. To improve upon this bound  we randomly sample x  z ∈ Rn×r i.i.d. from the standard
Gaussian  and evaluate δub(x  zzT ) using MOSEK [2]. We perform the experiment for 3 hours
on each tuple (n  r) ∈ {1  2  . . .   10} × {1  2} but obtain δub(x  zzT ) ≥ 1/2 for every x and z
considered.
The threshold is likely δ(cid:63) = 1/2. Now  we randomly sample x  z ∈ Rn×r i.i.d. from the standard
Gaussian. For each ﬁxed {x  z}  we set U = [x  z] and evaluate δlb(x  Z  U ) using MOSEK [2].
We perform the same experiment as the above  but ﬁnd that δlb(x  zzT   U ) ≥ 1/2 for every x and z
considered. Combined with the existence of the upper-bound δ(cid:63) = 1/2  these experiments strongly
suggest that δ(cid:63) = 1/2.

5 Experiment 2: SGD escapes spurious local minima

How is the performance of SGD affected by the presence of spurious local minima? Given that
spurious local minima cease to exist with δ < 1/5  we might conjecture that the performance of
SGD is a decreasing function of δ. Indeed  this conjecture is generally supported by evidence from

2We thank an anonymous reviewer for this key insight.

7

Figure 2: “Bad” instance (n = 12  r = 2) with RIP constant δ = 0.973 and spurious local min at
xloc satisfying (cid:107)xxT(cid:107)F /(cid:107)zzT(cid:107)F ≈ 4. Here  γ controls initial SGD x = γw + (1− γ)xloc where w
is random Gaussian. (Left) Error distribution after 10 000 SGD steps (rate 10−4  momentum 0.9)
over 1 000 trials. Line: median. Inner bands: 5%-95% quantile. Outer bands: min/max. (Right
top) Random initialization with γ = 1; (Right bottom) Initialization at local min with γ = 0.

Figure 3: “Good” instance (n = 12  r = 1) with RIP constant δ = 1/2 and spurious local min at
xloc satisfying (cid:107)xxT(cid:107)F /(cid:107)zzT(cid:107)F = 1/2 and xT z = 0. Here  γ controls initial SGD x = γw +
(1 − γ)xloc where w is random Gaussian. (Left) Error distribution after 10 000 SGD steps (rate
10−3  momentum 0.9) over 1 000 trials. Line: median.
Inner bands: 5%-95% quantile. Outer
bands: min/max. (Right top) Random initialization γ = 1 with success; (Right bottom) Random
initialization γ = 1 with failure.

the nearly-isotropic measurement ensembles [6  5  32  1]  all of which show improving performance
with increasing number of measurements m.
This section empirically measures SGD (with momentum  ﬁxed learning rates  and batchsizes of
one) on two instances of (1) with different values of δ  both engineered to contain spurious local
minima by numerically solving (10). We consider a “bad” instance  with δ = 0.975 and rank r = 2 
and a “good” instance  with δ = 1/2 and rank r = 1. The condition number of the “bad” instance
is 25 times higher than the “good” instance  so classical theory suggests the former to be a factor of
5-25 times harder to solve than the former. Moreover  the “good” instance is locally strongly convex
at its isolated global minima while the “bad” instance is only locally weakly convex  so ﬁrst-order
methods like SGD should locally converge at a linear rate for the former  and sublinearly for the
latter.
SGD consistently succeeds on “bad” instance with δ = 0.975 and r = 2. We generate the “bad”
instance by ﬁxing n = 12  r = 2  selecting x  z ∈ Rn×r i.i.d. from the standard Gaussian  rescale z
so that (cid:107)zzT(cid:107)F = 1 and rescale x so that (cid:107)xxT(cid:107)F /(cid:107)zzT(cid:107)F ≈ 4  and solving (10); the results are
shown in Figure 2. The results at γ ≈ 0 validate xloc as a true local minimum: if initialized here 
then SGD remains stuck here with > 100% error. The results at γ ≈ 1 shows randomly initialized

8

00.20.40.60.8110-410-21000200040006000800010000-1010200040006000800010000-101200.20.40.60.8110-410-21000200040006000800010000-0.500.50200040006000800010000-0.500.5SGD either escaping our engineered spurious local minimum  or avoiding it altogether. All 1 000
trials at γ = 1 recover the ground truth to < 1% accuracy  with 95% quantile at ≈ 0.6%.
SGD consistently fails on “good” instance with δ = 1/2 and r = 1. We generate the “good”
instance with n = 12 and r = 1 using the procedure in the previous Section; the results are shown
in Figure 3. As expected  the results at γ ≈ 0 validate xloc as a true local minimum. However  even
with γ = 1 yielding a random initialization  59 of the 1 000 trials still result in an error of > 50% 
thereby yielding a failure rate of 5.90 ± 2.24% up to three standard deviations. Examine the failed
trials closer  we do indeed ﬁnd SGD hovering around our engineered spurious local minimum.
Repeating the experiment over other instances of (1) obtained by solving (10) with randomly se-
lected x  z  we generally obtain graphs that look like Figure 2. In other words  SGD usually escapes
spurious local minima even when they are engineered to exist. These observations continue to hold
true with even massive condition numbers on the order of 104  with corresponding RIP constant
δ = 1 − 10−4. On the other hand  we do occasionally sample well-conditioned instances that
behave closer to the “good” instance describe above  causing SGD to consistently fail.

6 Conclusions

The nonconvex formulation of low-rank matrix recovery is highly effective  despite the apparent
risk of getting stuck at a spurious local minimum. Recent results have shown that if the linear
measurements of the low-rank matrix satisfy a restricted isometry property (RIP)  then the problem
contains no spurious local minima  so exact recovery is guaranteed. Most of these existing results
are based on a norm-preserving argument: relating (cid:107)A(xxT − Z)(cid:107) ≈ (cid:107)xxT − Z(cid:107)F and arguing that
a lack of spurious local minima in the latter implies a similar statement in the former.
Our key message in this paper is that moderate RIP is not enough to eliminate spurious local min-
ima. To prove this  we formulate a convex optimization problem in Section 2 that generates coun-
terexamples that satisfy RIP but contain spurious local minima. Solving this convex formulation
in closed-form in Section 3 shows that counterexamples are ubiquitous: almost any rank-1 Z (cid:23) 0
and any x ∈ Rn can respectively be the ground truth and spurious local minimum to an instance of
matrix recovery satisfying RIP. We gave one speciﬁc counterexample with RIP constant δ = 1/2 in
the introduction that causes randomly initialized stochastic gradient descent (SGD) to fail 12% of
the time.
Moreover  stochastic gradient descent (SGD) is often but not always able to avoid and escape spuri-
ous local minima. In Section 5  randomly initialized SGD solved one example with a 100% success
rate over 1 000 trials  despite the presence of spurious local minima. However  it failed with a con-
sistent rate of ≈ 6% on another other example with an RIP constant of just 1/2. Hence  as long
as spurious local minima exist  we cannot expect to guarantee exact recovery with SGD (without a
much deeper understanding of the algorithm).
Overall  exact recovery guarantees will generally require a proof of no spurious local minima. How-
ever  arguments based solely on norm preservation are conservative  because most measurements
are not isotropic enough to eliminate spurious local minima.

Acknowledgements

We thank our three NIPS reviewers for helpful comments and suggestions. In particular  we thank
reviewer #2 for a key insight that allowed us to lower-bound δ(cid:63) in Section 4. This work was sup-
ported by the ONR Awards N00014-17-1-2933 and ONR N00014-18-1-2526  NSF Award 1808859 
DARPA Award D16AP00002  and AFOSR Award FA9550- 17-1-0163.

References

[1] Alekh Agarwal  Olivier Chapelle  Miroslav Dudík  and John Langford. A reliable effective
terascale linear learning system. The Journal of Machine Learning Research  15(1):1111–
1133  2014.

9

[2] Erling D Andersen and Knud D Andersen. The MOSEK interior point optimizer for linear
programming: an implementation of the homogeneous algorithm. In High performance opti-
mization  pages 197–232. Springer  2000.

[3] Francis R Bach. Consistency of trace norm minimization. Journal of Machine Learning Re-

search  9(Jun):1019–1048  2008.

[4] Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro. Global optimality of local search
for low rank matrix recovery. In Advances in Neural Information Processing Systems  pages
3873–3881  2016.

[5] Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTAT’2010  pages 177–186. Springer  2010.

[6] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural

information processing systems  pages 161–168  2008.

[7] Nicolas Boumal  P-A Absil  and Coralia Cartis. Global rates of convergence for nonconvex

optimization on manifolds. IMA Journal of Numerical Analysis  2018.

[8] Nicolas Boumal  Vlad Voroninski  and Afonso Bandeira. The non-convex Burer-Monteiro ap-
proach works on smooth semideﬁnite programs. In Advances in Neural Information Processing
Systems  pages 2757–2765  2016.

[9] Stephen Boyd  Laurent El Ghaoui  Eric Feron  and Venkataramanan Balakrishnan. Linear

matrix inequalities in system and control theory  volume 15. SIAM  1994.

[10] T Tony Cai and Anru Zhang. Sharp RIP bound for sparse signal and low-rank matrix recovery.

Applied and Computational Harmonic Analysis  35(1):74–93  2013.

[11] Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery
from a minimal number of noisy random measurements. IEEE Transactions on Information
Theory  57(4):2342–2359  2011.

[12] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics  9(6):717  2009.

[13] Emmanuel J Candes  Justin K Romberg  and Terence Tao. Stable signal recovery from in-
complete and inaccurate measurements. Communications on pure and applied mathematics 
59(8):1207–1223  2006.

[14] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions

on information theory  51(12):4203–4215  2005.

[15] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. IEEE Transactions on Information Theory  56(5):2053–2080  2010.

[16] Coralia Cartis  Nicholas IM Gould  and Ph L Toint. Complexity bounds for second-order

optimality in unconstrained optimization. Journal of Complexity  28(1):93–108  2012.

[17] Andrew R Conn  Nicholas IM Gould  and Ph L Toint. Trust region methods  volume 1. SIAM 

2000.

[18] Simon S Du  Chi Jin  Jason D Lee  Michael I Jordan  Aarti Singh  and Barnabas Poczos.
Gradient descent can take exponential time to escape saddle points. In Advances in Neural
Information Processing Systems  pages 1067–1077  2017.

[19] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points–online stochas-
In Conference on Learning Theory  pages 797–842 

tic gradient for tensor decomposition.
2015.

[20] Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems:
A uniﬁed geometric analysis. In International Conference on Machine Learning  pages 1233–
1242  2017.

10

[21] Rong Ge  Jason D Lee  and Tengyu Ma. Matrix completion has no spurious local minimum.

In Advances in Neural Information Processing Systems  pages 2973–2981  2016.

[22] Moritz Hardt  Ben Recht  and Yoram Singer. Train faster  generalize better: Stability of
stochastic gradient descent. In International Conference on Machine Learning  pages 1225–
1234  2016.

[23] Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory
of computing  pages 665–674. ACM  2013.

[24] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape
saddle points efﬁciently. In International Conference on Machine Learning  pages 1724–1732 
2017.

[25] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a

few entries. IEEE Transactions on Information Theory  56(6):2980–2998  2010.

[26] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from

noisy entries. Journal of Machine Learning Research  11(Jul):2057–2078  2010.

[27] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[28] Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent only

converges to minimizers. In Conference on Learning Theory  pages 1246–1257  2016.

[29] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[30] Dohyung Park  Anastasios Kyrillidis  Constantine Carmanis  and Sujay Sanghavi. Non-square
matrix sensing without spurious local minima via the Burer-Monteiro approach. In Artiﬁcial
Intelligence and Statistics  pages 65–74  2017.

[31] Benjamin Recht  Maryam Fazel  and Pablo A Parrilo. Guaranteed minimum-rank solutions of
linear matrix equations via nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[32] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale

matrix completion. Mathematical Programming Computation  5(2):201–226  2013.

[33] Jos F Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones.

Optimization methods and software  11(1-4):625–653  1999.

[34] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery using nonconvex optimiza-

tion. In International Conference on Machine Learning  pages 2351–2360  2015.

[35] Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retrieval. In Information

Theory (ISIT)  2016 IEEE International Symposium on  pages 2379–2383. IEEE  2016.

[36] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.

IEEE Transactions on Information Theory  62(11):6535–6579  2016.

[37] Kim-Chuan Toh  Michael J Todd  and Reha H Tütüncü. Sdpt3–a matlab software package for
semideﬁnite programming  version 1.3. Optimization methods and software  11(1-4):545–581 
1999.

[38] HuiMin Wang and Song Li. The bounds of restricted isometry constants for low rank matrices

recovery. Science China Mathematics  56(6):1117–1127  2013.

[39] Ashia C Wilson  Rebecca Roelofs  Mitchell Stern  Nati Srebro  and Benjamin Recht. The
In Advances in Neural

marginal value of adaptive gradient methods in machine learning.
Information Processing Systems  pages 4151–4161  2017.

11

[40] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 
2016.

[41] Tuo Zhao  Zhaoran Wang  and Han Liu. A nonconvex optimization framework for low rank
matrix estimation. In Advances in Neural Information Processing Systems  pages 559–567 
2015.

[42] Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank minimiza-
tion and semideﬁnite programming from random linear measurements. In Advances in Neural
Information Processing Systems  pages 109–117  2015.

[43] Zhihui Zhu  Qiuwei Li  Gongguo Tang  and Michael B Wakin. Global optimality in low-rank

matrix optimization. IEEE Transactions on Signal Processing  66(13):3614–3628  2018.

12

,Ching-An Cheng
Byron Boots
Richard Zhang
Cedric Josz
Somayeh Sojoudi
Javad Lavaei