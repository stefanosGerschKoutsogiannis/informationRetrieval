2019,Bayesian Joint Estimation of Multiple Graphical Models,In this paper  we propose a novel Bayesian group regularization method based on the spike and slab Lasso priors for jointly estimating multiple graphical models. The proposed method can be used to estimate the common sparsity structure underlying the graphical models while capturing potential heterogeneity of the precision matrices corresponding to those models. Our theoretical results show that the proposed method enjoys the optimal rate of convergence in $\ell_\infty$ norm for estimation consistency and has a strong structure recovery guarantee even when the signal strengths over different graphs are heterogeneous. Through  simulation studies and an application to the capital bike-sharing network data  we demonstrate the competitive performance of our method compared to existing alternatives.,Bayesian Joint Estimation of
Multiple Graphical Models

Lingrui Gan  Xinming Yang  Naveen N. Nariestty  Feng Liang

Department of Statistics

University of Illinois at Urbana-Champaign

{lgan6  xyang104  naveen  liangf}@illinois.edu

Abstract

In this paper  we propose a novel Bayesian group regularization method based on
the spike and slab Lasso priors for jointly estimating multiple graphical models. The
proposed method can be used to estimate common sparsity structure underlying
the graphical models while capturing potential heterogeneity of the precision
matrices corresponding to those models. Our theoretical results show that the
proposed method enjoys the optimal rate of convergence in (cid:96)8 norm for estimation
consistency and has a strong structure recovery guarantee even when the signal
strengths over different graphs are heterogeneous. Through simulation studies
and an application to the capital bike-sharing network data  we demonstrate the
competitive performance of our method compared to existing alternatives.

1

Introduction

Gaussian graphical models (GGMs) are widely studied from both the frequentist [30  9  3  18  28]
and the Bayesian perspectives [4  7  26  1  19  10  14]. A GGM model assumes that a collection of
variables jointly follows a multivariate Gaussian distribution with an unknown precision matrix. It is
well known that there is a one-to-one correspondence between the sparsity pattern of the precision
matrix of a Gaussian distribution and the graph that describes the conditional dependence structure
among the variables: Non-zero entries in the precision matrix correspond to edges in the graph [6].
Due to this connection  given data from a GGM  we are interested in estimating not only the precision
matrix but also its support.
In many applications  observations are naturally grouped into different classes. For example  in
biological experiments  subjects are classiﬁed into categories based on their experimental conditions;
in social network data  users are grouped by users’ characteristics; and in gene expression analysis 
expression data are classiﬁed into different tissues or disease states. In such situations  it is restrictive
to assume that all observations follow the same graphical model  i.e.  have the same precision matrix 
and it would be more suitable to assume that different classes have different precision matrices.
Meaningful insights can be more effectively extracted  if we utilize the cross-class similarities of the
precision matrices and estimate graphs for the multiple classes jointly.
Several approaches have been proposed for jointly estimating multiple GGMs. From the penalized
likelihood perspective  [12  5  13  17] extended approaches for estimating a single graph to the
multiple-graph setting by introducing group-level penalty terms and studied the theoretical properties
of these approaches. From the Bayesian perspective  Peterson et al. [21] proposed a Markov random
ﬁeld prior on multiple graphs to encourage the selection of common edges in related graphs. Tan
et al. [25] proposed to use a Chung-Lu random graph model as the prior for hierarchical modeling of
multiple GGMs. However  theoretical guarantees of the Bayesian methods are not available.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper  we develop a Bayesian approach for jointly estimating multiple GGMs under the
assumption that the multiple precision matrices share a common sparsity structure but they can
have heterogeneous signal magnitudes. We provide theoretical results showing that the maximum
a posteriori (MAP) estimators have the optimal rate of convergence in (cid:96)8 norm  even under model
a
mis-speciﬁcation when precision matrices for different classes do not share a common sparsity
structure. When the multiple GGMs do share a common sparsity structure  the proposed approach is
proved to be consistent in recovering such a structure  even when some of the within group signals
are weaker than the p
plog pq{nq rate  the minimal signal strength usually required for consistency
results on structure recovery .
The remaining part of the paper is organized as follows. The Bayesian formulation and parameter
estimation procedure for our model are provided in Section 2. Theoretical guarantees of our approach
are presented in Section 3 and empirical studies are provided in Section 4.

2 Method

Let Y1  . . .   YK denote the data from K classes  where the k-th dataset Yk consists of nk observations
pYk 1  . . .   Yk nkq  each of which is a p-dimensional vector. Throughout we assume the p variables are
common across the K classes and data from each class follow a p-dimensional Gaussian distribution:

Our target is to estimate the K precision matrices Θk “ pθk ijq  k P t1  . . .   Ku  and recover their
common sparsity structure.

Yk 1  . . .   Yk nk „ Npp0  Θ´1
k q.

2.1 Bayesian Formulation

For regularized estimation and sparsity recovery  we shall place prior distributions on the Θk’s by ﬁrst
introducing binary latent variables γij which indicate whether nodes i and j have an edge (γij “ 1)
or not (γij “ 0). A Bernoulli prior on γij and a spike and slab prior on the off-diagonal elements of
Θk are placed as follows:

ppθk ij|γijq “

(2.1)
where v1 ą v0 ą 0 and LPp¨; vq is the Laplace distribution with parameter v. For γij “ 1  θk ij’s
represent signals modeled by a slab distribution with high variance  and for γij “ 0  θk ij’s represent
noise modeled by a spike distribution with mass tightly centered around zero. Integrating over γij  we
have the following multivariate spike and slab prior on the group of K entries θij “ pθ1 ij  . . .   θK ijq:

γij „ Bernpp1q 

"
LPpθk ij; v1q when γij “ 1 
LPpθk ij; v0q when γij “ 0 

Kź

LPpθk ij; v1q ` p1 ´ p1q Kź

ppθijq “ p1

LPpθk ij; v0q.

k“1

(2.2)
When K “ 1  the distribution above is the one-dimensional spike and slab Lasso prior utilized for
linear regression by [23  24] and for analyzing a single GGM by [10].
We impose the aforementioned spike and slab Lasso prior (2.2) on the upper triangular of the precision
matrices  i.e.  on θij for i ă j  and enforce θji “ θij to keep Θk symmetric. In addition  we place
independent exponential priors on the positive diagonal entries to introduce a small shrinkage. In
summary  the Bayesian prior formulation of our model is as follows:

k“1

Kź
θk ii „ Exppτq 
θij „ p1
where i ă j and k “ 1  . . .   K.

k“1

LPpθk ij; v1q ` p1 ´ p1q Kź

k“1

LPpθk ij; v0q 

(2.3)

2.2 Parameter Estimation
We ﬁrst focus on the point estimation of Θ “ pΘ1  . . .   ΘKq and then discuss the posterior inference
of the sparsity structure conditional on the point estimator (discussed in Section 2.3). Motivated

2

by [16  10]  we estimate Θ by solving the following optimization problem under the constraint of
Ω “ tΘ : Θk ą 0  }Θk}2 ď B  k “ 1  . . .   Ku where Θk ą 0 indicates that Θk is positive deﬁnite
and }Θk}2 denotes the spectral norm of Θk. Our constrained MAP estimator is given by

ˆΘ “ arg min
ΘPΩ

p´ log ppY | Θq ` αPenpΘqq   α ě 1 
nkÿ

where log ppY | Θq is the log likelihood and PenpΘq is the negative log of the prior on Θ:
˙

plog detpΘkq ´ trpSkΘkqq   Sk “ 1
nk

log ppY | Θq “ Kÿ
PenpΘq “ pÿ

τ θk ii `

Yk iY T
k i 

´ log

ÿ

´ }θij}1

nk
2

Kÿ

´ }θij}1

i“1
v1 ` 1 ´ p1
p2v0qK e

$’’’’’&’’’’’%

p2v1qK e

ˆ

k“1

p1

v0

(2.4)

 

i“1

k“1

iăj

with Sk being the sample covariance matrix of the k-th class and }θij}1 being the (cid:96)1 norm of the
vector θij. From the Bayesian viewpoint  our estimator (2.4) is equivalent to the MAP estimator of
the posterior ppΘ | Y q 9 ppY | ΘqppΘqα  where the prior is raised to the power of α so that its
inﬂuence on inference can be appropriately magniﬁed. From the penalized likelihood viewpoint  we
are essentially multiplying the penalty function PenpΘq by a multiplier of α  which is equivalent
to scaling the log likelihood by 1{α. Such an adjustment is commonly adopted to develop optimal
theoretical results  for example  by [8  31  16].
For scalability  we propose to compute the MAP estimator instead of sampling from the full posterior.
Full posterior sampling for high-dimensional GGMs is computationally expensive  for example  in
[21  25]  the dimension p in all empirical studies is at most 22 due to the computational limitations.
Although we only propose a point estimator (2.4) for the precision matrices  our model is still
formulated from a Bayesian perspective with a continuous spike and slab prior distribution. While
this prior does not directly place mass on sparse solutions as in [27  29]  the latent binary indicators
γij introduced can distinguish between “signal” and “noise”. This is a common technique used in
the Bayesian literature [11  20  23  10] to avoid the computational bottleneck of degenerate priors.
Furthermore  with the Bayesian machinery  we are able to extract the posterior inclusion probabilities
for structure recovery in Section 2.3 and provide strong guarantees for graph selection in Section 3.3.
The term PenpΘq  which is induced from our prior speciﬁcation  acts as a non-convex penalty function.
The non-convexity of the penalty brings desired shrinkage effects  as shown in our theoretical results
in Section 3 and prior results in the literature [31  16  15]. However  it may cause the whole objective
function to be non-convex  and consequently  we need to deal with multiple local solutions to the
minimization problem (2.4). In the following theorem  we show that when being constrained to the
parameter space Ω  the minimization problem (2.4) is in fact strictly convex. Thus  the solution ˆΘ of
the objective function (2.4) will be unique. Proof of this result is in Supplementary Material.
Theorem 1 If B ă
Remark. The upper bound on B can increase with sample size n. When we establish the selection
consistency of ˆΘ in Theorem 2  we will require the order of v2
. Therefore 
  which can go to `8 as long as the order of α is greater
the upper bound on B is O
than K log p.

2nv2
αK   then the constrained minimization problem (2.4) is strictly convex.
0

¯
α{pK log pq

α2{pn log pq

´a

0 to be O

b

`

˘

2.3 Common Structure Recovery

Utilizing the hierarchical structure (2.1) of the spike and slab prior  we make inference on the common
sparsity structure based on the following posterior inclusion probability (PIP)

Ppγij “ 1 | ˆθijq “

We can estimate the common sparsity structure by thresholding the PIP  e.g.  with t “ 1{2:

!
´p 1

1

) ﬁ pp ˆθijq.

v0

p1

p v1

qK exp

1 ` 1´p1
!
pi  jq : pp ˆθijq ą t 

v1

v0

´ 1

q} ˆθij}1
)
for t P p0  1q

.

ˆS “

(2.5)

(2.6)

3

Note that the PIP is a function of } ˆθij}1  so that the signal strength in the whole group is utilized
together to estimate the common sparsity pattern. Even when some individual entries are of small
signal strength  the information shared within its group could help us to identify the shared structure.
Our theoretical results provided in Section 3 conﬁrm that this strategy will be indeed beneﬁcial for
recovering such signals.

3 Theoretical Guarantees

In this section  we develop the theoretical properties of the proposed estimator ˆΘ including estimation
accuracy and structure recovery consistency. For simplicity  we assume the sample sizes of the K
classes are the same with n1 “ ¨¨¨ “ nK “ n in the theoretical analysis.
Notations: For a square matrix Apˆp “ paijq  we denote its element-wise (cid:96)8 norm by }A}8 “
max1ďi jďp |aij|; its Frobenius norm by }A}F ; and its spectral norm by }A}2. We denote the largest
eigenvalue and smallest eigenvalue of A by λmaxpAq and λminpAq  respectively. When A is a square
symmetric matrix  we note }A}2 “ λmaxpAq. For a collection of K square matrices of the same
dimension A “ pA1  . . .   AKq  write }A}8 “ sup1ďkďK }Ak}8. Let Θ0 “ pΘ0
Kq denote
k ij ‰ 0u denote the index set of nonzero
the collection of true precision matrices and S 0
k as dk “ maxi cardptj :
entries in the true precision matrix Θ0
k ij ‰ 0uq where cardp¨q denotes the cardinality of a set and let d “ maxk dk.
θ0
3.1 Conditions

k. Deﬁne column sparsity of Θ0

k “ tpi  jq : θ0

1  . . .   Θ0

In our theoretical analysis  we do not restrict the observed data to follow a Gaussian distribution.
Thus  our Bayesian hierarchical model (2.3) could be treated as a working model. The observed data
are allowed to be from any distribution with exponential tails (e.g.  sub-Gaussian distributions) or
polynomial tails (e.g.  t distributions)  which is the same setup considered in [3  10] when the class
ppq
size K “ 1. Speciﬁcally  for all the p-dimensional random vectors Yk i “ pY
k i q  i “
1  . . .   nk and k “ 1  . . .   K  we have the following assumptions:
(A.1) Exponential tail condition: there exist some constants 0 ă η ă 1{4 and U ą 0 such that

p1q
k i   . . .   Y

plog pq{n ă η and

EpetY

pjq
k i q ď U for any |t| ď η and j “ 1  . . .   p;

(3.1)
(A.2) Polynomial tail condition: there exist some constants κ1  κ2  κ3  U ą 0 such that p ď κ1nκ2
(3.2)

pjq
k i |4`4κ2`κ3 ď U for j “ 1  . . .   p.

E|Y

and

We shall establish estimation and selection consistency of our method when the true data distribution
satisﬁes (A.1) or (A.2) and is not necessarily a multivariate Gaussian distribution. Note that when the
data indeed follows a multivariate Gaussian distribution  (A.1) is satisﬁed.
In addition  we assume that the eigenvalues of the true precision matrices are bounded:
(A.3) Eigenvalue condition: 1{ξ0 ď λminpΘ0

kq ď 1{ξ1 for k “ 1  . . .   K.

kq ď λmaxpΘ0

3.2 Estimation Accuracy

The following theorem establishes the rate of convergence of the proposed estimator under (cid:96)8 norm.
For this result  we do not require the different precision matrices to have the same sparsity structure.
a
Theorem 2 Suppose one of the tail conditions  (A.1) or (A.2)  holds and the true precision matrices
satisfy (A.3). Let C1 “ η´1p2 ` κ0 ` η´1U 2q when the exponential tail condition (A.1) holds and
C1 “
p}Θ0}8 ` 1qp4 ` κ0q when the polynomial tail condition (A.2) holds for some κ0 ą 0. In
b
addition  assume that
(i) the hyperparameters pv1  v0  p1  τq satisfy:
n log p
  1
α2
v0
p1´p1q
ď vK`2
vK`2

b
ą C4
ď 2p0{α;

$&%maxp 3

  2τq ă C3
v1
1 p1´p1q
vK
0 p1

2 ă vK

n log p

α2

p1

 

1

0

4

(ii) the sample size n satisﬁes:
(iii) the bounds on the spectral norms of the estimated precision matrices satisfy:

log p;

?

n ě M0 maxpd 

c

?

Kq?
ˆ

1{ξ1 ` dC5

log p

ă B ă

2nv2
0
αK

˙1{2

;

n
(iv) the parameter α satisﬁes: αp0{α ą KC 2
Then  the minimizer ˆΘ is unique and satisﬁes

3 log p{p2ξ2
1q.
c

} ˆΘ ´ Θ0}8 ă C5

with probability greater than 1 ´ Kδ  where δ “ 2p´κ0 when condition (A.1) holds  and δ “
Opn´κ3{8 ` p´κ0{2q when condition (A.2) holds. Moreover 
c. Here 
C3  2 are sufﬁciently small positive constants   M0  C4  C5  0 are positive constants only depend on
the ground truth Θ0.

“ 0 for pi  jq P

S 0

ˆΘk

ij

k

log p

n

 

´

¯

`

˘

Our proof is motivated by the constructive proof technique used in [22] and [10]. Details of the
deﬁnitions of M0  C4  C5  0 and the proof of Theorem 2 are provided in Supplementary Material.
Condition (i)  which is related to the rates of the hyperparameters  controls the level of shrinkage
of the penalty function PenpΘq. With a proper choice of the hyperparameters  our penalty function
induces an appropriate adaptive shrinkage effect: the shrinkage is strong enough when the magnitude
of θ is small to kill the noise and produce exact zero  and is insigniﬁcant when the magnitude of θ is
large so that the bias is controlled. Condition (ii) is on the relationship between the sample size n and
the number of variables p  and p could grow nearly exponentially with n. Condition (iii) deals with
a
the parameter space of the constrained optimization problem  which ensures both the feasibility and
convexity of the problem. Under these conditions  our Theorem 2 states that  as long as the parameter
α satisﬁes the condition (iv)  the error rate of every entry of the estimated precision matrices is at
most Opp

plog pq{nq.

3.3 Sparsity Structure Recovery Consistency

a
Besides the estimation accuracy  another important task is to identify the sparsity structure of the
precision matrices as it tells the conditional dependence relationships between the p variables of
plog pq{n for
interest. If the minimal signal strength satisﬁes mink mini‰j pi jqPS 0
some sufﬁciently large constant L0  Theorem 2 directly gives rise to the result that our estimator ˆΘk
has the same sparsity structure as the truth S 0
k with probability converging to 1  even when different
classes do not have the same sparsity structure. If all precision matrices share a common sparsity
K “ S 0  then our proposed method achieves selection consistency with
structure  i.e.  S 0
a weaker condition on the minimal signal strength as stated in the following theorem.

1 “ ¨¨¨ “ S 0

k ij|q ą L0

p|θ0

k

a
plog pq{n 

Theorem 3 Suppose conditions in Theorem 2 all hold. In addition  assume that:
(v) the minimal signal strength satisﬁes

min
pi jqPS 0

|θ0
k ij|q ě L0
where L0 ą C5 is a sufﬁciently large constant;
(vi) rates of the hyperparameters v1  v0  and p1 satisfy
ą

pmax
˙

K ě 1 ´ t

ˆ

k

1 ´ p1
p1

v1
v0

ppC4´C3qpL0´C5q{α
where t is an arbitrary thresholding value between 0 and 1. Then we have

t

 

1´p1
p1

p v1

v0

qK

Pp ˆS “ S 0q Ñ 1.

5

Condition (v) is the condition on the minimal signal strength. Compared to similar conditions required
by approaches that estimate each GGM individually  this condition is weaker since it only places
requirements on the largest signal within each group. Therefore  the whole group would beneﬁt from
one large signal. Under the weaker minimal signal strength condition and with appropriate choice of
the hyperparameters satisfying condition (vi)  we can differentiate between the “signal” and “noise”
groups with probability going to 1. A proof of Theorem 3 is provided in the Supplementary Material.

3.4 Comparisons with Existing Works

n

˙

ř

K

pp`q1q log p

ˆb

k“1 } ˜Θk ´ Θ0

k}F “ Op

  where q1 “ cardpYktS 0

In this section  we compare our theoretical results in estimation accuracy and selection consistency
with other alternatives [12  13]. In the following discussion  we use ˜Θ as a generic notation to denote
estimators proposed by others.
Guo et al. [12] established the estimation accuracy of their estimator ˜Θ in Frobenius norm for a ﬁxed
kuq´ p. We note that
K value:
our Theorem 2 gives rise to the same rate as theirs under Frobenius norm. For recovering the graph
structure  Guo et al. [12] obtained sparsistency  i.e.  the zero entries in the true precision matrices
are estimated as zeroes with probability tending to one. However  there is no guarantee that the
nonzero entries could be detected. This is weaker than our Theorem 3 as we recover the entire graph
structure. Moreover  to achieve sparsistency  Guo et al. [12] require the minimum signal strength
k ij|q to be lower bounded by some constant while we allow it to go to zero.
´
mink mini‰j pi jqPS 0
Lee and Liu [13] established the estimation accuracy of their estimator ˜Θ in the averaged version
of the (cid:96)8-(cid:96)1 norm: maxi j
. Our estimation error rate
from Theorem 2 is on the maximum over all entries of all precision matrices without averaging 
and therefore is stronger. In particular  their result is a direct consequence of ours. For selection
consistency  the major difference between theirs and ours is the condition on the signal strength.
k ij|q to be lower bounded at the rate of
Lee and Liu [13] implicitly require mink mini‰j pi jqPS 0
Kplog p{nq1{2  where K is the class size  while we only require a smaller signal strength plog p{nq1{2.
k ij|q  which is weaker
In addition  our requirement is on the lower bound of mink maxi‰j pi jqPS 0
than requirement on the lower bound of mink mini‰j pi jqPS 0

ˇˇˇ˜θk ij ´ θ0

ˆb

p|θ0

k

p|θ0

k

p|θ0

k

“ Op

1
K

K
k“1

ˇˇˇ¯

ř

˙

log p

n

p|θ0

k ij|q.

k

k ij

4 Numerical Studies

4.1 Computation: an EM Algorithm
For computation  we propose an EM algorithm by treating Γ “ pγijq as latent variables and estimating
Θ by applying the following two steps iteratively:
• E-step: Calculate the posterior distribution Ppγij “ 1 | Θptqq :“ pijpθ

ptq
ij q  which follows the
formula in (2.5)  and compute the so-called Q function  the expectation of the full log-likelihood
with respect to Ppγij “ 1 | Θptqq:

ﬀ

+

τ θk ii´

i“1

ptq
ij q
pijpθ
v1

` 1 ´ pijpθ

v0

ptq
ij q

|θk ij|

.

QpΘq “ Kÿ

k“1

#

nk
2α

plog detpΘkq´trpSkΘkqq´ pÿ

«

ÿ

iăj

• M-step: The Q function is a summation of K terms with each to be a weighted graphical Lasso
[9] problem. Therefore  in the M-step  we maximize the Q function within in the parameter space
Ω  utilizing algorithms for graphical Lasso. As a result  the computational complexity of our EM
algorithm is Opp3q  which is as efﬁcient as the state-of-the-art algorithms for graphical Lasso
problems [9  10].

Derivations and implementation details of the algorithm are provided in the Supplementary Material.

6

4.2 Simulation Results

Following the simulation setups in [12  5  21  13]  we assess the performance of our proposed method
under six different settings: three nearest-neighbor networks and three scale-free networks. The
details of the settings are described as follows.

1. Nearest-neighbor network: we randomly generate p points on a unit square and ﬁnd the o nearest
neighbors of each point in terms of the Euclidean distance. The baseline nearest-neighbor network
is constructed by linking any two points which are the o-nearest neighbors of each other. Larger o
induces a denser network and here  we use o “ 3. After that  we generate K individual networks
by adding ρM individual edges to the baseline graph with M to be number of edges in the baseline
graph and ρ “ 0  0.25  0.5.
Given a network structure  we generate the corresponding precision matrix Θk by assigning
ones on diagonal entries  zeros on entries not corresponding to network edges  and values
from a uniform distribution with support on r´1 ´0.5s Y r0.5  1s on entries corresponding
to edges. To ensure positive deﬁniteness  we then divide each off-diagonal element θk ij by
1.01

bř
j:j‰i |θk ij|.

bř

i:i‰j |θk ij|

2. Scale-free network: many real-world large networks  such as the world wide web  social networks 
and collaboration networks  are thought to be scale-free. We construct the baseline scale-free
network using the Barabási-Albert model [2]. Next  individual networks and corresponding
precision matrices are generated in the same way as in the ﬁrst design.

In each setting  we set K “ 3 and p “ nk “ 100  and  for each k P t1  . . .   Ku  we generate nk
independently and identically distributed observations from a multivariate Gaussian distribution
with mean 0 and precision matrix Θk. We compare our method with α “ 1 and α “ n with three
different methods: ﬁtting each class individually by BAGUS (denoted as BAGUS) [10]; ignoring
the class information and ﬁtting a single model by BAGUS (denoted as Pooled); the group graphical
Lasso (denoted as GGL) [5]. Bayesian approaches based on full posterior sampling [21  25] are
a
not considered for comparison as their Markov chain Monte Carlo (MCMC) samplers are not
scalable with large p. For all methods  we use a grid search to select the set of hyperparamters that
a
minimizes BIC. For BAGUS and Pooled methods  we follow the same tuning procedure in [10] and
tune the spike and slab prior parameters pv0  v1q with v0 “ p0.25  0.5  0.75  1q ˆ
1{pn log pq and
v1 “ p2.5  5  7.5  10q ˆ
1{pn log pq. For GGL  we tune the two penalty parameters pλ1  λ2q as in
[5] with λ1 “ p0.1  0.2  . . .   1q and λ2 “ p0.1  0.3  0.5q.
To compare the performance of the methods  we calculate speciﬁcity (Spec)  sensitivity (Senc) 
Matthews correlation coefﬁcient (MCC)  area under the ROC curve (AUC)  Frobenius norm (F-norm) 
and element-wise (cid:96)8 norm ((cid:96)8 norm) for each class. In Table 1-2  we report the maximum of (cid:96)8
norm and the average of the other measures over the K classes and the results are aggregated based
on 100 replications. From the results  we observe that our method performs the best in all the designs
in terms of both selection accuracy (MCC and AUC) and estimation accuracy (F-norm and (cid:96)8 norm).
Even when ρ ‰ 0  that is  the sparsity patterns over classes are different  which deviates from our
assumption  our method still has the best performance.
The average computational times of all the methods using a MacBook Pro with 2.9 GHz Intel Core
i5 processor and 8.00 GB memory are reported in Table 3. The computational time of our method
is comparable to the competitors except the Pooled method  which restrictively assumes the same
precision matrix for all classes and has much worse performance compared to our method. Therefore 
our method is competitive even after considering the runtimes.

4.3 Application to Capital Bikeshare Data

We use Capital Bikeshare trip data1 to evaluate the performance of the proposed method. The data
contains records of bike rentals in a bicycle sharing system with more than 500 stations. We consider
p “ 237 stations located in Washington  D.C. and record the number of rentals started at these
stations for every day in 2016  2017 and 2018. Following the same processing procedure in [32] 
we remove the seasonal trend and marginally transform each station’s data to a normal distribution.

1Data available at https://www.capitalbikeshare.com/system-data

7

Table 1: Result of nearest-neighbor network

Spec

Sens

MCC

AUC
n “ 100  p “ 100  ρ “ 0

1.000(0.000)
1.000(0.000)
0.994(0.002)
0.989(0.003)
0.948(0.008)

0.994(0.001)
0.992(0.003)
0.988(0.003)
0.976(0.004)
0.966(0.010)

0.920(0.038)
0.993(0.008)
0.816(0.039)
0.664(0.056)
0.707(0.074)

0.955(0.022)
0.991(0.009)
0.794(0.033)
0.616(0.048)
0.401(0.044)

0.974(0.017)
0.996(0.004)
0.903(0.022)
0.840(0.029)
0.845(0.038)

n “ 100  p “ 100  ρ “ 0.25

0.823(0.034)
0.889(0.021)
0.813(0.030)
0.571(0.045)
0.769(0.043)

0.803(0.015)
0.823(0.025)
0.732(0.025)
0.472(0.029)
0.552(0.054)

0.954(0.014)
0.943(0.010)
0.917(0.017)
0.783(0.024)
0.879(0.022)

n “ 100  p “ 100  ρ “ 0.5

F-norm

(cid:96)8 norm

2.576(0.221)
2.094(0.150)
3.184(0.190)
7.115(0.380)
6.338(0.382)

0.503(0.083)
0.449(0.099)
0.551(0.093)
0.983(0.035)
0.604(0.037)

2.862(0.145)
2.867(0.154)
3.372(0.148)
6.179(0.256)
5.274(0.122)

0.443(0.066)
0.443(0.074)
0.591(0.102)
0.871(0.104)
0.529(0.029)

0.992(0.002)
0.986(0.008)
0.986(0.002)
0.976(0.003)
0.980(0.007)

0.664(0.043)
0.770(0.043)
0.710(0.030)
0.469(0.031)
0.684(0.077)

0.699(0.023)
0.713(0.035)
0.667(0.023)
0.421(0.027)
0.608(0.028)

0.920(0.030)
0.882(0.020)
0.878(0.014)
0.777(0.033)
0.838(0.038)

3.170(0.170)
3.256(0.112)
3.707(0.146)
5.538(0.208)
4.940(0.256)

0.426(0.050)
0.427(0.043)
0.587(0.089)
0.735(0.111)
0.502(0.026)

Table 2: Result of scale-free network

Spec

Sens

MCC

AUC
n “ 100  p “ 100  ρ “ 0

1.000(0.000)
0.996(0.002)
0.997(0.001)
0.958(0.003)
0.938(0.007)

0.993(0.001)
0.991(0.002)
0.990(0.001)
0.959(0.004)
0.959(0.006)

1.000(0.002)
0.976(0.014)
0.995(0.004)
0.746(0.043)
1.000(0.001)

0.993(0.006)
0.906(0.047)
0.936(0.019)
0.429(0.027)
0.483(0.022)

1.000(0.000)
0.988(0.007)
0.998(0.002)
0.903(0.018)
1.000(0.001)

n “ 100  p “ 100  ρ “ 0.25

0.921(0.024)
0.914(0.020)
0.919(0.021)
0.654(0.040)
0.964(0.013)

0.833(0.011)
0.808(0.021)
0.801(0.019)
0.415(0.027)
0.591(0.029)

0.992(0.003)
0.955(0.010)
0.967(0.009)
0.833(0.021)
0.980(0.007)

n “ 100  p “ 100  ρ “ 0.5

F-norm

(cid:96)8 norm

1.664(0.088)
1.942(0.133)
1.747(0.096)
7.148(0.300)
5.043(0.282)

0.514(0.117)
0.432(0.092)
0.492(0.107)
0.869(0.024)
0.545(0.019)

2.032(0.079)
2.365(0.083)
2.407(0.100)
6.331(0.229)
4.705(0.137)

0.454(0.087)
0.435(0.050)
0.518(0.088)
0.799(0.040)
0.540(0.024)

0.988(0.001)
0.983(0.007)
0.986(0.003)
0.972(0.003)
0.978(0.007)

0.787(0.031)
0.816(0.042)
0.822(0.023)
0.508(0.032)
0.847(0.041)

0.719(0.018)
0.696(0.036)
0.716(0.028)
0.405(0.025)
0.672(0.035)

0.958(0.009)
0.904(0.020)
0.938(0.011)
0.761(0.030)
0.921(0.021)

2.548(0.088)
2.813(0.099)
3.106(0.131)
5.816(0.178)
4.808(0.275)

0.437(0.058)
0.443(0.046)
0.595(0.116)
0.741(0.052)
0.539(0.030)

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

Table 3: Average computational time (in seconds) based on 10 replications.

Nearest-neighbor Network

Our method pα “ 1q
Our method pα “ nq
BAGUS
Pooled
GGL

ρ “ 0

3.667(0.040)
7.792(0.456)
3.635(0.023)
1.211(0.010)
8.715(0.314)

ρ “ 0.25
3.645(0.087)
4.596(0.643)
3.572(0.027)
1.178(0.013)
8.034(0.689)

ρ “ 0.5

ρ “ 0

3.552(0.026)
3.597(0.049)
3.547(0.021)
1.169(0.008)
5.482(1.528)

3.556(0.037)
5.285(2.623)
3.553(0.012)
1.184(0.015)
8.086(0.262)

Scale-free Network

ρ “ 0.25
3.545(0.030)
3.600(0.025)
3.546(0.022)
1.173(0.008)
6.139(0.678)

ρ “ 0.5

3.537(0.033)
3.578(0.023)
3.534(0.018)
1.168(0.010)
3.074(0.270)

We divide the observations into K “ 3 classes by year as it is natural to expect the precision matrix
changes over year due to annual policy decisions  economic conditions  and other aspects of the
business. Then  we take the ﬁrst 80% of observations in each class as training data and the other 20%
as test data.
We apply our method with α “ 365 as well as other methods we compared in the simulation studies 
i.e.  BAGUS  Pooled  and GGL  on the training data to estimate µk’s and Θk’s. For year k and
day i  we divide the data Yk i “ py
q and
Yk i2 “ py
q. Assuming the ﬁrst half Yk i1 is observed  we predict the second half
Yk i2 by the following best linear predictor derived from the multivariate Gaussian distribution:
k11pYk i1 ´ ˆµk1q  for k “ 1  2  3  and i P Tk 
ˆΘ´1

ˆYk i2 “ EpYk i2 | Yk i1q “ ˆµk2 ` ˆΘk21

q into two parts  Yk i1 “ py

p1q
k i   . . .   y

p237q
k i

p1q
k i   . . .   y

p118q
k i

p119q
k i

  . . .   y

p237q
k i

8

Figure 1: Averaged AAFE versus the
total number of nonzero off-diagonal en-
tries in the estimated precision matrices.

Figure 2: Degree distributions of the estimated common
station networks over three years by our method and
BAGUS.

(a) Our method

(b) BAGUS

where Tk is the index set of the k-th class of test data  µk “ pµk1  µk2q and Θk “
We use the average absolute forecast error (AAFE) of each class for performance comparison:

Θk11 Θk12
Θk21 Θk22

ˆ

˙

.

237ÿ

ÿ

iPTk

AAFEk “ 1
119

1

cardpTkq

j“119

pjq
|ˆy
k i ´ y

pjq
k i|  k “ 1  2  3.

In Figure 1  we plot the averaged AAFE versus the number of nonzero off-diagonal entries in the
estimated precision matrices. For our method  BAGUS  and Pooled methods  we plot the curves by
ﬁxing v1 and varying v0. For GGL  we ﬁx the ratio between its two tuning parameters and varying
them together. Different ratios would output similar curves and only one of them is plotted. We
observe that our method not only achieves the lowest averaged AAFE  but also outputs the sparsest
estimated precision matrices when the lowest averaged AAFE is attained.
To get estimates for the station networks  we select the hyperparameters of our method and BAGUS
by BIC and and plot the degree distributions of the estimated common station networks over three
years in Figure 2. From the common structure learned by our method  two stations are found to with
higher connectivity and identiﬁed as hubs. It turns out that one is close to Union Station (a major
transportation hub) and the other is close to Dupont Circle (a popular residential neighborhood).
Therefore  it is not surprising the two stations play an important role in the dependence graph.

Acknowledgment

This work is supported in part by grants NSF DMS-1916472 and NSF DMS-1811768.

References
[1] Banerjee  S. and Ghosal  S. (2015). Bayesian structure learning in graphical models. Journal of Multivariate

Analysis  136:147–162.

[2] Barabási  A.-L. and Albert  R. (1999). Emergence of scaling in random networks. Science  286(5439):509–

512.

[3] Cai  T.  Liu  W.  and Luo  X. (2011). A constrained (cid:96)1 minimization approach to sparse precision matrix

estimation. Journal of the American Statistical Association  106(494):594–607.

[4] Carvalho  C. M. and Scott  J. G. (2009). Objective Bayesian model selection in Gaussian graphical models.

Biometrika  96(3):497–512.

[5] Danaher  P.  Wang  P.  and Witten  D. M. (2014). The joint graphical lasso for inverse covariance estimation

across multiple classes. Journal of the Royal Statistical Society: Series B  76(2):373–397.

[6] Dempster  A. P. (1972). Covariance selection. Biometrics  28(1):157–175.

9

500015000250000.300.320.34Averaged AAFENumber of Nonzero Off−diagonal EntriesOur methodBAGUSPooledGGL05101520250204060DegreeCount050100150024DegreeCount[7] Dobra  A.  Lenkoski  A.  and Rodriguez  A. (2011). Bayesian inference for general Gaussian graphi-
cal models with application to multivariate lattice data. Journal of the American Statistical Association 
106(496):1418–1433.

[8] Fan  J. and Li  R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.

Journal of the American Statistical Association  96(456):1348–1360.

[9] Friedman  J.  Hastie  T.  and Tibshirani  R. (2008). Sparse inverse covariance estimation with the graphical

lasso. Biostatistics  9(3):432–441.

[10] Gan  L.  Narisetty  N. N.  and Liang  F. (2019). Bayesian regularization for graphical models with unequal

shrinkage. Journal of the American Statistical Association  114(527):1218–1231.

[11] George  E. I. and McCulloch  R. E. (1993). Variable selection via Gibbs sampling. Journal of the American

Statistical Association  88(423):881–889.

[12] Guo  J.  Levina  E.  Michailidis  G.  and Zhu  J. (2011). Joint estimation of multiple graphical models.

Biometrika  98(1):1–15.

[13] Lee  W. and Liu  Y. (2015). Joint estimation of multiple precision matrices with common structures. The

Journal of Machine Learning Research  16(1):1035–1062.

[14] Li  Z.  Mccormick  T.  and Clark  S. (2019). Bayesian joint spike-and-slab graphical lasso. In International

Conference on Machine Learning  pages 3877–3885.

[15] Loh  P.-L. and Wainwright  M. J. (2015). Regularized M-estimators with nonconvexity: Statistical and

algorithmic theory for local optima. Journal of Machine Learning Research  16:559–616.

[16] Loh  P.-L. and Wainwright  M. J. (2017). Support recovery without incoherence: A case for nonconvex

regularization. The Annals of Statistics  45(6):2455–2482.

[17] Ma  J. and Michailidis  G. (2016). Joint structural estimation of multiple graphical models. The Journal of

Machine Learning Research  17(1):5777–5824.

[18] Mazumder  R. and Hastie  T. (2012). The graphical lasso: New insights and alternatives. Electronic

Journal of Statistics  6:2125–2149.

[19] Mohammadi  A. and Wit  E. C. (2015). Bayesian structure learning in sparse Gaussian graphical models.

Bayesian Analysis  10:109–138.

[20] Narisetty  N. N. and He  X. (2014). Bayesian variable selection with shrinking and diffusing priors. The

Annals of Statistics  42(2):789–817.

[21] Peterson  C.  Stingo  F. C.  and Vannucci  M. (2015). Bayesian inference of multiple Gaussian graphical

models. Journal of the American Statistical Association  110(509):159–174.

[22] Ravikumar  P.  Wainwright  M. J.  Raskutti  G.  and Yu  B. (2011). High-dimensional covariance estimation

by minimizing (cid:96)1-penalized log-determinant divergence. Electronic Journal of Statistics  5:935–980.

[23] Roˇcková  V. and George  E. I. (2014). EMVS: The EM approach to Bayesian variable selection. Journal

of the American Statistical Association  109(506):828–846.

[24] Roˇcková  V. and George  E. I. (2018). The spike-and-slab lasso. Journal of the American Statistical

Association  113(521):431–444.

[25] Tan  L. S. L.  Jasra  A.  De Iorio  M.  and Ebbels  T. M. D. (2017). Bayesian inference for multiple
Gaussian graphical models with application to metabolic association networks. The Annals of Applied
Statistics  11(4):2222–2251.

[26] Wang  H. and Li  S. (2012). Efﬁcient Gaussian graphical model determination under G-Wishart prior

distributions. Electronic Journal of Statistics  6:168–198.

[27] Xu  X. and Ghosh  M. (2015). Bayesian variable selection and estimation for group lasso. Bayesian

Analysis  10(4):909–936.

[28] Yang  C.  Gan  L.  Wang  Z.  Shen  Jiaming  X.  Jinfeng  and Han  J. (2019). Query-speciﬁc knowledge
summarization with entity evolutionary networks. In The 28th ACM International Conference on Information
and Knowledge Management (CIKM).

10

[29] Yang  X. and Narisetty  N. N. (in press). Consistent group selection with Bayesian high dimensional

modeling. Bayesian Analysis.

[30] Yuan  M. and Lin  Y. (2007). Model selection and estimation in the Gaussian graphical model. Biometrika 

94(1):19–35.

[31] Zhang  C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of

Statistics  38(2):894–942.

[32] Zhu  Y. and Barber  R. F. (2015). The log-shift penalty for adaptive estimation of multiple Gaussian

graphical models. In International Conference on Artiﬁcial Intelligence and Statistics  pages 1153–1161.

11

,Lingrui Gan
Xinming Yang
Naveen Narisetty
Feng Liang