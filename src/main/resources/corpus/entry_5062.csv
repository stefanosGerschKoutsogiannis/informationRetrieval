2018,Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons,We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider range of perturbation models  including discrete ones. We give an application to recovering assemblies of neurons.
		
Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies.  This suggests that an animal's memory is a complex web of associations  and poses the problem of recovering this representation from cognitive data.  Motivated by this problem  we study the following more general question: Can we reconstruct the Venn diagram of a family of sets  given the sizes of their l-wise intersections? We show that as long as the family of sets is randomly perturbed  it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.,Smoothed Analysis of Discrete Tensor Decomposition

and Assemblies of Neurons

Nima Anari

Computer Science
Stanford University

anari@cs.stanford.edu

Constantinos Daskalakis

EECS
MIT

costis@csail.mit.edu

Wolfgang Maass

Theoretical Computer Science
Graz University of Technology

maass@igi.tugraz.at

Christos H. Papadimitriou

Computer Science
Columbia University

christos@cs.columbia.edu

Amin Saberi

MS&E

Stanford University

saberi@stanford.edu

Santosh Vempala
Computer Science

Georgia Tech

vempala@gatech.edu

Abstract

We analyze linear independence of rank one tensors produced by tensor powers
of randomly perturbed vectors. This enables efﬁcient decomposition of sums of
high-order tensors. Our analysis builds upon Bhaskara et al. [3] but allows for a
wider range of perturbation models  including discrete ones. We give an application
to recovering assemblies of neurons.
Assemblies are large sets of neurons representing speciﬁc memories or concepts.
The size of the intersection of two assemblies has been shown in experiments
to represent the extent to which these memories co-occur or these concepts are
related; the phenomenon is called association of assemblies. This suggests that
an animal’s memory is a complex web of associations  and poses the problem of
recovering this representation from cognitive data. Motivated by this problem  we
study the following more general question: Can we reconstruct the Venn diagram
of a family of sets  given the sizes of their `-wise intersections? We show that as
long as the family of sets is randomly perturbed  it is enough for the number of
measurements to be polynomially larger than the number of nonempty regions of
the Venn diagram to fully reconstruct the diagram.

1

Introduction

Tensor decomposition is one of the key algorithmic tools for learning many latent variable models
[1  5  14  19]. In practice  tensor decomposition methods based on gradient descent and power method
have been observed to work well [9  16]. Theoretically  determining the minimum number of rank one
components in the tensor decomposition is known to be NP-hard in the worst case [11  12]  so usually
tensor decomposition is analyzed in the average case. Several algorithms have been analyzed in the
average case  where the input tensor is produced according to some probabilistic model  for example
see Bhaskara et al. [3]  De Lathauwer et al. [7]  Goyal et al. [10] as well as sum-of-squares-based
algorithms like Barak et al. [2]  Ge and Ma [8]  Hopkins et al. [13]  Ma et al. [18].
The average case models studied in the literature generally fall into two categories. They either
assume components of the tensor are fully random  i.e.  generated from a known distribution (e.g. 
Gaussian)  or they follow a smoothed analysis setting where some adversarially chosen instance is
perturbed by random noise  see for example Bhaskara et al. [3]  Goyal et al. [10]  Ma et al. [18]. Our
work falls into the second category.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

We build upon the framework used in Bhaskara et al. [3] which reduces decomposing sums of rank
one tensors to showing robust linear independence of related rank one tensors  by using Jennrich’s
algorithm  also known as Chang’s lemma [5  17]. The main departing point of our work is our
smoothed analysis of linear independence  which we base on a new notion we call echelon trees  a
generalization of Gaussian elimination and echelon form to high-order tensors  which might be of
independent interest. We also get improved guarantees compared to Bhaskara et al. [3] when the
tensors are of high enough order.
The main feature of our analysis is that it can handle discrete perturbations. To illustrate  suppose that
vectors X1  . . .   Xm 2 Rn are drawn from some unknown distribution and our goal is to recover them
by (noisily) observingPi X⌦`
for small values of `. Bhaskara et al. [3] showed that up to constant
factor blow-ups in ` an efﬁcient algorithm can do this as long as X⌦`
are linearly independent in a
robust sense. Note that the set of vector tuples (X1  . . .   Xm) for which X⌦`
m are linearly
dependent can be deﬁned by polynomial equations  using determinants  and is therefore an algebraic
variety. As long as m ⌧ n`  this variety will have dimension smaller than the whole space  so we
expect most vector tuples to fall outside. Bhaskara et al. [3] showed that starting from an arbitrary set
of vectors X1  . . .   Xm  by adding Gaussian noise  the new tuple will lie far away from this variety.
Our analysis on the other hand  handles a much wider class of perturbations. For example  if each
Xi is independently chosen at random from a “large enough” discrete set such as the vertices of
an arbitrary hypercube  we show that with very high probability the resulting tensors are linearly
independent  again in a robust sense.
For our main application  described in the next section  it is important to assume components of the
tensor come from a discrete set.

1   . . .   X⌦`

i

i

1.1 Assemblies of neurons and recovering sparse Venn diagrams

Experiments by neuroscientists over the past three decades [21] have identiﬁed neurons which are
selectively activated when a real-world object1 is seen (or more generally sensed). It is now widely
accepted [4] that these neurons are part of large cell assemblies  stable sets of highly interconnected
neurons whose ﬁring (more or less simultaneous and in unison) is tantamount to a cognitive event
such as the sensing or imagining of a person  or of a word or concept (hence the other common name
“concept cells”).
In a recent experiment [15]  a neuron ﬁring when one real-world entity is seen (say  the Eiffel tower)
but not another (e.g.  Barak Obama) may start ﬁring on presentation of an image of Obama after a
visual experience associating the two — for example  a picture of Obama in front of the Eiffel tower.
This experiment has taught us that assemblies seem to be “mobile” and able to intersect in complex
ways reﬂecting perceived varying degrees of associations between the corresponding entities. The
stronger the association between the entities  the larger the intersection will be of the corresponding
assemblies. During one’s life  presumably a complex mesh of entities and associations will be created 
of some degree of permanence  reﬂecting the sum total of one’s cognitive experiences.
All said  this complex mesh of memories in somebody’s brain can be modeled as a Venn diagram
where each set or assembly consists of neurons ﬁring for a particular concept  and each region of
the Venn diagram  a minimal set obtained from an intersection of assemblies and their complements 
represents a class of neurons behaving the same way towards all concepts.
Alternatively to the Venn diagram  one may record associations between assemblies in a hypergraph.
The entities are the sets or nodes  and the edges reﬂect associations between the nodes. Furthermore 
the hypergraph representing a person’s state of knowledge can be adorned with edge weights reﬂecting
the degree of afﬁnity between a set of nodes (or equivalently  the size of the intersection of their
corresponding sets).
This gives rise to several natural questions. The ﬁrst question concerns reconstruction. How many
experiments or observations are needed to identify the structure of cell assembly intersections  or in
other words the Venn diagram? Here  we make two crucial assumptions. First  we assume that we
can only measure the degree of association between a small number of entities or concepts. Second 
the total number of classes of neurons (which behave similarly in response to stimuli) is bounded.
In the language of sets  we assume the number of non-empty regions of the Venn diagram is upper

1Or person  these are commonly known as “Jennifer Aniston neurons”.

2

1 +n

2 + ··· +n

bounded by some number m and we can measure the sizes of k-wise intersections of any k of our n
sets for 1  k  ` for some small `. We also allow for measurement errors.
Our main result here is as follows: As long as the cell assemblies are slightly randomly perturbed 
and as long as the number of measurements n

number of nonempty regions of the Venn diagram  m  we can fully reconstruct the Venn diagram.
The perturbation of cell assemblies  a process which likely occurs naturally in the brain  is a
mild assumption that we need in order to escape idiosyncratic cases. We solve the problem of
reconstructing the Venn diagram by casting it as a tensor decomposition problem where the elements
of the decomposition come from high order tensors of the vertices of the hypercube.
We also explore a simpler graph-theoretic model of assembly association  motivated by more recent
experimental ﬁndings [6  15]: Assume that all assemblies have the same size K  and that two
assemblies are associated if their intersection is of size at least b  and are not associated if the
intersection is less than another threshold a < b; the results of De Falco et al. [6]  Ison et al. [15]
suggest that a is 4% of K  while b is 8% of K. We show that an unreasonably rich and complex
family of graphs can be realized by associations (roughly  any graph of degree O(K/a)).

`  is polynomially larger than the

1.2 Problem formulation
Suppose that we have a Venn diagram formed by some n sets S1  . . .  Sn. We will assume that this
Venn diagram has at most m nonempty regions. For our main application  each set Si corresponds to
neurons that respond to a particular stimuli  so we are assuming that there are at most m classes of
neurons. We let U denote the set of neuron classes. We also have a weight function w : U! R0
representing the sizes of various classes. Each set Si ✓U is an assembly and w(Si) =Pu2Si
w(u)
is its weight. Our main question is the following:
Question 1. Given the sizes of `-wise intersections of S1  . . .  Sn for some constant `  i.e.  w(Si1 \
···\S i`) for all i1  . . .   i` 2 [n]  can we recover the full Venn diagram of S1  . . .  Sn  i.e.  the weight
of all intersections formed by these sets and their complements?

Our main result is that as long as the set memberships of elements are slightly perturbed to avoid
worst case scenarios  and as long as n` is polynomially larger than m = |U|  the answer is yes and
moreover there is an efﬁcient algorithm that performs recovery. Our algorithm is also robust to
inverse polynomial noise in the input.
We pose the question as a tensor decomposition problem in the following way: To each element
u 2U assign a vector (u) 2{ 0  1}n  where (u)i indicates whether u 2S i. Then the entries of
the following tensor capture all `-wise intersections:

w(u) (u) ⌦···⌦ (u)

.

T =Xu2U

|

` times

{z

}

For simplicity of exposition  we assume weights are all equal to 1  but our results easily generalize 
since each weight w(u) can be absorbed into (u)⌦`.

2 Notations and preliminaries
We denote the set {1  . . .   n} by [n]. For a matrix A  we denote the minimum and maximum singular
values of A by min(A) and max(A). We use h· ·i to denote the standard inner product.
We denote the tensor product of two vectors  2 Rn and 0 2 Rm by  ⌦ 0 which belongs to
Rn ⌦ Rm ' Rn⇥m. We use the notation ⌦` to denote
 ⌦···⌦ 
}

By abuse of notation we identify tensors T 2 Rn1 ⌦···⌦ Rn` with multilinear maps from Rn1 ⇥
···⇥ Rn` to R. In other words we let T (v1  . . .   v`) denote hT  v1 ⌦···⌦ v`i. We also use the
notation T (·  v2  . . .   v`) to denote the multilinear map from Rn1 to R given by:

{z

` times

|

.

T (·  v2  . . .   v`)(v1) = T (v1  . . .   v`).

3

In general we can use · in place of any of the arguments of T . So for example T (· ·  v3  . . .   v`) is
interpreted as living in Rn1 ⌦ Rn2. With a slight abuse of notation we let some of the inputs of T be
merged together by tensor operations. In other words we let T (v1 ⌦ v2  v3  . . .   v`) be the same as
T (v1  . . .   v`).
We use e1  . . .   en to denote the standard basis of Rn. For a tuple of coordinates I = (i1  . . .   i`) we
let eI denote ei1 ⌦···⌦ ei`. With this notation  the entry corresponding to coordinate (i1  . . .   i`) of
a tensor T can be written as T (eI) = T (ei1  . . .   ei`).

3 Tensor decomposition
Suppose that we have a ﬁnite universe U of elements with a vector (u) 2 Rn assigned to each u 2U .
Our goal is to recover (u)’s by observingPu (u)⌦`. A necessary condition is for (u)⌦`’s to be
linearly independent  otherwise it is an easy exercise to show that there is another decomposition
Pu(cu(u))⌦` for some positive weights {cu}u2U not all equal to 1. The framework introduced by

Bhaskara et al. [3] shows that linear independence is not just necessary  but up to a constant factor
blow-up in `  it is sufﬁcient. A more detailed account is given in supplementary materials.
We also use another trick from this framework which allows us to replace symmetric tensors (u)⌦`
with asymmetric ones. If we divide the coordinates [n] into ` roughly-equal sized parts I1  . . .   I`
and deﬁne (u)(i) to be the projection of (u) onto the i-th part  then (u)(1) ⌦···⌦ (u)` is a
subtensor of (u)⌦`. So linear independence of these tensors proves linear independence of (u)⌦`’s.
The advantage of this trick is that when we introduce perturbations to (u)(1)  . . .   (u)(`)  we do
not have to worry about consistently perturbing the same coordinates and we can potentially use
independent randomness. For simplicity of notation  from here on  we use n (as opposed to n/`) to
denote the dimension of each (u)(i). So now we can work with the following tensor:

T =Xu2U

(u)(1) ⌦···⌦ (u)(`).

Our main result is that the components of this sum are robustly linearly independent  assuming the
components (u)(i) are randomly perturbed. We remark that this implies robust linear independence

P[Xi 2 (t    t + ) | Xi]  p 

of {(u)⌦`}u2U as well  so we can recover them from the sumPu2U (u)⌦`.
We ﬁrst deﬁne our model of perturbations:
Deﬁnition 2. Assume that a vector X 2 Rd is drawn according to some distribution D. We call
D a (  p)-nondeterministic distribution if for every coordinate i 2 [d] and any interval of the form
(t    t + ) we have
where Xi represents the projection of X onto the coordinates [d] { i}.
For a set of random vectors {Xi}  we call their joint distribution (  p)-nondeterministic iff their
concatenation is (  p)-nondeterministic. In our setting  we will assume that for each u 2U   the
vectors (u)(1)  . . .   (u)(`) are chosen from a (  p)-nondeterministic distribution.
Two examples of (  p)-nondeterministic perturbations can be obtained as follows:
Example 3. Suppose that each (u)(i) is chosen adversarially from {0  1}n  but then each bit is inde-
pendently ﬂipped with some probability q. This distribution is ( 1
2   max(q  1  q))-nondeterministic.
Example 4. Suppose that each (u)(i) is chosen adversarially from Rn  but a standard Gaussian noise
of total variance ⇢2 is added to each one. Then for any > 0  this distribution is (  erf(pn/⇢))-
nondeterministic.
Gaussian perturbations are the model used in Bhaskara et al. [3]. Our main result is the following:
Theorem 5. Assume that for each u 2U   the concatenation of the n-dimensional vectors
{(u)(i)}i2[`] is drawn from a distribution D that is (  p)-nondeterministic. Let A be the ma-
trix whose columns are given by ﬂattened a(u) = (u)(1) ⌦···⌦ (u)` for various u. Then 
assuming |U|  (cn)`  we have

P[min(A) < (/n)`]  n2`p(1c)n.

4

This theorem shows how the (  p)-nondeterministic property ensures robust linear independence. To
prove it  we use a strategy similar to Bhaskara et al. [3]  by proving a bound on the leave-one-out
distance. The leave-one-out distance is closely related to min(A)  and only differs from it by a factor

of at mostp|U|  n`/2 [3]. It is enough to prove that for any ﬁxed u
dista(u)  span{a(u0)}u02U{u}  (/pn)`
with probability at least 1n`p(1c)n. Here dist measures the distance of a vector to the closet point in
a linear subspace. A union bound implies the leave-one-out distance for all u is large. As in Bhaskara
et al. [3]  we simplify the analysis by treating span{a(u0)}u02U{u} as a generic linear subspace
V ✓ (Rn)⌦`  and only using the fact that dim(V ) < (cn)`. Noting that n`  1+n+n2 +···+n`1 
it is enough to prove the following
Lemma 6. Assume that vectors (1)  . . .   (`) are drawn according to a (  p)-nondeterministic
distribution. Further assume that V ✓ (Rn)⌦` is a subspace of dimension at most (cn)`. Then
Phdist⇣(1) ⌦···⌦ (`)  V⌘ < (/pn)`i  (1 + n + n2 + ··· + n`1)p(1c)n.

In the rest of this section we prove lemma 6.
Let W = V ? ✓ (Rn)⌦` be the linear subspace of all tensors that vanish on V   or in other words
have zero dot product with every member of V . Then dim(W )  (1 c`)n`. We will show that with
high probability there is an element T 2 W such that kTk  n`/2 and

hT  (1) ⌦···⌦ (`)i = T ((1)  . . .   (`))  `.

This implies that

dist⇣(1) ⌦···⌦ (`)  V⌘ 

`
kTk

= n`/2` = (/pn)` 

and the proof would be complete.
We ﬁnd it instructive to ﬁrst prove this fact for ` = 1 and then for general `.

3.1 The case ` = 1
Proof of lemma 6 for ` = 1. We will generate a sequence T1  . . .   Tdim(W ) 2 W   such that kTik1 
1 for all i. This ensures that kTik  pn. We will then show that
P[9i : |Ti()| ]  1  p(1c)n.

(1)
We will ﬁrst pick T1 to be any nonzero element of W . By rescaling  we can assume that kT1k1 = 1
and that T1(ej) = 1 for some j. Let us call j the pivot point of T1. By rearranging the coordinates
we can assume without loss of generality that j = 1. In other words T1(e1) = 1 and kT1k1 = 1.
In order to pick T2  consider the subspace {T 2 W | T (e1) = 0}. This subspace has dimension at
least dim(W )  1  and we can pick T2 to be any nonzero element of it. As before  we can without
loss of generality and by scaling assume that T2(e2) = 1 and kT2k1 = 1.
When picking Ti  we pick any nonzero element of {T 2 W | T (ej) = 0 8j < i} and by rescaling
and rearranging the coordinates assume that Ti(ei) = 1 and kTik1 = 1. Thus we make sure that the
pivot point of Ti is i. A keen observer would notice that T1  . . .   Tdim(W ) can also be obtained by a
modiﬁed Gaussian elimination procedure run on some basis of the space W .
Now that we have ﬁxed T1  . . .   Tdim(W ) it remains to prove eq. (1).
To do this  let us ﬁx the coordinates of the random vector  = (1) one-by-one  starting from n and
going backwards to 1. Once we have ﬁxed dim(W )+1  . . .   n we can argue about the probability
of the event |Tdim(W )()| < . Since Tdim(W )(ei) = 0 for i < dim(W )  we have

Tdim(W )() = dim(W ) + Tdim(W )(edim(W )+1)dim(W )+1 + ··· + Tdim(W )(en)n.

But t := Tdim(W )(edim(W )+1)dim(W )+1 + ··· + Tdim(W )(en)n is a constant once we have ﬁxed
dim(W )+1  . . .   n. So |Tdim(W )()| < if and only if dim(W ) 2 (t   t + ). Because  is

5

distributed according to a (  p)-nondeterministic distribution  this event happens with probability at
most p. In other words

If this event does not occur  we are already done. Otherwise we can condition on dim(W )  . . .   n 
and look at the event |Tdim(W )1()| < . Once we condition on dim(W )  this event becomes
independent of the previous event and we can again upperbound its probability by p. So we have

P[|Tdim(W )()| < ]  p.

which implies

By continuing this  in the end we get
P[^dim(W )
which is the complement of eq. (1).

i=1

P[|Tdim(W )1()| < | |Tdim(W )()| < ]  p

P[|Tdim(W )1()| < ^| Tdim(W )()| < ]  p2.

|Ti()| < ]  pdim(W )  p(1c)n 

3.2 The general case

Here we describe a structure that we name echelon tree. This deﬁnition is motivated by the Gaussian
elimination procedure for matrices that produces an echelon form. Our deﬁnition can be seen as a
generalization of this form for tensor spaces.
We ﬁrst describe an index tree for Rn1⇥···⇥n`: Consider an abstract rooted tree T of height ` where
the nodes at level k are labeled by different partial indices from [n1] ⇥ [n2] ⇥···⇥ [nk]; the root has
the empty label and resides at level 0  and all leaves reside at level `. We require the indices to be
consistent with the tree structure  i.e.  all children (and by extension descendants) of a node labeled
I = (i1  . . .   ik) must contain I as the preﬁx of their label. We further assume that T is ordered  i.e. 
each node of T has an ordering over its children. This enables us to talk about post-order traversal of
the tree  a linear ordering of the nodes of the tree  which we denote by the binary relation . For two
nodes labeled I and J  we let I  J exactly when (i) I is a descendant of J or (ii) there are ancestors
I0  J0 of I  J with a common parent who places I0 before J0 (according to the ordering induced by
the parent on its children).
Deﬁnition 7. An index tree for Rn1⇥···⇥n` is a height ` tree T of partial indices together with a
post-traversal ordering  on its nodes as described above.
We emphasize that nodes of an index tree have different labels  so we consider the partial indices the
same as the nodes. For example  an index tree of height 1 is identical to an ordered list i(1)  . . .   i(s)
of elements in [n1]  with no repetitions allowed. Next we deﬁne an echelon tree.
Deﬁnition 8. An echelon tree is an index tree where each leaf I is additionally labeled by an element
TI 2 Rn1⇥···⇥n`. We require that TI(eI) 6= 0 and that for every node J that appears before I in the
post-order traversal  i.e.  J  I  the following identity to hold:

TI(eJ  ·  . . .  ·) = 0.

Note that the identity in the above deﬁnition is requiring an entire sub-array of TI to be zero. For
example a height 1 echelon tree is a list of unique indices i(1)  . . .   i(s) of [n1] together with vectors
T (1)  . . .   T (s) 2 Rn1 such that T (j) has zeros in the i(1)  . . .   i(j1) entries and has a nonzero i(j)-th
entry. Notice the similarity to the echelon form obtained by Gaussian elimination in a matrix. In
particular  for a height 1 echelon tree  the vectors T (1)  . . .   T (s) must be linearly independent.
We say that T is an echelon tree for the linear subspace W ✓ Rn1⇥···⇥n` if for all leaves I  we
have TI 2 W . Notice that we can collapse or ﬂatten consecutive levels of an echelon tree  and the
result would remain an echelon tree. In this operation  nodes of a particular level i are removed 
and each orphaned node of level i + 1 is assigned to its grandparent (of level i  1). We then treat
the indices as coming from [n1] ⇥···⇥ [nini+1] ⇥···⇥ [n`]  i.e.  we merge the i  i + 1-st level
indices. This also corresponds to partially ﬂattening tensors TI and considering them as elements of
Rn1⇥···⇥nini+1⇥...n`. It is easy to check that these operations preserve the properties in deﬁnition 8:
Fact 9. Collapsing an echelon tree at level i produces an echelon tree.

6

The main question we would like to address here is how large of an echelon tree can be constructed
for a subspace W . For example  for Rn1⇥···⇥n` one can get a full tree  where nodes at level i 1 have
branching factor ni  by simply placing the standard basis for Rn1⇥···⇥n` at the leaves. We measure
the size of a tree by its fractional branching factor.
Deﬁnition 10. An echelon tree T has fractional branching (↵1  . . .  ↵ `) 2 [0  1]` if each node I
at level i  1 has at least ↵ini children. For a single number ↵ 2 [0  1]  we say T has fractional
branching ↵ when it has fractional branching (↵  ↵  . . .   ↵).

Note that fractional branching ↵ implies that the tree has at least ↵`n1 . . . n` leaves. On the other
hand  repeated applications of fact 9 on the echelon tree would produce a height 1 echelon tree 
and we have already observed that the vectors assigned to the leaves in such a tree must be linearly
independent. So this implies that dim(W )  ↵`n1 . . . n`. There is a partial inverse to this statement:
If W ✓ Rn1⇥···⇥n` has dimension (1  c`) · n1 . . . n`  then there is an echelon tree with fractional
branching 1  c for W . However  this fact is not “robust”  since the elements of W assigned to the
leaves can have arbitrarily small or large entries. Instead we prove the following:
Theorem 11. If W ✓ Rn1⇥···⇥n` has dimension (1c`)·n1 . . . n`  then there is an echelon tree with
fractional branching 1  c for W such that for every leaf I we have kTIk1 = 1 and |TI(eI)| = 1.
Let us see ﬁrst see why theorem 11 is enough to prove lemma 6.

Proof of lemma 6 for general `. Note that kTIk1 = 1 implies that kTIk  n`/2. So it sufﬁces to
show that TI((1)  . . .   (`))  ` for some I with high probability.
Let us say that an echelon tree is x-large when |TI(eI)| x for all leaves I. Theorem 11 guarantees
that the echelon tree produced by it is 1-large.
Our strategy is to ﬁx (`)  (`1)  . . .   (1) in that order  and simultaneously reduce the height of our
echelon tree by 1 each time. When we ﬁx (`)  we can get a smaller echelon tree in the following way:
For each leaf I in the echelon tree  consider the reduced tensor TI(·  . . .   (`)) 2 Rn1⇥···⇥n`1 as a
candidate tensor for the parent of I. Now let J be a node of level `  1. Its children have produced
candidate tensors for J. Pick the candidate T with the highest |T (eJ )| to be TJ. In this way we have
removed the lowest level of the tree and have assigned appropriate tensors to the new leaves.
Our goal is to prove that if we start with an x-large echelon tree  then with high probability the next
echelon tree is x-large. Inductively this would prove that with high probability over the choice
of (1)  . . .   (`)  we have TI((1)  . . .   (`))  ` for some leaf I of the original echelon tree 
completing the proof.
For a ﬁxed node J of level `  1  we want to show that the quantity TI(eJ   (`)) is at least x in
magnitude for some child I of J. But this is very similar to the ` = 1 case of lemma 6  which we
have already proved. The difference is that the pivots are not necessarily equal to 1  but are at least x
in magnitude. This implies that

P[8I child of J : |TI(eJ   (`))| x]  p(1c)n.

The number of nodes at level `  1 is at most n`1  so by a union bound  we get that with probability
at least 1  n`1p(1c)n  the tree produced at the next level is x-large (the union bound is over
fewer than n`1 events  each corresponding to one J). Induction completes the proof.

Now we give a proof of theorem 11. We use induction to prove a stronger version. Theorem 11 will
be a corollary of the following by setting ↵1 = ··· = ↵` = 1  c.
Theorem 12. If W ✓ Rn1⇥···⇥n` is a subspace  and ↵1  . . .  ↵ ` 2 [0  1] are such that

(1  ↵1)(1  ↵2)··· (1  ↵`)  1  dim(W )/(n1 ··· n`) 

then there is an echelon tree for W with fractional branching (↵1  . . .  ↵ n) such that for each leaf I
we have kTIk1 = 1 and |TI(eI)| = 1.
Proof. We use induction on `. For the base case of ` = 1  we have ↵1  dim(W )/n1 and we want
an echelon tree with branching factor ↵1n1  dim(W ). We have already proved this case.

7

Now assume we have proved the statement for `  1 and want to prove it for `. Consider partially
ﬂattening the tensor space by merging the ﬁrst two dimensions  i.e.  considering W as a subspace
of Rn1n2⇥n3⇥···⇥n`. Let us ﬁx  2 [0  1] such that the premise of the induction hypothesis holds
and we can get an echelon tree of height `  1 with fractional branching (  ↵ 3  . . .  ↵ `). Nodes at
level 1 of this tree have indices in [n1n2]  and there are n1n2 of them. Considering these indices
as living in [n1] ⇥ [n2]  by the pigeonhole principle at least n1n2/n1 = n2 of them will have the
same ﬁrst component; let’s call this component i1 2 [n1]. We can now extract the subtrees of these
n2 elements and join them into an echelon tree of height `. The common parent of these nodes
will have index i1. So far we have constructed an echelon tree of height ` with fractional branching
(1/n1  ↵ 3  . . .  ↵ `).
Now consider the subspace {T 2 W | T (ei1 ·  . . .  ·) = 0}. We think of W as living in
R(n11)n2⇥n3⇥···⇥n`  since index i1 has been eliminated from the ﬁrst dimension. We can again
apply the induction hypothesis to this space and as long as the premise holds obtain an echelon tree
of height `  1 with fractional branching (  ↵ 3  . . .  ↵ `). We can apply the pigeonhole principle
again to ﬁnd (n1  1)n2/(n1  1) = n2 level-1 nodes having the same ﬁrst index i2. We extract
a height ` echelon tree from them and join this with the height ` echelon tree we already have. At the
end we will have an echelon tree with fractional branching (2/n1  ↵ 3  . . .  ↵ `).
Suppose we have repeated this procedure n1  1 many times and currently have a height ` echelon
tree with fractional branching (    ↵ 3  . . .  ↵ `). As long as the premise of the induction hypothesis
holds we can grow this echelon tree. The current subspace is {T 2 W | W (eij  ·  . . .  ·) = 0 for j 2
[n1]} which lives in R(1)n1n2⇥n3⇥···⇥n`. The dimension of this subspace is at least dim(W ) 
n1n2 ··· n`. So the premise of the induction hypothesis holds as long as

(1  )(1  ↵3)··· (1  ↵`)  1 

dim(W )  n1 ··· n`
(1  )n1n2 ··· n`

=

1  dim(W )/n1 ··· n`

.

1  

This means that as long as (1  )(1  )(1  ↵3)··· (1  ↵`)  1  dim(W )/n1 ··· n`  we can
grow the echelon tree.
To ﬁnish the proof  we set  = ↵2  which means that while <↵ 1  we can grow the echelon tree.
So when this procedure stops we have an echelon tree with fractional branching (↵1  . . .  ↵ `).

Implications for the main question

3.3
Our result  theorem 5  together with results from [3] (see the supplementary material)  imply that
under very mild assumptions we can recover S1  . . .  Sn from their `-wise intersections as long as
|U|  n⇥(`). These mild assumptions are necessary to prevent adversarially constructed examples
that have no hope of unique recovery.
To get a sense of the mild assumptions that we need  let us discuss the parameters that appear in
theorem 5. We assume that ` is a constant that does not grow with n. We can take c to be some ﬁxed
constant as well. For example 1/2  or even 1/ lp2. If we perturb our cell assemblies according to
example 3  i.e.  ﬂip assembly memberships for each neuron class and assembly pair with probability
q  how large of a q do we need for the conditions of theorem 5 and [3] to be satisﬁed? The distribution
we get for (u)s is going to be (1/2  1  q)-nondeterministic as long as q  1/2. So  = 1/2 is a
constant. The only condition we need is now for the failure probability to be small. This roughly
translates to

nO(`)(1  q)(1c)n ⌧ 1 

which will be satisﬁed for q = ⌦(log n/n). In other words  we only have to ﬂip each coordinate of
(u) with probability O(log n/n). On average  each neuron’s membership will be changed in about
O(log(n)) of the assemblies  which is a very small fraction of the assemblies. For slightly larger
values of q  e.g.  q = n✏1  the probability of failure becomes exponentially small similar to [3].
We also assumed that w(u) = 1 for all u 2U . In general this is not needed. As long as the weights
w(u) are in a range whose upper bound is at most a polynomially bounded factor larger than the
lower bound  we can absorb the weights into the vector (u) and the running time and accuracy will
only suffer by a polynomially bounded factor.
We also remark that recovering a {0  1}n vector within an additive error of 1/n is the same as exact
recovery (by rounding the coordinates). So by setting the recovery error (see supplementary material)
to 1/n we get exact recovery.

8

Finally  we remark that even though we are mostly interested in the case where ` = O(1)  our
dependencies on ` seem to be better than the results of [3] even in the setting of Gaussian perturbations.
In particular  our running time (as well as our tolerance for error) grows polynomially with n`  whereas
the running time of [3] grows with n3`. When adding Gaussian noise of total variance ⇢2 as in
example 4  we can treat our vectors as coming from a (O(⇢/pn)  1/2)-nondeterministic distribution.
This means our probability of failure will be at most n2`/2(1c)n. To have a fair comparison  we
need to allow for the number of components to be roughly half the total dimension  so we need to let

c = 1/ `p2 ' 1  ⇥(1/`). So the probability of failure will be roughly exp(O(` log n)  ⌦(n/`)).
For large enough values of ` this is much better than the guarantee of exp(⇥(n1/3`)) of [3].
4 Association graphs and the soft model

When the number of observations is smaller than what is needed for reconstruction  we can still ask
whether there exists some Venn diagram that is consistent with the observations. Which classes of
weighted graphs (or hypergraphs) can be represented by Venn diagrams?
Interestingly  a similar model was formulated almost three decades ago  motivated by quantum
mechanics and spin glass systems  and a mathematical object called correlation polytope was deﬁned
to frame that investigation [20]. It is not hard to show that membership in the polytope is an NP-hard
problem and natural optimization variants of it are hard to approximate.
In this section we formulate a promise version of the problem where either the intersection is above a
certain threshold (corresponding to association) or below another (corresponding to non-association)
which seems to be more tractable.
More precisely  we are given a graph that is unweighted. The nodes still stand for assemblies of
neurons  all of the same size K  out of a universe of N neurons  and the edges signify association;
the difference is that  in this model  if two assemblies are associated then they have an intersection of
size at least a; whereas if they are not  then their intersection is at most b. The intended relationship
between these numbers is that N is much larger than K (we take it to be a power of K)  and K
is in turn much larger than a  while a is quite a bit larger than b. To ﬁx ideas  in the sequel we
take N = K2 and b < a small constant fractions of K; in the experiment in [6  15] a and b are
found to be about 8% and 4% of K  respectively. We call a graph G = (V  E) representable with
parameters (N  K  a  b) if every node of G can be associated with a set of K neurons such that
for any two adjacent nodes the corresponding sets have intersection at least a  while for any two
non-adjacent nodes the corresponding sets have intersection at most b. The question is  which graphs
are representable?
Theorem 13. Any graph of maximum degree at most 2K/a is representable  and so is any tree of
maximum degree 2K2/a2.

The 2K/a bound follows from the fact that the edges of a regular Eulerian graph can be decomposed
into cycles  while the 2K2/a2 follows from the theory of block designs. Recalling that a is a small
fraction of K  we conclude that rather rich and complex “association graphs” can be represented
in principle. But can these sophisticated combinatorial constructions be carried out with surgical
precision in the wet chaos of the brain?
Here is a more realistic framework which we call
the soft model: Suppose that we are given an
association graph G = (V  E). We wish to determine whether a model of G exists  i.e.  |V | sets
corresponding to nodes of G whose pairwise intersections realize G according to the rules above
involving a and b. We wish to create sets of expected size K representing the nodes  starting from the
universe of neurons [N ] and executing instructions of the following form (in the following C  C1  C2
are previously constructed sets  and A is the set being constructed):
A C1  C2 

A C1 [ C2 

A C1 \ C2 

where by S(C  p) we denote the result of sampling each node in set C with probability p — a simple
and realistic enough primitive. The question is  which graphs can be realized in such a way that the
intended relations between the nodes and their intersections are not corrupted  with high enough
probability  by the randomness of the process? We can show the following:
Theorem 14. Any graph with maximum degree 1
probability.

a can be realized in the soft model with high

e · K

A S(C  p) 

9

References
[1] Animashree Anandkumar  Daniel Hsu  and Sham M Kakade. A method of moments for mixture

models and hidden markov models. In Conference on Learning Theory  pages 33–1  2012.

[2] Boaz Barak  Jonathan A Kelner  and David Steurer. Dictionary learning and tensor decom-
position via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM
symposium on Theory of computing  pages 143–151. ACM  2015.

[3] Aditya Bhaskara  Moses Charikar  Ankur Moitra  and Aravindan Vijayaraghavan. Smoothed
analysis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on
Theory of computing  pages 594–603. ACM  2014.

[4] György Buzsáki. Neural syntax: cell assemblies  synapsembles  and readers. Neuron  68(3):

362–385  2010.

[5] Joseph T Chang. Full reconstruction of markov models on evolutionary trees: identiﬁability

and consistency. Mathematical biosciences  137(1):51–73  1996.

[6] Emanuela De Falco  Matias J Ison  Itzhak Fried  and Rodrigo Quian Quiroga. Long-term coding
of personal and universal associations underlying the memory web in the human brain. Nature
communications  7:13408  2016.

[7] Lieven De Lathauwer  Josphine Castaing  and Jean-Franois Cardoso. Fourth-order cumulant-
based blind identiﬁcation of underdetermined mixtures. IEEE Transactions on Signal Processing 
55(6):2965–2973  2007.

[8] Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares

algorithms. arXiv preprint arXiv:1504.05287  2015.

[9] Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances

in Neural Information Processing Systems  pages 3656–3666  2017.

[10] Navin Goyal  Santosh Vempala  and Ying Xiao. Fourier pca and robust tensor decomposition. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing  pages 584–593.
ACM  2014.

[11] Johan Håstad. Tensor rank is np-complete. Journal of Algorithms  11(4):644–654  1990.
[12] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM

(JACM)  60(6):45  2013.

[13] Samuel B Hopkins  Tselil Schramm  Jonathan Shi  and David Steurer. Fast spectral algorithms
from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In Proceedings
of the forty-eighth annual ACM symposium on Theory of Computing  pages 178–191. ACM 
2016.

[14] Daniel Hsu  Sham M Kakade  and Tong Zhang. A spectral algorithm for learning hidden markov

models. Journal of Computer and System Sciences  78(5):1460–1480  2012.

[15] Matias J Ison  Rodrigo Quian Quiroga  and Itzhak Fried. Rapid encoding of new memories by

individual neurons in the human brain. Neuron  87(1):220–230  2015.

[16] Tamara G Kolda and Jackson R Mayo. Shifted power method for computing tensor eigenpairs.

SIAM Journal on Matrix Analysis and Applications  32(4):1095–1124  2011.

[17] SE Leurgans  RT Ross  and RB Abel. A decomposition for three-way arrays. SIAM Journal on

Matrix Analysis and Applications  14(4):1064–1083  1993.

[18] Tengyu Ma  Jonathan Shi  and David Steurer. Polynomial-time tensor decompositions with sum-
of-squares. In Foundations of Computer Science (FOCS)  2016 IEEE 57th Annual Symposium
on  pages 438–446. IEEE  2016.

[19] Elchanan Mossel and Sébastien Roch. Learning nonsingular phylogenies and hidden markov
models. In Proceedings of the thirty-seventh annual ACM symposium on theory of computing 
pages 366–375. ACM  2005.

10

[20] Itamar Pitowsky. Correlation polytopes: their geometry and complexity. Mathematical Pro-

gramming  50(1):395–414  1991.

[21] Rodrigo Quian Quiroga. Concept cells: the building blocks of declarative memory functions.

Nature reviews. Neuroscience  13(8):587  2012.

11

,Nima Anari
Constantinos Daskalakis
Wolfgang Maass
Christos Papadimitriou
Amin Saberi
Santosh Vempala