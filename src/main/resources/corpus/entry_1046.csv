2019,Complexity of Highly Parallel Non-Smooth Convex Optimization,A landmark result of non-smooth convex optimization is that gradient descent is an optimal algorithm whenever the number of computed gradients is smaller than the dimension $d$. In this paper we study the extension of this result to the parallel optimization setting. Namely we consider optimization algorithms interacting with a highly parallel gradient oracle  that is one that can answer $\mathrm{poly}(d)$ gradient queries in parallel. We show that in this case gradient descent is optimal only up to $\tilde{O}(\sqrt{d})$ rounds of interactions with the oracle. The lower bound improves upon a decades old construction by Nemirovski which proves optimality only up to $d^{1/3}$ rounds (as recently observed by Balkanski and Singer)  and the suboptimality of gradient descent after $\sqrt{d}$ rounds was already observed by Duchi  Bartlett and Wainwright. In the latter regime we propose a new method with improved complexity  which we conjecture to be optimal. The analysis of this new method is based upon a generalized version of the recent results on optimal acceleration for highly smooth convex optimization.,Complexity of Highly Parallel

Non-Smooth Convex Optimization

Sébastien Bubeck
Microsoft Research

sebubeck@microsoft.com

Qijia Jiang

Stanford University

qjiang2@stanford.edu

Yin Tat Lee

University of Washington

& Microsoft Research

yintat@uw.edu

Yuanzhi Li

Stanford University

yuanzhil@stanford.edu

Aaron Sidford

Stanford University

sidford@stanford.edu

Abstract

A landmark result of non-smooth convex optimization is that gradient descent is
an optimal algorithm whenever the number of computed gradients is smaller than
the dimension d. In this paper we study the extension of this result to the parallel
optimization setting. Namely we consider optimization algorithms interacting
with a highly parallel gradient oracle  that is one that can answer poly(d) gradient
queries in parallel. We show that in this case gradient descent is optimal only

up to eO(pd) rounds of interactions with the oracle. The lower bound improves

upon a decades old construction by Nemirovski which proves optimality only up to
d1/3 rounds (as recently observed by Balkanski and Singer)  and the suboptimality
of gradient descent after pd rounds was already observed by Duchi  Bartlett
and Wainwright. In the latter regime we propose a new method with improved
complexity  which we conjecture to be optimal. The analysis of this new method is
based upon a generalized version of the recent results on optimal acceleration for
highly smooth convex optimization.

1

Introduction

Much of the research in convex optimization has focused on the oracle model  where an algorithm
optimizing some objective function f : Rd ! R does so by sequential interaction with  e.g.  a
gradient oracle (given a query x 2 Rd  the oracle returns rf (x))  [Nemirovski and Yudin  1983 
Nesterov  2004  Bubeck  2015].1 In the early 1990s  Arkadi Nemirovski introduced the parallel
version of this problem [Nemirovski  1994]: instead of submitting queries one by one sequentially 
the algorithm can submit in parallel up to Q  1 queries. We refer to the depth of such a parallel
algorithm as the number of rounds of interaction with the oracle  and the work as the total number of
queries (in particular work  Q⇥ depth). In this paper we study the optimal depth achievable for
highly parallel algorithms  namely we consider the regime Q = poly(d). We focus on non-smooth
convex optimization  that is we want to optimize a Lipschitz  convex function f on the unit Euclidean
ball.

1Throughout we assume that f is differentiable  though our results carry over to the case where f is non-
differentiable and given by a sub-gradient oracle. This generalization is immediate as our analysis and algorithms
are stable under ﬁnite-precision arithmetic and convex functions are almost everywhere differentiable.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Our key result is a new form a quadratic acceleration: while for purely sequential methods the critical

1.1 Classical optimality results

Classically  when Q = 1  it is known that gradient descent’s query complexity is order optimal for

depth at which one can improve upon local search is eO(d)  we show that in the highly parallel regime
the critical depth is eO(pd).
any target accuracy " in the range⇥d1/2  1⇤. More precisely  it is known that the query complexity
of gradient descent is O(1/"2) and that for any " in the range⇥d1/2  1⇤  and for any algorithm 

there exists a Lipschitz and convex function f on which the number of oracle queries the algorithm
makes to achieve additive " accuracy is ⌦(1/"2). Furthermore  whenever " is smaller than d1/2
there exists a better algorithm (i.e.  with smaller depth)  namely the center of gravity whose depth is
O(d log(1/")). Consequently  an alternative formulation of these results is that  for Q = 1  gradient

for the exact statements.)

descent is order optimal if and only if the depth is smaller than eO(d). (See previously cited references

1.2 Optimality for highly parallel algorithms

The main result of this paper is to show that in the highly parallel regime (Q = poly(d))  gradient

descent is order optimal if and only if the depth is smaller than eO(pd). Thus one has a “quadratic"

improvement over the purely sequential setting in terms of the critical depth at which naive local
search becomes suboptimal.
The only if part of the above statement follows from Duchi et al. [2012]  where randomized smoothing
with accelerated gradient descent was proposed (henceforth referred to as distributed randomized
smoothing [Scaman et al.  2018])  and shown to achieve depth d1/4/"  which is order better than
1/"2 exactly when the latter is equal to pd. A ﬁrst key contribution of our work is a matching lower

descent is possible  i.e. Q = 1 and Q = poly(d) have essentially the same power. Importantly we
note that our lower bound applies to randomized algorithms. The previous state of the art lower

bound showing that  when the depth is smaller than eO(pd)  no signiﬁcant improvement over gradient
bound was that gradient descent is optimal up to depth eO(d1/3) [Balkanski and Singer  2018]. In

fact the construction in the latter paper is exactly the same as the original construction of Nemirovski
in [Nemirovski  1994] (however the ﬁnal statements are different  as Nemirovski was concerned
with an `1 setting instead of `2  see also Diakonikolas and Guzmán [2018] for more results about
non-Euclidean setups).
A second key contribution of this work is to improve the state of the art complexity of parallel
algorithms with depth between pd and d. Improving the depth d1/4/" of Duchi et al. [2012] was
explicitly mentioned as an open problem by Scaman et al. [2018]. Leveraging the recent higher order
acceleration schemes of Gasnikov et al. [2018]  Jiang et al. [2018]  Bubeck et al. [2018]  we propose
a new method with depth d1/3/"2/3. This means that for any value of " in the range⇥d1  d1/4⇤

there is an algorithm that is order better than both gradient descent and center of gravity. Moreover
we conjecture that the depth d1/3/"2/3 is in fact optimal for any " in this range (as the arguments
of Section 2.3 would imply this if a similar argument could be made for  = "  i.e. a smaller
walling radius). We leave this question  the optimality of the center of gravity method for small
"< 1/poly(d)   and the optimal work among optimal depth algorithms  for future works.

1.3 Related works

Though Nemirovski’s prescient work stood alone for decades  more recently the subﬁeld of paral-
lel/distributed optimization is booming  propelled by problems in machine learning  see e.g.  [Boyd
et al.  2011]. Chief among those problems is how to leverage mini-batches in stochastic gradient
descent as efﬁciently as possible [Dekel et al.  2012]. The literature on this topic is sprawling  see for
example [Duchi et al.  2018] which studies the total work achievable in parallel stochastic convex
optimization  or [Zhang and Xiao  2018] where the stochastic assumptions are leveraged to take
advantage of second order information. More directly related to our work is [Nemirovski  1994 
Diakonikolas and Guzmán  2018  Balkanski and Singer  2018] from the lower bound side (we directly

2

improve upon the result in the latter paper)  and [Duchi et al.  2012  Scaman et al.  2018] from the
upper bound side (we directly improve upon the depth provided by the algorithms in those works).

2 Lower bound

Fix "> 0 such that 1/"2 = eO(pd). In this section we construct a random function f such that 

for any deterministic algorithm with depth O(1/"2) and total work poly(d)  the output point x is
such that E[f (x)  f⇤] >"   where the expectation is with respect to the random function f  and
f⇤ denotes the minimum value of f on the unit centered Euclidean ball. Note that by the minimax
theorem  this implies that for any randomized algorithm there exists a deterministic function such
that the same conclusion applies. Formally  we prove the following:
Theorem 1 (Lower Bound) Let ⇢ 2 (0  1) and C = 12 + 4 logd(Q/⇢). Further  assume that it
4 (i.e.  N .qd/ log3(d)). Fix a randomized algorithm that
holds that log(N )NpC log(d)/d  1
queries at most Q points per iteration (both function value and gradient)  and that runs for at most
N iterations. Then  with probability at least 1  ⇢  when run on the shielded Nemirovski function f
(see Section 2.3 and Section 2.4)) one has for any queried point: f (x)  f⇤  1
4pN
The details of the proof of this theorem are deferred to Appendix A. In the remainder of this section
we instead provide a sketch of its proof. We ﬁrst recall in Section 2.1 why  for purely sequential
algorithms  the above statement holds true  and in fact one can even replace pd by d in this case (this
construction goes back to [Yudin and Nemirovski  1976]  see also [Nemirovski and Yudin  1983]).
Next  in Section 2.2 we explain Nemirovski [1994]’s construction  which yields a weaker version of
the above statement  with pd replaced by d1/3 (as rediscovered by [Balkanski and Singer  2018]).
We then explain in Section 2.3 our key new construction  a type of shielding operation. Finally  we
conclude the proof sketch in Section 2.4.
For the rest of the section we let v1  . . .   vN denote N random orthonormal vectors in Rd (in particular

.

i=1 vi. We deﬁne the Nemirovski function with parameter   0 by:

N  d)  and x⇤ =  1pNPN

Note that

N (x) = max

i2[N ]vi · x  i  
1
pN

.

N ⇤ N (x⇤)  

(1)

2.1 The classical argument
We consider here the Nemirovski function with parameter  = 0. Each gradient query reveals a single
vector in the collection of the vi  so after N/2 iterations one might know say v1  . . .   vN/2  but the rest
remain unknown (or in other words they remain random orthonormal vectors in span(v1  . . .   vN/2)?).
Thus for any output x that depends on only N/2 queries  one has E[N (x)]  0 (formally this
inequality follows from Jensen’s inequality and the tower rule). Thus  together with (1)  it follows
that E[N (x) N ⇤]  1/pN. In other words the best rate of convergence of sequential methods is
1/pN  provided that N  d.
2.2 The basic parallel argument

We consider here the Nemirovski function with parameter  = Cplog(d)/d for some large enough

constant C (more precisely that constant C depends on the exponent in the poly(d) number of
allowed queries per round). The key observation is as follows: Imagine that the algorithm has already
discovered v1  . . .   vi1. Then for any set of poly(d) queries  with high probability with respect to
the random draw of vi  . . .   vN  one has that the inner product of any of those vectors with any of the
queried points is in [/2 / 2] (using both basic concentration of measure on the sphere  and a union
bound). Thus the maximum in the deﬁnition of N is attained at some index  i. This means that this
set of poly(d) queries can only reveal vi  and not any of the vj  j > i. Thus after N  1 rounds we
know that with high probability any output x satisﬁes N (x)  vN · x  N  (N + 1) (since

3

vN is a random direction orthogonal to span(v1  . . .   vN1) and x only depends on v1  . . .   vN1).
Thus we obtain that the suboptimality gap is

1pN  (N + 1). Let us assume that

N 3/2 

1
2

 

(2)

i.e.  N = eO(d1/3) (since  = Cplog(d)/d). Then one has that the best rate of convergence with a

highly parallel algorithm is ⌦(1/pN ) (i.e.  the same as with purely sequential methods).

2.3 The wall function

Our new idea to improve upon Nemirovski’s construction is to introduce a new random wall function
W (with parameter > 0)  where the randomness come from v1  . . .   vN. Our new random hard
function  which we term shielded-Nemirovski function  is then deﬁned by:

f (x) = max{N (x) W(x)} .

We construct the convex function W so that one can essentially repeat the argument of Section
2.2 with a smaller value of  (the parameter in the Nemirovski function)  so that the condition (2)

becomes less restrictive and allows to take N as large as eO(pd).

Roughly speaking the wall function will satisfy the following properties:

1. The value of W at x⇤ is small  namely W(x⇤)   1pN .
2. The value of W at “most" vectors x with kxk   is large  namely W(x) N (x)  and
moreover it is does not depend on the collection vi (in fact at most points we will have the
simple formula W(x) = 2kxk1+↵  for some small ↵ that depends on   to be deﬁned later).
The key argument is that  by property 2  one can expect (roughly) that information on the random
collection of v0is can only be obtained by querying points of norm smaller than . This means that

tension between property 1 and 2  so that one cannot take  too small. We will show below that it

one can repeat the argument of Section 2.2 with a smaller value of   namely  =  · Cplog(d)/d.
In turn the condition (2) now becomes N = eOd1/3/2/3. Due to convexity of W  there is a
is possible to take  =pN/d. In turn this means that the argument proves that 1/pN is the best
possible rate  up to N = eO(pd).

The above argument is imprecise because the meaning of “most" in property 2 is unclear. A more
precise formulation of the required property is as follows:

20 Let x = w + z with w 2 Vi and z 2 V ?i where Vi = span(v1  . . .   vi). Assume
that kzk    then the total variation distance between the conditional distribution of
vi+1  . . .   vN given rW(x) (and W(x)) and the unconditional distribution is polynomially
small in d with high probability (here high probability is with respect to the realization of
rW(x) and W(x)  see below for an additional comment about such conditional reasoning).
Moreover if the argmax in the deﬁnition of N (x) is attained at some index > i  then
W(x) N (x).

Given both property 1 and 20 it is actually easy to formalize the whole argument. We do so by
consdering a game between Alice  who is choosing the query points  and Bob who is choosing the
random vectors v1  . . .   vN. Moreover  to clarify the reasoning about conditional distributions  Bob
will resample the vectors vi  . . .   vN at the beginning of the ith round of interaction  so that one
explicitly does not have any information about those vectors given the ﬁrst i 1 rounds of interaction.
Then we argue that with high probability all the oracle answers remain consistent throughout this
resampling process. See Appendix A for the details. Next we explain how to build W so as to satisfy
property 1 and 2’.

4

2.4 Building the wall
Let h(x) = 2kxk1+↵ be the basic building block of the wall. Consider the correlation cones:

Ci =(x 2 Rd :vi ·

x

d ) .
kxk  Cr log(d)

Note that for any ﬁxed query x  the probability (with respect to the random draw of vi) that x is in Ci
is polynomially small in d. We now deﬁne the wall W as follows: it is equal to the function h outside
of the correlation cones and the ball of radius   and it is extended by convexity to the rest of the unit
ball. In other words  let ⌦= {x 2 Rd : kxk 2 [  1] and x 62 Ci for all i 2 [N ]}  and

W(x) = max

y2⌦ {h(y) + rh(y) · (x  y)} .

+ 1pN

. Then W(x⇤)   1pN

.

y · x
kyk1↵ .

(3)

Let us ﬁrst prove property 1:

1

Lemma 2 Let ↵ =
Proof One has rh(y) = 2(1 + ↵)

log2(1/)  1  and



log2(1/) = 4Cq N log(d)
kyk1↵ and thus

d

y

Moreover for any y 2 ⌦ one has:
|y · x⇤|

h(y) + rh(y) · (x  y) = 2↵kyk1+↵ + 2(1 + ↵)
|y · vi| Cr N log(d)

1
pN

d

·k yk .

NXi=1

Thus for any y 2 ⌦ we have:

h(y) + rh(y) · (x⇤  y)  2↵1+↵ + 2(1 + ↵)Cr N log(d)

d

.

The proof is straightforwardly concluded by using the values of ↵ and .

max

a b2R+:a2+b22[2 1](2↵(a2 + b2)

Next we prove a simple formula for W(x) in the context of property 20. More precisely we assume
that x = w + z with w 2 Vi and z 2 V ?i with z 62 Cj for any j > i. Note that for any ﬁxed z  the
latter condition happens with high probability with respect to the random draw of vi+1  . . .   vN.
Lemma 3 Let x = w + z with w 2 Vi and z 2 V ?i with z 62 Cj for any j > i. Then one has:
W(x) =

y · w + bkzk!)  
wheree⌦= {x 2 Vi : x 62 Cj for all j 2 [i]} and we use the convention that the maximum of an
empty set is 1.
Proof Recall (3)  and let us optimize over y 2 ⌦ subject to kPViyk = a and kPV ?i
yk = b for some
a  b such that a2 + b2 2 [2  1]. Note that in fact there is an upper bound constraint on a for such a y
to exists (for if the projection of y onto Vi is large  then necessarily y must be in one of the correlation
cones)  which we can ignore thanks to the convention choice for the maximum of an empty set. Thus
the only calculation we have to do is to verify that:

2 max
y2e⌦ kyk=a

(a2 + b2) 1↵

2 + 2

1 + ↵

1+↵

max

y2⌦:kPVi yk=a and kPV ?i

yk=b

y · x = max
y2e⌦ kyk=a

y · w + bkzk .

Note that y · x = PViy · w + PV ?i
y · z. Thus the right hand side is clearly an upper bound on the
left hand side (note that PViy 2e⌦). To see that it is also a lower bound take y = y0 + b z
for some
arbitrary y0 2e⌦ with ky0k = a  and note that y 2 ⌦ (in particular using the assumption on z) with

kzk

5

yk = b.

kPViyk = a and kPV ?i
The key point of the formula given in Lemma 3 is that it does not depend on vi+1  . . .   vN. Thus
when the algorithm queries the point x and obtains the above value for W(x) (and the corresponding
gradient)  the only information that it obtains is that z 62 Cj for any j > i. Since the latter condition
holds with high probability  the algorithm essentially learns nothing (more precisely the conditional
distribution of vi+1  . . .   vN only changes by 1/poly(d) compared to the unconditional distribution).
Thus to complete the proof of property 20 it only remains to show that if kzk   and the argmax in
N (x) is attained at an index > i  then the formula in Lemma 3 is larger than N (x). By taking a = 0
and b =  one obtains that this formula is equal to (using also the values assigned to ↵ in Lemma 2):

2↵1+↵ + 2

1 + ↵
1↵ kzk = ↵ + (1 + ↵)kzk  kzk .

On the other hand one has (by assumption that the arg max index is > i)

N (x) = max

j>i {vj · x  j} k zk .

This concludes the proof of property 20  and in turn concludes the proof sketch of our lower bound.

3 Upper bound

Here we present our highly parallel optimization procedure. Throughout this section we let f :
Rd ! R denote a differentiable L-Lipschitz function that obtains its minimum value at x⇤ 2 Rd
with kx⇤k2  R. The main result of this section is the following theorem  which provides an
eO(d1/3/"2/3)-depth highly-parallel algorithm that computes an "-optimal point with high probability.
Theorem 4 (Highly Parallel Function Minimization) There is a randomized highly-parallel algo-
rithm which given any differentiable L-Lipschitz f : Rd ! R minimized at x⇤ with kx⇤k  R
computes with probability 1  ⌫ a point x 2 Rd with f (x)  f (x⇤)  " in depth eO(d1/3(LR/")2/3)
and work eO(d4/3(LR/")8/3) where eO(·) hides factors polylogarithmic in d " L R  and ⌫1.

Our starting point for obtaining this result are the O(d1/4/") depth highly parallel algorithms of
[Duchi et al.  2012]. This paper considers the convolution of f with simple functions  e.g. Gaussians
and uniform distributions  and shows this preserves the convexity and continuity of f while improving
the smoothness and thereby enables methods like accelerated gradient descent (AGD) to run efﬁciently.
Since the convolved function can be accessed efﬁciently in parallel by random sampling  working
with the convolved function is comparable to working with the original function in terms of query
depth (up to the sampling error). Consequently  the paper achieves its depth bound by trading off the
error induced by convolution with the depth improvements gained from stochastic variants of AGD.
To improve upon this bound  we apply a similar approach of working with the convolution of f
with a Gaussian. However  instead of applying standard stochastic AGD we consider accelerated
methods which build a more sophisticated model of the convolved function in parallel. Instead of
using random sampling to approximate only the gradient of the convolved function  we obtain our
improvements by using random sampling to glean more local information with each highly-parallel
query and then use this to minimize the convolved function at an accelerated rate.
To enable the use of these more sophisticated models we develop a general acceleration framework
that allows us to leverage any subroutine for approximate minimization a local model/approximate
gradient computations into an accelerated minimization scheme. We believe this framework is of
independent interest  as we show that we can analyze the performance of this method just in terms
of simple quantities regarding the local model. This framework is discussed in Section 3.1 and in
Appendix C where we show how it generalizes multiple previous results on near-optimal acceleration.
Using this framework  proving Theorem 4 reduces to showing that we can minimize high quality
local models of the convolved function. Interestingly  it is possible to nearly obtain this result by
simply random sampling to estimate all derivatives up to some order k and then use this to minimize
a regularized k-th order Taylor approximation to the function. Near-optimal convergence for such
methods under Lipschitz bounds on the k-th derivatives were recently given by [Gasnikov et al.  2018 
Jiang et al.  2018  Bubeck et al.  2018] (and follow from our framework). This approach can be shown

6

to give a highly-parallel algorithm of depth eO(d1/3+c/"2/3) for any c > 0 (with an appropriately

large k). Unfortunately  the work of these methods is O(dpoly(1/c)) and expensive for small c.
To overcome this limitation  we leverage the full power of our acceleration framework and instead
show that we can randomly sample to build a model of the convolved function accurate within a ball
of sufﬁciently large radius. In Section 3.2 we bound this quality of approximation and show that this
local model can be be optimized to sufﬁcient accuracy efﬁciently. By combining this result with our
framework we prove Theorem 4. We believe this demonstrates the utility of our general acceleration
scheme and we plan to further explore its implications in future work.

3.1 Acceleration framework

Here we provide a general framework for accelerated convex function minimization. Throughout
this section we assume that there is a twice-differentiable convex function g : Rd ! R given by an
approximate proximal step oracle and an approximate gradient oracle deﬁned as follows.

Deﬁnition 5 (Approximate Proximal Step Oracle) Let ! : R+ ! R+ be a non-decreasing func-
tion    0  and ↵ 2 [0  1). We call Tprox an (↵  )-approximate !-proximal step oracle for
g : Rd ! R if  for all x 2 Rd  when queried at x 2 Rd the oracle returns y = Tprox(x) such that
(4)

krg(y) + !(ky  xk)(y  x)k  ↵ · !(ky  xk)ky  xk + .

Deﬁnition 6 (Approximate Gradient Oracle) We call Tgrad an -approximate gradient oracle for
g : Rd ! R if when queried at x 2 Rd the oracle returns v = Tgrad(x) such that kv  rg(x)k  .
We show that there is an efﬁcient accelerated optimization algorithm for minimizing g using only
these oracles. Its performance is encapsulated by the following theorem.

Theorem 7 (Acceleration Framework) Let g : Rd ! R be a convex twice-differentiable function
minimized at x⇤ with kx⇤k  R  "> 0  ↵ 2 [0  1)  and   1 such that 128↵2  1. Further  let
! : R+ ! R+ be a monotonically increasing continuously differentiable function with 0 <! 0(s) 
 · !(s)/s for all s > 0. There is an algorithm which for all k computes a point yk with

g(yk)  g⇤  max(" 

32 · !⇣ 40kx⇤k

k3/2 ⌘kx⇤k2

)

k2

2 and Tprox(x) ⇡ arg miny g(y) + L

using k(6 + log2[10206R2 · !(1052R) · "1])2 queries to a (↵  )-approximate !-proximal step
oracle for g and a -approximate gradient oracle for g provided that both   "/[10202R] and
"  10204R3!(80R).
This theorem generalizes multiple accelerated methods (up to polylogarithmic factors) and sheds light
on the rates of these methods (See Appendix C for applications). For example  choosing !(x) def= L
2
and Tprox(x) = x  1
Lrf (x) recovers standard accelerated minimization of L-smooth functions 
2 ky  xk2 recovers a variant of approximate
choosing !(x) def= L
proximal point [Frostig et al.] and Catalyst [Lin et al.  2015]  and choosing !(x) def= Lp·(p+1)
xp1
and Tprox(x) = arg miny gp(y; x) + Lp
p! ky  xkp+1 where gp(y; x) is the value of the p’th order
Taylor approximation of g about x evaluated at y recovers highly smooth function minimization
[Monteiro and Svaiter  2013  Gasnikov et al.  2018  Jiang et al.  2018  Bubeck et al.  2018].
We prove Theorem 7 by generalizing an acceleration framework due to [Monteiro and Svaiter  2013].
This framework was recently used by several results to obtain near-optimal query complexities
for minimizing highly smooth convex functions [Gasnikov et al.  2018  Jiang et al.  2018  Bubeck
et al.  2018]. In Section B.1 we provide a variant of this general framework that is amenable to the
noise induced by our oracles. In Section B.2 we show how to instantiate our framework using the
oracles assuming a particular type of line search can be performed. In Section B.3 we then prove the
Theorem 7. The algorithm for and analysis of line search is deferred to Appendix E.

p!

7

3.2 Highly parallel optimization

With Theorem 7 in hand  to obtain our result we need to provide  for an appropriate function !  a
highly parallel implementation of an approximate proximal step oracle and an approximate gradient
oracle for a function that is an O(") additive approximation f. As with previous work [Duchi et al. 
2012  Scaman et al.  2018] we consider the convolution of f with a Gaussian of covariance r2 · Id for
r > 0 we will tune later. Formally  we deﬁne g : Rd ! R for all x 2 Rd as

g(x) def=ZRd

r(y)f (x  y)dy where r(x) def=

1

(p2⇡r)d

exp✓kxk2
2r2 ◆

It is straightforward to prove (See Section D.1) the following standard facts regarding g.

Lemma 8 The function g is convex  L-Lipschitz  and satisﬁes both |g(y)  f (y)| pd · Lr and
r2g(y)  (L/r) · Id for all y 2 Rd.
Consequently  to minimize f up to " error  it sufﬁces to minimize g to O(") error with r = O( "pdL
).
In the remainder of this section we simply show how to provide highly parallel implementations of
the requisite oracles to achieve this by Theorem 7.
Now  as we have discussed  one way we could achieve this goal would be to use random sampling
to approximate (in parallel) the k-th order Taylor approximation to g and minimize a regularization
of this function to implement the approximate proximal step oracle. While this procedure is depth-
efﬁcient  its work is quite large. Instead  we provide a more work efﬁcient application of our
acceleration framework. To implement a query to the oracles at some point c 2 Rd we instead simply
take multiple samples from r(x  c)  i.e.  the normal distribution with covariance r2Id and mean c 
and use these samples to build an approximation to the gradient ﬁeld of g. The algorithm for this
procedure is given by Algorithm 1 and carefully combines the gradients of the sampled points to
build a model with small bias and variance. By concentration bound and "-net argument  we can
show that Algorithm 1 outputs a vector ﬁeld v : Rd ! Rd that is an uniform approximation of rg
within a small ball (See Section D.2 for the proof.)
Algorithm 1: Compute vector ﬁeld approximating rg
1 Input: Number of samples N  radius r > 0  error parameter ⌘ 2 (0  1)  center c 2 Rd.
2 Sample x1  x2 ···   xN independently from r(x  c).
3 return v : Rd ! Rd deﬁned for all y 2 Rd by

v(y) =

1
N

NXi=1

r(y  xi)
r(c  xi) ·r f (xi) · ((xi  c)>(y  c)) · 1

kxick(pd+ 1
⌘ )r

where (t) def= 0  if |t| r2  (t) def= 1 if |t| r2
Lemma 9 (Uniform Approximation) Algorithm 1 outputs vector ﬁeld v : Rd ! R such that for
any  2 (0  1

2 and (t) def= 2  2|t|r2 otherwise.

2 ) with probability at least 1   the following holds
2⌘2◆ +
8L

4 r kv(y)  rg(y)k  5L · exp✓
Consequently  for any " 2 [0  1]  N = O([d log d log( 1

" ) + log( 1

y:kyck ⌘

max

1

pNs d

⌘2 log(9d) + log

1


.

yields that maxy:kycker kv(y)  rg(y)k  L · " whereer =

This lemma immediately yields that we can use Algorithm 1 to implement a highly-parallel approx-
imate gradient oracle for g. Interestingly  it can also be leveraged to implement a highly-parallel
approximate proximal step oracle. Formally  we show how to use it to ﬁnd y such that

 )]"2)  and ⌘ =
8qlog( 10

" )

.

r

this

1

2qlog( 10

" )

rg(y) + !(ky  xk) · (y  x) ⇡ 0 where !(s) def=

8

4Lsp

erp+1

(5)

for some p to be determined later. Ignoring logarithmic factors and supposing for simplicity that
times we could

achieve function error on the order

L  R  1  Theorem 7 shows that by invoking this procedure eO(k) ⇡ d

3p+4 " 2p2

3p+4

p+1

2 "(p+1)k 3p+4

2 ⇡ "

and therefore achieve the desired result by setting p to be polylogarithmic in the problem parameters.
Consequently  we simply need to ﬁnd y satisfying (5). The algorithm that achieves this is Algorithm 2
which essentially performs gradient descent on

p+1

2 ⇡ d

!(1/k3/2)/k2 ⇡er(p+1)k 3p+4
g(y) +( ky  ck) where (s) =Z s

0

!(t) · t dt .

(6)

The performance of this algorithm is given by Theorem 24 in Section D.3. Combined with all of the
above it proves Theorem 4  see Section D.4 for the details.
Algorithm 2: Approximate minimization of g(y) +( ky  ck)

r

1 Input: center c 2 Rd  accuracy "  inner radiuser =
48ppdL.
er
6.
2 Use Algorithm 1 to ﬁnd a vector ﬁeld v such that maxy:kycker kv(y)  rg(y)k  L · "
3 y c.
4 for i = 1  2 ···1 do
y = v(y) + !(ky  ck) · (y  c) where ! is deﬁned by (5) with p  1.
if kyk  L · 5"

6 then return y else y = y  h · y;

  and step size h =

8qlog( 60

" )

5
6
7 end

Acknowledgments
The authors thank the anonymous reviewers for their helpful feedback in preparing this ﬁnal version.
Further  the authors are grateful for multiple funding sources which supported this work in part 
including NSF Awards CCF-1740551  CCF-1749609  and DMS-1839116 and NSF CAREER Award
CCF-1844855.

References
E. Balkanski and Y. Singer. Parallelization does not accelerate convex optimization: Adaptivity lower

bounds for non-smooth convex minimization. Arxiv preprint arXiv:1808.03880  2018.

K. Ball. An elementary introduction to modern convex geometry. In S. Levy  editor  Flavors of

Geometry  pages 1–58. Cambridge University Press  1997.

S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R in Machine
Learning  3(1):1–122  2011.

S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine

Learning  8(3-4):231–357  2015.

S. Bubeck  Q. Jiang  Y.T. Lee  Y. Li  and A. Sidford. Near-optimal method for highly smooth convex

optimization. Arxiv preprint arXiv:1812.08026  2018.

O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction using

mini-batches. Journal of Machine Learning Research  13:165–202  2012.

J. Diakonikolas and C. Guzmán. Lower bounds for parallel and randomized convex optimization.

Arxiv preprint arXiv:1811.01903  2018.

J. Duchi  P. Bartlett  and M. Wainwright. Randomized smoothing for stochastic optimization. SIAM

Journal on Optimization  22(2):674–701  2012.

9

J. Duchi  F. Ruan  and C. Yun. Minimax bounds on stochastic batched convex optimization. In
Proceedings of the 31st Conference On Learning Theory (COLT)  volume 75 of Proceedings of
Machine Learning Research  pages 3065–3162  2018.

Roy Frostig  Rong Ge  Sham Kakade  and Aaron Sidford. Un-regularizing: approximate proximal
point and faster stochastic algorithms for empirical risk minimization. In Proceedings of the 32nd
International Conference on Machine Learning (ICML).

A. Gasnikov  E. Gorbunov  D. Kovalev  A. Mohhamed  and E. Chernousova. The global rate of conver-
gence for optimal tensor methods in smooth convex optimization. Arxiv preprint arXiv:1809.00382 
2018.

B. Jiang  H. Wang  and S. Zhang. An optimal high-order tensor method for convex optimization.

Arxiv preprint arXiv:1812.06557  2018.

B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann.
Statist.  28(5):1302–1338  10 2000. doi: 10.1214/aos/1015957395. URL https://doi.org/10.
1214/aos/1015957395.

Hongzhou Lin  Julien Mairal  and Zaïd Harchaoui. A universal catalyst for ﬁrst-order optimization. In
Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems (NIPS) 2015  pages 3384–3392  2015.

R. D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for convex
optimization and its implications to second-order methods. SIAM Journal on Optimization  23(2):
1092–1125  2013.

A. Nemirovski. On parallel complexity of nonsmooth convex optimization. Journal of Complexity 

10(4):451 – 463  1994.

A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley

Interscience  1983.

Y. Nesterov. Introductory lectures on convex optimization: A basic course. Kluwer Academic

Publishers  2004.

Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. Ann. Probab. 
22(4):1679–1706  10 1994. doi: 10.1214/aop/1176988477. URL https://doi.org/10.1214/
aop/1176988477.

K. Scaman  F. Bach  S. Bubeck  L. Massoulié  and Y.T. Lee. Optimal algorithms for non-smooth
distributed optimization in networks. In Advances in Neural Information Processing Systems 31 
pages 2740–2749. 2018.

D. Yudin and A. Nemirovski. Estimating the computational complexity of mathematica-programming

problems. Ekonomika i matem metody  12(1):128–142  1976. In Russian.

Y. Zhang and L. Xiao. Communication-efﬁcient distributed optimization of self-concordant empirical

loss. In Large-Scale and Distributed Optimization  pages 289–341. Springer  2018.

10

,Sebastien Bubeck
Qijia Jiang
Yuanzhi Li
Aaron Sidford