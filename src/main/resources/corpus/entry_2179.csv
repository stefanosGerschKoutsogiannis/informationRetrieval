2019,Fully Neural Network based Model for General Temporal Point Processes,A temporal point process is a mathematical model for a time series of discrete events  which covers various applications. Recently  recurrent neural network (RNN) based models have been developed for point processes and have been found effective. RNN based models usually assume a specific functional form for the time course of the intensity function of a point process (e.g.  exponentially decreasing or increasing with the time since the most recent event). However  such an assumption can restrict the expressive power of the model. We herein propose a novel RNN based model in which the time course of the intensity function is represented in a general manner. In our approach  we first model the integral of the intensity function using a feedforward neural network and then obtain the intensity function as its derivative. This approach enables us to both obtain a flexible model of the intensity function and exactly evaluate the log-likelihood function  which contains the integral of the intensity function  without any numerical approximations. Our model achieves competitive or superior performances compared to the previous state-of-the-art methods for both synthetic and real datasets.,Fully Neural Network based Model for General

Temporal Point Processes

Takahiro Omi

The University of Tokyo  RIKEN AIP

takahiro.omi.em@gmail.com

Naonori Ueda

NTT Communication Science Laboratories  RIKEN AIP

naonori.ueda.fr@hco.ntt.co.jp

Kazuyuki Aihara

The University of Tokyo

aihara@sat.t.u-tokyo.ac.jp

Abstract

A temporal point process is a mathematical model for a time series of discrete
events  which covers various applications. Recently  recurrent neural network
(RNN) based models have been developed for point processes and have been
found effective. RNN based models usually assume a speciﬁc functional form
for the time course of the intensity function of a point process (e.g.  exponentially
decreasing or increasing with the time since the most recent event). However  such
an assumption can restrict the expressive power of the model. We herein propose a
novel RNN based model in which the time course of the intensity function is rep-
resented in a general manner. In our approach  we ﬁrst model the integral of the
intensity function using a feedforward neural network and then obtain the intensity
function as its derivative. This approach enables us to both obtain a ﬂexible model
of the intensity function and exactly evaluate the log-likelihood function  which
contains the integral of the intensity function  without any numerical approxima-
tions. Our model achieves competitive or superior performances compared to the
previous state-of-the-art methods for both synthetic and real datasets.

1

Introduction

The activity of many diverse systems is characterized as a sequence of temporally discrete events.
The examples include ﬁnancial transactions  communication in a social network  and user activity
at a web site. In many cases  the occurrences of the event are correlated to each other in a certain
manner  and information on future events may be extracted from the information of past events.
Therefore  the appropriate modeling of the dependence of the event occurrence on the history of
past events is important for understanding the system and predicting future events.
A temporal point process is a useful mathematical tool for modeling the time series of discrete
events. In this framework  the dependence on the event history is characterized using a conditional
intensity function that maps the history of the past events to the intensity function of the point
process. The most common models  such as the Poisson process or the Hawkes process [1  2  3] 
assume a speciﬁc parametric form for the conditional intensity function. Recently  Du et al. (2016)
proposed a model based on a recurrent neural network (RNN) for point processes [4]  and the variant
models were further developed [5  6  7  8  9  10]. In this approach  an RNN is used to obtain a
compact representation of the event history. The conditional intensity function is then modeled as
a function of the hidden state of the RNN. Consequently  the RNN based models outperform the
parametric models in prediction performance.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Although such RNN based models aim to capture the dependence of the event occurrence on the
event history in a general manner  a speciﬁc functional form is usually assumed for the time course of
the conditional intensity function (see [6  7] for exception). For example  the model in [4] assumed
that the conditional intensity function exponentially decreases or increases with the elapsed time
from the most recent event until the next event. However  using such an assumption can limit
the expressive ability of the model and potentially deteriorate the predictive skill if the employed
assumption is incorrect. We herein generalize RNN based models such that the time evolution of the
conditional intensity function is represented in a general manner. For this purpose  we formulate the
conditional intensity function based on a neural network rather than assuming a speciﬁc functional
form.
However  exactly evaluating the log-likelihood function for such a general model is generally in-
tractable because the log-likelihood function of a temporal point process contains the integral of
the conditional intensity function. Although some studies  which considered a general model of
the intensity function  used numerical approximations to evaluate the integral [6  7]  numerical ap-
proximations can deteriorate the ﬁtting accuracy and can also be computationally expensive. To
overcome this limitation  we ﬁrst model the integral of the conditional intensity function using a
feedforward neural network rather than directly modeling the conditional intensity function itself.
Then  the conditional intensity function is obtained by differentiating it. This approach enables us to
exactly evaluate the log-likelihood function of our general model without numerical approximations.
Finally  we show the effectiveness of our proposed model by analyzing synthetic and real datasets.

2 Method

2.1 Temporal point process

A temporal point process is a stochastic process that generates a sequence of discrete events at times
ftign
i=1 in a given observation interval [0; T ]. The process is characterized via a conditional intensity
function (cid:21)(tjHt)  which is the intensity function of the event at the time t conditioned on the event
history Ht = ftijti < tg up to the time t  given as follows:

(cid:21)(tjHt) = lim
∆!0

P (one event occurs in [t; t + ∆)jHt)

∆

:

(1)

∫

{

If the conditional intensity function is speciﬁed  the probability density function of the time ti+1 of
the next event  given the times ft1; t2; : : : ; tig of the past events  is obtained as follows:

}
(cid:21)(tjHt)dt
}
where the exponential term in the right-hand side represents the probability that no events occur in
[ti; ti+1). The probability density function to observe an event sequence ftign
i=1 is then obtained as
follows:
(cid:21)(tjHt)dt

p(ti+1jt1; t2; : : : ; ti) = (cid:21)(ti+1jHti+1) exp
{

(cid:21)(tijHti) exp

p(ftign

i=1) =

∫

ti+1

(cid:0)

ti

;

(2)

:

(3)

(cid:0)

T

0

n∏

i=1

The most basic example of a temporal point process is a stationary Poisson process  which assumes
∑
that the events are independent of each other. The conditional intensity function of the stationary
Poisson process is given as (cid:21)(tjHt) = (cid:21). Another popular example is the Hawkes process [1  2  3] 
which is a simple model of a self-exciting point process. The conditional intensity function of the
ti<t g(t(cid:0)ti)  where g(s) is a kernel function (g(s) = 0
Hawkes process is given as (cid:21)(tjHt) = (cid:22)+
if s < 0) that represents the triggering effect from the past event.

2.2 Recurrent neural network approach to a temporal point process

The conditional intensity function  which maps the event history to the intensity function  plays
a major role in the modeling of point processes. Du et al.
(2016) proposed to use an RNN to
model the conditional intensity function [4]. In this approach  an input vector xi  which extracts
the information of the event time ti  is ﬁrst fed into the RNN. A simple form of the input is the

2

inter-event interval as xi = (ti (cid:0) ti(cid:0)1) or its logarithm as xi = (log(ti (cid:0) ti(cid:0)1)). A hidden state hi
of the RNN is updated as follows:

hi = f (W hhi(cid:0)1 + W xxi + bh);

(4)
where W h  W x  and bh denote the recurrent weight matrix  input weight matrix  and bias term 
respectively  and f is an activation function. We here treat the hidden state of the RNN as a compact
vector representation of the event history. The conditional intensity function is then formulated as a
function of the elapsed time from the most recent event and the hidden state of the RNN  given as
follows:

(cid:21)(tjHt) = ϕ(t (cid:0) tijhi);

(5)

where ϕ is a non-negative function referred to as a hazard function.
Du et al. (2016) assumed the following form for the hazard function [4] :

ϕ((cid:28)jhi) = exp(wt(cid:28) + vϕ (cid:1) hi + bϕ):

(6)
The exponential function in the above equation is used to ensure the non-negativity of the inten-
sity. In this model  the conditional intensity function exponentially decreases or increases with the
elapsed time (cid:28) from the most recent event until the next event.
A simpliﬁed model  where the conditional intensity function is constant over the period between the
successive events  was also considered in [8  10]. We here formulate such a model as a special case
of the model of eq. (6)  given as follows:

(7)
In this model  the inter-event interval (cid:28)i = ti+1 (cid:0) ti follows the exponential distribution with mean
1= exp(vϕ (cid:1) hi + bϕ).
The log-likelihood function of the RNN based model can be obtained from eq. (3) as follows:

ϕ((cid:28)jhi) = exp(vϕ (cid:1) hi + bϕ):
[
∫
∑
log ϕ(ti+1 (cid:0) tijhi) (cid:0)

ti+1(cid:0)ti

ϕ((cid:28)jhi)d(cid:28)

:

(8)

log L(ftig) =

]

i

0

The parameter values of the model are estimated by maximizing the log-likelihood function. For this
purpose  the backpropagation through time (BPTT) is employed to obtain the gradient of the log-
likelihood function. In the BPTT  the RNN is unfolded to a feedforward network  whose weights
are shared across layers  and the backpropagation is applied to the unfolded network. Although the
hidden state hi of the RNN originally depends on all the preceding events  fully considering the
history dependence for a long sequence can be problematic due to gradient vanishing or explosion.
Only the dependence on a ﬁxed number d of the most recent events is considered herein as was done
in [4]  where d is a hyperparameter called the truncation depth. Namely  for each time index i  the
hidden state hi is obtained by feeding the inputs fxjgi
j=i(cid:0)d+1 from the d most recent events to the
RNN.

2.3 Problem

The main problem of the previous studies is that a speciﬁc functional form is usually assumed for
the time course of the hazard function ϕ((cid:28)jhi) as in eqs. (6) or (7)  which can miss the general
dependence of the event occurrence on the past events. One may want to exploit a more complex
model for the hazard function ϕ((cid:28)jhi) to generalize the model. However  such a complex model
is generally intractable because the log-likelihood function in eq. (8) includes the integral of the
hazard function. Although the integral may be approximately evaluated using numerical methods
[6  7]  the numerical approximations can deteriorate the ﬁtting accuracy and be computationally
expensive. This is the main limitation in the ﬂexible modeling of the hazard function.

2.4 The proposed model 1

∫

Rather than directly modeling the hazard function  we herein propose to model the cumulative haz-
ard function (cid:8)((cid:28)jhi)  deﬁned as follows:

(cid:8)((cid:28)jhi) =

(cid:28)

ϕ(sjhi)ds:

(9)

1A source code is available online. https://github.com/omitakahiro/NeuralNetworkPointProcess

0

3

Figure 1: Network architecture to output the cumulative hazard function.

The hazard function itself can be then obtained by differentiating the cumulative hazard function
with respect to (cid:28) as follows:

The log-likelihood function is reformulated as follows using the cumulative hazard function:

[

∑

@
@(cid:28)

(cid:8)((cid:28)jhi):

ϕ((cid:28)jhi) =
{

}
(cid:8)((cid:28) = ti+1 (cid:0) tijhi)

log L(ftig) =

log

i

@
@(cid:28)

(10)

]

(cid:0) (cid:8)(ti+1 (cid:0) tijhi)

:

(11)

Now  the log-likelihood function does not include the integral term in contrast to eq. (8) and can be
exactly evaluated even for a complex model of the cumulative hazard function.
In the present study  we model the cumulative hazard function using a feedforward neural network (a
cumulative hazard function network; Fig. 1) for ﬂexible modeling. The cumulative hazard function
is a monotonically increasing function of (cid:28) and is positive-valued. The cumulative hazard func-
tion network is designed to reproduce these properties. The positivity of the network output can be
ensured using an output unit  in which the activation function is positive-valued. Considering mono-
tonicity  we employ the idea used in [11  12]. To summarize  the weights of the particular network
connections are constrained to be positive (Fig. 1).
The detail of the cumulative hazard function network is described below. In the network  each unit
receives the weighted sum of the inputs and applies an activation function to produce the output. The
ﬁrst hidden layer in the network receives the elapsed time (cid:28) and the hidden state hi of the RNN as
the inputs2. The weights of the connections from the elapsed time (cid:28) to the ﬁrst hidden layer and all
the connections from the hidden layers are constrained to be positive3. The connections  in which the
weights are constrained to be positive  are indicated by the red line in Fig. 1. The activation functions
of the hidden units and the output unit are set to be the tanh function and the sof tplus function 
log(1 + exp((cid:1)))  respectively. In this setting  the network output is monotonically increasing with
respect to the elapsed time (cid:28) and takes only a positive value  which mimics the cumulative hazard
function.
The cumulative hazard function (cid:8)((cid:28)jhi) and the hazard function ϕ((cid:28)jhi) are now formulated as
follows based on the output Zi((cid:28) ) of the cumulative hazard function network:

(cid:8)((cid:28)jhi) = Zi((cid:28) );
(cid:8)((cid:28)jhi) =

@
@(cid:28)

@
@(cid:28)

ϕ((cid:28)jhi) =

Zi((cid:28) ):

(12)

(13)

2We may input log (cid:28) to the ﬁrst layer rather than (cid:28) if the variation of the inter-event interval is large.
3In order to enforce the weights to be positive  if a weight is updated to be a negative value during training 

we replace it with its absolute value.

4

RNN!"#"$Φ$#"Cumulative Hazard Function Networkcumulativehazard function&"positive weight'"($)The differentiation term @Zi((cid:28) )=@(cid:28)  which is the derivative of the network output with respect to
the network input  is computed using automatic differentiation [13  14] (see Supplementary Material
for more details). Automatic differentiation is a method to calculate the derivative of an arbitrary
function  and it can be easily carried out using neural network libraries such as TensorFlow and
PyTorch. The log-likelihood function in eq. (11) of our model is then given based on the network
output via the eqs. (12) and (13). The gradient of the log-likelihood function with respect to the
parameters is obtained using backpropagation (see Supplementary Material for more details).
We note that our model can also efﬁciently generate a prediction of the timing of the coming event
(tjt1; t2; : : : ; ti) of the time ti+1
(cid:3)
in the following way. The predictive probability density function p
of the coming event given the past events ft1; t2; : : : ; tig is calculated from eq. (2). We herein use
(cid:3)
the median t
i+1  we use
(cid:0) tijhi) = log(2). This relation is derived from the property that the integral of
(cid:3)
the relation (cid:8)(t
i+1
the intensity function over [ti; ti+1] follows the exponential distribution with mean 1  or is derived
(cid:3)
i+1 can be efﬁciently obtained by solving the
by directly integrating eq. (2). Then  the median t
above relation using a root ﬁnding method (e.g.  the bisection method); it takes only a second for
our model to generate predictions for 20000 events. Therefore  the cumulative hazard function also
plays a crucial role in generating a median predictor.

(cid:3) to predict ti+1. To obtain the median t

(cid:3)
i+1 of the predictive distribution p

3 Related works

The RNN based point process models were proposed in [4]. Most previous studies assumed a
speciﬁc functional form for the time-course of the hazard function. The exponential hazard function
in eq. (6) is commonly assumed [4  9]. Some studies [8  10] assumed the constant hazard function
as in eq. (7)  which is equivalent to assume that the inter-event intervals follow the exponential
distribution. In contrast to these studies  our model does not assume any speciﬁc functional form for
the hazard function  and the time course of the hazard function is formulated in a general manner
based on a neural network.
A few studies addressed the general modeling of the hazard function. Jing and Somla (2017) pro-
posed to discretize the continuous hazard function to a piecewise constant function [6]  given as
follows:

ϕ((cid:28)jhi) = sof tplus(vp

j

(cid:1) hi + bp

j ) if (j (cid:0) 1)l < (cid:28) (cid:20) jl;

(14)

for j = 1; 2; : : : ; (cid:28)max=l for some choice of l and (cid:28)max. Mei and Eisner (2017) proposed a
continuous-time long short-term memory (LSTM) where the output continuously evolves in time 
and the conditional intensity function is given as a function of the output [7]. This model used the
Monte Carlo method to approximate the integral. In all cases  numerical approximations are used
to evaluate the integral in the log-likelihood function in eq. (8). However  numerical approxima-
tions can be computationally expensive and can also affect the ﬁtting accuracy. In contrast to these
studies  the log-likelihood function of our general model can be exactly evaluated without any nu-
merical approximations because the integral of the hazard function is modeled by a feedforward
neural network in our approach. Therefore  a more accurate estimate can be efﬁciently obtained by
our approach.

4 Experiments

In this section  we conduct experiments using synthetic and real data. We herein evaluate the predic-
tive performances of the four RNN based point process models. The number of units in the RNN is
ﬁxed to 64 for all the models. The ﬁrst two models are equipped with the constant hazard function
in eq. (7) (the constant model) and the exponential hazard function in eq. (6) (the exponential
model)  respectively. The third model employs the piecewise constant hazard function in eq. (14)
(the piecewise constant model). We set (cid:28)max to the maximum value of the inter-event interval in
each dataset and use the condition l = (cid:28)max=128. The fourth model employs the neural network
based hazard function proposed in this study (the neural network based model). For this model 
we use two hidden layers for the cumulative hazard function network  and the number of units in
each layer is 64. In this setting  the numbers of the parameters are almost the same between the third
and fourth models.

5

Each dataset is divided into the training and test data. In the training phase  the model parameters
are estimated using the training data. For this optimization  the Adam optimizer with the learning
rate 0:001  (cid:12)1 = 0:9  and (cid:12)2 = 0:999 is used [16]  and the batch size is 256. We also choose
the truncation depth d from [5; 10; 20; 40] using 20% of the training data (see Sec. 2.2 for more
details on truncation depth). In the test phase  we evaluate the predictive performances of the trained
(tjt1; t2; : : : ; ti) of
(cid:3)
models using the test data. In each time step  the probability density function p
the time of the coming event given the past events ft1; t2; : : : ; tig is calculated from eq. (2)  and
(ti+1jt1; t2; : : : ; ti) for the actually observed ti+1 (a
scored by the negative log-likelihood (cid:0) log p
(cid:3)
smaller score means a better predictive performance). The score is ﬁnally averaged over the events
in the test data. We performed these computations under a GPU environment provided by Google
Colaboratory.

4.1 Synthetic data

We use the synthetic data generated from the following stochastic processes. In this experiment 
100 000 events are generated from each process  and 80 000/20 000 events are used for train-
ing/testing.
Stationary Poisson Process (S-Poisson): The conditional intensity function is given as (cid:21)(tjHt) =
1.
Non-stationary Poisson Process (N-Poisson): The conditional intensity function is given as
(cid:21)(tjHt) = 0:99 sin(2(cid:25)t=20000) + 1.
Stationary Renewal Process (S-Renewal): In this process  the inter-event intervals f(cid:28)i = ti+1(cid:0)tig
are independent and identically distributed according to a given probability density function p((cid:28) ).
We herein use the log-normal distribution with a mean of 1.0 and a standard deviation of 6.0 for
p((cid:28) ). In this setting  a generated sequence exhibits a bursty behavior: multiple events tend to occur
in a short period and are followed by a long silence period like the burst ﬁring of biological neurons.
Non-stationary Renewal Process (N-Renewal): A sequence ftig following a non-stationary re-
newal process is obtained as follows [17]: we ﬁrst generate a sequence ft
g from a stationary re-
′
i
ti
newal process  and then we rescale the time according to t
0 r(t)dt for a non-negative trend
function r(t). We use the gamma distribution with a mean of 1.0 and a standard deviation of 0.5 to
generate the stationary renewal process and set the trend function to r(t) = 0:99 sin(2(cid:25)t=20000)+1.
In this process  an inter-event interval tends to be followed by the one with similar length  but the
∑
expected length gradually varies in time.
Self-correcting Process (SC): The conditional intensity function is given as (cid:21)(tjHt) = exp(t (cid:0)

′
i =

∫

ti<t 1).

M

ti<t

∑

∑
Hawkes Processes (Hawkes1 and Hawkes2): We use the Hawkes process  in which the kernel
function is given by the sum of multiple exponential functions: the conditional intensity function
is given by (cid:21)(tjHt) = (cid:22) +
j=1 (cid:11)j(cid:12)j expf(cid:0)(cid:12)j(t (cid:0) ti)g. For the Hawkes1 model  we set
M = 1; (cid:22) = 0:2; (cid:11)1 = 0:8; and (cid:12)1 = 1:0. For the Hawkes2 model  we set M = 2; (cid:22) = 0:2; (cid:11)1 =
0:4; (cid:12)1 = 1:0; (cid:11)2 = 0:4; and (cid:12)2 = 20:0. Compared to the Hawkes1 model  the kernel function of
the Hawkes2 model rapidly varies in time for small (cid:28).
In addition to the four RNN based models  we evaluate the predictive performance of the true model 
i.e.  the model that generated the data  and use it as a reference. The scores of the RNN based models
are standardized by subtracting the score of the true model. The value of 0 in the standardized score
corresponds to the score of the true model.
Figure 2 summarizes the performances of the four RNN based models for the synthetic datasets (a
smaller score means a better performance). We ﬁrst ﬁnd that the proposed neural network based
model achieves a competitive or better performance against performances of the other models. The
neural network based model also performs robustly for all the datasets: the performance of the neural
network based model is always close to that of the true model. These results demonstrate that (i) the
performance is improved by employing the neural network based hazard function and that (ii) our
model can be applicable to a diverse class of data generating processes.
The performances of the constant model and the exponential model critically depend on whether the
hazard function is correctly speciﬁed. The constant hazard function is correct for the S-Poisson and

6

Figure 2: Performances for the synthetic datasets. For each synthetic dataset  we evaluate the predic-
tive performances of the four RNN based models according to the negative log-likelihood averaged
per event calculated for the test data (the smaller is more preferable). Each score in the panel is
standardized by subtracting the score of the true model. Each error bar represents the 25% and 75%
percentiles of the score calculated for every 300 samples.

Figure 3: The conditional intensity functions estimated from the exponential model and the neural
network based model are compared with the true one. The yellow circles in the bottom of each panel
represent the events. The exponential hazard function is valid for the self-correcting process  but not
for the Hawkes2 process.

N-Poisson processes. The exponential hazard function is correct for the S-Poisson  N-Poisson  and
SC processes  and is approximately correct for the Hawkes1 process (a constant term is included
in the hazard function of the Hawkes1 process but not in the exponential hazard function). In fact 
these models perform similarly to the true model for the cases where the hazard function is correctly
speciﬁed but perform poorly for the other cases. Figure 3 shows the estimated conditional intensity
function and clearly demonstrates that the exponential model captures well the true conditional
intensity function for the self-correcting process where the exponential hazard function is valid;
however  it fails for the Hawkes2 process where the exponential hazard function is not valid. In
contrast  our neural network based model can reproduce well the true model for both cases. In this
manner  the constant and exponential models are sensitive to model misspeciﬁcation.
The performance of the piecewise constant model is much worse than the neural network based
model  particularly for the S-Renewal  N-Renewal  and Hawkes2 processes. For these processes 
the variability of the inter-event intervals is large  and the conditional intensity function can rapidly
vary for a short period after an event. For such cases  the piecewise constant approximation might
not work well. The performance of the piecewise constant model would be improved if the approx-
imation accuracy is improved; however  this increases the computational cost. In this experiment 
the numbers of the parameters are set to be almost the same between the piecewise constant model
and the neural network based model. Moreover  the neural network based model performs better

7

Figure 4: Performances for the real datasets. The score of each RNN based model is standardized
by subtracting the score of the neural network based model.

than the piecewise constant model  indicating that the neural network based model is more efﬁcient
than the piecewise constant model.

4.2 Real data

We use the following real datasets for the next experiment.
Finance dataset: This dataset contains the trading records of Nikkei 225 mini  which is the most
liquid features contracts in Asia [18]. The timestamps of 182 373 transactions in one day are ana-
lyzed  and the ﬁrst 80% and the last 20% of the data are used for training and testing  respectively.
Emergency call dataset: This dataset contains the records of the police department calls for
service in San Francisco [19]. Each record contains the timestamp and address from which the call
was made. We prepare 100 separate sequences for the 100 most frequent addresses  which contain
a total of 294 865 events. The ﬁrst 80% and the last 20% of the events in the sequences are used for
training and testing  respectively.
Meme dataset: MemeTracker tracks the popular phrases from numerous online resources such as
news media and personal blogs [20]. This dataset records the timestamps when the focused phrases
appear on the internet. We ﬁrst extract the 50 most frequent phrases and obtain the corresponding
50 separate sequences. We use 40 sequences out of 50 with 247 579 events for training and the
remaining 10 sequences with 61 095 events for testing.
Music dataset: This dataset records the history of music listening of users at https://www.last.
fm/ [21]. We prepare the 100 sequences for the 100 most active users in Jan 2009  which contain a
total of 299 046 events. The ﬁrst 80% and the last 20% of the events in the sequences are used for
training and testing  respectively.
Figure 4 summarizes the performances for the real datasets. We ﬁnd that the neural network based
model exhibits a competitive or superior score as compared to those of the other models; this demon-
strates the practical effectiveness of the proposed model. For the ﬁnance dataset  the performances
of all the models are close to each other  implying that the constant or exponential hazard function
reproduces the event occurrence process of the ﬁnancial transactions. The neural network based
model performs much better than the other models  particularly for the Meme and music datasets:
the difference in the scores between the neural network based model and the other models is greater
than 0.5. This difference should be signiﬁcant (e.g.  in the case of the right panel in Fig. 3  where
the exponential model clearly fails to reproduce the true model  the score difference is about 0.4 be-
tween the true and exponential models). For the two datasets  the data contain inter-event intervals
that are much longer than the average  and the variability of the inter-event intervals is large. Other
than the neural network based model  the models presumably fail to adapt to such a feature.

4.3 Time prediction experiment

To evaluate the predictive performances based on a metric other than the log-likelihood  we also
carry out the time prediction experiments. Speciﬁcally  we use the median of the predictive distri-

8

FinanceEmergencycallMemeMusic0.00.51.01.5average negative log-likelihood relative to the NN based modelpredicting skill for real dataconstantexponentialpiecewise constantneural networkbution to predict the timing of the coming event and evaluate the prediction by the mean absolute
error. The result is summarized in the table below. In the table  the best score is in bold  and it is
in red if the difference between the best score and the second best score is statistically signiﬁcant
(p < 0:01). We ﬁnd that our model performs better than the other models on average and that the
performance of our model is best or close to the best for the most individual datasets. These results
demonstrate the effectiveness of our model in the prediction task.

constant

exponential

piecewise const.
neural network

S-Poisson N-Poisson S-Renewal N-Renewal SC Hakes1 Hawkes2 Finance Call Meme Music AVERAGE

0.696
0.696
0.696
0.696

0.704
0.716
0.716
0.710

1.085
0.917
0.982
0.894

0.450
0.452
0.437
0.414

0.543 0.915
0.498 0.854
0.494 0.850
0.496 0.848

1.075
0.986
0.965
0.962

0.883 0.876 0.848 1.066
0.840 0.861 0.823 0.788
0.839 0.862 0.826 0.856
0.847 0.853 0.811 0.783

0.831
0.766
0.775
0.756

4.4 Comparison with the continuous-time LSTM model

We here compare our model with the continuous-time LSTM (CT-LSTM) model [7]. Both the two
models aim at ﬂexibly estimating the intensity function. The technical advantage of our model
over the CT-LSTM model is that the log-likelihood function can be exactly evaluated  therefore the
estimation can be carried out efﬁciently. Our model is also relatively easy to implement. For the
CT-LSTM model  the evaluation of the log-likelihood function is based on the Monte Carlo method 
which can be computationally expensive and can deteriorate the performance.
We evaluate the predictive performance of the CT-LSTM model in terms of the mean negative log-
likelihood (MNLL) and the mean absolute error (MAE). The number of the hidden units in the CT-
LSTM model is set to 42  so that the numbers of the free parameters are almost the same between the
CT-LSTM model and our model. The following table lists the scores of the CT-LSTM model relative
to our model; the positive score means that our model is better than the CT-LSTM model. In the
table  the score is in blue if our model is signiﬁcantly better than the CT-LSTM model (p < 0:01).
The performance of our model is better than the CT-LSTM model on average  demonstrating the
effectiveness of our model.

S-Poisson N-Poisson S-Renewal N-Renewal SC Hakes1 hawkes2 Finance Call Meme Music AVERAGE

MNLL -0.002
MAE
0.000

-0.013
-0.007

0.012
0.000

-0.007
0.042

-0.008 -0.002
0.016 0.002

0.002
0.005

-0.011 -0.017 0.590 0.163
-0.013 -0.002 -0.002 0.026

0.064
0.009

5 Discussion and Conclusions

In this study  we extended the RNN based point process models such that the time course of the
hazard function is represented in a general manner based on a neural network. We then showed the
effectiveness of our model by analyzing both synthetic and real datasets. Primary advantages of the
proposed model are summarized as follows:
(cid:15) By using a feedforward neural network  our model can reproduce any time course of the haz-
ard function in principle  i.e.  the usefulness of fully neural network based modeling for point
processes is indicated.
(cid:15) By modeling the cumulative hazard function rather than the hazard function itself  we can avoid
the direct evaluation of the integral in the log-likelihood function. The log-likelihood function
can be exactly and efﬁciently evaluated without relying on the numerical approximation because
of this approach.

We note that the cumulative hazard function also plays an important role in the diagnostic analysis
[22  23  24]. We did not consider herein the marks of each event (i.e.  the information associated
with each event other than the timestamp) because the primary contribution of this work is the de-
velopment of a general model of the hazard function. However  our approach can be easily extended
to marked temporal point processes as well.

Acknowledgments

This research was partly supported by AMED under Grant Number JP19dm0307009. T. O. and K.
A. are supported by Kozo Keikaku Engineering Inc.

9

References

[1] Alan G. Hawkes. Spectra of some self-exciting and mutually exciting point processes.

Biometrika  Volume 58  Pages 83-90  1971.

[2] Ke Zhou  Hongyuan Zha  and Le Song. Learning social infectivity in sparse low-rank net-
works using multi-dimensional hawkes processes. In Artiﬁcial Intelligence and Statistics 
pages 641-649  2013.

[3] Emmanuel Bacry  Iacopo Mastromatteo  and Jean-Francois Muzy. Hawkes processes in ﬁ-

nance. Market Microstructure and Liquidity  Volume 1  1550005  2015.

[4] Nan Du  Hanjun Dai  Rakshit Trivedi  Utkarsh Upadhyay  Manuel Gomez-Rodriguez  and
Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  pages 1555-1564. ACM  2016.

[5] Shuai Xiao  Junchi Yan  Xiaokang Yang  Hongyuan Zha  and Stephen M. Chu. Modeling
the intensity function of point process via recurrent neural networks. In Proceedings of the
Thirty-First AAAI Conference on Artiﬁcial Intelligence  pages 1598-1603. 2017.

[6] How Jing and Alexander J. Smola. Neural survival recommender. In Proceedings of the Tenth
ACM International Conference on Web Search and Data Mining (WSDM ’17)  Pages 515-
524  2017.

[7] Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems  pages
6757-6767  2017.

[8] Shuang Li  Shuai Xiao  Shixiang Zhu  Nan Du  Yao Xie  and Le Song. Learning temporal
point processes via reinforcement learning. In Advances in Neural Information Processing
Systems  2018.

[9] Utkarsh Upadhyay  Abir De  and Manuel Gomez Rodriguez. Deep reinforcement learning
of marked temporal point processes. In Advances in Neural Information Processing Systems 
2018.

[10] Hengguang Huang  Hao Wang  and Brian Mak. Recurrent poisson process unit for speech

recognition. In AAAI 2019.

[11] Joseph Sill. Monotonic networks. In Advances in Neural Information Processing Systems 

pages 661-667  1998.

[12] Pawel Chilinski and Ricardo Silva. Neural likelihoods via cumulative distribution functions.

arXiv:1811.00974  2018.

[13] Atilim Gunes Baydin  Barak A. Pearlmutter  Alexey Andreyevich Radul  and Jeffrey Mark
Siskind. Automatic differentiation in machine learning: a survey. Journal of Machine Learn-
ing Research  18  1-43  2018.

[14] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016.
[15] H. Drucker and Y. Le Cun. Improving generalization performance using double backpropa-

gation. IEEE Transactions on Neural Networks  3  991-997  1992.

[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv:1412.6980  2014.

[17] B. H. Lindqvist  G. Elvebakk  and K. Heggland. The trend-renewal process for statistical

analysis of repairable systems. Technometrics  Volume 45  Pages 31-44  2003.

[18] purchased from http://db-ec.jpx.co.jp
[19] https://catalog.data.gov/dataset/police-calls-for-service
[20] http://www.memetracker.org/data.html
[21] https://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.

html

[22] Yosihiko Ogata. Statistical models for earthquake occurrences and residual analysis for point

processes. Journal of the American Statistical association  Volume 83  Pages 9-27  1988.

10

[23] Emery N. Brown  Riccardo Barbieri  Valerie Ventura  Robert E. Kass  and Loren M. Frank.
The time-rescaling theorem and its application to neural spike train data analysis. Neural
computation  Volume 14  Pages 325-346  2002.

[24] Takahiro Omi  Yoshito Hirata  and Kazuyuki Aihara. Hawkes process model with a time-
dependent background rate and its application to high-frequency ﬁnancial data. Physical Re-
view E  96  012303  2017.

11

,Abbas Edalat
Takahiro Omi
naonori ueda
Kazuyuki Aihara