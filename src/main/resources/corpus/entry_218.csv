2014,A Statistical Decision-Theoretic Framework for Social Choice,In this paper  we take a statistical decision-theoretic viewpoint on social choice  putting a focus on the decision to be made on behalf of a system of agents. In our framework  we are given a statistical ranking model  a decision space  and a loss function defined on (parameter  decision) pairs  and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators  which minimize Bayesian expected loss  for the Mallows model and the Condorcet model respectively  and the Kemeny rule. We consider various normative properties  in addition to computational complexity and asymptotic behavior. In particular  we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity  neutrality  and monotonicity  can be computed in polynomial time  and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.,A Statistical Decision-Theoretic Framework for

Social Choice

Hossein Azari Souﬁani∗

David C. Parkes †

Lirong Xia‡

Abstract

In this paper  we take a statistical decision-theoretic viewpoint on social choice 
putting a focus on the decision to be made on behalf of a system of agents. In
our framework  we are given a statistical ranking model  a decision space  and a
loss function deﬁned on (parameter  decision) pairs  and formulate social choice
mechanisms as decision rules that minimize expected loss. This suggests a general
framework for the design and analysis of new social choice mechanisms. We
compare Bayesian estimators  which minimize Bayesian expected loss  for the
Mallows model and the Condorcet model respectively  and the Kemeny rule. We
consider various normative properties  in addition to computational complexity
and asymptotic behavior. In particular  we show that the Bayesian estimator for the
Condorcet model satisﬁes some desired properties such as anonymity  neutrality 
and monotonicity  can be computed in polynomial time  and is asymptotically
different from the other two rules when the data are generated from the Condorcet
model for some ground truth parameter.

1

Introduction

Social choice studies the design and evaluation of voting rules (or rank aggregation rules). There
have been two main perspectives: reach a compromise among subjective preferences of agents  or
make an objectively correct decision. The former has been extensively studied in classical social
choice in the context of political elections  while the latter is relatively less developed  even though
it can be dated back to the Condorcet Jury Theorem in the 18th century [9].
In many multi-agent and social choice scenarios the main consideration is to achieve the second
objective  and make an objectively correct decision. Meanwhile  we also want to respect agents’
preferences and opinions  and require the voting rule to satisfy well-established normative proper-
ties in social choice. For example  when a group of friends vote to choose a restaurant for dinner 
perhaps the most important goal is to ﬁnd an objectively good restaurant  but it is also important
to use a good voting rule in the social choice sense. Even for applications with less societal con-
text  e.g. using voting rules to aggregate rankings in meta-search engines [12]  recommender sys-
tems [15]  crowdsourcing [23]  semantic webs [27]  some social choice normative properties are still
desired. For example  monotonicity may be desired  which requires that raising the position of an
alternative in any vote does not hurt the alternative in the outcome of the voting rule. In addition 
we require voting rules to be efﬁciently computable.
Such scenarios propose the following new challenge: How can we design new voting rules with
good statistical properties as well as social choice normative properties?
To tackle this challenge  we develop a general framework that adopts statistical decision theory [3].
Our approach couples a statistical ranking model with an explicit decision space and loss function.
∗azari@google.com  Google Research  New York  NY 10011  USA. The work was done when the author
†parkes@eecs.harvard.edu  Harvard University  Cambridge  MA 02138  USA.
‡xial@cs.rpi.edu  Rensselaer Polytechnic Institute  Troy  NY 12180  USA.

was at Harvard University.

1

Anonymity  neutrality

Monotonicity

Majority 
Condorcet Consistency

Y

Y

Y

Y

N

N

N

N

N

Kemeny

Bayesian est. of
M1
ϕ (uni. prior)
Bayesian est. of
M2
ϕ (uni. prior)

Complexity

Min. Bayesian risk

NP-hard  PNP|| -hard
NP-hard  PNP|| -hard

(Theorem 3)
P (Theorem 4)
ϕ and M2

N

Y

Y

Table 1: Kemeny for winners vs. Bayesian estimators of M1

ϕ to choose winners.

ϕ is superior from a computational viewpoint.

Given these  we can adopt Bayesian estimators as social choice mechanisms  which make decisions
to minimize the expected loss w.r.t. the posterior distribution on the parameters (called the Bayesian
risk). This provides a principled methodology for the design and analysis of new voting rules.
To show the viability of the framework  we focus on selecting multiple alternatives (the alternatives
that can be thought of as being “tied” for the ﬁrst place) under a natural extension of the 0-1 loss
function for two models: let M1
ϕ denote the Mallows model with ﬁxed dispersion [22]  and let M2
ϕ
denote the Condorcet model proposed by Condorcet in the 18th century [9  34]. In both models the
dispersion parameter  denoted ϕ  is taken as a ﬁxed parameter. The difference is that in the Mallows
model the parameter space is composed of all linear orders over alternatives  while in the Condorcet
model the parameter space is composed of all possibly cyclic rankings over alternatives (irreﬂexive 
antisymmetric  and total binary relations). M2
ϕ is a natural model that captures real-world scenarios
where the ground truth may contain cycles  or agents’ preferences are cyclic  but they have to report
a linear order due to the protocol. More importantly  as we will show later  a Bayesian estimator on
M2
Through this approach  we obtain two voting rules as Bayesian estimators and then evaluate them
with respect to various normative properties  including anonymity  neutrality  monotonicity  the ma-
jority criterion  the Condorcet criterion and consistency. Both rules satisfy anonymity  neutrality 
and monotonicity  but fail the majority criterion  Condorcet criterion 1 and consistency. Admittedly 
the two rules do not enjoy outstanding normative properties  but they are not bad either. We also
investigate the computational complexity of the two rules. Strikingly  despite the similarity of the
two models  the Bayesian estimator for M2
ϕ can be computed in polynomial time  while computing
the Bayesian estimator for M1
ϕ is PNP|| -hard  which means that it is at least NP-hard. Our results are
summarized in Table 1.
We also compare the asymptotic outcomes of the two rules with the Kemeny rule for winners 
which is a natural extension of the maximum likelihood estimator of M1
ϕ proposed by Fishburn
[14]. It turns out that when n votes are generated under M1
ϕ  all three rules select the same winner
asymptotically almost surely (a.a.s.) as n → ∞. When the votes are generated according to M2
ϕ 
the rule for M1
ϕ still selects the same winner as Kemeny a.a.s.; however  for some parameters  the
winner selected by the rule for M2
ϕ is different with non-negligible probability. These are conﬁrmed
by experiments on synthetic datasets.
Related work. Along the second perspective in social choice (to make an objectively correct de-
cision)  in addition to Condorcet’s statistical approach to social choice [9  34]  most previous work
in economics  political science  and statistics focused on extending the theorem to heterogeneous 
correlated  or strategic agents for two alternatives  see [25  1] among many others. Recent work in
computer science views agents’ votes as i.i.d. samples from a statistical model  and computes the
MLE to estimate the parameters that maximize the likelihood [10  11  33  32  2  29  7]. A limitation
of these approaches is that they estimate the parameters of the model  but may not directly inform
the right decision to make in the multi-agent context. The main approach has been to return the
modal rank order implied by the estimated parameters  or the alternative with the highest  predicted
marginal probability of being ranked in the top position.
There have also been some proposals to go beyond MLE in social choice.
In fact  Young [34]
proposed to select a winning alternative that is “most likely to be the best (i.e.  top-ranked in the true
ranking)” and provided formulas to compute it for three alternatives. This idea has been formalized
and extended by Procaccia et al. [29] to choose a given number of alternatives with highest marginal

1The new voting rule for M1

ϕ fails them for all ϕ < 1/

2

√
2.

probability under the Mallows model. More recently  independent to our work  Elkind and Shah
[13] investigated a similar question for choosing multiple winners under the Condorcet model. We
will see that these are special cases of our proposed framework in Example 2. Pivato [26] conducted
a similar study to Conitzer and Sandholm [10]  examining voting rules that can be interpreted as
expect-utility maximizers.
We are not aware of previous work that frames the problem of social choice from the viewpoint
of statistical decision theory  which is our main conceptual contribution. Technically  the approach
taken in this paper advocates a general paradigm of “design by statistics  evaluation by social choice
and computer science”. We are not aware of a previous work following this paradigm to design
and evaluate new rules. Moreover  the normative properties for the two voting rules investigated in
this paper are novel  even though these rules are not really novel. Our result on the computational
complexity of the ﬁrst rule strengthens the NP-hardness result by Procaccia et al. [29]  and the
complexity for the second rule (Theorem 5) was independently discovered by Elkind and Shah [13].
The statistical decision-theoretic framework is quite general  allowing considerations such as estima-
tors that minimize the maximum expected loss  or the maximum expected regret [3]. In a different
context  focused on uncertainty about the availability of alternatives  Lu and Boutilier [20] adopt a
decision-theoretic view of the design of an optimal voting rule. Caragiannis et al. [8] studied the
robustness of social choice mechanisms w.r.t. model uncertainty  and characterized a unique social
choice mechanism that is consistent w.r.t. a large class of ranking models.
A number of recent papers in computational social choice take utilitarian and decision-theoretical
approaches towards social choice [28  6  4  5]. Most of them evaluate the joint decision w.r.t. agents’
subjective preferences  for example the sum of agents’ subjective utilities (i.e. the social welfare).
We don’t view this as ﬁtting into the classical approach to statistical decision theory as formulated
by Wald [30]. In our framework  the joint decision is evaluated objectively w.r.t. the ground truth in
the statistical model. Several papers in machine learning developed algorithms to compute MLE or
Bayesian estimators for popular ranking models [18  19  21]  but without considering the normative
properties of the estimators.

2 Preliminaries
In social choice  we have a set of m alternatives C = {c1  . . .   cm} and a set of n agents. Let
L(C) denote the set of all linear orders over C. For any alternative c  let Lc(C) denote the set
of linear orders over C where c is ranked at the top. Agent j uses a linear order Vj ∈ L(C) to
represent her preferences  called her vote. The collection of agents votes is called a proﬁle  denoted
by P = {V1  . . .   Vn}. A (irresolute) voting rule r : L(C)n → (2C \ ∅) selects a set of winners that
are “tied” for the ﬁrst place for every proﬁle of n votes.
For any pair of linear orders V  W   let Kendall(V  W ) denote the Kendall-tau distance between
V and W   that is  the number of different pairwise comparisons in V and W . The Kemeny rule
(a.k.a. Kemeny-Young method) [17  35] selects all linear orders with the minimum Kendall-tau dis-
tance from the preference proﬁle P   that is  Kemeny(P ) = arg minW Kendall(P  W ). The most
well-known variant of Kemeny to select winning alternatives  denoted by KemenyC  is due to Fish-
burn [14]  who deﬁned it as a voting rule that selects all alternatives that are ranked in the top
position of some winning linear orders under the Kemeny rule. That is  KemenyC(P ) = {top(V ) :
V ∈ Kemeny(P )}  where top(V ) is the top-ranked alternative in V .
Voting rules are often evaluated by the following normative properties. An irresolute rule r satisﬁes:
• anonymity  if r is insensitive to permutations over agents;
• neutrality  if r is insensitive to permutations over alternatives;
• monotonicity  if for any P   c ∈ r(P )  and any P (cid:48) that is obtained from P by only raising the
positions of c in one or multiple votes  then c ∈ r(P (cid:48));
• Condorcet criterion  if for any proﬁle P where a Condorcet winner exists  it must be the unique
winner. A Condorcet winner is the alternative that beats every other alternative in pair-wise elections.
• majority criterion  if for any proﬁle P where an alternative c is ranked in the top positions for more
than half of the votes  then r(P ) = {c}. If r satisﬁes Condorcet criterion then it also satisﬁes the
majority criterion.
• consistency  if for any pair of proﬁles P1  P2 with r(P1)∩r(P2) (cid:54)= ∅  r(P1∪P2) = r(P1)∩r(P2).

3

For any proﬁle P   its weighted majority graph (WMG)  denoted by WMG(P )  is a weighted directed
graph whose vertices are C  and there is an edge between any pair of alternatives (a  b) with weight
wP (a  b) = #{V ∈ P : a (cid:31)V b} − #{V ∈ P : b (cid:31)V a}.
A parametric model M = (Θ S  Pr) is composed of three parts: a parameter space Θ  a sample
space S composing of all datasets  and a set of probability distributions over S indexed by elements
of Θ: for each θ ∈ Θ  the distribution indexed by θ is denoted by Pr(·|θ).2
Given a parametric model M  a maximum likelihood estimator (MLE) is a function fMLE : S → Θ
such that for any data P ∈ S  fMLE(P ) is a parameter that maximizes the likelihood of the data.
That is  fMLE(P ) ∈ arg maxθ∈Θ Pr(P|θ).
In this paper we focus on parametric ranking models. Given C  a parametric ranking model MC =
(Θ  Pr) is composed of a parameter space Θ and a distribution Pr(·|θ) over L(C) for each θ ∈
(cid:81)
Θ  such that for any number of voters n  the sample space is Sn = L(C)n  where each vote is
generated i.i.d. from Pr(·|θ). Hence  for any proﬁle P ∈ Sn and any θ ∈ Θ  we have Pr(P|θ) =
V ∈P Pr(V |θ). We omit the sample space because it is determined by C and n.
(cid:81)
Deﬁnition 1 In the Mallows model [22]  a parameter is composed of a linear order W ∈ L(C)
and a dispersion parameter ϕ with 0 < ϕ < 1. For any proﬁle P and θ = (W  ϕ)  Pr(P|θ) =

Z ϕKendall(V W )  where Z is the normalization factor with Z =(cid:80)

V ∈L(C) ϕKendall(V W ).

1

V ∈P

Statistical decision theory [30  3] studies scenarios where the decision maker must make a decision
d ∈ D based on the data P generated from a parametric model  generally M = (Θ S  Pr). The
quality of the decision is evaluated by a loss function L : Θ×D → R  which takes the true parameter
and the decision as inputs.
In this paper  we focus on the Bayesian principle of statistical decision theory to design social
choice mechanisms as choice functions that minimize the Bayesian risk under a prior distribution
over Θ. More precisely  the Bayesian risk  RB(P  d)  is the expected loss of the decision d when
the parameter is generated according to the posterior distribution given data P . That is  RB(P  d) =
Eθ|P L(θ  d). Given a parametric model M  a loss function L  and a prior distribution over Θ  a
(deterministic) Bayesian estimator fB is a decision rule that makes a deterministic decision in D
to minimize the Bayesian risk  that is  for any P ∈ S  fB(P ) ∈ arg mind RB(P  d). We focus on
deterministic estimators in this work and leave randomized estimators for future research.
Example 1 When Θ is discrete  an MLE of a parametric model M is a Bayesian estimator of the
statistical decision problem (M D = Θ  L0-1) under the uniform prior distribution  where L0-1 is
the 0-1 loss function such that L0-1(θ  d) = 0 if θ = d  otherwise L0-1(θ  d) = 1.

In this sense  all previous MLE approaches in social choice can be viewed as the Bayesian estimators
of a statistical decision-theoretic framework for social choice where D = Θ  a 0-1 loss function  and
the uniform prior.

3 Our Framework
Our framework is quite general and ﬂexible because we can choose any parametric ranking model 
any decision space  any loss function  and any prior to use the Bayesian estimators social choice
mechanisms. Common choices of both Θ and D are L(C)  C  and (2C \ ∅).
Deﬁnition 2 A statistical decision-theoretic framework for social choice is a tuple F =
(MC D  L)  where C is the set of alternatives  MC = (Θ  Pr) is a parametric ranking model 
D is the decision space  and L : Θ × D → R is a loss function.
Let B(C) denote the set of all irreﬂexive  antisymmetric  and total binary relations over C. For
any c ∈ C  let Bc(C) denote the relations in B(C) where c (cid:31) a for all a ∈ C − {c}. It follows
that L(C) ⊆ B(C)  and moreover  the Kendall-tau distance can be deﬁned to count the number of
pairwise disagreements between elements of B(C).
In the rest of the paper  we focus on the following two parametric ranking models  where the disper-
sion is a ﬁxed parameter.

2This notation should not be taken to mean a conditional distribution over S unless we are taking a Bayesian

point of view.

4

Deﬁnition 3 (Mallows model with ﬁxed dispersion  and the Condorcet model) Let M1
ϕ denote
the Mallows model with ﬁxed dispersion  where the parameter space is Θ = L(C) and given any
W ∈ Θ  Pr(·|W ) is Pr(·|(W  ϕ)) in the Mallows model  where ϕ is ﬁxed.
In the Condorcet model  M2

(cid:0) 1
Z ϕKendall(V W )(cid:1)  where Z is the normalization factor such that

ϕ  the parameter space is Θ = B(C). For any W ∈ Θ and any proﬁle
V ∈P

P   we have Pr(P|W ) = (cid:81)
Z =(cid:80)

V ∈B(C) ϕKendall(V W )  and parameter ϕ is ﬁxed.3

ϕ for any ϕ.

ϕ = (M1

ϕ and M2

ϕ  2C \ ∅  Ltop) and F 2

ϕ = (M2
B (respectively  f 2

ϕ degenerate to the Condorcet model for two alternatives [9]. The Kemeny rule that

M1
selects a linear order is an MLE of M1
We now formally deﬁne two statistical decision-theoretic frameworks associated with M1
ϕ and M2
ϕ 
which are the focus of the rest of our paper.
Deﬁnition 4 For Θ = L(C) or B(C)  any θ ∈ Θ  and any c ∈ C  we deﬁne a loss function Ltop(θ  c)
such that Ltop(θ  c) = 0 if for all b ∈ C  c (cid:31) b in θ; otherwise Ltop(θ  c) = 1.
(cid:80)
Let F 1
c∈C Ltop(θ  c)/|C|. Let f 1
F 2
ϕ) under the uniform prior.
We note that Ltop in the above deﬁnition takes a parameter and a decision in 2C \ ∅ as inputs  which
makes it different from the 0-1 loss function L0-1 that takes a pair of parameters as inputs  as the
B are not the MLEs of their respective models  as was the
one in Example 1. Hence  f 1
case in Example 1. We focus on voting rules obtained by our framework with Ltop. Certainly our
framework is not limited to this loss function.
Example 2 Bayesian estimators f 1
B coincide with Young [34]’s idea of selecting the al-
B and f 2
ternative that is “most likely to be the best (i.e.  top-ranked in the true ranking)”  under F 1
ϕ and
F 2
ϕ respectively. This gives a theoretical justiﬁcation of Young’s idea and other followups under
B was
our framework. Speciﬁcally  f 1
independently studied by Elkind and Shah [13].

ϕ  2C \ ∅  Ltop)  where for any C ⊆ C  Ltop(θ  C) =
B) denote the Bayesian estimators of F 1
ϕ (respectively 

B is similar to rule studied by Procaccia et al. [29] and f 2

B and f 2

4 Normative Properties of Bayesian Estimators
All omitted proofs can be found in the full version on arXiv.
Theorem 1 For any ϕ  f 1
majority or the Condorcet criterion for any ϕ < 1√
2

 4 and it does not satisfy consistency.

B satisﬁes anonymity  neutrality  and monotonicity. f 1

B does not satisfy

Proof sketch: Anonymity and neutrality are obviously satisﬁed.
Monotonicity. Monotonicity follows from the following lemma.
Lemma 1 For any c ∈ C  let P (cid:48) denote a proﬁle obtained from P by raising the position of c in
one vote. For any W ∈ Lc(C)  Pr(P (cid:48)|W ) = Pr(P|W )/ϕ; for any b ∈ C and any V ∈ Lb(C) 
Pr(P (cid:48)|V ) ≤ Pr(P|V )/ϕ.
Majority and the Condorcet criterion. Let C = {c  b  c3  . . .   cm}. We construct a proﬁle P ∗
where c is ranked in the top positions for more than half of the votes  but c (cid:54)∈ f 1
For any k  let P ∗ denote a proﬁle composed of k copies of [c (cid:31) b (cid:31) c3 (cid:31) ··· (cid:31) cm]  1 of
[c (cid:31) b (cid:31) cm (cid:31) ··· (cid:31) c3] and k − 1 copies of [b (cid:31) cm (cid:31) ··· (cid:31) c3 (cid:31) c]. It is not hard to verify that
the WMG of P ∗ is as in Figure 1 (a).
Then  we prove that for any ϕ < 1√
1+ϕ2k+···+ϕ2k(m−2)
1+ϕ2+···+ϕ2(m−2)
minimize the Bayesian risk under M1

(cid:80)
(cid:80)
V ∈Lc (C) Pr(P|V )
W ∈Lb (C) Pr(P|W ) =
It follows that c is the Condorcet winner in P ∗ but it does not

ϕ  which means that it is not the winner under f 1
B.

  we can ﬁnd m and k so that

· ϕ2 < 1.

B(P ∗).

2

3In the Condorcet model the sample space is B(C)n [31]. We study a variant with sample space L(C)n.
4Characterizing majority and Condorcet criterion of f 1

is an open question.

B for ϕ ≥ 1√

2

5

(a) The WMG of P ∗.

(c) The WMG of P (cid:48) (Thm. 3).
Figure 1: WMGs of the proﬁles for proofs: (a) for majority and Condorcet (Thm. 1); (b) for consistency
(Thm. 1); (c) for computational complexity (Thm. 3).

(b) The WMGs of P1 (left) and P2 (right).

Consistency. We construct an example to show that f 1
B does not satisfy consistency. In our con-
struction m and n are even  and C = {c  b  c3  c4}. Let P1 and P2 denote proﬁles whose WMGs are
as shown in Figure 1 (b)  respectively. We have the following lemma.
Lemma 2 Let P ∈ {P1  P2} 

(cid:80)
(cid:80)
V ∈Lc (C) Pr(P|V )
W ∈Lb (C) Pr(P|W ) = 3(1+ϕ4k)

2(1+ϕ2k+ϕ4k) .

2(1+ϕ2k+ϕ4k) > 1 for all k. It is not hard to verify that f 1

B(P1) = f 1

B(P2) = {c}
(cid:3)

3(1+ϕ4k)

B(P1 ∪ P2) = {c  b}  which means that f 1

For any 0 < ϕ < 1 
and f 1
B.
Similarly  we can prove the following theorem for f 2
Theorem 2 For any ϕ  f 2
majority  the Condorcet criterion  or consistency.

B is not consistent.

B satisﬁes anonymity  neutrality  and monotonicity.

It does not satisfy

B and f 2

By Theorem 1 and 2  f 1
rule for winners. On the other hand  they minimize Bayesian risk under F 1
for which Kemeny does neither. In addition  neither f 1
that they are not positional scoring rules.

B do not satisfy as many desired normative properties as the Kemeny
ϕ  respectively 
B satisfy consistency  which means

ϕ and F 2

B nor f 2

5 Computational Complexity
We consider the following two types of decision problems.
Deﬁnition 5 In the BETTER BAYESIAN DECISION problem for a statistical decision-theoretic
framework (MC D  L) under a prior distribution  we are given d1  d2 ∈ D  and a proﬁle P . We are
asked whether RB(P  d1) ≤ RB(P  d2).
We are also interested in checking whether a given alternative is the optimal decision.
Deﬁnition 6 In the OPTIMAL BAYESIAN DECISION problem for a statistical decision-theoretic
framework (MC D  L) under a prior distribution  we are given d ∈ D and a proﬁle P . We are
asked whether d minimizes the Bayesian risk RB(P ·).
is the class of decision problems that can be computed by a P oracle machine with polynomial
PNP||
number of parallel calls to an NP oracle. A decision problem A is PNP|| -hard  if for any PNP|| problem
B  there exists a polynomial-time many-one reduction from B to A. It is known that PNP|| -hard
problems are NP-hard.
Theorem 3 For any ϕ  BETTER BAYESIAN DECISION and OPTIMAL BAYESIAN DECISION for F 1
under uniform prior are PNP|| -hard.
Proof: The hardness of both problems is proved by a uniﬁed reduction from the KEMENY WINNER
problem  which is PNP|| -complete [16]. In a KEMENY WINNER problem  we are given a proﬁle P and
an alternative c  and we are asked if c is ranked in the top of at least one V ∈ L(C) that minimizes
Kendall(P  V ).
For any alternative c  the Kemeny score of c under M1
P and any linear order where c is ranked in the top. We next prove that when ϕ < 1
risk of c is largely determined by the Kemeny score of c.
Lemma 3 For any ϕ < 1
smaller than the Kemeny score of b in P   then RB(P  c) < RB(P  b) for M1
ϕ.

ϕ is the smallest distance between the proﬁle
m!  the Bayesian
m!   any c  b ∈ C  and any proﬁle P   if the Kemeny score of c is strictly

ϕ

6

cbc3	
cm	
c4	
…2k	
2	
2	
2	
2	
2k	
2k	
cbc3	
c4	
4k	
2k	
2k	
cbc3	
c4	
4k	
2k	
2k	
c	
ab	
4	
6	
WMG of 6P	
6	
6	
2	
6	
6	
Let t be any natural number such that ϕt < 1
m!. For any KEMENY WINNER instance (P  c) for
alternatives C(cid:48)  we add two more alternatives {a  b} and deﬁne a proﬁle P (cid:48) whose WMG is as
shown in Figure 3(c) using McGarvey’s trick [24]. The WMG of P (cid:48) contains the WMG(P ) as a
subgraph  where the weights are 6 times the weights in WMG(P ).
Then  we let P ∗ = tP (cid:48)  which is t copies of P (cid:48). It follows that for any V ∈ L(C)  Pr(P ∗|V  ϕ) =
Pr(P (cid:48)|V  ϕt). By Lemma 3  if an alternative e has the strictly lowest Kemeny score for proﬁle P (cid:48) 
then it the unique alternative that minimizes the Bayesian risk for P (cid:48) and dispersion parameter ϕt 
which means that e minimizes the Bayesian risk for P ∗ and dispersion parameter ϕ.
Let O denote the set of linear orders over C(cid:48) that minimizes the Kendall tau distance from P and let
k denote this minimum distance. Choose an arbitrary V (cid:48) ∈ O. Let V = [b (cid:31) a (cid:31) V (cid:48)]. It follows
that Kendall(P (cid:48)  V ) = 4 + 6k. If there exists W (cid:48) ∈ O where c is ranked in the top position  then
we let W = [a (cid:31) c (cid:31) b (cid:31) (V (cid:48) − {c})]. We have Kendall(P (cid:48)  W ) = 2 + 6k. If c is not a Kemeny
winner in P   then for any W where d is not ranked in the top position  Kendall(P (cid:48)  W ) ≥ 6 + 6k.
Therefore  a minimizes the Bayesian risk if and only if c is a Kemeny winner in P   and if c does not
minimize the Bayesian risk  then b does. Hence BETTER DECISION (checking if a is better than b)
(cid:3)
and OPTIMAL BAYESIAN DECISION (checking if a is the optimal alternative) are PNP|| -hard.
We note that OPTIMAL BAYESIAN DECISION in Theorem 3 is equivalent to checking whether a
B(P ). We do not know whether these problems are PNP|| -complete. In
given alternative c is in f 1
sharp contrast to f 1
Theorem 4 For any rational number5 ϕ  BETTER BAYESIAN DECISION and OPTIMAL BAYESIAN
DECISION for F 2
The theorem is a corollary of the following stronger theorem that provides a closed-form formula
for Bayesian loss for F 2
ϕ.6 We recall that for any proﬁle P and any pair of alternatives c  b  that
wP (c  b) is the weight on c → b in the weighted majority graph of P .
ϕ under uniform prior  for any c ∈ C and any proﬁle P   RB(P  c) = 1 −
Theorem 5 For F 2
.

B  the next theorem states that f 2

ϕ under uniform prior are in P.

B under uniform prior is in P.

(cid:81)

1

b(cid:54)=c

1 + ϕwP (c b)

B  and f 2

B are summarized in Table 1. According to the criteria we
The comparisons of Kemeny  f 1
consider  none of the three outperforms the others. Kemeny does well in normative properties  but
does not minimize Bayesian risk under either F 1
B minimizes the
Bayesian risk under F 1
B  which minimizes
the Bayesian risk under F 2
ϕ  and more importantly  can be computed in polynomial time despite the
similarity between F 1

ϕ  but is hard to compute. We would like to highlight f 2
ϕ and F 2
ϕ.

ϕ  and is hard to compute. f 1

ϕ or F 2

6 Asymptotic Comparisons
In this section  we ask the following question: as the number of voters  n → ∞  what is the
probability that Kemeny  f 1
B  and f 2
B choose different winners? We show that when the data is
generated from M1
ϕ  all three methods are equal asymptotically almost surely (a.a.s.)  that is  they
are equal with probability 1 as n → ∞.
Theorem 6 Let Pn denote a proﬁle of n votes generated i.i.d. from M1
ϕ given W ∈ Lc(C). Then 
Prn→∞(Kemeny(Pn) = f 1
However  when the data are generated from M2
Theorem 7 For any W ∈ B(C) and any ϕ  f 1
Pn are generated i.i.d. from M2
For any m ≥ 5  there exists W ∈ B(C) such that for any ϕ  there exists  > 0 such that with
B(Pn) as n → ∞ and votes in Pn
probability at least   f 1
are generated i.i.d. from M2

ϕ  we have a different story.
B(Pn) = Kemeny(Pn) a.a.s. as n → ∞ and votes in

B(Pn) and Kemeny(Pn) (cid:54)= f 2

B(Pn) (cid:54)= f 2

B(Pn) = c) = 1.

B(Pn) = f 2

ϕ given W .

ϕ given W .

5We require ϕ to be rational to avoid representational issues.
6The formula resembles Young’s calculation for three alternatives [34]  where it was not clear whether the

calculation was done for F 2

ϕ. Recently it was clariﬁed by Xia [31] that this is indeed the case.

7

(a) W ∈ B(C) for m = 5.

(b) Probability that g is different from Kemeny under M2
ϕ.

Figure 2: The ground truth W and asymptotic comparisons between Kemeny and g in Deﬁnition 7.

Proof sketch: The ﬁrst part of Theorem 7 is proved by the Central Limit Theorem. For the second
part  the proof for m = 5 uses an acyclic W ∈ B(C) illustrated in Figure 2 (a).
(cid:3)
Theorem 6 suggests that  when n is large and the votes are generated from M1
ϕ  it does not matter
B  and Kemeny we use. A similar observation has been made for other voting
much which of f 1
rules by Caragiannis et al. [7]. On the other hand  Theorem 7 states that when the votes are generated
from M2
B is different from the other two with
non-negligible probability  and as we will see in the experiments  this probability can be quite large.

ϕ  interestingly  for some ground truth parameter  f 2

B  f 2

6.1 Experiments
B and Kemeny using synthetic data generated from M2
We focus on the comparison between rule f 2
ϕ
given the binary relation W illustrated in Figure 2 (a). By Theorem 5  the computation involves
computing ϕΩ(n)  which is exponentially small for large n since ϕ < 1. Hence  we need a special
data structure to handle the computation of f 2
B  because a straightforward implementation easily
loses precision. In our experiments  we use the following approximation for f 2
B.

Deﬁnition 7 For any c ∈ C and proﬁle P   let s(c  P ) =(cid:80)

b:wP (b c)>0 wP (b  c). Let g be the voting

rule such that for any proﬁle P   g(P ) = arg minc s(c  P ).

In words  g selects the alternative c with the minimum total weight on the incoming edges in the
WMG. By Theorem 5  the Bayesian risk is largely determined by ϕ−s(c P ). Therefore  g is a good
approximation of f 2
Theorem 8 For any W ∈ B(C) and any ϕ  f 2
generated i.i.d. from M2

B with reasonably large n. Formally  this is stated in the following theorem.

B(Pn) = g(Pn) a.a.s. as n → ∞ and votes in Pn are

ϕ given W .

In our experiments  data are generated by M2
ϕ given W in Figure 2 (a) for m = 5  n ∈
{100  200  . . .   2000}  and ϕ ∈ {0.1  0.5  0.9}. For each setting we generate 3000 proﬁles  and
calculate the fraction of trials in which g and Kemeny are different. The results are shown in Figu-
ire 2 (b). We observe that for ϕ = 0.1 and 0.5  the probability for g(Pn) (cid:54)= Kemeny(Pn) is about
30% for most n in our experiments; when ϕ = 0.9  the probability is about 10%. In light of Theo-
rem 8  these results conﬁrm Theorem 7. We have also conducted similar experiments for M1
ϕ  and
found that the g winner is the same as the Kemeny winner in all 10000 randomly generated proﬁles
with m = 5  n = 100. This provides a check for Theorem 6.

7 Acknowledgments

We thank Shivani Agarwal  Craig Boutilier  Yiling Chen  Vincent Conitzer  Edith Elkind  Ariel
Procaccia  and anonymous reviewers of AAAI-14 and NIPS-14 for helpful suggestions and discus-
sions. Azari Souﬁani acknowledges Siebel foundation for the scholarship in his last year of PhD
studies. Parkes was supported in part by NSF grant CCF #1301976 and the SEAS TomKat fund.
Xia acknowledges an RPI startup fund for support.

8

c1	
c2	
c3	
c4	
c5	
References
[1] David Austen-Smith and Jeffrey S. Banks. Information Aggregation  Rationality  and the Condorcet Jury

Theorem. The American Political Science Review  90(1):34–45  1996.

[2] Hossein Azari Souﬁani  David C. Parkes  and Lirong Xia. Random utility theory for social choice. In

Proc. NIPS  pages 126–134  2012.

[3] James O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer  2nd edition  1985.
[4] Craig Boutilier and Tyler Lu. Probabilistic and Utility-theoretic Models in Social Choice: Challenges for

Learning  Elicitation  and Manipulation. In IJCAI-11 Workshop on Social Choice and AI  2011.

[5] Craig Boutilier  Ioannis Caragiannis  Simi Haber  Tyler Lu  Ariel D. Procaccia  and Or Sheffet. Optimal

social choice functions: A utilitarian view. In Proc. EC  pages 197–214  2012.

[6] Ioannis Caragiannis and Ariel D. Procaccia. Voting Almost Maximizes Social Welfare Despite Limited

Communication. Artiﬁcial Intelligence  175(9–10):1655–1671  2011.

[7] Ioannis Caragiannis  Ariel Procaccia  and Nisarg Shah. When do noisy votes reveal the truth? In Proc. EC 

2013.

Rule. In Proc. AAAI  2014.

[8] Ioannis Caragiannis  Ariel D. Procaccia  and Nisarg Shah. Modal Ranking: A Uniquely Robust Voting

[9] Marquis de Condorcet. Essai sur l’application de l’analyse `a la probabilit´e des d´ecisions rendues `a la

pluralit´e des voix. Paris: L’Imprimerie Royale  1785.

[10] Vincent Conitzer and Tuomas Sandholm. Common voting rules as maximum likelihood estimators. In

Proc. UAI  pages 145–152  Edinburgh  UK  2005.

[11] Vincent Conitzer  Matthew Rognlie  and Lirong Xia. Preference functions that score rankings and maxi-

mum likelihood estimation. In Proc. IJCAI  pages 109–115  2009.

[12] Cynthia Dwork  Ravi Kumar  Moni Naor  and D. Sivakumar. Rank aggregation methods for the web. In

Proc. WWW  pages 613–622  2001.

[13] Edith Elkind and Nisarg Shah. How to Pick the Best Alternative Given Noisy Cyclic Preferences? In

[14] Peter C. Fishburn. Condorcet social choice functions. SIAM Journal on Applied Mathematics  33(3):

Proc. UAI  2014.

469–489  1977.

[15] Sumit Ghosh  Manisha Mundhe  Karina Hernandez  and Sandip Sen. Voting for movies: the anatomy of

a recommender system. In Proc. AAMAS  pages 434–435  1999.

[16] Edith Hemaspaandra  Holger Spakowski  and J¨org Vogel. The complexity of Kemeny elections. Theoret-

ical Computer Science  349(3):382–391  December 2005.

[17] John Kemeny. Mathematics without numbers. Daedalus  88:575–591  1959.
[18] Jen-Wei Kuo  Pu-Jen Cheng  and Hsin-Min Wang. Learning to Rank from Bayesian Decision Inference.

In Proc. CIKM  pages 827–836  2009.

[19] Bo Long  Olivier Chapelle  Ya Zhang  Yi Chang  Zhaohui Zheng  and Belle Tseng. Active Learning for

Ranking Through Expected Loss Optimization. In Proc. SIGIR  pages 267–274  2010.

[20] Tyler Lu and Craig Boutilier. The Unavailable Candidate Model: A Decision-theoretic View of Social

Choice. In Proc. EC  pages 263–274  2010.

[21] Tyler Lu and Craig Boutilier. Learning mallows models with pairwise preferences. In Proc. ICML  pages

[22] Colin L. Mallows. Non-null ranking model. Biometrika  44(1/2):114–130  1957.
[23] Andrew Mao  Ariel D. Procaccia  and Yiling Chen. Better human computation through principled voting.

[24] David C. McGarvey. A theorem on the construction of voting paradoxes. Econometrica  21(4):608–610 

145–152  2011.

In Proc. AAAI  2013.

1953.

[25] Shmuel Nitzan and Jacob Paroush. The signiﬁcance of independent decisions in uncertain dichotomous

choice situations. Theory and Decision  17(1):47–60  1984.

[26] Marcus Pivato. Voting rules as statistical estimators. Social Choice and Welfare  40(2):581–630  2013.
[27] Daniele Porello and Ulle Endriss. Ontology Merging as Social Choice: Judgment Aggregation under the

Open World Assumption. Journal of Logic and Computation  2013.

[28] Ariel D. Procaccia and Jeffrey S. Rosenschein. The Distortion of Cardinal Preferences in Voting.

In

Proc. CIA  volume 4149 of LNAI  pages 317–331. 2006.

[29] Ariel D. Procaccia  Sashank J. Reddi  and Nisarg Shah. A maximum likelihood approach for selecting

sets of alternatives. In Proc. UAI  2012.

[30] Abraham Wald. Statistical Decision Function. New York: Wiley  1950.
[31] Lirong Xia. Deciphering young’s interpretation of condorcet’s model. ArXiv  2014.
[32] Lirong Xia and Vincent Conitzer. A maximum likelihood approach towards aggregating partial orders. In

Proc. IJCAI  pages 446–451  Barcelona  Catalonia  Spain  2011.

[33] Lirong Xia  Vincent Conitzer  and J´erˆome Lang. Aggregating preferences in multi-issue domains by using

maximum likelihood estimators. In Proc. AAMAS  pages 399–406  2010.

[34] H. Peyton Young. Condorcet’s theory of voting. American Political Science Review  82:1231–1244  1988.
[35] H. Peyton Young and Arthur Levenglick. A consistent extension of Condorcet’s election principle. SIAM

Journal of Applied Mathematics  35(2):285–300  1978.

9

,Hossein Azari Soufiani
David Parkes
Lirong Xia