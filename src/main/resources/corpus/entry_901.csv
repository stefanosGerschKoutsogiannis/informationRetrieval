2009,Canonical Time Warping for Alignment of Human Behavior,Alignment of time series is an important problem to solve in many scientific disciplines. In particular  temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW)  an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of the behavior between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping for temporal alignment; and (ii) it extends CCA to allow local spatial deformations. We show CTWs effectiveness in three experiments: alignment of synthetic data  alignment of motion capture data of two subjects performing similar actions  and alignment of two people with similar facial expressions. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on dynamic time warping.,Canonical Time Warping

for Alignment of Human Behavior

Feng Zhou

Robotics Institute

Carnegie Mellon University

www.f-zhou.com

Fernando de la Torre

Robotics Institute

Carnegie Mellon University
ftorre@cs.cmu.edu

Abstract

Alignment of time series is an important problem to solve in many scientiﬁc dis-
ciplines. In particular  temporal alignment of two or more subjects performing
similar activities is a challenging problem due to the large temporal scale differ-
ence between human actions as well as the inter/intra subject variability. In this
paper we present canonical time warping (CTW)  an extension of canonical cor-
relation analysis (CCA) for spatio-temporal alignment of human motion between
two subjects. CTW extends previous work on CCA in two ways: (i) it combines
CCA with dynamic time warping (DTW)  and (ii) it extends CCA by allowing
local spatial deformations. We show CTW’s effectiveness in three experiments:
alignment of synthetic data  alignment of motion capture data of two subjects per-
forming similar actions  and alignment of similar facial expressions made by two
people. Our results demonstrate that CTW provides both visually and qualitatively
better alignment than state-of-the-art techniques based on DTW.

Introduction

1
Temporal alignment of time series has been an active research topic in many scientiﬁc disciplines
such as bioinformatics  text analysis  computer graphics  and computer vision. In particular  tem-
poral alignment of human behavior is a fundamental step in many applications such as recognition
[1]  temporal segmentation [2] and synthesis of human motion [3]. For instance consider Fig. 1a
which shows one subject walking with varying speed and different styles and Fig. 1b which shows
two subjects reading the same text.
Previous work on alignment of human motion has been addressed mostly in the context of recog-
nizing human activities and synthesizing realistic motion. Typically  some models such as hidden
Markov models [4  5  6]  weighted principal component analysis [7]  independent component anal-
ysis [8  9] or multi-linear models [10] are learned from training data and in the testing phase the
time series is aligned w.r.t.
the learned dynamic model. In the context of computer vision a key
aspect for successful recognition of activities is building view-invariant representations. Junejo et
al. [1] proposed a view-invariant descriptor for actions making use of the afﬁnity matrix between
time instances. Caspi and Irani [11] temporally aligned videos from two closely attached cameras.
Rao et al. [12  13] aligned trajectories of two moving points using constraints from the fundamental
matrix. In the literature of computer graphics  Hsu et al. [3] proposed the iterative motion warping 
a method that ﬁnds a spatio-temporal warping between two instances of motion captured data. In the
context of data mining there have been several extensions of DTW [14] to align time series. Keogh
and Pazzani [15] used derivatives of the original signal to improve alignment with DTW. Listgarten
et al. [16] proposed continuous proﬁle models  a probabilistic method for simultaneously aligning
and normalizing sets of time series.
A relatively unexplored problem in behavioral analysis is the alignment between the motion of the
body of face in two or more subjects (e.g.  Fig. 1). Major challenges to solve human motion align-

1

Figure 1: Temporal alignment of human behavior. (a) One person walking in normal pose  slow
speed  another viewpoint and exaggerated steps (clockwise). (b) Two people reading the same text.

ment problems are: (i) allowing alignment between different sets of multidimensional features (e.g. 
audio/video)  (ii) introducing a feature selection or feature weighting mechanism to compensate for
subject variability or irrelevant features and (iii) execution rate [17]. To solve these problems  this
paper proposes canonical time warping (CTW) for accurate spatio-temporal alignment between two
behavioral time series. We pose the problem as ﬁnding the temporal alignment that maximizes the
spatial correlation between two behavioral samples coming from two subjects. To accommodate for
subject variability and take into account the difference in the dimensionally of the signals  CTW uses
CCA as a measure of spatial alignment. To allow temporal changes CTW incorporates DTW. CTW
extends DTW by adding a feature weighting mechanism that is able to align signals of different
dimensionality. CTW also extends CCA by incorporating time warping and allowing local spatial
transformations.
The remainder of the paper is organized as follows. Section 2 reviews related work on dynamic time
warping and canonical correlation analysis. Section 3 describes the new CTW algorithm. Section 4
extends CTW to take into account local transformations. Section 5 provides experimental results.

2 Previous work
This section describes previous work on canonical correlation analysis and dynamic time warping.

2.1 Canonical correlation analysis
Canonical correlation analysis (CCA) [18] is a technique to extract common features from a pair of
multivariate data. CCA identiﬁes relationships between two sets of variables by ﬁnding the linear
combinations of the variables in the ﬁrst set1 (X ∈ Rdx×n) that are most correlated with the linear
combinations of the variables in the second set (Y ∈ Rdy×n). Assuming zero-mean data  CCA ﬁnds
a combination of the original variables that minimizes:

Jcca(Vx  Vy) = (cid:107)VT

x X − VT

y Y(cid:107)2

F

s.t. VT

x X  vT

y YYT Vy = Ib 

x XXT Vx = VT

(1)
where Vx ∈ Rdx×b is the projection matrix for X (similarly for Vy). The pair of canonical variates
y Y) is uncorrelated with other canonical variates of lower order. Each successive canon-
(vT
ical variate pair achieves the maximum correlation orthogonal to the preceding pairs. Eq. 1 has a
closed form solution in terms of a generalized eigenvalue problem. See [19] for a uniﬁcation of
several component analysis methods and a review of numerical techniques to efﬁciently solve the
generalized eigenvalue problems.
In computer vision  CCA has been used for matching sets of images in problems such as activity
recognition from video [20] and activity correlation from cameras [21]. Recently  Fisher et al. [22]
1Bold capital letters denote a matrix X  bold lower-case letters a column vector x. xi represents the ith
column of the matrix X. xij denotes the scalar in the ith row and jth column of the matrix X. All non-bold
letters represent scalars. 1m×n  0m×n ∈ Rm×n are matrices of ones and zeros. In ∈ Rn×n is an identity
xT x denotes the Euclidean distance. (cid:107)X(cid:107)2
matrix. (cid:107)x(cid:107) =
F = Tr(XT X) designates the Frobenious norm.
X ◦ Y and X ⊗ Y are the Hadamard and Kronecker product of matrices. Vec(X) denotes the vectorization of
matrix X. {i : j} lists the integers  {i  i + 1 ···   j − 1  j}.

√

2

(a)(b)Figure 2: Dynamic time warping. (a) 1-D time series (nx = 7 and ny = 9). (b) DTW alignment.
(d) Policy function at each node  where ↑ (cid:45) ← denote the policy 
(c) Binary distance matrix.
π(pt) = [1  0]T   [1  1]T   [0  1]T   respectively. The optimal alignment path is denoted in bold.

proposed an extension of CCA with parameterized warping functions to align protein expressions.
The learned warping function is a linear combination of hyperbolic tangent functions with non-
negative coefﬁcients  ensuring monotonicity. Unlike our method  the warping function is unable to
deal with feature weighting.

2.2 Dynamic time warping
Given two time series  X = [x1  x2 ···   xnx] ∈ Rd×nx and Y = [y1  y2 ···   yny] ∈ Rd×ny 
dynamic time warping [14] is a technique to optimally align the samples of X and Y such that the
following sum-of-squares cost is minimized:

Jdtw(P) =

(cid:107)xpx

t

− ypy

t

(cid:107)2 

(2)

m(cid:88)

t=1

where m is the number of indexes (or steps) needed to align both signals. The correspondence
matrix P can be parameterized by a pair of path vectors  P = [px  py]T ∈ R2×m  in which px ∈
{1 : nx}m×1 and py ∈ {1 : ny}m×1 denote the composition of alignment in frames. For instance 
t ]T = [i  j]T
the ith frame in X and the jth frame in Y are aligned iff there exists pt = [px
for some t. P has to satisfy three additional constraints: boundary condition (p1 ≡ [1  1]T and
pm ≡ [nx  ny]T )  continuity (0 ≤ pt − pt−1 ≤ 1) and monotonicity (t1 ≥ t2 ⇒ pt1 − pt2 ≥ 0).
Although the number of possible ways to align X and Y is exponential in nx and ny  dynamic pro-

(cid:1)) approach to minimize Jdtw using Bellman’s equation:

gramming [23] offers an efﬁcient (O(cid:0)nxny

t   py

L∗(pt) = min

(cid:107)xpx

t

− ypy

(cid:107)2 + L∗(pt+1) 

π(pt)

(3)
where the cost-to-go value function  L∗(pt)  represents the remaining cost starting at tth step to
be incurred following the optimum policy π∗. The policy function  π : {1 : nx} × {1 : ny} →
{[1  0]T   [0  1]T   [1  1]T}  deﬁnes the deterministic transition between consecutive steps  pt+1 =
pt + π(pt). Once the policy queue is known  the alignment steps can be recursively constructed
from the starting point  p1 = [1  1]T . Fig. 2 shows an example of DTW to align two 1-D time series.

t

3 Canonical time warping (CTW)
This section describes the energy function and optimization strategies for CTW.

3.1 Energy function for CTW
In order to have a compact and compressible energy function for CTW  it is important to notice that
Eq. 2 can be rewritten as:

Jdtw(Wx  Wy) =

T wy

j(cid:107)xi − yj(cid:107)2 = (cid:107)XWT

x − YWT

y (cid:107)2
F  

wx
i

(4)

where Wx ∈ {0  1}m×nx  Wy ∈ {0  1}m×ny are binary selection matrices that need to be inferred
to align X and Y. In Eq. 4 the matrices Wx and Wy encode the alignment path. For instance 

i=1

j=1

3

nx(cid:88)

ny(cid:88)

123456723451234567892345(a)(b)(c)(d)01110110110111111011011111111101001111101001101110110110111111012345678912345671234567891234567tpx
t

tpy
t

= wy

x Wx  Dy = WT

th frame in X and py
t

= 1 assigns correspondence between the px
t

th frame in Y. For
wx
convenience  we denote  Dx = WT
x Wy. Observe that Eq.
4 is very similar to the CCA’s objective (Eq. 1). CCA applies a linear transformation to the rows
(features)  while DTW applies binary transformations to the columns (time).
In order to accommodate for differences in style and subject variability  add a feature selection mech-
anism  and reduce the dimensionality of the signals  CTW adds a linear transformation (VT
x   VT
y )
(as CCA) to the least-squares form of DTW (Eq. 4). Moreover  this transformation allows aligning
temporal signals with different dimensionality (e.g.  video and motion capture). CTW combines
DTW and CCA by minimizing:

y Wy and W = WT

x − VT

Jctw(Wx  Wy  Vx  Vy) = (cid:107)VT

y YWT

x XWT

(5)
where Vx ∈ Rdx×b  Vy ∈ Rdy×b  b ≤ min(dx  dy) parameterize the spatial warping by pro-
jecting the sequences into the same coordinate system. Wx and Wy warp the signal in time to
achieve optimum temporal alignment. Similar to CCA  to make CTW invariant to translation  rota-
y 1m = 0dy  (ii)
tion and scaling  we impose the following constraints: (i) XWT
x XWYT Vy to be a diagonal matrix. Eq. 5
VT
is the main contribution of this paper. CTW is a direct and clean extension of CCA and DTW to
align two signals X and Y in space and time. It extends previous work on CCA by adding temporal
alignment and on DTW by allowing a feature selection and dimensionality reduction mechanism for
aligning signals of different dimensions.

y YDyYT Vy = Ib and (iii) VT

x XDxXT Vx = VT

x 1m = 0dx  YWT

y (cid:107)2
F  

3.2 Optimization for CTW

Algorithm 1: Canonical Time Warping
input : X  Y
output: Vx  Vy  Wx  Wy
begin

Initialize Vx = Idx   Vy = Idy
repeat

0

YWT XT

V =

0

0

YDyYT

VΛ

Use dynamic programming to compute  Wx  Wy  for aligning the sequences  VT
Set columns of  VT = [VT

x   VT

y ]  be the leading b generalized eigenvectors of:
XWYT

0

(cid:20) XDxXT

(cid:21)

(cid:20)

(cid:21)

x X  VT

y Y

until Jctw converges

end

Optimizing Jctw is a non-convex optimization problem with respect to the alignment matrices
(Wx  Wy) and projection matrices (Vx  Vy). We alternate between solving for Wx  Wy using
DTW  and optimally computing the spatial projections using CCA. These steps monotonically de-
crease Jctw and since the function is bounded below it will converge to a critical point.
Alg. 1 illustrates the optimization process (e.g.  Fig. 3e). The algorithm starts by initializing Vx
and Vy with identity matrices. Alternatively  PCA can be applied independently to each set  and
used as initial estimation of Vx and Vy if dx (cid:54)= dy. In the case of high-dimensional data  the
generalized eigenvalue problem is solved by regularizing the covariance matrices adding a scaled
identity matrix. The dimension b is selected to preserve 90% of the total correlation. We consider
the algorithm to converge when the difference between two consecutive values of Jctw is small.

4 Local canonical time warping (LCTW)
In the previous section we have illustrated how CTW can align in space and time two time series of
different dimensionality. However  there are many situations (e.g.  aligning long sequences) where
a global transformation of the whole time series is not accurate. For these cases  local models
have been shown to provide better performance [3  24  25]. This section extends CTW by allowing
multiple local spatial deformations.

4

4.1 Energy function for LCTW
Let us assume that the spatial transformation for each frame in X and Y can be model as a
T ]T ∈ Rkxdx×b  Vy =
linear combination of kx or ky bases. Let be Vx = [Vx
1
T ]T ∈ Rkydy×b and b ≤ min(kxdx  kydy). CTW allows for a more ﬂexible spatial
[Vy
1
warping by minimizing:

T  ···   Vx

T  ···   Vy

kx

ky

Jlctw(Wx  Wy  Vx  Vy  Rx  Ry)

nx(cid:88)

i=1

=

j(cid:107)(cid:16) kx(cid:88)

ny(cid:88)
(cid:104)
(1kx ⊗ X) ◦ (RT

T wy

wx
i

j=1

cx=1

rx
icx

Vx
cx

(cid:105)
x ⊗ 1dx)

T(cid:17)

xi −(cid:16) ky(cid:88)
(cid:104)

cy=1

ry
jcy

Vy
cy

(cid:107)Fxrx

T(cid:17)

yj(cid:107)2 +

kx(cid:88)
(cid:105)
y ⊗ 1dy)

cx=1

(6)

(cid:107)Fyry

cy

(cid:107)2

(cid:107)2 +

ky(cid:88)
F + (cid:107)FxRx(cid:107)2

cy=1

cx

F + (cid:107)FyRy(cid:107)2
F  

y

x

WT

x − VT

(1ky ⊗ Y) ◦ (RT

=(cid:107)VT
where Rx ∈ Rnx×kx   Ry ∈ Rny×ky are the weighting matrices. rx
icx denotes the coefﬁcient (or
). We further constrain the weights
weight) of the cth
to be positive (i.e.  Rx  Ry ≥ 0) and the sum of weights to be one (i.e.  Rx1kx = 1nx  Ry1ky =
1ny) for each frame. The last two regularization terms  Fx ∈ Rnx×nx   Fy ∈ Rny×ny  are 1st
∈ Rny×1  encouraging smooth solutions over time.
order differential operators of rx
cx
Observe that Jctw is a special case of Jlctw when kx = ky = 1.

x basis for the ith frame of X (similarly for ry
jcy

∈ Rnx×1  ry

y (cid:107)2

WT

cy

4.2 Optimization for LCTW

Algorithm 2: Local Canonical Time Warping
input : X  Y
output: Wx  Wy  Vx  Vy  Rx  Ry
begin

Initialize 

Vx = 1kx ⊗ Idx   Vy = 1ky ⊗ Idy
(cid:99) < i ≤ (cid:98) cxnx
kx

= 1 for (cid:98)(cy − 1)ny

ry
jcy

(cid:99) 

ky

(cid:99) < j ≤ (cid:98) cyny
ky

(cid:99)

= 1 for (cid:98)(cx − 1)nx

kx

rx
icx

repeat

Denote 

Zx = (1kx ⊗ X) ◦ (RT

x ⊗ 1dx)  Zy = (1ky ⊗ Y) ◦ (RT

y ⊗ 1dy)

Qx = VT

x (Ikx ⊗ X)  Qy = VT

y (Iky ⊗ Y)

Use dynamic programming to compute  Wx  Wy  between the sequences  VT
Set columns of  VT = [VT

y ]  be the leading b generalized eigenvectors 
ZxWZT
y

(cid:20) ZxDxZT

(cid:21)

(cid:21)

0

0

x

V =

0

0

ZyDyZT
y

VΛ

x Zx  VT

y Zy

Set  r = Vec([Rx  Ry])  be the solution of the quadratic programming problem 

−1ky×kx ⊗ WT ◦ QT

x Fx

x Qx + Ikx ⊗ FT
(cid:21)

y Qx

r = 1nx+ny

r ≥ 0nxkx+nyky

−1kx×ky ⊗ W ◦ QT

x Qy

1ky×ky ⊗ Dy ◦ QT

y Qy + Iky ⊗ FT

y Fy

x   VT

ZyWT ZT
x

(cid:20)
(cid:20) 1kx×kx ⊗ Dx ◦ QT
(cid:20) 1T

rT

kx

⊗ Inx
0

0
⊗ Iny

1T
ky

min

r

s.t.

(cid:21)

r

until Jlctw converges

end

As in the case of CTW  we use an alternating scheme for optimizing Jlctw  which is summarized in
Alg. 2. In the initialization  we assume that each time series is divided into kx or ky equal parts 
being the identity matrix the starting value for Vx
cy and block structure matrices for Rx  Ry.
cx

  Vy

5

The main difference between the alternating scheme of Alg. 1 and Alg. 2 is that the alternation
step is no longer unique. For instance  when ﬁxing Vx  Vy  one can optimize either Wx  Wy
or Rx  Ry. Consider a simple example of warping sin(t1) towards sin(t2)  one could shift the
ﬁrst sequence along time axis by δt = t2 − t1 or do the linear transformation  at1 sin(t1) + bt1 
where at1 = cos(t2 − t1) and bt1 = cos(t1) sin(t2 − t1).
In order to better control the trade-
off between time warping and spatial transformation  we propose a stochastic selection process.
Let us denote pw|v the conditional probability of optimizing W when ﬁxing V. Given the prior
probabilities [pw  pv  pr]  we can derive the conditional probabilities using Bayes’ theorem and the
fact that  [pr|w  pr|v  pv|r] = 1 − [pv|w  pw|v  pw|r].
[pv|w  pw|v  pw|r]T = A−1b   where A =

(cid:35)

and b =

. Fig. 3f (right-lower corner) shows the optimization

(cid:34) pw −pv

0
pw
0 −pv

(cid:35)

0
pr
pr

(cid:34)

0

pw−pv + pr

strategy  pw = .5  pv = .3  pr = .2  where the time warping process is more often optimized.

5 Experiments
This section demonstrates the beneﬁts of CTW and LCTW against state-of-the-art DTW approaches
to align synthetic data  motion capture data of two subjects performing similar actions  and similar
facial expressions made by two people.

x and Y = UT

y ZMT

x ZMT

5.1 Synthetic data
In the ﬁrst experiment we synthetically generated two spatio-temporal signals (3-D in space and 1-D
in time) to evaluate the performance of CTW and LCTW. The ﬁrst two spatial dimensions and the
y   where Z ∈ R2×m is
time dimension are generated as follows: X = UT
a curve in two dimensions (Fig. 3a). Ux  Uy ∈ R2×2 are randomly generated afﬁne transformation
matrices for the spatial warping and Mx ∈ Rnx×m  My ∈ Rny×m  m ≥ max(nx  ny) are randomly
generated matrices for time warping2. The third spatial dimension is generated by adding a (1× nx)
or (1 × ny) extra row to X and Y respectively  with zero-mean Gaussian noise (see Fig. 3a-b).
We compared the performance of CTW and LCTW against three other methods: (i) dynamic time
warping (DTW) [14]  (ii) derivative dynamic time warping (DDTW) [15] and (iii) iterative time
warping (IMW) [3]. Recall that in the case of synthetic data we know the ground truth alignment
matrix Wtruth = MxMT
y . The error between the ground truth and a given alignment Walg is
computed by the area enclosed between both paths (see Fig. 3g).
Fig. 3c-f show the spatial warping estimated by each algorithm. DDTW (Fig. 3c) cannot deal with
this example because the feature derivatives do not capture well the structure of the sequence. IMW
(Fig. 3d) warps one sequence towards the other by translating and re-scaling each frame in each
dimension. Fig. 3h shows the testing error (space and time) for 100 new generated time series. As it
can be observed CTW and LCTW obtain the best performance. IMW has more parameters (O(dn))
than CTW (O(db)) and LCTW (O(kdb + kn))  and hence IMW is more prone to overﬁtting. IMW
tries to ﬁt the noisy dimension (3rd spatial component) biasing alignment in time (Fig. 3g)  whereas
CTW and LCTW have a feature selection mechanism which effectively cancels the third dimension.
y =
Observe that the null space for the projection matrices in CTW is vT
[−.002 −.001 −.071]T .

x = [.002  .001 −.067]T   vT

5.2 Motion capture data
In the second experiment we apply CTW and LCTW to align human motion with similar behavior.
The motion capture data is taken from the CMU-Multimodal Activity Database [26]. We selected
a pair of sub-sequences from subject 1 and subject 3 cooking brownies. Typically  each sequence
contains 500-1000 frames. For each instance we computed the quaternions for the 20 joints resulting
in a 60 dimensional feature vector that describes the body conﬁguration. CTW and LCTW are
initialized as described in previous sections and optimized until convergence. The parameters of
LCTW are manually set to kx = 3  ky = 3 and pw = .5  pv = .3  pr = .2.

2The generation of time transformation matrix Mx (similar for My) is initialized by setting Mx = Inx.
Then  randomly pick and replicate m − nx columns of Mx. We normalize each row Mx1m = 1nx to make
the new frame to be an interpolation of zi.

6

Figure 3: Example with synthetic data. Time series are generated by (a) spatio-temporal transfor-
mation of 2-D latent sequence (b) adding Gaussian noise in the 3rd dimension. The result of space
warping is computed by (c) derivative dynamic time warping (DDTW)  (d) iterative time warping
(IMW)  (e) canonical time warping (CTW) and (f) local canonical time warping (LCTW). The en-
ergy function and order of optimizing the parameters for CTW and LCTW are shown in the top right
and lower right corner of the graphs. (g) Comparison of the alignment results for several methods.
(h) Mean and variance of the alignment error.

Figure 4: Example of motion capture data alignment. (a) PCA. (b) CTW. (c) LCTW. (d) Alignment
path. (e) Motion capture data. 1st row subject one  rest of the rows aligned subject two.

Fig. 4 shows the alignment results for the action of opening a cabinet. The projection on the principal
components for both sequences can be seen in Fig. 4a. CTW and LCTW project the sequences in
a low dimensional space that maximizes the correlation (Fig. 4b-c). Fig. 4d shows the alignment
path. In this case  we do not have ground truth data  and we evaluated the results visually. The ﬁrst
row of Fig. 4e shows few instances of the ﬁrst subject  and the last three rows the alignment of the
third subject for DTW  CTW and LCTW. Observe that CTW and LCTW achieve better temporal
alignment.

7

(a)(b)(c)(d)(e)(f)−100102030−10010203040−202−202−2024−4−202451015202530352530354045−20220406080102030405060708090 TruthDTWDDTWIMWCTWLCTW−0.100.10.2−0.15−0.1−0.0500.050.10.15510150.20.45101520WV−0.100.10.2−0.15−0.1−0.0500.050.10.15510150245101520WVR(g)(h)−10010203040−10010203040DTWDDTWIMWCTWLCTW00.020.040.060.080.10.120.140.16(a)(b)(c)(d)−0.500.5−0.500.500.050.10.15−0.0500.05−0.15−0.1−0.050−0.0500.05200400600800100200300400500600700 DTWCTWLCTWDTWCTWLCTW(e)5.3 Facial expression data
In this experiment we tested the ability of CTW and LCTW to align facial expressions. We took
29 subjects from the RU-FACS database [27] which consists of interviews with men and women
of varying ethnicity. The action units (AUs) in this database have been manually coded  and we
selected AU12 (smiling) to run our experiments. Each event of AU12 is coded with an onset (start) 
peak and offset (end). We used person-speciﬁc AAM [28] to track 66 landmark points on the face.
For the alignment of AU12 we only used 18 landmarks corresponding to the outline of the mouth 
so for each frame we have a vector (R36×1) with (x  y) coordinates.
We took subject 14 and 30 and ran CTW and LCTW on the segments where the AU12 was coded.
The parameters of LCTW are manually set to kx = 3  ky = 3 and pw = .5  pv = .3  pr = .2. Fig. 5
shows the results of the alignment. Fig. 5b-c shows that the low dimensional projection obtained
with CTW and LCTW has better alignment than DTW in Fig. 5a. Fig. 5d shows the position of
the peak frame as the intersection of the two dotted lines. As we can observe from Fig. 5d  the
alignment paths found by CTW and LCTW are closer to the manually labeled peak than the ones
found by DTW. This shows that CTW and LCTW provide better alignment because the manually
labeled peaks in both sequences should be aligned. Fig. 5e shows several frames illustrating the
alignment.

Figure 5: Example of facial expression alignment. (a) PCA. (b) CTW. (c) LCTW. (d) Alignment
path. (e) Frames from an AU12 event. The AU peaks are indicated by arrows.

6 Conclusions

In this paper we proposed CTW and LCTW for spatio-temporal alignment of time series. CTW
integrates the beneﬁts of DTW and CCA into a clean and simple formulation. CTW extends DTW by
adding a feature selection mechanism and enables alignment of signals with different dimensionality.
CTW extends CCA by adding temporal alignment and allowing temporal local projections. We
illustrated the beneﬁts of CTW for alignment of motion capture data and facial expressions.

7 Acknowledgements

This material is based upon work partially supported by the National Science Foundation under
Grant No. EEC-0540865.

8

(a)(e)DTWCTWLCTW−150−100−50050100−50050(b)−0.2−0.100.10.2−0.1−0.0500.050.10.150.20.25(c)−0.2−0.100.1−0.2−0.100.10.2(d)50100150102030405060708090 DTWCTWLCTWReferences
[1] I. N. Junejo  E. Dexter  I. Laptev  and P. P´erez. Cross-view action recognition from temporal self-

similarities. In ECCV  pages 293–306  2008.

[2] F. Zhou  F. de la Torre  and J. K. Hodgins. Aligned cluster analysis for temporal segmentation of human

motion. In FGR  pages 1–7  2008.

[3] E. Hsu  K. Pulli  and J. Popovic. Style translation for human motion. In SIGGRAPH  2005.
[4] M. Brand  N. Oliver  and A. Pentland. Coupled hidden Markov models for complex action recognition.

In CVPR  pages 994–999  1997.

[5] M. Brand and A. Hertzmann. Style machines. In SIGGRAPH  pages 183–192  2000.
[6] G. W. Taylor  G. E. Hinton  and S. T. Roweis. Modeling human motion using binary latent variables. In

NIPS  volume 19  page 1345  2007.

[7] A. Heloir  N. Courty  S. Gibet  and F. Multon. Temporal alignment of communicative gesture sequences.

J. Visual. Comp. Animat.  17(3-4):347–357  2006.

[8] A. Shapiro  Y. Cao  and P. Faloutsos. Style components. In Graphics Interface  pages 33–39  2006.
[9] G. Liu  Z. Pan  and Z. Lin. Style subspaces for character animation. J. Visual. Comp. Animat.  19(3-

4):199–209  2008.

[10] A. M. Elgammal and C.-S. Lee. Separating style and content on a nonlinear manifold. In CVPR  2004.
[11] Y. Caspi and M. Irani. Aligning non-overlapping sequences. Int. J. Comput. Vis.  48(1):39–51  2002.
[12] C. Rao  A. Gritai  M. Shah  and T. Fathima Syeda-Mahmood. View-invariant alignment and matching of

video sequences. In ICCV  pages 939–945  2003.

[13] A. Gritai  Y. Sheikh  C. Rao  and M. Shah. Matching trajectories of anatomical landmarks under view-

point  anthropometric and temporal transforms. Int. J. Comput. Vis.  2009.

[14] L. Rabiner and B.-H. Juang. Fundamentals of speech recognition. Prentice Hall  1993.
[15] E. J. Keogh and M. J. Pazzani. Derivative dynamic time warping. In SIAM ICDM  2001.
[16] J. Listgarten  R. M. Neal  S. T. Roweis  and A. Emili. Multiple alignment of continuous time series. In

NIPS  pages 817–824  2005.

[17] Y. Sheikh  M. Sheikh  and M. Shah. Exploring the space of a human action. In ICCV  2005.
[18] T. W. Anderson. An introduction to multivariate statistical analysis. Wiley-Interscience  2003.
[19] F. de la Torre. A uniﬁcation of component analysis methods. Handbook of Pattern Recognition and

Computer Vision  2009.

[20] T. K. Kim and R. Cipolla. Canonical correlation analysis of video volume tensors for action categorization

and detection. IEEE Trans. Pattern Anal. Mach. Intell.  31:1415–1428  2009.

[21] C. C. Loy  T. Xiang  and S. Gong. Multi-camera activity correlation analysis. In CVPR  2009.
[22] B. Fischer  V. Roth  and J. Buhmann. Time-series alignment by non-negative multiple generalized canon-

ical correlation analysis. BMC bioinformatics  8(10)  2007.

[23] D. P. Bertsekas. Dynamic programming and optimal control. 1995.
[24] Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. University of

Toronto Tec. Rep.  1997.

[25] J. J. Verbeek  S. T. Roweis  and N. A. Vlassis. Non-linear CCA and PCA by alignment of local models.

In NIPS  2003.

[26] F. de la Torre  J. K. Hodgins  J. Montano  S. Valcarcel  A. Bargteil  X. Martin  J. Macey  A. Collado 
and P. Beltran. Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database.
Carnegie Mellon University Tec. Rep.  2009.

[27] M. S. Bartlett  G. C. Littlewort  M. G. Frank  C. Lainscsek  I. Fasel  and J. R. Movellan. Automatic

recognition of facial actions in spontaneous expressions. J. Multimed.  1(6):22–35  2006.

[28] I. Matthews and S. Baker. Active appearance models revisited. Int. J. Comput. Vis.  60(2):135–164  2004.

9

,Vincent Roulet
Alexandre d'Aspremont
Shupeng Gui
Haotao Wang
Haichuan Yang
Chen Yu
Zhangyang Wang
Ji Liu