2014,Conditional Swap Regret and Conditional Correlated Equilibrium,We introduce a natural extension of the notion of swap regret  conditional swap regret  that allows for action modifications conditioned on the player’s action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium  conditional correlated equilibrium  that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies  then the empirical distribution approaches this equilibrium. Finally  we extend our results to the multi-armed bandit scenario.,Conditional Swap Regret and

Conditional Correlated Equilibrium

Mehryar Mohri

Courant Institute and Google

251 Mercer Street

New York  NY 10012
mohri@cims.nyu.edu

Scott Yang

Courant Institute
251 Mercer Street

New York  NY 10012
yangs@cims.nyu.edu

Abstract

We introduce a natural extension of the notion of swap regret  conditional swap
regret  that allows for action modiﬁcations conditioned on the player’s action his-
tory. We prove a series of new results for conditional swap regret minimization.
We present algorithms for minimizing conditional swap regret with bounded con-
ditioning history. We further extend these results to the case where conditional
swaps are considered only for a subset of actions. We also deﬁne a new notion
of equilibrium  conditional correlated equilibrium  that is tightly connected to the
notion of conditional swap regret: when all players follow conditional swap regret
minimization strategies  then the empirical distribution approaches this equilib-
rium. Finally  we extend our results to the multi-armed bandit scenario.

1

Introduction

On-line learning has received much attention in recent years.
In contrast to the standard batch
framework  the online learning scenario requires no distributional assumption. It can be described
in terms of sequential prediction with expert advice [13] or formulated as a repeated two-player
game between a player (the algorithm) and an opponent with an unknown strategy [7]: at each time
step  the algorithm probabilistically selects an action  the opponent chooses the losses assigned to
each action  and the algorithm incurs the loss corresponding to the action it selected.
The standard measure of the quality of an online algorithm is its regret  which is the difference
between the cumulative loss it incurs after some number of rounds and that of an alternative policy.
The cumulative loss can be compared to that of the single best action in retrospect [13] (external
regret)  to the loss incurred by changing every occurrence of a speciﬁc action to another [9] (internal
regret)  or  more generally  to the loss of action sequences obtained by mapping each action to some
other action [4] (swap regret). Swap regret  in particular  accounts for situations where the algorithm
could have reduced its loss by swapping every instance of one action with another (e.g. every time
the player bought Microsoft  he should have bought IBM).
There are many algorithms for minimizing external regret [7]  such as  for example  the randomized
weighted-majority algorithm of [13]. It was also shown in [4] and [15] that there exist algorithms for
minimizing internal and swap regret. These regret minimization techniques have been shown to be
useful for approximating game-theoretic equilibria: external regret algorithms for Nash equilibria
and swap regret algorithms for correlated equilibria [14].
By deﬁnition  swap regret compares a player’s action sequence against all possible modiﬁcations at
each round  independently of the previous time steps. In this paper  we introduce a natural extension
of swap regret  conditional swap regret  that allows for action modiﬁcations conditioned on the
player’s action history. Our deﬁnition depends on the number of past time steps we condition upon.

1

As a motivating example  let us limit this history to just the previous one time step  and suppose we
design an online algorithm for the purpose of investing  where one of our actions is to buy bonds
and another to buy stocks. Since bond and stock prices are known to be negatively correlated  we
should always be wary of buying one immediately after the other – unless our objective was to pay
for transaction costs without actually modifying our portfolio! However  this does not mean that we
should avoid purchasing one or both of the two assets completely  which would be the only available
alternative in the swap regret scenario. The conditional swap class we introduce provides precisely
a way to account for such correlations between actions. We start by introducing the learning set-up
and the key notions relevant to our analysis (Section 2).

2 Learning set-up and model

We consider the standard online learning set-up with a set of actions N = {1  . . .   N}. At each
round t ∈ {1  . . .   T}  T ≥ 1  the player selects an action xt ∈ N according to a distribution pt
over N   in response to which the adversary chooses a function f t : N t → [0  1] and causes the
player to incur a loss f t(xt  xt−1  . . .   x1). The objective of the player is to choose a sequence of
actions (x1  . . .   xT ) that minimizes his cumulative loss�T
A standard metric used to measure the performance of an online algorithm A over T rounds is its
(expected) external regret  which measures the player’s expected performance against the best ﬁxed
action in hindsight:

t=1 f t(xt  xt−1  . . .   x1).

[f t(xt  ..  x1)] − min
j∈N

f t(j  j  ...  j).

Reg
Ext

(A  T ) =

T�t=1

E

(xt .. x1)∼
(pt ... p1)

T�t=1
(A  T ) = �T

C

There are several common modiﬁcations to the above online learning scenario: (1) we may com-
pare regret against stronger competitor classes: Reg
t=1 Ept ... p1 f t(xt  ..  x1) −
minϕ∈C�T
t=1 Ept ... p1[f t(ϕ(xt)  ϕ(xt−1)  ...  ϕ(x1))] for some function class C ⊆ N N ; (2)
the player may have access to only partial information about the loss 
i.e. only knowledge
of f t(xt  ..  x1) as opposed to f t(a  xt−1  . . .   x1)∀a ∈ N (also known as the bandit sce-
nario); (3) the loss function may have bounded memory: f t(xt  ...  xt−k  xt−k−1  ...  x1) =
f t(xt  ...  xt−k  yt−k−1  ...  y1)  ∀xj  yj ∈ N .
The scenario where C = N N in (1) is called the swap regret case  and the case where k = 0 in (3) is
referred to as the oblivious adversary. (Sublinear) regret minimization is possible for loss functions
against any competitor class of the form described in (1)  with only partial information  and with
at least some level of bounded memory. See [4] and [1] for a reference on (1)  [2] and [5] for (2) 
and [1] for (3). [6] also provides a detailed summary of the best known regret bounds in all of these
scenarios and more.
The introduction of adversaries with bounded memory naturally leads to an interesting question:
what if we also try to increase the power of the competitor class in this way?
While swap regret is a natural competitor class and has many useful game theoretic consequences
(see [14])  one important missing ingredient is that the competitor class of functions does not have
memory. In fact  in most if not all online learning scenarios and regret minimization algorithms
considered so far  the point of comparison has been against modiﬁcation of the player’s actions
at each point of time independently of the previous actions. But  as we discussed above in the
ﬁnancial markets example  there exist cases where a player should be measured against alternatives
that depend on the past and the player should take into account the correlations between actions.
Speciﬁcally  we consider competitor functions of the form Φt : N t → N t. Let Call = {Φt : N t →
N t}∞t=1 denote the class of all such functions. This leads us to the expression:�T
t=1 Ep1 ... pt[f t]−
minΦt∈Call�T
t=1 Ep1 ... pt[f t ◦ Φt]. Call is clearly a substantially richer class of competitor func-
tions than traditional swap regret. In fact  it is the most comprehensive class  since we can always
reach �T
t=1 min(x1 .. xt) f t(x1  ..  xt) by choosing Φt to map all points to
argmin(xt .. x1) f t(xt  ...  x1). Not surprisingly  however  it is not possible to obtain a sublinear
regret bound against this general class.

t=1 Ep1 ... pt[f t] −�T

2

���

�

��������

�

���

���

��������
��������

��������

��������

��������

��������

��������

�

(b)

������
������
������

�

(a)

��������

�

���

Figure 1: (a) unigram conditional swap class interpreted as a ﬁnite-state transducer. This is the same
as the usual swap class and has only the trivial state; (b) bigram conditional swap class interpreted as
a ﬁnite-state transducer. The action at time t−1 deﬁnes the current state and inﬂuences the potential
swap at time t.

Theorem 1. No algorithm can achieve sublinear regret against the class Call  regardless of the loss
function’s memory.

This result is well-known in the on-line learning community  but  for completeness  we include a
proof in Appendix 9. Theorem 1 suggests examining more reasonable subclasses of Call. To simplify
the notation and proofs that follow in the paper  we will henceforth restrict ourselves to the scenario
of an oblivious adversary  as in the original study of swap regret [4]. However  an application of the
batching technique of [1] should produce analogous results in the non-oblivious case for all of the
theorems that we provide.
Now consider the collection of competitor functions Ck = {ϕ: N k → N}. Then  a player
who has played actions {as}t−1
s=1 in the past should have his performance compared against
ϕ(at  at−1  at−2  . . .   at−(k−1)) at time t  where ϕ ∈ Ck. We call this class Ck of functions the
k-gram conditional swap regret class  which also leads us to the regret deﬁnition:

Reg
Ck

(A  T ) =

T�t=1

E

xt∼pt

[f t(xt)] − min
ϕ∈Ck

T�t=1

E

xt∼pt

[f t(ϕ(xt  at−1  at−2  . . .   at−(k−1)))].

Note that this is a direct extension of swap regret to the scenario where we allow for swaps condi-
tioned on the history of the previous (k − 1) actions. For k = 1  this precisely coincides with swap
regret.
One important remark about the k-gram conditional swap regret is that it is a random quantity that
depends on the particular sequence of actions played. A natural deterministic alternative would be
of the form:

T�t=1

T�t=1

E

xt∼pt

[f t(xt)] − min
ϕ∈Ck

E

(xt ... x1)∼(pt ... p1)

[f t(ϕ(xt  xt−1  xt−2  . . .   xt−(k−1)))].

However  by taking the expectation of Reg
Jensen’s inequality  we obtain

Ck(A  T ) with respect to aT−1  aT2  . . .   a1 and applying

T�t=1

T�t=1
(A  T )≥

E

xt∼pt

E

(xt ... x1)∼(pt ... p1)

[f t(xt)]− min
ϕ∈Ck

[f t(ϕ(xt  xt−1  xt−2  . . .   xt−(k−1)))] 

Reg
Ck
and so no generality is lost by considering the randomized sequence of actions in our regret term.
Another interpretation of the bigram conditional swap class is in the context of ﬁnite-state transduc-
ers. Taking a player’s sequence of actions (x1  ...  xT )  we may view each competitor function in
the conditional swap class as an application of a ﬁnite-state transducer with N states  as illustrated
by Figure 1. Each state encodes the history of actions (xt−1  . . .   xt−(k−1)) and admits N outgoing
transitions representing the next action along with its possible modiﬁcation. In this framework  the
original swap regret class is simply a transducer with a single state.

3

3 Full Information Scenario

Here  we prove that it is in fact possible to minimize k-gram conditional swap regret against an
oblivious adversary  starting with the easier to interpret bigram scenario. Our proof constructs a
meta-algorithm using external regret algorithms as subroutines  as in [4]. The key is to attribute
a fraction of the loss to each external regret algorithm  so that these losses sum up to our actual
realized loss and also press the subroutines to minimize regret against each of the conditional swaps.
Theorem 2. There exists an online algorithm A with bigram swap regret bounded as follows:
C2(A  T ) ≤ O�N√T log N�.
Reg

1  ...  pt

Proof. Since the distribution pt at round t is ﬁnite-dimensional  we can represent it as a vector
N ). Similarly  since oblivious adversaries take only N arguments  we can write f t
pt = (pt
as the loss vector f t = (f t
t=1 be a sequence of random variables denoting the
player’s actions at each time t  and let δt
at denote the (random) Dirac delta distribution concentrated
at at and applied to variable xt. Then  we can rewrite the bigram swap regret as follows:

N ). Let {at}T

1  ...  f t

Reg
C2

(A  T ) =

=

T�t=1
T�t=1

E
pt

T�t=1
[f t(xt)] − min
ϕ∈C2
N�i j=1
N�i=1
T�t=1

i − min
if t
pt
ϕ∈C2

E
pt δt−1
at−1

[f t(ϕ(xt  xt−1)]

iδt−1
pt

{at−1=j}

f t
ϕ(i j)

Our algorithm for achieving sublinear regret is deﬁned as follows:

1. At t = 1 

initialize N 2 external regret minimizing algorithms Ai k  (i  k) ∈ N 2.
k=1  where for each
is a row vector consisting of the distribution weights generated

We can view these in the form of N matrices in RN×N   {Qt k}N
k ∈ {1  . . .   N}  Qt k
by algorithm Ai k at time t based on losses received at times 1  . . .   t − 1.

i

2. At each time t  let at−1 denote the random action played at time t − 1 and let δt−1
at−1 denote
the (random) Dirac delta distribution for this action. Deﬁne the N × N matrix Qt =
�N
Qt k. Qt is a Markov chain (i.e.  its rows sum up to one)  so it admits a
stationary distribution pt which we we will use as our distribution for time t.

k=1 δt−1

{at−1=k}

3. When we draw from pt  we play a random action at and receive loss f t. Attribute the
. Notice

f t to algorithm Ai k  and generate distributions Qt k

iδt−1

i

f t = f t  so that the actual realized loss is allocated completely.

portion of loss pt
iδt−1

i k=1 pt

that�N

{at−1=k}

{at−1=k}

t=1 f t i k

j

min  T  N) = O��Li k

minN
pt = ptQt is a stationary distribution  we can write:

Recall that an optimal external regret minimizing algorithm A (e.g. randomized weighted majority)
min log(N)�  where Li k
admits a regret bound of the form Ri k = Ri k(Li k
min =
j=1�T
t=1 incurred by the algorithm. Since
T�t=1
pt · f t =

for the sequence of loss vectors {f t i k}T

δt−1
{it−1=k}

N�k=1

N�j=1

N�j=1

N�j=1

N�i=1

N�i=1

T�t=1

T�t=1

T�t=1

pt
iQt

i jf t

j =

Qt k

i j f t
j .

pt
jf t

j =

pt
i

4

iδt−1
pt

{it−1=k}

Qt k

i j f t
j

iδt−1
pt

{it−1=k}

iδt−1
pt

{it−1=k}

f t

f t

ϕ(i k)� + Ri k(Lmin  T  N)� (for arbitrary ϕ: N 2 → N )
ϕ(i k)� +

Ri k(Lmin  T  N).

N�i k=1

Rearranging leads to

T�t=1

pt · f t =

≤

=

N�j=1
T�t=1
N�i k=1
N�i k=1�� T�t=1
N�i k=1� T�t=1
T�t=1

Since ϕ is arbitrary  we obtain

Reg
C2

(A  T ) =

Using the fact that Ri k = O��Li k

iδt−1
pt
implies

{it−1=k}

iδt−1
pt

{it−1=k}

f t
ϕ(i k) ≤

N�i k=1

Ri k(Lmin  T  N).

pt · f t − min
ϕ∈C2

N�i k=1
T�t=1
min log(N)� and that we scaled the losses to algorithm Ai k by
k=1�N
  the following inequality holds: �N
min ≤ T . By Jensen’s inequality  this
min ≤���� 1
N�j=1�Lk j
N�k=1
N�j=1
min ≤ N√T . Combining this with our regret bound yields
min log N� ≤ O�N�T log N�  
O��Li k
N�i k=1

N�k=1
j=1�Lk j

Ri k(Lmin  T  N) =

Lk j
min ≤

j=1 Lk j

√T
N

1
N 2

N 2

 

or  equivalently �N
k=1�N
N�i k=1
(A  T ) ≤

Reg
C2

which concludes the proof.
Remark 1. The computational complexity of a standard external regret minimization algorithm such
as randomized weighted majority per round is in O(N) (update the distribution on each of the N
actions multiplicatively and then renormalize)  which implies that updating the N 2 subroutines will
cost O(N 3) per round. Allocating losses to these subroutines and combining the distributions that
they return will cost an additional O(N 3) time. Finding the stationary distribution of a stochastic
matrix can be done via matrix inversion in O(N 3) time. Thus  the total computational complexity
of achieving O(N�T log(N)) regret is only O(N 3T ). We remark that in practice  one often uses
iterative methods to compute dominant eigenvalues (see [16] for a standard reference and [11] for
recent improvements). [10] has also studied techniques to avoid computing the exact stationary
distribution at every iteration step for similar types of problems.

j

{at−1=k}

1  ...  Qt

j   ...  Qt N

N   picking one subset Qt

to randomly choose among the algorithms Qt 1

; after locating this algorithm  the player uses the distribution from algorithm Qt at−1

The meta-algorithm above can be interpreted in three equivalent ways: (1) the player draws an
action xt from distribution pt at time t; (2) the player uses distribution pt to choose among the
j; next  after drawing j from pt  the
N subsets of algorithms Qt
player uses δt−1
  picking algorithm
Qt at−1
to draw
an action; (3) the player chooses algorithm Qt k
and draws an action
from its distribution.
The following more general bound can be given for an arbitrary k-gram swap scenario.
Theorem 3. There exists an online algorithm A with k-gram swap regret bounded as follows:
Ck(A  T ) ≤ O��N kT log N�.
Reg
The algorithm used to derive this result is a straightforward extension of the algorithm provided in
the bigram scenario  and the proof is given in Appendix 11.
Remark 2. The computational complexity of achieving the above regret bound is O(N k+1T ).

j with probability pt

{at−1=k}

jδt−1

j

j

5

��������
��������

���
���

�

�

���

���

��������
��������

��������

��������

�

Figure 2: bigram conditional swap class restricted to a ﬁnite number of active states. When the
action at time t − 1 is 1 or 2  the transducer is in the same state  and the swap function is the same.

4 State-Dependent Bounds

In some situations  it may not be relevant to consider conditional swaps for every possible action 
either because of the speciﬁc problem at hand or simply for the sake of computational efﬁciency.
Thus  for any S ⊆ N 2  we deﬁne the following competitor class of functions:

C2 S = {ϕ: N 2 → N|ϕ(i  k) = ˜ϕ(i) for (i  k) ∈ S where ˜ϕ: N → N}.

exists

an

online

algorithm A such

See Figure 2 for a transducer interpretation of this scenario.
We will now show that the algorithm above can be easily modiﬁed to derive a tighter bound that
is dependent on the number of states in our competitor class. We will focus on the bigram case 
although a similar result can be shown for the general k-gram conditional swap regret.
Theorem 4. There

O(�T (|S c| + N) log(N)).
The proof of this result is given in Appendix 10. Note that when S = ∅  we are in the scenario where
all the previous states matter  and our bound coincides with that of the previous section.
Remark 3. The computational complexity of achieving the above regret bound is O((N(|π1(S)| +
|S c|) + N 3)T )  where π1 is projection onto the ﬁrst component. This follows from the fact that
we allocate the same loss to all {Ai k}k:(i k)∈S ∀i ∈ π1(S)  so we effectively only have to manage
|π1(S)| + |S c| subroutines.
5 Conditional Correlated Equilibrium and �-Dominated Actions

that Reg

(A  T )

C2 S

≤

It is well-known that regret minimization in on-line learning is related to game-theoretic equilibria
[14]. Speciﬁcally  when both players in a two-player zero-sum game follow external regret mini-
mizing strategies  then the product of their individual empirical distributions converges to a Nash
equilibrium. Moreover  if all players in a general K-player game follow swap regret minimizing
strategies  then their empirical joint distribution converges to a correlated equilibrium [7].
We will show in this section that when all players follow conditional swap regret minimization
strategies  then the empirical joint distribution will converge to a new stricter type of correlated
equilibrium.
Deﬁnition 1. Let Nk = {1  ...  Nk}  for k ∈ {1  ...  K} and G = (S = ×K
k=1Nk {l(k) : S →
k=1) denote a K-player game. Let s = (s1  ...  sK) ∈ S denote the strategies of all players in
[0  1]}K
one instance of the game  and let s(−k) denote the (K − 1)-vector of strategies played by all players
aside from player k. A joint distribution P on two rounds of this game is a conditional correlated
equilibrium if for any player k  actions j  j� ∈ Nk  and map ϕk : N 2

k → Nk  we have
P (s  r)�l(k)(sk  s(−k)) − l(k)(ϕk(sk  rk)  s(−k))� ≤ 0.

�(s r)∈S2 : sk=j rk=j�

The standard interpretation of correlated equilibrium  which was ﬁrst introduced by Aumann  is a
scenario where an external authority assigns mixed strategies to each player in such a way that no
player has an incentive to deviate from the recommendation  provided that no other player deviates

6

from his [3]. In the context of repeated games  a conditional correlated equilibrium is a situation
where an external authority assigns mixed strategies to each player in such a way that no player
has an incentive to deviate from the recommendation in the second round  even after factoring in
information from the previous round of the game  provided that no other player deviates from his.
It is important to note that the concept of conditional correlated equilibrium presented here is differ-
ent from the notions of extensive form correlated equilibrium and repeated game correlated equilib-
rium that have been studied in the game theory and economics literature [8  12].
Notice that when the values taken for ϕk are independent of its second argument  we retrieve the
familiar notion of correlated equilibrium.
Theorem 5. Suppose that all players in a K-player repeated game follow bigram conditional swap
regret minimizing strategies. Then  the joint empirical distribution of all players converges to a
conditional correlated equilibrium.

Proof. Let I t ∈ S be a random vector denoting the actions played by all K players in the game
at round t. The empirical joint distribution of every two subsequent rounds of a K-player game
t=1�(s r)∈S2 δ{I t=s I t−1=r}  where
played repeatedly for T total rounds has the form �P T = 1
I = (I1  ..  IK) and Ik ∼ p(k) denotes the action played by player k using the mixed strategy p(k).
Let qt (k) denote δt−1
{it−1=k} ⊗ pt−1 (k−1). Then  the conditional swap regret of each player k 
reg(k  T )  can be bounded as follows since he is playing with a conditional swap regret minimizing
strategy:

T �T

reg(k  T ) =

st

E

1
T

T�t=1

k∼pt (k)�l(k)(sk  s(−k))� − min
≤ O�N�log(N)
T �.

ϕ

1
T

T�t=1

Deﬁne the instantaneous conditional swap regret vector as

∼(pt (k) qt (k))�l(k)(ϕ(st

E
k st−1

(st

)

k

k  st−1

k

)  st

(−k))�

{I t

{I t−1

k = j0)δ

(k)=j0 I t−1

t j0 j1 = δ

�r(k)

r(k)
t j0 j1 = P(st

and the expected instantaneous conditional swap regret vector as

(−k)��  
(−k)� − l(k)�ϕk(j0  j1)  I t
t j0 j1. Thus  {Rt = r(k)

(k) =j1}�l(k)�I t� − l(k)�ϕk(j0  j1)  I t
(k) =j1}�l(k)�j0  I t
Consider the ﬁltration Gt = {information of opponents at time t and of the player’s actions up to
time t − 1}. Then  we see that E��r(k)
t j0 j1|Gt� = r(k)
t j0 j1}∞t=1 is a
sequence of bounded martingale differences  and by the Hoeffding-Azuma inequality  we can write
for any α > 0  that P[|�T
Now deﬁne the sets AT := ���� 1
t=1 Rt��� >� C
δT��. By our concentration bound  we
T �T
have P (AT ) ≤ δT . Setting δT = exp(−√T ) and applying the Borel-Cantelli lemma  we obtain
T �T
t=1 Rt| = 0 a.s..
that lim supT→∞ | 1
Finally  since each player followed a conditional swap regret minimizing strategy  we can write
T �T
t=1�r(k)
lim supT→∞
t j0 j1 ≤ 0. Now  if the empirical distribution did not converge to a con-
ditional correlated equilibrium  then by Prokhorov’s theorem  there exists a subsequence {�P Tj}j
satisfying the conditional correlated equilibrium inequality but converging to some limit P ∗ that is
not a conditional correlated equilibrium. This cannot be true because the inequality is closed under
weak limits.

(−k)�� .
t j0 j1 −�r(k)

t=1 Rt| > α] ≤ 2 exp(−Cα2/T ) for some constant C > 0.

T log� 2

1

Convergence to equilibria over the course of repeated game-playing also naturally implies the
scarcity of “very suboptimal” strategies.

7

Deﬁnition 2. An action pair (sk  rk) ∈ N 2
there exists a map ϕk : N 2
k → Nk such that
l(k)(sk  s(−k)) − l(k)(ϕk(sk  rk)  s(−k)) ≥ �.

k played by player k is conditionally �-dominated if

Theorem 6. Suppose player k follows a conditional swap regret minimizing strategy that produces
a regret R over T instances of the repeated game. Then  on average  an action pair of player k is
conditionally �-dominated at most R

�T fraction of the time.

The proof of this result is provided in Appendix 12.

6 Bandit Scenario

As discussed earlier  the bandit scenario differs from the full-information scenario in that the player
only receives information about the loss of his action f t(xt) at each time and not the entire loss
function f t. One standard external regret minimizing algorithm is the Exp3 algorithm introduced
by [2]  and it is the base learner off of which we will build a conditional swap regret minimizing
algorithm.
To derive a sublinear conditional swap regret bound  we require an external regret bound on Exp3:

T�t=1

E
pt

[f t(xt)] − min
a∈N

T�t=1

f t(a) ≤ 2�LminN log(N) 

which can be found in Theorem 3.1 of [5]. Using this estimate  we can derive the following result.
Theorem 7. There exists an algorithm A such that Reg
The proof is given in Appendix 13 and is very similar to the proof for the full information setting.
It can also easily be extended in the analogous way to provide a regret bound for the k-gram regret
in the bandit scenario.
Theorem 8. There exists an algorithm A such that Reg
See Appendix 14 for an outline of the algorithm.

Ck bandit(A  T ) ≤ O��N k+1 log(N)T�.

C2 bandit(A  T ) ≤ O��N 3 log(N)T�.

7 Conclusion

We analyzed the extent to which on-line learning scenarios are learnable. In contrast to some of
the more recent work that has focused on increasing the power of the adversary (see e.g. [1])  we
increased the power of the competitor class instead by allowing history-dependent action swaps and
thereby extending the notion of swap regret. We proved that this stronger class of competitors can
still be beaten in the sense of sublinear regret as long as the memory of the competitor is bounded.
We also provided a state-dependent bound that gives a more favorable guarantee when only some
parts of the history are considered. In the bigram setting  we introduced the notion of conditional
correlated equilibrium in the context of repeated K-player games  and showed how it can be seen
as a generalization of the traditional correlated equilibrium. We proved that if all players follow
bigram conditional swap regret minimizing strategies  then the empirical joint distribution converges
to a conditional correlated equilibrium and that no player can play very suboptimal strategies too
often. Finally  we showed that sublinear conditional swap regret can also be achieved in the partial
information bandit setting.

8 Acknowledgements

We thank the reviewers for their comments  many of which were very insightful. We are particularly
grateful to the reviewer who found an issue in our discussion on conditional correlated equilibrium
and proposed a helpful resolution. This work was partly funded by the NSF award IIS-1117591. The
material is also based upon work supported by the National Science Foundation Graduate Research
Fellowship under Grant No. DGE 1342536.

8

,Harish Ramaswamy
Shivani Agarwal
Ambuj Tewari
Mehryar Mohri
Scott Yang
Pratik Kumar Jawanpuria
Maksim Lapin
Matthias Hein
Bernt Schiele