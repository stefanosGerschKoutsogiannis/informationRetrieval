2009,A Data-Driven Approach to Modeling Choice,We visit the following fundamental problem: For a `generic model of consumer choice (namely  distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information)  how may one predict revenues from offering a particular assortment of choices? This problem is central to areas within operations research  marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same.,A Data-Driven Approach to Modeling Choice

Vivek F. Farias

Srikanth Jagabathula

Devavrat Shah∗

Abstract

We visit the following fundamental problem: For a ‘generic’ model of con-
sumer choice (namely  distributions over preference lists) and a limited
amount of data on how consumers actually make decisions (such as marginal
preference information)  how may one predict revenues from oﬀering a par-
ticular assortment of choices? This problem is central to areas within op-
erations research  marketing and econometrics. We present a framework to
answer such questions and design a number of tractable algorithms (from
a data and computational standpoint) for the same.

1 Introduction

Consider a seller who must pick from a universe of N products  N   a subset M of products
to oﬀer to his customers. The ith product has price pi. Given a probabilistic model of how
customers make choices  P(·|·)  where P(i|M) is the probability that a potential customer
purchases product i when faced with options M  the seller may solve
(1)

max

M⊂N Øi∈M

piP(i|M).

In addition to being a potentially non-trivial optimization problem  one faces a far more
fundamental obstacle here: specifying the ‘choice’ model P(·|·) is a diﬃcult task and it
is unlikely that a seller will have suﬃcient data to estimate a generic such model. Thus 
simply predicting expected revenues  R(M) = qi∈M piP(i|M)  for a given oﬀer set  M  is

a diﬃcult task. This problem  and variants thereof  are central in the ﬁelds of marketing 
operations research and econometrics. With a few exceptions  the typical approach to
dealing with the challenge of specifying a choice model with limited data has been to make
parametric assumptions on the choice model that allow for its estimation from a limited
amount of data. This approach has a natural deﬁciency – the implicit assumptions made in
specifying a parametric model of choice may not hold. Indeed  for one of the most commonly
used parametric models in modern practice (the multinomial logit)  it is a simple task to
come up with a list of deﬁciencies  ranging from serious economic fallacies presumed by
the model ([5])  to a lack of statistical ﬁt to observed data for real-world problems ([1  8]).
These issues have led to a proliferation of increasingly arcane parametric choice models.

The present work considers the following question: given a limited amount of data on
customer preferences and assuming only a ‘generic’ model of customer choice  what can one
predict about expected revenues from a given set of products? We take as our ‘generic’
model of customer choice  the set of distributions over all possible customer preference lists
(i.e. all possible permutations of N ). We will subsequently see that essentially all extant
models of customer choice can be viewed as a special case of this generic model. We view
‘data’ as some linear transformation of the distribution specifying the choice model  yielding
marginal information. Again  we will see that this view is consistent with reality.

∗VF  DS are aﬃliated with ORC; VF with Sloan School of Management; SJ and DS with LIDS
and Department of EECS at MIT. Emails: vfarias  jskanth  devavrat@mit.edu. The work was
supported in part by NSF CAREER CNS 0546590.

1

Given these views  we ﬁrst consider ﬁnding the ‘simplest’ choice model  consistent with the
observed marginal data on customer preferences. Here we take as our notion of simple 
a distribution over permutations of N with the sparsest support. We present two simple
abstract properties that if satisﬁed by the ‘true’ choice model  allow us to solve the sparsest
ﬁt problem exactly via a simple combinatorial procedure (Theorem 2). In fact  the sparsest
ﬁt in this case coincides with the true model (Theorem 1). We present a generative family of
choice models that illustrates when the two properties we posit may be expected to hold (see
Theorem 3). More generally  when we may not anticipate the above abstract properties  we
seek to ﬁnd a ‘worst-case’ distribution consistent with the observed data in the sense that
this distribution yields minimum expected revenues for a given oﬀer set M while remaining
consistent with the observed marginal data. This entails solving mathematical programs
with as many variables as there are permutations (N !).
In spite of this  we present a
simple eﬃcient procedure to solve this problem that is exact for certain interesting types
of data and produces approximations (and computable error bounds) in general. Finally 
we present a computational study illustrating the eﬃcacy of our approach relative to a
parametric technique on a real-world data set.

Our main contribution is thus a novel approach to modeling customer choice given limited
data. The approach we propose is complemented with eﬃcient  implementable algorithms.
These algorithms yield subroutines that make non-parametric revenue predictions for any
given oﬀer set (i.e. predict R(M) for any M) given limited data. Such subroutines could
then be used in conjunction with generic set-function optimization heuristics to solve (1).

Relevant Literature: There is a vast body of literature on the parametric modeling
of customer choice; a seminal paper in this regard is [10]. See also [14] and references
therein for an overview of the area with an emphasis on applications. There is a stream of
research (eg. [6]) on estimating and optimizing (parametric) choice models when products
possess measurable attributes that are the sole inﬂuencers of choice; we do not assume the
availability of such attributes and thus do not consider this situation here. A non-parametric
approach to choice modeling is considered by [12]; that work studies a somewhat distinct
pricing problem  and assumes the availability of a speciﬁc type of rich observable data.
Fitting a sparsest model to observable data has recently become of great interest in the area
of compressive sensing in signal processing [3  7]  and in the design of sketches for streaming
algorithms [2  4]. This work focuses on deriving precise conditions on the support size of
the true model  which  when satisﬁed  guarantee that the sparsest solution is indeed the
true solution. However  these prior methods do not apply in the present context (see [9]);
therefore  we take a distinct approach to the problem in this paper.

2 The Choice Model and Problem Formulations

We consider a universe of N products  N = {0  1  2  . . .   N − 1}. We assume that the 0th
product in N corresponds to the ‘outside’ or ‘no-purchase’ option. A consumer is associated
with a permutation σ of the elements of N ; the customer prefers product i to product j
iﬀ σ(i) < σ(j). Given that the customer is faced with a set of alternatives M ⊂ N   she
chooses to purchase her single most preferred product among those in M. In particular  she
purchases argmini∈M σ(i).
Choice Model: We take as our model of customer choice a distribution  λ : SN → [0  1] 
over all possible permutations (i.e. the set of all permutations SN ). Deﬁne the set

Sj(M) = {σ ∈ SN : σ(j) < σ(i) ∀i ∈ M  i Ó= j}

as the set of all customer types that would result in a purchase of j when the oﬀer set is
M. Our choice model is thus

P(j|M) = Øσ∈Sj (M)

λ(σ)   λj(M).

This model subsumes a vast body of extant parametric choice models.
Revenues: We associate every product in N with a retail price pj. Of course  p0 = 0. The
expected revenues to a retailer from oﬀering a set of products M to his customers under
our choice model is thus given by R(M) = qj∈M pjλj(M).

2

Data: A seller will have limited data with which to estimate λ. We simply assume that
the data observed by the seller is given by an m-dimensional ‘partial information’ vector
y = Aλ  where A ∈ {0  1}m×N ! makes precise the relationship between the observed data
and the underlying choice model. For the purposes of illustration  we consider the following
concrete examples of data vectors y:

Ranking Data: This data represents the fraction of customers that rank a given product i
as their rth choice. Here the partial information vector y is indexed by i  r with 0 ≤ i  r ≤ N .
For each i  r  yri denotes the probability that product i is ranked at position r. The matrix
A is thus in {0  1}N 2×N !. For a column of A corresponding to the permuation σ  A(σ)  we
will thus have A(σ)ri = 1 iﬀ σ(i) = r.

This data represents the fraction of customers that prefer a given
Comparison Data:
product i to a product j. The partial information vector y is indexed by i  j with 0 ≤ i  j ≤
N ; i Ó= j. For each i  j  yi j denotes the probability that product i is preferred to product j.
The matrix A is thus in {0  1}N (N −1)×N !. A column of A  A(σ)  will thus have A(σ)ij = 1
if and only if σ(i) < σ(j).

Top Set Data: This data refers to a concatenation of the “Comparison Data” and informa-
tion on the fraction of customers who have a given product i as their topmost choice for
each i. Thus AÛ = [AÛ
2 ] where A1 is simply the A matrix for comparison data  and
A2 ∈ {0  1}N ×N ! has A2(σ)i = 1 iﬀ σ(i) = 1.
Many other types of data vectors consistent with the above view are possible; all we antici-
pate is that the dimension of the observed data m is substantially smaller than N !. We are
now in a position to formulate the questions broached in the previous section precisely:

1 AÛ

“Simplest” Model: In ﬁnding the simplest choice model consistent with the observed data
we attempt to solve:

minimizeëλë0

(2)
Robust Approach: For a given oﬀer set M  and data vector y  what are the minimal
expected revenues we might expect from M consistent with the observed data? To answer
this question  we attempt to solve :

λ ≥ 0.

subject to Aλ = y 

1Ûλ = 1 

(3)

minimize

λ

qj∈M

pjλj(M)

subject to Aλ = y 

1Ûλ = 1 

λ ≥ 0.

3 Estimating Sparse Choice Models

Here we consider ﬁnding the sparsest model consistent with the observed data (i.e. problem
(2)). We face two questions: (a) Why is sparsity an interesting criterion? (b) Is there
an eﬃcient procedure to solve the program in (2)? We begin by identifying two simple
conditions that deﬁne a class of choice models (i.e. a class of distributions λ). Assuming
that the ‘true’ underlying model λ belongs to this class  we prove that the sparsest model
(i.e the solution to (2)) is in fact this true model. This answers the ﬁrst question. We then
propose a simple procedure inspired by [9] that correctly solves the program in (2) assuming
these conditions. It is diﬃcult to expect the program in (2) to recover the true solution in
general (see [9] for a justiﬁcation). Nonetheless  we show that the conditions we impose are
not overly restrictive: we prove that a “suﬃciently” sparse model generated uniformly at
random from the set of all possible choice models satisﬁes the two conditions with a high
probability.

Before we describe the conditions we impose on the true underlying distribution  we intro-
duce some notation. Let λ denote the true underlying distribution  and let K denote the
support size  ëλë0. Let σ1  σ2  . . .   σK denote the permutations in the support  i.e  λ(σi) Ó= 0
for 1 ≤ i ≤ K  and λ(σ) = 0 for all σ Ó= σi  1 ≤ i ≤ K. Recall that y is of dimension m and
we index its elements by d. The two conditions we impose are as follows:

in the support  there exists a d(i) ∈
Signature Condition: For every permutation σi
{1  2  . . .   m} such that A(σi)d(i) = 1 and A(σj)d(i) Ó= 0  for every j Ó= i and 1 ≤ i  j ≤ K.
In other words  for each permutation σi in the support  yd(i) serves as its ‘signature’.

3

Linear Independence Condition: qK
i=1 ciλ(σi) Ó= 0  for any ci ∈ Z and |ci| ≤ C  where Z
denotes the set of integers and C is a suﬃciently large number ≥ K. This condition is
satisﬁed with probability 1 if [λ1λ2 . . . λK]Û is drawn uniformly from K-dim simplex.

When the two conditions are satisﬁed  the sparsest solution is indeed the true solution as
stated in the following theorem:

Theorem 1. Suppose we are given y = Aλ and λ satisﬁes the “Signature” condition and
the “Linear Independence” condition. Then  λ is the unique solution to the program in (2).

The proof of Theorem 1 is given in the appendix. Next we describe the algorithm we propose
for recovery. The algorithm takes y and A and as the input and outputs λi (denotes λ(σi))
and A(σi) for every permutation σi in the support. The algorithm assumes the observed
values yd are sorted. Therefore  without loss of generality  assume that y1 < y2 < . . . < ym.
Then  the algorithm is as follows:

Algorithm:

Initialization: λ0 = 0  k(0) = 0 and A(σi)d = 0  1 ≤ i ≤ K and 1 ≤ d ≤ m.
for

d = 1 to m

if

else

yd = qi∈T λi
k(d) = k(d − 1) 
k(d) = k(d − 1) + 1 

end if

for some T ⊆ {1  . . .   k(d − 1)}

A(σi)d = 1 ∀ i ∈ T

λk(d) = yd 

A(σk(d))d = 1 

end for
Output K = k(m) and (λi  A(σi))  1 ≤ i ≤ K.
Now  we have the following theorem:

Theorem 2. Suppose we are given y = Aλ and λ satisﬁes the “signature” and the “linear
independence” conditions. Then  the above described algorithm recovers λ.

Theorem 2 is proved in the appendix. The algorithm we have described either succeeds
in ﬁnding a valid λ or else determines that the two properties are not satisﬁed. We now
show that the conditions we have imposed do not restrict the class of plausible models
severely. For this  we show that models drawn from the following generative model satisfy
the conditions with high probability.

Generative Model. Given K and an interval [a  b] on the positive real line  we generate a
choice model λ as follows: choose K permutations  σ1  σ2  . . .   σK  uniformly at random with
replacement  choose K numbers uniformly at random from the interval [a  b]  normalize the
numbers so that they sum to 1  and assign them to the permutations σi  1 ≤ i ≤ K. For all
other permutations σ Ó= σi  λ(σ) = 0. Note that  since we are choosing permutations in the
support with replacement  there could be repetitions. However  for large N and K ¹ N ! 
this happens with a vanishing probability.

Depending on the observed data  we characterize values of sparsity K for which distribu-
tions generated by the above generative model can be recovered with a high probability.
Speciﬁcally  we have the following theorem for the three forms of observed data mentioned
in Section 2. The proof may be found in the appendix.

Theorem 3. Suppose λ is a choice model of support size K drawn from the generative model.
Then  λ satisﬁes the “signature” and “linear independence” conditions with probability 1 −
o(1) as N → ∞ provided K = O(N ) for ranking data  K = o(log N ) for comparison data 
and K = o(√N ) for the top set data.

Of course  in general  the underlying choice model may not satisfy the two conditions we
have posited or be exactly recoverable from the observed data. In order to deal with this
more general scenario  we next propose an approach that implicitly identiﬁes a ‘worst-case’
distribution consistent with the observed data.

4

4 Robust Revenue Estimates Consistent with Data

In this section  we propose a general algorithm for the solution of program (3). This LP
has N ! variables and is clearly not amenable to direct solution; hence we consider its dual.
In preparation for taking the dual  let Aj(M)   {A(σ) : σ ∈ Sj(M)}  where  recall that 
Sj(M) denotes the set of all permutations that result in the purchase of j ∈ M when oﬀered
the assortment M. Since SN = ∪j∈MSj(M)  we have implicitly speciﬁed a partition of the
columns of the matrix A. Armed with this notation  the dual of (3) is:

(4) maximize

α ν

αÛy + ν

subject to

max

xj ∈Aj (M)

!αÛxj + ν" ≤ pj 

for each j ∈ M.

Our solution procedure will rely on an eﬀective representation of the sets Aj(M).
4.1 A Canonical Representation of Aj(M) and its Application
We assume that every set Sj(M) can be expressed as a disjoint union of Dj sets. We denote
the dth such set by Sjd(M) and let Ajd(M) be the corresponding set of columns. Consider
the convex hull of the set Ajd(M)  conv{Ajd(M)}   ¯Ajd(M). ¯Ajd(M) is by deﬁnition a
polytope contained in the m-dimensional unit cube  [0  1]m. In other words 

·

(5)

  bjd

·

1   Ajd

2 xjd = bjd

2   Ajd

1 xjd ≥ bjd

3 xjd ≤ bjd
3  

¯Ajd(M) = {xjd : Ajd
for appropriately deﬁned Ajd
. By a canonical representation of Aj(M)  we will thus un-
derstand a partition of Sj(M) and a polyhedral representation of the columns corresponding
to every set in the partition as given by (5). Ignoring the problem of actually obtaining this
representation for now  we assume access to a canonical representation and present a simple
program whose size is polynomial in the size of this representation that is equivalent to (3) 
(4). For simplicity of notation  we assume that each of the polytopes ¯Ajd(M) is in standard
xjd ≥ 0.}. Now since an aﬃne function is
form  i.e.
always optimized at the vertices of a polytope  we know:

xjd ≥ 0.}

¯Ajd(M) = {xjd : Ajd xjd = bjd 
xj ∈Aj (M)!αÛxj + ν" =

max

max

d xjd∈ ¯Ajd(M)!αÛxjd + ν" .

We have thus reduced (4) to a ‘robust’ LP. By strong duality we have:

(6)

max

xjd∈ ¯Ajd(M)!αÛxjd + ν"  

maximize

xjd

αÛxjd + ν

subject to Ajd xjd = bjd

xjd ≥ 0.

minimize

γjd

=

Û

bjd

γjd + ν

subject to γjd

Û

Ajd ≥ α

We have thus established the following useful equality:

;α  ν : max

xj ∈ ¯Aj (M)!αÛxj + ν" ≤ pj< = îα  ν : bjd

Û

γjd + ν ≤ pj  γjd

Û

Ajd ≥ α  d = 1  2  . . .   Djï .

It follows that solving (3) is equivalent to the following LP whose complexity is polynomial
in the description of our canonical representation:

(7)

maximize

α ν

subject to

αÛy + ν

Û

Û

bjd
γjd

γjd + ν ≤ pj
Ajd ≥ α

for all j ∈ M  d = 1  2  . . .   Dj
for all j ∈ M  d = 1  2  . . .   Dj.

Our ability to solve (7) relies on our ability to produce an eﬃcient canonical representation
of Sj(M).
In what follows  we ﬁrst consider an example where such a representation is
readily available  and then consider the general case.

Canonical Representation for Ranking Data: Recall the deﬁnition of ranking data
from Section 2. Consider partitioning Sj(M) into N sets wherein the dth set is given by

5

(8)

N −1

xjd
ri = 1

qi=0
xjd
qr=0
ri = 1
xjd
ri ∈ {0  1}
xjd
dj = 1
xjd
dÍi = 0

for 0 ≤ i ≤ N − 1
for 0 ≤ r ≤ N − 1
for 0 ≤ i  r ≤ N − 1.
for all i ∈ M  i Ó= jand 0 ≤ dÍ < d.

Sjd(M) = {σ ∈ Sj(M) : σ(j) = d}. It is not diﬃcult to show that the set Ajd(M) is equal
to the set of all vectors xjd in {0  1}N satisfying:

N −1

Our goal is  of course  to ﬁnd a description for ¯Ajd(M) of the type (5). Now consider
replacing the third (integrality) constraint in (8) with simply the non-negativity constraint
ri ≥ 0. It is clear that the resulting polytope contains ¯Ajd(M). In addition  one may
xjd
show that the resulting polytope has integral vertices since it is simply a matching polytope
with some variables forced to be integers  so that in fact the polytope is precisely ¯Ajd(M) 
and we have our canonical representation. Further  notice that this representation yields an
eﬃcient algorithm to solve (3) via (7)!

4.2 Computing a Canonical Representation: Comparison Data

Recall the deﬁnition of comparison data from Section 2. We use this data as an example to
illustrate a general procedure for computing a canonical representation. Consider Sj(M).
It is not diﬃcult to see that the corresponding set of columns Aj(M) is equal to the set of
vectors in {0  1}(N −1)N satisfying the following constraints:

(9)

kl − 1

ik + xj
ki = 1

xj
il ≥ xj
xj
ik + xj
xj
ji = 1
xj
ik ∈ {0  1}

for all i  k  l ∈ N   i Ó= k Ó= l
for all i  k ∈ N   i Ó= k
for all i ∈ M  i Ó= j
for all i  k ∈ N   i Ó= k

ik ≥ 0. Call this polytope ¯Ao
j (M) Ó= ¯Aj(M) in general. In this case we resort to the following procedure.

Brieﬂy  the second constraint follows since for any i  k  i Ó= k  either σ(i) > σ(k) or else σ(i) <
σ(k). The ﬁrst constraint enforces transitivity: σ(i) < σ(k) and σ(k) < σ(l) together imply
σ(i) < σ(l). The third constraint enforces that all σ ∈ Sj(M) must satisfy σ(j) < σ(i) for all
i ∈ M. Now consider the polytope obtained by relaxing the fourth (integrality) constraint
j (M) ⊇ ¯Aj(M).
to simply xj
Unlike the case of ranking data  however  ¯Ao
j (M) can in fact be shown to be non-integral 
so that ¯Ao
[1.] Solve (7) using the representation of ¯Ao
bound on (3) since ¯Ao
[2.] Solve the optimization problem max αÛ
j (M) for each j. If the
optimal solution ˆxj is integral for each j  then stop; the solution computed in the ﬁrst step
is in fact optimal.

j (M) ⊃ ¯Aj(M). Call the corresponding solution α(1)  ν(1).

j (M) in place of ¯Aj(M). This yields a lower

j (M). Of course  we must have ¯Ao

(1)xj subject to xj ∈ ¯Ao

[3.] Let ˆxj
ik be a non-integral variable. Partition Sj(M) on this variable - i.e. deﬁne
Sj1(M) = {σ : σ ∈ Sj(M)  σ(i) < σ(k)} and Sj2(M) = {σ : σ ∈ Sj(M)  σ(i) > σ(k)}.
Deﬁne outer-approximations to ¯Aj1(M) and ¯Aj2(M) as the projection of ¯Ao
ik = 1
and xj

j (M) on xj

ik = 0 respectively. Go to step 1.

The above procedure is ﬁnite  but the size of the LP we solve at each iteration doubles.
Nonetheless  each iteration produces a lower bound to (3) whose quality is easily measured
(for instance  by solving the maximization version of (3) using the same procedure)  and
this quality improves with each iteration. In our computational experiments with a related
type of data  it suﬃced to stop after a single iteration.

6

5 An Empirical Evaluation of the Approach

We have presented simple sub-routines to estimate the revenues R(M) from a particular oﬀer
set M  given marginal preference data y. These sub-routines are eﬀectively ‘non-parametric’
and can form the basis of a procedure that solves the revenue optimization problem posed in
the introduction. Here we seek to contrast this approach with a commonly used parametric
approach. We consider two types of observable data: ranking data and a ‘censored’ version
of the comparison data which gives us for every pair of products i  j Ó= 0  the fraction of
customers that prefer i to j  and in addition prefer i to 0 (i.e. not buying). The latter type
of data is quite realistic.

The parametric recipe we consider is the following: One ﬁts a Multinomial Logit (MNL)
model to the observable data and picks an optimal oﬀer set by evaluating R(M) =
qj∈M pj P(j|M) assuming P(·|M) follows the estimated model. The MNL is a commonly
used parametric model that associates with each product i in N a positive scalar wi; w0 = 1
by convention. The model assumes P(i|M) = wi/qj∈M wj. In place of making this para-
metric assumption  we could instead evaluate R(M) using the robust sub-routine developed
in the previous section and pick M to maximize this conservative estimate. It is clear that
if the MNL model is a poor ﬁt to the true choice model  P  our robust approach is likely
to outperform the parametric approach substantially.
Instead  what we focus on here is
what happens if the MNL model is a perfect ﬁt to the true choice model. In this case  the
parametric approach is the best possible. How sub-optimal is our non-parametric approach
here?

We consider an MNL model on N = 25 products. The model and prices were speciﬁed
using customer utilities for Amazon.com’s highest selling DVDs (and their prices) during a
3-month period from 1 July 2005 to 30 September 2005 estimated by [13] 1. We generate
synthetic observed data (of both the ranking type and the comparison type) according to this
ﬁtted MNL model. This represents a scenario where the ﬁtted MNL is a perfect descriptor
of reality. We conduct the following experiments:

Quality of Revenue Predictions: For each type of observable data we compute our
estimate of the minimum value that R(M) can take on  consistent with that data  by
solving (3). We compare this with the value of R(M) predicted under the MNL model
(which in this case  is exact). Figures 1(b) and 1(d) compare these two quantities for a
set of randomly chosen subsets M of the 25 potential DVD’s assuming ranking data and
the censored comparison data respectively. In both cases  our procedure produces excellent
predictions of expected revenue without making the assumptions on P(·|·) inherent in the
MNL model.

Quality of Optimal Solutions to Revenue Maximization Problems: For each type
of observable data  we compute optimal oﬀer sets M of varying capacities assuming the
ﬁtted MNL model and an optimization procedure described in [13]. We then evaluate the
revenue predictions for these optimal oﬀer sets by solving (3). Figures 1(a) and 1(c) plot
these estimates for the two types of observable data. The gap between the ‘MNL’ and the
‘MIN’ curves is thus an upper bound on the expected revenue loss if one used our non-
parametric procedure to pick an optimal oﬀer set M over the parametric procedure (which
in this setting is optimal). Again  we see that the revenue loss is surprisingly small.

6 Conclusion and Potential Future Directions

We have presented a general framework that allows us to answer questions related to how
consumers choose among alternatives using limited observable data and without making
additional parametric assumptions. The approaches we have proposed are feasible from
a data availability standpoint as well as a computational standpoint and provide a much
needed non-parametric ‘sub-routine’ for the revenue optimization problems described at the
outset. This paper also opens up the potential for a stream of future work.

1The problem of optimizing over M is particularly relevant to Amazon.com given limited screen

real-estate and cannibilization eﬀects

7

14
14
14

12
12
12

10
10
10

8
8
8

6
6
6

4
4
4

2
2
2

)
)
)
s
s
s
r
r
r
a
a
a

l
l
l
l
l
l

o
o
o
d
d
d
(
(
(
 
 
 

e
e
e
u
u
u
n
n
n
e
e
e
v
v
v
e
e
e
R
R
R
d
d
d
e
e
e

 
 
 

t
t
t
c
c
c
e
e
e
p
p
p
x
x
x
E
E
E

0
0
0

 
 
 
0
0
0

MNL Expected Revenue
MNL Expected Revenue
MNL Expected Revenue
MIN Expected Revenue
MIN Expected Revenue
MIN Expected Revenue

 
 
 

14

12

10

8

6

4

2

)
s
r
a

l
l

o
d
(
 

e
u
n
e
v
e
R
d
e

 

t
c
e
p
x
E

MNL Expected Revenue
MIN Expected Revenue

 

5
5
5

10
10
10

15
15
15

20
20
20

25
25
25

Optimal MNL assortment (size)
Optimal MNL assortment (size)
Optimal MNL assortment (size)

0

 
0

5

10
15
Assortment index

20

25

(a) Ranking Data: Optimal M

(b) Ranking Data: Random M

14
14
14
14

12
12
12
12

10
10
10
10

8
8
8
8

6
6
6
6

4
4
4
4

2
2
2
2

)
)
)
)
s
s
s
s
r
r
r
r
a
a
a
a

l
l
l
l
l
l
l
l

o
o
o
o
d
d
d
d
(
(
(
(
 
 
 
 

e
e
e
e
u
u
u
u
n
n
n
n
e
e
e
e
v
v
v
v
e
e
e
e
R
R
R
R
d
d
d
d
e
e
e
e

 
 
 
 

t
t
t
t
c
c
c
c
e
e
e
e
p
p
p
p
x
x
x
x
E
E
E
E

0
0
0
0

 
 
 
 
0
0
0
0

MNL Expected Revenue
MNL Expected Revenue
MNL Expected Revenue
MNL Expected Revenue
MIN Expected Revenue
MIN Expected Revenue
MIN Expected Revenue
MIN Expected Revenue

 
 
 
 

14
14

12
12

10
10

8
8

6
6

4
4

2
2

)
)
s
s
r
r
a
a

l
l
l
l

o
o
d
d
(
(
 
 

e
e
u
u
n
n
e
e
v
v
e
e
R
R
d
d
e
e

 
 

t
t
c
c
e
e
p
p
x
x
E
E

MNL Expected Revenue
MNL Expected Revenue
MIN Expected Revenue
MIN Expected Revenue

 
 

5
5
5
5

10
10
10
10

15
15
15
15

20
20
20
20

25
25
25
25

Optimal MNL assortment (size)
Optimal MNL assortment (size)
Optimal MNL assortment (size)
Optimal MNL assortment (size)

0
0

 
 
0
0

5
5

10
15
10
15
Assortment index
Assortment index

20
20

25
25

(c) Comparison Data: Optimal M

(d) Comparison Data: Random M

References

[1] K. Bartels  Y. Boztug  and M. M. Muller. Testing the multinomial logit model. Working

Paper  1999.

[2] R. Berinde  AC Gilbert  P. Indyk  H. Karloﬀ  and MJ Strauss. Combining geometry

and combinatorics: A uniﬁed approach to sparse signal recovery. Preprint  2008.

[3] E.J. Candes  J.K. Romberg  and T. Tao. Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics  59(8) 
2006.

[4] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for compressed sensing.

Lecture Notes in Computer Science  4056:280  2006.

[5] G. Debreu. Review of r.d. luce  ‘individual choice behavior: A theoretical analysis’.

American Economic Review  50:186–188  1960.

[6] G. Dobson and S. Kalish. Positioning and pricing a product line. Marketing Science 

7(2):107–125  1988.

[7] DL Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):

1289–1306  2006.

[8] J. L. Horowitz. Semiparametric estimation of a work-trip mode choice model. Journal

of Econometrics  58:49–70  1993.

[9] S. Jagabathula and D. Shah. Inferring rankings under constrained sensing. In NIPS 

pages 7–1  2008.

[10] D. McFadden. Econometric models for probabiistic choice among products. The Journal

of Business  53(3):S13–S29  1980.

[11] EH McKinney. Generalized birthday problem. American Mathematical Monthly  pages

385–387  1966.

[12] P. Rusmevichientong  B. Van Roy  and P. Glynn. A nonparametric approach to multi-

product pricing. Operations Research  54(1)  2006.

[13] P. Rusmevichientong  ZJ Shen  and D.B. Shmoys. Dynamic Assortment Optimization
with a Multinomial Logit Choice Model and Capacity Constraint. Technical report 
Working Paper  2008.

[14] Kalyan T. Talluri and Garrett J. van Ryzin. The Theory and Practice of Revenue

Management. Springer Science+Business Media  2004.

8

,Kevin Jamieson
Daniel Haas
Benjamin Recht