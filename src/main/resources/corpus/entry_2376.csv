2018,Graphical model inference: Sequential Monte Carlo meets deterministic approximations,Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences  but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency  but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically  we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable  we show explicitly how this can be done with loopy belief propagation  expectation propagation  and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and  indeed  numerical results show clear improvements over the baseline deterministic methods as well as over "plain" SMC.,Graphical model inference: Sequential Monte Carlo

meets deterministic approximations

Department of Information Technology

Department of Science and Technology

Fredrik Lindsten

Uppsala University
Uppsala  Sweden

Jouni Helske

Linköping University
Norrköping  Sweden

fredrik.lindsten@it.uu.se

jouni.helske@liu.se

Matti Vihola

Department of Mathematics and Statistics

University of Jyväskylä

Jyväskylä  Finland

matti.s.vihola@jyu.fi

Abstract

Approximate inference in probabilistic graphical models (PGMs) can be grouped
into deterministic methods and Monte-Carlo-based methods. The former can often
provide accurate and rapid inferences  but are typically associated with biases
that are hard to quantify. The latter enjoy asymptotic consistency  but can suffer
from high computational costs. In this paper we present a way of bridging the
gap between deterministic and stochastic inference. Speciﬁcally  we suggest an
efﬁcient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage
the output from deterministic inference methods. While generally applicable  we
show explicitly how this can be done with loopy belief propagation  expectation
propagation  and Laplace approximations. The resulting algorithm can be viewed as
a post-correction of the biases associated with these methods and  indeed  numerical
results show clear improvements over the baseline deterministic methods as well
as over “plain” SMC.

1

Introduction

Probabilistic graphical models (PGMs) are ubiquitous in machine learning for encoding dependencies
in complex and high-dimensional statistical models [18]. Exact inference over these models is
intractable in most cases  due to non-Gaussianity and non-linear dependencies between variables.
Even for discrete random variables  exact inference is not possible unless the graph has a tree-topology 
due to an exponential (in the size of the graph) explosion of the computational cost. This has resulted
in the development of many approximate inference methods tailored to PGMs. These methods can
roughly speaking be grouped into two categories: (i) methods based on deterministic (and often
heuristic) approximations  and (ii) methods based on Monte Carlo simulations.
The ﬁrst group includes methods such as Laplace approximations [30]  expectation propagation [23] 
loopy belief propagation [26]  and variational inference [36]. These methods are often promoted as
being fast and can reach higher accuracy than Monte-Carlo-based methods for a ﬁxed computational
cost. The downside  however  is that the approximation errors can be hard to quantify and even if the
computational budget allows for it  simply spending more computations to improve the accuracy can
be difﬁcult. The second group of methods  including Gibbs sampling [28] and sequential Monte Carlo
(SMC) [11  24]  has the beneﬁt of being asymptotically consistent. That is  under mild assumptions

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

they can often be shown to converge to the correct solution if simply given enough compute time. The
problem  of course  is that “enough time” can be prohibitively long in many situations  in particular if
the sampling algorithms are not carefully tuned.
In this paper we propose a way of combining deterministic inference methods with SMC for inference
in general PGMs expressed as factor graphs. The method is based on a sequence of artiﬁcial target
distributions for the SMC sampler  constructed via a sequential graph decomposition. This approach
has previously been used by [24] for enabling SMC-based inference in PGMs. The proposed method
has one important difference however; we introduce a so called twisting function in the targets
obtained via the graph decomposition which allows for taking dependencies on “future” variables
of the sequence into account. Using twisted target distributions for SMC has recently received
signiﬁcant attention in the statistics community  but to our knowledge  it has mainly been developed
for inference in state space models [14  16  34  31]. We extend this idea to SMC-based inference in
general PGMs  and we also propose a novel way of constructing the twisting functions  as described
below. We show in numerical illustrations that twisting the targets can signiﬁcantly improve the
performance of SMC for graphical models.
A key question when using this approach is how to construct efﬁcient twisting functions. Computing
the optimal twisting functions boils down to performing exact inference in the model  which is
assumed to be intractable. However  this is where the use of deterministic inference algorithms comes
into play. We show how it is possible to compute sub-optimal  but nevertheless efﬁcient  twisting
functions using some popular methods—Laplace approximations  expectation propagation and loopy
belief propagation. Furthermore  the framework can easily be used with other methods as well  to
take advantage of new and more efﬁcient methods for approximate inference in PGMs.
The resulting algorithm can be viewed as a post-correction of the biases associated with the deter-
ministic inference method used  by taking advantage of the rigorous convergence theory for SMC
(see e.g.  [9]). Indeed  the approximation of the twisting functions only affect the efﬁciency of the
SMC sampler  not its asymptotic consistency  nor the unbiasedness of the normalizing constant
estimate (which is a key merit of SMC samplers). An implication of the latter point is that the
resulting algorithm can be used together with pseudo-marginal [1] or particle Markov chain Monte
Carlo (MCMC) [3] samplers  or as a post-correction of approximate MCMC [34]. This opens up the
possibility of using well-established approximate inference methods for PGMs in this context.
Additional related work: An alternative approach to SMC-based inference in PGMs is to make use
of tempering [10]. For discrete models  [15] propose to start with a spanning tree to which edges are
gradually added within an SMC sampler to recover the original model. This idea is extended by [6] by
deﬁning the intermediate targets based on conditional mean ﬁeld approximations. Contrary to these
methods our approach can handle both continuous and/or non-Gaussian interactions  and does not rely
on intermediate MCMC steps within each SMC iteration. When it comes to combining deterministic
approximations and Monte-Carlo-based inference  previous work has largely been focused on using
the approximation as a proposal distribution for importance sampling [13] or MCMC [8]. Our method
has the important difference that we do not only use the deterministic approximation to design the
proposal  but also to select the intermediate SMC targets via the design of efﬁcient twisting functions.

2 Setting the stage

2.1 Problem formulation

Let π(x1:T ) denote a distribution of interest over a collection of random variables x1:T =
{x1  . . .   xT}. The model may also depend on some “top-level” hyperparameters  but for brevity
we do not make this dependence explicit. In Bayesian statistics  π would typically correspond to a
posterior distribution over some latent variables given observed data. We assume that there is some
structure in the model which is encoded in a factor graph representation [20] 

(cid:89)

j∈F

π(x1:T ) =

1
Z

fj(xIj ) 

(1)

where F denotes the set of factors  I := {1  . . .   T} is the set of variables  Ij denotes the index set
of variables on which factor fj depends  and xIj := {xt : t ∈ Ij}. Note that Ij = Ne(j) is simply
the set of neighbors of factor fj in the graph (recall that in a factor graph all edges are between factor

2

nodes and variable nodes). Lastly  Z is the normalization constant  also referred to as the partition
function of the model  which is assumed to be intractable. The factor graph is a general representation
of a probabilistic graphical model and both directed and undirected PGMs can be written as factor
graphs. The task at hand is to approximate the distribution π(x1:T )  as well as the normalizing
constant Z. The latter plays a key role  e.g.  in model comparison and learning of top-level model
parameters.

2.2 Sequential Monte Carlo

Sequential Monte Carlo (SMC  see  e.g.  [11]) is a class of importance-sampling-based algorithms
that can be used to approximate some  quite arbitrary  sequence of probability distributions of interest.
Let

πt(x1:t) =

γt(x1:t)

Zt

 

t = 1  . . .   T 

be a sequence of probability density functions deﬁned on spaces of increasing dimension  where
γt can be evaluated point-wise and Zt is a normalizing constant. SMC approximates each πt by a
collection of N weighted particles {(xi

i=1  generated according to Algorithm 1.

t)}N

1:t  wi

Algorithm 1 Sequential Monte Carlo (all steps are for i = 1  . . .   N)

1.

1/(cid:80)N
j=1 (cid:101)wj
1 = (cid:101)wi
i=1 with probabilities {νi
t}N
t−1}N
i=1.
t/(cid:80)N
t}.
j=1 (cid:101)wj
1:t−1  xi
t .

1:t = {xai
t = (cid:101)wi

t

1:t−1) and set xi
t−1/νai
t−1 and wi

t

t

1 ∼ q1(x1)  set (cid:101)wi

1. Sample xi
2. for t = 2  . . .   T :

1 = γ1(xi

1)/q1(xi

1) and wi

(a) Resampling: Simulate ancestor indices {ai
(b) Propagation: Simulate xi

(c) Weighting: Compute (cid:101)wi

t ∼ qt(xt|xai
1:t)wai
t = ωt(xi

t

t−1}N

In step 2(a) we use arbitrary resampling weights {νi
i=1  which may depend on all variables
generated up to iteration t − 1. This allows for the use of look-ahead strategies akin to the auxiliary
particle ﬁlter [27]  as well as adaptive resampling based on effective sample size (ESS) [19]: if the
ESS is below a given threshold  say N/2  set νi
t−1 to resample according to the importance
t−1 ≡ 1/N which  together with the use of a low-variance (e.g.  stratiﬁed)
weights. Otherwise  set νi
resampling method  effectively turns the resampling off at iteration t.
At step 2(b) the particles are propagated forward by simulating from a user-chosen proposal distri-
bution qt(xt|x1:t−1)  which may depend on the complete history of the particle path. The locally
optimal proposal  which minimizes the conditional weight variance at iteration t  is given by

t−1 = wi

qt(xt|x1:t−1) ∝ γt(x1:t)/γt−1(x1:t−1)

t−1 ∝(cid:82) γt({xi

(2)
for t ≥ 2 and q1(x1) ∝ γ1(x1). If  in addition to using the locally optimal proposal  the resampling
weights are computed as νi
1:t−1)  then the SMC sampler is said
to be fully adapted. At step 2(c) new importance weights are computed using the weight function
ωt(x1:t) = γt(x1:t)/ (γt−1(x1:t−1)qt(xt|x1:t−1)) .
The weighted particles generated by Algorithm 1 can be used to approximate each πt by the empirical
(dx1:t). Furthermore  the algorithm provides unbiased estimates of the
; see [9] and the supplementary

distribution(cid:80)N
normalizing constants Zt  computed as (cid:98)Zt =(cid:81)t

1:t−1  xt})dxt/γt−1(xi

(cid:80)N
i=1 (cid:101)wi

(cid:110) 1

i=1 wi

(cid:111)

tδxi

s=1

N

1:t

s

material.

3 Graph decompositions and twisted targets

We now turn our attention to the factor graph (1). To construct a sequence of target distributions for
an SMC sampler  [24] proposed to decompose the graphical model into a sequence of sub-graphs 
each deﬁning an intermediate target for the SMC sampler. This is done by ﬁrst ordering the variables 
or the factors  of the model in some way—here we assume a ﬁxed order of the variables x1:T as

3

indicated by the notation; see Section 5 for a discussion about the ordering. We then deﬁne a
sequence of unnormalized densities {γt(x1:t)}T
t=1 by gradually including the model variables and
the corresponding factors. This is done in such a way that the ﬁnal density of the sequence includes
all factors and coincides with the original target distribution of interest 

γT (x1:T ) =

fj(xIj ) ∝ π(x1:T ).

(3)

(cid:89)

j∈F

We can then target {γt(x1:t)}T

jectories can be taken as (weighted) samples from π  and (cid:98)Z := (cid:98)ZT will be an unbiased estimate

t=1 with an SMC sampler. At iteration T the resulting particle tra-

of Z.
To deﬁne the intermediate densities  let F1  . . .   FT be a partitioning of the factor set F deﬁned by:

Ft = {j ∈ F : t ∈ Ij  t + 1 /∈ Ij  . . .   T /∈ Ij}.

In words  Ft is the set of factors depending on xt  and possibly x1:t−1  but not xt+1:T . Furthermore 
let Ft = (cid:116)t

s=1Fs. Naesseth et al. [24] deﬁned a sequence of intermediate target densities as1

γt(x1:t) =

fj(xIj ) 

t = 1  . . .   T.

(4)

(cid:89)

j∈Ft

(cid:89)

γψ
t (x1:t) := ψt(x1:t)γt(x1:t) = ψt(x1:t)

Since FT = F  it follows that the condition (3) is satisﬁed. However  even though this is a valid
choice of target distributions  leading to a consistent SMC algorithm  the resulting sampler can
have poor performance. The reason is that the construction (4) neglects the dependence on “future”
variables xt+1:T which may have a strong inﬂuence on x1:t. Neglecting this dependence can result
in samples at iteration t which provide an accurate approximation of the intermediate target γt  but
which are nevertheless very unlikely under the actual target distribution π.
To mitigate this issue we propose to use a sequence of twisted intermediate target densities 
t = 1  . . .   T − 1 

where ψt(x1:t) is an arbitrary positive “twisting function” such that(cid:82) γψ

t (x1:t)dx1:t < ∞. (Note
that there is no need to explicitly compute this integral as long as it can be shown to be ﬁnite.)
Twisting functions have previously been used by [14  16] to “twist” the Markov transition kernel of a
state space (or Feynman-Kac) model; we take a slightly different viewpoint and simply consider the
twisting function as a multiplicative adjustment of the SMC target distribution.
The deﬁnition of the twisted targets in (5) is of course very general and not very useful unless
additional guidance is provided. To this end we state the following simple optimality condition (the
proof is in the supplementary material; see also [14  Proposition 2]).
Proposition 1. Assume that the twisting functions in (5) are given by

fj(xIj ) 

j∈Ft

(5)

ψ∗
t (x1:t) :=

fj(xIj )dxt+1:T

t = 1  . . .   T − 1 

(6)

(cid:90) (cid:89)

j∈F\Ft

that the locally optimal proposals (2) are used in the SMC sampler  and that νi
t. Then 
Algorithm 1 results in particle trajectories exactly distributed according to π(x1:T ) and the estimate

of the normalizing constant is exact; (cid:98)Z = Z w.p.1.

t = wi

Clearly  the optimal twisting functions are intractable in all situations of interest. Indeed  computing
(6) essentially boils down to solving the original inference problem. However  guided by this  we
will strive to select ψt(x1:t) ≈ ψ∗
t (x1:t). As pointed out above  the approximation error  here  only
Various ways for approximating ψ∗

affects the efﬁciency of the SMC sampler  not its asymptotic consistency or the unbiasedness of (cid:98)Z.

t are discussed in the next section.

1More precisely  [24] use a ﬁxed ordering of the factors (and not the variables) of the model. They then
include one or more additional factors  together with the variables on which these factors depend  in each step of
the SMC algorithm. This approach is more or less equivalent to the one adopted here.

4

4 Twisting functions via deterministic approximations

In this section we show how a few popular deterministic inference methods can be used to approximate
the optimal twisting functions in (6)  namely loopy belief propagation (Section 4.1)  expectation
propagation (Section 4.2)  and Laplace approximations (Section 4.3). These methods are likely to be
useful for computing the twisting functions in many situations  however  we emphasize that they are
mainly used to illustrate the general methodology which can be used with other inference procedures
as well.

4.1 Loopy belief propagation

Belief propagation [26] is an exact inference procedure for tree-structured graphical models  although
its “loopy” version has been used extensively as a heuristic approximation for general graph topologies.
Belief propagation consists of passing messages:

In graphs with loops  the messages are passed until convergence.
To see how loopy belief propagation can be used to approximate the twisting functions for SMC  we
start with the following result for tree-structured model (the proof is in the supplementary material).
Proposition 2. Assume that the factor graph with variable nodes {1  . . .   t} and factor nodes
{fj : j ∈ Ft} form a (connected) tree for all t = 1  . . .   T . Then  the optimal twisting function (6)
is given by

µj→(1:t)(x1:t)

where

µj→(1:t)(x1:t) =

µj→s(xs).

(7)

ψ∗
t (x1:t) =

(cid:89)

j∈F\Ft

(cid:89)

s∈{1  ...  t}∩Ij

Remark 1. The sub-tree condition of Proposition 2 implies that the complete model is a tree  since this
is obtained for t = T . The connectedness assumption can easily be enforced by gradually growing
the tree  lumping model variables together if needed.
While the optimality of (7) only holds for tree-structured models  we can still make use of this
expression for models with cycles  analogously to loopy belief propagation. Note that the message
µj→(1:t)(x1:t) is the product of factor-to-variable messages going from the non-included factor
j ∈ F \ Ft to included variables s ∈ {1  . . .   t}. For a tree-based model there is at most one
such message (under the connectedness assumption of Proposition 2)  whereas for a cyclic model
µj→(1:t)(x1:t) might be the product of several “incoming” messages.
It should be noted that the numerous modiﬁcations of the loopy belief propagation algorithm that are
available can be used within the proposed framework as well. In fact  methods based on tempering of
the messages  such as tree-reweighting [35]  could prove to be particularly useful. The reason is that
these methods counteract the double-counting of information in classical loopy belief propagation 
which could be problematic for the following SMC sampler due to an over-concentration of probability
mass. That being said  we have found that even the standard loopy belief propagation algorithm
can result in efﬁcient twisting  as illustrated numerically in Section 6.1  and we do not pursue
message-tempering further in this paper.

4.2 Expectation propagation

Expectation propagation (EP  [23]) is based on introducing approximate factors  (cid:101)fj(xIj ) ≈ fj(xIj )

such that

approximates π(x1:T )  and where the (cid:101)fj’s are assumed to be simple enough so that the integral in the

expression above is tractable. The approximate factors are updated iteratively until some convergence

(8)

(cid:101)π(x1:T ) =

(cid:81)
j∈F (cid:101)fj(xIj )
(cid:82)(cid:81)
j∈F (cid:101)fj(xIj )dx1:T

Factor → variable :

Variable → factor :

µj→s(xs) =

λs→j(xs) =

(cid:89)

fj(xIj )

(cid:89)

i∈Ne(s)\{j}

u∈Ne(j)\{s}

µi→s(xs).

λu→j(xu)dxIj\{s} 

(cid:90)

5

the Kullback–Leibler divergence between the two distributions. We refer to [23] for additional details
on the EP algorithm.
Once the EP approximation has been computed  it can naturally be used to approximate the optimal

criterion is met. To update factor (cid:101)fj  we ﬁrst remove it from the approximation to obtain the so called
cavity distribution(cid:101)π−j(x1:T ) ∝(cid:101)π(x1:T )/(cid:101)fj(xIj ). We then compute a new approximate factor (cid:101)fj 
such that (cid:101)fj(xIj )(cid:101)π−j(x1:T ) approximates fj(xIj )(cid:101)π−j(x1:T ). Typically  this is done by minimizing
twisting functions in (6). By simply plugging in (cid:101)fj in place of fj we get
(cid:101)fj(xIj )dxt+1:T .
(cid:82)(cid:81)
j∈F\Ft (cid:101)fj(xIj )dxt+1:T
(cid:82)(cid:81)
j∈F\Ft−1 (cid:101)fj(xIj )dxt:T
(cid:89)
(cid:101)fj(xIj )

Furthermore  the EP approximation can be used to approximate the optimal SMC proposal. Speciﬁ-
cally  at iteration t we can select the proposal distribution as

qt(xt|x1:t−1) =(cid:101)π(xt|x1:t−1) =

(cid:90) (cid:89)
(cid:89)

j∈Ft

This choice has the advantage that the weight function gets a particularly simple form:

(cid:101)fj(xIj )

fj(xIj )

.

t−1(x1:t−1)qt(xt|x1:t−1)
γψ

=

j∈Ft

.

(10)

γψ
t (x1:t)

ψt(x1:t) =

j∈F\Ft

ωt(x1:t) =

(11)

(9)

4.3 Laplace approximations for Gaussian Markov random ﬁelds

A speciﬁc class of PGMs with a large number of applications in spatial statistics are latent Gaussian
Markov random ﬁelds (GMRFs  see  e.g.  [29  30]). These models are deﬁned via a Gaussian prior
p(x1:T ) = N (x1:T|µ  Q−1) where the precision matrix Q has Qij (cid:54)= 0 if and only if variables xi
and xj share a factor in the graph. When this latent ﬁeld is combined with some non-Gaussian or non-
linear observational densities p(yt|xt)  t = 1  . . .   T   the posterior π(x1:T ) is typically intractable.
However  when p(yt|xt) is twice differentiable  it is straightforward to ﬁnd an approximating Gaussian
model based on a Laplace approximation by simple numerical optimization [12  33  30]  and use the
obtained model as a basis of twisted SMC. Speciﬁcally  we use

(cid:90)

T(cid:89)

(cid:8)(cid:101)p(ys|xs)(cid:9)p(xt+1:T|x1:t)dxt+1:T  

ψt(x1:t) =

where(cid:101)p(yt|xt) ≈ p(yt|xt)  t = 1  . . .   T are the Gaussian approximations obtained using Laplace’s
method. For proposal distributions  we simply use the obtained Gaussian densities(cid:101)p(xt|x1:t−1  y1:T ).
The weight functions have similar form as in (11)  ωt(x1:t) = p(yt|xt)/(cid:101)p(yt|xt). For state space

s=t+1

models  this approach was recently used in [34].

(12)

5 Practical considerations

A natural question is how to order the variables of the model. In a time series context a trivial
processing order exists  but it is more difﬁcult to ﬁnd an appropriate order for a general PGM.
However  in Section 6.3 we show numerically that while the processing order has a big impact on the
performance of non-twisted SMC  the effect of the ordering is less severe for twisted SMC. Intuitively
this can be explained by the look-ahead effect of the twisting functions: even if the variables are
processed in a non-favorable order they will not “come as a surprise”.
Still  intuitively a good candidate for the ordering is to make the model as “chain-like” as possible by
minimizing the bandwidth (see  e.g.  [7]) of the adjacency matrix of the graphical model. A related
strategy is to instead minimize the ﬁll-in of the Cholesky decomposition of the full posterior precision
matrix. Speciﬁcally  this is recommended in the GMRF setting for faster matrix algebra [29] and this
is the approach we use in Section 6.3. Alternatively  [25] propose a heuristic method for adaptive
order selection that can be used in the context of twisted SMC as well.

6

Application of twisting often leads to nearly constant SMC weights and good performance. However 
the boundedness of the SMC weights is typically not guaranteed.
Indeed  the approximations
may have lighter tails than the target  which may occasionally lead to very large weights. This
is particularly problematic when the method is applied within a pseudo-marginal MCMC scheme 
because unbounded likelihood estimators lead to poor mixing MCMC [1  2]. Fortunately  it is
relatively easy to add a ‘regularization’ to the twisting  which leads to bounded weights. We discuss
the regularization in more detail in the supplement.
Finally  we comment on the computational cost of the proposed method. Once a sequence of
twisting functions has been found  the cost of running twisted SMC is comparable to that of running
non-twisted SMC. Thus  the main computational overhead comes from executing the deterministic
inference procedure used for computing the twisting functions. Since the cost of this is independent
of the number of particles N used for the subsequent SMC step  the relative computational overhead
will diminish as N increases. As for the scaling with problem size T   this will very much depend
on the choice of deterministic inference procedure  as well as on the connectivity of the graph  as
is typical for graphical model inference. It is worth noting  however  that even for a sparse graph
the SMC sampler needs to be efﬁciently implemented to obtain a favorable scaling with T . Due to
the (in general) non-Markovian dependencies of the random variables x1:T   it is necessary to keep
track of the complete particle trajectories {xi
i=1 for each t = 1  . . .   T . Resampling of these
trajectories can however result in the copying of large chunks of memory (of the order N t at iteration
t)  if implemented in a ’straightforward manner’. Fortunately  it is possible to circumvent this issue
by an efﬁcient storage of the particle paths  exploiting the fact that the paths tend to coalesce in log N
steps; see [17] for details. We emphasize that this issue is inherent to the SMC framework itself 
when applied to non-Markovian models  and does not depend on the proposed twisting method.

1:t}N

6 Numerical illustration

We illustrate the proposed twisted SMC method on three PGMs using the three deterministic approxi-
mation methods discussed in Section 4. In all examples we compare with the baseline SMC algorithm
by [24] and the two samplers are denoted as SMC-Twist and SMC-Base  respectively. While the
methods can be used to estimate both the normalizing constant Z and expectations with respect to
π  we focus the empirical evaluation on the former. The reasons for this are: (i) estimating Z is of
signiﬁcant importance on its own  e.g.  for model comparison and for pseudo-marginal MCMC  (ii) in
our experience  the accuracy of the normalizing constant estimate is a good indicator for the accuracy
of other estimates as well  and (iii) the fact that SMC produces unbiased estimates of Z means that

we can more easily assess the quality of the estimates. Speciﬁcally  log(cid:98)Z—which is what we actually

compute—is negatively biased and it therefore typically holds that higher estimates are better.

Ising model

6.1
As a ﬁrst proof of concept we consider a 16 × 16 square
lattice Ising model with periodic boundary condition 

(cid:18) (cid:88)

(i j)∈E

(cid:19)

(cid:88)

i∈I

π(x1:T ) =

1
Z

exp

Jijxixj +

Hixi

.

i.i.d.∼ Uniform(−1  1).

where T = 256 and xi ∈ {−1  +1}. We let the inter-
actions be Jij ≡ 0.44 and the external magnetic ﬁeld is
simulated according to Hi
We use the Left-to-Right sequential decomposition consid-
ered by [24]. For SMC-Twist we use loopy belief prop-
agation to compute the twisting potentials  as described
in Section 4.1. Both SMC-Base and SMC-Twist use fully
adapted proposals  which is possible due to the discrete na-
ture of the problem. Apart for the computational overhead
of running the belief propagation algorithm (which is quite
small  and independent of the number of particles used in
the subsequent SMC algorithm)  the computational costs
of the two SMC samplers is more or less the same.

7

Figure 1: Results for the Ising model.
See text for details.

252254256258642561024Number of particlesLog ZMethodSMC−PGMSMC−TwistFigure 2: Results for LDA likelihood evaluation for the toy model (left)  PubMed data (mid)  and 20
newsgroups data (right). Dotted lines correspond to the plain EP estimates. See text for details.

Each algorithm is run 50 times for varying number of particles. Box-plots over the obtained
normalizing constant estimates are shown in Figure 1  together with a “ground truth” estimate (dashed
line) obtained with an annealed SMC sampler [10] with a large number of particles and temperatures.
As is evident from the ﬁgure  the twisted SMC sampler outperforms the baseline SMC. Indeed  with
twisting we get similar accuracy using N = 64 particles  as the baseline SMC with N = 1024
particles.

6.2 Topic model evaluation

Topic models  such as latent Dirichlet allocation (LDA) [4]  are widely used for information retrieval
from large document collections. To assess the quality of a learned model it is common to evaluate
the likelihood of a set of held out documents. However  this turns out to be a challenging inference
problem on its own which has attracted signiﬁcant attention [37  5  32  22]. Naesseth et al. [24]
obtained good performance for this problem with a (non-twisted) SMC method  outperforming the
special purpose Left-Right-Sequential sampler by [5]. Here we repeat this experiment and compare
this baseline SMC with a twisted SMC. For computing the twisting functions we use the EP algorithm
by Minka and Lafferty [22]  speciﬁcally developed for inference in the LDA model. See [37  22] and
the supplementary material for additional details on the model and implementation details.
First we consider a synthetic toy model with 4 topics and 10 words  for which the exact likelihood
can be computed. Figure 2 (left) shows the mean-squared errors in the estimates of the log-likelihood
estimates for the two SMC samplers as we increase the number of particles. As can be seen  twisting
reduces the error by about half an order-of-magnitude compared to the baseline SMC. In the middle
and right panels of Figure 2 we show results for two real datasets  PubMed Central abstracts and 20
newsgroups  respectively (see [37]). For each dataset we compute the log-likelihood of 10 held-out
documents. The box-plots are for 50 independent runs of each algorithm  for different number
of particles. As pointed out above  due to the unbiasedness of the SMC likelihood estimates it is
typically the case that “higher is better”. This is also supported by the fact that the estimates increase
on average as we increase the number of particles. With this in mind  we see that EP-based twisting
signiﬁcantly improves the performance of the SMC algorithm. Furthermore  even with as few as 50
particles  SMC-Twist clearly improves the results of the EP algorithm itself  showing that twisted
SMC can successfully correct for the bias of the EP method.

6.3 Conditional autoregressive model with Binomial observations
Consider a latent GMRF x1:T ∼ N (0  τ Q−1)  where Qtt = nt + d  Qtt(cid:48) = −1 if t ∼ t(cid:48)  and
Qtt(cid:48) = 0 otherwise. Here nt is the number of neighbors of xt  τ = 0.1 is a scaling parameter  and
d = 1 is a regularization parameter ensuring a positive deﬁnite precision matrix. Given the latent
ﬁeld we assume binomial observations yt ∼ Binomial(10  logit−1(xt)). The spatial structure of the
GMRF corresponds to the map of Germany obtained from the R package INLA [21]  containing
T = 544 regions. We simulated one realization of x1:T and y1:T from this conﬁguration and then
estimated the log-likelihood of the model 10 000 times with a baseline SMC using a bootstrap
proposal  as well as with twisted SMC where the twisting functions were computed using a Laplace

8

MethodSMC−BaseSMC−Twist0.0010.0100.100−9000−8950−8900−8850−8800−13800−13700−136001101001000502501000502501000Number of particlesNumber of particlesNumber of particlesMSE in log ZLog ZLog ZMedlineNewsgroupsSynthetic dataFigure 3: Results for GMRF likelihood evaluation. See text for details.

approximation (see details in the supplementary material). To test the sensitivity of the algorithms to
the ordering of the latent variables  we randomly permuted the variables for each replication. We
compare this random order with approximate minimum degree reordering (AMD) of the variables 
applied before running the SMC. We also varied N  the number of particles  from 64 up to 1024. For
both SMC approaches  we used adaptive resampling based on effective sample size with threshold of
N/2. In addition  we ran a twisted sequential importance sampler (SIS)  i.e.  we set the resampling
threshold to zero.
Figure 3 shows the log-likelihood estimates for SMC-Base  SIS and SMC-Twist with N = 64 and
N = 1024 particles  with dashed lines corresponding to the estimates obtained from a single SMC-
Twist run with 100 000 particles  and dotted lines to the estimates from the Laplace approximation.
SMC-Base is highly affected by the ordering of the variables  while the effect is minimal in case of
SIS and SMC-Twist. Twisted SMC is relatively accurate already with 64 particles  whereas sequential
importance sampling and SMC-Base exhibit large variation and bias still with 1024 particles.

7 Conclusions

The twisted SMC method for PGMs presented in this paper is a promising way to combine deter-
ministic approximations with efﬁcient Monte Carlo inference. We have demonstrated how three
well-established methods can be used to approximate the optimal twisting functions  but we stress
that the general methodology is applicable also with other methods.
An important feature of our approach is that it may be used as ‘plug-in’ module with pseudo-marginal
[1] or particle MCMC [3] methods  allowing for consistent hyperparameter inference. It may also
be used as (parallelizable) post-processing of approximate hyperparameter MCMC  which is based
purely on deterministic PGM inferences [cf. 34].
An interesting direction for future work is to investigate which properties of the approximations
that are most favorable to the SMC sampler. Indeed  it is not necessarily the case that the twisting
functions obtained directly from the most accurate deterministic method result in the most efﬁcient
SMC sampler. It is also interesting to consider iterative reﬁnements of the twisting functions  akin to
the method proposed by [14]  in combination with the approach taken here.

Acknowledgments

FL has received support from the Swedish Foundation for Strategic Research (SSF) via the project
Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015) and
from the Swedish Research Council (VR) via the projects Learning of Large-Scale Probabilistic
Dynamical Models (contract number: 2016-04278) and NewLEADS – New Directions in Learning
Dynamical Systems (contract number: 621-2016-06079). JH and MV have received support from the
Academy of Finland (grants 274740  284513 and 312605).

9

OrderingAMDrandom−3330−3320−3310−3300−3290−3305−3300−3295−3290SISSMC−baseSMC−twistSISSMC−baseSMC−twistMethodMethodLog ZLog ZN = 1024N = 64References
[1] C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efﬁcient Monte Carlo computations.

The Annals of Statistics  37(2):697–725  2009.

[2] C. Andrieu and M. Vihola. Convergence properties of pseudo-marginal Markov chain Monte Carlo

algorithms. The Annals of Applied Probability  25(2):1030–1077  2015.

[3] C. Andrieu  A. Doucet  and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal of the

Royal Statistical Society: Series B  72(3):269–342  2010.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research 

3:993–1022  2003. ISSN 1532-4435.

[5] W. Buntine. Estimating likelihoods for topic models. In Proceedings of the 1st Asian Conference on

Machine Learning: Advances in Machine Learning  2009.

[6] Peter Carbonetto and Nando D. Freitas. Conditional mean ﬁeld. In Advances in Neural Information

Processing Systems (NIPS) 19  pages 201–208. 2006.

[7] E. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In Proceedings of the

1969 24th National Conference  1969.

[8] N. de Freitas  P. Højen-Sørensen  M. I. Jordan  and S. Russell. Variational MCMC. In Proceedings of the

17th Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 120–127  2001.

[9] P. Del Moral. Feynman-Kac Formulae - Genealogical and Interacting Particle Systems with Applications.

Probability and its Applications. Springer  2004.

[10] P. Del Moral  A. Doucet  and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical

Society: Series B  68(3):411–436  2006.

[11] A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later. In D. Crisan
and B. Rozovskii  editors  The Oxford Handbook of Nonlinear Filtering  pages 656–704. Oxford University
Press  Oxford  UK  2011.

[12] J. Durbin and S. J. Koopman. Monte Carlo maximum likelihood estimation for non-Gaussian state space

models. Biometrika  84(3):669–684  1997. doi: 10.1093/biomet/84.3.669.

[13] Zoubin Ghahramani and Matthew J. Beal. Variational inference for Bayesian mixtures of factor analysers.

In Advances in Neural Information Processing Systems (NIPS) 12  pages 449–455. 1999.

[14] P. Guarniero  A. M. Johansen  and A. Lee. The iterated auxiliary particle ﬁlter. Journal of the American

Statistical Association  112(520):1636–1647  2017.

[15] F. Hamze and N. de Freitas. Hot coupling: A particle approach to inference and normalization on pairwise
undirected graphs. In Advances in Neural Information Processing Systems (NIPS) 18  pages 491–498.
2005.

[16] J. Heng  A. N. Bishop  G. Deligiannidis  and A. Doucet. Controlled sequential Monte Carlo. arXiv.org 

arXiv:1708.08396  2018.

[17] P. E. Jacob  L. M. Murray  and S. Rubenthaler. Path storage in the particle ﬁlter. Statistics and Computing 

25(2):487–496  2015. doi: 10.1007/s11222-013-9445-x.

[18] M. I. Jordan. Graphical models. Statistical Science  19(1):140–155  2004.
[19] A. Kong  J. S. Liu  and W. H. Wong. Sequential imputations and Bayesian missing data problems. Journal

of the American Statistical Association  89(425):278–288  1994.

[20] F. Kschischang  B. J. Frey  and H.-A. Loeliger. Factor graphs and the sum–product algorithm. IEEE

Transactions on Information Theory  47:498–519  2001.

[21] F. Lindgren and H. Rue. Bayesian spatial modelling with R-INLA. Journal of Statistical Software  63(19):

1–25  2015.

[22] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Proceedings of the

18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2002.

[23] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th

Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2001.

[24] C. A. Naesseth  F. Lindsten  and T. B. Schön. Sequential Monte Carlo methods for graphical models. In

Advances in Neural Information Processing Systems (NIPS) 27  pages 1862–1870. 2014.

[25] C. A. Naesseth  F. Lindsten  and T. B. Schön. Towards automated sequential Monte Carlo for probabilistic

graphical models. NIPS Workshop on Black Box Inference and Learning  2015.

[26] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan

Kaufmann  San Francisco  CA  USA  2nd edition  1988.

10

[27] M. K. Pitt and N. Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the American

Statistical Association  94(446):590–599  1999.

[28] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer  2004.
[29] H. Rue and L. Held. Gaussian Markov Random Fields: Theory And Applications (Monographs on Statistics

and Applied Probability). Chapman & Hall/CRC  2005. ISBN 1584884320.

[30] H. Rue  S. Martino  and N. Chopin. Approximate Bayesian inference for latent Gaussian models by
using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B  71(2):
319–392  2009.

[31] H. C. Ruiz and H. J. Kappen. Particle smoothing for hidden diffusion processes: adaptive path integral

smoother. IEEE Transactions on Signal Processing  65(12):3191–3203  2017.

[32] G. S. Scott and J. Baldridge. A recursive estimate for the predictive likelihood in a topic model. In

Proceedings of the 16th International Conference on Artiﬁcial Intelligence and Statistics  2009.

[33] N. Shephard and M. K. Pitt. Likelihood analysis of non-Gaussian measurement time series. Biometrika 

84(3):653–667  1997. ISSN 00063444.

[34] M. Vihola  J. Helske  and J. Franks. Importance sampling type estimators based on approximate marginal

MCMC. arXiv.org  arXiv:1609.02541  2018.

[35] M. Wainwright  T. Jaakkola  and A. Willsky. A new class of upper bounds on the log partition function.

IEEE Transactions on Information Theory  51(7):2313–2335  2005.

[36] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[37] H. M Wallach  I. Murray  R. Salakhutdinov  and D. Mimno. Evaluation methods for topic models. In

Proceedings of the 26th International Conference on Machine Learning  2009.

[38] Nick Whiteley  Anthony Lee  Kari Heine  et al. On the role of interaction in sequential Monte Carlo

algorithms. Bernoulli  22(1):494–529  2016.

11

,Fredrik Lindsten
Jouni Helske
Matti Vihola
Vincent Chen
Sen Wu
Alexander Ratner
Jen Weng
Christopher Ré