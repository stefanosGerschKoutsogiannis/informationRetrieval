2019,MintNet: Building Invertible Neural Networks with Masked Convolutions,We propose a new way of constructing invertible neural networks by combining simple building blocks with a novel set of composition rules. This leads to a rich set of invertible architectures  including those similar to ResNets. Inversion is achieved with a locally convergent iterative procedure that is parallelizable and very fast in practice. Additionally  the determinant of the Jacobian can be computed analytically and efficiently  enabling their generative use as flow models. To demonstrate their flexibility  we show that our invertible neural networks are competitive with ResNets on MNIST and CIFAR-10 classification. When trained as generative models  our invertible networks achieve competitive likelihoods on MNIST  CIFAR-10 and ImageNet 32x32  with bits per dimension of 0.98  3.32 and 4.06 respectively.,MintNet: Building Invertible Neural Networks with

Masked Convolutions

Yang Song∗

Stanford University

yangsong@cs.stanford.edu

Chenlin Meng∗
Stanford University

chenlin@cs.stanford.edu

Abstract

Stefano Ermon

Stanford University

ermon@cs.stanford.edu

We propose a new way of constructing invertible neural networks by combining
simple building blocks with a novel set of composition rules. This leads to a
rich set of invertible architectures  including those similar to ResNets. Inversion
is achieved with a locally convergent iterative procedure that is parallelizable
and very fast in practice. Additionally  the determinant of the Jacobian can be
computed analytically and efﬁciently  enabling their generative use as ﬂow models.
To demonstrate their ﬂexibility  we show that our invertible neural networks are
competitive with ResNets on MNIST and CIFAR-10 classiﬁcation. When trained
as generative models  our invertible networks achieve competitive likelihoods on
MNIST  CIFAR-10 and ImageNet 32×32  with bits per dimension of 0.98  3.32
and 4.06 respectively.

1

Introduction

Invertible neural networks have many applications in machine learning. They have been employed to
investigate representations of deep classiﬁers [15]  understand the cause of adversarial examples [14] 
learn transition operators for MCMC [28  18]  create generative models that are directly trainable by
maximum likelihood [6  5  24  16  9  1]  and perform approximate inference [27  17].
Many applications of invertible neural networks require that both inverting the network and computing
the Jacobian determinant be efﬁcient. While typical neural networks are not invertible  achieving these
properties often imposes restrictive constraints to the architecture. For example  planar ﬂows [27]
and Sylvester ﬂow [2] constrain the number of hidden units to be smaller than the input dimension.
NICE [5] and Real NVP [6] rely on dimension partitioning heuristics and speciﬁc architectures
such as coupling layers  which could make training more difﬁcult [1]. Methods like FFJORD [9] 
i-ResNets [1] have fewer architectural constraints. However  their Jacobian determinants have to be
approximated  which is problematic if repeatedly performed at training time as in ﬂow models.
In this paper  we propose a new method of constructing invertible neural networks which are ﬂexible 
efﬁcient to invert  and whose Jacobian can be computed exactly and efﬁciently. We use triangular
matrices as our basic module. Then  we provide a set of composition rules to recursively build
more complex non-linear modules from the basic module  and show that the composed modules are
invertible as long as their Jacobians are non-singular. As in previous work [6  24]  the Jacobians
of our modules are triangular  allowing efﬁcient determinant computation. The inverse of these
modules can be obtained by an efﬁciently parallelizable ﬁxed-point iteration method  making the cost
of inversion comparable to that of an i-ResNet [1] block.
Using our composition rules and masked convolutions as the basic triangular building block  we
construct a rich set of invertible modules to form a deep invertible neural network. The architecture of
our proposed invertible network closely follows that of ResNet [10]—the state-of-the-art architecture

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

of discriminative learning. We call our model Masked Invertible Network (MintNet). To demonstrate
the capacity of MintNets  we ﬁrst test them on image classiﬁcation. We found that a MintNet
classiﬁer achieves 99.6% accuracy on MNIST  matching the performance of a ResNet with a similar
architecture. On CIFAR-10  it achieves 91.2% accuracy  comparable to the 92.6% accuracy of ResNet.
When using MintNets as generative models  they achieve the new state-of-the-art results of bits per
dimension (bpd) on uniformly dequantized images. Speciﬁcally  MintNet achieves bpd values of 0.98 
3.32  and 4.06 on MNIST  CIFAR-10 and ImageNet 32×32  while former best published results are
0.99 (FFJORD [9])  3.35 (Glow [16]) and 4.09 (Glow) respectively. Moreover  MintNet uses fewer
parameters and less computational resources. Our MNIST model uses 30% fewer parameters than
FFJORD [9]. For CIFAR-10 and ImageNet 32×32  MintNet uses 60% and 74% fewer parameters
than the corresponding Glow [16] models. When training on dataset such as CIFAR-10  MintNet
required 2 GPUs for approximately 5 days  while FFJORD [9] used 6 GPUs for approximately 5
days  and Glow [16] used 8 GPUs for approximately 7 days.

2 Background
Consider a neural network f : RD → RL that maps a data point x ∈ RD to a latent representation
z ∈ RL. When for every z ∈ RL there exists a unique x ∈ RD such that f (x) = z  we call f an
invertible neural network. There are several basic properties of invertible networks. First  when f (x)
is continuous  a necessary condition for f to be invertible is D = L. Second  if f1 : RD → RD
and f2 : RD → RD are both invertible  f = f2 ◦ f1 will also be invertible. In this work  we mainly
consider applications of invertible neural networks to classiﬁcation and generative modeling.

2.1 Classiﬁcation with invertible neural networks

Neural networks for classiﬁcation are usually not invertible because the number of classes L is usually
different from the input dimension D. Therefore  when discussing invertible neural networks for
classiﬁcation  we separate the classiﬁer into two parts f = f2 ◦ f1: feature extraction z = f1(x) and
classiﬁcation y = f2(z)  where f2 is usually the softmax function. We say the classiﬁer is invertible
when f1 is invertible. Invertible classiﬁers are arguably more interpretable  because a prediction can
be traced down by inverting latent representations [15  14].

2.2 Generative modeling with invertible neural networks
An invertible network f : x ∈ RD (cid:55)→ z ∈ RD can be used to warp a complex probability density
p(x) to a simple base distribution π(z) (e.g.  a multivariate standard Gaussian) [5  6]. Under the
condition that both f and f−1 are differentiable  the densities of p(x) and π(z) are related by the
following change of variable formula

log p(x) = log π(z) + log | det(Jf (x))| 

(1)
where Jf (x) denotes the Jacobian of f (x) and we require Jf (x) to be non-singular so that
log | det(Jf (x))| is well-deﬁned. Using this formula  p(x) can be easily computed if the Jaco-
bian determinant det(Jf (x)) is cheaply computable and π(z) is known.
Therefore  an invertible neural network fθ(x) implicitly deﬁnes a normalized density model pθ(x) 
which can be directly trained by maximum likelihood. The invertibility of fθ is critical to fast sample
generation. Speciﬁcally  in order to generate a sample x from pθ(x)  we can ﬁrst draw z ∼ π(z) 
and warp it back through the inverse of fθ to obtain x = f−1
Note that multiple invertible models f1  f2 ···   fK can be stacked together to form a deeper invertible
model f = fK ◦ ··· ◦ f2 ◦ f1  without much impact on the inverse and determinant computation.
◦ ··· ◦ f−1
This is because we can sequentially invert each component  i.e.  f−1 = f−1
K  
and the total Jacobian determinant equals the product of each individual Jacobian determinant  i.e. 
| det(Jf )| = | det(Jf1)|| det(Jf2)|···| det(JfK )|.

θ (z).

◦ f−1

2

1

3 Building invertible modules compositionally

In this section  we discuss how simple blocks like masked convolutions can be composed to build
invertible modules that allow efﬁcient  parallelizable inversion and determinant computation. To this

2

Figure 1: Illustration of a masked convolution with 3 ﬁlters and kernel size 3 × 3. Solid checkerboard
cubes inside each ﬁlter represent unmasked weights  while the transparent blue blocks represent the
weights that have been masked out. The receptive ﬁeld of each ﬁlter on the input feature maps is
indicated by regions shaded with the pattern (the colored square) below the corresponding ﬁlter.

end  we ﬁrst introduce the basic building block of our models. Then  we propose a set of composition
rules to recursively build up complex non-linear modules with triangular Jacobians. Next  we prove
that these composed modules are invertible as long as their Jacobians are non-singular. Finally  we
discuss how these modules can be inverted efﬁciently using numerical methods.

3.1 The basic module
We start from considering linear transformations f (x) = Wx + b  with W ∈ RD×D  and b ∈ RD.
For a general W  computing its Jacobian determinant requires O(D3) operations. We therefore
choose W to be a triangular matrix. In this case  the Jacobian determinant det(Jf (x)) = det(W) is
the product of all diagonal entries of W  and the computational complexity is reduced to O(D). The
linear function f (x) = Wx + b with W being triangular is our basic module.

Masked convolutions. Convolution is a special type of linear transformation that is very effective
for image data. The triangular structure of the basic module can be achieved using masked con-
volutions (e.g.  causal convolutions in PixelCNN [22]). We provide the formula of our masks in
Appendix B and an illustration of a 3 × 3 masked convolution with 3 ﬁlters in Fig. 1. Intuitively  the
causal structure of the ﬁlters (ordering of the pixels) enforces a triangular structure.

3.2 The calculus of building invertible modules

Complex non-linear invertible functions can be constructed from our basic modules in two steps.
First  we follow several composition rules so that the composed module has a triangular Jacobian.
Next  we impose appropriate constraints so that the module is invertible. To simplify the discussion 
we only consider modules with lower triangular Jacobians here  and we note that it is straightforward
to extend the analysis to modules with upper triangular Jacobians.
The following proposition summarizes several rules to compositionally build new modules with
triangular Jacobians using existing ones.
Proposition 1. Deﬁne F as the set of all continuously differentiable functions whose Jacobian is
lower triangular. Then F contains the basic module in Section 3.1  and is closed under the following
composition rules.

• Rule of addition. f1 ∈ F ∧ f2 ∈ F ⇒ λf1 + µf2 ∈ F  where λ  µ ∈ R.
• Rule of composition. f1 ∈ F ∧ f2 ∈ F ⇒ f2 ◦ f1 ∈ F. A special case is f ∈ F ⇒ h◦ f ∈
F  where h(·) is a continuously differentiable non-linear activation function that is applied
element-wise.

The proof of this proposition is straightforward and deferred to Appendix A. By repetitively applying
the rules in Proposition 1  our basic linear module can be composed to construct complex non-linear
modules having continuous and triangular Jacobians. Note that besides our linear basic modules 

3

Figure 2: Venn Diagram relationships between invertible functions (I)  the function sets of F and
M  functions that meet the conditions of Theorem 1 (det(Jf ) (cid:54)= 0)  functions whose Jacobian is
triangular and Jacobian diagonals are strictly positive (diag(Jf ) > 0)  functions whose Jacobian is
triangular and Jacobian diagonals are all 1s (diag(Jf ) = 1).

other functions with triangular and continuous Jacobians can also be made more expressive using the
composition rules. For example  the layers of dimension partitioning models (e.g.  NICE [5]  Real
NVP [6]  Glow [16]) and autoregressive ﬂows (e.g.  MAF [24]) all have continuous and triangular
Jacobians and therefore belong to F. Note that the rule of addition in Proposition 1 preserves
triangular Jacobians but not invertibility. Therefore  we need additional constraints if we want the
composed functions to be invertible.
Next  we state the condition for f ∈ F to be invertible  and denote the invertible subset of F as M.
Theorem 1. If f ∈ F and Jf (x) is non-singular for all x in the domain  then f is invertible.

Proof. A proof can be found in Appendix A.

The non-singularity of Jf (x) constraint in Theorem 1 is natural in the context of generative modeling.
This is because in order for Eq. (1) to make sense  log | det(Jf )| has to be well-deﬁned  which
requires Jf (x) to be non-singular.
In many cases  Theorem 1 can be easily used to check and enforce the invertibility of f ∈ F. For
example  the layers of autoregressive ﬂow models and dimension partitioning models can all be
viewed as elements of F because they are continuously differentiable and have triangular Jacobians.
Since the diagonal entries of their Jacobians are always strictly positive and hence non-singular  we
can immediately conclude that they are invertible with Theorem 1  thus generalizing their model-
speciﬁc proofs of invertibility.
In Fig. 2  we provide a Venn Diagram to illustrate the set of functions that satisfy the condition of
Theorem 1. As depicted by the orange set labeled by det(Jf ) (cid:54)= 0  Theorem 1 captures a subset of
M where the Jacobians of functions are non-singular so that the change of variable formula is usable.
Note the condition in Theorem 1 is sufﬁcient but not necessary. For example  f (x) = x3 ∈ M is
invertible  but Jf (x = 0) = 3x2|x=0 = 0 is singular. Many previous invertible models with special
architectures  such as NICE  Real NVP  and MAF  can be viewed as elements belonging to subsets of
det(Jf ) (cid:54)= 0.

3.3 Efﬁcient inversion of the invertible modules

In this section  we show that when the conditions in Theorem 1 hold  not only do we know that f is
invertible (f ∈ M)  but also we have a ﬁxed-point iteration method to invert f with strong theoretical
guarantees and good performance in practice.
The pseudo-code of our proposed inversion algorithm is described in Algorithm 1. Theoretically  we
can prove that this method is locally convergent—as long as the initial value is close to the true value 
the method is guaranteed to ﬁnd the correct inverse. We formally summarize this result in Theorem 2.
Theorem 2. The iterative method of Algorithm 1 is locally convergent whenever 0 < α < 2.

4

(cid:46) T is the number of iterations; 0 < α < 2 is the step size.

Algorithm 1 Fixed-point iteration method for computing f−1(z).
Require: T  α
1: Initialize x0
2: for t ← 1 to T do
3:
4:
5:
6: end for

Compute f (xt−1)
Compute diag(Jf (xt−1))
xt ← xt−1 − α diag(Jf (xt−1))−1(f (xt−1) − z)

return xT

Proof. We provide a more rigorous proof in Appendix A.

In practice  the method is also easily parallelizable on GPUs  making the cost of inverting f ∈ M
similar to that of an i-ResNet [1] layer. Within each iteration  the computation is mostly matrix
operations that can be vectorized and run efﬁciently in parallel. Therefore  the time cost will be
roughly proportional to the number of iterations  i.e.  O(T ). As will be shown in our experiments 
Algorithm 1 converges fast and usually the error quickly becomes negligible when T (cid:28) D. This is in
stark contrast to existing methods of inverting autoregressive ﬂow models such as MAF [24]  where
D univariate equations need to be solved sequentially  requiring at least O(D) iterations. There are
also other approaches for inverting f. For example  the bisection method is guaranteed to converge
globally  but its computational cost is O(D)  and is usually much more expensive than Algorithm 1.
Note that as discussed earlier  autoregressive ﬂow models can also be viewed as special cases of our
framework. Therefore  Algorithm 1 is also applicable to inverting autoregressive ﬂow models and
could potentially result in large improvements of sampling speed.

4 Masked Invertible Networks

We show that techniques developed in Section 3 can be used to build our Masked Invertible Network
(MintNet). First  we discuss how we compose several masked convolutions to form the Masked
Invertible Layer (Mint layer). Next  we stack multiple Mint layers to form a deep neural network  i.e. 
the MintNet. Finally  we compare MintNets with several existing invertible architectures.

4.1 Building the Masked Invertible Layer
We construct an invertible module in M that serves as the basic layer of our MintNet. This invertible
module  named Mint layer  is deﬁned as

L(x) = t (cid:12) x +

W3

i h

W2

ijh(W1

j x + b1

j ) + b2
ij

+ b3
i  

(2)

K(cid:88)

(cid:18) K(cid:88)

i=1

j=1

(cid:19)

i }|K

i=1  {W2

ij}|1≤i j≤K  and {W3

where (cid:12) denotes the elementwise multiplication  {W1
i=1 are all
lower triangular matrices with additional constraints to be speciﬁed later  and t > 0. Additionally 
Mint layers use a monotonic activation function h  so that h(cid:48) ≥ 0. Common choices of h include
ELU [4]  tanh and sigmoid. Note that every individual weight matrix has the same size  and the 3
groups of weights {W1
i=1 can be implemented with 3 masked
convolutions (see Appendix B). We design the form of L(x) so that it resembles a ResNet / i-ResNet
block that also has 3 convolutions with K × C ﬁlters  with C being the number of channels of x.
When using Algorithm 1 to invert Mint layers  we initialize x0 = z (cid:12) 1
t .
From Proposition 1 in Section 3.2  we can easily conclude that L ∈ F. Now  we consider additional
constraints on the weights so that L ∈ M  i.e.  it is invertible. Note that the analytic form of its
Jacobian is

ij}|1≤i j≤K and {W3

i }|K

i=1  {W2

i }|K

i }|K

JL(x) =

W3

i Ai

W2

ijBjW1

j + t 

(3)

K(cid:88)

K(cid:88)

i=1

j=1

5

with Ai = diag(h(cid:48)(cid:0)(cid:80)K

t > 0. Therefore  once we impose the following constraint

j=1 W2

ijh(W1

j x + b1

j ) + b2
ij

(cid:1)) ≥ 0  Bj = diag(h(cid:48)(W1

j ) ≥ 0 ∀1 ≤ i  j ≤ K 

j x + b1

j )) ≥ 0  and

diag(W3

i ) diag(W2

ij) diag(W1

(4)
we have diag(JL(x)) > 0  which satisﬁes the condition of Theorem 1 and as a conse-
quence we know L ∈ M.
In practice  the constraint Eq. (4) can be easily implemented.
For all 1 ≤ i  j ≤ K  we impose no constraint on W3
j   but replace W2
ij with
ij) has the same signs as
V2
ij = W2
ij is almost
diag(W3
everywhere differentiable w.r.t. W2

ij)) sign(diag(W3
j ) and therefore diag(W3

i and W1
j )). Note that diag(V2

ij  which allows gradients to backprop through.

j ) ≥ 0. Moreover  V2

i W1
i ) diag(V2

ij) diag(W1

ij sign(diag(W2

i ) diag(W1

4.2 Constructing the Masked Invertible Network

In this section  we introduce design choices that help stack multiple Mint layers together to form
an expressive invertible neural network  namely the MintNet. The full MintNet is constructed by
stacking the following paired Mint layers and squeezing layers.

Paired Mint layers. As discussed above  our Mint layer L(x) always has a triangular Jacobian. To
maximize the expressive power of our invertible neural network  it is undesirable to constrain the
Jacobian of the network to be triangular since this limits capacity and will cause blind spots in the
receptive ﬁeld of masked convolutions. We thus always pair two Mint layers together—one with a
lower triangular Jacobian and the other with an upper triangular Jacobian  so that the Jacobian of the
paired layers is not triangular  and blind spots can be eliminated.

Squeezing layers. Subsampling is important for enlarging the receptive ﬁeld of convolutions.
However  common subsampling operations such as pooling and strided convolutions are usually not
invertible. Following [6] and [1]  we use a “squeezing” operation to reshape the feature maps so
that they have smaller resolution but more channels. After a squeezing operation  the height and
width will decrease by a factor of k   but the number of channels will increase by a factor of k2. This
procedure is invertible and the Jacobian is an identity matrix. Throughout the paper  we use k = 2.

4.3 Comparison to other approaches

In what follows we compare MintNets to several existing methods for developing invertible archi-
tectures. We will focus on architectures with a tractable Jacobian determinant. However  we note
that there are models (cf .  [7  21  8]) that allow fast inverse computation but do not have tractable
Jacobian determinants. Following [1]  we also provide some comparison in Tab. 5 (see Appendix E).

4.3.1 Models based on identities of determinants

Some identities can be used to speed up the computation of determinants if the Jacobians have
special structures. For example  in Sylvester ﬂow [2]  the invertible transformation has the form
f (x) (cid:44) x + Ah(Bx + b)  where h(·) is a nonlinear activation function  A ∈ RD×M   B ∈ RM×D 
b ∈ RM and M ≤ D. By Sylvester’s determinant identity  det(Jf (x)) can be computed in O(M 3) 
which is much less than O(D3) if M (cid:28) D. However  the requirement that M is small becomes a
bottleneck of the architecture and limits its expressive power. Similarly  Planar ﬂow [27] uses the
matrix determinant lemma  but has an even narrower bottleneck.
The form of L(x) bears some resemblance to Sylvester ﬂow. However  we improve the capacity of
Sylvester ﬂow in two ways. First  we add one extra non-linear convolutional layer. Second  we avoid
the bottleneck that limits the maximum dimension of latent representations in Sylvester ﬂow.

4.3.2 Models based on dimension partitioning

NICE [5]  Real NVP [6]  and Glow [16] all depend on an afﬁne coupling layer. Given d < D  x is
ﬁrst partitioned into two parts x = [x1:d; xd+1:D]. The coupling layer is an invertible transformation 
deﬁned as f : x (cid:55)→ z 
zd+1:D = xd+1:D (cid:12) exp(s(x1:d)) + t(x1:d)  where s(·)
and t(·) are two arbitrary functions. However  the partitioning of x relies on heuristics  and the
performance is sensitive to this choice (cf .  [16  1]). In addition  the Jacobian of f is a triangular

z1:d = x1:d 

6

matrix with diagonal [1d; exp(s(x1:d))]. In contrast  the Jacobian of MintNets has more ﬂexible
diagonals—without being partially restricted to 1’s.

4.3.3 Models based on autoregressive transformations

By leveraging autoregressive transformations  the Jacobian can be made triangular. For example 
MAF [24] deﬁnes the invertible tranformation as f : x (cid:55)→ z 
zi = µ(x1:i−1) + σ(x1:i−1)xi  where
µ(·) ∈ R and σ(·) ∈ R+. Note that f−1(z) can be obtained by sequentially solving xi based on
previous solutions x1:i−1. Therefore  a naïve approach requires Ω(D) computations for inverting
autoregressive models. Moreover  the architecture of f is only an afﬁne combination of autoregressive
functions with x. In contrast  MintNets are inverted with faster ﬁxed-point iteration methods  and the
architecture of MintNets is arguably more ﬂexible.

4.3.4 Free-form invertible models

Some work proposes invertible transformations whose Jacobians are not limited by special structures.
For example  FFJORD [9] uses a continuous version of change of variables formula [3] where the
determinant is replaced by trace. Unlike MintNets  FFJORD needs an ODE solver to compute
its value and inverse  and uses a stochastic estimator to approximate the trace. Another work is
i-ResNet [1] which constrains the Lipschitz-ness of ResNet layers to make it invertible. Both i-
ResNet and MintNet use ResNet blocks with 3 convolutions. The inverse of i-ResNet can be obtained
efﬁciently by a parallelizable ﬁxed-point iteration method  which has comparable computational
cost as our Algorithm 1. However  unlike MintNets whose Jacobian determinants are exact  the
log-determinant of Jacobian of an i-ResNet must be approximated by truncating a power series and
estimating each term with stochastic estimators.

4.3.5 Other models using masked convolutions
Emerging convolutions [13] and MaCow [20] improve the Glow architecture by replacing 1 × 1
convolutions in the original Glow model with masked convolutions similar to those employed in
MintNets. Emerging convolutions and MaCow are both inverted using forward/back substitutions
designed for inverting triangular matrices  which requires the same number of iterations as the input
dimension. In stark contrast  MintNets use a ﬁxed-point iteration method (Algorithm 1) for inversion 
which is similar to i-ResNet and requires substantially fewer iterations than the input dimension. For
example  our method of inversion takes 120 iterations to converge on CIFAR-10  while inverting
emerging convolutions will need 3072 iterations. In other words  our inversion can be 25 times faster
on powerful GPUs. Additionally  the architecture of MintNet is very different. The architectures of
[13] and [20] are both built upon Glow. In contrast  MintNet is a ResNet architecture where normal
convolutions are replaced by causal convolutions.

5 Experiments

In this section  we evaluate our MintNet architectures on both image classiﬁcation and density
estimation. We focus on three common image datasets  namely MNIST  CIFAR-10 and ImageNet
32×32. We also empirically verify that Algorithm 1 can provide accurate solutions within a small
number of iterations. We provide more details about settings and model architectures in Appendix D.

5.1 Classiﬁcation

To check the capacity of MintNet and understand the trade-off of invertibility  we test its classiﬁcation
performance on MNIST and CIFAR-10  and compare it to a ResNet with a similar architecture.
On MNIST  MintNet achieves a test accuracy of 99.6%  which is the same as that of the ResNet.
On CIFAR-10  MintNet reaches 91.2% test accuracy while ResNet reaches 92.6%. Both MintNet
and ResNet achieve 100% training accuracy on MNIST and CIFAR-10 datasets. This indicates
that MintNet has enough capacity to ﬁt all data labels on the training dataset  and the invertible
representations learned by MintNet are comparable to representations learned by non-invertible
networks in terms of generalizability. Note that the small degradation in classiﬁcation accuracy is
also observed in other invertible networks. For example  depending on the Lipschitz constant  the
gap between test accuracies of i-ResNet and ResNet can be as large as 1.92% on CIFAR-10.

7

Table 1: MNIST  CIFAR-10  ImageNet 32×32 bits per dimension (bpd) results. Smaller values are
better. †Result not directly comparable because ZCA preprocssing was used.

Method
NICE [5]
MAF [24]
Real NVP [6]
Glow [16]
FFJORD [9]
i-ResNet [1]
MintNet (ours)

MNIST CIFAR-10

ImageNet 32×32

4.36
1.89
1.06
1.05
0.99
1.06
0.98

4.48†
4.31
3.49
3.35
3.40
3.45
3.32

4.28
4.09

-
-

-
-

4.06

5.2 Density estimation and veriﬁcation of invertibility

In this section  we demonstrate the superior performance of MintNet on density estimation by training
it as a ﬂow generative model. In addition  we empirically verify that Algorithm 1 can accurately
produce the inverse using a small number of iterations. We show that samples can be efﬁciently
generated from MintNet by inverting each Mint layer with Algorithm 1.

Density estimation.
In Tab. 1  we report bits per dimension (bpd) on MNIST  CIFAR-10  and
ImageNet 32×32 datasets. It is notable that MintNet sets the new records of bpd on all three datasets.
Moreover  when compared to previous best models  our MNIST model uses 30% fewer parameters
than FFJORD  and our CIFAR-10 and ImageNet 32×32 models respectively use 60% and 74% fewer
parameters than Glow. When trained on datasets such as CIFAR-10  MintNet requires 2 GPUs for
approximately ﬁve days  while FFJORD is trained on 6 GPUs for ﬁve days  and Glow on 8 GPUs for
seven days. Note that all values in Tab. 1 are with respect to the continuous distribution of uniformly
dequantized images  and results of models that view images as discrete distributions are not directly
comparable (e.g.  PixelCNN [22]  IAF-VAE [17]  and Flow++ [12]). To show that MintNet learns
semantically meaningful representations of images  we also perform latent space interpolation similar
to the interpolation experiments in Real NVP (see Appendix C).

Veriﬁcation of invertibility. We ﬁrst examine the performance of Algorithm 1 by measuring the
reconstruction error of MintNets. We compute the inverse of MintNet by sequentially inverting each
Mint layer with Algorithm 1. We used grid search to select the step size α in Algorithm 1 and chose
α = 3.5  1.1  1.15 respectively for MNIST  CIFAR-10 and ImageNet 32×32. An interesting fact
is for MNIST  α = 3.5 actually works better than other values of α within (0  2)  even though it
does not have the theoretical gurantee of local convergence. As Fig. 4a shows  the normalized L2
reconstruction error converges within 120 iterations for all datasets considered. Additionally  Fig. 4b
demonstrates that the reconstructed images look visually indistinguishable to true images.

Samples. Using Algorithm 1  we can generate samples efﬁciently by computing the inverse of
MintNets. We use the same step sizes as in the reconstruction error analysis  and run Algorithm 1 for
120 iterations for all three datasets. We provide uncurated samples in Fig. 3  and more samples can
be found in Appendix F. In addition  we compare our sampling time to that of the other models (see
Tab. 6 in Appendix E). Our sampling method has comparable speed as i-ResNet. It is approximately
5 times faster than autoregressive sampling on MNIST  and is roughly 25 times faster on CIFAR-10
and ImageNet 32×32.

6 Conclusion

We propose a new method to compositionally construct invertible modules that are ﬂexible  efﬁcient
to invert  and with a tractable Jacobian. Starting from linear transformations with triangular matrices 
we apply a set of composition rules to recursively build new modules that are non-linear and more
expressive (Proposition 1). We then show that the composed modules are invertible as long as their
Jacobians are non-singular (Theorem 1)  and propose an efﬁciently parallelizable numerical method

8

(a) MNIST

(c) ImageNet-32×32
Figure 3: Uncurated samples on MNIST  CIFAR-10  and ImageNet 32×32 datasets.

(b) CIFAR-10

(a) Reconstruction error analysis.

(b) Reconstructed images.

Figure 4: Accuracy analysis of Algorithm 1 on MNIST  CIFAR-10  and ImageNet 32×32 datasets.
Each curve in (a) represents the mean value of normalized reconstruction errors for 128 images. The
2nd  4th and 6th rows in (b) are reconstructions  while other rows are original images.

(Algorithm 1) with theoretical guarantees (Theorem 2) to compute the inverse. The Jacobians of our
modules are all triangular  which allows efﬁcient and exact determinant computation.
As an application of this idea  we use masked convolutions as our basic module. Using our com-
position rules  we compose multiple masked convolutions together to form a module named Mint
layer  following the architecture of a ResNet block. To enforce its invertibility  we constrain the
masked convolutions to satisfy the condition of Theorem 1. We show that multiple Mint layers can
be stacked together to form a deep invertible network which we call MintNet. The architecture can
be efﬁciently inverted using a ﬁxed point iteration algorithm (Algorithm 1). Experimentally  we show
that MintNet performs well on MNIST and CIFAR-10 classiﬁcation. Moreover  when trained as a
generative model  MintNet achieves new state-of-the-art performance on MNIST  CIFAR-10 and
ImageNet 32×32.

Acknowledgements

This research was supported by Intel Corporation  Amazon AWS  TRI  NSF (#1651565  #1522054 
#1733686)  ONR (N00014-19-1-2145)  AFOSR (FA9550- 19-1-0024).

References

[1] J. Behrmann  D. D. Will Grathwohl  Ricky T. Q. Chen  and J.-H. Jacobsen. Invertible residual

networks. arXiv preprint arXiv:1811.00995  2019.

9

[2] R. v. d. Berg  L. Hasenclever  J. M. Tomczak  and M. Welling. Sylvester normalizing ﬂows for

variational inference. arXiv preprint arXiv:1803.05649  2018.

[3] T. Q. Chen  Y. Rubanova  J. Bettencourt  and D. K. Duvenaud. Neural ordinary differential
equations.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and
R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages 6571–6583.
Curran Associates  Inc.  2018.

[4] D.-A. Clevert  T. Unterthiner  and S. Hochreiter. Fast and accurate deep network learning by

exponential linear units (elus). arXiv preprint arXiv:1511.07289  2015.

[5] L. Dinh  D. Krueger  and Y. Bengio. NICE: non-linear independent components estimation. In
3rd International Conference on Learning Representations  ICLR 2015  San Diego  CA  USA 
May 7-9  2015  Workshop Track Proceedings  2015.

[6] L. Dinh  J. Sohl-Dickstein  and S. Bengio. Density estimation using real nvp. arXiv preprint

arXiv:1605.08803  2016.

[7] A. N. Gomez  M. Ren  R. Urtasun  and R. B. Grosse. The reversible residual network: Back-
propagation without storing activations. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach 
R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural Information Processing
Systems 30  pages 2214–2224. Curran Associates  Inc.  2017.

[8] A. N. Gomez  M. Ren  R. Urtasun  and R. B. Grosse. The reversible residual network: Back-
propagation without storing activations. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach 
R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural Information Processing
Systems 30  pages 2214–2224. Curran Associates  Inc.  2017.

[9] W. Grathwohl  I. S. Ricky T. Q. Chen  Jesse Bettencourt  and D. Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv preprint
arXiv:1810.01367  2018.

[10] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–
778  2016.

[11] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In European

conference on computer vision  pages 630–645. Springer  2016.

[12] J. Ho  X. Chen  A. Srinivas  Y. Duan  and P. Abbeel. Flow++: Improving ﬂow-based generative

models with variational dequantization and architecture design  2019.

[13] E. Hoogeboom  R. Van Den Berg  and M. Welling. Emerging convolutions for generative
normalizing ﬂows. In International Conference on Machine Learning  pages 2771–2780  2019.
[14] J.-H. Jacobsen  J. Behrmann  R. Zemel  and M. Bethge. Excessive invariance causes adversarial

vulnerability. In International Conference on Learning Representations  2019.

[15] J.-H. Jacobsen  A. W. Smeulders  and E. Oyallon.

i-revnet: Deep invertible networks. In

International Conference on Learning Representations  2018.

[16] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. arXiv

preprint arXiv:1807.03039  2018.

[17] D. P. Kingma  T. Salimans  R. Jozefowicz  X. Chen  I. Sutskever  and M. Welling. Improved
variational inference with inverse autoregressive ﬂow.
In D. D. Lee  M. Sugiyama  U. V.
Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems
29  pages 4743–4751. Curran Associates  Inc.  2016.

[18] D. Levy  M. D. Hoffman  and J. Sohl-Dickstein. Generalizing hamiltonian monte carlo with

neural networks. In International Conference on Learning Representations  2018.

[19] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint

arXiv:1608.03983  2016.

[20] X. Ma and E. Hovy. Macow: Masked convolutional generative ﬂow.

arXiv:1902.04208  2019.

arXiv preprint

[21] M. MacKay  P. Vicol  J. Ba  and R. B. Grosse. Reversible recurrent neural networks.

In
S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors 
Advances in Neural Information Processing Systems 31  pages 9029–9040. Curran Associates 
Inc.  2018.

10

[22] A. V. Oord  N. Kalchbrenner  and K. Kavukcuoglu. Pixel recurrent neural networks. In M. F.
Balcan and K. Q. Weinberger  editors  Proceedings of The 33rd International Conference on
Machine Learning  volume 48 of Proceedings of Machine Learning Research  pages 1747–1756 
New York  New York  USA  20–22 Jun 2016. PMLR.

[23] J. M. Ortega and W. C. Rheinboldt. Iterative solution of nonlinear equations in several variables 

volume 30. Siam  1970.

[24] G. Papamakarios  T. Pavlakou  and I. Murray. Masked autoregressive ﬂow for density estimation.

In Advances in Neural Information Processing Systems  pages 2338–2347  2017.

[25] T. T. Phuong and L. T. Phong. On the convergence proof of amsgrad and a new version. arXiv

preprint arXiv:1904.03590  2019.

[26] S. J. Reddi  S. Kale  and S. Kumar. On the convergence of adam and beyond. In International

Conference on Learning Representations  2018.

[27] D. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In F. Bach and
D. Blei  editors  Proceedings of the 32nd International Conference on Machine Learning 
volume 37 of Proceedings of Machine Learning Research  pages 1530–1538  Lille  France 
07–09 Jul 2015. PMLR.

[28] J. Song  S. Zhao  and S. Ermon. A-nice-mc: Adversarial training for mcmc. In Advances in

Neural Information Processing Systems  pages 5140–5150  2017.

11

,Yang Song
Chenlin Meng
Stefano Ermon