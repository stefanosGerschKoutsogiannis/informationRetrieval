2015,Individual Planning in Infinite-Horizon Multiagent Settings: Inference  Structure and Scalability,This  paper  provides  the first  formalization  of  self-interested planning in multiagent settings using expectation-maximization (EM). Our   formalization  in   the   context   of  infinite-horizon   and finitely-nested interactive  POMDPs (I-POMDP) is  distinct from EM formulations  for POMDPs  and cooperative  multiagent planning frameworks.  We  exploit the graphical model structure  specific to I-POMDPs  and present  a new  approach based  on block-coordinate  descent for  further speed up.  Forward  filtering-backward sampling -- a combination of exact filtering  with sampling -- is explored to exploit problem structure.,Individual Planning in Inﬁnite-Horizon Multiagent

Settings: Inference  Structure and Scalability

Xia Qu

Epic Systems

Verona  WI 53593

Prashant Doshi

THINC Lab  Dept. of Computer Science
University of Georgia  Athens  GA 30622

quxiapisces@gmail.com

pdoshi@cs.uga.edu

Abstract

This paper provides the ﬁrst formalization of self-interested planning in multia-
gent settings using expectation-maximization (EM). Our formalization in the con-
text of inﬁnite-horizon and ﬁnitely-nested interactive POMDPs (I-POMDP) is
distinct from EM formulations for POMDPs and cooperative multiagent planning
frameworks. We exploit the graphical model structure speciﬁc to I-POMDPs  and
present a new approach based on block-coordinate descent for further speed up.
Forward ﬁltering-backward sampling – a combination of exact ﬁltering with sam-
pling – is explored to exploit problem structure.

Introduction

1
Generalization of bounded policy iteration (BPI) to ﬁnitely-nested interactive partially observable
Markov decision processes (I-POMDP) [1] is currently the leading method for inﬁnite-horizon self-
interested multiagent planning and obtaining ﬁnite-state controllers as solutions. However  interac-
tive BPI is acutely prone to converge to local optima  which severely limits the quality of its solutions
despite the limited ability to escape from these local optima.
Attias [2] posed planning using MDP as a likelihood maximization problem where the “data” is
the initial state and the ﬁnal goal state or the maximum total reward. Toussaint et al. [3] extended
this to infer ﬁnite-state automata for inﬁnite-horizon POMDPs. Experiments reveal good quality
controllers of small sizes although run time is a concern. Given BPI’s limitations and the compelling
potential of this approach in bringing advances in inferencing to bear on planning  we generalize it
to inﬁnite-horizon and ﬁnitely-nested I-POMDPs. Our generalization allows its use toward planning
for an individual agent in noncooperation where we may not assume common knowledge of initial
beliefs or common rewards  due to which others’ beliefs  capabilities and preferences are modeled.
Analogously to POMDPs  we formulate a mixture of ﬁnite-horizon DBNs. However  the DBNs
differ by including models of other agents in a special model node. Our approach  labeled as I-EM 
improves on the straightforward extension of Toussaint et al.’s EM to I-POMDPs by utilizing various
types of structure. Instead of ascribing as many level 0 ﬁnite-state controllers as candidate models
and improving each using its own EM  we use the underlying graphical structure of the model node
and its update to formulate a single EM that directly provides the marginal of others’ actions across
all models. This rests on a new insight  which considerably simpliﬁes and speeds EM at level 1.
We present a general approach based on block-coordinate descent [4  5] for speeding up the non-
asymptotic rate of convergence of the iterative EM. The problem is decomposed into optimization
subproblems in which the objective function is optimized with respect to a small subset (block) of
variables  while holding other variables ﬁxed. We discuss the unique challenges and present the ﬁrst
effective application of this iterative scheme to multiagent planning.
Finally  sampling offers a way to exploit the embedded problem structure such as information in dis-
tributions. The exact forward-backward E-step is replaced with forward ﬁltering-backward sampling

1

(FFBS) that generates trajectories weighted with rewards  which are used to update the parameters of
the controller. While sampling has been integrated in EM previously [6]  FFBS speciﬁcally mitigates
error accumulation over long horizons due to the exact forward step.

2 Overview of Interactive POMDPs
A ﬁnitely-nested I-POMDP [7] for an agent i with strategy level  l  interacting with agent j is:
I-POMDPi l = �ISi l  A  Ti  Ωi  Oi  Ri  OCi�
• ISi l denotes the set of interactive states deﬁned as  ISi l = S × Mj l−1  where Mj l−1 =
{Θj l−1 ∪ SMj}  for l ≥ 1  and ISi 0 = S  where S is the set of physical states. Θj l−1 is the
set of computable  intentional models ascribed to agent j: θj l−1 = �bj l−1  ˆθj�. Here bj l−1 is
agent j’s level l− 1 belief  bj l−1 ∈ �(ISj l−1) where Δ(·) is the space of distributions  and ˆθj =
�A  Tj  Ωj  Oj  Rj  OCj�  is j’s frame. At level l=0  bj 0 ∈ �(S) and a intentional model reduces
to a POMDP. SMj is the set of subintentional models of j  an example is a ﬁnite state automaton.

• A = Ai × Aj is the set of joint actions of all agents.
• Other parameters – transition function  Ti  observations  Ωi  observation function  Oi  and prefer-
ence function  Ri – have their usual semantics analogously to POMDPs but involve joint actions.
• Optimality criterion  OCi  here is the discounted inﬁnite horizon sum.
An agent’s belief over its interactive states is a sufﬁcient statistic fully summarizing the agent’s
observation history. Given the associated belief update  solution to an I-POMDP is a policy. Using
the Bellman equation  each belief state in an I-POMDP has a value which is the maximum payoff
the agent can expect starting from that belief and over the future.

3 Planning in I-POMDP as Inference
We may represent the policy of agent i for the inﬁnite horizon case as a stochastic ﬁnite state
controller (FSC)  deﬁned as: πi = �Ni Ti Li Vi� where Ni is the set of nodes in the controller.
Ti : Ni × Ai × Ωi × Ni → [0  1] represents the node transition function; Li : Ni × Ai → [0  1] de-
notes agent i’s action distribution at each node; and an initial distribution over the nodes is denoted
by  Vi : Ni → [0  1]. For convenience  we group Vi  Ti and Li in ˆfi. Deﬁne a controller at level l for
agent i as  πi l = � Ni l  ˆfi l �  where Ni l is the set of nodes in the controller and ˆfi l groups remain-
ing parameters of the controller as mentioned before. Analogously to POMDPs [3]  we formulate
planning in multiagent settings formalized by I-POMDPs as a likelihood maximization problem:

π∗i l = arg max
Πi l

γT P r(rT

i = 1|T ; πi l)

(1)

(1 − γ)�∞

T =0

where Πi l are all level-l FSCs of agent i  rT
i
emitted after T time steps with probability proportional to the reward  Ri(s  ai  aj).

is a binary random variable whose value is 0 or 1

n0
i l

s0

a0
i

a0
j

M 0
j 0

a0
k

M 0
k 0

n0
i l

n1
i l

a0
i

o1
i

a1
i

r0
i

s0

s1

a0
j

a0
k

M 0
j 0

M 0
k 0

M 1
j 0

M 1
k 0

a1
j

a1
k

n2
i l

o2
i

s2

M 2
j 0

M 2
k 0

a2
i

a2
j

a2
k

nT
i l

oT
i

sT

M T
j 0

M T
k 0

aT
i

aT
j

a0
k

s0

rT
i

s1

sT

a0
j

o1
j

a1
j

oT
j

aT
j

rT
j

m0
j 0

m1
j 0

mT
j 0

Figure 1: (a) Mixture of DBNs with 1 to T time slices for I-POMDPi 1 with i’s level-1 policy represented as
a standard FSC whose “node state” is denoted by ni l. The DBNs differ from those for POMDPs by containing
special model nodes (hexagons) whose values are candidate models of other agents. (b) Hexagonal model nodes
and edges in bold for one other agent j in (a) decompose into this level-0 DBN. Values of the node mt
j 0 are the
candidate models. CPT of chance node at
j) is inferred using likelihood maximization.

j denoted by φj 0(mt

j 0  at

2

i  aT

i   aT

Ri(sT  aT

i |aT

j )−Rmin

Rmax−Rmin

The planning problem is modeled as a mixture of DBNs of increasing time from T =0 onwards
(Fig. 1). The transition and observation functions of I-POMDPi l parameterize the chance nodes s
and oi  respectively  along with P r(rT
. Here  Rmax and Rmin
j   sT ) ∝
are the maximum and minimum reward values in Ri.
The networks include nodes  ni l  of agent i’s level-l FSC. Therefore  functions in ˆfi l parameterize
the network as well  which are to be inferred. Additionally  the network includes the hexagonal
model nodes – one for each other agent – that contain the candidate level 0 models of the agent.
Each model node provides the expected distribution over another agent’s actions. Without loss of
generality  no edges exist between model nodes in the same time step. Correlations between agents
could be included as state variables in the models.
Agent j’s model nodes and the edges (in bold) between them  and between the model and chance
j 0  are
action nodes represent a DBN of length T as shown in Fig. 1(b). Values of the chance node  m0
the candidate models of agent j. Agent i’s initial belief over the state and models of j becomes the
parameters of s0 and m0
j 0. The likelihood maximization at level 0 seeks to obtain the distribution 
P r(aj|m0
Proposition 1 (Correctness). The likelihood maximization problem as deﬁned in Eq. 1 with the
mixture models as given in Fig. 1 is equivalent to the problem of solving the original I-POMDPi l
with discounted inﬁnite horizon whose solution assumes the form of a ﬁnite state controller.

j 0)  for each candidate model in node  m0

j 0  using EM on the DBN.

All proofs are given in the supplement. Given the unique mixture models above  the challenge is to
generalize the EM-based iterative maximization for POMDPs to the framework of I-POMDPs.

3.1 Single EM for Level 0 Models

The straightforward approach is to infer a likely FSC for each level 0 model. However  this approach
does not scale to many models. Proposition 2 below shows that the dynamic P r(at
j|st) is sufﬁcient
predictive information about other agent from its candidate models at time t  to obtain the most
likely policy of agent i. This is markedly different from using behavioral equivalence [8] that clusters
models with identical solutions. The latter continues to require the full solution of each model.
Proposition 2 (Sufﬁciency). Distributions P r(at
sufﬁcient predictive information about other agent j to obtain the most likely policy of i.

j ∈ Aj for each state st is

j|st) across actions at

In the context of Proposition 2  we seek to infer P r(at
all time steps  which is denoted as φj 0. Other terms in the computation of P r(at
parameters of the level 0 DBN. The likelihood maximization for the level 0 DBN is:

j 0) for each (updated) model of j at
j|st) are known

j|mt

φ∗j 0 = arg max
φj 0

(1 − γ)�∞

T =0�mj 0∈M T

j 0

γT P r(rT

j = 1|T  mj 0; φj 0)

j

j 0  at

j  ot

As the trajectory consisting of states  models  actions and observations of the other agent is hidden
at planning time  we may solve the above likelihood maximization using EM.
E-step Let z0:T
The log likelihood is obtained as an expectation of these hidden trajectories:

0 where the observation at t = 0 is null  be the hidden trajectory.
j}T
T =0�z0:T

= {st  mt
Q(φ�j 0|φj 0) =�∞

The “data” in the level 0 DBN consists of the initial belief over the state and models  b0
i 1  and the
observed reward at T . Analogously to EM for POMDPs  this motivates forward ﬁltering-backward
smoothing on a network with joint state (st mt
j 0) for computing the log likelihood. The transition
function for the forward and backward steps is:
φj 0(mt−1

  T ; φj 0) log P r(rT

j = 1  z0:T

j = 1  z0:T

  st) P r(mt

P r(st  mt

  T ; φ�j 0)

) Tmj (st−1  at−1

j 0   at−1

j 0   at−1

P r(rT

  ot
j)

(2)

j

j

j

j

j

j 0|mt−1

j

j 0|st−1  mt−1

j 0 ) =�at−1

j

 ot
j

× Omj (st  at−1

j

  ot
j)

where mj in the subscripts is j’s model at t − 1. Here  P r(mt
delta function that is 1 when j’s belief in mt−1
otherwise 0.

j 0 updated using at−1

j

j 0|at−1

  ot
j
and ot

(3)
j 0 ) is the Kronecker-
j 0;

j  mt−1
j equals the belief in mt

3

Forward ﬁltering gives the probability of the next state as follows:
j 0|st−1  mt−1

P r(st  mt

αt(st  mt

j 0) =�st−1 mt−1

j 0

j 0 ) αt−1(st−1  mt−1
j 0 )

where α0(s0  m0
probability of the state and model at t − 1 from the distribution at t is:

j 0) is the initial belief of agent i. The smoothing by which we obtain the joint

βh(st−1  mt−1

j 0 ) =�st mt

j 0

P r(st  mt

j 0|st−1  mt−1

j 0 ) βh−1(st  mt

j 0)

where h denotes the horizon to T and β0(sT   mT
j 0)]. Messages
αt and βh give the probability of a state at some time slice in the DBN. As we consider a mixture of
BNs  we seek probabilities for all states in the mixture model. Subsequently  we may compute the
forward and backward messages at all states for the entire mixture model in one sweep.

j = 1|sT   mT

j 0) = EaT

[P r(rT

j |mT

j 0

P r(T = t) αt(s  mj 0)

P r(T = h) βh(s  mj 0)

(4)

�α(s  mj 0) =�∞

t=0

�β(s  mj 0) =�∞

h=0

Model growth
As the other agent performs its actions and makes observations  the space
of j’s models grows exponentially: starting from a ﬁnite set of |M 0
j 0| models  we obtain
j 0|(|Aj||Ωj|)t) models at time t. This greatly increases the number of trajectories in Z 0:T
.
O(|M 0
We limit the growth in the model space by sampling models at the next time step from the distribu-
tion  αt(st  mt
j 0)  as we perform each step of forward ﬁltering. It limits the growth by exploiting
the structure present in φj 0 and Oj  which guide how the models grow.
M-step We obtain the updated φ�j 0 from the full log likelihood in Eq. 2 by separating the terms:

j

Q(φ�j 0|φj 0) = �terms independent of φ�j 0� +�∞

P r(rT

i = 1  z0:T

j

  T ; φ�j 0) �T

t=0

φ�j 0(at

j|mt

j 0)

j

T =0�z0:T
j)�α(st  mt

j)�st

Rmj (st  at

j 0) Tmj (st  at

j  st+1) P r(mt+1

j 0) +�st st+1 mt+1
j 0 |mt

j  ot+1

j 0  at

j

γ

j 0  ot+1
) Omj (st+1  at

1 − γ�β(st+1  mt+1

j  ot+1

)

j 0 )

j

j

and maximizing it w.r.t. φ�j 0:
φ�j 0(at

j 0) ∝ φj 0(at

j  mt

j  mt

× �α(st  mt

i  at

i  nt

i l  at

3.2 Improved EM for Level l I-POMDP
At strategy levels l ≥ 1  Eq. 1 deﬁnes the likelihood maximization problem  which is iteratively
solved using EM. We show the E- and M-steps next beginning with l = 1.
E-step
In a multiagent setting  the hidden variables additionally include what the other agent
may observe and how it acts over time. However  a key insight is that Prop. 2 allows us to limit
attention to the marginal distribution over other agents’ actions given the state. Thus  let z0:T
i =
0   where the observation at t = 0 is null  and other agents are labeled j
{st  ot
to k; this group is denoted −i. The full log likelihood involves an expectation over hidden variables:
(5)

  T ; πi l) log P r(rT

i = 1  z0:T

i = 1  z0:T

j  . . .   at

k}T
T =0�z0:T
Q(π�i l|πi l) =�∞

Due to the subjective perspective in I-POMDPs  Q computes the likelihood of agent i’s FSC only
(and not of joint FSCs as in team planning [9]).
In the T -step DBN of Fig. 1  observed evidence includes the reward  rT
i   at the end and the initial
belief. We seek the likely distributions  Vi  Ti  and Li  across time slices. We may again realize the
full joint in the expectation using a forward-backward algorithm on a hidden Markov model whose
state is (st  nt

i l). The transition function of this model is 

  T ; π�i l)

P r(rT

i

i

i

P r(st  nt

i l|st−1  nt−1

i l ) =�at−1

i

 at−1
−i  ot
× Ti(st−1  at−1

i

i l

i Li(nt−1
  at−1

  at−1

i

)�−i

P r(at−1

−i |st−1) Ti(nt−1
−i   ot
i)

i l

(6)
In addition to parameters of I-POMDPi l  which are given  parameters of agent i’s controller and
those relating to other agents’ predicted actions  φ−i 0  are present in Eq. 6. Notice that in conse-
quence of Proposition 2  Eq. 6 precludes j’s observation and node transition functions.

−i   st) Oi(st  at−1

  at−1

i

  at−1

i

  ot

i  nt

i l)

4

The forward message  αt = P r(st  nt
DBN at time t:

αt(st  nt

i l) =�st−1 nt−1

i l

i l)  represents the probability of being at some state of the

P r(st  nt

i l|st−1  nt−1

i l ) αt−1(st−1  nt−1
i l )

(7)

i l) =�st+1 nt+1
P r(rT
i = 1|sT   aT

i  aT
−i

i l

|st  nt
i   aT
−i) Li(nT
−i) ∝ Ri(sT   aT

i l) = Vi(n0

where  α0(s0  n0
reward in the ﬁnal T th time step given a state of the Markov model  βt(st  nt

i l(s0). The backward message gives the probability of observing the
i l):
(8)

i = 1|st  nt

i l) βh−1(st+1  nt+1
i l )

P r(st+1  nt+1
i l

i l) = P r(rT

βh(st  nt

i l)b0

i = 1|sT   aT
i   aT

i l) =�aT
−i|st) being dependent on t is that we can no longer conveniently deﬁne�α and

i )�−i P r(aT
where  β0(sT   nT
h ≤ T is the horizon. Here  P r(rT
A side effect of P r(at
�β for use in M-step at level 1. Instead  the computations are folded in the M-step.
M-step We update the parameters  Li  Ti and Vi  of πi l to obtain π�i l based on the expectation
  T ; πi l) with πi l substituted
in the E-step. Speciﬁcally  take log of the likelihood P r(rT = 1  z0:T
with π�i l and focus on terms involving the parameters of π�i l:

−i|sT )  and 1 ≤

i l  aT
i   aT

−i).

i

log P r(rT = 1  z0:T

i

  T ; π�i l) =�terms independent of π�i l� +�T

i l  at

i  ot+1

i

  nt+1

t=0

log L�i(nt

i l  at
i l ) + log V�i(ni l)

i)+

T =0

In order to update  Li  we partially differentiate the Q-function of Eq. 5 with respect to L�i. To
facilitate differentiation  we focus on the terms involving Li  as shown below.
Q(π�i l|πi l) = �terms indep. of L�i� +�∞
i = 1  z0:t
L�i on maximizing the above equation is:
L�i(nt
Node transition probabilities Ti and node distribution Vi for π�i l  is updated analogously to Li.
Because a FSC is inferred at level 1  at strategy levels l = 2 and greater  lower-level candidate
models are FSCs. EM at these higher levels proceeds by replacing the state of the DBN  (st  nt
i l)
with (st  nt

T =0�−i�sT  aT

t=0�z0:T

|T ; πi l) log L�i(nt

i)�∞

i = 1|sT   aT

i) ∝ Li(nt

−i) P r(aT

γT
1 − γ

P r(rT

Pr(rT

−i|sT ) αT (sT   nT
i l)

i l  at
i)

i l  at

i l  at

i   aT

−i

i

i

i l  nt

j l−1  . . .   nt

k l−1).

log T �i (nt

t=0

�T−1
Pr(T ) �T

3.3 Block-Coordinate Descent for Non-Asymptotic Speed Up

Block-coordinate descent (BCD) [4  5  10] is an iterative scheme to gain faster non-asymptotic rate
of convergence in the context of large-scale N-dimensional optimization problems. In this scheme 
within each iteration  a set of variables referred to as coordinates are chosen and the objective func-
tion  Q  is optimized with respect to one of the coordinate blocks while the other coordinates are
held ﬁxed. BCD may speed up the non-asymptotic rate of convergence of EM for both I-POMDPs
and POMDPs. The speciﬁc challenge here is to determine which of the many variables should be
grouped into blocks and how.
We empirically show in Section 5 that grouping the number of time slices  t  and horizon  h  in
Eqs. 7 and 8  respectively  at each level  into coordinate blocks of equal size is beneﬁcial. In other
words  we decompose the mixture model into blocks containing equal numbers of BNs. Alternately 
grouping controller nodes is ineffective because distribution Vi cannot be optimized for subsets of
nodes. Formally  let Ψt
1 be a subset of {T = 1  T = 2  . . .   T = Tmax}. Then  the set of blocks is 
3  . . .}. In practice  because both t and h are ﬁnite (say  Tmax)  the cardinality of
Bt = {Ψt
Bt is bounded by some C ≥ 1. Analogously  we deﬁne the set of blocks of h  denoted by Bh.
In the M-step now  we compute αt for the time steps in a single coordinate block Ψt
c only  while
using the values of αt from the previous iteration for the complementary coordinate blocks  ˜Ψt
c.
Analogously  we compute βh for the horizons in Ψh
c only  while using β values from the previous
iteration for the remaining horizons. We cyclically choose a block  Ψt
c  at iterations c + qC where
q ∈ {0  1  2  . . .}.

2  Ψt

1  Ψt

5

3.4 Forward Filtering - Backward Sampling

i

i l

  oT

i   nT

  aT−1

i l�  and so on from T to 0. Here  P r(rT

An approach for exploiting embedded structure in the transition and observation functions is to
replace the exact forward-backward message computations with exact forward ﬁltering and back-
ward sampling (FFBS) [11] to obtain a sampled reverse trajectory consisting of �sT   nT
i � 
i l  aT
�nT−1
−i) is the likelihood
weight of this trajectory sample. Parameters of the updated FSC  π�i l  are obtained by summing and
normalizing the weights.
Each trajectory is obtained by ﬁrst sampling ˆT ∼ P r(T )  which becomes the length of i’s DBN for
i l)  t = 0 . . . ˆT is computed exactly (Eq. 7) followed by the
this sample. Forward message  αt(st  nt
i l)  h = 0 . . . ˆT and t = ˆT − h. Computing βh differs from Eq. 8 by
backward message  βh(st  nt
utilizing the forward message:

i = 1|sT   aT

i   aT

βh(st  nt

i l|st+1  nt+1

i

i  at
i l  at

−i ot+1
i  ot+1

i

i l  at

αt(st  nt

i l) Li(nt
i l ) Oi(st+1  at

  nt+1

i  at

i l ) =�at
Ti(nt
i ) =�at

P r(at

i) �−i
−i  ot+1
−i|sT ) L(nT

)

i

−i|st) Ti(st  at

i  at

−i  st+1)
(9)
−i).
i   aT
i |sT   aT
from Eq. 9.

i l)�−i P r(aT
i l  aT
i � followed by sampling sT−1

i l  rT

i

i ) P r(rT
  nT−1

i l

i l  rT

where β0(sT   nT
αT (sT   nT
Subsequently  we may easily sample �sT   nT
We sample aT−1

  oT

i at

i  ot+1

−i

i

i

P r(at

i  ot+1

i

|st  nt

i ∼ P r(at
i l  st+1  nt+1

i l ) ∝�−i

|st  nt
P r(at

i l  st+1  nt+1
−i|st) Li(nt
i  at
−j  ot+1
)

i

i l )  where:
i) Ti(nt

i l  at

Oi(st+1  at

i l  at

i  ot+1

i

  nt+1

i l ) Ti(st  at

i  at

−i  st+1)

4 Computational Complexity

Our EM at level 1 is signiﬁcantly quicker compared to ascribing FSCs to other agents. In the latter 
nodes of others’ controllers must be included alongside s and ni l.
Proposition 3 (E-step speed up). Each E-step at level 1 using the forward-backward pass as shown
previously results in a net speed up of O((|M||N−i 0|)2K|Ω−i|K) over the formulation that ascribes
|M| FSCs each to K other agents with each having |N−i 0| nodes.
Analogously  updating the parameters Li and Ti
in the M-step exhibits a speedup of
O((|M||N−i 0|)2K|Ω−i|K)  while Vi leads to O((|M||N−i 0|)K). This improvement is exponential
in the number of other agents. On the other hand  the E-step at level 0 exhibits complexity that is
typically greater compared to the total complexity of the E-steps for |M| FSCs.
Proposition 4 (E-step ratio at level 0). E-steps when |M| FSCs are inferred for K agents exhibit a
ratio of complexity  O(|N−i 0|2
|M|
The ratio in Prop. 4 is < 1 when smaller-sized controllers are sought and there are several models.

)  compared to the E-step for obtaining φ−i 0.

5 Experiments

Five variants of EM are evaluated as appropriate: the exact EM inference-based planning (labeled as
I-EM); replacing the exact M-step with its greedy variant analogously to the greedy maximization in
EM for POMDPs [12] (I-EM-Greedy); iterating EM based on coordinate blocks (I-EM-BCD) and
coupled with a greedy M-step (I-EM-BCD-Greedy); and lastly  using forward ﬁltering-backward
sampling (I-EM-FFBS).
We use 4 problem domains: the noncooperative multiagent tiger problem [13] (|S|= 2  |Ai|= |Aj|=
3  |Oi|= |Oj|= 6 for level l ≥ 1  |Oj|= 3 at level 0  and γ = 0.9) with a total of 5 agents and 50
models for each other agent. A larger noncooperative 2-agent money laundering (ML) problem [14]
forms the second domain. It exhibits 99 physical states for the subject agent (blue team)  9 actions
for blue and 4 for the red team  11 observations for subject and 4 for the other  with about 100 models

6

l

e
u
a
V
1

 

 
l

e
v
e
L

l

e
u
a
V
1

 

 
l

e
v
e
L

5-agent Tiger

I-EM
I-EM-Greedy
I-EM-BCD
I-EM-FFBS

 0

-50

-100

-150

-200

-250

-300

 10

 100

 1000

time(s) in log scale

(I-a) EM methods

 0

-50

-100

-150

-200

-250

-300

I-EM-BCD
I-BPI

 10

 100

 1000

 10000

time(s) in log scale

(II-a) I-EM-BCD  I-BPI

-90

-100

-110

-120

-130

-140

-90

-100

-110

-120

-130

-140

2-agent ML

3-agent UAV

I-EM
I-EM-Greedy
I-EM-BCD-Greedy
I-EM-FFBS

 100

 1000

 10000

time(s) in log scale

(I-b) EM methods

I-EM-BCD-Greedy
I-BPI

 400

 350

 300

 250

 200

 150

 100

 50

 0

 400

 350

 300

 250

 200

 150

 100

 50

 0

I-EM-Greedy
I-EM-BCD-Greedy
I-EM-FFBS

 0

 10000 20000 30000 40000 50000 60000 70000

time(s)

(I-c) EM methods

I-EM-BCD-Greedy
I-BPI

 100

 1000

time(s) in log scale

 0

 10000

 20000

time(s)

 30000

 40000

(II-b) I-EM-BCD-Greedy  I-BPI

(II-c) I-EM-BCD-Greedy  I-BPI

 1100

 1000

 900

 800

 700

 600

 500

I-EM
I-EM-Greedy
I-EM-BCD
I-EM-BCD-Greedy

5-agent policing

 1200

 1100

 1000

 900

 800

 700

 600

 0

 5000

 10000

time(s)

 15000

 20000

 0

 5000

I-EM-BCD
I-BPI

 10000

time(s)

 15000

 20000

(I-d) EM methods

(II-d) I-EM-BCD  I-BPI

Figure 2: FSCs improve with time for I-POMDPi 1 in the (I-a) 5-agent tiger  (I-b) 2-agent money laundering 
(I-c) 3-agent UAV  and (I-d) 5-agent policing contexts. Observe that BCD causes substantially larger improve-
ments in the initial iterations until we are close to convergence. I-EM-BCD or its greedy variant converges
signiﬁcantly quicker than I-BPI to similar-valued FSCs for all four problem domains as shown in (II-a  b  c and
d)  respectively. All experiments were run on Linux with Intel Xeon 2.6GHz CPUs and 32GB RAM.

for red team. We also evaluate a 3-agent UAV reconnaissance problem involving a UAV tasked with
intercepting two fugitives in a 3x3 grid before they both reach the safe house [8]. It has 162 states for
the UAV  5 actions  4 observations for each agent  and 200 400 models for the two fugitives. Finally 
the recent policing protest problem is used in which police must maintain order in 3 designated
protest sites populated by 4 groups of protesters who may be peaceful or disruptive [15]. It exhibits
27 states  9 policing and 4 protesting actions  8 observations  and 600 models per protesting group.
The latter two domains are historically the largest test problems for self-interested planning.
Comparative performance of all methods
In Fig. 2-I(a-d)  we compare the variants on all prob-
lems. Each method starts with a random seed  and the converged value is signiﬁcantly better than
a random FSC for all methods and problems. Increasing the sizes of FSCs gives better values in
general but also increases time; using FSCs of sizes 5  3  9 and 5  for the 4 domains respectively
demonstrated a good balance. We explored various coordinate block conﬁgurations eventually set-
tling on 3 equal-sized blocks for both the tiger and ML  5 blocks for UAV and 2 for policing protest.
I-EM and the Greedy and BCD variants clearly exhibit an anytime property on the tiger  UAV and
policing problems. The noncooperative ML shows delayed increases because we show the value of
agent i’s controller and initial improvements in the other agent’s controller may maintain or decrease
the value of i’s controller. This is not surprising due to competition in the problem. Nevertheless 
after a small delay the values improve steadily which is desirable.
I-EM-BCD consistently improves on I-EM and is often the fastest: the corresponding value improves
by large steps initially (fast non-asymptotic rate of convergence). In the context of ML and UAV 
I-EM-BCD-Greedy shows substantive improvements leading to controllers with much improved
values compared to other approaches. Despite a low sample size of about 1 000 for the problems 
I-EM-FFBS obtains FSCs whose values improve in general for tiger and ML  though slowly and
not always to the level of others. This is because the EM gets caught in a worse local optima due

7

to sampling approximation – this strongly impacts the UAV problem; more samples did not escape
these optima. However  forward ﬁltering only (as used in Wu et al. [6]) required a much larger
sample size to reach these levels. FFBS did not improve the controller in the fourth domain.
Characterization of local optima While an exact solution for the smaller tiger problem with 5
agents (or the larger problems) could not be obtained for comparison  I-EM climbs to the optimal
value of 8.51 for the downscaled 2-agent version (not shown in Fig. 2). In comparison  BPI does
not get past the local optima of -10 using an identical-sized controller – corresponding controller
predominantly contains listening actions – relying on adding nodes to eventually reach optimum.
While we are unaware of any general technique to escape local convergence in EM  I-EM can reach
the global optimum given an appropriate seed. This may not be a coincidence: the I-POMDP value
function space exhibits a single ﬁxed point – the global optimum – which in the context of Propo-
sition 1 makes the likelihood function  Q(π�i l|πi l)  unimodal (if πi l is appropriately sized as we
do not have a principled way of adding nodes). If Q(π�i l|πi l) is continuously differentiable for the
domain on hand  Corollary 1 in Wu [16] indicates that πi l will converge to the unique maximizer.
Improvement on I-BPI We compare the quickest of the I-EM variants with previous best algo-
rithm  I-BPI (Figs. 2-II(a-d))  allowing the latter to escape local optima as well by adding nodes.
Observe that FSCs improved using I-EM-BCD converge to values similar to those of I-BPI almost
two orders of magnitude faster. Beginning with 5 nodes  I-BPI adds 4 more nodes to obtain the same
level of value as EM for the tiger problem. For money laundering  I-EM-BCD-Greedy converges to
controllers whose value is at least 1.5 times better than I-BPI’s given the same amount of allocated
time and less nodes. I-BPI failed to improve the seed controller and could not escape for the UAV
and policing protest problems. To summarize  this makes I-EM variants with emphasis on BCD the
fastest iterative approaches for inﬁnite-horizon I-POMDPs currently.

6 Concluding Remarks
The EM formulation of Section 3 builds on the EM for POMDP and differs drastically from the E-
and M-steps for the cooperative DEC-POMDP [9]. The differences reﬂect how I-POMDPs build on
POMDPs and differ from DEC-POMDPs. These begin with the structure of the DBNs where the
DBN for I-POMDPi 1 in Fig. 1 adds to the DBN for POMDP hexagonal model nodes that contain
candidate models; chance nodes for action; and model update edges for each other agent at each
time step. This differs from the DBN for DEC-POMDP  which adds controller nodes for all agents
and a joint observation chance node. The I-POMDP DBN contains controller nodes for the subject
agent only  and each model node collapses into an efﬁcient distribution on running EM at level 0.
In domains where the joint reward function may be decomposed into factors encompassing subsets
of agents  ND-POMDPs allow the value function to be factorized as well. Kumar et al. [17] exploit
this structure by simply decomposing the whole DBN mixture into a mixture for each factor and it-
erating over the factors. Interestingly  the M-step may be performed individually for each agent and
this approach scales beyond two agents. We exploit both graphical and problem structures to speed
up and scale in a way that is contextual to I-POMDPs. BCD also decomposes the DBN mixture
into equal blocks of horizons. While it has been applied in other areas [18  19]  these applications
do not transfer to planning. Additionally  problem structure is considered by using FFBS that ex-
ploits information in the transition and observation distributions of the subject agent. FFBS could be
viewed as a tenuous example of Monte Carlo EM  which is a broad category and also includes the
forward sampling utilized by Wu et al. [6] for DEC-POMDPs. However  fundamental differences
exist between the two: forward sampling may be run in simulation and does not require the transition
and observation functions. Indeed  Wu et al. utilize it in a model free setting. FFBS is model based
utilizing exact forward messages in the backward sampling phase. This reduces the accumulation of
sampling errors over many time steps in extended DBNs  which otherwise afﬂicts forward sampling.
The advance in this paper for self-interested multiagent planning has wider relevance to areas such
as game play and ad hoc teams where agents model other agents. Developments in online EM for
hidden Markov models [20] provide an interesting avenue to utilize inference for online planning.

Acknowledgments

This research is supported in part by a NSF CAREER grant  IIS-0845036  and a grant from ONR 
N000141310870. We thank Akshat Kumar for feedback that led to improvements in the paper.

8

References
[1] Ekhlas Sonu and Prashant Doshi. Scalable solutions of interactive POMDPs using generalized
and bounded policy iteration. Journal of Autonomous Agents and Multi-Agent Systems  pages
DOI: 10.1007/s10458–014–9261–5  in press  2014.

[2] Hagai Attias. Planning by probabilistic inference. In Ninth International Workshop on AI and

Statistics (AISTATS)  2003.

[3] Marc Toussaint and Amos J. Storkey. Probabilistic inference for solving discrete and con-
tinuous state markov decision processes. In International Conference on Machine Learning
(ICML)  pages 945–952  2006.

[4] Jeffrey A. Fessler and Alfred O. Hero.

Space-alternating generalized expectation-

maximization algorithm. IEEE Transactions on Signal Processing  42:2664–2677  1994.

[5] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization.

Journal of Optimization Theory and Applications  109:475–494  2001.

[6] Feng Wu  Shlomo Zilberstein  and Nicholas R. Jennings. Monte-carlo expectation maximiza-
tion for decentralized POMDPs. In Twenty-Third International Joint Conference on Artiﬁcial
Intelligence (IJCAI)  pages 397–403  2013.

[7] Piotr J. Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multiagent

settings. Journal of Artiﬁcial Intelligence Research  24:49–79  2005.

[8] Yifeng Zeng and Prashant Doshi. Exploiting model equivalences for solving interactive dy-

namic inﬂuence diagrams. Journal of Artiﬁcial Intelligence Research  43:211–255  2012.

[9] Akshat Kumar and Shlomo Zilberstein. Anytime planning for decentralized pomdps using

expectation maximization. In Conference on Uncertainty in AI (UAI)  pages 294–301  2010.

[10] Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate de-

scent methods. SIAM Journal on Optimization  23(1):576–601  2013.

[11] C. K. Carter and R. Kohn. Markov chainmonte carlo in conditionally gaussian state space

models. Biometrika  83:589–601  1996.

[12] Marc Toussaint  Laurent Charlin  and Pascal Poupart. Hierarchical POMDP controller opti-
mization by likelihood maximization. In Twenty-Fourth Conference on Uncertainty in Artiﬁ-
cial Intelligence (UAI)  pages 562–570  2008.

[13] Prashant Doshi and Piotr J. Gmytrasiewicz. Monte Carlo sampling methods for approximating

interactive POMDPs. Journal of Artiﬁcial Intelligence Research  34:297–337  2009.

[14] Brenda Ng  Carol Meyers  Koﬁ Boakye  and John Nitao. Towards applying interactive
POMDPs to real-world adversary modeling. In Innovative Applications in Artiﬁcial Intelli-
gence (IAAI)  pages 1814–1820  2010.

[15] Ekhlas Sonu  Yingke Chen  and Prashant Doshi.

Individual planning in agent populations:
Anonymity and frame-action hypergraphs. In International Conference on Automated Plan-
ning and Scheduling (ICAPS)  pages 202–211  2015.

[16] C. F. Jeff Wu. On the convergence properties of the em algorithm. Annals of Statistics 

11(1):95–103  1983.

[17] Akshat Kumar  Shlomo Zilberstein  and Marc Toussaint. Scalable multiagent planning using
probabilistic inference. In International Joint Conference on Artiﬁcial Intelligence (IJCAI) 
pages 2140–2146  2011.

[18] S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless chan-

nels. IEEE Transactions on Information Theory  18(1):14–20  1972.

[19] Jeffrey A. Fessler and Donghwan Kim. Axial block coordinate descent (ABCD) algorithm for
X-ray CT image reconstruction. In International Meeting on Fully Three-dimensional Image
Reconstruction in Radiology and Nuclear Medicine  volume 11  pages 262–265  2011.

[20] Olivier Cappe and Eric Moulines. Online expectation-maximization algorithm for latent
data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
71(3):593–613  2009.

9

,Xia Qu
Prashant Doshi
Huasen Wu
Xin Liu