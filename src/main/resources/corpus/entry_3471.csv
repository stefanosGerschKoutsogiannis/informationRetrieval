2012,Mirror Descent Meets Fixed Share (and feels no regret),Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis  we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting  adaptive  discounted  and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth  and can be refined in several ways  including improvements for small losses and adaptive tuning of parameters.,Mirror Descent Meets Fixed Share

(and feels no regret)

Nicolò Cesa-Bianchi

Università degli Studi di Milano

nicolo.cesa-bianchi@unimi.it

Gábor Lugosi

ICREA & Universitat Pompeu Fabra  Barcelona

gabor.lugosi@upf.edu

Pierre Gaillard

Ecole Normale Supérieure∗  Paris
pierre.gaillard@ens.fr

Gilles Stoltz

Ecole Normale Supérieure∗  Paris &
HEC Paris  Jouy-en-Josas  France

gilles.stoltz@ens.fr

Abstract

Mirror descent with an entropic regularizer is known to achieve shifting regret
bounds that are logarithmic in the dimension. This is done using either a carefully
designed projection or by a weight sharing technique. Via a novel uniﬁed analysis 
we show that these two approaches deliver essentially equivalent bounds on a no-
tion of regret generalizing shifting  adaptive  discounted  and other related regrets.
Our analysis also captures and extends the generalized weight sharing technique
of Bousquet and Warmuth  and can be reﬁned in several ways  including improve-
ments for small losses and adaptive tuning of parameters.

1

Introduction

Online convex optimization is a sequential prediction paradigm in which  at each time step  the
learner chooses an element from a ﬁxed convex set S and then is given access to a convex loss
function deﬁned on the same set. The value of the function on the chosen element is the learner’s
loss. Many problems such as prediction with expert advice  sequential investment  and online re-
gression/classiﬁcation can be viewed as special cases of this general framework. Online learning
algorithms are designed to minimize the regret. The standard notion of regret is the difference
between the learner’s cumulative loss and the cumulative loss of the single best element in S. A
much harder criterion to minimize is shifting regret  which is deﬁned as the difference between the
learner’s cumulative loss and the cumulative loss of an arbitrary sequence of elements in S. Shifting
regret bounds are typically expressed in terms of the shift  a notion of regularity measuring the length
of the trajectory in S described by the comparison sequence (i.e.  the sequence of elements against
which the regret is evaluated). In online convex optimization  shifting regret bounds for convex sub-
sets S ⊆ Rd are obtained for the projected online mirror descent (or follow-the-regularized-leader)
algorithm. In this case the shift is typically computed in terms of the p-norm of the difference of
consecutive elements in the comparison sequence —see [1  2] and [3].
We focus on the important special case when S is the simplex. In [1] shifting bounds are shown for
projected mirror descent with entropic regularizers using a 1-norm to measure the shift.1 When the
comparison sequence is restricted to the corners of the simplex (which is the setting of prediction
with expert advice)  then the shift is naturally deﬁned to be the number of times the trajectory moves

∗Ecole Normale Supérieure  Paris – CNRS – INRIA  within the project-team CLASSIC
1Similar 1-norm shifting bounds can also be proven using the analysis of [2]. However  without using
entropic regularizers it is not clear how to achieve a logarithmic dependence on the dimension  which is one of
the advantages of working in the simplex.

1

to a different corner. This problem is often called “tracking the best expert” —see  e.g.  [4  5  1  6  7] 
and it is well known that exponential weights with weight sharing  which corresponds to the ﬁxed-
share algorithm of [4]  achieves a good shifting bound in this setting. In [6] the authors introduce a
generalization of the ﬁxed-share algorithm  and prove various shifting bounds for any trajectory in
the simplex. However  their bounds are expressed using a quantity that corresponds to a proper shift
only for trajectories on the simplex corners.
In this paper we offer a uniﬁed analysis of mirror descent  ﬁxed share  and the generalized ﬁxed
share of [6] for the setting of online convex optimization in the simplex. Our bounds are expressed
in terms of a notion of shift based on the total variation distance. Our analysis relies on a generalized
notion of shifting regret which includes  as special cases  related notions of regret such as adaptive
regret  discounted regret  and regret with time-selection functions. Perhaps surprisingly  we show
that projected mirror descent and ﬁxed share achieve essentially the same generalized regret bound.
Finally  we show that widespread techniques in online learning  such as improvements for small
losses and adaptive tuning of parameters  are all easily captured by our analysis.

2 Preliminaries

(cid:80)T

For simplicity  we derive our results in the setting of online linear optimization. As we show in the
supplementary material  these results can be easily extended to the more general setting of online
convex optimization through a standard linearization step.
Online linear optimization may be cast as a repeated game between the forecaster and the environ-

ment as follows. We use ∆d to denote the simplex(cid:8)q ∈ [0  1]d : (cid:107)q(cid:107)1 = 1(cid:9).
1. Forecaster chooses(cid:98)pt = ((cid:98)p1 t  . . .  (cid:98)pd t) ∈ ∆d
3. Forecaster suffers loss(cid:98)p
The goal of the forecaster is to minimize the accumulated loss  e.g. (cid:98)LT =(cid:80)T
t=1(cid:98)p

2. Environment chooses a loss vector (cid:96)t = ((cid:96)1 t  . . .   (cid:96)d t) ∈ [0  1]d

Online linear optimization in the simplex. For each round t = 1  . . .   T  

(cid:62)
t (cid:96)t .

(cid:62)
t (cid:96)t. In the now
classical problem of prediction with expert advice  the goal of the forecaster is to compete with the
best ﬁxed component (often called “expert”) chosen in hindsight  that is  with mini=1 ... T
t=1 (cid:96)i t;
or even to compete with a richer class of sequences of components. In Section 3 we state more
speciﬁcally the goals considered in this paper.
We start by introducing our main algorithmic tool  described in Figure 1  a share algorithm whose
formulation generalizes the seemingly unrelated formulations of the algorithms studied in [4  1  6]. It
is parameterized by the “mixing functions” ψt : [0  1]td → ∆d for t (cid:62) 2 that assign probabilities to
past “pre-weights” as deﬁned below. In all examples discussed in this paper  these mixing functions
are quite simple  but working with such a general model makes the main ideas more transparent. We
then provide a simple lemma that serves as the starting point2 for analyzing different instances of
this generalized share algorithm.
Lemma 1. For all t (cid:62) 1 and for all qt ∈ ∆d  Algorithm 1 satisﬁes
vi t+1(cid:98)pi t
(cid:98)pj t e−η (cid:96)j t
By deﬁnition of vi t+1  for all i = 1  . . .   d we then have(cid:80)d

 +
j=1(cid:98)pj t e−η (cid:96)j t = (cid:98)pi t e−η (cid:96)i t/vi t+1 
t (cid:96)t (cid:54) (cid:96)i t + (1/η) ln(vi t+1/(cid:98)pi t) + η/8. The proof is concluded by taking a

(cid:0)(cid:98)pt − qt
d(cid:88)

Proof. By Hoeffding’s inequality (see  e.g.  [3  Section A.1.1]) 

d(cid:88)
 d(cid:88)

(cid:98)pj t (cid:96)j t (cid:54) − 1

(cid:96)t (cid:54) 1
η

(cid:1)(cid:62)

(cid:98)p

(cid:62)

+

.

η
8

qi t ln

i=1

which implies
convex aggregation with respect to qt.

j=1

ln

η

j=1

η
8

.

(1)

2We only deal with linear losses in this paper. However  it is straightforward that for sequences of η–exp-

concave loss functions  the additional term η/8 in the bound is no longer needed.

2

Parameters: learning rate η > 0 and mixing functions ψt for t (cid:62) 2

Initialization:(cid:98)p1 = v1 = (1/d  . . .   1/d)

For each round t = 1  . . .   T  

2. Observe loss (cid:96)t ∈ [0  1]d ;
3. [loss update] For each j = 1  . . .   d deﬁne

1. Predict(cid:98)pt ;
(cid:98)pj t e−η (cid:96)j t
(cid:80)d
i=1(cid:98)pi t e−η (cid:96)i t
Vt+1 =(cid:2)vi s
(cid:3)
4. [shared update] Deﬁne(cid:98)pt+1 = ψt+1

(cid:0)Vt+1

1(cid:54)i(cid:54)d  1(cid:54)s(cid:54)t+1

vj t+1 =

(cid:1).

the current pre-weights 

and vt+1 = (v1 t+1  . . .   vd t+1);
the d × (t + 1) matrix of all past and current pre-weights;

Algorithm 1: The generalized share algorithm.

3 A generalized shifting regret for the simplex

We now introduce a generalized notion of shifting regret which uniﬁes and generalizes the notions of
discounted regret (see [3  Section 2.11])  adaptive regret (see [8])  and shifting regret (see [2]). For
a ﬁxed horizon T   a sequence of discount factors βt T (cid:62) 0 for t = 1  . . .   T assigns varying weights
to the instantaneous losses suffered at each round. We compare the total loss of the forecaster with
the loss of an arbitrary sequence of vectors q1  . . .   qT in the simplex ∆d. Our goal is to bound the
regret

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

βt T(cid:98)p

t=1

t=1

βt T q(cid:62)
t (cid:96)t

in terms of the “regularity” of the comparison sequence q1  . . .   qT and of the variations of the
discounting weights βt T . By setting ut = βt T q(cid:62)

+  we can rephrase the above regret as

t ∈ Rd

u(cid:62)
t (cid:96)t .

(2)

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

(cid:107)ut(cid:107)1(cid:98)p

t=1

t=1

T(cid:88)

t=2

m(uT

1 ) =

In the literature on tracking the best expert [4  5  1  6]  the regularity of the sequence u1  . . .   uT is
measured as the number of times ut (cid:54)= ut+1. We introduce the following regularity measure

DTV(ut  ut−1)

+  we deﬁne DTV(x  y) =(cid:80)

(3)

(xi − yi).
where for x = (x1  . . .   xd)  y = (y1  . . .   yd) ∈ Rd
Note that when x  y ∈ ∆d  we recover the total variation distance DTV(x  y) = 1
2 (cid:107)x − y(cid:107)1  while
for general x  y ∈ Rd
+  the quantity DTV(x  y) is not necessarily symmetric and is always bounded
by (cid:107)x − y(cid:107)1. The traditional shifting regret of [4  5  1  6] is obtained from (2) when all ut are such
that (cid:107)ut(cid:107)1 = 1.

xi(cid:62)yi

4 Projected update

The shifting variant of the EG algorithm analyzed in [1] is a special case of the generalized share
algorithm in which the function ψt+1 performs a projection of the pre-weights on the convex set
d = [α/d  1]d ∩ ∆d. Here α ∈ (0  1) is a ﬁxed parameter. We can prove (using techniques similar
∆α
to the ones shown in the next section—see the supplementary material) the following bound which
generalizes [1  Theorem 16].

3

Theorem 1. For all T (cid:62) 1  for all sequences (cid:96)1  . . .   (cid:96)t ∈ [0  1]d of loss vectors  and for all
u1  . . .   uT ∈ Rd

+  if Algorithm 1 is run with the above update  then

t (cid:96)t (cid:54) (cid:107)u1(cid:107)1 ln d
u(cid:62)

η

+

m(uT
1 )

η

ln

d
α

+

+ α

(cid:107)ut(cid:107)1 .

(4)

(cid:16) η

8

(cid:17) T(cid:88)

t=1

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

(cid:107)ut(cid:107)1(cid:98)p

t=1

t=1

This bound can be optimized by a proper tuning of α and η parameters. We show a similarly tuned
(and slightly better) bound in Corollary 1.

5 Fixed-share update

Next  we consider a different instance of the generalized share algorithm corresponding to the update

(cid:98)pj t+1 =

d(cid:88)

(cid:16) α

d

i=1

(cid:17)

+ (1 − α)1i=j

vi t+1 =

α
d

+ (1 − α)vj t+1  

0 (cid:54) α (cid:54) 1

(5)

Despite seemingly different statements  this update in Algorithm 1 can be seen to lead exactly to the
ﬁxed-share algorithm of [4] for prediction with expert advice. We now show that this update delivers
a bound on the regret almost equivalent to (though slightly better than) that achieved by projection
on the subset ∆α
Theorem 2. With the above update  for all T (cid:62) 1  for all sequences (cid:96)1  . . .   (cid:96)T of loss vectors
(cid:96)t ∈ [0  1]d  and for all u1  . . .   uT ∈ Rd
+ 

d of the simplex.

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

(cid:107)ut(cid:107)1 (cid:98)p

t=1

t=1

t (cid:96)t (cid:54) (cid:107)u1(cid:107)1 ln d
u(cid:62)

η

+

T(cid:88)

+

t=1
m(uT
1 )

η
8

η

(cid:107)ut(cid:107)1

ln

d
α

+

(cid:80)T
t=2 (cid:107)ut(cid:107)1 − m(uT
1 )

η

ln

1

1 − α

.

1 ) and(cid:80)T

Note that if we only consider vectors of the form ut = qt = (0  . . .   0  1  0  . . .   0) then m(qT
1 )
corresponds to the number of times qt+1 (cid:54)= qt in the sequence qT
1 . We thus recover [4  Theorem 1]
and [6  Lemma 6] from the much more general Theorem 2.
The ﬁxed-share forecaster does not need to “know” anything in advance about the sequence of
the norms (cid:107)ut(cid:107) for the bound above to be valid. Of course  in order to minimize the obtained
upper bound  the tuning parameters α  η need to be optimized and their values will depend on the
t=1 (cid:107)ut(cid:107)1 for the sequences one wishes to compete against. This
maximal values of m(uT
is illustrated in the following corollary  whose proof is omitted. Therein  h(x) = −x ln x − (1 −
x) ln(1 − x) denotes the binary entropy function for x ∈ [0  1]. We recall3 that h(x) (cid:54) x ln(e/x)
for x ∈ [0  1].
Corollary 1. Suppose Algorithm 1 is run with the update (5). Let m0 > 0 and U0 > 0. For all T (cid:62)
1  for all sequences (cid:96)1  . . .   (cid:96)T of loss vectors (cid:96)t ∈ [0  1]d  and for all sequences u1  . . .   uT ∈ Rd
with (cid:107)u1(cid:107)1 + m(uT
T(cid:88)
(cid:107)ut(cid:107)1 (cid:98)p

1 ) (cid:54) m0 and(cid:80)T
(cid:118)(cid:117)(cid:117)(cid:116) U0

(cid:118)(cid:117)(cid:117)(cid:116) U0 m0

t (cid:96)t− T(cid:88)

(cid:32)
t=1 (cid:107)ut(cid:107)1

(cid:18) e U0

(cid:18) m0

m0 ln d + U0 h

(cid:19)(cid:33)

+

(cid:19)(cid:33)

u(cid:62)
t (cid:96)t (cid:54)

(cid:54) U0 

ln d + ln

(cid:32)

(cid:54)

(cid:62)

t=1

t=1
whenever η and α are optimally chosen in terms of m0 and U0.
Proof of Theorem 2. Applying Lemma 1 with qt = ut/(cid:107)ut(cid:107)1  and multiplying by (cid:107)ut(cid:107)1  we get
for all t (cid:62) 1 and ut ∈ Rd

m0

U0

2

2

+

(cid:107)ut(cid:107)1 (cid:98)p

3As can be seen by noting that ln(cid:0)1/(1 − x)(cid:1) < x/(1 − x)

i=1

ui t ln

(cid:62)
t (cid:96)t − u(cid:62)

t (cid:96)t (cid:54) 1
η

vi t+1(cid:98)pi t

+

η
8

(cid:107)ut(cid:107)1 .

(6)

d(cid:88)

4

d(cid:88)

i=1

We now examine

ui t ln

vi t+1(cid:98)pi t

(cid:18)

d(cid:88)

i=1

=

ui t ln

(cid:19)

+

− ui t−1 ln

1
vi t

(cid:18)

d(cid:88)

i=1

(cid:19)

ui t−1 ln

1
vi t

− ui t ln

1

vi t+1

1(cid:98)pi t
(cid:19)

(cid:88)
(cid:88)

i : ui t(cid:62)ui t−1

+

(cid:18)
(cid:18)

(cid:124)

(ui t − ui t−1) ln

+ ui t−1 ln

(ui t − ui t−1) ln

+ui t ln

1(cid:98)pi t

1
vi t

(cid:125)

(cid:123)(cid:122)

.

(7)

(8)

(cid:19)
vi t(cid:98)pi t
(cid:19)
vi t(cid:98)pi t

.

For the ﬁrst term on the right-hand side  we have

(cid:18)

d(cid:88)

i=1

ui t ln

1(cid:98)pi t

− ui t−1 ln

1
vi t

=

i : ui t<ui t−1

In view of the update (5)  we have 1/(cid:98)pi t (cid:54) d/α and vi t/(cid:98)pi t (cid:54) 1/(1 − α). Substituting in (8)  we
d(cid:88)

(cid:19)

get

(cid:54)0

(cid:18)
1(cid:98)pi t
(cid:54) (cid:88)

ui t ln

i=1

i : ui t(cid:62)ui t−1

− ui t−1 ln

1
vi t

(ui t − ui t−1) ln

+

d
α

 (cid:88)
ui t − (cid:88)
(cid:123)(cid:122)

i : ui t(cid:62)ui t−1

i: ui t(cid:62)ui t−1

 d(cid:88)
(cid:124)

i=1

= DTV(ut  ut−1) ln

d
α

+

 ln

ui t

1

1 − α

ui t−1 +

i: ui t<ui t−1

(ui t − ui t−1)

ln

1

1 − α

.

(cid:88)

(cid:125)

The sum of the second term in (7) telescopes. Substituting the obtained bounds in the ﬁrst sum of
the right-hand side in (7)  and summing over t = 2  . . .   T   leads to

T(cid:88)

d(cid:88)

t=2

i=1

ui t ln

vi t+1(cid:98)pi t

(cid:54) m(uT

1 ) ln

d
α

+

=(cid:107)ut(cid:107)1−DTV(ut ut−1)

(cid:32) T(cid:88)

t=2

(cid:33)
(cid:107)ut(cid:107)1 − m(uT
1 )
d(cid:88)

ln

1

1 − α

We hence get from (6)  which we use in particular for t = 1 

T(cid:88)

t=1

(cid:107)ut(cid:107)1(cid:98)p

(cid:62)
t (cid:96)t − u(cid:62)

t (cid:96)t (cid:54) 1
η

d(cid:88)

i=1

ui 1 ln

1(cid:98)pi 1

+

η
8

T(cid:88)

t=1

6 Applications

+

m(uT
1 )

η

ln

d
α

+

+

ui 1 ln

i=1

1
vi 2

1

(cid:124)

− ui T ln
vi T +1
(cid:54)0

(cid:123)(cid:122)

(cid:125)

.

(cid:107)ut(cid:107)1
(cid:80)T
t=2 (cid:107)ut(cid:107)1 m(uT
1 )

η

ln

1

1 − α

.

We now show how our regret bounds can be specialized to obtain bounds on adaptive and discounted
regret  and on regret with time-selection functions. We show regret bounds only for the speciﬁc
instance of the generalized share algorithm using update (5); but the discussion below also holds up
to minor modiﬁcations for the forecaster studied in Theorem 1.

Adaptive regret was introduced by [8] and can be viewed as a variant of discounted regret where
the monotonicity assumption is dropped. For τ0 ∈ {1  . . .   T}  the τ0-adaptive regret of a forecaster
is deﬁned by

(cid:41)

(cid:98)p
(cid:62)
t (cid:96)t − min
q∈∆d

s(cid:88)

t=r

q(cid:62)(cid:96)t

.

(9)

Rτ0−adapt

T

=

max

[r  s] ⊂ [1  T ]
s + 1 − r (cid:54) τ0

(cid:40) s(cid:88)

t=r

5

The fact that this is a special case of (2) clearly emerges from the proof of Corollary 2 below here.
Adaptive regret is an alternative way to measure the performance of a forecaster against a changing
environment. It is a straightforward observation that adaptive regret bounds also lead to shifting
regret bounds (in terms of hard shifts). In this paper we note that these two notions of regret share
an even tighter connection  as they can be both viewed as instances of the same alma mater notion
of regret  i.e.  the generalized shifting regret introduced in Section 3. The work [8] essentially
considered the case of online convex optimization with exp-concave loss function; in case of general
convex functions  they also mentioned that the greedy projection forecaster of [2] enjoys adaptive
regret guarantees. This is obtained in much the same way as we obtain an adaptive regret bound for
the ﬁxed-share forecaster in the next result.
Corollary 2. Suppose that Algorithm 1 is run with the shared update (5). Then for all T (cid:62) 1  for
all sequences (cid:96)1  . . .   (cid:96)T of loss vectors (cid:96)t ∈ [0  1]d  and for all τ0 ∈ {1  . . .   T} 

Rτ0−adapt

T

(cid:54)

(cid:115)

(cid:18)

τ0
2

(cid:18) 1

(cid:19)

τ0

(cid:19)

(cid:54)

(cid:114) τ0

2

τ0 h

+ ln d

ln(edτ0)

whenever η and α are chosen optimally (depending on τ0 and T ).

As mentioned in [8]  standard lower bounds on the regret show that the obtained bound is optimal
up to the logarithmic factors.
Proof. For 1 (cid:54) r (cid:54) s (cid:54) T and q ∈ ∆d  the regret in the right-hand side of (9) equals the
1 deﬁned as ut = q for t = r  . . .   s and
regret considered in Theorem 2 against the sequence uT
0 = (0  . . .   0) for the remaining t. When r (cid:62) 2  this sequence is such that DTV(ur  ur−1) =
1 ) = 1  while (cid:107)u1(cid:107)1 = 0.
DTV(q  0) = 1 and DTV(us+1  us) = DTV(0  q) = 0 so that m(uT
When r = 1  we have (cid:107)u1(cid:107)1 = 1 and m(uT
1 ) + (cid:107)u1(cid:107)1 = 1  that
is  m0 = 1. Specializing the bound of Theorem 2 with the additional choice U0 = τ0 gives the
result.

1 ) = 0. In all cases  m(uT

Discounted regret was introduced in [3  Section 2.11] and is deﬁned by

T(cid:88)

t=1

max
q∈∆d

(cid:0)(cid:98)p

βt T

(cid:62)
t (cid:96)t − q(cid:62)(cid:96)t

(cid:1) .

(10)

The discount factors βt T measure the relative importance of more recent losses to older losses. For
instance  for a given horizon T   the discounts βt T may be larger as t is closer to T . On the contrary 
in a game-theoretic setting  the earlier losses may matter more then the more recent ones (because of
interest rates)  in which case βt T would be smaller as t gets closer to T . We mostly consider below
monotonic sequences of discounts (both non-decreasing and non-increasing). Up to a normalization 
we assume that all discounts βt T are in [0  1]. As shown in [3]  a minimal requirement to get non-

trivial bounds is that the sum of the discounts satisﬁes UT =(cid:80)

t(cid:54)T βt T → ∞ as T → ∞.

√

A natural objective is to show that the quantity in (10) is o(UT )  for instance  by bounding it by
UT . We claim that Corollary 1 does so  at least whenever the sequences
something of the order of
(βt T ) are monotonic for all T . To support this claim  we only need to show that m0 = 1 is a suitable
value to deal with (10). Indeed  for all T (cid:62) 1 and for all q ∈ ∆d  the measure of regularity involved
in the corollary satisﬁes

T(cid:88)

(cid:0)βt T − βt−1 T

(cid:1)

+ = max(cid:8)β1 T   βT T

(cid:9) (cid:54) 1  

(cid:107)β1 T q(cid:107)1 + m(cid:0)(βt T q)t(cid:54)T

(cid:1) = β1 T +

t=2

where the second equality follows from the monotonicity assumption on the discounts.
The values of the discounts for all t and T are usually known in advance. However  the horizon T
is not. Hence  a calibration issue may arise. The online tuning of the parameters α and η shown
UT for all
in Section 7.3 entails a forecaster that can get discounted regret bounds of the order
T . The fundamental reason for this is that the discounts only come in the deﬁnition of the ﬁxed-
share forecaster via their sums. In contrast  the forecaster discussed in [3  Section 2.11] weighs each
instance t directly with βt T (i.e.  in the very deﬁnition of the forecaster) and enjoys therefore no
regret guarantees for horizons other than T (neither before T nor after T ). Therein  the knowledge

√

6

(cid:113)(cid:80)

of the horizon T is so crucial that it cannot be dealt with easily  not even with online calibration of
the parameters or with a doubling trick. We insist that for the ﬁxed-share forecaster  much ﬂexibility
is gained as some of the discounts βt T can change in a drastic manner for a round T to values
βt T +1 for the next round. However we must admit that the bound of [3  Section 2.11] is smaller
than the one obtained above  as it of the order of
t(cid:54)T βt T
bound. Again  this improvement was made possible because of the knowledge of the time horizon.
As for the comparison to the setting of discounted losses of [9]  we note that the latter can be cast as
a special case of our setting (since the discounting weights take the special form βt T = γt . . . γT−1
therein  for some sequence γs of positive numbers). In particular  the ﬁxed-share forecaster can
satisfy the bound stated in [9  Theorem 2]  for instance  by using the online tuning techniques of
Section 7.3. A ﬁnal reference to mention is the setting of time-selection functions of [10  Section 6] 
which basically corresponds to knowing in advance the weights (cid:107)ut(cid:107)1 of the comparison sequence
u1  . . .   uT the forecaster will be evaluated against. We thus generalize their results as well.

t T   in contrast to our

(cid:113)(cid:80)

t(cid:54)T β2

7 Reﬁnements and extensions

We now show that techniques for reﬁning the standard online analysis can be easily applied to our
framework. We focus on the following: improvement for small losses  sparse target sequences  and
dynamic tuning of parameters. Not all of them where within reach of previous analyses.

7.1

Improvement for small losses

The regret bounds of the ﬁxed-share forecaster can be signiﬁcantly improved when the cumulative
loss of the best sequence of experts is small. The next result improves on Corollary 1 whenever
L0 (cid:28) U0. For concreteness  we focus on the ﬁxed-share update (5).
Corollary 3. Suppose Algorithm 1 is run with the update (5). Let m0 > 0  U0 > 0  and L0 > 0.
For all T (cid:62) 1  for all sequences (cid:96)1  . . .   (cid:96)T of loss vectors (cid:96)t ∈ [0  1]d  and for all sequences
u1  . . .   uT ∈ Rd

+ with (cid:107)u1(cid:107)1 + m(uT

t=1 (cid:107)ut(cid:107)1

t (cid:96)t (cid:54) L0 

t=1 u(cid:62)

(cid:54) U0  and(cid:80)T
(cid:19)(cid:33)
(cid:18) e U0

m0

(cid:18) e U0

(cid:19)

m0

ln d + ln

+ ln d + ln

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

(cid:107)ut(cid:107)1 (cid:98)p

t=1

t=1

u(cid:62)
t (cid:96)t (cid:54)

1 ) (cid:54) m0 (cid:80)T
(cid:118)(cid:117)(cid:117)(cid:116)L0 m0
(cid:32)

whenever η and α are optimally chosen in terms of m0  U0  and L0.

Here again  the parameters α and η may be tuned online using the techniques shown in Section 7.3.
The above reﬁnement is obtained by mimicking the analysis of Hedge forecasters for small losses
(see  e.g.  [3  Section 2.4]). In particular  one should substitute Lemma 1 with the following lemma
in the analysis carried out in Section 5; its proof follows from the mere replacement of Hoeffding’s
inequality by [3  Lemma A.3]  which states that for all η ∈ R and for all random variable X taking
values in [0  1]  one has ln E[e−ηX ] (cid:54) (e−η − 1)EX.
Lemma 2. Algorithm 1 satisﬁes 1 − e−η
(cid:62)
t (cid:96)t − q(cid:62)

for all qt ∈ ∆d.

d(cid:88)

(cid:19)

qi t ln

(cid:98)p

(cid:18) vi t(cid:98)pi t+1

t (cid:96)t (cid:54) 1
η

i=1

η

7.2 Sparse target sequences

The work [6] introduced forecasters that are able to efﬁciently compete with the best sequence of
experts among all those sequences that only switch a bounded number of times and also take a
small number of different values. Such “sparse” sequences of experts appear naturally in many
applications. In this section we show that their algorithms in fact work very well in comparison with
a much larger class of sequences u1  . . .   uT that are “regular”—that is  m(uT
1 )  deﬁned in (3) is
small—and “sparse” in the sense that the quantity n(uT
i=1 maxt=1 ... T ui t is small. Note
that when qt ∈ ∆d for all t  then two interesting upper bounds can be provided. First  denoting
the union of the supports of these convex combinations by S ⊆ {1  . . .   d}  we have n(qT
1 ) (cid:54) |S| 
the cardinality of S. Also  n(qT
combinations. Thus  n(uT

t = 1  . . .   T}(cid:12)(cid:12)  the cardinality of the pool of convex

1 ) (cid:54) (cid:12)(cid:12){qt 

1 ) generalizes the notion of sparsity of [6].

1 ) =(cid:80)d

7

Here we consider a family of shared updates of the form

(cid:98)pj t = (1 − α)vj t + α

wj t
Zt

 

(cid:80)d

0 (cid:54) α (cid:54) 1  

(11)

where the wj t are nonnegative weights that may depend on past and current pre-weights and Zt =
i=1 wi t is a normalization constant. Shared updates of this form were proposed by [6  Sections 3
and 5.2]. Apart from generalizing the regret bounds of [6]  we believe that the analysis given below
is signiﬁcantly simpler and more transparent. We are also able to slightly improve their original
bounds.
We focus on choices of the weights wj t that satisfy the following conditions: there exists a constant
C (cid:62) 1 such that for all j = 1  . . .   d and t = 1  . . .   T  

and

The next result improves on Theorem 2 when T (cid:28) d and n(uT
dimension (or number of experts) d is large but the sequence uT
the supplementary material; it is a variation on the proof of Theorem 2.
Theorem 3. Suppose Algorithm 1 is run with the shared update (11) with weights satisfying the
conditions (12). Then for all T (cid:62) 1  for all sequences (cid:96)1  . . .   (cid:96)T of loss vectors (cid:96)t ∈ [0  1]d  and
for all sequences u1  . . .   uT ∈ Rd
+ 

(12)
1 )  that is  when the
1 is sparse. Its proof can be found in

1 ) (cid:28) m(uT

C wj t+1 (cid:62) wj t .

vj t (cid:54) wj t (cid:54) 1

t (cid:96)t (cid:54) n(uT
u(cid:62)
1 ) ln d
η

+

n(uT

1 ) T ln C

η

+

m(uT
1 )

η

ln

maxt(cid:54)T Zt

α

+

T(cid:88)
η
(cid:80)T
8
t=2 (cid:107)ut(cid:107)1 − m(uT
1 )

(cid:107)ut(cid:107)1

t=1

+

η

ln

1

1 − α

.

T(cid:88)

t (cid:96)t − T(cid:88)

(cid:62)

(cid:107)ut(cid:107)1 (cid:98)p

t=1

t=1

Corollaries 8 and 9 of [6] can now be generalized (and even improved); we do so—in the supple-
mentary material—by showing two speciﬁc instances of the generic update (11) that satisfy (12).

7.3 Online tuning of the parameters

vj t+1 =(cid:98)p

(cid:44) d(cid:88)

i=1

(cid:98)p

The forecasters studied above need their parameters η and α to be tuned according to various quan-
tities  including the time horizon T . We show here how the trick of [11] of having these parameters
vary over time can be extended to our setting. For the sake of concreteness we focus on the ﬁxed-
share update  i.e.  Algorithm 1 run with the update (5). We respectively replace steps 3 and 4 of its
description by the loss and shared updates

ηt

ηt−1
j t

e−ηt(cid:96)j t

ηt

ηt−1
i t

e−ηt(cid:96)i t

+ (1 − αt) vj t+1  

αt
d

and

pj t+1 =

(13)
for all t (cid:62) 1 and all j ∈ {1  . . .   d}  where (ητ ) and (ατ ) are two sequences of positive numbers 
indexed by τ (cid:62) 1. We also conventionally deﬁne η0 = η1. Theorem 2 is then adapted in the
following way (when ηt ≡ η and αt ≡ α  Theorem 2 is exactly recovered).
Theorem 4. The forecaster based on the updates (13) is such that whenever ηt (cid:54) ηt−1 and αt (cid:54)
αt−1 for all t (cid:62) 1  the following performance bound is achieved. For all T (cid:62) 1  for all sequences
(cid:96)1  . . .   (cid:96)T of loss vectors (cid:96)t ∈ [0  1]d  and for all u1  . . .   uT ∈ Rd
+ 
− 1
ηt−1
(cid:107)ut(cid:107)1
ηt−1

(cid:18) 1
T(cid:88)

(cid:107)ut(cid:107)1
d(1 − αT )

t (cid:96)t − T(cid:88)

(cid:32)(cid:107)ut(cid:107)1

(cid:107)ut(cid:107)1 (cid:98)p

m(uT
1 )
ηT

(cid:19)(cid:33)

u(cid:62)
t (cid:96)t (cid:54)

T(cid:88)

T(cid:88)

T(cid:88)

1

1 − αt

ηt−1

+

ln

ln d

ηt

+

ln

+

αT

η1

t=1

t=2

t=1

+

(cid:62)

(cid:107)ut(cid:107)1 .

8

t=1

t=2

Due to space constraints  we provide an illustration of this bound only in the supplementary material.

Acknowledgments

The authors acknowledge support from the French National Research Agency (ANR) under grant
EXPLO/RA (“Exploration–exploitation for efﬁcient resource allocation”) and by the PASCAL2
Network of Excellence under EC grant no. 506778.

8

References
[1] M. Herbster and M. Warmuth. Tracking the best linear predictor. Journal of Machine Learning

Research  1:281–309  2001.

[2] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

Proceedings of the 20th International Conference on Machine Learning  ICML 2003  2003.

[3] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[4] M. Herbster and M. Warmuth. Tracking the best expert. Machine Learning  32:151–178  1998.
[5] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning  35(3):247–282 

Jun. 1999.

[6] O. Bousquet and M.K. Warmuth. Tracking a small set of experts by mixing past posteriors.

Journal of Machine Learning Research  3:363–396  2002.

[7] A. György  T. Linder  and G. Lugosi. Tracking the best of many experts. In Proceedings of the
18th Annual Conference on Learning Theory (COLT)  pages 204–216  Bertinoro  Italy  Jun.
2005. Springer.

[8] E. Hazan and C. Seshadhri. Efﬁcient learning algorithms for changing environments. Proceed-

ings of the 26th International Conference of Machine Learning (ICML)  2009.

[9] A. Chernov and F. Zhdanov. Prediction with expert advice under discounted loss. In Proceed-
ings of the 21st International Conference on Algorithmic Learning Theory  ALT 2010  pages
255–269. Springer  2008.

[10] A. Blum and Y. Mansour. From extermal to internal regret. Journal of Machine Learning

Research  8:1307–1324  2007.

[11] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algo-

rithms. Journal of Computer and System Sciences  64:48–75  2002.

9

,Yichao Zhou
Haozhi Qi
Jingwei Huang
Yi Ma