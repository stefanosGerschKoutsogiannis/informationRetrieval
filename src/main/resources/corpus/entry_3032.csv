2013,Learning a Deep Compact Image Representation for Visual Tracking,In this paper  we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background.  In contrast to most existing trackers which only learn the appearance of the tracked object online  we take a different approach  inspired by recent advances in deep learning architectures  by putting more emphasis on the (unsupervised) feature learning problem.  Specifically  by using auxiliary natural images  we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations.  This is then followed by knowledge transfer from offline training to the online tracking process.  Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer.  Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object.  Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is very efficient as well as more accurate.,Learning a Deep Compact Image Representation for

Visual Tracking

Naiyan Wang

Dit-Yan Yeung

Department of Computer Science and Engineering
Hong Kong University of Science and Technology
winsty@gmail.com
dyyeung@cse.ust.hk

Abstract

In this paper  we study the challenging problem of tracking the trajectory of a
moving object in a video with possibly very complex background. In contrast to
most existing trackers which only learn the appearance of the tracked object on-
line  we take a different approach  inspired by recent advances in deep learning
architectures  by putting more emphasis on the (unsupervised) feature learning
problem. Speciﬁcally  by using auxiliary natural images  we train a stacked de-
noising autoencoder ofﬂine to learn generic image features that are more robust
against variations. This is then followed by knowledge transfer from ofﬂine train-
ing to the online tracking process. Online tracking involves a classiﬁcation neural
network which is constructed from the encoder part of the trained autoencoder as
a feature extractor and an additional classiﬁcation layer. Both the feature extrac-
tor and the classiﬁer can be further tuned to adapt to appearance changes of the
moving object. Comparison with the state-of-the-art trackers on some challenging
benchmark video sequences shows that our deep learning tracker is more accurate
while maintaining low computational cost with real-time performance when our
MATLAB implementation of the tracker is used with a modest graphics process-
ing unit (GPU).

1

Introduction

Visual tracking  also called object tracking  refers to automatic estimation of the trajectory of an
object as it moves around in a video. It has numerous applications in many domains  including
video surveillance for security  human-computer interaction  and sports video analysis. While a
certain application may require multiple moving objects be tracked  the typical setting is to treat
each object separately. After the object to track is identiﬁed either manually or automatically in the
ﬁrst video frame  the goal of visual tracking is to automatically track the trajectory of the object
over the subsequent frames. Although existing computer vision techniques may offer satisfactory
solutions to this problem under well-controlled environments  the problem can be very challenging
in many practical applications due to factors such as partial occlusion  cluttered background  fast
and abrupt motion  dramatic illumination changes  and large variations in viewpoint and pose.
Most existing trackers adopt either the generative or the discriminative approach. Generative track-
ers  like other generative models in machine learning  assume that the object being tracked can be
described by some generative process and hence tracking corresponds to ﬁnding the most prob-
able candidate among possibly inﬁnitely many. The motivation behind generative trackers is to
develop image representations which can facilitate robust tracking. They have been inspired by
recent advances in fast algorithms for robust estimation and sparse coding  such as the alternat-
ing direction method of multipliers (ADMM) and accelerated gradient methods. Some popular
generative trackers include incremental visual tracking (IVT) [18]  which represents the tracked ob-
ject based on principal component analysis (PCA)  and the l1 tracker (L1T) [16]  which assumes

1

that the tracked object can be represented by a sparse combination of overcomplete basis vectors.
Many extensions [26  25  4  21] have also been proposed. On the other hand  the discriminative
approach treats tracking as a binary classiﬁcation problem which learns to explicitly distinguish
the object being tracked from its background. Some representative trackers in this category are the
online AdaBoost (OAB) tracker [6]  multiple instance learning (MIL) tracker [3]  and structured
output tracker (Struck) [8]. While generative trackers usually produce more accurate results under
less complex environments due to the richer image representations used  discriminative trackers are
more robust against strong occlusion and variations since they explicitly take the background into
consideration. We refer the reader to a recent paper [23] which empirically compares many existing
trackers based on a common benchmark.
From the learning perspective  visual tracking is challenging because it has only one labeled instance
in the form of an identiﬁed object in the ﬁrst video frame. In the subsequent frames  the tracker has
to learn variations of the tracked object with only unlabeled data available. With no prior knowledge
about the object being tracked  it is easy for the tracker to drift away from the target. To address
this problem  some trackers taking the semi-supervised learning approach have been proposed [12 
7]. An alternative approach [22] ﬁrst learns a dictionary of image features (such as SIFT local
descriptors) from auxiliary data and then transfers the knowledge learned to online tracking.
Another issue is that many existing trackers make use of image representations that may not be good
enough for robust tracking in complex environments. This is especially the case for discriminative
trackers which usually put more emphasis on improving the classiﬁers rather than the image features
used. While many trackers simply use raw pixels as features  some attempts have used more infor-
mative features  such as Haar features  histogram features  and local binary patterns. However  these
features are all handcrafted ofﬂine but not tailor-made for the tracked object. Recently  deep learning
architectures have been used successfully to give very promising results for some complicated tasks 
including image classiﬁcation [14] and speech recognition [10]. The key to success is to make use
of deep architectures to learn richer invariant features via multiple nonlinear transformations. We
believe that visual tracking can also beneﬁt from deep learning for the same reasons.
In this paper  we propose a novel deep learning tracker (DLT) for robust visual tracking. We attempt
to combine the philosophies behind both generative and discriminative trackers by developing a
robust discriminative tracker which uses an effective image representation learned automatically.
There are some key features which distinguish DLT from other existing trackers. First  it uses a
stacked denoising autoencoder (SDAE) [20] to learn generic image features from a large image
dataset as auxiliary data and then transfers the features learned to the online tracking task. Second 
unlike some previous methods which also learn features from auxiliary data  the learned features in
DLT can be further tuned to adapt to speciﬁc objects during the online tracking process. Because
DLT makes use of multiple nonlinear transformations  the image representations obtained are more
expressive than those of previous methods based on PCA. Moreover  since representing the tracked
object does not require solving an optimization problem as in previous trackers based on sparse
coding  DLT is signiﬁcantly more efﬁcient and hence is more suitable for real-time applications.

2 Particle Filter Approach for Visual Tracking

The particle ﬁlter approach [5] is commonly used for visual tracking. From the statistical per-
spective  it is a sequential Monte Carlo importance sampling method for estimating the latent state
variables of a dynamical system based on a sequence of observations. Supppse st and yt denote
the latent state and observation variables  respectively  at time t. Mathematically  object tracking
corresponds to the problem of ﬁnding the most probable state for each time step t based on the
observations up to the previous time step:

st = argmax p(st | y1:t−1)

(cid:90)

= argmax

p(st | st−1) p(st−1 | y1:t−1) dst−1.

(1)

When a new observation yt arrives  the posterior distribution of the state variable is updated accord-
ing to Bayes’ rule:

p(st | y1:t) =

p(yt | st) p(st | y1:t−1)

p(yt | y1:t−1)

.

(2)

2

i

p(yt | st

i

)

.

· p(yt | st

i | st−1
q(st | s1:t−1  y1:t)

i

i = wt−1

i = wt−1
wt

What is speciﬁc to the particle ﬁlter approach is that it approximates the true posterior state dis-
tribution p(st | y1:t) by a set of n samples  called particles  {st
i}n
i=1 with corresponding impor-
i}n
tance weights {wt
i=1 which sum to 1. The particles are drawn from an importance distribution
q(st | s1:t−1  y1:t) and the weights are updated as follows:
i) p(st

(3)
For the choice of the importance distribution q(st | s1:t−1  y1:t)  it is often simpliﬁed to a ﬁrst-order
Markov process q(st | st−1) in which state transition is independent of the observation. Conse-
quently  the weights are updated as wt
i). Note that the sum of weights may no
longer be equal to 1 after each weight update step. In case it is smaller than a threshold  resampling
is applied to draw n particles from the current particle set in proportion to their weights and then
resetting their weights to 1/n. If the weight sum is above the threshold  linear normalization is
applied to ensure that the weights sum to 1.
For object tracking  the state variable si usually represents the six afﬁne transformation parameters
which correspond to translation  scale  aspect ratio  rotation  and skewness.
In particular  each
dimension of q(st | st−1) is modeled independently by a normal distribution. For each frame  the
tracking result is simply the particle with the largest weight. While many trackers also adopt the
same particle ﬁlter approach  the main difference lies in the formulation of the observation model
p(yt | st
i). Apparently  a good model should be able to distinguish well the tracked object from
the background while still being robust against various types of object variation. For discriminative
trackers  the formulation is often to set the probability exponentially related to the conﬁdence of the
classiﬁer output.
The particle ﬁlter framework is the dominant approach in visual tracking for several reasons. First 
it is more general than the Kalman ﬁlter approach by going beyond the Gaussian distribution. More-
over  it approximates the posterior state distribution by a set of particles instead of just a single point
such as the mode. For visual tracking  this property makes it easier for the tracker to recover from
incorrect tracking results. A tutorial on using particle ﬁlters for visual tracking can be found in [2].
Some recent work  e.g.  [15]  further improves the particle ﬁlter framework for visual tracking.

3 The DLT Tracker

We now present our DLT tracker. During the ofﬂine training stage  unsupervised feature learning is
carried out by training an SDAE with auxiliary image data to learn generic natural image features.
Layer-by-layer pretraining is ﬁrst applied and then the whole SDAE is ﬁne-tuned. During the online
tracking process  an additional classiﬁcation layer is added to the encoder part of the trained SDAE
to result in a classiﬁcation neural network. More details are provided in the rest of this section.

3.1 Ofﬂine Training with Auxiliary Data

3.1.1 Dataset and Preprocessing

We use the Tiny Images dataset [19] as auxiliary data for ofﬂine training. The dataset was collected
from the web by providing non-abstract English nouns to seven search engines  covering many of
the objects and scenes found in the real world. From the almost 80 million tiny images each of
size 32 × 32  we randomly sample 1 million images for ofﬂine training. Since most state-of-the-art
trackers included in our empirical comparison use only grayscale images  we have converted all the
sampled images to grayscale (but our method can also use the color images directly if necessary).
Consequently  each image is represented by a vector of 1024 dimensions corresponding to 1024
pixels. The feature value of each dimension is linearly scaled to the range [0  1] but no further
preprocessing is applied.

3.1.2 Learning Generic Image Features with a Stacked Denoising Autoencoder

The basic building block of an SDAE is a one-layer neural network called a denoising autoencoder
(DAE)  which is a more recent variant of the conventional autoencoder. It learns to recover a data
sample from its corrupted version. In so doing  robust features are learned since the neural network

3

contains a “bottleneck” which is a hidden layer with fewer units than the input units. We show the
architecture of DAE in Fig. 1(a).
Let there be a total of k training samples. For the ith sample  let xi denote the original data sample
and ˜xi be the corrupted version of xi  where the corruption could be masking corruption  additive
Gaussian noise or salt-and-pepper noise. For the network weights  let W and W(cid:48) denote the weights
for the encoder and decoder  respectively  which may be tied though it is not necessary. Similarly 
b and b(cid:48) refer to the bias terms. A DAE learns by solving the following (regularized) optimization
problem:

(cid:107)xi − ˆxi(cid:107)2

2 + λ((cid:107)W(cid:107)2

F + (cid:107)W(cid:48)(cid:107)2
F ) 

(4)

k(cid:88)

i=1

min

W W(cid:48) b b(cid:48)

where

hi = f (W˜xi + b)
(cid:48)
hi + b

(cid:48)

).

ˆxi = f (W

(5)
Here λ is a parameter which balances the reconstruction loss and weight penalty terms  (cid:107)·(cid:107)F denotes
the Frobenius norm  and f (·) is a nonlinear activation function which is typically the logistic sigmoid
function or hyperbolic tangent function. By reconstructing the input from a corrupted version of it 
a DAE is more effective than the conventional autoencoder in discovering more robust features by
preventing the autoencoder from simply learning the identity mapping.
To further enhance learning meaningful features  sparsity constraints [9] are imposed on the mean
activation values of the hidden units. If the logistic sigmoid activation function is used  the output
of each unit may be regarded as the probability of it being active. Let ρj denote the target sparsity
level of the jth unit and ˆρj its average empirical activation rate. The cross-entropy of ρ and ˆρ can
then be introduced as an additional penalty term to Eqn. 4:

(cid:104)

H(ρ(cid:107) ˆρ) = − m(cid:88)
k(cid:88)

j=1

ˆρ =

1
k

i=1

(cid:105)

ρj log(ˆρj) + (1 − ρj) log(1 − ˆρj)

hi 

(6)

where m is the number of hidden units. After the pretraining phase  the SDAE can be unrolled to
form a feedforward neural network. The whole network is ﬁne-tuned using the classical backprop-
agation algorithm. To increase the convergence rate  either the simple momentum method or more
advanced optimization techniques such as the L-BFGS or conjugate gradient method can be applied.
For the network architecture  we use overcomplete ﬁlters in the ﬁrst layer. This is a deliberate
choice since it has been found that an overcomplete basis can usually capture the image structure
better. This is in line with the neurophysiological mechanism in the V1 visual cortex [17]. Then the
number of units is reduced by half whenever a new layer is added until there are only 256 hidden
units  serving as the bottleneck of the autoencoder. The whole structure of the SDAE is depicted in
Fig. 1(b). To further speed up pretraining in the ﬁrst layer to learn localized features  we divide each
32 × 32 tiny image into ﬁve 16 × 16 patches (upper left  upper right  lower left  lower right  and
the center one which overlaps with the other four)  and then train ﬁve DAEs each of which has 512
hidden units. After that  we initialize a large DAE with the weights of the ﬁve small DAEs and then
train the large DAE normally. Some randomly selected ﬁlters in the ﬁrst layer are shown in Fig. 2.
As expected  most of the ﬁlters play the role of highly localized edge detectors.

3.2 Online Tracking Process

The object to track is speciﬁed by the location of its bounding box in the ﬁrst frame. Some neg-
ative examples are collected from the background at a short distance from the object. A sigmoid
classiﬁcation layer is then added to the encoder part of the SDAE obtained from ofﬂine training.
The overall network architecture is shown in Fig. 1(c). When a new video frame arrives  we ﬁrst
draw particles according to the particle ﬁlter approach. The conﬁdence pi of each particle is then
determined by making a simple forward pass through the network. An appealing characteristic of
this approach is that the computational cost of this step is very low even though it has high accuracy.

4

(a)

(b)

(c)

Figure 1: Some key components of the network architecture: (a) denoising autoencoder; (b) stacked
denoising autoencoder; (c) network for online tracking.

Figure 2: Some ﬁlters in the ﬁrst layer of the learned SDAE.

If the maximum conﬁdence of all particles in a frame is below a predeﬁned threshold τ  it may
indicate signiﬁcant appearance change of the object being tracked. To address this issue  the whole
network can be tuned again in case this happens. We note that the threshold τ should be set by
maintaining a tradeoff. If τ is too small  the tracker cannot adapt well to appearance changes. On
the other hand  if τ is too large  even an occluding object or the background may be mis-treated as
the tracked object and hence leads to drifting of the target.

4 Experiments

We empirically compare DLT with some state-of-the-art trackers in this section using 10 challenging
benchmark video sequences. These trackers are: MTT [26]  CT [24]  VTD [15]  MIL [3]  a latest
variant of L1T [4]  TLD [13]  and IVT [18]. We use the original implementations of these trackers
provided by their authors. In case a tracker can only deal with grayscale video  the rgb2gray
function provided by the MATLAB Image Processing Toolbox is used to convert the color video
to grayscale. To accelerate the computation  we also utilize GPU computation provided by the
MATLAB Parallel Computing Toolbox in both ofﬂine training and online tracking. The codes and
supplemental material are provided on the project page: http://winsty.net/dlt.html.

4.1 DLT Implementation Details

We use the gradient method with momentum for optimization. The momentum parameter is set
to 0.9. For ofﬂine training of the SDAE  we inject Gaussian noise with a variance of 0.0004 to
generate the corrupted input. We set λ = 0.0001  ρi = 0.05  and the mini-batch size to 100. For
online tuning  we use a larger λ value of 0.002 to avoid overﬁtting and a smaller mini-batch size
of 10. The threshold τ is set to 0.9. The particle ﬁlter uses 1000 particles. For other parameters such
as the afﬁne parameters in the particle ﬁlter and the search window size in the other methods  we
perform grid search to determine the best values. The same setting is applied to all other methods
compared if applicable.

4.2 Quantitative Comparison

We use two common performance metrics for quantitative comparison: success rate and central-
pixel error. Let BB T denote the bounding box produced by a tracker and BB G the ground-truth

5

bounding box. For each video frame  a tracker is considered successful if the overlap percentage
area(BB T ∩BB G)
area(BB T ∪BB G) > 50%. As for the central-pixel error  it is deﬁned as the Euclidean distance (in
pixels) between the centers of BB T and BB G. The quantitative comparison results are summarized
in Table 1 . For each row which corresponds to one of 10 video sequences  the best result is shown
in red and second best in blue. We also report the central-pixel errors over all frames for each video
sequence. Since TLD can report that the tracked object is missing in some frames  we exclude it
from the central-pixel error comparison. On average  DLT is the best according to both performance
metrics. For most video sequences  it is among the best two methods. We also list the running time
of each sequence in detail in Table 2. Thanks to advances of the GPU technology  our tracker can
achieve an average frame rate of 15fps (frames per second) which is sufﬁcient for many real-time
applications.

MTT
100(3.4)
100(1.3)
68.6(7.8)
66.3(33.7)

CT
24.7(95.4)
70.7(6.0)
25.3(15.3)
23.0(80.4)

VTD
35.2(41.5)
65.6(23.9)
49.4(27.1)
30.1(81.3)

MIL
24.7(81.8)
68.4(19.3)
17.7(13.1)
25.9(71.7)

Ours
car4
100(6.0)
car11
100(1.2)
davidin
66.1(7.1)
trellis
93.6(3.3)
woman
67.1(9.4) 19.8(257.8) 16.0(109.6) 17.1(133.6) 12.2(123.7) 21.1(138.2)
animal
87.3(10.2)
shaking 88.4(11.5)
singer1
100(3.3)
86.5(4.6)
surfer
65.9(16.8)
bird2
average
85.5(7.3)

91.5(10.8)
85.9(10.8)
99.2(5.2)
92.3(10.9)
99.4(3.4)
10.3(16.8)
13.5(18.7)
90.5(5.5)
58.2(19.7) 13.3(151.1)
42.0(38.4)
59.1(48.4)

63.4(16.1)
26.0(28.6)
10.3(26.0)
44.6(14.7)
69.4(16.3)
36.3(41.1)

88.7(11.1)
12.3(28.1)
35.6(34.0)
83.8(6.9)
9.2(92.8)
58.4(47.6)

L1T
30.8(16.8)

TLD
0.2(-)
100(1.3) 29.8(-)
27.3(17.5) 44.4(-)
62.1(37.6) 48.9(-)

IVT
100(4.2)
100(3.2)
92.0(3.9)
44.3(44.7)
5.8(-) 21.5(111.2)
81.7(10.8)
85.9(10.4) 63.4(-)
1.1(138.4)
0.5(90.8) 15.6(-)
96.3(7.9)
100(3.7) 53.6(-)
75.7(9.5) 40.5(-)
90.5(5.9)
45.9(57.5) 31.6(-) 10.2(104.1)
54.9(40.1) 33.4(-)
63.8(43.4)

Table 1: Comparison of 8 trackers on 10 video sequences. The ﬁrst number denotes the success rate
(in percentage)  while the number in parentheses denotes the central-pixel error (in pixels).

car4
15.27

car11
16.04

davidin
13.20

trellis woman
17.30
20.92

animal
10.93

shaking
12.72

singer1
15.18

surfer
14.17

bird2
14.36

Average
15.01

Table 2: Comparison of running time on 10 video sequences (in fps).

Figure 3: Frame-by-frame comparison of 7 trackers on 10 video sequences in terms of central-pixel
error (in pixels).

4.3 Qualitative Comparison

Fig. 4 shows some key frames with bounding boxes reported by all eight trackers for each of the
10 video sequences. More detailed results for the complete video sequences can be found in the
supplemental material.
In both the car4 and car11 sequences  the tracked objects are cars moving on an open road. For car4 
the challenge is that the illumination changes greatly near the entrance of a tunnel. For car11  the

6

0200400600800050100150200250Frame NumberCenter Errorcar4  0100200300400020406080100Frame NumberCenter Errorcar11  0100200300400500050100150Frame NumberCenter Errordavidin  0200400600050100150200250Frame NumberCenter Errortrellis  02004006000200400600800Frame NumberCenter Errorwoman  02004006008000200400600800Frame NumberCenter Erroranimal  0100200300400050100150200250300Frame NumberCenter Errorshaking  0100200300400050100150200250300Frame NumberCenter Errorsinger1  01002003004000102030405060Frame NumberCenter Errorsurfer  020406080100050100150200250300Frame NumberCenter Errorbird2  environment is very dark with illumination in the cluttered background. Since the car being tracked
is a rigid object  its shape does not change much and hence generative trackers like IVT  L1T and
MTT generally perform well for these two sequences. DLT can also track the car accurately.
In the davidin and trellis sequences  each tracker has to track a face in indoor and outdoor en-
vironments  respectively. Both sequences are challenging because the illumination and pose vary
drastically along the video. Moreover  out-of-plane rotation occurs in some frames. As a conse-
quence  all trackers drift or even fail to different degrees. Generally speaking  DLT and MTT yield
the best results which are followed by IVT.
In the woman sequence  we track a woman walking in the street. The woman is severely occluded
several times by the parked cars. TLD ﬁrst fails at frame 63 because of the pose change. All other
trackers compared fail when the woman walks close to the car at about frame 130. DLT can follow
the target accurately.
In the animal sequence  the target is a fast moving animal with motion blur. All methods can merely
track the target to the end. Only MIL and TLD fail in some frames. TLD is also misled by some
similar objects in the background  e.g.  in frame 41.
Both the shaking and singer1 sequences are recordings on the stage with illumination changes. For
shaking  the pose of the head being tracked also changes. L1T  IVT and TLD totally fail before
frame 10  while MTT and MIL show some drifting effects then. VTD and DLT give satisfactory
results which are followed by CT. Compared to shaking  the singer1 sequence is easier to track. All
trackers except MTT can track the object but CT and MIL do not support scale change and hence
the results are less satisfactory.
In the surfer sequence  the goal is to track the head of a surfer while its pose changes along the video
sequence. All trackers can merely track it except that TLD shows an incorrect scale and both CT
and MIL drift slightly.
The bird2 sequence is very challenging since the pose of the bird changes drastically when it is
occluded. Most trackers fail or drift at about frame 15 with the exception of L1T  TLD and DLT.
However  after the bird turns  L1T and TLD totally fail but CT and MIL can recover to some degree.
DLT can track the bird accurately along the entire sequence.

5 Discussions

Our proposed method is similar in spirit to that of [22] but there are some key differences that are
worth noting. First  we learn generic image features from a larger and more general dataset rather
than a smaller set with only some chosen image categories. Second  we learn the image features from
raw images automatically instead of relying on handcrafted SIFT features. Third  further learning is
allowed during the online tracking process of our method so as to adapt better to the speciﬁc object
being tracked.
For the choice of deep network architecture  we note that another potential candidate is the popu-
lar convolutional neural network (CNN) model. The resulting tracker would be similar to previous
patch (or fragment) based methods [1  11] which have been shown to be robust against partial oc-
clusion. Nevertheless  current research on CNN focuses on learning shift-invariant features for such
tasks as image classiﬁcation and object detection. However  the nature of object tracking is very dif-
ferent in that it has to learn shift-variant but similarity-preserving features to overcome the drifting
problem. As of now  there is very little relevant work  with the possible exception of [11] which
tries to improve the pooling step in the sparse coding literature to address this issue. This could be
an interesting future research direction to pursue.

6 Concluding Remarks

In this paper  we have successfully taken deep learning to a new territory of challenging applications.
Noting that the key to success for deep learning architectures is the learning of useful features  we
ﬁrst train a stacked denoising autoencoder using many auxiliary natural images to learn generic
image features. This alleviates the problem of not having much labeled data in visual tracking
applications. After ofﬂine training  the encoder part of the SDAE is used as a feature extractor

7

4
r
a
c

1
1
r
a
c

n
i
d
i
v
a
d

s
i
l
l
e
r
t

n
a
m
o
w

l
a
m
i
n
a

g
n
i
k
a
h
s

1
r
e
g
n
i
s

r
e
f
r
u
s

2
d
r
i
b

Figure 4: Comparison of 8 trackers on 10 video sequences in terms of the bounding box reported.

during the online tracking process to train a classiﬁcation neural network to distinguish the tracked
object from the background. This can be regarded as knowledge transfer from ofﬂine training using
auxiliary data to online tracking. Since further tuning is allowed during the online tracking process 
both the feature extractor and the classiﬁer can adapt to appearance changes of the moving object.
Through quantitative and qualitative comparison with state-of-the-art trackers on some challenging
benchmark video sequences  we demonstrate that our deep learning tracker gives very encouraging
results while having low computational cost.
As the ﬁrst work on applying deep neural networks to visual tracking  many opportunities remain
open for further research. As discussed above  it would be an interesting direction to investigate a
shift-variant CNN. Also  the classiﬁcation layer in our current tracker is just a linear classiﬁer for
simplicity. Extending it to more powerful classiﬁers  as in other discriminative trackers  may provide
more room for further performance improvement.

Acknowledgment

This research has been supported by General Research Fund 621310 from the Research Grants
Council of Hong Kong.

8

References
[1] A. Adam  E. Rivlin  and I. Shimshoni. Robust fragments-based tracking using the integral histogram. In

CVPR  pages 798–805  2006.

[2] M. Arulampalam  S. Maskell  N. Gordon  and T. Clapp. A tutorial on particle ﬁlters for online
IEEE Transactions on Signal Processing  50(2):174–188 

nonlinear/non-Gaussian Bayesian tracking.
2002.

[3] B. Babenko  M. Yang  and S. Belongie. Robust object tracking with online multiple instance learning.

IEEE Transactions on Pattern Analysis and Machine Intelligence  33(8):1619–1632  2011.

[4] C. Bao  Y. Wu  H. Ling  and H. Ji. Real time robust L1 tracker using accelerated proximal gradient

approach. In CVPR  pages 1830–1837  2012.

[5] A. Doucet  D. N. Freitas  and N. Gordon. Sequential Monte Carlo Methods In Practice. Springer  New

York  2001.

[6] H. Grabner  M. Grabner  and H. Bischof. Real-time tracking via on-line boosting. In BMVC  pages 47–56 

2006.

[7] H. Grabner  C. Leistner  and H. Bischof. Semi-supervised on-line boosting for robust tracking. In ECCV 

pages 234–247  2008.

[8] S. Hare  A. Saffari  and P. H. Torr. Struck: Structured output tracking with kernels.

263–270  2011.

In ICCV  pages

[9] G. Hinton. A practical guide to training restricted Boltzmann machines. In Neural Networks: Tricks of

the Trade  pages 599–619. 2012.

[10] G. Hinton  L. Deng  D. Yu  G. Dahl  A. Mohamed  N. Jaitly  A. Senior  V. Vanhoucke  P. Nguyen 
T. Sainath  and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE
Signal Processing Magazine  29(6):82–97  2012.

[11] X. Jia  H. Lu  and M. Yang. Visual tracking via adaptive structural local sparse appearance model. In

CVPR  pages 1822–1829  2012.

[12] Z. Kalal  J. Matas  and K. Mikolajczyk. P-N learning: Bootstrapping binary classiﬁers by structural

constraints. In CVPR  pages 49–56  2010.

[13] Z. Kalal  K. Mikolajczyk  and J. Matas. Tracking-learning-detection.

Analysis and Machine Intelligence  34(7):1409–1422  2012.

IEEE Transactions on Pattern

[14] A. Krizhevsky  I. Sutskever  and G. Hinton.
networks. In NIPS  pages 1106–1114  2012.

ImageNet classiﬁcation with deep convolutional neural

[15] J. Kwon and K. Lee. Visual tracking decomposition. In CVPR  pages 1269–1276  2010.
[16] X. Mei and H. Ling. Robust visual tracking using l1 minimization. In ICCV  pages 1436–1443  2009.
[17] B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1?

Vision Research  37(23):3311–3326  1997.

[18] D. Ross  J. Lim  R. Lin  and M. Yang. Incremental learning for robust visual tracking. International

Journal of Computer Vision  77(1):125–141  2008.

[19] A. Torralba  R. Fergus  and W. Freeman. 80 million tiny images: A large data set for nonparametric object
and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence  30(11):1958–
1970  2008.

[20] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Journal of Machine
Learning Research  11:3371–3408  2010.

[21] D. Wang  H. Lu  and M. Yang. Online object tracking with sparse prototypes. IEEE Transactions on

Image Processing  22(1)  2013.

[22] Q. Wang  F. Chen  J. Yang  W. Xu  and M. Yang. Transferring visual prior for online object tracking.

IEEE Transactions on Image Processing  21(7):3296–3305  2012.

[23] Y. Wu  J. Lim  and M. Yang. Online object tracking: A benchmark. In CVPR  2013.
[24] K. Zhang  L. Zhang  and M.-H. Yang. Real-time compressive tracking. In ECCV  pages 864–877  2012.
[25] T. Zhang  B. Ghanem  S. Liu  and N. Ahuja. Low-rank sparse learning for robust visual tracking. ECCV 

pages 470–484  2012.

[26] T. Zhang  B. Ghanem  S. Liu  and N. Ahuja. Robust visual tracking via multi-task sparse learning. In

CVPR  pages 2042–2049  2012.

9

,Naiyan Wang
Dit-Yan Yeung
Ming-Yu Liu
Jan Kautz