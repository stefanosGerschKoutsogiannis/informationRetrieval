2013,Accelerating Stochastic Gradient Descent using Predictive Variance Reduction,Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem  we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions  we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG).  However  our analysis is significantly simpler and more intuitive. Moreover  unlike SDCA or SAG  our method does not require the storage of gradients  and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.,Accelerating Stochastic Gradient Descent using

Predictive Variance Reduction

Rie Johnson

RJ Research Consulting

Tarrytown NY  USA

Tong Zhang

Baidu Inc.  Beijing  China

Rutgers University  New Jersey  USA

Abstract

Stochastic gradient descent is popular for large scale optimization but has slow
convergence asymptotically due to the inherent variance. To remedy this prob-
lem  we introduce an explicit variance reduction method for stochastic gradient
descent which we call stochastic variance reduced gradient (SVRG). For smooth
and strongly convex functions  we prove that this method enjoys the same fast con-
vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic
Average Gradient (SAG). However  our analysis is signiﬁcantly simpler and more
intuitive. Moreover  unlike SDCA or SAG  our method does not require the stor-
age of gradients  and thus is more easily applicable to complex problems such as
some structured prediction problems and neural network learning.

1

Introduction

In machine learning  we often encounter the following optimization problem. Let ψ1  . . .   ψn be
a sequence of vector functions from Rd to R. Our goal is to ﬁnd an approximate solution of the
following optimization problem

n(cid:88)

i=1

n(cid:88)

i=1

min P (w) 

P (w) :=

1
n

ψi(w).

(1)

For example  given a sequence of n training examples (x1  y1)  . . .   (xn  yn)  where xi ∈ Rd and
yi ∈ R  if we use the squared loss ψi(w) = (w(cid:62)xi − yi)2  then we can obtain least squares
regression. In this case  ψi(·) represents a loss function in machine learning. One may also include
regularization conditions. For example  if we take ψi(w) = ln(1 + exp(−w(cid:62)xiyi)) + 0.5λw(cid:62)w
(yi ∈ {±1})  then the optimization problem becomes regularized logistic regression.
A standard method is gradient descent  which can be described by the following update rule for
t = 1  2  . . .

w(t) = w(t−1) − ηt∇P (w(t−1)) = w(t−1) − ηt
n

∇ψi(w(t−1)).

(2)

However  at each step  gradient descent requires evaluation of n derivatives  which is expensive. A
popular modiﬁcation is stochastic gradient descent (SGD): where at each iteration t = 1  2  . . .  we
draw it randomly from {1  . . .   n}  and

w(t) = w(t−1) − ηt∇ψit(w(t−1)).

(3)
The expectation E[w(t)|w(t−1)] is identical to (2). A more general version of SGD is the following
(4)

w(t) = w(t−1) − ηtgt(w(t−1)  ξt) 

1

where ξt is a random variable that may depend on w(t−1)  and the expectation (with respect to ξt)
E[gt(w(t−1)  ξt)|w(t−1)] = ∇P (w(t−1)). The advantage of stochastic gradient is that each step
only relies on a single derivative ∇ψi(·)  and thus the computational cost is 1/n that of the standard
gradient descent. However  a disadvantage of the method is that the randomness introduces variance
— this is caused by the fact that gt(w(t−1)  ξt) equals the gradient ∇P (w(t−1)) in expectation  but
each gt(w(t−1)  ξt) is different. In particular  if gt(w(t−1)  ξt) is large  then we have a relatively
large variance which slows down the convergence. For example  consider the case that each ψi(w)
is smooth

ψi(w) − ψi(w(cid:48)) − 0.5L(cid:107)w − w(cid:48)(cid:107)2 ≤ ∇ψi(w(cid:48))(cid:62)(w − w(cid:48)) 

(5)

and convex; and P (w) is strongly convex

2 ≥ ∇P (w(cid:48))(cid:62)(w − w(cid:48)) 

P (w) − P (w(cid:48)) − 0.5γ(cid:107)w − w(cid:48)(cid:107)2

(6)
where L ≥ γ ≥ 0. As long as we pick ηt as a constant η < 1/L  we have linear convergence of
O((1 − γ/L)t) Nesterov [2004]. However  for SGD  due to the variance of random sampling  we
generally need to choose ηt = O(1/t) and obtain a slower sub-linear convergence rate of O(1/t).
This means that we have a trade-off of fast computation per iteration and slow convergence for
SGD versus slow computation per iteration and fast convergence for gradient descent. Although
the fast computation means it can reach an approximate solution relatively quickly  and thus has
been proposed by various researchers for large scale problems Zhang [2004]  Shalev-Shwartz et al.
[2007] (also see Leon Bottou’s Webpage http://leon.bottou.org/projects/sgd)  the
convergence slows down when we need a more accurate solution.
In order to improve SGD  one has to design methods that can reduce the variance  which allows
us to use a larger learning rate ηt. Two recent papers Le Roux et al. [2012]  Shalev-Shwartz and
Zhang [2012] proposed methods that achieve such a variance reduction effect for SGD  which leads
to a linear convergence rate when ψi(w) is smooth and strongly convex. The method in Le Roux
et al. [2012] was referred to as SAG (stochastic average gradient)  and the method in Shalev-Shwartz
and Zhang [2012] was referred to as SDCA. These methods are suitable for training convex linear
prediction problems such as logistic regression or least squares regression  and in fact  SDCA is the
method implemented in the popular lib-SVM package Hsieh et al. [2008]. However  both propos-
als require storage of all gradients (or dual variables). Although this issue may not be a problem
for training simple regularized linear prediction problems such as least squares regression  the re-
quirement makes it unsuitable for more complex applications where storing all gradients would be
impractical. One example is training certain structured learning problems with convex loss  and an-
other example is training nonconvex neural networks. In order to remedy the problem  we propose a
different method in this paper that employs explicit variance reduction without the need to store the
intermediate gradients. We show that if ψi(w) is strongly convex and smooth  then the same con-
vergence rate as those of Le Roux et al. [2012]  Shalev-Shwartz and Zhang [2012] can be obtained.
Even if ψi(w) is nonconvex (such as neural networks)  under mild assumptions  it can be shown that
asymptotically the variance of SGD goes to zero  and thus faster convergence can be achieved. In
summary  this work makes the following three contributions:

• Our method does not require the storage of full gradients  and thus is suitable for some
problems where methods such as Le Roux et al. [2012]  Shalev-Shwartz and Zhang [2012]
cannot be applied.
• We provide a much simpler proof of the linear convergence results for smooth and strongly
convex loss  and our view provides a signiﬁcantly more intuitive explanation of the fast
convergence by explicitly connecting the idea to variance reduction in SGD. The resulting
insight can easily lead to additional algorithmic development.
• The relatively intuitive variance reduction explanation also applies to nonconvex optimiza-
tion problems  and thus this idea can be used for complex problems such as training deep
neural networks.

2 Stochastic Variance Reduced Gradient

One practical issue for SGD is that in order to ensure convergence the learning rate ηt has to decay
to zero. This leads to slower convergence. The need for a small learning rate is due to the variance

2

of SGD (that is  SGD approximates the full gradient using a small batch of samples or even a single
example  and this introduces variance). However  there is a ﬁx described below. At each time  we
keep a version of estimated w as ˜w that is close to the optimal w. For example  we can keep a
snapshot of ˜w after every m SGD iterations. Moreover  we maintain the average gradient

˜µ = ∇P ( ˜w) =

1
n

∇ψi( ˜w) 

n(cid:88)

i=1

and its computation requires one pass over the data using ˜w. Note that the expectation of ∇ψi( ˜w)−
˜µ over i is zero  and thus the following update rule is generalized SGD: randomly draw it from
{1  . . .   n}:

w(t) = w(t−1) − ηt(∇ψi(w(t−1)) − ∇ψit( ˜w) + ˜µ).

(7)

We thus have
That is  if we let the random variable ξt = it and gt(w(t−1)  ξt) = ∇ψit(w(t−1)) − ∇ψit( ˜w) + ˜µ 
then (7) is a special case of (4).
The update rule in (7) can also be obtained by deﬁning the auxiliary function

E[w(t)|w(t−1)] = w(t−1) − ηt∇P (w(t−1)).

˜ψi(w) = ψi(w) − (∇ψi( ˜w) − ˜µ)(cid:62)w.

Since(cid:80)n

n(cid:88)
i=1(∇ψi( ˜w) − ˜µ) = 0  we know that

P (w) =

ψi(w) =

˜ψi(w).

1
n

i=1

n(cid:88)

i=1

1
n

(cid:80)n

Now we may apply the standard SGD to the new representation P (w) = 1
n
the update rule (7).
To see that the variance of the update rule (7) is reduced  we note that when both ˜w and w(t) converge
to the same parameter w∗  then ˜µ → 0. Therefore if ∇ψi( ˜w) → ∇ψi(w∗)  then

˜ψi(w) and obtain

i=1

∇ψi(w(t−1)) − ∇ψi( ˜w) + ˜µ → ∇ψi(w(t−1)) − ∇ψi(w∗) → 0.

This argument will be made more rigorous in the next section  where we will analyze the algorithm
in Figure 1 that summarizes the ideas described in this section. We call this method stochastic
variance reduced gradient (SVRG) because it explicitly reduces the variance of SGD. Unlike SGD 
the learning rate ηt for SVRG does not have to decay  which leads to faster convergence as one can
use a relatively large learning rate. This is conﬁrmed by our experiments.
In practical implementations  it is natural to choose option I  or take ˜ws to be the average of the
past t iterates. However  our analysis depends on option II. Note that each stage s requires 2m + n
gradient computations (for some convex problems  one may save the intermediate gradients ∇ψi( ˜w) 
and thus only m + n gradient computations are needed). Therefore it is natural to choose m to
be the same order of n but slightly larger (for example m = 2n for convex problems and m =
5n for nonconvex problems in our experiments). In comparison  standard SGD requires only m
gradient computations. Since gradient may be the computationally most intensive operation  for fair
comparison  we compare SGD to SVRG based on the number of gradient computations.

3 Analysis

For simplicity we will only consider the case that each ψi(w) is convex and smooth  and P (w) is
strongly convex.
Theorem 1. Consider SVRG in Figure 1 with option II. Assume that all ψi are convex and both (5)
and (6) hold with γ > 0. Let w∗ = arg minw P (w). Assume that m is sufﬁciently large so that

2Lη
1 − 2Lη
then we have geometric convergence in expectation for SVRG:

γη(1 − 2Lη)m

α =

+

1

< 1 

E P ( ˜ws) ≤ E P (w∗) + αs[P ( ˜w0) − P (w∗)]

3

Procedure SVRG

Parameters update frequency m and learning rate η
Initialize ˜w0
(cid:80)n
Iterate: for s = 1  2  . . .
i=1 ∇ψi( ˜w)

˜w = ˜ws−1
˜µ = 1
n
w0 = ˜w
Iterate: for t = 1  2  . . .   m
Randomly pick it ∈ {1  . . .   n} and update weight
wt = wt−1 − η(∇ψit(wt−1) − ∇ψit( ˜w) + ˜µ)
end
option I: set ˜ws = wm
option II: set ˜ws = wt for randomly chosen t ∈ {0  . . .   m − 1}
end

Figure 1: Stochastic Variance Reduced Gradient

Proof. Given any i  consider

gi(w) = ψi(w) − ψi(w∗) − ∇ψi(w∗)(cid:62)(w − w∗).

We know that gi(w∗) = minw gi(w) since ∇gi(w∗) = 0. Therefore

0 = gi(w∗) ≤ min
≤ min

η

η

[gi(w − η∇gi(w))]
[gi(w) − η(cid:107)∇gi(w)(cid:107)2

2 + 0.5Lη2(cid:107)∇gi(w)(cid:107)2

2] = gi(w) − 1
2L

(cid:107)∇gi(w)(cid:107)2
2.

That is 

n(cid:88)

(cid:107)∇ψi(w) − ∇ψi(w∗)(cid:107)2

2 ≤ 2L[ψi(w) − ψi(w∗) − ∇ψi(w∗)(cid:62)(w − w∗)].

By summing the above inequality over i = 1  . . .   n  and using the fact that ∇P (w∗) = 0  we obtain

n−1

(cid:107)∇ψi(w) − ∇ψi(w∗)(cid:107)2

2 ≤ 2L[P (w) − P (w∗)].

(8)
We can now proceed to prove the theorem. Let vt = ∇ψit(wt−1) − ∇ψit( ˜w) + ˜µ. Conditioned on
wt−1  we can take expectation with respect to it  and obtain:

i=1

E (cid:107)vt(cid:107)2

2

2 + 2 E (cid:107)[∇ψit( ˜w) − ∇ψit(w∗)] − ∇P ( ˜w)(cid:107)2
2 + 2 E (cid:107)[∇ψit( ˜w) − ∇ψit(w∗)]

≤2 E (cid:107)∇ψit(wt−1) − ∇ψit(w∗)(cid:107)2
=2 E (cid:107)∇ψit(wt−1) − ∇ψit(w∗)(cid:107)2
− E [∇ψit( ˜w) − ∇ψit(w∗)](cid:107)2
≤2 E (cid:107)∇ψit(wt−1) − ∇ψit(w∗)(cid:107)2
≤4L[P (wt−1) − P (w∗) + P ( ˜w) − P (w∗)].
2 and ˜µ = ∇P ( ˜w). The second inequality uses
2 + 2(cid:107)b(cid:107)2
2 for any random vector ξ. The third inequality uses (8).

2 + 2 E (cid:107)∇ψit( ˜w) − ∇ψit(w∗)(cid:107)2

The ﬁrst inequality uses (cid:107)a + b(cid:107)2
2 − (cid:107) E ξ(cid:107)2
E(cid:107)ξ − E ξ(cid:107)2
Now by noticing that conditioned on wt−1  we have E vt = ∇P (wt−1); and this leads to

2 ≤ 2(cid:107)a(cid:107)2
2 ≤ E(cid:107)ξ(cid:107)2

2 = E(cid:107)ξ(cid:107)2

2

2

2

2

E (cid:107)wt − w∗(cid:107)2
=(cid:107)wt−1 − w∗(cid:107)2
≤(cid:107)wt−1 − w∗(cid:107)2
≤(cid:107)wt−1 − w∗(cid:107)2
=(cid:107)wt−1 − w∗(cid:107)2

2 − 2η(wt−1 − w∗)(cid:62) E vt + η2 E (cid:107)vt(cid:107)2
2 − 2η(wt−1 − w∗)(cid:62)∇P (wt−1) + 4Lη2[P (wt−1) − P (w∗) + P ( ˜w) − P (w∗)]
2 − 2η[P (wt−1) − P (w∗)] + 4Lη2[P (wt−1) − P (w∗) + P ( ˜w) − P (w∗)]
2 − 2η(1 − 2Lη)[P (wt−1) − P (w∗)] + 4Lη2[P ( ˜w) − P (w∗)].

2

4

2 + 2η(1 − 2Lη)m E [P ( ˜ws) − P (w∗)]
2 + 4Lmη2 E[P ( ˜w) − P (w∗)]
2 + 4Lmη2 E[P ( ˜w) − P (w∗)]

E (cid:107)wm − w∗(cid:107)2
≤ E(cid:107)w0 − w∗(cid:107)2
= E(cid:107) ˜w − w∗(cid:107)2
≤ 2
γ
=2(γ−1 + 2Lmη2) E[P ( ˜w) − P (w∗)].

E[P ( ˜w) − P (w∗)] + 4Lmη2 E[P ( ˜w) − P (w∗)]

(cid:20)

(cid:21)

The second inequality uses the strong convexity property (6). We thus obtain

E [P ( ˜ws) − P (w∗)] ≤

1

γη(1 − 2Lη)m

+

2Lη
1 − 2Lη

E[P ( ˜ws−1) − P (w∗)].

This implies that E [P ( ˜ws) − P (w∗)] ≤ αs E [P ( ˜w0) − P (w∗)]. The desired bound follows.

The bound we obtained in Theorem 1 is comparable to those obtained in Le Roux et al. [2012] 
Shalev-Shwartz and Zhang [2012] (if we ignore the log factor). To see this  we may consider for
simplicity the most indicative case where the condition number L/γ = n. Due to the poor condition
number  the standard batch gradient descent requires complexity of n ln(1/) iterations over the
data to achieve accuracy of   which means we have to process n2 ln(1/) number of examples. In
comparison  in our procedure we may take η = 0.1/L and m = O(n) to obtain a convergence rate of
α = 1/2. Therefore to achieve an accuracy of   we need to process n ln(1/) number of examples.
This matches the results of Le Roux et al. [2012]  Shalev-Shwartz and Zhang [2012]. Nevertheless 
our analysis is signiﬁcantly simpler than both Le Roux et al. [2012] and Shalev-Shwartz and Zhang
[2012]  and the explicit variance reduction argument provides better intuition on why this method
works. In fact  in Section 4 we show that a similar intuition can be used to explain the effectiveness
of SDCA.
The SVRG algorithm can also be applied to smooth but not strongly convex problems. A con-
√
vergence rate of O(1/T ) may be obtained  which improves the standard SGD convergence rate of
T ). In order to apply SVRG to nonconvex problems such as neural networks  it is useful
O(1/
to start with an initial vector ˜w0 that is close to a local minimum (which may be obtained with
SGD)  and then the method can be used to accelerate the local convergence rate of SGD (which may
converge very slowly by itself). If the system is locally (strongly) convex  then Theorem 1 can be
directly applied  which implies local geometric convergence rate with a constant learning rate.

4 SDCA as Variance Reduction

It can be shown that both SDCA and SAG are connected to SVRG in the sense they are also a
variance reduction methods for SGD  although using different techniques.
In the following we
present the variance reduction view of SDCA  which provides additional insights into these recently
proposed fast convergence methods for stochastic optimization. In SDCA  we consider the following
problem with convex φi(w):

w∗ = arg min P (w) 

P (w) =

1
n

φi(w) + 0.5λw(cid:62)w.

(9)

The ﬁrst inequality uses the previously obtained inequality for E(cid:107)vt(cid:107)2
convexity of P (w)  which implies that −(wt−1 − w∗)(cid:62)∇P (wt−1) ≤ P (w∗) − P (wt−1).
We consider a ﬁxed stage s  so that ˜w = ˜ws−1 and ˜ws is selected after all of the updates have
completed. By summing the previous inequality over t = 1  . . .   m  taking expectation with all the
history  and using option II at stage s  we obtain

2  and the second inequality

n(cid:88)

i=1

This is the same as our formulation with ψi(w) = φi(w) + 0.5λw(cid:62)w.
We can take the derivative of (9) and derive a “dual” representation of w at the solution w∗ as:

w∗ =

α∗

i

(j = 1  . . .   k) 

n(cid:88)

i=1

5

where the dual variables

Therefore in the SGD update (3)  if we maintain a representation

∇φi(w∗).

i = − 1
α∗
λn
n(cid:88)

w(t) =

α(t)

i

 

(10)

(11)

(12)

then the update of α becomes:

α(t)

(cid:96) =

i=1

(cid:40)
(1 − ηtλ)α(t−1)
(1 − ηtλ)α(t−1)

i

(cid:96)

− ηt∇φi(w)

(cid:96) = i
(cid:96) (cid:54)= i

.

This update rule requires ηt → 0 when t → ∞.
Alternatively  we may consider starting with SGD by maintaining (11)  and then apply the following
Dual Coordinate Ascent rule:

α(t)

(cid:96) =

α(t−1)
α(t−1)

i

(cid:96)

− ηt(∇φi(w(t−1)) + λnα(t−1)

i

)

(cid:96) = i
(cid:96) (cid:54)= i

(j = 1  . . .   k)

(13)

(cid:40)

and then update w as w(t) = w(t−1) + (α(t)
It can be checked that if we take expectation over random i ∈ {1  . . .   n}  then the SGD rule in (12)
and the dual coordinate ascent rule (13) both yield the gradient descent rule

i − α(t−1)

).

i

E[w(t|w(t−1)] = w(t−1) − ηt∇P (w(t−1)).

Therefore both can be regarded as different realizations of the more general stochastic gradient rule
in (4). However  the advantage of (13) is that we may take a larger step when t → ∞. This is be-
cause according to (10)  when the primal-dual parameters (w  α) converge to the optimal parameters
(w∗  α∗)  we have

(∇φi(w) + λnαi) → 0 

which means that even if the learning rate ηt stays bounded away from zero  the procedure can
converge. This is the same effect as SVRG  in the sense that the variance goes to zero asymptotically:
as w → w∗ and α → α∗  we have

n(cid:88)

i=1

1
n

(∇φi(w) + λnαi)2 → 0.

That is  SDCA is also a variance reduction method for SGD  which is similar to SVRG.
From this discussion  we can view SVRG as an explicit variance reduction technique for SGD which
is similar to SDCA. However  it is simpler  more intuitive  and easier to analyze. This relationship
provides useful insights into the underlying optimization problem that may allow us to make further
improvements.

5 Experiments

To conﬁrm the theoretical results and insights  we experimented with SVRG (Fig. 1 Option I) in
comparison with SGD and SDCA with linear predictors (convex) and neural nets (nonconvex). In
all the ﬁgures  the x-axis is computational cost measured by the number of gradient computations
divided by n. For SGD  it is the number of passes to go through the training data  and for SVRG
in the nonconvex case (neural nets)  it includes the additional computation of ∇ψi( ˜w) both in each
iteration and for computing the gradient average ˜µ. For SVRG in our convex case  however  ∇ψi( ˜w)
does not have to be re-computed in each iteration. Since in this case the gradient is always a multiple
of xi  i.e.  ∇ψi(w) = φ(cid:48)
i(w(cid:62)xi)xi where ψi(w) = φi(w(cid:62)xi)  ∇ψi( ˜w) can be compactly saved in
memory by only saving scalars φ(cid:48)
i( ˜w(cid:62)xi) with the same memory consumption as SDCA and SAG.
The interval m was set to 2n (convex) and 5n (nonconvex). The weights for SVRG were initialized
by performing 1 iteration (convex) or 10 iterations (nonconvex) of SGD; therefore  the line for
SVRG starts after x = 1 (convex) or x = 10 (nonconvex) in the respective ﬁgures.

6

Figure 2: Multiclass logistic regression (convex) on MNIST. (a) Training loss comparison with SGD with
ﬁxed learning rates. The numbers in the legends are the learning rate. (b) Training loss residual P (w)−P (w∗);
comparison with best-tuned SGD with learning rate scheduling and SDCA. (c) Variance of weight update
(including multiplication with the learning rate).

First  we performed L2-regularized multiclass logistic regression (convex optimization) on MNIST1
with regularization parameter λ =1e-4. Fig. 2 (a) shows training loss (i.e.  the optimization objective
P (w)) in comparison with SGD with ﬁxed learning rates. The results are indicative of the known
weakness of SGD  which also illustrates the strength of SVRG. That is  when a relatively large
learning rate η is used with SGD  training loss drops fast at ﬁrst  but it oscillates above the minimum
and never goes down to the minimum. With small η  the minimum may be approached eventually 
but it will take many iterations to get there. Therefore  to accelerate SGD  one has to start with
relatively large η and gradually decrease it (learning rate scheduling)  as commonly practiced. By
contrast  using a single relatively large value of η  SVRG smoothly goes down faster than SGD.
This is in line with our theoretical prediction that one can use a relatively large η with SVRG  which
leads to faster convergence.
Fig. 2 (b) and (c) compare SVRG with best-tuned SGD with learning rate scheduling and SDCA.
‘SGD-best’ is the best-tuned SGD  which was chosen by preferring smaller training loss from a
large number of parameter combinations for two types of learning scheduling: exponential decay
η(t) = η0a(cid:98)t/n(cid:99) with parameters η0 and a to adjust and t-inverse η(t) = η0(1 + b(cid:98)t/n(cid:99))−1 with η0
and b to adjust. (Not surprisingly  the best-tuned SGD with learning rate scheduling outperformed
the best-tuned SGD with a ﬁxed learning rate throughout our experiments.) Fig. 2 (b) shows training
loss residual  which is training loss minus the optimum (estimated by running gradient descent for
a very long time): P (w) − P (w∗). We observe that SVRG’s loss residual goes down exponentially 
which is in line with Theorem 1  and that SVRG is competitive with SDCA (the two lines are almost
overlapping) and decreases faster than SGD-best. In Fig. 2 (c)  we show the variance of SVRG
update −η(∇ψi(w) − ∇ψi( ˜w) + ˜µ) in comparison with the variance of SGD update −η(t)∇ψi(w)
and SDCA. As expected  the variance of both SVRG and SDCA decreases as optimization proceeds 
and the variance of SGD with a ﬁxed learning rate (‘SGD:0.001’) stays high. The variance of the
best-tuned SGD decreases  but this is due to the forced exponential decay of the learning rate and
the variance of the gradients ∇ψi(w) (the dotted line labeled as ‘SGD-best/η(t)’) stays high.
Fig. 3 shows more convex-case results (L2-regularized logistic regression) in terms of training loss
residual (top) and test error rate (bottom) on rcv1.binary and covtype.binary from the LIBSVM site2 
protein3  and CIFAR-104. As protein and covtype do not come with labeled test data  we randomly
split the training data into halves to make the training/test split. CIFAR was normalized into [0  1] by
division with 255 (which was also done with MNIST and CIFAR in the other ﬁgures)  and protein
was standardized. λ was set to 1e-3 (CIFAR) and 1e-5 (rest). Overall  SVRG is competitive with
SDCA and clearly more advantageous than the best-tuned SGD. It is also worth mentioning that a
recent study Schmidt et al. [2013] reports that SAG and SDCA are competitive.
To test SVRG with nonconvex objectives  we trained neural nets (with one fully-connected hidden
layer of 100 nodes and ten softmax output nodes; sigmoid activation and L2 regularization) with
mini-batches of size 10 on MNIST and CIFAR-10  both of which are standard datasets for deep

1 http://yann.lecun.com/exdb/mnist/
2 http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/
3 http://osmot.cs.cornell.edu/kddcup/datasets.html
4 www.cs.toronto.edu/˜kriz/cifar.html

7

(a)                                                                                (b0.250.260.270.280.290.30.31050100Training loss#grad / nMNIST convex: training loss: P(w)SGD:0.005SGD:0.0025SGD:0.001SVRG:0.0251E-131E-101E-071E-041E-011E+020Training loss -optimumMNIST convex: training loss residual SVRGSGD-best(a)                                                                                (b)                                                                               (c)50100#grad / nMNIST convex: training loss residual P(w)-P(w*)SDCASGD:0.0011E-161E-131E-101E-071E-041E-011E+02050100Variance#grad / nMNIST convex: update varianceSVRGSDCASGD-bestSGD:0.001SGD-best/η(t)Figure 3: More convex-case results. Loss residual P (w) − P (w∗) (top) and test error rates (down). L2-
regularized logistic regression (10-class for CIFAR-10 and binary for the rest).

Figure 4: Neural net results (nonconvex).

neural net studies; λ was set to 1e-4 and 1e-3  respectively. In Fig. 4 we conﬁrm that the results are
similar to the convex case; i.e.  SVRG reduces the variance and smoothly converges faster than the
best-tuned SGD with learning rate scheduling  which is a de facto standard method for neural net
training. As said earlier  methods such as SDCA and SAG are not practical for neural nets due to
their memory requirement. We view these results as promising. However  further investigation  in
particular with larger/deeper neural nets for which training cost is a critical issue  is still needed.

6 Conclusion

This paper introduces an explicit variance reduction method for stochastic gradient descent meth-
ods. For smooth and strongly convex functions  we prove that this method enjoys the same fast
convergence rate as those of SDCA and SAG. However  our proof is signiﬁcantly simpler and more
intuitive. Moreover  unlike SDCA or SAG  this method does not require the storage of gradients  and
thus is more easily applicable to complex problems such as structured prediction or neural network
learning.

Acknowledgment

We thank Leon Bottou and Alekh Agarwal for spotting a mistake in the original theorem.

8

1E-121E-101E-81E-61E-41E-20102030training loss -optimum#grad / nrcv1 convexSGD-bestSDCASVRG0.0350.040.0450.050102030Test error rate#grad / nrcv1 convexSGD-bestSDCASVRG1E-61E-51E-41E-31E-20102030training loss -optimum#grad / nprotein convexSGD-bestSDCASVRG0.0020.0030.0040.0050.0060102030Test error rate#grad / nprotein convexSGD-bestSDCASVRG1E-141E-121E-101E-81E-61E-41E-20102030training loss -optimum#grad / ncover type convexSGD-bestSDCASVRG0.240.2450.250.2550.260102030Test error rate#grad / ncover type convexSGD-bestSDCASVRG1E-121E-101E-081E-061E-041E-021E+00050100training loss -optimum#grad / nCIFAR10 convexSGD-bestSDCASVRG0.580.60.620.640.66050100Test error rate#grad / nCIFAR10 convexSGD-bestSDCASVRG0.090.0950.10.1050.110100200Training loss#grad / nMNIST nonconvexSGD-bestSVRG1E-51E-41E-31E-21E-11E+00100200Variance#grad / nMNIST nonconvexSGD-best/η(t)SGD-bestSVRG1.41.451.51.551.60200400Training loss#grad / nCIFAR10 nonconvexSGD-bestSVRG0.450.460.470.480.490.50.510.520200400Test error rate#grad / nCIFAR10 nonconvexSGD-bestSVRGReferences
C.J. Hsieh  K.W. Chang  C.J. Lin  S.S. Keerthi  and S. Sundararajan. A dual coordinate descent

method for large-scale linear SVM. In ICML  pages 408–415  2008.

Nicolas Le Roux  Mark Schmidt  and Francis Bach. A Stochastic Gradient Method with an Ex-
ponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets. arXiv
preprint arXiv:1202.6258  2012.

Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  Boston  2004.
Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. arXiv preprint arXiv:1309.2388  2013.

S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for

SVM. In International Conference on Machine Learning  pages 807–814  2007.

Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. arXiv preprint arXiv:1209.1873  2012.

T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algo-
rithms. In Proceedings of the Twenty-First International Conference on Machine Learning  2004.

9

,Rie Johnson
Tong Zhang
firdaus janoos
Huseyin Denli
Niranjan Subrahmanya
Patrick Putzky
Max Welling