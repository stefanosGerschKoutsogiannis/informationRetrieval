2011,Generalised Coupled Tensor Factorisation,We derive algorithms for generalised tensor factorisation (GTF) by building upon the well-established theory of Generalised Linear Models. Our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework  derived for a broad class of exponential family distributions including special cases such as Tweedie's distributions corresponding to $\beta$-divergences. By bounding the step size of the Fisher Scoring iteration of the GLM  we obtain general updates for real data and multiplicative updates for non-negative data. The GTF framework is  then extended easily to address the problems when multiple observed tensors are factorised simultaneously. We illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.,Generalised Coupled Tensor Factorisation

Y. Kenan Yılmaz

A. Taylan Cemgil

Umut S¸ims¸ekli

Department of Computer Engineering
Bo˘gazic¸i University  Istanbul  Turkey

kenan@sibnet.com.tr  {taylan.cemgil  umut.simsekli}@boun.edu.tr

Abstract

We derive algorithms for generalised tensor factorisation (GTF) by building upon
the well-established theory of Generalised Linear Models. Our algorithms are
general in the sense that we can compute arbitrary factorisations in a message
passing framework  derived for a broad class of exponential family distribu-
tions including special cases such as Tweedie’s distributions corresponding to β-
divergences. By bounding the step size of the Fisher Scoring iteration of the GLM 
we obtain general updates for real data and multiplicative updates for non-negative
data. The GTF framework is  then extended easily to address the problems when
multiple observed tensors are factorised simultaneously. We illustrate our coupled
factorisation approach on synthetic data as well as on a musical audio restoration
problem.

1

Introduction

A fruitful modelling approach for extracting meaningful information from highly structured mul-
tivariate datasets is based on matrix factorisations (MFs). In fact  many standard data processing
methods of machine learning and statistics such as clustering  source separation  independent com-
ponents analysis (ICA)  nonnegative matrix factorisation (NMF)  latent semantic indexing (LSI)
can be expressed and understood as MF problems. These MF models also have well understood
probabilistic interpretations as probabilistic generative models. Indeed  many standard algorithms
mentioned above can be derived as maximum likelihood or maximum a-posteriori parameter esti-
mation procedures. It is also possible to do a full Bayesian treatment for model selection [1].

Tensors appear as a natural generalisation of matrix factorisation  when observed data and/or a latent
representation have several semantically meaningful dimensions. Before giving a formal deﬁnition 
consider the following motivating example

Z j r
2 Z q r

5

(1)

X i j k

1 ≈Xr

Z i r
1 Z j r

2 Z k r

3

X j p

2 ≈Xr

Z j r
2 Z p r

4

X j q

3 ≈Xr

where X1 is an observed 3-way array and X2  X3 are 2-way arrays  while Zα for α = 1 . . . 5 are
the latent 2-way arrays. Here  the 2-way arrays are just matrices but this can be easily extended to
objects having arbitrary number of indices. As the term ’N-way array’ is awkward  we prefer using
the more convenient term tensor. Here  Z2 is a shared factor  coupling all models. As the ﬁrst model
is a CP (Parafac) while the second and the third ones are MF’s  we call the combined factorization
as CP/MF/MF model. Such models are of interest when one can obtain different ’views’ of the
same piece of information (here Z2) under different experimental conditions. Singh and Gordon
[2] focused on a similar problem called as collective matrix factorisation (CMF) or multi-matrix
factorisation  for relational learning but only for matrix factors and observations. In addition  their
generalised Bregman divergence minimisation procedure assumes matching link and loss functions.
For coupled matrix and tensor factorization (CMTF)  recently [3] proposed a gradient-based all-
at-once optimization method as an alternative to alternating least square (ALS) optimization and

1

demonstrated their approach for a CP/MF coupled model. Similar models are used for protein-
protein interactions (PPI) problems in gene regulation [4].

The main motivation of the current paper is to construct a general and practical framework for
computation of tensor factorisations (TF)  by extending the well-established theory of Generalised
Linear Models (GLM). Our approach is also partially inspired by probabilistic graphical models:
our computation procedures for a given factorisation have a natural message passing interpretation.
This provides a structured and efﬁcient approach that enables very easy development of application
speciﬁc custom models  priors or error measures as well as algorithms for joint factorisations where
an arbitrary set of tensors can be factorised simultaneously. Well known models of multiway analysis
(Parafac  Tucker [5]) appear as special cases and novel models and associated inference algorithms
can be automatically be developed. In [6]  the authors take a similar approach to tensor factorisations
as ours  but that work is limited to KL and Euclidean costs  generalising MF models of [7] to the
tensor case. It is possible to generalise this line of work to β-divergences [8] but none of these works
address the coupled factorisation case and consider only a restricted class of cost functions.

2 Generalised Linear Models for Matrix/Tensor Factorisation

To set the notation and our approach  we brieﬂy review GLMs following closely the original notation
of [9  ch 5]. A GLM assumes that a data vector x has conditionally independently drawn components
xi according to an exponential family density

xi ∼ exp(cid:16) xiγi − b(γi)

τ 2

− c(xi  τ )(cid:17)

hxii = ˆxi =

∂b(γi)

∂γi

var(xi) = τ 2 ∂2b(γi)
∂γ2
i

(2)

i z where l⊤
i

Here  γi are canonical parameters and τ 2 is a known dispersion parameter. hxii is the expectation of
xi and b(·) is the log partition function  enforcing normalization. The canonical parameters are not
directly estimated  instead one assumes a link function g(·) that ’links’ the mean of the distribution
is the ith row vector of a known model matrix L and
ˆxi and assumes that g(ˆxi) = l⊤
z is the parameter vector to be estimated  A⊤ denotes matrix transpose of A. The model is linear
in the sense that a function of the mean is linear in parameters  i.e.  g(ˆx) = Lz . A Linear Model
(LM) is a special case of GLM that assumes normality  i.e. xi ∼ N (xi; ˆxi  σ2) as well as linearity
that implies identity link function as g(ˆxi) = ˆxi = l⊤
i z assuming li are known. Logistic regression
assumes a log link  g(ˆxi) = log ˆxi = l⊤
The goal in classical GLM is to estimate the parameter vector z. This is typically achieved via
a Gauss-Newton method (Fisher Scoring). The necessary objects for this computation are the log
likelihood  the derivative and the Fisher Information (the expected value of negative of the Fisher
Score). These are easily derived as:

i z; here log ˆxi and z have a linear relationship [9].

L =Xi

[xiγi − b(γi)]/τ 2 −Xi
1
τ 2 L⊤DG(x − ˆx)

=

∂L
∂z

c(xi  τ )

∂L
∂z

=

(cid:28) ∂2L
∂z2(cid:29) =

(xi − ˆxi)wigˆx(ˆxi)l⊤
i

1

τ 2 Xi
1
τ 2 L⊤DL

(3)

(4)

where w is a vector with elements wi  D and G are the diagonal matrices as D = diag(w)  G =
diag(gˆx(ˆxi)) and

wi =(cid:16)v(ˆxi)g2

ˆx(ˆxi)(cid:17)−1

gˆx(ˆxi) =

∂g(ˆxi)

∂ ˆxi

(5)

with v(ˆxi) being the variance function related to the observation variance by var(xi) = τ 2v(ˆxi).
Via Fisher Scoring  the general update equation in matrix form is written as

z ← z +(cid:16)L⊤DL(cid:17)−1

L⊤DG(x − ˆx)

(6)

Although this formulation is somewhat abstract  it covers a very broad range of model classes that
are used in practice. For example  an important special case appears when the variance functions
are in the form of v(ˆx) = ˆxp. By setting p = {0  1  2  3} these correspond to Gaussian  Poisson 
Exponential/Gamma  and Inverse Gaussian distributions [10  pp.30]  which are special cases of the
exponential family of distributions for any p named Tweedie’s family [11]. Those for p = {0  1  2} 
in turn  correspond to EU  KL and IS cost functions often used for NMF decompositions [12  7].

2

2.1 Tensor Factorisations (TF) as GLM’s

The key observation for expressing a TF model as a GLM is to identify the multilinear structure
and using an alternating optimization approach. To hide the notational complexity  we will give an
example with a simple matrix factorisation model; extension to tensors will require heavier notation 
but are otherwise conceptually straightforward. Consider a MF model

g( ˆX) = Z1Z2

in scalar

g( ˆX)i j =Xr

Z i r
1 Z j r

2

(7)

where Z1  Z2 and g( ˆX) are matrices of compatible sizes. Indeed  by applying the vec operator
(vectorization  stacking columns of a matrix to obtain a vector) to both sides of (7) we obtain two
equivalent representation of the same system

vec(g( ˆX)) = (I|j| ⊗ Z1) vec(Z2) =

∂(Z1Z2)

∂Z2

vec(Z2) =

∂g( ˆX)
∂Z2

vec(Z2) ≡ ∇2 ~Z2

(8)

where I|j| denotes the |j| × |j| identity matrix  ⊗ denotes the Kronecker product [13]  and vec Z ≡
~Z. Clearly  this is a GLM where ∇2 plays the role of a model matrix and ~Z2 is the parameter
vector. By alternating between Z1 and Z2  we can maximise the log-likelihood iteratively; indeed
this alternating maximisation is standard for solving matrix factorisation problems. In the sequel  we
will show that a much broader range of algorithms can be readily derived in the GLM framework.

2.2 Generalised Tensor Factorisation

We deﬁne a tensor Λ as a multiway array with an index set V = {i1  i2  . . .   i|α|} where each index
in for n = 1 . . . |α| runs as in = 1 . . . |in|. An element of the tensor Λ is a scalar that we denote
by Λ(i1  i2  . . .   i|α|) or Λi1 i2 ... i|α| or as a shorthand notation by Λ(v) with v being a particular
conﬁguration. |v| denotes number of all distinct conﬁgurations for V  and e.g. if V = {i1  i2} then
|v| = |i1||i2|. We call the form Λ(v) as element-wise; the notation [ ] yields a tensor by enumerating
all the indices  i.e.  Λ = [Λi1 i2 ... i|α|] or Λ = [Λ(v)]. For any two tensors X and Y of compatible
order  X ◦ Y is an element-wise multiplication and if not explicitly stressed X/Y is an element-wise
division. 1 is an object of all ones whose order depends on the context where it is used.

A generalised tensor factorisation problem is speciﬁed by an observed tensor X (with possibly
missing entries  to be treated later) and a collection of latent tensors to be estimated  Z1:|α| = {Zα}
for α = 1 . . . |α|  and by an exponential family of form (2). The index set of X is denoted by V0 and
α=1 Vα. We use vα (or v0)
to denote a particular conﬁguration of the indices for Zα (or X) while ¯vα denoting a conﬁguration
of the compliment ¯Vα = V/Vα. The goal is to ﬁnd the latent Zα that maximize the likelihood
p(X|Z1:α) where hXi = ˆX is given via

the index set of each Zα by Vα. The set of all model indices is V = S|α|

g( ˆX(v0)) =X¯v0 Yα

Zα(vα)

(9)

To clarify our notation with an example  we express the CP (Parafac) model  deﬁned as ˆX(i  j  k) =

Pr Z1(i  r)Z2(j  r)Z3(k  r). In our notation  we take identity link g( ˆX) = ˆX and the index sets

with V = {i  j  k  r}  V0 = {i  j  k}  ¯V0 = {r}  V1 = {i  r}  V2 = {j  r} and V3 = {k  r}. Our
notation deliberately follows that of graphical models; the reader might ﬁnd it useful to associate
indices with discrete random variables and factors with probability tables [14]. Obviously  while a
TF model does not represent a discrete probability measure  the algebraic structure is nevertheless
analogous.

To extend the discussion in Section 2.1 to the tensor case  we need the equivalent of the model
matrix  when updating Zα. This is obtained by summing over the product of all remaining factors

g( ˆX(v0)) = X¯v0∩vα
Lα(oα) = X¯v0∩¯vα Yα′6=α

Zα(vα) X¯v0∩¯vα Yα′6=α

Zα′ (vα′ ) = X¯v0∩vα

Zα(vα)Lα(oα)

with oα ≡ (v0 ∪ vα) ∩ (¯v0 ∪ ¯vα)

Zα′ (vα′ )

3

One related quantity to Lα is the derivative of the tensor g( ˆX) wrt the latent tensor Zα denoted as
∇α and is deﬁned as (following the convention [13  pp 196])

∇α =

∂g( ˆX)
∂Zα

= I|v0∩vα| ⊗ Lα

with Lα ∈ R|v0∩¯vα|×|¯v0∩vα|

(10)

The importance of Lα is that  all the update rules can be formulated by a product and subsequent
contraction of Lα with another tensor Q having exactly the same index set of the observed tensor
X. As a notational abstraction  it is useful to formulate the following function 
Deﬁnition 1. The tensor valued function ∆α(Q) : R|v0| → R|vα| is deﬁned as

∆ε

α(Q) =h Xv0∩¯vα

Q(v0) Lα(oα)εi

(11)

with ∆α(Q) being an object of the same order as Zα and oα ≡ (v0 ∪ vα) ∩ (¯v0 ∪ ¯vα). Here  on
the right side  the nonnegative integer ε denotes the element-wise power  not to be confused with an
index. On the left  it should be interpreted as a parameter of the ∆ function. Arguably  ∆ function
abstracts away all the tedious reshape and unfolding operations [5]. This abstraction has also an
important practical facet: the computation of ∆ is algebraically (almost) equivalent to computation
of marginal quantities on a factor graph  for which efﬁcient message passing algorithms exist [14].

Example 1. TUCKER3 is deﬁned as ˆX i j k = Pp q r Ai pBj qC k rGp q r with V =

{i  j  k  p  q  r}  V0 = {i  j  k}  VA = {i  p}  VB = {j  q}  VC = {k  r}  VG = {p  q  r}. Then
for the ﬁrst factor A  the objects LA and ∆ε

A() are computed as follows

LA ="Xq r
A(Q) =
Xj k

Bj qC k rGp q r# =h((C ⊗ B)G⊤)p
k j
A(cid:1)p
A(cid:1)p
 =(cid:2)(cid:0)QLε
i (cid:0)Lε
i(cid:3)

Qk j

∆ε

k ji =h(cid:0)LA(cid:1)p
k ji

(12)

(13)

The index sets marginalised out for LA and ∆A are ¯V0 ∩ ¯VA = {p  q  r} ∩ {j  q  k  r} = {q  r} and
V0 ∩ ¯VA = {i  j  k} ∩ {j  q  k  r} = {j  k}. Also we verify the order of the gradient ∇A (10) as
I i
i ⊗ LA

i k j that conforms the matrix derivation convention [13  pp.196].

k j = ∇i p

p

2.3

Iterative Solution for GTF

As we have now established a one to one relationship between GLM and GTF objects such as the
observation x ≡ vec X  the mean (and the model estimate) ˆx ≡ vec ˆX  the model matrix L ≡ Lα
and the parameter vector z ≡ vec Zα  we can write directly from (6) as

~Zα ← ~Zα +(cid:16)∇⊤

α D∇α(cid:17)−1

α DG( ~X − ~ˆX)
∇⊤

with ∇α =

∂g( ˆX)
∂Zα

(14)

There are at least two ways that this update can further simpliﬁed. We may assume an identity
link function  or alternatively we may choose a matching link and lost functions such that they
cancel each other smoothly [2]. In the sequel we consider identity link g( ˆX) = ˆX that results to
g ˆX ( ˆX) = 1. This implies G to be identity  i.e. G = I. We deﬁne a tensor W   that plays the same
role as w in (5)  which becomes simply the precision (inverse variance function)  i.e. W = 1/v( ˆX)
where for the Gaussian  Poisson  Exponential and Inverse Gaussian distributions we have simply
W = ˆX −p with p = {0  1  2  3} [10  pp 30]. Then  the update (14) is reduced to

~Zα ← ~Zα +(cid:16)∇⊤

α D∇α(cid:17)−1

α D( ~X − ~ˆX)
∇⊤

(15)

After this simpliﬁcation we obtain two update rules for GTF for non-negative and real data.

The update (15) can be used to derive multiplicative update rules (MUR) popularised by [15] for the
nonnegative matrix factorisation (NMF). MUR equations ensure the non-negative parameter updates
as long as starting some non-negative initial values.

4

Theorem 1. The update equation (15) for nonnegative GTF is reduced to multiplicative form as

Zα ← Zα ◦

∆α(W ◦ X)
∆α(W ◦ ˆX)

s.t. Zα(vα) > 0

(16)

(Proof sketch) Due to space limitation we leave the full details of the proof  but idea is that inverse
of H = ∇⊤D∇ is identiﬁed as step size and by use of the results of the Perron-Frobenious theorem
[16  pp 125] we further bound it as

η =

~Zα
∇⊤D

~ˆX

<

2 ~Zα
~ˆX
∇⊤D

≤

2

λmax(∇⊤D∇)

since λmax(H) ≤ max

vα (cid:0)H ~Zα(cid:1)(vα)

Zα(vα)

(17)

For the special case of the Tweedie family where the precision is a function of the mean as W =
ˆX −p for p = {0  1  2  3} the update (15) is reduced to

Zα ← Zα ◦

∆α( ˆX −p ◦ X)

∆α( ˆX 1−p)

(18)

For example  to update Z2 for the NMF model ˆX = Z1Z2  ∆2 is ∆2(Q) = Z ⊤
Gaussian (p = 0) this reduces to NMF-EU as Z2 ← Z2 ◦ (Z ⊤

1 Q. Then for the
ˆX). For the Poisson (p = 1)

1 X)/(Z ⊤
1

it reduces to NMF-KL as Z2 ← Z2 ◦(cid:0)Z ⊤

1 (X/ ˆX)(cid:1)/(cid:0)Z ⊤

1 1(cid:1) [15].

By dropping the non-negativity requirement we obtain the following update equation:
Theorem 2. The update equation for GTF with real data can be expressed as

Zα ← Zα +

2

∆α(W ◦ (X − ˆX))

λα/0

∆2

α(W )

with λα/0 = |vα ∩ ¯v0|

(19)

(Proof sketch) Again skipping the full details  as part of the proof we set Zα = 1 in (17) speciﬁcally 
and replacing matrix multiplication of ∇⊤D∇1 by ∇⊤2
D1λα/0 completes the proof. Here the
multiplier λα/0 is the cardinality arising from the fact that only λα/0 elements are non-zero in a row
of ∇⊤D∇. Note the example for λα/0 that if Vα ∩ ¯V0 = {p  q} then λα/0 = |p||q| which is number
of all distinct conﬁgurations for the index set {p  q}.
Missing data can be handled easily by dropping the missing data terms from the likelihood [17]. The

net effect of this is the addition of an indicator variable mi to the gradient ∂L/∂z = τ −2Pi(xi −

i with mi = 1 if xi is observed otherwise mi = 0. Hence we simply deﬁne a mask
ˆxi)miwigˆx(ˆxi)l⊤
tensor M having the same order as the observation X  where the element M (v0) is 1 if X(v0) is
observed and zero otherwise. In the update equations  we merely replace W with W ◦ M.

3 Coupled Tensor Factorization

Here we address the problem when multiple observed tensors Xν for ν = 1 . . . |ν| are factorised
simultaneously. Each observed tensor Xν now has a corresponding index set V0 ν and a particular
conﬁguration will be denoted by v0 ν ≡ uν. Next  we deﬁne a |ν| × |α| coupling matrix R where

Rν α =(cid:26) 1

0

Xν and Zα connected
otherwise

ˆXν(uν) =X¯uν Yα

Zα(vα)Rν α

(20)

For the coupled factorisation  we get the following expression as the derivative of the log likelihood

∂L

∂Zα(vα)

=Xν

Rν α Xuν ∩¯vα(cid:16)Xν(uν) − ˆXν(uν)(cid:17)Wν(uν)

∂ ˆXν(uν)
∂Zα(vα)

(21)

where Wν ≡ W ( ˆXν(uν)) are the precisions. Then proceeding as in section 2.3 (i.e. getting the
Hessian and ﬁnding Fisher Information) we arrive at the update rule in vector form as

~Zα ← ~Zα +(cid:16)Xν

Rν α∇⊤

α νDν∇α ν(cid:17)−1(cid:16)Xν

Rν α∇⊤

α νDν(cid:0) ~Xν − ~ˆXν(cid:1)(cid:17)

(22)

5

. . .

Zα

. . .

Z|α|

Z1

A

B

C

D

E

. . .

Xν

. . .

X|ν|

X1

X1

X2

X3

Figure 1: (Left) Coupled factorisation structure where the arrow indicates the existence of the inﬂu-
ence of latent tensor Zα onto the observed tensor Xν. (Right) The CP/MF/MF coupled factorisation
problem in 1.

where ∇α ν = ∂g( ˆXν)/∂Zα. The update equations for the coupled case are quite intuitive; we
calculate the ∆α ν functions deﬁned as

∆ε

α ν(Q) =h Xuν ∩¯vα

for each submodel and add the results:
Lemma 1. Update for non-negative CTF

Q(uν)(cid:16) Yα′6=α

Zα′ (vα′ )Rν α(cid:17)εi

(23)

(24)

(25)

(27)

In the special case of a Tweedie family  i.e. for the distributions whose precision as Wν = ˆX −p

ν   the

update is Zα ← Zα ◦(cid:16)Pν Rν α∆α ν(cid:16) ˆX −p

Lemma 2. General update for CTF

Zα ← Zα ◦ Pν Rν α∆α ν(Wν ◦ Xν)
Pν Rν α∆α ν(cid:16)Wν ◦ ˆXν(cid:17)
ν (cid:17)(cid:17).
ν ◦ Xν(cid:17)(cid:17) /(cid:16)Pν Rν α∆α ν(cid:16) ˆX 1−p
λα/0Pν Rν α∆α ν(cid:16)Wν ◦(cid:0)Xν − ˆXν(cid:1)(cid:17)

α ν(Wν)

2

Pν Rν α∆2

Zα ← Zα +

For the special case of the Tweedie family we plug Wν = ˆX −p

ν

and get the related formula.

4 Experiments

Here we want to solve the CTF problem introduced (1)  which is a coupled CP/MF/MF problem

ˆX i j k

1 =Xr

Ai rBj rC k r

Bj rDp r

ˆX j p

2 =Xr

ˆX j q

3 =Xr

Bj rEq r

(26)

where we employ the symbols A : E for the latent tensors instead of Zα. This factorisation problem
has the following R matrix with |α| = 5  |ν| = 3

R =" 1

0
0

1 1
1 0
1 0

0 0
1 0

0 1 #

with

ˆX1 =P A1B1C 1D0E0
ˆX2 =P A0B1C 0D1E0
ˆX3 =P A0B1C 0D0E1

α ν() for ν = 1 (CP)
We want to use the general update equation (25). This requires derivation of ∆ε
and ν = 2 (MF) but not for ν = 3 since that ∆α 3() has the same shape as ∆α 2(). Here we show
the computation for B  i.e. for Z2  which is the common factor

∆ε

∆ε

B 1(Q) ="Xik
B 2(Q) ="Xp

Qi j k(cid:16)Ai rC k r(cid:17)ε# = Q(1)(C ε ⊙ Aε)
Qj p(cid:0)Dp r(cid:1)ε# = QDε

6

(28)

(29)

with Q(n) being mode-n unfolding operation that turns a tensor into matrix form [5]. In addition 
for ν = 1 the required scalar value λB/0 is |r| here since VB ∩ ¯V0 = {j  r} ∩ {r} = {r} noting that
value λB/0 is the same for ν = 2  3. The simulated data size for observables is |i| = |j| = |k| =
|p| = |q| = 30 while the latent dimension is |r| = 5. The number of iterations is 1000 with the
Euclidean cost while the experiment produced similar results for KL cost as shown in Figure 2.

A

5
D

5

10

5

0

0

10

5

 
0

10

10

B

5
E

5

5

0

0

10

5

0

0

5

0

0

10

 

10

C

5

Orginal
Initial
Final

10

Figure 2: The ﬁgure compares the original  the initial (start up) and the ﬁnal (estimate) factors for
Zα = A  B  C  D  E. Only the ﬁrst column  i.e. Zα(1 : 10  1) is plotted. Note that CP factorisation
is unique up to permutation and scaling [5] while MF factorisation is not unique  but when coupled
with CP it recovers the original data as shown in the ﬁgure. For visualisation  to ﬁnd the correct
permutation  for each of Zα the matching permutation between the original and estimate are found
by solving an orthogonal Procrustes problem [18  pp 601].

4.1 Audio Experiments

In this section  we illustrate a real data application of our approach  where we reconstruct missing
parts of an audio spectrogram X(f  t)  that represents the STFT coefﬁcient magnitude at frequency
bin f and time frame t of a piano piece  see top left panel of Fig.3. This is a difﬁcult matrix
completion problem: as entire time frames (columns of X) are missing  low rank reconstruction
techniques are likely to be ineffective. Yet such missing data patterns arise often in practice  e.g. 
when packets are dropped during digital communication. We will develop here a novel approach 
expressed as a coupled TF model. In particular  the reconstruction will be aided by an approximate
musical score  not necessarily belonging to the played piece  and spectra of isolated piano sounds.

Pioneering work of [19] has demonstrated that  when a audio spectrogram of music is decomposed

using NMF as X1(f  t) ≈ ˆX(f  t) = Pi D(f  i)E(i  t)  the computed factors D and E tend to be

semantically meaningful and correlate well with the intuitive notion of spectral templates (harmonic
proﬁles of musical notes) and a musical score (reminiscent of a piano roll representation such as a
MIDI ﬁle). However  as time frames are modeled conditionally independently  it is impossible to
reconstruct audio with this model when entire time frames are missing.

In order to restore the missing parts in the audio  we form a model that can incorporates musical
information of chords structures and how they evolve in time. In order to achieve this  we hierarchi-
cally decompose the excitation matrix E as a convolution of some basis matrices and their weights:

E(i  t) = Pk τ B(i  τ  k)C(k  t − τ ). Here the basis tensor B encapsulates both vertical and tem-

poral information of the notes that are likely to be used in a musical piece; the musical piece to
be reconstructed will share B  possibly played at different times or tempi as modelled by G. After
replacing E with the decomposed version  we get the following model (eq 30):

D(f  i)B(i  τ  k)C(k  d)Z(d  t  τ )

Test ﬁle

(30)

B(i  τ  k)G(k  m)Y (m  n  τ )

D(f  i)F (i  p)T (i  p)

MIDI ﬁle

(31)

Merged training ﬁles

(32)

ˆX1(f  t) = Xi τ k d
ˆX2(i  n) = Xτ k m
ˆX3(f  p) =Xi

7

Here we have introduced new dummy indices d and m  and new (ﬁxed) factors Z(d  t  τ ) = δ(d −
t + τ ) and Y (m  n  τ ) = δ(m − n + τ ) to express this model in our framework. In eq 32  while
forming X3 we concatenate isolated recordings corresponding to different notes. Besides  T is a
0 − 1 matrix  where T (i  p) = 1(0) if the note i is played (not played) during the time frame p and
F models the time varying amplitudes of the training data. R matrix for this model is deﬁned as

R =" 1 1

0 1
1 0

1 1
0 0
0 0

0 0 0
1 1 0
0 0 1

0
0

1 #

with

ˆX1 =P D1B1C 1Z 1G0Y 0F 0T 0
ˆX2 =P D0B1C 0Z 0G1Y 1F 0T 0
ˆX3 =P D1B0C 0Z 0G0Y 0F 1T 1

(33)

Figure 3 illustrates the performance the model  using KL cost (W = ˆX −1) on a 30 second piano
recording where the 70% of the data is missing; we get about 5dB SNR improvement  gracefully
degrading from 10% to 80% missing data: the results are encouraging as quite long portions of audio
are missing  see bottom right panel of Fig.3.

X1

X2 (Transcription Data)

X3 (Isolated Recordings)

)
z
H

(
 
y
c
n
e
u
q
e
r
F

)
z
H

(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

2000

1500

1000

500

0

80

60

s
e

t

o
N

40

20

)
z
H

(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

5

10

15

Time (sec)

20

X1hat (Restored)

25

5

10

15

Time (sec)

20

25

50

100
Time (sec)

Ground Truth

150

5

10

15

Time (sec)

20

25

)
z
H

(
 
y
c
n
e
u
q
e
r
F

2000

1500

1000

500

0

)

B
d
(
 

R
N
S

15

10

5

0

 

100

200

Time (sec)

Performance

300

 

Reconst. SNR
Initial SNR

20

60
Missing Data Percentage (%)

40

80

Figure 3: Top row  left to right: Observed matrices X1: spectrum of the piano performance  darker
colors imply higher magnitude (missing data (70%) are shown white)  X2  a piano roll obtained
from a musical score of the piece  X3  spectra of 88 isolated notes from a piano. Bottom Row:
Reconstructed X1  the ground truth  and the SNR results with increasing missing data. Here  initial
SNR is computed by substituting 0 as missing values.

5 Discussion

This paper establishes a link between GLMs and TFs and provides a general solution for the compu-
tation of arbitrary coupled TFs  using message passing primitives. The current treatment focused on
ML estimation; as immediate future work  the probabilistic interpretation is to be extended to a full
Bayesian inference with appropriate priors and inference methods. A powerful aspect  which we
have not been able to summarize here is assigning different cost functions  i.e. distributions  to dif-
ferent observation tensors in a coupled factorization model. This requires only minor modiﬁcations
to the update equations. We believe that  as a whole  the GCTF framework covers a broad range
of models that can be useful in many different application areas beyond audio processing  such as
network analysis  bioinformatics or collaborative ﬁltering.
Acknowledgements: This work is funded by the T ¨UB˙ITAK grant number 110E292  Bayesian
matrix and tensor factorisations (BAYTEN) and Bo˘gazic¸i University research fund BAP5723. Umut
S¸ims¸ekli is also supported by a Ph.D. scholarship from T ¨UB˙ITAK. We also would like to thank to
Evrim Acar for the fruitful discussions.

8

References

[1] A. T. Cemgil  Bayesian inference for nonnegative matrix factorisation models  Computational Intelligence

and Neuroscience 2009 (2009) 1–17.

[2] A. P. Singh  G. J. Gordon  A uniﬁed view of matrix factorization models  in: ECML PKDD’08  Part II 

no. 5212  Springer  2008  pp. 358–373.

[3] E. Acar  T. G. Kolda  D. M. Dunlavy  All-at-once optimization for coupled matrix and tensor factoriza-

tions  CoRR abs/1105.3422. arXiv:1105.3422.

[4] Q. Xu  E. W. Xiang  Q. Yang  Protein-protein interaction prediction via collective matrix factorization 

in: In Proc. of the IEEE International Conference on BIBM  2010  pp. 62–67.

[5] T. G. Kolda  B. W. Bader  Tensor decompositions and applications  SIAM Review 51 (3) (2009) 455–500.
[6] Y. K. Yılmaz  A. T. Cemgil  Probabilistic latent tensor factorization  in: Proceedings of the 9th interna-
tional conference on Latent variable analysis and signal separation  LVA/ICA’10  Springer-Verlag  2010 
pp. 346–353.

[7] C. Fevotte  A. T. Cemgil  Nonnegative matrix factorisations as probabilistic inference in composite mod-

els  in: Proc. 17th EUSIPCO  2009.

[8] Y. K. Yılmaz  A. T. Cemgil  Algorithms for probabilistic latent tensor factorization  Signal Process-

ing(2011) doi:10.1016/j.sigpro.2011.09.033.

[9] C. E. McCulloch  S. R. Searle  Generalized  Linear  and Mixed Models  Wiley  2001.
[10] C. E. McCulloch  J. A. Nelder  Generalized Linear Models  2nd Edition  Chapman and Hall  1989.
[11] R. Kaas  Compound poisson distributions and glm’s  tweedie’s distribution  Tech. rep.  Lecture  Royal

Flemish Academy of Belgium for Science and the Arts  (2005).

[12] A. Cichocki  R. Zdunek  A. H. Phan  S. Amari  Nonnegative Matrix and Tensor Factorization  Wiley 

2009.

[13] J. R. Magnus  H. Neudecker  Matrix Differential Calculus with Applications in Statistics and Economet-

rics  3rd Edition  Wiley  2007.

[14] M. Wainwright  M. I. Jordan  Graphical models  exponential families  and variational inference  Founda-

tions and Trends in Machine Learning 1 (2008) 1–305.

[15] D. D. Lee  H. S. Seung  Algorithms for non-negative matrix factorization  in: NIPS  Vol. 13  2001  pp.

556–562.

[16] M. Marcus  H. Minc  A Survey of Matrix Theory and Matrix Inequalities  Dover  1992.
[17] R. Salakhutdinov  A. Mnih  Probabilistic matrix factorization  in: Advances in Neural Information Pro-

cessing Systems  Vol. 20  2008.

[18] G. H. Golub  C. F. V. Loan  Matrix computations  3rd Edition  Johns Hopkins UP  1996.
[19] P. Smaragdis  J. C. Brown  Non-negative matrix factorization for polyphonic music transcription  in:

WASPAA  2003  pp. 177–180.

9

,Tao Yu
Christopher De Sa