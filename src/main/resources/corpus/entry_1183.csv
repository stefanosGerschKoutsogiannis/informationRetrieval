2012,Convex Multi-view Subspace Learning,Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.  However  in many applications  data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles  or a document might consist of text and images).  The conditional independence of separate sources imposes constraints on their shared latent representation  which  if respected  can improve the quality of the learned low dimensional representation.  In this paper  we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.  For this formulation  we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer  then recovers the corresponding latent representation and reconstruction model  jointly and optimally.  Experiments illustrate that the proposed method produces high quality results.,Convex Multi-view Subspace Learning

Department of Computing Science  University of Alberta  Edmonton AB T6G 2E8  Canada

Martha White  Yaoliang Yu  Xinhua Zhang∗and Dale Schuurmans
{whitem yaoliang xinhua2 dale}@cs.ualberta.ca

Abstract

Subspace learning seeks a low dimensional representation of data that enables
accurate reconstruction. However  in many applications  data is obtained from
multiple sources rather than a single source (e.g. an object might be viewed by
cameras at different angles  or a document might consist of text and images). The
conditional independence of separate sources imposes constraints on their shared
latent representation  which  if respected  can improve the quality of a learned
low dimensional representation. In this paper  we present a convex formulation
of multi-view subspace learning that enforces conditional independence while re-
ducing dimensionality. For this formulation  we develop an efﬁcient algorithm
that recovers an optimal data reconstruction by exploiting an implicit convex reg-
ularizer  then recovers the corresponding latent representation and reconstruction
model  jointly and optimally. Experiments illustrate that the proposed method
produces high quality results.

Introduction

1
Dimensionality reduction is one of the most important forms of unsupervised learning  with roots
dating to the origins of data analysis. Re-expressing high dimensional data in a low dimensional
representation has been used to discover important latent information about individual data items 
visualize entire data sets to uncover their global organization  and even improve subsequent clus-
tering or supervised learning [1]. Modern data is increasingly complex  however  with descriptions
of increasing size and heterogeneity. For example  multimedia data analysis considers data objects
(e.g. documents or webpages) described by related text  image  video  and audio components. Multi-
view learning focuses on the analysis of such multi-modal data by exploiting its implicit conditional
independence structure. For example  given multiple camera views of a single object  the partic-
ular idiosyncrasies of each camera are generally independent  hence the images they capture will
be conditionally independent given the scene. Similarly  the idiosyncrasies of text and images are
generally conditionally independent given a topic. The goal of multi-view learning  therefore  is to
use known conditional independence structure to improve the quality of learning results.
In this paper we focus on the problem of multi-view subspace learning: reducing dimensionality
when data consists of multiple  conditionally independent sources. Classically  multi-view subspace
learning has been achieved by an application of canonical correlation analysis (CCA) [2  3]. In
particular  many successes have been achieved in using CCA to recover meaningful latent represen-
tations in a multi-view setting [4–6]. Such work has been extended to probabilistic [7] and sparse
formulations [8]. However  a key limitation of CCA-based approaches is that they only admit efﬁ-
cient global solutions when using the squared-error loss (i.e. Gaussian models)  while extensions to
robust models have had to settle for approximate solutions [9].
By contrast  in the single-view setting  recent work has developed new generalizations of subspace
learning that can accommodate arbitrary convex losses [10–12]. These papers replace the hard bound
on the dimension of the latent representation with a structured convex regularizer that still reduces
rank  but in a relaxed manner that admits greater ﬂexibility while retaining tractable formulations.

∗Xinhua Zhang is now at the National ICT Australia (NICTA)  Machine Learning Group.

1

Subspace learning can be achieved in this case by ﬁrst recovering an optimal reduced rank response
matrix and then extracting the latent representation and reconstruction model. Such formulations
have recently been extended to the multi-view case [13  14]. Unfortunately  the multi-view formu-
lation of subspace learning does not have an obvious convex form  and current work has resorted
to local training methods based on alternating descent minimization (or approximating intractable
integrals). Consequently  there is no guarantee of recovering a globally optimal subspace.
In this paper we provide a formulation of multi-view subspace learning that can be solved optimally
and efﬁciently. We achieve this by adapting the new single-view training methods of [11  12] to the
multi-view case. After deriving a new formulation of multi-view subspace learning that allows a
global solution  we also derive efﬁcient new algorithms. The outcome is an efﬁcient approach to
multi-view subspace discovery that can produce high quality repeatable results.
Notation: We use Ik for the k×k identity matrix  A(cid:48) for the transpose of matrix A  (cid:107) · (cid:107)2 for the
i σi(X) for the trace

Euclidean norm  (cid:107)X(cid:107)F =(cid:112)tr(X(cid:48)X) for the Frobenius norm and (cid:107)X(cid:107)tr =(cid:80)

norm  where σi(X) is the ith singular value of X.

(cid:110)(cid:104)xj

(cid:105)(cid:111)

yj

(cid:105)

(cid:104)X

2 Background
Assume one is given t paired observations
consisting of two views: an x-view and a y-view 
of lengths m and n respectively. The goal of multi-view subspace learning is to infer  for each pair 
a shared latent representation  hj  of dimension k < min(n  m)  such that the original data can be
accurately modeled. We ﬁrst consider a linear formulation. Given paired observations the goal is to
infer a set of latent representations  hj  and reconstruction models  A and B  such that Ahj ≈ xj
and Bhj ≈ yj for all j. Let X denote the n × t matrix of x observations  Y the m × t matrix of
the concatenated (n + m) × t data matrix. The problem can then be
y observations  and Z =
expressed as recovering a (n + m) × k matrix of model parameters  C =
  and a k × t matrix
of latent representations  H  such that Z ≈ CH.
The key assumption of multi-view learning is that each of the two views  xj and yj  is condition-
ally independent given the shared latent representation  hj. Although multi-view data can always
be concatenated and treated as a single view  if the conditional independence assumption holds  ex-
plicitly representing multiple views enables more accurate identiﬁcation of the latent representation
(as we will see). The classical formulation of multi-view subspace learning is given by canonical
correlation analysis (CCA)  which is typically expressed as the problem of projecting two views so
that the correlation between them is maximized [2]. Assuming the data is centered (i.e. X1 = 0 and
Y 1 = 0)  the sample covariance of X and Y is given by XX(cid:48)/t and Y Y (cid:48)/t respectively. CCA can
then be expressed as an optimization over matrix variables

(cid:104)A

(cid:105)

B

Y

max
U V

tr(U(cid:48)XY (cid:48)V ) s.t. U(cid:48)XX(cid:48)U = V (cid:48)Y Y (cid:48)V = I

(1)
for U ∈ Rn×k  V ∈ Rm×k [3]. Although this classical formulation (1) does not make the shared
latent representation explicit  CCA can be expressed by a generative model: given a latent represen-
tation  hj  the observations xj = Ahj +j and yj = Bhj +νj are generated by a linear mapping plus
independent zero mean Gaussian noise  ∼ N (0  Σx)  ν ∼ N (0  Σy) [7]. In fact  one can show that
the classical CCA problem (1) is equivalent to the following multi-view subspace learning problem.

(cid:20) (XX(cid:48))−1/2X

(Y Y (cid:48))−1/2Y

(cid:21)

(C  H) = arg min
C H

Proposition 1. Fix k  let ˜Z =

(cid:104)A

(cid:105)

and

(cid:107) ˜Z − CH(cid:107)2
F  

(2)

2 B provide an optimal solution to (1) 

where C =
implying that A(cid:48)A = B(cid:48)B = I is satisﬁed in the solution to (2).

2 A and V = (Y Y (cid:48))− 1

. Then U = (XX(cid:48))− 1

B

(The proof is given in Appendix A.) From Proposition 1  one can see how formulation (2) respects
the conditional independence of the separate views: given a latent representation hj  the reconstruc-
tion losses on the two views  xj and yj  cannot inﬂuence each other  since the reconstruction models
A and B are individually constrained. By contrast  in single-view subspace learning (i.e. principal

2

components analysis) A and B are concatenated in the larger variable C  where C as a whole is con-
strained but A and B are not. A and B must then compete against each other to acquire magnitude
to explain their respective “views” given hj (i.e. conditional independence is not enforced). Such
sharing can be detrimental if the two views really are conditionally independent given hj.
Despite its elegance  a key limitation of CCA is its restriction to squared loss under a particular
normalization. Recently  subspace learning algorithms have been greatly generalized in the single
view case by relaxing the rank(H) = k constraint while imposing a structured regularizer that is
a convex relaxation of rank [10–12]. Such a relaxation allows one to incorporate arbitrary convex
losses  including robust losses [10]  while maintaining tractability.
As mentioned  these relaxations of single-view subspace learning have only recently been proposed
for the multi-view setting [13  14]. An extension of these proposals can be achieved by reformulating
(2) to ﬁrst incorporate an arbitrary loss function L that is convex in its ﬁrst argument (for examples 
see [15])  then relaxing the rank constraint by replacing it with a rank-reducing regularizer on H. In
particular  we consider the following training problem that extends [14]:

(cid:19)

(cid:21)
(cid:18)(cid:20) A
(cid:26)(cid:20) a

B

H; Z

(cid:21)

min
A B H

L

+ α(cid:107)H(cid:107)2 1 

s.t.

(cid:21)

(cid:20) A: i
(cid:27)

B: i

∈ C for all i 

(cid:20) A

(cid:21)

b

  C =

: (cid:107)a(cid:107)2 ≤ β (cid:107)b(cid:107)2 ≤ γ

and (cid:107)H(cid:107)2 1 =(cid:80)

where C :=
(3)
i (cid:107)Hi :(cid:107)2 is a matrix block norm. The signiﬁcance of using the (2  1)-block norm
as a regularizer is that it encourages rows of H to become sparse  hence reducing the dimensionality
of the learned representation [16]. C must be constrained however  otherwise (cid:107)H(cid:107)2 1 can be pushed
arbitrarily close to zero simply by re-scaling H/s and Cs (s > 0) while preserving the same loss.
Unfortunately  (3) is not jointly convex in A  B and H. Thus  the algorithmic approaches proposed
by [13  14] have been restricted to alternating block coordinate descent between components A  B
and H  which cannot guarantee a global solution. Our main result is to show that (3) can in fact be
solved globally and efﬁciently for A  B and H  improving on the previous local solutions [13  14].

B

 

3 Reformulation

(cid:110)

(cid:111)

.

(cid:107)H(cid:107)2 1

Our ﬁrst main contribution is to derive an equivalent but tractable reformulation of (3)  followed
by an efﬁcient optimization algorithm. Note that (3) can in principle be tackled by a boosting
strategy; however  one would have to formulate a difﬁcult weak learning oracle that considers both
views simultaneously [17]. Instead  we ﬁnd that a direct matrix factorization approach of the form
developed in [11  12] is more effective.
To derive our tractable reformulation  we ﬁrst introduce the change of variable ˆZ = CH which
allows us to rewrite (3) equivalently as

min

ˆZ

L( ˆZ; Z) + α min

{C:C: i∈C} min

{H:CH= ˆZ}

(4)

A key step in the derivation is the following characterization of the inner minimization in (4).
(cid:107)H(cid:107)2 1 deﬁnes a norm ||| · |||∗ (on ˆZ) whose dual norm is
Proposition 2. min
|||Γ||| := max

{C:C: i∈C} min

{H:CH= ˆZ}

c(cid:48)Γh.

c∈C (cid:107)h(cid:107)2≤1

Proof. Let λi = (cid:107)Hi :(cid:107)2 be the Euclidean norm of the i-th row of H. Then Hi : = λi ˜Hi : where
˜Hi : has unit length (if λi = 0  then take ˜Hi : to be any unit vector). Therefore

(cid:107)H(cid:107)2 1 =

{C λi:C: i∈C λi≥0  ˆZ=(cid:80)

min

{H:CH= ˆZ}

{C:C: i∈C} min
where K is the convex hull of the set G := {ch(cid:48) : c ∈ C (cid:107)h(cid:107)2 = 1}. In other words  we seek a
rank-one decomposition of ˆZ  using only elements from G. Since the set K is convex and symmetric 

i λi = min

i λiC: i ˜Hi :}

{t≥0: ˆZ∈tK}

min

t 

(5)

(cid:80)

3

(5) is known as a gauge function and deﬁnes a norm on ˆZ (see e.g. [18  Proposition V.3.2.1]). This
norm has a dual given by

|||Γ|||

(6)
where the last equality follows because maximizing any linear function over the convex hull K of a
set G achieves the same value as maximizing over the set G itself.
(cid:4)

c∈C (cid:107)h(cid:107)2≤1

:= max

max

Z∈K tr(Γ(cid:48)Z) =

c(cid:48)Γh 

Applying Proposition 2 to problem (3) leads to a simpler formulation of the optimization problem.

Lemma 3. (3) = min
.
ˆZ
Proof. The lemma is proved by ﬁrst deriving an explicit form of the norm ||| · ||| in (6)  then deriving
(cid:4)
its dual norm. The details are given in Appendix B.

L( ˆZ; Z)+α max
ρ≥0

0

ρ

(cid:107)D−1

ˆZ(cid:107)tr  where Dρ =

(cid:20)(cid:112)β2 +γ2ρ In

(cid:21)

(cid:112)γ2 +β2/ρ Im

0

(cid:20) β/

√
η In
0

Unfortunately the inner maximization problem in Lemma 3 is not concave in ρ. However  it is
possible to re-parameterize Dρ to achieve a tractable formulation as follows. First  deﬁne a matrix

=

Eη := D β2(1−η)
Note that maxρ≥0 (cid:107)D−1
following lemma proves that this re-parameterization yields an efﬁcient computational approach.
Lemma 4. h(η) := (cid:107)E−1

0
1 − η Im
ˆZ(cid:107)  with ρ ≥ 0 corresponding to 0 ≤ η ≤ 1. The

√
γ/
ˆZ(cid:107) = max0≤η≤1 (cid:107)E−1

ˆZ(cid:107)tr is concave in η over [0  1].

  such that Dρ = E β2

γ2 ρ+β2

γ2 η

ρ

η

.

η

(cid:21)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:113) η
(cid:113) 1−η

β2

γ2

ˆZ X

ˆZ Y

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)tr

(cid:16)(cid:113) η
β2 ( ˆZ X )(cid:48) ˆZ X + 1−η

= tr

γ2 ( ˆZ Y )(cid:48) ˆZ Y

  where tr(

√·)

(cid:17)

Proof. Expand h(η) into

means summing the square root of the eigenvalues (i.e. a spectral function). By [19]  if a spectral
function f is concave on [0 ∞)  then tr(f (M )) must be concave on positive semideﬁnite matrices.
γ2 ( ˆZ Y )(cid:48) ˆZ Y is positive semi-deﬁnite for η ∈ [0  1] and
The result follows since η
(cid:4)
f =

√· is concave on [0 ∞).

β2 ( ˆZ X )(cid:48) ˆZ X + 1−η

From Lemmas 3 and 4 we achieve the ﬁrst main result.
Theorem 5.

(3) = min

ˆZ

L( ˆZ; Z) + α max
0≤η≤1

(cid:107)E−1

η

ˆZ(cid:107)tr = max
0≤η≤1

L( ˆZ; Z) + α(cid:107)E−1

η

ˆZ(cid:107)tr. (7)

min

ˆZ

Hence (3) is equivalent to a concave-convex maxi-min problem with no local maxima nor minima.

Thus we have achieved a new formulation for multi-view subspace learning that respects conditional
independence of the separate views (see discussion in Section 2) while allowing a globally solvable
formulation. To the best of our knowledge  this has not previously been achieved in the literature.

4 Efﬁcient Training Procedure

This new formulation for multi-view subspace learning also allows for an efﬁcient algorithmic ap-
proach. Before conducting an experimental comparison to other methods  we ﬁrst develop an ef-
ﬁcient implementation. To do so we introduce a further transformation ˆQ = E−1
ˆZ in (7)  which
leads to an equivalent but computationally more convenient formulation of (3):

η

(3) = max
0≤η≤1

min

ˆQ

L(Eη ˆQ; Z) + α(cid:107) ˆQ(cid:107)tr.

(8)

Denote g(η) := min ˆQ L(Eη ˆQ; Z) + α(cid:107) ˆQ(cid:107)tr. The transformation does not affect the concavity of
the problem with respect to η established in Lemma 4; therefore  (8) remains tractable. The training
procedure then consists of two stages: ﬁrst  solve (8) to recover η and ˆQ  which allows ˆZ = Eη ˆQ to
be computed; then  recover the optimal factors H and C (i.e. A and B) from ˆZ.

4

Recovering an optimal ˆZ: The key to efﬁciently recovering ˆZ is to observe that (8) has a conve-
nient form. The concave outer maximization is deﬁned over a scalar variable η  hence simple line
search can be used to solve the problem  normally requiring at most a dozen evaluations to achieve
a small tolerance. Crucially  the inner minimization in ˆQ is a standard trace-norm-regularized loss
minimization problem  which has been extensively studied in the matrix completion literature [20–
22]. By exploiting these algorithms  g(η) and its subgradient can both be computed efﬁciently.

CH = ˆZ 

Recovering C and H from ˆZ: Once ˆZ is obtained  we need to recover a C and H that satisfy

(cid:107)H(cid:107)2 1 = ||| ˆZ|||∗ 

(5) that ||| ˆZ|||∗ = min{C λi:C:i∈C λi≥0  ˆZ=(cid:80)
loss of generality. In such a case  ˆZ =(cid:80)
have ||| ˆZ|||∗ = (cid:107)E−1

and C: i ∈ C for all i.
(9)
We exploit recent sparse approximation methods [23  24] to solve this problem. First  note from
i λi  where (cid:107) ˜Hi :(cid:107)2 ≤ 1. Since we already
ˆZ(cid:107)tr from the ﬁrst stage  we can rescale the problem so that ||| ˆZ|||∗ = 1 without
i λi = 1 (we restore the
proper scale to ˜H afterward). So now  ˆZ lies in the convex hull of the set G := {ch(cid:48) : c ∈ C (cid:107)h(cid:107)2 ≤
1} and any expansion of ˆZ as a convex combination of the elements in G is a valid recovery. From
this connection  we can now exploit the recent greedy algorithms developed in [23  24] to solve the
recovery problem. In particular  the recovery just needs to solve

i λiC: i ˜Hi :}(cid:80)
i λiC: i ˜Hi : where λ ≥ 0 and(cid:80)

η

K∈convG f (K)  where
min

(10)
where conv denotes the convex hull. Note that the optimal value of (10) is 0. The greedy (boosting)
algorithm provided by [23  24] produces a factorization of ˆZ into C and H and proceeds as follows:
t ∈ argminG∈G (cid:104)∇f (Kt−1)  G(cid:105) . Note that this step
1. Weak learning step: greedily pick Gt = cth(cid:48)
can be computed efﬁciently with a form of power method iteration (see Appendix C.2).
2. “Totally corrective” step: µ(t) = argmin
µ(t)
i Gi.

(cid:16) t(cid:80)

  then Kt =

t(cid:80)

(cid:17)

µiGi

f

f (K) := (cid:107) ˆZ − K(cid:107)2
F .

µ≥0 (cid:80)

i µi=1

i=1

i=1

F <  within O(1/) iterations [23  24].

This procedure will ﬁnd a Kt satisfying (cid:107) ˆZ − Kt(cid:107)2
Acceleration: In practice  this procedure can be considerably accelerated via more reﬁned analysis.
Recall ˆZ is penalized by the dual of the norm in (6). Given ˆZ  it is not hard to recover its dual
variable Γ by exploiting the dual norm relationship: Γ= argmaxΓ:|||Γ|||≤1tr(Γ(cid:48) ˆZ). Then given Γ  the
following theorem guarantees many bases in C can be eliminated from the recovery problem (9).
Theorem 6. (C  H) satisfying ˆZ = CH is optimal iff (cid:107)Γ(cid:48)C: i(cid:107) = 1 and Hi : = (cid:107)Hi :(cid:107)2C(cid:48)
: iΓ  ∀i.
Theorem 6 prunes many elements from G and the weak learning step only needs to consider a proper
subset. Interestingly this constrained search can be solved with no increase in the computational
complexity. The accelerated boosting generates ct in the weak learning step  giving the recovery
C = [c1  . . .   ck] and H = diag(µ)C(cid:48)Γ. The rank  k  is implicitly determined by termination of the
boosting algorithm. The detailed algorithm and proof of Theorem 6 are given in Appendix C.

5 Comparisons
Below we compare the proposed global learning method  Multi-view Subspace Learning (MSL) 
against a few benchmark competitors.
Local Multi-view Subspace Learning (LSL) An obvious competitor is to solve (3) by alternating
descent over the variables: optimize H with A and B ﬁxed  optimize A with B and H ﬁxed  etc.
This is the computational strategy employed by [13  14]. Since A and B are both constrained and H
is regularized by the (2 1)-block norm which is not smooth  we optimized them using the proximal
gradient method [25].
Single-view Subspace Learning (SSL) Single view learning can be cast as a relaxation of (3) 
where the columns of C =

are normalized as a whole  rather than individually for A and B:

(cid:104)A

(cid:105)

B

L(CH; Z) + α(cid:107)H(cid:107)2 1 = min

L( ˆC ˆH; Z) + α(β2 +γ2)− 1

2(cid:107) ˆH(cid:107)2 1 (11)

min
{H C:(cid:107)C: i(cid:107)2≤

√

β2+γ2}

{ ˆH  ˆC:(cid:107) ˆC: i(cid:107)2≤1}

L( ˆZ; Z) + α(β2 + γ2)− 1

2(cid:107) ˆZ(cid:107)tr.

(12)

= min

ˆZ

5

C =(cid:112)β2 + γ2 ˆC and ˆH =(cid:112)β2 + γ2H. Equation (12) can be established from the basic results

Equation (12) matches the formulation given in [10]. The equality in (11) is by change of variable
of [11  12] (or specializing Proposition 2 to the case where C is the unit Euclidean ball). To solve
(12)  we used a variant of the boosting algorithm [22] when α is large  due to its effectiveness
when the solution has low rank. When α is small  we switch to the alternating direction augmented
Lagrangian method (ADAL) [26] which does not enforce low-rank at all iterations. This hybrid
choice of solver is also applied to the optimization of ˆQ in (8) for MSL. Once an optimal ˆZ is
achieved  the corresponding C and H can be recovered by an SVD: for ˆZ = U ΣV (cid:48)  set C =
2 ΣV (cid:48) which satisﬁes CH = ˆZ and (cid:107)H(cid:107)2 1 =(cid:107) ˆZ(cid:107)tr  and so is an
(β2 +γ2) 1
optimal solution to (11).

2 U and H = (β2 +γ2)− 1

6 Experimental results

Datasets We provide experimental results on two datasets: a synthetic dataset and a face-image
dataset. The synthetic dataset is generated as follows. First  we randomly generate a k-by-ttr matrix
Htr for training  a k-by-tte matrix Hte for testing  and two basis matrices  A (n-by-k) and B (m-
by-k)  by (iid) sampling from a zero-mean unit-variance Gaussian distribution. The columns of A
and B are then normalized to ensure that the Euclidean norm of each is 1. Then we set

Xtr = AHtr  Ytr = BHtr  Xte = AHte  Yte = BHte.

Next  we add noise to these matrices  to obtain ˜Xtr  ˜Ytr  ˜Xte  ˜Yte. Following [10]  we use sparse
non-Gaussian noise: 5% of the matrix entries were selected randomly and replaced with a value
drawn uniformly from [−M  M ]  where M is 5 times the maximal absolute entry of the matrices.
The second dataset is based on the Extended Yale Face Database B [27]. It contains grey level
face images of 28 human subjects  each with 9 poses and 64 lighting conditions. To construct the
dataset  we set the x-view to a ﬁxed lighting (+000E+00) and the y-view to a different ﬁxed lighting
(+000E+20). We obtain a pair of views by randomly drawing a subject and a pose (under the two
ﬁxed lightings). The underlying assumption is that each lighting has its own set of bases (A and B)
and each (person  pose) pair has the same latent representation for the two lighting conditions. All
images are down-sampled to 100-by-100  meaning n = m = 104. We kept one view (x-view) clean
and added pixel errors to the second view (y-view). We randomly set 5% of the pixel values to 1 
mimicking the noise in practice  e.g. occlusions and loss of pixel information from image transfer.
The goal is to enable appropriate reconstruction of a noisy image using other views.
Model speciﬁcation Due to the sparse noise model  we used L1 1 loss for L:

L

H  Z

= (cid:107)AH − X(cid:107)1 1

+(cid:107)BH − Y (cid:107)1 1

.

(13)

(cid:16)(cid:20) A

(cid:21)

B

(cid:17)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

:=L1(AH X)

:=L2(BH Y )

For computational reasons  we worked on a smoothed version of the L1 1 loss [26].

6.1 Comparing optimization quality

We ﬁrst compare the optimization performance of MSL (global solver) versus LSL (local solver).
Figure 1(a) indicates that MSL consistently obtains a lower objective value  sometimes by a large
margin: more than two times lower for α = 10−4 and 10−3. Interestingly  as α increases  the
difference shrinks. This result suggests that more local minima occur in the higher rank case (a large
α increases regularization and decreases the rank of the solution). In Section 6.2  we will see that
the lower optimization quality of LSL and the fact that SSL optimizes a less constrained objective
both lead to signiﬁcantly worse denoising performance.
Second  we compare the runtimes of the three algorithms. Figure 1(b) presents runtimes for LSL
and MSL for an increasing number of samples. Again  the runtime of LSL is signiﬁcantly worse
for smaller α  as much as 4000x slower; as α increases  the runtimes become similar. This result
is likely due to the fact that for small α  the MSL inner optimization is much faster via the ADAL
solver (the slowest part of the optimization)  whereas LSL still has to slowly iterate over the three
variables. They both appear to scale similarly with respect to the number of samples.

6

(a) Objectives for LSL:MSL

(b) Runtimes for LSL:MSL

(c) Runtimes for SSL and MSL

Figure 1: Comparison between LSL and MSL on synthetic datasets with changing α  n = m = 20 and 10
repeats. (a) LSL often gets stuck in local minima  with a signiﬁcantly higher objective than MSL. (b) For small
α  LSL is signiﬁcantly slower than MSL. They scale similarly with the number of samples (c) Runtimes of SSL
and MSL for training and recovery with α = 10−3. For growing sample size  n = m = 20. MSL-R stands for
the recovery algorithm. The recovery time for SSL is almost 0  so it is not included.

(a) tL = 100

(b) tL = 300

(a) MSL vs LSL

(b) MSL vs SSL

Figure 2: Signal-to-noise ratio of denoising algorithms on
synthetic data using recovered models on hold-out views. n =
m = 10. In (a)  we used tL = 100 pairs of views for training
A and B and tested on 100 hold-out pairs  with 30 repeated
random draws of training and test data. In (b) we used tL =
300. Parameters were set to optimize respective methods.

Figure 3: MSL versus SSL error in synthe-
sizing y-view  over 30 random runs. We set
n=m=200  tL=20  and ttest=80. In (a)  LSL er-
ror is generally above the diagonal line  indicat-
ing higher error than MSL. In (b)  SSL error is
considerably higher than MSL.

For SSL versus MSL  we expect SSL to be faster than MSL because it is a more straightforward
optimization: in MSL  each inner optimization of (8) over ˆQ (with a ﬁxed η) has the same form
as the SSL objective. Figure 1(c)  however  illustrates that this difference is not substantial for
increasing sample size. Interestingly  the recovery runtime seems independent of dataset size  and
is instead likely proportional to the rank of the data. The trend is similar for increasing features: for
tL = 200  at n = 200  MSL requires ∼ 20 seconds  and at n = 1000  it requires ∼ 60 seconds.

6.2 Comparing denoising quality

Next we compare the denoising capabilities of the algorithms on synthetic and face image datasets.
There are two denoising approaches. The simplest is to run the algorithm on the noisy ˜Yte  giving
the reconstructed ˆYte as the denoised image. Another approach is to recover the models  A and B  in
a training phase. Given a new set of instances  ˜Xte = {˜xi}s
i=1  noise in ˜Xte and
˜Yte can be removed using A and B  without re-training. This approach requires ﬁrst recovering the
latent representation  ˆHte = (h1  . . .   hs)  for ˜Xte and ˜Yte. We use a batch approach for inference:
(14)

L1(AH  ˜Xte)+L2(BH  ˜Yte)+α(cid:107)H(cid:107)2 1.

i=1 and ˜Yte = {˜yi}s

ˆHte = argmin

The x-views and y-views are then reconstructed using ˆXte = A ˆHte and ˆYte = B ˆHte. We compared
these reconstructions with the clean data  Xte and Yte  in terms of the signal-to-noise ratio:

SNR( ˆXte  ˆYte) =

F + (cid:107)Yte(cid:107)2

F

F + (cid:107)Yte − ˆYte(cid:107)2

F

.

(15)

H

(cid:16)(cid:107)Xte(cid:107)2

(cid:17)(cid:46)(cid:16)(cid:107)Xte − ˆXte(cid:107)2

(cid:17)

We present the recovery approach on synthetic data and the direct reconstruction approach on the
face dataset. We cross-validated over α ∈ {10−4  10−3  10−2  10−1  0.5  1} according to the highest
signal-to-noise ratio on the training data. We set γ = β = 1 because the data is in the [0  1] interval.

7

0200400600800100012000.511.522.5Number of SamplesObjective Value Ratio LSL:MSLObjective LSL:MSL For Varying α=1e-4α=1e-3α=1e-2α=1e-1α=1α020040060080010001200−1000010002000300040005000Number of SamplesRuntime Ratio LSL:MSLTraining Runtime LSL:MSL For Varying α=1e-4αα=1e-3α=1e-2α=1e-1α=1020040060080010000246810Runtime on Synthetic DataNumber of Samples/FeaturesRuntime (seconds) SSLMSLMSL−R01020300246810Run numberSNR MSLSSLLSL0102030024681012Run numberSNR MSLSSLLSL100102104100102104Error of MSLError of LSL100102104100102104Error of MSLError of SSLFigure 4: Reconstruction of a noisy image with 5% or 10% noise. LSL performs only slightly worse than
MSL for larger noise values: a larger regularization parameter is needed for more noise  resulting in fewer local
minima (as discussed in Figure 1). Conversely  SSL performs slightly worse than MSL for 5% noise  but as the
noise increases  the advantages of the MSL objective are apparent.

6.2.1 Using Recovered Models for Denoising

Figure 2 presents the signal-to-noise ratio for recovery on synthetic data. MSL produced the highest
value of signal-to-noise ratio. The performance of LSL is inferior to MSL  but still better than SSL 
corroborating the importance of modelling the data as two views.

Image Denoising

6.2.2
In Figure 4  we can see that MSL outperforms both SSL and LSL on the face image dataset for
two noise levels: 5% and 10%. Interestingly  in addition to having on average a 10x higher SNR
than SSL for these results  MSL also had signiﬁcantly different objective values. SSL had larger
reconstruction error on the clean x-view (10x higher)  lower reconstruction error on the noisy y-
view (3x lower) and a higher representation norm (3x higher). Likely  the noisy y-view skewed the
representation  due to the joint rather than separate constraint as in the MSL objective.

the latent representation is computed from only one view:

6.3 Comparing synthesis of views
In image synthesis 
argminH L1(AH  ˜Xte) + α(cid:107)H(cid:107)2 1. The y-view is then synthesized: ˆYte = B ˆHte.
Figure 3 shows the synthesis error  || ˆYte−Yte||2
F   of MSL  LSL  and SSL over 30 random runs: MSL
generally incurs less error than LSL  and SSL incurs much higher error because it is not modelling
the conditional independence between views.

ˆHte =

7 Conclusion
We provided a convex reformulation of multi-view subspace learning that enables global learning  as
opposed to previous local formulations. We also developed a new training procedure which recon-
structs the data optimally and discovers the latent representations efﬁciently. Experimental results
on synthetic data and image data conﬁrm the effectiveness of our method  which consistently out-
performed other approaches in denoising quality. For future work  we are investigating extensions
to semi-supervised settings  such as global methods for co-training and co-regularization. It should
also be possible to extend our approach to more than two views and incorporate kernels.
Acknowledgements
We thank the reviewers for their helpful comments  in particular  an anonymous reviewer whose
suggestions greatly improved the presentation. Research supported by AICML and NSERC.

8

Clean2040608010020406080100Noisy : 5%2040608010020406080100SSL2040608010020406080100LSL2040608010020406080100MSL2040608010020406080100Noisy : 10%2040608010020406080100SSL2040608010020406080100LSL2040608010020406080100MSL2040608010020406080100References
[1] J. Lee and M. Verleysen. Nonlinear Dimensionality Reduction. Springer  2010.
[2] D. Hardoon  S. Szedmak  and J. Shawe-Taylor. Canonical correlation analysis: An overview

with application to learning methods. Neural Computation  16:2639–2664  2004.

[3] T. De Bie  N. Cristianini  and R. Rosipal. Eigenproblems in pattern recognition. In Handbook

of Geometric Computing  pages 129–170  2005.

[4] P. Dhillon  D. Foster  and L. Ungar. Multi-view learning of word embeddings via CCA. In

NIPS  2011.

[5] C. Lampert and O. Kr¨omer. Weakly-paired maximum covariance analysis for multimodal

dimensionality reduction and transfer learning. In ECCV  2010.

[6] L. Sigal  R. Memisevic  and D. Fleet. Shared kernel information embedding for discriminative

inference. In CVPR  2009.

[7] F. Bach and M. Jordan. A probabilistic interpretation of canonical correlation analysis. Tech-

nical Report 688  Department of Statistics  University of California  Berkeley  2006.

[8] C. Archambeau and F. Bach. Sparse probabilistic projections. In NIPS  2008.
[9] J. Viinikanoja  A. Klami  and S. Kaski. Variational Bayesian mixture of robust CCA. In ECML 

2010.

[10] E. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J.ACM  58(1):

1–37  2011.

[11] X. Zhang  Y. Yu  M. White  R. Huang  and D. Schuurmans. Convex sparse coding  subspace

learning  and semi-supervised extensions. In AAAI  2011.

[12] F. Bach  J. Mairal  and J. Ponce. Convex sparse matrix factorizations. arXiv:0812.1869v1 

2008.

[13] N. Quadrinto and C. Lampert. Learning multi-view neighborhood preserving projections. In

ICML  2011.

[14] Y. Jia  M. Salzmann  and T. Darrell. Factorized latent spaces with structured sparsity. In NIPS 

pages 982–990  2010.

[15] M. White and D. Schuurmans. Generalized optimal reverse prediction. In AISTATS  2012.
[16] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learn-

ing  73(3):243–272  2008.

[17] D. Bradley and A. Bagnell. Convex coding. In UAI  2009.
[18] J-B Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms  I and

II  volume 305 and 306. Springer-Verlag  1993.

[19] D. Petz. A survey of trace inequalities. In Functional Analysis and Operator Theory  pages

287–298. Banach Center  2004.

[20] S. Ma  D. Goldfarb  and L. Chen. Fixed point and Bregman iterative methods for matrix rank

minimization. Mathematical Programming  128:321–353  2011.

[21] J. Cai  E. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion.

SIAM Journal on Optimization  20(4):1956–1982  2010.

[22] X. Zhang  Y. Yu  and D. Schuurmans. Accelerated training for matrix-norm regularization: A

boosting approach. In NIPS  2012.

[23] A. Tewari  P. Ravikumar  and I. S. Dhillon. Greedy algorithms for structurally constrained high

dimensional problems. In NIPS  2011.

[24] X. Yuan and S. Yan. Forward basis selection for sparse approximation over dictionary.

AISTATS  2012.

In

[25] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[26] D. Goldfarb  S. Ma  and K. Scheinberg. Fast alternating linearization methods for minimizing

the sum of two convex functions. Mathematical Programming  to appear.

[27] A. Georghiades  P. Belhumeur  and D. Kriegman. From few to many: Illumination cone models

for face recognition under variable lighting and pose. IEEE TPAMI  23:643–660  2001.

9

Supplementary Material

A Proof of Proposition 1

To show that (1) and (2) have equivalent solutions we exploit some developments from [28]. Let
N = (XX(cid:48))− 1

2 and M = (Y Y (cid:48))− 1

2   hence

First consider (1). Its solution can be characterized by the maximal solutions to the generalized

eigenvalue problem [3]:(cid:20)

0
Y X(cid:48)

XY (cid:48)
0

= λ

which  under the change of variables u = N a and v = M b and then shifting the eigenvalues by 1  is
equivalent to
≡

XY (cid:48)M

= λ

0

(cid:20)

(cid:21)

˜Z ˜Z(cid:48) =

I

M Y X(cid:48)N

N XY (cid:48)M

I

v

(cid:20)
(cid:21)(cid:20) u
(cid:21)
(cid:21)(cid:20) a
(cid:21)(cid:20) a
˜Z ˜Z(cid:48)(cid:20) a

b

b

b

.

(cid:21)
(cid:21)(cid:20) u
(cid:21)
(cid:21)(cid:20) a
(cid:21)

v

 

b

0

0
Y Y (cid:48)

(cid:20) XX(cid:48)
(cid:20) N−1
(cid:21)(cid:20) a
(cid:20) I
(cid:21)
(cid:20) a

0
0 M−1
0
I

b

0

= λ

= (λ + 1)

b

(cid:21)
(cid:21)
(cid:21)

Y X(cid:48)N
0

M Y X(cid:48)N

N XY (cid:48)M

0

0

≡

≡

(cid:20)

(cid:104)A

(cid:105)

B

to the top k eigenvectors of ˜Z ˜Z(cid:48) one can show that U = N A and V = M B provides

By setting
an optimal solution to (1) [3].
By comparison  for (2)  an optimal H is given by H = C† ˜Z  where C† denotes pseudo-inverse.
Hence

min
C H

(cid:107) ˜Z − CH(cid:107)2
= tr( ˜Z ˜Z(cid:48)) − max

F = min
C

{C:C(cid:48)C=I} tr(C(cid:48) ˜Z ˜Z(cid:48)C).

(cid:107)(I − CC†) ˜Z(cid:107)2

F

Here again the solution is given by the top k eigenvectors of ˜Z ˜Z(cid:48) [29].1

B Proof for Lemma 3

First  observe that

(3) = min

{C:C: i∈C} min

H

L( ˆZ; Z) + α||| ˆZ|||∗ 

= min

ˆZ

L(CH; Z) + α(cid:107)H(cid:107)2 1 = min

L( ˆZ; Z) + α min

{C:C: i∈C} min

{H:CH= ˆZ}

(cid:107)H(cid:107)2 1

ˆZ

where the last step follows from Proposition 2.
It only remains to show ||| ˆZ|||∗ = maxρ≥0 (cid:107)D−1
the proof in [11] for the convenience of the reader.
We will use two diagonal matrices  I X = diag([1n; 0m]) and I Y = diag([0n; 1m]) such that I X +
I Y = Im+n. Similarly  for c ∈ Rm+n  we use cX (respectively cY ) to denote c1:m (respectively
cm+1:m+n).
The ﬁrst stage is to prove that the dual norm is characterized by

ˆZ(cid:107)tr  which was established in [11]. We reproduce

ρ

|||Γ||| = min
ρ≥0

(cid:107)DρΓ(cid:107)sp.

(16)

1 [30] gave a similar but not equivalent formulation to (2)  due to the lack of normalization.

10

where the spectral norm (cid:107)X(cid:107)sp = σmax(X) is the dual of the trace norm  (cid:107)X(cid:107)tr. To this end  recall
that

|||Γ||| = max

c∈C (cid:107)h(cid:107)2≤1

c(cid:48)Γh = max

c∈C (cid:107)c(cid:48)Γ(cid:107)2 =

max

{c:(cid:107)cX(cid:107)2=β  (cid:107)cY (cid:107)2=γ}

(cid:107)c(cid:48)Γ(cid:107)2

giving

|||Γ|||2 =

{c:(cid:107)cX(cid:107)2=β  (cid:107)cY (cid:107)2=γ} c(cid:48)ΓΓ(cid:48)c =

max

{Φ:Φ(cid:23)0  tr(ΦIX )≤β2  tr(ΦI Y )≤γ2} tr(ΦΓΓ(cid:48)) 

max

(17)

using the fact that when maximizing a convex function  one of the extreme points in the constraint
set {Φ : Φ(cid:23)0  tr(ΦIn)≤β2  tr(ΦIm)≤γ2} must be optimal. Furthermore  since the extreme points
have rank at most one in this case [31]  the rank constraint rank(Φ) = 1 can be dropped.
Next  form the Lagrangian L(Φ; λ  ν  Λ) = tr(ΦΓΓ(cid:48)) + tr(ΦΛ) + λ(β2 − tr(ΦI X )) + ν(γ2 −
tr(ΦI Y )) where λ ≥ 0  ν ≥ 0 and Λ (cid:23) 0. Note that the primal variable Φ can be eliminated
by formulating the equilibrium condition ∂L/∂Φ = ΓΓ(cid:48) + Λ − λI X − νI Y = 0  which implies
ΓΓ(cid:48) − λI X − νI Y (cid:22) 0. Therefore  we achieve the equivalent dual formulation

(17) =

(18)
Now observe that for λ ≥ 0 and ν ≥ 0  the relation ΓΓ(cid:48) (cid:22) λI X + νI Y holds if and only if
Dν/λΓΓ(cid:48)Dν/λ(cid:22) Dν/λ(λI X +νI Y )Dν/λ = (β2λ+γ2ν)In+m  hence

{λ ν:λ≥0  ν≥0  λIX +νI Y (cid:23)ΓΓ(cid:48)} β2λ + γ2ν.

min

(18) =

{λ ν:λ≥0  ν≥0  (cid:107)Dν/λΓ(cid:107)2

min

sp≤β2λ+γ2ν}β2λ+γ2ν

(19)

The third constraint must be met with equality at the optimum due to continuity  for otherwise we
would be able to further decrease the objective  a contradiction to optimality. Note that a standard
compactness argument would establish the existence of minimizers. So
(cid:107)DρΓ(cid:107)2
sp.

(cid:107)Dν/λΓ(cid:107)2

(19) = min

λ≥0 ν≥0

sp = min
ρ≥0

Finally  for the second stage  we characterize the target norm by observing that

||| ˆZ|||∗ = max
Γ:|||Γ|||≤1

tr(Γ(cid:48) ˆZ)

tr(Γ(cid:48) ˆZ)
tr(˜Γ(cid:48)D−1

ρ

ˆZ)

= max
ρ≥0

= max
ρ≥0

= max
ρ≥0

max

Γ:(cid:107)DρΓ(cid:107)sp≤1

max
˜Γ:(cid:107)˜Γ(cid:107)sp≤1
ˆZ(cid:107)tr.
(cid:107)D−1

ρ

(20)

(21)

where (20) uses (16)  and (21) exploits the conjugacy of the spectral and trace norms. The lemma
follows.

C Proof for Theorem 6 and Details of Recovery

Once an optimal reconstruction ˆZ is obtained  we need to recover the optimal factors C and H that
satisfy

CH = ˆZ   

(cid:107)H(cid:107)2 1 = ||| ˆZ|||∗ 

and C: i ∈ C for all i.

(22)

(23)

Note that by Proposition 2 and Lemma 3  the recovery problem (22) can be re-expressed as

min

{C H:C: i∈C ∀i  CH= ˆZ}

(cid:107)H(cid:107)2 1 =

{Γ:|||Γ|||≤1} tr(Γ(cid:48) ˆZ).

max

Our strategy will be to ﬁrst recover the optimal dual solution Γ given ˆZ  then use Γ to recover H
and C.
First  to recover Γ one can simply trace back from (21) to (20). Let U ΣV (cid:48) be the SVD of D−1
Then ˜Γ = U V (cid:48) and Γ = D−1
trace in (23) because tr(˜Γ(cid:48)D−1

ˆZ.
ρ U V (cid:48) automatically satisﬁes |||Γ||| = 1 while achieving the optimal
ˆZ) = tr(Σ) = (cid:107)D−1

ˆZ(cid:107)tr.

ρ

ρ

ρ

11

Given such an optimal Γ  we are then able to characterize an optimal solution (C  H). Introduce the
set

C(Γ) := arg max

(24)
Theorem 6. For a dual optimal Γ  (C  H) solves recovery problem (22) if and only if C: i ∈ C(Γ)
and Hi : = (cid:107)Hi :(cid:107)2C(cid:48)

: iΓ  such that CH = ˆZ.

c =

b

.

: (cid:107)a(cid:107) = β (cid:107)b(cid:107) = γ (cid:107)Γ(cid:48)c(cid:107) = 1

c∈C (cid:107)Γ(cid:48)c(cid:107) =

(cid:26)

(cid:20) a

(cid:21)

(cid:27)

(cid:88)

Proof. By (23)  if ˆZ = CH  then

||| ˆZ|||∗ = tr(Γ(cid:48) ˆZ) = tr(Γ(cid:48)CH) =

(25)
Note that ∀C: i ∈ C (cid:107)Γ(cid:48)C: i(cid:107)2 ≤ 1 since |||Γ||| ≤ 1 and Hi :Γ(cid:48)C: i = (cid:107)Hi :Γ(cid:48)C: i(cid:107)2 ≤
(cid:107)Hi :(cid:107)2(cid:107)Γ(cid:48)C: i(cid:107)2 ≤ (cid:107)Hi :(cid:107)2.
i (cid:107)Hi :(cid:107)2  hence implying
(cid:107)Γ(cid:48)C: i(cid:107)2 = 1 and Hi : = (cid:107)Hi :(cid:107)2C(cid:48)
On the other hand  if (cid:107)Γ(cid:48)C: i(cid:107)2 = 1 and Hi : = (cid:107)Hi :(cid:107)2C(cid:48)
implying the optimality of (C  H).

If (C  H) is optimal  then (25) = (cid:80)

: iΓ  then we have ||| ˆZ|||∗ =(cid:80)

i (cid:107)Hi :(cid:107)2 
(cid:4)

: iΓ.

Hi :Γ(cid:48)C: i.

i

Therefore  given Γ  the recovery problem (22) has been reduced to ﬁnding a vector µ and matrix C
such that µ ≥ 0  C: i ∈ C(Γ) for all i  and C diag(µ)C(cid:48)Γ = ˆZ.
Next we demonstrate how to incrementally recover µ and C. Denote the range of C diag(µ)C(cid:48) by
the set

S := {(cid:80)

i µicic(cid:48)

i : ci ∈ C(Γ)  µ ≥ 0} .

Note that S is the conic hull of (possibly inﬁnitely many) rank one matrices {cc(cid:48) : c ∈ C(Γ)}.
However  by Carath´eodory’s theorem [32  §17]  any matrix K ∈ S can be written as the conic com-
bination of ﬁnitely many rank one matrices of the form {cc(cid:48) : c ∈ C(Γ)}. Therefore  conceptually 
the recovery problem has been reduced to ﬁnding a sparse set of non-negative weights  µ  over the
set of feasible basis vectors  c ∈ C(Γ).
To ﬁnd these weights  we use a totally corrective “boosting” procedure [22] that is guaranteed to
converge to a feasible solution. Consider the following objective function for boosting

f (K) = (cid:107)KΓ − ˆZ(cid:107)2

F   where K ∈ S.

Note that f is clearly a convex function in K with a Lipschitz continuous gradient. Theorem 6
implies that an optimal solution of (22) corresponds precisely to those K ∈ S such that f (K) = 0.
The idea behind totally corrective boosting [22] is to ﬁnd a minimizer of f (hence optimal solution
of (22)) incrementally. After initializing K0 = 0  we iterate between two steps:
1. Weak learning step: ﬁnd

ct ∈ argmin
c∈C(Γ)

(cid:104)∇f (Kt−1)  cc(cid:48)(cid:105) = argmax
c∈C(Γ)

c(cid:48)Qc 

(26)

(27)

where Q = −∇f (Kt−1) = 2( ˆZ − Kt−1Γ)Γ(cid:48).

2. “Totally corrective” step:

(cid:17)

 

i=1 µicic(cid:48)

i

(cid:16)(cid:80)t

f
i cic(cid:48)
i.

µ(t) = argmin
µ:µi≥0

Kt = (cid:80)t

i=1 µ(t)

Three key facts can be established about this boosting procedure: (i) each weak learning step can
be solved efﬁciently; (ii) each totally corrective weight update can be solved efﬁciently; and (iii)
f (Kt) (cid:38) 0  hence a feasible solution can be arbitrarily well approximated. (iii) has been proved in
[22]  while (ii) is immediate because (27) is a standard quadratic program. Only (i) deserves some
explanation. We show in the next subsection that C(Γ)  deﬁned in (24)  can be much simpliﬁed  and
consequently we give in the last subsection an efﬁcient algorithm for the oracle problem (26) (the
idea is similar to the one inherent in the proof of Lemma 3).

12

C.1 Simpliﬁcation of C(Γ)

Since C(Γ) is the set of optimal solutions to

(28)
our idea is to ﬁrst obtain an optimal solution to its dual problem  and then use it to recover the
optimal c via the KKT conditions. In fact  its dual problem has been stated in (18). Once we obtain
the optimal ρ in (21) by solving (8)  it is straightforward to backtrack and recover the optimal λ and
ν in (18). Then by KKT condition [32  §28]  c is an optimal solution to (28) if and only if

max

(29)
(30)
Since (30) holds iff c is in the null space of R  we ﬁnd an orthonormal basis {n1  . . .   nk} for this
null space. Assume

(cid:104)R  cc(cid:48)(cid:105) = 0  where R = λI X + νI Y − ΓΓ(cid:48) (cid:23) 0.

c∈C (cid:107)Γ(cid:48)c(cid:107)  
(cid:13)(cid:13)cY(cid:13)(cid:13) = γ 

(cid:13)(cid:13)cX(cid:13)(cid:13) = β 

By (29)  we have

c = N α  where N = [n1  . . .   nk] =

0 = γ2(cid:13)(cid:13)cX(cid:13)(cid:13)2 − β2(cid:13)(cid:13)cY(cid:13)(cid:13)2

N Y

= α(cid:48)(cid:0)γ2(N X )(cid:48)N X − β2(N Y )(cid:48)N Y(cid:1) α.

  α = (α1  . . .   αk)(cid:48).

(31)

(32)

The idea is to go through some linear transformations for simpliﬁcation.
Perform eigen-
decomposition U ΣU(cid:48) = γ2(N X )(cid:48)N X − β2(N Y )(cid:48)N Y   where Σ = diag(σ1  . . .   σk)  and U ∈
Rk×k is orthonormal. Let v = U(cid:48)α. Then by (31) 

(cid:20) N X

(cid:21)

and (32) is satisﬁed if and only if

v(cid:48)Σv =

c = N U v 

(cid:88)

σiv2

i = 0.

Finally  (29) implies

i

β2 + γ2 = (cid:107)c(cid:107)2 = v(cid:48)U(cid:48)N(cid:48)N U v = v(cid:48)v.

In summary  by (33) we have

C(Γ) = {N U v : v satisﬁes (34) and (35)}

(cid:110)
N U v : v(cid:48)Σv = 0  (cid:107)v(cid:107)2 = β2 + γ2(cid:111)

=

(33)

(34)

(35)

(36)

.

C.2 Solving the weak oracle problem (26)

The weak oracle needs to solve

where Q = −∇f (Kt−1) = 2( ˆZ − Kt−1Γ)Γ(cid:48). By (36)  this optimization is equivalent to

where T = U(cid:48)N(cid:48)QN U. Using the same technique as in the proof of Lemma 3  we have

c(cid:48)Qc 

max
c∈C(Γ)

max

v:v(cid:48)Σv=0  (cid:107)v(cid:107)2=β2+γ2

v(cid:48)T v 

(let H = vv(cid:48)) =

(Lagrange dual) =

max

v:v(cid:48)v=1 v(cid:48)Σv=0

v(cid:48)T v

H(cid:23)0 tr(H)=1 tr(ΣH)=0

max

tr(T H)

ω

min

τ ω:τ Σ+ωI−T(cid:23)0
τ∈R λmax(T − τ Σ) 

= min

where λmax stands for the maximum eigenvalue. Since λmax is a convex function over real symmet-
ric matrices  the last line search problem is convex in τ  hence can be solved globally and efﬁciently.
Given the optimal τ and the optimal objective value ω  the optimal v can be recovered using a similar
trick as in Appendix C.1. Let the null space of ωI + τ Σ − T be spanned by ˆN = {ˆn1  . . .   ˆns}.
Then ﬁnd any ˆα ∈ Rs such that v := ˆN ˆα satisﬁes (cid:107)v(cid:107)2 = β2 + γ2 and v(cid:48)Σv = 0.

13

Auxiliary References
[28] L. Sun  S. Ji  and J. Ye. Canonical correlation analysis for multilabel classiﬁcation: A least-

squares formulation  extensions  and analysis. IEEE TPAMI  33(1):194–200  2011.

[29] M. Overton and R. Womersley. Optimality conditions and duality theory for minimizing sums
of the largest eigenvalues of symmetric matrices. Mathematical Programming  62:321–357 
1993.

[30] B. Long  P. Yu  and Z. Zhang. A general model for multiple view unsupervised learning. In

ICDM  2008.

[31] G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of

optimal eigenvalues. Mathematics of Operations Research  23(2):339–358  1998.

[32] R. Rockafellar. Convex Analysis. Princeton U. Press  1970.

14

,Ian Osband
Benjamin Van Roy
Jian Li
Yong Liu
Rong Yin
Hua Zhang
Lizhong Ding
Weiping Wang