2013,Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models,Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence  it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the `uncertainty' associated with a certain parameter estimate. Concretely  no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values.  We consider here a broad class of regression problems  and propose  an efficient algorithm  for constructing confidence intervals and p-values.  The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing  our method has nearly optimal power.  Our approach is based on constructing a `de-biased' version of regularized M-estimators. The new construction  improves over recent work in the field in that it does not assume a special structure on the design matrix. Furthermore  proofs are remarkably simple. We test our method on a diabetes prediction problem.,Conﬁdence Intervals and Hypothesis Testing for

High-Dimensional Statistical Models

Adel Javanmard
Stanford University
Stanford  CA 94305

adelj@stanford.edu

Andrea Montanari
Stanford University
Stanford  CA 94305

montanar@stanford.edu

Abstract

Fitting high-dimensional statistical models often requires the use of non-linear
parameter estimation procedures. As a consequence  it is generally impossible to
obtain an exact characterization of the probability distribution of the parameter
estimates. This in turn implies that it is extremely challenging to quantify the un-
certainty associated with a certain parameter estimate. Concretely  no commonly
accepted procedure exists for computing classical measures of uncertainty and
statistical signiﬁcance as conﬁdence intervals or p-values.
We consider here a broad class of regression problems  and propose an efﬁcient
algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁ-
dence intervals have nearly optimal size. When testing for the null hypothesis that
a certain parameter is vanishing  our method has nearly optimal power.
Our approach is based on constructing a ‘de-biased’ version of regularized M-
estimators. The new construction improves over recent work in the ﬁeld in that it
does not assume a special structure on the design matrix. Furthermore  proofs are
remarkably simple. We test our method on a diabetes prediction problem.

1

Introduction

It is widely recognized that modern statistical problems are increasingly high-dimensional  i.e. re-
quire estimation of more parameters than the number of observations/examples. Examples abound
from signal processing [16]  to genomics [21]  collaborative ﬁltering [12] and so on. A number
of successful estimation techniques have been developed over the last ten years to tackle these
problems. A widely applicable approach consists in optimizing a suitably regularized likelihood
function. Such estimators are  by necessity  non-linear and non-explicit (they are solution of certain
optimization problems).
The use of non-linear parameter estimators comes at a price. In general  it is impossible to char-
acterize the distribution of the estimator. This situation is very different from the one of classical
statistics in which either exact characterizations are available  or asymptotically exact ones can be
derived from large sample theory [26]. This has an important and very concrete consequence. In
classical statistics  generic and well accepted procedures are available for characterizing the uncer-
tainty associated to a certain parameter estimate in terms of conﬁdence intervals or p-values [28  14].
However  no analogous procedures exist in high-dimensional statistics.
In this paper we develop a computationally efﬁcient procedure for constructing conﬁdence intervals
and p-values for a broad class of high-dimensional regression problems. The salient features of
our procedure are: (i) Our approach guarantees nearly optimal conﬁdence interval sizes and testing
power. (ii) It is the ﬁrst one that achieves this goal under essentially no assumptions on the pop-
ulation covariance matrix of the parameters  beyond the standard conditions for high-dimensional
consistency. (iii) It allows for a streamlined analysis with respect to earlier work in the same area.

1

Table 1: Unbiased estimator for θ0 in high dimensional linear regression models
Input: Measurement vector y  design matrix X  parameter γ.

Output: Unbiased estimator(cid:98)θu.
1: Set λ = σγ  and let(cid:98)θn be the Lasso estimator as per Eq. (3).
2: Set(cid:98)Σ ≡ (XTX)/n.

3: for i = 1  2  . . .   p do
4:

Let mi be a solution of the convex program:

5: Set M = (m1  . . .   mp)T. If any of the above problems is not feasible  then set M = Ip×p.

6: Deﬁne the estimator(cid:98)θu as follows:

minimize mT(cid:98)Σm
subject to (cid:107)(cid:98)Σm − ei(cid:107)∞ ≤ γ
M XT(Y − X(cid:98)θn(λ))

(cid:98)θu =(cid:98)θn(λ) +

1
n

(4)

(5)

(2)

(iv) Our method has a natural generalization non-linear regression models (e.g.
logistic regres-
sion  see Section 4). We provide heuristic and numerical evidence supporting this generalization 
deferring a rigorous study to future work.
For the sake of clarity  we will focus our presentation on the case of linear regression  defer-
ring the generalization to Section 4.
In the random design model  we are given n i.i.d. pairs
(Y1  X1)  (Y2  X2)  . . .   (Yn  Xn)  with vectors Xi ∈ Rp and response variables Yi given by

Yi = (cid:104)θ0  Xi(cid:105) + Wi  

Wi ∼ N(0  σ2) .

(1)
Here (cid:104)·   ·(cid:105) is the standard scalar product in Rp. In matrix form  letting Y = (Y1  . . .   Yn)T and
denoting by X the design matrix with rows X T

1   . . .   X T

n   we have

Y = X θ0 + W  

W ∼ N(0  σ2In×n) .

The goal is estimate the unknown (but ﬁxed) vector of parameters θ0 ∈ Rp.
In the classic setting  n (cid:29) p and the estimation method of choice is ordinary least squares yielding

(cid:98)θOLS = (XTX)−1XTY . In particular (cid:98)θ is Gaussian with mean θ0 and covariance σ2(XTX)−1.

This directly allows to construct conﬁdence intervals1.
In the high-dimensional setting where p > n  the matrix (XTX) is rank deﬁcient and one has to
resort to biased estimators. A particularly successful approach is the Lasso [24  7] which promotes
sparse reconstructions through an (cid:96)1 penalty.

(cid:107)Y − Xθ(cid:107)2

2 + λ(cid:107)θ(cid:107)1

.

(3)

(cid:98)θn(Y  X; λ) ≡ arg min

(cid:110) 1

θ∈Rp

2n

(cid:111)

is not tractable in general  and hence there is no simple procedure to construct conﬁdence intervals
and p-values. In order to overcome this challenge  we construct a de-biased estimator from the Lasso

In case the right hand side has more than one minimizer  one of them can be selected arbitrarily for
our purposes. We will often omit the arguments Y   X  as they are clear from the context. We denote
by S ≡ supp(θ0) ⊆ [p] the support of θ0  and let s0 ≡ |S|. A copious theoretical literature [6  2  4]
shows that  under suitable assumptions on X  the Lasso is nearly as accurate as if the support S was
2 = O(s0σ2(log p)/n). These

known a priori. Namely  for n = Ω(s0 log p)  we have (cid:107)(cid:98)θn − θ0(cid:107)2
remarkable properties come at a price. Deriving an exact characterization for the distribution of(cid:98)θn
solution. The de-biased estimator is given by the simple formula(cid:98)θu =(cid:98)θn +(1/n) M XT(Y −X(cid:98)θn) 
as in Eq. (5). The basic intuition is that XT(Y − X(cid:98)θn)/(nλ) is a subgradient of the (cid:96)1 norm at the
Lasso solution(cid:98)θn. By adding a term proportional to this subgradient  our procedure compensates
i + 1.96σ(cid:112)Qii/n] is a 95% conﬁ-
1For instance  letting Q ≡ (XTX/n)−1 (cid:98)θOLS

i − 1.96σ(cid:112)Qii/n (cid:98)θOLS

the bias introduced by the (cid:96)1 penalty in the Lasso.

dence interval [28].

2

allows to construct conﬁdence intervals and p-values in complete analogy with classical statistics

i − 1.96σ(cid:112)Qii/n (cid:98)θu

A key role is played by the matrix M ∈ Rp×p whose function is to ‘decorrelate’ the columns of X.
We propose here to construct M by solving a convex program that aims at optimizing two objectives.

√
conﬁdence interval. The size of this interval is of order σ/
n  which is the optimal (minimum) one 
i.e. the same that would have been obtained by knowing a priori the support of θ0. In practice the

We will prove in Section 2 that (cid:98)θu is approximately Gaussian  with mean θ0 and covariance
σ2(M(cid:98)ΣM )/n  where(cid:98)Σ = (XTX/n) is the empirical covariance of the feature vectors. This result
i + 1.96σ(cid:112)Qii/n] is a 95%
procedures. For instance  letting Q ≡ M(cid:98)ΣM  [(cid:98)θu
noise standard deviation is not known  but σ can be replaced by any consistent estimator(cid:98)σ.
One one hand  we try to control |M(cid:98)Σ− I|∞ (here and below | · |∞ denotes the entrywise (cid:96)∞ norm)
which –as shown in Theorem 2.1– controls the non-Gaussianity and bias of(cid:98)θu. On the other  we
minimize [M(cid:98)ΣM ]i i  for each i ∈ [p]  which controls the variance of(cid:98)θu
The idea of constructing a de-biased estimator of the form (cid:98)θu = (cid:98)θn + (1/n) M XT(Y − X(cid:98)θn)
was used by Javanmard and Montanari in [10]  that suggested the choice M = cΣ−1  with Σ =
E{X1X T
1 } the population covariance matrix and c a positive constant. A simple estimator for Σ
was proposed for sparse covariances  but asymptotic validity and optimality were proven only for
uncorrelated Gaussian designs (i.e. Gaussian X with Σ = I). Van de Geer  B¨ulhmann and Ritov
[25] used the same construction with M an estimate of Σ−1 which is appropriate for sparse inverse
covariances. These authors prove semi-parametric optimality in a non-asymptotic setting  provided
0 log p). In this paper  we do not assume any sparsity constraint on
the sample size is at least n = Ω(s2
Σ−1  but still require the sample size scaling n = Ω(s2
0 log p). We refer to a forthcoming publication
wherein the condition on the sample size scaling is relaxed [11].
From a technical point of view  our proof starts from a simple decomposition of the de-biased esti-
from earlier work– we realize that M need not be a good estimator of Σ−1 in order for the de-biasing
procedure to work. We instead set M as to minimize the error term and the variance of the Gaussian
term. As a consequence of this choice  our approach applies to general covariance structures Σ. By
contrast  earlier approaches applied only to sparse Σ  as in [10]  or sparse Σ−1 as in [25]. The only
assumptions we make on Σ are the standard compatibility conditions required for high-dimensional
consistency [4]. We refer the reader to the long version of the paper [9] for the proofs of our main
results and the technical steps.

mator(cid:98)θu into a Gaussian part and an error term  already used in [25]. However –departing radically

i .

1.1 Further related work

The theoretical literature on high-dimensional statistical models is vast and rapidly growing. Re-
stricting ourselves to linear regression  earlier work investigated prediction error [8]  model selec-
tion properties [17  31  27  5]  (cid:96)2 consistency [6  2]. Of necessity  we do not provide a complete set
of references  and instead refer the reader to [4] for an in-depth introduction to this area.
The problem of quantifying statistical signiﬁcance in high-dimensional parameter estimation is  by
comparison  far less understood. Zhang and Zhang [30]  and B¨uhlmann [3] proposed hypothesis
testing procedures under restricted eigenvalue or compatibility conditions [4]. These methods are
however effective only for detecting very large coefﬁcients. Namely  they both require |θ0 i| ≥
c max{σs0 log p/ n  σ/
s0 larger than the ideal detection level [10]. In other words 
in order for the coefﬁcient θ0 i to be detectable with appreciable probability  it needs to be larger than
the overall (cid:96)2 error  rather than the (cid:96)2 error per coordinate.
Lockart et al. [15] develop a test for the hypothesis that a newly added coefﬁcient along the Lasso
regularization path is irrelevant. This however does not allow to test arbitrary coefﬁcients at a given
value of λ  which is instead the problem addressed in this paper. It further assumes that the current
Lasso support contains the actual support supp(θ0) and that the latter has bounded size. Finally 
resampling methods for hypothesis testing were studied in [29  18  19].

n}  which is

√

√

1.2 Preliminaries and notations

We let(cid:98)Σ ≡ XTX/n be the sample covariance matrix. For p > n (cid:98)Σ is always singular. However 
we may require(cid:98)Σ to be nonsingular for a restricted set of directions.

3

Deﬁnition 1.1. For a matrix(cid:98)Σ and a set S of size s0  the compatibility condition is met  if for some

φ0 > 0  and all θ satisfying (cid:107)θSc(cid:107)1 ≤ 3(cid:107)θS(cid:107)1  it holds that

θT(cid:98)Σθ .

(cid:107)θS(cid:107)2

1 ≤ s0
φ2
0

Deﬁnition 1.2. The sub-gaussian norm of a random variable X  denoted by (cid:107)X(cid:107)ψ2  is deﬁned as

(cid:107)X(cid:107)ψ2 = sup
p≥1

p−1/2(E|X|p)1/p .

The sub-gaussian norm of a random vector X ∈ Rn is deﬁned as (cid:107)X(cid:107)ψ2 = supx∈Sn−1 (cid:107)(cid:104)X  x(cid:105)(cid:107)ψ2.
Further  for a random variable X  its sub-exponential norm  denoted by (cid:107)X(cid:107)ψ1  is deﬁned as

(cid:107)X(cid:107)ψ1 = sup
p≥1

p−1(E|X|p)1/p .

For a matrix A and set of indices I  J  we let AI J denote the submatrix formed by the rows in
I and columns in J. Also  AI · (resp. A· I) denotes the submatrix containing just the rows (reps.
columns) in I. Likewise  for a vector v  vI is the restriction of v to indices in I. We use the shorthand
A−1
I J = (A−1)I J. In particular  A−1
i i = (A−1)i i. The maximum and the minimum singular values
of A are respectively denoted by σmax(A) and σmin(A). We write (cid:107)v(cid:107)p for the standard (cid:96)p norm of
a vector v and (cid:107)v(cid:107)0 for the number of nonzero entries of v. For a matrix A  (cid:107)A(cid:107)p is the (cid:96)p operator
i j |Aij|p)1/p. For an integer p ≥ 1 
we let [p] ≡ {1  . . .   p}. For a vector v  supp(v) represents the positions of nonzero entries of v.
Throughout  with high probability (w.h.p) means with probability converging to one as n → ∞  and

norm  and |A|p is the elementwise (cid:96)p norm  i.e.  |A|p = ((cid:80)
Φ(x) ≡(cid:82) x
Theorem 2.1. Consider the linear model (1) and let(cid:98)θu be deﬁned as per Eq. (5). Then 

2 An de-biased estimator for θ0

2π denotes the CDF of the standard normal distribution.

−∞ e−t2/2dt/

n((cid:98)θu − θ0) = Z + ∆   Z|X ∼ N(0  σ2M(cid:98)ΣM T)   ∆ =

√

√

n(M(cid:98)Σ − I)(θ0 −(cid:98)θ) .

√

Further  suppose that σmin(Σ) = Ω(1)  and σmax(Σ) = O(1). In addition assume the rows of the
whitened matrix XΣ−1/2 are sub-gaussian  i.e.  (cid:107)Σ−1/2X1(cid:107)ψ2 = O(1). Let E be the event that the
√
(see inputs in Table 1)  the following holds true. On the event E  w.h.p  (cid:107)∆(cid:107)∞ = O(s0 log p/
n).
Note that compatibility condition (and hence the event E) holds w.h.p. for random design matrices
of a general nature.
In fact [22] shows that under some general assumptions  the compatibility

compatibility condition holds for(cid:98)Σ  and maxi∈[p] (cid:98)Σi i = O(1). Then  using γ = O((cid:112)(log p)/n)
condition on Σ implies a similar condition on (cid:98)Σ  w.h.p.  when n is sufﬁciently large. Bounds on
the variances [M(cid:98)ΣM T]ii will be given in Section 3.2. Finally  the claim of Theorem 2.1 does not

rely on the speciﬁc choice of the objective function in optimization problem (4) and only uses the
optimization constraints.
√
Remark 2.2. Theorem 2.1 does not make any assumption about the parameter vector θ0. If we
n/ log p)  then we have (cid:107)∆(cid:107)∞ = o(1) 
further assume that the support size s0 satisﬁes s0 = o(

w.h.p. Hence (cid:98)θu is an asymptotically unbiased estimator for θ0.

3 Statistical inference

A direct application of Theorem 2.1 is to derive conﬁdence intervals and statistical hypothesis tests
for high dimensional models. Throughout  we make the sparsity assumption s0 = o(

n/ log p).

√

3.1 Conﬁdence intervals
We ﬁrst show that the variances of variables Zj|X are Ω(1).

4

Lemma 3.1. Let M = (m1  . . .   mp)T be the matrix with rows mT

program (4). Then for all i ∈ [p]  [M(cid:98)ΣM T]i i ≥ (1 − γ)2/(cid:98)Σi i .

i obtained by solving convex

By Remark 2.2 and Lemma 3.1  we have
≤ x

P(cid:110)√
n((cid:98)θu
σ[M(cid:98)ΣM T]1/2
i − θ0 i)

i i

(cid:111)

(cid:12)(cid:12)(cid:12)X

= Φ(x) + o(1)  

∀x ∈ R .

(6)

(cid:111)

{(cid:98)θn(λ) (cid:98)σ} ≡ arg min

(cid:110) 1

Since the limiting probability is independent of X  Eq. (6) also holds unconditionally for random
design X.
For constructing conﬁdence intervals  a consistent estimate of σ is needed. To this end  we use the
scaled Lasso [23] given by

(cid:107)Y − Xθ(cid:107)2

2 +

+ λ(cid:107)θ(cid:107)1

.

σ
2

2σn

θ∈Rp σ>0

This is a joint convex minimization which provides an estimate of the noise level in addition to an
estimate of θ0. We use λ = c1
of Theorem 2.1 (cf. [23]). We hence obtain the following.
Corollary 3.2. Let

(cid:112)(log p)/n that yields a consistent estimate(cid:98)σ  under the assumptions
δ(α  n) = Φ−1(1 − α/2)(cid:98)σ n−1/2

Then Ii = [(cid:98)θu
Notice that the same corollary applies to any other consistent estimator (cid:98)σ of the noise standard

(7)
i + δ(α  n)] is an asymptotic two-sided conﬁdence interval for θ0 i with

i − δ(α  n) (cid:98)θu

[M(cid:98)ΣM T]i i .

signiﬁcance α.

(cid:113)

deviation.

3.2 Hypothesis testing

An important advantage of sparse linear regression models is that they provide parsimonious expla-
nations of the data in terms of a small number of covariates. The easiest way to select the ‘active’
(cid:54)= 0. This approach however does not provide a

covariates is to choose the indexes i for which(cid:98)θn

measure of statistical signiﬁcance for the ﬁnding that the coefﬁcient is non-zero.
More precisely  we are interested in testing an individual null hypothesis H0 i : θ0 i = 0 versus the
alternative HA i : θ0 i (cid:54)= 0  and assigning p-values for these tests. We construct a p-value Pi for the
test H0 i as follows:

i

Pi = 2

1 − Φ

(cid:18) √
n|(cid:98)θu
(cid:98)σ[M(cid:98)ΣM T]1/2
i |

i i

(cid:19)(cid:19)

.

(cid:18)
(cid:26)1

The decision rule is then based on the p-value Pi:
if Pi ≤ α
otherwise

Ti X(y) =

0

(reject H0 i)  
(accept H0 i) .

(8)

(9)

We measure the quality of the test Ti X(y) in terms of its signiﬁcance level αi and statistical power
1− βi. Here αi is the probability of type I error (i.e. of a false positive at i) and βi is the probability
of type II error (i.e. of a false negative at i).
Note that it is important to consider the tradeoff between statistical signiﬁcance and power. Indeed
any signiﬁcance level α can be achieved by randomly rejecting H0 i with probability α. This test
achieves power 1 − β = α. Further note that  without further assumption  no nontrivial power can
be achieved. In fact  choosing θ0 i (cid:54)= 0 arbitrarily close to zero  H0 i becomes indistinguishable
from its alternative. We will therefore assume that  whenever θ0 i (cid:54)= 0  we have |θ0 i| > µ as well.
We take a minimax perspective and require the test to behave uniformly well over s0-sparse vectors.
Formally  for µ > 0 and i ∈ [p]  deﬁne

(cid:111)
(cid:110)Pθ0(Ti X(y) = 1) : θ0 ∈ Rp  (cid:107)θ0(cid:107)0 ≤ s0(n)  θ0 i = 0
(cid:110)Pθ0(Ti X(y) = 0) : θ0 ∈ Rp  (cid:107)θ0(cid:107)0 ≤ s0(n)  |θ0 i| ≥ µ
(cid:111)

αi(n) ≡ sup
βi(n; µ) ≡ sup

.

.

5

Here  we made dependence on n explicit. Also  Pθ(·) is the induced probability for random design
X and noise realization w  given the ﬁxed parameter vector θ. Our next theorem establishes bounds
on αi(n) and βi(n; µ).
√
Theorem 3.3. Consider a random design model that satisﬁes the conditions of Theorem 2.1. Under
the sparsity assumption s0 = o(
n/ log p)  the following holds true for any ﬁxed sequence of
integers i = i(n):

√
n µ
σ[Σ−1
i i ]1/2
where  for α ∈ [0  1] and u ∈ R+  the function G(α  u) is deﬁned as follows:

1 − β∗

lim
n→∞

lim

n→∞ αi(n) ≤ α .
1 − βi(µ; n)
≥ 1  
1 − β∗
i (µ; n)

(cid:18)
i (µ; n) ≡ G

α 

(cid:19)

(10)

(11)

 

G(α  u) = 2 − Φ(Φ−1(1 − α
2

) + u) − Φ(Φ−1(1 − α
2

) − u) .

It is easy to see that  for any α > 0  u (cid:55)→ G(α  u) is continuous and monotone increasing. Moreover 
G(α  0) = α which is the trivial power obtained by randomly rejecting H0 i with probability α. As
√
µ deviates from zero  we obtain nontrivial power. Notice that in order to achieve a speciﬁc power
β > α  our scheme requires µ = O(σ/

i i ≤ σmax(Σ−1) ≤ (σmin(Σ))−1 = O(1).

n)  since Σ−1

3.2.1 Minimax optimality

The authors of [10] prove an upper bound for the minimax power of tests with a given signiﬁcance
level α  under the Gaussian random design models (see Theorem 2.6 therein). This bound is obtained
by considering an oracle test that knows all the active parameters except i  i.e.  S\{i}. To state the
bound formally  for a set S ⊆ [p] and i ∈ Sc  deﬁne Σi|S ≡ Σi i − Σi S(ΣS S)−1ΣS i  and let

(cid:111)

.

(cid:110)

ηΣ s0 ≡ min
i∈[p] S

Σi|S : S ⊆ [p]\{i}  |S| < s0
√

In asymptotic regime and under our sparsity assumption s0 = o(
simpliﬁes to

n/ log p)  the bound of [10]

1 − βopt
(α; µ)
G(α  µ/σeﬀ )

i

lim
n→∞

≤ 1  

σeﬀ =

σ√
n ηΣ s0

 

(12)

Using the bound of (12) and specializing the result of Theorem 3.3 to Gaussian design X  we obtain
that our scheme achieves a near optimal minimax power for a broad class of covariance matrices.
We can compare our test to the optimal test by computing how much µ must be increased in order to
achieve the minimax optimal power. It follows from the above that µ must be increased to ˜µ  with
the two differing by a factor:

(cid:113)

ii ηΣ s0 ≤(cid:113)

Σ−1

i i Σi i ≤(cid:112)σmax(Σ)/σmin(Σ)  

Σ−1

˜µ/µ =

since Σ−1

ii ≤ (σmin(Σ))−1  and Σi|S ≤ Σi i ≤ σmax(Σ) due to ΣS S (cid:31) 0.

4 General regularized maximum likelihood

In this section  we generalize our results beyond the linear regression model to general regularized
maximum likelihood. Here  we only describe the de-biasing method. Formal guarantees can be
obtained under suitable restricted strong convexity assumptions [20] and will be the object of a
forthcoming publication.
For univariate Y   and vector X ∈ Rp  we let {fθ(Y |X)}θ∈Rp be a family of conditional probability
densities parameterized by θ  that are absolutely continuous with respect to a common measure
ω(dy)  and suppose that the gradient ∇θfθ(Y |X) exists and is square integrable.
As in for linear regression  we assume that the data is given by n i.i.d. pairs (X1  Y1)  . . . (Xn  Yn) 
where conditional on Xi  the response variable Yi is distributed as

Yi ∼ fθ0(·|Xi) .

6

(cid:80)n
for some parameter vector θ0 ∈ Rp. Let Li(θ) = − log fθ(Yi|Xi) be the normalized negative
log-likelihood corresponding to the observed pair (Yi  Xi)  and deﬁne L(θ) = 1
i=1 Li(θ) . We
consider the following regularized estimator:

n

where λ is a regularization parameter and R : Rp → R+ is a norm.

We next generalize the deﬁnition of(cid:98)Σ. Let Ii(θ) be the Fisher information of fθ(Y |Xi)  deﬁned as
(cid:17)(cid:12)(cid:12)(cid:12)Xi
(cid:105)
= −E(cid:104)(cid:16)∇2
Ii(θ) ≡ E(cid:104)(cid:16)∇θ log fθ(Y |Xi)
sian operator. We assume E[Ii(θ)] (cid:31) 0 deﬁne(cid:98)Σ ∈ Rp×p as follows:

where the second identity holds under suitable regularity conditions [13]  and ∇2

θ log f (Y |Xi  θ)

θ denotes the Hes-

 

θ∈Rp

(cid:98)θ ≡ arg min
(cid:17)(cid:16)∇θ log fθ(Y |Xi)
n(cid:88)

(cid:8)L(θ) + λR(θ)(cid:9)  
(cid:17)T(cid:12)(cid:12)(cid:12)Xi
Ii((cid:98)θ) .

(cid:98)Σ ≡ 1

(cid:105)

(13)

(14)

n

i=1

setting is somewhat more general) with the crucial difference of the construction of M.

Note that (in general) (cid:98)Σ depends on (cid:98)θ. Finally  the de-biased estimator (cid:98)θu is deﬁned by (cid:98)θu ≡
(cid:98)θ − M∇θL((cid:98)θ)   with M given again by the solution of the convex program (4)  and the deﬁnition of
(cid:98)Σ provided here. Notice that this construction is analogous to the one in [25] (although the present
A a simple heuristic derivation of this method is the following. By Taylor expansion of L((cid:98)θ)
around θ0 we get (cid:98)θu ≈ (cid:98)θ − M∇θL(θ0) − M∇2
θL(θ0) ≈ (cid:98)Σ
(which amounts to taking expectation with respect to the response variables yi)  we get(cid:98)θu − θ0 ≈
−M∇θL(θ0)− [M(cid:98)Σ− I]((cid:98)θ −(cid:98)θ0). Conditionally on {Xi}1≤i≤n  the ﬁrst term has zero expectation
and covariance [M(cid:98)ΣM ]. Further  by central limit theorem  its low-dimensional marginals are ap-
proximately Gaussian. The bias term −[M(cid:98)Σ− I]((cid:98)θ −(cid:98)θ0) can be bounded as in the linear regression
case  building on the fact that M is chosen such that |M(cid:98)Σ − I|∞ ≤ γ.
is given by Ii = [(cid:98)θu
δ(α  n) = Φ−1(1 − α/2)n−1/2[M(cid:98)ΣM T]1/2

θL(θ0)((cid:98)θ − θ0) . Approximating ∇2

Similar to the linear case  an asymptotic two-sided conﬁdence interval for θ0 i (with signiﬁcance α)

i − δ(α  n) (cid:98)θu

i + δ(α  n)]  where

.

i i

Moreover  an asymptotically valid p-value Pi for testing null hypothesis H0 i is constructed as:

(cid:18)

Pi = 2

1 − Φ

(cid:18) √
n|(cid:98)θu
[M(cid:98)ΣM T]1/2
i |

i i

(cid:19)(cid:19)

.

In the next section  we shall apply the general approach presented here to L1-regularized logistic
regression. In this case  the binary response Yi ∈ {0  1} is distributed as Yi ∼ fθ0 (·|Xi) where
fθ0 (1|x) = (1 + e−(cid:104)x θ0(cid:105))−1 and fθ0(0|x) = (1 + e(cid:104)x θ0(cid:105))−1. It is easy to see that in this case

Ii((cid:98)θ) = (cid:98)qi(1 −(cid:98)qi)XiX T

i   with(cid:98)qi = (1 + e−(cid:104)(cid:98)θ Xi(cid:105))−1  and thus
(cid:98)qi(1 −(cid:98)qi)XiX T

(cid:98)Σ =

n(cid:88)

i .

1
n

i=1

5 Diabetes data example

We consider the problem of estimating relevant attributes in predicting type-2 diabetes. We evaluate
the performance of our hypothesis testing procedure on the Practice Fusion Diabetes dataset [1].
This dataset contains de-identiﬁed medical records of 10000 patients  including information on di-
agnoses  medications  lab results  allergies  immunizations  and vital signs. From this dataset  we ex-
tract p numerical attributes resulting in a sparse design matrix Xtot ∈ Rntot×p  with ntot = 10000 

7

(a) Q-Q plot of Z

(b) Normalized histograms of ˜Z for one realization.

Figure 1: Q-Q plot of Z and normalized histograms of ˜ZS (in red) and ˜ZSc (in blue) for one real-
ization. No ﬁtting of the Gaussian mean and variance was done in panel (b).

and p = 805 (only 5.9% entries of Xtot are non-zero). Next  we standardize the columns of X to
have mean 0 and variance 1. The attributes consist of: (i)Transcript records: year of birth  gender
and BMI; (ii)Diagnoses informations: 80 binary attributes corresponding to different ICD-9 codes.
(iii)Medications: 80 binary attributes indicating the use of different medications. (iv) Lab results:
For 70 lab test observations  we include attributes indicating patients tested  abnormality ﬂags  and
the observed values. We also bin the observed values into 10 quantiles and make 10 binary attributes
indicating the bin of the corresponding observed value.
We consider logistic model as described in the previous section with a binary response identifying
the patients diagnosed with type-2 diabetes. For the sake of performance evaluation  we need to
know the true signiﬁcant attributes. Letting L(θ) be the logistic loss corresponding to the design
Xtot and response vector Y ∈ Rntot  we take θ0 as the minimizer of L(θ). Notice that here  we are
in the low dimensional regime (ntot > p) and no regularization is needed.
Next  we take random subsamples of size n = 500 from the patients  and examine the performance
of our testing procedure. The experiment is done using glmnet-package in R that ﬁts the entire path
of the regularized logistic estimator. We then choose the value of λ that yields maximum AUC (area
under ROC curve)  approximated by a 5-fold cross validation.
Results: Type I errors and powers of our decision rule (9) are computed by comparing to θ0. The
average error and power (over 20 random subsamples) and signiﬁcance level α = 0.05 are respec-
tively  0.0319 and 0.818. Let Z = (zi)p
i i .
In Fig. 1(a)  sample quantiles of Z are depicted versus the quantiles of a standard normal distribu-
tion. The plot clearly corroborates our theoretical result regarding the limiting distribution of Z.
In order to build further intuition about the proposed p-values  let ˜Z = (˜zi)p
i=1 be the vector with
˜zi ≡ √
i i . In Fig. 1(b)  we plot the normalized histograms of ˜ZS (in red) and ˜ZSc (in
blue). As the plot showcases  ˜ZSc has roughly standard normal distribution  and the entries of ˜ZS
appear as distinguishable spikes. The entries of ˜ZS with larger magnitudes are easier to be marked
off from the normal distribution tail.

n((cid:98)θu
i − θ0 i)/[M(cid:98)ΣM ]1/2

i=1 denote the vector with zi ≡ √

n(cid:98)θu
i /[M(cid:98)ΣM ]1/2

References

[1] Practice Fusion Diabetes Classiﬁcation. http://www.kaggle.com/c/pf2012-diabetes  2012. Kaggle

competition dataset.

8

-3-2-10123-4-2024Standard normal quantilesSample Quantiles of ZHistograms of Z~Density-10-505100.00.10.20.30.4Z~ScZ~SN(0  1)[2] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Amer. J.

of Mathematics  37:1705–1732  2009.

[3] P. B¨uhlmann. Statistical signiﬁcance in high-dimensional linear models. arXiv:1202.1377  2012.
[4] P. B¨uhlmann and S. van de Geer. Statistics for high-dimensional data. Springer-Verlag  2011.
[5] E. Cand`es and Y. Plan. Near-ideal model selection by (cid:96)1 minimization. The Annals of Statistics 

37(5A):2145–2177  2009.

[6] E. J. Cand´es and T. Tao. Decoding by linear programming. IEEE Trans. on Inform. Theory  51:4203–

4215  2005.

[7] S. Chen and D. Donoho. Examples of basis pursuit. In Proceedings of Wavelet Applications in Signal and

Image Processing III  San Diego  CA  1995.

[8] E. Greenshtein and Y. Ritov. Persistence in high-dimensional predictor selection and the virtue of over-

parametrization. Bernoulli  10:971–988  2004.

[9] A. Javanmard and A. Montanari. Conﬁdence Intervals and Hypothesis Testing for High-Dimensional

Regression. arXiv:1306.3171  2013.

[10] A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the gaussian

random design model: Asymptotic theory. arXiv:1301.4240  2013.

[11] A. Javanmard and A. Montanari. Nearly Optimal Sample Size in Hypothesis Testing for High-

Dimensional Regression. arXiv:1311.0274  2013.

[12] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

42(8):30–37  August 2009.

[13] E. Lehmann and G. Casella. Theory of point estimation. Springer  2 edition  1998.
[14] E. Lehmann and J. Romano. Testing statistical hypotheses. Springer  2005.
[15] R. Lockhart  J. Taylor  R. Tibshirani  and R. Tibshirani. A signiﬁcance test for the lasso. arXiv preprint

arXiv:1301.7161  2013.

[16] M. Lustig  D. Donoho  J. Santos  and J. Pauly. Compressed sensing mri. IEEE Signal Processing Maga-

zine  25:72–82  2008.

[17] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso.

Ann. Statist.  34:1436–1462  2006.

[18] N. Meinshausen and P. B¨uhlmann. Stability selection. J. R. Statist. Soc. B  72:417–473  2010.
[19] J. Minnier  L. Tian  and T. Cai. A perturbation method for inference on regularized regression estimates.

Journal of the American Statistical Association  106(496)  2011.

[20] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557  2012.

[21] J. Peng  J. Zhu  A. Bergamaschi  W. Han  D.-Y. Noh  J. R. Pollack  and P. Wang. Regularized multivari-
ate regression for identifying master predictors with application to integrative genomics study of breast
cancer. The Annals of Applied Statistics  4(1):53–77  2010.

[22] M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. IEEE Transactions

on Information Theory  59(6):3434–3447  2013.

[23] T. Sun and C.-H. Zhang. Scaled sparse linear regression. Biometrika  99(4):879–898  2012.
[24] R. Tibshirani. Regression shrinkage and selection with the Lasso. J. Royal. Statist. Soc B  58:267–288 

1996.

[25] S. van de Geer  P. B¨uhlmann  and Y. Ritov. On asymptotically optimal conﬁdence regions and tests for

high-dimensional models. arXiv:1303.0518  2013.

[26] A. W. Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press  2000.
[27] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained

quadratic programming. IEEE Trans. on Inform. Theory  55:2183–2202  2009.

[28] L. Wasserman. All of statistics: a concise course in statistical inference. Springer Verlag  2004.
[29] L. Wasserman and K. Roeder. High dimensional variable selection. Annals of statistics  37(5A):2178 

2009.

[30] C.-H. Zhang and S. Zhang. Conﬁdence Intervals for Low-Dimensional Parameters in High-Dimensional

Linear Models. arXiv:1110.2563  2011.

[31] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research 

7:2541–2563  2006.

9

,Adel Javanmard
Andrea Montanari
Balázs Szörényi
Remi Munos
Kevin Ellis
Armando Solar-Lezama
Josh Tenenbaum
Shuangfei Zhai
Yu Cheng
Zhongfei (Mark) Zhang
Weining Lu