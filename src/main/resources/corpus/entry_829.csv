2011,Blending Autonomous Exploration and Apprenticeship Learning,We present theoretical and empirical results for a framework  that combines the benefits of apprenticeship and autonomous reinforcement learning.  Our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment.  The first change is replacing previously used Mistake Bound model learners with a recently proposed framework that melds the  KWIK and Mistake Bound supervised learning protocols.  The second change is introducing a communication of expected utility from the student to the teacher.   The resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.,Blending Autonomous Exploration and

Apprenticeship Learning

Thomas J. Walsh

Center for Educational
Testing and Evaluation
University of Kansas
Lawrence  KS 66045
twalsh@ku.edu

Daniel Hewlett

Clayton T. Morrison

{dhewlett@cs clayton@sista}.arizona.edu

School of Information:

Science  Technology and Arts

University of Arizona

Tucson  AZ 85721

Abstract

We present theoretical and empirical results for a framework that combines the
beneﬁts of apprenticeship and autonomous reinforcement learning. Our approach
modiﬁes an existing apprenticeship learning framework that relies on teacher
demonstrations and does not necessarily explore the environment. The ﬁrst change
is replacing previously used Mistake Bound model learners with a recently pro-
posed framework that melds the KWIK and Mistake Bound supervised learning
protocols. The second change is introducing a communication of expected util-
ity from the student to the teacher. The resulting system only uses teacher traces
when the agent needs to learn concepts it cannot efﬁciently learn on its own.

1

Introduction

As problem domains become more complex  human guidance becomes increasingly necessary to
improve agent performance. For instance  apprenticeship learning  where teachers demonstrate
behaviors for agents to follow  has been used to train agents to control complicated systems such
as helicopters [1]. However  most work on this topic burdens the teacher with demonstrating even
the simplest nuances of a task. By contrast  in autonomous reinforcement learning [2] a number
of domain classes can be efﬁciently learned by an actively exploring agent  although this class is
provably smaller than those learnable with the help of a teacher [3].
Thus the ﬁeld seems to be largely bifurcated. Either agents learn autonomously and eschew the larger
learning capacity from teacher interaction  or the agent overburdens the teacher by not exploring
simple concepts it could garner on its own. Intuitively  this seems like a false choice  as human
teachers often use demonstration but also let students explore parts of the domain on their own. We
show how to build a provably efﬁcient learning system that balances teacher demonstrations and
autonomous exploration. Speciﬁcally  our protocol and algorithms cause a teacher to only step in
when its advice will be signiﬁcantly more helpful than autonomous exploration by the agent.
We extend a previously proposed apprenticeship learning protocol [3] where a learning agent and
teacher take turns running trajectories. This version of apprenticeship learning is fundamentally
different from Inverse Reinforcement Learning [4] and imitation learning [5] because our agents are
allowed to enact better policies than their teachers and observe reward signals. In this setting  the
number of times the teacher outperforms the student was proven to be related to the learnability of
the domain class in a mistake bound predictor (MBP) framework.
Our work modiﬁes previous apprenticeship learning efforts in two ways. First  we will show that
replacing the MBP framework with a different learning architecture called KWIK-MBP (based on
a similar recently proposed protocol [6]) indicates areas where the agent should autonomously ex-
plore  and melds autonomous and apprenticeship learning. However  this change alone is not sufﬁ-

1

cient to keep the teacher from intervening when an agent is capable of learning on its own. Hence 
we introduce a communication of the agent’s expected utility  which provides enough information
for the teacher to decide whether or not to provide a trace (a property not shared by any of the pre-
vious efforts). Furthermore  we show the number of such interactions grows only with the MBP
portion of the KWIK-MBP bound. We then discuss how to relax the communication requirement
when the teacher observes the student for many episodes. This gives us the ﬁrst apprenticeship
learning framework where a teacher only shows demonstrations when they are needed for efﬁcient
learning  and gracefully blends autonomous exploration and apprenticeship learning.

2 Background

The main focus of this paper is blending KWIK autonomous exploration strategies [7] and appren-
ticeship learning techniques [3]  utilizing a framework for measuring mistakes and uncertainty based
on KWIK-MB [6]. We begin by reviewing results relating the learnability of domain parameters in
a supervised setting to the efﬁciency of model-based RL agents.

2.1 MDPs and KWIK Autonomous Learning
We will consider environments modeled as a Markov Decision Process (MDP) [2] hS  A  T  R  γi 
with states and actions S and A  transition function T : S  A 7→ P r[S]  rewards R : S  A 7→ <  and
discount factor γ ∈ [0  1). The value of a state under policy π : S 7→ A is Vπ(s) = R(s  π(s)) +

s0∈S T (s  a  s0)Vπ(s0) and the optimal policy π∗ satisﬁes ∀πVπ∗ ≥ Vπ.

γP

In model-based reinforcement learning  recent advancements [7] have linked the efﬁcient learnabil-
ity of T and R in the KWIK (“Knows What It Knows”) framework for supervised learning with
PAC-MDP behavior [8]. Formally  KWIK learning is:
Deﬁnition 1. A hypothesis class H : X 7→ Y is KWIK learnable with parameters  and δ if the
following holds. For each (adversarial) input xt the learner predicts yt ∈ Y or “I don’t know”
(⊥). With probability (1 − δ) (1) when yt 6= ⊥ ||yt − E[h(xt)]|| <  and (2) the total number of ⊥
predictions is bounded by a polynomial function of (|H|  1
Intuitively  KWIK caps the number of times the agent will admit uncertainty in its predictions. Prior
work [7] showed that if the transition and reward functions (T and R) of an MDP are KWIK learn-
able  then a PAC-MDP agent (which takes only a polynomial number of suboptimal steps with high
probability) can be constructed for autonomous exploration. The mechanism for this construction is
an optimistic interpretation of the learned model. Speciﬁcally  KWIK-learners LT and LR are built
for T and R and the agent replaces any ⊥ predictions with transitions to a trap state with reward
Rmax  causing the agent to explore these uncertain regions. This exploration requires only a polyno-
mial (with respect to the domain parameters) number of suboptimal steps  thus the link from KWIK
to PAC-MDP. While the class of functions that is KWIK learnable includes tabular and factored
MDPs  it does not cover many larger dynamics classes (such as STRIPS rules with conjunctions for
pre-conditions) that are efﬁciently learnable in the apprenticeship setting.

   1

δ ).

2.2 Apprenticeship Learning with Mistake Bound Predictor

We now describe an existing apprenticeship learning framework [3]  which we will be modifying
throughout this paper. In that protocol  an agent is presented with a start state s0 and is asked to take
actions according to its current policy πA  until a horizon H or a terminal state is reached. After each
of these episodes  a teacher is allowed to (but may choose not to) demonstrate their own policy πT
starting from s0. The learning agent is able to fully observe each transition and reward received both
in its own trajectories as well as those of the teacher  who may be able to provide highly informative
samples. For example  in an environment with n bits representing a combination lock that can only
be opened with a single setting of the bits  the teacher can demonstrate the combination in a single
trace  while an autonomous agent could spend 2n steps trying to open it.
Also in that work  the authors describe a measure of sample complexity called PAC-MDP-Trace
(analogous to PAC-MDP from above) that measures (with probability 1− δ) the number of episodes
where VπA(s0) < VπT (s0)−   that is where the expected value of the agent’s policy is signiﬁcantly
worse than the expected value of the teacher’s policy (VA and VT for short). A result analogous

2

to the KWIK to PAC-MDP result was shown connecting a supervised framework called Mistake
Bound Predictor (MBP) to PAC-MDP-Trace behavior. MBP extends the classic mistake bound
learning framework [9] to handle data with noisy labels  or more speciﬁcally:
Deﬁnition 2. A hypothesis class H : X 7→ Y is Mistake Bound Predictor (MBP) learnable with
parameters  and δ if the following holds. For each adversarial input xt  the learner predicts yt ∈ Y .
If ||Eh∗[xt]− yt|| >   then the agent has made a mistake. The number of mistakes must be bounded
by a polynomial over ( 1

δ  |H|) with probability (1 − δ).

   1

An agent using MBP learners LT and LR for the MDP model components will be PAC-MDP-
Trace. The conversion mirrors the KWIK to PAC-MDP connection described earlier  except that the
interpretation of the model is strict  and often pessimistic (sometimes resulting in an underestimate
of the value function). For instance  if the transition function is based on a conjunction (e.g. our
combination lock)  the MBP learners default to predicting “false” where the data is incomplete 
leading an agent to think its action will not work in those situations. Such interpretations would
be catastrophic in the autonomous case (where the agent would fail to explore such areas)  but are
permissible in apprenticeship learning where teacher traces will provide the missing data.
Notice that under a criteria where the number of teacher traces is to be minimized  MBP learning
may overburden the teacher. For example  in a simple ﬂat MDP  an MBP-Agent picks actions that
maximize utility in the part of the state space that has been exposed by the teacher  never exploring 
so the number of teacher traces scales with |S||A|. But a ﬂat MDP is autonomously (KWIK) learn-
able  so no traces should be required. Ideally an agent would explore the state space where it can
learn efﬁciently  and only rely on the teacher for difﬁcult to learn concepts (like conjunctions).

3 Teaching by Demonstration with Mixed Interpretations

We now introduce a different criteria with the goal of minimizing teacher traces while not forcing
the agent to explore exponentially long.
Deﬁnition 3. A Teacher Interaction (TI) bound for a student-teacher pair is the number of episodes
where the teacher provides a trace to the agent that guarantees (with probability 1 − δ) that the
number of agent steps between each trace (or after the last one) where VA(s0) < VT (s0) −  is
polynomial in 1

δ   and the domain parameters.

   1

A good TI bound minimizes the teacher traces needed to achieve good behavior  but only requires
the suboptimal exploration steps to be polynomially bounded  not minimized. This reﬂects our
judgement that teacher interactions are far more costly than autonomous agent steps  so as long
as the latter are reasonably constrained  we should seek to minimize the former. The relationship
between TI and PAC-MDP-Trace is the following:
Theorem 1. The TI bound for a domain class and learning algorithm is upper-bounded by the
PAC-MDP-Trace bound for the same domain/algorithm with the same  and δ parameters.
Proof. A PAC-MDP-Trace bound quantiﬁes (with probability 1 − δ) the worst-case number of
episodes where the student performs worse than the teacher  speciﬁcally where VA(s0) < VT (s0)−.
Suppose an environment existed with a PAC-MDP-Trace bound of B1 and a TI bound of B2 > B1.
This would mean the domain was learnable with at most B1 teacher traces. But this is a contradiction
because no more traces are needed to keep the autonomous exploration steps polynomial.

3.1 The KWIK-MBP Protocol

We would like to describe a supervised learning framework (like KWIK or MBP) that can quantify
the number of changes made to a model through exploration and teacher demonstrations. Here  we
propose such a model based on the recent KWIK-MB protocol [6]  which we extend below to cover
stochastic labels (KWIK-MBP).
Deﬁnition 4. A hypothesis class H : X 7→ Y is KWIK-MBP with parameters  and δ under
the following conditions. For each (adversarial) input xt the learner must predict yt ∈ Y or ⊥.
With probability (1 − δ)  the number of ⊥ predictions must be bounded by a polynomial K over
h|H|  1/  1/δi and the number of mistakes (by Deﬁnition 2) must be bounded by a polynomial M
over h|H|  1/  1/δi.

3

s0 = Environment.startState

if VT (s0) − k−1

k  > UA(s0) then

k value accuracy w.h.p. for k ≥ 2

Algorithm 1 KWIK-MBP-Agent with Value Communication
1: The agent A knows   δ  S  A  H and planner P .
2: The teacher T has policy πT with expected value VT
3: Initialize KWIK-MBP learners LT and LR to ensure 
4: for each episode do
5:
6: A calculates the value function UA of πA from ˆS  A  ˆT and ˆR (see construction below).
7: A communicates its expected utility UA(s0) on this episode to T
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

T provides a trace τ starting from s0.
∀hs  a  r  s0i Update LT (s  a  s0) and LR(s  a  r)
while episode not ﬁnished and t < H do
ˆR = LR(s  a) or Rmax if LR(s  a) = ⊥
ˆT = LT (s  a) or Smax if LT (s  a) = ⊥.
at = P.getPlan(st  ˆS  ˆT   ˆR).
hrt  st+1i = E.executeAct(at)
LT .Update(st  at  st+1); LR.U pdate(st  at  rt)

ˆS = SS Smax  the Rmax trap state

KWIK-MB was originally designed for a situation where mistakes are more costly than ⊥ predic-
tions. So mistakes are minimized while ⊥ predictions are only bounded. This is analogous to our
TI criteria (traces minimized with exploration bounded) so we now examine a KWIK-MBP learner
in the apprenticeship setting.

3.2 Mixing Optimism and Pessimism

16k and T =  (1−γ)2

Algorithm 1 (KWIK-MBP-Agent) shows an apprenticeship learning agent built over KWIK-MBP
learners LT and LR. Both of these model learners are instantiated to ensure the learned value
k accuracy for k ≥ 2 (for reasons discussed in the main theorem)  which can
function will have 
be done by setting R =  1−γ
(details follow the same form as standard
connections between model learners and value function accuracy  for example in Theorem 3 from
[7]). When planning with the subsequent model  the agent constructs a “mixed” interpretation 
trusting the learner’s predictions where mistakes might be made  but replacing (lines 13-14) all ⊥
predictions from LR with a reward of Rmax and any ⊥ predictions from ˆT with transitions to the
Rmax trap state Smax. This has the effect of drawing the agent to explore explicitly uncertain regions
(⊥) and to either explore on its own or rely on the teacher for areas where a mistake might be made.
For instance  in the experiments in Figure 2 (left)  discussed in depth later  a KWIK-MBP agent
only requires traces for learning the pre-conditions in a noisy blocks world but uses autonomous
exploration to discover the noise probabilities.

16kγVmax

4 Teaching by Demonstration with Explicit Communication

Thus far we have not discussed communication from the student to the teacher in KWIK-MBP-
Agent (line 7). We now show that this communication is vital in keeping the TI bound small.
Example 1. Suppose there was no communication in Algorithm 1 and the teacher provided a trace
when πA was suboptimal. Consider a domain where the pre-conditions of actions are governed by
a disjunction over the n state factors (if the disjunction fails  the action fails). Disjunctions can be
learned with M = n/3 mistakes and K = 3n/2 − 3M ⊥ predictions [6]. However  that algorithm
defaults to predicting “true” and only learns from negative examples. This optimistic interpretation
means the agent will expect success  and can learn autonomously. However  the teacher will provide
a trace to the agent since it sees it performing suboptimally during exploration. Such traces are
unnecessary and uninformative (their positive examples are useless to LT ).

This illustrates the need for student communication to give some indication of its internal model
to the teacher. The protocol in Algorithm 1 captures this intuition by providing a channel (line 7)

4

where the student communicates its expected utility UA. The teacher then only shows a trace to
a pessimistic agent (line 8)  but will “stand back” and let an over-conﬁdent student learn from its
own mistakes. We note that there are many other possible forms of this communication such as
announcing the probability of reaching a goal or an equivalence query [10] type model  where the
student exposes its entire internal model to the teacher. We focus here on the communication of
utility  which is general enough for MDP domains but has low communication overhead.

4.1 Theoretical Properties

k

The proof of the algorithm’s TI bound ap-
pears below and is illustrated in Figure 1
but intuitively we show that if we force
k -accurate
the student to (w.h.p.) learn an 
value function for k ≥ 2 then we can guar-
antee traces where UA < VT − 
k will
be helpful  but are not needed until UA
is reported below VT − (k−1)
  at which
point UA alone cannot guarantee that VA
is within  of VT and so a trace must be
given. Because traces are only given when
the student undervalues a potential policy 
the number of traces is related only to the
MBP portion of the KWIK-MBP bound  and more speciﬁcally to the number of pessimistic mis-
takes  deﬁned as:
Deﬁnition 5. A mistake is pessimistic if and only if it causes some policy π to be undervalued in the
agent’s model  that is in our case Uπ < Vπ − 
k .
Note that by the construction of our model  KWIK-learnable parameters (⊥ replaced by Rmax-style
interpretations) never result in such pessimistic mistakes. We can now state the following:
Theorem 2. Algorithm 1 with KWIK-MBP learners will have a TI bound that is polynomial in 1
1−γ and P   where P is the number of pessimistic mistakes (P ≤ M) made by LT and LR.
and 1

Figure 1: The areas for UA and VA corresponding to
the cases in the main theorem. In all cases UT ≤ UA
and when k = 2 the two dashed lines collapse together.

δ  
   1

Proof. The proof stems from an expansion of the Explore-Explain-Exploit Lemma from [3]. That
original lemma categorized the three possible outcomes of an episode in an apprenticeship learning
setting where the teacher always gives a trace and with LT and LR built to learn V within 
2.
The three possibilities for an episode were (1) exploration  when the agent’s value estimate of πA
is inaccurate  ||VA − UA|| > /2  (2) exploitation when the agent’s prediction of its own return is
accurate (||UA−VA|| ≤ /2) and the agent is near-optimal with respect to the teacher (VA ≥ VT −) 
and (3) explanation when ||VA − UA|| ≤ /2  but VA < VT − . Because both (1) and (3) provide
samples to LT and LR  the number of times they can occur is bounded (in the original lemma) by the
MBP bound on those learners and in both cases a relevant sample is produced with high probability
due to the simulation lemma (c.f. Lemma 9 of [7])  which states that two different value returns
from two MDPs means that  with high probability  their parameters must be different.
We need to extend the lemma to cover our change in protocol (the teacher may not step in on every
episode) and in evaluation criteria (TI bound instead of PAC-MDP-Trace). Speciﬁcally  we need to
show: (i) The number of steps between traces where VA < VT − is polynomially bounded. (ii) Only
a polynomial number of traces are given  and they are all guaranteed to improve some parameter in
the agent’s model with high probability. (iii) Only pessimistic mistakes (Deﬁnition 5) cause a teacher
intervention. Note that properties (i) and (ii) imply that VA < VT −  for only a polynomial number
of episodes and correspond directly to the TI criteria from Deﬁnition 3. We now consider Algorithm
1 according to these properties in all of the cases from the original explore-exploit-explain lemma.
We begin with the Explain case where VA < VT −  and ||UA − VA|| ≤ 
k . Combining these
inequalities  we know UA < VT − (k−1)
  so a trace will deﬁnitely be provided. Since UT ≤ UA
(UT is the value of πT in the student’s model and UA was optimal) we have at least UT < VT − 
k
and the simulation lemma implies the trace will (with high probability) be helpful. Since there are a

k

5

VT VA UA e/k e/k Explain Explore (4)  Exploit (2)  (1)  UA (2)  UA UA VA UA (1)VA (3)VA (4)VA (2)VA VT-e limited number of such mistakes (because LR and LT are KWIK-MBP learners) we have satisﬁed
property (ii). Property (iii) is true because both πT and πA are undervalued.
We now consider the Exploit case where VA ≥ VT −  and ||UA − VA|| ≤ 
k . There are two possible
. If UA ≥ VT − (k−1)
situations here  because UA can either be larger or smaller than VT − (k−1)
k
If
then no trace is given  but the agent’s policy is near optimal so property (i) is not violated.
UA < VT − (k−1)
  then a trace is given  even in this exploit case  because the teacher does not
know VA and cannot distinguish this case from the “explain” case above. However  this trace will
still be helpful  because UT ≤ UA  so at least UT < VT − 
k (satisfying iii)  and again by the
simulation lemma  the trace will help us learn a parameter and there are a limited number of such
mistakes  so (ii) holds.
Finally  we have the Explore case  where ||UA − VA|| > 
will help it learn a parameter  but in terms of traces we have the following cases:
UA ≥ VT − (k−1)
k
property (i) holds.
UA ≥ VT − (k−1)
k . No trace is given here  but this is the classical exploration
case (UA is optimistic  as in KWIK learning). Since UA and VA are sufﬁciently separated  the
agent’s own experience will provide a useful sample  and because all parameters are polynomially
learnable  property (i) is satisﬁed.
UA < VT − (k−1)
and either VA > UA + 
provided but UT ≤ UA so at least UT < VT − 
(ii)). Pessimistic mistakes are causing the trace (property iii) since πT is undervalued.

k . In that case  the agent’s own experience
k . In this case no trace is given but we have VA > VT −   so

and UA > VA + 

k or UA > VA + 

k . In either case  a trace will be
k and the trace will be helpful (satisfying property

and VA > UA + 

k

k

k

k

2-accurate learners [3] to 

Our result improves on previous results by attempting to minimize the number of traces while rea-
sonably bounding exploration. The result also generalizes earlier apprenticeship learning results
on 
k -accuracy  while ensuring a more practical and stronger bound (TI
instead of PAC-MDP-Trace). The choice of k in this situation is somewhat complicated. Larger k
requires more accuracy of the learned model  but decreases the size of the “bottom region” above
where a limited number of traces may be given to an already near-optimal agent. So increasing k
can either increase or decrease the number of traces  depending on the exact problem instance.

4.2 Experiments

We now present experiments in two domains. The ﬁrst domain is a blocks world with dynamics
based on stochastic STRIPS operators  a −1 step cost  and a goal of stacking the blocks. That is  the
environment state is described as a set of grounded relations (e.g. On(a  b)) and actions are described
by relational (with variables) operators that have conjunctive pre-conditions that must hold for the
action to execute (e.g. putDown(X  To) cannot execute unless the agent is holding X and To is
clear and a block). If the pre-conditions hold  then one of a set of possible effects (pairs of Add
and Delete lists)  chosen based on a probability distribution over effects  will change the current
state. The actions in our blocks world are two versions of pickup(X  From) and two versions of
putDown(X  To)  with one version being “reliable”  producing the expected result 80% of the time
and otherwise doing nothing. The other version of each action has the probabilities reversed. The
literals in the effects of the STRIPS operators (the Add and Delete lists) are given to the learning
agents  but the pre-conditions and the probabilities of the effects need to be learned. This is an
interesting case because the effect probabilities can be learned autonomously while the conjunctive
pre-conditions (of sizes 3 and 4)  require teacher input (like our combination lock example).
Figure 2  column 1  shows KWIK  MBP  and KWIK-MBP agents as trained by a teacher who uses
unreliable actions half the time. The KWIK learner never receives traces (since its expected utility 
shown in 1a  is always high)  but spends an exponential (in the number of literals) time exploring
the potential pre-conditions of actions (1b).
In contrast  the MBP and KWIK-MBP agents use
the ﬁrst trace to learn the pre-conditions. The proportion of trials (out of 30) that the MBP and
KWIK-MBP learners received teacher traces across episodes is shown in the bar graphs 1c and 1d
of Fig. 2. The MBP learner continues to get traces for several episodes afterwards  using them to

6

help learn the probabilities well after the pre-conditions are learned. This probability learning could
be accomplished autonomously  but the MBP pessimistic value function prevents such exploration
in this case. By contrast  KWIK-MBP receives 1 trace to learn the pre-conditions  and then explores
the probabilities on its own. KWIK-MBP actually learns the probabilities faster than MBP because
it targets areas it does not know about rather than relying on potentially redundant teacher samples.
However  in rare cases KWIK-MBP receives additional traces; in fact there were two exceptions in
the 30 trials  indicated by ∗’s at episodes 5 and 19 in 1d. The reason for this is that sometimes the
learner may be unlucky and construct an inaccurate value estimate and the teacher then steps in and
provides a trace.
The second domain is a variant of “Wumpus
World” with 5 locations in a chain  an agent
who can move  ﬁre arrows (unlimited supply)
or pick berries (also unlimited)  and a wum-
pus moving randomly. The domain is repre-
sented by a Dynamic Bayes Net (DBN) based
on these factors and the reward is represented
as a linear combination of the factor values
(−5 for a live wumpus and +2 for picking
a berry). The action effects are noisy  espe-
cially the probability of killing the wumpus 
which depends on the exact (not just relative)
locations of the agent  wumpus  and whether
the wumpus is dead yet (three parent fac-
tors in the DBN). While the reward function
is KWIK learnable through linear regression
[7] and though DBN CPTs with small parent
sizes are also KWIK learnable  the high con-
nectivity of this particular DBN makes au-
tonomous exploration of all the parent-value
conﬁgurations prohibitive. Because of this 
in our KWIK-MBP implementation  we com-
bined a KWIK linear regression learner for
LR with an MBP learner for LT that is given
the DBN structure and learns the parameters
from experience  but when entries in the con-
ditional probability tables are the result of
only a few data points  the learner predicts no change for this factor  which was generally a pes-
simistic outcome. We constructed an “optimal hunting” teacher that ﬁnds the best combination of
locations to shoot the wumpus from/at  but ignores the berries. We concentrate on the ability of our
algorithm to ﬁnd a better policy than the teacher (i.e.  learning to pick berries)  while staying close
enough to the teacher’s traces that it can hunt the wumpus effectively.
Figure 2  column 2  presents the results from this experiment. In plot 2a we see the predicted values
of the three learners  while plot 2b shows their performance. The KWIK learner starts with high
UA that gradually descends (in 2a)  but without traces the agent spends most of its time exploring
fruitlessly (very slowly inclining slope of 2b). The MBP agent learns to hunt from the teacher
and quickly achieves good behavior  but rarely learns to pick berries (only gaining experience on
the reward of berries if it ends up in completely unknown state and picks berries at random many
times). The KWIK-MBP learner starts with high expected utility and explores the structure of just
the reward function  discovering berries but not the proper location combinations for killing the
wumpus. Its UA thus initially drops precipitously as it thinks all it can do is collect berries. Once
this crosses the teacher’s threshold  the teacher steps in with a number of traces showing the best
way to hunt the wumpus—this is seen in plot 2d with the small bump in the proportion of trials
with traces  starting at episode 2 and declining roughly linearly until episode 10. The KWIK-MBP
student is then able to ﬁll in the CPTs with information from the teacher and reach an optimal policy
that kills the wumpus and picks berries  avoiding both the over- and under-exploration of the KWIK
and MBP agents. This increased overall performance is seen in plot 2b as KWIK-MBP’s average
cumulative reward surpasses MBP between episodes 5 and 10 .

Figure 2: A plot matrix with rows (a) value predic-
tions UA(s0)  (b) average undiscounted cumulative
reward and (c and d) the proportion of trials where
MBP and KWIK-MBP received teacher traces. The
left column is Blocks World and the right a modiﬁed
Wumpus World. Red corresponds to KWIK  blue to
MBP  and black to KWIK-MBP.

7

051015202500.51MBPPr(Trace)051015202500.51Pr(Trace)EpisodesKWIK−MBP0510152025−10−8−6−4−20Predicted ValuesBlocks World  KWIKMBPKWIK−MBP0510152025−16−14−12−10−8−6−4Avg Undiscounted Reward05101520253000.51MBPPr(Trace)05101520253000.51Pr(Trace)EpisodesKWIK−MBP051015202530−100−500Predicted ValuesWumpus World  KWIKMBPKWIK−MBP051015202530−60−50−40−30−20−100Avg Undiscounted RewardPr(Trace)Avg Cumulative RewardPr(Trace)Predicted ValuesEpisodesEpisodesMBPMBPKWIK-MBPKWIK-MBP**051015202500.51MBPPr(Trace)051015202500.51Pr(Trace)EpisodesKWIK−MBP0510152025−10−8−6−4−20Predicted ValuesBlocks World  KWIKMBPKWIK−MBP0510152025−16−14−12−10−8−6−4Avg Undiscounted Reward05101520253000.51MBPPr(Trace)05101520253000.51Pr(Trace)EpisodesKWIK−MBP051015202530−100−500Predicted ValuesWumpus World  KWIKMBPKWIK−MBP051015202530−60−50−40−30−20−100Avg Undiscounted RewardPr(Trace)Avg Undiscounted RewardPr(Trace)Predicted ValuesEpisodesEpisodesMBPMBPKWIK-MBPKWIK-MBP**1a2a1b2b1c2c1d2dKWIKKWIK-MBPMBPKWIKKWIK-MBPMBP5

Inferring Student Aptitude

We now describe a method for a teacher to infer the student’s aptitude by using long periods without
teacher interventions as observation phases. This interaction protocol is an extension of Algorithm
1  but instead of using direct communication  the teacher will allow the student to run some number
of trajectories m from a ﬁxed start state and then decide whether to show a trace or not.
We would like to show that the length (m) of each observation phase can be polynomially bounded
and the system as a whole can still maintain a good TI bound. We show below that such an m exists
and is related to the PAC-MDP bound for a portion of the environment we call the zone of tractable
exploration (ZTE). The ZTE (inspired by the zone of proximal development [11]) is the area of an
MDP that an agent with background knowledge B and model learners LT and LR can act in with
a polynomial number of suboptimal steps as judged only within that area. Combining the ZTE  B 
LT and LR induces a learning sub-problem where the agent must learn to act as well as possible
without the teacher’s help.
Remark 1. If the learning agent is KWIK-MBP and the evaluation phase has length m = A1 + A2
where A1 is the PAC-MDP bound for the ZTE and A2 is the number of trials all starting from s0
needed to estimate VA(s0) ( ˆVA) within accuracy /k for k ≥ 4  and the teacher only steps in when
ˆVA < VT − (k−1)
  the resulting interaction will have a TI bound equivalent to the earlier one 
although the student needs to wait m trials to get a trace from the teacher.
A1 trials are necessary because the agent may need to explore all the ⊥ or optimistic mistakes within
the ZTE  and each episode might contain only one of the A1 suboptimal steps. Since each trajectory
with a ﬁxed policy results in an i.i.d. sample with mean VA  A2 can be polynomially bounded using
a Chernoff bound [12]. Note we require here that k ≥ 4 (a stricter requirement than earlier). This is
because we have errors of ||VA− ˆVA|| ≤ /k and ||UA− VA|| ≤ /k  so ˆVA needs to be at least 3/k
below VT to ensure UT < VT − /k  and therefore traces are helpful. But ˆVA may also overestimate
VA  leading to an extra /k slack term  and hence k ≥ 4.

k

6 Related Work and Conclusions

Our teaching protocol extends early apprenticeship learning work for linear MDPs [1]  which
showed a polynomial number of upfront traces followed by greedy (not explicitly exploring) tra-
jectories could achieve good behavior. Our protocol is similar to a recent “practice/critique” in-
teraction [13] where a teacher observed an agent and then labeled individual actions as “good” or
“bad”  but the teacher did not provide demonstrations in that work. Our setting differs from inverse
reinforcement learning [4  5] because our student can act better than the teacher  does not know the
dynamics  and observes rewards. Studies have also been done on humans providing shaping rewards
as feedback to agents rather than our demonstration technique [14  15].
Some works have taken a heuristic approach to mixing autonomous learning and teacher-provided
trajectories. This has been done in robot reinforcement learning domains [16] and for bootstrapping
classiﬁers [17]. Many such approaches give all the teacher data at the beginning  while our teaching
protocol has the teacher only step in selectively  and our theoretical results ensure the teacher will
only step in when its advice will have a signiﬁcant effect.
We have shown how to use an extension of the KWIK-MB [6] (now KWIK-MBP) framework as
the basis for model-based RL agents in the apprenticeship paradigm. These agents have a “mixed”
interpretation of their learned models that admits a degree of autonomous exploration. Furthermore 
introducing a communication channel from the student to the teacher and having the teacher only
give traces when VT is signiﬁcantly better than UA guarantees the teacher will only provide demon-
strations that attempt to teach concepts the agent could not tractably learn on its own  which has
clear beneﬁts when demonstrations are far more costly than exploration steps.

Acknowledgments

We thank Michael Littman and Lihong Li for discussions and DARPA-27001328 for funding.

8

References

[1] Pieter Abbeel and Andrew Y. Ng. Exploration and apprenticeship learning in reinforcement

learning. In ICML  2005.

[2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT

Press  Cambridge  MA  March 1998.

[3] Thomas J. Walsh  Kaushik Subramanian  Michael L. Littman  and Carlos Diuk. Generalizing

apprenticeship learning across hypothesis classes. In ICML  2010.

[4] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning.

In ICML  2004.

[5] Nathan Ratliff  David Silver  and J. Bagnell. Learning to search: Functional gradient tech-

niques for imitation learning. Autonomous Robots  27:25–53  2009.

[6] Amin Sayedi  Morteza Zadimoghaddam  and Avrim Blum. Trading off mistakes and don’t-

know predictions. In NIPS  2010.

[7] Lihong Li  Michael L. Littman  Thomas J. Walsh  and Alexander L. Strehl. Knows what it

knows: A framework for self-aware learning. Machine Learning  82(3):399–443  2011.

[8] Alexander L. Strehl  Lihong Li  and Michael L. Littman. Reinforcement learning in ﬁnite

MDPs: PAC analysis. Journal of Machine Learning Research  10:2413–2444  2009.

[9] Nick Littlestone. Learning quickly when irrelevant attributes abound. Machine Learning 

2:285–318  1988.

[10] Dana Angluin. Queries and concept learning. Machine Learning  2(4):319–342  1988.
[11] Lev Vygotsky. Interaction between learning and development. In Mind In Society. Harvard

University Press  Cambridge  MA  1978.

[12] Michael J. Kearns  Yishay Mansour  and Andrew Y. Ng. Approximate planning in large

pomdps via reusable trajectories. In NIPS  1999.

[13] Kshitij Judah  Saikat Roy  Alan Fern  and Thomas G. Dietterich. Reinforcement learning via

practice and critique advice. In AAAI  2010.

[14] W. Bradley Knox and Peter Stone. Combining manual feedback with subsequent mdp reward

signals for reinforcement learning. In AAMAS  2010.

[15] Andrea Lockerd Thomaz and Cynthia Breazeal. Teachable robots: Understanding human
teaching behavior to build more effective robot learners. Artiﬁcial Intelligence  172(6-7):716–
737  2008.

[16] William D. Smart and Leslie Pack Kaelbling. Effective reinforcement learning for mobile

robots. In ICRA  2002.

[17] Sonia Chernova and Manuela Veloso.

Interactive policy learning through conﬁdence-based

autonomy. Journal of Artiﬁcial Intelligence Research  34(1):1–25  2009.

9

,Shane Griffith
Kaushik Subramanian
Jonathan Scholz
Charles Isbell
Andrea Thomaz
Ryan Kiros
Richard Zemel
Russ Salakhutdinov
Mahdi Soltanolkotabi
Piotr Mirowski
Matt Grimes
Mateusz Malinowski
Karl Moritz Hermann
Keith Anderson
Denis Teplyashin
Karen Simonyan
koray kavukcuoglu
Andrew Zisserman
Raia Hadsell