2008,An Online Algorithm for Maximizing Submodular Functions,We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time  and one can complete the jobs by investing time in a number of abstract activities  according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone  submodular function of a set of pairs (v t)  where t is the time invested in activity v. Under this assumption  our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T  for some fixed deadline T > 0  and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn  online  a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition.,An Online Algorithm for Maximizing

Submodular Functions

Matthew Streeter

Google  Inc.

Pittsburgh  PA 15213

mstreeter@google.com

Daniel Golovin

Carnegie Mellon University

Pittsburgh  PA 15213

dgolovin@cs.cmu.edu

Abstract

We present an algorithm for solving a broad class of online resource allocation
problems. Our online algorithm can be applied in environments where abstract
jobs arrive one at a time  and one can complete the jobs by investing time in a
number of abstract activities  according to some schedule. We assume that the
fraction of jobs completed by a schedule is a monotone  submodular function of
a set of pairs (v  τ)  where τ is the time invested in activity v. Under this as-
sumption  our online algorithm performs near-optimally according to two natural
metrics: (i) the fraction of jobs completed within time T   for some ﬁxed dead-
line T > 0  and (ii) the average time required to complete each job. We evaluate
our algorithm experimentally by using it to learn  online  a schedule for allocating
CPU time among solvers entered in the 2007 SAT solver competition.

1 Introduction

This paper presents an algorithm for solving the following class of online resource allocation prob-
lems. We are given as input a ﬁnite set V of activities. A pair (v  τ) ∈ V × R>0 is called an
action  and represents spending time τ performing activity v. A schedule is a sequence of actions.
We use S to denote the set of all schedules. A job is a function f : S → [0  1]  where for any
schedule S ∈ S  f(S) represents the proportion of some task that is accomplished by performing
the sequence of actions S. We require that a job f have the following properties (here ⊕ is the
concatenation operator):

1. (monotonicity) for any schedules S1  S2 ∈ S  we have f(S1) ≤ f(S1 ⊕ S2) and f(S2) ≤
2. (submodularity) for any schedules S1  S2 ∈ S and any action a ∈ V ×R>0  fa(S1⊕ S2) ≤

f(S1 ⊕ S2)
fa(S1)  where we deﬁne fa(S) ≡ f(S ⊕ (cid:104)a(cid:105)) − f(S).

We will evaluate schedules in terms of two objectives. The ﬁrst objective  which we call beneﬁt-
maximization  is to maximize f (S) subject to the constraint (cid:96) (S) ≤ T   for some ﬁxed T > 0  where
(cid:96) (S) equals the sum of the durations of the actions in S. For example if S = (cid:104)(v1  3)  (v2  3)(cid:105)  then
(cid:96)(S) = 6. The second objective is to minimize the cost of a schedule  which we deﬁne as

(cid:90) ∞

1 − f(cid:0)S(cid:104)t(cid:105)(cid:1) dt

c (f  S) =

t=0

where S(cid:104)t(cid:105) is the schedule that results from truncating schedule S at time t. For example if S =
(cid:104)(v1  3)  (v2  3)(cid:105) then S(cid:104)5(cid:105) = (cid:104)(v1  3)  (v2  2)(cid:105).1 One way to interpret this objective is to imagine
where k is the largest integer such thatPk
1More formally  if S = (cid:104)a1  a2  . . .(cid:105)  where ai = (vi  τi)  then S(cid:104)t(cid:105) = (cid:104)a1  a2  . . .   ak−1  ak  (vk+1  τ(cid:48))(cid:105) 

i=1 τi < t and τ(cid:48) = t −Pk

i=1 τi.

1

t=0

S. For any non-negative random variable X  we have E [X] =(cid:82) ∞

that f(S) is the probability that some desired event occurs as a result of performing the actions in
P [X > t] dt. Thus c (f  S) is
the expected time we must wait before the desired event occurs if we execute actions according to
the schedule S. The following example illustrates these deﬁnitions.
Example 1. Let each activity v represent a randomized algorithm for solving some decision prob-
lem  and let the action (v  τ) represent running the algorithm (with a fresh random seed) for time
τ. Fix some particular instance of the decision problem  and for any schedule S  let f(S) be the
probability that one (or more) of the runs in the sequence S yields a solution to that instance. So
f(S(cid:104)T(cid:105)) is (by deﬁnition) the probability that performing the runs in schedule S yields a solution
to the problem instance in time ≤ T   while c (f  S) is the expected time that elapses before a so-
lution is obtained. It is clear that f(S) is monotone  because adding runs to the sequence S can
only increase the probability that one of the runs is successful. The fact that f is submodular can
be seen as follows. For any schedule S and action a  fa(S) equals the probability that action a
succeeds after every action in S has failed  which can also be written as (1 − f(S)) · f((cid:104)a(cid:105)). This 
together with the monotonicity of f  implies that for any schedules S1  S2 and any action a  we have
fa(S1 ⊕ S2) = (1 − f(S1 ⊕ S2)) · f((cid:104)a(cid:105)) ≤ (1 − f(S1)) · f((cid:104)a(cid:105)) = fa(S1).
In the online setting  an arbitrary sequence (cid:104)f (1)  f (2)  . . .   f (n)(cid:105) of jobs arrive one at a time  and
we must ﬁnish each job (via some schedule) before moving on to the next job. When selecting a
schedule S(i) to use to ﬁnish job f (i)  we have knowledge of the previous jobs f (1)  f (2)  . . .   f (i−1)
but we have no knowledge of f (i) itself or of any subsequent jobs. In this setting we aim to minimize
regret  which measures the difference between the average cost (or average beneﬁt) of the schedules
produced by our online algorithm and that of the best single schedule (in hindsight) for the given
sequence of jobs.

1.1 Problems that ﬁt into this framework

(cid:80)n

(cid:17)
(v τ )∈S (1 − pi(v  τ))

(cid:16)

1 −(cid:81)

i=1

A number of previously-studied problems can be cast as the task of computing a schedule S that
minimizes c (f  S)  where f is of the form f(S) = 1
. This
n
expression can be interpreted as follows: the job f consists of n subtasks  and pi(v  τ) is the prob-
ability that investing time τ in activity v completes the ith subtask. Thus  f(S) is the expected
fraction of subtasks that are ﬁnished after performing the sequence of actions S. Assuming pi(v  τ)
is a non-decreasing function of τ for all i and v  it can be shown that any function f of this form is
monotone and submodular. PIPELINED SET COVER [11  15] can be deﬁned as the special case in
which for each activity v there is an associated time τv  and pi(v  τ) = 1 if τ ≥ τv and pi(v  τ) = 0
otherwise. MIN-SUM SET COVER [7] is the special case in which  additionally  τv = 1 or τv = ∞
for all v ∈ V. The problem of constructing efﬁcient sequences of trials [5] corresponds to the case
in which we are given a matrix q  and pi(v  τ) = qv i if τ ≥ 1 and pi(v  τ) = 0 otherwise.
The problem of maximizing f(S(cid:104)T(cid:105)) is a slight generalization of the problem of maximizing a
monotone submodular set function subject to a knapsack constraint [14  20] (which in turn gener-
alizes BUDGETED MAXIMUM COVERAGE [12]  which generalizes MAX k-COVERAGE [16]). The
only difference between the two problems is that  in the latter problem  f(S) may only depend on
the set of actions in the sequence S  and not on the order in which the actions appear.

1.2 Applications
We now discuss three applications  the ﬁrst of which is the focus of our experiments in §5.
1. Online algorithm portfolio design. An algorithm portfolio [9] is a schedule for interleaving the
execution of multiple (randomized) algorithms and periodically restarting them with a fresh random
seed. Previous work has shown that combining multiple heuristics for NP-hard problems into a port-
folio can dramatically reduce average-case running time [8  9  19]. In particular  algorithms based
on chronological backtracking often exhibit heavy-tailed run length distributions  and periodically
restarting them with a fresh random seed can reduce the mean running time by orders of magnitude
[8]. As illustrated in Example 1  our algorithms can be used to learn an effective algorithm portfolio
online  in the course of solving a sequence of problem instances.

2

2. Database query processing. In database query processing  one must extract all the records in a
database that satisfy every predicate in a list of one or more predicates (the conjunction of predicates
comprises the query). To process the query  each record is evaluated against the predicates one
at a time until the record either fails to satisfy some predicate (in which case it does not match
the query) or all predicates have been examined. The order in which the predicates are examined
affects the time required to process the query. Munagala et al. [15] introduced and studied a problem
called PIPELINED SET COVER (discussed in §1.1)  which entails ﬁnding an evaluation order for the
predicates that minimizes the average time required to process a record. Our work addresses the
online version of this problem  which arises naturally in practice.
3. Sensor placement. Sensor placement is the task of assigning locations to a set of sensors so
as to maximize the value of the information obtained (e.g.  to maximize the number of intrusions
that are detected by the sensors). Many sensor placement problems can be optimally solved by
maximizing a monotone submodular set function subject to a knapsack constraint [13]  a special
case of our beneﬁt-maximization problem (see §1.1). Our online algorithms could be used to select
sensor placements when the same set of sensors is repeatedly deployed in an unknown or adversarial
environment.

1.3 Summary of results

We ﬁrst consider the ofﬂine variant of our problem. As an immediate consequence of existing
results [6  7]  we ﬁnd that  for any  > 0  (i) achieving an approximation ratio of 4 −  for the
cost-minimization problem is NP-hard and (ii) achieving an approximation ratio of 1− 1
e +  for the
beneﬁt-maximization problem is NP-hard. We then present a greedy approximation algorithm that
simultaneously achieves the optimal approximation ratios (of 4 and 1 − 1
e ) for these two problems 
building on and generalizing previous work on special cases of these two problems [7  20].
In the online setting we provide an online algorithm whose worst-case performance guarantees ap-
proach those of the ofﬂine greedy approximation algorithm asymptotically (as the number of jobs
approaches inﬁnity). We then show how to modify our online algorithm for use in several different
“bandit” feedback settings. Finally  we prove information-theoretic lower bounds on regret. We
conclude with an experimental evaluation.

2 Related Work
As discussed in §1.1  the ofﬂine cost-minimization problem considered here generalizes MIN-SUM
SET COVER [7]  PIPELINED SET COVER [11  15]  and the problem of constructing efﬁcient se-
quences of trials [5]. Several of these problems have been considered in the online setting. Mu-
nagala et al. [15] gave an online algorithm for PIPELINED SET COVER that is asymptotically
O (log |V|)-competitive. Babu et al. [3] and Kaplan et al. [11] gave online algorithms for PIPE-
LINED SET COVER that are asymptotically 4-competitive  but only in the special case where the
jobs are drawn independently at random from a ﬁxed probability distribution (whereas our online
algorithm is asymptotically 4-competitive on an arbitrary sequence of jobs).
Our ofﬂine beneﬁt-maximization problem generalizes the problem of maximizing a monotone sub-
modular set function subject to a knapsack constraint. Previous work gave ofﬂine greedy approx-
imation algorithms for this problem [14  20]  which generalized earlier algorithms for BUDGETED
MAXIMUM COVERAGE [12] and MAX k-COVERAGE [16]. To our knowledge  none of these prob-
lems have previously been studied in an online setting. Note that our problem is quite different from
online set covering problems (e.g.  [1]) that require one to construct a single collection of sets that
covers each element in a sequence of elements that arrive online.
In this paper we convert a speciﬁc greedy approximation algorithm into an online algorithm. Re-
cently  Kakade et al. [10] gave a generic procedure for converting an α-approximation algorithm
into an online algorithm that is asymptotically α-competitive. Their algorithm applies to linear
optimization problems  but not to the non-linear problems we consider here.
Independently of us  Radlinkski et al. [17] developed a no-regret algorithm for the online version of
MAX k-COVERAGE  and applied it to online ranking. As it turns out  their algorithm is a special
case of the algorithm OGunit that we present in §4.1.

3

3 Ofﬂine Greedy Algorithm
In the ofﬂine setting  we are given as input a job f : S (cid:31) [0(cid:44) 1]. Our goal is to compute a schedule S
that achieves one of two objectives  either minimizing the cost c (f(cid:44) S) or maximizing f(S) subject
to the constraint (cid:31)(S) (cid:30) T .2 As already mentioned  this ofﬂine problem generalizes MIN-SUM SET
COVER under the former objective and generalizes MAX k-COVERAGE under the latter objective 
which implies the following computational complexity result [6  7].
Theorem 1. For any (cid:30) > 0  achieving a 4 (cid:29)(cid:30)(resp. 1 (cid:29) 1
minimization (resp. beneﬁt-maximization) problem is NP-hard.

e + (cid:30)) approximation ratio for the cost-

(cid:31)f(v(cid:44)(cid:31))(Gj )

(cid:30)(cid:29)sj. We will prove

 

(cid:31)

(cid:31)j

(cid:31)f(v(cid:44)(cid:31))(Gj )

We now consider an arbitrary schedule G  whose jth action is gj = (vj(cid:44) (cid:29)j). Let sj = fgj (Gj )
where Gj = (cid:28)g1(cid:44) g2(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) gj(cid:31)1(cid:27)  and let (cid:30)j = max(v(cid:44)(cid:31))(cid:30)V(cid:215) R>0
(cid:30)
bounds on the performance of G in terms of the (cid:30)j values. Note that we can ensure (cid:30)j = 0 (cid:26)j by
greedily choosing gj = arg max(v(cid:44)(cid:31))(cid:30)V(cid:215) R>0
(i.e.  greedily appending actions to the
schedule so as to maximize the resulting increase in f per unit time). A key property is stated in the
following lemma  which follows from the submodularity assumption (for the proof  see [18]).
Lemma 1. For any schedule S  any positive integer j  and any t > 0  f(S(cid:29)t(cid:28)) (cid:30) f(Gj)+t(cid:183) (sj +(cid:30)j).
Using Lemma 1  together with a geometric proof technique developed in [7]  we now show that the
greedy algorithm achieves the optimal approximation ratio for the cost-minimization problem.
generally  let L be a positive integer  and let T =(cid:29) L
Theorem 2. Let S(cid:27) = arg minS(cid:30)S c (f(cid:44) S). If (cid:30)j = 0 (cid:26)j  then c (f(cid:44) G) (cid:30) 4 (cid:183) c (f(cid:44) S(cid:27)). More
t=0 1 (cid:29)f(cid:27)S(cid:29)t(cid:28)(cid:26)dt. Then cT (f(cid:44) G) (cid:30) 4 (cid:183) c (f(cid:44) S(cid:27)) +(cid:29) L
(cid:28)T
j=1 (cid:29)j. For any schedule S  deﬁne cT (f(cid:44) S) (cid:25)

j=1 Ej(cid:29)j  where Ej =(cid:29)

l<j (cid:30)l(cid:29)l.

(cid:31)

; let yj = Rj

2 ; and let h(x) = 1 (cid:29)f(S(cid:27)

Proof. We consider the special case (cid:30)j = 0 (cid:26)j; for the full proof see [18]. Let Rj = 1 (cid:29)f (Gj);
non-increasing. These facts imply that(cid:28)(cid:26)
let xj = Rj
2 = yj.
2sj
The monotonicity of f implies that h(x) is non-increasing and also that the sequence (cid:28)y1(cid:44) y2(cid:44) (cid:46)(cid:46)(cid:46)(cid:27)is
j(cid:25)1 xj (yj (cid:29)yj+1) (see Figure 1). The
(cid:29)
left hand side equals c (f(cid:44) S(cid:27))  and  using the fact that sj = Rj(cid:31)Rj+1
  the right hand side simpliﬁes
to 1
4

4 c (f(cid:44) G)  proving c (f(cid:44) G) (cid:30) 4 (cid:183) c (f(cid:44) S(cid:27)).

x=0 h(x) dx (cid:24)(cid:29)

(cid:29)x(cid:28)). By Lemma 1  h(xj) (cid:24) Rj (cid:29) Rj

j(cid:25)1 Rj(cid:29)j (cid:24) 1

(cid:31)j

h(x)

y1

y2

y3
y4
y5

x1

x2

x3

x4

Pn

4

2Given a set of jobs (cid:123) f (1)(cid:44) f (2)(cid:44) (cid:46)(cid:46)(cid:46)(cid:44) f (n)(cid:125)

by applying our ofﬂine algorithm to the job f = 1
n

  we can optimize the average schedule cost (or beneﬁt) simply
i=1 f (i) (since any convex combination of jobs is a job).

x5

x

Figure 1: An illustration of the inequality(cid:28)(cid:26)

x=0 h(x) dx (cid:24)(cid:29)
Theorem 3. Let L be a positive integer  and let T = (cid:29) L
(cid:27)1 (cid:29) 1

(cid:26)maxS(cid:30)S(cid:25)f(cid:27)S(cid:29)T(cid:28)(cid:26)(cid:24) (cid:29)(cid:29) L

j=1 (cid:30)j(cid:29)j.

e

The greedy algorithm also achieves the optimal approximation ratio for the beneﬁt-maximization
problem  as can be shown using arguments similar to the ones in [14  20]; see [18] for details.

j(cid:25)1 xj (yj (cid:29)yj+1).

Then f(cid:27)G(cid:29)T(cid:28)(cid:26) >

j=1 (cid:29)j.

4 Online Greedy Algorithm
In the online setting we are fed  one at a time  a sequence (cid:104)f (1)  f (2)  . . .   f (n)(cid:105) of jobs. Prior to
receiving job f (i)  we must specify a schedule S(i). We then receive complete access to the function
f (i).
We measure performance using two different notions of regret. For the cost-minimization objective 
we deﬁne Rcost = 1
n

i=1 c(cid:0)S  f (i)(cid:1)(cid:9)  for some ﬁxed T > 0.
(cid:80)n
t=0 1 − f(cid:0)S(cid:104)t(cid:105)(cid:1) dt to be the value of
Here for any schedule S and job f  we deﬁne cT (S  f) = (cid:82) T
c(cid:0)S(i)  f (i)(cid:1) could be inﬁnite  and without bounding it we could not prove any ﬁnite bound on regret
i=1 f (i)(cid:0)S(cid:104)T(cid:105)(cid:1)(cid:9) − 1
i=1 f (i)(cid:0)S(i)(cid:1). Here we require
deﬁne Rbeneﬁt =(cid:0)1 − 1
(cid:80)n
(cid:80)n
that for each i  E(cid:2)(cid:96)(cid:0)S(i)(cid:1)(cid:3) = T   where the expectation is over the online algorithm’s random bits.

i=1 cT(cid:0)S(i)  f (i)(cid:1)−4·minS∈S(cid:8) 1
(cid:80)n
(cid:1) maxS∈S(cid:8) 1

c (S  f) when the integral is truncated at time T . Some form of truncation is necessary because

(our regret bounds will be stated as a function of T ). For the beneﬁt-maximization objective  we

That is  we allow the online algorithm to treat T as a budget in expectation  rather than a hard budget.
Our goal is to bound the worst-case expected values of Rcost and Rbeneﬁt. For simplicity  we
consider the oblivious adversary model  in which the sequence of jobs is ﬁxed in advance and does
not change in response to the decisions made by our online algorithm. We conﬁne our attention to
schedules that consist of actions that come from some ﬁnite set A  and assume that the actions in A
have integer durations (i.e. A ⊆ V × Z>0).

n

n

n

e

a

.

(cid:16)

O

T

and E [Rcost] =

(cid:16)
(cid:16)(cid:113) T
n ln|A|(cid:17)

4.1 Unit-cost actions
In the special case in which each action takes unit time (i.e.  A ⊆ V × {1})  our online algorithm
OGunit is very simple. OGunit runs T action-selection algorithms  E1 E2  . . .  ET   where T is
the number of time steps for which our schedule is deﬁned. The intent is that each action-selection
algorithm is a no-regret algorithm such as randomized weighted majority (WMR) [4]  which selects
actions so as to maximize payoffs associated with the actions. Just before job f (i) arrives  each
action-selection algorithm Et selects an action ai
t. The schedule used by OGunit on job f (i) is
S(i) = (cid:104)ai

T(cid:105). The payoff that Et associates with action a is f (i)

2  . . .   ai

1  ai

(cid:17)

S(i)(cid:104)t−1(cid:105)

in the worst case  when WMR [4] is the subroutine action-selection algorithm.

Theorem 4. Algorithm OGunit has E [Rbeneﬁt] = O

(cid:113) T
n ln|A|(cid:17)
(cid:80)n
Proof. We will view OGunit as producing an approximate version of the ofﬂine greedy schedule for
i=1 f (i). First  view the sequence of actions selected by Et as a single meta-action
the job f = 1
n
˜at  and extend the domain of each f (i) to include the meta-actions by deﬁning f (i)(S ⊕ (cid:104)˜at(cid:105)) =
t(cid:105)) for all S ∈ S (note each f (i) remains monotone and submodular). Thus  the online
f (i)(S ⊕ (cid:104)ai
(cid:17)
algorithm produces a single schedule ˜S = (cid:104)˜a1  ˜a2  . . .   ˜aT(cid:105) for all i. Let rt be the regret experienced
by action-selection algorithm Et. By construction  rt = maxa∈A
.
Thus OGunit behaves exactly like the greedy schedule G for the function f  with t = rt. Thus 
t=1 rt ≡ R. Similarly  Theorem 2 implies that Rcost ≤ T R.
To complete the analysis  it remains to bound E [R]. WMR has worst-case expected regret
  where Gmax is the maximum sum of payoffs payoff for any single ac-
O
tion.3 Because each payoff is at most 1 and there are n rounds  Gmax ≤ n  so a trivial bound is
E [R] = O
algorithms  leading to an improved bound of E [R] = O
completes the proof.

. In fact  the worst case is when Gmax = Θ(cid:0) n
(cid:16)(cid:113) T
n ln|A|(cid:17)

Theorem 3 implies that Rbeneﬁt ≤(cid:80)T
(cid:16) 1

(cid:112)Gmax ln|A|(cid:17)
(cid:113) 1
(cid:16)
n ln|A|(cid:17)

(cid:1) for all T action-selection

T
(for details see [18])  which

(cid:17)(cid:111) − f˜at

(cid:16) ˜S(cid:104)t−1(cid:105)

(cid:16) ˜S(cid:104)t−1(cid:105)

(cid:110)

fa

n

T

3This bound requires Gmax to be known in advance; however  the same guarantee can be achieved by

guessing a value of Gmax and doubling the guess whenever it is proven wrong.

5

4.2 From unit-cost actions to arbitrary actions

t

t(cid:105)) if ai

τ . Let S(i)

In this section we generalize the online greedy algorithm presented in the previous section to accom-
modate actions with arbitrary durations. Like OGunit  our generalized algorithm OG makes use
of a series of action-selection algorithms E1 E2  . . .  EL (for L to be determined). On each round
i  OG constructs a schedule S(i) as follows: for t = 1  2  . . .   L  it uses Et to choose an action
t = (v  τ) ∈ A  and appends this action to S(i) with probability 1
denote the schedule
ai
that results from the ﬁrst t steps of this process (so S(i)
contains between 0 and t actions). The
payoff that Et associates with an action a = (v  τ) equals 1
τ fa(S(i)
t−1) (i.e.  the increase in f per unit
time that would have resulted from appending a to the schedule-under-construction).
As in the previous section  we view each action-selection algorithm Et as selecting a single meta-
action ˜at. We extend the domain of each f (i) to include the meta-actions by deﬁning f (i)(S⊕(cid:104)˜at(cid:105)) =
f (i)(S ⊕(cid:104)ai
t was appended to S(i)  and f (i)(S ⊕(cid:104)˜at(cid:105)) = f (i)(S) otherwise. Thus  the online
algorithm produces a single schedule ˜S = (cid:104)˜a1  ˜a2  . . .   ˜aL(cid:105) for all i. Note that each f (i) remains
monotone and submodular.
For the purposes of analysis  we will imagine that each meta-action ˜at always takes unit time
(whereas in fact  ˜at takes unit time per job in expectation). We show later that this assumption
(cid:80)n
does not invalidate any of our arguments.
i=1 f (i)  and let ˜St = (cid:104)˜a1  ˜a2  . . .   ˜at(cid:105). Thus ˜S can be viewed as a version of the
Let f = 1
n
greedy schedule from §3  with t = max(v τ )∈A
  where we
are using the assumption that ˜at takes unit time. Let rt be the regret experienced by Et. Although
rt (cid:54)= t in general  the two quantities are equal in expectation (proof omitted).
Lemma 2. E [t] = E [rt].

(cid:17)(cid:111) −(cid:16)

f(v τ )( ˜St−1)

f˜at( ˜St−1)

(cid:110) 1

(cid:17)

t

(cid:16)

τ

t=1 rt.

We now prove a bound on E [Rbeneﬁt]. Because each f (i) is monotone and submodular  f is mono-
tone and submodular as well  so the greedy schedule’s approximation guarantees apply to f. In
t=1 t. Thus by Lemma 2  E [Rbeneﬁt] ≤ E [R] 

particular  by Theorem 3  we have Rbeneﬁt ≤ (cid:80)T
where R =(cid:80)T
a constraint violation of the form E(cid:2)(cid:96)(cid:0)S(i)(cid:1)(cid:3) (cid:54)= T . But by construction  E(cid:2)(cid:96)(cid:0)S(i)(cid:1)(cid:3) = L for each

To bound E [Rbeneﬁt]  it remains to justify the assumption that each meta-action ˜at always takes unit
time. First  note that the value of the objective function f( ˜S) is independent of how long each meta-
action ˜at takes. Thus  the only potential danger is that in making this assumption we have overlooked

i  regardless of what actions are chosen by each action-selection algorithm. Thus if we set L = T
there is no constraint violation. Combining the bound on E [R] stated in the proof of Theorem 4
with the fact that E [Rbeneﬁt] ≤ E [R] yields the following theorem.
Theorem 5. Algorithm OG  run with input L = T   has E [Rbeneﬁt] ≤ E [R]. If WMR [4] is used
as the subroutine action-selection algorithm  then E [R] = O

(cid:16)(cid:113) T
n ln|A|(cid:17)

.

The argument bounding E [Rcost] is similar  although somewhat more involved (for details  see [18]).

One additional complication is that (cid:96)(cid:0)S(i)(cid:1) is now a random variable  whereas in the deﬁnition of
probability that (cid:96)(cid:0)S(i)(cid:1) < T sufﬁciently small  which can be done by setting L (cid:29) T and applying

Rcost the cost of a schedule is always calculated up to time T . This can be addressed by making the

concentration of measure inequalities. However  E [R] grows as a function of L  so we do not want
to make L too large. The (approximately) best bound is obtained by setting L = T ln n.
Theorem 6. Algorithm OG  run with input L = T ln n  has E [Rcost] = O(T ln n · E [R] + T√
In particular  E [Rcost] = O
selection algorithm.

n).
if WMR [4] is used as the subroutine action-

(cid:113) T
n ln|A|(cid:17)

(ln n) 3

(cid:16)

2 T

6

4.3 Dealing with limited feedback

Thus far we have assumed that  after specifying a schedule S(i)  the online algorithm receives com-
plete access to the job f (i). We now consider three more limited feedback settings that may arise
in practice. In the priced feedback model  to receive access to f (i) we must pay a price C  which
for

is added to our regret. In the partially transparent feedback model  we only observe f (i)(cid:16)
each t > 0. In the opaque feedback model  we only observe f (i)(cid:0)S(i)(cid:1).

S(i)(cid:104)t(cid:105)

(cid:17)

The priced and partially transparent feedback models arise naturally in the case where action (v  τ)
represents running a deterministic algorithm v for τ time units  and f(S) = 1 if some action in S
yields a solution to some particular problem instance  and f(S) = 0 otherwise. If we execute a
schedule S and halt as soon as some action yields a solution  we obtain exactly the information that
is revealed in the partially transparent model. Alternatively  running each algorithm v until it returns
a solution would completely reveal the function f (i)  but incurs a computational cost  as reﬂected in
the priced feedback model.
Algorithm OG can be adapted to work in each of these three feedback settings; see [18] for the
speciﬁc bounds. In all cases  the high-level idea is to replace the unknown quantities used by OG
with (unbiased) estimates of those quantities. This technique has been used in a number of online
algorithms (e.g.  see [2]).

4.4 Lower bounds on regret

We now state lower bounds on regret; for the proofs see the full paper [18]. Our proofs have the
same high-level structure as that of the lower bound given in [4]  in that we deﬁne a distribution
over jobs that allows any online algorithm’s expected performance to be easily bounded  and then
prove a bound on the expected performance of the best schedule in hindsight. The upper bounds in
Theorem 4 match the lower bounds in Theorem 7 up to logarithmic factors  although the latter apply
to standard regret as opposed to Rbeneﬁt and Rcost (which include factors of 1 − 1
Theorem 7. Let X =
(resp. Ω (T X)) for the online beneﬁt-maximization (resp. cost-minimization) problem.

T . Then any online algorithm has worst-case expected regret Ω (X)

n ln |V|

e and 4).

(cid:113)

T

5 Experimental Evaluation on SAT 2007 Competition Data

The annual SAT solver competition (www.satcompetition.org) is designed to encourage the
development of efﬁcient Boolean satisﬁability solvers  which are used as subroutines in state-of-
the-art model checkers  theorem provers  and planners. The competition consists of running each
submitted solver on a number of benchmark instances  with a per-instance time limit. Solvers are
ranked according to the instances they solve within each of three instance categories: industrial 
random  and hand-crafted.
We evaluated the online algorithm OG by using it to combine solvers from the 2007 SAT solver
competition. To do so  we used data available on the competition web site to construct a matrix
X  where Xi j is the time that the jth solver required on the ith benchmark instance. We used this
data to determine whether or not a given schedule would solve an instance within the time limit
T (schedule S solves instance i if and only if  for some j  S(cid:104)T(cid:105) contains an action (hj  τ) with
τ ≥ Xi j). As illustrated in Example 1  the task of maximizing the number of instances solved
within the time limit  in an online setting in which a sequence of instances must be solved one at a
time  is an instance of our online problem (under the beneﬁt-maximization objective).
Within each instance category  we compared OG to the ofﬂine greedy schedule  to the individual
solver that solved the most instances within the time limit  and to a schedule that ran each solver
in parallel at equal strength. For these experiments  we ran OG in the full-information feedback
model  after ﬁnding that the number of benchmark instances was too small for OG to be effective
in the limited feedback models. Table 1 summarizes the results. In each category  the ofﬂine greedy
schedule and the online greedy algorithm outperform all solvers entered in the competition as well
as the na¨ıve parallel schedule.

7

Table 1: Number of benchmark instances solved within the time limit.

Category

Industrial
Random
Hand-crafted

Ofﬂine Online Parallel
greedy
schedule
132
147
302
350
114
95

greedy
149
347
107

Top
solver
139
257
98

References
[1] Noga Alon  Baruch Awerbuch  and Yossi Azar. The online set cover problem. In Proceedings

of the 35th STOC  pages 100–105  2003.

[2] Peter Auer  Nicol`o Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic

multiarmed bandit problem. SIAM Journal on Computing  32(1):48–77  2002.

[3] Shivnath Babu  Rajeev Motwani  Kamesh Munagala  Itaru Nishizawa  and Jennifer Widom.
Adaptive ordering of pipelined stream ﬁlters. In Proc. Intl. Conf. on Management of Data 
pages 407–418  2004.

[4] Nicol`o Cesa-Bianchi  Yoav Freund  David Haussler  David Helmbold  Robert Schapire  and

Manfred Warmuth. How to use expert advice. Journal of the ACM  44(3):427–485  1997.

[5] Edith Cohen  Amos Fiat  and Haim Kaplan. Efﬁcient sequences of trials. In Proceedings of

the 14th SODA  pages 737–746  2003.

[6] Uriel Feige. A threshold of ln n for approximating set cover. Journal of the ACM  45(4):634–

652  1998.

[7] Uriel Feige  L´aszl´o Lov´asz  and Prasad Tetali. Approximating min sum set cover. Algorith-

mica  40(4):219–234  2004.

[8] Carla P. Gomes and Bart Selman. Algorithm portfolios. Artiﬁcial Intelligence  126:43–62 

2001.

[9] Bernardo A. Huberman  Rajan M. Lukose  and Tad Hogg. An economics approach to hard

computational problems. Science  275:51–54  1997.

[10] Sham Kakade  Adam Kalai  and Katrina Ligett. Playing games with approximation algorithms.

In Proceedings of the 39th STOC  pages 546–555  2007.

[11] Haim Kaplan  Eyal Kushilevitz  and Yishay Mansour. Learning with attribute costs. In Pro-

ceedings of the 37th STOC  pages 356–365  2005.

[12] Samir Khuller  Anna Moss  and Joseph (Sefﬁ) Naor. The budgeted maximum coverage prob-

lem. Information Processing Letters  70(1):39–45  1999.

[13] Andreas Krause and Carlos Guestrin. Near-optimal nonmyopic value of information in graph-

ical models. In Proceedings of the 21st UAI  pages 324–331  2005.

[14] Andreas Krause and Carlos Guestrin. A note on the budgeted maximization of submodular

functions. Technical Report CMU-CALD-05-103  Carnegie Mellon University  2005.

[15] Kamesh Munagala  Shivnath Babu  Rajeev Motwani  Jennifer Widom  and Eiter Thomas. The

pipelined set cover problem. In Proc. Intl. Conf. on Database Theory  pages 83–98  2005.

[16] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maxi-

mizing submodular set functions. Mathematical Programming  14(1):265–294  1978.

[17] Filip Radlinski  Robert Kleinberg  and Thorsten Joachims. Learning diverse rankings with

multi-armed bandits. In Proceedings of the 25th ICML  pages 784–791  2008.

[18] Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular func-

tions. Technical Report CMU-CS-07-171  Carnegie Mellon University  2007.

[19] Matthew Streeter  Daniel Golovin  and Stephen F. Smith. Combining multiple heuristics on-

line. In Proceedings of the 22nd AAAI  pages 1197–1203  2007.

[20] Maxim Sviridenko. A note on maximizing a submodular set function subject to a knapsack

constraint. Operations Research Letters  32:41–43  2004.

8

,Kamalika Chaudhuri
Daniel Hsu
Shuang Song
James McInerney
Rajesh Ranganath
David Blei