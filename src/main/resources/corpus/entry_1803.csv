2010,Empirical Bernstein Inequalities for U-Statistics,We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernstein-type inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities  as it reduces to them when U-statistics of order 1 are considered. In addition  it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities  especially in the realm of learning ranking/scoring functions. In the process  we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.,Empirical Bernstein Inequalities for U-Statistics

Thomas Peel

LIF  Aix-Marseille Universit´e

39  rue F. Joliot Curie

F-13013 Marseille  France

Sandrine Anthoine

LATP  Aix-Marseille Universit´e  CNRS

thomas.peel@lif.univ-mrs.fr

anthoine@cmi.univ-mrs.fr

39  rue F. Joliot Curie

F-13013 Marseille  France

Liva Ralaivola

LIF  Aix-Marseille Universit´e

39  rue F. Joliot Curie

F-13013 Marseille  France

liva.ralaivola@lif.univ-mrs.fr

Abstract

We present original empirical Bernstein inequalities for U-statistics with bounded
symmetric kernels q. They are expressed with respect to empirical estimates of
either the variance of q or the conditional variance that appears in the Bernstein-
type inequality for U-statistics derived by Arcones [2]. Our result subsumes other
existing empirical Bernstein inequalities  as it reduces to them when U-statistics
of order 1 are considered. In addition  it is based on a rather direct argument using
two applications of the same (non-empirical) Bernstein inequality for U-statistics.
We discuss potential applications of our new inequalities  especially in the realm
of learning ranking/scoring functions. In the process  we exhibit an efﬁcient pro-
cedure to compute the variance estimates for the special case of bipartite ranking
that rests on a sorting argument. We also argue that our results may provide test set
bounds and particularly interesting empirical racing algorithms for the problem of
online learning of scoring functions.

1

Introduction

The motivation of the present work lies in the growing interest of the machine learning commu-
nity for learning tasks that are richer than now well-studied classiﬁcation and regression. Among
those  we especially have in mind the task of ranking  where one is interested in learning a ranking
function capable of predicting an accurate ordering of objects according to some attached relevance
information. Tackling such problems generally implies the use of loss functions other than the 0-1
misclassiﬁcation loss such as  for example  a misranking loss [6] or a surrogate thereof. For (x  y)
and (x(cid:48)  y(cid:48)) two pairs from some space Z := X × Y (e.g.  X = Rd and Y = R) the misranking loss
(cid:96)rank and a surrogate convex loss (cid:96)sur may be deﬁned for a scoring function f ∈ YX as:

(cid:96)rank(f  (x  y)  (x(cid:48)  y(cid:48))) := 1{(y−y(cid:48))(f (x)−f (x(cid:48)))<0} 
(cid:96)sur(f  (x  y)  (x(cid:48)  y(cid:48))) := (1 − (y − y(cid:48))(f (x) − f (x(cid:48)))2 .

(1)
(2)
Given such losses or  more generally  a loss (cid:96) : YX × Z × Z → R  and a training sample Z n =
{(Xi  Yi)}n
i=1 of independent copies of some random variable Z := (X  Y ) distributed according
to D  the learning task is to derive a function f ∈ X Y such that the expected risk R(cid:96)(f ) of f

R(cid:96)(f ) := EZ Z(cid:48)∼D(cid:96)(f  Z  Z(cid:48)) = EZ Z(cid:48)∼D(cid:96)(f  (X  Y )  (X(cid:48)  Y (cid:48)))

1

is as small as possible. In practice  this naturally brings up the empirical estimate ˆR(cid:96)(f  Z n)

ˆR(cid:96)(f  Z n) :=

1

n(n − 1)

(cid:96)(f  (Xi  Yi)  (Xj  Yj)) 

(3)

(cid:88)

i(cid:54)=j

which is a U-statistic [6  10].
An important question is to precisely characterize how ˆR(cid:96)(f  Z n) is related to R(cid:96)(f ) and  more
speciﬁcally  one may want to derive an upper bound on R(cid:96)(f ) that is expressed in terms of ˆR(cid:96)(f  Z n)
and other quantities such as a measure of the capacity of the class of functions f belongs to and the
size n of Z n – in other words  we may talk about generalization bounds [4]. Pivotal tools to perform
such analysis are tail/concentration inequalities  which say how probable it is for a function of
several independent variables to deviate from its expectation; of course  the sharper the concentration
inequalities the more accurate the characterization of the relation between the empirical estimate and
its expectation. It is therefore of the utmost importance to have at hand tail inequalities that are sharp;
it is just as important that these inequalities rely as much as possible on empirical quantities.
Here  we propose new empirical Bernstein inequalities for U-statistics. As indicated by the name
(i) our results are Bernstein-type inequalities and therefore make use of information on the variance
of the variables under consideration  (ii) instead of resting on some assumed knowledge about this
variance  they only rely on empirical related quantities and (iii) they apply to U-statistics. Our new
inequalities generalize those of [3] and [13]  which also feature points (i) and (ii) (but not (iii)) 
while based on simple arguments. To the best of our knowledge  these are the ﬁrst results that fulﬁll
(i)  (ii) and (iii); they may give rise to a few applications  of which we describe two in the sequel.
The paper is organized as follows. Section 2 introduces the notations and brieﬂy recalls the basics of
U-statistics as well as tail inequalities our results are based upon. Our empirical Bernstein inequali-
ties are presented in Section 3; we also provide an efﬁcient way of computing the empirical variance
when the U-statistics considered are based on the misranking loss (cid:96)rank of (1). Section 4 discusses
two applications of our new results: test set bounds for bipartite ranking and online ranking.

2 Background

2.1 Notation

The following notation will hold from here on. Z is a random variable of distribution D taking
values in Z := X ×Y; Z(cid:48)  Z1  . . .   Zn are independent copies of Z and Z n := {Zi = (Xi  Yi)}n
and Z p:q := {Zi}q
i=1
i=p.
Am
n denotes the set Am
Finally  a function q : Z m → R is said to be symmetric if the value of q(z) = q(z1  . . .   zm) is
independent of the order of the zi’s in z.

n := {(i1  . . .   im) : 1 ≤ i1 (cid:54)= . . . (cid:54)= im ≤ n}   with 0 ≤ m ≤ n.

2.2 U-statistics and Tail Inequalities

q(Z1  . . .   Zm) = EZn

is a U-statistic of order m with kernel q  when q : Z m → R is a measurable function on Z m.
ˆUq(Z n) is a lowest
Remark 1. Obviously  EZm
variance estimate of EZm
q(Z1  . . .   Zm) based on Z n [10]. Also  reusing some notation from the
introduction  ˆR(cid:96)(f  Z n) of Eq. (3) is a U-statistic of order 2 with kernel qf (Z  Z(cid:48)) := (cid:96)(f  Z  Z(cid:48)).
Remark 2. Two peculiarities of U-statistics that entail a special care are the following: (i) they are
sums of identically distributed but dependent variables: special tools need be resorted to in order
to deal with these dependencies to characterize the deviation of ˆUq(Z n) from Eq  and (ii) from an
algorithmic point of view  their direct computations may be expensive  as it scales as O(nm); in
Section 3  we show for the special case of bipartite ranking how this complexity can be reduced.

ˆUq(Z n); in addition  EZn

Deﬁnition 1 (U-statistic  Hoeffding [10]). The random variable ˆUq(Z n) deﬁned as

ˆUq(Z n) :=

1
|Am
n |

q(Zi1  . . .   Zim) 

(cid:88)

i∈Am

n

2

Figure 1: First two plots: values of the right-hand size of (5) and (6)  for Duni and kernel qm for
m = 2 and m = 10 (see Example 1) as functions of n. Last two plots: same for DBer(0.15).

We now recall three tail inequalities (Eq. (5)  (6)  (7)) that hold for U-statistics with symmetric and
bounded kernels q. Normally  these inequalities make explicit use of the length qmax − qmin of the
range [qmin  qmax] of q. To simplify the reading  we will consider without loss of generality that q
has range [0  1] (an easy way of retrieving the results for bounded q is to consider q/(cid:107)q(cid:107)∞).
One key quantity that appears in the original versions of tail inequalities (5) and (6) below is (cid:98)n/m(cid:99) 
the integer part of the ratio n/m – this quantity might be thought of as the effective number of data.
To simplify the notation  we will assume that n is a multiple of m and  therefore  (cid:98)n/m(cid:99) = (n/m).
Theorem 1 (First order tail inequality for ˆUq  [11].). Hoeffding proved the following:

∀ε > 0  PZn

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

n

Hence ∀δ ∈ (0  1]  with probability at least 1 − δ over the random draw of Z n:

(cid:12)(cid:12)(cid:12) ≥ ε
(cid:111) ≤ 2 exp(cid:8)−(n/m)ε2(cid:9)  
(cid:115)
(cid:12)(cid:12)(cid:12) ≤

ln

1

.

(n/m)

2
δ

(cid:110)(cid:12)(cid:12)(cid:12)EZ(cid:48)
(cid:12)(cid:12)(cid:12)EZ(cid:48)

n

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

(4)

(5)

To go from the tail inequality (4) to the bound version (5)  it sufﬁces to make use of the elementary
inequality reversal lemma (Lemma 1) provided in section 3  used also for the bounds given below.
Theorem 2 (Bernstein Inequalities for ˆUq  [2  11]). Hoeffding [11] and  later  Arcones [2] reﬁned
the previous result in the form of Bernstein-type inequalities of the form

(cid:12)(cid:12)(cid:12) ≥ ε

(cid:111) ≤ a exp

(cid:26)

(cid:27)

 

− (n/m)ε2
2ϑq m + bmε

(cid:110)(cid:12)(cid:12)(cid:12)EZ(cid:48)

n

∀ε > 0  PZn

ˆUq(Z(cid:48)

For Hoeffding  a = 2  ϑq m = Σ2
Hence  ∀δ ∈ (0  1]  with probability at least 1 − δ:

q where  Σ2

q is the variance of q(Z1  . . .   Zm) and bm = 2/3.

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

2Σ2
q

(n/m)

ln

2
δ

+

2

3(n/m)

ln

2
δ

.

(6)

For Arcones  a = 4  ϑq m = mσ2
q is the variance of EZ2 ... Zmq(Z1  Z2  . . .   Zm) (this is
a function of Z1) and bm = 2m+3mm−1 + (2/3)m−2. ∀δ ∈ (0  1]  with probability at least 1 − δ:

q where σ2

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

n

2mσ2
q
(n/m)

ln

4
δ

+

bm

(n/m)

ln

4
δ

.

(7)

n) − ˆUq(Z n)
(cid:115)

(cid:12)(cid:12)(cid:12) ≤
(cid:12)(cid:12)(cid:12) ≤

(cid:115)

n

(cid:12)(cid:12)(cid:12)EZ(cid:48)
(cid:12)(cid:12)(cid:12)EZ(cid:48)

ˆUq(Z n).

q(Z m) = EZn

With a slight abuse  we will now refer to Eq. (5)  (6) and (7) as tail inequalities. In essence  these
are conﬁdence intervals at level 1 − δ for EZm
Remark 3. Eq. (7) is based on the so-called Hoeffding decomposition of U-statistics [11]. It provides
a more accurate Bernstein-type inequality than that of Eq. (6)  as mσ2
q is known to be smaller than
q (see [16]). However  for moderate values of n/m (e.g. n/m < 105) and reasonable values of δ
Σ2
(e.g. δ = 0.05)  the inﬂuence of the log terms might be such that the advantage of (7) over (6) goes
unnoticed. Thus  we detail our results focusing on an empirical version of (6).
Example 1. To illustrate how the use of the variance information provides smaller conﬁdence inter-
i=1 zi and two distributions Duni and DBer(p). Duni is the uniform
4m . DBer(p) is the Bernoulli distribution with parameter
distribution on [0  1]  for which Σ2 = 1
p ∈ [0  1]  for which Σ2 = pm(1 − pm). Figure 1 shows the behaviors of (6) and (5) for various
values of m as functions of n. Observe that the variance information renders the bound smaller.

vals  consider the kernel qm :=(cid:81)m

3m − 1

3

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9101102103m=2εBernsteinεHoeffding 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9102103104m=10εBernsteinεHoeffding 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9101102103m=2εBernsteinεHoeffding 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9101102103m=10εBernsteinεHoeffding3 Main Results

This section presents the main results of the paper. We ﬁrst introduce the inequality reversal lemma 
which allows to transform tail inequalities into upper bounds (or conﬁdence intervals)  as in (5)-(7).
Lemma 1 (Inequality Reversal lemma). Let X be a random variable and a  b > 0  c  d≥ 0 such that

(cid:26)

∀ε > 0  PX (|X| ≥ ε) ≤ a exp

− bε2
c + dε

(cid:114) c
(cid:114)

b

+

(cid:18)

then  with probability at least 1 − δ

|X| ≤

ln

a
δ

+

d
b

ln

a
δ

.

Proof. Solving for ε such that the right hand side of (8) is equal to δ gives:

ε =

1
2b

d ln

a
δ

d2 ln2 a
δ

+ 4bc ln

a
δ

√

a + b ≤ √

a +

√

Using

b gives an upper bound on ε and provides the result.

3.1 Empirical Bernstein Inequalities

 

(8)

(9)

(cid:27)

(cid:19)

.

Let us now deﬁne the empirical variances we will use in our main result.
Deﬁnition 2. Let ˆΣ2

q be the U-statistic of order 2m deﬁned as:

(cid:0)q(Zi1  . . .   Zim ) − q(Zim+1   . . .   Zi2m )(cid:1)2

 

(10)

ˆΣ2

q(Z n) :=

1
|A2m
n |

and ˆσ2

q be the U-statistic of order 2m − 1 deﬁned as:

ˆσ2
q (Z n) :=

1

|A2m−1

n

|

q(Zi1  Zi2  . . .   Zim)q(Zi1   Zim+1   . . .   Zi2m−1) 

(11)

(cid:88)

i∈A2m

n

(cid:88)

i∈A2m−1

n

It is straightforward to see that (cf. the deﬁnitions of Σ2

q in (6) and σ2

q in (7))

EZn

ˆΣ2

q(Z n) = Σ2
q 

and EZn

ˆσ2
q (Z n) = σ2

q + E2

Zm

q(Z1  . . .   Zm).

We have the following main result.
Theorem 3 (Empirical Bernstein Inequalities/Bounds). With probability at least 1 − δ over Z n 

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

+
And  also  with probability at least 1 − δ  (bm is the same as in (7))
√

ln

n

2 ˆΣ2
q
(n/m)

4
δ

5

(n/m)

ln

4
δ

.

ˆUq(Z(cid:48)

n) − ˆUq(Z n)

2mˆσ2
q
(n/m)

ln

+

8
δ

5

m + bm
(n/m)

ln

8
δ

.

(12)

(13)

(cid:115)

(cid:12)(cid:12)(cid:12) ≤
(cid:115)
(cid:12)(cid:12)(cid:12) ≤

(cid:12)(cid:12)(cid:12)EZ(cid:48)
(cid:12)(cid:12)(cid:12)EZ(cid:48)

n

Proof. We provide the proof of (12) for the upper bound of the conﬁdence interval; the same rea-
soning carries over to prove the lower bound. The proof of (13) is very similar.
First  let us call Q the kernel of ˆΣ2
q:

Q(Z1  . . .   Z2m) := (q(Z1  . . .   Zm) − q(Zm+1  . . .   Z2m))2 .

4

Q is of order 2m  has range [0  1] but it is not necessarily symmetric. An equivalent symmetric
kernel for ˆΣ2

q is Qsym:

(cid:0)q(Zω(1)  . . .   Zω(m)) − q(Zω(m+1)  . . .   Zω(2m))(cid:1)2

Qsym(Z1  . . .   Z2m) :=

1

(2m)!

(cid:88)

where Pm is the set of all the permutations over {1  . . .   m}. This kernel is symmetric (and has
range [0  1]) and Theorem 2 can be applied to bound Σ2 as follows: with prob. at least 1 − δ

ω∈P2m

Σ2 = EZ(cid:48)

2m

Qsym(Z(cid:48)

2m) = EZ(cid:48)

n

q(Z(cid:48)
ˆΣ2

n) ≤ ˆΣ2

q(Z n) +

2V(Qsym)
(n/2m)

ln

2
δ

+

2

3(n/2m)

ln

2
δ

 

(cid:115)

(cid:115)

where V(Qsym) is the variance of Qsym. As Qsym has range [0  1] 

V(Qsym) = EQ2

sym − E2Qsym ≤ EQ2

sym ≤ EQsym = Σ2 

and therefore

Σ2 ≤ ˆΣ2

q(Z n) +

4Σ2

ln

+

2
δ

4

ln

2
δ

.

(To establish (13) we additionally use ˆσ2

Following the approach of [13]  we introduce

3(n/m)

(n/m)
(cid:16)√
(cid:17)2
Σ2 −(cid:112)(m/n) ln(2/δ)
q (Z n) ≥ σ2
q).
(cid:17)2 ≤ ˆΣ2
Σ2 −(cid:112)(m/n) ln(2/δ)
(cid:115)

3(n/m)
√

q(Z n) +

7

and taking the square root of both side  using 1 +(cid:112)7/3 < 3 and

(cid:16)√

ln

a + b ≤ √

2
δ

 

Σ2 ≤(cid:113)

√

ˆΣ2

q(Z n) + 3

1

(n/m)

ln

2
δ

.

and we get

√

b again gives

a +

n

ˆUq(Z(cid:48)

n)− ˆUq(Z n)|  and plug in the latter equation  adjusting
We now apply Theorem 2 to bound |EZ(cid:48)
δ to δ/2 so the obtained inequality still holds with probability 1− δ. Bounding appropriate constants
gives the desired result.
Remark 4. In addition to providing an empirical Bernstein bound for U-statistics based on arbitrary
bounded kernels  our result differs from that of Maurer and Pontil [13] by the way we derive it. Here 
we apply the same tail inequality twice  taking advantage of the fact that estimates for the variances
we are interested in are also U-statistics. Maurer and Pontil use a tail inequality on self bounded
random variables and do not explicitly take advantage of the estimates they use being U-statistics.

3.2 Efﬁcient Computation of the Variance Estimate for Bipartite Ranking

We have just showed how empirical Bernstein inequalities can be derived for U-statistics. The
estimates that enter into play in the presented results are U-statistics with kernels of order 2m (or
2m − 1)  meaning that a direct approach to practically compute them would scale as O(n2m) (or
O(n2m−1)). This scaling might be prohibitive as soon as n gets large.
Here  we propose an efﬁcient way of evaluating the estimate ˆΣ2
q) in the special case where Y = {−1  +1} and the kernel qf induces the misranking loss (1):
ˆσ2

q (a similar reasoning carries over for

qf ((x  y)  (x(cid:48)  y(cid:48))) := 1{(y−y(cid:48))(f (x)−f (x(cid:48)))<0}  ∀f ∈ RX  

which is a symmetric kernel of order m = 2 with range [0  1]. In other words  we address the
bipartite ranking problem. We have the following result.
Proposition 1 (Efﬁcient computation of ˆΣ2

qf ). ∀n  the computation of

ˆΣqf (zn) =

1
|A4
n|

1{(yi1−yi2 )(f (xi1 )−f (xi2 ))<0} − 1{(yi3−yi4 )(f (xi3 )−f (xi4 ))<0}

(cid:17)2

(cid:88)

(cid:16)

i∈A4

n

can be performed in O(n ln n).

5

Proof. We simply provide an algorithmic way to compute ˆΣ2
qf
replace i1  i2  i3  i4 by i  j  k  l  respectively. We also drop the normalization factor |A4
the use of ∝ instead of = in the ﬁrst line below). We have

(zn). To simplify the reading  we
n|−1 (hence
f (zk  zl)(cid:1)  

f (zi  zj) − 2qf (zi  zj)qf (zk  zl) + q2

qf (zn) ∝ (cid:88)

(qf (zi  zj) − qf (zk  zl))2 =

(cid:0)q2

ˆΣ2

qf (zi  zj) − 2

i j

i j k l

i(cid:54)=j(cid:54)=k(cid:54)=l

qf (zi  zj)qf (zk  zl)

since

f =qf

qf (z z)=0

.

(cid:16) q2

(cid:17)

i j k l

i(cid:54)=j(cid:54)=k(cid:54)=l

= 2(n − 2)(n − 3)

(cid:88)

(cid:88)
(cid:88)

i j k l

i(cid:54)=j(cid:54)=k(cid:54)=l



(14)

(15)

(cid:33)2

(cid:88)

i(cid:54)=j(cid:54)=k

The ﬁrst term of the last line is proportional to the well-known Wilcoxon-Mann-Whitney statistic
[9]. There exist efﬁcient ways (O(n ln n)) to compute it  based on sorting the values of the f (xi)’s.
We show how to deal with the second term  using sorting arguments as well. Note that

i j k l

i(cid:54)=j(cid:54)=k(cid:54)=l

(cid:88)

qf (zi  zj)qf (zk  zl) =

(cid:32)(cid:88)
We have subtracted from the square of(cid:80)
f = qf )  deﬁning R(zn) :=(cid:80)

i j

i j qf (zi  zj) all the products qf (zi  zj)qf (zk  zl) such that
exactly one of the variables appears both in qf (zi  zj) and qf (zk  zl)  which happens when i = k 
i = l  j = k  j = l; using the symmetry of qf then provides the second term (together with the factor
4). We also have subtracted all the products qf (zi  zj)qf (zk  zl) where i = k and j = l or i = l and
f (zi  zj) (hence the factor 2) – this gives the last term.
j = k  in which case the product reduces to q2
Thus (using q2
ij qf (zi  zj) and doing some simple calculations:

qf (zi  zj)

− 4

qf (zi  zj)qf (zi  zk) − 2

q2
f (zi  zj).

(cid:88)

i j

−2R2(zn) + 2(n2 − 5n + 8)R(zn) + 8

(cid:88)

i(cid:54)=j(cid:54)=k

ˆΣqf (zn) =

1
|A4
n|

qf (zi  zj)qf (zi  zk)

The only term that now requires special care is the last one (which is proportional to ˆσ2
qf

(zn)).

Recalling that qf (zi  zj) = 1{(yi−yj )(f (xi)−f (xj ))<0}  we observe that

(cid:26) yi = −1  yj = yk = +1 and f (xi) > f (xj)  f (xk)  or

yi = +1  yj = yk = −1 and f (xi) < f (xj)  f (xk).

qf (zi  zj)qf (zi  zk) = 1 ⇔

Let us deﬁne E +(i) and E−(i) as

E +(i) := {j : yj = −1  f (xj) > f (xi)}   and E−(i) := {j : yj = +1  f (xj) < f (xi)} .

:= |E +(i)|  and κ−

:= |E−(i)|.

i

and their sizes κ+
i
For i such that yi = 1  κ+
is the number of negative instances that have been scored higher than xi
i
by f. From (15)  we see that the contribution of i to the last term of (14) corresponds to the number
κ+
i (κ+

i − 1) of ordered pairs of indices in E +(i) (similarly for κ−
i   with yi = −1). Henceforth:
(cid:88)
i − 1) +

qf (zi  zj)qf (zi  zk) =

i − 1).
−

(cid:88)

(cid:88)

κ+
i (κ+

−
i (κ

κ

i(cid:54)=j(cid:54)=k

i:yi=+1

i:yi=−1

A simple way to compute the ﬁrst sum (on i such that yi = +1) is to sort and visit the data by
descending order of scores and then to incrementally compute the κ+
i ’s and the corresponding sum:
when a negative instance is encountered  κ+
is incremented by 1 and when a positive instance is
i − 1) is added to the current sum. An identical reasoning works for the second sum.
i
visited  κ+
The cost of computing ˆΣqf is therefore that of sorting the scores  which has cost O(n ln n).

i (κ+

4 Applications and Discussion

Here  we mention potential applications of the new empirical inequalities we have just presented.

6

Figure 2: Left: UCI banana dataset  data labelled +1 (−1) in red (green). Right: half the conﬁdence
interval of the Hoeffding bound and that of the empirical Bernstein bound as functions of ntest.

4.1 Test Set Bounds

A direct use of the empirical Bernstein inequalities is to draw test set bounds. In this scenario  a
sample Z n is split into a training set Ztrain := Z 1:ntrain of ntrain data and a hold-out set Ztest :=
Z ntrain+1:n of size ntest. Ztrain is used to train a model f that minimizes an empirical risk based on a
U-statistic inducing loss (such as in (1) or (2)) and Ztest is used to compute a conﬁdence interval on
the expected risk of f. For instance  if we consider the bipartite ranking problem  the loss is (cid:96)rank 
the corresponding kernel is qf (Z  Z(cid:48)) = (cid:96)rank(f  Z  Z(cid:48))  and  with probability at least 1 − δ

R(cid:96)rank(f ) ≤ ˆR(cid:96)rank(f  Ztest) +

4 ˆΣ2
qf

(Ztest) ln(4/δ)

ntest

+

10
ntest

ln

4
δ

 

(16)

(Ztest) is naturally the empirical variance of qf computed on Ztest.

where ˆΣ2
qf
Figure 2 displays the behavior of such test set bounds as ntest grows for the UCI banana dataset. To
produce this plot  we have learned a linear scoring function f (·) = (cid:104)w ·(cid:105) by minimizing

(cid:115)

(cid:88)

i(cid:54)=j

λ(cid:107)w(cid:107)2 +

(1 − (Yi − Yj)(cid:104)w  Xi − Xj(cid:105))2

for λ = 1.0. Of course  a purely linear scoring function would not make it possible to achieve
good ranking accuracy so we in fact work in the reproducing kernel hilbert space associated with the
Gaussian kernel k(x  x(cid:48)) = exp(−(cid:107)x−x(cid:48)(cid:107)2/2). We train our scoring function on ntrain = 1000 data
points and evaluate the test set bound on ntest = 100  500  1000  5000  10000 data points. Figure 2
(right) reports the size of half the conﬁdence interval of the Hoeffding bound (5) and that of the
empirical Bernstein bound given in (16). Just as in the situation described in Example 1  the use of
variance information gives rise to smaller conﬁdence intervals  even for moderate sizes of test sets.

4.2 Online Ranking and Empirical Racing Algorithms

Another application that we would like to describe is online bipartite ranking. Due to space limita-
tion  we only provide the main ideas on how we think our empirical tail inequalities and the efﬁcient
computation of the variance estimates we propose might be particularly useful in this scenario.
First  let us precise what we mean by online bipartite ranking. Obviously  this means that Y =
{−1  +1} and that the loss of interest is (cid:96)rank. In addition  it means that given a training set Z =
{Zi := (Xi  Yi)}n
i=1 the learning procedure will process the data of Z incrementally to give rise to
hypotheses f1  f2  . . .   fT . As (cid:96)rank entails a kernel of order m = 2  we assume that n = 2T and
that we process the data from Z pairs by pairs  i.e. (Z1  Z2) are used to learn f1  (Z3  Z4) and f1
are used to learn f2 and  more generally  (Z2t−1  Z2t) and ft−1 are used to produce ft (there exist
more clever ways to handle the data but this goes out of the scope of the present paper). We do not
specify any learning algorithm but we may imagine trying to minimize a penalized empirical risk
based on the surrogate loss (cid:96)sur: if linear functions f (·) = (cid:104)w ·(cid:105) are considered and a penalization

7

-1 0 1 2-2-1 0 1 2Banana dataset 0 0.1 0.2 0.3 0.4 0.5102103104Bernstein vs HoeffdingεHoeffdingεBernsteinlike (cid:107)w(cid:107)2 is used then the optimization problem to solve is of the same form as in the batch case:

λ(cid:107)w(cid:107)2 +

(1 − (Yi − Yj)(cid:104)w  Xi − Xj(cid:105))2  

(cid:88)

i(cid:54)=j

but is solved incrementally here. Rank-1 update formulas for inverses of matrices easily provide
means to incrementally solve this problem as new data arrive (this is the main reason why we have
mentioned this surrogate function).
As evoked by [5]  a nice feature of online learning is that the expected risk of hypothesis ft can
be estimated on the n − 2t examples of Z it was not trained on. Namely  when 2τ data have been
processed  there exist τ hypotheses f1  . . .   fτ and  for t < τ  with probability at least 1 − δ:

2 ˆΣ2
qf

(Z 2t:2τ ) ln(4/δ)

τ − t

+

5
τ − t

ln

4
δ

.

If one wants to have these conﬁdence intervals to simultaneously hold for all t and all τ with prob-
ability 1 − δ  basic computations to calculate the number of pairs (t  τ )  with 1 ≤ t < τ ≤ n show
that it sufﬁces to adjust δ to 4δ/(n + 1)2. Hence  with probability at least 1 − δ: ∀1 ≤ t < τ ≤ n 

(cid:115)

(cid:12)(cid:12)(cid:12) ≤

(cid:12)(cid:12)(cid:12)R(cid:96)rank(ft) − ˆR(cid:96)rank(ft  Z 2t:2τ ))
(cid:115)

(cid:12)(cid:12)(cid:12)R(cid:96)rank(ft) − ˆR(cid:96)rank(ft  Z 2t:2τ ))

(cid:12)(cid:12)(cid:12) ≤

4 ˆΣ2
qf

(Z 2t:2τ ) ln((n + 1)/δ)

τ − t

+

10
τ − t

ln

n + 1

δ

.

(17)

We would like to draw the attention of the reader on two features: one has to do with statistical
considerations and the other with algorithmic ones. First  if the conﬁdence intervals simultaneously
hold for all t and all τ as in (17)  it is possible  as the online learning process goes through  to discard
the hypotheses ft which have their lower bound (according to (17)) on R(cid:96)rank(ft) that is higher
than the upper bound (according to (17) as well) on R(cid:96)rank(ft(cid:48)) for some other hypothesis ft(cid:48). This
corresponds to a racing algorithm as described in [12]. Theoretically analyzing the relevance of such
a race can be easily done with the results of [14]  which deal with empirical Bernstein racing  but for
non-U-statistics. This full analysis will be provided in a long version of the present paper. Second  it
is algorithmically possible to preserve some efﬁciency in computing the various variance estimates
through the online learning process: these computations rely on sorting arguments  and it is possible
to take advantage of structures like binary search trees such as AVL trees  that are precisely designed
to efﬁciently maintain and update sorted lists of numbers. The remaining question is whether it is
possible to have shared such structures to summarize the sorted lists of scores for various hypotheses
(recall that the scores are computed on the same data). This will be the subject of further research.

5 Conclusion

We have proposed new empirical Bernstein inequalities designed for U-statistics. They generalize
the empirical inequalities of [13] and [3] while they merely result from two applications of the same
non-empirical tail inequality for U-statistics. We also show how  in the bipartite ranking situation 
the empirical variance can be efﬁciently computed. We mention potential applications  with illustra-
tive results for the case of test set bounds in the realm of bipartite ranking. In addition to the possible
extensions discussed in the previous section  we wonder whether it is possible to draw similar empir-
ical inequalities for other types of rich statistics such as  e.g.  linear rank statistics [8]. Obviously  we
plan to work on establishing generalization bounds derived from the new concentration inequalities
presented. This would require to carefully deﬁne a sound notion of capacity for U-statistic-based
classes of functions (inspired  for example  from localized Rademacher complexities). Such new
bounds would be compared with those proposed in [1  6  7  15] for the bipartite ranking and/or pair-
wise classiﬁcation problems. Finally  we also plan to carry out intensive simulations —in particular
for the task of online ranking— to get even more insights on the relevance of our contribution.

Acknowledgments

This work is partially supported by the IST Program of the EC  under the FP7 Pascal 2 Network of
Excellence  ICT-216886-NOE. LR is partially supported by the ANR project ASAP.

8

References
[1] S. Agarwal  T. Graepel  R. Herbrich  S. Har-Peled  and D. Roth. Generalization Bounds for

the Area under the ROC Curve. Journal of Machine Learning Research  6:393–425  2005.

[2] M. A. Arcones. A bernstein-type inequality for u-statistics and u-processes. Statistics &

probability letters  22(3):239–247  1995.

[3] J.-Y. Audibert  R. Munos  and C. Szepesv´ari. Tuning bandit algorithms in stochastic environ-
ments. In ALT ’07: Proceedings of the 18th international conference on Algorithmic Learning
Theory  pages 150–165  Berlin  Heidelberg  2007. Springer-Verlag.

[4] S. Boucheron  O. Bousquet  and G. Lugosi. Theory of classiﬁcation : A survey of some recent

advances. ESAIM. P&S  9:323–375  2005.

[5] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of online learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[6] S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical minimization of u -statistics.

The Annals of Statistics  36(2):844–874  April 2008.

[7] Y. Freund  R. Iyer  R.E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for combining

preferences. Journal of Machine Learning Research  4:933–969  2003.

[8] J. H´ajek and Z. Sid´ak. Theory of Rank Tests. Academic Press  1967.
[9] J. A. Hanley and B. J. Mcneil. The meaning and use of the area under a receiver operating

characteristic (roc) curve. Radiology  143(1):29–36  April 1982.

[10] W. Hoeffding. A Class of Statistics with Asymptotically Normal Distribution. Annals of

Mathematical Statistics  19(3):293–325  1948.

[11] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the

American Statistical Association  58(301):13–30  1963.

[12] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classiﬁca-
tion and function approximation. In Adv. in Neural Information Processing Systems NIPS 93 
pages 59–66  1993.

[13] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In

COLT 09: Proc. of The 22nd Annual Conference on Learning Theory  2009.
[14] V. Mnih  C. Szepesv´ari  and J.-Y. Audibert. Empirical bernstein stopping.

In ICML ’08:
Proceedings of the 25th international conference on Machine learning  pages 672–679  New
York  NY  USA  2008. ACM.

[15] C. Rudin and R. E. Schapire. Margin-based ranking and an equivalence between AdaBoost

and RankBoost. Journal of Machine Learning Research  10:2193–2232  Oct 2009.

[16] R. J. Serﬂing. Approximation theorems of mathematical statistics. J. Wiley & Sons  1980.

9

,Yuchen Zhang
John Duchi
Michael Jordan
Martin Wainwright