2014,Online Decision-Making in General Combinatorial Spaces,We study online combinatorial decision problems  where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010)  and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work  and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes  more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally  we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices  and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).,Online Decision-Making in

General Combinatorial Spaces

Arun Rajkumar

Shivani Agarwal

Department of Computer Science and Automation
Indian Institute of Science  Bangalore 560012  India
{arun r shivani}@csa.iisc.ernet.in

Abstract

We study online combinatorial decision problems  where one must make sequen-
tial decisions in some combinatorial space without knowing in advance the cost of
decisions on each trial; the goal is to minimize the total regret over some sequence
of trials relative to the best ﬁxed decision in hindsight. Such problems have been
studied mostly in settings where decisions are represented by Boolean vectors and
costs are linear in this representation. Here we study a general setting where costs
may be linear in any suitable low-dimensional vector representation of elements
of the decision space. We give a general algorithm for such problems that we
call low-dimensional online mirror descent (LDOMD); the algorithm generalizes
both the Component Hedge algorithm of Koolen et al. (2010)  and a recent algo-
rithm of Suehiro et al. (2012). Our study offers a uniﬁcation and generalization of
previous work  and emphasizes the role of the convex polytope arising from the
vector representation of the decision space; while Boolean representations lead to
0-1 polytopes  more general vector representations lead to more general polytopes.
We study several examples of both types of polytopes. Finally  we demonstrate the
beneﬁt of having a general framework for such problems via an application to an
online transportation problem; the associated transportation polytopes generalize
the Birkhoff polytope of doubly stochastic matrices  and the resulting algorithm
generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).

1

Introduction

In an online combinatorial decision problem  the decision space is a set of combinatorial structures 
such as subsets  trees  paths  permutations  etc. On each trial  one selects a combinatorial structure
from the decision space  and incurs a loss; the goal is to minimize the regret over some sequence of
trials relative to the best ﬁxed structure in hindsight. Such problems have been studied extensively
in the last several years  primarily in the setting where the combinatorial structures are represented
by Boolean vectors  and costs are linear in this representation; this includes online learning of paths 
permutations  and various other speciﬁc combinatorial structures [16  17  12]  as well as the Compo-
nent Hedge algorithm of Koolen et al. [14] which generalizes many of these previous studies. More
recently  Suehiro et al. [15] considered a setting where the combinatorial structures of interest are
represented by the vertices of the base polytope of a submodular function  and costs are linear in this
representation; this includes as special cases several of the Boolean examples considered earlier  as
well as new settings such as learning permutations with certain position-based losses (see also [2]).
In this work  we consider a general form of the online combinatorial decision problem  where costs
can be linear in any suitable low-dimensional vector representation of the combinatorial structures
of interest. This encompasses representations as Boolean vectors and vertices of submodular base
polytopes as special cases  but also includes many other settings. We give a general algorithm for

1

such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm gener-
alizes both the Component Hedge algorithm of Koolen et al. for Boolean representations [14]  and
the algorithm of Suehiro et al. for submodular polytope vertex representations [15].1 As we show  in
many settings of interest  the regret bounds for LDOMD are better than what can be obtained with
other algorithms for online decision problems  such as the Hedge algorithm of Freund and Schapire
[10] and the Follow the Perturbed Leader algorithm of Kalai and Vempala [13].
We start with some preliminaries and background in Section 2  and describe the LDOMD algorithm
and its analysis in Section 3. Our study emphasizes the role of the convex polytope arising from the
vector representation of the decision space; we study several examples of such polytopes  including
matroid polytopes  polytopes associated with submodular functions  and permutation polytopes in
Sections 4–6  respectively. Section 7 applies our framework to an online transportation problem.

2 Preliminaries and Background
Notation. For n ∈ Z+  we will denote [n] = {1  . . .   n}. For a vector z ∈ Rd  we will denote by
(cid:107)z(cid:107)1  (cid:107)z(cid:107)2  and (cid:107)z(cid:107)∞ the standard L1  L2  and L∞ norms of z  respectively. For a set Z ⊆ Rd  we
will denote by conv(Z) the convex hull of Z  and by int(Z) the interior of Z. For a closed convex
set K ⊆ Rd and Legendre function F : K→R 2 we will denote by BF : K × int(K)→R+ the
Bregman divergence associated with F   deﬁned as BF (x  x(cid:48)) = F (x)− F (x(cid:48))−∇F (x(cid:48))· (x− x(cid:48)) 
and by F ∗ : ∇F (int(K))→R the Fenchel conjugate of F   deﬁned as F ∗(u) = supx∈K(x·u−F (x)).
Problem Setup. Let C be a (ﬁnite but large) set of
combinatorial structures. Let φ : C→Rd be some in-
jective mapping that maps each c ∈ C to a unique
vector φ(c) ∈ Rd (so that |φ(C)| = |C|). We will
generally assume d (cid:28) |C| (e.g. d = poly log(|C|)).
The online combinatorial decision-making problem
we consider can be described as follows: On each
trial t  one makes a decision in C by selecting a struc-
ture ct ∈ C  and receives a loss vector (cid:96)t ∈ [0  1]d;
the loss incurred is given by φ(ct) · (cid:96)t (see Figure 1).
The goal is to minimize the regret relative to the sin-
gle best structure in C in hindsight; speciﬁcally  the
regret of an algorithm A that selects ct ∈ C on trial t
over T trials is deﬁned as

– Predict ct ∈ C
– Receive loss vector (cid:96)t ∈ [0  1]d
– Incur loss φ(ct) · (cid:96)t

Figure 1: Online decision-making in a gen-
eral combinatorial space.

Online Combinatorial Decision-Making
Inputs:

Finite set of combinatorial structures C
Mapping φ : C→Rd

For t = 1 . . . T :

RT [A] = (cid:80)T

t=1 φ(ct) · (cid:96)t − minc∈C(cid:80)T

t=1 φ(c) · (cid:96)t .

In particular  we would like to design algorithms whose worst-case regret (over all possible loss se-
quences) is sublinear in T (and also has as good a dependence as possible on other relevant problem
parameters). From standard results  it follows that for any deterministic algorithm  there is always a
loss sequence that forces the regret to be linear in T ; as is common in the online learning literature 
we will therefore consider randomized algorithms that maintain a probability distribution pt over C
from which ct is randomly drawn  and consider bounding the expected regret of such algorithms.
Online Mirror Descent (OMD). Recall that online mirror descent (OMD) is a general algorithmic
framework for online convex optimization problems  where on each trial t  one selects a point xt in
some convex set Ω ⊆ Rn  receives a convex cost function ft : Ω→R  and incurs a loss ft(xt); the
goal is to minimize the regret relative to the best single point in Ω in hindsight. The OMD algorithm
makes use of a Legendre function F : K→R deﬁned on a closed convex set K ⊇ Ω  and effectively
performs a form of projected gradient descent in the dual space of int(K) under F   the projections
being in terms of the Bregman divergence BF associated with F . See Appendix A.1 for an outline
of OMD and its regret bound for the special case of online linear optimization  where costs ft are
linear (so that ft(x) = (cid:96)t · x for some (cid:96)t ∈ Rn)  which will be relevant to our study.

1We note that the recent online stochastic mirror descent (OSMD) algorithm of Audibert et al. [3] also
generalizes the Component Hedge algorithm  but in a different direction: OSMD (as described in [3]) applies
to only Boolean representations  but allows also for partial information (bandit) settings; here we consider only
full information settings  but allow for more general vector representations.
2Recall that for a closed convex set K ⊆ Rd  a function F : K→R is Legendre if it is strictly convex 
differentiable on int(K)  and (for any norm (cid:107) · (cid:107) on Rd) (cid:107)∇F (xn)(cid:107)→ + ∞ whenever {xn} converges to a
point in the boundary of K.

2

Hedge/Na¨ıve OMD. The Hedge algorithm proposed by Freund and Schapire [10] is widely used
for online decision problems in general. The algorithm maintains a probability distribution over the
decision space  and can be viewed as an instantiation of the OMD framework  with Ω (and K) the
probability simplex over the decision space  linear costs ft (since one works with expected losses) 
and F the negative entropy. When applied to online combinatorial decision problems in a na¨ıve
manner  the Hedge algorithm requires maintaining a probability distribution over the combinatorial
decision space C  which in many cases can be computationally prohibitive (see Appendix A.2 for
an outline of the algorithm  which we also refer to as Na¨ıve OMD). The following bound on the
expected regret of the Hedge/Na¨ıve OMD algorithm is well known:
Theorem 1 (Regret bound for Hedge/Na¨ıve OMD). Let φ(c) · (cid:96)t ∈ [a  b] ∀c ∈ C  t ∈ [T ]. Then
setting η∗ = 2

(cid:113) 2 ln |C|

gives

(b−a)

T

(cid:104)

E

RT

(cid:114)
(cid:2) Hedge(η∗)(cid:3)(cid:105) ≤ (b − a)

T ln|C|

.

2

Follow the Perturbed Leader (FPL). Another widely used algorithm for online decision problems
is the Follow the Perturbed Leader (FPL) algorithm proposed by Kalai and Vempala [13] (see Ap-
pendix A.3 for an outline of the algorithm). Note that in the combinatorial setting  FPL requires the
solution to a combinatorial optimization problem on each trial  which may or may not be efﬁciently
solvable depending on the form of the mapping φ. The following bound on the expected regret of
the FPL algorithm is well known:
Theorem 2 (Regret bound for FPL). Let (cid:107)φ(c) − φ(c(cid:48))(cid:107)1 ≤ D1  (cid:107)(cid:96)t(cid:107)1 ≤ G1  and |φ(c) · (cid:96)t| ≤ B
∀c  c(cid:48) ∈ C  t ∈ [T ]. Then setting η∗ =

(cid:113) D1
(cid:2) FPL(η∗)(cid:3)(cid:105) ≤ 2
(cid:112)

BG1T gives

(cid:104)

E

RT

D1BG1T .

m ∈ Rd such that S = conv({x(cid:48)

Polytopes. Recall that a set S ⊂ Rd is a polytope if there exist a ﬁnite number of points x1  . . .   xn ∈
Rd such that S = conv({x1  . . .   xn}). Any polytope S ⊂ Rd has a unique minimal set of points
m}); these points are called the vertices of S. A
x(cid:48)
1  . . .   x(cid:48)
polytope S ⊂ Rd is said to be a 0-1 polytope if all its vertices lie in the Boolean hypercube {0  1}d.
As we shall see  in our study of online combinatorial decision problems as above  the polytope
conv(φ(C)) ⊂ Rd will play a central role. Clearly  if φ(C) ⊆ {0  1}d  then conv(φ(C)) is a 0-1
polytope; in general  however  conv(φ(C)) can be any polytope in Rd.

1  . . .   x(cid:48)

3 Low-Dimensional Online Mirror Descent (LDOMD)
We describe the Low-Dimensional OMD (LDOMD) algorithm in Figure 2. The algorithm maintains
a point xt in the polytope conv(φ(C)). It makes use of a Legendre function F : K→R deﬁned on
a closed convex set K ⊇ conv(φ(C))  and effectively performs OMD in a d-dimensional space
rather than in a |C|-dimensional space as in the case of Hedge/Na¨ıve OMD. Note that an efﬁcient
implementation of LDOMD requires two operations to be performed efﬁciently: (a) given a point
xt ∈ conv(φ(C))  one needs to be able to efﬁciently ﬁnd a ‘decomposition’ of xt into a convex
combination of a small number of points in φ(C) (this yields a distribution pt ∈ ∆C that satisﬁes
Ec∼pt[φ(c)] = xt and also has small support  allowing efﬁcient sampling); and (b) given a point

(cid:101)xt+1 ∈ K  one needs to be able to efﬁciently ﬁnd a ‘projection’ of(cid:101)xt+1 onto conv(φ(C)) in terms

of the Bregman divergence BF . The following regret bound for LDOMD follows directly from the
standard OMD regret bound (see Theorem 4 in Appendix A.1):
Theorem 3 (Regret bound for LDOMD). Let BF (φ(c)  x1) ≤ D2 ∀c ∈ C. Let (cid:107) · (cid:107) be any norm
in Rd such that (cid:107)(cid:96)t(cid:107) ≤ G ∀t ∈ [T ]  and such that the restriction of F to conv(φ(C)) is α-strongly
convex w.r.t. (cid:107) · (cid:107)∗  the dual norm of (cid:107) · (cid:107). Then setting η∗ = D

(cid:113) 2α
(cid:114)
(cid:2) LDOMD(η∗)(cid:3)(cid:105) ≤ DG

G

(cid:104)

E

RT

T gives
2T
α

.

As we shall see below  the LDOMD algorithm generalizes both the Component Hedge algorithm
of Koolen et al. [14]  which applies to settings where φ(C) ⊆ {0  1}d (Section 3.1)  and the recent
algorithm of Suehiro et al. [15]  which applies to settings where conv(φ(C)) is the base polytope
associated with a submodular function (Section 5).

3

Algorithm Low-Dimensional OMD (LDOMD) for Online Combinatorial Decision-Making
Inputs:

Finite set of combinatorial structures C
Mapping φ : C→Rd

Parameters:

Initialize:

For t = 1 . . . T :

η > 0
Closed convex set K ⊇ conv(φ(C))  Legendre function F : K→R
x1 = argminx∈conv(φ(C)) F (x) (or x1 = any other point in conv(φ(C)))
– Let pt be any distribution over C such that Ec∼pt[φ(c)] = xt
– Randomly draw ct ∼ pt
– Receive loss vector (cid:96)t ∈ [0  1]d
– Incur loss φ(ct) · (cid:96)t

– Update:(cid:101)xt+1 ← ∇F ∗(∇F (xt) − η(cid:96)t)

xt+1 ← argminx∈conv(φ(C)) BF (x (cid:101)xt+1)

[Bregman projection step]

[Decomposition step]

Figure 2: The LDOMD algorithm.

(cid:17)

√

(cid:104)

(cid:2) Hedge(η∗)(cid:3)(cid:105)

3.1 LDOMD with 0-1 Polytopes
Consider ﬁrst a setting where each c ∈ C is represented as a Boolean vector  so that φ(C) ⊆ {0  1}d.
In this case conv(φ(C)) is a 0-1 polytope. This is the setting commonly studied under the term
‘online combinatorial learning’ [14  8  3]. In analyzing this setting  one generally introduces an
additional problem parameter  namely an upper bound m on the ‘size’ of each Boolean vector φ(c).
Speciﬁcally  let us assume (cid:107)φ(c)(cid:107)1 ≤ m ∀c ∈ C for some m ∈ [d].
Under the above assumption  it is easy to verify that applying Theorems 1 and 2 gives

(cid:2) FPL(η∗)(cid:3)(cid:105)
let F : K→R be the unnormalized negative entropy  deﬁned as F (x) =(cid:80)d
+ and to
i=1 xi 
which leads to a multiplicative update algorithm; the resulting algorithm was termed Component
m ) ∀c ∈ C;
Hedge in [14]. For the above choice of F   it is easy to see that BF (φ(c)  x1) ≤ m ln( d
moreover  (cid:107)(cid:96)t(cid:107)∞ ≤ 1 ∀t  and the restriction of F on conv(φ(C)) is ( 1
m )-strongly convex w.r.t. (cid:107)·(cid:107)1.
(cid:17)
Therefore  applying Theorem 3 with appropriate η∗  one gets

(cid:104)
RT
T d) .
+  it is common to take K = Rd

E
For the LDOMD algorithm  since conv(φ(C)) ⊆ [0  1]d ⊂ Rd

i=1 xi ln xi −(cid:80)d

T m ln( d

(cid:113)

(cid:113)

(cid:2) LDOMD(η∗)(cid:3)(cid:105)

= O

m

= O

m

= O(m

m )

;

E

RT

(cid:16)

(cid:104)

E

RT

(cid:16)

T ln( d

m )

.

Thus  when φ(C) ⊆ {0  1}d  the LDOMD algorithm with the above choice of F gives a better regret
bound than both Hedge/Na¨ıve OMD and FPL; in fact the performance of LDOMD in this setting is
essentially optimal  as one can easily show a matching lower bound [3].
Below we will see how several online combinatorial decision problems studied in the literature can
be recovered under the above framework (e.g. see [16  17  12  14  8]); in many of these cases  both
decomposition and unnormalized relative entropy projection steps in LDOMD can be performed
efﬁciently (in poly(d) time) (e.g. see [14]). As a warm-up  consider the following simple example:
Example 1 (m-sets with element-based losses). Here C contains all size-m subsets of a ground set
of d elements: C = {S ⊆ [d]||S| = m}. On each trial t  one selects a subset St ∈ C and receives
a loss vector (cid:96)t ∈ [0  1]d  with (cid:96)t
i specifying the loss for including element i ∈ [d]; the loss for the
i. Here it is natural to deﬁne a mapping φ : C→{0  1}d that maps
i∈St (cid:96)t
each S ∈ C to its characteristic vector  deﬁned as φi(S) = 1(i ∈ S) ∀i ∈ [d]; the loss incurred
on predicting St ∈ C is then simply φ(St) · (cid:96)t. Thus φ(C) = {x ∈ {0  1}d |(cid:107)x(cid:107)1 = m}  and
conv(φ(C)) = {x ∈ [0  1]d |(cid:107)x(cid:107)1 = m}. LDOMD with unnormalized negative entropy as above

subset St is given by(cid:80)
(cid:113)
has a regret bound of O(cid:0)m

m )(cid:1). It can be shown that both decomposition and unnormalized

T ln( d

relative entropy projection steps take O(d2) time [17  14].

4

3.2 LDOMD with General Polytopes
Now consider a general setting where φ : C→Rd  and conv(φ(C)) ⊂ Rd is an arbitrary polytope.
Let us assume again (cid:107)φ(c)(cid:107)1 ≤ m ∀c ∈ C for some m > 0.
Again  it is easy to verify that applying Theorems 1 and 2 gives

(cid:2) Hedge(η∗)(cid:3)(cid:105)

= O(m(cid:112)T ln|C|) ;

√
= O(m

T d) .

(cid:104)

(cid:2) FPL(η∗)(cid:3)(cid:105)

E

RT

E

RT

(cid:104)

E

=

(cid:40)

(cid:2) LDOMD(η∗)(cid:3)(cid:105)

O(cid:0)m(cid:112)T ln(d)(cid:1)
O(cid:0)m(cid:112)T ln(m)(cid:1)

For the LDOMD algorithm  we consider two cases:
+. Here one can again take K = Rd
Case 1: φ(C) ⊂ Rd
+ and let F : K→R be the unnormalized
In this case  one gets BF (φ(c)  x1) ≤ m ln(d) + m ∀c ∈ C if m < d  and
negative entropy.
BF (φ(c)  x1) ≤ m ln(m) + d ∀c ∈ C if m ≥ d. As before  (cid:107)(cid:96)t(cid:107)∞ ≤ 1 ∀t  and the restriction of F
(cid:104)
m )-strongly convex w.r.t. (cid:107) · (cid:107)1  so applying Theorem 3 for appropriate η∗ gives
on conv(φ(C)) is ( 1

RT
Thus  when φ(C) ⊂ Rd
+  if ln|C| = ω(max(ln(m)  ln(d)))) and d = ω(ln(m))  then the
LDOMD algorithm with unnormalized negative entropy again gives a better regret bound than both
Hedge/Na¨ıve OMD and FPL.
Case 2: φ(C) (cid:54)⊂ Rd
If one can efﬁciently compute bi = minc∈C φi(c) (or more generally  a
+.
i(c) = φi(c) − bi ∀i gives a non-
lower bound bi ≤ minc∈C φi(c)) for each i ∈ [d]  then deﬁning φ(cid:48)
+ ∀c ∈ C  and one can apply the LDOMD algorithm with
negative vector representation φ(cid:48)(c) ∈ Rd
unnormalized negative entropy as above to the polytope φ(cid:48)(C) ⊂ Rd
+; clearly  the resulting regret
bound for φ(cid:48) also applies to φ. Another possibility is to use LDOMD with squared L2-norm  where
one takes K = Rd and lets F : K→R be deﬁned as F (x) = 1
2(cid:107)x(cid:107)2
2  which leads to an additive
2(cid:107)φ(c) − x1(cid:107)2
2 ≤ 2m2 ∀c ∈ C; moreover 
(cid:107)(cid:96)t(cid:107)2 ≤ √
update algorithm. In this case  one gets BF (φ(c)  x1) = 1
d ∀t  and F is 1-strongly convex w.r.t. (cid:107) · (cid:107)2. Applying Theorem 3 for appropriate η∗

if m < d
if m ≥ d.

(cid:104)

(cid:2) LDOMD(η∗)(cid:3)(cid:105)

E

RT

√
= O(m

T d) .

then gives

Thus LDOMD with squared L2-norm has a similar regret bound as that of Hedge/Na¨ıve OMD and
FPL. In certain cases  it may also be possible to implement LDOMD with other choices of K and F
that lead to better regret bounds.
In the following sections we will consider several examples of applications of LDOMD to online
combinatorial decision problems involving both 0-1 polytopes and general polytopes in Rd.

e ∈ E; the loss for the independent set I t is given by(cid:80)

4 Matroid Polytopes
Consider an online decision problem in which the decision space C contains (not necessarily all)
independent sets in a matroid M = (E I). Speciﬁcally  on each trial t  one selects an independent
set I t ∈ C  and receives a loss vector (cid:96)t ∈ [0  1]|E|  with (cid:96)t
e specifying the loss for including element
e. Here it is natural to deﬁne a
mapping φ : C→{0  1}|E| that maps each independent set I ∈ C to its characteristic vector  deﬁned
as φe(I) = 1(e ∈ I); the loss on selecting I t ∈ C is then φ(I t) · (cid:96)t. Thus here d = |E|  and
φ(C) ⊆ {0  1}|E|. A particularly interesting case is obtained by taking C to contain all the maximal
independent sets (bases) in I; in this case  the polytope conv(φ(C)) is known as the matroid base
polytope of M. This polytope  often denoted as B(M)  is also given by

e∈I t (cid:96)t

x ∈ R|E| (cid:12)(cid:12)(cid:12) (cid:80)
(cid:110)

e∈S xe ≤ rankM(S) ∀S ⊂ E  and(cid:80)

e∈E xe = rankM(E)

 

B(M) =

where rankM : 2E→R is the matroid rank function of M deﬁned as

rankM(S) = max(cid:8)|I| | I ∈ I  I ⊆ S(cid:9) ∀S ⊆ E .

(cid:111)

We will see below (Section 5) that both decomposition and unnormalized relative entropy projection
steps in this case can be performed efﬁciently assuming an appropriate oracle.
We note that Example 1 (m-subsets of a ground set of d elements) can be viewed as a special case of
the above setting for the matroid Msub = (E I) deﬁned by E = [d] and I = {S ⊆ E ||S| ≤ m};
the set C of m-subsets of [d] is then simply the set of bases in I  and conv(φ(C)) = B(Msub). The
following is another well-studied example:

5

Example 2 (Spanning trees with edge-based losses). Here one is given a connected  undirected
graph G = ([n]  E)  and the decision space C is the set of all spanning trees in G. On each trial t 
one selects a spanning tree T t ∈ C and receives a loss vector (cid:96)t ∈ [0  1]|E|  with (cid:96)t
e specifying the
e. It is well known that the set of
all spanning trees in G is the set of bases in the graphic matroid MG = (E I)  where I contains
edge sets of all acyclic subgraphs of G. Therefore here d = |E|  φ(C) is the set of incidence vectors
of all spanning trees in G  and conv(φ(C)) = B(MG)  also known as the spanning tree polytope.

loss for using edge e; the loss for the tree T t is given by(cid:80)
Here LDOMD with unnormalized negative entropy has a regret bound of O(cid:0)n

e∈T t (cid:96)t

(cid:113)

T ln(

n−1 )(cid:1).

|E|

5 Polytopes Associated with Submodular Functions
Next we consider settings where the decision space C is in one-to-one correspondence with the set
of vertices of the base polytope associated with a submodular function  and losses are linear in the
corresponding vertex representations of elements in C. This setting was considered recently in [15] 
and as we shall see  encompasses both of the examples we saw earlier  as well as many others. Let
f : 2[n]→R be a submodular function with f (∅) = 0. The base polytope of f is deﬁned as

x ∈ Rn (cid:12)(cid:12)(cid:12)(cid:80)
(cid:110)

i∈S xi ≤ f (S) ∀S ⊂ [n]  and(cid:80)n

(cid:111)

i=1 xi = f ([n])

.

B(f ) =

+ [4]. Therefore in this case one can take K = Rn

Let φ : C→Rn be a bijective mapping from C to the vertices of B(f ); thus conv(φ(C)) = B(f ).
5.1 Monotone Submodular Functions
It is known that when f is a monotone submodular function (which means U ⊆ V =⇒ f (U ) ≤
f (V ))  then B(f ) ⊆ Rn
+ and F : K→R to be the
unnormalized negative entropy. Both decomposition and unnormalized relative entropy projection
steps can be performed in time O(n6 + n5Q)  where Q is the time taken by an oracle that given
S returns f (S); for cardinality-based submodular functions  for which f (S) = g(|S|) for some
g : [n]→R  these steps can be performed in just O(n2) time [15].
Remark on matroid base polytopes and spanning trees. We note that the matroid rank function
of any matroid M is a monotone submodular function  and that the matroid base polytope B(M)
is the same as B(rankM). Therefore Examples 1 and 2 can also be viewed as special cases of the
above setting. For the spanning trees of Example 2  the decomposition step of [14] makes use of a
linear programming formulation whose exact time complexity is unclear. Instead  one could use the
decomposition step associated with the submodular function rankMG  which takes O(|E|6) time.
Matroid polytopes are 0-1 polytopes; the example below illustrates a more general polytope:
Example 3 (Permutations with a certain position-based loss). Let C = Sn  the set of all permutations
of n objects: C = {σ : [n]→[n]| σ is bijective}. On each trial t  one selects a permutation σt ∈ C
i (n−σt(i)+1).
This type of loss arises in scheduling applications  where (cid:96)t
i denotes the time taken to complete the
i-th job  and the loss of a job schedule (permutation of jobs) is the total waiting time of all jobs
(the waiting time of a job is its own completion time plus the sum of completion times of all jobs
scheduled before it) [15]. Here it is natural to deﬁne a mapping φ : C→Rn
+ that maps σ ∈ C to
φ(σ) = (n − σ(1) + 1  . . .   n − σ(n) + 1); the loss on selecting σt ∈ C is then φ(σt) · (cid:96)t. Thus
here we have d = n  and φ(C) = {(σ(1)  . . .   σ(n))| σ ∈ Sn}. It is known that the n! vectors in
φ(C) are exactly the vertices of the base polytope corresponding to the monotone (cardinality-based)
i=1(n − i + 1). Thus conv(φ(C)) =
B(fperm); this is a well-known polytope called the permutahedron [21]  and has recently been studied
∀σ ∈ C  and
in the context of online learning applications in [18  15  1]. Here (cid:107)φ(σ)(cid:107)1 = n(n+1)

submodular function fperm : 2[n]→R deﬁned as fperm(S) =(cid:80)|S|
therefore LDOMD with unnormalized negative entropy has a regret bound of O(cid:0)n2(cid:112)T ln(n)(cid:1). As

and receives a loss vector (cid:96)t ∈ [0  1]n; the loss of the permutation is given by(cid:80)n

i=1 (cid:96)t

2

noted above  decomposition and unnormalized relative entropy projection steps take O(n2) time.

5.2 General Submodular Functions
In general  when f is non-monotone  B(f ) ⊂ Rn can contain vectors with non-negative entries.
Here one can use LOMD with the squared L2-norm. The Euclidean projection step can again be
performed in time O(n6 + n5Q) in general  where Q is the time taken by an oracle that given S
returns f (S)  and in O(n2) time for cardinality-based submodular functions [15].

6

C = {σ : [n]→[n]| σ is bijective} .

6 Permutation Polytopes
There has been increasing interest in recent years in online decision problems involving rankings or
permutations  largely due to their role in applications such as information retrieval  recommender
systems  rank aggregation  etc [12  18  19  15  1  2]. Here the decision space is C = Sn  the set of
all permutations of n objects:
On each trial t  one predicts a permutation σt ∈ C and receives some type of loss. We saw one special
type of loss in Example 3; we now consider any loss that can be represented as a linear function of
some vector representation of the permutations in C. Speciﬁcally  let d ∈ Z+  and let φ : C→Rd be
any injective mapping such that on predicting σt  one receives a loss vector (cid:96)t ∈ [0  1]d and incurs
loss φ(σt) · (cid:96)t. For any such mapping φ  the polytope conv(φ(C)) is called a permutation polytope
[5].3 The permutahedron we saw in Example 3 is one example of a permutation polytope; here
we consider various other examples. For any such polytope  if one can perform the decomposition
and suitable Bregman projection steps efﬁciently  then one can use the LDOMD algorithm to obtain
good regret guarantees with respect to the associated loss.
Example 4 (Permutations with assignment-based losses). Here on each trial t  one selects a per-
mutation σt ∈ C and receives a loss matrix (cid:96)t ∈ [0  1]n×n  with (cid:96)t
ij specifying the loss for assigning
i σt(i). Here it is natural
to deﬁne a mapping φ : C→{0  1}n×n that maps each σ ∈ C to its associated permutation matrix
ij = 1(σ(i) = j) ∀i  j ∈ [n]; the loss incurred on predicting σt ∈ C is
P σ ∈ {0  1}n×n  deﬁned as P σ
ij. Thus we have here that d = n2  φ(C) = {P σ ∈ {0  1}n×n | σ ∈ Sn} 
and conv(φ(C)) is the well-known Birkhoff polytope containing all doubly stochastic matrices in
[0  1]n×n (also known as the assignment polytope or the perfect matching polytope of the complete
bipartite graph Kn n). Here LDOMD with unnormalized negative entropy has a regret bound of

element i to position j; the loss for the permutation σt is given by(cid:80)n
then(cid:80)n
O(cid:0)n(cid:112)T ln(n)(cid:1). This recovers exactly the PermELearn algorithm used in [12]; see [12] for efﬁ-
i γ(σt(i)); the total loss of the permutation σt is given by(cid:80)n

cient implementations of the decomposition and unnormalized relative entropy projection steps.
Example 5 (Permutations with general position-based losses). Here on each trial t  one selects
a permutation σt ∈ C and receives a loss vector (cid:96)t ∈ [0  1]n. There is a weight function γ :
[n]→R+ that weights the loss incurred at each position  such that the loss contributed by element
i is (cid:96)t
i γ(σt(i)). Note that the
particular loss considered in Example 3 (and in [15]) is a special case of such a position-based loss 
with weight function γ(i) = (n−i+1). Several other position-dependent losses are used in practice;
for example  the discounted cumulative gain (DCG) based loss  which is widely used in information
retrieval applications  effectively uses γ(i) = 1 −
log2(i)+1 [9]. For a general position-based loss
with weight function γ  one can deﬁne φ : C→Rn
+ as φ(σ) = (γ(σ(1))  . . .   γ(σ(n))). This yields a
+. Provided
one can implement the decomposition and suitable Bregman projection steps efﬁciently  one can use
the LDOMD algorithm to get a sublinear regret.

permutation polytope conv(φ(C)) = conv(cid:0)(cid:8)(γ(σ(1))  . . .   γ(σ(n))) | σ ∈ Sn

(cid:9)(cid:1) ⊂ Rn

j=1 φij(σt)(cid:96)t

(cid:80)n

i=1

i=1 (cid:96)t

i=1 (cid:96)t

1

i=1 ai =(cid:80)n

7 Application to an Online Transportation Problem
Consider now the following transportation problem: there are m supply locations for a particular
commodity and n demand locations  with a supply vector a ∈ Zm
+ and demand vector b ∈ Zn
+
specifying the (integer) quantities of the commodity supplied/demanded by the various locations.
(cid:52)
= q. In the ofﬂine setting  there is a cost matrix (cid:96) ∈ [0  1]m×n  with
(cid:96)ij specifying the cost of transporting one unit of the commodity from supply location i to demand
location j  and the goal is to decide on a transportation matrix Q ∈ Zm×n
that speciﬁes suitable
(integer) quantities of the commodity to be transported between the various supply and demand

Assume(cid:80)m
locations so as to minimize the total transportation cost (cid:80)m

(cid:80)n

j=1 bj

+

j=1 Qij(cid:96)ij.

i=1

Here we consider an online variant of this problem where the supply vector a and demand vector b
are viewed as remaining constant over some period of time  while the costs of transporting the com-

3The term ‘permutation polytope’ is sometimes used to refer to various polytopes obtained through speciﬁc
mappings φ : Sn→Rd; here we use the term in a broad sense for any such polytope  following terminology of
Bowman [5]. (Note that the description Bowman [5] gives of a particular 0-1 permutation polytope in Rn(n−1) 
known as the binary choice polytope or the linear ordering polytope [20]  is actually incorrect; e.g. see [11].)

7

Algorithm Decomposition Step for Transportation Polytopes
Input: X ∈ T (a  b)
Initialize: A1 ← X; k ← 0
Repeat:

(where a ∈ Zm

+   b ∈ Zn
+)

(cid:16) Ak

– k ← k + 1
– Find an extreme point Qk ∈ T (a  b) such that Ak
– αk ← min(i j):Qk
– Ak+1 ← Ak − αkQk

(cid:17)
Until(cid:0) all entries of Ak+1 are zero(cid:1)

ij
Qk
ij

ij >0

X =(cid:80)k

Ouput: Decomposition of X as convex combination of extreme points Q1  . . .   Qk:
r=1 αr = 1)
Figure 3: Decomposition step in applying LDOMD to transportation polytopes.

(it can be veriﬁed that αr ∈ (0  1] ∀r and(cid:80)k

r=1 αrQr

ij = 0 =⇒ Qk

ij = 0 (see Appendix B)

+

+

+

i=1

ij(cid:96)t

C =(cid:8)Q ∈ Zm×n

modity between various supply and demand locations change over time. Speciﬁcally  the decision
space here is the set of all valid (integer) transportation matrices satisfying constraints given by a  b:

|(cid:80)n
j=1 Qij = ai ∀i ∈ [m]   (cid:80)m
[0  1]m×n; the loss incurred is(cid:80)m
(cid:80)n
conv(φ(C)) = T (a  b) =(cid:8)X ∈ Rm×n
|(cid:80)n
j=1 Xij = ai ∀i ∈ [m]   (cid:80)m

On each trial t  one selects a transportation matrix Qt ∈ C  and receives a cost matrix (cid:96)t ∈
ij. A natural mapping here is simply the identity:
φ : C→Zm×n
with φ(Q) = Q ∀Q ∈ C. Thus we have here d = mn  φ(C) = C  and conv(φ(C)) is
the well-known transportation polytope T (a  b) (e.g. see [6]):

i=1 Xij = bj ∀j ∈ [n](cid:9) .

i=1 Qij = bj ∀j ∈ [n](cid:9) .

j=1 Qt

Transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices  which can
be seen to arise as a special case when m = n and ai = bi = 1 ∀i ∈ [n] (see Example 4). While the
Birkhoff polytope is a 0-1 polytope  a general transportation polytope clearly includes non-Boolean
vertices. Nevertheless  we do have T (a  b) ⊂ Rm×n
  which suggests we can use the LDOMD
algorithm with unnormalized negative entropy.
For the decomposition step in LDOMD  one can use an algorithm broadly similar to that used for the
Birkhoff polytope in [12]. Speciﬁcally  given a matrix X ∈ conv(φ(C)) = T (a  b)  one successively
subtracts off multiples of extreme points Qk ∈ C from X until one is left with a zero matrix (see
Figure 3). However  a key step of this algorithm is to ﬁnd a suitable extreme point to subtract off
on each iteration. In the case of the Birkhoff polytope  this involved ﬁnding a suitable permutation
matrix  and was achieved by ﬁnding a perfect matching in a suitable bipartite graph. For general
transportation polytopes  we make use of a characterization of extreme points in terms of spanning
forests in a suitable bipartite graph (see Appendix B for details). The overall decomposition results
in a convex combination of at most mn extreme points in C  and takes O(m3n3) time.
The unnormalized relative entropy projection step can be performed efﬁciently by using a procedure
similar to the Sinkhorn balancing used for the Birkhoff polytope in [12]. Speciﬁcally  given a non-
  one alternately scales the rows and columns to match the desired row
and column sums until some convergence criterion is met. As with Sinkhorn balancing  this results
in an approximate projection step  but does not hurt the overall regret analysis (other than a constant

negative matrix (cid:101)X ∈ Rm×n
additive term)  yielding a regret bound of O(cid:0)q(cid:112)T ln(max(mn  q))(cid:1).

+

+

8 Conclusion
We have considered a general form of online combinatorial decision problems  where costs can be
linear in any suitable low-dimensional vector representation of elements of the decision space  and
have given a general algorithm termed low-dimensional online mirror descent (LDOMD) for such
problems. Our study emphasizes the role of the convex polytope arising from the vector representa-
tion of the decision space; this both yields a uniﬁcation and generalization of previous algorithms 
and gives a general framework that can be used to design new algorithms for speciﬁc applications.
Acknowledgments. Thanks to the anonymous reviewers for helpful comments and Chandrashekar
Lakshminarayanan for helpful discussions. AR is supported by a Microsoft Research India PhD
Fellowship. SA thanks DST and the Indo-US Science & Technology Forum for their support.

8

References
[1] Nir Ailon. Bandit online optimization over the permutahedron. CoRR  abs/1312.1530  2013.
[2] Nir Ailon. Online ranking: Discrete choice  spearman correlation and other feedback. CoRR 

abs/1308.6797  2013.

[3] Jean-Yves Audibert  S´ebastien Bubeck  and G´abor Lugosi. Regret in online combinatorial

optimization. Mathematics of Operations Research  39(1):31–45  2014.

[4] Francis Bach. Learning with submodular functions: A convex optimization perspective. Foun-

dations and Trends in Machine Learning  6(2-3):145–373  2013.

[5] V. J. Bowman. Permutation polyhedra. SIAM Journal on Applied Mathematics  22(4):580–

589  1972.

[6] Richard A Brualdi. Combinatorial Matrix Classes. Cambridge University Press  2006.
[7] S´ebastion Bubeck. Introduction to online optimization. Lecture Notes  Princeton University 

2011.

[8] Nicol`o Cesa-Bianchi and G´abor Lugosi. Combinatorial bandits. Journal of Computer and

System Sciences  78(5):1404–1422  2012.

[9] David Cossock and Tong Zhang. Statistical analysis of Bayes optimal subset ranking. IEEE

Transactions on Information Theory  54(11):5140–5154  2008.

[10] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences  55(1):119–139 
1997.

[11] M. Gr¨otschel  M. J¨unger  and G. Reinelt. Facets of the linear ordering polytope. Mathematical

Programming  33:43–60  1985.

[12] David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential

weights. Journal of Machine Learning Research  10:1705–1736  2009.

[13] Adam Tauman Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems.

Journal of Computer and System Sciences  71(3):291–307  2005.

[14] Wouter M. Koolen  Manfred K. Warmuth  and Jyrki Kivinen. Hedging structured concepts. In

COLT  2010.

[15] Daiki Suehiro  Kohei Hatano  Shuji Kijima  Eiji Takimoto  and Kiyohito Nagano. Online

prediction under submodular constraints. In ALT  2012.

[16] Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of

Machine Learning Research  4:773–818  2003.

[17] Manfred K. Warmuth and Dima Kuzmin. Randomized online PCA algorithms with regret
bounds that are logarithmic in the dimension. Journal of Machine Learning Research  9:2287–
2320  2008.

[18] Shota Yasutake  Kohei Hatano  Shuji Kijima  Eiji Takimoto  and Masayuki Takeda. Online

linear optimization over permutations. In ISAAC  pages 534–543  2011.

[19] Shota Yasutake  Kohei Hatano  Eiji Takimoto  and Masayuki Takeda. Online rank aggregation.

In ACML  2012.

[20] Jun Zhang. Binary choice  subset choice  random utility  and ranking: A uniﬁed perspective

using the permutahedron. Journal of Mathematical Psychology  48:107–134  2004.

[21] G¨unter M. Ziegler. Lectures on Polytopes. Springer  1995.

9

,Arun Rajkumar
Shivani Agarwal
Meisam Razaviyayn
David Tse
Bayan Saparbayeva
Michael Zhang
Lizhen Lin