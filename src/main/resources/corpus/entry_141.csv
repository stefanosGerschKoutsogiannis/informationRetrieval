2013,A New Convex Relaxation for Tensor Completion,We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on the extension of trace norm regularization  which has been used extensively for learning low rank matrices  to the tensor setting.  In this paper  we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean unit ball.  We then describe a technique to solve the associated regularization problem  which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error  while remaining computationally tractable.,A New Convex Relaxation for Tensor Completion

Bernardino Romera-Paredes
Department of Computer Science

and UCL Interactive Centre
University College London

Malet Place  London WC1E 6BT  UK

Massimiliano Pontil

Department of Computer Science and

Centre for Computational Statistics

and Machine Learning

University College London

B.RomeraParedes@cs.ucl.ac.uk

Malet Place  London WC1E 6BT  UK

m.pontil@cs.ucl.ac.uk

Abstract

We study the problem of learning a tensor from a set of linear measurements.
A prominent methodology for this problem is based on a generalization of trace
norm regularization  which has been used extensively for learning low rank ma-
trices  to the tensor setting. In this paper  we highlight some limitations of this
approach and propose an alternative convex relaxation on the Euclidean ball. We
then describe a technique to solve the associated regularization problem  which
builds upon the alternating direction method of multipliers. Experiments on one
synthetic dataset and two real datasets indicate that the proposed method improves
signiﬁcantly over tensor trace norm regularization in terms of estimation error 
while remaining computationally tractable.

1

Introduction

During the recent years  there has been a growing interest on the problem of learning a tensor from
a set of linear measurements  such as a subset of its entries  see [9  17  22  23  25  26  27] and
references therein. This methodology  which is also referred to as tensor completion  has been
applied to various ﬁelds  ranging from collaborative ﬁltering [15]  to computer vision [17]  and
medical imaging [9]  among others. In this paper  we propose a new method to tensor completion 
which is based on a convex regularizer which encourages low rank tensors and develop an algorithm
for solving the associated regularization problem.

Arguably the most widely used convex approach to tensor completion is based upon the extension
of trace norm regularization [24] to that context. This involves computing the average of the trace
norm of each matricization of the tensor [16]. A key insight behind using trace norm regularization
for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix
deﬁned on the spectral unit ball [8]. Unfortunately  the extension of this methodology to the more
general tensor setting presents some difﬁculties. In particular  we shall prove in this paper that the
tensor trace norm is not a tight convex relaxation of the tensor rank.

The above negative result stems from the fact that the spectral norm  used to compute the convex
relaxation for the trace norm  is not an invariant property of the matricization of a tensor. This
observation leads us to take a different route and study afresh the convex relaxation of tensor rank on
the Euclidean ball. We show that this relaxation is tighter than the tensor trace norm  and we describe
a technique to solve the associated regularization problem. This method builds upon the alternating
direction method of multipliers and a subgradient method to compute the proximity operator of the
proposed regularizer. Furthermore  we present numerical experiments on one synthetic dataset and
two real-life datasets  which indicate that the proposed method improves signiﬁcantly over tensor
trace norm regularization in terms of estimation error  while remaining computationally tractable.

1

The paper is organized in the following manner. In Section 2  we describe the tensor completion
framework. In Section 3  we highlight some limitations of the tensor trace norm regularizer and
present an alternative convex relaxation for the tensor rank. In Section 4  we describe a method to
solve the associated regularization problem. In Section 5  we report on our numerical experience
with the proposed method. Finally  in Section 6  we summarize the main contributions of this paper
and discuss future directions of research.

2 Preliminaries

In this section  we begin by introducing some notation and then proceed to describe the learning
problem. We denote by N the set of natural numbers and  for every k ∈ N  we deﬁne [k] =
{1  . . .   k}. Let N ∈ N and let1 p1  . . .   pN ≥ 2. An N -order tensor W ∈ Rp1×···×pN   is a
collection of real numbers (Wi1 ... iN : in ∈ [pn]  n ∈ [N ]). Boldface Euler scripts  e.g. W  will be
used to denote tensors of order higher than two. Vectors are 1-order tensors and will be denoted by
lower case letters  e.g. x or a; matrices are 2-order tensors and will be denoted by upper case letters 
e.g. W . If x ∈ Rd then for every r ≤ s ≤ d  we deﬁne xr:s := (xi : r ≤ i ≤ s). We also use the
notation pmin = min{p1  . . .   pN} and pmax = max{p1  . . .   pN}.
A mode-n ﬁber of a tensor W is a vector composed of the elements of W obtained by ﬁxing all
indices but one  corresponding to the n-th mode. This notion is a higher order analogue of columns
(mode-1 ﬁbers) and rows (mode-2 ﬁbers) for matrices. The mode-n matricization (or unfolding) of
W  denoted by W(n)  is a matrix obtained by arranging the mode-n ﬁbers of W so that each of

is not important as long as it is used consistently.

them is a column of W(n) ∈ Rpn×Jn   where Jn :=Qk6=n pk. Note that the ordering of the columns
We are now ready to describe the learning problem. We choose a linear operator I : Rp1×···×pN →
Rm  representing a set of linear measurements obtained from a target tensor W 0 as y = I(W 0)+ξ 
where ξ is some disturbance noise. Tensor completion is an important example of this setting  in
this case the operator I returns the known elements of the tensor. That is  we have I(W 0) =
(W 0
i1(j) ... iN (j) : j ∈ [m])  where  for every j ∈ [m] and n ∈ [N ]  the index in(j) is a prescribed
integer in the set [pn]. Our aim is to recover the tensor W 0 from the data (I  y). To this end  we

solve the regularization problem

(1)
where γ is a positive parameter which may be chosen by cross validation. The role of the regularizer
R is to encourage solutions W which have a simple structure in the sense that they involve a small
number of “degrees of freedom”. A natural choice is to consider the average of the rank of the
tensor’s matricizations. Speciﬁcally  we consider the combinatorial regularizer

2 + γR(W) : W ∈ Rp1×···×pN(cid:9)

min(cid:8)ky − I(W)k2

Finding a convex relaxation of this regularizer has been the subject of recent works [9  17  23]. They
all agree to use the sum of nuclear norms as a convex proxy of R. This is deﬁned as the average of
the trace norm of each matricization of W  that is 

rank(W(n)).

(2)

R(W) =

1
N

N

Xn=1

kWktr =

1
N

N

Xn=1

kW(n)ktr

(3)

where kW(n)ktr is the trace (or nuclear) norm of matrix W(n)  namely the ℓ1-norm of the vector of
singular values of matrix W(n) (see  e.g. [14]). Note that in the particular case of 2-order tensors 
functions (2) and (3) coincide with the usual notion of rank and trace norm of a matrix  respectively.

A rational behind the regularizer (3) is that the trace norm is the tightest convex lower bound to the
rank of a matrix on the spectral unit ball  see [8  Thm. 1]. This lower bound is given by the convex
envelope of the function

Ψ(W ) =(cid:26) rank(W ) 

+∞ 

if kWk∞ ≤ 1
otherwise

(4)

1For simplicity we assume that pn ≥ 2 for every n ∈ [N ]  otherwise we simply reduce the order of the

tensor without loss of information.

2

where k·k∞ is the spectral norm  namely the largest singular value of W . The convex envelope can
be derived by computing the double conjugate of Ψ. This is deﬁned as

Ψ∗∗(W ) = sup(cid:8)hW  Si − Ψ∗(W ) : S ∈ Rp1×p2(cid:9)

where Ψ∗ is the conjugate of Ψ  namely Ψ∗(S) = sup{hW  Si − Ψ(W ) : W ∈ Rp1×p2}.
Note that Ψ is a spectral function  that is  Ψ(W ) = ψ(σ(W )) where ψ : Rd
associated symmetric gauge function. Using von Neumann’s trace theorem (see e.g.
easily seen that Ψ∗(S) is also a spectral function. That is  Ψ∗(S) = ψ∗(σ(S))  where

+ → R denotes the
[14]) it is

(5)

ψ∗(σ) = sup(cid:8)hσ  wi − ψ(w) : w ∈ Rd

+(cid:9)   with d := min(p1  p2).

We refer to [8] for a detailed discussion of these ideas. We will use this equivalence between spectral
and gauge functions repeatedly in the paper.

3 Alternative Convex Relaxation

In this section  we show that the tensor trace norm is not a tight convex relaxation of the tensor rank
R in equation (2). We then propose an alternative convex relaxation for this function.

Note that due to the composite nature of the function R  computing its convex envelope is a chal-
lenging task and one needs to resort to approximations. In [22]  the authors note that the tensor trace
norm k · ktr in equation (3) is a convex lower bound to R on the set

G∞ :=(cid:8)W ∈ Rp1×···×pN : (cid:13)(cid:13)W(n)(cid:13)(cid:13)∞ ≤ 1  ∀n ∈ [N ](cid:9) .

The key insight behind this observation is summarized in Lemma 4  which we report in Appendix A.
However  the authors of [22] leave open the question of whether the tensor trace norm is the convex
envelope of R on the set G∞. In the following  we will prove that this question has a negative answer
by showing that there exists a convex function Ω 6= k · ktr which underestimates the function R on
G∞ and such that for some tensor W ∈ G∞ it holds that Ω(W) > kWktr.
To describe our observation we introduce the set

G2 :=(cid:8)W ∈ Rp1×...×pN : kWk2 ≤ 1(cid:9)

where k · k2 is the Euclidean norm for tensors  that is 
XiN =1

Xi1=1

kWk2

2 :=

···

p1

pN

We will choose

Ω(W) = Ωα(W) :=

1
N

(Wi1 ... iN )2.

N

Xn=1

ω∗∗α  σ W(n)(cid:1)(cid:1)

(6)

where ω∗∗α is the convex envelope of the cardinality of a vector on the ℓ2-ball of radius α and we
will choose α = √pmin. Note  by Lemma 4 stated in Appendix A  that for every α > 0  function
Ωα is a convex lower bound of function R on the set αG2.
Below  for every vector s ∈ Rd we denote by s↓ the vector obtained by reordering the components
of s so that they are non increasing in absolute value  that is  |s↓1| ≥ ··· ≥ |s↓d|.
Lemma 1. Let ω∗∗α be the convex envelope of the cardinality on the ℓ2-ball of radius α. Then  for
every x ∈ Rd such that kxk2 = α  it holds that ω∗∗α (x) = card (x).
This lemma is proved in Appendix B. The function ω∗∗α resembles the norm developed in [1]  which
corresponds to the convex envelope of the indicator function of the cardinality of a vector in the ℓ2
ball. The extension of its application to tensors is not straighforward though  as it is required to
specify beforehand the rank of each matricization.

The next lemma provides  together with Lemma 1  a sufﬁcient condition for the existence of a tensor
W ∈ G∞ at which the regularizer in equation (6) is strictly larger than the tensor trace norm.

3

Lemma 2.

If N ≥ 3 and p1  . . .   pN are not all equal

W ∈ Rp1×···×pN such that: (a) kWk2 = √pmin  (b) W ∈ G∞  (c) min
n∈[N ]
max
n∈[N ]

rank(W(n)).

to each other 

then there exists
rank(W(n)) <

The proof of this lemma is presented in Appendix C. We are now ready to formulate the main result
of this section.
Proposition 3. Let p1  . . .   pN ∈ N  let k · ktr be the tensor trace norm in equation (3) and let
Ωα be the function in equation (6) for α = √pmin.
If pmin < pmax  then there are inﬁnitely
many tensors W ∈ G∞ such that Ωα(W) > kWktr. Moreover  for every W ∈ G2  it holds that
Ω1(W) ≥ kWktr.
Proof. By construction Ωα(W) ≤ R(W) for every W ∈ αG2. Since G∞ ⊂ αG2 then Ωα is a
convex lower bound for the tensor rank R on the set G∞ as well. The ﬁrst claim now follows by
Lemmas 1 and 2. Indeed  all tensors obtained following the process described in the proof of Lemma
2 (in Appendix C) have the property that

N

kWktr =

<

1
N

1
N

1

kσ(W(n))k1 =

N (cid:18)pmin(N − 1) +qp2
Xn=1
(pmin(N − 1) + pmin + 1) = Ω(W) = R(W).

min + pmin(cid:19)

Furthermore there are inﬁnitely many such tensors which satisfy this claim (see Appendix C).
With respect to the second claim  given that ω∗∗1
is the convex envelope of the cardinality card on
the Euclidean unit ball  then ω∗∗1 (σ) ≥ kσk1 for every vector σ such that kσk2 ≤ 1. Consequently 

Ω1(W) = 1

N PN

n=1 ω∗∗1  σ W(n)(cid:1)(cid:1) ≥ 1

N PN

n=1 kσ(W(n))k1 = kWktr.

The above result stems from the fact that the spectral norm is not an invariant property of the matri-
cization of a tensor  whereas the Euclidean (Frobenius) norm is. This observation leads us to further
study the function Ωα.

4 Optimization Method

In this section  we explain how to solve the regularization problem associated with the regularizer
(6). For this purpose  we ﬁrst recall the alternating direction method of multipliers (ADMM) [4] 
which was conveniently applied to tensor trace norm regularization in [9  22].

4.1 Alternating Direction Method of Multipliers (ADMM)

To explain ADMM we consider a more general problem comprising both tensor trace norm regular-
ization and the regularizer we propose 

Xn=1
where E(W) is an error term such as ky − I(W)k2
deﬁned  for every matrix A  as

W (E (W) + γ

min

N

Ψ W(n)(cid:1))

2 and Ψ is a convex spectral function. It is

(7)

Ψ(A) = ψ(σ(A))

where ψ is a gauge function  namely a function which is symmetric and invariant under permuta-
tions. In particular  if ψ is the ℓ1 norm then problem (7) corresponds to tensor trace norm regular-
ization  whereas if ψ = ω∗∗α it implements the proposed regularizer.
Problem (7) poses some difﬁculties because the terms under the summation are interdependent  due
to the different matricizations of W having the same elements rearranged in a different way. In

4

order to overcome this difﬁculty  the authors of [9  22] proposed to use ADMM as a natural way to
decouple the regularization term appearing in problem (7). This strategy is based on the introduction

of N auxiliary tensors  B1  . . .   BN ∈ Rp1×···×pN   so that problem (7) can be reformulated as2

W B1 ... BN( 1

min

γ

E (W) +

N

Xn=1

Ψ Bn(n)(cid:1) : Bn = W  n ∈ [N ])

The corresponding augmented Lagrangian (see e.g. [4  5]) is given by

L (W  B  A) =

1
γ

E (W) +

N

Xn=1(cid:18)Ψ Bn(n)(cid:1) − hAn  W − Bni +

β

2(cid:19)  
2 kW − Bnk2

where h· ·i denotes the scalar product between tensors  β is a positive parameter and A1  . . . AN ∈
Rp1×···×pN are the set of Lagrange multipliers associated with the constraints in problem (8).

ADMM is based on the following iterative scheme

(8)

(9)

(10)

(11)

(12)

n

W [i+1] ← argmin
B[i+1]
← argmin
← A[i]

W L(cid:16)W  B[i]  A[i](cid:17)
Bn L(cid:16)W [i+1]  B  A[i](cid:17)
n −(cid:16)βW [i+1] − B[i+1]
(cid:17) .

A[i+1]

n

n

Step (12) is straightforward  whereas step (10) is described in [9]. Here we focus on the step (11)
since this is the only problem which involves function Ψ. We restate it with more explanatory
notations as

By completing the square in the right hand side  the solution of this problem is given by

argmin

Bn(n) (cid:26)Ψ Bn(n)(cid:1) −(cid:10)An(n)  W(n) − Bn(n)(cid:11) +

β

ˆBn(n) = prox 1

β Ψ (X) := argmin

Bn(n) (cid:26) 1

β

Ψ Bn(n)(cid:1) +

2

2(cid:27) .

2 (cid:13)(cid:13)W(n) − Bn(n)(cid:13)(cid:13)
2(cid:27)  
2(cid:13)(cid:13)Bn(n) − X(cid:13)(cid:13)

1

2

where X = W(n) − 1

know that if ψ is a gauge function then

β An(n). By using properties of proximity operators (see e.g. [2  Prop. 3.1]) we

prox 1

β Ψ (X) = UX diag(cid:16)prox 1

β ψ (σ(X))(cid:17)V ⊤X  

where UX and VX are the orthogonal matrices formed by the left and right singular vectors of
X  respectively. If we choose ψ = k·k1 the associated proximity operator is the well-known soft
thresholding operator  that is  prox 1

(σ) = v  where the vector v has components

β k·k1

vi = sign (σi)(cid:18)|σi| −

1

β(cid:19) .

On the other hand  if we choose ψ = ω∗∗α   we need to compute prox 1
describe a method to accomplish this task.

β ω∗∗
α

. In the next section  we

4.2 Computation of the Proximity Operator

To compute the proximity operator of the function 1
calculus. First  we use the formula (see e.g. [7]) proxg∗ (x) = x − proxg (x) for g∗ = 1
we use a property of conjugate functions from [21  13]  which states that g(·) = 1
by the scaling property of proximity operators [7]  we have that proxg (x) = 1

β ω∗∗α we will use several properties of proximity
β ω∗∗α . Next
β ω∗α(β·). Finally 

(βx).

β proxβω∗

α

2The somewhat cumbersome notation Bn(n) denotes the mode-n matricization of tensor Bn  that is 

Bn(n) = (Bn)(n).

5

Algorithm 1 Computation of proxβω∗

α

(y)

Input: y ∈ Rd  α  β > 0.
Output: ˆw ∈ Rd.
Initialization: initial step τ0 = 1
for t = 1  2  . . . do

2   initial and best found solution w0 = ˆw = PS (y) ∈ Rd.

τ ← τ0√t
Find k such that k ∈ argmax(cid:8)αkwt−1
1:r k2 − r : 0 ≤ r ≤ d(cid:9)
1:k (cid:18)1 + αβ
1:k − τ(cid:18)wt−1
1:k k2(cid:19) − y1:k(cid:19)
˜w1:k ← wt−1
kwt−1
˜wk+1:d ← wt−1
k+1:d − τ wt−1
k+1:d − yk+1:d(cid:1)
wt ← ˜PS ( ˜w)
If h(wt) < h( ˆw) then ˆw ← wt

If “Stopping Condition = True” then terminate.

end for

It remains to compute the proximity operator of a multiple of the function ω∗α in equation (13)  that
is  for any β > 0  y ∈ S  we wish to compute

proxβω∗

α

(y) = argmin

{h (w) : w ∈ S}

w

where we have deﬁned S := {w ∈ Rd : w1 ≥ ··· ≥ wd ≥ 0} and

h (w) =

1
2 kw − yk2

2 + β

d

r=0 {αkw1:rk2 − r} .
max

In order to solve this problem we employ the projected subgradient method  see e.g. [6]. It consists
in applying two steps at each iteration. First  it advances along a negative subgradient of the current
solution; second  it projects the resultant point onto the feasible set S. In fact  according to [6]  it
is sufﬁcient to compute an approximate projection  a step which we describe in Appendix D. To
{αkw1:rk2 − r}.
compute a subgradient of h at w  we ﬁrst ﬁnd any integer k such that k ∈
Then  we calculate a subgradient g of the function h at w by the formula

argmax

r=0

d

gi =( (cid:16)1 + αβ
wi − yi 

kw1:kk2(cid:17) wi − yi 

if i ≤ k 
otherwise.

Now we have all the ingredients to apply the projected subgradient method  which is summarized
in Algorithm 1. In our implementation we stop the algorithm when an update of ˆw is not made for
more than 102 iterations.

5 Experiments

We have conducted a set of experiments to assess whether there is any advantage of using the pro-
posed regularizer over the tensor trace norm for tensor completion3. First  we have designed a
synthetic experiment to evaluate the performance of both approaches under controlled conditions.
Then  we have tried both methods on two tensor completion real data problems. In all cases  we have
used a validation procedure to tune the hyper-parameter γ  present in both approaches  among the

values(cid:8)10j : j = −7 −6  . . .   1(cid:9). In our proposed approach there is one further hyper-parameter 

α  to be speciﬁed. It should take the value of the Euclidean norm of the underlying tensor. Since
this is unknown  we propose to use the estimate

ˆα =vuutkwk2

2 + (mean(w)2 + var(w))! N
Yi=1

pi − m#  

where m is the number of known entries and w ∈ Rm contains their values. This estimator assumes
that each value in the tensor is sampled from N (mean(w)  var(w))  where mean(w) and var(w)
are the average and the variance of the elements in w.

3The code is available at http://romera-paredes.com/code/tensor-completion

6

Tensor Trace Norm
Proposed Regularizer

0.0115

0.011

0.0105

0.01

0.0095

0.009

E
S
M
R

0.0085

−5

−4

−3

2
log σ

−2

−1

s
d
n
o
c
e
S

3000

2500

2000

1500

1000

500

0

Tensor Trace Norm
Proposed Regularizer

50

100

p

150

200

Figure 1: Synthetic dataset: (Left) Root Mean Squared Error (RMSE) of tensor trace norm and the
proposed regularizer. (Right) Running time execution for different sizes of the tensor.

5.1 Synthetic Dataset

We have generated a 3-order tensor W 0 ∈ R40×20×10 by the following procedure. First we gener-
ated a tensor W with ranks (12  6  3) using Tucker decomposition (see e.g. [16])

12

6

3

Wi1 i2 i3 =

Xj1=1

Xj2=1

Xj3=1

Cj1 j2 j3 M (1)

i1 j1

M (2)
i2 j2

M (3)
i3 j3

 

(i1  i2  i3) ∈ [40] × [20] × [10]

where each entry of the Tucker decomposition components is sampled from the standard Gaussian

distribution N (0  1). We then created the ground truth tensor W 0 by the equation

i1 i2 i3 = Wi1 i2 i3 − mean(W)

√N std(W)

W 0

+ ξi1 i2 i3

where mean(W) and std(W) are the mean and standard deviation of the elements of W  N is
the total number of elements of W  and the ξi1 i2 i3 are i.i.d. Gaussian random variables with zero
mean and variance σ2. We have randomly sampled 10% of the elements of the tensor to compose
the training set  45% for the validation set  and the remaining 45% for the test set. After repeating
this process 20 times  we report the average results in Figure 1 (Left). Having conducted a paired
t-test for each value of σ2  we conclude that the visible differences in the performances are highly
signiﬁcant  obtaining always p-values less than 0.01 for σ2 ≤ 10−2.

Furthermore  we have conducted an experiment to test the running time of both approaches. We

have generated tensors W 0 ∈ Rp×p×p for different values of p ∈ {20  40  . . .   200}  following

the same procedure as outlined above. The results are reported in Figure 1 (Right). For low values
of p  the ratio between the running time of our approach and that of the trace norm regularization
method is quite high. For example in the lowest value tried for p in this experiment  p = 20  this
ratio is 22.661. However  as the volume of the tensor increases  the ratio quickly decreases. For
example  for p = 200  the running time ratio is 1.9113. These outcomes are expected because when
p is low  the most demanding routine in our method is the one described in Algorithm 1  where

p increases the singular value decomposition routine  which is common to both methods  becomes

each iteration is of order O (p) and O p2(cid:1) in the best and worst case  respectively. However  as
the most demanding because it has a time complexity O p3(cid:1) [10]. Therefore  we can conclude

that even though our approach is slower than the trace norm based method  this difference becomes
much smaller as the size of the tensor increases.

5.2 School Dataset

The ﬁrst real dataset we have tried is the Inner London Education Authority (ILEA) dataset. It is
composed of examination marks ranging from 0 to 70  of 15362 students who are described by a set
of attributes such as school and ethnic group. Most of these attributes are categorical  thereby we can
think of exam mark prediction as a tensor completion problem where each of the modes corresponds
to a categorical attribute. In particular  we have used the following attributes: school (139)  gender
(2)  VR-band (3)  ethnic (11)  and year (3)  leading to a 5-order tensor W ∈ R139×2×3×11×3.

7

E
S
M
R

11.6

11.4

11.2

11

10.8

10.6

10.4

4000

Tensor Trace Norm
Proposed Regularizer

6000

8000

10000
m (Training Set Size)

12000

E
S
M
R

42

40

38

36

34

32

30

28

26

24

2

4

6

Tensor Trace Norm
Proposed Regularizer

8

12
m (Training Set Size)

10

14

16

4
x 10

Figure 2: Root Mean Squared Error (RMSE) of tensor trace norm and the proposed regularizer for
ILEA dataset (Left) and Ocean video (Right).

We have selected randomly 5% of the instances to make the test set and another 5% of the instances
for the validation set. From the remaining instances  we have randomly chosen m of them for several
values of m. This procedure has been repeated 20 times and the average performance is presented
in Figure 2 (Left). There is a distinguishable improvement of our approach with respect to tensor
trace norm regularization for values of m > 7000. To check whether this gap is signiﬁcant  we have
conducted a set of paired t-tests in this regime. In all these cases we obtained a p-value below 0.01.

5.3 Video Completion

In the second real-data experiment we have performed a video completion test. Any video can be
treated as a 4-order tensor: “width” × “height” × “RGB” × “video length”  so we can use tensor
completion algorithms to rebuild a video from a few inputs  a procedure that can be useful for
compression purposes. In our case  we have used the Ocean video  available at [17]. This video

sequence can be treated as a tensor W ∈ R160×112×3×32. We have randomly sampled m tensors
elements as training data  5% of them as validation data  and the remaining ones composed the test
set. After repeating this procedure 10 times  we present the average results in Figure 2 (Right). The
proposed approach is noticeably better than the tensor trace norm in this experiment. This apparent
outcome is strongly supported by the paired t-tests which we run for each value of m  obtaining
always p-values below 0.01  and for the cases m > 5 × 104  we obtained p-values below 10−6.

6 Conclusion

In this paper  we proposed a convex relaxation for the average of the rank of the matricizations of
a tensor. We compared this relaxation to a commonly used convex relaxation used in the context
of tensor completion  which is based on the trace norm. We proved that this second relaxation is
not tight and argued that the proposed convex regularizer may be advantageous. Our numerical
experience indicates that our method consistently improves in terms of estimation error over tensor
trace norm regularization  while being computationally comparable on the range of problems we
considered. In the future it would be interesting to study methods to speed up the computation of the
proximity operator of our regularizer and investigate its utility in tensor learning problems beyond
tensor completion such as multilinear multitask learning [20].

Acknowledgements

We wish to thank Andreas Argyriou  Raphael Hauser  Charles Micchelli and Marco Signoretto for
useful comments. A valuable contribution was made by one of the anonymous referees. Part of this
work was supported by EPSRC Grant EP/H017178/1  EP/H027203/1 and Royal Society Interna-
tional Joint Project 2012/R2.

8

References

[1] A. Argyriou  R. Foygel and N. Srebro. Sparse Prediction with the k-Support Norm. Advances in Neural

Information Processing Systems 25  pages 1466–1474  2012.

[2] A. Argyriou  C.A. Micchelli  M. Pontil  L. Shen and Y. Xu. Efﬁcient ﬁrst order methods for linear com-

posite regularizers. arXiv:1104.1436  2011.

[3] R. Bhatia. Matrix Analysis. Springer Verlag  1997.

[4] D.P. Bertsekas  J.N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall 

1989.

[5] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning  3(1):1–
122  2011.

[6] S. Boyd  L. Xiao  A. Mutapcic. Subgradient methods  Stanford University  2003.

[7] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In Fixed-Point Al-
gorithms for Inverse Problems in Science and Engineering (H. H. Bauschke et al. Eds)  pages 185–212 
Springer  2011.

[8] M. Fazel  H. Hindi  and S. Boyd. A rank minimization heuristic with application to minimum order

system approximation. Proc. American Control Conference  Vol. 6  pages 4734–4739  2001.

[9] S. Gandy  B. Recht  I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimiza-

tion. Inverse Problems  27(2)  2011.

[10] G. H. Golub  C. F. Van Loan. Matrix Computations. 3rd Edition. Johns Hopkins University Press  1996.

[11] Z. Harchaoui  M. Douze  M. Paulin  M. Dudik  J. Malick. Large-scale image classiﬁcation with trace-
norm regularization. IEEE Conference on Computer Vision & Pattern Recognition (CVPR)  pages 3386–
3393  2012.

[12] J-B. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms  Part I. Springer 

1996.

[13] J-B. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms  Part II. Springer 

1993.

[14] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press  2005.

[15] A. Karatzoglou  X. Amatriain  L. Baltrunas  N. Oliver. Multiverse recommendation: n-dimensional ten-
sor factorization for context-aware collaborative ﬁltering. Proc. 4th ACM Conference on Recommender
Systems  pages 79–86  2010.

[16] T.G. Kolda and B.W. Bade. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

[17] J. Liu  P. Musialski  P. Wonka  J. Ye. Tensor completion for estimating missing values in visual data.

Proc. 12th International Conference on Computer Vision (ICCV)  pages 2114–2121  2009.

[18] Y. Nesterov. Gradient methods for minimizing composite objective functions. ECORE Discussion Paper 

2007/96  2007.

[19] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research  12:3413–

3430  2009.

[20] B. Romera-Paredes  H. Aung  N. Bianchi-Berthouze and M. Pontil. Multilinear multitask learning. Proc.

30th International Conference on Machine Learning (ICML)  pages 1444–1452  2013.

[21] N. Z. Shor. Minimization Methods for Non-differentiable Functions. Springer  1985.

[22] M. Signoretto  Q. Tran Dinh  L. De Lathauwer  J.A.K. Suykens. Learning with tensors: a framework

based on convex optimization and spectral regularization. Machine Learning  to appear.

[23] M. Signoretto  R. Van de Plas  B. De Moor  J.A.K. Suykens. Tensor versus matrix completion: a com-

parison with application to spectral data. IEEE Signal Processing Letters  18(7):403–406  2011.

[24] N. Srebro  J. Rennie and T. Jaakkola. Maximum margin matrix factorization. Advances in Neural Infor-

mation Processing Systems (NIPS) 17  pages 1329–1336  2005.

[25] R. Tomioka  K. Hayashi  H. Kashima  J.S.T. Presto. Estimation of low-rank tensors via convex optimiza-

tion. arXiv:1010.0789  2010.

[26] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured Schatten norm regularization.

arXiv:1303.6370  2013.

[27] R. Tomioka  T. Suzuki  K. Hayashi  H. Kashima. Statistical performance of convex tensor decomposition.

Advances in Neural Information Processing Systems (NIPS) 24  pages 972–980  2013.

9

,Bernardino Romera-Paredes
Massimiliano Pontil
Medhini Narasimhan
Svetlana Lazebnik
Alexander Schwing
Deepak Pathak
Christopher Lu
Trevor Darrell
Phillip Isola
Alexei Efros