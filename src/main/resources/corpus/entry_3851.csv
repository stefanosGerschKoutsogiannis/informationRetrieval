2016,Object based Scene Representations using Fisher Scores of Local Subspace Projections,Several works have shown that deep CNN classifiers can be easily transferred across datasets  e.g. the transfer of a CNN trained to recognize objects on ImageNET to an object detector on Pascal VOC. Less clear  however  is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification that should leverage localized object detections to recognize holistic visual concepts. While this problem is currently addressed with Fisher vector representations  these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model  the Gaussian mixture of diagonal covariances  which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model  the mixture of factor analyzers (MFA)  which approximates the non-linear data manifold by a collection of local subspaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually  outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary  in the sense that their combination outperforms each of the representations by itself. When combined  they produce a state of the art scene classifier.,Object based Scene Representations using Fisher

Scores of Local Subspace Projections

Mandar Dixit and Nuno Vasconcelos

Department of Electrical and Computer Engineering

University of California  San Diego
{mdixit  nvasconcelos}@ucsd.edu

Abstract

Several works have shown that deep CNNs can be easily transferred across datasets 
e.g. the transfer from object recognition on ImageNet to object detection on Pascal
VOC. Less clear  however  is the ability of CNNs to transfer knowledge across tasks.
A common example of such transfer is the problem of scene classiﬁcation  that
should leverage localized object detections to recognize holistic visual concepts.
While this problems is currently addressed with Fisher vector representations  these
are now shown ineffective for the high-dimensional and highly non-linear features
extracted by modern CNNs. It is argued that this is mostly due to the reliance on
a model  the Gaussian mixture of diagonal covariances  which has a very limited
ability to capture the second order statistics of CNN features. This problem is
addressed by the adoption of a better model  the mixture of factor analyzers (MFA) 
which approximates the non-linear data manifold by a collection of local sub-spaces.
The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an
image representation for holistic image classiﬁers. Extensive experiments show
that the MFA-FS has state of the art performance for object-to-scene transfer and
this transfer actually outperforms the training of a scene CNN from a large scene
dataset. The two representations are also shown to be complementary  in the sense
that their combination outperforms each of the representations by itself. When
combined  they produce a state-of-the-art scene classiﬁer.

1

Introduction

In recent years  convolutional neural networks (CNNs) trained on large scale datasets have achieved
remarkable performance on traditional vision problems such as image classiﬁcation [8  18  26]  object
detection and localization [5  16] and others. The success of CNNs can be attributed to their ability
to learn highly discriminative  non-linear  visual transformations with the help of supervised back-
propagation [9]. Beyond the impressive  sometimes even superhuman  results on certain datasets 
a remarkable property of these classiﬁers is the solution of the dataset bias problem [20] that has
plagued computer vision for decades. It has now been shown many times that a network trained to
solve a task on a certain dataset (e.g. object recognition on ImageNet) can be very easily ﬁne-tuned to
solve a related problem on another dataset (e.g. object detection on the Pascal VOC or MS-COCO).
Less clear  however  is the robustness of current CNNs to the problem of task bias  i.e. their ability to
generalize accross tasks. Given the large number of possible vision tasks  it is impossible to train a
CNN from scratch for each. In fact  it is likely not even feasible to collect the large number of images
needed to train effective deep CNNs for every task. Hence  there is a need to investigate the problem
of task transfer.
In this work  we consider a very common class of such problems  where a classiﬁer trained on a class
of instances is to be transferred to a second class of instances  which are loose combinations of the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

original ones. In particular  we consider the problem where the original instances are objects and
the target instances are scene-level concepts that somehow depend on those objects. Examples of
this problem include the transfer of object classiﬁers to tasks such as scene classiﬁcation [6  11  2] or
image captioning [23]. In all these cases  the goal is to predict holistic scene tags from the scores (or
features) from an object CNN classiﬁer. The dependence of the holistic descriptions on these objects
could range from very explicit to very subtle. For example  on the explicit end of the spectrum  an
image captioning system could produce sentence such as “a person is sitting on a stool and feeding a
zebra.” On the other hand  on the subtle end of the spectrum  a scene classiﬁcation system would
leverage the recognition of certain rocks  tree stumps  bushes and a particular lizard species to label
an image with the tag “Joshua Tree National Park”. While it is obviously possible 1) to collect a large
dataset of images  and 2) use them to train a CNN to directly solve each of these tasks  this approach
has two main limitations. First  it is extremely time consuming. Second  the “directly learned” CNN
will typically not accommodate explicit relations between the holistic descriptions and the objects in
the scene. This has  for example  been documented in the scene classiﬁcation literature  where the
performance of the best “directly learned” CNNs [26]  can be substantially improved by fusion with
object recognition CNNs [6  11  2].
So far  the transfer from object CNNs to holistic scene description has been most extensively studied
in the area of scene classiﬁcation  where state of the art results have been obtained with the bag of
semantics representation of [2]. This consists of feeding image patches through an object recognition
CNN  collecting a bag vectors of object recognition scores  and embedding this bag into a ﬁxed
dimensional vector space with recourse to a Fisher vector [7]. While there are variations of detail 
all other competitive methods are based on a similar architecture [6  11]. This observation is  in
principle  applicable to other tasks. For example  the state of the art in image captioning is to use a
CNN as an image encoder that extracts a feature vector from the image. This feature vector is the fed
to a natural language decoder (typically an LSTM) that produces sentences. While there has not yet
been an extensive investigation of the best image encoder  it is likely that the best representations for
scene classiﬁcation should also be effective encodings for language generation. For these reasons 
we restrict our attention to the scene classifcation problem in the remainder of this work  focusing
on the question of how to address possible limitations of the Fisher vector embedding. We note 
in particular  that while Fisher vectors have been classically deﬁned using gradients of image log-
likelihood with respect to the means and variances of a Gaussian mixture model (GMM) [13]  this
deﬁnition has not been applied universally in the CNN transfer context  where variance statistics are
often disregarded [6  2].
In this work we make several contributions to the use of Fisher vector type of representations for
object to scene transfer. The ﬁrst is to show that  for object recognition scores produced by a CNN [2] 
variance statistics are much less informative of scene class distributions than the mean gradients  and
can even degrade scene classiﬁcation performance. We then argue that this is due to the inability of the
standard GMM of diagonal covariances to provide a good approximation to the non-linear manifold
of CNN responses. This leads to the adoption of a richer generative model  the mixture of factor
analyzers (MFA) [4  22]  which locally approximates the scene class manifold by low-dimensional
linear spaces. Our second contribution is to show that  by locally projecting the feature data into
these spaces  the MFA can efﬁciently model its local covariance structure. For this  we derive the
Fisher score of the MFA model  denoted the MFA Fisher score (MFA-FS)  a representation similar to
the GMM Fisher vector of [13  17]. We show that  for high dimensional CNN features  the MFA-FS
captures highly discriminative covariance statistics  which were previously unavailable in [6  2] 
producing signiﬁcantly improved scene classiﬁcation over the conventional GMM Fisher vector. The
third contribution is a detailed experimental investigation of the MFA-FS. Since this can be seen
as a second order pooling mechanism  we compare it to a number of recent methods for second
order pooling of CNN features [21  3]. Although these methods describe global covariance structure 
they lack the ability of the MFA-FS to capture that information along locally linear approximations
of the highly non-linear CNN feature manifold. This is shown to be important  as the MFA-FS is
shown to outperform all these representations by non-trivial margins. Finally  we show that the
MFA-FS enables effective task transfer  by showing that MFA-FS vectors extracted from deep CNNs
trained for ImageNet object recognition [8  18]  achieve state-of-the-art results on challenging scene
recognition benchmarks  such as SUN [25] and MIT Indoor Scenes [14].

2

2 Fisher scores
In computer vision  an image I is frequently interpreted as a set of descriptors D = {x1  . . .   xn}
sampled from some generative model p(x; θ). Since most classiﬁers require ﬁxed-length inputs  it
is common to map the set D into a ﬁxed-length vector. A popular mapping consists of computing
∂θ log p(D; θ) for a model θb. This
the gradient (with respect to θ) of the log-likelihood ∇θL(θ) = ∂
is known as the Fisher score of θ. This gradient vector is often normalized by the square root
of the Fisher information matrix F  according to F− 1
2∇θL(θ). This is referred to as the Fisher
vector (FV) [7] representation of I. While the Fisher vector is frequently used with a Gaussian
mixture model (GMM) [13  17]  any generative model p(x; θ) can be used. However  the information
matrix is not always easy to compute. When this is case  it is common to rely on the simpler
representation of I by the score ∇θL(θ). This is  for example  the case with the sparse coded gradient
vectors in [11]. We next show that  for models with hidden variables  the Fisher score can be obtained
trivially from the steps of the expectation maximization (EM) algorithm commonly used to learn
such models.

2.1 Fisher Scores from EM

Consider the log-likelihood of D under a latent-variable model log p(D; θ) = log(cid:82) p(D  z; θ)dz of

hidden variable z. Since the left-hand side does not depend on the hidden variable  this can be written
in an alternate form  which is widely used in the EM literature 

log p(D; θ) =

q(z) log p(D  z; θ)dz −

q(z) log q(z)dz +

q(z) log

q(z)

p(z|D; θ)

dz

= Q(q; θ) + H(q) + KL(q||p; θ)

(1)
where Q(q; θ) is the “Q” function  q(z) a general probability distribution  H(q) its differential entropy
and KL(q||p; θ) the Kullback Liebler divergence between the posterior p(z|D; θ) and q(z). Hence 

(cid:90)

(cid:90)

log p(D; θ) =

∂
∂θ

where

∂
∂θ

Q(q; θ) +

KL(q||p; θ)

∂
∂θ

(2)

KL(q||p; θ) = −

(3)
In each iteration of the EM algorithm the q distribution is chosen as q(z) = p(z|D; θb)  where θb is a
reference parameter vector (the parameter estimates from the previous EM iteration) and

p(z|D; θ)

q(z)

p(z|D; θ)dz.

∂
∂θ

∂
∂θ

Q(q; θ) =

p(z|D; θb) log p(D  z; θ)dz = Ez|D;θb [log p(D  z; θ)].

(cid:12)(cid:12)(cid:12)(cid:12)θ=θb

It follows that
KL(q||p; θ)

∂
∂θ

and

(cid:90)

p(z|D; θb)

(cid:90) p(z|D; θb)
(cid:12)(cid:12)(cid:12)(cid:12)θ=θb

(cid:90)

(cid:90)

3

= −

p(z|D; θ)

∂
∂θ

dz = − ∂
∂θ

p(z|D; θ)

(4)

dz = 0

(cid:12)(cid:12)(cid:12)(cid:12)θ=θb

(cid:12)(cid:12)(cid:12)(cid:12)θ=θb

(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)θ=θb

log p(D; θ)

∂
∂θ

Q(p(z|D; θb); θ)

∂
∂θ

.

=

(5)
In summary  the Fisher score ∇θL(θ)|{θ=θb} of background model θb is the gradient of the Q-function
of EM evaluated at reference model θb. The computation of the score thus simpliﬁes into the two
steps of EM. First  the E step computes the Q function Q(p(z|x; θb); θ) at the reference θb. Second 
the M-step evaluates the gradient of the Q function with respect to θ at θ = θb. This interpretation
of the Fisher score is particularly helpful when efﬁcient implementations of the EM algorithm are
available  e.g. the recursive Baum-Welch computations commonly used to learn hidden Markov
models [15]. For more tractable distributions  such as the GMM  it enables the simple reuse of the
EM equations  which are always required to learn the reference model θb  to compute the Fisher
score.

Q(p(z|D; θb); θ) =

(cid:104)(cid:88)

(cid:88)
(cid:88)

i

Ezi|xi;θb
hik log p(xi|zi = k; θ)wk

k

I(zi  k) log p(xi  k; θ)

(cid:105)

2.2 Bag of features

Fisher scores are usually combined with the bag-of-features representation  where an image is
described as an orderless collection of localized descriptors D = {x1  x2  . . . xn}. These were tradi-
tionally SIFT descriptors  but have more recently been replaced with responses of object recognition
CNNs [6  1  2]. In this work we use the semantic features proposed in [2]  which are obtained by
transforming softmax probability vectors pi  obtained for image patches  into their natural parameter
form. These features were shown to perform better than activations of other CNN layers [2].

2.3 Gaussian Mixture Fisher Vectors

A GMM is a model with a discrete hidden variable that determines the mixture component which
explains the observed data. The generative process is as follows. A mixture component zi is ﬁrst
sampled from a multinomial distribution p(z = k) = wk. An observation xi is then sampled from
the Gaussian component p(x|z = k) ∼ G(x  µk  σk) of mean µk and variance σk. Both the hidden
and observed variables are sampled independently  and the Q function simpliﬁes to

i k

=

(6)
where I(.) is the indicator function and hik is the posterior probability p(k|xi; θb). The probability
vectors hi are the only quantities computed in the E-step.
In the Fisher vector literature [13  17]  the GMM is assumed to have diagonal covariances. This is
denoted as the variance-GMM. Substituting the expressions of p(xi|zi = k; θ) and differentiating the
Q function with respect to parameters θ = {µk  σk} leads to the two components of the Fisher score

L(θ) =

i

(7)

L(θ) =

Gσd

k

Gµd

k

∂
∂σd
k

(I) =

(I) =

∂
∂µd
k

(cid:88)

p(k|xi)

p(k|xi)

i − µd
(σd
k)2
i − µd
k)2
− 1
(8)
(σd
σd
k)3
k
These quantities are evaluated using a reference model θb = {µb
k} learned (with EM) from all
k  σb
training data. To compute the Fisher vectors  scores in (7) and (8) are often scaled by an approximate
Fisher information matrix  as detailed in [17]. When used with SIFT descriptors  these mean
and variance scores usually capture complimentary discriminative information  useful for image
classiﬁcation [13]. Yet  FVs computed from CNN features only use the mean gradients similar
to (7)  ignoring second-order statistics [6  2]. In the experimental section  we show that the variance
statistics of CNN features perform poorly compared to the mean gradients. This is perhaps due to the
inability of the variance-GMM to accurately model data in high dimensions. We test this hypothesis
by considering a model better suited for this task.

(cid:21)

.

(cid:19)

k

(cid:88)

i

(cid:18) xd
(cid:20) (xd

2.4 Fisher Scores for the Mixture of Factor Analyzers

A factor analyzer (FA) is a type of a Gaussian distribution that models high dimensional observations
x ∈ RD in terms of latent variables or “factors” z ∈ RR deﬁned on a low-dimensional subspace
R << D [4]. The process can be written as x = Λz +   where Λ is known as the factor loading
matrix and  models the additive noise in dimensions of x. Factors z are assumed distributed as
G(z  0  I) and the noise is assumed to be G(  0  ψ)  where ψ is a diagonal matrix. It can be shown
that x has full covariance S = ΛΛT + ψ  making the FA better suited for high dimensional modeling
than a Gaussian of diagonal covariance.
A mixture of factor analyzers (MFA) is an extension of the FA that allows a piece-wise linear
approximation of a non-linear data manifold. Unlike the GMM  it has two hidden variables: a discrete
variable s  p(s = k) = wk  which determines the mixture assignments and a continuous latent
variable z ∈ RR  p(z|s = k) = G(z  0  I)  which is a low dimensional projection of the observation
variable x ∈ RD  p(x|z  s = k) = G(x  Λkz + µk  ψ). Hence  the kth MFA component is a FA of
mean µk and subspace deﬁned by Λk. Overall  the MFA components approximate the distribution of

4

the observations x by a set of sub-spaces in observation space. The Q function is

(cid:88)

Q(p(s  z|D; θb); θ) =

(cid:88)

(cid:104)(cid:88)

k

Ezi si|xi;θb

i

I(si  k) log p(xi  zi  si = k; θ)

(cid:105)

(9)

(10)

(11)
(12)

=

hikEzi|xi;θb [log G(xi  Λkzi + µk  ψ) + log G(zi  0  I) + log wk] .
where hik = p(si = k|xi; θb). After some simpliﬁcations  the E step reduces to computing

i k

hik = p(k|xi; θb) ∝ wb

kN (xi  µb

k  Sb
k)

Ezi|xi;θb [zi] = βb

Ezi|xi;θb [zizT

i ] =

(cid:16)
k(xi − µb
k)
I − βb
k + βb
kΛb

(cid:17)

(cid:0)Sb

k

k(xi − µb

(cid:1)−1. The M-step then evaluates the Fisher score of

k)T βbT

(13)

k

with Sb
θ = {µb

k

k = ΛbT

k + ψb and βb

kΛbT
k}. With some algebraic manipulations  this can be shown to have components
(cid:105)

p(k|xi; θb)ψb−1(cid:0)I − Λb

(cid:1)(cid:0)xi − µb
(cid:104)

(cid:88)
(cid:88)

k = Λb
k  Λb
Gµk (I) =
GΛk (I) =

p(k|xi; θb)ψb−1

k)T βbT

k − Λb

kβb
k
k − I)

(xi − µb

(Λb

kβb

k

i

i

(14)

(15)

.

k)(xi − µb
(cid:1)
k)(xi − µb

k

For a detailed discussion of the Q function  the reader is referred to the EM derivation in [4]. Note
that the scores with respect to the means are functionally similar to the ﬁrst order residuals in (7).
However  the scores with respect to the factor loading matrices Λk account for covariance statistics of
the observations xi  not just variances. We refer to the representations (14) and (15) as MFA Fisher
scores (MFA-FS). Note that these are not FVs due to the absence of normalization by the Fisher
information  which is more complex to compute than for the variance-GMM.

3 Related work

N (Λz  σ2I) conditioned on Laplace factors p(z) ∝(cid:81)
p(x) is intractable  it can be approximated by an evidence lower bound p(x) ≥ (cid:82) q(z) p(x z)
Λ. [21] proposed an alternative bilinear pooling mechanism(cid:80)

The most popular approach to transfer object scores (usually from an ImageNet CNN) into a feature
vector for scene classiﬁcation is to rely on FV-style pooling. Although most classiﬁers default to the
GMM-FV embedding [6  1  2  24]  some recent works have explored different encoding [11] and
pooling schemes [21  3] with promising results. Liu et al. [11] derived an FV like representation from
sparse coding. Their model can be described as a factor analyzer with Gaussian observations p(x|z) ∼
r exp(−|zr|). While the sparse FA marginal
q(z) dz
derived from a suitable variational posterior q(z). In [11]  q is a point posterior δ(z − z∗) and the
MAP inference simpliﬁes into sparse coding. The image representation is obtained using gradients
of the sparse coding objective evaluated at the MAP factors z∗  with respect to the factor loadings
i . Similar to the MFA-FS  this
captures second order statistics of CNN feature space  albeit globally. Due to its simplicity  this
mechanism supports ﬁne-tuning of the object CNN to scene classes. Gao et al. [3] have recently
shown that this representation can be compressed with minimal performance loss.

i xixT

4 Experiments

The MFA-FS was evaluated on the scene classiﬁcation problem  using the 67 class MIT Indoor scenes
dataset [14] and the 397 class MIT SUN dataset [25]. For Indoor scenes  a single training set of 80
images per class is provided by the authors. The test set consists of 20 images per class. Results are
reported as average per class classiﬁcation accuracy. The authors of SUN provide multiple train/test
splits  each test set containing 50 images per class. Results are reported as mean average per class
classiﬁcation accuracy over splits. Three object recognition CNNs  pre-trained on ImageNet  were
used to extract features: the 8 layer network of [8] (denoted as AlexNet) and the deeper 16 and 19
layer networks of [18] (denoted VGG-16 and VGG-19  respectively). These CNNs assign 1000
dimensional object recognition probabilities to P × P patches (sampled on a grid of ﬁxed spacing)
of the scene images  with P ∈ {128  160  96}. Patch probability vectors were converted into their
natural parameter form and PCA-reduced to 500 dimensions as in [2]. Each image was mapped into a

5

Table 1: Classiﬁcation accu-
racy (K = 50  R = 10).

MIT Indoor

GMM FV (µ)
GMM FV (σ)
MFA FS (µ)
MFA FS (Λ)

66.08
53.86
67.68
71.11

SUN
GMM FV (µ)
GMM FV (σ)
MFA FS (µ)
MFA FS (Λ)

50.01
37.71
51.43
53.38

Figure 1: Classiﬁcation accuracy vs. descriptor size for MFA-FS(Λ) of K = 50
components and R factor dimensions and GMM-FV(σ) of K components. Left:
MIT Indoor. Right: SUN.

GMM-FV [13] using a background GMM  and an MFA-FS  using (14)  (15) and a background MFA.
As usual in the FV literature  these vectors were power normalized  L2 normalized  and classiﬁed
with a cross-validated linear SVM. These classiﬁers were compared to scene CNNs  trained on the
large scale Places dataset. In this case  the features from the penultimate CNN layer were used as a
holistic scene representation and classiﬁed with a linear SVM  as in [26]. We used the places CNNs
with the AlexNet and VGG-16 architectures provided by the authors.

4.1

Impact of Covariance Modeling

We begin with an experiment to compare the modeling power of MFAs to variance-GMMs. This was
based on AlexNet features  mixtures of K = 50 components  and an MFA latent space dimension of
R = 10. Table 1 presents the classiﬁcation accuracy of a GMM-FV that only considers the mean
- GMM-FV(µ) - or variance - GMM-FV(σ) - parameters and a MFA-FS that only considers the
mean - MFA-FS(µ) - or covariance - MFA-FS(Λ) - parameters. The most interesting observation
is the complete failure of the GMM-FV (σ)  which under-performs the GMM-FV(µ) by more than
10%. The difference between the two components of the GMM-FV is not as startling for lower
dimensional SIFT features [13]. However  for CNN features  the discriminative power of variance
statistics is exceptionally low. This explains why previous FV representations for CNNs [6  2] only
consider gradients with respect to the means. A second observation of importance is that the improved
modeling of covariances by the MFA eliminates this problem. In fact  MFA-FS(Λ) is signiﬁcantly
better than both GMM-FVs. It could be argued that a fair comparison requires an increase in the
GMM modeling capacity. Fig. 1 tests this hypothesis by comparing GMM-FVs(σ) and MFA-FS
(Λ) for various numbers of GMM components (K ∈ {50  . . .   500}) and MFA hidden sub-spaces
dimensions (R ∈ {1  . . .   10}). For comparable vector dimensions  the covariance based scores
always signiﬁcantly outperforms the variance statistics on both datasets. A ﬁnal observation is that 
due to covariance modeling in MFAs  the MFA-FS(µ) performs better the GMM-FV(µ). The ﬁrst
order residuals pooled to obtain the MFA-FS(µ) (14) are scaled by covariance matrices instead of
variances. This local de-correlation provides a non-trivial improvement for the MFA-FS(µ) over the
GMM-FV(µ)(∼ 1.5% points). Covariance modeling was previously used in [19] to obtain FVs w.r.t.
Gaussian means and local subspace variances (eigen-values of covariance). Their subspace variance
FV  derived with our MFAs  performs much better than the variance GMM-FV (σ)  due to a better
underlying model (60.7% v 53.86% on Indoor). It is  however  still inferior to the MFA-FS(Λ) which
captures full covariance within local subspaces.
While a combination of the MFA-FS(µ) and MFA-FS(Λ) produces a small improvement (∼ 1%)  we
restrict to using the latter in the remainder of this work.

4.2 Multi-scale learning and Deep CNNs

Recent works have demonstrated value in combining deep CNN features extracted at multiple-scales.
Table 2 presents the classiﬁcation accuracies of the MFA-FS (Λ) based on AlexNet  and 16 and 19
layer VGG features extracted from 96x96  128x128 and 160x160 pixel image patches  as well as
their concatenation (3 scales)  as suggested by [2]. These results conﬁrm the beneﬁts of multi-scale
feature combination  which achieves the best performance for all CNNs and datasets.

6

00.511.522.53x 1050.40.450.50.550.60.650.70.750.8Descriptor DimensionsAccuracy MFA Grad (Factor Loading)GMM FV (Variance)K = 200K = 300K = 400K = 500K = 50R = 2K = 50R = 5K = 50R = 10K = 50R = 1K = 50K = 10000.511.522.53x 1050.20.250.30.350.40.450.50.55Descriptor DimensionsAccuracy MFA Grad (Factor Loading)GMM FV (Variance)K = 50R = 2K = 50R = 1K = 50R = 5K = 50R = 10K = 500K = 400K = 300K = 200K = 100K = 50Table 2: MFA-FS classiﬁcation ac-
curacy as a function of patch scale.

Table 3: Performance of scene classiﬁcation methods.
combination of patch scales (128  96  160).

*-

MIT
Indoor
AlexNet
69.83
71.11
70.51
73.58
VGG-16
77.26
77.28
79.57
80.1
VGG-19
77.21
79.39
79.9
81.43

160x160
128x128
96x96
3 scales

160x160
128x128
96x96
3 scales

160x160
128x128
96x96
3 scales

SUN

52.36
53.38
53.54
55.95

59.77
60.99
61.71
63.31

-
-
-
-

Method

MFA-FS + Places (VGG)

MFA-FS + Places (AlexNet)

MFA-FS (VGG)

MFA-FS (AlexNet)
Full BN (VGG) [3]

Compact BN (VGG) [3]
H-Sparse (VGG) [12]

Sparse Coding (VGG) [12]

Sparse Coding (AlexNet) [11]

MetaClass (AlexNet) + Places [24]
FV (AlexNet)(4 scales) + Places [2]
FV (AlexNet)(3 scales) + Places [2]

FV (AlexNet) (4 scales) [2]
FV (Alexnet)(3 scales) [2]

VLAD (AlexNet) [6]
FV+FC (VGG) [1]

Mid Level [10]

MIT Indoor

87.23
79.86
81.43
73.58
77.55
76.17
79.5
77.6
68.2
78.9
79.0
78.5∗
72.86
71.24
68.88
81.0
70.46

SUN
71.06
63.16
63.31
55.95

-
-
-
-

58.11
61.72

54.4
53.0
51.98

-

-
-

4.3 Comparison with ImageNet based Classiﬁers

We next compared the MFA-FS to state of the art scene classiﬁers also based on transfer from
ImageNet CNN features [11  1–3]. Since all these methods only report results for MIT Indoor  we
limited the comparison to this dataset  with the results of Table 4. The GMM-FV of [2] operates
on AlexNet CNN semantics extracted from image patches of multiple sizes (96  128  160  80). The
FV in [1] is computed using convolutional features from AlexNet or VGG-16 extracted in a large
multi-scale setting. Liu et al. proposed a gradient representation based on sparse codes. Their initial
results were reported on a single patch scale of 128x128 using AlexNet features [11]. More recently 
they have proposed an improved H-Sparse representation  combined multiple patch scales and used
VGG features in [12]. The recently proposed bilinear (BN) descriptor pooling of [21] is similar to
the MFA-FS in the sense that it captures global second order descriptor statistics. The simplicity of
these descriptors enables the ﬁne-tuning of the CNN layers to the scene classiﬁcation task. However 
their results  reproduced in [3] for VGG-16 features  are clearly inferior to those of the MFA-FS
without ﬁne-tuning. Gao et al. [3] propose a way to compress these bilinear statistics with trainable
transformations. For a compact image representation of size 8K  their accuracy is inferior to a
representation of 5K dimensions obtained by combining the MFA-FS with a simple PCA.
These experiments show that the MFA-FS is a state of the art procedure for task transfer from object
recognition (on ImageNet) to scene classiﬁcation (e.g. on MIT Indoor or SUN). Its closest competitor
is the classiﬁer of [1]  which combines CNN features in a massive multiscale setting ( 10 image sizes).
While MFA-FS outperforms [1] with only 3 image scales  its performance improves even further with
addition of more scales (82% with VGG  4 patch sizes).

4.4 Task transfer performance

The next question is how object-to-scene transfer compares to the much more intensive process 
pursued by [26]  of collecting a large scale labeled dataset and training a deep CNN from it. Their
scene dataset  known as Places  consists of 2.4M images  from which both AlexNet and VGG Net
CNNs were trained for scene classiﬁcation. The fully connected features from the networks are used
as scene representations and classiﬁed with linear SVMs on Indoor scenes and SUN. The Places
CNN features are a direct alternatives to the MFA-FS. While the use of the former is an example of
dataset transfer (features trained on scenes to classify scenes) the use of the latter is an example of
task transfer (features trained on objects to classify scenes).
A comparison between the two transfer approaches is shown in table 5. Somewhat surprisingly 
task transfer with the MFA-FS outperformed dataset transfer with the Places CNN  on both MIT
Indoors and SUN and for both the AlexNet and VGG architectures. This supports the hypothesis that
the variability of conﬁgurations of most scenes makes scene classiﬁcation much harder than object
recognition  to the point where CNN architectures that have close-to or above human performance for

7

Table 4: Comparison to task transfer methods (Ima-
geNet CNNs) on MIT Indoor.

Method

1 scale mscale

AlexNet

MFA-FS

GMM FV [2]
FV+FC [1]

Sparse Coding [11]

VGG

MFA-FS

Sparse Coding [12]

H-Sparse [12]

BN [3]

FV+FC [1]

71.11
68.5

-

68.2

73.58
72.86
71.6

-

79.9

77.55

-
-

-

81.43
77.6
79.5

-

81.0

VGG + dim. reduction

MFA-FS + PCA (5k)

BN (8k) [3]

79.3
76.17

-
-

Table 5: Comparison with the Places trained Scene
CNNs.

Method

SUN

Indoor

AlexNet

VGG

MFA-FS
Places

Combined

MFA-FS
Places

Combined

AlexNet + VGG

Places (VGG + Alex)

MFA-FS(Alex) + Places(VGG)
MFA-FS(VGG) + Places(Alex)

55.95
54.3
63.16

63.31
61.32
71.06

65.91
68.8
67.34

73.58
68.24
79.86

81.43
79.47
87.23

81.29
85.6
82.82

object recognition are much less effective for scenes. It is  instead  preferable to pool object detections
across the scene image  using a pooling mechanism such as the MFA-FS. This observation is in line
with an interesting result of [2]  showing that the object-based and scene-based representations are
complementary  by concatenating ImageNet- and Places-based feature vectors. By replacing the
the GMM-FV of [2] with the MFA-FS now proposed  we improve upon these results. For both the
AlexNet and VGG CNNs  the combination of the ImageNet-based MFA-FS and the Places CNN
feature vector outperformed both the MFA-FS and the Places CNN features by themselves  in both
SUN and MIT Indoor. To the best of our knowledge  no method using these or deeper CNNs has
reported better results than the combined MFA-FS and Places VGG features of Table 5.
It could be argued that this improvement is just an effect of the often observed beneﬁts of fusing
different classiﬁers. Many works even resort to “bagging” of multiple CNNs to achieve performance
improvements [18]. To test this hypothesis we also implemented a classiﬁer that combines two Places
CNNs with the AlexNet and VGG architectures. This is shown as Places (VGG+AlexNet) in the last
section of Table 5. While improving on the performance of both MFA-FS and Places  its performance
is not as good as that of the combination of the object-based and scene-based representations (MFA-
FS + Places). As shown in the remainder of the last section of the table  any combination of an object
CNN with MFA-FS based transfer and a scene CNN outperforms this classiﬁer.
Finally  table 3 compares results to the best recent scene classiﬁcation methods in the literature.
This comparison shows that MFA-FS + Places combination is a state-of-the-art classiﬁer with
substantial gains over all other proposals. The results of 71.06% on SUN and 87.23% on Indoor
scenes substantially outperform the previous best results of 61.7% and 81.7%  respectively.

5 Conclusion

It is now well established that deep CNNs can be transferred across datasets that address similar
tasks. It is less clear  however  whether they are robust to transfer across tasks. In this work  we have
considered a class of problems that involve this type of transfer  namely problems that beneﬁt from
transferring object detections into holistic scene level inference  eg. scene classiﬁcation. While such
problems have been addressed with FV-like representations in the past  we have shown that these are
not very effective for the high-dimensional CNN features. The reason is their reliance on a model  the
variance-GMM  with a limited ﬂexibility. We have addressed this problem by adopting a better model 
the MFA  which approximates the non-linear data manifold by a set of local sub-spaces. We then
introduced the Fisher score with respect to this model  denoted as the MFA-FS. Through extensive
experiments  we have shown that the MFA-FS has state of the art performance for object-to-scene
transfer and this transfer actually outperforms a scene CNN trained on a large scene dataset. These
results are signiﬁcant given that 1) MFA training takes only a few hours versus training a CNN  and
2) transfer requires a much smaller scene dataset.

8

References
[1] M. Cimpoi  S. Maji  I. Kokkinos  and A. Vedaldi. Deep ﬁlter banks for texture recognition  description 

and segmentation. International Journal of Computer Vision  2015.

[2] Mandar Dixit  Si Chen  Dashan Gao  Nikhil Rasiwasia  and Nuno Vasconcelos. Scene classiﬁcation with
semantic ﬁsher vectors. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
June 2015.

[3] Yang Gao  Oscar Beijbom  Ning Zhang  and Trevor Darrell. Compact bilinear pooling. CoRR 

[4] Zoubin Ghahramani and Geoffrey E. Hinton. The em algorithm for mixtures of factor analyzers. Technical

abs/1511.06062  2015.

report  1997.

[5] Ross Girshick  Jeff Donahue  Trevor Darrell  and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR)  2014.

[6] Yunchao Gong  Liwei Wang  Ruiqi Guo  and Svetlana Lazebnik. Multi-scale orderless pooling of deep
convolutional activation features. In Computer Vision ECCV 2014  volume 8695  pages 392–407  2014.
[7] Tommi S. Jaakkola and David Haussler. Exploiting generative models in discriminative classiﬁers. In
Proceedings of the 1998 conference on Advances in neural information processing systems II  pages
487–493  1999.

[8] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in Neural Information Processing Systems 25  pages 1097–1105  2012.

[9] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  November 1998.

[10] Yao Li  Lingqiao Liu  Chunhua Shen  and Anton van den Hengel. Mid-level deep pattern mining. In The

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2015.

[11] Lingqiao Liu  Chunhua Shen  Lei Wang  Anton Hengel  and Chao Wang. Encoding high dimensional local
features by sparse coding based ﬁsher vectors. In Advances in Neural Information Processing Systems 27 
pages 1143–1151. 2014.

[12] Lingqiao Liu  Peng Wang  Chunhua Shen  Lei Wang  Anton van den Hengel  Chao Wang  and Heng Tao
Shen. Compositional model based ﬁsher vector coding for image classiﬁcation. CoRR  abs/1601.04143 
2016.

[13] Florent Perronnin  Jorge Sánchez  and Thomas Mensink. Improving the ﬁsher kernel for large-scale image
classiﬁcation. In Proceedings of the 11th European conference on Computer vision: Part IV  ECCV’10 
pages 143–156  2010.

[14] A. Quattoni and A. Torralba. Recognizing indoor scenes. 2012 IEEE Conference on Computer Vision and
Pattern Recognition  0:413–420  2009. doi: http://doi.ieeecomputersociety.org/10.1109/CVPRW.2009.
5206537.

[15] L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition.

Proceedings of the IEEE  77(2):257–286  Feb 1989. ISSN 0018-9219. doi: 10.1109/5.18626.

[16] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster R-CNN: Towards real-time object

detection with region proposal networks. In Neural Information Processing Systems (NIPS)  2015.

[17] Jorge Sánchez  Florent Perronnin  Thomas Mensink  and Jakob J. Verbeek. Image classiﬁcation with the

ﬁsher vector: Theory and practice. International Journal of Computer Vision  105(3):222–245  2013.

[18] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

CoRR  abs/1409.1556  2014.

[19] Masayuki Tanaka  Akihiko Torii  and Masatoshi Okutomi. Fisher vector based on full-covariance gaussian
mixture model. IPSJ Transactions on Computer Vision and Applications  5:50–54  2013. doi: 10.2197/
ipsjtcva.5.50.

[20] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR’11  June 2011.
[21] Aruni RoyChowdhury Tsung-Yu Lin and Subhransu Maji. Bilinear cnn models for ﬁne-grained visual

recognition. In International Conference on Computer Vision (ICCV)  2015.

[22] Jakob Verbeek. Learning nonlinear image manifolds by global alignment of local linear models. IEEE

Transactions on Pattern Analysis and Machine Intelligence  28(8):1236–1250  August 2006.

[23] Oriol Vinyals  Alexander Toshev  Samy Bengio  and Dumitru Erhan. Show and tell: A neural image

caption generator. CoRR  abs/1411.4555  2014.

[24] Ruobing Wu  Baoyuan Wang  Wenping Wang  and Yizhou Yu. Harvesting discriminative meta objects
with deep cnn features for scene classiﬁcation. In The IEEE International Conference on Computer Vision
(ICCV)  December 2015.

[25] Jianxiong Xiao  J. Hays  K.A. Ehinger  A. Oliva  and A. Torralba. Sun database: Large-scale scene recog-
nition from abbey to zoo. In Computer Vision and Pattern Recognition (CVPR)  2010 IEEE Conference on 
pages 3485–3492  2010.

[26] B. Zhou  A. Lapedriza  J. Xiao  A. Torralba  and A. Oliva. Learning Deep Features for Scene Recognition

using Places Database. NIPS  2014.

9

,Xiao-Ming Wu
Zhenguo Li
Shih-Fu Chang
Dohyung Park
Constantine Caramanis
Sujay Sanghavi
Mandar Dixit
Nuno Vasconcelos