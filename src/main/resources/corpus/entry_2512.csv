2018,Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation,Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge  with modern learning-based approaches to achieve structured  robust  and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence  a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database  or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning  guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets  generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition  our model achieves the highest detection precision of medical abnormality terminologies  and improved human evaluation performance.,Hybrid Retrieval-Generation Reinforced Agent for

Medical Image Report Generation

Christy Y. Li∗
Duke University
yl558@duke.edu

Xiaodan Liang†

Carnegie Mellon University
xiaodan1@cs.cmu.edu

Zhiting Hu

Carnegie Mellon University
zhitingh@cs.cmu.edu

Eric P. Xing
Petuum  Inc

epxing@cs.cmu.edu

Abstract

Generating long and coherent reports to describe medical images poses challenges
to bridging visual patterns with informative human linguistic descriptions. We
propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent)
which reconciles traditional retrieval-based approaches populated with human prior
knowledge  with modern learning-based approaches to achieve structured  robust 
and diverse report generation. HRGR-Agent employs a hierarchical decision-
making procedure. For each sentence  a high-level retrieval policy module chooses
to either retrieve a template sentence from an off-the-shelf template database  or
invoke a low-level generation module to generate a new sentence. HRGR-Agent
is updated via reinforcement learning  guided by sentence-level and word-level
rewards. Experiments show that our approach achieves the state-of-the-art results
on two medical report datasets  generating well-balanced structured sentences with
robust coverage of heterogeneous medical report contents. In addition  our model
achieves the highest detection precision of medical abnormality terminologies  and
improved human evaluation performance.

1

Introduction

Beyond the traditional visual captioning task [41  28  43  40  18] that produces one single sentence 
generating long and topic-coherent stories or reports to describe visual contents (images or videos)
has recently attracted increasing research interests [19  35  22]  posed as a more challenging and
realistic goal towards bridging visual patterns with human linguistic descriptions. Particularly  report
generation has several challenges to be resolved: 1) The generated report is a long narrative consisting
of multiple sentences or paragraphs  which must have a plausible logic and consistent topics; 2) There
is a presumed content coverage and speciﬁc terminology/phrases  depending on the task at hand.
For example  a sports game report should describe competing teams  wining points  and outstanding
players [38]. 3) The content ordering is very crucial. For example  a sports game report usually talks
about the competition results before describing teams and players in detail.
As one of the most representative and practical report generation task  the desired medical image
report generation must satisfy more critical protocols and ensure the correctness of medical term
usage. As shown in Figure 1  a medical report consists of a ﬁndings section describing medical
observations in details of both normal and abnormal features  an impression or conclusion sentence

∗This work was conducted when Christy Y. Li was at Petuum  Inc.
†Corresponding author.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: An example of medical image report generation. The middle column is a report written by
radiologists for the chest x-ray image on the left column. The right column contains three reports
generated by a retrieval-based system (R)  a generation-based model (G) and our proposed model
(HRGR-Agent) respectively. The retrieval-based model correctly detects effusion while the generative
model fails to. Our HRGR-Agent detects effusion and also describes supporting evidence.

indicating the most prominent medical observation or conclusion  and comparison and indication
sections that list patient’s peripheral information. Among these sections  the ﬁndings section posed as
the most important component  ought to cover contents of various aspects such as heart size  lung
opacity  bone structure; any abnormality appearing at lungs  aortic and hilum; and potential diseases
such as effusion  pneumothorax and consolidation. And  in terms of content ordering  the narrative of
ﬁndings section usually follows a presumptive order  e.g. heart size  mediastinum contour followed
by lung opacity  remarkable abnormalities followed by mild or potential abnormalities.
State-of-the-art caption generation models [41  9  43  34] tend to perform poorly on medical report
generation with speciﬁc content requirements due to several reasons. First  medical reports are usually
dominated by normal ﬁndings  that is  a small portion of majority sentences usually forms a template
database. For these normal cases  a retrieval-based system (e.g. directly perform classiﬁcation
among a list of majority sentences given image features) can perform surprisingly well due to the
low variance of language. For instance  in Figure 1  a retrieval-based system correctly detects
effusion from a chest x-ray image  while a generative model that generates word-by-word given
image features  fails to detect effusion. On the other hand  abnormal ﬁndings which are relatively rare
and remarkably diverse  however  are of higher importance. Current text generation approaches [16]
often fail to capture the diversity of such small portion of descriptions  and pure generation pipelines
are biased towards generating plausible sentences that look natural by the language model but poor at
ﬁnding visual groundings [17]. On the contrary  a desirable medical report usually has to not only
describe normal and abnormal ﬁndings  but also support itself by visual evidences such as location
and attributes of the detected ﬁndings appearing in the image.
Inspired by the fact that radiologists often follow templates for writing reports and modify them
accordingly for each individual case [5  12  10]  we propose a Hybrid Retrieval-Generation Reinforced
Agent (HRGR-Agent) which is the ﬁrst attempt to incorporate human prior knowledge with learning-
based generation for medical reports. HRGR-Agent employs a retrieval policy module to decide
between automatically generating sentences by a generation module and retrieving speciﬁc sentences
from the template database  and then sequentially generates multiple sentences via a hierarchical
decision-making. The template database is built based on human prior knowledge collected from
available medical reports. To enable effective and robust report generation  we jointly train the
retrieval policy module and generation module via reinforcement learning (RL) [30] guided by
sentence-level and word-level rewards  respectively. Figure 1 shows an example generated report
by our HRGR-Agent which correctly describes "a small effusion" from the chest x-ray image  and
successfully supports its ﬁnding by providing the appearance ("blunting") and location ("costophrenic
sulcus") of the evidence.
Our main contribution is to bridge rule-based (retrieval) and learning-based generation via reinforce-
ment learning  which can achieve plausible  correct and diverse medical report generation. Moreover 
our HRGR-Agenet has several technical merits compared to existing retrieval-generation-based
models: 1) our retrieval and generation modules are updated and beneﬁt from each other via policy
learning; 2) the retrieval actions are regarded as a part of the generation whose selection of templates
directly inﬂuences the ﬁnal generated result. 3) the generation module is encouraged to learn diverse
and complicated sentences while the retrieval policy module learns template-like sentences  driven by
distinct word-level and sentence-level rewards  respectively. Other work such as [24] still enforces
the generative model to predict template-like sentences.

2

Comparison: Indication: 60-year-old male with seizure  ethanol abuseFindings: The heart size and mediastinal contours appear within normal limits. There is blunting of the right lateral costophrenic sulcus which could be secondary to a small effusion versus scarring. No focal airspace consolidation or pneumothorax. No acute bony abnormalities.Impression: Blunting of the right costophrenic sulcus could be secondary to a pleural effusion versus scarring. Findings: [R]: The heart size is normal. There is mild effusion. No acute bony abnormalities. [G]: The heart size normal. No pleural effusion or pneumothorax. No acute bony abnormalities. [HRGR-Agent]: The heart size and mediastinal contours are normal. There is blunting of costophrenic sulcus suggesting a small effusion. No bony abnormalities. We conduct extensive experiments on two medical image report dataset [8]. Our HRGR-Agent
achieves the state-of-the-art performance on both datasets under three kinds of evaluation metrics:
automatic metrics such as CIDEr [33]  BLEU [25] and ROUGE [20]  human evaluation  and detection
precision of medical terminologies. Experiments show that the generated sentences by HRGR-Agent
shares a descent balance between concise template sentences  and complicated and diverse sentences.

2 Related Work

Visual Captioning and Report Generation. Visual captioning aims at generating a descriptive
sentence for images or videos. State-of-the-art approaches use CNN-RNN architectures and attention
mechanisms [27  41  43  28]. The generated sequence is usually short  describing only the dominating
visual event  and is primarily rewarded by language ﬂuency in practice. Generating reports that are
informative and have multiple sentences [38  16] poses higher requirements on content selection 
relation generation  and content ordering. The task differs from image captioning [43  23] and
sentence generation [14  6] where usually single or few sentences are required  or summarization [2 
44] where summaries tend to be more diverse without clear template sentences. State-of-the-art
methods on report generation [16] are still remarkably cloning expert behaviour  and incapable
of diversifying language and depicting rare but prominent ﬁndings. Our approach prevents from
mimicking teacher behaviour by sparing the burden of automatic generative model with a template
selection and retrieval mechanism  which by design promotes language diversity and better content
selection.
Template Based Sequence Generation. Some of the recent approaches bridged generative language
approaches and traditional template-based methods. However  state-of-the-art approaches either
treat a retrieval mechanism as latent guidance [44]  the impact of which to text generation is limited 
or still encourage the generation network to mimic template-like sequences [24]. Our method is
close to previous copy mechanism work such as pointer-generator [2]  however  we are different
in that: 1) our retrieval module aims to retrieve from an external common template base  which is
particularly effective to the task  as opposed to copying from a speciﬁc source article; 2) we formulate
the retrieval-generation choices as discrete actions (as opposed to soft weights as in previous work)
and learn with hierarchical reinforcement learning for optimizing both short- and long-term goals.
Reinforcement Learning for Sequence Generation. Recently  reinforcement learning (RL) has
been receiving increasing popularity in sequence generation [27  3  13] such as visual captioning [21 
28  18]  text summarization [26]  and machine translation [39]. Traditional methods use cross entropy
loss which is prone to exposure bias [27  31] and do not necessarily optimize evaluation metrics such
as CIDEr [33]  ROUGE [20]  BLEU [25] and METEOR [4]. In contrast  reinforcement learning can
directly use the evaluation metrics as reward and update model parameters via policy gradient. There
has been some recent efforts [42] devoted in applying hierarchical reinforcement learning (HRL) [7]
where sequence generation is broken down into several sub-tasks each of which targets at a chunk of
words. However  HRL for long report generation is still under-explored.

3 Approach

Medical image report generation aims at generating a report consisting of a sequence of sentences
Y = (y1  y2  . . .   yM ) given a set of medical images I = {Ij}K
j=1 of a patient case. Each sentence
comprises a sequence of words yi = (yi 1  yi 2  . . .   yi N )  yi j ∈ V where i is the index of sentences 
j the index of words  and V the vocabulary of all output tokens. In order to generate long and
topic-coherent reports  we formulate the decoding process in a hierarchical framework that ﬁrst
produces a sequence of hidden sentence topics  and then predicts words of each sentence conditioning
on each topic.
It is observed that doctors writing a report tend to follow certain patterns and reuse templates  while
adjusting statements for each individual case when necessary. To mimic the procedure  we propose
to combine retrieval and generation for automatic report generation. In particular  we ﬁrst compile
an off-the-shelf template database T that consists of a set of sentences that occur frequently in the
training corpus. Such sentences typically describe general observations  and are often inserted into
medical reports  e.g.  "the heart size is normal" and "there is no pleural effusion or pneumothorax".
(Table 1 provides more examples.)

3

Figure 2: Hybrid Retrieval-Generation Reinforced Agent. Visual features are encoded by a CNN
and image encoder  and fed to a sentence decoder to recurrently generate hidden topic states. A
retrieval policy module decides for each topic state to either automatic generate a sentence  or retrieve
a speciﬁc template from a template database. Dashed black lines indicate hierarchical policy learning.
As described in Figure 2  a set of images for each sample is ﬁrst fed into a CNN to extract visual
features which is then transformed into a context vector by an image encoder. Then a sentence
decoder recurrently generates a sequence of hidden states q = (q1  q2  . . .   qM ) which represent
sentence topics. Given each topic state qi  a retrieval policy module decides to either automatically
generate a new sentence by invoking a generation module  or retrieve an existing template from the
template database. Both the retrieval policy module (that determines between automatic generation
or template retrieval) and the generation module (that generates words) are making discrete decisions
and be updated via the REINFORCE algorithm [37  30]. We devise sentence-level and word-level
rewards accordingly for the two modules  respectively.

j=1  we ﬁrst extract their features {vj}K

3.1 Hybrid Retrieval-Generation Reinforced Agent
Image Encoder. Given a set of images {Ij}K
j=1 with a
pretrained CNN  and then average {vj}K
j=1 to obtain v. The image encoder converts v into a context
vector hv ∈ RD which is used as the visual input for all subsequent modules. Speciﬁcally  the image
encoder is parameterized as a fully-connected layer  and the visual features are extracted from the
last convolution layer of a DenseNet [15] or VGG-19 [29].
Sentence Decoder. Sentence decoder comprises stacked RNN layers which generates a sequence
of topic states q. We equip the stacked RNNs with attention mechanism to enhance text generation 
i   where
inspired by [32  41  23]. Each stacked RNN ﬁrst generates an attentive context vector cs
i indicates time steps  given the image context vector hv and previous hidden state hs
i−1. It then
i is further projected
generates a hidden state hs
into a topic space as qi and a stop control probability zi ∈ [0  1] through non-linear functions
respectively. Formally  the sentence decoder can be written as:

i based on cs

i and hs

i−1. The generated hidden state hs

attn(hv  hs
cs
i = F s
i−1)
hs
RNN(cs
i   hs
i = F s
i−1)
qi = σ(Wqhs
i + bq)
zi = Sigmoid(Wzhs

i + bz) 

(1)
(2)
(3)
(4)

attn denotes a function of the attention mechanism [28]  F s

where F s
RNN denotes the non-linear functions
of Stacked RNN  Wq and bq are parameters which project hidden states into the topic space while
Wz and bz are parameters for stop control  and σ is a non-linear activation function. The stop control
probability zi greater than or equal to a predeﬁned threshold (e.g. 0.5) indicates stopping generating
topic states  and thus the hierarchical report generation process.
Retrieval Policy Module. Given each topic state qi  the retrieval policy module takes two steps.
First  it predicts a probability distribution ui ∈ R1+|T| over actions of generating a new sentence
and retrieving from |T| candidate template sentences. Based on the prediction of the ﬁrst step  it
triggers different actions. If automatic generation obtains the highest probability  the generation
module is activated to generate a sequence of words conditioned on current topic state (the second
row on the right side of Figure 2). If a template in T obtains the highest probability  it is retrieved
from the off-the-shelf template database and serves as the generation result of current sentence topic
(the ﬁrst row on the right side of Figure 2). We reserve 0 index to indicate the probability of selecting
automatic generation and positive integers in {1 |T|} to index the probability of selecting templates

4

Generation Module Template DatabaseReward ModuleRetrievalPolicy ModuleTemplate sentenceImage EncoderGenerated sentenceSentence DecoderRetrieve templateAutomatic generationCNNContext vectorVisual featuresTopic stateTopic stateReward of sentenceReward of wordsTopic statein T. The ﬁrst step is parameterized as a fully-connected layer with Softmax activation:

ui = Softmax(Wuqi + bu)
mi = argmax(ui) 

(5)
(6)

where Wu and bu are network parameters  and the resulting mi is the index of highest probability in
ui.
Generation Module. Generation module generates a sequence of words conditioned on current topic
state qi and image context vector hv for each sentence. It comprises RNNs which take environment
parameters and previous hidden state hg
i t which is
further transformed to a probability distribution ai t over all words in V  where t indicates t-th word.
We deﬁne environment parameters as a concatenation of current topic state qi  context vector cg
i t
encoded by following the same attention paradigm in sentence decoder  and embedding of previous
word ei t−1. The procedure of generating each word is written as follows  which is an attentional
decoding step:

i t−1 as input  and generate a new hidden state hg

i t−1)

i t−1)

attn(hv  [ei t−1; qi]  hg
i t; ei t−1; qi]  hg
RNN([cg
i t + by)

cg
i t = F g
i t = F g
hg
at = Softmax(Wyhg
yt = argmax(at)
ei t = WeO(yi t) 

(7)
(8)
(9)
(10)
(11)

attn denotes the attention mechanism of generation module  F g

where F g
RNN denotes non-linear functions
of RNNs  Wy and by are parameters for generating word probability distribution  yi t is index of the
maximum probable word  We is a learnable word embedding matrix initialized uniformly  and O
denotes one hot vector.
Reward Module. We use automatic metrics CIDEr for computing rewards since recent work on
image captioning [28] has shown that CIDEr performs better than many traditional automatic metrics
such as BLEU  METEOR and ROUGE. We consider two kinds of reward functions: sentence-level
reward and word-level reward. For the i-th generated sentence yi = (yi 1  yi 2  . . .   yi N ) either
from retrieval or generation outputs  we compute a delta CIDEr score at sentence level  which is
Rsent(yi) = f ({yk}i
k=1  gt)  where f denotes CIDEr evaluation  and gt denotes
ground truth report. This assesses the advantages the generated sentence brings in to the existing
sentences when evaluating the quality of the whole report. For a single word input  we use reward as
delta CIDEr score which is Rword(yt) = f ({yk}t
k=1  gts) − f ({yk}t−1
k=1  gts) where gts denotes the
ground truth sentence. The sentence-level and word-level rewards are used for computing discounted
reward for retrieval policy module and generation module respectively.

k=1  gt) − f ({yk}i−1

3.2 Hierarchical Reinforcement Learning
Our objective is to maximize the reward of generated report Y compared to ground truth report Y∗.
Omitting the condition on image features for simplicity  the loss function can be written as:

L(θ) = −Ez m y[R(Y  Y∗)]

∇θL(θ) = −Ez m y [∇θ log p(z  m  y)R(Y  Y∗)]

(cid:17)(cid:35)
(cid:16)∇θrL(θr) + 1(mi = 0|mi−1)∇θgL(θg)

(12)
(13)

 

(14)

(cid:34)(cid:88)

i=1

= −Ez m y

1(zi <

|zi−1)

1
2

where θ  θr  and θg denote parameters of the whole network  retrieval policy module  and generation
module respectively; 1(·) is binary indicator; zi is the probability of topic stop control in Equation 4;
mi is the action chosen by retrieval policy module among automatic generation (mi = 0) and all
templates (mi ∈ [1 |T|]) in the template database. The loss of HRGR-Agent comes from two parts:
retrieval policy module L(θr) and generation module L(θg) as deﬁned below.
Policy Update for Retrieval Policy Module. We deﬁne the reward for retrieval policy module Rr
at sentence level. The generated sentence or retrieved template sentence is used for computing the

5

reward. The discounted sentence-level reward and its corresponding policy update according to
REINFORCE algorithm [30] can be written as:

Rr(yi) =
γjRsent(yi+j)
L(θr) = −Emi[Rr(mi  m∗
i )]

j=0

∇θrL(θr) = −Emi[∇θr log p(mi|mi−1)Rr(mi  m∗

(16)
(17)
where γ is a discount factor; yi is the i-th generated sequence; and θr represents parameters of
retrieval policy module which are Wu and bu in Equation 5 .
Policy Update for Generation Module. We deﬁne the word-level reward Rg(yt) for each word
generated by generation module as discounted reward of all generated words after the considered
word. The discounted reward function and its policy update for generation module can be written as:

i )] 

∞(cid:88)

∞(cid:88)

(15)

(18)

(19)
(20)

γjRword(yt+j)

Rg(yt) =
L(θg) = −Eyt[Rg(yt  y∗
t )]

(cid:88)
∇θgL(θg) = −Eyt[

j=0

∇θg log p(yt|yt−1)Rg(yt  y∗

t )] 

where γ is a discount factor  and θg represents the parameters of generation module such as Wy  by 
We in Equation 9-11 and parameters of attention functions in Equation 7 and RNNs in Equation 8.
Detailed policy update algorithm is provides in supplementary materials.

t=1

4 Experiments and Analysis

Datasets. We conduct experiments on two medical image report datasets. First  Indiana University
Chest X-Ray Collection (IU X-Ray) [8] is a public dataset consists of 7 470 frontal and lateral-view
chest x-ray images paired with their corresponding diagnostic reports. Each patient has 2 images and
a report which includes impression  ﬁndings  comparison and indication sections. We preprocess the
reports by tokenizing  converting to lower-cases  and ﬁltering tokens of frequency no less than 3 as
vocabulary  which results in 1185 unique tokens covering over 99.0% word occurrences in the corpus.
CX-CHR is a proprietary internal dataset of chest X-ray images with Chinese reports collected from
a professional medical institution for health checking. The dataset consists of 35 500 patients. Each
patient has one or multiple chest x-ray images in different views such as posteroanterior and lateral 
and a corresponding Chinese report. We select patients with no more than 2 images and obtained
33 236 patient samples in total which covers over 93% of the dataset. We preprocess the reports
through tokenizing by Jieba [1] and ﬁltering tokens of frequency no less than 3 as vocabulary  which
results in 1282 unique tokens.
On both datasets  we randomly split the data by patients into training  validation and testing by a ratio
of 7:1:2. There is no overlap between patients in different sets. We predict the ’ﬁndings’ section as
it is the most important component of reports. On CX-CHR dataset  we pretrain a DenseNet with
public available ChestX-ray8 dataset [36] on classiﬁcation  and ﬁne-tune it on CX-CHR dataset on 20
common thorax disease labels. As IU X-Ray dataset is relatively small  we do not directly ﬁne-tune
the pretrained DenseNet on it  and instead extract visual features from a DenseNet pretrained jointly
on ChestX-ray8 dataset [36] and CX-CHR datasets. Please see Supplementary Material for more
details.
Template Database. We select sentences in the training set whose document frequencies (the number
of occurrence of a sentence in training documents) are no less than a threshold as template candidates.
We further group candidates that express the same meaning but have a little linguistic variations. For
example  "no pleural effusion or pneumothorax" and "there is no pleural effusion or pneumonthorax"
are grouped as one template. This results in 97 templates with greater than 500 document frequency
for CX-CHR and 28 templates with greater than 100 document frequency for IU X-Ray. Upon
retrieval  only the most frequent sentence of a template group will be retrieved for HRGR-Agent or
any rule-based models that we compare with. Although this introduces minor but inevitable error in

6

the generated results  our experiments show that the error is negligible compared to the advantages
that a hybrid of retrieval-based and generation-based approaches brings in. Besides  separating
templates of the same meaning into different categories diminishes the capability of retrieval policy
module to predict the most suitable template for a given visual input  as multiple templates share the
exact same meaning. Table 1 shows examples of templates for IU X-Ray dataset. More template
examples are provided in supplementary materials.

Template

No pneumothorax or pleural effusion.
No pleural effusion or pneumothorax.

There is no pleural effusion or pneumothorax.

The lungs are clear

Lungs are clear.

The lung are clear bilaterally.

No evidence of focal consolidation  pneumothorax  or pleural effusion.

no focal consolidation  pneumothorax or large pleural effusion.

No focal consolidation  pleural effusion  or pneumothorax identiﬁed.

Cardiomediastin silhouett is within normal limit.

The cardiomediastin silhouett is within normal limit.

The cardiomediastin silhouett is within normal limit for size and contour.

df(%)

18.36

23.60

6.55

5.12

Table 1: Examples of template database of IU X-Ray dataset. Each template is constructed by a group
of sentences of the same meaning but slightly different linguistic variations. Top 3 most frequent
sentences for a template are displayed in the ﬁrst and third column. The second column shows
document frequency (in percentage of training corpus) of each template.
Evaluation Metrics. We use three kinds of evaluation metrics: 1) automatic metrics including
CIDEr  ROUGE  and BLEU; 2) medical abnormality terminology detection accuracy: we select 10
most frequent medical abnormality terminologies in medical reports and evaluate average precision
and average false positive (AFP) of compared models; 3) human evaluation: we randomly select
100 samples from testing set for each method and conduct surveys through Amazon Mechanical
Turk. Each survey question gives a ground truth report  and ask candidate to choose among reports
generated by different models that matches with the ground truth report the best in terms of language
ﬂuency  content selection  and correctness of medical abnormal ﬁnding. A default choice is provided
in case of no or both reports are preferred. We collect results from 20 participants and compute the
average preference percentage for each model excluding default choices.
Training Details. We implement our model on PyTorch and train on a GeForce GTX TITAN GPU.
We ﬁrst train all models with cross entropy loss for 30 epochs with an initial learning rate of 5e-4 
and then ﬁne-tune the retrieval policy module and generation module of HRGR-Agent via RL with
a ﬁxed learning rate 5e-5 for another 30 epochs. We use 512 as dimension of all hidden states and
word embeddings  and batch size 16. We set the maximum number of sentences of a report and
maximum number of tokens in a sentence as 18 and 44 for CX-CHR and 7 and 15 for IU X-Ray.
Besides  as observed from baseline models which overly predict most popular and normal reports
for all testing samples and the fact that most medical reports describe normal cases  we add post-
processing to increase the length and comprehensiveness of the generated reports for both datasets
while maintaining the design of HRGR-Agent to better predict abnormalities. The post-processing
we use is that we ﬁrst select 4 most commonly predicted key words with normal descriptions by
other baselines  then for each key word  if the generated report does not describe any abnormality
nor normality of these key words  we add the a corresponding sentence of these key words that
describe their normal cases respectively. The key words for IU X-Ray are ’heart size and mediastinal
contours’  ’pleural effusion or pneumothorax’  ’consolidation’  and ’lungs are clear’. As observed
in our experiments  this step maintains the same medical abnormality term detection results  and
improves the automatic report generation metrics  especially on BLEU-n metrics.
Baselines. On both datasets  we compare with four state-of-the-art image captioning models: CNN-
RNN [34]  LRCN [9]  AdaAtt [23]  and Att2in [28]. Visual features for all models are extracted
from the last convolutional layer of pretrained densetNets respectively as mentioned in 4  yielding
16 × 16 × 256 feature maps for both datasets. We use greedy search and argmax sampling for
HRGR-Agent and the baselines on both datasets. On IU X-Ray dataset  we also compare with
CoAtt [16] which uses different visual features extracted from a pretrained ResNet [11]. The authors
of CoAtt [16] re-trained their model using our train/test split  and provided evaluation results for

7

Dataset

CX-CHR

IU X-Ray

Model

CNN-RNN [34]

LRCN [9]
AdaAtt [23]
Att2in [28]
Generation
Retrieval

HRG

HRGR-Agent
CNN-RNN [34]

LRCN [9]
AdaAtt [23]
Att2in [28]
CoAtt* [16]
HRGR-Agent

CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE
1.580
0.577
0.577
1.588
0.575
1.568
0.576
1.566
0.322
0.361
2.565
0.536
0.588
2.800
0.612
2.895
0.306
0.294
0.284
0.305
0.308
0.295
0.308
0.297
0.369
0.277
0.343
0.322

0.590
0.593
0.588
0.587
0.307
0.535
0.629
0.673
0.216
0.223
0.220
0.224
0.455
0.438

0.411
0.413
0.409
0.408
0.121
0.409
0.463
0.486
0.066
0.067
0.068
0.068
0.154
0.151

0.450
0.452
0.446
0.446
0.160
0.437
0.497
0.530
0.087
0.089
0.089
0.089
0.205
0.208

0.506
0.508
0.503
0.503
0.216
0.475
0.547
0.587
0.124
0.128
0.127
0.129
0.288
0.298

Table 2: Automatic evaluation results on CX-CHR (upper part) and IU X-Ray Datasets (lower part).
BLEU-n denotes BLEU score uses up to n-grams.

Retrieval Generation HRGR-Agent CNN-RNN [34] CoAtt [16] HRGR-Agent

Dataset
Models
Prec. (%)

AFP
Hit (%)

14.13
0.133

–

CX-CHR

27.50
0.064
23.42

29.19
0.059
52.32

0.00
0.000

–

IU X-Ray

5.01
0.019
28.00

12.14
0.043
48.00

Table 3: Average precision (Prec.) and average false positive (AFP) of medical abnormality terminol-
ogy detection  and human evaluation (Hit). The higher Prec. and the lower AFP  the better.

automatic report generation metrics using greedy search and sampling temperature 0.5 at test time. We
further evaluated their prediction to obtain medical abnormality terminology detection precision and
AFP. Due to the relatively large size of CX-CHR  we conduct additional experiments on it to compare
HRGR-Agent with its different variants by removing individual components (Retrieval  Generation 
RL). We train a hierarchical generative model (Generation) without any template retrieval or RL
ﬁne-tuning  and our model without RL ﬁne-tuning (HRG). To exam the quality of our pre-deﬁned
templates  we separately evaluate the retrieval policy module of HRGR-Agent by masking out the
generation part and only use the retrieved templates as prediction (Retrieval). Note that Retrieval
uses the same model as HRG-Agent whose training involves automatic generation of sentences 
thus the results of which may be higher than a general retrieval-based system (e.g. directly perform
classiﬁcation among a list of majority sentences given image features).

4.1 Results and Analyses

Automatic Evaluation. Table 2 shows automatic evaluation comparison of state-of-the-art methods
and our model variants. Most importantly  HRGR-Agent outperforms all baseline models that have
no retrieval mechanism or hierarchical structure on both datasets by great margins  demonstrating its
effectiveness and robustness. On IU X-Ray dataset  HRGR-Agent achieves slightly lower BLEU-
1 4 and ROUGE score than that of CoAtt [16]. However  CoAtt uses different pre-processing of
reports and visual features  jointly predicts ’impression’ and ’ﬁndings’  and uses single-image input
while our method focuses on ’ﬁndings’ and use combined frontal and lateral view of patients. On
CX-CHR  HRGR-Agent increases CIDEr score by 0.73 compared to HRG  demonstrating that
reinforcement ﬁne-tuning is crucial to performance increase since it directly optimizes the evaluation
metric. Besides  Retrieval surpasses Generation by relatively large margins  showing that retrieval-
based method is beneﬁcial to generating structured reports  which leads to boosted performance of
HRGR-Agent when combined with neural generation approaches (generation module). To better
understand HRGR-Agent’s performance  each generated report at testing has on average 7.2 and 4.8
sentences for CX-CHR and IU X-Ray dataset  respectively. The percentages of retrieval vs generation
are 83.5 vs 16.5 on the CX-CHR data  and 82.0 vs 18.0 on IU X-Ray  respectively.
Medical Abnormality Terminology Evaluation. Table 3 shows evaluation results of average preci-
sion and average false positive of medical abnormality terminology detection. HGRG-Agent achieves
the highest precision  and is only slightly lower AFP than CoAtt  demonstrating that its robustness on
detecting rare abnormal ﬁndings which are among the most important components of medical reports.

8

Ground Truth
The cardiomediastinal silhouette is within
normal limits. Calciﬁed right lower lobe
granuloma. No focal airspace consoli-
dation. No visualized pneumothorax or
large pleural effusion. No acute bony ab-
normalities.

Exam limited by patient rotation. Mild
rightward deviation of the trachea. Sta-
ble cardiomegaly. Unfolding of the tho-
racic aorta. Persistent right pleural effu-
sion with adjacent atelectasis. Low lung
volumes. No focal airspace consolidation.
There is severe degenerative changes of
the right shoulder.

Frontal and lateral views of the chest with
overlying external cardiac monitor leads
show reduced lung volumes with bron-
chovascular crowding of basilar atelecta-
sis. No deﬁnite focal airspace consolida-
tion or pleural effusion. The cardiac sil-
houette appears mildly enlarged.

Apparent cardiomegaly partially accentu-
ated by low lung volumes. No focal con-
solidation  pneumothorax or large pleural
effusion. Right base calciﬁed granuloma.
Stable right infrahilar nodular density (lat-
eral view). Negative for acute bone abnor-
mality.

is normal

CoAtt [16]
in
The heart
size. The mediastinum is
unremarkable. The lungs
are clear.

The heart size and pul-
monary vascularity ap-
pear within normal limits.
The lungs are free of fo-
cal airspace disease. No
pleural effusion or pneu-
mothorax. No acute bony
abnormality.

The heart size and pul-
monary vascularity ap-
pear within normal limits.
The lungs are free of fo-
cal airspace disease. No
pleural effusion or pneu-
mothorax. no acute bony
abnormality.

is normal

The heart
in
size. The mediastinum is
unremarkable. The lungs
are clear.

HRGR-Agent
The cardiomediastinal silhouette is nor-
mal size and conﬁguration. Pulmonary
vasculature within normal limits. There
is right middle lobe airspace disease 
may reﬂect granuloma or pneumonia.
No pleural effusion. No pneumothorax.
No acute bony abnormalities.
The heart is enlarged. Possible car-
diomegaly. There is pulmonary vascular
congestion with diffusely increased inter-
stitial and mild patchy airspace opacities.
Suspicious pleural effusion. There is no
pneumothorax. There are no acute bony
ﬁndings.

The heart is mildly enlarged. The
aorta is atherosclerotic and ectatic.
Chronic parenchymal changes are noted
with mild scarring and/or subsegmental
atelectasis in the right lung base. No
focal consolidation or signiﬁcant pleural
effusion identiﬁed. Costophrenic UNK
are blunted.

The heart size and pulmonary vascular-
ity appear within normal limits. Low
lung volumes.
Suspicious calciﬁed
granuloma. No pleural effusion or pneu-
mothorax. No acute bony abnormality.

Figure 3: Examples of ground truth report and generated reports by CoAtt [16] and HRGR-Agent.
Highlighted phrases are medical abnormality terms. Italicized text is retrieved from template database.
Retrieval vs. Generation. It’s worth knowing that on CX-CHR  Retrieval achieves higher automatic
evaluation scores (Table 2 the 7th row) but lower medical term detection precision (Table 3 the 2nd
column) than Generation. Note that Retrieval evaluates retrieval policy module of HRGR-Agent by
masking out the generation results of generation module. The result shows that simply composing
templates that mostly describe normal medical ﬁndings can lead to high automatic evaluation scores
since the majority reports describe normal cases. However  this kind retrieval-based approaches
lack of the capability of detecting signiﬁcant but rare abnormal ﬁndings. On the other hand  the
high medical abnormality term detection precision and low average false positive of HRGR-Agent
veriﬁes that its generation module learns to describe abnormal ﬁndings. The win-win combination of
retrieval policy module and generation module leads to state-of-the-art performance of HRGE-Agent 
surpassing a generative model (Generation) that is purely trained without any retrieval mechanism.
Human Evaluation. Table 3 (last row) shows average human preference percentage of HRGR-Agent
compared with Generation and CoAtt [16] on CX-CHR and IU X-Ray respectively  evaluated in terms
of content coverage  speciﬁc terminology accuracy and language ﬂuency. HRGR-Agent achieves
much higher human preference than baseline models  showing that it is able to generate natural and
plausible reports that are human preferable.
Qualitative Analysis. Figure 3 demonstrate qualitative results of HRGR-Agent and baseline models
on IU X-Ray dataset. The reports of HRGR-Agent are generally longer than that of the baseline
models  and share a well balance of templates and generated sentences. And  among the generated
sentences  HRGR-Agent has higher rate of detecting abnormal ﬁndings.

5 Conclusion

In this paper  we introduce a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent)
to perform robust medical image report generation. Our approach is the ﬁrst attempt to bridge
human prior knowledge and generative neural network via reinforcement learning. Experiments
show that HRGR-Agent does not only achieve state-of-the-art performance on two medical image
report datasets  but also generates robust reports that has high precision on medical abnormal ﬁndings
detection and best human preference.

9

References
[1] "jieba" (chinese for "to stutter") chinese text segmentation: built to be the best python chinese word

segmentation module. https://github.com/fxsjy/jieba  2018. Accessed: 2018-05-01.

[2] C. D. M. Abigail See  Peter J. Liu. Get to the point: Summarization with pointer-generator networks. in

ACL  2017.

[3] D. Bahdanau  P. Brakel  K. Xu  A. Goyal  R. Lowe  J. Pineau  A. Courville  and Y. Bengio. An actor-critic

algorithm for sequence prediction. ICLR  2017.

[4] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with

human judgments. In ACL workshop  2005.

[5] J. M. Bosmans  J. J. Weyler  A. M. De Schepper  and P. M. Parizel. The radiology report as seen by

radiologists and referring clinicians: results of the cover and rover surveys. Radiology  2011.

[6] S. R. Bowman  L. Vilnis  O. Vinyals  A. M. Dai  R. Jozefowicz  and S. Bengio. Generating sentences from

a continuous space. CoNLL  2016.

[7] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In NeurIPS  1993.

[8] D. Demner-Fushman  M. D. Kohli  M. B. Rosenman  S. E. Shooshan  L. Rodriguez  S. Antani  G. R.
Thoma  and C. J. McDonald. Preparing a collection of radiology examinations for distribution and retrieval.
Journal of the American Medical Informatics Association  2015.

[9] J. Donahue  L. Anne Hendricks  S. Guadarrama  M. Rohrbach  S. Venugopalan  K. Saenko  and T. Darrell.

Long-term recurrent convolutional networks for visual recognition and description. In CVPR  2015.

[10] S. K. Goergen  F. J. Pool  T. J. Turner  J. E. Grimm  M. N. Appleyard  C. Crock  M. C. Fahey  M. F.
Fay  N. J. Ferris  S. M. Liew  et al. Evidence-based guideline for the written radiology report: Methods 
recommendations and implementation challenges. Journal of medical imaging and radiation oncology 
2013.

[11] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR  2016.

[12] Y. Hong and C. E. Kahn. Content analysis of reporting templates and free-text radiology reports. Journal

of digital imaging  2013.

[13] Z. Hu  H. Shi  Z. Yang  B. Tan  T. Zhao  J. He  W. Wang  X. Yu  L. Qin  D. Wang  et al. Texar: A
modularized  versatile  and extensible toolkit for text generation. arXiv preprint arXiv:1809.00794  2018.

[14] Z. Hu  Z. Yang  X. Liang  R. Salakhutdinov  and E. P. Xing. Toward controlled generation of text. In

ICML  2017.

[15] G. Huang  Z. Liu  K. Q. Weinberger  and L. van der Maaten. Densely connected convolutional networks.

In CVPR  2017.

[16] B. Jing  P. Xie  and E. Xing. On the automatic generation of medical imaging reports. In ACL  2018.

[17] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR 

2015.

[18] L. Li and B. Gong. End-to-end video captioning with multitask reinforcement learning. In ICLR  2017.

[19] X. Liang  Z. Hu  H. Zhang  C. Gan  and E. P. Xing. Recurrent topic-transition gan for visual paragraph

generation. In ICCV  2017.

[20] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL  2013.

[21] S. Liu  Z. Zhu  N. Ye  S. Guadarrama  and K. Murphy. Improved image captioning via policy gradient

optimization of spider. In Proc. IEEE Int. Conf. Comp. Vis  volume 3  2017.

[22] Y. Liu  J. Fu  T. Mei  and C. W. Chen. Let your photos talk: Generating narrative paragraph for photo

stream via bidirectional attention recurrent neural networks. In AAAI  2017.

[23] J. Lu  C. Xiong  D. Parikh  and R. Socher. Knowing when to look: Adaptive attention via a visual sentinel

for image captioning. In CVPR  2017.

[24] J. Lu  J. Yang  D. Batra  and D. Parikh. Neural baby talk. In CVPR  2018.

10

[25] K. Papineni  S. Roukos  T. Ward  and W.-J. Zhu. Bleu: a method for automatic evaluation of machine

translation. In ACL  2002.

[26] R. Paulus  C. Xiong  and R. Socher. A deep reinforced model for abstractive summarization. In ICLR 

2018.

[27] M. Ranzato  S. Chopra  M. Auli  and W. Zaremba. Sequence level training with recurrent neural networks.

In ICLR  2016.

[28] S. J. Rennie  E. Marcheret  Y. Mroueh  J. Ross  and V. Goel. Self-critical sequence training for image

captioning. In CVPR  2017.

[29] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

ICLR  2015.

[30] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction  volume 1. MIT press Cambridge 

1998.

[31] B. Tan  Z. Hu  Z. Yang  R. Salakhutdinov  and E. Xing. Connecting the dots between mle and rl for text

generation. 2018.

[32] A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  Ł. Kaiser  and I. Polosukhin.

Attention is all you need. In NeurIPS  2017.

[33] R. Vedantam  C. Lawrence Zitnick  and D. Parikh. Cider: Consensus-based image description evaluation.

In CVPR  2015.

[34] O. Vinyals  A. Toshev  S. Bengio  and D. Erhan. Show and tell: A neural image caption generator. In

CVPR  2015.

[35] X. Wang  W. Chen  Y.-F. Wang  and W. Y. Wang. No metrics are perfect: Adversarial reward learning for

visual storytelling. In ACL  2018.

[36] X. Wang  Y. Peng  L. Lu  Z. Lu  M. Bagheri  and R. M. Summers. Chestx-ray8: Hospital-scale chest x-ray
database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases.
In CVPR  2017.

[37] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

In Reinforcement Learning  pages 5–32. Springer  1992.

[38] S. Wiseman  S. M. Shieber  and A. M. Rush. Challenges in data-to-document generation. In ICCV  2017.

[39] Y. Wu  M. Schuster  Z. Chen  Q. V. Le  M. Norouzi  W. Macherey  M. Krikun  Y. Cao  Q. Gao  K. Macherey 
et al. Google’s neural machine translation system: Bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144  2016.

[40] Z. Y. Y. Y. Y. Wu and R. S. W. W. Cohen. Encode  review  and decode: Reviewer module for caption

generation. In NeurIPS  2016.

[41] K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhudinov  R. Zemel  and Y. Bengio. Show  attend and

tell: Neural image caption generation with visual attention. In ICML  2015.

[42] D. Yarats and M. Lewis. Hierarchical text generation and planning for strategic dialogue. In EMNLP 

2017.

[43] Q. You  H. Jin  Z. Wang  C. Fang  and J. Luo. Image captioning with semantic attention. In CVPR  2016.

[44] S. L. Ziqiang Cao  Wenjie Li and F. Wei. Retrieve  rerank and rewrite: Soft template based neural

summarization. In ACL  2018.

11

,Scott Reed
Yi Zhang
Yuting Zhang
Honglak Lee
Yuan Li
Xiaodan Liang
Zhiting Hu
Eric Xing