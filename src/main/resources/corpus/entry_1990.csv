2013,Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA,We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-$d$ projection matrices (the Fantope). The convex problem can be solved efficiently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate  in terms of the sparsity  ambient dimension  and sample size  for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of $d=1$  our result implies the near- optimality of DSPCA even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical  properties of the method for arbitrary input matrices that extends the  applicability and provable guarantees to a wide array of settings.  We  demonstrate this with an application to Kendall's tau correlation matrices  and transelliptical component analysis.,Fantope Projection and Selection:

A near-optimal convex relaxation of sparse PCA

Vincent Q. Vu

The Ohio State University
vqv@stat.osu.edu

Juhee Cho

University of Wisconsin  Madison
chojuhee@stat.wisc.edu

Jing Lei

Carnegie Mellon University

leij09@gmail.com

Karl Rohe

University of Wisconsin  Madison
karlrohe@stat.wisc.edu

Abstract

We propose a novel convex relaxation of sparse principal subspace estimation
based on the convex hull of rank-d projection matrices (the Fantope). The convex
problem can be solved efﬁciently using alternating direction method of multipli-
ers (ADMM). We establish a near-optimal convergence rate  in terms of the spar-
sity  ambient dimension  and sample size  for estimation of the principal subspace
of a general covariance matrix without assuming the spiked covariance model.
In the special case of d = 1  our result implies the near-optimality of DSPCA
(d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a
general theoretical framework for analyzing the statistical properties of the method
for arbitrary input matrices that extends the applicability and provable guarantees
to a wide array of settings. We demonstrate this with an application to Kendall’s
tau correlation matrices and transelliptical component analysis.

1 Introduction

Principal components analysis (PCA) is a popular technique for unsupervised dimension reduction
that has a wide range of application—science  engineering  and any place where multivariate data
is abundant. PCA uses the eigenvectors of the sample covariance matrix to compute the linear
combinations of variables with the largest variance. These principal directions of variation explain
the covariation of the variables and can be exploited for dimension reduction.
In contemporary
applications where variables are plentiful (large p) but samples are relatively scarce (small n)  PCA
suffers from two major weaknesses : 1) the interpretability and subsequent use of the principal
directions is hindered by their dependence on all of the variables; 2) it is generally inconsistent in
high-dimensions  i.e. the estimated principal directions can be noisy and unreliable [see 2  and the
references therein].
Over the past decade  there has been a fever of activity to address the drawbacks of PCA with a class
of techniques called sparse PCA that combine the essence of PCA with the assumption that the phe-
nomena of interest depend mostly on a few variables. Examples include algorithmic [e.g.  1  3–10]
and theoretical [e.g.  11–14] developments. However  much of this work has focused on the ﬁrst
principal component. One rationale behind this focus is by analogy with ordinary PCA: additional
components can be found by iteratively deﬂating the input matrix to account for variation uncovered
by previous components. However  the use of deﬂation with sparse PCA entails complications of
non-orthogonality  sub-optimality  and multiple tuning parameters [15]. Identiﬁability and consis-
tency present more subtle issues. The principal directions of variation correspond to eigenvectors
of some population matrix Σ. There is no reason to assume a priori that the d largest eigenvalues

1

of Σ are distinct. Even if the eigenvalues are distinct  estimates of individual eigenvectors can be
unreliable if the gap between their eigenvalues is small. So it seems reasonable  if not necessary 
to de-emphasize eigenvectors and to instead focus on their span  i.e.
the principal subspace of
variation.
There has been relatively little work on the problem of estimating the principal subspace or even
multiple eigenvectors simultaneously. Most works that do are limited to iterative deﬂation schemes
or optimization problems whose global solution is intractable to compute. Sole exceptions are the
diagonal thresholding method [2]  which is just ordinary PCA applied to the subset of variables
with largest marginal sample variance  or reﬁnements such as iterative thresholding [16]  which use
diagonal thresholding as an initial estimate. These works are limited  because they cannot be used
when the variables have equal variances (e.g.  correlation matrices). Theoretical results are equally
limited in their applicability. Although the optimal minimax rates for the sparse principal subspace
problem are known in both the spiked [17] and general [18] covariance models  existing statistical
guarantees only hold under the restrictive spiked covariance model  which essentially guarantees that
diagonal thresholding has good properties  or for estimators that are computationally intractable.
In this paper  we propose a novel convex optimization problem to estimate the d-dimensional princi-
pal subspace of a population matrix Σ based on a noisy input matrix S. We show that if S is a sample
covariance matrix and the projection Π of the d-dimensional principal subspace of Σ depends only
on s variables  then with a suitable choice of regularization parameter  the Frobenius norm of the

error of our estimator !X is bounded with high probability

|||!X − Π|||2 = O"(λ1/δ)s#log p/n$

where λ1 is the largest eigenvalue of Σ and δ the gap between the dth and (d + 1)th largest eigen-
values of Σ. This rate turns out to be nearly minimax optimal (Corollary 3.3)  and under additional
assumptions on signal strength  it also allows us to recover the support of the principal subspace
(Theorem 3.2). Moreover  we provide easy to verify conditions (Theorem 3.3) that yield near-
optimal statistical guarantees for other choices of input matrix  such as Pearson’s correlation and
Kendall’s tau correlation matrices (Corollary 3.4).
Our estimator turns out to be a semideﬁnite program (SDP) that generalizes the DSPCA approach
of [1] to d ≥ 1 dimensions. It is based on a convex body  called the Fantope  that provides a tight
relaxation for simultaneous rank and orthogonality constraints on the positive semideﬁnite cone.
Solving the SDP is non-trivial. We ﬁnd that an alternating direction method of multipliers (ADMM)
algorithm [e.g.  19] can efﬁciently compute its global optimum (Section 4).
In summary  the main contributions of this paper are as follows.

1. We formulate the sparse principal subspace problem as a novel semideﬁnite program with

a Fantope constraint (Section 2).

2. We show that the proposed estimator achieves a near optimal rate of convergence in sub-
space estimation without assumptions on the rank of the solution or restrictive spiked co-
variance models. This is a ﬁrst for both d = 1 and d > 1 (Section 3).

3. We provide a general theoretical framework that accommodates other matrices  in addition

to sample covariance  such as Pearson’s correlation and Kendall’s tau.

4. We develop an efﬁcient ADMM algorithm to solve the SDP (Section 4)  and provide nu-
merical examples that demonstrate the superiority of our approach over deﬂation methods
in both computational and statistical efﬁciency (Section 5).

The remainder of the paper explains each of these contributions in detail  but we defer all proofs to
Appendix A.

Related work Existing work most closely related to ours is the DSPCA approach for single com-
ponent sparse PCA that was ﬁrst proposed in [1]. Subsequently  there has been theoretical analysis
under a spiked covariance model and restrictions on the entries of the eigenvectors [11]  and algo-
rithmic developments including block coordinate ascent [9] and ADMM [20]. The crucial difference
with our work is that this previous work only considered d = 1. The d > 1 case requires invention
and novel techniques to deal with a convex body  the Fantope  that has never before been used in
sparse PCA.

2

Notation For matrices A  B of compatible dimension ⟨A  B⟩ := tr(AT B) is the Frobenius inner
product  and |||A|||2
2 := ⟨A  A⟩ is the squared Frobenius norm. ∥x∥q is the usual ℓq norm with ∥x∥0
deﬁned as the number of nonzero entries of x. ∥A∥a b is the (a  b)-norm deﬁned to be the ℓb norm
is the maximum absolute row sum. For a
of the vector of rowwise ℓa norms of A  e.g. ∥A∥1 ∞
symmetric matrix A  we deﬁne λ1(A) ≥ λ2(A) ≥···
to be the eigenvalues of A with multiplicity.
When the context is obvious we write λj := λj(A) as shorthand. For two subspaces M1 and M2 
sin Θ(M1 M2) is deﬁned to be the matrix whose diagonals are the sines of the canonical angles
between the two subspaces [see 21  §VII].
2 Sparse subspace estimation

deﬁned to be a solution of the semideﬁnite program

Given a symmetric input matrix S  we propose a sparse principal subspace estimator !X that is

(1)

maximize
subject to X ∈F d 

⟨S  X⟩ − λ∥X∥1 1

in the variable X  where

F d :=%X : 0 ≼ X ≼ I and tr(X) = d&

is a convex body called the Fantope [22  §2.3.2]  and λ ≥ 0 is a regularization parameter that
encourages sparsity. When d = 1  the spectral norm bound in F d is redundant and (1) reduces to
the DSPCA approach of [1]. The motivation behind (1) is based on two key insights.
The ﬁrst insight is a variational characterization of the principal subspace of a symmetric matrix.
The sum of the d largest eigenvalues of a symmetric matrix A can be expressed as

d’i=1

λi(A)

(a)
= max

V T V =Id⟨A  V V T⟩

(b)
= max

X∈F d⟨A  X⟩ .

(2)

Identity (a) is an extremal property known as Ky Fan’s maximum principle [23]; (b) is based on the
less well known observation that

F d = conv({V V T : V T V = Id})  

i.e. the extremal points of F d are the rank-d projection matrices. See [24] for proofs of both.
The second insight is a connection between the (1  1)-norm and a notion of subspace sparsity intro-
duced by [18]. Any X ≽ 0 can be factorized (non-uniquely) as X = V V T .
Lemma 2.1. If X = V V T   then ∥X∥1 1 ≤ ∥V ∥2
Consequently  any X ∈F d that has at most s non-zero rows necessarily has ∥X∥1 1 ≤ s2d. Thus 
∥X∥1 1 is a convex relaxation of what [18] call row sparsity for subspaces.
These two insights reveal that (1) is a semideﬁnite relaxation of the non-convex problem

2 1 ≤ ∥V ∥2

2 0 tr(X).

maximize
subject to V T V = Id .

⟨S  V V T⟩ − λ∥V ∥2

2 0d

[18] proposed solving an equivalent form of the above optimization problem and showed that the
estimator corresponding to its global solution is minimax rate optimal under a general statistical
model for S. Their estimator requires solving an NP-hard problem. The advantage of (1) is that it is
computationally tractable.

Subspace estimation The constraint !X ∈F d guarantees that its rank is ≥ d. However !X need not
be an extremal point of F d  i.e. a rank-d projection matrix. In order to obtain a proper d-dimensional
subspace estimate  we can extract the d leading eigenvectors of !X  say !V   and form the projection
matrix!Π= !V!V T . The projection is unique  but the choice of basis is arbitrary. We can follow the
convention of standard PCA by choosing an orthogonal matrix O so that (!V O)T S(!V O) is diagonal 
and take!V O as the orthonormal basis for the subspace estimate.

3

3 Theory

In this section we describe our theoretical framework for studying the statistical properties of !X
given by (1) with arbitrary input matrices that satisfy the following assumptions.
Assumption 1 (Symmetry). S and Σ are p × p symmetric matrices.
Assumption 2 (Identiﬁability). δ = δ(Σ) = λd(Σ) − λd+1(Σ) > 0.
Assumption 3 (Sparsity). The projection Π onto the subspace spanned by the eigenvectors of Σ
corresponding to its d largest eigenvalues satisﬁes ∥Π∥2 0 ≤ s  or equivalently  ∥diag(Π)∥0 ≤ s.
The key result (Theorem 3.1 below) implies that the statistical properties of the error of the estimator

can be derived  in many cases  by routine analysis of the entrywise errors of the input matrix

∆ := !X − Π  

W := S − Σ .

There are two main ideas in our analysis of !X. The ﬁrst is relating the difference in the values of
the objective function in (1) at Π and !X to ∆. The second is exploiting the decomposability of

the regularizer. Conceptually  this is the same approach taken by [25] in analyzing the statistical
properties of regularized M-estimators. It is worth noting that the curvature result in our problem
comes from the geometry of the constraint set in (1).
It is different from the “restricted strong
convexity” in [25]  a notion of curvature tailored for regularization in the form of penalizing an
unconstrained convex objective.

3.1 Variational analysis on the Fantope

The ﬁrst step of our analysis is to establish a bound on the curvature of the objective function along
the Fantope and away from the truth.
Lemma 3.1 (Curvature). Let A be a symmetric matrix and E be the projection onto the subspace
spanned by the eigenvectors of A corresponding to its d largest eigenvalues λ1 ≥ λ2 ≥···
. If
δA = λd − λd+1 > 0  then

δA
2 |||E − F|||2
for all F satisfying 0 ≼ F ≼ I and tr(F ) = d.
A version of Lemma 3.1 ﬁrst appeared in [18] with the additional restriction that F is a projection
matrix. Our proof of the above extension is a minor modiﬁcation of their proof.
The following is an immediate corollary of Lemma 3.1 and the Ky Fan maximal principle.
Corollary 3.1 (A sin Θ theorem [18]). Let A B be symmetric matrices and MA  MB be their
respective d-dimensional principal subspaces. If δA B = [λd+1(A)− λd(A)]∨ [λd+1(B)− λd(B)] 
then

2 ≤ ⟨A  E − F⟩

|||sin Θ(MA MB)|||2 ≤

√2
δA B |||A − B|||2 .

will be close to that of Π if ∆ is small.

The advantage of Corollary 3.1 over the Davis-Kahan Theorem [see  e.g.  21  §VII.3] is that it does
not require a bound on the differences between eigenvalues of A and eigenvalues of B. This means
that typical applications of the Davis-Kahan Theorem require the additional invocation of Weyl’s
Theorem. Our primary use of this result is to show that even if rank(!X) ̸= d  its principal subspace
Corollary 3.2 (Subspace error bound). If M is the principal d-dimensional subspace of Σ and (M
is the principal d-dimensional subspace of !X  then

|||sin Θ(M  (M)|||2 ≤ √2|||∆|||2 .

4

3.2 Deterministic error

With Lemma 3.1  it is straightforward to prove the following theorem.
Theorem 3.1 (Deterministic error bound). If λ ≥ ∥W∥∞ ∞
|||∆|||2 ≤ 4sλ/δ .
Theorem 3.1 holds for any global optimizer !X of (1). It does not assume that the solution is rank-d as
!X.

in [11]. The next theorem gives a sufﬁcient condition for support recovery by diagonal thresholding

Theorem 3.2 (Support recovery). For all t > 0

and s ≥ ∥Π∥2 0 then

As a consequence 
minj:Πjj̸=0 Πjj ≥ 2t > 2|||∆|||2.
3.3 Statistical properties

)){j :Π jj = 0  !Xjj ≥ t})) +)){j :Π jj ≥ 2t  !Xjj < t})) ≤ |||∆|||2
the variable selection procedure !J(t) := %j : !Xjj ≥ t& succeeds if
In this section we use Theorem 3.1 to derive the statistical properties of !X in a generic setting where
general result possible  but it allows us to illustrate the statistical properties of !X for two different

types of input matrices: sample covariance and Kendall’s tau correlation. The former is the standard
input for PCA; the latter has recently been shown to be a useful robust and nonparametric tool for
high-dimensional graphical models [26].
Theorem 3.3 (General statistical error bound). If there exists σ> 0 and n > 0 such that Σ and S
satisfy

the entries of W uniformly obey a restricted sub-Gaussian deviation inequality. This is not the most

t2

2

.

max

ij

for all t ≤ σ and
then

P"|Sij − Σij|≥ t$ ≤ 2 exp" − 4nt2/σ2$

λ = σ#log p/n ≤ σ 
s#log p/n
|||!X − Π|||2 ≤

4σ
δ

with probability at least 1 − 2/p2.
Sample covariance Consider the setting where the input matrix is the sample covariance matrix
of a random sample of size n > 1 from a sub-Gaussian distribution. A random vector Y with
Σ = Var(Y ) has sub-Gaussian distribution if there exists a constant L > 0 such that

P"|⟨Y − EY  u⟩| ≥ t$ ≤ exp" − Lt2/∥Σ1/2u∥2
2$

for all u and t ≥ 0. Under this condition we have the following corollary of Theorem 3.3.
Corollary 3.3. Let S be the sample covariance matrix of an i.i.d. sample of size n > 1 from a
sub-Gaussian distribution (5) with population covariance matrix Σ. If λ is chosen to satisfy (4) with
σ = cλ1  then

(5)

(3)

(4)

|||!X − Π|||2 ≤ C

λ1
δ

s#log p/n

#λ1/λd+1 ·#s/d

with probablity at least 1 − 2/p2  where c  C are constants depending only on L.
Comparing with the minimax lower bounds derived in [17  18]  we see that the rate in Corollary 3.3
is roughly larger than the optimal minimax rate by a factor of

The ﬁrst term only becomes important in the near-degenerate case where λd+1 ≪ λ1. It is possible
with much more technical work to get sharp dependence on the eigenvalues  but we prefer to retain
brevity and clarity in our proof of the version here. The second term is likely to be unimprovable
without additional conditions on S and Σ such as a spiked covariance model. Very recently  [14]
showed in a testing framework with similar assumptions as ours when d = 1 that the extra factor
√s is necessary for any polynomial time procedure if the planted clique problem cannot be solved
in randomized polynomial time.

5

Kendall’s tau Kendall’s tau correlation provides a robust and nonparametric alternative to ordi-
nary (Pearson) correlation. Given an n × p matrix whose rows are i.i.d. p-variate random vectors 
the theoretical and empirical versions of Kendall’s tau correlation matrix are

τij := Cor" sign(Y1i − Y2i)   sign(Y1j − Y2j)$

2

ˆτij :=

sign(Ysi − Yti) sign(Ysj − Ytj) .

n(n − 1)’s<t

A key feature of Kendall’s tau is that it is invariant under strictly monotone transformations  i.e.

sign(Ysi − Yti) sign(Ysj − Ytj)) = sign(fi(Ysi) − fi(Yti)) sign(fj(Ysj) − fj(Ytj))  

where fi  fj are strictly monotone transformations. When Y is multivariate Gaussian  there is also
a one-to-one correspondence between τij and ρij = Cor(Y1i  Y1j) [27] :

(6)

(7)

These two observations led [26] to propose using

τij =

2
π

arcsin(ρij) .

!Tij =*sin" π
2 ˆτij$

1

if i ̸= j
if i = j .

as an input matrix to Gaussian graphical model estimators in order to extend the applicability of those
procedures to the wider class of nonparanormal distributions [28]. This same idea was extended to

sparse PCA procedure of [13]. A shortcoming of that approach is that their theoretical guarantees
only hold for the global solution of an NP-hard optimization problem. The following corollary of

sparse PCA by [29]; they proposed and analyzed using !T as an input matrix to the non-convex
Theorem 3.3 rectiﬁes the situation by showing that !X with Kendall’s tau is nearly optimal.
Corollary 3.4. Let S = !T as deﬁned in (7) for an i.i.d. sample of size n > 1 and let Σ= T be the
analogous quantity with τij in place of ˆτij. If λ is chosen to satisfy (4) with σ = √8π  then

|||!X − Π|||2 ≤

8√2π

δ

s#log p/n

with probablity at least 1 − 2/p2.
Note that Corollary 3.4 only requires that ˆτ be computed from an i.i.d. sample. It does not specify
the marginal distribution of the observations. So Σ= T is not necessarily positive semideﬁnite
and may be difﬁcult to interpret. However  under additional conditions  the following lemma gives
meaning to T by extending (6) to a wide class of distributions  called transelliptical by [29]  that
includes the nonparanormal. See [29  30] for further information.
Lemma ([29  30]). If (Y11  . . .   Y1p) has continuous distribution and there exist monotone transfor-
mations f1  . . .   fp such that

has elliptical distribution with scatter matrix ˜Σ  then

"f1(Y11)  . . .   fp(Y1p)$
Tij = ˜Σij/+ ˜Σii ˜Σjj .

Moreover  if fj(Y1j)  j = 1  . . .   p have ﬁnite variance  then Tij = Cor"fi(Y1i)  fj(Y1j)$.

This lemma together with Corollary 3.4 shows that Kendall’s tau can be used in place of the sample
correlation matrix for a wide class of distributions without much loss of efﬁciency.

4 An ADMM algorithm

The chief difﬁculty in directly solving (1) is the interaction between the penalty and the Fantope
constraint. Without either of these features  the optimization problem would be much easier. ADMM
can exploit this fact if we ﬁrst rewrite (1) as the equivalent equality constrained problem

minimize ∞· 1F d(X) − ⟨S  X⟩ + λ∥Y ∥1 1
subject to X − Y = 0  
6

(8)

t = 0  1  2  3  . . .

Y (0) ← 0  U (0) ← 0
repeat

Algorithm 1 Fantope Projection and Selection (FPS)
Require: S = ST   d ≥ 1  λ ≥ 0  ρ> 0  ϵ> 0
X (t+1) ←P F d"Y (t) − U (t) + S/ρ$
Y (t+1) ←S λ/ρ"X (t+1) + U (t)$
until max(|||X (t) − Y (t)|||2
return Y (t)

U (t+1) ← U (t) + X (t+1) − Y (t+1)

2  ρ 2|||Y (t) − Y (t−1)|||2

2) ≤ dϵ2

◃ Initialization

◃ Fantope projection
◃ Elementwise soft thresholding
◃ Dual variable update
◃ Stopping criterion

in the variables X and Y   where 1F d is the 0-1 indicator function for F d and we adopt the convention
∞· 0 = 0. The augmented Lagrangian associated with (8) has the form

Lρ(X  Y  U ) := ∞· 1F d(X) − ⟨S  X⟩ + λ∥Y ∥1 1 +

ρ

2 |||X − Y + U|||2

2-  
2 −||| U|||2

(9)

where U = (1/ρ)Z is the scaled ADMM dual variable and ρ is the ADMM penalty parameter [see
19  §3.1]. ADMM consists of iteratively minimizing Lρ with respect to X  minimizing Lρ with
respect to Y   and then updating the dual variable. Algorithm 1 summarizes the main steps.
In light of the separation of X and Y in (9) and some algebraic manipulation  the X and Y updates
reduce to computing the proximal operators

PF d"Y − U + S/ρ$ := arg min

X∈F d
Sλ/ρ(X + U ) := arg min

Y

1
2|||X − (Y − U + S/ρ)|||2
λ
ρ∥Y ∥1 1 +

1
2|||(X + U ) − Y |||2
2 .

2

Sλ/ρ is the elementwise soft thresholding operator [e.g.  19  §4.4.3] deﬁned as

Sλ/ρ(x) = sign(x) max(|x|− λ/ρ  0) .

i

i (θ)uiuT

i (θ) = d.

i   where γ+

is a spectral decomposition of X  then
i (θ) = min(max(γi − θ  0)  1) and θ satisﬁes the equation

PF d is the Euclidean projection onto F d and is given in closed form in the following lemma.
Lemma 4.1 (Fantope projection). If X = .i γiuiuT
PF d(X) = .i γ+
.i γ+
Thus  PF d(X) involves computing an eigendecomposition of Y   and then modifying the eigenvalues
by solving a monotone  piecewise linear equation.
Rather than ﬁx the ADMM penalty parameter ρ in Algorithm 1 at some constant value  we recom-
mend using the varying penalty scheme described in [19  §3.4.1] that dynamically updates ρ after
each iteration of the ADMM to keep the primal and dual residual norms (the two sum of squares
in the stopping criterion of Algorithm 1) within a constant factor of each other. This eliminates an
additional tuning parameter  and in our experience  yields faster convergence.

5 Simulation results

We conducted a simulation study to compare the effectiveness of FPS against three deﬂation-based
methods: DSPCA (which is just FPS with d = 1)  GPowerℓ1 [7]  and SPC [5  6]. These methods
obtain multiple component estimates by taking the kth component estimate ˆvk from input matrix Sk 
and then re-running the method with the deﬂated input matrix: Sk+1 = (I − ˆvkˆvT
k ).
k )Sk(I − ˆvkˆvT
The resulting d-dimensional principal subspace estimate is the span of ˆv1  . . .   ˆvd. Tuning parameter
selection can be much more complicated for these iterative deﬂation methods. In our simulations 
we simply ﬁxed the regularization parameter to be the same for all d components.
We generated input matrices by sampling n = 100  i.i.d. observations from a Np(0  Σ)  p = 200
distribution and taking S to be the usual sample covariance matrix. We considered two different
types of sparse Π= V V T of rank d = 5: those with disjoint support for the nonzero entries of the

7

1
0
−1
−2
−3
1
0
−1

)

E
S
M
(
g
o

l

s:10  noise:1

s:10  noise:10

s:25  noise:1

s:25  noise:10

s
u
p
p
o
r
t
:

i

j

d
s
o
n

i

t

s
u
p
p
o
r
t
:
s
h
a
r
e
d

20 30 5

10

20 30

5

10

20 30 5

20 30 5

10
(2 1)−norm of estimate

10

)  and
) across 100 replicates each of a variety of simulation designs with n = 100  p = 200 

)  DSPCA with deﬂation (

)  GPowerℓ1 (

Figure 1: Mean squared error of FPS (
SPC (
d = 5  s ∈{ 10  25}  noise σ2 ∈{ 1  10}.

columns of V and those with shared support. We generated V by sampling its nonzero entries from
a standard Gaussian distribution and then orthnormalizing V while retaining the desired sparsity
pattern. In both cases  the number of nonzero rows of V is equal to s ∈{ 10  25}. We then embedded
Π inside the population covariance matrix Σ= αΠ+ ( I − Π)Σ0(I − Π)  where Σ0 is a Wishart
matrix with p degrees of freedom and α> 0 is chosen so that the effective noise level (in the optimal
minimax rate [18])  σ2 =#λ1λd+1/(λd − λd+1) ∈{ 1  10}.
Figure 1 summarizes the resulting mean squared error |||!Π − Π|||2

2 across 100 replicates for each
of the different combinations of simulation parameters. Each method’s regularization parameter
varies over a range and the x-axis shows the (2  1)-norm of the corresponding estimate. At the
right extreme  all methods essentially correspond to standard PCA. It is clear that regularization is
beneﬁcial  because all the methods have signiﬁcantly smaller MSE than standard PCA when they
are sufﬁciently sparse. Comparing between methods  we see that FPS dominates in all cases  but
the competition is much closer in the disjoint support case. Finally  all methods degrade when the
number of active variables or noise level increases.

6 Discussion

Estimating sparse principal subspaces in high-dimensions poses both computational and statistical
challenges. The contribution of this paper—a novel SDP based estimator  an efﬁcient algorithm 
and strong statistical guarantees for a wide array of input matrices—is a signiﬁcant leap forward on
both fronts. Yet  there are newly open problems and many possible extensions related to this work.
For instance  it would be interesting to investigate the performance of FPS a under weak  rather than
exact  sparsity assumption on Π (e.g.  ℓq  0 < q ≤ sparsity). The optimization problem (1) and
ADMM algorithm can easily be modiﬁed handle other types of penalties. In some cases  extensions
of Theorem 3.1 would require minimal modiﬁcations to its proof. Finally  the choices of dimension
d and regularization parameter λ are of great practical interest. Techniques like cross-validation
need to be carefully formulated and studied in the context of principal subspace estimation.

Acknowledgments
This research was supported in part by NSF grants DMS-0903120  DMS-1309998  BCS-0941518 
and NIH grant MH057881.

8

References
[1] A. d’Aspremont et al. “A direct formulation of sparse PCA using semideﬁnite programming ”. In: SIAM

[2]

[3]

Review 49.3 (2007).
I. M. Johnstone and A. Y. Lu. “On consistency and sparsity for principal components analysis in high
dimensions ”. In: JASA 104.486 (2009)  pp. 682–693.
I. T. Jolliffe  N. T. Trendaﬁlov  and M. Uddin. “A modiﬁed principal component technique based on the
Lasso ”. In: JCGS 12 (2003)  pp. 531–547.

[4] H. Zou  T. Hastie  and R. Tibshirani. “Sparse principal component analysis ”. In: JCGS 15.2 (2006) 

pp. 265–286.

[5] H. Shen and J. Z. Huang. “Sparse principal component analysis via regularized low rank matrix approx-

imation ”. In: Journal of Multivariate Analysis 99 (2008)  pp. 1015–1034.

[6] D. M. Witten  R. Tibshirani  and T. Hastie. “A penalized matrix decomposition  with applications to
sparse principal components and canonical correlation analysis ”. In: Biostatistics 10 (2009)  pp. 515–
534.

[7] M. Journee et al. “Generalized power method for sparse principal component analysis ”. In: JMLR 11

(2010)  pp. 517–553.

[8] B. K. Sriperumbudur  D. A. Torres  and G. R. G. Lanckriet. “A majorization-minimization approach to

the sparse generalized eigenvalue problem ”. In: Machine Learning 85.1–2 (2011)  pp. 3–39.

[9] Y. Zhang and L. E. Ghaoui. “Large-scale sparse principal component analysis with application to text

data ”. In: NIPS 24. Ed. by J. Shawe-Taylor et al. 2011  pp. 532–539.

[10] X. Yuan and T. Zhang. “Truncated power method for sparse eigenvalue problems ”. In: JMLR 14 (2013) 

pp. 899–925.

[11] A. A. Amini and M. J. Wainwright. “High-dimensional analysis of semideﬁnite relaxations for sparse

principal components ”. In: Ann. Statis. 37.5B (2009)  pp. 2877–2921.

[12] A. Birnbaum et al. “Minimax bounds for sparse pca with noisy high-dimensional data ”. In: Ann. Statis.

41.3 (2013)  pp. 1055–1084.

[13] V. Q. Vu and J. Lei. “Minimax rates of estimation for sparse PCA in high dimensions ”. In: AISTATS 15.

Ed. by N. Lawrence and M. Girolami. Vol. 22. JMLR W&CP. 2012  pp. 1278–1286.

[14] Q. Berthet and P. Rigollet. “Computational lower bounds for sparse PCA ”. In: (2013). arXiv: 1304.

0828.

[15] L. Mackey. “Deﬂation methods for sparse PCA ”. In: NIPS 21. Ed. by D. Koller et al. 2009  pp. 1017–

1024.

[16] Z. Ma. “Sparse principal component analysis and iterative thresholding ”. In: Ann. Statis. 41.2 (2013).
[17] T. T. Cai  Z. Ma  and Y. Wu. “Sparse PCA: optimal rates and adaptive estimation ”. In: Ann. Statis.

(2013). to appear. arXiv: 1211.1309.

[18] V. Q. Vu and J. Lei. “Minimax sparse principal subspace estimation in high dimensions ”. In: Ann. Statis.

(2013). to appear. arXiv: 1211.0373.

[19] S. Boyd et al. “Distributed optimization and statistical learning via the alternating direction method of

multipliers ”. In: Foundations and Trends in Machine Learning 3.1 (2010)  pp. 1–122.

[20] S. Ma. “Alternating direction method of multipliers for sparse principal component analysis ”. In: (2011).

arXiv: 1111.6703.

[21] R. Bhatia. Matrix analysis. Springer-Verlag  1997.
[22]
[23] K. Fan. “On a theorem of Weyl concerning eigenvalues of linear transformations I ”. In: Proceedings of

J. Dattorro. Convex optimization & euclidean distance geometry. Meboo Publishing USA  2005.

the National Academy of Sciences 35.11 (1949)  pp. 652–655.

[24] M. Overton and R. Womersley. “On the sum of the largest eigenvalues of a symmetric matrix ”. In: SIAM

Journal on Matrix Analysis and Applications 13.1 (1992)  pp. 41–45.

[25] S. N. Negahban et al. “A uniﬁed framework for the high-dimensional analysis of M-estimators with

decomposable regularizers ”. In: Statistical Science 27.4 (2012)  pp. 538–557.

[26] H. Liu et al. “High-dimensional semiparametric gaussian copula graphical models ”. In: Ann. Statis.

40.4 (2012)  pp. 2293–2326.

[27] W. H. Kruskal. “Ordinal measures of association ”. In: JASA 53.284 (1958)  pp. 814–861.
[28] H. Liu  J. Lafferty  and L. Wasserman. “The nonparanormal: semiparametric estimation of high dimen-

sional undirected graphs ”. In: JMLR 10 (2009)  pp. 2295–2328.

[29] F. Han and H. Liu. “Transelliptical component analysis ”. In: NIPS 25. Ed. by P. Bartlett et al. 2012 

pp. 368–376.

[30] F. Lindskog  A. McNeil  and U. Schmock. “Kendall’s tau for elliptical distributions ”. In: Credit Risk.

Ed. by G. Bol et al. Contributions to Economics. Physica-Verlag HD  2003  pp. 149–156.

9

,Vincent Vu
Juhee Cho
Jing Lei
Karl Rohe
Aryan Mokhtari
Alejandro Ribeiro