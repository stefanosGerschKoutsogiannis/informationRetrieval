2019,Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs,Spectral clustering is one of the most popular  yet still incompletely understood  methods for community detection on graphs. This article studies spectral clustering based on the Bethe-Hessian matrix H_r= (r^2−1)I_n+D−rA for sparse heterogeneous graphs (following the degree-corrected stochastic block model) in a two-class setting. For a specific value r=ζ  clustering is shown to be insensitive to the degree heterogeneity. We then study the behavior of the informative eigenvector of H_ζ and  as a result  predict the clustering accuracy. The article concludes with an overview of the generalization to more than two classes along with extensive simulations on synthetic and real networks corroborating our findings.,Revisiting the Bethe-Hessian: Improved Community

Detection in Sparse Heterogeneous Graphs

Lorenzo Dall’Amico

GIPSA-lab  UGA  CNRS  Grenoble INP

lorenzo.dall-amico@gipsa-lab.fr

Romain Couillet

GIPSA-lab  UGA  CNRS  Grenoble INP

L2S  CentraleSupélec  University of Paris Saclay

Nicolas Tremblay

GIPSA-lab  UGA  CNRS  Grenoble INP

Abstract

Spectral clustering is one of the most popular  yet still incompletely understood 
methods for community detection on graphs. This article studies spectral cluster-
ing based on the Bethe-Hessian matrix Hr = (r2 − 1)In + D − rA for sparse
heterogeneous graphs (following the degree-corrected stochastic block model) in a
two-class setting. For a speciﬁc value r = ζ  clustering is shown to be insensitive to
the degree heterogeneity. We then study the behavior of the informative eigenvector
of Hζ and  as a result  predict the clustering accuracy. The article concludes with
an overview of the generalization to more than two classes along with extensive
simulations on synthetic and real networks corroborating our ﬁndings.

1

Introduction

Network theory studies the interaction of connected systems of agents. Real networks tend to be
structured in afﬁnity classes and the problem of clustering consists in retrieving these unknown
classes from the observed network pairwise interactions [1]. Belief propagation (BP) is an efﬁcient
way to reconstruct communities and – under certain conditions (see [2]) – was proved to give optimal
reconstruction. On the negative side  BP suffers from a possibly long convergence time and a
non-trivial implementation. Among the alternative clustering algorithms  spectral techniques proved
particularly efﬁcient in terms of speed and analytical tractability [3  4  5  6]. In the dense regime  in
particular  where the average node degree scales like the size of the network  random matrix theory
[4  7  8] manages to predict the asymptotic spectral clustering performances and to identify transition
points beyond which asymptotic non trivial classiﬁcation is achievable. This is however not the
typical condition for real networks that tend instead to be sparse. For a graph G(V E) with |V| = n
nodes  the condition of sparsity means that the average degree d does not depend on the size of the
network and in particular d (cid:28) n.
Both standard spectral clustering methods and their associated random matrix asymptotics collapse
in this regime. As an answer  many intuitions emerged from statistical physics and led to important
seminal steps. Notably  two deeply connected matrices recently proved to overcome the problem of
sparsity: the n×n Bethe-Hessian [9] Hr with r ∈ R a parameter to be ﬁxed – the study of which is the
object of the present article–  and the non symmetric non backtracking operator B ∈ {0  1}2|E|×2|E|
[10]. Both matrices were introduced and studied under the homogeneous degree stochastic block
model (SBM). Narrowing to the case of two communities it was proved both experimentally and
theoretically [11  2  12  13] that  if there exists an algorithm able to detect communities better then
random guess  then these two matrices can be used to give non-trivial node partition. It is said that
both algorithms work down to the detectability threshold.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

However  real networks are rarely homogeneous and typically follow a power law degree distribution
[14]. The results of [15  16] generalize the above studies to heterogeneous networks  generated
by degree-corrected stochastic block models (DC-SBM) [17] and suggest that both B and Hr
provide also in this case non trivial clustering down to the detectability threshold. Yet  a precise
characterization of their behavior and performances is still lacking; the present article shows that
some aspects of the behavior of B and Hr have indeed been overlooked.
Spectral clustering in sparse heterogeneous networks has also been tackled using various regularized
Laplacian matrices [18  19  20] but  to our knowledge  these are not proved to operate down to the
detectability threshold. These structurally different methods are discussed in concluding remarks.
The main message of the present communication is that  under a DC-SBM setting  the choice of r
in Hr proposed in [9] for the SBM setting is suboptimal. We propose and theoretically support an
improved parametrization r = ζ that allows the Bethe-Hessian Hζ to efﬁciently detect communities
in sparse and heterogeneous graphs. In detail  under the DC-SBM setting  a) we propose a spectral
algorithm on Hζ which performs efﬁciently down to the detectability threshold  with an informative
eigenvector not tainted by the degree distribution (unlike in [9]); b) the algorithm is generalized to k-
class clustering with a consistent estimation procedure for k; c) substantial performance improvements
on the originally proposed Bethe-Hessian are testiﬁed by simulations on synthetic and real networks.
The remainder of the article is organized as follows: Section 2 argues on the optimal value r = ζ
for Hr and  based on heuristic arguments  studies the behavior of the informative eigenvector of
Hζ  concluding with an explicit expression of the clustering performance; Section 3 provides an
unsupervised method to estimate ζ  drawing on connections with the non-backtracking matrix B;
Section 4 extends the algorithm to a k-class scenario; numerical supports are then provided in
Section 5 on both synthetic and real networks; concluding remarks close the article.
Reproducibility. A Python implementation of the proposed algorithm along with codes to reproduce
the results of the article are available at lorenzodallamico.github.io/codes.

2 Model and Main Results

size (i.e. (cid:80)

2.1 Model setting
Consider an undirected binary graph G(E V)  with nodes V = {1  . . .   n} (|V| = n) and edges
E ⊂ V × V (|E| = m). Let σ ∈ {−1  1}n be the vector of class labels  both classes being of equal
cout cin ). These assumptions are meant to set the problem in
a more readable symmetric scenario. Section 4 extends the results to multiple classes of possibly
different sizes. In order to account both for sparsity and heterogeneity  we consider the DC-SBM as a
generative model for G. Denoting A ∈ {0  1}n×n the adjacency matrix deﬁned by Aij = 1(i j)∈E 
the DC-SBM generates edges independently according to:

i σi = 0)  and C = ( cin cout

P(Aij = 1|σi  σj  θi  θj) = θiθj

Cσi σj

n

 

(1)

where θ = (θ1  . . .   θn) is the vector of random intrinsic connection “probabilities” of each node.
The θi’s are assumed i.i.d. and independent of the class labels  and we impose E[θi] = 1  E[θ2
i ] = Φ.
The 1/n term bounds the degree of each node to an n-independent value  making the network sparse.
Denoting c = (cin + cout)/2  the detectability condition [16] reads:

α ≡ cin − cout√

c

≥ 2√
Φ

≡ αc.

(2)

For α < αc  no algorithm can partition the nodes better than by random guess. Letting D = diag(A1)
be the degree matrix  the Bethe-Hessian is deﬁned as

Hr = (r2 − 1)In + D − rA 

r ∈ R.

(3)

cΦ  which asymptotically provides non trivial
This matrix was originally proposed in [9] for r =
clustering down to the detectability threshold (for α > αc). The informative eigenvector of Hr is
associated with the second smallest eigenvalue and we denote it x(2)
are
however strongly tainted by the θi’s  sensibly altering the algorithm performance.

r . The components of x(2)√

cΦ

√

2

We show here that for α ≥ αc there exists a value ζ ≤ √

cΦ for which the components of the second
ζ of Hζ align to the labels irrespective of the θi’s  thus largely improving the algorithm

eigenvector x(2)
performance while maintaining detectability down to the threshold.

2.2

Informative eigenvector of Hr

and therefore P(σ∂i|σi) (cid:39)(cid:81)

In the sequel we assume that: (i) being sparse  we can locally approximate the graph by a tree [21]
P(σj|σi)  with ∂i the neighbourhood of i; (ii) n → ∞ and c is
bounded by an n-independent value while being arbitrarily larger than one  i.e.  n (cid:29) c (cid:29) 1.
For ease of notation we work here with D − rA rather than Hr  both having the same eigenvectors.
The core of our proposed method lies in the following observation  related to the action of Hr on σ:

j∈∂i

[(D − rA)σ]i = diσi

1 − r

(cid:34)

(cid:32)|∂(s)

|

|

− |∂(o)

i
di

i
di

(cid:33)(cid:35)

(4)

i

i

| (resp.  |∂(o)

where |∂(s)
|) stands for the number of neighbors of i belonging to the same (resp. 
opposite) class as i. We show next that a proper choice of r can annihilate the right-hand side of (4)
“on average” or whenever the typical degrees di are not too small  turning (4) into an eigenvector
equation. To this end  we need to quantify the random variables |∂(s)
From a Bayesian perspective  σ and θ are unknown parameters and A (and thus di) known realizations.
We may thus write

| and |∂(o)

|.

i

i

P(σi|σj  Aij = 1) =
∝

P(σi  σj|Aij = 1)
P(σj|Aij = 1)

(cid:90)(cid:90)

= 2

P(σi  σj  θi  θj|Aij = 1)dθidθj

P(Aij = 1|σi  σj  θi  θj)P(σi  σj  θi  θj)dθidθj ∝ C(σi  σj) 

r =

cin + cout
cin − cout

=

c

≡ ζα.

(6)

with α as in (2) the proper control parameter for the clustering problem (as shown e.g.  in [7  15  16 
22]). For simplicity of notation the dependence on α of ζ = ζα will be made explicit only when
relevant. Intuitively  this calculus suggests that ζ is the only value of r that ensures that Hr has an
informative eigenvector not signiﬁcantly tainted by the degree distribution. This claim is supported
by the following two remarks.
Remark 1 (Consistency of ζ for trivial classiﬁcation). In the limit of trivial clustering where cout → 0 
|∂(s)
| tend to their mean. In particular  for cout = 0  ζ = 1 and (D−ζA)σ = (D−A)σ =
0  so that σ is an exact eigenvector of Hζ=1 associated with its zero eigenvalue.
Remark 2 (Mapping to Ising). The original intuition behind the Bethe-Hessian matrix arises from a
mapping of the community labels into the spins of a Ising Hamiltonian. The “temperature-related”
parameter r guarantees a correct mapping only for r = ζ. This is elaborated in details in Section A
of the supplementary material.

| and |∂(o)

i

i

Although one commonly assumes an assortative model for the communities  by which cin > cout 
the Bethe-Hessian matrix is oblivious of the sign of cin − cout.

3

where we used the facts that the classes are of equal size (P(σi) is constant)  and the θi are i.i.d. 
independent of the classes with E[θi] = 1. Normalizing  one ﬁnally obtains P(σi|σj  Aij = 1) =
C(σi  σj)/(cin + cout)  which is independent of the degree distribution. We further know that
|∂(s)
| = di  which is a deterministic observation. Given the locally tree-like structure of the
graph  neighbors of the same node are conditionally independent – see (i) – so that |∂(s)
| is the sum
of di i.i.d. Bernoulli random variables with parameter p = cin/(cin + cout). We thus obtain

| + |∂(o)

i

i

i

E[[(D − rA)σ]i | A] = diσi

1 − r

.

(5)

(cid:19)

cin − cout
cin + cout

This equation suggests that  for the expectation of (4) to be an eigenvector equation in the large (but
ﬁnite) di regime  r should be taken equal to

(cid:90)(cid:90)

(cid:18)

√
2

α

Remark 3 (Disassortative networks). The case where cout > cin does not invalidate the above
analysis which results in ζ < 0. Clustering with Hζ is thus also valid in disassortative networks.

In practice  for a given (non averaged) realization of the σi’s  σ is not an exact eigenvector of Hζ.
By a perturbation analysis around σ  we next analyze the behavior of the corresponding informative
eigenvector of Hζ and theoretically predict the overlap performance.

2.3 Performance Analysis

ζ ≡ σ + δ.
To generalize the averaged analysis of (5)  we perturb σ by a “noise” term δ and write x(2)
Since ζ is however maintained  the associated eigenvalue of D − ζA  which is zero in (5)  now
possibly deviates from zero; this eigenvalue is denoted λα  i.e. 

(D − ζαA)(σ + δ) = λα(σ + δ).

(7)

From Remark 1  we already know that limα→√
In the following  expectations are taken for a ﬁxed realization of the network  i.e. E[·] ≡ E[·|A].
Writing |∂s
i | = di 
we obtain:
(8)

i |]+∆i and |∂o
[(D − ζαA)(σ + δ)]i = −2ζασi∆i + diδi − ζα

i |]−∆i  where we exploited the relation |∂s

i | = E[|∂o

i | = E[|∂s

i |+|∂o

(cid:88)

λα = 0.

δj.

2cin

The random variable ∆i is a sum of di independent (centered) Bernoulli random variables  tending in
the large c limit to a zero mean Gaussian  i.e. 

j∈∂i

√
cincout
cin − cout

(cid:114)

=

1
α

c − α2
4

.

(9)

∆i ∼ N (0  dicincout/(cin + cout)2) ≡ N (0  dif 2

α/ζ 2

α) 

fα ≡

αβ2

the sparse (and thus locally tree-like) nature of the graph.

Our analysis of (8) relies on the following claim that we shall justify next.
Assumption 1. The random variables δi  1 ≤ i ≤ n  are distributed as δi ∼ N (−µασi  f 2
i ) for
some µα ∈ R depending on α only  and βi ∈ R depending on i only. Besides  the δi’s are “weakly
dependent” in the sense that E[δiδj] = E[δi]E[δj] + O(1/c).
The elements of Assumption 1 rely on the following observations:
• Weak dependence: This claim follows from the weak dependence of the ∆i’s  which results from
• Gaussianity: The right-hand side of (8) features 3 random variables  the leftmost being Gaussian
and rightmost the sum of di variables tending to an (asymptotically independent) Gaussian. It is
• Mean of δi: The symmetry of the problem at hand (equal class sizes  same afﬁnity cin for each
class)  along with the fact that the right-hand side of (4) vanishes in its ﬁrst order approximation in
di  suggest that the mean of δi does not depend in the ﬁrst order on di but only on σi. The amplitude
of the mean then depends on α characterized here through µα.
• Variance of δi: The variance appears as the product of two terms: one that depends on i (βi) and
one that depends on α. This follows from assuming that the ﬂuctuations of δi follow the ﬂuctuations
of ∆i for which the variance is similarly factorized.

thus reasonable that δi be Gaussian (so to ensure (7)) yet not independent of ∆i or(cid:80)

j∈∂i

δj.

Imposing the norm of the eigenvector x(2)
ζ = σ + δ to be constant with respect to α and the boundary
condition µαc = 1 (i.e.  there is no information about the classes at the detectability threshold)  we
ﬁnd the following explicit expressions for µα and βi.

(cid:114) cΦ − ζ 2

α

cΦ − 1

1 − µα =

 

βi =

2√
di

.

Details are provided in Section B of the supplementary material. Figure 1-(a) supports the analysis by
comparing this prediction to simulations for a synthetic network with power law degree distribution.
The previous line of argument provides a large dimensional approximation for the performance
of spectral clustering based on the eigenvector x(2)
ζ . The performance measure of interest is the

4

√

Figure 1: (a) Theoretical values of mean and variance (red line indicates 1 − µα ± 2fα/
simulation (green dots) for power-law distributed θi’s (θi ∼ Z−1[U(3  10)]4). (b) Theoretical (10) vs
simulated overlap  averaged over 10 realizations  for θi constant (left)  and power-law distributed
(right). For both ﬁgures  n = 5000  cout = 6  cin = 7 → 36.

c) vs

(cid:2) 1

(cid:80)n
i=1 δσi ˆσi − 1

2

(cid:3)where ˆσ denotes the vector of estimated

overlap  deﬁned as Ov ≡ 2 maxP ˆσ
labels  Pˆσ the set of permutations of the labels  and δ the Kronecker symbol (δij = 1 if i = j  and
0 otherwise). In this particularly symmetric setting only ˆσi = sign[(x(2)
ζ )i] where sign is the sign
function. (Remark 5 underlines the necessity not to cluster based on sign in asymmetric scenarios).
From the expression of µα and βi  we ﬁnd that  conditionally to A 

n

(cid:34)(cid:115)

n(cid:88)

i=1

erf

E[Ov] (cid:39) 1
n

α2di

8c − 2α2

(cid:19)(cid:35)

(cid:18) cΦ − ζ 2

α

cΦ − 1

(10)

(proof details are provided in Section B of the supplementary material). Figure 1-(b) compares the
prediction of Equation (10) to simulations on networks with θi = 1 constant (left) or power-law
distributed (right). The observed match on this 5 000-node synthetic network is close to perfect.
As a side remark  our analysis reveals an interesting connection between Hζ and D−1A.
Remark 4 (Relation to the random walk Laplacian). Similar to A  D − A  and D− 1
2   the
matrix D−1A is claimed inappropriate as a spectral community detection matrix for sparse graphs.
This is in fact a slight overstatement: as already observed in [20]  as the graph under study gets
sparser  D−1A still possesses one or possibly more informative eigenvectors  however not necessarily
corresponding to dominant isolated eigenvalues (it was in particular noted that for the real network
polblogs [23] the informative eigenvector is associated to the third and not the second largest
eigenvalue). This observation is easily explained in our analysis framework. Similar to our derivation
for D − ζA  the average action of D−1A on the class vector σ reads E[[D−1Aσ]i|A] = σi/ζ
and thus  for large di  σ is a close eigenvector to D−1A  correctly predicting the existence of an
informative eigenvalue also for this matrix. However  the associated eigenvalue 1/ζ decays with
increasing ζ and thus with harder detection tasks  hence explaining why the informative eigenvectors
are associated with eigenvalues found deeper into the spectrum of D−1A.

2 AD− 1

3 Estimating ζ

√

√

cΦ  ζ is not readily accessible (as it depends on
While r = ζ is more appropriate a choice than r =
cin − cout)  unlike
cΦ that is easily estimated from the di’s. To estimate ζ  we elaborate on the deep
relations between the Bethe Hessian Hr and the non-backtracking operator B ∈ R2|E|×2|E| deﬁned 
for all (ij)  (lm) ∈ ED the set of directed edges of G  as B(ij)(lm) = δjl(1 − δim).
When r is an eigenvalue of B  then det Hr = 0 [11  24]. This is convenient as B only has a few
isolated real eigenvalues (B is non symmetric) that can send the associated isolated eigenvalues of
Hr to zero. This provides us with two alternative methods to estimate ζ.

3.1 Exploiting the eigenvalues outside the bulk of B

It is proved in [15] that  for the DC-SBM and beyond the phase transition (α > αc)  the eigenvalues
γ1  . . .   γ2m of B  decreasingly sorted in modulus  satisfy in the large n setting: γ1 → Φ(cin+cout)/2 
γ2 → Φ(cin − cout)/2 >

γ1 and  for i > 2  lim supn |γi| ≤ √

γ1  almost surely.

√

5

Since ζ = limn γ1/γ2  denoting νi(r) the eigenvalues of Hr sorted in increasing order  this result
conveys the following ﬁrst method to estimate ζ.

Figure 2: Superposed spectra of B for 3 values of α: n = 4000  cin = 12  11  10 and cout = 1  2  3
(cin + cout is ﬁxed); θ with power law distribution; all eigenvalues displayed in blue except top three
dominant real displayed in colors for each (cin  cout) pair.
Method 1 (First estimation of ζ). Under the previous notations ζ (cid:39) γ1/γ2. The eigenvalues γ1 and

γ2 of B can be estimated by a line search over r ∈ ((cid:112)ρ(B) ∞) on changing signs of ν1(r) and

ν2(r) that correspond to r = γ1 and r = γ2  respectively.1

3.2 Exploiting the eigenvalues inside the bulk of B

The matrix B can be obtained from the linearization of the belief propagation (BP) equations (see
[10] for details). In particular  the linear expansion to ﬁrst order of the beliefs around their ﬁxed points
yields Bw (cid:39) ζw. According to this argument  one expects the matrix B to have a real eigenvalue
equal to ζ with2 ζ ≤ √
cΦ. Figure 2 visually emphasizes this eigenvalue for three different values of
α  maintaining c constant. The matrix B thus has four eigenvalues inside its main bulk: −1  0  1 and
ζ. As the community detection problem gets harder  both ζ and γ2 shift towards the edge of the bulk
(from the left for the former and from the right for the latter) and then meet exactly at
cΦ when
α = αc. Then  for α < αc  they reach the complex part of the bulk.

√

√

α − 1)In in Equation (7) coincides with −(ζ 2

More fundamentally  simulations further suggest that the eigenvector associated with the null eigen-
value of Hζ is precisely x(2)
ζ = σ + δ studied in Section 2.3. This indicates that the informative
eigenvalue λα of D − ζαA = Hζα − (ζ 2
α − 1). It
further explains why H√
cΦ  initially proposed in [9]  works well close to the detectability threshold
as ζ → √
cΦ when α → αc. We thus expect most of the improvement of the choice r = ζ to emerge
in the easier scenarios.
Note that  as was already observed in [9]  if |r| > 1  then the eigenvalues of the bulk of Hr are strictly
positive for |r| (cid:54)=
is necessarily isolated when α > αc and so spectral
clustering on Hζ works down to the detectability threshold. To the best of our knowledge  this
property is not formally proved  but we point out that it agrees with the shape of the spectrum of B: if
the bulk of Hr was negative for some |r| > 1  then there would be a ‘continuum’ of real eigenvalues
in [1 
cΦ] if r > 1 (in the assortative case). As this is not the case  the smallest eigenvalue in the
bulk of Hr cannot be negative.
Claim 1 (Informative eigenvalue of Hζα). The eigenvalue associated to the informative eigenvector
of Hζα is equal to zero. Equivalently  the eigenvalue λα associated to the informative eigenvector of
D − ζαA is given by λα = −(ζ 2
This claim gives rise to a second method to estimate ζ.

cΦ. As a consequence  x(2)

√

ζ

1The spectral radius of the matrix B  ρ(B)  can be estimated as ρ(B) (cid:39)(cid:80)

α which vanishes for cout → 0.
i /(cid:80)

α − 1) = −4f 2

i d2

2This eigenvalue is visible in [10  11] but not commented.

i di.

6

Figure 3: Overlap comparison as a function of α  using the second smallest eigenvector of Hr  for
different values of r. In color code the values of r ranging from r = 1 (blue) to r = cΦ (yellow).
The red squares indicate r = (cin − cout)Φ/2  that is equivalent to clustering with the matrix B [10] 
the purple hexagons represent the Bethe-Hessian of [9]  the green diamonds are the proposed
Algorithm 1 and the blue crosses are the graph Laplacian. In the top left corner  a zoom of the
overlap close to the transition. For these simulations  n = 5000  cin : 15 → 9.4  cout : 1 → 6.6
(while keeping c ﬁxed)  θi ∼ [U(3  10)]4.

corresponds to the position of change of sign of ν2(r) in the set r ∈ (1 (cid:112)ρ(B)).

Method 2 (Second estimation of ζ). Under the previous notations ν2(ζ) = 0. The parameter ζ then

4 Extension to multiple uneven-sized classes

The analysis performed in the previous sections is resilient to heterogeneous degree distributions
and can be generalized to k uneven-sized classes  with last clustering step by k-means. To this
end  let Π ∈ Rk×k be diagonal with Πii the fraction of nodes in class i and assume CΠ1 = c1.
This assumption is a standard hypothesis [10  22  11  25] which ensures that the averaged node
connectivity is independent of the class. For 1 ≤ p ≤ k  let (τp  v(p)) be the p-th largest eigenpair
of CΠ  and u(p) ∈ Rn deﬁned as u(p)
∀ 1 ≤ i ≤ n for (cid:96)i the class of node i. The vector
u(p) contains plateaus with heights corresponding to the values of v(p). Repeating the arguments of
Section 2 (see details in Section C of the supplementary material)  we obtain k choices for r:

i = v(p)
(cid:96)i

E[[(D − rA)u(p)]i] = diu(p)

i

1 − r

τp
c

and thus

r =

c
τp

≡ ζp 

1 ≤ p ≤ k.

(11)

(cid:104)

(cid:105)

√

Since the largest eigenpair (c  1) of CΠ is not informative of the class structure  only the k − 1
next largest eigenvectors v(p) of CΠ are informative. The vector u(p) (corresponding to the p-th
largest eigenvalue τp) is in one-to-one mapping with v(p) and corresponds to the p-th smallest value
of ζp = c/τp. Considering r =
cΦ  all the informative eigenvalues of Hr are negative [9]. By
decreasing r they progressively become positive: for r = ζk (the largest among ζp) the k-th smallest
eigenvalue is the ﬁrst to hit zero. By further decreasing r  all the informative eigenvalues follow  until
r = ζ1 = 1 for which the smallest eigenvalue is null. We conclude that u(p) is associated with the
p-th smallest eigenvector x(p)
ζp
Method 1 and Method 2 both generalize to this scenario. In particular the outer eigenvalues of B
converge as γp → τpΦ and the linearization of BP retrieves the ﬁxed points as ζp = c/τp.
cΦ still plays an important role. It was chosen in [9] because  asymp-
For k > 2  the value r =
totically  for this value of r only the informative eigenvalues of H√
cΦ are negative. The number of
classes is then directly obtained from counting the number of negative eigenvalues of H√
cΦ. The
relation between Hr and B further guarantees that the number of isolated eigenvalues of B (hence of
Hr) is asymptotically equal to the number of detectable classes.

of Hζp.

√

7

Figure 4: (a) Comparison of spectral clustering for θi = 1 (left) and with power law distribution
θi ∼ Z−1[U(3  10)]4. “D−1A best” indicates spectral clustering on the best (among the ﬁrst 25)
eigenvector of D−1A. Here  n = 5000  cout = 1  cin = 2 → 16. Averaged over 10 samples. The
error bars indicate one standard deviation. (b) x(2)
distributed θi (left) and for θi = θ0  i ≤ n/4 and n/2 ≤ i ≤ 3n/4  and θi = 4θ0 otherwise (right).

(bottom) for power law

(top) and x(2)√

ζ

cΦ

Remark 5 (On k-means versus signed-based clustering). Under a symmetric 2-class of even size
setting  the classiﬁcation of the entries of the informative eigenvector of Hr can be performed based
on their signs. This sign-based method ﬁrst does not generalize to more than two or uneven sized
classes  where k-means or expectation-maximization based clustering is required. But it also hinders
the fact that the eigenvector entries may be quite concentrated around zero (close to 0+ or 0−
according to the class) and thus not clustered  a situation where k-means has no discriminative power.
Simulations (and reported results in [9] based on signs rather than k-means) suggest that the
informative eigenvector of H√
cΦ precisely suffers this condition. We have demonstrated here instead
that the informative eigenvector of Hζ has the convenient feature of being genuinely clustered.

5 Experimental validation

indicates the corresponding eigenvector.

Our results can be summarized by Algorithm 1  where we recall that νp(r) is the p-th smallest
eigenvalue of Hr and where x(p)
r
Figure 3 depicts the overlap  as a function of α  of the output of a two-class k-means on the informative
eigenvector of Hr  for different values of r  ranging from 1 to cΦ. When α is large enough  small
values of r lead to better partitions than large values of r that are more affected by degree heterogeneity.
However  for r small  the informative eigenvector is not necessarily corresponding to the second
smallest eigenvalue  leading to a meaningless partition. On the contrary  larger values of r show
for α → αc  ζ is "large enough" so that the informative eigenvalue is isolated  while for α → √
isolated eigenvectors also in the "hard regime". We recall that r = ζ is an α-dependent parameter:
2cin
it is "small enough" to give good partitions. Also the value of r = (cin − cout)Φ/2 is α-dependent
and it corresponds to clustering with B as indicated in [10]. While it gives good partitions very close
to the transition  this choice of r seems largely sub-optimal for easier tasks.
Figure 4-(a) compares the overlaps obtained with Algorithm 1 versus related spectral clustering
cΦ  D−1A and A. Accordingly with Remark 5  k-means clustering (rather than
methods based on H√
sign-based) on the informative eigenvectors is systematically performed. For θi = 1  the left display
recovers the results of [9]  evidencing a strong advantage for Hr versus Laplacian methods. Since the
degrees are similar  both r =
cΦ and r = ζ induce similar Hr performances. The improvement
provided by Hζ arises in the right display for power-law distributed θi  with most of the gain appearing
away from the detection threshold. On both displays is also depicted the performance of D−1A
based on its second largest eigenvector and on an oracle choice of the informative eigenvector with
maximal overlap. These curves conﬁrm Remark 4 on the non-dominant position of the informative
eigenvector of D−1A in hard tasks.3 Figure 4-(b) depicts the informative eigenvectors of H√
cΦ and
Hζ  demonstrating the negative impact of θi on H√

cΦ  in stark contrast with the resilience of Hζ.

√

3The low performance of D−1A  even in an oracle setting  can be attributed to the high density of eigenvalues
in the bulk of the spectrum which induces a “dispersion” of the informative eigenvectors to the eigenvectors
associated to neighboring eigenvalues. The class information is thus “spread” across several eigenvectors.

8

Algorithm 1 Improved Bethe-Hessian Community Detection
1: Input : adjacency matrix of undirected graph G
√
cΦ) < 0}|.
2: Detect the number of classes: ˆk ← |{i  νi(
3: for 2 ≤ p ≤ ˆk do
4:
5:
6: Estimate community labels ˆ(cid:96) as output of ˆk-class k-means on the rows of X = [X2  . . .   Xˆk].

ζp ← r such that νp(r) = 0
Xp ← x(p)

ζp

return Estimated number ˆk of communities and label vector ˆ(cid:96).

Table 1 next provides a comparison of the algorithm performances on real networks  both labelled
and unlabelled  conﬁrming the overall superiority of Algorithm 1  quite unlike H√
cΦ which fails on
several examples.4

L

A

n

U
Mail

n
34
62
105
115
1221

k
2
2
3
12
2

Alg.1 H√
1.00
0.97
0.77
0.92
0.91

cΦ
1.00
0.87
0.74
0.92
0.32

Karate [28]
Dolphins [29]
Polbooks [30]
Football [31]
Polblogs [23]
Table 1: Performance comparison on real networks. Labelled datasets with k known and overlap
comparison: (left). Unlabelled networks [32] with k estimated and modularity comparison. Only
assortative features are kept into account.

1.00
0.65
0.57
0.92
0.26

1133
4039
4941
6301
7115

Facebook
Power grid

Nutella
Wikipedia

k
21
65
53
5
21

Alg.1 H√
cΦ
0.40
0.50
0.77
0.48
0.61
0.92
0.15
0.34
0.18
0.21

A

0.32
0.38
0.31
0.14
0.15

6 Concluding Remarks

2 A(D + τ In)− 1

Beyond the demonstration of superiority of Hζ to H√
cΦ  originally proposed in [9]  the article
provides a consistent understanding of the natural limitations and strengths of the wide class of
spectral clustering methods involving combinations of A and D.
Yet  other methods  the performances of which cannot always be compared on even grounds  have
been proposed in the literature that marginally relate to the present study. This is notably the case
of [18] which performs spectral clustering on Lτ = (D + τ In)− 1
2 (with a proposed
choice τ = c) which aims at neutralizing the deleterious effects of small di. Although evidently
affecting the spectrum (and thus the informative structure) of A by the non-linear normalization 
simulations on Lτ suggest competitive performances to Hζ in almost all studied examples. A
systematic analysis of this and similarly proposed methods in the literature is clearly called for.
Despite its demonstrated signiﬁcant performance improvement  Algorithm 1 suffers from a slightly
larger computational cost than most competing methods (O(nk3) instead of the usual O(nk2)
complexity in the case of sparse graph) due to the successive estimations of ζ. We are currently
working on improving this computation time.
From a theoretical standpoint  the request for c (cid:29) 1 is still inappropriate to many practical networks.
A ﬁrst consequence of smaller values for c is the loss of Gaussianity of the eigenvector entries as
already evidenced in Figures 1 and 4 where Gaussianity is clearly lost in the easiest tasks in proﬁt of
a “one-sided” distribution. This suggests further improvement of our analysis framework along with
the development of algorithms more appropriate than k-means to handle the last clustering step.

Acknowledgments

This work is supported by the ANR Project RMT4GRAPH (ANR-14-CE28-0006)  the IDEX GSTATS
Chair at University Grenoble Alpes and by CNRS PEPS I3A (Project RW4SPEC). The authors thank
Jean-Louis Barrat for fruitful discussions.

4In Table 1  the modularity is deﬁned as M = 1
2|E|

Aij − didj
2|E|

i j=1

δ(ˆ(cid:96)i  ˆ(cid:96)j)  see e.g.  [26  27].

(cid:17)

(cid:16)

(cid:80)n

9

References
[1] Santo Fortunato. Community detection in graphs. Physics reports  486(3-5):75–174  2010.

[2] Elchanan Mossel  Joe Neeman  and Allan Sly. Belief propagation  robust reconstruction and
optimal recovery of block models. In Conference on Learning Theory  pages 356–370  2014.

[3] Karl Rohe  Sourav Chatterjee  Bin Yu  et al. Spectral clustering and the high-dimensional

stochastic blockmodel. The Annals of Statistics  39(4):1878–1915  2011.

[4] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing  17(4):395–416 

2007.

[5] Lennart Gulikers  Marc Lelarge  and Laurent Massoulié. A spectral method for community
detection in moderately sparse degree-corrected stochastic block models. Advances in Applied
Probability  49(3):686–721  2017.

[6] J. Lei and A. Rinaldo. Consistency of spectral clustering in stochastic block models. The Annals

of Statistics  43(1):215–237  2015.

[7] Raj Rao Nadakuditi and Mark EJ Newman. Graph spectra and the detectability of community

structure in networks. Physical review letters  108(18):188701  2012.

[8] Haﬁz Tiomoko Ali and Romain Couillet. Random matrix improved community detection in
heterogeneous networks. In Signals  Systems and Computers  2016 50th Asilomar Conference
on  pages 1385–1389. IEEE  2016.

[9] Alaa Saade  Florent Krzakala  and Lenka Zdeborová. Spectral clustering of graphs with the
bethe hessian. In Advances in Neural Information Processing Systems  pages 406–414  2014.

[10] Florent Krzakala  Cristopher Moore  Elchanan Mossel  Joe Neeman  Allan Sly  Lenka Zde-
borová  and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the
National Academy of Sciences  110(52):20935–20940  2013.

[11] Charles Bordenave  Marc Lelarge  and Laurent Massoulié. Non-backtracking spectrum of
random graphs: community detection and non-regular ramanujan graphs. In Foundations of
Computer Science (FOCS)  2015 IEEE 56th Annual Symposium on  pages 1347–1357. IEEE 
2015.

[12] Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing  pages 694–703.
ACM  2014.

[13] Elchanan Mossel  Joe Neeman  and Allan Sly. Reconstruction and estimation in the planted

partition model. Probability Theory and Related Fields  162(3-4):431–461  2015.

[14] Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. science 

286(5439):509–512  1999.

[15] Lennart Gulikers  Marc Lelarge  and Laurent Massoulié. Non-Backtracking Spectrum of
In 8th Innovations in Theoretical Computer
Degree-Corrected Stochastic Block Models.
Science Conference (ITCS 2017)  volume 67 of Leibniz International Proceedings in Informatics
(LIPIcs)  pages 44:1–44:27  2017.

[16] Lennart Gulikers  Marc Lelarge  Laurent Massoulié  et al. An impossibility result for recon-
struction in the degree-corrected stochastic block model. The Annals of Applied Probability 
28(5):3002–3027  2018.

[17] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in

networks. Physical review E  83(1):016107  2011.

[18] Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic
blockmodel. In Advances in Neural Information Processing Systems  pages 3120–3128  2013.

10

[19] Can M Le  Elizaveta Levina  and Roman Vershynin. Concentration and regularization of random

graphs. Random Structures & Algorithms  51(3):538–561  2017.

[20] Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. arXiv preprint

arXiv:1312.1733  2013.

[21] Amir Dembo  Andrea Montanari  et al. Gibbs measures and phase transitions on sparse random

graphs. Brazilian Journal of Probability and Statistics  24(2):137–211  2010.

[22] Aurelien Decelle  Florent Krzakala  Cristopher Moore  and Lenka Zdeborová. Asymptotic
analysis of the stochastic block model for modular networks and its algorithmic applications.
Physical Review E  84(6):066106  2011.

[23] Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election: divided
they blog. In Proceedings of the 3rd international workshop on Link discovery  pages 36–43.
ACM  2005.

[24] Audrey Terras. Zeta functions of graphs: a stroll through the garden  volume 128. Cambridge

University Press  2010.

[25] Lenka Zdeborová and Florent Krzakala. Statistical physics of inference: Thresholds and

algorithms. Advances in Physics  65(5):453–552  2016.

[26] Mark EJ Newman and Michelle Girvan. Finding and evaluating community structure in

networks. Physical review E  69(2):026113  2004.

[27] M. E. J. Newman. Modularity and community structure in networks. Proceedings of the

National Academy of Sciences  103:8577–8582  2006.

[28] Wayne W Zachary. An information ﬂow model for conﬂict and ﬁssion in small groups. Journal

of anthropological research  33(4):452–473  1977.

[29] David Lusseau  Karsten Schneider  Oliver J Boisseau  Patti Haase  Elisabeth Slooten  and
Steve M Dawson. The bottlenose dolphin community of doubtful sound features a large
proportion of long-lasting associations. Behavioral Ecology and Sociobiology  54(4):396–405 
2003.

[30] http://www.orgnet.com/.

[31] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks.

Proceedings of the national academy of sciences  99(12):7821–7826  2002.

[32] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection.

http://snap.stanford.edu/data  June 2014.

11

,Lorenzo Dall'Amico
Romain Couillet
Nicolas Tremblay