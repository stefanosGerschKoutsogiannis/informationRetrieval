2009,Linearly constrained Bayesian matrix factorization for blind source separation,We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of non-negative matrix factorization and factor analysis.  The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.,Linearly constrained Bayesian matrix factorization

for blind source separation

Mikkel N. Schmidt

Department of Engineering
University of Cambridge

mns@imm.dtu.dk

Abstract

We present a general Bayesian approach to probabilistic matrix factorization sub-
ject to linear constraints. The approach is based on a Gaussian observation model
and Gaussian priors with bilinear equality and inequality constraints. We present
an efﬁcient Markov chain Monte Carlo inference procedure based on Gibbs sam-
pling. Special cases of the proposed model are Bayesian formulations of non-
negative matrix factorization and factor analysis. The method is evaluated on a
blind source separation problem. We demonstrate that our algorithm can be used
to extract meaningful and interpretable features that are remarkably different from
features extracted using existing related matrix factorization techniques.

1 Introduction

Source separation problems arise when a number of signals are mixed together  and the objective
is to estimate the underlying sources based on the observed mixture. In the supervised  model-
based approach to source separation  examples of isolated sources are used to train source models 
which are then combined in order to separate a mixture. Oppositely  in unsupervised  blind source
separation  only very general information about the sources is available. Instead of estimating mod-
els of the sources  blind source separation is based on relatively weak criteria such as minimizing
correlations  maximizing statistical independence  or ﬁtting data subject to constraints.

Under the assumptions of linear mixing and additive noise  blind source separation can be expressed
as a matrix factorization problem 

X

I×J

= A

I×K

B

K×J

+ N

I×J

 

or equivalently  xij =

aikbkj + nij  

(1)

K

Xk=1

where the subscripts below the matrices denote their dimensions. The columns of A represent K
unknown sources  and the elements of B are the mixing coefﬁcients. Each of the J columns of
X contains an observation that is a mixture of the sources plus additive noise represented by the
columns of N. The objective is to estimate the sources  A  as well as the mixing coefﬁcients  B 
when only the data matrix  X  is observed. In a Bayesian formulation  the aim is not to compute a
single value for A and B  but to infer their posterior distribution under a set of model assumptions.
These assumptions are speciﬁed in the likelihood function  p(X|A  B)  which expresses the proba-
bility of the data given the factorizing matrices  and in the prior  p(A  B)  which describes available
knowledge before observing the data. Depending on the speciﬁc choice of likelihood and priors 
matrix factorizations with different characteristics can be devised.

Non-negative matrix factorization (NMF)  which is distinguished from other matrix factorization
techniques by its non-negativity constraints  has been shown to decompose data into meaningful 
interpretable parts [3]; however  a parts-based decomposition is not necessarily useful  unless it

1

Linear subspace

Afﬁne subspace

Simplicial cone

Simplicial cone in
non-neg. orthant

No constraints

(a)

Polytope

bkj ≥ 0  Pkbkj = 1

(e)

Pkbkj = 1

Polytope in

(b)

non-neg. orthant

aik ≥ 0 

bkj ≥ 0  Pkbkj = 1

(f)

bkj ≥ 0

(c)

Polytope on
unit simplex

aik ≥ 0  bkj ≥ 0

(d)

Polytope in

unit hypercube

aik ≥ 0  Piaik = 1 
bkj ≥ 0  Pkbkj = 1

(g)

0 ≤ aik ≤ 1 

bkj ≥ 0  Pkbkj = 1

(h)

Figure 1: Examples of model spaces that can be attained using matrix factorization with different
indicates the feasible region for the source
linear constraints in A and B. The red hatched area
vectors (columns of A). Dots 
  are examples of speciﬁc positions of source vectors  and the black
hatched area 
  is the corresponding feasible region for the data vectors. Special cases include (a)
factor analysis and (d) non-negative matrix factorization.

ﬁnds the “correct” parts. The main contribution in this paper is that specifying relevant constraints
other than non-negativity substantially changes the qualities of the results obtained using matrix
factorization. Some intuition about how the incorporation of different constraints affects the matrix
factorization can be gained by considering their geometric implications. Figure 1 shows how differ-
ent linear constraints on A and B constrain the model space. For example  if the mixing coefﬁcients
are constrained to be non-negative  data is modelled as the convex hull of a simplicial cone  and if
the mixing coefﬁcients are further constrained to sum to unity  data is modelled as the hull of a
convex polytope.

In this paper  we develop a general and ﬂexible framework for Bayesian matrix factorization  in
which the unknown sources and the mixing coefﬁcients are treated as hidden variables. Furthermore 
we allow any number of linear equality or inequality constraints to be speciﬁed. On an unsupervised
image separation problem  we demonstrate  that when relevant constraints are speciﬁed  the method
ﬁnds interpretable features that are remarkably different from features computed using other matrix
factorization techniques.

The proposed method is related to recently proposed Bayesian matrix factorization techniques:
Bayesian matrix factorization based on Gibbs sampling has been demonstrated [7  8] to scale up
to very large datasets and to avoid the problem of overﬁtting associated with non-Bayesian tech-
niques. Bayesian methods for non-negative matrix factorization have also been proposed  either
based on variational inference [1] or Gibbs sampling [4  9]. The latter can be seen as special cases
of the algorithm proposed here.

The paper is structured as follows: In section 2  the linearly constrained Bayesian matrix factoriza-
tion model is described. Section 3 presents an inference procedure based on Gibbs sampling. In
Section 4  the method is applied to an unsupervised source separation problem and compared to
other existing matrix factorization methods. We discuss our results and conclude in Section 5.

2

2 The linearly constrained Bayesian matrix factorization model

In the following  we describe the linearly constrained Bayesian matrix factorization model. We
make speciﬁc choices for the likelihood and priors that keep the formulation general while allowing
for efﬁcient inference based on Gibbs sampling.

2.1 Noise model

We choose an iid. zero mean Gaussian noise model 

p(nij ) = N (nij|0  vij) =

1√2πvij

exp(cid:16)−

n2
ij

2vij(cid:17)  

where  in the most general formulation  each matrix element has its own variance  vij; however  the
variance parameters can easily be joined  e.g.  to have a single noise variance per row or just one
overall variance  which corresponds to an isotropic noise model. The noise model gives rise to the
likelihood  i.e.  the probability of the observations given the parameters of the model. The likelihood
is given by

I

J

K

Yi=1

Yj=1

(cid:19)  (3)
p(x|θ) =
where θ = (cid:8)A  B {vij}(cid:9) denotes all parameters in the model. For the noise variance parameters

we choose conjugate inverse-gamma priors 

aikbkj   vij(cid:17) =

N(cid:16)xij(cid:12)(cid:12)(cid:12)

k=1 aikbkj)2
2vij

(x −PK

p2πvij

exp(cid:18)−

Xk=1

Yj=1

Yi=1

1

J

I

p(vij ) = IG(vij|α  β) =

βα
Γ(α)

v−(α+1)
ij

exp(cid:18)−β
vij(cid:19) .

2.2 Priors for sources and mixing coefﬁcients

We now deﬁne the prior distribution for the factorizing matrices  A and B. To simplify the notation 
we specify the matrices by vectors a = vec(A⊤) = [a11  a12  . . .   aIK]⊤ and b = vec(B) =
[b11  b21  . . .   bKJ ]⊤. We choose a Gaussian prior over a and b subject to inequality constraints  Q 
and equality constraints  R 

p(a  b) ∝




b(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N (cid:20)a

0 

µb(cid:21)
(cid:20)µa
|{z}≡µ

Σ⊤ab

 (cid:20) Σa Σab
Σb(cid:21)
}
|

{z

≡Σ

! 

if Q(a  b) ≤ 0  R(a  b) = 0 

(5)

otherwise.

In slight abuse of denotation  we refer to µ and Σ as the mean and covariance matrix  although the
actual mean and covariance of a and b depends on the constraints.
In the most general formulation  the constraints  Q : RIK×RKJ→ RNQ and R : RIK×RKJ→ RNR 
are biafﬁne maps  that deﬁne NQ inequality and NR equality constraints jointly in a and b. Specif-
ically  each inequality constraint has the form

Qm(a  b) = qm + a⊤q(a)

m + b⊤q(b)

m + a⊤Q(ab)

By rearranging terms and combining the NQ constraints in matrix notation  we may write

(2)

(4)

(6)

(7)

1 +Q(ab)

1

hq(a)
|

+Q(ab)
NQ

NQ

b ··· q(a)
{z
≡Qa

⊤a ≤
bi

}
|

−q1−b⊤q(b)

1

−qNQ−b⊤q(b)

NQ

...

≡qa

{z

m b ≤ 0.


}

 

Q⊤a a ≤ qa 

from which it is clear that the constraints are linear in a. Likewise  the constraints can be rearranged
to a linear form in b. The equality constraints  R  are deﬁned analogously.

This general formulation of the priors allows all elements of a and b to have prior dependencies
both through their covariance matrix  Σab  and through the joint constraints; however  in some

3

µ  Σ  Q  R

aik

k=1...K

bkj

i

=

1

xij

.

.

.

I

α  β

vij

j = 1...J

Figure 2: Graphical model for linearly constrained Bayesian matrix factorization  when A and B are
independent in the prior. White and grey nodes represent latent and observed variables respectively 
and arrows indicate stochastic dependensies. The colored plates denote repeated variables over the
indicated indices.

applications it is not relevant or practical to specify all of these dependencies in advance. We may
restrict the model such that a and b are independent a priori by setting Σab  Q(ab)
m to zero 
and restricting q(a)
m 6= 0 and vice versa. Furthermore  we can decouple
the elements of A  or groups of elements such as rows or columns  by choosing Σa  Qa  and Ra to
have an appropriate block structure. Similarly we can decouple elements of B.

m = 0 for all m where q(b)

m   and R(ab)

2.3 Posterior distribution

Having speciﬁed the model and the prior densities  we can now write the posterior  which is the
distribution of the parameters conditioned on the observed data and hyper-parameters. The posterior
is given by

p(θ|x  ψ) ∝ p(a  b)p(x|θ)

p(vij) 

(8)

where ψ = {α  β  µ  Σ  Q  R} denotes all hyper-parameters in the model. A graphical representa-
tion of the model is given in Figure 2.

I

Yi=1

J

Yj=1

3 Inference

In a Bayesian framework  we are interested in computing the posterior distribution over the param-
eters  p(θ|x  ψ). The posterior  given in Eq. (8)  is only known up to a multiplicative constant  and
direct computation of this normalizing constant involves integrating over the unnormalized poste-
rior  which is not analytically tractable. Instead  we approximate the posterior distribution using
Markov chain Monte Carlo (MCMC).

3.1 Gibbs sampling

We propose an inference procedure based on Gibbs sampling. Gibbs sampling is applicable when
the joint density of the parameters is not known  but the parameters can be partitioned into groups 
such that their posterior conditional densities are known. We iteratively sweep through the groups of
parameters and generate a random sample for each  conditioned on the current value of the others.
This procedure forms a homogenous Markov chain and its stationary distribution is exactly the joint
posterior.

In the following  we derive the posterior conditional densities required in the Gibbs sampler. First 
we consider the noise variances  vij. Due to the choice of conjugate prior  the posterior density is an
inverse-gamma 

p(vij|θ\vij) = IG(vij|¯α  ¯β) 
2(cid:0)xij −PK
¯α = α + 1
2  

¯β = β + 1

k=1 aikbkj(cid:1)2

 

(9)

(10)

4

from which samples can be generated using standard acceptance-rejection methods.

p(a|b) ∝( N (a| ˜µa  ˜Σa) 
b (b − µb) 

˜µa = µa + ΣabΣ−1

Next  we consider the factorizing matrices  represented by the vectors a and b. We only discuss
generating samples from a  since the sampling procedure for b is identical due to the symmetry of
the model. Conditioned on b  the prior density of a is a constrained Gaussian 
if Q⊤a a ≤ qa  R⊤a a = ra 
otherwise 

(11)

0 

(12)
where we have used Eq. (7) and the standard result for a conditional Gaussian density. In the special
case when a and b are independent in the prior  we simply have ˜µa = µa and ˜Σa = Σa. Further 
conditioning on the data leads to the ﬁnal expression for the posterior conditional density of a 

˜Σa = Σa − ΣabΣ−1

Σ⊤ab 

b

p(a|x  θ\a) ∝( N (a| ¯µa  ¯Σa) 
¯Σa =(cid:0) ˜Σ−1
a + ˜BV −1 ˜B⊤(cid:1)−1

0 

 

if Q⊤a a ≤ qa  R⊤a a = ra 
otherwise 
¯µa = ¯Σa(cid:0) ˜Σ−1
˜µa + ˜BV −1x(cid:1) 

a

(14)
where V = diag(v11  v12  . . .   vIJ ) and ˜B = diag(B  . . .   B) is a diagonal block matrix with I
repetitions of B.

(13)

The Gibbs sampler proceeds iteratively: First  the noise variance is generated from the inverse-
gamma density in Eq. (9); second  a is generated from the constrained Gaussian density in Eq. (13);
and third  b is generated from a constrained Gaussian analogous to Eq. (13).

3.2 Sampling from a constrained Gaussian

An essential component in the proposed matrix factorization method is an algorithm for generat-
ing random samples from a multivariate Gaussian density subject to linear equality and inequality
constraints. With a slight change of notation  we consider generating x ∈ RN from the density

p(x) ∝( N (x|µx  Σx) 

0 

if Q⊤xx ≤ qx  R⊤xx = rx 
otherwise.

A similar problem has previously been treated by Geweke [2]  who proposes a Gibbs sampling
procedure  that does not handle equality constraints and no more than N inequality constraints.
Rodriguez-Yam et al. [6] extends the method in [2] to an arbitrary number of inequality constraints 
but do not provide an algorithm for handling equality constraints. Here  we present a general Gibbs
sampling procedure that handles any number of equality and inequality constraints.
The equality constraints restrict the distribution to an afﬁne subspace of dimensionality N − R 
where R is the number of linearly independent constraints. The conditional distribution on that
subspace is a Gaussian subject to inequality constraints. To handle the equality constraints  we map
the distribution onto this subspace. Using the singular value decomposition (SVD)  we can robustly
compute an orthonormal basis  T   for the constraints  as well as its orthogonal complement  T⊥ 

(15)

(16)

(18)

(19)

Rx = U SV ⊤ =(cid:20) T

T⊥(cid:21)⊤(cid:20)ST

0

0

0(cid:21) V ⊤ 

where ST = diag(s1  . . .   sR) holds the R non-zero singular values. We now deﬁne a transformed
variable  y  that is related to x by

(17)
where x0 is some vector that satisﬁes the equality constraints  e.g.  computed using the pseudo-
inverse  x0 = R†⊤x rx. This transformation ensures  that for any value of y  the corresponding x
satisﬁes the equality constraints. We can now compute the distribution of y conditioned on the
equality constraints  which is Gaussian subject to inequality constraints 

y = T⊥(x − x0)  y ∈ RN−R

p(y|R⊤xx = rx) ∝( N (y|µy  Σy)

if Q⊤y y ≤ qy
otherwise 

0
µy = Λ(µx − x0)  Σy = ΛΣxT ⊤
⊥

  Qy = T⊥Qx 

qy = qx − Q⊤xx0 

5

where Λ = T⊥(I − ΣxT ⊤(T ΣxT ⊤)−1T ).
We introduce a second transformation with the purpose of reducing the correlations between the
variables. This may potentially improve the sampling procedure  because Gibbs sampling can suffer
from slow mixing when the distribution is highly correlated. Correlations between the elements
of y are due to both the Gaussian covariance structure and the inequality constraints; however 
for simplicity we only decorrelate with respect to the covariance of the underlying unconstrained
Gaussian. To this end  we deﬁne the transformed variable  z  given by

where L is the Cholesky factorization of the covariance matrix  LL⊤ = Σy. The distribution of z
is then a standard Gaussian subject to inequality constraints 

z = L−⊤(y − µy) 

(20)

p(z|R⊤xx = rx) ∝( N (z|0  I) 

if Q⊤z z ≤ qz 
otherwise 

Qz = LQy 

0 
qz = qy − Q⊤y µy.

(21)

(22)

We can now sample from z using a Gibbs sampling procedure by sweeping over the elements zi
and generating samples from their conditional distributions  which are univariate truncated standard
Gaussian 

p(zi|z\zi) = q 2

i

π exp(cid:16) −z2
2 (cid:17)

erf(cid:16) ui√2(cid:17) − erf(cid:16) ℓi√2(cid:17) ∝(cid:26) N (zi|0  1) 

0 

ℓi ≤ zi ≤ ui 
otherwise.

(23)

Samples from this density can be generated using standard methods such as inverse transform sam-
pling (transforming a uniform random variable by the inverse cumulative density function); the
efﬁcient mixed rejection sampling algorithm proposed by Geweke [2]; or slice sampling [5].

The upper and lower points of truncation can be computed as

[Qz]⊤i:

Q⊤z z ≤ qz
zi ≤ qz − [Qz]⊤˜i: z˜i
| {z }d
{z
}
|
: dk > 0(cid:9) = ui 
: dk < 0(cid:9) ≤ zi ≤ min(cid:8)∞  nk

dk

n

ℓi = max(cid:8)−∞  nk

dk

(24)
(25)

(26)

where [Qz]i: denotes the ith row of Qz  [Qz]˜i: denotes all rows except the ith  and z˜i denotes the
vector of all elements of z except the ith.

Finally  when a sample of z has been generated after a number of Gibbs sweeps  it can be trans-
formed into a sample of the original variable  x  using

The sampling procedure is illustrated in Figure 3.

x = T ⊤
⊥

(L⊤z + µy) + x0.

(27)

4 Experiments

We demonstrate the proposed linearly constrained Bayesian matrix factorization method on a blind
image separation problem  and compare it to two other matrix factorization techniques: independent
component analysis (ICA) and non-negative matrix factorization (NMF).

Data We used a subset from the MNIST dataset which consists of 28 × 28 pixel grayscale images
of handwritten digits (see Figure 4.a). We selected the ﬁrst 800 images of each digit  0–9  which
gave us 8  000 unique images. From these images we created 4  000 image mixtures by adding the
grayscale intensities of the images two and two  such that the different digits were combined in equal
proportion. We rescaled the mixed images so that their pixel intensities were in the 0–1 interval  and
arranged the vectorized images as the columns of the matrix X ∈ RI×J   where I = 784 and
J = 4  000. Examples of the image mixtures are shown in Figure 4.b.

6

x2

x∗

p(x1|x2 = x∗)

x2

1

2

5

3 4

x1

x1

x1

ℓ

u

(a)

ℓ

u

(b)

(c)

Figure 3: Gibbs sampling from a multivariate Gaussian density subject to linear constraints. a) Two-
dimensional Gaussian subject to three inequality constraints. b) The conditional distribution of x1
given x2 = x∗ is a truncated Gaussian. c) Gibbs sampling proceeds iteratively by sweeping over
the dimensions and sampling from the conditional distribution in each dimension conditioned on the
current value in the other dimensions.

Task The objective is to factorize the data matrix in order to ﬁnd a number of source images that
explain the data. Ideally  the sources should correspond to the original digits. We cannot hope to
ﬁnd exactly 10 sources that each corresponds to a digit  because there are large variations as to how
each digit is written. For that reason  we used 40 hidden sources in our experiments  which allowed
4 exemplars on average for each digit.

Method For comparison we factorized the mixed image data using two standard matrix factor-
ization techniques: ICA  where we used the FastICA algorithm  and NMF  where we used Lee and
Seung’s multiplicative update algorithm [3]. The sources determined using these methods are shown
in Figure 4.c–d.

For the linearly constrained Bayesian matrix factorization  we used an isotropic noise model. We
chose a decoupled prior for A and B with zero mean  µ = 0  and unit diagonal covariance matrix 
Σ = I. The hidden sources were constrained to have the same range of pixel intensities as the
image mixtures  0 ≤ aik ≤ 1. This constraint allows the sources to be interpreted as images since
pixel intensities outside this interval are not meaningful. The mixing coefﬁcients were constrained
to be non-negative  bkj ≥ 0  and sum to unity PK
k=1 bkj = 1; thus  the observed data is modeled
as a convex combination of the sources. The constraints ensure that only additive combinations of
the sources are allowed  and introduces a negative correlation between the mixing coefﬁcients. This
negative correlation implies that if one source contributes more to a mixture the other sources must
correspondingly contribute less. The idea behind this constraint is that it will lead the sources to
compete as opposed to collaborate to explain the data. A geometric interpretation of the constraints
is illustrated in Figure 1.h: The data vectors are modeled by a convex polytope in the non-negative
unit hypercube  and the hidden sources are the vertices of this polytope. We computed 10  000 Gibbs
samples  which appeared sufﬁcient for the sampler to converge. The result of the matrix factorization
are shown in Figure 4.e  which displays a single sample of A at the last iteration.

Results
In ICA (see Figure 4.c) the sources are not constrained to be non-negative  and therefore
do not have a direct interpretation as images. Most of the computed sources are complex patterns 
that appear to be dominated by a single digit. While ICA certainly does ﬁnd structure in the data 
the estimated sources lack a clear interpretation.

The sources computed using NMF (see Figure 4.d) have the property which Lee and Seung [3]
refer to as a parts-based representation. Spatially  the sources are local as opposed to global. The
decomposition has an intuitive interpretation: Each source is a short line segment or a dot  and the
different digits can be constructed by combining these parts.

Linearly constrained Bayesian matrix factorization (see Figure 4.e) computes sources with a very
clear and intuitive interpretation: Almost all of the 40 computed sources visually resemble hand-
written digits  and are thus well aligned with the sources that were used to generate the mixtures.
Compared to the original data  the computed sources are a bit bolder and have slightly smeared

7

(a) Original dataset: MNIST digits

(b) Training data: Mixture of digits

(c) Independent component analysis

(d) Non-negative matrix factorization

(e) Linearly constrained Bayesian matrix factorization

Figure 4: Data and results of the analyses of an image separation problem. a) The MNIST digits data
(20 examples shown) used to generate the mixture data. b) The mixture data consists of 4000 images
of two mixed digits (20 examples shown). c) Sources computed using independent component
analysis (color indicate sign). d) Sources computed using non-negative matrix factorization. e)
Sources computed using linearly constrained Bayesian matrix factorization (details explained in the
text).

edges. Two sources stand out: One is a black blob of approximately the same size as the digits  and
another is an all white feature  which are useful for adjusting the brightness.

5 Conclusions

We presented a linearly constrained Bayesian matrix factorization method as well as an inference
procedure for this model. On an unsupervised image separation problem  we have demonstrated that
the method ﬁnds sources that have a clear an interpretable meaning. As opposed to ICA and NMF 
our method ﬁnds sources that visually resemble handwritten digits.

We formulated the model in general terms  which allows speciﬁc prior information to be incorpo-
rated in the factorization. The Gaussian priors over the sources can be used if knowledge is available
about the covariance structure of the sources  e.g.  if the sources are known to be smooth. The con-
straints we used in our experiments were separate for A and B  but the framework allows bilinearly
dependent constraints to be speciﬁed  which can be used for example to specify constraints in the
data domain  i.e.  on the product AB.

As a general framework for constrained Bayesian matrix factorization  the proposed method has
applications in many other areas than blind source separation. Interesting applications include blind
deconvolution  music transcription  spectral unmixing  and collaborative ﬁltering. The method can
also be used in a supervised source separation setting  where the distributions over sources and
mixing coefﬁcients are learned from a training set of isolated sources. It is an interesting challenge
to develop methods for learning relevant constraints from data.

8

References
[1] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational

Intelligence and Neuroscience  2009. doi: 10.1155/2009/785152.

[2] J. Geweke. Efﬁcient simulation from the multivariate normal and student-t distributions subject
to linear constraints and the evaluation of constraint probabilities. In Computer Sciences and
Statistics  Proceedings the 23rd Symposium on the Interface between  pages 571–578  1991.
doi: 10.1.1.26.6892.

[3] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization.

Nature  pages 788–791  October 1999. doi: 10.1038/44565.

[4] S. Moussaoui  D. Brie  A. Mohammad-Djafari  and C. Carteret. Separation of non-negative
mixture of non-negative sources using a Bayesian approach and MCMC sampling. Signal Pro-
cessing  IEEE Transactions on  54(11):4133–4145  Nov 2006. doi: 10.1109/TSP.2006.880310.

[5] R. M. Neal. Slice sampling. Annals of Statistics  31(3):705–767  2003.
[6] G. Rodriguez-Yam  R. Davis  and L. Scharf. Efﬁcient gibbs sampling of truncated multivari-
ate normal with application to constrained linear regression. Technical report  Colorado State
University  Fort Collins  2004.

[7] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Neural Information Pro-

cessing Systems  Advances in (NIPS)  pages 1257–1264  2008.

[8] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain
Monte Carlo. In Machine Learning  International Conference on (ICML)  pages 880–887  2008.
[9] M. N. Schmidt  O. Winther  and L. K. Hansen. Bayesian non-negative matrix factorization. In
Independent Component Analysis and Signal Separation  International Conference on  volume
5441 of Lecture Notes in Computer Science (LNCS)  pages 540–547. Springer  2009. doi: 10.
1007/978-3-642-00599-2 68.

9

,Alexey Dosovitskiy
Thomas Brox