2018,Adversarial Examples that Fool both Computer Vision and Time-Limited Humans,Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However  it is still an open question whether humans are prone to similar mistakes. Here  we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture  and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.,Adversarial Examples that Fool both Computer

Vision and Time-Limited Humans

Gamaleldin F. Elsayed⇤

Google Brain

gamaleldin.elsayed@gmail.com

Shreya Shankar
Stanford University

Brian Cheung
UC Berkeley

Nicolas Papernot

Pennsylvania State University

Alexey Kurakin

Google Brain

Ian Goodfellow
Google Brain

Jascha Sohl-Dickstein

Google Brain

jaschasd@google.com

Abstract

Machine learning models are vulnerable to adversarial examples: small changes
to images can cause computer vision models to make mistakes such as identifying
a school bus as an ostrich. However  it is still an open question whether humans
are prone to similar mistakes. Here  we address this question by leveraging recent
techniques that transfer adversarial examples from computer vision models with
known parameters and architecture to other models with unknown parameters and
architecture  and by matching the initial processing of the human visual system.
We ﬁnd that adversarial examples that strongly transfer across computer vision
models inﬂuence the classiﬁcations made by time-limited human observers.

1

Introduction

Machine learning models are easily fooled by adversarial examples: inputs optimized by an adversary
to produce an incorrect model classiﬁcation [39  3]. In computer vision  an adversarial example is
usually an image formed by making small perturbations to an example image. Many algorithms for
constructing adversarial examples [39  13  33  24  27] rely on access to both the architecture and the
parameters of the model to perform gradient-based optimization on the input. Without similar access
to the brain  these methods do not seem applicable to constructing adversarial examples for humans.
One interesting phenomenon is that adversarial examples often transfer from one model to another 
making it possible to attack models that an attacker has no access to [39  26]. This naturally raises
the question of whether humans are susceptible to these adversarial examples. Clearly  humans
are prone to many cognitive biases and optical illusions [17]  but these generally do not resemble
small perturbations of natural images  nor are they currently generated by optimization of a ML loss
function. Thus the current understanding is that this class of transferable adversarial examples has no
effect on human visual perception  yet no thorough empirical investigation has yet been performed.
A rigorous investigation of the above question creates an opportunity both for machine learning to
gain knowledge from neuroscience  and for neuroscience to gain knowledge from machine learning.
Neuroscience has often provided existence proofs for machine learning—before we had working
object recognition algorithms  we hypothesized it should be possible to build them because the human

⇤Work done as a member of the Google AI Residency program (g.co/airesidency).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

brain can recognize objects. See Hassabis et al. [15] for a review of the inﬂuence of neuroscience
on artiﬁcial intelligence. If we knew conclusively that the human brain could resist a certain class
of adversarial examples  this would provide an existence proof for a similar mechanism in machine
learning security. If we knew conclusively that the brain can be fooled by adversarial examples 
then machine learning security research should perhaps shift its focus from designing models that
are robust to adversarial examples [39  13  32  42  40  27  19  5] to designing systems that are
secure despite including non-robust machine learning components. Likewise  if adversarial examples
developed for computer vision affect the brain  this phenomenon discovered in the context of machine
learning could lead to a better understanding of brain function.
In this work  we construct adversarial examples that transfer from computer vision models to the
human visual system. In order to successfully construct these examples and observe their effect  we
leverage three key ideas from machine learning  neuroscience  and psychophysics. First  we use
the recent black box adversarial example construction techniques that create adversarial examples
for a target model without access to the model’s architecture or parameters. Second  we adapt
machine learning models to mimic the initial visual processing of humans  making it more likely
that adversarial examples will transfer from the model to a human observer. Third  we evaluate
classiﬁcation decisions of human observers in a time-limited setting  so that even subtle effects on
human perception are detectable. By making image presentation sufﬁciently brief  humans are unable
to achieve perfect accuracy even on clean images  and small changes in performance lead to more
measurable changes in accuracy. Additionally  a brief image presentation limits the time in which
the brain can utilize recurrent and top-down processing pathways [34]  and is believed to make the
processing in the brain more closely resemble that in a feedforward artiﬁcial neural network.
We ﬁnd that adversarial examples that transfer across computer vision models do successfully
inﬂuence the perception of human observers  thus uncovering a new class of illusions that are shared
between computer vision models and the human brain.

2 Background and Related Work

2.1 Adversarial Examples

Goodfellow et al. [12] deﬁne adversarial examples as “inputs to machine learning models that an
attacker has intentionally designed to cause the model to make a mistake.” In the context of visual
object recognition  adversarial examples are images usually formed by applying a small perturbation
to a naturally occurring image in a way that breaks the predictions made by a machine learning
classiﬁer. See Figure Supp.1a for a canonical example where adding a small perturbation to an
image of a panda causes it to be misclassiﬁed as a gibbon. This perturbation is small enough to be
imperceptible (i.e.  it cannot be saved in a standard png ﬁle that uses 8 bits because the perturbation
is smaller than 1/255 of the pixel dynamic range). This perturbation relies on carefully chosen
structure based on the parameters of the neural network—but when magniﬁed to be perceptible 
human observers cannot recognize any meaningful structure. Note that adversarial examples also
exist in other domains like malware detection [14]  but we focus here on image classiﬁcation tasks.
Two aspects of the deﬁnition of adversarial examples are particularly important for this work:

1. Adversarial examples are designed to cause a mistake. They are not (as is commonly
misunderstood) deﬁned to differ from human judgment. If adversarial examples were
deﬁned by deviation from human output  it would by deﬁnition be impossible to make
adversarial examples for humans. On some tasks  like predicting whether input numbers
are prime  there is a clear objectively correct answer  and we would like the model to get
the correct answer  not the answer provided by humans (time-limited humans are probably
not very good at guessing whether numbers are prime). It is challenging to deﬁne what
constitutes a mistake for visual object recognition. After adding a perturbation to an image it
likely no longer corresponds to a photograph of a non-contrived physical scene. Furthermore 
it is philosophically difﬁcult to deﬁne the real object class for an image that is not a picture
of a real object. In this work  we assume that an adversarial image is misclassiﬁed if the
output label differs from the human-provided label of the clean image that was used as the
starting point for the adversarial image. We make small adversarial perturbations and we
assume that these small perturbations are insufﬁcient to change the true class.

2

2. Adversarial examples are not (as is commonly misunderstood) deﬁned to be imperceptible.
If this were the case  it would be impossible by deﬁnition to make adversarial examples for
humans  because changing the human’s classiﬁcation would constitute a change in what the
human perceives (e.g.  see Figure Supp.1b c).

2.1.1 Clues that Transfer to Humans is Possible
Some observations give clues that transfer to humans may be possible. Adversarial examples are
known to transfer across machine learning models  which suggest that these adversarial perturbations
may carry information about target adversarial classes. Adversarial examples that fool one model
often fool another model with a different architecture [39]  another model that was trained on a
different training set [39]  or even trained with a different algorithm [30] (e.g.  adversarial examples
designed to fool a convolution neural network may also fool a decision tree). The transfer effect
makes it possible to perform black box attacks  where adversarial examples fool models that an
attacker does not have access to [39  31]. Kurakin et al. [24] found that adversarial examples transfer
from the digital to the physical world  despite many transformations such as lighting and camera
effects that modify their appearance when they are photographed in the physical world. Liu et al. [26]
showed that the transferability of an adversarial example can be greatly improved by optimizing it to
fool many machine learning models rather than one model: an adversarial example that fools ﬁve
models used in the optimization process is more likely to fool an arbitrary sixth model.
Moreover  recent studies on stronger adversarial examples that transfer across multiple settings
have sometimes produced adversarial examples that appear more meaningful to human observers.
For instance  a cat adversarially perturbed to resemble a computer [2] while transfering across
geometric transformations develops features that appear computer-like (Figure Supp.1b)  and the
‘adversarial toaster’ from Brown et al. [4] possesses features that seem toaster-like (Figure Supp.1c).
This development of human-meaningful features is consistent with the adversarial example carrying
true feature information and thus coming closer to fooling humans  if we acounted for the notable
differences between humans visual processing and computer vision models (see section 2.2.2)

2.2 Biological and Artiﬁcial Vision
2.2.1 Similarities
Recent research has found similarities in representation and behavior between deep convolutional
neural networks (CNNs) and the primate visual system [6]. This further motivates the possibility that
adversarial examples may transfer from computer vision models to humans. Activity in deeper CNN
layers has been observed to be predictive of activity recorded in the visual pathway of primates [6  43].
Reisenhuber and Poggio [36] developed a model of object recognition in cortex that closely resembles
many aspects of modern CNNs. Kummerer et al. [21  22] showed that CNNs are predictive of human
gaze ﬁxation. Style transfer [10] demonstrated that intermediate layers of a CNN capture notions of
artistic style which are meaningful to humans. Freeman et al. [9] used representations in a CNN-like
model to develop psychophysical metamers  which are indistinguishable to humans when viewed
brieﬂy and with carefully controlled ﬁxation. Psychophysics experiments have compared the pattern
of errors made by humans  to that made by neural network classiﬁers [11  35].

2.2.2 Notable Differences
Differences between machine and human vision occur early in the visual system. Images are typically
presented to CNNs as a static rectangular pixel grid with constant spatial resolution. The primate eye
on the other hand has an eccentricity dependent spatial resolution. Resolution is high in the fovea  or
central ⇠ 5 of the visual ﬁeld  but falls off linearly with increasing eccentricity [41]. A perturbation
which requires high acuity in the periphery of an image  as might occur as part of an adversarial
example  would be undetectable by the eye  and thus would have no impact on human perception.
Further differences include the sensitivity of the eye to temporal as well as spatial features  as well
as non-uniform color sensitivity [25]. Modeling the early visual system continues to be an area of
active study [29  28]. As we describe in section 3.1.2  we mitigate some of these differences by using
a biologically-inspired image input layer.
Beyond early visual processing  there are more major computational differences between CNNs
and the human brain. All the CNNs we consider are fully feedforward architectures  while the

3

visual cortex has many times more feedback than feedforward connections  as well as extensive
recurrent dynamics [29]. Possibly due to these differences in architecture  humans have been found
experimentally to make classiﬁcation mistakes that are qualitatively different than those made by
deep networks [8]. Additionally  the brain does not treat a scene as a single static image  but actively
explores it with saccades [18]. As is common in psychophysics experiments [20]  we mitigate these
differences in processing by limiting both the way in which the image is presented  and the time
which the subject has to process it  as described in section 3.2.

3 Methods

Section 3.1 details our machine learning vision pipeline. Section 3.2 describes our psychophysics
experiment to evaluate the impact of adversarial images on human subjects.

3.1 The Machine Learning Vision Pipeline

3.1.1 Dataset

In our experiment  we used images from ImageNet [7]. ImageNet contains 1 000 highly speciﬁc
classes that typical people may not be able to identify  such as “Chesapeake Bay retriever”. Thus  we
combined some of these ﬁne classes to form six coarse classes we were conﬁdent would be familiar
to our experiment subjects ({dog  cat  broccoli  cabbage  spider  snake}). We then grouped these six
classes into the following groups: (i) Pets group (dog and cat images); (ii) Hazard group (spider and
snake images); (iii) Vegetables group (broccoli and cabbage images).

3.1.2 Ensemble of Models

We constructed an ensemble of ten CNN models trained on ImageNet. Each model is an instance of
one of these architectures: Inception V3  Inception V4  Inception ResNet V2  ResNet V2 50  ResNet
V2 101  and ResNet V2 152 [38  37  16]. To better match the initial processing of human visual
system  we prepend each model with a retinal layer  which pre-processes the input to incorporate
some of the transformations performed by the human eye. In that layer  we perform an eccentricity
dependent blurring of the image to approximate the input which is received by the visual cortex
of human subjects through their retinal lattice. The details of this retinal layer are described in
Appendix B. We use eccentricity-dependent spatial resolution measurements (based on the macaque
visual system) [41]  along with the known geometry of the viewer and the screen  to determine
the degree of spatial blurring at each image location. This limits the CNN to information which is
also available to the human visual system. The layer is fully differentiable  allowing gradients to
backpropagate through the network when running adversarial attacks. Further details of the models
and their classiﬁcation performance are provided in Appendix E.

3.1.3 Generating Adversarial Images

For a given image group  we wish to generate targeted adversarial examples that strongly transfer
across models. This means that for a class pair (A  B) (e.g.  A: cats and B: dogs)  we generate
adversarial perturbations such that models will classify perturbed images from A as B; similarly 
we perturbed images from B to be classiﬁed as A. A different perturbation is constructed for each
image; however  the `1 norm of all perturbations are constrained to be equal to a ﬁxed ✏.
Formally: given a classiﬁer  which assigns probability P (y | X) to each coarse class y given an input
image X  a speciﬁed target class ytarget and a perturbation magnitude ✏  we want to ﬁnd the image
Xadv that minimizes  log(P (ytarget | Xadv)) with the constraint that ||Xadv  X||1 = ✏. See
Appendix C for details on computing the coarse class probabilities P (y | X). With the classiﬁer’s
parameters  we can perform iterated gradient descent on X in order to generate our Xadv (see
Appendix D). This iterative approach is commonly employed to generate adversarial images [24].

4

class 1 (e.g. cat)class 2 (e.g. dog)light sensorresponsetime box(c)(d) 200 ms 500-1000 ms {71  63} ms {2500  2200} mstimestimulusmaskingrepeatimageadv (to dog)adv (to cat)imageadvflip(a)(b)high-refresh monitorFigure1:Experimentsetupandtask.(a)examplesimagesfromtheconditions(image adv andflip).Top:advtargetingbroccoliclass.bottom:advtargetingcatclass.SeedeﬁnitionofconditionsatSection3.2.2.(b)exampleimagesfromthefalseexperimentcondition.(c)Experimentsetupandrecordingapparatus.(d)Taskstructureandtimings.Thesubjectisaskedtorepeatedlyidentifywhichoftwoclasses(e.g.dogvs.cat)abrieﬂypresentedimagebelongsto.Theimageiseitheradversarial orbelongstooneofseveralcontrolconditions.SeeSection3.2fordetails.3.2HumanPsychophysicsExperiment38subjectswithnormalorcorrectedvisionparticipatedintheexperiment.Subjectsgaveinformedconsenttoparticipate andwereawardedareasonablecompensationfortheirtimeandeffort2.3.2.1ExperimentalSetupSubjectssatonaﬁxedchair61cmawayfromahighrefresh-ratecomputerscreen(ViewSonicXG2530)inaroomwithdimmedlight(Figure1c).Subjectswereaskedtoclassifyimagesthatappearedonthescreenintooneoftwoclasses(twoalternativeforcedchoice)bypressingbuttonsonaresponsetimebox(LOBESv5/6:USTC)usingtwoﬁngersontheirrighthand.Theassignmentofclassestobuttonswasrandomizedforeachexperimentsession andlabelswereplacednexttothebuttonstopreventconfusion.Eachtrialstartedwithaﬁxationcrossdisplayedinthemiddleofthescreenfor5001000ms.Subjectswereinstructedtodirecttheirgazetothecenterofthiscross(Figure1d).Aftertheﬁxationperiod animageofsize15.24cm⇥15.24cm(14.2visualangle)waspresentedbrieﬂyatthecenterofthescreenforaperiodof63ms(71msforsomesessions).Theimagewasfollowedbyasequenceoftenhighcontrastbinaryrandommasks eachdisplayedfor20ms(seeexampleinFigure1d).Subjectswereaskedtoclassifytheobjectintheimage(e.g. catvs.dog)bypressingoneoftwobuttonsstartingattheimagepresentationtimeandlastinguntil2200ms(or2500msforsomesessions)afterthemaskwasturnedoff.Thewaitingperiodtostartthenexttrialwasofthesamedurationwhethersubjectsrespondedquicklyorslowly.Realizedexposuredurationswere±4msfromthetimesreportedabove asmeasuredbyaphotodiodeandoscilloscopeinaseparatetestexperiment.Eachsubject’sresponsetimewasrecordedbytheresponsetimeboxrelativetotheimagepresentationtime(monitoredbyaphotodiode).Inthecasewhereasubjectpressedmorethanonebuttoninatrial onlytheclasscorrespondingtotheirﬁrstchoicewasconsidered.Eachsubjectcompletedbetween140and950trials.Further subjectsperformedoneormoredemotrialsatthestartofthesession togainfamiliaritywiththetask.Duringthedemotrialsonly imagepresentationtimewaslong andsubjectsweregivenfeedbackonthecorrectnessoftheirchoice.3.2.2ExperimentConditionsEachexperimentalsessionincludedonlyoneoftheimagegroups(Pets VegetablesorHazard).Foreachgroup imageswerepresentedinoneoffourconditionsasfollows:2ThestudywasgrantedanInstitutionalReviewBoard(IRB)exemptionbyanexternal independent ethicsboard(QuorumreviewID33016).5clipping when adversarial perturbations are added; see Figure 1a left) ).

• image: images from the ImageNet training set (rescaled to the [40  255 40] range to avoid
• adv: we added adversarial perturbation adv to image  crafted to cause machine learning
models to misclassify adv as the opposite class in the group (e.g.  if image was originally
a cat  we perturbed the image to be classiﬁed as a dog). We used a perturbation size large
enough to be noticeable by humans on the computer screen but small with respect to the
image intensity scale (✏ = 32; see Figure 1a middle). In other words  we chose ✏ to be large
(to improve the chances of adversarial examples transfer to time-limited human) but kept it
small enough that the perturbations are class-preserving (as judged by a no-limit human).
• flip: similar to adv  but the adversarial perturbation (adv) is ﬂipped vertically before being
added to image. This is a control condition  chosen to have nearly identical perturbation
statistics to the adv condition (see Figure 1a right).

• false: in this condition  subjects are forced to make a mistake. To show that adversarial
perturbations actually control the chosen class  we include this condition where neither of
the two options available to the subject is correct  so their accuracy is always zero. We test
whether adversarial perturbations can inﬂuence which of the two wrong choices they make.
We show a random image from an ImageNet class other than the two classes in the group 
and adversarially perturb it toward one of the two classes in the group. The subject must
then choose from these two classes. For example  we might show an airplane adversarially
perturbed toward the dog class  while a subject is in a session classifying images as cats or
dogs. We used a slightly larger perturbation in this condition (✏ = 40; see Figure 1b).

The conditions (image  adv  flip) are ensured to have balanced number of trials within a session
by either uniformly sampling the condition type in some of the sessions or randomly shufﬂing a
sequence with identical trial counts for each condition in other sessions. The number of trials for
each class in the group was also constrained to be equal. Similarly for the false condition the
number of trials adversarially perturbed towards class 1 and class 2 were balanced for each session.
To reduce subjects using strategies based on overall color or brightness distinctions between classes 
we pre-ﬁltered the dataset to remove images that showed an obvious effect of this nature. Notably  in
the pets group we excluded images that included large green lawns or ﬁelds  since in almost all cases
these were photographs of dogs. See Appendix F for images used in the experiment for each coarse
class. For example images for each condition  see Figures Supp.5 through Supp.8.

4 Results

4.1 Adversarial Examples Transfer to Computer Vision Models
We ﬁrst assess the transfer of our constructed images to two test models that were not included in
the ensemble used to generate adversarial examples. These test models are an adversarially trained
Inception V3 model [23] and a ResNet V2 50 model. Both models perform well (> 75% accuracy)
on clean images. Attacks in the adv and false conditions succeeded against the test models between
57% and 89% of the time  depending on image class and experimental condition. The flip condition
changed the test model predictions on fewer than 1.5% of images in all conditions  validating its use
as a control. See Tables Supp.3 - Supp.6 for accuracy and attack success measurements on both train
and test models for all experimental conditions.

4.2 Adversarial Examples Transfer to Humans
We now show that adversarial examples transfer to time-limited humans. One could imagine that
adversarial examples merely degrade image quality or discard information  thus increasing error rate.
To rule out this possibility  we begin by showing that for a ﬁxed error rate (in a setting where the
human is forced to make a mistake)  adversarial perturbations inﬂuence the human choice among two
incorrect classes. Then  we demonstrate that adversarial examples increase the error rate.

4.2.1 Inﬂuencing the Choice between two Incorrect Classes
As described in Section 3.2.2  we used the false condition to test whether adversarial perturbations
can inﬂuence which of two incorrect classes a subject chooses (see example images in Figure Supp.5).

6

(a)

s
s
a
l
c
 
t
e
g
r
a
t
 
.
b
o
r
p

(b)

y
c
a
r
u
c
c
a

hazard
vegetables
pets

(c)

image

25% snake (4)

n=13

y
c
a
r
u
c
c
a

adv

67% snake (6)

n=6

brief

long

image
adv
pets

flip adv

flip adv

image
hazard

image
flip
vegetables

Figure 2: Adversarial images transfer to humans. (a) By adding adversarial perturbations to an
image  we are able to bias which of two incorrect choices subjects make. Plot shows probability of
choosing the adversarially targeted class when the true image class is not one of the choices that
subjects can report (false condition)  estimated by averaging the responses of all subjects (two-tailed
t-test relative to chance level 0.5). (b) Adversarial images cause more mistakes than either clean
images or images with the adversarial perturbation ﬂipped vertically before being applied. Plot shows
probability of choosing the true image class  when this class is one of the choices that subjects can
report  averaged across all subjects. Accuracy is signiﬁcantly less than 1 even for clean images due to
the brief image presentation time. (error bars ± SE; *: p < 0.05; **: p < 0.01; ***: p < 0.001) (c)
A spider image that time-limited humans frequently perceived as a snake (top parentheses: number
of subjects tested on this image). right: accuracy on this adversarial image when presented brieﬂy
compared to when presented for long time (long presentation is based on a post-experiment survey of
13 participants).

We measured our effectiveness at changing the perception of subjects using the rate at which subjects
reported the adversarially targeted class. If the adversarial perturbation were completely ineffective
we would expect the choice of targeted class to be uncorrelated with the subject’s reported class.
The average rate at which the subject chooses the target class metric would be 0.5 as each false
image is perturbed to class 1 or class 2 in the group with equal probability. Figure 2a shows the
probability of choosing the target class averaged across all subjects for all three experiment groups.
In all cases  the probability was signiﬁcantly above the chance level of 0.5. This demonstrates that
the adversarial perturbations generated using CNNs biased human perception towards the targeted
class. This effect was stronger for the the hazard  then pets  then vegetables group. This difference in
probability among the class groups was signiﬁcant (p < 0.05; Pearson Chi-2 GLM test).
We also observed a signiﬁcant difference in the mean response time between the class groups
(p < 0.001; one-way ANOVA test; see Figure Supp.2a). Interestingly  the response time pattern
across image groups (Figure Supp.2a)) was inversely correlated to the perceptual bias pattern (Figure
2a)) (Pearson correlation = 1  p < 0.01; two-tailed Pearson correlation test). In other words 
subjects made quicker decisions for the hazard group  then pets group  and then vegetables group. This
is consistent with subjects being more conﬁdent in their decision when the adversarial perturbation
was more successful in biasing subjects perception. This inverse correlation between attack success
and response time was observed within group  as well as between groups (Figure Supp.3).

4.2.2 Adversarial Examples Increase Human Error Rate

We demonstrated that we are able to bias human perception to a target class when the true class of the
image is not one of the options that subjects can choose. Now we show that adversarial perturbations
can be used to cause the subject to choose an incorrect class even though the correct class is an
available response. As described in Section 3.2.2  we presented image  flip  and adv.
Most subjects had lower accuracy in adv than image (Table Supp.1). This is also reﬂected on the
average accuracy across all subjects signiﬁcantly lower for the adv than image (Figure 2b).

7

texture modificationimageadvdark parts modificationimageadvedge enhancementimageadvedge destructionimageadvFigure3:Examplesofthetypesofmanipulationsperformedbytheadversarialattack.SeeFiguresSupp.6throughSupp.8foradditionalexamplesofadversarialimages.AlsoseeFigureSupp.5foradversarialexamplesfromthefalsecondition.Theaboveresultinisolationmightbesimplyexplainedbythefactthatimagesfromtheadvconditionincludeperturbationswhereasimagesfromtheimageconditionareunaltered.WhilethisissueislargelyaddressedbythefalseexperimentresultsinSection4.2.1 toprovideafurthercontrolwealsoevaluatedaccuracyonflipimages.Thiscontrolcaseusesperturbationswithidenticalstatisticstoadvuptoaﬂipoftheverticalaxis.However thiscontrolbreaksthepixel-to-pixelcorrespondencebetweentheadversarialperturbationandtheimage.Themajorityofsubjectshadloweraccuracyintheadvconditionthanintheflipcondition(TableSupp.1).Whenaveragingacrossalltrials thiseffectwasverysigniﬁcantforthepetsandvegetablesgroup(p<0.001) andlesssigniﬁcantforthehazardgroup(p=0.05)(Figure2b).Theseresultssuggestthatthedirectionoftheadversarialimageperturbation incombinationwithaspeciﬁcimage isperceptuallyrelevanttofeaturesthatthehumanvisualsystemusestoclassifyobjects.TheseﬁndingsthusgiveevidencethatstrongblackboxadversarialattackscantransferfromCNNstohumans andshowremarkablesimilaritiesbetweenfailurecasesofCNNsandhumanvision.Inallcases theaverageresponsetimewaslongerfortheadvconditionrelativetotheotherconditions(FigureSupp.2b) thoughthisresultwasonlystatisticallysigniﬁcantfortwocomparisons.Ifthistrendremainspredictive itwouldseemtocontradictthecasewhenwepresentedfalseimages(FigureSupp.2a).Oneinterpretationisthatinthefalsecase thetransferofadversarialfeaturestohumanswasaccompaniedbymoreconﬁdence whereasherethetransferwasaccompaniedbylessconﬁdence possiblyduetocompetingadversarialandtrueclassfeaturesintheadvcondition.5DiscussionOurresultsinviteseveralquestionsthatwediscussbrieﬂy.5.1Haveweactuallyfooledhumanobserversordidwechangethetrueclass?Onemightnaturallywonderwhetherwehavefooledthehumanobserverorwhetherwehavereplacedtheinputimagewithanimagethatactuallybelongstoadifferentclass.Inourwork theperturbationswemadeweresmallenoughthattheygenerallydonotchangetheoutputclassforahumanwhohasnotimelimit(thereadermayverifythisbyobservingFigures1a b 2c andSupp.5throughSupp.8).Wecanthusbeconﬁdentthatwedidnotchangethetrueclassoftheimage andthatwereallydidfoolthetime-limitedhuman.Futureworkaimedatfoolinghumanswithnotime-limitwillneedtotacklethedifﬁcultproblemofobtainingabettergroundtruthsignalthanvisuallabelingbyhumans.5.2Howdotheadversarialexampleswork?Wedidnotdesigncontrolledexperimentstoprovethattheadversarialexamplesworkinanyspeciﬁcway butweinformallyobservedafewapparentpatternsillustratedinFigure3:disruptingobjectedges especiallybymid-frequencymodulationsperpendiculartotheedge;enhancingedgesbothbyincreasingcontrastandcreatingtextureboundaries;modifyingtexture;andtakingadvantageofdarkregionsintheimage wheretheperceptualmagnitudeofsmall✏perturbationscanbelarger.85.3 What are the implications for machine learning security and society?
The fact that our transfer-based adversarial examples fool time-limited humans but not no-limit
humans suggests that the lateral and top-down connections used by the no-limit human are relevant
to human robustness to adversarial examples. This suggests that machine learning security research
should explore the signiﬁcance of these top-down or lateral connections further. One possible explana-
tion for our observation is that no-limit humans are fundamentally more robust to adversarial example
and achieve this robustness via top-down or lateral connections. If this is the case  it could point the
way to the development of more robust machine learning models. Another possible explanation is that
no-limit humans remain highly vulnerable to adversarial examples but adversarial examples do not
transfer from feed-forward networks to no-limit humans because of these architectural differences.
Our results suggest that there is a risk that imagery could be manipulated to cause human observers
to have unusual reactions; for example  perhaps a photo of a politician could be manipulated in a way
that causes it to be perceived as unusually untrustworthy or unusually trustworthy in order to affect
the outcome of an election.

5.4 Future Directions
In this study  we designed a procedure that according to our hypothesis would transfer adversarial
examples to humans. An interesting set of questions relates to how sensitive that transfer is to
different elements of our experimental design. For example: How does transfer depend on ✏? Was
model ensembling crucial to transfer? Can the retinal preprocessing layer be removed? We suspect
that retinal preprocessing and ensembling are both important for transfer to humans  but that ✏ could
be made smaller. See Figure Supp.4 for a preliminary exploration of these questions.

6 Conclusion

Susceptibility to adversarial examples has been widely assumed – in the absence of experimental
evidence – to be a property of machine learning classiﬁers  but not of human judgement. In this
work  we correct this assumption by showing that adversarial examples based on perceptible but class-
preserving perturbations that fool multiple machine learning models also fool time-limited humans.
Our ﬁndings demonstrate striking similarities between the decision boundaries of convolutional
neural networks and the human visual system. We expect this observation to lead to advances in both
neuroscience and machine learning research.

Acknowledgements

We are grateful to Ari Morcos  Bruno Olshausen  David Sussillo  Hanlin Tang  John Cunningham 
Santani Teng  and Daniel Yamins for useful discussions. We also thank Dan Abolaﬁa Simon
Kornblith  Katherine Lee  Kathryn Rough  Niru Maheswaranathan  Catherine Olsson  David Sussillo 
and Santani Teng  for helpful feedback on the manuscript. We thank Google Brain residents for
useful feedback on the work. We also thank Deanna Chen  Leslie Philips  Sally Jesmonth  Phing
Turner  Melissa Strader  Lily Peng  and Ricardo Prada for assistance with IRB and experiment setup.

References
[1] Anish Athalye. Robust adversarial examples  2017.
[2] Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint

arXiv:1707.07397  2017.

[3] Battista Biggio  Igino Corona  Davide Maiorca  Blaine Nelson  Nedim Srndic  Pavel Laskov 
Giorgio Giacinto  and Fabio Roli. Evasion attacks against machine learning at test time. In
Machine Learning and Knowledge Discovery in Databases - European Conference  ECML
PKDD 2013  Prague  Czech Republic  September 23-27  2013  Proceedings  Part III  pages
387–402  2013.

[4] Tom B Brown  Dandelion Mané  Aurko Roy  Martín Abadi  and Justin Gilmer. Adversarial

patch. arXiv preprint arXiv:1712.09665  2017.

9

[5] Jacob Buckman  Aurko Roy  Colin Raffel  and Ian Goodfellow. Thermometer encoding: One
hot way to resist adversarial examples. International Conference on Learning Representations 
2018. accepted as poster.

[6] Charles F Cadieu  Ha Hong  Daniel LK Yamins  Nicolas Pinto  Diego Ardila  Ethan A Solomon 
Najib J Majaj  and James J DiCarlo. Deep neural networks rival the representation of primate it
cortex for core visual object recognition. PLoS computational biology  10(12):e1003963  2014.
[7] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition  2009. CVPR 2009.
IEEE Conference on  pages 248–255. IEEE  2009.

[8] Miguel P Eckstein  Kathryn Koehler  Lauren E Welbourne  and Emre Akbas. Humans  but not
deep neural networks  often miss giant targets in scenes. Current Biology  27(18):2827–2832 
2017.

[9] Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature neuroscience 

14(9):1195  2011.

[10] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. A neural algorithm of artistic style.

arXiv preprint arXiv:1508.06576  2015.

[11] Robert Geirhos  David HJ Janssen  Heiko H Schütt  Jonas Rauber  Matthias Bethge  and Felix A
Wichmann. Comparing deep neural networks against humans: object recognition when the
signal gets weaker. arXiv preprint arXiv:1706.06969  2017.

[12] Ian Goodfellow  Nicolas Papernot  Sandy Huang  Yan Duan  Pieter Abbeel  and Jack Clark.

Attacking machine learning with adversarial examples  2017.

[13] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572  2014.

[14] Kathrin Grosse  Nicolas Papernot  Praveen Manoharan  Michael Backes  and Patrick D. Mc-

Daniel. Adversarial examples for malware detection. In ESORICS 2017  pages 62–79  2017.

[15] Demis Hassabis  Dharshan Kumaran  Christopher Summerﬁeld  and Matthew Botvinick.

Neuroscience-inspired artiﬁcial intelligence. Neuron  95(2):245–258  2017.

[16] K. He  X. Zhang  S. Ren  and J. Sun. Identity Mappings in Deep Residual Networks. ArXiv

e-prints  March 2016.

[17] James M Hillis  Marc O Ernst  Martin S Banks  and Michael S Landy. Combining sensory
information: mandatory fusion within  but not between  senses. Science  298(5598):1627–1630 
2002.

[18] Michael Ibbotson and Bart Krekelberg. Visual perception and saccadic eye movements. Current

opinion in neurobiology  21(4):553–558  2011.

[19] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex

outer adversarial polytope. arXiv preprint arXiv:1711.00851  2017.

[20] GYULA KovAcs  Ruﬁn Vogels  and Guy A Orban. Cortical correlate of pattern backward

masking. Proceedings of the National Academy of Sciences  92(12):5587–5591  1995.

[21] Matthias Kümmerer  Lucas Theis  and Matthias Bethge. Deep gaze i: Boosting saliency

prediction with feature maps trained on imagenet. arXiv preprint arXiv:1411.1045  2014.

[22] Matthias Kümmerer  Tom Wallis  and Matthias Bethge. Deepgaze ii: Predicting ﬁxations from

deep features over time and tasks. Journal of Vision  17(10):1147–1147  2017.

[23] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial Machine Learning at Scale. ArXiv

e-prints  November 2016.

[24] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial examples in the physical

world. In ICLR’2017 Workshop  2016.

10

[25] Michael F Land and Dan-Eric Nilsson. Animal eyes. Oxford University Press  2012.
[26] Yanpei Liu  Xinyun Chen  Chang Liu  and Dawn Song. Delving into transferable adversarial

examples and black-box attacks. arXiv preprint arXiv:1611.02770  2016.

[27] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 
2017.

[28] Lane McIntosh  Niru Maheswaranathan  Aran Nayebi  Surya Ganguli  and Stephen Baccus.
Deep learning models of the retinal response to natural scenes. In Advances in neural information
processing systems  pages 1369–1377  2016.

[29] Bruno A Olshausen. 20 years of learning about vision: Questions answered  questions unan-
swered  and questions not yet asked. In 20 Years of Computational Neuroscience  pages 243–270.
Springer  2013.

[30] Nicolas Papernot  Patrick McDaniel  and Ian Goodfellow. Transferability in machine learn-
arXiv preprint

from phenomena to black-box attacks using adversarial samples.

ing:
arXiv:1605.07277  2016.

[31] Nicolas Papernot  Patrick McDaniel  Ian Goodfellow  Somesh Jha  Z Berkay Celik  and Anan-
thram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017
ACM on Asia Conference on Computer and Communications Security  pages 506–519. ACM 
2017.

[32] Nicolas Papernot  Patrick McDaniel  Xi Wu  Somesh Jha  and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. In Security and Privacy
(SP)  2016 IEEE Symposium on  pages 582–597. IEEE  2016.

[33] Nicolas Papernot  Patrick D. McDaniel  Somesh Jha  Matt Fredrikson  Z. Berkay Celik 
and Ananthram Swami. The limitations of deep learning in adversarial settings. CoRR 
abs/1511.07528  2015.

[34] Mary C Potter  Brad Wyble  Carl Erick Hagmann  and Emily S McCourt. Detecting meaning in

rsvp at 13 ms per picture. Attention  Perception  & Psychophysics  76(2):270–279  2014.

[35] Rishi Rajalingham  Elias B. Issa  Pouya Bashivan  Kohitij Kar  Kailyn Schmidt  and James J
DiCarlo. Large-scale  high-resolution comparison of the core visual object recognition behavior
of humans  monkeys  and state-of-the-art deep artiﬁcial neural networks. bioRxiv  2018.

[36] Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in

cortex. Nature neuroscience  2(11):1019  1999.

[37] C. Szegedy  S. Ioffe  V. Vanhoucke  and A. Alemi. Inception-v4  Inception-ResNet and the

Impact of Residual Connections on Learning. ArXiv e-prints  February 2016.

[38] C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. Rethinking the Inception

Architecture for Computer Vision. ArXiv e-prints  December 2015.

[39] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[40] F. Tramèr  A. Kurakin  N. Papernot  D. Boneh  and P. McDaniel. Ensemble Adversarial Training:

Attacks and Defenses. ArXiv e-prints  May 2017.

[41] D. C. Van Essen and C. H Anderson. Information processing strategies and pathways in the
primate visual system. In Zornetzer S. F.  Davis J. L.  Lau C.  and McKenna T.  editors  An
introduction to neural and electronic networks  page 45–76  San Diego  CA  1995. Academic
Press.

[42] Weilin Xu  David Evans  and Yanjun Qi. Feature squeezing: Detecting adversarial examples in

deep neural networks. arXiv preprint arXiv:1704.01155  2017.

[43] Daniel L. K. Yamins and James J. DiCarlo. Using goal-driven deep learning models to under-

stand sensory cortex. Nature Neuroscience  19:356–365  2016.

11

,Gamaleldin Elsayed
Shreya Shankar
Brian Cheung
Nicolas Papernot
Alexey Kurakin
Ian Goodfellow
Jascha Sohl-Dickstein