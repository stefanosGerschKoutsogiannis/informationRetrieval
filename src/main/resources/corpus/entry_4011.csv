2019,On two ways to use determinantal point processes for Monte Carlo integration,When approximating an integral by a weighted sum of function evaluations  determinantal point processes (DPPs) provide a way to enforce repulsion between the evaluation points.
This negative dependence is encoded by a kernel.
Fifteen years before the discovery of DPPs  Ermakov & Zolotukhin (EZ  1960) had the intuition of sampling a DPP and solving a linear system to compute an unbiased Monte Carlo estimator of the integral.
In the absence of DPP machinery to derive an efficient sampler and analyze their estimator  the idea of Monte Carlo integration with DPPs was stored in the cellar of numerical integration. 
Recently  Bardenet & Hardy (BH  2019) came up with a more natural estimator with a fast central limit theorem (CLT).
In this paper  we first take the EZ estimator out of the cellar  and analyze it using modern arguments.
Second  we provide an efficient implementation to sample exactly a particular multidimensional DPP called multivariate Jacobi ensemble.
The latter satisfies the assumptions of the aforementioned CLT. 
Third  our new implementation lets us investigate the behavior of the two unbiased Monte Carlo estimators in yet unexplored regimes.
We demonstrate experimentally good properties when the kernel is adapted to basis of functions in which the integrand is sparse or has fast-decaying coefficients.
If such a basis and the level of sparsity are known (e.g.  we integrate a linear combination of kernel eigenfunctions)  the EZ estimator can be the right choice  but otherwise it can display an erratic behavior.,On two ways to use determinantal point processes

for Monte Carlo integration

Guillaume Gautier†⇤
g.gautier@inria.fr
valkom@deepmind.com
†Univ. Lille  CNRS  Centrale Lille  UMR 9189 – CRIStAL  59651 Villeneuve d’Ascq  France

remi.bardenet@gmail.com

Michal Valko‡⇤†

Rémi Bardenet†

⇤Inria Lille-Nord Europe  40 avenue Halley 59650 Villeneuve d’Ascq  France

‡DeepMind Paris  14 Rue de Londres  75009 Paris  France

Abstract

When approximating an integral by a weighted sum of function evaluations  de-
terminantal point processes (DPPs) provide a way to enforce repulsion between
the evaluation points. This negative dependence is encoded by a kernel. Fifteen
years before the discovery of DPPs  Ermakov & Zolotukhin (EZ  1960) had the
intuition of sampling a DPP and solving a linear system to compute an unbiased
Monte Carlo estimator of the integral. In the absence of DPP machinery to derive
an efﬁcient sampler and analyze their estimator  the idea of Monte Carlo integration
with DPPs was stored in the cellar of numerical integration. Recently  Bardenet &
Hardy (BH  2019) came up with a more natural estimator with a fast central limit
theorem (CLT). In this paper  we ﬁrst take the EZ estimator out of the cellar  and an-
alyze it using modern arguments. Second  we provide an efﬁcient implementation1
to sample exactly a particular multidimensional DPP called multivariate Jacobi
ensemble. The latter satisﬁes the assumptions of the aforementioned CLT. Third 
our new implementation lets us investigate the behavior of the two unbiased Monte
Carlo estimators in yet unexplored regimes. We demonstrate experimentally good
properties when the kernel is adapted to basis of functions in which the integrand is
sparse or has fast-decaying coefﬁcients. If such a basis and the level of sparsity are
known (e.g.  we integrate a linear combination of kernel eigenfunctions)  the EZ
estimator can be the right choice  but otherwise it can display an erratic behavior.

1

Introduction

Numerical integration is a core task of many machine learning applications  including most Bayesian
methods (Robert  2007). Both deterministic (Davis & Rabinowitz  1984; Dick & Pillichshammer 
2010) and random (Robert & Casella  2004) algorithms have been proposed; see also (Evans &
Swartz  2000) for a survey. All methods require evaluating the integrand at carefully chosen points 
called quadrature nodes  and combining these evaluations to minimize the approximation error.
Recently  a stream of work has made use of prior knowledge on the smoothness of the integrand using
kernels. Oates et al. (2017) and Liu & Lee (2017) used kernel-based control variates  splitting the
computational budget into regressing the integrand and integrating the residual. Bach (2017) looked
for the best way to sample i.i.d. nodes and combine the resulting evaluations. Finally  Bayesian
quadrature (O’Hagan  1991; Huszár & Duvenaud  2012; Briol et al.  2015)  herding (Chen et al. 
2010; Bach et al.  2012)  or the biased importance sampling estimate of Delyon & Portier (2016) all
favor dissimilar nodes  where dissimilarity is measured by a kernel. Our work falls in this last cluster.
We build on the particular approach of Bardenet & Hardy (2019) for Monte Carlo integration based
on projection determinantal point processes (DPPs  Hough et al.  2006; Kulesza & Taskar  2012).
DPPs are a repulsive distribution over conﬁgurations of points  where repulsion is again parametrized
by a kernel. In a sense  DPPs are the kernel machines of point processes.
1 github.com/guilgautier/DPPy

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Fifteen years before Macchi (1975) even formalized DPPs  Ermakov & Zolotukhin (EZ  1960) had
the intuition to use a determinantal structure to sample quadrature nodes  followed by solving a
linear system to aggregate the evaluations of the integrand into an unbiased estimator. This linear
system yields a simple and interpretable characterization of the variance of their estimator. Ermakov
& Zolotukhin’s result did not diffuse much2 in the Monte Carlo community  partly because the
mathematical and computational machinery to analyze and implement it was not available. Seemingly
unaware of this previous work  Bardenet & Hardy (2019) came up with a more natural estimator of
the integral of interest  and they could build upon the thorough study of DPPs in random matrix theory
(Johansson  2006) to obtain a fast central limit theorem (CLT). Since then  DPPs with stationary
kernels have also been used by Mazoyer et al. (2019) for Monte Carlo integration. In any case 
these DPP-based Monte Carlo estimators crucially depend on efﬁcient sampling procedures for the
corresponding (potentially multidimensional) DPP.

Our contributions. First  we reveal the close link between DPPs and the approach of Ermakov &
Zolotukhin (1960). Second  we provide a simple proof of their result and survey the properties of the
EZ estimator with modern arguments. In particular  when the integrand is a linear combination of the
eigenfunctions of the kernel of the underlying DPP  the corresponding Fourier-like coefﬁcients can
be estimated with zero variance. In other words  one sample of the corresponding DPP yields perfect
interpolation of the underlying integrand  by solving a linear system. Third  we propose an efﬁcient
Python implementation for exact sampling of a particular DPP  called multivariate Jacobi ensemble.
The code1 is available in the DPPy toolbox of Gautier et al. (2019). This implementation allows to
numerically investigate the behavior of the two Monte Carlo estimators derived by Bardenet & Hardy
(2019) and Ermakov & Zolotukhin (1960)  in regimes yet unexplored for any of the two. Fourth 
important theoretical properties of both estimators  like the CLT of (Bardenet & Hardy  2019)  are
technically involved. A CLT for EZ promises to be even more difﬁcult to establish. The current
empirical investigation provides a motivation and guidelines for more theoretical work. Our point
is not to compare DPP-based Monte Carlo estimators to the wide choice of numerical integration
algorithms  but to get a ﬁne understanding of their properties so as to ﬁne-tune their design and guide
theoretical developments.

2 Quadrature  DPPs  and the multivariate Jacobi ensemble

In this section  we quickly survey classical quadrature rules. Then  we deﬁne DPPs and give the key
properties that make them useful for Monte Carlo integration. Finally  among so-called projection
DPPs  we introduce the multivariate Jacobi ensemble used by Bardenet & Hardy (2019) to generate
quadrature nodes  and on which we base our experimental work.

2.1 Standard quadrature
Following Briol et al. (2015  Section 2.1)  let µ(dx) = !(x) dx be a positive Borel measure on
X ⇢ Rd with ﬁnite mass and density ! w.r.t. the Lebesgue measure. This paper aims to compute
integrals of the formR f (x)µ(dx) for some test function f : X ! R. A quadrature rule approximates
such integrals as a weighted sum of evaluations of f at some nodes {x1  . . .   xN}⇢ X 

Z f (x)µ(dx) ⇡

NXn=1

!nf (xn) 

(1)

where the weights !n   !n(x1  . . .   xN ) do not need to be non-negative nor sum to one.
Among the many quadrature designs mentioned in the introduction (Evans & Swartz  2000  Section 5) 
we pay special attention to the textbook example of the (deterministic) Gauss-Jacobi rule. This scheme
applies to dimension d = 1  for X   [1  1] and !(x)   (1  x)a(1 + x)b with a  b > 1. In this
case  the nodes {x1  . . .   xN} are taken to be the zeros of pN  the orthonormal Jacobi polynomial
of degree N  and the weights !n   1/K(xn  xn) with K(x  x)  PN1
k=0 pk(x)2. In particular  this
speciﬁc quadrature rule allows to perfectly integrate polynomials up to degree 2N  1 (Davis &
Rabinowitz  1984  Section 2.7). In a sense  the DPPs of Bardenet & Hardy (2019) are a random 
multivariate generalization of Gauss-Jacobi quadrature  as we shall see in Section 3.1.
2 Many thanks to Mathieu Gerber of Univ. Bristol  UK  for digging up this result from his human memory.

2

Monte Carlo integration can be deﬁned as random choices of nodes in (1). Importance sampling  for
instance  corresponds to i.i.d. nodes  while Markov chain Monte Carlo corresponds to nodes drawn
from a carefully chosen Markov chain; see  e.g.  Robert & Casella (2004) for more details. Finally 
quasi-Monte Carlo (QMC  Dick & Pillichshammer  2010) applies to µ uniform over a compact subset
of Rd  and constructs deterministic nodes that spread uniformly  as measured by their discrepancy.

2.2 Projection DPPs
DPPs can be understood as a parametric class of point processes  speciﬁed by a base measure µ and
a kernel K : X ⇥ X ! C. The latter is commonly assumed to be Hermitian and trace-class. For
the resulting process to be well deﬁned  it is necessary and sufﬁcient that the kernel K is positive
semi-deﬁnite with eigenvalues in [0  1]  see  e.g.  Soshnikov (2000  Theorem 3). When the eigenvalues
further belong to {0  1}  we speak of a projection kernel and a projection DPP. One practical feature
of projection DPPs is that they almost surely produce samples with ﬁxed cardinality  equal to the
rank N of the kernel. More generally  they are the building blocks of DPPs. Indeed  under general
assumptions  all DPPs are mixtures of projection DPPs (Hough et al.  2006  Theorem 7). Hereafter 
unless speciﬁcally stated  K is assumed to be a real-valued  symmetric  projection kernel.
One way to deﬁne a projection DPP with N points is to take N functions 0  . . .   N1 orthonormal
w.r.t. µ  i.e.  hk  `i  R k(x)`(x)µ(dx) = k`  and consider the kernel KN associated to the
orthogonal projector onto HN   span{k  0  k  N  1}  i.e. 

KN (x  y)  

k(x)k(y).

(2)

N1Xk=0

We say that the set {x1  . . .   xN}⇢ X is drawn from the projection DPP with base measure µ and
kernel KN  denoted by {x1  . . .   xN}⇠ DPP(µ  KN )  when (x1  . . .   xN ) has joint distribution
(3)

det(KN (xp  xn))N

p n=1 µ⌦N (dx).

1
N !

DPP(µ  KN ) indeed deﬁnes a probability measure over sets since (3) is invariant by permutation
and the orthonormality of the ks yields the normalization. See also Appendix A.1 for more details
on the construction of projection DPPs from sets of linearly independent functions.
The repulsion of projection DPPs may be understood geometrically by considering the Gram formu-
lation of the kernel (2)  namely

KN (x  y) =( x)T(y)  where (x)   (0(x)  . . .   N1(x))T.

This allows to rewrite the joint distribution (3) as

1
N !

|

det (x1:N )(x1:N )T

=(det (x1:N ))2

{z

µ⌦N (dx)  where (x1:N )  0B@

}

0(x1)

...

. . .  N1(x1)

...

0(xN )

. . .  N1(xN )

Thus  the larger the determinant of the feature matrix (x1:N )  i.e.  the larger the volume of the
parallelotope spanned by the feature vectors (x1)  . . .   (xN )  the more likely x1  . . .   xN co-occur.

(4)

1CA. (5)

2.3 The multivariate Jacobi ensemble
In this part  we specify a projection kernel. We follow Bardenet & Hardy (2019) and take its
eigenfunctions to be multivariate orthonormal polynomials. In dimension d = 1  letting (k)k0 in (2)
be the orthonormal polynomials w.r.t. µ results in a projection DPP called an orthogonal polynomial
ensemble (OPE  König  2004). When d > 1  orthonormal polynomials can still be uniquely deﬁned
by applying the Gram-Schmidt procedure to a set of monomials  provided the base measure is not
pathological. However  there is no natural order on multivariate monomials: an ordering b : Nd ! N
must be picked before we apply Gram-Schmidt to the monomials in L2(µ). We follow Bardenet
& Hardy (2019  Section 2.1.3) and consider multi-indices k   (k1  . . .   kd) 2 Nd ordered by their
maximum degree maxi ki  and for constant maximum degree  by the usual lexicographic order. We
still denote the corresponding multivariate orthonormal polynomials by (k)k2Nd.

3

By multivariate OPE we mean the projection DPP with base measure µ(dx)   !(x) dx and orthogo-
nal projection kernel KN (x  y)  PN1
b(k)=0 k(x)k(y). When the base measure is separable  i.e. 
!(x) = !1(x1) ⇥···⇥ !d(xd)  multivariate orthonormal polynomials are products of univariate
ones  and the kernel (2) reads

KN (x  y) =

i
ki(xi)i

ki(yi) 

(6)

N1Xb(k)=0

dYi=1

`)`0 are the orthonormal polynomials w.r.t. !i(z) dz. For X = [1  1]d and !i(z) =

where (i
(1  z)ai(1 + z)bi  with ai  bi > 1  the resulting DPP is called a multivariate Jacobi ensemble.
3 Monte Carlo integration with projection DPPs
Our goal is to design random quadrature rules (1) on X   [1  1]d with desirable properties. We
focus on computingR f (x)µ(dx) with the two unbiased DPP-based Monte Carlo estimators of
Bardenet & Hardy (BH  2019) and Ermakov & Zolotukhin (EZ  1960). We start by presenting the
natural BH estimator which  when associated to the multivariate Jacobi ensemble  comes with a CLT
with a faster rate than classical Monte Carlo. Then  we survey the properties of the less obvious EZ
estimator. Using a generalization of the Cauchy-Binet formula we provide a slight improvement of
the key result of EZ. Despite the lack of result illustrating a fast convergence rate  the EZ estimator
has a practical and interpretable variance. In particular  this estimator turns a single DPP sample
into a perfect integrator as well as a perfect interpolator of functions that are linear combinations
of eigenfunctions of the associated kernel. Finally  we detail our exact sampling procedure for
multivariate Jacobi ensemble  which allows to exploit the best of both the BH and EZ estimators.

3.1 A natural estimator
For f 2 L1(µ)  Bardenet & Hardy (2019) consider
NXn=1

N (f )  

bI BH

f (xn)

KN (xn  xn)

 

(7)

N (f )i =

VarhbI BH

as an unbiased estimator ofR f (x)µ(dx)  with variance (see  e.g.  Lavancier et al.  2012  Section 2.1)

f (y)

1

KN (x  y)2µ(dx)µ(dy) 

(8)

2Z ✓ f (x)

KN (x  x) 

KN (y  y)◆2

which clearly captures a notion of smoothness of f w.r.t. KN but its interpretation is not obvious.
For X = [1  1]d  the interest in multivariate Jacobi ensemble among DPPs comes from the fact that
(7) can be understood as a (randomized) multivariate counterpart of the Gauss-Jacobi quadrature
introduced in Section 2.1. Moreover  for f essentially C1  Bardenet & Hardy (2019  Theorem 2.7)
proved a CLT with faster-than-classical-Monte-Carlo decay 

N (f ) Z f (x)µ(dx)◆ law!N!1 N0  ⌦2
f ! 

pN 1+1/d✓bI BH
2Pk2Nd(k1 + ··· + kd)F f!
(k)2  where Fg denotes the Fourier transform of g  and
i=1 ⇡p1  (xi)2. In the fast CLT (9)  the asymptotic variance is governed by the

smoothness of f since ⌦f ! is a measure of the decay of the Fourier coefﬁcients of the integrand.

!eq(x)   1/Qd

f !   1

with ⌦2

(9)

!eq

3.2 The Ermakov-Zolotukhin estimator
We start by stating the main ﬁnding of Ermakov & Zolotukhin (1960)  see also Evans & Swartz (2000 
Section 6.4.3) and references therein. To the best of our knowledge  we are the ﬁrst to make the con-
nection with projection DPPs  as deﬁned in Section 2.2. This allows us to give a slight improvement
and provide a simpler proof of the original result  based on a generalization of the Cauchy-Binet
formula (Johansson  2006). Finally  we apply Ermakov & Zolotukhin’s (1960) technique to build an

unbiased estimator ofR f (x)µ(dx)  which comes with a practical and interpretable variance.

4

...

0(x1)

0B@

. . .  N1(x1)

Theorem 1. Consider f 2 L2(µ) and N functions 0  . . .   N1 2 L2(µ) orthonormal w.r.t. µ. Let
{x1  . . .   xN}⇠ DPP(µ  KN )  with KN (x  y) =PN1
k=0 k(x)k(y). Consider the linear system
1CA
0B@

det k1 f (x1:N )
 
Then  the solution of (10) is unique  µ-almost surely  with coordinates yk =
where k1 f (x1:N ) is the matrix obtained by replacing the k-th column of (x1:N ) by f (x1:N ).
Moreover  for all 1  k  N  the coordinate yk of the solution vector satisﬁes
hf  `i2.

and Var[yk] = kfk2 

1CA =0B@

E[yk] = hf  k1i 

. . .  N1(xN )

1CA.

y1
...
yN

0(xN )

det (x1:N )

f (xN )

f (x1)

(10)

(11)

...

...

We improved the original result by showing that Cov[yj  yk] = 0  for all 1  j 6= k  N.
Before we provide the proof  also detailed in Appendix A.2  several remarks are in order. We start by
considering a function f  PM1
k=0 hf  kik  1  M  1  where (k)k0 forms an orthonormal
basis of L2(µ)  e.g.  the Fourier basis or wavelet bases (Mallat & Peyré  2009). Next  we build the
orthogonal projection kernel KN onto HN   span{0  . . .   N1} as in (2). Then  Theorem 1
shows that solving (10)  with points {x1  . . .   xN}⇠ DPP(µ  KN )  provides unbiased estimates of
the N Fourier-like coefﬁcients (hf  ki)N1
k=0 . Remarkably  these estimates are uncorrelated and have
the same variance (11) equal to the residualP1k=Nhf  ki2. The faster the decay of the coefﬁcients 
the smaller the variance. In particular  for M  N  i.e.  f 2H N  the estimators have zero variance.
Put differently  f can be reconstructed perfectly from only one sample of DPP(µ  KN ).

N1X`=0

Proof. First  the joint distribution (5) of (x1  . . .   xN ) is proportional to (det (x1:N ))2µ⌦N (x).
Thus  the matrix (x1:N ) deﬁning the linear system (10) is invertible  µ-almost surely  and the
expression of the coordinates follows from Cramer’s rule. Then  we treat the case k = 1  the others
follow the same lines. The proof relies on the orthonormality of the ks and a generalization of the
Cauchy-Binet formula (A.1)  cf. Lemma A. The expectation in (11) reads

E det 0 f (x1:N )
det (x1:N )  (5)

=

Similarly  the second moment reads
1

E"✓ det 0 f (x1:N )

det (x1:N ) ◆2# (5)

=

1

`=1

(A.1)

IN1

0N1 1

⌘ = hf  0i.

N !Z det 0 f (x1:N ) det (x1:N ) µ⌦N (dx)
= det⇣ hf 0i (hf `i)N1
N !Z det 0 f (x1:N ) det 0 f (x1:N ) µ⌦N (dx)
= det⇣
N1Xk=1

⌘ = kfk2 

(hf ki)N1

(hf `i)N1

kfk2

IN1

k=1

`=1

(A.1)

hf  ki2.

(12)

(13)

Finally  the variance in (11) = (13) - (12)2. The covariance is treated in Appendix A.2.

In the setting of the multivariate Jacobi ensemble described in Section 2.3  the ﬁrst orthonormal

= µ[1  1]d1/2 det 0 f (x1:N )

polynomial 0 is constant  equal to µ[1  1]d1/2. Hence  a direct application of Theorem 1 yields
N (f )   y1
bI EZ
0
as an unbiased estimator ofR[1 1]d f (x)µ(dx)  see Appendix A.3. We also show that (14) can be
viewed as a quadrature rule (1) with weights summing to µ([1  1]d). Unlike the variance ofbI BH
N (f )
in (8)  the variance ofbI EZ
N (f ) clearly reﬂects the accuracy of the approximation of f by its projection
onto HN. In particular  it allows to integrate and interpolate polynomials up to “degree” b1(N  1) 
perfectly. Nonetheless  its limiting theoretical properties  like a CLT  look hard to establish. In
particular  the dependence of each quadrature weight on all quadrature nodes makes the estimator a
peculiar object that doesn’t ﬁt the assumptions of traditional CLTs for DPPs (Soshnikov  2000).

det (x1:N )

(14)

 

5

3.3 How to sample from projection DPPs and the multivariate Jacobi ensemble
To perform Monte Carlo integration with DPPs  it is crucial to sample the points and evaluate the
weights efﬁciently. However  sampling from continuous DPPs remains a challenge. In this part  we
review brieﬂy the main technique for projection DPP sampling before we develop our method to
generate samples from the multivariate Jacobi ensemble. The code1 is available in the DPPy toolbox
of Gautier et al. (2019)  the associated documentation3 contains a lot more details on DPP sampling.
In both ﬁnite and continuous cases  except for some speciﬁc instances  exact DPP sampling usually
requires the spectral decomposition of the underlying kernel (Lavancier et al.  2012  Section 2.4).
However  for projection DPPs  prior knowledge of the eigenfunctions is not necessary  only an oracle
to evaluate the kernel is required. Next  we describe the generic exact sampler of Hough et al. (2006 
Algorithm 18). It is based on the chain rule and valid exclusively for projection DPPs.
For simplicity  consider a projection DPP(µ  KN ) with µ(dx) = !(x) dx and KN as in (2). This
DPP has exactly N points  µ-almost surely (Hough et al.  2006  Lemma 17). To get a valid sample
{x1  . . .   xN}  it is enough to apply the chain rule to sample (x1  . . .   xN ) and forget the order the
points were selected. The chain rule scheme can be derived from two different perspectives. Either
using Schur complements to express the determinant in the joint distribution (3) 

KN (x1  x1)

N

!(x1) dx1

NYn=2

KN (xn  xn)  Kn1(xn)TK1

n1Kn1(xn)

!(xn) dxn 

(15)

N  (n  1)

!(xn) dxn.

(16)

where Kn1(·) = (KN (x1 ·)  . . .   KN (xn1 ·))T  and Kn1 = (KN (xp  xq))n1
cally using the base⇥height formula to express the squared volume in the joint distribution (5) 

p q=1. Or geometri-

k(x1)k2

N

!(x1) dx1

distance2(xn)  span{(xp)}n1
p=1

N  (n  1)

NYn=2

parametersai bi  1/2  cf. Section 2.3. We remodeled the original implementation4 of the

Note that the numerators in (15) correspond to the incremental posterior variances of a noise-free
Gaussian process model with kernel KN (Rasmussen & Williams  2006)  giving yet another intuition
for repulsion. When using the chain rule  the practical challenge is twofold: ﬁnd efﬁcient ways to (i)
evaluate the conditional densities  (ii) sample exactly from the conditionals.
In this work  we take X = [1  1]d and focus on sampling the multivariate Jacobi ensemble with
multivariate Jacobi ensemble sampler accompanying the work of Bardenet & Hardy (BH  2019) in a
more Pythonic way. In particular  we address the previous challenges in the following way:
(i) contrary to BH  we leverage the Gram structure to vectorize the computations and consider (16)
instead of (15)  and evaluate KN (x  y) via (4) instead of (6). The overall procedure is akin to a
sequential Gram-Schmidt orthogonalization of the feature vectors (x1)  . . .   (xN ). Moreover we
pay special attention to avoiding unnecessary evaluations of orthogonal polynomials (OP) when com-
puting a feature vector (x). This is done by coupling the slicing feature of the Python language with
the dedicated method scipy.special.eval_jacobi  used to evaluate the three-term recurrence
relations satisﬁed by each of the univariate Jacobi polynomials. Given the chosen ordering b  the
computation of (x) requires the evaluation of d recurrence relations up to depth dpN.
(ii) like BH  we sample each conditional in turn using a rejection sampling mechanism with the same
proposal distribution. But BH take as proposal !eq(x) dx  which corresponds to the limiting marginal
of the multivariate Jacobi ensemble as N goes to inﬁnity; see (Simon  2011  Section 3.11). On our
side  we use a two-layer rejection sampling scheme. We rather sample from the n-th conditional using
the marginal distribution N1KN (x  x)!(x) dx as proposal and rejection constant N/(N  (n 1)).
This allows us to reduce the number of (costly) evaluations of the acceptance ratio. The marginal
distribution itself is sampled using the same proposal !eq(x) dx and rejection constant as BH. The
rejection constant  of order 2d  is derived from Chow et al. (1994) and Gautschi (2009). We further
reduced the number of OP evaluations by considering N1KN (x  x)!(x) dx as a mixture  where
each component in (6) involves only one OP. In the end  the expected total number of rejections
is of order 2dN log N. Finally  we implemented a speciﬁc rejection free method for the univariate
Jacobi ensemble; a special continuous projection DPP which can be sampled exactly in O(N 2)  by
computing the eigenvalues of a random tridiagonal matrix (Killip & Nenciu  2004  Theorem 2).
3 dppy.readthedocs.io 4 github.com/rbardenet/dppmc

6

All these improvements resulted in dramatic speedups. For example  on a modern laptop  sampling a
2D Jacobi ensemble with N = 1000 points  see Figure 1(a)  takes less than a minute  compared to
hours previously. For more details on the sampling procedure  we refer to Appendix A.4.

(a) / !1  / !2  !eq

(b) htimei to get one sample

(c) h#rejectionsi to get one sample

Figure 1: (a) A sample from a 2D Jacobi ensemble with N = 1000 points. (b)-(c) ai  bi = 1/2 
the colors and numbers correspond to the dimension. For d = 1  the tridiagonal model (tri) of Killip
& Nenciu offers tremendous time savings. (c) The total number of rejections grows as 2dN log(N ).
4 Empirical investigation

We perform three main sets of experiments to investigate the properties of the BH (7) and EZ (14)

estimators of the integralR f (x)µ(dx). We add the baseline vanilla Monte Carlo  where points are

drawn i.i.d. proportionally to µ. The two estimators are built from the multivariate Jacobi ensemble 
cf. Section 2.3. First  we extend  for larger N  the experiments of Bardenet & Hardy (2019) illustrating
their fast CLT (9) on a smooth function. Then  we illustrate Theorem 1 by considering polynomial
functions that can be either fully or partially decomposed in the eigenbasis of the DPP kernel. Finally 
we compare the potential of both estimators on various non smooth functions.

(a) d = 1

(b) d = 2

(c) d = 3

(d) d = 4

(e) d = 1

(f) d = 2

(g) d = 3

(h) d = 4

Figure 2: (a)-(d) cf. Section 4.1  the numbers in the legend are the slope and R2 (e)-(h) cf. Section 4.2.

4.1 The bump experiment

N dramatically outperformsbI BH

Bardenet & Hardy (2019  Section 3) illustrate the behavior ofbI BH
N and its CLT (9) on a unimodal 
smooth bump function; see Appendix B.1. The expected variance decay is of order 1/N 1+1/d. We
reproduce their experiment in Figure 2 for larger N  and compare with the behavior of bI EZ
N . In
short bI EZ
N in d  2  with surprisingly fast empirical convergence rates.
When d  3  performance decreases  andbI BH
N shows both faster and more regular variance decay.
To know whether we can hope for a CLT for bI EZ
N   we performed Kolmogorov-Smirnov tests for
N = 300  which yielded small p-values across dimensions  from 0.03 to 0.24. This is compared to
the same p-values forbI BH
N   which range from 0.60 to 0.99. The results are presented in Appendix B.1.
The lack of normality ofbI EZ
N is partly due to a few outliers. Where these outliers come from is left
for future work; ill-conditioning of the linear system (10) is an obvious candidate. Besides  contrary
tobI BH
N   the estimatorbI EZ
N has no guarantee to preserve the sign of integrands having constant sign.

7

4.2

Integrating sums of eigenfunctions

f (x) =XM1

b(k)=0

k(x) 

b(k) + 1

prescribed by Theorem 1. To that end  we consider functions of the form

In the next series of experiments  we are mainly interested in testing the variance decay ofbI EZ

1

N (f )

(17)

whose integral w.r.t. µ is to be estimated based on realizations of the multivariate Jacobi ensemble with
kernel KN (x  y) =PN1
b(k)=0 k(x)k(y)  where N 6= M a priori. This means that the function f
can be either fully (M  N) or partially (M > N) decomposed in the eigenbasis of the kernel. In
both cases  we let the number of points N used to build the two estimators vary from 10 to 100 in
dimensions d = 1 to 4. In the ﬁrst setting  we set M = 70. Thus  N eventually reaches the number
of functions used to build f in (17)  after whatbI EZ
N is an exact estimator  see Figure 2(e)-(h). The
second setting has M = N + 1  so that the number of points N is never enough for the variance in
(11) to be zero. The results of both settings are to be found in Appendix B.2.
In the ﬁrst case  for each dimension d  we indeed observe a drop in the variance of bI EZ
N once the
number of points of the DPP hits the threshold N = M. This is in perfect agreement with Theorem 1:
once f 2H M ✓H N  the variance in (11) is zero. In the second setting  as N increases the
contribution of the extra mode b1(N ) in (17) decreases as 1
N . Hence  from Theorem 1 we expect a
variance decay of order 1

N 2   which we observe in practice.

4.3 Further experiments
In Appendices B.3-B.6 we test the robustness of both BH and EZ estimators  when applied to functions
presenting discontinuities or which do not belong to the span of the eigenfunctions of the kernel.

Although the conditions of the CLT (9) associated tobI BH are violated  the corresponding variance
decay is smooth but not as fast. ForbI EZ  the performance deteriorates with the dimension. Indeed 
the cross terms arising from the Taylor expansion of the different functions introduce monomials 
associated to large coefﬁcients  that do not belong to HN. Sampling more points would reduce the
variance (11). But more importantly  for EZ to excel  this suggests to adapt the kernel to the basis
where the integrand is known to be sparse or to have fast-decaying coefﬁcients. In regimes where BH
and EZ do not shine  vanilla Monte Carlo becomes competitive for small values of N.

5 Conclusion

Ermakov & Zolotukhin (EZ  1960) proposed a non-obvious unbiased Monte Carlo estimator using
projection DPPs. It requires solving a linear system  which in turn involves evaluating both the N
eigenfunctions of the corresponding kernel and the integrand at the N points of the DPP sample.
This is yet another connection between DPPs and linear algebra. In fact  solving this linear system
provides unbiased estimates of the Fourier-like coefﬁcients of the integrand f with each of the N
eigenfunctions of the DPP kernel. Remarkably  these estimators have identical variance  and this
variance measures the accuracy of the approximation of f by its projection onto these eigenfunctions.
With modern arguments  we have provided a much shorter proof of these properties than in the
original work of (Ermakov & Zolotukhin  1960). Beyond this  little is known on the EZ estimator.
While coming with a less interpretable variance  the more direct estimator proposed by Bardenet &
Hardy (BH  2019) has an intrinsic connection with the classical Gauss quadrature and further enjoys
stronger theoretical properties when using multivariate Jacobi ensemble.
Our experiments highlight the key features of both estimators when the underlying DPP is a multi-
variate Jacobi ensemble  and further demonstrate the known properties of the BH estimator in yet
unexplored regimes. Although EZ shows a surprisingly fast empirical convergence rate for d  2 
its behavior is more erratic for d  3. Ill-conditioning of the linear system is a potential source of
outliers in the distribution of the estimator. Regularization may help but would introduce a stabil-
ity/bias trade-off. More generally  EZ seems worth investigating for integration or even interpolation
tasks where the function is known to be decomposable in the eigenbasis of the kernel  i.e.  in a
setting similar to the one of Bach (2017). Finally  the new implementation of an exact sampler for
multivariate Jacobi ensemble unlocks more large-scale empirical investigations and asks for more
theoretical work. The associated code1 is available in the DPPy toolbox of Gautier et al. (2019).

8

References
Bach  F. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions.

Journal of Machine Learning Research  2017. arXiv:1502.06800.

Bach  F.  Lacoste-Julien  S.  and Obozinski  G. On the Equivalence between Herding and Condi-
tional Gradient Algorithms. In International Conference on Machine Learning (ICML)  2012.
arXiv:1203.4523.

Bardenet  R. and Hardy  A. Monte Carlo with Determinantal Point Processes. Annals of Applied

Probability  in press  2019. arXiv:1605.00361.

Briol  F.-X.  Oates  C. J.  Girolami  M.  and Osborne  M. A. Frank-Wolfe Bayesian Quadrature: Prob-
abilistic Integration with Theoretical Guarantees. In Advances in Neural Information Processing
Systems (NeurIPS)  2015. arXiv:1506.02681.

Chen  Y.  Welling  M.  and Smola  A. Super-Samples from Kernel Herding. In Conference on

Uncertainty in Artiﬁcial Intelligence (UAI)  2010. arXiv:1203.3472.

Chow  Y.  Gatteschi  L.  and Wong  R. A Bernstein-type inequality for the Jacobi polynomial.

Proceedings of the American Mathematical Society  1994.

Davis  P. J. and Rabinowitz  P. Methods of numerical integration. Academic Press. 1984.

Delyon  B. and Portier  F.

arXiv:1409.0733.

Integral approximation by kernel smoothing. Bernoulli  2016.

Dick  J. and Pillichshammer  F. Digital nets and sequences : discrepancy and quasi-Monte Carlo

integration. Cambridge University Press. 2010.

Ermakov  S. M. and Zolotukhin  V. G. Polynomial Approximations and the Monte-Carlo Method.

Theory of Probability & Its Applications  1960.

Evans  M. and Swartz  T. Approximating integrals via Monte Carlo and deterministic methods.

Oxford University Press. 2000.

Gautier  G.  Polito  G.  Bardenet  R.  and Valko  M. DPPy: DPP Sampling with Python. Journal of
Machine Learning Research - Machine Learning Open Source Software (JMLR-MLOSS)  in press 
2019. arXiv:1809.07258.

Gautschi  W. How sharp is Bernstein’s Inequality for Jacobi polynomials? Electronic Transactions

on Numerical Analysis  2009.

Hough  J. B.  Krishnapur  M.  Peres  Y.  and Virág  B. Determinantal Processes and Independence. In

Probability Surveys. 2006. arXiv:math/0503110.

Huszár  F. and Duvenaud  D. Optimally-Weighted Herding is Bayesian Quadrature. In Conference

on Uncertainty in Artiﬁcial Intelligence (UAI)  2012. arXiv:1204.1664.

Johansson  K. Random matrices and determinantal processes. Les Houches Summer School Proceed-

ings  2006.

Killip  R. and Nenciu  I. Matrix models for circular ensembles. International Mathematics Research

Notices  2004. arXiv:math/0410034.

König  W. Orthogonal polynomial ensembles in probability theory. Probability Surveys  2004.

arXiv:math/0403090.

Kulesza  A. and Taskar  B. Determinantal Point Processes for Machine Learning. Foundations and

Trends in Machine Learning  2012. arXiv:1207.6083.

Lavancier  F.  Møller  J.  and Rubak  E. Determinantal point process models and statistical inference :
Extended version. Journal of the Royal Statistical Society. Series B: Statistical Methodology  2012.
arXiv:1205.4818.

9

Liu  Q. and Lee  J. D. Black-Box Importance Sampling. In Internation Conference on Artiﬁcial

Intelligence and Statistics (AISTATS)  2017. arXiv:1610.05247.

Macchi  O. The coincidence approach to stochastic point processes. Advances in Applied Probability 

1975.

Mallat  S. and Peyré  G. A wavelet tour of signal processing : the sparse way. Elsevier/Academic

Press. 2009.

Mazoyer  A.  Coeurjolly  J.-F.  and Amblard  P.-O. Projections of determinantal point processes.

ArXiv e-prints  2019. arXiv:1901.02099v3.

Oates  C. J.  Girolami  M.  and Chopin  N. Control functionals for Monte Carlo integration. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  2017. arXiv:1410.2392.
O’Hagan  A. Bayes–Hermite quadrature. Journal of Statistical Planning and Inference  1991.
Rasmussen  C. E. and Williams  C. K. I. Gaussian processes for machine learning. MIT Press. 2006.
Robert  C. P. The Bayesian choice : from decision-theoretic foundations to computational implemen-

tation. Springer. 2007.

Robert  C. P. and Casella  G. Monte Carlo statistical methods. Springer-Verlag New York. 2004.
Simon  B. Szeg˝o’s theorem and its descendants: Spectral theory for l2 perturbations of orthogonal

polynomials. M. B. Porter Lecture Series  Princeton Univ. Press  Princeton  NJ. 2011.

Soshnikov  A. Determinantal random point ﬁelds.

arXiv:math/0002099.

Russian Mathematical Surveys  2000.

10

,Guillaume Gautier
Rémi Bardenet
Michal Valko