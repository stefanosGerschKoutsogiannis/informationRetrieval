2013,Regression-tree Tuning in a Streaming Setting,We consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time. We prove that it is possible to maintain such a structure in time $O(\log n)$ at any time step $n$ while achieving a nearly-optimal regression rate of $\tilde{O}(n^{-2/(2+d)})$ in terms of the unknown metric dimension $d$. Finally we prove a new regression lower-bound which is independent of a given data size  and hence is more appropriate for the streaming setting.,Regression-tree Tuning in a Streaming Setting

Samory Kpotufe∗

Toyota Technological Institute at Chicago†

firstname@ttic.edu

Francesco Orabona∗

Toyota Technological Institute at Chicago

francesco@orabona.com

Abstract

We consider the problem of maintaining the data-structures of a partition-based
regression procedure in a setting where the training data arrives sequentially over
time. We prove that it is possible to maintain such a structure in time O (log n) at

any time step n while achieving a nearly-optimal regression rate of ˜O(cid:0)n−2/(2+d)(cid:1)

in terms of the unknown metric dimension d. Finally we prove a new regression
lower-bound which is independent of a given data size  and hence is more appro-
priate for the streaming setting.

1

Introduction

Traditional nonparametric regression such as kernel or k-NN can be expensive to estimate given
modern large training data sizes. It is therefore common to resort to cheaper methods such as tree-
based regression which precompute the regression estimates over a partition of the data space [7].
Given a future query x  the estimate fn(x) simply consists of ﬁnding the closest cell of the partition
by traversing an appropriate tree-structure and returning the precomputed estimate. The partition
and precomputed estimates depend on the training data and are usually maintained in batch-mode.
We are interested in maintaining such a partition and estimates in a real-world setting where the
training data arrives sequentially over time. Our constraints are that of fast-update at every time
step  while maintaining a near-minimax regression error-rate at any point in time.
The error-rate of tree-based regression is well known to depend on the size of the partition’s cells.
We will call this size the binwidth. The minimax-optimal binwidth n is known to be of the form

O(cid:0)n−1/(2+d)(cid:1)  assuming a training data of size n from a metric space of unknown dimension d 
of O(cid:0)n−2/(2+d)(cid:1). Thus  the dimension d is the most important problem variable entering the rate

and unknown Lipschitz target function f. This setting of n would then yield a minimax error rate

(and the tuning of n)  while other problem variables such as the Lipschitz properties of f are less
crucial in comparison. The main focus of this work is therefore that of adapting to the unknown d
while maintaining fast partition estimates in a streaming setting.
A ﬁrst idea would be to start with an initial dimension estimation phase (where the regression esti-
mates are suboptimal)  and using the estimated dimension for subsequent data in a following phase 
which leaves only the problem of maintaining partition estimates over time. However  while this
sounds reasonable  it is generally unclear when to conﬁdently stop such an initial phase since this
would depend on the unknown d and the distribution of the data.
Our solution is to interleave dimension estimation with regression updates as the data arrives se-
quentially. However the algorithm never relies on the estimated dimensions and views them rather
as guesses di. Even if di (cid:54)= d  it is kept as long as it is not hurtful to regression performance. The
guess di is discarded once we detect that it hurts the regression  a new di+1 is then estimated and a
new phase i+1 is started. The decision to discard di relies on monitoring quantities that play into the
tradeoff between regression variance and bias  more precisely we monitor the size of the partition

∗SK and FO contributed equally to this paper.
†Other afﬁliation: Max Planck Institute for Intelligent Systems  Germany

1

and the partition’s binwidth n. We note that the idea can be applied to other forms of regression
where other quantities control the regression variance and bias (see longer version of the paper).

1.1 Technical Overview of Results

those of [2  10]. We prove that the L2 error of the algorithm is ˜O(cid:0)n−2/(2+d)(cid:1)  nearly optimal in

We assume that training data (xi  Yi) is sampled sequentially over time  xi belongs to a general
metric space X of unknown dimension d  and Yi is real. The exact setup is given in Section 2.
The algorithm (presented in Section 2.3) maintains regression estimates for all training samples
xn (cid:44) {xt}n
t=1 arriving over time  while constantly updating a partition of the data and partition
binwidth. At any time t = n  all updates are proveably of order log n with constants depending on
the unknown dimension d of X .
At time t = n  the estimate for a query point x is given by the precomputed estimate for the closest
point to x in xn  which can be found fast using an off-the-shelf similarity search structure  such as
terms of the unknown dimension d of the metric X .
Finally  we prove a new lower-bound for regression on a generic metric X   where the worst-case
distribution is the same as n increases. Note that traditional lower-bounds for the ofﬂine setting
derive a different worst-case distribution for each sample size n. Thus  our lower-bound is more
appropriate to the streaming setting where the data arrives over time from the same distribution.
The results are discussed in more technical details in Section 3.

1.2 Related Work

Although various interesting heuristics have been proposed for maintaining tree-based learners in
streaming settings (see e.g. [1  5  11  15])  the problem has not received much theoretical attention.
This is however an important problem given the growing size of modern datasets  and given that in
many modern applications  training data is actually acquired sequentially over time and incremental
updates have to be efﬁcient (see e.g. Robotics [12  16]  Finance [8]).
The most closely related theoretical work is that of [6] which treats the problem of tuning a local
polynomial regressor where the training data is acquired over time. Their setting however is that
of a Euclidean space where d is known (ambient Euclidean dimension). [6] is thus concerned with
maintaining a minimax error rate w.r.t. the known dimension d  while efﬁciently tuning regression
bandwidth.
A possible alternative to the method analyzed here is to employ some form of cross-validation or
even online solutions based on mixture of experts [3]  by keeping track of different partitions  each
corresponding to some setting of the bindwidth n. This is however likely expensive to maintain in
practice if good prediction performance is desired.

2 Preliminaries

2.1 Notions of metric dimension

We consider the following notion of dimension which extends traditional notions of dimension (e.g.
Euclidean dimension and manifold dimension) to general metric spaces [4]. We assume throughout 
w.l.o.g. that the space X has diameter at most 1 under a metric ρ.
Deﬁnition 1. The metric measure space (X   µ  ρ) has metric measure-dimension d  if there exist
ˇCµ  ˆCµ such that for all  > 0  and x ∈ X   ˇCµd ≤ µ(B(x  )) ≤ ˆCµd.
The assumption of ﬁnite metric-measure dimension ensures that the measure µ has mass everywhere
on the space ρ. This assumption is a generalization (to a metric space) of common assumptions
where the measure has an upper and lower-bounded density on a compact Euclidean space  however
is more general in that it does not require the measure µ to have a density (relative to any reference
measure). The metric-measure dimension implies the following other notion of metric dimension.
Deﬁnition 2. The metric (X   ρ) has metric dimension d  if there exists ˆCρ such that  for all  > 0 
X has an -cover of size at most ˆCρ−d.

2

The relation between the two notions of dimension is stated in the following lemma of [9]  which
allows us to use either notion as needed.
Lemma 1 ([9]). If (X   µ  ρ) has metric-measure dimension d  then there exists ˇCρ  ˆCρ such that  for
all  > 0  any ball B(x  r) centered on (X   ρ) has an r-cover of size in [ ˇCρ−d  ˆCρ−d].

2.2 Problem Setup

We receive data pairs (x1  Y1)  (x2  Y2)  . . . sequentially  i.i.d. The input xt belongs to a metric
measure space (X   ρ  µ) of diameter at most 1  and of metric measure dimension d. The output Yt
belongs to a subset of R of bounded diameter ∆Y   and satisﬁes Yt = f (xt) + η(xt). The noise
η(xt) has 0 mean. The unknown function f is assumed to be λ-Lipschitz w.r.t. ρ for an unknown
parameter λ  that is ∀x  x(cid:48) ∈ X  
L2 error: Our main performance result bounds the excess L2 risk

|f (x) − f (x(cid:48))| ≤ λρ (x  x(cid:48)).

E

xn Y n

(cid:107)fn − f(cid:107)2

2 µ

(cid:44) E

xn Y n

E
X

|fn(X) − f (X)|2 .

We will often also be interested in the average error on the training sample: recall that at any time t 
an estimate ft(xs) of f is produced for every xs ∈ xt. The average error on xn at t = n is denoted

n(cid:88)

t=1

|fn(xt) − f (xt)|2 .

E n E

Y n

|fn(X) − f (X)|2 (cid:44) 1
n

2.3 Algorithm

−1/(2+di)
i

The procedure (Algorithm 1) works by partitioning the data into small regions of size roughly t/2 at
any time t  and computing the regression estimate of the centers of each region. All points falling in
the same region (identiﬁed by a center point) are assigned the same regression estimate: the average
Y values of all points in the region.
The procedure works in phases  where each Phase i corresponds to a guess di of the metric dimen-
sion d. Where t might have been set to t−1/(2+d) if we knew d  we set it to t
where ti is
the current time step within Phase i.
We ensure that in each phase our guess di does not hurt the variance-bias tradeoff of the estimates:
this is done by monitoring the size of the partition (|Xi| in the algorithm)  which controls the vari-
ance (see analysis in Section 4)  relative to the bindwidth t  which controls bias. Whenever |Xi| is
too large relative to t  the variance of the procedure is likely too large  so we start a new phase with
an new guess of di.
Since the algorithm maintains at any time n an estimate fn(xt) for all xt ∈ xn  for any query point
x ∈ X   we simply return fn(x) = fn(xt) where xt is the closest point to x in xn.
Despite having to adaptively tune to the unknown d  the main computation at each time step con-
sists of just a 2-approximate nearest neighbor search for the closest center. These searches can be
done fast in time O (log n) by employing the off-the-shelf online search-procedure of [10]. This is
emphasized in Lemma 2 below.
Finally  the algorithm employs a constant ¯C which is assumed to upper-bound the constant Cρ in
Deﬁnition 2. This is a minor assumption since Cρ is generally taken to be small  e.g. 1  in machine
learning literature  and is exactly quantiﬁeable for various metrics [4  10].

3 Discussion of Results

3.1 Time complexity

The time complexity of updates is emphasized in the following Lemma.
Lemma 2. Suppose (X   ρ  µ) has metric dimension d. Then there exist C depending on d such that
all computations of the algorithm at any time t = n can be done in time C log n.

3

−1/(2+di)
i

Receive (xt  yt)
ti ← ti + 1 // counts the time steps within Phase i
t ← t
Set xs ∈ Xi to the 2-approximate nearest neighbor of xt
if ρ (xt  xs) ≤ t then

Algorithm 1 Incremental tree-based regressor.
1: Initialize: i = 1  di = 1  ti = 0  Centers Xi = ∅
2: for t = 1  2  . . .   T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
end if
18:
19: end for

Assign xt to xs
fn(xs) ← update average Y for center xs with yt
For every r ≤ t assigned to xs  fn(xr) = fn(xs)
if |Xi| + 1 > ˆC 4di 

end if
Add xt as a new center in Xi

−di
t
// Start of Phase i + 1
|Xi|+1

di+1 ←(cid:108)

log(

i ← i + 1

)/ log(4/t)

ˆC

else

then

(cid:109)

Figure 1: As t varies over
time  a ball around a cen-
ter xs can eventually contain
both points assigned to xs
and points non-assigned to it 
and even contain other cen-
ters. This results in a complex
partitioning of the data.

Proof. The main computation at time n consists of ﬁnding the 2-approximate nearest neighbor xn in
Xi and update the data structure for the nearest neighbor search. These centers are all at least n
2 far-
apart. Using the results of [10]  this can be done online in time O (log(1/n) + log log(1/n)).

3.2 Convergence rates

The main theorem below bounds the L2 error of the algorithm at any given point in time. The main
difﬁculty is in the fact that the data is partitioned in a complicated way due to the ever-changing
bindwidth t: every ball around a center can eventually contain both points assigned to the center
and points not assigned to the center  in fact can contain other centers (see Figure 1). This makes
it hard to get a handle on the number of points assigned to a single center xt (contributing to the
variance of fn(xt)) and the distance between points assigned to the same center (contributing to the
bias). This is not the case in classical analyses of tree-based regression since the data partitioning is
usually clearly deﬁned.
The problem is handled by ﬁrst looking at the average error over points in xn  which is less difﬁcult.
Theorem 1. Suppose the space (X   µ  ρ) has metric-measure dimension d.
For any x ∈ X   deﬁne fn(x) = fn(xt) where xt is the closest point to x in xn. Then at any time
t = n  we have for some C independent of n 

(cid:19)2/d

(cid:18) d log n
Y + λ2(cid:1) n−2/(2+d) .

n

E

xn Y n

(cid:107)fn − f(cid:107)2

2 µ ≤ C(d log n) sup

xn

E n E

Y n

(cid:107)fn(X) − f (X)(cid:107)2 + Cλ2

+

∆2
Y
n

.

If the algorithm parameter ˆC ≥ ˆCρ  then for some constant C(cid:48) independent of n  we have at any
time n that

|fn(X) − f (X)|2 ≤ C(cid:48)(cid:0)∆2

E n E

Y n

sup
xn

The convergence rate is therefore ˜O(n−2/(2+d))  nearly optimal in terms of the unknown d (up to a
log n factor). In the simulation of Figure 2(Left) we compare our procedure to tree-based regressors
with a ﬁxed setting of d and of t = t−1/(2+d). We use the classic rotating-Teapot dataset  where the
target output values are the cosine of the rotation angles. Our method attains the same performance
as the one with the right ﬁxed setting of d.
As alluded to above 
E n EY n |fn(X) − f (X)|2 on the sample xn.

the proof of Theorem 1 proceeds by ﬁrst bounding the average error
Interestingly  the analysis of the average error is

4

Figure 2: Simulation results on Teapot (Left) and Synthetic (Right) datasets. ˆC set to 1  size of the
test sets 1800 and 12500  respectively.

of a worst-case nature where the data x1  x2  . . . is allowed to arrive adversarially (see analysis of
Section 4.1). This shows a sense in which the algorithm is robust to bad dimension estimates: the
average error is of the optimal form in terms of d  even though the data could trick us into picking a
bad guess di of d. Thus the insights behind the algorithm are perhaps of wider applicability to prob-
lems of a more adversarial nature. This is shown empirically in Figure 2(Right)  where we created a
synthetic datasets with d = 5  while the ﬁrst 1000 samples are from a line in X . An algorithm that
estimates the dimension in a ﬁrst phase would end up using the suboptimal setting d = 1  while our
algorithm robustly updates its estimate over time.
As mentioned in the introduction  the same insights can be applied to other forms of regression in
a streaming setting. We show in the longer version of the paper a procedure more akin to kernel
regression  which employs other quantities (appropriate to the method) to control the bias-variance
tradeoff while deciding on keeping or rejecting the guess di.

3.3 Lower-bounds

We have to produce a distribution for which the problem is hard  and which matches our streaming
setting as well as possible. With this in mind  our lower-bound result differs from existing non-
parametric lower-bounds by combining two important aspects. First  the lower-bound holds for any
given metric measure space (X   ρ  µ) with ﬁnite measure-dimension: we constrain the worst-case
distribution to have the marginal µ that nature happens to choose. In contrast  lower-bounds in lit-
erature would commonly pick a suitable marginal on the space X [13  14]. Second  the worst-case
distribution does not depend on the sample size as is common in literature. Instead  we show that
the rate of n−2/(2+d) holds for inﬁnitely many n for a distribution ﬁxed beforehand. This is more
appropriate for the online setting where the data is generated over time from a ﬁxed distribution.
The lower-bound result of [9] also holds for a given measure space (X   µ  ρ)  but the worst-case dis-
tribution depends on sample size. A lower-bound of [7] holds for inﬁnitely many n  but is restricted
to distributions on a Euclidean cube  and thus beneﬁts from the regularity of the cube. Our result
combines some technical intuition from these two results in a way described in Section 4.3.
We need the following deﬁnition.
Deﬁnition 3. Given a metric-measure space (X   µ  ρ)  we let Dµ λ denote the set of distributions on
X  Y   X ∈ X   Y ∈ R  where the marginal on X is µ  and where the function f (x) = E[Y |X = x]
is λ-Lipschitz w.r.t. ρ.
Theorem 2. Let (X   µ  ρ) be a metric space of diameter 1  and metric-measure dimension d. For
any n ∈ N  deﬁne r2
There exists an indexing subsequence {nt}t∈N   nt+1 > nt  such that

n = (λ2n)−2/(2+d). Pick any positive sequence {βn}n∈N   βn = o(cid:0)λ2r2

(cid:1).

n

EX nt  Y nt (cid:107)fnt − f(cid:107)2

2 µ

= ∞ 

inf{fn} sup
Dµ λ

lim
t→∞

βnt

where the inﬁmum is taken over all sequences {fn} of estimators fn : X n  Y n (cid:55)→ L2 µ.

5

01000200030004000500060000.20.30.40.50.60.70.80.91# Training samplesNormalized RMSE on test setTeapot dataset  Incremental Tree−basedfixed d=1fixed d=4fixed d=800.511.522.533.54x 1040.10.20.30.40.50.60.70.80.911.1# Training samplesNormalized RMSE on test setSynthetic dataset  d=5  D=100  first 1000 samples d=1  Incremental Tree−basedfixed d=1fixed d=5fixed d=10By the statement of the theorem  if we pick any rate βn faster than n−2/(2+d)  then there exists a
distribution with marginal µ for which E (cid:107)fn − f(cid:107)2 /βn either diverges or tends to ∞.

4 Analysis

We ﬁrst analyze the average error of the algorithm over the data xn in Section 4.1. The proof of the
main theorem follows in Section 4.2.

4.1 Bounds on Average Error

We start by bounding the average error on the sample xn at time n  that is we upper-bound
E n EY n |fn(X) − f (X)|2.
The proof idea of the upper bound is the following. We bound the error in a given phase (Lemma 4) 
then combine these errors over all phases to obtain the ﬁnal bounds (Corollary 1). To bound the
error in a phase  we decompose the error in terms of squared bias and variance. The main technical
difﬁculty is that the bandwidth t varies over time and thus points at varying distances are included
in each estimate. Nevertheless  if ni is the number of steps in Phase i  we will see that both average
squared bias and variance can be bounded by roughly n
Finally  the algorithm ensures that the guess di is always an under-estimate of the unknown dimen-
sion d  as proven in Lemma 3 (proof in the supplemental appendix)  so integrating over all phases
yields an adaptive bound on the average error. We assume throughout this section that the space
(X   ρ) has dimension d for some ˆCρ (see Def. 2).
Lemma 3. Suppose the algorithm parameter ˆC ≥ ˆCρ. The following invariants hold throughout
the procedure for all phases i ≥ 1 of Algorithm 1:

−2/(2+di)
i

.

• i ≤ di ≤ d.
• For any t ∈ Phase i we have |Xi| ≤ ˆC 4di

−di
t

.

Lemma 4 (Bound on single phase). Suppose the algorithm parameter ˆC ≥ ˆCρ. Consider Phase
i ≥ 1 and suppose this phase lasts ni steps. Let Eni denote expectation relative to the uniform
choice of X out of {xt : t ∈ Phase i}. We have the following bound:

|fn(X) − f (X)|2 ≤(cid:16) ˆC4d∆2

Y + 12λ2(cid:17)

− 2
i

2+d

.

n

E
ni

E
Y n

Proof. Let Xi(X) denote the center closest to X in Xi. Suppose Xi(X) = xs  s ∈ [n]  we let nxs
denote the number of points assigned to the center xs. We use the notation xt → xs to say that xt is
assigned to center xs.
(cid:80)
First ﬁx X ∈ {xt : t ∈ Phase i} and let xs = Xi(xt). Deﬁne ˜fn(X) ≡ EY n fn(X) =

f (xt). We proceed with the following standard bias-variance decomposition

1
nxs

xt→xs

E
Y n

|fn(X) − f (X)|2 = E

(1)
Let X = xr  r ≥ s. We ﬁrst bound the bias term. Using the Lipschitz property of f  and Jensen’s
inequality  we have

Y n

+

.

(cid:12)(cid:12)(cid:12)fn(X) − ˜fn(X)
(cid:12)(cid:12)(cid:12)2
(cid:33)2

(cid:12)(cid:12)(cid:12) ˜fn(X) − f (X)
(cid:12)(cid:12)(cid:12)2
(cid:88)

λ2ρ (xr  xt)2

(cid:12)(cid:12)(cid:12) ˜fn(X) − f (X)
(cid:12)(cid:12)(cid:12)2 ≤

(cid:32)

xt→xs

λρ (xr  xt)

≤ 1
nxs

1
nxs
≤ 2λ2
nxs

(cid:88)
ρ (xr  xs)2 + ρ (xs  xt)2(cid:17) ≤ 2λ2
(cid:16)
(cid:88)
(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)fn(X) − ˜fn(X)
(cid:88)

EY n |Yt − f (xt)|2

xt→xs

xt→xs

nxs

=

xt→xs

n2
xs

E
Y n

≤ ∆2
Y
nxs

.

(cid:88)

(cid:0)2

xt→xs

(cid:1) .

r + 2
t

The variance term is easily bounded as follows:

6

Now take the expectation over X ∼ U {xt : t ∈ Phase i}. We have:

E
ni

E
Y n

|fn(X) − f (X)|2 =

E n E

|fn(X) − f (X)|2 · 1{X → xs}

xs∈Xi

xs∈Xi

(cid:88)
(cid:88)
(cid:88)
1
ni
xs∈Xi
Y · |Xi|
∆2
ni
(cid:88)

≤ 1
ni

=

=

(cid:32)

Y n

(cid:88)

xr→xs

∆2

Y +

+

4λ2
ni

2λ2
ni

1
nxs

∆2
Y
nxs

2λ2
nxs

+

xs∈Xi

(cid:88)
(cid:88)
(cid:88)
(cid:90) ni

xs∈Xi

xt→xs

(cid:1)(cid:33)
(cid:0)2

xt→xs

(cid:88)
(cid:0)2
(cid:88)

r + 2
t

(cid:88)
xt→xs
xr→xs
Y · |Xi|
∆2
ni

2
t =

(cid:1)

r + 2
t

+

4λ2
ni

(cid:88)

t∈Phase i

2
t .

To bound the last term  we have(cid:88)

2
t =

t∈Phase i

ti∈[ni]

− 2
2+di ≤

t

0

− 2
2+di dτ ≤ 3n

τ

1− 2
2+di
i

.

Combine with the previous derivation and with both statements of Lemma 3 to get

E
ni

E
Y n

|fn(X) − f (X)|2 ≤ ∆2

Y · |Xi|
ni

+ 12λ2n

− 2
2+di
i

Y + 12λ2(cid:17)

− 2
i

2+d

n

.

≤(cid:16) ˆC 4d ∆2
Y + 12λ2(cid:17)

n− 2

2+d .

(cid:16) ˆC 4d∆2

Corollary 1 (Combined phases). Suppose the algorithm parameter ˆC ≥ ˆCρ  then we have

E n E

Y n

|fn(X) − f (X)|2 ≤ 2

Proof. Let I denote the number of phases up to time n. We will decompose the expectation E n in
terms of the various phases i ∈ [I] and apply Lemma 4. Let B (cid:44) ˆC 4d ∆2

Y + 12λ2. We have:

I(cid:88)

(cid:32) I(cid:88)

(cid:33) d

2+d

E n E

Y n

|fn(X) − f (X)|2 ≤ B

= B

− 2
i

2+d

ni
n

n

(cid:17) d

2+d

= B

= B · I

d

n

2+d

i ≤ B

1
I
I
n
2+d ≤ B · d
2+d n− 2

i=1

2

ni
I
n
I
2+d n− 2

i=1

2+d  

2

I(cid:88)
(cid:16) n

i=1
I
n

I

where in the second inequality we use Jensen’s inequality  and in the last inequality Lemma 3.

4.2 Bound on L2 Error

We need the following lemma  whose proof is in the supplemental appendix  which bounds the
probability that a ρ-ball of a given radius contains a sample from xn. This will then allow us to
bound the bias induced by transforming a solution for the adversarial setting to a solution for the
stochastic setting.
Lemma 5. Suppose (X   ρ  µ) has metric measure dimension d. Let µ be a distribution on X and
let µn denote an empirical distribution on an i.i.d sample xn from µ. For  > 1/n  let B denote the
class of ρ-balls centered on X of radius . There exists C depending on d such that the following
holds. Let 0 < δ < 1  Deﬁne αn δ = C (d log n + log(1/δ)). Then  with probability at least 1 − δ 
for all B ∈ B satisfying µ(B) ≥ αn δ/n we have µn(B) > 1/n.
We are now ready to prove Theorem 1.
Proof of Theorem 1. Fix δ = 1/n and deﬁne αn δ as in Lemma 5. Pick  = (αn δ/C1n)1/d ≥ 1/n 
where C1 is such that every B ∈ B has mass at least C1d. Since for every B ∈ B  µ(B) ≥
C1−d ≥ αn δ/n  we have by Lemma 5  that with probability at least 1 − δ  all B ∈ B contain a
point from xn. In other words  the event E that xn forms an -cover of X is (1 − δ)-likely.

7

Suppose xt is the closest point in xn to x ∈ X . We write x → xt. Then  under E  we have 
(cid:107)f (x) − f (xt)(cid:107) ≤ λ. We therefore have by Fubini’s theorem

|fn(X) − f (X)|2 · (1{E} + 1(cid:8) ¯E(cid:9))

E

Y n|xn
2µ(x : x → xt) E
Y n|xn

|fn(xt) − f (xt)|2 + 2λ22 + δ∆2

Y

E

xn Y n

(cid:107)fn − f(cid:107)2

2 µ = E

xn

≤ E

xn

≤ E

xn

E
X

n(cid:88)
n(cid:88)

t=1

2C2d E
Y n|xn

|fn(xt) − f (xt)|2 + 2λ22 + δ∆2

Y

≤ 2C2αn δ

t=1

C1

E n E

Y n

|fn(xt) − f (xt)|2 + 2λ22 + δ∆2
Y  

sup
xn

where in the ﬁrst inequality we break the integration over the Voronoi partition of X deﬁned by the
points in xn  and introduce f (xt); the second inequality uses {x : x → xt} ⊂ B(xt  ) under E.

4.3 Lower-bound

to show that

Let’s consider ﬁrst the case of a ﬁxed n. The idea behind the proof is as follows: for µ ﬁxed  we
have to come up with a class F of functions which vary considerably on the space X . To this end we
discretize X into as many cells as possible  and let any f ∈ F potentially change sign from one cell
to the other. The larger the dimension d the more we can discretize the space and the more complex
F  subject to a Lipschitz constraint. The problem of picking the right f can thus be reduced to that
of classiﬁcation  since the learner has to discover the sign of f on sufﬁciently many cells.
In order to handle many data sizes n simultaneously  we borrow from the idea above.
the lower-bound holds for a subsequence {ni} simultaneously.
Say we want
Then we reserve a subset of the space X for each n1  n2  . . .  and discretize each sub-
The functions in F have to then vary sufﬁciently in each sub-
set according to ni.
set of the space X according to the corresponding ni.
This is illustrated in Figure 3.
We can then apply the same idea of reduction to classi-
ﬁcation for each nt separately. This sort of idea appears
in [7] where µ is uniform on the Euclidean cube  where
they use the regularity of the cube to set up the right se-
quence of discretizations over subsets of the cube. The
main technicality in our result is that we work with a gen-
eral space without much regularity. The lack of regularity
makes it unclear a priori how to divide such a space into
subsets of the proper size for each ni.
Last  we have to ensure that the functions f ∈ F resulting
from our discretization of a general metric space X are in fact Lipschitz. For this  we extend some
of the ideas from [9] which handles the case of a ﬁxed n. For lack of space  the complete proof is in
the extended version of the paper.

Figure 3: Lower bound proof idea.

5 Conclusions

We presented an efﬁcient and nearly minimax optimal approach to nonparametric regression in a
streaming setting. The streaming setting is gaining more attention as modern data sizes are getting
larger  and as data is being acquired online in many applications.
The main insights behind the approach presented extend to other nonparametric methods  and are
likely to extend to settings of a more adversrial nature. We left open the question of optimal adap-
tation to the smoothness of the unknown function  while we effciently solve the equally or more
important question of adapting to the the unknown dimension of the data  which generally has a
stronger effect on the convergence rate.

8

References
[1] Y. Ben-Haim and E. Tom-Tov. A streaming parallel decision tree algorithm. Journal of Ma-

chine Learning Research  11:849–872  2010.

[2] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbors. ICML  2006.
[3] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  New York  NY  USA  2006.

[4] K. Clarkson. Nearest-neighbor searching and metric space dimensions. Nearest-Neighbor

Methods for Learning and Vision: Theory and Practice  2005.

[5] P. Domingos and G. Hulten. Mining high-speed data streams. In Proceedings of the 6th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 71–80 
2000.

[6] H. Gu and J. Lafferty. Sequential nonparametric regression. ICML  2012.
[7] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution Free Theory of Nonparametric

Regression. Springer  New York  NY  2002.

[8] A. Kalai and S. Vempala. Efﬁcient algorithms for universal portfolios. Journal of Machine

Learning Research  3:423–440  2002.

[9] S. Kpotufe. k-NN Regression Adapts to Local Intrinsic Dimension. NIPS  2011.
[10] R. Krauthgamer and J. R. Lee. Navigating nets: simple algorithms for proximity search. In
Proceedings of the ﬁfteenth annual ACM-SIAM symposium on Discrete algorithms  SODA ’04 
pages 798–807  Philadelphia  PA  USA  2004. Society for Industrial and Applied Mathematics.
[11] B. Pfahringer  G. Holmes  and R. Kirkby. Handling numeric attributes in hoeffding trees. In
Advances in Knowledge Discovery and Data Mining: Proceedings of the 12th Paciﬁc-Asia
Conference (PAKDD)  volume 5012  pages 296–307. Springer  2008.

[12] S. Schaal and C. Atkeson. Robot Juggling: An Implementation of Memory-based Learning.

Control Systems Magazine  IEEE  1994.

[13] C. J. Stone. Optimal rates of convergence for non-parametric estimators. Ann. Statist.  8:1348–

1360  1980.

[14] C. J. Stone. Optimal global rates of convergence for non-parametric estimators. Ann. Statist. 

10:1340–1353  1982.

[15] M. A. Taddy  R. B. Gramacy  and N. G. Polson. Dynamic trees for learning and design. Journal

of the American Statistical Association  106(493)  2011.

[16] S. Vijayakumar and S. Schaal. Locally weighted projection regression: An O(n) algorithm for
incremental real time learning in high dimensional space. In in Proceedings of the Seventeenth
International Conference on Machine Learning (ICML)  pages 1079–1086  2000.

9

,Samory Kpotufe
Francesco Orabona