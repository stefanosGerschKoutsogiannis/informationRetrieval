2016,Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs,We define and study the problem of predicting the solution to a linear program (LP) given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function  that has been studied under the name “Learning from Revealed Preferences". We give mistake bound learning algorithms in two settings: in the first  the objective of the LP is known to the learner but there is an arbitrary  fixed set of constraints which are unknown. Each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the LP given the union of the known and unknown constraints. This models the problem of predicting the behavior of a rational agent whose goals are known  but whose resources are unknown. In the second setting  the objective of the LP is unknown  and changing in a controlled way. The constraints of the LP may also change every day  but are known. An example is given by a set of constraints and partial information about the objective  and the task of the learner is again to predict the optimal solution of the partially known LP.,Learning from Rational Behavior:

Predicting Solutions to Unknown Linear Programs

Shahin Jabbari  Ryan Rogers  Aaron Roth  Zhiwei Steven Wu

University of Pennsylvania

{jabbari@cis  ryrogers@sas  aaroth@cis  wuzhiwei@cis}.upenn.edu

Abstract

We deﬁne and study the problem of predicting the solution to a linear program (LP)
given only partial information about its objective and constraints. This generalizes
the problem of learning to predict the purchasing behavior of a rational agent who
has an unknown objective function  that has been studied under the name “Learning
from Revealed Preferences". We give mistake bound learning algorithms in two
settings: in the ﬁrst  the objective of the LP is known to the learner but there is an
arbitrary  ﬁxed set of constraints which are unknown. Each example is deﬁned by
an additional known constraint and the goal of the learner is to predict the optimal
solution of the LP given the union of the known and unknown constraints. This
models the problem of predicting the behavior of a rational agent whose goals
are known  but whose resources are unknown. In the second setting  the objective
of the LP is unknown  and changing in a controlled way. The constraints of the
LP may also change every day  but are known. An example is given by a set of
constraints and partial information about the objective  and the task of the learner
is again to predict the optimal solution of the partially known LP.

1

Introduction

We initiate the systematic study of a general class of multi-dimensional prediction problems  where
the learner wishes to predict the solution to an unknown linear program (LP)  given some partial
information about either the set of constraints or the objective. In the special case in which there is a
single known constraint that is changing and the objective that is unknown and ﬁxed  this problem
has been studied under the name learning from revealed preferences [1  2  3  16] and captures the
following scenario: a buyer  with an unknown linear utility function over d goods u : Rd ! R
deﬁned as u(x) = c · x faces a purchasing decision every day. On day t  she observes a set of prices
0 and buys the bundle of goods that maximizes her unknown utility  subject to a budget b:
pt 2 Rd

x(t) = argmax

x

c · x

such that pt · x  b

In this problem  the goal of the learner is to predict the bundle that the buyer will buy  given the
prices that she faces. Each example at day t is speciﬁed by the vector pt 2 Rd
0 (which ﬁxes the
constraint)  and the goal is to accurately predict the purchased bundle x(t) 2 [0  1]d that is the result
of optimizing the unknown linear objective.
It is also natural to consider the class of problems in which the goal is to predict the outcome to a LP
broadly e.g. suppose the objective c · x is known but there is an unknown set of constraints Ax  b.
An instance is again speciﬁed by a changing known constraint (pt  bt) and the goal is to predict:

x(t) = argmax

x

c · x

such that Ax  b and pt · x  bt.

(1)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

This models the problem of predicting the behavior of an agent whose goals are known  but whose
resource constraints are unknown.
Another natural generalization is the problem in which the objective is unknown  and may vary in a
speciﬁed way across examples  and in which there may also be multiple arbitrary known constraints
which vary across examples. Speciﬁcally  suppose that there are n distinct  unknown linear objective
functions v1  . . .   vn. An instance on day t is speciﬁed by a subset of the unknown objective
functions  St ✓ [n] := {1  . . .   n} and a convex feasible region P t  and the goal is to predict:

such that x 2P t.

(2)

x(t) = argmax

x Xi2St

vi · x

When the changing feasible regions P t correspond simply to varying prices as in the revealed
preferences problem  this models a setting in which at different times  purchasing decisions are made
by different members of an organization  with heterogeneous preferences — but are still bound by
an organization-wide budget. The learner’s problem is  given the subset of decision makers and the
prices at day t  to predict which bundle they will purchase. This generalizes some of the preference
learning problems recently studied by Blum et al [6]. Of course  in this generality  we may also
consider a richer set of changing constraints which represent things beyond prices and budgets.
In all of the settings we study  the problem can be viewed as the task of predicting the behavior of a
rational decision maker  who always chooses the action that maximizes her objective function subject
to a set of constraints. Some part of her optimization problem is unknown  and the goal is to learn 
through observing her behavior  that unknown part of her optimization problem sufﬁciently so that
we may reliably predict her future actions.

1.1 Our Results
We study both variants of the problem (see below) in the strong mistake bound model of learning
[13]. In this model  the learner encounters an arbitrary adversarially chosen sequence of examples
online and must make a prediction for the optimal solution in each example before seeing future
examples. Whenever the learner’s prediction is incorrect  the learner encounters a mistake  and
the goal is to prove an upper bound on the number of mistakes the learner can make  in the worst
case over the sequence of examples. Mistake bound learnability is stronger than (and implies) PAC
learnability [15].

Known Objective and Unknown Constraints We ﬁrst study this problem under the assumption
that there is a uniform upper bound on the number of bits of precision used to specify the constraint
deﬁning each example. In this case  we show that there is a learning algorithm with both running time
and mistake bound linear in the number of edges of the polytope formed by the unknown constraint
matrix Ax  b. We note that this is always polynomial in the dimension d when the number of
unknown constraints is at most d + O(1). (In the supplementary material  we show that by allowing
the learner to run in time exponential in d  we can give a mistake bound that is always linear in
the dimension and the number of rows of A  but we leave as an open question whether or not this
mistake bound can be achieved by an efﬁcient algorithm.) We then show that our bounded precision
assumption is necessary — i.e. we show that when the precision to which constraints are speciﬁed
need not be uniformly upper bounded  then no algorithm for this problem in dimension d  3 can
have a ﬁnite mistake bound.
This lower bound motivates us to study a PAC style variant of the problem  where the examples are
not chosen in an adversarial manner  but instead are drawn independently at random from an arbitrary
unknown distribution. In this setting  we show that even if the constraints can be speciﬁed to arbitrary
(even inﬁnite) precision  there is a learner that requires sample complexity only linear in the number
of edges of the unknown constraint polytope. This learner can be implemented efﬁciently when the
constraints are speciﬁed with ﬁnite precision.

Known Constraints and Unknown Objective For the variant of the problem in which the objec-
tive is unknown and changing and the constraints are known but changing  we give an algorithm
that has a mistake bound and running time polynomial in the dimension d. Our algorithm uses the
Ellipsoid algorithm to learn the coefﬁcients of the unknown objective by implementing a separation
oracle that generates separating hyperplanes given examples on which our algorithm made a mistake.

2

We leave the study of either of our problems under natural relaxations (e.g. under a less demanding
loss function) and whether it is possible to substantially improve our results in these relaxations as an
interesting open problem.

1.2 Related Work

Beigman and Vohra [3] were the ﬁrst to study revealed preference problems (RPP) as a learning
problems and to relate them to multi-dimensional classiﬁcation. They derived sample complexity
bounds for such problems by computing the fat shattering dimension of the class of target utility
functions  and showed that the set of Lipschitz-continuous valuation functions had ﬁnite fat-shattering
dimension. Zadimoghaddam and Roth [16] gave efﬁcient algorithms with polynomial sample
complexity for PAC learning of the RPP over the class of linear (and piecewise linear) utility
functions. Balcan et al. [2] showed a connection between RPP and the structured prediction problem
of learning d-dimensional linear classes [7  8  12]  and use an efﬁcient variant of the compression
techniques given by Daniely and Shalev-Shwartz [9] to give efﬁcient PAC algorithms with optimal
sample complexity for various classes of economically meaningful utility functions. Amin et al. [1]
study the RPP for linear valuation functions in the mistake bound model  and in the query model
in which the learner gets to set prices and wishes to maximize proﬁt. Roth et al. [14] also study
the query model of learning and give results for strongly concave objective functions  leveraging an
algorithm of Belloni et al. [4] for bandit convex optimization with adversarial noise.
All of the works above focus on the setting of predicting the optimizer of a ﬁxed unknown objective
function  together with a single known  changing constraint representing prices. This is the primary
point of departure for our work — we give algorithms for the more general settings of predicting the
optimizer of a LP when there may be many unknown constraints  or when the unknown objective
function is changing. Finally  the literature on preference learning (see e.g. [10]) has similar goals 
but is technically quite distinct: the canonical problem in preference learning is to learn a ranking on
distinct elements. In contrast  the problem we consider here is to predict the outcome of a continuous
optimization problem as a function of varying constraints.

2 Model and Preliminaries

We ﬁrst formally deﬁne the geometric notions used throughout this paper. A hyperplane and a
halfspace in Rd are the set of points satisfying the linear equation a1x1 + . . . adxd = b and the
linear inequality a1x1 + . . . + adxd  b for a set of ais respectively  assuming that not all ai’s are
simultaneously zero. A set of hyperplanes are linearly independent if the normal vectors to the
hyperplanes are linearly independent. A polytope (denoted by P✓ Rd) is the bounded intersection
of ﬁnitely many halfspaces  written as P = {x | Ax  b}. An edge-space e of a polytope P is a one
dimensional subspace that is the intersection of d  1 linearly independent hyperplanes of P  and an
edge is the intersection between an edge-space e and the polytope P.We denote the set of edges of
polytope P by EP. A vertex of P is a point where d linearly independent hyperplanes of P intersect.
Equivalently  P can be written as the convex hull of its vertices V denoted by Conv(V ). Finally  we
deﬁne a set of points to be collinear if there exists a line that contains all the points in the set.
We study an online prediction problem with the goal of predicting the optimal solution of a changing
LP whose parameters are only partially known. Formally  in each day t = 1  2  . . . an adversary
chooses a LP speciﬁed by a polytope P (t) (a set of linear inequalities) and coefﬁcients c(t) 2 Rd
of the linear objective function. The learner’s goal is to predict the solution x(t) where x(t) =
argmaxx2P (t) c(t) · x. After making the prediction ˆx(t)  the learner observes the optimal x(t) and
learns whether she has made a mistake (ˆx(t) 6= x(t)). The mistake bound is deﬁned as follows.
Deﬁnition 1. Given a LP with feasible polytope P and objective function c  let (t) denote the
parameters of the LP that are revealed to the learner on day t. A learning algorithm A takes as
input the sequence {(t)}t  the known parameters of an adaptively chosen sequence {(P (t)  c(t))}t
of LPs and outputs a sequence of predictions {ˆx(t)}t. We say that A has mistake bound M if
max{(P (t) c(t))}t⌃1t=11⇥ˆx(t) 6= x(t)⇤  M  where x(t) = argmaxx2P (t) c(t) · x on day t.
We consider two different instances of the problem described above. First  in Section 3  we study
the problem given in (1) in which c(t) = c is ﬁxed and known to the learner but the polytope P (t) =

3

P\N (t) consists of an unknown ﬁxed polytope P and a new constraint N (t) = {x | p(t) · x  b(t)}
which is revealed to the learner on day t i.e. (t) = (N (t)  c). We refer to this as the Known Objective
problem. Then  in Section 4  we study the problem in which the polytope P (t) is changing and known
but the objective function c(t) =Pi2S(t) vi is unknown and changing as in (2) where the set S(t) is
known i.e. (t) = (P (t)  S(t)). We refer to this as the Known Constraints problem.
In order for our prediction problem to be well deﬁned  we make Assumption 1 about the observed
solution x(t) in each day. Assumption 1 guarantees that each solution is on a vertex of P (t).
Assumption 1. The optimal solution to the LP: maxx2P (t) c(t) · x is unique for all t.
3 The Known Objective Problem

In this section  we focus on the Known Objective Problem where the coefﬁcients of the objective
function c are ﬁxed and known to the learner but the feasible region P (t) on day t is unknown and
changing. In particular  P (t) is the intersection of a ﬁxed and unknown polytope P = {x | Ax 
b  A ✓ Rm⇥d} and a known halfspace N (t) = {x | p(t) · x  b(t)} i.e. P (t) = P\N (t).
Throughout this section we make the following assumptions. First  we assume w.l.o.g. (up to scaling)
that the points in P have `1-norm bounded by 1.
Assumption 2. The unknown polytope P lies inside the unit `1-ball i.e. P✓{ x | ||x||1  1}.
We also assume that the coordinates of the vertices in P can be written with ﬁnite precision (this is
implied if the halfspaces deﬁning P can be described with ﬁnite precision). 1
Assumption 3. The coordinates of each vertex of P can be written with N bits of precision.
We show in Section 3.3 that Assumption 3 is necessary — without any upper bound on precision 
there is no algorithm with a ﬁnite mistake bound. Next  we make some non-degeneracy assumptions
on polytopes P and P (t)  respectively. We require these assumptions to hold on each day.
Assumption 4. Any subset of d  1 rows of A have rank d  1 where A is the constraint matrix in
P = {x | Ax  b}.
Assumption 5. Each vertex of P (t) is the intersection of exactly d-hyperplanes of P (t).
The rest of this section is organized as follows. We present LearnEdge for the Known Objective
Problem and analyze its mistake bound in Sections 3.1 and 3.2  respectively. Then in Section 3.3 
we prove the necessity of Assumption 3 to get a ﬁnite mistake bound. Finally in Section 3.4  we
present the LearnHull in a PAC style setting where the new constraint each day is drawn i.i.d. from
an unknown distribution  rather than selected adversarially.

3.1 LearnEdge Algorithm
In this section we introduce LearnEdge and show in Theorem 1 that the number of mistakes of
LearnEdge depends linearly on the number of edges EP and the precision parameter N and only
logarithmically on the dimension d. We defer all the missing proofs to the supplementary material.
Theorem 1. The number of mistakes and per day running time of LearnEdge in the Known Objective
Problem are O(|EP|N log(d)) and poly(m  d |EP|) respectively when A ✓ Rm⇥d.
At a high level  LearnEdge maintains a set of prediction information I (t) about the prediction history
up to day t  and makes prediction in each day based on I (t) and a set of prediction rules (P.1  P.4).
After making a mistake  LearnEdge updates the information with a set of update rules (U.1  U.4).
Prediction Information It is natural to ask “What information is useful for prediction?" Lemma 2
establishes the importance of the set of edges EP by showing that all the observed solutions will be
on an element of EP.

1Lemma 6.2.4 from Grotschel et al. [11] states that if each constraint in P✓ Rd has encoding length at most
N then each vertex of P has encoding length at most 4d2N. Typically the ﬁnite precision assumption is made
on the constraints of the LP. However  since this assumption implies that the vertices can be described with ﬁnite
precision  for simplicity  we make our assumption directly on the vertices.

4

Lemma 2. On any day t  the observed solution x(t) lies on an edge in EP.
In the proof of Lemma 2 we also show that when x(t) does not bind the new constraint N (t)  then
x(t) is the solution for the underlying LP: argmaxx2P c · x.
Corollary 1. If x(t) 2{ x | p(t)x < b(t)} then x(t) = x⇤ ⌘ argmaxx2P c · x.
We then show how an edge-space e of P can be recovered after seeing 3 collinear observed solutions.
Lemma 3. Let x  y  z be 3 distinct collinear points on edges of P. Then they are all on the same
edge of P and the 1-dimensional subspace containing them is an edge-space of P.
Given the relation between observed solutions and edges  the information I(t) is stored as follows:

0

Me

1

Me

}}

}}}

1

Qe

1

Ye

0

Ye

0

Qe

Fe

Figure 1: Regions on an edge-space e: feasible
region Fe (blue)  questionable intervals Q0
e and
e and
e and M 1
Q1
infeasible regions Y 0

e (green) with their mid-points M 0

e (dashed).

e and Y 1

I.1 (Observed Solutions) LearnEdge keeps track of the set of observed solutions that were
ˆx(⌧ ) 6= x(⌧ )} and also the solution for
predicted incorrectly so far X (t) = {x(⌧ ) : ⌧  t
the underlying unknown polytope x⇤ ⌘ argmaxx2P c · x if it is observed.
I.2 (Edges) LearnEdge keeps track of the set of edge-spaces E(t) given by any 3 collinear
points in X (t). For each e 2 E(t)  it also maintains the regions on e that are certainly
feasible or infeasible. The remaining parts of e called the questionable region is where
LearnEdge cannot classify as infeasible or feasible with certainty (see Figure 1). Formally 

1. (Feasible Interval) The feasible interval Fe is an interval along e that is identiﬁed to be on

e and Y 1

the boundary of P. More formally  Fe = Conv(X (t) \ e).
2. (Infeasible Region) The infeasible region Ye = Y 0
e is the union of two disjoint
e [ Y 1
e that are identiﬁed to be outside of P. By Assumption 2  we initialize
intervals Y 0
the infeasible region Ye to {x 2 e |k xk1 > 1} for all e.
e on e is the union of two
disjoint questionable intervals along e. Formally  Qe = e \ (Fe [ Ye). The points in Qe
cannot be certiﬁed to be either inside or outside of P by LearnEdge.
e denote the midpoint of Qi
e.

3. (Questionable Region) The questionable region Qe = Q0

4. (Midpoints in Qe) For each questionable interval Qi

e [ Q1

e  let M i

We add the superscript (t) to show the dependence of these quantities on days. Furthermore  we
e .

eliminate the subscript e when taking the union over all elements in E(t)  e.g. F (t) =Se2E(t) F (t)
So the information I(t) can be written as follows: I(t) =X (t)  E(t)  F (t)  Y (t)  Q(t)  M (t) .
Prediction Rules We now focus on the prediction rules of LearnEdge. On day t  let eN (t) = {x |
p(t) · x = b(t)} be the hyperplane speciﬁed by the additional constraint N (t). If x(t) /2 eN (t)  then
x(t) = x⇤ by Corollary 1. So whenever the algorithm observes x⇤  it will store x⇤ and predict it in
the future days when x⇤ 2N (t). This is case P.1. So in the remaining cases we know x⇤ /2N (t).
The analysis of Lemma 2 shows that x(t) must be in the intersection between eN (t) and the edges EP 
c · x. Hence  LearnEdge can restrict its prediction to the following
so x(t) = argmaxx2eN (t)\EP
candidate set: Cand(t) = {(E(t) [ X (t)) \ ¯E(t)}\ eN (t) where ¯E(t) = {e 2 E(t) | e ✓ eN (t)}. As
Lemma 4. Let e be an edge-space of P such that e ✓ eN (t)  then x(t) 62 e.
However  Cand(t) can be empty or only contain points in the infeasible regions of the edge-spaces. If
so  then there is simply not enough information to predict a feasible point in P. Hence  LearnEdge
predicts an arbitrary point outside of Cand(t). This is case P.2.

we show in Lemma 4  x(t) will not be in ¯E(t)  so it is safe to remove ¯E(t) from Cand(t).

5

two mid-points (M 0

Otherwise Cand(t) contains points from the feasible and questionable regions of the edge-spaces.
LearnEdge predicts from a subset of Cand(t) called the extended feasible region Ext(t) instead of
directly predicting from Cand(t). Ext(t) contains the whole feasible region and only parts of the
questionable region on all the edge-spaces in E(t) \ ¯E(t). We will show later that this guarantees
LearnEdge makes progress in learning the true feasible region on some edge-space upon making a
mistake. More formally  Ext(t) is the intersection of eN (t) with the union of intervals between the
e )(t) on every edge-space e 2 E(t) \ ¯E(t) and all points in X (t):
Ext(t) =X (t) [[e2E(t)\ ¯E(t)Conv(M 0
In P.3  if Ext(t) 6= ; then LearnEdge predicts the point with the highest objective value in Ext(t).
Finally  if Ext(t) = ;  then we know eN (t) only intersects within the questionable regions of the

learned edge-spaces. In this case  LearnEdge predicts the intersection point with the lowest objective
value  which corresponds to P.4. Although it might seem counter-intuitive to predict the point with the
lowest objective value  this guarantees that LearnEdge makes progress in learning the true feasible
region on some edge-space upon making a mistake. The prediction rules are summarized as follows:

e )(t) \ eN (t).

e )(t) and (M 1

e )(t)  (M 1

P.1 First  if x⇤ is observed and x⇤ 2N (t)  then predict ˆx(t) x⇤;
P.2 Else if Cand = ; or Cand(t) ✓Se2E(t) Y (t)
P.3 Else if Ext(t) 6= ;  then predict ˆx(t) = argmaxx2Ext(t) c · x;
P.4 Else  predict ˆx(t) = argminx2Cand(t) c · x.

e

  then predict any point outside Cand(t);

Update Rules Next we describe how LearnEdge updates its information. Upon making a mistake 
LearnEdge adds x(t) to the set of previously observed solutions X (t) i.e. X (t+1) X (t) [{ x(t)}.
Then it performs one of the following four mutually exclusive update rules (U.1-U.4) in order.
U.1 If x(t) /2 eN (t)  then LearnEdge records x(t) as the unconstrained optimal solution x⇤.

U.2 Then if x(t) is not on any learned edge-space in E(t)  LearnEdge will try to learn a new
edge-space by checking the collinearity of x(t) and any couple of points in X (t). So after
this update LearnEdge might recover a new edge-space of the polytope.

If the previous updates were not invoked  then x(t) was on some learned edge-space e. LearnEdge
then compares the objective values of ˆx(t) and x(t) (we know c · ˆx(t) 6= c · x(t) by Assumption 1):
U.3 If c · ˆx(t) > c · x(t)  then ˆx(t) must be infeasible and LearnEdge then updates the question-
U.4 If c · ˆx(t) < c · x(t) then x(t) was outside of the extended feasible region of e. LearnEdge

able and infeasible regions for e.

then updates the questionable region and feasible interval on e.

In both of U.3 and U.4  LearnEdge will shrink some questionable interval substantially till the
interval has length less than 2N in which case Assumption 3 implies that the interval contains no
points. So LearnEdge can update the adjacent feasible region and infeasible interval accordingly.

3.2 Analysis of LearnEdge
Whenever LearnEdge makes a mistake  one of the update rules U.1 - U.4 is invoked. So the number
of mistakes of LearnEdge is bounded by the number of times each update rule is invoked. The
mistake bound of LearnEdge in Theorem 1 is hence the sum of mistakes bounds in Lemmas 5-7.
Lemma 5. Update U.1 is invoked at most 1 time.
Lemma 6. Update U.2 is invoked at most 3|EP| times. 2
Lemma 7. Updates U.3 and U.4 are invoked at most O(|EP|N log(d)) times.

2The dependency on |EP| can be improved by replacing it with the set of edges of P on which an optimal

solution is observed. This applies to all the dependencies on |EP| in our bounds.

6

3.3 Necessity of the Precision Bound
We show the necessity of Assumption 3 by showing that the dependence on the precision parameter
N in our mistake bound is tight. We show that subject to Assumption 3  there exist a polytope and a
sequence of additional constraints such that any learning algorithm will make ⌦(N ) mistakes. This
implies that without any upper bound on precision  it is impossible to learn with ﬁnite mistakes.
Theorem 8. For any learning algorithm A in the Known Objective Problem and any d  3  there
exists a polytope P and a sequence of additional constraints {N (t)}t such that the number of mistakes
made by A is at least ⌦(N ). 3
3.4 Stochastic Setting
Given the lower bound in Theorem 8  we ask “In what settings we can still learn without an upper
bound on the precision to which constraints are speciﬁed?” The lower bound implies we must
abandon the adversarial setting so we consider a PAC style variant. In this variant  the additional
constraint at each day t is drawn i.i.d. from some ﬁxed but unknown distribution D over Rd ⇥ R such
that each point (p  b) drawn from D corresponds to the halfspace N = {x | p · x  b}. We make no
assumption on the form of D and require our bounds to hold in the worst case over all choices of D.
We describe LearnHull an algorithm based on the following high level idea: LearnHull keeps track
of the convex hull C(t1) of all the solutions observed up to day t. LearnHull then behaves as if this
convex hull is the entire feasible region. So at day t  given the constraint N (t) = {x | p(t) · x  b(t)} 
LearnHull predicts ˆx(t) where ˆx(t) = argmaxx2C(t1)\N (t) c · x.
LearnHull’s hypothetical feasible region is therefore always a subset of the true feasible region –
i.e. it can never make a mistake because its prediction was infeasible  but only because its prediction
was sub-optimal. Hence  whenever LearnHull makes a mistake  it must have observed a point that
expands the convex hull. Hence  whenever it fails to predict x(t)  LearnHull will enlarge its feasible
region by adding the point x(t) to the convex hull: C(t) Conv(C(t1) [{ x(t)})  otherwise it
will simply set C(t) C (t1) for the next day. We show that the expected number of mistakes of
LearnHull over T days is linear in the number of edges of P and only logarithmic in T . 4
Theorem 9. For any T > 0 and any constraint distribution D  the expected number of mistakes of
LearnHull after T days is bounded by O (|EP| log(T )).
To prove Theorem 9  ﬁrst in Lemma 10 we bound the probability that the solution observed at day t
falls outside of the convex hull of the previously observed solutions. This is the only event that can
cause LearnHull to make a mistake. In Lemma 10  we abstract away the fact that the point observed
at each day is the solution to some optimization problem.
Lemma 10. Let P be a polytope and D a distribution over points on EP. Let X = {x1  . . .   xt1} be
t 1 i.i.d. draws from D and xt an additional independent draw from D. Then Pr[xt 62 Conv(X)] 
2|EP|/t where the probability is taken over the draws of points x1  . . .   xt from D.
Finally in Theorem 11 we convert the bound on the expected number of mistakes of LearnHull in
Theorem 9 to a high probability bound. 5
Theorem 11. There exists a deterministic procedure such that after T = O (|EP| log (1/)) days 
the probability (over the randomness of the additional constraint) that the procedure makes a mistake
on day T + 1 is at most  for any  2 (0  1/2).
4 The Known Constraints Problem

We now consider the Known Constraints Problem in which the learner observes the changing
constraint polytope P (t) at each day  but does not know the changing objective function which we
3 We point out that the condition d  3 is necessary in the statement of Theorem 8 since there exists learning
algorithms for d = 1 and d = 2 with ﬁnite mistake bounds independent of N. See the supplementary material.
4LearnHull can be implemented efﬁciently in time poly(T  N  d) if all of the coefﬁcients in the unknown
constraints in P are represented in N bits. Note that given the observed solutions so far and a new point  a
separation oracle can be implemented in time poly(T  N  d) using a LP solver.

5LearnEdge fails to give any non-trivial mistake bound in the adversarial setting.

7

assume to be written as c(t) =Pi2S(t) vi  where {vi}i2[n] are ﬁxed but unknown. Given P (t) and
the subset S(t) ✓ [n]  the learner must make a prediction ˆx(t) on each day. Inspired by Bhaskar et
al. [5]  we use the Ellipsoid algorithm to learn the coefﬁcients {vi}i2[n]  and show that the mistake
bound of the resulting algorithm is bounded by the (polynomial) running time of the Ellipsoid. We
use V 2 Rd⇥n to denote the matrix whose columns are vi and make the following assumption on V .
Assumption 6. Each entry in V can be written with N bits of precision. Also w.l.o.g. ||V ||F  1.
Similar to Section 3 we assume the coordinates of P (t)’s vertices can be written with ﬁnite precision.6
Assumption 7. The coordinates of each vertex of P (t) can be written with N bits of precision.
We ﬁrst observe that the coefﬁcients of the objective function represent a point that is guaranteed to
lie in a region F (described below) which may be written as the intersection of possibly inﬁnitely
many halfspaces. Given a subset S ✓ [n] and a polytope P  let xS P denote the optimal solution to
the instance deﬁned by S and P. Informally  the halfspaces deﬁning F ensure that for any problem
instance deﬁned by arbitrary choices of S and P  the objective value of the optimal solution xS P
must be at least as high as the objective value of any feasible point in P. Since the convergence rate
of the Ellipsoid algorithm depends on the precision to which constraints are speciﬁed  we do not in
fact consider a hyperplane for every feasible solution but only for those solutions that are vertices of
the feasible polytope P. This is not a relaxation  since LPs always have vertex-optimal solutions.
We denote the set of all vertices of polytope P by vert(P)  and the set of polytopes P satisfying
Assumption 7 by . We then deﬁne F as follows:

F =(W = (w1  . . .   wn) 2 Rn⇥d |8 S ✓ [n] 8P 2  Xi2S

wi ·xS P  x  0 8x 2 vert(P))

The idea behind our LearnEllipsoid algorithm is that we will run a copy of the Ellipsoid algorithm
with variables w 2 Rd⇥n  as if we were solving the feasibility LP deﬁned by the constraints deﬁning
F. We will always predict according to the centroid of the ellipsoid maintained by the Ellipsoid
algorithm (i.e. its candidate solution). Whenever a mistake occurs  we are able to ﬁnd one of the
constraints that deﬁne F such that our prediction violates the constraint – exactly what is needed to
take a step in solving the feasibility LP. Since we know F is non-empty (at least the true objective
function V lies within it) we know that the LP we are solving is feasible. Given the polynomial
convergence time of the Ellipsoid algorithm  this gives a polynomial mistake bound for our algorithm.
The Ellipsoid algorithm will generate a sequence of ellipsoids with decreasing volume
such that each one contains feasible region F.
Given the ellipsoid E (t) at day t 
LearnEllipsoid uses the centroid of E (t) as its hypothesis for the objective function W (t) =
(w1)(t)  . . .   (wn)(t). Given the subset S(t) and polytope P (t)  LearnEllipsoid predicts
ˆx(t) 2 argmaxx2P (t){Pi2S(t)(wi)(t) · x}. When a mistake occurs  LearnEllipsoid ﬁnds the
hyperplane H(t) =W = (w1  . . .   wn) 2 Rn⇥d :Pi2S(t) wi · (x(t)  ˆx(t)) > 0 that separates
the centroid of the current ellipsoid (the current candidate objective) from F.
After the update  we use the Ellipsoid algorithm to compute the minimum-volume ellipsoid E (t+1)
that contains H(t) \E (t). On day t + 1  LearnEllipsoid sets W (t+1) to be the centroid of E (t+1).
We left the procedure used to solve the LP in the prediction rule of LearnEllipsoid unspeciﬁed. To
simplify our analysis  we use a speciﬁc LP solver to obtain a prediction ˆx(t) which is a vertex of P (t).
Theorem 12 (Theorem 6.4.12 and Remark 6.5.2 [11]). There exists a LP solver that runs in time
polynomial in the length of its input and returns an exact solution that is a vertex of P (t).
In Theorem 13  we show that the number of mistakes made by LearnEllipsoid is at most the
number of updates that the Ellipsoid algorithm makes before it ﬁnds a point in F and the number of
updates of the Ellipsoid algorithm can be bounded by well-known results from the literature on LP.
Theorem 13. The total number of mistakes and the running time of LearnEllipsoid in the Known
Constraints Problem is at most poly(n  d  N ).

6We again point out that this is implied if the halfspaces deﬁning the polytope are described with ﬁnite

precision [11].

8

References
[1] AMIN  K.  CUMMINGS  R.  DWORKIN  L.  KEARNS  M.  AND ROTH  A. Online learning and
proﬁt maximization from revealed preferences. In Proceedings of the 29th AAAI Conference on
Artiﬁcial Intelligence (2015)  pp. 770–776.

[2] BALCAN  M.  DANIELY  A.  MEHTA  R.  URNER  R.  AND VAZIRANI  V. Learning economic
parameters from revealed preferences. In Proceeding of the 10th International Conference on
Web and Internet Economics (2014)  pp. 338–353.

[3] BEIGMAN  E.  AND VOHRA  R. Learning from revealed preference. In Proceedings of the 7th

ACM Conference on Electronic Commerce (2006)  pp. 36–42.

[4] BELLONI  A.  LIANG  T.  NARAYANAN  H.  AND RAKHLIN  A. Escaping the local minima
via simulated annealing: Optimization of approximately convex functions. In Proceeding of the
28th Conference on Learning Theory (2015)  pp. 240–265.

[5] BHASKAR  U.  LIGETT  K.  SCHULMAN  L.  AND SWAMY  C. Achieving target equilibria in
network routing games without knowing the latency functions. In Proceeding of the 55th IEEE
Annual Symposium on Foundations of Computer Science (2014)  pp. 31–40.

[6] BLUM  A.  MANSOUR  Y.  AND MORGENSTERN  J. Learning what’s going on: Reconstructing
preferences and priorities from opaque transactions. In Proceedings of the 16th ACM Conference
on Economics and Computation (2015)  pp. 601–618.

[7] COLLINS  M. Discriminative reranking for natural language parsing. In Proceedings of the
17th International Conference on Machine Learning (2000)  Morgan Kaufmann  pp. 175–182.
[8] COLLINS  M. Discriminative training methods for hidden Markov models: Theory and
experiments with perceptron algorithms. In Proceedings of the ACL-02 Conference on Empirical
Methods in Natural Language Processing (2002)  pp. 1–8.

[9] DANIELY  A.  AND SHALEV-SHWARTZ  S. Optimal learners for multiclass problems. In

Proceedings of the 27th Conference on Learning Theory (2014)  pp. 287–316.

[10] FÜRNKRANZ  J.  AND HÜLLERMEIER  E. Preference learning. Springer  2010.
[11] GRÖTSCHEL  M.  LOVÁSZ  L.  AND SCHRIJVER  A. Geometric Algorithms and Combina-
torial Optimization  second corrected ed.  vol. 2 of Algorithms and Combinatorics. Springer 
1993.

[12] LAFFERTY  J.  MCCALLUM  A.  AND PEREIRA  F. Conditional random ﬁelds: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International
Conference on Machine Learning (2001)  pp. 282–289.

[13] LITTLESTONE  N. Learning quickly when irrelevant attributes abound: A new linear-threshold

algorithm. Machine Learning 2  4 (1988)  285–318.

[14] ROTH  A.  ULLMAN  J.  AND WU  Z. Watch and learn: Optimizing from revealed preferences
feedback. In Proceedings of the 48th Annual ACMSymposium on Theory of Computing (2016) 
pp. 949–962.

[15] VALIANT  L. A theory of the learnable. Communications of the ACM 27  11 (1984)  1134–1142.
[16] ZADIMOGHADDAM  M.  AND ROTH  A. Efﬁciently learning from revealed preference. In
Proceedings of the 8th International Workshop on Internet and Network Economics (2012) 
pp. 114–127.

9

,Shahin Jabbari
Ryan Rogers
Aaron Roth
Steven Wu