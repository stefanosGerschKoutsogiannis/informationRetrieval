2015,Associative Memory via a Sparse Recovery Model,An associative memory is a structure learned from a dataset $\mathcal{M}$ of vectors (signals) in a way such that  given a noisy version of one of the vectors as input  the nearest valid  vector from $\mathcal{M}$ (nearest neighbor) is provided as output  preferably via a fast iterative algorithm. Traditionally  binary (or $q$-ary) Hopfield neural networks are used to  model the above structure. In this paper  for the first time  we propose a model of associative memory based on sparse recovery of signals. Our basic premise is simple. For a dataset  we learn a set of linear constraints that every vector in the  dataset must satisfy. Provided these linear constraints possess  some special properties  it is possible to cast the task of finding nearest neighbor as a sparse recovery problem. Assuming  generic random models for the dataset  we show that it is possible to store super-polynomial or exponential number of $n$-length vectors in a neural network of size $O(n)$. Furthermore  given a noisy version of one of the  stored vectors corrupted in near-linear number of coordinates  the vector can be correctly recalled using a neurally feasible algorithm.,Associative Memory via a Sparse Recovery Model

Arya Mazumdar
Department of ECE

University of Minnesota Twin Cities

arya@umn.edu

Ankit Singh Rawat⇤

Computer Science Department
Carnegie Mellon University

asrawat@andrew.cmu.edu

Abstract

An associative memory is a structure learned from a dataset M of vectors (signals)
in a way such that  given a noisy version of one of the vectors as input  the nearest
valid vector from M (nearest neighbor) is provided as output  preferably via a fast
iterative algorithm. Traditionally  binary (or q-ary) Hopﬁeld neural networks are
used to model the above structure. In this paper  for the ﬁrst time  we propose
a model of associative memory based on sparse recovery of signals. Our basic
premise is simple. For a dataset  we learn a set of linear constraints that every
vector in the dataset must satisfy. Provided these linear constraints possess some
special properties  it is possible to cast the task of ﬁnding nearest neighbor as
a sparse recovery problem. Assuming generic random models for the dataset 
we show that it is possible to store super-polynomial or exponential number of
n-length vectors in a neural network of size O(n). Furthermore  given a noisy
version of one of the stored vectors corrupted in near-linear number of coordinates 
the vector can be correctly recalled using a neurally feasible algorithm.

1

Introduction

Neural associative memories with exponential storage capacity and large (potentially linear) fraction
of error-correction guarantee have been the topic of extensive research for the past three decades.
A networked associative memory model must have the ability to learn and remember an arbitrary
but speciﬁc set of n-length messages. At the same time  when presented with a noisy query  i.e.  an
n-length vector close to one of the messages  the system must be able to recall the correct message.
While the ﬁrst task is called the learning phase  the second one is referred to as the recall phase.
Associative memories are traditionally modeled by what is called binary Hopﬁeld networks [15] 
where a weighted graph of size n is considered with each vertex representing a binary state neuron.
The edge-weights of the network are learned from the set of binary vectors to be stored by the
Hebbian learning rule [13].
It has been shown in [22] that  to recover the correct vector in the
presence of a linear (in n) number of errors  it is not possible to store more than O( n
log n ) arbitrary
binary vectors in the above model of learning. In the pursuit of networks that can store exponential
(in n) number of messages  some works [26  12  21] do show the existence of Hopﬁeld networks that
can store ⇠ 1.22n messages. However  for such Hopﬁeld networks  only a small number of errors
in the query render the recall phase unsuccessful. The Hopﬁeld networks that store non-binary
message vectors are studied in [17  23]  where the storage capacity of such networks against large
fraction of errors is again shown to be linear in n. There have been multiple efforts to increase the
storage capacity of the associative memories to exponential by moving away from the framework
of the Hopﬁeld networks (in term of both the learning and the recall phases)[14  11  19  25  18].
These efforts also involve relaxing the requirement of storing the collections of arbitrary messages.
In [11]  Gripon and Berrou stored O(n2) number of sparse message vectors in the form of neural
cliques. Another setting where neurons have been assumed to have a large (albeit constant) number
⇤This work was done when the author was with the Dept. of ECE  University of Texas at Austin  TX  USA.

1

xn

rm

nodes

Message

nodes

r1

r2

r3

Constraint

x1 x2 x3

of states  and at the same time the message set (or the dataset) is assumed to form a linear subspace
is considered in [19  25  18].
The most basic premise of the works on neural
associative memory is to design a graph dynamic
system such that the vectors to be stored are the
steady states of the system. One way to attain
this is to learn a set of constraints that every vec-
tor in the dataset must satisfy. The inclusion re-
lation between the variables in the vectors and the
constraints can be represented by a bipartite graph
(cf. Fig. 1). For the recall phase  noise removal
can be done by running belief propagation on this
bipartite graph.
It can be shown that the correct
message is recovered successfully under conditions
such as sparsity and expansion properties of the
graph. This is the main idea that has been explored
in [19  25  18].
In particular  under the assump-
tion that the messages belong to a linear subspace 
[19  25] propose associative memories that can
store exponential number of messages while toler-
ating at most constant number of errors. This ap-
proach is further reﬁned in [18]  where each mes-
sage vector from the dataset is assumed to com-
prise overlapping sub-vectors which belong to dif-
ferent linear subspaces. The learning phase ﬁnds
the (sparse) linear constraints for the subspaces as-
sociated with these sub-vectors. For the recall phase then belief propagation decoding ideas of
error-correcting codes have been used. In [18]  Karbasi et al. show that the associative memories
obtained in this manner can store exponential (in n) messages. They further show that the recall
phase can correct linear (in n) number of random errors provided that the bipartite graph associated
with learnt linear constraints (during learning phase) has certain structural properties.
Our work is very closely related to the above principle. Instead of ﬁnding a sparse set of constraints 
we aim to ﬁnd a set of linear constraints that satisfy 1) the coherence property  2) the null-space
property or 3) the restricted isometry property (RIP). Indeed  for a large class of random signal
models  we show that  such constraints must exists and can be found in polynomial time. Any of
the three above mentioned properties provide sufﬁcient condition for recovery of sparse signals or
vectors [8  6]. Under the assumption that the noise in the query vector is sparse  denoising can
be done very efﬁciently via iterative sparse recovery algorithms that are neurally feasible [9]. A
neurally feasible algorithm for our model employs only local computations at the vertices of the
corresponding bipartite graph based on the information obtained from their neighboring nodes.

Figure 1: The complete bipartite graph corre-
sponding to the associative memory. Here  we
depict only a small fraction of edges. The edge
weights of the bipartite graph are obtained from the
linear constraints satisﬁed by the messages. Infor-
mation can ﬂow in both directions in the graph  i.e. 
from a message node to a constraint node and from
a constraint node to a message node. In the steady
state n message nodes store n coordinates of a valid
message  and all the m constraints nodes are satis-
ﬁed  i.e.  the weighted sum of the values stored on
the neighboring message nodes (according to the as-
sociated edge weights) is equal to zero. Note that an
edge is relevant for the information ﬂow iff the cor-
responding edge weight is nonzero.

1.1 Our techniques and results

Our main provable results pertain to two different models of datasets  and are given below.
Theorem 1 (Associative memory with sub-gaussian dataset model). It is possible to store a dataset
of size ⇠ exp(n3/4) of n-length vectors in a neural network of size O(n) such that a neurally
feasible algorithm can output the correct vector from the dataset given a noisy version of the vector
corrupted in ⇥(n1/4) coordinates.
Theorem 2 (Associative memory with dataset spanned by random rows of ﬁxed orthonormal basis).
It is possible to store a dataset of size ⇠ exp(r) of n-length vectors in a neural network of size O(n)
such that a neurally feasible algorithm can output the correct vector from the dataset given a noisy
version of the vector corrupted in ⇥( nr

log6 n ) coordinates.

Theorem 1 follows from Prop. 1 and Theorem 3  while Theorem 2 follows from Prop. 2 and 3; and
by also noting the fact that all r-vectors over any ﬁnite alphabet can be linearly mapped to exp(r)
number of points in a space of dimensionality r. The neural feasibility of the recovery follows
from the discussion of Sec. 5. In contrast with [18]  our sparse recovery based approach provides

2

associative memories that are robust against stronger error model which comprises adversarial error
patterns as opposed to random error patterns. Even though we demonstrate the associative memories
which have sub-exponential storage capacity and can tolerate sub-linear (but polynomial) number
of errors  neurally feasible recall phase is guaranteed to recover the message vector from adversarial
errors. On the other hand  the recovery guarantees in [18  Theorem 3 and 5] hold if the bipartite
graph obtained during learning phase possesses certain structures (e.g. degree sequence). However 
it is not apparent in their work if the learnt bipartite graph indeed has these structural properties.
Similar to the aforementioned papers  our operations are performed over real numbers. We show the
dimensionality of the dataset to be large enough  as referenced in Theorem 1 and 2. As in previous
works such as [18]  we can therefore ﬁnd a large number of points  exponential in the dimensionality 
with ﬁnite (integer) alphabet that can be treated as the message vectors or dataset.
Our main contribution is to bring in the model of sparse recovery in the domain of associative
memory - a very natural connection. The main techniques that we employ are as follows: 1) In
Sec. 3  we present two models of ensembles for the dataset. The dataset belongs to subspaces
that have associated orthogonal subspace with ‘good’ basis. These good basis for the orthogonal
subspaces satisfy one or more of the conditions introduced in Sec. 2  a section that provides some
background material on sparse recovery and various sufﬁcient conditions relevant to the problem.
2) In Sec. 4  we brieﬂy describe a way to obtain a ‘good’ null basis for the dataset. The found bases
serve as measurement matrices that allow for sparse recovery. 3) Sec. 5 focus on the recall phases
of the proposed associative memories. The algorithms are for sparse recovery  but stated in a way
that are implementable in a neural network.
In Sec. 6  we present some experimental results showcasing the performance of the proposed asso-
ciative memories. In Appendix C  we describe another approach to construct associative memory
based on the dictionary learning problem [24].

2 Deﬁnition and mathematical preliminaries

Notation: We use lowercase boldface letters to denote vectors. Uppercase boldface letters represent
matrices. For a matrix B  BT denotes the transpose of B. A vector is called k-sparse if it has only k
nonzero entries. For a vector x 2 Rn and any set of coordinates I ✓ [n] ⌘ {1  2  . . .   n}  xI denotes
the projection of x on to the coordinates of I. For any set of coordinates I ✓ [n]  I c ⌘ [n] \ I.
Similarly  for a matrix B  BI denotes the sub-matrix obtained by the rows of B that are indexed by
the set I. We use span(B) to denote the subspace spanned by the columns of B. Given an m ⇥ n
matrix B  denote the columns of the matrix as bj  j = 1  . . .   n and assume  for all the matrices in
this section  that the columns are all unit norm  i.e.  kbjk2 = 1.
Deﬁnition 1 (Coherence). The mutual coherence of the matrix B is deﬁned to be

µ(B) = max

i6=j |hbi  bji|.

(1)

(2)

Deﬁnition 2 (Null-space property). The matrix B is supposed to satisfy the null-space property
with parameters (k  ↵ < 1) if khIk1  ↵khI ck1  for every vector h 2 Rn with Bh = 0 and any
set I ✓ [n] |I| = k.
Deﬁnition 3 (RIP). A matrix B is said to satisfy the restricted isometry property with parameters k
and   or the the (k  )-RIP  if for all k-sparse vectors x 2 Rn 

(1  )kxk2

2  kBxk2

2  (1 + )kxk2
2.

Next we list some results pertaining to sparse signal recovery guarantee based on these aforemen-
tioned parameters. The sparse recovery problem seeks the solution ˆx  that has the smallest number
of nonzero entries  of the underdetermined system of equation Bx = r  where  B 2 Rm⇥n and
x 2 Rn. The basis pursuit algorithm for sparse recovery provides the following estimate.
(3)

Bx=r kxk1.
Let xk denote the projection of x on its largest k coordinates.
Proposition 1. If B has null-space property with parameters (k  ↵ < 1)  then  we have 

ˆx = arg min

kˆx  xk1 

2(1 + ↵)

1  ↵ kx  xkk1.
3

(4)

The proof of this is quite standard and delegated to the Appendix A.

Proposition 2 ([5] ). The (2k p2  1)-RIP of the sampling matrix implies  for a constant c 

kˆx  xk2 

c
pkkx  xkk1.

(5)

Furthermore  it can be easily seen that any matrix is (k  (k  1)µ)-RIP  where µ is the mutual
coherence of the sampling matrix.

3 Properties of the datasets

In this section  we show that  under reasonable random models that represent quite general assump-
tions on the datasets  it is possible to learn linear constraints on the messages  that satisfy one of the
sufﬁcient properties of sparse recovery: incoherence  null-space property or RIP. We mainly con-
sider two models for the dataset: 1) sub-gaussian model 2) span of a random set from an orthonormal
basis.

3.1 Sub-Gaussian model for the dataset and the null-space property

In this section we consider the message sets that are spanned by a basis matrix which has its entries
distributed according to a sub-gaussain distribution. The sub-gaussian distributions are prevalent
in machine learning literature and provide a broad class of random models to analyze and validate
various learning algorithms. We refer the readers to [27  10] for the background on these distribution.
Let A 2 Rn⇥r be an n ⇥ r random matrix that has independent zero mean sub-gaussian random
variables as its entries. We assume that the subspace spanned by the columns of the matrix A
represents our dataset M. The main result of this section is the following.
Theorem 3. The dataset above satisﬁes a set of linear constraints  that has the null-space property.
That is  for any h 2 M ⌘ span(A)  the following holds with high probability:
for all I ✓ [n] such that |I|  k 

khIk1  ↵khI ck1

(6)

for k = O(n1/4)  r = O(n/k) and a constant ↵ < 1.

The rest of this section is dedicated to the proof of this theorem. But  before we present the proof 
we state a result from [27] which we utilize to prove Theorem 3.
Proposition 3. [27  Theorem 5.39] Let A be an s ⇥ r matrix whose rows ai are independent
sub-gaussian isotropic random vectors in Rn. Then for every t  0  with probability at least
1  2 exp(ct2) one has

ps  Cpr  t  smin(A) =
 smax(A) =

min

x2Rr:kxk2=1kAxk2
x2Rr:kxk2=1kAxk2  ps + Cpr + t.

max

(7)

Here C and c depends on the sub-gaussian norms of the rows of the matrix A.
Proof of Theorem 3: Consider an n ⇥ r matrix A which has independent sub-gaussian isotropic
random vectors as its rows. Now for a given set I ✓ [n]  we can focus on two disjoint sub-matrices
of A: 1) A1 = AI and 2) A2 = AI c.
Using Proposition 3 with s = |I|  we know that  with probability at least 1  2 exp(ct2)  we have
(8)

smax(A1) =

max

x2Rr:kxk2=1kA1xk2 p|I| + Cpr + t.

Since we know that kA1xk1 p|I|kA1xk2  using (8) the following holds with probability at least
1  2 exp(ct2).
k(Ax)Ik1 = kA1xk1 p|I|kA1xk2  |I| + Cp|I|r + tp|I| 8 x 2 Rr : kxk2 = 1.

(9)

4

We now consider A2. It follows from Proposition 3 with s = |I c| = n  |I| that with probability at
least 1  2 exp(ct2) 

smin(A2) =

min

x2Rr:kxk2=1kA2xk2 pn  |I|  Cpr  t.

Combining (10) with the observation that kA2xk1  kA2xk2  the following holds with probability
at least 1  2 exp(ct2).
k(Ax)I ck1 = kA2xk1  kA2xk2 pn  |I|  Cpr  t
Note that we are interested in showing that for all h 2 M  we have

for all x 2 Rr : kxk2 = 1.

(11)

khIk1  ↵khI ck1

for all I ✓ [n] such that |I|  k.

This is equivalent to showing that the following holds for all x 2 Rr : kxk2 = 1.
for all I ✓ [n] such that |I|  k.

k(Ax)Ik1  ↵k(Ax)I ck1

(10)

(12)

(13)

(14)

For a given I ✓ [n]  we utilize (9) and (11) to guarantee that (13) holds with probability at least
1  2 exp(ct2) as long as

Now  given that k = |I| satisﬁes (14)  (13) holds for all I ⇢ [n] : |I| = k with probability at least
(15)

|I| + Cp|I|r + tp|I|  ↵pn  |I|  Cpr  t
1  2✓n
k k exp(ct2).

k◆ exp(ct2)  1  en

Let’s consider the following set of parameters: k = O(n1/4)  r = O(n/k) = O(n3/4) and t =

(cf. (15)).
Remark 1. In Theorem 3  we specify one particular set of parameters for which the null-space
property holds. Using (14) and (15)  it can be shown that the null-space property in general holds

⇥(pk log(n/k)). This set of parameters ensures that (14) holds with overwhelming probability
for the following set of parameters: k = O(pn/ log n)  r = O(n/k) and t = ⇥(pk log(n/k)).

Therefore  it possible to trade-off the number of correctable errors during the recall phase (denoted
by k) with the dimension of the dataset (represented by r).

3.2 Span of a random set of columns of an orthonormal basis

Next  in this section  we consider the ensemble of signals spanned by a random subset of rows from
a ﬁxed orthonormal basis B. Assume B to be an n ⇥ n matrix with orthonormal rows. Let  ⇢ [n]
be a random index set such that E(||) = r. The vectors in the dataset have form h = BT
 u for
some u 2 R||. In other words  the dataset M ⌘ span(BT
 ).
In this case  Bc constitutes a basis matrix for the null space of the dataset. Since we have selected
the set  randomly  set ⌦ ⌘ c is also a random set with E(⌦) = n  E() = n  r.
Proposition 4 ([7]). Assume that B be an n ⇥ n orthonormal basis for Rn with the property that
maxi j |Bi j|  ⌫. Consider a random |⌦|⇥ n matrix C obtained by selecting a random set of rows
of B indexed by the set ⌦ 2 [n] such that E(⌦) = m. Then the matrix C obeys (k  )-RIP with
probability at least 1  O(n⇢/↵) for some ﬁxed constant ⇢ > 0  where k = ↵m
Therefore  we can invoke Proposition 4 to conclude that the matrix Bc obeys (k  )-RIP with
k = ↵(nr)

⌫2 log6 n with ⌫ being the largest absolute value among the entries of Bc.

⌫2 log6 n .

4 Learning the constraints: null space with small coherence

In the previous section  we described some random ensemble of datasets that can be stored on
an associative memory based on sparse recovery. This approach involves ﬁnding a basis for the

5

orthogonal subspace to the message or the signal subspace (dataset). Indeed  our learning algorithm
simply ﬁnds a null space of the dataset M. While obtaining the basis vectors of null(M)  we
require them to satisfy null-space property  RIP or small mutual coherence so that the a signal can
be recovered from its noisy version via the basis pursuit algorithm  that can be neurally implemented
(see Sec. 5.2). However  for a given set of message vectors  it is computationally intractable to check
if the obtained (learnt) orthogonal basis has null-space property or RIP with suitable parameters
associated with these properties. Mutual coherence of the orthogonal basis  on the other hand  can
indeed be veriﬁed in a tractable manner. Further  the more straight-forward iterative soft thresholding
algorithm will be successful if null(M) has low coherence. This will also lead to fast convergence
of the recovery algorithm (see  Sec. 5.1). Towards this  we describe one approach that ensures the
selection of a orthogonal basis that has smallest possible mutual coherence. Subsequently  using
the mutual coherence based recovery guarantees for sparse recovery  this basis enables an efﬁcient
recovery phase for the associative memory.
One underlying assumption on the dataset that we make is its less than full dimensionality. That
is  the dataset must belong to a low dimensional subspace  so that its null-space is not trivial. In
practical cases  M is approximately low-dimensional. We use a preprocessing step  employing
principal component analysis (PCA) to make sure that the dataset is low-dimensional. We do not
indulge in to a more detailed description of this phase  as it seems to be quite standard (see  [18]).

Algorithm 1 Find null-space with low coherence

Input: The dataset M with n dimensional vectors. An initial coherence µ0 and a step-size 
Output: A m ⇥ n orthogonal matrix B and coherence µ
Preprocessing. Perform PCA on M
Find the n ⇥ r basis matrix A of M
for l = 0  1  2  . . . do

Find a feasible point of the quadratically constrained quadratic problem (QCQP) below (interior
point method): BA = 0; kbik = 1 8i 2 [n]; |hbi  bji|  µl where B is (n  r) ⇥ n
if No feasible point found then

break

else

µ µl
µl+1 = µl  

end if
end for

5 Recall via neurally feasible algorithms

We now focus on the second aspect of an associative memory  namely the recovery phase. For the
signal model that we consider in this paper  the recovery phase is equivalent to solving a sparse signal
recovery problem. Given a noisy vector y = x + e from the dataset  we can use the basis of the
null-space B associated to our dataset that we constructed during the learning phase to obtain r =
By = Be. Now given that e is sufﬁciently sparse enough and the matrix B obeys the properties of
Sec. 2  we can solve for e using a sparse recovery algorithm. Subsequently  we can remove the error
vector e from the noisy signal y to construct the underlying message vector x. There is a plethora of
algorithms available in the literature to solve this problem. However  we note that for the purpose of
an associative memory  the recovery phase should be neurally feasible and computationally simple.
In other words  each node (or storage unit) should be able to recover the coordinated associated to
it locally by applying simple computation on the information received from its neighboring nodes
(potentially in an iterative manner).

5.1 Recovery via Iterative soft thresholding (IST) algorithm

Among the various  sparse recovery algorithms in the literature  iterative soft thresholding (IST)
algorithm is a natural candidate for implementing the recovery phase of the associative memories
with underlying setting. The IST algorithm tries to solve the following unconstrained `1-regularized

6

least square problem which is closely related to the basis pursuit problem described in (3) and (18).

1
2kBe  rk2.
For the IST algorithm  its t + 1-th iteration is described as follows.

⌫|ek1 +

ˆe = arg min

e

(16)

(IST)

et+1 = ⌘S(et  ⌧ BT (Bet  r);  = ⌧ ⌫).

(17)
Here  ⌧ is a constant and ⌘S(x; ) = (sgn(x1)(x1)+  sgn(x2)(x2)+  . . .   sgn(xn)(xn)+)
denotes the soft thresholding (or shrinkage) operator. Note that the IST algorithm as described in
(17) is neurally feasible as it involves only 1) performing matrix vector multiplications and 2) soft
thresholding a coordinate in a vector independent of the values of other coordinates in the vector.
In Appendix B  we describe in details how the IST algorithm can be performed over a bipartite
neural network with B as its edge weight matrix. Under suitable assumption on the coherence of the
measurement matrix B  the IST algorithm is also known to converge to the correct k-sparse vector
e [20]. In particular  Maleki [20] allows the thresholding parameter  to be varied in every iteration
such that all but at most the largest k coordinates (in terms of their absolute values) are mapped
to zero by the soft thresholding operation. In this setting  Maleki shows that the solution of the
IST algorithm recovers the correct support of the optimal solution in ﬁnite steps and subsequently
converges to the true solution very fast. However  we are interested in analysis of the IST algorithm
in a setting where thresholding parameter is kept a suitable constant depending on other system
parameters so that the algorithm remains neurally feasible. Towards this  we note that there exists
general analysis of the IST algorithm even without the coherence assumption.
Proposition 5. [4  Theorem 3.1] Let {et}t1 be as deﬁned in (17) with 1
for any t  1  h(et)  h(e)  ke0ek2
function deﬁned in (16).

⌧  max(BT B)1. Then 
2kr  Bek2 + ⌫kek1 is the objective

. Here  h(e) = 1

2t

5.2 Recovery via Bregman iterative algorithm

Recall that the basis pursuit algorithm refers to the following optimization problem.

ˆe = arg min

e {kek1 : r = Be}.

(18)
Even though the IST algorithm as described in the previous subsection solves the problem in (16) 
the parameter value ⌫ needs to be set small enough so that the recovered solution ˆe nearly satisﬁes
the constraint Be = r in (18). However  if we insist on recovering the solution e which exactly
meets the constraint  one can employ the Bregman iterative algorithm from [29]. The Bregman
iterative algorithm relies on the Bregman distance Dp
(· ·) based on k · k1 which is deﬁned as
follows.

k·k1

Dp

k·k1

(e1  e2) = ke1k1  ke2k1  hp  e1  e2i 

where p 2 @ke2k1 is a sub-gradient of the `1-norm at the point e2. The (t + 1)-th iteration of the
Bregman iterative algorithm is then deﬁned as follows.

et+1 = arg min

e

Dpt
k·k1

(e  et) +

1
2kBe  rk2 

= arg min

e kek1  (pt)T e +

1
2kBe  rk2  ketk1 + (pt)T et 

(19)

pt+1 = pt  BT (Bet+1  r).

(20)
Note that  for the (t + 1)-th iteration  the objective function in (19) is essentially equivalent to the
objective function in (16). Therefore  each iteration of the Bergman iterative algorithm can be solved
using the IST algorithm. It is shown in [29] that after a ﬁnite number of iteration of the Bregman
iterative algorithm  one recovers the solution of the problem in (18) (Theorem 3.2 and 3.3 in [29]).
Remark 2. We know that the IST algorithm is neurally feasible. Furthermore  the step described
in (20) is neurally feasible as it only involve matrix-vector multiplications in the spirit of Eq. (17).
Since each iteration of the Bregman iterative algorithm only relies on these two operations  it follows
that the Bregman iterative algorithm is neurally feasible as well. It should be noted that the neural
feasibility of the Bregman iterative algorithm was discussed in [16] as well  however the neural
structures employed by [16] is different from ours.

1Note that max(BT B)  the maximum eigenvalue of the matrix BT B serves as a Lipschitz constant for the

gradient f (e) of the function f (e) = 1

2kr  Bek2

7

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
u

l
i

a

f
 
f

o
 
y
t
i
l
i

b
a
b
o
r
P

0
 
100

150

200

250

Sparsity

300

 

m = 500 (PD)
m = 500 (BI)
m = 700 (PD)
m = 700 (BI)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
u

l
i

a

f
 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

350

400

450

0
 
100

150

200

250

Sparsity

300

 

m = 500 (PD)
m = 500 (BI)
m = 700 (PD)
m = 700 (BI)

350

400

450

(a) Gaussian matrix and Gaussian noise

(b) Gaussian matrix and Discrete noise

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
u

l
i

a

f
 
f

o
 
y
t
i
l
i

b
a
b
o
r
P

0
 
100

150

200

 

Student Version of MATLAB

m = 500 (PD)
m = 500 (BI)
m = 700 (PD)
m = 700 (BI)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
r
u

l
i

a

f
 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

350

400

450

0
 
100

150

200

250

Sparsity

300

 

Student Version of MATLAB

m = 500 (PD)
m = 500 (BI)
m = 700 (PD)
m = 700 (BI)

350

400

450

250

Sparsity

300

(c) Bernoulli matrix and Gaussian noise

(d) Bernoulli matrix and Discrete noise

Figure 2: Performance of the proposed associative memory approach during recall phase. The PD algorithm
refers to the primal dual algorithm to solve linear program associated with the problem in (18). The BI algorithm
refers to the Bregman iterative algorithm described in Sec. 5.2.

Student Version of MATLAB

Student Version of MATLAB

6 Experimental results

In this section  we demonstrate the feasibility of the associative memory framework using computer
generated data. Along the line of the discussion in Sec. 3.1  we ﬁrst sample an n ⇥ r sub-gaussian
matrix A with i.i.d entries. We consider two sub-gaussian distributions: 1) Gaussian distribution
and 2) Bernoulli distribution over {+1 1}. The message vectors to be stored are then assumed to
be spanned by the k columns of the sampled matrix. For the learning phase  we ﬁnd a good basis
for the subspace orthogonal to the space spanned by the columns of the matrix A. For noise during
the recall phase  we consider two noise models: 1) Gaussian noise and 2) discrete noise where each
nonzero elements take value in the set {M (M  1)  . . .   M}\{0}.
Figure 2 presents our simulation results for n = 1000. For recall phase  we employ the Bregman
iterative (BI) algorithm with the IST algorithm as a subroutine. We also plot the performance of
the primal dual (PD) algorithm based linear programming solution for the recovery problem of
interest (cf. (18)). This allows us to gauge the disadvantage due to the restriction of working with
a neurally feasible recovery algorithm  e.g.  the BI algorithm in our case. Furthermore  we consider
message sets with two different dimensions which amounts to m = 500 and m = 700. Note that
the dimension of the message set is n  m. We run 50 iterations of the recovery algorithms for a
given set of parameters to obtain the estimates of the probability of failure (of exact recovery of error
vector). In Fig. 2a  we focus on the setting with Gaussian basis matrix (for message set) and unit
variance zero mean Gaussian noise during the recall phase. It is evident that the proposed associative
memory do allow for the exact recovery of error vectors up to certain sparsity level. This corroborate
our ﬁndings in Sec. 3. We also note that the performance of the BI algorithm is very close to the
PD algorithm. Fig. 2b shows the performance of the recall phase for the setting with Gaussian basis
for message set and discrete noise model with M = 4. In this case  even though the BI algorithm
is able to exactly recover the noise vector up to a particular sparsity level  it’s performance is worse
than that of PD algorithm. The performance of the recall phase with Bernoulli bases matrices for
message set is shown in Fig. 2c and 2d. The results are similar to those in the case of Gaussian bases
matrices for the message sets.

8

References
[1] A. Agarwal  A. Anandkumar  P. Jain  P. Netrapalli  and R. Tandon. Learning sparsely used overcomplete

dictionaries via alternating minimization. CoRR  abs/1310.7991  2013.

[2] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for sparse coding. CoRR 

abs/1503.00778  2015.

[3] S. Arora  R. Ge  and A. Moitra. New algorithms for learning incoherent and overcomplete dictionaries.

arXiv preprint arXiv:1308.6273  2013.

[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[5] E. J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes

Rendus Mathematique  346(9):589–592  2008.

[6] E. J. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information. IEEE Trans. on Inf. Theory  52(2):489–509  2006.

[7] E. J. Candes and T. Tao. Near-optimal signal recovery from random projections: Universal encoding

strategies? IEEE Trans. on Inf. Theory  52(12):5406–5425  Dec 2006.

[8] D. L. Donoho. Compressed sensing. IEEE Trans. on Inf. Theory  52(4):1289–1306  2006.
[9] D. L. Donoho  A. Maleki  and A. Montanari. Message-passing algorithms for compressed sensing. Pro-

ceedings of the National Academy of Sciences  106(45):18914–18919  2009.

[10] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Birkhauser Basel  2013.
[11] V. Gripon and C. Berrou. Sparse neural networks with large learning diversity. IEEE Transactions on

Neural Networks  22(7):1087–1096  2011.

[12] D. J. Gross and M. Mezard. The simplest spin glass. Nuclear Physics B  240(4):431 – 452  1984.
[13] D. O. Hebb. The organization of behavior: A neuropsychological theory. Psychology Press  2005.
[14] C. Hillar and N. M. Tran.

Robust exponential memory in hopﬁeld networks.

arXiv preprint

arXiv:1411.4625  2014.

[15] J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.

Proceedings of the national academy of sciences  79(8):2554–2558  1982.

[16] T. Hu  A. Genkin  and D. B. Chklovskii. A network of spiking neurons for computing sparse representa-

tions in an energy-efﬁcient way. Neural computation  24(11):2852–2872  2012.

[17] S. Jankowski  A. Lozowski  and J. M. Zurada. Complex-valued multistate neural associative memory.

IEEE Transactions on Neural Networks  7(6):1491–1496  Nov 1996.

[18] A. Karbasi  A. H. Salavati  and A. Shokrollahi. Convolutional neural associative memories: Massive

capacity with noise tolerance. CoRR  abs/1407.6513  2014.

[19] K. R. Kumar  A. H. Salavati  and A. Shokrollahi. Exponential pattern retrieval capacity with non-binary

associative memory. In 2011 IEEE Information Theory Workshop (ITW)  pages 80–84  Oct 2011.

[20] A. Maleki. Coherence analysis of iterative thresholding algorithms. In 47th Annual Allerton Conference

on Communication  Control  and Computing  2009. Allerton 2009  pages 236–243  Sept 2009.

[21] R. J. McEliece and E. C. Posner. The number of stable points of an inﬁnite-range spin glass memory.

Telecommunications and Data Acquisition Progress Report  83:209–215  1985.

[22] R. J. McEliece  E. C. Posner  E. R. Rodemich  and S. S. Venkatesh. The capacity of the hopﬁeld associa-

tive memory. Information Theory  IEEE Transactions on  33(4):461–482  1987.

[23] M. K. Muezzinoglu  C. Guzelis  and J. M. Zurada. A new design method for the complex-valued multi-

state hopﬁeld associative memory. IEEE Transactions on Neural Networks  14(4):891–899  July 2003.

[24] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by

v1? Vision Research  37(23):3311 – 3325  1997.

[25] A. H. Salavati and A. Karbasi. Multi-level error-resilient neural networks. In 2012 IEEE International

Symposium on Information Theory Proceedings (ISIT)  pages 1064–1068  July 2012.

[26] F. Tanaka and S. F. Edwards. Analytic theory of the ground state properties of a spin glass. i. ising spin

glass. Journal of Physics F: Metal Physics  10(12):2769  1980.

[27] R. Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

arXiv:1011.3027  2010.

arXiv preprint

[28] M. J. Wainwright.

Sharp thresholds for high-dimensional and noisy sparsity recovery using `1 -
constrained quadratic programming (lasso). IEEE Trans. Inform. Theory  55(5):2183–2202  May 2009.
[29] W. Yin  S. Osher  D. Goldfarb  and J. Darbon. Bregman iterative algorithms for `1-minimization with

applications to compressed sensing. SIAM Journal on Imaging Sciences  1(1):143–168  2008.

9

,Arya Mazumdar
Ankit Singh Rawat