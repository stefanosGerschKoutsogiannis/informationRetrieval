2014,Bayesian Sampling Using Stochastic Gradient Thermostats,Dynamics-based sampling methods  such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD)  are commonly used to sample target distributions. Recently  such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem  we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.,Bayesian Sampling Using Stochastic Gradient

Thermostats

Nan Ding∗
Google Inc.

dingnan@google.com

Youhan Fang∗
Purdue University

yfang@cs.purdue.edu

Changyou Chen
Duke University

cchangyou@gmail.com

Robert D. Skeel
Purdue University

skeel@cs.purdue.edu

Ryan Babbush

Google Inc.

babbush@google.com

Hartmut Neven

Google Inc.

neven@google.com

Abstract

Dynamics-based sampling methods  such as Hybrid Monte Carlo (HMC) and
Langevin dynamics (LD)  are commonly used to sample target distributions. Re-
cently  such approaches have been combined with stochastic gradient techniques
to increase sampling efﬁciency when dealing with large datasets. An outstanding
problem with this approach is that the stochastic gradient introduces an unknown
amount of noise which can prevent proper sampling after discretization. To rem-
edy this problem  we show that one can leverage a small number of additional
variables to stabilize momentum ﬂuctuations induced by the unknown noise. Our
method is inspired by the idea of a thermostat in statistical physics and is justiﬁed
by a general theory.

1

Introduction

The generation of random samples from a posterior distribution is a pervasive problem in Bayesian
statistics which has many important applications in machine learning. The Markov Chain Monte
Carlo method (MCMC)  proposed by Metropolis et al.[16]  generates unbiased samples from a
desired distribution when the density function is known up to a normalizing constant. However 
traditional MCMC methods are based on random walk proposals which lead to highly correlated
samples. On the other hand  dynamics-based sampling methods  e.g. Hybrid Monte Carlo (HMC)
[6  10]  avoid this high degree of correlation by combining dynamic systems with the Metropolis
step. The dynamic system uses information from the gradient of the log density to reduce the ran-
dom walk effect  and the Metropolis step serves as a correction of the discretization error introduced
by the numerical integration of the dynamic systems.
The computational cost of HMC methods depends primarily on the gradient evaluation. In many
machine learning problems  expensive gradient computations are a consequence of working with
extremely large datasets. In such scenarios  methods based on stochastic gradients have been very
successful. A stochastic gradient uses the gradient obtained from a random subset of the data to
approximate the true gradient. This idea was ﬁrst used in optimization [9  19] but was recently
adapted for sampling methods based on stochastic differential equations (SDEs) such as Brownian
dynamics [1  18  24] and Langevin dynamics [5].
Due to discretization  stochastic gradients introduce an unknown amount of noise into the dynamic
system. Existing methods sample correctly only when the step size is small or when a good estimate
of the noise is available. In this paper  we propose a method based on SDEs that self-adapts to the

∗ indicates equal contribution.

1

unknown noise with the help of a small number of additional variables. This allows for the use
of larger discretization step  smaller diffusion factor  or smaller minibatch to improve the sampling
efﬁciency without sacriﬁcing accuracy.
From the statistical physics perspective  all these dynamics-based sampling methods are approaches
that use dynamics to approximate a canonical ensemble [23]. In a canonical ensemble  the dis-
tribution of the states follows the canonical distribution which corresponds to the target posterior
distribution of interests. In attemping to sample from the canonical ensemble  existing methods
have neglected the condition that  the system temperature must remain near a target temperature
(Eq.(4) of Sec. 3). When this requirement is ignored  noise introduced by stochastic gradients may
drive the system temperature away from the target temperature and cause inaccurate sampling. The
additional variables in our method essentially play the role of a thermostat which controls the tem-
perature and  as a consequence  handles the unknown noise. This approach can also be found by
following a general recipe which helps designing dynamic systems that produce correct samples.
The rest of the paper is organized as follows. Section 2 brieﬂy reviews the related background.
Section 3 proposes the stochastic gradient Nos´e-Hoover thermostat method which maintains the
canonical ensemble. Section 4 presents the general recipe for ﬁnding proper SDEs and mathemati-
cally shows that the proposed method produces samples from the correct target distribution. Section
5 compares our method with previous methods on synthetic and real world machine learning appli-
cations. The paper is concluded in Section 6.

2 Background
Our objective is to generate random samples from the posterior probability density p(θ| X) ∝
p(X|θ)p(θ)  where θ represents an n-dim parameter vector and X represents data. The canoni-
cal form is p(θ| X) = (1/Z) exp(−U (θ)) where U (θ) = − log p(X|θ) − log p(θ) is referred to
as the potential energy and Z is the normalizing constant. Here  we brieﬂy review a few dynamics-
based sampling methods  including HMC  LD  stochastic gradient LD (SGLD) [24]  and stochastic
gradient HMC (SGHMC) [5]  while relegating a more comprehensive review to Appendix A.
HMC [17] works in an extended space Γ = (θ  p)  where θ and p simulate the positions and the
momenta of particles in a system. Although some works  e.g. [7  8]  make use of variable mass 
we assume that all particles have unit constant mass (i.e. mi = 1). The joint density of θ and
p can be written as ρ(θ  p) ∝ exp(−H(θ  p))  where H(θ  p) = U (θ) + K(p) is called the
Hamiltonian (the total energy). U (θ) is called the potential energy and K(p) = p(cid:62) p /2 is called
the kinetic energy. Note that p has standard normal distribution. The force on the system is deﬁned
as f (θ) = −∇U (θ). It can be shown that the Hamiltonian dynamics

dθ = p dt  d p = f (θ)dt 

maintain a constant total energy [17]. In each step of the HMC algorithm  one ﬁrst randomizes
p according to the standard normal distribution; then evolves (θ  p) according to the Hamiltonian
dynamics (solved by numerical integrators); and ﬁnally uses the Metropolis step to correct the dis-
cretization error.
Langevin dynamics (with diffusion factor A) are described by the following SDE 

dθ = p dt  d p = f (θ)dt − A p dt +

(1)
where W is n independent Wiener processes (see Appendix A)  and d W can be informally written
as N (0  I dt) or simply N (0  dt) as in [5]. Brownian dynamics
dθ = f (θ)dt + N (0  2dt)

is obtained from Langevin dynamics by rescaling time t ← At and letting A → ∞  i.e.  on long
of the gradient of − log p(X|θ) = −(cid:80)N
time scales inertia effects can be neglected [11]. When the size of the dataset is big  the computation
i=1 log p(xi |θ) can be very expensive. In such situations 
˜N(cid:88)
one could use the likelihood of a random subset of the data xi’s to approximate the true likelihood 

√

2Ad W 

˜U (θ) = − N
˜N

log p(x(i) |θ) − log p(θ) 

(2)

i=1

2

where(cid:8)x(i)

(cid:9) represents a random subset of {xi} and ˜N (cid:28) N. Deﬁne the stochastic force ˜f (θ) =

−∇ ˜U (θ). The SGLD algorithm [24] uses ˜f (θ) and the Brownian dynamics to generate samples 

dθ = ˜f (θ)dt + N (0  2dt).

In [5]  the stochastic force with a discretization step h is approximated as h˜f (θ) (cid:39) h f (θ) +
N (0  2h B(θ)) (note that the argument is not rigorous and that other signiﬁcant artifacts of dis-
cretization may have been neglected). The SGHMC algorithm uses a modiﬁed LD 
dθ = p dt  d p = ˜f (θ)dt − A p dt + N (0  2(A I− ˆB(θ))dt) 

(3)

where ˆB(θ) is intended to offset B(θ)  the noise from the stochastic force.
However  ˆB(θ) is hard to estimate in practice and cannot be omitted when the discretization step h
is not small enough. Since poor estimation of ˆB(θ) may lead to inaccurate sampling  we attempt to
ﬁnd a dynamic system which is able to adaptively ﬁt to the noise without explicit estimation. The
intuition comes from the practice of sampling a canonical ensemble in statistical physics.
The Metropolis step in SDE-based samplers with stochastic gradients is sometimes omitted on large
datasets  because the evaluation of the potential energy requires using the entire dataset which can-
cels the beneﬁt of using stochastic gradients. There is some recent work [2  3  14] which attempts
to estimate the Metropolis step using partial data. Although an interesting direction for future work 
in this paper we do not consider applying Metropolis step in conjunction with stochastic gradients.

3 Stochastic Gradient Thermostats

In statistical physics  a canonical ensemble represents the possible states of a system in thermal
equilibrium with a heat bath at ﬁxed temperature T [23]. The probability of the states in a canonical
ensemble follows the canonical distribution ρ(θ  p) ∝ exp(−H(θ  p)/(kBT ))  where kB is the
Boltzmann constant. A critical characteristic of the canonical ensemble is that the system tempera-
ture  deﬁned as the mean kinetic energy  satisﬁes the following thermal equilibrium condition 

kBT

2

=

1
n

E[K(p)]  or equivalently  kBT =

E[p(cid:62) p].

1
n

(4)

All dynamics-based sampling methods approximate the canonical ensemble to generate samples. In
Bayesian statistics  n is the dimension of θ  and kBT = 1 so that ρ(θ  p) ∝ exp(−H(θ  p)) and
more importantly ρθ(θ) ∝ exp(−U (θ)). However  one key fact that was overlooked in previous
methods  is that the dynamics that correctly simulate the canonical ensemble must maintain the
thermal equilibrium condition (4). Besides its physical meaning  the condition is necessary for p
being distributed as its marginal canonical distribution ρp(p) ∝ exp(−K(p)).
It can be veriﬁed that ordinary HMC and LD (1) with true force both maintain (4). However  after
combination with the stochastic force ˜f (θ)  the dynamics (3) may drift away from thermal equilib-
rium if ˆB(θ) is poorly estimated. Therefore  to generate correct samples  one needs to introduce a
proper thermostat  which adaptively controls the mean kinetic energy. To this end  we introduce an
additional variable ξ  and use the following dynamics (with diffusion factor A and kBT = 1) 

dθ = p dt  d p = ˜f (θ)dt − ξ p dt +

dξ = (

1
n

p(cid:62) p−1)dt.

2AN (0  dt) 

(5)

(6)

√

Intuitively  if the mean kinetic energy is higher than 1/2  then ξ gets bigger and p experiences more
friction in (5); on the other hand  if the mean kinetic energy is lower  then ξ gets smaller and p
experiences less friction. Because (6) appears to be the same as the Nos´e-Hoover thermostat [13]
in statistical physics  we call our method stochastic gradient Nos´e-Hoover thermostat (SGNHT 
Algorithm 1). In Section 4  we will show that (6) is a simpliﬁed version of a more general SGNHT
method that is able to handle high dimensional non-isotropic noise from ˜f. But before that  let us
ﬁrst look at a 1-D illustration of the SGNHT sampling in the presence of unknown noise.

3

Algorithm 1: Stochastic Gradient Nos´e-Hoover Thermostat
Input: Parameters h  A.
Initialize θ(0) ∈ Rn  p(0) ∼ N (0  I)  and ξ(0) = A ;
for t = 1  2  . . . do

Evaluate ∇ ˜U (θ(t−1)) from (2) ;
p(t) = p(t−1) −ξ(t−1) p(t−1) h − ∇ ˜U (θ(t−1))h +
θ(t) = θ(t−1) + p(t) h;
n p(cid:62)
ξ(t) = ξ(t−1) + ( 1

(t) p(t) −1)h;

end

√

2AN (0  h);

Illustrations of a Double-well Potential To illustrate that the adaptive update (6) is able to control
the mean kinetic energy  and more importantly  produce correct sampling with unknown noise on
the gradient  we consider the following double-well potential 

U (θ) = (θ + 4)(θ + 1)(θ − 1)(θ − 3)/14 + 0.5.

The target distribution is ρ(θ) ∝ exp(−U (θ)). To simulate the unknown noise  we let ∇ ˜U (θ)h =
∇U (θ)h + N (0  2Bh)  where h = 0.01 and B = 1. In the interest of clarity we did not inject
additional noise other than the noise from ∇ ˜U (θ)  namely A = 0. In Figure 1 we plot the estimated
density based on 106 samples and the mean kinetic energy over iterations  when ξ is ﬁxed at 0.1  1  10
successively  as well as when ξ follows our thermostat update in (6).
From Figure 1  when ξ = B = 1  the SDE is the ordinary Langevin dynamics. In this case  the
sampling is accurate and the kinetic energy is controlled around 0.5. When ξ > B  the kinetic
energy drops to a low value  and the sampling gets stuck in one local minimum; this is what happens
in the SGD optimization with momentum. When ξ < B  the kinetic energy gets too high  and the
sampling looks like a random walk. For SGNHT  the sampling looks as accurate as the one with
ξ = B and the kinetic energy is also controlled around 0.5. Actually in Appendix B  we see that the
value of ξ of SGNHT quickly converges to B = 1.

True distribution

ξ = 1

True distribution

ξ = 10

0.6

0.4

0.2

)
θ
(
ρ

0
−6

0.6

0.5

0.4

0.3

0.2

)
p
(

K

−4

−2

0

θ

2

4

6

ξ = 1

2

1

)
θ
(
ρ

0
−6

−4

−2

0

θ

)
p
(

K

1.2

1

0.8

0.6

0.4

0.2

0

)
θ
(
ρ

0.6

0.4

0.2

0

2

4

6

−6

−4

−2

ξ = 10

)
p
(

K

4

2

0

True distribution

SGNHT

True distribution

ξ = 0.1

0.6

0.4

0.2

)
θ
(
ρ

2

4

6

0

θ

ξ = 0.1

0
−6

−4

−2

2

4

6

0

θ

SGNHT

)
p
(

K

0.5

0.4

0.3

0.2

0

0.2

0.4
0.6
iterations

0.8

1
·106

0

0.2

0.4
0.6
iterations

0.8

1
·106

0

0.2

0.4
0.6
iterations

0.8

1
·106

0

0.2

0.4
0.6
iterations

0.8

1
·106

Figure 1: The samples on ρ(θ) and the mean kinetic energy over iterations K(p) with ξ = 1 (1st) 
ξ = 10 (2nd)  ξ = 0.1 (3rd)  and the SGNHT (4th). The ﬁrst three do not use a thermostat. The
fourth column shows that the SGNHT method is able to sample accurately and maintains the mean
kinetic energy with unknown noise.

4 The General Recipe

In this section  we mathematically justify the proposed SGNHT method. We begin with a theorem
showing why and how a sampler based on SDEs using stochastic gradients can produce the correct
target distribution. The theorem serves two purposes. First  one can examine whether a given SDE
sampler is correct or not. The theorem is more general than previous ones in [5][24] which focus
on justifying individual methods. Second  the theorem can be a general recipe for proposing new
methods. As a concrete example of using this approach  we show how to obtain SGNHT from the
main theorem.

4

4.1 The Main Theorem

Consider the following general stochastic differential equations that use the stochastic force:

dΓ = v(Γ)dt + N (0  2 D(θ)dt)

(7)
where Γ = (θ  p  ξ)  and both p and ξ are optional. v is a vector ﬁeld that characterizes the
deterministic part of the dynamics. D(θ) = A +diag(0  B(θ)  0)  where the injected noise A is
known and constant  whereas the noise of the stochastic gradient B(θ) is unknown  may vary  and
only appears in blocks corresponding to rows of the momentum. Both A and B are symmetric
positive semideﬁnite. Taking the dynamics of SGHMC as an example  it has Γ = (θ  p)  v =
(p  f −Ap) and D(θ) = diag(0  A I− ˆB(θ) + B(θ)).
Let ρ(Γ) = (1/Z) exp(−H(Γ)) be the joint probability density of all variables  and write H as
H(Γ) = U (θ) + Q(θ  p  ξ). The marginal density for θ must equal the target density 

exp (−U (θ)) ∝

exp (−U (θ) − Q(θ  p  ξ)) dpdξ

(8)

(cid:90)(cid:90)

which will be referred as the marginalization condition.
Main Theorem. The stochastic process of θ generated by the stochastic differential equation (7) has
the target distribution ρθ(θ) = (1/Z) exp(−U (θ)) as its stationary distribution  if ρ ∝ exp (−H)
satisﬁes the marginalization condition (8)  and

∇ · (ρv) = ∇∇(cid:62) : (ρ D) 

(9)

where we use concise notation  ∇ = (∂/∂θ  ∂/∂ p  ∂/∂ξ) being a column vector 
· representing a vector inner product x· y = x(cid:62) y  and : representing a matrix double dot product
X : Y = trace(X(cid:62) Y).

Proof. See Appendix C.
Remark. The theorem implies that when the SDE is solved exactly (namely h → 0)  then the noise
of the stochastic force has no effect  because limh→0 D = A [5]. In this case  any dynamics that
produce the correct distribution with the true gradient  such as the original Langevin dynamics  can
also produce the correct distribution with the stochastic gradient.

However  when there is discretization error one must ﬁnd the proper H  v and A to ensure pro-
duction of the correct distribution of θ. Towards this end  the theorem provides a general recipe
for ﬁnding proper dynamics that can sample correctly in the presence of stochastic forces. To use
this prescription  one may freely select the dynamics characterized by v and A as well as the joint
stationary distribution for which the marginalization condition holds. Together  the selected v  A
and ρ must satisfy this main theorem.
The marginalization condition is important because for some stochastic differential equations there
exists a ρ that makes (9) hold even though the marginalized distribution is not the target distribution.
Therefore  care must be taken when designing the dynamics. In the following subsection  we will
use the proposed stochastic gradient Nos´e-Hoover thermostats as an illustrative example of how our
recipe may be used to discover new methods. We will show more examples in Appendix D.

4.2 Revisiting the Stochastic Gradient Nos´e-Hoover Thermostat

Let us start from the following dynamics:

dθ = p dt  d p = f dt − Ξ p dt + N (0  2 D dt) 

where both Ξ and D are n × n matrices. Apparently  when Ξ (cid:54)= D  the dynamics will not generate
the correct target distribution (see Appendix D). Now let us add dynamics for Ξ  denoted by dΞ =
v(Ξ) dt  and demonstrate application of the main theorem.
Let ρ(θ  p  Ξ) = (1/Z) exp(−H(θ  p  Ξ)) be our target distribution  where H(θ  p  Ξ) = U (θ) +
Q(p  Ξ) and Q(p  Ξ) is also to be determined. Clearly  the marginalization condition is satisﬁed for
such H(θ  p  Ξ).

5

Let Rz denote the gradient of a function R  and Rz z denote the Hessian. For simplicity  we constrain
∇Ξ · v(Ξ) = 0  and assume that D is a constant matrix. Then the LHS and RHS of (9) become

LHS = (∇ · v −∇H · v)ρ = (−trace(Ξ) + f Tp − QT
RHS = D : ρpp = D : (QpQT

p − Qpp)ρ.

pf + QT

pΞp − QΞ : v(Ξ))ρ 

Equating both sides  one gets

−trace(Ξ) + f Tp − QT

pf + QT

pΞp − QΞ : v(Ξ) = D : (QpQT

p) − D : Qpp.

To cancel the f terms  set Qp = p  then Q(p  Ξ) = 1
determined. The equation becomes

2 pT p + S(Ξ)  which leaves S(Ξ) to be

−Ξ : I +Ξ : (ppT) − SΞ : v(Ξ) = D : (ppT) − D : I .

(10)
Obviously  v(Ξ) must be a function of ppT since SΞ is independent of p. Also  D must only appear
in SΞ  since we want v(Ξ) to be independent of the unknown D. Finally  v(Ξ) should be independent
of Ξ  since we let ∇Ξ · v(Ξ) = 0. Combining all three observations  we let v(Ξ) be a linear function
of ppT  and SΞ a linear function of Ξ. With some algebra  one ﬁnds that

v(Ξ) = (ppT − I)/µ 
2 pTp + 1

(11)
and SΞ = (Ξ − D)µ which means Q(p  Ξ) = 1
2 µ(Ξ − D) : (Ξ − D). (11) deﬁnes a
general stochastic gradient Nos´e-Hoover thermostats. When D = D I and Ξ = ξ I (here D and ξ
are both scalars and I is the identity matrix)  one can simplify (10) and obtain v(ξ) = (pTp − n)/µ.
It reduces to (6) of the SGNHT in section 3 when µ = n.
The Nos´e-Hoover thermostat without stochastic terms has ξ ∼ N (0  µ−1). When there is
a stochastic term N (0  2 D dt)  the distribution of Ξ changes to a matrix normal distribution
MN (D  µ−1 I  I) (in the scalar case  N (D  µ−1)). This indicates that the thermostat absorbs the
stochastic term D  since the expected value of Ξ is equal to D  and leaves the marginal distribution
of θ invariant.
In the derivation above  we assumed that D is constant (by assuming B constant). This assumption
is reasonable when the data size is large so that the posterior of θ has small variance. In addition 
the full dynamics of Ξ requires additional n × n equations of motion  which is generally too costly.
In practice  we found that Algorithm 1 with a single scalar ξ works well.

5 Experiments

5.1 Gaussian Distribution Estimation Using Stochastic Gradient

We ﬁrst demonstrate our method on a simple example: Bayesian inference on 1D normal distri-
butions. The ﬁrst part of the experiment tries to estimate the mean of the normal distribution with
known variance and N = 100 random examples from N (0  1). The likelihood is N (xi|µ  1)  and an
improper prior of µ being uniform is assigned. Each iteration we randomly select ˜N = 10 examples.
The noise of the stochastic gradient is a constant given ˜N (Appendix E).
Figure 2 shows the density of 106 samples obtained by SGNHT (1st plot) and SGHMC (2nd plot).
As we can see  SGNHT samples accurately without knowing the variance of the noise of the stochas-
tic force under all parameter settings  whereas SGHMC samples accurately only when h is small and
A is large. The 3rd plot shows the mean of ξ values in SGNHT. When h = 0.001  ξ and A are close.
However  when h = 0.01  ξ becomes much larger than A. This indicates that the discretization in-
troduces a large noise from the stochastic gradient  and the ξ variable effectively absorbs the noise.
The second part of the experiment is to estimate both mean and variance of the normal distribu-
tion. We use the likelihood function N (xi|µ  γ−1) and the Normal-Gamma distribution µ  γ ∼
N (µ|0  γ)Gam(γ|1  1) as prior. The variance of the stochastic gradient noise is no longer a con-
stant and depends on the values of µ and γ (see Appendix E).
Similar density plots are available in Appendix E. Here we plot the Root Mean Square Error (RMSE)
of the density estimation vs. the autocorrelation time of the observable µ + γ under various h and
A in the 4th plot in Figure 2. We can see that SGNHT has signiﬁcantly lower autocorrelation time
than SGHMC at similar sampling accuracy. More details about the h  A values which produces the
plot are also available in Appendix E.

6

Density of µ (SGNHT)

Density of µ (SGHMC)

ξ value of SGNHT

RMSE vs. Autocorrelation time

5

4

3

2

1

µ
f
o

y
t
i
s
n
e
D

True

h=0.01 A=1
h=0.01 A=10
h=0.001 A=1
h=0.001 A=10

5

4

3

2

1

µ
f
o

y
t
i
s
n
e
D

0
−0.6 −0.4 −0.2

0

0.2
µ

0.4

0.6

0.8

1

0
−0.6 −0.4 −0.2

0

0.2
µ

h=0.01 A=1
h=0.01 A=10
h=0.001 A=1
h=0.001 A=10

True

h=0.01 A=1
h=0.01 A=10
h=0.001 A=1
h=0.001 A=10

ξ

15

10

5

0

0.4

0.6

0.8

1

0

0.2

0.4
0.6
iterations

0.8

1
·106

0.6

0.4

0.2

n
o
i
t
a
m

i
t
s
E
y
t
i
s
n
e
D

f
o
E
S
M
R

0

0

SGHMC
SGNHT

400

100

200

300
Autocorrrelation Time

Figure 2: Density of µ obtained by SGNHT with known variance (1st)  density of µ obtained by
SGHMC with known variance (2nd)  mean of ξ over iterations with known variance in SGNHT
(3rd)  RMSE vs. Autocorrelation time for both methods with unknown variance (4th).
5.2 Machine Learning Applications

In the following machine learning experiments  we used a reformulation of (5) and (6) similar to [5] 
by letting u = p h  η = h2  α = ξh and a = Ah. The resulting Algorithm 2 is provided in Appendix
F. In [5]  SGHMC has been extensively compared with SGLD  SGD and SGD-momentum. Our
experiments will focus on comparing SGHMC and SGNHT. Details of the experiment settings are
described below. The test results over various parameters are reported in Figure 3.

Bayesian Neural Network We ﬁrst evaluate the benchmark MNIST dataset  using the Bayesian
Neural Network (BNN) as in [5]. The MNIST dataset contains 50 000 training examples  10 000
validation examples  and 10 000 test examples. To show our algorithm being able to handle large
stochastic gradient noise due to small minibatch  we chose the minibatch of size 20. Each algorithm
is run for a total number of 50k iterations with burn-in of the ﬁrst 10k iterations. The hidden layer
size is 100  parameter a is from {0.001  0.01} and η from {2  4  6  8} × 10−7.

Bayesian Matrix Factorization Next  we evaluate our methods on two collaborative ﬁltering
tasks: the Movielens ml-1m dataset and the Netﬂix dataset  using the Bayesian probabilistic matrix
factorization (BPMF) model [21]. The Movielens dataset contains 6 050 users and 3 883 movies
with about 1M ratings  and the Netﬂix dataset contains 480 046 users and 17 000 movies with about
100M ratings. To conduct the experiments  Each dataset is partitioned into training (80%) and test-
ing (20%)  and the training set is further partitioned for 5-fold cross validation. Each minibatch
contains 400 ratings for Movielens1M and 40k ratings for Netﬂix. Each algorithm is run for 100k
iterations with burn-in of the ﬁrst 20k iterations. The base number is chosen as 10  parameter a is
from {0.01  0.1} and η from {2  4  6  8} × 10−7.

Latent Dirichlet Allocation Finally  we evaluate our method on the ICML dataset using Latent
Dirichlet Allocation [4]. The ICML dataset contains 765 documents from the abstracts or ICML
proceedings from 2007 to 2011. After simple stopword removal  we obtained a vocabulary size of
about 2K and total words of about 44K. We used 80% documents for 5-fold cross validation and
the remaining 20% for testing. Similar to [18]  we used the semi-collapsed LDA whose posterior
of θkw is provided in Appendix H. The Dirichlet prior parameter for the topic distribution for each
document is set to 0.1 and the Gaussian prior for θkw is set as N (0.1  1). Each minibatch contains
100 documents. Each algorithm is run for 50k iterations with the ﬁrst 10k iterations as burn-in.
Topic number is 30  parameter a is from {0.01  0.1} and η from {2  4  6  8} × 10−5.

5.2.1 Result Analysis

From Figure 3  SGNHT is apparently more stable than SGHMC when the discretization step η is
larger. In all four datasets  especially with the smaller a  SGHMC gets worse and worse results as η
increases. With the largest η  SGHMC diverges (as the green curve is way beyond the range) due to
its failure to handle the large unknown noise with small a.
Figure 3 also gives a comprehensive view of the critical role that a plays on. On one hand  larger
a may cause more random walk effect which slows down the convergence (as in Movielens1M and
Netﬂix). On the other hand  it is helpful to increase the ergodicity and compensate the unknown
noise from the stochastic gradient (as in MNIST and ICML).

7

Throughout the experiment  we ﬁnd that the kinetic energy of SGNHT is always maintained around
0.5 while that of SGHMC is usually higher. And overall SGNHT has better test performance with
the choice of the parameters selected by cross validation (see Table 2 of Appendix G).

6

5

4

3

2

0.94

0.92

0.9

0.88

0.86

0.86

0.85

0.84

0.83

0.82

1 500

1 400

1 300

1 200

1 100

1 000

r
o
r
r
E

t
s
e
T

E
S
M
R

t
s
e
T

E
S
M
R

t
s
e
T

y
t
i
x
e
l
p
r
e
P

t
s
e
T

MNIST (η = 2 × 10−7)

·10−2

SGHMC(a = 0.001)
SGNHT(a = 0.001)
SGHMC(a = 0.01)
SGNHT(a = 0.01)

1

2

3

4

iterations

Movielens1M (η = 2 × 10−7)

5
·104

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

Netﬂix (η = 2 × 10−7)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

ICML (η = 2 × 10−5)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

1

2

3

4

iterations

5
·104

r
o
r
r
E

t
s
e
T

E
S
M
R

t
s
e
T

E
S
M
R

t
s
e
T

6

5

4

3

2

0.94

0.92

0.9

0.88

0.86

0.86

0.85

0.84

0.83

0.82

y
t
i
x
e
l
p
r
e
P

t
s
e
T

1 500

1 400

1 300

1 200

1 100

1 000

MNIST (η = 4 × 10−7)

·10−2

SGHMC(a = 0.001)
SGNHT(a = 0.001)
SGHMC(a = 0.01)
SGNHT(a = 0.01)

1

2

3

4

iterations

Movielens1M (η = 4 × 10−7)

5
·104

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

Netﬂix (η = 4 × 10−7)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

ICML (η = 4 × 10−5)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

1

2

3

4

iterations

5
·104

r
o
r
r
E

t
s
e
T

E
S
M
R

t
s
e
T

E
S
M
R

t
s
e
T

6

5

4

3

2

0.94

0.92

0.9

0.88

0.86

0.86

0.85

0.84

0.83

0.82

y
t
i
x
e
l
p
r
e
P

t
s
e
T

1 500

1 400

1 300

1 200

1 100

1 000

MNIST (η = 6 × 10−7)

·10−2

SGHMC(a = 0.001)
SGNHT(a = 0.001)
SGHMC(a = 0.01)
SGNHT(a = 0.01)

1

2

3

4

iterations

Movielens1M (η = 6 × 10−7)

5
·104

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

Netﬂix (η = 6 × 10−7)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

ICML (η = 6 × 10−5)

r
o
r
r
E

t
s
e
T

E
S
M
R

t
s
e
T

E
S
M
R

t
s
e
T

6

5

4

3

2

0.94

0.92

0.9

0.88

0.86

0.86

0.85

0.84

0.83

0.82

MNIST (η = 8 × 10−7)

·10−2

SGHMC(a = 0.001)
SGNHT(a = 0.001)
SGHMC(a = 0.01)
SGNHT(a = 0.01)

2

3
iterations

4

5
·104

Movielens1M (η = 8 × 10−7)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

Netﬂix (η = 8 × 10−7)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

0.2

0.4

0.6
iterations

0.8

1
·105

ICML (η = 8 × 10−5)

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

1

2

3

4

iterations

5
·104

y
t
i
x
e
l
p
r
e
P

t
s
e
T

1 500

1 400

1 300

1 200

1 100

1 000

SGHMC(a = 0.01)
SGNHT(a = 0.01)
SGHMC(a = 0.1)
SGNHT(a = 0.1)

1

2

3

4

iterations

5
·104

Figure 3: The test error of MNIST (1st row)  test RMSE of Movielens1M (2nd row)  test RMSE
of Netﬂix (3rd row) and test perplexity of ICML (4th row) datasets with their standard deviations
(close to 0 in row 2 and 3) under various η and a.

6 Conclusion and Discussion

In this paper  we ﬁnd proper dynamics that adpatively ﬁt to the noise introduced by stochastic gra-
dients. Experiments show that our method is able to control the temperature  estimate the unknown
noise  and perform competitively in practice. Our method can be justiﬁed in continuous time by a
general theorem. The discretization of continuous SDEs  however  introduces bias. This issue has
been extensively studied by previous work such as [20  22  15  12]. The existency of an invariant
measure has been proved (e.g.  Theorem 3.2 [22] and Proposition 2.5 [12]) and the bound of the
error has been obtained (e.g  O(h2) for a symmetric splitting scheme [12]). Due to space limitation 
we leave a deeper discussion on this topic and a more rigorous justiﬁcation to future work.

Acknowledgments

We acknowledge Kevin P. Murphy and Julien Cornebise for helpful discussions and comments.

References
[1] S. Ahn  A. K. Balan  and M. Welling. Bayesian Posterior Sampling via Stochastic Gradient
Fisher Scoring. Proceedings of the 29th International Conference on Machine Learning  pages

8

1591–1598  2012.

[2] A. K. Balan  Y. Chen  and M. Welling. Austerity in MCMC Land: Cutting the Metropolis-
Hastings Budget. Proceedings of the 31st International Conference on Machine Learning 
2014.

[3] R. Bardenet  A. Doucet  and C. Holmes. Towards Scaling up Markov Chain Monte Carlo:
an Adaptive Subsampling Approach. Proceedings of the 31st International Conference on
Machine Learning  pages 405–413  2014.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet Allocation. J. Mach. Learn. Res. 

3:993–1022  March 2003.

[5] T. Chen  E. B. Fox  and C. Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. Proceed-

ings of the 31st International Conference on Machine Learning  2014.

[6] S. Duane  A. D. Kennedy  B. J. Pendleton  and D. Roweth. Hybrid Monte Carlo. Phys. Lett.

B  195:216–222  1987.

[7] Y. Fang  J. M. Sanz-Serna  and R. D. Skeel. Compressible Generalized Hybrid Monte Carlo.

J. Chem. Phys.  140:174108 (10 pages)  2014.

[8] M. Girolami and B. Calderhead. Riemann Manifold Langevin and Hamiltonian Monte Carlo

Methods. J. R. Statist. Soc. B  73  Part 2:123–214(with discussion)  2011.

[9] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. Stochastic Variational Inference. Journal

of Maching Learning Research  14(1):1303–1347  2013.

[10] A. M. Horowitz. A Generalized Guided Monte-Carlo Algorithm. Phys. Lett. B  268:247–252 

1991.

[11] B. Leimkuhler and C. Matthews. Rational Construction of Stochastic Numerical Methods for

Molecular Sampling. arXiv:1203.5428  2012.

[12] B. Leimkuhler  C. Matthews  and G. Stoltz. The Computation of Averages from Equilibrium

and Nonequilibrium Langevin Molecular Dynamics. IMA J Num. Anal.  2014.

[13] B. Leimkuhler and S. Reich. A Metropolis Adjusted Nos´e-Hoover Thermostat. Math. Mod-

ellinig Numer. Anal.  43(4):743–755  2009.

[14] D. Maclaurin and R. P. Adams. Fireﬂy Monte Carlo: Exact MCMC with Subsets of Data.

arXiv: 1403.5693  2014.

[15] J. C. Mattingly  A. M. Stuart  and M. Tretyakov. Convergence of Numerical Time-averaging

and Stationary Measures via Poisson Equations. SIAM J. Num. Anal.  48:552–577  2014.

[16] N. Metropolis  A. Rosenbluth  M. Rosenbluth  A. Teller  and E. Teller. Equation of State

Calculations by Fast Computing Machines. J. Chem. Phys.  21:1087–1092  1953.

[17] R. M. Neal. MCMC Using Hamiltonian Dynamics. arXiv:1206.1901  2012.
[18] S. Patterson and Y. W. Teh. Stochastic Gradient Riemannian Langevin Dynamics on the Prob-
ability Simplex. Advances in Neural Information Processing Systems 26  pages 3102–3110 
2013.

[19] H. Robbins and S. Monro. A Stochastic Approximation Method. The Annals of Mathematical

Statistics  22(3):400–407  1951.

[20] G. Roberts and R. Tweedie. Exponential Convergence of Langevin Distributions and Their

Discrete Approximations. Bernoulli  2:341–363  1996.

[21] R. Salakhutdinov and A. Mnih. Bayesian Probabilistic Matrix Factorization Using Markov
Chain Monte Carlo. Proceedings of the 25th International Conference on Machine Learning 
pages 880–887  2008.

[22] D. Talay. Second Order Discretization Schemes of Stochastic Differential Systems for the

Computation of the Invariant Law. Stochastics and Stochastics Reports  29:13–36  1990.

[23] M. E. Tuckerman. Statistical Mechanics: Theory and Molecular Simulation. Oxford Univer-

sity Press  2010.

[24] M. Welling and Y. W. Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics.

Proceedings of the 28th International Conference on Machine Learning  2011.

9

,Nan Ding
Youhan Fang
Ryan Babbush
Changyou Chen
Robert Skeel
Hartmut Neven
Rinu Boney
Norman Di Palo
Mathias Berglund
Alexander Ilin
Juho Kannala
Antti Rasmus
Harri Valpola