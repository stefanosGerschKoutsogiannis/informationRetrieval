2011,Probabilistic amplitude and frequency demodulation,A number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude- and frequency-demodulation (AFD)  there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed  we approach the problem using probabilistic inference. The new approach  called probabilistic amplitude and frequency demodulation (PAFD)  models instantaneous frequency using an auto-regressive generalization of the von Mises distribution  and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding  it outperforms previous approaches on synthetic and real signals in clean  noisy and missing data settings.,Probabilistic amplitude and frequency demodulation

Computational and Biological Learning Lab  Department of Engineering
University of Cambridge  Trumpington Street  Cambridge  CB2 1PZ  UK

Richard E. Turner∗

ret26@cam.ac.uk

Gatsby Computational Neuroscience Unit  University College London

Alexandra House  17 Queen Square  London  WC1N 3AR  UK

Maneesh Sahani

maneesh@gatsby.ucl.ac.uk

Abstract

A number of recent scientiﬁc and engineering problems require signals to be de-
composed into a product of a slowly varying positive envelope and a quickly vary-
ing carrier whose instantaneous frequency also varies slowly over time. Although
signal processing provides algorithms for so-called amplitude- and frequency-
demodulation (AFD)  there are well known problems with all of the existing
methods. Motivated by the fact that AFD is ill-posed  we approach the problem
using probabilistic inference. The new approach  called probabilistic amplitude
and frequency demodulation (PAFD)  models instantaneous frequency using an
auto-regressive generalization of the von Mises distribution  and the envelopes us-
ing Gaussian auto-regressive dynamics with a positivity constraint. A novel form
of expectation propagation is used for inference. We demonstrate that although
PAFD is computationally demanding  it outperforms previous approaches on syn-
thetic and real signals in clean  noisy and missing data settings.

1

Introduction

Amplitude and frequency demodulation (AFD) is the process by which a signal (yt) is decomposed
into the product of a slowly varying envelope or amplitude component (at) and a quickly varying
sinusoidal carrier (cos(φt))  that is yt = at cos(φt). In its general form this is an ill-posed problem
[1]  and so algorithms must impose implicit or explicit assumptions about the form of carrier and
envelope to realise a solution. In this paper we make the standard assumption that the amplitude
variables are slowly varying positive variables  and the derivatives of the carrier phase  ωt = φt −
φt−1 called the instantaneous frequencies (IFs)  are also slowly varying variables.
It has been argued that the subbands of speech are well characterised by such a representation [2  3]
and so AFD has found a range of applications in audio processing including audio coding [4  2] 
speech enhancement [5] and source separation [6]  and it is used in hearing devices [5]. AFD has
been used as a scientiﬁc tool to investigate the perception of sounds [7]. AFD is also of importance
in neural signal processing applications. Aggregate ﬁeld measurements such as those collected at the
scalp by electroencephalography (EEG) or within tissue as local ﬁeld potentials often exhibit tran-
sient sharp spectral lines at characteristic frequencies. Within each such band  both the amplitude of
the oscillation and the precise center frequencies may vary with time; and both of these phenomena
may reveal important elements of the mechanism by which the ﬁeld oscillation arises.

∗Richard Turner would like to thank the Laboratory for Computational Vision  New York University  New

York  NY 10003-6603  USA  where he carried out this research.

1

Despite the fact that AFD has found a wide range of important applications  there are well-known
problems with existing AFD algorithms [8  1  9  10  5]. Because of these problems  the Hilbert
method  which recovers an amplitude from the magnitude of the analytic signal  is still considered
to be the benchmark despite a number of limitations [11  12]. In this paper  we show examples of de-
modulation of synthetic  audio  and hippocampal theta rhythm signals using various AFD techniques
that highlights some of the anomalies associated with existing methods.

Motivated by the deﬁciencies in the existing methods this paper develops a probabilistic form of
AFD. This development begins in the next section where we reinterpret two existing probabilistic
algorithms in the context of AFD. The limitations of these methods suggest an improved model
(section 2) which we demonstrate on a range of synthetic and natural signals (sections 4 and 5).

1.1 Simple models for probabilistic amplitude and frequency demodulation

In this paper  we view demodulation as an estimation problem in which a signal is ﬁt with a sinusoid
of time-varying amplitude and phase 

yt = ℜ (at exp (iφt)) + ǫt.

y  that is p(ǫt) = Norm(ǫt; 0  σ2

(1)
The expression also includes a noise term which will be modeled as a zero-mean Gaussian with
variance σ2
y). We are interested in the situation where the IF of the
sinusoid varies slowly around a mean value ¯ω. In this case  the phase can be expressed in terms of
the integrated mean frequency and a small perturbation  φt = ¯ωt + θt.
Clearly  the problem of inferring at and θt from yt is ill-posed  and results will depend on the
speciﬁcation of prior distributions over the amplitude and phase perturbation variables. Our goal in
this paper is to specify such prior distributions directly  but this will require the development of new
techniques to handle the resulting non-linearities. A simpler alternative is to generate the sinusoidal
signal from a rotating two-dimensional phasor. For example  re-parametrizing the likelihood in
terms of the components x1 t = at cos(θt) and x2 t = at sin(θt)  yields a linear likelihood function
yt = at (cos(¯ωt) cos(θt) − sin(¯ωt) sin(θt)) + ǫt = cos(¯ωt)x1 t − sin(¯ωt)x2 t + ǫt = wT
xt + ǫt.
Here the phasor components  which have been collected into a vector xT
t = [x1 t  x2 t]  are multiplied
by time-varying weights  wT
t = [cos(¯ωt)  − sin(¯ωt)]. To complete the model  prior distributions can
be now be speciﬁed over xt. One choice that results in a particularly simple inference algorithm is
a Gaussian one-step auto-regressive (AR(1)) prior 

t

p(xk t|xk t−1) = Norm(xk t; λxk t−1  σ2

(2)
When the dynamical parameter tends to unity (λ → 1) and the dynamical noise variance to zero
(σ2
x → 0)  the dynamics become very slow  and this slowness is inherited by the phase perturbations
and amplitudes. This model is an instance of the Bayesian Spectrum Estimation (BSE) model [13]
(when λ = 1)  but re-interpreted in terms of amplitude- and frequency-modulated sinusoids  rather
than ﬁxed frequency basis functions. As the model is a linear Gaussian state space model  exact
inference proceeds via the Kalman smoothing algorithm.

x).

Before discussing the properties of BSE in the context of ﬁtting amplitude- and frequency-modulated
sinusoids  we derive an equivalent model by returning to the likelihood function (eq. 1). Now the
full complex representation of the sinusoid is retained. As before  the real part corresponds to the
observed data  but the imaginary part is now treated explicitly as missing data 

yt = ℜ (x1 t cos(¯ωt) − x2 t sin(¯ωt) + ix1 t sin(¯ωt) + ix2 t cos(¯ωt)) + ǫt.

(3)
The new form of the likelihood function can be expressed in vector form  yt = [1  0]zt + ǫt  using a
new set of variables  zt  which are rotated versions of the original variables  zt = R(¯ωt)xt where

cos(θ) (cid:21) .
R(θ) =(cid:20) cos(θ) − sin(θ)

sin(θ)

(4)

An auto-regressive expression for the new variables  zt  can now be found using the fact that rotation
matrices commute  R(θ1 + θ2) = R(θ1)R(θ2) = R(θ2)R(θ1)  together with expression for the
dynamics of the original variables  xt (eq. 2) 

zt = λR(¯ω)R(¯ω(t − 1))xt−1 + R(¯ωt)ǫt = λR(¯ω)zt−1 + ǫ

(5)
T
t iRT(¯ωt) = σ2
where the noise is a zero mean Gaussian with covariance hǫ
xI.
This equivalent formulation of the BSE model is called the Probabilistic Phase Vocoder (PPV) [14].
Again exact inference is possible using the Kalman smoothing algorithm.

′
t
′T
t i = R(¯ωt)hǫt ǫ

t ǫ

′

2

1.2 Problems with simple models for probabilistic amplitude and frequency demodulation

BSE-PPV is used to demodulate synthetic and natural signals in Figs. 1  2 and 7. The decomposition
is compared to the Hilbert method. These examples immediately reveal several problems with BSE-
PPV. Perhaps most unsatisfactory is the fact that the IF estimates are often ill behaved  to the extent
that they go negative  especially in regions where the amplitude of the signal is low. It is easy to
understand why this occurs by considering the prior distribution over amplitude and phase implied
by our choice of prior distribution over xt (or equivalently over zt) 

p(at  φt|at−1  φt−1) =

at

2πσ2
x

exp(cid:20)−

1
2σ2

λ
σ2
x

atat−1 cos(φt − φt−1 − ¯ω)(cid:21) . (6)

t + λ2a2

x (cid:0)a2

t−1(cid:1) +

Phase and amplitude are dependent in the implied distribution  which is conditionally a uniform
distribution over phase when the amplitude is zero and a strongly peaked von Mises distribution
[15] when the amplitude is large. Consequently  the model favors more highly variable IFs at low
amplitudes. In some applications this may be desirable  but for signals like sounds it presents a
problem. First it may assign substantial probability to unphysical negative IFs. Second  the same
noiseless signal at different intensities will yield different estimated IF content. Third  the complex
coupling makes it difﬁcult to select domain-appropriate time-scale parameters. Consideration of
IF reveals yet another problem. When the phase-perturbations vary slowly (λ → 1)  there is no
correlation between successive IFs (hωtωt−1i − hωtihωt−1i → 0). One of the main goals of the
model was to capture correlated IFs through time  and the solution is to move to priors with higher
order temporal dependencies.

In the next section we will propose a new model for PAFD which addresses these problems  retaining
the same likelihood function  but modifying the prior to include independent distributions over the
phase and amplitude variables.

t
r
e
b
l
i

H

V
P
P
E
S
B

/

D
F
A
P

2

0

−2

2 

0 

−2

2 

0 

−2

 

y
a
ˆa
ˆy

z
H

/
 

y
c
n
e
u
q
e
r
f
 

100

50

0

 
0

 

ω
ˆω
0.35

0.05

0.1

0.15

0.2

0.25

0.3

 
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

100

50

0

100

50

0

0

0.05

0.1

0.15

0.2
time /s

0.25

0.3

0.35

0

0.05

0.1

0.15

0.2
time /s

0.25

0.3

0.35

Figure 1: Comparison of AFD methods on a sinusoidally amplitude- and frequency-modulated sinu-
soid in broad-band noise. Estimated values are shown in red. The gray areas show the region where
the true amplitude falls below the noise ﬂoor (a < σy) and the estimates become less accurate. See
section 4 for details.

2 PAFD using Auto-regressive and generalized von Mises distributions

We have argued that the amplitude and phase variables in a model for PAFD should be indepen-
dently parametrized  but that this introduces difﬁculties as the likelihood is highly non-linear in
these variables. This section and the next develop the tools necessary to handle this non-linearity.

3

s
e
p
o
l
e
v
n
e

4

2

0

−2

−4

0

3000

2000

1000

0

0

2500

2000

1500

1000

z
H

/

y
c
n
e
u
q
e
r
f

z
H

/
y
c
n
e
u
q
e
r
f

0.05

0.1

0.15

0.2

0.25

0.05

0.1

0.15

0.2

0.25

0

0.05

0.1

0.15
time /s

0.2

0.25

Figure 2: AFD of a starling song. Top: The original waveform with estimated envelopes  shifted
apart vertically to aid visualization. The light gray bar indicates the problematic low amplitude
region. Bottom panels: IF estimates superposed onto the spectrum of the signal. PAFD tracks the
FM/AM well  but the other methods have artifacts.

An important initial consideration is whether to use a representation for phase which is wrapped  θ ∈
(−π  π]  or unwrapped  θ ∈ R. Although the latter has the advantage of implying simpler dynamics 
it leads to a potential inﬁnity of local modes at multiples of 2π making inference extremely difﬁcult.
It is therefore necessary to work with wrapped phases and a sensible starting point for a prior is thus
the von Mises distribution 

p(θ|k  µ) =

1

2πI0(k)

exp(k cos(θ − µ)) = vonMises(θ; k  µ).

(7)

The two parameters  the concentration (k) and the mean (µ)  determine the circular variance and
mean of the distribution respectively. The normalizing constant is given by a modiﬁed Bessel func-
tion of the second kind  I0(k). Crucially for our purposes  the von Mises distribution can be obtained
by taking a bivariate isotropic Gaussian with an arbitrary mean  and conditioning onto the unit-circle
(this connects with BSE-PPV  see eq. 6). The Generalized von Mises distribution is formed in an
identical way when the bivariate Gaussian is anisotropic [16]. These constructions suggest a simple
extension to time-series data by conditioning a temporal bivariate Gaussian time-series onto the unit
circle at all sample times. For example  when two independent Gaussian AR(2) distributions are
used to construct the prior we have 

T

2

1(x2

1 t + x2

2 t = 1)

Norm(xm t; λ1xm t−1 + λ2xm t−2  σ2

x).

(8)

where 1(x2
change of variables x1 t = cos(θt)  x2 t = sin(θt) this yields 

2 t = 1) is an indicator function representing the unit circle constraint. Upon a

1 t + x2

p(x1:2 1:T ) ∝

Yt=1

Ym=1

p(θ1:T |k1  k2) ∝

T

Yt=1

exp (k1 cos(θt − θt−1) + k2 cos(θt − θt−2))  

(9)

4

x and k2 = λ2/σ2

where k1 = λ1(1 − λ2)/σ2
x. One of the attractive features of this prior is that when
it is combined with the likelihood (eq. 1) the resulting posterior distribution over phase variables
is a temporal version of the Generalized von Mises distribution. That is  it can be expressed as a
bivariate anisotropic Gaussian  which is constrained to the unit circle. It is this representation which
will prove essential for inference.

Having established a candidate prior over phases  we turn to the amplitude variables. With one eye
upon the fact that the prior over phases can be interpreted as product of a Gaussian and a constraint 
we employ a prior of a similar form for the amplitude variables; a truncated Gaussian AR(τ ) process 

p(a1:T |λ1:τ   σ2) ∝

T

Yt=1

1(at ≥ 0) Norm at;

λt′ at−t′   σ2! .

(10)

τ

Xt′=1

The model formed from equations 1  9 and 10 will be termed Probabilistic Amplitude and Frequency
Demodulation. PAFD is closely related to the BSE-PPV model [13  14]. Moreover  when the
phase variables are drawn from a uniform distribution (k1 = k2 = 0) it reduces to the convex
amplitude demodulation model [17]  which itself is a form of probabilistic amplitude demodulation
[18  19  20]. The AR prior over phases has also been used in a regression setting [21].

3

Inference via expectation propagation

The PAFD model introduced in the last section contains three separate types of non-linearity: the
multiplicative interaction in the likelihood  the unit circle constraint  and the positivity constraint. Of
these  it is the circular constraint which is most challenging as the development of general purpose
machine learning methods for handling hard  non-convex constraints is an open research problem.
Following [22]  we propose a novel method which uses expectation propagation (EP) [23] to replace
the hard constraints with soft  local  Gaussian approximations which are iteratively reﬁned.

In order to apply EP  the model is ﬁrst rewritten into a simpler form. Making use of the
fact that an AR(τ ) process can be rewritten as an equivalent multi-dimensional AR(1) model
with τ states  we concatenate the latent variables into an augmented state vector  sT
t =
[at  at−1  . . .   at−τ +1  x1 t  x2 t  x1 t−1  x2 t−1]  and express the model as a product of clique po-
tentials in terms of this variable 

T

p(y1:T   s1:T ) ∝

πt(st  st−1)ψt(s1 t  s1+τ t  s2+τ t)  where πt(st  st−1) = Norm(st; Λsst−1  Σs) 

ψt(at  x1 t  x2 t) = Norm(cid:0)yt; at(cos(¯ωt)x1 t − sin(¯ωt)x2 t)  σ2

(See the supplementary material for details of the dynamical matrices Λs and Σs). In this new form
the constraints have been incorporated with the non-linear likelihood into the potential ψt  leav-
ing a standard Gaussian dynamical potential πt(st  st−1). Using EP we approximate the posterior
distribution using a product of forward  backward and constrained-likelihood messages [24] 

y(cid:1) 1(at ≥ 0)1(x2

1 t + x2

2 t = 1).

T

T

q(s1:T ) =

αt(st)βt(st) ˜ψt(a1 t  x1 t  x2 t) =

qt(st).

(11)

Yt=1

Yt=1

Yt=1

The messages should be interpreted as follows: αt(st) is the effect of πt(st−1  st) and q(st−1)
on the belief q(st)  whilst βt(st) is the effect of πt+1(st  st+1) and q(st+1) on the belief q(st).
Finally  ˜ψt(a1 t  x1 t  x2 t) is the effect of the likelihood and the constraints on the belief q(st). All
of these messages will be un-normalized Gaussians. The updates for the messages can be found by
removing the messages from q(s1:T ) that correspond to the effect of a particular potential. These
messages are replaced by the corresponding potential. The deleted messages are then updated by
moment matching the two distributions. The updates for the forward and backward messages are
a straightforward application of EP and result in updates that are nearly identical to those used for
Kalman smoothing. The updates for the constrained likelihood potential are more complicated:

update ˜ψt such that q(xt)

(12)
The difﬁculty is the moment computation which we evaluate in two stages. First  we integrate
over the amplitude variable  which involves computing the moments of a truncated Gaussian and

MOM
= ˆpψ(st) = αt(st)βt(st)ψt(at  x1 t  x2 t).

5

is therefore computationally efﬁcient. Second  we numerically integrate over the one dimensional
phase variable. For the details we again refer the reader to the supplementary material.

A standard forward-backward message update schedule was used. Adaptive damping improved
the numerical stability of the algorithm substantially. The computational complexity of PAFD is

O(cid:0)T (N + τ 3)(cid:1) where N are the number of points used to compute the integral over the phase

variable. For the experiments we used a second order process over the amplitude variables (τ = 2)
and N = 1000 integration points. In this case  the 16-32 forward-backward passes required for
convergence took one minute on a modern laptop computer for signals of length T = 1000.

4 Application to synthetic signals

One of the main challenges posed by the evaluation of AFD algorithms is that the ground truth
for real-world signals is unknown. This means that a quantitative comparison of different schemes
must take an indirect approach. The ﬁrst set of evaluations presented here uses synthetic signals  for
which the ground truth is known. In particular  we consider amplitude- and frequency-modulated
dθ
dt = ¯f + ∆f sin(2πff t)  which have
sinusoids  yt = at cos(θt) where at = 1 + sin(2πfat) and 1
2π
been corrupted by Gaussian noise. Fig. 1 compares AFD of one such signal ( ¯f = 50Hz  fa = 8Hz 
ff = 5Hz and ∆f = 25Hz) by the Hilbert  BSE-PPV and PAFD methods. Fig. 3 summarizes the
results at different noise levels in terms of the signal to noise ratio (SNR) of the estimated variables
t=1 (at − ˆat)2. PAFD
consistently outperforms the other methods by this measure. Furthermore  Fig. 4 demonstrates that
PAFD can be used to accurately reconstruct missing sections of this signal  outperforming BSE-PPV.

and the reconstructed signal  i.e. SNR(a) = 10 log10PT

t=1 a2

t − 10 log10PT

120

100

80

60

40

20

0

 

B
d
/
ˆa
R
N
S

 

100

B
d
/

ˆω
R
N
S

50

0

−50

Hilbert
BSE−PPV
PAFD

10

20

30

40

SNR signal /dB

∞

80

60

40

20

0

B
d
/

ˆy
R
N
S

10

20

30

40

SNR signal /dB

∞

10

20

30

40

50

SNR signal /dB

Figure 3: Noisy synthetic data. SNR of estimated variables as a function of the SNR of the signal.
Envelopes (left)  IFs (center) and denoised signal (right). Solid markers denote examples in Fig. 1.

5 Application to real world signals

Having validated PAFD on simple synthetic examples  we now consider real-world signals. Bird-
song is used as a prototypical signal as it has strong frequency-modulation content. We isolate a
300ms component of a starling song using a bandpass ﬁlter and apply AFD. Fig. 2 shows that PAFD
can track the underlying frequency modulation even though there is noise in the signal which causes
the other methods to fail. This example forms the basis of two important robustness and consistency
tests. In the ﬁrst  spectrally matched noise is added to the signal and the IFs and amplitudes are re-
estimated and compared to those derived from the clean signal. Fig. 5 shows that the PAFD method
is considerably more robust to this manipulation than both the Hilbert and BSE-PPV methods. In the
second test  regions of the signal are removed and the model’s predictions for the missing regions
are compared to the estimates derived from the clean signal (see ﬁg. 6). Once again PAFD is more
accurate. As a ﬁnal test of PAFD we consider the important neuroscientiﬁc task of estimating the
phase  equivalently the IF  of theta oscillations in an EEG signal. The EEG signal typically contains
broadband noise and so a conventional analysis applies a band-pass ﬁlter before using the Hilbert
method to estimate the IF. Although this improves the estimates markedly  the noise component
cannot be completely eradicated which leads to artifacts in the IF estimates (see Fig. 7). In contrast

6

B
d
/

ˆy
R
N
S

80

60

40

20

0

−20

0

 

PAFD
BSE−PPV

40
20
time /ms

60

80

60

40

20

0

B
d
/

ˆω
R
N
S

−20

0

40
20
time /ms

60

0.05

0.1

0.15

0.2

0.25

0.3

0.35

B
d
/
ˆa
R
N
S

100

50

0

−50

 
0

 
0

2

0

−2

2

0

−2

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

time /s

 

y
ˆy
a
0.4
ˆa

z
H

/
 
.

q
e
r
f

z
H

/
 
.

q
e
r
f

100

50

0

 
0

100

50

0

0

40
20
time /ms

60

 

ω
ˆω

0.2

0.4

0.2

time /s

0.4

Figure 4: Missing synthetic data experiments. TOP: SNR of estimated variables as a function of gap
duration in the input signal. Envelopes (left)  IFs (center) and denoised signal (right). Solid markers
indicate the examples shown in the bottom rows of the ﬁgure. BOTTOM: Two examples of PAFD
reconstruction. Light gray regions indicate missing sections of the signal.

40
35
30
25
20

15
10

B
d
/
ˆa
R
N
S

 

10

15

Hilbert
BSE−PPV
PAFD

20

25

30
SNR signal /dB

 

B
d
/

ˆω
R
N
S

20
15
10
5
0
−5
−10

35

10

15

20

25

30
SNR signal /dB

35

Figure 5: Noisy bird song experiments. SNR of estimated variables as compared to those estimated
from the clean signal  as a function of the SNR of the input signal. Envelopes (left)  IFs (right).

PAFD returns sensible estimates from both the ﬁltered and original signal. Critically  both estimates
are similar to one another suggesting the new estimation scheme is reliable.

6 Conclusion

Amplitude and frequency demodulation is a difﬁcult  ill-posed estimation problem. We have devel-
oped a new inferential solution called probabilistic amplitude and frequency demodulation which
employs a von Mises time-series prior over phase  constructed by conditioning a bivariate Gaussian
auto-regressive distribution onto the unit circle. The construction naturally leads to an expectation
propagation inference scheme which approximates the hard constraints using soft local Gaussians.

7

60

40

20

0

 

B
d
/
ˆa
R
N
S

2

0

−2

 
0

2

0

−2

0

 
PAFD
BSE−PPV

B
d
/

ˆω
R
N
S

60

40

20

0

B
d
/

ˆy
R
N
S

40

20

0

 2.8

time /ms

12.5

 2.8

time /ms

12.5

0.005

0.01

0.015

0.02

0.025

 

y
ac
ˆy
0.03
ˆa

z
H

/
 
.

q
e
r
f

z
H

/
 
.

q
e
r
f

0.005

0.01

0.015
time /s

0.02

0.025

0.03

 2.8

time /ms

12.5

 

ωc
ˆω

0.01

0.02

0.03

0.02

0.01
time /s

0.03

3000

2500

2000

1500

3000

2500

2000

1500

 
0

0

Figure 6: Missing natural data experiments. TOP: SNR of estimated variables as a function of gap
duration in the input signal. Envelopes (left)  IFs (center) and denoised signal (right). Solid markers
indicate the examples shown in the bottom rows of the ﬁgure. BOTTOM: Two examples of PAFD
reconstruction. Light gray regions indicate missing sections of the signal.

s
e
p
o
l
e
v
n
e

2

0

−2

z
H

/
 
y
c
n
e
u
q
e
r
f
 

z
H

/
 

y
c
n
e
u
q
e
r
f
 

10

5

0

10

5

0

 
0

 
0

 
0

0.5

0.5

0.5

 

y
ˆaPAFD
ˆaHE
ˆaPPV-BSE
1.5
 

ˆωHE
ˆωBSE-PPV

1.5
 

ˆωPAFD

1.5

s
e
p
o
l
e
v
n
e

2

0

−2

z
H

/
 
y
c
n
e
u
q
e
r
f
 

z
H

/
 

y
c
n
e
u
q
e
r
f
 

0

0

0

10

5

0

10

5

0

1

1

1

0.5

0.5

0.5

1

1

1

1.5

1.5

1.5

time /s

time /s

Figure 7: Comparison of AFD methods on EEG data. The left hand side shows estimates derived
from the raw EEG signal  whilst the right shows estimates derived from a band-pass ﬁltered version.
The gray areas show the region where the true amplitude falls below the noise ﬂoor (a < σy)  where
conventional methods fail.

We have demonstrated the utility of the new method on synthetic and natural signals  where it outper-
formed conventional approaches. Future research will consider extensions of the model to multiple
sinusoids  and learning the model parameters so that the algorithm can adapt to novel signals.
Acknowledgments
Richard Turner was funded by the EPRC  and Maneesh Sahani by the Gatsby Charitable Foundation.

8

References

[1] P. J. Loughlin and B. Tacer. On the amplitude- and frequency-modulation decomposition of signals. The

Journal of the Acoustical Society of America  100(3):1594–1601  1996.

[2] J. L. Flanagan. Parametric coding of speech spectra. The Journal of the Acoustical Society of America 

68:412–419  1980.

[3] P. Clark and L.E. Atlas. Time-frequency coherent modulation ﬁltering of nonstationary signals. Signal

Processing  IEEE Transactions on  57(11):4323 –4332  nov. 2009.

[4] J. L. Flanagan and R. M. Golden. Phase vocoder. Bell System Technical Journal  pages 1493–1509  1966.
[5] S. M. Schimmel. Theory of Modulation Frequency Analysis and Modulation Filtering  with Applications

to Hearing Devices. PhD thesis  University of Washington  2007.

[6] L. E. Atlas and C. Janssen. Coherent modulation spectral ﬁltering for single-channel music source sepa-

ration. In Proceedings of the IEEE Conference on Acoustics Speech and Signal Processing  2005.

[7] Z. M. Smith  B. Delgutte  and A. J. Oxenham. Chimaeric sounds reveal dichotomies in auditory percep-

tion. Nature  416(6876):87–90  2002.

[8] J. Dugundji. Envelopes and pre-envelopes of real waveforms. IEEE Transactions on Information Theory 

4:53–57  1958.

[9] O. Ghitza. On the upper cutoff frequency of the auditory critical-band envelope detectors in the context

of speech perception. The Journal of the Acoustical Society of America  110(3):1628–1640  2001.

[10] F. G. Zeng  K. Nie  S. Liu  G. Stickney  E. Del Rio  Y. Y. Kong  and H. Chen. On the dichotomy in
auditory perception between temporal envelope and ﬁne structure cues (L). The Journal of the Acoustical
Society of America  116(3):1351–1354  2004.

[11] D. Vakman. On the analytic signal  the Teager-Kaiser energy algorithm  and other methods for deﬁning

amplitude and frequency. IEEE Journal of Signal Processing  44(4):791–797  1996.

[12] G. Girolami and D. Vakman. Instantaneous frequency estimation and measurement: a quasi-local method.

Measurement Science and Technology  13(6):909–917  2002.

[13] Y. Qi  T. P. Minka  and R. W. Picard. Bayesian spectrum estimation of unevenly sampled nonstationary

data. In International Conference on Acoustics  Speech  and Signal Processing  2002.

[14] A. T. Cemgil and S. J. Godsill. Probabilistic Phase Vocoder and its application to Interpolation of Missing

Values in Audio Signals. In 13th European Signal Processing Conference  Antalya/Turkey  2005.

[15] C. Bishop. Pattern Recognition and Machine Learning. Springer  2006.
[16] R. Gatto and S. R. Jammalamadaka. The generalized von mises distribution. Statistical Methodology 

4:341–353  2007.

[17] G. Sell and M. Slaney. Solving demodulation as an optimization problem. IEEE Transactions on Audio 

Speech and Language Processing  18:2051–2066  November 2010.

[18] R. E. Turner and M. Sahani. Probabilistic amplitude demodulation. In Independent Component Analysis

and Signal Separation  pages 544–551  2007.

[19] R. E. Turner and M. Sahani. Statistical inference for single- and multi-band probabilistic amplitude
demodulation. In Proceedings of the IEEE International Conference on Acoustics  Speech  and Signal
Processing (ICASSP)  pages 5466–5469  2010.

[20] R. E. Turner and M. Sahani. Demodulation as probabilistic inference.

Speech and Language Processing  2011.

IEEE Transactions on Audio 

[21] J. Breckling. The analysis of directional time series: Application to wind speed and direction. Springer-

Verlag  1989.

[22] J. P. Cunningham. Algorithms for Understanding Motor Cortical Processing and Neural Prosthetic Sys-
tems. PhD thesis  Stanford University  Department of Electrical Engineering  (Stanford  California  USA 
2009.

[23] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  MIT Media Lab  2001.
[24] T. Heskes and O. Zoeter. Expectation propagation for approximate inference in dynamic bayesian net-

works. In A. Darwiche and N. Friedman  pages 216–233. Morgan Kaufmann Publishers  2002.

9

,Remi Munos
Tom Stepleton
Anna Harutyunyan
Marc Bellemare
Yanjun Li
Yoram Bresler