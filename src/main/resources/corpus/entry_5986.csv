2010,b-Bit Minwise Hashing for Estimating Three-Way Similarities,Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits  b-bit minwise hashing only stores the lowest b bits (where b>= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that $b$-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.,b-Bit Minwise Hashing for Estimating Three-Way Similarities

Ping Li

Dept. of Statistical Science

Cornell University

Arnd Christian K¨onig

Microsoft Research
Microsoft Corporation

Wenhao Gui

Dept. of Statistical Science

Cornell University

Abstract

Computing1 two-way and multi-way set similarities is a fundamental problem.
This study focuses on estimating 3-way resemblance (Jaccard similarity) using
b-bit minwise hashing. While traditional minwise hashing methods store each
hashed value using 64 bits  b-bit minwise hashing only stores the lowest b bits
(where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work
on 2-way similarity is technically non-trivial. We develop the precise estimator
which is accurate and very complicated; and we recommend a much simpliﬁed
estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing
can normally achieve a 10 to 25-fold improvement in the storage space required
for a given estimator accuracy of the 3-way resemblance.

1 Introduction
The efﬁcient computation of the similarity (or overlap) between sets is a central operation in a variety
of applications  such as word associations (e.g.  [13])  data cleaning (e.g.  [40  9])  data mining
(e.g.  [14])  selectivity estimation (e.g.  [30]) or duplicate document detection [3  4]. In machine
learning applications  binary (0/1) vectors can be naturally viewed as sets. For scenarios where the
underlying data size is sufﬁciently large to make storing them (in main memory) or processing them
in their entirety impractical  probabilistic techniques have been proposed for this task.
Word associations (collocations  co-occurrences)
If one inputs a query NIPS machine learning 
all major search engines will report the number of pagehits (e.g.  one reports 829 003)  in addition to
the top ranked URLs. Although no search engines have revealed how they estimate the numbers of
pagehits  one natural approach is to treat this as a set intersection estimation problem. Each word can
be represented as a set of document IDs; and each set belongs to a very large space Ω. It is expected
that |Ω| > 1010. Word associations have many other applications in Computational Linguistics [13 
38]  and were recently used for Web search query reformulation and query suggestions [42  12].
Here is another example. Commercial search engines display various form of “vertical” content
(e.g.  images  news  products) as part of Web search. In order to determine from which “vertical”
to display information  there exist various techniques to select verticals. Some of these (e.g.  [29 
15]) use the number of documents the words in a search query occur in for different text corpora
representing various verticals as features. Because this selection is invoked for all search queries
(and the tight latency bounds for search)  the computation of these features has to be very fast.
Moreover  the accuracy of vertical selection depends on the number/size of document corpora that
can be processed within the allotted time [29]  i.e.  the processing speed can directly impact quality.
Now  because of the large number of word-combinations in even medium-sized text corpora (e.g. 
the Wikipedia corpus contains > 107 distinct terms)  it is impossible to pre-compute and store the
associations for all possible multi-term combinations (e.g.  > 1014 for 2-way and > 1021 for 3-way);
instead the techniques described in this paper can be used for fast estimates of the co-occurrences.
Database query optimization
Set intersection is a routine operation in databases  employed for
example during the evaluation of conjunctive selection conditions in the presence of single-column
indexes. Before conducting intersections  a critical task is to (quickly) estimate the sizes of the
intermediate results to plan the optimal intersection order [20  8  25]. For example  consider the task
of intersecting four sets of record identiﬁers: A ∩ B ∩ C ∩ D. Even though the ﬁnal outcome will
be the same  the order of the join operations  e.g.  (A ∩ B) ∩ (C ∩ D) or ((A ∩ B) ∩ C) ∩ D  can
signiﬁcantly affect the performance  in particular if the intermediate results  e.g.  A∩B∩C  become
too large for main memory and need to be spilled to disk. A good query plan aims to minimize

1This work is supported by NSF (DMS-0808864)  ONR (YIP-N000140910911) and Microsoft.

the total size of intermediate results. Thus  it is highly desirable to have a mechanism which can
estimate join sizes very efﬁciently  especially for the lower-order (2-way and 3-way) intersections 
which could potentially result in much larger intermediate results than higher-order intersections.
Duplicate Detection in Data Cleaning: A common task in data cleaning is the identiﬁcation of
duplicates (e.g.  duplicate names  organizations  etc.) among a set of items. Now  despite the fact
that there is considerable evidence (e.g.  [10]) that reliable duplicate-detection should be based on
local properties of groups of duplicates  most current approaches base their decisions on pairwise
similarities between items only. This is in part due to the computational overhead associated with
more complex interactions  which our approach may help to overcome.
Clustering Most clustering techniques are based on pair-wise distances between the items to be
clustered. However  there are a number of natural scenarios where the afﬁnity relations are not
pairwise  but rather triadic  tetradic or higher (e.g. [1  43]). Again  our approach may improve the
performance in these scenarios if the distance measures can be expressed in the form of set-overlap.
Data mining
A lot of work in data mining has focused on efﬁcient candidate pruning in the
context of pairwise associations (e.g.  [14])  a number of such pruning techniques leverage minwise
hashing to prune pairs of items  but in many contexts (e.g.  association rules with more than 2 items)
multi-way associations are relevant; here  pruning based on pairwise interactions may perform much
less well than multi-way pruning.
1.1 Ultra-high dimensional data are often binary
For duplicate detection in the context of Web crawling/search  each document can be represented as
a set of w-shingles (w contiguous words); w = 5 or 7 in several studies [3  4  17]. Normally only the
abscence/presence (0/1) information is used  as a w-shingle rarely occurs more than once in a page
if w ≥ 5. The total number of shingles is commonly set to be |Ω| = 264; and thus the set intersection
corresponds to computing the inner product in binary data vectors of 264 dimensions. Interestingly 
even when the data are not too high-dimensional (e.g.  only thousands)  empirical studies [6  23  26]
achieved good performance using SVM with binary-quantized (text or image) data.
1.2 Minwise Hashing and SimHash
Two of the most widely adopted approaches for estimating set intersections are minwise hashing [3 
4] and sign (1-bit) random projections (also known as simhash) [7  34]  which are both special
instances of the general techniques proposed in the context of locality-sensitive hashing [7  24].
These techniques have been successfully applied to many tasks in machine learning  databases  data
mining  and information retrieval [18  36  11  22  16  39  28  41  27  5  2  37  7  24  21].
Limitations of random projections
The method of random projections (including simhash) is
limited to estimating pairwise similarities. Random projections convert any data distributions to
(zero-mean) multivariate normals  whose density functions are determined by the covariance matrix
which contains only the pairwise information of the original data. This is a serious limitation.
1.3 Prior work on b-Bit Minwise Hashing
Instead of storing each hashed value using 64 bits as in prior studies  e.g.  [17]  [35] suggested to
store only the lowest b bits. [35] demonstrated that using b = 1 reduces the storage space at least
by a factor of 21.3 (for a given accuracy) compared to b = 64  if one is interested in resemblance
≥ 0.5  the threshold used in prior studies [3  4]. Moreover  by choosing the value b of bits to be
retained  it becomes possible to systematically adjust the degree to which the estimator is “tuned”
towards higher similarities as well as the amount of hashing (random permutations) required.
[35] concerned only the pairwise resemblance. To extend it to the multi-way case  we have to solve
new and challenging probability problems. Compared to the pairwise case  our new estimator is
signiﬁcantly different. In fact  as we will show later  estimating 3-way resemblance requires b ≥ 2.
1.4 Notation

Figure 1: Notation for 2-way and 3-way set intersections.

a12f1aa23f3a13f2r1r3s12ss23r2s13Fig. 1 describes the notation used in 3-way intersections for three sets S1  S2  S3 ∈ Ω  |Ω| = D.

• f1 = |S1|  f2 = |S2|  f3 = |S3|.
• a12 = |S1 ∩ S2|  a13 = |S1 ∩ S3|  a23 = |S2 ∩ S3|  a = a123 = |S1 ∩ S2 ∩ S3|.
• r1 = f1
D   s = s123 = a
D .
D . s12 = a12
• u = r1 + r2 + r3 − s12 − s13 − s23 + s.

D   s23 = a23

D   s13 = a13

D   r2 = f2

D   r3 = f3

We deﬁne three 2-way resemblances (R12  R13  R23) and one 3-way resemblance (R) as:

R12 =

|S1 ∩ S2|
|S1 ∪ S2|   R13 =

|S2 ∩ S3|
|S2 ∪ S3|  
which  using our notation  can be expressed in various forms:

|S1 ∩ S3|
|S1 ∪ S3|   R23 =

R = R123 =

|S1 ∩ S2 ∩ S3|
|S1 ∪ S2 ∪ S3| .

(1)

(2)

s

Rij =

aij

sij

  i (cid:54)= j 

=

fi + fj − aij
f1 + f2 + f3 − a12 − a23 − a13 + a

ri + rj − sij
a

R =

(3)
Note that  instead of a123  s123  R123  we simply use a  s  R. When the set sizes  fi = |Si|  can be
assumed to be known  we can compute resemblances from intersections and vice versa:

r1 + r2 + r3 − s12 − s23 − s13 + s

=

=

.

s
u

aij =

Rij

1 + Rij

(fi + fj) 

a =

R

1 − R

(f1 + f2 + f3 − a12 − a13 − a23) .

Thus  estimating resemblances and estimating intersection sizes are two closely related problems.

1.5 Our Main Contributions

• We derive the basic probability formula for estimating 3-way resemblance using b-bit hash-
ing. The derivation turns out to be signiﬁcantly much more complex than the 2-way case.
This basic probability formula naturally leads to a (complicated) estimator of resemblance.
D ≈
0  but fi/fj = ri/rj may be still signiﬁcant) to develop a much simpliﬁed estimator  which
is desired in practical applications. This assumption of fi/D → 0 signiﬁcantly simpliﬁes
the estimator and frees us from having to know the cardinalities fi.

• We leverage the observation that many real applications involve sparse data (i.e.  ri = fi

• We analyze the theoretical variance of the simpliﬁed estimator and compare it with the
original minwise hashing method (using 64 bits). Our theoretical analysis shows that b-
bit minwise hashing can normally achieve a 10 to 25-fold improvement in storage space
(for a given estimator accuracy of the 3-way resemblance) when the set similarities are not
extremely low (e.g.  when the 3-way resemblance > 0.02). These results are particularly
important for applications in which only detecting high resemblance/overlap is relevant 
such as many data cleaning scenarios or duplicate detection.

The recommended procedure for estimating 3-way resemblances (in sparse data) is shown as Alg. 1.

Algorithm 1 The b-bit minwise hashing algorithm  applied to estimating 3-way resemblances in a
collection of N sets. This procedure is suitable for sparse data  i.e.  ri = fi/D ≈ 0.
Input: Sets Sn ∈ Ω = {0  1  ...  D − 1}  n = 1 to N.
Pre-processing phrase:
1) Generate k random permutations πj : Ω → Ω  j = 1 to k.
2) For each set Sn and permutation πj  store the lowest b bits of min (πj (Sn))  denoted by en t πj   t = 1 to b.
Estimation phrase: (Use three sets S1  S2  and S3 as an example.)
1) Compute ˆP12 b = 1
k
2) Compute ˆPb = 1
k

(cid:111)
t=1 1{e1 t πj = e2 t πj = e3 t πj}

. Similarly  compute ˆP13 b and ˆP23 b.

t=1 1{e1 t πj = e2 t πj}

(cid:110)(cid:81)b
(cid:110)(cid:81)b

(cid:80)k
(cid:80)k

(cid:111)

j=1

.

j=1
4b ˆPb−2b( ˆP12 b+ ˆP13 b+ ˆP23 b)+2

3) Estimate R by ˆRb =
4) If needed  the 2-way resemblances Rij b can be estimated as ˆRij b = 2b ˆPij b−1
2b−1

(2b−1)(2b−2)

.

.

2 The Precise Theoretical Probability Analysis
Minwise hashing applies k random permutations πj : Ω −→ Ω  Ω = {0  1  ...  D − 1}  and then
estimates R12 (and similarly other 2-way resemblances) using the following probability:

Pr (min(πj(S1)) = min(πj(S2))) =

|S1 ∩ S2|
|S1 ∪ S2| = R12.

This method naturally extends to estimating 3-way resemblances for three sets S1  S2  S3 ∈ Ω:

Pr (min(πj(S1)) = min(πj(S2)) = min(πj(S3))) =

|S1 ∩ S2 ∩ S3|
|S1 ∪ S2 ∪ S3| = R.

To describe b-bit hashing  we deﬁne the minimum values under π and their lowest b bits to be:

zi = min (π (Si))  

ei t = t-th lowest bit of zi.

(cid:195)

b(cid:89)

(cid:33)

To estimate R  we need to computes the empirical estimates of the probabilities Pij b and Pb  where

Pij b = Pr

1{ei t = ej t} = 1

 

Pb = P123 b = Pr

1{e1 t = e2 t = e3 t} = 1

.

t=1

t=1

The main theoretical task is to derive Pb. The prior work[35] already derived Pij b; see Appendix A.
To simplify the algebra  we assume that D is large  which is virtually always satisﬁed in practice.

(4)

(5)

(cid:33)

(cid:195)

b(cid:89)

(cid:33)

Theorem 1 Assume D is large.

(cid:195)

b(cid:89)

i=1

where u = r1 + r2 + r3 − s12 − s13 − s23 + s  and
Z =(s12 − s)A3 b +

(r3 − s13 − s23 + s)

r1 + r2 − s12

(r1 − s12 − s13 + s)

+(s23 − s)A1 b +

r2 + r3 − s23
+ [(r1 − s13)A3 b + (r3 − s13)A1 b]

+ [(r1 − s12)A2 b + (r2 − s12)A1 b]

(r2 − s12 − s23 + s)

r1 + r3 − s13

(r3 − s13 − s23 + s)

r1 + r2 − s12

G13 b

G12 b 

Pb = Pr

1{e1 i = e2 i = e3 i} = 1

=

Z
u

+ R =

Z + s

u

 

(6)

s12G12 b + (s13 − s)A2 b +

r1 + r3 − s13
s23G23 b + [(r2 − s23)A3 b + (r3 − s23)A2 b]

(r2 − s12 − s23 + s)

s13G13 b

(r1 − s12 − s13 + s)

r2 + r3 − s23

G23 b

Aj b =

rj (1 − rj )2b−1
1 − (1 − rj )2b

 

Gij b =

(ri + rj − sij )(1 − ri − rj + sij )2b−1

1 − (1 − ri − rj + sij )2b

i  j ∈ {1  2  3}  i (cid:54)= j.

 

Theorem 1 naturally suggests an iterative estimation procedure  by writing Eq. (6) as s = Pbu− Z.

Figure 2: Pb  for verifying the probability formula in Theorem 1. The empirical estimates and the
theoretical predictions essentially overlap regardless of the sparsity measure ri = fi/D.

A Simulation Study
For the purpose of verifying Theorem 1  we use three sets corresponding
to the occurrences of three common words (“OF”  “AND”  and “OR”) in a chunk of real world Web
crawl data. Each (word) set is a set of document (Web page) IDs which contained that word at least
once. The three sets are not too sparse and D = 216 sufﬁces to represent their elements. The ri = fi
D
values are 0.5697  0.5537  and 0.3564  respectively. The true 3-way resemblance is R = 0.47.

01002003004005000.460.480.50.520.540.560.58Sample size kPb D = 216b = 2b = 3b = 42 bits3 bits4 bitsTheoretical01002003004005000.460.480.50.520.540.560.58Sample size kPb D = 220b = 2b = 3b = 42 bits3 bits4 bitsTheoreticalWe can also increase D by mapping these sets into a larger space using a random mapping  with
D = 216  218  220  or 222. When D = 222  the ri values are 0.0089  0.0087  0.0056.
Fig. 2 presents the empirical estimates of the probability Pb  together with the theoretical predictions
by Theorem 1. The empirical estimates essentially overlap the theoretical predictions. Even though
the proof assumes D → ∞  D does not have to be too large for Theorem 1 to be accurate.
3 The Much Simpliﬁed Estimator for Sparse Data
The basic probability formula (Theorem 1) we derive could be too complicated for practical use. To
D ≈ 0 
obtain a simpler formula  we leverage the observation that in practice we often have ri = fi
even though both fi and D can be very large. For example  consider web duplicate detection [17].
Here  D = 264  which means that even for a web page with fi = 254 shingles (corresponding to the
D ≈ 0.001. Note that  even when ri → 0  the ratios  e.g.  r2
text of a small novel)  we still have fi
 
can be still large. Recall the resemblances (2) and (3) are only determined by these ratios.

r1

D using two real-life datasets: the UCI dataset containing 3 × 105
We analyzed the distribution of fi
NYTimes articles; and a Microsoft proprietary dataset with 106 news articles [19]. For the UCI-
NYTimes dataset  each document was already processed as a set of single words. For the anonymous
dataset  we report results using three different representations: single words (1-shingle)  2-shingles
(two contiguous words)  and 3-shingles. Table 1 reports the summary statistics of the fi

D values.

Table 1: Summary statistics of the fi

Data
3 × 105 UCI-NYTimes articles
106 Microsoft articles (1-shingle)
106 Microsoft articles (2-shingle)
106 Microsoft articles (3-shingle)

D values in two datasets
Std.
0.0011
0.00023
0.00005
0.00002

Mean
0.0022
0.00032
0.00004
0.00002

Median
0.0021
0.00027
0.00003
0.00002

For truly large-scale applications  prior studies [3  4  17] commonly used 5-shingles. This means
that real world data may be signiﬁcantly more sparse than the values reported in Table 1.
3.1 The Simpliﬁed Probability Formula and the Practical Estimator
Theorem 2 Assume D is large. Let T = R12 + R13 + R23. As r1  r2  r3 → 0 

Pb = Pr

1{e1 i = e2 i = e3 i} = 1

(2b − 1)(2b − 2)R + (2b − 1)T + 1

.

(7)

(cid:111)

(cid:33)

(cid:110)

=

1
4b

(cid:195)

b(cid:89)

i=1

Interestingly  if b = 1  then P1 = 1
contained. Hence  it is necessary to use b ≥ 2 to estimate 3-way similarities.
Alg. 1 uses ˆPb and ˆPij b to respectively denote the empirical estimates of the theoretical probabilities
Pb and Pij b. Assuming r1  r2  r3 → 0  the proposed estimator of R  denoted by ˆRb  is

4 (1 + T )  i.e.  no information about the 3-way resemblance R is

(cid:179)

(cid:180)

ˆP12 b + ˆP13 b + ˆP23 b
(2b − 1)(2b − 2)

+ 2

.

ˆRb =

4b ˆPb − 2b
(cid:110)

(cid:179)

(cid:111)
Theorem 3 Assume D is large and r1  r2  r3 → 0. Then ˆRb in (8) is unbiased with the variance
R − (2b − 1)(2b − 2)R2

4b − 6 × 2b + 10

1 + (2b − 3)T +

(cid:180)

(cid:179)

(cid:180)

1

V ar

ˆRb

=

1
k

(2b − 1)(2b − 2)

(8)

.

(9)

It is interesting to examine several special cases:

• b = 1: V ar( ˆR1) = ∞  i.e.  one must use b ≥ 2.
• b = 2: V ar( ˆR2) = 1
• b = ∞: V ar( ˆR∞) = 1

(cid:161)
1 + T + 2R − 6R2
k R(1 − R) = V ar( ˆRM ). ˆRM is the original minwise hashing esti-
mator for 3-way resemblance. In principle  the estimator ˆRM requires an inﬁnite precision
(i.e.  b = ∞). Numerically  V ar( ˆRM ) and V ar( ˆR64) are indistinguishable.

(cid:162)

6k

.

3.2 Simulations for Validating Theorem 3
We now present a simulation study for verifying Theorem 3  using the same three sets used in Fig. 2.
Fig. 3 presents the resulting empirical biases: E( ˆRb)−Rb. Fig. 4 presents the empirical mean square
errors (MSE = bias2+variance) together with the theoretical variances V ar( ˆRb) in Theorem 3.

Figure 3: Bias of ˆRb (8). We used 3 (word) sets: “OF”  “AND”  and “OR” and four D values: 216 
218  220  and 222. We conducted experiments using b = 2  3  and 4 as well as the original minwise
hashing (denoted by “M”). The plots verify that as ri decreases (to zero)  the biases vanish. Note
that the set sizes fi remain the same  but the relative values ri = fi

D decrease as D increases.

Figure 4: MSE of ˆRb (8). The solid curves are the empirical MSEs (=var+bias2) and the dashed
lines are the theoretical variances (9)  under the assumption of ri → 0. Ideally  we would like to see
the solid and dashed lines overlap. When D = 220 and D = 222  even though the ri values are not
too small  the solid and dashed lines almost overlap. Note that  at the same sample size k  we always
have V ar( ˆR2) > V ar( ˆR3) > V ar( ˆR4) > V ar( ˆRM )  where ˆRM is the original minwise hashing
estimator. We can see that  V ar( ˆR3) and V ar( ˆR4) are very close to V ar( ˆRM ).

We can summarize the results in Fig. 3 and Fig. 4 as follows:

• When the ri = fi

D values are large (e.g.  ri ≈ 0.5 when D = 216)  the estimates using
(8) can be noticeably biased. The estimation biases diminish as the ri values decrease. In
fact  even when the ri values are not small (e.g.  ri ≈ 0.05 when D = 220)  the biases are
already very small (roughly 0.005 when D = 220).
• The variance formula (9) becomes accurate when the ri values are not too large. For exam-
ple  when D = 218 (ri ≈ 0.1)  the empirical MSEs largely overlap the theoretical variances
which assumed ri → 0  unless the sample size k is large. When D = 220 (and D = 222) 
the empirical MSEs and theoretical variances overlap.
• For real applications  as we expect D will be very large (e.g.  264) and the ri values (fi/D)
will be very small  our proposed simple estimator (8) will be very useful in practice  be-
cause it becomes unbiased and the variance can be reliably predicted by (9).

4 Improving Estimates for Dense Data Using Theorem 1
While we believe the simple estimator in (8) and Alg. 1 should sufﬁce in most applications  we
demonstrate here that the sparsity assumption of ri → 0 is not essential if one is willing to use the
more sophisticated estimation procedure provided by Theorem 1.
By Eq. (6)  s = Pbu − Z  where Z contains s  sij  ri etc. We ﬁrst estimate sij (from the estimated
Rij) using the precise formula for the two-way case; see Appendix A. We then iteratively solve for
s using the initial guess provided by the estimator ˆRb in (8). Usually a few iterations sufﬁce.
Fig. 5 reports the bias (left most panel  only for D = 216) and MSE  corresponding to Fig. 3 and
Fig. 4. In Fig. 5  the solid curves are obtained using the precise estimation procedure by Theorem 1.
The dashed curves are the estimates using the simpliﬁed estimator ˆRb which assumes ri → 0.

0100200300400500−0.1−0.0500.05Sample size kBias D = 216b = 2b = 3b = 4M0100200300400500−0.03−0.02−0.0100.01Sample size kBias b = 4Mb = 3b = 2D = 2180100200300400500−10−505x 10−3Sample size kBias D = 220MM4b = 230100200300400500−10−505x 10−3Sample size kBias D = 222Mb = 2b = 431010050010−310−210−1Sample size kMean square error (MSE) D = 216b = 2b = 3b = 4M2 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error (MSE) D = 218b = 2M3342 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error (MSE) D = 220b = 2M342 bits3 bits4 bitsminwiseTheoretical1010050010−310−210−1Sample size kMean square error (MSE) D = 222b = 23M42 bits3 bits4 bitsminwiseTheoreticalEven when the data are not sparse  the precise estimation procedure provides unbiased estimates
as veriﬁed by the leftmost panel of Fig. 5. Using the precise procedure results in noticeably more
accurate estimates in non-sparse data  as veriﬁed by the second panel of Fig. 5. However  as long as
the data are reasonably sparse (the right two panels)  the simple estimator ˆRb in (8) is accurate.

Figure 5: The bias (leftmost panel) and MSE of the precise estimation procedure  using the same
data used in Fig. 3 and Fig. 4. The dashed curves correspond to the estimates using the simpliﬁed
estimator ˆRb in (8) which assumes ri → 0.

5 Quantifying the Improvements Using b-Bit Hashing

This section is devoted to analyzing the improvements of b-bit minwise hashing  compared to using
64 bits for each hashed value. Throughout the paper  we use the terms “sample” and “sample size”
(denoted by k). The original minwise hashing stores each “sample” using 64 bits (as in [17]). For
b-bit minwise hashing  we store each “sample” using b bits only. Note that V ar( ˆR64) and V ar( ˆRM )
(the variance of the original minwise hashing) are numerically indistinguishable.
As we decrease b  the space needed for each sample will be smaller; the estimation variance at
the same sample size k  however  will increase. This variance-space trade-off can be quantiﬁed by
× k  which is called the storage factor. Lower B(b) is more desirable. The
B(b) = b × Var
ratio B(64)

B(b) precisely characterizes the improvements of b-bit hashing compared to using 64 bits.

ˆRb

(cid:179)

(cid:180)

Fig. 6 conﬁrms the substantial improvements of b-bit hashing over the original minwise hashing
using 64 bits. The improvements in terms of the storage space are usually 10 (or 15) to 25-fold
when the sets are reasonably similar (i.e.  when the 3-way resemblance > 0.1). When the three sets
are very similar (e.g.  the top left panel)  the improvement will be even 25 to 30-fold.

Figure 6: B(64)
B(b)   the relative storage improvement of using b = 2  3  4  6 bits  compared to using 64
bits. Since the variance (9) contains both R and T = R12 + R13 + R23  we compare variances using
different T /R ratios. As 3R ≤ T always  we let T = αR  for some α ≥ 3. Since T ≤ 3  we know
R ≤ 3/α. Practical applications are often interested in cases with reasonably large R values.

0100200300400500−1−0.500.51x 10−3Sample size kBias D = 216Biasb = 3b = 21010050010−310−210−1Sample size kMean square error (MSE) D = 216b = 2b = 3b = 3b = 21010050010−310−210−1Sample size kMean square error (MSE) D = 218b = 2b = 3b = 2b = 31010050010−310−210−1Sample size kMean square error (MSE) b = 3b = 2D = 22000.20.40.60.81051015202530RStorage ratio ( B(64) / B(b) ) T = 3Rb = 2b = 4b = 3b = 600.10.20.30.40.50.60.70.805101520RStorage ratio B(64) / B(b) T = 4Rb = 2b = 3b = 4b = 6b = 200.10.20.30.40.505101520RStorage ratio B(64) / B(b) T = 6Rb = 3b = 2b = 44b = 2b = 600.050.10.150.20.250.3024681012RStorage ratio B(64) / B(b) T = 10Rb = 3b = 2b = 4b = 600.050.10.150246810RStorage ratio ( B(64) / B(b) ) T = 20Rb = 4b = 3b = 2b = 600.010.020.030.040.050.0601234567RStorage ratio ( B(64) / B(b) ) T = 50Rb = 4b = 3b = 2b = 66 Evaluation of Accuracy
We conducted a duplicate detection experiment on a public (UCI) collection of 300 000 NYTimes
news articles. The task is to identify 3-groups with 3-way resemblance R exceeding a threshold R0.
We used a subset of the data; the total number of 3-groups is about one billion. We experimented
with b = 2  4 and the original minwise hashing. Fig. 7 presents the precision curves for a represen-
tative set of thresholds R0’s. Just like in [35]  the recall curves are not shown because they could not
differentiate estimators. These curves conﬁrm the signiﬁcant improvement of using b-bit minwise
hashing when the threshold R0 is quite high (e.g.  0.3). In fact  when R0 = 0.3  using b = 4 re-
sulted in similar precisions as using the original minwise hashing (i.e.  a 64/4=16-fold reduction in
storage). Even when R0 = 0.1  using b = 4 can still achieve similar precisions as using the original
minwise hashing by only slightly increasing the sample size k.

Figure 7: Precision curves on the UCI collection of news data. The task is to retrieve news article
3-groups with resemblance R ≥ R0. For example  consider R0 = 0.2. To achieve a precision of
at least 0.8  2-bit hashing and 4-bit hashing require about k = 500 samples and k = 260 samples
respectively  while the original minwise hashing (denoted by M) requires about 170 samples.
7 Conclusion
In machine learning  high-
Computing set similarities is fundamental in many applications.
dimensional binary data are common and are equivalent to sets. This study is devoted to simul-
taneously estimating 2-way and 3-way similarities using b-bit minwise hashing. Compared to the
prior work on estimating 2-way resemblance [35]  the extension to 3-way is important for many
application scenarios (as described in Sec. 1) and is technically non-trivial.
For estimating 3-way resemblance  our analysis shows that b-bit minwise hashing can normally
achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy 
when the set similarities are not extremely low (e.g.  3-way resemblance > 0.02). Many applications
such as data cleaning and de-duplication are mainly concerned with relatively high set similarities.
For many practical applications  the reductions in storage directly translate to improvements in pro-
cessing speed as well  especially when memory latency is the main bottleneck  which  with the
advent of many-core processors  is more and more common.
Future work: We are interested in developing a b-bit version for Conditional Random Sampling
(CRS) [31  32  33]  which requires only one permutation (instead of k permutations) and naturally
extends to non-binary data. CRS is also provably more accurate than minwise hashing for binary
data. However  the analysis for developing the b-bit version of CRS appears to be very difﬁcult.
A Review of b-Bit Minwise Hashing for 2-Way Resemblance
Theorem 4 ([35]) Assume D is large.

(cid:33)

(cid:195)

b(cid:89)

where

P12 b = Pr
r2

C1 b = A1 b

i=1

+ A2 b

r1

r1 + r2

r1 + r2

1{e1 i = e2 i} = 1

= C1 b + (1 − C2 b) R12
r2

r1

  C2 b = A1 b

r1 + r2

+ A2 b

 

r1 + r2

A1 b =

r1 [1 − r1]2b−1
1 − [1 − r1]2b  
If r1  r2 → 0  P12 b = 1+(2b−1)R12
  where ˆP12 b is the
empirical observation of P12 b. If r1  r2 are not small  R12 is estimated by ( ˆP12 b−C1 b)/(1−C2 b).

and one can estimate R12 by 2b ˆP12 b−1
2b−1

r2 [1 − r2]2b−1
1 − [1 − r2]2b .

A2 b =

2b

010020030040050000.20.40.60.81Sample size kPrecision R0 = 0.1b=2b=4M010020030040050000.20.40.60.81Sample size kPrecision R0 = 0.2b=2Mb=4010020030040050000.20.40.60.81Sample size kPrecision R0 = 0.3b=2MM4References
[1] S. Agarwal  J. Lim  L. Zelnik-Manor  P. Perona  D. Kriegman  and S. Belongie. Beyond pairwise clustering. In CVPR  2005.
[2] M. Bendersky and W. B. Croft. Finding text reuse on the web. In WSDM  pages 262–271  Barcelona  Spain  2009.
[3] A. Z. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences  pages 21–29 

Positano  Italy  1997.

[4] A. Z. Broder  S. C. Glassman  M. S. Manasse  and G. Zweig. Syntactic clustering of the web. In WWW  pages 1157 – 1166  Santa Clara 

CA  1997.

[5] G. Buehrer and K. Chellapilla. A scalable pattern mining approach to web graph compression with communities. In WSDM  pages

95–106  Stanford  CA  2008.

[6] O. Chapelle  P. Haffner  and V. N. Vapnik. Support vector machines for histogram-based image classiﬁcation. 10(5):1055–1064  1999.
[7] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  pages 380–388  Montreal  Quebec  Canada  2002.
[8] S. Chaudhuri. An Overview of Query Optimization in Relational Systems. In PODS  pages 34–43  1998.
[9] S. Chaudhuri  V. Ganti  and R. Kaushik. A primitive operatior for similarity joins in data cleaning. In ICDE  2006.
[10] S. Chaudhuri  V. Ganti  and R. Motwani. Robust identiﬁcation of fuzzy duplicates. In ICDE  pages 865–876  Tokyo  Japan  2005.
[11] F. Chierichetti  R. Kumar  S. Lattanzi  M. Mitzenmacher  A. Panconesi  and P. Raghavan. On compressing social networks. In KDD 

pages 219–228  Paris  France  2009.

[12] K. Church. Approximate lexicography and web search. International Journal of Lexicography  21(3):325–336  2008.
[13] K. Church and P. Hanks. Word association norms  mutual information and lexicography. Computational Linguistics  16(1):22–29  1991.
[14] E. Cohen  M. Datar  S. Fujiwara  A. Gionis  P. Indyk  R. Motwani  J. D. Ullman  and C. Yang. Finding interesting associations without

support pruning. IEEE Trans. on Knowl. and Data Eng.  13(1)  2001.

[15] F. Diaz. Integration of News Content into Web Results. In WSDM  2009.
[16] Y. Dourisboure  F. Geraci  and M. Pellegrini. Extraction and classiﬁcation of dense implicit communities in the web graph. ACM Trans.

Web  3(2):1–36  2009.

[17] D. Fetterly  M. Manasse  M. Najork  and J. L. Wiener. A large-scale study of the evolution of web pages. In WWW  pages 669–678 

Budapest  Hungary  2003.

[18] G. Forman  K. Eshghi  and J. Suermondt. Efﬁcient detection of large-scale redundancy in enterprise ﬁle systems. SIGOPS Oper. Syst.

Rev.  43(1):84–91  2009.

[19] M. Gamon  S. Basu  D. Belenko  D. Fisher  M. Hurst  and A. C. K¨onig. Blews: Using blogs to provide context for news articles. In AAAI

Conference on Weblogs and Social Media  2008.

[20] H. Garcia-Molina  J. D. Ullman  and J. Widom. Database Systems: the Complete Book. Prentice Hall  New York  NY  2002.
[21] A. Gionis  D. Gunopulos  and N. Koudas. Efﬁcient and tunable similar set retrieval. In SIGMOD  pages 247–258  CA  2001.
[22] S. Gollapudi and A. Sharma. An axiomatic approach for result diversiﬁcation. In WWW  pages 381–390  Madrid  Spain  2009.
[23] M. Hein and O. Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability measures.

In AISTATS  pages 136–143 

Barbados  2005.

[24] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC  pages 604–613 

Dallas  TX  1998.

[25] Y. E. Ioannidis. The history of histograms (abridged). In VLDB  2003.
[26] Y. Jiang  C. Ngo  and J. Yang. Towards optimal bag-of-features for object categorization and semantic video retrieval. In CIVR  pages

494–501  Amsterdam  Netherlands  2007.

[27] N. Jindal and B. Liu. Opinion spam and analysis. In WSDM  pages 219–230  Palo Alto  California  USA  2008.
[28] K. Kalpakis and S. Tang. Collaborative data gathering in wireless sensor networks using measurement co-occurrence. Computer

Communications  31(10):1979–1992  2008.

[29] A. C. K¨onig  M. Gamon  and Q. Wu. Click-Through Prediction for News Queries. In SIGIR  2009.
[30] H. Lee  R. T. Ng  and K. Shim. Power-law based estimation of set similarity join size. In PVLDB  2009.
[31] P. Li and K. W. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics  33(3):305–

354  2007 (Preliminary results appeared in HLT/EMNLP 2005).

[32] P. Li  K. W. Church  and T. J. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS  pages

873–880  Vancouver  BC  Canada  2006.

[33] P. Li  K. W. Church  and T. J. Hastie. One sketch for all: Theory and applications of conditional random sampling. In NIPS  Vancouver 

BC  Canada  2008.

[34] P. Li  T. J. Hastie  and K. W. Church. Improving random projections using marginal information. In COLT  pages 635–649  Pittsburgh 

PA  2006.

[35] P. Li and A. C. K¨onig. b-bit minwise hashing. In WWW  pages 671–680  Raleigh  NC  2010.
[36] Ludmila  K. Eshghi  C. B. M. III  J. Tucek  and A. Veitch. Probabilistic frequent itemset mining in uncertain databases. In KDD  pages

1087–1096  Paris  France  2009.

[37] G. S. Manku  A. Jain  and A. D. Sarma. Detecting Near-Duplicates for Web-Crawling. In WWW  Banff  Alberta  Canada  2007.
[38] C. D. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. The MIT Press  Cambridge  MA  1999.
[39] M. Najork  S. Gollapudi  and R. Panigrahy. Less is more: sampling the neighborhood graph makes salsa better and faster. In WSDM 

pages 242–251  Barcelona  Spain  2009.

[40] S. Sarawagi and A. Kirpal. Efﬁcient set joins on similarity predicates. In SIGMOD  pages 743–754  2004.
[41] T. Urvoy  E. Chauveau  P. Filoche  and T. Lavergne. Tracking web spam with html style similarities. ACM Trans. Web  2(1):1–28  2008.
[42] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In CIKM  pages 479–488 

Napa Valley  California  USA  2008.

[43] D. Zhou  J. Huang  and B. Sch¨olkopf. Beyond pairwise classiﬁcation and clustering using hypergraphs. 2006.

,Cong Han Lim
Stephen Wright
Marco Fraccaro
Søren Kaae Sønderby
Ulrich Paquet
Ole Winther
Xiuming Zhang
Zhoutong Zhang
Chengkai Zhang
Josh Tenenbaum
Bill Freeman
Jiajun Wu