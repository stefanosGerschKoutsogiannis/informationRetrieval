2011,On Tracking The Partition Function,Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However  their use is often limited by an inability to estimate the partition function $Z$. In this paper  we exploit the gradient descent training procedure of restricted Boltzmann machines (a type of MRF) to {\bf track} the log partition function during learning. Our method relies on two distinct sources of information: (1) estimating the change $\Delta Z$ incurred by each gradient update  (2) estimating the difference in $Z$ over a small set of tempered distributions using bridge sampling. The two sources of information are then combined using an inference procedure similar to Kalman filtering.  Learning MRFs through Tempered Stochastic Maximum Likelihood  we can estimate $Z$ using no more temperatures than are required for learning. Comparing to both exact values and estimates using annealed importance sampling (AIS)  we show on several datasets that our method is able to accurately track the log partition function. In contrast to AIS  our method provides this estimate at each time-step  at a computational cost similar to that required for training alone.,On Tracking The Partition Function

Guillaume Desjardins  Aaron Courville  Yoshua Bengio

{desjagui courvila bengioy}@iro.umontreal.ca

D´epartement d’informatique et de recherche op´erationnelle

Universit´e de Montr´eal

Abstract

Markov Random Fields (MRFs) have proven very powerful both as density esti-
mators and feature extractors for classiﬁcation. However  their use is often limited
by an inability to estimate the partition function Z. In this paper  we exploit the
gradient descent training procedure of restricted Boltzmann machines (a type of
MRF) to track the log partition function during learning. Our method relies on
two distinct sources of information: (1) estimating the change ∆Z incurred by
each gradient update  (2) estimating the difference in Z over a small set of tem-
pered distributions using bridge sampling. The two sources of information are
then combined using an inference procedure similar to Kalman ﬁltering. Learn-
ing MRFs through Tempered Stochastic Maximum Likelihood  we can estimate
Z using no more temperatures than are required for learning. Comparing to both
exact values and estimates using annealed importance sampling (AIS)  we show
on several datasets that our method is able to accurately track the log partition
function. In contrast to AIS  our method provides this estimate at each time-step 
at a computational cost similar to that required for training alone.

˜q(x)
Z(β)

=

Z(β)

˜q(x).

x

Introduction

1
In many areas of application  problems are naturally expressed as a Gibbs measure  where the dis-
tribution over the domain X is given by  for x ∈ X :
exp{−βE(x)}

  with Z(β) =(cid:88)

q(x) =

(1)

E(x) is refered to as the “energy” of conﬁguration x  β is a free parameter known as the inverse
temperature and Z(β) is the normalization factor commonly refered to as the partition function. Un-
der certain general conditions on the form of E  these models are known as Markov Random Fields
(MRF)  and have been very popular within the vision and natural language processing communi-
ties. MRFs with latent variables – in particular restricted Boltzmann machines (RBMs) [9] – are
among the most popular building block for deep architectures [1]  being used in the unsupervised
initialization of both Deep Belief Networks [9] and Deep Boltzmann Machines [22].
As illustrated in Eq. 1  the partition function is computed by summing over all variable conﬁgura-
tions. Since the number of conﬁgurations scales exponentially with the number of variables  exact
calculation of the partition function is generally computationally intractable. Without the parti-
tion function  probabilities under the model can only be determined up to a multiplicative constant 
which seriously limits the model’s utility. One method recently proposed for estimating Z(β) is
annealed importance sampling (AIS) [18  23]. In AIS  Z(β) is approximated by the sum of a set of
importance-weighted samples drawn from the model distribution. With a large number of variables 
drawing a set of importance-weighted samples is generally subject to extreme variance in the im-
portance weights. AIS alleviates this issue by annealing the model distribution through a series of
slowly changing distributions that link the target model distribution to one where the log partition
function is tractable. While AIS is quite successful  it generally requires the use of tens of thou-
sands of annealing distributions in order to achieve accurate results. This computationally intensive

1

requirement renders AIS inappropriate as a means of maintaining a running estimate of the log par-
tition function throughout training. Yet  having ready access to this quantity throughout learning
opens the door to a range of possibilities. Likelihood could be used as a basis for model comparison
throughout training; early-stopping could be accomplished by monitoring an estimate of the likeli-
hood of a validation set. Another important application is in Bayesian inference in MRFs [17] where
we require the partition function for each value of the parameters in the region of support. Track-
ing the log partition function would also enable simultaneous estimation of all the parameters of a
heterogeneous model  for example an extended directed graphical model with Gibbs distributions
forming some of the model components.
In this work  we consider a method of tracking the log partition function during training  which
builds upon the parallel tempering (PT) framework [7  10  15]. Our method relies on two basic ob-
servations. First  when using stochastic gradient descent 1  parameters tend to change slowly during
training; consequently  the partition function Z(β) also tends to evolve slowly. We exploit this prop-
erty of the learning process by using importance sampling to estimate changes in the log partition
function from one learning iteration the next. If the changes in the distribution from time-step t to
t + 1 are small  the importance sampling estimate can be very accurate  even with relatively few
samples. This is the same basic strategy employed in AIS  but while with AIS one constructs a path
of close distributions through an annealing schedule  in our procedure we simply rely on the path
of distributions that emerges from the learning process. Second  parallel tempering (PT) relies on
simulating an extended system  consisting of multiple models each running at their own tempera-
ture. These temperatures are chosen such that neighboring models overlap sufﬁciently as to allow
for frequent cross-temperature state swaps. This is an ideal operating regime for bridge sampling
[2  19]  which can thus serve to estimate the difference in log partition functions between neighbor-
ing models. While with relatively few samples  each method on its own tends not to provide reliable
estimates  we propose to combine these measurements using a variation of the well-known Kalman
ﬁlter (KF)  allowing us to accurately track the evolution of the log partition function throughout
learning. The efﬁciency of our method stems from the fact that our estimator makes use of the
samples generated in the course of training  thus incurring relatively little additional computational
cost.
This paper is structured as follows. In Section 2  we provide a brief overview of RBMs and the
SML-PT training algorithm  which serves as the basis of our tracking algorithm. Sections (3.1-3.3)
cover the details of the importance and bridge sampling estimates  while Section 3.4 provides a
comprehensive look at our ﬁltering procedure and the tracking algorithm as a whole. Experimental
results are presented in Section 4.

2 Stochastic Maximum Likelihood with Parallel Tempering

Our proposed log partition function tracking strategy is applicable to any Gibbs distribution model
that is undergoing relatively smooth changes in the partition function. However  we concentrate on
its application to the RBM since it has become a model of choice for learning unsupervised features
for use in deep feed-forward architectures [9  1] as well as for modeling complex  high-dimensional
distributions [27  24  12].
RBMs are bipartite graphical models where visible units v ∈ {0  1}nv interact with hidden units
h ∈ {0  1}nh through the energy function E(v  h) = −hT W v − cT h − bT v. The model parameters
θ = [W  c  b] consist of the weight matrix W ∈ Rnh×nv  whose entries Wij connect units (vi  hj) 
and offset vectors b and c. RBMs can be trained through a stochastic approximation to the nega-
]  where F (v) is the free-energy function deﬁned as
tive log-likelihood gradient ∂F (v)
h exp(−E(v  h)). In Stochastic Maximum Likelihood (SML) [25]  we replace the
expectation by a sample average  where approximate samples are drawn from a persistent Markov
chain  updated through k-steps of Gibbs sampling between parameter updates. Other algorithms
improve upon this default formulation by replacing Gibbs sampling with more powerful sampling
algorithms [26  7  21  20]. By increasing the mixing rate of the underlying Markov chain  these
methods can lead to lower variance estimates of the maximum likelihood gradient and faster conver-

F (v) = − logP

∂θ − Ep[ ∂F (v)

∂θ

1Stochastic gradient descent is one of the most popular methods for training MRFs precisely because second
order optimization methods typically require a deterministic gradient  whereas sampling-based estimators are
the only practical option for models with an intractable partition function.

2

gence. However  from the perspective of tracking the log partition function  we will see in Section 3
that the SML-PT scheme [7] presents a rather unique advantage.
Throughout training  parallel tempering draws samples from an extended system Mt = {qi t; i ∈
[1  M]}  where qi t denotes the model with inverse temperature βi ∈ [0  1] obtained after t steps
of gradient descent. Each model qi t (associated with a unique partition function Zi t) represents
a smoothed version of the target distribution: q1 t (with β1 = 1). The inverse temperature βi =
1/Ti ∈ [0  1] controls the degree of smoothing  with smaller values of βi leading to distributions
which are easier to sample from. To leverage these fast-mixing chains  PT alternates k steps of Gibbs
sampling (performed independently at each temperature) with cross-temperature state swaps. These
are proposed between neighboring chains using a Metropolis-Hastings-based acceptance criterion.
If we denote the particle obtained by each model qi t after k steps of Gibbs sampling as xi t  then
the swap acceptance ratio ri t for chains (i  i + 1) is given by:

(cid:18)

(cid:19)

ri t = min

1 

˜qi t(xi+1 t)˜qi+1 t(xi t)
˜qi t(xi t)˜qi+1 t(xi+1 t)

(2)

These swaps ensure that samples from highly ergodic chains are gradually swapped into lower tem-
perature chains. Our swapping schedule is the deterministic even-odd algorithm [14] which proposes
swaps between all pairs (qi t  qi+1 t) with even i’s  followed by those with odd i’s. The gradient is
then estimated by using the sample which was last swapped into temperature β1. To reduce the
variance on our estimate  we run multiple Markov chains per temperature  yielding a mini-batch of
model samples Xi t = {x(n)
SML with Adaptive parallel tempering (SML-APT) [6]  further improves upon SML-PT by automat-
ing the choice of temperatures. It does so by maximizing the ﬂow of particles between extremal
temperatures  yielding better ergodicity and more robust sampling in the negative phase of training.

i t ∼ qi t(x); 1 ≤ n ≤ N} at each time-step and temperature.

3 Tracking the Partition Function

Unrolling in time (learning iterations) the M models being simulated by PT  we can envision a two-
dimensional lattice of RBMs indexed by (i  t). As previously mentioned  gradient descent learning
causes qi t  the model with inverse temperature βi obtained at time-step t  to be close to qi t−1. We
can thus apply importance sampling between adjacent temporal models 2 to obtain an estimate of
ζi t − ζi t−1  denoted as O∆t
i t . Inspired by the annealing distributions used in AIS  one could think to
iterate this process from a known quantity ζi 1  in order to estimate ζi t. Unfortunately  the variance
of such an estimate would grow quickly with t.
PT provides an interesting solution to this problem  by simulating an extended system Mt where the
βi’s are selected such that qi t and qi+1 t have enough overlap to allow for frequent cross-temperature
state swaps. This motivates using bridge sampling [2] to provide an estimate of ζi+1 t − ζi t  the
difference in log partitions between temperatures βi+1 and βi. We denote this estimate as O∆β
i t . Ad-
ditionally  we can treat ζM t as a known quantity during training  by setting βM = 0 3. Beginning
with ζM t (see deﬁnition in Fig. 1)  repeated application of bridge sampling alone could in principle
arrive at an accurate estimate of {ζi t; i ∈ [1  M ]  t ∈ [1  T ]}. However  reducing the variance sufﬁ-
ciently to provide useful estimates of the log partition function would require using a relatively large
number of samples at each temperature. Within the context of RBM training  the required number
of samples at each of the parallel chains would have an excessive computational cost. Nonetheless
even with relatively few samples  the bridge sampling estimate provides an additional source of
information regarding the log partition function.
i t by treating the unknown
Our strategy is to combine these two high variance estimates O∆t
i t and O∆β
In this framework  we
log partition functions as a latent state to be tracked by a Kalman ﬁlter.
consider O∆t
i t as observed quantities  used to iteratively reﬁne the joint distribution over the
latent state at each learning iteration. Formally  we deﬁne this latent state to be ζt = [ζ1 t  . . .   ζM t  bt]
  where bt is an extra term to account for a systematic bias in O∆β
1 t (see Sec. 3.2 for details). The
corresponding graphical model is shown in Figure 1.

i t and O∆β

thus given asP

2 This same technique was recently used in [5]  in the context of learning rate adaptation.
3 The visible units of an RBM with zero weights are marginally independent. Its log partition function is

i log(1 + exp(bi)) + nh · log(2).

3

System Equations:
p(ζ0) = N (µ0  Σ0)
p(ζt | ζt−1) = N (ζt−1  Σζ)
p(O(∆t)

t

| ζt  ζt−1) = N (C[ζt  ζt−1]T   Σ∆t)
| ζt) = N (Hζt  Σ∆β)

p(O(∆β)

t

C =

H =

2664 IM
2664

3775

0
0

1
0

−IM

0

...

...
0
−1 +1
0
...
0 −1 +1
. . .
0 −1 +1

0

0

0

0

3775

0

0
0
0

Figure 1: A directed graphical model for log partition function tracking. The shaded nodes represent observed
variables  and the double-walled nodes represent the tractable ζM : with βM = 0. For clarity of presentation 
we show the bias term as distinct from the other ζi t (recall bt = ζM +1 t.)

3.1 Model Dynamics

The ﬁrst step is to specify how we expect the log partition function to change over training iterations 
i.e. our prior over the model dynamics. SML training of the RBM model parameters is a stochastic
gradient descent algorithm (typically over a mini-batch of N examples) where the parameters change
by small increments speciﬁed by an approximation to the likelihood gradient. This implies that both
the model distribution and the partition function change relatively slowly over learning increments
with a rate of change being a function of the SML learning rate; i.e. we expect qi t and ζi t to be
close to qi t−1 and ζi t−1 respectively.
Our model dynamics are thus simple and capture the fact that the log partition function is slowly
changing. Characterizing the evolution of the log partition functions as independent Gaussian pro-
cesses  we model the probability of ζt conditioned on ζt−1 as p(ζt|ζt−1) = N (ζt−1  Σζ)  a normal
distribution with mean ζt−1 and ﬁxed diagonal covariance Σζ = Diag[σ2
Z and σ2
b
are hyper-parameters controlling how quickly the latent states ζi t and bt are expected to change
between learning iterations.

Z   . . .   σ2

b ]. σ2

Z   σ2

3.2

Importance Sampling Between Learning Iterations

The observation distribution p(O(∆t)
ship between the evolution of the latent log partitions and the statistical measurements O(∆t)
[O(∆t)

| ζt  ζt−1) = N (C[ζt  ζt−1]T   Σ∆t) models the relation-
=

M t ] given by importance sampling  with O∆t

i t deﬁned as:

  . . .   O(∆t)

1 t

t

t

(cid:41)

(cid:40) 1

N(cid:88)

N

n=1

O∆t

i t = log

w(n)
i t

with w(n)

i t =

˜qi t(x(n)
i t−1)
˜qi t−1(x(n)
i t−1)

.

(3)

In the above distribution  the matrix C encodes the fact that the average importance weights esti-
mate ζi t − ζi t−1 + bt · Ii=1  where I is the indicator function. It is formally deﬁned in Fig. 1.
Σ∆t is a diagonal covariance matrix  whose elements are updated online from the estimated vari-
ances of the log-importance weights. At time-step t  the i-th entry of its diagonal is thus given by

Var[wi t]/(cid:2)(cid:80)

n w(n)(cid:3)2

.

. It stems from the reuse of samples X1 t−1: ﬁrst 
The term bt accounts for a systematic bias in O(∆t)
for estimating the negative phase gradient at time-step t− 1 (i.e. the gradient applied between qi t−1

1 t

4

O∆t1 tO∆t2 tO∆β1 tO∆β1 t−1O∆βM−1 t−1O∆βM−1 tζM t−1ζM tζ2 tζ1 tζ1 t−1ζ2 t−1btbt−1and qi t) and second  to compute the importance weights of Eq. 3. Since the SML gradient acts to
lower the probability of negative particles  w(n)

is biased.

i t

3.3 Bridging the Parallel Tempering Temperature Gaps

Consider now the other dimension of our parallel tempered lattice of RBMs: temperature. As pre-
viously mentioned  neighboring distributions in PT are designed to have signiﬁcant overlap in their
densities in order to permit particle swaps. However  the intermediate distributions qi t(v  h) are not
so close to one another that we can use them as the intermediate distributions of AIS. AIS typically
requires thousands of intermediate chains  and maintaining that number of parallel chains would
carry a prohibitive computational burden. On the other hand  the parallel tempering strategy of
spacing the temperature to ensure moderately frequent swapping nicely matches the ideal operating
regime of bridge sampling [2].

We thus consider a second observation model as p(O(∆β)
Fig.1. The quantities O(∆β)
ζi+1 t − ζi t. Entries O∆β

1 t   . . .   O∆β

i t are given by:

= [O∆β

t

| ζt) = N (Hζt  Σ∆β)  with H deﬁned in
M−1 t] are obtained via bridge sampling as estimates of

t

N(cid:88)

N(cid:88)

O∆β

i t = log

i t − log
u(n)

n=1

n=1

v(n)
i t

  where u(n)

i t =

q∗

i t
˜qi t

x(n)
i t
x(n)
i t

(cid:16)
(cid:16)

(cid:17)
(cid:17)   v(n)

i t =

(cid:16)
(cid:16)

q∗

i t
˜qi+1 t

x(n)
i+1 t
x(n)
i+1 t

(cid:17)
(cid:17) .

(4)

˜qi t(x)˜qi+1 t(x)

The bridging distribution [2  19] q∗
i t is chosen such that it has large support with both qi and
qi+1. For all i ∈ [1  M − 1]  we choose the approximately optimal distribution q(opt)
(x) =
si t ˜qi t(x)+˜qi+1 t(x) where si t ≈ Zi+1 t/Zi t. Since the Zi t’s are the very quantities we are trying to
estimate  this deﬁnition may seem problematic. However it is possible to start with a coarse estimate
of si 1 and reﬁne it in subsequent iterations by using the output of our tracking algorithm. Σ∆β is
once again a diagonal covariance matrix  updated online from the variance of the log-importance
weights u and v [19]. The i-th entry is given by Var[ui t]
u(n)
i t

i2 + Var[vi t]
i2 .

hP

hP

v(n)
i t

i t

n

n

3.4 Kalman Filtering of the Log-Partition Function

In the above we have described two sources of information regarding the log partition function for
each of the RBMs in the lattice. In this section we describe a method to fuse all available information
to improve the overall accuracy of the estimate of every log partition function. We now consider the
steps involved in the inference process in moving from an estimate of the posterior over the latent
state at time t − 1 to an estimate of the posterior at time t. We begin by assuming we know the
posterior p(ζt−1 | O(∆t)

t−1:0)  where O(·)

t−1:0 = [O(·)

1   . . .   O(·)

t−1:0  O(∆β)

t−1].

We follow the treatment of Neal [18] in characterizing our uncertainty regarding ζi t as a Gaussian
distribution and deﬁne p(ζt−1 | O(∆t)
t−1:0) ∼ N (µt−1 t−1  Pt−1 t−1)  a multivariate Gaussian
with mean µt−1 t−1 and covariance Pt−1 t−1. The double index notation is used to indicate which
is the latest observation being conditioned on for each of the two types of observations: e.g. µt t−1
represents the posterior mean given O(∆t)

t−1:0  O(∆β)

and O(∆β)
t−1:0.

t:0

t

t−1:0  O(∆β)

t−1:0) = p(ζt | ζt−1)p(ζt−1 | O(∆t)

depends on both ζt and ζt−1. In order to
Departing from the typical Kalman ﬁlter setting  O(∆t)
incorporate this observation into our estimate of the latent state  we ﬁrst need to specify the prior
joint distribution p(ζt−1  ζt | O(∆t)
t−1:0)  with p(ζt |
ζt−1) as deﬁned in Sec. 3.1. Observation O(∆t)
is then incorporated through Bayes rule  yielding
p(ζt−1  ζt | O(∆t)
t−1:0) . Having incorporated the importance sampling estimate into the model 
we can then marginalize over ζt−1 (which is no longer required)  to yield p(ζt
t−1:0).
Finally  it remains only to incorporate the bridge sampler estimate O(∆β)
by a second application
of Bayes rule  which gives us p(ζt | O(∆t)
)  the updated posterior over the latent state at
time-step t. The detailed inference equations are provided in Fig. 2 and can be derived easily from
standard textbook equations on products and marginals of normal distributions [4].

t−1:0  O(∆β)

t:0   O(∆β)

t:0   O(∆β)

t:0   O(∆β)

| O(∆t)

t:0

t

t

5

Inference Equations:

(i)

p

ζt−1  ζt | O(∆t)

(ii)

with ηt−1 t−1 =
p(ζt−1  ζt | O(∆t)
with Vt t−1 = (V

“

“

» Pt−1 t−1

”
» µt−1 t−1

t−1:0  O(∆β)
t−1:0

–

= N (ηt−1 t−1  Vt−1 t−1)
and Vt−1 t−1 =
µt−1 t−1
t−1:0) = N (ηt t−1   Vt t−1)
”

−1
t−1 t−1 + C T Σ

t:0   O(∆β)

–

Pt−1 t−1

Pt−1 t−1 Σζ + Pt−1 t−1

−1
∆t C)−1 and ηt t−1 = Vt t−1(C T Σ∆tO(∆t)

−1
t−1 t−1ηt−1 t−1)
= N (µt t−1   Pt t−1) with µt t−1 = [ηt t−1]2 and Pt t−1 = [Vt t−1]2 2

+ V

t

ζt | O(∆t)

(iii) p
(iv) p(ζt | O(∆t)

t:0   O(∆β)

t:0   O(∆β)
t−1:0

with Pt t = (P

t:0
−1
t t−1 + H T Σ

) = N (µt t  Pt t)

−1
∆βH)−1 and µt t = Pt t(H T Σ∆βO(∆β)

t

+ P

−1
t t−1µt t−1)

Figure 2: Inference equations for our log partition tracking algorithm  a variant on the Kalman ﬁlter. For any
vector v and matrix V   we use the notation [v]2 to denote the vector obtained by preserving the bottom half
elements of v and [V ]2 2 to indicate the lower right-hand quadrant of V .

4 Experimental Results

α

For the following experiments  SML was performed using either constant or decreasing learning
rates. We used the decreasing schedule t = min(init
t+1   init)  where t is the learning rate at
time-step t  init is the initial or base learning rate and α is the decrease constant. Entries of Σζ
Z = +∞  which is to say that we did not exploit the
(see Section 3.1) were set as follows. We set σ2
smoothness prior when estimating the prior distribution over the joint p(ζt−1  ζt | O(∆t)
t−1:0). σ2
was set to 10−3 · t  allowing the estimated bias on O(∆t)
to change faster for large learning rates.
When initializing the RBM visible offsets4 as proposed in [8]  the intermediate distributions of Eq. 1
lead to sub-optimal swap rates between adjacent chains early in training  with a direct impact on the
quality of tracking. In our experiments  we avoid this issue by using the intermediate distributions
qi t(x) ∝ exp[βi · (−hT W v − cT h) − bT v]. We tested mini-batch sizes N ∈ [10  20].

t−1:0  O(∆β)

1 t

b

Comparing to Exact Likelihood We start by comparing the performance of our tracking algo-
rithm to the exact likelihood  obtained by marginalizing over both visible and hidden units. We
chose 25 hidden units and trained on the ubiquitous MNIST [13] dataset for 300k updates  using
both ﬁxed and adaptive learning rates. The main results are shown in Figure 3.
In Figure 3(a)  we can see that our tracker provides a very good ﬁt to the likelihood with init = 0.001
and decrease constants α in {103  104  105}. Increasing the base learning rate to init = 0.01 in
Figure 3(b)  we maintain a good ﬁt up to α = 104  with a small dip in performance at 50k updates.
Our tracker fails however to capture the oscillatory behavior engendered by too high of a learning
rate (init = 0.01  α = 105). It is interesting to note that the failure mode of our algorithm seems to
coincide with an unstable optimization process.

Comparing to AIS for Large-Scale Models
In evaluating the performance of our tracking algo-
rithm on larger models  exact computation of the likelihood is no longer possible  so we use AIS
as our baseline.5 Our models consisted of RBMs with 500 hidden units  trained using SML-APT
[6] on the MNIST and Caltech Silhouettes [16] datasets. We performed 200k updates  with learning
rate parameters init ∈ {.01  .001} and α ∈ {103  104  105}.
On MNIST  AIS estimated the test-likelihood of our best model at −94.34 ± 3.08 (where ± indi-
cates the 3σ conﬁdence interval)  while our tracking algorithm reported a value −89.96. On Cal-
tech Silhouettes  our model reached −134.23 ± 21.14 according to AIS  while our tracker reported

4Each bk is initialized to log ¯xk
1−¯xk
5Our base AIS conﬁg. was 103 intermediate distributions spaced linearly between β = [0  0.5]  104 dis-
tributions for the interval [0.5  0.9] and 104 for [0.9  1.0]. Estimates of log Z are averaged over 100 annealed
importance weights.

  where ¯xk is the mean of the k-th dimension on the training set.

6

Figure 3: Comparison of exact test-set likelihood and estimated likelihood as given by AIS and our tracking
algorithm. We trained a 25-hidden unit RBM for 300k updates using SML  with a learning rate schedule
t = min(α·init/(t+1)  init)  with (left) init = 0.001 and (right) init = 0.01 varying α ∈ {103  104  105}.

(left) Plotted on left y-axis are the Kalman ﬁlter measurements O

Figure 4:
  our log partition estimate
of ζ1 t and point estimates of ζ1 t obtained by AIS. On the right y-axis  measurement O
is plotted  along
with the estimated bias bt. Note how bt becomes progressively less-pronounced as t decreases and the model
converges. Also of interest  the variance on O
increases with t but is compensated by a decreasing variance
  yielding a relatively smooth estimate ζ1 t. (not shown) The ±3σ conﬁdence interval of the AIS
on O
estimate at 200k updates was measured to be 3.08. (right) Example of early-stopping on dna dataset.

(∆β)
t

(∆t)
t

(∆β)
t

(∆t)
t

−114.31. To put these numbers in perspective  Salakhutdinov and Murray [23] reports values of
−125.53  −105.50 and −86.34 for 500 hidden unit RBMs trained with CD{1 3 25} respectively.
Marlin et al. [16] report around −120 for Caltech Silhouettes  again using 500 hidden units.
Figure 4(left) shows a detailed view of the Kalman ﬁlter measurements and its output  for the best
performing MNIST model. We can see that the variance on O(∆β)
(plotted on the left y-axis) grows
slowly over time  which is mitigated by a decreasing variance on O(∆t)
(plotted on the right y-
axis). As the model converges and the learning rate decreases  qi t−1 and qi t become progressively
closer and the importance sampling estimates become more robust. The estimated bias term bt also
converges to zero.
An important point to note is that a naive linear-spacing of temperatures yielded low exchange
rates between neighboring temperatures  with adverse effects on the quality of our bridge sampling
estimates. As a result  we observed a drop in performance  both in likelihood as well as tracking
performance. Adaptive tempering [6] (with a ﬁxed number of chains M) proved crucial in getting
good tracking for these experiments.

t

t

Early-Stopping Experiments Our ﬁnal set of experiments highlights the performance of our
method on a wide-variety of datasets [11]. In these experiments  we use our estimate of the log

7

050100150200250300Updates(x1e3)−210−200−190−180−170−160−150−140Likelihood(nats)µi=0.001α=1e3µi=0.001α=1e4µi=0.001α=1e5ExactAISKalman050100150200250300Updates(x1e3)−210−200−190−180−170−160−150−140−130Likelihood(nats)µi=0.010α=1e3µi=0.010α=1e4µi=0.010α=1e5ExactAISKalman050100150200Updates(x1e3)380400420440460480500520540logZ(nats)ζtO(∆β)tAISO(∆t)tbt−20246∆tlogZ0100200300400500Epochs−105−100−95−90−85−80Likelihood(nats)ζt(train)ζt(valid)AIS(valid)Dataset

adult
connect4
dna
mushrooms
nips
ocr letters
rcv1
web

Kalman
-15.24
-15.77
-87.97
-10.49
-270.10
-33.87
-46.89
-28.95

RBM

-15.70
-16.81
-88.51
-14.68
-271.23
-31.45
-48.61
-29.91

AIS
(± 0.50)
(± 0.67)
(± 0.97)
(± 30.75)
(± 0.58)
(± 2.70)
(± 0.69)
(± 0.74)

RBM-25

NADE

-16.29
-22.66
-96.90
-15.15
-277.37
-43.05
-48.88
-29.38

-13.19
-11.99
-84.81
-9.81
-273.08
-27.22
-46.66
-28.39

Table 1: Test set likelihood on various datasets. Models were trained using SML-PT. Early-stopping was
performed by monitoring likelihood on a hold-out validation set  using our KF estimate of the log partition
function. Best models (i.e. the choice of hyper-parameters) were then chosen according to the AIS likelihood
estimate. Results for 25-hidden unit RBMs and NADE are taken from [11]. ± indicates a conﬁdence interval
of three standard deviations.

partition to monitor model performance on a held-out validation set. When the onset of over-ﬁtting
is detected  we store the model parameters and report the associated test-set likelihood  as estimated
by both AIS and our tracking algorithm. The advantages of such an early-stopping procedure is
shown in Figure 4(b)  where training log-likelihood increases throughout training while validation
performance starts to decrease around 250 epochs. Detecting over-ﬁtting without tracking the log
partition would require a dense grid of AIS runs which would prove computationally prohibitive.
We tested parameters in the following range: number of hidden units in {100  200  500  1000} (de-
pending on dataset size)  learning rates in {10−2  10−3  10−4} either held constant during training
or annealed with constants α ∈ {103  104  105}. For tempering  we used 10 ﬁxed temperatures 
spaced linearly between β = [0  1]. SGD was performed using mini-batches of size {10  100} when
estimating the gradient  and mini-batches of size {10  20} for our set of tempered-chains (we thus
simulate 10 × {10  20} tempered chains in total). As can be seen in Table 4  our tracker performs
very well compared to the AIS estimates and across all datasets. Efforts to lower the variance of the
AIS estimate proved unsuccessful  even going as far as 105 intermediate distributions.

5 Discussion

In this paper  we have shown that while exact calculation of the partition function of RBMs may be
intractable  one can exploit the smoothness of gradient descent learning in order to approximately
track the evolution of the log partition function during learning. Treating the ζi t’s as latent vari-
ables  the graphical model of Figure 1 allowed us to combine multiple sources of information to
achieve good tracking of the log partition function throughout training  on a variety of datasets. We
note however that good tracking performance is contingent on the ergodicity of the negative phase
sampler. Unsurprisingly  this is the same condition required by SML for accurate estimation of the
negative phase gradient.
The method presented in the paper is also computationally attractive  with only a small computaiton
overhead relative to SML-PT training. The added computational cost lies in the computation of
the importance weights for importance sampling and bridge sampling. However  this boils down to
computing free-energies which are mostly pre-computed in the course of gradient updates with the
sole exception being the computation of ˜qi t(xi t−1) in the importance sampling step. In comparison
to AIS  our method allows us to fairly accurately track the log partition function  and at a per-point
estimate cost well below that of AIS. Having a reliable and accurate online estimate of the log
partition function opens the door to a wide range of new research directions.

Acknowledgments

The authors acknowledge the ﬁnancial support of NSERC and CIFAR; and Calcul Qu´ebec for com-
putational resources. We also thank Hugo Larochelle for access to the datasets of Sec. 4; Hannes
Schulz  Andreas Mueller  Olivier Delalleau and David Warde-Farley for feedback on the paper and
algorithm; along with the developers of Theano [3].

8

References
[1] Bengio  Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine Learning  2(1) 

1–127. Also published as a book. Now Publishers  2009.

[2] Bennett  C. (1976). Efﬁcient estimation of free energy differences from Monte Carlo data. Journal of

Computational Physics  22(2)  245–268.

[3] Bergstra  J.  Breuleux  O.  Bastien  F.  Lamblin  P.  Pascanu  R.  Desjardins  G.  Turian  J.  Warde-Farley 
In Proceedings of the

D.  and Bengio  Y. (2010). Theano: a CPU and GPU math expression compiler.
Python for Scientiﬁc Computing Conference (SciPy). Oral.

[4] Bishop  C. M. (2006). Pattern Recognition and Machine Learning. Springer.
[5] Cho  K.  Raiko  T.  and Ilin  A. (2011). Enhanced gradient and adaptive learning rate for training restricted
boltzmann machines. In L. Getoor and T. Scheffer  editors  Proceedings of the 28th International Conference
on Machine Learning (ICML-11)  ICML ’11  pages 105–112  New York  NY  USA. ACM.

[6] Desjardins  G.  Courville  A.  and Bengio  Y. (2010a). Adaptive parallel tempering for stochastic maximum

likelihood learning of rbms. NIPS*2010 Deep Learning and Unsupervised Feature Learning Workshop.

[7] Desjardins  G.  Courville  A.  Bengio  Y.  Vincent  P.  and Delalleau  O. (2010b). Tempered Markov chain
monte carlo for training of restricted Boltzmann machine. In JMLR W&CP: Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010)  volume 9  pages 145–152.
[8] Hinton  G. (2010). A practical guide to training restricted boltzmann machines. Technical Report 2010003 

University of Toronto. version 1.

[9] Hinton  G. E.  Osindero  S.  and Teh  Y. (2006). A fast learning algorithm for deep belief nets. Neural

Computation  18  1527–1554.

[10] Iba  Y. (2001). Extended ensemble monte carlo. International Journal of Modern Physics  C12  623–656.
[11] Larochelle  H. and Murray  I. (2011). The Neural Autoregressive Distribution Estimator. In Proceedings
of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’2011)  volume
15 of JMLR: W&CP.

[12] Larochelle  H.  Bengio  Y.  and Turian  J. (2010). Tractable multivariate binary density estimation and the

restricted Boltzmann forest. Neural Computation  22(9)  2285–2307.

[13] LeCun  Y.  Bottou  L.  Bengio  Y.  and Haffner  P. (1998). Gradient based learning applied to document

recognition. IEEE  86(11)  2278–2324.

[14] Lingenheil  M.  Denschlag  R.  Mathias  G.  and Tavan  P. (2009). Efﬁciency of exchange schemes in

replica exchange. Chemical Physics Letters  478(1-3)  80 – 84.

[15] Marinari  E. and Parisi  G. (1992). Simulated tempering: A new monte carlo scheme. EPL (Europhysics

Letters)  19(6)  451.

[16] Marlin  B.  Swersky  K.  Chen  B.  and de Freitas  N. (2009). Inductive principles for restricted boltzmann
machine learning. In Proceedings of The Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS’10)  volume 9  pages 509–516.

[17] Murray  I. and Ghahramani  Z. (2004). Bayesian learning in undirected graphical models: Approximate

mcmc algorithms.

[18] Neal  R. M. (2001). Annealed importance sampling. Statistics and Computing  11(2)  125–139.
[19] Neal  R. M. (2005). Estimating ratios of normalizing constants using linked importance sampling.
[20] Salakhutdinov  R. (2010a). Learning deep boltzmann machines using adaptive mcmc. In L. Bottou and
M. Littman  editors  Proceedings of the Twenty-seventh International Conference on Machine Learning
(ICML-10)  volume 1  pages 943–950. ACM.

[21] Salakhutdinov  R. (2010b). Learning in Markov random ﬁelds using tempered transitions. In NIPS’09.
[22] Salakhutdinov  R. and Hinton  G. E. (2009). Deep Boltzmann machines. In AISTATS’2009  volume 5 

pages 448–455.

[23] Salakhutdinov  R. and Murray  I. (2008). On the quantitative analysis of deep belief networks. In W. W.

Cohen  A. McCallum  and S. T. Roweis  editors  ICML 2008  volume 25  pages 872–879. ACM.

[24] Taylor  G. and Hinton  G. (2009). Factored conditional restricted Boltzmann machines for modeling

motion style. In L. Bottou and M. Littman  editors  ICML 2009  pages 1025–1032. ACM.

[25] Tieleman  T. (2008). Training restricted Boltzmann machines using approximations to the likelihood
gradient. In W. W. Cohen  A. McCallum  and S. T. Roweis  editors  ICML 2008  pages 1064–1071. ACM.
[26] Tieleman  T. and Hinton  G. (2009). Using fast weights to improve persistent contrastive divergence. In

L. Bottou and M. Littman  editors  ICML 2009  pages 1033–1040. ACM.

[27] Welling  M.  Rosen-Zvi  M.  and Hinton  G. E. (2005). Exponential family harmoniums with an applica-

tion to information retrieval. In NIPS’04  volume 17  Cambridge  MA. MIT Press.

9

,Aditya Bhaskara
Silvio Lattanzi
Vahab Mirrokni
Cengiz Pehlevan
Dmitri Chklovskii
Marc Vuffray
Sidhant Misra
Andrey Lokhov
Michael Chertkov