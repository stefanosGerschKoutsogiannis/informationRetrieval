2019,Think out of the "Box": Generically-Constrained Asynchronous Composite Optimization and Hedging,We present two new algorithms  ASYNCADA and HEDGEHOG  for asynchronous sparse online and stochastic optimization. ASYNCADA is  to our knowledge  the first asynchronous stochastic optimization algorithm with finite-time data-dependent convergence guarantees for generic convex constraints. In addition  ASYNCADA: (a) allows for proximal (i.e.  composite-objective) updates and adaptive step-sizes; (b) enjoys any-time convergence guarantees without requiring an exact global clock; and (c) when the data is sufficiently sparse  its convergence rate for (non-)smooth  (non-)strongly-convex  and even a limited class of non-convex objectives matches the corresponding serial rate  implying a theoretical “linear speed-up”. The second algorithm  HEDGEHOG  is an asynchronous parallel version of the Exponentiated Gradient (EG) algorithm for optimization over the probability simplex (a.k.a. Hedge in online learning)  and  to our knowledge  the first asynchronous algorithm enjoying linear speed-ups under sparsity with non-SGD-style updates. Unlike previous work  ASYNCADA and HEDGEHOG and their convergence and speed-up analyses are not limited to individual coordinate-wise (i.e.  “box-shaped”) constraints or smooth and strongly-convex objectives. Underlying both results is a generic analysis framework that is of independent
interest  and further applicable to distributed and delayed feedback optimization,Think out of the “Box”: Generically-Constrained

Asynchronous Composite Optimization and Hedging

Pooria Joulani⇤
DeepMind  UK

pjoulani@google.com

András György
DeepMind  UK

agyorgy@google.com

Csaba Szepesvári
DeepMind  UK

szepi@google.com

Abstract

We present two new algorithms  ASYNCADA and HEDGEHOG  for asynchronous
sparse online and stochastic optimization. ASYNCADA is  to our knowledge 
the ﬁrst asynchronous stochastic optimization algorithm with ﬁnite-time data-
dependent convergence guarantees for generic convex constraints. In addition 
ASYNCADA: (a) allows for proximal (i.e.  composite-objective) updates and
adaptive step-sizes; (b) enjoys any-time convergence guarantees without requiring
an exact global clock; and (c) when the data is sufﬁciently sparse  its convergence
rate for (non-)smooth  (non-)strongly-convex  and even a limited class of non-
convex objectives matches the corresponding serial rate  implying a theoretical
“linear speed-up”. The second algorithm  HEDGEHOG  is an asynchronous parallel
version of the Exponentiated Gradient (EG) algorithm for optimization over the
probability simplex (a.k.a. Hedge in online learning)  and  to our knowledge  the
ﬁrst asynchronous algorithm enjoying linear speed-ups under sparsity with non-
SGD-style updates. Unlike previous work  ASYNCADA and HEDGEHOG and
their convergence and speed-up analyses are not limited to individual coordinate-
wise (i.e.  “box-shaped”) constraints or smooth and strongly-convex objectives.
Underlying both results is a generic analysis framework that is of independent
interest  and further applicable to distributed and delayed feedback optimization.

1

Introduction

Many modern machine learning methods are based on iteratively optimizing a regularized objective.
Given a convex  non-empty set of feasible model parameters X⇢ Rd  a differentiable loss function
f : Rd ! R  and a convex (possibly non-differentiable) regularizer function  : Rd ! R  these
methods seek the parameter vector x⇤ 2X that minimizes f +  (assuming a minimizer exists):
(1)

x⇤ = arg min

f (x) + (x) .

x2X

mPm

In particular  empirical risk minimization (ERM) methods such as (regularized) least-squares  logistic
regression  LASSO  and support vector machines solve optimization problems of the form (1). In
these cases  f (x) = 1
i=1 F (x  ⇠i) is the average of the loss F (x  ⇠i) of the model parameter x
on the given training data ⇠1 ⇠ 2  . . .  ⇠ m and (x) is a norm (or a combination of norms) on Rd (e.g. 
F (x  ⇠) = log(1 + exp(x>⇠)) and (x) = 1
To bring the power of modern parallel computing architectures to such optimization problems  several
papers in the past decade have studied parallel variants of the stochastic optimization algorithms
applied to these problems. Here one of the main questions is to quantify the cost of parallelization 
that is  how much extra work is needed by a parallel algorithm to achieve the same accuracy as its
serial variant. Ideally  a parallel algorithm is required to do no more work than the serial version  but

2 in linear logistic regression [13]).

2kxk2

⇤Work partially done when the author was at the University of Alberta  Edmonton  AB  Canada.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

this is very hard to achieve in our case. Instead  a somewhat weaker goal is to ensure that the price
of parallelism is at most a constant factor: that is  the parallel variant needs at most constant-times
more updates (or work). In other words  using ⌧ parallel process requires a wall-clock running time
that is only O(1/⌧)-times that of the serial variant. In this case we say that the parallel algorithm
achieves a linear speed-up. Of particular interest are asynchronous lock-free algorithms  where
Recht et al. [30] demostrated ﬁrst that linear speed-ups are possible: They showed that if ⌧ processes
run stochastic gradient descent (SGD) and apply their updates to the same shared iterate without
locking  then the overall algorithm (called Hogwild!) converges after the same amount of work as
serial SGD  up to a multiplicative factor that increases with the number of concurrent processes and
decreases with the sparsity of the problem. Thus  if the problem is sparse enough  this penalty can
be considered a constant  and the algorithm achieves linear speed-up. Several follow-up work (see
e.g.  [20  18  17  27  24  10  29  7  11  4  2  3  19  31  33  32  35  36  12  6  28] and the references
therein) have demonstrated linear speed-ups for methods based on (block-)coordinate descent (BCD) 
as well as other variants of SGD such as SVRG [15]  SAGA [8]  ADAGRAD [22  9]  and SGD with a
time-decaying step-size. Despite the great advances  however  several problems remain open.2
First  the existing convergence guarantees concern SGD when the constraint set X is box-shaped 
that is  a Cartesian product of (block-)coordinatewise constraints X = ⇥d
i=1Xi. This leaves it unclear
whether existing techniques apply to stochastic optimization algorithms that operate on non-box-
shaped constraints (e.g.  on the `2 ball)  or algorithms that use a non-Euclidean regularizer  such as
the exponentiated gradient (EG) algorithm used on the probability simplex (see  e.g.  [34  14]).
Second  with the exception of the works of Duchi et al. [10] and Pan et al. [26] (which still require
box-shaped constraints)  and De Sa et al. [7] (which only bounds the probability of “failure”  i.e.  of
producing no iterates in the ✏-ball around x⇤)  the existing analyses demonstrating linear speed-ups
are limited to strongly-convex (or Polyak-Łojasiewicz) objectives. Thus  so far it has remained
unclear whether a similar speed-up analysis is possible if the objective is simply convex or smooth
[20]  or if we are in the closely-related online-learning setting with the objective changing over time.
Third  with the exception of the work of Pedregosa et al. [27] (which still requires box-shaped
constraints  block-separable  and strongly-convex f)  the existing analyses do not take advantage of
the structure of problem (1). In particular  when  is “simple to optimize” over X (formally deﬁned
as having access to a proximal operator oracle  as we make precise in what follows)  serial algorithms
such as Proximal-SGD take advantage of this property to achieve considerably faster convergence
rates. Asynchronous variants of the Proximal-SGD algorithm with such faster rates have so far been
unavailable for non-strongly-convex objectives and non-box constraints.

1.1 Contributions
In this paper we address the aforementioned problems and present algorithms that are applicable
to general convex constraint sets  not just box-shaped X   but still achieve linear speed-ups (under
sparsity) for non-smooth and non-strongly-convex (as well as smooth or strongly convex) objectives 
and even for a speciﬁc class of non-convex problems. This is achieved through our new asynchronous
optimization algorithm  ASYNCADA  which generalizes the ASYNC-ADAGRAD (and ASYNC-DA)
algorithm of Duchi et al. [10] to proximal updates and its data-dependent bound to arbitrary constraint
sets. Instantiations of ASYNCADA under different settings are given in Table 1. Indeed  the results
are obtained by a more general analysis framework  built on the work of Duchi et al. [10]  that yields
data-dependent convergence guarantees for a generic class of adaptive  composite-objective online
optimization algorithms undergoing perturbations to their “state”. We further use this framework to
derive the ﬁrst asynchronous online and stochastic optimization algorithm with non-box constraints
that uses non-Euclidean regularizers. In particular  we present and analyze HEDGEHOG  the parallel
asynchronous variant of the EG algorithm  also known as Hedge in online linear optimization [34  14] 
2 In this paper  we do not further consider BCD-based methods  for two main reasons: a) in general  a
BCD update may unnecessarily slow down the convergence of the algorithm by focusing only on a single
coordinate of the gradient information  especially in the sparse-data problems we consider in this paper (see 
e.g.  Pedregosa et al. [27  Appendix F]); and b) BCD algorithms typically apply only to box-shaped constraints 
which is what our algorithms are designed to be able to avoid. We would like to note  however  that our
stochastic gradient oracle set-up (Section 2) does allow for building an unbiased gradient estimate using only
one randomly-selected (block-)coordinate  as done in BCD methods. Nevertheless  the literature on parallel
asynchronous BCD algorithms is vast  including especially algorithms for proximal  non-strongly-convex  and
non-convex optimization; see  e.g.  [29  11  4  2  3  19  31  33  32  35  36  12  6  28] and the references therein.

2

[10  26] X

[26] X

Strongly-convex

Smooth f + Strongly-convex

[26] X
[26]
X

[26] X

Nonsmooth
[10  26] X
[10  26]

Smooth f
[26] X
[26]
X

[30  7  20  17  24  26] X
[30  7  20  17  24  26]

Algorithm
X
SGD (DA) Rd
SGD (MD) ⇤
DA

⇤
AG / DA
AG / DA

⇤
Prox-MD
Prox-DA

Prox-AG

Hedge/EG 4
Table 1: (Star-)convex optimization settings under which sufﬁcient sparsity results in linear speed-up.
Previous work are cited under the settings they address. A X indicates a setting covered by the results
in this paper. The symbols ⇤  4  and  indicate  respectively  the case when the constraint set is
box-shaped  the probability simplex  or any convex constraint set with a projection oracle. AG  DA 
and MD stand  respectively  for ADAGRAD  Dual-Averaging  and Mirror Descent  while Prox-AG 
Prox-DA  and Prox-MD denote their proximal variants (using the proximal operator of ).

X

X
-
X
X
X

X

[26] X

X
[27]
X
X
X

X
-
X
X
X

X
-
X
X
X

and show that it enjoys similar parallel speed-up regimes as ASYNCADA. The results are derived for
the more general setting of noisy online optimization  and the generic framework is of independent
interest  in particular in the related settings of distributed and delayed-feedback learning.
The rest of the paper is organized as follows: The optimization problem and its solution with serial
algorithms are described in Section 2 and Section 3  respectively. The generic perturbed-iterate
framework is given in Section 4. Our main algorithms  ASYNCADA and HEDGEHOG are presented
and analyzed in Section 5 and Section 6  respectively. Conclusions are drawn and some open problems
are discussed in Section 7  while omitted technical details are given in the appendices.

↵ = 1

j=1 ↵(j)x(j)2  and k·k ↵ ⇤ its dual. We use (at)j
2Pd

1.2 Notation and deﬁnitions
We use [n] to denote the set {1  2  . . .   n}  I{E} for the indicator of an event E  and (H) to denote
the sigma-ﬁeld generated by a set H of random variables. The j-th coordinate of a vector a 2 Rd
is denoted a(j). For ↵ 2 Rd with positive entries  k·k ↵ denotes the ↵-weighted Euclidean norm 
given by kxk2
t=i to denote a sequence
ai  ai+1  . . .   aj and deﬁne ai:j :=Pj
t=i at  with ai:j := 0 if i > j. Given a differentiable function
h : Rd ! R  the Bregman divergence of y 2 Rd from x 2 Rd with respect to (w.r.t.) h is given by
Bh(y  x) := h(y)  h(x)  hrh(x)  y  xi. It can be shown that a differentiable function is convex
if and only if Bh(x  y)  0 for all x  y 2 Rd. The function h : Rd ! R is µ-strongly convex w.r.t. a
norm k·k on Rd if and only if for all x  y 2 Rd Bh(x  y)  µ
2kx yk2  and smooth w.r.t. a norm k·k
if and only if for all x  y 2 Rd  |Bh(x  y)| 1
2kx  yk2. A differentiable function f is star-convex if
and only if there exists a global minimizer x⇤ of f such that for all x 2 Rd  Bf (x⇤  x)  0.
2 Problem setting: noisy online optimization

We consider a generic iterative optimization setting that enables us to study both online learning
and stochastic composite optimization. The problem is deﬁned by a (known) constraint set X
and a (known) convex (possibly non-differentiable) function   as well as differentiable functions
f1  f2  . . . about which an algorithm learns iteratively. At each iteration t = 1  2  . . .   the algorithm
picks an iterate xt 2X   and observes an unbiased estimate gt 2 Rd of the gradient rft(xt)   that
is  E{gt|xt} = rft(xt). The goal is to minimize the composite-objective online regret after T
iterations  given by

R(f +)

T

=

TXt=1

(ft(xt) + (xt)  ft(x⇤T )  (x⇤T ))  

3

t=1(ft(x) + (x))o.

where x⇤T = arg minx2XnPT

In the absence of noise (i.e.  when gt =
rft(xt))  this reduces to the (composite-objective) online (convex) optimization setting [34  14].
Stochastic optimization  online regret  and iterate averaging.
If ft = f for all t = 1  2  . . .   we
recover the stochastic optimization setting  with the algorithm aiming to minimize the composite
objective f +  over X while receiving noisy estimates of rf at points (xt)T
t=1. The algorithm’s
online regret can then be used to control the optimization risk: Since ft ⌘ f  we have x⇤T = x⇤ =
arg minx2X {f (x) + (x)}  and by Jensen’s inequality  if f is convex and ¯xT = 1
T x1:T is the
average iterate 

f (¯xT ) + (¯xT )  f (x⇤)  (x⇤) 

1
T

R(f +)

T

.

In addition  if f is non-convex but ¯xT is selected uniformly at random from x1  . . .   xT   then the
above bound holds in expectation. As such  in the rest of the paper we study the optimization risk
through the lens of online regret.

Stochastic ﬁrst-order oracle. Throughout the paper  we assume that at time t  the noisy gradient
estimate gt is given by a randomized ﬁrst-order oracle3 gt : Rd ⇥ ⌅ ! Rd  where ⌅ is some space
of random variables  and there exists a sequence (⇠t)T
t=1 of independent elements from ⌅  with
distribution P⌅  such thatR⌅ gt(x  ⇠)dP⌅(⇠) = rft(x) for all x 2X .
For example  in the ﬁnite-sum stochastic optimization case when f = PN
i fi  selecting one fi
uniformly at random to estimate the gradient corresponds to P⌅ being the uniform distribution on ⌅=
{1  2  . . .   N} and gt(x  ⇠t) = rf⇠t(x)  whereas selecting a mini-batch of fi’s corresponds to ⌅ being
|⇠t|Pi2⇠t rfi(x).
the set of subsets (of a ﬁxed or varying size) of {1  2  . . .   N} and gt(x  ⇠t) = 1
This also covers variance-reduced gradient estimates as formed  e.g.  by SAGA and SVRG  in which
case gt is built using information from the previous rounds.4

3 Preliminaries: analysis in the serial setting

First  we recall the analysis of a generic serial dual-averaging algorithm  known as Adaptive Follow-
the-Regularized-Leader (ADA-FTRL) [21  25  16]  that generalizes regularized dual-averaging [37]
and captures the dual-averaging variants of SGD  Ada-Grad  Proximal-SGD and EG as special case.

Serial ADA-FTRL. The serial ADA-FTRL algorithm uses a sequence of regularizer functions
r0  r1  r2  . . . . At time t = 1  2  . . .   given the previous feedback gs 2 Rd  s 2 [t  1]  ADA-FTRL
selects the next point xt such that
(2)

xt 2 arg min

x2X hzt1  xi + t(x) + r0:t1(x)  

where zt1 = g1:t1 is the sum of the past feedback. We refer to (zt  t  r0:t) as the state of the
algorithm at time t  noting that apart from tie-breaking in (2)  this state determines xt.
It is straightforward to verify that with  = 0 X = Rd  and r0:t1 = ⌘
2k·k 2 for some ⌘> 0  we get
the SGD update xt =  1
  i 2 [d] are positive
step-sizes (possibly adaptively tuned [22  9])  ADA-FTRL reduces to xt = prox(t zt1 ⌘ t) 
where prox is the generalized proximal operator oracle5 over X that  given a function and vectors
z and ⌘  returns6

⌘ g1:t1. In addition  using r0:t1 = 1

⌘t where ⌘(i)

2k·k 2

t

(3)

prox(   z  ⌘ ) := arg min

x2X

 (x) +

1

2x  ⌘1  z2

⌘ .

3With a slight abuse of notation  gt(x  ⇠) (with arguments x  ⇠) is from now on used to denote the oracle at

time t evaluated at x  ⇠  where as gt (without arguments) denotes the observed noisy gradient gt(xt ⇠ t).
4Note that in this case ⇠t remains an independent sequence  even though gt changes with the history.
5 Serial proximal DA [37] and ADA-FTRL call prox with t  whereas the conventional Proximal-SGD
algorithm (based on Mirror-Descent) invokes the proximal operator with  irrespective of the iteration; see
the paper of Xiao [37  Sections 5 and 6] for a detailed discussion of this phenomenon.

6Here ⌘1 denotes the elementwise inverse of ⌘ and  denotes elementwise multiplication.

4

When ⌘ is the same for all coordinates (in which case we simply treat it as a scalar)  this reduces
to prox(   z  ⌘ ) = arg minx2X (x) + ⌘
2kx  z/⌘k2  which is the standard proximal operator;
the generalized version (3) makes it possible to use coordinatewise step-sizes as in ADAGRAD
[22  9]. Finally  when  = 0 and X is the probability simplex  ADA-FTRL with the negen-
tropy regularizer r0:t1(x) = r0(x) = ⌘Pd
i=1 xi log(xi) for some ⌘> 0  recovers the update
x(i)
t = Ct exp(z(i)
t1/⌘) is the constant
normalizing xt to lie in X . Other choices of rt recover algorithms such as the p-norm update; we refer
to Shalev-Shwartz [34]  Hazan [14]  McMahan [21]  and Orabona et al. [25] for further examples.

t1/⌘) of the EG algorithm  where Ct = 1/Pj=1 exp(z(j)

Analysis of ADA-FTRL ADA-FTRL and its special cases have been extensively studied in the
literature [5  34  14  21  25  16]. In particular  it has been shown that under speciﬁc conditions on rt
and   which we discuss in detail in Appendix F  ADA-FTRL enjoys the following bound on the
linearized regret [25  16]:
Theorem 1 (Regret of ADA-FTRL). For any x⇤ 2X and any sequence of vectors (gt)T
t=1 in Rd 
using any sequence of regularizers r0  r1  . . .   rT that are admissible w.r.t. a sequence of norms
k·k (t) (see Deﬁnition 2 in Appendix F)  the iterates (xt)T
(hgt  xt  x⇤i + (xt)  (x⇤))  r0:T (x⇤) 

t=1 generated by ADA-FTRL satisfy

1
2kgtk2

rt(xt+1) +

(t ⇤) .

(4)

TXt=0

TXt=1

TXt=1

Importantly  this bound holds for any feedback sequence gt irrespective of the way it is generated 
and serves as a solid basis to derive bounds under different assumptions on f    and rt [25  16].

4 Relaxing the serial analysis: algorithms with perturbed state
In this section  we show that Theorem 1 can be used to analyze ADA-FTRL when its state undergoes
speciﬁc perturbations. This relaxation of the generic serial analysis framework underlies our analysis
of parallel asynchronous algorithms  since parallel algorithms like ASYNCADA and HEDGEHOG
can be viewed as serial ADA-FTRL algorithms with perturbed states  as we show in Sections 5 and 6.
Perturbed ADA-FTRL. Next  we show that Theorem 1 also provides the basis to analyze ADA-
FTRL with perturbed states. Speciﬁcally  suppose that instead of (2)  the iterate xt is given by

xt 2 arg min

x2X hˆzt1  xi + ˆtt(x) + ˆr0:t1(x) 

t = 1  2  . . .  

(5)

where ˆzt1 denotes a perturbed version of the dual vector zt1  ˆtt denotes a perturbed version of
ADA-FTRL’s iteration counter t  and ˆr0:t1 denotes a perturbed version of the regularizer r0:t1.
Then  we can analyze the regret of the Perturbed-ADA-FTRL update (5) by comparing xt to the
“ideal” iterate ˜xt  given by

˜xt := arg min

x2X hzt1  xi + t(x) + r0:t1(x) 

t = 1  2  . . . .

(6)

Since (˜xt)T
t=1 is given by a non-perturbed ADA-FTRL update  it enjoys the bound of Theorem 1. The
crucial observation of Duchi et al. [10] (who studied the special case of (5) with  = 0  box-shaped
X   and ˆrt = rt) was that the regret of Perturbed-ADA-FTRL is related to the linearized regret of ˜xt.
When  may be non-zero  we capture this relation by the next lemma  proved in Appendix A:
t=1 and (˜xt)T
Lemma 1 (Perturbation penalty of ADA-FTRL). Consider any sequences (xt)T
t=1 in
X   and any sequence (gt)T
of the sequence (xt)T
t=1 satisﬁes
TXt=1

(hgt  ˜xt  x⇤i + (˜xt)  (x⇤)) + ˜✏1:T + 1:T  B1:T  

t=1 in Rd. Then  the regret R(f +)

where ˜✏t = hgt  xt  ˜xti + (xt)  (˜xt)  t = hrft(xt)  gt  xt  x⇤i and Bt = Bft(x⇤  xt).
Since gt is an unbiased estimate of rft(xt) (conditionally given xt)  1:T is zero in expectation
0  and for ˜xt given by (6)  the ﬁrst summation is bounded by Theorem 1. Also note that when
the ft are (star-)convex  B1:T  0. Thus  to bound the regret of Perturbed-ADA-FTRL  it only

R(f +)

(7)

=

T

T

5

remains to control the “perturbation penalty” terms ˜✏t capturing the difference in the composite linear
loss hgt ·i +  between xt and ˜xt. In Appendix A  we use the stability of ADA-FTRL algorithms
(Lemma 3) to control ˜✏1:T   under a speciﬁc perturbation structure (coming from delayed updates to ˆzt)
that captures the evolution of the state of asynchronous dual-averaging algorithms like ASYNCADA
and HEDGEHOG. Unlike Duchi et al. [10]  our derivation applies to any convex constraint set X and 
crucially  to ADA-FTRL updates incorporating non-zero  and a perturbed counter ˆtt. The following
(informal) theorem  whose formal version is given in Appendix A  captures the result.
Theorem 4 (informal). Under appropriate independence  regularity  and structural assumptions on
the regularizers and the perturbations  the Perturbed-ADA-FTRL update (5) satisﬁes

o  E(r0:T (x⇤) +

TXt=1 1 + p⇤⌫t +Ps:t2Os

⌫t!  B1:T)  
EnR(f +)
where p⇤ ⌫ t ⌧ t and t measure  respectively  the sparsity of the gradient estimates gt  the difference
ˆttt  and the amount of perturbations in ˆzt1  and ˆr0:t1  while Os is the set of time steps whose
attributed perturbations affect iteration s (i.e.  their updates are delayed beyond s).
As we show next  we can control the effect of p⇤ ⌧t and t in the bound by appropriately tuning ˆtt 
resulting in linear speed-ups for ASYNCADA and HEDGEHOG.

kgtk2

(t ⇤) +

t

T

⌧s
⌫s

2

5 ASYNCADA: Asynchronous Composite Adaptive Dual Averaging
In this section  we introduce and analyze ASYNCADA for asynchronous noisy online optimization.
ASYNCADA consists of ⌧ processes running in parallel (e.g.  threads on the same physical machine
or computing nodes distributed over a network accessing a shared data store). The processes can
access a shared memory  consisting of a dual vector z 2 Rd to store the sum of observed gradient
estimates gt  a step-size vector ⌘ 2 Rd  and an integer t  referred to as the clock  to track the number
of iterations completed at each point in time. The processes run copies of Algorithm 1 concurrently.

Algorithm 1: ASYNCADA: Asynchronous Composite Adaptive Dual Averaging

1 repeat
2
3
4
5
6
7
8

ˆ⌘ a full (lock-free) read of the shared step-sizes ⌘
ˆz a full (lock-free) read of the shared dual vector z
t t + 1
ˆt t + 
Receive ⇠t
Compute the next iterate: xt prox(ˆtt ˆzt1  ˆ⌘t)
Obtain the noisy gradient estimate: gt gt(xt ⇠ t)
for j such that g(j)
Update the shared step-size vector ⌘

6= 0 do z(j) z(j) + g(j)

t

t

9
10
11 until terminated

// atomic read-increment
// denote ˆzt1 = ˆz  ˆ⌘t = ˆ⌘  ˆtt = ˆt

// prox defined in (3)

// atomic update

Inconsistent reads. The processes access the shared memory without necessarily acquiring a lock:
as in previous Hogwild!-style algorithms [30  20  18  17  27]  we only assume that operations on
single coordinates of z and ⌘  as well as on t0  are atomic. This in particular means that the values of
ˆz or ˆ⌘ read by a process may not correspond to an actual state of z or ⌘ at any given point in time  as
different processes can modify the coordinates in parallel while the read is taking place. A process ⇡
is in write-conﬂict with another process ⇡0 (equivalently  ⇡0 is in read-conﬂict with ⇡) if ⇡0 reads parts
of the memory which should have been updated by ⇡ before. To limit the effects of asynchrony  we
assume that a process can be in write- and read conﬂicts with at most ⌧c1 processes  respectively.
The role of . ASYNCADA uses an over-estimate ˆtt of the current global clock t by an additional .
This over-estimation enables us to better handle the effect of asynchrony when composite objectives
are involved  in particular ensuring the appropriate tuning of ⌫t in Theorem 4; see Appendix C.
ASYNCADA can nevertheless be run without  (i.e.  with  = 0).7

7 In Theorems 2  5 and 6  we set  based on ⌧⇤ := max{⌧c  ⌧}. The analysis is still possible  and
straightforward  with  = 0  but results in a worst constant factor in the rate  as well as an extra additive term of
order O(⌧ 2
⇤ ) where = sup x y2X {(x)  (y)} is the diameter of X w.r.t. . This term does not diminish
with p⇤ and may be unnecessarily large  affecting convergence in early stages of the optimization process.

6

Exact vs estimated clock. ASYNCADA as given in Algorithm 1 maintains the exact global clock t.
However  this option may not be desirable (or available) in certain asynchronous computing scenarios.
For example  if the processes are distributed over a network  then maintaining an exact global clock
amounts to changing the pattern of asynchrony and delaying the computations by repeated calls over
a network. To mitigate this requirement  in Appendix B we provide ASYNCADA(⇢)  a version of
ASYNCADA in which the processes update the global clock only every ⇢ iterations. ASYNCADA as
presented in Algorithm 1 is equivalent to ASYNCADA(⇢) with ⇢ = 1  and both algorithms enjoy the
same rate of convergence and linear speed-up. Obviously  when  ⌘ 0 and t is not used for setting
the step-sizes ⌘ either  there is no need to maintain t physically  and Line 4 can be omitted.
Updating the step-sizes ⌘:
In Line 10 of Algorithm 1  the step-size ⌘ has to be updated based on
the information received. The exact way this is done depends on the speciﬁc step-size sched-
ule.
In particular  we consider two situations: First  when the step-size is either constant or
a simple function of t (or ˆtt in case of ASYNCADA(⇢))  and second  when diagonal ADA-
GRAD step-sizes are used. In the ﬁrst case  the vector ⌘ need not be kept in the shared mem-
ory explicitly  and Lines 2 and 10 can be omitted.
In the second case  following [10]  we
store the sum of squared gradients in the shared ⌘  i.e.  Line 10 is implemented as follows:

10* for j such that g(j)

t

6= 0 do ⌘(j)2

 ⌘(j)2

+ ↵2⇣g(j)
t ⌘2

// atomic update

for a ﬁxed hyper-parameter ↵> 0. In this case  we are storing the square of ⌘ in the shared memory 
so a square root operation needs to be applied after reading the shared memory in Line 2 to retrieve ⌘.
Forming the output ¯xT for stochastic optimization: For stochastic optimization  the algorithm
needs to output the average (or randomized) iterate ¯xT at the end. However  this needs no further
coordination between the processes. To form the average iterate  it sufﬁces for each process to keep a
local running sum of the iterates it produces and the number of updates it makes. At the end  ¯xT is
built from these sums and the total number of updates. Alternatively  we can return a random iterate
as ¯xT by terminating the algorithm  with probability 1/T   after calculating x in Line 7.

t

5.1 Analysis of ASYNCADA
The analysis of ASYNCADA is based on treating it as a special case of Perturbed-ADA-FTRL. In
order to be able to use Theorem 4  we start with the following independence assumption on ⇠t:
Assumption 1 (Independence of ⇠t). For all t = 1  2  . . .   T   the t-th sample ⇠t is independent of the

history ˆHt := (⇠s  ˆzs  ˆ⌘s+1)t1
s=1 .
This  in turn  implies that ⇠t is independent of xt as well as xs and ⇠s for all s < t.
For general (non-box-shaped) X   Assumption 1 is plausible  as ASYNCADA needs to read z (and ⌘)
completely and independently of ⇠t. If X is box-shaped and  is coordinate-separable  however  the
values of x(j)
for different coordinates j can be calculated independently. In this  case  the algorithm
may ﬁrst sample ⇠t  and then only read the relevant coordinates j from z (and ⌘) for which gt may
be non-zero  as calculating other values of x(j)
is unnecessary for calculating gt. As mentioned by
Mania et al. [20]  this violates Assumption 1. This is because multiple other processes are updating z
and ⌘  and the updates that are included the value read for ˆzt1 (and ˆ⌘t) would then depend on ⇠t.
Previous papers either assume that this independence holds in their analysis  e.g.  by enforcing a full
read of z and ⌘  [20  18  17  27]  or rely on the smoothness of the objective to bound the effect of
the possible change in the read values [20  Appendix A]. It seems possible to adapt the argument of
Mania et al. [20  Appendix A] to ASYNCADA for box-shaped X   by comparing xt to the iterate
that would have been created based on the content of the shared memory right before the start of the
execution of the t-th iteration. This makes the analysis more complicated  and is not necessary when
X is not box-shaped; hence  we do not further pursue this construction in this paper.
Sparsity of the gradient estimates. For t 2 [T ] and j 2 [d]  let pt j to denote the probability that
the j-th coordinate of gt is non-zero given the history ˆHt  that is  pt j = Pg(j)
denote an upper-bound on maxt2[T ] j2[d] pt j. We use p⇤ as a measure of the sparsity of the problem.8
8 In stochastic optimization with a ﬁnite-sum objective f =Pm
i=1 fi  where gt = rf⇠t (xt) and ⇠t 2 [m]
is an index at time t sampled uniformly at random and independently of the history  one could measure the

6= 0 ˆHt . Let p⇤

t

t

7

1

T

1

2 +

⌘0

G2

⇤◆ .

2(1 + p⇤⌧ 2
⇤ )

2  G2

⇤

Non-adaptive and time-decaying step-sizes. We ﬁrst study the case when ⌘t is either a constant 
or varies only as a function of the estimated iteration count ˆtt. Recall that each concurrent iteration of
the algorithms can be in read- and write-conﬂict with at most ⌧c  1 other iterations  respectively 
and that the algorithm uses ⌧ parallel processes. Deﬁne ⌧⇤ = max{⌧c  ⌧}. The next theorem gives
bounds on the regret of ASYNCADA under various scenarios. It is proved in Appendix C  where a
similar result is also given for ASYNCADA(⇢) (Theorem 5).
Theorem 2. Suppose that either all ft  t 2 [T ] are convex  or  ⌘ 0 and ft ⌘ f for some star-convex
function f. Consider ASYNCADA running under Assumption 1 for T >⌧ 2
.
updates  using  = 2⌧ 2
⇤
⇤
Let ⌘0 > 0. Then:
for all t 2 [T ]  then using a ﬁxed ⌘t = ⌘0pT or a time-varying ⌘t = ⌘0pˆtt 
(i) If Ekgtk2
pT ✓⌘0kx⇤k2
T EnR(f +)
o 
2   and for all ⇠ 2 ⌅  F (· ⇠ ) is convex
⇤ := EkrF (x⇤ ·)k2
(ii) If ft = f = E⇠⇠P⌅ {F (x  ⇠)}  2
and 1-smooth w.r.t. the norm k·k l for some l 2 Rd with positive entries  then given a constant
⇤ ) and using a ﬁxed ⌘t i = c0li + ⌘0pT or a time-varying ⌘t i = c0li + ⌘0pˆtt 
c0 > 8(1 + p⇤⌧ 2
pT ✓⌘0kx⇤k2
o 
T EnR(f +)
c0kx⇤k2
1
(iii) If  is µ-strongly-convex and Ekgtk2
2  G2
for all t 2 [T ]  then using ⌘t ⌘ 0 or  equivalently 
ˆtt(x) + hz  xi = r⇤(z/ˆtt) 
prox(ˆtt z  0) := arg minx2X
o 
T EnR(f +)

Remark 1. If c = p⇤⌧ 2
is constant  the bounds match the corresponding serial bounds [16] up to
⇤
constant factors  implying a linear speed-up. This also extends the analysis of ASYNC-DA [10] to
non-box-shaped X   non-zero   time-varying step sizes  and smooth and strongly-convex objectives.9
Remark 2. Note that (10) holds for all time steps  and converges to zero as T grows  without
the knowledge of T or epoch-based updates. In case of ASYNCADA(⇢)  the algorithm does not
maintain an exact clock either. To our knowledge  this makes ASYNCADA(⇢) the ﬁrst Hogwild!-style
algorithm with an any-time guarantee without maintaining a global clock.
Remark 3. Since strongly convex functions have unbounded gradients on unbounded domains  it
is not possible to impose a uniform bound on the gradient of f +  in part (iii) for unconstrained
optimization (i.e.  when X = Rd). However  we only require the gradients of f  the non-strongly-
convex part of the objective  to be bounded  which is a feasible assumption. Similarly  Nguyen
et al. [24] analyzed strongly-convex optimization with unconstrained Hogwild! while avoiding the
aforementioned uniform boundedness assumption s using a global clock. ASYNCADA(⇢) achieves
the same result  but applies to arbitrary convex X and   without requiring a global clock.
Adaptive step-sizes. Due to space constraints  we relegate the analysis of ASYNCADA(⇢) with
AdaGrad step-sizes given by Line 10* to Appendix D.

⇤ )G2
⇤(1 + log(T ))
µT

4(1 + p⇤⌧ 2
⇤ )

2

⇤◆ .

2

l

+

T

(1 + p⇤⌧ 2

1

T

2 +

⌘0

(8)

(9)

.

(10)

T

⇤

6 HEDGEHOG: Hogwild-Style Hedge

Next  we present HEDGEHOG  which is  to our knowledge the ﬁrst asynchronous version of the EG
algorithm. The parallelization scheme is very similar to ASYNCADA  the difference being that EG
uses multiplicative updates rather than additive SGD-style updates. We focus only on the case of
 ⌘ 0. Each processe runs Lines 3–10 of Algorithm 2 concurrently with the other processes  sharing
the dual vector z.
sparsity of the problem through a “conﬂict graph” [30  20  17  27]  which is a bi-partite graph with fi  i 2 [m]
on the left and coordinates j 2 [d] on the right  and an edge between fi and coordinate j if rfi(x)(j) can be
non-zero for some x 2X . In this graph  let j denote the degree of the node corresponding to coordinate j and
r be the largest j  j 2 [d]. Then  it is straightforward to see that pt j  j/m. Thus  p⇤ = r/m is a valid
upper-bound  and gives the sparsity measure used  e.g.  by Leblond et al. [17] and Pedregosa et al. [27].
9Note that under the conditions considered in [10]  which include that X is box-shaped and  = 0  ASYNC-

DA requires a less restrictive sparsity regime of p⇤⌧⇤  c for linear speed-up.

8

Algorithm 2: HEDGEHOG!: Asynchronous Stochastic Exponentiated Gradient.
Input: Step size ⌘

1 Initialization
2
3 repeat in parallel by each process
4
5

Let z 0 be the shared sum of observed gradient estimates
ˆz a full lock-free read of the shared dual vector z
Receive ⇠t
t1/⌘⌘  
Compute the next iterate: w(i)
Normalize: xt wt/kwtk1
Obtain the noisy gradient estimate: gt gt(xt ⇠ t)
for j such that g(j)

t exp⇣ˆz(i)
6= 0 do z(j) z(j) + g(j)

6

7
8

t

t

9
10 until terminated

// t t + 1  denote ˆzt1 = ˆz

i = 1  2  . . .   d

// atomic update

As in ASYNCADA(⇢)  we index the iterations by the time they ﬁnish the reading of z in Line 4

s=1o to denote the
of HEDGEHOG (“after-read” labeling [18]). Similarly  we use ˆHt =n(⇠s  ˆzs)t1
history of HEDGEHOG at time t  and use ˆHt to deﬁne the sparsity measure p⇤ as in Section 5.1. Then 
we have the following regret bound for HEDGEHOG.
Theorem 3. Let X be the probability simplex X = {x|x(j) > 0 kxk1 = 1}  and suppose that either
ft are all convex  or ft ⌘ f for a star-convex f. Assume that for all t 2 [T ]  the sampling of ⇠t in
Line 5 of HEDGEHOG is independent of the history ˆHt. Then  after T updates  HEDGEHOG satisﬁes

EnR(f )

T o  ⌘ log(d) +

TXt=1

E⇢ 1 + pp⇤⌧⇤

2⌘

1 .
kgtk2

Remark 4. As in the case of ASYNCADA  as long as pp⇤⌧⇤ is a constant  the rate above matches
the worst-case rate of serial EG up to constant factors  implying a linear speed-up. In particular 
given an upper-bound G⇤ on E{kgtk1} and setting ⌘ = G⇤/pT log(d)  we recover the well-known
O(G⇤pT log(d)) rate for EG [14]  but in the paralell asynchronous setting.

7 Conclusion  limitations  and future work

We presented and analyzed ASYNCADA  a parallel asynchronous online optimization algorithm
with composite  adaptive updates  and global convergence rates under generic convex constraints and
convex composite objectives which can be smooth  non-smooth  or non-strongly-convex. We also
showed a similar global convergence for the so-called “star-convex” class of non-convex functions.
Under all of the aforementioned settings  we showed that ASYNCADA enjoys linear speed-ups when
the data is sparse. We also derived and analyzed HEDGEHOG  to our knowledge the ﬁrst Hogwild-
style asynchronous variant of the Exponentiated Gradient algorithm working on the probability
simplex  and showed that HEDGEHOG enjoyed similar linear speed-ups.
To derive and analyze ASYNCADA and HEDGEHOG  we showed that the idea of perturbed iterates 
used previously in the analysis of asynchronous SGD algorithms  naturally extends to generic dual-
averaging algorithms  in the form of a perturbation in the “state” of the algorithm. Then  building on
the work of Duchi et al. [10]  we studied a uniﬁed framework for analyzing generic adaptive dual-
averaging algorithms for composite-objective noisy online optimization (including ASYNCADA
and HEDGEHOG as special cases). Possible directions for future research include applying the
analysis to other problem settings  such as multi-armed bandits. In addition  it remains an open
problem whether such an analysis is obtainable for constrained adaptive Mirror Descent without
further restrictions on the regularizers (e.g.  smoothness of the regularizer seems to help). Finally  the
derivation of such data-dependent bounds for the ﬁnal (rather than the average) iterate in stochastic
optimization  without the usual strong-convexity and smoothness assumptions  remains an interesting
open problem.

9

References

[1] Heinz H Bauschke and Patrick L Combettes. Convex analysis and monotone operator theory

in Hilbert spaces. Springer Science & Business Media  2011.

[2] Loris Cannelli et al. “Asynchronous Parallel Algorithms for Nonconvex Big-Data Optimization.

Part I: Model and Convergence”. In: arXiv preprint arXiv:1607.04818 (2017).

[3] Loris Cannelli et al. “Asynchronous Parallel Algorithms for Nonconvex Big-Data Optimization.

Part II: Complexity and Numerical Results”. In: arXiv preprint arXiv:1701.04900 (2017).

[4] Loris Cannelli et al. “Asynchronous parallel algorithms for nonconvex optimization”. In: arXiv

preprint arXiv:1607.04818 (2016).

[5] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  Learning  and Games. New York  NY 

USA: Cambridge University Press  2006.

[6] Damek Davis  Brent Edmunds  and Madeleine Udell. “The sound of apalm clapping: Faster
nonsmooth nonconvex optimization with stochastic asynchronous palm”. In: Advances in
Neural Information Processing Systems. 2016  pp. 226–234.

[7] Christopher De Sa et al. “Taming the Wild: A Uniﬁed Analysis of Hogwild!-Style Algorithms”.

In: arXiv preprint arXiv:1506.06438 (2015).

[9]

[8] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. “SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives”. In: Advances in neural
information processing systems. 2014  pp. 1646–1654.
John Duchi  Elad Hazan  and Yoram Singer. “Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization”. In: Journal of Machine Learning Research 12 (July
2011)  pp. 2121–2159.
John Duchi  Michael I Jordan  and Brendan McMahan. “Estimation  optimization  and paral-
lelism when data is sparse”. In: Advances in Neural Information Processing Systems. 2013 
pp. 2832–2840.

[10]

[11] Francisco Facchinei  Gesualdo Scutari  and Simone Sagratella. “Parallel selective algorithms
for nonconvex big data optimization”. In: IEEE Transactions on Signal Processing 63.7 (2015) 
pp. 1874–1889.

[12] Olivier Fercoq and Peter Richtárik. “Optimization in high dimensions via accelerated  parallel 

and proximal coordinate descent”. In: SIAM Review 58.4 (2016)  pp. 739–771.
Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The elements of statistical learning.
Vol. 1. Springer series in statistics Springer  Berlin  2001.

[13]

[14] Elad Hazan. “Introduction to online convex optimization”. In: Foundations and Trends in

Optimization 2.3-4 (2016)  pp. 157–325.

[15] Rie Johnson and Tong Zhang. “Accelerating stochastic gradient descent using predictive
variance reduction”. In: Advances in Neural Information Processing Systems. 2013  pp. 315–
323.

[16] Pooria Joulani  András György  and Csaba Szepesvári. “A Modular Analysis of Adaptive
(Non-) Convex Optimization: Optimism  Composite Objectives  and Variational Bounds”.
In: Proceedings of Machine Learning Research (Algorithmic Learning Theory 2017). 2017 
pp. 681–720.

[17] Rémi Leblond  Fabian Pederegosa  and Simon Lacoste-Julien. “Improved asynchronous
parallel optimization analysis for stochastic incremental methods”. In: arXiv preprint
arXiv:1801.03749 (2018).

[18] Rémi Leblond  Fabian Pedregosa  and Simon Lacoste-Julien. “ASAGA: asynchronous parallel

SAGA”. In: arXiv preprint arXiv:1606.04809 (2016).
Ji Liu et al. “An asynchronous parallel stochastic coordinate descent algorithm”. In: arXiv
preprint arXiv:1311.1873 (2013).

[19]

[20] H. Mania et al. “Perturbed Iterate Analysis for Asynchronous Stochastic Optimization”. In:

ArXiv e-prints (July 2015). arXiv: 1507.06970 [stat.ML].

[21] H. Brendan McMahan. “A survey of Algorithms and Analysis for Adaptive Online Learning”.

In: Journal of Machine Learning Research 18.90 (2017)  pp. 1–50.

[22] H. Brendan McMahan and Matthew Streeter. “Adaptive bound optimization for online convex

optimization”. In: Proceedings of the 23rd Conference on Learning Theory. 2010.

10

[23] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Vol. 87. Springer

Science & Business Media  2013.

[24] Lam M Nguyen et al. “SGD and Hogwild! convergence without the bounded gradients

assumption”. In: arXiv preprint arXiv:1802.03801 (2018).

[25] Francesco Orabona  Koby Crammer  and Nicolò Cesa-Bianchi. “A generalized online mirror
descent with applications to classiﬁcation and regression”. English. In: Machine Learning 99.3
(2015)  pp. 411–435.

[26] Xinghao Pan et al. “Cyclades: Conﬂict-free asynchronous machine learning”. In: Advances in

Neural Information Processing Systems. 2016  pp. 2568–2576.

[27] Fabian Pedregosa  Rémi Leblond  and Simon Lacoste-Julien. “Breaking the Nonsmooth
Barrier: A Scalable Parallel Method for Composite Optimization”. In: Advances in Neural
Information Processing Systems. 2017  pp. 55–64.

[28] Zhimin Peng et al. “Arock: an algorithmic framework for asynchronous parallel coordinate

updates”. In: SIAM Journal on Scientiﬁc Computing 38.5 (2016)  A2851–A2879.

[29] Meisam Razaviyayn et al. “Parallel successive convex approximation for nonsmooth nonconvex
optimization”. In: Advances in Neural Information Processing Systems. 2014  pp. 1440–1448.
[30] Benjamin Recht et al. “Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient
Descent”. In: Advances in Neural Information Processing Systems 24. Ed. by J. Shawe-Taylor
et al. Curran Associates  Inc.  2011  pp. 693–701.

[31] Gesualdo Scutari  Francisco Facchinei  and Lorenzo Lampariello. “Parallel and distributed
methods for constrained nonconvex optimization—Part I: Theory”. In: IEEE Transactions on
Signal Processing 65.8 (2016)  pp. 1929–1944.

[32] Gesualdo Scutari and Ying Sun. “Parallel and distributed successive convex approximation
methods for big-data optimization”. In: Multi-agent Optimization. Springer  2018  pp. 141–
308.

[33] Gesualdo Scutari et al. “Parallel and distributed methods for constrained nonconvex
optimization-part ii: Applications in communications and machine learning”. In: IEEE Trans-
actions on Signal Processing 65.8 (2016)  pp. 1945–1960.

[34] Shai Shalev-Shwartz. “Online learning and online convex optimization”. In: Foundations and

Trends in Machine Learning 4.2 (2011)  pp. 107–194.

[35] Tao Sun  Robert Hannah  and Wotao Yin. “Asynchronous coordinate descent under more real-
istic assumptions”. In: Advances in Neural Information Processing Systems. 2017  pp. 6182–
6190.

[36] Yu-Xiang Wang et al. “Parallel and distributed block-coordinate Frank-Wolfe algorithms”. In:

International Conference on Machine Learning. 2016  pp. 1548–1557.

[37] Lin Xiao. “Dual averaging method for regularized stochastic learning and online optimization”.
In: Advances in Neural Information Processing Systems. 2009  pp. 2116–2124. (Visited on
02/05/2015).

11

,Pooria Joulani
András György
Csaba Szepesvari