2019,Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes,A dynamic treatment regime (DTR) consists of a sequence of decision rules  one per stage of intervention  that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the key aspects towards more personalized decision-making. In this paper  we investigate the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We develop the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings  without any access to historical data. We further derive informative bounds on the system dynamics of the underlying DTR from confounded  observational data. Finally  we combine these results and develop a novel RL algorithm that efficiently learns the optimal DTR while leveraging the abundant  yet imperfect confounded observations.,Near-Optimal Reinforcement Learning

in Dynamic Treatment Regimes

Department of Computer Science

Department of Computer Science

Elias Bareinboim

Columbia University
New York  NY 10027
eb@cs.columbia.edu

Junzhe Zhang

Columbia University
New York  NY 10027

junzhez@cs.columbia.edu

Abstract

A dynamic treatment regime (DTR) consists of a sequence of decision rules  one
per stage of intervention  that dictates how to determine the treatment assignment
to patients based on evolving treatments and covariates’ history. These regimes are
particularly effective for managing chronic disorders and is arguably one of the key
aspects towards more personalized decision-making. In this paper  we investigate
the online reinforcement learning (RL) problem for selecting optimal DTRs pro-
vided that observational data is available. We develop the ﬁrst adaptive algorithm
that achieves near-optimal regret in DTRs in online settings  without any access to
historical data. We further derive informative bounds on the system dynamics of
the underlying DTR from confounded  observational data. Finally  we combine
these results and develop a novel RL algorithm that efﬁciently learns the optimal
DTR while leveraging the abundant  yet imperfect confounded observations.

1

Introduction

In medical practice  a patient typically has to be treated at multiple stages; the physician repeatedly
adapts each treatment  tailored to the patient’s time-varying  dynamic state (e.g.  level of virus  results
of diagnostic tests). Dynamic treatment regimes (DTRs) [18] provide an attractive framework of
personalized treatments in longitudinal settings. Operationally  a DTR consists of decision rules that
dictate what treatment to provide at each stage  given the patient’s evolving conditions and history.
These decision rules are alternatively known as adaptive treatment strategies [12  13  19  33  34] or
treatment policies [16  37  38]. DTRs offer an effective vehicle for personalized management of
chronic conditions  including cancer  diabetes  and mental illnesses [36].
Consider the DTR instance regarding the treatment of alcohol dependence [19  6]  which is graphically
represented in Fig. 1a . Based on the condition of alcohol dependant patients (S1)  the physician
may prescribe a medication or behavioral therapy (X1). Patients are classiﬁed as responders or
non-responders (S2) based on their level of heavy drinking within the next two months. The physician
then must decide whether to continue the initial treatment or switch to an augmented plan combining
both medication and behavioral therapy (X2). The unobserved covariate U summarizes all the
unknown factors about the patient. We are interested in the primary outcome Y that is the percentage
of abstinent days over a 12-month period. The treatment policy π in this set-up is a sequence of
decision rules x1 ← π1(s1)  x2 ← π2(s1  s2  x1) selecting the values of X1  X2 based on the history.
Policy learning in a DTR setting is concerned with ﬁnding an optimal policy π that maximizes the
primary outcome Y . The main challenge is that since the parameters of the DTR are often unknown 
it’s not immediate how to directly compute the consequences of executing the policy do(π)  i.e.  the
expected value Eπ[Y ]. Most of the current work in the causal inference literature focus on trying to
identify this quantity  Eπ[Y ]  from ﬁnite observational data and causal assumptions about the data-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

S1

S2

Y

S1

S2

Y

X1

X2

U

(a) P ( ¯x2  ¯s2  y)

X1

X2

U

(b) P ¯π2 ( ¯x2  ¯s2  y)

Figure 1: Causal diagrams of (a) a DTR with K = 2 stages of intervention; and (b) a DTR in (a)
under sequential interventions do(X1 ∼ π1(X1|S1)  X2 ∼ π2(X2|S1  S2  X1)).

generating mechanisms (commonly through causal graphs and potential outcomes). Several criteria
and algorithms have been developed [23  28  4]. For instance  a criterion called sequential backdoor
[24] permits one to determine whether causal effects can be obtained by covariate adjustment. This
condition is also referred to as conditional ignorability or unconfoundedness [27  18]: there exists
no unobserved confounders (UCs) that simultaneously affects the treatment at any stage and all the
subsequent outcomes given a set of observed covariates. Whenever ignorability holds  a number of
efﬁcient estimation procedures exist  including popular methods based on the propensity score [26] 
inverse probability of treatment weighting [21  25]  and Q-learning [31  20].
In general  the combination of observational data and causal assumptions does not always lead
to point-identiﬁcation [23  Ch. 3-4]. An alternative is to randomize patients’ treatments at each
stage based on the previous decisions and observed outcomes; for instance  one popular strategy
is known as the sequential multiple assignment randomized trail (SMART) [19]. By the virtue of
randomization  the sequential backdoor condition is entailed. However  in practice  performing a
randomized experiment in the actual environment can be extremely costly and undesirable (due to
unintended consequences)  especially for domains where humans are the main research subjects
(e.g.  medicine  epidemiology  and psychology). Reinforcement learning (RL) [31] provides a unique
opportunity to efﬁciently learning DTRs due to its nature of balancing exploration and exploitation. A
typical RL agent learns by conducting adaptive  sequential experimentation: it repeatedly adjusts the
policy that is currently deployed based on the past outcomes. The goal is to learn an optimal policy
while minimizing the experimental cost. Efﬁcient RL algorithms have been successfully developed to
very general settings such as Markov decision processes (MDPs) [30  11  32]  where a ﬁnite state is
statistically sufﬁcient to summarize the treatments and covariates’ history. Variations of this setting
include multi-armed bandits [1]  partially-observable MDP [10  2]  and factored MDPs [22].
Our focus here is on learning a policy for an unknown DTR while leveraging the observational data.
This is a challenging setting for both causal inference and RL. As an example  consider data collected
from an unknown behavior policy of the DTR in Fig. 1a (i..e  x1 ← f1(s1  u)  x2 ← f2(s1  s2  x1  u) 
where both U and {f1  f2} are unobserved)  which is materialized in the form of the observational
distribution P (x1  x2  s1  s2  y) [23  pp. 205]. The existence of the unmeasured confounder U
leads to an immediate violation of the sequential backdoor criterion (e.g.  due to the spurious path
X1 ← U → Y )  which implies that the effect of the policy Eπ[Y ] is not identiﬁable [23  Ch. 4.4].
On the other hand  existing RL algorithms are not applicable either  which can be seen by noting
that DTRs are inherently non-Markovian – in words  the initial treatment X1 directly affects the
outcome Y . Even though an heuristic approach may be pursued (e.g.  Thompson Sampling [35])  and
could eventually converge  the same is still not optimal since it’s oblivious to all the observational
data. 1. Indeed  it is acknowledged in the literature [7  8] that the “development of statistically sound
estimation and inference techniques” for online RL settings “seem to be another very important
research direction”  especially when the increasing use of mobiles devices allows for the possibility
of continuous monitoring and just-in-time intervention.
The goal of this paper is to overcome these challenges. We will introduce novel RL strategies capable
of optimizing an unknown DTR while efﬁciently leveraging the imperfect  but large amounts of
observational data. In particular  our contributions are as follows: (1) We introduce the ﬁrst algorithm
(UC-DTR (Alg. 1)) that reaches the near-optimal regret bound in the pure DTR setting  without

1Standard off-policy RL methods such as Q-Learning rely on the condition of sequential backdoor  thus not

applicable for the confounded observational data. For a more elaborate discussion  see [7  Ch. 3.5]

2

observational data; (2) We derive novel bounds capable of exploiting observational data based on the
DTR structure (Thms. 5 and 6)  which are provably tight; (3) We develop a novel algorithm (UCc-DTR
(Alg. 2)) that efﬁciently incorporates these bounds and accelerates learning in the online setting. Our
results are validated on randomly generated DTRs and multi-stage clinical trials on cancer treatment.

1.1 Preliminaries

In this section  we introduce the basic notation and deﬁnitions used throughout the paper. We use
capital letters to denote variables (X) and small letters for their values (x). Let X represent the
domain of X and |X| its dimension. We consistently use the abbreviation P (x) to represent the
probabilities P (X = x). ¯Xk stands for a sequence {X1  . . .   Xk} (∅ if k < 1)  and ¯X k represents
its domain  i.e.  X1 × ··· × Xk. Further  we denote by I{·} the indicator function.
The basic semantical framework of our analysis rests on structural causal models (SCM) [23  Ch. 7].
A SCM M is a tuple (cid:104)U   V   F   P (u)(cid:105) where U is a set of exogenous (unobserved) variables and V
is a set of endogenous (observed) variables. F is a set of structural functions where fi ∈ F decides
the values of Vi ∈ V taking as argument a combination of other endogenous and exogenous variables
(i.e.  Vi ← fi(P Ai  Ui)  P Ai ⊆ V   Ui ⊆ U). The values of U are drawn from the distribution
P (u)  and induce an observational distribution P (v) [23  pp. 205]. Each SCM is associated with a
causal diagram in the form of a directed acyclic graph G  where nodes represent endogenous variables 
dashed nodes exogenous variables  and arrows stand for functional relations (e.g.  see Fig. 1).
An intervention on a set of endogenous variables X  denoted by do(x)  is an operation where values
of X are set to constants x  regardless of how they were ordinarily determined (through the functions
{fX : ∀X ∈ X}). For a SCM M  let Mx be a sub-model of M induced by intervention do(x).
The interventional distribution Px(y) induced by do(x) is the distribution over variables Y in the
sub-model Mx. For a more detailed discussion of SCMs  we refer readers to [23  Ch. 7].

2 Optimizing Dynamic Treatment Regimes

In this section  we will formalize the problem of online optimization in DTRs with confounded
observations and provide an efﬁcient solution. We start by deﬁning DTRs in the structural semantics.
Deﬁnition 1 (Dynamic Treatment Regime [18]). A dynamic treatment regime (DTR) is a SCM
(cid:104)U   V   F   P (u)(cid:105) where the endogenous variables V = { ¯XK  ¯SK  Y }; K ∈ N+ is the total stages of
interventions. For stage k = 1  . . .   K: (1) Xk is a ﬁnite decision decided by a behavior policy xk ←
fk(¯sk  ¯xk−1  u); (2) Sk is a ﬁnite state decided by a transition function sk ← τk(¯xk−1  ¯sk−1  u). Y
is the primary outcome at the ﬁnal state K  decided by a reward function y ← r(¯xK  ¯sK  u) bounded
in [0  1]. Values of exogenous variables U are drawn from the distribution P (u).
A DTR M∗ induces an observational distribution P (¯xK  ¯sK  y). Fig. 1a shows the causal diagram
of a DTR with K = 2 stages of interventions. A policy π for a DTR is a sequence of decision rules
¯πK  where each πk(xk|¯sk  ¯xk−1) is a function mapping from the domain of histories ¯Sk  ¯Xk−1 up
to stage k to a distribution over decision Xk. A policy is called deterministic if the above mappings
are from histories ¯Sk  ¯Xk−1 to the domain of decision Xk  i.e.  xk ← πk(¯sk  ¯xk−1). The collection
of possible policies  depending on the domains of the history and decision  deﬁne a policy space Π.
A policy π deﬁnes a sequence of stochastic interventions do(X1 ∼ π1(X1| ¯S1)  . . .   XK ∼
πK(XK| ¯SK  ¯XK−1))  which induce an interventional distribution over variables ¯XK  ¯SK  Y   i.e.:

K−1(cid:89)

Pπ(¯xK  ¯sK  y) = P¯xK (y|¯sK)

P¯xk (sk+1|¯sk)πk+1(xk+1|¯sk+1  ¯xk) 

(1)

k=0

where P¯xk (sk+1|¯sk) is the transition distribution at stage k and P¯xK (y|¯sK) is the reward distribution
over the primary outcome. Fig. 1b describes a DTR under K = 2 stages of interventions do(X2 ∼
π1(X1|S1)  X2 ∼ π2(X2|S1  S2  X1)). The expected cumulative reward of a policy π in a DTR
M∗ is given by Vπ(M∗) = Eπ[Y ]. We are searching for an optimal policy π∗ that maximizes the
cumulative reward  i.e.  π∗ = arg maxπ∈Π Vπ(M∗). It is a well-known fact in decision theory
that no stochastic policy can improve on the utility of the best deterministic policy (see  e.g.  [15 
Lem. 2.1]). Thus  in what follows  we will usually consider the policy space Π to be deterministic.

3

Algorithm 1: UC-DTR

Input: failure tolerance δ ∈ (0  1).

1: for all episodes t = 1  2  . . . do
2:

t as  respectively (cid:80)t−1
k=¯xk and(cid:80)t−1
counts Rt(¯sK  ¯xK) prior to episode t as(cid:80)t−1

Deﬁne event counts N t(¯sk  ¯xk) and N t(¯sk  ¯xk−1) for horizon k = 1  . . .   K prior to episode
k−1=¯xk−1. Further  deﬁne reward
K =¯xK .

i=1 I ¯Si
i=1 Y iI ¯Si

K =¯sK   ¯X i

i=1 I ¯Si

k=¯sk  ¯X i

k=¯sk  ¯X i

Compute estimates ˆP t
¯xk

(sk+1|¯sk) =

ˆP t
¯xk

(sk+1|¯sk) and ˆEt
N t(¯sk+1  ¯xk)

¯xK

max{1  N t(¯sk  ¯xk)}  

[Y |¯sK] as

ˆEt

¯xK

[Y |¯sK] =

Rt(¯sK  ¯xK)

max{1  N t(¯sk  ¯xk)} .

3:

4:

Let Mt denote a set of DTRs such that for any M ∈ Mt  its transition probabilities

P¯xk (sk+1|¯sk) and reward E¯xK [Y |¯sK] are close to estimates ˆP t

(sk+1|¯sk)  ˆEt

[Y |¯sK]  i.e. 

¯xK

(cid:13)(cid:13)(cid:13)P¯xk (·|¯sk) − ˆP t
(cid:12)(cid:12)(cid:12)E¯xK [Y |¯sK] − ˆEt

¯xk

(cid:13)(cid:13)(cid:13)1

≤

(cid:115)
6(cid:12)(cid:12)Sk+1
(cid:115)
(cid:12)(cid:12)(cid:12) ≤

(·|¯sk)

[Y |¯sK]

¯xK

(cid:12)(cid:12) log(2K(cid:12)(cid:12) ¯S k

¯xk

(cid:12)(cid:12)(cid:12)(cid:12) ¯X k

(cid:12)(cid:12)t/δ)

max{1  N t(¯sk  ¯xk)}
2 log(2K|S||X|t/δ)
max{1  N t(¯sK  ¯xK)} .

5:

Find the optimal policy πt of an optimistic DTR Mt ∈ Mt such that

Vπt(Mt) =

max

π∈Π M∈Mt

Vπ(M )

Execute policy πt for episode t and observe the samples ¯St

K  ¯X t

K  Y t.

6:
7: end for

 

(2)

(3)

(4)

Our goal is to optimize an unknown DTR M∗ based solely on the domains S = ¯S K  X = ¯X K and
the observational distribution P (¯xK  ¯sK  y) (i.e.  both F   P (u) are unknown). The agent (e.g.  a
physician) learns through repeated experiments of episodes t = 1  . . .   T . Each episode t contains a
k  performs an intervention do(X t
complete DTR process: at stage k  the agent observes the state St
k)
k+1; the primary outcome Y t is received at the ﬁnal stage K. The cumulative
and moves to the state St
t=1(Vπ∗ (M∗) − Y t)  i.e  the loss due to the fact that
the agent does not always pick the optimal policy π∗. We will assess and compare algorithms in
terms of their regret R(T ). A desirable asymptotic property is to have limT→∞ E[R(T )]/T = 0 
meaning that the agent eventually converges and ﬁnds the optimal policy π∗.

regret up to episode T is deﬁned as R(T ) =(cid:80)T

2.1 The UC-DTR Algorithm

We now introduce a new RL algorithm for optimizing an unknown DTR  which we call UC-DTR. We
will later prove that UC-DTR achieves near-optimal bound on the total regret given only the knowledge
of the domains S and X . Like many other online RL algorithms [1  11  22]  UC-DTR follows the
principle of optimism under uncertainty to balance exploration and exploitation. The algorithm
generally works in phases of model learning  optimistic planning  and strategy execution.
The details of UC-DTR procedure can be found in Alg. 1. The algorithm proceeds in episodes and
computes a new strategy πt from samples { ¯Si
i=1 collected so far at the beginning of each
[Y |¯sK] of the
episode t. Speciﬁcally  UC-DTR computes in Steps 1-3  the empirical estimates ˆEt
(sk+1|¯sk) of the transitional probabilities P¯xk (sk+1|¯sk) from
expected reward E¯xK [Y |¯sK]  and ˆP t
experimental samples collected prior to episode t. In Step 4  a set Mt of plausible DTRs is deﬁned in
(sk+1|¯sk). This
terms of conﬁdence region around the the empirical estimates ˆEt
guarantees that the true DTR M∗ is in the set Mt with high probability. In Step 5  UC-DTR computes
the optimal policy πt of the most optimistic instance Mt in the family of DTRs Mt that induces the
maximal optimal expected reward. This policy πt is executed throughout episode t and new samples
¯St
K  ¯X t

K  Y t are collected (Step 6).

[Y |¯sK] and ˆP t

K  Y i}t−1

K  ¯X i

¯xK

¯xK

¯xk

¯xk

4

Finding Optimistic DTRs The Step 5 of UC-DTR tries to ﬁnd an optimal policy πt for an optimistic
DTR Mt. While the Bellman equation [5] allows one to optimize a ﬁxed DTR  we need to ﬁnd a
DTR Mt that gives the maximal optimal reward among all plausible DTRs in Mt given by Eq. (3).
We now introduce a method that extends standard dynamic programming planners [5] to solve this
problem. We ﬁrst combine all DTRs in Mt to get an extended DTR M+. That is  we consider a DTR
M+ with continuous decision space ¯X + = ¯X +
K  where for each horizon k  each action ¯xk ∈ ¯X k 
each admissible transition probabilities P¯xk (sk+1|¯sk) according to Eq. (2)  there is an action in ¯X +
inducing the same probabilities P¯xk (sk+1|¯sk). Similar arguments also apply to the expected reward
E¯xK [Y |¯sK]. Then  for each policy π+ on M+  there is an DTR Mt ∈ Mt and a policy πt ∈ Π
such that policies π+ and πt induces the same transition probabilities on the respective DTR  and
vice versa. Thus  solving the optimization problem in Eq. (4) is equivalent to ﬁnding an optimal
[Y |¯sk  ¯xk−1]
policy π∗
in M+. The Bellman equation on M+ for k = 1  . . .   K − 1 is deﬁned as follows:

+ on the extended DTR M+. Let V ∗(¯sk  ¯xk−1) denote the optimal value Eπ∗

k

+

(cid:40)

(cid:26)(cid:88)

(cid:27)(cid:41)

 

(5)

V ∗(¯sk  ¯xk−1) = max
and V ∗(¯sK  ¯xK−1) = max

xk

xK

max

P¯xk (·|¯sk)∈P k
E¯xK [Y |¯sK ]∈R E¯xK [Y |¯sK] 

max

sk+1

V ∗(¯sk+1  ¯xk)P¯xk (sk+1|¯sk)

where R and P k are the convex polytope of parameters E¯xK [Y |¯sK] and P¯xk (sk+1|¯sk) deﬁned in
Eqs. (2) and (3)  respectively. The inner maximum in Eq. (5) is a linear program (LP) over the convex
polytope P k (or R)  which is solvable using standard LP algorithms.

2.2 Theoretical Analysis

We now analyze the asymptotic behavior of UC-DTR that will lead to a better understanding of its
theoretical guarantees. Given space constraints  all proofs are provided in the full technical report [40 
Appendix I]. The following proposition shows that the cumulative regret of UC-DTR after T steps is

at most ˜O(K(cid:112)|S||X|T )2.

Theorem 1. Fix a δ ∈ (0  1). With probability (w.p.) of at least 1 − δ  it holds for any T > 1  the
regret of UC-DTR with parameter δ is bounded by

R(T ) ≤ 12K(cid:112)|S||X|T log(2K|S||X|T /δ) + 4K(cid:112)T log(2T /δ).

(6)
It is also possible to obtain the instance-dependent bound on the expected regret. Let Π− denote
a set of sub-optimal policies {π ∈ Π : Vπ(M∗) < Vπ∗ (M∗)}. For any π ∈ Π−  let its gap in
expected reward between the optimal policy π∗ be ∆π = Vπ∗ (M∗) − Vπ(M∗). We next derive the
gap-dependent logarithmic bound on the expected regret of UC-DTR after T steps.
Theorem 2. For any T ≥ 1  with parameter δ = 1

T   the expected regret of UC-DTR is bounded by

(cid:26) 332K 2|S||X| log(T )

(cid:27)

E[R(T )] ≤ max
π∈Π−

∆π

+

32
∆3
π

+

4
∆π

+ 1.

(7)

Since Eq. (7) is a decreasing function relative to the gap ∆π  the maximum of the regret in Thm. 2
is achieved with the second best policy π− = arg minπ∈Π− ∆π. We also provide a corresponding
lower bound on the expected regret of any experimental algorithm.

Theorem 3. For any algorithm A  any natural numbers K ≥ 1  and(cid:12)(cid:12)S k(cid:12)(cid:12) ≥ 2 (cid:12)(cid:12)X k(cid:12)(cid:12) ≥ 2 for any

k ∈ {1  . . .   K}  there is a DTR M with horizon K  state domains S and action domains X   such
that the expected regret of A after T ≥ |S||X| episodes is as least

Thm. 3 implies that for any DTR instance  the cumulative regret of Ω((cid:112)|S||X|T ) is inevitable. The
regret upper bound ˜O(K(cid:112)|S||X|T ) in Thm. 1 is close to the lower bound Ω((cid:112)|S||X|T ) in Thm. 3 

which means that UC-DTR is near-optimal provided with only the domains of state S and actions X .

(8)

E[R(T )] ≥ 0.05(cid:112)|S||X|T

2 ˜O(·) is similar to O(·) but ignores log-terms  i.e.  f = ˜O(g) if and only if ∃k  f = O(g logk(g)).

5

3 Learning from Confounded Observations
The results presented so far (Thms. 1 to 3) establish the dimension of the state-action domain |S||X|
as the an important parameter for the information complexity of online learning in DTRs. When
domains S × X are high-dimensional  the cumulative regret will be signiﬁcant for any online
algorithm  no matter how sophisticated it might be. This observation suggests that we should explore
other reasonable assumptions to address the issues of high-dimensional domains.
A natural approach is to utilize the abundant observational data  which could be obtained by passively
observing other agents behaving in the environment. Despite all its power  the UC-DTR algorithm
does not make use of any knowledge in the the observational distribution P (¯sK  ¯xK  y). For the
remainder of this paper  we will present and study an efﬁcient procedure to incorporate observational
samples of P (¯sK  ¯xK  y)  so that the performance of online learners could be improved.
When states ¯SK satisfy the sequential backdoor criterion [24] with respect to treatments ¯XK and
the primary outcome Y   one could identify the transition probabilities P¯xk (sk+1|¯sk) and expected
reward E¯xK [Y |¯sk] from P (¯sK  ¯xK  y). The optimal policy is thus solvable using the standard
off-policy learning methods such as Q-learning [31  20]. However  issues of non-identiﬁability arise
in the general settings where the sequential backdoor does not hold (e.g.  see Fig. 1a).
Theorem 4. Given P (¯sK  ¯xK  y) > 0  there exists DTRs M1  M2 such that P M1 (¯sK  ¯xK  y) =
P M2(¯sK  ¯xK  y) = P (¯sK  ¯xK  y) while P M1

¯xK (¯sK  y) (cid:54)= P M2

¯xK (¯sK  y).

Thm. 4 is stronger than the standard non-identiﬁability results (e.g.  [14  Thm. 1]). It shows that
given any observational distribution P (¯sK  ¯xK  y)  one to construct two DTRs both compatible with
P (¯sK  ¯xK  y)  but disagrees in the interventional probabilities P¯xK (¯sK  y).

3.1 Bounds and Partial Identiﬁcation in DTRs

In this section  we consider a partial identiﬁcation task in DTRs which bounds parameters of
P¯xk (sk+1|¯sk) and E¯xK [Y |¯sk] from the observational distribution P (¯sK  ¯xK  y). Our ﬁrst result
shows that the gap between causal quantities P¯xk (¯sk+1) and P¯xk (¯sk) in a DTR is bounded by the
gap between the corresponding observational distributions P (¯sk+1  ¯xk) and P (¯sk  ¯xk).
Lemma 1. For a DTR  given P (¯sK  ¯xK  y)  for any k = 1  . . .   K − 1 

P¯xk (¯sk+1) − P¯xk (¯sk) ≤ P (¯sk+1  ¯xk) − P (¯sk  ¯xk).

(9)
Lem. 1 allows one to derive informative bounds of transition probabilities P¯xk (sk+1|¯sk) in a DTR 
which are consistently estimable from the observational data P (¯sK  ¯xK).
Theorem 5. For a DTR  given P (¯sK  ¯xK  y) > 0  for any k = 1  . . .   K − 1 

P (¯sk+1  ¯xk)
Γ(¯sk  ¯xk−1)

≤ P¯xk (sk+1|¯sk) ≤ Γ(¯sk+1  ¯xk)
Γ(¯sk  ¯xk−1)

 

(10)

where Γ(¯sk+1  ¯xk) = P (¯sk+1  ¯xk) − P (¯sk  ¯xk) + Γ(¯sk  ¯xk−1) and Γ(s1) = P (s1).
Bounds in Thm. 5 exploit the sequential functional relationships among states and treatments in
the underlying DTR  which improve over the best-known bounds reported in [17  3  39]. Let

(cid:2)a¯xk ¯sk (sk+1)  b¯xk ¯sk (sk+1)(cid:3) denote the bound over P¯xk (sk+1|¯sk) given by Eq. (10). We next show
that P¯xk (sk+1|¯sk) ∈(cid:2)a¯xk ¯sk (sk+1)  b¯xk ¯sk (sk+1)(cid:3) is indeed optimal without additional assumption.
(sk+1)(cid:3)
strictly contained in(cid:2)a¯xk ¯sk (sk+1)  b¯xk ¯sk (sk+1)(cid:3). By Thm. 6  we could always ﬁnd DTRs M1  M2
(sk+1)(cid:3)  which is a contradiction.

Theorem 6. Given P (¯sK  ¯xK  y) > 0  for any k ∈ {1  . . .   K − 1}  there exists DTRs M1  M2 such
¯xk (sk+1|¯sk) = a¯xk ¯sk (sk+1) 
that P M1(¯sK  ¯xK  y) = P M2(¯sK  ¯xK  y) = P (¯sK  ¯xK  y) while P M1
¯xk (sk+1|¯sk) = b¯xk ¯sk (sk+1).
P M2
Thm. 6 ensures the optimality of Thm. 5. Suppose there exists a bound [a(cid:48)

that are compatible with the observational data P (¯sK  ¯xK  y) while their transition probabilities
P¯xk (sk+1|¯sk) lie outside of the bound [a(cid:48)
As a corollary  one could apply methods of Lem. 1 and Thm. 5 to bound expected rewards E¯xK [Y |¯sk]
from P (¯sK  ¯xK  y). The optimality of the derived bounds follows immediately after Thm. 6.

(sk+1)  b(cid:48)

¯xk ¯sk

(sk+1)  b(cid:48)

¯xk ¯sk

¯xk ¯sk

¯xk ¯sk

6

Algorithm 2: Causal UC-DTR (UCc-DTR)
Input: failure tolerance δ ∈ (0  1)  causal bounds C.
1: Let Mc denote a set of DTRs compatible with causal bounds C  i.e.  for any M ∈ Mc  its
causal quantities P¯xk (sk+1|¯sk) and E¯xK [Y |¯sK] satisfy Eq. (13) and Eq. (14) respectively.
2: for all episodes t = 1  2  . . . do
3:
4:

Execute Steps 2-4 of UC-DTR (Alg. 1).
Find the optimal policy πt of an optimistic DTR Mt in Mc
Vπ(M )

t = Mt ∩ Mc such that

Vπt(Mt) =

max

π∈Π M∈Mc

t

(12)

.

(11)

Execute policy πt for episode t and observe the samples ¯St

K  ¯X t

K  Y t.

5:
6: end for

Corollary 1. For a DTR  given P (¯sK  ¯xK  y) > 0 

E[Y |¯sK  ¯xK]P (¯sK  ¯xK)

Γ(¯sK  ¯xK−1)

≤ E¯xK [Y |¯sk] ≤ 1 − (1 − E[Y |¯sK  ¯xK])P (¯sK  ¯xK)

Γ(¯sK  ¯xK−1)

Since E[Y |¯sK  ¯xK] ∈ [0  1]  the bounds in Eq. (11) are contained in [0  1] and are thus informative.
The bounds developed so far are functions of the observational distribution P (¯sK  ¯xK  y) which is
identiﬁable by the sampling process  and so generally can be estimated consistently. Speciﬁcally  we
estimate the bounds in Thm. 5 and Corol. 1 by the corresponding sample mean estimates. Standard
results of large-deviation theory are thus applicable to control the uncertainties due to ﬁnite samples.

3.2 The Causal UC-DTR Algorithm

CK =

Our goal in this section is to introduce a simple  yet principled approach for leveraging the new-found
bounds deﬁned in Thm. 5 and Corol. 1  hopefully improving the performance of UC-DTR procedure.
For k = 1  . . .   K − 1  let Ck denote a set of bounds over transition probabilities P¯xk (sk+1|¯sk)  i.e. 
(13)

(cid:110)∀¯sk+1  ¯xk : P¯xk (sk+1|¯sk) ∈(cid:2)a¯xk ¯sk (sk+1)  b¯xk ¯sk (sk+1)(cid:3)(cid:111)

Ck =

.

Similarly  let CK denote a set of bounds over the conditional expected reward E¯xK [Y |¯sK]  i.e. 

(cid:110)∀¯sK  ¯xK : E¯xK [Y |¯sK] ∈(cid:2)a¯xK  ¯sK   b¯xK  ¯sK

(cid:3)(cid:111)

.

(cid:12)(cid:12)a¯xk ¯sk (sk+1) − b¯xk ¯sk (sk+1)(cid:12)(cid:12) 

(14)
We denote by C a set of bounds {C1  . . .  CK} on the system dynamics of the DTR  called causal
bounds. Our procedure Causal UC-DTR (for short  UCc-DTR) is summarized in Alg. 2. UCc-DTR is
similar to the original UC-DTR but exploits causal bounds C. It maintains a set of possible DTRs Mc
compatible with the causal bounds C (Step 1). Before each episode t  it computes the optimal policy
πt of an optimistic DTRs Mt in set Mc
t = Mt ∩ Mc (Step 3). Similar to UC-DTR  πt could be
obtained by solving LPs deﬁned in Eq. (5) subject to additional causal constraints Eqs. (13) and (14).
We next analyze asymptotic properties of UCc-DTR  showing that it consistently outperforms UC-DTR.

Let(cid:13)(cid:13)Ck
(cid:13)(cid:13)1 denote the maximal L1 norm of any parameter in Ck  i.e.  for any k = 1  . . .   K − 1 
(cid:88)
(cid:13)(cid:13)Ck
(cid:13)(cid:13)1 = max
(cid:12)(cid:12)a¯xK  ¯sK − b¯xK  ¯sK
(cid:12)(cid:12).
Further  let(cid:13)(cid:13)C(cid:13)(cid:13)1 =(cid:80)K
(cid:13)(cid:13)1. The total regret of UCc-DTR after T steps is bounded as follows.
(cid:110)
12K(cid:112)|S||X|T log(2K|S||X|T /δ) (cid:13)(cid:13)C(cid:13)(cid:13)1T

+ 4K(cid:112)T log(2T /δ).
Eq. (6) if T < 122|S||X| log(2K|S||X|T /δ)/(cid:13)(cid:13)C(cid:13)(cid:13)2
UC-DTR when the causal bounds C are informative  i.e.  the dimension(cid:13)(cid:13)C(cid:13)(cid:13)1 is small.

It is immediate from Thm. 7 that the regret bound in Eq. (15) is smaller than the bound given by
1. This means that UCc-DTR has a head start over

Theorem 7. Fix a δ ∈ (0  1). With probability of at least 1 − δ  it holds for any T > 1  the regret of
UCc-DTR with parameter δ and causal bounds C is bounded by

(cid:13)(cid:13)1 = max

and(cid:13)(cid:13)CK

R(T ) ≤ min

(cid:13)(cid:13)Ck

(cid:111)

(15)

¯xK  ¯sK

k=1

¯xk ¯sk

sk+1

7

(a) Random DTR

(b) Random DTR

(c) Cancer Treatment

Figure 2: Simulations comparing online learners that are randomized (rand)  adaptive (uc-dtr) and
causally enhanced (ucc-dtr). Graphs are rendered in high resolution and can be zoomed in.

We could also witness the improvements of causal bounds on the total expected regret. Let Π−
C be
the set of sub-optimal policies that their maximal expected rewards over instances in Mc are no less
C = {π ∈ Π− : maxM∈Mc Vπ(M ) ≥ Vπ∗ (M∗)}.
than the true optimal value Vπ∗ (M∗)  i.e.  Π−
The following is the instance-dependent bound on the total regret of UCc-DTR after T steps.
T and causal bounds C  the expected regret of
Theorem 8. For any T ≥ 1  with parameter δ = 1
UCc-DTR is bounded by

(cid:26) 332K 2|S||X| log(T )

(cid:27)

E[R(T )] ≤ max
−
π∈Π
C

∆π

+

32
∆3
π

+

4
∆π

+ 1.

(16)

C ⊆ Π−  it follows that the regret bound in Thm. 8 is small than or equal to Eq. (7)  i.e. 
Since Π−
UCc-DTR consistently dominates UC-DTR in terms of the performance. For instance  in a multi-armed
bandit model (i.e.  1-stage DTR with S1 = ∅) with optimal reward µ∗  the regret of UCc-DTR is
O(|X| log(T )/∆x) where ∆x is the smallest gap among sub-optimal arms x satisfying bx ≥ µ∗.

4 Experiments

We demonstrate our algorithms on several dynamic treatment regimes  including randomly generated
DTRs  and the survival model in the context of multi-stage cancer treatment. We found that our
algorithms could efﬁciently found the optimal policy; the observational data typically improve the
convergence rate of online RL learners despite the confounding bias.
In all experiments  we test sequentially randomized trials (rand)  UC-DTR algorithm (uc-dtr) and the
causal UC-DTR (ucc-dtr) with causal bounds derived from 1 × 105 confounded observational samples.
Each experiment lasts for T = 1.1 × 104 episodes. The parameter δ = 1
KT for uc-dtr and ucc-dtr
where K is the total stages of interventions. For all algorithms  we measure their cumulative regret
over 200 repetitions. We refer readers to the complete technical report [40  Appendix II] for the more
details on the experimental set-up.

Random DTRs We generate 200 random instances and observational distributions of the DTR
described in Fig. 1. We assume treatments X1  X2  states S1  S2 and primary outcome Y are all
binary variables; values of each variable are decided by their corresponding unobserved counter-
  Y¯x2 following deﬁnitions in [3  9]. The probabilities of the joint distribution
facutals S2x1
  y¯x2) are drawn randomly over [0  1]. The cumulative regrets average among
P (s1  x1  s2x1
all random DTRs are reported in Fig. 2a. We ﬁnd that online methods (uc-dtr  ucc-dtr) dominate
randomized assignments (rand); RL learners that leverage causal bounds (ucc-dtr) consistently domi-
nates learners that do not (uc-dtr). Fig. 2b reports the relative improvement in total regrets of ucc-dtr
compared to uc-dtr among 200 instances: ucc-dtr outperforms uc-dtr in over 80% of generated DTRs.
This suggests that causal bounds derived from the observational data are beneﬁcial in most instances.

  X2x1
  x2x1

Cancer Treatment We test the survival model of the two-stage clinical trial conducted by the
Cancer and Leukemia Group B [16  37]. Protocol 8923 was a double-blind  placebo controlled
two-stage trial reported by [29] examining the effects of infusions of granulocyte-macrophage
colony-stimulating factor (GM-CSF) after initial chemotherapy in patients with acute myelogenous

8

leukemia (AML). Standard chemotherapy for AML could place patients at increased risk of death
due to infection or bleeding-related complications. GM-CSF administered after chemotherapy might
assist patient recovery  thus reducing the number of deaths due to such complications. Patients
were randomized initially to GM-CSF or placebo following standard chemotherapy. Later  patients
meeting the criteria of complete remission and consenting to further participation were offered a
second randomization to one of two intensiﬁcation treatments.
Fig. 1a describes the DTR of this two-stage trail. X1 represents the initial GM-CSF administration
and X2 represents the intensiﬁcation treatment; the initial state S1 = ∅ and S2 indicates the complete
remission after the ﬁrst treatment; the primary outcome Y indicates the survival of patients at the time
of recording. We generate observational samples using age of patients as UCs U. The cumulative
regrets average among all random DTRs are reported in Fig. 2b. We ﬁnd that rand performs worst
among all strategies; uc-dtr ﬁnds the optimal policy with sub-linear regrets. Interestingly  ucc-dtr
converges almost immediately  suggesting that causal bounds derived from confounded observations
could signiﬁcantly improve the performance of online learners.

5 Conclusion

In this paper  we investigated the online reinforcement learning problem for selecting the optimal
DTR provided with abundant  yet imperfect observations made about the underlying environment.
We ﬁrst presented an online RL algorithm with near-optimal regret bounds in DTRs solely based
on the knowledge about state-action domains. We further derived causal bounds about the system
dynamics in DTRs from the observational data. These bounds could be incorporated in a simple  yet
principled way to improve the performance of online RL learners. In today’s healthcare  for example 
the growing use of mobile devices opens new opportunities in continuous monitoring of patients’
conditions and just-in-time interventions. We believe that our results constitute a signiﬁcant step
towards the development of a more principled and robust science of precision medicine.

Acknowledgments

This research is supported in parts by grants from IBM Research  Adobe Research  NSF IIS-1704352 
and IIS-1750807 (CAREER).

References
[1] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

[2] K. Azizzadenesheli  A. Lazaric  and A. Anandkumar. Reinforcement learning of pomdp’s using

spectral methods. In COLT  2016.

[3] A. Balke and J. Pearl. Counterfactuals and policy analysis in structural models. In Proceedings

of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence  pages 11–18  1995.

[4] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the

National Academy of Sciences  113:7345–7352  2016.

[5] R. Bellman. Dynamic programming. Science  153(3731):34–37  1966.

[6] B. Chakraborty. Dynamic treatment regimes for managing chronic health conditions: a statistical

perspective. American journal of public health  101(1):40–45  2011.

[7] B. Chakraborty and E. Moodie. Statistical methods for dynamic treatment regimes. Springer 

2013.

[8] B. Chakraborty and S. A. Murphy. Dynamic treatment regimes. Annual review of statistics and

its application  1:447–464  2014.

[9] C. Frangakis and D. Rubin. Principal stratiﬁcation in causal inference. Biometrics  1(58):21–29 

2002.

9

[10] Z. D. Guo  S. Doroudi  and E. Brunskill. A pac rl algorithm for episodic pomdps. In Artiﬁcial

Intelligence and Statistics  pages 510–518  2016.

[11] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research  11(Apr):1563–1600  2010.

[12] P. W. Lavori and R. Dawson. A design for testing clinical strategies: biased adaptive within-
subject randomization. Journal of the Royal Statistical Society: Series A (Statistics in Society) 
163(1):29–38  2000.

[13] P. W. Lavori and R. Dawson. Adaptive treatment strategies in chronic disease. Annu. Rev. Med. 

59:443–453  2008.

[14] S. Lee  J. D. Correa  and E. Bareinboim. General identiﬁability with arbitrary surrogate
experiments. In Proceedings of Thirty-ﬁfth Conference on Uncertainty in Artiﬁcial Intelligence
(UAI)  Corvallis  OR  2019. AUAI Press.

[15] Q. Liu and A. Ihler. Belief propagation for structured decision making. In Proceedings of
the Twenty-Eighth Conference on Uncertainty in Artiﬁcial Intelligence  pages 523–532. AUAI
Press  2012.

[16] J. K. Lunceford  M. Davidian  and A. A. Tsiatis. Estimation of survival distributions of treatment
policies in two-stage randomization designs in clinical trials. Biometrics  58(1):48–57  2002.
[17] C. Manski. Nonparametric bounds on treatment effects. American Economic Review  Papers

and Proceedings  80:319–323  1990.

[18] S. A. Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society:

Series B (Statistical Methodology)  65(2):331–355  2003.

[19] S. A. Murphy. An experimental design for the development of adaptive treatment strategies.

Statistics in medicine  24(10):1455–1481  2005.

[20] S. A. Murphy. A generalization error for q-learning. Journal of Machine Learning Research 

6(Jul):1073–1097  2005.

[21] S. A. Murphy  M. J. van der Laan  J. M. Robins  and C. P. P. R. Group. Marginal mean models
for dynamic regimes. Journal of the American Statistical Association  96(456):1410–1423 
2001.

[22] I. Osband and B. Van Roy. Near-optimal reinforcement learning in factored mdps. In Advances

in Neural Information Processing Systems  pages 604–612  2014.

[23] J. Pearl. Causality: Models  Reasoning  and Inference. Cambridge University Press  New York 

2000. 2nd edition  2009.

[24] J. Pearl and J. Robins. Probabilistic evaluation of sequential plans from causal models with
hidden variables. In P. Besnard and S. Hanks  editors  Uncertainty in Artiﬁcial Intelligence 11 
pages 444–453. Morgan Kaufmann  San Francisco  1995.

[25] J. Robins  L. Orellana  and A. Rotnitzky. Estimation and extrapolation of optimal treatment and

testing strategies. Statistics in medicine  27(23):4678–4721  2008.

[26] P. Rosenbaum and D. Rubin. The central role of propensity score in observational studies for

causal effects. Biometrika  70:41–55  1983.

[27] D. Rubin. Bayesian inference for causal effects: The role of randomization. Annals of Statistics 

6(1):34–58  1978.

[28] P. Spirtes  C. N. Glymour  and R. Scheines. Causation  prediction  and search. MIT press 

2000.

[29] R. M. Stone  D. T. Berg  S. L. George  R. K. Dodge  P. A. Paciucci  P. Schulman  E. J. Lee  J. O.
Moore  B. L. Powell  and C. A. Schiffer. Granulocyte–macrophage colony-stimulating factor
after initial chemotherapy for elderly patients with primary acute myelogenous leukemia. New
England Journal of Medicine  332(25):1671–1677  1995.

10

[30] A. L. Strehl  L. Li  E. Wiewiora  J. Langford  and M. L. Littman. Pac model-free reinforcement
learning. In Proceedings of the 23rd international conference on Machine learning  pages
881–888. ACM  2006.

[31] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press  1998.

[32] I. Szita and C. Szepesvári. Model-based reinforcement learning with nearly tight exploration
complexity bounds. In Proceedings of the 27th International Conference on Machine Learning
(ICML-10)  pages 1031–1038  2010.

[33] P. F. Thall  R. E. Millikan  and H.-G. Sung. Evaluating multiple treatment courses in clinical

trials. Statistics in medicine  19(8):1011–1028  2000.

[34] P. F. Thall  H.-G. Sung  and E. H. Estey. Selecting therapeutic strategies based on efﬁcacy and
death in multicourse clinical trials. Journal of the American Statistical Association  97(457):29–
39  2002.

[35] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[36] E. H. Wagner  B. T. Austin  C. Davis  M. Hindmarsh  J. Schaefer  and A. Bonomi. Improving

chronic illness care: translating evidence into action. Health affairs  20(6):64–78  2001.

[37] A. S. Wahed and A. A. Tsiatis. Optimal estimator for the survival distribution and related
quantities for treatment policies in two-stage randomization designs in clinical trials. Biometrics 
60(1):124–133  2004.

[38] A. S. Wahed and A. A. Tsiatis. Semiparametric efﬁcient estimation of survival distributions in
two-stage randomisation designs in clinical trials with censored data. Biometrika  93(1):163–
177  2006.

[39] J. Zhang and E. Bareinboim. Transfer learning in multi-armed bandits: a causal approach.
In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence  pages
1340–1346. AAAI Press  2017.

[40] J. Zhang and E. Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes.

Technical Report R-48  Causal AI Lab  Columbia University.  2019.

11

,Junzhe Zhang
Elias Bareinboim