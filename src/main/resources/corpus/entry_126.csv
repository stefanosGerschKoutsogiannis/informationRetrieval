2017,A Minimax Optimal Algorithm for Crowdsourcing,We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide  which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE)  an algorithm for estimating the reliability of workers. TE has low complexity  may be implemented in a streaming setting when labels are provided by workers in real time  and does not rely on an iterative procedure. We prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data.,A Minimax Optimal Algorithm for Crowdsourcing

Thomas Bonald
Telecom ParisTech

Richard Combes

Centrale-Supelec / L2S

thomas.bonald@telecom-paristech.fr

richard.combes@supelec.fr

Abstract

We consider the problem of accurately estimating the reliability of workers based
on noisy labels they provide  which is a fundamental question in crowdsourcing.
We propose a novel lower bound on the minimax estimation error which applies
to any estimation procedure. We further propose Triangular Estimation (TE)  an
algorithm for estimating the reliability of workers. TE has low complexity  may
be implemented in a streaming setting when labels are provided by workers in real
time  and does not rely on an iterative procedure. We prove that TE is minimax
optimal and matches our lower bound. We conclude by assessing the performance
of TE and other state-of-the-art algorithms on both synthetic and real-world data.

1 Introduction

The performance of many machine learning techniques  and in particular data classiﬁcation  strongly
depends on the quality of the labeled data used in the initial training phase. A common way to label
new datasets is through crowdsourcing: many workers are asked to label data  typically texts or
images  in exchange of some low payment. Of course  crowdsourcing is prone to errors due to
the difﬁculty of some classiﬁcation tasks  the low payment per task and the repetitive nature of the
job. Some workers may even introduce errors on purpose. Thus it is essential to assign the same
classiﬁcation task to several workers and to learn the reliability of each worker through her past
activity so as to minimize the overall error rate and to improve the quality of the labeled dataset.

Learning the reliability of each worker is a tough problem because the true label of each task  the
so-called ground truth  is unknown; it is precisely the objective of crowdsourcing to guess the true
label. Thus the reliability of each worker must be inferred from the comparison of her labels on
some set of tasks with those of other workers on the same set of tasks.

In this paper  we consider binary labels and study the problem of estimating the workers reliability
based on the answers they provide to tasks. We make two novel contributions to that problem:

(i) We derive a lower bound on the minimax estimation error which applies to any estimator of
the workers reliability. In doing so we identify "hard" instances of the problem  and show that the
minimax error depends on two factors: the reliability of the three most informative workers and the
mean reliability of all workers.

(ii) We propose TE (Triangular Estimation)  a novel algorithm for estimating the reliability of each
worker based on the correlations between triplets of workers. We analyze the performance of TE and
prove that it is minimax optimal in the sense that it matches the lower bound we previously derived.
Unlike most prior work  we provide non-asymptotic performance guarantees which hold even for a
ﬁnite number of workers and tasks. As our analysis reveals  non-asymptotic performance guarantees
require to use ﬁner concentration arguments than asymptotic ones.

TE has low complexity in terms of memory space and computation time  does not require to store
the whole data set in memory and can be easily applied in a setting in which answers to tasks arrive

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

sequentially  i.e.  in a streaming setting. Finally  we compare the performance of TE to state-of-the-
art algorithms through numerical experiments using both synthetic and real datasets.

2 Related Work

The ﬁrst problems of data classiﬁcation using independent workers appeared in the medical con-
text  where each label refers to the state of a patient (e.g.  sick or sane) and the workers are clini-
cians. [Dawid and Skene  1979] proposed an expectation-maximization (EM) algorithm  admitting
that the accuracy of the estimate was unknown. Several versions and extensions of this algorithm
have since been proposed and tested in various settings [Hui and Walter  1980  Smyth et al.  1995 
Albert and Dodd  2004  Raykar et al.  2010  Liu et al.  2012].

A number of Bayesian techniques have also been proposed and applied to this problem by
[Raykar et al.  2010  Welinder and Perona  2010  Karger et al.  2011  Liu et al.  2012  Karger et al. 
2014  2013] and references therein. Of particular interest is the belief-propagation (BP) algorithm
of [Karger et al.  2011]  which is provably order-optimal in terms of the number of workers required
per task for any given target error rate  in the limit of an inﬁnite number of tasks and an inﬁnite
population of workers.

Another family of algorithms is based on the spectral analysis of some matrix representing the
correlations between tasks or workers. [Ghosh et al.  2011] work on the task-task matrix whose
entries correspond to the number of workers having labeled two tasks in the same manner  while
[Dalvi et al.  2013] work on the worker-worker matrix whose entries correspond to the number of
tasks labeled in the same manner by two workers. Both obtain performance guarantees by the
perturbation analysis of the top eigenvector of the corresponding expected matrix. The BP algorithm
of Karger  Oh and Shah is in fact closely related to these spectral algorithms: their message-passing
scheme is very similar to the power-iteration method applied to the task-worker matrix  as observed
in [Karger et al.  2011].

Two notable recent contributions are [Chao and Dengyong  2015] and [Zhang et al.  2014]. The
former provides performance guarantees for two versions of EM  and derives lower bounds on the
attainable prediction error (the probability of estimating labels incorrectly). The latter provides
lower bounds on the estimation error of the workers’ reliability as well as performance guarantees
for an improved version of EM relying on spectral methods in the initialization phase. Our lower
bound cannot be compared to that of [Chao and Dengyong  2015] because it applies to the workers’
reliability and not the prediction error; and our lower bound is tighter than that of [Zhang et al. 
2014]. Our estimator shares some features of the algorithm proposed by [Zhang et al.  2014] to
initialize EM  which suggests that the EM phase itself is not essential to attain minimax optimality.

All these algorithms require the storage of all labels in memory and  to the best of our knowledge 
the only known streaming algorithm is the recursive EM algorithm of [Wang et al.  2013]  for which
no performance guarantees are available.

The remainder of the paper is organized as follows. In section 3 we state the problem and introduce
our notations. The important question of identiﬁability is addressed in section 4. In section 5 we
present a lower bound on the minimax error rate of any estimator. In section 6 we present TE  discuss
its compexity and prove that it is minimax optimal. In section 7 we present numerical experiments
on synthetic and real-world data sets and section 8 concludes the paper. Due to space constraints 
we only provide proof outlines for our two main results in this document. Complete proofs are
presented in the supplementary material.

3 Model

Consider n workers  for some integer n ≥ 3. Each task consists in determining the answer to a
binary question. The answer to task t  the “ground-truth"  is denoted by G(t) ∈ {+1 −1}. We
assume that the random variables G(1)  G(2)  . . . are i.i.d. and centered  so that there is no bias
towards one of the answers.
Each worker provides an answer with probability α ∈ (0  1]. When worker i ∈ {1  ...  n} provides
an answer  this answer is correct with probability 1
2 (1 + θi)  independently of the other workers  for
some parameter θi ∈ [−1  1] that we refer to as the reliability of worker i. If θi > 0 then worker

2

i tends to provide correct answers; if θi < 0 then worker i tends to provide incorrect anwsers; if
θi = 0 then worker i is non-informative. We denote by θ = (θ1  . . .   θn) the reliability vector. Both
α and θ are unknown.
Let Xi(t) ∈ {−1  0  1} be the output of worker i for task t  where the output 0 corresponds to the
absence of an answer. We have:

Xi(t) =


w.p. α 1+θi
G(t)
2  
−G(t) w.p. α 1−θi
2
0
1 − α.

w.p.

(1)

Since the workers are independent  the random variables X1(t)  ...  Xn(t) are independent given
G(t)  for each task t. We denote by X(t) the corresponding vector. The goal is to estimate the
ground-truth G(t) as accurately as possible by designing an estimator ˆG(t) that minimizes the error
probability P( ˆG(t) 6= G(t)). The estimator ˆG(t) is adaptive and may be a function of X(1)  ...  X(t)
but not of the unknown parameters α  θ.

It is well-known that  given θ and α = 1  an optimal estimator of G(t) is the weighted majority vote
[Nitzan and Paroush  1982  Shapley and Grofman  1984]  namely

ˆG(t) = 1{W (t) > 0} − 1{W (t) < 0} + Z 1{W (t) = 0} 
i=1 wiXi(t)  wi = ln( 1+θi
where W (t) = 1
) is the weight of worker i (possibly inﬁnite)  and Z
1−θi
is a Bernoulli random variable of parameter 1
2 over {+1 −1} (for random tie-breaking). We prove
this result for any α ∈ (0  1].
Proposition 1 Assuming that θ is known  the estimator (2) is an optimal estimator of G(t).

nPn

(2)

Proof. Finding an optimal estimator of G(t) amounts to ﬁnding an optimal statistical test between
hypotheses {G(t) = +1} and {G(t) = −1}  under a symmetry constraint so that type I and type II
error probability are equal. For any x ∈ {−1  0  1}n  let L+(x) and L−(x) be the probabilities that
X(t) = x under hypotheses {G(t) = +1} and {G(t) = −1}  respectively. We have

n

(1 + θi)1{xi=+1}(1 − θi)1{xi=−1} 

(1 + θi)1{xi=−1}(1 − θi)1{xi=+1} 

L−(x) = H(x)

L+(x) = H(x)

n

Yi=1
Yi=1
L−(x)(cid:17) = Pn

i=1 |xi| is the number of answers and H(x) = 1

where ℓ = Pn
2ℓ αℓ(1 − α)n−ℓ. We deduce the
log-likelihood ratio ln(cid:16) L+(x)
i=1 wixi. By the Neyman-Pearson theorem  for any level
of signiﬁcance  there exists a and b such that the uniformly most powerful test for that level is:
1{wT x > a} − 1{wT x < a} + Z 1{wT x = a}  where Z is a Bernoulli random variable of
parameter b over {+1 −1}. By symmetry  we must have a = 0 and b = 1
This result shows that estimating the true answer G(t) reduces to estimating the unknown parameters
α and θ  which is the focus of the paper. Note that the problem of estimating θ is important in itself 
due to the presence of "spammers" (i.e.  workers with low reliability); a good estimator can be used
by the crowdsourcing platform to incentivize good workers.

2   as announced.

(cid:3)

4 Identiﬁability

Estimating α and θ from X(1)  ...  X(t) is not possible unless we have identiﬁability  namely
there cannot exist two distinct sets of parameters α  θ and α′  θ′ under which the distribution of
X(1)  ...  X(t) is the same. Let X ∈ {−1  0  1}n be any sample  for some parameters α ∈ (0  1]
and θ ∈ [−1  1]n. The parameter α is clearly identiﬁable since α = P(X1 6= 0). The identiﬁability
of θ is less obvious. Assume for instance that θi = 0 for all i ≥ 3. It follows from (1) that for any
x ∈ {−1  0  1}n  with H(x) deﬁned as in the proof of Proposition 1:

P(X = x) = H(x) ×( 1 + θ1θ2
1 − θ1θ2
1

if x1x2 = 1 
if x1x2 = −1 
if x1x2 = 0.

3

In particular  two parameters θ  θ′ such that θ1θ2 = θ′1θ′2 and θi = θ′i = 0 for all i ≥ 3 cannot be dis-
tinguished. Similarly  by symmetry  two parameters θ  θ′ such that θ′ = −θ cannot be distinguished.

Let:

Θ =(θ ∈ [−1  1]n :

n

n

1{θi 6= 0} ≥ 3 

Xi=1

θi > 0) .

Xi=1

The ﬁrst condition states that there are at least 3 informative workers  the second that the average
reliability is positive.

Proposition 2 Any parameter θ ∈ Θ is identiﬁable.
Proof. Any parameter θ ∈ Θ can be expressed as a function of the covariance matrix of X (section
6 below): the absolute value and the sign of θ follow from (4) and (5)  respectively.

(cid:3)

5 Lower bound on the minimax error

The estimation of α is straightforward and we here focus on the best estimation of θ one can
expect  assuming α is known. Speciﬁcally  we derive a lower bound on the minimax error of
any estimator ˆθ of θ. Deﬁne ||ˆθ − θ||∞ = maxi=1 ... n |ˆθi − θi| and for all θ ∈ [−1  1]n 
A(θ) = mink maxi j6=kp|θiθj| and B(θ) =Pn
Observe that Θ = {θ ∈ [−1  1]n : A(θ) > 0  B(θ) > 0}. This suggests that the estimation of
θ becomes hard when either A(θ) or B(θ) is small. Deﬁne for any a  b ∈ (0  1)  Θa b =
{θ ∈ [−1  1]n : A(θ) ≥ a   B(θ) ≥ b}. We have the following lower bound on the minimax error.
As the proof reveals  the parameters a and b characterize the difﬁculty of estimating the absolute
value and the sign of θ  respectively.

i=1 θi.

Theorem 1 (Minimax error) Consider any estimator ˆθ(t) of θ.
For any ǫ ∈ (0  min(a  (1 − a)/2  1/4)) and δ ∈ (0  1/4)  we have

min
θ∈Θa b

P(cid:16)||ˆθ(t) − θ||∞ ≥ ǫ(cid:17) ≥ δ   ∀t ≤ max(T1  T2) 

(1−a)4(n−4)

α2a2b2

with T1 = c1

ln(cid:0) 1

1−a

α2a4ǫ2 ln(cid:0) 1

4δ(cid:1)  T2 = c2

4δ(cid:1) and c1  c2 > 0 two universal constants.
Outline of proof. The proof is based on an information theoretic argument. Denote by Pθ the dis-
tribution of X under parameter θ ∈ Θ  and D(.||.) the Kullback-Leibler (KL) divergence. The main
element of proof is lemma 1  where we bound D(Pθ ′||Pθ) for two well chosen pairs of parameters.
The pair θ  θ′ in statement (i) is hard to distinguish when a is small  hence it is hard to estimate the
absolute value of θ. The pair θ  θ′ of statement (ii) is also hard to distinguish when a or b are small 
which shows that it is difﬁcult to estimate the sign of θ. Proving lemma 1 is involved because of
the particular form of distribution Pθ  and requires careful manipulations of the likelihood ratio. We
conclude by reduction to a binary hypothesis test between θ and θ′ using lemma 2.
Lemma 1 (i) Let a ∈ (0  1)  θ = (1  a  a  0  . . .   0) and θ′ = (1 − 2ǫ 
Then: D(Pθ ′||Pθ) ≤ 1
(a  a −a −a  c  . . .   c)  θ′ = (−a −a  a  a  c  . . .   c). Then: D(Pθ ′||Pθ) ≤ 1
Lemma 2 [Tsybakov  2008  Theorem 2.2] Consider any estimator ˆθ(t).
For any θ  θ′ ∈ Θ with ||θ − θ′||∞ ≥ 2ǫ we have:

(ii) Let n > 4  deﬁne c = b/(n − 4)  and θ =

1−2ǫ   0  . . .   0).

α2a2b2

(n−4)(1−a)4 .

α2a4ǫ2
1−a

c1

a

a

1−2ǫ  

c2

1

min(cid:16)Pθ(||ˆθ(t) − θ||∞ ≥ ǫ)  Pθ ′(||ˆθ(t) − θ′||∞ ≥ ǫ)(cid:17) ≥

4 exp(−tD(Pθ ′||Pθ))

.

Relation with prior work. The lower bound derived in [Zhang et al.  2014][Theorem 3] shows
2 ). Our lower bound is
2 ). Another lower
bound was derived in [Chao and Dengyong  2015][Theorems 3.4 and 3.5]  but this concerns the

that the minimax error of any estimator ˆθ must be greater than O((αt)− 1
stricter  and shows that the minimax error is in fact greater than O(a−2α−1t− 1
prediction error rate  that is P( ˆG 6= G)  so that it cannot be easily compared to our result.

4

6 Triangular estimation

We here present our estimator. The absolute value of the reliability of each worker k is estimated
through the correlation of her answers with those of the most informative pair i  j 6= k. We refer to
this algorithm as triangular estimation (TE). The sign of the reliability of each worker is estimated
in a second step. We use the convention that sign(0) = +.

Covariance matrix. Let X ∈ {−1  0  1}n be any sample  for some parameters α ∈ (0  1] and
θ ∈ Θ. We shall see that the parameter θ could be recovered exactly if the covariance matrix of X
were perfectly known. For any i 6= j  let Cij be the covariance of Xi and Xj given that XiXj 6= 0
(that is  both workers i and j provide an answer). In view of (1) 

Cij =

E(XiXj)
E(|XiXj|)

= θiθj.

(3)

In particular  for any distinct indices i  j  k  CikCjk = θiθjθ2
k = 1  . . .   n and any pair i  j 6= k such that Cij 6= 0 

k = Cij θ2

k. We deduce that  for any

θ2
k =

CikCjk

Cij

.

(4)

i=1 θi = θ2

Note that such a pair exists for each k because θ ∈ Θ. To recover the sign of θk  we use the fact that
θkPn

k +Pi6=k Cik. Since θ ∈ Θ  we get
sign(θk) = sign
k +Xi6=k
θ2

Cik
 .

(5)

The TE algorithm consists in estimating the covariance matrix to recover θ from the above expres-
sions.

TE algorithm. At any time t  deﬁne

∀i  j = 1  . . .   n 

ˆCij =

.

(6)

For all k = 1  . . .   n  ﬁnd the most informative pair (ik  jk) ∈ arg maxi6=j6=k | ˆCij| and let

s=1 Xi(s)Xj(s)

Pt
max(cid:16)Pt
s=1 |Xi(s)Xj(s)|  1(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12)

if | ˆCik jk (t)| > 0 
otherwise.

ˆCik k ˆCjk k

ˆCik jk
0

|ˆθk| =
s(cid:12)(cid:12)(cid:12)(cid:12)

k +Pi6=k

ˆθ2

Next  deﬁne k∗ = arg maxk(cid:12)(cid:12)(cid:12)

and let

ˆCik(cid:12)(cid:12)(cid:12)
sign(ˆθk) =( sign(ˆθ2
k∗ +Pi6=k∗ ˆCik∗ )

sign(ˆθk∗ ˆCkk∗ )

if k = k∗ 
otherwise 

Complexity. First note that the TE algorithm is a streaming algorithm since ˆCij (t) can be written

ˆCij =

Mij

max(Nij  1)

with Mij =

t

Xs=1

Xi(s)Xj(s) and Nij =

t

Xs=1

|Xi(s)Xj (s)|.

Thus TE requires O(n2) memory space (to store the matrices M and N ) and has a time complexity
of O(n2ln(n)) per task: O(n2) operations to update ˆC  O(n2ln(n)) operations to sort the entries of
| ˆC(t)|  O(n2) operations to compute |ˆθ|  O(n2) operations to compute the sign of ˆθ.

5

Minimax optimality. The following result shows that the proposed estimator is minimax optimal.
Namely the sample complexity of our estimator matches the lower bound up to an additive logarith-
mic term ln(n) and a multiplicative constant.

Theorem 2 Let θ ∈ Θa b and denote by ˆθ(t) the estimator deﬁned above. For any ǫ ∈ (0  min( b
and δ ∈ (0  1)  we have

3   1))

with T ′1 = c′1

P(||ˆθ(t) − θ||∞ ≥ ǫ) ≤ δ   ∀t ≥ max(T ′1  T ′2) 
δ (cid:17)  T ′2 = c′2

α2a2b2 ln(cid:16) 4n2

δ (cid:17)  and c′1  c′2 > 0 two universal constants.

n

1

α2a4ǫ2 ln(cid:16) 6n2

Outline of proof. Deﬁne || ˆC − C||∞ = maxi j:i6=j | ˆCij − Cij|. The TE estimator is a function of
ˆCij . The main difﬁculty is to prove
the empirical pairwise correlations ( ˆCij )i j and the sumsPj6=i
lemma 3  a concentration inequality forPj6=i
( ˆCij − Cij )| ≥ ε(cid:17) ≤ 2 exp(cid:18)−

30 max(B(θ)2  n)(cid:19) + 2n exp(cid:18)−

Lemma 3 For all i = 1  . . .   n and all ε > 0 

8(n − 1)(cid:19) .

P(cid:16)|Xj6=i

ε2α2t

ˆCij .

tα2

Consider i ﬁxed. We dissociate the set of tasks answered by each worker from the actual answers
and the truth. Let U = (Uj(t))j t be i.i.d Bernoulli random variables with E(Uj(t)) = α and
V = (Vj (t))j t be independent random variables on {−1  1} with E(Vj (t)) = θj . One may readily
check that (Xj(t))j t has the same distribution as (G(t)Uj (t)Vj (t))j t. Hence  in distribution:

t

Ui(s)Uj(s)Vi(s)Vj(s)

t

Nj

Xs=1

Xj6=i

with Nj =

Ui(s)Uj(s).

ˆCij =Xj6=i

Xs=1
We prove lemma 3 by conditionning with respect to U . Denote by PU the conditional probability
with respect to U . Deﬁne N = minj6=i Nij . We prove that for all ε ≥ 0:
Ui(s)Uj(s)θj(cid:17)2
PU(cid:16)Xj6=i
The quantity σ is an upper bound on the conditional variance of Pj6=i

( ˆCij − Cij ) ≥ ε(cid:17) ≤ e− ε2

applying Chernoff’s inequality to both N and S. We get:

ˆCij   which we control by

Xs=1(cid:16)Xj6=i

(n − 1)N + S

σ2 with S =

and σ2 =

N 2

t

.

P(N ≤ α2t/2) ≤ (n − 1)e− tα2

8

and P(S ≥ 2tα2 max(Bi(θ)2  n − 1)) ≤ e− tα2

3(n−1) .

Removing the conditionning on U yields the result. We conclude the proof of theorem 2 by linking
the ﬂuctuations of ˆC to that of ˆθ in lemma 4.
Lemma 4 If (a) || ˆC−C||∞ ≤ ε ≤ A2(θ) min( 1
then ||ˆθ − θ||∞ ≤ 24ε
A2(θ) .

64 ) and (b) maxi |Pj6=i

ˆCij−Cij| ≤ A(θ)B(θ)

2   B(θ)

8

 

Relation with prior work. Our upper bound brings improvement over [Zhang et al.  2014] as
follows. Two conditions are required for the upper bound of [Zhang et al.  2014][Theorem 4] to
hold: (i) it is required that maxi |θi| < 1  and (ii) the number of workers n must grow with both δ
and t  and in fact must depend on a and b  so that n has to be large if b is smaller than √n. Our result
does not require condition (i) to hold. Further there are values of a and b such that condition (ii) is
never satisﬁed  for instance n ≥ 5  a = 1
n−4 ) ∈ Θa b.
For [Zhang et al.  2014][Theorem 4] to hold  n should satisfy n ≥ c3nln(t2n/δ) with c3 a universal
constant (see discussion in the supplement) and for t or 1/δ large enough no such n exists. It is
noted that for such values of a and b  our result remains informative. Our result shows that one can
obtain a minimax optimal algorithm for crowdsourcing which does not involve any EM step.

and θ = (a −a  a −a 

n−4   ... 

√n−4

2   b =

2

b

b

The analysis of [Chao and Dengyong  2015] also imposes n to grow with t and conditions on the min-
imal value of b. Speciﬁcally the ﬁrst and the last condition of [Chao and Dengyong  2015][Theorem

6

3.3]  require that n ≥ ln(t) and thatPi θ2
i ≥ 6ln(t). Using the previous example (even for t = 3) 
this translates to b ≥ 2√n − 4.
In fact  the value b = O(√n) seems to mark the transition between "easy" and "hard" instances
of the crowdsourcing problem. Indeed  when n is large and b is large with respect to √n  then the
majority vote outputs the truth with high probability by the Central Limit Theorem.

7 Numerical Experiments

b

n−4   ... 

Synthetic data. We consider three instances: (i) n = 50  t = 103  α = 0.25  θi = a if i ≤ n/2 and
0 otherwise; (ii) n = 50  t = 104  α = 0.25  θ = (1  a  a  0  ...  0); (iii) n = 50  t = 104  α = 0.25 
a = 0.9  θ = (a −a  a −a 
Instance (i) is an "easy" instance where half of the workers are informative  with A(θ) = a and
B(θ) = na/2. Instance (ii) is a "hard" instance  the difﬁculty being to estimate the absolute value
of θ accurately by identifying the 3 informative workers. Instance (iii) is another "hard" instance 
where estimating the sign of the components of θ is difﬁcult. In particular  one must distinguish θ
from θ′ = (−a  a −a  a 

n−4 )  otherwise a large error occurs.

n−4   ... 

b

n−4 ).

b

b

Both "hard" instances (ii) and (iii) are inspired by our derivation of the lower bound and constitute
the hardest instances in Θa b. For each instance we average the performance of algorithms on 103
independent runs and apply a random permutation of the components of θ before each run. We
consider the following algorithms: KOS (the BP algorithm of [Karger et al.  2011])  Maj (major-
ity voting)  Oracle (weighted majority voting with optimal weights  the optimal estimator of the
ground truth)  RoE (ﬁrst spectral algorithm of [Dalvi et al.  2013])  EoR (second spectral algorithm
of [Dalvi et al.  2013])  GKM (spectral algorithm of [Ghosh et al.  2011])  S-EMk (EM algorithm
with spectral initialization of [Zhang et al.  2014] with k iterations of EM) and TE (our algorithm).
We do not present the estimation error of KOS  Maj and Oracle since these algorithms only predict
the ground truth but do not estimate θ directly.

The results are shown in Tables 1 and 2  where the best results are indicated in bold. The spectral
algorithms RoE  EoR and GKM tend to be outperformed by the other algorithms. To perform well 
GKM needs θ1 to be positive and large (see [Ghosh et al.  2011]); whenever θ1 ≤ 0 or |θ1| is
small  GKN tends to make a sign mistake causing a large error. Also the analysis of RoE and EoR
assumes that the task-worker graph is a random D-regular graph (so that the worker-worker matrix
has a large spectral gap). Here this assumption is violated and the practical performance suffers
noticeably  so that this limitation is not only theoretical. KOS performs consistently well  and seems
immune to sign ambiguity  see instance (iii). Further  while the analysis of KOS also assumes that
the task-worker graph is random D-regular  its practical performance does not seem sensitive to that
assumption. The performance of S-EM is good except when sign estimation is hard (instance (iii) 
b = 1). This seems due to the fact that the initialization of S-EM (see the algorithm description) is
not good in this case. Hence the limitation of b being of order √n is not only theoretical but practical
as well. In fact (combining our results and the ideas of [Zhang et al.  2014])  this suggests a new
algorithm where one uses EM with TE as the initial value of θ.

Further  the number of iterations of EM brings signiﬁcant gains in some cases and should affect the
universal constants in front of the various error bounds (providing theoretical evidence for this seems
non trival). TE performs consistently well except for (i) a = 0.3 (which we believe is due to the
fact that t is relatively small in that instance). In particular when sign estimation is hard TE clearly
outperforms the competing algorithms. This indeed suggests two regimes for sign estimation: b =

O(1) (hard regime) and b = O(√n) (easy regime).

Real-world data. We next consider 6 publicly available data sets (see [Whitehill et al.  2009 
Zhou et al.  2015] and summary information in Table 3)  each consisting of labels provided by work-
ers and the ground truth. The density is the average number of labels per worker  i.e.  α in our model.
The worker degree is the average number of tasks labeled by a worker.

First  for data sets with more than 2 possible label values  we split the label values into two groups
and associate them with −1 and +1 respectively. The partition of the labels is given in Table 3.
Second  we remove any worker who provides less than 10 labels. Our preliminary numerical ex-
periments (not shown here for concision) show that without this  none of the studied algorithms

7

even match the majority consistently. Workers with low degree create noise and (to the best of our
knowledge) any theoretical analysis of crowdsourcing algorithms assumes that the worker degree
is sufﬁciently large. The performance of various algorithms is reported in Table 4. No information

about the workers reliability is available so we only report the prediction error P( ˆG 6= G). Further 

one cannot compare algorithms to the Oracle  so that the main goal is to outperform the majority.

Apart from "Bird" and "Web"  none of the algorithms seem to be able to signiﬁcantly outperform
the majority and are sometimes noticeably worse. For "Web" which has both the largest number of
labels and a high worker degree  there is a signiﬁcant gain over the majority vote  and TE  despite
its low complexity  slightly outperforms S-EM and is competitive with KOS and GKM which both
perform best on this dataset.

Instance
(i) a = 0.3
(i) a = 0.9
(ii) a = 0.55
(ii) a = 0.95
(iii) b = 1
(iii) b = √n

RoE
0.200
0.274
0.551
0.528
0.253
0.105

EoR GKM S-EM1
0.100
0.131
0.022
0.265
0.045
0.459
0.034
0.522
0.533
0.222
0.075
0.437

0.146
0.271
0.479
0.541
0.256
0.085

S-EM10

0.041
0.022
0.044
0.033
0.389
0.030

TE

0.134
0.038
0.050
0.039
0.061
0.045

Table 1: Synthetic data: estimation error E(||ˆθ − θ||∞).

Instance
(i) a = 0.3
(i) a = 0.9
(ii) a = 0.55
(ii) a = 0.95
(iii) b = 1
(iii) b = √n

Oracle Maj
0.298
0.227
0.004
0.046
0.441
0.284
0.419
0.219
0.472
0.181
0.126
0.315

KOS
0.228
0.004
0.292
0.220
0.185
0.133

RoE
0.402
0.217
0.496
0.495
0.443
0.266

EoR GKM S-EM1
0.251
0.398
0.004
0.218
0.284
0.497
0.219
0.496
0.388
0.455
0.284
0.258

0.374
0.202
0.495
0.483
0.386
0.207

S-EM10

0.228
0.004
0.285
0.219
0.404
0.127

TE

0.250
0.004
0.284
0.219
0.192
0.128

Table 2: Synthetic data: prediction error P( ˆG 6= G).

Data Set

# Tasks

# Workers

# Labels Density Worker Degree

Label Domain

Bird
Dog

Duchenne

RTE
Temp
Web

108
807
159
800
462
2 653

39
109
64
164
76
177

4 212
8 070
1 221
8 000
4 620
15 539

1

0.09
0.12
0.06
0.13
0.03

108
74
19
49
61
88

{0} vs {1}

{0 2} vs {1 3}

{0} vs {1}
{0} vs {1}
{1} vs {2}

{1 2 3} vs {4 5}

Table 3: Summary of the real-world datasets.

Data Set Maj KOS RoE EoR GKM S-EM1

S-EM10

Bird
Dog

Duchenne

RTE
Temp
Web

0.24
0.18
0.28
0.10
0.06
0.14

0.28
0.19
0.30
0.50
0.43
0.02

0.29
0.18
0.29
0.50
0.24
0.13

0.29
0.18
0.28
0.89
0.10
0.14

0.28
0.20
0.29
0.49
0.43
0.02

0.20
0.24
0.28
0.32
0.06
0.04

0.28
0.17
0.30
0.16
0.06
0.06

TE
0.18
0.20
0.26
0.38
0.08
0.03

Table 4: Real-world data: prediction error P( ˆG 6= G).

8 Conclusion

We have derived a minimax error lower bound for the crowdsourcing problem and have proposed
TE  a low-complexity algorithm which matches this lower bound. Our results open several questions
of interest. First  while recent work has shown that one can obtain strong theoretical guarantees by
combining one step of EM with a well-chosen initialization  we have shown that  at least in the case
of binary labels  one can forgo the EM phase altogether and still obtain both minimax optimality
and good numerical performance. It would be interesting to know if this is still possible when there
are more than two possible labels  and also if one can do so using a streaming algorithm.

8

References

Paul S Albert and Lori E Dodd. A cautionary note on the robustness of latent class models for

estimating diagnostic error without a gold standard. Biometrics  60(2):427–435  2004.

Gao Chao and Zhou Dengyong. Minimax optimal convergence rates for estimating ground truth

from crowdsourced labels. Tech Report http://arxiv.org/abs/1310.5764  2015.

Nilesh Dalvi  Anirban Dasgupta  Ravi Kumar  and Vibhor Rastogi. Aggregating crowdsourced

binary ratings. In Proc. of WWW  2013.

A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
EM algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics)  28(1):20–28 
1979.

Arpita Ghosh  Satyen Kale  and R. Preston McAfee. Who moderates the moderators?: crowdsourc-

ing abuse detection in user-generated content. In Proc. of ACM EC  2011.

Sui L Hui and Steven D Walter. Estimating the error rates of diagnostic tests. Biometrics  pages

167–171  1980.

David R. Karger  Sewoong Oh  and Devavrat Shah. Iterative learning for reliable crowdsourcing

systems. In Proc. of NIPS  2011.

David R Karger  Sewoong Oh  and Devavrat Shah. Efﬁcient crowdsourcing for multi-class labeling.

ACM SIGMETRICS Performance Evaluation Review  41(1):81–92  2013.

David R Karger  Sewoong Oh  and Devavrat Shah. Budget-optimal task allocation for reliable

crowdsourcing systems. Operations Research  62(1):1–24  2014.

Qiang Liu  Jian Peng  and Alex T Ihler. Variational inference for crowdsourcing. In Proc. of NIPS 

2012.

Shmuel Nitzan and Jacob Paroush. Optimal decision rules in uncertain dichotomous choice situa-

tions. International Economic Review  pages 289–297  1982.

Vikas C Raykar  Shipeng Yu  Linda H Zhao  Gerardo Hermosillo Valadez  Charles Florin  Luca
Bogoni  and Linda Moy. Learning from crowds. Journal of Machine Learning Research  11:
1297–1322  2010.

Lloyd Shapley and Bernard Grofman. Optimizing group judgmental accuracy in the presence of

interdependencies. Public Choice  43(3):329–343  1984.

Padhraic Smyth  Usama Fayyad  Michael Burl  Pietro Perona  and Pierre Baldi. Inferring ground

truth from subjective labelling of venus images. In Proc. of NIPS  1995.

Alexandre B. Tsybakov. Introduction to non-parametric estimation. Springer  2008.

Dong Wang  Tarek Abdelzaher  Lance Kaplan  and Charu C Aggarwal. Recursive fact-ﬁnding: A
streaming approach to truth estimation in crowdsourcing applications. In Proc. of IEEE ICDCS 
2013.

Peter Welinder and Pietro Perona. Online crowdsourcing: rating annotators and obtaining cost-

effective labels. In Proc. of IEEE CVPR (Workshops)  2010.

Jacob Whitehill  Ting-fan Wu  Jacob Bergsma  Javier R Movellan  and Paul L Ruvolo. Whose vote
should count more: Optimal integration of labels from labelers of unknown expertise. In Proc. of
NIPS  2009.

Yuchen Zhang  Xi Chen  Dengyong Zhou  and Michael I Jordan. Spectral methods meet EM: A

provably optimal algorithm for crowdsourcing. In Proc. of NIPS  2014.

Dengyong Zhou  Qiang Liu  John C Platt  Christopher Meek  and Nihar B Shah. Regularized
minimax conditional entropy for crowdsourcing. Tech Report  http://arxiv.org/pdf/1503.07240 
2015.

9

,Thomas Bonald
Richard Combes
Sanjeeb Dash
Oktay Gunluk
Dennis Wei