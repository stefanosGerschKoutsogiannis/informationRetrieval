2019,What the Vec? Towards Probabilistically Grounded Embeddings,Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast  efficient and their embeddings widely used. Moreover  the W2V algorithm has recently been adopted in the field of graph embedding  where it underpins several leading algorithms. However  despite their ubiquity and the relative simplicity of their common architecture  what the embedding parameters of W2V and Glove learn  and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of PMI vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection  theoretically explaining why the embeddings of W2V and Glove work  and  in turn  revealing an interesting mathematical interconnection between the semantic relationships of relatedness  similarity  paraphrase and analogy.,Towards Probabilistically Grounded Embeddings

What the Vec?

Carl Allen1

Ivana Balaževi´c1

Timothy Hospedales1 2

1 School of Informatics  University of Edinburgh  UK

2 Samsung AI Centre  Cambridge  UK

{carl.allen  ivana.balazevic  t.hospedales}@ed.ac.uk

Abstract

Word2Vec (W2V) and GloVe are popular  fast and efﬁcient word embedding al-
gorithms. Their embeddings are widely used and perform well on a variety of
natural language processing tasks. Moreover  W2V has recently been adopted in
the ﬁeld of graph embedding  where it underpins several leading algorithms. How-
ever  despite their ubiquity and relatively simple model architecture  a theoretical
understanding of what the embedding parameters of W2V and GloVe learn and
why that is useful in downstream tasks has been lacking. We show that different
interactions between PMI vectors reﬂect semantic word relationships  such as
similarity and paraphrasing  that are encoded in low dimensional word embeddings
under a suitable projection  theoretically explaining why embeddings of W2V
and GloVe work. As a consequence  we also reveal an interesting mathematical
interconnection between the considered semantic relationships themselves.

1

Introduction

Word2Vec1 (W2V) [25] and GloVe [29] are fast  straightforward algorithms for generating word
embeddings  or vector representations of words  often considered points in a semantic space. Their
embeddings perform well on downstream tasks  such as identifying word similarity by vector
comparison (e.g. cosine similarity) and solving analogies  such as the well known “man is to king as
woman is to queen”  by the addition and subtraction of respective embeddings [26  27  19].
In addition  the W2V algorithm has recently been adopted within the growing ﬁeld of graph em-
bedding  where the typical aim is to represent graph nodes in a common latent space such that their
relative positioning can be used to predict edge relationships. Several state-of-the-art models for
graph representation incorporate the W2V algorithm to learn node embeddings based on random
walks over the graph [13  30  31]. Furthermore  word embeddings often underpin embeddings of
word sequences  e.g. sentences. Although sentence embedding models can be complex [8  17]  as
shown recently [38] they sometimes learn little beyond the information available in word embeddings.
Despite their relative ubiquity  much remains unknown of the W2V and GloVe algorithms  perhaps
most fundamentally we lack a theoretical understanding of (i) what is learned in the embedding
parameters; and (ii) why that is useful in downstream tasks. Answering such core questions is of
interest in itself  particularly since the algorithms are unsupervised  but may also lead to improved
embedding algorithms  or enable better use to be made of the embeddings we have. For example 
both algorithms generate two embedding matrices  but little is known of how they relate or should
interact. Typically one is simply discarded  whereas empirically their mean can perform well [29] and
elsewhere they are assumed identical [14  4]. As for embedding interactions  a variety of heuristics
are in common use  e.g. cosine similarity [26] and 3CosMult [19].

1We refer exclusively  throughout  to the more common implementation Skipgram with negative sampling.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Of works that seek to theoretically explain these embedding models [20  14  4  9  18]  Levy and
Goldberg [20] identify the loss function minimised (implicitly) by W2V and  thereby  the relationship
between W2V word embeddings and the Pointwise Mutual Information (PMI) of word co-occurrences.
More recently  Allen and Hospedales [2] showed that this relationship explains the linear interaction
observed between embeddings of analogies. Building on these results  our key contributions are:

• to show how particular semantic relationships correspond to linear interactions of high dimensional
PMI vectors and thus to equivalent interactions of low dimensional word embeddings generated
by their linear projection  thereby explaining the semantic properties exhibited by embeddings of
W2V and GloVe;

• to derive a relationship between embedding matrices proving that they must differ  justifying the
heuristic use of their mean and enabling word embedding interactions – including the widely used
cosine similarity – to be semantically interpreted; and

• to establish a novel hierarchical mathematical inter-relationship between relatedness  similarity 

paraphrase and analogy (Fig 2).

2 Background
Word2Vec [25  26] takes as input word pairs {(wir   cjr )}D
r=1 extracted from a large text corpus 
where target word wi∈E ranges over the corpus and context word cj ∈E ranges over a window of
size l  symmetric about wi (E is the dictionary of distinct words  n =|E|). For each observed word
pair  k random pairs (negative samples) are generated from unigram distributions. For embedding
dimension d  W2V’s architecture comprises the product of two weight matrices W  C∈Rd×n subject
to the logistic sigmoid function. Columns of W and C are the word embeddings: wi ∈Rd  the ith
column of W  represents the ith word in E observed as the target word (wi); and cj ∈ Rd  the jth
column of C  represents the jth word in E observed as a context word (cj).
Levy and Goldberg [20] show that the loss function of W2V is given by:

(cid:96)W2V = − n(cid:88)

n(cid:88)

#(wi  cj) log σ(w(cid:62)

i cj) + k

D #(wi)#(cj) log(σ(−w(cid:62)

i cj)) 

(1)

i=1

j=1

which is minimised if w(cid:62)
information (PMI). In matrix form  this equates to factorising a shifted PMI matrix S∈Rn×n:

i cj = Pi j − log k  where Pi j = log p(wi  cj )

p(wi)p(cj ) is pointwise mutual

W(cid:62)C = S .

(2)

GloVe [29] has the same architecture as W2V  but a different loss function  minimised when:

i cj = log p(wi  cj) − bi − bj + log Z 
w(cid:62)

(3)
for biases bi  bj and normalising constant Z. In principle  the biases provide ﬂexibility  broadening
the family of statistical relationships that GloVe embeddings can learn.
Analogies are word relationships  such as the canonical “man is to king as woman is to queen” 
that are of particular interest because their word embeddings appear to satisfy a linear relationship
[27  19]. Allen and Hospedales [2] recently showed that this phenomenon follows from relationships
between PMI vectors  i.e. rows of the (unshifted) PMI matrix P∈ Rn×n. In doing so  the authors
deﬁne (i) the induced distribution of an observation ◦ as p(E|◦)  the probability distribution over all
context words observed given ◦; and (ii) that a word w∗ paraphrases a set of words W ⊂E if the
induced distributions p(E|w∗) and p(E|W) are (elementwise) similar.

3 Related Work

While many works explore empirical properties of word embeddings (e.g. [19  23  5])  we focus here
on those that seek to theoretically explain why W2V and GloVe word embeddings capture semantic
properties useful in downstream tasks. The ﬁrst of these is the previously mentioned derivation
by Levy and Goldberg [20] of the loss function (1) and the PMI relationship that minimises it (2).
Hashimoto et al. [14] and Arora et al. [4] propose generative language models to explain the structure

2

found in word embeddings. However  both contain strong a priori assumptions of an underlying
geometry that we do not require (further  we ﬁnd that several assumptions of [4] fail in practice
(Appendix D)). Cotterell et al. [9] and Landgraf and Bellay [18] show that W2V performs exponential
(binomial) PCA [7]  however this follows from the (binomial) negative sampling and so describes the
algorithm’s mechanics  not why it works. Several works focus on the linearity of analogy embeddings
[4  12  2  10]  but only [2] rigorously links semantics to embedding geometry (S.2).
To our knowledge  no previous work explains how the semantic properties of relatedness  similarity 
paraphrase and analogy are all encoded in the relationships of PMI vectors and thereby manifest in
the low dimensional word embeddings of W2V and GloVe.

4 PMI: linking geometry to semantics

The derivative of W2V’s loss function (1) with respect to embedding wi  is given by:

n(cid:88)

j=1

(cid:0) p(wi  cj) + kp(wi)p(cj)
(cid:124)
(cid:125)

(cid:123)(cid:122)

(cid:1)(cid:0) σ(Si j)− σ(w(cid:62)
(cid:124)
(cid:123)(cid:122)

(cid:1)cj = C D(i)e(i)  
(cid:125)

i cj)

1

D∇wi(cid:96)W2V =

d(i)

j

e(i)
j

(4)

for diagonal matrix D(i) = diag(d(i)) ∈ Rn×n; d(i)  e(i)∈Rn containing the probability and error
terms indicated; and all probabilities estimated empirically from the corpus. This conﬁrms that (1) is
minimised if W(cid:62)C = S (2)  since all e(i)
j = 0  but that requires W and C to each have rank at least
that of S. In the general case  including the typical case d(cid:28) n  (1) is minimised when probability
weighted error vectors D(i)e(i) are orthogonal to the rows of C. As such  embeddings wi can be
seen as a non-linear (due to the sigmoid function σ(·)) projection of rows of S  induced by the loss
function. (Note that the distinction between W and C is arbitrary: embeddings cj can also be viewed
as projections onto the rows of W.)
Recognising that the log k shift term is an artefact of the W2V algorithm (see Appendix A)  whose
effect can be evaluated subsequently (as in [2])  we exclude it and analyse properties and interactions
of word embeddings wi that are projections of pi  the corresponding rows of P (PMI vectors). We
aim to identify the properties of PMI vectors that capture semantics and are then preserved in word
embeddings under the low-rank projection induced by a suitably chosen loss function.

4.1 The domain of PMI vectors
PMI vector pi∈Rn has a component PMI(wi  cj) for all context words cj ∈E  given by:

PMI(wi  cj) = log p(cj  wi)

p(wi)p(cj ) = log p(cj|wi)

p(cj )

.

(5)

Any difference in the probability of observing cj having observed wi  relative to its marginal
probability  can be thought of as due to wi. Thus PMI(wi  cj) captures the inﬂuence of one word
on another. Speciﬁcally  by reference to marginal probability p(cj): PMI(wi  cj) > 0 implies cj is
more likely to occur in the presence of wi; PMI(wi  cj) < 0 implies cj is less likely to occur given
wi; and PMI(wi  cj) = 0 indicates that wi and cj occur independently  i.e. they are unrelated. PMI
thus reﬂects the semantic property of relatedness  as previously noted [36  6  15]. A PMI vector thus
reﬂects any change in the probability distribution over all words p(E)  given (or due to) wi:

pi (cid:44)(cid:8)log p(cj|wi)

(cid:9)
cj∈E (cid:44) log p(E|wi)
p(E)

.

p(cj )

(6)
While PMI values are unconstrained in R  PMI vectors are constrained to an n−1 dimensional surface
S ⊂Rn  where each dimension corresponds to a word (Fig 1) (although technically a hypersurface 
we refer to S simply as a “surface”). The geometry of S can be constructed step-wise from (6):
• the vector of numerator terms qi = p(E|wi) lies on the simplex Q⊂Rn;
• dividing all q∈Q (element-wise) by p = p(E)∈Q  gives probability ratio vectors q
“stretched simplex” R⊂Rn (containing 1∈Rn) that has a vertex at
• the natural logarithm transforms R to the surface S  with pi = log p(E|wi)

p that lie on a
p(cj ) on axis j  ∀cj ∈E; and
p(E) ∈S  ∀wi∈E.

1

3

Note  p = p(E) uniquely determines S. Considering each point s ∈ S as an element-wise log
p ∈S (q∈Q)  shows S to have the properties (proofs in Appendix B):
probability ratio vector s = log q
P1 S  and any subsurface of S  is non-linear. PMI vectors are thus not constrained to a linear
subspace  identiﬁable by low-rank factorisation of the PMI matrix  as may seem suggested by (2).
P2 S contains the origin  which can be considered the PMI vector of the null word ∅  i.e. p∅ =

log p(E|∅)

p(E) = log p(E)

p(E) = 0 ∈ Rn.

P3 Probability vector q∈Q is normal to the tangent plane of S at s = log q
P4 S does not intersect with the fully positive or fully negative orthants (excluding 0). Thus

p ∈S.

PMI vectors are not isotropically (i.e. uniformly) distributed in space (as assumed in [4]).

P5 The sum of 2 points s + s(cid:48) lies in S only for certain s  s(cid:48) ∈S. That is  for any s∈S (s(cid:54)= 0) 

there exists a (strict) subset Ss⊂S  such that s + s(cid:48)∈S iff s(cid:48)∈Ss. Trivially 0∈Ss  ∀s∈S.

Note that while all PMI vectors lie in S  certainly not all (inﬁnite) points in S correspond to the
(ﬁnite) PMI vectors of words. Interestingly  P2 and P5 allude to properties of a vector space  often
the desired structure for a semantic space [14]. Whilst the domain of PMI vectors is clearly not a
vector space  addition and subtraction of PMI vectors do have semantic meaning  as we now show.

Figure 1: The PMI surface S  showing sample PMI vectors of words (red dots)

4.2 Subtraction of PMI vectors ﬁnds similarity
Taking the deﬁnition from [2] (see S.2)  we consider a word wi that paraphrases a word set W ∈E 
where W ={wj} contains a single word. Since paraphrasing requires distributions of local context
words (induced distributions) to be similar  this intuitively ﬁnds wi that are interchangeable with  or
similar to  wj: in the limit wj itself or  less trivially  a synonym. Thus  word similarity corresponds
to a low KL divergence between p(E|wi) and p(E|wj). Interestingly  the difference between the
associated PMI vectors:

ρi j = pi − pj = log p(E|wi)
p(E|wj )  

(7)

is a vector of un-weighted KL divergence components. Thus  if dimensions were suitably weighted 
the sum of difference components (comparable to Manhattan distance but directed) would equate to
a KL divergence between induced distributions. That is  if qi = p(E|wi)  then a KL divergence is
given by qi(cid:62)ρi j. Furthermore  qi is the normal to the surface S at pi (with unit l1 norm)  by P3.
The projection onto the normal (to S) at pj  i.e. −qj(cid:62)ρi j  gives the other KL divergence. (Intuition
for the semantic interpretation of each KL divergence is discussed in Appendix A of [2].)

4

4.3 Addition of PMI vectors ﬁnds paraphrases
From geometric arguments (P5)  we know that only certain pairs of points in S sum to another point
in the surface. We can also consider the probabilistic conditions for PMI vectors to sum to another:

x = pi + pj = log p(E|wi)

= log p(E|wi wj)

p(E) + log p(E|wj )
p(E)
(cid:123)(cid:122)
(cid:123)(cid:122)
p(E)

− log p(wi wj|E)
p(wi|E)p(wj|E)

(cid:125)

(cid:124)

(cid:125)

pi j

σij

(cid:124)

+ log p(wi wj)
p(wi)p(wj)

= pi j − σij + τ ij1 

(8)

(cid:124)

(cid:123)(cid:122)

τ ij

(cid:125)

where (overloading notation) pi j ∈S is a vector of PMI terms involving p(E|wi  wj)  the induced
distribution of wi and wj observed together;2 and σij ∈Rn  τ ij ∈R are the conditional and marginal
dependence terms indicated (as seen in [2]). From (8)  if wi  wj ∈ E occur both independently
and conditionally independently given each and every word in E  then x = pi j ∈S  and (from P5)
pj ∈Spi and pi∈Spj . If not  error vector εij = σij−τ ij1 separates x and pi j and x /∈S  unless by
meaningless coincidence. (Note  whilst probabilistic aspects here mirror those of [2]  we combine
these with a geometric understanding.) Although certainly pi j ∈S  the extent to which pi j ≈ pk for
some wk ∈E depends on paraphrase error ρk {i j} = pk − pi j  that compares the induced distributions
of wk and {wi  wj}. Thus the PMI vector difference (pi +pj)−pk for any words wi  wj  wk ∈E
comprises: εij a component between pi +pj and the surface S (reﬂecting word dependence); and
ρk {i j} a component along the surface (reﬂecting paraphrase error). The latter captures a semantic
relationship with wk  which the former may obscure  irrespective of wk. (Further geometric and
probabilistic implications are considered in Appendix C.)

4.4 Linear combinations of PMI vectors ﬁnd analogies

PMI vectors of analogy relationships “wa is to wa∗ as wb is to wb∗” have been proven [2] to satisfy:

(9)
The proof builds on the concept of paraphrasing (with error terms similar to those in Section 4.3) 
comparing PMI vectors of analogous word pairs to show that pa + pb∗ ≈ pa∗

+ pb and thus (9).

pb∗ ≈ pa∗ − pa + pb.

5 Encoding PMI: from PMI vectors to word embeddings

Understanding how high dimensional PMI vectors encode semantic properties desirable in word
embeddings  we consider how they can be transferred to low dimensional representations. A key
observation is that all PMI vector interactions  for similarity (7)  paraphrases (8) and analogies (9) 
are additive  and are therefore preserved under linear projection. By comparison  the loss function
of W2V (1) projects PMI vectors non-linearly  and that of GloVe (3) does project linearly  but not
(necessarily) PMI vectors. Linear projection can be achieved by the least squares loss function:3

n(cid:88)

n(cid:88)

(cid:0)wi
(cid:62)cj − PMI(wi  cj)(cid:1)2

(cid:96)LSQ = 1
2

(10)
(cid:96)LSQ is minimised when ∇W(cid:62)(cid:96)LSQ = (W(cid:62)C−P)C(cid:62)= 0  or W(cid:62)= P C†  for C† = C(cid:62)(CC(cid:62))−1
the Moore–Penrose pseudoinverse of C. This explicit linear projection allows interactions performed
between word embeddings  e.g. dot product  to be mapped to interactions between PMI vectors  and
thereby semantically interpreted. However  we do better still by considering how W and C relate.

j=1

i=1

.

5.1 The relationship between W and C

Whilst W2V and GloVe train two embedding matrices  typically only W is used and C discarded.
Thus  although relationships are learned between W and C  they are tested between W and W. If

2Whilst wi  wj are both target words  by symmetry we can interchange roles of context and target words to

compute p(E|w  w(cid:48)) based on the distribution of target words for which wi and wj are both context words.

3We note that the W2V and GloVe loss functions include probability weightings (as considered in [35]) 

which we omit for simplicity.

5

W and C are equal  the distinction falls away  but that is not found to be the case in practice. Here 
we consider why typically W(cid:54)= C and  as such  what relationship between W and C does exist.
If the symmetric PMI matrix P is positive semi-deﬁnite (PSD)  its closest low-rank approximation
(minimising (cid:96)LSQ) is given by the eigendecomposition P = ΠΛΠ(cid:62)  Π  Λ ∈ Rn×n  Π(cid:62)Π = I;
and (cid:96)LSQ is minimised by W = C = S1/2U(cid:62)  where S∈ Rd×d  U∈ Rd×n are Λ  Π  respectively 
truncated to their d largest eigenvalue components. Any matrix pair W∗ = M(cid:62)W  C∗ = M−1W 
also minimises (cid:96)LSQ (for any invertible M∈ Rd×d)  but of these W  C are unique (up to rotation
and permutation) in satisfying W = C  a preferred solution for learning word embeddings since the
number of free parameters is halved and consideration of whether to use W  C or both falls away.
However  P is not typically PSD in practice and this preferred (real) factorisation does not exist
since P has negative eigenvalues  S1/2 is complex and any W  C minimising (cid:96)LSQ with W = C
must also be complex. (Complex word embeddings arise elsewhere  e.g. [16  22]  but since the word
embeddings we examine are real we keep to the real domain.) By implication  any W  C∈ Rd×n
that minimise (cid:96)LSQ cannot be equal  contradicting the assumption W = C sometimes made [14  4].
Returning to the eigendecomposition  if S contains the d largest absolute eigenvalues and U the
ii =±1) such that S =|S|I(cid:48). Thus 
corresponding eigenvectors of P  we deﬁne I(cid:48) = sign(S) (i.e. I(cid:48)
W =|S|1/2U(cid:62)and C = I(cid:48)W can be seen to minimise (cid:96)LSQ (i.e. W(cid:62)C≈ P) with W(cid:54)= C but where
corresponding rows of W  C (denoted by superscript) satisfy Wi =±Ci (recall word embeddings
wi  ci are columns of W  C). Such W  C can be seen as quasi-complex conjugate. Again  W  C
can be used to deﬁne a family of matrix pairs that minimise (cid:96)LSQ  of which W  C themselves are a
most parameter efﬁcient choice  with (n+1)d free parameters compared to 2nd.

5.2

Interpreting embedding interactions

i wj)  rather than W and C (e.g. w(cid:62)

Various word embedding interactions are used to predict semantic relationships  e.g. cosine similarity
[26] and 3CosMult [19]  although typically with little theoretical justiﬁcation. With a semantic un-
derstanding of PMI vector interactions (S.4) and the derived relationship C = I(cid:48)W  we now interpret
commonly used word embedding interactions and evaluate the effect of combining embeddings of W
only (e.g. w(cid:62)
i cj). For use below  we note that W(cid:62)C = USU(cid:62) 
C† = U|S|−1/2I(cid:48) and deﬁne: reconstruction error matrix E = P− W(cid:62)C  i.e. E = USU(cid:62)where
U  S contain the n−d smallest absolute eigenvalue components of Π  Σ (as omitted from U  S);
F = U( S − |S|
)U(cid:62)  comprising the negative eigenvalue components of P; and mean embeddings ai
as the columns of A = W+C
Dot Product: We compare the following interactions  associated with predicting relatedness:

2 = U|S|1/2I(cid:48)(cid:48)∈Rd×n  where I(cid:48)(cid:48) = I+I(cid:48)

ii∈{0 1}).

2

(i.e. I(cid:48)(cid:48)

2

i cj = Ui S Uj(cid:62)
W  C : w(cid:62)
i wj = Ui |S| Uj(cid:62)
W  W : w(cid:62)
i aj = Ui |S| I(cid:48)(cid:48) Uj(cid:62) = Ui(S − ( S − |S|
a(cid:62)
A  A :
This shows that w(cid:62)
i wj overestimates the PMI approximation given by w(cid:62)
i cj by twice any component
relating to negative eigenvalues – an overestimation that is halved using mean embeddings  a(cid:62)
i aj.
p(ck|wj )   x = U|S|−1/2I(cid:48)1
Difference sum:

= Ui(S − (S−|S|))Uj(cid:62) = Pi j − Ei j − 2 Fi j
))Uj(cid:62) = Pi j − Ei j − Fi j

(wi − wj)(cid:62)1 = (pi − pj)C†1 =(cid:80)n

k=1 xk log p(ck|wi)

= Pi j − Ei j

2

Thus  summing over the difference of embedding components compares to a KL divergence between
induced distributions (and so similarity) more so than for PMI vectors (S.4.2) as dimensions are
weighted by xk. However  unlike a KL divergence  x is not a probability distribution and does
not vary with wi or wj. We speculate that between x and the omitted probability weighting of the
loss function  the dimensions of low probability words are down-weighted  mitigating the effect of
“outliers” to which PMI is known to be sensitive [37]  and loosely reﬂecting a KL divergence.
Euclidean distance: (cid:107)wi − wj(cid:107)2 = (cid:107)(log p(E|wi)
Cosine similarity: Surprisingly  w(cid:62)
(cid:107)wi(cid:107)(cid:107)wj(cid:107)  as often used effectively to predict word relatedness
and/or similarity [33  5]  has no immediate semantic interpretation. However  recent work [3]
proposes a more holistic description of relatedness than PMI(wi  wj) > 0 (S.4.1): that related words

p(E|wj ) )C†(cid:107)2 shows no obvious meaning.

i wj

6

Table 1: Accuracy in semantic tasks using different loss functions on the text8 corpus [24].

Loss

Model
Relationship
W 2V W2V W(cid:62)C ≈ P
LSQ W(cid:62)W ≈ P
W=C
LSQ LSQ W(cid:62)C ≈ P

Relatedness [1]

Similarity [1]

Analogy [25]

.628
.721
.727

.703
.786
.791

.283
.411
.425

(wi  wj) have multiple positive PMI vector components in common  because all words associated
with any common semantic “theme” are also more likely to co-occur. The strength of relatedness
(similarity being the extreme case) is given by the number of common word associations  as reﬂected
in the dimensionality of a common aggregate PMI vector component  which projects to a common
embedding component. The magnitude of such common component is not directly meaningful  but
as relatedness increases and wi  wj share more common word associations  the angle between their
PMI vectors  and so too their embeddings  narrows  justifying the widespread use of cosine similarity.
Other statistical word embedding relationships assumed in [4] are considered in Appendix D.

6 Empirical evidence

Word embeddings (especially those of W2V) have been well empirically studied  with many exper-
imental ﬁndings. Here we draw on previous results and run test experiments to provide empirical
support for our main theoretical results:

1. Analogies form as linear relationships between linear projections of PMI vectors (S.4.4)

Whilst previously explained in [2]  we emphasise that their rationale for this well known phe-
nomenon ﬁts precisely within our broader explanation of W2V and GloVe embeddings. Further 
re-ordering paraphrase questions is observed to materially affect prediction accuracy [23]  which
can be justiﬁed from the explanation provided in [2] (see Appendix E).

2. The linear projection of additive PMI vectors captures semantic properties more accurately than

the non-linear projection of W2V (S.5).
Several works consider alternatives to the W2V loss function [20  21]  but none isolates the effect
of an equivalent linear loss function  which we therefore implement (detail below). Comparing
models W 2V and LSQ (Table 1) shows a material improvement across all semantic tasks from
linear projection.

3. Word embedding matrices W and C are dissimilar (S.5.1).

W  C are typically found to differ  e.g. [26  29  28]. To demonstrate the difference  we include
an experiment tying W = C. Comparing models W=C and LSQ (Table 1) shows a small but
consistent improvement in the former despite a lower data-to-parameter ratio.
i aj ≥ w(cid:62)

4. Dot products recover PMI with decreasing accuracy: w(cid:62)

i cj ≥ a(cid:62)

i wj (S.5.2).

The use of average embeddings a(cid:62)
recently  [5] show that relatedness correlates noticeably better to w(cid:62)
metric” choices (a(cid:62)

i aj over w(cid:62)

i wj is a well-known heuristic [29  21]. More
i cj than either of the “sym-

i aj or w(cid:62)

i wj).

5. Relatedness is reﬂected by interactions between W and C embeddings  and similarity is reﬂected

by interactions between W and W. (S.5.2)
Asr et al. [5] compare human judgements of similarity and relatedness to cosine similarity between
combinations of W  C and A. The authors ﬁnd a “very consistent” support for their conclusion that
“WC ... best measures ... relatedness” and “similarity [is] best predicted by ... WW”. An example
is given for house: w(cid:62)
i wj gives mansion  farmhouse and cottage  i.e. similar or synonymous
words; w(cid:62)

i cj gives barn  residence  estate  kitchen  i.e. related words.

Models: As we perform a standard comparison of loss functions  similar to [20  21]  we leave
experimental details to Appendix F. In summary  we learn 500 dimensional embeddings from word
co-occurrences extracted from a standard corpus (“text8” [24]). We implement loss function (1)
explicitly as model W 2V . Models W=C and LSQ use least squares loss (10)  with constraint W =C
in the latter (see point 3 above). Evaluation on popular data sets [1  25] uses the Gensim toolkit [32].

7

Global

relatedness

wi

wj

Similarity

wi

E

wi

W

Paraphrase

Relatedness

wi wj

PMI

W1 W2

Analogy

Figure 2: Interconnection between semantic relationships: relatedness is a base pairwise comparison
(measured by PMI); global relatedness considers relatedness to all words (PMI vector); similarity 
paraphrase and analogy depend on global relatedness between words (w∈E) and word sets (W ⊆E).

7 Discussion

Having established mathematical formulations for relatedness  similarity  paraphrase and analogy
that explain how they are captured in word embeddings derived from PMI vectors (S.4)  it can be
seen that they also imply an interesting  hierarchical interplay between the semantic relationships
themselves (Fig 2). At the core is relatedness  which correlates with PMI  both empirically [36  6  15]
and intuitively (S.4.2). As a pairwise comparison of words  relatedness acts somewhat akin to a
kernel (an actual kernel requires P to be PSD)  allowing words to be considered numerically in terms
of their relatedness to all words  as captured in a PMI vector  and compared according to how they
each relate to all other words  or globally relate. Given this meta-comparison  we see that one word is
similar to another if they are globally related (1-1); a paraphrase requires one word to globally relate
to the joint occurrence of a set of words (1-n); and analogies arise when joint occurrences of word
pairs are globally related (n-n). Continuing the “kernel” analogy  the PMI matrix mirrors a kernel
matrix  and word embeddings the representations derived from kernelised PCA [34].

8 Conclusion

In this work  we take two previous results – the well known link between W2V embeddings and
PMI [20]  and a recent connection between PMI and analogies [2] – to show how the semantic
properties of relatedness  similarity  paraphrase and analogy are captured in word embeddings that
are linear projections of PMI vectors. The loss functions of W2V (2) and GloVe (3) approximate
such a projection: non-linearly in the case of W2V and linearly projecting a variant of PMI in GloVe;
explaining why their embeddings exhibit semantic properties useful in downstream tasks.
We derive a relationship between embedding matrices W and C  enabling word embedding interac-
tions (e.g. dot product) to be semantically interpreted and justifying the familiar cosine similarity as a
measure of relatedness and similarity. Our theoretical results explain several empirical observations 
e.g. why W and C are not found to be equal despite representing the same words  their symmetric
treatment in the loss function and a symmetric PMI matrix; why mean embeddings (A) are often
found to outperform those from either W or C; and why relatedness corresponds to interactions
between W and C  and similarity to interactions between W and W.
We discover an interesting hierarchical structure between semantic relationships: with relatedness
as a basic pairwise comparison  similarity  paraphrase and analogy are deﬁned according to how
target words each relate to all words. Error terms arise in the latter higher order relationships due to
statistical dependence between words. Such errors can be interpreted geometrically with respect to
the hypersurface S on which all PMI vectors lie  and can  in principle  be evaluated from higher order
statistics (e.g trigrams co-occurrences).
Several further details of W2V and GloVe remain to be explained that we hope to address in future
work  e.g. the weighting of PMI components over the context window [31]  the exponent 3/4 often
applied to unigram distributions [26]  the probability weighting in the loss function (S.5)  and an
interpretation of the weight vector x in embedding differences (S.5.2).

8

Acknowledgements

We thank Ivan Titov  Jonathan Mallinson and the anonymous reviewers for helpful comments. Carl
Allen and Ivana Balaževi´c were supported by the Centre for Doctoral Training in Data Science 
funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.

References
[1] Eneko Agirre  Enrique Alfonseca  Keith Hall  Jana Kravalova  Marius Pa¸sca  and Aitor Soroa.
A study on similarity and relatedness using distributional and wordnet-based approaches. In
North American Chapter of the Association for Computational Linguistics  2009.

[2] Carl Allen and Timothy Hospedales. Analogies explained: Towards understanding word

embeddings. In International Conference on Machine Learning  2019.

[3] Carl Allen  Ivana Balazevic  and Timothy M Hospedales. On understanding knowledge graph

representation. arXiv preprint arXiv:1909.11611  2019.

[4] Sanjeev Arora  Yuanzhi Li  Yingyu Liang  Tengyu Ma  and Andrej Risteski. A latent vari-
able model approach to pmi-based word embeddings. Transactions of the Association for
Computational Linguistics  2016.

[5] Fatemeh Torabi Asr  Robert Zinkov  and Michael Jones. Querying word embeddings for
similarity and relatedness. In North American Chapter of the Association for Computational
Linguistics  2018.

[6] John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-
occurrence statistics: A computational study. Behavior research methods  39(3):510–526 
2007.

[7] Michael Collins  Sanjoy Dasgupta  and Robert E Schapire. A generalization of principal
components analysis to the exponential family. In Advances in Neural Information Processing
Systems  2002.

[8] Alexis Conneau  Douwe Kiela  Holger Schwenk  Loïc Barrault  and Antoine Bordes. Supervised
learning of universal sentence representations from natural language inference data. In Empirical
Methods in Natural Language Processing  2017.

[9] Ryan Cotterell  Adam Poliak  Benjamin Van Durme  and Jason Eisner. Explaining and gen-
eralizing skip-gram through exponential family principal component analysis. In European
Chapter of the Association for Computational Linguistics  2017.

[10] Kawin Ethayarajh  David Duvenaud  and Graeme Hirst. Towards understanding linear word

analogies. In Association for Computational Linguistics  2019.

[11] Lev Finkelstein  Evgeniy Gabrilovich  Yossi Matias  Ehud Rivlin  Zach Solan  Gadi Wolfman 
and Eytan Ruppin. Placing search in context: The concept revisited. In International Conference
on World Wide Web  2001.

[12] Alex Gittens  Dimitris Achlioptas  and Michael W Mahoney. Skip-Gram - Zipf + Uniform =

Vector Additivity. In Association for Computational Linguistics  2017.

[13] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.

International Conference on Knowledge Discovery and Data mining  2016.

In

[14] Tatsunori B Hashimoto  David Alvarez-Melis  and Tommi S Jaakkola. Word embeddings
as metric recovery in semantic spaces. Transactions of the Association for Computational
Linguistics  2016.

[15] Aminul Islam  Evangelos Milios  and Vlado Keselj. Comparing word relatedness measures

based on google n-grams. In International Conference on Computational Linguistics  2012.

9

[16] Amit Kumar Jaiswal  Guilherme Holdack  Ingo Frommholz  and Haiming Liu. Quantum-like
generalization of complex word embedding: a lightweight approach for textual classiﬁcation.
In Lernen  Wissen  Daten  Analysen  2018.

[17] Ryan Kiros  Yukun Zhu  Ruslan R Salakhutdinov  Richard Zemel  Raquel Urtasun  Antonio
Torralba  and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing
Systems  2015.

[18] Andrew J Landgraf and Jeremy Bellay. word2vec Skip-Gram with Negative Sampling is a

Weighted Logistic PCA. arXiv preprint arXiv:1705.09755  2017.

[19] Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representa-

tions. In Computational Natural Language Learning  2014.

[20] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In

Advances in Neural Information Processing Systems  2014.

[21] Omer Levy  Yoav Goldberg  and Ido Dagan. Improving distributional similarity with lessons
learned from word embeddings. Transactions of the Association for Computational Linguistics 
2015.

[22] Qiuchi Li  Sagar Uprety  Benyou Wang  and Dawei Song. Quantum-inspired complex word

embedding. In Workshop on Representation Learning for NLP  2018.

[23] Tal Linzen. Issues in evaluating semantic spaces using word analogies. In 1st Workshop on

Evaluating Vector-Space Representations for NLP  2016.

[24] Matt Mahoney.

text8 wikipedia dump. http://mattmahoney.net/dc/textdata.html 

2011. [Online; accessed May 2019].

[25] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word
representations in vector space. In International Conference on Learning Representations 
Workshop Track Proceedings  2013.

[26] Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg S Corrado  and Jeff Dean. Distributed repre-
sentations of words and phrases and their compositionality. In Advances in Neural Information
Processing Systems  2013.

[27] Tomas Mikolov  Wen-tau Yih  and Geoffrey Zweig. Linguistic regularities in continuous
space word representations. In North American Chapter of the Association for Computational
Linguistics  2013.

[28] David Mimno and Laure Thompson. The strange geometry of skip-gram with negative sampling.

In Empirical Methods in Natural Language Processing  2017.

[29] Jeffrey Pennington  Richard Socher  and Christopher Manning. Glove: Global vectors for word

representation. In Empirical Methods in Natural Language Processing  2014.

[30] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social
representations. In International Conference on Knowledge Discovery and Data mining  2014.

[31] Jiezhong Qiu  Yuxiao Dong  Hao Ma  Jian Li  Kuansan Wang  and Jie Tang. Network embed-
ding as matrix factorization: Unifying DeepWalk  line  pte  and node2vec. In International
Conference on Web Search and Data Mining  2018.

[32] Radim ˇReh˚uˇrek and Petr Sojka. Software framework for topic modelling with large corpora. In

Workshop on New Challenges for NLP Frameworks  2010.

[33] Tobias Schnabel  Igor Labutov  David Mimno  and Thorsten Joachims. Evaluation methods for
unsupervised word embeddings. In Empirical Methods in Natural Language Processing  2015.

[34] Bernhard Schölkopf  Alexander Smola  and Klaus-Robert Müller. Kernel principal component

analysis. In International Conference on Artiﬁcial Neural Networks  1997.

10

[35] Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In International

Conference on Machine Learning  2003.

[36] Peter D Turney. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In European

Conference on Machine Learning. Springer  2001.

[37] Peter D Turney and Patrick Pantel. From frequency to meaning: Vector space models of

semantics. Journal of Artiﬁcial Intelligence Research  37:141–188  2010.

[38] John Wieting and Douwe Kiela. No training required: Exploring random encoders for sentence

classiﬁcation. In International Conference on Learning Representations  2019.

11

,Carl Allen
Ivana Balazevic
Timothy Hospedales