2017,Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis,Synthesizing realistic profile faces is promising for more efficiently  training deep pose-invariant models for large-scale unconstrained face recognition  by populating samples with extreme poses and avoiding tedious annotations. However  learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap  we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model  which can improve the realism of a face simulator's output using unlabeled real faces  while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular  we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture  we make several key modifications to the standard GAN to preserve pose and texture  preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition  the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively.,Dual-Agent GANs for Photorealistic and Identity

Preserving Proﬁle Face Synthesis

Jian Zhao1 2∗† Lin Xiong3 Karlekar Jayashree3

Jianshu Li1

Fang Zhao1

Zhecan Wang4†

Sugiri Pranata3

Shengmei Shen3

Shuicheng Yan1 5

Jiashi Feng1

1National University of Singapore
3 Panasonic R&D Center Singapore

2National University of Defense Technology
4 Franklin. W. Olin College of Engineering

5 Qihoo 360 AI Institute

{zhaojian90  jianshu}@u.nus.edu

{lin.xiong  karlekar.jayashree  sugiri.pranata  shengmei.shen}@sg.panasonic.com

zhecan.wang@students.olin.edu

{elezhf  eleyans  elefjia}@u.nus.edu

Abstract

Synthesizing realistic proﬁle faces is promising for more efﬁciently training deep
pose-invariant models for large-scale unconstrained face recognition  by popu-
lating samples with extreme poses and avoiding tedious annotations. However 
learning from synthetic faces may not achieve the desired performance due to the
discrepancy between distributions of the synthetic and real face images. To narrow
this gap  we propose a Dual-Agent Generative Adversarial Network (DA-GAN)
model  which can improve the realism of a face simulator’s output using unlabeled
real faces  while preserving the identity information during the realism reﬁne-
ment. The dual agents are speciﬁcally designed for distinguishing real v.s. fake
and identities simultaneously. In particular  we employ an off-the-shelf 3D face
model as a simulator to generate proﬁle face images with varying poses. DA-GAN
leverages a fully convolutional network as the generator to generate high-resolution
images and an auto-encoder as the discriminator with the dual agents. Besides
the novel architecture  we make several key modiﬁcations to the standard GAN
to preserve pose and texture  preserve identity and stabilize training process: (i)
a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss
with a boundary equilibrium regularization term. Experimental results show that
DA-GAN not only presents compelling perceptual results but also signiﬁcantly
outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A un-
constrained face recognition benchmark. In addition  the proposed DA-GAN is
also promising as a new approach for solving generic transfer learning problems
more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A
2017 face recognition competitions  where we won the 1st places on the tracks of
veriﬁcation and identiﬁcation.

Introduction

1
Unconstrained face recognition is a very important yet extremely challenging problem. In recent years 
deep learning techniques have signiﬁcantly advanced large-scale unconstrained face recognition (8;
19; 27; 34; 29; 16)  arguably driven by rapidly increasing resource of face images. However  labeling
huge amount of data for feeding supervised deep learning algorithms is undoubtedly expensive
and time-consuming. Moreover  as often observed in real-world scenarios  the pose distribution of
available face recognition datasets (e.g.  IJB-A (15)) is usually unbalanced and has long-tail with

∗Homepage: https://zhaoj9014.github.io/.
†Jian Zhao and Zhecan Wang were interns at Panasonic R&D Center Singapore during this work.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(a) Extremely unbalanced pose distribution.
Figure 1: Comparison of pose distribution in the IJB-A (15) dataset w/o and w/ DA-GAN.

(b) Well balanced pose distribution with DA-GAN.

large pose variations  as shown in Figure. 1a. This has become a main obstacle for further pushing
unconstrained face recognition performance. To address this critical issue  several research attempts
(32; 31; 35) have been made to employ synthetic proﬁle face images as augmented extra data to
balance the pose variations.
However  naively learning from synthetic images can be problematic due to the distribution dis-
crepancy between synthetic and real face images—synthetic data is often not realistic enough with
artifacts and severe texture losses. The low-quality synthesis face images would mislead the learned
face recognition model to overﬁt to fake information only presented in synthetic images and fail to
generalize well on real faces. Brute-forcedly increasing the realism of the simulator is often expensive
in terms of time cost and manpower  if possible.
In this work  we propose a novel Dual-Agent Generative Adversarial Network (DA-GAN) for proﬁle
view synthesis  where the dual agents focus on discriminating the realism of synthetic proﬁle face
images from a simulator using unlabled real data and perceiving the identity information  respectively.
In other words  the generator needs to play against a real–fake discriminator as well as an identity
discriminator simultaneously to generate high-quality faces that are really useful for unconstrained
face recognition.
In our method  a synthetic proﬁle face image with a pre-speciﬁed pose is generated by a 3D morphable
face simulator. DA-GAN takes this synthetic face image as input and reﬁnes it through a conditioned
generative model. We leverage a Fully Convolutional Network (FCN) (17) that operates on the pixel
level as the generator to generate high-resolution face images and an auto-encoder network as the
discriminator. Different from vanilla GANs  DA-GAN introduces an auxiliary discriminative agent
to enforce the generator to preserve identity information of the generated faces  which is critical for
face recognition application. In addition  DA-GAN also imposes a pose perception loss to preserve
pose and texture. The reﬁned synthetic proﬁle face images present photorealistic quality with well
preserved identity information  which are used as augmented data together with real face images for
pose-invariant feature learning. For stabilizing the training process of such dual-agent GAN model 
we impose a boundary equilibrium regularization term.
Experimental results show that DA-GAN not only presents compelling perceptual results but also
signiﬁcantly outperforms state-of-the-arts on the large-scale and challenging National Institute
of Standards and Technology (NIST) IARPA Janus Benchmark A (IJB-A) (15) unconstrained
face recognition benchmark. DA-GAN leads us to further win the 1st places on veriﬁcation and
identiﬁcation tracks in the NIST IJB-A 2017 face recognition competitions. This strong evidence
shows that our “recognition via generation" framework is effective and generic  and we expect that it
beneﬁts for more face recognition and transfer learning applications in the real world.
Our contributions are summarized as follows.

istic and identity preserving proﬁle face synthesis even under extreme poses.

• We propose a novel Dual-Agent Generative Adversarial Network (DA-GAN) for photoreal-
• The proposed dual-agent architecture effectively combines prior knowledge from data distri-
bution (adversarial training) and domain knowledge of faces (pose and identity perception
losses) to exactly recover the lost information inherent in projecting a 3D face into the 2D
image space.
• We present qualitative and quantitative experiments showing the possibility of a “recognition
via generation" framework and achieve the top performance on the challenging NIST IJB-
A (15) unconstrained face recognition benchmark without extra human annotation efforts

2

-100-80-40-60400-2020608010000.010.020.030.040.050.060.07PoseFrequency-100-80-40-60400-2020608010000.010.020.030.040.050.060.07FrequencyPoseFigure 2: Overview of the proposed DA-GAN architecture. The simulator (upper panel) extracts
face RoI  localizes landmark points and produces synthesis faces with arbitrary poses  which are fed
to DA-GAN for realism reﬁnement. DA-GAN uses a fully convolutional skip-net as the generator
(middle panel) and an auto-encoder as the discriminator (bottom panel). The dual agents focus on
both discriminating real v.s. fake (minimizing the loss Ladv) and preserving identity information
(minimizing the loss Lip). Best viewed in color.

by training deep neural networks on the reﬁned face images together with real images.
To our best knowledge  our proposed DA-GAN is the ﬁrst model that is effective for
automatically generating augmented data for face recognition in challenging conditions and
indeed improves performance. DA-GAN won the 1st places on veriﬁcation and identiﬁcation
tracks in the NIST IJB-A 2017 face recognition competitions.

2 Related works

As one of the most signiﬁcant advancements on the research of deep generative models (14; 26) 
GAN has drawn substantial attention from the deep learning and computer vision community since
it was ﬁrst introduced by Goodfellow et al. (10). The GAN framework learns a generator network
and a discriminator network with competing loss. This min-max two-player game provides a simple
yet powerful way to estimate target distribution and to generate novel image samples. Mirza and
Osindero (21) introduce the conditional version of GAN  to condition on both the generator and
discriminator for effective image tagging. Berthelot et al. (2) propose a new Boundary Equilibrium
GAN (BE-GAN) framework paired with a loss derived from the Wasserstein distance for training
GAN  which derives a way of controlling the trade-off between image diversity and visual quality.
These successful applications of GAN motivate us to develop proﬁle view synthesis methods based
on GAN. However  the generator of previous methods usually focus on generating images based on a
random noise vector or conditioned data and the discriminator only has a single agent to distinguish
real v.s. fake. Thus  in contrast to our method  the generated images do not have any discriminative
information that can be used for training a deep learning based recognition model. This separates us
well with previous GAN-based attempts.

3

68-Point Landmark Detection3D Face ModelβSimulated Profile FaceSimulatorFace RoIExtractionGeneratorConv 64×7×7ReLU& BNInput 224×224×3++…Residual Block * 10Conv 3×1×1Output224×224×3DiscriminatorLip-LppConv 3×3×3ReLU……RealSynthetic-LadvTransition DownFC 784Transition UpConv 3×1×1ReLUAgent 1Agent 2Moreover  differnet from previous InfoGAN (5) which does not have the classiﬁcation agent  and
Auxiliary Classiﬁer GAN (AC-GAN) (22) which only performs classiﬁcation  our propsoed DA-
GAN performs face veriﬁcation with an intrigued data augmentation. DA-GAN is a novel and
practical model for efﬁcient data augmentation and it is really effective in practice as proved in Sec.
4. DA-GAN generates the data in a completely different way from InfoGAN (5) and AC-GAN (22)
which generate images from a random noise input or abstract semantic labels. Therefore  inferior to
our model  those existing GAN-like models cannot exploit useful and rich prior information (e.g.  the
shape  pose of faces) for effective data generation and augmentation. They cannot fully control the
generated images. In contrast  DA-GAN can fully control the generated images and adjust the face
pose (e.g.  yaw angles) distribution which is extremely unbalanced in real-world scenarios. DA-GAN
can facilitate training more accurate face analysis models to solve the large pose variation problem
and other relevant problems in unconstrained face recognition.
Our proposed DA-GAN shares a similar idea with TP-GAN (13) that considers face synthesis based
on GAN framework  and Apple GAN (28) that considers learning from simulated and unsupervised
images through adversarial training. Our method differs from them in following aspects: 1) DA-GAN
aims to synthesize photorealistic and identity preserving proﬁle faces to address the large variance
issue in unconstrained face recognition  whereas TP-GAN (13) tries to recover a frontal face from a
proﬁle view and Apple GAN (28) is designed for much simpler scenarios (e.g.  eye and hand image
reﬁnement); 2) TP-GAN (13) and Apple GAN (28) suffer from categorical information loss which
limits their effectiveness in promoting recognition performance. In contrast  our proposed DA-GAN
architecture effectively overcomes this issue by introducing dual discriminator agents.

3 Dual-Agent GAN
3.1 Simulator
The main challenge for unconstrained face recognition lies in the large variation and few proﬁle face
images for each subject  which is the main obstacle for learning a well-performed pose-invariant
model. To address this problem  we simulate face images with various pre-deﬁned poses (i.e.  yaw
angles)  which explicitly augments the available training data without extra human annotation efforts
and balances the pose distribution.
In particular  as shown in Figure. 2  we ﬁrst extracts the face Region of Interest (RoI) from each
available real face image  and estimate 68 facial landmark points using the Recurrent Attentive-
Reﬁnement (RAR) framework (31)  which is robust to illumination changes and does not require
a shape model in advance. We then estimate a transformation matrix between the detected 2D
landmarks and the corresponding landmarks in the 3D Morphable Model (3D MM) using least-
squares ﬁt (35). Finally  we simulate proﬁle face images in various poses with pre-deﬁned yaw
angles.
However  the performance of the simulator decreases dramatically under large poses (e.g.  yaw angles
∈ {[−90◦ −60◦]∪ [+60◦  +90◦]}) due to artifacts and severe texture losses  misleading the network
to overﬁt to fake information only presented in synthetic images and fail to generalize well on real
data.

3.2 Generator
In order to generate photorealistic and identity preserving proﬁle view face images which are truely
beneﬁcal for unconstrained face recognition  we further reﬁne the above-mentioned simulated proﬁle
face images with the proposed DA-GAN.
Inspired by the recent success of FCN-based methods on image-to-image applications (17; 9) and
the leading performance of skip-net on recognition tasks (12; 33)  we modify a skip-net (ResNet
(12)) into a FCN-based architecture as the generator Gθ : RH×W×C (cid:55)→ RH×W×C of DA-GAN to
learn a highly non-linear transformation for proﬁle face image reﬁnement  where θ are the network
parameters for the generator  and H  W   and C denote the image height  width  and channel number 
repectively.
Contextual information from global and local regions compensates each other and naturally beneﬁts
face recognition. The hierarchical features within a skip-net are multi-scale in nature due to the
increasing receptive ﬁeld sizes  which are combined together via skip connections. Such a com-
bined representation comprehensively maintains the contextual information  which is crucial for

4

artifact removal  fragement stitching  and texture padding. Moreover  the FCN-based architecture is
advantageous for generating high-resolution image-level results. More details are provided in Sec. 4.
More formally  let the simulated proﬁle face image be denoted by x and the reﬁned face image be
denoted by ˜x  then

˜x := Gθ(x).

(1)

The key requirements for DA-GAN are that the reﬁned face image ˜x should look like a real face
image in appearance while preserving the intrinsic identity and pose information from the simulator.
To this end  we propose to learn θ by minimizing a combination of three losses:

LGθ = (−Ladv + λ1Lip) + λ2Lpp 

(2)
where Ladv is the adversarial loss for adding realism to the synthetic images and alleviating artifacts 
Lip is the identity perception loss for preserving the identity information  and Lpp is the pose
perception loss for preserving pose and texture information.
Lpp is a pixel-wise (cid:96)1 loss  which is introduced to enforce the pose (i.e.  yaw angle) consistency for
the synthetic proﬁle face images before and after the reﬁnement via DA-GAN:

Lpp =

1

W × H

W(cid:88)

H(cid:88)

i

j

|xi j − ˜xi j| 

(3)

where i  j traverse all pixels of x and ˜x.
Although Lpp may lead some over smooth effects to the reﬁned results  it is still an essential part for
both pose and texture information preserving and accelerated optimization.
To add realism to the synthetic images to really beneﬁt face recognition performance  we need to
narrow the gap between the distributions of synthetic and real images. An ideal generator will make
it impossible to classify a given image as real or reﬁned with high conﬁdence. Meanwhile  preserving
the identity information is the essential and critical part for recognition. An ideal generator will
generate the reﬁned face images that have small intra-class distance and large inter-class distance in
the feature space spanned by the deep neural networks for unconstrained face recognition. These
motivate the use of an adversarial pixel-wise discriminator with dual agents.

3.3 Dual-agent discriminator
To incorporate the prior knowledge from the proﬁle faces’ distribution and domain knowledge of
identities’ distribution  we herein introduce a discriminator with dual agents for distinguishing real
v.s. fake and identities simultaneously. To facilitate this process  we leverage an auto-encoder as the
discriminator Dφ : RH×W×C (cid:55)→ RH×W×C to be as simple as possible to avoid typical GAN tricks 
which ﬁrst projects the input real / fake face image into high-dimensional feature space through
several Convolution (Conv) and Fully Connected (FC) layers of the encoder and then transformed
back to the image-level representation through several Deconvolution (Deconv) and Conv layers of
the decoder  as shown in Figure. 2. φ are the network parameters for the discriminator. More details
are provided in Sec. 4.
One agent of Dφ is trained with Ladv to minimize the Wasserstein distance with a boundary equilib-
rium regularization term for maintaining a balance between the generator and discriminator losses as
ﬁrst introduced in (2) 

(cid:88)
tional Control Theory to maintain the equilibrium E[(cid:80)
i |˜xi − Dφ(˜xi)|] = γE[(cid:80)

where y denotes the real face image  kt is a boundary equilibrium regularization term using Propor-
j |yj − Dφ(yj)|] 

|yj − Dφ(yj)| − kt

|˜xi − Dφ(˜xi)| 

Ladv =

(cid:88)

j

(4)

i

γ is the diversity ratio.
Here kt is updated by

kt+1 = kt + α(γ

(cid:88)

|yj − Dφ(yj)| −(cid:88)

j

i

|˜xi − Dφ(˜xi)|) 

(5)

where α is the learning rate (proportional gain) for k. In essence  Eq.(5) can be thought of as a form
of close-loop feedback control in which kt is adjusted at each step.

5

Ladv serves as a supervision to push the reﬁned face image to reside in the manifold of real images.
It can prevent the blurry effect  alleviate artifacts and produce visually pleasing results.
The other agent of Dφ is trained with Lip to preserve the identity discriminability of the reﬁned face
images. Specially  we deﬁne Lip with the multi-class cross-entropy loss based on the output from the
bottleneck layer of Dφ.
Lip =

−(Yjlog(Dφ(yj)) + (1 − Yj)log(1 − Dφ(yj)))

−(Yilog(Dφ(˜xi)) + (1 − Yi)log(1 − Dφ(˜xi))) 

(6)

(cid:88)
(cid:88)

j

i

1
N

+

1
N

where Y is the identity ground truth.
Thus  minimizing Lip would encourage deep features of the reﬁned face images belonging to the same
identity to be close to each other. If one visualizes the learned deep features in the high-dimensional
space  the learned deep features of reﬁned face image set form several compact clusters and each
cluster may be far away from others. Each cluster has a small variance. In this way  the reﬁned
face images are enforced with well preserved identity information. We also conduct experiments for
illustration.
Using Lip alone makes the results prone to annoying artifacts  because the search for a local minimum
of Lip may go through a path that resides outside the manifold of natural face images. Thus  we
combine Lip with Ladv as the ﬁnal objective function for Dφ to ensure that the search resides in that
manifold and produces photorealistic and identity preserving face image:

LDφ = Ladv + λ1Lip.

(7)

(8)

3.4 Loss functions for training

The goal of DA-GAN is to use a set of unlabeled real face images y to learn a generator Gθ that
adaptively reﬁnes a simulated proﬁle face image x. The overall objective function for DA-GAN is:

(cid:40)LDφ = Ladv + λ1Lip 

LGθ = (−Ladv + λ1Lip) + λ2Lpp.

We optimize DA-GAN by alternatively optimizing Dφ and Gθ for each training iteration. Similar
as in (2)  we measure the convergence of DA-GAN by using the boundary equilibrium concept:
j |yj − Dφ(yj)| with
the lowest absolute value of the instantaneous process error for the Proportion Control Theory

we can frame the convergence process as ﬁnding the closest reconstruction(cid:80)
|γ(cid:80)
j |yj − Dφ(yj)| −(cid:80)
(cid:88)

i |˜xi − Dφ(˜xi)||. This measurement can be formulated as:
|yj − Dφ(yj)| + |γ
|˜xi − Dφ(˜xi)||.

|yj − Dφ(yj)| −(cid:88)

(cid:88)

Lcon =

(9)

j

j

i

Lcon can be used to determine when the network has reached its ﬁnal state or if the model has
collapsed. Detailed algorithm on the training procedures is provided in supplementary material Sec.
1.
4 Experiments
4.1 Experimental settings

Benchmark dataset: Except for synthesizing natural looking proﬁle view face images  the pro-
posed DA-GAN also aims to generate identity preserving face images for accurate face-centric
analysis with state-of-the-art deep learning models. Therefore  we evaluate the possibility of “recogni-
tion via generation" of DA-GAN on the most challenging unconstrained face recognition benchmark
dataset IJB-A (15). IJB-A (15) contains both images and video frames from 500 subjects with
5 397 images and 2 042 videos that are split into 20 412 frames  11.4 images and 4.2 videos per
subject  captured from in-the-wild environment to avoid the near frontal bias  along with protocols for
evaluation of both veriﬁcation (1:1 comparison) and identiﬁcation (1:N search) tasks. For training
and testing  10 random splits are provided by each protocol  respectively. More details are provided
in supplementary material Sec. 2.

6

Figure 3: Quality of reﬁned results w.r.t. the network convergence measurement Lcon.

(a) Reﬁned results of DA-GAN.

(b) Feature space of real faces and DA-GAN synthetic faces.

Figure 4: Qualitative analysis of DA-GAN.

Reproducibility: The proposed method is implementated by extending the Keras framework (6).
All networks are trained on three NVIDIA GeForce GTX TITAN X GPUs with 12GB memory for
each. Please refer to supplementary material Sec. 3 & 4 for full details on network architectures and
training procedures.

4.2 Results and discussions
Qualitative results – DA-GAN:
In order to illustrate the compelling perceptual results generated
by the proposed DA-GAN  we ﬁrst visualize the quality of reﬁned results w.r.t. the network conver-
gence measurement Lcon  as shown in Figure. 3. As can be seen  our DA-GAN ensures a fast yet
stable convergence through the carefully designed optimization scheme and boundary equilibrium
regularization term. The network convergence measurement Lcon correlates well with image ﬁdelity.
Most of the previous works (31; 32; 35) on proﬁle view synthesis are dedicated to address this
problem within a pose range of ±60◦. Because it is commonly believed that with a pose that is larger
than 60◦  it is difﬁcult for a model to generate faithful proﬁle view images. Similarly  our simulator
is also good at normalizing small posed faces while suffers severe artifacts and texture losses under
large poses (e.g.  yaw angles ∈ {[−90◦ −60◦] ∪ [+60◦  +90◦]})  as shown in Figure. 4a the ﬁrst
row for each subject. However  with enough training data and proper architecture and objective
function design of the proposed DA-GAN  it is in fact feasible to further reﬁne such synthetic proﬁle
face images under very large poses for high-quality natural looking results generation  as shown
in Figure. 4a the second row for each subject. Compared with the raw simulated faces  the reﬁned
results by DA-GAN present a good photorealistic quality. More visualized samples are provided in
supplementary material Sec. 5.
To verify the superiority of DA-GAN as well as the contribution of each component  we also compare
the qualitative results produced by the vanilla GAN (10)  Apple GAN (28)  BE-GAN (2) and three
variations of DA-GAN in terms of w/o Ladv  Lip  Lpp in each case  repectively. Please refer to
supplementary material Sec. 5 for details.

7

1250002000001750001500002250002500001000007500050000250000Iterations0.050.251.251.501.000.501.752.00ConvergenceReal60°70°80°90°SimulatedRefinedSimulatedRefinedSimulatedRefinedReal FacesRefined Synthetic Faces with DA-GANTable 1: Performance comparison of DA-GAN with state-of-the-arts on IJB-A (15) veriﬁcation
protocol. For all metrics  a higher number means better performance. The results are averaged over
10 testing splits. Symbol “-" implies that the result is not reported for that method. Standard deviation
is not available for some methods. The results offered by our proposed method are highlighted in
bold.

Method

OpenBR (15)
GOTS (15)
Pooling faces (11)
LSFS (30)
Deep Multi-pose (1)
DCNNmanual (4)
Triplet Similarity (27)
VGG-Face (23)
PAMs (19)
DCNNf usion (3)
Masi et al. (20)
Triplet Embedding (27)
All-In-One (25)
Template Adaptation (8)
NAN (34)
(cid:96)2-softmax (24)
b1
b2
DA-GAN (ours)

-

0.631

0.911

TAR @
FAR=0.10
0.433 ± 0.006
0.627 ± 0.012
0.895 ± 0.013
0.947 ± 0.011
0.945 ± 0.002
0.652 ± 0.037
0.967 ± 0.009
0.964 ± 0.005
0.976 ± 0.004
0.979 ± 0.004
0.978 ± 0.003
0.984 ± 0.002
0.989 ± 0.003
0.978 ± 0.003
0.991 ± 0.003

-

Face veriﬁcation

0.787

0.309

TAR @
FAR=0.01
0.236 ± 0.009
0.406 ± 0.014
0.733 ± 0.034
0.787 ± 0.043
0.790 ± 0.030
0.805 ± 0.030
0.826 ± 0.018
0.838 ± 0.042
0.900 ± 0.010
0.922 ± 0.010
0.939 ± 0.013
0.941 ± 0.008
0.970 ± 0.004
0.963 ± 0.007
0.950 ± 0.009
0.976 ± 0.007

0.886

TAR @

FAR=0.001
0.104 ± 0.014
0.198 ± 0.008
0.514 ± 0.060

-

-
-

-
-
-

0.590 ± 0.050

0.725

0.813 ± 0.020
0.823 ± 0.020
0.836 ± 0.027
0.881 ± 0.011
0.943 ± 0.005
0.920 ± 0.006
0.901 ± 0.008
0.930 ± 0.005

To gain insights into the effectivenss of identity preserving quality of our DA-GAN  we further use
t-SNE (18) to visualize the deep features of both reﬁned proﬁle faces and real faces in a 2D space in
Figure. 4b. As can be seen  the reﬁned proﬁle face images present small intra-class distance and large
inter-class distance  which is similar to those of real faces. This reveals that DA-GAN ensures well
preserved identity information with the auxiliary agent for Lip.
Quantitative results – “recognition via generation": To quantitatively verify the superiority of
“recognition via generation" of DA-GAN  we conduct unconstrained face recognition (i.e.  veriﬁcation
and identiﬁcation) on IJB-A (15) benchmark dataset with three different settings. In the three settings 
the pre-trained deep recognition models are respectively ﬁne-tuned on the original training data
of each split without extra data (baseline 1: b1)  the original training data of each split with extra
synthetic faces by our simulator (baseline 2: b2)  and the original training data of each split with
extra reﬁned faces by our DA-GAN (our method: “recognition via generation" framework based on
DA-GAN  DA-GAN for short). The performance comparison of DA-GAN with the two baselines
and other state-of-the-arts on IJB-A (15) unconstrained face veriﬁcation and identiﬁcation protocols
are given in Table. 1 and Table. 2.
We can observe that even with extra training data  b2 presents inferior performance than b1 for all
metrics of both face veriﬁcation and identiﬁcation. This demonstrates that naively learning from
synthetic images can be problematic due to a gap between synthetic and real image distributions
– synthetic data is often not realistic enough with artifacts and severe texture losses  misleading
the network to overﬁt to fake information only presented in synthetic images and fail to generalize
well on real data. In contrast  with the injection of photorealistic and identity preserving faces
generated by DA-GAN without extra human annotation efforts  our method outperforms b1 by 1.00%
for TAR @ FAR=0.001 of veriﬁcation and 1.50% for FNIR @ FPIR=0.01  0.50% for Rank-1 of
identiﬁcation. Our method achieves comparable performance with (cid:96)2-softmax (24)  which employ a
much more computational complex recognition model even without ﬁne-tuning or template adaptation
procedures as we do. Moreover  DA-GAN outperforms NAN (34) by 4.90% for TAR @ FAR=0.001
of veriﬁcation and 7.30% for FNIR @ FPIR=0.01  1.30% for Rank1 of identiﬁcation. These results
won the 1st places on veriﬁcation and identiﬁcation tracks in NIST IJB-A 2017 face recognition
competitions3. This well veriﬁed the promissing potential of synthetic face images by our DA-GAN
on the large-scale and challenging unconstrained face recognition problem.

3We submitted our results for both veriﬁcation and identiﬁcation protocols to NIST IJB-A 2017 face
recognition competition committee on 29th  March  2017. We received the ofﬁcial notiﬁcation on our top

8

Table 2: Performance comparison of DA-GAN with state-of-the-arts on IJB-A (15) identiﬁcation
protocol. For FNIR metric  a lower number means better performance. For the other metrics  a higher
number means better performance. The results offered by our proposed method are highlighted in
bold.

Face identiﬁcation

0.480

-

-

FNIR @
FPIR=0.01
0.934 ± 0.017
0.953 ± 0.024
0.857 ± 0.027
0.617 ± 0.063

-

-

0.444 ± 0.065
0.539 ± 0.077
0.423 ± 0.094
0.247 ± 0.030
0.226 ± 0.049
0.208 ± 0.020
0.183 ± 0.041
0.085 ± 0.041
0.125 ± 0.035
0.179 ± 0.042
0.110 ± 0.039

Method

OpenBR (15)
GOTS (15)
B-CNN (7)
LSFS (30)
Pooling faces (11)
Deep Multi-pose (1)
DCNNmanual (4)
Triplet Similarity (27)
VGG-Face (23)
PAMs (19)
DCNNf usion (3)
Masi et al. (20)
Triplet Embedding (27)
Template Adaptation (8)
All-In-One (25)
NAN (34)
(cid:96)2-softmax (24)
b1
b2
DA-GAN (ours)

FNIR @
FPIR=0.10
0.851 ± 0.028
0.765 ± 0.033
0.659 ± 0.032
0.387 ± 0.032

-

0.250

-

-

-

0.246 ± 0.014
0.33 ± 0.031
0.210 ± 0.033
0.137 ± 0.014
0.118 ± 0.016
0.113 ± 0.014
0.083 ± 0.009
0.044 ± 0.006
0.068 ± 0.010
0.108 ± 0.008
0.051 ± 0.009

Rank1

0.246 ± 0.011
0.433 ± 0.021
0.588 ± 0.020
0.820 ± 0.024

Rank5

0.375 ± 0.008
0.595 ± 0.020
0.796 ± 0.017
0.929 ± 0.013

0.846
0.846

0.906

0.852 ± 0.018
0.880 ± 0.015
0.913 ± 0.011
0.840 ± 0.012
0.903 ± 0.012
0.932 ± 0.010
0.928 ± 0.010
0.947 ± 0.008
0.958 ± 0.005
0.973 ± 0.005
0.966 ± 0.006
0.960 ± 0.007
0.971 ± 0.007

0.933
0.927

0.937 ± 0.010
0.950 ± 0.007
0.925 ± 0.008
0.965 ± 0.008

-

0.962

-

-

0.977 ± 0.004
0.980 ± 0.005
0.987 ± 0.003
0.982 ± 0.004
0.989 ± 0.003

-

Finally  we visualize the veriﬁcation and identiﬁcation closed set results for IJB-A (15) split1 to gain
insights into unconstrained face recognition with the proposed “recognition via generation" framework
based on DA-GAN. For fully detailed visualization results in high resolution and corresponding
analysis  please refer to supplementary material Sec. 6 & 7.

5 Conclusion
We proposed a novel Dual-Agent Generative Adversarial Network (DA-GAN) for photorealistic and
identity preserving proﬁle face synthesis. DA-GAN combines prior knowledge from data distribution
(adversarial training) and domain knowledge of faces (pose and identity perception loss) to exactly
recover the lost information inherent in projecting a 3D face into the 2D image space. DA-GAN can
be optimized in a fast yet stable way with an imposed boundary equilibrium regularization term that
balances the power of the discriminator against the generator. One promissing potential application of
the proposed DA-GAN is for solving generic transfer learning problems more effectively. Qualitative
and quantitative experiments verify the possibility of our “recognition via generation" framework 
which achieved the top performance on the large-scale and challenging NIST IJB-A unconstrained
face recognition benchmark without extra human annotation efforts. Based on DA-GAN  we won the
1st places on veriﬁcation and identiﬁcation tracks in NIST IJB-A 2017 face recognition competitions.
It would be interesting to apply DA-GAN for other transfer learning applications in the future.

Acknowledgement
The work of Jian Zhao was partially supported by China Scholarship Council (CSC) grant
201503170248.
The work of Jiashi Feng was partially supported by National University of Singapore startup grant
R-263-000-C08-133  Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112
and NUS IDS grant R-263-000-C67-646.
We would like to thank Junliang Xing (Institute of Automation  Chinese Academy of Sciences) 
Hengzhu Liu  and Xucan Chen (National University of Defense Technology) for helpful discussions.

performance on both tracks on 26th  Apirl  2017. The IJB-A benchmark dataset  relevant information and
leaderboard can be found at https://www.nist.gov/programs-projects/face-challenges.

9

References
[1] W. AbdAlmageed  Y. Wu  S. Rawls  S. Harel  T. Hassner  I. Masi  J. Choi  J. Lekust  J. Kim  P. Natarajan 
In Proceedings of the IEEE Winter

et al. Face recognition using deep multi-pose representations.
Conference on Applications of Computer Vision (WACV)  pages 1–9  2016.

[2] D. Berthelot  T. Schumm  and L. Metz. Began: Boundary equilibrium generative adversarial networks.

arXiv preprint arXiv:1703.10717  2017.

[3] J.-C. Chen  V. M. Patel  and R. Chellappa. Unconstrained face veriﬁcation using deep cnn features. In
Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)  pages 1–9  2016.
[4] J.-C. Chen  R. Ranjan  A. Kumar  C.-H. Chen  V. M. Patel  and R. Chellappa. An end-to-end system for
unconstrained face veriﬁcation with deep convolutional neural networks. In Proceedings of the IEEE
International Conference on Computer Vision Workshops (CVPRW)  pages 118–126  2015.

[5] X. Chen  Y. Duan  R. Houthooft  J. Schulman  I. Sutskever  and P. Abbeel.

Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In Proceedings of the
Advances in Neural Information Processing Systems (NIPS)  pages 2172–2180  2016.

[6] F. Chollet. keras. https://github.com/fchollet/keras  2015.
[7] A. R. Chowdhury  T.-Y. Lin  S. Maji  and E. Learned-Miller. One-to-many face recognition with bilinear
cnns. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)  pages
1–9  2016.

[8] N. Crosswhite  J. Byrne  O. M. Parkhi  C. Stauffer  Q. Cao  and A. Zisserman. Template adaptation for

face veriﬁcation and identiﬁcation. arXiv preprint arXiv:1603.03958  2016.

[9] K. Gong  X. Liang  X. Shen  and L. Lin. Look into person: Self-supervised structure-sensitive learning

and a new benchmark for human parsing. arXiv preprint arXiv:1703.05446  2017.

[10] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.
Generative adversarial nets. In Proceedings of the Advances in Neural Information Processing Systems
(NIPS)  pages 2672–2680  2014.

[11] T. Hassner  I. Masi  J. Kim  J. Choi  S. Harel  P. Natarajan  and G. Medioni. Pooling faces: template based
face recognition with pooled face images. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW)  pages 59–67  2016.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 770–778  2016.

[13] R. Huang  S. Zhang  T. Li  and R. He. Beyond face rotation: Global and local perception gan for

photorealistic and identity preserving frontal view synthesis. arXiv preprint arXiv:1704.04086  2017.

[14] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114  2013.
[15] B. F. Klare  B. Klein  E. Taborsky  A. Blanton  J. Cheney  K. Allen  P. Grother  A. Mah  M. Burge  and
A. K. Jain. Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark
a. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages
1931–1939  2015.

[16] J. Li  J. Zhao  F. Zhao  H. Liu  J. Li  S. Shen  J. Feng  and T. Sim. Robust face recognition with deep
multi-view representation learning. In Proceedings of the ACM Conference on Multimedia (ACM MM) 
pages 1068–1072  2016.

[17] J. Long  E. Shelhamer  and T. Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3431–
3440  2015.

[18] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research

(JMLR)  9(Nov):2579–2605  2008.

[19] I. Masi  S. Rawls  G. Medioni  and P. Natarajan. Pose-aware face recognition in the wild. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 4838–4846  2016.
[20] I. Masi  A. T. Tran  J. T. Leksut  T. Hassner  and G. Medioni. Do we really need to collect millions of faces

for effective face recognition? arXiv preprint arXiv:1603.07057  2016.

[21] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784  2014.
[22] A. Odena  C. Olah  and J. Shlens. Conditional image synthesis with auxiliary classiﬁer gans. arXiv preprint

[23] O. M. Parkhi  A. Vedaldi  and A. Zisserman. Deep face recognition. In Proceedings of the British Machine

arXiv:1610.09585  2016.

Vision Conference (BMVC)  page 6  2015.

arXiv preprint arXiv:1703.09507  2017.

[24] R. Ranjan  C. D. Castillo  and R. Chellappa. L2-constrained softmax loss for discriminative face veriﬁcation.

[25] R. Ranjan  S. Sankaranarayanan  C. D. Castillo  and R. Chellappa. An all-in-one convolutional neural

network for face analysis. arXiv preprint arXiv:1611.00851  2016.

[26] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in

deep generative models. arXiv preprint arXiv:1401.4082  2014.

[27] S. Sankaranarayanan  A. Alavi  C. D. Castillo  and R. Chellappa. Triplet probabilistic embedding for face
veriﬁcation and clustering. In Proceedings of the IEEE Conference on Biometrics: Theory  Applications
and Systems (BTAS)  pages 1–8  2016.

[28] A. Shrivastava  T. Pﬁster  O. Tuzel  J. Susskind  W. Wang  and R. Webb. Learning from simulated and

unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828  2016.

[29] Y. Taigman  M. Yang  M. Ranzato  and L. Wolf. Deepface: Closing the gap to human-level performance in
face veriﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition

10

[30] D. Wang  C. Otto  and A. K. Jain. Face search at scale: 80 million gallery. arXiv preprint arXiv:1507.07242 

(CVPR)  pages 1701–1708  2014.

2015.

[31] S. Xiao  J. Feng  J. Xing  H. Lai  S. Yan  and A. Kassim. Robust facial landmark detection via recurrent
attentive-reﬁnement networks. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 57–72  2016.

[32] S. Xiao  L. Liu  X. Nie  J. Feng  A. A. Kassim  and S. Yan. A live face swapper. In Proceedings of the

ACM Conference on Multimedia (ACM MM)  pages 691–692  2016.

[33] S. Xie  R. Girshick  P. Dollár  Z. Tu  and K. He. Aggregated residual transformations for deep neural

networks. arXiv preprint arXiv:1611.05431  2016.

[34] J. Yang  P. Ren  D. Chen  F. Wen  H. Li  and G. Hua. Neural aggregation network for video face recognition.

arXiv preprint arXiv:1603.05474  2016.

[35] X. Zhu  J. Yan  D. Yi  Z. Lei  and S. Z. Li. Discriminative 3d morphable model ﬁtting. In Proceedings
of the IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG) 
volume 1  pages 1–8  2015.

11

,Ke Sun
Jun Wang
Stephane Marchand-Maillet
Jian Zhao
Lin Xiong
Panasonic Karlekar Jayashree
Jianshu Li
Fang Zhao
Zhecan Wang
Panasonic Sugiri Pranata
Panasonic Shengmei Shen
Shuicheng Yan
Jiashi Feng