2018,Robust Learning of Fixed-Structure Bayesian Networks,We investigate the problem of learning Bayesian networks in a robust model where an $\epsilon$-fraction of the samples are adversarially corrupted.  In this work  we study the fully observable discrete case where the structure of the network is given.  Even in this basic setting  previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees.  We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees.  Our algorithm has near-optimal sample complexity  runs in polynomial time  and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples.  Finally  we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.,Robust Learning of Fixed-Structure

Bayesian Networks

Yu Cheng

Department of Computer Science

Duke University

Durham  NC 27708

yucheng@cs.duke.edu

Ilias Diakonikolas

Department of Computer Science
University of Southern California

Los Angeles  CA 90089

ilias.diakonikolas@gmail.com

Daniel M. Kane

Department of Computer Science and Engineering

University of California  San Diego

La Jolla  CA 92093
dakane@ucsd.edu

Alistair Stewart

Department of Computer Science
University of Southern California

Los Angeles  CA 90089
stewart.al@gmail.com

Abstract

We investigate the problem of learning Bayesian networks in a robust model where
an -fraction of the samples are adversarially corrupted. In this work  we study
the fully observable discrete case where the structure of the network is given.
Even in this basic setting  previous learning algorithms either run in exponential
time or lose dimension-dependent factors in their error guarantees. We provide
the ﬁrst computationally efﬁcient robust learning algorithm for this problem with
dimension-independent error guarantees. Our algorithm has near-optimal sample
complexity  runs in polynomial time  and achieves error that scales nearly-linearly
with the fraction of adversarially corrupted samples. Finally  we show on both
synthetic and semi-synthetic data that our algorithm performs well in practice.

1

Introduction

Probabilistic graphical models [KF09] provide an appealing and unifying formalism to succinctly
represent structured high-dimensional distributions. The general problem of inference in graphi-
cal models is of fundamental importance and arises in many applications across several scientiﬁc
disciplines  see [WJ08] and references therein. In this work  we study the problem of learning
graphical models from data [Nea03  DSA11]. There are several variants of this general learning
problem depending on: (i) the precise family of graphical models considered (e.g.  directed  undi-
rected)  (ii) whether the data is fully or partially observable  and (iii) whether the structure of the
underlying graph is known a priori or not (parameter estimation versus structure learning). This
learning problem has been studied extensively along these axes during the past ﬁve decades  (see 
e.g.  [CL68  Das97  AKN06  WRL06  AHHK12  SW12  LW12  BMS13  BGS14  Bre15]) resulting
in a beautiful theory and a collection of algorithms in various settings.
The main vulnerability of all these algorithmic techniques is that they crucially rely on the assumption
that the samples are precisely generated by a graphical model in the given family. This simplifying
assumption is inherent for known guarantees in the following sense: if there exists even a very small
fraction of arbitrary outliers in the dataset  the performance of known algorithms can be totally
compromised. It is important to explore the natural setting when the aforementioned assumption
holds only in an approximate sense. Speciﬁcally  we study the following broad question:

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Question 1 (Robust Learning of Graphical Models). Can we efﬁciently learn graphical models
when a constant fraction of the samples are corrupted  or equivalently  when the model is slightly
misspeciﬁed?

In this paper  we focus on the model of corruptions considered in [DKK+16] (Deﬁnition 1) which
generalizes many other existing models  including Huber’s contamination model [Hub64]. Intuitively 
given a set of good samples (from the true model)  an adversary is allowed to inspect the samples
before corrupting them  both by adding corrupted points and deleting good samples. In contrast  in
Huber’s model  the adversary is oblivious to the samples and is only allowed to add bad points.
We would like to design robust learning algorithms for Question 1 whose sample complexity  N  is
close to the information-theoretic minimum  and whose computational complexity is polynomial
in N. We emphasize that the crucial requirement is that the error guarantee of the algorithm is
independent of the dimensionality d of the problem.

1.1 Formal Setting and Our Results

In this work  we study Question 1 in the context of Bayesian networks [JN07]. We focus on the
fully observable case when the underlying network is given. In the non-robust setting  this learning
problem is straightforward: the “empirical estimator” (which coincides with the maximum likelihood
estimator) is known to be sample and computationally efﬁcient [Das97]. In sharp contrast  even this
most basic regime is surprisingly challenging in the robust setting. For example  the very special case
of robustly learning a Bernoulli product distribution (corresponding to an empty network with no
edges) was analyzed only recently in [DKK+16].
To formally state our results  we ﬁrst give a detailed description of the corruption model we study.
Deﬁnition 1 (-Corrupted Samples). Given 0 <  < 1/2 and a distribution family P  the algorithm
speciﬁes some number of samples N  and N samples X1  X2  . . .   XN are drawn from some (un-
known) ground-truth P ∈ P. The adversary is allowed to inspect P and the samples  and replaces
N of them with arbitrary points. The set of N points is then given to the algorithm. We say that a
set of samples is -corrupted if it is generated by this process.

Bayesian Networks. Fix a directed acyclic graph  G  whose vertices are labelled [d] def=
{1  2  . . .   d} in topological order (every edge points from a vertex with a smaller index to one
with a larger index). We will denote by Parents(i) the set of parents of node i in G. A probability
distribution P on {0  1}d is deﬁned to be a Bayesian network (or Bayes net) with graph G if for
each i ∈ [d]  we have that PrX∼P [Xi = 1 | X1  . . .   Xi−1] depends only on the values Xj where
j ∈ Parents(i). Such a distribution P can be speciﬁed by its conditional probability table.
Deﬁnition 2 (Conditional Probability Table of Bayesian Networks). Let P be a Bayesian network
with graph G. Let Γ be the set {(i  a) : i ∈ [d]  a ∈ {0  1}|Parents(i)|}. Let m = |Γ|. For (i  a) ∈ Γ 
the parental conﬁguration Πi a is deﬁned to be the event that XParents(i) = a. The conditional
probability table p ∈ [0  1]m of P is given by pi a = PrX∼P [Xi = 1 | Πi a] .
Note that P is determined by G and p. We will frequently index p as a vector. We use the notation pk
and the associated events Πk  where each k ∈ [m] stands for an (i  a) ∈ Γ lexicographically ordered.

Our Results. We give the ﬁrst efﬁcient robust learning algorithm for Bayesian networks with a
known graph G. Our algorithm has information-theoretically near-optimal sample complexity  runs
in time polynomial in the size of the input (the samples)  and provides an error guarantee that scales
near-linearly with the fraction of adversarially corrupted samples  under the following restrictions:
First  we assume that each parental conﬁguration is reasonably likely. Intuitively  this assumption
seems necessary because we need to observe each conﬁguration many times in order to learn the
associated conditional probability to good accuracy. Second  we assume that each of the conditional
probabilities is balanced  i.e.  bounded away from 0 and 1. This assumption is needed for technical
reasons. In particular  we need this to show that a good approximation to the conditional probability
table implies that the corresponding Bayesian network is close in total variation distance.
Formally  we say that a Bayesian network is c-balanced  for some c > 0  if all coordinates of the
corresponding conditional probability table are between c and 1 − c. Throughout the paper  we use

2

i=1 2|Parents(i)| for the size of the conditional probability table of P   and α for the minimum
probability of parental conﬁguration of P : α = min(i a)∈S PrP [Πi a]. We now state our main result.
Theorem 3 (Main). Fix 0 <  < 1/2. Let P be a c-balanced Bayesian network on {0  1}d with known

m =(cid:80)d
structure G. Assume α ≥ Ω((cid:112)log(1/)/c). Let S be an -corrupted set of N =(cid:101)Ω(m log(1/τ )/2)
probability at least 1 − τ  dTV (P  Q) ≤ (cid:112)ln(1/)/(αc). Our algorithm runs in time (cid:101)O(N d2/).

samples from P . 1 Given G    τ  and S  we can compute a Bayesian network Q such that  with

Our algorithm is given in Section 3. We ﬁrst note that the sample complexity of our algorithm is
near-optimal for learning Bayesian networks with known structure. The following sample complexity
lower bound holds even without corrupted samples:
Fact 4 (Sample Complexity Lower Bound  [CDKS17]). Let BN d f denote the family of Bernoulli
Bayesian networks on d variables such that every node has at most f parents. The worst-case
sample complexity of learning BN d f   within total variation distance  and with probability 9/10  is
Ω(2f · d/2) for all f ≤ d/2 when the graph structure is known.
Consider Bayes nets whose average in-degree is close to the maximum in-degree  that is  when
m = Θ(2f d)  the sample complexity lower bound in Fact 4 becomes Ω(m/2)  so our sample
complexity is optimal up to polylogarithmic factors.
We remark that Theorem 3 is most useful when c is a constant and the Bayesian network has
bounded fan-in f. In this case  the condition on α follows from the c-balanced assumption: When
both c and f are constants  α = min(i a)∈S PrP [Πi a] ≥ cf is also a constant  so the condition

cf ≥ Ω((cid:112)log(1/)) automatically hold when  is smaller than some constant. On the other hand 

the problem of learning Bayesian networks is less interesting when the fan-in is too large. For
example  if some node has f = ω(log(d)) parents  then the size of the conditional probability table
is at least 2f   which is super-polynomial in the dimension d.

Experiments. We performed an experimental evaluation of our algorithm on both synthetic and
real data. Our evaluation allowed us to verify the accuracy and the sample complexity rates of our
theoretical results. In all cases  the experiments validate the usefulness of our algorithm  which
signiﬁcantly outperforms previous approaches  almost exactly matching the best rate without noise.

Related Work. Question 1 ﬁts in the framework of robust statistics [HR09  HRRS86]. Classical
estimators from this ﬁeld can be classiﬁed into two categories: either (i) they are computationally
efﬁcient but incur an error that scales polynomially with the dimension d  or (ii) they are provably
robust (in the aforementioned sense) but are hard to compute. In particular  essentially all known
estimators in robust statistics (e.g.  the Tukey median [Tuk75]) have been shown [JP78  Ber06  HM13]
to be intractable in the high-dimensional setting. We note that the robustness requirement does not
typically pose information-theoretic impediments for the learning problem. In most cases of interest
(see  e.g.  [CGR15  CGR16  DKK+16])  the sample complexity of robust learning is comparable to
its (easier) non-robust variant. The challenge is to design computationally efﬁcient algorithms.
Efﬁcient robust estimators are known for various low-dimensional structured distributions (see 
e.g.  [DDS14  CDSS13  CDSS14a  CDSS14b  ADLS16  ADLS17  DLS18]). However  the robust
learning problem becomes surprisingly challenging in high dimensions. Recently  there has been
algorithmic progress on this front: [DKK+16  LRV16] give polynomial-time algorithms with im-
proved error guarantees for certain “simple” high-dimensional structured distributions. The results
of [DKK+16] apply to simple distributions  including Bernoulli product distributions  Gaussians 
and mixtures thereof (under some natural restrictions). Since the works of [DKK+16  LRV16] 
computationally efﬁcient robust estimation in high dimensions has received considerable attention
(see  e.g.  [DKS17  DKK+17  BDLS17  DKK+18a  DKS18b  DKS18a  HL18  KSS18  PSBR18 
DKK+18b  KKM18  DKS18c  LSLC18]).

1.2 Overview of Algorithmic Techniques

Our algorithmic approach builds on the framework of [DKK+16] with new technical and conceptual
ideas. At a high level  our algorithm works as follows: We draw an -corrupted set of samples from a

1Throughout the paper  we use (cid:101)O(f ) to denote O(f polylog(f )).

3

Bayesian network P with known structure  and then iteratively remove samples until we can return
the empirical conditional probability table.
First  we associate a vector F (X) to each sample X so that learning the mean of F (X) to good
accuracy is sufﬁcient to recover the distribution. In the case of binary products  F (X) is simply X 
while in our case we need to take into account additional information about conditional means.
From this point  our algorithm will try to do one of two things: Either we show that the sample mean
of F (X) is close to the conditional mean of the true distribution (in which case we can already learn
the ground-truth Bayes net P )  or we are able to produce a ﬁlter  i.e.  we can remove some of our
samples  and it is guaranteed that we throw away more bad samples than good ones. If we produce a
ﬁlter  we then iterate on those samples that pass the ﬁlter.
To produce a ﬁlter  we compute a matrix M which is roughly the empirical covariance matrix of
F (X). We show that if the corruptions are sufﬁcient to notably disrupt the sample mean of F (X) 
there must be many erroneous samples that are all far from the mean in roughly the same direction 
and we can detect this direction by looking at the largest eigenvector of M. If we project all samples
onto this direction  concentration bounds of F (X) will imply that almost all samples far from the
mean are erroneous  and thus ﬁltering them out will provide a cleaner set of samples.

Organization. Section 2 contains some technical results speciﬁc to Bayesian networks that we
need. Section 3 gives the details of our algorithm and an overview of its analysis. In Section 4  we
present the experimental evaluations. In Section 5  we conclude and propose directions for future
work. Due to space constraints  we defer the proofs of the technical lemmas to the full version of the
paper.

2 Technical Preliminaries

The structure of this section is as follows: First  we bound the total variation distance between two
Bayes nets in terms of their conditional probability tables. Second  we deﬁne a function F (x  q) 
which takes a sample x and returns an m-dimensional vector that contains information about the
conditional means. Finally  we derive a concentration bound from Azuma’s inequality.
Lemma 5. Suppose that: (i) mink∈[m] PrP [Πk] ≥   and (ii) P or Q is c-balanced  and (iii)

k PrP [Πk](pk − qk)2 ≤ . Then we have that dTV (P  Q) ≤ .

(cid:112)(cid:80)

3
c

Lemma 5 says that to learn a balanced ﬁxed-structure Bayesian network  it is sufﬁcient to learn all
the relevant conditional means. However  each sample x ∼ P gives us information about pi a only if
x ∈ Πi a. To resolve this  we map each sample x to an m-dimensional vector F (x  q)  and “ﬁll in”
the entries that correspond to conditional means for which the condition failed to happen. We will set
these coordinates to their empirical conditional means q:
Deﬁnition 6. Let F (x  q) for {0  1}d × Rm → Rm be deﬁned as follows: If x ∈ Πi a  then
F (x  q)i a = xi  otherwise F (x  q)i a = qi a.

When q = p (the true conditional means)  the expectation of the (i  a)-th coordinate of F (X  p) 
for X ∼ P   is the same conditioned on either Πi a or ¬Πi a. Using the conditional independence
properties of Bayesian networks  we will show that the covariance of F (x  p) is diagonal.
Lemma 7. For X ∼ P   we have E(F (X  p)) = p. The covariance matrix of F (X  p) satisﬁes
Cov[F (X  p)] = diag(PrP [Πk]pk(1 − pk)).
Our algorithm makes crucial use of Lemma 7 (in particular  that Cov[F (X  p)] is diagonal) to detect
whether or not the empirical conditional probability table of the noisy distribution is close to the
conditional probabilities.
Finally  we will need a suitable concentration inequality that works under conditional independence
properties. We can use Azuma’s inequality to show that the projections of F (X  q) on any direction
v is concentrated around the projection of the sample mean q.
Lemma 8. For X ∼ P   any unit vector v ∈ Rd  and any q ∈ [0  1]m  we have
Pr[|v · (F (X  q) − q)| ≥ T + (cid:107)p − q(cid:107)2] ≤ 2 exp(−T 2/2) .

4

3 Robust Learning Algorithm

We ﬁrst look into the major ingredients required for our ﬁltering algorithm  and compare our proof
with that for product distributions in [DKK+16] on a more technical level.
In Section 2  we mapped each sample X to F (X  q) which contains information about the conditional
means q  and we showed that it is sufﬁcient to learn the mean of F (X  q) to learn the ground-truth
Bayes net.
Let M denote the empirical covariance matrix of (F (X  q) − q). We decompose M into three parts:
One coming from the ground-truth distribution  one coming from the subtractive error (because
the adversary can remove N good samples)  and one coming from the additive error (because the
adversary can add N bad samples). We will make use of the following observations:

(1) The noise-free distribution has a diagonal covariance matrix.
(2) The term coming from the subtractive error has no large eigenvalues.

These two observations imply that any large eigenvalues of M are due to the additive error. Finally 
we will reuse our concentration bounds to show that if the additive errors are frequently far from the
mean in a known direction  then they can be reliably distinguished from good samples.
For the case of binary product distributions in [DKK+16]  (1) is trivial because the coordinates
are independent; but for Bayesian networks we need to expand the dimension of the samples and
ﬁll in the missing entries properly. Condition (2) is due to concentration bounds  and for product
distributions it follows from standard Chernoff bounds  while for Bayes nets  we must instead rely on
martingale arguments and Azuma’s inequality.

3.1 Main Technical Lemma and Proof of Theorem 3

First  we need to show that a large enough set of samples with no noise satisfy properties we expect
from a representative set of samples. We need that the mean  covariance  and tail bounds of F (X  p)
behave like we would expect them to. We call a set of samples that satisﬁes these properties -good
for P .

Our algorithm takes as input an -corrupted multiset S(cid:48) of N =(cid:101)Ω(m log(1/τ )/2) samples. We write

S(cid:48) = (S \ L) ∪ E  where S is the set of samples before corruption  L contains the good samples that
have been removed or (in later iterations) incorrectly rejected by ﬁlters  and E represents the remaining
corrupted samples. We assume that S is -good. In the beginning  we have |E| + |L| ≤ 2|S|. As we
add ﬁlters in each iteration  E gets smaller and L gets larger. However  we will prove that our ﬁlter
rejects more samples from E than S  so |E| + |L| must get smaller.
We will prove Theorem 3 by iteratively running the following efﬁcient ﬁltering procedure:
Proposition 9 (Filtering). Let 0 <  < 1/2. Let P be a c-balanced Bayesian network on {0  1}d
with known structure G. Assume each parental conﬁguration of P occurs with probability at least

α ≥ Ω((cid:112)log(1/)/c). Let S(cid:48) = S ∪ E \ L be a set of samples such that S is -good for P and
|E| + |L| ≤ 2|S(cid:48)|. There is an algorithm that  given G    and S(cid:48)  runs in time (cid:101)O(d|S(cid:48)|)  and either
(i) Outputs a Bayesian network Q with dTV (P  Q) ≤ (cid:112)ln(1/)/(cα)  or

(ii) Returns an S(cid:48)(cid:48) = S ∪ E(cid:48) \ L(cid:48) such that |S(cid:48)(cid:48)| ≤ (1 − 

d ln d )|S(cid:48)| and |E(cid:48)| + |L(cid:48)| < |E| + |L|.
If this algorithm produces a subset S(cid:48)(cid:48)  then we iterate using S(cid:48)(cid:48) in place of S(cid:48). We will present the
algorithm establishing Proposition 9 in the following section. We ﬁrst use it to prove Theorem 3.

Proof of Theorem 3. First a set S of N = (cid:101)Ω(m log(1/τ )/2) samples are drawn from P . We

assume the set S is -good for P . Then an -fraction of these samples are adversarially corrupted 
giving a set S(cid:48) = S ∪ E \ L with |E| |L| ≤ |S(cid:48)|. Thus S(cid:48) satisﬁes the conditions of Proposition
9  and the algorithm outputs a smaller set S(cid:48)(cid:48) of samples that also satisﬁes the conditions of the
proposition  or else outputs a Bayesian network Q with small dTV (P  Q) that satisﬁes Theorem 3.
Since |S(cid:48)| decreases if we produce a ﬁlter  eventually we must output a Bayesian network.

5

Next we analyze the running time. Observe that we can ﬁlter out at most 2N samples  because we
reject more bad samples than good ones. By Proposition 9  every time we produce a ﬁlter  we remove

at least (cid:101)Ω(d/)|S(cid:48)| = (cid:101)Ω(N d/) samples. Therefore  there are at most (cid:101)O(d) iterations  and each
iteration takes time (cid:101)O(d|S(cid:48)|) = (cid:101)O(N d) by Proposition 9  so the overall running time is (cid:101)O(N d2).

3.2 Algorithm Filter-Known-Topology

In this section  we present Algorithm 1 that establishes Proposition 9. 2

Algorithm 1 Filter-Known-Topology
1: Input: The dependency graph G of P    > 0  and a (possibly corrupted) set of samples S(cid:48)
from P . S(cid:48) satisﬁes that there exists an -good S with S(cid:48) = S ∪ E \ L and |E| + |L| ≤ 2|S(cid:48)|.
2: Output: A Bayes net Q or a subset S(cid:48)(cid:48) ⊂ S(cid:48) that satisﬁes Proposition 9.
3: Compute the empirical conditional probabilities q(i  a) = PrX∈uS(cid:48) [Xi = 1 | Πi a].
4: Compute the empirical minimum parental conﬁguration probability α = min(i a) PrS(cid:48)[Π(i a)].
5: Deﬁne F (X  q): If x ∈ Πi a then F (x  q)i a = xi  otherwise F (x  q)i a = qi a (Deﬁnition 6).
6: Compute the empirical second-moment matrix of F (X  q) − q and zero its diagonal  i.e.  M ∈
Rm×m with Mk k = 0  and Mk (cid:96) = EX∈uS(cid:48)[(F (X  q)k − qk)(F (X  q)(cid:96) − q(cid:96))T ] for k (cid:54)= (cid:96).
7: Compute the largest (in absolute value) eigenvalue λ∗ of M  and the associated eigenvector v∗.
8: if |λ∗| ≤ O( log(1/)/α) then
9:
10: else
11:

Let δ := 3(cid:112)|λ∗|/α. Pick any T > 0 that satisﬁes

Return Q = the Bayes net with graph G and conditional probabilities q.

X∈uS(cid:48)[|v∗ · (F (X  q) − q)| > T + δ] > 7 exp(−T 2/2) + 32/(T 2 ln d) .

Pr

Return S(cid:48)(cid:48) = the set of samples x ∈ S(cid:48) with |v · (F (x  q) − q)| ≤ T + δ.

At a high level  Algorithm 1 computes a matrix M  and shows that: either (cid:107)M(cid:107)2 is small  and we can
output the empirical conditional probabilities  or (cid:107)M(cid:107)2 is large  and we can use the top eigenvector
of M to remove bad samples.

Setup and Structural Lemmas.
In order to understand the second-moment matrix with zeros on
the diagonal  M  we will need to break down this matrix in terms of several related matrices  where the
expectation is taken over different sets. For a set D = S(cid:48)  S  E or L  we use wD = |D|/|S(cid:48)| to denote
the fraction of the samples in D. Moreover  we use MD = EX∈uD[((F (X  q) − q)(F (X  q) − q)T ]
to denote the second-moment matrix of samples in D  and let MD 0 be the matrix we get from
zeroing out the diagonals of MD. Under this notation  we have MS(cid:48) = wSMS + wEME − wLML
and M = MS(cid:48) 0.
Our ﬁrst step is to analyze the spectrum of M  and in particular show that M is close in spectral norm
to wEME. To do this  we begin by showing that the spectral norm of MS 0 is relatively small. Since
S is good  we have bounds on the second moments F (X  p). We just need to deal with the error from
replacing p with q:

Lemma 10. (cid:107)MS 0(cid:107)2 ≤ O( +(cid:112)(cid:80)

k PrS[Πk](pk − qk)2 +(cid:80)

k PrS[Πk](pk − qk)2).

Next  we wish to bound the contribution to M coming from the subtractive error. We show that this
is small due to concentration bounds on P and hence on S. The idea is that for any unit vector v  we
have tail bounds for the random variable v · (F (X  q) − q) and  since L is a subset of S  L can at
worst consist of a small fraction of the tail of this distribution.
Lemma 11. wL(cid:107)ML(cid:107)2 ≤ O( log(1/) + (cid:107)p − q(cid:107)2
2).
Finally  combining the above results  since MS and ML have small contribution to the spectral norm
of M when (cid:107)p − q(cid:107)2 is small  most of it must come from ME.
Lemma 12.

(cid:107)M − wEME(cid:107)2 ≤ O(cid:0) log(1/) +(cid:112)(cid:80)

k PrS(cid:48)[Πk](pk − qk)2 +(cid:80)

k PrS(cid:48)[Πk](pk − qk)2(cid:1).

2 We use X ∈u S to denote that the point X is drawn uniformly from the set of samples S.

6

Lemma 12 follows using the identity |S(cid:48)|M = |S|MS 0 + |E|ME 0 − |L|ML 0 and bounding the
errors due to the diagonals of ME and ML.

In this section  we will prove that if (cid:107)M(cid:107)2 = O( log(1/)/α) 
The Case of Small Spectral Norm.
then we can output the empirical conditional means q. Recall that MS(cid:48) = EX∈uS(cid:48)[(F (X  q)i −
qi)(F (X  q)j − qj)T ] and M = MS(cid:48) 0.
We ﬁrst show that the contributions that L and E make to EX∈uS(cid:48)[F (X  q) − q)] can be bounded in
terms of the spectral norms of ML and ME. It follows from the Cauchy-Schwarz inequality that:

Lemma 13. (cid:107)EX∈uL[F (X  q) − q](cid:107)2 ≤(cid:112)(cid:107)ML(cid:107)2 and (cid:107)EX∈uE[F (X  q) − q](cid:107)2 ≤(cid:112)(cid:107)ME(cid:107)2.

Combining with the results about these norms in Section 3.2  Lemma 13 implies that if (cid:107)M(cid:107)2 is
small  then q = EX∈uS(cid:48)[F (X  q)] is close to EX∈uS[F (X  q)]  which is then necessarily close to
EX∼P [F (X  p)] = p. The following lemma states that the mean of (F (X  q) − q) under the good
samples is close to (p − q) scaled by the probabilities of parental conﬁgurations under S(cid:48):
Lemma 14. Let z ∈ Rm be the vector with zk = PrS(cid:48)[Πk](pk − qk). Then (cid:107)EX∈uS[F (X  q)− q]−
z(cid:107)2 ≤ O((1 + (cid:107)p − q(cid:107)2)).
Note that z is closely related to the total variation distance between P and Q (the Bayes net with
conditional probabilities q). We can write (EX∈uS(cid:48)[F (X  q)] − q) in terms of this expectation
under S  E  and L whose distance from q can be upper bounded using the previous lemmas. Using
Lemmas 11  12  13  and 14  we can bound (cid:107)z(cid:107)2 in terms of (cid:107)M(cid:107)2:

k PrS(cid:48)[Πk]2(pk − qk)2 ≤ 2(cid:112)(cid:107)M(cid:107)2 + O((cid:112)log(1/) + 1/α).

k PrP [Πk](pk − qk)2 is small. We can do so by losing a factor of 1/

k PrS(cid:48)[Πk]2(pk − qk)2. We can then use it
α to remove
S[Πk] = Θ(mink PrP [Πk]) when it is at least a
k PrP [Πk](pk − qk)2 is small  Lemma 5 tells us that dTV (P  Q)
is small. This completes the proof of the ﬁrst case of Proposition 9.
Corollary 16 (Part (i) of Proposition 9). If (cid:107)M(cid:107)2 ≤ O( log(1/)/α)  then dTV (P  Q) =

√

the square on PrS(cid:48)[Πk]  and showing that mink Pr(cid:48)

Lemma 15. (cid:112)(cid:80)
Lemma 15 implies that  if (cid:107)M(cid:107)2 is small then so is(cid:112)(cid:80)
to show that(cid:112)(cid:80)
large multiple of . Finally  if(cid:112)(cid:80)
O((cid:112)log(1/)/(c mink PrP [Πk])).
Claim 17. (cid:107)p − q(cid:107)2 ≤ δ := 3(cid:112)(cid:107)M(cid:107)2/α.

The Case of Large Spectral Norm. Now we consider the case when (cid:107)M(cid:107)2 ≥ C ln(1/)/α. We
begin by showing that p and q are not too far apart from each other. The bound given by Lemma 15
is now dominated by the (cid:107)M(cid:107)2 term. Lower bounding the PrS(cid:48)[Πk] by α gives the following claim.

2 v∗T M v∗.

Recall that v∗ is the largest eigenvector of M. We project all the points F (X  q) onto the direction of
v∗. Next we show that most of the variance of (v∗ · (F (X  q) − q)) comes from E.
Claim 18. v∗T (wEME)v∗ ≥ 1
Claim 18 follows from the observation that (cid:107)M − wEME(cid:107)2 is much smaller than (cid:107)M(cid:107)2. This is
obtained by substituting the bound on (cid:107)p − q(cid:107)2 (in terms of (cid:107)M(cid:107)2) from Claim 17 into the bound on
(cid:107)M − wEME(cid:107)2 given by Lemma 12.
Claim 18 implies that the tails of wEE are reasonably thick. In particular  we show that there must
be a threshold T > 0 satisfying the desired property in Step 9 of our algorithm.
Lemma 19. There exists a T ≥ 0 such that

X∈uS(cid:48)[|v · (F (X  q) − q)| > T + δ] > 7 exp(−T 2/2) + 3/(T 2 ln d) .

Pr

If Lemma 19 were not true  by integrating this tail bound  we can show that v∗T MEv∗ would be
small. Therefore  Step 11 of Algorithm 11 is guaranteed to ﬁnd some valid threshold T > 0.
Finally  we show that the set of samples S(cid:48)(cid:48) we return after the ﬁlter is better than S(cid:48) in terms of
|L| + |E|. This completes the proof of the second case of Proposition 9.
Claim 20 (Part (ii) of Proposition 9). If we write S(cid:48)(cid:48) = S ∪ E(cid:48) \ L(cid:48)  then |E(cid:48)| + |L(cid:48)| < |E| + |L|
and |S(cid:48)(cid:48)| ≤ (1 − 

d ln d )|S(cid:48)|.

7

Claim 9 follows from the fact that S is -good  so we only remove at most (3 exp(T 2/2) +
/T 2 log d)|S| samples from S. Since we remove more than twice as many samples from S(cid:48) 
most of the samples we throw away are from E. Moreover  we remove at least (1 − 
d ln d )|S(cid:48)|
samples because we can show that the threshold T is at most

√

d.

each iteration  we implement matrix-vector multiplication with M by writing M v as(cid:80)

Running Time of Our Algorithm 1 First  q and α can be computed in time O(N d) because each
sample only affects d entries of q. We do not explicitly write down F (X  q) or M. Then  we use
the power method to compute the largest eigenvalue λ∗ of M and the associated eigenvector v∗. In
i((F (xi  q)−
q)T v)(F (xi  q)− q) for any vector v ∈ Rm. Because each (F (xi  q)− q) is d-sparse  computing M v
takes time O(dN ). The power method takes (log m/(cid:48)) iterations to ﬁnd a (1 − (cid:48))-approximately
largest eigenvalue. We can set (cid:48) to a small constant  because we can tolerate a small multiplicative
error in estimating the spectral norm of M and we only need an approximate top eigenvector (see  e.g. 
Corollary 16 and Lemma 18). Thus  the power method takes time O(dN log m). Finally  computing
|v∗ · (F (x  q) − q)| takes time O(dN )  then we can sort the samples and ﬁnd a threshold T in time
O(N log N )  and throw out the samples in time O(N ).

4 Experiments

We test our algorithms using data generated from both synthetic and real-world networks (e.g.  the
ALARM network [BSCC89]) with synthetic noise. All experiments were run on a laptop with 2.6
GHz CPU and 8 GB of RAM. We found that our algorithm achieves the smallest error consistently in
all trials  and that the error of our algorithm almost matches the error of the empirical conditional
probabilities of the uncorrupted samples. Moreover  our algorithm can easily scale to thousands of
dimensions with millions of samples. 3

4.1 Synthetic Experiments

The results of our synthetic experiments are shown in Figure 1. In the synthetic experiment  we
set  = 0.1 and ﬁrst generate a Bayes net P with 100 ≤ m ≤ 1000 parameters. We then generate
samples  where a (1 − )-fraction of the samples come from the ground truth P   and
N = 10m
2
the remaining -fraction come from a noise distribution. The goal is to output a Bayes net Q that
minimizes dTV (P  Q).

Figure 1: Experiments with synthetic data: error is reported against the size of the conditional
probability table (lower is better). The error is the estimated total variation distance to the ground
truth Bayes net. We use the error of MLE without noise as our benchmark. We plot the performance
of our algorithm (Filtering)  empirical mean with noise (MLE)  and RANSAC. We report two settings:
the underlying structure of the Bayes net is a random tree (left) or a random graph (right).
We draw the parameters of P independently from [0  1/4] ∪ [3/4  1] uniformly at random  i.e.  in a
setting where the “balancedness” assumption does not hold. Our experiments show that our ﬁltering
algorithm works very well in this setting  even when the assumptions under which we can prove
theoretical guarantees are not satisﬁed. This complements our theoretical results and illustrates that
our algorithm is not limited by these assumptions and can apply to more general settings in practice.

3 The bottleneck of our algorithm is ﬁtting millions of samples of thousands dimension all in the memory.

8

00.10.20.30.40.50.60.70.80.91.001002003004005006007008009001000Numberofparameters(m)EstimateddTV++++++++++××××××××××ldldldldldldldldldldbcbcbcbcbcbcbcbcbcbc+MLEw/onoise×FilteringldMLEw/noisebcRANSAC00.10.20.30.40.50.60.70.80.91.001002003004005006007008009001000Numberofparameters(m)EstimateddTV++++++++++××××××××××ldldldldldldldldldldbcbcbcbcbcbcbcbcbcbc+MLEw/onoise×FilteringldMLEw/noisebcRANSACIn Figure 1  we compare the performance of (1) our ﬁltering algorithm  (2) the empirical conditional
probability table with noise  and (3) a RANSAC-based algorithm (see the end of Section 4 for a
detailed description). We use the error of the empirical conditional mean without noise (i.e.  MLE
estimator with only good samples) as the gold standard  since this is the best one could hope for even
if all the corrupted samples are identiﬁed. We tried various graph structures for the Bayes net P and
noise distributions  and similar patterns arise for all of them. In the top ﬁgure  the dependency graph
of P is a randomly generated tree  and the noise distribution is a binary product distribution; In the
bottom ﬁgure  the dependency graph of P is a random graph  and the noise distribution is the tree
Bayes net used as the ground truth in the ﬁrst experiment.

4.2 Semi-Synthetic Experiments

In the semi-synthetic experiments  we apply our algorithm to robustly learn real-world Bayesian
networks. The ALARM network [BSCC89] is a classic Bayes net that implements a medical
diagnostic system for patient monitoring.
Our experimental setup is as follows: The underlying graph of ALARM has 37 nodes and 509
parameters. Since the variables in ALARM can have up to 4 different values  we ﬁrst transform it
into an equivalent binary-valued Bayes net. After the transformation  the network has d = 61 nodes
and m = 820 parameters. We are interested in whether our ﬁltering algorithm can learn a Bayes net
that is “close” to ALARM when samples are corrupted; and how many corrupted samples can our
algorithm tolerate. For  = [0.05  0.1  . . .   0.4]  we draw N = 106 samples  where a (1 − )-fraction
of the samples come from ALARM  and the other -fraction comes from a noise distribution.

Figure 2: Experiments with semi-synthetic data:
error is reported against the fraction of corrupted
samples (lower is better). The error is the esti-
mated total variation distance to the ALARM net-
work. We use the sampling error without noise
as a benchmark  and compare the performance of
our algorithm (Filtering)  empirical mean with
noise (MLE)  and RANSAC.

In Figure 2  we compare the performance of (1) our ﬁltering algorithm  (2) the empirical conditional
means with noise  and (3) a RANSAC-based algorithm. We use the error of the empirical conditional
means without noise as the gold standard. We tried various noise distributions and observed similar
patterns. In Figure 2  the noise distribution is a Bayes net with random dependency graphs and
conditional probabilities drawn from [0  1
4   1] (same as the ground-truth Bayes net in Figure 1).
The experiments show that our ﬁltering algorithm outperforms MLE and RANSAC  and that the error of
our algorithm degrades gracefully as  increases. It is worth noting that even the ALARM network
does not satisfy our balancedness assumption on the parameters  our algorithm still performs well on
it and recovers the conditional probability table of ALARM in the presence of corrupted samples.

4 ] ∪ [ 3

5 Conclusions and Future Directions

In this paper  we initiated the study of the efﬁcient robust learning for graphical models. We described
a computationally efﬁcient algorithm for robustly learning Bayesian networks with a known topology 
under some mild assumptions on the conditional probability table. We evaluate our algorithm
experimentally  and we view our experiments as a proof of concept demonstration that our techniques
can be practical for learning ﬁxed-structure Bayesian networks. A challenging open problem is to
generalize our results to the case when the underlying directed graph is unknown.
This work is part of a broader agenda of systematically investigating the robust learnability of high-
dimensional structured probability distributions. There is a wealth of natural probabilistic models that
merit investigation in the robust setting  including undirected graphical models (e.g.  Ising models) 
and graphical models with hidden variables (i.e.  incorporating latent structure).

9

00.10.20.30.40.50.60.70.80.91.000.050.100.150.200.250.300.350.40Fractionofcorruptedsamples(ǫ)EstimateddTV++++++++××××××××ldldldldldldldldbcbcbcbcbcbcbcbc+MLEw/onoise×FilteringldMLEw/noisebcRANSACAcknowledgements. We are grateful to Daniel Hsu for suggesting the model of Bayes nets  and for
pointing us to [Das97]. Yu Cheng is supported in part by NSF CCF-1527084  CCF-1535972  CCF-
1637397  CCF-1704656  IIS-1447554  and NSF CAREER Award CCF-1750140. Ilias Diakonikolas
is supported by NSF CAREER Award CCF-1652862 and a Sloan Research Fellowship. Daniel Kane
is supported by NSF CAREER Award CCF-1553288 and a Sloan Research Fellowship.

10

References
[ADLS16] J. Acharya  I. Diakonikolas  J. Li  and L. Schmidt. Fast algorithms for segmented
regression. In Proceedings of the 33nd International Conference on Machine Learning 
ICML 2016  pages 2878–2886  2016.

[ADLS17] J. Acharya  I. Diakonikolas  J. Li  and L. Schmidt. Sample-optimal density estimation in
nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
on Discrete Algorithms  SODA 2017  pages 1278–1289  2017.

[AHHK12] A. Anandkumar  D. J. Hsu  F. Huang  and S. Kakade. Learning mixtures of tree
graphical models. In Proc. 27th Annual Conference on Neural Information Processing
Systems (NIPS)  pages 1061–1069  2012.

[AKN06] P. Abbeel  D. Koller  and A. Y. Ng. Learning factor graphs in polynomial time and

sample complexity. J. Mach. Learn. Res.  7:1743–1788  2006.

[BDLS17] S. Balakrishnan  S. S. Du  J. Li  and A. Singh. Computationally efﬁcient robust sparse
estimation in high dimensions. In Proc. 30th Annual Conference on Learning Theory
(COLT)  pages 169–212  2017.

[Ber06] T. Bernholt. Robust estimators are hard to compute. Technical report  University of

Dortmund  Germany  2006.

[BGS14] G. Bresler  D. Gamarnik  and D. Shah. Structure learning of antiferromagnetic Ising

models. In NIPS  pages 2852–2860  2014.

[BMS13] G. Bresler  E. Mossel  and A. Sly. Reconstruction of Markov random ﬁelds from
samples: Some observations and algorithms. SIAM J. Comput.  42(2):563–578  2013.

[Bre15] G. Bresler. Efﬁciently learning Ising models on arbitrary graphs. In Proc. 47th Annual

ACM Symposium on Theory of Computing (STOC)  pages 771–782  2015.

[BSCC89] I. A. Beinlich  H. J. Suermondt  R. M. Chavez  and G. F. Cooper. The ALARM
Monitoring System: A Case Study with two Probabilistic Inference Techniques for
Belief Networks. Springer  1989.

[CDKS17] C. L. Canonne  I. Diakonikolas  D. M. Kane  and A. Stewart. Testing Bayesian networks.
In Proc. 30th Annual Conference on Learning Theory (COLT)  pages 370–448  2017.

[CDSS13] S. Chan  I. Diakonikolas  R. Servedio  and X. Sun. Learning mixtures of structured
distributions over discrete domains. In Proc. 24th Annual Symposium on Discrete
Algorithms (SODA)  pages 1380–1394  2013.

[CDSS14a] S. Chan  I. Diakonikolas  R. Servedio  and X. Sun. Efﬁcient density estimation via
piecewise polynomial approximation. In Proc. 46th Annual ACM Symposium on Theory
of Computing (STOC)  pages 604–613  2014.

[CDSS14b] S. Chan  I. Diakonikolas  R. Servedio  and X. Sun. Near-optimal density estimation in
near-linear time using variable-width histograms. In Proc. 29th Annual Conference on
Neural Information Processing Systems (NIPS)  pages 1844–1852  2014.

[CGR15] M. Chen  C. Gao  and Z. Ren. Robust covariance and scatter matrix estimation under

Huber’s contamination model. CoRR  abs/1506.00691  2015.

[CGR16] M. Chen  C. Gao  and Z. Ren. A general decision theory for Huber’s -contamination

model. Electronic Journal of Statistics  10(2):3752–3774  2016.

[CL68] C. Chow and C. Liu. Approximating discrete probability distributions with dependence

trees. IEEE Trans. Inf. Theor.  14(3):462–467  1968.

[Das97] S. Dasgupta. The sample complexity of learning ﬁxed-structure Bayesian networks.

Machine Learning  29(2-3):165–180  1997.

11

[DDS14] C. Daskalakis  I. Diakonikolas  and R. A. Servedio. Learning k-modal distributions via

testing. Theory of Computing  10(20):535–570  2014.

[DKK+16] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proc. 57th
IEEE Symposium on Foundations of Computer Science (FOCS)  2016.

[DKK+17] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Being
robust (in high dimensions) can be practical. In Proc. 34th International Conference on
Machine Learning (ICML)  pages 999–1008  2017.

[DKK+18a] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robustly
In Proc. 29th ACM-SIAM

learning a Gaussian: Getting optimal error  efﬁciently.
Symposium on Discrete Algorithms (SODA)  2018.

[DKK+18b] I. Diakonikolas  G. Kamath  D. M Kane  J. Li  J. Steinhardt  and A. Stewart. Sever: A
robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815 
2018.

[DKS17] I. Diakonikolas  D. M. Kane  and A. Stewart. Statistical query lower bounds for robust
estimation of high-dimensional Gaussians and Gaussian mixtures. In Proc. 58th IEEE
Symposium on Foundations of Computer Science (FOCS)  pages 73–84  2017.

[DKS18a] I. Diakonikolas  D. M. Kane  and A. Stewart. Learning geometric concepts with nasty
noise. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC)  pages
1061–1073  2018.

[DKS18b] I. Diakonikolas  D. M. Kane  and A. Stewart. List-decodable robust mean estimation
and learning mixtures of spherical Gaussians. In Proc. 50th Annual ACM Symposium
on Theory of Computing (STOC)  pages 1047–1060  2018.

[DKS18c] I. Diakonikolas  W. Kong  and A. Stewart. Efﬁcient algorithms and lower bounds for

robust linear regression. CoRR  abs/1806.00040  2018.

[DLS18] I. Diakonikolas  J. Li  and L. Schmidt. Fast and sample near-optimal algorithms for
learning multidimensional histograms. In Conference On Learning Theory  COLT 2018 
pages 819–842  2018.

[DSA11] R. Daly  Q. Shen  and S. Aitken. Learning Bayesian networks: approaches and issues.

The Knowledge Engineering Review  26:99–157  2011.

[HL18] S. B. Hopkins and J. Li. Mixture models  robustness  and sum of squares proofs. In Proc.
50th Annual ACM Symposium on Theory of Computing (STOC)  pages 1021–1034 
2018.

[HM13] M. Hardt and A. Moitra. Algorithms and hardness for robust subspace recovery. In

Proc. 26th Annual Conference on Learning Theory (COLT)  pages 354–375  2013.

[HR09] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York  2009.

[HRRS86] F. R. Hampel  E. M. Ronchetti  P. J. Rousseeuw  and W. A. Stahel. Robust statistics:

The approach based on inﬂuence functions. Wiley New York  1986.

[Hub64] P. J. Huber. Robust estimation of a location parameter. Ann. Math. Statist.  35(1):73–

101  03 1964.

[JN07] F. V. Jensen and T. D. Nielsen. Bayesian Networks and Decision Graphs. Springer

Publishing Company  Incorporated  2nd edition  2007.

[JP78] D. S. Johnson and F. P. Preparata. The densest hemisphere problem. Theoretical

Computer Science  6:93–107  1978.

[KF09] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques

- Adaptive Computation and Machine Learning. The MIT Press  2009.

12

[KKM18] A. Klivans  P. Kothari  and R. Meka. Efﬁcient algorithms for outlier-robust regression.
In Proc. 31st Annual Conference on Learning Theory (COLT)  pages 1420–1430  2018.

[KSS18] P. K. Kothari  J. Steinhardt  and D. Steurer. Robust moment estimation and improved
clustering via sum of squares. In Proc. 50th Annual ACM Symposium on Theory of
Computing (STOC)  pages 1035–1046  2018.

[LRV16] K. A. Lai  A. B. Rao  and S. Vempala. Agnostic estimation of mean and covariance. In

Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS)  2016.

[LSLC18] L. Liu  Y. Shen  T. Li  and C. Caramanis. High dimensional robust sparse regression.

CoRR  abs/1805.11643  2018.

[LW12] P. L. Loh and M. J. Wainwright. Structure estimation for discrete graphical models:
Generalized covariance matrices and their inverses. In NIPS  pages 2096–2104  2012.

[Nea03] R. E. Neapolitan. Learning Bayesian Networks. Prentice-Hall  Inc.  2003.

[PSBR18] A. Prasad  A. S. Suggala  S. Balakrishnan  and P. Ravikumar. Robust estimation via

robust gradient estimation. arXiv preprint arXiv:1802.06485  2018.

[SW12] N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary
graphical models in high dimensions. IEEE Trans. Information Theory  58(7):4117–
4134  2012.

[Tuk75] J. W. Tukey. Mathematics and the picturing of data. In Proceedings of the International

Congress of Mathematicians  volume 6  pages 523–531  1975.

[WJ08] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and

variational inference. Found. Trends Mach. Learn.  1(1-2):1–305  2008.

[WRL06] M. J. Wainwright  P. Ravikumar  and J. D. Lafferty. High-dimensional graphical model
selection using (cid:96)1-regularized logistic regression. In Proc. 20th Annual Conference on
Neural Information Processing Systems (NIPS)  pages 1465–1472  2006.

13

,Yu Cheng
Ilias Diakonikolas
Daniel Kane
Alistair Stewart